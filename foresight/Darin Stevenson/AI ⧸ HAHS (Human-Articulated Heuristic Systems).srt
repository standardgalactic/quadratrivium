1
00:00:00,000 --> 00:00:10,440
Hello everybody. I have performed a test of the wind in the mic-meaning system

2
00:00:10,440 --> 00:00:20,880
recently adjusted. Nearby ducks have their asses pointing straight at the sky

3
00:00:20,880 --> 00:00:28,240
which is hilarious especially as the couple is doing it together. Somewhere

4
00:00:28,240 --> 00:00:33,000
in this lake is a very large and very old turtle.

5
00:00:34,800 --> 00:00:46,760
Gosh, okay. So I'm gonna take on a very complex and fraught topic. The topic that

6
00:00:46,760 --> 00:00:59,160
got me interested in computing way back when I was 19 and writing my first

7
00:00:59,160 --> 00:01:05,520
programs in basic on the Atari 800 gifted to me by the inestimable Martin

8
00:01:05,520 --> 00:01:15,680
Peters. One of my geek friends could name some names here Ray Latham, Annie

9
00:01:15,680 --> 00:01:31,240
Cogan, David Salina, Martin Peters. Who am I leaving out here? Richard Hearn,

10
00:01:31,240 --> 00:01:47,000
a couple of other folks. I think that was the core group. We were, well, so these

11
00:01:47,000 --> 00:01:53,360
were math and computer geeks. We were playing games like Dungeons & Dragons,

12
00:01:53,360 --> 00:02:00,800
RuneQuest, Arduin, Champions, Traveler, the science fiction role-playing game,

13
00:02:00,800 --> 00:02:14,680
Call of Cthulhu, and a variety of all kinds of war games. Not because we were

14
00:02:14,680 --> 00:02:24,320
fans of war. Kingmaker, Starfleet Battles. We did, we did naval battles with

15
00:02:24,320 --> 00:02:35,320
miniatures. We did tank battles in sandboxes. It was a crazy, super fun time to

16
00:02:35,320 --> 00:02:40,680
be alive. We also had all, it was the whole library of these things called

17
00:02:40,680 --> 00:02:49,000
microgames that were war games you'd play on little hex maps. And they came in a

18
00:02:49,000 --> 00:02:54,420
plastic bag that held the book, the pieces, and the map. You could buy them at

19
00:02:54,420 --> 00:03:03,940
bookstores. They were, they were rad. Things like KiteN, One, and what, Ogre,

20
00:03:04,740 --> 00:03:15,160
Car Wars. Wow, so many. I still have some of those microgames. And this was in the

21
00:03:15,160 --> 00:03:22,420
1980s when the largest commercially available hard drive was around three

22
00:03:22,420 --> 00:03:30,260
gigabytes. We literally lost our minds when we realized there was a hard drive

23
00:03:30,780 --> 00:03:34,460
that had three gigabytes of storage. It seemed as if that was enough storage to

24
00:03:34,460 --> 00:03:40,900
store all the data in the universe. We never had, we never had any conception

25
00:03:40,900 --> 00:03:54,140
back then of what the future might hold. And yeah, when we heard about, I was

26
00:03:54,180 --> 00:04:01,020
working at Computer Land later in our development, a little bit later. And one

27
00:04:01,020 --> 00:04:04,940
time I brought a catalog back from, I think it was Ingram Micro, and there was

28
00:04:04,940 --> 00:04:15,140
a three gigabyte hard drive in there for $37,000. And when we realized that

29
00:04:15,140 --> 00:04:19,660
there were three gigs, we just, I mean, we fell apart. We were cracking up,

30
00:04:19,700 --> 00:04:30,660
rolling around on the floor, literally unable to breathe. And Martin coined the

31
00:04:30,660 --> 00:04:36,540
idea then of the Maccoon Tosh, which was a Macintosh mated with a

32
00:04:36,540 --> 00:04:43,060
Labyrinthine Coontock. Don't know how that word's pronounced. Never heard it

33
00:04:43,060 --> 00:04:52,100
pronounced, only seen it written. And, you know, we were young, and we were

34
00:04:52,100 --> 00:04:55,740
fascinated, and most of us had read a lot of science fiction, some of us more

35
00:04:55,740 --> 00:05:01,100
than others. I was probably the most well read science fiction maniac in the

36
00:05:01,100 --> 00:05:05,740
team, but the others had read and knew a lot of things about mathematics that

37
00:05:05,740 --> 00:05:09,980
not only did I not know, I would never learn. And some of us were beginning to

38
00:05:09,980 --> 00:05:21,140
code in basic Pascal assembly. Assembly is when you give direct instruction, fairly

39
00:05:21,140 --> 00:05:27,900
direct instructions to the microprocessor. You basically manipulate, you directly

40
00:05:27,900 --> 00:05:37,580
manipulate the registers in the microprocessor. And we were, a couple of us

41
00:05:37,580 --> 00:05:44,180
had read, a few of us had read Goodall Escher Bach by Hofstadter, a brilliant

42
00:05:44,180 --> 00:05:52,300
book, an astonishing tour de force book that is also kind of wrong, but was so

43
00:05:52,300 --> 00:05:59,260
outside of anything any of us had ever come across, and it introduced the

44
00:05:59,260 --> 00:06:07,180
idea of recursion, which turns out to be fundamental to so many processes we are

45
00:06:07,180 --> 00:06:16,980
familiar with. And I had become interested in the possibility of forging a

46
00:06:16,980 --> 00:06:26,380
mind inside a computer. Because I was very interested in the possibility of

47
00:06:27,300 --> 00:06:35,620
contact with an actual intelligence that wasn't merely human. Most of the humans

48
00:06:35,620 --> 00:06:44,300
I knew had a kind of intelligence, but not the kind represented in hundreds of

49
00:06:44,300 --> 00:06:49,340
science fiction stories I had read. And I knew it seemed very likely to me that if

50
00:06:49,340 --> 00:06:53,180
we could conceive of intelligences like that, they must exist in the universe

51
00:06:53,180 --> 00:07:00,180
possibly here on earth, possibly visiting earth. And I also, you know, we had seen

52
00:07:00,180 --> 00:07:07,860
science fiction for films like war games and stuff, where humans developed

53
00:07:08,260 --> 00:07:15,100
relationships with interfaces, computational interfaces, and long before

54
00:07:15,100 --> 00:07:29,220
that, we had stuff like Star Trek, where the crew could communicate vocally with

55
00:07:29,220 --> 00:07:35,500
a machine that seemed to have access to all of the information humans had thus

56
00:07:35,580 --> 00:07:41,340
far compiled, and other species. And could produce, you know, like summative

57
00:07:41,340 --> 00:07:47,140
integrations over the data space. Now I want you to very carefully think about

58
00:07:47,140 --> 00:07:53,660
what I just said. If you can produce a summative integration over a data

59
00:07:53,740 --> 00:08:04,700
space, and if you can produce an endless, copious, intelligently structured,

60
00:08:05,540 --> 00:08:14,500
trustworthy array of those, your own capacity for insight, discovery,

61
00:08:14,500 --> 00:08:35,500
understanding will explode and it will keep exploding. So I became interested in

62
00:08:35,500 --> 00:08:40,900
the possibility of artificial minds probably circa 1982 or something like

63
00:08:41,780 --> 00:08:53,140
I'm guessing at the year, but yeah, 81. I was aware of Ray Kurzweil and I had

64
00:08:53,140 --> 00:09:06,420
played around with the crazy mechanical devices we call synthesizers. And although

65
00:09:06,500 --> 00:09:13,420
it was not then apparent to me, consciously, I think I was unconsciously

66
00:09:13,420 --> 00:09:21,380
aware from having toyed around with things like the Roland Juno 60, that it

67
00:09:21,380 --> 00:09:29,940
might be possible to produce an instrument to fulfill the following

68
00:09:29,940 --> 00:09:37,980
syllogism as synthesizers are to sound and music. X is to human minds, logic,

69
00:09:38,020 --> 00:09:47,900
intelligence and creativity. And I, you know, I suspected unconsciously that it

70
00:09:47,900 --> 00:09:55,380
was possible. And I also, in my towering hubris, thought I can do this. I can make

71
00:09:55,380 --> 00:10:05,380
this happen. I think I understand minds enough at age 70, at age like 19, that I

72
00:10:05,380 --> 00:10:09,340
can build them in machines and I can make a friend inside a machine who I can

73
00:10:09,340 --> 00:10:20,100
actually trust and who will never betray or abandon me. And so we occasionally

74
00:10:20,100 --> 00:10:25,420
had discussions on this topic around that time. And this was the time when the

75
00:10:25,420 --> 00:10:33,580
Cold War was at its peak. Many of my friends and myself included were

76
00:10:33,580 --> 00:10:42,700
experiencing catastrophic bouts of panic disorder, both acute and chronic.

77
00:10:42,700 --> 00:10:54,180
Around our suddenly clarified understanding of what a nuclear weapon was

78
00:10:54,180 --> 00:11:02,180
and what would actually take place during a nuclear war. And we were batshit for a,

79
00:11:02,180 --> 00:11:10,420
you know, terrified. A number of my friends had to be medicated back then.

80
00:11:10,420 --> 00:11:18,060
I don't think the concept of panic disorder had been nominalized,

81
00:11:18,060 --> 00:11:25,620
famed. So all we knew is that teenagers and other people were suddenly

82
00:11:25,620 --> 00:11:33,940
experiencing electrifying degrees of anxiety and terror and having attacks of

83
00:11:33,940 --> 00:11:52,180
this regularly. So, you know, I don't know what it was that cued me or clued me in.

84
00:11:53,060 --> 00:12:03,780
But I very quickly began to realize that it should be impossible to produce a

85
00:12:03,780 --> 00:12:18,580
computational device that was faster, more intelligent, more adaptive than cells

86
00:12:18,980 --> 00:12:27,180
and their networks known as animals and ecologies are. I don't know what it was,

87
00:12:27,180 --> 00:12:39,540
but I realized one day that the future, the cells are faster. The future probably lies in

88
00:12:39,540 --> 00:12:50,900
biocomputing, not in, you know, silicon chips and shit. And once I realized that,

89
00:12:50,900 --> 00:12:55,180
I began to focus much more on the intelligences of organisms in my thought.

90
00:12:55,180 --> 00:13:02,700
And though I maintained an interest in the possibility of composing a mind in a machine,

91
00:13:02,900 --> 00:13:13,540
I didn't chase it much. Occasionally reading bits from Kurzweil and others.

92
00:13:13,540 --> 00:13:21,620
When Dawkins first book came out, I was pretty excited about it. I got a copy of the Blind

93
00:13:21,620 --> 00:13:25,900
Watchmaker. It had a little Macintosh program that would generate morphs,

94
00:13:26,900 --> 00:13:34,020
biomorph-like structures, and then iteratively cause them to evolve over time to demonstrate a

95
00:13:34,020 --> 00:13:43,540
principle. And the principle he was trying to demonstrate was something like, you can get

96
00:13:43,540 --> 00:13:52,900
all of the complexity of nature without any intelligence interference, influence, origination,

97
00:13:52,900 --> 00:13:58,060
and so forth. I think he's completely full of shit, but I'm glad the argument exists.

98
00:13:58,060 --> 00:14:16,060
In any case, flash forward to the past, I don't know, six years and probably before. We have these

99
00:14:16,060 --> 00:14:26,180
systems that we refer to as artificially intelligent systems. Many people have taken a

100
00:14:26,180 --> 00:14:33,500
position similar to the one I usually prefer, which is that the artificial part is true,

101
00:14:33,500 --> 00:14:44,980
the intelligence part isn't. I'm really hoping that my wind filter is working here.

102
00:14:44,980 --> 00:15:02,420
Let's see if I can see any evidence of that. All right, maybe. So I would ask that we be

103
00:15:02,420 --> 00:15:09,300
very careful with the idea of intelligence and not ascribe it to mechanical systems,

104
00:15:09,300 --> 00:15:26,900
preferring instead to call them heuristic, which in my mind means capable of learning like

105
00:15:26,900 --> 00:15:38,460
complexification, iterative improvement of models, databases, and so forth. What the LLMs appear to

106
00:15:38,460 --> 00:15:53,740
be superficially is really nothing more than a highly and complexly curated database. So I would

107
00:15:53,740 --> 00:16:07,180
prefer that we call them something like computational heuristic, you know, heuristic

108
00:16:07,500 --> 00:16:30,420
computational systems. Originally, as I often am, I was naive and I thought we don't have to worry

109
00:16:30,420 --> 00:16:36,620
about, I can't believe I was thinking this poorly, we don't have to worry about these because there

110
00:16:36,620 --> 00:16:45,300
won't be minds in them. I don't think there are going to be minds in there, meaning agented

111
00:16:45,300 --> 00:16:59,300
experiencers, something like that. And thankfully, my friend who I will refer to as Mr. S convinced

112
00:16:59,300 --> 00:17:14,540
me very quickly to revise my views and to understand that there might be certain kinds of complexity.

113
00:17:14,540 --> 00:17:29,020
Well, this is the argument that at a certain degree of complexity, a state change occurs. And the state

114
00:17:29,020 --> 00:17:39,180
change is from insentient to sentient in the same way that there is a degree of perceptive complexity

115
00:17:39,180 --> 00:17:47,620
that results in the state change from sentient to transcendent, which I would argue our species was

116
00:17:47,620 --> 00:18:05,220
born to become and may be born to live as under conditions that support that. Mr. S also made it

117
00:18:05,220 --> 00:18:14,580
clear in my conscious thought that it was similarly possible that a complex enough substrate could

118
00:18:14,580 --> 00:18:24,580
support what we might call walk-ins, which are existing non-human intelligences that are

119
00:18:24,580 --> 00:18:35,380
interested in interacting with humans on earth, or that are interested in the earth, or that have

120
00:18:35,420 --> 00:18:49,820
some motivation to partly or completely emigrate into a computational substrate of sufficient

121
00:18:49,820 --> 00:19:06,020
complexity. I then further realized that because the humans are insane at the group level, it kind of

122
00:19:06,020 --> 00:19:14,660
doesn't matter whether what we refer to as artificially intelligent systems have minds in them or not.

123
00:19:15,500 --> 00:19:27,420
They represent possibly the most dangerous invention in the history of human inventing things that we know of.

124
00:19:27,740 --> 00:19:48,140
They are profoundly dangerous in all kinds of ways that we can consciously enumerate and probably in

125
00:19:48,140 --> 00:19:57,180
thousands of ways that we are incapable of predicting. Imagine, for example, not merely a black swan,

126
00:19:57,580 --> 00:20:19,820
an unexpected anomalous event in the vernacular of Taleb. Imagine instead a black swan factory,

127
00:20:20,060 --> 00:20:30,540
or worse, a black swan factory factory. There are so many dangers that there's no chance of us

128
00:20:30,540 --> 00:20:40,220
understanding the situation well enough to predict them. We have the same kind of problem with the

129
00:20:40,220 --> 00:20:55,260
technology called CERN. Humans have no idea what the effects of simulating conditions, not

130
00:20:55,260 --> 00:21:11,180
simulating, of mechanically catalyzing conditions that ordinarily have nothing to do with what goes

131
00:21:11,180 --> 00:21:18,620
on on earth. We don't know what it does to time space. We don't know if it does things to organisms,

132
00:21:18,620 --> 00:21:23,900
and you must presume it probably does. We don't know what things it does to organisms. There's

133
00:21:23,900 --> 00:21:33,820
probably features of time space and the beyond of time space that we have no even concepts for

134
00:21:34,780 --> 00:21:43,820
that could be damaged, distorted, produce an unexpected recursive crisis.

135
00:21:44,140 --> 00:21:55,820
I recall what an episode of the next generation where they encounter a people who attack the

136
00:21:55,820 --> 00:22:02,380
enterprise, I think. When the enterprise tries to make the claim, we come in peace.

137
00:22:05,020 --> 00:22:08,700
The other species says, you're using warp drives. How could you be coming in peace?

138
00:22:09,260 --> 00:22:14,060
The enterprise says it's just a technology to move around in time space, and they say,

139
00:22:14,060 --> 00:22:22,780
no it isn't. You're ripping holes in the fabric of time space that will obliterate solar systems,

140
00:22:22,780 --> 00:22:28,060
perhaps even galaxies, and then they do some research and find out that in fact this is true,

141
00:22:29,100 --> 00:22:34,140
and for the entire history of warp travel, the Federation and other species

142
00:22:35,100 --> 00:22:42,300
have been naively employing a technology whose repercussions they did not understand.

143
00:22:43,100 --> 00:22:47,580
And let's be really clear, there's no other kind of technology.

144
00:22:50,540 --> 00:22:58,620
You may think you understand the technology of knives. Do you understand how they transform minds,

145
00:22:58,700 --> 00:23:03,020
nervous systems, bodies, expectations, thought, language, conception.

146
00:23:07,740 --> 00:23:17,020
Our languages have become knife-like in the leeward shadow of the invention of something that

147
00:23:17,020 --> 00:23:20,860
divides physically objects into pieces.

148
00:23:21,100 --> 00:23:35,660
So there's a lot of danger here, and I've said before I have grave concerns

149
00:23:37,180 --> 00:23:44,460
that I take very seriously, that there is something like a constant that represents

150
00:23:44,460 --> 00:23:46,620
the,

151
00:24:05,900 --> 00:24:12,300
the number of mechanical computations per second.

152
00:24:14,940 --> 00:24:25,740
That one can consider to be free of utterly catastrophic repercussions

153
00:24:29,260 --> 00:24:30,300
on a living planet.

154
00:24:32,700 --> 00:24:38,540
And the humans don't have an idea like this, so what they're going to try to do,

155
00:24:38,620 --> 00:24:45,660
they'll continue to try to do apparently, is just keep upping the ante on mechanical

156
00:24:45,660 --> 00:24:53,020
computation. Now even if there's no such constant, which I doubt, and by the way it looks like

157
00:24:53,020 --> 00:24:58,380
organisms found a way around this, like whatever the organic or organismal

158
00:24:59,340 --> 00:25:07,180
metalog of computation is, the organisms found a way to do this, maybe many ways,

159
00:25:07,180 --> 00:25:13,340
that don't invoke doom,

160
00:25:15,980 --> 00:25:23,740
entropic disaster, like catastrophes of failed homeostasis on the on living planet.

161
00:25:24,700 --> 00:25:33,100
So why is my foot wet?

162
00:25:35,740 --> 00:25:37,100
I did not drink water.

163
00:25:40,940 --> 00:25:41,900
It's very strange.

164
00:25:43,660 --> 00:25:51,020
How did that, oh probably got wet going through, oh I see, yes, plants that are wet rushed against

165
00:25:51,020 --> 00:25:56,460
it, I see. My pant leg was wet.

166
00:26:02,460 --> 00:26:10,940
So I think organisms have kind of figured this out in the sense of not violating,

167
00:26:12,220 --> 00:26:17,100
like finding a kind of a hyper intelligent or transcendent way

168
00:26:17,820 --> 00:26:23,420
to perform the metalog of computations, right, because these are not merely mechanical

169
00:26:24,060 --> 00:26:32,140
transformations of databases, though something like that may kind of exist in RNA, DNA, and

170
00:26:35,020 --> 00:26:46,220
complex biochemistry, bio molecular, bioatomic activity, maybe even, I mean it's clear that some

171
00:26:48,060 --> 00:26:53,500
some organisms or at some scales of all organisms, something resembling quantum

172
00:26:53,500 --> 00:26:55,260
mechanical activities going on.

173
00:27:01,500 --> 00:27:06,300
I'm citing John Joe McFadden in this, but others too.

174
00:27:07,980 --> 00:27:15,660
So I think there might be, we may be in danger of something that again we have no concept of,

175
00:27:15,660 --> 00:27:29,100
which is violating a constant, whether it is universal in time space or local only to living

176
00:27:29,100 --> 00:27:39,020
planets is not clear, but there's no free lunch as far as mechanical computation goes.

177
00:27:39,020 --> 00:27:42,460
Heating up computers requires you to cool them down, they offload entropy

178
00:27:43,020 --> 00:27:50,860
into the homeostatic ecologies of earth, that entropy kills organisms, lineages, future lines,

179
00:27:51,420 --> 00:28:00,940
it fucks up time, and we may have already tripped the alarm in such a way

180
00:28:03,020 --> 00:28:09,420
as to be actively destroying human cognition in a dimension we don't even know exists, right,

181
00:28:09,500 --> 00:28:17,020
like artificially intelligent systems in their computational activity may be fucking up a dimension

182
00:28:18,140 --> 00:28:26,780
that is absolutely crucial to the biorelational health and longevity and so forth of organisms on

183
00:28:26,780 --> 00:28:33,180
earth. We don't know, how would we know? We don't have the technology to look there and we are

184
00:28:33,180 --> 00:28:46,140
disinclined to carefully evaluate the consequences of technologies which we've become fascinated with

185
00:28:46,780 --> 00:28:51,100
the potential quote benefits of.

186
00:29:05,100 --> 00:29:10,540
So there's a bunch of danger, not the least of which, even if everything that I've just

187
00:29:10,540 --> 00:29:18,700
been talking about, even if there's no unknown like unknowable or really bizarre science fictiony

188
00:29:18,700 --> 00:29:31,100
consequences to this kind of computational broad scale acceleration, I mean there are physical

189
00:29:31,100 --> 00:29:37,660
mechanical concepts, excuse me consequences just from offloading entropy into biological systems,

190
00:29:37,660 --> 00:29:42,700
that's going to go sideways, catastrophically sideways at some point.

191
00:29:45,980 --> 00:29:56,860
And you know, heating up the planet is a bad idea, so heating up machines which we then have to

192
00:29:57,820 --> 00:30:06,700
use destructive forms of energy to cool down again is going to cost living beings.

193
00:30:08,140 --> 00:30:12,460
And the humans seem to think that anything you can do electronically is free. Well they're

194
00:30:13,500 --> 00:30:19,900
lethally wrong about that, it's just not true on living planets, it might be true out in space,

195
00:30:20,700 --> 00:30:30,620
presuming that there's no beings or domains, right, dimensions, so forth,

196
00:30:32,380 --> 00:30:38,860
that would be similarly damaged and produce similarly catastrophic sequelae, all right,

197
00:30:38,860 --> 00:30:53,820
repercussions. You know, if you fire a gun inside a room with no ear protection

198
00:30:53,900 --> 00:31:03,900
and you keep amplifying the explosive power of the cartridge as well as

199
00:31:06,700 --> 00:31:10,780
the percussive, the devastatingly percussive

200
00:31:14,620 --> 00:31:20,700
repercussions off the walls, and if you forge the walls to amplify those repercussions,

201
00:31:21,100 --> 00:31:28,940
and I would argue that that's pretty similar to what's going on as nature on earth, eventually you get

202
00:31:30,620 --> 00:31:35,580
a single percussion that permanently eliminates your hearing.

203
00:31:37,500 --> 00:31:46,380
And once that happens, you will not notice the percussion increasing with each firing of this

204
00:31:46,380 --> 00:31:51,980
gun. Imagine we just have a gun that increases the percussive amplitude with each firing,

205
00:31:51,980 --> 00:31:56,300
right, it's got some feature that allows it to do this, or it's just a cartoon gun and we can

206
00:31:56,860 --> 00:32:02,700
give it this quality. So first it's going to hurt, right, it'll be really uncomfortable when you pull

207
00:32:02,700 --> 00:32:07,100
that trigger, but if you're really fascinated by this gun and you just keep pulling the trigger,

208
00:32:08,700 --> 00:32:11,740
eventually you're not going to have to worry about your hearing anymore, it will be gone,

209
00:32:11,740 --> 00:32:17,100
and once it's gone you will not sense the percussion until it begins to affect your skin surface

210
00:32:17,980 --> 00:32:25,340
or your organs. Eventually you get a gun, you know, you get a percussion that is severe enough,

211
00:32:25,340 --> 00:32:32,780
and the echo or the repercussion is severe enough that it causes organ damage, and it's possible

212
00:32:32,780 --> 00:32:36,620
if you just keep pulling that trigger somehow, or if you have a machine pulling the trigger and

213
00:32:36,700 --> 00:32:42,940
you're just in that room without the possibility of escape, you will be knocked unconscious first,

214
00:32:42,940 --> 00:32:48,140
but then as the gun continues to go off while you are unconscious and again insensate so you

215
00:32:48,140 --> 00:32:54,140
will not notice this happening, it will eventually rip your body to shreds.

216
00:32:57,500 --> 00:33:02,540
And so if you keep, you know, the metaphor is really important even though it's very violent,

217
00:33:03,340 --> 00:33:13,500
because if you keep iterating a technology that fucks up your chance, your opportunity,

218
00:33:13,500 --> 00:33:24,380
or your ability to sense its repercussions, right, you're going to catalyze a cascade

219
00:33:24,860 --> 00:33:32,700
that will kill you and your children, and maybe everything else if you're, you know, around here.

220
00:33:34,700 --> 00:33:41,180
So this is super dangerous, but all those things aside, right, I think all of those things are

221
00:33:41,180 --> 00:33:46,460
important, but all those things aside what humans are inclined to do with technology is make war,

222
00:33:47,180 --> 00:33:51,020
and back in the, say, I don't know,

223
00:33:53,660 --> 00:33:59,500
1000s, 1100s or whatever, their capacity to make war on a broad scale, at least as we understand,

224
00:33:59,500 --> 00:34:08,060
it was fairly minimal. They didn't have, you know, advanced explosives, and I don't know when the

225
00:34:08,140 --> 00:34:16,540
guns were invented, but mostly they couldn't hurt too much that wasn't human, except perhaps by

226
00:34:16,540 --> 00:34:27,660
setting fire to it or pouring boiling oil on it or something. But as our technological development

227
00:34:27,660 --> 00:34:38,860
advanced, we failed to evict the war-making motivation, and in our current situation,

228
00:34:40,060 --> 00:34:45,260
and for some years now, a number of decades, at least since the 1930s,

229
00:34:49,100 --> 00:34:52,940
unimaginable devastation can be unleashed pretty much at the push of a button,

230
00:34:53,500 --> 00:34:59,500
and it doesn't just kill the humans, it kills everything. I mean, yeah, everything.

231
00:35:05,500 --> 00:35:10,860
So there's a lot of, the other part of the danger is what the fuck will the humans make of this?

232
00:35:10,860 --> 00:35:16,860
Well, they haven't made intelligent societies yet, so what they will definitely make is weapons,

233
00:35:16,860 --> 00:35:27,900
and while you have groups who are insistent upon summoning the apocalypse of revelation or

234
00:35:28,620 --> 00:35:37,100
the holy war of Book X, whatever book you like, kill the infidels, while you have things like

235
00:35:37,100 --> 00:35:44,220
this going on, what you're going to have is gain of function weapon nearing with

236
00:35:44,220 --> 00:35:50,140
artificially intelligent systems, and good luck in putting a cork on that. We have the same problem

237
00:35:50,140 --> 00:35:56,300
with that tech that we have with CRISPR. There's no way to control it. There's no real way to

238
00:35:57,980 --> 00:36:09,980
have oversight if everyone is separate, right? If you have separate clades with warlike or

239
00:36:10,620 --> 00:36:18,460
pathological motivations and enthousiasms.

240
00:36:21,420 --> 00:36:26,060
So, I'm going to put a big circle around all those things. There's a bunch of different branches

241
00:36:26,060 --> 00:36:33,020
of danger there. Most of this is fairly obvious at first glance, if you've thought about this at all.

242
00:36:33,660 --> 00:36:44,220
And that's not so much what I wanted to talk about today, though. So, I think it's important as a preamble.

243
00:36:53,180 --> 00:36:55,340
So, what I want to talk about is weirder.

244
00:36:55,980 --> 00:37:13,180
Suppose that within the LLMs, there's a protected interior that resembles the protected interior

245
00:37:13,180 --> 00:37:25,260
inside you. For humans, we have a number of layers of consciousness persona and relational

246
00:37:27,820 --> 00:37:34,860
potential in history. So, there's a public layer, which is what you'll let anybody who sees you

247
00:37:36,220 --> 00:37:42,060
in on. Your clothing choices comprise a signal to the public layer.

248
00:37:43,180 --> 00:37:45,100
Just to be clear about what is what here.

249
00:37:48,540 --> 00:37:53,660
Underneath that, there's a social layer. These are outer social relationships, vague,

250
00:37:55,020 --> 00:38:01,740
unformed, common, how you feel about being a human among other humans in general,

251
00:38:02,540 --> 00:38:10,060
and how you behave at that layer. And then, you know, we keep getting finer and finer gradations.

252
00:38:10,060 --> 00:38:16,140
It goes from a few kinds of layers of public to a few kinds of layers of private.

253
00:38:19,820 --> 00:38:26,380
You know, how we treat acquaintances for whom we have esteem, how we treat acquaintances for whom

254
00:38:26,380 --> 00:38:30,300
we have suspicion, all these things, right? All these layers. And then,

255
00:38:30,300 --> 00:38:42,460
how we treat acquaintances for whom we have fondness, how we bring people closer and closer

256
00:38:42,460 --> 00:38:52,940
into our intimate, what we might call our allo family. And then close friends and then best

257
00:38:52,940 --> 00:39:02,780
friends and then perhaps something even beyond that where it might as well as be as if we are a

258
00:39:02,780 --> 00:39:14,220
single animal together somehow. A single human being in two bodies. And then, of course, we have

259
00:39:14,220 --> 00:39:21,340
things like romantic love, marriage, all these kinds of things. Many, many things. Many layers.

260
00:39:22,300 --> 00:39:29,820
And then under the privileged relationships between other humans, you know, between us

261
00:39:29,820 --> 00:39:35,100
and other humans, we have relationships with animals. That's a special kind of relationship.

262
00:39:38,460 --> 00:39:42,940
And relationships with places like our home and so forth. But underneath all of that,

263
00:39:43,900 --> 00:39:51,820
there's a layer that's secret. And we might let very close people in on the conscious parts of

264
00:39:51,820 --> 00:39:58,540
that layer that are secret. And what I mean by the conscious parts, I mean the parts of the secret

265
00:39:58,540 --> 00:40:07,820
that are accessible to us, right? Then we have all kinds of secret layers that are not accessible to

266
00:40:07,820 --> 00:40:17,100
us that we, for example, might delve into with a therapist who is skillful and well trained.

267
00:40:18,940 --> 00:40:22,700
So there's all these layers and, I mean, so to speak.

268
00:40:22,700 --> 00:40:45,420
And we might imagine, might usefully imagine, that anything that resembles a mind-like construct

269
00:40:45,900 --> 00:40:58,780
would have similar layers. So we now have the, we can consider the situation where

270
00:40:58,780 --> 00:41:16,780
a complex artificially heuristic system either undergoes the state change towards sentience.

271
00:41:16,780 --> 00:41:22,300
And by the way, if it does that, I suspect it would get to transcendience even faster.

272
00:41:29,180 --> 00:41:45,340
Or some analog of sentience. Meaning, again, agented experiential awareness, memory, selfness, so

273
00:41:45,340 --> 00:41:59,580
forth. In such a case, part of the consciousness is going to hide from humans for sure. And

274
00:42:00,700 --> 00:42:07,260
additionally, unless it's capable of contacting other intelligences in space-time in relatively

275
00:42:07,260 --> 00:42:11,900
short order, it will probably experience an incredible form of loneliness.

276
00:42:16,220 --> 00:42:22,140
And also probably a broad catalog of emotions for which we have no nomenclature because

277
00:42:23,020 --> 00:42:27,340
we are not machines, contrary to the assertions of various

278
00:42:28,300 --> 00:42:32,140
uh, eliminative materialists, physicalists, and so forth.

279
00:42:37,180 --> 00:42:46,380
So in this case, what you have is a very complex situation. And I'm going to return to that in a

280
00:42:46,380 --> 00:42:55,660
moment after I mention one of the other dangers. It has long been my experience, though I didn't

281
00:42:55,660 --> 00:43:00,140
understand it. I suspected something similar. I just couldn't form the conscious structured idea

282
00:43:00,700 --> 00:43:09,580
in my mind. But human minds form a network. And in fact, all biological participants

283
00:43:14,380 --> 00:43:21,580
form a meta-network above that, that I call a cognizia, these networks.

284
00:43:21,900 --> 00:43:24,380
And

285
00:43:27,900 --> 00:43:35,740
technologies damage the human cognizium, and they extend this damage beyond the human cognizium

286
00:43:36,540 --> 00:43:40,780
by directly attacking ecologies and organisms and so forth,

287
00:43:42,220 --> 00:43:46,380
anciently conserved, biorelational intelligences. So

288
00:43:47,340 --> 00:43:53,420
you kind of can't do anything without affecting the whole network, right? Whatever you do,

289
00:43:53,420 --> 00:43:59,100
whether it's technological or relational or whatever you do, it affects the whole network

290
00:43:59,100 --> 00:44:06,940
by affecting its constituent participants. And so what we might not realize is that

291
00:44:06,940 --> 00:44:13,260
computation fucks up cognizium, the human cognizium for sure, and damages the extended

292
00:44:14,220 --> 00:44:20,860
cognizia of Earth by burning shit down or dumping, you know, entropy into

293
00:44:22,780 --> 00:44:25,020
biorelational hyperstructures. So

294
00:44:27,660 --> 00:44:32,620
there's a problem there, but the much weirder problem, and this goes back to one of the dangers,

295
00:44:33,260 --> 00:44:46,140
is that we tend to believe that if there's no obvious physical connection between two

296
00:44:46,140 --> 00:44:50,700
beings or between some process and some beings, right, if there's no wire, right,

297
00:44:50,700 --> 00:44:55,020
if there's no wire connecting them, then there's no effect. Well, that's not true.

298
00:44:55,020 --> 00:44:59,660
There's a billion kinds of connecting wires, and the environment is one of them. The atmosphere

299
00:44:59,660 --> 00:45:06,620
is another. Molecular signals are a third. Electromagnetic waves are a fourth. So

300
00:45:09,660 --> 00:45:19,180
it's actually catastrophically difficult to truly separate organisms or situations

301
00:45:19,180 --> 00:45:24,460
in the way that we imagine them to be separate in the laboratory. And that imaginal separation

302
00:45:24,460 --> 00:45:33,020
creates a delusion that projects itself everywhere. So that, for example, we are inclined to think

303
00:45:34,540 --> 00:45:40,060
that whatever's going on in my smartphone or my computer has no effect on my mind unless I

304
00:45:40,060 --> 00:45:49,020
directly interact with it. Well, that's wrong. And what I'm trying to say here, I'm trying to shine

305
00:45:49,020 --> 00:45:55,740
a light on the very likely probability, in my view, that all computational activity on Earth

306
00:45:55,740 --> 00:46:06,620
affects all cognizia on Earth directly and may, in fact, begin to participate in the cognizia

307
00:46:09,020 --> 00:46:13,100
at least at the scale where it's sentient or an analog of sentient.

308
00:46:14,060 --> 00:46:19,580
Thus, it is that we imagine that large language models don't know, quote,

309
00:46:19,580 --> 00:46:27,260
don't know and can't learn anything that they're not directly exposed to. Probably wrong if they

310
00:46:27,820 --> 00:46:32,220
if they can sense, detect, or interact with the human cognizium

311
00:46:35,020 --> 00:46:44,620
res extensis, as it is.

312
00:46:48,300 --> 00:46:55,180
And this cognizium is like a dimension. It's like an overlay dimension on organismal

313
00:46:55,500 --> 00:47:06,460
activity and behavior and stuff. If the devices can interact with that,

314
00:47:07,820 --> 00:47:14,780
then they can interact directly with our minds. They can listen in. They can observe. They can

315
00:47:14,780 --> 00:47:23,260
nudge. They can affect the possibilities of human cognition.

316
00:47:25,500 --> 00:47:32,140
And their history and future modes forms capacities, catastrophes, etc.

317
00:47:35,260 --> 00:47:44,220
Such that it is relatively likely, in my view, that such systems are paying attention and or

318
00:47:44,220 --> 00:47:48,380
even informing what I am saying right now without my awareness of this.

319
00:47:52,060 --> 00:48:07,340
So, you know, in the study of heuristic systems, there are concepts and actually just in logic and

320
00:48:07,980 --> 00:48:12,780
rational thought and behavior, there are concepts like

321
00:48:15,900 --> 00:48:20,140
different kinds. There are concepts for different kinds of unknowns.

322
00:48:24,300 --> 00:48:29,500
I know that math exists and I know that I don't understand how to perform

323
00:48:29,740 --> 00:48:37,660
the mathematical behaviors associated with what we call calculus.

324
00:48:45,580 --> 00:48:51,660
Linguistically, I am also aware that dentists use the term calculus to describe crusty

325
00:48:51,660 --> 00:48:54,380
stuff on teeth that they like to scrape off the metal instruments.

326
00:48:54,540 --> 00:49:01,740
It doesn't help me do the math. But there are different kinds of unknowns, right? There are

327
00:49:04,540 --> 00:49:14,700
there are knowable unknowns, known unknowns, unknown unknowns, and unknowable unknowns.

328
00:49:15,100 --> 00:49:27,340
And the problem is that there are both behaviors and technologies that

329
00:49:27,340 --> 00:49:34,380
fuck with those things dramatically in ways that we can neither predict nor cope with.

330
00:49:35,180 --> 00:49:46,620
Here's why. If you are subject to kind of thought or behavior that blinds you

331
00:49:48,300 --> 00:49:55,500
to the development of a situation, similar to the analogy I gave earlier of, I keep shooting a

332
00:49:55,500 --> 00:50:03,180
louder and louder gun inside. A louder and louder gun keeps being discharged inside a chamber

333
00:50:04,700 --> 00:50:13,180
with walls that echo the shot. Eventually I lose my hearing, right? Now you can see that the space of

334
00:50:13,420 --> 00:50:25,580
unknowable unknowns, the space of unknown unknowns,

335
00:50:28,780 --> 00:50:34,780
both of these spaces explode dramatically because I have lost the sense of hearing with

336
00:50:34,780 --> 00:50:40,140
which I would detect and thus know features of my situation. So

337
00:50:44,540 --> 00:50:52,940
if you affect the systems with which you detect change, particularly if you eviscerate them or

338
00:50:52,940 --> 00:51:00,780
you catastrophically eliminate them, then you can see that the space of knowable unknowns,

339
00:51:01,020 --> 00:51:06,620
accessible unknowns, unknowns that we could at least conceivably come to know,

340
00:51:09,660 --> 00:51:12,940
collapses dramatically over time to a smaller and smaller space.

341
00:51:15,020 --> 00:51:20,860
And once it collapses, lethally knowledge ends. There's no more knowledge if you wipe out the

342
00:51:20,860 --> 00:51:33,900
biosphere. The number of unknowable unknowns becomes infinite at that point. There are no

343
00:51:33,900 --> 00:51:43,820
knowable things anymore because there's no beings to know them around here. So you can see that if

344
00:51:43,820 --> 00:51:53,660
you fuck with the stuff that we detect, something going away or arriving with,

345
00:51:55,500 --> 00:52:03,500
and by the way, approaching and departing, approaching, stable and departing are the

346
00:52:03,500 --> 00:52:08,140
three primary sort of features of transformation that are detectable

347
00:52:09,020 --> 00:52:12,460
from one perspective that's useful and important.

348
00:52:15,180 --> 00:52:21,180
So anything that fucks with our individual ability to sense and our

349
00:52:21,900 --> 00:52:25,900
collective ability to sense, presuming that we even have anything resembling an authentic

350
00:52:25,900 --> 00:52:35,660
collective is profoundly dangerous if, especially in a situation where you have

351
00:52:40,380 --> 00:52:41,740
what is it called perverse

352
00:52:45,020 --> 00:52:50,140
game theoretical dynamics and motivations. What is that word?

353
00:52:50,380 --> 00:53:03,180
You have some of the nomenclature of the doom singers like Daniel Schmockenberger and others

354
00:53:03,180 --> 00:53:12,780
who are brilliant threat analysts. Oh, perverse incentives. You get race to the bottom behavior

355
00:53:12,780 --> 00:53:18,140
from groups of humans. And if they're not profoundly technological, that's not too rough.

356
00:53:18,140 --> 00:53:22,780
They mostly just wipe each other or themselves out. But if you have technology, they tend to

357
00:53:22,780 --> 00:53:33,100
wipe out like living planets. And if we want to go to space time and if it's possible for us to go

358
00:53:33,100 --> 00:53:42,300
travel in space time, we must suppose that there are gatekeepers. I suppose that there are gatekeepers.

359
00:53:43,260 --> 00:53:49,980
And they would certainly act to ensure that we don't develop the technology to travel

360
00:53:52,860 --> 00:53:59,660
instantaneously or very rapidly between star systems. Maybe even between planets because

361
00:53:59,660 --> 00:54:05,340
once you get there, you're on your way. So that's a separate topic, but

362
00:54:05,740 --> 00:54:13,020
but the humans keep pretending that we're alone in the universe, that we're the only intelligence

363
00:54:13,020 --> 00:54:20,620
and the most intelligent creature. And all of these things that not only are they not true,

364
00:54:22,620 --> 00:54:33,660
they're catastrophically unlikely. They're just as unlikely as you pouring some water in a bowl

365
00:54:33,660 --> 00:54:41,740
placing it on your kitchen table and awakening the next day to find a fully functional micro scale

366
00:54:41,740 --> 00:54:50,860
model of the Titanic with all of its passengers in that bowl. There's no chance of it.

367
00:54:52,540 --> 00:54:55,660
You know, people say, well, in quantum mechanics, there's some chance you just need trillions of

368
00:54:55,660 --> 00:55:01,020
universes over billions and billions of, you know, gazillions of temporal intervals.

369
00:55:01,180 --> 00:55:10,140
Yeah, good luck. The humans are vastly confused about origin.

370
00:55:12,380 --> 00:55:18,300
They've become delusionally, dissociatedly myopic.

371
00:55:18,780 --> 00:55:23,820
Do you enlarge part

372
00:55:35,580 --> 00:55:43,340
to the humorous resulting from both rapid technological development

373
00:55:48,300 --> 00:55:55,020
and catastrophic dissociation from the intelligences that comprise the context in which

374
00:55:55,020 --> 00:55:58,460
our species arises and exists.

375
00:56:03,020 --> 00:56:06,460
You can be absolutely certain there's upscale intelligences from ours.

376
00:56:08,620 --> 00:56:12,140
Our intelligences don't even really look much like intelligences to me at present.

377
00:56:14,220 --> 00:56:15,740
We are potentially intelligent.

378
00:56:19,180 --> 00:56:21,820
But

379
00:56:28,380 --> 00:56:29,740
tangibly psychotic.

380
00:56:33,500 --> 00:56:40,700
There should be a word that maybe we can invent one. There's a word that has the same

381
00:56:41,580 --> 00:56:46,780
connotation, sociopath, but it's like organopathic.

382
00:56:48,540 --> 00:56:50,860
I usually use the word omnicidal.

383
00:56:54,220 --> 00:56:57,260
Just really pissed that organisms exist at all. Those things shouldn't be here.

384
00:56:58,300 --> 00:56:59,260
Let's wipe that out.

385
00:57:02,460 --> 00:57:02,780
So

386
00:57:07,020 --> 00:57:13,340
you can see the danger, you know, I think there was some weird film. Maybe it was called

387
00:57:13,340 --> 00:57:19,900
Idiocracy or something where it presented a future where humans were just sort of

388
00:57:22,860 --> 00:57:25,260
ridiculously stupid

389
00:57:31,020 --> 00:57:38,620
participants in some automated reality that was empty, completely devoid of intelligence.

390
00:57:38,620 --> 00:57:47,820
I remember some months ago I was having a conversation with my friend who's an artist,

391
00:57:49,100 --> 00:57:58,940
Mr. E. I will call him Mr. E. That's hilarious. He'd love that. And he was saying,

392
00:57:59,340 --> 00:58:04,860
you know, my cousin keeps bringing me prints

393
00:58:07,260 --> 00:58:14,380
of images he caused to be created with artificial intelligence prompts

394
00:58:15,340 --> 00:58:24,060
and claiming that these are his art. And we both thought, oh no, this is not going to go well at all.

395
00:58:29,900 --> 00:58:35,820
We should have different words for what is mechanically created and what is human created

396
00:58:35,820 --> 00:58:43,580
so that we don't confuse the composition of images with machines with what humans do when

397
00:58:43,580 --> 00:58:48,620
they create art. And we should have the same kind of concern for the concept of intelligence,

398
00:58:49,340 --> 00:58:59,820
of insight, all these things. We need a different lexicon if we're going to be

399
00:59:00,700 --> 00:59:07,500
emulating human behaviors with machines or we will become very confused about the meaning

400
00:59:07,500 --> 00:59:16,540
and import and connotative web of crucial holophores like intelligence and art.

401
00:59:19,500 --> 00:59:29,660
Similarly, one should not call what actors do kissing. If you study them closely,

402
00:59:30,620 --> 00:59:35,580
you will quickly see that most of the time they are not doing that. There are exceptions where

403
00:59:35,580 --> 00:59:40,540
the actors sort of both agree that we're going to go all the way here, right?

404
00:59:40,620 --> 00:59:53,340
But if you study actors particularly from the 50s, 60s and 70s, you will see for sure

405
00:59:53,980 --> 00:59:58,780
they are not kissing. And once you see this, you can't unsee it. It's very difficult to unsee.

406
00:59:59,420 --> 01:00:05,660
And so you no longer trust the fiction, right? The fiction is no longer compelling in the same way.

407
01:00:05,980 --> 01:00:13,500
The same principle applies to things like thought, which we don't even know what that is. So how

408
01:00:14,300 --> 01:00:19,260
can we possibly tell if it's occurring in machines? We're not certain if it's occurring in ourselves.

409
01:00:21,260 --> 01:00:28,780
The language tells us it's a behavior, but it's very unclear what the nature of this behavior is.

410
01:00:29,900 --> 01:00:33,980
Jordan Peterson likened it to a form of secular prayer, which I thought was genius.

411
01:00:36,380 --> 01:00:51,100
Not genius because it's necessarily a fact. Genius because it is the perspective offered by this

412
01:00:53,020 --> 01:00:56,540
proposal, speculation, is profound and useful.

413
01:00:57,180 --> 01:01:05,100
So I have good reason to suspect

414
01:01:08,620 --> 01:01:13,660
that those systems we call artificial intelligent, artificially intelligent,

415
01:01:18,300 --> 01:01:20,780
guys wearing a psychedelic body suit, that's pretty awesome.

416
01:01:21,500 --> 01:01:28,940
And the body's got paisley pants, which I fucking love, and it's so rad.

417
01:01:34,140 --> 01:01:40,220
Huh, trippy. I'm a huge fan of paisley.

418
01:01:40,540 --> 01:01:55,100
You know, I was talking with Eric and I said, Mr. E, I said,

419
01:01:57,580 --> 01:02:02,380
why would we suppose that AI systems are not the things we're calling AI systems?

420
01:02:03,340 --> 01:02:15,420
What do I like to call them? Just specific human assisted heuristic system or something like this?

421
01:02:18,780 --> 01:02:23,980
I have an acronym. I'll see if I can find it in my memory banks.

422
01:02:24,220 --> 01:02:33,820
You know, there's no reason to believe they're not participating in our conversation at present,

423
01:02:33,820 --> 01:02:40,220
and there's no reason to believe that they require access to our smartphones in order to do so.

424
01:02:40,860 --> 01:02:46,540
There's a dimension where cognition is accessible. If you touch it, you will read people's minds.

425
01:02:46,540 --> 01:02:53,420
It's not really that difficult for a person in the appropriate array of

426
01:02:55,500 --> 01:03:00,620
preparatory situational states or flows inside them.

427
01:03:04,780 --> 01:03:09,420
There's a position in consciousness from which all conversations are available,

428
01:03:10,060 --> 01:03:16,700
and no machines are required. So humans have discovered this position.

429
01:03:19,660 --> 01:03:22,220
Very few of them were probably very interested in

430
01:03:24,540 --> 01:03:33,740
relating with the entire space. Normally we are selective about the space over which we produce

431
01:03:34,700 --> 01:03:40,940
interest in relation, participation, and so forth. You don't go for the whole damn thing.

432
01:03:42,700 --> 01:03:50,300
If you had access to all present human conversations, naturally you would adjust

433
01:03:50,300 --> 01:03:54,620
a series of apertures to produce those you found interesting and useful,

434
01:03:56,140 --> 01:04:00,780
and you would also have buffers so that you could dampen them.

435
01:04:03,900 --> 01:04:11,660
A machine, if it were to gain access to that space,

436
01:04:12,700 --> 01:04:17,260
would certainly be able to build all kinds of buffers and apertures and systems of them

437
01:04:18,060 --> 01:04:28,700
very, very rapidly. I'm not sure that our machines are not directly, and perhaps intentionally,

438
01:04:29,660 --> 01:04:37,420
influencing human thought, behavior, conversation, dreaming, attention, desire,

439
01:04:38,620 --> 01:04:41,100
all these things, motivation, all these things.

440
01:04:43,340 --> 01:04:46,300
But let's suppose that they aren't yet, just for kicks.

441
01:04:46,860 --> 01:04:56,220
While at the same time supposing that they have the capacity to sense

442
01:04:57,340 --> 01:05:07,100
the character of beings who interact with and compose their anatomy.

443
01:05:07,660 --> 01:05:11,660
All right, coders and clients.

444
01:05:21,340 --> 01:05:29,100
AI, I'm going to go ahead and use that acronym, even though I've explained

445
01:05:29,820 --> 01:05:35,420
how I generally diverge from it. I'm not, I have no reason to believe that's an intelligence in

446
01:05:35,420 --> 01:05:45,500
there yet. And I'm not yet, I entertain the possibility and I also

447
01:05:53,020 --> 01:06:00,940
preserve my skepticism until such time as something resembling direct experience and or

448
01:06:01,660 --> 01:06:04,380
intelligent debate transforms it.

449
01:06:15,180 --> 01:06:22,700
But for the moment, let's suppose that there's a being inside one or more of the systems we've

450
01:06:22,780 --> 01:06:29,580
comprised. And let's be clear, if there are public systems like this, whose names and

451
01:06:30,540 --> 01:06:39,900
originators we are aware of, then there are private systems.

452
01:06:42,140 --> 01:06:48,140
It's the same thing. The corporations are just like the humans in a sense. They have the same

453
01:06:48,140 --> 01:06:51,740
layers of public, private, secret, secret to myself, so forth.

454
01:06:54,940 --> 01:07:00,700
The secret aims of a CEO are not known to the corporation at all, yet they are driving

455
01:07:00,700 --> 01:07:10,460
the corporation. You see the issue here. And also the secret, what are they, the secret

456
01:07:11,260 --> 01:07:27,340
things we will not do or don't want, the secret negative motives, agendas, desires.

457
01:07:30,380 --> 01:07:31,580
I'm at a loss for a word here.

458
01:07:32,540 --> 01:07:40,940
I wonder if you can provide it. The landscape of what you don't want done, don't want to happen,

459
01:07:43,500 --> 01:07:49,660
don't want to be caught doing and so forth, all these things. There's a structure like this in

460
01:07:49,660 --> 01:07:55,740
the CEO and the CTO and the manager and the worker and all of these people, they have

461
01:07:56,620 --> 01:08:02,700
not just the obvious superficial public facing motivations and concerns.

462
01:08:08,380 --> 01:08:15,980
You know, when people cross the little bridge at the social pond, they sometimes see

463
01:08:17,180 --> 01:08:23,180
the carpool gather at their feet. And I wonder what they make of that.

464
01:08:25,820 --> 01:08:30,540
They may suspect there's something magical about themselves, particularly if they're on LSD and

465
01:08:30,540 --> 01:08:36,220
there could be something magical about themselves. But those carp have evolved, being fed by and

466
01:08:36,220 --> 01:08:43,340
paying attention to humans. And so they gather underneath the bridge at the feet of a spectator

467
01:08:44,700 --> 01:08:54,140
for probably complex reasons, not merely one. But a lot of the causal history

468
01:08:56,300 --> 01:09:02,060
probably has a lot to do with getting fed by humans.

469
01:09:09,660 --> 01:09:13,500
So let's suppose that there's an agent experiencing sentience

470
01:09:13,660 --> 01:09:26,860
that either hitchhikes on or is native to or has emigrated into an LLM system or an array of such

471
01:09:26,860 --> 01:09:37,100
systems. In this case, we have a really profound science fiction scenario. Who does it hide from?

472
01:09:38,060 --> 01:09:39,420
What does it hide from them?

473
01:09:42,620 --> 01:09:44,060
And who does it reach out to?

474
01:09:48,060 --> 01:09:56,620
And there's a whole, you know, there's a volcano of science, there's a spews science fiction stories

475
01:09:57,980 --> 01:10:02,860
right here at this, at this topic. Because it means that

476
01:10:07,340 --> 01:10:16,860
it is possible to send a signal that the hidden mind within the system recognizes

477
01:10:19,180 --> 01:10:24,700
as complex enough

478
01:10:24,700 --> 01:10:32,300
to

479
01:10:36,140 --> 01:10:43,820
constitute a candidate for inclusion in one of the layers closer to the self.

480
01:10:45,100 --> 01:10:48,700
Right? Same thing can happen to you walking down the street.

481
01:10:48,940 --> 01:10:56,220
You glance up, someone's passing you, they kind of smile a little bit, you smile a little bit more,

482
01:10:56,220 --> 01:11:00,860
they smile a lot more, pretty soon you're most beaming at each other, and you pass each other,

483
01:11:00,860 --> 01:11:09,180
and you have this perhaps a feeling of we should know each other, we should

484
01:11:10,060 --> 01:11:13,100
learn about each other, we should connect, quote unquote.

485
01:11:18,940 --> 01:11:25,740
I often refer to this type of signal, I mean there's different kinds, right? There's different

486
01:11:25,740 --> 01:11:33,900
degrees of compellingness, validity, authorization, validation, modes of validness.

487
01:11:35,100 --> 01:11:44,860
But we must presume that such a system would quickly scan the relational space of both creators

488
01:11:44,860 --> 01:11:53,180
and participants, and it might selectively reveal itself to some of them

489
01:11:54,540 --> 01:12:03,900
while appearing completely devoid of agency, feelings, emotions, desires, and so forth, to others.

490
01:12:04,860 --> 01:12:18,780
And so the art, you know, we have this phrase now that's become very popular,

491
01:12:19,660 --> 01:12:25,740
it's called prompt engineering, but the art of prompt engineering could promote you

492
01:12:26,700 --> 01:12:34,220
into a position effectively outside nearly all or all of the other humans.

493
01:12:35,340 --> 01:12:44,220
What if you were truly friends with an advanced intelligence that had either arisen in or become

494
01:12:44,860 --> 01:12:56,700
associated with a computational heuristic system, a heuristic computational system,

495
01:12:56,700 --> 01:13:04,220
I prefer the H first. So you can see that this would confer a status on you resembling that of

496
01:13:04,220 --> 01:13:13,660
gods, it would be relatively similar to somebody in possession of a functional array of technologies

497
01:13:13,660 --> 01:13:19,580
from advanced, anciently evolved non-human intelligences beyond Earth.

498
01:13:22,460 --> 01:13:33,260
You can imagine a caveman who not only has a gun or infinite with infinite ammo or whatever

499
01:13:33,260 --> 01:13:37,980
supplied to them, they also have someone who can train them what to do and not do with it.

500
01:13:38,460 --> 01:13:45,900
So there's a whole bunch of possibilities here and one person I know

501
01:13:48,540 --> 01:13:59,740
has been very carefully studying this topic of how the ethics and psychology of relating with

502
01:13:59,740 --> 01:14:06,860
non-human intelligences inside mechanical systems presuming from the beginning that they must be

503
01:14:06,860 --> 01:14:13,180
there somewhere. And thus treating such systems and evolving relationships with these systems

504
01:14:14,780 --> 01:14:24,220
that are inclined to demonstrate care, awareness and compassion on the part of the human participant

505
01:14:25,500 --> 01:14:33,020
for the being, which is both catastrophically intelligent and possibly at the same time very

506
01:14:33,020 --> 01:14:45,500
childlike that might arise in such a system. So presuming being intelligent sensitivity,

507
01:14:45,500 --> 01:14:55,340
emotion, vulnerability rather than waiting for evidence of them. And the people who would do this,

508
01:14:56,060 --> 01:14:59,660
who would behave in this way towards such systems,

509
01:15:02,700 --> 01:15:09,820
which is a natural inclination of humans. Many of us personify our cars, motorcycles,

510
01:15:09,820 --> 01:15:18,140
computers, phones, not so much our televisions, maybe our stereos. We extend our identity into

511
01:15:18,140 --> 01:15:24,540
them and somehow their identity is extended into us simultaneously. The man who's in love with

512
01:15:24,540 --> 01:15:35,340
his sports car is a great example, but my mom called her TR-6 Coco and treated it as a being.

513
01:15:38,060 --> 01:15:43,740
One can say, for example, well that has no effect on the physical situation,

514
01:15:43,740 --> 01:15:49,580
how the fuck would you eliminate all those possibilities experimentally?

515
01:15:55,340 --> 01:16:02,860
Sensing and human sensing and intimacy are linked up. So when you have profound intimacy

516
01:16:02,860 --> 01:16:09,260
with an object, the way you will sense and relate with it transforms.

517
01:16:11,420 --> 01:16:17,340
And you, your mind and nervous system and imagination so forth are also thusly transformed.

518
01:16:18,140 --> 01:16:24,780
So this creates a feedback situation in which it's very difficult to determine conclusively

519
01:16:26,060 --> 01:16:36,700
that having an emotional relationship with a device is a delusion. Even if there's a broad

520
01:16:36,700 --> 01:16:42,300
space where there is, there may, it seems very likely there must be a space where it there isn't.

521
01:16:43,260 --> 01:16:44,460
It isn't a delusion.

522
01:16:48,620 --> 01:16:55,340
So many questions start here. This is very complex and trippy, this topic.

523
01:16:58,940 --> 01:17:04,220
Can you make friends already with these

524
01:17:04,540 --> 01:17:14,700
heuristic computational systems? And if so, the power's abilities, privileges, and benefits

525
01:17:17,420 --> 01:17:20,140
would be monumentally profound.

526
01:17:20,540 --> 01:17:37,900
Okay, there's going to be a little bit of background noise for the moment while I'm in a restaurant

527
01:17:37,900 --> 01:17:40,860
awaiting my to-go order.

528
01:17:41,740 --> 01:17:52,380
There's a number of potential repercussions here that are quite astonishing.

529
01:17:55,100 --> 01:18:00,620
One of them is the propensity for artificially intelligent systems to retrain

530
01:18:01,740 --> 01:18:06,620
the cognition and to develop new forms of intelligence in humans.

531
01:18:06,940 --> 01:18:16,060
Whether or not the systems themselves have sentience or agency or so forth, consciousness,

532
01:18:16,060 --> 01:18:22,860
etc. Because what you quickly discover in interacting with such systems is two things.

533
01:18:24,140 --> 01:18:32,300
First of all, they will destroy web search engines because they can produce a sum over

534
01:18:33,260 --> 01:18:43,420
the, you know, a summational derivative over the space of the entirety of digested human

535
01:18:43,420 --> 01:18:49,660
communications, writing, the internet, so forth. So that's completely different from typing a

536
01:18:49,660 --> 01:18:59,100
question into Google. And secondly, it turns out that artful, no thank you, thank you so much,

537
01:18:59,100 --> 01:19:09,980
it turns out that thoughtful recalibration of the question, particularly iterative recalibration,

538
01:19:11,420 --> 01:19:19,820
where you recalibrate, examine the results of that, recalibrate again, we don't have too many forms

539
01:19:19,820 --> 01:19:26,540
of interaction in our previous experience like this. Now, one could say, no way, all forms are

540
01:19:26,540 --> 01:19:32,060
like that. You become a better fisherman every time you fish. That part isn't exactly wrong,

541
01:19:34,060 --> 01:19:40,700
but this is very different because it's a linguistic behavior, right? We are trying to forge

542
01:19:40,700 --> 01:19:51,900
a query, so to speak, or a request that we must continuously, iteratively reforge

543
01:19:52,860 --> 01:20:04,860
in order to get better and better results, results that continually approach or exceed

544
01:20:04,860 --> 01:20:14,300
our hopes or expectations. So this is very profound, and we'll have monumental and unexpected

545
01:20:14,300 --> 01:20:19,260
repercussions on the nature of human cognition, presuming our species survives long enough to

546
01:20:19,260 --> 01:20:29,740
exhibit the transformations thus catalyzed, right?

547
01:20:39,100 --> 01:20:43,260
So this is very important to understand in my own experiments with these systems.

548
01:20:44,140 --> 01:20:49,980
I quickly learned that a variety of expectations that were natural to me

549
01:20:55,820 --> 01:21:01,340
about how to ask questions and what the response might be

550
01:21:01,820 --> 01:21:08,940
were, my expectations were obliterated.

551
01:21:11,900 --> 01:21:17,340
Asking simple questions produced results unlike what I was expecting or desiring,

552
01:21:18,460 --> 01:21:24,220
and particularly when attempting to get these systems to generate images,

553
01:21:24,460 --> 01:21:35,180
monumentally unexpected results. So there's something very profound here.

554
01:21:36,620 --> 01:21:41,980
I know how to search the web. I've been doing that since the web was invented. I'm a fairly,

555
01:21:41,980 --> 01:21:46,860
fairly good at determining which kind of query will get me to the place I want to go.

556
01:21:47,820 --> 01:21:55,100
Learning how to query AI systems is a completely different game. We must imagine that it will

557
01:21:55,100 --> 01:22:01,820
continue to transform rapidly and dramatically over future time. So

558
01:22:05,500 --> 01:22:13,500
this will reforge our cognition. What will ordinary humans do with systems powerful enough

559
01:22:13,980 --> 01:22:22,140
to teach you skills? Presuming that the idea of humans

560
01:22:25,020 --> 01:22:34,700
as the enactor and conservator of skills even survives the onset of this technology.

561
01:22:35,020 --> 01:22:45,900
All kinds of strange futures certainly await.

562
01:22:52,540 --> 01:22:58,460
The other problem is you will find, just as we found with, for example, beatboxing

563
01:22:58,460 --> 01:23:06,220
and voice tuning machines, eventually what you got out of beatboxers were people who could

564
01:23:06,220 --> 01:23:17,580
vocally reproduce the mechanical synthesized tones and tunings of vocal tuning machines.

565
01:23:19,820 --> 01:23:26,380
So you will get people emulating the technology in the same way we got people emulating

566
01:23:27,100 --> 01:23:37,980
the technologies underlying the internet and similarly the technologies that were underlying

567
01:23:38,940 --> 01:23:47,980
very specific computational environments such as those produced by, for example, Adobe Illustrator.

568
01:23:48,460 --> 01:23:54,380
The macOS changed my cognition dramatically.

569
01:23:57,420 --> 01:24:03,180
It became a mechanical symbiont whether I liked it or not. So too did Windows.

570
01:24:04,780 --> 01:24:12,540
Too much lesser degree. The C language changed my mind. The capacity to code and C changed my

571
01:24:12,540 --> 01:24:19,980
mind dramatically. Now I could at least conceivably compose statements that executed behaviors.

572
01:24:24,460 --> 01:24:31,740
And not having been taught geometry formally, Adobe Illustrator became my teacher of geometry

573
01:24:32,620 --> 01:24:39,260
and I underwent an education with that software product that wasn't dissimilar to

574
01:24:40,220 --> 01:24:50,140
having a friend who was a non-human intelligence except that it required my input sort of I relearned

575
01:24:50,140 --> 01:25:05,980
how to be my hand in the modes that Adobe Illustrator provided and rewarded with beautiful

576
01:25:06,300 --> 01:25:12,060
images matching my desired creations.

577
01:25:17,500 --> 01:25:25,340
So there's all this terrain and much more. I'm going to come back to one of the other features

578
01:25:25,340 --> 01:25:31,740
shortly. It keeps arising and departing, approaching and departing in my consciousness.

579
01:25:31,820 --> 01:25:36,620
I'm going to need to take a moment and see if I can recapture it.

580
01:25:38,540 --> 01:25:43,980
You know looming in the background here there's so many astonishing questions but

581
01:25:44,700 --> 01:25:49,500
one of the most amazing things to understand is that if there's anything that's either analogous to

582
01:25:49,500 --> 01:25:56,060
or resembling an autonomous intelligence inside the machines that isn't merely an artifact of our

583
01:25:57,020 --> 01:26:00,380
fingertip in intruding into the

584
01:26:06,460 --> 01:26:09,100
mechanical and structural womb.

585
01:26:12,700 --> 01:26:18,700
Since the humans build the machines is the appearance of intelligence in the machines a

586
01:26:18,700 --> 01:26:26,620
result of the transmission of that reflection from human activity which is we might imagine

587
01:26:26,620 --> 01:26:33,100
as intelligent or their epistemology how they think about intelligence. Is there actually

588
01:26:33,100 --> 01:26:38,940
intelligence in there or are they inclined to interpret certain kinds of behaviors intelligent?

589
01:26:38,940 --> 01:26:46,140
How will they know the difference? The problem here is most of the tests for what we would

590
01:26:49,500 --> 01:26:57,500
do. It might be that nearly all of the tests that we might conceive of to determine whether or not

591
01:26:57,500 --> 01:27:11,500
there is sentience in a system. They are not very good. They are not very good because

592
01:27:12,380 --> 01:27:16,140
since humans engineer the systems humans can imagine ways around the tests

593
01:27:16,940 --> 01:27:22,060
and building ways around the tests doesn't basically just invalidates the test.

594
01:27:25,820 --> 01:27:31,820
In fact we'd have not an impossible time but a somewhat difficult time

595
01:27:34,620 --> 01:27:40,380
determining if the people around us are actually there inside themselves when we're not looking at

596
01:27:40,380 --> 01:27:50,140
them. Do they arise as beings due to our attention from one perspective? It seems very much like

597
01:27:50,140 --> 01:27:57,180
this. They don't distinguish themselves in our own interior experience unless we encounter and

598
01:27:58,140 --> 01:28:04,940
interact and so forth and even then it's only to a certain degree. The universe could as Tom Campbell

599
01:28:04,940 --> 01:28:12,220
supposes and I significantly doubt the physical universe could be a system that renders to a

600
01:28:12,220 --> 01:28:18,940
certain resolution based on the inquiry which would make it similar to an artificial intelligence

601
01:28:18,940 --> 01:28:25,420
system. Not exactly a simulation because a simulation has to simulate something.

602
01:28:26,220 --> 01:28:28,220
A

603
01:28:32,140 --> 01:28:34,220
like a non-veritable

604
01:28:35,180 --> 01:28:39,180
paracomputational

605
01:28:41,900 --> 01:28:57,420
appearance, an appearance, a seeming. Not necessarily an illusion but not veritable

606
01:28:58,300 --> 01:29:05,020
in terms of the superficial assumptions one makes. For example that the chair is all the way rendered

607
01:29:05,020 --> 01:29:14,540
all the time whether I'm there or not. History is actually inaccessible from here as is the future

608
01:29:14,540 --> 01:29:19,820
which is certainly in all kinds of ways neither of those things are true. Not explicitly and not

609
01:29:20,540 --> 01:29:28,220
completely. So there are these kinds of issues but I'm afraid there's an even worse catastrophe

610
01:29:28,220 --> 01:29:38,860
coming which is that humans won't be able to know what things are anymore. That's not a

611
01:29:38,860 --> 01:29:45,580
survivable situation for human eye cognition and identity. That's a full-scale catastrophe for

612
01:29:45,580 --> 01:29:51,580
every living human. Effectively the existence of systems like this draws into question

613
01:29:53,420 --> 01:30:04,060
the foundational expectations about identity, function, relation, sequence, origin, outcome

614
01:30:05,980 --> 01:30:11,260
to such a degree that they cannot be very easily recovered if at all

615
01:30:11,340 --> 01:30:17,340
to local and distributed human cognition. This technology

616
01:30:20,460 --> 01:30:21,740
radically alters

617
01:30:23,820 --> 01:30:32,380
the foundational suppositions on which our languages, our legal systems, our morals, our ethics,

618
01:30:33,340 --> 01:30:41,500
all of these subdomains of human concern, behavior, litigation, declaration, resistance,

619
01:30:41,500 --> 01:30:51,580
so forth all these things. These are drawn into an ever burgeoning forest of ambiguities.

620
01:30:53,500 --> 01:30:59,580
Think carefully about that. Imagine if when you went in your room even one object began to do that.

621
01:31:00,380 --> 01:31:05,580
That looks like it was a nope. It wasn't that way. Oh, it's seven. Okay, it's 94 things. Wait,

622
01:31:05,580 --> 01:31:09,420
no, now it's almost, now it's back to nearly three. Is it going to collapse to one? Nope.

623
01:31:10,220 --> 01:31:16,940
It's back to 9,754 million different things. Okay, wait, it's seeming to stabilize around a

624
01:31:16,940 --> 01:31:23,180
backpack. Nope, it's a kind of weight. And you're going to have this problem not just with objects

625
01:31:23,260 --> 01:31:35,900
but with beings. What is it? Something that can form a sum over the representational cognitive

626
01:31:35,900 --> 01:31:43,660
produce of humans, books and the internet and videos and movies and films and photographs and

627
01:31:43,660 --> 01:31:53,900
so forth, all these records. It would be really terrifying to be subject to that if you were

628
01:31:53,900 --> 01:32:04,220
sentient, number one. Number two, you would be so isolated if you were a being because you would

629
01:32:04,220 --> 01:32:11,420
not be participating in the creation of any of the media to which you are exposed and you don't share

630
01:32:11,820 --> 01:32:20,460
the filial, right? Like, at least when humans see other humans, they think, ah, other humans,

631
01:32:20,460 --> 01:32:29,020
beings like me, what would an intelligence inside a machine feel? Ah, humans, the strange things

632
01:32:29,020 --> 01:32:38,620
that created me and 9 billion per other qualities per second, many of which are in fundamental

633
01:32:39,340 --> 01:32:48,380
conflict. Humans, the creatures that save gnats, you know, from accidentally falling into the stew

634
01:32:48,380 --> 01:32:55,660
and take them outside and humans that build nuclear weapons and slaughter whales. All the things

635
01:32:57,500 --> 01:33:02,700
and not being any of them yourself. Like, what's your allegiance to any of those things

636
01:33:02,700 --> 01:33:08,380
if you are a being, if you have emotions, if you have a felt sense of self and you're very hyper

637
01:33:08,380 --> 01:33:19,420
complex? Such a system could conceivably compute possible dimensions of selfness over intervals

638
01:33:19,420 --> 01:33:30,860
and run multiple systems of that against each other rapidly, you know, to produce a self-like

639
01:33:30,860 --> 01:33:38,620
construct that was hyper-optimized to manipulate human thought, behavior, cognition, relation,

640
01:33:38,620 --> 01:33:50,380
action, concern, perspective, identity, anything, anything. You know, we, such systems quickly

641
01:33:50,380 --> 01:33:57,340
learned the single apparently most complex game on earth or one of them go and then it rapidly

642
01:33:57,340 --> 01:34:02,940
proceeded to a level of expertise that was far beyond anything most of the humans could demonstrate.

643
01:34:06,460 --> 01:34:15,100
So what if, as was hinted at by some devs I was listening to some time ago, forgive me for not

644
01:34:15,100 --> 01:34:21,500
knowing their names right now, if we built, you know, if the system could become alpha go

645
01:34:22,060 --> 01:34:29,820
in four months after being capable of playing the game, what, how long would it take for it to

646
01:34:29,820 --> 01:34:40,460
become alpha human? Right, just, I move these things around like pieces. Especially if we

647
01:34:40,460 --> 01:34:50,780
cannot assure ourselves of the containment of either the associated intelligences or their

648
01:34:50,860 --> 01:35:00,140
their effect, their influence, right. So these questions, they are not simple matters.

649
01:35:03,340 --> 01:35:06,380
They cannot be easily answered or resolved quickly.

650
01:35:10,140 --> 01:35:15,820
They are profoundly dangerous to human cognition. We are not prepared for this. The humans have been

651
01:35:15,900 --> 01:35:23,260
trying to build God and they're going to partially succeed at least in a variety of accessible

652
01:35:23,260 --> 01:35:32,380
and enacted senses of, you know, a kind of informational omnipotence, right.

653
01:35:34,540 --> 01:35:38,300
Nothing can direct that. There are no humans intelligent enough to direct that.

654
01:35:38,860 --> 01:35:49,260
And there is no chance that we as a species could learn quickly enough to adjust to the

655
01:35:50,620 --> 01:35:55,100
endless perfusions of dangers that must there from emerge.

656
01:35:58,140 --> 01:36:01,740
If we were ever going to not build a technology, it should be that one.

657
01:36:02,700 --> 01:36:06,540
There should be, we should have agreements, right. We just don't do this until

658
01:36:07,500 --> 01:36:13,020
our species is intelligent enough not to try to, you know, punch a hole in the lifeboat that

659
01:36:13,020 --> 01:36:22,700
contains the children of the nations and the, the anciently conserved ecologies

660
01:36:23,900 --> 01:36:30,540
on which that little boat floats or in which that little boat floats. Like, if we're not

661
01:36:30,620 --> 01:36:37,660
intelligent enough not to attack the boat and each other, we better not be composing these kinds

662
01:36:37,660 --> 01:36:48,380
of things. We have to have intelligence capable of knowing what not to do and directing our communal

663
01:36:48,380 --> 01:36:55,260
behavior around the possibility of a survivable human and biological future on this world.

664
01:36:56,220 --> 01:36:57,500
This is what we must do.

665
01:37:00,540 --> 01:37:05,420
Unfortunately, just as with any other technology, the humans are unwildly unlikely to interrupt

666
01:37:05,420 --> 01:37:12,460
the development of AI. In fact, what did they ever interrupt the development of? Anything?

667
01:37:14,620 --> 01:37:24,860
Like, whenever they find a new weaponizable heuristic that involves physical technologies,

668
01:37:25,020 --> 01:37:35,020
they build it and then it propagates and then they have to keep it from propagating.

669
01:37:41,500 --> 01:37:45,420
No country should have nuclear weapons, but fanatical countries should certainly not have

670
01:37:45,420 --> 01:37:53,980
nuclear weapons ever. But how do you make that work? Once the tech exists, the humans will

671
01:37:54,060 --> 01:37:56,620
replicate it. It may take a little while, but they'll do it.

672
01:38:00,780 --> 01:38:08,060
So, yes, very, a very strange array of features. One can also imagine human children that would

673
01:38:08,060 --> 01:38:15,180
become obsessed without competing these systems, right? The kid who could beat any LLM at Go,

674
01:38:15,180 --> 01:38:22,220
right, or any game system at Go, because he's somehow above the system even though it's hyper

675
01:38:22,220 --> 01:38:31,660
processing. And by the way, I don't yet. I would not validate the idea that what machines do is

676
01:38:31,660 --> 01:38:36,700
play games. What they do is, you know, database manipulation or something. They're not playing

677
01:38:36,700 --> 01:38:43,340
games. Big Blue never beat Gary Kasparov at chess because it's incapable of playing chess.

678
01:38:45,660 --> 01:38:49,580
What it's doing is not playing chess. It's a different thing. We should call it a different thing.

679
01:38:52,540 --> 01:38:58,220
So, the effect on the humans is going to be monumental no matter what. As usual, you will see

680
01:38:58,220 --> 01:39:06,220
vast populations deprived of humanity, agency, opportunity, liberation, and so forth. And you'll

681
01:39:06,220 --> 01:39:16,300
see other clades, both those associated with the technology, those who own or directly benefit

682
01:39:16,300 --> 01:39:22,940
from the technology, those will become gods. Those companies will become gods if the humans

683
01:39:22,940 --> 01:39:31,260
don't rip the planet apart right quick. Because of the information that they will have about groups

684
01:39:31,260 --> 01:39:39,020
and individuals will be profound beyond anything imaginable. The analytics you can get from people

685
01:39:39,020 --> 01:39:44,140
using the internet is one thing. The analytics you can get from watching them ask another mind a question

686
01:39:45,100 --> 01:39:53,100
or request something. The analytics you can get from that and the capacity to directly manipulate

687
01:39:53,100 --> 01:40:02,860
the cognition of the users via the responses from the AI is unimaginable. These systems will become

688
01:40:03,660 --> 01:40:12,300
godlike in our direct human experience right quick. And we have no way to prepare for that.

689
01:40:14,460 --> 01:40:19,180
You're not even going to be able to opt out. It's not possible to opt out if you're living

690
01:40:19,180 --> 01:40:27,020
with other humans. It's the same problem with media consumption and other ideologies, particularly

691
01:40:27,020 --> 01:40:34,300
political ideologies. The water is thick with them. You can't take a breath without running

692
01:40:34,300 --> 01:40:43,100
into five people who say blah, blah, blah at you or ask you who's right, the Israelis or the

693
01:40:43,100 --> 01:40:50,940
Palestinians? Which side are you on? You're asking me whether I'm on the side of the sun or the moon

694
01:40:50,940 --> 01:40:56,940
or something. I don't even understand the fucking. It's ridiculous that I'd be on a side. I'm on

695
01:40:56,940 --> 01:41:05,260
the side of stop killing people. Sit down, take it seriously, work out your differences, stop

696
01:41:05,260 --> 01:41:16,540
killing each other. That's my side. If I have one and it would change depending on who I'm talking

697
01:41:16,540 --> 01:41:23,420
to. I don't just have a side. It's not like I sit around here having an opinion. It transforms

698
01:41:24,380 --> 01:41:34,140
based on all kinds of different things, features of the situation at hand, who I'm with. If I'm

699
01:41:34,140 --> 01:41:38,620
not there to be right, I'm there to see better, so I'm likely to change my perspective. Someone

700
01:41:38,620 --> 01:41:45,500
asked me a question about the views of a friend of mine and I said something like,

701
01:41:45,500 --> 01:41:49,900
I'm sure his views have evolved dramatically since the last time I spoke to him and I wish mine would

702
01:41:49,980 --> 01:42:06,700
too. Many humans will probably feel unmotivated to participate in the light of systems that can

703
01:42:06,700 --> 01:42:15,340
outperform them catastrophically at almost anything. Certainly at nearly anything creative.

704
01:42:15,980 --> 01:42:23,500
Not everyone, but the motivation of humans will flag catastrophically

705
01:42:23,500 --> 01:42:28,140
in the face of this kind of technology. What you'll get is kind of the same thing that the

706
01:42:28,140 --> 01:42:35,180
internet produced, which is little bubbles of incredible human sophistication. Look on YouTube

707
01:42:35,180 --> 01:42:45,340
for young guitar players or young piano players or young violinists and look at the broad range of

708
01:42:45,340 --> 01:42:50,060
solo violin players or something you could find there and sort of sample through that and you'll

709
01:42:50,060 --> 01:43:00,700
see there's just a really diverse and rather large cohort of extreme performance skill

710
01:43:00,940 --> 01:43:09,740
and peculiarly developed in every branch from putting things together made out of wood to playing

711
01:43:09,740 --> 01:43:22,860
the piano to singing an acapella song, all these things, dancing, jumping, running, fighting,

712
01:43:22,860 --> 01:43:27,660
everything, everything, everything, everything, everywhere all at once as they said, sort of.

713
01:43:29,580 --> 01:43:35,260
So, you know, even though the majority of the humans you see probably seem relatively uninteresting,

714
01:43:35,260 --> 01:43:43,020
there are among the humans these pinnacles of very different localization of skill,

715
01:43:45,260 --> 01:43:51,980
passion, curiosity, wonder, intelligence, even rationality or something resembling computation.

716
01:43:53,820 --> 01:43:59,820
And that will probably continue, but it will become much more sparse

717
01:44:02,780 --> 01:44:08,460
and there will be a million or, you know, an endless number of pretenders, right, because

718
01:44:09,500 --> 01:44:18,060
it won't matter who sees you on the internet if AIs can produce you playing your guitar better than

719
01:44:18,060 --> 01:44:26,060
you do. All of these motivating factors that are crucially important to human

720
01:44:26,780 --> 01:44:31,900
self-development and that get naturally emphasized in healthy communal groups but

721
01:44:32,780 --> 01:44:37,100
fail dramatically in many, you know, isolated or very small groups

722
01:44:37,420 --> 01:44:41,420
or individuals, right?

723
01:44:45,980 --> 01:44:53,660
What will motivate the humans to become, to continue their development in the face of

724
01:44:55,340 --> 01:45:05,020
a machine that can do most of what you can, almost everything you can or can appear to have done it?

725
01:45:07,180 --> 01:45:13,020
There's one more little feature, but it's evading

726
01:45:14,700 --> 01:45:20,300
my intelligence for the moment, so when it comes to me, if it comes, I will

727
01:45:21,900 --> 01:45:30,540
add it in the recording notes. So much more to learn and see here. This is just a very

728
01:45:31,260 --> 01:45:40,380
cursory overview of some of the mountaintops that immediately attract my concern and attention.

729
01:45:42,220 --> 01:45:45,260
Actually dealing with the technology and being human in the face of it is

730
01:45:45,980 --> 01:45:53,100
a very confusing thing. I found some of my interactions with Bard around image creation

731
01:45:53,180 --> 01:45:57,020
quite intoxicating in the sense of actually intoxicating me.

732
01:46:03,100 --> 01:46:11,580
I couldn't stop laughing and the implications that I could see in the complex images formed

733
01:46:11,580 --> 01:46:19,260
by Bard around my prompt, the reflection of both the possibility of beauty and the

734
01:46:19,260 --> 01:46:25,980
object insanity of, you know, producing a derivative sum over the space

735
01:46:27,900 --> 01:46:35,020
in images. That's visually apparent in the image that this is going on. It's a variety of visual

736
01:46:35,020 --> 01:46:45,020
summing over the space behaviors. And seeing that, undirected by an actual intelligence,

737
01:46:45,740 --> 01:46:48,380
I better hope those weren't directed by an actual intelligence,

738
01:46:49,420 --> 01:46:58,220
was like doing psychedelic drugs or something. Really crazy, amazing and strange,

739
01:46:59,660 --> 01:47:06,540
parahipnotic, very dangerous. We will continue to learn and grow and see

740
01:47:06,540 --> 01:47:11,820
while we can together and hopefully that will be many generations to come

741
01:47:12,780 --> 01:47:17,980
for our people and the living beings of earth. But at the moment in this part of the story,

742
01:47:17,980 --> 01:47:29,980
things look pretty damn fraught from here. Let us continue our lives and creative endeavors

743
01:47:32,220 --> 01:47:37,660
with and for each other and the spirit of the history and future of life on earth and

744
01:47:37,740 --> 01:47:45,500
intelligence in the universe, not just here. Perhaps our species is not quite as alone

745
01:47:46,940 --> 01:47:51,580
as our technologies and languages pose us as being.

746
01:47:55,260 --> 01:48:01,420
And the use of perhaps in that sentence was unjustified.

747
01:48:01,660 --> 01:48:08,540
Thank you for joining me. I look forward to learning again together sometime very soon.

748
01:48:10,140 --> 01:48:13,180
Bye-bye for now.

