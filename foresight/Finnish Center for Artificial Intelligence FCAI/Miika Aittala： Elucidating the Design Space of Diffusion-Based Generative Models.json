{"text": " All right. Thanks for the intro. Indeed, the title of the paper is Solucidating the Descent Space of Diffusion Based Tensive Models. This is work with Tero, myself, Timo and Samuli from NVIDIA. The agenda here is to try and make sense of these recently immersed diffusion models, but really dig into the fundamentals and with that understanding then ask what are the best practices for designing and wanting these methods. So for a brief background on generative modelling, there are many ways to do it, but the idea is usually you have a dataset of something, for example in this case phase photos, but it could be anything, even not images, and you want to train some kind of a neural method for basically converting random numbers into random novel instances from that data distribution. And after recently GANs where the leading contender in this space, and these are from there, but now the denoising diffusion methods have really immersed as the leading contender here. So I'm sure we've all seen these superimpressive results from these models like a stable diffusion, and everything I'm going to say is basically stuff that runs at the bottom of these things, and that is in some way directly applicable to anything like this. Okay, so all of these methods, the denoising diffusion methods, the way they implement this idea is you start from pure random noise, you feed it to a neural denoiser, you keep feeding it, and reducing the noise level until it reveals a random image that was hiding underneath the noise, and now you've generated a random image, so this is a generative model. One concern with these methods is efficiency. You need to call this denoiser tens or even thousands of times in some methods to get the best quality. On the other hand, it's indeed a trade with the quality of the individual generative images and with the distribution as a whole. And these tradeoffs are not really well understood in this previous work. And some methods simply work better than others, and it's a bit of a folklore that this one seems to be good one, good and so on. And there are many ways to formulate the theory of these methods. There are, like, market chains, stochastic differential equations, and some more exotic ways. But when you kind of strip away all those fancy theories, in the end they all do something like this. But they differ, lastly, in practical design choices. Like at what rate do you reduce the noise level at different stages of the generation? Do you do this? Oh, it's showing. Does anyone know it? Yeah, thanks. Yeah, whether you do this deterministically or stochastically, we'll see the difference soon. How do you deal with vastly different single magnitudes at different stages of this process? Do you predict the signal or the noise? And so on. And given that ultimately these are the only differences between these existing methods, these must be the explanation for their vastly different performance characteristics also. And these are something we wanted to understand in this process and project. So we'll be building on the differential equation formulation from a couple of years back, where the images seem to evolve according to stochastic or an ordinary differential equation. And in principle it's known that this kind of generalizes all of those other methods. You can express them in this framework. But nobody has really gone through the work of getting their hands dirty and sorting everything into a sort of common framework where you could compare and understand the impact of these design choices. So that's the first thing we are going to be doing here. And armed with that knowledge, we'll then ask what are the best practices for running this sampling process, namely how do you manage this chain of denoising steps in the best possible way. First the deterministic version and then the stochastic version. And then finally we'll come to best practices for training these neural networks. How do you precondition them? How do you, what are the lost functions? Why does this keep coming back? Okay. And just one thing we won't be looking at the actual neural architectures like whether you should use the answer or not. We'll leave that for future work. So we'll be studying a few key works in this field. There's this paper that presents the so-called VBVE method. There's preserving, there's exploding and there's DDIM, denoising diffusion implicit model. It's not really that important for us what the difference is between these but on the face of it they look kind of like packages that you have to take as a whole. You cannot mix and match their properties. But this is not really true. The running theme in this paper is that we identify this complete and exhaustive set of design choices that completely characterize and reproduce any given method or at least these three methods and many others in this space. And this gives us sort of an extra view into the internals of these methods. We can ask what are the exact design choices they made about this and this aspect. Now don't worry. We won't be looking at slides like this. I'll try to keep it visual and intuitive to the extent possible. But the important point here is that this can be done and with this knowledge we can then ask what is the best choice for any given design point here. And that gives us our method, which will be building piece by piece and that then yields significantly improved results. And we'll be measuring our progress with the FID metric, which is sort of the current cost standard in evaluating any kinds of generative models. So let's start looking at how Song and Colleagues build this and I'll formulate this denoting diffusion problem using differential equations. So throughout this talk I'll be using this running toy example, which is actually one detoy example, which is actually quite actually in many ways completely representative of the actual thing that's going on with images. So in a way this is one of the images where you would have more dimensions on the x-axis, with actual high-dimensional images, like one megapixel image is a million numbers, so that would be a million dimensional space. But this describes the essential characteristics of it. So the point is we have some distribution of data. Let's imagine there are cat and dog photos or something and it happens to be this bimodal thing, so certain pixel values are more probable than others. We want to learn to produce novel samples from this distribution. We have a handful of examples, or let's say millions of examples, which is our data set, and based on those we want to learn to do this. So in this analogy one of the samples we have might be this dog photo. On the other axis we have increasing time, which is essentially increasing noise level. That's what we are going to be dealing with when we want to reduce this noise. But before we do that let's look at the easier direction of adding noise, like destroying an image. So if I start taking this image from the training data set, I gradually start adding noise onto it. I end up doing this random work in this pixel value space until the image is completely drowned under this white noise. And if I have a population of images in the end they'll all become indistinguishable white noise. So if I plot the density that these trajectories make it'll look like this. So the density of the data on the left edge becomes diffused over time until it's completely normally distributed at the end. And this is really nice now because it has disappeared again. No. I'll try one thing. I'll try one thing. I'll try one thing. Well, maybe we'll just leave it. Do you think we can do that? Yeah. Yeah, let me know if something important seems to be missing underneath it. So okay. Okay. So yeah, as I said we can sample from this normal distribution at the right edge. We just go random in PyTorch. And that'll give us a sample from that edge. And the magic is that there exists a way to sort of reverse this path we took earlier. So go backward in time and that will land us on the left edge where we have the density of the actual data. And that of course generates an image. And so if I have a population of these complete random noises, oops. Okay. Yeah, if I had many images I would have gotten different instances of the image. Okay. And what makes it stick is that this can be seen as a stochastic differential equation. In this example it's about the simplest one we have. When we go forward in time over a very short time period the change in image dx equals the omega, which is white noise. So that's just a mathematical expression of doing cumulative sum of random noise. Now the magic is that to this forward equation corresponds a backward version that has this same stochastic component, random walk component. But on top of that it has this term that kind of attracts the samples towards the data density. You see it's some kind of a gradient of the density p. But the problem of course is that this p is unknown and here is the axiomagic. This is a well known function from previous literature in data science called the score function and it has the property that you do not need to know the p if you have a least gross optimal denoiser for this data set d. So you can directly evaluate that formula above by the formula below. And this is an opportunity. We train a neural network to be such a denoiser and this means that we can run this kind of backward equation evolution using that denoiser. So some colleagues also present this deterministic variant of this where you don't have the stochastic term you only have this chord term scaled in some appropriate way. And this has a somewhat different like a visual character. You see it's kind of fading in and out instead of like jittering around. And this one actually provides a much cleaner view into this sampling dynamic. So we'll be looking at this first and then returning to the stochastic later. And with this I can now always draw this paint flow lines of this ODE. So the idea is that we are trying to somehow follow these lines to do the generation. And indeed the way that happens is by discretization I take little but macroscopic steps in this space I reduce the time and for any change in time I want to jump. The ODE formula tells me how much the image changes. And again the ODE formula is already does a neural network so the neural network tells us where to go on the next step. That's the general idea. And that gives me a step. I keep stepping until I hit time zero and that's my generated sample. With the SDEs we would have some kind of noise addition on top of this so we would kind of jitter it but I said we'll leave that for later. And now we've exactly reproduced this intuitive picture using differential equations. Okay so that was song and colleagues for our purposes. And let's now identify some design choices involved in making this kind of an ODE or SD. But before we do that we should understand what can go wrong in this process. What are the error sources? Well the obvious one because I might end up like in a different place than I should have when I do this sampling chain. So the obvious one is that if the network gives me an incorrect direction I end up moving in the incorrect direction and in the end I end up somewhat in the wrong place. It's more subtle than this but this is kind of a cartoon. The other source of error is that we are trying to approximate this continuous trajectory in green here using these linear segments. And if I try to jump too far at once the curve will kind of move away from my feet and I'll end up veering off this path. It's of course familiar to anyone who's done like a physical simulation with ODE. And the proof of solutions to that is to take more steps but that's exactly what we want to avoid because that directly means more compute to generate an image. Okay and so what we argue and what is underappreciated in previous work is that these two effects should be analysed or can be and should be analysed in isolation. You don't have to sample in a certain way just because you train your network in a certain way and so on you can decouple this. And indeed we'll be looking at sampling first and then coming back to the training later. Okay so I promise to show you some choices and here is one finally. So when I built this example I added noise at a constant rate over every time step and that gives me this simplicity, it gives me this schedule where the noise level increases as a square root of time because that's how the variance will grow linearly so the standard will go square root. That's what you get if you call random and then do a comp sum on top of it. Had I added it at a different rate I might have arrived at a schedule like this for example where the standard deviation is the gross linearity and indeed I could do any kind of a choice here. I could do something even something crazy like this way we schedule here in the middle if I wanted to for some reason. And indeed we generalise in the paper the ODE form or we reparametise it in such a way that we get a clear view into these effects. So we can parameterise the noise level we want to have reached by explicitly by this function sigma. But the real question is why would you want to do something like this. Well one reason for that could be that if you look at this picture for example you see almost nothing happens until at almost zero noise level suddenly curves rapidly to one of these two basins and there's high curvature there so we'd probably want to be careful in stepping. We'll want to take somehow be more careful in sampling that region and less careful you're in the bulk. So there's two ideas of how you might do that. You might first you might take shorter steps at the more difficult parts usually is the low noise levels because that's where the image details are usually built. The other alternative would be to instead warp the noise schedule in such a way that you just end up spending more time at these difficult parts. And it's tempting to think that these two approaches would be equivalent. And this is an implicit assumption I think that many previous works do. But this is simply not true because the error characteristics can be vastly different between these choices like the error that comes from this tracking this continuous curve and we'll see the effect of that later. So now we've identified the first pair of design choices here. The time steps and the noise schedule. But let's introduce a couple more. And this address is the following problem. I zoom out a little because in reality we add a ton of noise. So at the other extreme the noise level is very large. I've been showing this zoom in so we can easier see what's going on. But I zoomed out now to see what's here. So the issue if you don't do anything is that the signal magnitude grows as the noise level grows. You keep piling noise. The signal is quite simply bigger numerically like the values that are in your sensor. They are much larger at the high noise levels than in the low noise levels. And this is known to be really bad for neural network training dynamics. And these kind of effects are actually critical to deal with to get good performance. So the way many previous works approach this is by using something called variance preserving schedules where you effectively introduce this additional so called scale schedule where you squeeze the signal magnitude into this constant with constant variance tube. So that makes that's one way to make the network happy here. So we generalize this idea again by just formulating an OD that allows you to directly specify any arbitrary scale schedule. And viewing this slide it again becomes appropriate that the only thing that the scale schedule does is distort these flow lines in some way. So you are just doing a coordinate transform in a way on this XT plane. Now there is an alternative way to deal with this scaling issue. And it is quite simply this. Instead of changing the OD at all you could change your neural network in such a way that it has an initial scaling layer that uses the known single scale scale it to something that's palatable for the neural network. And again you might think that this is completely accurate with the OD, but this is simply not true because again the error characteristics are vastly different between these two cases. And I'll come back soon to how the chosen practice. But now we've identified a second set, second pair of these matrices. The scaling schedule and the scaling that happens inside the neural network itself. And that we kind of saw so-called preconditioning of the neural network. Okay, so now we have quite a few collected here. And at this point we can ask, like get our hands dirty go look at the appendices of these papers, read their code for the final ground truth and ask what formulas actually exactly reproduce their approaches. And they are these. Again don't worry, but don't even try to read them. But the question now is what choices should we actually make, which ones of these are good, which ones are suboptimal. And that's going to be the topic of the next section. And for now, we will be ignoring these neural network training aspects. We will be using pre-trained networks from previous work. We won't be retraining anything yet. We'll just try to improve the sampling. So now we move on to the deterministic sampling and actual prescriptions of what you might want to do. So first the noise schedule. Why would some of them be better than others? For example this way one must be terrible for some reason, but why? Well, now we get a clear view. Well, parameterizing things in this way gives us a quite a clear view to this. So let's zoom out again. And consider the fact that we are trying to follow these curving trajectories by following these linear tangents. And that's probably going to be more successful when the tangents happen to coincide with this curve trajectory. So when the trajectory is as straight as possible in other words. So if I use a bad schedule like this one, you see there's already a visible gap between the tangent and the curve. So you easily fall off if you try to step too much. And indeed if I show you this random family of different schedules, we see that some of them seem to be better in this regard than others. In particular this one. And this is actually the same schedule used in the previous work DDIM, which is known to be quite good. And this in a way explains it. So this is the schedule where standard deviation cross-lineally and we do not use any scaling. And indeed we'll be leaving the scaling for neural network parameterization. And the reason for that is that this scaling also introduces unwanted curvature into these lines. Yeah, it just turns them unnecessarily at some point. It's actually better to let the signal in the ODE grow from that perspective. As further, and yeah, with this the is the ODE becomes very simple. So as a further demonstration, like an actual mathematical fact about this schedule and why it allows us to take long steps is that if I took a step directly to times zero, then with this schedule and only this schedule the tangent is pointing directly to the output of the denoiser. And that's very nice because the denoiser output changes only very slowly during the sampling process. And this means that well, the direction you are going to doesn't change almost at all. So it means you can take long bold steps and you can consequently only take a few steps or many fewer steps than with the alternatives. Okay, and then I said we'll want to direct our efforts to the difficult places. Now we've tied our hands with the noise schedule. So the remaining tool is to take different length steps at different stages of the generation. And indeed, when you go look at the possibly implicit choices the previous methods have done, all of them take shorter steps at low noise levels because that's where the detail is built again. And yeah, we formulate this family of these discretizations like a polynomial step length growth and we find that there is a broad optimum of good schedules there. You can read those details in the paper. So there's one more thing that this ODE framework allows you to do which is not so clear with for example the Markov chain form lessons is use higher order solvers. So again there is going to be curvature and it can be quite rapid at places. So you can still fall off the track if you just naively follow tangent and that method of following the tangent of course is called the Euler method. But there are higher order schemes for example in the Hoin scheme you take a second tentative step and you move it back to where you started from and your access step is going to be the average of that and the initial one. And this makes you much better follow these trajectories. This of course has a cost. You need to take these sub steps. And what we find in the paper by extension we're studying this is that this Hoin method strikes the best balance between these higher order methods for sort of the extra bang for the buck. And the improvement is actually quite substantial. Okay, so those are the choices we made. And now we can evaluate. So we'll be evaluating these results throughout the talk on a couple of very competitive generation categories. Saifat Sen at Resolution 32 using it at Resolution 64. And I want to say a couple of words on this might sound like a useless toy example to you if you're used to seeing like outputs from stable diffusion or something. But the way also those methods work is they first they something like a 64 by 64 image and then they upsample it sequentially. And it turns out that generating the 64 image is the difficult part there. The upsampling just kind of it just kind of works. So this is highly indicative of improvements we get in very relevant very relevant classes of models. Okay, so if we look at the performance of the original samples from these works, from a few previous methods on these data sets, we see that we have the quality on the y-axis, the FID lower is better, and we have a number of samples we need to take, like number of steps or the function evaluations on the x-axis. We see that we need to take something like hundreds or even thousands of steps to get kind of saturate the quality to get the best quality that model gives you. So introducing the point sampler and our discretization schedule we vastly improved this. I noticed that the x-axis is logarithmic, so we've gone from like hundreds to dozens of evaluations. And further introducing the noise schedule and scaling schedule further improved the results by a large amount except in DDIM which was already using those schedules. So now we've already made it quite far here and using some super fancy higher audio, so it's not worth the effort. Okay, so now we've covered the deterministic sampling and let's next return to the question of SDE which we put on the back burner on the other slides. And remember instead of following these nice smooth flow trajectories the SDE sort of cheaters around as some kind of exploration around that baseline. So it can be interpreted as replacing the noise as you go on top of like reducing it. And the reason why people care about the SDE is of course well one reason is that that's where this stuff is derived from but the other is that in practice you tend to get better results when you use the SDE instead of the ODE, at least in previous work. And the reason for that will become apparent soon. But let's first generalize this idea a little. So in the paper we present this generalized version of the SDE which allows you to specify the strength of this exploration by this sort of noise replacement schedule. So especially when you set it to zero you get just the ODE boosting this factor. Or you can do more exotic schedules like something like this where you have it behave like an SDE in the middle and like the ODE and so on. And samples would look like this. But again that's the question of is this just a nice streak or like what's the point. And as I said empirically this improves the results. And now looking at this SDE the reason becomes somewhat apparent. So don't try to read it unless you're an expert in SDEs but we can recognize a couple of familiar parts here. So the first term in the SDE is actually just the ODE from the previous section. So that means that we still have this force that is driving us towards the distribution of flow lines. And the remainder we can identify some kind of a lens around diffusion stochastic difference equation which is a well-known thing from a long ago. It has this property that it makes the samples sort of explore your distribution and if the samples are not distributed correctly it will kind of reduce that error. So it has this healing property. And because we do make errors during the sampling it kind of actively corrects for them. And this is how it looks like. So let's take this extreme situation we have our samples to blue dots. And let's say they are really bad. They are not following the underlying distribution at all. They are skewed to one side. So if we keep following the ODE it does nothing to actually correct the skew and we completely miss the other basin of the days for example. So when I introduce stochasticity to this process it starts looking like this. So these samples do this kind of random exploration and gradually forget where they came from and forget the error in this opposition. And now we've covered both modes for example in the generated images on the left edge. So that's the sort of reason why stochasticity is helpful. No, arguably this is the only benefit of the SDE over the ODE. But there are also downsides in using SDEs. For example we would technically have to use these complicated solvers that are arguably designed for much more complicated cases where you have more general SDEs. So we asked the question could we instead directly combine the ODE solving with this idea of this churning of the noise, adding and removing it. And the answer is this. So this is a stochastic example we proposed in the paper. So we have our current noise image at noise level TTI. Remember in our parent recession the noise level is now completely equivalent with time. So we have two sub steps in one step. So first we add noise. So this represents the lens of an exploration. So we landed some random noisier image so the time increases here. And then we solved the ODE to where we actually wanted to go with say lower noise level. And that simply follows the flow line there. And in practice we do this with a single point step. So we keep alternating between this noise addition and the point step. And this brings us closer and closer to time zero as we want. But underneath it is the ODE running the show and guiding us along these lines. But on top of that we now have this jittering with correct servers. Okay, so this all sounds really nice. You get free error correction but it's not actually free because the lens event term is also an approximation of some continuous thing. And you introduce new error also when you make it. So it's actually a quite delicate balance of how much you should do this. And now with this clear view into this dynamics we actually find that it is really finicky. You need to tune the amount of stochasticity on a per data set per architecture basis. You get the benefits but it's really annoying. So it's a mixed bag. Nonetheless it is very useful. So if we compare the ODEs from the previous section their performance with just original SDE samples from these respective works. We see that the SDE solvers are simply better in the end but they are also very slow. Now applying all of these improvements with our method, with the optimal tune settings for this data set we read both much better quality at a much faster rate. And yeah, there's been some previous works that applied also higher order solvers. So I want to highlight one result here. This image net 64 highly competitive just with this change of schedule. We went from a pretty mediocre FID of 2.07 to 1.55 which at the time of getting this result was state of the art but that record was broken before the publication but we'll have our events in a few slides. But just to show that this is just taking the existing network and using it better for improvements already. Okay, so that's it for deterministic sampling. At this point I have to say I'm going to go a bit overtime because of the hassle in the beginning and because this is kind of incompressible anyway. So if you need to leave then no problem. So yeah, that's it for stagastic sampling and for sampling as a whole. So just a brief recap. The way this works was that the role of the ODE is to give us the step direction which is given by the score function which is given by the score function which is given by the score function which is given by the score function which can be evaluated using a denoiser which can be approximated using the neural network and that is the role of the neural network. It tells you where to go in a single step or what's the direction you need to go to. And the theory says that as long as the denoiser does something that minimizes this loss, the L2 denoising loss the theory will be happy. And you can do this separately at every noise level so you can weight these loss according to the noise level also. But before we go to these loss weightings let's look at the denoiser itself. So I draw the CNN there in a bit of a hazy way and this is because it's actually a bad idea to directly connect the noise image to the input of the network or to read the denoised image from its output layer rather we'll want to wrap it between some kind of signal management layers to manage those signal scales of both the input and the output to standardize them somehow and also in this case we can often recycle stuff from the input because let's say if the input image is almost noise free then we don't really need to denoise much we should just copy what we know and only fix the remainder we're going to come to that soon. And this is super critical here. I mean this might sound like boring technical details but these kind of things really are critical for the success of the neural network training and we've seen this over and over again over the years. And in this case the noise levels vary so widely that this is extra critical here. So without too much to do here is how one of the previous methods, the VE method implements the denoiser. So the idea of this setup is that they are learning to predict the noise instead of the signal using those CNN layers. And the way that works, and I'll explain why soon the way that works is of course the loss will be happy if the denoiser can produce the clean image and we can interpret this model as having this kind of a skip connection so the noisy input goes through that. Now implicitly the task of the CNN will be to predict the negative of the noise component in that image and then they have an explicit layer that scales that noise to the known noise level. And so now when you add whatever came from the skip connection to this you get an estimate of the clean image. So this way they kind of turn it so that the CNN itself is concerned with the noise instead of like the signal. I'll explain soon why that is relevant. But first let's do the thing I promised to do a long ago. I said there's huge variations in the magnitude, just the numerical magnitude of these input signals. So fairs to accounts for that, which is problematic. And so we quite simply introduced this interscaling layer here that uses the known standard deviation of the noise to scale the image down. I want to highlight this not like a batch normalization or something. We know what the noise level is, we know what the signal magnitude should be. We divide by an appropriate formula. So that gives you one of the wishes we had on that orientation slide. On the output side we actually have something nice already. So this is very good because now the network only needs to produce a unit standard deviation output and this explicit scaling to the known noise level takes care of applying the actual like a magnitude of that noise. So this makes it again much easier for the neural network. It can always work with these standard sized signals. And that deals with the second hope we have there. But now the question of should we predict the noise or the signal and why? So it turns out this is actually a good idea at small noise levels but a bad idea at high noise levels. So I'll show you what happens at low noise levels. So if we have low noise, the stuff that goes through the skip connection is almost noise free already. And now the CNN predicts this negative noise component and it's scaled down by this very low noise level. And this is great because the neural network is actually the only source of error in this process. So if the network made errors, now we've downscaled them. So it doesn't really matter if the network is good or bad. We didn't do much error in this case. So that's great. We are sort of recycling what we already knew instead of trying to learn the identity function within your network. So that kind of deals with the third hope we had on the slide. But on high noise levels, the situation is reversed. Whatever comes through the skip connection is completely useless. It's a huge, huge noise signal with no signal at all. And now the CNN predicts what the noise is. And then it is massively boosted at this stage. So if the network made any errors, now there are going to be huge errors after this. And those are directly passed out of the denoiser. So now we've introduced a huge error into our stepping procedure in the ODE. It's also a bit of an absurd task because you're trying to subtract two massive signals to get a normal size signal. And kind of like trying to draw without two meter long pencil, not optimal. So instead what we'd like to do is somehow disable the skip connection when the noise level is high. And in that case, effectively the task of the CNN will be to just break the signal directly. There won't be any need to scale it up. So we won't end up boosting errors. And the way we implement that is by adding this sort of switch but in a continuous way. So we have this so-called skip scale, which when set to zero, effectively disables the skip connection. Set to one, you get the noise prediction. And furthermore, we make it so that it's actually a continuous value between zero and one that depends on the noise level. And if it's somewhere in between, that means that we are predicting some kind of a mixture of noise and the signal in this instead of just one of them. And there is a principle way of calculating what the optimal skip weight is. But I won't go there in the interest of time. We have it in the paper of Enix. And that deals with the remaining issues we had on this slide. And now we can look at what the previous works did and what we did. So these are the actual formulas that implement those ideas. Then there is the couple of training details. How should you weight the loss based on the noise level and how often should you show samples of different noise levels. So the general problem, if you don't deal with these issues, is that you might have a highly lopsided distribution of gradient feedback. So if you're not careful, you might be prodding the weights gently to one direction or the other. And then every few iterations you have this massive gradient smash on the weights and so on. And that's probably very bad for your training dynamics. So the role of the loss weighting or the scaling, the numerical scale in front of the loss term, should we be to just equalize the magnitude of the loss or equivalently equalize the magnitude of the gradient feedback that gives. And then the noise level distribution, maybe how often you show images of any given noise level. The role of that is to kind of direct your training efforts to the levels where you know it's relevant, where you can make an impact. And for that, in the paper, we have this sort of an important sampling argument that whatever we do, we end up with this kind of a loss curve. So we don't make much progress at very low and very high noise levels, but we do make a lot of progress in the middle. For example, at very low noise end, you're trying to predict noise from a noise free image. It's impossible, but it also doesn't matter if you can't do it. So we, based on this, we find that it's enough to, or suffice this to sort of have this very broad distribution of noise levels here that are targeted towards the levels where you know you can make progress. And this is a logarithmic scale on the x-axis, so it's a lot of normal distribution. So those are those sources. And it's starting to pretty full. There's one more thing, which I'll just keep in the interest of time. We have some mechanism presented in the paper for dealing with vastly like two small data sets when your network starts overfitting by this augmentation mechanism. You can look at it there. But yeah, let's not go there. It's really only relevant for various small data sets like Cypher. With ImageNet, we haven't found benefits from it, I think. Okay, so with all these improvements, we can stack them one by one. These are the lines here. And in the end, we get state-of-the-art results in various competitive categories. In deterministic sampling, we get an FID 179.97 on the Cypher categories, which might still be state-of-the-art. Also at very low sample counts compared to most previous work. That's more interestingly. Okay, that was with deterministic sampling. When we enable the stochastic sampling and tailor it for these architectures for ImageNet and use these retrained networks, we trained ourselves using these principles. We get an FID of 1.36, which was a state-of-the-art when this paper came out. It's been overtaken, I think in the last few weeks, possibly earlier. So all in all, we've turned this model that was okay-ish in the beginning. And by stacking all of these improvements, we get the best model in the world at that time for generating 64 ImageNet. Interestingly, the stochasticity is no longer helpful with Cypher in this resume or after the training improvement. And so it appears that the network has become so good that it doesn't make that many errors. And any exploration, lands and wine exploration you do, introduces more error than you are actually fixing with it. But this is still not the case with ImageNet. So there it's still pays to do stochasticity. Okay, so that was the, that was mostly it. Just a brief conclusion. So we've sort of exposed this completely modular design of these diffusion models. Instead of viewing them as tightly coupled packages where you can't change anything without breaking something, we show that you can pretty much change every, like you get a valid method no matter what you do as long as you follow these loose guidelines. And then with that knowledge, we get a clear view into what we should actually be doing with those choices. And doing so pays off in a big way. We get much improved quality at much, much more efficient models. One takeaway about stochasticity, it's a bit of a double edged sword. As said, it does help, but it also, it requires that annoying per case tuning. There are no clear principles how to do that tuning. There is also a danger that you can even have bugs in your code or something. And stochasticity will kind of fix them to an extent, which is of course not what you want to do if you're trying to understand what your potential improvements are, what their effect is and so on. Ideally, you'd be able to work in a completely deterministic setting. And if you want, then in the end just kind of reintroduce the stochasticity as the final, final cherry on the top. Okay, so we haven't talked about all the fancy stuff like higher resolutions, network architectures, classifier free guidance and so on. But probably many of these would be right for a similar principle of analysis. We hope this inspires you to also think about that kind of things and certainly we are. So with that, the code and everything is of course available. I would argue this is probably one of the better places to copy paste your code. If you want to experiment with stuff, it's very clean code based that directly implements these ideas. Yeah, so thank you for your attention. Hey, so we have time. Yeah, I have some. I just want to ask a question. All right. It probably has to do with the data's complexity. Seffari is maybe a bit too simplistic in the end. It's kind of learnable entirely. But it seems like that something like ImageNet, it's still so extremely cool.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.0, "text": " All right. Thanks for the intro.", "tokens": [50364, 1057, 558, 13, 2561, 337, 264, 12897, 13, 50514], "temperature": 0.0, "avg_logprob": -0.36371998210529705, "compression_ratio": 1.4697674418604652, "no_speech_prob": 0.00694805383682251}, {"id": 1, "seek": 0, "start": 3.0, "end": 7.0, "text": " Indeed, the title of the paper is", "tokens": [50514, 15061, 11, 264, 4876, 295, 264, 3035, 307, 50714], "temperature": 0.0, "avg_logprob": -0.36371998210529705, "compression_ratio": 1.4697674418604652, "no_speech_prob": 0.00694805383682251}, {"id": 2, "seek": 0, "start": 7.0, "end": 10.0, "text": " Solucidating the Descent Space of Diffusion Based Tensive Models.", "tokens": [50714, 7026, 1311, 327, 990, 264, 3885, 2207, 8705, 295, 413, 3661, 5704, 18785, 314, 2953, 6583, 1625, 13, 50864], "temperature": 0.0, "avg_logprob": -0.36371998210529705, "compression_ratio": 1.4697674418604652, "no_speech_prob": 0.00694805383682251}, {"id": 3, "seek": 0, "start": 10.0, "end": 14.0, "text": " This is work with Tero, myself, Timo and Samuli from NVIDIA.", "tokens": [50864, 639, 307, 589, 365, 314, 2032, 11, 2059, 11, 314, 6934, 293, 4832, 25484, 490, 426, 3958, 6914, 13, 51064], "temperature": 0.0, "avg_logprob": -0.36371998210529705, "compression_ratio": 1.4697674418604652, "no_speech_prob": 0.00694805383682251}, {"id": 4, "seek": 0, "start": 14.0, "end": 20.0, "text": " The agenda here is to try and make sense", "tokens": [51064, 440, 9829, 510, 307, 281, 853, 293, 652, 2020, 51364], "temperature": 0.0, "avg_logprob": -0.36371998210529705, "compression_ratio": 1.4697674418604652, "no_speech_prob": 0.00694805383682251}, {"id": 5, "seek": 0, "start": 20.0, "end": 24.0, "text": " of these recently immersed diffusion models,", "tokens": [51364, 295, 613, 3938, 35416, 25242, 5245, 11, 51564], "temperature": 0.0, "avg_logprob": -0.36371998210529705, "compression_ratio": 1.4697674418604652, "no_speech_prob": 0.00694805383682251}, {"id": 6, "seek": 0, "start": 24.0, "end": 28.0, "text": " but really dig into the fundamentals", "tokens": [51564, 457, 534, 2528, 666, 264, 29505, 51764], "temperature": 0.0, "avg_logprob": -0.36371998210529705, "compression_ratio": 1.4697674418604652, "no_speech_prob": 0.00694805383682251}, {"id": 7, "seek": 2800, "start": 28.0, "end": 32.0, "text": " and with that understanding then ask what are the best practices", "tokens": [50364, 293, 365, 300, 3701, 550, 1029, 437, 366, 264, 1151, 7525, 50564], "temperature": 0.0, "avg_logprob": -0.1668747112315188, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0008973145158961415}, {"id": 8, "seek": 2800, "start": 32.0, "end": 36.0, "text": " for designing and wanting these methods.", "tokens": [50564, 337, 14685, 293, 7935, 613, 7150, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1668747112315188, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0008973145158961415}, {"id": 9, "seek": 2800, "start": 36.0, "end": 41.0, "text": " So for a brief background on generative modelling,", "tokens": [50764, 407, 337, 257, 5353, 3678, 322, 1337, 1166, 42253, 11, 51014], "temperature": 0.0, "avg_logprob": -0.1668747112315188, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0008973145158961415}, {"id": 10, "seek": 2800, "start": 41.0, "end": 45.0, "text": " there are many ways to do it, but the idea is usually", "tokens": [51014, 456, 366, 867, 2098, 281, 360, 309, 11, 457, 264, 1558, 307, 2673, 51214], "temperature": 0.0, "avg_logprob": -0.1668747112315188, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0008973145158961415}, {"id": 11, "seek": 2800, "start": 45.0, "end": 49.0, "text": " you have a dataset of something, for example in this case phase photos,", "tokens": [51214, 291, 362, 257, 28872, 295, 746, 11, 337, 1365, 294, 341, 1389, 5574, 5787, 11, 51414], "temperature": 0.0, "avg_logprob": -0.1668747112315188, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0008973145158961415}, {"id": 12, "seek": 2800, "start": 49.0, "end": 52.0, "text": " but it could be anything, even not images,", "tokens": [51414, 457, 309, 727, 312, 1340, 11, 754, 406, 5267, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1668747112315188, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0008973145158961415}, {"id": 13, "seek": 2800, "start": 52.0, "end": 55.0, "text": " and you want to train some kind of a neural method", "tokens": [51564, 293, 291, 528, 281, 3847, 512, 733, 295, 257, 18161, 3170, 51714], "temperature": 0.0, "avg_logprob": -0.1668747112315188, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0008973145158961415}, {"id": 14, "seek": 5500, "start": 55.0, "end": 59.0, "text": " for basically converting random numbers into random", "tokens": [50364, 337, 1936, 29942, 4974, 3547, 666, 4974, 50564], "temperature": 0.0, "avg_logprob": -0.18572799170889506, "compression_ratio": 1.6095238095238096, "no_speech_prob": 9.1018300736323e-05}, {"id": 15, "seek": 5500, "start": 59.0, "end": 63.0, "text": " novel instances from that data distribution.", "tokens": [50564, 7613, 14519, 490, 300, 1412, 7316, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18572799170889506, "compression_ratio": 1.6095238095238096, "no_speech_prob": 9.1018300736323e-05}, {"id": 16, "seek": 5500, "start": 63.0, "end": 67.0, "text": " And after recently GANs where the leading contender in this space,", "tokens": [50764, 400, 934, 3938, 460, 1770, 82, 689, 264, 5775, 660, 3216, 294, 341, 1901, 11, 50964], "temperature": 0.0, "avg_logprob": -0.18572799170889506, "compression_ratio": 1.6095238095238096, "no_speech_prob": 9.1018300736323e-05}, {"id": 17, "seek": 5500, "start": 67.0, "end": 72.0, "text": " and these are from there, but now", "tokens": [50964, 293, 613, 366, 490, 456, 11, 457, 586, 51214], "temperature": 0.0, "avg_logprob": -0.18572799170889506, "compression_ratio": 1.6095238095238096, "no_speech_prob": 9.1018300736323e-05}, {"id": 18, "seek": 5500, "start": 72.0, "end": 76.0, "text": " the denoising diffusion methods have really immersed", "tokens": [51214, 264, 1441, 78, 3436, 25242, 7150, 362, 534, 35416, 51414], "temperature": 0.0, "avg_logprob": -0.18572799170889506, "compression_ratio": 1.6095238095238096, "no_speech_prob": 9.1018300736323e-05}, {"id": 19, "seek": 5500, "start": 76.0, "end": 80.0, "text": " as the leading contender here.", "tokens": [51414, 382, 264, 5775, 660, 3216, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18572799170889506, "compression_ratio": 1.6095238095238096, "no_speech_prob": 9.1018300736323e-05}, {"id": 20, "seek": 5500, "start": 80.0, "end": 84.0, "text": " So I'm sure we've all seen these superimpressive results", "tokens": [51614, 407, 286, 478, 988, 321, 600, 439, 1612, 613, 1687, 8814, 22733, 3542, 51814], "temperature": 0.0, "avg_logprob": -0.18572799170889506, "compression_ratio": 1.6095238095238096, "no_speech_prob": 9.1018300736323e-05}, {"id": 21, "seek": 8400, "start": 84.0, "end": 88.0, "text": " from these models like a stable diffusion,", "tokens": [50364, 490, 613, 5245, 411, 257, 8351, 25242, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11634116172790528, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00011615321272984147}, {"id": 22, "seek": 8400, "start": 88.0, "end": 92.0, "text": " and everything I'm going to say is basically stuff that runs", "tokens": [50564, 293, 1203, 286, 478, 516, 281, 584, 307, 1936, 1507, 300, 6676, 50764], "temperature": 0.0, "avg_logprob": -0.11634116172790528, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00011615321272984147}, {"id": 23, "seek": 8400, "start": 92.0, "end": 96.0, "text": " at the bottom of these things, and that is in some way directly applicable", "tokens": [50764, 412, 264, 2767, 295, 613, 721, 11, 293, 300, 307, 294, 512, 636, 3838, 21142, 50964], "temperature": 0.0, "avg_logprob": -0.11634116172790528, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00011615321272984147}, {"id": 24, "seek": 8400, "start": 96.0, "end": 100.0, "text": " to anything like this.", "tokens": [50964, 281, 1340, 411, 341, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11634116172790528, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00011615321272984147}, {"id": 25, "seek": 8400, "start": 100.0, "end": 104.0, "text": " Okay, so all of these methods, the denoising diffusion methods,", "tokens": [51164, 1033, 11, 370, 439, 295, 613, 7150, 11, 264, 1441, 78, 3436, 25242, 7150, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11634116172790528, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00011615321272984147}, {"id": 26, "seek": 8400, "start": 104.0, "end": 108.0, "text": " the way they implement this idea is you start from pure random noise,", "tokens": [51364, 264, 636, 436, 4445, 341, 1558, 307, 291, 722, 490, 6075, 4974, 5658, 11, 51564], "temperature": 0.0, "avg_logprob": -0.11634116172790528, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00011615321272984147}, {"id": 27, "seek": 8400, "start": 108.0, "end": 113.0, "text": " you feed it to a neural denoiser, you keep feeding it,", "tokens": [51564, 291, 3154, 309, 281, 257, 18161, 1441, 78, 6694, 11, 291, 1066, 12919, 309, 11, 51814], "temperature": 0.0, "avg_logprob": -0.11634116172790528, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00011615321272984147}, {"id": 28, "seek": 11300, "start": 113.0, "end": 117.0, "text": " and reducing the noise level until it reveals a random image", "tokens": [50364, 293, 12245, 264, 5658, 1496, 1826, 309, 20893, 257, 4974, 3256, 50564], "temperature": 0.0, "avg_logprob": -0.10614847650333327, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.238232577336021e-05}, {"id": 29, "seek": 11300, "start": 117.0, "end": 121.0, "text": " that was hiding underneath the noise, and now you've generated a random image,", "tokens": [50564, 300, 390, 10596, 7223, 264, 5658, 11, 293, 586, 291, 600, 10833, 257, 4974, 3256, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10614847650333327, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.238232577336021e-05}, {"id": 30, "seek": 11300, "start": 121.0, "end": 125.0, "text": " so this is a generative model.", "tokens": [50764, 370, 341, 307, 257, 1337, 1166, 2316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10614847650333327, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.238232577336021e-05}, {"id": 31, "seek": 11300, "start": 125.0, "end": 129.0, "text": " One concern with these methods is efficiency.", "tokens": [50964, 1485, 3136, 365, 613, 7150, 307, 10493, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10614847650333327, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.238232577336021e-05}, {"id": 32, "seek": 11300, "start": 129.0, "end": 133.0, "text": " You need to call this denoiser tens or even thousands of times in some methods", "tokens": [51164, 509, 643, 281, 818, 341, 1441, 78, 6694, 10688, 420, 754, 5383, 295, 1413, 294, 512, 7150, 51364], "temperature": 0.0, "avg_logprob": -0.10614847650333327, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.238232577336021e-05}, {"id": 33, "seek": 11300, "start": 133.0, "end": 137.0, "text": " to get the best quality. On the other hand,", "tokens": [51364, 281, 483, 264, 1151, 3125, 13, 1282, 264, 661, 1011, 11, 51564], "temperature": 0.0, "avg_logprob": -0.10614847650333327, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.238232577336021e-05}, {"id": 34, "seek": 11300, "start": 137.0, "end": 141.0, "text": " it's indeed a trade with the quality of the individual", "tokens": [51564, 309, 311, 6451, 257, 4923, 365, 264, 3125, 295, 264, 2609, 51764], "temperature": 0.0, "avg_logprob": -0.10614847650333327, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.238232577336021e-05}, {"id": 35, "seek": 14100, "start": 141.0, "end": 147.0, "text": " generative images and with the distribution as a whole.", "tokens": [50364, 1337, 1166, 5267, 293, 365, 264, 7316, 382, 257, 1379, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16020620874611727, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.9446380974841304e-05}, {"id": 36, "seek": 14100, "start": 147.0, "end": 151.0, "text": " And these tradeoffs are not really well understood in this previous work.", "tokens": [50664, 400, 613, 4923, 19231, 366, 406, 534, 731, 7320, 294, 341, 3894, 589, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16020620874611727, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.9446380974841304e-05}, {"id": 37, "seek": 14100, "start": 151.0, "end": 155.0, "text": " And some methods simply work better than others,", "tokens": [50864, 400, 512, 7150, 2935, 589, 1101, 813, 2357, 11, 51064], "temperature": 0.0, "avg_logprob": -0.16020620874611727, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.9446380974841304e-05}, {"id": 38, "seek": 14100, "start": 155.0, "end": 159.0, "text": " and it's a bit of a folklore that this one seems to be", "tokens": [51064, 293, 309, 311, 257, 857, 295, 257, 49195, 300, 341, 472, 2544, 281, 312, 51264], "temperature": 0.0, "avg_logprob": -0.16020620874611727, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.9446380974841304e-05}, {"id": 39, "seek": 14100, "start": 159.0, "end": 163.0, "text": " good one, good and so on.", "tokens": [51264, 665, 472, 11, 665, 293, 370, 322, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16020620874611727, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.9446380974841304e-05}, {"id": 40, "seek": 14100, "start": 163.0, "end": 167.0, "text": " And there are many ways to formulate the theory of these methods.", "tokens": [51464, 400, 456, 366, 867, 2098, 281, 47881, 264, 5261, 295, 613, 7150, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16020620874611727, "compression_ratio": 1.6169154228855722, "no_speech_prob": 1.9446380974841304e-05}, {"id": 41, "seek": 16700, "start": 167.0, "end": 171.0, "text": " There are, like, market chains, stochastic differential equations,", "tokens": [50364, 821, 366, 11, 411, 11, 2142, 12626, 11, 342, 8997, 2750, 15756, 11787, 11, 50564], "temperature": 0.0, "avg_logprob": -0.1991169898064582, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.00012134716234868392}, {"id": 42, "seek": 16700, "start": 171.0, "end": 175.0, "text": " and some more exotic ways. But when you kind of strip away", "tokens": [50564, 293, 512, 544, 27063, 2098, 13, 583, 562, 291, 733, 295, 12828, 1314, 50764], "temperature": 0.0, "avg_logprob": -0.1991169898064582, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.00012134716234868392}, {"id": 43, "seek": 16700, "start": 175.0, "end": 179.0, "text": " all those fancy theories, in the end they all do something like this.", "tokens": [50764, 439, 729, 10247, 13667, 11, 294, 264, 917, 436, 439, 360, 746, 411, 341, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1991169898064582, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.00012134716234868392}, {"id": 44, "seek": 16700, "start": 179.0, "end": 183.0, "text": " But they differ, lastly,", "tokens": [50964, 583, 436, 743, 11, 16386, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1991169898064582, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.00012134716234868392}, {"id": 45, "seek": 16700, "start": 183.0, "end": 187.0, "text": " in practical design choices.", "tokens": [51164, 294, 8496, 1715, 7994, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1991169898064582, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.00012134716234868392}, {"id": 46, "seek": 16700, "start": 187.0, "end": 191.0, "text": " Like at what rate do you reduce the noise level", "tokens": [51364, 1743, 412, 437, 3314, 360, 291, 5407, 264, 5658, 1496, 51564], "temperature": 0.0, "avg_logprob": -0.1991169898064582, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.00012134716234868392}, {"id": 47, "seek": 16700, "start": 191.0, "end": 195.0, "text": " at different stages of the generation? Do you do this?", "tokens": [51564, 412, 819, 10232, 295, 264, 5125, 30, 1144, 291, 360, 341, 30, 51764], "temperature": 0.0, "avg_logprob": -0.1991169898064582, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.00012134716234868392}, {"id": 48, "seek": 19500, "start": 195.0, "end": 199.0, "text": " Oh, it's showing.", "tokens": [50364, 876, 11, 309, 311, 4099, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17796148572649276, "compression_ratio": 1.373134328358209, "no_speech_prob": 0.0015747923171147704}, {"id": 49, "seek": 19500, "start": 199.0, "end": 203.0, "text": " Does anyone know it?", "tokens": [50564, 4402, 2878, 458, 309, 30, 50764], "temperature": 0.0, "avg_logprob": -0.17796148572649276, "compression_ratio": 1.373134328358209, "no_speech_prob": 0.0015747923171147704}, {"id": 50, "seek": 19500, "start": 207.0, "end": 211.0, "text": " Yeah, thanks.", "tokens": [50964, 865, 11, 3231, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17796148572649276, "compression_ratio": 1.373134328358209, "no_speech_prob": 0.0015747923171147704}, {"id": 51, "seek": 19500, "start": 211.0, "end": 215.0, "text": " Yeah, whether you do this deterministically", "tokens": [51164, 865, 11, 1968, 291, 360, 341, 15957, 20458, 51364], "temperature": 0.0, "avg_logprob": -0.17796148572649276, "compression_ratio": 1.373134328358209, "no_speech_prob": 0.0015747923171147704}, {"id": 52, "seek": 19500, "start": 215.0, "end": 219.0, "text": " or stochastically, we'll see the difference soon.", "tokens": [51364, 420, 342, 8997, 22808, 11, 321, 603, 536, 264, 2649, 2321, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17796148572649276, "compression_ratio": 1.373134328358209, "no_speech_prob": 0.0015747923171147704}, {"id": 53, "seek": 19500, "start": 219.0, "end": 223.0, "text": " How do you deal with vastly different", "tokens": [51564, 1012, 360, 291, 2028, 365, 41426, 819, 51764], "temperature": 0.0, "avg_logprob": -0.17796148572649276, "compression_ratio": 1.373134328358209, "no_speech_prob": 0.0015747923171147704}, {"id": 54, "seek": 22300, "start": 223.0, "end": 227.0, "text": " single magnitudes at different stages of this process? Do you predict the signal", "tokens": [50364, 2167, 4944, 16451, 412, 819, 10232, 295, 341, 1399, 30, 1144, 291, 6069, 264, 6358, 50564], "temperature": 0.0, "avg_logprob": -0.1769891421000163, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.000666470848955214}, {"id": 55, "seek": 22300, "start": 227.0, "end": 231.0, "text": " or the noise? And so on.", "tokens": [50564, 420, 264, 5658, 30, 400, 370, 322, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1769891421000163, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.000666470848955214}, {"id": 56, "seek": 22300, "start": 231.0, "end": 235.0, "text": " And given that ultimately these are the only differences between these existing methods,", "tokens": [50764, 400, 2212, 300, 6284, 613, 366, 264, 787, 7300, 1296, 613, 6741, 7150, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1769891421000163, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.000666470848955214}, {"id": 57, "seek": 22300, "start": 235.0, "end": 239.0, "text": " these must be the explanation for their vastly different performance", "tokens": [50964, 613, 1633, 312, 264, 10835, 337, 641, 41426, 819, 3389, 51164], "temperature": 0.0, "avg_logprob": -0.1769891421000163, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.000666470848955214}, {"id": 58, "seek": 22300, "start": 239.0, "end": 243.0, "text": " characteristics also. And these are something we wanted to understand", "tokens": [51164, 10891, 611, 13, 400, 613, 366, 746, 321, 1415, 281, 1223, 51364], "temperature": 0.0, "avg_logprob": -0.1769891421000163, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.000666470848955214}, {"id": 59, "seek": 22300, "start": 243.0, "end": 247.0, "text": " in this process and project.", "tokens": [51364, 294, 341, 1399, 293, 1716, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1769891421000163, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.000666470848955214}, {"id": 60, "seek": 22300, "start": 247.0, "end": 251.0, "text": " So we'll be building on the differential equation formulation", "tokens": [51564, 407, 321, 603, 312, 2390, 322, 264, 15756, 5367, 37642, 51764], "temperature": 0.0, "avg_logprob": -0.1769891421000163, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.000666470848955214}, {"id": 61, "seek": 25100, "start": 251.0, "end": 255.0, "text": " from a couple of years back, where the images seem to evolve", "tokens": [50364, 490, 257, 1916, 295, 924, 646, 11, 689, 264, 5267, 1643, 281, 16693, 50564], "temperature": 0.0, "avg_logprob": -0.12097657111383253, "compression_ratio": 1.5968379446640317, "no_speech_prob": 0.0004773730179294944}, {"id": 62, "seek": 25100, "start": 255.0, "end": 259.0, "text": " according to stochastic or an ordinary differential equation.", "tokens": [50564, 4650, 281, 342, 8997, 2750, 420, 364, 10547, 15756, 5367, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12097657111383253, "compression_ratio": 1.5968379446640317, "no_speech_prob": 0.0004773730179294944}, {"id": 63, "seek": 25100, "start": 259.0, "end": 263.0, "text": " And in principle it's known that this kind of generalizes", "tokens": [50764, 400, 294, 8665, 309, 311, 2570, 300, 341, 733, 295, 2674, 5660, 50964], "temperature": 0.0, "avg_logprob": -0.12097657111383253, "compression_ratio": 1.5968379446640317, "no_speech_prob": 0.0004773730179294944}, {"id": 64, "seek": 25100, "start": 263.0, "end": 267.0, "text": " all of those other methods. You can express them in this framework.", "tokens": [50964, 439, 295, 729, 661, 7150, 13, 509, 393, 5109, 552, 294, 341, 8388, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12097657111383253, "compression_ratio": 1.5968379446640317, "no_speech_prob": 0.0004773730179294944}, {"id": 65, "seek": 25100, "start": 267.0, "end": 271.0, "text": " But nobody has really gone through the work of getting their hands dirty", "tokens": [51164, 583, 5079, 575, 534, 2780, 807, 264, 589, 295, 1242, 641, 2377, 9360, 51364], "temperature": 0.0, "avg_logprob": -0.12097657111383253, "compression_ratio": 1.5968379446640317, "no_speech_prob": 0.0004773730179294944}, {"id": 66, "seek": 25100, "start": 271.0, "end": 275.0, "text": " and sorting everything into a sort of", "tokens": [51364, 293, 32411, 1203, 666, 257, 1333, 295, 51564], "temperature": 0.0, "avg_logprob": -0.12097657111383253, "compression_ratio": 1.5968379446640317, "no_speech_prob": 0.0004773730179294944}, {"id": 67, "seek": 25100, "start": 275.0, "end": 279.0, "text": " common framework where you could compare and", "tokens": [51564, 2689, 8388, 689, 291, 727, 6794, 293, 51764], "temperature": 0.0, "avg_logprob": -0.12097657111383253, "compression_ratio": 1.5968379446640317, "no_speech_prob": 0.0004773730179294944}, {"id": 68, "seek": 27900, "start": 279.0, "end": 283.0, "text": " understand the impact of these design choices. So that's the first thing", "tokens": [50364, 1223, 264, 2712, 295, 613, 1715, 7994, 13, 407, 300, 311, 264, 700, 551, 50564], "temperature": 0.0, "avg_logprob": -0.10941557540107019, "compression_ratio": 1.6623931623931625, "no_speech_prob": 8.894958591554314e-05}, {"id": 69, "seek": 27900, "start": 283.0, "end": 287.0, "text": " we are going to be doing here. And armed", "tokens": [50564, 321, 366, 516, 281, 312, 884, 510, 13, 400, 16297, 50764], "temperature": 0.0, "avg_logprob": -0.10941557540107019, "compression_ratio": 1.6623931623931625, "no_speech_prob": 8.894958591554314e-05}, {"id": 70, "seek": 27900, "start": 287.0, "end": 291.0, "text": " with that knowledge, we'll then ask what are the best", "tokens": [50764, 365, 300, 3601, 11, 321, 603, 550, 1029, 437, 366, 264, 1151, 50964], "temperature": 0.0, "avg_logprob": -0.10941557540107019, "compression_ratio": 1.6623931623931625, "no_speech_prob": 8.894958591554314e-05}, {"id": 71, "seek": 27900, "start": 291.0, "end": 295.0, "text": " practices for running this sampling process, namely how do you", "tokens": [50964, 7525, 337, 2614, 341, 21179, 1399, 11, 20926, 577, 360, 291, 51164], "temperature": 0.0, "avg_logprob": -0.10941557540107019, "compression_ratio": 1.6623931623931625, "no_speech_prob": 8.894958591554314e-05}, {"id": 72, "seek": 27900, "start": 295.0, "end": 299.0, "text": " manage this chain of denoising steps in the best possible way.", "tokens": [51164, 3067, 341, 5021, 295, 1441, 78, 3436, 4439, 294, 264, 1151, 1944, 636, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10941557540107019, "compression_ratio": 1.6623931623931625, "no_speech_prob": 8.894958591554314e-05}, {"id": 73, "seek": 27900, "start": 299.0, "end": 303.0, "text": " First the deterministic version and then the stochastic version.", "tokens": [51364, 2386, 264, 15957, 3142, 3037, 293, 550, 264, 342, 8997, 2750, 3037, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10941557540107019, "compression_ratio": 1.6623931623931625, "no_speech_prob": 8.894958591554314e-05}, {"id": 74, "seek": 27900, "start": 303.0, "end": 307.0, "text": " And then finally we'll come to", "tokens": [51564, 400, 550, 2721, 321, 603, 808, 281, 51764], "temperature": 0.0, "avg_logprob": -0.10941557540107019, "compression_ratio": 1.6623931623931625, "no_speech_prob": 8.894958591554314e-05}, {"id": 75, "seek": 30700, "start": 307.0, "end": 311.0, "text": " best practices for training these neural networks. How do you precondition them?", "tokens": [50364, 1151, 7525, 337, 3097, 613, 18161, 9590, 13, 1012, 360, 291, 4346, 684, 849, 552, 30, 50564], "temperature": 0.0, "avg_logprob": -0.16332132475716726, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0003407717449590564}, {"id": 76, "seek": 30700, "start": 311.0, "end": 315.0, "text": " How do you, what are the lost functions?", "tokens": [50564, 1012, 360, 291, 11, 437, 366, 264, 2731, 6828, 30, 50764], "temperature": 0.0, "avg_logprob": -0.16332132475716726, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0003407717449590564}, {"id": 77, "seek": 30700, "start": 315.0, "end": 319.0, "text": " Why does this keep coming back?", "tokens": [50764, 1545, 775, 341, 1066, 1348, 646, 30, 50964], "temperature": 0.0, "avg_logprob": -0.16332132475716726, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0003407717449590564}, {"id": 78, "seek": 30700, "start": 319.0, "end": 323.0, "text": " Okay.", "tokens": [50964, 1033, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16332132475716726, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0003407717449590564}, {"id": 79, "seek": 30700, "start": 323.0, "end": 327.0, "text": " And just one thing we won't be looking at", "tokens": [51164, 400, 445, 472, 551, 321, 1582, 380, 312, 1237, 412, 51364], "temperature": 0.0, "avg_logprob": -0.16332132475716726, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0003407717449590564}, {"id": 80, "seek": 30700, "start": 327.0, "end": 331.0, "text": " the actual neural architectures like whether you should use the answer or not.", "tokens": [51364, 264, 3539, 18161, 6331, 1303, 411, 1968, 291, 820, 764, 264, 1867, 420, 406, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16332132475716726, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0003407717449590564}, {"id": 81, "seek": 30700, "start": 331.0, "end": 335.0, "text": " We'll leave that for future work.", "tokens": [51564, 492, 603, 1856, 300, 337, 2027, 589, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16332132475716726, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0003407717449590564}, {"id": 82, "seek": 33500, "start": 335.0, "end": 339.0, "text": " So we'll be studying a few key works in this field.", "tokens": [50364, 407, 321, 603, 312, 7601, 257, 1326, 2141, 1985, 294, 341, 2519, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18159533028650765, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.0007123370305635035}, {"id": 83, "seek": 33500, "start": 339.0, "end": 343.0, "text": " There's this paper that presents the so-called VBVE method.", "tokens": [50564, 821, 311, 341, 3035, 300, 13533, 264, 370, 12, 11880, 691, 33, 7540, 3170, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18159533028650765, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.0007123370305635035}, {"id": 84, "seek": 33500, "start": 343.0, "end": 347.0, "text": " There's preserving, there's exploding and there's", "tokens": [50764, 821, 311, 33173, 11, 456, 311, 35175, 293, 456, 311, 50964], "temperature": 0.0, "avg_logprob": -0.18159533028650765, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.0007123370305635035}, {"id": 85, "seek": 33500, "start": 347.0, "end": 351.0, "text": " DDIM, denoising diffusion implicit model.", "tokens": [50964, 30778, 6324, 11, 1441, 78, 3436, 25242, 26947, 2316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18159533028650765, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.0007123370305635035}, {"id": 86, "seek": 33500, "start": 351.0, "end": 355.0, "text": " It's not really that important for us what the difference is between these", "tokens": [51164, 467, 311, 406, 534, 300, 1021, 337, 505, 437, 264, 2649, 307, 1296, 613, 51364], "temperature": 0.0, "avg_logprob": -0.18159533028650765, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.0007123370305635035}, {"id": 87, "seek": 33500, "start": 355.0, "end": 359.0, "text": " but on the face of it they look kind of like", "tokens": [51364, 457, 322, 264, 1851, 295, 309, 436, 574, 733, 295, 411, 51564], "temperature": 0.0, "avg_logprob": -0.18159533028650765, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.0007123370305635035}, {"id": 88, "seek": 33500, "start": 359.0, "end": 363.0, "text": " packages that you have to take as a whole.", "tokens": [51564, 17401, 300, 291, 362, 281, 747, 382, 257, 1379, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18159533028650765, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.0007123370305635035}, {"id": 89, "seek": 36300, "start": 363.0, "end": 367.0, "text": " You cannot mix and match their properties.", "tokens": [50364, 509, 2644, 2890, 293, 2995, 641, 7221, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1215201524587778, "compression_ratio": 1.63, "no_speech_prob": 0.00013928917178418487}, {"id": 90, "seek": 36300, "start": 367.0, "end": 371.0, "text": " But this is not really true.", "tokens": [50564, 583, 341, 307, 406, 534, 2074, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1215201524587778, "compression_ratio": 1.63, "no_speech_prob": 0.00013928917178418487}, {"id": 91, "seek": 36300, "start": 371.0, "end": 375.0, "text": " The running theme in this paper", "tokens": [50764, 440, 2614, 6314, 294, 341, 3035, 50964], "temperature": 0.0, "avg_logprob": -0.1215201524587778, "compression_ratio": 1.63, "no_speech_prob": 0.00013928917178418487}, {"id": 92, "seek": 36300, "start": 375.0, "end": 379.0, "text": " is that we identify this complete and exhaustive set of", "tokens": [50964, 307, 300, 321, 5876, 341, 3566, 293, 14687, 488, 992, 295, 51164], "temperature": 0.0, "avg_logprob": -0.1215201524587778, "compression_ratio": 1.63, "no_speech_prob": 0.00013928917178418487}, {"id": 93, "seek": 36300, "start": 379.0, "end": 383.0, "text": " design choices that completely characterize and reproduce", "tokens": [51164, 1715, 7994, 300, 2584, 38463, 293, 29501, 51364], "temperature": 0.0, "avg_logprob": -0.1215201524587778, "compression_ratio": 1.63, "no_speech_prob": 0.00013928917178418487}, {"id": 94, "seek": 36300, "start": 383.0, "end": 387.0, "text": " any given method or at least these three methods and many others", "tokens": [51364, 604, 2212, 3170, 420, 412, 1935, 613, 1045, 7150, 293, 867, 2357, 51564], "temperature": 0.0, "avg_logprob": -0.1215201524587778, "compression_ratio": 1.63, "no_speech_prob": 0.00013928917178418487}, {"id": 95, "seek": 36300, "start": 387.0, "end": 391.0, "text": " in this space. And this gives us sort of an", "tokens": [51564, 294, 341, 1901, 13, 400, 341, 2709, 505, 1333, 295, 364, 51764], "temperature": 0.0, "avg_logprob": -0.1215201524587778, "compression_ratio": 1.63, "no_speech_prob": 0.00013928917178418487}, {"id": 96, "seek": 39100, "start": 391.0, "end": 395.0, "text": " extra view into the internals of these methods. We can ask", "tokens": [50364, 2857, 1910, 666, 264, 2154, 1124, 295, 613, 7150, 13, 492, 393, 1029, 50564], "temperature": 0.0, "avg_logprob": -0.07850478459330439, "compression_ratio": 1.6903765690376569, "no_speech_prob": 3.054479748243466e-05}, {"id": 97, "seek": 39100, "start": 395.0, "end": 399.0, "text": " what are the exact design choices they made about this and this aspect.", "tokens": [50564, 437, 366, 264, 1900, 1715, 7994, 436, 1027, 466, 341, 293, 341, 4171, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07850478459330439, "compression_ratio": 1.6903765690376569, "no_speech_prob": 3.054479748243466e-05}, {"id": 98, "seek": 39100, "start": 399.0, "end": 403.0, "text": " Now don't worry. We won't be looking at slides", "tokens": [50764, 823, 500, 380, 3292, 13, 492, 1582, 380, 312, 1237, 412, 9788, 50964], "temperature": 0.0, "avg_logprob": -0.07850478459330439, "compression_ratio": 1.6903765690376569, "no_speech_prob": 3.054479748243466e-05}, {"id": 99, "seek": 39100, "start": 403.0, "end": 407.0, "text": " like this. I'll try to keep it visual and intuitive", "tokens": [50964, 411, 341, 13, 286, 603, 853, 281, 1066, 309, 5056, 293, 21769, 51164], "temperature": 0.0, "avg_logprob": -0.07850478459330439, "compression_ratio": 1.6903765690376569, "no_speech_prob": 3.054479748243466e-05}, {"id": 100, "seek": 39100, "start": 407.0, "end": 411.0, "text": " to the extent possible. But the important point here is that this can be done", "tokens": [51164, 281, 264, 8396, 1944, 13, 583, 264, 1021, 935, 510, 307, 300, 341, 393, 312, 1096, 51364], "temperature": 0.0, "avg_logprob": -0.07850478459330439, "compression_ratio": 1.6903765690376569, "no_speech_prob": 3.054479748243466e-05}, {"id": 101, "seek": 39100, "start": 411.0, "end": 415.0, "text": " and with this knowledge we can then ask what is the", "tokens": [51364, 293, 365, 341, 3601, 321, 393, 550, 1029, 437, 307, 264, 51564], "temperature": 0.0, "avg_logprob": -0.07850478459330439, "compression_ratio": 1.6903765690376569, "no_speech_prob": 3.054479748243466e-05}, {"id": 102, "seek": 39100, "start": 415.0, "end": 419.0, "text": " best choice for any given design point here.", "tokens": [51564, 1151, 3922, 337, 604, 2212, 1715, 935, 510, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07850478459330439, "compression_ratio": 1.6903765690376569, "no_speech_prob": 3.054479748243466e-05}, {"id": 103, "seek": 41900, "start": 419.0, "end": 423.0, "text": " And that gives us our method, which will be building piece by piece", "tokens": [50364, 400, 300, 2709, 505, 527, 3170, 11, 597, 486, 312, 2390, 2522, 538, 2522, 50564], "temperature": 0.0, "avg_logprob": -0.11593845413952339, "compression_ratio": 1.5209302325581395, "no_speech_prob": 2.0161925931461155e-05}, {"id": 104, "seek": 41900, "start": 423.0, "end": 427.0, "text": " and that then yields significantly improved results.", "tokens": [50564, 293, 300, 550, 32168, 10591, 9689, 3542, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11593845413952339, "compression_ratio": 1.5209302325581395, "no_speech_prob": 2.0161925931461155e-05}, {"id": 105, "seek": 41900, "start": 427.0, "end": 431.0, "text": " And we'll be measuring our progress with the FID", "tokens": [50764, 400, 321, 603, 312, 13389, 527, 4205, 365, 264, 479, 2777, 50964], "temperature": 0.0, "avg_logprob": -0.11593845413952339, "compression_ratio": 1.5209302325581395, "no_speech_prob": 2.0161925931461155e-05}, {"id": 106, "seek": 41900, "start": 431.0, "end": 435.0, "text": " metric, which is sort of the current cost standard", "tokens": [50964, 20678, 11, 597, 307, 1333, 295, 264, 2190, 2063, 3832, 51164], "temperature": 0.0, "avg_logprob": -0.11593845413952339, "compression_ratio": 1.5209302325581395, "no_speech_prob": 2.0161925931461155e-05}, {"id": 107, "seek": 41900, "start": 435.0, "end": 439.0, "text": " in evaluating any kinds of generative models.", "tokens": [51164, 294, 27479, 604, 3685, 295, 1337, 1166, 5245, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11593845413952339, "compression_ratio": 1.5209302325581395, "no_speech_prob": 2.0161925931461155e-05}, {"id": 108, "seek": 41900, "start": 439.0, "end": 443.0, "text": " So let's start looking at how", "tokens": [51364, 407, 718, 311, 722, 1237, 412, 577, 51564], "temperature": 0.0, "avg_logprob": -0.11593845413952339, "compression_ratio": 1.5209302325581395, "no_speech_prob": 2.0161925931461155e-05}, {"id": 109, "seek": 41900, "start": 443.0, "end": 447.0, "text": " Song and Colleagues build this", "tokens": [51564, 11862, 293, 4586, 68, 7063, 1322, 341, 51764], "temperature": 0.0, "avg_logprob": -0.11593845413952339, "compression_ratio": 1.5209302325581395, "no_speech_prob": 2.0161925931461155e-05}, {"id": 110, "seek": 44700, "start": 447.0, "end": 451.0, "text": " and I'll formulate this denoting diffusion problem", "tokens": [50364, 293, 286, 603, 47881, 341, 1441, 17001, 25242, 1154, 50564], "temperature": 0.0, "avg_logprob": -0.20935747700352822, "compression_ratio": 1.75, "no_speech_prob": 5.0179671234218404e-05}, {"id": 111, "seek": 44700, "start": 451.0, "end": 455.0, "text": " using differential equations.", "tokens": [50564, 1228, 15756, 11787, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20935747700352822, "compression_ratio": 1.75, "no_speech_prob": 5.0179671234218404e-05}, {"id": 112, "seek": 44700, "start": 455.0, "end": 459.0, "text": " So throughout this talk I'll be using this running toy example,", "tokens": [50764, 407, 3710, 341, 751, 286, 603, 312, 1228, 341, 2614, 12058, 1365, 11, 50964], "temperature": 0.0, "avg_logprob": -0.20935747700352822, "compression_ratio": 1.75, "no_speech_prob": 5.0179671234218404e-05}, {"id": 113, "seek": 44700, "start": 459.0, "end": 463.0, "text": " which is actually one detoy example, which is actually quite", "tokens": [50964, 597, 307, 767, 472, 1141, 939, 1365, 11, 597, 307, 767, 1596, 51164], "temperature": 0.0, "avg_logprob": -0.20935747700352822, "compression_ratio": 1.75, "no_speech_prob": 5.0179671234218404e-05}, {"id": 114, "seek": 44700, "start": 463.0, "end": 467.0, "text": " actually in many ways completely representative of the actual thing that's going on", "tokens": [51164, 767, 294, 867, 2098, 2584, 12424, 295, 264, 3539, 551, 300, 311, 516, 322, 51364], "temperature": 0.0, "avg_logprob": -0.20935747700352822, "compression_ratio": 1.75, "no_speech_prob": 5.0179671234218404e-05}, {"id": 115, "seek": 44700, "start": 467.0, "end": 471.0, "text": " with images. So in a way this is one of the images where", "tokens": [51364, 365, 5267, 13, 407, 294, 257, 636, 341, 307, 472, 295, 264, 5267, 689, 51564], "temperature": 0.0, "avg_logprob": -0.20935747700352822, "compression_ratio": 1.75, "no_speech_prob": 5.0179671234218404e-05}, {"id": 116, "seek": 44700, "start": 471.0, "end": 475.0, "text": " you would have more dimensions on the x-axis,", "tokens": [51564, 291, 576, 362, 544, 12819, 322, 264, 2031, 12, 24633, 11, 51764], "temperature": 0.0, "avg_logprob": -0.20935747700352822, "compression_ratio": 1.75, "no_speech_prob": 5.0179671234218404e-05}, {"id": 117, "seek": 47500, "start": 475.0, "end": 479.0, "text": " with actual high-dimensional images, like", "tokens": [50364, 365, 3539, 1090, 12, 18759, 5267, 11, 411, 50564], "temperature": 0.0, "avg_logprob": -0.19525030035721627, "compression_ratio": 1.6371308016877637, "no_speech_prob": 8.435426570940763e-05}, {"id": 118, "seek": 47500, "start": 479.0, "end": 483.0, "text": " one megapixel image is a million numbers, so that would be a million dimensional space.", "tokens": [50564, 472, 34733, 34599, 3256, 307, 257, 2459, 3547, 11, 370, 300, 576, 312, 257, 2459, 18795, 1901, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19525030035721627, "compression_ratio": 1.6371308016877637, "no_speech_prob": 8.435426570940763e-05}, {"id": 119, "seek": 47500, "start": 483.0, "end": 487.0, "text": " But this describes the essential characteristics of it.", "tokens": [50764, 583, 341, 15626, 264, 7115, 10891, 295, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19525030035721627, "compression_ratio": 1.6371308016877637, "no_speech_prob": 8.435426570940763e-05}, {"id": 120, "seek": 47500, "start": 487.0, "end": 491.0, "text": " So the point is we have some distribution of data.", "tokens": [50964, 407, 264, 935, 307, 321, 362, 512, 7316, 295, 1412, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19525030035721627, "compression_ratio": 1.6371308016877637, "no_speech_prob": 8.435426570940763e-05}, {"id": 121, "seek": 47500, "start": 491.0, "end": 495.0, "text": " Let's imagine there are cat and dog photos or something", "tokens": [51164, 961, 311, 3811, 456, 366, 3857, 293, 3000, 5787, 420, 746, 51364], "temperature": 0.0, "avg_logprob": -0.19525030035721627, "compression_ratio": 1.6371308016877637, "no_speech_prob": 8.435426570940763e-05}, {"id": 122, "seek": 47500, "start": 495.0, "end": 499.0, "text": " and it happens to be this bimodal thing, so certain pixel values", "tokens": [51364, 293, 309, 2314, 281, 312, 341, 272, 332, 378, 304, 551, 11, 370, 1629, 19261, 4190, 51564], "temperature": 0.0, "avg_logprob": -0.19525030035721627, "compression_ratio": 1.6371308016877637, "no_speech_prob": 8.435426570940763e-05}, {"id": 123, "seek": 47500, "start": 499.0, "end": 503.0, "text": " are more probable than others.", "tokens": [51564, 366, 544, 21759, 813, 2357, 13, 51764], "temperature": 0.0, "avg_logprob": -0.19525030035721627, "compression_ratio": 1.6371308016877637, "no_speech_prob": 8.435426570940763e-05}, {"id": 124, "seek": 50300, "start": 503.0, "end": 507.0, "text": " We want to learn to produce novel samples from this distribution.", "tokens": [50364, 492, 528, 281, 1466, 281, 5258, 7613, 10938, 490, 341, 7316, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08942738638983833, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.4881008610245772e-05}, {"id": 125, "seek": 50300, "start": 507.0, "end": 511.0, "text": " We have a handful of examples, or let's say millions", "tokens": [50564, 492, 362, 257, 16458, 295, 5110, 11, 420, 718, 311, 584, 6803, 50764], "temperature": 0.0, "avg_logprob": -0.08942738638983833, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.4881008610245772e-05}, {"id": 126, "seek": 50300, "start": 511.0, "end": 515.0, "text": " of examples, which is our data set, and based on those", "tokens": [50764, 295, 5110, 11, 597, 307, 527, 1412, 992, 11, 293, 2361, 322, 729, 50964], "temperature": 0.0, "avg_logprob": -0.08942738638983833, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.4881008610245772e-05}, {"id": 127, "seek": 50300, "start": 515.0, "end": 519.0, "text": " we want to learn to do this. So in this analogy", "tokens": [50964, 321, 528, 281, 1466, 281, 360, 341, 13, 407, 294, 341, 21663, 51164], "temperature": 0.0, "avg_logprob": -0.08942738638983833, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.4881008610245772e-05}, {"id": 128, "seek": 50300, "start": 519.0, "end": 523.0, "text": " one of the samples we have might be this dog photo.", "tokens": [51164, 472, 295, 264, 10938, 321, 362, 1062, 312, 341, 3000, 5052, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08942738638983833, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.4881008610245772e-05}, {"id": 129, "seek": 50300, "start": 523.0, "end": 527.0, "text": " On the other axis", "tokens": [51364, 1282, 264, 661, 10298, 51564], "temperature": 0.0, "avg_logprob": -0.08942738638983833, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.4881008610245772e-05}, {"id": 130, "seek": 50300, "start": 527.0, "end": 531.0, "text": " we have increasing time, which is essentially increasing", "tokens": [51564, 321, 362, 5662, 565, 11, 597, 307, 4476, 5662, 51764], "temperature": 0.0, "avg_logprob": -0.08942738638983833, "compression_ratio": 1.7227722772277227, "no_speech_prob": 1.4881008610245772e-05}, {"id": 131, "seek": 53100, "start": 531.0, "end": 535.0, "text": " noise level. That's what we are going to be dealing with when we want to reduce this noise.", "tokens": [50364, 5658, 1496, 13, 663, 311, 437, 321, 366, 516, 281, 312, 6260, 365, 562, 321, 528, 281, 5407, 341, 5658, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13943710132521026, "compression_ratio": 1.6375, "no_speech_prob": 3.2651147193973884e-05}, {"id": 132, "seek": 53100, "start": 535.0, "end": 539.0, "text": " But before we do that", "tokens": [50564, 583, 949, 321, 360, 300, 50764], "temperature": 0.0, "avg_logprob": -0.13943710132521026, "compression_ratio": 1.6375, "no_speech_prob": 3.2651147193973884e-05}, {"id": 133, "seek": 53100, "start": 539.0, "end": 543.0, "text": " let's look at the easier direction of adding noise, like", "tokens": [50764, 718, 311, 574, 412, 264, 3571, 3513, 295, 5127, 5658, 11, 411, 50964], "temperature": 0.0, "avg_logprob": -0.13943710132521026, "compression_ratio": 1.6375, "no_speech_prob": 3.2651147193973884e-05}, {"id": 134, "seek": 53100, "start": 543.0, "end": 547.0, "text": " destroying an image. So if I start taking this", "tokens": [50964, 19926, 364, 3256, 13, 407, 498, 286, 722, 1940, 341, 51164], "temperature": 0.0, "avg_logprob": -0.13943710132521026, "compression_ratio": 1.6375, "no_speech_prob": 3.2651147193973884e-05}, {"id": 135, "seek": 53100, "start": 547.0, "end": 551.0, "text": " image from the training data set, I gradually start adding noise onto it.", "tokens": [51164, 3256, 490, 264, 3097, 1412, 992, 11, 286, 13145, 722, 5127, 5658, 3911, 309, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13943710132521026, "compression_ratio": 1.6375, "no_speech_prob": 3.2651147193973884e-05}, {"id": 136, "seek": 53100, "start": 551.0, "end": 555.0, "text": " I end up doing this random work in this pixel", "tokens": [51364, 286, 917, 493, 884, 341, 4974, 589, 294, 341, 19261, 51564], "temperature": 0.0, "avg_logprob": -0.13943710132521026, "compression_ratio": 1.6375, "no_speech_prob": 3.2651147193973884e-05}, {"id": 137, "seek": 53100, "start": 555.0, "end": 559.0, "text": " value space until the image is completely drowned under", "tokens": [51564, 2158, 1901, 1826, 264, 3256, 307, 2584, 38233, 833, 51764], "temperature": 0.0, "avg_logprob": -0.13943710132521026, "compression_ratio": 1.6375, "no_speech_prob": 3.2651147193973884e-05}, {"id": 138, "seek": 55900, "start": 559.0, "end": 563.0, "text": " this white noise. And if I have a population of images", "tokens": [50364, 341, 2418, 5658, 13, 400, 498, 286, 362, 257, 4415, 295, 5267, 50564], "temperature": 0.0, "avg_logprob": -0.10020434201418699, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.3319842764758505e-05}, {"id": 139, "seek": 55900, "start": 563.0, "end": 567.0, "text": " in the end they'll all become", "tokens": [50564, 294, 264, 917, 436, 603, 439, 1813, 50764], "temperature": 0.0, "avg_logprob": -0.10020434201418699, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.3319842764758505e-05}, {"id": 140, "seek": 55900, "start": 567.0, "end": 571.0, "text": " indistinguishable white noise. So if I plot", "tokens": [50764, 1016, 468, 7050, 742, 712, 2418, 5658, 13, 407, 498, 286, 7542, 50964], "temperature": 0.0, "avg_logprob": -0.10020434201418699, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.3319842764758505e-05}, {"id": 141, "seek": 55900, "start": 571.0, "end": 575.0, "text": " the density that these trajectories make", "tokens": [50964, 264, 10305, 300, 613, 18257, 2083, 652, 51164], "temperature": 0.0, "avg_logprob": -0.10020434201418699, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.3319842764758505e-05}, {"id": 142, "seek": 55900, "start": 575.0, "end": 579.0, "text": " it'll look like this. So the density of the data", "tokens": [51164, 309, 603, 574, 411, 341, 13, 407, 264, 10305, 295, 264, 1412, 51364], "temperature": 0.0, "avg_logprob": -0.10020434201418699, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.3319842764758505e-05}, {"id": 143, "seek": 55900, "start": 579.0, "end": 583.0, "text": " on the left edge becomes diffused over time until it's completely", "tokens": [51364, 322, 264, 1411, 4691, 3643, 7593, 4717, 670, 565, 1826, 309, 311, 2584, 51564], "temperature": 0.0, "avg_logprob": -0.10020434201418699, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.3319842764758505e-05}, {"id": 144, "seek": 55900, "start": 583.0, "end": 587.0, "text": " normally distributed at the end. And this is really nice now", "tokens": [51564, 5646, 12631, 412, 264, 917, 13, 400, 341, 307, 534, 1481, 586, 51764], "temperature": 0.0, "avg_logprob": -0.10020434201418699, "compression_ratio": 1.6428571428571428, "no_speech_prob": 1.3319842764758505e-05}, {"id": 145, "seek": 58700, "start": 587.0, "end": 591.0, "text": " because it has disappeared again.", "tokens": [50364, 570, 309, 575, 13954, 797, 13, 50564], "temperature": 0.0, "avg_logprob": -0.24292971163379903, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.00042051123455166817}, {"id": 146, "seek": 58700, "start": 591.0, "end": 595.0, "text": " No.", "tokens": [50564, 883, 13, 50764], "temperature": 0.0, "avg_logprob": -0.24292971163379903, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.00042051123455166817}, {"id": 147, "seek": 58700, "start": 595.0, "end": 599.0, "text": " I'll try", "tokens": [50764, 286, 603, 853, 50964], "temperature": 0.0, "avg_logprob": -0.24292971163379903, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.00042051123455166817}, {"id": 148, "seek": 58700, "start": 599.0, "end": 603.0, "text": " one thing.", "tokens": [50964, 472, 551, 13, 51164], "temperature": 0.0, "avg_logprob": -0.24292971163379903, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.00042051123455166817}, {"id": 149, "seek": 58700, "start": 603.0, "end": 607.0, "text": " I'll try one thing.", "tokens": [51164, 286, 603, 853, 472, 551, 13, 51364], "temperature": 0.0, "avg_logprob": -0.24292971163379903, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.00042051123455166817}, {"id": 150, "seek": 58700, "start": 607.0, "end": 611.0, "text": " I'll try one thing.", "tokens": [51364, 286, 603, 853, 472, 551, 13, 51564], "temperature": 0.0, "avg_logprob": -0.24292971163379903, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.00042051123455166817}, {"id": 151, "seek": 58700, "start": 611.0, "end": 615.0, "text": " Well, maybe we'll just leave", "tokens": [51564, 1042, 11, 1310, 321, 603, 445, 1856, 51764], "temperature": 0.0, "avg_logprob": -0.24292971163379903, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.00042051123455166817}, {"id": 152, "seek": 61500, "start": 615.0, "end": 619.0, "text": " it. Do you think we can do that?", "tokens": [50364, 309, 13, 1144, 291, 519, 321, 393, 360, 300, 30, 50564], "temperature": 0.0, "avg_logprob": -0.16576497753461203, "compression_ratio": 1.52803738317757, "no_speech_prob": 8.302260539494455e-05}, {"id": 153, "seek": 61500, "start": 619.0, "end": 623.0, "text": " Yeah.", "tokens": [50564, 865, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16576497753461203, "compression_ratio": 1.52803738317757, "no_speech_prob": 8.302260539494455e-05}, {"id": 154, "seek": 61500, "start": 623.0, "end": 627.0, "text": " Yeah, let me know if something important seems to be missing underneath it.", "tokens": [50764, 865, 11, 718, 385, 458, 498, 746, 1021, 2544, 281, 312, 5361, 7223, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16576497753461203, "compression_ratio": 1.52803738317757, "no_speech_prob": 8.302260539494455e-05}, {"id": 155, "seek": 61500, "start": 627.0, "end": 631.0, "text": " So okay. Okay. So yeah, as I said", "tokens": [50964, 407, 1392, 13, 1033, 13, 407, 1338, 11, 382, 286, 848, 51164], "temperature": 0.0, "avg_logprob": -0.16576497753461203, "compression_ratio": 1.52803738317757, "no_speech_prob": 8.302260539494455e-05}, {"id": 156, "seek": 61500, "start": 631.0, "end": 635.0, "text": " we can sample from this normal distribution at the right edge. We just", "tokens": [51164, 321, 393, 6889, 490, 341, 2710, 7316, 412, 264, 558, 4691, 13, 492, 445, 51364], "temperature": 0.0, "avg_logprob": -0.16576497753461203, "compression_ratio": 1.52803738317757, "no_speech_prob": 8.302260539494455e-05}, {"id": 157, "seek": 61500, "start": 635.0, "end": 639.0, "text": " go random in PyTorch. And that'll give us a sample from that edge.", "tokens": [51364, 352, 4974, 294, 9953, 51, 284, 339, 13, 400, 300, 603, 976, 505, 257, 6889, 490, 300, 4691, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16576497753461203, "compression_ratio": 1.52803738317757, "no_speech_prob": 8.302260539494455e-05}, {"id": 158, "seek": 61500, "start": 639.0, "end": 643.0, "text": " And the magic is that there exists a way", "tokens": [51564, 400, 264, 5585, 307, 300, 456, 8198, 257, 636, 51764], "temperature": 0.0, "avg_logprob": -0.16576497753461203, "compression_ratio": 1.52803738317757, "no_speech_prob": 8.302260539494455e-05}, {"id": 159, "seek": 64300, "start": 643.0, "end": 647.0, "text": " to sort of reverse this path we took earlier.", "tokens": [50364, 281, 1333, 295, 9943, 341, 3100, 321, 1890, 3071, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12367424854012422, "compression_ratio": 1.5621890547263682, "no_speech_prob": 4.759436342283152e-05}, {"id": 160, "seek": 64300, "start": 647.0, "end": 651.0, "text": " So go backward in time and that will land us", "tokens": [50564, 407, 352, 23897, 294, 565, 293, 300, 486, 2117, 505, 50764], "temperature": 0.0, "avg_logprob": -0.12367424854012422, "compression_ratio": 1.5621890547263682, "no_speech_prob": 4.759436342283152e-05}, {"id": 161, "seek": 64300, "start": 651.0, "end": 655.0, "text": " on the left edge where we have the density of the actual data. And that of course generates an image.", "tokens": [50764, 322, 264, 1411, 4691, 689, 321, 362, 264, 10305, 295, 264, 3539, 1412, 13, 400, 300, 295, 1164, 23815, 364, 3256, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12367424854012422, "compression_ratio": 1.5621890547263682, "no_speech_prob": 4.759436342283152e-05}, {"id": 162, "seek": 64300, "start": 655.0, "end": 659.0, "text": " And so if I have a population of these", "tokens": [50964, 400, 370, 498, 286, 362, 257, 4415, 295, 613, 51164], "temperature": 0.0, "avg_logprob": -0.12367424854012422, "compression_ratio": 1.5621890547263682, "no_speech_prob": 4.759436342283152e-05}, {"id": 163, "seek": 64300, "start": 659.0, "end": 663.0, "text": " complete random noises, oops.", "tokens": [51164, 3566, 4974, 14620, 11, 34166, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12367424854012422, "compression_ratio": 1.5621890547263682, "no_speech_prob": 4.759436342283152e-05}, {"id": 164, "seek": 64300, "start": 663.0, "end": 667.0, "text": " Okay.", "tokens": [51364, 1033, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12367424854012422, "compression_ratio": 1.5621890547263682, "no_speech_prob": 4.759436342283152e-05}, {"id": 165, "seek": 64300, "start": 667.0, "end": 671.0, "text": " Yeah, if I had many images I would have gotten", "tokens": [51564, 865, 11, 498, 286, 632, 867, 5267, 286, 576, 362, 5768, 51764], "temperature": 0.0, "avg_logprob": -0.12367424854012422, "compression_ratio": 1.5621890547263682, "no_speech_prob": 4.759436342283152e-05}, {"id": 166, "seek": 67100, "start": 671.0, "end": 675.0, "text": " different instances of the image. Okay.", "tokens": [50364, 819, 14519, 295, 264, 3256, 13, 1033, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1240409179167314, "compression_ratio": 1.5360360360360361, "no_speech_prob": 1.8816117517417297e-05}, {"id": 167, "seek": 67100, "start": 675.0, "end": 679.0, "text": " And what makes it stick is that this can be seen", "tokens": [50564, 400, 437, 1669, 309, 2897, 307, 300, 341, 393, 312, 1612, 50764], "temperature": 0.0, "avg_logprob": -0.1240409179167314, "compression_ratio": 1.5360360360360361, "no_speech_prob": 1.8816117517417297e-05}, {"id": 168, "seek": 67100, "start": 679.0, "end": 683.0, "text": " as a stochastic differential equation. In this example", "tokens": [50764, 382, 257, 342, 8997, 2750, 15756, 5367, 13, 682, 341, 1365, 50964], "temperature": 0.0, "avg_logprob": -0.1240409179167314, "compression_ratio": 1.5360360360360361, "no_speech_prob": 1.8816117517417297e-05}, {"id": 169, "seek": 67100, "start": 683.0, "end": 687.0, "text": " it's about the simplest one we have. When we go forward in time", "tokens": [50964, 309, 311, 466, 264, 22811, 472, 321, 362, 13, 1133, 321, 352, 2128, 294, 565, 51164], "temperature": 0.0, "avg_logprob": -0.1240409179167314, "compression_ratio": 1.5360360360360361, "no_speech_prob": 1.8816117517417297e-05}, {"id": 170, "seek": 67100, "start": 687.0, "end": 691.0, "text": " over a very short time period", "tokens": [51164, 670, 257, 588, 2099, 565, 2896, 51364], "temperature": 0.0, "avg_logprob": -0.1240409179167314, "compression_ratio": 1.5360360360360361, "no_speech_prob": 1.8816117517417297e-05}, {"id": 171, "seek": 67100, "start": 691.0, "end": 695.0, "text": " the change in image dx equals the omega, which is", "tokens": [51364, 264, 1319, 294, 3256, 30017, 6915, 264, 10498, 11, 597, 307, 51564], "temperature": 0.0, "avg_logprob": -0.1240409179167314, "compression_ratio": 1.5360360360360361, "no_speech_prob": 1.8816117517417297e-05}, {"id": 172, "seek": 67100, "start": 695.0, "end": 699.0, "text": " white noise. So that's just a mathematical expression", "tokens": [51564, 2418, 5658, 13, 407, 300, 311, 445, 257, 18894, 6114, 51764], "temperature": 0.0, "avg_logprob": -0.1240409179167314, "compression_ratio": 1.5360360360360361, "no_speech_prob": 1.8816117517417297e-05}, {"id": 173, "seek": 69900, "start": 699.0, "end": 703.0, "text": " of doing cumulative sum of random noise.", "tokens": [50364, 295, 884, 38379, 2408, 295, 4974, 5658, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10618424666555304, "compression_ratio": 1.71875, "no_speech_prob": 7.579970406368375e-05}, {"id": 174, "seek": 69900, "start": 703.0, "end": 707.0, "text": " Now the magic is that to this forward equation", "tokens": [50564, 823, 264, 5585, 307, 300, 281, 341, 2128, 5367, 50764], "temperature": 0.0, "avg_logprob": -0.10618424666555304, "compression_ratio": 1.71875, "no_speech_prob": 7.579970406368375e-05}, {"id": 175, "seek": 69900, "start": 707.0, "end": 711.0, "text": " corresponds a backward version that has this same", "tokens": [50764, 23249, 257, 23897, 3037, 300, 575, 341, 912, 50964], "temperature": 0.0, "avg_logprob": -0.10618424666555304, "compression_ratio": 1.71875, "no_speech_prob": 7.579970406368375e-05}, {"id": 176, "seek": 69900, "start": 711.0, "end": 715.0, "text": " stochastic component, random walk component. But on top of that", "tokens": [50964, 342, 8997, 2750, 6542, 11, 4974, 1792, 6542, 13, 583, 322, 1192, 295, 300, 51164], "temperature": 0.0, "avg_logprob": -0.10618424666555304, "compression_ratio": 1.71875, "no_speech_prob": 7.579970406368375e-05}, {"id": 177, "seek": 69900, "start": 715.0, "end": 719.0, "text": " it has this term that kind of attracts the samples towards", "tokens": [51164, 309, 575, 341, 1433, 300, 733, 295, 37026, 264, 10938, 3030, 51364], "temperature": 0.0, "avg_logprob": -0.10618424666555304, "compression_ratio": 1.71875, "no_speech_prob": 7.579970406368375e-05}, {"id": 178, "seek": 69900, "start": 719.0, "end": 723.0, "text": " the data density. You see it's some kind of a gradient of the", "tokens": [51364, 264, 1412, 10305, 13, 509, 536, 309, 311, 512, 733, 295, 257, 16235, 295, 264, 51564], "temperature": 0.0, "avg_logprob": -0.10618424666555304, "compression_ratio": 1.71875, "no_speech_prob": 7.579970406368375e-05}, {"id": 179, "seek": 69900, "start": 723.0, "end": 727.0, "text": " density p. But the problem of course is that this p is unknown", "tokens": [51564, 10305, 280, 13, 583, 264, 1154, 295, 1164, 307, 300, 341, 280, 307, 9841, 51764], "temperature": 0.0, "avg_logprob": -0.10618424666555304, "compression_ratio": 1.71875, "no_speech_prob": 7.579970406368375e-05}, {"id": 180, "seek": 72700, "start": 727.0, "end": 731.0, "text": " and here is the axiomagic. This is a well known function", "tokens": [50364, 293, 510, 307, 264, 6360, 72, 298, 559, 299, 13, 639, 307, 257, 731, 2570, 2445, 50564], "temperature": 0.0, "avg_logprob": -0.12477033987812612, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.00023758155293762684}, {"id": 181, "seek": 72700, "start": 731.0, "end": 735.0, "text": " from previous literature in data science", "tokens": [50564, 490, 3894, 10394, 294, 1412, 3497, 50764], "temperature": 0.0, "avg_logprob": -0.12477033987812612, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.00023758155293762684}, {"id": 182, "seek": 72700, "start": 735.0, "end": 739.0, "text": " called the score function and it has the property", "tokens": [50764, 1219, 264, 6175, 2445, 293, 309, 575, 264, 4707, 50964], "temperature": 0.0, "avg_logprob": -0.12477033987812612, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.00023758155293762684}, {"id": 183, "seek": 72700, "start": 739.0, "end": 743.0, "text": " that you do not need to know the p if you have", "tokens": [50964, 300, 291, 360, 406, 643, 281, 458, 264, 280, 498, 291, 362, 51164], "temperature": 0.0, "avg_logprob": -0.12477033987812612, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.00023758155293762684}, {"id": 184, "seek": 72700, "start": 743.0, "end": 747.0, "text": " a least gross optimal denoiser for this data set d.", "tokens": [51164, 257, 1935, 11367, 16252, 1441, 78, 6694, 337, 341, 1412, 992, 274, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12477033987812612, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.00023758155293762684}, {"id": 185, "seek": 72700, "start": 747.0, "end": 751.0, "text": " So you can directly evaluate that formula above", "tokens": [51364, 407, 291, 393, 3838, 13059, 300, 8513, 3673, 51564], "temperature": 0.0, "avg_logprob": -0.12477033987812612, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.00023758155293762684}, {"id": 186, "seek": 72700, "start": 751.0, "end": 755.0, "text": " by the formula below. And this", "tokens": [51564, 538, 264, 8513, 2507, 13, 400, 341, 51764], "temperature": 0.0, "avg_logprob": -0.12477033987812612, "compression_ratio": 1.608910891089109, "no_speech_prob": 0.00023758155293762684}, {"id": 187, "seek": 75500, "start": 755.0, "end": 759.0, "text": " is an opportunity. We train a neural network to be such a denoiser", "tokens": [50364, 307, 364, 2650, 13, 492, 3847, 257, 18161, 3209, 281, 312, 1270, 257, 1441, 78, 6694, 50564], "temperature": 0.0, "avg_logprob": -0.11340488706316267, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.582687688525766e-05}, {"id": 188, "seek": 75500, "start": 759.0, "end": 763.0, "text": " and this means that we can run this kind of", "tokens": [50564, 293, 341, 1355, 300, 321, 393, 1190, 341, 733, 295, 50764], "temperature": 0.0, "avg_logprob": -0.11340488706316267, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.582687688525766e-05}, {"id": 189, "seek": 75500, "start": 763.0, "end": 767.0, "text": " backward equation evolution using that", "tokens": [50764, 23897, 5367, 9303, 1228, 300, 50964], "temperature": 0.0, "avg_logprob": -0.11340488706316267, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.582687688525766e-05}, {"id": 190, "seek": 75500, "start": 767.0, "end": 771.0, "text": " denoiser.", "tokens": [50964, 1441, 78, 6694, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11340488706316267, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.582687688525766e-05}, {"id": 191, "seek": 75500, "start": 771.0, "end": 775.0, "text": " So some colleagues also present this deterministic variant of this", "tokens": [51164, 407, 512, 7734, 611, 1974, 341, 15957, 3142, 17501, 295, 341, 51364], "temperature": 0.0, "avg_logprob": -0.11340488706316267, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.582687688525766e-05}, {"id": 192, "seek": 75500, "start": 775.0, "end": 779.0, "text": " where you don't have the stochastic term", "tokens": [51364, 689, 291, 500, 380, 362, 264, 342, 8997, 2750, 1433, 51564], "temperature": 0.0, "avg_logprob": -0.11340488706316267, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.582687688525766e-05}, {"id": 193, "seek": 75500, "start": 779.0, "end": 783.0, "text": " you only have this chord term scaled in some appropriate way.", "tokens": [51564, 291, 787, 362, 341, 14137, 1433, 36039, 294, 512, 6854, 636, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11340488706316267, "compression_ratio": 1.5893719806763285, "no_speech_prob": 4.582687688525766e-05}, {"id": 194, "seek": 78300, "start": 783.0, "end": 787.0, "text": " And this has a somewhat different like a visual character.", "tokens": [50364, 400, 341, 575, 257, 8344, 819, 411, 257, 5056, 2517, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13203416411409674, "compression_ratio": 1.5854700854700854, "no_speech_prob": 6.998058779572602e-06}, {"id": 195, "seek": 78300, "start": 787.0, "end": 791.0, "text": " You see it's kind of fading in and out instead of like", "tokens": [50564, 509, 536, 309, 311, 733, 295, 38644, 294, 293, 484, 2602, 295, 411, 50764], "temperature": 0.0, "avg_logprob": -0.13203416411409674, "compression_ratio": 1.5854700854700854, "no_speech_prob": 6.998058779572602e-06}, {"id": 196, "seek": 78300, "start": 791.0, "end": 795.0, "text": " jittering around. And this one actually provides", "tokens": [50764, 361, 3904, 278, 926, 13, 400, 341, 472, 767, 6417, 50964], "temperature": 0.0, "avg_logprob": -0.13203416411409674, "compression_ratio": 1.5854700854700854, "no_speech_prob": 6.998058779572602e-06}, {"id": 197, "seek": 78300, "start": 795.0, "end": 799.0, "text": " a much cleaner view into this sampling dynamic. So we'll be looking at", "tokens": [50964, 257, 709, 16532, 1910, 666, 341, 21179, 8546, 13, 407, 321, 603, 312, 1237, 412, 51164], "temperature": 0.0, "avg_logprob": -0.13203416411409674, "compression_ratio": 1.5854700854700854, "no_speech_prob": 6.998058779572602e-06}, {"id": 198, "seek": 78300, "start": 799.0, "end": 803.0, "text": " this first and then returning to the stochastic later.", "tokens": [51164, 341, 700, 293, 550, 12678, 281, 264, 342, 8997, 2750, 1780, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13203416411409674, "compression_ratio": 1.5854700854700854, "no_speech_prob": 6.998058779572602e-06}, {"id": 199, "seek": 78300, "start": 803.0, "end": 807.0, "text": " And with this I can now always draw this", "tokens": [51364, 400, 365, 341, 286, 393, 586, 1009, 2642, 341, 51564], "temperature": 0.0, "avg_logprob": -0.13203416411409674, "compression_ratio": 1.5854700854700854, "no_speech_prob": 6.998058779572602e-06}, {"id": 200, "seek": 78300, "start": 807.0, "end": 811.0, "text": " paint flow lines of this ODE. So the idea", "tokens": [51564, 4225, 3095, 3876, 295, 341, 422, 22296, 13, 407, 264, 1558, 51764], "temperature": 0.0, "avg_logprob": -0.13203416411409674, "compression_ratio": 1.5854700854700854, "no_speech_prob": 6.998058779572602e-06}, {"id": 201, "seek": 81100, "start": 811.0, "end": 815.0, "text": " is that we are trying to somehow follow these lines to do the generation.", "tokens": [50364, 307, 300, 321, 366, 1382, 281, 6063, 1524, 613, 3876, 281, 360, 264, 5125, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10723340380322802, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0003416419494897127}, {"id": 202, "seek": 81100, "start": 815.0, "end": 819.0, "text": " And indeed the way that happens is by discretization", "tokens": [50564, 400, 6451, 264, 636, 300, 2314, 307, 538, 25656, 2144, 50764], "temperature": 0.0, "avg_logprob": -0.10723340380322802, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0003416419494897127}, {"id": 203, "seek": 81100, "start": 819.0, "end": 823.0, "text": " I take little but macroscopic steps", "tokens": [50764, 286, 747, 707, 457, 7912, 38006, 299, 4439, 50964], "temperature": 0.0, "avg_logprob": -0.10723340380322802, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0003416419494897127}, {"id": 204, "seek": 81100, "start": 823.0, "end": 827.0, "text": " in this space I reduce the time and", "tokens": [50964, 294, 341, 1901, 286, 5407, 264, 565, 293, 51164], "temperature": 0.0, "avg_logprob": -0.10723340380322802, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0003416419494897127}, {"id": 205, "seek": 81100, "start": 827.0, "end": 831.0, "text": " for any change in time I want to jump. The ODE formula", "tokens": [51164, 337, 604, 1319, 294, 565, 286, 528, 281, 3012, 13, 440, 422, 22296, 8513, 51364], "temperature": 0.0, "avg_logprob": -0.10723340380322802, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0003416419494897127}, {"id": 206, "seek": 81100, "start": 831.0, "end": 835.0, "text": " tells me how much the image changes. And again", "tokens": [51364, 5112, 385, 577, 709, 264, 3256, 2962, 13, 400, 797, 51564], "temperature": 0.0, "avg_logprob": -0.10723340380322802, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0003416419494897127}, {"id": 207, "seek": 81100, "start": 835.0, "end": 839.0, "text": " the ODE formula is already does a neural network", "tokens": [51564, 264, 422, 22296, 8513, 307, 1217, 775, 257, 18161, 3209, 51764], "temperature": 0.0, "avg_logprob": -0.10723340380322802, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0003416419494897127}, {"id": 208, "seek": 83900, "start": 839.0, "end": 843.0, "text": " so the neural network tells us where to go on the next step.", "tokens": [50364, 370, 264, 18161, 3209, 5112, 505, 689, 281, 352, 322, 264, 958, 1823, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11567766359536955, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.5071717573155183e-06}, {"id": 209, "seek": 83900, "start": 843.0, "end": 847.0, "text": " That's the general idea. And that gives me a step.", "tokens": [50564, 663, 311, 264, 2674, 1558, 13, 400, 300, 2709, 385, 257, 1823, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11567766359536955, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.5071717573155183e-06}, {"id": 210, "seek": 83900, "start": 847.0, "end": 851.0, "text": " I keep stepping until I hit time zero and that's my generated sample.", "tokens": [50764, 286, 1066, 16821, 1826, 286, 2045, 565, 4018, 293, 300, 311, 452, 10833, 6889, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11567766359536955, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.5071717573155183e-06}, {"id": 211, "seek": 83900, "start": 851.0, "end": 855.0, "text": " With the SDEs we would have some kind of noise addition", "tokens": [50964, 2022, 264, 14638, 20442, 321, 576, 362, 512, 733, 295, 5658, 4500, 51164], "temperature": 0.0, "avg_logprob": -0.11567766359536955, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.5071717573155183e-06}, {"id": 212, "seek": 83900, "start": 855.0, "end": 859.0, "text": " on top of this so we would kind of jitter it", "tokens": [51164, 322, 1192, 295, 341, 370, 321, 576, 733, 295, 361, 3904, 309, 51364], "temperature": 0.0, "avg_logprob": -0.11567766359536955, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.5071717573155183e-06}, {"id": 213, "seek": 83900, "start": 859.0, "end": 863.0, "text": " but I said we'll leave that for later.", "tokens": [51364, 457, 286, 848, 321, 603, 1856, 300, 337, 1780, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11567766359536955, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.5071717573155183e-06}, {"id": 214, "seek": 83900, "start": 863.0, "end": 867.0, "text": " And now we've exactly reproduced this intuitive", "tokens": [51564, 400, 586, 321, 600, 2293, 11408, 1232, 341, 21769, 51764], "temperature": 0.0, "avg_logprob": -0.11567766359536955, "compression_ratio": 1.618421052631579, "no_speech_prob": 2.5071717573155183e-06}, {"id": 215, "seek": 86700, "start": 867.0, "end": 871.0, "text": " picture using differential equations.", "tokens": [50364, 3036, 1228, 15756, 11787, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12195951005686885, "compression_ratio": 1.5346938775510204, "no_speech_prob": 3.7248268199618906e-05}, {"id": 216, "seek": 86700, "start": 871.0, "end": 875.0, "text": " Okay so that was song and colleagues", "tokens": [50564, 1033, 370, 300, 390, 2153, 293, 7734, 50764], "temperature": 0.0, "avg_logprob": -0.12195951005686885, "compression_ratio": 1.5346938775510204, "no_speech_prob": 3.7248268199618906e-05}, {"id": 217, "seek": 86700, "start": 875.0, "end": 879.0, "text": " for our purposes. And let's now identify", "tokens": [50764, 337, 527, 9932, 13, 400, 718, 311, 586, 5876, 50964], "temperature": 0.0, "avg_logprob": -0.12195951005686885, "compression_ratio": 1.5346938775510204, "no_speech_prob": 3.7248268199618906e-05}, {"id": 218, "seek": 86700, "start": 879.0, "end": 883.0, "text": " some design choices involved in making this kind of an ODE or SD.", "tokens": [50964, 512, 1715, 7994, 3288, 294, 1455, 341, 733, 295, 364, 422, 22296, 420, 14638, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12195951005686885, "compression_ratio": 1.5346938775510204, "no_speech_prob": 3.7248268199618906e-05}, {"id": 219, "seek": 86700, "start": 883.0, "end": 887.0, "text": " But before we do that we should understand what can go wrong", "tokens": [51164, 583, 949, 321, 360, 300, 321, 820, 1223, 437, 393, 352, 2085, 51364], "temperature": 0.0, "avg_logprob": -0.12195951005686885, "compression_ratio": 1.5346938775510204, "no_speech_prob": 3.7248268199618906e-05}, {"id": 220, "seek": 86700, "start": 887.0, "end": 891.0, "text": " in this process. What are the error sources? Well the obvious one", "tokens": [51364, 294, 341, 1399, 13, 708, 366, 264, 6713, 7139, 30, 1042, 264, 6322, 472, 51564], "temperature": 0.0, "avg_logprob": -0.12195951005686885, "compression_ratio": 1.5346938775510204, "no_speech_prob": 3.7248268199618906e-05}, {"id": 221, "seek": 86700, "start": 891.0, "end": 895.0, "text": " because I might end up like in a different place than I should have", "tokens": [51564, 570, 286, 1062, 917, 493, 411, 294, 257, 819, 1081, 813, 286, 820, 362, 51764], "temperature": 0.0, "avg_logprob": -0.12195951005686885, "compression_ratio": 1.5346938775510204, "no_speech_prob": 3.7248268199618906e-05}, {"id": 222, "seek": 89500, "start": 895.0, "end": 899.0, "text": " when I do this sampling chain. So the obvious one is that if the network", "tokens": [50364, 562, 286, 360, 341, 21179, 5021, 13, 407, 264, 6322, 472, 307, 300, 498, 264, 3209, 50564], "temperature": 0.0, "avg_logprob": -0.07903456687927246, "compression_ratio": 1.6952789699570816, "no_speech_prob": 3.9753875171300024e-05}, {"id": 223, "seek": 89500, "start": 899.0, "end": 903.0, "text": " gives me an incorrect direction I end up moving in the incorrect", "tokens": [50564, 2709, 385, 364, 18424, 3513, 286, 917, 493, 2684, 294, 264, 18424, 50764], "temperature": 0.0, "avg_logprob": -0.07903456687927246, "compression_ratio": 1.6952789699570816, "no_speech_prob": 3.9753875171300024e-05}, {"id": 224, "seek": 89500, "start": 903.0, "end": 907.0, "text": " direction and in the end I end up somewhat in the wrong place.", "tokens": [50764, 3513, 293, 294, 264, 917, 286, 917, 493, 8344, 294, 264, 2085, 1081, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07903456687927246, "compression_ratio": 1.6952789699570816, "no_speech_prob": 3.9753875171300024e-05}, {"id": 225, "seek": 89500, "start": 907.0, "end": 911.0, "text": " It's more subtle than this but this is kind of a cartoon.", "tokens": [50964, 467, 311, 544, 13743, 813, 341, 457, 341, 307, 733, 295, 257, 18569, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07903456687927246, "compression_ratio": 1.6952789699570816, "no_speech_prob": 3.9753875171300024e-05}, {"id": 226, "seek": 89500, "start": 911.0, "end": 915.0, "text": " The other source of error is that we are trying to approximate this continuous", "tokens": [51164, 440, 661, 4009, 295, 6713, 307, 300, 321, 366, 1382, 281, 30874, 341, 10957, 51364], "temperature": 0.0, "avg_logprob": -0.07903456687927246, "compression_ratio": 1.6952789699570816, "no_speech_prob": 3.9753875171300024e-05}, {"id": 227, "seek": 89500, "start": 915.0, "end": 919.0, "text": " trajectory in green here using these linear", "tokens": [51364, 21512, 294, 3092, 510, 1228, 613, 8213, 51564], "temperature": 0.0, "avg_logprob": -0.07903456687927246, "compression_ratio": 1.6952789699570816, "no_speech_prob": 3.9753875171300024e-05}, {"id": 228, "seek": 89500, "start": 919.0, "end": 923.0, "text": " segments. And", "tokens": [51564, 19904, 13, 400, 51764], "temperature": 0.0, "avg_logprob": -0.07903456687927246, "compression_ratio": 1.6952789699570816, "no_speech_prob": 3.9753875171300024e-05}, {"id": 229, "seek": 92300, "start": 923.0, "end": 927.0, "text": " if I try to jump too far at once the curve will kind of", "tokens": [50364, 498, 286, 853, 281, 3012, 886, 1400, 412, 1564, 264, 7605, 486, 733, 295, 50564], "temperature": 0.0, "avg_logprob": -0.11230728030204773, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.5646181863266975e-05}, {"id": 230, "seek": 92300, "start": 927.0, "end": 931.0, "text": " move away from my feet and I'll end up veering off", "tokens": [50564, 1286, 1314, 490, 452, 3521, 293, 286, 603, 917, 493, 1241, 1794, 766, 50764], "temperature": 0.0, "avg_logprob": -0.11230728030204773, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.5646181863266975e-05}, {"id": 231, "seek": 92300, "start": 931.0, "end": 935.0, "text": " this path. It's of course familiar to anyone who's done like a physical simulation", "tokens": [50764, 341, 3100, 13, 467, 311, 295, 1164, 4963, 281, 2878, 567, 311, 1096, 411, 257, 4001, 16575, 50964], "temperature": 0.0, "avg_logprob": -0.11230728030204773, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.5646181863266975e-05}, {"id": 232, "seek": 92300, "start": 935.0, "end": 939.0, "text": " with ODE. And", "tokens": [50964, 365, 422, 22296, 13, 400, 51164], "temperature": 0.0, "avg_logprob": -0.11230728030204773, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.5646181863266975e-05}, {"id": 233, "seek": 92300, "start": 939.0, "end": 943.0, "text": " the proof of solutions to that is to take more steps", "tokens": [51164, 264, 8177, 295, 6547, 281, 300, 307, 281, 747, 544, 4439, 51364], "temperature": 0.0, "avg_logprob": -0.11230728030204773, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.5646181863266975e-05}, {"id": 234, "seek": 92300, "start": 943.0, "end": 947.0, "text": " but that's exactly what we want to avoid because that directly means", "tokens": [51364, 457, 300, 311, 2293, 437, 321, 528, 281, 5042, 570, 300, 3838, 1355, 51564], "temperature": 0.0, "avg_logprob": -0.11230728030204773, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.5646181863266975e-05}, {"id": 235, "seek": 92300, "start": 947.0, "end": 951.0, "text": " more compute to generate an image.", "tokens": [51564, 544, 14722, 281, 8460, 364, 3256, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11230728030204773, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.5646181863266975e-05}, {"id": 236, "seek": 95100, "start": 951.0, "end": 955.0, "text": " Okay and so what we argue and what is underappreciated in previous", "tokens": [50364, 1033, 293, 370, 437, 321, 9695, 293, 437, 307, 833, 1746, 3326, 770, 294, 3894, 50564], "temperature": 0.0, "avg_logprob": -0.09814315372043186, "compression_ratio": 1.7333333333333334, "no_speech_prob": 3.416368781472556e-05}, {"id": 237, "seek": 95100, "start": 955.0, "end": 959.0, "text": " work is that these two effects should be analysed", "tokens": [50564, 589, 307, 300, 613, 732, 5065, 820, 312, 23014, 292, 50764], "temperature": 0.0, "avg_logprob": -0.09814315372043186, "compression_ratio": 1.7333333333333334, "no_speech_prob": 3.416368781472556e-05}, {"id": 238, "seek": 95100, "start": 959.0, "end": 963.0, "text": " or can be and should be analysed in isolation.", "tokens": [50764, 420, 393, 312, 293, 820, 312, 23014, 292, 294, 16001, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09814315372043186, "compression_ratio": 1.7333333333333334, "no_speech_prob": 3.416368781472556e-05}, {"id": 239, "seek": 95100, "start": 963.0, "end": 967.0, "text": " You don't have to sample in a certain way just because you train your", "tokens": [50964, 509, 500, 380, 362, 281, 6889, 294, 257, 1629, 636, 445, 570, 291, 3847, 428, 51164], "temperature": 0.0, "avg_logprob": -0.09814315372043186, "compression_ratio": 1.7333333333333334, "no_speech_prob": 3.416368781472556e-05}, {"id": 240, "seek": 95100, "start": 967.0, "end": 971.0, "text": " network in a certain way and so on you can decouple this. And indeed", "tokens": [51164, 3209, 294, 257, 1629, 636, 293, 370, 322, 291, 393, 979, 263, 781, 341, 13, 400, 6451, 51364], "temperature": 0.0, "avg_logprob": -0.09814315372043186, "compression_ratio": 1.7333333333333334, "no_speech_prob": 3.416368781472556e-05}, {"id": 241, "seek": 95100, "start": 971.0, "end": 975.0, "text": " we'll be looking at sampling first and then coming back to the training later.", "tokens": [51364, 321, 603, 312, 1237, 412, 21179, 700, 293, 550, 1348, 646, 281, 264, 3097, 1780, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09814315372043186, "compression_ratio": 1.7333333333333334, "no_speech_prob": 3.416368781472556e-05}, {"id": 242, "seek": 95100, "start": 975.0, "end": 979.0, "text": " Okay so I promise to show you some", "tokens": [51564, 1033, 370, 286, 6228, 281, 855, 291, 512, 51764], "temperature": 0.0, "avg_logprob": -0.09814315372043186, "compression_ratio": 1.7333333333333334, "no_speech_prob": 3.416368781472556e-05}, {"id": 243, "seek": 97900, "start": 979.0, "end": 983.0, "text": " choices and here is one finally. So", "tokens": [50364, 7994, 293, 510, 307, 472, 2721, 13, 407, 50564], "temperature": 0.0, "avg_logprob": -0.13575482368469238, "compression_ratio": 1.6822033898305084, "no_speech_prob": 0.00011437421926530078}, {"id": 244, "seek": 97900, "start": 983.0, "end": 987.0, "text": " when I built this example I added noise at a constant rate", "tokens": [50564, 562, 286, 3094, 341, 1365, 286, 3869, 5658, 412, 257, 5754, 3314, 50764], "temperature": 0.0, "avg_logprob": -0.13575482368469238, "compression_ratio": 1.6822033898305084, "no_speech_prob": 0.00011437421926530078}, {"id": 245, "seek": 97900, "start": 987.0, "end": 991.0, "text": " over every time step and that gives me this simplicity, it gives me this schedule", "tokens": [50764, 670, 633, 565, 1823, 293, 300, 2709, 385, 341, 25632, 11, 309, 2709, 385, 341, 7567, 50964], "temperature": 0.0, "avg_logprob": -0.13575482368469238, "compression_ratio": 1.6822033898305084, "no_speech_prob": 0.00011437421926530078}, {"id": 246, "seek": 97900, "start": 991.0, "end": 995.0, "text": " where the noise level increases as a square root of time", "tokens": [50964, 689, 264, 5658, 1496, 8637, 382, 257, 3732, 5593, 295, 565, 51164], "temperature": 0.0, "avg_logprob": -0.13575482368469238, "compression_ratio": 1.6822033898305084, "no_speech_prob": 0.00011437421926530078}, {"id": 247, "seek": 97900, "start": 995.0, "end": 999.0, "text": " because that's how the variance will grow linearly so the standard", "tokens": [51164, 570, 300, 311, 577, 264, 21977, 486, 1852, 43586, 370, 264, 3832, 51364], "temperature": 0.0, "avg_logprob": -0.13575482368469238, "compression_ratio": 1.6822033898305084, "no_speech_prob": 0.00011437421926530078}, {"id": 248, "seek": 97900, "start": 999.0, "end": 1003.0, "text": " will go square root. That's what you get", "tokens": [51364, 486, 352, 3732, 5593, 13, 663, 311, 437, 291, 483, 51564], "temperature": 0.0, "avg_logprob": -0.13575482368469238, "compression_ratio": 1.6822033898305084, "no_speech_prob": 0.00011437421926530078}, {"id": 249, "seek": 97900, "start": 1003.0, "end": 1007.0, "text": " if you call random and then do a comp sum on top of it.", "tokens": [51564, 498, 291, 818, 4974, 293, 550, 360, 257, 715, 2408, 322, 1192, 295, 309, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13575482368469238, "compression_ratio": 1.6822033898305084, "no_speech_prob": 0.00011437421926530078}, {"id": 250, "seek": 100700, "start": 1007.0, "end": 1011.0, "text": " Had I added it at a different rate I might have arrived at a schedule", "tokens": [50364, 12298, 286, 3869, 309, 412, 257, 819, 3314, 286, 1062, 362, 6678, 412, 257, 7567, 50564], "temperature": 0.0, "avg_logprob": -0.21751930690047763, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.9179266018909402e-05}, {"id": 251, "seek": 100700, "start": 1011.0, "end": 1015.0, "text": " like this for example where the standard deviation is the gross linearity", "tokens": [50564, 411, 341, 337, 1365, 689, 264, 3832, 25163, 307, 264, 11367, 8213, 507, 50764], "temperature": 0.0, "avg_logprob": -0.21751930690047763, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.9179266018909402e-05}, {"id": 252, "seek": 100700, "start": 1015.0, "end": 1019.0, "text": " and indeed I could do any", "tokens": [50764, 293, 6451, 286, 727, 360, 604, 50964], "temperature": 0.0, "avg_logprob": -0.21751930690047763, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.9179266018909402e-05}, {"id": 253, "seek": 100700, "start": 1019.0, "end": 1023.0, "text": " kind of a choice here. I could do something even something", "tokens": [50964, 733, 295, 257, 3922, 510, 13, 286, 727, 360, 746, 754, 746, 51164], "temperature": 0.0, "avg_logprob": -0.21751930690047763, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.9179266018909402e-05}, {"id": 254, "seek": 100700, "start": 1023.0, "end": 1027.0, "text": " crazy like this way we schedule here in the middle if I wanted to", "tokens": [51164, 3219, 411, 341, 636, 321, 7567, 510, 294, 264, 2808, 498, 286, 1415, 281, 51364], "temperature": 0.0, "avg_logprob": -0.21751930690047763, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.9179266018909402e-05}, {"id": 255, "seek": 100700, "start": 1027.0, "end": 1031.0, "text": " for some reason. And indeed we generalise in the paper the", "tokens": [51364, 337, 512, 1778, 13, 400, 6451, 321, 2674, 908, 294, 264, 3035, 264, 51564], "temperature": 0.0, "avg_logprob": -0.21751930690047763, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.9179266018909402e-05}, {"id": 256, "seek": 100700, "start": 1031.0, "end": 1035.0, "text": " ODE form or we reparametise it in such", "tokens": [51564, 422, 22296, 1254, 420, 321, 319, 2181, 49083, 908, 309, 294, 1270, 51764], "temperature": 0.0, "avg_logprob": -0.21751930690047763, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.9179266018909402e-05}, {"id": 257, "seek": 103500, "start": 1035.0, "end": 1039.0, "text": " a way that we get a clear view into these effects.", "tokens": [50364, 257, 636, 300, 321, 483, 257, 1850, 1910, 666, 613, 5065, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12177320273525744, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.7226780983037315e-05}, {"id": 258, "seek": 103500, "start": 1039.0, "end": 1043.0, "text": " So we can parameterise the noise level we want to have reached", "tokens": [50564, 407, 321, 393, 13075, 908, 264, 5658, 1496, 321, 528, 281, 362, 6488, 50764], "temperature": 0.0, "avg_logprob": -0.12177320273525744, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.7226780983037315e-05}, {"id": 259, "seek": 103500, "start": 1043.0, "end": 1047.0, "text": " by explicitly by this function sigma.", "tokens": [50764, 538, 20803, 538, 341, 2445, 12771, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12177320273525744, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.7226780983037315e-05}, {"id": 260, "seek": 103500, "start": 1051.0, "end": 1055.0, "text": " But the real question is why would you want to do something like this.", "tokens": [51164, 583, 264, 957, 1168, 307, 983, 576, 291, 528, 281, 360, 746, 411, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12177320273525744, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.7226780983037315e-05}, {"id": 261, "seek": 103500, "start": 1055.0, "end": 1059.0, "text": " Well one reason for that could be that if you look at this picture for example", "tokens": [51364, 1042, 472, 1778, 337, 300, 727, 312, 300, 498, 291, 574, 412, 341, 3036, 337, 1365, 51564], "temperature": 0.0, "avg_logprob": -0.12177320273525744, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.7226780983037315e-05}, {"id": 262, "seek": 103500, "start": 1059.0, "end": 1063.0, "text": " you see almost nothing happens until", "tokens": [51564, 291, 536, 1920, 1825, 2314, 1826, 51764], "temperature": 0.0, "avg_logprob": -0.12177320273525744, "compression_ratio": 1.6095238095238096, "no_speech_prob": 1.7226780983037315e-05}, {"id": 263, "seek": 106300, "start": 1063.0, "end": 1067.0, "text": " at almost zero noise level suddenly curves rapidly", "tokens": [50364, 412, 1920, 4018, 5658, 1496, 5800, 19490, 12910, 50564], "temperature": 0.0, "avg_logprob": -0.15982013501619038, "compression_ratio": 1.7174887892376682, "no_speech_prob": 4.7734607505844906e-05}, {"id": 264, "seek": 106300, "start": 1067.0, "end": 1071.0, "text": " to one of these two basins and", "tokens": [50564, 281, 472, 295, 613, 732, 987, 1292, 293, 50764], "temperature": 0.0, "avg_logprob": -0.15982013501619038, "compression_ratio": 1.7174887892376682, "no_speech_prob": 4.7734607505844906e-05}, {"id": 265, "seek": 106300, "start": 1071.0, "end": 1075.0, "text": " there's high curvature there so we'd probably want to be careful", "tokens": [50764, 456, 311, 1090, 37638, 456, 370, 321, 1116, 1391, 528, 281, 312, 5026, 50964], "temperature": 0.0, "avg_logprob": -0.15982013501619038, "compression_ratio": 1.7174887892376682, "no_speech_prob": 4.7734607505844906e-05}, {"id": 266, "seek": 106300, "start": 1075.0, "end": 1079.0, "text": " in stepping. We'll want to take somehow be more careful in sampling", "tokens": [50964, 294, 16821, 13, 492, 603, 528, 281, 747, 6063, 312, 544, 5026, 294, 21179, 51164], "temperature": 0.0, "avg_logprob": -0.15982013501619038, "compression_ratio": 1.7174887892376682, "no_speech_prob": 4.7734607505844906e-05}, {"id": 267, "seek": 106300, "start": 1079.0, "end": 1083.0, "text": " that region and less careful you're in the bulk. So there's two ideas", "tokens": [51164, 300, 4458, 293, 1570, 5026, 291, 434, 294, 264, 16139, 13, 407, 456, 311, 732, 3487, 51364], "temperature": 0.0, "avg_logprob": -0.15982013501619038, "compression_ratio": 1.7174887892376682, "no_speech_prob": 4.7734607505844906e-05}, {"id": 268, "seek": 106300, "start": 1083.0, "end": 1087.0, "text": " of how you might do that. You might", "tokens": [51364, 295, 577, 291, 1062, 360, 300, 13, 509, 1062, 51564], "temperature": 0.0, "avg_logprob": -0.15982013501619038, "compression_ratio": 1.7174887892376682, "no_speech_prob": 4.7734607505844906e-05}, {"id": 269, "seek": 106300, "start": 1087.0, "end": 1091.0, "text": " first you might take shorter steps at the more difficult parts", "tokens": [51564, 700, 291, 1062, 747, 11639, 4439, 412, 264, 544, 2252, 3166, 51764], "temperature": 0.0, "avg_logprob": -0.15982013501619038, "compression_ratio": 1.7174887892376682, "no_speech_prob": 4.7734607505844906e-05}, {"id": 270, "seek": 109100, "start": 1091.0, "end": 1095.0, "text": " usually is the low noise levels because that's where the", "tokens": [50364, 2673, 307, 264, 2295, 5658, 4358, 570, 300, 311, 689, 264, 50564], "temperature": 0.0, "avg_logprob": -0.10911955458394597, "compression_ratio": 1.6351931330472103, "no_speech_prob": 2.8675603971350938e-05}, {"id": 271, "seek": 109100, "start": 1095.0, "end": 1099.0, "text": " image details are usually built. The other alternative", "tokens": [50564, 3256, 4365, 366, 2673, 3094, 13, 440, 661, 8535, 50764], "temperature": 0.0, "avg_logprob": -0.10911955458394597, "compression_ratio": 1.6351931330472103, "no_speech_prob": 2.8675603971350938e-05}, {"id": 272, "seek": 109100, "start": 1099.0, "end": 1103.0, "text": " would be to instead warp the noise schedule in such a way", "tokens": [50764, 576, 312, 281, 2602, 36030, 264, 5658, 7567, 294, 1270, 257, 636, 50964], "temperature": 0.0, "avg_logprob": -0.10911955458394597, "compression_ratio": 1.6351931330472103, "no_speech_prob": 2.8675603971350938e-05}, {"id": 273, "seek": 109100, "start": 1103.0, "end": 1107.0, "text": " that you just end up spending more time at these difficult parts.", "tokens": [50964, 300, 291, 445, 917, 493, 6434, 544, 565, 412, 613, 2252, 3166, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10911955458394597, "compression_ratio": 1.6351931330472103, "no_speech_prob": 2.8675603971350938e-05}, {"id": 274, "seek": 109100, "start": 1107.0, "end": 1111.0, "text": " And it's tempting to think", "tokens": [51164, 400, 309, 311, 37900, 281, 519, 51364], "temperature": 0.0, "avg_logprob": -0.10911955458394597, "compression_ratio": 1.6351931330472103, "no_speech_prob": 2.8675603971350938e-05}, {"id": 275, "seek": 109100, "start": 1111.0, "end": 1115.0, "text": " that these two approaches would be equivalent.", "tokens": [51364, 300, 613, 732, 11587, 576, 312, 10344, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10911955458394597, "compression_ratio": 1.6351931330472103, "no_speech_prob": 2.8675603971350938e-05}, {"id": 276, "seek": 109100, "start": 1115.0, "end": 1119.0, "text": " And this is an implicit assumption I think that many previous works do.", "tokens": [51564, 400, 341, 307, 364, 26947, 15302, 286, 519, 300, 867, 3894, 1985, 360, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10911955458394597, "compression_ratio": 1.6351931330472103, "no_speech_prob": 2.8675603971350938e-05}, {"id": 277, "seek": 111900, "start": 1119.0, "end": 1123.0, "text": " But this is simply not true because the error characteristics", "tokens": [50364, 583, 341, 307, 2935, 406, 2074, 570, 264, 6713, 10891, 50564], "temperature": 0.0, "avg_logprob": -0.12072124251400132, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.441705121076666e-05}, {"id": 278, "seek": 111900, "start": 1123.0, "end": 1127.0, "text": " can be vastly different between these choices like the error that comes", "tokens": [50564, 393, 312, 41426, 819, 1296, 613, 7994, 411, 264, 6713, 300, 1487, 50764], "temperature": 0.0, "avg_logprob": -0.12072124251400132, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.441705121076666e-05}, {"id": 279, "seek": 111900, "start": 1127.0, "end": 1131.0, "text": " from this tracking this continuous curve", "tokens": [50764, 490, 341, 11603, 341, 10957, 7605, 50964], "temperature": 0.0, "avg_logprob": -0.12072124251400132, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.441705121076666e-05}, {"id": 280, "seek": 111900, "start": 1131.0, "end": 1135.0, "text": " and we'll see the effect of that later.", "tokens": [50964, 293, 321, 603, 536, 264, 1802, 295, 300, 1780, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12072124251400132, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.441705121076666e-05}, {"id": 281, "seek": 111900, "start": 1135.0, "end": 1139.0, "text": " So now we've identified the first pair of design choices here.", "tokens": [51164, 407, 586, 321, 600, 9234, 264, 700, 6119, 295, 1715, 7994, 510, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12072124251400132, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.441705121076666e-05}, {"id": 282, "seek": 111900, "start": 1139.0, "end": 1143.0, "text": " The time steps and the noise schedule.", "tokens": [51364, 440, 565, 4439, 293, 264, 5658, 7567, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12072124251400132, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.441705121076666e-05}, {"id": 283, "seek": 111900, "start": 1143.0, "end": 1147.0, "text": " But let's introduce a couple more.", "tokens": [51564, 583, 718, 311, 5366, 257, 1916, 544, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12072124251400132, "compression_ratio": 1.588235294117647, "no_speech_prob": 3.441705121076666e-05}, {"id": 284, "seek": 114700, "start": 1147.0, "end": 1151.0, "text": " And this address is the following problem. I zoom out a little", "tokens": [50364, 400, 341, 2985, 307, 264, 3480, 1154, 13, 286, 8863, 484, 257, 707, 50564], "temperature": 0.0, "avg_logprob": -0.12827605501227424, "compression_ratio": 1.6890756302521008, "no_speech_prob": 0.00011955865193158388}, {"id": 285, "seek": 114700, "start": 1151.0, "end": 1155.0, "text": " because in reality we add a ton of noise. So at the", "tokens": [50564, 570, 294, 4103, 321, 909, 257, 2952, 295, 5658, 13, 407, 412, 264, 50764], "temperature": 0.0, "avg_logprob": -0.12827605501227424, "compression_ratio": 1.6890756302521008, "no_speech_prob": 0.00011955865193158388}, {"id": 286, "seek": 114700, "start": 1155.0, "end": 1159.0, "text": " other extreme the noise level is very large. I've been showing this zoom in", "tokens": [50764, 661, 8084, 264, 5658, 1496, 307, 588, 2416, 13, 286, 600, 668, 4099, 341, 8863, 294, 50964], "temperature": 0.0, "avg_logprob": -0.12827605501227424, "compression_ratio": 1.6890756302521008, "no_speech_prob": 0.00011955865193158388}, {"id": 287, "seek": 114700, "start": 1159.0, "end": 1163.0, "text": " so we can easier see what's going on. But I zoomed out", "tokens": [50964, 370, 321, 393, 3571, 536, 437, 311, 516, 322, 13, 583, 286, 8863, 292, 484, 51164], "temperature": 0.0, "avg_logprob": -0.12827605501227424, "compression_ratio": 1.6890756302521008, "no_speech_prob": 0.00011955865193158388}, {"id": 288, "seek": 114700, "start": 1163.0, "end": 1167.0, "text": " now to see what's here. So the issue", "tokens": [51164, 586, 281, 536, 437, 311, 510, 13, 407, 264, 2734, 51364], "temperature": 0.0, "avg_logprob": -0.12827605501227424, "compression_ratio": 1.6890756302521008, "no_speech_prob": 0.00011955865193158388}, {"id": 289, "seek": 114700, "start": 1167.0, "end": 1171.0, "text": " if you don't do anything is that the signal magnitude grows", "tokens": [51364, 498, 291, 500, 380, 360, 1340, 307, 300, 264, 6358, 15668, 13156, 51564], "temperature": 0.0, "avg_logprob": -0.12827605501227424, "compression_ratio": 1.6890756302521008, "no_speech_prob": 0.00011955865193158388}, {"id": 290, "seek": 114700, "start": 1171.0, "end": 1175.0, "text": " as the noise level grows. You keep piling noise. The signal", "tokens": [51564, 382, 264, 5658, 1496, 13156, 13, 509, 1066, 280, 4883, 5658, 13, 440, 6358, 51764], "temperature": 0.0, "avg_logprob": -0.12827605501227424, "compression_ratio": 1.6890756302521008, "no_speech_prob": 0.00011955865193158388}, {"id": 291, "seek": 117500, "start": 1175.0, "end": 1179.0, "text": " is quite simply bigger numerically like the values that", "tokens": [50364, 307, 1596, 2935, 3801, 7866, 984, 411, 264, 4190, 300, 50564], "temperature": 0.0, "avg_logprob": -0.11137808362642924, "compression_ratio": 1.6704980842911878, "no_speech_prob": 3.50633381458465e-05}, {"id": 292, "seek": 117500, "start": 1179.0, "end": 1183.0, "text": " are in your sensor. They are much larger at the high noise levels", "tokens": [50564, 366, 294, 428, 10200, 13, 814, 366, 709, 4833, 412, 264, 1090, 5658, 4358, 50764], "temperature": 0.0, "avg_logprob": -0.11137808362642924, "compression_ratio": 1.6704980842911878, "no_speech_prob": 3.50633381458465e-05}, {"id": 293, "seek": 117500, "start": 1183.0, "end": 1187.0, "text": " than in the low noise levels. And this is known to be really bad for neural network", "tokens": [50764, 813, 294, 264, 2295, 5658, 4358, 13, 400, 341, 307, 2570, 281, 312, 534, 1578, 337, 18161, 3209, 50964], "temperature": 0.0, "avg_logprob": -0.11137808362642924, "compression_ratio": 1.6704980842911878, "no_speech_prob": 3.50633381458465e-05}, {"id": 294, "seek": 117500, "start": 1187.0, "end": 1191.0, "text": " training dynamics. And these kind of", "tokens": [50964, 3097, 15679, 13, 400, 613, 733, 295, 51164], "temperature": 0.0, "avg_logprob": -0.11137808362642924, "compression_ratio": 1.6704980842911878, "no_speech_prob": 3.50633381458465e-05}, {"id": 295, "seek": 117500, "start": 1191.0, "end": 1195.0, "text": " effects are actually critical to deal with to get good performance.", "tokens": [51164, 5065, 366, 767, 4924, 281, 2028, 365, 281, 483, 665, 3389, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11137808362642924, "compression_ratio": 1.6704980842911878, "no_speech_prob": 3.50633381458465e-05}, {"id": 296, "seek": 117500, "start": 1195.0, "end": 1199.0, "text": " So the way many previous works approach this is by using", "tokens": [51364, 407, 264, 636, 867, 3894, 1985, 3109, 341, 307, 538, 1228, 51564], "temperature": 0.0, "avg_logprob": -0.11137808362642924, "compression_ratio": 1.6704980842911878, "no_speech_prob": 3.50633381458465e-05}, {"id": 297, "seek": 117500, "start": 1199.0, "end": 1203.0, "text": " something called variance preserving schedules where you effectively", "tokens": [51564, 746, 1219, 21977, 33173, 28078, 689, 291, 8659, 51764], "temperature": 0.0, "avg_logprob": -0.11137808362642924, "compression_ratio": 1.6704980842911878, "no_speech_prob": 3.50633381458465e-05}, {"id": 298, "seek": 120300, "start": 1203.0, "end": 1207.0, "text": " introduce this additional", "tokens": [50364, 5366, 341, 4497, 50564], "temperature": 0.0, "avg_logprob": -0.14922027311463287, "compression_ratio": 1.5240641711229947, "no_speech_prob": 6.268620199989527e-05}, {"id": 299, "seek": 120300, "start": 1207.0, "end": 1211.0, "text": " so called scale schedule where you squeeze", "tokens": [50564, 370, 1219, 4373, 7567, 689, 291, 13578, 50764], "temperature": 0.0, "avg_logprob": -0.14922027311463287, "compression_ratio": 1.5240641711229947, "no_speech_prob": 6.268620199989527e-05}, {"id": 300, "seek": 120300, "start": 1211.0, "end": 1215.0, "text": " the signal magnitude into this constant with constant", "tokens": [50764, 264, 6358, 15668, 666, 341, 5754, 365, 5754, 50964], "temperature": 0.0, "avg_logprob": -0.14922027311463287, "compression_ratio": 1.5240641711229947, "no_speech_prob": 6.268620199989527e-05}, {"id": 301, "seek": 120300, "start": 1215.0, "end": 1219.0, "text": " variance tube. So that makes", "tokens": [50964, 21977, 9917, 13, 407, 300, 1669, 51164], "temperature": 0.0, "avg_logprob": -0.14922027311463287, "compression_ratio": 1.5240641711229947, "no_speech_prob": 6.268620199989527e-05}, {"id": 302, "seek": 120300, "start": 1219.0, "end": 1223.0, "text": " that's one way to make the network happy here.", "tokens": [51164, 300, 311, 472, 636, 281, 652, 264, 3209, 2055, 510, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14922027311463287, "compression_ratio": 1.5240641711229947, "no_speech_prob": 6.268620199989527e-05}, {"id": 303, "seek": 120300, "start": 1223.0, "end": 1227.0, "text": " So we generalize this idea again by", "tokens": [51364, 407, 321, 2674, 1125, 341, 1558, 797, 538, 51564], "temperature": 0.0, "avg_logprob": -0.14922027311463287, "compression_ratio": 1.5240641711229947, "no_speech_prob": 6.268620199989527e-05}, {"id": 304, "seek": 120300, "start": 1227.0, "end": 1231.0, "text": " just formulating an OD that allows you to directly", "tokens": [51564, 445, 1254, 12162, 364, 48447, 300, 4045, 291, 281, 3838, 51764], "temperature": 0.0, "avg_logprob": -0.14922027311463287, "compression_ratio": 1.5240641711229947, "no_speech_prob": 6.268620199989527e-05}, {"id": 305, "seek": 123100, "start": 1231.0, "end": 1235.0, "text": " specify any arbitrary scale schedule. And viewing this slide", "tokens": [50364, 16500, 604, 23211, 4373, 7567, 13, 400, 17480, 341, 4137, 50564], "temperature": 0.0, "avg_logprob": -0.15421027722566025, "compression_ratio": 1.6266094420600858, "no_speech_prob": 4.905572495772503e-05}, {"id": 306, "seek": 123100, "start": 1235.0, "end": 1239.0, "text": " it again becomes appropriate that the only thing that the scale schedule does", "tokens": [50564, 309, 797, 3643, 6854, 300, 264, 787, 551, 300, 264, 4373, 7567, 775, 50764], "temperature": 0.0, "avg_logprob": -0.15421027722566025, "compression_ratio": 1.6266094420600858, "no_speech_prob": 4.905572495772503e-05}, {"id": 307, "seek": 123100, "start": 1239.0, "end": 1243.0, "text": " is distort these flow lines in some way.", "tokens": [50764, 307, 37555, 613, 3095, 3876, 294, 512, 636, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15421027722566025, "compression_ratio": 1.6266094420600858, "no_speech_prob": 4.905572495772503e-05}, {"id": 308, "seek": 123100, "start": 1243.0, "end": 1247.0, "text": " So you are just doing a coordinate transform in a way on this XT plane.", "tokens": [50964, 407, 291, 366, 445, 884, 257, 15670, 4088, 294, 257, 636, 322, 341, 1783, 51, 5720, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15421027722566025, "compression_ratio": 1.6266094420600858, "no_speech_prob": 4.905572495772503e-05}, {"id": 309, "seek": 123100, "start": 1247.0, "end": 1251.0, "text": " Now there is an alternative", "tokens": [51164, 823, 456, 307, 364, 8535, 51364], "temperature": 0.0, "avg_logprob": -0.15421027722566025, "compression_ratio": 1.6266094420600858, "no_speech_prob": 4.905572495772503e-05}, {"id": 310, "seek": 123100, "start": 1251.0, "end": 1255.0, "text": " way to deal with this scaling issue. And it is quite", "tokens": [51364, 636, 281, 2028, 365, 341, 21589, 2734, 13, 400, 309, 307, 1596, 51564], "temperature": 0.0, "avg_logprob": -0.15421027722566025, "compression_ratio": 1.6266094420600858, "no_speech_prob": 4.905572495772503e-05}, {"id": 311, "seek": 123100, "start": 1255.0, "end": 1259.0, "text": " simply this. Instead of changing the OD at all", "tokens": [51564, 2935, 341, 13, 7156, 295, 4473, 264, 48447, 412, 439, 51764], "temperature": 0.0, "avg_logprob": -0.15421027722566025, "compression_ratio": 1.6266094420600858, "no_speech_prob": 4.905572495772503e-05}, {"id": 312, "seek": 125900, "start": 1259.0, "end": 1263.0, "text": " you could change your neural network in such a way that it has an initial scaling", "tokens": [50364, 291, 727, 1319, 428, 18161, 3209, 294, 1270, 257, 636, 300, 309, 575, 364, 5883, 21589, 50564], "temperature": 0.0, "avg_logprob": -0.12286275686676969, "compression_ratio": 1.6574803149606299, "no_speech_prob": 3.991952689830214e-05}, {"id": 313, "seek": 125900, "start": 1263.0, "end": 1267.0, "text": " layer that uses the known single scale", "tokens": [50564, 4583, 300, 4960, 264, 2570, 2167, 4373, 50764], "temperature": 0.0, "avg_logprob": -0.12286275686676969, "compression_ratio": 1.6574803149606299, "no_speech_prob": 3.991952689830214e-05}, {"id": 314, "seek": 125900, "start": 1267.0, "end": 1271.0, "text": " scale it to something that's palatable for the neural network.", "tokens": [50764, 4373, 309, 281, 746, 300, 311, 3984, 31415, 337, 264, 18161, 3209, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12286275686676969, "compression_ratio": 1.6574803149606299, "no_speech_prob": 3.991952689830214e-05}, {"id": 315, "seek": 125900, "start": 1271.0, "end": 1275.0, "text": " And again you might think that this is completely", "tokens": [50964, 400, 797, 291, 1062, 519, 300, 341, 307, 2584, 51164], "temperature": 0.0, "avg_logprob": -0.12286275686676969, "compression_ratio": 1.6574803149606299, "no_speech_prob": 3.991952689830214e-05}, {"id": 316, "seek": 125900, "start": 1275.0, "end": 1279.0, "text": " accurate with the OD, but this is simply not true because again the error characteristics", "tokens": [51164, 8559, 365, 264, 48447, 11, 457, 341, 307, 2935, 406, 2074, 570, 797, 264, 6713, 10891, 51364], "temperature": 0.0, "avg_logprob": -0.12286275686676969, "compression_ratio": 1.6574803149606299, "no_speech_prob": 3.991952689830214e-05}, {"id": 317, "seek": 125900, "start": 1279.0, "end": 1283.0, "text": " are vastly different between these two cases. And I'll come", "tokens": [51364, 366, 41426, 819, 1296, 613, 732, 3331, 13, 400, 286, 603, 808, 51564], "temperature": 0.0, "avg_logprob": -0.12286275686676969, "compression_ratio": 1.6574803149606299, "no_speech_prob": 3.991952689830214e-05}, {"id": 318, "seek": 125900, "start": 1283.0, "end": 1287.0, "text": " back soon to how the chosen practice.", "tokens": [51564, 646, 2321, 281, 577, 264, 8614, 3124, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12286275686676969, "compression_ratio": 1.6574803149606299, "no_speech_prob": 3.991952689830214e-05}, {"id": 319, "seek": 128700, "start": 1287.0, "end": 1291.0, "text": " But now we've identified a second set, second pair of these matrices.", "tokens": [50364, 583, 586, 321, 600, 9234, 257, 1150, 992, 11, 1150, 6119, 295, 613, 32284, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18952081420204855, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00010664055298548192}, {"id": 320, "seek": 128700, "start": 1291.0, "end": 1295.0, "text": " The scaling schedule and the", "tokens": [50564, 440, 21589, 7567, 293, 264, 50764], "temperature": 0.0, "avg_logprob": -0.18952081420204855, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00010664055298548192}, {"id": 321, "seek": 128700, "start": 1295.0, "end": 1299.0, "text": " scaling that happens inside the neural network itself.", "tokens": [50764, 21589, 300, 2314, 1854, 264, 18161, 3209, 2564, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18952081420204855, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00010664055298548192}, {"id": 322, "seek": 128700, "start": 1299.0, "end": 1303.0, "text": " And that we kind of saw so-called preconditioning of the neural network.", "tokens": [50964, 400, 300, 321, 733, 295, 1866, 370, 12, 11880, 4346, 684, 849, 278, 295, 264, 18161, 3209, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18952081420204855, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00010664055298548192}, {"id": 323, "seek": 128700, "start": 1303.0, "end": 1307.0, "text": " Okay, so now we have quite a few", "tokens": [51164, 1033, 11, 370, 586, 321, 362, 1596, 257, 1326, 51364], "temperature": 0.0, "avg_logprob": -0.18952081420204855, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00010664055298548192}, {"id": 324, "seek": 128700, "start": 1307.0, "end": 1311.0, "text": " collected here. And", "tokens": [51364, 11087, 510, 13, 400, 51564], "temperature": 0.0, "avg_logprob": -0.18952081420204855, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00010664055298548192}, {"id": 325, "seek": 128700, "start": 1311.0, "end": 1315.0, "text": " at this point we can ask, like get our hands dirty", "tokens": [51564, 412, 341, 935, 321, 393, 1029, 11, 411, 483, 527, 2377, 9360, 51764], "temperature": 0.0, "avg_logprob": -0.18952081420204855, "compression_ratio": 1.5942028985507246, "no_speech_prob": 0.00010664055298548192}, {"id": 326, "seek": 131500, "start": 1315.0, "end": 1319.0, "text": " go look at the appendices of these papers, read their code", "tokens": [50364, 352, 574, 412, 264, 34116, 1473, 295, 613, 10577, 11, 1401, 641, 3089, 50564], "temperature": 0.0, "avg_logprob": -0.12639325272803212, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00034832744859158993}, {"id": 327, "seek": 131500, "start": 1319.0, "end": 1323.0, "text": " for the final ground truth and ask what", "tokens": [50564, 337, 264, 2572, 2727, 3494, 293, 1029, 437, 50764], "temperature": 0.0, "avg_logprob": -0.12639325272803212, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00034832744859158993}, {"id": 328, "seek": 131500, "start": 1323.0, "end": 1327.0, "text": " formulas actually exactly reproduce their", "tokens": [50764, 30546, 767, 2293, 29501, 641, 50964], "temperature": 0.0, "avg_logprob": -0.12639325272803212, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00034832744859158993}, {"id": 329, "seek": 131500, "start": 1327.0, "end": 1331.0, "text": " approaches. And they are these. Again don't worry, but don't even", "tokens": [50964, 11587, 13, 400, 436, 366, 613, 13, 3764, 500, 380, 3292, 11, 457, 500, 380, 754, 51164], "temperature": 0.0, "avg_logprob": -0.12639325272803212, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00034832744859158993}, {"id": 330, "seek": 131500, "start": 1331.0, "end": 1335.0, "text": " try to read them. But the question now is", "tokens": [51164, 853, 281, 1401, 552, 13, 583, 264, 1168, 586, 307, 51364], "temperature": 0.0, "avg_logprob": -0.12639325272803212, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00034832744859158993}, {"id": 331, "seek": 131500, "start": 1335.0, "end": 1339.0, "text": " what choices should we actually make, which ones of these are good, which ones are", "tokens": [51364, 437, 7994, 820, 321, 767, 652, 11, 597, 2306, 295, 613, 366, 665, 11, 597, 2306, 366, 51564], "temperature": 0.0, "avg_logprob": -0.12639325272803212, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00034832744859158993}, {"id": 332, "seek": 131500, "start": 1339.0, "end": 1343.0, "text": " suboptimal. And that's going to be the topic of the next section.", "tokens": [51564, 1422, 5747, 10650, 13, 400, 300, 311, 516, 281, 312, 264, 4829, 295, 264, 958, 3541, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12639325272803212, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00034832744859158993}, {"id": 333, "seek": 134300, "start": 1343.0, "end": 1347.0, "text": " And for now, we will be ignoring these neural network training", "tokens": [50364, 400, 337, 586, 11, 321, 486, 312, 26258, 613, 18161, 3209, 3097, 50564], "temperature": 0.0, "avg_logprob": -0.12719994670939896, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.00032606537570245564}, {"id": 334, "seek": 134300, "start": 1347.0, "end": 1351.0, "text": " aspects. We will be using pre-trained networks from previous work.", "tokens": [50564, 7270, 13, 492, 486, 312, 1228, 659, 12, 17227, 2001, 9590, 490, 3894, 589, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12719994670939896, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.00032606537570245564}, {"id": 335, "seek": 134300, "start": 1351.0, "end": 1355.0, "text": " We won't be retraining anything yet. We'll just try to improve the sampling.", "tokens": [50764, 492, 1582, 380, 312, 49356, 1760, 1340, 1939, 13, 492, 603, 445, 853, 281, 3470, 264, 21179, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12719994670939896, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.00032606537570245564}, {"id": 336, "seek": 134300, "start": 1355.0, "end": 1359.0, "text": " So now we move on to the deterministic sampling", "tokens": [50964, 407, 586, 321, 1286, 322, 281, 264, 15957, 3142, 21179, 51164], "temperature": 0.0, "avg_logprob": -0.12719994670939896, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.00032606537570245564}, {"id": 337, "seek": 134300, "start": 1359.0, "end": 1363.0, "text": " and actual prescriptions of what", "tokens": [51164, 293, 3539, 1183, 34173, 295, 437, 51364], "temperature": 0.0, "avg_logprob": -0.12719994670939896, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.00032606537570245564}, {"id": 338, "seek": 134300, "start": 1363.0, "end": 1367.0, "text": " you might want to do. So first the noise schedule. Why would", "tokens": [51364, 291, 1062, 528, 281, 360, 13, 407, 700, 264, 5658, 7567, 13, 1545, 576, 51564], "temperature": 0.0, "avg_logprob": -0.12719994670939896, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.00032606537570245564}, {"id": 339, "seek": 134300, "start": 1367.0, "end": 1371.0, "text": " some of them be better than others? For example this way one must be", "tokens": [51564, 512, 295, 552, 312, 1101, 813, 2357, 30, 1171, 1365, 341, 636, 472, 1633, 312, 51764], "temperature": 0.0, "avg_logprob": -0.12719994670939896, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.00032606537570245564}, {"id": 340, "seek": 137100, "start": 1371.0, "end": 1375.0, "text": " terrible for some reason, but why?", "tokens": [50364, 6237, 337, 512, 1778, 11, 457, 983, 30, 50564], "temperature": 0.0, "avg_logprob": -0.10303155491861064, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0004950666916556656}, {"id": 341, "seek": 137100, "start": 1375.0, "end": 1379.0, "text": " Well, now we get a clear view.", "tokens": [50564, 1042, 11, 586, 321, 483, 257, 1850, 1910, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10303155491861064, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0004950666916556656}, {"id": 342, "seek": 137100, "start": 1379.0, "end": 1383.0, "text": " Well, parameterizing things in this way gives us a", "tokens": [50764, 1042, 11, 13075, 3319, 721, 294, 341, 636, 2709, 505, 257, 50964], "temperature": 0.0, "avg_logprob": -0.10303155491861064, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0004950666916556656}, {"id": 343, "seek": 137100, "start": 1383.0, "end": 1387.0, "text": " quite a clear view to this. So let's zoom out again.", "tokens": [50964, 1596, 257, 1850, 1910, 281, 341, 13, 407, 718, 311, 8863, 484, 797, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10303155491861064, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0004950666916556656}, {"id": 344, "seek": 137100, "start": 1387.0, "end": 1391.0, "text": " And consider the fact that", "tokens": [51164, 400, 1949, 264, 1186, 300, 51364], "temperature": 0.0, "avg_logprob": -0.10303155491861064, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0004950666916556656}, {"id": 345, "seek": 137100, "start": 1391.0, "end": 1395.0, "text": " we are trying to follow these curving trajectories by following", "tokens": [51364, 321, 366, 1382, 281, 1524, 613, 1262, 798, 18257, 2083, 538, 3480, 51564], "temperature": 0.0, "avg_logprob": -0.10303155491861064, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0004950666916556656}, {"id": 346, "seek": 137100, "start": 1395.0, "end": 1399.0, "text": " these linear tangents. And that's probably going to be more successful", "tokens": [51564, 613, 8213, 10266, 791, 13, 400, 300, 311, 1391, 516, 281, 312, 544, 4406, 51764], "temperature": 0.0, "avg_logprob": -0.10303155491861064, "compression_ratio": 1.5613207547169812, "no_speech_prob": 0.0004950666916556656}, {"id": 347, "seek": 139900, "start": 1399.0, "end": 1403.0, "text": " when the tangents happen to coincide with this curve trajectory.", "tokens": [50364, 562, 264, 10266, 791, 1051, 281, 13001, 482, 365, 341, 7605, 21512, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07955687262795189, "compression_ratio": 1.7301587301587302, "no_speech_prob": 7.485044625354931e-05}, {"id": 348, "seek": 139900, "start": 1403.0, "end": 1407.0, "text": " So when the trajectory is as straight as possible in other words.", "tokens": [50564, 407, 562, 264, 21512, 307, 382, 2997, 382, 1944, 294, 661, 2283, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07955687262795189, "compression_ratio": 1.7301587301587302, "no_speech_prob": 7.485044625354931e-05}, {"id": 349, "seek": 139900, "start": 1407.0, "end": 1411.0, "text": " So if I use a bad schedule like this one, you see", "tokens": [50764, 407, 498, 286, 764, 257, 1578, 7567, 411, 341, 472, 11, 291, 536, 50964], "temperature": 0.0, "avg_logprob": -0.07955687262795189, "compression_ratio": 1.7301587301587302, "no_speech_prob": 7.485044625354931e-05}, {"id": 350, "seek": 139900, "start": 1411.0, "end": 1415.0, "text": " there's already a visible gap between the tangent and the curve.", "tokens": [50964, 456, 311, 1217, 257, 8974, 7417, 1296, 264, 27747, 293, 264, 7605, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07955687262795189, "compression_ratio": 1.7301587301587302, "no_speech_prob": 7.485044625354931e-05}, {"id": 351, "seek": 139900, "start": 1415.0, "end": 1419.0, "text": " So you easily fall off if you try to step too much.", "tokens": [51164, 407, 291, 3612, 2100, 766, 498, 291, 853, 281, 1823, 886, 709, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07955687262795189, "compression_ratio": 1.7301587301587302, "no_speech_prob": 7.485044625354931e-05}, {"id": 352, "seek": 139900, "start": 1419.0, "end": 1423.0, "text": " And indeed if I show you this random family of different", "tokens": [51364, 400, 6451, 498, 286, 855, 291, 341, 4974, 1605, 295, 819, 51564], "temperature": 0.0, "avg_logprob": -0.07955687262795189, "compression_ratio": 1.7301587301587302, "no_speech_prob": 7.485044625354931e-05}, {"id": 353, "seek": 139900, "start": 1423.0, "end": 1427.0, "text": " schedules, we see that some of them seem to be better in this regard than others.", "tokens": [51564, 28078, 11, 321, 536, 300, 512, 295, 552, 1643, 281, 312, 1101, 294, 341, 3843, 813, 2357, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07955687262795189, "compression_ratio": 1.7301587301587302, "no_speech_prob": 7.485044625354931e-05}, {"id": 354, "seek": 142700, "start": 1427.0, "end": 1431.0, "text": " In particular this one.", "tokens": [50364, 682, 1729, 341, 472, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14571448578231636, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.532492387923412e-05}, {"id": 355, "seek": 142700, "start": 1431.0, "end": 1435.0, "text": " And this is actually the same schedule used in the previous work", "tokens": [50564, 400, 341, 307, 767, 264, 912, 7567, 1143, 294, 264, 3894, 589, 50764], "temperature": 0.0, "avg_logprob": -0.14571448578231636, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.532492387923412e-05}, {"id": 356, "seek": 142700, "start": 1435.0, "end": 1439.0, "text": " DDIM, which is known to be quite good.", "tokens": [50764, 413, 3085, 44, 11, 597, 307, 2570, 281, 312, 1596, 665, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14571448578231636, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.532492387923412e-05}, {"id": 357, "seek": 142700, "start": 1439.0, "end": 1443.0, "text": " And this in a way explains it.", "tokens": [50964, 400, 341, 294, 257, 636, 13948, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14571448578231636, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.532492387923412e-05}, {"id": 358, "seek": 142700, "start": 1443.0, "end": 1447.0, "text": " So this is the schedule where standard deviation cross-lineally", "tokens": [51164, 407, 341, 307, 264, 7567, 689, 3832, 25163, 3278, 12, 1889, 379, 51364], "temperature": 0.0, "avg_logprob": -0.14571448578231636, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.532492387923412e-05}, {"id": 359, "seek": 142700, "start": 1447.0, "end": 1451.0, "text": " and we do not use any scaling. And indeed", "tokens": [51364, 293, 321, 360, 406, 764, 604, 21589, 13, 400, 6451, 51564], "temperature": 0.0, "avg_logprob": -0.14571448578231636, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.532492387923412e-05}, {"id": 360, "seek": 142700, "start": 1451.0, "end": 1455.0, "text": " we'll be leaving the scaling for neural network parameterization.", "tokens": [51564, 321, 603, 312, 5012, 264, 21589, 337, 18161, 3209, 13075, 2144, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14571448578231636, "compression_ratio": 1.5566037735849056, "no_speech_prob": 1.532492387923412e-05}, {"id": 361, "seek": 145500, "start": 1455.0, "end": 1459.0, "text": " And the reason for that is that this scaling also introduces unwanted", "tokens": [50364, 400, 264, 1778, 337, 300, 307, 300, 341, 21589, 611, 31472, 33745, 50564], "temperature": 0.0, "avg_logprob": -0.12933362248432206, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.0990359442075714e-05}, {"id": 362, "seek": 145500, "start": 1459.0, "end": 1463.0, "text": " curvature into these lines.", "tokens": [50564, 37638, 666, 613, 3876, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12933362248432206, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.0990359442075714e-05}, {"id": 363, "seek": 145500, "start": 1463.0, "end": 1467.0, "text": " Yeah, it just turns them unnecessarily at some point.", "tokens": [50764, 865, 11, 309, 445, 4523, 552, 16799, 3289, 412, 512, 935, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12933362248432206, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.0990359442075714e-05}, {"id": 364, "seek": 145500, "start": 1467.0, "end": 1471.0, "text": " It's actually better to let the", "tokens": [50964, 467, 311, 767, 1101, 281, 718, 264, 51164], "temperature": 0.0, "avg_logprob": -0.12933362248432206, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.0990359442075714e-05}, {"id": 365, "seek": 145500, "start": 1471.0, "end": 1475.0, "text": " signal in the ODE grow from that perspective.", "tokens": [51164, 6358, 294, 264, 422, 22296, 1852, 490, 300, 4585, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12933362248432206, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.0990359442075714e-05}, {"id": 366, "seek": 145500, "start": 1475.0, "end": 1479.0, "text": " As further, and yeah, with this the", "tokens": [51364, 1018, 3052, 11, 293, 1338, 11, 365, 341, 264, 51564], "temperature": 0.0, "avg_logprob": -0.12933362248432206, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.0990359442075714e-05}, {"id": 367, "seek": 145500, "start": 1479.0, "end": 1483.0, "text": " is the ODE becomes very simple. So as a further", "tokens": [51564, 307, 264, 422, 22296, 3643, 588, 2199, 13, 407, 382, 257, 3052, 51764], "temperature": 0.0, "avg_logprob": -0.12933362248432206, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.0990359442075714e-05}, {"id": 368, "seek": 148300, "start": 1483.0, "end": 1487.0, "text": " demonstration, like an actual mathematical fact about this", "tokens": [50364, 16520, 11, 411, 364, 3539, 18894, 1186, 466, 341, 50564], "temperature": 0.0, "avg_logprob": -0.08547541707061058, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.1181115951330867e-05}, {"id": 369, "seek": 148300, "start": 1487.0, "end": 1491.0, "text": " schedule and why it allows us to take long steps is that", "tokens": [50564, 7567, 293, 983, 309, 4045, 505, 281, 747, 938, 4439, 307, 300, 50764], "temperature": 0.0, "avg_logprob": -0.08547541707061058, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.1181115951330867e-05}, {"id": 370, "seek": 148300, "start": 1491.0, "end": 1495.0, "text": " if I took a step directly to times zero, then", "tokens": [50764, 498, 286, 1890, 257, 1823, 3838, 281, 1413, 4018, 11, 550, 50964], "temperature": 0.0, "avg_logprob": -0.08547541707061058, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.1181115951330867e-05}, {"id": 371, "seek": 148300, "start": 1495.0, "end": 1499.0, "text": " with this schedule and only this schedule", "tokens": [50964, 365, 341, 7567, 293, 787, 341, 7567, 51164], "temperature": 0.0, "avg_logprob": -0.08547541707061058, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.1181115951330867e-05}, {"id": 372, "seek": 148300, "start": 1499.0, "end": 1503.0, "text": " the tangent is pointing directly to the output of the denoiser.", "tokens": [51164, 264, 27747, 307, 12166, 3838, 281, 264, 5598, 295, 264, 1441, 78, 6694, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08547541707061058, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.1181115951330867e-05}, {"id": 373, "seek": 148300, "start": 1503.0, "end": 1507.0, "text": " And that's very nice because", "tokens": [51364, 400, 300, 311, 588, 1481, 570, 51564], "temperature": 0.0, "avg_logprob": -0.08547541707061058, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.1181115951330867e-05}, {"id": 374, "seek": 148300, "start": 1507.0, "end": 1511.0, "text": " the denoiser output changes only very slowly during the", "tokens": [51564, 264, 1441, 78, 6694, 5598, 2962, 787, 588, 5692, 1830, 264, 51764], "temperature": 0.0, "avg_logprob": -0.08547541707061058, "compression_ratio": 1.7425742574257426, "no_speech_prob": 1.1181115951330867e-05}, {"id": 375, "seek": 151100, "start": 1511.0, "end": 1515.0, "text": " sampling process. And this means that", "tokens": [50364, 21179, 1399, 13, 400, 341, 1355, 300, 50564], "temperature": 0.0, "avg_logprob": -0.13555388547936265, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.00015267249546013772}, {"id": 376, "seek": 151100, "start": 1515.0, "end": 1519.0, "text": " well, the direction you are going to doesn't change almost at all.", "tokens": [50564, 731, 11, 264, 3513, 291, 366, 516, 281, 1177, 380, 1319, 1920, 412, 439, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13555388547936265, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.00015267249546013772}, {"id": 377, "seek": 151100, "start": 1519.0, "end": 1523.0, "text": " So it means you can take long bold steps and you can consequently only take", "tokens": [50764, 407, 309, 1355, 291, 393, 747, 938, 11928, 4439, 293, 291, 393, 47259, 787, 747, 50964], "temperature": 0.0, "avg_logprob": -0.13555388547936265, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.00015267249546013772}, {"id": 378, "seek": 151100, "start": 1523.0, "end": 1527.0, "text": " a few steps or many fewer steps than with the alternatives.", "tokens": [50964, 257, 1326, 4439, 420, 867, 13366, 4439, 813, 365, 264, 20478, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13555388547936265, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.00015267249546013772}, {"id": 379, "seek": 151100, "start": 1527.0, "end": 1531.0, "text": " Okay, and then I said we'll want to direct", "tokens": [51164, 1033, 11, 293, 550, 286, 848, 321, 603, 528, 281, 2047, 51364], "temperature": 0.0, "avg_logprob": -0.13555388547936265, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.00015267249546013772}, {"id": 380, "seek": 151100, "start": 1531.0, "end": 1535.0, "text": " our efforts to the difficult places. Now we've tied our", "tokens": [51364, 527, 6484, 281, 264, 2252, 3190, 13, 823, 321, 600, 9601, 527, 51564], "temperature": 0.0, "avg_logprob": -0.13555388547936265, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.00015267249546013772}, {"id": 381, "seek": 151100, "start": 1535.0, "end": 1539.0, "text": " hands with the noise schedule. So the remaining tool", "tokens": [51564, 2377, 365, 264, 5658, 7567, 13, 407, 264, 8877, 2290, 51764], "temperature": 0.0, "avg_logprob": -0.13555388547936265, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.00015267249546013772}, {"id": 382, "seek": 153900, "start": 1539.0, "end": 1543.0, "text": " is to take different length steps at different stages", "tokens": [50364, 307, 281, 747, 819, 4641, 4439, 412, 819, 10232, 50564], "temperature": 0.0, "avg_logprob": -0.14242985309698644, "compression_ratio": 1.5931372549019607, "no_speech_prob": 0.00016372307436540723}, {"id": 383, "seek": 153900, "start": 1543.0, "end": 1547.0, "text": " of the generation. And indeed, when you go look", "tokens": [50564, 295, 264, 5125, 13, 400, 6451, 11, 562, 291, 352, 574, 50764], "temperature": 0.0, "avg_logprob": -0.14242985309698644, "compression_ratio": 1.5931372549019607, "no_speech_prob": 0.00016372307436540723}, {"id": 384, "seek": 153900, "start": 1547.0, "end": 1551.0, "text": " at the possibly implicit choices the previous methods have done,", "tokens": [50764, 412, 264, 6264, 26947, 7994, 264, 3894, 7150, 362, 1096, 11, 50964], "temperature": 0.0, "avg_logprob": -0.14242985309698644, "compression_ratio": 1.5931372549019607, "no_speech_prob": 0.00016372307436540723}, {"id": 385, "seek": 153900, "start": 1551.0, "end": 1555.0, "text": " all of them take shorter steps at low noise levels", "tokens": [50964, 439, 295, 552, 747, 11639, 4439, 412, 2295, 5658, 4358, 51164], "temperature": 0.0, "avg_logprob": -0.14242985309698644, "compression_ratio": 1.5931372549019607, "no_speech_prob": 0.00016372307436540723}, {"id": 386, "seek": 153900, "start": 1555.0, "end": 1559.0, "text": " because that's where the detail is built again.", "tokens": [51164, 570, 300, 311, 689, 264, 2607, 307, 3094, 797, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14242985309698644, "compression_ratio": 1.5931372549019607, "no_speech_prob": 0.00016372307436540723}, {"id": 387, "seek": 153900, "start": 1559.0, "end": 1563.0, "text": " And yeah, we", "tokens": [51364, 400, 1338, 11, 321, 51564], "temperature": 0.0, "avg_logprob": -0.14242985309698644, "compression_ratio": 1.5931372549019607, "no_speech_prob": 0.00016372307436540723}, {"id": 388, "seek": 153900, "start": 1563.0, "end": 1567.0, "text": " formulate this family of these discretizations", "tokens": [51564, 47881, 341, 1605, 295, 613, 25656, 14455, 51764], "temperature": 0.0, "avg_logprob": -0.14242985309698644, "compression_ratio": 1.5931372549019607, "no_speech_prob": 0.00016372307436540723}, {"id": 389, "seek": 156700, "start": 1567.0, "end": 1571.0, "text": " like a polynomial step length growth and we find that there is a broad", "tokens": [50364, 411, 257, 26110, 1823, 4641, 4599, 293, 321, 915, 300, 456, 307, 257, 4152, 50564], "temperature": 0.0, "avg_logprob": -0.09769073445746239, "compression_ratio": 1.550420168067227, "no_speech_prob": 7.750728400424123e-05}, {"id": 390, "seek": 156700, "start": 1571.0, "end": 1575.0, "text": " optimum of good schedules there.", "tokens": [50564, 39326, 295, 665, 28078, 456, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09769073445746239, "compression_ratio": 1.550420168067227, "no_speech_prob": 7.750728400424123e-05}, {"id": 391, "seek": 156700, "start": 1575.0, "end": 1579.0, "text": " You can read those details in the paper.", "tokens": [50764, 509, 393, 1401, 729, 4365, 294, 264, 3035, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09769073445746239, "compression_ratio": 1.550420168067227, "no_speech_prob": 7.750728400424123e-05}, {"id": 392, "seek": 156700, "start": 1579.0, "end": 1583.0, "text": " So there's one more thing that this ODE framework allows you to do", "tokens": [50964, 407, 456, 311, 472, 544, 551, 300, 341, 422, 22296, 8388, 4045, 291, 281, 360, 51164], "temperature": 0.0, "avg_logprob": -0.09769073445746239, "compression_ratio": 1.550420168067227, "no_speech_prob": 7.750728400424123e-05}, {"id": 393, "seek": 156700, "start": 1583.0, "end": 1587.0, "text": " which is not so clear with for example the Markov chain", "tokens": [51164, 597, 307, 406, 370, 1850, 365, 337, 1365, 264, 3934, 5179, 5021, 51364], "temperature": 0.0, "avg_logprob": -0.09769073445746239, "compression_ratio": 1.550420168067227, "no_speech_prob": 7.750728400424123e-05}, {"id": 394, "seek": 156700, "start": 1587.0, "end": 1591.0, "text": " form lessons is use higher order solvers. So again", "tokens": [51364, 1254, 8820, 307, 764, 2946, 1668, 1404, 840, 13, 407, 797, 51564], "temperature": 0.0, "avg_logprob": -0.09769073445746239, "compression_ratio": 1.550420168067227, "no_speech_prob": 7.750728400424123e-05}, {"id": 395, "seek": 156700, "start": 1591.0, "end": 1595.0, "text": " there is going to be curvature and it can be quite", "tokens": [51564, 456, 307, 516, 281, 312, 37638, 293, 309, 393, 312, 1596, 51764], "temperature": 0.0, "avg_logprob": -0.09769073445746239, "compression_ratio": 1.550420168067227, "no_speech_prob": 7.750728400424123e-05}, {"id": 396, "seek": 159500, "start": 1595.0, "end": 1599.0, "text": " rapid at places. So you can still fall off", "tokens": [50364, 7558, 412, 3190, 13, 407, 291, 393, 920, 2100, 766, 50564], "temperature": 0.0, "avg_logprob": -0.1359577791406474, "compression_ratio": 1.7126436781609196, "no_speech_prob": 2.7670481358654797e-05}, {"id": 397, "seek": 159500, "start": 1599.0, "end": 1603.0, "text": " the track if you just naively follow tangent and that method", "tokens": [50564, 264, 2837, 498, 291, 445, 1667, 3413, 1524, 27747, 293, 300, 3170, 50764], "temperature": 0.0, "avg_logprob": -0.1359577791406474, "compression_ratio": 1.7126436781609196, "no_speech_prob": 2.7670481358654797e-05}, {"id": 398, "seek": 159500, "start": 1603.0, "end": 1607.0, "text": " of following the tangent of course is called the Euler method.", "tokens": [50764, 295, 3480, 264, 27747, 295, 1164, 307, 1219, 264, 462, 26318, 3170, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1359577791406474, "compression_ratio": 1.7126436781609196, "no_speech_prob": 2.7670481358654797e-05}, {"id": 399, "seek": 159500, "start": 1607.0, "end": 1611.0, "text": " But there are higher order schemes for example in the Hoin scheme you take a second", "tokens": [50964, 583, 456, 366, 2946, 1668, 26954, 337, 1365, 294, 264, 3631, 259, 12232, 291, 747, 257, 1150, 51164], "temperature": 0.0, "avg_logprob": -0.1359577791406474, "compression_ratio": 1.7126436781609196, "no_speech_prob": 2.7670481358654797e-05}, {"id": 400, "seek": 159500, "start": 1611.0, "end": 1615.0, "text": " tentative step and you move it back to where you started from", "tokens": [51164, 7054, 1166, 1823, 293, 291, 1286, 309, 646, 281, 689, 291, 1409, 490, 51364], "temperature": 0.0, "avg_logprob": -0.1359577791406474, "compression_ratio": 1.7126436781609196, "no_speech_prob": 2.7670481358654797e-05}, {"id": 401, "seek": 159500, "start": 1615.0, "end": 1619.0, "text": " and your access step is going to be the average of that and the initial one.", "tokens": [51364, 293, 428, 2105, 1823, 307, 516, 281, 312, 264, 4274, 295, 300, 293, 264, 5883, 472, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1359577791406474, "compression_ratio": 1.7126436781609196, "no_speech_prob": 2.7670481358654797e-05}, {"id": 402, "seek": 159500, "start": 1619.0, "end": 1623.0, "text": " And this makes you much better follow these trajectories.", "tokens": [51564, 400, 341, 1669, 291, 709, 1101, 1524, 613, 18257, 2083, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1359577791406474, "compression_ratio": 1.7126436781609196, "no_speech_prob": 2.7670481358654797e-05}, {"id": 403, "seek": 162300, "start": 1623.0, "end": 1627.0, "text": " This of course has a cost. You need to take these sub steps.", "tokens": [50364, 639, 295, 1164, 575, 257, 2063, 13, 509, 643, 281, 747, 613, 1422, 4439, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1469024541426678, "compression_ratio": 1.583673469387755, "no_speech_prob": 1.024240191327408e-05}, {"id": 404, "seek": 162300, "start": 1627.0, "end": 1631.0, "text": " And what we find in the paper by extension we're studying this", "tokens": [50564, 400, 437, 321, 915, 294, 264, 3035, 538, 10320, 321, 434, 7601, 341, 50764], "temperature": 0.0, "avg_logprob": -0.1469024541426678, "compression_ratio": 1.583673469387755, "no_speech_prob": 1.024240191327408e-05}, {"id": 405, "seek": 162300, "start": 1631.0, "end": 1635.0, "text": " is that this Hoin method strikes the best balance between these higher order methods", "tokens": [50764, 307, 300, 341, 3631, 259, 3170, 16750, 264, 1151, 4772, 1296, 613, 2946, 1668, 7150, 50964], "temperature": 0.0, "avg_logprob": -0.1469024541426678, "compression_ratio": 1.583673469387755, "no_speech_prob": 1.024240191327408e-05}, {"id": 406, "seek": 162300, "start": 1635.0, "end": 1639.0, "text": " for sort of the extra bang for the buck.", "tokens": [50964, 337, 1333, 295, 264, 2857, 8550, 337, 264, 14894, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1469024541426678, "compression_ratio": 1.583673469387755, "no_speech_prob": 1.024240191327408e-05}, {"id": 407, "seek": 162300, "start": 1639.0, "end": 1643.0, "text": " And the improvement is actually quite substantial.", "tokens": [51164, 400, 264, 10444, 307, 767, 1596, 16726, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1469024541426678, "compression_ratio": 1.583673469387755, "no_speech_prob": 1.024240191327408e-05}, {"id": 408, "seek": 162300, "start": 1643.0, "end": 1647.0, "text": " Okay, so those are the choices we made.", "tokens": [51364, 1033, 11, 370, 729, 366, 264, 7994, 321, 1027, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1469024541426678, "compression_ratio": 1.583673469387755, "no_speech_prob": 1.024240191327408e-05}, {"id": 409, "seek": 162300, "start": 1647.0, "end": 1651.0, "text": " And now we can evaluate. So we'll be evaluating", "tokens": [51564, 400, 586, 321, 393, 13059, 13, 407, 321, 603, 312, 27479, 51764], "temperature": 0.0, "avg_logprob": -0.1469024541426678, "compression_ratio": 1.583673469387755, "no_speech_prob": 1.024240191327408e-05}, {"id": 410, "seek": 165100, "start": 1651.0, "end": 1655.0, "text": " these results throughout the talk on a couple of very competitive", "tokens": [50364, 613, 3542, 3710, 264, 751, 322, 257, 1916, 295, 588, 10043, 50564], "temperature": 0.0, "avg_logprob": -0.17568890432293496, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000549470481928438}, {"id": 411, "seek": 165100, "start": 1655.0, "end": 1659.0, "text": " generation categories. Saifat Sen", "tokens": [50564, 5125, 10479, 13, 6299, 351, 267, 3862, 50764], "temperature": 0.0, "avg_logprob": -0.17568890432293496, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000549470481928438}, {"id": 412, "seek": 165100, "start": 1659.0, "end": 1663.0, "text": " at Resolution 32 using it at Resolution 64.", "tokens": [50764, 412, 5015, 3386, 8858, 1228, 309, 412, 5015, 3386, 12145, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17568890432293496, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000549470481928438}, {"id": 413, "seek": 165100, "start": 1663.0, "end": 1667.0, "text": " And I want to say a couple of words on this might sound", "tokens": [50964, 400, 286, 528, 281, 584, 257, 1916, 295, 2283, 322, 341, 1062, 1626, 51164], "temperature": 0.0, "avg_logprob": -0.17568890432293496, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000549470481928438}, {"id": 414, "seek": 165100, "start": 1667.0, "end": 1671.0, "text": " like a useless toy example to you if you're used to seeing", "tokens": [51164, 411, 257, 14115, 12058, 1365, 281, 291, 498, 291, 434, 1143, 281, 2577, 51364], "temperature": 0.0, "avg_logprob": -0.17568890432293496, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000549470481928438}, {"id": 415, "seek": 165100, "start": 1671.0, "end": 1675.0, "text": " like outputs from stable diffusion or something.", "tokens": [51364, 411, 23930, 490, 8351, 25242, 420, 746, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17568890432293496, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000549470481928438}, {"id": 416, "seek": 165100, "start": 1675.0, "end": 1679.0, "text": " But the way also those methods work is they first", "tokens": [51564, 583, 264, 636, 611, 729, 7150, 589, 307, 436, 700, 51764], "temperature": 0.0, "avg_logprob": -0.17568890432293496, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.000549470481928438}, {"id": 417, "seek": 167900, "start": 1679.0, "end": 1683.0, "text": " they something like a 64 by 64 image and then they upsample it sequentially.", "tokens": [50364, 436, 746, 411, 257, 12145, 538, 12145, 3256, 293, 550, 436, 15497, 335, 781, 309, 5123, 3137, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18442118686178458, "compression_ratio": 1.6026785714285714, "no_speech_prob": 1.0551857485552318e-05}, {"id": 418, "seek": 167900, "start": 1683.0, "end": 1687.0, "text": " And it turns out that generating the 64 image is the difficult part there.", "tokens": [50564, 400, 309, 4523, 484, 300, 17746, 264, 12145, 3256, 307, 264, 2252, 644, 456, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18442118686178458, "compression_ratio": 1.6026785714285714, "no_speech_prob": 1.0551857485552318e-05}, {"id": 419, "seek": 167900, "start": 1687.0, "end": 1691.0, "text": " The upsampling just kind of it just kind of works.", "tokens": [50764, 440, 15497, 335, 11970, 445, 733, 295, 309, 445, 733, 295, 1985, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18442118686178458, "compression_ratio": 1.6026785714285714, "no_speech_prob": 1.0551857485552318e-05}, {"id": 420, "seek": 167900, "start": 1691.0, "end": 1695.0, "text": " So this is highly indicative of", "tokens": [50964, 407, 341, 307, 5405, 47513, 295, 51164], "temperature": 0.0, "avg_logprob": -0.18442118686178458, "compression_ratio": 1.6026785714285714, "no_speech_prob": 1.0551857485552318e-05}, {"id": 421, "seek": 167900, "start": 1695.0, "end": 1699.0, "text": " improvements we get in very relevant", "tokens": [51164, 13797, 321, 483, 294, 588, 7340, 51364], "temperature": 0.0, "avg_logprob": -0.18442118686178458, "compression_ratio": 1.6026785714285714, "no_speech_prob": 1.0551857485552318e-05}, {"id": 422, "seek": 167900, "start": 1699.0, "end": 1703.0, "text": " very relevant classes of models.", "tokens": [51364, 588, 7340, 5359, 295, 5245, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18442118686178458, "compression_ratio": 1.6026785714285714, "no_speech_prob": 1.0551857485552318e-05}, {"id": 423, "seek": 167900, "start": 1703.0, "end": 1707.0, "text": " Okay, so if we look at the performance of the original", "tokens": [51564, 1033, 11, 370, 498, 321, 574, 412, 264, 3389, 295, 264, 3380, 51764], "temperature": 0.0, "avg_logprob": -0.18442118686178458, "compression_ratio": 1.6026785714285714, "no_speech_prob": 1.0551857485552318e-05}, {"id": 424, "seek": 170700, "start": 1707.0, "end": 1711.0, "text": " samples from these works, from a few previous methods", "tokens": [50364, 10938, 490, 613, 1985, 11, 490, 257, 1326, 3894, 7150, 50564], "temperature": 0.0, "avg_logprob": -0.1883981704711914, "compression_ratio": 1.8296943231441047, "no_speech_prob": 0.0002057748060906306}, {"id": 425, "seek": 170700, "start": 1711.0, "end": 1715.0, "text": " on these data sets, we see", "tokens": [50564, 322, 613, 1412, 6352, 11, 321, 536, 50764], "temperature": 0.0, "avg_logprob": -0.1883981704711914, "compression_ratio": 1.8296943231441047, "no_speech_prob": 0.0002057748060906306}, {"id": 426, "seek": 170700, "start": 1715.0, "end": 1719.0, "text": " that we have the quality on the y-axis, the FID lower is", "tokens": [50764, 300, 321, 362, 264, 3125, 322, 264, 288, 12, 24633, 11, 264, 479, 2777, 3126, 307, 50964], "temperature": 0.0, "avg_logprob": -0.1883981704711914, "compression_ratio": 1.8296943231441047, "no_speech_prob": 0.0002057748060906306}, {"id": 427, "seek": 170700, "start": 1719.0, "end": 1723.0, "text": " better, and we have a number of samples we need to take, like number of steps", "tokens": [50964, 1101, 11, 293, 321, 362, 257, 1230, 295, 10938, 321, 643, 281, 747, 11, 411, 1230, 295, 4439, 51164], "temperature": 0.0, "avg_logprob": -0.1883981704711914, "compression_ratio": 1.8296943231441047, "no_speech_prob": 0.0002057748060906306}, {"id": 428, "seek": 170700, "start": 1723.0, "end": 1727.0, "text": " or the function evaluations on the x-axis. We see that we need to take", "tokens": [51164, 420, 264, 2445, 43085, 322, 264, 2031, 12, 24633, 13, 492, 536, 300, 321, 643, 281, 747, 51364], "temperature": 0.0, "avg_logprob": -0.1883981704711914, "compression_ratio": 1.8296943231441047, "no_speech_prob": 0.0002057748060906306}, {"id": 429, "seek": 170700, "start": 1727.0, "end": 1731.0, "text": " something like hundreds or even thousands of steps to get kind of", "tokens": [51364, 746, 411, 6779, 420, 754, 5383, 295, 4439, 281, 483, 733, 295, 51564], "temperature": 0.0, "avg_logprob": -0.1883981704711914, "compression_ratio": 1.8296943231441047, "no_speech_prob": 0.0002057748060906306}, {"id": 430, "seek": 170700, "start": 1731.0, "end": 1735.0, "text": " saturate the quality to get the best quality that model gives you.", "tokens": [51564, 21160, 473, 264, 3125, 281, 483, 264, 1151, 3125, 300, 2316, 2709, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1883981704711914, "compression_ratio": 1.8296943231441047, "no_speech_prob": 0.0002057748060906306}, {"id": 431, "seek": 173500, "start": 1735.0, "end": 1739.0, "text": " So introducing the point sampler and our discretization schedule", "tokens": [50364, 407, 15424, 264, 935, 3247, 22732, 293, 527, 25656, 2144, 7567, 50564], "temperature": 0.0, "avg_logprob": -0.1956797100248791, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.9161808924982324e-05}, {"id": 432, "seek": 173500, "start": 1739.0, "end": 1743.0, "text": " we vastly improved this. I noticed that", "tokens": [50564, 321, 41426, 9689, 341, 13, 286, 5694, 300, 50764], "temperature": 0.0, "avg_logprob": -0.1956797100248791, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.9161808924982324e-05}, {"id": 433, "seek": 173500, "start": 1743.0, "end": 1747.0, "text": " the x-axis is logarithmic, so we've gone from like hundreds", "tokens": [50764, 264, 2031, 12, 24633, 307, 41473, 355, 13195, 11, 370, 321, 600, 2780, 490, 411, 6779, 50964], "temperature": 0.0, "avg_logprob": -0.1956797100248791, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.9161808924982324e-05}, {"id": 434, "seek": 173500, "start": 1747.0, "end": 1751.0, "text": " to dozens of evaluations.", "tokens": [50964, 281, 18431, 295, 43085, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1956797100248791, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.9161808924982324e-05}, {"id": 435, "seek": 173500, "start": 1751.0, "end": 1755.0, "text": " And further introducing the noise schedule", "tokens": [51164, 400, 3052, 15424, 264, 5658, 7567, 51364], "temperature": 0.0, "avg_logprob": -0.1956797100248791, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.9161808924982324e-05}, {"id": 436, "seek": 173500, "start": 1755.0, "end": 1759.0, "text": " and scaling schedule further improved the results by a large amount", "tokens": [51364, 293, 21589, 7567, 3052, 9689, 264, 3542, 538, 257, 2416, 2372, 51564], "temperature": 0.0, "avg_logprob": -0.1956797100248791, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.9161808924982324e-05}, {"id": 437, "seek": 173500, "start": 1759.0, "end": 1763.0, "text": " except in DDIM which was already using those schedules.", "tokens": [51564, 3993, 294, 30778, 6324, 597, 390, 1217, 1228, 729, 28078, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1956797100248791, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.9161808924982324e-05}, {"id": 438, "seek": 176300, "start": 1763.0, "end": 1767.0, "text": " So now we've already made it quite far here", "tokens": [50364, 407, 586, 321, 600, 1217, 1027, 309, 1596, 1400, 510, 50564], "temperature": 0.0, "avg_logprob": -0.1396323717557467, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.00012578957830555737}, {"id": 439, "seek": 176300, "start": 1767.0, "end": 1771.0, "text": " and using some super fancy", "tokens": [50564, 293, 1228, 512, 1687, 10247, 50764], "temperature": 0.0, "avg_logprob": -0.1396323717557467, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.00012578957830555737}, {"id": 440, "seek": 176300, "start": 1771.0, "end": 1775.0, "text": " higher audio, so it's not worth the effort.", "tokens": [50764, 2946, 6278, 11, 370, 309, 311, 406, 3163, 264, 4630, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1396323717557467, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.00012578957830555737}, {"id": 441, "seek": 176300, "start": 1775.0, "end": 1779.0, "text": " Okay, so now we've covered the deterministic sampling", "tokens": [50964, 1033, 11, 370, 586, 321, 600, 5343, 264, 15957, 3142, 21179, 51164], "temperature": 0.0, "avg_logprob": -0.1396323717557467, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.00012578957830555737}, {"id": 442, "seek": 176300, "start": 1779.0, "end": 1783.0, "text": " and let's next", "tokens": [51164, 293, 718, 311, 958, 51364], "temperature": 0.0, "avg_logprob": -0.1396323717557467, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.00012578957830555737}, {"id": 443, "seek": 176300, "start": 1783.0, "end": 1787.0, "text": " return to the question of SDE which we", "tokens": [51364, 2736, 281, 264, 1168, 295, 14638, 36, 597, 321, 51564], "temperature": 0.0, "avg_logprob": -0.1396323717557467, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.00012578957830555737}, {"id": 444, "seek": 176300, "start": 1787.0, "end": 1791.0, "text": " put on the back burner on the other slides. And remember", "tokens": [51564, 829, 322, 264, 646, 36116, 322, 264, 661, 9788, 13, 400, 1604, 51764], "temperature": 0.0, "avg_logprob": -0.1396323717557467, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.00012578957830555737}, {"id": 445, "seek": 179100, "start": 1791.0, "end": 1795.0, "text": " instead of following these nice smooth flow trajectories", "tokens": [50364, 2602, 295, 3480, 613, 1481, 5508, 3095, 18257, 2083, 50564], "temperature": 0.0, "avg_logprob": -0.1364489681315872, "compression_ratio": 1.7075098814229248, "no_speech_prob": 6.279966328293085e-05}, {"id": 446, "seek": 179100, "start": 1795.0, "end": 1799.0, "text": " the SDE sort of cheaters around as some kind of", "tokens": [50564, 264, 14638, 36, 1333, 295, 947, 16749, 926, 382, 512, 733, 295, 50764], "temperature": 0.0, "avg_logprob": -0.1364489681315872, "compression_ratio": 1.7075098814229248, "no_speech_prob": 6.279966328293085e-05}, {"id": 447, "seek": 179100, "start": 1799.0, "end": 1803.0, "text": " exploration around that baseline. So it can be interpreted as", "tokens": [50764, 16197, 926, 300, 20518, 13, 407, 309, 393, 312, 26749, 382, 50964], "temperature": 0.0, "avg_logprob": -0.1364489681315872, "compression_ratio": 1.7075098814229248, "no_speech_prob": 6.279966328293085e-05}, {"id": 448, "seek": 179100, "start": 1803.0, "end": 1807.0, "text": " replacing the noise as you go on top of like reducing it.", "tokens": [50964, 19139, 264, 5658, 382, 291, 352, 322, 1192, 295, 411, 12245, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1364489681315872, "compression_ratio": 1.7075098814229248, "no_speech_prob": 6.279966328293085e-05}, {"id": 449, "seek": 179100, "start": 1807.0, "end": 1811.0, "text": " And the reason why people care about the SDE is of course", "tokens": [51164, 400, 264, 1778, 983, 561, 1127, 466, 264, 14638, 36, 307, 295, 1164, 51364], "temperature": 0.0, "avg_logprob": -0.1364489681315872, "compression_ratio": 1.7075098814229248, "no_speech_prob": 6.279966328293085e-05}, {"id": 450, "seek": 179100, "start": 1811.0, "end": 1815.0, "text": " well one reason is that that's where this stuff is derived from", "tokens": [51364, 731, 472, 1778, 307, 300, 300, 311, 689, 341, 1507, 307, 18949, 490, 51564], "temperature": 0.0, "avg_logprob": -0.1364489681315872, "compression_ratio": 1.7075098814229248, "no_speech_prob": 6.279966328293085e-05}, {"id": 451, "seek": 179100, "start": 1815.0, "end": 1819.0, "text": " but the other is that in practice you tend to get better results when you use the SDE", "tokens": [51564, 457, 264, 661, 307, 300, 294, 3124, 291, 3928, 281, 483, 1101, 3542, 562, 291, 764, 264, 14638, 36, 51764], "temperature": 0.0, "avg_logprob": -0.1364489681315872, "compression_ratio": 1.7075098814229248, "no_speech_prob": 6.279966328293085e-05}, {"id": 452, "seek": 181900, "start": 1819.0, "end": 1823.0, "text": " instead of the ODE, at least in previous work.", "tokens": [50364, 2602, 295, 264, 48447, 36, 11, 412, 1935, 294, 3894, 589, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12646719749937665, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.00026337060262449086}, {"id": 453, "seek": 181900, "start": 1823.0, "end": 1827.0, "text": " And the reason for that will become apparent soon.", "tokens": [50564, 400, 264, 1778, 337, 300, 486, 1813, 18335, 2321, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12646719749937665, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.00026337060262449086}, {"id": 454, "seek": 181900, "start": 1827.0, "end": 1831.0, "text": " But let's first generalize this idea a little.", "tokens": [50764, 583, 718, 311, 700, 2674, 1125, 341, 1558, 257, 707, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12646719749937665, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.00026337060262449086}, {"id": 455, "seek": 181900, "start": 1831.0, "end": 1835.0, "text": " So in the paper we present this generalized version of the SDE", "tokens": [50964, 407, 294, 264, 3035, 321, 1974, 341, 44498, 3037, 295, 264, 14638, 36, 51164], "temperature": 0.0, "avg_logprob": -0.12646719749937665, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.00026337060262449086}, {"id": 456, "seek": 181900, "start": 1835.0, "end": 1839.0, "text": " which allows you to specify the strength of this exploration", "tokens": [51164, 597, 4045, 291, 281, 16500, 264, 3800, 295, 341, 16197, 51364], "temperature": 0.0, "avg_logprob": -0.12646719749937665, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.00026337060262449086}, {"id": 457, "seek": 181900, "start": 1839.0, "end": 1843.0, "text": " by this sort of noise replacement schedule.", "tokens": [51364, 538, 341, 1333, 295, 5658, 14419, 7567, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12646719749937665, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.00026337060262449086}, {"id": 458, "seek": 181900, "start": 1843.0, "end": 1847.0, "text": " So especially when you set it to zero you get just the ODE", "tokens": [51564, 407, 2318, 562, 291, 992, 309, 281, 4018, 291, 483, 445, 264, 48447, 36, 51764], "temperature": 0.0, "avg_logprob": -0.12646719749937665, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.00026337060262449086}, {"id": 459, "seek": 184700, "start": 1847.0, "end": 1851.0, "text": " boosting this factor. Or you can do more exotic schedules", "tokens": [50364, 43117, 341, 5952, 13, 1610, 291, 393, 360, 544, 27063, 28078, 50564], "temperature": 0.0, "avg_logprob": -0.1874940984389361, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.70052317623049e-05}, {"id": 460, "seek": 184700, "start": 1851.0, "end": 1855.0, "text": " like something like this", "tokens": [50564, 411, 746, 411, 341, 50764], "temperature": 0.0, "avg_logprob": -0.1874940984389361, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.70052317623049e-05}, {"id": 461, "seek": 184700, "start": 1855.0, "end": 1859.0, "text": " where you have it behave like an SDE in the middle and like the ODE", "tokens": [50764, 689, 291, 362, 309, 15158, 411, 364, 14638, 36, 294, 264, 2808, 293, 411, 264, 48447, 36, 50964], "temperature": 0.0, "avg_logprob": -0.1874940984389361, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.70052317623049e-05}, {"id": 462, "seek": 184700, "start": 1859.0, "end": 1863.0, "text": " and so on. And samples would look like this.", "tokens": [50964, 293, 370, 322, 13, 400, 10938, 576, 574, 411, 341, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1874940984389361, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.70052317623049e-05}, {"id": 463, "seek": 184700, "start": 1863.0, "end": 1867.0, "text": " But again that's the question of is this just a nice streak", "tokens": [51164, 583, 797, 300, 311, 264, 1168, 295, 307, 341, 445, 257, 1481, 35634, 51364], "temperature": 0.0, "avg_logprob": -0.1874940984389361, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.70052317623049e-05}, {"id": 464, "seek": 184700, "start": 1867.0, "end": 1871.0, "text": " or like what's the point.", "tokens": [51364, 420, 411, 437, 311, 264, 935, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1874940984389361, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.70052317623049e-05}, {"id": 465, "seek": 184700, "start": 1871.0, "end": 1875.0, "text": " And as I said empirically", "tokens": [51564, 400, 382, 286, 848, 25790, 984, 51764], "temperature": 0.0, "avg_logprob": -0.1874940984389361, "compression_ratio": 1.566326530612245, "no_speech_prob": 8.70052317623049e-05}, {"id": 466, "seek": 187500, "start": 1875.0, "end": 1879.0, "text": " this improves the results. And now looking at this SDE", "tokens": [50364, 341, 24771, 264, 3542, 13, 400, 586, 1237, 412, 341, 14638, 36, 50564], "temperature": 0.0, "avg_logprob": -0.09753023448743318, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.0005300958291627467}, {"id": 467, "seek": 187500, "start": 1879.0, "end": 1883.0, "text": " the reason becomes somewhat apparent.", "tokens": [50564, 264, 1778, 3643, 8344, 18335, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09753023448743318, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.0005300958291627467}, {"id": 468, "seek": 187500, "start": 1883.0, "end": 1887.0, "text": " So don't try to read it unless you're an expert in SDEs", "tokens": [50764, 407, 500, 380, 853, 281, 1401, 309, 5969, 291, 434, 364, 5844, 294, 14638, 20442, 50964], "temperature": 0.0, "avg_logprob": -0.09753023448743318, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.0005300958291627467}, {"id": 469, "seek": 187500, "start": 1887.0, "end": 1891.0, "text": " but we can recognize a couple of familiar parts here.", "tokens": [50964, 457, 321, 393, 5521, 257, 1916, 295, 4963, 3166, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09753023448743318, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.0005300958291627467}, {"id": 470, "seek": 187500, "start": 1891.0, "end": 1895.0, "text": " So the first term in the SDE is actually just the ODE from the previous section.", "tokens": [51164, 407, 264, 700, 1433, 294, 264, 14638, 36, 307, 767, 445, 264, 48447, 36, 490, 264, 3894, 3541, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09753023448743318, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.0005300958291627467}, {"id": 471, "seek": 187500, "start": 1895.0, "end": 1899.0, "text": " So that means that we still have this force", "tokens": [51364, 407, 300, 1355, 300, 321, 920, 362, 341, 3464, 51564], "temperature": 0.0, "avg_logprob": -0.09753023448743318, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.0005300958291627467}, {"id": 472, "seek": 187500, "start": 1899.0, "end": 1903.0, "text": " that is driving us towards the distribution", "tokens": [51564, 300, 307, 4840, 505, 3030, 264, 7316, 51764], "temperature": 0.0, "avg_logprob": -0.09753023448743318, "compression_ratio": 1.5654008438818565, "no_speech_prob": 0.0005300958291627467}, {"id": 473, "seek": 190300, "start": 1903.0, "end": 1907.0, "text": " of flow lines. And the remainder we can identify", "tokens": [50364, 295, 3095, 3876, 13, 400, 264, 29837, 321, 393, 5876, 50564], "temperature": 0.0, "avg_logprob": -0.2041095645948388, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.0230188713176176e-05}, {"id": 474, "seek": 190300, "start": 1907.0, "end": 1911.0, "text": " some kind of a lens around diffusion stochastic difference equation", "tokens": [50564, 512, 733, 295, 257, 6765, 926, 25242, 342, 8997, 2750, 2649, 5367, 50764], "temperature": 0.0, "avg_logprob": -0.2041095645948388, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.0230188713176176e-05}, {"id": 475, "seek": 190300, "start": 1911.0, "end": 1915.0, "text": " which is a well-known thing from a long ago.", "tokens": [50764, 597, 307, 257, 731, 12, 6861, 551, 490, 257, 938, 2057, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2041095645948388, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.0230188713176176e-05}, {"id": 476, "seek": 190300, "start": 1915.0, "end": 1919.0, "text": " It has this property that it makes the samples sort of explore your", "tokens": [50964, 467, 575, 341, 4707, 300, 309, 1669, 264, 10938, 1333, 295, 6839, 428, 51164], "temperature": 0.0, "avg_logprob": -0.2041095645948388, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.0230188713176176e-05}, {"id": 477, "seek": 190300, "start": 1919.0, "end": 1923.0, "text": " distribution and if the samples are not distributed correctly", "tokens": [51164, 7316, 293, 498, 264, 10938, 366, 406, 12631, 8944, 51364], "temperature": 0.0, "avg_logprob": -0.2041095645948388, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.0230188713176176e-05}, {"id": 478, "seek": 190300, "start": 1923.0, "end": 1927.0, "text": " it will kind of reduce that error.", "tokens": [51364, 309, 486, 733, 295, 5407, 300, 6713, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2041095645948388, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.0230188713176176e-05}, {"id": 479, "seek": 190300, "start": 1927.0, "end": 1931.0, "text": " So it has this healing property.", "tokens": [51564, 407, 309, 575, 341, 9745, 4707, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2041095645948388, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.0230188713176176e-05}, {"id": 480, "seek": 193100, "start": 1931.0, "end": 1935.0, "text": " And because we do make errors during the sampling it kind of actively corrects", "tokens": [50364, 400, 570, 321, 360, 652, 13603, 1830, 264, 21179, 309, 733, 295, 13022, 3006, 82, 50564], "temperature": 0.0, "avg_logprob": -0.1282480139481394, "compression_ratio": 1.7328244274809161, "no_speech_prob": 6.411192316591041e-06}, {"id": 481, "seek": 193100, "start": 1935.0, "end": 1939.0, "text": " for them. And this is how it looks like. So let's take this extreme situation", "tokens": [50564, 337, 552, 13, 400, 341, 307, 577, 309, 1542, 411, 13, 407, 718, 311, 747, 341, 8084, 2590, 50764], "temperature": 0.0, "avg_logprob": -0.1282480139481394, "compression_ratio": 1.7328244274809161, "no_speech_prob": 6.411192316591041e-06}, {"id": 482, "seek": 193100, "start": 1939.0, "end": 1943.0, "text": " we have our samples to blue dots. And", "tokens": [50764, 321, 362, 527, 10938, 281, 3344, 15026, 13, 400, 50964], "temperature": 0.0, "avg_logprob": -0.1282480139481394, "compression_ratio": 1.7328244274809161, "no_speech_prob": 6.411192316591041e-06}, {"id": 483, "seek": 193100, "start": 1943.0, "end": 1947.0, "text": " let's say they are really bad. They are not following the underlying distribution at all.", "tokens": [50964, 718, 311, 584, 436, 366, 534, 1578, 13, 814, 366, 406, 3480, 264, 14217, 7316, 412, 439, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1282480139481394, "compression_ratio": 1.7328244274809161, "no_speech_prob": 6.411192316591041e-06}, {"id": 484, "seek": 193100, "start": 1947.0, "end": 1951.0, "text": " They are skewed to one side.", "tokens": [51164, 814, 366, 8756, 26896, 281, 472, 1252, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1282480139481394, "compression_ratio": 1.7328244274809161, "no_speech_prob": 6.411192316591041e-06}, {"id": 485, "seek": 193100, "start": 1951.0, "end": 1955.0, "text": " So if we keep following the ODE it does nothing to actually", "tokens": [51364, 407, 498, 321, 1066, 3480, 264, 48447, 36, 309, 775, 1825, 281, 767, 51564], "temperature": 0.0, "avg_logprob": -0.1282480139481394, "compression_ratio": 1.7328244274809161, "no_speech_prob": 6.411192316591041e-06}, {"id": 486, "seek": 193100, "start": 1955.0, "end": 1959.0, "text": " correct the skew and we completely miss the other basin of the days for example.", "tokens": [51564, 3006, 264, 8756, 86, 293, 321, 2584, 1713, 264, 661, 34863, 295, 264, 1708, 337, 1365, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1282480139481394, "compression_ratio": 1.7328244274809161, "no_speech_prob": 6.411192316591041e-06}, {"id": 487, "seek": 195900, "start": 1959.0, "end": 1963.0, "text": " So when I introduce stochasticity to this process", "tokens": [50364, 407, 562, 286, 5366, 342, 8997, 2750, 507, 281, 341, 1399, 50564], "temperature": 0.0, "avg_logprob": -0.12393948237101236, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.358582661254331e-05}, {"id": 488, "seek": 195900, "start": 1963.0, "end": 1967.0, "text": " it starts looking like this.", "tokens": [50564, 309, 3719, 1237, 411, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12393948237101236, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.358582661254331e-05}, {"id": 489, "seek": 195900, "start": 1967.0, "end": 1971.0, "text": " So these samples do this kind of random exploration", "tokens": [50764, 407, 613, 10938, 360, 341, 733, 295, 4974, 16197, 50964], "temperature": 0.0, "avg_logprob": -0.12393948237101236, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.358582661254331e-05}, {"id": 490, "seek": 195900, "start": 1971.0, "end": 1975.0, "text": " and gradually forget where they came from and forget the error", "tokens": [50964, 293, 13145, 2870, 689, 436, 1361, 490, 293, 2870, 264, 6713, 51164], "temperature": 0.0, "avg_logprob": -0.12393948237101236, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.358582661254331e-05}, {"id": 491, "seek": 195900, "start": 1975.0, "end": 1979.0, "text": " in this opposition. And now we've covered both modes for example", "tokens": [51164, 294, 341, 13504, 13, 400, 586, 321, 600, 5343, 1293, 14068, 337, 1365, 51364], "temperature": 0.0, "avg_logprob": -0.12393948237101236, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.358582661254331e-05}, {"id": 492, "seek": 195900, "start": 1979.0, "end": 1983.0, "text": " in the generated images on the left edge.", "tokens": [51364, 294, 264, 10833, 5267, 322, 264, 1411, 4691, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12393948237101236, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.358582661254331e-05}, {"id": 493, "seek": 195900, "start": 1983.0, "end": 1987.0, "text": " So that's the sort of reason why stochasticity is helpful.", "tokens": [51564, 407, 300, 311, 264, 1333, 295, 1778, 983, 342, 8997, 2750, 507, 307, 4961, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12393948237101236, "compression_ratio": 1.639269406392694, "no_speech_prob": 7.358582661254331e-05}, {"id": 494, "seek": 198700, "start": 1987.0, "end": 1991.0, "text": " No, arguably this is the only benefit of the SDE over the ODE.", "tokens": [50364, 883, 11, 26771, 341, 307, 264, 787, 5121, 295, 264, 14638, 36, 670, 264, 48447, 36, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1348753493765126, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.7610913346288726e-05}, {"id": 495, "seek": 198700, "start": 1991.0, "end": 1995.0, "text": " But there are also downsides in using SDEs.", "tokens": [50564, 583, 456, 366, 611, 21554, 1875, 294, 1228, 14638, 20442, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1348753493765126, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.7610913346288726e-05}, {"id": 496, "seek": 198700, "start": 1995.0, "end": 1999.0, "text": " For example we would technically have to use these", "tokens": [50764, 1171, 1365, 321, 576, 12120, 362, 281, 764, 613, 50964], "temperature": 0.0, "avg_logprob": -0.1348753493765126, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.7610913346288726e-05}, {"id": 497, "seek": 198700, "start": 1999.0, "end": 2003.0, "text": " complicated solvers that are arguably designed for much more complicated", "tokens": [50964, 6179, 1404, 840, 300, 366, 26771, 4761, 337, 709, 544, 6179, 51164], "temperature": 0.0, "avg_logprob": -0.1348753493765126, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.7610913346288726e-05}, {"id": 498, "seek": 198700, "start": 2003.0, "end": 2007.0, "text": " cases where you have more general SDEs.", "tokens": [51164, 3331, 689, 291, 362, 544, 2674, 14638, 20442, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1348753493765126, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.7610913346288726e-05}, {"id": 499, "seek": 198700, "start": 2007.0, "end": 2011.0, "text": " So we asked the question could we instead directly combine the ODE solving", "tokens": [51364, 407, 321, 2351, 264, 1168, 727, 321, 2602, 3838, 10432, 264, 48447, 36, 12606, 51564], "temperature": 0.0, "avg_logprob": -0.1348753493765126, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.7610913346288726e-05}, {"id": 500, "seek": 198700, "start": 2011.0, "end": 2015.0, "text": " with this idea of this", "tokens": [51564, 365, 341, 1558, 295, 341, 51764], "temperature": 0.0, "avg_logprob": -0.1348753493765126, "compression_ratio": 1.6283185840707965, "no_speech_prob": 1.7610913346288726e-05}, {"id": 501, "seek": 201500, "start": 2015.0, "end": 2019.0, "text": " churning of the noise, adding and removing it.", "tokens": [50364, 417, 10656, 295, 264, 5658, 11, 5127, 293, 12720, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14572417315314798, "compression_ratio": 1.6145833333333333, "no_speech_prob": 8.563008304918185e-05}, {"id": 502, "seek": 201500, "start": 2019.0, "end": 2023.0, "text": " And the answer is this.", "tokens": [50564, 400, 264, 1867, 307, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14572417315314798, "compression_ratio": 1.6145833333333333, "no_speech_prob": 8.563008304918185e-05}, {"id": 503, "seek": 201500, "start": 2023.0, "end": 2027.0, "text": " So this is a stochastic example we proposed in the paper.", "tokens": [50764, 407, 341, 307, 257, 342, 8997, 2750, 1365, 321, 10348, 294, 264, 3035, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14572417315314798, "compression_ratio": 1.6145833333333333, "no_speech_prob": 8.563008304918185e-05}, {"id": 504, "seek": 201500, "start": 2027.0, "end": 2031.0, "text": " So we have our current noise image at noise level TTI.", "tokens": [50964, 407, 321, 362, 527, 2190, 5658, 3256, 412, 5658, 1496, 314, 5422, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14572417315314798, "compression_ratio": 1.6145833333333333, "no_speech_prob": 8.563008304918185e-05}, {"id": 505, "seek": 201500, "start": 2031.0, "end": 2035.0, "text": " Remember in our parent recession the noise level is now", "tokens": [51164, 5459, 294, 527, 2596, 24828, 264, 5658, 1496, 307, 586, 51364], "temperature": 0.0, "avg_logprob": -0.14572417315314798, "compression_ratio": 1.6145833333333333, "no_speech_prob": 8.563008304918185e-05}, {"id": 506, "seek": 201500, "start": 2035.0, "end": 2039.0, "text": " completely equivalent with time.", "tokens": [51364, 2584, 10344, 365, 565, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14572417315314798, "compression_ratio": 1.6145833333333333, "no_speech_prob": 8.563008304918185e-05}, {"id": 507, "seek": 201500, "start": 2039.0, "end": 2043.0, "text": " So we have two sub steps in one step.", "tokens": [51564, 407, 321, 362, 732, 1422, 4439, 294, 472, 1823, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14572417315314798, "compression_ratio": 1.6145833333333333, "no_speech_prob": 8.563008304918185e-05}, {"id": 508, "seek": 204300, "start": 2043.0, "end": 2047.0, "text": " So first we add noise. So this represents the lens of an exploration.", "tokens": [50364, 407, 700, 321, 909, 5658, 13, 407, 341, 8855, 264, 6765, 295, 364, 16197, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1803831017535666, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.727719250600785e-05}, {"id": 509, "seek": 204300, "start": 2047.0, "end": 2051.0, "text": " So we landed some random", "tokens": [50564, 407, 321, 15336, 512, 4974, 50764], "temperature": 0.0, "avg_logprob": -0.1803831017535666, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.727719250600785e-05}, {"id": 510, "seek": 204300, "start": 2051.0, "end": 2055.0, "text": " noisier image so the time increases here.", "tokens": [50764, 572, 271, 811, 3256, 370, 264, 565, 8637, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1803831017535666, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.727719250600785e-05}, {"id": 511, "seek": 204300, "start": 2055.0, "end": 2059.0, "text": " And then we solved the ODE to where we actually wanted to go", "tokens": [50964, 400, 550, 321, 13041, 264, 48447, 36, 281, 689, 321, 767, 1415, 281, 352, 51164], "temperature": 0.0, "avg_logprob": -0.1803831017535666, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.727719250600785e-05}, {"id": 512, "seek": 204300, "start": 2059.0, "end": 2063.0, "text": " with say lower noise level.", "tokens": [51164, 365, 584, 3126, 5658, 1496, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1803831017535666, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.727719250600785e-05}, {"id": 513, "seek": 204300, "start": 2063.0, "end": 2067.0, "text": " And that simply follows the flow line there.", "tokens": [51364, 400, 300, 2935, 10002, 264, 3095, 1622, 456, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1803831017535666, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.727719250600785e-05}, {"id": 514, "seek": 204300, "start": 2067.0, "end": 2071.0, "text": " And in practice we do this with a single point step. So we keep alternating between", "tokens": [51564, 400, 294, 3124, 321, 360, 341, 365, 257, 2167, 935, 1823, 13, 407, 321, 1066, 40062, 1296, 51764], "temperature": 0.0, "avg_logprob": -0.1803831017535666, "compression_ratio": 1.6465116279069767, "no_speech_prob": 8.727719250600785e-05}, {"id": 515, "seek": 207100, "start": 2071.0, "end": 2075.0, "text": " this noise addition and the point step.", "tokens": [50364, 341, 5658, 4500, 293, 264, 935, 1823, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1596411925095778, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00012415855599101633}, {"id": 516, "seek": 207100, "start": 2075.0, "end": 2079.0, "text": " And this brings us closer and closer to time zero as we want.", "tokens": [50564, 400, 341, 5607, 505, 4966, 293, 4966, 281, 565, 4018, 382, 321, 528, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1596411925095778, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00012415855599101633}, {"id": 517, "seek": 207100, "start": 2079.0, "end": 2083.0, "text": " But underneath it is the ODE running the show", "tokens": [50764, 583, 7223, 309, 307, 264, 48447, 36, 2614, 264, 855, 50964], "temperature": 0.0, "avg_logprob": -0.1596411925095778, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00012415855599101633}, {"id": 518, "seek": 207100, "start": 2083.0, "end": 2087.0, "text": " and guiding us along these lines.", "tokens": [50964, 293, 25061, 505, 2051, 613, 3876, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1596411925095778, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00012415855599101633}, {"id": 519, "seek": 207100, "start": 2087.0, "end": 2091.0, "text": " But on top of that we now have this jittering with correct servers.", "tokens": [51164, 583, 322, 1192, 295, 300, 321, 586, 362, 341, 361, 3904, 278, 365, 3006, 15909, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1596411925095778, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00012415855599101633}, {"id": 520, "seek": 207100, "start": 2091.0, "end": 2095.0, "text": " Okay, so this all sounds really nice.", "tokens": [51364, 1033, 11, 370, 341, 439, 3263, 534, 1481, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1596411925095778, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00012415855599101633}, {"id": 521, "seek": 207100, "start": 2095.0, "end": 2099.0, "text": " You get free error correction but it's not actually free", "tokens": [51564, 509, 483, 1737, 6713, 19984, 457, 309, 311, 406, 767, 1737, 51764], "temperature": 0.0, "avg_logprob": -0.1596411925095778, "compression_ratio": 1.5852534562211982, "no_speech_prob": 0.00012415855599101633}, {"id": 522, "seek": 209900, "start": 2099.0, "end": 2103.0, "text": " because the lens event term is also an approximation of some continuous thing.", "tokens": [50364, 570, 264, 6765, 2280, 1433, 307, 611, 364, 28023, 295, 512, 10957, 551, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09493134156712946, "compression_ratio": 1.6563706563706564, "no_speech_prob": 7.208622264442965e-05}, {"id": 523, "seek": 209900, "start": 2103.0, "end": 2107.0, "text": " And you introduce new error also when you make it.", "tokens": [50564, 400, 291, 5366, 777, 6713, 611, 562, 291, 652, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09493134156712946, "compression_ratio": 1.6563706563706564, "no_speech_prob": 7.208622264442965e-05}, {"id": 524, "seek": 209900, "start": 2107.0, "end": 2111.0, "text": " So it's actually a quite delicate balance of how much you should do this.", "tokens": [50764, 407, 309, 311, 767, 257, 1596, 21417, 4772, 295, 577, 709, 291, 820, 360, 341, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09493134156712946, "compression_ratio": 1.6563706563706564, "no_speech_prob": 7.208622264442965e-05}, {"id": 525, "seek": 209900, "start": 2111.0, "end": 2115.0, "text": " And now with this clear view into this dynamics we actually find that it is really", "tokens": [50964, 400, 586, 365, 341, 1850, 1910, 666, 341, 15679, 321, 767, 915, 300, 309, 307, 534, 51164], "temperature": 0.0, "avg_logprob": -0.09493134156712946, "compression_ratio": 1.6563706563706564, "no_speech_prob": 7.208622264442965e-05}, {"id": 526, "seek": 209900, "start": 2115.0, "end": 2119.0, "text": " finicky. You need to tune the amount of stochasticity", "tokens": [51164, 962, 20539, 13, 509, 643, 281, 10864, 264, 2372, 295, 342, 8997, 2750, 507, 51364], "temperature": 0.0, "avg_logprob": -0.09493134156712946, "compression_ratio": 1.6563706563706564, "no_speech_prob": 7.208622264442965e-05}, {"id": 527, "seek": 209900, "start": 2119.0, "end": 2123.0, "text": " on a per data set per architecture basis.", "tokens": [51364, 322, 257, 680, 1412, 992, 680, 9482, 5143, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09493134156712946, "compression_ratio": 1.6563706563706564, "no_speech_prob": 7.208622264442965e-05}, {"id": 528, "seek": 209900, "start": 2123.0, "end": 2127.0, "text": " You get the benefits but it's really annoying.", "tokens": [51564, 509, 483, 264, 5311, 457, 309, 311, 534, 11304, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09493134156712946, "compression_ratio": 1.6563706563706564, "no_speech_prob": 7.208622264442965e-05}, {"id": 529, "seek": 212700, "start": 2127.0, "end": 2131.0, "text": " So it's a mixed bag.", "tokens": [50364, 407, 309, 311, 257, 7467, 3411, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14558653831481932, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.00016530945140402764}, {"id": 530, "seek": 212700, "start": 2131.0, "end": 2135.0, "text": " Nonetheless it is very useful. So if we compare the ODEs from the previous section", "tokens": [50564, 45437, 309, 307, 588, 4420, 13, 407, 498, 321, 6794, 264, 48447, 20442, 490, 264, 3894, 3541, 50764], "temperature": 0.0, "avg_logprob": -0.14558653831481932, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.00016530945140402764}, {"id": 531, "seek": 212700, "start": 2135.0, "end": 2139.0, "text": " their performance with just original SDE", "tokens": [50764, 641, 3389, 365, 445, 3380, 14638, 36, 50964], "temperature": 0.0, "avg_logprob": -0.14558653831481932, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.00016530945140402764}, {"id": 532, "seek": 212700, "start": 2139.0, "end": 2143.0, "text": " samples from these respective works.", "tokens": [50964, 10938, 490, 613, 23649, 1985, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14558653831481932, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.00016530945140402764}, {"id": 533, "seek": 212700, "start": 2143.0, "end": 2147.0, "text": " We see that the SDE solvers are simply better in the end", "tokens": [51164, 492, 536, 300, 264, 14638, 36, 1404, 840, 366, 2935, 1101, 294, 264, 917, 51364], "temperature": 0.0, "avg_logprob": -0.14558653831481932, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.00016530945140402764}, {"id": 534, "seek": 212700, "start": 2147.0, "end": 2151.0, "text": " but they are also very slow.", "tokens": [51364, 457, 436, 366, 611, 588, 2964, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14558653831481932, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.00016530945140402764}, {"id": 535, "seek": 212700, "start": 2151.0, "end": 2155.0, "text": " Now applying all of these improvements", "tokens": [51564, 823, 9275, 439, 295, 613, 13797, 51764], "temperature": 0.0, "avg_logprob": -0.14558653831481932, "compression_ratio": 1.5223880597014925, "no_speech_prob": 0.00016530945140402764}, {"id": 536, "seek": 215500, "start": 2155.0, "end": 2159.0, "text": " with our method, with the optimal tune settings for this data set", "tokens": [50364, 365, 527, 3170, 11, 365, 264, 16252, 10864, 6257, 337, 341, 1412, 992, 50564], "temperature": 0.0, "avg_logprob": -0.15498452643825583, "compression_ratio": 1.4712041884816753, "no_speech_prob": 4.3106938392156735e-05}, {"id": 537, "seek": 215500, "start": 2159.0, "end": 2163.0, "text": " we read both much better quality", "tokens": [50564, 321, 1401, 1293, 709, 1101, 3125, 50764], "temperature": 0.0, "avg_logprob": -0.15498452643825583, "compression_ratio": 1.4712041884816753, "no_speech_prob": 4.3106938392156735e-05}, {"id": 538, "seek": 215500, "start": 2163.0, "end": 2167.0, "text": " at a much faster rate.", "tokens": [50764, 412, 257, 709, 4663, 3314, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15498452643825583, "compression_ratio": 1.4712041884816753, "no_speech_prob": 4.3106938392156735e-05}, {"id": 539, "seek": 215500, "start": 2167.0, "end": 2171.0, "text": " And yeah, there's been some previous works", "tokens": [50964, 400, 1338, 11, 456, 311, 668, 512, 3894, 1985, 51164], "temperature": 0.0, "avg_logprob": -0.15498452643825583, "compression_ratio": 1.4712041884816753, "no_speech_prob": 4.3106938392156735e-05}, {"id": 540, "seek": 215500, "start": 2171.0, "end": 2175.0, "text": " that applied also higher order solvers.", "tokens": [51164, 300, 6456, 611, 2946, 1668, 1404, 840, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15498452643825583, "compression_ratio": 1.4712041884816753, "no_speech_prob": 4.3106938392156735e-05}, {"id": 541, "seek": 215500, "start": 2175.0, "end": 2179.0, "text": " So I want to highlight one result here.", "tokens": [51364, 407, 286, 528, 281, 5078, 472, 1874, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15498452643825583, "compression_ratio": 1.4712041884816753, "no_speech_prob": 4.3106938392156735e-05}, {"id": 542, "seek": 215500, "start": 2179.0, "end": 2183.0, "text": " This image net 64 highly competitive", "tokens": [51564, 639, 3256, 2533, 12145, 5405, 10043, 51764], "temperature": 0.0, "avg_logprob": -0.15498452643825583, "compression_ratio": 1.4712041884816753, "no_speech_prob": 4.3106938392156735e-05}, {"id": 543, "seek": 218300, "start": 2183.0, "end": 2187.0, "text": " just with this change of schedule.", "tokens": [50364, 445, 365, 341, 1319, 295, 7567, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13191611191322064, "compression_ratio": 1.4953703703703705, "no_speech_prob": 0.00033922854345291853}, {"id": 544, "seek": 218300, "start": 2187.0, "end": 2191.0, "text": " We went from a pretty mediocre FID of 2.07 to 1.55", "tokens": [50564, 492, 1437, 490, 257, 1238, 45415, 479, 2777, 295, 568, 13, 16231, 281, 502, 13, 13622, 50764], "temperature": 0.0, "avg_logprob": -0.13191611191322064, "compression_ratio": 1.4953703703703705, "no_speech_prob": 0.00033922854345291853}, {"id": 545, "seek": 218300, "start": 2191.0, "end": 2195.0, "text": " which at the time of getting this result was state of the art", "tokens": [50764, 597, 412, 264, 565, 295, 1242, 341, 1874, 390, 1785, 295, 264, 1523, 50964], "temperature": 0.0, "avg_logprob": -0.13191611191322064, "compression_ratio": 1.4953703703703705, "no_speech_prob": 0.00033922854345291853}, {"id": 546, "seek": 218300, "start": 2195.0, "end": 2199.0, "text": " but that record was broken before the publication", "tokens": [50964, 457, 300, 2136, 390, 5463, 949, 264, 19953, 51164], "temperature": 0.0, "avg_logprob": -0.13191611191322064, "compression_ratio": 1.4953703703703705, "no_speech_prob": 0.00033922854345291853}, {"id": 547, "seek": 218300, "start": 2199.0, "end": 2203.0, "text": " but we'll have our events in a few slides.", "tokens": [51164, 457, 321, 603, 362, 527, 3931, 294, 257, 1326, 9788, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13191611191322064, "compression_ratio": 1.4953703703703705, "no_speech_prob": 0.00033922854345291853}, {"id": 548, "seek": 218300, "start": 2203.0, "end": 2207.0, "text": " But just to show that this is just taking", "tokens": [51364, 583, 445, 281, 855, 300, 341, 307, 445, 1940, 51564], "temperature": 0.0, "avg_logprob": -0.13191611191322064, "compression_ratio": 1.4953703703703705, "no_speech_prob": 0.00033922854345291853}, {"id": 549, "seek": 218300, "start": 2207.0, "end": 2211.0, "text": " the existing network and using it better", "tokens": [51564, 264, 6741, 3209, 293, 1228, 309, 1101, 51764], "temperature": 0.0, "avg_logprob": -0.13191611191322064, "compression_ratio": 1.4953703703703705, "no_speech_prob": 0.00033922854345291853}, {"id": 550, "seek": 221100, "start": 2211.0, "end": 2215.0, "text": " for improvements already.", "tokens": [50364, 337, 13797, 1217, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13956958314646845, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0002888500748667866}, {"id": 551, "seek": 221100, "start": 2215.0, "end": 2219.0, "text": " Okay, so that's it for deterministic sampling.", "tokens": [50564, 1033, 11, 370, 300, 311, 309, 337, 15957, 3142, 21179, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13956958314646845, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0002888500748667866}, {"id": 552, "seek": 221100, "start": 2219.0, "end": 2223.0, "text": " At this point I have to say I'm going to go a bit overtime because of the hassle", "tokens": [50764, 1711, 341, 935, 286, 362, 281, 584, 286, 478, 516, 281, 352, 257, 857, 29863, 570, 295, 264, 39526, 50964], "temperature": 0.0, "avg_logprob": -0.13956958314646845, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0002888500748667866}, {"id": 553, "seek": 221100, "start": 2223.0, "end": 2227.0, "text": " in the beginning and because this is kind of incompressible anyway.", "tokens": [50964, 294, 264, 2863, 293, 570, 341, 307, 733, 295, 40393, 735, 964, 4033, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13956958314646845, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0002888500748667866}, {"id": 554, "seek": 221100, "start": 2227.0, "end": 2231.0, "text": " So if you need to leave then no problem.", "tokens": [51164, 407, 498, 291, 643, 281, 1856, 550, 572, 1154, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13956958314646845, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0002888500748667866}, {"id": 555, "seek": 221100, "start": 2231.0, "end": 2235.0, "text": " So yeah, that's it for stagastic sampling", "tokens": [51364, 407, 1338, 11, 300, 311, 309, 337, 342, 559, 2750, 21179, 51564], "temperature": 0.0, "avg_logprob": -0.13956958314646845, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0002888500748667866}, {"id": 556, "seek": 221100, "start": 2235.0, "end": 2239.0, "text": " and for sampling as a whole.", "tokens": [51564, 293, 337, 21179, 382, 257, 1379, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13956958314646845, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0002888500748667866}, {"id": 557, "seek": 223900, "start": 2239.0, "end": 2243.0, "text": " So just a brief recap.", "tokens": [50364, 407, 445, 257, 5353, 20928, 13, 50564], "temperature": 0.0, "avg_logprob": -0.33355577441229334, "compression_ratio": 2.2212389380530975, "no_speech_prob": 0.00021093069517519325}, {"id": 558, "seek": 223900, "start": 2243.0, "end": 2247.0, "text": " The way this works was that the role of the ODE", "tokens": [50564, 440, 636, 341, 1985, 390, 300, 264, 3090, 295, 264, 422, 22296, 50764], "temperature": 0.0, "avg_logprob": -0.33355577441229334, "compression_ratio": 2.2212389380530975, "no_speech_prob": 0.00021093069517519325}, {"id": 559, "seek": 223900, "start": 2247.0, "end": 2251.0, "text": " is to give us the step direction", "tokens": [50764, 307, 281, 976, 505, 264, 1823, 3513, 50964], "temperature": 0.0, "avg_logprob": -0.33355577441229334, "compression_ratio": 2.2212389380530975, "no_speech_prob": 0.00021093069517519325}, {"id": 560, "seek": 223900, "start": 2251.0, "end": 2255.0, "text": " which is given by the score function", "tokens": [50964, 597, 307, 2212, 538, 264, 6175, 2445, 51164], "temperature": 0.0, "avg_logprob": -0.33355577441229334, "compression_ratio": 2.2212389380530975, "no_speech_prob": 0.00021093069517519325}, {"id": 561, "seek": 223900, "start": 2255.0, "end": 2259.0, "text": " which is given by the score function", "tokens": [51164, 597, 307, 2212, 538, 264, 6175, 2445, 51364], "temperature": 0.0, "avg_logprob": -0.33355577441229334, "compression_ratio": 2.2212389380530975, "no_speech_prob": 0.00021093069517519325}, {"id": 562, "seek": 223900, "start": 2259.0, "end": 2263.0, "text": " which is given by the score function", "tokens": [51364, 597, 307, 2212, 538, 264, 6175, 2445, 51564], "temperature": 0.0, "avg_logprob": -0.33355577441229334, "compression_ratio": 2.2212389380530975, "no_speech_prob": 0.00021093069517519325}, {"id": 563, "seek": 223900, "start": 2263.0, "end": 2267.0, "text": " which is given by the score function", "tokens": [51564, 597, 307, 2212, 538, 264, 6175, 2445, 51764], "temperature": 0.0, "avg_logprob": -0.33355577441229334, "compression_ratio": 2.2212389380530975, "no_speech_prob": 0.00021093069517519325}, {"id": 564, "seek": 226700, "start": 2267.0, "end": 2271.0, "text": " which can be evaluated using a denoiser", "tokens": [50364, 597, 393, 312, 25509, 1228, 257, 1441, 78, 6694, 50564], "temperature": 0.0, "avg_logprob": -0.0903090846781828, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.627523983595893e-05}, {"id": 565, "seek": 226700, "start": 2271.0, "end": 2275.0, "text": " which can be approximated using the neural network", "tokens": [50564, 597, 393, 312, 8542, 770, 1228, 264, 18161, 3209, 50764], "temperature": 0.0, "avg_logprob": -0.0903090846781828, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.627523983595893e-05}, {"id": 566, "seek": 226700, "start": 2275.0, "end": 2279.0, "text": " and that is the role of the neural network.", "tokens": [50764, 293, 300, 307, 264, 3090, 295, 264, 18161, 3209, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0903090846781828, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.627523983595893e-05}, {"id": 567, "seek": 226700, "start": 2279.0, "end": 2283.0, "text": " It tells you where to go in a single step or what's the direction you need to go to.", "tokens": [50964, 467, 5112, 291, 689, 281, 352, 294, 257, 2167, 1823, 420, 437, 311, 264, 3513, 291, 643, 281, 352, 281, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0903090846781828, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.627523983595893e-05}, {"id": 568, "seek": 226700, "start": 2283.0, "end": 2287.0, "text": " And the theory says that as long as the denoiser does something", "tokens": [51164, 400, 264, 5261, 1619, 300, 382, 938, 382, 264, 1441, 78, 6694, 775, 746, 51364], "temperature": 0.0, "avg_logprob": -0.0903090846781828, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.627523983595893e-05}, {"id": 569, "seek": 226700, "start": 2287.0, "end": 2291.0, "text": " that minimizes this loss, the L2 denoising loss", "tokens": [51364, 300, 4464, 5660, 341, 4470, 11, 264, 441, 17, 1441, 78, 3436, 4470, 51564], "temperature": 0.0, "avg_logprob": -0.0903090846781828, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.627523983595893e-05}, {"id": 570, "seek": 226700, "start": 2291.0, "end": 2295.0, "text": " the theory will be happy.", "tokens": [51564, 264, 5261, 486, 312, 2055, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0903090846781828, "compression_ratio": 1.733009708737864, "no_speech_prob": 3.627523983595893e-05}, {"id": 571, "seek": 229500, "start": 2295.0, "end": 2299.0, "text": " And you can do this separately at every noise level", "tokens": [50364, 400, 291, 393, 360, 341, 14759, 412, 633, 5658, 1496, 50564], "temperature": 0.0, "avg_logprob": -0.12968385444497163, "compression_ratio": 1.7167381974248928, "no_speech_prob": 6.859355926280841e-05}, {"id": 572, "seek": 229500, "start": 2299.0, "end": 2303.0, "text": " so you can weight these loss according to the noise level also.", "tokens": [50564, 370, 291, 393, 3364, 613, 4470, 4650, 281, 264, 5658, 1496, 611, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12968385444497163, "compression_ratio": 1.7167381974248928, "no_speech_prob": 6.859355926280841e-05}, {"id": 573, "seek": 229500, "start": 2303.0, "end": 2307.0, "text": " But before we go to these loss weightings", "tokens": [50764, 583, 949, 321, 352, 281, 613, 4470, 3364, 1109, 50964], "temperature": 0.0, "avg_logprob": -0.12968385444497163, "compression_ratio": 1.7167381974248928, "no_speech_prob": 6.859355926280841e-05}, {"id": 574, "seek": 229500, "start": 2307.0, "end": 2311.0, "text": " let's look at the denoiser itself.", "tokens": [50964, 718, 311, 574, 412, 264, 1441, 78, 6694, 2564, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12968385444497163, "compression_ratio": 1.7167381974248928, "no_speech_prob": 6.859355926280841e-05}, {"id": 575, "seek": 229500, "start": 2311.0, "end": 2315.0, "text": " So I draw the CNN there in a bit of a hazy way", "tokens": [51164, 407, 286, 2642, 264, 24859, 456, 294, 257, 857, 295, 257, 324, 1229, 636, 51364], "temperature": 0.0, "avg_logprob": -0.12968385444497163, "compression_ratio": 1.7167381974248928, "no_speech_prob": 6.859355926280841e-05}, {"id": 576, "seek": 229500, "start": 2315.0, "end": 2319.0, "text": " and this is because it's actually a bad idea to directly connect the noise image to the input of the network", "tokens": [51364, 293, 341, 307, 570, 309, 311, 767, 257, 1578, 1558, 281, 3838, 1745, 264, 5658, 3256, 281, 264, 4846, 295, 264, 3209, 51564], "temperature": 0.0, "avg_logprob": -0.12968385444497163, "compression_ratio": 1.7167381974248928, "no_speech_prob": 6.859355926280841e-05}, {"id": 577, "seek": 229500, "start": 2319.0, "end": 2323.0, "text": " or to read the denoised image from its output layer", "tokens": [51564, 420, 281, 1401, 264, 1441, 78, 2640, 3256, 490, 1080, 5598, 4583, 51764], "temperature": 0.0, "avg_logprob": -0.12968385444497163, "compression_ratio": 1.7167381974248928, "no_speech_prob": 6.859355926280841e-05}, {"id": 578, "seek": 232300, "start": 2323.0, "end": 2329.0, "text": " rather we'll want to wrap it between some kind of signal management layers", "tokens": [50364, 2831, 321, 603, 528, 281, 7019, 309, 1296, 512, 733, 295, 6358, 4592, 7914, 50664], "temperature": 0.0, "avg_logprob": -0.09030604869761366, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.00041544027044437826}, {"id": 579, "seek": 232300, "start": 2329.0, "end": 2333.0, "text": " to manage those signal scales", "tokens": [50664, 281, 3067, 729, 6358, 17408, 50864], "temperature": 0.0, "avg_logprob": -0.09030604869761366, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.00041544027044437826}, {"id": 580, "seek": 232300, "start": 2333.0, "end": 2337.0, "text": " of both the input and the output to standardize them somehow", "tokens": [50864, 295, 1293, 264, 4846, 293, 264, 5598, 281, 3832, 1125, 552, 6063, 51064], "temperature": 0.0, "avg_logprob": -0.09030604869761366, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.00041544027044437826}, {"id": 581, "seek": 232300, "start": 2337.0, "end": 2341.0, "text": " and also in this case we can often recycle stuff from the input", "tokens": [51064, 293, 611, 294, 341, 1389, 321, 393, 2049, 32162, 1507, 490, 264, 4846, 51264], "temperature": 0.0, "avg_logprob": -0.09030604869761366, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.00041544027044437826}, {"id": 582, "seek": 232300, "start": 2341.0, "end": 2345.0, "text": " because let's say if the input image is almost noise free", "tokens": [51264, 570, 718, 311, 584, 498, 264, 4846, 3256, 307, 1920, 5658, 1737, 51464], "temperature": 0.0, "avg_logprob": -0.09030604869761366, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.00041544027044437826}, {"id": 583, "seek": 232300, "start": 2345.0, "end": 2347.0, "text": " then we don't really need to denoise much", "tokens": [51464, 550, 321, 500, 380, 534, 643, 281, 1441, 38800, 709, 51564], "temperature": 0.0, "avg_logprob": -0.09030604869761366, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.00041544027044437826}, {"id": 584, "seek": 232300, "start": 2347.0, "end": 2351.0, "text": " we should just copy what we know and only fix the remainder", "tokens": [51564, 321, 820, 445, 5055, 437, 321, 458, 293, 787, 3191, 264, 29837, 51764], "temperature": 0.0, "avg_logprob": -0.09030604869761366, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.00041544027044437826}, {"id": 585, "seek": 235100, "start": 2351.0, "end": 2355.0, "text": " we're going to come to that soon.", "tokens": [50364, 321, 434, 516, 281, 808, 281, 300, 2321, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15121120145951195, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0005905990838073194}, {"id": 586, "seek": 235100, "start": 2355.0, "end": 2359.0, "text": " And this is super critical here.", "tokens": [50564, 400, 341, 307, 1687, 4924, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15121120145951195, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0005905990838073194}, {"id": 587, "seek": 235100, "start": 2359.0, "end": 2363.0, "text": " I mean this might sound like boring technical details", "tokens": [50764, 286, 914, 341, 1062, 1626, 411, 9989, 6191, 4365, 50964], "temperature": 0.0, "avg_logprob": -0.15121120145951195, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0005905990838073194}, {"id": 588, "seek": 235100, "start": 2363.0, "end": 2367.0, "text": " but these kind of things really are critical for the success of the neural network training", "tokens": [50964, 457, 613, 733, 295, 721, 534, 366, 4924, 337, 264, 2245, 295, 264, 18161, 3209, 3097, 51164], "temperature": 0.0, "avg_logprob": -0.15121120145951195, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0005905990838073194}, {"id": 589, "seek": 235100, "start": 2367.0, "end": 2371.0, "text": " and we've seen this over and over again over the years.", "tokens": [51164, 293, 321, 600, 1612, 341, 670, 293, 670, 797, 670, 264, 924, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15121120145951195, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0005905990838073194}, {"id": 590, "seek": 235100, "start": 2371.0, "end": 2375.0, "text": " And in this case the noise levels vary so widely", "tokens": [51364, 400, 294, 341, 1389, 264, 5658, 4358, 10559, 370, 13371, 51564], "temperature": 0.0, "avg_logprob": -0.15121120145951195, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0005905990838073194}, {"id": 591, "seek": 235100, "start": 2375.0, "end": 2379.0, "text": " that this is extra critical here.", "tokens": [51564, 300, 341, 307, 2857, 4924, 510, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15121120145951195, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0005905990838073194}, {"id": 592, "seek": 237900, "start": 2379.0, "end": 2383.0, "text": " So without too much to do here is how one of the previous methods, the VE method", "tokens": [50364, 407, 1553, 886, 709, 281, 360, 510, 307, 577, 472, 295, 264, 3894, 7150, 11, 264, 691, 36, 3170, 50564], "temperature": 0.0, "avg_logprob": -0.13179821968078614, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.00019148345745634288}, {"id": 593, "seek": 237900, "start": 2383.0, "end": 2387.0, "text": " implements the denoiser.", "tokens": [50564, 704, 17988, 264, 1441, 78, 6694, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13179821968078614, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.00019148345745634288}, {"id": 594, "seek": 237900, "start": 2387.0, "end": 2391.0, "text": " So the idea of this setup is that they are", "tokens": [50764, 407, 264, 1558, 295, 341, 8657, 307, 300, 436, 366, 50964], "temperature": 0.0, "avg_logprob": -0.13179821968078614, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.00019148345745634288}, {"id": 595, "seek": 237900, "start": 2391.0, "end": 2395.0, "text": " learning to predict the noise instead of the signal using those CNN layers.", "tokens": [50964, 2539, 281, 6069, 264, 5658, 2602, 295, 264, 6358, 1228, 729, 24859, 7914, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13179821968078614, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.00019148345745634288}, {"id": 596, "seek": 237900, "start": 2395.0, "end": 2399.0, "text": " And the way that works, and I'll explain why soon", "tokens": [51164, 400, 264, 636, 300, 1985, 11, 293, 286, 603, 2903, 983, 2321, 51364], "temperature": 0.0, "avg_logprob": -0.13179821968078614, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.00019148345745634288}, {"id": 597, "seek": 237900, "start": 2399.0, "end": 2403.0, "text": " the way that works is of course the loss will be happy", "tokens": [51364, 264, 636, 300, 1985, 307, 295, 1164, 264, 4470, 486, 312, 2055, 51564], "temperature": 0.0, "avg_logprob": -0.13179821968078614, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.00019148345745634288}, {"id": 598, "seek": 237900, "start": 2403.0, "end": 2407.0, "text": " if the denoiser can produce the clean image", "tokens": [51564, 498, 264, 1441, 78, 6694, 393, 5258, 264, 2541, 3256, 51764], "temperature": 0.0, "avg_logprob": -0.13179821968078614, "compression_ratio": 1.6577777777777778, "no_speech_prob": 0.00019148345745634288}, {"id": 599, "seek": 240700, "start": 2407.0, "end": 2413.0, "text": " and we can interpret this model as having this kind of a skip connection", "tokens": [50364, 293, 321, 393, 7302, 341, 2316, 382, 1419, 341, 733, 295, 257, 10023, 4984, 50664], "temperature": 0.0, "avg_logprob": -0.08431468226692894, "compression_ratio": 1.6940639269406392, "no_speech_prob": 1.7323505744570866e-05}, {"id": 600, "seek": 240700, "start": 2413.0, "end": 2417.0, "text": " so the noisy input goes through that.", "tokens": [50664, 370, 264, 24518, 4846, 1709, 807, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08431468226692894, "compression_ratio": 1.6940639269406392, "no_speech_prob": 1.7323505744570866e-05}, {"id": 601, "seek": 240700, "start": 2417.0, "end": 2421.0, "text": " Now implicitly the task of the CNN will be to predict", "tokens": [50864, 823, 26947, 356, 264, 5633, 295, 264, 24859, 486, 312, 281, 6069, 51064], "temperature": 0.0, "avg_logprob": -0.08431468226692894, "compression_ratio": 1.6940639269406392, "no_speech_prob": 1.7323505744570866e-05}, {"id": 602, "seek": 240700, "start": 2421.0, "end": 2425.0, "text": " the negative of the noise component in that image", "tokens": [51064, 264, 3671, 295, 264, 5658, 6542, 294, 300, 3256, 51264], "temperature": 0.0, "avg_logprob": -0.08431468226692894, "compression_ratio": 1.6940639269406392, "no_speech_prob": 1.7323505744570866e-05}, {"id": 603, "seek": 240700, "start": 2425.0, "end": 2431.0, "text": " and then they have an explicit layer that scales that noise to the known noise level.", "tokens": [51264, 293, 550, 436, 362, 364, 13691, 4583, 300, 17408, 300, 5658, 281, 264, 2570, 5658, 1496, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08431468226692894, "compression_ratio": 1.6940639269406392, "no_speech_prob": 1.7323505744570866e-05}, {"id": 604, "seek": 240700, "start": 2431.0, "end": 2435.0, "text": " And so now when you add whatever came from the skip connection to this", "tokens": [51564, 400, 370, 586, 562, 291, 909, 2035, 1361, 490, 264, 10023, 4984, 281, 341, 51764], "temperature": 0.0, "avg_logprob": -0.08431468226692894, "compression_ratio": 1.6940639269406392, "no_speech_prob": 1.7323505744570866e-05}, {"id": 605, "seek": 243500, "start": 2435.0, "end": 2439.0, "text": " you get an estimate of the clean image.", "tokens": [50364, 291, 483, 364, 12539, 295, 264, 2541, 3256, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08923489114512569, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0004478630726225674}, {"id": 606, "seek": 243500, "start": 2439.0, "end": 2443.0, "text": " So this way they kind of turn it so that the CNN itself", "tokens": [50564, 407, 341, 636, 436, 733, 295, 1261, 309, 370, 300, 264, 24859, 2564, 50764], "temperature": 0.0, "avg_logprob": -0.08923489114512569, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0004478630726225674}, {"id": 607, "seek": 243500, "start": 2443.0, "end": 2447.0, "text": " is concerned with the noise instead of like the signal.", "tokens": [50764, 307, 5922, 365, 264, 5658, 2602, 295, 411, 264, 6358, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08923489114512569, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0004478630726225674}, {"id": 608, "seek": 243500, "start": 2447.0, "end": 2451.0, "text": " I'll explain soon why that is relevant.", "tokens": [50964, 286, 603, 2903, 2321, 983, 300, 307, 7340, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08923489114512569, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0004478630726225674}, {"id": 609, "seek": 243500, "start": 2451.0, "end": 2455.0, "text": " But first let's do the thing I promised to do a long ago.", "tokens": [51164, 583, 700, 718, 311, 360, 264, 551, 286, 10768, 281, 360, 257, 938, 2057, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08923489114512569, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0004478630726225674}, {"id": 610, "seek": 243500, "start": 2455.0, "end": 2459.0, "text": " I said there's huge variations in the magnitude, just the numerical magnitude", "tokens": [51364, 286, 848, 456, 311, 2603, 17840, 294, 264, 15668, 11, 445, 264, 29054, 15668, 51564], "temperature": 0.0, "avg_logprob": -0.08923489114512569, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0004478630726225674}, {"id": 611, "seek": 243500, "start": 2459.0, "end": 2463.0, "text": " of these input signals.", "tokens": [51564, 295, 613, 4846, 12354, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08923489114512569, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0004478630726225674}, {"id": 612, "seek": 246300, "start": 2463.0, "end": 2467.0, "text": " So fairs to accounts for that, which is problematic.", "tokens": [50364, 407, 3143, 82, 281, 9402, 337, 300, 11, 597, 307, 19011, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15531124607209237, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.00015617469034623355}, {"id": 613, "seek": 246300, "start": 2467.0, "end": 2471.0, "text": " And so we quite simply introduced this", "tokens": [50564, 400, 370, 321, 1596, 2935, 7268, 341, 50764], "temperature": 0.0, "avg_logprob": -0.15531124607209237, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.00015617469034623355}, {"id": 614, "seek": 246300, "start": 2471.0, "end": 2475.0, "text": " interscaling layer here that uses the known standard deviation of the noise", "tokens": [50764, 728, 4417, 4270, 4583, 510, 300, 4960, 264, 2570, 3832, 25163, 295, 264, 5658, 50964], "temperature": 0.0, "avg_logprob": -0.15531124607209237, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.00015617469034623355}, {"id": 615, "seek": 246300, "start": 2475.0, "end": 2479.0, "text": " to scale the image down.", "tokens": [50964, 281, 4373, 264, 3256, 760, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15531124607209237, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.00015617469034623355}, {"id": 616, "seek": 246300, "start": 2479.0, "end": 2483.0, "text": " I want to highlight this not like a batch normalization or something.", "tokens": [51164, 286, 528, 281, 5078, 341, 406, 411, 257, 15245, 2710, 2144, 420, 746, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15531124607209237, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.00015617469034623355}, {"id": 617, "seek": 246300, "start": 2483.0, "end": 2487.0, "text": " We know what the noise level is, we know what the signal magnitude should be.", "tokens": [51364, 492, 458, 437, 264, 5658, 1496, 307, 11, 321, 458, 437, 264, 6358, 15668, 820, 312, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15531124607209237, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.00015617469034623355}, {"id": 618, "seek": 246300, "start": 2487.0, "end": 2491.0, "text": " We divide by an appropriate formula.", "tokens": [51564, 492, 9845, 538, 364, 6854, 8513, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15531124607209237, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.00015617469034623355}, {"id": 619, "seek": 249100, "start": 2491.0, "end": 2496.0, "text": " So that gives you one of the wishes we had on that orientation slide.", "tokens": [50364, 407, 300, 2709, 291, 472, 295, 264, 15065, 321, 632, 322, 300, 14764, 4137, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14038429052933402, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.00010816867870744318}, {"id": 620, "seek": 249100, "start": 2496.0, "end": 2500.0, "text": " On the output side we actually have something nice already.", "tokens": [50614, 1282, 264, 5598, 1252, 321, 767, 362, 746, 1481, 1217, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14038429052933402, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.00010816867870744318}, {"id": 621, "seek": 249100, "start": 2500.0, "end": 2505.0, "text": " So this is very good because now the network only needs to produce", "tokens": [50814, 407, 341, 307, 588, 665, 570, 586, 264, 3209, 787, 2203, 281, 5258, 51064], "temperature": 0.0, "avg_logprob": -0.14038429052933402, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.00010816867870744318}, {"id": 622, "seek": 249100, "start": 2505.0, "end": 2509.0, "text": " a unit standard deviation output and this explicit scaling to the known noise level", "tokens": [51064, 257, 4985, 3832, 25163, 5598, 293, 341, 13691, 21589, 281, 264, 2570, 5658, 1496, 51264], "temperature": 0.0, "avg_logprob": -0.14038429052933402, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.00010816867870744318}, {"id": 623, "seek": 249100, "start": 2509.0, "end": 2514.0, "text": " takes care of applying the actual like a magnitude of that noise.", "tokens": [51264, 2516, 1127, 295, 9275, 264, 3539, 411, 257, 15668, 295, 300, 5658, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14038429052933402, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.00010816867870744318}, {"id": 624, "seek": 249100, "start": 2514.0, "end": 2517.0, "text": " So this makes it again much easier for the neural network.", "tokens": [51514, 407, 341, 1669, 309, 797, 709, 3571, 337, 264, 18161, 3209, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14038429052933402, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.00010816867870744318}, {"id": 625, "seek": 251700, "start": 2517.0, "end": 2521.0, "text": " It can always work with these standard sized signals.", "tokens": [50364, 467, 393, 1009, 589, 365, 613, 3832, 20004, 12354, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15278005599975586, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00023334323486778885}, {"id": 626, "seek": 251700, "start": 2521.0, "end": 2525.0, "text": " And that deals with the second hope we have there.", "tokens": [50564, 400, 300, 11215, 365, 264, 1150, 1454, 321, 362, 456, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15278005599975586, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00023334323486778885}, {"id": 627, "seek": 251700, "start": 2525.0, "end": 2531.0, "text": " But now the question of should we predict the noise or the signal and why?", "tokens": [50764, 583, 586, 264, 1168, 295, 820, 321, 6069, 264, 5658, 420, 264, 6358, 293, 983, 30, 51064], "temperature": 0.0, "avg_logprob": -0.15278005599975586, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00023334323486778885}, {"id": 628, "seek": 251700, "start": 2531.0, "end": 2535.0, "text": " So it turns out this is actually a good idea at small noise levels", "tokens": [51064, 407, 309, 4523, 484, 341, 307, 767, 257, 665, 1558, 412, 1359, 5658, 4358, 51264], "temperature": 0.0, "avg_logprob": -0.15278005599975586, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00023334323486778885}, {"id": 629, "seek": 251700, "start": 2535.0, "end": 2539.0, "text": " but a bad idea at high noise levels.", "tokens": [51264, 457, 257, 1578, 1558, 412, 1090, 5658, 4358, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15278005599975586, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00023334323486778885}, {"id": 630, "seek": 251700, "start": 2539.0, "end": 2543.0, "text": " So I'll show you what happens at low noise levels.", "tokens": [51464, 407, 286, 603, 855, 291, 437, 2314, 412, 2295, 5658, 4358, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15278005599975586, "compression_ratio": 1.6372549019607843, "no_speech_prob": 0.00023334323486778885}, {"id": 631, "seek": 254300, "start": 2543.0, "end": 2547.0, "text": " So if we have low noise, the stuff that goes through the skip connection", "tokens": [50364, 407, 498, 321, 362, 2295, 5658, 11, 264, 1507, 300, 1709, 807, 264, 10023, 4984, 50564], "temperature": 0.0, "avg_logprob": -0.12114728700130357, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0001209975453093648}, {"id": 632, "seek": 254300, "start": 2547.0, "end": 2550.0, "text": " is almost noise free already.", "tokens": [50564, 307, 1920, 5658, 1737, 1217, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12114728700130357, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0001209975453093648}, {"id": 633, "seek": 254300, "start": 2550.0, "end": 2553.0, "text": " And now the CNN predicts this negative noise component", "tokens": [50714, 400, 586, 264, 24859, 6069, 82, 341, 3671, 5658, 6542, 50864], "temperature": 0.0, "avg_logprob": -0.12114728700130357, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0001209975453093648}, {"id": 634, "seek": 254300, "start": 2553.0, "end": 2557.0, "text": " and it's scaled down by this very low noise level.", "tokens": [50864, 293, 309, 311, 36039, 760, 538, 341, 588, 2295, 5658, 1496, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12114728700130357, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0001209975453093648}, {"id": 635, "seek": 254300, "start": 2557.0, "end": 2563.0, "text": " And this is great because the neural network", "tokens": [51064, 400, 341, 307, 869, 570, 264, 18161, 3209, 51364], "temperature": 0.0, "avg_logprob": -0.12114728700130357, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0001209975453093648}, {"id": 636, "seek": 254300, "start": 2563.0, "end": 2566.0, "text": " is actually the only source of error in this process.", "tokens": [51364, 307, 767, 264, 787, 4009, 295, 6713, 294, 341, 1399, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12114728700130357, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0001209975453093648}, {"id": 637, "seek": 254300, "start": 2566.0, "end": 2569.0, "text": " So if the network made errors, now we've downscaled them.", "tokens": [51514, 407, 498, 264, 3209, 1027, 13603, 11, 586, 321, 600, 760, 4417, 5573, 552, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12114728700130357, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0001209975453093648}, {"id": 638, "seek": 254300, "start": 2569.0, "end": 2572.0, "text": " So it doesn't really matter if the network is good or bad.", "tokens": [51664, 407, 309, 1177, 380, 534, 1871, 498, 264, 3209, 307, 665, 420, 1578, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12114728700130357, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0001209975453093648}, {"id": 639, "seek": 257200, "start": 2572.0, "end": 2575.0, "text": " We didn't do much error in this case.", "tokens": [50364, 492, 994, 380, 360, 709, 6713, 294, 341, 1389, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07628153352176442, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00015884843014646322}, {"id": 640, "seek": 257200, "start": 2575.0, "end": 2578.0, "text": " So that's great. We are sort of recycling what we already knew", "tokens": [50514, 407, 300, 311, 869, 13, 492, 366, 1333, 295, 23363, 437, 321, 1217, 2586, 50664], "temperature": 0.0, "avg_logprob": -0.07628153352176442, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00015884843014646322}, {"id": 641, "seek": 257200, "start": 2578.0, "end": 2583.0, "text": " instead of trying to learn the identity function within your network.", "tokens": [50664, 2602, 295, 1382, 281, 1466, 264, 6575, 2445, 1951, 428, 3209, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07628153352176442, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00015884843014646322}, {"id": 642, "seek": 257200, "start": 2583.0, "end": 2587.0, "text": " So that kind of deals with the third hope we had on the slide.", "tokens": [50914, 407, 300, 733, 295, 11215, 365, 264, 2636, 1454, 321, 632, 322, 264, 4137, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07628153352176442, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00015884843014646322}, {"id": 643, "seek": 257200, "start": 2587.0, "end": 2591.0, "text": " But on high noise levels, the situation is reversed.", "tokens": [51114, 583, 322, 1090, 5658, 4358, 11, 264, 2590, 307, 30563, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07628153352176442, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00015884843014646322}, {"id": 644, "seek": 257200, "start": 2591.0, "end": 2594.0, "text": " Whatever comes through the skip connection is completely useless.", "tokens": [51314, 8541, 1487, 807, 264, 10023, 4984, 307, 2584, 14115, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07628153352176442, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00015884843014646322}, {"id": 645, "seek": 257200, "start": 2594.0, "end": 2599.0, "text": " It's a huge, huge noise signal with no signal at all.", "tokens": [51464, 467, 311, 257, 2603, 11, 2603, 5658, 6358, 365, 572, 6358, 412, 439, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07628153352176442, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00015884843014646322}, {"id": 646, "seek": 259900, "start": 2599.0, "end": 2602.0, "text": " And now the CNN predicts what the noise is.", "tokens": [50364, 400, 586, 264, 24859, 6069, 82, 437, 264, 5658, 307, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09996348028784399, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.00016805698396638036}, {"id": 647, "seek": 259900, "start": 2602.0, "end": 2605.0, "text": " And then it is massively boosted at this stage.", "tokens": [50514, 400, 550, 309, 307, 29379, 9194, 292, 412, 341, 3233, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09996348028784399, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.00016805698396638036}, {"id": 648, "seek": 259900, "start": 2605.0, "end": 2610.0, "text": " So if the network made any errors, now there are going to be huge errors after this.", "tokens": [50664, 407, 498, 264, 3209, 1027, 604, 13603, 11, 586, 456, 366, 516, 281, 312, 2603, 13603, 934, 341, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09996348028784399, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.00016805698396638036}, {"id": 649, "seek": 259900, "start": 2610.0, "end": 2613.0, "text": " And those are directly passed out of the denoiser.", "tokens": [50914, 400, 729, 366, 3838, 4678, 484, 295, 264, 1441, 78, 6694, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09996348028784399, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.00016805698396638036}, {"id": 650, "seek": 259900, "start": 2613.0, "end": 2620.0, "text": " So now we've introduced a huge error into our stepping procedure in the ODE.", "tokens": [51064, 407, 586, 321, 600, 7268, 257, 2603, 6713, 666, 527, 16821, 10747, 294, 264, 422, 22296, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09996348028784399, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.00016805698396638036}, {"id": 651, "seek": 259900, "start": 2620.0, "end": 2624.0, "text": " It's also a bit of an absurd task because you're trying to subtract", "tokens": [51414, 467, 311, 611, 257, 857, 295, 364, 19774, 5633, 570, 291, 434, 1382, 281, 16390, 51614], "temperature": 0.0, "avg_logprob": -0.09996348028784399, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.00016805698396638036}, {"id": 652, "seek": 259900, "start": 2624.0, "end": 2628.0, "text": " two massive signals to get a normal size signal.", "tokens": [51614, 732, 5994, 12354, 281, 483, 257, 2710, 2744, 6358, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09996348028784399, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.00016805698396638036}, {"id": 653, "seek": 262800, "start": 2628.0, "end": 2636.0, "text": " And kind of like trying to draw without two meter long pencil, not optimal.", "tokens": [50364, 400, 733, 295, 411, 1382, 281, 2642, 1553, 732, 9255, 938, 10985, 11, 406, 16252, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12149953842163086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.042766810627654e-05}, {"id": 654, "seek": 262800, "start": 2636.0, "end": 2639.0, "text": " So instead what we'd like to do is somehow disable the skip connection", "tokens": [50764, 407, 2602, 437, 321, 1116, 411, 281, 360, 307, 6063, 28362, 264, 10023, 4984, 50914], "temperature": 0.0, "avg_logprob": -0.12149953842163086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.042766810627654e-05}, {"id": 655, "seek": 262800, "start": 2639.0, "end": 2642.0, "text": " when the noise level is high.", "tokens": [50914, 562, 264, 5658, 1496, 307, 1090, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12149953842163086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.042766810627654e-05}, {"id": 656, "seek": 262800, "start": 2642.0, "end": 2648.0, "text": " And in that case, effectively the task of the CNN will be to just break the signal directly.", "tokens": [51064, 400, 294, 300, 1389, 11, 8659, 264, 5633, 295, 264, 24859, 486, 312, 281, 445, 1821, 264, 6358, 3838, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12149953842163086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.042766810627654e-05}, {"id": 657, "seek": 262800, "start": 2648.0, "end": 2650.0, "text": " There won't be any need to scale it up.", "tokens": [51364, 821, 1582, 380, 312, 604, 643, 281, 4373, 309, 493, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12149953842163086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.042766810627654e-05}, {"id": 658, "seek": 262800, "start": 2650.0, "end": 2653.0, "text": " So we won't end up boosting errors.", "tokens": [51464, 407, 321, 1582, 380, 917, 493, 43117, 13603, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12149953842163086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.042766810627654e-05}, {"id": 659, "seek": 262800, "start": 2653.0, "end": 2657.0, "text": " And the way we implement that is by adding this sort of switch", "tokens": [51614, 400, 264, 636, 321, 4445, 300, 307, 538, 5127, 341, 1333, 295, 3679, 51814], "temperature": 0.0, "avg_logprob": -0.12149953842163086, "compression_ratio": 1.5813953488372092, "no_speech_prob": 7.042766810627654e-05}, {"id": 660, "seek": 265700, "start": 2657.0, "end": 2659.0, "text": " but in a continuous way.", "tokens": [50364, 457, 294, 257, 10957, 636, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09991836547851562, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00013207503070589155}, {"id": 661, "seek": 265700, "start": 2659.0, "end": 2663.0, "text": " So we have this so-called skip scale, which when set to zero,", "tokens": [50464, 407, 321, 362, 341, 370, 12, 11880, 10023, 4373, 11, 597, 562, 992, 281, 4018, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09991836547851562, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00013207503070589155}, {"id": 662, "seek": 265700, "start": 2663.0, "end": 2665.0, "text": " effectively disables the skip connection.", "tokens": [50664, 8659, 717, 2965, 264, 10023, 4984, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09991836547851562, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00013207503070589155}, {"id": 663, "seek": 265700, "start": 2665.0, "end": 2668.0, "text": " Set to one, you get the noise prediction.", "tokens": [50764, 8928, 281, 472, 11, 291, 483, 264, 5658, 17630, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09991836547851562, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00013207503070589155}, {"id": 664, "seek": 265700, "start": 2668.0, "end": 2672.0, "text": " And furthermore, we make it so that it's actually a continuous value", "tokens": [50914, 400, 3052, 3138, 11, 321, 652, 309, 370, 300, 309, 311, 767, 257, 10957, 2158, 51114], "temperature": 0.0, "avg_logprob": -0.09991836547851562, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00013207503070589155}, {"id": 665, "seek": 265700, "start": 2672.0, "end": 2676.0, "text": " between zero and one that depends on the noise level.", "tokens": [51114, 1296, 4018, 293, 472, 300, 5946, 322, 264, 5658, 1496, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09991836547851562, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00013207503070589155}, {"id": 666, "seek": 265700, "start": 2676.0, "end": 2681.0, "text": " And if it's somewhere in between, that means that we are predicting", "tokens": [51314, 400, 498, 309, 311, 4079, 294, 1296, 11, 300, 1355, 300, 321, 366, 32884, 51564], "temperature": 0.0, "avg_logprob": -0.09991836547851562, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.00013207503070589155}, {"id": 667, "seek": 268100, "start": 2681.0, "end": 2690.0, "text": " some kind of a mixture of noise and the signal in this instead of just one of them.", "tokens": [50364, 512, 733, 295, 257, 9925, 295, 5658, 293, 264, 6358, 294, 341, 2602, 295, 445, 472, 295, 552, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11172844451150776, "compression_ratio": 1.5729166666666667, "no_speech_prob": 9.942227188730612e-05}, {"id": 668, "seek": 268100, "start": 2690.0, "end": 2697.0, "text": " And there is a principle way of calculating what the optimal skip weight is.", "tokens": [50814, 400, 456, 307, 257, 8665, 636, 295, 28258, 437, 264, 16252, 10023, 3364, 307, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11172844451150776, "compression_ratio": 1.5729166666666667, "no_speech_prob": 9.942227188730612e-05}, {"id": 669, "seek": 268100, "start": 2697.0, "end": 2701.0, "text": " But I won't go there in the interest of time.", "tokens": [51164, 583, 286, 1582, 380, 352, 456, 294, 264, 1179, 295, 565, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11172844451150776, "compression_ratio": 1.5729166666666667, "no_speech_prob": 9.942227188730612e-05}, {"id": 670, "seek": 268100, "start": 2701.0, "end": 2705.0, "text": " We have it in the paper of Enix.", "tokens": [51364, 492, 362, 309, 294, 264, 3035, 295, 2193, 970, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11172844451150776, "compression_ratio": 1.5729166666666667, "no_speech_prob": 9.942227188730612e-05}, {"id": 671, "seek": 268100, "start": 2705.0, "end": 2710.0, "text": " And that deals with the remaining issues we had on this slide.", "tokens": [51564, 400, 300, 11215, 365, 264, 8877, 2663, 321, 632, 322, 341, 4137, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11172844451150776, "compression_ratio": 1.5729166666666667, "no_speech_prob": 9.942227188730612e-05}, {"id": 672, "seek": 271000, "start": 2710.0, "end": 2713.0, "text": " And now we can look at what the previous works did and what we did.", "tokens": [50364, 400, 586, 321, 393, 574, 412, 437, 264, 3894, 1985, 630, 293, 437, 321, 630, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09667363660088901, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.7262718251440674e-05}, {"id": 673, "seek": 271000, "start": 2713.0, "end": 2719.0, "text": " So these are the actual formulas that implement those ideas.", "tokens": [50514, 407, 613, 366, 264, 3539, 30546, 300, 4445, 729, 3487, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09667363660088901, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.7262718251440674e-05}, {"id": 674, "seek": 271000, "start": 2719.0, "end": 2722.0, "text": " Then there is the couple of training details.", "tokens": [50814, 1396, 456, 307, 264, 1916, 295, 3097, 4365, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09667363660088901, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.7262718251440674e-05}, {"id": 675, "seek": 271000, "start": 2722.0, "end": 2725.0, "text": " How should you weight the loss based on the noise level", "tokens": [50964, 1012, 820, 291, 3364, 264, 4470, 2361, 322, 264, 5658, 1496, 51114], "temperature": 0.0, "avg_logprob": -0.09667363660088901, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.7262718251440674e-05}, {"id": 676, "seek": 271000, "start": 2725.0, "end": 2730.0, "text": " and how often should you show samples of different noise levels.", "tokens": [51114, 293, 577, 2049, 820, 291, 855, 10938, 295, 819, 5658, 4358, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09667363660088901, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.7262718251440674e-05}, {"id": 677, "seek": 271000, "start": 2730.0, "end": 2735.0, "text": " So the general problem, if you don't deal with these issues,", "tokens": [51364, 407, 264, 2674, 1154, 11, 498, 291, 500, 380, 2028, 365, 613, 2663, 11, 51614], "temperature": 0.0, "avg_logprob": -0.09667363660088901, "compression_ratio": 1.6181818181818182, "no_speech_prob": 3.7262718251440674e-05}, {"id": 678, "seek": 273500, "start": 2735.0, "end": 2740.0, "text": " is that you might have a highly lopsided distribution of gradient feedback.", "tokens": [50364, 307, 300, 291, 1062, 362, 257, 5405, 287, 3370, 2112, 7316, 295, 16235, 5824, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14602638545789218, "compression_ratio": 1.6313131313131313, "no_speech_prob": 0.0012036011321470141}, {"id": 679, "seek": 273500, "start": 2740.0, "end": 2749.0, "text": " So if you're not careful, you might be prodding the weights gently to one direction or the other.", "tokens": [50614, 407, 498, 291, 434, 406, 5026, 11, 291, 1062, 312, 15792, 3584, 264, 17443, 13073, 281, 472, 3513, 420, 264, 661, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14602638545789218, "compression_ratio": 1.6313131313131313, "no_speech_prob": 0.0012036011321470141}, {"id": 680, "seek": 273500, "start": 2749.0, "end": 2757.0, "text": " And then every few iterations you have this massive gradient smash on the weights and so on.", "tokens": [51064, 400, 550, 633, 1326, 36540, 291, 362, 341, 5994, 16235, 17960, 322, 264, 17443, 293, 370, 322, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14602638545789218, "compression_ratio": 1.6313131313131313, "no_speech_prob": 0.0012036011321470141}, {"id": 681, "seek": 273500, "start": 2757.0, "end": 2761.0, "text": " And that's probably very bad for your training dynamics.", "tokens": [51464, 400, 300, 311, 1391, 588, 1578, 337, 428, 3097, 15679, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14602638545789218, "compression_ratio": 1.6313131313131313, "no_speech_prob": 0.0012036011321470141}, {"id": 682, "seek": 276100, "start": 2761.0, "end": 2767.0, "text": " So the role of the loss weighting or the scaling, the numerical scale in front of the loss term,", "tokens": [50364, 407, 264, 3090, 295, 264, 4470, 3364, 278, 420, 264, 21589, 11, 264, 29054, 4373, 294, 1868, 295, 264, 4470, 1433, 11, 50664], "temperature": 0.0, "avg_logprob": -0.17707350759795218, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00034324772423133254}, {"id": 683, "seek": 276100, "start": 2767.0, "end": 2776.0, "text": " should we be to just equalize the magnitude of the loss or equivalently equalize the magnitude of the gradient feedback that gives.", "tokens": [50664, 820, 321, 312, 281, 445, 2681, 1125, 264, 15668, 295, 264, 4470, 420, 9052, 2276, 2681, 1125, 264, 15668, 295, 264, 16235, 5824, 300, 2709, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17707350759795218, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00034324772423133254}, {"id": 684, "seek": 276100, "start": 2776.0, "end": 2783.0, "text": " And then the noise level distribution, maybe how often you show images of any given noise level.", "tokens": [51114, 400, 550, 264, 5658, 1496, 7316, 11, 1310, 577, 2049, 291, 855, 5267, 295, 604, 2212, 5658, 1496, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17707350759795218, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00034324772423133254}, {"id": 685, "seek": 276100, "start": 2783.0, "end": 2789.0, "text": " The role of that is to kind of direct your training efforts to the levels where you know it's relevant,", "tokens": [51464, 440, 3090, 295, 300, 307, 281, 733, 295, 2047, 428, 3097, 6484, 281, 264, 4358, 689, 291, 458, 309, 311, 7340, 11, 51764], "temperature": 0.0, "avg_logprob": -0.17707350759795218, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00034324772423133254}, {"id": 686, "seek": 278900, "start": 2789.0, "end": 2791.0, "text": " where you can make an impact.", "tokens": [50364, 689, 291, 393, 652, 364, 2712, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12594582239786783, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00015752211038488895}, {"id": 687, "seek": 278900, "start": 2791.0, "end": 2797.0, "text": " And for that, in the paper, we have this sort of an important sampling argument that whatever we do,", "tokens": [50464, 400, 337, 300, 11, 294, 264, 3035, 11, 321, 362, 341, 1333, 295, 364, 1021, 21179, 6770, 300, 2035, 321, 360, 11, 50764], "temperature": 0.0, "avg_logprob": -0.12594582239786783, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00015752211038488895}, {"id": 688, "seek": 278900, "start": 2797.0, "end": 2799.0, "text": " we end up with this kind of a loss curve.", "tokens": [50764, 321, 917, 493, 365, 341, 733, 295, 257, 4470, 7605, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12594582239786783, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00015752211038488895}, {"id": 689, "seek": 278900, "start": 2799.0, "end": 2805.0, "text": " So we don't make much progress at very low and very high noise levels, but we do make a lot of progress in the middle.", "tokens": [50864, 407, 321, 500, 380, 652, 709, 4205, 412, 588, 2295, 293, 588, 1090, 5658, 4358, 11, 457, 321, 360, 652, 257, 688, 295, 4205, 294, 264, 2808, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12594582239786783, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00015752211038488895}, {"id": 690, "seek": 278900, "start": 2805.0, "end": 2811.0, "text": " For example, at very low noise end, you're trying to predict noise from a noise free image.", "tokens": [51164, 1171, 1365, 11, 412, 588, 2295, 5658, 917, 11, 291, 434, 1382, 281, 6069, 5658, 490, 257, 5658, 1737, 3256, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12594582239786783, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00015752211038488895}, {"id": 691, "seek": 278900, "start": 2811.0, "end": 2815.0, "text": " It's impossible, but it also doesn't matter if you can't do it.", "tokens": [51464, 467, 311, 6243, 11, 457, 309, 611, 1177, 380, 1871, 498, 291, 393, 380, 360, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12594582239786783, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00015752211038488895}, {"id": 692, "seek": 281500, "start": 2815.0, "end": 2829.0, "text": " So we, based on this, we find that it's enough to, or suffice this to sort of have this very broad distribution", "tokens": [50364, 407, 321, 11, 2361, 322, 341, 11, 321, 915, 300, 309, 311, 1547, 281, 11, 420, 3889, 573, 341, 281, 1333, 295, 362, 341, 588, 4152, 7316, 51064], "temperature": 0.0, "avg_logprob": -0.18094766767401443, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.00022447368246503174}, {"id": 693, "seek": 281500, "start": 2829.0, "end": 2837.0, "text": " of noise levels here that are targeted towards the levels where you know you can make progress.", "tokens": [51064, 295, 5658, 4358, 510, 300, 366, 15045, 3030, 264, 4358, 689, 291, 458, 291, 393, 652, 4205, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18094766767401443, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.00022447368246503174}, {"id": 694, "seek": 281500, "start": 2837.0, "end": 2842.0, "text": " And this is a logarithmic scale on the x-axis, so it's a lot of normal distribution.", "tokens": [51464, 400, 341, 307, 257, 41473, 355, 13195, 4373, 322, 264, 2031, 12, 24633, 11, 370, 309, 311, 257, 688, 295, 2710, 7316, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18094766767401443, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.00022447368246503174}, {"id": 695, "seek": 284200, "start": 2842.0, "end": 2844.0, "text": " So those are those sources.", "tokens": [50364, 407, 729, 366, 729, 7139, 13, 50464], "temperature": 0.0, "avg_logprob": -0.191311280805986, "compression_ratio": 1.5327510917030567, "no_speech_prob": 0.0013259295374155045}, {"id": 696, "seek": 284200, "start": 2844.0, "end": 2846.0, "text": " And it's starting to pretty full.", "tokens": [50464, 400, 309, 311, 2891, 281, 1238, 1577, 13, 50564], "temperature": 0.0, "avg_logprob": -0.191311280805986, "compression_ratio": 1.5327510917030567, "no_speech_prob": 0.0013259295374155045}, {"id": 697, "seek": 284200, "start": 2846.0, "end": 2850.0, "text": " There's one more thing, which I'll just keep in the interest of time.", "tokens": [50564, 821, 311, 472, 544, 551, 11, 597, 286, 603, 445, 1066, 294, 264, 1179, 295, 565, 13, 50764], "temperature": 0.0, "avg_logprob": -0.191311280805986, "compression_ratio": 1.5327510917030567, "no_speech_prob": 0.0013259295374155045}, {"id": 698, "seek": 284200, "start": 2850.0, "end": 2861.0, "text": " We have some mechanism presented in the paper for dealing with vastly like two small data sets when your network starts overfitting by this augmentation mechanism.", "tokens": [50764, 492, 362, 512, 7513, 8212, 294, 264, 3035, 337, 6260, 365, 41426, 411, 732, 1359, 1412, 6352, 562, 428, 3209, 3719, 670, 69, 2414, 538, 341, 14501, 19631, 7513, 13, 51314], "temperature": 0.0, "avg_logprob": -0.191311280805986, "compression_ratio": 1.5327510917030567, "no_speech_prob": 0.0013259295374155045}, {"id": 699, "seek": 284200, "start": 2861.0, "end": 2863.0, "text": " You can look at it there.", "tokens": [51314, 509, 393, 574, 412, 309, 456, 13, 51414], "temperature": 0.0, "avg_logprob": -0.191311280805986, "compression_ratio": 1.5327510917030567, "no_speech_prob": 0.0013259295374155045}, {"id": 700, "seek": 284200, "start": 2863.0, "end": 2866.0, "text": " But yeah, let's not go there.", "tokens": [51414, 583, 1338, 11, 718, 311, 406, 352, 456, 13, 51564], "temperature": 0.0, "avg_logprob": -0.191311280805986, "compression_ratio": 1.5327510917030567, "no_speech_prob": 0.0013259295374155045}, {"id": 701, "seek": 286600, "start": 2866.0, "end": 2870.0, "text": " It's really only relevant for various small data sets like Cypher.", "tokens": [50364, 467, 311, 534, 787, 7340, 337, 3683, 1359, 1412, 6352, 411, 10295, 79, 511, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1590031101590111, "compression_ratio": 1.458937198067633, "no_speech_prob": 0.0008471180335618556}, {"id": 702, "seek": 286600, "start": 2870.0, "end": 2874.0, "text": " With ImageNet, we haven't found benefits from it, I think.", "tokens": [50564, 2022, 29903, 31890, 11, 321, 2378, 380, 1352, 5311, 490, 309, 11, 286, 519, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1590031101590111, "compression_ratio": 1.458937198067633, "no_speech_prob": 0.0008471180335618556}, {"id": 703, "seek": 286600, "start": 2874.0, "end": 2878.0, "text": " Okay, so with all these improvements, we can stack them one by one.", "tokens": [50764, 1033, 11, 370, 365, 439, 613, 13797, 11, 321, 393, 8630, 552, 472, 538, 472, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1590031101590111, "compression_ratio": 1.458937198067633, "no_speech_prob": 0.0008471180335618556}, {"id": 704, "seek": 286600, "start": 2878.0, "end": 2880.0, "text": " These are the lines here.", "tokens": [50964, 1981, 366, 264, 3876, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1590031101590111, "compression_ratio": 1.458937198067633, "no_speech_prob": 0.0008471180335618556}, {"id": 705, "seek": 286600, "start": 2880.0, "end": 2890.0, "text": " And in the end, we get state-of-the-art results in various competitive categories.", "tokens": [51064, 400, 294, 264, 917, 11, 321, 483, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 294, 3683, 10043, 10479, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1590031101590111, "compression_ratio": 1.458937198067633, "no_speech_prob": 0.0008471180335618556}, {"id": 706, "seek": 289000, "start": 2890.0, "end": 2900.0, "text": " In deterministic sampling, we get an FID 179.97 on the Cypher categories, which might still be state-of-the-art.", "tokens": [50364, 682, 15957, 3142, 21179, 11, 321, 483, 364, 479, 2777, 3282, 24, 13, 23247, 322, 264, 10295, 79, 511, 10479, 11, 597, 1062, 920, 312, 1785, 12, 2670, 12, 3322, 12, 446, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11792746834132982, "compression_ratio": 1.3743016759776536, "no_speech_prob": 0.00023495549976360053}, {"id": 707, "seek": 289000, "start": 2900.0, "end": 2906.0, "text": " Also at very low sample counts compared to most previous work.", "tokens": [50864, 2743, 412, 588, 2295, 6889, 14893, 5347, 281, 881, 3894, 589, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11792746834132982, "compression_ratio": 1.3743016759776536, "no_speech_prob": 0.00023495549976360053}, {"id": 708, "seek": 289000, "start": 2906.0, "end": 2908.0, "text": " That's more interestingly.", "tokens": [51164, 663, 311, 544, 25873, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11792746834132982, "compression_ratio": 1.3743016759776536, "no_speech_prob": 0.00023495549976360053}, {"id": 709, "seek": 289000, "start": 2908.0, "end": 2910.0, "text": " Okay, that was with deterministic sampling.", "tokens": [51264, 1033, 11, 300, 390, 365, 15957, 3142, 21179, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11792746834132982, "compression_ratio": 1.3743016759776536, "no_speech_prob": 0.00023495549976360053}, {"id": 710, "seek": 291000, "start": 2910.0, "end": 2923.0, "text": " When we enable the stochastic sampling and tailor it for these architectures for ImageNet and use these retrained networks, we trained ourselves using these principles.", "tokens": [50364, 1133, 321, 9528, 264, 342, 8997, 2750, 21179, 293, 33068, 309, 337, 613, 6331, 1303, 337, 29903, 31890, 293, 764, 613, 1533, 31774, 9590, 11, 321, 8895, 4175, 1228, 613, 9156, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13827103304575725, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.001261450699530542}, {"id": 711, "seek": 291000, "start": 2923.0, "end": 2930.0, "text": " We get an FID of 1.36, which was a state-of-the-art when this paper came out.", "tokens": [51014, 492, 483, 364, 479, 2777, 295, 502, 13, 11309, 11, 597, 390, 257, 1785, 12, 2670, 12, 3322, 12, 446, 562, 341, 3035, 1361, 484, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13827103304575725, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.001261450699530542}, {"id": 712, "seek": 291000, "start": 2930.0, "end": 2936.0, "text": " It's been overtaken, I think in the last few weeks, possibly earlier.", "tokens": [51364, 467, 311, 668, 17038, 9846, 11, 286, 519, 294, 264, 1036, 1326, 3259, 11, 6264, 3071, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13827103304575725, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.001261450699530542}, {"id": 713, "seek": 293600, "start": 2937.0, "end": 2943.0, "text": " So all in all, we've turned this model that was okay-ish in the beginning.", "tokens": [50414, 407, 439, 294, 439, 11, 321, 600, 3574, 341, 2316, 300, 390, 1392, 12, 742, 294, 264, 2863, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15944859856053403, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.000492455146741122}, {"id": 714, "seek": 293600, "start": 2943.0, "end": 2954.0, "text": " And by stacking all of these improvements, we get the best model in the world at that time for generating 64 ImageNet.", "tokens": [50714, 400, 538, 41376, 439, 295, 613, 13797, 11, 321, 483, 264, 1151, 2316, 294, 264, 1002, 412, 300, 565, 337, 17746, 12145, 29903, 31890, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15944859856053403, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.000492455146741122}, {"id": 715, "seek": 293600, "start": 2954.0, "end": 2963.0, "text": " Interestingly, the stochasticity is no longer helpful with Cypher in this resume or after the training improvement.", "tokens": [51264, 30564, 11, 264, 342, 8997, 2750, 507, 307, 572, 2854, 4961, 365, 10295, 79, 511, 294, 341, 15358, 420, 934, 264, 3097, 10444, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15944859856053403, "compression_ratio": 1.5147058823529411, "no_speech_prob": 0.000492455146741122}, {"id": 716, "seek": 296300, "start": 2963.0, "end": 2969.0, "text": " And so it appears that the network has become so good that it doesn't make that many errors.", "tokens": [50364, 400, 370, 309, 7038, 300, 264, 3209, 575, 1813, 370, 665, 300, 309, 1177, 380, 652, 300, 867, 13603, 13, 50664], "temperature": 0.0, "avg_logprob": -0.22384105258517795, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0004031655262224376}, {"id": 717, "seek": 296300, "start": 2969.0, "end": 2976.0, "text": " And any exploration, lands and wine exploration you do, introduces more error than you are actually fixing with it.", "tokens": [50664, 400, 604, 16197, 11, 5949, 293, 7209, 16197, 291, 360, 11, 31472, 544, 6713, 813, 291, 366, 767, 19442, 365, 309, 13, 51014], "temperature": 0.0, "avg_logprob": -0.22384105258517795, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0004031655262224376}, {"id": 718, "seek": 296300, "start": 2976.0, "end": 2979.0, "text": " But this is still not the case with ImageNet.", "tokens": [51014, 583, 341, 307, 920, 406, 264, 1389, 365, 29903, 31890, 13, 51164], "temperature": 0.0, "avg_logprob": -0.22384105258517795, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0004031655262224376}, {"id": 719, "seek": 296300, "start": 2979.0, "end": 2984.0, "text": " So there it's still pays to do stochasticity.", "tokens": [51164, 407, 456, 309, 311, 920, 10604, 281, 360, 342, 8997, 2750, 507, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22384105258517795, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0004031655262224376}, {"id": 720, "seek": 296300, "start": 2984.0, "end": 2990.0, "text": " Okay, so that was the, that was mostly it.", "tokens": [51414, 1033, 11, 370, 300, 390, 264, 11, 300, 390, 5240, 309, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22384105258517795, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.0004031655262224376}, {"id": 721, "seek": 299000, "start": 2990.0, "end": 2994.0, "text": " Just a brief conclusion.", "tokens": [50364, 1449, 257, 5353, 10063, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12788537832406852, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0007895106100477278}, {"id": 722, "seek": 299000, "start": 2994.0, "end": 3001.0, "text": " So we've sort of exposed this completely modular design of these diffusion models.", "tokens": [50564, 407, 321, 600, 1333, 295, 9495, 341, 2584, 31111, 1715, 295, 613, 25242, 5245, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12788537832406852, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0007895106100477278}, {"id": 723, "seek": 299000, "start": 3001.0, "end": 3007.0, "text": " Instead of viewing them as tightly coupled packages where you can't change anything without breaking something,", "tokens": [50914, 7156, 295, 17480, 552, 382, 21952, 29482, 17401, 689, 291, 393, 380, 1319, 1340, 1553, 7697, 746, 11, 51214], "temperature": 0.0, "avg_logprob": -0.12788537832406852, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0007895106100477278}, {"id": 724, "seek": 299000, "start": 3007.0, "end": 3015.0, "text": " we show that you can pretty much change every, like you get a valid method no matter what you do as long as you follow these loose guidelines.", "tokens": [51214, 321, 855, 300, 291, 393, 1238, 709, 1319, 633, 11, 411, 291, 483, 257, 7363, 3170, 572, 1871, 437, 291, 360, 382, 938, 382, 291, 1524, 613, 9612, 12470, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12788537832406852, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.0007895106100477278}, {"id": 725, "seek": 301500, "start": 3015.0, "end": 3022.0, "text": " And then with that knowledge, we get a clear view into what we should actually be doing with those choices.", "tokens": [50364, 400, 550, 365, 300, 3601, 11, 321, 483, 257, 1850, 1910, 666, 437, 321, 820, 767, 312, 884, 365, 729, 7994, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15811896968532252, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.0002064291766146198}, {"id": 726, "seek": 301500, "start": 3022.0, "end": 3025.0, "text": " And doing so pays off in a big way.", "tokens": [50714, 400, 884, 370, 10604, 766, 294, 257, 955, 636, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15811896968532252, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.0002064291766146198}, {"id": 727, "seek": 301500, "start": 3025.0, "end": 3030.0, "text": " We get much improved quality at much, much more efficient models.", "tokens": [50864, 492, 483, 709, 9689, 3125, 412, 709, 11, 709, 544, 7148, 5245, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15811896968532252, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.0002064291766146198}, {"id": 728, "seek": 301500, "start": 3030.0, "end": 3034.0, "text": " One takeaway about stochasticity, it's a bit of a double edged sword.", "tokens": [51114, 1485, 30681, 466, 342, 8997, 2750, 507, 11, 309, 311, 257, 857, 295, 257, 3834, 1257, 3004, 10576, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15811896968532252, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.0002064291766146198}, {"id": 729, "seek": 303400, "start": 3034.0, "end": 3042.0, "text": " As said, it does help, but it also, it requires that annoying per case tuning.", "tokens": [50364, 1018, 848, 11, 309, 775, 854, 11, 457, 309, 611, 11, 309, 7029, 300, 11304, 680, 1389, 15164, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12521300542922248, "compression_ratio": 1.6680327868852458, "no_speech_prob": 0.009235884994268417}, {"id": 730, "seek": 303400, "start": 3042.0, "end": 3045.0, "text": " There are no clear principles how to do that tuning.", "tokens": [50764, 821, 366, 572, 1850, 9156, 577, 281, 360, 300, 15164, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12521300542922248, "compression_ratio": 1.6680327868852458, "no_speech_prob": 0.009235884994268417}, {"id": 731, "seek": 303400, "start": 3045.0, "end": 3050.0, "text": " There is also a danger that you can even have bugs in your code or something.", "tokens": [50914, 821, 307, 611, 257, 4330, 300, 291, 393, 754, 362, 15120, 294, 428, 3089, 420, 746, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12521300542922248, "compression_ratio": 1.6680327868852458, "no_speech_prob": 0.009235884994268417}, {"id": 732, "seek": 303400, "start": 3050.0, "end": 3054.0, "text": " And stochasticity will kind of fix them to an extent,", "tokens": [51164, 400, 342, 8997, 2750, 507, 486, 733, 295, 3191, 552, 281, 364, 8396, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12521300542922248, "compression_ratio": 1.6680327868852458, "no_speech_prob": 0.009235884994268417}, {"id": 733, "seek": 303400, "start": 3054.0, "end": 3059.0, "text": " which is of course not what you want to do if you're trying to understand what your potential improvements are,", "tokens": [51364, 597, 307, 295, 1164, 406, 437, 291, 528, 281, 360, 498, 291, 434, 1382, 281, 1223, 437, 428, 3995, 13797, 366, 11, 51614], "temperature": 0.0, "avg_logprob": -0.12521300542922248, "compression_ratio": 1.6680327868852458, "no_speech_prob": 0.009235884994268417}, {"id": 734, "seek": 303400, "start": 3059.0, "end": 3062.0, "text": " what their effect is and so on.", "tokens": [51614, 437, 641, 1802, 307, 293, 370, 322, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12521300542922248, "compression_ratio": 1.6680327868852458, "no_speech_prob": 0.009235884994268417}, {"id": 735, "seek": 306200, "start": 3062.0, "end": 3067.0, "text": " Ideally, you'd be able to work in a completely deterministic setting.", "tokens": [50364, 40817, 11, 291, 1116, 312, 1075, 281, 589, 294, 257, 2584, 15957, 3142, 3287, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14003408432006836, "compression_ratio": 1.5625, "no_speech_prob": 0.00020246255735401064}, {"id": 736, "seek": 306200, "start": 3067.0, "end": 3076.0, "text": " And if you want, then in the end just kind of reintroduce the stochasticity as the final, final cherry on the top.", "tokens": [50614, 400, 498, 291, 528, 11, 550, 294, 264, 917, 445, 733, 295, 319, 38132, 384, 264, 342, 8997, 2750, 507, 382, 264, 2572, 11, 2572, 20164, 322, 264, 1192, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14003408432006836, "compression_ratio": 1.5625, "no_speech_prob": 0.00020246255735401064}, {"id": 737, "seek": 306200, "start": 3076.0, "end": 3082.0, "text": " Okay, so we haven't talked about all the fancy stuff like higher resolutions, network architectures,", "tokens": [51064, 1033, 11, 370, 321, 2378, 380, 2825, 466, 439, 264, 10247, 1507, 411, 2946, 32179, 11, 3209, 6331, 1303, 11, 51364], "temperature": 0.0, "avg_logprob": -0.14003408432006836, "compression_ratio": 1.5625, "no_speech_prob": 0.00020246255735401064}, {"id": 738, "seek": 306200, "start": 3082.0, "end": 3084.0, "text": " classifier free guidance and so on.", "tokens": [51364, 1508, 9902, 1737, 10056, 293, 370, 322, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14003408432006836, "compression_ratio": 1.5625, "no_speech_prob": 0.00020246255735401064}, {"id": 739, "seek": 306200, "start": 3084.0, "end": 3088.0, "text": " But probably many of these would be right for a similar principle of analysis.", "tokens": [51464, 583, 1391, 867, 295, 613, 576, 312, 558, 337, 257, 2531, 8665, 295, 5215, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14003408432006836, "compression_ratio": 1.5625, "no_speech_prob": 0.00020246255735401064}, {"id": 740, "seek": 308800, "start": 3088.0, "end": 3095.0, "text": " We hope this inspires you to also think about that kind of things and certainly we are.", "tokens": [50364, 492, 1454, 341, 32566, 291, 281, 611, 519, 466, 300, 733, 295, 721, 293, 3297, 321, 366, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11370021482057209, "compression_ratio": 1.570754716981132, "no_speech_prob": 0.0006266505806706846}, {"id": 741, "seek": 308800, "start": 3095.0, "end": 3099.0, "text": " So with that, the code and everything is of course available.", "tokens": [50714, 407, 365, 300, 11, 264, 3089, 293, 1203, 307, 295, 1164, 2435, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11370021482057209, "compression_ratio": 1.570754716981132, "no_speech_prob": 0.0006266505806706846}, {"id": 742, "seek": 308800, "start": 3099.0, "end": 3105.0, "text": " I would argue this is probably one of the better places to copy paste your code.", "tokens": [50914, 286, 576, 9695, 341, 307, 1391, 472, 295, 264, 1101, 3190, 281, 5055, 9163, 428, 3089, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11370021482057209, "compression_ratio": 1.570754716981132, "no_speech_prob": 0.0006266505806706846}, {"id": 743, "seek": 308800, "start": 3105.0, "end": 3113.0, "text": " If you want to experiment with stuff, it's very clean code based that directly implements these ideas.", "tokens": [51214, 759, 291, 528, 281, 5120, 365, 1507, 11, 309, 311, 588, 2541, 3089, 2361, 300, 3838, 704, 17988, 613, 3487, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11370021482057209, "compression_ratio": 1.570754716981132, "no_speech_prob": 0.0006266505806706846}, {"id": 744, "seek": 311300, "start": 3113.0, "end": 3117.0, "text": " Yeah, so thank you for your attention.", "tokens": [50364, 865, 11, 370, 1309, 291, 337, 428, 3202, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3846149444580078, "compression_ratio": 1.082191780821918, "no_speech_prob": 0.017515387386083603}, {"id": 745, "seek": 311300, "start": 3124.0, "end": 3127.0, "text": " Hey, so we have time.", "tokens": [50914, 1911, 11, 370, 321, 362, 565, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3846149444580078, "compression_ratio": 1.082191780821918, "no_speech_prob": 0.017515387386083603}, {"id": 746, "seek": 311300, "start": 3127.0, "end": 3129.0, "text": " Yeah, I have some.", "tokens": [51064, 865, 11, 286, 362, 512, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3846149444580078, "compression_ratio": 1.082191780821918, "no_speech_prob": 0.017515387386083603}, {"id": 747, "seek": 312900, "start": 3129.0, "end": 3135.0, "text": " I just want to ask a question.", "tokens": [50364, 286, 445, 528, 281, 1029, 257, 1168, 13, 50664], "temperature": 0.0, "avg_logprob": -0.5021569569905598, "compression_ratio": 1.0705882352941176, "no_speech_prob": 0.0016339095309376717}, {"id": 748, "seek": 312900, "start": 3135.0, "end": 3138.0, "text": " All right.", "tokens": [50664, 1057, 558, 13, 50814], "temperature": 0.0, "avg_logprob": -0.5021569569905598, "compression_ratio": 1.0705882352941176, "no_speech_prob": 0.0016339095309376717}, {"id": 749, "seek": 312900, "start": 3147.0, "end": 3150.0, "text": " It probably has to do with the data's complexity.", "tokens": [51264, 467, 1391, 575, 281, 360, 365, 264, 1412, 311, 14024, 13, 51414], "temperature": 0.0, "avg_logprob": -0.5021569569905598, "compression_ratio": 1.0705882352941176, "no_speech_prob": 0.0016339095309376717}, {"id": 750, "seek": 315000, "start": 3150.0, "end": 3155.0, "text": " Seffari is maybe a bit too simplistic in the end.", "tokens": [50364, 1100, 602, 3504, 307, 1310, 257, 857, 886, 44199, 294, 264, 917, 13, 50614], "temperature": 0.0, "avg_logprob": -0.31905064053005644, "compression_ratio": 1.2598425196850394, "no_speech_prob": 0.018669094890356064}, {"id": 751, "seek": 315000, "start": 3155.0, "end": 3158.0, "text": " It's kind of learnable entirely.", "tokens": [50614, 467, 311, 733, 295, 1466, 712, 7696, 13, 50764], "temperature": 0.0, "avg_logprob": -0.31905064053005644, "compression_ratio": 1.2598425196850394, "no_speech_prob": 0.018669094890356064}, {"id": 752, "seek": 315000, "start": 3158.0, "end": 3165.0, "text": " But it seems like that something like ImageNet, it's still so extremely cool.", "tokens": [50764, 583, 309, 2544, 411, 300, 746, 411, 29903, 31890, 11, 309, 311, 920, 370, 4664, 1627, 13, 51114], "temperature": 0.0, "avg_logprob": -0.31905064053005644, "compression_ratio": 1.2598425196850394, "no_speech_prob": 0.018669094890356064}], "language": "fi"}