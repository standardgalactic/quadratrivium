Almost all microprocessors we have today started off with the idea of one processor.
It would be the CPU, the one chip that is running everything.
And although multiprocessor submissions are quite common these days,
each processor in the system is basically a copy of the other one.
There are all processors that were designed to run by themselves, but we just have more than one.
However, there is a processor that from its initial design was not intended to be there by itself.
It was designed from the outset to be part of a cluster of processors.
Yep, we're going to talk about the Transputer.
Now in this video we're going to talk about the Transputer in its history,
and also have a look at this weird little development box you can see in the background of shots here.
And towards the end of the video we'll talk about what might be one of Atari's least successful computers ever.
In fact, I'm pretty sure it is the least successful thing they've ever made.
And I'm even concluding a Jaguar in its CD player.
In fact, why don't we all play a little game together?
You have a guess at what you think the machine might be?
Make a comment in the section below, and at the end of the video you can see whether you guessed the right one or not.
Control-Alt-Risk, we already know you know the answer.
Now, as I mentioned in the introduction, most CPUs design with the idea that they're the one CPU.
When the design is created, they're not thinking about other processors, or at least they weren't initially.
These days, it's a little different.
When the Transputer was created, they were thinking about parallel processing from the start.
There was never going to be a system that had just one of these things.
And that makes the Transputer pretty unique.
And in a little bit, we're going to look at some of the architectural features that ended up in the Transputer.
So it can be this fairly unique design goal.
But first, we're going to look at the circumstances that caused the Transputer to come into existence in the first place.
In 1978, the UK's then Labour government was devising its industrial strategy for the IT sector.
And between that and the National Enterprise Board, they landed on the idea that they should create a new microprocessor design company.
Thus, a MOS was established by Ian Barron, Dick Petrus, and Paul Schroeder.
Those last two names should be familiar for anyone who knows about MOSTech,
because, well, they just sold MOSTech for basically a small fortune, and decided to get involved in this.
Now, the strategy the National Enterprise Board had was initially that they'd start a joint venture where they'd be manufacturing stuff in the US,
where it was already done, and then transfer that technology to the UK and set up manufacturing there.
And by the time that manufacturing was established and ready,
well then, MOS would have a chip design ready to be manufactured in the UK.
And the idea is, after that, this whole thing would have bootstrapped the technology industry in Bristol,
which it did actually manage to do, and is still there to this day, doing stuff.
So that's the why, guess we'd best get onto the whoo-hoo.
Now, why Ian MOS were getting on with manufacturing static RAM,
which eventually they managed to capture 60% of the market for,
they hired themselves Robert Milne and David May,
and not Brian May, like my friend thinks that they did,
and surprisingly the globally renowned astrophysicist Brian May, an occasional guitarist,
was not particularly useful when it came to processor design.
He's more of a stereo photography kind of a guy,
and of course guitarist for Smile and Queen, we probably shouldn't forget to mention that,
and of course patron saint of all badgers.
Barron had a vision to create a microprocessor explicitly for parallel computing,
and as in fact, the person who came up with the name Transputer.
The idea being that the transistor was pretty common and ubiquitous for making all logic work,
and well, he wanted this chip to be equally as ubiquitous and common as the transistor was,
but for computing, hence, Transputer.
Now, you might be surprised to know that Milne and May didn't jump straight into designing the processor itself.
Where they started is with the programming language Occum.
Before we get into discussing Occum though, I should mention that today's video is sponsored by PCBWay.
A company whose primary occupation seems to be sponsoring YouTube channels,
but when they're not doing that, they also make printed circuit boards,
they do surface mount type stuff for your pick and place.
They'll also do CNCing, freely printing.
Basically, if you need to make it, they can do quite a lot of it for you and send it to you in the post.
Occum was named after 13th slash 14th century razor blade manufacturer William of Occum,
and now, having just made that joke, I realize a number of you are reaching for the comments section saying
he didn't make razor blade.
I know, it's not that kind of razor.
Anyway, quickly back to the language.
Now, both Occum's creators had quite a lot of experience with creating or working on compilers before,
with David May having done a fair bit of work on BCPL,
and Tony Hoor, of course, being famous for creating alcohol 60,
and the quicksort algorithm, and basically a whole bunch of things.
Actually, I believe he's Sir Tony Hoor these days.
Now, this language wasn't going to be like any of the others they'd come up with before,
because the idea of Occum was to create a language that you could develop parallel programs in from the ground up.
It wasn't an idea that's bolted into the language later on.
And you'll see this from one of Occum's major two keywords, par and seek.
Each block of code would need to be labeled with either par,
for you can run all these instructions in parallel,
or seek these instructions must run in sequence.
There is also a third keyword like this, alt,
which means that if it's ready to do one thing, you can do that.
If it's ready to do another thing, either hardware is ready, you can do that.
It's all about listening on a channel and waiting for stuff to be ready on the channel,
and then you can do some work.
And the blocks after that can either be in sequence or in parallel.
Mostly they're in sequences, you're waiting for something to do some work upon.
Now, I should probably mention channels, because this is one of the key ideas in Occum as well.
In Occum, you don't have shared variables.
Two things that run in parallel cannot access the same memory or variable, if you will, at the same time.
Now, if you've done any development with anything that uses threads or does any parallel stuff,
you know that not all languages are like that.
But the reason both May and Hall were trying to avoid shared variables
is they knew that that's where deadlock lies.
If a parallel program is going to go wrong,
it's usually around something failing to lock a variable properly before it uses it or Reddit it.
Or a bunch of things fighting over access and deadlocking the whole system.
So they had a pretty good plan to avoid that, which is just not to let that be a feature of the language.
But obviously, information does need to be passed between bits of code executing in parallel,
and this is where the channel's idea comes from.
Essentially a message-passing system where the thing that owns the variable
can indeed pass that data along a channel to another process running in parallel so that it can use it.
And that's why that alt keyword was handy.
You'd have a bunch of things in parallel computing things and shoving them down a channel.
When you hit the alt statement, as soon as you can think of like a first-pass-the-post type system,
whichever channel delivers something first,
well, that alt statement means the blocker code attached to whatever channel is the first one ready gets to execute.
After having developed the language back, they then created the compiler for Ockham.
Now, you may be thinking, wait a minute, didn't he skip over to step when he designed the transputer?
And the answer is no.
I know it's quite common to go, designer CPU, making compiler for various languages, forcehead CPU.
But they deliberately went the other way round.
They created the language, then they created the compiler to get it down into the smallest number of instructions they thought were viable.
And then when they knew what instructions they'd need their processor to execute,
well, that's when they started work on the processor.
Now, in a few talks, David May has given.
He stated a few times that someone shouldn't be allowed to design a CPU
unless first they've had to create a compiler or two for somebody else's.
And the reason that lies behind that is a lot of CPUs that were created around the time or before David was working on the transputer
had some pretty, let's say, bat poop instruction sets.
Lots of these earlier CPU designs had an awful lot of instructions, and I mean a lot.
Basically, instruction almost for anything you might want to do.
And the reason things ended up that way is the CPU designers are not really thinking about compilers at this point.
They're thinking about people who are writing programs in Assembler.
Essentially, they're trying to make life easier for human beings who are going to use CPU instructions almost directly.
Problem is, by the time we get to languages that use a compiler,
well, most of the instructions a human being might reach for on a CPU are not the ones that a compiler's going to,
because they're a lot less optimal in some cases.
With Sysc Processes, this gets to the point where with Vax, you've almost got if, then, else in hardware.
And as most programs start to be written in a higher level language that's compiled rather than Assembler,
some of the very infrequently used CPU operations don't get used at all.
In fact, there's a famous study that went on to show that most applications use a relatively small subset of the instruction set on the CPU.
And this is what gives rise to the risk movement.
The reason I'm talking about all of this is to show you just what a good idea it was to go from language, compiler, then CPU,
because you know which instructions you need to implement.
You don't need to implement a CPU with thousands of instructions.
And this is why Occam and the transmitter are so interlinked with each other.
Occam was created to target the thing that became the transmitter,
and the transmitter was built to implement what Occam's compiler outputted.
And design features from the language also ended up essentially getting implemented in hardware.
We'll come to that later on.
Right, now we talked a lot about how we got to having transporters and why.
We've not actually talked a lot about the hardware yet, so let's do so.
And the first actual hardware became available in 1984.
And to say the transmitter is an odd fish was kind of underselling it, really.
Now, let's start with just the processing type bits.
First of all, it uses micro-encoded instructions, like a CISC processor, but it's not a CISC processor.
And most of those micro-coded instructions, well, they run in one clock cycle, very much unlike most CISC processors.
Then we come onto the fact that it's a stack processor, but it's a stack processor with registers,
which is a thing that normal CPUs have, and stack processors generally don't.
And then we come to the un-processor-like bits that are on the transmitter chip.
See, the transmitter includes its own built-in RAM, and of course, RAM controller as well.
And I don't mean that it had some cache memory, I mean, this is actual RAM-RAM.
The idea being that you could run a number of transporters together and you wouldn't have to connect up external memory.
I mean, obviously, when people started building machines, yeah, sure, they connected up external memory,
but they didn't have to, and not every dev board did.
Now, there's not a lot of RAM on the first implementation of this,
so the first-ever hardware, it's a 16-bit processor, comes with a grand total of 2 kilobytes of memory,
which is not a lot of RAM, but it's a heck of a lot of RAM for on the processor,
particularly in 1984, when my whole computer at the time had 32K of memory.
And that initial hardware chip was very quickly followed up by one with 4K of memory on board.
And it also added some breakpoint debugging support directly into the chip as well,
which made life a heck of a lot easier.
Now, this chip was very quickly followed up in 1985 by a 32-bit version, known as the T414,
and again, that version shipped with 2K built-in, and then there was a 4K version of it that followed later on.
Now, these early 16-bit and 32-bit chips, they were the ones that the first system builders started to get their hands on.
Now, so far, I've only talked about these things like they were normal processors,
but there's one big architectural feature I've not mentioned yet,
and if you know anything about the transputer, you're probably wondering why, because it is its most notable feature.
But I did foreshadow it when I was talking about channels in Occam earlier and said,
we'll come back to this later, well, we're coming back to it.
Transputer chips had a link feature that was just there to connect one transputer chip to another transputer chip.
The very early ones came with two links, but later on, by the time we get into the 32-bit ones,
we've got four links coming out of each chip, so you can build a grid array of transputer chips.
Now, this makes total sense to the chip designed for parallel computing.
After all, you can't just have one, because if you do, nothing's happening in parallel, is it?
And this is where we get the modelling in hardware of that channel system we were talking about before,
as the channels are essentially the inter-chip links, if you like.
And because that hardware element comes directly from the language we're using,
well, we get to make a pretty efficient use of them.
As your programmers are already thinking about having chips waiting on messages through channels,
and can see where the bottlenecks in their code, and therefore, when implemented in hardware, in the hardware would be.
Now, this whole chip link thing, as we end up with more and more and even more powerful transputer chips, all in one grid.
Well, four soon becomes not enough, and you eventually end up with ImOS actually implementing the first ever on-silicon switch.
And using that means that a transputer doesn't have to root to another transputer through a number of transputers to get to it.
The message can be switched to the right transputer using the silicon switch.
I mean, this is an idea that Ethernet really embraced and ran with later on.
But let's get back to the next major development of the transputer line, and that's with the T8 that gets introduced in 1987.
Now, this is a pretty big leap forward for transputer kind, because we get an extended instruction set, which provides floating point support.
Now, up until this point, if any computer had done any floating point maps, it'd done it to its own, well, standard unique to that computer, more or less.
And the reason why ImOS had waited up until this point to introduce floating point support was that finally it had been standardized by the IEEE.
This meant that floating point maps would work the same way on any machine implementing that IEEE floating point specification, which was a pretty big deal.
Now, first of all, ImOS and making these chips to go into the high-performance parallel computing space, which is, you know, mostly scientists and engineers, that sort of thing, universities.
So, they're being used for pretty mass-intensive tasks.
So, not having floating point, well, that was not as helpful as you might think.
But also, you needed a well-understood floating point standard that the mathematicians involved in this stuff knew understood and could get what the limitations were and whether what they were doing would fit into it well.
Now, this is where they actually took a bit of departure on how they went about implementing a standard compared to others, as ImOS decided to go full formal methods.
This means that it did take ImOS a fair chunk more time to do their IEEE implementation.
In fact, it added about six months on to it, is what they estimate, but it meant, come the end of it, they had a provably correct implementation of the IEEE standard.
And when you're selling a very high-ended expensive system to scientists, it helps to be able to tell them that it will definitely get its mass right, because there were other CPU manufacturers that did not get floating point right.
Yes, Intel, we're looking at you.
Yes, famously when Intel released the Pentium, they actually got a bug in their floating point division instruction.
Yeah, didn't actually divide things correctly.
Bit of a bummer when you write software that relies on that, really.
There were actually software work-arounds to these problems.
It wasn't so bugged that you couldn't work around it.
But it did mean that some poor seller actually did have to work around it.
And when you're a scientist who's buying a new expensive computer to do whatever it is you're going to do with it,
you kind of want it just to get its mass right.
Now, around the time the T8 was introduced, there was an excitement around the transputer had really built up.
And this is like its key commercial period where it's really leading its field at this point.
It must have been taking transputer-based systems to segraphs and really showing off what the machine could do,
to the point where some people didn't believe that the little box they were seeing the few transputers in was actually doing what they were seeing on-screen
and assumed that somewhere they must be hiding a massive mini-system somewhere.
It's in the same year in 1986 that Edinburgh University puts in their first experimental system using transputers to see what they can do.
This was regarded as a little system, only having 40 transputer chips in it.
Admittedly, that's more processing power than all the machines I'd ever seen at that point in my life added together.
But still, as supercomputers go relatively small,
but they were suitably impressed with it and installed a much larger system based on the T800
and using a microfax as the file server, because transputer systems didn't really do file IO particularly.
And that was completed and installed by 1987 and was effectively one of the most powerful supercomputers in the UK at that time.
Or, more or less anywhere, to be honest.
In fact, Edinburgh still is a major centre of supercomputing.
And this transputer-based system, which peaked at around 400 processors,
that stayed into its original form until about 1992, where it was reconfigured to add a spark-based machine from Sun to act as the host machine,
and then finally was decommissioned in 1994.
So, they got seven years' use of it, which for a supercomputer at that time, yeah, that was a good long lifespan.
Now, you may have noticed me using the phrase host system.
And that brings me onto one of the odd things about the world of transputers and also helps explain this machine I've got here.
Yes, this is a transputer-based system, but you might notice that it doesn't look a lot like other computers.
In fact, if I turn it round to the rear, you'll see a number of pretty important ports missing.
There's nowhere for a keyboard to go in, or attach a hard drive, or a floppy drive, or in fact any IO at all.
Now, there's a couple of decent reasons for that.
And the first thing comes down to the design feature of the transputer itself, in that it does not have a memory management unit,
or support for virtual memory at all, which for almost every high-end operating system at the time was a big no-no.
You were not going to port Unix to this thing, or even Windows NT for that matter.
Now, there was eventually a Unix-like operating system, if not Unix developed for the transputer,
but we'll come back to that later, and even that does involve a host system of some description.
Again, we'll get into the details of that one in a bit.
So, without an operating system readily portable, if you'd wanted to run this like a normal computer,
well, you would have needed to produce your own operating system, and then all your own drivers for every bit of hardware you want to plug into it.
And that starts consuming a lot of resource for, well, not much purpose.
Because no one was going to use a transputer as their usual workstation.
You're not going to word process or email on this thing. This is for supercomputing.
You don't care if it doesn't run a word processor. You want to get it to do an awful lot of maths.
Most transputer systems sold were kind of like the one in front of me here, a development system
that hangs off another system as the host computer.
The host computer would be the terminal, do all your file I owe, and they would just communicate with the transputer system
like a CPU accelerator almost, only it didn't, you know, run the same instruction set.
Kind of like a modern GPU, if you will.
In fact, this is what this ICER cards for. This would go in the host IBM PC, the software would run on there,
and this ribbon cable would connect it up to the transputer box.
You would then run your software under DOS, for example, in this particular case.
And that would provide you with your basic I owe in and out of the transputer system.
So that's where you'd see your terminal where you command it to do stuff.
And the transputer system will be able to request files and stuff back through that cable.
It would then use the application, pull it from the DOS filing system, return the file back to the transputer.
And file writes work in the same way, just in reverse.
Now, if you're wondering how the transputer system boots up, well, ordinarily when a transputer hits power on,
it just sits there and waits for messages to be passed over those inter-process links we talked about earlier.
It essentially just sits there and does absolutely nothing.
However, there is a pin on the transputer, and if you short it to ground,
well, instead of powering up in its normal state, the transputer chip will power up and read in a chunk of software from a ROM.
What's read in from the ROM gets dumped on the chip stack, and then it starts executing it,
and it will message all the other processors to essentially wake them up and get them to do something.
So inside this rack here is a ROM that the one boot transputer reads,
and it contains enough software so the transputer system sits there,
waiting to be told what to do via that ribbon cable that's linked to the PC.
Now, this isn't the only transputer development system for the IBM PC.
You would actually get ICER cards that had a couple of transputers on,
and essentially a chip that would sit there, talk to the ICER bus, pretend to be some form of serial interface,
and the PC would communicate with the transputers in more or less the same way it does with an external system.
Just it all happens to be on one card and there's a lot less transputers in there.
Now, this box is a little different in that there is also a graphics card in here.
This particular development system was used by a friend's dad for his medical physics PhD,
so the graphics card was used to display the output of what was being computed on the transputer system,
in a resolution and color depth that your average IBM PC was going to get nowhere near,
hence the graphics card costing nearly as much as the whole transputer system put together.
This also feels like a good chance to say,
ah, thanks for letting us have the system, Theo, very kind of you.
I should also mention, you're not going to see me power this thing up in the video,
apart from the fact that I don't have the software to run on the IBM PC side of things,
so it would be an extremely uneventful thing to watch.
This thing has also sat in the cupboard for a very long time since Theo's dad finished his PhD,
so I don't just want to switch it on and, you know, set fire to it.
And while I have checked out that a lot of the resistors, you know, resist the right amount
and capacitors still capacitor enough for it and haven't leaked out the smelly fish juice,
there is still a lot for me to check over on this machine before I apply power to it,
including the PSU itself, which I haven't got round to looking over,
or in fact testing any of the output voltages off first,
so there's a lot of work to be done before it can just be powered up.
I also still need to find the software from somewhere, so it, you know, does something.
But the kind of machine you see here represents the majority case of usage of the transputer,
back at that point in time.
People used it like a big parallel CPU accelerator,
you could bolt onto the end of another computer and, you know,
get it to do some serious computation with the sort of thing that your little host machine
couldn't really manage, even if your host machine was, you know,
a fairly decent sunwork station.
By the way, if anyone out there watching this video happens to have a copy of the software
for the transputer side, well, the PC side of the transputer system,
please do get in contact, because I am having no luck finding this stuff.
Commercially speaking, it must have been doing pretty darn well
with its range of chips it had available,
even from when it first got some chips out there into the market.
But being state-owned meant that shortly into its existence,
when there was a change of government,
that political party was ideologically opposed to the idea of the state owning basically anything.
So for most of its existence, the government had been looking for someone to sell it off to,
and eventually it did find a company to sell it to,
and it was probably not the most suitable company in the world.
It was Fawn EMI, a company at that point that can best be described as a sprawling basket case of an organization.
It seemed to have a finger in everything it made surprisingly rubbish record players.
I know we owned one.
Video recorders, TVs.
It was even into games publishing.
Fire EMI, it was a fairly big record label with a healthy film and TV business.
It owned part of ITV's Thames television.
It had TV and film studios.
It owned a big chunk of a cinema chain.
It was even invested into HBO,
and for some reason also owned a chain of bingo halls.
Of course it did.
And now it was the proud owner of a supercomputing processor manufacturing company.
Yeah, that fitted well.
I mean, you definitely can see the parallels between designing supercomputing chips,
and owning the winter gardens in Blackpool, for example.
Now, none of you will be surprised to hear that a forced wedding between Fawn and Imos
did not work out well for Imos.
It kind of struggled to get the funding it needed to do what it needed to do,
in order to keep the transputer line moving forwards.
And for all the time that Fawn owned them,
they seemed to be trying to find someone else to sell them to.
And eventually, they sold them to the sprawling basket case of a French organization that was Thompson.
Of course, Thompson quickly went bust,
ended up selling that bit of itself off to ST Microelectronics,
where finally the transputer did actually get some use,
unfortunately in set-top boxes of all things.
Now, during this period of being owned by Fawn,
the government did try and help Imos out,
and the transputer, and created the Transputer Support Center in Chaffield,
that was there to support people developing applications and uses for the transputer.
And for a brief while, they did a pretty good job.
The problem wasn't so much finding applications for the transputer,
it was that Imos' underfunding when it was being owned by Fawn
stopped them from really keeping ahead of where they needed to be,
to keep the individual power of each chip up where it needs to be,
and also hitting scales and costs that would be useful.
But during its time, the transputer was commercially very successful
and did achieve what it intended to do.
It brought parallel computing to a much wider audience in a useful way.
I wouldn't want you to come away from this video feeling that the transputer was a failed idea,
because it really, really wasn't.
It's just under the ownership of Fawn,
it just didn't receive the kind of money it should have done
because Fawn was busy circling the drain and turning into an absolute basket case of a company.
And then, unfortunately, Imos was then sold to another basket case,
doing exactly the same thing.
By the time ST Microelectronics got their hands on it,
well, it was a little too late and costly to get back into the high-performance computer space with it.
But ST did base a number of products on the transputer,
and he had a nice second life in set-top boxes and there's microcontrollers.
Which to be fair, was ST's wheelhouse.
Now, I'm going to end this video talking about one of the, well,
oddest moments of the transputer's history, or at least to me.
Remember I mentioned before that there was a Unix-like operating system for it?
Well, we're going to discuss that, and the Atari Transputer Workstation.
Which is, of course, the answer to the question I posed at the beginning of the video of,
what do you think Atari's least successful computer product is?
Hands up, how many of you had actually heard of the Atari Transputer Workstation?
I mean, if you had, you could probably guess that this was the answer,
given that this is a video about transputers.
So Atari, the people responsible for the ST in the 2600,
decided they would make a high-end workstation based on the transputer,
because no brand screams high-performance workstation like Atari.
Yeah, this must be Atari's, like, most obscure product, the Transputer Workstation,
and probably the one that sold the least quantity of anything they ever made.
And this is the company that brought us the Jaguar.
And more impressively, the Jaguar CD add-on.
Because what a failed console really needs is an add-on,
even if you did get armoured to chanks in to help with the design work.
Now, this story begins with Dr. Tim King,
a name that some of you will probably recognise,
and a few of you apparently know based on what you said in the comment section.
He worked for Metacomco, the people who did the operating system for the Amiga,
and when he left it, he started a new company with the aim to make a Unix-like operating system for the transputer.
Now, I say Unix-like, and that's because he was going to implement the POSIX standard for it,
so you could compile and run Unix software for it.
But it couldn't be Unix, because there's no MMU in the transputer.
It doesn't do memory management,
so you couldn't have a full Unix operating system on there with all the memory protection.
Now, that's not as important as it might sound for making a Unix-like operating system,
because thanks to the transputer being a stack-based system with stack registers,
not being able to do memory protection the way that Unix normally would with the MMU
is not as big a killer as you might think.
We weren't going to create a, you know, pre-OS10 Apple Mac here.
Applications would have to go out of their way to mess with the memory space of another application.
It isn't just a case of forgetting to put a null byte on the end of a string,
and all look, the word processor somehow overrope the operating system.
And if one task happens to hug an entire CPU,
well, in a transputer system, that's not really such a big deal
as it would be in, say, a single processor Mac.
Also, another transputer chip could just tell it to stop.
So, you know, process management is a real thing in this operating system.
So, the idea was to create this operating system.
He lost.
There'd be Unix-like, or Unix-like enough, and running on the transputer.
Of course, then they decided that they should split part of the company off
to start looking at building hardware that this could run on.
But as almost everyone who worked for the company was former Metacon Co. people,
they had contacts in both Commodore and Atari, as they'd worked on stuff for both of them in the past.
And one of the things they initially tried was making a hardware card to go inside the Amiga 2000.
So, the Amiga 2000 would act as a boot system, help bring up the card,
he lost would run on there, and IO would be handled by the Amiga.
Now, it seems Commodore did what Commodore does and just randomly lose interest in this,
so they started talking to Atari.
And that's where we get the Atari Transputer Workstation.
And it works on a very similar idea.
What we essentially have is an Atari ST Mega that acts much like a host system does
for all the other ways we run the transputer.
It does the IO, it provides it with floppy and hard drive controller,
and is what boots he lost.
Once he lost his booted and running on the transputers,
the 68000 on the Mega ST board just becomes an IO controller at this point.
Now, you may have noticed that at no point have I mentioned the Atari ST's graphics chips getting used.
And that's because the Transputer Workstation's got its own graphics card that's driven from the transputer side of things.
And that's known as the Blossom Board.
Now, this meant that Helix could run X-Window, so regular X-Window software would compile and work on it.
And the Blossom Board gives us the kind of resolution you'd expect on a high-end Unix workstation of the time.
So we've got 1280 by 690 at 16 colors, palette 4096.
We've got 1024 by 768 with 256 colors, and that's driven out of a palette of 16.7 million.
And then we've got modes that are more in your lower-end workstation than PC range, so we can do 640 by 480.
And again, that's limited to 256 colors at once.
And then we have a 512 by 480, in which we can use the whole 16.7 million colors at once.
There are also a number of sort of hardware acceleration features in there that X can make use of,
so we've got a Blitter that allows us to move sort of tiles and windows around and other things.
Now, oddly, for a transputer-based workstation, as it comes, it chips with one whole transputer chip,
which for a system designed to do stuff in parallel is less than ideal,
but the motherboard does have four slots to add what they refer to as farm cards,
which is a card containing four additional transputer chips,
which means that with a fully-expanded system, you could have 17 transputers in there, which is, you know, not bad.
And it actually has one of those InMOS switch chips we talked about earlier, so the internship performance was actually pretty good.
Originally Atari were going to call this thing the Abac, but then decided that they call it the Atari Transputer Workstation,
as that gave people a clue as to what it was and why you might want it.
And Atari can't have been expecting a lot of people to buy one of these, I mean, it's a very high-end system.
But you've got to think that Atari were hoping that they were going to sell more than 350 of them,
because that's the total number that were produced, and apparently somewhere between 50 to 100 of those were prototype machines as well.
Now, I am told this machine was actually designed in Sheffield, the city that I live in,
but I can't find any hard documenting evidence to say, yeah, 100% that was true,
but given that the transputer support centre was based in Sheffield, and Department of Computer Science Sheffield was one of the leading research groups for the transputer,
yeah, I'm not going to die of shock if that was true.
Now, one of the interesting spin-offs from the whole Transputer Workstation thing was the blossom graphics card,
apparently the team that were responsible for that, then would later go on to work on the Atari Jaguar and properly kill Atari this time.
But I think the Atari Transputer Workstation is probably the rarest of all Atari computers, because even the Falcon outsold this thing, and the Stacey.
Now, with all that said, would I like to get my hands on one? Yeah, of course, I'd absolutely love to. Who wouldn't want to?
Well, I suppose people who only want to play games on their Atari, because there were no games released for the Atari Transputer Workstation,
although you could actually get the Mega ST that was built in as the IO processor to actually boot TOS and then run whatever game you wanted to on it.
It's just, it's a heck of a lot of electricity to run a standard ST game.
If you've got all the way to the end, I would like to say thank you very much for watching.
I would also like to give a huge thank you to FIO, one of Sheffield's more talented instrument makers, musical instruments that is, not scientific,
for giving me the transputer system to set this whole video off on course.
If you enjoyed the video, please click that thumbs up buttony thing that YouTube's given us to indicate that that's a fact.
And as ever, please feel free to chat away in the comments, because it is quite nice to fully nerd out with all of you.
And if you'd like to help the channel out, please click the subscribe button, because that way YouTube actually believes that it's worth telling people that the videos exist.
