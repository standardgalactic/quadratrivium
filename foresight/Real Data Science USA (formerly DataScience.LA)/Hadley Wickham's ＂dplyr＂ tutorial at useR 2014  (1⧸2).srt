1
00:00:00,000 --> 00:00:15,440
Okay, I've been given a knob, so let's get started and it sounds like I've cut out already

2
00:00:15,440 --> 00:00:17,240
so let's just abandon this.

3
00:00:17,240 --> 00:00:18,920
Can you hear me okay at the back?

4
00:00:18,920 --> 00:00:19,920
Yeah.

5
00:00:19,920 --> 00:00:20,920
Is that okay?

6
00:00:20,920 --> 00:00:21,920
Great.

7
00:00:21,920 --> 00:00:27,160
So, today, hopefully, you're here to learn about D-Player and to give you a little bit

8
00:00:27,160 --> 00:00:32,360
of context, I'm interested in data manipulation in the context of data analysis, so you've

9
00:00:32,360 --> 00:00:39,160
got raw data coming in one side, understanding knowledge and insight coming out the other.

10
00:00:39,160 --> 00:00:44,800
And today we're going to be focusing on data manipulation, but I see this really as being

11
00:00:44,800 --> 00:00:49,880
part of the cycle of other data analysis or data science tools.

12
00:00:49,880 --> 00:00:54,080
So to me, there's really four main tools for data science.

13
00:00:54,080 --> 00:00:58,880
So the first is data tidying, getting your data into a form that's actually suitable

14
00:00:58,880 --> 00:00:59,880
for analysis.

15
00:00:59,880 --> 00:01:05,040
Now, in this diagram, I've drawn this little short arrow, but as many of you have actually

16
00:01:05,040 --> 00:01:09,520
worked with real data, you know, often the arrow is all the way around on the other

17
00:01:09,520 --> 00:01:10,520
side of the room.

18
00:01:10,520 --> 00:01:15,600
So often, one of the most challenging parts of doing a data analysis is just getting the

19
00:01:15,600 --> 00:01:20,720
data in the right form that you can work with it.

20
00:01:20,720 --> 00:01:25,400
Now once you've done that, you'll often do some basic manipulation, data transformation.

21
00:01:25,400 --> 00:01:28,880
You'll create new variables that are functions of existing variables.

22
00:01:28,880 --> 00:01:32,800
You might do a little bit of aggregation and so on, and that's mostly what we're going

23
00:01:32,800 --> 00:01:34,160
to be talking about today.

24
00:01:34,160 --> 00:01:39,920
But it's also important to bear in mind that you're doing this to fit into a cycle.

25
00:01:39,920 --> 00:01:43,400
You want your tools to easily plug into the other.

26
00:01:43,400 --> 00:01:49,000
You want your manipulation tools to easily plug into your data visualization and modeling

27
00:01:49,000 --> 00:01:50,160
tools.

28
00:01:50,160 --> 00:01:54,280
The visualizations are great because they uncover the unexpected.

29
00:01:54,280 --> 00:01:59,040
They help you make precise your questions about the data, but the problem with visualizations

30
00:01:59,040 --> 00:02:01,320
is that they fundamentally don't scale.

31
00:02:01,320 --> 00:02:06,280
On the other hand, the kind of complementary tools, statistical models, machine learning,

32
00:02:06,280 --> 00:02:10,600
data mining, basically whenever you've made a question sufficiently precise that you

33
00:02:10,600 --> 00:02:15,760
can answer it with a handful of summary statistics or an algorithm, you've got a model.

34
00:02:15,760 --> 00:02:21,040
Tools are great because they scale, but they don't fundamentally surprise your linear models

35
00:02:21,040 --> 00:02:23,880
and they're going to tell you your data is nonlinear.

36
00:02:23,880 --> 00:02:29,480
So any real data analysis, you're going to be circling between these tools multiple times.

37
00:02:29,480 --> 00:02:31,840
You might start by looking at a plot.

38
00:02:31,840 --> 00:02:34,040
Based on that plot, you develop a model.

39
00:02:34,040 --> 00:02:36,560
You then take some predictions from that model.

40
00:02:36,560 --> 00:02:39,600
You transform your data to look at the residuals.

41
00:02:39,600 --> 00:02:42,040
You visualize those and so on and so on.

42
00:02:42,040 --> 00:02:46,240
So while today we're going to be focusing on data manipulation, data transformation,

43
00:02:46,240 --> 00:02:53,480
the goal is to have tools that embed seamlessly into your data analysis process.

44
00:02:53,480 --> 00:03:00,560
And so the family of tools that I've been working on and others at RStudio are working

45
00:03:00,560 --> 00:03:06,880
on have recently sort of undergone somewhat of a change and if you're interested in hearing

46
00:03:06,880 --> 00:03:10,800
more about that, I'm going to be talking about that in my talk on Tuesday.

47
00:03:10,800 --> 00:03:17,000
So basically, for data tidying, now the tidier package, which is kind of another update of

48
00:03:17,000 --> 00:03:23,120
reshape and then reshape2 and now tidier, pliers become de-plier and ggplot is in the

49
00:03:23,120 --> 00:03:26,040
process of turning into ggplot2.

50
00:03:26,040 --> 00:03:32,600
And as you'll see today, there are some kind of very important commonalities that underlie

51
00:03:32,600 --> 00:03:37,720
all of these tools that make it easy for you to use them.

52
00:03:37,720 --> 00:03:39,800
So today we're going to talk about data manipulation.

53
00:03:39,800 --> 00:03:44,560
We'll start with a little intro to the data we're going to be using, then talk about single

54
00:03:44,560 --> 00:03:51,520
table verbs, a little bit about data pipelines, some more complicated types of filtering and

55
00:03:51,520 --> 00:03:59,320
grouping, joins, a very general do operator and then I'm just going to talk very briefly

56
00:03:59,320 --> 00:04:04,240
at the end about how all the tools you've learned today working with data frames also

57
00:04:04,240 --> 00:04:07,480
apply to databases as well.

58
00:04:07,480 --> 00:04:14,360
But before we begin, I kind of want to start with the caveat and then the bad news is whenever

59
00:04:14,360 --> 00:04:18,040
you're learning a new tool for a long time, you're going to suck.

60
00:04:18,040 --> 00:04:24,640
It's going to be very frustrating, but the good news is that that is typical, it's something

61
00:04:24,640 --> 00:04:28,200
that happens to everyone and it's only temporary.

62
00:04:28,200 --> 00:04:34,360
Unfortunately, there is no way to going from knowing nothing about a subject to knowing

63
00:04:34,360 --> 00:04:40,000
something about a subject and being an expert in it without going through a period of great

64
00:04:40,000 --> 00:04:42,880
frustration and much suckiness.

65
00:04:42,880 --> 00:04:46,000
But remember, when you're getting frustrated, that's a good thing.

66
00:04:46,000 --> 00:04:52,080
That's typical, it's temporary, keep pushing through and in time will become second nature.

67
00:04:52,080 --> 00:04:55,280
Okay, with that said, let's get started.

68
00:04:55,280 --> 00:05:00,000
We're going to be looking at four interrelated datasets today.

69
00:05:00,000 --> 00:05:04,840
I have given you them in a RStudio project.

70
00:05:04,840 --> 00:05:11,000
So if you have downloaded the code and data, you can double click on this R approach file.

71
00:05:11,000 --> 00:05:19,000
If you're not using RStudio, my apologies, but you can just change your working directory

72
00:05:19,000 --> 00:05:21,680
and I'll assume you'll be okay with that.

73
00:05:21,680 --> 00:05:27,080
So in this directory, we've got the scripts, which mostly correspond to what we're going

74
00:05:27,080 --> 00:05:31,120
to be working through today, and then we have got four datasets.

75
00:05:31,120 --> 00:05:42,640
I wanted to start briefly with a couple of hints about using ... I'm not going to do

76
00:05:42,640 --> 00:05:43,640
that.

77
00:05:43,640 --> 00:05:44,840
Okay, I'm going to tell you about the data.

78
00:05:44,840 --> 00:05:46,680
So we've got these four datasets.

79
00:05:46,680 --> 00:05:48,840
The first one is the main one we're going to be looking at.

80
00:05:48,840 --> 00:05:54,320
This is not a huge dataset, but it's recently sized, about 200,000 observations.

81
00:05:54,320 --> 00:06:01,240
This is every flight that departed from Houston in 2011, and then we have got three datasets

82
00:06:01,240 --> 00:06:05,800
that we can join with this dataset that provide useful additional metadata.

83
00:06:05,800 --> 00:06:09,760
So we have some data about the weather for each hour.

84
00:06:09,760 --> 00:06:14,920
As you can imagine, if we're looking at flight data, you might be interested in what causes

85
00:06:14,920 --> 00:06:15,920
flight delays.

86
00:06:15,920 --> 00:06:19,320
The weather is obviously a cause of that.

87
00:06:19,320 --> 00:06:23,720
You might also be interested in, are there planes that are consistently delayed?

88
00:06:23,720 --> 00:06:27,120
So we have some information about the planes that are flying these routes, when they're

89
00:06:27,120 --> 00:06:31,360
built, what type of plane they are, how many people they see, and so on.

90
00:06:31,360 --> 00:06:35,400
And then we have some information about the airports that the flights are flying to, which

91
00:06:35,400 --> 00:06:40,240
is mainly their location, so you can plot them on a map.

92
00:06:40,240 --> 00:06:45,520
Now to load this data in, I'm not going to talk to you about this code.

93
00:06:45,520 --> 00:06:51,000
It's there in the first file.

94
00:06:51,680 --> 00:06:55,960
To get started, you're going to want to source that file in that it's going to create these

95
00:06:55,960 --> 00:06:57,680
four datasets.

96
00:06:57,680 --> 00:07:04,320
The only thing that you might not have seen before is this tableDF function.

97
00:07:04,320 --> 00:07:11,080
What that is going to do, it's going to turn these data frames into dpliers tableDF objects,

98
00:07:11,080 --> 00:07:18,360
which are almost identical in every single way to data frames, except when you print out

99
00:07:18,360 --> 00:07:25,280
a tableDF, it does not print out 10,000 rows, it will only print out the first 10 rows.

100
00:07:25,280 --> 00:07:29,200
So it gives you some summary information about what's going on in that dataset.

101
00:07:29,200 --> 00:07:31,800
It prints all the variables that fit in one screen.

102
00:07:31,800 --> 00:07:37,960
It might make us a little wider.

103
00:07:37,960 --> 00:07:42,280
And if they don't fit on the screen, it just gives you a little summary, the names of the

104
00:07:42,280 --> 00:07:45,080
variables and what type of variable they have.

105
00:07:46,080 --> 00:07:55,400
It's identical in every way to a data frame, except when you look at the class, it is one,

106
00:07:55,400 --> 00:07:57,120
well, two additional things.

107
00:07:57,120 --> 00:08:02,120
If a package doesn't know about dpliers, it would just treat it exactly like a data frame.

108
00:08:02,120 --> 00:08:06,280
In fact, it is a data frame, it's just a special type of data frame.

109
00:08:06,280 --> 00:08:13,320
So we've got flights data, about 200,000 observations, weather, which is about 8,000.

110
00:08:13,320 --> 00:08:19,080
These planes, about 3,000, and then about 3,000 airports.

111
00:08:19,080 --> 00:08:25,240
Okay, now that you've introduced yourselves and hopefully have some questions to ask about

112
00:08:25,240 --> 00:08:31,200
the data, we're going to dive in and learn the first five important verbs associated

113
00:08:31,200 --> 00:08:32,480
with dpliers.

114
00:08:32,480 --> 00:08:37,360
So my kind of contention is if you know these five verbs and combine them with another tool,

115
00:08:37,400 --> 00:08:43,720
we'll learn about shortly, this will solve 90% say of your data manipulation problems.

116
00:08:43,720 --> 00:08:48,480
And that's really important because now when you have a data manipulation problem, instead

117
00:08:48,480 --> 00:08:53,080
of thinking, well, there's like 1,000 functions and base R, which one of those is the one

118
00:08:53,080 --> 00:08:54,080
I need.

119
00:08:54,080 --> 00:08:57,000
Now you just need to look through these five verbs.

120
00:08:57,000 --> 00:09:02,600
So the first verb is filter, where you're going to select rows based on the values of

121
00:09:02,600 --> 00:09:04,960
their variables.

122
00:09:04,960 --> 00:09:11,360
You might also want to just focus on a certain number of columns or variables that select.

123
00:09:11,360 --> 00:09:15,880
You might want to reorder the rows or arrange the data frame.

124
00:09:15,880 --> 00:09:20,320
You might want to add new variables that are functions of existing variables.

125
00:09:20,320 --> 00:09:26,160
Or finally, you might want to reduce multiple values down to a single value.

126
00:09:26,160 --> 00:09:29,760
So all of these functions work exactly the same way.

127
00:09:29,760 --> 00:09:32,760
The first argument is always a data frame.

128
00:09:32,760 --> 00:09:38,360
The subsequent arguments tell you what to do with that data frame.

129
00:09:38,360 --> 00:09:41,280
And then they always return a data frame.

130
00:09:41,280 --> 00:09:45,600
So none of these functions modify in place, so whenever you use them, if you do want to

131
00:09:45,600 --> 00:09:50,040
modify your data frame, you're going to have to assign the results.

132
00:09:50,040 --> 00:09:53,800
A lot of the times I'm just going to show you, I'm just going to run the code and kind

133
00:09:53,800 --> 00:09:57,200
of show you the results on screen and then throw it away.

134
00:09:57,200 --> 00:10:00,760
That's great for teaching, but obviously when you're doing a real data analysis, you actually

135
00:10:00,760 --> 00:10:03,480
want to save what you've done.

136
00:10:03,480 --> 00:10:10,240
To illustrate these, I'm going to start with a very simple five-row data frame, which I'm

137
00:10:10,240 --> 00:10:14,480
also going to show in slides.

138
00:10:14,480 --> 00:10:19,400
So filter allows you to select rows that match some criteria.

139
00:10:19,400 --> 00:10:26,160
So here we're going to say filterDF, we want all the rows with color equals blue.

140
00:10:26,160 --> 00:10:28,760
So this is the input and this is the output.

141
00:10:28,760 --> 00:10:36,200
So if you've used subset before and baseR, this is very, very similar.

142
00:10:36,200 --> 00:10:42,720
If you're looking to see if a value matches one of multiple values, you can use in.

143
00:10:42,720 --> 00:10:49,080
And then there's a whole set of other operators, the regular logic, the numerical comparison

144
00:10:49,080 --> 00:10:53,040
operator is greater than, greater than, equal to, so on and so on.

145
00:10:53,040 --> 00:10:57,400
Not equal, equal and member of the set.

146
00:10:57,400 --> 00:11:08,040
You might also want to use the Boolean algebra, so or and and not an exclusive or.

147
00:11:08,040 --> 00:11:12,120
So I'm just showing this here as a reference, hopefully if you've used R a little bit, you're

148
00:11:12,120 --> 00:11:14,720
familiar with these already.

149
00:11:14,720 --> 00:11:17,720
There are kind of two main things to be cautious of.

150
00:11:17,720 --> 00:11:23,760
When you're working with vectors, you want to use the single bar and the single ampersand.

151
00:11:23,760 --> 00:11:27,800
If you're working with scalars, if you're working with single numbers, like you're using

152
00:11:27,800 --> 00:11:32,520
an if statement, that's when you use the double bar or the double ampersand.

153
00:11:32,520 --> 00:11:37,880
But here we're going to be working with vectors and values, so we want to always use the single

154
00:11:37,880 --> 00:11:41,000
vertical bar or the single ampersand.

155
00:11:41,000 --> 00:11:44,480
And we'll talk about this in a little, very shortly.

156
00:11:44,480 --> 00:11:50,840
So what I want you guys to do is practice using filter by extracting the flights that

157
00:11:50,840 --> 00:11:51,840
match these criteria.

158
00:11:51,840 --> 00:11:56,880
So first of all, all of the flights that went to San Francisco, all of the flights that were

159
00:11:56,880 --> 00:12:01,840
in January, or all flights that were delayed by more than an hour, or they departed between

160
00:12:01,840 --> 00:12:08,040
midnight and 5 AM, or when the arrival delay was twice as much as the departure delay or

161
00:12:08,040 --> 00:12:09,040
corrective.

162
00:12:09,040 --> 00:12:10,040
So I'll give you a few minutes.

163
00:12:10,040 --> 00:12:13,480
I'll circulate around and help you again, there's only one of me and there's a lot of

164
00:12:13,480 --> 00:12:14,480
you.

165
00:12:14,480 --> 00:12:20,240
So if you get stuck on my behalf, please feel free to ask your neighbor for help.

166
00:12:20,240 --> 00:12:24,280
Okay, so let's have a go at how you might tackle this.

167
00:12:24,280 --> 00:12:36,800
So we wanted to find all the flights that went to SFO Oakland.

168
00:12:36,800 --> 00:12:39,040
So you might start like this.

169
00:12:39,040 --> 00:12:42,360
So there's 2,800 that went to SFO.

170
00:12:42,360 --> 00:12:45,720
Now a common mistake when you're first using R, you would say, I want the destination to

171
00:12:45,720 --> 00:12:53,600
equal San Francisco or Oakland, you do that, that's not going to work.

172
00:12:53,600 --> 00:12:59,240
So you either have to be very explicit and say destination equals SFO, or destination

173
00:12:59,240 --> 00:13:11,000
equals Oakland, or use the in operator.

174
00:13:11,000 --> 00:13:18,520
So that's all of the flights that his destination was San Francisco or Oakland.

175
00:13:18,520 --> 00:13:25,560
In January, that's actually a tricky one.

176
00:13:25,560 --> 00:13:32,360
The easiest way to do that is, in this case I know the first flight was January 1st, so

177
00:13:32,360 --> 00:13:38,520
I can just say give me all the flights before the 1st of February.

178
00:13:38,520 --> 00:13:47,120
That didn't work surprisingly, so we might need to just, oh, 2011, yeah, okay.

179
00:13:47,120 --> 00:13:53,200
So let's just see how that thing went down.

180
00:13:53,200 --> 00:14:00,120
So that gets us 18,000 flights in January.

181
00:14:00,120 --> 00:14:06,320
Again between midnight and 5am, there are two ways you can write this, so probably you

182
00:14:06,320 --> 00:14:12,240
might have written this, all of the flights where hour is greater than or equal to 0,

183
00:14:12,240 --> 00:14:14,720
and hour is less than or equal to 5.

184
00:14:14,720 --> 00:14:20,160
With filter, you can also supply multiple arguments to it, and those arguments are all

185
00:14:20,160 --> 00:14:23,520
ended together.

186
00:14:23,520 --> 00:14:29,480
There's no real benefit to doing it this way, rather than this way, except maybe one day

187
00:14:29,480 --> 00:14:33,680
we might be able to figure out how to do these in parallel, and it might be twice as fast

188
00:14:33,680 --> 00:14:35,920
if you do it this way.

189
00:14:35,920 --> 00:14:40,800
And then finally, all the flights delayed by more than an hour.

190
00:14:40,800 --> 00:14:45,520
There's two delay variables here, the departure delay and the arrival delay.

191
00:14:45,520 --> 00:14:51,080
I should have mentioned if it's a negative delay, that means it arrived early or departed

192
00:14:51,080 --> 00:14:52,080
early.

193
00:14:52,080 --> 00:14:58,600
We can find all the flights that were delayed by more than an hour, right, 10,000 flights,

194
00:14:58,920 --> 00:15:05,680
if any of you have, I assume you've all flown in the US, so you're not surprised by this.

195
00:15:05,680 --> 00:15:09,080
And we can also use more complicated expressions in there.

196
00:15:09,080 --> 00:15:15,480
We can find all of the flights where the arrival delay is twice as much as the departure delay.

197
00:15:15,480 --> 00:15:21,480
So these are cases where we have lost time during the flight.

198
00:15:22,480 --> 00:15:29,480
Well, on these ones we might also want to say, and the departure delay was greater than zero.

199
00:15:32,480 --> 00:15:38,480
Right, so this minute, this flight, wait a little longer, right?

200
00:15:38,480 --> 00:15:45,480
Yeah, this flight was two minutes delayed departing, and it was six minutes late on arrival.

201
00:15:47,480 --> 00:15:50,480
Any questions about Felter?

202
00:15:50,480 --> 00:15:51,480
Yep.

203
00:15:51,480 --> 00:15:54,480
Why would you use Felter instead of Felter?

204
00:15:54,480 --> 00:16:01,480
Because it's faster, because it is better defined, it just does one thing, and it does

205
00:16:01,480 --> 00:16:06,480
it one thing well, or a subset does multiple things, and then finally you can use Felter

206
00:16:06,480 --> 00:16:11,480
on database tables and it will generate SQL for you.

207
00:16:11,480 --> 00:16:14,480
Will it work on regular data frames?

208
00:16:14,480 --> 00:16:15,480
Yes.

209
00:16:16,480 --> 00:16:21,480
Okay, the next verb is select, which allows you to pick variables you're interested in.

210
00:16:21,480 --> 00:16:26,480
So this is most useful if you have a data set that has hundreds of variables, and you

211
00:16:26,480 --> 00:16:28,480
just want to look at a few of them.

212
00:16:28,480 --> 00:16:35,480
The syntax is the name of the data frame, and then the list of the variables you want to keep.

213
00:16:35,480 --> 00:16:40,480
So select works like the select argument to subset if you've ever used that.

214
00:16:40,480 --> 00:16:44,480
But basically you can treat the names of variables like their positions.

215
00:16:44,480 --> 00:16:50,480
So you can say use negative to say give me all the variables that are not color.

216
00:16:52,480 --> 00:16:55,480
What I want you guys to do now is read the help for select.

217
00:16:55,480 --> 00:17:01,480
What are the other ways you can select sets of variables, and then see if you can come

218
00:17:01,480 --> 00:17:07,480
up with three ways of selecting out the two delay variables from this data set.

219
00:17:07,480 --> 00:17:18,480
So if you look at the help for select, you'll see that all of these main verbs are documented

220
00:17:18,480 --> 00:17:24,480
together, and you'll see that I've been courteous to Americans.

221
00:17:24,480 --> 00:17:33,480
But if we scroll down, we can see that there are five ways of, well, at least five ways,

222
00:17:33,480 --> 00:17:36,480
extra ways of selecting variables.

223
00:17:36,480 --> 00:17:41,480
So you can select variables that start with a common prefix, then end with a common suffix

224
00:17:41,480 --> 00:17:47,480
that contain some character string or the match a regular expression, or you can do

225
00:17:47,480 --> 00:17:51,480
like a numeric range, say all of x1 to x10.

226
00:17:51,480 --> 00:17:57,480
So this is my attempt to come up with every way that you might reasonably want to select a variable.

227
00:17:57,480 --> 00:18:02,480
So a couple of ways you can select these two.

228
00:18:02,480 --> 00:18:06,480
You can select them just as individual variables.

229
00:18:06,480 --> 00:18:12,480
You could say pick all of the variables between from a rival delay to departure delay.

230
00:18:12,480 --> 00:18:18,480
You could find all of the variables that end with delay or all of the variables that contain delay.

231
00:18:18,480 --> 00:18:20,480
There's lots of other ways too.

232
00:18:20,480 --> 00:18:22,480
You could also write this.

233
00:18:22,480 --> 00:18:24,480
You could say make a vector of columns.

234
00:18:24,480 --> 00:18:26,480
We're using C.

235
00:18:26,480 --> 00:18:34,480
Basically, inside select variable names, you can treat them like the numeric positions.

236
00:18:34,480 --> 00:18:40,480
So anything you can do to a numeric position, you can do with a variable name.

237
00:18:40,480 --> 00:18:45,480
So the goal of select is to make it easy to refer to your variables by name.

238
00:18:45,480 --> 00:18:49,480
It's always a better idea to refer to your variables by name than by position,

239
00:18:49,480 --> 00:18:56,480
because you don't want your data input format changes and you're referring to variables by position.

240
00:18:56,480 --> 00:19:05,480
It's very easy to have code that works but gives you meaningless results because it's using the wrong variables.

241
00:19:05,480 --> 00:19:11,480
The next verb is a range which just changes the order of the rows.

242
00:19:11,480 --> 00:19:15,480
So if you just use a variable and orders it by that,

243
00:19:15,480 --> 00:19:20,480
you can order in descending order by using the desk wrapper.

244
00:19:20,480 --> 00:19:28,480
And I don't show you here but you can add additional variables to break ties if there are ties in this first variable.

245
00:19:28,480 --> 00:19:33,480
So again, order the flights by departure date and time.

246
00:19:33,480 --> 00:19:37,480
Figure out using a range which flights were most delayed

247
00:19:37,480 --> 00:19:42,480
and then which flights caught up the most time during the flight.

248
00:19:43,480 --> 00:19:46,480
So again, a few minutes to work on this and I'll show you the answers.

249
00:19:46,480 --> 00:19:51,480
Okay, if we want to order the flights by their departure date,

250
00:19:51,480 --> 00:19:55,480
we could say order it by date and then hour and then minute.

251
00:19:55,480 --> 00:19:59,480
Just want to see multiple, ordering by multiple variables.

252
00:19:59,480 --> 00:20:06,480
So you can see the first flight left on January 1st, one minute after midnight.

253
00:20:06,480 --> 00:20:15,480
So I should mention this depth variable is the departure time as like a 24-hour time

254
00:20:15,480 --> 00:20:18,480
but all the zeros got dropped off.

255
00:20:18,480 --> 00:20:23,480
And then the hour and minute are just that, this time split up into those pieces.

256
00:20:23,480 --> 00:20:30,480
So for example in this column, there's not going to be a 661,

257
00:20:30,480 --> 00:20:35,480
no flights left, it's 61 minutes past 6am.

258
00:20:35,480 --> 00:20:39,480
This is just a weird decimal time.

259
00:20:39,480 --> 00:20:43,480
We want to sort, find the most delayed, that's just a matter of sorting

260
00:20:43,480 --> 00:20:46,480
so that our delays are descending.

261
00:20:46,480 --> 00:20:51,480
We can see the most delayed flight was 981 minutes.

262
00:20:51,480 --> 00:20:56,480
So an impressive 16-hour delay.

263
00:20:56,480 --> 00:20:59,480
Now normally flights aren't delayed that long,

264
00:20:59,480 --> 00:21:02,480
not because flights aren't delayed that long

265
00:21:02,480 --> 00:21:10,480
but generally airlines cancel the flights to make their departure delay statistics look better.

266
00:21:10,480 --> 00:21:13,480
So similarly we could do the same thing for arrival delay,

267
00:21:13,480 --> 00:21:21,480
which is going to give us a pretty similar message.

268
00:21:21,480 --> 00:21:28,480
And the other thing I wanted to show here is that you can arrange on kind of compound expressions.

269
00:21:28,480 --> 00:21:33,480
I wanted to find the planes in a mode made up the most time

270
00:21:33,480 --> 00:21:37,480
and there's the biggest difference between the departure and arrival delay.

271
00:21:37,480 --> 00:21:43,480
So there's a flight, so for example this flight left one minute early

272
00:21:43,480 --> 00:21:47,480
and it arrived an hour and 10 minutes early.

273
00:21:47,480 --> 00:21:51,480
So you can arrange on compound expressions

274
00:21:51,480 --> 00:21:55,480
although generally it's going to be easier to add that as a new variable

275
00:21:55,480 --> 00:21:58,480
depending on what's going on and then arranged by that.

276
00:21:58,480 --> 00:22:00,480
Why are you reporting this descending?

277
00:22:00,480 --> 00:22:06,480
Because I wanted to find the one, I wanted to find the biggest difference.

278
00:22:06,480 --> 00:22:09,480
I may have...

279
00:22:09,480 --> 00:22:12,480
Actually I got the same result.

280
00:22:12,480 --> 00:22:16,480
I may have hit this round the wrong way.

281
00:22:16,480 --> 00:22:19,480
Oh yeah.

282
00:22:19,480 --> 00:22:23,480
So depending on which way round we need to track the arrival from departure

283
00:22:23,480 --> 00:22:26,480
to ascending or ascending.

284
00:22:26,480 --> 00:22:29,480
Any other questions about arrange?

285
00:22:29,480 --> 00:22:33,480
I had a problem with the NAs, the first time I did something

286
00:22:33,480 --> 00:22:35,480
I got all the NAs on top.

287
00:22:35,480 --> 00:22:39,480
I did it in a different way than you did once I...

288
00:22:39,480 --> 00:22:44,480
So NAs should always sort to the end and if they don't that's a bug.

289
00:22:44,480 --> 00:22:49,480
They do but what if I want the smallest without the NAs?

290
00:22:49,480 --> 00:22:53,480
So you have to use felt as a removal of the NAs currently.

291
00:22:53,480 --> 00:22:58,480
Is there an opposite of descending?

292
00:22:58,480 --> 00:23:01,480
Yes, just don't do descending.

293
00:23:01,480 --> 00:23:05,480
I think also the way that...

294
00:23:05,480 --> 00:23:11,480
I believe that if you do descending or descending that is ascending.

295
00:23:11,480 --> 00:23:15,480
It's the one still.

296
00:23:15,480 --> 00:23:19,480
If you really want an ascending function you can just do that.

297
00:23:25,480 --> 00:23:29,480
Okay, the starting to get more complicated.

298
00:23:29,480 --> 00:23:33,480
The next verb is mutate which allows you to add new variables

299
00:23:33,480 --> 00:23:36,480
that are functions of existing variables.

300
00:23:36,480 --> 00:23:39,480
So here we're adding a new variable called double

301
00:23:39,480 --> 00:23:42,480
which is two times our existing value variable.

302
00:23:42,480 --> 00:23:45,480
So again in all of the dplyr functions

303
00:23:45,480 --> 00:23:50,480
you never need to explicitly refer to the data frame that you're working with.

304
00:23:50,480 --> 00:23:52,480
That's always implicit.

305
00:23:52,480 --> 00:23:55,480
It's going to look for this value inside the data frame

306
00:23:55,480 --> 00:23:59,480
rather than in your global environment.

307
00:23:59,480 --> 00:24:04,480
Mutate is very similar to transform and base r if you've used that.

308
00:24:04,480 --> 00:24:08,480
One big difference with mutate is you can do multiple...

309
00:24:08,480 --> 00:24:13,480
In additional mutations or additional transformations

310
00:24:13,480 --> 00:24:16,480
you can refer to variables that you just created

311
00:24:16,480 --> 00:24:20,480
which you cannot do in transform and is a little bit annoying.

312
00:24:20,480 --> 00:24:24,480
So here we first double value and then we make a new column called quadruple

313
00:24:24,480 --> 00:24:28,480
which is just two times double a variable we just created.

314
00:24:28,480 --> 00:24:32,480
How does it compare to within?

315
00:24:32,480 --> 00:24:37,480
Basically I think within is a hideous monstrosity that no one should ever use.

316
00:24:37,480 --> 00:24:41,480
And if you want to know more I can tell you.

317
00:24:41,480 --> 00:24:45,480
Okay so your turn to create some variables.

318
00:24:45,480 --> 00:24:49,480
See if you can figure out the speed and miles per hour

319
00:24:49,480 --> 00:24:52,480
which flight flew the fastest.

320
00:24:52,480 --> 00:24:56,480
See if you can create a new variable that shows how much time was made up

321
00:24:56,480 --> 00:24:58,480
during the course of the flight or lost.

322
00:24:58,480 --> 00:25:02,480
And then how did I compute the hour and minute variables

323
00:25:02,480 --> 00:25:06,480
from that departure variable?

324
00:25:06,480 --> 00:25:10,480
Okay so if I wanted to compute the speed

325
00:25:10,480 --> 00:25:14,480
that is just the distance divided by the time divided by 60

326
00:25:14,480 --> 00:25:17,480
because time is in minutes.

327
00:25:17,480 --> 00:25:21,480
So if we print that out

328
00:25:21,480 --> 00:25:25,480
you know unless you make your screen really wide you can't see everything.

329
00:25:25,480 --> 00:25:29,480
So one thing you can do is use the view function

330
00:25:29,480 --> 00:25:33,480
which works in RStudio and other R ideas

331
00:25:33,480 --> 00:25:37,480
which will just show all of your variables

332
00:25:37,480 --> 00:25:40,480
on a nice kind of scrollable table

333
00:25:40,480 --> 00:25:45,480
or you can always just select the variables you want to see

334
00:25:45,480 --> 00:25:49,480
so from like departure to speed.

335
00:25:49,480 --> 00:25:54,480
So if you use a very handy way

336
00:25:54,480 --> 00:25:58,480
of just viewing a data frame in a nice table.

337
00:25:58,480 --> 00:26:02,480
Did you change flights?

338
00:26:02,480 --> 00:26:06,480
Yes so in this case I modified flights because I wanted to create

339
00:26:06,480 --> 00:26:10,480
a new variable and modify that original data set to add that new variable

340
00:26:10,480 --> 00:26:14,480
and then I can sort it to find the fastest ones

341
00:26:14,480 --> 00:26:19,480
and see 760 miles an hour.

342
00:26:19,480 --> 00:26:23,480
When you mutate does there

343
00:26:23,480 --> 00:26:27,480
an easy way to specify a position?

344
00:26:27,480 --> 00:26:31,480
No so when you add new variables they always go on to the end of the data frame.

345
00:26:31,480 --> 00:26:35,480
If you wanted to reposition them there's currently no particularly easy

346
00:26:35,480 --> 00:26:39,480
way to do that. You could create a big select statement but it's

347
00:26:39,480 --> 00:26:43,480
kind of a pain.

348
00:26:43,480 --> 00:26:47,480
We could create this delta variable

349
00:26:47,480 --> 00:26:51,480
which is just the difference between the departure and arrival delay.

350
00:26:51,480 --> 00:26:55,480
If you didn't care about the direction

351
00:26:55,480 --> 00:26:59,480
you could do whatever you want in this

352
00:26:59,480 --> 00:27:03,480
whatever R expression you want.

353
00:27:03,480 --> 00:27:07,480
The last thing I wanted to mention

354
00:27:07,480 --> 00:27:11,480
is just a useful trick. If I have this departure

355
00:27:11,480 --> 00:27:15,480
we have the first two digits of the hour and the second two digits of the minute

356
00:27:15,480 --> 00:27:19,480
you can use the integer division operator

357
00:27:19,480 --> 00:27:23,480
and the modular operator to extract those pieces out.

358
00:27:23,480 --> 00:27:27,480
This is just a useful little trick if you want to pull out certain digits

359
00:27:27,480 --> 00:27:31,480
from a long number.

360
00:27:31,480 --> 00:27:35,480
Any other questions about

361
00:27:35,480 --> 00:27:39,480
mutate?

362
00:27:39,480 --> 00:27:43,480
Okay next I want to talk about a new function group phi

363
00:27:43,480 --> 00:27:47,480
which is summarized together. You can use summarized and regular data frames

364
00:27:47,480 --> 00:27:51,480
but you always get a data frame that is only one row

365
00:27:51,480 --> 00:27:55,480
which is typically not very useful.

366
00:27:55,480 --> 00:27:59,480
That's exactly what I said. So summarized is going to give you a one row

367
00:27:59,480 --> 00:28:03,480
data frame. What you're going to want to do is actually group your data first

368
00:28:03,480 --> 00:28:07,480
and then summarized will operate by group.

369
00:28:07,480 --> 00:28:11,480
Here we're saying create a new data frame

370
00:28:11,480 --> 00:28:15,480
and use this old data frame grouped by color

371
00:28:15,480 --> 00:28:19,480
and then we're going to summarize this and for each group

372
00:28:19,480 --> 00:28:23,480
compute the total by summing up the value of your vehicle.

373
00:28:23,480 --> 00:28:27,480
So I'm going to create four useful ways

374
00:28:27,480 --> 00:28:31,480
of grouping the flights data. We might want to group it by date

375
00:28:31,480 --> 00:28:35,480
we might want to group it by hour, we might want to group it by plane

376
00:28:35,480 --> 00:28:39,480
or we might want to group it by destination.

377
00:28:39,480 --> 00:28:43,480
Just to bear in mind when you do create all these groupings

378
00:28:43,480 --> 00:28:47,480
dplyr is sort of smart enough that doesn't create a complete copy

379
00:28:47,480 --> 00:28:51,480
of your data every single time. It works the same way as the rest

380
00:28:51,480 --> 00:28:55,480
of R, it doesn't sort of a lazy way. If you modify one of these data sets

381
00:28:55,480 --> 00:28:59,480
you'll have to create a copy but until you do so they all point to the same

382
00:28:59,480 --> 00:29:03,480
place. So grouping data doesn't

383
00:29:03,480 --> 00:29:07,480
use up, it doesn't create a copy of the data, it does use up a little bit more

384
00:29:07,480 --> 00:29:11,480
memory because grouping builds up an index so

385
00:29:11,480 --> 00:29:15,480
you know what observations are in each group.

386
00:29:15,480 --> 00:29:19,480
Now there are lots of summary functions you can use, most of these

387
00:29:19,480 --> 00:29:23,480
are pretty standard, minimum, medium, maximum

388
00:29:23,480 --> 00:29:27,480
you can extract contiles, there are two functions

389
00:29:27,480 --> 00:29:31,480
that are special in dplyr in which just

390
00:29:31,480 --> 00:29:35,480
tells you how many observations are in a group, indistinct

391
00:29:35,480 --> 00:29:39,480
and I should have a

392
00:29:39,480 --> 00:29:43,480
x there, tells you how many different observations

393
00:29:43,480 --> 00:29:47,480
are in a variable, that's the same as doing

394
00:29:47,480 --> 00:29:51,480
length unique x but it's a little bit more efficient.

395
00:29:51,480 --> 00:29:55,480
You can sum, you can compute means. It's also often

396
00:29:55,480 --> 00:29:59,480
useful to do summaries of logical vectors because

397
00:29:59,480 --> 00:30:03,480
when you take a logical vector and treat it like it's a numeric

398
00:30:03,480 --> 00:30:07,480
all the falses turn into zeros and the trues turn into ones

399
00:30:07,480 --> 00:30:11,480
so what that means is when you sum a logical vector it tells

400
00:30:11,480 --> 00:30:15,480
you how many trues there were so this would tell you how many values

401
00:30:15,480 --> 00:30:19,480
of x are greater than 10. The mean is just the sum

402
00:30:19,480 --> 00:30:23,480
divided by the length so the mean of a logical vector is the

403
00:30:23,480 --> 00:30:27,480
proportion of values of the true. There's a really useful little

404
00:30:27,480 --> 00:30:31,480
trick. And then lots of other ways of measuring

405
00:30:31,480 --> 00:30:35,480
the variation, standard deviation, variance, interquadal range,

406
00:30:35,480 --> 00:30:39,480
median absolute deviation. So these are all just standard

407
00:30:39,480 --> 00:30:43,480
functions.

408
00:30:49,480 --> 00:30:53,480
Okay what I want you guys, what I've shown here is the distribution

409
00:30:53,480 --> 00:30:57,480
of departure delays. So I've got two views of this

410
00:30:57,480 --> 00:31:01,480
one which shows all of the delays and one which just shows the delays less than

411
00:31:01,480 --> 00:31:05,480
like two hours. So what I want you to do with your neighbor for two minutes

412
00:31:05,480 --> 00:31:09,480
is just quickly brainstorm given this distribution

413
00:31:09,480 --> 00:31:13,480
given what you know about flight delays

414
00:31:13,480 --> 00:31:17,480
how might you want to summarize this distribution. What function might

415
00:31:17,480 --> 00:31:21,480
what you want to use or do you want to use a mean or a median or something else.

416
00:31:21,480 --> 00:31:25,480
So take two minutes starting now, talk it over with your neighbor.

417
00:31:25,480 --> 00:31:29,480
So we're going to summarize by date

418
00:31:29,480 --> 00:31:33,480
what's one way we could use to summarize the distribution of

419
00:31:33,480 --> 00:31:37,480
delays. The median? We could use the median

420
00:31:37,480 --> 00:31:41,480
I mean probably want to use the departure delay

421
00:31:41,480 --> 00:31:45,480
so if we just run that

422
00:31:49,480 --> 00:31:53,480
we are going to get a new data frame and it is

423
00:31:53,480 --> 00:31:57,480
265 rows which you should have anticipated. You know how many

424
00:31:57,480 --> 00:32:01,480
days there are in a year. I've got one little problem here

425
00:32:01,480 --> 00:32:05,480
probably want to use Na.Ramq was true

426
00:32:05,480 --> 00:32:09,480
let's do that.

427
00:32:09,480 --> 00:32:13,480
How else could we summarize it?

428
00:32:13,480 --> 00:32:17,480
The mean is another obvious one

429
00:32:17,480 --> 00:32:21,480
let's just assume we've got that

430
00:32:21,480 --> 00:32:25,480
What else might you want to see? 90% quanta.

431
00:32:25,480 --> 00:32:29,480
Okay we've got max

432
00:32:29,480 --> 00:32:33,480
and actually typing all of this Na.Ramq.true

433
00:32:33,480 --> 00:32:37,480
is going to get tedious real fast so I'm just going to filter

434
00:32:37,480 --> 00:32:41,480
it and I say I want all of the ones that are not missing

435
00:32:41,480 --> 00:32:45,480
okay so that way I can just drop this off

436
00:32:45,480 --> 00:32:49,480
and I'll bother typing it

437
00:32:49,480 --> 00:32:53,480
so that's the median, the mean, the maximum and then something

438
00:32:53,480 --> 00:32:57,480
in between we could get the 90th

439
00:32:57,480 --> 00:33:01,480
quanta. Remember how to use that function

440
00:33:05,480 --> 00:33:09,480
Any other ideas?

441
00:33:09,480 --> 00:33:13,480
Is there a way to for example

442
00:33:13,480 --> 00:33:17,480
compute more than just the 90% quanta?

443
00:33:17,480 --> 00:33:21,480
Currently you have to type them in like this

444
00:33:21,480 --> 00:33:25,480
but there will be some way in the future that you do that

445
00:33:29,480 --> 00:33:33,480
Yeah we could also do some thresholds

446
00:33:33,480 --> 00:33:37,480
well first of all we could say

447
00:33:37,480 --> 00:33:41,480
what's the proportion that is delayed

448
00:33:41,480 --> 00:33:45,480
so that is the average of all of the ones where the delay

449
00:33:45,480 --> 00:33:49,480
is greater than zero

450
00:33:49,480 --> 00:33:51,480
so that is the

451
00:33:51,480 --> 00:33:53,480
presently high

452
00:33:53,480 --> 00:33:57,480
but you might say well who really cares if it's only

453
00:33:57,480 --> 00:34:01,480
a 5 minute delay or a 10 minute delay

454
00:34:01,480 --> 00:34:05,480
I might just say arbitrarily like a 15 minute delay that's not bad

455
00:34:05,480 --> 00:34:09,480
Why are we looking at departure not arrival?

456
00:34:09,480 --> 00:34:13,480
Yeah so equally you might say well it's

457
00:34:13,480 --> 00:34:17,480
the impact on our arrival that's what really matters because that's

458
00:34:17,480 --> 00:34:21,480
someone picking us up at the airport and our flight

459
00:34:21,480 --> 00:34:25,480
is now delayed by an hour and they're getting angry so we could switch all this to arrival

460
00:34:25,480 --> 00:34:29,480
delay too and the results are pretty similar

461
00:34:29,480 --> 00:34:33,480
So 15 minutes is kind of arbitrary you know you could look at a few

462
00:34:33,480 --> 00:34:37,480
other ones if you wanted to do that

463
00:34:37,480 --> 00:34:41,480
Yes? Is there a way to use this summer function?

464
00:34:41,480 --> 00:34:45,480
You could but I'm not sure

465
00:34:45,480 --> 00:34:49,480
that you would want to

466
00:34:53,480 --> 00:34:57,480
So current well so there's two problems

467
00:34:57,480 --> 00:35:01,480
so first of all I mean this is a reasonable thing to do

468
00:35:01,480 --> 00:35:05,480
currently though summarise

469
00:35:05,480 --> 00:35:09,480
when you summarise you have to reduce to a single number not multiple numbers

470
00:35:09,480 --> 00:35:13,480
because again a future version of dplyr will let you summarise multiple numbers

471
00:35:13,480 --> 00:35:17,480
at some point in the future

472
00:35:17,480 --> 00:35:21,480
What did I do? So this is what I did

473
00:35:21,480 --> 00:35:25,480
and you have to have urm everywhere

474
00:35:25,480 --> 00:35:29,480
or you can filter out all of the flights

475
00:35:29,480 --> 00:35:33,480
that are not missing

476
00:35:33,480 --> 00:35:37,480
but don't have a missing departure

477
00:35:37,480 --> 00:35:41,480
So this kind of

478
00:35:41,480 --> 00:35:45,480
brings me to my next point at any like in any real data manipulation

479
00:35:45,480 --> 00:35:49,480
task you're probably not just going to use one verb

480
00:35:49,480 --> 00:35:53,480
but you're going to string multiple verbs together first of all we group it

481
00:35:53,480 --> 00:35:57,480
then we filter it then we summarise it and we want some way

482
00:35:57,480 --> 00:36:01,480
to kind of express that more naturally or more simply which is that

483
00:36:01,480 --> 00:36:05,480
the idea of having a data pipeline

484
00:36:05,480 --> 00:36:09,480
you need to do quickly just take a minute

485
00:36:09,480 --> 00:36:13,480
talk this over with your neighbour what does this snippet of code do

486
00:36:13,480 --> 00:36:17,480
so you've got one minute starting now

487
00:36:17,480 --> 00:36:21,480
okay so this looks pretty complicated

488
00:36:21,480 --> 00:36:25,480
but if you kind of really carefully pass it you have to start from the innermost thing

489
00:36:25,480 --> 00:36:29,480
we're going to start with the flights data then we're going to filter it

490
00:36:29,480 --> 00:36:33,480
to remove any missing delays then we're going to group it by date

491
00:36:33,480 --> 00:36:37,480
in an hour then we're going to summarise it to compute the average delay

492
00:36:37,480 --> 00:36:41,480
and the number of observations in that hour

493
00:36:41,480 --> 00:36:45,480
then we're going to filter it to only look at the hours that have more than 10 flights

494
00:36:45,480 --> 00:36:49,480
so it's not too complicated

495
00:36:49,480 --> 00:36:53,480
but we have to read it in quite an unnatural way to read insight out

496
00:36:53,480 --> 00:36:57,480
and then also like the arguments to filter are quite far away

497
00:36:57,480 --> 00:37:01,480
so instead

498
00:37:01,480 --> 00:37:05,480
what we're going to talk about after the coffee break is this pipe operator

499
00:37:05,480 --> 00:37:09,480
and you'll see that that makes the code quite

500
00:37:09,480 --> 00:37:13,480
a lot easier to read so the coffee is outside now

501
00:37:13,480 --> 00:37:17,480
so let's have a coffee break and come back at

502
00:37:17,480 --> 00:37:21,480
3.40

503
00:37:21,480 --> 00:37:25,480
music

504
00:37:25,480 --> 00:37:29,480
music

505
00:37:29,480 --> 00:37:33,480
about this operator

506
00:37:33,480 --> 00:37:37,480
called the pipe operator so what this basically does

507
00:37:37,480 --> 00:37:41,480
is take the thing on the left hand side of the pipe

508
00:37:41,480 --> 00:37:45,480
and put it as the first argument as a thing on the right hand side

509
00:37:45,480 --> 00:37:49,480
and the advantage of this is it allows us to take something like this

510
00:37:49,480 --> 00:37:53,480
which is pretty hard to read and transform it into something like this

511
00:37:53,480 --> 00:37:57,480
and this is pretty easy to read particularly if you pronounce this operator as then

512
00:37:57,480 --> 00:38:01,480
so we can read this take flights then filter it

513
00:38:01,480 --> 00:38:05,480
to remove any values with a missing value for depth delay

514
00:38:05,480 --> 00:38:09,480
then group it by date and hour then summarise it

515
00:38:09,480 --> 00:38:13,480
computing the average delay and the number of observations in the group

516
00:38:13,480 --> 00:38:17,480
then filter it to look at all of the

517
00:38:17,480 --> 00:38:21,480
observations we're going to have in 10 so

518
00:38:21,480 --> 00:38:25,480
this pipe operator allows us to

519
00:38:25,480 --> 00:38:29,480
form chains of complicated

520
00:38:29,480 --> 00:38:33,480
data transformation operations that are made up of very simple pieces so the goal

521
00:38:33,480 --> 00:38:37,480
is you make something complex by joining together many simple

522
00:38:37,480 --> 00:38:41,480
things that are easy to understand in isolation

523
00:38:41,480 --> 00:38:45,480
so I want to give you some practice using that with

524
00:38:45,480 --> 00:38:49,480
three challenges so which destinations have the highest

525
00:38:49,480 --> 00:38:53,480
average delays which flights

526
00:38:53,480 --> 00:38:57,480
happen every day and where do they fly to and then on average

527
00:38:57,480 --> 00:39:01,480
how do delays vary over the course of a day

528
00:39:01,480 --> 00:39:05,480
and if you're going to do that probably look at the non cancelled flights

529
00:39:05,480 --> 00:39:09,480
so those three challenges are relatively

530
00:39:09,480 --> 00:39:13,480
simple but you're going to need to string together multiple of these verbs

531
00:39:13,480 --> 00:39:17,480
you've seen before you might have to use a range and group by and summarise

532
00:39:17,480 --> 00:39:21,480
and filter in some order so have a go at joining those together

533
00:39:21,480 --> 00:39:25,480
and again if you get stuck I'll come around and help you out or better

534
00:39:25,480 --> 00:39:29,480
ask your neighbour

535
00:39:29,480 --> 00:39:33,480
do well we start with the flights

536
00:39:33,480 --> 00:39:37,480
what are we going to do to that filter to remove anase yep we can

537
00:39:37,480 --> 00:39:41,480
remove the anase let's do rival delays

538
00:39:41,480 --> 00:39:45,480
what next

539
00:39:45,480 --> 00:39:49,480
group by so group by is kind of a fundamentally

540
00:39:49,480 --> 00:39:53,480
like statistical operator you're saying what is the unit of interest in this analysis

541
00:39:53,480 --> 00:39:57,480
and in this case it's the destination of the flight

542
00:39:57,480 --> 00:40:01,480
then for each destination what we want to do is summarise it

543
00:40:01,480 --> 00:40:05,480
I'm just going to say let's use the mean delay

544
00:40:05,480 --> 00:40:09,480
the other thing I think you always want to do whenever you do

545
00:40:09,480 --> 00:40:13,480
a group by summary is you always want to recall the number of observations

546
00:40:13,480 --> 00:40:17,480
in each group because when you start looking at these averages

547
00:40:17,480 --> 00:40:21,480
you know if there's a destination that has the highest

548
00:40:21,480 --> 00:40:25,480
average delay but only one flight flew there

549
00:40:25,480 --> 00:40:29,480
and that's probably not as interesting and then if we want to focus on the most

550
00:40:29,480 --> 00:40:33,480
delayed flights we're going to arrange it in

551
00:40:33,480 --> 00:40:37,480
descending mean so let's run this

552
00:40:37,480 --> 00:40:41,480
they've worked so you can see this is a good example

553
00:40:41,480 --> 00:40:45,480
so there's this airport BBT which

554
00:40:45,480 --> 00:40:49,480
see

555
00:40:49,480 --> 00:40:53,480
I think I've already looked at this before so that is Jack Brooks

556
00:40:53,480 --> 00:40:57,480
Brooks regional airport on the airport

557
00:40:57,480 --> 00:41:01,480
of Texas so there are only three flights flew there the entire

558
00:41:01,480 --> 00:41:05,480
year you're not going to trust this average that much so

559
00:41:05,480 --> 00:41:09,480
what we might want to do is filter out all of the

560
00:41:09,480 --> 00:41:13,480
flights where there's less than 10 observations

561
00:41:13,480 --> 00:41:17,480
we'll run that pipeline again

562
00:41:17,480 --> 00:41:21,480
now again I've constructed this pipeline

563
00:41:21,480 --> 00:41:25,480
just by typing every step and it worked

564
00:41:25,480 --> 00:41:29,480
which I have to say I'm slightly amazed at but generally when you're creating pipelines

565
00:41:29,480 --> 00:41:33,480
you want to do it a step at a time and this is one reason

566
00:41:33,480 --> 00:41:37,480
that I think the default printing is really important

567
00:41:37,480 --> 00:41:41,480
because you can just print out the result at every stage and you can see does that look right or not

568
00:41:41,480 --> 00:41:45,480
if you have a normal data frame it will print all of it right

569
00:41:45,480 --> 00:41:49,480
yes so if you have a normal data frame

570
00:41:49,480 --> 00:41:53,480
it will print the whole thing and if you want to turn

571
00:41:53,480 --> 00:41:57,480
you can always take a normal data frame

572
00:41:57,480 --> 00:42:01,480
and the first thing you can do is pipe it into tables here and turn it

573
00:42:01,480 --> 00:42:05,480
into a data frame the other thing

574
00:42:05,480 --> 00:42:09,480
the other thing that's useful is you might often pipe

575
00:42:09,480 --> 00:42:13,480
this into something rather than just printing it you could pipe it into view

576
00:42:13,480 --> 00:42:17,480
if you wanted to see more of the data

577
00:42:17,480 --> 00:42:21,480
that's kind of interesting

578
00:42:21,480 --> 00:42:25,480
if you wanted to just

579
00:42:25,480 --> 00:42:29,480
kind of step through it

580
00:42:29,480 --> 00:42:33,480
you could do

581
00:42:33,480 --> 00:42:37,480
talk about you could do something like

582
00:42:37,480 --> 00:42:41,480
this

583
00:42:41,480 --> 00:42:45,480
maybe

584
00:42:45,480 --> 00:42:49,480
so we're just taking the row number and taking a modulo 5

585
00:42:49,480 --> 00:42:53,480
equals zero so that's going to give us every fifth that would be one way to do it

586
00:42:53,480 --> 00:42:57,480
so if you

587
00:42:57,480 --> 00:43:01,480
shows you everything well it shows you the first so many rows

588
00:43:01,480 --> 00:43:05,480
in the future I think we'll make it so it shows you every row

589
00:43:05,480 --> 00:43:09,480
in a way that's reasonably efficient

590
00:43:09,480 --> 00:43:13,480
the other thing that's useful is to pipe it to str so you can see exactly what variables

591
00:43:13,480 --> 00:43:17,480
you've created and if they're the right type and so on

592
00:43:17,480 --> 00:43:21,480
or if you're so inclined

593
00:43:21,480 --> 00:43:25,480
could you put in two functions like head and tail after each other

594
00:43:25,480 --> 00:43:29,480
you can't basically so you want a pipeline that has a split

595
00:43:29,480 --> 00:43:33,480
in it right you want to have a pipeline that one pipe goes to head

596
00:43:33,480 --> 00:43:37,480
and the other pipe goes to tail

597
00:43:37,480 --> 00:43:41,480
at the same time yeah I don't

598
00:43:41,480 --> 00:43:45,480
like a data table does that by default on that I think that's a nice idea

599
00:43:45,480 --> 00:43:49,480
the reason dply doesn't do it is because you can do that for data

600
00:43:49,480 --> 00:43:53,480
frames but you can't in general do that efficiently for database queries

601
00:43:53,480 --> 00:43:57,480
you can always use tail off

602
00:43:57,480 --> 00:44:01,480
so there's another handy keyboard

603
00:44:01,480 --> 00:44:05,480
shortcut in our studio which I

604
00:44:05,480 --> 00:44:09,480
suspect no one knows about because the only reason I know about it is the

605
00:44:09,480 --> 00:44:13,480
Joe who added it told me about it there's this command called rerun

606
00:44:13,480 --> 00:44:17,480
previous has anyone used rerun previous before

607
00:44:17,480 --> 00:44:21,480
so what that does is if you have selected a

608
00:44:21,480 --> 00:44:25,480
block of code and press command enter

609
00:44:25,480 --> 00:44:29,480
now if I modify it it's kind of annoying I have to select that

610
00:44:29,480 --> 00:44:33,480
block of code again or you can press command shift P

611
00:44:33,480 --> 00:44:37,480
and it just sends those same lines of code into the R console

612
00:44:37,480 --> 00:44:41,480
so this is really useful if you want to iterate rapidly on your pipeline

613
00:44:41,480 --> 00:44:45,480
you can easily change things and maybe I wanted an ascending order

614
00:44:45,480 --> 00:44:49,480
and just command shift P and rerun the whole pipeline

615
00:44:51,480 --> 00:44:55,480
okay

616
00:44:55,480 --> 00:44:59,480
okay so

617
00:44:59,480 --> 00:45:03,480
any questions about that pipeline that we created to solve that problem

618
00:45:03,480 --> 00:45:07,480
so the next one is which flights

619
00:45:07,480 --> 00:45:11,480
happen every day and where do they fly to

620
00:45:11,480 --> 00:45:15,480
no

621
00:45:15,480 --> 00:45:19,480
so what are we going to start with that

622
00:45:19,480 --> 00:45:23,480
and which flights fly every day of the year what's probably the first

623
00:45:23,480 --> 00:45:27,480
thing we want to do we want to group by

624
00:45:27,480 --> 00:45:31,480
and we want to do that by carrier and the flight number

625
00:45:31,480 --> 00:45:35,480
now we want to find all

626
00:45:35,480 --> 00:45:39,480
flights that flew every day of the year

627
00:45:39,480 --> 00:45:43,480
any ideas so we're going to summarize what might we summarize

628
00:45:43,480 --> 00:45:47,480
we might use the dates

629
00:45:47,480 --> 00:45:51,480
what how well we're going to use the date how what are we going to do with that

630
00:45:51,480 --> 00:45:55,480
oh so we could do we could do count

631
00:45:55,480 --> 00:45:59,480
flights

632
00:45:59,480 --> 00:46:03,480
we could do count and then we could filter by

633
00:46:03,480 --> 00:46:07,480
let's give us a name

634
00:46:07,480 --> 00:46:11,480
365

635
00:46:11,480 --> 00:46:15,480
I forgot to put two equals

636
00:46:15,480 --> 00:46:19,480
now the problem with this is that it's possible

637
00:46:19,480 --> 00:46:23,480
this flight flew

638
00:46:23,480 --> 00:46:27,480
twice on one day and didn't fly it all on another day

639
00:46:27,480 --> 00:46:31,480
I feel like that's yeah so actually

640
00:46:31,480 --> 00:46:35,480
this is my solution too but now I think a better way would be to say

641
00:46:35,480 --> 00:46:39,480
count the number of distinct

642
00:46:39,480 --> 00:46:43,480
dates so if there's 365 distinct

643
00:46:43,480 --> 00:46:47,480
dates then we know it's flown every day

644
00:46:47,480 --> 00:46:51,480
I think this would give us a slightly different answer

645
00:46:51,480 --> 00:46:55,480
well in this case it gives us the same answer because there aren't flights that fly

646
00:46:55,480 --> 00:46:59,480
every day and then

647
00:46:59,480 --> 00:47:03,480
fly twice on one day but not on another

648
00:47:03,480 --> 00:47:07,480
now what if we wanted to add see what destinations these flights flew to

649
00:47:07,480 --> 00:47:11,480
any thoughts on that

650
00:47:11,480 --> 00:47:15,480
we could just add to the group by

651
00:47:15,480 --> 00:47:19,480
there are other ways we could do this which we'll see later but in this case it's easy enough

652
00:47:19,480 --> 00:47:23,480
to just add that into the group by

653
00:47:23,480 --> 00:47:27,480
and see Honolulu and a lot of flights to New York

654
00:47:27,480 --> 00:47:31,480
and Chicago and Seattle and Miami I think

655
00:47:35,480 --> 00:47:39,480
the last one on average

656
00:47:39,480 --> 00:47:43,480
the non cancelled flights vary over the course of the day

657
00:47:43,480 --> 00:47:47,480
so again so first of all we always want to say

658
00:47:47,480 --> 00:47:51,480
they're not cancelled which I think

659
00:47:51,480 --> 00:47:55,480
because cancelled equals zero cancelled is

660
00:47:55,480 --> 00:47:59,480
a reason code associated with it and then normally

661
00:47:59,480 --> 00:48:03,480
once you've kind of filtered out clearly wrong things the first step is going to be

662
00:48:03,480 --> 00:48:07,480
grouping it here we want to group by hour say

663
00:48:07,480 --> 00:48:11,480
or maybe hour and minute

664
00:48:11,480 --> 00:48:15,480
and then summarize again we want to

665
00:48:15,480 --> 00:48:19,480
count how many observations on each group so we can disregard the delayed flights

666
00:48:19,480 --> 00:48:23,480
and we could do the mean

667
00:48:27,480 --> 00:48:31,480
departure delay

668
00:48:31,480 --> 00:48:35,480
and

669
00:48:35,480 --> 00:48:39,480
summarize not summary

670
00:48:39,480 --> 00:48:43,480
so now when you get to this point

671
00:48:43,480 --> 00:48:47,480
it starts to get easier to see

672
00:48:47,480 --> 00:48:51,480
what's going on with the visualizations so

673
00:48:51,480 --> 00:48:55,480
this is basically that pipeline I just showed you

674
00:48:55,480 --> 00:48:59,480
I think I've done a slightly differently I created a new variable called time

675
00:48:59,480 --> 00:49:03,480
which is just hour plus minute divided by 60 that gives me like a floating point

676
00:49:03,480 --> 00:49:07,480
number that smoothly varies over the course of the day

677
00:49:07,480 --> 00:49:11,480
group it, summarize it and then I'm going to do a little ggplot to

678
00:49:15,480 --> 00:49:19,480
plot it

679
00:49:19,480 --> 00:49:23,480
so you can see very early in the day

680
00:49:23,480 --> 00:49:27,480
we have this kind of scattered cloud of some plots that are very

681
00:49:27,480 --> 00:49:31,480
delayed what might these be

682
00:49:31,480 --> 00:49:35,480
the ones from the end of the previous night

683
00:49:35,480 --> 00:49:39,480
the ones from the end of the previous night and why are the averages so high

684
00:49:39,480 --> 00:49:43,480
so variable

685
00:49:43,480 --> 00:49:47,480
these are the ones that have hardly any data

686
00:49:47,480 --> 00:49:51,480
there are hardly any flights leave after midnight so these averages

687
00:49:51,480 --> 00:49:55,480
are kind of suspicious we're not really seeing much of a pattern we're just seeing

688
00:49:55,480 --> 00:49:59,480
individual flights that were delayed a really long time from the previous day

689
00:49:59,480 --> 00:50:03,480
so we might want to, so one we could show then the visualization

690
00:50:03,480 --> 00:50:07,480
is to make the points proportional to the

691
00:50:07,480 --> 00:50:11,480
number of observations or we could filter it and add some other stuff

692
00:50:11,480 --> 00:50:15,480
there's no schedule flights

693
00:50:15,480 --> 00:50:19,480
exactly there's no schedule flights yeah

694
00:50:19,480 --> 00:50:23,480
so there's some kind of interesting pattern going on here

695
00:50:23,480 --> 00:50:27,480
I don't really understand if it's possible it's an artifact

696
00:50:27,480 --> 00:50:31,480
but it looks like it added these white lines on every hour

697
00:50:31,480 --> 00:50:35,480
but it looks like there's some kind of pattern where they start off

698
00:50:35,480 --> 00:50:39,480
delays kind of accumulate over the course of the day

699
00:50:39,480 --> 00:50:43,480
but there's also some weird pattern within the hour where they accumulate and then they drop

700
00:50:43,480 --> 00:50:47,480
back a little which I don't know what's going on

701
00:50:47,480 --> 00:50:51,480
but certainly the suggestion is if you want to leave on time fly early in the day

702
00:50:51,480 --> 00:50:55,480
or late in the hour

703
00:50:55,480 --> 00:50:59,480
or late in the hour

704
00:51:03,480 --> 00:51:07,480
any questions about those pipelines in general or how you can combine

705
00:51:07,480 --> 00:51:11,480
these pieces with a pipe operator?

706
00:51:11,480 --> 00:51:15,480
range is generally what the advantage is to chaining versus having

707
00:51:15,480 --> 00:51:19,480
a ton of parentheses inside

708
00:51:19,480 --> 00:51:23,480
but the sole example is that it makes it easier for you to read and understand what's going on

709
00:51:23,480 --> 00:51:27,480
does any advantage just having it line by line

710
00:51:27,480 --> 00:51:31,480
no basically no

711
00:51:31,480 --> 00:51:35,480
save a little bit of memory but it's not

712
00:51:35,480 --> 00:51:39,480
yep

713
00:51:39,480 --> 00:51:43,480
so yeah in all the versions of D player used

714
00:51:43,480 --> 00:51:47,480
percent dot percent

715
00:51:47,480 --> 00:51:51,480
now I prefer percent greater than percent for two reasons

716
00:51:51,480 --> 00:51:55,480
first of all it's easy to type because you can hold your finger on the shift button the whole time

717
00:51:55,480 --> 00:51:59,480
and secondly I think it's not a

718
00:51:59,480 --> 00:52:03,480
symmetric operation so having an asymmetric operator

719
00:52:03,480 --> 00:52:07,480
helps you understand what's going on, the data is flowing from left to right

720
00:52:07,480 --> 00:52:11,480
any other questions?

721
00:52:11,480 --> 00:52:15,480
is there a particular preferred order?

722
00:52:15,480 --> 00:52:19,480
no obviously the less

723
00:52:19,480 --> 00:52:23,480
data you have to work with the faster things are going to be so that generally suggests you should

724
00:52:23,480 --> 00:52:27,480
filter early on and you know so

725
00:52:27,480 --> 00:52:31,480
if you use a database, a database looks at the sequence of all the

726
00:52:31,480 --> 00:52:35,480
operations and says oh you did this filter at the end but it would actually

727
00:52:35,480 --> 00:52:39,480
be way more efficient to do that at the beginning, D player doesn't do anything like that

728
00:52:39,480 --> 00:52:43,480
D player executes it exactly as you give it so if you

729
00:52:43,480 --> 00:52:47,480
can think of a faster way to order the operations it might be worthwhile

730
00:52:47,480 --> 00:52:51,480
to do so generally and I'm not really going to talk about

731
00:52:51,480 --> 00:52:55,480
performance today but generally if you've got million like

732
00:52:55,480 --> 00:52:59,480
less than 10 million observations you won't even have to worry

733
00:52:59,480 --> 00:53:03,480
about the performance it's going to be a few seconds and it's not

734
00:53:03,480 --> 00:53:07,480
like it's a waste of time worrying about it because it's not going to take you that long

735
00:53:07,480 --> 00:53:11,480
ok, the next thing I'm going to talk about

736
00:53:11,480 --> 00:53:15,480
is a great thing

737
00:53:15,480 --> 00:53:19,480
music

738
00:53:19,480 --> 00:53:23,480
music

739
00:53:23,480 --> 00:53:25,480
music

