WEBVTT

00:00.000 --> 00:15.440
Okay, I've been given a knob, so let's get started and it sounds like I've cut out already

00:15.440 --> 00:17.240
so let's just abandon this.

00:17.240 --> 00:18.920
Can you hear me okay at the back?

00:18.920 --> 00:19.920
Yeah.

00:19.920 --> 00:20.920
Is that okay?

00:20.920 --> 00:21.920
Great.

00:21.920 --> 00:27.160
So, today, hopefully, you're here to learn about D-Player and to give you a little bit

00:27.160 --> 00:32.360
of context, I'm interested in data manipulation in the context of data analysis, so you've

00:32.360 --> 00:39.160
got raw data coming in one side, understanding knowledge and insight coming out the other.

00:39.160 --> 00:44.800
And today we're going to be focusing on data manipulation, but I see this really as being

00:44.800 --> 00:49.880
part of the cycle of other data analysis or data science tools.

00:49.880 --> 00:54.080
So to me, there's really four main tools for data science.

00:54.080 --> 00:58.880
So the first is data tidying, getting your data into a form that's actually suitable

00:58.880 --> 00:59.880
for analysis.

00:59.880 --> 01:05.040
Now, in this diagram, I've drawn this little short arrow, but as many of you have actually

01:05.040 --> 01:09.520
worked with real data, you know, often the arrow is all the way around on the other

01:09.520 --> 01:10.520
side of the room.

01:10.520 --> 01:15.600
So often, one of the most challenging parts of doing a data analysis is just getting the

01:15.600 --> 01:20.720
data in the right form that you can work with it.

01:20.720 --> 01:25.400
Now once you've done that, you'll often do some basic manipulation, data transformation.

01:25.400 --> 01:28.880
You'll create new variables that are functions of existing variables.

01:28.880 --> 01:32.800
You might do a little bit of aggregation and so on, and that's mostly what we're going

01:32.800 --> 01:34.160
to be talking about today.

01:34.160 --> 01:39.920
But it's also important to bear in mind that you're doing this to fit into a cycle.

01:39.920 --> 01:43.400
You want your tools to easily plug into the other.

01:43.400 --> 01:49.000
You want your manipulation tools to easily plug into your data visualization and modeling

01:49.000 --> 01:50.160
tools.

01:50.160 --> 01:54.280
The visualizations are great because they uncover the unexpected.

01:54.280 --> 01:59.040
They help you make precise your questions about the data, but the problem with visualizations

01:59.040 --> 02:01.320
is that they fundamentally don't scale.

02:01.320 --> 02:06.280
On the other hand, the kind of complementary tools, statistical models, machine learning,

02:06.280 --> 02:10.600
data mining, basically whenever you've made a question sufficiently precise that you

02:10.600 --> 02:15.760
can answer it with a handful of summary statistics or an algorithm, you've got a model.

02:15.760 --> 02:21.040
Tools are great because they scale, but they don't fundamentally surprise your linear models

02:21.040 --> 02:23.880
and they're going to tell you your data is nonlinear.

02:23.880 --> 02:29.480
So any real data analysis, you're going to be circling between these tools multiple times.

02:29.480 --> 02:31.840
You might start by looking at a plot.

02:31.840 --> 02:34.040
Based on that plot, you develop a model.

02:34.040 --> 02:36.560
You then take some predictions from that model.

02:36.560 --> 02:39.600
You transform your data to look at the residuals.

02:39.600 --> 02:42.040
You visualize those and so on and so on.

02:42.040 --> 02:46.240
So while today we're going to be focusing on data manipulation, data transformation,

02:46.240 --> 02:53.480
the goal is to have tools that embed seamlessly into your data analysis process.

02:53.480 --> 03:00.560
And so the family of tools that I've been working on and others at RStudio are working

03:00.560 --> 03:06.880
on have recently sort of undergone somewhat of a change and if you're interested in hearing

03:06.880 --> 03:10.800
more about that, I'm going to be talking about that in my talk on Tuesday.

03:10.800 --> 03:17.000
So basically, for data tidying, now the tidier package, which is kind of another update of

03:17.000 --> 03:23.120
reshape and then reshape2 and now tidier, pliers become de-plier and ggplot is in the

03:23.120 --> 03:26.040
process of turning into ggplot2.

03:26.040 --> 03:32.600
And as you'll see today, there are some kind of very important commonalities that underlie

03:32.600 --> 03:37.720
all of these tools that make it easy for you to use them.

03:37.720 --> 03:39.800
So today we're going to talk about data manipulation.

03:39.800 --> 03:44.560
We'll start with a little intro to the data we're going to be using, then talk about single

03:44.560 --> 03:51.520
table verbs, a little bit about data pipelines, some more complicated types of filtering and

03:51.520 --> 03:59.320
grouping, joins, a very general do operator and then I'm just going to talk very briefly

03:59.320 --> 04:04.240
at the end about how all the tools you've learned today working with data frames also

04:04.240 --> 04:07.480
apply to databases as well.

04:07.480 --> 04:14.360
But before we begin, I kind of want to start with the caveat and then the bad news is whenever

04:14.360 --> 04:18.040
you're learning a new tool for a long time, you're going to suck.

04:18.040 --> 04:24.640
It's going to be very frustrating, but the good news is that that is typical, it's something

04:24.640 --> 04:28.200
that happens to everyone and it's only temporary.

04:28.200 --> 04:34.360
Unfortunately, there is no way to going from knowing nothing about a subject to knowing

04:34.360 --> 04:40.000
something about a subject and being an expert in it without going through a period of great

04:40.000 --> 04:42.880
frustration and much suckiness.

04:42.880 --> 04:46.000
But remember, when you're getting frustrated, that's a good thing.

04:46.000 --> 04:52.080
That's typical, it's temporary, keep pushing through and in time will become second nature.

04:52.080 --> 04:55.280
Okay, with that said, let's get started.

04:55.280 --> 05:00.000
We're going to be looking at four interrelated datasets today.

05:00.000 --> 05:04.840
I have given you them in a RStudio project.

05:04.840 --> 05:11.000
So if you have downloaded the code and data, you can double click on this R approach file.

05:11.000 --> 05:19.000
If you're not using RStudio, my apologies, but you can just change your working directory

05:19.000 --> 05:21.680
and I'll assume you'll be okay with that.

05:21.680 --> 05:27.080
So in this directory, we've got the scripts, which mostly correspond to what we're going

05:27.080 --> 05:31.120
to be working through today, and then we have got four datasets.

05:31.120 --> 05:42.640
I wanted to start briefly with a couple of hints about using ... I'm not going to do

05:42.640 --> 05:43.640
that.

05:43.640 --> 05:44.840
Okay, I'm going to tell you about the data.

05:44.840 --> 05:46.680
So we've got these four datasets.

05:46.680 --> 05:48.840
The first one is the main one we're going to be looking at.

05:48.840 --> 05:54.320
This is not a huge dataset, but it's recently sized, about 200,000 observations.

05:54.320 --> 06:01.240
This is every flight that departed from Houston in 2011, and then we have got three datasets

06:01.240 --> 06:05.800
that we can join with this dataset that provide useful additional metadata.

06:05.800 --> 06:09.760
So we have some data about the weather for each hour.

06:09.760 --> 06:14.920
As you can imagine, if we're looking at flight data, you might be interested in what causes

06:14.920 --> 06:15.920
flight delays.

06:15.920 --> 06:19.320
The weather is obviously a cause of that.

06:19.320 --> 06:23.720
You might also be interested in, are there planes that are consistently delayed?

06:23.720 --> 06:27.120
So we have some information about the planes that are flying these routes, when they're

06:27.120 --> 06:31.360
built, what type of plane they are, how many people they see, and so on.

06:31.360 --> 06:35.400
And then we have some information about the airports that the flights are flying to, which

06:35.400 --> 06:40.240
is mainly their location, so you can plot them on a map.

06:40.240 --> 06:45.520
Now to load this data in, I'm not going to talk to you about this code.

06:45.520 --> 06:51.000
It's there in the first file.

06:51.680 --> 06:55.960
To get started, you're going to want to source that file in that it's going to create these

06:55.960 --> 06:57.680
four datasets.

06:57.680 --> 07:04.320
The only thing that you might not have seen before is this tableDF function.

07:04.320 --> 07:11.080
What that is going to do, it's going to turn these data frames into dpliers tableDF objects,

07:11.080 --> 07:18.360
which are almost identical in every single way to data frames, except when you print out

07:18.360 --> 07:25.280
a tableDF, it does not print out 10,000 rows, it will only print out the first 10 rows.

07:25.280 --> 07:29.200
So it gives you some summary information about what's going on in that dataset.

07:29.200 --> 07:31.800
It prints all the variables that fit in one screen.

07:31.800 --> 07:37.960
It might make us a little wider.

07:37.960 --> 07:42.280
And if they don't fit on the screen, it just gives you a little summary, the names of the

07:42.280 --> 07:45.080
variables and what type of variable they have.

07:46.080 --> 07:55.400
It's identical in every way to a data frame, except when you look at the class, it is one,

07:55.400 --> 07:57.120
well, two additional things.

07:57.120 --> 08:02.120
If a package doesn't know about dpliers, it would just treat it exactly like a data frame.

08:02.120 --> 08:06.280
In fact, it is a data frame, it's just a special type of data frame.

08:06.280 --> 08:13.320
So we've got flights data, about 200,000 observations, weather, which is about 8,000.

08:13.320 --> 08:19.080
These planes, about 3,000, and then about 3,000 airports.

08:19.080 --> 08:25.240
Okay, now that you've introduced yourselves and hopefully have some questions to ask about

08:25.240 --> 08:31.200
the data, we're going to dive in and learn the first five important verbs associated

08:31.200 --> 08:32.480
with dpliers.

08:32.480 --> 08:37.360
So my kind of contention is if you know these five verbs and combine them with another tool,

08:37.400 --> 08:43.720
we'll learn about shortly, this will solve 90% say of your data manipulation problems.

08:43.720 --> 08:48.480
And that's really important because now when you have a data manipulation problem, instead

08:48.480 --> 08:53.080
of thinking, well, there's like 1,000 functions and base R, which one of those is the one

08:53.080 --> 08:54.080
I need.

08:54.080 --> 08:57.000
Now you just need to look through these five verbs.

08:57.000 --> 09:02.600
So the first verb is filter, where you're going to select rows based on the values of

09:02.600 --> 09:04.960
their variables.

09:04.960 --> 09:11.360
You might also want to just focus on a certain number of columns or variables that select.

09:11.360 --> 09:15.880
You might want to reorder the rows or arrange the data frame.

09:15.880 --> 09:20.320
You might want to add new variables that are functions of existing variables.

09:20.320 --> 09:26.160
Or finally, you might want to reduce multiple values down to a single value.

09:26.160 --> 09:29.760
So all of these functions work exactly the same way.

09:29.760 --> 09:32.760
The first argument is always a data frame.

09:32.760 --> 09:38.360
The subsequent arguments tell you what to do with that data frame.

09:38.360 --> 09:41.280
And then they always return a data frame.

09:41.280 --> 09:45.600
So none of these functions modify in place, so whenever you use them, if you do want to

09:45.600 --> 09:50.040
modify your data frame, you're going to have to assign the results.

09:50.040 --> 09:53.800
A lot of the times I'm just going to show you, I'm just going to run the code and kind

09:53.800 --> 09:57.200
of show you the results on screen and then throw it away.

09:57.200 --> 10:00.760
That's great for teaching, but obviously when you're doing a real data analysis, you actually

10:00.760 --> 10:03.480
want to save what you've done.

10:03.480 --> 10:10.240
To illustrate these, I'm going to start with a very simple five-row data frame, which I'm

10:10.240 --> 10:14.480
also going to show in slides.

10:14.480 --> 10:19.400
So filter allows you to select rows that match some criteria.

10:19.400 --> 10:26.160
So here we're going to say filterDF, we want all the rows with color equals blue.

10:26.160 --> 10:28.760
So this is the input and this is the output.

10:28.760 --> 10:36.200
So if you've used subset before and baseR, this is very, very similar.

10:36.200 --> 10:42.720
If you're looking to see if a value matches one of multiple values, you can use in.

10:42.720 --> 10:49.080
And then there's a whole set of other operators, the regular logic, the numerical comparison

10:49.080 --> 10:53.040
operator is greater than, greater than, equal to, so on and so on.

10:53.040 --> 10:57.400
Not equal, equal and member of the set.

10:57.400 --> 11:08.040
You might also want to use the Boolean algebra, so or and and not an exclusive or.

11:08.040 --> 11:12.120
So I'm just showing this here as a reference, hopefully if you've used R a little bit, you're

11:12.120 --> 11:14.720
familiar with these already.

11:14.720 --> 11:17.720
There are kind of two main things to be cautious of.

11:17.720 --> 11:23.760
When you're working with vectors, you want to use the single bar and the single ampersand.

11:23.760 --> 11:27.800
If you're working with scalars, if you're working with single numbers, like you're using

11:27.800 --> 11:32.520
an if statement, that's when you use the double bar or the double ampersand.

11:32.520 --> 11:37.880
But here we're going to be working with vectors and values, so we want to always use the single

11:37.880 --> 11:41.000
vertical bar or the single ampersand.

11:41.000 --> 11:44.480
And we'll talk about this in a little, very shortly.

11:44.480 --> 11:50.840
So what I want you guys to do is practice using filter by extracting the flights that

11:50.840 --> 11:51.840
match these criteria.

11:51.840 --> 11:56.880
So first of all, all of the flights that went to San Francisco, all of the flights that were

11:56.880 --> 12:01.840
in January, or all flights that were delayed by more than an hour, or they departed between

12:01.840 --> 12:08.040
midnight and 5 AM, or when the arrival delay was twice as much as the departure delay or

12:08.040 --> 12:09.040
corrective.

12:09.040 --> 12:10.040
So I'll give you a few minutes.

12:10.040 --> 12:13.480
I'll circulate around and help you again, there's only one of me and there's a lot of

12:13.480 --> 12:14.480
you.

12:14.480 --> 12:20.240
So if you get stuck on my behalf, please feel free to ask your neighbor for help.

12:20.240 --> 12:24.280
Okay, so let's have a go at how you might tackle this.

12:24.280 --> 12:36.800
So we wanted to find all the flights that went to SFO Oakland.

12:36.800 --> 12:39.040
So you might start like this.

12:39.040 --> 12:42.360
So there's 2,800 that went to SFO.

12:42.360 --> 12:45.720
Now a common mistake when you're first using R, you would say, I want the destination to

12:45.720 --> 12:53.600
equal San Francisco or Oakland, you do that, that's not going to work.

12:53.600 --> 12:59.240
So you either have to be very explicit and say destination equals SFO, or destination

12:59.240 --> 13:11.000
equals Oakland, or use the in operator.

13:11.000 --> 13:18.520
So that's all of the flights that his destination was San Francisco or Oakland.

13:18.520 --> 13:25.560
In January, that's actually a tricky one.

13:25.560 --> 13:32.360
The easiest way to do that is, in this case I know the first flight was January 1st, so

13:32.360 --> 13:38.520
I can just say give me all the flights before the 1st of February.

13:38.520 --> 13:47.120
That didn't work surprisingly, so we might need to just, oh, 2011, yeah, okay.

13:47.120 --> 13:53.200
So let's just see how that thing went down.

13:53.200 --> 14:00.120
So that gets us 18,000 flights in January.

14:00.120 --> 14:06.320
Again between midnight and 5am, there are two ways you can write this, so probably you

14:06.320 --> 14:12.240
might have written this, all of the flights where hour is greater than or equal to 0,

14:12.240 --> 14:14.720
and hour is less than or equal to 5.

14:14.720 --> 14:20.160
With filter, you can also supply multiple arguments to it, and those arguments are all

14:20.160 --> 14:23.520
ended together.

14:23.520 --> 14:29.480
There's no real benefit to doing it this way, rather than this way, except maybe one day

14:29.480 --> 14:33.680
we might be able to figure out how to do these in parallel, and it might be twice as fast

14:33.680 --> 14:35.920
if you do it this way.

14:35.920 --> 14:40.800
And then finally, all the flights delayed by more than an hour.

14:40.800 --> 14:45.520
There's two delay variables here, the departure delay and the arrival delay.

14:45.520 --> 14:51.080
I should have mentioned if it's a negative delay, that means it arrived early or departed

14:51.080 --> 14:52.080
early.

14:52.080 --> 14:58.600
We can find all the flights that were delayed by more than an hour, right, 10,000 flights,

14:58.920 --> 15:05.680
if any of you have, I assume you've all flown in the US, so you're not surprised by this.

15:05.680 --> 15:09.080
And we can also use more complicated expressions in there.

15:09.080 --> 15:15.480
We can find all of the flights where the arrival delay is twice as much as the departure delay.

15:15.480 --> 15:21.480
So these are cases where we have lost time during the flight.

15:22.480 --> 15:29.480
Well, on these ones we might also want to say, and the departure delay was greater than zero.

15:32.480 --> 15:38.480
Right, so this minute, this flight, wait a little longer, right?

15:38.480 --> 15:45.480
Yeah, this flight was two minutes delayed departing, and it was six minutes late on arrival.

15:47.480 --> 15:50.480
Any questions about Felter?

15:50.480 --> 15:51.480
Yep.

15:51.480 --> 15:54.480
Why would you use Felter instead of Felter?

15:54.480 --> 16:01.480
Because it's faster, because it is better defined, it just does one thing, and it does

16:01.480 --> 16:06.480
it one thing well, or a subset does multiple things, and then finally you can use Felter

16:06.480 --> 16:11.480
on database tables and it will generate SQL for you.

16:11.480 --> 16:14.480
Will it work on regular data frames?

16:14.480 --> 16:15.480
Yes.

16:16.480 --> 16:21.480
Okay, the next verb is select, which allows you to pick variables you're interested in.

16:21.480 --> 16:26.480
So this is most useful if you have a data set that has hundreds of variables, and you

16:26.480 --> 16:28.480
just want to look at a few of them.

16:28.480 --> 16:35.480
The syntax is the name of the data frame, and then the list of the variables you want to keep.

16:35.480 --> 16:40.480
So select works like the select argument to subset if you've ever used that.

16:40.480 --> 16:44.480
But basically you can treat the names of variables like their positions.

16:44.480 --> 16:50.480
So you can say use negative to say give me all the variables that are not color.

16:52.480 --> 16:55.480
What I want you guys to do now is read the help for select.

16:55.480 --> 17:01.480
What are the other ways you can select sets of variables, and then see if you can come

17:01.480 --> 17:07.480
up with three ways of selecting out the two delay variables from this data set.

17:07.480 --> 17:18.480
So if you look at the help for select, you'll see that all of these main verbs are documented

17:18.480 --> 17:24.480
together, and you'll see that I've been courteous to Americans.

17:24.480 --> 17:33.480
But if we scroll down, we can see that there are five ways of, well, at least five ways,

17:33.480 --> 17:36.480
extra ways of selecting variables.

17:36.480 --> 17:41.480
So you can select variables that start with a common prefix, then end with a common suffix

17:41.480 --> 17:47.480
that contain some character string or the match a regular expression, or you can do

17:47.480 --> 17:51.480
like a numeric range, say all of x1 to x10.

17:51.480 --> 17:57.480
So this is my attempt to come up with every way that you might reasonably want to select a variable.

17:57.480 --> 18:02.480
So a couple of ways you can select these two.

18:02.480 --> 18:06.480
You can select them just as individual variables.

18:06.480 --> 18:12.480
You could say pick all of the variables between from a rival delay to departure delay.

18:12.480 --> 18:18.480
You could find all of the variables that end with delay or all of the variables that contain delay.

18:18.480 --> 18:20.480
There's lots of other ways too.

18:20.480 --> 18:22.480
You could also write this.

18:22.480 --> 18:24.480
You could say make a vector of columns.

18:24.480 --> 18:26.480
We're using C.

18:26.480 --> 18:34.480
Basically, inside select variable names, you can treat them like the numeric positions.

18:34.480 --> 18:40.480
So anything you can do to a numeric position, you can do with a variable name.

18:40.480 --> 18:45.480
So the goal of select is to make it easy to refer to your variables by name.

18:45.480 --> 18:49.480
It's always a better idea to refer to your variables by name than by position,

18:49.480 --> 18:56.480
because you don't want your data input format changes and you're referring to variables by position.

18:56.480 --> 19:05.480
It's very easy to have code that works but gives you meaningless results because it's using the wrong variables.

19:05.480 --> 19:11.480
The next verb is a range which just changes the order of the rows.

19:11.480 --> 19:15.480
So if you just use a variable and orders it by that,

19:15.480 --> 19:20.480
you can order in descending order by using the desk wrapper.

19:20.480 --> 19:28.480
And I don't show you here but you can add additional variables to break ties if there are ties in this first variable.

19:28.480 --> 19:33.480
So again, order the flights by departure date and time.

19:33.480 --> 19:37.480
Figure out using a range which flights were most delayed

19:37.480 --> 19:42.480
and then which flights caught up the most time during the flight.

19:43.480 --> 19:46.480
So again, a few minutes to work on this and I'll show you the answers.

19:46.480 --> 19:51.480
Okay, if we want to order the flights by their departure date,

19:51.480 --> 19:55.480
we could say order it by date and then hour and then minute.

19:55.480 --> 19:59.480
Just want to see multiple, ordering by multiple variables.

19:59.480 --> 20:06.480
So you can see the first flight left on January 1st, one minute after midnight.

20:06.480 --> 20:15.480
So I should mention this depth variable is the departure time as like a 24-hour time

20:15.480 --> 20:18.480
but all the zeros got dropped off.

20:18.480 --> 20:23.480
And then the hour and minute are just that, this time split up into those pieces.

20:23.480 --> 20:30.480
So for example in this column, there's not going to be a 661,

20:30.480 --> 20:35.480
no flights left, it's 61 minutes past 6am.

20:35.480 --> 20:39.480
This is just a weird decimal time.

20:39.480 --> 20:43.480
We want to sort, find the most delayed, that's just a matter of sorting

20:43.480 --> 20:46.480
so that our delays are descending.

20:46.480 --> 20:51.480
We can see the most delayed flight was 981 minutes.

20:51.480 --> 20:56.480
So an impressive 16-hour delay.

20:56.480 --> 20:59.480
Now normally flights aren't delayed that long,

20:59.480 --> 21:02.480
not because flights aren't delayed that long

21:02.480 --> 21:10.480
but generally airlines cancel the flights to make their departure delay statistics look better.

21:10.480 --> 21:13.480
So similarly we could do the same thing for arrival delay,

21:13.480 --> 21:21.480
which is going to give us a pretty similar message.

21:21.480 --> 21:28.480
And the other thing I wanted to show here is that you can arrange on kind of compound expressions.

21:28.480 --> 21:33.480
I wanted to find the planes in a mode made up the most time

21:33.480 --> 21:37.480
and there's the biggest difference between the departure and arrival delay.

21:37.480 --> 21:43.480
So there's a flight, so for example this flight left one minute early

21:43.480 --> 21:47.480
and it arrived an hour and 10 minutes early.

21:47.480 --> 21:51.480
So you can arrange on compound expressions

21:51.480 --> 21:55.480
although generally it's going to be easier to add that as a new variable

21:55.480 --> 21:58.480
depending on what's going on and then arranged by that.

21:58.480 --> 22:00.480
Why are you reporting this descending?

22:00.480 --> 22:06.480
Because I wanted to find the one, I wanted to find the biggest difference.

22:06.480 --> 22:09.480
I may have...

22:09.480 --> 22:12.480
Actually I got the same result.

22:12.480 --> 22:16.480
I may have hit this round the wrong way.

22:16.480 --> 22:19.480
Oh yeah.

22:19.480 --> 22:23.480
So depending on which way round we need to track the arrival from departure

22:23.480 --> 22:26.480
to ascending or ascending.

22:26.480 --> 22:29.480
Any other questions about arrange?

22:29.480 --> 22:33.480
I had a problem with the NAs, the first time I did something

22:33.480 --> 22:35.480
I got all the NAs on top.

22:35.480 --> 22:39.480
I did it in a different way than you did once I...

22:39.480 --> 22:44.480
So NAs should always sort to the end and if they don't that's a bug.

22:44.480 --> 22:49.480
They do but what if I want the smallest without the NAs?

22:49.480 --> 22:53.480
So you have to use felt as a removal of the NAs currently.

22:53.480 --> 22:58.480
Is there an opposite of descending?

22:58.480 --> 23:01.480
Yes, just don't do descending.

23:01.480 --> 23:05.480
I think also the way that...

23:05.480 --> 23:11.480
I believe that if you do descending or descending that is ascending.

23:11.480 --> 23:15.480
It's the one still.

23:15.480 --> 23:19.480
If you really want an ascending function you can just do that.

23:25.480 --> 23:29.480
Okay, the starting to get more complicated.

23:29.480 --> 23:33.480
The next verb is mutate which allows you to add new variables

23:33.480 --> 23:36.480
that are functions of existing variables.

23:36.480 --> 23:39.480
So here we're adding a new variable called double

23:39.480 --> 23:42.480
which is two times our existing value variable.

23:42.480 --> 23:45.480
So again in all of the dplyr functions

23:45.480 --> 23:50.480
you never need to explicitly refer to the data frame that you're working with.

23:50.480 --> 23:52.480
That's always implicit.

23:52.480 --> 23:55.480
It's going to look for this value inside the data frame

23:55.480 --> 23:59.480
rather than in your global environment.

23:59.480 --> 24:04.480
Mutate is very similar to transform and base r if you've used that.

24:04.480 --> 24:08.480
One big difference with mutate is you can do multiple...

24:08.480 --> 24:13.480
In additional mutations or additional transformations

24:13.480 --> 24:16.480
you can refer to variables that you just created

24:16.480 --> 24:20.480
which you cannot do in transform and is a little bit annoying.

24:20.480 --> 24:24.480
So here we first double value and then we make a new column called quadruple

24:24.480 --> 24:28.480
which is just two times double a variable we just created.

24:28.480 --> 24:32.480
How does it compare to within?

24:32.480 --> 24:37.480
Basically I think within is a hideous monstrosity that no one should ever use.

24:37.480 --> 24:41.480
And if you want to know more I can tell you.

24:41.480 --> 24:45.480
Okay so your turn to create some variables.

24:45.480 --> 24:49.480
See if you can figure out the speed and miles per hour

24:49.480 --> 24:52.480
which flight flew the fastest.

24:52.480 --> 24:56.480
See if you can create a new variable that shows how much time was made up

24:56.480 --> 24:58.480
during the course of the flight or lost.

24:58.480 --> 25:02.480
And then how did I compute the hour and minute variables

25:02.480 --> 25:06.480
from that departure variable?

25:06.480 --> 25:10.480
Okay so if I wanted to compute the speed

25:10.480 --> 25:14.480
that is just the distance divided by the time divided by 60

25:14.480 --> 25:17.480
because time is in minutes.

25:17.480 --> 25:21.480
So if we print that out

25:21.480 --> 25:25.480
you know unless you make your screen really wide you can't see everything.

25:25.480 --> 25:29.480
So one thing you can do is use the view function

25:29.480 --> 25:33.480
which works in RStudio and other R ideas

25:33.480 --> 25:37.480
which will just show all of your variables

25:37.480 --> 25:40.480
on a nice kind of scrollable table

25:40.480 --> 25:45.480
or you can always just select the variables you want to see

25:45.480 --> 25:49.480
so from like departure to speed.

25:49.480 --> 25:54.480
So if you use a very handy way

25:54.480 --> 25:58.480
of just viewing a data frame in a nice table.

25:58.480 --> 26:02.480
Did you change flights?

26:02.480 --> 26:06.480
Yes so in this case I modified flights because I wanted to create

26:06.480 --> 26:10.480
a new variable and modify that original data set to add that new variable

26:10.480 --> 26:14.480
and then I can sort it to find the fastest ones

26:14.480 --> 26:19.480
and see 760 miles an hour.

26:19.480 --> 26:23.480
When you mutate does there

26:23.480 --> 26:27.480
an easy way to specify a position?

26:27.480 --> 26:31.480
No so when you add new variables they always go on to the end of the data frame.

26:31.480 --> 26:35.480
If you wanted to reposition them there's currently no particularly easy

26:35.480 --> 26:39.480
way to do that. You could create a big select statement but it's

26:39.480 --> 26:43.480
kind of a pain.

26:43.480 --> 26:47.480
We could create this delta variable

26:47.480 --> 26:51.480
which is just the difference between the departure and arrival delay.

26:51.480 --> 26:55.480
If you didn't care about the direction

26:55.480 --> 26:59.480
you could do whatever you want in this

26:59.480 --> 27:03.480
whatever R expression you want.

27:03.480 --> 27:07.480
The last thing I wanted to mention

27:07.480 --> 27:11.480
is just a useful trick. If I have this departure

27:11.480 --> 27:15.480
we have the first two digits of the hour and the second two digits of the minute

27:15.480 --> 27:19.480
you can use the integer division operator

27:19.480 --> 27:23.480
and the modular operator to extract those pieces out.

27:23.480 --> 27:27.480
This is just a useful little trick if you want to pull out certain digits

27:27.480 --> 27:31.480
from a long number.

27:31.480 --> 27:35.480
Any other questions about

27:35.480 --> 27:39.480
mutate?

27:39.480 --> 27:43.480
Okay next I want to talk about a new function group phi

27:43.480 --> 27:47.480
which is summarized together. You can use summarized and regular data frames

27:47.480 --> 27:51.480
but you always get a data frame that is only one row

27:51.480 --> 27:55.480
which is typically not very useful.

27:55.480 --> 27:59.480
That's exactly what I said. So summarized is going to give you a one row

27:59.480 --> 28:03.480
data frame. What you're going to want to do is actually group your data first

28:03.480 --> 28:07.480
and then summarized will operate by group.

28:07.480 --> 28:11.480
Here we're saying create a new data frame

28:11.480 --> 28:15.480
and use this old data frame grouped by color

28:15.480 --> 28:19.480
and then we're going to summarize this and for each group

28:19.480 --> 28:23.480
compute the total by summing up the value of your vehicle.

28:23.480 --> 28:27.480
So I'm going to create four useful ways

28:27.480 --> 28:31.480
of grouping the flights data. We might want to group it by date

28:31.480 --> 28:35.480
we might want to group it by hour, we might want to group it by plane

28:35.480 --> 28:39.480
or we might want to group it by destination.

28:39.480 --> 28:43.480
Just to bear in mind when you do create all these groupings

28:43.480 --> 28:47.480
dplyr is sort of smart enough that doesn't create a complete copy

28:47.480 --> 28:51.480
of your data every single time. It works the same way as the rest

28:51.480 --> 28:55.480
of R, it doesn't sort of a lazy way. If you modify one of these data sets

28:55.480 --> 28:59.480
you'll have to create a copy but until you do so they all point to the same

28:59.480 --> 29:03.480
place. So grouping data doesn't

29:03.480 --> 29:07.480
use up, it doesn't create a copy of the data, it does use up a little bit more

29:07.480 --> 29:11.480
memory because grouping builds up an index so

29:11.480 --> 29:15.480
you know what observations are in each group.

29:15.480 --> 29:19.480
Now there are lots of summary functions you can use, most of these

29:19.480 --> 29:23.480
are pretty standard, minimum, medium, maximum

29:23.480 --> 29:27.480
you can extract contiles, there are two functions

29:27.480 --> 29:31.480
that are special in dplyr in which just

29:31.480 --> 29:35.480
tells you how many observations are in a group, indistinct

29:35.480 --> 29:39.480
and I should have a

29:39.480 --> 29:43.480
x there, tells you how many different observations

29:43.480 --> 29:47.480
are in a variable, that's the same as doing

29:47.480 --> 29:51.480
length unique x but it's a little bit more efficient.

29:51.480 --> 29:55.480
You can sum, you can compute means. It's also often

29:55.480 --> 29:59.480
useful to do summaries of logical vectors because

29:59.480 --> 30:03.480
when you take a logical vector and treat it like it's a numeric

30:03.480 --> 30:07.480
all the falses turn into zeros and the trues turn into ones

30:07.480 --> 30:11.480
so what that means is when you sum a logical vector it tells

30:11.480 --> 30:15.480
you how many trues there were so this would tell you how many values

30:15.480 --> 30:19.480
of x are greater than 10. The mean is just the sum

30:19.480 --> 30:23.480
divided by the length so the mean of a logical vector is the

30:23.480 --> 30:27.480
proportion of values of the true. There's a really useful little

30:27.480 --> 30:31.480
trick. And then lots of other ways of measuring

30:31.480 --> 30:35.480
the variation, standard deviation, variance, interquadal range,

30:35.480 --> 30:39.480
median absolute deviation. So these are all just standard

30:39.480 --> 30:43.480
functions.

30:49.480 --> 30:53.480
Okay what I want you guys, what I've shown here is the distribution

30:53.480 --> 30:57.480
of departure delays. So I've got two views of this

30:57.480 --> 31:01.480
one which shows all of the delays and one which just shows the delays less than

31:01.480 --> 31:05.480
like two hours. So what I want you to do with your neighbor for two minutes

31:05.480 --> 31:09.480
is just quickly brainstorm given this distribution

31:09.480 --> 31:13.480
given what you know about flight delays

31:13.480 --> 31:17.480
how might you want to summarize this distribution. What function might

31:17.480 --> 31:21.480
what you want to use or do you want to use a mean or a median or something else.

31:21.480 --> 31:25.480
So take two minutes starting now, talk it over with your neighbor.

31:25.480 --> 31:29.480
So we're going to summarize by date

31:29.480 --> 31:33.480
what's one way we could use to summarize the distribution of

31:33.480 --> 31:37.480
delays. The median? We could use the median

31:37.480 --> 31:41.480
I mean probably want to use the departure delay

31:41.480 --> 31:45.480
so if we just run that

31:49.480 --> 31:53.480
we are going to get a new data frame and it is

31:53.480 --> 31:57.480
265 rows which you should have anticipated. You know how many

31:57.480 --> 32:01.480
days there are in a year. I've got one little problem here

32:01.480 --> 32:05.480
probably want to use Na.Ramq was true

32:05.480 --> 32:09.480
let's do that.

32:09.480 --> 32:13.480
How else could we summarize it?

32:13.480 --> 32:17.480
The mean is another obvious one

32:17.480 --> 32:21.480
let's just assume we've got that

32:21.480 --> 32:25.480
What else might you want to see? 90% quanta.

32:25.480 --> 32:29.480
Okay we've got max

32:29.480 --> 32:33.480
and actually typing all of this Na.Ramq.true

32:33.480 --> 32:37.480
is going to get tedious real fast so I'm just going to filter

32:37.480 --> 32:41.480
it and I say I want all of the ones that are not missing

32:41.480 --> 32:45.480
okay so that way I can just drop this off

32:45.480 --> 32:49.480
and I'll bother typing it

32:49.480 --> 32:53.480
so that's the median, the mean, the maximum and then something

32:53.480 --> 32:57.480
in between we could get the 90th

32:57.480 --> 33:01.480
quanta. Remember how to use that function

33:05.480 --> 33:09.480
Any other ideas?

33:09.480 --> 33:13.480
Is there a way to for example

33:13.480 --> 33:17.480
compute more than just the 90% quanta?

33:17.480 --> 33:21.480
Currently you have to type them in like this

33:21.480 --> 33:25.480
but there will be some way in the future that you do that

33:29.480 --> 33:33.480
Yeah we could also do some thresholds

33:33.480 --> 33:37.480
well first of all we could say

33:37.480 --> 33:41.480
what's the proportion that is delayed

33:41.480 --> 33:45.480
so that is the average of all of the ones where the delay

33:45.480 --> 33:49.480
is greater than zero

33:49.480 --> 33:51.480
so that is the

33:51.480 --> 33:53.480
presently high

33:53.480 --> 33:57.480
but you might say well who really cares if it's only

33:57.480 --> 34:01.480
a 5 minute delay or a 10 minute delay

34:01.480 --> 34:05.480
I might just say arbitrarily like a 15 minute delay that's not bad

34:05.480 --> 34:09.480
Why are we looking at departure not arrival?

34:09.480 --> 34:13.480
Yeah so equally you might say well it's

34:13.480 --> 34:17.480
the impact on our arrival that's what really matters because that's

34:17.480 --> 34:21.480
someone picking us up at the airport and our flight

34:21.480 --> 34:25.480
is now delayed by an hour and they're getting angry so we could switch all this to arrival

34:25.480 --> 34:29.480
delay too and the results are pretty similar

34:29.480 --> 34:33.480
So 15 minutes is kind of arbitrary you know you could look at a few

34:33.480 --> 34:37.480
other ones if you wanted to do that

34:37.480 --> 34:41.480
Yes? Is there a way to use this summer function?

34:41.480 --> 34:45.480
You could but I'm not sure

34:45.480 --> 34:49.480
that you would want to

34:53.480 --> 34:57.480
So current well so there's two problems

34:57.480 --> 35:01.480
so first of all I mean this is a reasonable thing to do

35:01.480 --> 35:05.480
currently though summarise

35:05.480 --> 35:09.480
when you summarise you have to reduce to a single number not multiple numbers

35:09.480 --> 35:13.480
because again a future version of dplyr will let you summarise multiple numbers

35:13.480 --> 35:17.480
at some point in the future

35:17.480 --> 35:21.480
What did I do? So this is what I did

35:21.480 --> 35:25.480
and you have to have urm everywhere

35:25.480 --> 35:29.480
or you can filter out all of the flights

35:29.480 --> 35:33.480
that are not missing

35:33.480 --> 35:37.480
but don't have a missing departure

35:37.480 --> 35:41.480
So this kind of

35:41.480 --> 35:45.480
brings me to my next point at any like in any real data manipulation

35:45.480 --> 35:49.480
task you're probably not just going to use one verb

35:49.480 --> 35:53.480
but you're going to string multiple verbs together first of all we group it

35:53.480 --> 35:57.480
then we filter it then we summarise it and we want some way

35:57.480 --> 36:01.480
to kind of express that more naturally or more simply which is that

36:01.480 --> 36:05.480
the idea of having a data pipeline

36:05.480 --> 36:09.480
you need to do quickly just take a minute

36:09.480 --> 36:13.480
talk this over with your neighbour what does this snippet of code do

36:13.480 --> 36:17.480
so you've got one minute starting now

36:17.480 --> 36:21.480
okay so this looks pretty complicated

36:21.480 --> 36:25.480
but if you kind of really carefully pass it you have to start from the innermost thing

36:25.480 --> 36:29.480
we're going to start with the flights data then we're going to filter it

36:29.480 --> 36:33.480
to remove any missing delays then we're going to group it by date

36:33.480 --> 36:37.480
in an hour then we're going to summarise it to compute the average delay

36:37.480 --> 36:41.480
and the number of observations in that hour

36:41.480 --> 36:45.480
then we're going to filter it to only look at the hours that have more than 10 flights

36:45.480 --> 36:49.480
so it's not too complicated

36:49.480 --> 36:53.480
but we have to read it in quite an unnatural way to read insight out

36:53.480 --> 36:57.480
and then also like the arguments to filter are quite far away

36:57.480 --> 37:01.480
so instead

37:01.480 --> 37:05.480
what we're going to talk about after the coffee break is this pipe operator

37:05.480 --> 37:09.480
and you'll see that that makes the code quite

37:09.480 --> 37:13.480
a lot easier to read so the coffee is outside now

37:13.480 --> 37:17.480
so let's have a coffee break and come back at

37:17.480 --> 37:21.480
3.40

37:21.480 --> 37:25.480
music

37:25.480 --> 37:29.480
music

37:29.480 --> 37:33.480
about this operator

37:33.480 --> 37:37.480
called the pipe operator so what this basically does

37:37.480 --> 37:41.480
is take the thing on the left hand side of the pipe

37:41.480 --> 37:45.480
and put it as the first argument as a thing on the right hand side

37:45.480 --> 37:49.480
and the advantage of this is it allows us to take something like this

37:49.480 --> 37:53.480
which is pretty hard to read and transform it into something like this

37:53.480 --> 37:57.480
and this is pretty easy to read particularly if you pronounce this operator as then

37:57.480 --> 38:01.480
so we can read this take flights then filter it

38:01.480 --> 38:05.480
to remove any values with a missing value for depth delay

38:05.480 --> 38:09.480
then group it by date and hour then summarise it

38:09.480 --> 38:13.480
computing the average delay and the number of observations in the group

38:13.480 --> 38:17.480
then filter it to look at all of the

38:17.480 --> 38:21.480
observations we're going to have in 10 so

38:21.480 --> 38:25.480
this pipe operator allows us to

38:25.480 --> 38:29.480
form chains of complicated

38:29.480 --> 38:33.480
data transformation operations that are made up of very simple pieces so the goal

38:33.480 --> 38:37.480
is you make something complex by joining together many simple

38:37.480 --> 38:41.480
things that are easy to understand in isolation

38:41.480 --> 38:45.480
so I want to give you some practice using that with

38:45.480 --> 38:49.480
three challenges so which destinations have the highest

38:49.480 --> 38:53.480
average delays which flights

38:53.480 --> 38:57.480
happen every day and where do they fly to and then on average

38:57.480 --> 39:01.480
how do delays vary over the course of a day

39:01.480 --> 39:05.480
and if you're going to do that probably look at the non cancelled flights

39:05.480 --> 39:09.480
so those three challenges are relatively

39:09.480 --> 39:13.480
simple but you're going to need to string together multiple of these verbs

39:13.480 --> 39:17.480
you've seen before you might have to use a range and group by and summarise

39:17.480 --> 39:21.480
and filter in some order so have a go at joining those together

39:21.480 --> 39:25.480
and again if you get stuck I'll come around and help you out or better

39:25.480 --> 39:29.480
ask your neighbour

39:29.480 --> 39:33.480
do well we start with the flights

39:33.480 --> 39:37.480
what are we going to do to that filter to remove anase yep we can

39:37.480 --> 39:41.480
remove the anase let's do rival delays

39:41.480 --> 39:45.480
what next

39:45.480 --> 39:49.480
group by so group by is kind of a fundamentally

39:49.480 --> 39:53.480
like statistical operator you're saying what is the unit of interest in this analysis

39:53.480 --> 39:57.480
and in this case it's the destination of the flight

39:57.480 --> 40:01.480
then for each destination what we want to do is summarise it

40:01.480 --> 40:05.480
I'm just going to say let's use the mean delay

40:05.480 --> 40:09.480
the other thing I think you always want to do whenever you do

40:09.480 --> 40:13.480
a group by summary is you always want to recall the number of observations

40:13.480 --> 40:17.480
in each group because when you start looking at these averages

40:17.480 --> 40:21.480
you know if there's a destination that has the highest

40:21.480 --> 40:25.480
average delay but only one flight flew there

40:25.480 --> 40:29.480
and that's probably not as interesting and then if we want to focus on the most

40:29.480 --> 40:33.480
delayed flights we're going to arrange it in

40:33.480 --> 40:37.480
descending mean so let's run this

40:37.480 --> 40:41.480
they've worked so you can see this is a good example

40:41.480 --> 40:45.480
so there's this airport BBT which

40:45.480 --> 40:49.480
see

40:49.480 --> 40:53.480
I think I've already looked at this before so that is Jack Brooks

40:53.480 --> 40:57.480
Brooks regional airport on the airport

40:57.480 --> 41:01.480
of Texas so there are only three flights flew there the entire

41:01.480 --> 41:05.480
year you're not going to trust this average that much so

41:05.480 --> 41:09.480
what we might want to do is filter out all of the

41:09.480 --> 41:13.480
flights where there's less than 10 observations

41:13.480 --> 41:17.480
we'll run that pipeline again

41:17.480 --> 41:21.480
now again I've constructed this pipeline

41:21.480 --> 41:25.480
just by typing every step and it worked

41:25.480 --> 41:29.480
which I have to say I'm slightly amazed at but generally when you're creating pipelines

41:29.480 --> 41:33.480
you want to do it a step at a time and this is one reason

41:33.480 --> 41:37.480
that I think the default printing is really important

41:37.480 --> 41:41.480
because you can just print out the result at every stage and you can see does that look right or not

41:41.480 --> 41:45.480
if you have a normal data frame it will print all of it right

41:45.480 --> 41:49.480
yes so if you have a normal data frame

41:49.480 --> 41:53.480
it will print the whole thing and if you want to turn

41:53.480 --> 41:57.480
you can always take a normal data frame

41:57.480 --> 42:01.480
and the first thing you can do is pipe it into tables here and turn it

42:01.480 --> 42:05.480
into a data frame the other thing

42:05.480 --> 42:09.480
the other thing that's useful is you might often pipe

42:09.480 --> 42:13.480
this into something rather than just printing it you could pipe it into view

42:13.480 --> 42:17.480
if you wanted to see more of the data

42:17.480 --> 42:21.480
that's kind of interesting

42:21.480 --> 42:25.480
if you wanted to just

42:25.480 --> 42:29.480
kind of step through it

42:29.480 --> 42:33.480
you could do

42:33.480 --> 42:37.480
talk about you could do something like

42:37.480 --> 42:41.480
this

42:41.480 --> 42:45.480
maybe

42:45.480 --> 42:49.480
so we're just taking the row number and taking a modulo 5

42:49.480 --> 42:53.480
equals zero so that's going to give us every fifth that would be one way to do it

42:53.480 --> 42:57.480
so if you

42:57.480 --> 43:01.480
shows you everything well it shows you the first so many rows

43:01.480 --> 43:05.480
in the future I think we'll make it so it shows you every row

43:05.480 --> 43:09.480
in a way that's reasonably efficient

43:09.480 --> 43:13.480
the other thing that's useful is to pipe it to str so you can see exactly what variables

43:13.480 --> 43:17.480
you've created and if they're the right type and so on

43:17.480 --> 43:21.480
or if you're so inclined

43:21.480 --> 43:25.480
could you put in two functions like head and tail after each other

43:25.480 --> 43:29.480
you can't basically so you want a pipeline that has a split

43:29.480 --> 43:33.480
in it right you want to have a pipeline that one pipe goes to head

43:33.480 --> 43:37.480
and the other pipe goes to tail

43:37.480 --> 43:41.480
at the same time yeah I don't

43:41.480 --> 43:45.480
like a data table does that by default on that I think that's a nice idea

43:45.480 --> 43:49.480
the reason dply doesn't do it is because you can do that for data

43:49.480 --> 43:53.480
frames but you can't in general do that efficiently for database queries

43:53.480 --> 43:57.480
you can always use tail off

43:57.480 --> 44:01.480
so there's another handy keyboard

44:01.480 --> 44:05.480
shortcut in our studio which I

44:05.480 --> 44:09.480
suspect no one knows about because the only reason I know about it is the

44:09.480 --> 44:13.480
Joe who added it told me about it there's this command called rerun

44:13.480 --> 44:17.480
previous has anyone used rerun previous before

44:17.480 --> 44:21.480
so what that does is if you have selected a

44:21.480 --> 44:25.480
block of code and press command enter

44:25.480 --> 44:29.480
now if I modify it it's kind of annoying I have to select that

44:29.480 --> 44:33.480
block of code again or you can press command shift P

44:33.480 --> 44:37.480
and it just sends those same lines of code into the R console

44:37.480 --> 44:41.480
so this is really useful if you want to iterate rapidly on your pipeline

44:41.480 --> 44:45.480
you can easily change things and maybe I wanted an ascending order

44:45.480 --> 44:49.480
and just command shift P and rerun the whole pipeline

44:51.480 --> 44:55.480
okay

44:55.480 --> 44:59.480
okay so

44:59.480 --> 45:03.480
any questions about that pipeline that we created to solve that problem

45:03.480 --> 45:07.480
so the next one is which flights

45:07.480 --> 45:11.480
happen every day and where do they fly to

45:11.480 --> 45:15.480
no

45:15.480 --> 45:19.480
so what are we going to start with that

45:19.480 --> 45:23.480
and which flights fly every day of the year what's probably the first

45:23.480 --> 45:27.480
thing we want to do we want to group by

45:27.480 --> 45:31.480
and we want to do that by carrier and the flight number

45:31.480 --> 45:35.480
now we want to find all

45:35.480 --> 45:39.480
flights that flew every day of the year

45:39.480 --> 45:43.480
any ideas so we're going to summarize what might we summarize

45:43.480 --> 45:47.480
we might use the dates

45:47.480 --> 45:51.480
what how well we're going to use the date how what are we going to do with that

45:51.480 --> 45:55.480
oh so we could do we could do count

45:55.480 --> 45:59.480
flights

45:59.480 --> 46:03.480
we could do count and then we could filter by

46:03.480 --> 46:07.480
let's give us a name

46:07.480 --> 46:11.480
365

46:11.480 --> 46:15.480
I forgot to put two equals

46:15.480 --> 46:19.480
now the problem with this is that it's possible

46:19.480 --> 46:23.480
this flight flew

46:23.480 --> 46:27.480
twice on one day and didn't fly it all on another day

46:27.480 --> 46:31.480
I feel like that's yeah so actually

46:31.480 --> 46:35.480
this is my solution too but now I think a better way would be to say

46:35.480 --> 46:39.480
count the number of distinct

46:39.480 --> 46:43.480
dates so if there's 365 distinct

46:43.480 --> 46:47.480
dates then we know it's flown every day

46:47.480 --> 46:51.480
I think this would give us a slightly different answer

46:51.480 --> 46:55.480
well in this case it gives us the same answer because there aren't flights that fly

46:55.480 --> 46:59.480
every day and then

46:59.480 --> 47:03.480
fly twice on one day but not on another

47:03.480 --> 47:07.480
now what if we wanted to add see what destinations these flights flew to

47:07.480 --> 47:11.480
any thoughts on that

47:11.480 --> 47:15.480
we could just add to the group by

47:15.480 --> 47:19.480
there are other ways we could do this which we'll see later but in this case it's easy enough

47:19.480 --> 47:23.480
to just add that into the group by

47:23.480 --> 47:27.480
and see Honolulu and a lot of flights to New York

47:27.480 --> 47:31.480
and Chicago and Seattle and Miami I think

47:35.480 --> 47:39.480
the last one on average

47:39.480 --> 47:43.480
the non cancelled flights vary over the course of the day

47:43.480 --> 47:47.480
so again so first of all we always want to say

47:47.480 --> 47:51.480
they're not cancelled which I think

47:51.480 --> 47:55.480
because cancelled equals zero cancelled is

47:55.480 --> 47:59.480
a reason code associated with it and then normally

47:59.480 --> 48:03.480
once you've kind of filtered out clearly wrong things the first step is going to be

48:03.480 --> 48:07.480
grouping it here we want to group by hour say

48:07.480 --> 48:11.480
or maybe hour and minute

48:11.480 --> 48:15.480
and then summarize again we want to

48:15.480 --> 48:19.480
count how many observations on each group so we can disregard the delayed flights

48:19.480 --> 48:23.480
and we could do the mean

48:27.480 --> 48:31.480
departure delay

48:31.480 --> 48:35.480
and

48:35.480 --> 48:39.480
summarize not summary

48:39.480 --> 48:43.480
so now when you get to this point

48:43.480 --> 48:47.480
it starts to get easier to see

48:47.480 --> 48:51.480
what's going on with the visualizations so

48:51.480 --> 48:55.480
this is basically that pipeline I just showed you

48:55.480 --> 48:59.480
I think I've done a slightly differently I created a new variable called time

48:59.480 --> 49:03.480
which is just hour plus minute divided by 60 that gives me like a floating point

49:03.480 --> 49:07.480
number that smoothly varies over the course of the day

49:07.480 --> 49:11.480
group it, summarize it and then I'm going to do a little ggplot to

49:15.480 --> 49:19.480
plot it

49:19.480 --> 49:23.480
so you can see very early in the day

49:23.480 --> 49:27.480
we have this kind of scattered cloud of some plots that are very

49:27.480 --> 49:31.480
delayed what might these be

49:31.480 --> 49:35.480
the ones from the end of the previous night

49:35.480 --> 49:39.480
the ones from the end of the previous night and why are the averages so high

49:39.480 --> 49:43.480
so variable

49:43.480 --> 49:47.480
these are the ones that have hardly any data

49:47.480 --> 49:51.480
there are hardly any flights leave after midnight so these averages

49:51.480 --> 49:55.480
are kind of suspicious we're not really seeing much of a pattern we're just seeing

49:55.480 --> 49:59.480
individual flights that were delayed a really long time from the previous day

49:59.480 --> 50:03.480
so we might want to, so one we could show then the visualization

50:03.480 --> 50:07.480
is to make the points proportional to the

50:07.480 --> 50:11.480
number of observations or we could filter it and add some other stuff

50:11.480 --> 50:15.480
there's no schedule flights

50:15.480 --> 50:19.480
exactly there's no schedule flights yeah

50:19.480 --> 50:23.480
so there's some kind of interesting pattern going on here

50:23.480 --> 50:27.480
I don't really understand if it's possible it's an artifact

50:27.480 --> 50:31.480
but it looks like it added these white lines on every hour

50:31.480 --> 50:35.480
but it looks like there's some kind of pattern where they start off

50:35.480 --> 50:39.480
delays kind of accumulate over the course of the day

50:39.480 --> 50:43.480
but there's also some weird pattern within the hour where they accumulate and then they drop

50:43.480 --> 50:47.480
back a little which I don't know what's going on

50:47.480 --> 50:51.480
but certainly the suggestion is if you want to leave on time fly early in the day

50:51.480 --> 50:55.480
or late in the hour

50:55.480 --> 50:59.480
or late in the hour

51:03.480 --> 51:07.480
any questions about those pipelines in general or how you can combine

51:07.480 --> 51:11.480
these pieces with a pipe operator?

51:11.480 --> 51:15.480
range is generally what the advantage is to chaining versus having

51:15.480 --> 51:19.480
a ton of parentheses inside

51:19.480 --> 51:23.480
but the sole example is that it makes it easier for you to read and understand what's going on

51:23.480 --> 51:27.480
does any advantage just having it line by line

51:27.480 --> 51:31.480
no basically no

51:31.480 --> 51:35.480
save a little bit of memory but it's not

51:35.480 --> 51:39.480
yep

51:39.480 --> 51:43.480
so yeah in all the versions of D player used

51:43.480 --> 51:47.480
percent dot percent

51:47.480 --> 51:51.480
now I prefer percent greater than percent for two reasons

51:51.480 --> 51:55.480
first of all it's easy to type because you can hold your finger on the shift button the whole time

51:55.480 --> 51:59.480
and secondly I think it's not a

51:59.480 --> 52:03.480
symmetric operation so having an asymmetric operator

52:03.480 --> 52:07.480
helps you understand what's going on, the data is flowing from left to right

52:07.480 --> 52:11.480
any other questions?

52:11.480 --> 52:15.480
is there a particular preferred order?

52:15.480 --> 52:19.480
no obviously the less

52:19.480 --> 52:23.480
data you have to work with the faster things are going to be so that generally suggests you should

52:23.480 --> 52:27.480
filter early on and you know so

52:27.480 --> 52:31.480
if you use a database, a database looks at the sequence of all the

52:31.480 --> 52:35.480
operations and says oh you did this filter at the end but it would actually

52:35.480 --> 52:39.480
be way more efficient to do that at the beginning, D player doesn't do anything like that

52:39.480 --> 52:43.480
D player executes it exactly as you give it so if you

52:43.480 --> 52:47.480
can think of a faster way to order the operations it might be worthwhile

52:47.480 --> 52:51.480
to do so generally and I'm not really going to talk about

52:51.480 --> 52:55.480
performance today but generally if you've got million like

52:55.480 --> 52:59.480
less than 10 million observations you won't even have to worry

52:59.480 --> 53:03.480
about the performance it's going to be a few seconds and it's not

53:03.480 --> 53:07.480
like it's a waste of time worrying about it because it's not going to take you that long

53:07.480 --> 53:11.480
ok, the next thing I'm going to talk about

53:11.480 --> 53:15.480
is a great thing

53:15.480 --> 53:19.480
music

53:19.480 --> 53:23.480
music

53:23.480 --> 53:25.480
music

