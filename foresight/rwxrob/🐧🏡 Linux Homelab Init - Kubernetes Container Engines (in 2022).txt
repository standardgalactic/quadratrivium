All right, we're gonna jump right into this.
So this is the results of the best amount
of research that I can do.
And now when I do research R being the,
you know, the research being the principal meaning of R,
don't put Rust out there again.
Why are you bullying up Rust right now?
We're not talking about Rust.
No, we're not.
So anyway, so by the way,
my feelings on Rust have totally changed
within the last few days
because of the whole other thing
that we've been talking about.
So please don't, we're not gonna talk about that right now.
I did talk about Rust a little bit when we,
yesterday because we were talking about Firecracker
and how I think Rust could be kind of the golden child
of the micro virtualization emerging,
but that is unrelated to what we're gonna talk about
right now.
So I'm gonna try to look away from the chat,
but let's try at least for the next 45 minutes
or 90 minutes to stay focused on one topic
so that we can have the speed of this YouTube video
be kind of to the point.
What is going on with containers
and specifically Kubernetes containers in 2022?
And we're about to start KubeCon on Monday.
So there's gonna be a lot of other new content
and announcements, I'm sure.
But this, what you're seeing on the screen right now,
and by the way, if you wanna join me,
let me post a link one more time.
If you wanna join me and move your pointer around
and participate, you can, you're welcome to do that.
This is a read-only Excalidraw thing.
I'm a huge fan of Excalidraw.
Full disclosure, I was gifted a membership to Excalidraw
from one of our community members
and I never look back, it's so amazing.
I've also been posting the PNGs of this to the Discord.
So if you wanna see those, you can go there.
But you should be able to use the URL anytime to come to it.
Now, this, hey, look at all these fun little arrows.
So what I'm gonna start by saying is a disclaimer.
I consider myself, I mean, I've had people call me advanced
in Kubernetes compared to the average Kubernetes admin
and I don't feel that way myself.
I don't know if it's imposter syndrome or what,
I feel like, yes, I've installed several clusters
and I administer one, but I still feel like
I'm always learning something
that I should have already known.
And this video right here is about that.
It's about one of the things that I,
every time I think I understand it,
I like get introduced to a new piece of the puzzle
and I'm like, oh, oh, so that's how that works.
And short of looking at the source code,
which I am very prepared to do,
this is the best that I could come up with.
So I say that as a disclaimer
because there might be some glaring mistakes in here.
And if they are in your YouTube, just let me have it.
Write it in the comments, say you got this wrong
and you got this wrong and you got this wrong
and I'll go in and I'll fix it
so we can have our hopefully corrective version of this.
And the reason I put that out there
is because in order for me to even come to this knowledge
that I am showing you right now,
I had to sift through a ton of bad information
on the internet, bad information, misuse terminology,
confusion, renaming, reorganization of projects
and all kinds of stuff.
It's a pain in the butt to get to this.
So I'm really glad that I was able to leverage
my research skills to get to the right thing.
So the overall topic for the next 45 to 90 minutes
is gonna be Kubernetes containers in 2022.
And let me ask the question.
First of all, why do you think you know this?
And please, if you have input, please let me know.
Why do you think understanding container engines
in 2022 is important?
Why?
Why is understanding container?
I mean, it's pretty obvious to a lot of us,
but what's your best way of succinctly saying that?
Why do you care about the container engine
when you're deploying Kubernetes?
Why do you care about Kubernetes in general?
Why do you care about container engines in general?
Does anybody have any input on that?
You can go ahead and throw it out there in the chat
if you want to.
But I'm gonna submit to you that the reason
that you should know about Kubernetes container engines
is because they are the very basic elements
of everything that you use in Kubernetes.
They are, Kubernetes is described
as a container orchestration framework.
So if you don't know what containers are,
then you're gonna be totally screwed up.
So we should probably talk about, yeah,
we should probably talk about
what containers are versus virtualization.
We talked about that a lot last time
and about how I think that the future,
in fact, we have that in a different diagram.
I'm just gonna refer to it really quickly.
In my airgapped home network yesterday,
we were talking about, this is my conceptual thing,
we were talking about Kubernetes,
so here's gonna be, I've decided to do
a TALOS cluster over here out of OptiPlex servers
if I can get Pixiboot to work.
We have open PBS for batch processes
and then we have a traditional
on-prem Kubernetes cluster here.
And then we have all of these VMs,
potentially micro VMs on our single VM server.
So containerization and virtualization
are important that you understand.
I'm not gonna dive into that right now.
So we're assuming that you actually understand
the difference and why they matter.
And so we can jump right to the point
where we talk about, what do we talk about
just about Kubernetes?
Okay, so, where did that come from?
Did somebody draw that or did I draw that?
Did I draw that?
I might have drawn that on accident.
I did.
Or did somebody else?
All right, so, no, Docker Compose
is not a part of the diagram
because this is Kubernetes containers.
Who's drawing on there?
Did I give somebody draw access on accident?
Or did I do that drawing?
I need to make sure that I didn't allow that.
It might've just been a different color.
Do, do, do, that should not be there.
I feel like something didn't get saved somehow.
Yeah, that should not be there either.
I don't know what happened there.
Oh my God, it reverted my whole diagram.
It did.
It did, it reverted my diagram.
This is a different version of the diagram I had before.
It's got a lot of stuff on it that wasn't there before.
I'm kind of worried about that,
but it's good enough we can go with it.
There might be a bug in Excalidraw.
So, let's go back to it.
That was weird.
That was really weird.
All right, so back here we go.
So, there are some strong,
there are some really strong opinions on this diagram.
And I challenge myself to justify those all the time.
If you want to challenge my opinions,
please do in the chat.
You can do it right now if you want to,
or in Discord if you don't want to do it
like right out of the gate.
But what I'm gonna start by saying
that this is Kubernetes specific.
So, none of this has anything to do with other stuff.
So like, for example, Docker Compose and any of that.
So, and I think it's super important especially now
that we start to think of containerization
without Kubernetes as a separate beast.
And let me, this entire diagram is completely focused
on that separation based on the container runtime interface
as defined by the Kubernetes project,
which was led by the Kubernetes project,
but it's not necessarily just a container thing.
It was something that came out
from the Open Container Initiative.
So, let's talk about all of this and why this is relevant.
So, this, first of all, is for orchestration
and my orchestration, we've primarily been Kubernetes,
but it could have easily just been an old man or anything.
So, I'm gonna try to get into this
by telling you a number of stories.
So, the first story is how did Kubernetes start?
So, Kubernetes, it was really quickly,
Kubernetes started as a way to manage Kubernetes clusters,
I mean, containers, and to run them in a way
that could be easily started up again and all that.
But, the original version of Kubernetes
depended on Docker and Docker has always been
a proprietary thing and that was well and good.
But then in 2017, they created the people behind Kubernetes
came together and said, you know what?
Kubernetes probably shouldn't be depending
on this ever-changing underlying deep low-level API
from Kubernetes, we should probably standardize that.
So, they created something called
the Open Container Initiative, or OCI,
and that led to the creation of the container runtime
interface, which is this.
So, the container run, CRI defines the API
used to talk to container engines.
The CRI API comes from the Open Container Initiative
formed in 2017 to deal with problems from Docker
being proprietary and non-standardized.
As of Kubernetes 1.24, all container run times
must provide a full CRI implementation.
And if you ever read anything about Docker shim,
I mean, or any of these things, that's because,
and I'm gonna read it right here,
Docker is a significant decline after failing
to respond to the needs of Kubernetes
forcing the creation of Docker shim
and later hitting the industry with a bait
and switch tactic to force enterprises to pay up.
So, Docker itself is in massive decline.
Nobody's using it, people are abandoning it like crazy.
Enterprises like mine, which is a huge multinational
corporation, have decided to basically banish Docker
from their entire enterprise.
It's in massive decline, the founder of Docker
has left to pursue other interests.
The company was already in financial peril before,
and it's just a matter of time before it goes under.
That being said, it is still holding onto
the maintenance of the number one container engine
recommended in the certification exam for Kubernetes,
according to the official companies that IOS site,
which is Container D.
Now, those docs are kind of old,
those docs were created a while ago,
and Creo, which is coming out of the OpenShift world,
has really stepped up, and it doesn't have
as many bugs as before, and it's extremely lightweight,
we're gonna talk about that in a bit,
and it's a full implementation of the CRI and nothing more.
So, Container D actually has a lot of extra bloat on it,
because it tried to be more,
and that has an advantage or a disadvantage,
the ultimate decision in 2022 is, in my opinion,
is whether you use Creo or you use Container D.
That's really, Kubernetes is an orchestration platform
for Kubernetes, right?
People have been saying Docker's in decline for six years.
Yeah, well, I can tell you that a company
of the significance of the one that I work for,
and the other ones that I've heard of,
who've decided not to allow Docker on the desktop ever,
and not to depend on Docker CE, has actually been executed.
I'm in the process of doing it right now.
You can see it, and so there's a number of reasons,
even most recently, so Podman desktop,
let's continue the story.
So Kubernetes got started,
Docker was a good player for a while,
and then everybody's like, no more Docker,
and they've been trying to get off of it,
but Docker had such a stranglehold on the whole industry
that they've been really holding onto it,
and all of the other players weren't really that well
implemented, Creo had lots of problems
like two, three years ago, two years ago,
and it's since kind of come around from what I can tell.
And so now everybody's like, no, we're gonna use Creo,
Creo is the standard thing to use,
you should use Creo, it's the best thing.
But more importantly, underneath all of that,
is this thing that used to be called libcontainer,
which is now called RunC,
which is the thing that actually does the container creation,
it's the low level go library that talks to...
I think it's RunC in C or go, either one,
but it's the thing that actually does
all the container creation and stuff.
So that's really the biggest piece of this whole puzzle.
And actually containerD used to use libcontainer,
I was an ancient blog I read,
used to use libcontainer,
and then I read someplace that I wrote this here,
whereas the containerD is still maintained
by the Docker company,
which requires some Docker C packages to be installed.
The RunC container runtime engine was once libcontainer,
causing some confusion when researching
the actual container source libraries
used by the container tooling.
So if you're like me and you like to get into
the low level details to see, okay,
which libraries are the most popular,
which have the most stars,
which are getting the most usage,
and what are up the chain,
what are the products that are using that,
what are the projects that are using it,
then it's really obvious right now in 2022
that anything that's not using RunC
is gonna be completely irrelevant in this space
at this moment.
So in my opinion, RunC is where it's at.
And if you're not using RunC,
just, I mean, technically speaking,
Creo, which stands for the container runtime interface
dash open or open shift, depending on who you ask,
I've read both in two different places,
was created up by Docker, I'm sorry, by Red Hat,
and is meant to just fulfill the minimum elements
of the container runtime interface.
So let's read about the container runtime interface
for a bit.
So the CRI defines the API I use
to talk to container engines, I talked about that.
Okay, so Creo CRI dash show,
which is either open or open shift allows you
to run containers directly from Kubernetes.
Now, it was designed for use by Kubernetes,
but they were very careful to not say
this is only for Kubernetes, right?
So this is a container runtime interface,
which defines a set of operations.
So you might be wondering, well, what is it really, right?
So it's basically just, you can go read the spec,
it's really long and boring,
but it defines a set of operations
which are implemented either through direct calls
or through API calls or whatever.
And the operations that must be implemented are,
create, start, kill, delete, and state.
And you can go read about these in the specification,
but those are the only things required by the CRI standard.
And that is kind of an interesting thing.
And I've discovered this through the hard way.
So there is a tool that now ships with Kubernetes
called CRI CTO.
This tool does not ship with Creo.
There's a dependency on it,
but it's designed to be a part of the Kubernetes space.
And as I said, so there's no,
like an interface and programming, yeah.
So there's, the Kubernetes project is very good
at not putting their foot down on something.
It's kind of annoying.
So like the open container initiative
was maybe kicked off by Kubernetes,
but it didn't necessarily mean all things Kubernetes.
So everybody's following it, right?
So they're all implementing the CRI,
no matter what, to talk to containers.
And to tell you the truth,
there is actually the hope that I think
that maybe BSD containers at some point
could implement the CRI.
And if they actually do that,
then that means all of this cloud-native hardware
and every software and everything could run on BSD.
And that it, there is nothing that says it has to be,
you know, LXC, you know, Linux specific containers,
as far as I can tell in the specification
of how to make the container interface work.
So it's very possible that some day we might be able
to do that.
Now there's a whole big problem
about how do you store the containers,
you know, and this little green dots here.
I mean, these green things here, you know, and that thing.
So I don't want to get into that rabbit hole,
but there is some standardization here.
So the, if you want it, so the Cree CTL thing,
which I, as I understand it gets installed by Kubernetes
packages when you're installing those things,
allows you to talk directly to,
I think it's Run-C, I don't think it is directly
to the keyboard.
I can't remember, this one I might have wrong actually.
I think that this might, does anybody know?
I think that this might actually not be here.
I think it might be to, I think it, I had some,
at one point I had it pointing to the Qubelet
and then I had it pointing to the Run-C.
I think it might actually be able to talk to both
because I do know that when the Qubelet is not running
because the Qubelet crashes, right?
You can get on CRCTL and you can run CRCTL list
and you can list all the pods on the machine,
whether or not they're in the Qubelet.
And pods are things, static pods that have been defined
on the host, that's like, that's what all of that stuff is.
And they are just defined in configuration files
that point to containers that are running via Run-C
on the machine.
So you don't have to have a Qubelet.
CRCTL I think can talk to the Qubelet,
but it goes directly through Run-C
to the containers on the machine,
but it also has knowledge of the configuration files
that the Qubelet has access to.
In fact, that is probably something I wanna add here.
Let me go ahead and add that really quick.
So I think it would be safe to add
like a configuration file icon here and stuff.
So, you know, where's that doing it?
Where am I?
I don't know.
So yeah, I think that let's do that.
So that is the, why is that the fill?
Oh, yeah, we need to do, let's do white on this.
I'm almost positive that that's true
because it does not communicate with the Qubelet.
I know it doesn't need the Qubelet to be there
because the Qubelet is not there
when you do Qubetium in it.
But it can look at the file itself.
So this would be the configuration file.
So I'll add that in there.
What's that?
Those are possible options for components.
Is that a minimum you need?
When you're talking about a Qubelet endpoint,
you need a runtime.
You need a Qubelet, a runtime engine, I should say.
You don't need, and this is another thing.
There's the terminology regarding a runtime
and that people have said it doesn't have a runtime.
I've heard that statement three times
and I restated that to some other team today
and I was wrong.
I further did further research on that
to say that Podman does not have a runtime
is a false statement.
So this is very, very frustrating
because this terminology gets misused all the time
and I have read it at least in three places
since I repeated it incorrectly.
Yeah, but what is Run C?
Run C by definition,
and I have at least three or four sources of this
is a container runtime.
So I've heard at least five people tell me incorrectly
that Podman, as far as I know,
that Podman is a container runtime
and that's not true.
And they've told me that Creo is a container engine
that doesn't have a runtime
and that I cannot overstate how wrong that is.
And I just repeated it incorrectly.
It's using completely different wrong terminology.
So let's get to terminology right here, okay?
So according to the documentation,
Creo is a container engine
that fulfills the container runtime interface.
And it does so by calling into Run C
which fulfills the CRI stuff.
Now, container D, which if it's not confusing enough
has its own component called CRI
which also fulfills this CRI and its own way.
And I've read two different blogs
that said container D doesn't use Run C
and then I read another two blogs that were newer
that said container D now uses Run C
and then I put it together
that the other ones were saying
that it used container lib or lib container.
And then I actually read another blog that said,
actually Run C is lib container,
it got modernized and that's its new name.
So I just saved you like two hours of research
to get that down if that turns out to be true
because that was so frustrating.
I was like, what the hell are we talking about here?
So as of this moment, knock on wood,
I think it's safe to say
that all of the major container engines
that are used by Kubernetes all use Run C.
There is no other competitor to this.
And so anyway, this is the problem with blogging, right?
As it gets all outdated and everything.
So I was actually really happy to hear that
because that simplified my graph
because before I had to have like another piece here
or something, right?
There was another thing that fulfilled the CRI
and I found out, oh, it's just Run C, right?
So is Nerd CTL only for container D?
Show me that in writing someplace.
Show me that Nerd CTL can't talk to CRIO.
If you could show me that in writing,
I'd be happy to change it.
Cause I think that's what that is, right?
You can use CTR or that, right?
I think that's right.
Frankly, I didn't do any research on Nerd CTL at all.
I've just heard people talking about it.
I don't plan on using it at all.
I just let it put it there.
So if you know for a fact
that that only talks to container D
please let me know.
There is a CTR command that's documented
in the Talos documentation.
It's also documented in the QADM,
a NIT documentation on the official community site.
So in fact, container D is still, as I said,
is still front and center when it comes to certification.
It doesn't have anything
on the certification preparation materials for CRIO at all.
This is the Nerd CTL Docker compatible CLI for container D.
Docker compatible CLI for container D.
Okay, well, that actually really helps.
Thank you very much.
That's why I'm doing this.
All right.
Yeah, I didn't even go there.
I didn't know I didn't read anything
about the Nerd CTL at all, little or nothing.
I just put it on here
because people keep recommending it.
So that's good to know, that's good to know.
So container D is still a really strong container
but let me tell you the reason I'm not using it.
Container D is still maintained
by the Docker company officially
and requires some Docker CE packages
to be installed causing confusion.
I did this last year.
Last year, I installed container D as my runtime engine
and it forces you to do,
as soon as you pack, install or whatever you wanna do,
I had to install Docker CE in order to use container D.
And I was like, what the hell?
Because there were packages from it.
And they have never, as far as I know,
they haven't separated them.
So when you read the documentation for installing container D
because it's the same thing that's used in Docker now,
you'll read all this conflicting information
about how to install it.
It turns out that I had to,
yes, I had to follow these installation instructions
and then I had to go in and change all of the defaults
that were installed by Docker CE
so that they would use only container D.
And that step was a pain in the butt.
It took me like an hour to do it, felt like a year
but it took me like an hour to do it.
And container D filled the same role.
Well, I know that's what we're trying to get at.
So the way of JS libraries is if I feel like
the next best thing has come out every week,
yeah, well, the effect of it is is every time they do that,
they also complicate things.
But also, I mean, the abstraction of the interfaces,
the CNI as well, which is the networking part of all this.
We haven't even got that yet.
That completely complicates it even more.
So, you have to know about these things.
And then we got micro virtualization
coming on the scene too.
So anyway, so this is my attempt to simplify this
and make decisions for myself
and to stand by my recommendations for other people.
This is something that's very rare to find in our industry.
There's a lot of people that will tell you
all the possible ways to do things
and they'll have very few opinions.
And I'm gonna tell you why I chose to do these things.
I mean, Tal was, of course, all of this stuff
is irrelevant, right?
Because it's all black box, it's all hidden from you.
But there's other considerations for that.
So if you're installing your own on-prem Kubernetes,
you need to know about these choices.
And so, I mean, I still think it's extremely confusing
that the name Podman has nothing to do with pods at all.
Why did they name it Podman?
I need to drink after that statement.
Red Hat, can you please change the name?
That'll just confuse us all even more.
There's nothing to do with the Kubernetes pod
in the program Podman, not at all.
There is not a single reference to pods at all.
It just happens to be misnamed.
It has pods, but they're different from Kubernetes.
Oh, that's nice to know.
Okay, all right, I'm gonna update my,
I gotta make my thing more accurate then.
Actually, Podman,
as if it wasn't confusing enough,
Podman does have Kubernetes,
does have pods,
have pods,
just not Kubernetes pods.
That makes it even worse.
That makes it even worse.
I mean, the foundational concept is the same, of course it is,
but gotta help you if you think you're gonna be able
to go check all your pods on an input with Podman.
If you install Podman on your endpoint,
this is another thing that's really dangerous,
because I did it.
I'm about to tell you another story of something I did
that I learned from that I'm gonna save you some time on.
So it's like, so I was going through,
I was going through doing my Kubernetes installation
and I needed to get a container in order to use Vault.
No, in order to use
theCUBE VIP, what is it?
Virtual IP, CUBE VIP.
In order to install CUBE VIP,
which is a load balancer, right?
I had to have a running Docker instance,
a container engine on my endpoint,
but I wanted to install it on the control plane
and my control plane, I had Creo on there.
So I'm gonna tell you my sad story, okay?
So don't be me.
I was like, okay, I picked Creo,
I picked the lightweight engine, everything.
And that's where I learned this important lesson.
Now, just because something is CRI compatible
does not mean it's going to have run or exec
or any of the other stuff that you expect
when you're gonna use Docker or Potman
or any of these higher level tools.
So what happened?
I went in there and the documentation for CUBE VIP says,
either use CTR to install the container
that then you can then run,
which will then install the software.
You have to have a container engine running
and Docker quotes to even install this static pod,
which makes no sense at all.
But that's, in fact, I have soured
on that entire CUBE VIP thing now so much
by those, because of those decisions.
And there's a huge thread on their support channel
about how confusing this is for new users
and all this other stuff.
And now the fact that Creo is like the leading container engine
has further complicated the issue
and they don't even have anything there.
In fact, everybody's pushing for the CUBE VIP project
to just put examples of the config files for static pods,
which just have config files in the API
to just leave that and just let people figure it out
because they don't need to install this container runtime.
So because of all of this crap,
I mean, I lost two hours on this shit.
So I'm saving you some time here.
So I was like, okay, I'm following the instructions
and it says that I need to install,
it says I need to install a Docker engine
or something like that, a container engine.
And then I needed to run it
as a way to bootstrap the manifest system.
Yeah, I agree.
And so it either lists CTR or Docker, that's it.
CTR or Docker.
So what did I do?
I assumed, well, okay, I'll try,
I didn't have CTR because I didn't have
a container runtime engine.
So I made a bunch of bad guesses.
And the first bad guess I made was,
oh, I'll just use Cree CTL because it has a list command.
It shows me all of my containers.
It shows me all of my pods
and I don't even have to have a running queue,
but this will be awesome.
I'm sure it's Docker compatible and not yet, we will.
And so I went to go see if it would work
and it didn't work, obviously, why?
Because Cree CTL only deals with pods.
And pods, yeah, sure.
We had some people tell,
oh, it's fine, it does containers.
No, it doesn't do containers unless they're a pod.
So if you have written all the configuration file
to turn a container that's on the system into a pod,
yeah, it's happy to play with that
and it'll do it and you can run it.
You can start it, you can't run it, you have to start it
because it only implements the minimum necessary
for CRR, which is create, start, kill, delete.
There's no run.
Okay, so that was the first mistake I made.
I was like, I cannot do this.
And then I had a whole bunch of people on the stream
telling me, yes you can, you just have to turn it into a pod.
I'm like, I just wanna run a stupid install script here.
Why did they not give us an installer?
Why did they not give us an install script?
Why didn't they give us config files?
And everybody came up with opinions that are like,
oh, blah, blah, blah, and I'm like, look, I'm the noob here.
I just wanna use this thing.
And I about threw it all out
because just because of the bad installation steps
because of all these complications
that they didn't even think about.
Yeah, create your YAML and then you're good.
You just copy and paste it.
But it doesn't say that in the documentation at all.
It says your best way to do this
is just go run the container and do this thing.
They could have just provided like a bash script
and been done with it or any of these things.
They didn't do any of that.
And that frustration was enough to almost send me
down the HA proxy path,
which is completely separate from all of this stuff,
which runs under its own daemon,
which I would probably run as a VM
completely outside of Kubernetes if I ended up doing that,
which is a load balance.
It's a different topic, but the accuracy,
I think they should.
I think they should, a lot of people agree with it.
They should do that.
So anyway, I mean, I lost hours on that.
And so did everybody else that was watching this.
And I'm saving you the time by looking at this annoying,
don't do that.
And the reason, I actually, it was so annoyed by it
that I actually read the entire,
I briefly scanned, I should say,
the entire CRI specification to see why this command
isn't in there.
And that was when I uncovered that only create,
start, kill, and delete are truly supported.
They're the only operations required by the CRI,
by the entire specification.
So I was like, okay, so mistake number two that I did,
mistake number two.
I said, okay, well, if I need a container runtime engine
over there and I need to get to these containers,
I need to be able to do the,
I need to follow the instructions,
which are Docker run, right?
Or CTR run.
I'm like, huh, well, there's no Docker, there's no CTR.
What's my next best guess?
Podman.
So I mean, I was like, well,
Podman is supposed to work with everything, right?
It's CRI, it's got CRI built into it.
I'm sure that will work.
Wrong.
So I installed Podman onto the cube controller,
the Kubernetes plane, so that for one reason only,
just so that I could run the container that would then,
you know, start and install my cube, my QVIP, right?
And it worked, it worked, it installed it just fine.
But then I later on, I started noticing
that what the Podman was showing me
was completely different from what Cree CTL was showing me.
And I noticed that they were looking at different sockets
or different services and things completely entirely.
And then I realized a very, very important lesson.
And I cannot overemphasize this enough.
Do not mix your Creo container engines
on a Kubernetes node with any other high level
runtime management tool.
Just don't do it, because you cannot be sure
that they're gonna use the same thing underneath.
In fact, the very nature of the engine makes it
so that they have radically different ways
of implementing things.
Container D has its whole socket thing.
Creo has got its own service thing.
And you know what I'm saying?
It's like there's just a ton of reasons not to do that
because the way that containers are implemented
by the engine, even if the underlying LXC stuff
is the same, because that's what this is all into, right?
But that middleware between the person talking
to the container for, or the thing talking
to the container, person or service,
and the underlying LXC execution of the containers,
the stuff in between there can radically differ.
And this has been the whole area of the fight
between Creo and Container D.
Container D is insecure.
I mean, they fixed a lot of it now,
but Creo does not have, get privileged,
it doesn't have a socket that's running its route.
All the things that Docker just got beat up over
for just doing poorly.
Inefficiently, poorly, and insecurely,
Docker just really messed up.
And they just never intended to fix it.
And so Container D has tried to kind of fix
all those things over the years and say,
well, yeah, okay, we did that wrong.
If you wanna do it this other way, do it this other way,
but it's not the default, blah, blah, blah.
It's a pain in the ass.
The bottom line is that the stuff
in this middle layer here, right?
The stuff right here and this middle layer
is so different depending on the stuff
in the top layer that gets installed
that you just don't wanna mix the two.
Do not cross the streams.
If you do, you're gonna get burned.
And I'm gonna read the statement down here
that goes with this.
I'm gonna zoom in on this a little bit and read this.
But this summation down here with the explosion
is my conclusion about this.
So with Creo containers,
the Creo containers must be put into pods
to run them at all, all right?
So this is considered good
because using a container runtime for anything
but Kubernetes on a node, on a Kubernetes node
is considered bad practice.
In fact, when I suggested on the stream,
while I was doing this,
that our company regularly does this
because they don't allow users to have their own Docker
and they regularly tell people
if you need to run Docker for anything,
just go get on one of the head nodes
and run Docker straight up on it
and just reuse the Docker container engine on there.
That's also running the control plane containers
and pods on there.
That is like a really bad offense.
It's not the kind of thing you're gonna get fired for yet
but when I mentioned that on the stream
that people were doing it,
people were laughing their asses off.
They were freaking the fuck out.
They could not believe that any company
would ever allow that
because it's not only is it insecure
but it's just asking to blow up.
It is, it's asking to blow up.
It's absolutely horrible to do that.
Now, I tried to make the case wrongly
that well, what if you only have one machine
then you have to decide
and maybe you wanna share both of them over there.
You know what I mean?
And the fact of the matter is
thou shalt never ever use your Kubernetes nodes,
the container engine on your Kubernetes nodes
for anything but Kubernetes.
Here's some VMs on ES6, of course.
So yeah, right.
And so let me just continue reading this.
So it's considered very bad practice today.
It is also a bad practice to install Podman
or any container utility on a node
because the engine used may not coincide.
And it may, it'll still use LXC down on the bottom.
It has to, but the stuff in between there
is totally different, whether it uses a service
or a daemon or a container.
So it's not gonna be what you would expect, right?
And so as you become familiar with the internals
of how, you know, Creo uses its thing, which is a service
then you know that how to do that
but a container uses a socket, which is totally different.
It may be that there's multiple options for both of them
but the point is, is that you can't be sure
that you're apt install or DNF install Podman
is actually going to do the same thing
that you used when you installed your container runtime engine
which is a totally, whether it's containerity
or it's the other thing.
So do not mix the two.
If you mix the two, you're just asking for pain and hurt
even though you think it might be saving something, it's not.
It is also bad practice itself on that, okay.
As a rule of thumb, use kreectl for pods,
containers on a note.
And I said the word pod there on purpose, right?
Kreectl cannot directly access containers, period.
It doesn't have to go to kubelet.
That's why I drew that little picture
of a configuration file.
So if you have static pods on the machine,
it can talk directly to those pods
but it has to have knowledge of those pods.
That means it has to see the configuration file
to know how to wrap up that container
and put it into a pod and then access it using a pod.
And of course, you can't use kubectl
on the endpoint without kubelet.
That has to have the API as far as I know.
So as a rule of thumb, use kreectl for pods,
containers on a note, everything on a note,
for everything on a note.
And podman, despite the horribly confusing name
or nerdctl for non-kubernetes containers.
So containers are still very relevant.
So people were asking me about Docker compose,
why that isn't on here and stuff.
Because Docker compose and these top-level things,
these top-level things up here,
they're nice and good and all
but they're not really, these things up here,
these things do not have to be associated with Kubernetes.
But that is an entirely different approach to containers.
And I really wanna emphasize that in 2022,
the best thing you can do for yourself as a beginner
or as a veteran is to think of Kubernetes containers
differently than everything else.
Because if you do, you'll save yourself.
You say, well, they're all just containers.
And yeah, okay, they're containers
at the lowest, lowest, lowest, lowest, lowest, lowest level.
They're all using LXC.
Okay, that's true.
But everything in between there and you
is totally different depending on whether you're doing it
for Kubernetes or you're doing it for Docker compose.
And to save yourself a lot of pain and suffering,
I cannot overstate this enough
because I went through hours of it.
That you need to separate those concepts in your brain.
And should you learn Docker compose?
Yeah, should you learn Docker swarm?
I don't know, maybe not.
But Docker compose is not bad.
And you can do podman compose too, right?
But those are things that are for a totally different approach.
And frankly, we need to have, in fact,
from now on I'm gonna refer to them as Kubernetes containers
and probably Docker compose containers.
Because those, to me, that's the biggest separation, right?
If you're using Docker compose
or you're using podman compose or whatever,
but Docker, as soon as you were Docker and compose,
you're like, oh, we're talking about that.
You're talking about all the container wonderfulness
that doesn't have anything to do with Kubernetes.
But when you start talking about Kubernetes containers,
you're talking about a different beast altogether,
even though some of these things have similarities
at certain points in the architecture.
Whew, now that seems like a lot to say.
But I really feel like I finally understand it.
And I just had to capture that really quickly into a video.
That's all I have for this specific video.
And hopefully that will save you some pain and suffering
as you go about your Kubernetes admin installations
and all of the other stuff that you might end up wanting to do.
This diagram is available.
Anybody to come see, I'll put it in the Discord.
And I'll be using this diagram to make sense of my world
as I go about the installation of my,
the next thing I'm gonna be doing,
I'm gonna be installing these clusters.
And I, did we save this already?
This, I think it saves automatically actually.
Supposed to anyway.
But so yeah, so this, the next thing I'm gonna be doing,
which I may or may not do today,
is I'll be installing some other Kubernetes clusters
over here and we're gonna go back to using QBDM for that.
I do need to solve all so I can get some PKI root CA stuff.
And I need to get my head around
how I'm gonna manage VMs on this machine
because I'm gonna be running a Cordeon S&D,
a Key Cloak and Vault and all of that stuff
before I go forth with my QBDM installation,
which is probably not gonna happen tonight or even tomorrow,
but I am going out of town for next week.
Yeah, and you like Vault, yeah.
So just to give you an overview of what's next
and why this might relate.
Oh, we use build a lot.
Yeah, so builda is actually built into Podman.
That's another thing I like about Podman
is the build is built into,
and builda is just,
it's all it does is just build the images, right?
And escopio is another good one
that will transfer containers between registries
and stuff like that.
So yeah, just to finish out the idea
about where this is going.
I keep drawing on here
because I'm still got that selected.
So the next steps are probably,
so as I got my whole Vlan all set up,
I'm so happy about that.
There's a video on that if you wanna watch that.
That we're gonna go ahead and,
where does Istio fit?
Istio is just a service mesh.
Yeah, so we are gonna do Istio,
but it's gonna be like the last thing.
The main thing Istio gives you,
it gives you lots of things,
but the main thing it gives you
is like virtual machine, virtual servers
that have domain names that get a little balance of stuff.
Yeah.
Yeah, we still have to pick a CNI and all that.
So before, so a couple of changes
just to update everybody who hasn't watched.
So this is gonna be the TALIS cluster over here.
Right now, I'm doing some testing offline
to see if I can TFTP boot
one of the Dell Optuplex machines that I already have.
There is some problems with the Pixie boot
and it requires you to push F11 to do the booting.
So that does annoy me a lot actually.
And because I can't, I don't know.
I'm really tempted to just throw TALIS out for now.
As soon as I read that TALIS cannot do,
I mean, this is another topic.
I should talk about this, not right now,
but when I found out that TALIS cannot,
that these machines cannot do true Pixie boot
that any time you, TALIS requires
being able to reboot the machine.
And I have not gotten around.
I need to ask somebody from Cedaro or TALIS
as like, what happens about that, right?
So I want a totally, you know,
hands off installation and management of TALIS.
That's the thing that they keep promising you.
But if every time I reconfigure a server,
I have to go in and touch every server and reboot them.
I don't, it's not even, it's not, it's a non-starter.
I don't even want to do that.
Thank you for that.
So, so I, I'm, you know what?
I'm just gonna make a decision.
I'm gonna throw these out.
I'm gonna throw TALIS out of the environment for now.
For now, I got too much other stuff to deploy.
And we will go with that.
It might be that I do PBS for this.
We don't know, the key HTTP from IC.
Yeah, did you know, did you know that,
did you hear that we found out that
IC DHCP is supported on OpenSense?
That's the, that's the DHCP it uses.
So you can go in there and edit it.
The problem is, is you got to save your files someplace
because if OpenSense, if you update the firmware,
it'll throw out your entire configuration.
So you got to, it will back it up for you.
But it's something that's kind of, I was,
we, I found out, is it, yeah.
I think the key DHCP server is probably gonna be the one
that I'm gonna install.
I am going to install a DHCP server on my internal VLAN,
but I don't know how much of a priority it is right away
for years, very, very fantastic, very good stuff.
So, I mean, as far as YouTube goes,
I'm gonna go ahead and end that video here.
So just as a follow-up, so, you know,
have fun with that, with the container runtime engine,
maybe play around with it.
The last thing I will say, for a very long time,
I've been, I was anti-podman and I was wrong.
I was wrong for a number of reasons,
but the most important reason that I was wrong,
I was mostly wrong because I was looking at how
podman manages a container.
For example, podman does a lot of magic
to the container.
For example, it injects the system CTL into any pod
or stuff that enables system CTL to be more specific
because they make a big case at Red Hat
that container should be able to simulate machines,
you know, and be basically miniature VMs,
which I completely disagree with.
I think micro, you know, use the virtual machine
for a virtual machine and don't use podman for that.
It was annoying though, like a year and a half ago
when I was doing the boost and we were using containers
because I couldn't simulate the installation
and management of services the same way you would
on an actual Linux virtual machine.
And that's one of the main reasons that we switched over
to this when you're doing like system CTL, you know,
restart, you know, whatever, HTTPD or NGINX or whatever,
those kinds of commands can't be done in a container
and they can't be practiced in a container.
But if you just need access to a terminal
to practice bash and stuff, great.
But if you want to practice systems administration
and on whatever, I think you need a virtual machine
which prompted the change to use virtual machines
in the boost in 2022.
So in 2023, however, I'm strongly, well,
I already decided that we're gonna be doing all four ways,
all five ways of getting Linux, which I've mentioned before.
And one of those ways is podman.
And the reason I'm even saying about this now
is that it's six-caliber, you guys wanna copy again.
So as I've said, the podman, now I've tested it
in three different machines, I've tested on Mac, Linux
and Windows, the podman desktop installation process
is as easy as desktop, Docker desktop used to be.
I need a lot of commands.
The command thing wasn't even working till yesterday.
So I'll update all of that, I need lots of things.
I also need help and time and a clean room.
Okay, so we will be providing instructions
in the boost about how to get podman up
and running on your desktop.
But I just wanna kind of conclude this YouTube video
with that, if you haven't tried podman out yet
on the desktop, whatever your desktop is, you might try it.
Something else I'm super interested in
is that podman actually chose to use Kimu and KVM,
which is exactly what Docker desktop does, by the way.
Okay, here's a fun fact that you might not understand.
I'm gonna actually sip some wine for effect.
Here it comes.
You cannot run a container on anything but Linux.
Someone explain that while I take another sip of my wine.
You cannot run a container,
if you wanna get really pedantic with your friends,
pick you up, you cannot run a container
on anything but Linux.
A modern container, a Docker container,
if you wanna put the word Docker in front of it,
you cannot run a Docker container on anything but Linux.
BSD has jails, they are not Docker containers.
So if you wanna use the word that people will recognize,
the statement, you cannot run Docker.
But I run it on my Mac all the time, you know, you don't.
You do not run containers on your Mac.
You do not run containers on your Windows machine.
You don't even run containers in WSL.
On Mac, they use a compatibility layer.
And on, you know what the compatibility layer is?
Kimu, Docker desktop and Podman desktop.
And I assume now Rancher desktop all use a minimal virtual machine
that uses Kimu in order to emulate Linux.
And that's how they do it.
You don't even run containers in WSL.
On Mac, they use a compatibility layer.
Linux, and that's how they do it.
They all do it.
And so I was actually looking for the internals of the Podman.
I'm like, what are they doing here?
It turns out they completely straight up copied
what Docker desktop was doing.
Docker desktop is this nice, happy, you know,
electron front end to everything.
But under the hood, it's running Kimu.
And I think Zen, it's not Zen, no, it's not Zen,
because I couldn't run it on the hardware.
Containers need C groups for Linux kernel, I think.
Yeah, LXC, C groups and LXC are what modern Docker containers
are defined to be.
So you cannot have a Docker container without Linux.
So next time you run that on Windows or whatever.
But there's something else I want you to consider here.
It used to be that I would like complain people,
or I would like, well, God, I got to install a whole virtual machine.
In fact, Podman was kind of slow to do Docker desktop.
You know why?
Because there are pedantic engineers over at Red Hat,
and they're like, well, you got to have Linux anyway.
Why don't you just install your own VM and just do it that way?
Am I wrong?
That was Red Hat's position in my mind up till like last year,
or within the last three months.
Before that, they were like, you know what?
Podman's like, we don't need to do stupid desktop stuff.
Nobody needs that.
I mean, everybody knows that containers have to have Linux anyway.
They'll figure it out, and then they'll install a VM,
and then they'll put Linux on there,
and then they'll do Docker as God intended by installing it on Linux
using Podman install.
They can do apt install Podman or DNF install Podman or whatever they want.
And so Red Hat missed the whole desktop market by saying,
just do it that way, and then what happened?
And then last year, Docker famously said,
psych, you'll have to pay us now if you're a certain size,
which is pretty much everybody using it.
And Docker perked their ears up, and Rancher perked their ears up,
and they're like, oh crap, there's no free desktop option anymore.
People will stop using containers and be annoyed by it,
and they'll stop using our other products.
We got to fix this.
And so I imagine somebody over at Red Hat,
somebody over at Red Hat's like, damn, we got to get on this.
So then Rancher desktop and Podman desktop
kind of seem like they're in a race to see who can make the best
desktop container option.
And now we have Podman.
Last week we had, I tested it all out, and Podman works wonderfully
on everything.
It's really weird that it uses WSL instead of WSL2.
I think that's very interesting.
Probably because of the hypervisor dependency.
But I don't know.
I don't know.
You know, our instrument windows is on the Linux containers.
Yes, you can.
That's pretty cool to have that, isn't it?
Anyway.
So we're virtual machines in disguise behind a poorly optimized
electron interface.
I know, exactly.
I mean, you know, in retrospect, I got to tell you,
it actually has improved my position on what I think of Red Hat.
Because Red Hat's like, no, we're not going to do that.
We're not going to bundle a bunch of really beautiful garbage.
We're not going to put lipstick on this turd and get people to
use this virtual machine that's going to screw up.
I mean, Docker did it because they wanted the user experience
to be nice and easy.
That's what made Docker famous.
Because the Linux LXC people forever have been saying,
you can do containers.
And Docker comes along and says, well, yeah, but the CLI sucks.
There's no way to do it.
The average person can't do it.
So we're going to make this nice, pretty happy command line
thing called Docker.
And guess who gets all the credit?
Now they're called Docker containers.
They're not called Linux containers.
Everybody who worked on the LXC project and C groups,
and all the people who made containers happen,
which have been around for more than two decades,
about a decade, I think, probably something like 2011 or something.
They're like just smacking their foreheads.
And they're like, what the hell?
Along comes somebody who cares about their users.
And they're like, no, we're going to make a nice, happy front
and interface to this thing, to the great anger of all the people
who were using containers the hard way.
And they get all their credit.
And then they decided to do this bait and switch on top
of everything else, which just makes the Linux community
fucking hate them because they're, you know,
and that part of the Linux community would have been,
in my estimation, would have been the Red Hat.
Red Hat is more a member of that technical part
of the Linux community than Docker has ever been.
And I mean, there's good people with Docker.
I'm not attacking any specific individuals,
but as a company, they've had really poor decisions.
And so they won't call it good 3C groups.
No, they won't.
I call them containers and container images.
I don't say Docker, nobody does anymore.
Docker is more like canonical in that sense.
Docker has the user's considerations in the forefront
and they will violate good back-end architecture
to make it easier for the user.
And I think that's a good thing.
I do.
I think that we need to, it's a combination of both, right?
And it took them doing that to get Podman
to have a decent desktop offering.
And I'm really glad they did that.
So it's a big, you know, it's a big dynamic going on
all the time.
And many of the things I've said have probably offended
many people very seriously here.
I'm sorry.
I largely don't care because I feel like
we need more opinions in this industry.
There are just too many choices.
And we need more best practice opinions.
And we need to justify those opinions and say,
hey, if you meet this criteria, this is what you should do.
And we should have the courage to start saying those things
and then fixing them when something better comes along.
Otherwise, we're just going to be just floundering around
like we have like for years and we still will continue to do that.
Both the Docker Project and KhanCon have always gone their way
out of the aspect of show typical signs.
Oh, right.
Yes.
Yeah.
But not invented here.
They do that.
Yeah.
I mean, that's just a tech problem in general.
But at the same time, I do believe, to their credit,
that Docker and KhanCon have been more focused on user experience
than the technology.
And that's not a bad thing.
That part of it's not bad.
So I don't want to Docker to integrate better with system
B, but Docker just already said, nah.
Yeah.
Docker's been doing that all along.
The whole system B stuff.
The system B integration and the, you know,
they were trying to stand their ground and say,
we want to have an independent socket.
And I actually agree with that.
I mean, for some of the stuff that I was doing,
I was able to, you can't do Docker in a Docker.
You know that, right?
You can't do Podman and Podman.
It's impossible.
Before you could just remount the socket and you could have,
you could have a Docker running inside a Docker container,
use the same host container image.
Now, some people would call that an absolute Frankenstein's
monster of problems.
But yeah, the idea is it's just bad,
but it doesn't matter because it was very practical.
I use that all the time.
I use that all the time because I didn't have any Linux on my system
before I switched back over to VMs because, you know,
and by the way, the VM is the clear way to do that, right?
If you install a Linux VM, then that's not a problem.
All your stuff lives in the VM.
You know, you're good to go.
Assuming you can get a virtual machine on your machine.
So I think there's going to be a strong resurgence of VMs in the world.
But yeah, but the Docker and Docker thing was always kind of a hack.
And by the way, it was actually using pods.
There's Kubernetes pods and stuff that would use the Docker
and Docker thing, which is just like such a security violation.
It's outrageous.
So that's why people are owning it.
But I did think it was useful for pipelines.
That's why I used it.
I did.
I used it for that for a long time.
And there is no alternative in the Podman landscape.
The Creo landscape, Podman landscape is not there
because it's insecure to them.
So not enough for VMs.
Yeah.
And if you don't have enough for VMs,
then you got to do the Docker and Docker thing.
And so that was, again, it was a very practical thing,
but it probably wasn't a good thing.
For developers, it was nice.
But for all the other reasons, probably not with a VM.
Yeah.
Not everybody has the option of running non-windows on their desktops.
I just read another blog from a guy who was super annoyed
that he had to run windows on his desktop at work.
And it was all the ways he got around it.
But yeah.
Hmm.
And ruin the nodes quite often.
You've seen DIND pipelines in the wild.
Devs have probably used Docker.
I'm not messing up.
Oh my God.
DIND was always used in CI as tools to simplify stuff.
So it's not that bad keeping things simple.
I think that's right.
And that's one of the reasons I think that DIND,
it's a huge debate.
It's very good wine.
It's also the end of the wine.
We don't have any more.
Hmm.
But it's the weekend.
And I hope everybody, you know, I'm going to,
I'll say cheers to everybody as I go get more wine.
I'm going to go ahead and end the YouTube videos here.
So I'm trying to keep the YouTube videos shorter.
I apologize for how long winded I am.
And I do hope that this doesn't scare you away from all this technology.
I do find it extremely exciting.
I think the movement in the industry is in the right direction, actually.
All of this movement might make CI CD more annoying
and developer processes more annoying,
but it's going to make for a more secure container orchestration
as we adopt these practices.
And it's going to, actually, it's going to ferret out
a bunch of bad practices.
So for example, putting Creo as your container engine on your nodes,
including your control plane,
is automatically going to shut down any user who's using
that control plane head node for Docker.
And their process is going to break.
And they probably should break because they shouldn't be mixing the two
as we talked about.
So hopefully this will be something that you can consider as you go forward.
I don't know if I'm going to do another actual video tonight.
I'm probably just going to be shooting the breeze on Twitch.
And as a general rule, though, going forward,
I'm going to try to start each one of my streams
with a YouTube video quality content that's very organized
and about a specific thing.
And then we'll stop the video.
And then we'll just do silly stuff for the rest of the night on Twitch.
So we can start on a good note and get some value out of it.
And then we can do the crazy stuff that I tend to do.
All right, so if you want to come and join us for the crazy stuff,
come on over to Twitch.
Otherwise, thank you for stopping by whether you're on Twitch or YouTube.
