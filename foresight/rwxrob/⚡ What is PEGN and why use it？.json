{"text": " So, what is Pagan and why use it? So Pagan, I've talked about this a lot, but I'm going to go through and review it again. So Pagan is a language that I wrote, you can go to pagan.dev, I think I still have that up there. I've been using it a lot because I'm using Pagan to write KML. Sometimes you need a language to write a language, it's called a meta language, and that's what Pagan is, you can go here and read about it. So Pagan is a language for defining languages, more precisely as universal notation for expressing any grammar, including natural language, which I'm very proud of, in a way that is easy to parse cognitively and programmatically without any specific application or implementation in mind. And this is the problem with Pagan, which is from Brian Ford, which I'll get to. Pagan builds on the best of the existing meta data structures such as Pagan, ABNF, EBNF, and JSON. If you've ever read the GoLang specification, it's all an EBNF, it's very common for people to write language specifications, an EBNF, it's more common to see ABNF for things like Internet IRFCs and IRFCs, things like that. But ABNF, you can't even use characters in it, you have to use hexadecimal notation for everything, just because it's so precise. It's like, how do I define the HTTP protocol? Go read ABNF for that. Now, ABNF allows ranges and stuff like that, Pagan does not do that, Pagan is a very, very friendly kind of flying in the face of traditional parsing mentality, primarily by saying we have infinite memory in our assumption, our base assumption, rather than saying we only have one byte of memory, which is what all the other parsers do. I am absolutely obsessed with parsing, I think it's so fun. If I was going to have done a computer science thing, I would have really, really obsessed about parsing. And I've already written in Pagan parser some time ago, a couple years back, and we've had lots of people use Pagan, Quint comes to mind, Quint, if you're out there, thank you for your contributions. Quint has actually got his Pagan parser that we kind of collaborated on running in production for a rather large company that I won't say. But it is a way of specifying languages, and so what does that actually mean? So the motivation here, let me just keep reading here, and then I'll close this up. So the motivation, technology increases complexity, the need for a better human-computer interaction becomes more pronounced, creating language grammars quickly and simply has become a critical need. And there's something that I have found out recently has got the name Fluid APIs, and that's why I made Bonsai. So Bonsai defines a natural language API that could be spoken on the command line as opposed to demanding dashes and get off and all that other crap, which is another project I made related to this. But Pagan is somewhat related to the same need, because we need a way to define domain-specific languages very quickly. The way that I learned about Pagan, quite frankly, yet again, TJ Hollowaycheck, I was following him and he wrote his own domain-specific language for parsing and querying logs for his APEX, one of his projects on his APEX company, and he used Pagan for that. And so I found Pagan, I actually helped contribute to one of the GoPeg parsers that's out there, and then I realized that it didn't have enough, and I needed to add a little bit more, and so I made Pagan. So Pagan has decided to keep things by allowing any data to be represented as a grammar and breaking it down into universal data form, it can be composed, combined, and analyzed in a remarkable way. So my eventual dream is to allow Pagan notation to replace regular expressions. Regular expressions are nice and all, but when it comes to structured data and the ASTs that are produced by it, it was really, really problematic. Whether it be simple counting all the words in a document, creating a simple query language to make searching logs easier, coding a human-friendly interface to an otherwise complicated web API, simplifying the parsing of a form of common markup, implementing a fully-programmed language that leverages LVM to quickly create high-performance compilers, or developing a binary language for moisture evaporators, ah, Star Wars joke. Pagan addresses these issues by prioritizing the creation of other language grammars without weighing them down with any specific bias, and I keep hitting that point because that's why all of the Pagan parsers that are out there right now, Guido van Rosen, Pagan of the Python creator, obsessed with Pagan, there's lots of videos from him trying to re-implement the entire grammar of Python and Pagan. I don't know if he ever made it through that or not, but lots and lots of implementations of Pagan that forced you to use that specific language, and so Pagan, the N in Pagan stands for notation, which means we don't care. It means we are not associated with any specific language, and therefore it's universal. It's now, and see, Python, is it okay? Weighing it down is specific bias commitment. In fact, Pagan is so flexible it can also be used to define written language as a music notation. Pagan grammars are exploding, but inconsistent in 2004, Pagan grammars have exploded in popularity and that's when I started this. God, that was, I mean, that's not true. I started, that's when they came become popular. I didn't get into it and probably tell, I want to say 2020. I was already streaming, so it'd be like 2021, 20. Anyway, Brian Ford's example, Pagan grammar is all but ignored. It's also incorrect. While we were going through the parsing of his example, we realized it violates his own syntax. I still haven't ever told Brian that. His example is invalid, Peg, according to his own specification. I don't think he actually ever tested it with a parser or anything. We actually found that out very surprisingly, quit when we were going through making our own parser, and we use this example. So Brian Ford's example, Pagan grammar is all but ignored as people continue to build on their own syntax as a very little resemblance to the original. I think that's probably why it doesn't work, because he just threw it in there because everyone else has got their own idea of what it should be. There are more implementation code than Peg. This is demonstrated by many projects that contain both a grammar file for becoming acquainted with the syntax and another virtually identical file containing additional implementation specific to the code, a specific implementation language. So this is how it goes, right? You run your code through this and through the parser, right? And then the parser will actually, if you do a Peg parser, right, it'll actually generate code for you. It'll generate parser code in your target language, and, you know, like Lex and Yak and all those other things, except for its much more modern approach. And so there's lots of them out there, you can go look at them. So anyway, this redundancy and specialization are not only less sustainable, but they're also highly rigid and counterproductive, and you can't exchange them with anybody else. Pagan is a language grammar specification that does not allow implementation code so that the resulting grammar specifications stand on their own, allowing their creation of any variety of lunches and co-generators in different language implementations. Even different design variations in the same implementation language, AST, callbacks, etc. I'm going to get there, but Pagan actually defines the AST format as well, which is not included in any other thing. So I mean, having the AST, which is, you know, the thing that you care about when you run some code through a parser, you want the AST then so you can check for the syntax properly, or you can, you know, create your compile in any number of ways, or you can transpile or whatever. You have to have an AST. I mean, there are implementations that don't use an AST because it takes, it's a minimally, you know, slower to do that. That's the compile part of regular expressions, by the way. When you say you have to compile a regular expression, you're pretty much building a binary that's going to give you an internal AST really quickly. And then, you know, it's kind of internal. I don't even know if there's an AST for most regular expressions, actually. But these days you want the AST because that's the thing that's going to give it to you. So like if you do, if you do pandoc, let's say you take, pandoc actually has this built in, you can do pandoc t, is it raw? You can do JSON if you want. So JSON, this is a representation of this document in an AST. It's a very simple document, right? And it's parsed using pandoc with the specification for markdown, pandoc markdown. And this is the resulting JSON version of the AST. Now, there's another one. I don't know the name of it. Is it dead? I don't know what the name of it is. I want to say raw. Maybe it's data. It might be AST, data. I wish pandoc had completion. It does not have completion. There's another number of reasons I don't like it. Oh wait, maybe it does have completion. When did it add it? Maybe it already said, I always had it, I just forgot. Which is the one we want. We want not GFM, we want not LaTeX native. There we go. So this is the internal Haskell AST that is used by pandoc. So it tells you the thing, so you have a header, and then you have a string, and then another string, space, space, string, space, string, space, space. So this is Haskell is the perfect language for writing ASTs. I want you to notice something here that all of the nodes that are parsed in the AST have three elements to them. They have the type. I think they have an array of how many there are, and then they have the actual content. I can't remember. Oh no, no, that's a variation on a header. It must be an attribute of it, whatever. So that's the AST, and you need an AST if you're going to do anything. It's a Haskell data representation, yeah. This is a Haskell data representation of, and I don't know, they have nodes with attributes which I hate. So Pagan does not allow attributes for a reason. You can never decide whether you want your node to have attributes or not, or you want it to just have the attributes to just be subnodes. And so Pagan doesn't allow that. It's a pretty deep comment, and if you don't know about parsers and stuff, but it's really a... ASTs are all over the place, by the way. If you go look at the web page, this is an AST. This is an A... Any time you look at this, this is an AST. The reason node is called node is because the document object model is a specification of how to parse the thing. That was at one point defined through something else. It can get tricky without properties and attributes. I haven't had any problem with it so far because you can just define... You don't want to have one type have a different attribute to it. I mean, like H1, H2, H3, you can actually have a parent, and then you can have the child be the attribute. It does get a little bit harder to traverse it, but it can be done. So anyway, Pagan doesn't allow for properties like that, and neither does Peg, actually. Strictly speaking, though, I mean, you could use Peg or Pagan, and then just... I mean, the AST is... You could use your own AST instead of the one that I have in here, if you wanted. But original Peg lacks specificity. For yours, ABNF and ABNF provided a cruising level of specificity. Yeah, I don't trust this channel. You have to switch, yes. But it's very stateful. So it's like one state at a time, and that's what I like about it. It's not some variation on the state. You see all different variations in node trees when people are doing this kind of thing. There's people that will make all the nodes the same, and then they'll have attributes on each of the nodes, and that's the only distinction between all the nodes. And there's a million ways to model this. And my favorite is to just... You either have a node that's apparent that has children, or you have a node that is a leaf and that is an attribute or whatever in this case. So anyway, ABNF and ABNF provided a cruising level of specificity in their grammars, but lack of obvious advantages of order priority and a simplicity of the original ASCII Peg grammar. For example, Pagan adds count and min max to provide limits and adds unicode tokens. So there's no unicode tokens in ABNF at all. You have to do everything based on that. In fact, they created an extension to ABNF to allow for... There's one of the data things from ABNF had to be expanded. I actually wrote a VIM syntax header for ABNF if anybody's wondering too. If you ever just want to open an ABNF VIM file and have it actually make sense, then I updated it to include some of the new stuff from the latest extension to ABNF. I was obsessed with ABNF for a long time because I was just looking for a way to specify grammars and those are the only ways until I made Pagan. So the hope is that Pagan's language itself can be more explicit, better performing and readable replacement for grammar, meta languages, and inline regular expressions. Code generators producing parses of different types and different implementation languages can be created from the same grammar specification expressed in Pagan. In other words, you can take a Pagan thing and you can run it through a code generator and generate Ruby code. You could generate Rust code. You could generate Perl code. You could generate C code. It doesn't matter because there's nothing specific, language specific in Pagan and that's the biggest selling point of all of the whole thing. Pagan parses and center libraries can even provide highly optimized handling of Pagan grammars, including directly in code as strings and constants, much like compiled regular expressions are handled today, but with much greater clarity and efficiency. So for example, when you use a regular expression, you have the parenthesized list, but it's a linked list. It's not a structure. It does kind of make one, but you just get an array out of it. You have to know the number and you have to figure out where your parentheses are inside of this other thing to see whether it's number one or two because you have to unpack the parentheses in your brain in order to understand what integer it becomes in terms of index. And with Pagan, you don't have to do that because you get an AST out of it. You get a standardized AST out of it every time. You always get the same one and that is a structure. It's a full structure that you can walk however you want and do it. So that, in my opinion, might be slightly slower than regular expressions, but it gives you more power when you're dealing with grammars and specifying them. A progressive best example is what is itself, which is specified in Pagan. Here's another example, the JSON specification in Pagan. So Pagan is itself specified in Pagan and this is an old one. Yeah, this is, that link is broken. I got to fix that, obviously. I haven't picked up Pagan for a long time. I've used it for lots of things, but this site needs to be fixed. So here's the JSON RFC according to RCA 259. This is the implementation of JSON in Pagan. And you have the overall grammar, grammar is a conventional name for the top level node. And then you have the white space and then a value or whatever and then more white space. And I use the actual terminology from the thing. There are syntax conventions for the name. So if they're initial caps, those are kind of easily to identify. The other couple of things when you have two, I don't want to teach you all Pagan right now, take forever, but two dashes, this is pretty significant. The two dashes signifies a substantial node or a semantic, what I call a semantic node. That means capture it. If we don't have that, it's just a simplification of code so that, you know, the spec of the Pagan spec code can become simpler. So anytime you see value, value is all of these things, right? But this says don't make value a node. It just says this is how we refer to value so I can put value down here later. And then, but any of these things would be a node. If there's a node, it would be implemented at that point, it would be captured. So an object is a significant node and it would, you know, it's just, I mean, it's basically the same as peg. You get a bracket and then zero or more white spaces and then a member and then you go down and what's a member? A member is a string plus a colon plus a value and then you get, you know, zero or more of this, you know, sort of thing combined together. In that sense, if you know regular expressions, you can see this. One of the things that's nice about this is it is a lot easier to read than regular expressions. We do have an entire standard subset of predefined tokens like DQ and everything, which APNF has also done so you can just use those if you already know them. In fact, anything that's lowercase is considered a class and some of the classes can be used without specifying them because they're assumed there to be coming from the, the peg and specification itself. So they're predefined such as WS. And that includes every positive span range and things like that, digit and stuff. And so, yeah, so digit is another class. Actually, I think digit is a token, you know, there's differences between classes and tokens. Class is like one of these set and the other one, a specific token is like this exact single character or, you know, three characters or something like that. And so, the other thing that's cool about peg and peg is that they're left to right versus ABNF and EBNF, which are like, when you specify something with a slash and EBNF and EBNF, it can be any of those things at any order. And the really, really, really great simplification of peg is it's guaranteed to match first left wins. So you put the stuff to the left. So for example, in my specification for, for Kegumel, you know, wait, let me go see you. Where is my spec? Is it over here? Okay. So in my specification for Kegumel, let's, let's do the, let's do the, the, you see here I have bulleted and then numbered and then figure, right? Well, if I was going to do those bulleted, right, a bulleted list begins, I'm not bulleted, let's, let's do, let's do spans, spans are better. So let's do strong emphasis. The reason strong emphasis first, even though you might not want to list it that way is because strong emphasis for me is three stars plus, you know, something and that this is the part that's kind of interesting. So right here, we're going to put a plane and so then we can put a plane. So it's a, you know, it's a plane span and followed by another token, right? And so, so there, the reason that's the strongest are that the reason that that one comes first, of course, is because the strong one has two and the emphasis one, which is italic is just one. Now, what if I put, so if I was trying to specify this in EB enough, you see how problematic this would be? By just placing the order to check for each one, I can have tokens that would otherwise be included in the other tokens. This is where you get all the craziness with XML and everything because they don't have the idea of, well, first look for this, and I can't tell you how valuable this is when it comes to code generation or just writing the code by hand, because just by looking at the peg and I know right away what comes first, I know that I need to write, I need to check for a strong emphasis token before I check for an emphasis token so that I can fail out or, you know, go to or hand off to the next, to the next parser, the next token parser, because I have the order and I know, I keep talking about this, but that is such a huge thing when it comes to writing parsers, because otherwise you just don't know. And it's really nice because it keeps the syntax of Kegamel or Bonzai. Very simple. I'm a huge fan of just one way to do things in a markdown language like this. And so I just use stars all the time, and even though you can use underscores in any number of combinations, infinite number of combinations, which to represent in a grammar would be crazy hard, not to mention putting an unnecessary amount of overload on a parser. And if you're going to make a specification for something that's going to potentially be capturing all the knowledge of the world. We want to be optimized in our parsing. And so this is one of those cases where we just want one best way to do it. Other grammars would nest them. Lots of grammars would nest these, and you can represent these as nested, but I do not ever want to do that. And neither does medium. Medium does not allow you to nest your grammar, to nest your emphasis. You cannot have italics in something that's already been bolded. And if you've ever played around with any kind of syntax highlighters or Vim plugins or anything like that, you have experienced a broken emphasis highlighter, something that gets it wrong because it wasn't well specified, or it thinks it's got a span when it really doesn't because it's on a different line. Or I'm guarantee you, if you spend any amount of time doing any kind of syntax highlighting stuff, you will find something that's really broken. And it's really too bad because of that. So for my particular grammar, and this is one of the reasons I made Pagan, I wanted to be able to very precisely specify, I wanted to be very precise in my specification of the grammar, and to keep it deliberately simple. And people might accuse me of keeping it too simple, and they're like, why is it simple? I was like, because I don't need more. But it's not as simple as Godoc, for example. Godoc doesn't have anything. Godoc is even worse. It's just text. And you can indent something by four spaces to get it to be the same. And headers are lines by themselves. So there is a grammar there. It's just not specified. You have to go read the Go source code to understand what the grammar is. This is why I made Pagan. At least when somebody comes to me and says, OK, I said, well, what's Kegelman, I don't want to have to learn another thing. I said, you probably already know it. I was like, how do you know it? Well, do you know Markdown? Yeah. So it's a simplified version of Pandoc Markdown, which means it's got semantic div brakes and it's got math support, which GitHub now has. That's what Pagan is. Pagan is a way to specify a language. You can go read more about it if you want to get into the details. I feel like I've gone too far in the details already. I do think it's important that I mentioned that the grammar, the Pagan itself does specify the AST to be used. I am seriously considering a revamp of this AST at some point. So the format for the AST, the text version of the AST is also written in Pagan. You can go read that if you want, or you can actually look at the long form. I'm trying to find an example. So here is a long form example of the JSON AST. So this is the AST of the JSON file itself, but in compact form, it's actually extremely compact. It's the smallest text-based JSON compatible, human readable AST that you can get, and it can be easily expanded. So that was by design. When I looked at the native Haskell, this was all inspired by Pandoc. Pandoc's JSON AST is abysmally bad. I mean, let me just show you. I talked about this earlier, but I'm going to talk about it again. I just love salmon on this particular thing. So when you do Pandoc, I mean, they didn't have as much information as they do now. It's too bad because they can't go back and redo it. But I don't know if you can see this, but so they have strings here, right? They have spaces, like a space, meaning white space, and then Kegamel. And they have, first of all, they have way too many, too much, like two verbose rich thing. But what I really, really don't like is when they have strings, I'm trying to find one. There's no one there. Okay. So here, this right here, string C documentation slash docs. What is that? Oh, this is a link. Okay. So we have a link and the properties of the link are, and it's got all the properties in order, so they're not named. And what I'm trying to show you is that they, when they have strings, the strings include quotes on them and stuff like that. So let me see if I can find another version. So if I have, I don't know, let me see if I can find one, pandoc-tjson, read me. Did I just go to look at the same one again? I did. And it's in the spec, ml spec. All right. So let's go, let's look at this, read me. Okay. So look at this one. So you know, you have the links here, you have the types, you have a soft break and it, it deliberately uses long words. There's no way to make it shorter. So the, the textual ASTs that are generated from pandoc are so fucking long that, and the standard instruction from pandoc, if you want to write your own conversions is to convert to the JSON AST and, and then walk the, the JSON AST and parse that and produce your other thing. And I guarantee you, you, you should see how long it takes to do that. Because this thing is a monstrously big waste and it also gets tons of stuff wrong. So for example, it, it deliberately lumps together, uh, quoted content as a string. So for example, or a parenthesized word as a string, it doesn't consider the idea of fields and then words and then things like that. It's, it's so, it's just a mess. And that's, I, I, look, I really, really appreciate that we have pandoc. I don't want it to be misunderstood. I think it's so great that we have it, uh, JQ. I could pipe it through JQ, but yeah, uh, Jeff needs some love. Yeah, it does. So, and they could actually make a different representation. You could probably do a Haskell, uh, you know, PR and, and do that. But the, the, the, yeah, they probably prefer Haskell reformos. I agree. But the, the, the point is, is that, I mean, if you're going to tell somebody that you're a standard way of supporting other conversion method out methods is to, and by the way, you can make your own. You can write your own Lua plugins that will render the whole entire thing. It's beautiful Haskell. If you get into the functional program, if you, if you want a really good example of when functional programming really shines, uh, you know, pandoc is a good example of that because it was Haskell is like Taylor made for parsing syntaxes and grammars and stuff. It's just so perfect for it. And, and, you know, I imagine, uh, you know, Lisper or Lang or something, uh, would be just as good. Uh, but, but that's, you know, that's my experience with Haskell too. Um, and, you know, it's super fast, but, but, I mean, it, it does lack a lot in the, in the, you know, the, the, to me, the AST is really core because if you want to have a, the ability to make a conversion from one thing into anything else, uh, it's so important that you get that AST, right? Because then everybody else can render it however they want to. Well, you've just created a data model for the thing that you're representing, whatever it is, a document or whatever. And, and, and that's really core of the whole thing. And the people who made pandoc, of course, are not web people. They are, you know, publishers and Haskell people and academics and they're over at Berkeley and, you know, JGM's an amazing spearheaded common mark and a bunch of other amazing things. And I, I've had some interactions with JGM. JGM is completely 100% unimpressed with anything I've ever done, which is fine because, but I'm doing things that I need, uh, and yeah, including Peg. And I, I ran Peg and by him and he was like, why am I, well, you know, why Peg? And he doesn't, he's not interested. He just, he got his answer. He's a philosophy major and he's a great guy, but, uh, a philosophy professor. So, um, anyway, here's the parent nodes, uh, given the following data example, you get this copyright and, and, um, yeah, here you get another AST. No, no AST node attributes. Summary node 2 data structure models, a lot for attributes. Peg and Stacy does not, since attributes can more, uh, efficiently and precisely indicated by adding an additional parameter parent or terminal type, which is another word for leaf node. Uh, Peg was conceived originally developed by me. Uh, I'm working with tokenizers and text generation. Oh, nice. Uh, while creating grammars, uh, was needed for artifacts knowledge net. That's keg, uh, easy mark, data mark, mate, man, I can update this. These are all of the different, uh, base QL, all of the ones that I've been doing. Uh, live coding streams. Others to update and contribute it and help this is, that's, I got to update them. Uh, related tools to Vimplug and Emacs. I don't have them in plugin done yet. I need to other efforts out there. You can go look at peg leg. That's the one I helped contribute to go peg. It's the one I contributed to actually space on peg. Look, uh, the Python ones, the Guido one that, that they added 3.9 and Python, I think, uh, Pigeon Pigeon pest.rs, which I was so excited about. Turned out it's crap and antler antler has been his Java is all about Java. But it's, it's exactly in the same sort of space, uh, for defining grammars and stuff, but it's Java only. Um, from a peg a grammar specifications, blah, blah, blah, go read that. Uh, linking documentation with definitions, uh, legal considerations, what adoption is blah, mime type. I'm, I'm hoping to get X dash Pagan from my type, but I haven't submitted that yet. Trademarks, uh, blah, blah, licensing, it's Apache tube, uh, attribution, patents, uh, contributing, IFC, or you can't see the race. So I would like to get this eventually submitted, but I don't want, I want to use it for like a year or two and have all the tools done for it before I do that. Um, Pagan has, Pagan parsering has been used in production at at least one company that I know of besides mine, uh, for over a year and a half at this point. And so if you're wondering whether it's worthy of abuse and there has been, uh, some tweaking to the specification in that time based on, uh, you know, the very intense usage that's happening over in that other company. Um, so, so it's out there. You can go use it if you want. You can play around with it. If you ever want to write your own language, you probably can. I, I may very well do, uh, write a book about Pagan. Uh, I'm, I'm all about writing books these days. Um, I got to finish the terminal velocity. That's the book I'm working on right now. And after that, uh, you know, these other books in spaces that are not covered by anything, uh, will be all right. I mean, write a book about bonsai, write a book about Pagan and, uh, some of these other things so that people can take them and use them. So that's all I have to say about that.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.6, "text": " So, what is Pagan and why use it?", "tokens": [50364, 407, 11, 437, 307, 430, 14167, 293, 983, 764, 309, 30, 50544], "temperature": 0.0, "avg_logprob": -0.20868687947591147, "compression_ratio": 1.8678571428571429, "no_speech_prob": 0.05417265743017197}, {"id": 1, "seek": 0, "start": 3.6, "end": 7.96, "text": " So Pagan, I've talked about this a lot, but I'm going to go through and review it again.", "tokens": [50544, 407, 430, 14167, 11, 286, 600, 2825, 466, 341, 257, 688, 11, 457, 286, 478, 516, 281, 352, 807, 293, 3131, 309, 797, 13, 50762], "temperature": 0.0, "avg_logprob": -0.20868687947591147, "compression_ratio": 1.8678571428571429, "no_speech_prob": 0.05417265743017197}, {"id": 2, "seek": 0, "start": 7.96, "end": 14.120000000000001, "text": " So Pagan is a language that I wrote, you can go to pagan.dev, I think I still have that", "tokens": [50762, 407, 430, 14167, 307, 257, 2856, 300, 286, 4114, 11, 291, 393, 352, 281, 38238, 13, 40343, 11, 286, 519, 286, 920, 362, 300, 51070], "temperature": 0.0, "avg_logprob": -0.20868687947591147, "compression_ratio": 1.8678571428571429, "no_speech_prob": 0.05417265743017197}, {"id": 3, "seek": 0, "start": 14.120000000000001, "end": 15.120000000000001, "text": " up there.", "tokens": [51070, 493, 456, 13, 51120], "temperature": 0.0, "avg_logprob": -0.20868687947591147, "compression_ratio": 1.8678571428571429, "no_speech_prob": 0.05417265743017197}, {"id": 4, "seek": 0, "start": 15.120000000000001, "end": 19.88, "text": " I've been using it a lot because I'm using Pagan to write KML.", "tokens": [51120, 286, 600, 668, 1228, 309, 257, 688, 570, 286, 478, 1228, 430, 14167, 281, 2464, 591, 44, 43, 13, 51358], "temperature": 0.0, "avg_logprob": -0.20868687947591147, "compression_ratio": 1.8678571428571429, "no_speech_prob": 0.05417265743017197}, {"id": 5, "seek": 0, "start": 19.88, "end": 23.76, "text": " Sometimes you need a language to write a language, it's called a meta language, and that's what", "tokens": [51358, 4803, 291, 643, 257, 2856, 281, 2464, 257, 2856, 11, 309, 311, 1219, 257, 19616, 2856, 11, 293, 300, 311, 437, 51552], "temperature": 0.0, "avg_logprob": -0.20868687947591147, "compression_ratio": 1.8678571428571429, "no_speech_prob": 0.05417265743017197}, {"id": 6, "seek": 0, "start": 23.76, "end": 25.16, "text": " Pagan is, you can go here and read about it.", "tokens": [51552, 430, 14167, 307, 11, 291, 393, 352, 510, 293, 1401, 466, 309, 13, 51622], "temperature": 0.0, "avg_logprob": -0.20868687947591147, "compression_ratio": 1.8678571428571429, "no_speech_prob": 0.05417265743017197}, {"id": 7, "seek": 0, "start": 25.16, "end": 29.14, "text": " So Pagan is a language for defining languages, more precisely as universal notation for expressing", "tokens": [51622, 407, 430, 14167, 307, 257, 2856, 337, 17827, 8650, 11, 544, 13402, 382, 11455, 24657, 337, 22171, 51821], "temperature": 0.0, "avg_logprob": -0.20868687947591147, "compression_ratio": 1.8678571428571429, "no_speech_prob": 0.05417265743017197}, {"id": 8, "seek": 2914, "start": 29.14, "end": 33.620000000000005, "text": " any grammar, including natural language, which I'm very proud of, in a way that is easy", "tokens": [50364, 604, 22317, 11, 3009, 3303, 2856, 11, 597, 286, 478, 588, 4570, 295, 11, 294, 257, 636, 300, 307, 1858, 50588], "temperature": 0.0, "avg_logprob": -0.205988400777181, "compression_ratio": 1.704968944099379, "no_speech_prob": 0.014498121105134487}, {"id": 9, "seek": 2914, "start": 33.620000000000005, "end": 37.1, "text": " to parse cognitively and programmatically without any specific application or implementation", "tokens": [50588, 281, 48377, 15605, 356, 293, 37648, 5030, 1553, 604, 2685, 3861, 420, 11420, 50762], "temperature": 0.0, "avg_logprob": -0.205988400777181, "compression_ratio": 1.704968944099379, "no_speech_prob": 0.014498121105134487}, {"id": 10, "seek": 2914, "start": 37.1, "end": 38.42, "text": " in mind.", "tokens": [50762, 294, 1575, 13, 50828], "temperature": 0.0, "avg_logprob": -0.205988400777181, "compression_ratio": 1.704968944099379, "no_speech_prob": 0.014498121105134487}, {"id": 11, "seek": 2914, "start": 38.42, "end": 44.82, "text": " And this is the problem with Pagan, which is from Brian Ford, which I'll get to.", "tokens": [50828, 400, 341, 307, 264, 1154, 365, 430, 14167, 11, 597, 307, 490, 10765, 11961, 11, 597, 286, 603, 483, 281, 13, 51148], "temperature": 0.0, "avg_logprob": -0.205988400777181, "compression_ratio": 1.704968944099379, "no_speech_prob": 0.014498121105134487}, {"id": 12, "seek": 2914, "start": 44.82, "end": 50.3, "text": " Pagan builds on the best of the existing meta data structures such as Pagan, ABNF, EBNF,", "tokens": [51148, 430, 14167, 15182, 322, 264, 1151, 295, 264, 6741, 19616, 1412, 9227, 1270, 382, 430, 14167, 11, 13838, 45, 37, 11, 462, 32006, 37, 11, 51422], "temperature": 0.0, "avg_logprob": -0.205988400777181, "compression_ratio": 1.704968944099379, "no_speech_prob": 0.014498121105134487}, {"id": 13, "seek": 2914, "start": 50.3, "end": 51.3, "text": " and JSON.", "tokens": [51422, 293, 31828, 13, 51472], "temperature": 0.0, "avg_logprob": -0.205988400777181, "compression_ratio": 1.704968944099379, "no_speech_prob": 0.014498121105134487}, {"id": 14, "seek": 2914, "start": 51.3, "end": 54.46, "text": " If you've ever read the GoLang specification, it's all an EBNF, it's very common for people", "tokens": [51472, 759, 291, 600, 1562, 1401, 264, 1037, 43, 656, 31256, 11, 309, 311, 439, 364, 462, 32006, 37, 11, 309, 311, 588, 2689, 337, 561, 51630], "temperature": 0.0, "avg_logprob": -0.205988400777181, "compression_ratio": 1.704968944099379, "no_speech_prob": 0.014498121105134487}, {"id": 15, "seek": 2914, "start": 54.46, "end": 58.980000000000004, "text": " to write language specifications, an EBNF, it's more common to see ABNF for things like", "tokens": [51630, 281, 2464, 2856, 29448, 11, 364, 462, 32006, 37, 11, 309, 311, 544, 2689, 281, 536, 13838, 45, 37, 337, 721, 411, 51856], "temperature": 0.0, "avg_logprob": -0.205988400777181, "compression_ratio": 1.704968944099379, "no_speech_prob": 0.014498121105134487}, {"id": 16, "seek": 5898, "start": 59.82, "end": 64.1, "text": " Internet IRFCs and IRFCs, things like that.", "tokens": [50406, 7703, 16486, 18671, 82, 293, 16486, 18671, 82, 11, 721, 411, 300, 13, 50620], "temperature": 0.0, "avg_logprob": -0.24122970581054687, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.029300197958946228}, {"id": 17, "seek": 5898, "start": 64.1, "end": 68.7, "text": " But ABNF, you can't even use characters in it, you have to use hexadecimal notation for", "tokens": [50620, 583, 13838, 45, 37, 11, 291, 393, 380, 754, 764, 4342, 294, 309, 11, 291, 362, 281, 764, 23291, 762, 66, 10650, 24657, 337, 50850], "temperature": 0.0, "avg_logprob": -0.24122970581054687, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.029300197958946228}, {"id": 18, "seek": 5898, "start": 68.7, "end": 70.74, "text": " everything, just because it's so precise.", "tokens": [50850, 1203, 11, 445, 570, 309, 311, 370, 13600, 13, 50952], "temperature": 0.0, "avg_logprob": -0.24122970581054687, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.029300197958946228}, {"id": 19, "seek": 5898, "start": 70.74, "end": 73.7, "text": " It's like, how do I define the HTTP protocol?", "tokens": [50952, 467, 311, 411, 11, 577, 360, 286, 6964, 264, 33283, 10336, 30, 51100], "temperature": 0.0, "avg_logprob": -0.24122970581054687, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.029300197958946228}, {"id": 20, "seek": 5898, "start": 73.7, "end": 75.1, "text": " Go read ABNF for that.", "tokens": [51100, 1037, 1401, 13838, 45, 37, 337, 300, 13, 51170], "temperature": 0.0, "avg_logprob": -0.24122970581054687, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.029300197958946228}, {"id": 21, "seek": 5898, "start": 75.1, "end": 80.42, "text": " Now, ABNF allows ranges and stuff like that, Pagan does not do that, Pagan is a very, very", "tokens": [51170, 823, 11, 13838, 45, 37, 4045, 22526, 293, 1507, 411, 300, 11, 430, 14167, 775, 406, 360, 300, 11, 430, 14167, 307, 257, 588, 11, 588, 51436], "temperature": 0.0, "avg_logprob": -0.24122970581054687, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.029300197958946228}, {"id": 22, "seek": 5898, "start": 80.42, "end": 88.38, "text": " friendly kind of flying in the face of traditional parsing mentality, primarily by saying we", "tokens": [51436, 9208, 733, 295, 7137, 294, 264, 1851, 295, 5164, 21156, 278, 21976, 11, 10029, 538, 1566, 321, 51834], "temperature": 0.0, "avg_logprob": -0.24122970581054687, "compression_ratio": 1.5895522388059702, "no_speech_prob": 0.029300197958946228}, {"id": 23, "seek": 8838, "start": 88.38, "end": 93.66, "text": " have infinite memory in our assumption, our base assumption, rather than saying we only", "tokens": [50364, 362, 13785, 4675, 294, 527, 15302, 11, 527, 3096, 15302, 11, 2831, 813, 1566, 321, 787, 50628], "temperature": 0.0, "avg_logprob": -0.14740755918214646, "compression_ratio": 1.698360655737705, "no_speech_prob": 0.020326072350144386}, {"id": 24, "seek": 8838, "start": 93.66, "end": 97.02, "text": " have one byte of memory, which is what all the other parsers do.", "tokens": [50628, 362, 472, 40846, 295, 4675, 11, 597, 307, 437, 439, 264, 661, 21156, 433, 360, 13, 50796], "temperature": 0.0, "avg_logprob": -0.14740755918214646, "compression_ratio": 1.698360655737705, "no_speech_prob": 0.020326072350144386}, {"id": 25, "seek": 8838, "start": 97.02, "end": 101.69999999999999, "text": " I am absolutely obsessed with parsing, I think it's so fun.", "tokens": [50796, 286, 669, 3122, 16923, 365, 21156, 278, 11, 286, 519, 309, 311, 370, 1019, 13, 51030], "temperature": 0.0, "avg_logprob": -0.14740755918214646, "compression_ratio": 1.698360655737705, "no_speech_prob": 0.020326072350144386}, {"id": 26, "seek": 8838, "start": 101.69999999999999, "end": 105.78, "text": " If I was going to have done a computer science thing, I would have really, really obsessed", "tokens": [51030, 759, 286, 390, 516, 281, 362, 1096, 257, 3820, 3497, 551, 11, 286, 576, 362, 534, 11, 534, 16923, 51234], "temperature": 0.0, "avg_logprob": -0.14740755918214646, "compression_ratio": 1.698360655737705, "no_speech_prob": 0.020326072350144386}, {"id": 27, "seek": 8838, "start": 105.78, "end": 106.78, "text": " about parsing.", "tokens": [51234, 466, 21156, 278, 13, 51284], "temperature": 0.0, "avg_logprob": -0.14740755918214646, "compression_ratio": 1.698360655737705, "no_speech_prob": 0.020326072350144386}, {"id": 28, "seek": 8838, "start": 106.78, "end": 111.25999999999999, "text": " And I've already written in Pagan parser some time ago, a couple years back, and we've", "tokens": [51284, 400, 286, 600, 1217, 3720, 294, 430, 14167, 21156, 260, 512, 565, 2057, 11, 257, 1916, 924, 646, 11, 293, 321, 600, 51508], "temperature": 0.0, "avg_logprob": -0.14740755918214646, "compression_ratio": 1.698360655737705, "no_speech_prob": 0.020326072350144386}, {"id": 29, "seek": 8838, "start": 111.25999999999999, "end": 116.14, "text": " had lots of people use Pagan, Quint comes to mind, Quint, if you're out there, thank", "tokens": [51508, 632, 3195, 295, 561, 764, 430, 14167, 11, 2326, 686, 1487, 281, 1575, 11, 2326, 686, 11, 498, 291, 434, 484, 456, 11, 1309, 51752], "temperature": 0.0, "avg_logprob": -0.14740755918214646, "compression_ratio": 1.698360655737705, "no_speech_prob": 0.020326072350144386}, {"id": 30, "seek": 8838, "start": 116.14, "end": 117.14, "text": " you for your contributions.", "tokens": [51752, 291, 337, 428, 15725, 13, 51802], "temperature": 0.0, "avg_logprob": -0.14740755918214646, "compression_ratio": 1.698360655737705, "no_speech_prob": 0.020326072350144386}, {"id": 31, "seek": 11714, "start": 117.26, "end": 122.62, "text": " Quint has actually got his Pagan parser that we kind of collaborated on running in production", "tokens": [50370, 2326, 686, 575, 767, 658, 702, 430, 14167, 21156, 260, 300, 321, 733, 295, 42463, 322, 2614, 294, 4265, 50638], "temperature": 0.0, "avg_logprob": -0.186292200503142, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.1258486807346344}, {"id": 32, "seek": 11714, "start": 122.62, "end": 128.5, "text": " for a rather large company that I won't say.", "tokens": [50638, 337, 257, 2831, 2416, 2237, 300, 286, 1582, 380, 584, 13, 50932], "temperature": 0.0, "avg_logprob": -0.186292200503142, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.1258486807346344}, {"id": 33, "seek": 11714, "start": 128.5, "end": 132.02, "text": " But it is a way of specifying languages, and so what does that actually mean?", "tokens": [50932, 583, 309, 307, 257, 636, 295, 1608, 5489, 8650, 11, 293, 370, 437, 775, 300, 767, 914, 30, 51108], "temperature": 0.0, "avg_logprob": -0.186292200503142, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.1258486807346344}, {"id": 34, "seek": 11714, "start": 132.02, "end": 135.54, "text": " So the motivation here, let me just keep reading here, and then I'll close this up.", "tokens": [51108, 407, 264, 12335, 510, 11, 718, 385, 445, 1066, 3760, 510, 11, 293, 550, 286, 603, 1998, 341, 493, 13, 51284], "temperature": 0.0, "avg_logprob": -0.186292200503142, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.1258486807346344}, {"id": 35, "seek": 11714, "start": 135.54, "end": 139.66, "text": " So the motivation, technology increases complexity, the need for a better human-computer interaction", "tokens": [51284, 407, 264, 12335, 11, 2899, 8637, 14024, 11, 264, 643, 337, 257, 1101, 1952, 12, 1112, 13849, 9285, 51490], "temperature": 0.0, "avg_logprob": -0.186292200503142, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.1258486807346344}, {"id": 36, "seek": 11714, "start": 139.66, "end": 144.14, "text": " becomes more pronounced, creating language grammars quickly and simply has become a critical", "tokens": [51490, 3643, 544, 23155, 11, 4084, 2856, 17570, 685, 2661, 293, 2935, 575, 1813, 257, 4924, 51714], "temperature": 0.0, "avg_logprob": -0.186292200503142, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.1258486807346344}, {"id": 37, "seek": 14414, "start": 144.14, "end": 145.14, "text": " need.", "tokens": [50364, 643, 13, 50414], "temperature": 0.0, "avg_logprob": -0.16435616307979006, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.3916948437690735}, {"id": 38, "seek": 14414, "start": 145.14, "end": 149.1, "text": " And there's something that I have found out recently has got the name Fluid APIs, and", "tokens": [50414, 400, 456, 311, 746, 300, 286, 362, 1352, 484, 3938, 575, 658, 264, 1315, 33612, 327, 21445, 11, 293, 50612], "temperature": 0.0, "avg_logprob": -0.16435616307979006, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.3916948437690735}, {"id": 39, "seek": 14414, "start": 149.1, "end": 150.85999999999999, "text": " that's why I made Bonsai.", "tokens": [50612, 300, 311, 983, 286, 1027, 363, 892, 1301, 13, 50700], "temperature": 0.0, "avg_logprob": -0.16435616307979006, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.3916948437690735}, {"id": 40, "seek": 14414, "start": 150.85999999999999, "end": 155.5, "text": " So Bonsai defines a natural language API that could be spoken on the command line as opposed", "tokens": [50700, 407, 363, 892, 1301, 23122, 257, 3303, 2856, 9362, 300, 727, 312, 10759, 322, 264, 5622, 1622, 382, 8851, 50932], "temperature": 0.0, "avg_logprob": -0.16435616307979006, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.3916948437690735}, {"id": 41, "seek": 14414, "start": 155.5, "end": 160.33999999999997, "text": " to demanding dashes and get off and all that other crap, which is another project I made", "tokens": [50932, 281, 19960, 8240, 279, 293, 483, 766, 293, 439, 300, 661, 12426, 11, 597, 307, 1071, 1716, 286, 1027, 51174], "temperature": 0.0, "avg_logprob": -0.16435616307979006, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.3916948437690735}, {"id": 42, "seek": 14414, "start": 160.33999999999997, "end": 161.33999999999997, "text": " related to this.", "tokens": [51174, 4077, 281, 341, 13, 51224], "temperature": 0.0, "avg_logprob": -0.16435616307979006, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.3916948437690735}, {"id": 43, "seek": 14414, "start": 161.33999999999997, "end": 165.94, "text": " But Pagan is somewhat related to the same need, because we need a way to define domain-specific", "tokens": [51224, 583, 430, 14167, 307, 8344, 4077, 281, 264, 912, 643, 11, 570, 321, 643, 257, 636, 281, 6964, 9274, 12, 29258, 51454], "temperature": 0.0, "avg_logprob": -0.16435616307979006, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.3916948437690735}, {"id": 44, "seek": 14414, "start": 165.94, "end": 167.54, "text": " languages very quickly.", "tokens": [51454, 8650, 588, 2661, 13, 51534], "temperature": 0.0, "avg_logprob": -0.16435616307979006, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.3916948437690735}, {"id": 45, "seek": 14414, "start": 167.54, "end": 171.5, "text": " The way that I learned about Pagan, quite frankly, yet again, TJ Hollowaycheck, I was", "tokens": [51534, 440, 636, 300, 286, 3264, 466, 430, 14167, 11, 1596, 11939, 11, 1939, 797, 11, 46402, 46731, 320, 15723, 11, 286, 390, 51732], "temperature": 0.0, "avg_logprob": -0.16435616307979006, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.3916948437690735}, {"id": 46, "seek": 17150, "start": 171.5, "end": 177.34, "text": " following him and he wrote his own domain-specific language for parsing and querying logs for", "tokens": [50364, 3480, 796, 293, 415, 4114, 702, 1065, 9274, 12, 29258, 2856, 337, 21156, 278, 293, 7083, 1840, 20820, 337, 50656], "temperature": 0.0, "avg_logprob": -0.1920897509600665, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.04741838201880455}, {"id": 47, "seek": 17150, "start": 177.34, "end": 182.58, "text": " his APEX, one of his projects on his APEX company, and he used Pagan for that.", "tokens": [50656, 702, 316, 5208, 55, 11, 472, 295, 702, 4455, 322, 702, 316, 5208, 55, 2237, 11, 293, 415, 1143, 430, 14167, 337, 300, 13, 50918], "temperature": 0.0, "avg_logprob": -0.1920897509600665, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.04741838201880455}, {"id": 48, "seek": 17150, "start": 182.58, "end": 188.58, "text": " And so I found Pagan, I actually helped contribute to one of the GoPeg parsers that's out there,", "tokens": [50918, 400, 370, 286, 1352, 430, 14167, 11, 286, 767, 4254, 10586, 281, 472, 295, 264, 1037, 47, 1146, 21156, 433, 300, 311, 484, 456, 11, 51218], "temperature": 0.0, "avg_logprob": -0.1920897509600665, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.04741838201880455}, {"id": 49, "seek": 17150, "start": 188.58, "end": 191.58, "text": " and then I realized that it didn't have enough, and I needed to add a little bit more, and", "tokens": [51218, 293, 550, 286, 5334, 300, 309, 994, 380, 362, 1547, 11, 293, 286, 2978, 281, 909, 257, 707, 857, 544, 11, 293, 51368], "temperature": 0.0, "avg_logprob": -0.1920897509600665, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.04741838201880455}, {"id": 50, "seek": 17150, "start": 191.58, "end": 193.62, "text": " so I made Pagan.", "tokens": [51368, 370, 286, 1027, 430, 14167, 13, 51470], "temperature": 0.0, "avg_logprob": -0.1920897509600665, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.04741838201880455}, {"id": 51, "seek": 17150, "start": 193.62, "end": 196.98, "text": " So Pagan has decided to keep things by allowing any data to be represented as a grammar and", "tokens": [51470, 407, 430, 14167, 575, 3047, 281, 1066, 721, 538, 8293, 604, 1412, 281, 312, 10379, 382, 257, 22317, 293, 51638], "temperature": 0.0, "avg_logprob": -0.1920897509600665, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.04741838201880455}, {"id": 52, "seek": 17150, "start": 196.98, "end": 200.9, "text": " breaking it down into universal data form, it can be composed, combined, and analyzed", "tokens": [51638, 7697, 309, 760, 666, 11455, 1412, 1254, 11, 309, 393, 312, 18204, 11, 9354, 11, 293, 28181, 51834], "temperature": 0.0, "avg_logprob": -0.1920897509600665, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.04741838201880455}, {"id": 53, "seek": 20090, "start": 200.9, "end": 201.9, "text": " in a remarkable way.", "tokens": [50364, 294, 257, 12802, 636, 13, 50414], "temperature": 0.0, "avg_logprob": -0.20602897706070566, "compression_ratio": 1.6645569620253164, "no_speech_prob": 0.048834577202796936}, {"id": 54, "seek": 20090, "start": 201.9, "end": 208.9, "text": " So my eventual dream is to allow Pagan notation to replace regular expressions.", "tokens": [50414, 407, 452, 33160, 3055, 307, 281, 2089, 430, 14167, 24657, 281, 7406, 3890, 15277, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20602897706070566, "compression_ratio": 1.6645569620253164, "no_speech_prob": 0.048834577202796936}, {"id": 55, "seek": 20090, "start": 208.9, "end": 214.38, "text": " Regular expressions are nice and all, but when it comes to structured data and the ASTs", "tokens": [50764, 45659, 15277, 366, 1481, 293, 439, 11, 457, 562, 309, 1487, 281, 18519, 1412, 293, 264, 316, 6840, 82, 51038], "temperature": 0.0, "avg_logprob": -0.20602897706070566, "compression_ratio": 1.6645569620253164, "no_speech_prob": 0.048834577202796936}, {"id": 56, "seek": 20090, "start": 214.38, "end": 216.38, "text": " that are produced by it, it was really, really problematic.", "tokens": [51038, 300, 366, 7126, 538, 309, 11, 309, 390, 534, 11, 534, 19011, 13, 51138], "temperature": 0.0, "avg_logprob": -0.20602897706070566, "compression_ratio": 1.6645569620253164, "no_speech_prob": 0.048834577202796936}, {"id": 57, "seek": 20090, "start": 216.38, "end": 219.14000000000001, "text": " Whether it be simple counting all the words in a document, creating a simple query language", "tokens": [51138, 8503, 309, 312, 2199, 13251, 439, 264, 2283, 294, 257, 4166, 11, 4084, 257, 2199, 14581, 2856, 51276], "temperature": 0.0, "avg_logprob": -0.20602897706070566, "compression_ratio": 1.6645569620253164, "no_speech_prob": 0.048834577202796936}, {"id": 58, "seek": 20090, "start": 219.14000000000001, "end": 224.06, "text": " to make searching logs easier, coding a human-friendly interface to an otherwise complicated web", "tokens": [51276, 281, 652, 10808, 20820, 3571, 11, 17720, 257, 1952, 12, 22864, 9226, 281, 364, 5911, 6179, 3670, 51522], "temperature": 0.0, "avg_logprob": -0.20602897706070566, "compression_ratio": 1.6645569620253164, "no_speech_prob": 0.048834577202796936}, {"id": 59, "seek": 20090, "start": 224.06, "end": 230.02, "text": " API, simplifying the parsing of a form of common markup, implementing a fully-programmed", "tokens": [51522, 9362, 11, 6883, 5489, 264, 21156, 278, 295, 257, 1254, 295, 2689, 1491, 1010, 11, 18114, 257, 4498, 12, 32726, 1912, 51820], "temperature": 0.0, "avg_logprob": -0.20602897706070566, "compression_ratio": 1.6645569620253164, "no_speech_prob": 0.048834577202796936}, {"id": 60, "seek": 23002, "start": 230.02, "end": 233.9, "text": " language that leverages LVM to quickly create high-performance compilers, or developing a", "tokens": [50364, 2856, 300, 12451, 1660, 441, 53, 44, 281, 2661, 1884, 1090, 12, 50242, 715, 388, 433, 11, 420, 6416, 257, 50558], "temperature": 0.0, "avg_logprob": -0.22175979614257812, "compression_ratio": 1.7378378378378379, "no_speech_prob": 0.45273080468177795}, {"id": 61, "seek": 23002, "start": 233.9, "end": 237.34, "text": " binary language for moisture evaporators, ah, Star Wars joke.", "tokens": [50558, 17434, 2856, 337, 13814, 26315, 3391, 11, 3716, 11, 5705, 9818, 7647, 13, 50730], "temperature": 0.0, "avg_logprob": -0.22175979614257812, "compression_ratio": 1.7378378378378379, "no_speech_prob": 0.45273080468177795}, {"id": 62, "seek": 23002, "start": 237.34, "end": 240.58, "text": " Pagan addresses these issues by prioritizing the creation of other language grammars without", "tokens": [50730, 430, 14167, 16862, 613, 2663, 538, 14846, 3319, 264, 8016, 295, 661, 2856, 17570, 685, 1553, 50892], "temperature": 0.0, "avg_logprob": -0.22175979614257812, "compression_ratio": 1.7378378378378379, "no_speech_prob": 0.45273080468177795}, {"id": 63, "seek": 23002, "start": 240.58, "end": 244.78, "text": " weighing them down with any specific bias, and I keep hitting that point because that's", "tokens": [50892, 31986, 552, 760, 365, 604, 2685, 12577, 11, 293, 286, 1066, 8850, 300, 935, 570, 300, 311, 51102], "temperature": 0.0, "avg_logprob": -0.22175979614257812, "compression_ratio": 1.7378378378378379, "no_speech_prob": 0.45273080468177795}, {"id": 64, "seek": 23002, "start": 244.78, "end": 248.78, "text": " why all of the Pagan parsers that are out there right now, Guido van Rosen, Pagan of", "tokens": [51102, 983, 439, 295, 264, 430, 14167, 21156, 433, 300, 366, 484, 456, 558, 586, 11, 2694, 2925, 3161, 11144, 268, 11, 430, 14167, 295, 51302], "temperature": 0.0, "avg_logprob": -0.22175979614257812, "compression_ratio": 1.7378378378378379, "no_speech_prob": 0.45273080468177795}, {"id": 65, "seek": 23002, "start": 248.78, "end": 253.14000000000001, "text": " the Python creator, obsessed with Pagan, there's lots of videos from him trying to re-implement", "tokens": [51302, 264, 15329, 14181, 11, 16923, 365, 430, 14167, 11, 456, 311, 3195, 295, 2145, 490, 796, 1382, 281, 319, 12, 332, 43704, 51520], "temperature": 0.0, "avg_logprob": -0.22175979614257812, "compression_ratio": 1.7378378378378379, "no_speech_prob": 0.45273080468177795}, {"id": 66, "seek": 23002, "start": 253.14000000000001, "end": 254.74, "text": " the entire grammar of Python and Pagan.", "tokens": [51520, 264, 2302, 22317, 295, 15329, 293, 430, 14167, 13, 51600], "temperature": 0.0, "avg_logprob": -0.22175979614257812, "compression_ratio": 1.7378378378378379, "no_speech_prob": 0.45273080468177795}, {"id": 67, "seek": 23002, "start": 254.74, "end": 259.42, "text": " I don't know if he ever made it through that or not, but lots and lots of implementations", "tokens": [51600, 286, 500, 380, 458, 498, 415, 1562, 1027, 309, 807, 300, 420, 406, 11, 457, 3195, 293, 3195, 295, 4445, 763, 51834], "temperature": 0.0, "avg_logprob": -0.22175979614257812, "compression_ratio": 1.7378378378378379, "no_speech_prob": 0.45273080468177795}, {"id": 68, "seek": 25942, "start": 259.42, "end": 264.46000000000004, "text": " of Pagan that forced you to use that specific language, and so Pagan, the N in Pagan stands", "tokens": [50364, 295, 430, 14167, 300, 7579, 291, 281, 764, 300, 2685, 2856, 11, 293, 370, 430, 14167, 11, 264, 426, 294, 430, 14167, 7382, 50616], "temperature": 0.0, "avg_logprob": -0.2899267023259943, "compression_ratio": 1.7649122807017543, "no_speech_prob": 0.1822553277015686}, {"id": 69, "seek": 25942, "start": 264.46000000000004, "end": 267.02000000000004, "text": " for notation, which means we don't care.", "tokens": [50616, 337, 24657, 11, 597, 1355, 321, 500, 380, 1127, 13, 50744], "temperature": 0.0, "avg_logprob": -0.2899267023259943, "compression_ratio": 1.7649122807017543, "no_speech_prob": 0.1822553277015686}, {"id": 70, "seek": 25942, "start": 267.02000000000004, "end": 273.38, "text": " It means we are not associated with any specific language, and therefore it's universal.", "tokens": [50744, 467, 1355, 321, 366, 406, 6615, 365, 604, 2685, 2856, 11, 293, 4412, 309, 311, 11455, 13, 51062], "temperature": 0.0, "avg_logprob": -0.2899267023259943, "compression_ratio": 1.7649122807017543, "no_speech_prob": 0.1822553277015686}, {"id": 71, "seek": 25942, "start": 273.38, "end": 276.94, "text": " It's now, and see, Python, is it okay?", "tokens": [51062, 467, 311, 586, 11, 293, 536, 11, 15329, 11, 307, 309, 1392, 30, 51240], "temperature": 0.0, "avg_logprob": -0.2899267023259943, "compression_ratio": 1.7649122807017543, "no_speech_prob": 0.1822553277015686}, {"id": 72, "seek": 25942, "start": 276.94, "end": 278.38, "text": " Weighing it down is specific bias commitment.", "tokens": [51240, 492, 24412, 309, 760, 307, 2685, 12577, 8371, 13, 51312], "temperature": 0.0, "avg_logprob": -0.2899267023259943, "compression_ratio": 1.7649122807017543, "no_speech_prob": 0.1822553277015686}, {"id": 73, "seek": 25942, "start": 278.38, "end": 283.58000000000004, "text": " In fact, Pagan is so flexible it can also be used to define written language as a music", "tokens": [51312, 682, 1186, 11, 430, 14167, 307, 370, 11358, 309, 393, 611, 312, 1143, 281, 6964, 3720, 2856, 382, 257, 1318, 51572], "temperature": 0.0, "avg_logprob": -0.2899267023259943, "compression_ratio": 1.7649122807017543, "no_speech_prob": 0.1822553277015686}, {"id": 74, "seek": 25942, "start": 283.58000000000004, "end": 284.58000000000004, "text": " notation.", "tokens": [51572, 24657, 13, 51622], "temperature": 0.0, "avg_logprob": -0.2899267023259943, "compression_ratio": 1.7649122807017543, "no_speech_prob": 0.1822553277015686}, {"id": 75, "seek": 25942, "start": 284.58000000000004, "end": 289.1, "text": " Pagan grammars are exploding, but inconsistent in 2004, Pagan grammars have exploded in popularity", "tokens": [51622, 430, 14167, 17570, 685, 366, 35175, 11, 457, 36891, 294, 15817, 11, 430, 14167, 17570, 685, 362, 27049, 294, 19301, 51848], "temperature": 0.0, "avg_logprob": -0.2899267023259943, "compression_ratio": 1.7649122807017543, "no_speech_prob": 0.1822553277015686}, {"id": 76, "seek": 28910, "start": 289.66, "end": 290.78000000000003, "text": " and that's when I started this.", "tokens": [50392, 293, 300, 311, 562, 286, 1409, 341, 13, 50448], "temperature": 0.0, "avg_logprob": -0.27696629424593344, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.17769816517829895}, {"id": 77, "seek": 28910, "start": 290.78000000000003, "end": 292.94, "text": " God, that was, I mean, that's not true.", "tokens": [50448, 1265, 11, 300, 390, 11, 286, 914, 11, 300, 311, 406, 2074, 13, 50556], "temperature": 0.0, "avg_logprob": -0.27696629424593344, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.17769816517829895}, {"id": 78, "seek": 28910, "start": 292.94, "end": 295.54, "text": " I started, that's when they came become popular.", "tokens": [50556, 286, 1409, 11, 300, 311, 562, 436, 1361, 1813, 3743, 13, 50686], "temperature": 0.0, "avg_logprob": -0.27696629424593344, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.17769816517829895}, {"id": 79, "seek": 28910, "start": 295.54, "end": 298.5, "text": " I didn't get into it and probably tell, I want to say 2020.", "tokens": [50686, 286, 994, 380, 483, 666, 309, 293, 1391, 980, 11, 286, 528, 281, 584, 4808, 13, 50834], "temperature": 0.0, "avg_logprob": -0.27696629424593344, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.17769816517829895}, {"id": 80, "seek": 28910, "start": 298.5, "end": 301.18, "text": " I was already streaming, so it'd be like 2021, 20.", "tokens": [50834, 286, 390, 1217, 11791, 11, 370, 309, 1116, 312, 411, 7201, 11, 945, 13, 50968], "temperature": 0.0, "avg_logprob": -0.27696629424593344, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.17769816517829895}, {"id": 81, "seek": 28910, "start": 301.18, "end": 306.1, "text": " Anyway, Brian Ford's example, Pagan grammar is all but ignored.", "tokens": [50968, 5684, 11, 10765, 11961, 311, 1365, 11, 430, 14167, 22317, 307, 439, 457, 19735, 13, 51214], "temperature": 0.0, "avg_logprob": -0.27696629424593344, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.17769816517829895}, {"id": 82, "seek": 28910, "start": 306.1, "end": 307.94, "text": " It's also incorrect.", "tokens": [51214, 467, 311, 611, 18424, 13, 51306], "temperature": 0.0, "avg_logprob": -0.27696629424593344, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.17769816517829895}, {"id": 83, "seek": 28910, "start": 307.94, "end": 311.58000000000004, "text": " While we were going through the parsing of his example, we realized it violates his own", "tokens": [51306, 3987, 321, 645, 516, 807, 264, 21156, 278, 295, 702, 1365, 11, 321, 5334, 309, 3448, 1024, 702, 1065, 51488], "temperature": 0.0, "avg_logprob": -0.27696629424593344, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.17769816517829895}, {"id": 84, "seek": 28910, "start": 311.58000000000004, "end": 313.38, "text": " syntax.", "tokens": [51488, 28431, 13, 51578], "temperature": 0.0, "avg_logprob": -0.27696629424593344, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.17769816517829895}, {"id": 85, "seek": 28910, "start": 313.38, "end": 315.38, "text": " I still haven't ever told Brian that.", "tokens": [51578, 286, 920, 2378, 380, 1562, 1907, 10765, 300, 13, 51678], "temperature": 0.0, "avg_logprob": -0.27696629424593344, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.17769816517829895}, {"id": 86, "seek": 31538, "start": 315.38, "end": 320.74, "text": " His example is invalid, Peg, according to his own specification.", "tokens": [50364, 2812, 1365, 307, 34702, 11, 28007, 11, 4650, 281, 702, 1065, 31256, 13, 50632], "temperature": 0.0, "avg_logprob": -0.20194929359603103, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.008846023119986057}, {"id": 87, "seek": 31538, "start": 320.74, "end": 324.62, "text": " I don't think he actually ever tested it with a parser or anything.", "tokens": [50632, 286, 500, 380, 519, 415, 767, 1562, 8246, 309, 365, 257, 21156, 260, 420, 1340, 13, 50826], "temperature": 0.0, "avg_logprob": -0.20194929359603103, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.008846023119986057}, {"id": 88, "seek": 31538, "start": 324.62, "end": 329.5, "text": " We actually found that out very surprisingly, quit when we were going through making our", "tokens": [50826, 492, 767, 1352, 300, 484, 588, 17600, 11, 10366, 562, 321, 645, 516, 807, 1455, 527, 51070], "temperature": 0.0, "avg_logprob": -0.20194929359603103, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.008846023119986057}, {"id": 89, "seek": 31538, "start": 329.5, "end": 332.26, "text": " own parser, and we use this example.", "tokens": [51070, 1065, 21156, 260, 11, 293, 321, 764, 341, 1365, 13, 51208], "temperature": 0.0, "avg_logprob": -0.20194929359603103, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.008846023119986057}, {"id": 90, "seek": 31538, "start": 332.26, "end": 335.06, "text": " So Brian Ford's example, Pagan grammar is all but ignored as people continue to build", "tokens": [51208, 407, 10765, 11961, 311, 1365, 11, 430, 14167, 22317, 307, 439, 457, 19735, 382, 561, 2354, 281, 1322, 51348], "temperature": 0.0, "avg_logprob": -0.20194929359603103, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.008846023119986057}, {"id": 91, "seek": 31538, "start": 335.06, "end": 337.5, "text": " on their own syntax as a very little resemblance to the original.", "tokens": [51348, 322, 641, 1065, 28431, 382, 257, 588, 707, 20695, 37271, 281, 264, 3380, 13, 51470], "temperature": 0.0, "avg_logprob": -0.20194929359603103, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.008846023119986057}, {"id": 92, "seek": 31538, "start": 337.5, "end": 340.42, "text": " I think that's probably why it doesn't work, because he just threw it in there because", "tokens": [51470, 286, 519, 300, 311, 1391, 983, 309, 1177, 380, 589, 11, 570, 415, 445, 11918, 309, 294, 456, 570, 51616], "temperature": 0.0, "avg_logprob": -0.20194929359603103, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.008846023119986057}, {"id": 93, "seek": 31538, "start": 340.42, "end": 343.38, "text": " everyone else has got their own idea of what it should be.", "tokens": [51616, 1518, 1646, 575, 658, 641, 1065, 1558, 295, 437, 309, 820, 312, 13, 51764], "temperature": 0.0, "avg_logprob": -0.20194929359603103, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.008846023119986057}, {"id": 94, "seek": 34338, "start": 343.38, "end": 345.14, "text": " There are more implementation code than Peg.", "tokens": [50364, 821, 366, 544, 11420, 3089, 813, 28007, 13, 50452], "temperature": 0.0, "avg_logprob": -0.17043502051551063, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.3206726312637329}, {"id": 95, "seek": 34338, "start": 345.14, "end": 349.02, "text": " This is demonstrated by many projects that contain both a grammar file for becoming acquainted", "tokens": [50452, 639, 307, 18772, 538, 867, 4455, 300, 5304, 1293, 257, 22317, 3991, 337, 5617, 50224, 50646], "temperature": 0.0, "avg_logprob": -0.17043502051551063, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.3206726312637329}, {"id": 96, "seek": 34338, "start": 349.02, "end": 352.78, "text": " with the syntax and another virtually identical file containing additional implementation", "tokens": [50646, 365, 264, 28431, 293, 1071, 14103, 14800, 3991, 19273, 4497, 11420, 50834], "temperature": 0.0, "avg_logprob": -0.17043502051551063, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.3206726312637329}, {"id": 97, "seek": 34338, "start": 352.78, "end": 355.7, "text": " specific to the code, a specific implementation language.", "tokens": [50834, 2685, 281, 264, 3089, 11, 257, 2685, 11420, 2856, 13, 50980], "temperature": 0.0, "avg_logprob": -0.17043502051551063, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.3206726312637329}, {"id": 98, "seek": 34338, "start": 355.7, "end": 357.5, "text": " So this is how it goes, right?", "tokens": [50980, 407, 341, 307, 577, 309, 1709, 11, 558, 30, 51070], "temperature": 0.0, "avg_logprob": -0.17043502051551063, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.3206726312637329}, {"id": 99, "seek": 34338, "start": 357.5, "end": 365.18, "text": " You run your code through this and through the parser, right?", "tokens": [51070, 509, 1190, 428, 3089, 807, 341, 293, 807, 264, 21156, 260, 11, 558, 30, 51454], "temperature": 0.0, "avg_logprob": -0.17043502051551063, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.3206726312637329}, {"id": 100, "seek": 34338, "start": 365.18, "end": 369.54, "text": " And then the parser will actually, if you do a Peg parser, right, it'll actually generate", "tokens": [51454, 400, 550, 264, 21156, 260, 486, 767, 11, 498, 291, 360, 257, 28007, 21156, 260, 11, 558, 11, 309, 603, 767, 8460, 51672], "temperature": 0.0, "avg_logprob": -0.17043502051551063, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.3206726312637329}, {"id": 101, "seek": 34338, "start": 369.54, "end": 370.54, "text": " code for you.", "tokens": [51672, 3089, 337, 291, 13, 51722], "temperature": 0.0, "avg_logprob": -0.17043502051551063, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.3206726312637329}, {"id": 102, "seek": 37054, "start": 370.54, "end": 374.42, "text": " It'll generate parser code in your target language, and, you know, like Lex and Yak", "tokens": [50364, 467, 603, 8460, 21156, 260, 3089, 294, 428, 3779, 2856, 11, 293, 11, 291, 458, 11, 411, 24086, 293, 31484, 50558], "temperature": 0.0, "avg_logprob": -0.20337521479679987, "compression_ratio": 1.7173252279635258, "no_speech_prob": 0.10368145257234573}, {"id": 103, "seek": 37054, "start": 374.42, "end": 377.38, "text": " and all those other things, except for its much more modern approach.", "tokens": [50558, 293, 439, 729, 661, 721, 11, 3993, 337, 1080, 709, 544, 4363, 3109, 13, 50706], "temperature": 0.0, "avg_logprob": -0.20337521479679987, "compression_ratio": 1.7173252279635258, "no_speech_prob": 0.10368145257234573}, {"id": 104, "seek": 37054, "start": 377.38, "end": 381.74, "text": " And so there's lots of them out there, you can go look at them.", "tokens": [50706, 400, 370, 456, 311, 3195, 295, 552, 484, 456, 11, 291, 393, 352, 574, 412, 552, 13, 50924], "temperature": 0.0, "avg_logprob": -0.20337521479679987, "compression_ratio": 1.7173252279635258, "no_speech_prob": 0.10368145257234573}, {"id": 105, "seek": 37054, "start": 381.74, "end": 385.74, "text": " So anyway, this redundancy and specialization are not only less sustainable, but they're", "tokens": [50924, 407, 4033, 11, 341, 27830, 6717, 293, 2121, 2144, 366, 406, 787, 1570, 11235, 11, 457, 436, 434, 51124], "temperature": 0.0, "avg_logprob": -0.20337521479679987, "compression_ratio": 1.7173252279635258, "no_speech_prob": 0.10368145257234573}, {"id": 106, "seek": 37054, "start": 385.74, "end": 390.18, "text": " also highly rigid and counterproductive, and you can't exchange them with anybody else.", "tokens": [51124, 611, 5405, 22195, 293, 5682, 14314, 20221, 11, 293, 291, 393, 380, 7742, 552, 365, 4472, 1646, 13, 51346], "temperature": 0.0, "avg_logprob": -0.20337521479679987, "compression_ratio": 1.7173252279635258, "no_speech_prob": 0.10368145257234573}, {"id": 107, "seek": 37054, "start": 390.18, "end": 393.90000000000003, "text": " Pagan is a language grammar specification that does not allow implementation code so", "tokens": [51346, 430, 14167, 307, 257, 2856, 22317, 31256, 300, 775, 406, 2089, 11420, 3089, 370, 51532], "temperature": 0.0, "avg_logprob": -0.20337521479679987, "compression_ratio": 1.7173252279635258, "no_speech_prob": 0.10368145257234573}, {"id": 108, "seek": 37054, "start": 393.90000000000003, "end": 397.58000000000004, "text": " that the resulting grammar specifications stand on their own, allowing their creation", "tokens": [51532, 300, 264, 16505, 22317, 29448, 1463, 322, 641, 1065, 11, 8293, 641, 8016, 51716], "temperature": 0.0, "avg_logprob": -0.20337521479679987, "compression_ratio": 1.7173252279635258, "no_speech_prob": 0.10368145257234573}, {"id": 109, "seek": 39758, "start": 397.58, "end": 401.78, "text": " of any variety of lunches and co-generators in different language implementations.", "tokens": [50364, 295, 604, 5673, 295, 6349, 279, 293, 598, 12, 21848, 3391, 294, 819, 2856, 4445, 763, 13, 50574], "temperature": 0.0, "avg_logprob": -0.17001448737250435, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.5228726863861084}, {"id": 110, "seek": 39758, "start": 401.78, "end": 407.34, "text": " Even different design variations in the same implementation language, AST, callbacks, etc.", "tokens": [50574, 2754, 819, 1715, 17840, 294, 264, 912, 11420, 2856, 11, 316, 6840, 11, 818, 17758, 11, 5183, 13, 50852], "temperature": 0.0, "avg_logprob": -0.17001448737250435, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.5228726863861084}, {"id": 111, "seek": 39758, "start": 407.34, "end": 415.09999999999997, "text": " I'm going to get there, but Pagan actually defines the AST format as well, which is not", "tokens": [50852, 286, 478, 516, 281, 483, 456, 11, 457, 430, 14167, 767, 23122, 264, 316, 6840, 7877, 382, 731, 11, 597, 307, 406, 51240], "temperature": 0.0, "avg_logprob": -0.17001448737250435, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.5228726863861084}, {"id": 112, "seek": 39758, "start": 415.09999999999997, "end": 418.29999999999995, "text": " included in any other thing.", "tokens": [51240, 5556, 294, 604, 661, 551, 13, 51400], "temperature": 0.0, "avg_logprob": -0.17001448737250435, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.5228726863861084}, {"id": 113, "seek": 39758, "start": 418.29999999999995, "end": 424.26, "text": " So I mean, having the AST, which is, you know, the thing that you care about when you run", "tokens": [51400, 407, 286, 914, 11, 1419, 264, 316, 6840, 11, 597, 307, 11, 291, 458, 11, 264, 551, 300, 291, 1127, 466, 562, 291, 1190, 51698], "temperature": 0.0, "avg_logprob": -0.17001448737250435, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.5228726863861084}, {"id": 114, "seek": 42426, "start": 424.26, "end": 429.7, "text": " some code through a parser, you want the AST then so you can check for the syntax properly,", "tokens": [50364, 512, 3089, 807, 257, 21156, 260, 11, 291, 528, 264, 316, 6840, 550, 370, 291, 393, 1520, 337, 264, 28431, 6108, 11, 50636], "temperature": 0.0, "avg_logprob": -0.15410635630289712, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.4918504059314728}, {"id": 115, "seek": 42426, "start": 429.7, "end": 433.58, "text": " or you can, you know, create your compile in any number of ways, or you can transpile", "tokens": [50636, 420, 291, 393, 11, 291, 458, 11, 1884, 428, 31413, 294, 604, 1230, 295, 2098, 11, 420, 291, 393, 7132, 794, 50830], "temperature": 0.0, "avg_logprob": -0.15410635630289712, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.4918504059314728}, {"id": 116, "seek": 42426, "start": 433.58, "end": 434.58, "text": " or whatever.", "tokens": [50830, 420, 2035, 13, 50880], "temperature": 0.0, "avg_logprob": -0.15410635630289712, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.4918504059314728}, {"id": 117, "seek": 42426, "start": 434.58, "end": 435.58, "text": " You have to have an AST.", "tokens": [50880, 509, 362, 281, 362, 364, 316, 6840, 13, 50930], "temperature": 0.0, "avg_logprob": -0.15410635630289712, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.4918504059314728}, {"id": 118, "seek": 42426, "start": 435.58, "end": 441.38, "text": " I mean, there are implementations that don't use an AST because it takes, it's a minimally,", "tokens": [50930, 286, 914, 11, 456, 366, 4445, 763, 300, 500, 380, 764, 364, 316, 6840, 570, 309, 2516, 11, 309, 311, 257, 4464, 379, 11, 51220], "temperature": 0.0, "avg_logprob": -0.15410635630289712, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.4918504059314728}, {"id": 119, "seek": 42426, "start": 441.38, "end": 443.3, "text": " you know, slower to do that.", "tokens": [51220, 291, 458, 11, 14009, 281, 360, 300, 13, 51316], "temperature": 0.0, "avg_logprob": -0.15410635630289712, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.4918504059314728}, {"id": 120, "seek": 42426, "start": 443.3, "end": 445.86, "text": " That's the compile part of regular expressions, by the way.", "tokens": [51316, 663, 311, 264, 31413, 644, 295, 3890, 15277, 11, 538, 264, 636, 13, 51444], "temperature": 0.0, "avg_logprob": -0.15410635630289712, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.4918504059314728}, {"id": 121, "seek": 42426, "start": 445.86, "end": 449.98, "text": " When you say you have to compile a regular expression, you're pretty much building a binary", "tokens": [51444, 1133, 291, 584, 291, 362, 281, 31413, 257, 3890, 6114, 11, 291, 434, 1238, 709, 2390, 257, 17434, 51650], "temperature": 0.0, "avg_logprob": -0.15410635630289712, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.4918504059314728}, {"id": 122, "seek": 42426, "start": 449.98, "end": 453.18, "text": " that's going to give you an internal AST really quickly.", "tokens": [51650, 300, 311, 516, 281, 976, 291, 364, 6920, 316, 6840, 534, 2661, 13, 51810], "temperature": 0.0, "avg_logprob": -0.15410635630289712, "compression_ratio": 1.7467948717948718, "no_speech_prob": 0.4918504059314728}, {"id": 123, "seek": 45318, "start": 453.18, "end": 454.78000000000003, "text": " And then, you know, it's kind of internal.", "tokens": [50364, 400, 550, 11, 291, 458, 11, 309, 311, 733, 295, 6920, 13, 50444], "temperature": 0.0, "avg_logprob": -0.19976677000522614, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.010327324271202087}, {"id": 124, "seek": 45318, "start": 454.78000000000003, "end": 457.7, "text": " I don't even know if there's an AST for most regular expressions, actually.", "tokens": [50444, 286, 500, 380, 754, 458, 498, 456, 311, 364, 316, 6840, 337, 881, 3890, 15277, 11, 767, 13, 50590], "temperature": 0.0, "avg_logprob": -0.19976677000522614, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.010327324271202087}, {"id": 125, "seek": 45318, "start": 457.7, "end": 461.90000000000003, "text": " But these days you want the AST because that's the thing that's going to give it to you.", "tokens": [50590, 583, 613, 1708, 291, 528, 264, 316, 6840, 570, 300, 311, 264, 551, 300, 311, 516, 281, 976, 309, 281, 291, 13, 50800], "temperature": 0.0, "avg_logprob": -0.19976677000522614, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.010327324271202087}, {"id": 126, "seek": 45318, "start": 461.90000000000003, "end": 466.5, "text": " So like if you do, if you do pandoc, let's say you take, pandoc actually has this built", "tokens": [50800, 407, 411, 498, 291, 360, 11, 498, 291, 360, 4565, 905, 11, 718, 311, 584, 291, 747, 11, 4565, 905, 767, 575, 341, 3094, 51030], "temperature": 0.0, "avg_logprob": -0.19976677000522614, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.010327324271202087}, {"id": 127, "seek": 45318, "start": 466.5, "end": 472.5, "text": " in, you can do pandoc t, is it raw?", "tokens": [51030, 294, 11, 291, 393, 360, 4565, 905, 256, 11, 307, 309, 8936, 30, 51330], "temperature": 0.0, "avg_logprob": -0.19976677000522614, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.010327324271202087}, {"id": 128, "seek": 45318, "start": 472.5, "end": 474.14, "text": " You can do JSON if you want.", "tokens": [51330, 509, 393, 360, 31828, 498, 291, 528, 13, 51412], "temperature": 0.0, "avg_logprob": -0.19976677000522614, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.010327324271202087}, {"id": 129, "seek": 45318, "start": 474.14, "end": 478.78000000000003, "text": " So JSON, this is a representation of this document in an AST.", "tokens": [51412, 407, 31828, 11, 341, 307, 257, 10290, 295, 341, 4166, 294, 364, 316, 6840, 13, 51644], "temperature": 0.0, "avg_logprob": -0.19976677000522614, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.010327324271202087}, {"id": 130, "seek": 47878, "start": 478.78, "end": 480.7, "text": " It's a very simple document, right?", "tokens": [50364, 467, 311, 257, 588, 2199, 4166, 11, 558, 30, 50460], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 131, "seek": 47878, "start": 480.7, "end": 486.41999999999996, "text": " And it's parsed using pandoc with the specification for markdown, pandoc markdown.", "tokens": [50460, 400, 309, 311, 21156, 292, 1228, 4565, 905, 365, 264, 31256, 337, 1491, 5093, 11, 4565, 905, 1491, 5093, 13, 50746], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 132, "seek": 47878, "start": 486.41999999999996, "end": 489.53999999999996, "text": " And this is the resulting JSON version of the AST.", "tokens": [50746, 400, 341, 307, 264, 16505, 31828, 3037, 295, 264, 316, 6840, 13, 50902], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 133, "seek": 47878, "start": 489.53999999999996, "end": 491.17999999999995, "text": " Now, there's another one.", "tokens": [50902, 823, 11, 456, 311, 1071, 472, 13, 50984], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 134, "seek": 47878, "start": 491.17999999999995, "end": 494.53999999999996, "text": " I don't know the name of it.", "tokens": [50984, 286, 500, 380, 458, 264, 1315, 295, 309, 13, 51152], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 135, "seek": 47878, "start": 494.53999999999996, "end": 495.53999999999996, "text": " Is it dead?", "tokens": [51152, 1119, 309, 3116, 30, 51202], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 136, "seek": 47878, "start": 495.53999999999996, "end": 498.38, "text": " I don't know what the name of it is.", "tokens": [51202, 286, 500, 380, 458, 437, 264, 1315, 295, 309, 307, 13, 51344], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 137, "seek": 47878, "start": 498.38, "end": 499.38, "text": " I want to say raw.", "tokens": [51344, 286, 528, 281, 584, 8936, 13, 51394], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 138, "seek": 47878, "start": 499.38, "end": 500.38, "text": " Maybe it's data.", "tokens": [51394, 2704, 309, 311, 1412, 13, 51444], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 139, "seek": 47878, "start": 500.38, "end": 504.82, "text": " It might be AST, data.", "tokens": [51444, 467, 1062, 312, 316, 6840, 11, 1412, 13, 51666], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 140, "seek": 47878, "start": 504.82, "end": 506.41999999999996, "text": " I wish pandoc had completion.", "tokens": [51666, 286, 3172, 4565, 905, 632, 19372, 13, 51746], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 141, "seek": 47878, "start": 506.41999999999996, "end": 507.41999999999996, "text": " It does not have completion.", "tokens": [51746, 467, 775, 406, 362, 19372, 13, 51796], "temperature": 0.0, "avg_logprob": -0.20465707045335035, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.17776092886924744}, {"id": 142, "seek": 50742, "start": 507.42, "end": 509.14000000000004, "text": " There's another number of reasons I don't like it.", "tokens": [50364, 821, 311, 1071, 1230, 295, 4112, 286, 500, 380, 411, 309, 13, 50450], "temperature": 0.0, "avg_logprob": -0.2750384264653272, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004069700371474028}, {"id": 143, "seek": 50742, "start": 509.14000000000004, "end": 510.62, "text": " Oh wait, maybe it does have completion.", "tokens": [50450, 876, 1699, 11, 1310, 309, 775, 362, 19372, 13, 50524], "temperature": 0.0, "avg_logprob": -0.2750384264653272, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004069700371474028}, {"id": 144, "seek": 50742, "start": 510.62, "end": 512.14, "text": " When did it add it?", "tokens": [50524, 1133, 630, 309, 909, 309, 30, 50600], "temperature": 0.0, "avg_logprob": -0.2750384264653272, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004069700371474028}, {"id": 145, "seek": 50742, "start": 512.14, "end": 515.3000000000001, "text": " Maybe it already said, I always had it, I just forgot.", "tokens": [50600, 2704, 309, 1217, 848, 11, 286, 1009, 632, 309, 11, 286, 445, 5298, 13, 50758], "temperature": 0.0, "avg_logprob": -0.2750384264653272, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004069700371474028}, {"id": 146, "seek": 50742, "start": 515.3000000000001, "end": 516.3000000000001, "text": " Which is the one we want.", "tokens": [50758, 3013, 307, 264, 472, 321, 528, 13, 50808], "temperature": 0.0, "avg_logprob": -0.2750384264653272, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004069700371474028}, {"id": 147, "seek": 50742, "start": 516.3000000000001, "end": 523.5, "text": " We want not GFM, we want not LaTeX native.", "tokens": [50808, 492, 528, 406, 460, 37, 44, 11, 321, 528, 406, 2369, 14233, 55, 8470, 13, 51168], "temperature": 0.0, "avg_logprob": -0.2750384264653272, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004069700371474028}, {"id": 148, "seek": 50742, "start": 523.5, "end": 524.5, "text": " There we go.", "tokens": [51168, 821, 321, 352, 13, 51218], "temperature": 0.0, "avg_logprob": -0.2750384264653272, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004069700371474028}, {"id": 149, "seek": 50742, "start": 524.5, "end": 533.62, "text": " So this is the internal Haskell AST that is used by pandoc.", "tokens": [51218, 407, 341, 307, 264, 6920, 8646, 43723, 316, 6840, 300, 307, 1143, 538, 4565, 905, 13, 51674], "temperature": 0.0, "avg_logprob": -0.2750384264653272, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.004069700371474028}, {"id": 150, "seek": 53362, "start": 533.62, "end": 539.46, "text": " So it tells you the thing, so you have a header, and then you have a string, and then", "tokens": [50364, 407, 309, 5112, 291, 264, 551, 11, 370, 291, 362, 257, 23117, 11, 293, 550, 291, 362, 257, 6798, 11, 293, 550, 50656], "temperature": 0.0, "avg_logprob": -0.1990367479560789, "compression_ratio": 1.8956521739130434, "no_speech_prob": 0.13832272589206696}, {"id": 151, "seek": 53362, "start": 539.46, "end": 543.0600000000001, "text": " another string, space, space, string, space, string, space, space.", "tokens": [50656, 1071, 6798, 11, 1901, 11, 1901, 11, 6798, 11, 1901, 11, 6798, 11, 1901, 11, 1901, 13, 50836], "temperature": 0.0, "avg_logprob": -0.1990367479560789, "compression_ratio": 1.8956521739130434, "no_speech_prob": 0.13832272589206696}, {"id": 152, "seek": 53362, "start": 543.0600000000001, "end": 546.14, "text": " So this is Haskell is the perfect language for writing ASTs.", "tokens": [50836, 407, 341, 307, 8646, 43723, 307, 264, 2176, 2856, 337, 3579, 316, 6840, 82, 13, 50990], "temperature": 0.0, "avg_logprob": -0.1990367479560789, "compression_ratio": 1.8956521739130434, "no_speech_prob": 0.13832272589206696}, {"id": 153, "seek": 53362, "start": 546.14, "end": 551.66, "text": " I want you to notice something here that all of the nodes that are parsed in the AST", "tokens": [50990, 286, 528, 291, 281, 3449, 746, 510, 300, 439, 295, 264, 13891, 300, 366, 21156, 292, 294, 264, 316, 6840, 51266], "temperature": 0.0, "avg_logprob": -0.1990367479560789, "compression_ratio": 1.8956521739130434, "no_speech_prob": 0.13832272589206696}, {"id": 154, "seek": 53362, "start": 551.66, "end": 552.82, "text": " have three elements to them.", "tokens": [51266, 362, 1045, 4959, 281, 552, 13, 51324], "temperature": 0.0, "avg_logprob": -0.1990367479560789, "compression_ratio": 1.8956521739130434, "no_speech_prob": 0.13832272589206696}, {"id": 155, "seek": 53362, "start": 552.82, "end": 556.62, "text": " They have the type.", "tokens": [51324, 814, 362, 264, 2010, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1990367479560789, "compression_ratio": 1.8956521739130434, "no_speech_prob": 0.13832272589206696}, {"id": 156, "seek": 53362, "start": 556.62, "end": 562.66, "text": " I think they have an array of how many there are, and then they have the actual content.", "tokens": [51514, 286, 519, 436, 362, 364, 10225, 295, 577, 867, 456, 366, 11, 293, 550, 436, 362, 264, 3539, 2701, 13, 51816], "temperature": 0.0, "avg_logprob": -0.1990367479560789, "compression_ratio": 1.8956521739130434, "no_speech_prob": 0.13832272589206696}, {"id": 157, "seek": 56266, "start": 562.66, "end": 563.66, "text": " I can't remember.", "tokens": [50364, 286, 393, 380, 1604, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1744190129366788, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.014954566955566406}, {"id": 158, "seek": 56266, "start": 563.66, "end": 565.4599999999999, "text": " Oh no, no, that's a variation on a header.", "tokens": [50414, 876, 572, 11, 572, 11, 300, 311, 257, 12990, 322, 257, 23117, 13, 50504], "temperature": 0.0, "avg_logprob": -0.1744190129366788, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.014954566955566406}, {"id": 159, "seek": 56266, "start": 565.4599999999999, "end": 567.24, "text": " It must be an attribute of it, whatever.", "tokens": [50504, 467, 1633, 312, 364, 19667, 295, 309, 11, 2035, 13, 50593], "temperature": 0.0, "avg_logprob": -0.1744190129366788, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.014954566955566406}, {"id": 160, "seek": 56266, "start": 567.24, "end": 571.5, "text": " So that's the AST, and you need an AST if you're going to do anything.", "tokens": [50593, 407, 300, 311, 264, 316, 6840, 11, 293, 291, 643, 364, 316, 6840, 498, 291, 434, 516, 281, 360, 1340, 13, 50806], "temperature": 0.0, "avg_logprob": -0.1744190129366788, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.014954566955566406}, {"id": 161, "seek": 56266, "start": 571.5, "end": 574.18, "text": " It's a Haskell data representation, yeah.", "tokens": [50806, 467, 311, 257, 8646, 43723, 1412, 10290, 11, 1338, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1744190129366788, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.014954566955566406}, {"id": 162, "seek": 56266, "start": 574.18, "end": 580.5, "text": " This is a Haskell data representation of, and I don't know, they have nodes with attributes", "tokens": [50940, 639, 307, 257, 8646, 43723, 1412, 10290, 295, 11, 293, 286, 500, 380, 458, 11, 436, 362, 13891, 365, 17212, 51256], "temperature": 0.0, "avg_logprob": -0.1744190129366788, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.014954566955566406}, {"id": 163, "seek": 56266, "start": 580.5, "end": 581.74, "text": " which I hate.", "tokens": [51256, 597, 286, 4700, 13, 51318], "temperature": 0.0, "avg_logprob": -0.1744190129366788, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.014954566955566406}, {"id": 164, "seek": 56266, "start": 581.74, "end": 587.14, "text": " So Pagan does not allow attributes for a reason.", "tokens": [51318, 407, 430, 14167, 775, 406, 2089, 17212, 337, 257, 1778, 13, 51588], "temperature": 0.0, "avg_logprob": -0.1744190129366788, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.014954566955566406}, {"id": 165, "seek": 56266, "start": 587.14, "end": 589.9399999999999, "text": " You can never decide whether you want your node to have attributes or not, or you want", "tokens": [51588, 509, 393, 1128, 4536, 1968, 291, 528, 428, 9984, 281, 362, 17212, 420, 406, 11, 420, 291, 528, 51728], "temperature": 0.0, "avg_logprob": -0.1744190129366788, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.014954566955566406}, {"id": 166, "seek": 58994, "start": 589.94, "end": 592.86, "text": " it to just have the attributes to just be subnodes.", "tokens": [50364, 309, 281, 445, 362, 264, 17212, 281, 445, 312, 1422, 77, 4789, 13, 50510], "temperature": 0.0, "avg_logprob": -0.2114754419257171, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.01640092022716999}, {"id": 167, "seek": 58994, "start": 592.86, "end": 594.46, "text": " And so Pagan doesn't allow that.", "tokens": [50510, 400, 370, 430, 14167, 1177, 380, 2089, 300, 13, 50590], "temperature": 0.0, "avg_logprob": -0.2114754419257171, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.01640092022716999}, {"id": 168, "seek": 58994, "start": 594.46, "end": 602.5, "text": " It's a pretty deep comment, and if you don't know about parsers and stuff, but it's really", "tokens": [50590, 467, 311, 257, 1238, 2452, 2871, 11, 293, 498, 291, 500, 380, 458, 466, 21156, 433, 293, 1507, 11, 457, 309, 311, 534, 50992], "temperature": 0.0, "avg_logprob": -0.2114754419257171, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.01640092022716999}, {"id": 169, "seek": 58994, "start": 602.5, "end": 603.5, "text": " a...", "tokens": [50992, 257, 485, 51042], "temperature": 0.0, "avg_logprob": -0.2114754419257171, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.01640092022716999}, {"id": 170, "seek": 58994, "start": 603.5, "end": 604.5, "text": " ASTs are all over the place, by the way.", "tokens": [51042, 316, 6840, 82, 366, 439, 670, 264, 1081, 11, 538, 264, 636, 13, 51092], "temperature": 0.0, "avg_logprob": -0.2114754419257171, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.01640092022716999}, {"id": 171, "seek": 58994, "start": 604.5, "end": 607.6600000000001, "text": " If you go look at the web page, this is an AST.", "tokens": [51092, 759, 291, 352, 574, 412, 264, 3670, 3028, 11, 341, 307, 364, 316, 6840, 13, 51250], "temperature": 0.0, "avg_logprob": -0.2114754419257171, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.01640092022716999}, {"id": 172, "seek": 58994, "start": 607.6600000000001, "end": 608.6600000000001, "text": " This is an A...", "tokens": [51250, 639, 307, 364, 316, 485, 51300], "temperature": 0.0, "avg_logprob": -0.2114754419257171, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.01640092022716999}, {"id": 173, "seek": 58994, "start": 608.6600000000001, "end": 611.46, "text": " Any time you look at this, this is an AST.", "tokens": [51300, 2639, 565, 291, 574, 412, 341, 11, 341, 307, 364, 316, 6840, 13, 51440], "temperature": 0.0, "avg_logprob": -0.2114754419257171, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.01640092022716999}, {"id": 174, "seek": 58994, "start": 611.46, "end": 617.4000000000001, "text": " The reason node is called node is because the document object model is a specification", "tokens": [51440, 440, 1778, 9984, 307, 1219, 9984, 307, 570, 264, 4166, 2657, 2316, 307, 257, 31256, 51737], "temperature": 0.0, "avg_logprob": -0.2114754419257171, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.01640092022716999}, {"id": 175, "seek": 58994, "start": 617.4000000000001, "end": 618.94, "text": " of how to parse the thing.", "tokens": [51737, 295, 577, 281, 48377, 264, 551, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2114754419257171, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.01640092022716999}, {"id": 176, "seek": 61894, "start": 618.94, "end": 622.0600000000001, "text": " That was at one point defined through something else.", "tokens": [50364, 663, 390, 412, 472, 935, 7642, 807, 746, 1646, 13, 50520], "temperature": 0.0, "avg_logprob": -0.19023960515072472, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.06951536983251572}, {"id": 177, "seek": 61894, "start": 622.0600000000001, "end": 624.7, "text": " It can get tricky without properties and attributes.", "tokens": [50520, 467, 393, 483, 12414, 1553, 7221, 293, 17212, 13, 50652], "temperature": 0.0, "avg_logprob": -0.19023960515072472, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.06951536983251572}, {"id": 178, "seek": 61894, "start": 624.7, "end": 629.5400000000001, "text": " I haven't had any problem with it so far because you can just define...", "tokens": [50652, 286, 2378, 380, 632, 604, 1154, 365, 309, 370, 1400, 570, 291, 393, 445, 6964, 485, 50894], "temperature": 0.0, "avg_logprob": -0.19023960515072472, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.06951536983251572}, {"id": 179, "seek": 61894, "start": 629.5400000000001, "end": 632.1, "text": " You don't want to have one type have a different attribute to it.", "tokens": [50894, 509, 500, 380, 528, 281, 362, 472, 2010, 362, 257, 819, 19667, 281, 309, 13, 51022], "temperature": 0.0, "avg_logprob": -0.19023960515072472, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.06951536983251572}, {"id": 180, "seek": 61894, "start": 632.1, "end": 639.5, "text": " I mean, like H1, H2, H3, you can actually have a parent, and then you can have the child", "tokens": [51022, 286, 914, 11, 411, 389, 16, 11, 389, 17, 11, 389, 18, 11, 291, 393, 767, 362, 257, 2596, 11, 293, 550, 291, 393, 362, 264, 1440, 51392], "temperature": 0.0, "avg_logprob": -0.19023960515072472, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.06951536983251572}, {"id": 181, "seek": 61894, "start": 639.5, "end": 640.5, "text": " be the attribute.", "tokens": [51392, 312, 264, 19667, 13, 51442], "temperature": 0.0, "avg_logprob": -0.19023960515072472, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.06951536983251572}, {"id": 182, "seek": 61894, "start": 640.5, "end": 645.62, "text": " It does get a little bit harder to traverse it, but it can be done.", "tokens": [51442, 467, 775, 483, 257, 707, 857, 6081, 281, 45674, 309, 11, 457, 309, 393, 312, 1096, 13, 51698], "temperature": 0.0, "avg_logprob": -0.19023960515072472, "compression_ratio": 1.6626984126984128, "no_speech_prob": 0.06951536983251572}, {"id": 183, "seek": 64562, "start": 645.62, "end": 653.58, "text": " So anyway, Pagan doesn't allow for properties like that, and neither does Peg, actually.", "tokens": [50364, 407, 4033, 11, 430, 14167, 1177, 380, 2089, 337, 7221, 411, 300, 11, 293, 9662, 775, 28007, 11, 767, 13, 50762], "temperature": 0.0, "avg_logprob": -0.37504668313948836, "compression_ratio": 1.59765625, "no_speech_prob": 0.024420931935310364}, {"id": 184, "seek": 64562, "start": 653.58, "end": 659.18, "text": " Strictly speaking, though, I mean, you could use Peg or Pagan, and then just...", "tokens": [50762, 745, 3740, 356, 4124, 11, 1673, 11, 286, 914, 11, 291, 727, 764, 28007, 420, 430, 14167, 11, 293, 550, 445, 485, 51042], "temperature": 0.0, "avg_logprob": -0.37504668313948836, "compression_ratio": 1.59765625, "no_speech_prob": 0.024420931935310364}, {"id": 185, "seek": 64562, "start": 659.18, "end": 661.34, "text": " I mean, the AST is...", "tokens": [51042, 286, 914, 11, 264, 316, 6840, 307, 485, 51150], "temperature": 0.0, "avg_logprob": -0.37504668313948836, "compression_ratio": 1.59765625, "no_speech_prob": 0.024420931935310364}, {"id": 186, "seek": 64562, "start": 661.34, "end": 666.3, "text": " You could use your own AST instead of the one that I have in here, if you wanted.", "tokens": [51150, 509, 727, 764, 428, 1065, 316, 6840, 2602, 295, 264, 472, 300, 286, 362, 294, 510, 11, 498, 291, 1415, 13, 51398], "temperature": 0.0, "avg_logprob": -0.37504668313948836, "compression_ratio": 1.59765625, "no_speech_prob": 0.024420931935310364}, {"id": 187, "seek": 64562, "start": 666.3, "end": 668.0600000000001, "text": " But original Peg lacks specificity.", "tokens": [51398, 583, 3380, 28007, 31132, 2685, 507, 13, 51486], "temperature": 0.0, "avg_logprob": -0.37504668313948836, "compression_ratio": 1.59765625, "no_speech_prob": 0.024420931935310364}, {"id": 188, "seek": 64562, "start": 668.0600000000001, "end": 670.9, "text": " For yours, ABNF and ABNF provided a cruising level of specificity.", "tokens": [51486, 1171, 6342, 11, 13838, 45, 37, 293, 13838, 45, 37, 5649, 257, 5140, 3436, 1496, 295, 2685, 507, 13, 51628], "temperature": 0.0, "avg_logprob": -0.37504668313948836, "compression_ratio": 1.59765625, "no_speech_prob": 0.024420931935310364}, {"id": 189, "seek": 64562, "start": 670.9, "end": 673.9, "text": " Yeah, I don't trust this channel.", "tokens": [51628, 865, 11, 286, 500, 380, 3361, 341, 2269, 13, 51778], "temperature": 0.0, "avg_logprob": -0.37504668313948836, "compression_ratio": 1.59765625, "no_speech_prob": 0.024420931935310364}, {"id": 190, "seek": 67390, "start": 673.9, "end": 676.14, "text": " You have to switch, yes.", "tokens": [50364, 509, 362, 281, 3679, 11, 2086, 13, 50476], "temperature": 0.0, "avg_logprob": -0.17840422958624166, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.19174429774284363}, {"id": 191, "seek": 67390, "start": 676.14, "end": 678.14, "text": " But it's very stateful.", "tokens": [50476, 583, 309, 311, 588, 1785, 906, 13, 50576], "temperature": 0.0, "avg_logprob": -0.17840422958624166, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.19174429774284363}, {"id": 192, "seek": 67390, "start": 678.14, "end": 681.54, "text": " So it's like one state at a time, and that's what I like about it.", "tokens": [50576, 407, 309, 311, 411, 472, 1785, 412, 257, 565, 11, 293, 300, 311, 437, 286, 411, 466, 309, 13, 50746], "temperature": 0.0, "avg_logprob": -0.17840422958624166, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.19174429774284363}, {"id": 193, "seek": 67390, "start": 681.54, "end": 684.5, "text": " It's not some variation on the state.", "tokens": [50746, 467, 311, 406, 512, 12990, 322, 264, 1785, 13, 50894], "temperature": 0.0, "avg_logprob": -0.17840422958624166, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.19174429774284363}, {"id": 194, "seek": 67390, "start": 684.5, "end": 689.42, "text": " You see all different variations in node trees when people are doing this kind of thing.", "tokens": [50894, 509, 536, 439, 819, 17840, 294, 9984, 5852, 562, 561, 366, 884, 341, 733, 295, 551, 13, 51140], "temperature": 0.0, "avg_logprob": -0.17840422958624166, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.19174429774284363}, {"id": 195, "seek": 67390, "start": 689.42, "end": 693.86, "text": " There's people that will make all the nodes the same, and then they'll have attributes", "tokens": [51140, 821, 311, 561, 300, 486, 652, 439, 264, 13891, 264, 912, 11, 293, 550, 436, 603, 362, 17212, 51362], "temperature": 0.0, "avg_logprob": -0.17840422958624166, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.19174429774284363}, {"id": 196, "seek": 67390, "start": 693.86, "end": 697.18, "text": " on each of the nodes, and that's the only distinction between all the nodes.", "tokens": [51362, 322, 1184, 295, 264, 13891, 11, 293, 300, 311, 264, 787, 16844, 1296, 439, 264, 13891, 13, 51528], "temperature": 0.0, "avg_logprob": -0.17840422958624166, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.19174429774284363}, {"id": 197, "seek": 67390, "start": 697.18, "end": 699.46, "text": " And there's a million ways to model this.", "tokens": [51528, 400, 456, 311, 257, 2459, 2098, 281, 2316, 341, 13, 51642], "temperature": 0.0, "avg_logprob": -0.17840422958624166, "compression_ratio": 1.763779527559055, "no_speech_prob": 0.19174429774284363}, {"id": 198, "seek": 69946, "start": 699.46, "end": 701.5, "text": " And my favorite is to just...", "tokens": [50364, 400, 452, 2954, 307, 281, 445, 485, 50466], "temperature": 0.0, "avg_logprob": -0.20371188411006222, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.01744033955037594}, {"id": 199, "seek": 69946, "start": 701.5, "end": 707.5400000000001, "text": " You either have a node that's apparent that has children, or you have a node that is a", "tokens": [50466, 509, 2139, 362, 257, 9984, 300, 311, 18335, 300, 575, 2227, 11, 420, 291, 362, 257, 9984, 300, 307, 257, 50768], "temperature": 0.0, "avg_logprob": -0.20371188411006222, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.01744033955037594}, {"id": 200, "seek": 69946, "start": 707.5400000000001, "end": 712.58, "text": " leaf and that is an attribute or whatever in this case.", "tokens": [50768, 10871, 293, 300, 307, 364, 19667, 420, 2035, 294, 341, 1389, 13, 51020], "temperature": 0.0, "avg_logprob": -0.20371188411006222, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.01744033955037594}, {"id": 201, "seek": 69946, "start": 712.58, "end": 716.5, "text": " So anyway, ABNF and ABNF provided a cruising level of specificity in their grammars, but", "tokens": [51020, 407, 4033, 11, 13838, 45, 37, 293, 13838, 45, 37, 5649, 257, 5140, 3436, 1496, 295, 2685, 507, 294, 641, 17570, 685, 11, 457, 51216], "temperature": 0.0, "avg_logprob": -0.20371188411006222, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.01744033955037594}, {"id": 202, "seek": 69946, "start": 716.5, "end": 721.14, "text": " lack of obvious advantages of order priority and a simplicity of the original ASCII Peg", "tokens": [51216, 5011, 295, 6322, 14906, 295, 1668, 9365, 293, 257, 25632, 295, 264, 3380, 7469, 34, 9503, 28007, 51448], "temperature": 0.0, "avg_logprob": -0.20371188411006222, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.01744033955037594}, {"id": 203, "seek": 69946, "start": 721.14, "end": 722.14, "text": " grammar.", "tokens": [51448, 22317, 13, 51498], "temperature": 0.0, "avg_logprob": -0.20371188411006222, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.01744033955037594}, {"id": 204, "seek": 69946, "start": 722.14, "end": 726.7800000000001, "text": " For example, Pagan adds count and min max to provide limits and adds unicode tokens.", "tokens": [51498, 1171, 1365, 11, 430, 14167, 10860, 1207, 293, 923, 11469, 281, 2893, 10406, 293, 10860, 517, 299, 1429, 22667, 13, 51730], "temperature": 0.0, "avg_logprob": -0.20371188411006222, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.01744033955037594}, {"id": 205, "seek": 69946, "start": 726.7800000000001, "end": 729.14, "text": " So there's no unicode tokens in ABNF at all.", "tokens": [51730, 407, 456, 311, 572, 517, 299, 1429, 22667, 294, 13838, 45, 37, 412, 439, 13, 51848], "temperature": 0.0, "avg_logprob": -0.20371188411006222, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.01744033955037594}, {"id": 206, "seek": 72914, "start": 729.14, "end": 731.86, "text": " You have to do everything based on that.", "tokens": [50364, 509, 362, 281, 360, 1203, 2361, 322, 300, 13, 50500], "temperature": 0.0, "avg_logprob": -0.20114302846182763, "compression_ratio": 1.6194331983805668, "no_speech_prob": 0.01168507244437933}, {"id": 207, "seek": 72914, "start": 731.86, "end": 740.3, "text": " In fact, they created an extension to ABNF to allow for...", "tokens": [50500, 682, 1186, 11, 436, 2942, 364, 10320, 281, 13838, 45, 37, 281, 2089, 337, 485, 50922], "temperature": 0.0, "avg_logprob": -0.20114302846182763, "compression_ratio": 1.6194331983805668, "no_speech_prob": 0.01168507244437933}, {"id": 208, "seek": 72914, "start": 740.3, "end": 744.6999999999999, "text": " There's one of the data things from ABNF had to be expanded.", "tokens": [50922, 821, 311, 472, 295, 264, 1412, 721, 490, 13838, 45, 37, 632, 281, 312, 14342, 13, 51142], "temperature": 0.0, "avg_logprob": -0.20114302846182763, "compression_ratio": 1.6194331983805668, "no_speech_prob": 0.01168507244437933}, {"id": 209, "seek": 72914, "start": 744.6999999999999, "end": 748.1, "text": " I actually wrote a VIM syntax header for ABNF if anybody's wondering too.", "tokens": [51142, 286, 767, 4114, 257, 691, 6324, 28431, 23117, 337, 13838, 45, 37, 498, 4472, 311, 6359, 886, 13, 51312], "temperature": 0.0, "avg_logprob": -0.20114302846182763, "compression_ratio": 1.6194331983805668, "no_speech_prob": 0.01168507244437933}, {"id": 210, "seek": 72914, "start": 748.1, "end": 752.54, "text": " If you ever just want to open an ABNF VIM file and have it actually make sense, then", "tokens": [51312, 759, 291, 1562, 445, 528, 281, 1269, 364, 13838, 45, 37, 691, 6324, 3991, 293, 362, 309, 767, 652, 2020, 11, 550, 51534], "temperature": 0.0, "avg_logprob": -0.20114302846182763, "compression_ratio": 1.6194331983805668, "no_speech_prob": 0.01168507244437933}, {"id": 211, "seek": 72914, "start": 752.54, "end": 756.5, "text": " I updated it to include some of the new stuff from the latest extension to ABNF.", "tokens": [51534, 286, 10588, 309, 281, 4090, 512, 295, 264, 777, 1507, 490, 264, 6792, 10320, 281, 13838, 45, 37, 13, 51732], "temperature": 0.0, "avg_logprob": -0.20114302846182763, "compression_ratio": 1.6194331983805668, "no_speech_prob": 0.01168507244437933}, {"id": 212, "seek": 75650, "start": 756.5, "end": 760.84, "text": " I was obsessed with ABNF for a long time because I was just looking for a way to specify", "tokens": [50364, 286, 390, 16923, 365, 13838, 45, 37, 337, 257, 938, 565, 570, 286, 390, 445, 1237, 337, 257, 636, 281, 16500, 50581], "temperature": 0.0, "avg_logprob": -0.16525065898895264, "compression_ratio": 1.7625, "no_speech_prob": 0.287606418132782}, {"id": 213, "seek": 75650, "start": 760.84, "end": 764.14, "text": " grammars and those are the only ways until I made Pagan.", "tokens": [50581, 17570, 685, 293, 729, 366, 264, 787, 2098, 1826, 286, 1027, 430, 14167, 13, 50746], "temperature": 0.0, "avg_logprob": -0.16525065898895264, "compression_ratio": 1.7625, "no_speech_prob": 0.287606418132782}, {"id": 214, "seek": 75650, "start": 764.14, "end": 767.94, "text": " So the hope is that Pagan's language itself can be more explicit, better performing and", "tokens": [50746, 407, 264, 1454, 307, 300, 430, 14167, 311, 2856, 2564, 393, 312, 544, 13691, 11, 1101, 10205, 293, 50936], "temperature": 0.0, "avg_logprob": -0.16525065898895264, "compression_ratio": 1.7625, "no_speech_prob": 0.287606418132782}, {"id": 215, "seek": 75650, "start": 767.94, "end": 771.86, "text": " readable replacement for grammar, meta languages, and inline regular expressions.", "tokens": [50936, 49857, 14419, 337, 22317, 11, 19616, 8650, 11, 293, 294, 1889, 3890, 15277, 13, 51132], "temperature": 0.0, "avg_logprob": -0.16525065898895264, "compression_ratio": 1.7625, "no_speech_prob": 0.287606418132782}, {"id": 216, "seek": 75650, "start": 771.86, "end": 774.7, "text": " Code generators producing parses of different types and different implementation languages", "tokens": [51132, 15549, 38662, 10501, 21156, 279, 295, 819, 3467, 293, 819, 11420, 8650, 51274], "temperature": 0.0, "avg_logprob": -0.16525065898895264, "compression_ratio": 1.7625, "no_speech_prob": 0.287606418132782}, {"id": 217, "seek": 75650, "start": 774.7, "end": 778.02, "text": " can be created from the same grammar specification expressed in Pagan.", "tokens": [51274, 393, 312, 2942, 490, 264, 912, 22317, 31256, 12675, 294, 430, 14167, 13, 51440], "temperature": 0.0, "avg_logprob": -0.16525065898895264, "compression_ratio": 1.7625, "no_speech_prob": 0.287606418132782}, {"id": 218, "seek": 75650, "start": 778.02, "end": 782.34, "text": " In other words, you can take a Pagan thing and you can run it through a code generator", "tokens": [51440, 682, 661, 2283, 11, 291, 393, 747, 257, 430, 14167, 551, 293, 291, 393, 1190, 309, 807, 257, 3089, 19265, 51656], "temperature": 0.0, "avg_logprob": -0.16525065898895264, "compression_ratio": 1.7625, "no_speech_prob": 0.287606418132782}, {"id": 219, "seek": 78234, "start": 782.34, "end": 784.62, "text": " and generate Ruby code.", "tokens": [50364, 293, 8460, 19907, 3089, 13, 50478], "temperature": 0.0, "avg_logprob": -0.20565909046237751, "compression_ratio": 1.800711743772242, "no_speech_prob": 0.13289965689182281}, {"id": 220, "seek": 78234, "start": 784.62, "end": 785.82, "text": " You could generate Rust code.", "tokens": [50478, 509, 727, 8460, 34952, 3089, 13, 50538], "temperature": 0.0, "avg_logprob": -0.20565909046237751, "compression_ratio": 1.800711743772242, "no_speech_prob": 0.13289965689182281}, {"id": 221, "seek": 78234, "start": 785.82, "end": 786.98, "text": " You could generate Perl code.", "tokens": [50538, 509, 727, 8460, 3026, 75, 3089, 13, 50596], "temperature": 0.0, "avg_logprob": -0.20565909046237751, "compression_ratio": 1.800711743772242, "no_speech_prob": 0.13289965689182281}, {"id": 222, "seek": 78234, "start": 786.98, "end": 788.4200000000001, "text": " You could generate C code.", "tokens": [50596, 509, 727, 8460, 383, 3089, 13, 50668], "temperature": 0.0, "avg_logprob": -0.20565909046237751, "compression_ratio": 1.800711743772242, "no_speech_prob": 0.13289965689182281}, {"id": 223, "seek": 78234, "start": 788.4200000000001, "end": 792.4200000000001, "text": " It doesn't matter because there's nothing specific, language specific in Pagan and that's the", "tokens": [50668, 467, 1177, 380, 1871, 570, 456, 311, 1825, 2685, 11, 2856, 2685, 294, 430, 14167, 293, 300, 311, 264, 50868], "temperature": 0.0, "avg_logprob": -0.20565909046237751, "compression_ratio": 1.800711743772242, "no_speech_prob": 0.13289965689182281}, {"id": 224, "seek": 78234, "start": 792.4200000000001, "end": 797.22, "text": " biggest selling point of all of the whole thing.", "tokens": [50868, 3880, 6511, 935, 295, 439, 295, 264, 1379, 551, 13, 51108], "temperature": 0.0, "avg_logprob": -0.20565909046237751, "compression_ratio": 1.800711743772242, "no_speech_prob": 0.13289965689182281}, {"id": 225, "seek": 78234, "start": 797.22, "end": 800.58, "text": " Pagan parses and center libraries can even provide highly optimized handling of Pagan", "tokens": [51108, 430, 14167, 21156, 279, 293, 3056, 15148, 393, 754, 2893, 5405, 26941, 13175, 295, 430, 14167, 51276], "temperature": 0.0, "avg_logprob": -0.20565909046237751, "compression_ratio": 1.800711743772242, "no_speech_prob": 0.13289965689182281}, {"id": 226, "seek": 78234, "start": 800.58, "end": 804.98, "text": " grammars, including directly in code as strings and constants, much like compiled regular", "tokens": [51276, 17570, 685, 11, 3009, 3838, 294, 3089, 382, 13985, 293, 35870, 11, 709, 411, 36548, 3890, 51496], "temperature": 0.0, "avg_logprob": -0.20565909046237751, "compression_ratio": 1.800711743772242, "no_speech_prob": 0.13289965689182281}, {"id": 227, "seek": 78234, "start": 804.98, "end": 808.38, "text": " expressions are handled today, but with much greater clarity and efficiency.", "tokens": [51496, 15277, 366, 18033, 965, 11, 457, 365, 709, 5044, 16992, 293, 10493, 13, 51666], "temperature": 0.0, "avg_logprob": -0.20565909046237751, "compression_ratio": 1.800711743772242, "no_speech_prob": 0.13289965689182281}, {"id": 228, "seek": 80838, "start": 808.38, "end": 815.54, "text": " So for example, when you use a regular expression, you have the parenthesized list, but it's a", "tokens": [50364, 407, 337, 1365, 11, 562, 291, 764, 257, 3890, 6114, 11, 291, 362, 264, 23350, 279, 1602, 1329, 11, 457, 309, 311, 257, 50722], "temperature": 0.0, "avg_logprob": -0.16188789136482007, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.23629583418369293}, {"id": 229, "seek": 80838, "start": 815.54, "end": 817.06, "text": " linked list.", "tokens": [50722, 9408, 1329, 13, 50798], "temperature": 0.0, "avg_logprob": -0.16188789136482007, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.23629583418369293}, {"id": 230, "seek": 80838, "start": 817.06, "end": 819.06, "text": " It's not a structure.", "tokens": [50798, 467, 311, 406, 257, 3877, 13, 50898], "temperature": 0.0, "avg_logprob": -0.16188789136482007, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.23629583418369293}, {"id": 231, "seek": 80838, "start": 819.06, "end": 822.98, "text": " It does kind of make one, but you just get an array out of it.", "tokens": [50898, 467, 775, 733, 295, 652, 472, 11, 457, 291, 445, 483, 364, 10225, 484, 295, 309, 13, 51094], "temperature": 0.0, "avg_logprob": -0.16188789136482007, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.23629583418369293}, {"id": 232, "seek": 80838, "start": 822.98, "end": 827.1, "text": " You have to know the number and you have to figure out where your parentheses are inside", "tokens": [51094, 509, 362, 281, 458, 264, 1230, 293, 291, 362, 281, 2573, 484, 689, 428, 34153, 366, 1854, 51300], "temperature": 0.0, "avg_logprob": -0.16188789136482007, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.23629583418369293}, {"id": 233, "seek": 80838, "start": 827.1, "end": 831.58, "text": " of this other thing to see whether it's number one or two because you have to unpack the", "tokens": [51300, 295, 341, 661, 551, 281, 536, 1968, 309, 311, 1230, 472, 420, 732, 570, 291, 362, 281, 26699, 264, 51524], "temperature": 0.0, "avg_logprob": -0.16188789136482007, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.23629583418369293}, {"id": 234, "seek": 83158, "start": 831.58, "end": 838.98, "text": " parentheses in your brain in order to understand what integer it becomes in terms of index.", "tokens": [50364, 34153, 294, 428, 3567, 294, 1668, 281, 1223, 437, 24922, 309, 3643, 294, 2115, 295, 8186, 13, 50734], "temperature": 0.0, "avg_logprob": -0.14475488662719727, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.19188255071640015}, {"id": 235, "seek": 83158, "start": 838.98, "end": 842.62, "text": " And with Pagan, you don't have to do that because you get an AST out of it.", "tokens": [50734, 400, 365, 430, 14167, 11, 291, 500, 380, 362, 281, 360, 300, 570, 291, 483, 364, 316, 6840, 484, 295, 309, 13, 50916], "temperature": 0.0, "avg_logprob": -0.14475488662719727, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.19188255071640015}, {"id": 236, "seek": 83158, "start": 842.62, "end": 844.9000000000001, "text": " You get a standardized AST out of it every time.", "tokens": [50916, 509, 483, 257, 31677, 316, 6840, 484, 295, 309, 633, 565, 13, 51030], "temperature": 0.0, "avg_logprob": -0.14475488662719727, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.19188255071640015}, {"id": 237, "seek": 83158, "start": 844.9000000000001, "end": 847.22, "text": " You always get the same one and that is a structure.", "tokens": [51030, 509, 1009, 483, 264, 912, 472, 293, 300, 307, 257, 3877, 13, 51146], "temperature": 0.0, "avg_logprob": -0.14475488662719727, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.19188255071640015}, {"id": 238, "seek": 83158, "start": 847.22, "end": 851.7800000000001, "text": " It's a full structure that you can walk however you want and do it.", "tokens": [51146, 467, 311, 257, 1577, 3877, 300, 291, 393, 1792, 4461, 291, 528, 293, 360, 309, 13, 51374], "temperature": 0.0, "avg_logprob": -0.14475488662719727, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.19188255071640015}, {"id": 239, "seek": 83158, "start": 851.7800000000001, "end": 854.7800000000001, "text": " So that, in my opinion, might be slightly slower than regular expressions, but it gives", "tokens": [51374, 407, 300, 11, 294, 452, 4800, 11, 1062, 312, 4748, 14009, 813, 3890, 15277, 11, 457, 309, 2709, 51524], "temperature": 0.0, "avg_logprob": -0.14475488662719727, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.19188255071640015}, {"id": 240, "seek": 83158, "start": 854.7800000000001, "end": 859.1, "text": " you more power when you're dealing with grammars and specifying them.", "tokens": [51524, 291, 544, 1347, 562, 291, 434, 6260, 365, 17570, 685, 293, 1608, 5489, 552, 13, 51740], "temperature": 0.0, "avg_logprob": -0.14475488662719727, "compression_ratio": 1.6952054794520548, "no_speech_prob": 0.19188255071640015}, {"id": 241, "seek": 85910, "start": 859.46, "end": 864.02, "text": " A progressive best example is what is itself, which is specified in Pagan.", "tokens": [50382, 316, 16131, 1151, 1365, 307, 437, 307, 2564, 11, 597, 307, 22206, 294, 430, 14167, 13, 50610], "temperature": 0.0, "avg_logprob": -0.2832209579343718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06751108169555664}, {"id": 242, "seek": 85910, "start": 864.02, "end": 866.62, "text": " Here's another example, the JSON specification in Pagan.", "tokens": [50610, 1692, 311, 1071, 1365, 11, 264, 31828, 31256, 294, 430, 14167, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2832209579343718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06751108169555664}, {"id": 243, "seek": 85910, "start": 866.62, "end": 873.66, "text": " So Pagan is itself specified in Pagan and this is an old one.", "tokens": [50740, 407, 430, 14167, 307, 2564, 22206, 294, 430, 14167, 293, 341, 307, 364, 1331, 472, 13, 51092], "temperature": 0.0, "avg_logprob": -0.2832209579343718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06751108169555664}, {"id": 244, "seek": 85910, "start": 873.66, "end": 877.82, "text": " Yeah, this is, that link is broken.", "tokens": [51092, 865, 11, 341, 307, 11, 300, 2113, 307, 5463, 13, 51300], "temperature": 0.0, "avg_logprob": -0.2832209579343718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06751108169555664}, {"id": 245, "seek": 85910, "start": 877.82, "end": 880.4200000000001, "text": " I got to fix that, obviously.", "tokens": [51300, 286, 658, 281, 3191, 300, 11, 2745, 13, 51430], "temperature": 0.0, "avg_logprob": -0.2832209579343718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06751108169555664}, {"id": 246, "seek": 85910, "start": 880.4200000000001, "end": 882.02, "text": " I haven't picked up Pagan for a long time.", "tokens": [51430, 286, 2378, 380, 6183, 493, 430, 14167, 337, 257, 938, 565, 13, 51510], "temperature": 0.0, "avg_logprob": -0.2832209579343718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06751108169555664}, {"id": 247, "seek": 85910, "start": 882.02, "end": 884.5400000000001, "text": " I've used it for lots of things, but this site needs to be fixed.", "tokens": [51510, 286, 600, 1143, 309, 337, 3195, 295, 721, 11, 457, 341, 3621, 2203, 281, 312, 6806, 13, 51636], "temperature": 0.0, "avg_logprob": -0.2832209579343718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06751108169555664}, {"id": 248, "seek": 85910, "start": 884.5400000000001, "end": 889.0600000000001, "text": " So here's the JSON RFC according to RCA 259.", "tokens": [51636, 407, 510, 311, 264, 31828, 497, 18671, 4650, 281, 497, 15515, 3552, 24, 13, 51862], "temperature": 0.0, "avg_logprob": -0.2832209579343718, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06751108169555664}, {"id": 249, "seek": 88906, "start": 889.06, "end": 892.5, "text": " This is the implementation of JSON in Pagan.", "tokens": [50364, 639, 307, 264, 11420, 295, 31828, 294, 430, 14167, 13, 50536], "temperature": 0.0, "avg_logprob": -0.22295562222472623, "compression_ratio": 1.75, "no_speech_prob": 0.000855801859870553}, {"id": 250, "seek": 88906, "start": 892.5, "end": 896.8599999999999, "text": " And you have the overall grammar, grammar is a conventional name for the top level node.", "tokens": [50536, 400, 291, 362, 264, 4787, 22317, 11, 22317, 307, 257, 16011, 1315, 337, 264, 1192, 1496, 9984, 13, 50754], "temperature": 0.0, "avg_logprob": -0.22295562222472623, "compression_ratio": 1.75, "no_speech_prob": 0.000855801859870553}, {"id": 251, "seek": 88906, "start": 896.8599999999999, "end": 902.18, "text": " And then you have the white space and then a value or whatever and then more white space.", "tokens": [50754, 400, 550, 291, 362, 264, 2418, 1901, 293, 550, 257, 2158, 420, 2035, 293, 550, 544, 2418, 1901, 13, 51020], "temperature": 0.0, "avg_logprob": -0.22295562222472623, "compression_ratio": 1.75, "no_speech_prob": 0.000855801859870553}, {"id": 252, "seek": 88906, "start": 902.18, "end": 905.42, "text": " And I use the actual terminology from the thing.", "tokens": [51020, 400, 286, 764, 264, 3539, 27575, 490, 264, 551, 13, 51182], "temperature": 0.0, "avg_logprob": -0.22295562222472623, "compression_ratio": 1.75, "no_speech_prob": 0.000855801859870553}, {"id": 253, "seek": 88906, "start": 905.42, "end": 908.9799999999999, "text": " There are syntax conventions for the name.", "tokens": [51182, 821, 366, 28431, 33520, 337, 264, 1315, 13, 51360], "temperature": 0.0, "avg_logprob": -0.22295562222472623, "compression_ratio": 1.75, "no_speech_prob": 0.000855801859870553}, {"id": 254, "seek": 88906, "start": 908.9799999999999, "end": 915.02, "text": " So if they're initial caps, those are kind of easily to identify.", "tokens": [51360, 407, 498, 436, 434, 5883, 13855, 11, 729, 366, 733, 295, 3612, 281, 5876, 13, 51662], "temperature": 0.0, "avg_logprob": -0.22295562222472623, "compression_ratio": 1.75, "no_speech_prob": 0.000855801859870553}, {"id": 255, "seek": 88906, "start": 915.02, "end": 918.7399999999999, "text": " The other couple of things when you have two, I don't want to teach you all Pagan right", "tokens": [51662, 440, 661, 1916, 295, 721, 562, 291, 362, 732, 11, 286, 500, 380, 528, 281, 2924, 291, 439, 430, 14167, 558, 51848], "temperature": 0.0, "avg_logprob": -0.22295562222472623, "compression_ratio": 1.75, "no_speech_prob": 0.000855801859870553}, {"id": 256, "seek": 91874, "start": 918.74, "end": 922.78, "text": " now, take forever, but two dashes, this is pretty significant.", "tokens": [50364, 586, 11, 747, 5680, 11, 457, 732, 8240, 279, 11, 341, 307, 1238, 4776, 13, 50566], "temperature": 0.0, "avg_logprob": -0.17473312826717602, "compression_ratio": 1.581151832460733, "no_speech_prob": 0.006691179238259792}, {"id": 257, "seek": 91874, "start": 922.78, "end": 932.86, "text": " The two dashes signifies a substantial node or a semantic, what I call a semantic node.", "tokens": [50566, 440, 732, 8240, 279, 1465, 11221, 257, 16726, 9984, 420, 257, 47982, 11, 437, 286, 818, 257, 47982, 9984, 13, 51070], "temperature": 0.0, "avg_logprob": -0.17473312826717602, "compression_ratio": 1.581151832460733, "no_speech_prob": 0.006691179238259792}, {"id": 258, "seek": 91874, "start": 932.86, "end": 935.1, "text": " That means capture it.", "tokens": [51070, 663, 1355, 7983, 309, 13, 51182], "temperature": 0.0, "avg_logprob": -0.17473312826717602, "compression_ratio": 1.581151832460733, "no_speech_prob": 0.006691179238259792}, {"id": 259, "seek": 91874, "start": 935.1, "end": 942.3, "text": " If we don't have that, it's just a simplification of code so that, you know, the spec of the", "tokens": [51182, 759, 321, 500, 380, 362, 300, 11, 309, 311, 445, 257, 6883, 3774, 295, 3089, 370, 300, 11, 291, 458, 11, 264, 1608, 295, 264, 51542], "temperature": 0.0, "avg_logprob": -0.17473312826717602, "compression_ratio": 1.581151832460733, "no_speech_prob": 0.006691179238259792}, {"id": 260, "seek": 91874, "start": 942.3, "end": 944.54, "text": " Pagan spec code can become simpler.", "tokens": [51542, 430, 14167, 1608, 3089, 393, 1813, 18587, 13, 51654], "temperature": 0.0, "avg_logprob": -0.17473312826717602, "compression_ratio": 1.581151832460733, "no_speech_prob": 0.006691179238259792}, {"id": 261, "seek": 94454, "start": 944.54, "end": 948.8199999999999, "text": " So anytime you see value, value is all of these things, right?", "tokens": [50364, 407, 13038, 291, 536, 2158, 11, 2158, 307, 439, 295, 613, 721, 11, 558, 30, 50578], "temperature": 0.0, "avg_logprob": -0.17406485373513741, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.015423153527081013}, {"id": 262, "seek": 94454, "start": 948.8199999999999, "end": 952.78, "text": " But this says don't make value a node.", "tokens": [50578, 583, 341, 1619, 500, 380, 652, 2158, 257, 9984, 13, 50776], "temperature": 0.0, "avg_logprob": -0.17406485373513741, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.015423153527081013}, {"id": 263, "seek": 94454, "start": 952.78, "end": 957.42, "text": " It just says this is how we refer to value so I can put value down here later.", "tokens": [50776, 467, 445, 1619, 341, 307, 577, 321, 2864, 281, 2158, 370, 286, 393, 829, 2158, 760, 510, 1780, 13, 51008], "temperature": 0.0, "avg_logprob": -0.17406485373513741, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.015423153527081013}, {"id": 264, "seek": 94454, "start": 957.42, "end": 959.62, "text": " And then, but any of these things would be a node.", "tokens": [51008, 400, 550, 11, 457, 604, 295, 613, 721, 576, 312, 257, 9984, 13, 51118], "temperature": 0.0, "avg_logprob": -0.17406485373513741, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.015423153527081013}, {"id": 265, "seek": 94454, "start": 959.62, "end": 962.5799999999999, "text": " If there's a node, it would be implemented at that point, it would be captured.", "tokens": [51118, 759, 456, 311, 257, 9984, 11, 309, 576, 312, 12270, 412, 300, 935, 11, 309, 576, 312, 11828, 13, 51266], "temperature": 0.0, "avg_logprob": -0.17406485373513741, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.015423153527081013}, {"id": 266, "seek": 94454, "start": 962.5799999999999, "end": 970.18, "text": " So an object is a significant node and it would, you know, it's just, I mean, it's basically", "tokens": [51266, 407, 364, 2657, 307, 257, 4776, 9984, 293, 309, 576, 11, 291, 458, 11, 309, 311, 445, 11, 286, 914, 11, 309, 311, 1936, 51646], "temperature": 0.0, "avg_logprob": -0.17406485373513741, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.015423153527081013}, {"id": 267, "seek": 97018, "start": 970.18, "end": 971.18, "text": " the same as peg.", "tokens": [50364, 264, 912, 382, 17199, 13, 50414], "temperature": 0.0, "avg_logprob": -0.18546407650678587, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.20175091922283173}, {"id": 268, "seek": 97018, "start": 971.18, "end": 976.02, "text": " You get a bracket and then zero or more white spaces and then a member and then you go down", "tokens": [50414, 509, 483, 257, 16904, 293, 550, 4018, 420, 544, 2418, 7673, 293, 550, 257, 4006, 293, 550, 291, 352, 760, 50656], "temperature": 0.0, "avg_logprob": -0.18546407650678587, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.20175091922283173}, {"id": 269, "seek": 97018, "start": 976.02, "end": 977.02, "text": " and what's a member?", "tokens": [50656, 293, 437, 311, 257, 4006, 30, 50706], "temperature": 0.0, "avg_logprob": -0.18546407650678587, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.20175091922283173}, {"id": 270, "seek": 97018, "start": 977.02, "end": 984.14, "text": " A member is a string plus a colon plus a value and then you get, you know, zero or more", "tokens": [50706, 316, 4006, 307, 257, 6798, 1804, 257, 8255, 1804, 257, 2158, 293, 550, 291, 483, 11, 291, 458, 11, 4018, 420, 544, 51062], "temperature": 0.0, "avg_logprob": -0.18546407650678587, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.20175091922283173}, {"id": 271, "seek": 97018, "start": 984.14, "end": 987.9799999999999, "text": " of this, you know, sort of thing combined together.", "tokens": [51062, 295, 341, 11, 291, 458, 11, 1333, 295, 551, 9354, 1214, 13, 51254], "temperature": 0.0, "avg_logprob": -0.18546407650678587, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.20175091922283173}, {"id": 272, "seek": 97018, "start": 987.9799999999999, "end": 990.62, "text": " In that sense, if you know regular expressions, you can see this.", "tokens": [51254, 682, 300, 2020, 11, 498, 291, 458, 3890, 15277, 11, 291, 393, 536, 341, 13, 51386], "temperature": 0.0, "avg_logprob": -0.18546407650678587, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.20175091922283173}, {"id": 273, "seek": 97018, "start": 990.62, "end": 995.0999999999999, "text": " One of the things that's nice about this is it is a lot easier to read than regular expressions.", "tokens": [51386, 1485, 295, 264, 721, 300, 311, 1481, 466, 341, 307, 309, 307, 257, 688, 3571, 281, 1401, 813, 3890, 15277, 13, 51610], "temperature": 0.0, "avg_logprob": -0.18546407650678587, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.20175091922283173}, {"id": 274, "seek": 99510, "start": 995.1, "end": 1003.1, "text": " We do have an entire standard subset of predefined tokens like DQ and everything, which APNF", "tokens": [50364, 492, 360, 362, 364, 2302, 3832, 25993, 295, 659, 37716, 22667, 411, 413, 48, 293, 1203, 11, 597, 5372, 45, 37, 50764], "temperature": 0.0, "avg_logprob": -0.18234575857030283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.34848642349243164}, {"id": 275, "seek": 99510, "start": 1003.1, "end": 1007.1, "text": " has also done so you can just use those if you already know them.", "tokens": [50764, 575, 611, 1096, 370, 291, 393, 445, 764, 729, 498, 291, 1217, 458, 552, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18234575857030283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.34848642349243164}, {"id": 276, "seek": 99510, "start": 1007.1, "end": 1011.58, "text": " In fact, anything that's lowercase is considered a class and some of the classes can be used", "tokens": [50964, 682, 1186, 11, 1340, 300, 311, 3126, 9765, 307, 4888, 257, 1508, 293, 512, 295, 264, 5359, 393, 312, 1143, 51188], "temperature": 0.0, "avg_logprob": -0.18234575857030283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.34848642349243164}, {"id": 277, "seek": 99510, "start": 1011.58, "end": 1017.74, "text": " without specifying them because they're assumed there to be coming from the, the peg and specification", "tokens": [51188, 1553, 1608, 5489, 552, 570, 436, 434, 15895, 456, 281, 312, 1348, 490, 264, 11, 264, 17199, 293, 31256, 51496], "temperature": 0.0, "avg_logprob": -0.18234575857030283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.34848642349243164}, {"id": 278, "seek": 99510, "start": 1017.74, "end": 1018.74, "text": " itself.", "tokens": [51496, 2564, 13, 51546], "temperature": 0.0, "avg_logprob": -0.18234575857030283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.34848642349243164}, {"id": 279, "seek": 99510, "start": 1018.74, "end": 1020.94, "text": " So they're predefined such as WS.", "tokens": [51546, 407, 436, 434, 659, 37716, 1270, 382, 343, 50, 13, 51656], "temperature": 0.0, "avg_logprob": -0.18234575857030283, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.34848642349243164}, {"id": 280, "seek": 102094, "start": 1020.94, "end": 1032.98, "text": " And that includes every positive span range and things like that, digit and stuff.", "tokens": [50364, 400, 300, 5974, 633, 3353, 16174, 3613, 293, 721, 411, 300, 11, 14293, 293, 1507, 13, 50966], "temperature": 0.0, "avg_logprob": -0.2805277422854775, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.09532171487808228}, {"id": 281, "seek": 102094, "start": 1032.98, "end": 1035.5800000000002, "text": " And so, yeah, so digit is another class.", "tokens": [50966, 400, 370, 11, 1338, 11, 370, 14293, 307, 1071, 1508, 13, 51096], "temperature": 0.0, "avg_logprob": -0.2805277422854775, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.09532171487808228}, {"id": 282, "seek": 102094, "start": 1035.5800000000002, "end": 1042.54, "text": " Actually, I think digit is a token, you know, there's differences between classes and tokens.", "tokens": [51096, 5135, 11, 286, 519, 14293, 307, 257, 14862, 11, 291, 458, 11, 456, 311, 7300, 1296, 5359, 293, 22667, 13, 51444], "temperature": 0.0, "avg_logprob": -0.2805277422854775, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.09532171487808228}, {"id": 283, "seek": 102094, "start": 1042.54, "end": 1049.46, "text": " Class is like one of these set and the other one, a specific token is like this exact", "tokens": [51444, 9471, 307, 411, 472, 295, 613, 992, 293, 264, 661, 472, 11, 257, 2685, 14862, 307, 411, 341, 1900, 51790], "temperature": 0.0, "avg_logprob": -0.2805277422854775, "compression_ratio": 1.6117021276595744, "no_speech_prob": 0.09532171487808228}, {"id": 284, "seek": 104946, "start": 1049.46, "end": 1054.06, "text": " single character or, you know, three characters or something like that.", "tokens": [50364, 2167, 2517, 420, 11, 291, 458, 11, 1045, 4342, 420, 746, 411, 300, 13, 50594], "temperature": 0.0, "avg_logprob": -0.2104856790589892, "compression_ratio": 1.75, "no_speech_prob": 0.4450748562812805}, {"id": 285, "seek": 104946, "start": 1054.06, "end": 1058.5, "text": " And so, the other thing that's cool about peg and peg is that they're left to right", "tokens": [50594, 400, 370, 11, 264, 661, 551, 300, 311, 1627, 466, 17199, 293, 17199, 307, 300, 436, 434, 1411, 281, 558, 50816], "temperature": 0.0, "avg_logprob": -0.2104856790589892, "compression_ratio": 1.75, "no_speech_prob": 0.4450748562812805}, {"id": 286, "seek": 104946, "start": 1058.5, "end": 1062.8600000000001, "text": " versus ABNF and EBNF, which are like, when you specify something with a slash and EBNF", "tokens": [50816, 5717, 13838, 45, 37, 293, 462, 32006, 37, 11, 597, 366, 411, 11, 562, 291, 16500, 746, 365, 257, 17330, 293, 462, 32006, 37, 51034], "temperature": 0.0, "avg_logprob": -0.2104856790589892, "compression_ratio": 1.75, "no_speech_prob": 0.4450748562812805}, {"id": 287, "seek": 104946, "start": 1062.8600000000001, "end": 1066.98, "text": " and EBNF, it can be any of those things at any order.", "tokens": [51034, 293, 462, 32006, 37, 11, 309, 393, 312, 604, 295, 729, 721, 412, 604, 1668, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2104856790589892, "compression_ratio": 1.75, "no_speech_prob": 0.4450748562812805}, {"id": 288, "seek": 104946, "start": 1066.98, "end": 1073.94, "text": " And the really, really, really great simplification of peg is it's guaranteed to match first left", "tokens": [51240, 400, 264, 534, 11, 534, 11, 534, 869, 6883, 3774, 295, 17199, 307, 309, 311, 18031, 281, 2995, 700, 1411, 51588], "temperature": 0.0, "avg_logprob": -0.2104856790589892, "compression_ratio": 1.75, "no_speech_prob": 0.4450748562812805}, {"id": 289, "seek": 104946, "start": 1073.94, "end": 1075.1000000000001, "text": " wins.", "tokens": [51588, 10641, 13, 51646], "temperature": 0.0, "avg_logprob": -0.2104856790589892, "compression_ratio": 1.75, "no_speech_prob": 0.4450748562812805}, {"id": 290, "seek": 104946, "start": 1075.1000000000001, "end": 1076.58, "text": " So you put the stuff to the left.", "tokens": [51646, 407, 291, 829, 264, 1507, 281, 264, 1411, 13, 51720], "temperature": 0.0, "avg_logprob": -0.2104856790589892, "compression_ratio": 1.75, "no_speech_prob": 0.4450748562812805}, {"id": 291, "seek": 107658, "start": 1076.58, "end": 1085.02, "text": " So for example, in my specification for, for Kegumel, you know, wait, let me go see", "tokens": [50364, 407, 337, 1365, 11, 294, 452, 31256, 337, 11, 337, 591, 1146, 449, 338, 11, 291, 458, 11, 1699, 11, 718, 385, 352, 536, 50786], "temperature": 0.0, "avg_logprob": -0.2308784267766689, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.017440548166632652}, {"id": 292, "seek": 107658, "start": 1085.02, "end": 1086.02, "text": " you.", "tokens": [50786, 291, 13, 50836], "temperature": 0.0, "avg_logprob": -0.2308784267766689, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.017440548166632652}, {"id": 293, "seek": 107658, "start": 1086.02, "end": 1087.02, "text": " Where is my spec?", "tokens": [50836, 2305, 307, 452, 1608, 30, 50886], "temperature": 0.0, "avg_logprob": -0.2308784267766689, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.017440548166632652}, {"id": 294, "seek": 107658, "start": 1087.02, "end": 1088.02, "text": " Is it over here?", "tokens": [50886, 1119, 309, 670, 510, 30, 50936], "temperature": 0.0, "avg_logprob": -0.2308784267766689, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.017440548166632652}, {"id": 295, "seek": 107658, "start": 1088.02, "end": 1089.02, "text": " Okay.", "tokens": [50936, 1033, 13, 50986], "temperature": 0.0, "avg_logprob": -0.2308784267766689, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.017440548166632652}, {"id": 296, "seek": 107658, "start": 1089.02, "end": 1093.1, "text": " So in my specification for Kegumel, let's, let's do the, let's do the, the, you see here", "tokens": [50986, 407, 294, 452, 31256, 337, 591, 1146, 449, 338, 11, 718, 311, 11, 718, 311, 360, 264, 11, 718, 311, 360, 264, 11, 264, 11, 291, 536, 510, 51190], "temperature": 0.0, "avg_logprob": -0.2308784267766689, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.017440548166632652}, {"id": 297, "seek": 107658, "start": 1093.1, "end": 1097.1799999999998, "text": " I have bulleted and then numbered and then figure, right?", "tokens": [51190, 286, 362, 4693, 10993, 293, 550, 40936, 293, 550, 2573, 11, 558, 30, 51394], "temperature": 0.0, "avg_logprob": -0.2308784267766689, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.017440548166632652}, {"id": 298, "seek": 107658, "start": 1097.1799999999998, "end": 1104.5, "text": " Well, if I was going to do those bulleted, right, a bulleted list begins, I'm not bulleted,", "tokens": [51394, 1042, 11, 498, 286, 390, 516, 281, 360, 729, 4693, 10993, 11, 558, 11, 257, 4693, 10993, 1329, 7338, 11, 286, 478, 406, 4693, 10993, 11, 51760], "temperature": 0.0, "avg_logprob": -0.2308784267766689, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.017440548166632652}, {"id": 299, "seek": 110450, "start": 1104.5, "end": 1107.06, "text": " let's, let's do, let's do spans, spans are better.", "tokens": [50364, 718, 311, 11, 718, 311, 360, 11, 718, 311, 360, 44086, 11, 44086, 366, 1101, 13, 50492], "temperature": 0.0, "avg_logprob": -0.1944297186218866, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.07367288321256638}, {"id": 300, "seek": 110450, "start": 1107.06, "end": 1108.38, "text": " So let's do strong emphasis.", "tokens": [50492, 407, 718, 311, 360, 2068, 16271, 13, 50558], "temperature": 0.0, "avg_logprob": -0.1944297186218866, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.07367288321256638}, {"id": 301, "seek": 110450, "start": 1108.38, "end": 1113.9, "text": " The reason strong emphasis first, even though you might not want to list it that way is", "tokens": [50558, 440, 1778, 2068, 16271, 700, 11, 754, 1673, 291, 1062, 406, 528, 281, 1329, 309, 300, 636, 307, 50834], "temperature": 0.0, "avg_logprob": -0.1944297186218866, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.07367288321256638}, {"id": 302, "seek": 110450, "start": 1113.9, "end": 1121.94, "text": " because strong emphasis for me is three stars plus, you know, something and that this is", "tokens": [50834, 570, 2068, 16271, 337, 385, 307, 1045, 6105, 1804, 11, 291, 458, 11, 746, 293, 300, 341, 307, 51236], "temperature": 0.0, "avg_logprob": -0.1944297186218866, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.07367288321256638}, {"id": 303, "seek": 110450, "start": 1121.94, "end": 1124.02, "text": " the part that's kind of interesting.", "tokens": [51236, 264, 644, 300, 311, 733, 295, 1880, 13, 51340], "temperature": 0.0, "avg_logprob": -0.1944297186218866, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.07367288321256638}, {"id": 304, "seek": 110450, "start": 1124.02, "end": 1132.5, "text": " So right here, we're going to put a plane and so then we can put a plane.", "tokens": [51340, 407, 558, 510, 11, 321, 434, 516, 281, 829, 257, 5720, 293, 370, 550, 321, 393, 829, 257, 5720, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1944297186218866, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.07367288321256638}, {"id": 305, "seek": 113250, "start": 1132.5, "end": 1138.74, "text": " So it's a, you know, it's a plane span and followed by another token, right?", "tokens": [50364, 407, 309, 311, 257, 11, 291, 458, 11, 309, 311, 257, 5720, 16174, 293, 6263, 538, 1071, 14862, 11, 558, 30, 50676], "temperature": 0.0, "avg_logprob": -0.1772324779246113, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.03513650596141815}, {"id": 306, "seek": 113250, "start": 1138.74, "end": 1144.62, "text": " And so, so there, the reason that's the strongest are that the reason that that one comes first,", "tokens": [50676, 400, 370, 11, 370, 456, 11, 264, 1778, 300, 311, 264, 16595, 366, 300, 264, 1778, 300, 300, 472, 1487, 700, 11, 50970], "temperature": 0.0, "avg_logprob": -0.1772324779246113, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.03513650596141815}, {"id": 307, "seek": 113250, "start": 1144.62, "end": 1155.22, "text": " of course, is because the strong one has two and the emphasis one, which is italic is just", "tokens": [50970, 295, 1164, 11, 307, 570, 264, 2068, 472, 575, 732, 293, 264, 16271, 472, 11, 597, 307, 22366, 299, 307, 445, 51500], "temperature": 0.0, "avg_logprob": -0.1772324779246113, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.03513650596141815}, {"id": 308, "seek": 113250, "start": 1155.22, "end": 1156.22, "text": " one.", "tokens": [51500, 472, 13, 51550], "temperature": 0.0, "avg_logprob": -0.1772324779246113, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.03513650596141815}, {"id": 309, "seek": 113250, "start": 1156.22, "end": 1160.5, "text": " Now, what if I put, so if I was trying to specify this in EB enough, you see how problematic", "tokens": [51550, 823, 11, 437, 498, 286, 829, 11, 370, 498, 286, 390, 1382, 281, 16500, 341, 294, 50148, 1547, 11, 291, 536, 577, 19011, 51764], "temperature": 0.0, "avg_logprob": -0.1772324779246113, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.03513650596141815}, {"id": 310, "seek": 116050, "start": 1160.5, "end": 1161.7, "text": " this would be?", "tokens": [50364, 341, 576, 312, 30, 50424], "temperature": 0.0, "avg_logprob": -0.15875764046945878, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.17320531606674194}, {"id": 311, "seek": 116050, "start": 1161.7, "end": 1166.74, "text": " By just placing the order to check for each one, I can have tokens that would otherwise", "tokens": [50424, 3146, 445, 17221, 264, 1668, 281, 1520, 337, 1184, 472, 11, 286, 393, 362, 22667, 300, 576, 5911, 50676], "temperature": 0.0, "avg_logprob": -0.15875764046945878, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.17320531606674194}, {"id": 312, "seek": 116050, "start": 1166.74, "end": 1168.18, "text": " be included in the other tokens.", "tokens": [50676, 312, 5556, 294, 264, 661, 22667, 13, 50748], "temperature": 0.0, "avg_logprob": -0.15875764046945878, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.17320531606674194}, {"id": 313, "seek": 116050, "start": 1168.18, "end": 1171.7, "text": " This is where you get all the craziness with XML and everything because they don't have", "tokens": [50748, 639, 307, 689, 291, 483, 439, 264, 46348, 1324, 365, 43484, 293, 1203, 570, 436, 500, 380, 362, 50924], "temperature": 0.0, "avg_logprob": -0.15875764046945878, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.17320531606674194}, {"id": 314, "seek": 116050, "start": 1171.7, "end": 1175.74, "text": " the idea of, well, first look for this, and I can't tell you how valuable this is when", "tokens": [50924, 264, 1558, 295, 11, 731, 11, 700, 574, 337, 341, 11, 293, 286, 393, 380, 980, 291, 577, 8263, 341, 307, 562, 51126], "temperature": 0.0, "avg_logprob": -0.15875764046945878, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.17320531606674194}, {"id": 315, "seek": 116050, "start": 1175.74, "end": 1178.98, "text": " it comes to code generation or just writing the code by hand, because just by looking", "tokens": [51126, 309, 1487, 281, 3089, 5125, 420, 445, 3579, 264, 3089, 538, 1011, 11, 570, 445, 538, 1237, 51288], "temperature": 0.0, "avg_logprob": -0.15875764046945878, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.17320531606674194}, {"id": 316, "seek": 116050, "start": 1178.98, "end": 1185.5, "text": " at the peg and I know right away what comes first, I know that I need to write, I need", "tokens": [51288, 412, 264, 17199, 293, 286, 458, 558, 1314, 437, 1487, 700, 11, 286, 458, 300, 286, 643, 281, 2464, 11, 286, 643, 51614], "temperature": 0.0, "avg_logprob": -0.15875764046945878, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.17320531606674194}, {"id": 317, "seek": 118550, "start": 1185.5, "end": 1192.46, "text": " to check for a strong emphasis token before I check for an emphasis token so that I can", "tokens": [50364, 281, 1520, 337, 257, 2068, 16271, 14862, 949, 286, 1520, 337, 364, 16271, 14862, 370, 300, 286, 393, 50712], "temperature": 0.0, "avg_logprob": -0.18786937632459275, "compression_ratio": 1.781725888324873, "no_speech_prob": 0.7952533960342407}, {"id": 318, "seek": 118550, "start": 1192.46, "end": 1199.18, "text": " fail out or, you know, go to or hand off to the next, to the next parser, the next token", "tokens": [50712, 3061, 484, 420, 11, 291, 458, 11, 352, 281, 420, 1011, 766, 281, 264, 958, 11, 281, 264, 958, 21156, 260, 11, 264, 958, 14862, 51048], "temperature": 0.0, "avg_logprob": -0.18786937632459275, "compression_ratio": 1.781725888324873, "no_speech_prob": 0.7952533960342407}, {"id": 319, "seek": 118550, "start": 1199.18, "end": 1205.3, "text": " parser, because I have the order and I know, I keep talking about this, but that is such", "tokens": [51048, 21156, 260, 11, 570, 286, 362, 264, 1668, 293, 286, 458, 11, 286, 1066, 1417, 466, 341, 11, 457, 300, 307, 1270, 51354], "temperature": 0.0, "avg_logprob": -0.18786937632459275, "compression_ratio": 1.781725888324873, "no_speech_prob": 0.7952533960342407}, {"id": 320, "seek": 118550, "start": 1205.3, "end": 1210.9, "text": " a huge thing when it comes to writing parsers, because otherwise you just don't know.", "tokens": [51354, 257, 2603, 551, 562, 309, 1487, 281, 3579, 21156, 433, 11, 570, 5911, 291, 445, 500, 380, 458, 13, 51634], "temperature": 0.0, "avg_logprob": -0.18786937632459275, "compression_ratio": 1.781725888324873, "no_speech_prob": 0.7952533960342407}, {"id": 321, "seek": 121090, "start": 1210.9, "end": 1217.5400000000002, "text": " And it's really nice because it keeps the syntax of Kegamel or Bonzai.", "tokens": [50364, 400, 309, 311, 534, 1481, 570, 309, 5965, 264, 28431, 295, 591, 1146, 16103, 420, 7368, 89, 1301, 13, 50696], "temperature": 0.0, "avg_logprob": -0.19969335283551898, "compression_ratio": 1.6827794561933536, "no_speech_prob": 0.8513103723526001}, {"id": 322, "seek": 121090, "start": 1217.5400000000002, "end": 1218.5400000000002, "text": " Very simple.", "tokens": [50696, 4372, 2199, 13, 50746], "temperature": 0.0, "avg_logprob": -0.19969335283551898, "compression_ratio": 1.6827794561933536, "no_speech_prob": 0.8513103723526001}, {"id": 323, "seek": 121090, "start": 1218.5400000000002, "end": 1222.5400000000002, "text": " I'm a huge fan of just one way to do things in a markdown language like this.", "tokens": [50746, 286, 478, 257, 2603, 3429, 295, 445, 472, 636, 281, 360, 721, 294, 257, 1491, 5093, 2856, 411, 341, 13, 50946], "temperature": 0.0, "avg_logprob": -0.19969335283551898, "compression_ratio": 1.6827794561933536, "no_speech_prob": 0.8513103723526001}, {"id": 324, "seek": 121090, "start": 1222.5400000000002, "end": 1226.5, "text": " And so I just use stars all the time, and even though you can use underscores in any", "tokens": [50946, 400, 370, 286, 445, 764, 6105, 439, 264, 565, 11, 293, 754, 1673, 291, 393, 764, 16692, 66, 2706, 294, 604, 51144], "temperature": 0.0, "avg_logprob": -0.19969335283551898, "compression_ratio": 1.6827794561933536, "no_speech_prob": 0.8513103723526001}, {"id": 325, "seek": 121090, "start": 1226.5, "end": 1230.3000000000002, "text": " number of combinations, infinite number of combinations, which to represent in a grammar", "tokens": [51144, 1230, 295, 21267, 11, 13785, 1230, 295, 21267, 11, 597, 281, 2906, 294, 257, 22317, 51334], "temperature": 0.0, "avg_logprob": -0.19969335283551898, "compression_ratio": 1.6827794561933536, "no_speech_prob": 0.8513103723526001}, {"id": 326, "seek": 121090, "start": 1230.3000000000002, "end": 1235.1000000000001, "text": " would be crazy hard, not to mention putting an unnecessary amount of overload on a parser.", "tokens": [51334, 576, 312, 3219, 1152, 11, 406, 281, 2152, 3372, 364, 19350, 2372, 295, 28777, 322, 257, 21156, 260, 13, 51574], "temperature": 0.0, "avg_logprob": -0.19969335283551898, "compression_ratio": 1.6827794561933536, "no_speech_prob": 0.8513103723526001}, {"id": 327, "seek": 121090, "start": 1235.1000000000001, "end": 1237.8600000000001, "text": " And if you're going to make a specification for something that's going to potentially", "tokens": [51574, 400, 498, 291, 434, 516, 281, 652, 257, 31256, 337, 746, 300, 311, 516, 281, 7263, 51712], "temperature": 0.0, "avg_logprob": -0.19969335283551898, "compression_ratio": 1.6827794561933536, "no_speech_prob": 0.8513103723526001}, {"id": 328, "seek": 121090, "start": 1237.8600000000001, "end": 1239.9, "text": " be capturing all the knowledge of the world.", "tokens": [51712, 312, 23384, 439, 264, 3601, 295, 264, 1002, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19969335283551898, "compression_ratio": 1.6827794561933536, "no_speech_prob": 0.8513103723526001}, {"id": 329, "seek": 123990, "start": 1239.9, "end": 1242.5, "text": " We want to be optimized in our parsing.", "tokens": [50364, 492, 528, 281, 312, 26941, 294, 527, 21156, 278, 13, 50494], "temperature": 0.0, "avg_logprob": -0.12568290145308883, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.13287152349948883}, {"id": 330, "seek": 123990, "start": 1242.5, "end": 1247.44, "text": " And so this is one of those cases where we just want one best way to do it.", "tokens": [50494, 400, 370, 341, 307, 472, 295, 729, 3331, 689, 321, 445, 528, 472, 1151, 636, 281, 360, 309, 13, 50741], "temperature": 0.0, "avg_logprob": -0.12568290145308883, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.13287152349948883}, {"id": 331, "seek": 123990, "start": 1247.44, "end": 1249.42, "text": " Other grammars would nest them.", "tokens": [50741, 5358, 17570, 685, 576, 15646, 552, 13, 50840], "temperature": 0.0, "avg_logprob": -0.12568290145308883, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.13287152349948883}, {"id": 332, "seek": 123990, "start": 1249.42, "end": 1253.02, "text": " Lots of grammars would nest these, and you can represent these as nested, but I do not", "tokens": [50840, 15908, 295, 17570, 685, 576, 15646, 613, 11, 293, 291, 393, 2906, 613, 382, 15646, 292, 11, 457, 286, 360, 406, 51020], "temperature": 0.0, "avg_logprob": -0.12568290145308883, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.13287152349948883}, {"id": 333, "seek": 123990, "start": 1253.02, "end": 1255.26, "text": " ever want to do that.", "tokens": [51020, 1562, 528, 281, 360, 300, 13, 51132], "temperature": 0.0, "avg_logprob": -0.12568290145308883, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.13287152349948883}, {"id": 334, "seek": 123990, "start": 1255.26, "end": 1256.5400000000002, "text": " And neither does medium.", "tokens": [51132, 400, 9662, 775, 6399, 13, 51196], "temperature": 0.0, "avg_logprob": -0.12568290145308883, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.13287152349948883}, {"id": 335, "seek": 123990, "start": 1256.5400000000002, "end": 1259.38, "text": " Medium does not allow you to nest your grammar, to nest your emphasis.", "tokens": [51196, 38915, 775, 406, 2089, 291, 281, 15646, 428, 22317, 11, 281, 15646, 428, 16271, 13, 51338], "temperature": 0.0, "avg_logprob": -0.12568290145308883, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.13287152349948883}, {"id": 336, "seek": 123990, "start": 1259.38, "end": 1263.68, "text": " You cannot have italics in something that's already been bolded.", "tokens": [51338, 509, 2644, 362, 22366, 1167, 294, 746, 300, 311, 1217, 668, 11928, 292, 13, 51553], "temperature": 0.0, "avg_logprob": -0.12568290145308883, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.13287152349948883}, {"id": 337, "seek": 123990, "start": 1263.68, "end": 1267.26, "text": " And if you've ever played around with any kind of syntax highlighters or Vim plugins", "tokens": [51553, 400, 498, 291, 600, 1562, 3737, 926, 365, 604, 733, 295, 28431, 5078, 433, 420, 691, 332, 33759, 51732], "temperature": 0.0, "avg_logprob": -0.12568290145308883, "compression_ratio": 1.7191780821917808, "no_speech_prob": 0.13287152349948883}, {"id": 338, "seek": 126726, "start": 1267.26, "end": 1273.86, "text": " or anything like that, you have experienced a broken emphasis highlighter, something that", "tokens": [50364, 420, 1340, 411, 300, 11, 291, 362, 6751, 257, 5463, 16271, 40455, 11, 746, 300, 50694], "temperature": 0.0, "avg_logprob": -0.1606367301940918, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.10084589570760727}, {"id": 339, "seek": 126726, "start": 1273.86, "end": 1280.78, "text": " gets it wrong because it wasn't well specified, or it thinks it's got a span when it really", "tokens": [50694, 2170, 309, 2085, 570, 309, 2067, 380, 731, 22206, 11, 420, 309, 7309, 309, 311, 658, 257, 16174, 562, 309, 534, 51040], "temperature": 0.0, "avg_logprob": -0.1606367301940918, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.10084589570760727}, {"id": 340, "seek": 126726, "start": 1280.78, "end": 1283.18, "text": " doesn't because it's on a different line.", "tokens": [51040, 1177, 380, 570, 309, 311, 322, 257, 819, 1622, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1606367301940918, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.10084589570760727}, {"id": 341, "seek": 126726, "start": 1283.18, "end": 1287.86, "text": " Or I'm guarantee you, if you spend any amount of time doing any kind of syntax highlighting", "tokens": [51160, 1610, 286, 478, 10815, 291, 11, 498, 291, 3496, 604, 2372, 295, 565, 884, 604, 733, 295, 28431, 26551, 51394], "temperature": 0.0, "avg_logprob": -0.1606367301940918, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.10084589570760727}, {"id": 342, "seek": 126726, "start": 1287.86, "end": 1290.86, "text": " stuff, you will find something that's really broken.", "tokens": [51394, 1507, 11, 291, 486, 915, 746, 300, 311, 534, 5463, 13, 51544], "temperature": 0.0, "avg_logprob": -0.1606367301940918, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.10084589570760727}, {"id": 343, "seek": 126726, "start": 1290.86, "end": 1294.54, "text": " And it's really too bad because of that.", "tokens": [51544, 400, 309, 311, 534, 886, 1578, 570, 295, 300, 13, 51728], "temperature": 0.0, "avg_logprob": -0.1606367301940918, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.10084589570760727}, {"id": 344, "seek": 129454, "start": 1294.54, "end": 1297.34, "text": " So for my particular grammar, and this is one of the reasons I made Pagan, I wanted", "tokens": [50364, 407, 337, 452, 1729, 22317, 11, 293, 341, 307, 472, 295, 264, 4112, 286, 1027, 430, 14167, 11, 286, 1415, 50504], "temperature": 0.0, "avg_logprob": -0.18973548940364146, "compression_ratio": 1.7641196013289036, "no_speech_prob": 0.2507682740688324}, {"id": 345, "seek": 129454, "start": 1297.34, "end": 1303.22, "text": " to be able to very precisely specify, I wanted to be very precise in my specification of", "tokens": [50504, 281, 312, 1075, 281, 588, 13402, 16500, 11, 286, 1415, 281, 312, 588, 13600, 294, 452, 31256, 295, 50798], "temperature": 0.0, "avg_logprob": -0.18973548940364146, "compression_ratio": 1.7641196013289036, "no_speech_prob": 0.2507682740688324}, {"id": 346, "seek": 129454, "start": 1303.22, "end": 1306.94, "text": " the grammar, and to keep it deliberately simple.", "tokens": [50798, 264, 22317, 11, 293, 281, 1066, 309, 23506, 2199, 13, 50984], "temperature": 0.0, "avg_logprob": -0.18973548940364146, "compression_ratio": 1.7641196013289036, "no_speech_prob": 0.2507682740688324}, {"id": 347, "seek": 129454, "start": 1306.94, "end": 1311.1, "text": " And people might accuse me of keeping it too simple, and they're like, why is it simple?", "tokens": [50984, 400, 561, 1062, 43610, 385, 295, 5145, 309, 886, 2199, 11, 293, 436, 434, 411, 11, 983, 307, 309, 2199, 30, 51192], "temperature": 0.0, "avg_logprob": -0.18973548940364146, "compression_ratio": 1.7641196013289036, "no_speech_prob": 0.2507682740688324}, {"id": 348, "seek": 129454, "start": 1311.1, "end": 1312.5, "text": " I was like, because I don't need more.", "tokens": [51192, 286, 390, 411, 11, 570, 286, 500, 380, 643, 544, 13, 51262], "temperature": 0.0, "avg_logprob": -0.18973548940364146, "compression_ratio": 1.7641196013289036, "no_speech_prob": 0.2507682740688324}, {"id": 349, "seek": 129454, "start": 1312.5, "end": 1314.74, "text": " But it's not as simple as Godoc, for example.", "tokens": [51262, 583, 309, 311, 406, 382, 2199, 382, 1265, 905, 11, 337, 1365, 13, 51374], "temperature": 0.0, "avg_logprob": -0.18973548940364146, "compression_ratio": 1.7641196013289036, "no_speech_prob": 0.2507682740688324}, {"id": 350, "seek": 129454, "start": 1314.74, "end": 1316.98, "text": " Godoc doesn't have anything.", "tokens": [51374, 1265, 905, 1177, 380, 362, 1340, 13, 51486], "temperature": 0.0, "avg_logprob": -0.18973548940364146, "compression_ratio": 1.7641196013289036, "no_speech_prob": 0.2507682740688324}, {"id": 351, "seek": 129454, "start": 1316.98, "end": 1318.3799999999999, "text": " Godoc is even worse.", "tokens": [51486, 1265, 905, 307, 754, 5324, 13, 51556], "temperature": 0.0, "avg_logprob": -0.18973548940364146, "compression_ratio": 1.7641196013289036, "no_speech_prob": 0.2507682740688324}, {"id": 352, "seek": 129454, "start": 1318.3799999999999, "end": 1319.82, "text": " It's just text.", "tokens": [51556, 467, 311, 445, 2487, 13, 51628], "temperature": 0.0, "avg_logprob": -0.18973548940364146, "compression_ratio": 1.7641196013289036, "no_speech_prob": 0.2507682740688324}, {"id": 353, "seek": 129454, "start": 1319.82, "end": 1323.54, "text": " And you can indent something by four spaces to get it to be the same.", "tokens": [51628, 400, 291, 393, 44494, 746, 538, 1451, 7673, 281, 483, 309, 281, 312, 264, 912, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18973548940364146, "compression_ratio": 1.7641196013289036, "no_speech_prob": 0.2507682740688324}, {"id": 354, "seek": 132354, "start": 1323.54, "end": 1326.18, "text": " And headers are lines by themselves.", "tokens": [50364, 400, 45101, 366, 3876, 538, 2969, 13, 50496], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 355, "seek": 132354, "start": 1326.18, "end": 1327.54, "text": " So there is a grammar there.", "tokens": [50496, 407, 456, 307, 257, 22317, 456, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 356, "seek": 132354, "start": 1327.54, "end": 1328.54, "text": " It's just not specified.", "tokens": [50564, 467, 311, 445, 406, 22206, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 357, "seek": 132354, "start": 1328.54, "end": 1331.78, "text": " You have to go read the Go source code to understand what the grammar is.", "tokens": [50614, 509, 362, 281, 352, 1401, 264, 1037, 4009, 3089, 281, 1223, 437, 264, 22317, 307, 13, 50776], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 358, "seek": 132354, "start": 1331.78, "end": 1332.78, "text": " This is why I made Pagan.", "tokens": [50776, 639, 307, 983, 286, 1027, 430, 14167, 13, 50826], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 359, "seek": 132354, "start": 1332.78, "end": 1336.74, "text": " At least when somebody comes to me and says, OK, I said, well, what's Kegelman, I don't", "tokens": [50826, 1711, 1935, 562, 2618, 1487, 281, 385, 293, 1619, 11, 2264, 11, 286, 848, 11, 731, 11, 437, 311, 591, 1146, 338, 1601, 11, 286, 500, 380, 51024], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 360, "seek": 132354, "start": 1336.74, "end": 1337.74, "text": " want to have to learn another thing.", "tokens": [51024, 528, 281, 362, 281, 1466, 1071, 551, 13, 51074], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 361, "seek": 132354, "start": 1337.74, "end": 1338.74, "text": " I said, you probably already know it.", "tokens": [51074, 286, 848, 11, 291, 1391, 1217, 458, 309, 13, 51124], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 362, "seek": 132354, "start": 1338.74, "end": 1339.94, "text": " I was like, how do you know it?", "tokens": [51124, 286, 390, 411, 11, 577, 360, 291, 458, 309, 30, 51184], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 363, "seek": 132354, "start": 1339.94, "end": 1340.94, "text": " Well, do you know Markdown?", "tokens": [51184, 1042, 11, 360, 291, 458, 3934, 5093, 30, 51234], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 364, "seek": 132354, "start": 1340.94, "end": 1341.94, "text": " Yeah.", "tokens": [51234, 865, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 365, "seek": 132354, "start": 1341.94, "end": 1347.86, "text": " So it's a simplified version of Pandoc Markdown, which means it's got semantic div brakes and", "tokens": [51284, 407, 309, 311, 257, 26335, 3037, 295, 16995, 905, 3934, 5093, 11, 597, 1355, 309, 311, 658, 47982, 3414, 19950, 293, 51580], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 366, "seek": 132354, "start": 1347.86, "end": 1353.5, "text": " it's got math support, which GitHub now has.", "tokens": [51580, 309, 311, 658, 5221, 1406, 11, 597, 23331, 586, 575, 13, 51862], "temperature": 0.0, "avg_logprob": -0.2023891040257045, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.11589156091213226}, {"id": 367, "seek": 135350, "start": 1354.46, "end": 1355.46, "text": " That's what Pagan is.", "tokens": [50412, 663, 311, 437, 430, 14167, 307, 13, 50462], "temperature": 0.0, "avg_logprob": -0.17200872466320127, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.025169188156723976}, {"id": 368, "seek": 135350, "start": 1355.46, "end": 1357.3, "text": " Pagan is a way to specify a language.", "tokens": [50462, 430, 14167, 307, 257, 636, 281, 16500, 257, 2856, 13, 50554], "temperature": 0.0, "avg_logprob": -0.17200872466320127, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.025169188156723976}, {"id": 369, "seek": 135350, "start": 1357.3, "end": 1359.1, "text": " You can go read more about it if you want to get into the details.", "tokens": [50554, 509, 393, 352, 1401, 544, 466, 309, 498, 291, 528, 281, 483, 666, 264, 4365, 13, 50644], "temperature": 0.0, "avg_logprob": -0.17200872466320127, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.025169188156723976}, {"id": 370, "seek": 135350, "start": 1359.1, "end": 1361.3, "text": " I feel like I've gone too far in the details already.", "tokens": [50644, 286, 841, 411, 286, 600, 2780, 886, 1400, 294, 264, 4365, 1217, 13, 50754], "temperature": 0.0, "avg_logprob": -0.17200872466320127, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.025169188156723976}, {"id": 371, "seek": 135350, "start": 1361.3, "end": 1365.94, "text": " I do think it's important that I mentioned that the grammar, the Pagan itself does specify", "tokens": [50754, 286, 360, 519, 309, 311, 1021, 300, 286, 2835, 300, 264, 22317, 11, 264, 430, 14167, 2564, 775, 16500, 50986], "temperature": 0.0, "avg_logprob": -0.17200872466320127, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.025169188156723976}, {"id": 372, "seek": 135350, "start": 1365.94, "end": 1368.42, "text": " the AST to be used.", "tokens": [50986, 264, 316, 6840, 281, 312, 1143, 13, 51110], "temperature": 0.0, "avg_logprob": -0.17200872466320127, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.025169188156723976}, {"id": 373, "seek": 135350, "start": 1368.42, "end": 1374.82, "text": " I am seriously considering a revamp of this AST at some point.", "tokens": [51110, 286, 669, 6638, 8079, 257, 3698, 1215, 295, 341, 316, 6840, 412, 512, 935, 13, 51430], "temperature": 0.0, "avg_logprob": -0.17200872466320127, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.025169188156723976}, {"id": 374, "seek": 135350, "start": 1374.82, "end": 1383.18, "text": " So the format for the AST, the text version of the AST is also written in Pagan.", "tokens": [51430, 407, 264, 7877, 337, 264, 316, 6840, 11, 264, 2487, 3037, 295, 264, 316, 6840, 307, 611, 3720, 294, 430, 14167, 13, 51848], "temperature": 0.0, "avg_logprob": -0.17200872466320127, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.025169188156723976}, {"id": 375, "seek": 138318, "start": 1383.26, "end": 1387.46, "text": " You can go read that if you want, or you can actually look at the long form.", "tokens": [50368, 509, 393, 352, 1401, 300, 498, 291, 528, 11, 420, 291, 393, 767, 574, 412, 264, 938, 1254, 13, 50578], "temperature": 0.0, "avg_logprob": -0.23525530497233074, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0037068177480250597}, {"id": 376, "seek": 138318, "start": 1387.46, "end": 1390.14, "text": " I'm trying to find an example.", "tokens": [50578, 286, 478, 1382, 281, 915, 364, 1365, 13, 50712], "temperature": 0.0, "avg_logprob": -0.23525530497233074, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0037068177480250597}, {"id": 377, "seek": 138318, "start": 1390.14, "end": 1397.26, "text": " So here is a long form example of the JSON AST.", "tokens": [50712, 407, 510, 307, 257, 938, 1254, 1365, 295, 264, 31828, 316, 6840, 13, 51068], "temperature": 0.0, "avg_logprob": -0.23525530497233074, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0037068177480250597}, {"id": 378, "seek": 138318, "start": 1397.26, "end": 1409.7, "text": " So this is the AST of the JSON file itself, but in compact form, it's actually extremely", "tokens": [51068, 407, 341, 307, 264, 316, 6840, 295, 264, 31828, 3991, 2564, 11, 457, 294, 14679, 1254, 11, 309, 311, 767, 4664, 51690], "temperature": 0.0, "avg_logprob": -0.23525530497233074, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0037068177480250597}, {"id": 379, "seek": 138318, "start": 1409.7, "end": 1410.7, "text": " compact.", "tokens": [51690, 14679, 13, 51740], "temperature": 0.0, "avg_logprob": -0.23525530497233074, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0037068177480250597}, {"id": 380, "seek": 141070, "start": 1410.74, "end": 1422.06, "text": " It's the smallest text-based JSON compatible, human readable AST that you can get, and it", "tokens": [50366, 467, 311, 264, 16998, 2487, 12, 6032, 31828, 18218, 11, 1952, 49857, 316, 6840, 300, 291, 393, 483, 11, 293, 309, 50932], "temperature": 0.0, "avg_logprob": -0.21895475554884525, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0028004690539091825}, {"id": 381, "seek": 141070, "start": 1422.06, "end": 1423.46, "text": " can be easily expanded.", "tokens": [50932, 393, 312, 3612, 14342, 13, 51002], "temperature": 0.0, "avg_logprob": -0.21895475554884525, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0028004690539091825}, {"id": 382, "seek": 141070, "start": 1423.46, "end": 1426.7, "text": " So that was by design.", "tokens": [51002, 407, 300, 390, 538, 1715, 13, 51164], "temperature": 0.0, "avg_logprob": -0.21895475554884525, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0028004690539091825}, {"id": 383, "seek": 141070, "start": 1426.7, "end": 1430.98, "text": " When I looked at the native Haskell, this was all inspired by Pandoc.", "tokens": [51164, 1133, 286, 2956, 412, 264, 8470, 8646, 43723, 11, 341, 390, 439, 7547, 538, 16995, 905, 13, 51378], "temperature": 0.0, "avg_logprob": -0.21895475554884525, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0028004690539091825}, {"id": 384, "seek": 141070, "start": 1430.98, "end": 1433.38, "text": " Pandoc's JSON AST is abysmally bad.", "tokens": [51378, 16995, 905, 311, 31828, 316, 6840, 307, 410, 749, 76, 379, 1578, 13, 51498], "temperature": 0.0, "avg_logprob": -0.21895475554884525, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0028004690539091825}, {"id": 385, "seek": 141070, "start": 1433.38, "end": 1435.3400000000001, "text": " I mean, let me just show you.", "tokens": [51498, 286, 914, 11, 718, 385, 445, 855, 291, 13, 51596], "temperature": 0.0, "avg_logprob": -0.21895475554884525, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0028004690539091825}, {"id": 386, "seek": 141070, "start": 1435.3400000000001, "end": 1437.3, "text": " I talked about this earlier, but I'm going to talk about it again.", "tokens": [51596, 286, 2825, 466, 341, 3071, 11, 457, 286, 478, 516, 281, 751, 466, 309, 797, 13, 51694], "temperature": 0.0, "avg_logprob": -0.21895475554884525, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0028004690539091825}, {"id": 387, "seek": 141070, "start": 1437.3, "end": 1439.6200000000001, "text": " I just love salmon on this particular thing.", "tokens": [51694, 286, 445, 959, 18518, 322, 341, 1729, 551, 13, 51810], "temperature": 0.0, "avg_logprob": -0.21895475554884525, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0028004690539091825}, {"id": 388, "seek": 143962, "start": 1439.6599999999999, "end": 1445.06, "text": " So when you do Pandoc, I mean, they didn't have as much information as they do now.", "tokens": [50366, 407, 562, 291, 360, 16995, 905, 11, 286, 914, 11, 436, 994, 380, 362, 382, 709, 1589, 382, 436, 360, 586, 13, 50636], "temperature": 0.0, "avg_logprob": -0.2108269639917322, "compression_ratio": 1.7411347517730495, "no_speech_prob": 0.0027147287037223577}, {"id": 389, "seek": 143962, "start": 1445.06, "end": 1447.1799999999998, "text": " It's too bad because they can't go back and redo it.", "tokens": [50636, 467, 311, 886, 1578, 570, 436, 393, 380, 352, 646, 293, 29956, 309, 13, 50742], "temperature": 0.0, "avg_logprob": -0.2108269639917322, "compression_ratio": 1.7411347517730495, "no_speech_prob": 0.0027147287037223577}, {"id": 390, "seek": 143962, "start": 1447.1799999999998, "end": 1452.1399999999999, "text": " But I don't know if you can see this, but so they have strings here, right?", "tokens": [50742, 583, 286, 500, 380, 458, 498, 291, 393, 536, 341, 11, 457, 370, 436, 362, 13985, 510, 11, 558, 30, 50990], "temperature": 0.0, "avg_logprob": -0.2108269639917322, "compression_ratio": 1.7411347517730495, "no_speech_prob": 0.0027147287037223577}, {"id": 391, "seek": 143962, "start": 1452.1399999999999, "end": 1457.1399999999999, "text": " They have spaces, like a space, meaning white space, and then Kegamel.", "tokens": [50990, 814, 362, 7673, 11, 411, 257, 1901, 11, 3620, 2418, 1901, 11, 293, 550, 591, 1146, 16103, 13, 51240], "temperature": 0.0, "avg_logprob": -0.2108269639917322, "compression_ratio": 1.7411347517730495, "no_speech_prob": 0.0027147287037223577}, {"id": 392, "seek": 143962, "start": 1457.1399999999999, "end": 1460.6999999999998, "text": " And they have, first of all, they have way too many, too much, like two verbose rich", "tokens": [51240, 400, 436, 362, 11, 700, 295, 439, 11, 436, 362, 636, 886, 867, 11, 886, 709, 11, 411, 732, 9595, 541, 4593, 51418], "temperature": 0.0, "avg_logprob": -0.2108269639917322, "compression_ratio": 1.7411347517730495, "no_speech_prob": 0.0027147287037223577}, {"id": 393, "seek": 143962, "start": 1460.6999999999998, "end": 1461.6999999999998, "text": " thing.", "tokens": [51418, 551, 13, 51468], "temperature": 0.0, "avg_logprob": -0.2108269639917322, "compression_ratio": 1.7411347517730495, "no_speech_prob": 0.0027147287037223577}, {"id": 394, "seek": 143962, "start": 1461.6999999999998, "end": 1467.5, "text": " But what I really, really don't like is when they have strings, I'm trying to find one.", "tokens": [51468, 583, 437, 286, 534, 11, 534, 500, 380, 411, 307, 562, 436, 362, 13985, 11, 286, 478, 1382, 281, 915, 472, 13, 51758], "temperature": 0.0, "avg_logprob": -0.2108269639917322, "compression_ratio": 1.7411347517730495, "no_speech_prob": 0.0027147287037223577}, {"id": 395, "seek": 143962, "start": 1467.5, "end": 1468.5, "text": " There's no one there.", "tokens": [51758, 821, 311, 572, 472, 456, 13, 51808], "temperature": 0.0, "avg_logprob": -0.2108269639917322, "compression_ratio": 1.7411347517730495, "no_speech_prob": 0.0027147287037223577}, {"id": 396, "seek": 143962, "start": 1468.5, "end": 1469.5, "text": " Okay.", "tokens": [51808, 1033, 13, 51858], "temperature": 0.0, "avg_logprob": -0.2108269639917322, "compression_ratio": 1.7411347517730495, "no_speech_prob": 0.0027147287037223577}, {"id": 397, "seek": 146950, "start": 1469.5, "end": 1478.1, "text": " So here, this right here, string C documentation slash docs.", "tokens": [50364, 407, 510, 11, 341, 558, 510, 11, 6798, 383, 14333, 17330, 45623, 13, 50794], "temperature": 0.0, "avg_logprob": -0.18015825630414603, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.00029594896477647126}, {"id": 398, "seek": 146950, "start": 1478.1, "end": 1479.1, "text": " What is that?", "tokens": [50794, 708, 307, 300, 30, 50844], "temperature": 0.0, "avg_logprob": -0.18015825630414603, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.00029594896477647126}, {"id": 399, "seek": 146950, "start": 1479.1, "end": 1480.1, "text": " Oh, this is a link.", "tokens": [50844, 876, 11, 341, 307, 257, 2113, 13, 50894], "temperature": 0.0, "avg_logprob": -0.18015825630414603, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.00029594896477647126}, {"id": 400, "seek": 146950, "start": 1480.1, "end": 1481.1, "text": " Okay.", "tokens": [50894, 1033, 13, 50944], "temperature": 0.0, "avg_logprob": -0.18015825630414603, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.00029594896477647126}, {"id": 401, "seek": 146950, "start": 1481.1, "end": 1486.42, "text": " So we have a link and the properties of the link are, and it's got all the properties", "tokens": [50944, 407, 321, 362, 257, 2113, 293, 264, 7221, 295, 264, 2113, 366, 11, 293, 309, 311, 658, 439, 264, 7221, 51210], "temperature": 0.0, "avg_logprob": -0.18015825630414603, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.00029594896477647126}, {"id": 402, "seek": 146950, "start": 1486.42, "end": 1489.82, "text": " in order, so they're not named.", "tokens": [51210, 294, 1668, 11, 370, 436, 434, 406, 4926, 13, 51380], "temperature": 0.0, "avg_logprob": -0.18015825630414603, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.00029594896477647126}, {"id": 403, "seek": 146950, "start": 1489.82, "end": 1495.58, "text": " And what I'm trying to show you is that they, when they have strings, the strings include", "tokens": [51380, 400, 437, 286, 478, 1382, 281, 855, 291, 307, 300, 436, 11, 562, 436, 362, 13985, 11, 264, 13985, 4090, 51668], "temperature": 0.0, "avg_logprob": -0.18015825630414603, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.00029594896477647126}, {"id": 404, "seek": 146950, "start": 1495.58, "end": 1496.98, "text": " quotes on them and stuff like that.", "tokens": [51668, 19963, 322, 552, 293, 1507, 411, 300, 13, 51738], "temperature": 0.0, "avg_logprob": -0.18015825630414603, "compression_ratio": 1.6303317535545023, "no_speech_prob": 0.00029594896477647126}, {"id": 405, "seek": 149698, "start": 1496.98, "end": 1499.9, "text": " So let me see if I can find another version.", "tokens": [50364, 407, 718, 385, 536, 498, 286, 393, 915, 1071, 3037, 13, 50510], "temperature": 0.0, "avg_logprob": -0.2991734591397372, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.03114037774503231}, {"id": 406, "seek": 149698, "start": 1499.9, "end": 1510.74, "text": " So if I have, I don't know, let me see if I can find one, pandoc-tjson, read me.", "tokens": [50510, 407, 498, 286, 362, 11, 286, 500, 380, 458, 11, 718, 385, 536, 498, 286, 393, 915, 472, 11, 4565, 905, 12, 83, 73, 3015, 11, 1401, 385, 13, 51052], "temperature": 0.0, "avg_logprob": -0.2991734591397372, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.03114037774503231}, {"id": 407, "seek": 149698, "start": 1510.74, "end": 1513.98, "text": " Did I just go to look at the same one again?", "tokens": [51052, 2589, 286, 445, 352, 281, 574, 412, 264, 912, 472, 797, 30, 51214], "temperature": 0.0, "avg_logprob": -0.2991734591397372, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.03114037774503231}, {"id": 408, "seek": 149698, "start": 1513.98, "end": 1514.98, "text": " I did.", "tokens": [51214, 286, 630, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2991734591397372, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.03114037774503231}, {"id": 409, "seek": 149698, "start": 1514.98, "end": 1517.5, "text": " And it's in the spec, ml spec.", "tokens": [51264, 400, 309, 311, 294, 264, 1608, 11, 23271, 1608, 13, 51390], "temperature": 0.0, "avg_logprob": -0.2991734591397372, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.03114037774503231}, {"id": 410, "seek": 149698, "start": 1517.5, "end": 1518.58, "text": " All right.", "tokens": [51390, 1057, 558, 13, 51444], "temperature": 0.0, "avg_logprob": -0.2991734591397372, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.03114037774503231}, {"id": 411, "seek": 149698, "start": 1518.58, "end": 1522.46, "text": " So let's go, let's look at this, read me.", "tokens": [51444, 407, 718, 311, 352, 11, 718, 311, 574, 412, 341, 11, 1401, 385, 13, 51638], "temperature": 0.0, "avg_logprob": -0.2991734591397372, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.03114037774503231}, {"id": 412, "seek": 149698, "start": 1522.46, "end": 1523.46, "text": " Okay.", "tokens": [51638, 1033, 13, 51688], "temperature": 0.0, "avg_logprob": -0.2991734591397372, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.03114037774503231}, {"id": 413, "seek": 149698, "start": 1523.46, "end": 1524.46, "text": " So look at this one.", "tokens": [51688, 407, 574, 412, 341, 472, 13, 51738], "temperature": 0.0, "avg_logprob": -0.2991734591397372, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.03114037774503231}, {"id": 414, "seek": 152446, "start": 1524.94, "end": 1529.3400000000001, "text": " So you know, you have the links here, you have the types, you have a soft break and it,", "tokens": [50388, 407, 291, 458, 11, 291, 362, 264, 6123, 510, 11, 291, 362, 264, 3467, 11, 291, 362, 257, 2787, 1821, 293, 309, 11, 50608], "temperature": 0.0, "avg_logprob": -0.18082990577752642, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.031129904091358185}, {"id": 415, "seek": 152446, "start": 1529.3400000000001, "end": 1531.06, "text": " it deliberately uses long words.", "tokens": [50608, 309, 23506, 4960, 938, 2283, 13, 50694], "temperature": 0.0, "avg_logprob": -0.18082990577752642, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.031129904091358185}, {"id": 416, "seek": 152446, "start": 1531.06, "end": 1532.98, "text": " There's no way to make it shorter.", "tokens": [50694, 821, 311, 572, 636, 281, 652, 309, 11639, 13, 50790], "temperature": 0.0, "avg_logprob": -0.18082990577752642, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.031129904091358185}, {"id": 417, "seek": 152446, "start": 1532.98, "end": 1539.58, "text": " So the, the textual ASTs that are generated from pandoc are so fucking long that, and", "tokens": [50790, 407, 264, 11, 264, 2487, 901, 316, 6840, 82, 300, 366, 10833, 490, 4565, 905, 366, 370, 5546, 938, 300, 11, 293, 51120], "temperature": 0.0, "avg_logprob": -0.18082990577752642, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.031129904091358185}, {"id": 418, "seek": 152446, "start": 1539.58, "end": 1545.02, "text": " the standard instruction from pandoc, if you want to write your own conversions is to convert", "tokens": [51120, 264, 3832, 10951, 490, 4565, 905, 11, 498, 291, 528, 281, 2464, 428, 1065, 42256, 307, 281, 7620, 51392], "temperature": 0.0, "avg_logprob": -0.18082990577752642, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.031129904091358185}, {"id": 419, "seek": 152446, "start": 1545.02, "end": 1551.22, "text": " to the JSON AST and, and then walk the, the JSON AST and parse that and produce your other", "tokens": [51392, 281, 264, 31828, 316, 6840, 293, 11, 293, 550, 1792, 264, 11, 264, 31828, 316, 6840, 293, 48377, 300, 293, 5258, 428, 661, 51702], "temperature": 0.0, "avg_logprob": -0.18082990577752642, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.031129904091358185}, {"id": 420, "seek": 152446, "start": 1551.22, "end": 1552.22, "text": " thing.", "tokens": [51702, 551, 13, 51752], "temperature": 0.0, "avg_logprob": -0.18082990577752642, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.031129904091358185}, {"id": 421, "seek": 152446, "start": 1552.22, "end": 1554.42, "text": " And I guarantee you, you, you should see how long it takes to do that.", "tokens": [51752, 400, 286, 10815, 291, 11, 291, 11, 291, 820, 536, 577, 938, 309, 2516, 281, 360, 300, 13, 51862], "temperature": 0.0, "avg_logprob": -0.18082990577752642, "compression_ratio": 1.8064516129032258, "no_speech_prob": 0.031129904091358185}, {"id": 422, "seek": 155442, "start": 1554.9, "end": 1561.74, "text": " Because this thing is a monstrously big waste and it also gets tons of stuff wrong.", "tokens": [50388, 1436, 341, 551, 307, 257, 47137, 5098, 955, 5964, 293, 309, 611, 2170, 9131, 295, 1507, 2085, 13, 50730], "temperature": 0.0, "avg_logprob": -0.16629881370724656, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.01132963877171278}, {"id": 423, "seek": 155442, "start": 1561.74, "end": 1568.9, "text": " So for example, it, it deliberately lumps together, uh, quoted content as a string.", "tokens": [50730, 407, 337, 1365, 11, 309, 11, 309, 23506, 44948, 1214, 11, 2232, 11, 30047, 2701, 382, 257, 6798, 13, 51088], "temperature": 0.0, "avg_logprob": -0.16629881370724656, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.01132963877171278}, {"id": 424, "seek": 155442, "start": 1568.9, "end": 1572.8600000000001, "text": " So for example, or a parenthesized word as a string, it doesn't consider the idea of", "tokens": [51088, 407, 337, 1365, 11, 420, 257, 23350, 279, 1602, 1349, 382, 257, 6798, 11, 309, 1177, 380, 1949, 264, 1558, 295, 51286], "temperature": 0.0, "avg_logprob": -0.16629881370724656, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.01132963877171278}, {"id": 425, "seek": 155442, "start": 1572.8600000000001, "end": 1575.74, "text": " fields and then words and then things like that.", "tokens": [51286, 7909, 293, 550, 2283, 293, 550, 721, 411, 300, 13, 51430], "temperature": 0.0, "avg_logprob": -0.16629881370724656, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.01132963877171278}, {"id": 426, "seek": 155442, "start": 1575.74, "end": 1577.74, "text": " It's, it's so, it's just a mess.", "tokens": [51430, 467, 311, 11, 309, 311, 370, 11, 309, 311, 445, 257, 2082, 13, 51530], "temperature": 0.0, "avg_logprob": -0.16629881370724656, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.01132963877171278}, {"id": 427, "seek": 155442, "start": 1577.74, "end": 1581.78, "text": " And that's, I, I, look, I really, really appreciate that we have pandoc.", "tokens": [51530, 400, 300, 311, 11, 286, 11, 286, 11, 574, 11, 286, 534, 11, 534, 4449, 300, 321, 362, 4565, 905, 13, 51732], "temperature": 0.0, "avg_logprob": -0.16629881370724656, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.01132963877171278}, {"id": 428, "seek": 155442, "start": 1581.78, "end": 1583.7, "text": " I don't want it to be misunderstood.", "tokens": [51732, 286, 500, 380, 528, 309, 281, 312, 33870, 13, 51828], "temperature": 0.0, "avg_logprob": -0.16629881370724656, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.01132963877171278}, {"id": 429, "seek": 158370, "start": 1583.7, "end": 1586.46, "text": " I think it's so great that we have it, uh, JQ.", "tokens": [50364, 286, 519, 309, 311, 370, 869, 300, 321, 362, 309, 11, 2232, 11, 508, 48, 13, 50502], "temperature": 0.0, "avg_logprob": -0.21353966225194568, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0027147578075528145}, {"id": 430, "seek": 158370, "start": 1586.46, "end": 1590.6200000000001, "text": " I could pipe it through JQ, but yeah, uh, Jeff needs some love.", "tokens": [50502, 286, 727, 11240, 309, 807, 508, 48, 11, 457, 1338, 11, 2232, 11, 7506, 2203, 512, 959, 13, 50710], "temperature": 0.0, "avg_logprob": -0.21353966225194568, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0027147578075528145}, {"id": 431, "seek": 158370, "start": 1590.66, "end": 1591.5, "text": " Yeah, it does.", "tokens": [50712, 865, 11, 309, 775, 13, 50754], "temperature": 0.0, "avg_logprob": -0.21353966225194568, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0027147578075528145}, {"id": 432, "seek": 158370, "start": 1591.5, "end": 1596.14, "text": " So, and they could actually make a different representation.", "tokens": [50754, 407, 11, 293, 436, 727, 767, 652, 257, 819, 10290, 13, 50986], "temperature": 0.0, "avg_logprob": -0.21353966225194568, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0027147578075528145}, {"id": 433, "seek": 158370, "start": 1596.14, "end": 1600.66, "text": " You could probably do a Haskell, uh, you know, PR and, and do that.", "tokens": [50986, 509, 727, 1391, 360, 257, 8646, 43723, 11, 2232, 11, 291, 458, 11, 11568, 293, 11, 293, 360, 300, 13, 51212], "temperature": 0.0, "avg_logprob": -0.21353966225194568, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0027147578075528145}, {"id": 434, "seek": 158370, "start": 1600.66, "end": 1603.46, "text": " But the, the, the, yeah, they probably prefer Haskell reformos.", "tokens": [51212, 583, 264, 11, 264, 11, 264, 11, 1338, 11, 436, 1391, 4382, 8646, 43723, 8290, 329, 13, 51352], "temperature": 0.0, "avg_logprob": -0.21353966225194568, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0027147578075528145}, {"id": 435, "seek": 158370, "start": 1603.6200000000001, "end": 1604.06, "text": " I agree.", "tokens": [51360, 286, 3986, 13, 51382], "temperature": 0.0, "avg_logprob": -0.21353966225194568, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0027147578075528145}, {"id": 436, "seek": 158370, "start": 1604.3400000000001, "end": 1608.18, "text": " But the, the, the point is, is that, I mean, if you're going to tell somebody that", "tokens": [51396, 583, 264, 11, 264, 11, 264, 935, 307, 11, 307, 300, 11, 286, 914, 11, 498, 291, 434, 516, 281, 980, 2618, 300, 51588], "temperature": 0.0, "avg_logprob": -0.21353966225194568, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0027147578075528145}, {"id": 437, "seek": 160818, "start": 1608.18, "end": 1613.5800000000002, "text": " you're a standard way of supporting other conversion method out methods is to,", "tokens": [50364, 291, 434, 257, 3832, 636, 295, 7231, 661, 14298, 3170, 484, 7150, 307, 281, 11, 50634], "temperature": 0.0, "avg_logprob": -0.14824037409540433, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.21726645529270172}, {"id": 438, "seek": 160818, "start": 1613.6200000000001, "end": 1614.8200000000002, "text": " and by the way, you can make your own.", "tokens": [50636, 293, 538, 264, 636, 11, 291, 393, 652, 428, 1065, 13, 50696], "temperature": 0.0, "avg_logprob": -0.14824037409540433, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.21726645529270172}, {"id": 439, "seek": 160818, "start": 1614.8200000000002, "end": 1617.38, "text": " You can write your own Lua plugins that will render the whole entire thing.", "tokens": [50696, 509, 393, 2464, 428, 1065, 441, 4398, 33759, 300, 486, 15529, 264, 1379, 2302, 551, 13, 50824], "temperature": 0.0, "avg_logprob": -0.14824037409540433, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.21726645529270172}, {"id": 440, "seek": 160818, "start": 1617.66, "end": 1618.94, "text": " It's beautiful Haskell.", "tokens": [50838, 467, 311, 2238, 8646, 43723, 13, 50902], "temperature": 0.0, "avg_logprob": -0.14824037409540433, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.21726645529270172}, {"id": 441, "seek": 160818, "start": 1618.94, "end": 1622.02, "text": " If you get into the functional program, if you, if you want a really good example", "tokens": [50902, 759, 291, 483, 666, 264, 11745, 1461, 11, 498, 291, 11, 498, 291, 528, 257, 534, 665, 1365, 51056], "temperature": 0.0, "avg_logprob": -0.14824037409540433, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.21726645529270172}, {"id": 442, "seek": 160818, "start": 1622.02, "end": 1627.5, "text": " of when functional programming really shines, uh, you know, pandoc is a good example", "tokens": [51056, 295, 562, 11745, 9410, 534, 28056, 11, 2232, 11, 291, 458, 11, 4565, 905, 307, 257, 665, 1365, 51330], "temperature": 0.0, "avg_logprob": -0.14824037409540433, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.21726645529270172}, {"id": 443, "seek": 160818, "start": 1627.5, "end": 1632.1000000000001, "text": " of that because it was Haskell is like Taylor made for parsing syntaxes and", "tokens": [51330, 295, 300, 570, 309, 390, 8646, 43723, 307, 411, 12060, 1027, 337, 21156, 278, 28431, 279, 293, 51560], "temperature": 0.0, "avg_logprob": -0.14824037409540433, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.21726645529270172}, {"id": 444, "seek": 160818, "start": 1632.1000000000001, "end": 1632.74, "text": " grammars and stuff.", "tokens": [51560, 17570, 685, 293, 1507, 13, 51592], "temperature": 0.0, "avg_logprob": -0.14824037409540433, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.21726645529270172}, {"id": 445, "seek": 160818, "start": 1632.74, "end": 1633.94, "text": " It's just so perfect for it.", "tokens": [51592, 467, 311, 445, 370, 2176, 337, 309, 13, 51652], "temperature": 0.0, "avg_logprob": -0.14824037409540433, "compression_ratio": 1.7372013651877134, "no_speech_prob": 0.21726645529270172}, {"id": 446, "seek": 163394, "start": 1634.3, "end": 1638.8600000000001, "text": " And, and, you know, I imagine, uh, you know, Lisper or Lang or something,", "tokens": [50382, 400, 11, 293, 11, 291, 458, 11, 286, 3811, 11, 2232, 11, 291, 458, 11, 30812, 610, 420, 13313, 420, 746, 11, 50610], "temperature": 0.0, "avg_logprob": -0.15697719739831012, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.01168521586805582}, {"id": 447, "seek": 163394, "start": 1638.98, "end": 1640.26, "text": " uh, would be just as good.", "tokens": [50616, 2232, 11, 576, 312, 445, 382, 665, 13, 50680], "temperature": 0.0, "avg_logprob": -0.15697719739831012, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.01168521586805582}, {"id": 448, "seek": 163394, "start": 1640.6200000000001, "end": 1643.14, "text": " Uh, but, but that's, you know, that's my experience with Haskell too.", "tokens": [50698, 4019, 11, 457, 11, 457, 300, 311, 11, 291, 458, 11, 300, 311, 452, 1752, 365, 8646, 43723, 886, 13, 50824], "temperature": 0.0, "avg_logprob": -0.15697719739831012, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.01168521586805582}, {"id": 449, "seek": 163394, "start": 1643.54, "end": 1649.66, "text": " Um, and, you know, it's super fast, but, but, I mean, it, it does lack a lot in the,", "tokens": [50844, 3301, 11, 293, 11, 291, 458, 11, 309, 311, 1687, 2370, 11, 457, 11, 457, 11, 286, 914, 11, 309, 11, 309, 775, 5011, 257, 688, 294, 264, 11, 51150], "temperature": 0.0, "avg_logprob": -0.15697719739831012, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.01168521586805582}, {"id": 450, "seek": 163394, "start": 1649.7, "end": 1656.3400000000001, "text": " in the, you know, the, the, to me, the AST is really core because if you want to have", "tokens": [51152, 294, 264, 11, 291, 458, 11, 264, 11, 264, 11, 281, 385, 11, 264, 316, 6840, 307, 534, 4965, 570, 498, 291, 528, 281, 362, 51484], "temperature": 0.0, "avg_logprob": -0.15697719739831012, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.01168521586805582}, {"id": 451, "seek": 163394, "start": 1656.38, "end": 1662.8200000000002, "text": " a, the ability to make a conversion from one thing into anything else, uh, it's so", "tokens": [51486, 257, 11, 264, 3485, 281, 652, 257, 14298, 490, 472, 551, 666, 1340, 1646, 11, 2232, 11, 309, 311, 370, 51808], "temperature": 0.0, "avg_logprob": -0.15697719739831012, "compression_ratio": 1.7740585774058577, "no_speech_prob": 0.01168521586805582}, {"id": 452, "seek": 166282, "start": 1662.82, "end": 1664.1399999999999, "text": " important that you get that AST, right?", "tokens": [50364, 1021, 300, 291, 483, 300, 316, 6840, 11, 558, 30, 50430], "temperature": 0.0, "avg_logprob": -0.18506872887704887, "compression_ratio": 1.7212121212121212, "no_speech_prob": 0.011329757049679756}, {"id": 453, "seek": 166282, "start": 1664.1399999999999, "end": 1667.26, "text": " Because then everybody else can render it however they want to.", "tokens": [50430, 1436, 550, 2201, 1646, 393, 15529, 309, 4461, 436, 528, 281, 13, 50586], "temperature": 0.0, "avg_logprob": -0.18506872887704887, "compression_ratio": 1.7212121212121212, "no_speech_prob": 0.011329757049679756}, {"id": 454, "seek": 166282, "start": 1667.26, "end": 1670.1399999999999, "text": " Well, you've just created a data model for the thing that you're", "tokens": [50586, 1042, 11, 291, 600, 445, 2942, 257, 1412, 2316, 337, 264, 551, 300, 291, 434, 50730], "temperature": 0.0, "avg_logprob": -0.18506872887704887, "compression_ratio": 1.7212121212121212, "no_speech_prob": 0.011329757049679756}, {"id": 455, "seek": 166282, "start": 1670.1399999999999, "end": 1672.1, "text": " representing, whatever it is, a document or whatever.", "tokens": [50730, 13460, 11, 2035, 309, 307, 11, 257, 4166, 420, 2035, 13, 50828], "temperature": 0.0, "avg_logprob": -0.18506872887704887, "compression_ratio": 1.7212121212121212, "no_speech_prob": 0.011329757049679756}, {"id": 456, "seek": 166282, "start": 1672.3799999999999, "end": 1674.1799999999998, "text": " And, and, and that's really core of the whole thing.", "tokens": [50842, 400, 11, 293, 11, 293, 300, 311, 534, 4965, 295, 264, 1379, 551, 13, 50932], "temperature": 0.0, "avg_logprob": -0.18506872887704887, "compression_ratio": 1.7212121212121212, "no_speech_prob": 0.011329757049679756}, {"id": 457, "seek": 166282, "start": 1674.1799999999998, "end": 1676.82, "text": " And the people who made pandoc, of course, are not web people.", "tokens": [50932, 400, 264, 561, 567, 1027, 4565, 905, 11, 295, 1164, 11, 366, 406, 3670, 561, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18506872887704887, "compression_ratio": 1.7212121212121212, "no_speech_prob": 0.011329757049679756}, {"id": 458, "seek": 166282, "start": 1676.82, "end": 1680.62, "text": " They are, you know, publishers and Haskell people and academics and they're", "tokens": [51064, 814, 366, 11, 291, 458, 11, 30421, 293, 8646, 43723, 561, 293, 25695, 293, 436, 434, 51254], "temperature": 0.0, "avg_logprob": -0.18506872887704887, "compression_ratio": 1.7212121212121212, "no_speech_prob": 0.011329757049679756}, {"id": 459, "seek": 166282, "start": 1680.62, "end": 1685.22, "text": " over at Berkeley and, you know, JGM's an amazing spearheaded common mark and a", "tokens": [51254, 670, 412, 23684, 293, 11, 291, 458, 11, 508, 25152, 311, 364, 2243, 26993, 28409, 2689, 1491, 293, 257, 51484], "temperature": 0.0, "avg_logprob": -0.18506872887704887, "compression_ratio": 1.7212121212121212, "no_speech_prob": 0.011329757049679756}, {"id": 460, "seek": 166282, "start": 1685.22, "end": 1686.22, "text": " bunch of other amazing things.", "tokens": [51484, 3840, 295, 661, 2243, 721, 13, 51534], "temperature": 0.0, "avg_logprob": -0.18506872887704887, "compression_ratio": 1.7212121212121212, "no_speech_prob": 0.011329757049679756}, {"id": 461, "seek": 166282, "start": 1686.22, "end": 1688.3, "text": " And I, I've had some interactions with JGM.", "tokens": [51534, 400, 286, 11, 286, 600, 632, 512, 13280, 365, 508, 25152, 13, 51638], "temperature": 0.0, "avg_logprob": -0.18506872887704887, "compression_ratio": 1.7212121212121212, "no_speech_prob": 0.011329757049679756}, {"id": 462, "seek": 168830, "start": 1688.3, "end": 1692.78, "text": " JGM is completely 100% unimpressed with anything I've ever done, which is fine", "tokens": [50364, 508, 25152, 307, 2584, 2319, 4, 517, 8814, 3805, 365, 1340, 286, 600, 1562, 1096, 11, 597, 307, 2489, 50588], "temperature": 0.0, "avg_logprob": -0.20439832673655997, "compression_ratio": 1.5923344947735192, "no_speech_prob": 0.017983580008149147}, {"id": 463, "seek": 168830, "start": 1693.26, "end": 1697.62, "text": " because, but I'm doing things that I need, uh, and yeah, including Peg.", "tokens": [50612, 570, 11, 457, 286, 478, 884, 721, 300, 286, 643, 11, 2232, 11, 293, 1338, 11, 3009, 28007, 13, 50830], "temperature": 0.0, "avg_logprob": -0.20439832673655997, "compression_ratio": 1.5923344947735192, "no_speech_prob": 0.017983580008149147}, {"id": 464, "seek": 168830, "start": 1697.62, "end": 1702.02, "text": " And I, I ran Peg and by him and he was like, why am I, well, you know, why Peg?", "tokens": [50830, 400, 286, 11, 286, 5872, 28007, 293, 538, 796, 293, 415, 390, 411, 11, 983, 669, 286, 11, 731, 11, 291, 458, 11, 983, 28007, 30, 51050], "temperature": 0.0, "avg_logprob": -0.20439832673655997, "compression_ratio": 1.5923344947735192, "no_speech_prob": 0.017983580008149147}, {"id": 465, "seek": 168830, "start": 1702.02, "end": 1703.82, "text": " And he doesn't, he's not interested.", "tokens": [51050, 400, 415, 1177, 380, 11, 415, 311, 406, 3102, 13, 51140], "temperature": 0.0, "avg_logprob": -0.20439832673655997, "compression_ratio": 1.5923344947735192, "no_speech_prob": 0.017983580008149147}, {"id": 466, "seek": 168830, "start": 1703.82, "end": 1704.98, "text": " He just, he got his answer.", "tokens": [51140, 634, 445, 11, 415, 658, 702, 1867, 13, 51198], "temperature": 0.0, "avg_logprob": -0.20439832673655997, "compression_ratio": 1.5923344947735192, "no_speech_prob": 0.017983580008149147}, {"id": 467, "seek": 168830, "start": 1704.98, "end": 1708.98, "text": " He's a philosophy major and he's a great guy, but, uh, a philosophy professor.", "tokens": [51198, 634, 311, 257, 10675, 2563, 293, 415, 311, 257, 869, 2146, 11, 457, 11, 2232, 11, 257, 10675, 8304, 13, 51398], "temperature": 0.0, "avg_logprob": -0.20439832673655997, "compression_ratio": 1.5923344947735192, "no_speech_prob": 0.017983580008149147}, {"id": 468, "seek": 168830, "start": 1709.3, "end": 1715.34, "text": " So, um, anyway, here's the parent nodes, uh, given the following data example, you", "tokens": [51414, 407, 11, 1105, 11, 4033, 11, 510, 311, 264, 2596, 13891, 11, 2232, 11, 2212, 264, 3480, 1412, 1365, 11, 291, 51716], "temperature": 0.0, "avg_logprob": -0.20439832673655997, "compression_ratio": 1.5923344947735192, "no_speech_prob": 0.017983580008149147}, {"id": 469, "seek": 171534, "start": 1715.34, "end": 1720.5, "text": " get this copyright and, and, um, yeah, here you get another AST.", "tokens": [50364, 483, 341, 17996, 293, 11, 293, 11, 1105, 11, 1338, 11, 510, 291, 483, 1071, 316, 6840, 13, 50622], "temperature": 0.0, "avg_logprob": -0.3111980081462174, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00912501197308302}, {"id": 470, "seek": 171534, "start": 1720.5, "end": 1722.4199999999998, "text": " No, no AST node attributes.", "tokens": [50622, 883, 11, 572, 316, 6840, 9984, 17212, 13, 50718], "temperature": 0.0, "avg_logprob": -0.3111980081462174, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00912501197308302}, {"id": 471, "seek": 171534, "start": 1722.6599999999999, "end": 1724.6599999999999, "text": " Summary node 2 data structure models, a lot for attributes.", "tokens": [50730, 8626, 76, 822, 9984, 568, 1412, 3877, 5245, 11, 257, 688, 337, 17212, 13, 50830], "temperature": 0.0, "avg_logprob": -0.3111980081462174, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00912501197308302}, {"id": 472, "seek": 171534, "start": 1724.6599999999999, "end": 1728.62, "text": " Peg and Stacy does not, since attributes can more, uh, efficiently and precisely", "tokens": [50830, 28007, 293, 43644, 775, 406, 11, 1670, 17212, 393, 544, 11, 2232, 11, 19621, 293, 13402, 51028], "temperature": 0.0, "avg_logprob": -0.3111980081462174, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00912501197308302}, {"id": 473, "seek": 171534, "start": 1728.62, "end": 1733.34, "text": " indicated by adding an additional parameter parent or terminal type, which is", "tokens": [51028, 16176, 538, 5127, 364, 4497, 13075, 2596, 420, 14709, 2010, 11, 597, 307, 51264], "temperature": 0.0, "avg_logprob": -0.3111980081462174, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00912501197308302}, {"id": 474, "seek": 171534, "start": 1733.34, "end": 1734.5, "text": " another word for leaf node.", "tokens": [51264, 1071, 1349, 337, 10871, 9984, 13, 51322], "temperature": 0.0, "avg_logprob": -0.3111980081462174, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00912501197308302}, {"id": 475, "seek": 171534, "start": 1734.98, "end": 1737.1, "text": " Uh, Peg was conceived originally developed by me.", "tokens": [51346, 4019, 11, 28007, 390, 34898, 7993, 4743, 538, 385, 13, 51452], "temperature": 0.0, "avg_logprob": -0.3111980081462174, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00912501197308302}, {"id": 476, "seek": 171534, "start": 1737.58, "end": 1740.82, "text": " Uh, I'm working with tokenizers and text generation.", "tokens": [51476, 4019, 11, 286, 478, 1364, 365, 14862, 22525, 293, 2487, 5125, 13, 51638], "temperature": 0.0, "avg_logprob": -0.3111980081462174, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00912501197308302}, {"id": 477, "seek": 171534, "start": 1740.82, "end": 1741.3, "text": " Oh, nice.", "tokens": [51638, 876, 11, 1481, 13, 51662], "temperature": 0.0, "avg_logprob": -0.3111980081462174, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00912501197308302}, {"id": 478, "seek": 171534, "start": 1741.86, "end": 1744.6599999999999, "text": " Uh, while creating grammars, uh, was needed for artifacts knowledge net.", "tokens": [51690, 4019, 11, 1339, 4084, 17570, 685, 11, 2232, 11, 390, 2978, 337, 24617, 3601, 2533, 13, 51830], "temperature": 0.0, "avg_logprob": -0.3111980081462174, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00912501197308302}, {"id": 479, "seek": 174466, "start": 1744.66, "end": 1748.7, "text": " That's keg, uh, easy mark, data mark, mate, man, I can update this.", "tokens": [50364, 663, 311, 803, 70, 11, 2232, 11, 1858, 1491, 11, 1412, 1491, 11, 11709, 11, 587, 11, 286, 393, 5623, 341, 13, 50566], "temperature": 0.0, "avg_logprob": -0.3080198964764995, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.002182431286200881}, {"id": 480, "seek": 174466, "start": 1749.02, "end": 1753.14, "text": " These are all of the different, uh, base QL, all of the ones that I've been doing.", "tokens": [50582, 1981, 366, 439, 295, 264, 819, 11, 2232, 11, 3096, 1249, 43, 11, 439, 295, 264, 2306, 300, 286, 600, 668, 884, 13, 50788], "temperature": 0.0, "avg_logprob": -0.3080198964764995, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.002182431286200881}, {"id": 481, "seek": 174466, "start": 1753.66, "end": 1755.74, "text": " Uh, live coding streams.", "tokens": [50814, 4019, 11, 1621, 17720, 15842, 13, 50918], "temperature": 0.0, "avg_logprob": -0.3080198964764995, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.002182431286200881}, {"id": 482, "seek": 174466, "start": 1757.1000000000001, "end": 1759.7, "text": " Others to update and contribute it and help this is, that's, I got to update them.", "tokens": [50986, 20277, 281, 5623, 293, 10586, 309, 293, 854, 341, 307, 11, 300, 311, 11, 286, 658, 281, 5623, 552, 13, 51116], "temperature": 0.0, "avg_logprob": -0.3080198964764995, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.002182431286200881}, {"id": 483, "seek": 174466, "start": 1760.22, "end": 1762.0600000000002, "text": " Uh, related tools to Vimplug and Emacs.", "tokens": [51142, 4019, 11, 4077, 3873, 281, 691, 332, 564, 697, 293, 3968, 44937, 13, 51234], "temperature": 0.0, "avg_logprob": -0.3080198964764995, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.002182431286200881}, {"id": 484, "seek": 174466, "start": 1762.18, "end": 1763.5, "text": " I don't have them in plugin done yet.", "tokens": [51240, 286, 500, 380, 362, 552, 294, 23407, 1096, 1939, 13, 51306], "temperature": 0.0, "avg_logprob": -0.3080198964764995, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.002182431286200881}, {"id": 485, "seek": 174466, "start": 1763.5400000000002, "end": 1765.74, "text": " I need to other efforts out there.", "tokens": [51308, 286, 643, 281, 661, 6484, 484, 456, 13, 51418], "temperature": 0.0, "avg_logprob": -0.3080198964764995, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.002182431286200881}, {"id": 486, "seek": 174466, "start": 1765.74, "end": 1766.66, "text": " You can go look at peg leg.", "tokens": [51418, 509, 393, 352, 574, 412, 17199, 1676, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3080198964764995, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.002182431286200881}, {"id": 487, "seek": 174466, "start": 1766.66, "end": 1768.78, "text": " That's the one I helped contribute to go peg.", "tokens": [51464, 663, 311, 264, 472, 286, 4254, 10586, 281, 352, 17199, 13, 51570], "temperature": 0.0, "avg_logprob": -0.3080198964764995, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.002182431286200881}, {"id": 488, "seek": 174466, "start": 1768.78, "end": 1770.66, "text": " It's the one I contributed to actually space on peg.", "tokens": [51570, 467, 311, 264, 472, 286, 18434, 281, 767, 1901, 322, 17199, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3080198964764995, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.002182431286200881}, {"id": 489, "seek": 177066, "start": 1770.66, "end": 1774.8600000000001, "text": " Look, uh, the Python ones, the Guido one that, that they added 3.9 and Python, I", "tokens": [50364, 2053, 11, 2232, 11, 264, 15329, 2306, 11, 264, 2694, 2925, 472, 300, 11, 300, 436, 3869, 805, 13, 24, 293, 15329, 11, 286, 50574], "temperature": 0.0, "avg_logprob": -0.29212844203895244, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.012051038444042206}, {"id": 490, "seek": 177066, "start": 1774.8600000000001, "end": 1778.38, "text": " think, uh, Pigeon Pigeon pest.rs, which I was so excited about.", "tokens": [50574, 519, 11, 2232, 11, 430, 3969, 266, 430, 3969, 266, 31068, 13, 22943, 11, 597, 286, 390, 370, 2919, 466, 13, 50750], "temperature": 0.0, "avg_logprob": -0.29212844203895244, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.012051038444042206}, {"id": 491, "seek": 177066, "start": 1778.38, "end": 1782.38, "text": " Turned out it's crap and antler antler has been his Java is all about Java.", "tokens": [50750, 7956, 292, 484, 309, 311, 12426, 293, 2511, 1918, 2511, 1918, 575, 668, 702, 10745, 307, 439, 466, 10745, 13, 50950], "temperature": 0.0, "avg_logprob": -0.29212844203895244, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.012051038444042206}, {"id": 492, "seek": 177066, "start": 1782.38, "end": 1786.8600000000001, "text": " But it's, it's exactly in the same sort of space, uh, for defining grammars and", "tokens": [50950, 583, 309, 311, 11, 309, 311, 2293, 294, 264, 912, 1333, 295, 1901, 11, 2232, 11, 337, 17827, 17570, 685, 293, 51174], "temperature": 0.0, "avg_logprob": -0.29212844203895244, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.012051038444042206}, {"id": 493, "seek": 177066, "start": 1786.8600000000001, "end": 1787.94, "text": " stuff, but it's Java only.", "tokens": [51174, 1507, 11, 457, 309, 311, 10745, 787, 13, 51228], "temperature": 0.0, "avg_logprob": -0.29212844203895244, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.012051038444042206}, {"id": 494, "seek": 177066, "start": 1788.5, "end": 1792.3000000000002, "text": " Um, from a peg a grammar specifications, blah, blah, blah, go read that.", "tokens": [51256, 3301, 11, 490, 257, 17199, 257, 22317, 29448, 11, 12288, 11, 12288, 11, 12288, 11, 352, 1401, 300, 13, 51446], "temperature": 0.0, "avg_logprob": -0.29212844203895244, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.012051038444042206}, {"id": 495, "seek": 177066, "start": 1792.9, "end": 1797.5, "text": " Uh, linking documentation with definitions, uh, legal considerations, what", "tokens": [51476, 4019, 11, 25775, 14333, 365, 21988, 11, 2232, 11, 5089, 24070, 11, 437, 51706], "temperature": 0.0, "avg_logprob": -0.29212844203895244, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.012051038444042206}, {"id": 496, "seek": 179750, "start": 1797.5, "end": 1799.82, "text": " adoption is blah, mime type.", "tokens": [50364, 19215, 307, 12288, 11, 275, 1312, 2010, 13, 50480], "temperature": 0.0, "avg_logprob": -0.2685479987157534, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.008846652694046497}, {"id": 497, "seek": 179750, "start": 1799.9, "end": 1803.22, "text": " I'm, I'm hoping to get X dash Pagan from my type, but I haven't submitted that yet.", "tokens": [50484, 286, 478, 11, 286, 478, 7159, 281, 483, 1783, 8240, 430, 14167, 490, 452, 2010, 11, 457, 286, 2378, 380, 14405, 300, 1939, 13, 50650], "temperature": 0.0, "avg_logprob": -0.2685479987157534, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.008846652694046497}, {"id": 498, "seek": 179750, "start": 1803.62, "end": 1810.7, "text": " Trademarks, uh, blah, blah, licensing, it's Apache tube, uh, attribution,", "tokens": [50670, 22017, 443, 20851, 11, 2232, 11, 12288, 11, 12288, 11, 29759, 11, 309, 311, 46597, 9917, 11, 2232, 11, 9080, 1448, 11, 51024], "temperature": 0.0, "avg_logprob": -0.2685479987157534, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.008846652694046497}, {"id": 499, "seek": 179750, "start": 1810.7, "end": 1814.74, "text": " patents, uh, contributing, IFC, or you can't see the race.", "tokens": [51024, 38142, 11, 2232, 11, 19270, 11, 286, 18671, 11, 420, 291, 393, 380, 536, 264, 4569, 13, 51226], "temperature": 0.0, "avg_logprob": -0.2685479987157534, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.008846652694046497}, {"id": 500, "seek": 179750, "start": 1814.74, "end": 1817.58, "text": " So I would like to get this eventually submitted, but I don't want, I want to", "tokens": [51226, 407, 286, 576, 411, 281, 483, 341, 4728, 14405, 11, 457, 286, 500, 380, 528, 11, 286, 528, 281, 51368], "temperature": 0.0, "avg_logprob": -0.2685479987157534, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.008846652694046497}, {"id": 501, "seek": 179750, "start": 1817.58, "end": 1821.1, "text": " use it for like a year or two and have all the tools done for it before I do that.", "tokens": [51368, 764, 309, 337, 411, 257, 1064, 420, 732, 293, 362, 439, 264, 3873, 1096, 337, 309, 949, 286, 360, 300, 13, 51544], "temperature": 0.0, "avg_logprob": -0.2685479987157534, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.008846652694046497}, {"id": 502, "seek": 179750, "start": 1821.42, "end": 1825.82, "text": " Um, Pagan has, Pagan parsering has been used in production at at least one", "tokens": [51560, 3301, 11, 430, 14167, 575, 11, 430, 14167, 21156, 1794, 575, 668, 1143, 294, 4265, 412, 412, 1935, 472, 51780], "temperature": 0.0, "avg_logprob": -0.2685479987157534, "compression_ratio": 1.6643598615916955, "no_speech_prob": 0.008846652694046497}, {"id": 503, "seek": 182582, "start": 1825.82, "end": 1829.78, "text": " company that I know of besides mine, uh, for over a year and a half at this point.", "tokens": [50364, 2237, 300, 286, 458, 295, 11868, 3892, 11, 2232, 11, 337, 670, 257, 1064, 293, 257, 1922, 412, 341, 935, 13, 50562], "temperature": 0.0, "avg_logprob": -0.13574661067658406, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.004198310896754265}, {"id": 504, "seek": 182582, "start": 1830.1799999999998, "end": 1834.54, "text": " And so if you're wondering whether it's worthy of abuse and there has been, uh,", "tokens": [50582, 400, 370, 498, 291, 434, 6359, 1968, 309, 311, 14829, 295, 9852, 293, 456, 575, 668, 11, 2232, 11, 50800], "temperature": 0.0, "avg_logprob": -0.13574661067658406, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.004198310896754265}, {"id": 505, "seek": 182582, "start": 1834.54, "end": 1838.54, "text": " some tweaking to the specification in that time based on, uh, you know, the", "tokens": [50800, 512, 6986, 2456, 281, 264, 31256, 294, 300, 565, 2361, 322, 11, 2232, 11, 291, 458, 11, 264, 51000], "temperature": 0.0, "avg_logprob": -0.13574661067658406, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.004198310896754265}, {"id": 506, "seek": 182582, "start": 1838.54, "end": 1841.26, "text": " very intense usage that's happening over in that other company.", "tokens": [51000, 588, 9447, 14924, 300, 311, 2737, 670, 294, 300, 661, 2237, 13, 51136], "temperature": 0.0, "avg_logprob": -0.13574661067658406, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.004198310896754265}, {"id": 507, "seek": 182582, "start": 1841.74, "end": 1844.62, "text": " Um, so, so it's out there.", "tokens": [51160, 3301, 11, 370, 11, 370, 309, 311, 484, 456, 13, 51304], "temperature": 0.0, "avg_logprob": -0.13574661067658406, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.004198310896754265}, {"id": 508, "seek": 182582, "start": 1844.62, "end": 1845.7, "text": " You can go use it if you want.", "tokens": [51304, 509, 393, 352, 764, 309, 498, 291, 528, 13, 51358], "temperature": 0.0, "avg_logprob": -0.13574661067658406, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.004198310896754265}, {"id": 509, "seek": 182582, "start": 1845.7, "end": 1846.5, "text": " You can play around with it.", "tokens": [51358, 509, 393, 862, 926, 365, 309, 13, 51398], "temperature": 0.0, "avg_logprob": -0.13574661067658406, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.004198310896754265}, {"id": 510, "seek": 182582, "start": 1846.74, "end": 1848.7, "text": " If you ever want to write your own language, you probably can.", "tokens": [51410, 759, 291, 1562, 528, 281, 2464, 428, 1065, 2856, 11, 291, 1391, 393, 13, 51508], "temperature": 0.0, "avg_logprob": -0.13574661067658406, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.004198310896754265}, {"id": 511, "seek": 182582, "start": 1848.9399999999998, "end": 1851.9399999999998, "text": " I, I may very well do, uh, write a book about Pagan.", "tokens": [51520, 286, 11, 286, 815, 588, 731, 360, 11, 2232, 11, 2464, 257, 1446, 466, 430, 14167, 13, 51670], "temperature": 0.0, "avg_logprob": -0.13574661067658406, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.004198310896754265}, {"id": 512, "seek": 182582, "start": 1852.3799999999999, "end": 1854.8999999999999, "text": " Uh, I'm, I'm all about writing books these days.", "tokens": [51692, 4019, 11, 286, 478, 11, 286, 478, 439, 466, 3579, 3642, 613, 1708, 13, 51818], "temperature": 0.0, "avg_logprob": -0.13574661067658406, "compression_ratio": 1.747634069400631, "no_speech_prob": 0.004198310896754265}, {"id": 513, "seek": 185490, "start": 1854.9, "end": 1856.8600000000001, "text": " Um, I got to finish the terminal velocity.", "tokens": [50364, 3301, 11, 286, 658, 281, 2413, 264, 14709, 9269, 13, 50462], "temperature": 0.0, "avg_logprob": -0.17145035483620383, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.004397663287818432}, {"id": 514, "seek": 185490, "start": 1856.8600000000001, "end": 1858.18, "text": " That's the book I'm working on right now.", "tokens": [50462, 663, 311, 264, 1446, 286, 478, 1364, 322, 558, 586, 13, 50528], "temperature": 0.0, "avg_logprob": -0.17145035483620383, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.004397663287818432}, {"id": 515, "seek": 185490, "start": 1858.66, "end": 1863.66, "text": " And after that, uh, you know, these other books in spaces that are not covered by", "tokens": [50552, 400, 934, 300, 11, 2232, 11, 291, 458, 11, 613, 661, 3642, 294, 7673, 300, 366, 406, 5343, 538, 50802], "temperature": 0.0, "avg_logprob": -0.17145035483620383, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.004397663287818432}, {"id": 516, "seek": 185490, "start": 1863.66, "end": 1865.38, "text": " anything, uh, will be all right.", "tokens": [50802, 1340, 11, 2232, 11, 486, 312, 439, 558, 13, 50888], "temperature": 0.0, "avg_logprob": -0.17145035483620383, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.004397663287818432}, {"id": 517, "seek": 185490, "start": 1865.38, "end": 1869.38, "text": " I mean, write a book about bonsai, write a book about Pagan and, uh, some of these", "tokens": [50888, 286, 914, 11, 2464, 257, 1446, 466, 33922, 1301, 11, 2464, 257, 1446, 466, 430, 14167, 293, 11, 2232, 11, 512, 295, 613, 51088], "temperature": 0.0, "avg_logprob": -0.17145035483620383, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.004397663287818432}, {"id": 518, "seek": 185490, "start": 1869.38, "end": 1871.14, "text": " other things so that people can take them and use them.", "tokens": [51088, 661, 721, 370, 300, 561, 393, 747, 552, 293, 764, 552, 13, 51176], "temperature": 0.0, "avg_logprob": -0.17145035483620383, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.004397663287818432}, {"id": 519, "seek": 185490, "start": 1871.8200000000002, "end": 1873.38, "text": " So that's all I have to say about that.", "tokens": [51210, 407, 300, 311, 439, 286, 362, 281, 584, 466, 300, 13, 51288], "temperature": 0.0, "avg_logprob": -0.17145035483620383, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.004397663287818432}], "language": "en"}