1
00:00:00,000 --> 00:00:07,440
Today, we tackle the famous MNIST classification problem, which has become the Hello World

2
00:00:07,440 --> 00:00:10,160
of Deep Learning.

3
00:00:10,160 --> 00:00:16,560
I introduced the MNIST database way back in the first episode of this series.

4
00:00:16,560 --> 00:00:22,480
As a reminder, it's a dataset containing a collection of handwritten numbers from 0

5
00:00:22,480 --> 00:00:24,960
to 9.

6
00:00:24,960 --> 00:00:33,320
It was created in 1998 to answer the question, is it possible to teach a computer to recognize

7
00:00:33,320 --> 00:00:35,440
handwritten digits?

8
00:00:35,440 --> 00:00:42,960
So, is it possible to teach your computer how to recognize handwritten digits?

9
00:00:42,960 --> 00:00:48,440
Well, let's find out.

10
00:00:48,440 --> 00:00:53,160
Welcome to Julia for Talented Amateurs, where I may call some Julia tutorials for talented

11
00:00:53,160 --> 00:00:55,160
amateurs everywhere.

12
00:00:55,160 --> 00:01:00,480
I am your host, the Dabbling Doggo.

13
00:01:00,480 --> 00:01:06,480
In today's tutorial, we'll get an introduction to Artificial Neural Networks, or simply known

14
00:01:06,480 --> 00:01:09,160
as Neural Networks.

15
00:01:09,160 --> 00:01:13,600
If you look on the Wikipedia page for machine learning and expand the supervised learning

16
00:01:13,600 --> 00:01:18,320
section, you'll see Artificial Neural Networks listed there.

17
00:01:19,320 --> 00:01:24,600
But you'll also notice that there's a separate section further down called Artificial Neural

18
00:01:24,600 --> 00:01:26,600
Networks.

19
00:01:26,600 --> 00:01:33,640
If you expand that section, you'll see a lot of different subjects listed there.

20
00:01:33,640 --> 00:01:38,400
Artificial Neural Networks are part of a very large field of study that goes well beyond

21
00:01:38,400 --> 00:01:41,400
the scope of the series.

22
00:01:41,400 --> 00:01:46,880
There is no way I'm going to be able to cover all of those subjects now, so I will try to

23
00:01:46,880 --> 00:01:51,400
cover them in a future 13-part series.

24
00:01:51,400 --> 00:01:56,560
For this series, I will use this tutorial and the next tutorial to provide a high-level

25
00:01:56,560 --> 00:02:04,160
introduction to Artificial Neural Networks, which is the gateway to deep learning.

26
00:02:04,160 --> 00:02:08,480
There are many different ways to approach the MNIST classification problem using various

27
00:02:08,480 --> 00:02:11,480
programming languages.

28
00:02:11,480 --> 00:02:16,640
In today's tutorial, we'll go through a pure Julia approach by building an Artificial Neural

29
00:02:16,640 --> 00:02:21,560
Network using the flux.jl package.

30
00:02:21,560 --> 00:02:26,280
In the next tutorial, we'll take the code that we developed today and go through it

31
00:02:26,280 --> 00:02:30,800
in more detail to understand the concepts behind the code.

32
00:02:30,800 --> 00:02:36,960
So, the game plan for today is to get through all of the code and to get our model up and

33
00:02:36,960 --> 00:02:39,040
running.

34
00:02:39,040 --> 00:02:43,640
As a result, I will not be providing a lot of explanations today.

35
00:02:47,200 --> 00:02:52,920
For today's tutorial, knowledge of Julia and VS Code is required.

36
00:02:52,920 --> 00:02:59,320
I'm also assuming that you're watching this entire machine learning playlist.

37
00:02:59,320 --> 00:03:05,920
In your VS Code Explorer panel, create a new folder for this tutorial.

38
00:03:05,920 --> 00:03:14,960
In the tutorial folder, create a new file called sl underscore ann.jl.

39
00:03:14,960 --> 00:03:21,200
Change the Julia REPL by using ALT J then ALT O.

40
00:03:21,200 --> 00:03:25,120
Maximize the REPL panel.

41
00:03:25,120 --> 00:03:30,400
Change the present working directory to your tutorial directory.

42
00:03:30,400 --> 00:03:35,880
Enter the package REPL by hitting the closing square bracket.

43
00:03:35,880 --> 00:03:39,560
Activate your tutorial directory.

44
00:03:39,560 --> 00:03:57,080
Add the following packages, flux, images, ml data sets, and plots.

45
00:03:57,080 --> 00:04:01,480
Type in status to confirm the version numbers.

46
00:04:01,480 --> 00:04:06,080
Exit the package REPL by hitting backspace.

47
00:04:06,080 --> 00:04:08,960
Change the REPL panel.

48
00:04:08,960 --> 00:04:19,360
Okay, let's get started.

49
00:04:19,360 --> 00:04:26,000
Let's start by loading some packages.

50
00:04:26,000 --> 00:04:30,640
One of the quirks of the flux package is that you also need to load some of the specific

51
00:04:30,640 --> 00:04:35,840
functions that you want to use in your session.

52
00:04:35,840 --> 00:04:42,200
And finally, we need to use some of Julia's standard libraries.

53
00:04:42,200 --> 00:04:49,960
Using a random seed is not required, but I'm using it so I can reproduce my results.

54
00:04:49,960 --> 00:04:55,240
The ml data sets package contains several data sets that are commonly used in machine

55
00:04:55,240 --> 00:04:58,120
learning.

56
00:04:58,120 --> 00:05:03,080
One of those data sets is the mNIST data sets.

57
00:05:03,080 --> 00:05:11,320
The ml data sets package also contains a convenient way to load those data sets into memory.

58
00:05:11,320 --> 00:05:16,720
If this is your first time loading the mNIST data set onto your computer, you should be

59
00:05:16,720 --> 00:05:21,440
prompted to confirm whether or not you want to download the data set, since it's quite

60
00:05:21,440 --> 00:05:23,600
large.

61
00:05:23,600 --> 00:05:26,680
Select yes to begin the download.

62
00:05:26,680 --> 00:05:32,000
Depending on your internet connection, it may take a few minutes.

63
00:05:32,000 --> 00:05:37,080
Since I already have the mNIST data set on my computer, the data set was loaded right

64
00:05:37,080 --> 00:05:40,080
away.

65
00:05:40,080 --> 00:05:44,480
Now that we have the data, let's take a look at it.

66
00:05:44,480 --> 00:05:50,720
This is the input training data.

67
00:05:50,720 --> 00:05:57,560
So it looks like a tensor of floating point numbers that has 28 rows and 28 columns and

68
00:05:57,560 --> 00:06:01,400
is 60,000 layers deep.

69
00:06:01,400 --> 00:06:06,680
This is by far the largest data set that I've used on my channel to date.

70
00:06:06,680 --> 00:06:12,080
Unfortunately, there's not a great way to view it in the REPL.

71
00:06:12,080 --> 00:06:19,200
The 28 rows and 28 columns contain numbers from 0 to 1 that represent a black and white

72
00:06:19,200 --> 00:06:23,120
image of a handwritten digit.

73
00:06:23,120 --> 00:06:30,520
We can use the images package to view one of the images.

74
00:06:30,520 --> 00:06:34,440
For some reason, the images are loaded horizontally.

75
00:06:34,440 --> 00:06:38,960
You need to transpose the image in order to view it vertically, so don't forget that

76
00:06:38,960 --> 00:06:42,040
apostrophe.

77
00:06:42,040 --> 00:06:47,760
You should see a white handwritten number on a black background.

78
00:06:47,760 --> 00:06:53,000
No offense, but that's not exactly the nicest handwriting.

79
00:06:53,000 --> 00:06:57,960
But it's a useful sample since everyone has a slightly different handwriting style, and

80
00:06:57,960 --> 00:07:02,560
it's important for your computer to be able to read different handwritten versions of

81
00:07:02,560 --> 00:07:05,280
the same number.

82
00:07:05,280 --> 00:07:09,840
Now the question is, what number is it?

83
00:07:09,840 --> 00:07:15,640
Like any other data set used for classification problems, every sample in the MNIST data set

84
00:07:15,640 --> 00:07:19,760
includes a label with a correct number.

85
00:07:19,760 --> 00:07:24,560
Let's take a look at the label attached to this image.

86
00:07:24,560 --> 00:07:30,800
So the labels are contained in a column vector containing 60,000 integers.

87
00:07:30,800 --> 00:07:35,960
These labels are integers from 0 to 9.

88
00:07:35,960 --> 00:07:40,720
The label for the first sample is 5.

89
00:07:40,720 --> 00:07:47,320
If you look at the image, it sort of looks like a 5, and it sort of doesn't.

90
00:07:47,320 --> 00:07:52,000
If it's this hard for humans to figure out, you can imagine how challenging this will

91
00:07:52,000 --> 00:07:57,000
be for our computer to try to figure this out.

92
00:07:57,000 --> 00:08:00,680
Now let's take a look at the test data.

93
00:08:00,680 --> 00:08:07,560
In past tutorials, we took our data set and split it between training and testing.

94
00:08:07,560 --> 00:08:14,360
The MNIST data set contains 60,000 samples for training and another 10,000 samples for

95
00:08:14,360 --> 00:08:21,800
testing, so there's no need to split the data since it's already been split for you.

96
00:08:21,800 --> 00:08:26,680
Let's take a look at a testing sample.

97
00:08:26,680 --> 00:08:44,840
So it's the same 28x28 matrix for the image, but it's only 10,000 layers deep.

98
00:08:44,840 --> 00:08:48,360
So this image is easier to read.

99
00:08:48,360 --> 00:09:00,320
It looks like a 7, but let's check the label to be sure.

100
00:09:00,320 --> 00:09:06,000
So the label is indeed 7, which is comforting.

101
00:09:06,000 --> 00:09:11,640
Now that we have our data loaded into memory and have it split between inputs and outputs,

102
00:09:11,640 --> 00:09:19,560
as well as split between training and testing, we should be ready to build our model, right?

103
00:09:19,560 --> 00:09:22,600
Well no, not exactly.

104
00:09:22,600 --> 00:09:27,360
There are a couple of additional preprocessing steps that we need to do.

105
00:09:27,360 --> 00:09:33,440
Fortunately, the Flux.jl package comes included with utilities to make these preprocessing

106
00:09:33,440 --> 00:09:36,200
steps easy.

107
00:09:36,200 --> 00:09:42,440
For the input data, we need to, quote unquote, flatten the three-dimensional tensor into

108
00:09:42,440 --> 00:09:44,840
a two-dimensional matrix.

109
00:09:44,840 --> 00:09:51,560
Normally, that would mean reshaping our Julia array, but the Flux.jl package has a utility

110
00:09:51,560 --> 00:09:56,760
function called flatten that will do that for us.

111
00:09:56,760 --> 00:10:06,760
You can see that our 28x28x60,000 tensor is now a 784x60,000 matrix.

112
00:10:06,760 --> 00:10:14,880
All it did was take the 28x28 image and convert it into a column vector with 784 elements,

113
00:10:14,880 --> 00:10:21,240
so each column contains the floating point numbers associated with each image.

114
00:10:21,240 --> 00:10:27,240
We also need to flatten the inputs for the testing data as well.

115
00:10:27,240 --> 00:10:42,440
For the labels, we need to do something called one-hot encoding.

116
00:10:42,440 --> 00:10:46,480
If you look in the REPL, you'll see what it did.

117
00:10:46,480 --> 00:10:53,080
For each label, it replaced the integer with a column vector with a one at the index representing

118
00:10:53,080 --> 00:10:55,520
the integer.

119
00:10:55,520 --> 00:10:59,840
So in the first column, that one is in the sixth row.

120
00:10:59,840 --> 00:11:05,760
Well, you'll recall that the first label is five, not six.

121
00:11:05,760 --> 00:11:10,400
That's because this column vector starts at zero and goes to nine.

122
00:11:10,400 --> 00:11:16,160
So the sixth row represents the label for the number five.

123
00:11:16,160 --> 00:11:21,800
The one-hot batch function also concatenates all of these column vectors horizontally, so

124
00:11:21,800 --> 00:11:29,800
the result is a 10x60,000 one-hot matrix made up of Boolean values.

125
00:11:29,800 --> 00:11:34,200
All of those dots are zeros.

126
00:11:34,200 --> 00:11:41,200
We need to do the same thing for the testing labels.

127
00:11:41,200 --> 00:11:48,560
In the REPL, you can see that it's a similar result, except the testing data only has 10,000

128
00:11:48,560 --> 00:11:52,560
labels.

129
00:11:52,560 --> 00:12:02,880
Okay, we are now ready to define our model.

130
00:12:02,880 --> 00:12:08,600
Unlike other machine learning packages that we've seen so far, the flux.jl package is

131
00:12:08,600 --> 00:12:10,760
not an algorithm.

132
00:12:10,760 --> 00:12:16,560
Instead, it's a deep learning toolkit that provides building blocks that you can use to

133
00:12:16,560 --> 00:12:20,120
create your own custom deep learning models.

134
00:12:20,120 --> 00:12:26,680
We'll go through the flux package in more detail in the next tutorial, but for now, let's

135
00:12:26,680 --> 00:12:36,000
just use those building blocks to create our first artificial neural network.

136
00:12:36,000 --> 00:12:41,800
There's a lot going on in this deceptively simple code.

137
00:12:41,800 --> 00:12:46,760
Here's a visualization of what we just built.

138
00:12:46,760 --> 00:12:49,920
This is a diagram of a neural network.

139
00:12:49,920 --> 00:12:58,240
Specifically, this is an example of a multi-layer perceptron, or MLP, which is a type of artificial

140
00:12:58,240 --> 00:13:01,120
neural network.

141
00:13:01,240 --> 00:13:07,440
Although this model is more complex than any model that we've seen so far, the MLP is

142
00:13:07,440 --> 00:13:12,560
considered relatively simple in the deep learning world.

143
00:13:12,560 --> 00:13:18,520
We'll go through this diagram in more detail in the next tutorial, but at a very high level,

144
00:13:18,520 --> 00:13:24,880
this diagram represents a model containing a lot of parameters.

145
00:13:24,880 --> 00:13:29,680
We're going to feed our training inputs into this model, and then the model will try to

146
00:13:29,680 --> 00:13:35,400
learn the parameters necessary to predict the training labels, just like any other machine

147
00:13:35,400 --> 00:13:38,240
learning model.

148
00:13:38,240 --> 00:13:44,440
In other words, the model will digest the data and calculate a loss.

149
00:13:44,440 --> 00:13:50,600
Based on the result after a single epoch, the model will then update the parameter slightly

150
00:13:50,600 --> 00:13:56,800
in order to reduce the loss by using an optimization algorithm.

151
00:13:56,800 --> 00:14:01,520
Your model will repeat this process until you tell it to stop.

152
00:14:01,520 --> 00:14:08,920
I'll cover all of those other coding terms like chain, dense, reilu, and softmax in the

153
00:14:08,920 --> 00:14:10,800
next tutorial.

154
00:14:10,800 --> 00:14:20,920
Let's move on to defining the loss function.

155
00:14:20,920 --> 00:14:26,440
There are many different loss functions used in deep learning, and the flux.jl package

156
00:14:26,440 --> 00:14:29,680
supports all of the major loss functions.

157
00:14:29,680 --> 00:14:35,280
Today, we're using a loss function called cross entropy.

158
00:14:35,280 --> 00:14:41,080
Again, more on this in the next tutorial.

159
00:14:41,080 --> 00:14:45,360
Our model contains a lot of different parameters.

160
00:14:45,360 --> 00:14:51,760
The flux.jl package initializes all of those parameters using random values.

161
00:14:51,760 --> 00:14:58,600
Next, we need to select an optimization algorithm that will determine how our computer will

162
00:14:58,600 --> 00:15:02,880
quote unquote, learn the data.

163
00:15:02,880 --> 00:15:07,960
There are several different optimizers used in deep learning, and the flux.jl package

164
00:15:07,960 --> 00:15:11,320
supports all of the commonly used optimizers.

165
00:15:12,320 --> 00:15:21,320
Today, we're going to use an optimizer called atom, which is short for adaptive moment estimation.

166
00:15:21,320 --> 00:15:27,800
Okay, we are now ready to train our model.

167
00:15:27,800 --> 00:15:32,840
The flux.jl package comes included with a handy utility that makes it easy to train

168
00:15:32,840 --> 00:15:34,960
our model.

169
00:15:34,960 --> 00:15:39,480
All we need to do is provide a for loop to repeat the training process over multiple

170
00:15:39,480 --> 00:15:42,760
epochs.

171
00:15:42,760 --> 00:15:54,000
Just a warning, depending on your computer's CPU, this may take several minutes to train.

172
00:15:54,000 --> 00:16:09,720
I'm going to fast forward through this part.

173
00:16:09,720 --> 00:16:15,520
After an initial delay, you should see some outputs in the REPL, and if everything went

174
00:16:15,520 --> 00:16:27,320
okay, the training loss should be decreasing over time.

175
00:16:27,320 --> 00:16:31,880
We now have a trained model with lots of updated parameters.

176
00:16:31,880 --> 00:16:37,520
So, how do we use this model in order to make predictions?

177
00:16:38,520 --> 00:16:44,040
Well, we can just run our test data through our newly trained model in order to get the

178
00:16:44,040 --> 00:16:46,200
predictions.

179
00:16:46,200 --> 00:16:54,680
So, running our test data through our model results in a 10 by 10,000 matrix, which you

180
00:16:54,680 --> 00:16:57,440
can see in the REPL.

181
00:16:57,440 --> 00:17:03,160
It's a little difficult to read, but those crazy looking numbers are really small values

182
00:17:03,160 --> 00:17:05,600
close to zero.

183
00:17:05,600 --> 00:17:11,240
In each column, you should see a single value close to one.

184
00:17:11,240 --> 00:17:17,880
The sum of each column adds up to 100%, and each row contains the probability of the

185
00:17:17,880 --> 00:17:20,800
prediction.

186
00:17:20,800 --> 00:17:27,520
Remember that the index numbers for the rows go from 1 to 10, but our labels go from 0

187
00:17:27,520 --> 00:17:28,520
to 9.

188
00:17:28,520 --> 00:17:35,040
So, the first row is the probability that the image is a zero, and the second row is

189
00:17:35,040 --> 00:17:40,880
the probability that the image is a one, and so on.

190
00:17:40,880 --> 00:17:46,600
In order to make it easier to work with these predictions, we can use the one cold utility

191
00:17:46,600 --> 00:17:52,800
function from the flux.jl package, which is like the opposite of the one hot batch utility

192
00:17:52,800 --> 00:17:55,300
function.

193
00:17:55,300 --> 00:18:00,800
A one cold function converts a matrix into a column vector containing the index number

194
00:18:00,800 --> 00:18:04,400
that has the highest probability value.

195
00:18:04,400 --> 00:18:10,320
In order to convert index numbers into labels, we need to subtract one from each of the index

196
00:18:10,320 --> 00:18:12,920
numbers.

197
00:18:12,920 --> 00:18:19,480
You can see that our predictions are now contained in a column vector with 10,000 elements.

198
00:18:19,480 --> 00:18:25,680
Now, all we need to do is compare the predicted labels with the actual labels for the test

199
00:18:25,680 --> 00:18:27,200
data.

200
00:18:28,200 --> 00:18:36,440
So, our little artificial neural network model achieved an accuracy score of 96.24%, which

201
00:18:36,440 --> 00:18:40,720
is pretty amazing considering how difficult the challenge is.

202
00:18:40,720 --> 00:18:47,360
Now, before we go out and celebrate, let's take a look at the current best-in-class accuracy

203
00:18:47,360 --> 00:18:49,800
score.

204
00:18:49,800 --> 00:18:55,400
According to the Wikipedia article on the MNIST database, the highest average accuracy

205
00:18:55,400 --> 00:19:03,960
score for any machine learning model is 99.83%, which was achieved in 2020 using something

206
00:19:03,960 --> 00:19:08,800
called a convolutional neural network.

207
00:19:08,800 --> 00:19:14,080
In any event, let's take a look at some of our misclassifications to see if we can gain

208
00:19:14,080 --> 00:19:32,720
any insights.

209
00:19:32,720 --> 00:19:36,880
So this table contains 10,000 rows.

210
00:19:36,880 --> 00:19:44,880
The first column is the index number for both the predictions as well as the actual

211
00:19:44,880 --> 00:19:47,160
label data.

212
00:19:47,160 --> 00:19:52,920
The second column contains the predicted labels, and the third column contains the actual

213
00:19:52,920 --> 00:19:56,120
labels from the test data sets.

214
00:19:56,120 --> 00:20:02,520
The fourth column contains one for true if the predicted label has been classified correctly,

215
00:20:02,520 --> 00:20:08,920
and a zero for false if the predicted label has been misclassified.

216
00:20:08,920 --> 00:20:19,640
If you sort on column 4, you can see all of the misclassifications.

217
00:20:19,640 --> 00:20:27,200
So there are 376 misclassifications, which sounds like a lot, but remember, there are

218
00:20:27,200 --> 00:20:31,320
10,000 test samples.

219
00:20:31,320 --> 00:20:47,000
Let's take a look at the first misclassification, which has an index number of 9.

220
00:20:47,000 --> 00:20:51,880
So I don't know about you, but I cannot read this handwriting.

221
00:20:51,880 --> 00:21:03,360
I mean, what is that?

222
00:21:03,360 --> 00:21:10,760
It's labeled as a 5, but does that look like a 5 to you?

223
00:21:10,760 --> 00:21:17,200
Our model guessed 4, and you can kind of forgive it since I don't think I could have classified

224
00:21:17,200 --> 00:21:21,480
this image correctly as a 5 just by looking at it.

225
00:21:21,480 --> 00:21:25,000
Anyways, you get my point.

226
00:21:25,000 --> 00:21:31,600
This is not a trivial classification problem, and it's amazing that any model can achieve

227
00:21:31,600 --> 00:21:37,000
accuracy levels that are comparable to human beings, given the wide range in handwriting

228
00:21:37,000 --> 00:21:40,160
styles.

229
00:21:40,160 --> 00:21:43,480
So that was fun, right?

230
00:21:43,480 --> 00:22:12,000
Here we go, let's plot a learning curve to see how our model did while training.

231
00:22:12,000 --> 00:22:18,560
So this learning curve looks similar to other learning curves that we've seen in past tutorials.

232
00:22:18,560 --> 00:22:23,360
Even though the code for this neural network looks very different, the overall learning

233
00:22:23,360 --> 00:22:29,560
workflow is very similar to other machine learning algorithms.

234
00:22:29,560 --> 00:22:40,360
Let's save this plot and recap what we just witnessed.

235
00:22:40,360 --> 00:22:47,040
Today we became very familiar with the MNIST data sets, and we got a quick introduction

236
00:22:47,040 --> 00:22:50,680
to the Flux.jl package.

237
00:22:50,680 --> 00:22:56,400
After doing a little preprocessing work on the data, we immediately built an artificial

238
00:22:56,400 --> 00:23:03,320
neural network model using the tools provided by the Flux.jl package.

239
00:23:03,320 --> 00:23:08,760
Then we followed a similar workflow that we used with other machine learning algorithms

240
00:23:08,760 --> 00:23:14,960
by defining a loss function, by initializing parameters, by selecting an optimizer, and

241
00:23:14,960 --> 00:23:18,800
by training our model using a for loop.

242
00:23:18,800 --> 00:23:25,880
But the actual details of that workflow are very different with a lot of new terms.

243
00:23:25,880 --> 00:23:31,760
In the end, we were able to use this trained model in order to make predictions, like we

244
00:23:31,760 --> 00:23:35,480
did with other machine learning models.

245
00:23:35,480 --> 00:23:40,920
We were also able to calculate an accuracy score and plot a learning curve.

246
00:23:40,920 --> 00:23:48,480
Hopefully, today's tutorial has left you both excited and confused.

247
00:23:48,480 --> 00:23:52,840
After seeing what's possible with artificial neural networks, you probably want to use

248
00:23:52,840 --> 00:23:58,320
it right away, but may be hesitant since you may be wondering what's going on under the

249
00:23:58,320 --> 00:24:00,680
hood.

250
00:24:00,680 --> 00:24:06,480
In the next tutorial, we'll revisit this code in order to gain a better understanding of

251
00:24:06,480 --> 00:24:12,600
the concepts that make these artificial neural networks such a modern marvel.

252
00:24:12,600 --> 00:24:16,600
So stay tuned for that.

253
00:24:16,600 --> 00:24:21,160
Well, that's all for today.

254
00:24:21,160 --> 00:24:27,040
If you made it this far, congratulations!

255
00:24:27,040 --> 00:24:32,120
If you enjoyed this video and you feel like you learned something new, please give it

256
00:24:32,120 --> 00:24:34,440
a thumbs up.

257
00:24:34,440 --> 00:24:41,520
For more wholesome Julia tutorials, please be sure to subscribe and hit that bell.

258
00:24:41,520 --> 00:24:47,640
If you like what I do, then please consider joining and becoming a channel member.

259
00:24:47,640 --> 00:24:52,600
New tutorials are posted on Sundays slash Mondays.

260
00:24:52,600 --> 00:24:56,600
Thanks for watching, and I'll see you in the next video.

