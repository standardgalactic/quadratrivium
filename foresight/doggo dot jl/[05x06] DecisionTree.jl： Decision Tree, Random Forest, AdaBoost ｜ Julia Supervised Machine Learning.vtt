WEBVTT

00:00.000 --> 00:07.400
Last week, we learned about the Support Vector Machine, which is a powerful algorithm with a science-fiction feel.

00:08.200 --> 00:15.300
But despite its usefulness, it can be difficult to understand and even more difficult to explain.

00:16.600 --> 00:23.300
What if I told you that there's another supervised machine learning algorithm that's also powerful and useful,

00:23.500 --> 00:30.000
but is grounded in real life so that it's much easier to understand and much easier to explain?

00:32.100 --> 00:35.600
That algorithm is the Decision Tree.

00:37.400 --> 00:43.000
The Decision Tree algorithm is one of the most popular machine learning algorithms in use today,

00:43.900 --> 00:47.800
but it's different than any of the algorithms that we've studied so far.

00:49.500 --> 00:52.500
What makes the Decision Tree algorithm so different?

00:54.200 --> 00:56.400
Well, let's find out.

00:58.900 --> 01:05.200
Welcome to Julia for Talented Amateurs, where I may call some Julia tutorials for Talented Amateurs everywhere.

01:05.900 --> 01:08.900
I am your host, TheDabblingDoggo.

01:09.500 --> 01:10.200
I dabble.

01:11.300 --> 01:16.100
For those of you who are visual learners, you're going to love the Decision Tree.

01:17.100 --> 01:25.200
While other machine learning algorithms can be highly abstract and difficult to visualize, the Decision Tree is just the opposite.

01:26.500 --> 01:31.100
The way a Decision Tree works is similar to how you might make decisions every day.

01:32.500 --> 01:37.900
The idea is that you can make a big decision based on a collection of little decisions.

01:38.900 --> 01:46.900
For example, if you're about to go outside during the rainy season, you might wonder, should I bring an umbrella?

01:48.400 --> 01:53.900
Before answering that question, your brain may ask a collection of smaller questions first.

01:54.400 --> 01:59.900
For example, it might ask a simple yes or no question like, is it raining now?

02:01.400 --> 02:04.900
If the answer is yes, then it tells you to bring an umbrella.

02:05.900 --> 02:13.900
If the answer is no, then it might ask another simple question like, is it forecasted to rain later today?

02:14.900 --> 02:18.900
If the answer is yes, it tells you to bring an umbrella.

02:19.900 --> 02:24.900
If the answer is no, then it tells you that you don't need to bring an umbrella.

02:25.900 --> 02:35.900
Whether you're conscious of these micro-decisions or not, your brain is going through a similar thought process as you make countless decisions throughout your day.

02:36.900 --> 02:42.900
What if you could build a computer model that followed that thought process to help your computer make decisions?

02:43.900 --> 02:47.900
That's the idea behind the Decision Tree algorithm.

02:48.900 --> 02:58.900
The Decision Tree is used in several different academic disciplines, but in machine learning, it's typically used for supervised machine learning.

02:59.900 --> 03:09.900
The Decision Tree may be used for either classification or regression, but in this tutorial, I will only be covering Decision Trees for classification.

03:10.900 --> 03:21.900
As an academic subject, Decision Trees have been around for over 100 years, but more recently, Decision Trees have been adapted for the computer age.

03:22.900 --> 03:34.900
There are several different Decision Tree algorithms used in machine learning, but the version that we will be using is called Classification and Regression Trees, or CART for short.

03:35.900 --> 03:48.900
The first version of the CART algorithm was completed in 1977 by Leo Breiman and Charles Stone from UC Berkeley and Jerome Friedman and Richard Olsson from Stanford University.

03:49.900 --> 04:02.900
In 1984, they published a book appropriately named Classification and Regression Trees, which detailed both the concepts and the mechanics of constructing Decision Trees.

04:04.900 --> 04:14.900
Decision Trees are typically visualized using some form of graph theory representation, meaning a visual combination of nodes and edges.

04:15.900 --> 04:25.900
Decision Trees are inverted trees with a root node at the top. A root node contains all of the data in the training data sets.

04:27.900 --> 04:37.900
For a binary decision, there are two edges coming out of the root node, typically representing a yes-no or true-false decision.

04:38.900 --> 04:41.900
These edges are also called branches.

04:42.900 --> 04:46.900
Connected to those branches will be another node.

04:47.900 --> 04:52.900
This time, the node may either be a leaf node or a decision node.

04:54.900 --> 05:04.900
A leaf node contains a subset of the training data and represents a final classification, so there are no further decisions to be made for that subset.

05:05.900 --> 05:13.900
A decision node also contains a subset of the training data set, but requires additional decisions to be made.

05:15.900 --> 05:23.900
Like the root node, the decision node has two edges coming out of it to represent a different yes-no or true-false question.

05:23.900 --> 05:36.900
Unless given constraints, the Decision Tree algorithm will continue this process of interrogating the data and growing the tree until every sample of data has been classified.

05:38.900 --> 05:46.900
Since the objective of using a Decision Tree algorithm is to create a model that can be used to make predictions on unseen data,

05:46.900 --> 05:54.900
it's a good practice to place some constraints on the Decision Tree algorithm so that it stops going through your data at some point.

05:55.900 --> 06:02.900
That way, you can have a more generalized model that you can use to classify new unseen data.

06:04.900 --> 06:11.900
Now that you know what a Decision Tree looks like, let's build one using the DecisionTree.jl package.

06:12.900 --> 06:26.900
While the Decision Tree is easy to visualize, it is surprisingly difficult to implement the code from scratch, so we'll be using the DecisionTree.jl package.

06:27.900 --> 06:31.900
DecisionTree.jl was created by Ben Sadeghi.

06:32.900 --> 06:42.900
On the surface, DecisionTree.jl seems like a simple package, but the more you dig into it, the more you'll understand how sophisticated it really is.

06:44.900 --> 06:49.900
DecisionTree.jl is not a wrapper to a library written in a different language.

06:50.900 --> 06:56.900
Instead, DecisionTree.jl is a pure Julia implementation of the CART algorithm.

06:57.900 --> 07:00.900
Let's start by setting up our programming environment.

07:02.900 --> 07:06.900
For today's tutorial, knowledge of Julia and VS Code is required.

07:07.900 --> 07:15.900
I'm also assuming that you're watching this entire machine learning playlist, so episodes 501 through 505 are required.

07:17.900 --> 07:22.900
In your VS Code Explorer panel, create a new folder for this tutorial.

07:22.900 --> 07:31.900
In the tutorial folder, create a new file called SL underscore trees.jl.

07:33.900 --> 07:38.900
Launch the Julia REPL by using ALT J, then ALT O.

07:40.900 --> 07:45.900
For this tutorial, I think it's useful to dock the REPL panel to the right side panel.

07:46.900 --> 07:51.900
This is optional, but we will be referring to the REPL throughout this tutorial.

07:53.900 --> 08:01.900
In order to dock the REPL to the right side panel, you can click and drag the REPL panel header until your mouse reaches the right edge of your window.

08:03.900 --> 08:07.900
A panel highlight should appear indicating that you can dock it there.

08:09.900 --> 08:13.900
Once it's docked, you can adjust the panel size to your liking.

08:13.900 --> 08:23.900
For those of you who use Julia's workspace panel, you can also dock the workspace panel in the right side panel if you wish.

08:26.900 --> 08:31.900
In the REPL, change the present working directory to your tutorial directory.

08:33.900 --> 08:36.900
Enter the package REPL by hitting the closing square bracket.

08:38.900 --> 08:40.900
Activate your tutorial directory.

08:41.900 --> 08:44.900
Add the decision tree.jl package.

08:47.900 --> 08:50.900
Type in status to confirm the version number.

08:52.900 --> 08:55.900
Exit the package REPL by hitting Backspace.

08:57.900 --> 09:02.900
Adjust the panels in VS Code so that you can see both your text editor as well as the REPL panel.

09:05.900 --> 09:07.900
Okay, let's get started.

09:10.900 --> 09:12.900
Let's start by loading some packages.

09:18.900 --> 09:20.900
Next, let's load some data.

09:22.900 --> 09:28.900
We'll be using the iris dataset as our motivating example to learn how to make predictions using decision trees.

09:29.900 --> 09:35.900
The decision tree.jl package comes included with its own version of the iris flower dataset.

09:36.900 --> 09:44.900
For some reason, there are some slight differences between this dataset and the one we used last week from the rdatasets package.

09:45.900 --> 09:53.900
It's not a material difference and it won't change the outcome of our models, but I did want to let you know that these are not identical datasets.

09:54.900 --> 10:03.900
The load underscore data function is from the decision tree.jl package and it splits the data for you between inputs and outputs.

10:05.900 --> 10:11.900
But if you view the data, you'll see that there are no Julia data types assigned to that data.

10:12.900 --> 10:18.900
As you can see, both the inputs and the outputs have a data type of any.

10:18.900 --> 10:31.900
In order to improve performance, the decision tree.jl package recommends assigning data types to your data before building any models.

10:41.900 --> 10:44.900
This dataset should look familiar to you.

10:45.900 --> 10:54.900
In this particular dataset, there is no header row, but as a reminder, the first four columns are the features, which are the various measurements and centimeters.

10:55.900 --> 11:00.900
And the fifth column contains the labeled class data, which is one of three classes.

11:01.900 --> 11:08.900
Satosa, Versicolor, or Virginica, which are the three different species of the iris flower.

11:09.900 --> 11:14.900
Next, we will need to split this data between training and testing.

11:15.900 --> 11:19.900
In order to do this, we'll be using Huda Nassar's code from last week.

11:20.900 --> 11:27.900
I am simply going to copy and paste this code from last week's tutorial so that you won't have to watch me retype this code.

11:27.900 --> 11:34.900
I'm going to go through the next few steps fairly quickly, since it's the same workflow that we used last week.

11:36.900 --> 11:45.900
I'm using the same random seed that I used last week, so the index numbers we used this week for training and testing should be the same.

11:46.900 --> 11:51.900
Next, we need to split the features between training and testing.

11:52.900 --> 12:00.900
I'm using the same random seed that I used last week, so the index numbers we used this week for training and testing should be the same as we used last week.

12:05.900 --> 12:10.900
Next, we need to split the features between training and testing.

12:16.900 --> 12:21.900
And then, we need to split the classes between training and testing.

12:26.900 --> 12:33.900
Unlike last week, we do not need to transpose our data, since the DecisionTree.jl package does not require it.

12:36.900 --> 12:40.900
Okay, we are now ready to build our first DecisionTree.

12:46.900 --> 12:54.900
In order to create a DecisionTree, all you need to do is use the DecisionTree classifier constructor.

12:56.900 --> 13:08.900
There are a lot of different keyword arguments that you can include in this constructor, but I'm only using the max underscore depth keyword argument, which will stop the tree from growing too large.

13:09.900 --> 13:15.900
You can find all of the available keyword arguments in the DecisionTree.jl documentation.

13:16.900 --> 13:29.900
The DecisionTree.jl package also includes a DecisionTree regressor constructor, which is used for regression problems, but I will only be covering the classifier in this tutorial.

13:31.900 --> 13:36.900
The rest of the workflow is very similar to the workflow that we use for SVM.

13:37.900 --> 13:42.900
You can fit your training data to the model by using the FitBang function.

13:44.900 --> 13:45.900
And that's it!

13:46.900 --> 13:51.900
You can see your DecisionTree in the REPL by using the print underscore tree function.

13:54.900 --> 13:59.900
I know it doesn't look like much, but there's a lot of information in this compact output.

14:01.900 --> 14:03.900
Here's the visualization of that output.

14:04.900 --> 14:07.900
The top line of the output is the root node.

14:08.900 --> 14:19.900
The algorithm has scanned all of the features and has determined that splitting feature number 4, or the fourth column, will provide the most useful information.

14:20.900 --> 14:25.900
The algorithm has selected a value of 0.8 as the threshold.

14:26.900 --> 14:35.900
Samples that have a feature 4 value of less than 0.8 go into the left node, and everything else goes into the right node.

14:37.900 --> 14:42.900
With the left node, the algorithm has determined that no further splits are required.

14:43.900 --> 14:49.900
This left node is a leaf node, and the classification is Satosa.

14:50.900 --> 14:58.900
The 35 slash 35 means that 35 samples have a class label of Satosa out of 35 total samples.

15:00.900 --> 15:08.900
The right node is a Decision node, since the algorithm has determined that more information may be gained by splitting the data further.

15:09.900 --> 15:27.900
For this subset of data, the algorithm has determined that splitting feature number 3, or the third column, will yield the most useful information, and it has selected a value of 4.95 as the threshold.

15:28.900 --> 15:38.900
Samples with a feature 3 value of less than 4.95 go into the left node, and everything else goes into the right node.

15:39.900 --> 15:47.900
At this point, the tree stops growing because we use the keyword argument max underscore depth equals 2.

15:48.900 --> 15:55.900
If we didn't use that keyword argument, the tree would continue to grow until the algorithm determined its own stopping point.

15:57.900 --> 16:01.900
As a result, the last two nodes are leaf nodes by default.

16:02.900 --> 16:14.900
For the left node, the algorithm is classifying that data subset as VersaColor, and for the right node, the algorithm is classifying that data subset as Virginia.

16:16.900 --> 16:20.900
But the algorithm knows that there are some misclassifications.

16:21.900 --> 16:34.900
In the left node, the 25 slash 26 means that 25 samples have a class label of VersaColor out of 26 total samples, meaning there's one sample that has been misclassified.

16:35.900 --> 16:49.900
In the right node, that 32 slash 33 means that 32 samples have a class label of Virginia out of 33 total samples, meaning there's one sample that has been misclassified.

16:50.900 --> 16:58.900
In order to get a better understanding of what the algorithm is looking at, let's take a look at the actual data subsets.

16:59.900 --> 17:04.900
Let's start by looking at the training data, which is what the root node is looking at.

17:10.900 --> 17:14.900
This data subset contains 94 samples.

17:15.900 --> 17:20.900
If you add up all the numbers in the decision tree nodes, you should get 94.

17:20.900 --> 17:30.900
If you sort column 4, you can see why the algorithm selected 0.8 as the threshold for feature number 4.

17:33.900 --> 17:38.900
It doesn't matter if it's less than or less than or equal to 0.8.

17:39.900 --> 17:45.900
As you can see, none of the samples have a feature 4 value of 0.8.

17:46.900 --> 17:52.900
Instead, there's a clear dividing line between the values of 0.6 and 1.

17:53.900 --> 17:57.900
0.8 is just the average between those two values.

17:59.900 --> 18:05.900
There are 35 samples above the line, and all of them have a class label of Satosa.

18:06.900 --> 18:09.900
That's where the 35 slash 35 comes from.

18:10.900 --> 18:16.900
Now, let's take a closer look at the data subset that was sent to the decision node on the right.

18:27.900 --> 18:34.900
This subset contains 59 samples, and the class labels are either VersaColor or Virginica.

18:34.900 --> 18:41.900
If you sort on column 3, you'll see why the algorithm selected 4.95 as the threshold.

18:45.900 --> 18:50.900
There is no sample with a feature 3 value of 4.95.

18:51.900 --> 18:59.900
Instead, there's a clear dividing line between 4.9 and 5, so 4.95 is just the average.

19:00.900 --> 19:05.900
There are 26 samples above the line, and 33 samples below the line.

19:07.900 --> 19:15.900
Out of the samples above the line, 25 have a class label of VersaColor, which is where the 25 slash 26 comes from.

19:17.900 --> 19:25.900
And of the samples below the line, 32 have a class label of Virginica, which is where the 32 slash 33 comes from.

19:30.900 --> 19:35.900
So in each class, there's one sample that has been misclassified.

19:36.900 --> 19:40.900
You can see the misclassified samples in this data view.

19:42.900 --> 19:49.900
Being able to visualize this much detail is unlike any machine learning algorithm that we've used in the past,

19:50.900 --> 19:53.900
which is one of the reasons why decision trees are so popular.

19:54.900 --> 20:02.900
But we already know that this model has some mistakes in it, so let's see how well it does at making predictions.

20:03.900 --> 20:08.900
Like last week, you can make predictions by using the predict function.

20:13.900 --> 20:18.900
Now that we have a prediction, we can check the accuracy of that prediction.

20:19.900 --> 20:26.900
So this accuracy score of 89% is not very good.

20:28.900 --> 20:34.900
You'll recall that our SVM model got an accuracy score of 95% right out of the box.

20:36.900 --> 20:39.900
It seems like all of this transparency came at a price.

20:41.900 --> 20:45.900
Although decision trees are easy to understand and easy to explain,

20:46.900 --> 20:49.900
they are often not very good at making predictions,

20:50.900 --> 20:55.900
which of course is the primary reason for building machine learning classification models in the first place.

20:58.900 --> 21:04.900
Let's take a deeper dive into the misclassifications to gain a better understanding of what went wrong.

21:06.900 --> 21:11.900
A common analytic tool used in machine learning is called a confusion matrix.

21:12.900 --> 21:19.900
A confusion matrix provides a simple way to visualize where your model got confused while making predictions.

21:21.900 --> 21:26.900
The decisionTree.jl package comes included with a confusion matrix constructor.

21:30.900 --> 21:36.900
The way to read this confusion matrix is that the classes from the first arguments are in the rows,

21:36.900 --> 21:40.900
and the classes from the second arguments are in the columns.

21:41.900 --> 21:46.900
So since we used y underscore test as the first argument,

21:46.900 --> 21:52.900
the rows show the counts for the actual labeled data that we used for testing purposes.

21:53.900 --> 21:57.900
Since we used y underscore hat as the second argument,

21:57.900 --> 22:02.900
the columns show the classes predicted by our decision tree model.

22:04.900 --> 22:08.900
The numbers shown along the diagonal are the correct predictions,

22:08.900 --> 22:12.900
and any other number represents an incorrect prediction.

22:13.900 --> 22:23.900
For example, that one indicates that our model predicted for genica once when the actual labeled class was versicolor.

22:24.900 --> 22:32.900
That five indicates that our model predicted versicolor five times when the actual labeled class was virginica.

22:33.900 --> 22:40.900
The accuracy score shown here matches the accuracy score that we calculated manually.

22:41.900 --> 22:47.900
The kappa coefficient is a metric that you can derive from this confusion matrix.

22:48.900 --> 22:56.900
I won't go into the math, but the kappa coefficient offers a more nuanced assessment of your model's predictive abilities.

22:57.900 --> 23:06.900
When making predictions, there's always a possibility that your model made a correct prediction purely by chance,

23:07.900 --> 23:10.900
thus artificially inflating the accuracy score.

23:12.900 --> 23:17.900
The kappa coefficient attempts to remove the probability of random positive predictions

23:17.900 --> 23:23.900
in order to present a more conservative way to assess your model's performance.

23:24.900 --> 23:31.900
As you can see, after factoring out the probability of making a correct prediction just by getting lucky,

23:32.900 --> 23:36.900
the accuracy score is closer to 83%.

23:38.900 --> 23:45.900
We can see the incorrect predictions using the same code that we used last week to display the results.

23:53.900 --> 24:08.900
As a reminder, the first column contains the predictions, and the second column contains the class from the labeled test data,

24:09.900 --> 24:13.900
and the third column indicates whether or not the prediction was correct.

24:15.900 --> 24:21.900
If you sort on column three, you can see there's one sample where our model predicted virginica

24:21.900 --> 24:24.900
when the actual species was versicolor.

24:25.900 --> 24:32.900
And there are five samples where our model predicted versicolor when the actual species was virginica,

24:33.900 --> 24:36.900
which is consistent with what the confusion matrix showed.

24:39.900 --> 24:44.900
You'll recall from last week that the SVM model is non-probabilistic,

24:45.900 --> 24:48.900
meaning that it makes predictions without assigning probabilities.

24:51.900 --> 24:53.900
The decision tree algorithm is different.

24:54.900 --> 25:02.900
You can see the probabilities determined by the decision tree model by using the predict underscore proper function.

25:08.900 --> 25:12.900
So this display shows the 56 samples in the test data set.

25:13.900 --> 25:19.900
Column one is the probability that the test sample has a class label of Satosa.

25:20.900 --> 25:25.900
Column two is the probability that the test sample has a class label of versicolor.

25:26.900 --> 25:31.900
And column three is the probability that the test sample has a class label of virginica.

25:33.900 --> 25:41.900
As you can see, our model is 100% confident that the first 15 samples have a class label of Satosa.

25:42.900 --> 25:46.900
But for every other sample, our model is not as confident,

25:46.900 --> 25:50.900
since it knows that there's some probability of misclassification.

25:51.900 --> 25:58.900
All of the other samples show a 96% probability when predicting either versicolor or virginica.

26:01.900 --> 26:04.900
Okay, so now what do we do?

26:06.900 --> 26:10.900
The decision tree is great since it's a wealth of information,

26:11.900 --> 26:13.900
but it's not so great at making predictions,

26:13.900 --> 26:16.900
which is really what we need.

26:17.900 --> 26:22.900
The good news is that there are a couple of ways to improve this accuracy score,

26:23.900 --> 26:28.900
but like many things in life, those improvements will come with some tradeoffs.

26:29.900 --> 26:37.900
One of those tradeoffs is a fundamental concept in machine learning called the bias variance tradeoff.

26:37.900 --> 26:47.900
On the surface, the bias variance tradeoff may seem like a relatively easy concept to understand,

26:48.900 --> 26:53.900
but upon closer inspection, one learns that it is very nuanced,

26:54.900 --> 26:57.900
so it's surprisingly difficult to implement in real life.

26:58.900 --> 27:04.900
The bias variance tradeoff is typically visualized by these three diagrams.

27:05.900 --> 27:10.900
The diagram on the left is an example of underfitting the data,

27:11.900 --> 27:17.900
meaning that it hasn't captured enough of the relevant relationships between features and outputs.

27:18.900 --> 27:23.900
The diagram on the right is an example of overfitting the data,

27:24.900 --> 27:27.900
meaning that it has learned the training data too well,

27:28.900 --> 27:33.900
so that it will make a lot of mistakes when trying to make predictions using new data.

27:35.900 --> 27:40.900
The diagram in the middle is an example of a model being just right,

27:41.900 --> 27:46.900
meaning that it has captured enough of the relevant relationships between features and outputs,

27:47.900 --> 27:53.900
while still being able to be generalized so that it can be used to make predictions using unseen data.

27:55.900 --> 27:59.900
So these diagrams are really from the data's perspective,

27:59.900 --> 28:04.900
so they aren't necessarily helpful when you're trying to design a model.

28:05.900 --> 28:12.900
A different way to visualize the bias variance tradeoff is by looking at these four diagrams,

28:13.900 --> 28:15.900
which is from the model's perspective.

28:16.900 --> 28:24.900
The lower left diagram shows a model with low bias and low variance, which is the best case scenario.

28:25.900 --> 28:33.900
The upper right diagram shows a model with high bias and high variance, which is the worst case scenario.

28:35.900 --> 28:40.900
In reality, most models are neither best case nor worst case.

28:41.900 --> 28:50.900
Instead, most models fall along that diagonal region between the upper left diagram and the lower right diagram,

28:51.900 --> 29:00.900
meaning that your model will most likely have a high bias and low variance, or low bias and high variance.

29:02.900 --> 29:07.900
The best that you can hope to achieve is some result near the middle of all four diagrams,

29:08.900 --> 29:11.900
resulting in a model with some bias and some variance.

29:12.900 --> 29:22.900
The model generated by the decision tree algorithm is an example of a model with high variance and low bias,

29:23.900 --> 29:26.900
meaning that it should be placed in that lower right box.

29:28.900 --> 29:32.900
Decision trees have a natural tendency to overfit the data,

29:33.900 --> 29:40.900
since in theory, it can continue to split your data until it has correctly classified every sample in the training data sets.

29:42.900 --> 29:51.900
But that's no good, since you can't generalize such a model, since new unseen data will most likely be different than the training data.

29:53.900 --> 30:00.900
You can improve the predictive accuracy of decision trees by using something called ensemble learning.

30:01.900 --> 30:07.900
There are two main ensemble learning techniques that are used to improve the performance of decision trees.

30:08.900 --> 30:12.900
One is called bagging, and the other is called boosting.

30:14.900 --> 30:21.900
We'll cover bagging first. An example of a bagging algorithm is known as random forest.

30:25.900 --> 30:29.900
Bagging is a shortened form of the term bootstrap aggregating.

30:30.900 --> 30:40.900
Without getting into the details, the concept behind bagging is that you can reduce the variance of your machine learning model by doing two things.

30:41.900 --> 30:49.900
One, you can increase the independence of your features, and two, you can increase the number of models.

30:50.900 --> 31:00.900
You then take this increased number of models with features that are more independent, then you aggregate them into one metamodel.

31:01.900 --> 31:07.900
The result is a model with lower variance, but with a slightly higher bias.

31:09.900 --> 31:12.900
This is the bias variance tradeoff in action.

31:13.900 --> 31:20.900
One of the best known examples of a bagging algorithm is random forest.

31:21.900 --> 31:29.900
The first algorithm for random decision forests was created in 1995 by Tin Cam Ho.

31:30.900 --> 31:39.900
The way that random forest works is that it introduces independence between the features and then generates multiple decision tree models.

31:40.900 --> 31:49.900
But the random forest introduces even more randomness into the algorithm by only considering a fraction of the features at every split.

31:50.900 --> 31:57.900
This results in even more independence between the features, which results in even lower variance.

31:59.900 --> 32:04.900
I realize that this is a lot of hand-waving, so let's see random forest in action.

32:05.900 --> 32:12.900
Fortunately, we can use the decisionTree.jl package to generate a random forest model.

32:13.900 --> 32:20.900
All we need to do is use the random forest classifier constructor to generate our model.

32:21.900 --> 32:26.900
By default, this constructor will generate 10 decision trees.

32:26.900 --> 32:33.900
You can set the number of decision trees manually by using the n underscore trees keyword argument.

32:35.900 --> 32:41.900
After that, the rest of the code is similar to the code that we used for the decision tree model.

32:41.900 --> 33:02.900
So, we created a random forest model using the exact same training data that we used earlier and the exact same testing data.

33:03.900 --> 33:17.900
This time, we achieved a predictive accuracy score of around 95%, which is a significant improvement over the 89% that we scored earlier and comparable to the SVM model from last week.

33:19.900 --> 33:23.900
Just like before, we can view our results on a confusion matrix.

33:24.900 --> 33:39.900
As you can see, our random forest model only made 3 misclassifications versus the 6 misclassifications made by the decision tree model.

33:40.900 --> 33:44.900
And just like before, we can view the actual mistakes.

33:53.900 --> 34:07.900
If you click on the header of column 3, you'll see the 3 misclassifications.

34:08.900 --> 34:18.900
It predicted virginica twice when the actual class was versicolor, and it predicted versicolor once when the actual class was virginica.

34:18.900 --> 34:24.900
You can also view the probabilities of those predictions.

34:34.900 --> 34:38.900
This time, the probabilities look a little different.

34:39.900 --> 34:45.900
This random forest model is not as confident as before when it comes to the Satosa class.

34:46.900 --> 34:51.900
There are several samples that have a 10% probability of being Satosa.

34:52.900 --> 35:04.900
On the flip side, this random forest model is much more confident about the versicolor and virginica predictions, with many samples having 100% probabilities.

35:05.900 --> 35:15.900
There are also more probability splits like 90-10, 85-15, 70-30, and so on.

35:17.900 --> 35:27.900
Although we can still view some interesting information, unfortunately, we do not have as much access to information as we had with the decision tree model.

35:28.900 --> 35:39.900
For example, even though I know that this random forest model generated 20 decision trees, I do not know what those trees look like.

35:40.900 --> 35:45.900
Nor do I know what features they used, or what threshold levels were determined.

35:46.900 --> 35:51.900
I can no longer present a visualization of what's going on under the hood.

35:52.900 --> 35:58.900
So the random forest is kind of like a black box, just like our SVM model from last week.

36:00.900 --> 36:06.900
So even though this random forest model is more accurate, it's more difficult to explain.

36:08.900 --> 36:14.900
As you are no doubt concluding, creating machine learning models is a series of tradeoffs.

36:16.900 --> 36:31.900
Before we conclude for today, let's go through an example of boosting, which is another ensemble learning technique that will reinforce this concept of tradeoffs when creating machine learning models.

36:35.900 --> 36:41.900
While bagging is used to lower variance, boosting is generally used to lower bias.

36:42.900 --> 36:57.900
While it makes sense to use bagging with decision trees, since decision trees naturally tend to have high variance, it's not immediately clear why using boosting would help improve the performance of decision trees.

36:59.900 --> 37:09.900
The motivation behind boosting is to answer the question, can a set of weak learners create a single, strong learner?

37:10.900 --> 37:20.900
An example of a weak learner is a classifier that is only slightly better at making predictions compared to a random guess.

37:22.900 --> 37:29.900
An example of a boosting algorithm is Adaboost, which is short for adaptive boosting.

37:30.900 --> 37:36.900
It was first introduced by Joachim Freund and Robert Shapur in 1997.

37:37.900 --> 37:45.900
Adaboost may be used with various learning algorithms, but it seems to pair particularly well with decision trees.

37:47.900 --> 38:05.900
Unlike random forests that grow multiple decision trees and then aggregate them to form a new metamodel, Adaboost starts with what is known as a stump, meaning that it's a decision tree with only a root node and two leaf nodes.

38:06.900 --> 38:18.900
It's not clear to me how this works for classifications with three classes, like the IRIS dataset, but Adaboost does work for multi-class classification problems.

38:19.900 --> 38:35.900
Based on the information gathered by the Adaboost algorithm from that single stump, the algorithm will increase the weights given to the misclassified data so that it can focus on the misclassifications in the next round.

38:36.900 --> 38:45.900
The algorithm will then use that new weighted dataset and pass it through another stump and make another set of predictions.

38:46.900 --> 38:57.900
Again, the algorithm will increase the weight given to the misclassified data so that it can focus on the remaining misclassified data in the next round.

38:58.900 --> 39:08.900
The theory is that by doing this, the algorithm will focus on the samples that are hardest to classify as it continues going through this process.

39:09.900 --> 39:21.900
It will repeat this process and add the knowledge it gains from all of these weak learners to form a single, strong metamodel that can be used to make predictions.

39:22.900 --> 39:32.900
I realize that this is even more abstract than the random forest, so let's take a look at Adaboost in action.

39:33.900 --> 39:39.900
Fortunately, we can use the DecisionTree.jl package to generate an Adaboost model.

39:40.900 --> 39:47.900
All we need to do is use the Adaboost Stump classifier constructor to generate our model.

39:48.900 --> 40:00.900
This model does not have a default setting for the number of iterations, so you will need to enter that number manually by using the N underscore iterations keyword arguments.

40:01.900 --> 40:14.900
Setting the N iterations to 20 means that our model will generate 20 stumps to create one tree, as opposed to creating 20 full trees used in the random forest model.

40:15.900 --> 40:26.900
After that, all of the code is exactly the same as the code used in the random forest example, so I'm just going to copy and paste the code from above.

40:26.900 --> 40:44.900
So, we created an Adaboost model using the exact same training data and testing data that we used for both the DecisionTree and the random forest.

40:45.900 --> 41:00.900
This time, we achieved a predictive accuracy score of around 95%, which is a significant improvement over the 89% of the DecisionTree and comparable to the random forest and SVM models.

41:01.900 --> 41:06.900
Like the random forest model, this Adaboost model is sort of a black box.

41:07.900 --> 41:15.900
We do not have access to as much information as the DecisionTree model, but we still have access to some information.

41:16.900 --> 41:21.900
Just like before, we can view our results on a confusion matrix.

41:22.900 --> 41:28.900
The confusion matrix looks exactly the same as the confusion matrix on the random forest model.

41:36.900 --> 41:47.900
And the prediction versus actual data looks identical to the random forest, so you may be wondering if anything is different.

41:48.900 --> 41:53.900
Let's take a look at the probability distribution of the predictions.

41:54.900 --> 42:08.900
So, this looks very different from the DecisionTree and the random forest.

42:09.900 --> 42:19.900
Although the Adaboost model had the same predictive accuracy as the random forest model, it is a lot less confident about its predictions.

42:20.900 --> 42:27.900
Most of the predictions have a 65-35 splits, or a 70-30 splits.

42:28.900 --> 42:35.900
There are even some examples with a 52-48 splits, which is barely better than a coin flip.

42:36.900 --> 42:45.900
There's not a single sample with a 100% probability, although there are plenty of samples with a 0% probability.

42:46.900 --> 42:49.900
So that's really interesting, isn't it?

42:50.900 --> 43:00.900
Both random forest and Adaboost offer the potential for enhanced predictive accuracy, but they approach the problem using very different techniques.

43:04.900 --> 43:09.900
We covered a lot of material today, so let's go through a quick summary.

43:10.900 --> 43:18.900
While DecisionTrees are relatively easy to visualize and easy to explain, they are surprisingly difficult to implement.

43:19.900 --> 43:28.900
They also have some flaws, namely that they are not known for their predictive accuracy and they are prone to overfitting.

43:29.900 --> 43:39.900
This led to a discussion about the bias variance trade-off, which will be an ongoing subject of conversation throughout this machine learning series.

43:40.900 --> 43:52.900
Through that discussion, we learned that we could potentially enhance the performance of DecisionTrees by using ensemble learning methods, namely bagging and boosting.

43:53.900 --> 44:02.900
Bagging is short for bootstrap aggregating, and Random Forest is one of the best known implementations of that method.

44:03.900 --> 44:14.900
Random Forest works by using various techniques to increase the independence of features and by increasing the number of DecisionTrees to be used in a metamodel.

44:15.900 --> 44:26.900
Boosting, on the other hand, uses an additive approach by using a collection of so-called weak learners to combine into a single strong learner.

44:27.900 --> 44:34.900
Adaboost, which is short for adaptive boosting, is a well-known implementation of boosting.

44:35.900 --> 44:47.900
Along the way, we were introduced to another analytic tool, the confusion matrix, along with the CAPA coefficients, that we can use to better understand our results.

44:49.900 --> 44:55.900
That's a lot to cover in one tutorial, and that's a lot to include in a Julia package.

44:56.900 --> 45:04.900
Like I said at the beginning, the DecisionTree.jl package is surprisingly sophisticated and is full of Easter eggs.

45:06.900 --> 45:18.900
Please show your appreciation to the developer and the contributors for sharing their knowledge with the rest of us by going to the GitHub page of DecisionTree.jl and leave them a star.

45:19.900 --> 45:29.900
Well, that's all for today. If you made it this far, congratulations.

45:30.900 --> 45:38.900
If you enjoyed this video and you feel like you learned something new, please give it a thumbs up.

45:39.900 --> 45:45.900
For more wholesome Julia tutorials, please be sure to subscribe and hit that bell.

45:45.900 --> 45:51.900
If you like what I do, then please consider joining and becoming a channel member.

45:52.900 --> 45:55.900
New tutorials are posted on Sundays slash Mondays.

45:56.900 --> 46:00.900
Thanks for watching, and I'll see you in the next video.

