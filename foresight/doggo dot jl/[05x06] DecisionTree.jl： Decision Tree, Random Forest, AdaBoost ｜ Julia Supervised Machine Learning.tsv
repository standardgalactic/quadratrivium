start	end	text
0	7400	Last week, we learned about the Support Vector Machine, which is a powerful algorithm with a science-fiction feel.
8200	15300	But despite its usefulness, it can be difficult to understand and even more difficult to explain.
16600	23300	What if I told you that there's another supervised machine learning algorithm that's also powerful and useful,
23500	30000	but is grounded in real life so that it's much easier to understand and much easier to explain?
32100	35600	That algorithm is the Decision Tree.
37400	43000	The Decision Tree algorithm is one of the most popular machine learning algorithms in use today,
43900	47800	but it's different than any of the algorithms that we've studied so far.
49500	52500	What makes the Decision Tree algorithm so different?
54200	56400	Well, let's find out.
58900	65200	Welcome to Julia for Talented Amateurs, where I may call some Julia tutorials for Talented Amateurs everywhere.
65900	68900	I am your host, TheDabblingDoggo.
69500	70200	I dabble.
71300	76100	For those of you who are visual learners, you're going to love the Decision Tree.
77100	85200	While other machine learning algorithms can be highly abstract and difficult to visualize, the Decision Tree is just the opposite.
86500	91100	The way a Decision Tree works is similar to how you might make decisions every day.
92500	97900	The idea is that you can make a big decision based on a collection of little decisions.
98900	106900	For example, if you're about to go outside during the rainy season, you might wonder, should I bring an umbrella?
108400	113900	Before answering that question, your brain may ask a collection of smaller questions first.
114400	119900	For example, it might ask a simple yes or no question like, is it raining now?
121400	124900	If the answer is yes, then it tells you to bring an umbrella.
125900	133900	If the answer is no, then it might ask another simple question like, is it forecasted to rain later today?
134900	138900	If the answer is yes, it tells you to bring an umbrella.
139900	144900	If the answer is no, then it tells you that you don't need to bring an umbrella.
145900	155900	Whether you're conscious of these micro-decisions or not, your brain is going through a similar thought process as you make countless decisions throughout your day.
156900	162900	What if you could build a computer model that followed that thought process to help your computer make decisions?
163900	167900	That's the idea behind the Decision Tree algorithm.
168900	178900	The Decision Tree is used in several different academic disciplines, but in machine learning, it's typically used for supervised machine learning.
179900	189900	The Decision Tree may be used for either classification or regression, but in this tutorial, I will only be covering Decision Trees for classification.
190900	201900	As an academic subject, Decision Trees have been around for over 100 years, but more recently, Decision Trees have been adapted for the computer age.
202900	214900	There are several different Decision Tree algorithms used in machine learning, but the version that we will be using is called Classification and Regression Trees, or CART for short.
215900	228900	The first version of the CART algorithm was completed in 1977 by Leo Breiman and Charles Stone from UC Berkeley and Jerome Friedman and Richard Olsson from Stanford University.
229900	242900	In 1984, they published a book appropriately named Classification and Regression Trees, which detailed both the concepts and the mechanics of constructing Decision Trees.
244900	254900	Decision Trees are typically visualized using some form of graph theory representation, meaning a visual combination of nodes and edges.
255900	265900	Decision Trees are inverted trees with a root node at the top. A root node contains all of the data in the training data sets.
267900	277900	For a binary decision, there are two edges coming out of the root node, typically representing a yes-no or true-false decision.
278900	281900	These edges are also called branches.
282900	286900	Connected to those branches will be another node.
287900	292900	This time, the node may either be a leaf node or a decision node.
294900	304900	A leaf node contains a subset of the training data and represents a final classification, so there are no further decisions to be made for that subset.
305900	313900	A decision node also contains a subset of the training data set, but requires additional decisions to be made.
315900	323900	Like the root node, the decision node has two edges coming out of it to represent a different yes-no or true-false question.
323900	336900	Unless given constraints, the Decision Tree algorithm will continue this process of interrogating the data and growing the tree until every sample of data has been classified.
338900	346900	Since the objective of using a Decision Tree algorithm is to create a model that can be used to make predictions on unseen data,
346900	354900	it's a good practice to place some constraints on the Decision Tree algorithm so that it stops going through your data at some point.
355900	362900	That way, you can have a more generalized model that you can use to classify new unseen data.
364900	371900	Now that you know what a Decision Tree looks like, let's build one using the DecisionTree.jl package.
372900	386900	While the Decision Tree is easy to visualize, it is surprisingly difficult to implement the code from scratch, so we'll be using the DecisionTree.jl package.
387900	391900	DecisionTree.jl was created by Ben Sadeghi.
392900	402900	On the surface, DecisionTree.jl seems like a simple package, but the more you dig into it, the more you'll understand how sophisticated it really is.
404900	409900	DecisionTree.jl is not a wrapper to a library written in a different language.
410900	416900	Instead, DecisionTree.jl is a pure Julia implementation of the CART algorithm.
417900	420900	Let's start by setting up our programming environment.
422900	426900	For today's tutorial, knowledge of Julia and VS Code is required.
427900	435900	I'm also assuming that you're watching this entire machine learning playlist, so episodes 501 through 505 are required.
437900	442900	In your VS Code Explorer panel, create a new folder for this tutorial.
442900	451900	In the tutorial folder, create a new file called SL underscore trees.jl.
453900	458900	Launch the Julia REPL by using ALT J, then ALT O.
460900	465900	For this tutorial, I think it's useful to dock the REPL panel to the right side panel.
466900	471900	This is optional, but we will be referring to the REPL throughout this tutorial.
473900	481900	In order to dock the REPL to the right side panel, you can click and drag the REPL panel header until your mouse reaches the right edge of your window.
483900	487900	A panel highlight should appear indicating that you can dock it there.
489900	493900	Once it's docked, you can adjust the panel size to your liking.
493900	503900	For those of you who use Julia's workspace panel, you can also dock the workspace panel in the right side panel if you wish.
506900	511900	In the REPL, change the present working directory to your tutorial directory.
513900	516900	Enter the package REPL by hitting the closing square bracket.
518900	520900	Activate your tutorial directory.
521900	524900	Add the decision tree.jl package.
527900	530900	Type in status to confirm the version number.
532900	535900	Exit the package REPL by hitting Backspace.
537900	542900	Adjust the panels in VS Code so that you can see both your text editor as well as the REPL panel.
545900	547900	Okay, let's get started.
550900	552900	Let's start by loading some packages.
558900	560900	Next, let's load some data.
562900	568900	We'll be using the iris dataset as our motivating example to learn how to make predictions using decision trees.
569900	575900	The decision tree.jl package comes included with its own version of the iris flower dataset.
576900	584900	For some reason, there are some slight differences between this dataset and the one we used last week from the rdatasets package.
585900	593900	It's not a material difference and it won't change the outcome of our models, but I did want to let you know that these are not identical datasets.
594900	603900	The load underscore data function is from the decision tree.jl package and it splits the data for you between inputs and outputs.
605900	611900	But if you view the data, you'll see that there are no Julia data types assigned to that data.
612900	618900	As you can see, both the inputs and the outputs have a data type of any.
618900	631900	In order to improve performance, the decision tree.jl package recommends assigning data types to your data before building any models.
641900	644900	This dataset should look familiar to you.
645900	654900	In this particular dataset, there is no header row, but as a reminder, the first four columns are the features, which are the various measurements and centimeters.
655900	660900	And the fifth column contains the labeled class data, which is one of three classes.
661900	668900	Satosa, Versicolor, or Virginica, which are the three different species of the iris flower.
669900	674900	Next, we will need to split this data between training and testing.
675900	679900	In order to do this, we'll be using Huda Nassar's code from last week.
680900	687900	I am simply going to copy and paste this code from last week's tutorial so that you won't have to watch me retype this code.
687900	694900	I'm going to go through the next few steps fairly quickly, since it's the same workflow that we used last week.
696900	705900	I'm using the same random seed that I used last week, so the index numbers we used this week for training and testing should be the same.
706900	711900	Next, we need to split the features between training and testing.
712900	720900	I'm using the same random seed that I used last week, so the index numbers we used this week for training and testing should be the same as we used last week.
725900	730900	Next, we need to split the features between training and testing.
736900	741900	And then, we need to split the classes between training and testing.
746900	753900	Unlike last week, we do not need to transpose our data, since the DecisionTree.jl package does not require it.
756900	760900	Okay, we are now ready to build our first DecisionTree.
766900	774900	In order to create a DecisionTree, all you need to do is use the DecisionTree classifier constructor.
776900	788900	There are a lot of different keyword arguments that you can include in this constructor, but I'm only using the max underscore depth keyword argument, which will stop the tree from growing too large.
789900	795900	You can find all of the available keyword arguments in the DecisionTree.jl documentation.
796900	809900	The DecisionTree.jl package also includes a DecisionTree regressor constructor, which is used for regression problems, but I will only be covering the classifier in this tutorial.
811900	816900	The rest of the workflow is very similar to the workflow that we use for SVM.
817900	822900	You can fit your training data to the model by using the FitBang function.
824900	825900	And that's it!
826900	831900	You can see your DecisionTree in the REPL by using the print underscore tree function.
834900	839900	I know it doesn't look like much, but there's a lot of information in this compact output.
841900	843900	Here's the visualization of that output.
844900	847900	The top line of the output is the root node.
848900	859900	The algorithm has scanned all of the features and has determined that splitting feature number 4, or the fourth column, will provide the most useful information.
860900	865900	The algorithm has selected a value of 0.8 as the threshold.
866900	875900	Samples that have a feature 4 value of less than 0.8 go into the left node, and everything else goes into the right node.
877900	882900	With the left node, the algorithm has determined that no further splits are required.
883900	889900	This left node is a leaf node, and the classification is Satosa.
890900	898900	The 35 slash 35 means that 35 samples have a class label of Satosa out of 35 total samples.
900900	908900	The right node is a Decision node, since the algorithm has determined that more information may be gained by splitting the data further.
909900	927900	For this subset of data, the algorithm has determined that splitting feature number 3, or the third column, will yield the most useful information, and it has selected a value of 4.95 as the threshold.
928900	938900	Samples with a feature 3 value of less than 4.95 go into the left node, and everything else goes into the right node.
939900	947900	At this point, the tree stops growing because we use the keyword argument max underscore depth equals 2.
948900	955900	If we didn't use that keyword argument, the tree would continue to grow until the algorithm determined its own stopping point.
957900	961900	As a result, the last two nodes are leaf nodes by default.
962900	974900	For the left node, the algorithm is classifying that data subset as VersaColor, and for the right node, the algorithm is classifying that data subset as Virginia.
976900	980900	But the algorithm knows that there are some misclassifications.
981900	994900	In the left node, the 25 slash 26 means that 25 samples have a class label of VersaColor out of 26 total samples, meaning there's one sample that has been misclassified.
995900	1009900	In the right node, that 32 slash 33 means that 32 samples have a class label of Virginia out of 33 total samples, meaning there's one sample that has been misclassified.
1010900	1018900	In order to get a better understanding of what the algorithm is looking at, let's take a look at the actual data subsets.
1019900	1024900	Let's start by looking at the training data, which is what the root node is looking at.
1030900	1034900	This data subset contains 94 samples.
1035900	1040900	If you add up all the numbers in the decision tree nodes, you should get 94.
1040900	1050900	If you sort column 4, you can see why the algorithm selected 0.8 as the threshold for feature number 4.
1053900	1058900	It doesn't matter if it's less than or less than or equal to 0.8.
1059900	1065900	As you can see, none of the samples have a feature 4 value of 0.8.
1066900	1072900	Instead, there's a clear dividing line between the values of 0.6 and 1.
1073900	1077900	0.8 is just the average between those two values.
1079900	1085900	There are 35 samples above the line, and all of them have a class label of Satosa.
1086900	1089900	That's where the 35 slash 35 comes from.
1090900	1096900	Now, let's take a closer look at the data subset that was sent to the decision node on the right.
1107900	1114900	This subset contains 59 samples, and the class labels are either VersaColor or Virginica.
1114900	1121900	If you sort on column 3, you'll see why the algorithm selected 4.95 as the threshold.
1125900	1130900	There is no sample with a feature 3 value of 4.95.
1131900	1139900	Instead, there's a clear dividing line between 4.9 and 5, so 4.95 is just the average.
1140900	1145900	There are 26 samples above the line, and 33 samples below the line.
1147900	1155900	Out of the samples above the line, 25 have a class label of VersaColor, which is where the 25 slash 26 comes from.
1157900	1165900	And of the samples below the line, 32 have a class label of Virginica, which is where the 32 slash 33 comes from.
1170900	1175900	So in each class, there's one sample that has been misclassified.
1176900	1180900	You can see the misclassified samples in this data view.
1182900	1189900	Being able to visualize this much detail is unlike any machine learning algorithm that we've used in the past,
1190900	1193900	which is one of the reasons why decision trees are so popular.
1194900	1202900	But we already know that this model has some mistakes in it, so let's see how well it does at making predictions.
1203900	1208900	Like last week, you can make predictions by using the predict function.
1213900	1218900	Now that we have a prediction, we can check the accuracy of that prediction.
1219900	1226900	So this accuracy score of 89% is not very good.
1228900	1234900	You'll recall that our SVM model got an accuracy score of 95% right out of the box.
1236900	1239900	It seems like all of this transparency came at a price.
1241900	1245900	Although decision trees are easy to understand and easy to explain,
1246900	1249900	they are often not very good at making predictions,
1250900	1255900	which of course is the primary reason for building machine learning classification models in the first place.
1258900	1264900	Let's take a deeper dive into the misclassifications to gain a better understanding of what went wrong.
1266900	1271900	A common analytic tool used in machine learning is called a confusion matrix.
1272900	1279900	A confusion matrix provides a simple way to visualize where your model got confused while making predictions.
1281900	1286900	The decisionTree.jl package comes included with a confusion matrix constructor.
1290900	1296900	The way to read this confusion matrix is that the classes from the first arguments are in the rows,
1296900	1300900	and the classes from the second arguments are in the columns.
1301900	1306900	So since we used y underscore test as the first argument,
1306900	1312900	the rows show the counts for the actual labeled data that we used for testing purposes.
1313900	1317900	Since we used y underscore hat as the second argument,
1317900	1322900	the columns show the classes predicted by our decision tree model.
1324900	1328900	The numbers shown along the diagonal are the correct predictions,
1328900	1332900	and any other number represents an incorrect prediction.
1333900	1343900	For example, that one indicates that our model predicted for genica once when the actual labeled class was versicolor.
1344900	1352900	That five indicates that our model predicted versicolor five times when the actual labeled class was virginica.
1353900	1360900	The accuracy score shown here matches the accuracy score that we calculated manually.
1361900	1367900	The kappa coefficient is a metric that you can derive from this confusion matrix.
1368900	1376900	I won't go into the math, but the kappa coefficient offers a more nuanced assessment of your model's predictive abilities.
1377900	1386900	When making predictions, there's always a possibility that your model made a correct prediction purely by chance,
1387900	1390900	thus artificially inflating the accuracy score.
1392900	1397900	The kappa coefficient attempts to remove the probability of random positive predictions
1397900	1403900	in order to present a more conservative way to assess your model's performance.
1404900	1411900	As you can see, after factoring out the probability of making a correct prediction just by getting lucky,
1412900	1416900	the accuracy score is closer to 83%.
1418900	1425900	We can see the incorrect predictions using the same code that we used last week to display the results.
1433900	1448900	As a reminder, the first column contains the predictions, and the second column contains the class from the labeled test data,
1449900	1453900	and the third column indicates whether or not the prediction was correct.
1455900	1461900	If you sort on column three, you can see there's one sample where our model predicted virginica
1461900	1464900	when the actual species was versicolor.
1465900	1472900	And there are five samples where our model predicted versicolor when the actual species was virginica,
1473900	1476900	which is consistent with what the confusion matrix showed.
1479900	1484900	You'll recall from last week that the SVM model is non-probabilistic,
1485900	1488900	meaning that it makes predictions without assigning probabilities.
1491900	1493900	The decision tree algorithm is different.
1494900	1502900	You can see the probabilities determined by the decision tree model by using the predict underscore proper function.
1508900	1512900	So this display shows the 56 samples in the test data set.
1513900	1519900	Column one is the probability that the test sample has a class label of Satosa.
1520900	1525900	Column two is the probability that the test sample has a class label of versicolor.
1526900	1531900	And column three is the probability that the test sample has a class label of virginica.
1533900	1541900	As you can see, our model is 100% confident that the first 15 samples have a class label of Satosa.
1542900	1546900	But for every other sample, our model is not as confident,
1546900	1550900	since it knows that there's some probability of misclassification.
1551900	1558900	All of the other samples show a 96% probability when predicting either versicolor or virginica.
1561900	1564900	Okay, so now what do we do?
1566900	1570900	The decision tree is great since it's a wealth of information,
1571900	1573900	but it's not so great at making predictions,
1573900	1576900	which is really what we need.
1577900	1582900	The good news is that there are a couple of ways to improve this accuracy score,
1583900	1588900	but like many things in life, those improvements will come with some tradeoffs.
1589900	1597900	One of those tradeoffs is a fundamental concept in machine learning called the bias variance tradeoff.
1597900	1607900	On the surface, the bias variance tradeoff may seem like a relatively easy concept to understand,
1608900	1613900	but upon closer inspection, one learns that it is very nuanced,
1614900	1617900	so it's surprisingly difficult to implement in real life.
1618900	1624900	The bias variance tradeoff is typically visualized by these three diagrams.
1625900	1630900	The diagram on the left is an example of underfitting the data,
1631900	1637900	meaning that it hasn't captured enough of the relevant relationships between features and outputs.
1638900	1643900	The diagram on the right is an example of overfitting the data,
1644900	1647900	meaning that it has learned the training data too well,
1648900	1653900	so that it will make a lot of mistakes when trying to make predictions using new data.
1655900	1660900	The diagram in the middle is an example of a model being just right,
1661900	1666900	meaning that it has captured enough of the relevant relationships between features and outputs,
1667900	1673900	while still being able to be generalized so that it can be used to make predictions using unseen data.
1675900	1679900	So these diagrams are really from the data's perspective,
1679900	1684900	so they aren't necessarily helpful when you're trying to design a model.
1685900	1692900	A different way to visualize the bias variance tradeoff is by looking at these four diagrams,
1693900	1695900	which is from the model's perspective.
1696900	1704900	The lower left diagram shows a model with low bias and low variance, which is the best case scenario.
1705900	1713900	The upper right diagram shows a model with high bias and high variance, which is the worst case scenario.
1715900	1720900	In reality, most models are neither best case nor worst case.
1721900	1730900	Instead, most models fall along that diagonal region between the upper left diagram and the lower right diagram,
1731900	1740900	meaning that your model will most likely have a high bias and low variance, or low bias and high variance.
1742900	1747900	The best that you can hope to achieve is some result near the middle of all four diagrams,
1748900	1751900	resulting in a model with some bias and some variance.
1752900	1762900	The model generated by the decision tree algorithm is an example of a model with high variance and low bias,
1763900	1766900	meaning that it should be placed in that lower right box.
1768900	1772900	Decision trees have a natural tendency to overfit the data,
1773900	1780900	since in theory, it can continue to split your data until it has correctly classified every sample in the training data sets.
1782900	1791900	But that's no good, since you can't generalize such a model, since new unseen data will most likely be different than the training data.
1793900	1800900	You can improve the predictive accuracy of decision trees by using something called ensemble learning.
1801900	1807900	There are two main ensemble learning techniques that are used to improve the performance of decision trees.
1808900	1812900	One is called bagging, and the other is called boosting.
1814900	1821900	We'll cover bagging first. An example of a bagging algorithm is known as random forest.
1825900	1829900	Bagging is a shortened form of the term bootstrap aggregating.
1830900	1840900	Without getting into the details, the concept behind bagging is that you can reduce the variance of your machine learning model by doing two things.
1841900	1849900	One, you can increase the independence of your features, and two, you can increase the number of models.
1850900	1860900	You then take this increased number of models with features that are more independent, then you aggregate them into one metamodel.
1861900	1867900	The result is a model with lower variance, but with a slightly higher bias.
1869900	1872900	This is the bias variance tradeoff in action.
1873900	1880900	One of the best known examples of a bagging algorithm is random forest.
1881900	1889900	The first algorithm for random decision forests was created in 1995 by Tin Cam Ho.
1890900	1899900	The way that random forest works is that it introduces independence between the features and then generates multiple decision tree models.
1900900	1909900	But the random forest introduces even more randomness into the algorithm by only considering a fraction of the features at every split.
1910900	1917900	This results in even more independence between the features, which results in even lower variance.
1919900	1924900	I realize that this is a lot of hand-waving, so let's see random forest in action.
1925900	1932900	Fortunately, we can use the decisionTree.jl package to generate a random forest model.
1933900	1940900	All we need to do is use the random forest classifier constructor to generate our model.
1941900	1946900	By default, this constructor will generate 10 decision trees.
1946900	1953900	You can set the number of decision trees manually by using the n underscore trees keyword argument.
1955900	1961900	After that, the rest of the code is similar to the code that we used for the decision tree model.
1961900	1982900	So, we created a random forest model using the exact same training data that we used earlier and the exact same testing data.
1983900	1997900	This time, we achieved a predictive accuracy score of around 95%, which is a significant improvement over the 89% that we scored earlier and comparable to the SVM model from last week.
1999900	2003900	Just like before, we can view our results on a confusion matrix.
2004900	2019900	As you can see, our random forest model only made 3 misclassifications versus the 6 misclassifications made by the decision tree model.
2020900	2024900	And just like before, we can view the actual mistakes.
2033900	2047900	If you click on the header of column 3, you'll see the 3 misclassifications.
2048900	2058900	It predicted virginica twice when the actual class was versicolor, and it predicted versicolor once when the actual class was virginica.
2058900	2064900	You can also view the probabilities of those predictions.
2074900	2078900	This time, the probabilities look a little different.
2079900	2085900	This random forest model is not as confident as before when it comes to the Satosa class.
2086900	2091900	There are several samples that have a 10% probability of being Satosa.
2092900	2104900	On the flip side, this random forest model is much more confident about the versicolor and virginica predictions, with many samples having 100% probabilities.
2105900	2115900	There are also more probability splits like 90-10, 85-15, 70-30, and so on.
2117900	2127900	Although we can still view some interesting information, unfortunately, we do not have as much access to information as we had with the decision tree model.
2128900	2139900	For example, even though I know that this random forest model generated 20 decision trees, I do not know what those trees look like.
2140900	2145900	Nor do I know what features they used, or what threshold levels were determined.
2146900	2151900	I can no longer present a visualization of what's going on under the hood.
2152900	2158900	So the random forest is kind of like a black box, just like our SVM model from last week.
2160900	2166900	So even though this random forest model is more accurate, it's more difficult to explain.
2168900	2174900	As you are no doubt concluding, creating machine learning models is a series of tradeoffs.
2176900	2191900	Before we conclude for today, let's go through an example of boosting, which is another ensemble learning technique that will reinforce this concept of tradeoffs when creating machine learning models.
2195900	2201900	While bagging is used to lower variance, boosting is generally used to lower bias.
2202900	2217900	While it makes sense to use bagging with decision trees, since decision trees naturally tend to have high variance, it's not immediately clear why using boosting would help improve the performance of decision trees.
2219900	2229900	The motivation behind boosting is to answer the question, can a set of weak learners create a single, strong learner?
2230900	2240900	An example of a weak learner is a classifier that is only slightly better at making predictions compared to a random guess.
2242900	2249900	An example of a boosting algorithm is Adaboost, which is short for adaptive boosting.
2250900	2256900	It was first introduced by Joachim Freund and Robert Shapur in 1997.
2257900	2265900	Adaboost may be used with various learning algorithms, but it seems to pair particularly well with decision trees.
2267900	2285900	Unlike random forests that grow multiple decision trees and then aggregate them to form a new metamodel, Adaboost starts with what is known as a stump, meaning that it's a decision tree with only a root node and two leaf nodes.
2286900	2298900	It's not clear to me how this works for classifications with three classes, like the IRIS dataset, but Adaboost does work for multi-class classification problems.
2299900	2315900	Based on the information gathered by the Adaboost algorithm from that single stump, the algorithm will increase the weights given to the misclassified data so that it can focus on the misclassifications in the next round.
2316900	2325900	The algorithm will then use that new weighted dataset and pass it through another stump and make another set of predictions.
2326900	2337900	Again, the algorithm will increase the weight given to the misclassified data so that it can focus on the remaining misclassified data in the next round.
2338900	2348900	The theory is that by doing this, the algorithm will focus on the samples that are hardest to classify as it continues going through this process.
2349900	2361900	It will repeat this process and add the knowledge it gains from all of these weak learners to form a single, strong metamodel that can be used to make predictions.
2362900	2372900	I realize that this is even more abstract than the random forest, so let's take a look at Adaboost in action.
2373900	2379900	Fortunately, we can use the DecisionTree.jl package to generate an Adaboost model.
2380900	2387900	All we need to do is use the Adaboost Stump classifier constructor to generate our model.
2388900	2400900	This model does not have a default setting for the number of iterations, so you will need to enter that number manually by using the N underscore iterations keyword arguments.
2401900	2414900	Setting the N iterations to 20 means that our model will generate 20 stumps to create one tree, as opposed to creating 20 full trees used in the random forest model.
2415900	2426900	After that, all of the code is exactly the same as the code used in the random forest example, so I'm just going to copy and paste the code from above.
2426900	2444900	So, we created an Adaboost model using the exact same training data and testing data that we used for both the DecisionTree and the random forest.
2445900	2460900	This time, we achieved a predictive accuracy score of around 95%, which is a significant improvement over the 89% of the DecisionTree and comparable to the random forest and SVM models.
2461900	2466900	Like the random forest model, this Adaboost model is sort of a black box.
2467900	2475900	We do not have access to as much information as the DecisionTree model, but we still have access to some information.
2476900	2481900	Just like before, we can view our results on a confusion matrix.
2482900	2488900	The confusion matrix looks exactly the same as the confusion matrix on the random forest model.
2496900	2507900	And the prediction versus actual data looks identical to the random forest, so you may be wondering if anything is different.
2508900	2513900	Let's take a look at the probability distribution of the predictions.
2514900	2528900	So, this looks very different from the DecisionTree and the random forest.
2529900	2539900	Although the Adaboost model had the same predictive accuracy as the random forest model, it is a lot less confident about its predictions.
2540900	2547900	Most of the predictions have a 65-35 splits, or a 70-30 splits.
2548900	2555900	There are even some examples with a 52-48 splits, which is barely better than a coin flip.
2556900	2565900	There's not a single sample with a 100% probability, although there are plenty of samples with a 0% probability.
2566900	2569900	So that's really interesting, isn't it?
2570900	2580900	Both random forest and Adaboost offer the potential for enhanced predictive accuracy, but they approach the problem using very different techniques.
2584900	2589900	We covered a lot of material today, so let's go through a quick summary.
2590900	2598900	While DecisionTrees are relatively easy to visualize and easy to explain, they are surprisingly difficult to implement.
2599900	2608900	They also have some flaws, namely that they are not known for their predictive accuracy and they are prone to overfitting.
2609900	2619900	This led to a discussion about the bias variance trade-off, which will be an ongoing subject of conversation throughout this machine learning series.
2620900	2632900	Through that discussion, we learned that we could potentially enhance the performance of DecisionTrees by using ensemble learning methods, namely bagging and boosting.
2633900	2642900	Bagging is short for bootstrap aggregating, and Random Forest is one of the best known implementations of that method.
2643900	2654900	Random Forest works by using various techniques to increase the independence of features and by increasing the number of DecisionTrees to be used in a metamodel.
2655900	2666900	Boosting, on the other hand, uses an additive approach by using a collection of so-called weak learners to combine into a single strong learner.
2667900	2674900	Adaboost, which is short for adaptive boosting, is a well-known implementation of boosting.
2675900	2687900	Along the way, we were introduced to another analytic tool, the confusion matrix, along with the CAPA coefficients, that we can use to better understand our results.
2689900	2695900	That's a lot to cover in one tutorial, and that's a lot to include in a Julia package.
2696900	2704900	Like I said at the beginning, the DecisionTree.jl package is surprisingly sophisticated and is full of Easter eggs.
2706900	2718900	Please show your appreciation to the developer and the contributors for sharing their knowledge with the rest of us by going to the GitHub page of DecisionTree.jl and leave them a star.
2719900	2729900	Well, that's all for today. If you made it this far, congratulations.
2730900	2738900	If you enjoyed this video and you feel like you learned something new, please give it a thumbs up.
2739900	2745900	For more wholesome Julia tutorials, please be sure to subscribe and hit that bell.
2745900	2751900	If you like what I do, then please consider joining and becoming a channel member.
2752900	2755900	New tutorials are posted on Sundays slash Mondays.
2756900	2760900	Thanks for watching, and I'll see you in the next video.
