1
00:00:00,000 --> 00:00:07,400
Last week, we learned about the Support Vector Machine, which is a powerful algorithm with a science-fiction feel.

2
00:00:08,200 --> 00:00:15,300
But despite its usefulness, it can be difficult to understand and even more difficult to explain.

3
00:00:16,600 --> 00:00:23,300
What if I told you that there's another supervised machine learning algorithm that's also powerful and useful,

4
00:00:23,500 --> 00:00:30,000
but is grounded in real life so that it's much easier to understand and much easier to explain?

5
00:00:32,100 --> 00:00:35,600
That algorithm is the Decision Tree.

6
00:00:37,400 --> 00:00:43,000
The Decision Tree algorithm is one of the most popular machine learning algorithms in use today,

7
00:00:43,900 --> 00:00:47,800
but it's different than any of the algorithms that we've studied so far.

8
00:00:49,500 --> 00:00:52,500
What makes the Decision Tree algorithm so different?

9
00:00:54,200 --> 00:00:56,400
Well, let's find out.

10
00:00:58,900 --> 00:01:05,200
Welcome to Julia for Talented Amateurs, where I may call some Julia tutorials for Talented Amateurs everywhere.

11
00:01:05,900 --> 00:01:08,900
I am your host, TheDabblingDoggo.

12
00:01:09,500 --> 00:01:10,200
I dabble.

13
00:01:11,300 --> 00:01:16,100
For those of you who are visual learners, you're going to love the Decision Tree.

14
00:01:17,100 --> 00:01:25,200
While other machine learning algorithms can be highly abstract and difficult to visualize, the Decision Tree is just the opposite.

15
00:01:26,500 --> 00:01:31,100
The way a Decision Tree works is similar to how you might make decisions every day.

16
00:01:32,500 --> 00:01:37,900
The idea is that you can make a big decision based on a collection of little decisions.

17
00:01:38,900 --> 00:01:46,900
For example, if you're about to go outside during the rainy season, you might wonder, should I bring an umbrella?

18
00:01:48,400 --> 00:01:53,900
Before answering that question, your brain may ask a collection of smaller questions first.

19
00:01:54,400 --> 00:01:59,900
For example, it might ask a simple yes or no question like, is it raining now?

20
00:02:01,400 --> 00:02:04,900
If the answer is yes, then it tells you to bring an umbrella.

21
00:02:05,900 --> 00:02:13,900
If the answer is no, then it might ask another simple question like, is it forecasted to rain later today?

22
00:02:14,900 --> 00:02:18,900
If the answer is yes, it tells you to bring an umbrella.

23
00:02:19,900 --> 00:02:24,900
If the answer is no, then it tells you that you don't need to bring an umbrella.

24
00:02:25,900 --> 00:02:35,900
Whether you're conscious of these micro-decisions or not, your brain is going through a similar thought process as you make countless decisions throughout your day.

25
00:02:36,900 --> 00:02:42,900
What if you could build a computer model that followed that thought process to help your computer make decisions?

26
00:02:43,900 --> 00:02:47,900
That's the idea behind the Decision Tree algorithm.

27
00:02:48,900 --> 00:02:58,900
The Decision Tree is used in several different academic disciplines, but in machine learning, it's typically used for supervised machine learning.

28
00:02:59,900 --> 00:03:09,900
The Decision Tree may be used for either classification or regression, but in this tutorial, I will only be covering Decision Trees for classification.

29
00:03:10,900 --> 00:03:21,900
As an academic subject, Decision Trees have been around for over 100 years, but more recently, Decision Trees have been adapted for the computer age.

30
00:03:22,900 --> 00:03:34,900
There are several different Decision Tree algorithms used in machine learning, but the version that we will be using is called Classification and Regression Trees, or CART for short.

31
00:03:35,900 --> 00:03:48,900
The first version of the CART algorithm was completed in 1977 by Leo Breiman and Charles Stone from UC Berkeley and Jerome Friedman and Richard Olsson from Stanford University.

32
00:03:49,900 --> 00:04:02,900
In 1984, they published a book appropriately named Classification and Regression Trees, which detailed both the concepts and the mechanics of constructing Decision Trees.

33
00:04:04,900 --> 00:04:14,900
Decision Trees are typically visualized using some form of graph theory representation, meaning a visual combination of nodes and edges.

34
00:04:15,900 --> 00:04:25,900
Decision Trees are inverted trees with a root node at the top. A root node contains all of the data in the training data sets.

35
00:04:27,900 --> 00:04:37,900
For a binary decision, there are two edges coming out of the root node, typically representing a yes-no or true-false decision.

36
00:04:38,900 --> 00:04:41,900
These edges are also called branches.

37
00:04:42,900 --> 00:04:46,900
Connected to those branches will be another node.

38
00:04:47,900 --> 00:04:52,900
This time, the node may either be a leaf node or a decision node.

39
00:04:54,900 --> 00:05:04,900
A leaf node contains a subset of the training data and represents a final classification, so there are no further decisions to be made for that subset.

40
00:05:05,900 --> 00:05:13,900
A decision node also contains a subset of the training data set, but requires additional decisions to be made.

41
00:05:15,900 --> 00:05:23,900
Like the root node, the decision node has two edges coming out of it to represent a different yes-no or true-false question.

42
00:05:23,900 --> 00:05:36,900
Unless given constraints, the Decision Tree algorithm will continue this process of interrogating the data and growing the tree until every sample of data has been classified.

43
00:05:38,900 --> 00:05:46,900
Since the objective of using a Decision Tree algorithm is to create a model that can be used to make predictions on unseen data,

44
00:05:46,900 --> 00:05:54,900
it's a good practice to place some constraints on the Decision Tree algorithm so that it stops going through your data at some point.

45
00:05:55,900 --> 00:06:02,900
That way, you can have a more generalized model that you can use to classify new unseen data.

46
00:06:04,900 --> 00:06:11,900
Now that you know what a Decision Tree looks like, let's build one using the DecisionTree.jl package.

47
00:06:12,900 --> 00:06:26,900
While the Decision Tree is easy to visualize, it is surprisingly difficult to implement the code from scratch, so we'll be using the DecisionTree.jl package.

48
00:06:27,900 --> 00:06:31,900
DecisionTree.jl was created by Ben Sadeghi.

49
00:06:32,900 --> 00:06:42,900
On the surface, DecisionTree.jl seems like a simple package, but the more you dig into it, the more you'll understand how sophisticated it really is.

50
00:06:44,900 --> 00:06:49,900
DecisionTree.jl is not a wrapper to a library written in a different language.

51
00:06:50,900 --> 00:06:56,900
Instead, DecisionTree.jl is a pure Julia implementation of the CART algorithm.

52
00:06:57,900 --> 00:07:00,900
Let's start by setting up our programming environment.

53
00:07:02,900 --> 00:07:06,900
For today's tutorial, knowledge of Julia and VS Code is required.

54
00:07:07,900 --> 00:07:15,900
I'm also assuming that you're watching this entire machine learning playlist, so episodes 501 through 505 are required.

55
00:07:17,900 --> 00:07:22,900
In your VS Code Explorer panel, create a new folder for this tutorial.

56
00:07:22,900 --> 00:07:31,900
In the tutorial folder, create a new file called SL underscore trees.jl.

57
00:07:33,900 --> 00:07:38,900
Launch the Julia REPL by using ALT J, then ALT O.

58
00:07:40,900 --> 00:07:45,900
For this tutorial, I think it's useful to dock the REPL panel to the right side panel.

59
00:07:46,900 --> 00:07:51,900
This is optional, but we will be referring to the REPL throughout this tutorial.

60
00:07:53,900 --> 00:08:01,900
In order to dock the REPL to the right side panel, you can click and drag the REPL panel header until your mouse reaches the right edge of your window.

61
00:08:03,900 --> 00:08:07,900
A panel highlight should appear indicating that you can dock it there.

62
00:08:09,900 --> 00:08:13,900
Once it's docked, you can adjust the panel size to your liking.

63
00:08:13,900 --> 00:08:23,900
For those of you who use Julia's workspace panel, you can also dock the workspace panel in the right side panel if you wish.

64
00:08:26,900 --> 00:08:31,900
In the REPL, change the present working directory to your tutorial directory.

65
00:08:33,900 --> 00:08:36,900
Enter the package REPL by hitting the closing square bracket.

66
00:08:38,900 --> 00:08:40,900
Activate your tutorial directory.

67
00:08:41,900 --> 00:08:44,900
Add the decision tree.jl package.

68
00:08:47,900 --> 00:08:50,900
Type in status to confirm the version number.

69
00:08:52,900 --> 00:08:55,900
Exit the package REPL by hitting Backspace.

70
00:08:57,900 --> 00:09:02,900
Adjust the panels in VS Code so that you can see both your text editor as well as the REPL panel.

71
00:09:05,900 --> 00:09:07,900
Okay, let's get started.

72
00:09:10,900 --> 00:09:12,900
Let's start by loading some packages.

73
00:09:18,900 --> 00:09:20,900
Next, let's load some data.

74
00:09:22,900 --> 00:09:28,900
We'll be using the iris dataset as our motivating example to learn how to make predictions using decision trees.

75
00:09:29,900 --> 00:09:35,900
The decision tree.jl package comes included with its own version of the iris flower dataset.

76
00:09:36,900 --> 00:09:44,900
For some reason, there are some slight differences between this dataset and the one we used last week from the rdatasets package.

77
00:09:45,900 --> 00:09:53,900
It's not a material difference and it won't change the outcome of our models, but I did want to let you know that these are not identical datasets.

78
00:09:54,900 --> 00:10:03,900
The load underscore data function is from the decision tree.jl package and it splits the data for you between inputs and outputs.

79
00:10:05,900 --> 00:10:11,900
But if you view the data, you'll see that there are no Julia data types assigned to that data.

80
00:10:12,900 --> 00:10:18,900
As you can see, both the inputs and the outputs have a data type of any.

81
00:10:18,900 --> 00:10:31,900
In order to improve performance, the decision tree.jl package recommends assigning data types to your data before building any models.

82
00:10:41,900 --> 00:10:44,900
This dataset should look familiar to you.

83
00:10:45,900 --> 00:10:54,900
In this particular dataset, there is no header row, but as a reminder, the first four columns are the features, which are the various measurements and centimeters.

84
00:10:55,900 --> 00:11:00,900
And the fifth column contains the labeled class data, which is one of three classes.

85
00:11:01,900 --> 00:11:08,900
Satosa, Versicolor, or Virginica, which are the three different species of the iris flower.

86
00:11:09,900 --> 00:11:14,900
Next, we will need to split this data between training and testing.

87
00:11:15,900 --> 00:11:19,900
In order to do this, we'll be using Huda Nassar's code from last week.

88
00:11:20,900 --> 00:11:27,900
I am simply going to copy and paste this code from last week's tutorial so that you won't have to watch me retype this code.

89
00:11:27,900 --> 00:11:34,900
I'm going to go through the next few steps fairly quickly, since it's the same workflow that we used last week.

90
00:11:36,900 --> 00:11:45,900
I'm using the same random seed that I used last week, so the index numbers we used this week for training and testing should be the same.

91
00:11:46,900 --> 00:11:51,900
Next, we need to split the features between training and testing.

92
00:11:52,900 --> 00:12:00,900
I'm using the same random seed that I used last week, so the index numbers we used this week for training and testing should be the same as we used last week.

93
00:12:05,900 --> 00:12:10,900
Next, we need to split the features between training and testing.

94
00:12:16,900 --> 00:12:21,900
And then, we need to split the classes between training and testing.

95
00:12:26,900 --> 00:12:33,900
Unlike last week, we do not need to transpose our data, since the DecisionTree.jl package does not require it.

96
00:12:36,900 --> 00:12:40,900
Okay, we are now ready to build our first DecisionTree.

97
00:12:46,900 --> 00:12:54,900
In order to create a DecisionTree, all you need to do is use the DecisionTree classifier constructor.

98
00:12:56,900 --> 00:13:08,900
There are a lot of different keyword arguments that you can include in this constructor, but I'm only using the max underscore depth keyword argument, which will stop the tree from growing too large.

99
00:13:09,900 --> 00:13:15,900
You can find all of the available keyword arguments in the DecisionTree.jl documentation.

100
00:13:16,900 --> 00:13:29,900
The DecisionTree.jl package also includes a DecisionTree regressor constructor, which is used for regression problems, but I will only be covering the classifier in this tutorial.

101
00:13:31,900 --> 00:13:36,900
The rest of the workflow is very similar to the workflow that we use for SVM.

102
00:13:37,900 --> 00:13:42,900
You can fit your training data to the model by using the FitBang function.

103
00:13:44,900 --> 00:13:45,900
And that's it!

104
00:13:46,900 --> 00:13:51,900
You can see your DecisionTree in the REPL by using the print underscore tree function.

105
00:13:54,900 --> 00:13:59,900
I know it doesn't look like much, but there's a lot of information in this compact output.

106
00:14:01,900 --> 00:14:03,900
Here's the visualization of that output.

107
00:14:04,900 --> 00:14:07,900
The top line of the output is the root node.

108
00:14:08,900 --> 00:14:19,900
The algorithm has scanned all of the features and has determined that splitting feature number 4, or the fourth column, will provide the most useful information.

109
00:14:20,900 --> 00:14:25,900
The algorithm has selected a value of 0.8 as the threshold.

110
00:14:26,900 --> 00:14:35,900
Samples that have a feature 4 value of less than 0.8 go into the left node, and everything else goes into the right node.

111
00:14:37,900 --> 00:14:42,900
With the left node, the algorithm has determined that no further splits are required.

112
00:14:43,900 --> 00:14:49,900
This left node is a leaf node, and the classification is Satosa.

113
00:14:50,900 --> 00:14:58,900
The 35 slash 35 means that 35 samples have a class label of Satosa out of 35 total samples.

114
00:15:00,900 --> 00:15:08,900
The right node is a Decision node, since the algorithm has determined that more information may be gained by splitting the data further.

115
00:15:09,900 --> 00:15:27,900
For this subset of data, the algorithm has determined that splitting feature number 3, or the third column, will yield the most useful information, and it has selected a value of 4.95 as the threshold.

116
00:15:28,900 --> 00:15:38,900
Samples with a feature 3 value of less than 4.95 go into the left node, and everything else goes into the right node.

117
00:15:39,900 --> 00:15:47,900
At this point, the tree stops growing because we use the keyword argument max underscore depth equals 2.

118
00:15:48,900 --> 00:15:55,900
If we didn't use that keyword argument, the tree would continue to grow until the algorithm determined its own stopping point.

119
00:15:57,900 --> 00:16:01,900
As a result, the last two nodes are leaf nodes by default.

120
00:16:02,900 --> 00:16:14,900
For the left node, the algorithm is classifying that data subset as VersaColor, and for the right node, the algorithm is classifying that data subset as Virginia.

121
00:16:16,900 --> 00:16:20,900
But the algorithm knows that there are some misclassifications.

122
00:16:21,900 --> 00:16:34,900
In the left node, the 25 slash 26 means that 25 samples have a class label of VersaColor out of 26 total samples, meaning there's one sample that has been misclassified.

123
00:16:35,900 --> 00:16:49,900
In the right node, that 32 slash 33 means that 32 samples have a class label of Virginia out of 33 total samples, meaning there's one sample that has been misclassified.

124
00:16:50,900 --> 00:16:58,900
In order to get a better understanding of what the algorithm is looking at, let's take a look at the actual data subsets.

125
00:16:59,900 --> 00:17:04,900
Let's start by looking at the training data, which is what the root node is looking at.

126
00:17:10,900 --> 00:17:14,900
This data subset contains 94 samples.

127
00:17:15,900 --> 00:17:20,900
If you add up all the numbers in the decision tree nodes, you should get 94.

128
00:17:20,900 --> 00:17:30,900
If you sort column 4, you can see why the algorithm selected 0.8 as the threshold for feature number 4.

129
00:17:33,900 --> 00:17:38,900
It doesn't matter if it's less than or less than or equal to 0.8.

130
00:17:39,900 --> 00:17:45,900
As you can see, none of the samples have a feature 4 value of 0.8.

131
00:17:46,900 --> 00:17:52,900
Instead, there's a clear dividing line between the values of 0.6 and 1.

132
00:17:53,900 --> 00:17:57,900
0.8 is just the average between those two values.

133
00:17:59,900 --> 00:18:05,900
There are 35 samples above the line, and all of them have a class label of Satosa.

134
00:18:06,900 --> 00:18:09,900
That's where the 35 slash 35 comes from.

135
00:18:10,900 --> 00:18:16,900
Now, let's take a closer look at the data subset that was sent to the decision node on the right.

136
00:18:27,900 --> 00:18:34,900
This subset contains 59 samples, and the class labels are either VersaColor or Virginica.

137
00:18:34,900 --> 00:18:41,900
If you sort on column 3, you'll see why the algorithm selected 4.95 as the threshold.

138
00:18:45,900 --> 00:18:50,900
There is no sample with a feature 3 value of 4.95.

139
00:18:51,900 --> 00:18:59,900
Instead, there's a clear dividing line between 4.9 and 5, so 4.95 is just the average.

140
00:19:00,900 --> 00:19:05,900
There are 26 samples above the line, and 33 samples below the line.

141
00:19:07,900 --> 00:19:15,900
Out of the samples above the line, 25 have a class label of VersaColor, which is where the 25 slash 26 comes from.

142
00:19:17,900 --> 00:19:25,900
And of the samples below the line, 32 have a class label of Virginica, which is where the 32 slash 33 comes from.

143
00:19:30,900 --> 00:19:35,900
So in each class, there's one sample that has been misclassified.

144
00:19:36,900 --> 00:19:40,900
You can see the misclassified samples in this data view.

145
00:19:42,900 --> 00:19:49,900
Being able to visualize this much detail is unlike any machine learning algorithm that we've used in the past,

146
00:19:50,900 --> 00:19:53,900
which is one of the reasons why decision trees are so popular.

147
00:19:54,900 --> 00:20:02,900
But we already know that this model has some mistakes in it, so let's see how well it does at making predictions.

148
00:20:03,900 --> 00:20:08,900
Like last week, you can make predictions by using the predict function.

149
00:20:13,900 --> 00:20:18,900
Now that we have a prediction, we can check the accuracy of that prediction.

150
00:20:19,900 --> 00:20:26,900
So this accuracy score of 89% is not very good.

151
00:20:28,900 --> 00:20:34,900
You'll recall that our SVM model got an accuracy score of 95% right out of the box.

152
00:20:36,900 --> 00:20:39,900
It seems like all of this transparency came at a price.

153
00:20:41,900 --> 00:20:45,900
Although decision trees are easy to understand and easy to explain,

154
00:20:46,900 --> 00:20:49,900
they are often not very good at making predictions,

155
00:20:50,900 --> 00:20:55,900
which of course is the primary reason for building machine learning classification models in the first place.

156
00:20:58,900 --> 00:21:04,900
Let's take a deeper dive into the misclassifications to gain a better understanding of what went wrong.

157
00:21:06,900 --> 00:21:11,900
A common analytic tool used in machine learning is called a confusion matrix.

158
00:21:12,900 --> 00:21:19,900
A confusion matrix provides a simple way to visualize where your model got confused while making predictions.

159
00:21:21,900 --> 00:21:26,900
The decisionTree.jl package comes included with a confusion matrix constructor.

160
00:21:30,900 --> 00:21:36,900
The way to read this confusion matrix is that the classes from the first arguments are in the rows,

161
00:21:36,900 --> 00:21:40,900
and the classes from the second arguments are in the columns.

162
00:21:41,900 --> 00:21:46,900
So since we used y underscore test as the first argument,

163
00:21:46,900 --> 00:21:52,900
the rows show the counts for the actual labeled data that we used for testing purposes.

164
00:21:53,900 --> 00:21:57,900
Since we used y underscore hat as the second argument,

165
00:21:57,900 --> 00:22:02,900
the columns show the classes predicted by our decision tree model.

166
00:22:04,900 --> 00:22:08,900
The numbers shown along the diagonal are the correct predictions,

167
00:22:08,900 --> 00:22:12,900
and any other number represents an incorrect prediction.

168
00:22:13,900 --> 00:22:23,900
For example, that one indicates that our model predicted for genica once when the actual labeled class was versicolor.

169
00:22:24,900 --> 00:22:32,900
That five indicates that our model predicted versicolor five times when the actual labeled class was virginica.

170
00:22:33,900 --> 00:22:40,900
The accuracy score shown here matches the accuracy score that we calculated manually.

171
00:22:41,900 --> 00:22:47,900
The kappa coefficient is a metric that you can derive from this confusion matrix.

172
00:22:48,900 --> 00:22:56,900
I won't go into the math, but the kappa coefficient offers a more nuanced assessment of your model's predictive abilities.

173
00:22:57,900 --> 00:23:06,900
When making predictions, there's always a possibility that your model made a correct prediction purely by chance,

174
00:23:07,900 --> 00:23:10,900
thus artificially inflating the accuracy score.

175
00:23:12,900 --> 00:23:17,900
The kappa coefficient attempts to remove the probability of random positive predictions

176
00:23:17,900 --> 00:23:23,900
in order to present a more conservative way to assess your model's performance.

177
00:23:24,900 --> 00:23:31,900
As you can see, after factoring out the probability of making a correct prediction just by getting lucky,

178
00:23:32,900 --> 00:23:36,900
the accuracy score is closer to 83%.

179
00:23:38,900 --> 00:23:45,900
We can see the incorrect predictions using the same code that we used last week to display the results.

180
00:23:53,900 --> 00:24:08,900
As a reminder, the first column contains the predictions, and the second column contains the class from the labeled test data,

181
00:24:09,900 --> 00:24:13,900
and the third column indicates whether or not the prediction was correct.

182
00:24:15,900 --> 00:24:21,900
If you sort on column three, you can see there's one sample where our model predicted virginica

183
00:24:21,900 --> 00:24:24,900
when the actual species was versicolor.

184
00:24:25,900 --> 00:24:32,900
And there are five samples where our model predicted versicolor when the actual species was virginica,

185
00:24:33,900 --> 00:24:36,900
which is consistent with what the confusion matrix showed.

186
00:24:39,900 --> 00:24:44,900
You'll recall from last week that the SVM model is non-probabilistic,

187
00:24:45,900 --> 00:24:48,900
meaning that it makes predictions without assigning probabilities.

188
00:24:51,900 --> 00:24:53,900
The decision tree algorithm is different.

189
00:24:54,900 --> 00:25:02,900
You can see the probabilities determined by the decision tree model by using the predict underscore proper function.

190
00:25:08,900 --> 00:25:12,900
So this display shows the 56 samples in the test data set.

191
00:25:13,900 --> 00:25:19,900
Column one is the probability that the test sample has a class label of Satosa.

192
00:25:20,900 --> 00:25:25,900
Column two is the probability that the test sample has a class label of versicolor.

193
00:25:26,900 --> 00:25:31,900
And column three is the probability that the test sample has a class label of virginica.

194
00:25:33,900 --> 00:25:41,900
As you can see, our model is 100% confident that the first 15 samples have a class label of Satosa.

195
00:25:42,900 --> 00:25:46,900
But for every other sample, our model is not as confident,

196
00:25:46,900 --> 00:25:50,900
since it knows that there's some probability of misclassification.

197
00:25:51,900 --> 00:25:58,900
All of the other samples show a 96% probability when predicting either versicolor or virginica.

198
00:26:01,900 --> 00:26:04,900
Okay, so now what do we do?

199
00:26:06,900 --> 00:26:10,900
The decision tree is great since it's a wealth of information,

200
00:26:11,900 --> 00:26:13,900
but it's not so great at making predictions,

201
00:26:13,900 --> 00:26:16,900
which is really what we need.

202
00:26:17,900 --> 00:26:22,900
The good news is that there are a couple of ways to improve this accuracy score,

203
00:26:23,900 --> 00:26:28,900
but like many things in life, those improvements will come with some tradeoffs.

204
00:26:29,900 --> 00:26:37,900
One of those tradeoffs is a fundamental concept in machine learning called the bias variance tradeoff.

205
00:26:37,900 --> 00:26:47,900
On the surface, the bias variance tradeoff may seem like a relatively easy concept to understand,

206
00:26:48,900 --> 00:26:53,900
but upon closer inspection, one learns that it is very nuanced,

207
00:26:54,900 --> 00:26:57,900
so it's surprisingly difficult to implement in real life.

208
00:26:58,900 --> 00:27:04,900
The bias variance tradeoff is typically visualized by these three diagrams.

209
00:27:05,900 --> 00:27:10,900
The diagram on the left is an example of underfitting the data,

210
00:27:11,900 --> 00:27:17,900
meaning that it hasn't captured enough of the relevant relationships between features and outputs.

211
00:27:18,900 --> 00:27:23,900
The diagram on the right is an example of overfitting the data,

212
00:27:24,900 --> 00:27:27,900
meaning that it has learned the training data too well,

213
00:27:28,900 --> 00:27:33,900
so that it will make a lot of mistakes when trying to make predictions using new data.

214
00:27:35,900 --> 00:27:40,900
The diagram in the middle is an example of a model being just right,

215
00:27:41,900 --> 00:27:46,900
meaning that it has captured enough of the relevant relationships between features and outputs,

216
00:27:47,900 --> 00:27:53,900
while still being able to be generalized so that it can be used to make predictions using unseen data.

217
00:27:55,900 --> 00:27:59,900
So these diagrams are really from the data's perspective,

218
00:27:59,900 --> 00:28:04,900
so they aren't necessarily helpful when you're trying to design a model.

219
00:28:05,900 --> 00:28:12,900
A different way to visualize the bias variance tradeoff is by looking at these four diagrams,

220
00:28:13,900 --> 00:28:15,900
which is from the model's perspective.

221
00:28:16,900 --> 00:28:24,900
The lower left diagram shows a model with low bias and low variance, which is the best case scenario.

222
00:28:25,900 --> 00:28:33,900
The upper right diagram shows a model with high bias and high variance, which is the worst case scenario.

223
00:28:35,900 --> 00:28:40,900
In reality, most models are neither best case nor worst case.

224
00:28:41,900 --> 00:28:50,900
Instead, most models fall along that diagonal region between the upper left diagram and the lower right diagram,

225
00:28:51,900 --> 00:29:00,900
meaning that your model will most likely have a high bias and low variance, or low bias and high variance.

226
00:29:02,900 --> 00:29:07,900
The best that you can hope to achieve is some result near the middle of all four diagrams,

227
00:29:08,900 --> 00:29:11,900
resulting in a model with some bias and some variance.

228
00:29:12,900 --> 00:29:22,900
The model generated by the decision tree algorithm is an example of a model with high variance and low bias,

229
00:29:23,900 --> 00:29:26,900
meaning that it should be placed in that lower right box.

230
00:29:28,900 --> 00:29:32,900
Decision trees have a natural tendency to overfit the data,

231
00:29:33,900 --> 00:29:40,900
since in theory, it can continue to split your data until it has correctly classified every sample in the training data sets.

232
00:29:42,900 --> 00:29:51,900
But that's no good, since you can't generalize such a model, since new unseen data will most likely be different than the training data.

233
00:29:53,900 --> 00:30:00,900
You can improve the predictive accuracy of decision trees by using something called ensemble learning.

234
00:30:01,900 --> 00:30:07,900
There are two main ensemble learning techniques that are used to improve the performance of decision trees.

235
00:30:08,900 --> 00:30:12,900
One is called bagging, and the other is called boosting.

236
00:30:14,900 --> 00:30:21,900
We'll cover bagging first. An example of a bagging algorithm is known as random forest.

237
00:30:25,900 --> 00:30:29,900
Bagging is a shortened form of the term bootstrap aggregating.

238
00:30:30,900 --> 00:30:40,900
Without getting into the details, the concept behind bagging is that you can reduce the variance of your machine learning model by doing two things.

239
00:30:41,900 --> 00:30:49,900
One, you can increase the independence of your features, and two, you can increase the number of models.

240
00:30:50,900 --> 00:31:00,900
You then take this increased number of models with features that are more independent, then you aggregate them into one metamodel.

241
00:31:01,900 --> 00:31:07,900
The result is a model with lower variance, but with a slightly higher bias.

242
00:31:09,900 --> 00:31:12,900
This is the bias variance tradeoff in action.

243
00:31:13,900 --> 00:31:20,900
One of the best known examples of a bagging algorithm is random forest.

244
00:31:21,900 --> 00:31:29,900
The first algorithm for random decision forests was created in 1995 by Tin Cam Ho.

245
00:31:30,900 --> 00:31:39,900
The way that random forest works is that it introduces independence between the features and then generates multiple decision tree models.

246
00:31:40,900 --> 00:31:49,900
But the random forest introduces even more randomness into the algorithm by only considering a fraction of the features at every split.

247
00:31:50,900 --> 00:31:57,900
This results in even more independence between the features, which results in even lower variance.

248
00:31:59,900 --> 00:32:04,900
I realize that this is a lot of hand-waving, so let's see random forest in action.

249
00:32:05,900 --> 00:32:12,900
Fortunately, we can use the decisionTree.jl package to generate a random forest model.

250
00:32:13,900 --> 00:32:20,900
All we need to do is use the random forest classifier constructor to generate our model.

251
00:32:21,900 --> 00:32:26,900
By default, this constructor will generate 10 decision trees.

252
00:32:26,900 --> 00:32:33,900
You can set the number of decision trees manually by using the n underscore trees keyword argument.

253
00:32:35,900 --> 00:32:41,900
After that, the rest of the code is similar to the code that we used for the decision tree model.

254
00:32:41,900 --> 00:33:02,900
So, we created a random forest model using the exact same training data that we used earlier and the exact same testing data.

255
00:33:03,900 --> 00:33:17,900
This time, we achieved a predictive accuracy score of around 95%, which is a significant improvement over the 89% that we scored earlier and comparable to the SVM model from last week.

256
00:33:19,900 --> 00:33:23,900
Just like before, we can view our results on a confusion matrix.

257
00:33:24,900 --> 00:33:39,900
As you can see, our random forest model only made 3 misclassifications versus the 6 misclassifications made by the decision tree model.

258
00:33:40,900 --> 00:33:44,900
And just like before, we can view the actual mistakes.

259
00:33:53,900 --> 00:34:07,900
If you click on the header of column 3, you'll see the 3 misclassifications.

260
00:34:08,900 --> 00:34:18,900
It predicted virginica twice when the actual class was versicolor, and it predicted versicolor once when the actual class was virginica.

261
00:34:18,900 --> 00:34:24,900
You can also view the probabilities of those predictions.

262
00:34:34,900 --> 00:34:38,900
This time, the probabilities look a little different.

263
00:34:39,900 --> 00:34:45,900
This random forest model is not as confident as before when it comes to the Satosa class.

264
00:34:46,900 --> 00:34:51,900
There are several samples that have a 10% probability of being Satosa.

265
00:34:52,900 --> 00:35:04,900
On the flip side, this random forest model is much more confident about the versicolor and virginica predictions, with many samples having 100% probabilities.

266
00:35:05,900 --> 00:35:15,900
There are also more probability splits like 90-10, 85-15, 70-30, and so on.

267
00:35:17,900 --> 00:35:27,900
Although we can still view some interesting information, unfortunately, we do not have as much access to information as we had with the decision tree model.

268
00:35:28,900 --> 00:35:39,900
For example, even though I know that this random forest model generated 20 decision trees, I do not know what those trees look like.

269
00:35:40,900 --> 00:35:45,900
Nor do I know what features they used, or what threshold levels were determined.

270
00:35:46,900 --> 00:35:51,900
I can no longer present a visualization of what's going on under the hood.

271
00:35:52,900 --> 00:35:58,900
So the random forest is kind of like a black box, just like our SVM model from last week.

272
00:36:00,900 --> 00:36:06,900
So even though this random forest model is more accurate, it's more difficult to explain.

273
00:36:08,900 --> 00:36:14,900
As you are no doubt concluding, creating machine learning models is a series of tradeoffs.

274
00:36:16,900 --> 00:36:31,900
Before we conclude for today, let's go through an example of boosting, which is another ensemble learning technique that will reinforce this concept of tradeoffs when creating machine learning models.

275
00:36:35,900 --> 00:36:41,900
While bagging is used to lower variance, boosting is generally used to lower bias.

276
00:36:42,900 --> 00:36:57,900
While it makes sense to use bagging with decision trees, since decision trees naturally tend to have high variance, it's not immediately clear why using boosting would help improve the performance of decision trees.

277
00:36:59,900 --> 00:37:09,900
The motivation behind boosting is to answer the question, can a set of weak learners create a single, strong learner?

278
00:37:10,900 --> 00:37:20,900
An example of a weak learner is a classifier that is only slightly better at making predictions compared to a random guess.

279
00:37:22,900 --> 00:37:29,900
An example of a boosting algorithm is Adaboost, which is short for adaptive boosting.

280
00:37:30,900 --> 00:37:36,900
It was first introduced by Joachim Freund and Robert Shapur in 1997.

281
00:37:37,900 --> 00:37:45,900
Adaboost may be used with various learning algorithms, but it seems to pair particularly well with decision trees.

282
00:37:47,900 --> 00:38:05,900
Unlike random forests that grow multiple decision trees and then aggregate them to form a new metamodel, Adaboost starts with what is known as a stump, meaning that it's a decision tree with only a root node and two leaf nodes.

283
00:38:06,900 --> 00:38:18,900
It's not clear to me how this works for classifications with three classes, like the IRIS dataset, but Adaboost does work for multi-class classification problems.

284
00:38:19,900 --> 00:38:35,900
Based on the information gathered by the Adaboost algorithm from that single stump, the algorithm will increase the weights given to the misclassified data so that it can focus on the misclassifications in the next round.

285
00:38:36,900 --> 00:38:45,900
The algorithm will then use that new weighted dataset and pass it through another stump and make another set of predictions.

286
00:38:46,900 --> 00:38:57,900
Again, the algorithm will increase the weight given to the misclassified data so that it can focus on the remaining misclassified data in the next round.

287
00:38:58,900 --> 00:39:08,900
The theory is that by doing this, the algorithm will focus on the samples that are hardest to classify as it continues going through this process.

288
00:39:09,900 --> 00:39:21,900
It will repeat this process and add the knowledge it gains from all of these weak learners to form a single, strong metamodel that can be used to make predictions.

289
00:39:22,900 --> 00:39:32,900
I realize that this is even more abstract than the random forest, so let's take a look at Adaboost in action.

290
00:39:33,900 --> 00:39:39,900
Fortunately, we can use the DecisionTree.jl package to generate an Adaboost model.

291
00:39:40,900 --> 00:39:47,900
All we need to do is use the Adaboost Stump classifier constructor to generate our model.

292
00:39:48,900 --> 00:40:00,900
This model does not have a default setting for the number of iterations, so you will need to enter that number manually by using the N underscore iterations keyword arguments.

293
00:40:01,900 --> 00:40:14,900
Setting the N iterations to 20 means that our model will generate 20 stumps to create one tree, as opposed to creating 20 full trees used in the random forest model.

294
00:40:15,900 --> 00:40:26,900
After that, all of the code is exactly the same as the code used in the random forest example, so I'm just going to copy and paste the code from above.

295
00:40:26,900 --> 00:40:44,900
So, we created an Adaboost model using the exact same training data and testing data that we used for both the DecisionTree and the random forest.

296
00:40:45,900 --> 00:41:00,900
This time, we achieved a predictive accuracy score of around 95%, which is a significant improvement over the 89% of the DecisionTree and comparable to the random forest and SVM models.

297
00:41:01,900 --> 00:41:06,900
Like the random forest model, this Adaboost model is sort of a black box.

298
00:41:07,900 --> 00:41:15,900
We do not have access to as much information as the DecisionTree model, but we still have access to some information.

299
00:41:16,900 --> 00:41:21,900
Just like before, we can view our results on a confusion matrix.

300
00:41:22,900 --> 00:41:28,900
The confusion matrix looks exactly the same as the confusion matrix on the random forest model.

301
00:41:36,900 --> 00:41:47,900
And the prediction versus actual data looks identical to the random forest, so you may be wondering if anything is different.

302
00:41:48,900 --> 00:41:53,900
Let's take a look at the probability distribution of the predictions.

303
00:41:54,900 --> 00:42:08,900
So, this looks very different from the DecisionTree and the random forest.

304
00:42:09,900 --> 00:42:19,900
Although the Adaboost model had the same predictive accuracy as the random forest model, it is a lot less confident about its predictions.

305
00:42:20,900 --> 00:42:27,900
Most of the predictions have a 65-35 splits, or a 70-30 splits.

306
00:42:28,900 --> 00:42:35,900
There are even some examples with a 52-48 splits, which is barely better than a coin flip.

307
00:42:36,900 --> 00:42:45,900
There's not a single sample with a 100% probability, although there are plenty of samples with a 0% probability.

308
00:42:46,900 --> 00:42:49,900
So that's really interesting, isn't it?

309
00:42:50,900 --> 00:43:00,900
Both random forest and Adaboost offer the potential for enhanced predictive accuracy, but they approach the problem using very different techniques.

310
00:43:04,900 --> 00:43:09,900
We covered a lot of material today, so let's go through a quick summary.

311
00:43:10,900 --> 00:43:18,900
While DecisionTrees are relatively easy to visualize and easy to explain, they are surprisingly difficult to implement.

312
00:43:19,900 --> 00:43:28,900
They also have some flaws, namely that they are not known for their predictive accuracy and they are prone to overfitting.

313
00:43:29,900 --> 00:43:39,900
This led to a discussion about the bias variance trade-off, which will be an ongoing subject of conversation throughout this machine learning series.

314
00:43:40,900 --> 00:43:52,900
Through that discussion, we learned that we could potentially enhance the performance of DecisionTrees by using ensemble learning methods, namely bagging and boosting.

315
00:43:53,900 --> 00:44:02,900
Bagging is short for bootstrap aggregating, and Random Forest is one of the best known implementations of that method.

316
00:44:03,900 --> 00:44:14,900
Random Forest works by using various techniques to increase the independence of features and by increasing the number of DecisionTrees to be used in a metamodel.

317
00:44:15,900 --> 00:44:26,900
Boosting, on the other hand, uses an additive approach by using a collection of so-called weak learners to combine into a single strong learner.

318
00:44:27,900 --> 00:44:34,900
Adaboost, which is short for adaptive boosting, is a well-known implementation of boosting.

319
00:44:35,900 --> 00:44:47,900
Along the way, we were introduced to another analytic tool, the confusion matrix, along with the CAPA coefficients, that we can use to better understand our results.

320
00:44:49,900 --> 00:44:55,900
That's a lot to cover in one tutorial, and that's a lot to include in a Julia package.

321
00:44:56,900 --> 00:45:04,900
Like I said at the beginning, the DecisionTree.jl package is surprisingly sophisticated and is full of Easter eggs.

322
00:45:06,900 --> 00:45:18,900
Please show your appreciation to the developer and the contributors for sharing their knowledge with the rest of us by going to the GitHub page of DecisionTree.jl and leave them a star.

323
00:45:19,900 --> 00:45:29,900
Well, that's all for today. If you made it this far, congratulations.

324
00:45:30,900 --> 00:45:38,900
If you enjoyed this video and you feel like you learned something new, please give it a thumbs up.

325
00:45:39,900 --> 00:45:45,900
For more wholesome Julia tutorials, please be sure to subscribe and hit that bell.

326
00:45:45,900 --> 00:45:51,900
If you like what I do, then please consider joining and becoming a channel member.

327
00:45:52,900 --> 00:45:55,900
New tutorials are posted on Sundays slash Mondays.

328
00:45:56,900 --> 00:46:00,900
Thanks for watching, and I'll see you in the next video.

