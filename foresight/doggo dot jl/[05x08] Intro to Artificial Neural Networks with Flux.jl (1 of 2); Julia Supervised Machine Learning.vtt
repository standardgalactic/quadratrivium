WEBVTT

00:00.000 --> 00:07.440
Today, we tackle the famous MNIST classification problem, which has become the Hello World

00:07.440 --> 00:10.160
of Deep Learning.

00:10.160 --> 00:16.560
I introduced the MNIST database way back in the first episode of this series.

00:16.560 --> 00:22.480
As a reminder, it's a dataset containing a collection of handwritten numbers from 0

00:22.480 --> 00:24.960
to 9.

00:24.960 --> 00:33.320
It was created in 1998 to answer the question, is it possible to teach a computer to recognize

00:33.320 --> 00:35.440
handwritten digits?

00:35.440 --> 00:42.960
So, is it possible to teach your computer how to recognize handwritten digits?

00:42.960 --> 00:48.440
Well, let's find out.

00:48.440 --> 00:53.160
Welcome to Julia for Talented Amateurs, where I may call some Julia tutorials for talented

00:53.160 --> 00:55.160
amateurs everywhere.

00:55.160 --> 01:00.480
I am your host, the Dabbling Doggo.

01:00.480 --> 01:06.480
In today's tutorial, we'll get an introduction to Artificial Neural Networks, or simply known

01:06.480 --> 01:09.160
as Neural Networks.

01:09.160 --> 01:13.600
If you look on the Wikipedia page for machine learning and expand the supervised learning

01:13.600 --> 01:18.320
section, you'll see Artificial Neural Networks listed there.

01:19.320 --> 01:24.600
But you'll also notice that there's a separate section further down called Artificial Neural

01:24.600 --> 01:26.600
Networks.

01:26.600 --> 01:33.640
If you expand that section, you'll see a lot of different subjects listed there.

01:33.640 --> 01:38.400
Artificial Neural Networks are part of a very large field of study that goes well beyond

01:38.400 --> 01:41.400
the scope of the series.

01:41.400 --> 01:46.880
There is no way I'm going to be able to cover all of those subjects now, so I will try to

01:46.880 --> 01:51.400
cover them in a future 13-part series.

01:51.400 --> 01:56.560
For this series, I will use this tutorial and the next tutorial to provide a high-level

01:56.560 --> 02:04.160
introduction to Artificial Neural Networks, which is the gateway to deep learning.

02:04.160 --> 02:08.480
There are many different ways to approach the MNIST classification problem using various

02:08.480 --> 02:11.480
programming languages.

02:11.480 --> 02:16.640
In today's tutorial, we'll go through a pure Julia approach by building an Artificial Neural

02:16.640 --> 02:21.560
Network using the flux.jl package.

02:21.560 --> 02:26.280
In the next tutorial, we'll take the code that we developed today and go through it

02:26.280 --> 02:30.800
in more detail to understand the concepts behind the code.

02:30.800 --> 02:36.960
So, the game plan for today is to get through all of the code and to get our model up and

02:36.960 --> 02:39.040
running.

02:39.040 --> 02:43.640
As a result, I will not be providing a lot of explanations today.

02:47.200 --> 02:52.920
For today's tutorial, knowledge of Julia and VS Code is required.

02:52.920 --> 02:59.320
I'm also assuming that you're watching this entire machine learning playlist.

02:59.320 --> 03:05.920
In your VS Code Explorer panel, create a new folder for this tutorial.

03:05.920 --> 03:14.960
In the tutorial folder, create a new file called sl underscore ann.jl.

03:14.960 --> 03:21.200
Change the Julia REPL by using ALT J then ALT O.

03:21.200 --> 03:25.120
Maximize the REPL panel.

03:25.120 --> 03:30.400
Change the present working directory to your tutorial directory.

03:30.400 --> 03:35.880
Enter the package REPL by hitting the closing square bracket.

03:35.880 --> 03:39.560
Activate your tutorial directory.

03:39.560 --> 03:57.080
Add the following packages, flux, images, ml data sets, and plots.

03:57.080 --> 04:01.480
Type in status to confirm the version numbers.

04:01.480 --> 04:06.080
Exit the package REPL by hitting backspace.

04:06.080 --> 04:08.960
Change the REPL panel.

04:08.960 --> 04:19.360
Okay, let's get started.

04:19.360 --> 04:26.000
Let's start by loading some packages.

04:26.000 --> 04:30.640
One of the quirks of the flux package is that you also need to load some of the specific

04:30.640 --> 04:35.840
functions that you want to use in your session.

04:35.840 --> 04:42.200
And finally, we need to use some of Julia's standard libraries.

04:42.200 --> 04:49.960
Using a random seed is not required, but I'm using it so I can reproduce my results.

04:49.960 --> 04:55.240
The ml data sets package contains several data sets that are commonly used in machine

04:55.240 --> 04:58.120
learning.

04:58.120 --> 05:03.080
One of those data sets is the mNIST data sets.

05:03.080 --> 05:11.320
The ml data sets package also contains a convenient way to load those data sets into memory.

05:11.320 --> 05:16.720
If this is your first time loading the mNIST data set onto your computer, you should be

05:16.720 --> 05:21.440
prompted to confirm whether or not you want to download the data set, since it's quite

05:21.440 --> 05:23.600
large.

05:23.600 --> 05:26.680
Select yes to begin the download.

05:26.680 --> 05:32.000
Depending on your internet connection, it may take a few minutes.

05:32.000 --> 05:37.080
Since I already have the mNIST data set on my computer, the data set was loaded right

05:37.080 --> 05:40.080
away.

05:40.080 --> 05:44.480
Now that we have the data, let's take a look at it.

05:44.480 --> 05:50.720
This is the input training data.

05:50.720 --> 05:57.560
So it looks like a tensor of floating point numbers that has 28 rows and 28 columns and

05:57.560 --> 06:01.400
is 60,000 layers deep.

06:01.400 --> 06:06.680
This is by far the largest data set that I've used on my channel to date.

06:06.680 --> 06:12.080
Unfortunately, there's not a great way to view it in the REPL.

06:12.080 --> 06:19.200
The 28 rows and 28 columns contain numbers from 0 to 1 that represent a black and white

06:19.200 --> 06:23.120
image of a handwritten digit.

06:23.120 --> 06:30.520
We can use the images package to view one of the images.

06:30.520 --> 06:34.440
For some reason, the images are loaded horizontally.

06:34.440 --> 06:38.960
You need to transpose the image in order to view it vertically, so don't forget that

06:38.960 --> 06:42.040
apostrophe.

06:42.040 --> 06:47.760
You should see a white handwritten number on a black background.

06:47.760 --> 06:53.000
No offense, but that's not exactly the nicest handwriting.

06:53.000 --> 06:57.960
But it's a useful sample since everyone has a slightly different handwriting style, and

06:57.960 --> 07:02.560
it's important for your computer to be able to read different handwritten versions of

07:02.560 --> 07:05.280
the same number.

07:05.280 --> 07:09.840
Now the question is, what number is it?

07:09.840 --> 07:15.640
Like any other data set used for classification problems, every sample in the MNIST data set

07:15.640 --> 07:19.760
includes a label with a correct number.

07:19.760 --> 07:24.560
Let's take a look at the label attached to this image.

07:24.560 --> 07:30.800
So the labels are contained in a column vector containing 60,000 integers.

07:30.800 --> 07:35.960
These labels are integers from 0 to 9.

07:35.960 --> 07:40.720
The label for the first sample is 5.

07:40.720 --> 07:47.320
If you look at the image, it sort of looks like a 5, and it sort of doesn't.

07:47.320 --> 07:52.000
If it's this hard for humans to figure out, you can imagine how challenging this will

07:52.000 --> 07:57.000
be for our computer to try to figure this out.

07:57.000 --> 08:00.680
Now let's take a look at the test data.

08:00.680 --> 08:07.560
In past tutorials, we took our data set and split it between training and testing.

08:07.560 --> 08:14.360
The MNIST data set contains 60,000 samples for training and another 10,000 samples for

08:14.360 --> 08:21.800
testing, so there's no need to split the data since it's already been split for you.

08:21.800 --> 08:26.680
Let's take a look at a testing sample.

08:26.680 --> 08:44.840
So it's the same 28x28 matrix for the image, but it's only 10,000 layers deep.

08:44.840 --> 08:48.360
So this image is easier to read.

08:48.360 --> 09:00.320
It looks like a 7, but let's check the label to be sure.

09:00.320 --> 09:06.000
So the label is indeed 7, which is comforting.

09:06.000 --> 09:11.640
Now that we have our data loaded into memory and have it split between inputs and outputs,

09:11.640 --> 09:19.560
as well as split between training and testing, we should be ready to build our model, right?

09:19.560 --> 09:22.600
Well no, not exactly.

09:22.600 --> 09:27.360
There are a couple of additional preprocessing steps that we need to do.

09:27.360 --> 09:33.440
Fortunately, the Flux.jl package comes included with utilities to make these preprocessing

09:33.440 --> 09:36.200
steps easy.

09:36.200 --> 09:42.440
For the input data, we need to, quote unquote, flatten the three-dimensional tensor into

09:42.440 --> 09:44.840
a two-dimensional matrix.

09:44.840 --> 09:51.560
Normally, that would mean reshaping our Julia array, but the Flux.jl package has a utility

09:51.560 --> 09:56.760
function called flatten that will do that for us.

09:56.760 --> 10:06.760
You can see that our 28x28x60,000 tensor is now a 784x60,000 matrix.

10:06.760 --> 10:14.880
All it did was take the 28x28 image and convert it into a column vector with 784 elements,

10:14.880 --> 10:21.240
so each column contains the floating point numbers associated with each image.

10:21.240 --> 10:27.240
We also need to flatten the inputs for the testing data as well.

10:27.240 --> 10:42.440
For the labels, we need to do something called one-hot encoding.

10:42.440 --> 10:46.480
If you look in the REPL, you'll see what it did.

10:46.480 --> 10:53.080
For each label, it replaced the integer with a column vector with a one at the index representing

10:53.080 --> 10:55.520
the integer.

10:55.520 --> 10:59.840
So in the first column, that one is in the sixth row.

10:59.840 --> 11:05.760
Well, you'll recall that the first label is five, not six.

11:05.760 --> 11:10.400
That's because this column vector starts at zero and goes to nine.

11:10.400 --> 11:16.160
So the sixth row represents the label for the number five.

11:16.160 --> 11:21.800
The one-hot batch function also concatenates all of these column vectors horizontally, so

11:21.800 --> 11:29.800
the result is a 10x60,000 one-hot matrix made up of Boolean values.

11:29.800 --> 11:34.200
All of those dots are zeros.

11:34.200 --> 11:41.200
We need to do the same thing for the testing labels.

11:41.200 --> 11:48.560
In the REPL, you can see that it's a similar result, except the testing data only has 10,000

11:48.560 --> 11:52.560
labels.

11:52.560 --> 12:02.880
Okay, we are now ready to define our model.

12:02.880 --> 12:08.600
Unlike other machine learning packages that we've seen so far, the flux.jl package is

12:08.600 --> 12:10.760
not an algorithm.

12:10.760 --> 12:16.560
Instead, it's a deep learning toolkit that provides building blocks that you can use to

12:16.560 --> 12:20.120
create your own custom deep learning models.

12:20.120 --> 12:26.680
We'll go through the flux package in more detail in the next tutorial, but for now, let's

12:26.680 --> 12:36.000
just use those building blocks to create our first artificial neural network.

12:36.000 --> 12:41.800
There's a lot going on in this deceptively simple code.

12:41.800 --> 12:46.760
Here's a visualization of what we just built.

12:46.760 --> 12:49.920
This is a diagram of a neural network.

12:49.920 --> 12:58.240
Specifically, this is an example of a multi-layer perceptron, or MLP, which is a type of artificial

12:58.240 --> 13:01.120
neural network.

13:01.240 --> 13:07.440
Although this model is more complex than any model that we've seen so far, the MLP is

13:07.440 --> 13:12.560
considered relatively simple in the deep learning world.

13:12.560 --> 13:18.520
We'll go through this diagram in more detail in the next tutorial, but at a very high level,

13:18.520 --> 13:24.880
this diagram represents a model containing a lot of parameters.

13:24.880 --> 13:29.680
We're going to feed our training inputs into this model, and then the model will try to

13:29.680 --> 13:35.400
learn the parameters necessary to predict the training labels, just like any other machine

13:35.400 --> 13:38.240
learning model.

13:38.240 --> 13:44.440
In other words, the model will digest the data and calculate a loss.

13:44.440 --> 13:50.600
Based on the result after a single epoch, the model will then update the parameter slightly

13:50.600 --> 13:56.800
in order to reduce the loss by using an optimization algorithm.

13:56.800 --> 14:01.520
Your model will repeat this process until you tell it to stop.

14:01.520 --> 14:08.920
I'll cover all of those other coding terms like chain, dense, reilu, and softmax in the

14:08.920 --> 14:10.800
next tutorial.

14:10.800 --> 14:20.920
Let's move on to defining the loss function.

14:20.920 --> 14:26.440
There are many different loss functions used in deep learning, and the flux.jl package

14:26.440 --> 14:29.680
supports all of the major loss functions.

14:29.680 --> 14:35.280
Today, we're using a loss function called cross entropy.

14:35.280 --> 14:41.080
Again, more on this in the next tutorial.

14:41.080 --> 14:45.360
Our model contains a lot of different parameters.

14:45.360 --> 14:51.760
The flux.jl package initializes all of those parameters using random values.

14:51.760 --> 14:58.600
Next, we need to select an optimization algorithm that will determine how our computer will

14:58.600 --> 15:02.880
quote unquote, learn the data.

15:02.880 --> 15:07.960
There are several different optimizers used in deep learning, and the flux.jl package

15:07.960 --> 15:11.320
supports all of the commonly used optimizers.

15:12.320 --> 15:21.320
Today, we're going to use an optimizer called atom, which is short for adaptive moment estimation.

15:21.320 --> 15:27.800
Okay, we are now ready to train our model.

15:27.800 --> 15:32.840
The flux.jl package comes included with a handy utility that makes it easy to train

15:32.840 --> 15:34.960
our model.

15:34.960 --> 15:39.480
All we need to do is provide a for loop to repeat the training process over multiple

15:39.480 --> 15:42.760
epochs.

15:42.760 --> 15:54.000
Just a warning, depending on your computer's CPU, this may take several minutes to train.

15:54.000 --> 16:09.720
I'm going to fast forward through this part.

16:09.720 --> 16:15.520
After an initial delay, you should see some outputs in the REPL, and if everything went

16:15.520 --> 16:27.320
okay, the training loss should be decreasing over time.

16:27.320 --> 16:31.880
We now have a trained model with lots of updated parameters.

16:31.880 --> 16:37.520
So, how do we use this model in order to make predictions?

16:38.520 --> 16:44.040
Well, we can just run our test data through our newly trained model in order to get the

16:44.040 --> 16:46.200
predictions.

16:46.200 --> 16:54.680
So, running our test data through our model results in a 10 by 10,000 matrix, which you

16:54.680 --> 16:57.440
can see in the REPL.

16:57.440 --> 17:03.160
It's a little difficult to read, but those crazy looking numbers are really small values

17:03.160 --> 17:05.600
close to zero.

17:05.600 --> 17:11.240
In each column, you should see a single value close to one.

17:11.240 --> 17:17.880
The sum of each column adds up to 100%, and each row contains the probability of the

17:17.880 --> 17:20.800
prediction.

17:20.800 --> 17:27.520
Remember that the index numbers for the rows go from 1 to 10, but our labels go from 0

17:27.520 --> 17:28.520
to 9.

17:28.520 --> 17:35.040
So, the first row is the probability that the image is a zero, and the second row is

17:35.040 --> 17:40.880
the probability that the image is a one, and so on.

17:40.880 --> 17:46.600
In order to make it easier to work with these predictions, we can use the one cold utility

17:46.600 --> 17:52.800
function from the flux.jl package, which is like the opposite of the one hot batch utility

17:52.800 --> 17:55.300
function.

17:55.300 --> 18:00.800
A one cold function converts a matrix into a column vector containing the index number

18:00.800 --> 18:04.400
that has the highest probability value.

18:04.400 --> 18:10.320
In order to convert index numbers into labels, we need to subtract one from each of the index

18:10.320 --> 18:12.920
numbers.

18:12.920 --> 18:19.480
You can see that our predictions are now contained in a column vector with 10,000 elements.

18:19.480 --> 18:25.680
Now, all we need to do is compare the predicted labels with the actual labels for the test

18:25.680 --> 18:27.200
data.

18:28.200 --> 18:36.440
So, our little artificial neural network model achieved an accuracy score of 96.24%, which

18:36.440 --> 18:40.720
is pretty amazing considering how difficult the challenge is.

18:40.720 --> 18:47.360
Now, before we go out and celebrate, let's take a look at the current best-in-class accuracy

18:47.360 --> 18:49.800
score.

18:49.800 --> 18:55.400
According to the Wikipedia article on the MNIST database, the highest average accuracy

18:55.400 --> 19:03.960
score for any machine learning model is 99.83%, which was achieved in 2020 using something

19:03.960 --> 19:08.800
called a convolutional neural network.

19:08.800 --> 19:14.080
In any event, let's take a look at some of our misclassifications to see if we can gain

19:14.080 --> 19:32.720
any insights.

19:32.720 --> 19:36.880
So this table contains 10,000 rows.

19:36.880 --> 19:44.880
The first column is the index number for both the predictions as well as the actual

19:44.880 --> 19:47.160
label data.

19:47.160 --> 19:52.920
The second column contains the predicted labels, and the third column contains the actual

19:52.920 --> 19:56.120
labels from the test data sets.

19:56.120 --> 20:02.520
The fourth column contains one for true if the predicted label has been classified correctly,

20:02.520 --> 20:08.920
and a zero for false if the predicted label has been misclassified.

20:08.920 --> 20:19.640
If you sort on column 4, you can see all of the misclassifications.

20:19.640 --> 20:27.200
So there are 376 misclassifications, which sounds like a lot, but remember, there are

20:27.200 --> 20:31.320
10,000 test samples.

20:31.320 --> 20:47.000
Let's take a look at the first misclassification, which has an index number of 9.

20:47.000 --> 20:51.880
So I don't know about you, but I cannot read this handwriting.

20:51.880 --> 21:03.360
I mean, what is that?

21:03.360 --> 21:10.760
It's labeled as a 5, but does that look like a 5 to you?

21:10.760 --> 21:17.200
Our model guessed 4, and you can kind of forgive it since I don't think I could have classified

21:17.200 --> 21:21.480
this image correctly as a 5 just by looking at it.

21:21.480 --> 21:25.000
Anyways, you get my point.

21:25.000 --> 21:31.600
This is not a trivial classification problem, and it's amazing that any model can achieve

21:31.600 --> 21:37.000
accuracy levels that are comparable to human beings, given the wide range in handwriting

21:37.000 --> 21:40.160
styles.

21:40.160 --> 21:43.480
So that was fun, right?

21:43.480 --> 22:12.000
Here we go, let's plot a learning curve to see how our model did while training.

22:12.000 --> 22:18.560
So this learning curve looks similar to other learning curves that we've seen in past tutorials.

22:18.560 --> 22:23.360
Even though the code for this neural network looks very different, the overall learning

22:23.360 --> 22:29.560
workflow is very similar to other machine learning algorithms.

22:29.560 --> 22:40.360
Let's save this plot and recap what we just witnessed.

22:40.360 --> 22:47.040
Today we became very familiar with the MNIST data sets, and we got a quick introduction

22:47.040 --> 22:50.680
to the Flux.jl package.

22:50.680 --> 22:56.400
After doing a little preprocessing work on the data, we immediately built an artificial

22:56.400 --> 23:03.320
neural network model using the tools provided by the Flux.jl package.

23:03.320 --> 23:08.760
Then we followed a similar workflow that we used with other machine learning algorithms

23:08.760 --> 23:14.960
by defining a loss function, by initializing parameters, by selecting an optimizer, and

23:14.960 --> 23:18.800
by training our model using a for loop.

23:18.800 --> 23:25.880
But the actual details of that workflow are very different with a lot of new terms.

23:25.880 --> 23:31.760
In the end, we were able to use this trained model in order to make predictions, like we

23:31.760 --> 23:35.480
did with other machine learning models.

23:35.480 --> 23:40.920
We were also able to calculate an accuracy score and plot a learning curve.

23:40.920 --> 23:48.480
Hopefully, today's tutorial has left you both excited and confused.

23:48.480 --> 23:52.840
After seeing what's possible with artificial neural networks, you probably want to use

23:52.840 --> 23:58.320
it right away, but may be hesitant since you may be wondering what's going on under the

23:58.320 --> 24:00.680
hood.

24:00.680 --> 24:06.480
In the next tutorial, we'll revisit this code in order to gain a better understanding of

24:06.480 --> 24:12.600
the concepts that make these artificial neural networks such a modern marvel.

24:12.600 --> 24:16.600
So stay tuned for that.

24:16.600 --> 24:21.160
Well, that's all for today.

24:21.160 --> 24:27.040
If you made it this far, congratulations!

24:27.040 --> 24:32.120
If you enjoyed this video and you feel like you learned something new, please give it

24:32.120 --> 24:34.440
a thumbs up.

24:34.440 --> 24:41.520
For more wholesome Julia tutorials, please be sure to subscribe and hit that bell.

24:41.520 --> 24:47.640
If you like what I do, then please consider joining and becoming a channel member.

24:47.640 --> 24:52.600
New tutorials are posted on Sundays slash Mondays.

24:52.600 --> 24:56.600
Thanks for watching, and I'll see you in the next video.

