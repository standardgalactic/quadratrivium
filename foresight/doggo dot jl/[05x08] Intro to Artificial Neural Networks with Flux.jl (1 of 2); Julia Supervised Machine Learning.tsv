start	end	text
0	7440	Today, we tackle the famous MNIST classification problem, which has become the Hello World
7440	10160	of Deep Learning.
10160	16560	I introduced the MNIST database way back in the first episode of this series.
16560	22480	As a reminder, it's a dataset containing a collection of handwritten numbers from 0
22480	24960	to 9.
24960	33320	It was created in 1998 to answer the question, is it possible to teach a computer to recognize
33320	35440	handwritten digits?
35440	42960	So, is it possible to teach your computer how to recognize handwritten digits?
42960	48440	Well, let's find out.
48440	53160	Welcome to Julia for Talented Amateurs, where I may call some Julia tutorials for talented
53160	55160	amateurs everywhere.
55160	60480	I am your host, the Dabbling Doggo.
60480	66480	In today's tutorial, we'll get an introduction to Artificial Neural Networks, or simply known
66480	69160	as Neural Networks.
69160	73600	If you look on the Wikipedia page for machine learning and expand the supervised learning
73600	78320	section, you'll see Artificial Neural Networks listed there.
79320	84600	But you'll also notice that there's a separate section further down called Artificial Neural
84600	86600	Networks.
86600	93640	If you expand that section, you'll see a lot of different subjects listed there.
93640	98400	Artificial Neural Networks are part of a very large field of study that goes well beyond
98400	101400	the scope of the series.
101400	106880	There is no way I'm going to be able to cover all of those subjects now, so I will try to
106880	111400	cover them in a future 13-part series.
111400	116560	For this series, I will use this tutorial and the next tutorial to provide a high-level
116560	124160	introduction to Artificial Neural Networks, which is the gateway to deep learning.
124160	128480	There are many different ways to approach the MNIST classification problem using various
128480	131480	programming languages.
131480	136640	In today's tutorial, we'll go through a pure Julia approach by building an Artificial Neural
136640	141560	Network using the flux.jl package.
141560	146280	In the next tutorial, we'll take the code that we developed today and go through it
146280	150800	in more detail to understand the concepts behind the code.
150800	156960	So, the game plan for today is to get through all of the code and to get our model up and
156960	159040	running.
159040	163640	As a result, I will not be providing a lot of explanations today.
167200	172920	For today's tutorial, knowledge of Julia and VS Code is required.
172920	179320	I'm also assuming that you're watching this entire machine learning playlist.
179320	185920	In your VS Code Explorer panel, create a new folder for this tutorial.
185920	194960	In the tutorial folder, create a new file called sl underscore ann.jl.
194960	201200	Change the Julia REPL by using ALT J then ALT O.
201200	205120	Maximize the REPL panel.
205120	210400	Change the present working directory to your tutorial directory.
210400	215880	Enter the package REPL by hitting the closing square bracket.
215880	219560	Activate your tutorial directory.
219560	237080	Add the following packages, flux, images, ml data sets, and plots.
237080	241480	Type in status to confirm the version numbers.
241480	246080	Exit the package REPL by hitting backspace.
246080	248960	Change the REPL panel.
248960	259360	Okay, let's get started.
259360	266000	Let's start by loading some packages.
266000	270640	One of the quirks of the flux package is that you also need to load some of the specific
270640	275840	functions that you want to use in your session.
275840	282200	And finally, we need to use some of Julia's standard libraries.
282200	289960	Using a random seed is not required, but I'm using it so I can reproduce my results.
289960	295240	The ml data sets package contains several data sets that are commonly used in machine
295240	298120	learning.
298120	303080	One of those data sets is the mNIST data sets.
303080	311320	The ml data sets package also contains a convenient way to load those data sets into memory.
311320	316720	If this is your first time loading the mNIST data set onto your computer, you should be
316720	321440	prompted to confirm whether or not you want to download the data set, since it's quite
321440	323600	large.
323600	326680	Select yes to begin the download.
326680	332000	Depending on your internet connection, it may take a few minutes.
332000	337080	Since I already have the mNIST data set on my computer, the data set was loaded right
337080	340080	away.
340080	344480	Now that we have the data, let's take a look at it.
344480	350720	This is the input training data.
350720	357560	So it looks like a tensor of floating point numbers that has 28 rows and 28 columns and
357560	361400	is 60,000 layers deep.
361400	366680	This is by far the largest data set that I've used on my channel to date.
366680	372080	Unfortunately, there's not a great way to view it in the REPL.
372080	379200	The 28 rows and 28 columns contain numbers from 0 to 1 that represent a black and white
379200	383120	image of a handwritten digit.
383120	390520	We can use the images package to view one of the images.
390520	394440	For some reason, the images are loaded horizontally.
394440	398960	You need to transpose the image in order to view it vertically, so don't forget that
398960	402040	apostrophe.
402040	407760	You should see a white handwritten number on a black background.
407760	413000	No offense, but that's not exactly the nicest handwriting.
413000	417960	But it's a useful sample since everyone has a slightly different handwriting style, and
417960	422560	it's important for your computer to be able to read different handwritten versions of
422560	425280	the same number.
425280	429840	Now the question is, what number is it?
429840	435640	Like any other data set used for classification problems, every sample in the MNIST data set
435640	439760	includes a label with a correct number.
439760	444560	Let's take a look at the label attached to this image.
444560	450800	So the labels are contained in a column vector containing 60,000 integers.
450800	455960	These labels are integers from 0 to 9.
455960	460720	The label for the first sample is 5.
460720	467320	If you look at the image, it sort of looks like a 5, and it sort of doesn't.
467320	472000	If it's this hard for humans to figure out, you can imagine how challenging this will
472000	477000	be for our computer to try to figure this out.
477000	480680	Now let's take a look at the test data.
480680	487560	In past tutorials, we took our data set and split it between training and testing.
487560	494360	The MNIST data set contains 60,000 samples for training and another 10,000 samples for
494360	501800	testing, so there's no need to split the data since it's already been split for you.
501800	506680	Let's take a look at a testing sample.
506680	524840	So it's the same 28x28 matrix for the image, but it's only 10,000 layers deep.
524840	528360	So this image is easier to read.
528360	540320	It looks like a 7, but let's check the label to be sure.
540320	546000	So the label is indeed 7, which is comforting.
546000	551640	Now that we have our data loaded into memory and have it split between inputs and outputs,
551640	559560	as well as split between training and testing, we should be ready to build our model, right?
559560	562600	Well no, not exactly.
562600	567360	There are a couple of additional preprocessing steps that we need to do.
567360	573440	Fortunately, the Flux.jl package comes included with utilities to make these preprocessing
573440	576200	steps easy.
576200	582440	For the input data, we need to, quote unquote, flatten the three-dimensional tensor into
582440	584840	a two-dimensional matrix.
584840	591560	Normally, that would mean reshaping our Julia array, but the Flux.jl package has a utility
591560	596760	function called flatten that will do that for us.
596760	606760	You can see that our 28x28x60,000 tensor is now a 784x60,000 matrix.
606760	614880	All it did was take the 28x28 image and convert it into a column vector with 784 elements,
614880	621240	so each column contains the floating point numbers associated with each image.
621240	627240	We also need to flatten the inputs for the testing data as well.
627240	642440	For the labels, we need to do something called one-hot encoding.
642440	646480	If you look in the REPL, you'll see what it did.
646480	653080	For each label, it replaced the integer with a column vector with a one at the index representing
653080	655520	the integer.
655520	659840	So in the first column, that one is in the sixth row.
659840	665760	Well, you'll recall that the first label is five, not six.
665760	670400	That's because this column vector starts at zero and goes to nine.
670400	676160	So the sixth row represents the label for the number five.
676160	681800	The one-hot batch function also concatenates all of these column vectors horizontally, so
681800	689800	the result is a 10x60,000 one-hot matrix made up of Boolean values.
689800	694200	All of those dots are zeros.
694200	701200	We need to do the same thing for the testing labels.
701200	708560	In the REPL, you can see that it's a similar result, except the testing data only has 10,000
708560	712560	labels.
712560	722880	Okay, we are now ready to define our model.
722880	728600	Unlike other machine learning packages that we've seen so far, the flux.jl package is
728600	730760	not an algorithm.
730760	736560	Instead, it's a deep learning toolkit that provides building blocks that you can use to
736560	740120	create your own custom deep learning models.
740120	746680	We'll go through the flux package in more detail in the next tutorial, but for now, let's
746680	756000	just use those building blocks to create our first artificial neural network.
756000	761800	There's a lot going on in this deceptively simple code.
761800	766760	Here's a visualization of what we just built.
766760	769920	This is a diagram of a neural network.
769920	778240	Specifically, this is an example of a multi-layer perceptron, or MLP, which is a type of artificial
778240	781120	neural network.
781240	787440	Although this model is more complex than any model that we've seen so far, the MLP is
787440	792560	considered relatively simple in the deep learning world.
792560	798520	We'll go through this diagram in more detail in the next tutorial, but at a very high level,
798520	804880	this diagram represents a model containing a lot of parameters.
804880	809680	We're going to feed our training inputs into this model, and then the model will try to
809680	815400	learn the parameters necessary to predict the training labels, just like any other machine
815400	818240	learning model.
818240	824440	In other words, the model will digest the data and calculate a loss.
824440	830600	Based on the result after a single epoch, the model will then update the parameter slightly
830600	836800	in order to reduce the loss by using an optimization algorithm.
836800	841520	Your model will repeat this process until you tell it to stop.
841520	848920	I'll cover all of those other coding terms like chain, dense, reilu, and softmax in the
848920	850800	next tutorial.
850800	860920	Let's move on to defining the loss function.
860920	866440	There are many different loss functions used in deep learning, and the flux.jl package
866440	869680	supports all of the major loss functions.
869680	875280	Today, we're using a loss function called cross entropy.
875280	881080	Again, more on this in the next tutorial.
881080	885360	Our model contains a lot of different parameters.
885360	891760	The flux.jl package initializes all of those parameters using random values.
891760	898600	Next, we need to select an optimization algorithm that will determine how our computer will
898600	902880	quote unquote, learn the data.
902880	907960	There are several different optimizers used in deep learning, and the flux.jl package
907960	911320	supports all of the commonly used optimizers.
912320	921320	Today, we're going to use an optimizer called atom, which is short for adaptive moment estimation.
921320	927800	Okay, we are now ready to train our model.
927800	932840	The flux.jl package comes included with a handy utility that makes it easy to train
932840	934960	our model.
934960	939480	All we need to do is provide a for loop to repeat the training process over multiple
939480	942760	epochs.
942760	954000	Just a warning, depending on your computer's CPU, this may take several minutes to train.
954000	969720	I'm going to fast forward through this part.
969720	975520	After an initial delay, you should see some outputs in the REPL, and if everything went
975520	987320	okay, the training loss should be decreasing over time.
987320	991880	We now have a trained model with lots of updated parameters.
991880	997520	So, how do we use this model in order to make predictions?
998520	1004040	Well, we can just run our test data through our newly trained model in order to get the
1004040	1006200	predictions.
1006200	1014680	So, running our test data through our model results in a 10 by 10,000 matrix, which you
1014680	1017440	can see in the REPL.
1017440	1023160	It's a little difficult to read, but those crazy looking numbers are really small values
1023160	1025600	close to zero.
1025600	1031240	In each column, you should see a single value close to one.
1031240	1037880	The sum of each column adds up to 100%, and each row contains the probability of the
1037880	1040800	prediction.
1040800	1047520	Remember that the index numbers for the rows go from 1 to 10, but our labels go from 0
1047520	1048520	to 9.
1048520	1055040	So, the first row is the probability that the image is a zero, and the second row is
1055040	1060880	the probability that the image is a one, and so on.
1060880	1066600	In order to make it easier to work with these predictions, we can use the one cold utility
1066600	1072800	function from the flux.jl package, which is like the opposite of the one hot batch utility
1072800	1075300	function.
1075300	1080800	A one cold function converts a matrix into a column vector containing the index number
1080800	1084400	that has the highest probability value.
1084400	1090320	In order to convert index numbers into labels, we need to subtract one from each of the index
1090320	1092920	numbers.
1092920	1099480	You can see that our predictions are now contained in a column vector with 10,000 elements.
1099480	1105680	Now, all we need to do is compare the predicted labels with the actual labels for the test
1105680	1107200	data.
1108200	1116440	So, our little artificial neural network model achieved an accuracy score of 96.24%, which
1116440	1120720	is pretty amazing considering how difficult the challenge is.
1120720	1127360	Now, before we go out and celebrate, let's take a look at the current best-in-class accuracy
1127360	1129800	score.
1129800	1135400	According to the Wikipedia article on the MNIST database, the highest average accuracy
1135400	1143960	score for any machine learning model is 99.83%, which was achieved in 2020 using something
1143960	1148800	called a convolutional neural network.
1148800	1154080	In any event, let's take a look at some of our misclassifications to see if we can gain
1154080	1172720	any insights.
1172720	1176880	So this table contains 10,000 rows.
1176880	1184880	The first column is the index number for both the predictions as well as the actual
1184880	1187160	label data.
1187160	1192920	The second column contains the predicted labels, and the third column contains the actual
1192920	1196120	labels from the test data sets.
1196120	1202520	The fourth column contains one for true if the predicted label has been classified correctly,
1202520	1208920	and a zero for false if the predicted label has been misclassified.
1208920	1219640	If you sort on column 4, you can see all of the misclassifications.
1219640	1227200	So there are 376 misclassifications, which sounds like a lot, but remember, there are
1227200	1231320	10,000 test samples.
1231320	1247000	Let's take a look at the first misclassification, which has an index number of 9.
1247000	1251880	So I don't know about you, but I cannot read this handwriting.
1251880	1263360	I mean, what is that?
1263360	1270760	It's labeled as a 5, but does that look like a 5 to you?
1270760	1277200	Our model guessed 4, and you can kind of forgive it since I don't think I could have classified
1277200	1281480	this image correctly as a 5 just by looking at it.
1281480	1285000	Anyways, you get my point.
1285000	1291600	This is not a trivial classification problem, and it's amazing that any model can achieve
1291600	1297000	accuracy levels that are comparable to human beings, given the wide range in handwriting
1297000	1300160	styles.
1300160	1303480	So that was fun, right?
1303480	1332000	Here we go, let's plot a learning curve to see how our model did while training.
1332000	1338560	So this learning curve looks similar to other learning curves that we've seen in past tutorials.
1338560	1343360	Even though the code for this neural network looks very different, the overall learning
1343360	1349560	workflow is very similar to other machine learning algorithms.
1349560	1360360	Let's save this plot and recap what we just witnessed.
1360360	1367040	Today we became very familiar with the MNIST data sets, and we got a quick introduction
1367040	1370680	to the Flux.jl package.
1370680	1376400	After doing a little preprocessing work on the data, we immediately built an artificial
1376400	1383320	neural network model using the tools provided by the Flux.jl package.
1383320	1388760	Then we followed a similar workflow that we used with other machine learning algorithms
1388760	1394960	by defining a loss function, by initializing parameters, by selecting an optimizer, and
1394960	1398800	by training our model using a for loop.
1398800	1405880	But the actual details of that workflow are very different with a lot of new terms.
1405880	1411760	In the end, we were able to use this trained model in order to make predictions, like we
1411760	1415480	did with other machine learning models.
1415480	1420920	We were also able to calculate an accuracy score and plot a learning curve.
1420920	1428480	Hopefully, today's tutorial has left you both excited and confused.
1428480	1432840	After seeing what's possible with artificial neural networks, you probably want to use
1432840	1438320	it right away, but may be hesitant since you may be wondering what's going on under the
1438320	1440680	hood.
1440680	1446480	In the next tutorial, we'll revisit this code in order to gain a better understanding of
1446480	1452600	the concepts that make these artificial neural networks such a modern marvel.
1452600	1456600	So stay tuned for that.
1456600	1461160	Well, that's all for today.
1461160	1467040	If you made it this far, congratulations!
1467040	1472120	If you enjoyed this video and you feel like you learned something new, please give it
1472120	1474440	a thumbs up.
1474440	1481520	For more wholesome Julia tutorials, please be sure to subscribe and hit that bell.
1481520	1487640	If you like what I do, then please consider joining and becoming a channel member.
1487640	1492600	New tutorials are posted on Sundays slash Mondays.
1492600	1496600	Thanks for watching, and I'll see you in the next video.
