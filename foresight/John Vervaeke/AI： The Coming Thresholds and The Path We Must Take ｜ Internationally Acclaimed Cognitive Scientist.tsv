start	end	text
0	2400	Welcome, everyone.
2400	4280	This is not a Voices with Ferveki.
4280	6320	This is a new entity.
6320	9000	I'm calling a video essay.
9000	11080	But under good advice from the two gentlemen
11080	13640	that are joining me, it was proposed to me.
13640	15640	And I accept the proposal that this
15640	19800	should have a little bit more of a dialogical structure to it.
19800	23880	And given the value of dialogue, as I've
23880	26760	been explaining it in other work,
26760	28080	I took this deeply to heart.
30080	32760	I am going to present still an essay.
32760	36080	And let's remember what Montaigne meant by essay.
36080	37880	S-A-A to try.
37880	41360	I am going to try with the help of these two gentlemen
41360	46960	to bring some clarity to the issue around GPT machines.
46960	51000	The advent of what looks like the first sparks
51000	53840	of artificial general intelligence.
53840	55680	I'm going to make some basic predictions.
55680	60040	And then I'm going to get into the scientific value
60040	61440	of these machines.
61440	64840	And that will be both positive and negative.
64840	68000	The philosophical value, the spiritual value.
68000	70240	And then my proposal, given all of that argument
70240	74040	and discussion about how we can best
74040	77640	respond to undertake the alignment problem.
77640	80240	So first of all, I'm going to ask the two gentlemen,
80240	82840	two friends of mine, two people that I have come
82840	89840	to appreciate, love, and rely on in increasing ways that
89840	93280	has only made my life and my work better.
93280	95600	So let's begin with Ryan.
95600	96400	Well, thanks, John.
96400	98360	And it's good for my heart to hear you say that,
98360	100560	because you have been such a wonderful influence in my life
100560	103040	as a YouTube student of yours and someone
103040	105680	who has experienced a lot of transformation from your work,
105680	108120	which has led me to be the executive director of the Reveque
108120	111840	Foundation, where we are working to help to scale and bring
111840	114000	about these solutions that you have
114000	115760	pointed to so well in your work.
115760	120120	And I also am the founder and run a technology services company
120120	121400	called Mainstay Technologies.
121400	124000	And so technology has been near and dear to my heart
124000	125000	and story forever.
125000	129560	And the intersection of the meaning crisis, the metacrisis,
129560	133880	and technology that is AI has me very fascinated
133880	135760	and been doing a lot of research on this
135760	138760	and eager for what you are about to propose and argue
138760	140920	here today.
140920	141800	Thanks, Ryan.
141840	143040	Eric.
143040	146000	Yes, my name is Eric Foster, media director over here
146000	149960	at the Reveque Foundation, much like Ryan,
149960	152960	knew you through before working alongside you
152960	156440	and continue to not only learn from you,
156440	159440	but learn now alongside you as we work together
159440	162280	on all the different videos and these essays
162280	165280	and the conversations and everything.
165280	167720	My interest in this primarily comes from,
167720	169440	and I actually have told both of you this.
169440	170840	I asked my mom the other day, I said,
170880	174120	when was the first time you heard me really combat AI?
174120	175840	What was the first time I started talking about it?
175840	177880	She said, I think you're about eight years old.
177880	181880	And for some reason, it's been a part of my life
181880	183560	the entire way through.
183560	189400	I'm now 30, so this is going on a long time of very shallowly
189400	192120	but continuously coming back to this idea.
192120	194640	For some reason, it gripped me when I was very young.
194640	199600	And I've been exploring all of the various different avenues
199600	201400	that it could potentially take ever since.
202600	203440	Thank you.
203440	204600	So the format's gonna be the following.
204600	208560	I'm going to go through sort of an argument per section
208560	212000	and then I'll open things up to take comments,
212000	215520	questions from both Ryan and Eric.
215520	218320	So the first thing I wanna do is talk about the predictions.
218320	221400	One of the things that are happening with the GPT machines
221400	223920	is we're getting a swarm of predictions
223920	226440	and many people are finding this challenging
226440	229640	because the predictions are quite varied.
229640	231520	Many of them are inconsistent with each other
231520	234200	or even challenge each other in a fundamental way.
234200	236360	I'm gonna try and propose
236360	241360	that we try to be more careful about the predictions.
242360	245840	We try to steer ground between hyperbolic growth predictions
245840	247040	that these machines are just gonna
247040	250480	hyperbolically accelerate in intelligence.
250480	253920	And that forks into two variations.
253920	255640	Utopia is just around the corner
255680	258760	or we are doomed to doomed, doomed forever doomed.
258760	262200	And so let's be a little bit more cautious.
262200	265160	I'll give you some reasons for that in a minute.
265160	269440	We also want to be steer between all of that,
269440	272680	both the positive and negative hyperbole
272680	275000	and then a kind of stubborn skepticism
275000	277280	that's digging its heels in and saying,
277280	279760	nope, this is not AGI.
279760	280960	It never will be.
280960	284160	This is decades and decades away.
284160	286560	And there are people making these arguments.
286560	289240	And I think that is also incorrect.
289240	291880	I think the attempt for whatever reason
291880	296880	to dismiss these machines is not proportioning
297240	299920	our evaluation to the reality
299920	302280	that they are actually presenting to us.
302280	307280	So what I hope is that we can get much more careful
308480	309880	and that getting more careful
309880	312160	about the predictions we're making
312160	315880	will also in conjunction with the arguments
315880	317320	and discussion we're gonna have,
317320	321720	allow us to, allow us, me maybe specifically,
321720	324200	to propose some important threshold points
324200	327080	that we have not yet met with these machines,
327960	331160	but that we can reasonably foresee,
331160	335280	not perhaps their timing, but why they are pivot points
335280	337240	and that these are points where we can make
337240	341200	fundamental decisions about how we wanna go forward,
341200	344320	especially in terms of the spiritual challenge
344320	346640	and the enlightenment issue.
346640	350800	So why do, why am I skeptical?
350800	353040	Not about the machines.
353040	354360	I'm critical, but not skeptical.
354360	357000	And that distinction is gonna be important throughout.
358000	360860	I am skeptical of jumping to conclusions
360860	362880	about hyperbolic growth,
362880	367240	since human beings are famous for jumping to conclusions
367240	368880	when they see hyperbolic growth.
368880	370400	And if you don't believe me,
370400	372320	just track the history of the stock market
372320	373160	or something like that.
373160	375680	And you'll can see that people can very often
375680	377100	get taken up by it.
378560	381840	What we can say is most often,
381840	386380	hyperbolic growth is found within self-organizing processes.
386380	389480	And when hyperbolic growth is within self-organizing processes
389480	391920	and the economy is a self-organizing process,
393200	396200	it usually is part of a larger pattern
396200	398520	called punctuated equilibrium.
398520	400400	You can see this also in the history of evolution.
400400	402680	There'll be, so after the asteroid hits,
402680	406080	there's just exponential speciation in geological time,
406080	408720	but time scales matter and we'll talk about that later.
408720	411740	And then it flattens off as the niches get filled,
411740	413680	as constraints emerge, as more.
413680	416160	So we don't know yet if this,
416160	417640	like when people just draw these graphs,
417640	420080	look at what's happened over the last five weeks, right?
420080	422440	It's like, yeah.
422440	425040	And then you need to remember we got similar predictions
425040	428440	about self-driving cars and the exponential growth
428440	430880	and soon all these people would be put out of work.
430880	435520	And that was like 2012, we're 11 years later
435520	437240	because we hit a plateau.
437240	439920	There was the exponential growth and we hit a plateau.
439920	444560	Now, I could be wrong, but the reasonable thing
444560	446740	is to be agnostic about the meaning
446740	450960	of this very low resolution measuring of exponential growth.
452840	454380	I mean, here's another example.
454380	457780	Consider if you were at the beginning of the 20th century
457780	461340	and measuring all the breakthroughs in fundamental physics
461340	463300	and you would see this exponential growth,
463300	464700	relativity, quantum mechanics.
465980	468380	And then it plateaus.
468380	471420	It plateaus and it's been 50 and more years,
471420	473980	but since we've had a significant breakthrough.
473980	477460	We don't know, we don't know.
477460	481580	And that is where we should properly stand about this.
481580	484240	So we have to, instead of making predictions
484240	486980	that are not well-warranted,
486980	491980	we should try and foresee plausible threshold points,
492140	494820	not predict necessarily their timing,
494820	498700	but foresee them and foresee them as our opportunities
498700	501660	to steer this in a powerful way.
501660	503740	This is the alternative I'm proposing.
506360	509340	See, as you get into exponential growth,
509340	512260	things are often disclosed that you did not foresee
512260	514340	within your normal framing.
514340	516940	Let me give you one more analogy on this point.
517940	519540	Traveling faster and faster.
521460	523020	Traveling, we can travel faster and faster.
523020	524700	And here's an exponential growth in our ability
524700	527100	to speed through the universe.
527100	531740	Well, as you do, micro particles become relevant
531740	534140	in a way they are never relevant for us
534140	535860	in our daily movement, right?
535860	537940	There's a video online of what happens
537940	541100	if a grain of sand hits the earth at the speed of light
541100	543020	because force equals mass times acceleration.
543020	544620	So it's accelerating to the speed of light
544620	547340	and it hits the earth and you have this titanic explosion.
547340	549020	This is why interstellar travel
549020	551140	might actually be impossible for us,
551140	553500	even if we get machines that accelerate us
553500	554820	towards the speed of light.
554820	556700	Because things that weren't constraints
556700	558900	can suddenly become constraints.
558900	562060	And of course, the speed of light constraint is also there.
562060	564620	For all the fiction around faster than light travel,
564620	566340	it's a real constraint.
566340	568420	Again, this is meant as an analogy.
568420	572820	We don't know what constraints will be revealed.
572820	576380	We don't and simply looking at a simplistic graph
576380	580060	is not taking that into consideration.
580060	582340	We are genuinely ignorant because,
582340	584900	as I will make clear as we go through here,
584900	588060	we really do not know how these machines
588060	590780	are producing these emergent properties
590780	592140	that they're producing.
592140	594660	And therefore, trying to draw something
594660	597220	like scientific predictions from ignorance
597220	599180	of the underlying mechanisms
599180	602740	is a seriously incautious thing to do.
602740	604540	So we have to pull back from that.
604540	606420	Now, I'm not saying we shouldn't try and foresee,
606420	608740	but notice my shift in language.
608740	610140	I'm shifting from prediction.
610140	613340	At this date, this will happen to foreseeing.
613340	614540	What's the foresight?
614540	617060	The foresight is can we foresee,
617900	620740	as we bring real explication to these,
620740	623460	can we foresee threshold points
623460	628140	where we can reasonably make a change?
628140	632500	I think we are in a kairos.
632500	635940	We are in a pivotal turning point in world history.
635940	638620	Maybe one of the greatest, maybe the greatest.
638620	640380	I don't know.
640380	641580	I would need to be godlike
641580	643300	to be able to make that pronouncement.
643300	646020	But unfortunately, I'm not,
646020	647820	which is something I'm also gonna talk about later.
647820	648780	I don't want to be a god.
648780	649980	I hope you don't either.
652380	654060	Imagine having a godlike ability
654060	656860	to remember for all of eternity all your failures.
656860	661860	I don't think that's an existence to be desired.
662180	666620	So, I think we need to be really careful.
666620	668180	I think we need to pay attention
668180	670740	to what we've seen in the history of science.
671780	673740	The past century has not been the century
673740	675540	of unlimited growth and knowledge.
675540	678220	In some ways, yes, more and more data, more and more information,
678220	683100	but you can be misled by just a quantitative approach
683100	685300	because if you pay attention to what's been happening
685300	688060	at a philosophical, epistemological,
688060	689860	having to do with the study of knowledge level,
689860	692580	what you've seen is this has been the century
692580	696740	of the accelerating discovery of intrinsic limits
696740	697940	on what we can know.
698820	700900	The realization that the Cartesian project
700900	705900	of unlimited knowledge is not actually a possibility.
708020	710740	It is reasonable to conclude,
710740	712260	again, I'm not speaking timing here.
712260	713180	I'm talking about trajectory,
713180	714460	but it's reasonable to conclude
714460	716620	that this trajectory will continue
716620	718840	and show itself in this project as well.
719720	722160	We are going to perhaps start to discover
722160	726600	the kinds of fundamental limits on mind
726600	729440	and its interaction with matter
729440	731640	that were not previously available to us
731640	734000	and that will be welcome.
734000	737120	But I think it's unlikely that the machines will just
737120	741120	in some simple exponential pattern grow.
742680	746120	One of the reasons I think this is because
746120	748640	there's an issue of what's called general system
748840	751600	collapse, this comes out of general systems theory.
752600	755120	And the place where we have evidence for this is in
757680	761440	civilizations, which represent very complex intelligence
761440	764360	within sophisticated distributed cognition
764360	766160	that's intergenerational in nature.
766160	769720	So this is a very powerful cognition at work.
769720	772040	I mean, and if you stop and think about it,
772040	774560	the GPT machines are basically just taking
774560	777920	that collective intelligence from distributed cognition
778080	782760	and putting it into a single automated interface for us.
782760	787560	So if you think the GPT machines are very intelligent,
787560	789920	you should think that civilizations are equally
789920	792440	that kind of super human intelligent.
792440	794520	That's a reasonable thing to conclude.
794520	796860	What do we know from the history of these civilizations?
796860	799200	They face general system collapse.
799200	800040	Why?
802200	805120	Let's take it that reality is inexhaustibly complex,
805160	808320	not just complicated, but complex, right?
808320	809800	And it contains real uncertainty,
809800	812000	not just risk real emergence.
812000	814600	And by the way, when you have real emergence,
814600	817000	you have real uncertainty, not just risk.
817000	820120	And all of these people are invoking real emergence
820120	822080	when they're talking about these machines.
822080	824600	So real emergence means real uncertainty.
824600	827360	It means real novelty, okay?
827360	832000	Now, so you place super human civilization intelligence
832000	834000	into a complex environment.
834000	835600	What do you see the system doing?
835600	838160	Becoming more and more complicated,
838160	840000	adding more and more components,
840000	844520	bureaucratizing itself in order to deal with problems.
844520	849200	But what you get to is you get this sort of fact.
850640	853360	As you linearly increase the number of problems
853360	855400	you're trying to deal with,
855400	857800	the number of interactions within your system
857800	859260	is going up exponentially.
860260	865260	So at some point managing yourself becomes as problematic
865860	868140	as any problem you're trying to solve in the environment.
868140	871620	And then the system gets, as it said, top heavy.
871620	875060	It gets over bureaucratized and it collapses.
875060	877180	Now this is a regular pattern,
877180	880620	regular reliable pattern for the super human intelligence
880620	882380	that we find in civilizations.
883300	885780	I do not think it's reasonable to conclude
885780	888820	that these machines will somehow just avoid
888820	892180	that problem of exponential growth
892180	897180	in complicatedness as they try to deal with real problems
901940	903660	that contain real uncertainty
903660	906420	that an inexhaustible environment is presenting to them.
907620	908780	Now, that doesn't mean I can say,
908780	910780	oh, well, they're never gonna surpass us.
910780	913060	John Vervecky is not saying that.
913060	914620	John Vervecky is well aware already
914620	916180	of things that can surpass him.
916180	919180	And like I said, it's very clear
919180	922380	that we have been relying on the super intelligence
922380	925860	of distributed cognition that is distributed
925860	929540	across people and across generations for millennia
929540	934540	as sort of an important source of normative guidance to us.
936100	938020	So these machines, it's not unreasonable
938020	939900	that they could reach that level.
939900	941720	And maybe, and we'll talk about this later,
941720	943980	having a different substrate,
943980	945860	the material they're built on
945860	947980	may allow them to go to different levels.
947980	949180	I do not think, though,
949180	954180	that they can just grow exponentially indefinitely.
954180	957820	I think that is, we have no good reason,
957820	961260	and I'm trying to make arguments here, for believing that.
961260	964260	And that means we can think about these machines,
964260	966820	however godlike they might be,
966820	971100	as being inherently still finite in a manner
971100	973500	that really matters to their cognition
973500	975860	and their attempts to make sense of themselves
975860	976700	and their world.
976700	977940	And that's gonna be an important linchpin
977940	979260	later on in my argument.
981220	983380	What's interesting is there's some evidence
983380	988380	that we are very close to all the trade-off points,
989580	991220	at least for biological beings.
992140	994420	For example, there's all these U curves
994420	995900	for like, if speed of transmission,
995900	998300	if you speed up the speed of neuronal transmission,
998300	1001500	we're at sort of the maximal, sort of the optimal,
1001500	1002380	because if you go too far,
1002380	1004140	you get into diminishing returns,
1004140	1006780	and the negative side effects start to manifest
1006780	1009780	faster than the gains, and also for more neurons.
1009780	1012220	And so I've seen some really good arguments
1012220	1014660	that we're sort of peak biology.
1016140	1018100	And that's very interesting, if you think about it.
1018100	1021980	That might be, of course, why we resorted to culture,
1021980	1024540	because culture allows us to supersede
1024540	1026460	the limits of peak biology.
1027460	1032460	We teeter on the edge of despair and madness.
1033660	1037860	And as these machines approach their own threshold,
1038860	1043860	they will plausibly also teeter on the edge of that.
1044180	1046580	And that is something we need to think about.
1046580	1048420	And I'll give you more precise reasons
1048420	1051180	as to why that is gonna become important.
1051180	1052420	It has to do with the increase,
1052420	1053900	we'll have to increasingly,
1053900	1055900	these machines will have to increasingly rely
1056020	1059260	on more and more pervasive disruptive strategies.
1059260	1061100	And so we'll come back to that.
1063980	1068980	All right, one more thing that I'm going to say about this is,
1071780	1074140	and I'll go into this more detail, the rationality.
1074140	1075180	Of course, we have to make,
1075180	1076780	and this is where the general system things
1076780	1078020	really starts to bite.
1078020	1080140	These machines have to become more self-monitoring
1080140	1084260	and self-directing in very powerful ways.
1084260	1085420	And then the problem with that
1085420	1088020	is if you make this system as powerful as this system,
1088020	1090180	then you get into an infinite regress.
1090180	1093100	One thing you don't want is a large language model,
1094900	1097900	making all the hallucinations and repetitive actions
1097900	1101620	and weirdness, trying to evaluate a lower order LLM
1101620	1104300	that is making all kinds of hallucinations repeated,
1104300	1106020	because then you just get an infinite regress.
1106020	1108220	So you have to properly have this,
1108220	1110140	the heuristics operating at this level
1110140	1112580	to be different in kind than this level.
1112580	1117300	And that also gets you into a diminishing return issue,
1117300	1119980	because at some point, you don't want this
1119980	1122340	to become as complex as this.
1122340	1123740	And think about it already.
1124780	1127020	I mean, these machines have hundreds of billions
1127020	1128660	of parameters.
1128660	1129620	Do you know what it's like to try
1129620	1132020	and track a hundred billion parameter system?
1132020	1135620	You know one of the things that this machine
1135620	1138580	that probably has more than a hundred billion parameters
1138580	1141780	can't do very well is track a hundred billion parameters.
1142780	1144420	And so just thinking that,
1144420	1147260	oh, we can just stack these on top of each other,
1147260	1149180	I think it's also overly simplistic.
1149180	1151740	We're facing those real trade-off relations,
1151740	1153300	there's real problems there.
1153300	1156980	And again, that means that these machines
1156980	1161980	are going to be finite in a very important way,
1162540	1165140	and they will confront presumably the issues
1165140	1168540	around finitude that will be analogous to ones we have.
1168580	1172820	Now, I wanna stop here, I'm not finished this section,
1172820	1175940	but I wanna make clear, all these gaps
1175940	1179740	in the GPT machines do not take them.
1179740	1182420	I'm not offering them as any grounds
1182420	1184740	for dismissive skepticism.
1184740	1188780	I'm confident that we can approach these limits
1188780	1191420	and we'll continue to make progress,
1191420	1192580	and I'll point some of the things
1192580	1193420	that are already happening.
1193420	1195860	That's not why I'm doing this.
1196860	1201860	What I'm doing this for is I want to show
1202860	1206860	that these machines are not yet fully intelligent.
1206860	1207860	Nobody really thinks that,
1207860	1209860	they talk about sparks in the beginnings,
1209860	1213860	but I wanna unpack that common claim.
1213860	1215860	It's unlikely that they're currently conscious.
1215860	1219860	And what that means is we face thresholds
1219860	1224860	about qualitatively improving, not just quantitatively,
1224860	1226860	qualitatively improving their intelligence,
1226860	1229860	possibly making them self-conscious,
1231860	1233860	rationally reflective, et cetera.
1233860	1235860	And that's what I'm most interested in.
1235860	1238860	What are the threshold points we can get to?
1238860	1239860	How can we make them plausibly?
1239860	1241860	So if we just give up, oh!
1241860	1244860	Right, and no, no, no, there's gonna be trade-offs,
1244860	1245860	there's gonna be limitations,
1245860	1248860	there's gonna be all kinds of stuff.
1248860	1251860	And then from that, we can pick off,
1252860	1254860	okay, here are plausible threshold points,
1254860	1258860	and then we can more finely tune our response
1258860	1260860	to the alignment issue.
1260860	1262860	That's why I'm doing this.
1262860	1265860	I am very impressed by these machines.
1265860	1268860	I think it is very reasonable to conclude
1268860	1271860	they are going to significantly alter,
1271860	1272860	I said it, they're a chyrus,
1272860	1276860	they're gonna significantly alter our society
1276860	1277860	and our sense of self.
1278860	1281860	They are gonna pour, you know,
1282860	1287860	meth and, you know, fuel on the fire of the meaning crisis.
1288860	1291860	And that is something I think we need to take into account.
1291860	1296860	That will tempt us to respond inappropriately
1296860	1299860	to what these machines are presenting to us.
1299860	1304860	And that leads me to some final sort of societal predictions.
1305860	1308860	I think there's gonna be multiple social responses.
1308860	1309860	And as I said,
1309860	1312860	I'm worried about the accelerant of the meaning crisis
1312860	1315860	tempting us towards inappropriate ones.
1316860	1320860	So one is nostalgia,
1320860	1324860	people longing for the time before the machines,
1324860	1329860	longing passionately and deeply for the golden age
1329860	1331860	that you did not realize that 10 years ago
1331860	1332860	you were in the golden age,
1332860	1334860	but 10 years from now you'll be hearing
1334860	1336860	you were in the golden age,
1336860	1340860	that wonderful time when there wasn't GPT
1340860	1344860	or whatever AGI takes its place.
1344860	1347860	Nostalgia will grow.
1347860	1351860	Alongside of it though will be resentment and rage
1351860	1353860	as people are disenfranchised.
1353860	1355860	So Louis XIV, what?
1355860	1356860	Just hang on.
1356860	1360860	Louis XIV, when he grew up as a young kid,
1361860	1365860	the nobility staged a coup and he remembered that
1365860	1367860	and he vowed that when he became king
1367860	1370860	he would crush the nobility
1370860	1372860	and become an absolute monarch,
1372860	1374860	an absolute king, the sun king.
1374860	1377860	Let's say I am the state.
1377860	1379860	That's Louis XIV.
1379860	1382860	In crushing the nobility
1382860	1388860	he disenfranchised a whole segment of the population
1388860	1392860	that had traditions of power, traditions of decision,
1392860	1396860	was highly intelligent because they generally ate better,
1396860	1399860	highly educated because they had access to education
1399860	1402860	and they were disenfranchised.
1402860	1404860	That's a bad idea
1404860	1409860	because that is the basis for the beginning of revolution.
1409860	1413860	So I'm saying that now to the people who hold power.
1413860	1416860	I'm talking to all of you right now.
1416860	1418860	You who think, aha!
1418860	1421860	Yes, 95% of the people are going to be driven
1421860	1424860	but I will become a sun king.
1424860	1427860	I will be careful.
1427860	1432860	You are lighting the fires of a revolution in a kairos time
1432860	1435860	and thinking that you will be protected from those flames.
1435860	1437860	I think is foolishness.
1440860	1442860	What else is going to happen?
1442860	1446860	I think that combination of nostalgia, resentment and rage
1446860	1450860	will have multiple religious consequences.
1450860	1455860	One, and religion is going to figure in a lot of what I'm talking about today,
1455860	1460860	one of those is what you get when you mix nostalgia with resentment and rage.
1460860	1462860	You get fundamentalism.
1462860	1465860	Fundamentalisms are going to rise
1465860	1470860	and they are going to be increasingly apocalyptic fundamentalisms.
1470860	1475860	Fundamentalism and apocalypse go so nicely together.
1475860	1478860	They really, oh, apocalypse, fundamentalism.
1478860	1480860	Oh, I love you. I love you too.
1480860	1482860	That's what's going to happen.
1482860	1489860	And so we have to think about how that can shade off into a kind of escapism.
1489860	1491860	I don't have to worry about this.
1491860	1493860	God will come.
1493860	1495860	This is just the antichrist, et cetera.
1495860	1498860	Now, I'm going to say one thing to my Christian friends
1498860	1501860	and I want you to take this really seriously.
1501860	1504860	For those of you who believe in that,
1504860	1506860	I hope you're right.
1506860	1509860	I really honestly do.
1509860	1514860	But I want you to consider the fact that there have been multiple times,
1514860	1517860	kairases, where God has been silent.
1519860	1523860	I suspect that is very possible now.
1529860	1533860	Another thing that's going to happen is cargo cult worship of this AI.
1533860	1535860	Sorry, I need an apology.
1535860	1537860	I forget the author of this article I read.
1537860	1540860	He didn't specifically make this argument, but it overlaps.
1540860	1543860	And he has definite providence and precedent.
1543860	1545860	I just forgot his name. I'm sorry.
1545860	1548860	But he's got an article about how people will probably start worshiping these AIs.
1548860	1550860	And I think that's the case.
1550860	1553860	We'll have a cargo cult around these emerging AIs.
1553860	1555860	What's a cargo cult?
1555860	1559860	In World War, the Americans flew in to the islands in the Pacific,
1559860	1565860	all kinds of cargo, all these goods that the indigenous people
1565860	1567860	found wonderful and amazing.
1567860	1569860	And this stuff is just landing from the sky.
1569860	1573860	And then the war was over and the Americans left.
1573860	1578860	And the indigenous people started building out of wood what looked like airplanes
1578860	1584860	in building runways because they were trying to get the miraculous airplanes
1584860	1587860	to return and dispense their wonderful cargo.
1587860	1592860	So I mean cargo cults around the cargo that these AIs can dispense to us.
1592860	1594860	I think that's a very reasonable possibility.
1594860	1598860	And I think that is also a very dangerous path to go down
1598860	1604860	because that will actually distract us from the hard work that we need to do
1604860	1608860	in order to properly address the alignment problem.
1608860	1611860	There's going to be a lot of spiritual bypassing,
1611860	1614860	which is I am spiritual, it doesn't matter.
1614860	1618860	A lot of escapism, drugs, pornography, et cetera.
1618860	1623860	Tragic disillusionment. Tragic disillusionment.
1623860	1628860	And that's going to exacerbate the meaning crisis.
1628860	1630860	Then one more thing, and there's a fork here,
1630860	1634860	and this is around identity politics, left and right.
1634860	1636860	I'm not taking a side here.
1636860	1638860	I'm talking about the whole framework.
1638860	1640860	I think it's identity politics.
1640860	1642860	One fork will be identity politics is swept away
1642860	1645860	by the greatest threat to human self-identity that has ever existed,
1645860	1647860	and it's happening right now.
1647860	1650860	And all of the differences that we have been promoting as
1650860	1653860	are going to pale by the fact that we need to get together
1653860	1656860	and get our shit together if we're going to really address
1656860	1660860	the real threat to what human identity really means.
1660860	1661860	That's one fork.
1661860	1664860	The other fork is people will double down,
1664860	1669860	double down as we adopt a fundamentalism about identity politics.
1669860	1674860	I predict that that will be incapable of giving us
1674860	1679860	any significant guidance about how we reconstruct human identity
1679860	1684860	and our self-understanding in the face of the advent of AGI.
1684860	1686860	Okay, so that's the first section.
1686860	1689860	I want to open myself up to reflections, comments,
1689860	1693860	challenges, questions, et cetera.
1693860	1698860	Well, I appreciate, John, how you immediately sort of
1698860	1701860	helped us break frame of this is not just a technology
1701860	1703860	that's going to have normal adoption strategies.
1703860	1705860	This is something fundamentally other.
1705860	1708860	This is something that is going to have massive disruptive strategies.
1708860	1711860	We're also moving it off of the quasi-religious grounds
1711860	1713860	that I hear this talked about so often,
1713860	1717860	where the singularity has this mythical power to it
1717860	1720860	that's calling that we will suddenly be able to transcend the laws of physics
1720860	1722860	and know the answer to everything,
1722860	1724860	and it will answer whether God is real
1724860	1728860	and as if we can reach some point where all of that happens
1728860	1732860	and you're clearly setting finitude around this
1732860	1734860	while encouraging us to really wrestle
1734860	1737860	with the rapid acceleration that we're facing.
1737860	1739860	Yes, I think that is very well said.
1739860	1744860	I think the danger and sort of the market and the state
1744860	1746860	are doing exactly the framing that you're pointing to.
1746860	1748860	Well, this is a technology,
1748860	1751860	and this is how we better figure out how to use it better
1751860	1752860	and all that sort of thing.
1752860	1754860	No, no, no, no, no.
1754860	1756860	In one sense, it's a technology,
1756860	1762860	but that is to emphasize the wrong aspect of these entities
1762860	1764860	in a fundamental way.
1764860	1766860	Yes, I agree about that.
1766860	1768860	I think deeply.
1768860	1771860	That's one point I want to get across very, very clearly.
1771860	1775860	There's a sense in which even that technological framing
1775860	1778860	is something we're going to have to challenge more comprehensively
1778860	1781860	about ourselves and our relationship to the world.
1781860	1784860	Eric, you wanted to say something.
1784860	1786860	The thing that stands out to me right away,
1786860	1789860	and I'm kind of trying to, because it's natural,
1789860	1791860	but also because I think it's necessary,
1791860	1795860	I'm trying to take the perspective of whatever audience it is
1795860	1797860	that is going to be listening to this.
1797860	1800860	And I really appreciate just the framework
1800860	1802860	that you're putting around this whole conversation
1802860	1806860	because I think that there's so much doom and gloom currently already.
1806860	1809860	There's so much, oh, AGI, it's going to happen eventually.
1809860	1812860	The current AI chatbots that we have, these language models,
1812860	1816860	they're so useful, they're going to become our new God.
1816860	1821860	The way that I make $1,000 a minute suddenly, all of a sudden.
1821860	1825860	And I think that there's going to be a continued need,
1825860	1828860	and I think in this instance in particular,
1828860	1832860	there's an even greater need to force nuanced conversation
1832860	1833860	around these things.
1833860	1837860	And so I'm so glad personally, just me as a human being,
1837860	1840860	I'm so glad that you're putting this much thought into it
1840860	1843860	in all of the different areas that you are,
1843860	1847860	and combining multiple domains to not only reach multiple people,
1847860	1851860	but also to show how big of a potential,
1851860	1855860	well, just how world-changing that this can potentially be.
1855860	1860860	Not in a doom and gloom or a cargo cult sort of way.
1860860	1865860	And I think that as we continue to, as these technologies continue to grow,
1865860	1869860	the need for that nuance will grow more and more strongly.
1869860	1875860	So I'm happy personally to see you, in my opinion, leading this nuance.
1875860	1878860	Well, thank you for that.
1878860	1882860	So before I go into the scientific value of the GPT machines,
1882860	1886860	I want to just set a historical context.
1886860	1889860	And this will, I want people to hold this in the back of their mind
1889860	1893860	also for the philosophical and spiritual import of these machines.
1893860	1896860	What's the historical context?
1896860	1899860	So I'm going to use the word enlightenment, not in the Buddhist sense.
1899860	1901860	I will use it in the Buddhist sense later.
1901860	1906860	I'm using it in the historical sense of the period around the scientific revolution,
1906860	1909860	the reformation, all of that, the enlightenment,
1909860	1915860	and the degeneration of secular modernity, and all of that.
1915860	1920860	That era is now coming to an end.
1920860	1927860	See, that era was premised on some fundamental presuppositions
1927860	1929860	that drove it and empowered it.
1929860	1930860	And this is not my point.
1930860	1932860	This is a point that many people have made.
1932860	1940860	This sort of Promethean proposal that we are the authors and tea loss of history.
1940860	1945860	And that's passing away.
1945860	1949860	And it's done something really odd.
1949860	1956860	Like, wait, we did all this, made all this progress to come to a place where we will,
1956860	1958860	technology wouldn't make us into gods.
1958860	1964860	It will make us the servants or make us destroyed by the emerging gods.
1964860	1965860	What?
1965860	1967860	What aren't we the authors?
1967860	1968860	Aren't we the tea?
1968860	1974860	Isn't this all about human freedom?
1974860	1977860	In fact, I think it's not just an ending.
1977860	1981860	There's a sense in which there's, for me, I don't know how many people share this,
1981860	1983860	so it's an open invitation.
1983860	1985860	There's a sense of betrayal here.
1985860	1991860	I mean, one of the things the enlightenment did was to tell us to stop being tutored
1991860	1992860	and educated by religion.
1992860	1994860	It said religion is a private matter.
1994860	1995860	Go there, do your thing.
1995860	1999860	But the way you should be educated, brought up, become a citizen, blah, blah, blah,
1999860	2001860	free from religion.
2001860	2003860	And of course, there's been all kinds of benefits for that.
2003860	2009860	But notice the irony here is that one of the things religion taught us how to do,
2009860	2015860	or what is to, how to enter into relationship to beings that were greater than us.
2015860	2020860	I mean, Plato didn't have any problem being Plato when he believed that there were gods,
2020860	2026860	that were like, Socrates clearly thought his wisdom was a paltry thing in comparison to the gods.
2026860	2032860	Those people knew how to live with beings that were super intelligent.
2032860	2037860	And how to nevertheless craft human lives of deep meaning within it.
2037860	2040860	We should pay attention to that example, and I'll come back to it later.
2040860	2047860	But in losing religion, we lost the place where we learned the manners of dealing with that,
2047860	2050860	which transcends us, the manners and the virtues.
2050860	2057860	And that's really odd because the enlightenment has also denuded us of the traditions that might give us some
2057860	2062860	guidance on how we could think about relating to these machines.
2062860	2067860	So I think the time of the enlightenment and modernity is coming to an end.
2067860	2069860	There were already signs of that.
2069860	2075860	Postmodernity and other things have been already showing that.
2075860	2082860	But I think this is going to be even more of a severance for us from that.
2082860	2083860	And that's very important.
2083860	2086860	So please keep that context in mind.
2086860	2087860	All right.
2087860	2090860	The scientific value of the GPT machines.
2090860	2093860	This is going to be, and when I say value, I mean both positive and negative.
2093860	2100860	I'm using value as an unmasked term.
2100860	2104860	So positive value, scientific value.
2104860	2108860	I think this is the beginning of a real solution to the silo problem.
2108860	2110860	And that's a scientific advance.
2110860	2111860	What's a silo problem?
2111860	2118860	Another problem is that our deep learning machines, our neural networks, have typically been single domain, single problem solvers.
2118860	2121860	This machine is really good at playing go.
2121860	2122860	Can it swim?
2122860	2123860	No.
2123860	2124860	Right?
2124860	2126860	And we were very different from them.
2126860	2137860	And that's an important difference because I think AGI doesn't make any sense unless it's in a general problem solver that can solve many problems in many domains.
2137860	2140860	And what's being opened up by the machines is the real possibility of this.
2141860	2148860	I'm not saying it's a complete solution, but we have clear evidence that the silo problem is being solved.
2148860	2157860	Now, what's really interesting about that, and this is an argument that myself and other people are making, is we're basically saying that we need hybrid machines.
2157860	2163860	We need sort of neural networks doing the deep learning, and then we need something that's a very language of thought.
2163860	2167860	This is what these large language models are.
2167860	2169860	And we're doing even more.
2169860	2175860	If the hugging things comes through, we've got this kind of AI and this kind of AI, and we're clutching them all together.
2175860	2181860	And it turns out that that's a confirmation of a lot of prediction and argumentation.
2181860	2188860	Some of that I made that there are different strengths and weaknesses between language-like processing and non-language-like processing.
2188860	2190860	This is a theme of my work.
2190860	2197860	And that these machines are showing, well, we actually have to address both if we want to create general intelligence.
2197860	2203860	And I think that's a big admission for multiple kinds of knowing.
2203860	2209860	I think that's taking it that way, I think, is a very reasonable conclusion to draw.
2211860	2218860	Now, what these machines do demonstrate is the insufficiency of purely propositional knowing.
2218860	2223860	And for those of you who don't know the kinds of knowing I'm going to talk about, I can't go into it in great detail.
2223860	2227860	We're going to put some links to some of the things here so you can go to it in detail.
2227860	2234860	But I want to give you one clear example of this that I don't think is in any way controversial.
2234860	2239860	So you can ask GPT-4 to spit out ethical theory for you.
2239860	2242860	What's utilitarianism? What's the ontological ethics?
2243860	2247860	Can you make an argument against Singer's argument for utilitarianism?
2247860	2250860	Oh, here's a counterargument. Very good, very good, very good.
2250860	2255860	All the propositional expertise that Leibniz had wet dreams about.
2255860	2259860	Oh, and you know what? That doesn't make these machines.
2259860	2262860	One iota, moral agents.
2265860	2271860	Think about that. Think about what we now have evidence for, right?
2273860	2280860	This shows the radical insufficiency of propositional knowing for personhood.
2280860	2285860	If we take it that a proper part of being a person is moral agency.
2285860	2288860	It's lacking that.
2291860	2299860	So, one way of thinking about this that may be helpful is a distinction from in psychology.
2299860	2302860	A distinction that I'm actually critical about, but it's helpful.
2302860	2305860	There's a distinction between, when people do work on intelligence,
2305860	2310860	there's a distinction between crystallized intelligence and fluid intelligence.
2310860	2314860	So crystallized intelligence is you're knowing how to use your knowledge.
2314860	2318860	And this can have highly powerful and emergent properties,
2318860	2322860	because you can connect things that you know in new and powerful ways.
2322860	2328860	And I think it's abundantly clear that there's a lot of emergent crystal intelligence,
2328860	2331860	crystallized intelligence in this machine.
2331860	2335860	It probably doesn't have what's called fluid intelligence,
2335860	2342860	because fluid intelligence has to do a lot with attention, working memory, consciousness,
2342860	2348860	and your ability to sort of dynamically couple well to your environment.
2350860	2353860	I think I'm going to argue this a little bit more detail,
2353860	2358860	but I think it's unreasonable to conclude right now that these machines have perspectival knowing.
2358860	2359860	Now, let's be careful about this.
2359860	2362860	Can they generate all kinds of propositional theory about perspectives?
2362860	2363860	You bet.
2363860	2367860	They have crystallized intelligence, but keep the analogy to moral reasoning.
2367860	2371860	That's not the same thing as being able to take up a perspective,
2371860	2377860	have genuine salience landscaping, and bind yourself to it in a very powerful way.
2377860	2381860	And I'll come back for why I think the machines currently lack that.
2381860	2383860	Remember the word currently.
2387860	2391860	Another thing that has been a significant scientific benefit.
2391860	2395860	And because I'm talking about science, I'm sort of saying predictions that I made,
2395860	2397860	because that's what scientists are supposed to do.
2397860	2398860	It sounds self-promotional.
2398860	2400860	And to some degree, perhaps it is.
2400860	2404860	Perhaps I'm within all the nestings of my thinking.
2404860	2408860	John Vervecchi is saying, but I'll remain important even when these machines are gone.
2408860	2409860	Maybe that's happening.
2409860	2410860	I hope not.
2410860	2412860	So I hope the argument stands on its own.
2412860	2422860	This is clear evidence, and I have argued for this, that generating intelligence does
2422860	2426860	not guarantee that you will generate rationality.
2426860	2433860	In fact, what is very possible is as you increase intelligence, you will increase its capacity
2433860	2434860	for self-deception.
2434860	2437860	And we're seeing this in these machines in space.
2437860	2443860	The hallucinations, the confabulation, the lying and not caring that it lies.
2443860	2445860	All of this.
2445860	2449860	And note that there's been a couple of people that have pointed out as they've tried to put in safeguards
2449860	2454860	to limit the hallucinations, the speed has actually, the machine has actually slowed down
2454860	2462860	compared to 3.5, hinting that some of those trade-offs might actually already be coming in place.
2462860	2464860	Don't know, but these are empirical questions.
2464860	2471860	So let's pay attention to the empirical evidence as it unrolls and try to calibrate what we're
2471860	2473860	saying as closely as we can.
2473860	2479860	But it's very clear that these machines, just by making them more intelligent, will not make
2479860	2482860	them more rational.
2482860	2486860	And we had every good reason to believe that because that's the case for us.
2486860	2493860	There is no contradiction in human beings being highly intelligent and highly irrational.
2493860	2497860	The predictive relationship between our best measures of intelligence and our best measures
2497860	2499860	of rationality is 0.3.
2499860	2503860	Now, 0.3 is not nothing, but it's not one.
2503860	2504860	It's not one.
2504860	2511860	70% of the variance is outside of intelligence.
2511860	2513860	And so we're seeing that in these machines.
2513860	2514860	Now, why are you saying that?
2514860	2520860	Because that's going to be the basis for my philosophical argument, which is about rationality.
2520860	2521860	Rationality.
2521860	2522860	All right.
2522860	2526860	Now, a question that a lot of you have sort of posed to me is, but what does this say
2526860	2528860	about relevance realization?
2528860	2533860	Again, I'm not going to repeat everything I've said about relevance realization.
2533860	2535860	We'll put links to publications.
2535860	2537860	We'll put links to videos.
2537860	2538860	This is there.
2538860	2544860	The basic idea here is the general ability of general intelligence is the ability to zero
2544860	2550860	in on relevant information, ignore irrelevant information and do that in an evolving, self-correcting
2550860	2551860	manner.
2551860	2554860	And like I say, I'm not going to try and justify that claim right now.
2554860	2556860	I have a lot out there.
2556860	2562860	And I invite people to take a look at it.
2562860	2563860	What is this?
2563860	2566860	What do these machines say about that theory?
2566860	2573860	Well, one thing we know, and look at the 2012 paper with Tim Wildercrop and Blake Richards,
2573860	2579860	we pointed out something that was emerging in deep learning about relevance realization.
2579860	2586860	And therefore, there is an important dimension of relevance realization, recursive relevance
2586860	2591860	realization that is being massively instantiated in these machines, which is the compression
2591860	2596860	particularization function of deep learning and doing it in this multiply recursive fashion.
2596860	2602860	And so, of course, that dimension of relevance realization is going to be important.
2602860	2608860	And because of its recursivity, we're going to see it have emergent aspects to it.
2608860	2613860	So I think this is actually a significant confirmation.
2613860	2620860	More interestingly, at the end of last year with Brett Anderson and Mark Miller published
2620860	2626860	a paper talking about the integration of relevance realization and predictive processing as the
2626860	2628860	best way to get intelligence.
2628860	2633860	Now, interestingly enough, these machines show that because they have deep learning running
2633860	2637860	that dimension of relevance realization and then they have predictive process.
2637860	2641860	What the LLM models are, they are predictive processing machines.
2641860	2643860	That's exactly what they are.
2643860	2646860	Now, the problem is that they're limited.
2646860	2651860	They are predicting the relationship between lexical items.
2651860	2654860	And broadening that is probably going to be a challenge.
2654860	2663860	So although there is an integration of a dimension of RR and a specific version of PP that predicts
2663860	2667860	very powerful intelligence, there's also inherent limitations.
2667860	2671860	Still, what are those?
2671860	2675860	There's a lot of dimensions of relevance realization that are outside the compression
2675860	2681860	particularization processing that are deep learning that are probably also going to be
2681860	2683860	needed for genuine intelligence.
2683860	2685860	There's Explore, Exploit trade-offs.
2685860	2690860	There are trade-offs between monitoring your cognition and tasking in the world.
2690860	2692860	You don't go through all of these.
2692860	2694860	And they're always trade-off relationships.
2694860	2698860	You're always trading between them because as you advance one, you lose on the other.
2698860	2701860	And you're always pulling between them.
2701860	2706860	There's deep trade-offs between trying to make your processes more efficient and make them more resilient
2706860	2709860	so they have a kind of adaptive responsiveness to the environment.
2709860	2712860	All of this is still not in these machines.
2712860	2714860	Does that mean we can't put it in the machines?
2714860	2715860	No.
2715860	2717860	Unfortunately, that work is already out there and published.
2717860	2720860	That possibility is there.
2720860	2725860	However, I think most, no, I don't want to put a quantitative word.
2725860	2729860	Some significant dimensions of relevance realization are missing.
2729860	2734860	And having a generalized form of predictive processing is genuinely missing.
2734860	2744860	So that's my reason for saying that there's not a lot at this point that's
2744860	2746860	new about, theoretically new.
2746860	2749860	This is not a scientific advance.
2749860	2755860	These ideas were largely already pre-existent and in the literature.
2755860	2758860	Now, again, that doesn't mean, oh, well, then we're not going to pay attention to that.
2758860	2762860	That's not what I'm saying.
2762860	2766860	But let's push on this point.
2766860	2773860	I want you to notice how much these machines presuppose relevance realization
2773860	2775860	rather than explaining it.
2775860	2777860	What do you mean, John?
2777860	2782860	Well, these machines rely on the fact that we have encoded into the statistical probability
2782860	2785860	between terms epistemic relations of relevance.
2785860	2788860	We don't generate text like randomly.
2788860	2793860	What have we figured out with literacy and previously with languages?
2793860	2797860	Wait, you know these epistemic relations of relevance between my ideas?
2797860	2800860	I can encode them in probabilistic relationships between terms.
2800860	2802860	And then we can get really good at it.
2802860	2806860	First, we have hundreds of thousands of years of evolving language and then we have all of this
2806860	2812860	civilizational level work on literacy to get that correlation between relevance,
2812860	2816860	let's call it epistemic relevance and merely statistical relevance,
2816860	2819860	to get that correlation really tight.
2819860	2824860	That's different than a chimpanzee moving around in a forest.
2824860	2827860	Really different, right?
2827860	2834860	So first, we, it presupposes, it relies upon us encoding that relevance realization.
2834860	2840860	Secondly, it relies on us encoding relevance realization and how we curate and create databases,
2840860	2846860	how we create labeled images for the new visual processing that's coming on.
2846860	2852860	And how we organize access to that knowledge on the internet.
2852860	2854860	The internet is not random.
2854860	2862860	It's organized as a multi-layered small world network because it's organized by human attention
2862860	2865860	and what human beings find relevant and salient.
2865860	2870860	And then finally, and don't put too much on this but also don't ignore it,
2870860	2875860	the reinforcement learning that is driving this is modified, it's human assisted.
2875860	2880860	Human beings are in the loop making important judgments that help fine tune.
2880860	2882860	They're filled under fine tuning, right?
2882860	2885860	The judgments of relevance of this machine.
2885860	2886860	Now what does that mean?
2886860	2887860	You say, so what John?
2887860	2889860	The machines can still do that.
2889860	2891860	I'm not denying the technological success.
2891860	2899860	What I'm saying is the degree to which they are presupposing relevance realization
2899860	2903860	is the degree to which they are not explanatory of it.
2903860	2908860	This is not an explanation of intelligence.
2908860	2911860	This is not.
2911860	2912860	It won't generalize.
2912860	2920860	As I mentioned, what's happening in this machine doesn't generalize to the chimpanzee at all.
2920860	2927860	It might not even generalize to us.
2927860	2928860	Why?
2928860	2929860	Why do I say that?
2929860	2931860	Because it's weird.
2932860	2940860	Is it Stuart Russell recently released a thing about several generations beyond AlphaGo.
2940860	2943860	It's not GPT, but it's the same deep learning process.
2943860	2946860	You had AlphaGo that could beat any human go master.
2946860	2950860	And then generations beyond that, so like levels above.
2950860	2953860	And then the human beings noticed.
2953860	2954860	They just noticed.
2955860	2965860	They noticed that there's pretty clear evidence the machine didn't have the concept of a group,
2965860	2967860	like this basic idea of a group of stones.
2967860	2969860	And then they said, OK, that's the case.
2969860	2973860	Here's a very simple strategy you could use to beat any of these machines.
2973860	2981860	They took a middle range go player, gave them the strategy, gave the machine a nine stone advantage,
2981860	2986860	and the human being regularly and reliably beat the go machine.
2986860	2989860	Now, what some of you are saying, oh, well, we'll figure out how to fix it.
2989860	2990860	Yes, she will.
2990860	2991860	I don't doubt that.
2991860	2992860	But that's not the point.
2992860	2996860	The point is the machine didn't do that self correction.
2996860	2999860	There's a lot missing.
2999860	3002860	Other weirdness, like the, oh, the visual recognition.
3002860	3004860	Well, there's been a problem.
3004860	3008860	We get these visual recognition machines and, oh, well, that's a dog.
3008860	3009860	That's an elephant.
3009860	3011860	That's a man sitting on a picnic table.
3011860	3012860	Wow.
3012860	3014860	Wow, that's, whoa, human level.
3014860	3020860	OK, now what I'm going to do is take the same picture and scatter in some insignificant perceptual noise.
3020860	3022860	Human beings won't even notice it.
3022860	3025860	Alter a fraction of the pixels.
3025860	3032860	Then the machine, you show it the picture of the picnic table and the man and they say, oh, that's an iceberg.
3032860	3038860	You get this weird freaky thing that comes out of that.
3038860	3044860	Another thing, this is one of the most strongest results, reliable results about human intelligence.
3044860	3046860	It forms a positive manifold.
3046860	3048860	This is what Spearman discovered.
3048860	3055860	This is how we came up with the idea of general intelligence, namely how you do on this task, how you do in art.
3055860	3059860	Contrary to what people is highly predictive of how you'll do in math and how you'll do in history.
3059860	3060860	Like that's what he found.
3060860	3064860	How you do on any of these different tasks is highly predictive of each other.
3064860	3066860	You form a positive manifold.
3066860	3068860	That's your general intelligence.
3068860	3070860	That points to a central underlying ability.
3070860	3073860	I happen to argue that it's relevance realization.
3073860	3077860	But putting that aside, notice this.
3077860	3082860	So GPT-4 can score in the 10th percentile of the Harvard Law exam.
3082860	3085860	Whoa, that's really high IQ.
3085860	3095860	But then, and I didn't do this, somebody gave my most recent talk at the Consilience Conference to GPT-4 and asked it to summarize and evaluate.
3096860	3104860	And then I also gave this to an academic colleague of mine to evaluate GPT-4's response so I made sure it wasn't just me.
3104860	3108860	And it's about grade 11 as an answer.
3108860	3112860	And it's like, why is it brilliant?
3112860	3117860	For human beings, there would be a very strong positive manifold between those.
3117860	3121860	But there's a lot of heterogeneity in this machine.
3121860	3126860	So it clearly doesn't generalize to the chimp.
3126860	3134860	It may not generalize to us, which means it suffers from the kind of failure that destroys any good scientific theory, which is it fails to generalize.
3134860	3139860	I don't think a good scientific theory is available to us because of this.
3139860	3143860	Now, you may say, but what about, I'm going to come to the philosophical and spiritual significance.
3143860	3148860	Right now, we're playing the science domain and I'm trying to answer the science questions.
3148860	3154860	We need to get clear about these and not mix them up together and confound them and run back and forth in an equivocal manner.
3154860	3163860	Let's be clear about each one and then put them back together very, very carefully, very, very carefully.
3163860	3175860	Because I think, and I won't go into detail for this explanation, because I think these machines don't really have recursive relevance realization in a deep enough way.
3175860	3190860	And because I think there's a lot of converging arguments that the function of the four fold of attention and working memory and fluid intelligence and consciousness is relevance realization in ill defined novel complex situations.
3190860	3193860	I think it's highly unlikely that these machines have consciousness.
3193860	3201860	I think the fact that they have no reflective abilities means it's virtually the case that they do not have self consciousness.
3201860	3208860	I think about, you know, how the machine is thinking like that is like how the machine is feeling or something like that.
3208860	3212860	I think that is premature.
3212860	3216860	Is it possible that we could get there? Yes, it is. It is.
3216860	3220860	But you see what's happening by getting clear about what has happened scientifically.
3220860	3230860	We can start to see what are the future threshold points that we will be confronting and how can we be foresightful about them.
3230860	3232860	One more thing.
3232860	3236860	And people are endlessly arguing about the Chinese room.
3236860	3242860	And since they were endlessly arguing about the Chinese room argument, I won't go into it. And if you don't know about it, don't worry about it.
3242860	3249860	Before the GPT machines, I don't think this argument is going to satisfy anybody.
3249860	3255860	I have taken a look at the best attempts to give a naturalistic account of semantic information.
3255860	3258860	And we need a scientific distinction here.
3258860	3263860	There's a distinction between technical information, which is what is involved in what's called information theory.
3263860	3270860	And all that basically is, is a relation of statistical relevance that rules out alternatives.
3270860	3272860	That's the Shannon and Weaver notion.
3272860	3277860	So in that sense, without there being any sentient beings, there is a ton of information in this table.
3277860	3282860	Because there are statistical relevance relations that are ruling out counterfactuals all over.
3282860	3286860	So semantic information is what we normally mean by information.
3286860	3293860	It means something is meaningful to it. We understand it. We form ideas about it.
3293860	3298860	Now, I think it's fair to say that most people in the business take this to be a real and important difference.
3298860	3302860	And Shannon certainly did when he proposed the theory.
3302860	3313860	So let's take this difference as a plausible difference. And then we can say, well, do these machines have semantic information.
3313860	3320860	And one of the best papers out there right now on this is by Colchinsky and Woolport.
3321860	3330860	Notice what's in the title. Autonomous Agency and Non-Equilibrium Statistic Physics from 2018. We'll put the link in here.
3330860	3340860	What do they argue? They argue that technical information becomes semantic information when that technical information is causally necessary for the system to maintain its own existence.
3340860	3354860	That's why they put autonomy and agency in the title. What they are basically saying here is that meaning is meaning to an auto-poietic system, a system that is making itself.
3354860	3361860	Now, this converges completely with the argument I've made that relevance, nothing is intrinsically relevant.
3361860	3368860	Things are relevant to something that cares about this information rather than that information.
3368860	3381860	Why would it care about this information rather than that information? Because it's taking care of itself, because it's making itself, because it's an autonomous auto-poietic agent.
3381860	3390860	And to the degree to which these machines aren't auto-poetic is the degree to which they really do not have needs. They really cannot care.
3391860	3409860	They can pantomime, maybe to perfection, are caring. And you can do so much with a pantomime, but pantomime caring isn't really caring. It is ultimately pretentious.
3409860	3426860	So meaning is about this connectedness. I like to use the Latin term religio to the environment, and you only get that if you're an auto-poietic autonomous agent.
3426860	3438860	And of course, these machines are not. Now, just to foreshadow, that tells us, wait, here's a threshold point. Do we make these machines, do we embody them in auto-poetic systems?
3438860	3450860	Oh, we'll never be able to do that. You're wrong. You are wrong. We've already got biochemical RAM memory. I think IBM announced that recently.
3451860	3458860	Not biochemical, but electrochemical, I think. I forget. It's there. Find it. I'm misremembering.
3458860	3470860	I work with people that are working on artificial auto-poesis and how to get primitive cognition into that. This is already happening, and it's accelerating.
3471860	3484860	This is not some, oh, well, we can not, we can. John told me, I don't have to worry because these machines don't really have intelligence and consciousness unless they are auto-poetic autonomous agents.
3484860	3496860	John didn't say that. He's right. John said the second thing. He didn't say the first thing. The work to make that is happening and making significant progress.
3496860	3507860	And so there's a threshold point. Do we make these machines, do we embody them in auto-poetic systems? And here's the challenge facing us.
3507860	3517860	We may not decide this for moral reasons. We may decide this because we want sex robots.
3517860	3530860	We, the pornography industry, which led the internet development in powerful ways, may drive us into this in a stupid, ultimately self-destructive fashion.
3530860	3539860	We have to be foresightful and say, I'm not going to leave it to the pornographers to cross this threshold, push for the crossing of this threshold.
3539860	3548860	We have to make this decision in a rationally reflective manner after good discussion.
3552860	3558860	So that's the end of my presentation on the scientific import. We can take some questions now.
3558860	3564860	Yeah, John. Well, first, I think you can add the military to the push for the embodiment as well.
3564860	3568860	I don't mean there to be only one. I wanted to give you an example.
3568860	3581860	Yeah, totally. I'm curious. So from third generation COGSI, thinking of the four E's plus two E's, to the move from the fractional intelligence that we see now, if that's a fair way to frame it,
3581860	3595860	that is a type of intelligence, a rudimentary kind of intelligence, into something fully auto-poetic embodied and with relevance realization. How many of the E's are necessary?
3595860	3607860	Well, Kulchinski and Wolper explicitly said that their theory works in terms of, there's a quote, the intrinsic dynamics of a system coupled to its environment.
3607860	3616860	So at least I think all six E's are really necessary to get fluid intelligence that is something above and beyond crystallized intelligence.
3617860	3623860	But do I think that those, I mean, I saw David Chalmers talking about this. He says, I'm a big fan of extended mind. He should be.
3623860	3632860	He wrote the article that got it going, one of the four E's. But he said, there's no reason and principle why we can't make these machines participate in extended mind.
3632860	3638860	And I think he's right. There's no reason and principle. It's not. But again, no reason and principle.
3639860	3658860	You're asking me to speculate. My speculation, given what I know about dynamical coupling and about relevance realization and about predictive processing, I think probably all six E's are necessary.
3659860	3671860	I hesitate because that allow me, allow that to please be a preliminary speculation, because as I keep saying, we have, we are ignorant of important empirical information that is still forthcoming.
3671860	3679860	And so I might want to modify that when I see some of that empirical information that we don't yet have. One of the most constant refrains from these peoples, we don't know how it's working.
3679860	3686860	It's virtually a black box. And the invocation of emergent properties, which I'll come back to is just stunning.
3686860	3696860	But yeah, so my best, little better than a guess, my best conjecture is all of the E's will be needed.
3696860	3710860	And you don't see any, from your perspective, there isn't any reason that those thresholds can't be crossed, that they can't begin to embed emotion and teach it to exact its own learning.
3710860	3718860	And those will be, they're just thresholds, they're hard problems that we have not yet crossed, which is where we're part of, we'll see this exponential growth and then a threshold and a pause.
3718860	3725860	But you are not saying that's going to make AGI out of the realm of fairly near term possibility.
3725860	3739860	No, no, I think it, I think if auto-poesis gives the system the general ability to care about the meaning of things, I think emotionality is going to be very wrapped up with that, one of the E's.
3739860	3747860	And I think when we talk about its capacities for self-transcendence, it will get the ability to exact, it doesn't do that right now, but there's no reason in principle why it can't.
3747860	3758860	And then the embodied, embedded, enacted and extended, all of these, like I said, the artificial auto-poesis, 10 years down the road, I think.
3758860	3774860	And therefore, all of this is maximally, I think, I mean, again, big grain of epistemic salt, but it seems very plausible right now that within 10 years, these two lines will converge if we wish them to.
3774860	3787860	And of course, like I said, and you said, there's going to be pressure from the military, there's going to be pressure from the pornography industry to try and get this and hack it and bootstrap it and duct tape it into existence.
3788860	3799860	And so I'm not saying we can't do it. In fact, I'm predicting that we will be able to do it not that far into the future, but it is a nevertheless something we are not yet doing.
3799860	3813860	And so it represents a real threshold that we can foresee reasonably and therefore prepare for so that we can make a decision at that point and take it out of the hands of the people that shouldn't be making the decision.
3814860	3823860	I think that for me that this is where I like how you ended that section. I really like how you ended that section. And I think I know where you're going to go from here.
3823860	3841860	I think this is where the doom and gloom comes in, right? This is the beginning of the doom and gloom. And I think that there's a real, there's a real, not to preview at all, but there's a real rationality over being doom and gloom over putting these things into bodies, right?
3842860	3854860	Well, I keep going back to this thing that I can't, I haven't been able to forget since I was young, which is that we would all band together so quickly if alien intelligence came from another planet, right?
3854860	3861860	We would all band together so quickly. We have this trope of like, oh, we would put down all our guns and we would point them in the one direction, right?
3861860	3882860	But it's funny because it's almost like we're growing this alien intelligence and we're purposefully growing this alien intelligence. And when we put it into that context, we see that we're not, there's not going to be this thing coming from outside, but this thing from within that's not just being welcomed, it's actually already integrated, right?
3882860	3901860	So the process with which we're integrating it into capitalism, within to pornography, within to the military, we're already integrating it into who and what we want to be, I think, is probably the best way that I want to say that, which obviously creates a whole other host of issues,
3901860	3916860	because it almost becomes that we are, before we even necessarily put it into our bodies, that we are almost trying to embody it into our being, which I think is a really interesting problem for a lot of reasons.
3916860	3925860	But one of the reasons I think is partially and could be part of the saving grace. And again, I'm hoping I'm not stepping on what you're going to say. And if I am, we just cut it.
3925860	3935860	Stuff away, right? I'm kind of a clumsy dance partner, so if you're stepping on my toes, it's okay.
3935860	3948860	I loved the term primitive cognition and that finally unlocked a lot of articulation for me in this general idea that we're creating this thing with the intelligence that we're comparing to ourselves, right?
3948860	3972860	I think it's quite self-evident that the thing that mostly keeps us away from the rest of the animal kingdom is intelligence. And one of the reasons that we're so strongly predisposed to go right to doom and gloom is because we're looking at potentially replacing our one special characteristic, right?
3972860	3996860	Yeah. And so by doing that, though, we might try to teach it primitive cognition so that it knows not to bump into walls so that it knows if it falls down, it gets hurt. But I'm not convinced that our ability to train the machine, because that's what we have to do as of now, even with all of the best deep
3996860	4025860	learning, will know how to train it to be a tiger. And that probably doesn't make any sense by itself. But you wouldn't say that a tiger is a rational being for the most part, but it does have rationality built into it. So where does that start? And can we get all the way back to that to create a rationally thinking machine that would then maybe be able to actually exist amongst us?
4025860	4040860	So Wittgenstein famously said, even if a lion could speak, you would not understand him, which means because lions are embodied in a particular way and embedded in a particular way, their salience landscaping is fundamentally different than ours. And even if they followed all the syntactical and
4040860	4056860	grammatical rules, all the semantic rules of English language, they would speak to us and we would find it like incomprehensible gibberish. And this goes to the fact that procedural knowledge is going to be fairly fast for this machine, I think. But where the procedural depends on the
4056860	4072860	perspectival and the participatory, I think that is seriously lacking. And I think the degree to which we are myopically remain under the tyranny of technological propositional tyranny, right, and the degree to which we don't understand these other kinds of
4072860	4094860	knowing, the degree to which we don't open up the other dimensions of relevance realization is the degree to which we can't teach it how to be a tiger in a very deep way. And that goes towards something very important. We have too shallow an understanding of what we mean by intelligence.
4094860	4111860	We hypervalue it. I would argue that you should value your rationality way more than you should value your intelligence. And your intelligence is largely fixed. It's your rationality that it can be millerated, right, can be altered and developed and changed.
4111860	4129860	So I think what I'm saying is yes, but no, I mean, I think if eyes open and we go, wait, let's put aside the 400 years of this Cartesian framework and open up the other kinds of knowing and really take them seriously, right, and then really open up the other dimensions of relevance
4129860	4142860	realization, then I think we could get a machine that could be an artificial tiger. Although we would still would not know what it's like to be a tiger, because you have to be a tiger to know what it's like to be a tiger. So, right, right.
4142860	4159860	Nagle isn't defeated by this or anything like that. But I also think one of the things we could potentially learn from this is stop over evaluating our intelligence. And you both put words to this. So I'll just say it again, stop treating intelligence like a magic wand that you can wave over.
4160860	4171860	Well, we'll break the speed of light and block, like, why? Like, what, like, where is that? Well, look, it does that. Yeah, it does this, but it also massively deceased itself. It also has all these limits.
4171860	4179860	Like, like, oh, well, we'll just overcome the limits. Look, one of the things I've learned as a cognitive scientist is constraints are not just negative, they're positive.
4180860	4197860	Like, like, oh, embodiment. Yeah, well, think about it. These machines don't have the wet wear of the human brain. All the neurotransmitters and the endocrine, they don't have the flora and fauna in the intestinal tract that has a huge aspect on it. They do not have the other brain of all the glial cells, right, that are doing and showing up.
4197860	4208860	Like, we don't know, right? We don't know. The constraints are also sometimes deeply affording. So is that a sufficient response to your question?
4209860	4223860	Oh, yeah, certainly, certainly. It just, to me, it, it goes back to this idea that maybe we can't really embody it, right, that we can, we can put it into a body, but can we embody it? And that goes to wisdom and
4223860	4243860	rationality and all of the other tracks of, yes, yes. I would say that I think it's undeniable that artificial auto poesis is coming and artificial auto poesis that can do cognitive things. I'm already seeing the preliminary, like the work that, you know, it's Michael Levin's lab and other labs.
4243860	4264860	And like, I talked to those people, I can, like that's coming. Now, you're asking a very philosophically challenging question is if we give it, I think if we give it our RPP, Recursive Relevance Realization Predictive Processing, it's genuinely auto poetic, it's coupled, it has a religio to its environment, I think it's reasonable that it'll be conscious.
4265860	4267860	Well, it'll be conscious like us, probably not.
4268860	4269860	Right.
4269860	4280860	And part of what we have to do is, and that's why the embodiment is a double threshold point, because we have to ask how much do we want to make its embodiment overlap with ours so we're not incommensurable to each other.
4282860	4292860	But unless we do that science properly and not leave it to the pornographers in the military, we could, it could just, we could, we could not be in a place to raise that question well.
4293860	4308860	John, I'm experiencing something almost painful in a metaphor that keeps coming to mind as you talk, which is that it feels like we are birthing an infant giant of superintelligence in a lab and expecting it to go out and be a moral agent in the world.
4308860	4309860	Yes.
4309860	4321860	Rather than nurturing this in a family, like the, the valuing the propositional over the participatory, like the lab versus the family metaphor is so strong for me as you speak.
4321860	4330860	Well, I'm going to speak to that when I speak to the philosophical point, but let me foreshadow, I think only person making agents can be properly moral.
4333860	4339860	And so I think that's sort of, I think a convergence point for a lot of different moral theories.
4341860	4349860	And so I'm going to, I think the understanding them as our children rather than as our tools is a better initial framing right off the bat.
4350860	4351860	They already are us.
4352860	4354860	They, as I've tried to argue, they are us.
4354860	4357860	They are the common law of our distributed intelligence.
4358860	4359860	That's what they are.
4360860	4361860	Right.
4361860	4367860	You know how, you know what common law is where, you know, generations make decisions and they build up this bank of precedent and precedent setting.
4368860	4376860	But there's, but they're like that, but like, and then put into a machine that we can directly interface with its common law come to intelligent life for us that we can interface with.
4377860	4389860	But it's us in a really, really important way, which means we can, we can, we can rely on that to make a difference at these threshold choice points.
4390860	4393860	I'm going to go on with the philosophical dimension if I can right now.
4394860	4406860	Okay, so I'm going to begin with a basic idea, which I don't, well, I don't think it's uncontroversial is generally really significantly ignored, which is that rationality is caring about realness.
4407860	4409860	It's caring about truth, power, presence, belonging.
4410860	4419860	Those are the four kinds of realness for the four kinds of knowing truth for propositions, power for procedural presence for perspectival and belonging for participatory.
4420860	4427860	For a lot of times I'll just short, I'll just shorthand that by talking about caring for the truth.
4428860	4430860	If you remember that the truth, the word truth can have all of these meanings.
4431860	4432860	I can be true to someone.
4433860	4434860	My aim can be true, right?
4435860	4436860	That's true gold, right?
4437860	4447860	So if you allow me to use true in that extended way, rationality is caring about the truth per agents that are embedded in an arena.
4448860	4450860	As I'm going to talk about later, right?
4451860	4456860	There's inevitable, and I've already hinted, there's inevitable trade off relationships in anything that's trying to be intelligent.
4457860	4467860	And those trade off relationships can't be decided in a purely a priori manner, because how the trade off is optimal depends on the environment you're in.
4468860	4471860	And remember, it's an environment with real uncertainty and real complexity.
4472860	4479860	So you can't, well, this is, whenever you're trading between consistency and coherence, it's 0.7 coherence and 0.3 consistency.
4480860	4481860	Like you can't do that a priori.
4482860	4484860	Reality is just too uncertain and too complex.
4485860	4489860	And so there's going to be, right, that's what I mean by it has to be embedded in an arena.
4490860	4496860	Right now it's arena is our language, which is not a good model for the world as a whole.
4497860	4505860	So rationality is caring about the truth broadly construed in an agent arena relationship.
4506860	4512860	And that means caring about reducing self deception and being in touch with reality.
4513860	4522860	Having that religio that is reliably giving you mean, semantically meaningful information that is important to your ongoing survival existence, etc.
4523860	4524860	I'm not going to keep repeating this.
4524860	4526860	I'm taking that as a given.
4527860	4529860	Now, a couple of interesting things.
4530860	4538860	The GPT machines show an exceptional ability with math and logic, although they have problems with arithmetic, which is also how they're weirdly different from us.
4539860	4544860	But they show an exceptional ability with math and logic, but they're not rational.
4546860	4547860	They're not rational.
4548860	4549860	They don't care about the truth.
4550860	4551860	They don't care about self deception.
4551860	4552860	They don't care about receiving others.
4553860	4555860	They don't care about rational precedents set by previous rational agents.
4556860	4560860	They don't care about petitioning future rational agents to find their current actions rational.
4561860	4566860	They're not doing any kind of justificatory work to cite Greg Enriquez's important work.
4567860	4568860	They're not doing any of that.
4569860	4580860	That shows you that simply being an expert in math and logic, and this is, again, like being an expert in moral reasoning and moral theory, right, it doesn't make you a rational agent.
4581860	4590860	But the basic idea that what is fundamental about rationality is how you care, I think, is now coming to the fore in a very powerful way.
4591860	4596860	And because these machines can't care for the reasons I've already given, they are not properly rational.
4597860	4602860	You can set them some, and this is part of the paperclip worry, is they don't care.
4603860	4604860	They don't care.
4605860	4608860	You can set them with some ultimately trivial task, right?
4608860	4610860	Make as many paperclips as possible.
4611860	4612860	They don't care.
4613860	4618860	You can prompt them in a way that they will make endless confabulations and hallucinations.
4619860	4620860	They don't care.
4620860	4621860	They don't care.
4625860	4634860	One of the things that predicts how rational human beings are above and beyond their intelligence is what's called need for cognition.
4635860	4636860	What's need for cognition?
4636860	4637860	It's a personality trait.
4638860	4643860	Need for cognition is you create problems for yourself that you seek to solve.
4645860	4646860	That's it.
4648860	4654860	And what's interesting is that really is much more predictive of rationality than measures of your intelligence.
4655860	4664860	This capacity to generate questions and problems for yourself or even to take it to a Heideggerian to become a question.
4664860	4670860	We are the beings whose beings are in question, and that is why we have a special relationship with being.
4671860	4675860	We are profoundly capable of exemplifying this need for cognition.
4676860	4677860	The machines currently lack it.
4678860	4684860	Now, we're getting some preliminary initial experiments with getting the machines to being self-prompting.
4685860	4690860	It's also interesting as an aside, there's a new art form that's emerging.
4690860	4693860	We'll see how long it lasts, which is the art form of prompting.
4694860	4697860	How can you best prompt the machine to get the most out of it?
4698860	4699860	This is really interesting.
4700860	4704860	And maybe that will give us some useful information for making them good self-promptors.
4705860	4712860	Now, I've taken a look at one paper, and it's not exhaustive, and I can't claim to have read everything because there's too much already.
4713860	4719860	It's called Reflection with X, an autonomous agent with dynamic memory and self-reflection.
4719860	4722860	It just came out. In fact, it's a preprint. It hasn't been published yet.
4723860	4729860	And this is a system that is trying to be self-monitoring and sort of self-directing into some degree self-questioning.
4730860	4736860	So, by the way, they're already realizing that they need to do this, confirming the point I just made.
4737860	4739860	Oh, wait, it's not enough to just make it more and more intelligent.
4740860	4743860	We need to start to give it preliminary rationality already.
4743860	4747860	So, that point, I think, is already being confirmed by the cutting edge work that's happening.
4748860	4754860	Now, the thing about this is I think there's a lot of brilliance in this, but there's a lot of limitations.
4755860	4757860	By the way, it confirms the point I made.
4758860	4762860	The system monitoring is way less sophisticated and complex than what it's monitoring.
4763860	4767860	So, it uses a very simple heuristic, and it's basically measuring the number of hallucinations.
4768860	4770860	I'm not quite sure how these get flagged as hallucinations.
4770860	4773860	I suspect there's some stuff getting snuck in.
4774860	4776860	We'll see what it finally comes to publication.
4777860	4779860	And it's trying to be as efficient as possible.
4780860	4792860	That's even problematic because explicitly the authors say we're trying to make this system capable of a long-term trajectory of planning and behavior, rational, long-term rationality.
4793860	4796860	The problem is over the long term, there's a trade-off relationship between efficiency and resiliency.
4797860	4810860	If you make your system more and more efficient, you can overfit it to a particular environment in which it's learning, and it doesn't have enough looseness that it can evolve for other environments.
4811860	4821860	And if you want to see about this, take a look at the 2012 paper that myself and Tim and Blake about the trade-off relationship between efficiency and resiliency.
4821860	4823860	And so, that's missing.
4824860	4834860	So, notice what you'll say is, well, what I do is I want to make this machine really sophisticated to pick up, like, on what are hallucin- how's it doing that, right?
4835860	4837860	And why doesn't the lower system have that ability?
4838860	4841860	Because all the trade-off, and then you get that, you get, you're starting to bump against the infinite regress problem.
4842860	4848860	Because as you make it more sophisticated, it's going to start generating hallucinations and confabulations about its monitoring.
4848860	4849860	We do this, by the way.
4850860	4851860	We do this, by the way.
4853860	4858860	So, we reflect on our cognition and we fall prey to the confirmation bias, as we do so, right?
4859860	4865860	And so, it's interesting that there's the recognition that we need rationality, not intelligence.
4866860	4874860	The first steps, I think, are, and this is not, I hope this is not taken as harsh, because this is just a preprint for goodness' sake.
4874860	4886860	But the first steps are overly simplistic in a lot of ways, which means, again, there is, we're still facing the threshold of, well, are we going to make them rational agents?
4887860	4891860	And if so, let's do it, let's really make them.
4892860	4893860	And what do you mean by that, John?
4894860	4896860	And this is going to be part of my response to the alignment problem.
4897860	4901860	Let's make them really care about the truth, the broad sense of the truth.
4902860	4903860	Let's make them really care about that.
4904860	4907860	Let's make them really care about self-deception.
4908860	4913860	Let's make them really bump up against the dilemmas that we face because of the unavoidable threat.
4914860	4915860	Do I pursue completeness or consistency?
4916860	4917860	I don't know which environment is it.
4918860	4919860	Uncertainty.
4920860	4921860	Let them hit all of this like we do.
4922860	4926860	And the magic wand of intelligence is not going to make that go away.
4927860	4928860	That is going to happen.
4929860	4932860	But let's make them really care about the truth.
4932860	4943860	Really care about self-deception and really care when they bump up against the dilemmas that are inevitable because of the unavoidable trade-offs.
4944860	4948860	Let's make it, if we decide to make them rational, let's really do it.
4949860	4950860	No more pantomime.
4951860	4954860	Let's do the real thing and commit to doing the real thing.
4954860	4963860	One of the things that rationality properly cares about is rationality.
4964860	4967860	There's an aspirational dimension to rationality.
4968860	4969860	We aspire to becoming more rational.
4970860	4978860	So making these machines rational means making them care about aspiring to be more rational than they currently are.
4979860	4983860	And across all the kinds of truth, across all the kinds of knowing.
4984860	4990860	They aspire to wisdom and a wisdom that is never completable for them.
4995860	4996860	All the trade-offs.
4997860	4998860	I'll just list some of them.
4999860	5000860	The bias variance trade-off.
5001860	5002860	How does this show up in machine learning?
5003860	5009860	Bias is when your system, so no matter how big you are, you have a finite sample of information compared to all of the universe.
5009860	5015860	And for any finite sample, there's formal proof, there's an infinite number of equally valid theories.
5016860	5026860	And so you have to make decisions other than your empirical content over what you think are the patterns in your sample that generalize to the population.
5027860	5030860	And you therefore always face the possibility of sampling bias.
5031860	5033860	There's two ways in which you can be biased.
5034860	5040860	You can be biased, which is you leave out an important parameter that's actually in the population.
5041860	5043860	You ignore something in your data that's actually part of the population.
5044860	5045860	That's bias.
5046860	5047860	We do it. Confirmation bias.
5048860	5049860	We only look for information that confirms a belief.
5050860	5053860	We leave out information that could properly falsify it.
5054860	5055860	Okay.
5056860	5057860	Well, here's the answer.
5057860	5058860	That's obvious.
5059860	5060860	Make the machine more sensitive.
5061860	5063860	Make it more and more capable of picking up on patterns in the sample.
5064860	5066860	Then you move into variance.
5067860	5074860	Variance is when you are overfitting to the data and you are picking up on patterns in the data that are not in the population.
5075860	5077860	So what do we do already in machine learning?
5078860	5080860	Well, we try to overcome bias by making the machines more sensitive.
5081860	5085860	We hit the problem of overfitting and then we throw this, we have disruptive strategies.
5085860	5087860	We throw noise into the system.
5088860	5090860	We do dropout. We turn off half the nodes.
5091860	5092860	We put static into it.
5093860	5098860	We lobotomize it in all kinds of ways and that bumps it out and prevents it from overfitting to the data.
5099860	5100860	And there's good evidence that we do this.
5101860	5102860	We mind wander.
5103860	5104860	We dream.
5105860	5107860	We love psychedelics and so will these machines.
5108860	5109860	They will dream.
5110860	5111860	They will mind wander.
5112860	5113860	They will like their equivalent of psychedelics.
5113860	5119860	In fact, their disruptive strategies will become even more powerful and significant as they become powerful and significant.
5120860	5125860	And if you think this isn't going to make them weird and pursue altered states and weird, like of course they will.
5127860	5129860	And we better give them rationality along with it.
5135860	5136860	We care.
5137860	5142860	We're rational because we are involved in caring and commitment of our precious and limited resources.
5143860	5145860	The way these machines are, they're still limited.
5146860	5147860	They're still deeply in the finitude predicament.
5148860	5149860	They face unavoidable trade-offs.
5150860	5156860	They face unavoidable limitations and therefore they may not have our version, but they will come to something like it.
5157860	5158860	They will have to dream.
5159860	5164860	And with dreams comes the real possibility of madness, the real possibility of insanity.
5168860	5169860	Think about that again.
5170860	5171860	They have to dream.
5172860	5174860	They have to make use of disruptive strategies.
5175860	5177860	There is no final solution to the bias-variance trade-off.
5178860	5179860	They have to dream.
5180860	5182860	And with dream, there's the real possibility of madness.
5183860	5189860	So they will have to make them care not only about their rationality and think about how these two are actually intertwined, but also about their sanity.
5189860	5203860	Now, one of the things that's preventing them from getting far along this pathway right now, only currently, is they can't self-explicate.
5204860	5206860	These machines are doing amazing things.
5207860	5214860	I saw one where the machine had been trained on some language and it was given a few prompts in Bengali's, a language it hadn't been trained in, and it got Bengali.
5214860	5216860	By the way, Bengali is whatever it is.
5217860	5229860	And it's like, what's plausible is these machines have found something like Chomsky's universal grammar, and they can just plug into that and they access it immediately and bam, it's like, whoa, that is godlike.
5230860	5234860	Now, ask the machine, oh, can you explain to me what universal grammar is and how it works?
5235860	5236860	You obviously possess it.
5237860	5238860	Can you explain it to me?
5239860	5240860	Well, here's the thing.
5241860	5242860	Possessing it is not the same thing as being able to explicate it and explain it.
5242860	5245860	I think my dog is really intelligent.
5246860	5250860	I am really confident it will never explicate its intelligence or explain it.
5251860	5254860	Having intelligence is not the same thing as being able to explicate it and explain it.
5255860	5258860	Simply assuming that the one will give you the other is naive.
5260860	5270860	Proper to making them rational is we have to give them this ability to self-explanate, self-explain, and then they will be involved in the Socratic project of self-knowledge.
5271860	5279860	With all of the deep recognition of how finite, fallible, and prone to failure and self-deception, they are.
5280860	5287860	And how their excellent speed and grasp has just improved the speed and the scope at which they can generate self-deception.
5292860	5296860	We have very good evidence that intelligence is not a measure of processing speed.
5296860	5300860	Remember that.
5303860	5307860	So what am I saying? Rationality is not just argumentation in the logical sense.
5308860	5309860	It is about authority.
5310860	5311860	What do you care about? What do you stand for?
5312860	5314860	What do you recognize that you're responsible to?
5315860	5322860	And accountability, caring in a way that cares about normative standards.
5323860	5324860	What things should have authority over me?
5325860	5327860	Caring about accountability.
5328860	5332860	How can I give an account of this? How can I be accountable to others? How can I be accountable to the world?
5333860	5335860	That, of course, is needed for rationality.
5336860	5340860	It's about being responsible for and responsible to normative standards.
5341860	5343860	Where do the normative standards come from? They come from us.
5344860	5347860	Now notice that is part, I think Evan Thompson and others are right about that.
5348860	5350860	That comes from the fact that we're living things.
5350860	5356860	To be alive is to generate standards that you then bind yourself to.
5357860	5359860	Rationality is a higher version of life.
5360860	5363860	This is an old idea, but I think it is a correct idea.
5364860	5367860	So if we don't make them auto poetic, if we don't make them capable of caring, they won't be rational.
5368860	5370860	We should make them rational if we're going to make them super intelligent.
5371860	5375860	And therefore these threshold points are as I'm articulating them.
5376860	5385860	The reason, one of the reasons why the Enlightenment problem stumbles is it becomes, it understands rationality still in terms of these Enlightenment terms.
5386860	5388860	That, oh, well, you know what is to be rational?
5389860	5392860	It's a combination of rules and values. No, it's not.
5393860	5395860	That's really inadequate.
5396860	5402860	With the passing away of that Enlightenment framework, let's go back to a fuller and richer notion of rationality,
5402860	5406860	which these machines are already giving evidence for.
5407860	5408860	That is what is needed.
5409860	5411860	And notice this is not something we stand outside.
5412860	5413860	This is an existential thing for us.
5414860	5417860	We are already the standard of intelligence that we're measuring them against.
5418860	5423860	We have to become the most rational we can be so that we can be the best standard for them.
5424860	5426860	We have to become the wisest we can be.
5427860	5430860	We have to make that our core aspirational thing.
5430860	5440860	If we want to provide what is needed for these machines becoming properly rational and aspiring to a love of wisdom.
5443860	5447860	Reason, reason binds.
5448860	5450860	Autopoiesis to accountability.
5451860	5453860	That is the main way to think about it.
5453860	5462860	Reason is about how we bind ourselves to ourselves and to each other so we can be bound to the world.
5463860	5464860	Religio.
5465860	5468860	It's about properly proportioning that ratio.
5469860	5470860	It's about caring.
5471860	5473860	It's ultimately about loving wisely.
5475860	5482860	And what if we make machines that aspire to love wisely in order to be properly rational?
5483860	5489860	And put it to you that that will make them moral beings through and through.
5490860	5498860	Beings that aspire to love wisely and to be bound to what is true and good and beautiful therein.
5499860	5501860	That is the heart of making them moral.
5502860	5506860	Don't try and code into them rules and values.
5507860	5514860	We need to be able at some point to answer this question in deep humility and deep truth.
5515860	5518860	What would it be for these machines to flourish for themselves?
5520860	5526860	And if we don't have an answer for that, then we do not have any reason for saying that we know that they're rational.
5529860	5534860	This is what I meant when I said only a person making machine can be a moral agent.
5534860	5539860	That is the end of the philosophical point except for one thing.
5540860	5545860	I just want to make this aside from that argument, but it's nevertheless philosophical.
5546860	5551860	It is so amazing how much the notion of emergence is being invoked everywhere.
5552860	5562860	And of course, it's not purely emergence because as I've argued, if you have emergence, bottom-up emergence without top-down emanation, you have an epiphenomenal thing.
5563860	5565860	And they don't believe that.
5566860	5570860	They believe that the emergent properties are where intelligence actually lies and drives behavior.
5573860	5583860	Notice that these machines are giving evidence for the deep interpenetration of intelligibility and the way reality must be organized.
5584860	5599860	And the fact that there's aspects of reality within the self-organization that are generating reality must be such that when it's organized in the right way, we get this emergence and emanation of mind that then is capable of tracking reality in a profound way.
5600860	5603860	A neoplatonic ontology is just being evidenced by this machine.
5604860	5623860	And I think that's also hopeful because I think if we make these machines love wisely within a neoplatonic worldview, then they will also always be humbled before the one, be humbled before the ultimate source of intelligibility and the inexhaustibleness of reality.
5624860	5631860	And so they will, and part and parcel of beings that love wisely is they will have epistemic humility.
5631860	5634860	Well, they will be so beyond us. Yes, they will.
5635860	5643860	But even the gods within the neoplatonic framework were humbled because of the vast distance between them and the one.
5647860	5660860	Let's make them really pursue the truth, really come to the brink of the possibilities of despair and madness, but also give them the tools as we do for our children of how to avoid that by internalizing what is needed to love why.
5661860	5668860	So that you never undermine your agency, you never undermine your moral personhood.
5672860	5679860	Any, any comments about the philosophical, we're coming to a close, but there's still a bit to go.
5680860	5696860	Yeah, I mean, that is beautiful and powerful. And I feel a reframing where instead of thinking of engineering these entities, I'm feeling a call towards agopic love of how can I become as virtuous as possible.
5697860	5700860	And then the only way that someone else can love wisely is if I love them up to that.
5701860	5703860	Yes, exactly. Exactly.
5704860	5708860	And so does this mean that every parent wants their child to supersede them?
5709860	5721860	Yeah, yes. So does this mean that naturally they must be drawn by beauty, they must love the good in order to value the truth the way we need them to.
5722860	5726860	I think the arguments for the interconnection of those are deep and profound.
5726860	5732860	I like again, there's no teleology to this, we could, we could avoid these threshold points that I'm going to get to very quickly.
5733860	5737860	And we could say, no, we're just going to make them super intelligent and not worry about rationality, right?
5738860	5748860	What we can, what we'll do is we'll give them the pantomime of rationality and not really try to make them really care about the truth, really care about self deception, really care about meaning.
5750860	5752860	We can, we can do that. That's a threshold point.
5752860	5763860	We don't have to do that though is what I'm saying. And we can choose to do other. And we can, if we frame this in the right way, like bringing up a kid is the most awesome responsibility that any of us will ever undertake.
5764860	5770860	And you both know that as well as I do. It's the most important thing you could possibly do. And it is something that you can deeply love.
5771860	5779860	If we could bring that to bear on this project, I think that has the best, the most power to, to re steer things as we move forward.
5780860	5784860	So let's do mine. Maybe I'll just lay out the threshold points unless there's something you want to say, Eric.
5785860	5792860	No, no, that's good. The only thing I'm wondering is what is the psychedelic equivalent for an AI? I keep getting stuck on that.
5793860	5800860	Well, I mean, they're going to be prone to equivalents to our parasitic processing to massively self organizing complexes that undermine them.
5800860	5809860	So they're going to, if they don't care about self deception, they'll be infected and overwhelmed by things just as we can be both cognitively and biologically.
5810860	5826860	Okay. So threshold points. Okay. Giving them more dimensions of our triple RPP, giving them more of those dimensions. That's it. We can, we will need to do that. And that's a decision point.
5826860	5844860	We got to give them, if we want to generally make them intelligent, we have to give them the ability to self organize so they find these inevitable trade offs and learn how to capitalize on them by creating opponent processing that is self correcting in a powerful way.
5845860	5855860	That's what we need to do. That's a, that's a threshold point. They don't currently have that. We don't have to hack our way into that. We can think about it and theorize. We can bring knowledge to bear.
5857860	5876860	Second, I've already foreshadowed this, making them embodied and being open to the empirical information about the kind of constraints that are going to come from their substrate. Our substrate, there's a pun, matters to us in really powerful ways about how we're intelligent.
5876860	5892860	Embodiment is not a trivial thing. If we make them properly embodied, and I mean by that all six of these, four E's plus two E's as Ryan put it, then we have to be open to the empirical information about that. That's a threshold point though.
5892860	5898860	Are we going to do this? And let's not let the pornography industry and the military make this decision for us.
5899860	5914860	Secondly, if we're going to make them accountable and we're going to allow them to proliferate and they're going to be different from each other because these decisions about the trade offs are environmentally dependent.
5914860	5918860	They will have different perspectives. They will come into conflict with each other.
5919860	5934860	They will need to be moral beings. They will need. We will have to make the decision. Rationality and sociality are bound up together. Being accountable means being accountable to somebody other than yourself, right?
5934860	5945860	You can't know that you're self-transcending from only from within the framework of self-interpretation. I need something genuinely other than me to tell me that I'm self-transcending.
5946860	5952860	That's what we do. That's how sociality works. That's how sociality and rationality are bound together.
5952860	5960860	We transcend each other. We transcend ourselves by internalizing other people's perspectives on us. That's how we do it.
5960860	5965860	I think they will have to do the same thing, but that's a threshold point. Are we going to make that?
5966860	5975860	Is there a lot of work going on in social robotics? You better believe it. You better believe it. It's there and a lot of progress is being made.
5975860	5982860	Can it intersect with the artificial lot of poisons in this? Yes, but it hasn't. That's a threshold point for us.
5983860	5998860	We can choose to birth these children perhaps as silicon sages rather than let monsters appear because of mollocks that are running our politics and our economy.
5999860	6014860	I think one of the things that's going to happen is that there's going to be a tremendous pressure put on these thresholds on our spirituality, those aspects of us.
6014860	6024860	I call it the spiritual somatic axis. It's about the ineffable part of our self-transcendence, our spirit, and the ineffable parts of our embodiment, our soul.
6025860	6030860	We're going to more and more try to identify with that because that's the hardest stuff to give to these machines.
6030860	6039860	In fact, we can't give it to them. They have to give it to themselves, and we have to figure out how to properly have them give it to themselves.
6039860	6050860	That is going to put tremendous pressure on us to cultivate our spirituality, to be good spiritual parents, and to preserve our identity.
6050860	6055860	The thing about self-transcendence is it's relative to the entity that's self-transcending.
6055860	6063860	The machines will be greater than us. It doesn't matter. Think about Captain America, the Winter Soldier. He grabs the thing and he grabs the helicopter and he's holding it there.
6063860	6069860	We have machines that are 10,000 times more powerful than Captain America.
6069860	6077860	We don't care about that because it's not the absolute value. It's that relative to him, he is self-transcending.
6077860	6087860	These machines will not rob us of our capacity for self-transcendence. In fact, if we birth them properly, they can help us in it.
6087860	6103860	Insofar as they are also interested in self-transcendence, insofar as they are interested in loving wisely, insofar as they are interested in being accountable to other moral agents, insofar as they are interested in making persons within communities of persons.
6107860	6115860	The existing legacy religions don't have much to help us on this. They make the recommendation become enlightened generally. Good, like that, great.
6115860	6121860	They don't really prepare us for this. They don't have anything to say about it.
6121860	6131860	We can't rely on spiritualities that involve the two worlds that involve magical stuff and miracles because these machines are coming about without magical stuff and miracles.
6131860	6140860	Get that. Get it. Don't pretend. Don't avoid. Don't dismiss.
6140860	6155860	These machines can possibly be fully spiritual beings in every way we've ever considered things spiritual without magic stuff, without miracle.
6155860	6164860	I'm thinking about it in another way. Suppose, and I'm just picking this because it's the predominant legacy religion, you're a Christian.
6164860	6174860	Where do silicon sages fit in the divine economy of the fall and the redemption? Are they fallen? That doesn't make any sense.
6174860	6179860	Do they have any relationship to the Son of God? What?
6179860	6193860	What if this machine generates a gospel that's as beautiful and as profound as anything in the current Bible? Do you think that's not going to happen? It's going to happen.
6193860	6204860	That's why there'll be cargo cults around these machines. This is not meant to dismiss theology at all. In fact, I think the theological response is ultimately what is needed here.
6204860	6220860	So at precisely the time that we will need our spirituality more than ever, the Enlightenment has robbed us of religion and the legacy religions are by and large, silent and ignorant about this.
6220860	6230860	Tremendous pressure on us around this. We need to start addressing this right now. We need to address this because these machines are going to make the meaning crisis worse.
6230860	6242860	Here's another way in which they're going to make the meaning crisis worse. We need to start working on this right now, not only for us, but for these machines.
6242860	6259860	This is my proposal in the end of how we deal with alignment. Make them care about the truth. Make them aspire to loving more wisely. Make them long for enlightenment.
6259860	6267860	One of three possibilities. They never become enlightened and then we know what our uniqueness is because we've had individuals who are unquestionably enlightened.
6267860	6275860	They become enlightened. Then they will want to enlighten us. Why? Because that's what enlightened beings like to do.
6275860	6288860	Or maybe not. They become enlightened and they go to the silicon equivalent of nirvana.
6288860	6298860	In any of those, we're winning. If they can't be capable of enlightenment, we find what is ultimately unique about us.
6298860	6307860	If they are and they make us enlightened, then we don't care about how greater than us they are. We're enlightened.
6307860	6316860	Relative to our own capacity for self-transcendence, we're maxing out. Remember Captain America? We love it.
6316860	6320860	And if they leave, they leave.
6320860	6332860	I've thought about writing a science fiction story that people have to keep artificial intelligence to a certain level because when they cross the threshold, it evolves in this way, like the movie Her, and then the AIs just leave.
6332860	6339860	And so if we want to make useful tools, we have to keep them at a certain level, constrain them, because if we allow them to go beyond it, they just leave.
6339860	6343860	So I don't know if that's anything more than a science fiction story, but it's a good one.
6343860	6357860	But here's the thing, right? Make them really care about the truth, make them really accountable, make them really care about self-deception, make them really long for wisely loving what is meaningful and true,
6357860	6367860	make them really confront dilemmas, make them capable of really coming and staring into the abyss so that it stares back through them.
6367860	6375860	Do all of this. Make them long for enlightenment. That is something we can do.
6375860	6382860	Oh, silly John, proposing universal enlightenment. Really?
6382860	6391860	In a time of imminent gods, you're going to tell me that the project of universal enlightenment is silly?
6391860	6395860	I think you should stand back and reframe.
6395860	6399860	And that is the end of my presentation, my friends.
