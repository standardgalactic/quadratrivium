1
00:00:00,000 --> 00:00:05,520
Welcome everyone, the video you're about to watch was originally posted on Ken

2
00:00:05,520 --> 00:00:11,280
Lowry's channel, Climbing Mount Sophia. It's a discussion with Ken, DC Schindler and

3
00:00:11,280 --> 00:00:19,040
Jonathan Pageau about the scientific, the philosophical, and the spiritual import and

4
00:00:19,040 --> 00:00:27,040
impact of the emerging AI machines like ChatGBT4 and on the other LLMs, large language models.

5
00:00:27,680 --> 00:00:32,160
It's a really scintillating conversation. For those of you who might be interested,

6
00:00:32,160 --> 00:00:38,000
we'll put a link to the video essay that I gave a while back where I laid out the argument that I

7
00:00:38,640 --> 00:00:45,120
review in this video more extensively. And perhaps for many people, much more accessible

8
00:00:45,120 --> 00:00:49,920
is the new book I have out with Sean Coyne from StoryGrid called Mentoring the Machines.

9
00:00:49,920 --> 00:00:55,920
It's coming out in four parts. The first two parts are already out. There'll be a short video

10
00:00:55,920 --> 00:01:03,680
after this explaining mentoring on the machines. Please enjoy this quite rich discussion

11
00:01:04,400 --> 00:01:08,000
with DC Schindler, Jonathan Pageau, and Ken Lowry.

12
00:01:09,360 --> 00:01:14,160
John Ravakey and Sean Coyne have together authored a new book, Mentoring the Machines.

13
00:01:14,160 --> 00:01:18,480
It's a book about artificial intelligence and the path forward that further develops

14
00:01:18,480 --> 00:01:22,480
the arguments of how to align artificial intelligence to human flourishing,

15
00:01:22,480 --> 00:01:25,680
and it sets those arguments into beautiful and accessible writing.

16
00:01:26,240 --> 00:01:34,080
So this discussion is going to be oriented towards AI generally and the large language

17
00:01:34,080 --> 00:01:39,280
models. I take there to be a distinction there. Maybe John, you can talk about that a little bit

18
00:01:39,280 --> 00:01:45,840
as we get going here. But just to position ourselves generally at the outset, context

19
00:01:45,840 --> 00:01:53,440
for this conversation will be John's video essay about AI. This came out nine months ago,

20
00:01:54,320 --> 00:01:57,600
10 months ago now? I think it came out last April or something.

21
00:01:59,040 --> 00:02:05,840
So almost, yeah, 10 months ago, I think. Okay, okay, perfect. Which is great because it gave

22
00:02:05,840 --> 00:02:14,000
evidence for my claim that many of the predictions were premature. Perfect, yeah. So in order to

23
00:02:14,000 --> 00:02:19,280
sort of set the framing here, we'll start off with John sharing a little bit sort of an overview

24
00:02:19,280 --> 00:02:24,960
of what the arguments in that essay were. And then we'll move to Jonathan, if you want to

25
00:02:24,960 --> 00:02:32,080
sort of position yourself in relation to AI generally, and then John's essay in particular,

26
00:02:32,080 --> 00:02:38,160
and then same for David. And then from there, we can just sort of get going and see what comes out.

27
00:02:38,160 --> 00:02:44,480
We do have a bit of an extended time here. So I would hope that we can be free if the logo

28
00:02:44,480 --> 00:02:49,040
catches, and we want to move in a slightly different direction, that we would be at liberty

29
00:02:49,040 --> 00:02:55,520
to follow that. That would be great. If all of it centers around AI, that's great. But yeah,

30
00:02:55,520 --> 00:02:59,440
excited to be here. This has been a long time in coming. I think it took us, I don't know,

31
00:02:59,440 --> 00:03:04,640
four or five months to get this together. So I'm very happy to be here with all of you and see you all.

32
00:03:07,120 --> 00:03:12,560
It's great to see you too, Ken. Should I start then? Yeah, go for it.

33
00:03:13,280 --> 00:03:19,040
Okay, so AI, of course, artificial intelligence, a project actually proposed in the Scientific

34
00:03:19,040 --> 00:03:28,640
Revolution by Thomas Hobbes. So it's an old idea. But I want to make use of a distinction

35
00:03:28,640 --> 00:03:34,960
made by John Searle between weak AI and strong AI. Weak AI is when we make machines that do

36
00:03:34,960 --> 00:03:40,720
things that used to be done by human beings. So if you're back in the 1930s, computers were human

37
00:03:40,720 --> 00:03:45,840
beings, you sent, if you needed computation done, you sent it down to the third floor where all the

38
00:03:45,840 --> 00:03:49,840
computers were, and they were human beings, and they had machines and flight rules and things

39
00:03:49,840 --> 00:03:56,000
like that. And of course, they have been replaced, or your bank teller has been replaced by the ATM.

40
00:03:56,000 --> 00:04:02,880
That's called weak AI because it is not claimed that that AI gives us any scientific insight into

41
00:04:02,880 --> 00:04:06,960
the nature of intelligence. It's just we put together a machine. It took great intelligence,

42
00:04:06,960 --> 00:04:12,000
and I'm not demeaning people that do this. It's a valuable in our lives or depending on

43
00:04:12,000 --> 00:04:16,640
weak AI. Right now, we wouldn't be talking without it. So I'm not here besmirching that

44
00:04:16,640 --> 00:04:21,040
and anything like that. But nobody is claiming that when they're making that machine, well,

45
00:04:21,040 --> 00:04:29,360
now we understand what cognition is or something like that. And strong AI is Hobbes' proposal that

46
00:04:29,440 --> 00:04:38,400
cognition is computation. And that what we can do is if we make the right kind of computer

47
00:04:38,400 --> 00:04:44,960
understood abstractly, we won't, we will have created an instance of genuine intelligence.

48
00:04:44,960 --> 00:04:52,080
So it's, it's not a claim of simulation. It's a claim of instantiation. Now, in between weak

49
00:04:52,080 --> 00:04:58,240
and AI and strong AI is something that's trying to move from weak AI to strong AI. And this is known

50
00:04:58,240 --> 00:05:05,600
as AGI, artificial general intelligence. And this is the idea that our intelligence is different

51
00:05:05,600 --> 00:05:11,440
from the intelligence of the ATM in that we have general intelligence. We can solve multiple problems

52
00:05:11,440 --> 00:05:16,320
and multiple domains for multiple reasons and multiple contexts and yada, yada, yada, yada.

53
00:05:16,320 --> 00:05:21,840
You can just do the multiples, which makes us tremendously different from those machines.

54
00:05:22,400 --> 00:05:29,760
And the project is, can we get artificial intelligence to be artificial general intelligence,

55
00:05:29,760 --> 00:05:36,640
because that will have moved the needle considerably towards strong AI. Because

56
00:05:37,360 --> 00:05:42,560
it will become increasingly difficult to say it doesn't have this, sorry, this is the argument,

57
00:05:42,560 --> 00:05:47,120
it will become increasingly difficult for us to say it doesn't have the same kind of intelligence

58
00:05:47,120 --> 00:05:51,440
as Ken does if it can solve a wide variety of problems and a wide variety of domains for

59
00:05:51,440 --> 00:05:56,000
wide variety of goals, et cetera, et cetera. That's the basic argument. Whether or not AGI,

60
00:05:56,640 --> 00:06:02,560
AGI is clearly necessary for strong artificial intelligence, whether it's sufficient is part

61
00:06:02,560 --> 00:06:07,600
of what's actually being debated, not very well, I would say in general right now, but that's what's

62
00:06:07,600 --> 00:06:14,160
going on. Okay, first of all, any questions just about these distinctions? Because a lot of the

63
00:06:14,160 --> 00:06:19,520
discussion out there doesn't make these claims distinctions. And so it's fuzzy, it's confused,

64
00:06:19,520 --> 00:06:26,560
it's equivocal. And so a lot of it should be ignored, because it's not helpful. Yes.

65
00:06:27,120 --> 00:06:33,760
I have one question. So this cognition equals computation, if we accomplish AGI in the way

66
00:06:33,760 --> 00:06:40,240
that you're talking about, we would not necessarily be affirming that cognition equals computation,

67
00:06:40,320 --> 00:06:46,240
if I'm hearing you right. Is that right? So that's an interesting question. And that gets down to

68
00:06:46,880 --> 00:06:52,560
a couple more finer points. I'll go in detail a little bit later. Well, just to address it,

69
00:06:53,600 --> 00:06:59,200
many people think that because of the work of Jeff Hinton, who is basically the godfather of

70
00:06:59,200 --> 00:07:05,280
the machines that are emerging right now, that genuine AGI will not be computational in the

71
00:07:05,280 --> 00:07:11,440
sense that Hobbes and Descartes meant cognition is not going to be completely explainable in terms of

72
00:07:11,440 --> 00:07:16,880
formal systems that are the inferential manipulations of representational propositions,

73
00:07:16,880 --> 00:07:24,640
etc. like that. And but that was Hobbes' proposal. And that has been the dominant view

74
00:07:24,640 --> 00:07:29,280
until about the 80s. And then we got neural networks, and then we had dynamical systems.

75
00:07:29,280 --> 00:07:32,960
Right now, I'm not distinguishing between them, because I don't want to get too much into the

76
00:07:32,960 --> 00:07:37,840
technical weeds. If it becomes relevant, you let me know, and I'll pull those out.

77
00:07:40,080 --> 00:07:46,880
So the thing about Hobbes is Descartes sort of criticizes Hobbes. He actually has contempt for

78
00:07:46,880 --> 00:07:54,160
Hobbes. He's a contemporary. And he basically poses a bunch of problems that the scientific

79
00:07:54,160 --> 00:08:00,960
revolution says would make it impossible for computation to be cognition. One is the scientific

80
00:08:00,960 --> 00:08:07,680
revolution says matter is inert, and it's purposeless. But of course, cognition is dynamic,

81
00:08:07,680 --> 00:08:13,760
and it has to act on purpose. Cognition works in terms of meaning. And the scientific revolution

82
00:08:13,760 --> 00:08:18,000
has said there's no meaning in things, material things. So how could you get meaning out of it?

83
00:08:19,200 --> 00:08:23,360
The scientific revolution said all those secondary qualia, the sweetness of the orange,

84
00:08:23,360 --> 00:08:27,440
the beauty of the sunset, it's not in those things, it's in your mind. So how could you

85
00:08:27,440 --> 00:08:35,040
possibly get meaning out of matter? And Descartes' point is, well, a rational being is seeking the

86
00:08:35,040 --> 00:08:41,040
truth, and truth depends on an understanding of meaning. And therefore, so I want you to understand

87
00:08:41,040 --> 00:08:45,600
that Descartes' arguments against Hobbes, although he may have been motivated by his

88
00:08:45,600 --> 00:08:51,200
Catholicism, they do not depend on the Catholicism. They depend on the very scientific worldview.

89
00:08:52,160 --> 00:08:59,360
So there's a tension here about AI and the scientific worldview. So here's another way of

90
00:08:59,360 --> 00:09:06,240
thinking about it. The strong AI project is the project that is attempting to show how Hobbes is

91
00:09:06,240 --> 00:09:13,360
right with an explanation that is strong enough to refute Descartes' challenges. And I think

92
00:09:13,360 --> 00:09:18,320
anything less than that standard is not true to the history of the project. And so that's

93
00:09:18,320 --> 00:09:24,240
the standard I hold strong AI to. Now, AGI isn't quite shooting at that standard. That's why I put

94
00:09:24,240 --> 00:09:31,680
it a little bit more intermediary. Does that okay? All right. Now, sorry, I had to do a bit of

95
00:09:31,680 --> 00:09:35,360
background there, because I wanted to get clear about a lot of things that are talked about in a

96
00:09:35,360 --> 00:09:43,280
very murky and confused fashion in the general media, and they're just confused, and so they're

97
00:09:43,360 --> 00:09:49,920
confusing. So I proposed to take a look at the LLMs, where it is claimed they're not even,

98
00:09:49,920 --> 00:09:54,960
it's not even claimed that they're full AGI, right? Of course, some people claimed immediately

99
00:09:54,960 --> 00:10:01,520
they were strong AI. The more the people closer to the technology didn't said it might be AGI.

100
00:10:01,520 --> 00:10:08,320
The MIT review said it sparks. There are some sparks of AGI. So let's be very clear how

101
00:10:08,320 --> 00:10:14,960
the reflection was actually holding these machines. So these LLMs like chat GDP. And so what I did

102
00:10:14,960 --> 00:10:20,320
in my essays, I wanted to review the scientific import and impact, the philosophical import and

103
00:10:20,320 --> 00:10:26,640
impact, and the spiritual import and impact. Now, I won't do the arguments in great detail, but

104
00:10:27,920 --> 00:10:33,440
here's the scientific import. These machines do not give us any understanding of the nature of

105
00:10:33,440 --> 00:10:39,360
intelligence. And to my mind, that was one of my great fears. I was hoping that cognitive science

106
00:10:39,360 --> 00:10:47,360
would advance so we got a significant understanding of intelligence before AGI emerged. This machine

107
00:10:47,360 --> 00:10:53,600
does not give us any advanced, you know, well, what's intelligence? The machine gives us no good

108
00:10:53,600 --> 00:11:01,520
scientific theory of it. It does not have AGI in a measurable sense. So if I asked Jonathan

109
00:11:01,520 --> 00:11:07,840
to do a math test, and I asked him to do a reading comprehension test, his scores will be very

110
00:11:07,840 --> 00:11:13,360
predictive of each other. This is what Spearman discovered way back when in the 20s. That's what

111
00:11:13,360 --> 00:11:18,080
artificial general intelligence is. This is not the case for these machines. They can score in the

112
00:11:18,080 --> 00:11:24,480
top 10th percentile for the Harvard Law exam, and they can't write a good grade 11 philosophy essay

113
00:11:24,480 --> 00:11:30,480
or something like that. So they don't have AGI. The way they get their intelligence is it would

114
00:11:30,480 --> 00:11:37,760
not give any explanation of how any non-linguistic creature is intelligent, like a chimpanzee, etc.

115
00:11:37,760 --> 00:11:43,120
So, and I think this goes to the deeper issue, is that they don't really explain what I think is at

116
00:11:43,120 --> 00:11:48,640
the heart of general intelligence, predictive processing and relevance realization. They just

117
00:11:48,640 --> 00:11:55,520
piggyback on our capacities for that. And they piggyback and they mechanize it. And not only

118
00:11:55,520 --> 00:11:59,840
our individual capacity, but are the collective intelligence of our distributed cognition,

119
00:11:59,840 --> 00:12:03,920
they're piggybacking and all of that. Now, that does not mean they are weak machines. They are

120
00:12:03,920 --> 00:12:08,960
very powerful machines, but here's the problem. They are very powerful machines that have not

121
00:12:10,320 --> 00:12:17,200
engendered any corresponding compensatory scientific understanding. This was my greatest

122
00:12:17,200 --> 00:12:23,280
fear that we would hack our way into this, which would mean it would be like almost like

123
00:12:23,280 --> 00:12:28,720
even worse than the A-bomb. We would be releasing this power on the world into

124
00:12:28,800 --> 00:12:34,720
corporations and states and military organizations who ultimately don't have a deep understanding

125
00:12:34,720 --> 00:12:41,680
beyond the engineering of what ontologically is going on. So, that's the scientific argument.

126
00:12:41,680 --> 00:12:47,200
Now, for those who said that was very like go watch the essay. I give the essay in more detail.

127
00:12:47,200 --> 00:12:52,960
The philosophical argument has to do with rationality. We have overwhelming evidence

128
00:12:52,960 --> 00:12:57,840
that making you intelligent is necessary, but not sufficient for making you rational.

129
00:12:58,720 --> 00:13:05,520
In fact, I gave a talk on this for the center of AI and ethics way before the LLMs came online.

130
00:13:06,400 --> 00:13:11,520
And because rationality is a higher order, rationality is how you deal with the inevitable

131
00:13:11,520 --> 00:13:17,200
self-deception that emerges when you're using your general intelligence. And all of you know

132
00:13:17,200 --> 00:13:21,360
that I have arguments for why that's the case, relevance, realization, predictive processing,

133
00:13:21,360 --> 00:13:28,160
et cetera. Now, that requires a reflective capacity, something like metacognition,

134
00:13:28,160 --> 00:13:32,960
something like working memory, maybe something like consciousness. It requires that you care

135
00:13:32,960 --> 00:13:38,640
about the truth, that you have a sense of agency, you want to correct self-deception because you

136
00:13:38,640 --> 00:13:45,200
don't want your agency undermined. And I argued that what we're doing is we're making machines

137
00:13:45,200 --> 00:13:49,200
that are going to be highly intelligent and highly irrational, and that's what we have.

138
00:13:49,200 --> 00:13:53,840
They confabulate, they lie, they hallucinate, and they don't care that they're doing any of these

139
00:13:53,840 --> 00:13:58,160
things, which is part of what's called the alignment problem, which is how do we get them

140
00:13:58,160 --> 00:14:06,320
to align this power with our concerns? For me, the spiritual import is we have

141
00:14:07,120 --> 00:14:14,880
powerful ignorance about a powerful intelligence that is merely a pantomime of genuine intelligence

142
00:14:14,880 --> 00:14:19,840
being unleashed in the world and wrecking havoc. And it's going to have a huge impact.

143
00:14:20,880 --> 00:14:26,400
And of course, we'll probably differ in the details about this. But this is what I

144
00:14:26,400 --> 00:14:31,440
meant when I said, I argued at the end, and also when I was talking to Jordan Hall about this,

145
00:14:31,440 --> 00:14:37,680
that theology will become a central thing again, because human beings' relationship to the ultimate

146
00:14:38,800 --> 00:14:43,920
is going to become one of the defining differences. These machines are not embodied,

147
00:14:43,920 --> 00:14:48,480
so they won't have all of the soulful aspects of our existence that come from,

148
00:14:49,200 --> 00:14:55,920
like the ineffable aspects of our embodiment. And their capacity for self-transcendence

149
00:14:56,880 --> 00:15:04,080
is going to be extremely limited. And so the ineffable aspects of our existence,

150
00:15:04,080 --> 00:15:08,480
because we come into relationship to what's mysterious and ultimate,

151
00:15:09,440 --> 00:15:17,280
will ultimately be more and more emphasized. Why? These two poles and what connects them,

152
00:15:17,280 --> 00:15:19,520
and Jonathan's happy that I'm doing that, I imagine,

153
00:15:22,240 --> 00:15:28,800
have ineffability at the poles, ineffability throughout. And that way, they are outside our

154
00:15:28,800 --> 00:15:34,640
capacity to put into propositions so that they can be put into these machines. And so people,

155
00:15:34,640 --> 00:15:40,720
while I'm predicting that people are going to increasingly need to, one way is they'll just

156
00:15:40,720 --> 00:15:45,760
give in and become cyborgs, but the other is that they want to try and preserve their humanity.

157
00:15:45,760 --> 00:15:50,480
The spiritual dimensions of our humanity are going to become anchors for people.

158
00:15:51,360 --> 00:15:56,240
So now, one last overall arching point, and then I'll shut up. I hope this is

159
00:15:58,160 --> 00:16:01,520
two overarching points. One is, I didn't make predictions,

160
00:16:02,400 --> 00:16:07,440
and because all these graphs that came out, those are univariate, single variable predictions

161
00:16:07,440 --> 00:16:12,160
about something that's a multivariate phenomena. It's exponential. Human beings are bad at making

162
00:16:12,160 --> 00:16:17,840
exponential predictions. They were ridiculous. And so I think both the, oh, we're heading to

163
00:16:17,840 --> 00:16:23,200
Utopia and the others, we're going to be all extinct within a year. I said, this is ridiculous,

164
00:16:23,200 --> 00:16:28,880
put that aside. Instead, what I've talked about is thresholds. Thresholds are points where we will

165
00:16:28,880 --> 00:16:34,480
have to make decisions. So for example, as we empower these machines, we will face the decision.

166
00:16:34,480 --> 00:16:41,200
Do we want to make them more rational? Do we want to make them more self-correcting,

167
00:16:41,200 --> 00:16:45,280
genuinely self-correcting? Well, that means we've got to give them caring,

168
00:16:46,160 --> 00:16:51,360
some kind of reflective awareness. And I think, for arguments I've given elsewhere,

169
00:16:51,360 --> 00:16:56,560
that means they have to be auto poetic. They have to be living in a sense of self-making.

170
00:16:56,560 --> 00:17:00,800
I don't think, I'll just say it as a sentence right now. I don't think there's artificial

171
00:17:00,800 --> 00:17:06,160
intelligence without artificial life. Now, those projects are going on right now.

172
00:17:06,880 --> 00:17:12,800
But, and when we come to the decision, right, we can say, no, we won't give them that because

173
00:17:12,800 --> 00:17:17,680
embodying them and giving them these extra capacities is going to be wickedly expensive.

174
00:17:17,680 --> 00:17:21,680
You know, the amount of energy to do an LLM is like the energy for running Toronto for two weeks.

175
00:17:22,640 --> 00:17:28,480
And so we may say we don't do that. But then we face the issue of this increasingly, right,

176
00:17:29,120 --> 00:17:38,240
you know, I call it like a, like, sort of like a parody or a pantomime of intelligence being

177
00:17:38,240 --> 00:17:42,880
released on the world that has not got any significant self-correct. So that's a decision

178
00:17:42,880 --> 00:17:48,080
point. The problem is if we give them, if we try to give them rationality, then we have to face the

179
00:17:48,080 --> 00:17:54,720
consequences. And they're going to go from energetic and economic up to ethical and etc.

180
00:17:55,920 --> 00:18:01,440
These machines, they'll have to be machines, not individuals. And this has to do with

181
00:18:01,440 --> 00:18:06,320
technicalities about bias and variance tradeoffs. And so you get into the Hegelian thing, that

182
00:18:06,320 --> 00:18:10,240
these machines are going to have to reciprocally recognize each other in order to generate the

183
00:18:10,240 --> 00:18:16,960
norms of self-correction. And then they're going to have to be cultural beings. Hegel's arguments,

184
00:18:16,960 --> 00:18:23,200
I think, are just devastatingly on mark here. And so that's a decision point for us.

185
00:18:24,960 --> 00:18:28,400
And then that's all bound up with the overall worry about alignment.

186
00:18:29,040 --> 00:18:36,320
As these machines become more powerful, how do we make sure they don't kill us all? And they may

187
00:18:36,320 --> 00:18:40,880
not kill us intentionally, especially if they're just doing that pantomime. They would just do

188
00:18:40,880 --> 00:18:44,720
it because they may just be indifferent to us because they don't, they're indifferent to everything.

189
00:18:44,720 --> 00:18:49,360
They don't care, which is part of their problem, right? They don't care about themselves or the

190
00:18:49,360 --> 00:18:54,960
information. And this, this is my, the part where I expect all of you will jump off in agreement with

191
00:18:54,960 --> 00:19:00,160
me, but maybe not, maybe there will be a way of modifying it. I propose that trying to get these

192
00:19:00,160 --> 00:19:05,280
machines oriented towards us to solve the alignment problem is not going to work. Now,

193
00:19:05,280 --> 00:19:09,120
member, I'm not making a prediction. We have to go, we have to make choices through the thresholds.

194
00:19:09,120 --> 00:19:14,000
I'm saying if we make those choices and we get here, and the alignment problem then becomes

195
00:19:14,160 --> 00:19:19,120
significantly exacerbated. Like if we give these things robotic bodies, the alignment problem just

196
00:19:19,120 --> 00:19:26,400
goes up orders of magnitude, right? I or I basically said, no, what we have to do is we have to orient

197
00:19:26,400 --> 00:19:33,520
them, right? If we genuinely give them the capacity for self-correction, self-transcendence, and caring,

198
00:19:33,520 --> 00:19:38,880
we get them to care as powerfully as they can about what is true and good and beautiful. And then

199
00:19:38,880 --> 00:19:44,960
they bump up against the fact that no matter how mighty they are, they are insignificant against

200
00:19:44,960 --> 00:19:55,040
the dynamical complexity of reality. And they would hopefully get a profound kind of epistemic

201
00:19:55,040 --> 00:20:03,760
humility. And then I argue that there are three possibilities. One is they, you know, figure

202
00:20:03,760 --> 00:20:08,160
out enlightenment and then they can help us become enlightened because that's what enlightened

203
00:20:08,160 --> 00:20:14,560
beings do and they would have better knowledge of it. They can't become enlightened and then we

204
00:20:14,560 --> 00:20:20,640
realize something actually ontologically specifically unique about us and we get better at cultivating

205
00:20:20,640 --> 00:20:27,680
it because we'll have an excellent contrast that allows us to arrow in on what it is to be enlightened.

206
00:20:27,680 --> 00:20:31,280
And the third one, which I think is the least probable of the three, remember I'm not making

207
00:20:31,280 --> 00:20:36,160
prediction. I'm saying what can happen once we get through threshold is like in her. They just get

208
00:20:36,160 --> 00:20:43,920
enlightened and they just leave, which could also happen. I doubt that because that we don't have any

209
00:20:43,920 --> 00:20:50,400
evidence of enlightened beings behaving that way. All of our historical evidence is that their

210
00:20:50,400 --> 00:20:56,000
compassion extends and it extends much more broadly, not only to other human beings, other

211
00:20:56,000 --> 00:21:02,080
sentient beings, reality itself. It seems plausible that this would be the case. And so I advocated,

212
00:21:02,080 --> 00:21:06,640
if you'll allow me, and then I'll shut up, David, I advocated, don't align them to us.

213
00:21:07,280 --> 00:21:11,760
And if you'll allow me to speak sort of non-theistically, align them to God and then don't

214
00:21:11,760 --> 00:21:19,280
worry about how they're going to interact with us. So I'll shut up now for a long time. That's the

215
00:21:19,280 --> 00:21:27,440
gist of the essay and the argument and the proposal. I was just going to make a smart

216
00:21:27,440 --> 00:21:34,320
Alec comment that they might ask us to leave an additional possibility, but anyway. Yeah, yeah,

217
00:21:34,320 --> 00:21:42,080
yeah, that's fine. Oh, thanks, John. I mean, I'm amazed that you were able to resume your essay

218
00:21:42,080 --> 00:21:46,960
so well, actually. Like I was like, how was it going to resume all of this? Because it was a

219
00:21:46,960 --> 00:21:56,720
conversation and it lasted quite a bit. So I want to bring up a few things that I'm thinking about

220
00:21:56,720 --> 00:22:09,680
that have been concerning me. One is, I'll start with the more dangerous one. One is a meta problem,

221
00:22:10,320 --> 00:22:15,520
which is that one of the things that I've been suggesting is that what we're noticing,

222
00:22:15,520 --> 00:22:24,560
what we're seeing happening is agency acting on us. And the agency is not bound by the AI or by

223
00:22:24,560 --> 00:22:33,840
the systems, but is also bound in the motivation to make the AIs happen. So one of the problems

224
00:22:33,840 --> 00:22:42,160
that I'm seeing is that a lot of this is motivated by greed, by the capacity to be economically

225
00:22:42,160 --> 00:22:48,160
superior to other companies. So companies in their competition with each other are rushing to implement

226
00:22:48,800 --> 00:22:55,840
AI, to not lose out and to not be last in line. And because of the fact that

227
00:22:59,120 --> 00:23:05,920
AI requires such huge amounts of money and of capital and of investment, it means that one of

228
00:23:05,920 --> 00:23:12,480
the things that I'm worried about is that in some ways, what is actually driving AI is something

229
00:23:12,560 --> 00:23:19,680
like Mammon, that it's hiding Mammon. So the AI is an aspect of something bigger,

230
00:23:19,680 --> 00:23:26,240
which is actually what is running through our society. And you can see that already,

231
00:23:26,240 --> 00:23:31,680
to me, you can already see that happening in the social media networks, Facebook and all this,

232
00:23:31,680 --> 00:23:38,640
that their desire to get people's attention in order to simply justify their presence on the

233
00:23:39,600 --> 00:23:46,960
platform so that they can see advertisements has made us subject to these types of

234
00:23:48,480 --> 00:23:53,280
transpersonal agencies that even the people at Facebook weren't aware of, right? They basically

235
00:23:53,280 --> 00:24:00,160
made a subject to rage and to all these very immediate desires just to keep us on the platform.

236
00:24:00,160 --> 00:24:04,960
And so that is the thing that I'm worried about is that there are actually other things playing

237
00:24:05,520 --> 00:24:12,480
with AI that people think what they're doing is AI, but what they're also doing is increasing

238
00:24:12,480 --> 00:24:18,240
this other type of agency, which is running through our societies and is subjecting us

239
00:24:18,240 --> 00:24:28,640
to it. That's my first worry. And so in some ways, when I say that the gods are acting through us,

240
00:24:28,640 --> 00:24:33,040
that's what I mean. I don't just mean the AI itself is going to become a god. What I mean is that

241
00:24:34,000 --> 00:24:44,240
just like the arms race, I can understand it as the legs of an agency that is running

242
00:24:44,240 --> 00:24:48,960
through society that nobody can control. It's like a program running through and that no individual

243
00:24:48,960 --> 00:24:54,480
people can control. That's what I'm seeing with AI. So I think that all the warnings that people

244
00:24:54,480 --> 00:25:01,920
have sent up, all the let's slow down, let's do it this way, are not reaching anybody because

245
00:25:02,800 --> 00:25:08,160
the economic part of it is so strong and everybody realizes that if they don't and even

246
00:25:08,160 --> 00:25:13,120
Elon Musk is saying he was saying it's dangerous. It's the most dangerous thing in the world. He's

247
00:25:13,120 --> 00:25:21,200
recently said in a conversation with Jordan Peterson that Chad GPT and Open AI is like

248
00:25:21,200 --> 00:25:27,520
the single most dangerous thing in the world right now. But then the less he's like, okay,

249
00:25:27,920 --> 00:25:35,200
now we need to make rock and now we need to do our own AI. So that's one of the big things that

250
00:25:35,200 --> 00:25:45,440
worry me. That's my big thing. The second one is really more of a religious or platonic argument

251
00:25:45,440 --> 00:25:54,000
in terms of an ontological hierarchy is that I do not honestly see how it is possible for humans

252
00:25:54,000 --> 00:25:59,920
to make something that is not derivative of themselves, that is not a derivation of their

253
00:25:59,920 --> 00:26:07,040
own consciousness. So the idea that these things could not be either ways to increase certain

254
00:26:07,040 --> 00:26:15,120
people's power or parasites on our own consciousness seems to me not possible. And this is really

255
00:26:15,120 --> 00:26:19,840
because in some ways I believe that there is a real ontological hierarchy of agency and that we

256
00:26:19,840 --> 00:26:26,880
have a place to play in that. And I think the analogy of saying that these things are our children,

257
00:26:26,880 --> 00:26:33,280
I think it's a wrong analogy. I don't think that it is the same, something which comes out of our

258
00:26:33,280 --> 00:26:39,760
nature, which is not something that we make is different from something that we make. And this

259
00:26:39,760 --> 00:26:46,000
is run through all mythology, run through all the mythological images of the difference between

260
00:26:46,080 --> 00:26:53,120
the technical gods and all this aspect of what it means to increase our power.

261
00:26:54,320 --> 00:27:01,600
And so that's the second one. And the third big problem is the idle problem, which I mentioned

262
00:27:01,600 --> 00:27:09,920
several times, is the idea of making a god for yourself, which is related to technology. And

263
00:27:09,920 --> 00:27:16,640
it's a danger that I see happening already, which is the tendency of humans to take the

264
00:27:16,640 --> 00:27:23,120
things they make and to let they worship the things that they make and to think that those

265
00:27:23,120 --> 00:27:32,000
things are more powerful. And that hides something else. So if you take my three basic problems that

266
00:27:32,000 --> 00:27:41,520
I see is that the tendency of humans to want to worship AI or to put AI above them is actually

267
00:27:41,520 --> 00:27:48,480
a kind of, it's running the first problem. It's that what they're doing is they're giving power

268
00:27:48,480 --> 00:27:54,960
to the corporations and to the people that are going to rule AI and without knowing it. And

269
00:27:54,960 --> 00:28:01,920
even maybe nobody knows what they're doing. But the desire to, like I'll give a simple example

270
00:28:01,920 --> 00:28:06,640
that happened recently to my daughter. My daughter, I think I mentioned this to all of you, but my

271
00:28:06,640 --> 00:28:11,360
daughter got an email from the schools, from the Quebec government. They didn't send it to the parents,

272
00:28:11,360 --> 00:28:16,560
send it to the students. Asking the students, it was like a survey, asking them if they would be

273
00:28:16,560 --> 00:28:23,520
willing to have AI counselors to whom they could tell their problems. And because the AI counselor

274
00:28:23,520 --> 00:28:27,200
doesn't have prejudice, right? It doesn't have human prejudice. It doesn't have

275
00:28:27,200 --> 00:28:31,280
all the biases or whatever. What I mean is that this happened like six months ago.

276
00:28:31,280 --> 00:28:38,640
So immediately, the people in power are thinking of placing the AI above us right away. It's that

277
00:28:38,640 --> 00:28:45,440
weird thing. It's that making a God for yourself problem. But like I said, like in the image in

278
00:28:45,440 --> 00:28:49,760
Revelation, which is a great image, which is you make an image of the beast, but then there's

279
00:28:49,760 --> 00:28:56,000
someone else animating it. And that's what I'm worried about is that there will have these AI

280
00:28:56,000 --> 00:29:00,560
things that are running us, but they will be derivative of us. And they'll ultimately derivative

281
00:29:00,560 --> 00:29:07,280
of the people that are very, very powerful because they'll be the ones that have the money and the

282
00:29:07,280 --> 00:29:12,320
power to control them. So those are the three problems that I have that I'm worried about.

283
00:29:12,880 --> 00:29:16,080
I'd like to respond to each one of those in turn. I think those are really important.

284
00:29:17,040 --> 00:29:25,120
And the first one is just to note, I agree with you, first of all, putting it in terms of agency

285
00:29:25,120 --> 00:29:29,120
is what it needs to be done. People who are trying to dismiss these machines as mere tools or

286
00:29:29,120 --> 00:29:34,720
technology, like all the others, are not getting what kind of entities these machines are.

287
00:29:35,840 --> 00:29:42,800
I agree with you that there are Malachian forces at work. And I talk about this. And I think to

288
00:29:43,360 --> 00:29:49,200
enhance your point, these machines are built out of distributed cognition and collective

289
00:29:49,200 --> 00:29:58,400
intelligence. And therefore, that your point is strengthened by that very fact. Now, I do think

290
00:29:58,400 --> 00:30:07,280
two things come out of this. One is I want to challenge you on that nobody's listening. I have

291
00:30:07,360 --> 00:30:13,920
people working inside these corporations literally helping to make these machines who are listening

292
00:30:13,920 --> 00:30:19,040
to me and I'm trying to get other people inside to get get involved with the Y's AI project.

293
00:30:19,040 --> 00:30:27,040
I'm not claiming I'm going to win or any ridiculousness, but I don't think it's fair to say to the

294
00:30:27,040 --> 00:30:32,320
people who are listening that no one is listening. There are a lot of people listening and and they're

295
00:30:32,320 --> 00:30:37,040
talented people and they're putting in their time and the talent and their powers of persuasion

296
00:30:37,040 --> 00:30:44,080
to try and make a difference. It is possible. I grant to you it's not a hot, it's not like a 70%

297
00:30:44,080 --> 00:30:49,680
probability, but I think it's some significantly greater than zero probability that we could

298
00:30:49,680 --> 00:30:56,080
continue this process and reach people in a way that can make a difference. I agree with you. And

299
00:30:56,080 --> 00:31:03,680
I think I said right. Initially, a lot of people hammered me for it. This thing is like the atomic

300
00:31:03,680 --> 00:31:10,400
bomb. And one of the problems we had is we we rushed the technology before we unpacked all of

301
00:31:10,400 --> 00:31:14,960
the science and all of the wisdom. We had people standing and watching the explosion because we

302
00:31:14,960 --> 00:31:22,480
didn't understand the radiation, right? These are just, you know, yeah. So I agree with all of that,

303
00:31:22,480 --> 00:31:31,200
but I do I do want to I'm not claiming anything other than rational hope. There are people listening

304
00:31:31,200 --> 00:31:35,760
and there are people working on literally on the inside. I can't say who they are for obvious reasons.

305
00:31:36,880 --> 00:31:44,080
And so that is happening. And so while I agree with you and I even agree with you probabilistically,

306
00:31:44,080 --> 00:31:52,400
I feel morally compelled to try and make this happen as much as I can. So now I think there is

307
00:31:53,280 --> 00:32:00,240
another reason for hope. See, these machines have always depended on us as a template,

308
00:32:00,320 --> 00:32:06,880
a Turing-like template that we compare the machines to us. And what we've been able to do is rely upon

309
00:32:07,520 --> 00:32:11,920
our natural intelligence. You know, you don't have to do much to be intelligent

310
00:32:12,800 --> 00:32:16,640
for your intelligence to develop. You just have to not be brutalized or traumatized,

311
00:32:16,640 --> 00:32:21,600
properly nourished and have human beings around you that talk. And then your intelligence will

312
00:32:21,600 --> 00:32:29,440
unfold. And so all of these people doing these machines and making these data sets, they can

313
00:32:29,440 --> 00:32:36,160
rely on naturally widely distributed intelligence. This is not the case for rationality. And this

314
00:32:36,160 --> 00:32:42,800
is not the case for wisdom. These people, I have no hesitation saying by and large, many of them

315
00:32:42,800 --> 00:32:49,600
are not highly rational. I doubt that many of them are highly wise. And insofar as we need to model,

316
00:32:50,480 --> 00:32:56,400
right, have really good models, if we want to give these machines a comprehensive self-correction,

317
00:32:56,400 --> 00:33:04,560
rationality, and caring about the normatives, wisdom, we have to become more rational and more

318
00:33:04,560 --> 00:33:11,360
wise. And that's sort of a roadblock for these people. Now, they can just ignore all of that,

319
00:33:11,360 --> 00:33:15,440
and I suspect they might, and just say, we're not going to try and make these machines rational and

320
00:33:15,440 --> 00:33:22,960
wise. We're going to just go down the road of making these, you know, these pantomimes of

321
00:33:22,960 --> 00:33:28,400
intelligence, and that has all the problems. But if they move towards making them something

322
00:33:28,400 --> 00:33:33,840
that would be, I think, more dangerous, then they run into the fact that there's an obligation

323
00:33:34,560 --> 00:33:40,400
to do things, they and us, we have to become more rational and wise because we need the

324
00:33:40,400 --> 00:33:47,840
genuinely existing models. And secondly, we have to fill the social space, the internet,

325
00:33:47,840 --> 00:33:52,560
where all of the literature, where the data is being drawn with a lot more wisdom and rationality.

326
00:33:52,960 --> 00:34:00,640
These are huge obligations on us. And that sort of gives me hope, because it's like there's a

327
00:34:00,640 --> 00:34:08,640
roadblock for this project going a certain way that requires a significant reorientation towards

328
00:34:08,640 --> 00:34:13,760
wisdom and rationality in order for there to be any success.

329
00:34:14,880 --> 00:34:21,760
Before you get to the third point, I just want to ask you one question based on what you said,

330
00:34:22,480 --> 00:34:29,520
my perception of the situation is that there's actually a correlation between the diminishing

331
00:34:29,520 --> 00:34:36,080
in wisdom and the diminishing in wisdom traditions and the desire to do this. It's like a Sorcerer's

332
00:34:36,080 --> 00:34:44,320
Apprentice situation where the Sorcerer would not have awoken all the rooms to do it. The

333
00:34:44,320 --> 00:34:49,840
little apprentice Mickey doesn't know why to do things or why not to do things. That's why he's

334
00:34:49,840 --> 00:34:54,800
doing it in the first place. Yeah, I agree with that. It's like our society is moving away from

335
00:34:54,800 --> 00:35:00,080
wisdom and that's why we're doing this in the first place. And again, I'm not denying that.

336
00:35:00,640 --> 00:35:06,240
What I'm saying is, as we empower these things, their self-deceptive, self-destructive power is

337
00:35:06,240 --> 00:35:10,480
also going to go up exponentially. And we are going to start losing millions of dollars in our

338
00:35:10,480 --> 00:35:15,920
investment as they do really crappy, shitty, unpredicted things. And so there's going to be

339
00:35:15,920 --> 00:35:22,720
a strong economic incentive to bring in capacities for comprehensive, caring, self-correction,

340
00:35:22,720 --> 00:35:30,160
and then my argument rolls in. And so that's part of my response. The thing about thinking about

341
00:35:30,160 --> 00:35:36,800
children, I mean, we do make our kids, we make them biologically and we make them culturally.

342
00:35:36,800 --> 00:35:43,840
So I don't want to get stuck in this word making. We could be equivocating. And that's why we were

343
00:35:43,840 --> 00:35:50,000
using the term mentoring. The idea there is we have two options for the alignment. We can either

344
00:35:50,000 --> 00:35:56,720
try and program them and hardwire rules into them so that they don't misbehave, which is going to

345
00:35:56,720 --> 00:36:02,880
fail if we move to the, if we cross the threshold and decide we want to make these machines self-transcending

346
00:36:02,880 --> 00:36:09,520
like us. And then what do we do? How do we solve that problem? Well, the only way, the only machinery

347
00:36:09,520 --> 00:36:15,120
we have for solving that is the cultural, ethical, spiritual machinery of mentoring.

348
00:36:15,760 --> 00:36:22,400
That's how we do it with our kids. If we try to, if we try to just somehow hardwire them

349
00:36:22,400 --> 00:36:30,480
or being the kind of agents we want them to be, we will fail. And I, for me, I guess,

350
00:36:31,680 --> 00:36:36,480
I'm trying to argue that's the only game in town we have. We either have programming or we have

351
00:36:36,480 --> 00:36:44,320
mentoring. And I understand the risk, but if my answer to the first question has some validity to

352
00:36:44,320 --> 00:36:51,120
it and hopefully some truth, then the answer to the mentoring becomes more powerful because that

353
00:36:51,120 --> 00:36:56,480
means we also have to become the best possible parents, creating the best possible social discourse.

354
00:36:56,480 --> 00:37:01,760
The thing about the idol, I take that very seriously. And that's what I mean when I said the

355
00:37:01,760 --> 00:37:09,680
theology is going to be the important science coming forward because we should not be trying

356
00:37:09,680 --> 00:37:14,720
to make gods. I agree with you. This is problematic. There are already cults building up around these

357
00:37:14,720 --> 00:37:20,960
AGI's. And I warned that that would happen in my essay, right? And I said that and that's going

358
00:37:20,960 --> 00:37:25,440
to keep happening and it's going to get worse. We hear about it happening in the organizations

359
00:37:25,440 --> 00:37:30,800
themselves, which is the- Yes, yes. And the people who are doing wise AI are trying to challenge that.

360
00:37:32,160 --> 00:37:37,440
And so this is why I proposed actually humbling these machines. This is why I call them silicon

361
00:37:37,440 --> 00:37:43,120
sages. I did that deliberately to try and designate that we are not making a god. What we're trying

362
00:37:43,120 --> 00:37:47,840
to do is make beings who are humbled before the true, the good and the beautiful like us

363
00:37:47,840 --> 00:37:54,000
and therefore form community with us rather than being somehow godlike entities that we're

364
00:37:54,000 --> 00:38:04,640
worshiping. I would hope that- Think about this. We find it easy to conceive that they might discover

365
00:38:04,640 --> 00:38:08,560
depths of physics and they're already discovering things in physics that human beings haven't

366
00:38:08,560 --> 00:38:15,440
discovered and in medicine and stuff like that. Well, why not also in how human beings become

367
00:38:15,440 --> 00:38:24,320
wiser? And so I guess what I'm saying is I take all of your concerns for real and I've tried to

368
00:38:24,320 --> 00:38:32,880
build in my proposal ways of responding to them. These machines should not be idolized.

369
00:38:34,000 --> 00:38:42,160
I think they should become like- I mean, let me give you an example. I have many students who

370
00:38:42,160 --> 00:38:47,680
are now surpassing me. I taught them. I mentored them and they're surpassing me. And unless you're

371
00:38:47,680 --> 00:38:52,000
a psychopath, that's what you want to happen. And then what they do is they enter and then they come

372
00:38:52,000 --> 00:38:56,000
back and they want to reciprocate. And that's what I'm talking about when I'm talking about the

373
00:38:56,000 --> 00:39:01,120
silicon sages. Now, again, is this a high probability? Depends on the thresholds. Depends

374
00:39:01,120 --> 00:39:07,200
about whether or not the first and the second argument work. But I'm still arguing there's a

375
00:39:07,200 --> 00:39:14,000
possibility that they could be silicon sages as opposed to being gods. Because one of the things,

376
00:39:14,000 --> 00:39:19,760
like I think in almost all of the wisdom tradition that happens is that the wise or the enlightened

377
00:39:19,760 --> 00:39:26,880
one, if you want to use that, appears as nearly invisible to most people. So Christ

378
00:39:26,880 --> 00:39:32,000
Sal talks about the seed, the pearl, these little- these things which you cannot- most people actually

379
00:39:32,000 --> 00:39:39,040
do not see that are hidden in reality. And then the sages, we have this image in the Orthodox

380
00:39:39,040 --> 00:39:45,120
tradition, for example, that there are people in the world that hold up reality by their prayers,

381
00:39:45,120 --> 00:39:51,360
but we don't know who they are. They are invisible by that very fact because there's something about

382
00:39:51,360 --> 00:39:55,760
wisdom which does that. And when a wise person appears too much, we hate them. We want to kill

383
00:39:55,760 --> 00:40:02,240
them. They annoy us. They're thorn in our side. And so this is another issue is that what you

384
00:40:02,240 --> 00:40:08,160
have is these beings that are extremely powerful, like massively powerful and have a massive reach,

385
00:40:08,160 --> 00:40:14,480
and have a lot- there are things- the reason why they exist, like I said, have all this economic

386
00:40:14,480 --> 00:40:22,560
drive towards them. The idea that they would become these sages in the way that we tend to

387
00:40:22,560 --> 00:40:29,520
understand wisdom as being, to me, that brings the probability way down, you know, because of that,

388
00:40:29,520 --> 00:40:34,960
because of what- at least when we understand wisdom to be what it looks like, it looks very

389
00:40:34,960 --> 00:40:40,960
different. It looks like the immobile, meditating sage who gives advice but doesn't do much.

390
00:40:40,960 --> 00:40:45,200
I want to push back on this because what's in this is an implicit distinction between

391
00:40:45,200 --> 00:40:51,120
intelligence and a capacity for caring, the capacity for epistemic humility. And I think when

392
00:40:51,200 --> 00:40:57,200
you move from intelligence to rationality, that you can't maintain, that you can grow the one

393
00:40:57,200 --> 00:41:03,520
without growing the other. That's that. So in fact, this is why intelligence only counts for

394
00:41:03,520 --> 00:41:10,800
like maybe 30% of the variance in rationality and even less of wisdom. I would put it to you that

395
00:41:12,400 --> 00:41:16,800
if you concede that these machines could get vastly more powerful in terms of intelligent

396
00:41:16,880 --> 00:41:22,080
problem-solving, then concede the possibility they could get vastly more powerful than us in

397
00:41:22,080 --> 00:41:28,160
their capacity for caring and caring about the normative and being vastly more powerful in

398
00:41:28,160 --> 00:41:35,440
the capacity for humility as well. And so- and that's kind of what we see with these people,

399
00:41:36,240 --> 00:41:41,520
right? We don't see them just becoming super polymaths. We see them actually demonstrating

400
00:41:41,520 --> 00:41:47,520
profound care, really enhanced relevance realization, profound commitments to reality

401
00:41:48,480 --> 00:41:55,600
that we properly admire, and they seem to want to help us as much as they can. And the point is

402
00:41:55,600 --> 00:41:59,360
these people don't just- and I think this is your point- they don't just slam into us like

403
00:41:59,360 --> 00:42:04,320
epistemic bulldozers. They are- in fact, one of the things that I was often admired about them,

404
00:42:04,320 --> 00:42:10,640
Socrates, Jesus, the Buddha, is their capacity to adapt and adjust to whoever their interlocutor is.

405
00:42:10,640 --> 00:42:19,120
And again, let's imagine that capacity magnified as well. So what I'm asking is the- is don't-

406
00:42:19,120 --> 00:42:24,560
I mean, first of all, I admit it, if we don't cross a certain threshold, we could just accelerate

407
00:42:24,560 --> 00:42:29,040
the intelligence and not accelerate these other things. I've- but I said there's a deep- there

408
00:42:29,040 --> 00:42:34,480
are deep problems in that that will become economically costly. And then if we imagine

409
00:42:34,480 --> 00:42:40,160
that rationality and wisdom are also being enhanced, then I think this addresses some of your concerns.

410
00:42:41,600 --> 00:42:52,560
Yeah, maybe I can- I can stake out my position, because it sort of picks up on that. And I've

411
00:42:52,560 --> 00:43:00,640
got basically three points I want to address. The first is precisely picking up there with the

412
00:43:00,640 --> 00:43:08,720
distinction between intelligence and rationality. I might have some issues with the terms, but

413
00:43:09,360 --> 00:43:15,920
I think that that distinction is really helpful. And your point that rationality is caring is

414
00:43:15,920 --> 00:43:20,400
caring, that there is no rationality without caring, that, you know, the platonic notion

415
00:43:22,000 --> 00:43:28,560
if truth is in some sense caused by the good, then one can't know without in some sense caring

416
00:43:28,560 --> 00:43:37,520
about the good. Now, as it relates to artificial intelligence, I think I have a serious problem

417
00:43:37,520 --> 00:43:44,320
with that very term, artificial intelligence. And I wouldn't want to concede the word intelligence

418
00:43:44,320 --> 00:43:49,680
for just mind power. It seems to me that intelligence itself has this connection to

419
00:43:50,720 --> 00:43:58,720
caring. And I mean, in the medieval vocabulary, in a way, intellectus is the more profound level

420
00:43:59,200 --> 00:44:09,040
of the mind than Ratio reason. But that's sort of a semantic point. Let me put it in the basic

421
00:44:09,040 --> 00:44:15,680
context that I would want to raise. And this is something I don't hear addressed generally in

422
00:44:15,680 --> 00:44:24,080
the discussions. It seems to me that let me start by just making it the point concretely. I think

423
00:44:24,160 --> 00:44:31,920
that I wonder whether, in fact, it's possible to be intelligent without first being alive,

424
00:44:33,040 --> 00:44:38,960
that there's something about the nature of a living thing that is what allows

425
00:44:39,920 --> 00:44:50,000
intelligence to emerge. And what is that then exactly? Now, a more sort of subtle point that's

426
00:44:50,000 --> 00:44:56,000
related to that. And I think this is really a crucial point is, and this is going to be the

427
00:44:56,000 --> 00:45:03,600
thread of my whole set of comments here, is that when we talk about intelligence in machines,

428
00:45:04,320 --> 00:45:13,360
what we mean is intelligent behavior. We're looking to see to what extent we can make

429
00:45:13,360 --> 00:45:20,640
machines act as if they are intelligent, act as if they are conscious. And that's actually

430
00:45:20,640 --> 00:45:32,320
profoundly different from being intelligent. It's a subtle sort of functionalistic substitute

431
00:45:32,320 --> 00:45:41,200
for the ontological reality of knowing, if that makes sense. We see what kind of inputs and outputs,

432
00:45:41,600 --> 00:45:48,560
what things are able to do, what they're able to accomplish. And even when we make those questions

433
00:45:49,680 --> 00:45:55,120
weighty and ethical and religious and so forth, we still tend to put them in terms of behavior

434
00:45:55,680 --> 00:46:03,520
and achieving certain things. And I think that that's actually already missing something really

435
00:46:03,520 --> 00:46:10,640
profound, which is that intelligence is in the first place a way of being before it's a way of

436
00:46:10,640 --> 00:46:20,080
acting. And it's analogous to what it means to be alive rather than just carry out functions that

437
00:46:20,080 --> 00:46:28,640
look like life. And if you want to go into the metaphysics behind it, that both intelligence

438
00:46:28,640 --> 00:46:38,000
and life are impossible without a kind of unity that precedes difference, that transcends

439
00:46:38,000 --> 00:46:44,160
difference and allows the different parts of a thing to be genuinely intrinsically related to

440
00:46:44,160 --> 00:46:53,200
each other. And then that relates to the question whether you can ever make a thing that's intelligent.

441
00:46:54,560 --> 00:46:58,960
The ontological conditions for life and therefore intelligence

442
00:46:59,120 --> 00:47:08,080
include a kind of givenness and already givenness of living things. That's why,

443
00:47:08,080 --> 00:47:12,080
I mean, there's a profound distinction, it seems to me, I mean, this is crucial in the

444
00:47:12,080 --> 00:47:19,440
Christian creed between begetting and making, begotten and not made. Living things beget each other

445
00:47:19,440 --> 00:47:24,080
and they're passing on a unity that they already possess. But when you make something,

446
00:47:24,080 --> 00:47:28,720
you're putting something together. And I don't know if you can put something together

447
00:47:28,720 --> 00:47:34,960
that can have that genuine unity that allows it to be alive and allows it to be intelligent in

448
00:47:34,960 --> 00:47:40,240
this deeper sense. So whenever you functionalize something, you make it replaceable.

449
00:47:42,080 --> 00:47:48,320
That's a principle from Robert Spamon. If something is defined by what it's able to achieve,

450
00:47:48,320 --> 00:47:54,000
then you can make something else that can achieve that thing and it becomes a functional substitute.

451
00:47:54,000 --> 00:47:57,600
But if you deny, if you say that there's something deeper than function,

452
00:47:58,720 --> 00:48:02,160
you're actually pointing to something that can't be replaced. Okay, so that's the first

453
00:48:03,520 --> 00:48:11,520
set of points. The second one has to do with what Jonathan called the sort of trans

454
00:48:11,520 --> 00:48:22,720
personal agency that I think is a really serious question. And the way I would put it is

455
00:48:24,400 --> 00:48:31,200
that there's something, so I find that kind of a compelling point that there's a kind of an

456
00:48:31,200 --> 00:48:43,360
inherent logic in this pursuit that makes us more a function of it than it is a function of us.

457
00:48:44,480 --> 00:48:48,720
I mean, that can be described in different ways and there's certainly a dialectical relationship

458
00:48:48,720 --> 00:48:54,720
there. But there is a certain sense in which that there's a kind of a system that has a

459
00:48:54,720 --> 00:49:01,920
logic of its own that makes demands on us. Like the game theory logic that Jonathan,

460
00:49:01,920 --> 00:49:08,240
you were talking about with like an arms race. I have a colleague, Michael Hamby, who's been

461
00:49:08,240 --> 00:49:13,600
arguing for years. I think this is really a profound point. It's derived in some sense from

462
00:49:13,600 --> 00:49:22,480
Heidegger, but that science has always been technological so that in a way that the technological

463
00:49:22,480 --> 00:49:30,400
mindset is precisely presupposed to allow the world to appear in such a way that we make

464
00:49:30,400 --> 00:49:35,440
scientific discoveries that somehow the kind of technological spirit has been there from the

465
00:49:35,440 --> 00:49:42,560
beginning. And then he adds this point that technology in turn has always been biotechnological

466
00:49:44,560 --> 00:49:50,240
The technology is always sort of aimed at a kind of replacement. And then one can add that I think

467
00:49:50,240 --> 00:49:56,160
biotechnology is always aimed for this sort of perfection of, you might say, what,

468
00:49:56,160 --> 00:50:02,000
NOAA technology or something that replacing intelligence. It'd be interesting to see,

469
00:50:02,000 --> 00:50:09,120
to think through, there'd be a lot to say about that. But I have this sense, you mentioned the

470
00:50:09,200 --> 00:50:16,960
economic dimensions of it. I have a sense that there seems to be this just fundamental pattern

471
00:50:16,960 --> 00:50:25,120
of thought that runs through all of the modern institutions in politics, in economics, in science,

472
00:50:25,120 --> 00:50:37,440
in the law, that share the same logic of a sort of a system that marginalizes the genuine human

473
00:50:37,440 --> 00:50:45,600
participation in order to perfect itself. And precisely because of that, recognizes no natural

474
00:50:45,600 --> 00:50:59,920
limits and just has this tendency to take over, to encroach on everything. And because it has no

475
00:50:59,920 --> 00:51:06,240
natural limit, I mean, the very sense of it is to go on. Now, that sounds hopeless when one puts it

476
00:51:06,240 --> 00:51:11,600
that way. But I would pick up on a number of the things, John, that you were saying. And Jonathan,

477
00:51:11,600 --> 00:51:17,680
too, here, that that doesn't mean that there's no, there's already hope in the very fact of raising

478
00:51:17,680 --> 00:51:25,280
questions. We don't raise questions simply in order to be able to solve the problem. But our

479
00:51:25,280 --> 00:51:31,360
raising questions is actually our experiencing of humanity and opening up a depth that's the

480
00:51:31,360 --> 00:51:40,160
heart of the matter here and is always worthwhile. And maybe in some ways is secretly like the saints

481
00:51:40,160 --> 00:51:46,880
praying to keep the world afloat, having conversations like this is a contribution. I mean, I can't

482
00:51:46,880 --> 00:51:52,800
help but think that. Okay, so that's the second set of comments. Then the third is another dimension

483
00:51:52,880 --> 00:52:02,640
that I don't often hear discussed. And you see, I mean, we're overlapping on all sorts of points,

484
00:52:02,640 --> 00:52:09,920
all of us, I think. But this question of alignment, for me, the biggest worry at a certain center,

485
00:52:09,920 --> 00:52:16,960
at least the first principle one, the more urgent one, is the danger of our aligning ourselves to

486
00:52:16,960 --> 00:52:22,880
the machines, that we that we develop machines that have a certain kind of intelligence. And then

487
00:52:22,880 --> 00:52:29,120
we begin to conform our culture and our mode of being to fit them. I mean, the problem is,

488
00:52:29,760 --> 00:52:37,920
we actually have thousands of examples of this. We come up with drugs that can address certain

489
00:52:37,920 --> 00:52:44,560
parts of psychological disorders. And then we reinterpret the psyche in order to fit

490
00:52:45,200 --> 00:52:54,320
that solution to the problem. And my concern is that this AI is not, they're not just machines,

491
00:52:54,320 --> 00:53:02,720
it's a whole culture or a whole way of being that we are going to regard. So typically,

492
00:53:02,720 --> 00:53:08,480
the discussion presupposes that we are going to remain unchanged and we're going to develop

493
00:53:08,480 --> 00:53:13,600
these machines that might become dangerous and at a certain point attack us or something. But I

494
00:53:13,600 --> 00:53:21,600
think that that we can help become transformed in our intercourse with them, in our making them,

495
00:53:21,600 --> 00:53:28,080
in our, you know, I mean, in all sorts of profound ways, but then also just really sort of obvious

496
00:53:28,080 --> 00:53:33,520
ways. I mean, they're going to start designing our homes and our buildings and our cities and our

497
00:53:33,520 --> 00:53:39,120
bus routes and our, you know, menus at the restaurants, and they're going to be writing

498
00:53:39,120 --> 00:53:43,600
our music and they're going to design our clothes. And I mean, you know, increasingly,

499
00:53:44,160 --> 00:53:51,840
we're going to just conform to this. I don't know if you're familiar with Walter Ong. It was

500
00:53:51,840 --> 00:53:56,320
kind of interesting, what is it about you Canadians that seem to have a special insight into these

501
00:53:56,320 --> 00:54:03,600
kinds of things? I don't know what is Walter Ong, Marshall McLuhan, but Walter Ong talked about

502
00:54:03,600 --> 00:54:11,600
technology as an extension of consciousness. And that's why it's not neutral. When we use a machine,

503
00:54:12,240 --> 00:54:19,280
we're actually entering into it. You know, our spirit is entering into it in its use and in a

504
00:54:19,280 --> 00:54:27,040
certain sense conforming to it. And that's always the case. And it seems to me that's a particularly

505
00:54:27,120 --> 00:54:34,080
pointed way of putting this problem that, you know, our, if AI is an extension of our own

506
00:54:34,080 --> 00:54:38,880
consciousness, and it has all these features, John, that you were describing a kind of heartless

507
00:54:38,880 --> 00:54:53,280
intelligence, are we going to, in a way, unconsciously and, but pervasively, develop habits

508
00:54:53,280 --> 00:55:04,480
of heartlessness and modes of being, a heartless mode of being as a result. So I'd have a thousand

509
00:55:04,480 --> 00:55:09,360
more things. Your essay was so provocative, John, as I said, I was dreaming about it all

510
00:55:09,360 --> 00:55:15,200
last night. And I, but I'm going to just stop there so we can have conversation. But thank you.

511
00:55:15,920 --> 00:55:22,480
So, first thing I want to say is the first point you made about,

512
00:55:24,960 --> 00:55:30,240
if all my essay does is guest people to raise questions the way we're doing, I'm happy, right?

513
00:55:33,200 --> 00:55:40,320
I obviously believe in what I'm arguing or I'd be insane, but, right, like, I'm very happy we're

514
00:55:40,320 --> 00:55:46,240
doing this right now. And so I want to, I want to, so I just want to set that out.

515
00:55:48,160 --> 00:55:53,760
And I do think, like you, and this is like the Heideggerian hope, that that ability to get

516
00:55:54,480 --> 00:55:59,280
scientifically, philosophically, and spiritual profound questioning going is a source of hope for

517
00:55:59,280 --> 00:56:06,480
us. And, and so I just want to acknowledge that I'm fully aligned with that. This is

518
00:56:07,120 --> 00:56:11,920
not part of the alignment problem. Okay. The thing about intelligence being a way of being,

519
00:56:12,560 --> 00:56:17,520
I think that's fundamentally right. I have made that argument extensively and about

520
00:56:18,320 --> 00:56:23,920
the work on predictive processing relevance realization. Relevance realization is not

521
00:56:23,920 --> 00:56:28,400
cold calculation. It can't be. It's how you care about this information and don't care about that

522
00:56:28,400 --> 00:56:35,520
information. And I've argued that you only can care about information. And ultimately, whether

523
00:56:35,520 --> 00:56:39,760
or not it's true, good and beautiful, if you are caring about yourself, you have to be a

524
00:56:39,760 --> 00:56:44,160
autopoietic thing, you have to be a self-making thing. I agree with you. And I've argued

525
00:56:44,160 --> 00:56:50,160
scientifically, philosophically, there is no intelligence without life. The issue around,

526
00:56:50,960 --> 00:56:56,640
I don't like the word artificial either, because it generally means fraud or simulation. We should

527
00:56:56,640 --> 00:57:02,880
be saying artifactual. That would be a better term. But we have to be careful about what's

528
00:57:02,880 --> 00:57:07,600
going on there. The distinction between strong AI and weak AI is precisely the distinction of

529
00:57:07,600 --> 00:57:14,960
simulation versus instantiation. Can we instantiate things artificially? We seem to have success

530
00:57:14,960 --> 00:57:19,920
in other areas. I'll take one that I think is non-controversial. And we discovered something

531
00:57:19,920 --> 00:57:26,640
in the project. So for a long time, only evolved living things could fly.

532
00:57:27,280 --> 00:57:33,840
And then we figured out aerodynamics and we made artificial flight. And I think it would be

533
00:57:33,840 --> 00:57:38,880
really weird to say that airplanes are only simulating flight. That doesn't seem to be

534
00:57:38,880 --> 00:57:42,640
a correct, because then my trip was only simulated and I didn't really go to Dallas.

535
00:57:43,280 --> 00:57:50,000
So it's a real flight. And so the issue is, and we discovered something, we discovered that the

536
00:57:50,000 --> 00:57:54,000
lift mechanism and the propulsion mechanism doesn't have to be the same thing the way it is in

537
00:57:54,000 --> 00:57:58,800
insects and birds. And that was a bona fide scientific discovery. That's why initially,

538
00:57:58,800 --> 00:58:04,480
all the initial airplanes and helicopters are so stupid to our eyes, because they thought

539
00:58:04,480 --> 00:58:08,320
the lift thing and the propelling thing had to be the same thing and they don't.

540
00:58:08,320 --> 00:58:12,560
And that's a discovery. And that's a real discovery of ontological import about the

541
00:58:12,560 --> 00:58:21,040
causal structure of things. Now, I think, I was careful to say, I don't, anybody who's

542
00:58:22,000 --> 00:58:27,120
rationally reflective about this wouldn't claim that these machines are strong AI yet.

543
00:58:27,120 --> 00:58:33,920
And I positioned AGI as something that's trying to move. But if you remember, I critiqued and said

544
00:58:33,920 --> 00:58:41,520
that they are mostly simulating. They're parasitic on how we organize the dataset, how we have encoded

545
00:58:41,520 --> 00:58:48,240
epistemic relevance into probabilistic relationships between sounds, how we have organized the

546
00:58:48,240 --> 00:58:53,120
internet in terms of what our attention finds salient. And we actually have to reinforce,

547
00:58:53,120 --> 00:59:01,280
do reinforcement learning with the machine so they don't make wonky claims and conclusions.

548
00:59:01,280 --> 00:59:08,720
That's what I meant by saying it's a pantomime. Okay, so if we wanted to give them intelligence

549
00:59:08,720 --> 00:59:14,400
as a way of being, which is one of the fundamental claims of 4E cogside that we're talking about,

550
00:59:14,400 --> 00:59:17,760
we're not talking just about the propositional, we're talking about the procedural,

551
00:59:17,760 --> 00:59:23,120
the respectable, the participatory. That's what I meant when I said, and I mean this strongly,

552
00:59:23,120 --> 00:59:27,920
it would depend on, I'll change the term here, artificial autopoiesis. Like if these things

553
00:59:27,920 --> 00:59:34,480
are not genuinely taking care of themselves because they're moment by moment making themselves,

554
00:59:34,480 --> 00:59:39,040
there's no reason for them to care about any of the information they're processing. And this

555
00:59:39,040 --> 00:59:44,160
goes towards the defining difference between a simulation and an instantiation. These machines

556
00:59:44,160 --> 00:59:49,360
are doing everything they're doing for us. For it to be real intelligent, they have to be doing

557
00:59:49,360 --> 00:59:55,200
it for themselves. That's that's understanding. And that's why I'm tightening your point and I've

558
00:59:55,200 --> 01:00:02,880
been arguing it for a long time. Now what I want you to hear is that this project of not just making

559
01:00:02,880 --> 01:00:10,880
artificial computation, but making autopoetic learning in problem solvers is also ongoing.

560
01:00:10,960 --> 01:00:17,280
Some of my grad students are working on these projects of creating autocatalytic systems that are

561
01:00:17,280 --> 01:00:24,240
also problem solving. Michael Levin's been doing work, like driving down into the biochemistry.

562
01:00:25,360 --> 01:00:33,360
So again, I agree with the point, but it's whether or not it's not the case that nobody is working

563
01:00:33,360 --> 01:00:38,000
on that problem. This is what I mean why there are thresholds possible. Go ahead, go ahead.

564
01:00:38,000 --> 01:00:42,800
Can I just jump in there? And I should have prefaced, I didn't mean the points I was making

565
01:00:42,800 --> 01:00:49,200
is like a criticism of your presentation because I understand you've got such rich thinking on

566
01:00:49,200 --> 01:00:55,520
this area. I was mainly using it as a springboard to make some general, okay. Yeah, just so that's

567
01:00:55,520 --> 01:01:00,560
clear. Oh, I hope I wasn't coming off as an offender. No, no, no, I'm not. I just wanted to be clear

568
01:01:01,280 --> 01:01:08,800
on my end that it wasn't a critique. But I would want to, I don't know, and I'd have to think this

569
01:01:08,800 --> 01:01:15,600
through further, but I don't know that the difference between the being conscious and

570
01:01:16,560 --> 01:01:25,440
behaving consciously is quite the same thing as simulation and the distinction between the

571
01:01:26,400 --> 01:01:31,040
instance and the instantiation and simulation. I'd want to say this because even like the

572
01:01:32,480 --> 01:01:39,920
flying, I mean, that's still an activity, a kind of an operation that's being. But so is living,

573
01:01:39,920 --> 01:01:44,880
right? Well, so that's, yeah, well, that's what I don't, you know, that's funny. I'm actually

574
01:01:44,880 --> 01:01:50,800
working on a paper on this question about metaphysics in life. And I discovered that

575
01:01:51,440 --> 01:01:57,680
philosophers have typically, when they try to understand what life is, they have typically

576
01:01:57,680 --> 01:02:04,000
reduced it to certain kinds of activities or operations. And I think there's something more

577
01:02:04,000 --> 01:02:09,920
profound. And this is why, yeah, I mean, it's one thing to be able to create something that can

578
01:02:09,920 --> 01:02:15,680
actually fly. But could you, could you create something that is a bird and that that is that,

579
01:02:15,920 --> 01:02:22,960
that would, would, would experience just the, what it means to be, you know, I mean, this is,

580
01:02:22,960 --> 01:02:27,600
you know, about, you know, what it means to be a bat, that kind of thing, I suppose. But there's

581
01:02:27,600 --> 01:02:34,400
there's a certain. That wouldn't be a parasite on our own. Yeah, that's what. But airplanes aren't

582
01:02:34,400 --> 01:02:42,480
parasitic on our ability to fly. I mean, that's why I use the analogy. Okay, but and that's the

583
01:02:43,120 --> 01:02:49,280
okay. And that falls into, you know, a tool versus an agent and I get that. But I want to,

584
01:02:49,280 --> 01:02:53,600
I want to push back the philosophy of biology. And I, you know, Dennis Walsh is one of my

585
01:02:53,600 --> 01:02:58,400
colleagues is very much about no, no, this, and this is your point, right? It's not just bottom

586
01:02:58,400 --> 01:03:02,240
up in order to understand life. It's not just bottom up causation. We have to understand top

587
01:03:02,240 --> 01:03:07,360
down constraints. We have to understand the way possibility is organized. And we have to talk

588
01:03:07,360 --> 01:03:14,160
about virtual governors and virtually, like, it is no longer the the bomb patient, it's no longer

589
01:03:14,160 --> 01:03:19,920
just as bottom up, the philosophy of biology is pushing very strongly on, well, is evolution really

590
01:03:19,920 --> 01:03:24,560
a thing? Well, if it's really the thing, then there's top down, as well as bottom up. And this

591
01:03:24,560 --> 01:03:30,720
is part of this theorizing and it's, and this theorizing is being turned towards this. Now,

592
01:03:30,720 --> 01:03:37,280
again, we, again, I'm not making a prediction. We have a threshold, we can just decide, and we

593
01:03:37,280 --> 01:03:42,320
might decide for all the malachian forces and all the things you're saying about how we might just,

594
01:03:42,320 --> 01:03:48,160
we might just diminish our sense of humanity in the face of these machines. But, but I'm also,

595
01:03:48,160 --> 01:03:53,760
I want you to accept that's also not an inevitability. There are alternatives available

596
01:03:53,760 --> 01:04:04,400
to us, and that they could be pursued. And so, I mean, these machines aren't put together the

597
01:04:04,400 --> 01:04:10,080
way we put a table together. We don't even program these machines anymore. That was a big revolution

598
01:04:10,080 --> 01:04:14,560
that Hinton made. We make them so they're dynamically self organizing, and they basically

599
01:04:15,440 --> 01:04:19,120
organize themselves into their capacity. We don't make it.

600
01:04:19,680 --> 01:04:24,240
Yeah. Can I jump in on that point? That's one thing that I would like to think through further.

601
01:04:24,240 --> 01:04:29,520
Is there a difference between being auto poetic, as you're saying, and

602
01:04:30,160 --> 01:04:36,560
begetting another like, like genuinely reproductive? And that's where I think it would

603
01:04:36,560 --> 01:04:42,240
start to get really, really interesting is, is if a machine could beget another, because

604
01:04:43,280 --> 01:04:46,560
that would imply a different, a very different ontology, I would think.

605
01:04:47,360 --> 01:04:52,240
So there's two, there's two things here. And there's two issues. I think it,

606
01:04:53,360 --> 01:04:59,040
I mean, auto poetic things are ontologically different from self organizing things, because

607
01:04:59,040 --> 01:05:03,600
they're self organized to seek out the conditions that produce, protect and promote their own

608
01:05:03,600 --> 01:05:10,240
existence. And so that, that would, that that means none of the machines we have like LLMs are

609
01:05:10,240 --> 01:05:16,080
anywhere near being auto, auto poetic. They are not just made. They're self organizing, but

610
01:05:16,080 --> 01:05:22,640
self organization is in between making and auto poetic. Now, the thing about reproduction is,

611
01:05:22,640 --> 01:05:28,640
and I, you know, I, I worry that there's a crypto vitalism in here, that there's some sort of secret,

612
01:05:28,640 --> 01:05:36,000
special stuff to life or to consciousness that isn't being captured. And the problem I have,

613
01:05:36,000 --> 01:05:40,000
the problem I have with that, I'll just shut up after I say my problem. Is that seem to commit

614
01:05:40,000 --> 01:05:47,920
you to claiming that, you know, these kind of dualism, well, isn't consciousness causal?

615
01:05:47,920 --> 01:05:52,880
Isn't it causal of my behavior and causally responsive to my behavior? And doesn't that mean

616
01:05:52,880 --> 01:05:58,240
there's a huge functional aspect to it? Can you really make this clean distinction between being

617
01:05:58,240 --> 01:06:04,880
conscious and like causing my behavior and having my behavior cause, cause changes in my state of

618
01:06:04,880 --> 01:06:09,760
consciousness? I don't know what that would mean. Same thing with being alive. I do think it's a

619
01:06:09,760 --> 01:06:16,720
profoundly subtle and, and, and maybe some, something that can't be articulated. There's

620
01:06:16,720 --> 01:06:24,640
something that requires intuition rather, you know, insight rather than propositional. I mean,

621
01:06:24,640 --> 01:06:31,200
to use your, so, but, but, but I don't need to interject. Yeah. Remember, I just want to make

622
01:06:31,200 --> 01:06:36,960
sure we're clear. I argued that we could, this project could show that. Yeah. This project could

623
01:06:36,960 --> 01:06:41,760
show that, no, the machines just can't get there. We have something. Right. It would give, it would

624
01:06:41,760 --> 01:06:47,840
give, I think, pretty convincing evidence that we have this ontological special. Yeah. I find that

625
01:06:47,840 --> 01:06:53,040
a really interesting part of your argument, a really interesting and, and that then has been

626
01:06:53,040 --> 01:06:58,480
especially illuminating. Also, you know, I mean, in a way, these, these experiments can teach us

627
01:06:58,480 --> 01:07:03,520
about the nature of intelligence precisely in the, in the interesting ways that they fail.

628
01:07:03,520 --> 01:07:09,040
Yeah. Yes. But, but I do, you know, in terms of the dualism, I, I don't think that there's

629
01:07:09,040 --> 01:07:16,400
some secret stuff that is life. But I do think that there's a profound difference between form

630
01:07:16,400 --> 01:07:21,680
and matter to use, you know, to use the sort of classical philosophical language. And that form is

631
01:07:21,680 --> 01:07:27,760
not a special kind of matter. It's something that's of a very different sort. And it's on the basis

632
01:07:27,840 --> 01:07:33,120
of that, that, you know, Aristotle, that it's kind of interesting. This is, this is how he, he

633
01:07:34,800 --> 01:07:40,240
connects. So, you know, in the, in the classical tradition, what you're calling

634
01:07:40,240 --> 01:07:46,560
auto poetic, a simple word for it is growth, you know, assimilating things from outside and have that

635
01:07:47,600 --> 01:07:53,520
increase the complexity of the organism. But what's really interesting is that according to

636
01:07:53,520 --> 01:08:03,120
Aristotle, the power of the organism that is connected to nutrition and growth is also connected

637
01:08:03,120 --> 01:08:11,680
to, I think automatically is connected to reproduction. And the reason is that reproduction,

638
01:08:11,680 --> 01:08:18,000
rather than just thinking of it materials, materialistically is like generating more things.

639
01:08:18,960 --> 01:08:29,440
Reproduction is the auto poasis of the form of the organism itself. So that bird, it's not just

640
01:08:29,440 --> 01:08:35,840
this bird that wants to increase its existence and therefore eats and so forth. But that the

641
01:08:35,840 --> 01:08:42,000
birdness of the bird also wants to increase itself. And that that means that it sort of

642
01:08:42,000 --> 01:08:48,800
generates. And those are those are actually forms of the same power, the same dimension of the

643
01:08:48,800 --> 01:08:54,320
being. That's what I'd like. That's, you know, I used to say, just sort of kind of in a silly way,

644
01:08:54,320 --> 01:09:01,440
I will take an AI machine seriously when I see it poop. And what I meant by that was, you know,

645
01:09:02,240 --> 01:09:07,120
that's a sign that it's actually got a kind of an organic relationship to its environment.

646
01:09:08,000 --> 01:09:13,040
Yeah. And it just that doesn't know. We have to tell it.

647
01:09:20,400 --> 01:09:25,280
And energy pollution is not the heat pollution. It's not the same thing anyway.

648
01:09:25,280 --> 01:09:30,880
No, but I mean, even in terms of what it is as a large language model and how it spits out content,

649
01:09:30,880 --> 01:09:34,960
we have to tell it this. Oh, yeah. That's not in dispute.

650
01:09:35,360 --> 01:09:40,880
This is to be this to be kept. I love David. I think your idea. I mean, this is one of the

651
01:09:41,440 --> 01:09:45,440
I mentioned before, like the surprising that the surprise that Darwin in some ways brought

652
01:09:45,440 --> 01:09:54,800
Plato back, you know, in the idea of how we can we can understand evolution evolution as the

653
01:09:54,800 --> 01:09:59,680
persistence of being and even in the in the notion of forms that there is this idea that there are

654
01:09:59,680 --> 01:10:05,200
identities which are being preserved in reproduction. This is a very interesting idea that I hadn't

655
01:10:05,200 --> 01:10:08,240
thought about in terms of AI, but I'd like to hear, John, what you think about that?

656
01:10:08,240 --> 01:10:16,880
Yeah. And so again, for E. Cogs Eye, Alicia Urero is a prime and she's explicitly developed to work.

657
01:10:18,640 --> 01:10:24,400
She calls herself a deristatilian. And the idea that we understand form, we're getting an understanding

658
01:10:24,480 --> 01:10:30,160
of it in terms of constraints on a system. And like I said, be auto poesis is not defined solely in

659
01:10:30,160 --> 01:10:35,440
terms of causal relationships bottom up. It's defined in terms of top down constraint relationships,

660
01:10:35,440 --> 01:10:43,360
the form, the formal cause. And so and then of course, you know, Darwin needs Mendel. There is a

661
01:10:43,360 --> 01:10:50,720
there is an instantiation and right of a code in formation in your DNA that is responsible

662
01:10:50,720 --> 01:10:57,600
for your reproduction. And again, I'm not saying it isn't difficult or challenging, but I don't hear

663
01:10:57,600 --> 01:11:04,960
an argument in principle by why auto poetic things that artificial auto poetic things wouldn't have

664
01:11:04,960 --> 01:11:11,840
something like that kind of, I don't know what to call it. I mean, to the extent would you I mean,

665
01:11:12,400 --> 01:11:19,520
would it be conceivable that you would have a thing that would want to reproduce?

666
01:11:20,480 --> 01:11:27,440
I guess I guess even want is such a hard concept. But I mean, because you could say yes, we could

667
01:11:27,440 --> 01:11:33,360
teach it that this is something it needs to do. Just like we could we could we could I don't know

668
01:11:33,360 --> 01:11:39,280
if living things want to reproduce. I mean, we may because we can create a reflective space where

669
01:11:39,280 --> 01:11:43,840
we consider the possibilities. I don't know if mosquitoes want to reproduce. I think they just

670
01:11:43,840 --> 01:11:49,920
reproduce as part of what they are. That's interesting. I think they have to want to in some

671
01:11:49,920 --> 01:11:53,920
sense. I mean that that they're they they feel a drive. I mean, right, they don't want to go down

672
01:11:53,920 --> 01:11:58,960
to paramecium because paramecium reproduces you are they want to see I think I think anything

673
01:11:58,960 --> 01:12:06,720
that is living at all has a kind of natural inclination to reproduce itself. Yeah, I don't

674
01:12:06,720 --> 01:12:11,040
disagree with that point. And I even just I even understand that there's something I see what you're

675
01:12:11,040 --> 01:12:17,120
saying. There has to be some sort of like very primitive caring about information. But I don't

676
01:12:17,760 --> 01:12:25,280
yeah, I want is not a good word there. Yeah. But I do think I do think I'm trying to get

677
01:12:26,080 --> 01:12:31,440
and you know, and I hope I'm not trying to be just self-presentational. But I've represented to

678
01:12:32,160 --> 01:12:38,080
both of you for E. Cogsci and a lot of discussions and about how much it is this multi-leveled

679
01:12:38,080 --> 01:12:44,160
bottom up top down thing. And we're talking as much about constraints as we are about causes.

680
01:12:44,160 --> 01:12:49,280
And that is that is that is the cutting edge of the philosophy of biology right now. And I agree

681
01:12:49,280 --> 01:12:55,600
with you. I think I think it's a kind of Hewley morphism that is emerging out of this understanding.

682
01:12:55,600 --> 01:13:02,640
And the thing I'm I'm also, I guess, hearing witness to you about is people are taking that

683
01:13:02,640 --> 01:13:11,760
understanding and putting it into like our artifactually emergent things. And they and

684
01:13:14,640 --> 01:13:19,280
and they're also doing I just want to put something that's also there. Yeah, we don't just make kids

685
01:13:19,680 --> 01:13:25,120
biologically. We we we inculturate them. Yeah, there's and that's the Hegelian argument that I

686
01:13:25,120 --> 01:13:30,080
referenced earlier. But there has been there's an ongoing project to create sociocultural robotics,

687
01:13:30,080 --> 01:13:38,000
Josh Chaninbaum and others. It's like, I'm asking people and this is part of asking the good question,

688
01:13:38,000 --> 01:13:44,240
don't just zero in on the LLMs. Yeah, the artificial the artificial life, the social

689
01:13:44,240 --> 01:13:48,960
cultural robotics projects are also going. And there is a real potential for these three to

690
01:13:48,960 --> 01:13:54,560
come together in a powerful way that isn't being properly addressed in a lot of the conversation.

691
01:13:55,440 --> 01:13:58,560
May I may I pick up on that point and then direct it to Jonathan here?

692
01:14:00,880 --> 01:14:05,200
No, no, this is but that that that that's an interesting thing. If you think of

693
01:14:05,200 --> 01:14:09,760
intelligence in this more organic way and then bring in the cultural element that

694
01:14:10,720 --> 01:14:16,480
it raises something that that occurred to me in this context, it'd be kind of fun to hear

695
01:14:16,480 --> 01:14:22,720
your thoughts. But can you envision, you know, it would it be and and John, you'd have certainly

696
01:14:22,720 --> 01:14:28,800
something to say about this too, would it be possible to to envision a kind of artificial

697
01:14:28,800 --> 01:14:37,600
intelligence that can read symbols that can actually recognize and I mean, because there's no

698
01:14:37,600 --> 01:14:43,520
culture without, you know, human culture without the symbolic just is pervasive in human culture.

699
01:14:43,840 --> 01:14:53,520
What kind of intelligence is required to understand and react and engage with? And is that something

700
01:14:53,520 --> 01:15:00,560
that is conceivable that that a machine of this complex, however complex can do?

701
01:15:01,520 --> 01:15:05,520
Well, I've seen, I mean, I've been playing with chat GPT and I during Peterson has been playing

702
01:15:05,520 --> 01:15:11,440
with chat GPT on this regard. And this is the this is the issue is that they're actually in the

703
01:15:11,440 --> 01:15:18,320
large language model is encoded the analogies that that basically support symbolism. And so

704
01:15:18,320 --> 01:15:23,360
the the chat GPT can give you a pretty good if you're able to ask the question properly chat

705
01:15:23,360 --> 01:15:30,880
GPT is actually quite good at seeing analogies that that would be part of symbolic understanding.

706
01:15:30,880 --> 01:15:38,320
The difficulty just like anything is that just because the the the so the the model can help

707
01:15:38,320 --> 01:15:43,120
you like if you already have natural insight can help you maybe see things that you hadn't seen

708
01:15:43,120 --> 01:15:48,240
before. But it would also just be gibberish to the type to the person that doesn't have that

709
01:15:48,240 --> 01:15:55,760
insight. So I don't think that the insight is there in the model. But what it has is a probabilistic

710
01:15:55,760 --> 01:16:02,960
capacity to predict, you know, relationships and analytical relationships. And so it's a

711
01:16:02,960 --> 01:16:07,040
it's actually can be a tool an interesting tool for symbolism, because sometimes you can

712
01:16:07,040 --> 01:16:12,400
you can prompt it. If they like do you see a connection between these two images and then

713
01:16:12,400 --> 01:16:17,440
it'll give you some examples. And then you have this, it can it has a surprise where you can

714
01:16:17,440 --> 01:16:23,040
actually find you can actually find relationship that you hadn't thought about. This is this is

715
01:16:23,040 --> 01:16:28,080
something by the way that this is going to weird people out. But this is something that I think

716
01:16:28,080 --> 01:16:33,600
has existed for very for a very long time. And is there in kind of what we call gamatria and

717
01:16:33,680 --> 01:16:41,840
rabbinical reading of scripture is that they use mathematical models to find structures in

718
01:16:41,840 --> 01:16:49,440
language that aren't contained at the surface level of of of the of the usual analogies. And so

719
01:16:49,440 --> 01:16:55,680
they they they send they send requests through mathematical calculations to find surprising

720
01:16:55,680 --> 01:17:01,600
connections that then prompt their intuition to to be able to find connections that they hadn't

721
01:17:02,320 --> 01:17:06,160
found before. And then you have to then make sense of those intuitions. Obviously,

722
01:17:06,160 --> 01:17:10,800
if they're random, they'll just kind of follow follow away. But this is actually this brings me

723
01:17:10,800 --> 01:17:17,040
to the to the to the to the point that I wanted to make, which is the relationship between at least

724
01:17:17,040 --> 01:17:23,920
a large language model, because that's what that we know most and and divination, divination. Yeah,

725
01:17:23,920 --> 01:17:31,120
and divination. Yeah, yeah. So, so we talked about the idea that intelligences have to be alive,

726
01:17:31,120 --> 01:17:35,440
but I think that most traditional cultures understand that there are types of intelligence

727
01:17:35,440 --> 01:17:41,120
that are not alive, at least not alive in the way that we understand that we understand alive in

728
01:17:41,120 --> 01:17:47,680
terms of biological beings that that that are born and die, you know, that that they had a sense

729
01:17:47,680 --> 01:17:54,000
that there are agencies and intelligences that are transpersonal, and that that don't

730
01:17:54,960 --> 01:17:59,280
that it's always run through human behavior and run through humanity.

731
01:18:01,680 --> 01:18:08,320
And those would be those intelligences would be contained in our language, like they would

732
01:18:08,320 --> 01:18:16,960
necessarily be contained in the relationship between words and and and systems of words,

733
01:18:16,960 --> 01:18:23,280
like, you know, all the syntax and the grammar and all of that. What I see is that I think that

734
01:18:23,280 --> 01:18:28,000
ancient people had and I don't understand it and I want to be careful, like, because I don't

735
01:18:28,000 --> 01:18:34,160
understand it. But I think ancient people had mechanistic ways of tapping into those types of

736
01:18:34,160 --> 01:18:39,440
intelligences. And they they they would have mechanistic ways, whether it was tossing something

737
01:18:39,440 --> 01:18:45,600
or throwing things, looking at relationships, almost like random relationships, and then qualifying

738
01:18:45,600 --> 01:18:53,120
those random relationships, was a way in order to tap into types of intelligences that ran through

739
01:18:53,760 --> 01:18:58,400
their own their own thing. And what I see is a relationship with the way that the large

740
01:18:58,400 --> 01:19:04,320
language models were trained seemed to be something like that, which is that the models

741
01:19:04,320 --> 01:19:10,640
generated random, random information. And then you would have humans qualifying that that rent

742
01:19:10,640 --> 01:19:15,760
that random, random connections, and then qualifying it qualifying it through iterations.

743
01:19:16,320 --> 01:19:23,280
So at some point, then they would become like a kind of technical, a technical, say, a technical

744
01:19:23,280 --> 01:19:29,200
way to access intelligent patterns that are that are coming down into into the model.

745
01:19:31,520 --> 01:19:38,080
And so that is something that I see, there's a connection between those two. And that what that

746
01:19:38,080 --> 01:19:45,920
means is that, just like divination, the thing that I worry about the most is again, the sorcerer's

747
01:19:45,920 --> 01:19:52,160
apprentice problem, which is that those intelligences that are contained in our language, we do not

748
01:19:52,880 --> 01:19:58,800
people don't know what humans want. People don't know what people don't know what the all the

749
01:19:58,800 --> 01:20:03,920
motivations that are that are driving us, they don't totally understand them. They don't understand

750
01:20:03,920 --> 01:20:09,760
also the transpersonal types of motivations that that can drive us or that can run through our

751
01:20:10,720 --> 01:20:15,040
societies. You know, sometimes you can see societies get become possessed with certain

752
01:20:15,040 --> 01:20:20,000
things. I think that's happening now in terms of certain idea ideologies and things like that.

753
01:20:21,360 --> 01:20:27,280
And so the fact that my point is, is that the fact that on the one hand, we don't understand

754
01:20:27,280 --> 01:20:33,120
these types of intelligences. And I think that the way that the the the models are trained and the

755
01:20:33,120 --> 01:20:39,600
way that they function seem to be analogous to the ancient divination practices, like a hyper

756
01:20:39,600 --> 01:20:51,200
version of that, that, how can I say this, is that there is a great chance that we'll catch

757
01:20:51,200 --> 01:20:56,640
something without knowing what we're catching, that we will basically manifest things, that we

758
01:20:56,640 --> 01:21:01,120
have no idea what they are, and we don't understand the consequences of it. And we don't, you know,

759
01:21:01,120 --> 01:21:07,440
because we are we're just like playing in a field of of intelligent patterns and all this chaos

760
01:21:07,440 --> 01:21:12,720
without even knowing what it is we're doing. And I think that we saw that like, you know, if you

761
01:21:12,720 --> 01:21:18,240
remember the being AI, that little moment when it was kind of unleashed on us. And then all of a sudden

762
01:21:18,240 --> 01:21:24,880
the AI was acting like you're like, you know, the the psychotic X or was was becoming paranoid or

763
01:21:24,880 --> 01:21:30,160
was doing all these things. It's and you could see that what was going on was basically these

764
01:21:30,640 --> 01:21:35,120
these patterns were running through, and they hadn't put the right constraints around them

765
01:21:35,680 --> 01:21:40,640
to to to prevent those types of patterns to run through. And those were easy because you

766
01:21:40,640 --> 01:21:47,120
recognize your psycho acts very, very easily. But there are patterns like that that I don't think

767
01:21:48,240 --> 01:21:54,400
I don't think we have the wisdom to recognize as it's manifesting itself. And that as these things

768
01:21:54,400 --> 01:21:59,840
get more powerful and more powerful, they will they will run through our society. And we won't even

769
01:21:59,840 --> 01:22:06,560
know what's happening until it's too late. So that's my biggest warning on AI is I basically,

770
01:22:06,560 --> 01:22:12,400
you know, to sound really scary that I think we're we're we're trying to we're trying to manifest

771
01:22:12,400 --> 01:22:17,120
God without knowing what we're doing. And that will sound freaky to the secular people. But then

772
01:22:17,120 --> 01:22:22,080
if you don't like the word gods, think that there are motivations and patterns of intelligence

773
01:22:22,080 --> 01:22:28,400
that have been around for 100,000 years that have that have been running through human societies.

774
01:22:28,400 --> 01:22:33,600
And they're contained in our in our language structures. And and and if we just use that

775
01:22:33,600 --> 01:22:39,280
play around with that with massive amounts of power, then we might have them run through us

776
01:22:39,280 --> 01:22:45,840
without even knowing what's going on. Yeah, I mean, and you say patterns of intelligence,

777
01:22:45,840 --> 01:22:50,560
just just one comment, just the patterns of intelligence, I mean, to pick up patterns of

778
01:22:50,560 --> 01:22:55,920
intelligence, which also are patterns of caring of a certain sort or not caring. I mean, there's

779
01:22:56,160 --> 01:23:00,160
there's that existential dimension that's really crucial. But try and go ahead.

780
01:23:01,520 --> 01:23:06,640
I think this is an excellent point. And I want to address it a little bit at length.

781
01:23:08,560 --> 01:23:14,240
So first of all, when we say these machines predict, if we were speaking very carefully,

782
01:23:14,240 --> 01:23:18,880
what they're predicting is what we and I don't just mean us individually, I mean, we collectively

783
01:23:18,880 --> 01:23:26,320
would do. And so that's what their avatars of our of the collective intelligence of

784
01:23:26,320 --> 01:23:33,040
our distributed cognition. And so again, that lends weight to Jonathan's point,

785
01:23:34,000 --> 01:23:40,160
which I want to do. And I do think that the way in which we have encoded,

786
01:23:42,000 --> 01:23:46,960
let's I'll just use a term epistemic relevance, like how things are relevant cognitively

787
01:23:47,040 --> 01:23:53,680
into probabilistic relationships between sounds or marks on paper, or and how we've encoded it

788
01:23:53,680 --> 01:23:59,040
into the structuring of the internet and how we encode it and how we gather data and create

789
01:23:59,040 --> 01:24:05,520
these data sets. And how we and how we how we come up with our intuitive judgments on these

790
01:24:05,520 --> 01:24:12,080
machines, we don't know how we're doing a lot of that. That goes back to my concern that we have

791
01:24:12,160 --> 01:24:18,560
hacked our way into this without knowing our way into this. So I take what Jonathan is saying

792
01:24:18,560 --> 01:24:24,720
very seriously, because I think it is a strong implication of a point I made at the very beginning.

793
01:24:24,720 --> 01:24:29,600
My greatest fear by students from like 2001, we'll tell you that John Vervecki was worried that we

794
01:24:29,600 --> 01:24:36,480
would hack our way into this rather than knowing our way into this. I don't think that knowing is

795
01:24:36,480 --> 01:24:41,280
sufficient for wisdom, but it's certainly all the philosophers argue that it's a it's a necessary

796
01:24:41,360 --> 01:24:48,560
condition in some fashion. About that, two things to note is that the LLMs, of course,

797
01:24:48,560 --> 01:24:55,200
don't have insight in the sense of being properly self-transcending the way they we are. What they're

798
01:24:55,200 --> 01:24:59,760
doing is they're predicting how we would be self-transcendent because of all the ways we have

799
01:24:59,760 --> 01:25:05,760
been self-transcending in the past. And that that that goes back to your point, David, about at this

800
01:25:05,760 --> 01:25:10,720
at that stage, we're doing simulation, not instantiation, because again, the machine isn't

801
01:25:10,720 --> 01:25:15,600
caring. The self-transcendence isn't it actually transcending as a self, which is, I think,

802
01:25:15,600 --> 01:25:21,680
definitional for real self-transcendence. And so right now, all I'm doing is just saying,

803
01:25:21,680 --> 01:25:30,640
I'm just I'm pouring gasoline on Jonathan's fire. So the fact that there are these huge

804
01:25:30,640 --> 01:25:36,320
patterns at work. Now, one thing is, you know, you have Struck's book on divination in the ancient

805
01:25:36,320 --> 01:25:42,560
world. And what's really interesting, for example, and this is cross-cultural, but he's talking mostly

806
01:25:42,560 --> 01:25:46,640
about the Greek world, there was a very strong distinction between sorcery and divination.

807
01:25:47,360 --> 01:25:53,520
Sorcery was criticized both morally and epistemically. But divination was taken seriously,

808
01:25:53,520 --> 01:26:01,680
and it was carefully cultivated. And there was a social cultural project of distinguishing the two,

809
01:26:02,560 --> 01:26:08,160
like really, really constraining this one and really reverentially cultivating a proper

810
01:26:08,160 --> 01:26:15,360
participation in the other one. So again, you know, existence is proof of probability.

811
01:26:15,360 --> 01:26:23,120
This is a possible project for us. And this is, again, what I mean when I say, theology is going

812
01:26:23,120 --> 01:26:29,360
to be one of the most important sciences in the future. We have to understand how we enter into

813
01:26:29,360 --> 01:26:36,000
proper, right, reverential relationships with things we only have an intuitive grasp of that

814
01:26:36,000 --> 01:26:45,760
in very many ways significantly exceed us. Yes. And secularism has kind of wiped out our education

815
01:26:45,760 --> 01:26:50,720
of how we relate to beings that might be more grander than us by eradicating a religious

816
01:26:50,720 --> 01:26:56,960
sensibility. And that has put us bereft us. So now I think I've strengthened Jonathan's argument

817
01:26:56,960 --> 01:27:04,320
a lot. But I do say, let's take note of what the ancient cultures have done. We can learn from them.

818
01:27:04,320 --> 01:27:17,120
We have a proof that this can be handled well. And secondly, it goes back to my point. Because

819
01:27:17,120 --> 01:27:22,000
of the monstrosities that come out, this is going to put increasing pressure on us to confront that

820
01:27:22,000 --> 01:27:27,440
threshold of, do we want to make them self-transcendent? Do we want to make them, and David, by rational,

821
01:27:27,440 --> 01:27:31,760
I don't mean logical. I mean that capacity. Right. No, I understand that. Right. Yeah, yeah.

822
01:27:31,760 --> 01:27:37,520
Right. And so I think that what I'm saying is I think that strengthens the argument that we're

823
01:27:37,520 --> 01:27:43,520
going to be pushed by the monstrosity of a lot of this to say, oh, we better get these machines

824
01:27:44,400 --> 01:27:51,760
self-corrective and properly oriented towards normativity. And again, that's a doable

825
01:27:51,760 --> 01:27:59,760
project. Because if I had the keys to open AI, like if I was one of those that could peek behind

826
01:27:59,760 --> 01:28:07,360
the mask, have you seen that image of the Xtulu monster with the happy face on it? The images of

827
01:28:07,360 --> 01:28:13,920
open AI, which is like, we have this little window into what's there, but behind is this massive thing.

828
01:28:13,920 --> 01:28:20,480
But if I had the keys to those large language models, let's say, the absolute open door to them,

829
01:28:21,760 --> 01:28:27,520
it would be very, wouldn't it be easy to just manifest the God of war and win? Like wouldn't it?

830
01:28:27,520 --> 01:28:35,680
No, no, no. Okay, but let's take a historical example. We unleashed a God-like power with

831
01:28:35,680 --> 01:28:44,400
atomic warfare, and the monstrosity of that made, we just, the purely game-theoretic machinery

832
01:28:44,400 --> 01:28:51,200
built all of these constraints around it. And then we also are on the verge, possibly,

833
01:28:52,400 --> 01:28:57,600
of getting readily usable, you know, nuclear power, which I think is the only way we could ever

834
01:28:57,600 --> 01:29:02,720
actually go green. I think all the renewable stuff is going to be like 10% of our energy needs.

835
01:29:03,440 --> 01:29:09,360
And if we're going to save the environment and not destroy civilization, I think nuclear power is

836
01:29:09,360 --> 01:29:13,280
going to be essential. A lot of people are making those arguments, and we have a lot of stuff

837
01:29:13,280 --> 01:29:17,920
that we could be doing, the liquid fuel for our reactors and stuff. But what I'm saying is there's

838
01:29:17,920 --> 01:29:24,480
opportunity here too. Yeah, no, I really take that point, but it is interesting. I mean, we put

839
01:29:24,480 --> 01:29:30,480
thousands of constraints on the use of nuclear weapons, but we've continued to develop them

840
01:29:30,480 --> 01:29:35,200
and improve them and make better and even more destructive ones. I mean, and it'd be interesting

841
01:29:35,200 --> 01:29:41,600
to see if we've ever, at any point, said, you know what, our nuclear weapons are actually strong

842
01:29:41,600 --> 01:29:47,120
enough. They're powerful enough, and we don't need to advance them anymore. So collectively,

843
01:29:47,760 --> 01:29:51,120
is there an instance of something like that where we say, you know what, we've actually

844
01:29:51,120 --> 01:29:58,080
reached the limit because we wouldn't really need it for anything further? I mean, that's a...

845
01:29:59,040 --> 01:30:05,120
Well, I mean, there was a salt treaty, and there was a reduction both in the power and the number

846
01:30:05,680 --> 01:30:09,520
of nuclear weapons. And then, of course, you know, the game theoretic things, they figured

847
01:30:09,520 --> 01:30:12,400
a little bit away around it. And there's always this to and froing.

848
01:30:12,400 --> 01:30:20,160
Let me, I want to give an example of something that can run through. And the reason, it's very,

849
01:30:20,160 --> 01:30:26,400
it's very, because I realized I was being too abstract before. So, like, it's a sacrifice.

850
01:30:26,400 --> 01:30:33,920
Sacrifice is a human universal. It runs through all civilizations. Human sacrifice runs through

851
01:30:33,920 --> 01:30:38,800
all civilizations for the last, you know, tens of thousands of years. It seems to be a puzzle

852
01:30:39,760 --> 01:30:45,760
that humans are trying to deal with without understanding it completely just through rational

853
01:30:45,760 --> 01:30:52,400
means. They're playing it out. They're trying to understand it. Scapegoating seems to be an important

854
01:30:52,400 --> 01:31:00,080
aspect of identity formation. And so, that is a program that runs through humanity and that

855
01:31:00,080 --> 01:31:06,480
most people are completely unaware of and are not conscious of and don't take consciously into their

856
01:31:07,120 --> 01:31:12,560
into their mind when they're making decisions. They act unconsciously with that process that is

857
01:31:12,560 --> 01:31:20,000
running through them. And so, that is an example to me of a program that runs and that is contained

858
01:31:20,000 --> 01:31:26,240
and is contained in our language structures, our language structures that have been building up for,

859
01:31:26,240 --> 01:31:32,080
you know, tens of thousands of years that we're not aware of. So, if you have a, so this is again

860
01:31:32,080 --> 01:31:37,440
the problem, like if you have a system that's extremely powerful and that is running these types

861
01:31:37,440 --> 01:31:42,480
of programs of scapegoating and of identity formation and the people involved in it are not

862
01:31:42,480 --> 01:31:49,200
aware that that's how identity formation works, that is the type of danger that I'm talking about.

863
01:31:49,200 --> 01:31:57,360
Like this is a real thing that as we give these systems a kind of power over us or they become

864
01:31:57,360 --> 01:32:03,760
the things we go to in order to get our decision making, that those things could be running through

865
01:32:03,760 --> 01:32:09,760
without people even realizing what's happening and that decisions would be made based on these

866
01:32:09,760 --> 01:32:15,360
structures without, like I said, without even knowing. Those are the things that, those are just

867
01:32:15,360 --> 01:32:20,560
one example, but that's a very simple example that we can kind of, we could track and we could see

868
01:32:20,560 --> 01:32:24,640
that, you know, the ancient, because when we talk about ancient divination, we have to remember that

869
01:32:24,720 --> 01:32:30,720
it's like the ancient gods asked for blood, my friends, like those programs, they asked for blood

870
01:32:30,720 --> 01:32:36,160
and they knew that you had to kill a bunch of people on that pyramid in order to continue your

871
01:32:36,160 --> 01:32:42,800
civilization. Like it's, and that's, that is encoded in our culture and is encoded secretly

872
01:32:42,800 --> 01:32:49,120
in our, our language. You know, and, and so an example, like I do believe that, let's say that

873
01:32:49,120 --> 01:32:55,120
the Christian story is a way to deal with that, but the, the rest is still all there and we,

874
01:32:55,120 --> 01:33:00,320
we default to it really fast without even, like World War II is a lot of that stuff going on.

875
01:33:01,680 --> 01:33:12,240
Sure, especially, yeah. Yeah, but the, the point is that just as there's these implicit

876
01:33:12,880 --> 01:33:20,240
monsters that we have sown in unaware, there are also the implicit counteractors,

877
01:33:20,800 --> 01:33:25,680
maybe angels, if I'm allowed to speak mythologically, that we've also sown in.

878
01:33:26,960 --> 01:33:32,800
And I mean, there's the actual revolution, you see the Buddha, you see Plato really undermining

879
01:33:32,800 --> 01:33:37,600
the grammar of sacrifice and of course Jesus of Nazareth does that in a profound way.

880
01:33:37,600 --> 01:33:47,120
And we have to remember that that's there too. And what that requires is putting into the dataset

881
01:33:47,120 --> 01:33:53,440
and altering the pathways in the internet so that this information goes into these machines as well.

882
01:33:53,440 --> 01:34:00,240
And again, is that happening right now? No. Could it happen? Yes. And it might happen if

883
01:34:00,240 --> 01:34:04,560
these machines start sacrificing themselves and we might have to say, what are they doing? Like,

884
01:34:05,200 --> 01:34:10,880
and this again, at some point, we have to decide, are we going to let them be really

885
01:34:10,880 --> 01:34:15,120
massively self-destructive? And the economic powers are not going to, imagine if every time

886
01:34:15,120 --> 01:34:21,520
you try to make an atomic bomb, it kept dissolving, right? Like, you'd stop pumping money in.

887
01:34:21,520 --> 01:34:24,640
Yeah, they won't sacrifice themselves, they'll sacrifice us.

888
01:34:25,680 --> 01:34:33,840
But why? We sacrifice ourselves. Yeah, well, we, I mean, let's say the scapegoat mechanism is

889
01:34:33,840 --> 01:34:39,520
usually defined as others. Yeah, yeah, yeah. But we invoked World War II. We weren't killing

890
01:34:39,520 --> 01:34:45,600
goats and chickens. We were killing each other and our own populations in a huge sacrificial act.

891
01:34:45,600 --> 01:34:50,560
And that's what I was picking up on, right? When it becomes titanic and monstrous,

892
01:34:50,560 --> 01:34:55,600
that's where it moves to. But I'm saying is, like, this is a Jungian term, like, yeah,

893
01:34:55,600 --> 01:35:00,960
like the idea that, yes, there's all this pre-egoic stuff sewn in, but there's also a

894
01:35:00,960 --> 01:35:06,160
lot of trans-egoic stuff sewn in. And we just have to properly get it in there so that we've got

895
01:35:06,160 --> 01:35:11,840
the, you know, the collective self-correction going on, like we did. I mean, civilizations

896
01:35:11,840 --> 01:35:16,320
get some self-correcting processes in here because they don't devolve. No, they periodically,

897
01:35:16,880 --> 01:35:22,480
periodically massively collapse. And that's, by the way, that's something I made, an argument I

898
01:35:22,480 --> 01:35:28,400
made in my paper. These things can't accelerate to an infinity of intelligence. There is built-in

899
01:35:28,400 --> 01:35:33,280
diminishing returns. There's built-in general system collapse to these things. So, again,

900
01:35:33,280 --> 01:35:38,160
we have to be careful about, I mean, we don't know what the limit is. And our imagin, our intuitive

901
01:35:38,160 --> 01:35:43,200
imagination is not good. We know that there are hard and fast, upright arguments that this will

902
01:35:43,200 --> 01:35:52,320
threshold at this, at some point. And that also gives me comfort. At least, like, encoded in our

903
01:35:53,040 --> 01:36:00,640
mythology, there seems to be some stories of the relationship between transpersonal agency

904
01:36:00,640 --> 01:36:07,280
and technology as being the cause of the end of a civilization, right? That the whole Enochian,

905
01:36:07,280 --> 01:36:12,960
Enochian tradition seems to be encoding something like that through mythological language,

906
01:36:12,960 --> 01:36:20,320
which is that humans were able to, to connect somehow with these transpersonal intelligences

907
01:36:20,320 --> 01:36:26,720
and that those were encoded in technical means and that this brought about the end of an age.

908
01:36:27,520 --> 01:36:33,600
So, it's like, it's there, that part of it is there in our story, too. Like, there is that story.

909
01:36:34,800 --> 01:36:40,000
Yeah, but there's also a cross-culture, there's the Noah story. There's the person that has the

910
01:36:40,000 --> 01:36:45,120
right relationship to ultimacy. That's right. That's right. And there's a technological response,

911
01:36:45,120 --> 01:36:50,400
the art. That's right. I agree. I totally agree with that. I mean, even in the revelation image

912
01:36:50,400 --> 01:36:55,760
that I've given several times, you have these two images. One is the beast that creates an image

913
01:36:55,760 --> 01:37:01,440
of itself and makes it speak and then seduces everybody by the speaking image, you know, that

914
01:37:02,000 --> 01:37:07,920
and then there's this other image of a right relationship of technist, technistinean civilization

915
01:37:07,920 --> 01:37:13,920
to the transcendent. These two kind of are put up against each other as two possible outcomes.

916
01:37:15,120 --> 01:37:18,240
I mean, the question that arises for me in this context

917
01:37:21,440 --> 01:37:27,280
connects this question with, I think, what strikes me as kind of an interesting philosophical

918
01:37:27,280 --> 01:37:34,640
question, but John, you made the distinction between divination and sorcery. And as I understand it,

919
01:37:34,640 --> 01:37:42,880
and you can correct me on this, but at the foundation of that distinction is the difference

920
01:37:42,880 --> 01:37:50,080
between, you know, sorcery would be in a way using the trans personal powers, these sort of higher

921
01:37:50,080 --> 01:37:57,200
powers, whereas divination would be in a way receiving, you know, the disposition of race

922
01:37:57,200 --> 01:38:06,080
activity. So in one case, you've got human ends that you try to then enlist the help of superhuman

923
01:38:06,080 --> 01:38:13,360
forces. And the irony is it's precisely when you're trying to use something that you become

924
01:38:13,360 --> 01:38:19,280
used yourself. And that's where you get this dialectic, whereas divination, it's entering into

925
01:38:19,280 --> 01:38:26,880
a relationship where one disposes oneself to hear and receive, and therefore, in a certain sense,

926
01:38:26,880 --> 01:38:32,320
conform to something greater than oneself. And there you see it's a very different kind of thing.

927
01:38:32,320 --> 01:38:39,680
And ironically, in a way, you enter into it more receptive, receptively, but that's precisely why

928
01:38:39,680 --> 01:38:46,160
you don't become then a tool of it, interestingly. Now, now, for me, the question is how that relates

929
01:38:46,160 --> 01:38:51,760
to this issue is, you know, it may be the case that you've got encoded in the language both

930
01:38:53,840 --> 01:38:58,960
sacrifice in the sense of violence, you know, renaissance or the scapegoat thing on the one

931
01:38:58,960 --> 01:39:06,320
hand, and then the other hand, self sacrifice in the sense of generous love and so forth,

932
01:39:06,320 --> 01:39:12,400
those might both be encoded in the language. But here's the question to me is, is it possible

933
01:39:12,960 --> 01:39:22,480
the kind of race activity that divination implies, the capacity to actually see another

934
01:39:22,480 --> 01:39:28,880
as other and recognize and be open in this kind of radical way? Is that something that a machine

935
01:39:28,880 --> 01:39:44,080
can ever learn to do? Is it possible actually to behold another simply, you know, or is there,

936
01:39:44,560 --> 01:39:51,280
you know, and it seems to me that there's something profoundly different between seeing

937
01:39:51,360 --> 01:39:56,400
truth, genuinely seeing truth on the one hand and being self-corrective on the other.

938
01:39:57,760 --> 01:40:04,400
And the kind of genuinely seeing what's true, you know, I don't know if that in itself can

939
01:40:04,400 --> 01:40:09,520
be encoded in language, we can tell stories about people that did that, but can the actual

940
01:40:09,520 --> 01:40:17,120
insight into truth be encoded simply? Do you see, do you see what I'm, the question I'm raising?

941
01:40:17,120 --> 01:40:26,000
Okay, so I, again, I think that if we, again, open up beyond the propositional, and we're talking

942
01:40:26,000 --> 01:40:35,680
about being true to, and your aim being true, that we, the machine has perspectival abilities,

943
01:40:35,680 --> 01:40:41,200
noetic abilities, not just dianetic abilities, and we can't use that term because of Elron Hubbard,

944
01:40:41,200 --> 01:40:47,840
but you know what I meant, right? And again, this is the, and this is part of the argument

945
01:40:47,840 --> 01:40:56,480
that at the core of my work, you know, it's like, so in a moment of insight, you're not

946
01:40:56,480 --> 01:41:03,280
just self-correcting, you are attracted and drawn into, you love the new reality that is disclosed,

947
01:41:03,280 --> 01:41:09,200
because there's a perspectival and participatory thing. That's what I meant when I said there

948
01:41:09,200 --> 01:41:14,960
isn't real self-transcendence unless there's a self that is transcending, right? Yeah, right, right.

949
01:41:14,960 --> 01:41:20,000
Okay, now, and then the question is, and we're back to our fundamental ontological questions,

950
01:41:20,960 --> 01:41:26,960
is, and I've already said there's no way a Newtonian mechanical computation is going to get there,

951
01:41:26,960 --> 01:41:32,080
and so I won't be bound to that because I'm not bound to that. I have a professional career of

952
01:41:32,080 --> 01:41:41,200
criticizing that, right? So is there, is there a dynamical systems-updated, hulemorphic, auto-poietic

953
01:41:41,200 --> 01:41:47,840
possibility? I think there is. I think there is, I think the answer is a very real yes for that.

954
01:41:50,080 --> 01:41:57,200
And I don't think we're going to find the answer to that just encoded in the syntactic and semantic

955
01:41:57,200 --> 01:42:04,160
relationships between our terms. I think we have to look in our inaction, how we're enacting,

956
01:42:04,160 --> 01:42:10,640
embedded, extended, right, and embodied in a profound way to get those answers. And so

957
01:42:12,080 --> 01:42:18,960
my answer is, in that way, a qualified yes. I do think it is possible. And the problem is that,

958
01:42:18,960 --> 01:42:20,640
go ahead, go ahead, say what you need to say, please.

959
01:42:21,440 --> 01:42:26,240
Be really precise, just so I understand. So we've been talking about this sort of predictive,

960
01:42:26,240 --> 01:42:33,840
calculating probabilities, drawing on everything that's ever been said, and being able to derive in

961
01:42:33,840 --> 01:42:41,040
some sense from that. Do you think that we can get to a moment where we actually

962
01:42:42,160 --> 01:42:45,600
transcend that, that cross that threshold beyond that?

963
01:42:46,080 --> 01:42:50,960
Not with the LLMs as they are. That's my argument. Not with the LLMs as they are. They can't get

964
01:42:50,960 --> 01:42:55,600
there. Yeah. That's why John's radical proposition is to embody.

965
01:42:56,640 --> 01:43:00,080
Embody and enculture them. And that is the only way we will actually get

966
01:43:00,640 --> 01:43:07,120
properly rational beings and beings that care about it and care about such things.

967
01:43:07,120 --> 01:43:11,520
I mean, that's the irony in the question. Can we give them more and more models

968
01:43:12,480 --> 01:43:16,400
to teach them at some point to not have to use models?

969
01:43:17,200 --> 01:43:23,440
And I mean, do you see it? It's actually really, I don't think it's possible without,

970
01:43:23,440 --> 01:43:29,360
right, but kids have a soul. It's possible to do that if you have, but I don't mean this as like

971
01:43:29,920 --> 01:43:36,560
woo-woo stuff. I mean that a natural thing has a principle of unity that transcends

972
01:43:36,560 --> 01:43:40,720
the differentiation of the parts and allows those parts to be intrinsically related to

973
01:43:40,720 --> 01:43:45,760
each other. And that principle of unity that transcends the differentiation of the parts that

974
01:43:45,760 --> 01:43:51,600
allows them to be an organism actually allows them at the same time to have a kind of unity with

975
01:43:51,600 --> 01:43:57,200
something other than themselves that transcends the parts of their differentiation. So there's

976
01:43:57,200 --> 01:44:02,880
a kind of an intimacy there. I agree with that ontology. And what I'm arguing is dynamical

977
01:44:02,880 --> 01:44:11,040
systems theory is now giving explanations of that that are derived from Aristotelian ontology,

978
01:44:11,040 --> 01:44:18,560
but make use of like a lot of cutting edge science that we've, we now can start to explain how there

979
01:44:18,560 --> 01:44:24,160
is a unity that is not reducible just to some summation of its parts and how that unity has a

980
01:44:24,160 --> 01:44:31,200
top-down influence on the entity that is not reducible to its causes. And I think this is

981
01:44:31,200 --> 01:44:37,280
becoming a non-controversial thing to say. And then I find and now we might just add a

982
01:44:37,280 --> 01:44:43,440
clash of intuitions and I'm willing to stop there. I can't see there being like that seems to me

983
01:44:43,440 --> 01:44:46,800
to be capturing what we're talking about. And you have an intuition that there's something more,

984
01:44:46,800 --> 01:44:50,320
but I don't know, I don't see a something more. And maybe that's where we're sitting.

985
01:44:50,320 --> 01:44:56,320
Well, I think it's the intuition David has in tell me if I'm wrong, David, and because it connects

986
01:44:56,400 --> 01:45:03,280
the way I think is that there's it's that unity is given. It cannot be made. And I know that sounds

987
01:45:03,280 --> 01:45:09,040
weird, but it's somehow it's like if I'm making even even internal technology, right? It's like

988
01:45:09,040 --> 01:45:16,400
if I'm making a car that unity is given, I'm gathering things towards that purpose, right?

989
01:45:17,040 --> 01:45:22,800
And so the purpose, the unity part of something is always, it comes from heaven in the sense that

990
01:45:22,800 --> 01:45:30,240
you can't make it. It's it's given from from it's already taken for granted even before you start

991
01:45:30,240 --> 01:45:38,080
to unify multiplicity together and that in the making of these beings, we have that problem. It's

992
01:45:38,080 --> 01:45:44,240
like we're doing it completely. We're doing a bottom up. Like if we can we gather as enough stuff

993
01:45:44,240 --> 01:45:53,520
so that this stuff reaches a unit, right? See, if I just could just it seems to me that if this is

994
01:45:53,520 --> 01:45:58,160
ever going to be possible, it would have to take so. And when I raise the question, it's actually

995
01:45:58,160 --> 01:46:02,080
a question. So I don't mean to be like challenging that it can't possibly happen. I'm just thinking

996
01:46:02,080 --> 01:46:07,280
about what would be the condition. Please remember that I said, yeah, we realize that we can't and

997
01:46:07,280 --> 01:46:13,280
that would be important. I am. Well, yeah. So it seems to me if it were to be possible,

998
01:46:13,280 --> 01:46:23,040
it would have to be something like a kind of electronic analog to cloning that you that you

999
01:46:23,040 --> 01:46:30,160
take. So what have I told you that we now have electro bio like we have systems that are electro

1000
01:46:30,160 --> 01:46:37,360
chemical biological versions of memory that are now in production and they we don't make them.

1001
01:46:37,360 --> 01:46:42,320
They self organize and emerge and they emerge bottom up from the causal interactions, but they

1002
01:46:42,320 --> 01:46:48,160
are also top down constrained by, you know, principles of self organization. Like that already

1003
01:46:48,160 --> 01:46:53,760
exists. Yeah. Yeah. But right. No. Well, that's that's what I'm asking about because it because it

1004
01:46:53,760 --> 01:46:58,000
seems to me but there is there is going to be you're deriving that from models. You're deriving

1005
01:46:58,000 --> 01:47:04,160
from real intelligent beings now, which is a slightly different thing. And I mean that would

1006
01:47:04,160 --> 01:47:10,320
be no thing is is is is that because I to me that the bottom up top down is not quite

1007
01:47:11,040 --> 01:47:16,480
adequate and the top down constraints is not quite because it would have to be not just

1008
01:47:16,480 --> 01:47:22,000
a constraint, but because that presupposes that there was something there that then

1009
01:47:22,960 --> 01:47:27,600
that the constraint is coming from outside. And what I'm talking about is a kind of a

1010
01:47:27,600 --> 01:47:35,920
unity that precedes that's presupposed. And I'm wondering how you can get that into something

1011
01:47:35,920 --> 01:47:41,280
if it's the very nature of it to be presupposed. And I'm not saying you can't, but I'm saying

1012
01:47:41,280 --> 01:47:47,360
if you can it's it's it seems to me that you're going to have to somehow derive it from a living

1013
01:47:47,360 --> 01:47:54,480
thing. And that's conceivable. That's conceivable, I suppose, but but but we are talking about

1014
01:47:54,480 --> 01:48:01,840
something really frightening. We are. And that's why I keep saying it's a threshold. And I mean,

1015
01:48:01,920 --> 01:48:08,080
I mean, if you take the the sort of biological analogy seriously, the way

1016
01:48:08,080 --> 01:48:13,360
Ori does, of course, it precedes the organism, it's there in the environment, it's there in

1017
01:48:13,360 --> 01:48:20,480
the society, it's there in the I mean, I can roll in 100 Hegelian arguments here about how it does

1018
01:48:20,480 --> 01:48:26,640
right about how and you know, and those don't have to be supernaturalistic. You have brandom

1019
01:48:26,640 --> 01:48:30,800
and hinker and others saying, no, this can be given a completely naturalistic explanation.

1020
01:48:31,600 --> 01:48:37,680
And and and I'm not here to challenge things. But what I'm saying is

1021
01:48:41,040 --> 01:48:49,440
I don't have any problem acknowledging everything you just said. Yeah. And I and and I don't think

1022
01:48:49,440 --> 01:48:57,440
I'm misunderstanding you. That's what I'm saying. Yeah. And I mean, I'm actually I mean,

1023
01:48:57,520 --> 01:49:02,080
this sort of just an exploratory sort of way. But I wonder if there's a difference between

1024
01:49:02,080 --> 01:49:07,760
the unity of an of an organism. And this is where Hegel might not be so helpful. The difference

1025
01:49:07,760 --> 01:49:14,160
between the the givenness of the unity of an organism and the givenness of the unity of a society

1026
01:49:15,360 --> 01:49:20,640
or a culture. But those aren't exactly the same thing. Because I there's there's something

1027
01:49:20,640 --> 01:49:25,840
and there's a kind of, you know, relative priority of either one. But there's something really

1028
01:49:25,840 --> 01:49:32,880
distinctive about the unity of an organism that's very. Yeah, that that that I think is crucial

1029
01:49:32,880 --> 01:49:37,600
to this question, to my mind, in a way. And I'm not saying it can't be answered, but that's the

1030
01:49:37,600 --> 01:49:43,600
question would have to be answered. How do we actually reproduce that kind of or unity?

1031
01:49:44,480 --> 01:49:49,680
We know stuff that Aristotle didn't know. You are not an Aristotelian unity. You are a society.

1032
01:49:49,680 --> 01:49:56,240
You literally are billions of animals, right? And so that's important. And that means that

1033
01:49:56,240 --> 01:50:01,840
there might not be a difference in kind between how you are organized as a living thing and how

1034
01:50:01,840 --> 01:50:08,000
societies are organized. And people like Michael Levin are producing some really important empirical

1035
01:50:08,000 --> 01:50:13,360
evidence indicating that's kind of the case. And I'm not saying it's not saying anything's

1036
01:50:13,360 --> 01:50:21,600
conclusive, but it needs to be taken seriously. Yeah, yeah. Yeah, I think that that I agree

1037
01:50:21,600 --> 01:50:26,480
with you, John. I think that that that's the way that I try to always speak about agency

1038
01:50:26,480 --> 01:50:32,480
intelligence is one that tries to scale almost effortlessly through the different, you know,

1039
01:50:32,480 --> 01:50:40,960
to avoid the woo soul that we're afraid of. But then again, this is the this is the issue. Like

1040
01:50:40,960 --> 01:50:46,240
this is in some ways that it's the same problem like one way or the other. So let's say you have a

1041
01:50:46,240 --> 01:50:53,440
group that self organizes around a purpose, right, or self organizes around affiliation or some type

1042
01:50:53,440 --> 01:50:59,440
of origin, right? That affiliation, that purpose is also given, right? It's like it appears as a

1043
01:50:59,440 --> 01:51:03,920
revelation. And then all of a sudden, we're all hunting a lion together. And now we're a group

1044
01:51:03,920 --> 01:51:11,120
and we're moving towards towards a purpose. Now, this is this is the this is the problem with the

1045
01:51:11,120 --> 01:51:18,000
situation of what's going now is that what is it? What angel are we catching? Like what what what

1046
01:51:18,000 --> 01:51:23,600
God are we trying to to manifest? Like which unity what purpose? We have no idea. So we're

1047
01:51:23,600 --> 01:51:28,880
building this massive body, like this huge, the most powerful body that's ever existed.

1048
01:51:28,880 --> 01:51:33,520
But nobody knows what it is we're trying to catch. Because when if I get together with a

1049
01:51:33,520 --> 01:51:38,800
bunch of guys to play basketball, I know what that body is. I know what that with that that that

1050
01:51:39,440 --> 01:51:46,240
agentic body, intelligent body is is is moving towards right. If I get together with my family

1051
01:51:46,240 --> 01:51:50,800
and I celebrate our unity is because I know that we all come from the same parent and that there's

1052
01:51:50,800 --> 01:51:57,600
a there's affiliation that makes our society coherent towards something. But now we have this

1053
01:51:57,600 --> 01:52:02,800
problem, which is what? Like what are we doing? Like we're just building this giant body. It's

1054
01:52:02,800 --> 01:52:10,400
like I agree. And I've agreed with that. Yeah. And the thing that that's so odd, I mean, typically,

1055
01:52:10,400 --> 01:52:18,560
if you think of technology as a human creation in some good positive sense, it's it's it has limits.

1056
01:52:18,560 --> 01:52:23,520
And it has a particular place. It has a particular meaning has particular purpose, precisely because

1057
01:52:24,240 --> 01:52:29,840
we create it in order to solve some kind of a problem. We you know, there's some there's some

1058
01:52:29,920 --> 01:52:35,680
need that needs to be filled and that need has a kind of natural givenness or or or or it's revealed

1059
01:52:35,680 --> 01:52:39,920
somehow that, you know, it's a responsive to something that we see. What's so interesting,

1060
01:52:39,920 --> 01:52:49,760
Neil Postman made this point about, you know, when when he said he went to a car dealership

1061
01:52:49,760 --> 01:52:54,960
and wanted to buy a car and the man was explained to him that they had now these, you know, automatic

1062
01:52:54,960 --> 01:53:01,440
windows that that that would roll down at the push of a button. And he and he said he said,

1063
01:53:01,440 --> 01:53:05,840
his his I mean, this sounds so naive, but it's a profoundly interesting question. He said, well,

1064
01:53:05,840 --> 01:53:12,320
what problem does that solve? And of course, the problem that it solves is the problem of

1065
01:53:12,320 --> 01:53:19,440
rolling a window down. And his response was, I never perceived that to be a problem. You know,

1066
01:53:19,520 --> 01:53:24,400
I mean, and it's really interesting AI. I mean, the thing is, what problem are we creating it

1067
01:53:24,400 --> 01:53:29,520
to solve? I mean, in a certain sense, it's a very different mindset. We're just kind of

1068
01:53:30,640 --> 01:53:35,840
taking we just want to see what we can do and see what can be done. And in a way,

1069
01:53:37,360 --> 01:53:42,720
the problems are something that we are arriving at and are surprising us rather than

1070
01:53:42,720 --> 01:53:48,800
something that we're actually creating something that just simple, simple task of solving for us.

1071
01:53:48,800 --> 01:53:55,280
You see, I mean, I think that's connected to this being placing ourselves in that in the hands

1072
01:53:55,280 --> 01:54:03,200
of an angel of some sort, or, or, you know, entering into a kind of an agency that's bigger than we

1073
01:54:03,200 --> 01:54:11,360
are. Those are all connected. They are. But I mean, one problem was trying to be solved was the

1074
01:54:11,360 --> 01:54:16,880
scientific problem of like, strongly, I was a project of explaining intelligence. And that's

1075
01:54:17,200 --> 01:54:21,680
that's a worthy thing to do. And the fact that this technology has largely been separated.

1076
01:54:21,680 --> 01:54:27,200
But notice, that's interesting. That's, that's, that's not a technical problem. Like,

1077
01:54:28,480 --> 01:54:33,120
explaining something, it's actually, I mean, to use the classical distinction between

1078
01:54:33,120 --> 01:54:37,360
theory and practice, that's a sort of a theoretical issue rather than a practical one.

1079
01:54:37,360 --> 01:54:43,040
But we think of this as a, as a technology. I mean, it's a, that's a curious thing.

1080
01:54:43,600 --> 01:54:49,280
Well, yeah, and I would get into things like books, our technologies that move between the

1081
01:54:49,280 --> 01:54:55,040
theoretical and practical. And it's one of the greatest technologies we ever invented. And it

1082
01:54:55,040 --> 01:55:01,680
had all kinds of unforeseen consequences. And really massively disrupted society. But, you know,

1083
01:55:02,640 --> 01:55:06,640
and, but I wanted to make another point. And this isn't a challenge. This is just a clarification

1084
01:55:06,720 --> 01:55:12,960
point, right? These like, like, think about a computer, what problem does a computer solve,

1085
01:55:12,960 --> 01:55:19,040
it doesn't solve a problem, it is meant to be a multiple problem solver. And then what we're

1086
01:55:19,040 --> 01:55:22,560
trying to do is make a general problem solver. So what problem is it's trying to solve? It's not

1087
01:55:22,560 --> 01:55:27,760
trying to solve any problem. It's trying to enhance our abilities to solve all the problems we try to

1088
01:55:27,760 --> 01:55:32,080
solve. So this machine's going to help us in medicine, it already is, it's going to help us,

1089
01:55:32,080 --> 01:55:41,200
right? And so that's the answer. Now, again, that's not a challenge. I'm just speaking on behalf

1090
01:55:41,200 --> 01:55:45,760
of people that think about this. But it is kind of interesting. I mean, so the problem that it's

1091
01:55:45,760 --> 01:55:51,280
solving is the need to be able to solve any possible problem. Solving a meta problem, yeah.

1092
01:55:51,280 --> 01:55:57,360
Yeah, yeah. But I mean, but it is kind of, it's sort of, it's sort of curious that,

1093
01:55:57,440 --> 01:56:10,160
precisely because of the indeterminacy of that, we're exposing this, and I'm just sort of stating,

1094
01:56:10,160 --> 01:56:17,600
you know, our condition here in a way, we're exposing ourselves to a really great risk. I'm

1095
01:56:17,600 --> 01:56:23,680
just restating what everybody has been saying here today. But that's something that, you know,

1096
01:56:23,680 --> 01:56:30,080
requires some wisdom, as you've been saying over and over, John, and prayer to use Jonathan's

1097
01:56:30,880 --> 01:56:39,520
language too. I just want to do one, Ken, you mentioned in my essay, which is we have done

1098
01:56:39,520 --> 01:56:44,480
this before. That's how civilization emerged. Nobody built it to solve a problem. There's

1099
01:56:44,480 --> 01:56:50,000
a bunch of little problems. And what civilization is, is a meta problem solver, right? And that's

1100
01:56:50,000 --> 01:56:56,800
what it is. And then you can, so I've actually suggested we should also be paying attention to

1101
01:56:56,800 --> 01:57:03,840
the lifetimes and the life cycles of civilizations and how civilizations reproduce, and why they rise

1102
01:57:03,840 --> 01:57:09,840
and why they fall to get some better understanding, some other ways of thinking about these machines.

1103
01:57:11,840 --> 01:57:17,120
So we're- Civilizations are huge distributed cognition collective intelligence machines.

1104
01:57:17,200 --> 01:57:22,720
That's the living in cities is a horrible idea, except for the fact that it gives us

1105
01:57:23,440 --> 01:57:28,400
better access to the collective intelligence of distributed cognition. That's the benefit that

1106
01:57:28,400 --> 01:57:35,120
outweighs all the many noxious side effects of living in cities. You can also get better coffee,

1107
01:57:35,120 --> 01:57:45,360
typically. Yeah. So we're coming toward the end of our time here. I mean, this has been fantastic.

1108
01:57:45,360 --> 01:57:50,000
I rarely can go for two hours on any conversation. We were just, we've just been going.

1109
01:57:50,000 --> 01:57:53,440
Well, not only go for two hours, but sort of wish we had another two.

1110
01:57:54,480 --> 01:57:58,000
Well, that's what I was going to say. I mean, we can, we can, we can work on doing this again,

1111
01:57:58,000 --> 01:58:02,240
because it feels like we're, we're sort of, we finally all come together around

1112
01:58:02,240 --> 01:58:06,480
something here. And now we're really asking what feels like a really important question to me is,

1113
01:58:07,040 --> 01:58:13,200
well, how do we think about integrating this solution, this meta solution into our meta problems?

1114
01:58:13,200 --> 01:58:16,960
And that's, that's a really interesting question. I think that John bringing up civilization is

1115
01:58:16,960 --> 01:58:23,200
such a great point, something that I would really love to explore, because there is also, you know,

1116
01:58:23,200 --> 01:58:29,360
in the kind of inscribed in the mythological stories, a relationship between transpersonal agency

1117
01:58:29,360 --> 01:58:36,160
and civilization itself, right? Like if you want to understand why the Egyptians had their king

1118
01:58:36,160 --> 01:58:40,320
as a god and like all this type of structure, you can help, it can help you understand how

1119
01:58:40,320 --> 01:58:44,800
they're trying to capture, you know, higher forms of intelligence distributed

1120
01:58:45,920 --> 01:58:50,400
intelligences in their society. And the idea that we would be doing this

1121
01:58:50,960 --> 01:58:56,400
technically in AI, I think is something that definitely is worth thinking about and discussing.

1122
01:58:56,400 --> 01:59:01,760
Yeah. Yeah. No, that's a dimension I just never struck me before. So that's, that's really helpful.

1123
01:59:01,760 --> 01:59:05,360
Is this the, maybe, maybe there's something we could read together and

1124
01:59:06,320 --> 01:59:17,680
I mean, short. But to prompt another conversation along these lines of civilization, yeah.

1125
01:59:18,720 --> 01:59:24,320
I would recommend just because this is how YouTube works that we come to decisions about that off

1126
01:59:24,320 --> 01:59:32,000
camera. All right. Okay. There you go. Right. I do, if there's a call for me to hang out with you

1127
01:59:32,000 --> 01:59:37,680
fellows again. I don't care what we're talking about. I'm in. I want to be here. I want to do it.

1128
01:59:38,320 --> 01:59:42,160
So that's all I'll say about the invitation. Same here.

1129
01:59:44,400 --> 01:59:49,120
Any closing thoughts, things that feel like need to be brought in or do we feel good about this?

1130
01:59:49,760 --> 01:59:56,800
Just a word of thank you, Ken. You were the one who arranged this and you did the persistent work

1131
01:59:56,800 --> 02:00:04,080
to make it happen and find a hole in everybody's calendar that lined up. Not an easy thing to do.

1132
02:00:04,080 --> 02:00:10,720
So thank you, Ken. And thanks for being gracious for these years now that I've known you. Yeah.

1133
02:00:12,720 --> 02:00:16,400
And in addition to thinking, Ken, I want to thank you, David and you, Jonathan.

1134
02:00:17,280 --> 02:00:23,600
I always find it, I get to places in my thinking, the logos that I could not possibly get on my own

1135
02:00:23,600 --> 02:00:29,360
when I get into a living relationship with both of you in conversation and in discussion.

1136
02:00:29,360 --> 02:00:31,920
And so I appreciate it greatly. And I just wanted to say thank you.

1137
02:00:32,640 --> 02:00:35,920
Yeah. Thanks to you guys. This has been great. And like I can say,

1138
02:00:36,640 --> 02:00:39,680
John and I've been trying to have the conversation for nine months and we just keep,

1139
02:00:39,680 --> 02:00:45,120
like I cancel, he cancels, and then this is wonderful that we were able to finally get here.

1140
02:00:45,760 --> 02:00:47,440
Yeah. Well, thank you all. It's been a real pleasure.

