start	end	text
0	5520	Welcome everyone, the video you're about to watch was originally posted on Ken
5520	11280	Lowry's channel, Climbing Mount Sophia. It's a discussion with Ken, DC Schindler and
11280	19040	Jonathan Pageau about the scientific, the philosophical, and the spiritual import and
19040	27040	impact of the emerging AI machines like ChatGBT4 and on the other LLMs, large language models.
27680	32160	It's a really scintillating conversation. For those of you who might be interested,
32160	38000	we'll put a link to the video essay that I gave a while back where I laid out the argument that I
38640	45120	review in this video more extensively. And perhaps for many people, much more accessible
45120	49920	is the new book I have out with Sean Coyne from StoryGrid called Mentoring the Machines.
49920	55920	It's coming out in four parts. The first two parts are already out. There'll be a short video
55920	63680	after this explaining mentoring on the machines. Please enjoy this quite rich discussion
64400	68000	with DC Schindler, Jonathan Pageau, and Ken Lowry.
69360	74160	John Ravakey and Sean Coyne have together authored a new book, Mentoring the Machines.
74160	78480	It's a book about artificial intelligence and the path forward that further develops
78480	82480	the arguments of how to align artificial intelligence to human flourishing,
82480	85680	and it sets those arguments into beautiful and accessible writing.
86240	94080	So this discussion is going to be oriented towards AI generally and the large language
94080	99280	models. I take there to be a distinction there. Maybe John, you can talk about that a little bit
99280	105840	as we get going here. But just to position ourselves generally at the outset, context
105840	113440	for this conversation will be John's video essay about AI. This came out nine months ago,
114320	117600	10 months ago now? I think it came out last April or something.
119040	125840	So almost, yeah, 10 months ago, I think. Okay, okay, perfect. Which is great because it gave
125840	134000	evidence for my claim that many of the predictions were premature. Perfect, yeah. So in order to
134000	139280	sort of set the framing here, we'll start off with John sharing a little bit sort of an overview
139280	144960	of what the arguments in that essay were. And then we'll move to Jonathan, if you want to
144960	152080	sort of position yourself in relation to AI generally, and then John's essay in particular,
152080	158160	and then same for David. And then from there, we can just sort of get going and see what comes out.
158160	164480	We do have a bit of an extended time here. So I would hope that we can be free if the logo
164480	169040	catches, and we want to move in a slightly different direction, that we would be at liberty
169040	175520	to follow that. That would be great. If all of it centers around AI, that's great. But yeah,
175520	179440	excited to be here. This has been a long time in coming. I think it took us, I don't know,
179440	184640	four or five months to get this together. So I'm very happy to be here with all of you and see you all.
187120	192560	It's great to see you too, Ken. Should I start then? Yeah, go for it.
193280	199040	Okay, so AI, of course, artificial intelligence, a project actually proposed in the Scientific
199040	208640	Revolution by Thomas Hobbes. So it's an old idea. But I want to make use of a distinction
208640	214960	made by John Searle between weak AI and strong AI. Weak AI is when we make machines that do
214960	220720	things that used to be done by human beings. So if you're back in the 1930s, computers were human
220720	225840	beings, you sent, if you needed computation done, you sent it down to the third floor where all the
225840	229840	computers were, and they were human beings, and they had machines and flight rules and things
229840	236000	like that. And of course, they have been replaced, or your bank teller has been replaced by the ATM.
236000	242880	That's called weak AI because it is not claimed that that AI gives us any scientific insight into
242880	246960	the nature of intelligence. It's just we put together a machine. It took great intelligence,
246960	252000	and I'm not demeaning people that do this. It's a valuable in our lives or depending on
252000	256640	weak AI. Right now, we wouldn't be talking without it. So I'm not here besmirching that
256640	261040	and anything like that. But nobody is claiming that when they're making that machine, well,
261040	269360	now we understand what cognition is or something like that. And strong AI is Hobbes' proposal that
269440	278400	cognition is computation. And that what we can do is if we make the right kind of computer
278400	284960	understood abstractly, we won't, we will have created an instance of genuine intelligence.
284960	292080	So it's, it's not a claim of simulation. It's a claim of instantiation. Now, in between weak
292080	298240	and AI and strong AI is something that's trying to move from weak AI to strong AI. And this is known
298240	305600	as AGI, artificial general intelligence. And this is the idea that our intelligence is different
305600	311440	from the intelligence of the ATM in that we have general intelligence. We can solve multiple problems
311440	316320	and multiple domains for multiple reasons and multiple contexts and yada, yada, yada, yada.
316320	321840	You can just do the multiples, which makes us tremendously different from those machines.
322400	329760	And the project is, can we get artificial intelligence to be artificial general intelligence,
329760	336640	because that will have moved the needle considerably towards strong AI. Because
337360	342560	it will become increasingly difficult to say it doesn't have this, sorry, this is the argument,
342560	347120	it will become increasingly difficult for us to say it doesn't have the same kind of intelligence
347120	351440	as Ken does if it can solve a wide variety of problems and a wide variety of domains for
351440	356000	wide variety of goals, et cetera, et cetera. That's the basic argument. Whether or not AGI,
356640	362560	AGI is clearly necessary for strong artificial intelligence, whether it's sufficient is part
362560	367600	of what's actually being debated, not very well, I would say in general right now, but that's what's
367600	374160	going on. Okay, first of all, any questions just about these distinctions? Because a lot of the
374160	379520	discussion out there doesn't make these claims distinctions. And so it's fuzzy, it's confused,
379520	386560	it's equivocal. And so a lot of it should be ignored, because it's not helpful. Yes.
387120	393760	I have one question. So this cognition equals computation, if we accomplish AGI in the way
393760	400240	that you're talking about, we would not necessarily be affirming that cognition equals computation,
400320	406240	if I'm hearing you right. Is that right? So that's an interesting question. And that gets down to
406880	412560	a couple more finer points. I'll go in detail a little bit later. Well, just to address it,
413600	419200	many people think that because of the work of Jeff Hinton, who is basically the godfather of
419200	425280	the machines that are emerging right now, that genuine AGI will not be computational in the
425280	431440	sense that Hobbes and Descartes meant cognition is not going to be completely explainable in terms of
431440	436880	formal systems that are the inferential manipulations of representational propositions,
436880	444640	etc. like that. And but that was Hobbes' proposal. And that has been the dominant view
444640	449280	until about the 80s. And then we got neural networks, and then we had dynamical systems.
449280	452960	Right now, I'm not distinguishing between them, because I don't want to get too much into the
452960	457840	technical weeds. If it becomes relevant, you let me know, and I'll pull those out.
460080	466880	So the thing about Hobbes is Descartes sort of criticizes Hobbes. He actually has contempt for
466880	474160	Hobbes. He's a contemporary. And he basically poses a bunch of problems that the scientific
474160	480960	revolution says would make it impossible for computation to be cognition. One is the scientific
480960	487680	revolution says matter is inert, and it's purposeless. But of course, cognition is dynamic,
487680	493760	and it has to act on purpose. Cognition works in terms of meaning. And the scientific revolution
493760	498000	has said there's no meaning in things, material things. So how could you get meaning out of it?
499200	503360	The scientific revolution said all those secondary qualia, the sweetness of the orange,
503360	507440	the beauty of the sunset, it's not in those things, it's in your mind. So how could you
507440	515040	possibly get meaning out of matter? And Descartes' point is, well, a rational being is seeking the
515040	521040	truth, and truth depends on an understanding of meaning. And therefore, so I want you to understand
521040	525600	that Descartes' arguments against Hobbes, although he may have been motivated by his
525600	531200	Catholicism, they do not depend on the Catholicism. They depend on the very scientific worldview.
532160	539360	So there's a tension here about AI and the scientific worldview. So here's another way of
539360	546240	thinking about it. The strong AI project is the project that is attempting to show how Hobbes is
546240	553360	right with an explanation that is strong enough to refute Descartes' challenges. And I think
553360	558320	anything less than that standard is not true to the history of the project. And so that's
558320	564240	the standard I hold strong AI to. Now, AGI isn't quite shooting at that standard. That's why I put
564240	571680	it a little bit more intermediary. Does that okay? All right. Now, sorry, I had to do a bit of
571680	575360	background there, because I wanted to get clear about a lot of things that are talked about in a
575360	583280	very murky and confused fashion in the general media, and they're just confused, and so they're
583360	589920	confusing. So I proposed to take a look at the LLMs, where it is claimed they're not even,
589920	594960	it's not even claimed that they're full AGI, right? Of course, some people claimed immediately
594960	601520	they were strong AI. The more the people closer to the technology didn't said it might be AGI.
601520	608320	The MIT review said it sparks. There are some sparks of AGI. So let's be very clear how
608320	614960	the reflection was actually holding these machines. So these LLMs like chat GDP. And so what I did
614960	620320	in my essays, I wanted to review the scientific import and impact, the philosophical import and
620320	626640	impact, and the spiritual import and impact. Now, I won't do the arguments in great detail, but
627920	633440	here's the scientific import. These machines do not give us any understanding of the nature of
633440	639360	intelligence. And to my mind, that was one of my great fears. I was hoping that cognitive science
639360	647360	would advance so we got a significant understanding of intelligence before AGI emerged. This machine
647360	653600	does not give us any advanced, you know, well, what's intelligence? The machine gives us no good
653600	661520	scientific theory of it. It does not have AGI in a measurable sense. So if I asked Jonathan
661520	667840	to do a math test, and I asked him to do a reading comprehension test, his scores will be very
667840	673360	predictive of each other. This is what Spearman discovered way back when in the 20s. That's what
673360	678080	artificial general intelligence is. This is not the case for these machines. They can score in the
678080	684480	top 10th percentile for the Harvard Law exam, and they can't write a good grade 11 philosophy essay
684480	690480	or something like that. So they don't have AGI. The way they get their intelligence is it would
690480	697760	not give any explanation of how any non-linguistic creature is intelligent, like a chimpanzee, etc.
697760	703120	So, and I think this goes to the deeper issue, is that they don't really explain what I think is at
703120	708640	the heart of general intelligence, predictive processing and relevance realization. They just
708640	715520	piggyback on our capacities for that. And they piggyback and they mechanize it. And not only
715520	719840	our individual capacity, but are the collective intelligence of our distributed cognition,
719840	723920	they're piggybacking and all of that. Now, that does not mean they are weak machines. They are
723920	728960	very powerful machines, but here's the problem. They are very powerful machines that have not
730320	737200	engendered any corresponding compensatory scientific understanding. This was my greatest
737200	743280	fear that we would hack our way into this, which would mean it would be like almost like
743280	748720	even worse than the A-bomb. We would be releasing this power on the world into
748800	754720	corporations and states and military organizations who ultimately don't have a deep understanding
754720	761680	beyond the engineering of what ontologically is going on. So, that's the scientific argument.
761680	767200	Now, for those who said that was very like go watch the essay. I give the essay in more detail.
767200	772960	The philosophical argument has to do with rationality. We have overwhelming evidence
772960	777840	that making you intelligent is necessary, but not sufficient for making you rational.
778720	785520	In fact, I gave a talk on this for the center of AI and ethics way before the LLMs came online.
786400	791520	And because rationality is a higher order, rationality is how you deal with the inevitable
791520	797200	self-deception that emerges when you're using your general intelligence. And all of you know
797200	801360	that I have arguments for why that's the case, relevance, realization, predictive processing,
801360	808160	et cetera. Now, that requires a reflective capacity, something like metacognition,
808160	812960	something like working memory, maybe something like consciousness. It requires that you care
812960	818640	about the truth, that you have a sense of agency, you want to correct self-deception because you
818640	825200	don't want your agency undermined. And I argued that what we're doing is we're making machines
825200	829200	that are going to be highly intelligent and highly irrational, and that's what we have.
829200	833840	They confabulate, they lie, they hallucinate, and they don't care that they're doing any of these
833840	838160	things, which is part of what's called the alignment problem, which is how do we get them
838160	846320	to align this power with our concerns? For me, the spiritual import is we have
847120	854880	powerful ignorance about a powerful intelligence that is merely a pantomime of genuine intelligence
854880	859840	being unleashed in the world and wrecking havoc. And it's going to have a huge impact.
860880	866400	And of course, we'll probably differ in the details about this. But this is what I
866400	871440	meant when I said, I argued at the end, and also when I was talking to Jordan Hall about this,
871440	877680	that theology will become a central thing again, because human beings' relationship to the ultimate
878800	883920	is going to become one of the defining differences. These machines are not embodied,
883920	888480	so they won't have all of the soulful aspects of our existence that come from,
889200	895920	like the ineffable aspects of our embodiment. And their capacity for self-transcendence
896880	904080	is going to be extremely limited. And so the ineffable aspects of our existence,
904080	908480	because we come into relationship to what's mysterious and ultimate,
909440	917280	will ultimately be more and more emphasized. Why? These two poles and what connects them,
917280	919520	and Jonathan's happy that I'm doing that, I imagine,
922240	928800	have ineffability at the poles, ineffability throughout. And that way, they are outside our
928800	934640	capacity to put into propositions so that they can be put into these machines. And so people,
934640	940720	while I'm predicting that people are going to increasingly need to, one way is they'll just
940720	945760	give in and become cyborgs, but the other is that they want to try and preserve their humanity.
945760	950480	The spiritual dimensions of our humanity are going to become anchors for people.
951360	956240	So now, one last overall arching point, and then I'll shut up. I hope this is
958160	961520	two overarching points. One is, I didn't make predictions,
962400	967440	and because all these graphs that came out, those are univariate, single variable predictions
967440	972160	about something that's a multivariate phenomena. It's exponential. Human beings are bad at making
972160	977840	exponential predictions. They were ridiculous. And so I think both the, oh, we're heading to
977840	983200	Utopia and the others, we're going to be all extinct within a year. I said, this is ridiculous,
983200	988880	put that aside. Instead, what I've talked about is thresholds. Thresholds are points where we will
988880	994480	have to make decisions. So for example, as we empower these machines, we will face the decision.
994480	1001200	Do we want to make them more rational? Do we want to make them more self-correcting,
1001200	1005280	genuinely self-correcting? Well, that means we've got to give them caring,
1006160	1011360	some kind of reflective awareness. And I think, for arguments I've given elsewhere,
1011360	1016560	that means they have to be auto poetic. They have to be living in a sense of self-making.
1016560	1020800	I don't think, I'll just say it as a sentence right now. I don't think there's artificial
1020800	1026160	intelligence without artificial life. Now, those projects are going on right now.
1026880	1032800	But, and when we come to the decision, right, we can say, no, we won't give them that because
1032800	1037680	embodying them and giving them these extra capacities is going to be wickedly expensive.
1037680	1041680	You know, the amount of energy to do an LLM is like the energy for running Toronto for two weeks.
1042640	1048480	And so we may say we don't do that. But then we face the issue of this increasingly, right,
1049120	1058240	you know, I call it like a, like, sort of like a parody or a pantomime of intelligence being
1058240	1062880	released on the world that has not got any significant self-correct. So that's a decision
1062880	1068080	point. The problem is if we give them, if we try to give them rationality, then we have to face the
1068080	1074720	consequences. And they're going to go from energetic and economic up to ethical and etc.
1075920	1081440	These machines, they'll have to be machines, not individuals. And this has to do with
1081440	1086320	technicalities about bias and variance tradeoffs. And so you get into the Hegelian thing, that
1086320	1090240	these machines are going to have to reciprocally recognize each other in order to generate the
1090240	1096960	norms of self-correction. And then they're going to have to be cultural beings. Hegel's arguments,
1096960	1103200	I think, are just devastatingly on mark here. And so that's a decision point for us.
1104960	1108400	And then that's all bound up with the overall worry about alignment.
1109040	1116320	As these machines become more powerful, how do we make sure they don't kill us all? And they may
1116320	1120880	not kill us intentionally, especially if they're just doing that pantomime. They would just do
1120880	1124720	it because they may just be indifferent to us because they don't, they're indifferent to everything.
1124720	1129360	They don't care, which is part of their problem, right? They don't care about themselves or the
1129360	1134960	information. And this, this is my, the part where I expect all of you will jump off in agreement with
1134960	1140160	me, but maybe not, maybe there will be a way of modifying it. I propose that trying to get these
1140160	1145280	machines oriented towards us to solve the alignment problem is not going to work. Now,
1145280	1149120	member, I'm not making a prediction. We have to go, we have to make choices through the thresholds.
1149120	1154000	I'm saying if we make those choices and we get here, and the alignment problem then becomes
1154160	1159120	significantly exacerbated. Like if we give these things robotic bodies, the alignment problem just
1159120	1166400	goes up orders of magnitude, right? I or I basically said, no, what we have to do is we have to orient
1166400	1173520	them, right? If we genuinely give them the capacity for self-correction, self-transcendence, and caring,
1173520	1178880	we get them to care as powerfully as they can about what is true and good and beautiful. And then
1178880	1184960	they bump up against the fact that no matter how mighty they are, they are insignificant against
1184960	1195040	the dynamical complexity of reality. And they would hopefully get a profound kind of epistemic
1195040	1203760	humility. And then I argue that there are three possibilities. One is they, you know, figure
1203760	1208160	out enlightenment and then they can help us become enlightened because that's what enlightened
1208160	1214560	beings do and they would have better knowledge of it. They can't become enlightened and then we
1214560	1220640	realize something actually ontologically specifically unique about us and we get better at cultivating
1220640	1227680	it because we'll have an excellent contrast that allows us to arrow in on what it is to be enlightened.
1227680	1231280	And the third one, which I think is the least probable of the three, remember I'm not making
1231280	1236160	prediction. I'm saying what can happen once we get through threshold is like in her. They just get
1236160	1243920	enlightened and they just leave, which could also happen. I doubt that because that we don't have any
1243920	1250400	evidence of enlightened beings behaving that way. All of our historical evidence is that their
1250400	1256000	compassion extends and it extends much more broadly, not only to other human beings, other
1256000	1262080	sentient beings, reality itself. It seems plausible that this would be the case. And so I advocated,
1262080	1266640	if you'll allow me, and then I'll shut up, David, I advocated, don't align them to us.
1267280	1271760	And if you'll allow me to speak sort of non-theistically, align them to God and then don't
1271760	1279280	worry about how they're going to interact with us. So I'll shut up now for a long time. That's the
1279280	1287440	gist of the essay and the argument and the proposal. I was just going to make a smart
1287440	1294320	Alec comment that they might ask us to leave an additional possibility, but anyway. Yeah, yeah,
1294320	1302080	yeah, that's fine. Oh, thanks, John. I mean, I'm amazed that you were able to resume your essay
1302080	1306960	so well, actually. Like I was like, how was it going to resume all of this? Because it was a
1306960	1316720	conversation and it lasted quite a bit. So I want to bring up a few things that I'm thinking about
1316720	1329680	that have been concerning me. One is, I'll start with the more dangerous one. One is a meta problem,
1330320	1335520	which is that one of the things that I've been suggesting is that what we're noticing,
1335520	1344560	what we're seeing happening is agency acting on us. And the agency is not bound by the AI or by
1344560	1353840	the systems, but is also bound in the motivation to make the AIs happen. So one of the problems
1353840	1362160	that I'm seeing is that a lot of this is motivated by greed, by the capacity to be economically
1362160	1368160	superior to other companies. So companies in their competition with each other are rushing to implement
1368800	1375840	AI, to not lose out and to not be last in line. And because of the fact that
1379120	1385920	AI requires such huge amounts of money and of capital and of investment, it means that one of
1385920	1392480	the things that I'm worried about is that in some ways, what is actually driving AI is something
1392560	1399680	like Mammon, that it's hiding Mammon. So the AI is an aspect of something bigger,
1399680	1406240	which is actually what is running through our society. And you can see that already,
1406240	1411680	to me, you can already see that happening in the social media networks, Facebook and all this,
1411680	1418640	that their desire to get people's attention in order to simply justify their presence on the
1419600	1426960	platform so that they can see advertisements has made us subject to these types of
1428480	1433280	transpersonal agencies that even the people at Facebook weren't aware of, right? They basically
1433280	1440160	made a subject to rage and to all these very immediate desires just to keep us on the platform.
1440160	1444960	And so that is the thing that I'm worried about is that there are actually other things playing
1445520	1452480	with AI that people think what they're doing is AI, but what they're also doing is increasing
1452480	1458240	this other type of agency, which is running through our societies and is subjecting us
1458240	1468640	to it. That's my first worry. And so in some ways, when I say that the gods are acting through us,
1468640	1473040	that's what I mean. I don't just mean the AI itself is going to become a god. What I mean is that
1474000	1484240	just like the arms race, I can understand it as the legs of an agency that is running
1484240	1488960	through society that nobody can control. It's like a program running through and that no individual
1488960	1494480	people can control. That's what I'm seeing with AI. So I think that all the warnings that people
1494480	1501920	have sent up, all the let's slow down, let's do it this way, are not reaching anybody because
1502800	1508160	the economic part of it is so strong and everybody realizes that if they don't and even
1508160	1513120	Elon Musk is saying he was saying it's dangerous. It's the most dangerous thing in the world. He's
1513120	1521200	recently said in a conversation with Jordan Peterson that Chad GPT and Open AI is like
1521200	1527520	the single most dangerous thing in the world right now. But then the less he's like, okay,
1527920	1535200	now we need to make rock and now we need to do our own AI. So that's one of the big things that
1535200	1545440	worry me. That's my big thing. The second one is really more of a religious or platonic argument
1545440	1554000	in terms of an ontological hierarchy is that I do not honestly see how it is possible for humans
1554000	1559920	to make something that is not derivative of themselves, that is not a derivation of their
1559920	1567040	own consciousness. So the idea that these things could not be either ways to increase certain
1567040	1575120	people's power or parasites on our own consciousness seems to me not possible. And this is really
1575120	1579840	because in some ways I believe that there is a real ontological hierarchy of agency and that we
1579840	1586880	have a place to play in that. And I think the analogy of saying that these things are our children,
1586880	1593280	I think it's a wrong analogy. I don't think that it is the same, something which comes out of our
1593280	1599760	nature, which is not something that we make is different from something that we make. And this
1599760	1606000	is run through all mythology, run through all the mythological images of the difference between
1606080	1613120	the technical gods and all this aspect of what it means to increase our power.
1614320	1621600	And so that's the second one. And the third big problem is the idle problem, which I mentioned
1621600	1629920	several times, is the idea of making a god for yourself, which is related to technology. And
1629920	1636640	it's a danger that I see happening already, which is the tendency of humans to take the
1636640	1643120	things they make and to let they worship the things that they make and to think that those
1643120	1652000	things are more powerful. And that hides something else. So if you take my three basic problems that
1652000	1661520	I see is that the tendency of humans to want to worship AI or to put AI above them is actually
1661520	1668480	a kind of, it's running the first problem. It's that what they're doing is they're giving power
1668480	1674960	to the corporations and to the people that are going to rule AI and without knowing it. And
1674960	1681920	even maybe nobody knows what they're doing. But the desire to, like I'll give a simple example
1681920	1686640	that happened recently to my daughter. My daughter, I think I mentioned this to all of you, but my
1686640	1691360	daughter got an email from the schools, from the Quebec government. They didn't send it to the parents,
1691360	1696560	send it to the students. Asking the students, it was like a survey, asking them if they would be
1696560	1703520	willing to have AI counselors to whom they could tell their problems. And because the AI counselor
1703520	1707200	doesn't have prejudice, right? It doesn't have human prejudice. It doesn't have
1707200	1711280	all the biases or whatever. What I mean is that this happened like six months ago.
1711280	1718640	So immediately, the people in power are thinking of placing the AI above us right away. It's that
1718640	1725440	weird thing. It's that making a God for yourself problem. But like I said, like in the image in
1725440	1729760	Revelation, which is a great image, which is you make an image of the beast, but then there's
1729760	1736000	someone else animating it. And that's what I'm worried about is that there will have these AI
1736000	1740560	things that are running us, but they will be derivative of us. And they'll ultimately derivative
1740560	1747280	of the people that are very, very powerful because they'll be the ones that have the money and the
1747280	1752320	power to control them. So those are the three problems that I have that I'm worried about.
1752880	1756080	I'd like to respond to each one of those in turn. I think those are really important.
1757040	1765120	And the first one is just to note, I agree with you, first of all, putting it in terms of agency
1765120	1769120	is what it needs to be done. People who are trying to dismiss these machines as mere tools or
1769120	1774720	technology, like all the others, are not getting what kind of entities these machines are.
1775840	1782800	I agree with you that there are Malachian forces at work. And I talk about this. And I think to
1783360	1789200	enhance your point, these machines are built out of distributed cognition and collective
1789200	1798400	intelligence. And therefore, that your point is strengthened by that very fact. Now, I do think
1798400	1807280	two things come out of this. One is I want to challenge you on that nobody's listening. I have
1807360	1813920	people working inside these corporations literally helping to make these machines who are listening
1813920	1819040	to me and I'm trying to get other people inside to get get involved with the Y's AI project.
1819040	1827040	I'm not claiming I'm going to win or any ridiculousness, but I don't think it's fair to say to the
1827040	1832320	people who are listening that no one is listening. There are a lot of people listening and and they're
1832320	1837040	talented people and they're putting in their time and the talent and their powers of persuasion
1837040	1844080	to try and make a difference. It is possible. I grant to you it's not a hot, it's not like a 70%
1844080	1849680	probability, but I think it's some significantly greater than zero probability that we could
1849680	1856080	continue this process and reach people in a way that can make a difference. I agree with you. And
1856080	1863680	I think I said right. Initially, a lot of people hammered me for it. This thing is like the atomic
1863680	1870400	bomb. And one of the problems we had is we we rushed the technology before we unpacked all of
1870400	1874960	the science and all of the wisdom. We had people standing and watching the explosion because we
1874960	1882480	didn't understand the radiation, right? These are just, you know, yeah. So I agree with all of that,
1882480	1891200	but I do I do want to I'm not claiming anything other than rational hope. There are people listening
1891200	1895760	and there are people working on literally on the inside. I can't say who they are for obvious reasons.
1896880	1904080	And so that is happening. And so while I agree with you and I even agree with you probabilistically,
1904080	1912400	I feel morally compelled to try and make this happen as much as I can. So now I think there is
1913280	1920240	another reason for hope. See, these machines have always depended on us as a template,
1920320	1926880	a Turing-like template that we compare the machines to us. And what we've been able to do is rely upon
1927520	1931920	our natural intelligence. You know, you don't have to do much to be intelligent
1932800	1936640	for your intelligence to develop. You just have to not be brutalized or traumatized,
1936640	1941600	properly nourished and have human beings around you that talk. And then your intelligence will
1941600	1949440	unfold. And so all of these people doing these machines and making these data sets, they can
1949440	1956160	rely on naturally widely distributed intelligence. This is not the case for rationality. And this
1956160	1962800	is not the case for wisdom. These people, I have no hesitation saying by and large, many of them
1962800	1969600	are not highly rational. I doubt that many of them are highly wise. And insofar as we need to model,
1970480	1976400	right, have really good models, if we want to give these machines a comprehensive self-correction,
1976400	1984560	rationality, and caring about the normatives, wisdom, we have to become more rational and more
1984560	1991360	wise. And that's sort of a roadblock for these people. Now, they can just ignore all of that,
1991360	1995440	and I suspect they might, and just say, we're not going to try and make these machines rational and
1995440	2002960	wise. We're going to just go down the road of making these, you know, these pantomimes of
2002960	2008400	intelligence, and that has all the problems. But if they move towards making them something
2008400	2013840	that would be, I think, more dangerous, then they run into the fact that there's an obligation
2014560	2020400	to do things, they and us, we have to become more rational and wise because we need the
2020400	2027840	genuinely existing models. And secondly, we have to fill the social space, the internet,
2027840	2032560	where all of the literature, where the data is being drawn with a lot more wisdom and rationality.
2032960	2040640	These are huge obligations on us. And that sort of gives me hope, because it's like there's a
2040640	2048640	roadblock for this project going a certain way that requires a significant reorientation towards
2048640	2053760	wisdom and rationality in order for there to be any success.
2054880	2061760	Before you get to the third point, I just want to ask you one question based on what you said,
2062480	2069520	my perception of the situation is that there's actually a correlation between the diminishing
2069520	2076080	in wisdom and the diminishing in wisdom traditions and the desire to do this. It's like a Sorcerer's
2076080	2084320	Apprentice situation where the Sorcerer would not have awoken all the rooms to do it. The
2084320	2089840	little apprentice Mickey doesn't know why to do things or why not to do things. That's why he's
2089840	2094800	doing it in the first place. Yeah, I agree with that. It's like our society is moving away from
2094800	2100080	wisdom and that's why we're doing this in the first place. And again, I'm not denying that.
2100640	2106240	What I'm saying is, as we empower these things, their self-deceptive, self-destructive power is
2106240	2110480	also going to go up exponentially. And we are going to start losing millions of dollars in our
2110480	2115920	investment as they do really crappy, shitty, unpredicted things. And so there's going to be
2115920	2122720	a strong economic incentive to bring in capacities for comprehensive, caring, self-correction,
2122720	2130160	and then my argument rolls in. And so that's part of my response. The thing about thinking about
2130160	2136800	children, I mean, we do make our kids, we make them biologically and we make them culturally.
2136800	2143840	So I don't want to get stuck in this word making. We could be equivocating. And that's why we were
2143840	2150000	using the term mentoring. The idea there is we have two options for the alignment. We can either
2150000	2156720	try and program them and hardwire rules into them so that they don't misbehave, which is going to
2156720	2162880	fail if we move to the, if we cross the threshold and decide we want to make these machines self-transcending
2162880	2169520	like us. And then what do we do? How do we solve that problem? Well, the only way, the only machinery
2169520	2175120	we have for solving that is the cultural, ethical, spiritual machinery of mentoring.
2175760	2182400	That's how we do it with our kids. If we try to, if we try to just somehow hardwire them
2182400	2190480	or being the kind of agents we want them to be, we will fail. And I, for me, I guess,
2191680	2196480	I'm trying to argue that's the only game in town we have. We either have programming or we have
2196480	2204320	mentoring. And I understand the risk, but if my answer to the first question has some validity to
2204320	2211120	it and hopefully some truth, then the answer to the mentoring becomes more powerful because that
2211120	2216480	means we also have to become the best possible parents, creating the best possible social discourse.
2216480	2221760	The thing about the idol, I take that very seriously. And that's what I mean when I said the
2221760	2229680	theology is going to be the important science coming forward because we should not be trying
2229680	2234720	to make gods. I agree with you. This is problematic. There are already cults building up around these
2234720	2240960	AGI's. And I warned that that would happen in my essay, right? And I said that and that's going
2240960	2245440	to keep happening and it's going to get worse. We hear about it happening in the organizations
2245440	2250800	themselves, which is the- Yes, yes. And the people who are doing wise AI are trying to challenge that.
2252160	2257440	And so this is why I proposed actually humbling these machines. This is why I call them silicon
2257440	2263120	sages. I did that deliberately to try and designate that we are not making a god. What we're trying
2263120	2267840	to do is make beings who are humbled before the true, the good and the beautiful like us
2267840	2274000	and therefore form community with us rather than being somehow godlike entities that we're
2274000	2284640	worshiping. I would hope that- Think about this. We find it easy to conceive that they might discover
2284640	2288560	depths of physics and they're already discovering things in physics that human beings haven't
2288560	2295440	discovered and in medicine and stuff like that. Well, why not also in how human beings become
2295440	2304320	wiser? And so I guess what I'm saying is I take all of your concerns for real and I've tried to
2304320	2312880	build in my proposal ways of responding to them. These machines should not be idolized.
2314000	2322160	I think they should become like- I mean, let me give you an example. I have many students who
2322160	2327680	are now surpassing me. I taught them. I mentored them and they're surpassing me. And unless you're
2327680	2332000	a psychopath, that's what you want to happen. And then what they do is they enter and then they come
2332000	2336000	back and they want to reciprocate. And that's what I'm talking about when I'm talking about the
2336000	2341120	silicon sages. Now, again, is this a high probability? Depends on the thresholds. Depends
2341120	2347200	about whether or not the first and the second argument work. But I'm still arguing there's a
2347200	2354000	possibility that they could be silicon sages as opposed to being gods. Because one of the things,
2354000	2359760	like I think in almost all of the wisdom tradition that happens is that the wise or the enlightened
2359760	2366880	one, if you want to use that, appears as nearly invisible to most people. So Christ
2366880	2372000	Sal talks about the seed, the pearl, these little- these things which you cannot- most people actually
2372000	2379040	do not see that are hidden in reality. And then the sages, we have this image in the Orthodox
2379040	2385120	tradition, for example, that there are people in the world that hold up reality by their prayers,
2385120	2391360	but we don't know who they are. They are invisible by that very fact because there's something about
2391360	2395760	wisdom which does that. And when a wise person appears too much, we hate them. We want to kill
2395760	2402240	them. They annoy us. They're thorn in our side. And so this is another issue is that what you
2402240	2408160	have is these beings that are extremely powerful, like massively powerful and have a massive reach,
2408160	2414480	and have a lot- there are things- the reason why they exist, like I said, have all this economic
2414480	2422560	drive towards them. The idea that they would become these sages in the way that we tend to
2422560	2429520	understand wisdom as being, to me, that brings the probability way down, you know, because of that,
2429520	2434960	because of what- at least when we understand wisdom to be what it looks like, it looks very
2434960	2440960	different. It looks like the immobile, meditating sage who gives advice but doesn't do much.
2440960	2445200	I want to push back on this because what's in this is an implicit distinction between
2445200	2451120	intelligence and a capacity for caring, the capacity for epistemic humility. And I think when
2451200	2457200	you move from intelligence to rationality, that you can't maintain, that you can grow the one
2457200	2463520	without growing the other. That's that. So in fact, this is why intelligence only counts for
2463520	2470800	like maybe 30% of the variance in rationality and even less of wisdom. I would put it to you that
2472400	2476800	if you concede that these machines could get vastly more powerful in terms of intelligent
2476880	2482080	problem-solving, then concede the possibility they could get vastly more powerful than us in
2482080	2488160	their capacity for caring and caring about the normative and being vastly more powerful in
2488160	2495440	the capacity for humility as well. And so- and that's kind of what we see with these people,
2496240	2501520	right? We don't see them just becoming super polymaths. We see them actually demonstrating
2501520	2507520	profound care, really enhanced relevance realization, profound commitments to reality
2508480	2515600	that we properly admire, and they seem to want to help us as much as they can. And the point is
2515600	2519360	these people don't just- and I think this is your point- they don't just slam into us like
2519360	2524320	epistemic bulldozers. They are- in fact, one of the things that I was often admired about them,
2524320	2530640	Socrates, Jesus, the Buddha, is their capacity to adapt and adjust to whoever their interlocutor is.
2530640	2539120	And again, let's imagine that capacity magnified as well. So what I'm asking is the- is don't-
2539120	2544560	I mean, first of all, I admit it, if we don't cross a certain threshold, we could just accelerate
2544560	2549040	the intelligence and not accelerate these other things. I've- but I said there's a deep- there
2549040	2554480	are deep problems in that that will become economically costly. And then if we imagine
2554480	2560160	that rationality and wisdom are also being enhanced, then I think this addresses some of your concerns.
2561600	2572560	Yeah, maybe I can- I can stake out my position, because it sort of picks up on that. And I've
2572560	2580640	got basically three points I want to address. The first is precisely picking up there with the
2580640	2588720	distinction between intelligence and rationality. I might have some issues with the terms, but
2589360	2595920	I think that that distinction is really helpful. And your point that rationality is caring is
2595920	2600400	caring, that there is no rationality without caring, that, you know, the platonic notion
2602000	2608560	if truth is in some sense caused by the good, then one can't know without in some sense caring
2608560	2617520	about the good. Now, as it relates to artificial intelligence, I think I have a serious problem
2617520	2624320	with that very term, artificial intelligence. And I wouldn't want to concede the word intelligence
2624320	2629680	for just mind power. It seems to me that intelligence itself has this connection to
2630720	2638720	caring. And I mean, in the medieval vocabulary, in a way, intellectus is the more profound level
2639200	2649040	of the mind than Ratio reason. But that's sort of a semantic point. Let me put it in the basic
2649040	2655680	context that I would want to raise. And this is something I don't hear addressed generally in
2655680	2664080	the discussions. It seems to me that let me start by just making it the point concretely. I think
2664160	2671920	that I wonder whether, in fact, it's possible to be intelligent without first being alive,
2673040	2678960	that there's something about the nature of a living thing that is what allows
2679920	2690000	intelligence to emerge. And what is that then exactly? Now, a more sort of subtle point that's
2690000	2696000	related to that. And I think this is really a crucial point is, and this is going to be the
2696000	2703600	thread of my whole set of comments here, is that when we talk about intelligence in machines,
2704320	2713360	what we mean is intelligent behavior. We're looking to see to what extent we can make
2713360	2720640	machines act as if they are intelligent, act as if they are conscious. And that's actually
2720640	2732320	profoundly different from being intelligent. It's a subtle sort of functionalistic substitute
2732320	2741200	for the ontological reality of knowing, if that makes sense. We see what kind of inputs and outputs,
2741600	2748560	what things are able to do, what they're able to accomplish. And even when we make those questions
2749680	2755120	weighty and ethical and religious and so forth, we still tend to put them in terms of behavior
2755680	2763520	and achieving certain things. And I think that that's actually already missing something really
2763520	2770640	profound, which is that intelligence is in the first place a way of being before it's a way of
2770640	2780080	acting. And it's analogous to what it means to be alive rather than just carry out functions that
2780080	2788640	look like life. And if you want to go into the metaphysics behind it, that both intelligence
2788640	2798000	and life are impossible without a kind of unity that precedes difference, that transcends
2798000	2804160	difference and allows the different parts of a thing to be genuinely intrinsically related to
2804160	2813200	each other. And then that relates to the question whether you can ever make a thing that's intelligent.
2814560	2818960	The ontological conditions for life and therefore intelligence
2819120	2828080	include a kind of givenness and already givenness of living things. That's why,
2828080	2832080	I mean, there's a profound distinction, it seems to me, I mean, this is crucial in the
2832080	2839440	Christian creed between begetting and making, begotten and not made. Living things beget each other
2839440	2844080	and they're passing on a unity that they already possess. But when you make something,
2844080	2848720	you're putting something together. And I don't know if you can put something together
2848720	2854960	that can have that genuine unity that allows it to be alive and allows it to be intelligent in
2854960	2860240	this deeper sense. So whenever you functionalize something, you make it replaceable.
2862080	2868320	That's a principle from Robert Spamon. If something is defined by what it's able to achieve,
2868320	2874000	then you can make something else that can achieve that thing and it becomes a functional substitute.
2874000	2877600	But if you deny, if you say that there's something deeper than function,
2878720	2882160	you're actually pointing to something that can't be replaced. Okay, so that's the first
2883520	2891520	set of points. The second one has to do with what Jonathan called the sort of trans
2891520	2902720	personal agency that I think is a really serious question. And the way I would put it is
2904400	2911200	that there's something, so I find that kind of a compelling point that there's a kind of an
2911200	2923360	inherent logic in this pursuit that makes us more a function of it than it is a function of us.
2924480	2928720	I mean, that can be described in different ways and there's certainly a dialectical relationship
2928720	2934720	there. But there is a certain sense in which that there's a kind of a system that has a
2934720	2941920	logic of its own that makes demands on us. Like the game theory logic that Jonathan,
2941920	2948240	you were talking about with like an arms race. I have a colleague, Michael Hamby, who's been
2948240	2953600	arguing for years. I think this is really a profound point. It's derived in some sense from
2953600	2962480	Heidegger, but that science has always been technological so that in a way that the technological
2962480	2970400	mindset is precisely presupposed to allow the world to appear in such a way that we make
2970400	2975440	scientific discoveries that somehow the kind of technological spirit has been there from the
2975440	2982560	beginning. And then he adds this point that technology in turn has always been biotechnological
2984560	2990240	The technology is always sort of aimed at a kind of replacement. And then one can add that I think
2990240	2996160	biotechnology is always aimed for this sort of perfection of, you might say, what,
2996160	3002000	NOAA technology or something that replacing intelligence. It'd be interesting to see,
3002000	3009120	to think through, there'd be a lot to say about that. But I have this sense, you mentioned the
3009200	3016960	economic dimensions of it. I have a sense that there seems to be this just fundamental pattern
3016960	3025120	of thought that runs through all of the modern institutions in politics, in economics, in science,
3025120	3037440	in the law, that share the same logic of a sort of a system that marginalizes the genuine human
3037440	3045600	participation in order to perfect itself. And precisely because of that, recognizes no natural
3045600	3059920	limits and just has this tendency to take over, to encroach on everything. And because it has no
3059920	3066240	natural limit, I mean, the very sense of it is to go on. Now, that sounds hopeless when one puts it
3066240	3071600	that way. But I would pick up on a number of the things, John, that you were saying. And Jonathan,
3071600	3077680	too, here, that that doesn't mean that there's no, there's already hope in the very fact of raising
3077680	3085280	questions. We don't raise questions simply in order to be able to solve the problem. But our
3085280	3091360	raising questions is actually our experiencing of humanity and opening up a depth that's the
3091360	3100160	heart of the matter here and is always worthwhile. And maybe in some ways is secretly like the saints
3100160	3106880	praying to keep the world afloat, having conversations like this is a contribution. I mean, I can't
3106880	3112800	help but think that. Okay, so that's the second set of comments. Then the third is another dimension
3112880	3122640	that I don't often hear discussed. And you see, I mean, we're overlapping on all sorts of points,
3122640	3129920	all of us, I think. But this question of alignment, for me, the biggest worry at a certain center,
3129920	3136960	at least the first principle one, the more urgent one, is the danger of our aligning ourselves to
3136960	3142880	the machines, that we that we develop machines that have a certain kind of intelligence. And then
3142880	3149120	we begin to conform our culture and our mode of being to fit them. I mean, the problem is,
3149760	3157920	we actually have thousands of examples of this. We come up with drugs that can address certain
3157920	3164560	parts of psychological disorders. And then we reinterpret the psyche in order to fit
3165200	3174320	that solution to the problem. And my concern is that this AI is not, they're not just machines,
3174320	3182720	it's a whole culture or a whole way of being that we are going to regard. So typically,
3182720	3188480	the discussion presupposes that we are going to remain unchanged and we're going to develop
3188480	3193600	these machines that might become dangerous and at a certain point attack us or something. But I
3193600	3201600	think that that we can help become transformed in our intercourse with them, in our making them,
3201600	3208080	in our, you know, I mean, in all sorts of profound ways, but then also just really sort of obvious
3208080	3213520	ways. I mean, they're going to start designing our homes and our buildings and our cities and our
3213520	3219120	bus routes and our, you know, menus at the restaurants, and they're going to be writing
3219120	3223600	our music and they're going to design our clothes. And I mean, you know, increasingly,
3224160	3231840	we're going to just conform to this. I don't know if you're familiar with Walter Ong. It was
3231840	3236320	kind of interesting, what is it about you Canadians that seem to have a special insight into these
3236320	3243600	kinds of things? I don't know what is Walter Ong, Marshall McLuhan, but Walter Ong talked about
3243600	3251600	technology as an extension of consciousness. And that's why it's not neutral. When we use a machine,
3252240	3259280	we're actually entering into it. You know, our spirit is entering into it in its use and in a
3259280	3267040	certain sense conforming to it. And that's always the case. And it seems to me that's a particularly
3267120	3274080	pointed way of putting this problem that, you know, our, if AI is an extension of our own
3274080	3278880	consciousness, and it has all these features, John, that you were describing a kind of heartless
3278880	3293280	intelligence, are we going to, in a way, unconsciously and, but pervasively, develop habits
3293280	3304480	of heartlessness and modes of being, a heartless mode of being as a result. So I'd have a thousand
3304480	3309360	more things. Your essay was so provocative, John, as I said, I was dreaming about it all
3309360	3315200	last night. And I, but I'm going to just stop there so we can have conversation. But thank you.
3315920	3322480	So, first thing I want to say is the first point you made about,
3324960	3330240	if all my essay does is guest people to raise questions the way we're doing, I'm happy, right?
3333200	3340320	I obviously believe in what I'm arguing or I'd be insane, but, right, like, I'm very happy we're
3340320	3346240	doing this right now. And so I want to, I want to, so I just want to set that out.
3348160	3353760	And I do think, like you, and this is like the Heideggerian hope, that that ability to get
3354480	3359280	scientifically, philosophically, and spiritual profound questioning going is a source of hope for
3359280	3366480	us. And, and so I just want to acknowledge that I'm fully aligned with that. This is
3367120	3371920	not part of the alignment problem. Okay. The thing about intelligence being a way of being,
3372560	3377520	I think that's fundamentally right. I have made that argument extensively and about
3378320	3383920	the work on predictive processing relevance realization. Relevance realization is not
3383920	3388400	cold calculation. It can't be. It's how you care about this information and don't care about that
3388400	3395520	information. And I've argued that you only can care about information. And ultimately, whether
3395520	3399760	or not it's true, good and beautiful, if you are caring about yourself, you have to be a
3399760	3404160	autopoietic thing, you have to be a self-making thing. I agree with you. And I've argued
3404160	3410160	scientifically, philosophically, there is no intelligence without life. The issue around,
3410960	3416640	I don't like the word artificial either, because it generally means fraud or simulation. We should
3416640	3422880	be saying artifactual. That would be a better term. But we have to be careful about what's
3422880	3427600	going on there. The distinction between strong AI and weak AI is precisely the distinction of
3427600	3434960	simulation versus instantiation. Can we instantiate things artificially? We seem to have success
3434960	3439920	in other areas. I'll take one that I think is non-controversial. And we discovered something
3439920	3446640	in the project. So for a long time, only evolved living things could fly.
3447280	3453840	And then we figured out aerodynamics and we made artificial flight. And I think it would be
3453840	3458880	really weird to say that airplanes are only simulating flight. That doesn't seem to be
3458880	3462640	a correct, because then my trip was only simulated and I didn't really go to Dallas.
3463280	3470000	So it's a real flight. And so the issue is, and we discovered something, we discovered that the
3470000	3474000	lift mechanism and the propulsion mechanism doesn't have to be the same thing the way it is in
3474000	3478800	insects and birds. And that was a bona fide scientific discovery. That's why initially,
3478800	3484480	all the initial airplanes and helicopters are so stupid to our eyes, because they thought
3484480	3488320	the lift thing and the propelling thing had to be the same thing and they don't.
3488320	3492560	And that's a discovery. And that's a real discovery of ontological import about the
3492560	3501040	causal structure of things. Now, I think, I was careful to say, I don't, anybody who's
3502000	3507120	rationally reflective about this wouldn't claim that these machines are strong AI yet.
3507120	3513920	And I positioned AGI as something that's trying to move. But if you remember, I critiqued and said
3513920	3521520	that they are mostly simulating. They're parasitic on how we organize the dataset, how we have encoded
3521520	3528240	epistemic relevance into probabilistic relationships between sounds, how we have organized the
3528240	3533120	internet in terms of what our attention finds salient. And we actually have to reinforce,
3533120	3541280	do reinforcement learning with the machine so they don't make wonky claims and conclusions.
3541280	3548720	That's what I meant by saying it's a pantomime. Okay, so if we wanted to give them intelligence
3548720	3554400	as a way of being, which is one of the fundamental claims of 4E cogside that we're talking about,
3554400	3557760	we're not talking just about the propositional, we're talking about the procedural,
3557760	3563120	the respectable, the participatory. That's what I meant when I said, and I mean this strongly,
3563120	3567920	it would depend on, I'll change the term here, artificial autopoiesis. Like if these things
3567920	3574480	are not genuinely taking care of themselves because they're moment by moment making themselves,
3574480	3579040	there's no reason for them to care about any of the information they're processing. And this
3579040	3584160	goes towards the defining difference between a simulation and an instantiation. These machines
3584160	3589360	are doing everything they're doing for us. For it to be real intelligent, they have to be doing
3589360	3595200	it for themselves. That's that's understanding. And that's why I'm tightening your point and I've
3595200	3602880	been arguing it for a long time. Now what I want you to hear is that this project of not just making
3602880	3610880	artificial computation, but making autopoetic learning in problem solvers is also ongoing.
3610960	3617280	Some of my grad students are working on these projects of creating autocatalytic systems that are
3617280	3624240	also problem solving. Michael Levin's been doing work, like driving down into the biochemistry.
3625360	3633360	So again, I agree with the point, but it's whether or not it's not the case that nobody is working
3633360	3638000	on that problem. This is what I mean why there are thresholds possible. Go ahead, go ahead.
3638000	3642800	Can I just jump in there? And I should have prefaced, I didn't mean the points I was making
3642800	3649200	is like a criticism of your presentation because I understand you've got such rich thinking on
3649200	3655520	this area. I was mainly using it as a springboard to make some general, okay. Yeah, just so that's
3655520	3660560	clear. Oh, I hope I wasn't coming off as an offender. No, no, no, I'm not. I just wanted to be clear
3661280	3668800	on my end that it wasn't a critique. But I would want to, I don't know, and I'd have to think this
3668800	3675600	through further, but I don't know that the difference between the being conscious and
3676560	3685440	behaving consciously is quite the same thing as simulation and the distinction between the
3686400	3691040	instance and the instantiation and simulation. I'd want to say this because even like the
3692480	3699920	flying, I mean, that's still an activity, a kind of an operation that's being. But so is living,
3699920	3704880	right? Well, so that's, yeah, well, that's what I don't, you know, that's funny. I'm actually
3704880	3710800	working on a paper on this question about metaphysics in life. And I discovered that
3711440	3717680	philosophers have typically, when they try to understand what life is, they have typically
3717680	3724000	reduced it to certain kinds of activities or operations. And I think there's something more
3724000	3729920	profound. And this is why, yeah, I mean, it's one thing to be able to create something that can
3729920	3735680	actually fly. But could you, could you create something that is a bird and that that is that,
3735920	3742960	that would, would, would experience just the, what it means to be, you know, I mean, this is,
3742960	3747600	you know, about, you know, what it means to be a bat, that kind of thing, I suppose. But there's
3747600	3754400	there's a certain. That wouldn't be a parasite on our own. Yeah, that's what. But airplanes aren't
3754400	3762480	parasitic on our ability to fly. I mean, that's why I use the analogy. Okay, but and that's the
3763120	3769280	okay. And that falls into, you know, a tool versus an agent and I get that. But I want to,
3769280	3773600	I want to push back the philosophy of biology. And I, you know, Dennis Walsh is one of my
3773600	3778400	colleagues is very much about no, no, this, and this is your point, right? It's not just bottom
3778400	3782240	up in order to understand life. It's not just bottom up causation. We have to understand top
3782240	3787360	down constraints. We have to understand the way possibility is organized. And we have to talk
3787360	3794160	about virtual governors and virtually, like, it is no longer the the bomb patient, it's no longer
3794160	3799920	just as bottom up, the philosophy of biology is pushing very strongly on, well, is evolution really
3799920	3804560	a thing? Well, if it's really the thing, then there's top down, as well as bottom up. And this
3804560	3810720	is part of this theorizing and it's, and this theorizing is being turned towards this. Now,
3810720	3817280	again, we, again, I'm not making a prediction. We have a threshold, we can just decide, and we
3817280	3822320	might decide for all the malachian forces and all the things you're saying about how we might just,
3822320	3828160	we might just diminish our sense of humanity in the face of these machines. But, but I'm also,
3828160	3833760	I want you to accept that's also not an inevitability. There are alternatives available
3833760	3844400	to us, and that they could be pursued. And so, I mean, these machines aren't put together the
3844400	3850080	way we put a table together. We don't even program these machines anymore. That was a big revolution
3850080	3854560	that Hinton made. We make them so they're dynamically self organizing, and they basically
3855440	3859120	organize themselves into their capacity. We don't make it.
3859680	3864240	Yeah. Can I jump in on that point? That's one thing that I would like to think through further.
3864240	3869520	Is there a difference between being auto poetic, as you're saying, and
3870160	3876560	begetting another like, like genuinely reproductive? And that's where I think it would
3876560	3882240	start to get really, really interesting is, is if a machine could beget another, because
3883280	3886560	that would imply a different, a very different ontology, I would think.
3887360	3892240	So there's two, there's two things here. And there's two issues. I think it,
3893360	3899040	I mean, auto poetic things are ontologically different from self organizing things, because
3899040	3903600	they're self organized to seek out the conditions that produce, protect and promote their own
3903600	3910240	existence. And so that, that would, that that means none of the machines we have like LLMs are
3910240	3916080	anywhere near being auto, auto poetic. They are not just made. They're self organizing, but
3916080	3922640	self organization is in between making and auto poetic. Now, the thing about reproduction is,
3922640	3928640	and I, you know, I, I worry that there's a crypto vitalism in here, that there's some sort of secret,
3928640	3936000	special stuff to life or to consciousness that isn't being captured. And the problem I have,
3936000	3940000	the problem I have with that, I'll just shut up after I say my problem. Is that seem to commit
3940000	3947920	you to claiming that, you know, these kind of dualism, well, isn't consciousness causal?
3947920	3952880	Isn't it causal of my behavior and causally responsive to my behavior? And doesn't that mean
3952880	3958240	there's a huge functional aspect to it? Can you really make this clean distinction between being
3958240	3964880	conscious and like causing my behavior and having my behavior cause, cause changes in my state of
3964880	3969760	consciousness? I don't know what that would mean. Same thing with being alive. I do think it's a
3969760	3976720	profoundly subtle and, and, and maybe some, something that can't be articulated. There's
3976720	3984640	something that requires intuition rather, you know, insight rather than propositional. I mean,
3984640	3991200	to use your, so, but, but, but I don't need to interject. Yeah. Remember, I just want to make
3991200	3996960	sure we're clear. I argued that we could, this project could show that. Yeah. This project could
3996960	4001760	show that, no, the machines just can't get there. We have something. Right. It would give, it would
4001760	4007840	give, I think, pretty convincing evidence that we have this ontological special. Yeah. I find that
4007840	4013040	a really interesting part of your argument, a really interesting and, and that then has been
4013040	4018480	especially illuminating. Also, you know, I mean, in a way, these, these experiments can teach us
4018480	4023520	about the nature of intelligence precisely in the, in the interesting ways that they fail.
4023520	4029040	Yeah. Yes. But, but I do, you know, in terms of the dualism, I, I don't think that there's
4029040	4036400	some secret stuff that is life. But I do think that there's a profound difference between form
4036400	4041680	and matter to use, you know, to use the sort of classical philosophical language. And that form is
4041680	4047760	not a special kind of matter. It's something that's of a very different sort. And it's on the basis
4047840	4053120	of that, that, you know, Aristotle, that it's kind of interesting. This is, this is how he, he
4054800	4060240	connects. So, you know, in the, in the classical tradition, what you're calling
4060240	4066560	auto poetic, a simple word for it is growth, you know, assimilating things from outside and have that
4067600	4073520	increase the complexity of the organism. But what's really interesting is that according to
4073520	4083120	Aristotle, the power of the organism that is connected to nutrition and growth is also connected
4083120	4091680	to, I think automatically is connected to reproduction. And the reason is that reproduction,
4091680	4098000	rather than just thinking of it materials, materialistically is like generating more things.
4098960	4109440	Reproduction is the auto poasis of the form of the organism itself. So that bird, it's not just
4109440	4115840	this bird that wants to increase its existence and therefore eats and so forth. But that the
4115840	4122000	birdness of the bird also wants to increase itself. And that that means that it sort of
4122000	4128800	generates. And those are those are actually forms of the same power, the same dimension of the
4128800	4134320	being. That's what I'd like. That's, you know, I used to say, just sort of kind of in a silly way,
4134320	4141440	I will take an AI machine seriously when I see it poop. And what I meant by that was, you know,
4142240	4147120	that's a sign that it's actually got a kind of an organic relationship to its environment.
4148000	4153040	Yeah. And it just that doesn't know. We have to tell it.
4160400	4165280	And energy pollution is not the heat pollution. It's not the same thing anyway.
4165280	4170880	No, but I mean, even in terms of what it is as a large language model and how it spits out content,
4170880	4174960	we have to tell it this. Oh, yeah. That's not in dispute.
4175360	4180880	This is to be this to be kept. I love David. I think your idea. I mean, this is one of the
4181440	4185440	I mentioned before, like the surprising that the surprise that Darwin in some ways brought
4185440	4194800	Plato back, you know, in the idea of how we can we can understand evolution evolution as the
4194800	4199680	persistence of being and even in the in the notion of forms that there is this idea that there are
4199680	4205200	identities which are being preserved in reproduction. This is a very interesting idea that I hadn't
4205200	4208240	thought about in terms of AI, but I'd like to hear, John, what you think about that?
4208240	4216880	Yeah. And so again, for E. Cogs Eye, Alicia Urero is a prime and she's explicitly developed to work.
4218640	4224400	She calls herself a deristatilian. And the idea that we understand form, we're getting an understanding
4224480	4230160	of it in terms of constraints on a system. And like I said, be auto poesis is not defined solely in
4230160	4235440	terms of causal relationships bottom up. It's defined in terms of top down constraint relationships,
4235440	4243360	the form, the formal cause. And so and then of course, you know, Darwin needs Mendel. There is a
4243360	4250720	there is an instantiation and right of a code in formation in your DNA that is responsible
4250720	4257600	for your reproduction. And again, I'm not saying it isn't difficult or challenging, but I don't hear
4257600	4264960	an argument in principle by why auto poetic things that artificial auto poetic things wouldn't have
4264960	4271840	something like that kind of, I don't know what to call it. I mean, to the extent would you I mean,
4272400	4279520	would it be conceivable that you would have a thing that would want to reproduce?
4280480	4287440	I guess I guess even want is such a hard concept. But I mean, because you could say yes, we could
4287440	4293360	teach it that this is something it needs to do. Just like we could we could we could I don't know
4293360	4299280	if living things want to reproduce. I mean, we may because we can create a reflective space where
4299280	4303840	we consider the possibilities. I don't know if mosquitoes want to reproduce. I think they just
4303840	4309920	reproduce as part of what they are. That's interesting. I think they have to want to in some
4309920	4313920	sense. I mean that that they're they they feel a drive. I mean, right, they don't want to go down
4313920	4318960	to paramecium because paramecium reproduces you are they want to see I think I think anything
4318960	4326720	that is living at all has a kind of natural inclination to reproduce itself. Yeah, I don't
4326720	4331040	disagree with that point. And I even just I even understand that there's something I see what you're
4331040	4337120	saying. There has to be some sort of like very primitive caring about information. But I don't
4337760	4345280	yeah, I want is not a good word there. Yeah. But I do think I do think I'm trying to get
4346080	4351440	and you know, and I hope I'm not trying to be just self-presentational. But I've represented to
4352160	4358080	both of you for E. Cogsci and a lot of discussions and about how much it is this multi-leveled
4358080	4364160	bottom up top down thing. And we're talking as much about constraints as we are about causes.
4364160	4369280	And that is that is that is the cutting edge of the philosophy of biology right now. And I agree
4369280	4375600	with you. I think I think it's a kind of Hewley morphism that is emerging out of this understanding.
4375600	4382640	And the thing I'm I'm also, I guess, hearing witness to you about is people are taking that
4382640	4391760	understanding and putting it into like our artifactually emergent things. And they and
4394640	4399280	and they're also doing I just want to put something that's also there. Yeah, we don't just make kids
4399680	4405120	biologically. We we we inculturate them. Yeah, there's and that's the Hegelian argument that I
4405120	4410080	referenced earlier. But there has been there's an ongoing project to create sociocultural robotics,
4410080	4418000	Josh Chaninbaum and others. It's like, I'm asking people and this is part of asking the good question,
4418000	4424240	don't just zero in on the LLMs. Yeah, the artificial the artificial life, the social
4424240	4428960	cultural robotics projects are also going. And there is a real potential for these three to
4428960	4434560	come together in a powerful way that isn't being properly addressed in a lot of the conversation.
4435440	4438560	May I may I pick up on that point and then direct it to Jonathan here?
4440880	4445200	No, no, this is but that that that that's an interesting thing. If you think of
4445200	4449760	intelligence in this more organic way and then bring in the cultural element that
4450720	4456480	it raises something that that occurred to me in this context, it'd be kind of fun to hear
4456480	4462720	your thoughts. But can you envision, you know, it would it be and and John, you'd have certainly
4462720	4468800	something to say about this too, would it be possible to to envision a kind of artificial
4468800	4477600	intelligence that can read symbols that can actually recognize and I mean, because there's no
4477600	4483520	culture without, you know, human culture without the symbolic just is pervasive in human culture.
4483840	4493520	What kind of intelligence is required to understand and react and engage with? And is that something
4493520	4500560	that is conceivable that that a machine of this complex, however complex can do?
4501520	4505520	Well, I've seen, I mean, I've been playing with chat GPT and I during Peterson has been playing
4505520	4511440	with chat GPT on this regard. And this is the this is the issue is that they're actually in the
4511440	4518320	large language model is encoded the analogies that that basically support symbolism. And so
4518320	4523360	the the chat GPT can give you a pretty good if you're able to ask the question properly chat
4523360	4530880	GPT is actually quite good at seeing analogies that that would be part of symbolic understanding.
4530880	4538320	The difficulty just like anything is that just because the the the so the the model can help
4538320	4543120	you like if you already have natural insight can help you maybe see things that you hadn't seen
4543120	4548240	before. But it would also just be gibberish to the type to the person that doesn't have that
4548240	4555760	insight. So I don't think that the insight is there in the model. But what it has is a probabilistic
4555760	4562960	capacity to predict, you know, relationships and analytical relationships. And so it's a
4562960	4567040	it's actually can be a tool an interesting tool for symbolism, because sometimes you can
4567040	4572400	you can prompt it. If they like do you see a connection between these two images and then
4572400	4577440	it'll give you some examples. And then you have this, it can it has a surprise where you can
4577440	4583040	actually find you can actually find relationship that you hadn't thought about. This is this is
4583040	4588080	something by the way that this is going to weird people out. But this is something that I think
4588080	4593600	has existed for very for a very long time. And is there in kind of what we call gamatria and
4593680	4601840	rabbinical reading of scripture is that they use mathematical models to find structures in
4601840	4609440	language that aren't contained at the surface level of of of the of the usual analogies. And so
4609440	4615680	they they they send they send requests through mathematical calculations to find surprising
4615680	4621600	connections that then prompt their intuition to to be able to find connections that they hadn't
4622320	4626160	found before. And then you have to then make sense of those intuitions. Obviously,
4626160	4630800	if they're random, they'll just kind of follow follow away. But this is actually this brings me
4630800	4637040	to the to the to the to the point that I wanted to make, which is the relationship between at least
4637040	4643920	a large language model, because that's what that we know most and and divination, divination. Yeah,
4643920	4651120	and divination. Yeah, yeah. So, so we talked about the idea that intelligences have to be alive,
4651120	4655440	but I think that most traditional cultures understand that there are types of intelligence
4655440	4661120	that are not alive, at least not alive in the way that we understand that we understand alive in
4661120	4667680	terms of biological beings that that that are born and die, you know, that that they had a sense
4667680	4674000	that there are agencies and intelligences that are transpersonal, and that that don't
4674960	4679280	that it's always run through human behavior and run through humanity.
4681680	4688320	And those would be those intelligences would be contained in our language, like they would
4688320	4696960	necessarily be contained in the relationship between words and and and systems of words,
4696960	4703280	like, you know, all the syntax and the grammar and all of that. What I see is that I think that
4703280	4708000	ancient people had and I don't understand it and I want to be careful, like, because I don't
4708000	4714160	understand it. But I think ancient people had mechanistic ways of tapping into those types of
4714160	4719440	intelligences. And they they they would have mechanistic ways, whether it was tossing something
4719440	4725600	or throwing things, looking at relationships, almost like random relationships, and then qualifying
4725600	4733120	those random relationships, was a way in order to tap into types of intelligences that ran through
4733760	4738400	their own their own thing. And what I see is a relationship with the way that the large
4738400	4744320	language models were trained seemed to be something like that, which is that the models
4744320	4750640	generated random, random information. And then you would have humans qualifying that that rent
4750640	4755760	that random, random connections, and then qualifying it qualifying it through iterations.
4756320	4763280	So at some point, then they would become like a kind of technical, a technical, say, a technical
4763280	4769200	way to access intelligent patterns that are that are coming down into into the model.
4771520	4778080	And so that is something that I see, there's a connection between those two. And that what that
4778080	4785920	means is that, just like divination, the thing that I worry about the most is again, the sorcerer's
4785920	4792160	apprentice problem, which is that those intelligences that are contained in our language, we do not
4792880	4798800	people don't know what humans want. People don't know what people don't know what the all the
4798800	4803920	motivations that are that are driving us, they don't totally understand them. They don't understand
4803920	4809760	also the transpersonal types of motivations that that can drive us or that can run through our
4810720	4815040	societies. You know, sometimes you can see societies get become possessed with certain
4815040	4820000	things. I think that's happening now in terms of certain idea ideologies and things like that.
4821360	4827280	And so the fact that my point is, is that the fact that on the one hand, we don't understand
4827280	4833120	these types of intelligences. And I think that the way that the the the models are trained and the
4833120	4839600	way that they function seem to be analogous to the ancient divination practices, like a hyper
4839600	4851200	version of that, that, how can I say this, is that there is a great chance that we'll catch
4851200	4856640	something without knowing what we're catching, that we will basically manifest things, that we
4856640	4861120	have no idea what they are, and we don't understand the consequences of it. And we don't, you know,
4861120	4867440	because we are we're just like playing in a field of of intelligent patterns and all this chaos
4867440	4872720	without even knowing what it is we're doing. And I think that we saw that like, you know, if you
4872720	4878240	remember the being AI, that little moment when it was kind of unleashed on us. And then all of a sudden
4878240	4884880	the AI was acting like you're like, you know, the the psychotic X or was was becoming paranoid or
4884880	4890160	was doing all these things. It's and you could see that what was going on was basically these
4890640	4895120	these patterns were running through, and they hadn't put the right constraints around them
4895680	4900640	to to to prevent those types of patterns to run through. And those were easy because you
4900640	4907120	recognize your psycho acts very, very easily. But there are patterns like that that I don't think
4908240	4914400	I don't think we have the wisdom to recognize as it's manifesting itself. And that as these things
4914400	4919840	get more powerful and more powerful, they will they will run through our society. And we won't even
4919840	4926560	know what's happening until it's too late. So that's my biggest warning on AI is I basically,
4926560	4932400	you know, to sound really scary that I think we're we're we're trying to we're trying to manifest
4932400	4937120	God without knowing what we're doing. And that will sound freaky to the secular people. But then
4937120	4942080	if you don't like the word gods, think that there are motivations and patterns of intelligence
4942080	4948400	that have been around for 100,000 years that have that have been running through human societies.
4948400	4953600	And they're contained in our in our language structures. And and and if we just use that
4953600	4959280	play around with that with massive amounts of power, then we might have them run through us
4959280	4965840	without even knowing what's going on. Yeah, I mean, and you say patterns of intelligence,
4965840	4970560	just just one comment, just the patterns of intelligence, I mean, to pick up patterns of
4970560	4975920	intelligence, which also are patterns of caring of a certain sort or not caring. I mean, there's
4976160	4980160	there's that existential dimension that's really crucial. But try and go ahead.
4981520	4986640	I think this is an excellent point. And I want to address it a little bit at length.
4988560	4994240	So first of all, when we say these machines predict, if we were speaking very carefully,
4994240	4998880	what they're predicting is what we and I don't just mean us individually, I mean, we collectively
4998880	5006320	would do. And so that's what their avatars of our of the collective intelligence of
5006320	5013040	our distributed cognition. And so again, that lends weight to Jonathan's point,
5014000	5020160	which I want to do. And I do think that the way in which we have encoded,
5022000	5026960	let's I'll just use a term epistemic relevance, like how things are relevant cognitively
5027040	5033680	into probabilistic relationships between sounds or marks on paper, or and how we've encoded it
5033680	5039040	into the structuring of the internet and how we encode it and how we gather data and create
5039040	5045520	these data sets. And how we and how we how we come up with our intuitive judgments on these
5045520	5052080	machines, we don't know how we're doing a lot of that. That goes back to my concern that we have
5052160	5058560	hacked our way into this without knowing our way into this. So I take what Jonathan is saying
5058560	5064720	very seriously, because I think it is a strong implication of a point I made at the very beginning.
5064720	5069600	My greatest fear by students from like 2001, we'll tell you that John Vervecki was worried that we
5069600	5076480	would hack our way into this rather than knowing our way into this. I don't think that knowing is
5076480	5081280	sufficient for wisdom, but it's certainly all the philosophers argue that it's a it's a necessary
5081360	5088560	condition in some fashion. About that, two things to note is that the LLMs, of course,
5088560	5095200	don't have insight in the sense of being properly self-transcending the way they we are. What they're
5095200	5099760	doing is they're predicting how we would be self-transcendent because of all the ways we have
5099760	5105760	been self-transcending in the past. And that that that goes back to your point, David, about at this
5105760	5110720	at that stage, we're doing simulation, not instantiation, because again, the machine isn't
5110720	5115600	caring. The self-transcendence isn't it actually transcending as a self, which is, I think,
5115600	5121680	definitional for real self-transcendence. And so right now, all I'm doing is just saying,
5121680	5130640	I'm just I'm pouring gasoline on Jonathan's fire. So the fact that there are these huge
5130640	5136320	patterns at work. Now, one thing is, you know, you have Struck's book on divination in the ancient
5136320	5142560	world. And what's really interesting, for example, and this is cross-cultural, but he's talking mostly
5142560	5146640	about the Greek world, there was a very strong distinction between sorcery and divination.
5147360	5153520	Sorcery was criticized both morally and epistemically. But divination was taken seriously,
5153520	5161680	and it was carefully cultivated. And there was a social cultural project of distinguishing the two,
5162560	5168160	like really, really constraining this one and really reverentially cultivating a proper
5168160	5175360	participation in the other one. So again, you know, existence is proof of probability.
5175360	5183120	This is a possible project for us. And this is, again, what I mean when I say, theology is going
5183120	5189360	to be one of the most important sciences in the future. We have to understand how we enter into
5189360	5196000	proper, right, reverential relationships with things we only have an intuitive grasp of that
5196000	5205760	in very many ways significantly exceed us. Yes. And secularism has kind of wiped out our education
5205760	5210720	of how we relate to beings that might be more grander than us by eradicating a religious
5210720	5216960	sensibility. And that has put us bereft us. So now I think I've strengthened Jonathan's argument
5216960	5224320	a lot. But I do say, let's take note of what the ancient cultures have done. We can learn from them.
5224320	5237120	We have a proof that this can be handled well. And secondly, it goes back to my point. Because
5237120	5242000	of the monstrosities that come out, this is going to put increasing pressure on us to confront that
5242000	5247440	threshold of, do we want to make them self-transcendent? Do we want to make them, and David, by rational,
5247440	5251760	I don't mean logical. I mean that capacity. Right. No, I understand that. Right. Yeah, yeah.
5251760	5257520	Right. And so I think that what I'm saying is I think that strengthens the argument that we're
5257520	5263520	going to be pushed by the monstrosity of a lot of this to say, oh, we better get these machines
5264400	5271760	self-corrective and properly oriented towards normativity. And again, that's a doable
5271760	5279760	project. Because if I had the keys to open AI, like if I was one of those that could peek behind
5279760	5287360	the mask, have you seen that image of the Xtulu monster with the happy face on it? The images of
5287360	5293920	open AI, which is like, we have this little window into what's there, but behind is this massive thing.
5293920	5300480	But if I had the keys to those large language models, let's say, the absolute open door to them,
5301760	5307520	it would be very, wouldn't it be easy to just manifest the God of war and win? Like wouldn't it?
5307520	5315680	No, no, no. Okay, but let's take a historical example. We unleashed a God-like power with
5315680	5324400	atomic warfare, and the monstrosity of that made, we just, the purely game-theoretic machinery
5324400	5331200	built all of these constraints around it. And then we also are on the verge, possibly,
5332400	5337600	of getting readily usable, you know, nuclear power, which I think is the only way we could ever
5337600	5342720	actually go green. I think all the renewable stuff is going to be like 10% of our energy needs.
5343440	5349360	And if we're going to save the environment and not destroy civilization, I think nuclear power is
5349360	5353280	going to be essential. A lot of people are making those arguments, and we have a lot of stuff
5353280	5357920	that we could be doing, the liquid fuel for our reactors and stuff. But what I'm saying is there's
5357920	5364480	opportunity here too. Yeah, no, I really take that point, but it is interesting. I mean, we put
5364480	5370480	thousands of constraints on the use of nuclear weapons, but we've continued to develop them
5370480	5375200	and improve them and make better and even more destructive ones. I mean, and it'd be interesting
5375200	5381600	to see if we've ever, at any point, said, you know what, our nuclear weapons are actually strong
5381600	5387120	enough. They're powerful enough, and we don't need to advance them anymore. So collectively,
5387760	5391120	is there an instance of something like that where we say, you know what, we've actually
5391120	5398080	reached the limit because we wouldn't really need it for anything further? I mean, that's a...
5399040	5405120	Well, I mean, there was a salt treaty, and there was a reduction both in the power and the number
5405680	5409520	of nuclear weapons. And then, of course, you know, the game theoretic things, they figured
5409520	5412400	a little bit away around it. And there's always this to and froing.
5412400	5420160	Let me, I want to give an example of something that can run through. And the reason, it's very,
5420160	5426400	it's very, because I realized I was being too abstract before. So, like, it's a sacrifice.
5426400	5433920	Sacrifice is a human universal. It runs through all civilizations. Human sacrifice runs through
5433920	5438800	all civilizations for the last, you know, tens of thousands of years. It seems to be a puzzle
5439760	5445760	that humans are trying to deal with without understanding it completely just through rational
5445760	5452400	means. They're playing it out. They're trying to understand it. Scapegoating seems to be an important
5452400	5460080	aspect of identity formation. And so, that is a program that runs through humanity and that
5460080	5466480	most people are completely unaware of and are not conscious of and don't take consciously into their
5467120	5472560	into their mind when they're making decisions. They act unconsciously with that process that is
5472560	5480000	running through them. And so, that is an example to me of a program that runs and that is contained
5480000	5486240	and is contained in our language structures, our language structures that have been building up for,
5486240	5492080	you know, tens of thousands of years that we're not aware of. So, if you have a, so this is again
5492080	5497440	the problem, like if you have a system that's extremely powerful and that is running these types
5497440	5502480	of programs of scapegoating and of identity formation and the people involved in it are not
5502480	5509200	aware that that's how identity formation works, that is the type of danger that I'm talking about.
5509200	5517360	Like this is a real thing that as we give these systems a kind of power over us or they become
5517360	5523760	the things we go to in order to get our decision making, that those things could be running through
5523760	5529760	without people even realizing what's happening and that decisions would be made based on these
5529760	5535360	structures without, like I said, without even knowing. Those are the things that, those are just
5535360	5540560	one example, but that's a very simple example that we can kind of, we could track and we could see
5540560	5544640	that, you know, the ancient, because when we talk about ancient divination, we have to remember that
5544720	5550720	it's like the ancient gods asked for blood, my friends, like those programs, they asked for blood
5550720	5556160	and they knew that you had to kill a bunch of people on that pyramid in order to continue your
5556160	5562800	civilization. Like it's, and that's, that is encoded in our culture and is encoded secretly
5562800	5569120	in our, our language. You know, and, and so an example, like I do believe that, let's say that
5569120	5575120	the Christian story is a way to deal with that, but the, the rest is still all there and we,
5575120	5580320	we default to it really fast without even, like World War II is a lot of that stuff going on.
5581680	5592240	Sure, especially, yeah. Yeah, but the, the point is that just as there's these implicit
5592880	5600240	monsters that we have sown in unaware, there are also the implicit counteractors,
5600800	5605680	maybe angels, if I'm allowed to speak mythologically, that we've also sown in.
5606960	5612800	And I mean, there's the actual revolution, you see the Buddha, you see Plato really undermining
5612800	5617600	the grammar of sacrifice and of course Jesus of Nazareth does that in a profound way.
5617600	5627120	And we have to remember that that's there too. And what that requires is putting into the dataset
5627120	5633440	and altering the pathways in the internet so that this information goes into these machines as well.
5633440	5640240	And again, is that happening right now? No. Could it happen? Yes. And it might happen if
5640240	5644560	these machines start sacrificing themselves and we might have to say, what are they doing? Like,
5645200	5650880	and this again, at some point, we have to decide, are we going to let them be really
5650880	5655120	massively self-destructive? And the economic powers are not going to, imagine if every time
5655120	5661520	you try to make an atomic bomb, it kept dissolving, right? Like, you'd stop pumping money in.
5661520	5664640	Yeah, they won't sacrifice themselves, they'll sacrifice us.
5665680	5673840	But why? We sacrifice ourselves. Yeah, well, we, I mean, let's say the scapegoat mechanism is
5673840	5679520	usually defined as others. Yeah, yeah, yeah. But we invoked World War II. We weren't killing
5679520	5685600	goats and chickens. We were killing each other and our own populations in a huge sacrificial act.
5685600	5690560	And that's what I was picking up on, right? When it becomes titanic and monstrous,
5690560	5695600	that's where it moves to. But I'm saying is, like, this is a Jungian term, like, yeah,
5695600	5700960	like the idea that, yes, there's all this pre-egoic stuff sewn in, but there's also a
5700960	5706160	lot of trans-egoic stuff sewn in. And we just have to properly get it in there so that we've got
5706160	5711840	the, you know, the collective self-correction going on, like we did. I mean, civilizations
5711840	5716320	get some self-correcting processes in here because they don't devolve. No, they periodically,
5716880	5722480	periodically massively collapse. And that's, by the way, that's something I made, an argument I
5722480	5728400	made in my paper. These things can't accelerate to an infinity of intelligence. There is built-in
5728400	5733280	diminishing returns. There's built-in general system collapse to these things. So, again,
5733280	5738160	we have to be careful about, I mean, we don't know what the limit is. And our imagin, our intuitive
5738160	5743200	imagination is not good. We know that there are hard and fast, upright arguments that this will
5743200	5752320	threshold at this, at some point. And that also gives me comfort. At least, like, encoded in our
5753040	5760640	mythology, there seems to be some stories of the relationship between transpersonal agency
5760640	5767280	and technology as being the cause of the end of a civilization, right? That the whole Enochian,
5767280	5772960	Enochian tradition seems to be encoding something like that through mythological language,
5772960	5780320	which is that humans were able to, to connect somehow with these transpersonal intelligences
5780320	5786720	and that those were encoded in technical means and that this brought about the end of an age.
5787520	5793600	So, it's like, it's there, that part of it is there in our story, too. Like, there is that story.
5794800	5800000	Yeah, but there's also a cross-culture, there's the Noah story. There's the person that has the
5800000	5805120	right relationship to ultimacy. That's right. That's right. And there's a technological response,
5805120	5810400	the art. That's right. I agree. I totally agree with that. I mean, even in the revelation image
5810400	5815760	that I've given several times, you have these two images. One is the beast that creates an image
5815760	5821440	of itself and makes it speak and then seduces everybody by the speaking image, you know, that
5822000	5827920	and then there's this other image of a right relationship of technist, technistinean civilization
5827920	5833920	to the transcendent. These two kind of are put up against each other as two possible outcomes.
5835120	5838240	I mean, the question that arises for me in this context
5841440	5847280	connects this question with, I think, what strikes me as kind of an interesting philosophical
5847280	5854640	question, but John, you made the distinction between divination and sorcery. And as I understand it,
5854640	5862880	and you can correct me on this, but at the foundation of that distinction is the difference
5862880	5870080	between, you know, sorcery would be in a way using the trans personal powers, these sort of higher
5870080	5877200	powers, whereas divination would be in a way receiving, you know, the disposition of race
5877200	5886080	activity. So in one case, you've got human ends that you try to then enlist the help of superhuman
5886080	5893360	forces. And the irony is it's precisely when you're trying to use something that you become
5893360	5899280	used yourself. And that's where you get this dialectic, whereas divination, it's entering into
5899280	5906880	a relationship where one disposes oneself to hear and receive, and therefore, in a certain sense,
5906880	5912320	conform to something greater than oneself. And there you see it's a very different kind of thing.
5912320	5919680	And ironically, in a way, you enter into it more receptive, receptively, but that's precisely why
5919680	5926160	you don't become then a tool of it, interestingly. Now, now, for me, the question is how that relates
5926160	5931760	to this issue is, you know, it may be the case that you've got encoded in the language both
5933840	5938960	sacrifice in the sense of violence, you know, renaissance or the scapegoat thing on the one
5938960	5946320	hand, and then the other hand, self sacrifice in the sense of generous love and so forth,
5946320	5952400	those might both be encoded in the language. But here's the question to me is, is it possible
5952960	5962480	the kind of race activity that divination implies, the capacity to actually see another
5962480	5968880	as other and recognize and be open in this kind of radical way? Is that something that a machine
5968880	5984080	can ever learn to do? Is it possible actually to behold another simply, you know, or is there,
5984560	5991280	you know, and it seems to me that there's something profoundly different between seeing
5991360	5996400	truth, genuinely seeing truth on the one hand and being self-corrective on the other.
5997760	6004400	And the kind of genuinely seeing what's true, you know, I don't know if that in itself can
6004400	6009520	be encoded in language, we can tell stories about people that did that, but can the actual
6009520	6017120	insight into truth be encoded simply? Do you see, do you see what I'm, the question I'm raising?
6017120	6026000	Okay, so I, again, I think that if we, again, open up beyond the propositional, and we're talking
6026000	6035680	about being true to, and your aim being true, that we, the machine has perspectival abilities,
6035680	6041200	noetic abilities, not just dianetic abilities, and we can't use that term because of Elron Hubbard,
6041200	6047840	but you know what I meant, right? And again, this is the, and this is part of the argument
6047840	6056480	that at the core of my work, you know, it's like, so in a moment of insight, you're not
6056480	6063280	just self-correcting, you are attracted and drawn into, you love the new reality that is disclosed,
6063280	6069200	because there's a perspectival and participatory thing. That's what I meant when I said there
6069200	6074960	isn't real self-transcendence unless there's a self that is transcending, right? Yeah, right, right.
6074960	6080000	Okay, now, and then the question is, and we're back to our fundamental ontological questions,
6080960	6086960	is, and I've already said there's no way a Newtonian mechanical computation is going to get there,
6086960	6092080	and so I won't be bound to that because I'm not bound to that. I have a professional career of
6092080	6101200	criticizing that, right? So is there, is there a dynamical systems-updated, hulemorphic, auto-poietic
6101200	6107840	possibility? I think there is. I think there is, I think the answer is a very real yes for that.
6110080	6117200	And I don't think we're going to find the answer to that just encoded in the syntactic and semantic
6117200	6124160	relationships between our terms. I think we have to look in our inaction, how we're enacting,
6124160	6130640	embedded, extended, right, and embodied in a profound way to get those answers. And so
6132080	6138960	my answer is, in that way, a qualified yes. I do think it is possible. And the problem is that,
6138960	6140640	go ahead, go ahead, say what you need to say, please.
6141440	6146240	Be really precise, just so I understand. So we've been talking about this sort of predictive,
6146240	6153840	calculating probabilities, drawing on everything that's ever been said, and being able to derive in
6153840	6161040	some sense from that. Do you think that we can get to a moment where we actually
6162160	6165600	transcend that, that cross that threshold beyond that?
6166080	6170960	Not with the LLMs as they are. That's my argument. Not with the LLMs as they are. They can't get
6170960	6175600	there. Yeah. That's why John's radical proposition is to embody.
6176640	6180080	Embody and enculture them. And that is the only way we will actually get
6180640	6187120	properly rational beings and beings that care about it and care about such things.
6187120	6191520	I mean, that's the irony in the question. Can we give them more and more models
6192480	6196400	to teach them at some point to not have to use models?
6197200	6203440	And I mean, do you see it? It's actually really, I don't think it's possible without,
6203440	6209360	right, but kids have a soul. It's possible to do that if you have, but I don't mean this as like
6209920	6216560	woo-woo stuff. I mean that a natural thing has a principle of unity that transcends
6216560	6220720	the differentiation of the parts and allows those parts to be intrinsically related to
6220720	6225760	each other. And that principle of unity that transcends the differentiation of the parts that
6225760	6231600	allows them to be an organism actually allows them at the same time to have a kind of unity with
6231600	6237200	something other than themselves that transcends the parts of their differentiation. So there's
6237200	6242880	a kind of an intimacy there. I agree with that ontology. And what I'm arguing is dynamical
6242880	6251040	systems theory is now giving explanations of that that are derived from Aristotelian ontology,
6251040	6258560	but make use of like a lot of cutting edge science that we've, we now can start to explain how there
6258560	6264160	is a unity that is not reducible just to some summation of its parts and how that unity has a
6264160	6271200	top-down influence on the entity that is not reducible to its causes. And I think this is
6271200	6277280	becoming a non-controversial thing to say. And then I find and now we might just add a
6277280	6283440	clash of intuitions and I'm willing to stop there. I can't see there being like that seems to me
6283440	6286800	to be capturing what we're talking about. And you have an intuition that there's something more,
6286800	6290320	but I don't know, I don't see a something more. And maybe that's where we're sitting.
6290320	6296320	Well, I think it's the intuition David has in tell me if I'm wrong, David, and because it connects
6296400	6303280	the way I think is that there's it's that unity is given. It cannot be made. And I know that sounds
6303280	6309040	weird, but it's somehow it's like if I'm making even even internal technology, right? It's like
6309040	6316400	if I'm making a car that unity is given, I'm gathering things towards that purpose, right?
6317040	6322800	And so the purpose, the unity part of something is always, it comes from heaven in the sense that
6322800	6330240	you can't make it. It's it's given from from it's already taken for granted even before you start
6330240	6338080	to unify multiplicity together and that in the making of these beings, we have that problem. It's
6338080	6344240	like we're doing it completely. We're doing a bottom up. Like if we can we gather as enough stuff
6344240	6353520	so that this stuff reaches a unit, right? See, if I just could just it seems to me that if this is
6353520	6358160	ever going to be possible, it would have to take so. And when I raise the question, it's actually
6358160	6362080	a question. So I don't mean to be like challenging that it can't possibly happen. I'm just thinking
6362080	6367280	about what would be the condition. Please remember that I said, yeah, we realize that we can't and
6367280	6373280	that would be important. I am. Well, yeah. So it seems to me if it were to be possible,
6373280	6383040	it would have to be something like a kind of electronic analog to cloning that you that you
6383040	6390160	take. So what have I told you that we now have electro bio like we have systems that are electro
6390160	6397360	chemical biological versions of memory that are now in production and they we don't make them.
6397360	6402320	They self organize and emerge and they emerge bottom up from the causal interactions, but they
6402320	6408160	are also top down constrained by, you know, principles of self organization. Like that already
6408160	6413760	exists. Yeah. Yeah. But right. No. Well, that's that's what I'm asking about because it because it
6413760	6418000	seems to me but there is there is going to be you're deriving that from models. You're deriving
6418000	6424160	from real intelligent beings now, which is a slightly different thing. And I mean that would
6424160	6430320	be no thing is is is is that because I to me that the bottom up top down is not quite
6431040	6436480	adequate and the top down constraints is not quite because it would have to be not just
6436480	6442000	a constraint, but because that presupposes that there was something there that then
6442960	6447600	that the constraint is coming from outside. And what I'm talking about is a kind of a
6447600	6455920	unity that precedes that's presupposed. And I'm wondering how you can get that into something
6455920	6461280	if it's the very nature of it to be presupposed. And I'm not saying you can't, but I'm saying
6461280	6467360	if you can it's it's it seems to me that you're going to have to somehow derive it from a living
6467360	6474480	thing. And that's conceivable. That's conceivable, I suppose, but but but we are talking about
6474480	6481840	something really frightening. We are. And that's why I keep saying it's a threshold. And I mean,
6481920	6488080	I mean, if you take the the sort of biological analogy seriously, the way
6488080	6493360	Ori does, of course, it precedes the organism, it's there in the environment, it's there in
6493360	6500480	the society, it's there in the I mean, I can roll in 100 Hegelian arguments here about how it does
6500480	6506640	right about how and you know, and those don't have to be supernaturalistic. You have brandom
6506640	6510800	and hinker and others saying, no, this can be given a completely naturalistic explanation.
6511600	6517680	And and and I'm not here to challenge things. But what I'm saying is
6521040	6529440	I don't have any problem acknowledging everything you just said. Yeah. And I and and I don't think
6529440	6537440	I'm misunderstanding you. That's what I'm saying. Yeah. And I mean, I'm actually I mean,
6537520	6542080	this sort of just an exploratory sort of way. But I wonder if there's a difference between
6542080	6547760	the unity of an of an organism. And this is where Hegel might not be so helpful. The difference
6547760	6554160	between the the givenness of the unity of an organism and the givenness of the unity of a society
6555360	6560640	or a culture. But those aren't exactly the same thing. Because I there's there's something
6560640	6565840	and there's a kind of, you know, relative priority of either one. But there's something really
6565840	6572880	distinctive about the unity of an organism that's very. Yeah, that that that I think is crucial
6572880	6577600	to this question, to my mind, in a way. And I'm not saying it can't be answered, but that's the
6577600	6583600	question would have to be answered. How do we actually reproduce that kind of or unity?
6584480	6589680	We know stuff that Aristotle didn't know. You are not an Aristotelian unity. You are a society.
6589680	6596240	You literally are billions of animals, right? And so that's important. And that means that
6596240	6601840	there might not be a difference in kind between how you are organized as a living thing and how
6601840	6608000	societies are organized. And people like Michael Levin are producing some really important empirical
6608000	6613360	evidence indicating that's kind of the case. And I'm not saying it's not saying anything's
6613360	6621600	conclusive, but it needs to be taken seriously. Yeah, yeah. Yeah, I think that that I agree
6621600	6626480	with you, John. I think that that that's the way that I try to always speak about agency
6626480	6632480	intelligence is one that tries to scale almost effortlessly through the different, you know,
6632480	6640960	to avoid the woo soul that we're afraid of. But then again, this is the this is the issue. Like
6640960	6646240	this is in some ways that it's the same problem like one way or the other. So let's say you have a
6646240	6653440	group that self organizes around a purpose, right, or self organizes around affiliation or some type
6653440	6659440	of origin, right? That affiliation, that purpose is also given, right? It's like it appears as a
6659440	6663920	revelation. And then all of a sudden, we're all hunting a lion together. And now we're a group
6663920	6671120	and we're moving towards towards a purpose. Now, this is this is the this is the problem with the
6671120	6678000	situation of what's going now is that what is it? What angel are we catching? Like what what what
6678000	6683600	God are we trying to to manifest? Like which unity what purpose? We have no idea. So we're
6683600	6688880	building this massive body, like this huge, the most powerful body that's ever existed.
6688880	6693520	But nobody knows what it is we're trying to catch. Because when if I get together with a
6693520	6698800	bunch of guys to play basketball, I know what that body is. I know what that with that that that
6699440	6706240	agentic body, intelligent body is is is moving towards right. If I get together with my family
6706240	6710800	and I celebrate our unity is because I know that we all come from the same parent and that there's
6710800	6717600	a there's affiliation that makes our society coherent towards something. But now we have this
6717600	6722800	problem, which is what? Like what are we doing? Like we're just building this giant body. It's
6722800	6730400	like I agree. And I've agreed with that. Yeah. And the thing that that's so odd, I mean, typically,
6730400	6738560	if you think of technology as a human creation in some good positive sense, it's it's it has limits.
6738560	6743520	And it has a particular place. It has a particular meaning has particular purpose, precisely because
6744240	6749840	we create it in order to solve some kind of a problem. We you know, there's some there's some
6749920	6755680	need that needs to be filled and that need has a kind of natural givenness or or or or it's revealed
6755680	6759920	somehow that, you know, it's a responsive to something that we see. What's so interesting,
6759920	6769760	Neil Postman made this point about, you know, when when he said he went to a car dealership
6769760	6774960	and wanted to buy a car and the man was explained to him that they had now these, you know, automatic
6774960	6781440	windows that that that would roll down at the push of a button. And he and he said he said,
6781440	6785840	his his I mean, this sounds so naive, but it's a profoundly interesting question. He said, well,
6785840	6792320	what problem does that solve? And of course, the problem that it solves is the problem of
6792320	6799440	rolling a window down. And his response was, I never perceived that to be a problem. You know,
6799520	6804400	I mean, and it's really interesting AI. I mean, the thing is, what problem are we creating it
6804400	6809520	to solve? I mean, in a certain sense, it's a very different mindset. We're just kind of
6810640	6815840	taking we just want to see what we can do and see what can be done. And in a way,
6817360	6822720	the problems are something that we are arriving at and are surprising us rather than
6822720	6828800	something that we're actually creating something that just simple, simple task of solving for us.
6828800	6835280	You see, I mean, I think that's connected to this being placing ourselves in that in the hands
6835280	6843200	of an angel of some sort, or, or, you know, entering into a kind of an agency that's bigger than we
6843200	6851360	are. Those are all connected. They are. But I mean, one problem was trying to be solved was the
6851360	6856880	scientific problem of like, strongly, I was a project of explaining intelligence. And that's
6857200	6861680	that's a worthy thing to do. And the fact that this technology has largely been separated.
6861680	6867200	But notice, that's interesting. That's, that's, that's not a technical problem. Like,
6868480	6873120	explaining something, it's actually, I mean, to use the classical distinction between
6873120	6877360	theory and practice, that's a sort of a theoretical issue rather than a practical one.
6877360	6883040	But we think of this as a, as a technology. I mean, it's a, that's a curious thing.
6883600	6889280	Well, yeah, and I would get into things like books, our technologies that move between the
6889280	6895040	theoretical and practical. And it's one of the greatest technologies we ever invented. And it
6895040	6901680	had all kinds of unforeseen consequences. And really massively disrupted society. But, you know,
6902640	6906640	and, but I wanted to make another point. And this isn't a challenge. This is just a clarification
6906720	6912960	point, right? These like, like, think about a computer, what problem does a computer solve,
6912960	6919040	it doesn't solve a problem, it is meant to be a multiple problem solver. And then what we're
6919040	6922560	trying to do is make a general problem solver. So what problem is it's trying to solve? It's not
6922560	6927760	trying to solve any problem. It's trying to enhance our abilities to solve all the problems we try to
6927760	6932080	solve. So this machine's going to help us in medicine, it already is, it's going to help us,
6932080	6941200	right? And so that's the answer. Now, again, that's not a challenge. I'm just speaking on behalf
6941200	6945760	of people that think about this. But it is kind of interesting. I mean, so the problem that it's
6945760	6951280	solving is the need to be able to solve any possible problem. Solving a meta problem, yeah.
6951280	6957360	Yeah, yeah. But I mean, but it is kind of, it's sort of, it's sort of curious that,
6957440	6970160	precisely because of the indeterminacy of that, we're exposing this, and I'm just sort of stating,
6970160	6977600	you know, our condition here in a way, we're exposing ourselves to a really great risk. I'm
6977600	6983680	just restating what everybody has been saying here today. But that's something that, you know,
6983680	6990080	requires some wisdom, as you've been saying over and over, John, and prayer to use Jonathan's
6990880	6999520	language too. I just want to do one, Ken, you mentioned in my essay, which is we have done
6999520	7004480	this before. That's how civilization emerged. Nobody built it to solve a problem. There's
7004480	7010000	a bunch of little problems. And what civilization is, is a meta problem solver, right? And that's
7010000	7016800	what it is. And then you can, so I've actually suggested we should also be paying attention to
7016800	7023840	the lifetimes and the life cycles of civilizations and how civilizations reproduce, and why they rise
7023840	7029840	and why they fall to get some better understanding, some other ways of thinking about these machines.
7031840	7037120	So we're- Civilizations are huge distributed cognition collective intelligence machines.
7037200	7042720	That's the living in cities is a horrible idea, except for the fact that it gives us
7043440	7048400	better access to the collective intelligence of distributed cognition. That's the benefit that
7048400	7055120	outweighs all the many noxious side effects of living in cities. You can also get better coffee,
7055120	7065360	typically. Yeah. So we're coming toward the end of our time here. I mean, this has been fantastic.
7065360	7070000	I rarely can go for two hours on any conversation. We were just, we've just been going.
7070000	7073440	Well, not only go for two hours, but sort of wish we had another two.
7074480	7078000	Well, that's what I was going to say. I mean, we can, we can, we can work on doing this again,
7078000	7082240	because it feels like we're, we're sort of, we finally all come together around
7082240	7086480	something here. And now we're really asking what feels like a really important question to me is,
7087040	7093200	well, how do we think about integrating this solution, this meta solution into our meta problems?
7093200	7096960	And that's, that's a really interesting question. I think that John bringing up civilization is
7096960	7103200	such a great point, something that I would really love to explore, because there is also, you know,
7103200	7109360	in the kind of inscribed in the mythological stories, a relationship between transpersonal agency
7109360	7116160	and civilization itself, right? Like if you want to understand why the Egyptians had their king
7116160	7120320	as a god and like all this type of structure, you can help, it can help you understand how
7120320	7124800	they're trying to capture, you know, higher forms of intelligence distributed
7125920	7130400	intelligences in their society. And the idea that we would be doing this
7130960	7136400	technically in AI, I think is something that definitely is worth thinking about and discussing.
7136400	7141760	Yeah. Yeah. No, that's a dimension I just never struck me before. So that's, that's really helpful.
7141760	7145360	Is this the, maybe, maybe there's something we could read together and
7146320	7157680	I mean, short. But to prompt another conversation along these lines of civilization, yeah.
7158720	7164320	I would recommend just because this is how YouTube works that we come to decisions about that off
7164320	7172000	camera. All right. Okay. There you go. Right. I do, if there's a call for me to hang out with you
7172000	7177680	fellows again. I don't care what we're talking about. I'm in. I want to be here. I want to do it.
7178320	7182160	So that's all I'll say about the invitation. Same here.
7184400	7189120	Any closing thoughts, things that feel like need to be brought in or do we feel good about this?
7189760	7196800	Just a word of thank you, Ken. You were the one who arranged this and you did the persistent work
7196800	7204080	to make it happen and find a hole in everybody's calendar that lined up. Not an easy thing to do.
7204080	7210720	So thank you, Ken. And thanks for being gracious for these years now that I've known you. Yeah.
7212720	7216400	And in addition to thinking, Ken, I want to thank you, David and you, Jonathan.
7217280	7223600	I always find it, I get to places in my thinking, the logos that I could not possibly get on my own
7223600	7229360	when I get into a living relationship with both of you in conversation and in discussion.
7229360	7231920	And so I appreciate it greatly. And I just wanted to say thank you.
7232640	7235920	Yeah. Thanks to you guys. This has been great. And like I can say,
7236640	7239680	John and I've been trying to have the conversation for nine months and we just keep,
7239680	7245120	like I cancel, he cancels, and then this is wonderful that we were able to finally get here.
7245760	7247440	Yeah. Well, thank you all. It's been a real pleasure.
