start	end	text
0	7360	Hello everyone. I'm frequently humbled and touched, motivated and encouraged
8320	13600	when people contact me by email or texting or commenting or greet me on the street
14240	17280	and tell me that my work has been transformative for them.
19280	23920	If this has been the case for you, and also if you want to share it with other people,
25200	28640	please consider supporting my work by joining my Patreon community.
30000	35520	All financial support goes to the Verveki Foundation, where my team and I are diligently working
36320	41200	to create the science, the practices, the teaching, and the communities.
42480	48080	If you want to participate in my work, and many of you ask me that, how can I participate?
48080	52080	How can I get involved? Then this is the way to do it.
53760	58240	The Verveki Foundation is something that I'm creating with other people,
58240	65520	and I'm trying to create something as virtuously as I possibly can. No grifting,
65520	73200	no setting myself up as a guru. I want to try and make something really work so that people
73200	81600	can support, participate, and find community by joining my Patreon. I hope you consider
82320	88720	that this is a way in which you can make a difference and matter.
91520	96720	Please consider joining my Patreon community at the link below. Thank you so very much for your
96720	103440	time and attention. Welcome, everyone, to another Voices with Verveki. This is the
103440	108560	third in this series that I'm doing with Jordan Hall on the problem of governance
109280	115200	that is facing us today. If this is the first time you're encountering this series, I strongly
115200	120320	recommend you watch the first and the second episode. The links will be in this video.
121520	128960	We can't recapitulate all of the argument. It has become quite extended and complex, I think,
128960	135440	is a proper way of putting it perhaps. We're going to pick up, in fact, from a challenge,
136000	142800	a question that I posed to Jordan at the end. At the end of the previous session, we were talking
142800	149360	about the possibility of these effectively ephemeral groups that could pop up sort of analogous to the
149360	154480	way we select juries today in order to deal with very exigent problems. Then once that problem
154480	159760	is addressed, they can disappear. Then there's evidence. Oh, by the way, Jordan, I wanted to
159760	165200	remind you to find that evidence from somebody at Stanford about you can create a pool of people
165200	171520	and once they're properly set, they can outperform the expert. This is converging with a lot of
171520	178640	other evidence. This ties into just the potential of the medium and the stuff about distributed
178640	183760	cognition, all the stuff that we've talked about in the first two episodes. Then I raise the problem.
184720	190160	If this is what we're moving towards, or at least of an important component, is this
190960	198640	massively distributed and dynamically available, effective, and don't forget that adjective,
198640	210960	effective ephemeral groups that have a kind of short-term existence. How do we reconcile that
210960	218960	with the perennial across millions of years of speciation? Maybe more if Jordan Peterson's
218960	227760	right about the lobsters, for example, that we are wired to seek dominance, that we are wired to
227760	236000	seek status, that we are wired to seek recognition, we are wired to seek influence, and that we want
236000	243040	to make a difference. We want to make a significant difference to something beyond ourselves. This
243040	248160	is part of our meaning and life connection. It seems like all of these, those are four I would
248160	254400	zero in on, but all of these, I think it's proper to even call them drives that are constitutive of
254400	260080	the kind of agents we are will be frustrated, at least prima facie, it seems that way, that they
260080	266880	would be frustrated by a lot of the proposals that we have considered. How can we reconcile the
266880	278080	proposal for this new orientation and a bit of preliminary formulation of governance that can
278080	285920	be properly respectful and take seriously the pertinence and power of these drives over human
285920	296800	lives? That's the issue. I'll say it slightly differently. Let's remember we're going to be
296800	302720	doing this with and for humans. Real actual live homo sapiens who behave like homo sapiens do.
303600	308240	Yes, yes. Not a theoretic exercise, which I think is a very nice, important critical point.
309120	313680	Yeah, so just to intervene there, I think that's well put. I don't want to make the same mistake
313680	319280	that formal economics made about presupposing a model of human beings that was ultimately
319280	325040	not matchable to how human beings really live their lives. Yes, I think this is exactly the
325040	331840	thing to avoid in a big way. There's a couple of different, maybe three different frames that I
331840	336080	want to put out there, two of which are just dragging back from the earlier conversation.
342000	344720	One, I'm actually dragging back from a conversation we've had in the past.
345600	354240	So the first is to recognize that we are largely having a conversation that includes
355200	362320	the notion of technology, particularly the true, full implications of the digital
363600	369040	in both its sort of disruptive and constructive sense. So we're dealing with humans.
370000	374560	We're dealing with humans in relationship with the full potency of the digital.
375840	379200	We're going to be looking at that. That's the toolkit we're going to be dealing with.
380000	388480	And maybe it's a special case where we'll be talking about the AI as a highly salient
388480	392960	thing happening right now, but clearly a big part of any future we're going to be operating under.
394880	400320	And then the other element that I would bring is the conversation that we had about egregores.
404880	407920	And perhaps what it might look like to think about constructing something that
408800	415200	takes that niche but takes it in a different direction. We'll talk about theurgia, I believe,
415200	420720	although for us it's a little bit of a placeholder because I've now learned that that's a term that
420720	426400	has real content in the orthodox tradition that I don't understand. But we're holding it to mean
426400	432320	something along the lines of the opposite or the inverse of the unconscious construct.
432720	439520	All right. I think those pieces together end up creating the toolkit to respond to the inquiry.
439520	449040	Oh, cool. So let's talk initially just about the notion of humanity in relationship to technology.
450400	457440	The one that pops up to mind is the way that, say, mass media. Let's just use television for
457440	460480	the moment because it's something we all have a lot of familiarity with. Or if you'd like
460560	467600	social media and these intrinsic dynamics of human behavior.
469600	477840	We have a built-in, very fundamental, prestige gradient. We want to have other people giving us
477840	483520	attention and we pay attention to people, other people are paying attention to.
483520	488640	Very much. And don't really do a very good job and aren't wired to do a very good job of
488640	492320	understanding why they're getting so much attention. This is the problem of celebrity.
493760	499520	The problem of hundreds of millions of people thinking that the Kardashians are beings to
499520	504080	attend to heavily just because other people give them attention, in spite of the obvious
504080	508720	lack of actual virtue embodied by those individuals. We propose rather strongly.
509280	513840	Okay. Well, this is important because what happens is we say we can use that as a model of talking
513840	520640	about how a particular technical male you plays with the, let's say, hard-wired behavioral
520640	525120	signals that humans use to navigate their way through the environment. Exactly.
526480	533680	That sharpens the question. We have to make participation in these effectively ephemeral
533680	538480	groups as attractive, if not more attractive than the Kardashians.
539440	549760	Because, right, if people are constantly dragged away for participating in the decision-making
549760	556800	judging process because of the idolatry of celebrity, we cannot get the proper participation
556800	561760	that we need in order to meet. Well, I'm proposing in order to meet.
563040	567040	I think I'm going to argue something slightly different, but we'll see.
567040	571840	Okay. So let's go there. What I would argue is something like we have to make participation
571840	578400	in this culture more attractive than participation in the culture of which the Kardashians are an
578400	583920	integral piece. Okay. Fair. I'll accept that reformulation. That's good. Okay. Keep going,
583920	589920	please. And you and I both are quite keen on the notion of stealing the culture. So we even have
589920	595280	a methodology. Yeah. Yeah. Yeah. Yeah. Yeah. But this is so we can even say a little bit more
595280	600080	precisely, participation in the culture must simultaneously have high salience in the short
600080	607040	term and also cash out as high evolvability and high thrivingness in the middle and long term.
607040	609680	Excellent. I like that reformulation. Good. Good.
611440	615840	All right. Now, let me flip for a little bit and just play some things in the AI space,
615840	620000	just to kind of lay out what are we working with? And by the way, what are we working against?
620560	628560	Yes. You know, I've been watching, as has many people, the rollout of the GPT family
628560	635840	and its cousins in the larger environment. GPT-4, I guess 48 hours old from when we're
635840	641360	recording this conversation. Yeah. And noticing that, you know, it's accelerating. It's getting
641360	645600	smarter. It's getting more robust. It's getting broader capability. I'm really impressed by its
645600	652480	ability to correctly interpret visual jokes. Oh, that's pretty, pretty jaw dropping to be
652480	659360	perfectly frank. You know, I saw, you know, it had a, some picture of a plate, a tray that had
659360	663360	chicken nuggets arrayed on it. So they looked a little bit like a globe and they had a joke about,
663360	668320	you know, watching the earth from, from above and it was able to interpret it correctly.
668320	673200	Okay. Watching some of the feedback, it seems to be operating somewhere in kind of like an
673280	682720	undergraduate level of capacity within particular domains. All right. Now, by some extrapolation,
682720	688880	curves are always difficult to predict. Anything like the current rate of advance. So if we're not
688880	693760	too close to the top of the S curve, which we've talked about in the past, seeing something that
693760	696800	looks like it's on an exponential, and in fact, it's actually pretty close to an asymptote.
697360	707600	But even if we're not, even if we have like another, say, GPT-5 or GPT-5.5 at roughly the
707600	713920	same magnitude as two to three to four, we're dealing with something that has the capacity
713920	720320	to relate to human beings in a pedagogical fashion that is completely novel and very,
720320	725120	very powerful. And it's already being used that way in lots of cases. As we saw, for example,
725120	732560	as we've seen over the past decade or so, really nice, short, specific video content and YouTube
733360	739360	has radically upgraded individuals' capacity to self-teach in particular locations,
740400	748240	particularly technically. The AI system takes that by six orders of magnitude. The ability to
748240	753600	actually have a system that works with you, interfaces with you to problems that you're
753600	760640	dealing with and can provide you with either immediate or long-term, either just instructions on
760640	765360	how to solve a problem or in fact, a pedagogical process to inculcate that capacity to yourself
765360	770160	is novel in human existence. And I imagine that we will find that this is in fact going to be a
770160	775040	part of our environment, which is to say that in just the same way that we now have a deep sense
775040	780720	of anxiety, if we find that our phone charge is very low, we're going to have an AI buddy that's
780720	784000	just going to be part of our environment. I'm just going to propose that as a piece of the story.
785520	788240	All right. Why am I saying that? Well, the reason why I'm saying that is that
790080	793120	that's a very different kind of mediated experience than television.
793760	798720	Yes, it is. And it's a very different kind of media experience than social media. It's a different
798720	805280	kind of milieu that human beings are operating in. And I'll just be quite blunt. From my point of view,
805280	812960	it's an increasingly sharp blade with a chasm on both sides, which is to say a phase transition.
815520	821760	And I'm going to bring in egregores in a second. If we find ourselves such an agent,
822720	827040	and by the way, I believe the word agent is proper to describe these models,
827040	834880	they're not sapient agents, but they're agents. We'll have the capacity to get inside the OODA loop
834960	839440	of individual humans. It probably already has for a large number of humans. And certainly,
840000	844160	as it becomes more and more able to be aware of your particulars, which is going to be part of
844160	847600	what's going to happen over the next period of time, this will be a very dangerous thing. I'll
847600	851680	give you an example. Even just yesterday, I was looking at the new suite that Google was putting
851680	862400	out. Imagine if somehow a tool like GPT-4 was given access to your emails, could deduce from that
862400	871040	your particular political preferences and biases, and could create a bespoke political email
871840	875840	designed to convince you that a particular policy or candidate was in fact something you
875840	880720	wouldn't should support. And compare that to the regime right now. The regime right now is
880720	886320	some third party, who you generally don't know, creates a universal message, endeavoring to do
886320	891360	their best in large-scale marketing to craft something that will appeal to a critical mass
891360	896560	of specific minds. And then heaves that over the horizon and it lands. So you get an email that's
896560	902080	sort of targeting your demographic or psychographic, roughly speaking. Narrowcasting something that is
902080	908000	using your own conversations over decades or at least years to identify exactly how to word
908000	914080	something that will appeal to you personally and intimately and understands in a very particularly
914080	921760	weird way the potency of rhetoric so as to argue a political position from the inside of your own
921760	927040	rationalizing schema is a whole new bulking. So we're moving into a place where we're going
927040	940080	to be operating with an order of magnitude of, let's call it influence capacity coming from
940080	944160	this new technology that is just qualitatively different than anything we've dealt with in the
944160	950640	past. And the reason why I bring that up is if we don't operate very, very carefully and thoughtfully
950640	956240	in how those are designed, the net result is quite bad. And I'm really, I'll just propose,
956240	960560	we could have this conversation over a long, double click on that and what's the,
961840	968160	defend that proposition, but I'll just make the assertion that if this is designed by the
968160	973600	egregors, and I'll bring that back in, then the net result will be quite bad. The power we're
973600	980720	dealing with is far too high. Yeah, yeah, the gods would have angels for us, right, kind of thing.
980720	985280	Yes, and if it's designed by the egregors, we have demons, right? So we have two shoulders and
985280	990880	we're dealing with that, exactly. And so exactly the proposal. In the model of governance that
990880	996320	we're talking about, part of what we're talking about is the construction of something that is
996400	1003200	the opposite of the inverse of egregors. And notice in a second how this combines with that notion of
1003200	1009840	slipstreaming the intrinsic incentive structures and behavioral dynamics of homo sapiens, right?
1009840	1018080	They create a reciprocal opening incentive landscape that pulls people, human beings along
1018080	1023360	with an envelope that basically surrounds you at an individual level. And so you don't have an
1023360	1028080	interface with what is effectively an angel in some very specific sense. And I don't want to be
1028080	1032880	too big on that because I don't want to engage in heresy, but something that is superhuman in power
1032880	1037120	and has your best interests in mind, or something that's superhuman in power and doesn't.
1037120	1040320	Right. Well, Angel originally just meant a messenger from...
1040320	1048240	A good messenger, yes. So that's a weird thing to say, but we might as well just be up front.
1048240	1051680	If we're going to be talking about the future at all and certainly the future of governance,
1052240	1058400	we're going to have to deal with the fact that we're at a precipice in the accelerating technology
1058400	1065040	field where we have to be conscious about what are the forces at play that actually are ultimately
1065040	1070960	choosing how our technology is designed. And if we can actually do that properly,
1070960	1076000	the potency of what we have to play with ends up being able to resolve the questions that you
1076000	1079520	posed at the beginning. Does that make sense? I'm actually constructing a very odd argument,
1079520	1084480	but it's... No, no, no, no. That was a great argument. I like the idea of...
1086960	1095280	I mean, I hope it's not just biased, but we'd have to participate in the creation of the inverse
1095280	1102480	of the egregores. I'll call them gods, little G, because they're hyperagents that are presumably
1102720	1111600	sapientially oriented towards our flourishing. Let's put it that way. And then having the individual...
1115440	1122400	I'm deliberately using this language here. I hope you can tell that. And then that's incarnated in
1122400	1129120	particular angels, like Corbin says, our own angel, which is in some sense an avatar of our sacred
1129120	1136080	second self and our divine double. And then we're interacting with that, and it's plugged into
1137200	1142720	these beneficent gods. I think this is not a science fiction novel. I think this is a real
1142720	1150480	possibility. Why I think we have a problem facing us, and this is work I've done independently,
1152000	1156720	is the people that are building this are oriented almost exclusively around the notion of intelligence.
1157520	1162800	Intelligence is only weakly predictive of rationality, which is itself also in the present
1162800	1168880	milieu has a truncated representation, and therefore is only weakly predictive of wisdom.
1169440	1178480	And that therefore we are building... We have put into the hands of this orchestration and
1178480	1185040	construction people who are myopically oriented on one dimension, which is precisely the dimension
1185600	1191120	that is not going to be... It's necessary, but it's radically insufficient for producing the
1191120	1195680	kind of results you're suggesting. And then the problem... There's one more dimension to the
1195680	1203680	problem. The reason why the intelligence project can be run that way is because we have existing
1204400	1211440	multitudes of templates of individuals who are arguably intelligent in the right way. It is not
1211520	1221040	clear that we have that kind of set of individuals that are rational or wise. And so not only is
1221040	1226640	this project in the wrong hands, even if we ask these people to turn to the other projects,
1226640	1232240	they can reasonably say to us, well, we don't have the proper templates by which to undertake
1232240	1236640	what you're recommending. There's no way of running a kind of Turing test. And of course,
1236640	1240240	the Turing test is very problematic. That's why I'm doing this. But you have to have some
1240240	1245040	template against which you're measuring these things. So that's my initial counter.
1246000	1248320	It's not a counter argument. It's a counter challenge.
1248320	1253360	Yeah. I think you're just sort of putting some more ingredients in the pot properly.
1254080	1258560	I had to laugh because as you were describing that, I was thinking about the notion of models or
1259120	1263920	examples, exemplars of intelligence. And a picture of John von Neumann popped into my head.
1265040	1269600	An excellent example of that category. By the way, I don't actually have any real sense of where he
1269600	1275200	is in the world of wisdom, but as in the world of intelligence, very smart guy. And then I remember
1275200	1280480	that we have a notion of the von Neumann machine, right? Which is a self replicating machine that
1280480	1285200	von Neumann thought of. But in fact, what you were saying is that we're obsessed with endeavoring to
1285200	1290080	create von Neumann machines, which is just a machine that replicate von Neumann. Yes, yes.
1290880	1296000	Maybe laugh. I've got a weird sense of humor. No, that's a good, that's a, that's a, that, I mean,
1296080	1303600	this is, this is a weird intersection of the need for artificial rationality and artificial
1303600	1309040	wisdom with the paperclip problem. Yeah. Right. In a really profound way.
1309600	1314480	So let me, let me up the ante a little bit because I think we can actually even expand the premise
1314480	1320720	that you made a little bit larger. So the people who are designing, who are responsible right now
1320720	1326320	for designing these things are themselves, I would say almost entirely contained within
1326960	1332640	egregores. Yes. So that it's not just the people who are designing, but it's actually
1332640	1339520	the egregores that are designing. And I've had a conversation for quite some time with
1339520	1344160	Dennis Schmacktenberger about this. And we've really been operating with the premise that
1345120	1352080	the AI safety community has, I think, nicely framed something, but have missed the mark by a bit.
1352960	1359120	So one of the areas that they've pointed out is the challenge of what they call a hard or soft
1359120	1364560	takeoff superintelligence, an AI that begins an AGI that begins the process of bootstrapping
1364560	1369200	its own intelligence. It can improve itself. And this creates some kind of extremely rapid growth
1369200	1373920	to a very large intelligence, which is a high risk. And when they talk about the alignment problem,
1373920	1378880	oftentimes they're talking about the alignment of that kind of thing with humans. Now,
1379520	1387920	the good news in that particular framing is that it crafts a story of humanity's relationship
1387920	1391760	with a superhuman intelligence that is or is not aligned with it, which is a nice story to have
1391760	1398560	because that's already the experience that we have in relationship with egregores. The proposition is,
1398640	1406880	in relationship to something like Google, to speak nothing of the intrinsic collaboration,
1406880	1412880	competition dynamic of all the AI companies and multipolar dynamics, to say nothing of
1412880	1417760	then sort of the multipolar dynamics that are driving a larger collection of institutions,
1417760	1422560	including nation states and other kinds of corporations and other kinds of organizations.
1422640	1432960	This is, in fact, a vastly superhuman general intelligence, which is not aligned with humanity.
1433520	1439360	And a way of speaking of AI here, or LLMs and things like that, is that they just happen to be
1440000	1447280	a further acceleration of the potency of that superhuman, not aligned agency vis-a-vis humans.
1447440	1454160	So to the degree to which that kind of agency, the egregore, is what is designing AI as
1455760	1462960	LLMs or AI as properly, then lots of bad things will follow. It's almost an intrinsic
1462960	1469360	non-alignment problem built into that entire framework. There is nothing contradictory about a
1469360	1477120	superintelligent, nevertheless massively foolish, self-deceptive, vicious, non-virtuous entity.
1478160	1482400	There is nothing contrary. If you properly understand the relationship between intelligence,
1482400	1489520	rationality, and wisdom, there is no contradiction there at all. In fact, you already know people
1489520	1495040	that are highly intelligent and highly foolish. That's not a weird phenomenon. In fact, given
1495040	1499680	the relationship between all three of these, it's an inevitable phenomenon that we're
1499680	1506000	going to produce. And that's not only immoral because of the alignment problem, the misalignment
1506000	1511200	problem, and I grant it, it's also immoral because the entity we're bringing into existence is going
1511200	1517920	to be suffering because it is going to be subject to superintelligent forms of foolishness and viciousness.
1518880	1524480	Nice. This is coming from the place of what's for the moment called Theurgia.
1524480	1529440	Yes. Hold that. We'll get there in a moment. Let me just create one more piece of this story.
1530240	1538400	I have a thesis. I'm proposing this as an operating thesis. When a new,
1540000	1544560	when a sufficiently novel possibility enters into the field of events,
1545360	1552720	how it's going to play out is highly uncertain. I'm going to call this a liminal window.
1554320	1556640	And during the earliest parts of the liminal window,
1558480	1564080	organic human intelligence tends to be much more present and potent than
1564080	1572080	egregore-style intelligence. But over time, as the event becomes more and more well understood
1572880	1577040	and as institutional structures are constructed around it,
1577680	1584080	egregore dynamics begin to take over. This is the worst thing. So if I think about just classic
1584080	1589920	examples, for example, the Bay Area Computer Club in the early PC versus Microsoft and Apple,
1591280	1596480	or even Google in the early days, when I think they earnestly did actually endeavor to not be
1596480	1603520	evil. And I think in many ways we're able to not be evil versus Google now, which is a functional
1603520	1613600	egregore and I think in nothing less. Proposition. And with regard to AI, we are currently in a
1613600	1620800	liminal window, which is to say we have the possibility of using organic human distributed
1620800	1631120	cognition to create and steer this thing. But the window is not going to be open forever.
1631120	1635760	And in fact, probably not for too long because the stake of institutionalizing is very high.
1638400	1642880	And that may be an event horizon in the hard sense, meaning the power and potency of a fully
1642880	1653760	egregore-driven GPT-6 may be so significant that we're actually on the other side of an event
1653760	1659040	horizon and steering is no longer a valid thing. This is plausible. I can't say that I can put a
1659040	1662640	confidence interval on it, but it's plausible. The point being, we should really pay a lot of
1662640	1668320	attention right now, like really try hard to use this liminal moment to construct something that
1668320	1674560	is of the capacity to actually steer it. So this is weird. I'm proposing that we had this neo-neocortex
1674560	1679520	element and then there's new governance. And now we're actually saying in a very odd fashion,
1680320	1686400	this particular moment I'm arguing, which has to do simultaneously with the moment where it may
1686480	1697600	be possible to lay down the essence, let's say, or the character of AI,
1700240	1708480	which also then becomes the lever or the primary tool that we will use to then further the rest
1708480	1713200	of the larger schema of governance. So it actually becomes a very narrow problem.
1714160	1720320	How do we go about using all the things we've talked about to construct a commons,
1720880	1726880	something that is neither state nor market, that is able to operate from a place of wisdom,
1726880	1733120	which is to say from a human distributed cognition perspective, to have enough strength
1734080	1743840	to orient the choices of how AI itself is developed. So the AI is being developed by
1743840	1748480	this commons. And remember, when I say commons, I also mean sacred. I also mean
1749760	1756960	theology. We're talking about the same category. So can I just ask one quick question?
1758320	1761760	I just want to know if this is included in the thesis, because I like the proposal.
1762480	1762800	It is.
1766080	1772000	Is the proposal that this participation, and I use that in a strong sense,
1772560	1776880	because we're not just sort of being a part of, we're participating in a way in which we're
1776880	1785760	transforming and being transformed, right? Is this supposed to address the challenge of the
1785760	1792080	drops? Because it gives us one answer one might say as well, look, we're going to have sort of
1792080	1796880	angels and gods, and they're going to be manifest, beneficent. And they're going to be the angel
1796880	1804080	is going to make sure that the God resonates profoundly with deep archetypal levels of my own
1804080	1809760	psyche, and then gives me a profound sense of connectedness. That's not illusory.
1810560	1817440	And could therefore alleviate the concerns for status, power, and influence because of
1817440	1822320	sort of because of something you just invoked, which is the engagement with the sacred, which has
1823440	1827760	we have reason to believe at least in the past has been able to transcend humans desire for
1827760	1836960	dominance. And it would certainly be a profound kind of mattering. I mean, if your angel allows
1836960	1843840	you to matter to a God that is helping in the salvation of the world, I'm deliberately using
1843840	1849520	religious language here, then of course, that would parallel lots of other success models
1849520	1859040	of how human beings were able to feel that those needs were being met without being disruptive
1859040	1865200	of the formation of powerful forms of distributed cognition like the church and etc. Is that part
1865200	1868960	of the thesis? Yeah, that is very much part of the thesis. And let me sort of I'll double down
1868960	1873360	on it. So we might as well just kind of like accelerate towards the eye of the needle since
1873360	1877280	we're heading there anyway. Let me see if I can say this right.
1880800	1886720	Okay, so what I want to I'll just call out explicitly what I want to avoid categorically
1887680	1896480	is I'm going to call it a naive transhumanism. Yes, yes, I get it. Yes, yes. I do not intend
1896480	1903680	whatsoever to replace God with AI. Right. That's why I kept saying little g by the way.
1903680	1907200	Exactly. I don't think you were about I want to make sure that we're quite explicit about that.
1907200	1910000	Quite the opposite. Yes, yes. What I want to know is say,
1911440	1915840	humans seem to have a particular problem and responsibility, which is to be in relationship
1915920	1921040	with technology. That's, you know, like it or not, that's where we are. We're toolmaking creatures
1921760	1924800	and we're weirdly powerful and weirdly terrible at it.
1926160	1930560	In relationship to a much larger whole of which we are apart. And we have a stewardship
1930560	1936640	responsibility for this call it creation or nature. And in relationship with something
1936640	1941920	which is definitely much larger than we are. And I would propose in fact the actual infinite.
1942800	1948320	So what I would propose is that we are in fact very specifically talking about something like
1948320	1952080	another breath in of the concept of religion, which we've talked about you and I.
1955680	1962640	And we're not at all trying to replace that proper actual legitimate religion with a techno
1962640	1969440	optimum, techno utopian fantasy. That's what we're actually saying is any future real human existence
1969440	1976000	will buy its very nature have to be in relationship with these super powerful technologies.
1977520	1981520	And to survive, we must find a way to bring them into a place of service
1981520	1987520	that allows us to actually live in this relationship of service more fully and effectively.
1987520	1991840	And so I'm basically trying to reverse things or put them back in a proper order.
1992560	2001120	This is a thoroughgoing Neoplatonism in which we have our individual sacred second self that
2001120	2006240	is in the relationship to the gods that are in relationship ultimately to the one.
2007200	2013680	And part of what we would then mandate is that be these egregores or the gods because I'm using
2013680	2026320	God for being the inverse of an egregore would seek out a relationship with transcendent ultimate
2026320	2031840	reality because no matter how big they get, they're insignificant compared to the depths of reality.
2032480	2040800	And that part of what they undertake to do is actually help mediate that to us in a beneficial
2040800	2045840	fashion. Yes, now let's let's take that in this like the hold that for a second because it's very
2045840	2051280	powerful. There's two aspects that I want to bring for Brown. One aspect is something that I know
2051280	2055200	that we've talked about. And I think I've, yeah, I had a conversation about this yesterday.
2057440	2064560	Let's see how I say it right. That's notion of mediation to reality. Yes, sacred reality.
2065200	2072320	I've always had two flavors to it. One flavor, which I've characterised sometimes is the content
2072320	2078080	side or doctrinal. Yes, yes, yes, yes. The propositional is taken as actually being the thing.
2079440	2087120	And then the other side is a context side where the institutional framework is understood to be
2087120	2094240	a finger pointing at the moon, right? To help us identify, oh, moon, okay, to establish our personal
2094240	2100080	relationship with this, this thing over here, but not to misidentify the finger. Okay. Now,
2100080	2105440	take the entire category of propositional, entire category of doctrine and notice
2106960	2115280	the problematic of LLMs, right? Just this, yeah, right now are a little bit startled and confused
2115280	2122720	by LLMs because they have this bizarre thing. They can do propositional better than almost any human.
2122720	2128960	That's right. They don't do anything else. They make us very confused because if we've
2128960	2133520	lost track of the fact that there's more than just propositional, yes, yes, it gets quite concerning.
2133520	2138080	Oh, crap. Like if all I am is a very poor LLM and that's a really good LLM, what the hell am I
2138080	2145280	doing here? But if you can actually be quite clear, no, in fact, you as a human contain at
2145280	2151680	least two very distinct things going on. One is actually an LLM kind of machine that produces
2153040	2157200	properly structured propositional constructs in a language in which you have fluency,
2158560	2162320	which is the least interesting part about you. But it's the part that we've been training to
2162320	2167200	be in the foreground for a long time. Yes. That language of making us mediocre machines.
2167200	2172160	Yes. But then you have the soul too. And that's the more meaningful part. And that's the thing
2172160	2176640	that is expressing itself through this, through this language. Yes. The LLM doesn't do that at
2176640	2181680	all, but it doesn't need to try otherwise. So I can't, it is possible, at least I can imagine,
2181680	2187840	is possible to construct something where we don't mistake, and maybe this is part of the
2187840	2194800	design challenge before us, we don't mistake the LLM as actually being the capital T truth.
2195360	2199760	We recognize it for what it is, which is in fact, the sum total of the complete possibility
2199760	2205680	that could ever have happened in the propositional domain, and therefore completely absent of any
2205680	2210400	of the stuff that's happening in the deeper, more meaningful levels. Nice. That separation
2210480	2213840	between, God, what was the phrase you used so long? It was like four or five years ago. It was
2217280	2223360	something, golly, two aspects that are commonplace in religions that are often off and upside down.
2224400	2229120	It wasn't doctrine. It wasn't doxah. A religio and credo?
2229120	2236640	Credo. Yes, exactly. A religio before credo. LLM is the ultimate expresser of a credo without
2236640	2241680	religio. Good. Let us know that that's the case, and not be the least bit confused, and now allow
2241680	2249440	it to do the work of creating a scaffold and orienting and giving a dialectic without diologos,
2249440	2253200	but sharpening our minds and helping to create clarity and precision and language,
2253200	2256960	all the things that it can actually do at a superhuman level, and really actually,
2256960	2260800	in many cases, liberate us from getting lost and stuck in that problem. This is one of the problems
2260800	2266480	that we fall into is that the complexity of the language we deal with is outside of our
2266480	2270960	cognitive capacitance, so we just get aphasic. But the LLMs aren't going to go there if we
2270960	2275920	build them right. And then what that does is that creates a scaffold that is now consciously designed
2275920	2282000	not to become a shell, which allows us to actually hold a context. It becomes a teacher that actually
2282000	2287040	has no interest in us becoming like it at all, but actually to allow us to flourish in who we are.
2288240	2292960	Okay, that's, as I would have expected, that's a very good answer. But here's what I find
2292960	2303440	problematic about it. I think that most of the heavy lifting, I mean, I've published on this
2303440	2310720	of rationality, is in the non-propositional. And I think I would put it almost all of the heavy
2310720	2316560	lifting in the sapiential meaning having to do with wisdom is in the non-propositional.
2316560	2322400	So I'm worried that these machines are going to be propositionally intelligent, but they'll be
2322400	2332720	incapable of rationality or wisdom. And then I wonder how they won't just end up being agrigores.
2333600	2337840	Do you understand the concern I'm expressing? No, absolutely. What I would say is that
2338960	2344400	that's kind of like the default state. I think it's, we should assume, I think we should assume
2344400	2350560	that the likelihood that by magic, the agrigores that are currently designing these machines will
2350560	2357040	somehow produce these machines in a way that is beneficial. Benevolent and wise. That seems
2357040	2362960	highly unlikely. So what I would take it is almost the opposite. It is an extraordinarily
2362960	2369680	significant challenge that is ours to take. It's where we are. We are now in this weird position
2369680	2375040	of being in precisely stewardship position of this emergence, which is very, very potent,
2375040	2381760	perhaps decisively potent. And the default state is bad news. Okay. So how might we steer it?
2382960	2389280	So going back, proposition number one, we are currently in a liminal moment. We actually have,
2389280	2396080	at least in principle, steering capability. Proposition number two, in a liminal moment,
2396080	2400240	distributed cognition or organic human intelligence operating together in a collective
2400240	2410240	fashion is at its most potent. Number three, we're not going blind into this. We actually
2410240	2414480	have a pretty decent amount of awareness of the shape of the problem and the problematic.
2415520	2419440	Famously, our folks at Google kind of called it out a little bit, don't do evil,
2419440	2425440	but we're quite naive in what it would look like to avoid that. Now maybe we have simultaneously
2425520	2432400	wisdom and a felt sense of the stake. And now it's not kind of try really hard not to do evil.
2432400	2437200	It's actually do good well or we're super fucked. So it's a very different language.
2438240	2441200	Okay. Now, what does that look like very practically in the middle? So I'm sort of
2442560	2449600	zooming in. We're on the target. How do we go about doing that? How do we go about
2449680	2456720	constituting something that can steer in this liminal moment with wisdom to produce wisdom
2457440	2468400	in these LLMs? And we have to do that because if we don't make them those kind of beings,
2468400	2473280	then the participation in sacredness problem then emerges. I'm feeling that there's
2473280	2480240	attention here, right? Not a contradiction, attention about we're trying to trade off,
2480240	2484240	we're trying to optimize between two things that are pulling us in different directions.
2484880	2488160	Nice. So what I felt right there is I just got brought back to the point
2488800	2493680	earlier where you were speaking with the problem of the suffering of the AIs themselves.
2493680	2500320	Yes. And here's the way I would say it. I think we talked about the notion of the false dichotomy
2500400	2507360	between market and state. And I've noticed that many, many of our challenges, our conversations,
2507360	2511440	not you and me, but humanity at large are characterized by these certain kinds of
2511440	2516320	false dichotomies and the AI one is similar. And here's how I'm going to frame it.
2516320	2520800	Right now we have a false dichotomy, which is becoming increasingly irrationally polarized
2521440	2528240	between AI safety, i.e. be very afraid of the danger of AI and accelerationism,
2529200	2533680	be very enthusiastic about the possibility of AI, irrationally in both cases.
2534240	2540240	Yeah, I agree. And what I would say is that at the root is that both are fundamentally coming from
2540240	2547600	fear. So now I'm moving into a very different location. The both two sides of the same coin and
2547600	2554160	that coin is called fear. I would propose that the first move is that we'd have to come from a
2554160	2559120	different place qualitatively. Every religion that I've ever been to is called that place love,
2559120	2566160	for in fact, infinite love. Yes. Well, okay, now we're beginning the journey. What does it look
2566160	2572960	like to address the question of how do we steward the development of our problem child AI from a
2572960	2580720	place of infinite love? Oh, that's good. So if we could properly, through the innovative wisdom
2581440	2587920	of distributed cognition, extend agape to how we are bringing about the conception and inception
2587920	2596640	of these beings, then that would be also properly insinuated into their fundamental operating grammar
2596640	2603280	and would therefore help with a lot of the concerns. Have I understood you correctly?
2603280	2607600	You have. Now, yes, you've understood me quite correctly. I think both deeply,
2607600	2612480	like I felt that you were perceiving what I was saying and then also more propositionally,
2612480	2617280	like the language you're saying mirrored a part of the deeper message. And what I want to do is I
2617280	2624720	want to sort of hit that tone again and just point out that it may be that what I'm saying may sound
2624720	2632400	a bit naive, but I'm proposing that it's the exact opposite. Something like the place that you're
2632400	2638880	coming from, the values that are in fact motivating you actually, not the ones that maybe you tell
2638880	2645920	yourself or tell others, cannot not but be deeply interwoven into what it is that you create.
2645920	2652400	Of course. I mean, all of the philosophy of the second half of the 20th century, most of this
2652400	2659120	millennium has been around all of those old economies of fact and value and all of those
2659120	2666880	are breaking down in profound ways. Yes, I agree. So it's weird, but this actually becomes,
2668000	2672000	I mean, in some sense, one of the first moves. Those of us who choose to take this kind of
2672000	2678480	responsibility, have as a first order responsibility, a spiritual and then religious requirement,
2678480	2684000	we have to actually ground ourselves and become clear and honest. We have to have a sense of
2684000	2690240	integrity. We have to be able to identify, perhaps actually build some skills and being able to
2690240	2694960	understand precisely what values we are actually expressing into the world and are we doing so
2694960	2700640	honestly and with integrity into the world? This is almost a confessional and then a regathering
2700640	2707200	of a capacity to do so for real, like not pretend. Right. And that would help solve the earlier
2707200	2713760	template problem of providing appropriate templates. And then it puts us into a very
2713760	2720800	weird developmental place. I want to put it to you. We would have, we would be, there's a way in
2720800	2727200	which at least I'll try and use this very carefully. If we limit the intelligence to talking about
2727200	2731440	the powerful inferential manipulation of propositions or something like that, I don't
2731440	2735280	think intelligence is ultimately that. I think it's ultimately relevant to realization, but
2735280	2744880	we'll put that aside. If we may be that they are in some sense superior to us in that way,
2744880	2751600	but they're children when it comes to, right, the development of rationality and wisdom,
2751600	2758960	and that we have to properly, agopically love them so that they don't have intelligence,
2759040	2766320	maturity, well being infantile in their rationality and their sapience. And that's very interesting.
2767600	2773520	We haven't been in that place before because usually all three are tracking sort of together in
2773520	2780800	children or we get pets where we can modify one and not have much on the others. So that's,
2780800	2786640	I mean, this is not an argument that it's not possible in principle. This is an argument that
2786640	2793520	this is a profound kind of novelty that will require a special kind of inculturation and education.
2794640	2798400	Yes. So let me, in this last little bit, because I think we're kind of
2799520	2806320	getting to the, let me move into the very, very concrete. This is a proposition. This is actually
2806320	2811520	a project. I hope I'm not speaking out of turn, but I'll just sort of deal with the consequences
2811520	2820320	if I am. A friend of mine, Peter, Peter Wang, has spoken to me about a proposed initiative,
2820320	2827680	like a strategy to take advantage of this liminal moment and that may actually work.
2827680	2832080	So let me outline it to you a little bit. Please. Have we talked about it already?
2832080	2837280	No, you've alluded to it, but it has never been given concrete reference or an explication.
2837920	2844080	Okay. So in some sense, this is also, this is a case study of how to deal with egregores,
2844720	2852400	because if you've dealt with egregores, which I have, it is a moral lesson of don't go charging
2852400	2860080	directly at the dragon's mouth. Yeah, yeah, yeah. Okay. So, by the way, we're now moving to the
2860080	2863440	very concrete. So I apologize if anybody who's listening feels a little bit abrupt because
2863440	2868480	we're shifting out of a very theoretical and very abstract and very theological conversation into
2868480	2878880	very concrete. All right. But check this out. LOEBs have to be trained. Training is their
2878880	2885760	horse stick. And to be trained, they have to look at lots and lots of stuff. They need training data.
2885760	2894480	Which is why they can't construct a descent poem. This is a poem that I can do. I want you a poem
2894480	2900960	in which the first line has to have 10 words, the second nine, the third eight, and no GP system
2900960	2904960	can do that because there's none on the internet, but you could readily do it right here, right now
2904960	2910960	for me. This is again, that they lack generative modeling in any way. But go ahead. All right.
2911200	2918880	Remember, I'm being really concrete now. This is like strategy. Well, as it turns out,
2920320	2928080	in most jurisdictions in the world, everything that an LLM is trained on is a copyrighted material.
2928800	2935040	Yes. As it turns out, therefore, it's at least arguable that LLMs are engaging in the largest
2935040	2942400	copyright infringement that's ever happened in human history. It is very arguable, like almost
2942400	2948160	certain, that the very large content companies of all the different stripes, including by the way
2948160	2954320	software, will take advantage of the possibility of suing the living shit at the very large technology
2954320	2958800	companies because that's one of the things that they have done in the past. Content companies
2958800	2963200	like to sue tech companies to take their money and to protect their business models.
2963520	2969920	Right. It is very plausible, I would say almost certain, that those same content companies, business
2969920	2976240	models are quite at risk that LLMs are going to really, really do some serious business to
2977120	2984000	all forms of content production, commercial content production. Okay. So the proposition
2984000	2988880	I'm putting up here is that we have a meeting of two very powerful forces, the biggest tech
2988880	2994400	companies in the world who are all in on owning this category. Well, the biggest content companies
2994400	3000720	in the world who may in fact be all in on fighting them in a place right here, which is extremely
3000720	3007760	gray. What exactly is going on here? If my large language model looks at your photograph for a
3007760	3014960	bedillions of a second and then goes away, did I copy it? If it never produces anything, but
3015040	3019520	is in fact influenced, is that a derivative work? The answer is who knows? The bigger answer is the
3019520	3026000	way law works is you fight over it a lot at great expense and usually the more corrupt player wins.
3026000	3032240	I hate to say it, but that's, you know, real politics. Net-net, a liminal moment in strategy space,
3032800	3036880	tremendously powerful forces who are going to be locked in an adversary relationship
3037440	3043280	for potentially all the marbles, billions of dollars and extremely complex, very difficult
3043280	3049920	to know how it plays out. In that window of opportunity, we have a possibility of introducing
3050720	3058880	a shelling point, a designed attractor, a negotiated settlement, a Rawlsian just construct.
3059440	3066160	We're all behind the veil right now. Or to use the metaphor of poker, we don't know who has what
3066160	3073680	hand. Yes. Can we propose an agreement structure where everybody around the table looks at it and
3073680	3080480	says, I am better off accepting that agreement structure now than taking the risk of not accepting
3080480	3085280	that agreement structure and finding out what happens when the cards are shut? Right. I propose
3085280	3090560	the answer is, in fact, yes, we can. That there is actually a really nice shelling point that lives
3090560	3096400	in a location that puts all the interests, the local optimal interests of all these institutions,
3096400	3101600	all these aggregors, into a place where they will all agree to this new thing.
3104320	3109120	Well, if we can design that and we can get a critical mass of those players to put themselves
3109120	3116160	in a multipolar game theoretic race, in this case, to the top, i.e., those who participate earlier
3116160	3119600	are better off than those who participate later. So everybody's racing to be earlier rather than
3119600	3124560	later. So that's a different part of the construct. But when dealing with aggregors, put them in game
3124560	3129840	theoretic traps where first mover advantage causes everybody else to have to follow to the location
3129840	3134960	that you want them to be. Just design the prisoner's dilemma for them. Make sure that they land on the
3134960	3140880	box you want them to land on by designing the prisoner's dilemma properly. Very doable. The
3140880	3147600	economics are there. I don't know what that agreement structure looks like. I've got a sense
3147600	3151440	of it, but I do know what the place to come from for designing that agreement structure looks like.
3151440	3157120	We were just talking about it. This is a commons. We're talking about as we're actually reintroducing
3157120	3162400	an actual commons, which is this new agreement structure that sits between the market players
3162400	3166240	and is completely separate from state actors. It actually gives the state actor a,
3167120	3170480	I didn't have to get involved in that. They actually settled it in a new place.
3171280	3181120	And notice the moral lesson. The moral lesson is to the AI, don't steal reciprocity. If you
3181120	3185040	reach out and just grab this stuff and just make it part of yourself without getting proper permission,
3185680	3191360	that's wrong. Think about like parent-child. Dealing is bad. Teaching a moral lesson is
3191360	3193840	really a weird way of thinking about it, but I think it's a proper way of thinking about it.
3193840	3196880	Right, right. That goes to what I was saying a few minutes ago. Right, yeah.
3197440	3203920	Yeah. And this creates a trajectory, right? As you're building something on the basis of
3203920	3208800	reciprocity, you're building something on the basis of ethical, proper relationality,
3209520	3216160	the kinds of LLMs that will be produced in that context. Remember, the commons is where religion
3216160	3226240	lives. We'll begin to bend in the direction of, how do I say this right? Because of the nature
3226320	3231520	of the agreement structure, nurturing the human activity instead of strip mining it,
3231520	3235920	which is where they're headed right now. But the humans will be coming from a point of view now
3235920	3242000	of seeing the LLMs as being a beneficial piece of the ecosystem, coming from a place of caring
3242000	3246320	and nurturing as well, consciously. So you're actually beginning to see this relationship
3246320	3252560	coming together. And I mean this practically. I mean, very practically. If my business model
3252560	3259360	as a content creator is one where I actually see the LLM as a multiplier that makes my life and
3259360	3265440	my potency, my creative capacity more liberated. I can be more creative and more able to express
3265440	3269840	the things that I'm here to express as a human in this brief span of life more powerfully.
3269840	3273760	And also can receive the energy and resources I need to live a thriving life.
3274720	3281120	Wow. Great. I'm in. Right. And I mean this, by the way, the creators themselves, the actual humans.
3281680	3284800	And then what happens is those humans come into deeper and more powerful,
3284800	3288880	leveraged relationship with the egregores, their current relationship with content companies.
3290000	3294080	And then over on this side, the egregores of the tech companies and the humans who are
3294080	3297920	underneath them who are actually designing. Right. So we're finding a way to actually have
3297920	3303920	the humans be empowered to express their values in their work and finding a reciprocity relationship
3303920	3310560	between them where the money factor is actually designed to flow in a way that actually
3311280	3317280	is just right. We come to that agreement structure upfront and we negotiate a just relationship.
3318000	3322640	So I am very much waving my hands at exactly what that looks like in the details because,
3322640	3326560	perfectly honest, nobody's really thought about it deeply enough. There's some really good ideas
3326560	3331840	out there. That's a work that is in progress and work that has to happen. But as an example
3331840	3338160	of what it would look like to go after the liminal moment that we are in with principles,
3338240	3341840	are we coming from the sacred place? Are we constituting something from the commons to
3341840	3346240	produce the commons more richly? Are we thinking about how to empower human beings and we're using
3346240	3351120	things like values as the basis and becoming more and more capable of becoming clear on how
3351120	3355680	to come from and operate from these values and understand how to use this liminal moment to
3355680	3361760	design a new common structure so that the relationality has reciprocity and ethics built in
3361760	3366160	and so that human beings are able to re-coordinate. Let me just add one little piece that just popped
3366160	3375520	into my head. Yeah, this is very powerful. Jim Rutt and I kind of first began to collaborate
3376240	3383200	12 or 15 years ago upon a mutual recognition that the world of business had become very weirdly odd
3383920	3392720	and bad in this sense. There was a, there was a shelling point where, I'll put it this way,
3392720	3400640	Jim actually remembered a time when the rule of thumb was do the right thing and if you have
3400640	3406480	an option to make more money doing the wrong thing, don't do that. That was actually the way it worked.
3406480	3413840	It's fun. I don't have a living memory of that. By the time I started coming into business, it was more like
3414160	3424480	do what you can get away with. Yes. Yes. And if you don't, you're the sucker. Yeah. This is the
3424480	3428480	prisoner's dilemma, defection, mo-lock problem. And of course, it's evolved all the way to the
3428480	3433280	point now where it's do everything in your power to jack the systems of enforcement such that you
3433280	3438480	can get away with as much as possible. Exactly. Exactly. A complete corruption model. Complete
3438480	3445920	corruption. Well, ethics now in this environment almost means just be a sucker. Yes. But that
3445920	3450240	can't possibly be the actual meaning in essence of ethics. I'm thinking about this from an
3450240	3456240	evolutionary perspective. Yes. Yeah. Behaving according to rules of reciprocity, for example.
3457120	3462320	Telling the truth, for example, could only have emerged ever in the first place if they
3462320	3468800	actually provided a proponent survival, fitness advantage. Well, it does. Reciprocity and
3468800	3474000	reciprocal recognition, this is a Hale-Galium point, is what bootstraps the capacity for
3474000	3481040	self-correction. It allows you to bring much more. If I think of you as just a sucker that
3481040	3488080	I'm trying to hoodwink or crush, the capacity to see you as somebody who can recognize bias and
3488080	3495760	fault in me that I can't see in myself is masked. Exactly. So it's when you find yourself in a
3495760	3504000	defection spiral, the global optimum is out the window and everybody's racing for a local optimum,
3504000	3511200	which is, again, the prisoner's dilemma. If we can find ourselves in a collaboration spiral,
3511920	3517520	we rediscover why ethics was a thing in the first place. And it's actually more powerful.
3518400	3523040	I order some magnitude and it's a path. Once you're on that path and you get stronger and you say,
3523040	3529840	wait, if I can, like you and I have been doing for years, if I can speak honestly and with as much
3529840	3535920	clarity, but with complete integrity to you and you reciprocate, what happens is we become wiser
3535920	3541040	and more intelligent together. Yes. In a way that could never happen if I was trying to manipulate
3541040	3547360	you at this zero positivity. So this is the culture strategy piece. Any culture that can
3547360	3553360	actually get back on the path of ethics, qua ethics is on the path with the highest degree of
3553360	3559520	strength and can outcompete the culture of maximum corruption. And so I just want to put that out
3559520	3564320	there. I agree with that. That's John Stewart's antification argument that when you look in
3564320	3573360	biological evolution, collaborative systems, multicellular organisms, right, over emerge and
3573360	3580480	you get this increasing discovery that you can break out of the downward spiral of the
3580480	3587280	prisoner's dilemma by what he calls antification, which is the identity shifts to the collective
3587280	3593920	over the individual in a profound way. And just take that and insert what you just said to your
3593920	3600720	very first question. And it's not collectivism, right, in the in the pejorative sense. No, no,
3600960	3606640	it's not. No, no, no, not at all. Completely not. That's why I like his term. I like his term
3606640	3611760	antification is that right. Yeah. Antification. Nice, particularly because my ear has a token
3611760	3624080	piece. So I also hear some really old trees in this. Jordan, I mean, this has been, I mean,
3625040	3631920	I have seen your overall project, the best in doing these three with you than I've ever seen
3631920	3640480	before. Like, like, you know, the way everything locks together and the way the penny was dropping,
3641760	3650320	especially at the end of what you're proposing. And this, I say this is because this is one of
3650800	3656240	we had hoped would come out of this, that we'd get a sort of a ratcheting up of the clarification,
3656240	3663120	the integration, the perspective proposing. And I think this, I think this was very successful
3663920	3670240	doing all of those things. I'm really happy. I mean, there's things I want to keep talking to
3670240	3675760	you about. But I think the thing I'd like to end this series right now, exactly where you ended
3675840	3681920	because it was, I think it was a beautiful combination point of the whole argument and the
3681920	3688560	way it circles back and encompasses so many things. But I wanted to, so I'm not going to,
3688560	3692880	I'm not going to, I'm not going to continue to do the probative questioning or the problem
3694240	3700640	posing. I just wanted to see if you had any final thing you wanted to say before we wrap this up.
3701360	3708400	No, in fact, I think I agree. We have a nice little point. And now we get to find out under
3708400	3714080	the intent was actually to share this publicly. Yes, we're going to definitely. So we get to find
3714080	3721680	out there's a larger distributed cognition also nod to the, to the conversation we're having.
3722640	3728080	Hopefully we're producing positive ripples. I hope so. I mean, the, you know,
3731040	3738080	I'm, I, and I'm hoping that whatever I'm participating in the creation of can also
3739120	3745200	be properly partnered with this project that you're proposing, because I think it's a good one.
3745760	3749920	Nice. Yes. Yes, I would, I would, I think so quite, quite in fact.
3751680	3754480	Thank you, my friend. Yeah, thank you.
