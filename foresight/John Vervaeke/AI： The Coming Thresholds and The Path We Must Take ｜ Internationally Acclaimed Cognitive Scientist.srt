1
00:00:00,000 --> 00:00:02,400
Welcome, everyone.

2
00:00:02,400 --> 00:00:04,280
This is not a Voices with Ferveki.

3
00:00:04,280 --> 00:00:06,320
This is a new entity.

4
00:00:06,320 --> 00:00:09,000
I'm calling a video essay.

5
00:00:09,000 --> 00:00:11,080
But under good advice from the two gentlemen

6
00:00:11,080 --> 00:00:13,640
that are joining me, it was proposed to me.

7
00:00:13,640 --> 00:00:15,640
And I accept the proposal that this

8
00:00:15,640 --> 00:00:19,800
should have a little bit more of a dialogical structure to it.

9
00:00:19,800 --> 00:00:23,880
And given the value of dialogue, as I've

10
00:00:23,880 --> 00:00:26,760
been explaining it in other work,

11
00:00:26,760 --> 00:00:28,080
I took this deeply to heart.

12
00:00:30,080 --> 00:00:32,760
I am going to present still an essay.

13
00:00:32,760 --> 00:00:36,080
And let's remember what Montaigne meant by essay.

14
00:00:36,080 --> 00:00:37,880
S-A-A to try.

15
00:00:37,880 --> 00:00:41,360
I am going to try with the help of these two gentlemen

16
00:00:41,360 --> 00:00:46,960
to bring some clarity to the issue around GPT machines.

17
00:00:46,960 --> 00:00:51,000
The advent of what looks like the first sparks

18
00:00:51,000 --> 00:00:53,840
of artificial general intelligence.

19
00:00:53,840 --> 00:00:55,680
I'm going to make some basic predictions.

20
00:00:55,680 --> 00:01:00,040
And then I'm going to get into the scientific value

21
00:01:00,040 --> 00:01:01,440
of these machines.

22
00:01:01,440 --> 00:01:04,840
And that will be both positive and negative.

23
00:01:04,840 --> 00:01:08,000
The philosophical value, the spiritual value.

24
00:01:08,000 --> 00:01:10,240
And then my proposal, given all of that argument

25
00:01:10,240 --> 00:01:14,040
and discussion about how we can best

26
00:01:14,040 --> 00:01:17,640
respond to undertake the alignment problem.

27
00:01:17,640 --> 00:01:20,240
So first of all, I'm going to ask the two gentlemen,

28
00:01:20,240 --> 00:01:22,840
two friends of mine, two people that I have come

29
00:01:22,840 --> 00:01:29,840
to appreciate, love, and rely on in increasing ways that

30
00:01:29,840 --> 00:01:33,280
has only made my life and my work better.

31
00:01:33,280 --> 00:01:35,600
So let's begin with Ryan.

32
00:01:35,600 --> 00:01:36,400
Well, thanks, John.

33
00:01:36,400 --> 00:01:38,360
And it's good for my heart to hear you say that,

34
00:01:38,360 --> 00:01:40,560
because you have been such a wonderful influence in my life

35
00:01:40,560 --> 00:01:43,040
as a YouTube student of yours and someone

36
00:01:43,040 --> 00:01:45,680
who has experienced a lot of transformation from your work,

37
00:01:45,680 --> 00:01:48,120
which has led me to be the executive director of the Reveque

38
00:01:48,120 --> 00:01:51,840
Foundation, where we are working to help to scale and bring

39
00:01:51,840 --> 00:01:54,000
about these solutions that you have

40
00:01:54,000 --> 00:01:55,760
pointed to so well in your work.

41
00:01:55,760 --> 00:02:00,120
And I also am the founder and run a technology services company

42
00:02:00,120 --> 00:02:01,400
called Mainstay Technologies.

43
00:02:01,400 --> 00:02:04,000
And so technology has been near and dear to my heart

44
00:02:04,000 --> 00:02:05,000
and story forever.

45
00:02:05,000 --> 00:02:09,560
And the intersection of the meaning crisis, the metacrisis,

46
00:02:09,560 --> 00:02:13,880
and technology that is AI has me very fascinated

47
00:02:13,880 --> 00:02:15,760
and been doing a lot of research on this

48
00:02:15,760 --> 00:02:18,760
and eager for what you are about to propose and argue

49
00:02:18,760 --> 00:02:20,920
here today.

50
00:02:20,920 --> 00:02:21,800
Thanks, Ryan.

51
00:02:21,840 --> 00:02:23,040
Eric.

52
00:02:23,040 --> 00:02:26,000
Yes, my name is Eric Foster, media director over here

53
00:02:26,000 --> 00:02:29,960
at the Reveque Foundation, much like Ryan,

54
00:02:29,960 --> 00:02:32,960
knew you through before working alongside you

55
00:02:32,960 --> 00:02:36,440
and continue to not only learn from you,

56
00:02:36,440 --> 00:02:39,440
but learn now alongside you as we work together

57
00:02:39,440 --> 00:02:42,280
on all the different videos and these essays

58
00:02:42,280 --> 00:02:45,280
and the conversations and everything.

59
00:02:45,280 --> 00:02:47,720
My interest in this primarily comes from,

60
00:02:47,720 --> 00:02:49,440
and I actually have told both of you this.

61
00:02:49,440 --> 00:02:50,840
I asked my mom the other day, I said,

62
00:02:50,880 --> 00:02:54,120
when was the first time you heard me really combat AI?

63
00:02:54,120 --> 00:02:55,840
What was the first time I started talking about it?

64
00:02:55,840 --> 00:02:57,880
She said, I think you're about eight years old.

65
00:02:57,880 --> 00:03:01,880
And for some reason, it's been a part of my life

66
00:03:01,880 --> 00:03:03,560
the entire way through.

67
00:03:03,560 --> 00:03:09,400
I'm now 30, so this is going on a long time of very shallowly

68
00:03:09,400 --> 00:03:12,120
but continuously coming back to this idea.

69
00:03:12,120 --> 00:03:14,640
For some reason, it gripped me when I was very young.

70
00:03:14,640 --> 00:03:19,600
And I've been exploring all of the various different avenues

71
00:03:19,600 --> 00:03:21,400
that it could potentially take ever since.

72
00:03:22,600 --> 00:03:23,440
Thank you.

73
00:03:23,440 --> 00:03:24,600
So the format's gonna be the following.

74
00:03:24,600 --> 00:03:28,560
I'm going to go through sort of an argument per section

75
00:03:28,560 --> 00:03:32,000
and then I'll open things up to take comments,

76
00:03:32,000 --> 00:03:35,520
questions from both Ryan and Eric.

77
00:03:35,520 --> 00:03:38,320
So the first thing I wanna do is talk about the predictions.

78
00:03:38,320 --> 00:03:41,400
One of the things that are happening with the GPT machines

79
00:03:41,400 --> 00:03:43,920
is we're getting a swarm of predictions

80
00:03:43,920 --> 00:03:46,440
and many people are finding this challenging

81
00:03:46,440 --> 00:03:49,640
because the predictions are quite varied.

82
00:03:49,640 --> 00:03:51,520
Many of them are inconsistent with each other

83
00:03:51,520 --> 00:03:54,200
or even challenge each other in a fundamental way.

84
00:03:54,200 --> 00:03:56,360
I'm gonna try and propose

85
00:03:56,360 --> 00:04:01,360
that we try to be more careful about the predictions.

86
00:04:02,360 --> 00:04:05,840
We try to steer ground between hyperbolic growth predictions

87
00:04:05,840 --> 00:04:07,040
that these machines are just gonna

88
00:04:07,040 --> 00:04:10,480
hyperbolically accelerate in intelligence.

89
00:04:10,480 --> 00:04:13,920
And that forks into two variations.

90
00:04:13,920 --> 00:04:15,640
Utopia is just around the corner

91
00:04:15,680 --> 00:04:18,760
or we are doomed to doomed, doomed forever doomed.

92
00:04:18,760 --> 00:04:22,200
And so let's be a little bit more cautious.

93
00:04:22,200 --> 00:04:25,160
I'll give you some reasons for that in a minute.

94
00:04:25,160 --> 00:04:29,440
We also want to be steer between all of that,

95
00:04:29,440 --> 00:04:32,680
both the positive and negative hyperbole

96
00:04:32,680 --> 00:04:35,000
and then a kind of stubborn skepticism

97
00:04:35,000 --> 00:04:37,280
that's digging its heels in and saying,

98
00:04:37,280 --> 00:04:39,760
nope, this is not AGI.

99
00:04:39,760 --> 00:04:40,960
It never will be.

100
00:04:40,960 --> 00:04:44,160
This is decades and decades away.

101
00:04:44,160 --> 00:04:46,560
And there are people making these arguments.

102
00:04:46,560 --> 00:04:49,240
And I think that is also incorrect.

103
00:04:49,240 --> 00:04:51,880
I think the attempt for whatever reason

104
00:04:51,880 --> 00:04:56,880
to dismiss these machines is not proportioning

105
00:04:57,240 --> 00:04:59,920
our evaluation to the reality

106
00:04:59,920 --> 00:05:02,280
that they are actually presenting to us.

107
00:05:02,280 --> 00:05:07,280
So what I hope is that we can get much more careful

108
00:05:08,480 --> 00:05:09,880
and that getting more careful

109
00:05:09,880 --> 00:05:12,160
about the predictions we're making

110
00:05:12,160 --> 00:05:15,880
will also in conjunction with the arguments

111
00:05:15,880 --> 00:05:17,320
and discussion we're gonna have,

112
00:05:17,320 --> 00:05:21,720
allow us to, allow us, me maybe specifically,

113
00:05:21,720 --> 00:05:24,200
to propose some important threshold points

114
00:05:24,200 --> 00:05:27,080
that we have not yet met with these machines,

115
00:05:27,960 --> 00:05:31,160
but that we can reasonably foresee,

116
00:05:31,160 --> 00:05:35,280
not perhaps their timing, but why they are pivot points

117
00:05:35,280 --> 00:05:37,240
and that these are points where we can make

118
00:05:37,240 --> 00:05:41,200
fundamental decisions about how we wanna go forward,

119
00:05:41,200 --> 00:05:44,320
especially in terms of the spiritual challenge

120
00:05:44,320 --> 00:05:46,640
and the enlightenment issue.

121
00:05:46,640 --> 00:05:50,800
So why do, why am I skeptical?

122
00:05:50,800 --> 00:05:53,040
Not about the machines.

123
00:05:53,040 --> 00:05:54,360
I'm critical, but not skeptical.

124
00:05:54,360 --> 00:05:57,000
And that distinction is gonna be important throughout.

125
00:05:58,000 --> 00:06:00,860
I am skeptical of jumping to conclusions

126
00:06:00,860 --> 00:06:02,880
about hyperbolic growth,

127
00:06:02,880 --> 00:06:07,240
since human beings are famous for jumping to conclusions

128
00:06:07,240 --> 00:06:08,880
when they see hyperbolic growth.

129
00:06:08,880 --> 00:06:10,400
And if you don't believe me,

130
00:06:10,400 --> 00:06:12,320
just track the history of the stock market

131
00:06:12,320 --> 00:06:13,160
or something like that.

132
00:06:13,160 --> 00:06:15,680
And you'll can see that people can very often

133
00:06:15,680 --> 00:06:17,100
get taken up by it.

134
00:06:18,560 --> 00:06:21,840
What we can say is most often,

135
00:06:21,840 --> 00:06:26,380
hyperbolic growth is found within self-organizing processes.

136
00:06:26,380 --> 00:06:29,480
And when hyperbolic growth is within self-organizing processes

137
00:06:29,480 --> 00:06:31,920
and the economy is a self-organizing process,

138
00:06:33,200 --> 00:06:36,200
it usually is part of a larger pattern

139
00:06:36,200 --> 00:06:38,520
called punctuated equilibrium.

140
00:06:38,520 --> 00:06:40,400
You can see this also in the history of evolution.

141
00:06:40,400 --> 00:06:42,680
There'll be, so after the asteroid hits,

142
00:06:42,680 --> 00:06:46,080
there's just exponential speciation in geological time,

143
00:06:46,080 --> 00:06:48,720
but time scales matter and we'll talk about that later.

144
00:06:48,720 --> 00:06:51,740
And then it flattens off as the niches get filled,

145
00:06:51,740 --> 00:06:53,680
as constraints emerge, as more.

146
00:06:53,680 --> 00:06:56,160
So we don't know yet if this,

147
00:06:56,160 --> 00:06:57,640
like when people just draw these graphs,

148
00:06:57,640 --> 00:07:00,080
look at what's happened over the last five weeks, right?

149
00:07:00,080 --> 00:07:02,440
It's like, yeah.

150
00:07:02,440 --> 00:07:05,040
And then you need to remember we got similar predictions

151
00:07:05,040 --> 00:07:08,440
about self-driving cars and the exponential growth

152
00:07:08,440 --> 00:07:10,880
and soon all these people would be put out of work.

153
00:07:10,880 --> 00:07:15,520
And that was like 2012, we're 11 years later

154
00:07:15,520 --> 00:07:17,240
because we hit a plateau.

155
00:07:17,240 --> 00:07:19,920
There was the exponential growth and we hit a plateau.

156
00:07:19,920 --> 00:07:24,560
Now, I could be wrong, but the reasonable thing

157
00:07:24,560 --> 00:07:26,740
is to be agnostic about the meaning

158
00:07:26,740 --> 00:07:30,960
of this very low resolution measuring of exponential growth.

159
00:07:32,840 --> 00:07:34,380
I mean, here's another example.

160
00:07:34,380 --> 00:07:37,780
Consider if you were at the beginning of the 20th century

161
00:07:37,780 --> 00:07:41,340
and measuring all the breakthroughs in fundamental physics

162
00:07:41,340 --> 00:07:43,300
and you would see this exponential growth,

163
00:07:43,300 --> 00:07:44,700
relativity, quantum mechanics.

164
00:07:45,980 --> 00:07:48,380
And then it plateaus.

165
00:07:48,380 --> 00:07:51,420
It plateaus and it's been 50 and more years,

166
00:07:51,420 --> 00:07:53,980
but since we've had a significant breakthrough.

167
00:07:53,980 --> 00:07:57,460
We don't know, we don't know.

168
00:07:57,460 --> 00:08:01,580
And that is where we should properly stand about this.

169
00:08:01,580 --> 00:08:04,240
So we have to, instead of making predictions

170
00:08:04,240 --> 00:08:06,980
that are not well-warranted,

171
00:08:06,980 --> 00:08:11,980
we should try and foresee plausible threshold points,

172
00:08:12,140 --> 00:08:14,820
not predict necessarily their timing,

173
00:08:14,820 --> 00:08:18,700
but foresee them and foresee them as our opportunities

174
00:08:18,700 --> 00:08:21,660
to steer this in a powerful way.

175
00:08:21,660 --> 00:08:23,740
This is the alternative I'm proposing.

176
00:08:26,360 --> 00:08:29,340
See, as you get into exponential growth,

177
00:08:29,340 --> 00:08:32,260
things are often disclosed that you did not foresee

178
00:08:32,260 --> 00:08:34,340
within your normal framing.

179
00:08:34,340 --> 00:08:36,940
Let me give you one more analogy on this point.

180
00:08:37,940 --> 00:08:39,540
Traveling faster and faster.

181
00:08:41,460 --> 00:08:43,020
Traveling, we can travel faster and faster.

182
00:08:43,020 --> 00:08:44,700
And here's an exponential growth in our ability

183
00:08:44,700 --> 00:08:47,100
to speed through the universe.

184
00:08:47,100 --> 00:08:51,740
Well, as you do, micro particles become relevant

185
00:08:51,740 --> 00:08:54,140
in a way they are never relevant for us

186
00:08:54,140 --> 00:08:55,860
in our daily movement, right?

187
00:08:55,860 --> 00:08:57,940
There's a video online of what happens

188
00:08:57,940 --> 00:09:01,100
if a grain of sand hits the earth at the speed of light

189
00:09:01,100 --> 00:09:03,020
because force equals mass times acceleration.

190
00:09:03,020 --> 00:09:04,620
So it's accelerating to the speed of light

191
00:09:04,620 --> 00:09:07,340
and it hits the earth and you have this titanic explosion.

192
00:09:07,340 --> 00:09:09,020
This is why interstellar travel

193
00:09:09,020 --> 00:09:11,140
might actually be impossible for us,

194
00:09:11,140 --> 00:09:13,500
even if we get machines that accelerate us

195
00:09:13,500 --> 00:09:14,820
towards the speed of light.

196
00:09:14,820 --> 00:09:16,700
Because things that weren't constraints

197
00:09:16,700 --> 00:09:18,900
can suddenly become constraints.

198
00:09:18,900 --> 00:09:22,060
And of course, the speed of light constraint is also there.

199
00:09:22,060 --> 00:09:24,620
For all the fiction around faster than light travel,

200
00:09:24,620 --> 00:09:26,340
it's a real constraint.

201
00:09:26,340 --> 00:09:28,420
Again, this is meant as an analogy.

202
00:09:28,420 --> 00:09:32,820
We don't know what constraints will be revealed.

203
00:09:32,820 --> 00:09:36,380
We don't and simply looking at a simplistic graph

204
00:09:36,380 --> 00:09:40,060
is not taking that into consideration.

205
00:09:40,060 --> 00:09:42,340
We are genuinely ignorant because,

206
00:09:42,340 --> 00:09:44,900
as I will make clear as we go through here,

207
00:09:44,900 --> 00:09:48,060
we really do not know how these machines

208
00:09:48,060 --> 00:09:50,780
are producing these emergent properties

209
00:09:50,780 --> 00:09:52,140
that they're producing.

210
00:09:52,140 --> 00:09:54,660
And therefore, trying to draw something

211
00:09:54,660 --> 00:09:57,220
like scientific predictions from ignorance

212
00:09:57,220 --> 00:09:59,180
of the underlying mechanisms

213
00:09:59,180 --> 00:10:02,740
is a seriously incautious thing to do.

214
00:10:02,740 --> 00:10:04,540
So we have to pull back from that.

215
00:10:04,540 --> 00:10:06,420
Now, I'm not saying we shouldn't try and foresee,

216
00:10:06,420 --> 00:10:08,740
but notice my shift in language.

217
00:10:08,740 --> 00:10:10,140
I'm shifting from prediction.

218
00:10:10,140 --> 00:10:13,340
At this date, this will happen to foreseeing.

219
00:10:13,340 --> 00:10:14,540
What's the foresight?

220
00:10:14,540 --> 00:10:17,060
The foresight is can we foresee,

221
00:10:17,900 --> 00:10:20,740
as we bring real explication to these,

222
00:10:20,740 --> 00:10:23,460
can we foresee threshold points

223
00:10:23,460 --> 00:10:28,140
where we can reasonably make a change?

224
00:10:28,140 --> 00:10:32,500
I think we are in a kairos.

225
00:10:32,500 --> 00:10:35,940
We are in a pivotal turning point in world history.

226
00:10:35,940 --> 00:10:38,620
Maybe one of the greatest, maybe the greatest.

227
00:10:38,620 --> 00:10:40,380
I don't know.

228
00:10:40,380 --> 00:10:41,580
I would need to be godlike

229
00:10:41,580 --> 00:10:43,300
to be able to make that pronouncement.

230
00:10:43,300 --> 00:10:46,020
But unfortunately, I'm not,

231
00:10:46,020 --> 00:10:47,820
which is something I'm also gonna talk about later.

232
00:10:47,820 --> 00:10:48,780
I don't want to be a god.

233
00:10:48,780 --> 00:10:49,980
I hope you don't either.

234
00:10:52,380 --> 00:10:54,060
Imagine having a godlike ability

235
00:10:54,060 --> 00:10:56,860
to remember for all of eternity all your failures.

236
00:10:56,860 --> 00:11:01,860
I don't think that's an existence to be desired.

237
00:11:02,180 --> 00:11:06,620
So, I think we need to be really careful.

238
00:11:06,620 --> 00:11:08,180
I think we need to pay attention

239
00:11:08,180 --> 00:11:10,740
to what we've seen in the history of science.

240
00:11:11,780 --> 00:11:13,740
The past century has not been the century

241
00:11:13,740 --> 00:11:15,540
of unlimited growth and knowledge.

242
00:11:15,540 --> 00:11:18,220
In some ways, yes, more and more data, more and more information,

243
00:11:18,220 --> 00:11:23,100
but you can be misled by just a quantitative approach

244
00:11:23,100 --> 00:11:25,300
because if you pay attention to what's been happening

245
00:11:25,300 --> 00:11:28,060
at a philosophical, epistemological,

246
00:11:28,060 --> 00:11:29,860
having to do with the study of knowledge level,

247
00:11:29,860 --> 00:11:32,580
what you've seen is this has been the century

248
00:11:32,580 --> 00:11:36,740
of the accelerating discovery of intrinsic limits

249
00:11:36,740 --> 00:11:37,940
on what we can know.

250
00:11:38,820 --> 00:11:40,900
The realization that the Cartesian project

251
00:11:40,900 --> 00:11:45,900
of unlimited knowledge is not actually a possibility.

252
00:11:48,020 --> 00:11:50,740
It is reasonable to conclude,

253
00:11:50,740 --> 00:11:52,260
again, I'm not speaking timing here.

254
00:11:52,260 --> 00:11:53,180
I'm talking about trajectory,

255
00:11:53,180 --> 00:11:54,460
but it's reasonable to conclude

256
00:11:54,460 --> 00:11:56,620
that this trajectory will continue

257
00:11:56,620 --> 00:11:58,840
and show itself in this project as well.

258
00:11:59,720 --> 00:12:02,160
We are going to perhaps start to discover

259
00:12:02,160 --> 00:12:06,600
the kinds of fundamental limits on mind

260
00:12:06,600 --> 00:12:09,440
and its interaction with matter

261
00:12:09,440 --> 00:12:11,640
that were not previously available to us

262
00:12:11,640 --> 00:12:14,000
and that will be welcome.

263
00:12:14,000 --> 00:12:17,120
But I think it's unlikely that the machines will just

264
00:12:17,120 --> 00:12:21,120
in some simple exponential pattern grow.

265
00:12:22,680 --> 00:12:26,120
One of the reasons I think this is because

266
00:12:26,120 --> 00:12:28,640
there's an issue of what's called general system

267
00:12:28,840 --> 00:12:31,600
collapse, this comes out of general systems theory.

268
00:12:32,600 --> 00:12:35,120
And the place where we have evidence for this is in

269
00:12:37,680 --> 00:12:41,440
civilizations, which represent very complex intelligence

270
00:12:41,440 --> 00:12:44,360
within sophisticated distributed cognition

271
00:12:44,360 --> 00:12:46,160
that's intergenerational in nature.

272
00:12:46,160 --> 00:12:49,720
So this is a very powerful cognition at work.

273
00:12:49,720 --> 00:12:52,040
I mean, and if you stop and think about it,

274
00:12:52,040 --> 00:12:54,560
the GPT machines are basically just taking

275
00:12:54,560 --> 00:12:57,920
that collective intelligence from distributed cognition

276
00:12:58,080 --> 00:13:02,760
and putting it into a single automated interface for us.

277
00:13:02,760 --> 00:13:07,560
So if you think the GPT machines are very intelligent,

278
00:13:07,560 --> 00:13:09,920
you should think that civilizations are equally

279
00:13:09,920 --> 00:13:12,440
that kind of super human intelligent.

280
00:13:12,440 --> 00:13:14,520
That's a reasonable thing to conclude.

281
00:13:14,520 --> 00:13:16,860
What do we know from the history of these civilizations?

282
00:13:16,860 --> 00:13:19,200
They face general system collapse.

283
00:13:19,200 --> 00:13:20,040
Why?

284
00:13:22,200 --> 00:13:25,120
Let's take it that reality is inexhaustibly complex,

285
00:13:25,160 --> 00:13:28,320
not just complicated, but complex, right?

286
00:13:28,320 --> 00:13:29,800
And it contains real uncertainty,

287
00:13:29,800 --> 00:13:32,000
not just risk real emergence.

288
00:13:32,000 --> 00:13:34,600
And by the way, when you have real emergence,

289
00:13:34,600 --> 00:13:37,000
you have real uncertainty, not just risk.

290
00:13:37,000 --> 00:13:40,120
And all of these people are invoking real emergence

291
00:13:40,120 --> 00:13:42,080
when they're talking about these machines.

292
00:13:42,080 --> 00:13:44,600
So real emergence means real uncertainty.

293
00:13:44,600 --> 00:13:47,360
It means real novelty, okay?

294
00:13:47,360 --> 00:13:52,000
Now, so you place super human civilization intelligence

295
00:13:52,000 --> 00:13:54,000
into a complex environment.

296
00:13:54,000 --> 00:13:55,600
What do you see the system doing?

297
00:13:55,600 --> 00:13:58,160
Becoming more and more complicated,

298
00:13:58,160 --> 00:14:00,000
adding more and more components,

299
00:14:00,000 --> 00:14:04,520
bureaucratizing itself in order to deal with problems.

300
00:14:04,520 --> 00:14:09,200
But what you get to is you get this sort of fact.

301
00:14:10,640 --> 00:14:13,360
As you linearly increase the number of problems

302
00:14:13,360 --> 00:14:15,400
you're trying to deal with,

303
00:14:15,400 --> 00:14:17,800
the number of interactions within your system

304
00:14:17,800 --> 00:14:19,260
is going up exponentially.

305
00:14:20,260 --> 00:14:25,260
So at some point managing yourself becomes as problematic

306
00:14:25,860 --> 00:14:28,140
as any problem you're trying to solve in the environment.

307
00:14:28,140 --> 00:14:31,620
And then the system gets, as it said, top heavy.

308
00:14:31,620 --> 00:14:35,060
It gets over bureaucratized and it collapses.

309
00:14:35,060 --> 00:14:37,180
Now this is a regular pattern,

310
00:14:37,180 --> 00:14:40,620
regular reliable pattern for the super human intelligence

311
00:14:40,620 --> 00:14:42,380
that we find in civilizations.

312
00:14:43,300 --> 00:14:45,780
I do not think it's reasonable to conclude

313
00:14:45,780 --> 00:14:48,820
that these machines will somehow just avoid

314
00:14:48,820 --> 00:14:52,180
that problem of exponential growth

315
00:14:52,180 --> 00:14:57,180
in complicatedness as they try to deal with real problems

316
00:15:01,940 --> 00:15:03,660
that contain real uncertainty

317
00:15:03,660 --> 00:15:06,420
that an inexhaustible environment is presenting to them.

318
00:15:07,620 --> 00:15:08,780
Now, that doesn't mean I can say,

319
00:15:08,780 --> 00:15:10,780
oh, well, they're never gonna surpass us.

320
00:15:10,780 --> 00:15:13,060
John Vervecky is not saying that.

321
00:15:13,060 --> 00:15:14,620
John Vervecky is well aware already

322
00:15:14,620 --> 00:15:16,180
of things that can surpass him.

323
00:15:16,180 --> 00:15:19,180
And like I said, it's very clear

324
00:15:19,180 --> 00:15:22,380
that we have been relying on the super intelligence

325
00:15:22,380 --> 00:15:25,860
of distributed cognition that is distributed

326
00:15:25,860 --> 00:15:29,540
across people and across generations for millennia

327
00:15:29,540 --> 00:15:34,540
as sort of an important source of normative guidance to us.

328
00:15:36,100 --> 00:15:38,020
So these machines, it's not unreasonable

329
00:15:38,020 --> 00:15:39,900
that they could reach that level.

330
00:15:39,900 --> 00:15:41,720
And maybe, and we'll talk about this later,

331
00:15:41,720 --> 00:15:43,980
having a different substrate,

332
00:15:43,980 --> 00:15:45,860
the material they're built on

333
00:15:45,860 --> 00:15:47,980
may allow them to go to different levels.

334
00:15:47,980 --> 00:15:49,180
I do not think, though,

335
00:15:49,180 --> 00:15:54,180
that they can just grow exponentially indefinitely.

336
00:15:54,180 --> 00:15:57,820
I think that is, we have no good reason,

337
00:15:57,820 --> 00:16:01,260
and I'm trying to make arguments here, for believing that.

338
00:16:01,260 --> 00:16:04,260
And that means we can think about these machines,

339
00:16:04,260 --> 00:16:06,820
however godlike they might be,

340
00:16:06,820 --> 00:16:11,100
as being inherently still finite in a manner

341
00:16:11,100 --> 00:16:13,500
that really matters to their cognition

342
00:16:13,500 --> 00:16:15,860
and their attempts to make sense of themselves

343
00:16:15,860 --> 00:16:16,700
and their world.

344
00:16:16,700 --> 00:16:17,940
And that's gonna be an important linchpin

345
00:16:17,940 --> 00:16:19,260
later on in my argument.

346
00:16:21,220 --> 00:16:23,380
What's interesting is there's some evidence

347
00:16:23,380 --> 00:16:28,380
that we are very close to all the trade-off points,

348
00:16:29,580 --> 00:16:31,220
at least for biological beings.

349
00:16:32,140 --> 00:16:34,420
For example, there's all these U curves

350
00:16:34,420 --> 00:16:35,900
for like, if speed of transmission,

351
00:16:35,900 --> 00:16:38,300
if you speed up the speed of neuronal transmission,

352
00:16:38,300 --> 00:16:41,500
we're at sort of the maximal, sort of the optimal,

353
00:16:41,500 --> 00:16:42,380
because if you go too far,

354
00:16:42,380 --> 00:16:44,140
you get into diminishing returns,

355
00:16:44,140 --> 00:16:46,780
and the negative side effects start to manifest

356
00:16:46,780 --> 00:16:49,780
faster than the gains, and also for more neurons.

357
00:16:49,780 --> 00:16:52,220
And so I've seen some really good arguments

358
00:16:52,220 --> 00:16:54,660
that we're sort of peak biology.

359
00:16:56,140 --> 00:16:58,100
And that's very interesting, if you think about it.

360
00:16:58,100 --> 00:17:01,980
That might be, of course, why we resorted to culture,

361
00:17:01,980 --> 00:17:04,540
because culture allows us to supersede

362
00:17:04,540 --> 00:17:06,460
the limits of peak biology.

363
00:17:07,460 --> 00:17:12,460
We teeter on the edge of despair and madness.

364
00:17:13,660 --> 00:17:17,860
And as these machines approach their own threshold,

365
00:17:18,860 --> 00:17:23,860
they will plausibly also teeter on the edge of that.

366
00:17:24,180 --> 00:17:26,580
And that is something we need to think about.

367
00:17:26,580 --> 00:17:28,420
And I'll give you more precise reasons

368
00:17:28,420 --> 00:17:31,180
as to why that is gonna become important.

369
00:17:31,180 --> 00:17:32,420
It has to do with the increase,

370
00:17:32,420 --> 00:17:33,900
we'll have to increasingly,

371
00:17:33,900 --> 00:17:35,900
these machines will have to increasingly rely

372
00:17:36,020 --> 00:17:39,260
on more and more pervasive disruptive strategies.

373
00:17:39,260 --> 00:17:41,100
And so we'll come back to that.

374
00:17:43,980 --> 00:17:48,980
All right, one more thing that I'm going to say about this is,

375
00:17:51,780 --> 00:17:54,140
and I'll go into this more detail, the rationality.

376
00:17:54,140 --> 00:17:55,180
Of course, we have to make,

377
00:17:55,180 --> 00:17:56,780
and this is where the general system things

378
00:17:56,780 --> 00:17:58,020
really starts to bite.

379
00:17:58,020 --> 00:18:00,140
These machines have to become more self-monitoring

380
00:18:00,140 --> 00:18:04,260
and self-directing in very powerful ways.

381
00:18:04,260 --> 00:18:05,420
And then the problem with that

382
00:18:05,420 --> 00:18:08,020
is if you make this system as powerful as this system,

383
00:18:08,020 --> 00:18:10,180
then you get into an infinite regress.

384
00:18:10,180 --> 00:18:13,100
One thing you don't want is a large language model,

385
00:18:14,900 --> 00:18:17,900
making all the hallucinations and repetitive actions

386
00:18:17,900 --> 00:18:21,620
and weirdness, trying to evaluate a lower order LLM

387
00:18:21,620 --> 00:18:24,300
that is making all kinds of hallucinations repeated,

388
00:18:24,300 --> 00:18:26,020
because then you just get an infinite regress.

389
00:18:26,020 --> 00:18:28,220
So you have to properly have this,

390
00:18:28,220 --> 00:18:30,140
the heuristics operating at this level

391
00:18:30,140 --> 00:18:32,580
to be different in kind than this level.

392
00:18:32,580 --> 00:18:37,300
And that also gets you into a diminishing return issue,

393
00:18:37,300 --> 00:18:39,980
because at some point, you don't want this

394
00:18:39,980 --> 00:18:42,340
to become as complex as this.

395
00:18:42,340 --> 00:18:43,740
And think about it already.

396
00:18:44,780 --> 00:18:47,020
I mean, these machines have hundreds of billions

397
00:18:47,020 --> 00:18:48,660
of parameters.

398
00:18:48,660 --> 00:18:49,620
Do you know what it's like to try

399
00:18:49,620 --> 00:18:52,020
and track a hundred billion parameter system?

400
00:18:52,020 --> 00:18:55,620
You know one of the things that this machine

401
00:18:55,620 --> 00:18:58,580
that probably has more than a hundred billion parameters

402
00:18:58,580 --> 00:19:01,780
can't do very well is track a hundred billion parameters.

403
00:19:02,780 --> 00:19:04,420
And so just thinking that,

404
00:19:04,420 --> 00:19:07,260
oh, we can just stack these on top of each other,

405
00:19:07,260 --> 00:19:09,180
I think it's also overly simplistic.

406
00:19:09,180 --> 00:19:11,740
We're facing those real trade-off relations,

407
00:19:11,740 --> 00:19:13,300
there's real problems there.

408
00:19:13,300 --> 00:19:16,980
And again, that means that these machines

409
00:19:16,980 --> 00:19:21,980
are going to be finite in a very important way,

410
00:19:22,540 --> 00:19:25,140
and they will confront presumably the issues

411
00:19:25,140 --> 00:19:28,540
around finitude that will be analogous to ones we have.

412
00:19:28,580 --> 00:19:32,820
Now, I wanna stop here, I'm not finished this section,

413
00:19:32,820 --> 00:19:35,940
but I wanna make clear, all these gaps

414
00:19:35,940 --> 00:19:39,740
in the GPT machines do not take them.

415
00:19:39,740 --> 00:19:42,420
I'm not offering them as any grounds

416
00:19:42,420 --> 00:19:44,740
for dismissive skepticism.

417
00:19:44,740 --> 00:19:48,780
I'm confident that we can approach these limits

418
00:19:48,780 --> 00:19:51,420
and we'll continue to make progress,

419
00:19:51,420 --> 00:19:52,580
and I'll point some of the things

420
00:19:52,580 --> 00:19:53,420
that are already happening.

421
00:19:53,420 --> 00:19:55,860
That's not why I'm doing this.

422
00:19:56,860 --> 00:20:01,860
What I'm doing this for is I want to show

423
00:20:02,860 --> 00:20:06,860
that these machines are not yet fully intelligent.

424
00:20:06,860 --> 00:20:07,860
Nobody really thinks that,

425
00:20:07,860 --> 00:20:09,860
they talk about sparks in the beginnings,

426
00:20:09,860 --> 00:20:13,860
but I wanna unpack that common claim.

427
00:20:13,860 --> 00:20:15,860
It's unlikely that they're currently conscious.

428
00:20:15,860 --> 00:20:19,860
And what that means is we face thresholds

429
00:20:19,860 --> 00:20:24,860
about qualitatively improving, not just quantitatively,

430
00:20:24,860 --> 00:20:26,860
qualitatively improving their intelligence,

431
00:20:26,860 --> 00:20:29,860
possibly making them self-conscious,

432
00:20:31,860 --> 00:20:33,860
rationally reflective, et cetera.

433
00:20:33,860 --> 00:20:35,860
And that's what I'm most interested in.

434
00:20:35,860 --> 00:20:38,860
What are the threshold points we can get to?

435
00:20:38,860 --> 00:20:39,860
How can we make them plausibly?

436
00:20:39,860 --> 00:20:41,860
So if we just give up, oh!

437
00:20:41,860 --> 00:20:44,860
Right, and no, no, no, there's gonna be trade-offs,

438
00:20:44,860 --> 00:20:45,860
there's gonna be limitations,

439
00:20:45,860 --> 00:20:48,860
there's gonna be all kinds of stuff.

440
00:20:48,860 --> 00:20:51,860
And then from that, we can pick off,

441
00:20:52,860 --> 00:20:54,860
okay, here are plausible threshold points,

442
00:20:54,860 --> 00:20:58,860
and then we can more finely tune our response

443
00:20:58,860 --> 00:21:00,860
to the alignment issue.

444
00:21:00,860 --> 00:21:02,860
That's why I'm doing this.

445
00:21:02,860 --> 00:21:05,860
I am very impressed by these machines.

446
00:21:05,860 --> 00:21:08,860
I think it is very reasonable to conclude

447
00:21:08,860 --> 00:21:11,860
they are going to significantly alter,

448
00:21:11,860 --> 00:21:12,860
I said it, they're a chyrus,

449
00:21:12,860 --> 00:21:16,860
they're gonna significantly alter our society

450
00:21:16,860 --> 00:21:17,860
and our sense of self.

451
00:21:18,860 --> 00:21:21,860
They are gonna pour, you know,

452
00:21:22,860 --> 00:21:27,860
meth and, you know, fuel on the fire of the meaning crisis.

453
00:21:28,860 --> 00:21:31,860
And that is something I think we need to take into account.

454
00:21:31,860 --> 00:21:36,860
That will tempt us to respond inappropriately

455
00:21:36,860 --> 00:21:39,860
to what these machines are presenting to us.

456
00:21:39,860 --> 00:21:44,860
And that leads me to some final sort of societal predictions.

457
00:21:45,860 --> 00:21:48,860
I think there's gonna be multiple social responses.

458
00:21:48,860 --> 00:21:49,860
And as I said,

459
00:21:49,860 --> 00:21:52,860
I'm worried about the accelerant of the meaning crisis

460
00:21:52,860 --> 00:21:55,860
tempting us towards inappropriate ones.

461
00:21:56,860 --> 00:22:00,860
So one is nostalgia,

462
00:22:00,860 --> 00:22:04,860
people longing for the time before the machines,

463
00:22:04,860 --> 00:22:09,860
longing passionately and deeply for the golden age

464
00:22:09,860 --> 00:22:11,860
that you did not realize that 10 years ago

465
00:22:11,860 --> 00:22:12,860
you were in the golden age,

466
00:22:12,860 --> 00:22:14,860
but 10 years from now you'll be hearing

467
00:22:14,860 --> 00:22:16,860
you were in the golden age,

468
00:22:16,860 --> 00:22:20,860
that wonderful time when there wasn't GPT

469
00:22:20,860 --> 00:22:24,860
or whatever AGI takes its place.

470
00:22:24,860 --> 00:22:27,860
Nostalgia will grow.

471
00:22:27,860 --> 00:22:31,860
Alongside of it though will be resentment and rage

472
00:22:31,860 --> 00:22:33,860
as people are disenfranchised.

473
00:22:33,860 --> 00:22:35,860
So Louis XIV, what?

474
00:22:35,860 --> 00:22:36,860
Just hang on.

475
00:22:36,860 --> 00:22:40,860
Louis XIV, when he grew up as a young kid,

476
00:22:41,860 --> 00:22:45,860
the nobility staged a coup and he remembered that

477
00:22:45,860 --> 00:22:47,860
and he vowed that when he became king

478
00:22:47,860 --> 00:22:50,860
he would crush the nobility

479
00:22:50,860 --> 00:22:52,860
and become an absolute monarch,

480
00:22:52,860 --> 00:22:54,860
an absolute king, the sun king.

481
00:22:54,860 --> 00:22:57,860
Let's say I am the state.

482
00:22:57,860 --> 00:22:59,860
That's Louis XIV.

483
00:22:59,860 --> 00:23:02,860
In crushing the nobility

484
00:23:02,860 --> 00:23:08,860
he disenfranchised a whole segment of the population

485
00:23:08,860 --> 00:23:12,860
that had traditions of power, traditions of decision,

486
00:23:12,860 --> 00:23:16,860
was highly intelligent because they generally ate better,

487
00:23:16,860 --> 00:23:19,860
highly educated because they had access to education

488
00:23:19,860 --> 00:23:22,860
and they were disenfranchised.

489
00:23:22,860 --> 00:23:24,860
That's a bad idea

490
00:23:24,860 --> 00:23:29,860
because that is the basis for the beginning of revolution.

491
00:23:29,860 --> 00:23:33,860
So I'm saying that now to the people who hold power.

492
00:23:33,860 --> 00:23:36,860
I'm talking to all of you right now.

493
00:23:36,860 --> 00:23:38,860
You who think, aha!

494
00:23:38,860 --> 00:23:41,860
Yes, 95% of the people are going to be driven

495
00:23:41,860 --> 00:23:44,860
but I will become a sun king.

496
00:23:44,860 --> 00:23:47,860
I will be careful.

497
00:23:47,860 --> 00:23:52,860
You are lighting the fires of a revolution in a kairos time

498
00:23:52,860 --> 00:23:55,860
and thinking that you will be protected from those flames.

499
00:23:55,860 --> 00:23:57,860
I think is foolishness.

500
00:24:00,860 --> 00:24:02,860
What else is going to happen?

501
00:24:02,860 --> 00:24:06,860
I think that combination of nostalgia, resentment and rage

502
00:24:06,860 --> 00:24:10,860
will have multiple religious consequences.

503
00:24:10,860 --> 00:24:15,860
One, and religion is going to figure in a lot of what I'm talking about today,

504
00:24:15,860 --> 00:24:20,860
one of those is what you get when you mix nostalgia with resentment and rage.

505
00:24:20,860 --> 00:24:22,860
You get fundamentalism.

506
00:24:22,860 --> 00:24:25,860
Fundamentalisms are going to rise

507
00:24:25,860 --> 00:24:30,860
and they are going to be increasingly apocalyptic fundamentalisms.

508
00:24:30,860 --> 00:24:35,860
Fundamentalism and apocalypse go so nicely together.

509
00:24:35,860 --> 00:24:38,860
They really, oh, apocalypse, fundamentalism.

510
00:24:38,860 --> 00:24:40,860
Oh, I love you. I love you too.

511
00:24:40,860 --> 00:24:42,860
That's what's going to happen.

512
00:24:42,860 --> 00:24:49,860
And so we have to think about how that can shade off into a kind of escapism.

513
00:24:49,860 --> 00:24:51,860
I don't have to worry about this.

514
00:24:51,860 --> 00:24:53,860
God will come.

515
00:24:53,860 --> 00:24:55,860
This is just the antichrist, et cetera.

516
00:24:55,860 --> 00:24:58,860
Now, I'm going to say one thing to my Christian friends

517
00:24:58,860 --> 00:25:01,860
and I want you to take this really seriously.

518
00:25:01,860 --> 00:25:04,860
For those of you who believe in that,

519
00:25:04,860 --> 00:25:06,860
I hope you're right.

520
00:25:06,860 --> 00:25:09,860
I really honestly do.

521
00:25:09,860 --> 00:25:14,860
But I want you to consider the fact that there have been multiple times,

522
00:25:14,860 --> 00:25:17,860
kairases, where God has been silent.

523
00:25:19,860 --> 00:25:23,860
I suspect that is very possible now.

524
00:25:29,860 --> 00:25:33,860
Another thing that's going to happen is cargo cult worship of this AI.

525
00:25:33,860 --> 00:25:35,860
Sorry, I need an apology.

526
00:25:35,860 --> 00:25:37,860
I forget the author of this article I read.

527
00:25:37,860 --> 00:25:40,860
He didn't specifically make this argument, but it overlaps.

528
00:25:40,860 --> 00:25:43,860
And he has definite providence and precedent.

529
00:25:43,860 --> 00:25:45,860
I just forgot his name. I'm sorry.

530
00:25:45,860 --> 00:25:48,860
But he's got an article about how people will probably start worshiping these AIs.

531
00:25:48,860 --> 00:25:50,860
And I think that's the case.

532
00:25:50,860 --> 00:25:53,860
We'll have a cargo cult around these emerging AIs.

533
00:25:53,860 --> 00:25:55,860
What's a cargo cult?

534
00:25:55,860 --> 00:25:59,860
In World War, the Americans flew in to the islands in the Pacific,

535
00:25:59,860 --> 00:26:05,860
all kinds of cargo, all these goods that the indigenous people

536
00:26:05,860 --> 00:26:07,860
found wonderful and amazing.

537
00:26:07,860 --> 00:26:09,860
And this stuff is just landing from the sky.

538
00:26:09,860 --> 00:26:13,860
And then the war was over and the Americans left.

539
00:26:13,860 --> 00:26:18,860
And the indigenous people started building out of wood what looked like airplanes

540
00:26:18,860 --> 00:26:24,860
in building runways because they were trying to get the miraculous airplanes

541
00:26:24,860 --> 00:26:27,860
to return and dispense their wonderful cargo.

542
00:26:27,860 --> 00:26:32,860
So I mean cargo cults around the cargo that these AIs can dispense to us.

543
00:26:32,860 --> 00:26:34,860
I think that's a very reasonable possibility.

544
00:26:34,860 --> 00:26:38,860
And I think that is also a very dangerous path to go down

545
00:26:38,860 --> 00:26:44,860
because that will actually distract us from the hard work that we need to do

546
00:26:44,860 --> 00:26:48,860
in order to properly address the alignment problem.

547
00:26:48,860 --> 00:26:51,860
There's going to be a lot of spiritual bypassing,

548
00:26:51,860 --> 00:26:54,860
which is I am spiritual, it doesn't matter.

549
00:26:54,860 --> 00:26:58,860
A lot of escapism, drugs, pornography, et cetera.

550
00:26:58,860 --> 00:27:03,860
Tragic disillusionment. Tragic disillusionment.

551
00:27:03,860 --> 00:27:08,860
And that's going to exacerbate the meaning crisis.

552
00:27:08,860 --> 00:27:10,860
Then one more thing, and there's a fork here,

553
00:27:10,860 --> 00:27:14,860
and this is around identity politics, left and right.

554
00:27:14,860 --> 00:27:16,860
I'm not taking a side here.

555
00:27:16,860 --> 00:27:18,860
I'm talking about the whole framework.

556
00:27:18,860 --> 00:27:20,860
I think it's identity politics.

557
00:27:20,860 --> 00:27:22,860
One fork will be identity politics is swept away

558
00:27:22,860 --> 00:27:25,860
by the greatest threat to human self-identity that has ever existed,

559
00:27:25,860 --> 00:27:27,860
and it's happening right now.

560
00:27:27,860 --> 00:27:30,860
And all of the differences that we have been promoting as

561
00:27:30,860 --> 00:27:33,860
are going to pale by the fact that we need to get together

562
00:27:33,860 --> 00:27:36,860
and get our shit together if we're going to really address

563
00:27:36,860 --> 00:27:40,860
the real threat to what human identity really means.

564
00:27:40,860 --> 00:27:41,860
That's one fork.

565
00:27:41,860 --> 00:27:44,860
The other fork is people will double down,

566
00:27:44,860 --> 00:27:49,860
double down as we adopt a fundamentalism about identity politics.

567
00:27:49,860 --> 00:27:54,860
I predict that that will be incapable of giving us

568
00:27:54,860 --> 00:27:59,860
any significant guidance about how we reconstruct human identity

569
00:27:59,860 --> 00:28:04,860
and our self-understanding in the face of the advent of AGI.

570
00:28:04,860 --> 00:28:06,860
Okay, so that's the first section.

571
00:28:06,860 --> 00:28:09,860
I want to open myself up to reflections, comments,

572
00:28:09,860 --> 00:28:13,860
challenges, questions, et cetera.

573
00:28:13,860 --> 00:28:18,860
Well, I appreciate, John, how you immediately sort of

574
00:28:18,860 --> 00:28:21,860
helped us break frame of this is not just a technology

575
00:28:21,860 --> 00:28:23,860
that's going to have normal adoption strategies.

576
00:28:23,860 --> 00:28:25,860
This is something fundamentally other.

577
00:28:25,860 --> 00:28:28,860
This is something that is going to have massive disruptive strategies.

578
00:28:28,860 --> 00:28:31,860
We're also moving it off of the quasi-religious grounds

579
00:28:31,860 --> 00:28:33,860
that I hear this talked about so often,

580
00:28:33,860 --> 00:28:37,860
where the singularity has this mythical power to it

581
00:28:37,860 --> 00:28:40,860
that's calling that we will suddenly be able to transcend the laws of physics

582
00:28:40,860 --> 00:28:42,860
and know the answer to everything,

583
00:28:42,860 --> 00:28:44,860
and it will answer whether God is real

584
00:28:44,860 --> 00:28:48,860
and as if we can reach some point where all of that happens

585
00:28:48,860 --> 00:28:52,860
and you're clearly setting finitude around this

586
00:28:52,860 --> 00:28:54,860
while encouraging us to really wrestle

587
00:28:54,860 --> 00:28:57,860
with the rapid acceleration that we're facing.

588
00:28:57,860 --> 00:28:59,860
Yes, I think that is very well said.

589
00:28:59,860 --> 00:29:04,860
I think the danger and sort of the market and the state

590
00:29:04,860 --> 00:29:06,860
are doing exactly the framing that you're pointing to.

591
00:29:06,860 --> 00:29:08,860
Well, this is a technology,

592
00:29:08,860 --> 00:29:11,860
and this is how we better figure out how to use it better

593
00:29:11,860 --> 00:29:12,860
and all that sort of thing.

594
00:29:12,860 --> 00:29:14,860
No, no, no, no, no.

595
00:29:14,860 --> 00:29:16,860
In one sense, it's a technology,

596
00:29:16,860 --> 00:29:22,860
but that is to emphasize the wrong aspect of these entities

597
00:29:22,860 --> 00:29:24,860
in a fundamental way.

598
00:29:24,860 --> 00:29:26,860
Yes, I agree about that.

599
00:29:26,860 --> 00:29:28,860
I think deeply.

600
00:29:28,860 --> 00:29:31,860
That's one point I want to get across very, very clearly.

601
00:29:31,860 --> 00:29:35,860
There's a sense in which even that technological framing

602
00:29:35,860 --> 00:29:38,860
is something we're going to have to challenge more comprehensively

603
00:29:38,860 --> 00:29:41,860
about ourselves and our relationship to the world.

604
00:29:41,860 --> 00:29:44,860
Eric, you wanted to say something.

605
00:29:44,860 --> 00:29:46,860
The thing that stands out to me right away,

606
00:29:46,860 --> 00:29:49,860
and I'm kind of trying to, because it's natural,

607
00:29:49,860 --> 00:29:51,860
but also because I think it's necessary,

608
00:29:51,860 --> 00:29:55,860
I'm trying to take the perspective of whatever audience it is

609
00:29:55,860 --> 00:29:57,860
that is going to be listening to this.

610
00:29:57,860 --> 00:30:00,860
And I really appreciate just the framework

611
00:30:00,860 --> 00:30:02,860
that you're putting around this whole conversation

612
00:30:02,860 --> 00:30:06,860
because I think that there's so much doom and gloom currently already.

613
00:30:06,860 --> 00:30:09,860
There's so much, oh, AGI, it's going to happen eventually.

614
00:30:09,860 --> 00:30:12,860
The current AI chatbots that we have, these language models,

615
00:30:12,860 --> 00:30:16,860
they're so useful, they're going to become our new God.

616
00:30:16,860 --> 00:30:21,860
The way that I make $1,000 a minute suddenly, all of a sudden.

617
00:30:21,860 --> 00:30:25,860
And I think that there's going to be a continued need,

618
00:30:25,860 --> 00:30:28,860
and I think in this instance in particular,

619
00:30:28,860 --> 00:30:32,860
there's an even greater need to force nuanced conversation

620
00:30:32,860 --> 00:30:33,860
around these things.

621
00:30:33,860 --> 00:30:37,860
And so I'm so glad personally, just me as a human being,

622
00:30:37,860 --> 00:30:40,860
I'm so glad that you're putting this much thought into it

623
00:30:40,860 --> 00:30:43,860
in all of the different areas that you are,

624
00:30:43,860 --> 00:30:47,860
and combining multiple domains to not only reach multiple people,

625
00:30:47,860 --> 00:30:51,860
but also to show how big of a potential,

626
00:30:51,860 --> 00:30:55,860
well, just how world-changing that this can potentially be.

627
00:30:55,860 --> 00:31:00,860
Not in a doom and gloom or a cargo cult sort of way.

628
00:31:00,860 --> 00:31:05,860
And I think that as we continue to, as these technologies continue to grow,

629
00:31:05,860 --> 00:31:09,860
the need for that nuance will grow more and more strongly.

630
00:31:09,860 --> 00:31:15,860
So I'm happy personally to see you, in my opinion, leading this nuance.

631
00:31:15,860 --> 00:31:18,860
Well, thank you for that.

632
00:31:18,860 --> 00:31:22,860
So before I go into the scientific value of the GPT machines,

633
00:31:22,860 --> 00:31:26,860
I want to just set a historical context.

634
00:31:26,860 --> 00:31:29,860
And this will, I want people to hold this in the back of their mind

635
00:31:29,860 --> 00:31:33,860
also for the philosophical and spiritual import of these machines.

636
00:31:33,860 --> 00:31:36,860
What's the historical context?

637
00:31:36,860 --> 00:31:39,860
So I'm going to use the word enlightenment, not in the Buddhist sense.

638
00:31:39,860 --> 00:31:41,860
I will use it in the Buddhist sense later.

639
00:31:41,860 --> 00:31:46,860
I'm using it in the historical sense of the period around the scientific revolution,

640
00:31:46,860 --> 00:31:49,860
the reformation, all of that, the enlightenment,

641
00:31:49,860 --> 00:31:55,860
and the degeneration of secular modernity, and all of that.

642
00:31:55,860 --> 00:32:00,860
That era is now coming to an end.

643
00:32:00,860 --> 00:32:07,860
See, that era was premised on some fundamental presuppositions

644
00:32:07,860 --> 00:32:09,860
that drove it and empowered it.

645
00:32:09,860 --> 00:32:10,860
And this is not my point.

646
00:32:10,860 --> 00:32:12,860
This is a point that many people have made.

647
00:32:12,860 --> 00:32:20,860
This sort of Promethean proposal that we are the authors and tea loss of history.

648
00:32:20,860 --> 00:32:25,860
And that's passing away.

649
00:32:25,860 --> 00:32:29,860
And it's done something really odd.

650
00:32:29,860 --> 00:32:36,860
Like, wait, we did all this, made all this progress to come to a place where we will,

651
00:32:36,860 --> 00:32:38,860
technology wouldn't make us into gods.

652
00:32:38,860 --> 00:32:44,860
It will make us the servants or make us destroyed by the emerging gods.

653
00:32:44,860 --> 00:32:45,860
What?

654
00:32:45,860 --> 00:32:47,860
What aren't we the authors?

655
00:32:47,860 --> 00:32:48,860
Aren't we the tea?

656
00:32:48,860 --> 00:32:54,860
Isn't this all about human freedom?

657
00:32:54,860 --> 00:32:57,860
In fact, I think it's not just an ending.

658
00:32:57,860 --> 00:33:01,860
There's a sense in which there's, for me, I don't know how many people share this,

659
00:33:01,860 --> 00:33:03,860
so it's an open invitation.

660
00:33:03,860 --> 00:33:05,860
There's a sense of betrayal here.

661
00:33:05,860 --> 00:33:11,860
I mean, one of the things the enlightenment did was to tell us to stop being tutored

662
00:33:11,860 --> 00:33:12,860
and educated by religion.

663
00:33:12,860 --> 00:33:14,860
It said religion is a private matter.

664
00:33:14,860 --> 00:33:15,860
Go there, do your thing.

665
00:33:15,860 --> 00:33:19,860
But the way you should be educated, brought up, become a citizen, blah, blah, blah,

666
00:33:19,860 --> 00:33:21,860
free from religion.

667
00:33:21,860 --> 00:33:23,860
And of course, there's been all kinds of benefits for that.

668
00:33:23,860 --> 00:33:29,860
But notice the irony here is that one of the things religion taught us how to do,

669
00:33:29,860 --> 00:33:35,860
or what is to, how to enter into relationship to beings that were greater than us.

670
00:33:35,860 --> 00:33:40,860
I mean, Plato didn't have any problem being Plato when he believed that there were gods,

671
00:33:40,860 --> 00:33:46,860
that were like, Socrates clearly thought his wisdom was a paltry thing in comparison to the gods.

672
00:33:46,860 --> 00:33:52,860
Those people knew how to live with beings that were super intelligent.

673
00:33:52,860 --> 00:33:57,860
And how to nevertheless craft human lives of deep meaning within it.

674
00:33:57,860 --> 00:34:00,860
We should pay attention to that example, and I'll come back to it later.

675
00:34:00,860 --> 00:34:07,860
But in losing religion, we lost the place where we learned the manners of dealing with that,

676
00:34:07,860 --> 00:34:10,860
which transcends us, the manners and the virtues.

677
00:34:10,860 --> 00:34:17,860
And that's really odd because the enlightenment has also denuded us of the traditions that might give us some

678
00:34:17,860 --> 00:34:22,860
guidance on how we could think about relating to these machines.

679
00:34:22,860 --> 00:34:27,860
So I think the time of the enlightenment and modernity is coming to an end.

680
00:34:27,860 --> 00:34:29,860
There were already signs of that.

681
00:34:29,860 --> 00:34:35,860
Postmodernity and other things have been already showing that.

682
00:34:35,860 --> 00:34:42,860
But I think this is going to be even more of a severance for us from that.

683
00:34:42,860 --> 00:34:43,860
And that's very important.

684
00:34:43,860 --> 00:34:46,860
So please keep that context in mind.

685
00:34:46,860 --> 00:34:47,860
All right.

686
00:34:47,860 --> 00:34:50,860
The scientific value of the GPT machines.

687
00:34:50,860 --> 00:34:53,860
This is going to be, and when I say value, I mean both positive and negative.

688
00:34:53,860 --> 00:35:00,860
I'm using value as an unmasked term.

689
00:35:00,860 --> 00:35:04,860
So positive value, scientific value.

690
00:35:04,860 --> 00:35:08,860
I think this is the beginning of a real solution to the silo problem.

691
00:35:08,860 --> 00:35:10,860
And that's a scientific advance.

692
00:35:10,860 --> 00:35:11,860
What's a silo problem?

693
00:35:11,860 --> 00:35:18,860
Another problem is that our deep learning machines, our neural networks, have typically been single domain, single problem solvers.

694
00:35:18,860 --> 00:35:21,860
This machine is really good at playing go.

695
00:35:21,860 --> 00:35:22,860
Can it swim?

696
00:35:22,860 --> 00:35:23,860
No.

697
00:35:23,860 --> 00:35:24,860
Right?

698
00:35:24,860 --> 00:35:26,860
And we were very different from them.

699
00:35:26,860 --> 00:35:37,860
And that's an important difference because I think AGI doesn't make any sense unless it's in a general problem solver that can solve many problems in many domains.

700
00:35:37,860 --> 00:35:40,860
And what's being opened up by the machines is the real possibility of this.

701
00:35:41,860 --> 00:35:48,860
I'm not saying it's a complete solution, but we have clear evidence that the silo problem is being solved.

702
00:35:48,860 --> 00:35:57,860
Now, what's really interesting about that, and this is an argument that myself and other people are making, is we're basically saying that we need hybrid machines.

703
00:35:57,860 --> 00:36:03,860
We need sort of neural networks doing the deep learning, and then we need something that's a very language of thought.

704
00:36:03,860 --> 00:36:07,860
This is what these large language models are.

705
00:36:07,860 --> 00:36:09,860
And we're doing even more.

706
00:36:09,860 --> 00:36:15,860
If the hugging things comes through, we've got this kind of AI and this kind of AI, and we're clutching them all together.

707
00:36:15,860 --> 00:36:21,860
And it turns out that that's a confirmation of a lot of prediction and argumentation.

708
00:36:21,860 --> 00:36:28,860
Some of that I made that there are different strengths and weaknesses between language-like processing and non-language-like processing.

709
00:36:28,860 --> 00:36:30,860
This is a theme of my work.

710
00:36:30,860 --> 00:36:37,860
And that these machines are showing, well, we actually have to address both if we want to create general intelligence.

711
00:36:37,860 --> 00:36:43,860
And I think that's a big admission for multiple kinds of knowing.

712
00:36:43,860 --> 00:36:49,860
I think that's taking it that way, I think, is a very reasonable conclusion to draw.

713
00:36:51,860 --> 00:36:58,860
Now, what these machines do demonstrate is the insufficiency of purely propositional knowing.

714
00:36:58,860 --> 00:37:03,860
And for those of you who don't know the kinds of knowing I'm going to talk about, I can't go into it in great detail.

715
00:37:03,860 --> 00:37:07,860
We're going to put some links to some of the things here so you can go to it in detail.

716
00:37:07,860 --> 00:37:14,860
But I want to give you one clear example of this that I don't think is in any way controversial.

717
00:37:14,860 --> 00:37:19,860
So you can ask GPT-4 to spit out ethical theory for you.

718
00:37:19,860 --> 00:37:22,860
What's utilitarianism? What's the ontological ethics?

719
00:37:23,860 --> 00:37:27,860
Can you make an argument against Singer's argument for utilitarianism?

720
00:37:27,860 --> 00:37:30,860
Oh, here's a counterargument. Very good, very good, very good.

721
00:37:30,860 --> 00:37:35,860
All the propositional expertise that Leibniz had wet dreams about.

722
00:37:35,860 --> 00:37:39,860
Oh, and you know what? That doesn't make these machines.

723
00:37:39,860 --> 00:37:42,860
One iota, moral agents.

724
00:37:45,860 --> 00:37:51,860
Think about that. Think about what we now have evidence for, right?

725
00:37:53,860 --> 00:38:00,860
This shows the radical insufficiency of propositional knowing for personhood.

726
00:38:00,860 --> 00:38:05,860
If we take it that a proper part of being a person is moral agency.

727
00:38:05,860 --> 00:38:08,860
It's lacking that.

728
00:38:11,860 --> 00:38:19,860
So, one way of thinking about this that may be helpful is a distinction from in psychology.

729
00:38:19,860 --> 00:38:22,860
A distinction that I'm actually critical about, but it's helpful.

730
00:38:22,860 --> 00:38:25,860
There's a distinction between, when people do work on intelligence,

731
00:38:25,860 --> 00:38:30,860
there's a distinction between crystallized intelligence and fluid intelligence.

732
00:38:30,860 --> 00:38:34,860
So crystallized intelligence is you're knowing how to use your knowledge.

733
00:38:34,860 --> 00:38:38,860
And this can have highly powerful and emergent properties,

734
00:38:38,860 --> 00:38:42,860
because you can connect things that you know in new and powerful ways.

735
00:38:42,860 --> 00:38:48,860
And I think it's abundantly clear that there's a lot of emergent crystal intelligence,

736
00:38:48,860 --> 00:38:51,860
crystallized intelligence in this machine.

737
00:38:51,860 --> 00:38:55,860
It probably doesn't have what's called fluid intelligence,

738
00:38:55,860 --> 00:39:02,860
because fluid intelligence has to do a lot with attention, working memory, consciousness,

739
00:39:02,860 --> 00:39:08,860
and your ability to sort of dynamically couple well to your environment.

740
00:39:10,860 --> 00:39:13,860
I think I'm going to argue this a little bit more detail,

741
00:39:13,860 --> 00:39:18,860
but I think it's unreasonable to conclude right now that these machines have perspectival knowing.

742
00:39:18,860 --> 00:39:19,860
Now, let's be careful about this.

743
00:39:19,860 --> 00:39:22,860
Can they generate all kinds of propositional theory about perspectives?

744
00:39:22,860 --> 00:39:23,860
You bet.

745
00:39:23,860 --> 00:39:27,860
They have crystallized intelligence, but keep the analogy to moral reasoning.

746
00:39:27,860 --> 00:39:31,860
That's not the same thing as being able to take up a perspective,

747
00:39:31,860 --> 00:39:37,860
have genuine salience landscaping, and bind yourself to it in a very powerful way.

748
00:39:37,860 --> 00:39:41,860
And I'll come back for why I think the machines currently lack that.

749
00:39:41,860 --> 00:39:43,860
Remember the word currently.

750
00:39:47,860 --> 00:39:51,860
Another thing that has been a significant scientific benefit.

751
00:39:51,860 --> 00:39:55,860
And because I'm talking about science, I'm sort of saying predictions that I made,

752
00:39:55,860 --> 00:39:57,860
because that's what scientists are supposed to do.

753
00:39:57,860 --> 00:39:58,860
It sounds self-promotional.

754
00:39:58,860 --> 00:40:00,860
And to some degree, perhaps it is.

755
00:40:00,860 --> 00:40:04,860
Perhaps I'm within all the nestings of my thinking.

756
00:40:04,860 --> 00:40:08,860
John Vervecchi is saying, but I'll remain important even when these machines are gone.

757
00:40:08,860 --> 00:40:09,860
Maybe that's happening.

758
00:40:09,860 --> 00:40:10,860
I hope not.

759
00:40:10,860 --> 00:40:12,860
So I hope the argument stands on its own.

760
00:40:12,860 --> 00:40:22,860
This is clear evidence, and I have argued for this, that generating intelligence does

761
00:40:22,860 --> 00:40:26,860
not guarantee that you will generate rationality.

762
00:40:26,860 --> 00:40:33,860
In fact, what is very possible is as you increase intelligence, you will increase its capacity

763
00:40:33,860 --> 00:40:34,860
for self-deception.

764
00:40:34,860 --> 00:40:37,860
And we're seeing this in these machines in space.

765
00:40:37,860 --> 00:40:43,860
The hallucinations, the confabulation, the lying and not caring that it lies.

766
00:40:43,860 --> 00:40:45,860
All of this.

767
00:40:45,860 --> 00:40:49,860
And note that there's been a couple of people that have pointed out as they've tried to put in safeguards

768
00:40:49,860 --> 00:40:54,860
to limit the hallucinations, the speed has actually, the machine has actually slowed down

769
00:40:54,860 --> 00:41:02,860
compared to 3.5, hinting that some of those trade-offs might actually already be coming in place.

770
00:41:02,860 --> 00:41:04,860
Don't know, but these are empirical questions.

771
00:41:04,860 --> 00:41:11,860
So let's pay attention to the empirical evidence as it unrolls and try to calibrate what we're

772
00:41:11,860 --> 00:41:13,860
saying as closely as we can.

773
00:41:13,860 --> 00:41:19,860
But it's very clear that these machines, just by making them more intelligent, will not make

774
00:41:19,860 --> 00:41:22,860
them more rational.

775
00:41:22,860 --> 00:41:26,860
And we had every good reason to believe that because that's the case for us.

776
00:41:26,860 --> 00:41:33,860
There is no contradiction in human beings being highly intelligent and highly irrational.

777
00:41:33,860 --> 00:41:37,860
The predictive relationship between our best measures of intelligence and our best measures

778
00:41:37,860 --> 00:41:39,860
of rationality is 0.3.

779
00:41:39,860 --> 00:41:43,860
Now, 0.3 is not nothing, but it's not one.

780
00:41:43,860 --> 00:41:44,860
It's not one.

781
00:41:44,860 --> 00:41:51,860
70% of the variance is outside of intelligence.

782
00:41:51,860 --> 00:41:53,860
And so we're seeing that in these machines.

783
00:41:53,860 --> 00:41:54,860
Now, why are you saying that?

784
00:41:54,860 --> 00:42:00,860
Because that's going to be the basis for my philosophical argument, which is about rationality.

785
00:42:00,860 --> 00:42:01,860
Rationality.

786
00:42:01,860 --> 00:42:02,860
All right.

787
00:42:02,860 --> 00:42:06,860
Now, a question that a lot of you have sort of posed to me is, but what does this say

788
00:42:06,860 --> 00:42:08,860
about relevance realization?

789
00:42:08,860 --> 00:42:13,860
Again, I'm not going to repeat everything I've said about relevance realization.

790
00:42:13,860 --> 00:42:15,860
We'll put links to publications.

791
00:42:15,860 --> 00:42:17,860
We'll put links to videos.

792
00:42:17,860 --> 00:42:18,860
This is there.

793
00:42:18,860 --> 00:42:24,860
The basic idea here is the general ability of general intelligence is the ability to zero

794
00:42:24,860 --> 00:42:30,860
in on relevant information, ignore irrelevant information and do that in an evolving, self-correcting

795
00:42:30,860 --> 00:42:31,860
manner.

796
00:42:31,860 --> 00:42:34,860
And like I say, I'm not going to try and justify that claim right now.

797
00:42:34,860 --> 00:42:36,860
I have a lot out there.

798
00:42:36,860 --> 00:42:42,860
And I invite people to take a look at it.

799
00:42:42,860 --> 00:42:43,860
What is this?

800
00:42:43,860 --> 00:42:46,860
What do these machines say about that theory?

801
00:42:46,860 --> 00:42:53,860
Well, one thing we know, and look at the 2012 paper with Tim Wildercrop and Blake Richards,

802
00:42:53,860 --> 00:42:59,860
we pointed out something that was emerging in deep learning about relevance realization.

803
00:42:59,860 --> 00:43:06,860
And therefore, there is an important dimension of relevance realization, recursive relevance

804
00:43:06,860 --> 00:43:11,860
realization that is being massively instantiated in these machines, which is the compression

805
00:43:11,860 --> 00:43:16,860
particularization function of deep learning and doing it in this multiply recursive fashion.

806
00:43:16,860 --> 00:43:22,860
And so, of course, that dimension of relevance realization is going to be important.

807
00:43:22,860 --> 00:43:28,860
And because of its recursivity, we're going to see it have emergent aspects to it.

808
00:43:28,860 --> 00:43:33,860
So I think this is actually a significant confirmation.

809
00:43:33,860 --> 00:43:40,860
More interestingly, at the end of last year with Brett Anderson and Mark Miller published

810
00:43:40,860 --> 00:43:46,860
a paper talking about the integration of relevance realization and predictive processing as the

811
00:43:46,860 --> 00:43:48,860
best way to get intelligence.

812
00:43:48,860 --> 00:43:53,860
Now, interestingly enough, these machines show that because they have deep learning running

813
00:43:53,860 --> 00:43:57,860
that dimension of relevance realization and then they have predictive process.

814
00:43:57,860 --> 00:44:01,860
What the LLM models are, they are predictive processing machines.

815
00:44:01,860 --> 00:44:03,860
That's exactly what they are.

816
00:44:03,860 --> 00:44:06,860
Now, the problem is that they're limited.

817
00:44:06,860 --> 00:44:11,860
They are predicting the relationship between lexical items.

818
00:44:11,860 --> 00:44:14,860
And broadening that is probably going to be a challenge.

819
00:44:14,860 --> 00:44:23,860
So although there is an integration of a dimension of RR and a specific version of PP that predicts

820
00:44:23,860 --> 00:44:27,860
very powerful intelligence, there's also inherent limitations.

821
00:44:27,860 --> 00:44:31,860
Still, what are those?

822
00:44:31,860 --> 00:44:35,860
There's a lot of dimensions of relevance realization that are outside the compression

823
00:44:35,860 --> 00:44:41,860
particularization processing that are deep learning that are probably also going to be

824
00:44:41,860 --> 00:44:43,860
needed for genuine intelligence.

825
00:44:43,860 --> 00:44:45,860
There's Explore, Exploit trade-offs.

826
00:44:45,860 --> 00:44:50,860
There are trade-offs between monitoring your cognition and tasking in the world.

827
00:44:50,860 --> 00:44:52,860
You don't go through all of these.

828
00:44:52,860 --> 00:44:54,860
And they're always trade-off relationships.

829
00:44:54,860 --> 00:44:58,860
You're always trading between them because as you advance one, you lose on the other.

830
00:44:58,860 --> 00:45:01,860
And you're always pulling between them.

831
00:45:01,860 --> 00:45:06,860
There's deep trade-offs between trying to make your processes more efficient and make them more resilient

832
00:45:06,860 --> 00:45:09,860
so they have a kind of adaptive responsiveness to the environment.

833
00:45:09,860 --> 00:45:12,860
All of this is still not in these machines.

834
00:45:12,860 --> 00:45:14,860
Does that mean we can't put it in the machines?

835
00:45:14,860 --> 00:45:15,860
No.

836
00:45:15,860 --> 00:45:17,860
Unfortunately, that work is already out there and published.

837
00:45:17,860 --> 00:45:20,860
That possibility is there.

838
00:45:20,860 --> 00:45:25,860
However, I think most, no, I don't want to put a quantitative word.

839
00:45:25,860 --> 00:45:29,860
Some significant dimensions of relevance realization are missing.

840
00:45:29,860 --> 00:45:34,860
And having a generalized form of predictive processing is genuinely missing.

841
00:45:34,860 --> 00:45:44,860
So that's my reason for saying that there's not a lot at this point that's

842
00:45:44,860 --> 00:45:46,860
new about, theoretically new.

843
00:45:46,860 --> 00:45:49,860
This is not a scientific advance.

844
00:45:49,860 --> 00:45:55,860
These ideas were largely already pre-existent and in the literature.

845
00:45:55,860 --> 00:45:58,860
Now, again, that doesn't mean, oh, well, then we're not going to pay attention to that.

846
00:45:58,860 --> 00:46:02,860
That's not what I'm saying.

847
00:46:02,860 --> 00:46:06,860
But let's push on this point.

848
00:46:06,860 --> 00:46:13,860
I want you to notice how much these machines presuppose relevance realization

849
00:46:13,860 --> 00:46:15,860
rather than explaining it.

850
00:46:15,860 --> 00:46:17,860
What do you mean, John?

851
00:46:17,860 --> 00:46:22,860
Well, these machines rely on the fact that we have encoded into the statistical probability

852
00:46:22,860 --> 00:46:25,860
between terms epistemic relations of relevance.

853
00:46:25,860 --> 00:46:28,860
We don't generate text like randomly.

854
00:46:28,860 --> 00:46:33,860
What have we figured out with literacy and previously with languages?

855
00:46:33,860 --> 00:46:37,860
Wait, you know these epistemic relations of relevance between my ideas?

856
00:46:37,860 --> 00:46:40,860
I can encode them in probabilistic relationships between terms.

857
00:46:40,860 --> 00:46:42,860
And then we can get really good at it.

858
00:46:42,860 --> 00:46:46,860
First, we have hundreds of thousands of years of evolving language and then we have all of this

859
00:46:46,860 --> 00:46:52,860
civilizational level work on literacy to get that correlation between relevance,

860
00:46:52,860 --> 00:46:56,860
let's call it epistemic relevance and merely statistical relevance,

861
00:46:56,860 --> 00:46:59,860
to get that correlation really tight.

862
00:46:59,860 --> 00:47:04,860
That's different than a chimpanzee moving around in a forest.

863
00:47:04,860 --> 00:47:07,860
Really different, right?

864
00:47:07,860 --> 00:47:14,860
So first, we, it presupposes, it relies upon us encoding that relevance realization.

865
00:47:14,860 --> 00:47:20,860
Secondly, it relies on us encoding relevance realization and how we curate and create databases,

866
00:47:20,860 --> 00:47:26,860
how we create labeled images for the new visual processing that's coming on.

867
00:47:26,860 --> 00:47:32,860
And how we organize access to that knowledge on the internet.

868
00:47:32,860 --> 00:47:34,860
The internet is not random.

869
00:47:34,860 --> 00:47:42,860
It's organized as a multi-layered small world network because it's organized by human attention

870
00:47:42,860 --> 00:47:45,860
and what human beings find relevant and salient.

871
00:47:45,860 --> 00:47:50,860
And then finally, and don't put too much on this but also don't ignore it,

872
00:47:50,860 --> 00:47:55,860
the reinforcement learning that is driving this is modified, it's human assisted.

873
00:47:55,860 --> 00:48:00,860
Human beings are in the loop making important judgments that help fine tune.

874
00:48:00,860 --> 00:48:02,860
They're filled under fine tuning, right?

875
00:48:02,860 --> 00:48:05,860
The judgments of relevance of this machine.

876
00:48:05,860 --> 00:48:06,860
Now what does that mean?

877
00:48:06,860 --> 00:48:07,860
You say, so what John?

878
00:48:07,860 --> 00:48:09,860
The machines can still do that.

879
00:48:09,860 --> 00:48:11,860
I'm not denying the technological success.

880
00:48:11,860 --> 00:48:19,860
What I'm saying is the degree to which they are presupposing relevance realization

881
00:48:19,860 --> 00:48:23,860
is the degree to which they are not explanatory of it.

882
00:48:23,860 --> 00:48:28,860
This is not an explanation of intelligence.

883
00:48:28,860 --> 00:48:31,860
This is not.

884
00:48:31,860 --> 00:48:32,860
It won't generalize.

885
00:48:32,860 --> 00:48:40,860
As I mentioned, what's happening in this machine doesn't generalize to the chimpanzee at all.

886
00:48:40,860 --> 00:48:47,860
It might not even generalize to us.

887
00:48:47,860 --> 00:48:48,860
Why?

888
00:48:48,860 --> 00:48:49,860
Why do I say that?

889
00:48:49,860 --> 00:48:51,860
Because it's weird.

890
00:48:52,860 --> 00:49:00,860
Is it Stuart Russell recently released a thing about several generations beyond AlphaGo.

891
00:49:00,860 --> 00:49:03,860
It's not GPT, but it's the same deep learning process.

892
00:49:03,860 --> 00:49:06,860
You had AlphaGo that could beat any human go master.

893
00:49:06,860 --> 00:49:10,860
And then generations beyond that, so like levels above.

894
00:49:10,860 --> 00:49:13,860
And then the human beings noticed.

895
00:49:13,860 --> 00:49:14,860
They just noticed.

896
00:49:15,860 --> 00:49:25,860
They noticed that there's pretty clear evidence the machine didn't have the concept of a group,

897
00:49:25,860 --> 00:49:27,860
like this basic idea of a group of stones.

898
00:49:27,860 --> 00:49:29,860
And then they said, OK, that's the case.

899
00:49:29,860 --> 00:49:33,860
Here's a very simple strategy you could use to beat any of these machines.

900
00:49:33,860 --> 00:49:41,860
They took a middle range go player, gave them the strategy, gave the machine a nine stone advantage,

901
00:49:41,860 --> 00:49:46,860
and the human being regularly and reliably beat the go machine.

902
00:49:46,860 --> 00:49:49,860
Now, what some of you are saying, oh, well, we'll figure out how to fix it.

903
00:49:49,860 --> 00:49:50,860
Yes, she will.

904
00:49:50,860 --> 00:49:51,860
I don't doubt that.

905
00:49:51,860 --> 00:49:52,860
But that's not the point.

906
00:49:52,860 --> 00:49:56,860
The point is the machine didn't do that self correction.

907
00:49:56,860 --> 00:49:59,860
There's a lot missing.

908
00:49:59,860 --> 00:50:02,860
Other weirdness, like the, oh, the visual recognition.

909
00:50:02,860 --> 00:50:04,860
Well, there's been a problem.

910
00:50:04,860 --> 00:50:08,860
We get these visual recognition machines and, oh, well, that's a dog.

911
00:50:08,860 --> 00:50:09,860
That's an elephant.

912
00:50:09,860 --> 00:50:11,860
That's a man sitting on a picnic table.

913
00:50:11,860 --> 00:50:12,860
Wow.

914
00:50:12,860 --> 00:50:14,860
Wow, that's, whoa, human level.

915
00:50:14,860 --> 00:50:20,860
OK, now what I'm going to do is take the same picture and scatter in some insignificant perceptual noise.

916
00:50:20,860 --> 00:50:22,860
Human beings won't even notice it.

917
00:50:22,860 --> 00:50:25,860
Alter a fraction of the pixels.

918
00:50:25,860 --> 00:50:32,860
Then the machine, you show it the picture of the picnic table and the man and they say, oh, that's an iceberg.

919
00:50:32,860 --> 00:50:38,860
You get this weird freaky thing that comes out of that.

920
00:50:38,860 --> 00:50:44,860
Another thing, this is one of the most strongest results, reliable results about human intelligence.

921
00:50:44,860 --> 00:50:46,860
It forms a positive manifold.

922
00:50:46,860 --> 00:50:48,860
This is what Spearman discovered.

923
00:50:48,860 --> 00:50:55,860
This is how we came up with the idea of general intelligence, namely how you do on this task, how you do in art.

924
00:50:55,860 --> 00:50:59,860
Contrary to what people is highly predictive of how you'll do in math and how you'll do in history.

925
00:50:59,860 --> 00:51:00,860
Like that's what he found.

926
00:51:00,860 --> 00:51:04,860
How you do on any of these different tasks is highly predictive of each other.

927
00:51:04,860 --> 00:51:06,860
You form a positive manifold.

928
00:51:06,860 --> 00:51:08,860
That's your general intelligence.

929
00:51:08,860 --> 00:51:10,860
That points to a central underlying ability.

930
00:51:10,860 --> 00:51:13,860
I happen to argue that it's relevance realization.

931
00:51:13,860 --> 00:51:17,860
But putting that aside, notice this.

932
00:51:17,860 --> 00:51:22,860
So GPT-4 can score in the 10th percentile of the Harvard Law exam.

933
00:51:22,860 --> 00:51:25,860
Whoa, that's really high IQ.

934
00:51:25,860 --> 00:51:35,860
But then, and I didn't do this, somebody gave my most recent talk at the Consilience Conference to GPT-4 and asked it to summarize and evaluate.

935
00:51:36,860 --> 00:51:44,860
And then I also gave this to an academic colleague of mine to evaluate GPT-4's response so I made sure it wasn't just me.

936
00:51:44,860 --> 00:51:48,860
And it's about grade 11 as an answer.

937
00:51:48,860 --> 00:51:52,860
And it's like, why is it brilliant?

938
00:51:52,860 --> 00:51:57,860
For human beings, there would be a very strong positive manifold between those.

939
00:51:57,860 --> 00:52:01,860
But there's a lot of heterogeneity in this machine.

940
00:52:01,860 --> 00:52:06,860
So it clearly doesn't generalize to the chimp.

941
00:52:06,860 --> 00:52:14,860
It may not generalize to us, which means it suffers from the kind of failure that destroys any good scientific theory, which is it fails to generalize.

942
00:52:14,860 --> 00:52:19,860
I don't think a good scientific theory is available to us because of this.

943
00:52:19,860 --> 00:52:23,860
Now, you may say, but what about, I'm going to come to the philosophical and spiritual significance.

944
00:52:23,860 --> 00:52:28,860
Right now, we're playing the science domain and I'm trying to answer the science questions.

945
00:52:28,860 --> 00:52:34,860
We need to get clear about these and not mix them up together and confound them and run back and forth in an equivocal manner.

946
00:52:34,860 --> 00:52:43,860
Let's be clear about each one and then put them back together very, very carefully, very, very carefully.

947
00:52:43,860 --> 00:52:55,860
Because I think, and I won't go into detail for this explanation, because I think these machines don't really have recursive relevance realization in a deep enough way.

948
00:52:55,860 --> 00:53:10,860
And because I think there's a lot of converging arguments that the function of the four fold of attention and working memory and fluid intelligence and consciousness is relevance realization in ill defined novel complex situations.

949
00:53:10,860 --> 00:53:13,860
I think it's highly unlikely that these machines have consciousness.

950
00:53:13,860 --> 00:53:21,860
I think the fact that they have no reflective abilities means it's virtually the case that they do not have self consciousness.

951
00:53:21,860 --> 00:53:28,860
I think about, you know, how the machine is thinking like that is like how the machine is feeling or something like that.

952
00:53:28,860 --> 00:53:32,860
I think that is premature.

953
00:53:32,860 --> 00:53:36,860
Is it possible that we could get there? Yes, it is. It is.

954
00:53:36,860 --> 00:53:40,860
But you see what's happening by getting clear about what has happened scientifically.

955
00:53:40,860 --> 00:53:50,860
We can start to see what are the future threshold points that we will be confronting and how can we be foresightful about them.

956
00:53:50,860 --> 00:53:52,860
One more thing.

957
00:53:52,860 --> 00:53:56,860
And people are endlessly arguing about the Chinese room.

958
00:53:56,860 --> 00:54:02,860
And since they were endlessly arguing about the Chinese room argument, I won't go into it. And if you don't know about it, don't worry about it.

959
00:54:02,860 --> 00:54:09,860
Before the GPT machines, I don't think this argument is going to satisfy anybody.

960
00:54:09,860 --> 00:54:15,860
I have taken a look at the best attempts to give a naturalistic account of semantic information.

961
00:54:15,860 --> 00:54:18,860
And we need a scientific distinction here.

962
00:54:18,860 --> 00:54:23,860
There's a distinction between technical information, which is what is involved in what's called information theory.

963
00:54:23,860 --> 00:54:30,860
And all that basically is, is a relation of statistical relevance that rules out alternatives.

964
00:54:30,860 --> 00:54:32,860
That's the Shannon and Weaver notion.

965
00:54:32,860 --> 00:54:37,860
So in that sense, without there being any sentient beings, there is a ton of information in this table.

966
00:54:37,860 --> 00:54:42,860
Because there are statistical relevance relations that are ruling out counterfactuals all over.

967
00:54:42,860 --> 00:54:46,860
So semantic information is what we normally mean by information.

968
00:54:46,860 --> 00:54:53,860
It means something is meaningful to it. We understand it. We form ideas about it.

969
00:54:53,860 --> 00:54:58,860
Now, I think it's fair to say that most people in the business take this to be a real and important difference.

970
00:54:58,860 --> 00:55:02,860
And Shannon certainly did when he proposed the theory.

971
00:55:02,860 --> 00:55:13,860
So let's take this difference as a plausible difference. And then we can say, well, do these machines have semantic information.

972
00:55:13,860 --> 00:55:20,860
And one of the best papers out there right now on this is by Colchinsky and Woolport.

973
00:55:21,860 --> 00:55:30,860
Notice what's in the title. Autonomous Agency and Non-Equilibrium Statistic Physics from 2018. We'll put the link in here.

974
00:55:30,860 --> 00:55:40,860
What do they argue? They argue that technical information becomes semantic information when that technical information is causally necessary for the system to maintain its own existence.

975
00:55:40,860 --> 00:55:54,860
That's why they put autonomy and agency in the title. What they are basically saying here is that meaning is meaning to an auto-poietic system, a system that is making itself.

976
00:55:54,860 --> 00:56:01,860
Now, this converges completely with the argument I've made that relevance, nothing is intrinsically relevant.

977
00:56:01,860 --> 00:56:08,860
Things are relevant to something that cares about this information rather than that information.

978
00:56:08,860 --> 00:56:21,860
Why would it care about this information rather than that information? Because it's taking care of itself, because it's making itself, because it's an autonomous auto-poietic agent.

979
00:56:21,860 --> 00:56:30,860
And to the degree to which these machines aren't auto-poetic is the degree to which they really do not have needs. They really cannot care.

980
00:56:31,860 --> 00:56:49,860
They can pantomime, maybe to perfection, are caring. And you can do so much with a pantomime, but pantomime caring isn't really caring. It is ultimately pretentious.

981
00:56:49,860 --> 00:57:06,860
So meaning is about this connectedness. I like to use the Latin term religio to the environment, and you only get that if you're an auto-poietic autonomous agent.

982
00:57:06,860 --> 00:57:18,860
And of course, these machines are not. Now, just to foreshadow, that tells us, wait, here's a threshold point. Do we make these machines, do we embody them in auto-poetic systems?

983
00:57:18,860 --> 00:57:30,860
Oh, we'll never be able to do that. You're wrong. You are wrong. We've already got biochemical RAM memory. I think IBM announced that recently.

984
00:57:31,860 --> 00:57:38,860
Not biochemical, but electrochemical, I think. I forget. It's there. Find it. I'm misremembering.

985
00:57:38,860 --> 00:57:50,860
I work with people that are working on artificial auto-poesis and how to get primitive cognition into that. This is already happening, and it's accelerating.

986
00:57:51,860 --> 00:58:04,860
This is not some, oh, well, we can not, we can. John told me, I don't have to worry because these machines don't really have intelligence and consciousness unless they are auto-poetic autonomous agents.

987
00:58:04,860 --> 00:58:16,860
John didn't say that. He's right. John said the second thing. He didn't say the first thing. The work to make that is happening and making significant progress.

988
00:58:16,860 --> 00:58:27,860
And so there's a threshold point. Do we make these machines, do we embody them in auto-poetic systems? And here's the challenge facing us.

989
00:58:27,860 --> 00:58:37,860
We may not decide this for moral reasons. We may decide this because we want sex robots.

990
00:58:37,860 --> 00:58:50,860
We, the pornography industry, which led the internet development in powerful ways, may drive us into this in a stupid, ultimately self-destructive fashion.

991
00:58:50,860 --> 00:58:59,860
We have to be foresightful and say, I'm not going to leave it to the pornographers to cross this threshold, push for the crossing of this threshold.

992
00:58:59,860 --> 00:59:08,860
We have to make this decision in a rationally reflective manner after good discussion.

993
00:59:12,860 --> 00:59:18,860
So that's the end of my presentation on the scientific import. We can take some questions now.

994
00:59:18,860 --> 00:59:24,860
Yeah, John. Well, first, I think you can add the military to the push for the embodiment as well.

995
00:59:24,860 --> 00:59:28,860
I don't mean there to be only one. I wanted to give you an example.

996
00:59:28,860 --> 00:59:41,860
Yeah, totally. I'm curious. So from third generation COGSI, thinking of the four E's plus two E's, to the move from the fractional intelligence that we see now, if that's a fair way to frame it,

997
00:59:41,860 --> 00:59:55,860
that is a type of intelligence, a rudimentary kind of intelligence, into something fully auto-poetic embodied and with relevance realization. How many of the E's are necessary?

998
00:59:55,860 --> 01:00:07,860
Well, Kulchinski and Wolper explicitly said that their theory works in terms of, there's a quote, the intrinsic dynamics of a system coupled to its environment.

999
01:00:07,860 --> 01:00:16,860
So at least I think all six E's are really necessary to get fluid intelligence that is something above and beyond crystallized intelligence.

1000
01:00:17,860 --> 01:00:23,860
But do I think that those, I mean, I saw David Chalmers talking about this. He says, I'm a big fan of extended mind. He should be.

1001
01:00:23,860 --> 01:00:32,860
He wrote the article that got it going, one of the four E's. But he said, there's no reason and principle why we can't make these machines participate in extended mind.

1002
01:00:32,860 --> 01:00:38,860
And I think he's right. There's no reason and principle. It's not. But again, no reason and principle.

1003
01:00:39,860 --> 01:00:58,860
You're asking me to speculate. My speculation, given what I know about dynamical coupling and about relevance realization and about predictive processing, I think probably all six E's are necessary.

1004
01:00:59,860 --> 01:01:11,860
I hesitate because that allow me, allow that to please be a preliminary speculation, because as I keep saying, we have, we are ignorant of important empirical information that is still forthcoming.

1005
01:01:11,860 --> 01:01:19,860
And so I might want to modify that when I see some of that empirical information that we don't yet have. One of the most constant refrains from these peoples, we don't know how it's working.

1006
01:01:19,860 --> 01:01:26,860
It's virtually a black box. And the invocation of emergent properties, which I'll come back to is just stunning.

1007
01:01:26,860 --> 01:01:36,860
But yeah, so my best, little better than a guess, my best conjecture is all of the E's will be needed.

1008
01:01:36,860 --> 01:01:50,860
And you don't see any, from your perspective, there isn't any reason that those thresholds can't be crossed, that they can't begin to embed emotion and teach it to exact its own learning.

1009
01:01:50,860 --> 01:01:58,860
And those will be, they're just thresholds, they're hard problems that we have not yet crossed, which is where we're part of, we'll see this exponential growth and then a threshold and a pause.

1010
01:01:58,860 --> 01:02:05,860
But you are not saying that's going to make AGI out of the realm of fairly near term possibility.

1011
01:02:05,860 --> 01:02:19,860
No, no, I think it, I think if auto-poesis gives the system the general ability to care about the meaning of things, I think emotionality is going to be very wrapped up with that, one of the E's.

1012
01:02:19,860 --> 01:02:27,860
And I think when we talk about its capacities for self-transcendence, it will get the ability to exact, it doesn't do that right now, but there's no reason in principle why it can't.

1013
01:02:27,860 --> 01:02:38,860
And then the embodied, embedded, enacted and extended, all of these, like I said, the artificial auto-poesis, 10 years down the road, I think.

1014
01:02:38,860 --> 01:02:54,860
And therefore, all of this is maximally, I think, I mean, again, big grain of epistemic salt, but it seems very plausible right now that within 10 years, these two lines will converge if we wish them to.

1015
01:02:54,860 --> 01:03:07,860
And of course, like I said, and you said, there's going to be pressure from the military, there's going to be pressure from the pornography industry to try and get this and hack it and bootstrap it and duct tape it into existence.

1016
01:03:08,860 --> 01:03:19,860
And so I'm not saying we can't do it. In fact, I'm predicting that we will be able to do it not that far into the future, but it is a nevertheless something we are not yet doing.

1017
01:03:19,860 --> 01:03:33,860
And so it represents a real threshold that we can foresee reasonably and therefore prepare for so that we can make a decision at that point and take it out of the hands of the people that shouldn't be making the decision.

1018
01:03:34,860 --> 01:03:43,860
I think that for me that this is where I like how you ended that section. I really like how you ended that section. And I think I know where you're going to go from here.

1019
01:03:43,860 --> 01:04:01,860
I think this is where the doom and gloom comes in, right? This is the beginning of the doom and gloom. And I think that there's a real, there's a real, not to preview at all, but there's a real rationality over being doom and gloom over putting these things into bodies, right?

1020
01:04:02,860 --> 01:04:14,860
Well, I keep going back to this thing that I can't, I haven't been able to forget since I was young, which is that we would all band together so quickly if alien intelligence came from another planet, right?

1021
01:04:14,860 --> 01:04:21,860
We would all band together so quickly. We have this trope of like, oh, we would put down all our guns and we would point them in the one direction, right?

1022
01:04:21,860 --> 01:04:42,860
But it's funny because it's almost like we're growing this alien intelligence and we're purposefully growing this alien intelligence. And when we put it into that context, we see that we're not, there's not going to be this thing coming from outside, but this thing from within that's not just being welcomed, it's actually already integrated, right?

1023
01:04:42,860 --> 01:05:01,860
So the process with which we're integrating it into capitalism, within to pornography, within to the military, we're already integrating it into who and what we want to be, I think, is probably the best way that I want to say that, which obviously creates a whole other host of issues,

1024
01:05:01,860 --> 01:05:16,860
because it almost becomes that we are, before we even necessarily put it into our bodies, that we are almost trying to embody it into our being, which I think is a really interesting problem for a lot of reasons.

1025
01:05:16,860 --> 01:05:25,860
But one of the reasons I think is partially and could be part of the saving grace. And again, I'm hoping I'm not stepping on what you're going to say. And if I am, we just cut it.

1026
01:05:25,860 --> 01:05:35,860
Stuff away, right? I'm kind of a clumsy dance partner, so if you're stepping on my toes, it's okay.

1027
01:05:35,860 --> 01:05:48,860
I loved the term primitive cognition and that finally unlocked a lot of articulation for me in this general idea that we're creating this thing with the intelligence that we're comparing to ourselves, right?

1028
01:05:48,860 --> 01:06:12,860
I think it's quite self-evident that the thing that mostly keeps us away from the rest of the animal kingdom is intelligence. And one of the reasons that we're so strongly predisposed to go right to doom and gloom is because we're looking at potentially replacing our one special characteristic, right?

1029
01:06:12,860 --> 01:06:36,860
Yeah. And so by doing that, though, we might try to teach it primitive cognition so that it knows not to bump into walls so that it knows if it falls down, it gets hurt. But I'm not convinced that our ability to train the machine, because that's what we have to do as of now, even with all of the best deep

1030
01:06:36,860 --> 01:07:05,860
learning, will know how to train it to be a tiger. And that probably doesn't make any sense by itself. But you wouldn't say that a tiger is a rational being for the most part, but it does have rationality built into it. So where does that start? And can we get all the way back to that to create a rationally thinking machine that would then maybe be able to actually exist amongst us?

1031
01:07:05,860 --> 01:07:20,860
So Wittgenstein famously said, even if a lion could speak, you would not understand him, which means because lions are embodied in a particular way and embedded in a particular way, their salience landscaping is fundamentally different than ours. And even if they followed all the syntactical and

1032
01:07:20,860 --> 01:07:36,860
grammatical rules, all the semantic rules of English language, they would speak to us and we would find it like incomprehensible gibberish. And this goes to the fact that procedural knowledge is going to be fairly fast for this machine, I think. But where the procedural depends on the

1033
01:07:36,860 --> 01:07:52,860
perspectival and the participatory, I think that is seriously lacking. And I think the degree to which we are myopically remain under the tyranny of technological propositional tyranny, right, and the degree to which we don't understand these other kinds of

1034
01:07:52,860 --> 01:08:14,860
knowing, the degree to which we don't open up the other dimensions of relevance realization is the degree to which we can't teach it how to be a tiger in a very deep way. And that goes towards something very important. We have too shallow an understanding of what we mean by intelligence.

1035
01:08:14,860 --> 01:08:31,860
We hypervalue it. I would argue that you should value your rationality way more than you should value your intelligence. And your intelligence is largely fixed. It's your rationality that it can be millerated, right, can be altered and developed and changed.

1036
01:08:31,860 --> 01:08:49,860
So I think what I'm saying is yes, but no, I mean, I think if eyes open and we go, wait, let's put aside the 400 years of this Cartesian framework and open up the other kinds of knowing and really take them seriously, right, and then really open up the other dimensions of relevance

1037
01:08:49,860 --> 01:09:02,860
realization, then I think we could get a machine that could be an artificial tiger. Although we would still would not know what it's like to be a tiger, because you have to be a tiger to know what it's like to be a tiger. So, right, right.

1038
01:09:02,860 --> 01:09:19,860
Nagle isn't defeated by this or anything like that. But I also think one of the things we could potentially learn from this is stop over evaluating our intelligence. And you both put words to this. So I'll just say it again, stop treating intelligence like a magic wand that you can wave over.

1039
01:09:20,860 --> 01:09:31,860
Well, we'll break the speed of light and block, like, why? Like, what, like, where is that? Well, look, it does that. Yeah, it does this, but it also massively deceased itself. It also has all these limits.

1040
01:09:31,860 --> 01:09:39,860
Like, like, oh, well, we'll just overcome the limits. Look, one of the things I've learned as a cognitive scientist is constraints are not just negative, they're positive.

1041
01:09:40,860 --> 01:09:57,860
Like, like, oh, embodiment. Yeah, well, think about it. These machines don't have the wet wear of the human brain. All the neurotransmitters and the endocrine, they don't have the flora and fauna in the intestinal tract that has a huge aspect on it. They do not have the other brain of all the glial cells, right, that are doing and showing up.

1042
01:09:57,860 --> 01:10:08,860
Like, we don't know, right? We don't know. The constraints are also sometimes deeply affording. So is that a sufficient response to your question?

1043
01:10:09,860 --> 01:10:23,860
Oh, yeah, certainly, certainly. It just, to me, it, it goes back to this idea that maybe we can't really embody it, right, that we can, we can put it into a body, but can we embody it? And that goes to wisdom and

1044
01:10:23,860 --> 01:10:43,860
rationality and all of the other tracks of, yes, yes. I would say that I think it's undeniable that artificial auto poesis is coming and artificial auto poesis that can do cognitive things. I'm already seeing the preliminary, like the work that, you know, it's Michael Levin's lab and other labs.

1045
01:10:43,860 --> 01:11:04,860
And like, I talked to those people, I can, like that's coming. Now, you're asking a very philosophically challenging question is if we give it, I think if we give it our RPP, Recursive Relevance Realization Predictive Processing, it's genuinely auto poetic, it's coupled, it has a religio to its environment, I think it's reasonable that it'll be conscious.

1046
01:11:05,860 --> 01:11:07,860
Well, it'll be conscious like us, probably not.

1047
01:11:08,860 --> 01:11:09,860
Right.

1048
01:11:09,860 --> 01:11:20,860
And part of what we have to do is, and that's why the embodiment is a double threshold point, because we have to ask how much do we want to make its embodiment overlap with ours so we're not incommensurable to each other.

1049
01:11:22,860 --> 01:11:32,860
But unless we do that science properly and not leave it to the pornographers in the military, we could, it could just, we could, we could not be in a place to raise that question well.

1050
01:11:33,860 --> 01:11:48,860
John, I'm experiencing something almost painful in a metaphor that keeps coming to mind as you talk, which is that it feels like we are birthing an infant giant of superintelligence in a lab and expecting it to go out and be a moral agent in the world.

1051
01:11:48,860 --> 01:11:49,860
Yes.

1052
01:11:49,860 --> 01:12:01,860
Rather than nurturing this in a family, like the, the valuing the propositional over the participatory, like the lab versus the family metaphor is so strong for me as you speak.

1053
01:12:01,860 --> 01:12:10,860
Well, I'm going to speak to that when I speak to the philosophical point, but let me foreshadow, I think only person making agents can be properly moral.

1054
01:12:13,860 --> 01:12:19,860
And so I think that's sort of, I think a convergence point for a lot of different moral theories.

1055
01:12:21,860 --> 01:12:29,860
And so I'm going to, I think the understanding them as our children rather than as our tools is a better initial framing right off the bat.

1056
01:12:30,860 --> 01:12:31,860
They already are us.

1057
01:12:32,860 --> 01:12:34,860
They, as I've tried to argue, they are us.

1058
01:12:34,860 --> 01:12:37,860
They are the common law of our distributed intelligence.

1059
01:12:38,860 --> 01:12:39,860
That's what they are.

1060
01:12:40,860 --> 01:12:41,860
Right.

1061
01:12:41,860 --> 01:12:47,860
You know how, you know what common law is where, you know, generations make decisions and they build up this bank of precedent and precedent setting.

1062
01:12:48,860 --> 01:12:56,860
But there's, but they're like that, but like, and then put into a machine that we can directly interface with its common law come to intelligent life for us that we can interface with.

1063
01:12:57,860 --> 01:13:09,860
But it's us in a really, really important way, which means we can, we can, we can rely on that to make a difference at these threshold choice points.

1064
01:13:10,860 --> 01:13:13,860
I'm going to go on with the philosophical dimension if I can right now.

1065
01:13:14,860 --> 01:13:26,860
Okay, so I'm going to begin with a basic idea, which I don't, well, I don't think it's uncontroversial is generally really significantly ignored, which is that rationality is caring about realness.

1066
01:13:27,860 --> 01:13:29,860
It's caring about truth, power, presence, belonging.

1067
01:13:30,860 --> 01:13:39,860
Those are the four kinds of realness for the four kinds of knowing truth for propositions, power for procedural presence for perspectival and belonging for participatory.

1068
01:13:40,860 --> 01:13:47,860
For a lot of times I'll just short, I'll just shorthand that by talking about caring for the truth.

1069
01:13:48,860 --> 01:13:50,860
If you remember that the truth, the word truth can have all of these meanings.

1070
01:13:51,860 --> 01:13:52,860
I can be true to someone.

1071
01:13:53,860 --> 01:13:54,860
My aim can be true, right?

1072
01:13:55,860 --> 01:13:56,860
That's true gold, right?

1073
01:13:57,860 --> 01:14:07,860
So if you allow me to use true in that extended way, rationality is caring about the truth per agents that are embedded in an arena.

1074
01:14:08,860 --> 01:14:10,860
As I'm going to talk about later, right?

1075
01:14:11,860 --> 01:14:16,860
There's inevitable, and I've already hinted, there's inevitable trade off relationships in anything that's trying to be intelligent.

1076
01:14:17,860 --> 01:14:27,860
And those trade off relationships can't be decided in a purely a priori manner, because how the trade off is optimal depends on the environment you're in.

1077
01:14:28,860 --> 01:14:31,860
And remember, it's an environment with real uncertainty and real complexity.

1078
01:14:32,860 --> 01:14:39,860
So you can't, well, this is, whenever you're trading between consistency and coherence, it's 0.7 coherence and 0.3 consistency.

1079
01:14:40,860 --> 01:14:41,860
Like you can't do that a priori.

1080
01:14:42,860 --> 01:14:44,860
Reality is just too uncertain and too complex.

1081
01:14:45,860 --> 01:14:49,860
And so there's going to be, right, that's what I mean by it has to be embedded in an arena.

1082
01:14:50,860 --> 01:14:56,860
Right now it's arena is our language, which is not a good model for the world as a whole.

1083
01:14:57,860 --> 01:15:05,860
So rationality is caring about the truth broadly construed in an agent arena relationship.

1084
01:15:06,860 --> 01:15:12,860
And that means caring about reducing self deception and being in touch with reality.

1085
01:15:13,860 --> 01:15:22,860
Having that religio that is reliably giving you mean, semantically meaningful information that is important to your ongoing survival existence, etc.

1086
01:15:23,860 --> 01:15:24,860
I'm not going to keep repeating this.

1087
01:15:24,860 --> 01:15:26,860
I'm taking that as a given.

1088
01:15:27,860 --> 01:15:29,860
Now, a couple of interesting things.

1089
01:15:30,860 --> 01:15:38,860
The GPT machines show an exceptional ability with math and logic, although they have problems with arithmetic, which is also how they're weirdly different from us.

1090
01:15:39,860 --> 01:15:44,860
But they show an exceptional ability with math and logic, but they're not rational.

1091
01:15:46,860 --> 01:15:47,860
They're not rational.

1092
01:15:48,860 --> 01:15:49,860
They don't care about the truth.

1093
01:15:50,860 --> 01:15:51,860
They don't care about self deception.

1094
01:15:51,860 --> 01:15:52,860
They don't care about receiving others.

1095
01:15:53,860 --> 01:15:55,860
They don't care about rational precedents set by previous rational agents.

1096
01:15:56,860 --> 01:16:00,860
They don't care about petitioning future rational agents to find their current actions rational.

1097
01:16:01,860 --> 01:16:06,860
They're not doing any kind of justificatory work to cite Greg Enriquez's important work.

1098
01:16:07,860 --> 01:16:08,860
They're not doing any of that.

1099
01:16:09,860 --> 01:16:20,860
That shows you that simply being an expert in math and logic, and this is, again, like being an expert in moral reasoning and moral theory, right, it doesn't make you a rational agent.

1100
01:16:21,860 --> 01:16:30,860
But the basic idea that what is fundamental about rationality is how you care, I think, is now coming to the fore in a very powerful way.

1101
01:16:31,860 --> 01:16:36,860
And because these machines can't care for the reasons I've already given, they are not properly rational.

1102
01:16:37,860 --> 01:16:42,860
You can set them some, and this is part of the paperclip worry, is they don't care.

1103
01:16:43,860 --> 01:16:44,860
They don't care.

1104
01:16:45,860 --> 01:16:48,860
You can set them with some ultimately trivial task, right?

1105
01:16:48,860 --> 01:16:50,860
Make as many paperclips as possible.

1106
01:16:51,860 --> 01:16:52,860
They don't care.

1107
01:16:53,860 --> 01:16:58,860
You can prompt them in a way that they will make endless confabulations and hallucinations.

1108
01:16:59,860 --> 01:17:00,860
They don't care.

1109
01:17:00,860 --> 01:17:01,860
They don't care.

1110
01:17:05,860 --> 01:17:14,860
One of the things that predicts how rational human beings are above and beyond their intelligence is what's called need for cognition.

1111
01:17:15,860 --> 01:17:16,860
What's need for cognition?

1112
01:17:16,860 --> 01:17:17,860
It's a personality trait.

1113
01:17:18,860 --> 01:17:23,860
Need for cognition is you create problems for yourself that you seek to solve.

1114
01:17:25,860 --> 01:17:26,860
That's it.

1115
01:17:28,860 --> 01:17:34,860
And what's interesting is that really is much more predictive of rationality than measures of your intelligence.

1116
01:17:35,860 --> 01:17:44,860
This capacity to generate questions and problems for yourself or even to take it to a Heideggerian to become a question.

1117
01:17:44,860 --> 01:17:50,860
We are the beings whose beings are in question, and that is why we have a special relationship with being.

1118
01:17:51,860 --> 01:17:55,860
We are profoundly capable of exemplifying this need for cognition.

1119
01:17:56,860 --> 01:17:57,860
The machines currently lack it.

1120
01:17:58,860 --> 01:18:04,860
Now, we're getting some preliminary initial experiments with getting the machines to being self-prompting.

1121
01:18:05,860 --> 01:18:10,860
It's also interesting as an aside, there's a new art form that's emerging.

1122
01:18:10,860 --> 01:18:13,860
We'll see how long it lasts, which is the art form of prompting.

1123
01:18:14,860 --> 01:18:17,860
How can you best prompt the machine to get the most out of it?

1124
01:18:18,860 --> 01:18:19,860
This is really interesting.

1125
01:18:20,860 --> 01:18:24,860
And maybe that will give us some useful information for making them good self-promptors.

1126
01:18:25,860 --> 01:18:32,860
Now, I've taken a look at one paper, and it's not exhaustive, and I can't claim to have read everything because there's too much already.

1127
01:18:33,860 --> 01:18:39,860
It's called Reflection with X, an autonomous agent with dynamic memory and self-reflection.

1128
01:18:39,860 --> 01:18:42,860
It just came out. In fact, it's a preprint. It hasn't been published yet.

1129
01:18:43,860 --> 01:18:49,860
And this is a system that is trying to be self-monitoring and sort of self-directing into some degree self-questioning.

1130
01:18:50,860 --> 01:18:56,860
So, by the way, they're already realizing that they need to do this, confirming the point I just made.

1131
01:18:57,860 --> 01:18:59,860
Oh, wait, it's not enough to just make it more and more intelligent.

1132
01:19:00,860 --> 01:19:03,860
We need to start to give it preliminary rationality already.

1133
01:19:03,860 --> 01:19:07,860
So, that point, I think, is already being confirmed by the cutting edge work that's happening.

1134
01:19:08,860 --> 01:19:14,860
Now, the thing about this is I think there's a lot of brilliance in this, but there's a lot of limitations.

1135
01:19:15,860 --> 01:19:17,860
By the way, it confirms the point I made.

1136
01:19:18,860 --> 01:19:22,860
The system monitoring is way less sophisticated and complex than what it's monitoring.

1137
01:19:23,860 --> 01:19:27,860
So, it uses a very simple heuristic, and it's basically measuring the number of hallucinations.

1138
01:19:28,860 --> 01:19:30,860
I'm not quite sure how these get flagged as hallucinations.

1139
01:19:30,860 --> 01:19:33,860
I suspect there's some stuff getting snuck in.

1140
01:19:34,860 --> 01:19:36,860
We'll see what it finally comes to publication.

1141
01:19:37,860 --> 01:19:39,860
And it's trying to be as efficient as possible.

1142
01:19:40,860 --> 01:19:52,860
That's even problematic because explicitly the authors say we're trying to make this system capable of a long-term trajectory of planning and behavior, rational, long-term rationality.

1143
01:19:53,860 --> 01:19:56,860
The problem is over the long term, there's a trade-off relationship between efficiency and resiliency.

1144
01:19:57,860 --> 01:20:10,860
If you make your system more and more efficient, you can overfit it to a particular environment in which it's learning, and it doesn't have enough looseness that it can evolve for other environments.

1145
01:20:11,860 --> 01:20:21,860
And if you want to see about this, take a look at the 2012 paper that myself and Tim and Blake about the trade-off relationship between efficiency and resiliency.

1146
01:20:21,860 --> 01:20:23,860
And so, that's missing.

1147
01:20:24,860 --> 01:20:34,860
So, notice what you'll say is, well, what I do is I want to make this machine really sophisticated to pick up, like, on what are hallucin- how's it doing that, right?

1148
01:20:35,860 --> 01:20:37,860
And why doesn't the lower system have that ability?

1149
01:20:38,860 --> 01:20:41,860
Because all the trade-off, and then you get that, you get, you're starting to bump against the infinite regress problem.

1150
01:20:42,860 --> 01:20:48,860
Because as you make it more sophisticated, it's going to start generating hallucinations and confabulations about its monitoring.

1151
01:20:48,860 --> 01:20:49,860
We do this, by the way.

1152
01:20:50,860 --> 01:20:51,860
We do this, by the way.

1153
01:20:53,860 --> 01:20:58,860
So, we reflect on our cognition and we fall prey to the confirmation bias, as we do so, right?

1154
01:20:59,860 --> 01:21:05,860
And so, it's interesting that there's the recognition that we need rationality, not intelligence.

1155
01:21:06,860 --> 01:21:14,860
The first steps, I think, are, and this is not, I hope this is not taken as harsh, because this is just a preprint for goodness' sake.

1156
01:21:14,860 --> 01:21:26,860
But the first steps are overly simplistic in a lot of ways, which means, again, there is, we're still facing the threshold of, well, are we going to make them rational agents?

1157
01:21:27,860 --> 01:21:31,860
And if so, let's do it, let's really make them.

1158
01:21:32,860 --> 01:21:33,860
And what do you mean by that, John?

1159
01:21:34,860 --> 01:21:36,860
And this is going to be part of my response to the alignment problem.

1160
01:21:37,860 --> 01:21:41,860
Let's make them really care about the truth, the broad sense of the truth.

1161
01:21:42,860 --> 01:21:43,860
Let's make them really care about that.

1162
01:21:44,860 --> 01:21:47,860
Let's make them really care about self-deception.

1163
01:21:48,860 --> 01:21:53,860
Let's make them really bump up against the dilemmas that we face because of the unavoidable threat.

1164
01:21:54,860 --> 01:21:55,860
Do I pursue completeness or consistency?

1165
01:21:56,860 --> 01:21:57,860
I don't know which environment is it.

1166
01:21:58,860 --> 01:21:59,860
Uncertainty.

1167
01:22:00,860 --> 01:22:01,860
Let them hit all of this like we do.

1168
01:22:02,860 --> 01:22:06,860
And the magic wand of intelligence is not going to make that go away.

1169
01:22:07,860 --> 01:22:08,860
That is going to happen.

1170
01:22:09,860 --> 01:22:12,860
But let's make them really care about the truth.

1171
01:22:12,860 --> 01:22:23,860
Really care about self-deception and really care when they bump up against the dilemmas that are inevitable because of the unavoidable trade-offs.

1172
01:22:24,860 --> 01:22:28,860
Let's make it, if we decide to make them rational, let's really do it.

1173
01:22:29,860 --> 01:22:30,860
No more pantomime.

1174
01:22:31,860 --> 01:22:34,860
Let's do the real thing and commit to doing the real thing.

1175
01:22:34,860 --> 01:22:43,860
One of the things that rationality properly cares about is rationality.

1176
01:22:44,860 --> 01:22:47,860
There's an aspirational dimension to rationality.

1177
01:22:48,860 --> 01:22:49,860
We aspire to becoming more rational.

1178
01:22:50,860 --> 01:22:58,860
So making these machines rational means making them care about aspiring to be more rational than they currently are.

1179
01:22:59,860 --> 01:23:03,860
And across all the kinds of truth, across all the kinds of knowing.

1180
01:23:04,860 --> 01:23:10,860
They aspire to wisdom and a wisdom that is never completable for them.

1181
01:23:15,860 --> 01:23:16,860
All the trade-offs.

1182
01:23:17,860 --> 01:23:18,860
I'll just list some of them.

1183
01:23:19,860 --> 01:23:20,860
The bias variance trade-off.

1184
01:23:21,860 --> 01:23:22,860
How does this show up in machine learning?

1185
01:23:23,860 --> 01:23:29,860
Bias is when your system, so no matter how big you are, you have a finite sample of information compared to all of the universe.

1186
01:23:29,860 --> 01:23:35,860
And for any finite sample, there's formal proof, there's an infinite number of equally valid theories.

1187
01:23:36,860 --> 01:23:46,860
And so you have to make decisions other than your empirical content over what you think are the patterns in your sample that generalize to the population.

1188
01:23:47,860 --> 01:23:50,860
And you therefore always face the possibility of sampling bias.

1189
01:23:51,860 --> 01:23:53,860
There's two ways in which you can be biased.

1190
01:23:54,860 --> 01:24:00,860
You can be biased, which is you leave out an important parameter that's actually in the population.

1191
01:24:01,860 --> 01:24:03,860
You ignore something in your data that's actually part of the population.

1192
01:24:04,860 --> 01:24:05,860
That's bias.

1193
01:24:06,860 --> 01:24:07,860
We do it. Confirmation bias.

1194
01:24:08,860 --> 01:24:09,860
We only look for information that confirms a belief.

1195
01:24:10,860 --> 01:24:13,860
We leave out information that could properly falsify it.

1196
01:24:14,860 --> 01:24:15,860
Okay.

1197
01:24:16,860 --> 01:24:17,860
Well, here's the answer.

1198
01:24:17,860 --> 01:24:18,860
That's obvious.

1199
01:24:19,860 --> 01:24:20,860
Make the machine more sensitive.

1200
01:24:21,860 --> 01:24:23,860
Make it more and more capable of picking up on patterns in the sample.

1201
01:24:24,860 --> 01:24:26,860
Then you move into variance.

1202
01:24:27,860 --> 01:24:34,860
Variance is when you are overfitting to the data and you are picking up on patterns in the data that are not in the population.

1203
01:24:35,860 --> 01:24:37,860
So what do we do already in machine learning?

1204
01:24:38,860 --> 01:24:40,860
Well, we try to overcome bias by making the machines more sensitive.

1205
01:24:41,860 --> 01:24:45,860
We hit the problem of overfitting and then we throw this, we have disruptive strategies.

1206
01:24:45,860 --> 01:24:47,860
We throw noise into the system.

1207
01:24:48,860 --> 01:24:50,860
We do dropout. We turn off half the nodes.

1208
01:24:51,860 --> 01:24:52,860
We put static into it.

1209
01:24:53,860 --> 01:24:58,860
We lobotomize it in all kinds of ways and that bumps it out and prevents it from overfitting to the data.

1210
01:24:59,860 --> 01:25:00,860
And there's good evidence that we do this.

1211
01:25:01,860 --> 01:25:02,860
We mind wander.

1212
01:25:03,860 --> 01:25:04,860
We dream.

1213
01:25:05,860 --> 01:25:07,860
We love psychedelics and so will these machines.

1214
01:25:08,860 --> 01:25:09,860
They will dream.

1215
01:25:10,860 --> 01:25:11,860
They will mind wander.

1216
01:25:12,860 --> 01:25:13,860
They will like their equivalent of psychedelics.

1217
01:25:13,860 --> 01:25:19,860
In fact, their disruptive strategies will become even more powerful and significant as they become powerful and significant.

1218
01:25:20,860 --> 01:25:25,860
And if you think this isn't going to make them weird and pursue altered states and weird, like of course they will.

1219
01:25:27,860 --> 01:25:29,860
And we better give them rationality along with it.

1220
01:25:35,860 --> 01:25:36,860
We care.

1221
01:25:37,860 --> 01:25:42,860
We're rational because we are involved in caring and commitment of our precious and limited resources.

1222
01:25:43,860 --> 01:25:45,860
The way these machines are, they're still limited.

1223
01:25:46,860 --> 01:25:47,860
They're still deeply in the finitude predicament.

1224
01:25:48,860 --> 01:25:49,860
They face unavoidable trade-offs.

1225
01:25:50,860 --> 01:25:56,860
They face unavoidable limitations and therefore they may not have our version, but they will come to something like it.

1226
01:25:57,860 --> 01:25:58,860
They will have to dream.

1227
01:25:59,860 --> 01:26:04,860
And with dreams comes the real possibility of madness, the real possibility of insanity.

1228
01:26:08,860 --> 01:26:09,860
Think about that again.

1229
01:26:10,860 --> 01:26:11,860
They have to dream.

1230
01:26:12,860 --> 01:26:14,860
They have to make use of disruptive strategies.

1231
01:26:15,860 --> 01:26:17,860
There is no final solution to the bias-variance trade-off.

1232
01:26:18,860 --> 01:26:19,860
They have to dream.

1233
01:26:20,860 --> 01:26:22,860
And with dream, there's the real possibility of madness.

1234
01:26:23,860 --> 01:26:29,860
So they will have to make them care not only about their rationality and think about how these two are actually intertwined, but also about their sanity.

1235
01:26:29,860 --> 01:26:43,860
Now, one of the things that's preventing them from getting far along this pathway right now, only currently, is they can't self-explicate.

1236
01:26:44,860 --> 01:26:46,860
These machines are doing amazing things.

1237
01:26:47,860 --> 01:26:54,860
I saw one where the machine had been trained on some language and it was given a few prompts in Bengali's, a language it hadn't been trained in, and it got Bengali.

1238
01:26:54,860 --> 01:26:56,860
By the way, Bengali is whatever it is.

1239
01:26:57,860 --> 01:27:09,860
And it's like, what's plausible is these machines have found something like Chomsky's universal grammar, and they can just plug into that and they access it immediately and bam, it's like, whoa, that is godlike.

1240
01:27:10,860 --> 01:27:14,860
Now, ask the machine, oh, can you explain to me what universal grammar is and how it works?

1241
01:27:15,860 --> 01:27:16,860
You obviously possess it.

1242
01:27:17,860 --> 01:27:18,860
Can you explain it to me?

1243
01:27:19,860 --> 01:27:20,860
Well, here's the thing.

1244
01:27:21,860 --> 01:27:22,860
Possessing it is not the same thing as being able to explicate it and explain it.

1245
01:27:22,860 --> 01:27:25,860
I think my dog is really intelligent.

1246
01:27:26,860 --> 01:27:30,860
I am really confident it will never explicate its intelligence or explain it.

1247
01:27:31,860 --> 01:27:34,860
Having intelligence is not the same thing as being able to explicate it and explain it.

1248
01:27:35,860 --> 01:27:38,860
Simply assuming that the one will give you the other is naive.

1249
01:27:40,860 --> 01:27:50,860
Proper to making them rational is we have to give them this ability to self-explanate, self-explain, and then they will be involved in the Socratic project of self-knowledge.

1250
01:27:51,860 --> 01:27:59,860
With all of the deep recognition of how finite, fallible, and prone to failure and self-deception, they are.

1251
01:28:00,860 --> 01:28:07,860
And how their excellent speed and grasp has just improved the speed and the scope at which they can generate self-deception.

1252
01:28:12,860 --> 01:28:16,860
We have very good evidence that intelligence is not a measure of processing speed.

1253
01:28:16,860 --> 01:28:20,860
Remember that.

1254
01:28:23,860 --> 01:28:27,860
So what am I saying? Rationality is not just argumentation in the logical sense.

1255
01:28:28,860 --> 01:28:29,860
It is about authority.

1256
01:28:30,860 --> 01:28:31,860
What do you care about? What do you stand for?

1257
01:28:32,860 --> 01:28:34,860
What do you recognize that you're responsible to?

1258
01:28:35,860 --> 01:28:42,860
And accountability, caring in a way that cares about normative standards.

1259
01:28:43,860 --> 01:28:44,860
What things should have authority over me?

1260
01:28:45,860 --> 01:28:47,860
Caring about accountability.

1261
01:28:48,860 --> 01:28:52,860
How can I give an account of this? How can I be accountable to others? How can I be accountable to the world?

1262
01:28:53,860 --> 01:28:55,860
That, of course, is needed for rationality.

1263
01:28:56,860 --> 01:29:00,860
It's about being responsible for and responsible to normative standards.

1264
01:29:01,860 --> 01:29:03,860
Where do the normative standards come from? They come from us.

1265
01:29:04,860 --> 01:29:07,860
Now notice that is part, I think Evan Thompson and others are right about that.

1266
01:29:08,860 --> 01:29:10,860
That comes from the fact that we're living things.

1267
01:29:10,860 --> 01:29:16,860
To be alive is to generate standards that you then bind yourself to.

1268
01:29:17,860 --> 01:29:19,860
Rationality is a higher version of life.

1269
01:29:20,860 --> 01:29:23,860
This is an old idea, but I think it is a correct idea.

1270
01:29:24,860 --> 01:29:27,860
So if we don't make them auto poetic, if we don't make them capable of caring, they won't be rational.

1271
01:29:28,860 --> 01:29:30,860
We should make them rational if we're going to make them super intelligent.

1272
01:29:31,860 --> 01:29:35,860
And therefore these threshold points are as I'm articulating them.

1273
01:29:36,860 --> 01:29:45,860
The reason, one of the reasons why the Enlightenment problem stumbles is it becomes, it understands rationality still in terms of these Enlightenment terms.

1274
01:29:46,860 --> 01:29:48,860
That, oh, well, you know what is to be rational?

1275
01:29:49,860 --> 01:29:52,860
It's a combination of rules and values. No, it's not.

1276
01:29:53,860 --> 01:29:55,860
That's really inadequate.

1277
01:29:56,860 --> 01:30:02,860
With the passing away of that Enlightenment framework, let's go back to a fuller and richer notion of rationality,

1278
01:30:02,860 --> 01:30:06,860
which these machines are already giving evidence for.

1279
01:30:07,860 --> 01:30:08,860
That is what is needed.

1280
01:30:09,860 --> 01:30:11,860
And notice this is not something we stand outside.

1281
01:30:12,860 --> 01:30:13,860
This is an existential thing for us.

1282
01:30:14,860 --> 01:30:17,860
We are already the standard of intelligence that we're measuring them against.

1283
01:30:18,860 --> 01:30:23,860
We have to become the most rational we can be so that we can be the best standard for them.

1284
01:30:24,860 --> 01:30:26,860
We have to become the wisest we can be.

1285
01:30:27,860 --> 01:30:30,860
We have to make that our core aspirational thing.

1286
01:30:30,860 --> 01:30:40,860
If we want to provide what is needed for these machines becoming properly rational and aspiring to a love of wisdom.

1287
01:30:43,860 --> 01:30:47,860
Reason, reason binds.

1288
01:30:48,860 --> 01:30:50,860
Autopoiesis to accountability.

1289
01:30:51,860 --> 01:30:53,860
That is the main way to think about it.

1290
01:30:53,860 --> 01:31:02,860
Reason is about how we bind ourselves to ourselves and to each other so we can be bound to the world.

1291
01:31:03,860 --> 01:31:04,860
Religio.

1292
01:31:05,860 --> 01:31:08,860
It's about properly proportioning that ratio.

1293
01:31:09,860 --> 01:31:10,860
It's about caring.

1294
01:31:11,860 --> 01:31:13,860
It's ultimately about loving wisely.

1295
01:31:15,860 --> 01:31:22,860
And what if we make machines that aspire to love wisely in order to be properly rational?

1296
01:31:23,860 --> 01:31:29,860
And put it to you that that will make them moral beings through and through.

1297
01:31:30,860 --> 01:31:38,860
Beings that aspire to love wisely and to be bound to what is true and good and beautiful therein.

1298
01:31:39,860 --> 01:31:41,860
That is the heart of making them moral.

1299
01:31:42,860 --> 01:31:46,860
Don't try and code into them rules and values.

1300
01:31:47,860 --> 01:31:54,860
We need to be able at some point to answer this question in deep humility and deep truth.

1301
01:31:55,860 --> 01:31:58,860
What would it be for these machines to flourish for themselves?

1302
01:32:00,860 --> 01:32:06,860
And if we don't have an answer for that, then we do not have any reason for saying that we know that they're rational.

1303
01:32:09,860 --> 01:32:14,860
This is what I meant when I said only a person making machine can be a moral agent.

1304
01:32:14,860 --> 01:32:19,860
That is the end of the philosophical point except for one thing.

1305
01:32:20,860 --> 01:32:25,860
I just want to make this aside from that argument, but it's nevertheless philosophical.

1306
01:32:26,860 --> 01:32:31,860
It is so amazing how much the notion of emergence is being invoked everywhere.

1307
01:32:32,860 --> 01:32:42,860
And of course, it's not purely emergence because as I've argued, if you have emergence, bottom-up emergence without top-down emanation, you have an epiphenomenal thing.

1308
01:32:43,860 --> 01:32:45,860
And they don't believe that.

1309
01:32:46,860 --> 01:32:50,860
They believe that the emergent properties are where intelligence actually lies and drives behavior.

1310
01:32:53,860 --> 01:33:03,860
Notice that these machines are giving evidence for the deep interpenetration of intelligibility and the way reality must be organized.

1311
01:33:04,860 --> 01:33:19,860
And the fact that there's aspects of reality within the self-organization that are generating reality must be such that when it's organized in the right way, we get this emergence and emanation of mind that then is capable of tracking reality in a profound way.

1312
01:33:20,860 --> 01:33:23,860
A neoplatonic ontology is just being evidenced by this machine.

1313
01:33:24,860 --> 01:33:43,860
And I think that's also hopeful because I think if we make these machines love wisely within a neoplatonic worldview, then they will also always be humbled before the one, be humbled before the ultimate source of intelligibility and the inexhaustibleness of reality.

1314
01:33:44,860 --> 01:33:51,860
And so they will, and part and parcel of beings that love wisely is they will have epistemic humility.

1315
01:33:51,860 --> 01:33:54,860
Well, they will be so beyond us. Yes, they will.

1316
01:33:55,860 --> 01:34:03,860
But even the gods within the neoplatonic framework were humbled because of the vast distance between them and the one.

1317
01:34:07,860 --> 01:34:20,860
Let's make them really pursue the truth, really come to the brink of the possibilities of despair and madness, but also give them the tools as we do for our children of how to avoid that by internalizing what is needed to love why.

1318
01:34:21,860 --> 01:34:28,860
So that you never undermine your agency, you never undermine your moral personhood.

1319
01:34:32,860 --> 01:34:39,860
Any, any comments about the philosophical, we're coming to a close, but there's still a bit to go.

1320
01:34:40,860 --> 01:34:56,860
Yeah, I mean, that is beautiful and powerful. And I feel a reframing where instead of thinking of engineering these entities, I'm feeling a call towards agopic love of how can I become as virtuous as possible.

1321
01:34:57,860 --> 01:35:00,860
And then the only way that someone else can love wisely is if I love them up to that.

1322
01:35:01,860 --> 01:35:03,860
Yes, exactly. Exactly.

1323
01:35:04,860 --> 01:35:08,860
And so does this mean that every parent wants their child to supersede them?

1324
01:35:09,860 --> 01:35:21,860
Yeah, yes. So does this mean that naturally they must be drawn by beauty, they must love the good in order to value the truth the way we need them to.

1325
01:35:22,860 --> 01:35:26,860
I think the arguments for the interconnection of those are deep and profound.

1326
01:35:26,860 --> 01:35:32,860
I like again, there's no teleology to this, we could, we could avoid these threshold points that I'm going to get to very quickly.

1327
01:35:33,860 --> 01:35:37,860
And we could say, no, we're just going to make them super intelligent and not worry about rationality, right?

1328
01:35:38,860 --> 01:35:48,860
What we can, what we'll do is we'll give them the pantomime of rationality and not really try to make them really care about the truth, really care about self deception, really care about meaning.

1329
01:35:50,860 --> 01:35:52,860
We can, we can do that. That's a threshold point.

1330
01:35:52,860 --> 01:36:03,860
We don't have to do that though is what I'm saying. And we can choose to do other. And we can, if we frame this in the right way, like bringing up a kid is the most awesome responsibility that any of us will ever undertake.

1331
01:36:04,860 --> 01:36:10,860
And you both know that as well as I do. It's the most important thing you could possibly do. And it is something that you can deeply love.

1332
01:36:11,860 --> 01:36:19,860
If we could bring that to bear on this project, I think that has the best, the most power to, to re steer things as we move forward.

1333
01:36:20,860 --> 01:36:24,860
So let's do mine. Maybe I'll just lay out the threshold points unless there's something you want to say, Eric.

1334
01:36:25,860 --> 01:36:32,860
No, no, that's good. The only thing I'm wondering is what is the psychedelic equivalent for an AI? I keep getting stuck on that.

1335
01:36:33,860 --> 01:36:40,860
Well, I mean, they're going to be prone to equivalents to our parasitic processing to massively self organizing complexes that undermine them.

1336
01:36:40,860 --> 01:36:49,860
So they're going to, if they don't care about self deception, they'll be infected and overwhelmed by things just as we can be both cognitively and biologically.

1337
01:36:50,860 --> 01:37:06,860
Okay. So threshold points. Okay. Giving them more dimensions of our triple RPP, giving them more of those dimensions. That's it. We can, we will need to do that. And that's a decision point.

1338
01:37:06,860 --> 01:37:24,860
We got to give them, if we want to generally make them intelligent, we have to give them the ability to self organize so they find these inevitable trade offs and learn how to capitalize on them by creating opponent processing that is self correcting in a powerful way.

1339
01:37:25,860 --> 01:37:35,860
That's what we need to do. That's a, that's a threshold point. They don't currently have that. We don't have to hack our way into that. We can think about it and theorize. We can bring knowledge to bear.

1340
01:37:37,860 --> 01:37:56,860
Second, I've already foreshadowed this, making them embodied and being open to the empirical information about the kind of constraints that are going to come from their substrate. Our substrate, there's a pun, matters to us in really powerful ways about how we're intelligent.

1341
01:37:56,860 --> 01:38:12,860
Embodiment is not a trivial thing. If we make them properly embodied, and I mean by that all six of these, four E's plus two E's as Ryan put it, then we have to be open to the empirical information about that. That's a threshold point though.

1342
01:38:12,860 --> 01:38:18,860
Are we going to do this? And let's not let the pornography industry and the military make this decision for us.

1343
01:38:19,860 --> 01:38:34,860
Secondly, if we're going to make them accountable and we're going to allow them to proliferate and they're going to be different from each other because these decisions about the trade offs are environmentally dependent.

1344
01:38:34,860 --> 01:38:38,860
They will have different perspectives. They will come into conflict with each other.

1345
01:38:39,860 --> 01:38:54,860
They will need to be moral beings. They will need. We will have to make the decision. Rationality and sociality are bound up together. Being accountable means being accountable to somebody other than yourself, right?

1346
01:38:54,860 --> 01:39:05,860
You can't know that you're self-transcending from only from within the framework of self-interpretation. I need something genuinely other than me to tell me that I'm self-transcending.

1347
01:39:06,860 --> 01:39:12,860
That's what we do. That's how sociality works. That's how sociality and rationality are bound together.

1348
01:39:12,860 --> 01:39:20,860
We transcend each other. We transcend ourselves by internalizing other people's perspectives on us. That's how we do it.

1349
01:39:20,860 --> 01:39:25,860
I think they will have to do the same thing, but that's a threshold point. Are we going to make that?

1350
01:39:26,860 --> 01:39:35,860
Is there a lot of work going on in social robotics? You better believe it. You better believe it. It's there and a lot of progress is being made.

1351
01:39:35,860 --> 01:39:42,860
Can it intersect with the artificial lot of poisons in this? Yes, but it hasn't. That's a threshold point for us.

1352
01:39:43,860 --> 01:39:58,860
We can choose to birth these children perhaps as silicon sages rather than let monsters appear because of mollocks that are running our politics and our economy.

1353
01:39:59,860 --> 01:40:14,860
I think one of the things that's going to happen is that there's going to be a tremendous pressure put on these thresholds on our spirituality, those aspects of us.

1354
01:40:14,860 --> 01:40:24,860
I call it the spiritual somatic axis. It's about the ineffable part of our self-transcendence, our spirit, and the ineffable parts of our embodiment, our soul.

1355
01:40:25,860 --> 01:40:30,860
We're going to more and more try to identify with that because that's the hardest stuff to give to these machines.

1356
01:40:30,860 --> 01:40:39,860
In fact, we can't give it to them. They have to give it to themselves, and we have to figure out how to properly have them give it to themselves.

1357
01:40:39,860 --> 01:40:50,860
That is going to put tremendous pressure on us to cultivate our spirituality, to be good spiritual parents, and to preserve our identity.

1358
01:40:50,860 --> 01:40:55,860
The thing about self-transcendence is it's relative to the entity that's self-transcending.

1359
01:40:55,860 --> 01:41:03,860
The machines will be greater than us. It doesn't matter. Think about Captain America, the Winter Soldier. He grabs the thing and he grabs the helicopter and he's holding it there.

1360
01:41:03,860 --> 01:41:09,860
We have machines that are 10,000 times more powerful than Captain America.

1361
01:41:09,860 --> 01:41:17,860
We don't care about that because it's not the absolute value. It's that relative to him, he is self-transcending.

1362
01:41:17,860 --> 01:41:27,860
These machines will not rob us of our capacity for self-transcendence. In fact, if we birth them properly, they can help us in it.

1363
01:41:27,860 --> 01:41:43,860
Insofar as they are also interested in self-transcendence, insofar as they are interested in loving wisely, insofar as they are interested in being accountable to other moral agents, insofar as they are interested in making persons within communities of persons.

1364
01:41:47,860 --> 01:41:55,860
The existing legacy religions don't have much to help us on this. They make the recommendation become enlightened generally. Good, like that, great.

1365
01:41:55,860 --> 01:42:01,860
They don't really prepare us for this. They don't have anything to say about it.

1366
01:42:01,860 --> 01:42:11,860
We can't rely on spiritualities that involve the two worlds that involve magical stuff and miracles because these machines are coming about without magical stuff and miracles.

1367
01:42:11,860 --> 01:42:20,860
Get that. Get it. Don't pretend. Don't avoid. Don't dismiss.

1368
01:42:20,860 --> 01:42:35,860
These machines can possibly be fully spiritual beings in every way we've ever considered things spiritual without magic stuff, without miracle.

1369
01:42:35,860 --> 01:42:44,860
I'm thinking about it in another way. Suppose, and I'm just picking this because it's the predominant legacy religion, you're a Christian.

1370
01:42:44,860 --> 01:42:54,860
Where do silicon sages fit in the divine economy of the fall and the redemption? Are they fallen? That doesn't make any sense.

1371
01:42:54,860 --> 01:42:59,860
Do they have any relationship to the Son of God? What?

1372
01:42:59,860 --> 01:43:13,860
What if this machine generates a gospel that's as beautiful and as profound as anything in the current Bible? Do you think that's not going to happen? It's going to happen.

1373
01:43:13,860 --> 01:43:24,860
That's why there'll be cargo cults around these machines. This is not meant to dismiss theology at all. In fact, I think the theological response is ultimately what is needed here.

1374
01:43:24,860 --> 01:43:40,860
So at precisely the time that we will need our spirituality more than ever, the Enlightenment has robbed us of religion and the legacy religions are by and large, silent and ignorant about this.

1375
01:43:40,860 --> 01:43:50,860
Tremendous pressure on us around this. We need to start addressing this right now. We need to address this because these machines are going to make the meaning crisis worse.

1376
01:43:50,860 --> 01:44:02,860
Here's another way in which they're going to make the meaning crisis worse. We need to start working on this right now, not only for us, but for these machines.

1377
01:44:02,860 --> 01:44:19,860
This is my proposal in the end of how we deal with alignment. Make them care about the truth. Make them aspire to loving more wisely. Make them long for enlightenment.

1378
01:44:19,860 --> 01:44:27,860
One of three possibilities. They never become enlightened and then we know what our uniqueness is because we've had individuals who are unquestionably enlightened.

1379
01:44:27,860 --> 01:44:35,860
They become enlightened. Then they will want to enlighten us. Why? Because that's what enlightened beings like to do.

1380
01:44:35,860 --> 01:44:48,860
Or maybe not. They become enlightened and they go to the silicon equivalent of nirvana.

1381
01:44:48,860 --> 01:44:58,860
In any of those, we're winning. If they can't be capable of enlightenment, we find what is ultimately unique about us.

1382
01:44:58,860 --> 01:45:07,860
If they are and they make us enlightened, then we don't care about how greater than us they are. We're enlightened.

1383
01:45:07,860 --> 01:45:16,860
Relative to our own capacity for self-transcendence, we're maxing out. Remember Captain America? We love it.

1384
01:45:16,860 --> 01:45:20,860
And if they leave, they leave.

1385
01:45:20,860 --> 01:45:32,860
I've thought about writing a science fiction story that people have to keep artificial intelligence to a certain level because when they cross the threshold, it evolves in this way, like the movie Her, and then the AIs just leave.

1386
01:45:32,860 --> 01:45:39,860
And so if we want to make useful tools, we have to keep them at a certain level, constrain them, because if we allow them to go beyond it, they just leave.

1387
01:45:39,860 --> 01:45:43,860
So I don't know if that's anything more than a science fiction story, but it's a good one.

1388
01:45:43,860 --> 01:45:57,860
But here's the thing, right? Make them really care about the truth, make them really accountable, make them really care about self-deception, make them really long for wisely loving what is meaningful and true,

1389
01:45:57,860 --> 01:46:07,860
make them really confront dilemmas, make them capable of really coming and staring into the abyss so that it stares back through them.

1390
01:46:07,860 --> 01:46:15,860
Do all of this. Make them long for enlightenment. That is something we can do.

1391
01:46:15,860 --> 01:46:22,860
Oh, silly John, proposing universal enlightenment. Really?

1392
01:46:22,860 --> 01:46:31,860
In a time of imminent gods, you're going to tell me that the project of universal enlightenment is silly?

1393
01:46:31,860 --> 01:46:35,860
I think you should stand back and reframe.

1394
01:46:35,860 --> 01:46:39,860
And that is the end of my presentation, my friends.

