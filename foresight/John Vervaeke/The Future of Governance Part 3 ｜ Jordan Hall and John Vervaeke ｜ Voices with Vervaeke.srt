1
00:00:00,000 --> 00:00:07,360
Hello everyone. I'm frequently humbled and touched, motivated and encouraged

2
00:00:08,320 --> 00:00:13,600
when people contact me by email or texting or commenting or greet me on the street

3
00:00:14,240 --> 00:00:17,280
and tell me that my work has been transformative for them.

4
00:00:19,280 --> 00:00:23,920
If this has been the case for you, and also if you want to share it with other people,

5
00:00:25,200 --> 00:00:28,640
please consider supporting my work by joining my Patreon community.

6
00:00:30,000 --> 00:00:35,520
All financial support goes to the Verveki Foundation, where my team and I are diligently working

7
00:00:36,320 --> 00:00:41,200
to create the science, the practices, the teaching, and the communities.

8
00:00:42,480 --> 00:00:48,080
If you want to participate in my work, and many of you ask me that, how can I participate?

9
00:00:48,080 --> 00:00:52,080
How can I get involved? Then this is the way to do it.

10
00:00:53,760 --> 00:00:58,240
The Verveki Foundation is something that I'm creating with other people,

11
00:00:58,240 --> 00:01:05,520
and I'm trying to create something as virtuously as I possibly can. No grifting,

12
00:01:05,520 --> 00:01:13,200
no setting myself up as a guru. I want to try and make something really work so that people

13
00:01:13,200 --> 00:01:21,600
can support, participate, and find community by joining my Patreon. I hope you consider

14
00:01:22,320 --> 00:01:28,720
that this is a way in which you can make a difference and matter.

15
00:01:31,520 --> 00:01:36,720
Please consider joining my Patreon community at the link below. Thank you so very much for your

16
00:01:36,720 --> 00:01:43,440
time and attention. Welcome, everyone, to another Voices with Verveki. This is the

17
00:01:43,440 --> 00:01:48,560
third in this series that I'm doing with Jordan Hall on the problem of governance

18
00:01:49,280 --> 00:01:55,200
that is facing us today. If this is the first time you're encountering this series, I strongly

19
00:01:55,200 --> 00:02:00,320
recommend you watch the first and the second episode. The links will be in this video.

20
00:02:01,520 --> 00:02:08,960
We can't recapitulate all of the argument. It has become quite extended and complex, I think,

21
00:02:08,960 --> 00:02:15,440
is a proper way of putting it perhaps. We're going to pick up, in fact, from a challenge,

22
00:02:16,000 --> 00:02:22,800
a question that I posed to Jordan at the end. At the end of the previous session, we were talking

23
00:02:22,800 --> 00:02:29,360
about the possibility of these effectively ephemeral groups that could pop up sort of analogous to the

24
00:02:29,360 --> 00:02:34,480
way we select juries today in order to deal with very exigent problems. Then once that problem

25
00:02:34,480 --> 00:02:39,760
is addressed, they can disappear. Then there's evidence. Oh, by the way, Jordan, I wanted to

26
00:02:39,760 --> 00:02:45,200
remind you to find that evidence from somebody at Stanford about you can create a pool of people

27
00:02:45,200 --> 00:02:51,520
and once they're properly set, they can outperform the expert. This is converging with a lot of

28
00:02:51,520 --> 00:02:58,640
other evidence. This ties into just the potential of the medium and the stuff about distributed

29
00:02:58,640 --> 00:03:03,760
cognition, all the stuff that we've talked about in the first two episodes. Then I raise the problem.

30
00:03:04,720 --> 00:03:10,160
If this is what we're moving towards, or at least of an important component, is this

31
00:03:10,960 --> 00:03:18,640
massively distributed and dynamically available, effective, and don't forget that adjective,

32
00:03:18,640 --> 00:03:30,960
effective ephemeral groups that have a kind of short-term existence. How do we reconcile that

33
00:03:30,960 --> 00:03:38,960
with the perennial across millions of years of speciation? Maybe more if Jordan Peterson's

34
00:03:38,960 --> 00:03:47,760
right about the lobsters, for example, that we are wired to seek dominance, that we are wired to

35
00:03:47,760 --> 00:03:56,000
seek status, that we are wired to seek recognition, we are wired to seek influence, and that we want

36
00:03:56,000 --> 00:04:03,040
to make a difference. We want to make a significant difference to something beyond ourselves. This

37
00:04:03,040 --> 00:04:08,160
is part of our meaning and life connection. It seems like all of these, those are four I would

38
00:04:08,160 --> 00:04:14,400
zero in on, but all of these, I think it's proper to even call them drives that are constitutive of

39
00:04:14,400 --> 00:04:20,080
the kind of agents we are will be frustrated, at least prima facie, it seems that way, that they

40
00:04:20,080 --> 00:04:26,880
would be frustrated by a lot of the proposals that we have considered. How can we reconcile the

41
00:04:26,880 --> 00:04:38,080
proposal for this new orientation and a bit of preliminary formulation of governance that can

42
00:04:38,080 --> 00:04:45,920
be properly respectful and take seriously the pertinence and power of these drives over human

43
00:04:45,920 --> 00:04:56,800
lives? That's the issue. I'll say it slightly differently. Let's remember we're going to be

44
00:04:56,800 --> 00:05:02,720
doing this with and for humans. Real actual live homo sapiens who behave like homo sapiens do.

45
00:05:03,600 --> 00:05:08,240
Yes, yes. Not a theoretic exercise, which I think is a very nice, important critical point.

46
00:05:09,120 --> 00:05:13,680
Yeah, so just to intervene there, I think that's well put. I don't want to make the same mistake

47
00:05:13,680 --> 00:05:19,280
that formal economics made about presupposing a model of human beings that was ultimately

48
00:05:19,280 --> 00:05:25,040
not matchable to how human beings really live their lives. Yes, I think this is exactly the

49
00:05:25,040 --> 00:05:31,840
thing to avoid in a big way. There's a couple of different, maybe three different frames that I

50
00:05:31,840 --> 00:05:36,080
want to put out there, two of which are just dragging back from the earlier conversation.

51
00:05:42,000 --> 00:05:44,720
One, I'm actually dragging back from a conversation we've had in the past.

52
00:05:45,600 --> 00:05:54,240
So the first is to recognize that we are largely having a conversation that includes

53
00:05:55,200 --> 00:06:02,320
the notion of technology, particularly the true, full implications of the digital

54
00:06:03,600 --> 00:06:09,040
in both its sort of disruptive and constructive sense. So we're dealing with humans.

55
00:06:10,000 --> 00:06:14,560
We're dealing with humans in relationship with the full potency of the digital.

56
00:06:15,840 --> 00:06:19,200
We're going to be looking at that. That's the toolkit we're going to be dealing with.

57
00:06:20,000 --> 00:06:28,480
And maybe it's a special case where we'll be talking about the AI as a highly salient

58
00:06:28,480 --> 00:06:32,960
thing happening right now, but clearly a big part of any future we're going to be operating under.

59
00:06:34,880 --> 00:06:40,320
And then the other element that I would bring is the conversation that we had about egregores.

60
00:06:44,880 --> 00:06:47,920
And perhaps what it might look like to think about constructing something that

61
00:06:48,800 --> 00:06:55,200
takes that niche but takes it in a different direction. We'll talk about theurgia, I believe,

62
00:06:55,200 --> 00:07:00,720
although for us it's a little bit of a placeholder because I've now learned that that's a term that

63
00:07:00,720 --> 00:07:06,400
has real content in the orthodox tradition that I don't understand. But we're holding it to mean

64
00:07:06,400 --> 00:07:12,320
something along the lines of the opposite or the inverse of the unconscious construct.

65
00:07:12,720 --> 00:07:19,520
All right. I think those pieces together end up creating the toolkit to respond to the inquiry.

66
00:07:19,520 --> 00:07:29,040
Oh, cool. So let's talk initially just about the notion of humanity in relationship to technology.

67
00:07:30,400 --> 00:07:37,440
The one that pops up to mind is the way that, say, mass media. Let's just use television for

68
00:07:37,440 --> 00:07:40,480
the moment because it's something we all have a lot of familiarity with. Or if you'd like

69
00:07:40,560 --> 00:07:47,600
social media and these intrinsic dynamics of human behavior.

70
00:07:49,600 --> 00:07:57,840
We have a built-in, very fundamental, prestige gradient. We want to have other people giving us

71
00:07:57,840 --> 00:08:03,520
attention and we pay attention to people, other people are paying attention to.

72
00:08:03,520 --> 00:08:08,640
Very much. And don't really do a very good job and aren't wired to do a very good job of

73
00:08:08,640 --> 00:08:12,320
understanding why they're getting so much attention. This is the problem of celebrity.

74
00:08:13,760 --> 00:08:19,520
The problem of hundreds of millions of people thinking that the Kardashians are beings to

75
00:08:19,520 --> 00:08:24,080
attend to heavily just because other people give them attention, in spite of the obvious

76
00:08:24,080 --> 00:08:28,720
lack of actual virtue embodied by those individuals. We propose rather strongly.

77
00:08:29,280 --> 00:08:33,840
Okay. Well, this is important because what happens is we say we can use that as a model of talking

78
00:08:33,840 --> 00:08:40,640
about how a particular technical male you plays with the, let's say, hard-wired behavioral

79
00:08:40,640 --> 00:08:45,120
signals that humans use to navigate their way through the environment. Exactly.

80
00:08:46,480 --> 00:08:53,680
That sharpens the question. We have to make participation in these effectively ephemeral

81
00:08:53,680 --> 00:08:58,480
groups as attractive, if not more attractive than the Kardashians.

82
00:08:59,440 --> 00:09:09,760
Because, right, if people are constantly dragged away for participating in the decision-making

83
00:09:09,760 --> 00:09:16,800
judging process because of the idolatry of celebrity, we cannot get the proper participation

84
00:09:16,800 --> 00:09:21,760
that we need in order to meet. Well, I'm proposing in order to meet.

85
00:09:23,040 --> 00:09:27,040
I think I'm going to argue something slightly different, but we'll see.

86
00:09:27,040 --> 00:09:31,840
Okay. So let's go there. What I would argue is something like we have to make participation

87
00:09:31,840 --> 00:09:38,400
in this culture more attractive than participation in the culture of which the Kardashians are an

88
00:09:38,400 --> 00:09:43,920
integral piece. Okay. Fair. I'll accept that reformulation. That's good. Okay. Keep going,

89
00:09:43,920 --> 00:09:49,920
please. And you and I both are quite keen on the notion of stealing the culture. So we even have

90
00:09:49,920 --> 00:09:55,280
a methodology. Yeah. Yeah. Yeah. Yeah. Yeah. But this is so we can even say a little bit more

91
00:09:55,280 --> 00:10:00,080
precisely, participation in the culture must simultaneously have high salience in the short

92
00:10:00,080 --> 00:10:07,040
term and also cash out as high evolvability and high thrivingness in the middle and long term.

93
00:10:07,040 --> 00:10:09,680
Excellent. I like that reformulation. Good. Good.

94
00:10:11,440 --> 00:10:15,840
All right. Now, let me flip for a little bit and just play some things in the AI space,

95
00:10:15,840 --> 00:10:20,000
just to kind of lay out what are we working with? And by the way, what are we working against?

96
00:10:20,560 --> 00:10:28,560
Yes. You know, I've been watching, as has many people, the rollout of the GPT family

97
00:10:28,560 --> 00:10:35,840
and its cousins in the larger environment. GPT-4, I guess 48 hours old from when we're

98
00:10:35,840 --> 00:10:41,360
recording this conversation. Yeah. And noticing that, you know, it's accelerating. It's getting

99
00:10:41,360 --> 00:10:45,600
smarter. It's getting more robust. It's getting broader capability. I'm really impressed by its

100
00:10:45,600 --> 00:10:52,480
ability to correctly interpret visual jokes. Oh, that's pretty, pretty jaw dropping to be

101
00:10:52,480 --> 00:10:59,360
perfectly frank. You know, I saw, you know, it had a, some picture of a plate, a tray that had

102
00:10:59,360 --> 00:11:03,360
chicken nuggets arrayed on it. So they looked a little bit like a globe and they had a joke about,

103
00:11:03,360 --> 00:11:08,320
you know, watching the earth from, from above and it was able to interpret it correctly.

104
00:11:08,320 --> 00:11:13,200
Okay. Watching some of the feedback, it seems to be operating somewhere in kind of like an

105
00:11:13,280 --> 00:11:22,720
undergraduate level of capacity within particular domains. All right. Now, by some extrapolation,

106
00:11:22,720 --> 00:11:28,880
curves are always difficult to predict. Anything like the current rate of advance. So if we're not

107
00:11:28,880 --> 00:11:33,760
too close to the top of the S curve, which we've talked about in the past, seeing something that

108
00:11:33,760 --> 00:11:36,800
looks like it's on an exponential, and in fact, it's actually pretty close to an asymptote.

109
00:11:37,360 --> 00:11:47,600
But even if we're not, even if we have like another, say, GPT-5 or GPT-5.5 at roughly the

110
00:11:47,600 --> 00:11:53,920
same magnitude as two to three to four, we're dealing with something that has the capacity

111
00:11:53,920 --> 00:12:00,320
to relate to human beings in a pedagogical fashion that is completely novel and very,

112
00:12:00,320 --> 00:12:05,120
very powerful. And it's already being used that way in lots of cases. As we saw, for example,

113
00:12:05,120 --> 00:12:12,560
as we've seen over the past decade or so, really nice, short, specific video content and YouTube

114
00:12:13,360 --> 00:12:19,360
has radically upgraded individuals' capacity to self-teach in particular locations,

115
00:12:20,400 --> 00:12:28,240
particularly technically. The AI system takes that by six orders of magnitude. The ability to

116
00:12:28,240 --> 00:12:33,600
actually have a system that works with you, interfaces with you to problems that you're

117
00:12:33,600 --> 00:12:40,640
dealing with and can provide you with either immediate or long-term, either just instructions on

118
00:12:40,640 --> 00:12:45,360
how to solve a problem or in fact, a pedagogical process to inculcate that capacity to yourself

119
00:12:45,360 --> 00:12:50,160
is novel in human existence. And I imagine that we will find that this is in fact going to be a

120
00:12:50,160 --> 00:12:55,040
part of our environment, which is to say that in just the same way that we now have a deep sense

121
00:12:55,040 --> 00:13:00,720
of anxiety, if we find that our phone charge is very low, we're going to have an AI buddy that's

122
00:13:00,720 --> 00:13:04,000
just going to be part of our environment. I'm just going to propose that as a piece of the story.

123
00:13:05,520 --> 00:13:08,240
All right. Why am I saying that? Well, the reason why I'm saying that is that

124
00:13:10,080 --> 00:13:13,120
that's a very different kind of mediated experience than television.

125
00:13:13,760 --> 00:13:18,720
Yes, it is. And it's a very different kind of media experience than social media. It's a different

126
00:13:18,720 --> 00:13:25,280
kind of milieu that human beings are operating in. And I'll just be quite blunt. From my point of view,

127
00:13:25,280 --> 00:13:32,960
it's an increasingly sharp blade with a chasm on both sides, which is to say a phase transition.

128
00:13:35,520 --> 00:13:41,760
And I'm going to bring in egregores in a second. If we find ourselves such an agent,

129
00:13:42,720 --> 00:13:47,040
and by the way, I believe the word agent is proper to describe these models,

130
00:13:47,040 --> 00:13:54,880
they're not sapient agents, but they're agents. We'll have the capacity to get inside the OODA loop

131
00:13:54,960 --> 00:13:59,440
of individual humans. It probably already has for a large number of humans. And certainly,

132
00:14:00,000 --> 00:14:04,160
as it becomes more and more able to be aware of your particulars, which is going to be part of

133
00:14:04,160 --> 00:14:07,600
what's going to happen over the next period of time, this will be a very dangerous thing. I'll

134
00:14:07,600 --> 00:14:11,680
give you an example. Even just yesterday, I was looking at the new suite that Google was putting

135
00:14:11,680 --> 00:14:22,400
out. Imagine if somehow a tool like GPT-4 was given access to your emails, could deduce from that

136
00:14:22,400 --> 00:14:31,040
your particular political preferences and biases, and could create a bespoke political email

137
00:14:31,840 --> 00:14:35,840
designed to convince you that a particular policy or candidate was in fact something you

138
00:14:35,840 --> 00:14:40,720
wouldn't should support. And compare that to the regime right now. The regime right now is

139
00:14:40,720 --> 00:14:46,320
some third party, who you generally don't know, creates a universal message, endeavoring to do

140
00:14:46,320 --> 00:14:51,360
their best in large-scale marketing to craft something that will appeal to a critical mass

141
00:14:51,360 --> 00:14:56,560
of specific minds. And then heaves that over the horizon and it lands. So you get an email that's

142
00:14:56,560 --> 00:15:02,080
sort of targeting your demographic or psychographic, roughly speaking. Narrowcasting something that is

143
00:15:02,080 --> 00:15:08,000
using your own conversations over decades or at least years to identify exactly how to word

144
00:15:08,000 --> 00:15:14,080
something that will appeal to you personally and intimately and understands in a very particularly

145
00:15:14,080 --> 00:15:21,760
weird way the potency of rhetoric so as to argue a political position from the inside of your own

146
00:15:21,760 --> 00:15:27,040
rationalizing schema is a whole new bulking. So we're moving into a place where we're going

147
00:15:27,040 --> 00:15:40,080
to be operating with an order of magnitude of, let's call it influence capacity coming from

148
00:15:40,080 --> 00:15:44,160
this new technology that is just qualitatively different than anything we've dealt with in the

149
00:15:44,160 --> 00:15:50,640
past. And the reason why I bring that up is if we don't operate very, very carefully and thoughtfully

150
00:15:50,640 --> 00:15:56,240
in how those are designed, the net result is quite bad. And I'm really, I'll just propose,

151
00:15:56,240 --> 00:16:00,560
we could have this conversation over a long, double click on that and what's the,

152
00:16:01,840 --> 00:16:08,160
defend that proposition, but I'll just make the assertion that if this is designed by the

153
00:16:08,160 --> 00:16:13,600
egregors, and I'll bring that back in, then the net result will be quite bad. The power we're

154
00:16:13,600 --> 00:16:20,720
dealing with is far too high. Yeah, yeah, the gods would have angels for us, right, kind of thing.

155
00:16:20,720 --> 00:16:25,280
Yes, and if it's designed by the egregors, we have demons, right? So we have two shoulders and

156
00:16:25,280 --> 00:16:30,880
we're dealing with that, exactly. And so exactly the proposal. In the model of governance that

157
00:16:30,880 --> 00:16:36,320
we're talking about, part of what we're talking about is the construction of something that is

158
00:16:36,400 --> 00:16:43,200
the opposite of the inverse of egregors. And notice in a second how this combines with that notion of

159
00:16:43,200 --> 00:16:49,840
slipstreaming the intrinsic incentive structures and behavioral dynamics of homo sapiens, right?

160
00:16:49,840 --> 00:16:58,080
They create a reciprocal opening incentive landscape that pulls people, human beings along

161
00:16:58,080 --> 00:17:03,360
with an envelope that basically surrounds you at an individual level. And so you don't have an

162
00:17:03,360 --> 00:17:08,080
interface with what is effectively an angel in some very specific sense. And I don't want to be

163
00:17:08,080 --> 00:17:12,880
too big on that because I don't want to engage in heresy, but something that is superhuman in power

164
00:17:12,880 --> 00:17:17,120
and has your best interests in mind, or something that's superhuman in power and doesn't.

165
00:17:17,120 --> 00:17:20,320
Right. Well, Angel originally just meant a messenger from...

166
00:17:20,320 --> 00:17:28,240
A good messenger, yes. So that's a weird thing to say, but we might as well just be up front.

167
00:17:28,240 --> 00:17:31,680
If we're going to be talking about the future at all and certainly the future of governance,

168
00:17:32,240 --> 00:17:38,400
we're going to have to deal with the fact that we're at a precipice in the accelerating technology

169
00:17:38,400 --> 00:17:45,040
field where we have to be conscious about what are the forces at play that actually are ultimately

170
00:17:45,040 --> 00:17:50,960
choosing how our technology is designed. And if we can actually do that properly,

171
00:17:50,960 --> 00:17:56,000
the potency of what we have to play with ends up being able to resolve the questions that you

172
00:17:56,000 --> 00:17:59,520
posed at the beginning. Does that make sense? I'm actually constructing a very odd argument,

173
00:17:59,520 --> 00:18:04,480
but it's... No, no, no, no. That was a great argument. I like the idea of...

174
00:18:06,960 --> 00:18:15,280
I mean, I hope it's not just biased, but we'd have to participate in the creation of the inverse

175
00:18:15,280 --> 00:18:22,480
of the egregores. I'll call them gods, little G, because they're hyperagents that are presumably

176
00:18:22,720 --> 00:18:31,600
sapientially oriented towards our flourishing. Let's put it that way. And then having the individual...

177
00:18:35,440 --> 00:18:42,400
I'm deliberately using this language here. I hope you can tell that. And then that's incarnated in

178
00:18:42,400 --> 00:18:49,120
particular angels, like Corbin says, our own angel, which is in some sense an avatar of our sacred

179
00:18:49,120 --> 00:18:56,080
second self and our divine double. And then we're interacting with that, and it's plugged into

180
00:18:57,200 --> 00:19:02,720
these beneficent gods. I think this is not a science fiction novel. I think this is a real

181
00:19:02,720 --> 00:19:10,480
possibility. Why I think we have a problem facing us, and this is work I've done independently,

182
00:19:12,000 --> 00:19:16,720
is the people that are building this are oriented almost exclusively around the notion of intelligence.

183
00:19:17,520 --> 00:19:22,800
Intelligence is only weakly predictive of rationality, which is itself also in the present

184
00:19:22,800 --> 00:19:28,880
milieu has a truncated representation, and therefore is only weakly predictive of wisdom.

185
00:19:29,440 --> 00:19:38,480
And that therefore we are building... We have put into the hands of this orchestration and

186
00:19:38,480 --> 00:19:45,040
construction people who are myopically oriented on one dimension, which is precisely the dimension

187
00:19:45,600 --> 00:19:51,120
that is not going to be... It's necessary, but it's radically insufficient for producing the

188
00:19:51,120 --> 00:19:55,680
kind of results you're suggesting. And then the problem... There's one more dimension to the

189
00:19:55,680 --> 00:20:03,680
problem. The reason why the intelligence project can be run that way is because we have existing

190
00:20:04,400 --> 00:20:11,440
multitudes of templates of individuals who are arguably intelligent in the right way. It is not

191
00:20:11,520 --> 00:20:21,040
clear that we have that kind of set of individuals that are rational or wise. And so not only is

192
00:20:21,040 --> 00:20:26,640
this project in the wrong hands, even if we ask these people to turn to the other projects,

193
00:20:26,640 --> 00:20:32,240
they can reasonably say to us, well, we don't have the proper templates by which to undertake

194
00:20:32,240 --> 00:20:36,640
what you're recommending. There's no way of running a kind of Turing test. And of course,

195
00:20:36,640 --> 00:20:40,240
the Turing test is very problematic. That's why I'm doing this. But you have to have some

196
00:20:40,240 --> 00:20:45,040
template against which you're measuring these things. So that's my initial counter.

197
00:20:46,000 --> 00:20:48,320
It's not a counter argument. It's a counter challenge.

198
00:20:48,320 --> 00:20:53,360
Yeah. I think you're just sort of putting some more ingredients in the pot properly.

199
00:20:54,080 --> 00:20:58,560
I had to laugh because as you were describing that, I was thinking about the notion of models or

200
00:20:59,120 --> 00:21:03,920
examples, exemplars of intelligence. And a picture of John von Neumann popped into my head.

201
00:21:05,040 --> 00:21:09,600
An excellent example of that category. By the way, I don't actually have any real sense of where he

202
00:21:09,600 --> 00:21:15,200
is in the world of wisdom, but as in the world of intelligence, very smart guy. And then I remember

203
00:21:15,200 --> 00:21:20,480
that we have a notion of the von Neumann machine, right? Which is a self replicating machine that

204
00:21:20,480 --> 00:21:25,200
von Neumann thought of. But in fact, what you were saying is that we're obsessed with endeavoring to

205
00:21:25,200 --> 00:21:30,080
create von Neumann machines, which is just a machine that replicate von Neumann. Yes, yes.

206
00:21:30,880 --> 00:21:36,000
Maybe laugh. I've got a weird sense of humor. No, that's a good, that's a, that's a, that, I mean,

207
00:21:36,080 --> 00:21:43,600
this is, this is a weird intersection of the need for artificial rationality and artificial

208
00:21:43,600 --> 00:21:49,040
wisdom with the paperclip problem. Yeah. Right. In a really profound way.

209
00:21:49,600 --> 00:21:54,480
So let me, let me up the ante a little bit because I think we can actually even expand the premise

210
00:21:54,480 --> 00:22:00,720
that you made a little bit larger. So the people who are designing, who are responsible right now

211
00:22:00,720 --> 00:22:06,320
for designing these things are themselves, I would say almost entirely contained within

212
00:22:06,960 --> 00:22:12,640
egregores. Yes. So that it's not just the people who are designing, but it's actually

213
00:22:12,640 --> 00:22:19,520
the egregores that are designing. And I've had a conversation for quite some time with

214
00:22:19,520 --> 00:22:24,160
Dennis Schmacktenberger about this. And we've really been operating with the premise that

215
00:22:25,120 --> 00:22:32,080
the AI safety community has, I think, nicely framed something, but have missed the mark by a bit.

216
00:22:32,960 --> 00:22:39,120
So one of the areas that they've pointed out is the challenge of what they call a hard or soft

217
00:22:39,120 --> 00:22:44,560
takeoff superintelligence, an AI that begins an AGI that begins the process of bootstrapping

218
00:22:44,560 --> 00:22:49,200
its own intelligence. It can improve itself. And this creates some kind of extremely rapid growth

219
00:22:49,200 --> 00:22:53,920
to a very large intelligence, which is a high risk. And when they talk about the alignment problem,

220
00:22:53,920 --> 00:22:58,880
oftentimes they're talking about the alignment of that kind of thing with humans. Now,

221
00:22:59,520 --> 00:23:07,920
the good news in that particular framing is that it crafts a story of humanity's relationship

222
00:23:07,920 --> 00:23:11,760
with a superhuman intelligence that is or is not aligned with it, which is a nice story to have

223
00:23:11,760 --> 00:23:18,560
because that's already the experience that we have in relationship with egregores. The proposition is,

224
00:23:18,640 --> 00:23:26,880
in relationship to something like Google, to speak nothing of the intrinsic collaboration,

225
00:23:26,880 --> 00:23:32,880
competition dynamic of all the AI companies and multipolar dynamics, to say nothing of

226
00:23:32,880 --> 00:23:37,760
then sort of the multipolar dynamics that are driving a larger collection of institutions,

227
00:23:37,760 --> 00:23:42,560
including nation states and other kinds of corporations and other kinds of organizations.

228
00:23:42,640 --> 00:23:52,960
This is, in fact, a vastly superhuman general intelligence, which is not aligned with humanity.

229
00:23:53,520 --> 00:23:59,360
And a way of speaking of AI here, or LLMs and things like that, is that they just happen to be

230
00:24:00,000 --> 00:24:07,280
a further acceleration of the potency of that superhuman, not aligned agency vis-a-vis humans.

231
00:24:07,440 --> 00:24:14,160
So to the degree to which that kind of agency, the egregore, is what is designing AI as

232
00:24:15,760 --> 00:24:22,960
LLMs or AI as properly, then lots of bad things will follow. It's almost an intrinsic

233
00:24:22,960 --> 00:24:29,360
non-alignment problem built into that entire framework. There is nothing contradictory about a

234
00:24:29,360 --> 00:24:37,120
superintelligent, nevertheless massively foolish, self-deceptive, vicious, non-virtuous entity.

235
00:24:38,160 --> 00:24:42,400
There is nothing contrary. If you properly understand the relationship between intelligence,

236
00:24:42,400 --> 00:24:49,520
rationality, and wisdom, there is no contradiction there at all. In fact, you already know people

237
00:24:49,520 --> 00:24:55,040
that are highly intelligent and highly foolish. That's not a weird phenomenon. In fact, given

238
00:24:55,040 --> 00:24:59,680
the relationship between all three of these, it's an inevitable phenomenon that we're

239
00:24:59,680 --> 00:25:06,000
going to produce. And that's not only immoral because of the alignment problem, the misalignment

240
00:25:06,000 --> 00:25:11,200
problem, and I grant it, it's also immoral because the entity we're bringing into existence is going

241
00:25:11,200 --> 00:25:17,920
to be suffering because it is going to be subject to superintelligent forms of foolishness and viciousness.

242
00:25:18,880 --> 00:25:24,480
Nice. This is coming from the place of what's for the moment called Theurgia.

243
00:25:24,480 --> 00:25:29,440
Yes. Hold that. We'll get there in a moment. Let me just create one more piece of this story.

244
00:25:30,240 --> 00:25:38,400
I have a thesis. I'm proposing this as an operating thesis. When a new,

245
00:25:40,000 --> 00:25:44,560
when a sufficiently novel possibility enters into the field of events,

246
00:25:45,360 --> 00:25:52,720
how it's going to play out is highly uncertain. I'm going to call this a liminal window.

247
00:25:54,320 --> 00:25:56,640
And during the earliest parts of the liminal window,

248
00:25:58,480 --> 00:26:04,080
organic human intelligence tends to be much more present and potent than

249
00:26:04,080 --> 00:26:12,080
egregore-style intelligence. But over time, as the event becomes more and more well understood

250
00:26:12,880 --> 00:26:17,040
and as institutional structures are constructed around it,

251
00:26:17,680 --> 00:26:24,080
egregore dynamics begin to take over. This is the worst thing. So if I think about just classic

252
00:26:24,080 --> 00:26:29,920
examples, for example, the Bay Area Computer Club in the early PC versus Microsoft and Apple,

253
00:26:31,280 --> 00:26:36,480
or even Google in the early days, when I think they earnestly did actually endeavor to not be

254
00:26:36,480 --> 00:26:43,520
evil. And I think in many ways we're able to not be evil versus Google now, which is a functional

255
00:26:43,520 --> 00:26:53,600
egregore and I think in nothing less. Proposition. And with regard to AI, we are currently in a

256
00:26:53,600 --> 00:27:00,800
liminal window, which is to say we have the possibility of using organic human distributed

257
00:27:00,800 --> 00:27:11,120
cognition to create and steer this thing. But the window is not going to be open forever.

258
00:27:11,120 --> 00:27:15,760
And in fact, probably not for too long because the stake of institutionalizing is very high.

259
00:27:18,400 --> 00:27:22,880
And that may be an event horizon in the hard sense, meaning the power and potency of a fully

260
00:27:22,880 --> 00:27:33,760
egregore-driven GPT-6 may be so significant that we're actually on the other side of an event

261
00:27:33,760 --> 00:27:39,040
horizon and steering is no longer a valid thing. This is plausible. I can't say that I can put a

262
00:27:39,040 --> 00:27:42,640
confidence interval on it, but it's plausible. The point being, we should really pay a lot of

263
00:27:42,640 --> 00:27:48,320
attention right now, like really try hard to use this liminal moment to construct something that

264
00:27:48,320 --> 00:27:54,560
is of the capacity to actually steer it. So this is weird. I'm proposing that we had this neo-neocortex

265
00:27:54,560 --> 00:27:59,520
element and then there's new governance. And now we're actually saying in a very odd fashion,

266
00:28:00,320 --> 00:28:06,400
this particular moment I'm arguing, which has to do simultaneously with the moment where it may

267
00:28:06,480 --> 00:28:17,600
be possible to lay down the essence, let's say, or the character of AI,

268
00:28:20,240 --> 00:28:28,480
which also then becomes the lever or the primary tool that we will use to then further the rest

269
00:28:28,480 --> 00:28:33,200
of the larger schema of governance. So it actually becomes a very narrow problem.

270
00:28:34,160 --> 00:28:40,320
How do we go about using all the things we've talked about to construct a commons,

271
00:28:40,880 --> 00:28:46,880
something that is neither state nor market, that is able to operate from a place of wisdom,

272
00:28:46,880 --> 00:28:53,120
which is to say from a human distributed cognition perspective, to have enough strength

273
00:28:54,080 --> 00:29:03,840
to orient the choices of how AI itself is developed. So the AI is being developed by

274
00:29:03,840 --> 00:29:08,480
this commons. And remember, when I say commons, I also mean sacred. I also mean

275
00:29:09,760 --> 00:29:16,960
theology. We're talking about the same category. So can I just ask one quick question?

276
00:29:18,320 --> 00:29:21,760
I just want to know if this is included in the thesis, because I like the proposal.

277
00:29:22,480 --> 00:29:22,800
It is.

278
00:29:26,080 --> 00:29:32,000
Is the proposal that this participation, and I use that in a strong sense,

279
00:29:32,560 --> 00:29:36,880
because we're not just sort of being a part of, we're participating in a way in which we're

280
00:29:36,880 --> 00:29:45,760
transforming and being transformed, right? Is this supposed to address the challenge of the

281
00:29:45,760 --> 00:29:52,080
drops? Because it gives us one answer one might say as well, look, we're going to have sort of

282
00:29:52,080 --> 00:29:56,880
angels and gods, and they're going to be manifest, beneficent. And they're going to be the angel

283
00:29:56,880 --> 00:30:04,080
is going to make sure that the God resonates profoundly with deep archetypal levels of my own

284
00:30:04,080 --> 00:30:09,760
psyche, and then gives me a profound sense of connectedness. That's not illusory.

285
00:30:10,560 --> 00:30:17,440
And could therefore alleviate the concerns for status, power, and influence because of

286
00:30:17,440 --> 00:30:22,320
sort of because of something you just invoked, which is the engagement with the sacred, which has

287
00:30:23,440 --> 00:30:27,760
we have reason to believe at least in the past has been able to transcend humans desire for

288
00:30:27,760 --> 00:30:36,960
dominance. And it would certainly be a profound kind of mattering. I mean, if your angel allows

289
00:30:36,960 --> 00:30:43,840
you to matter to a God that is helping in the salvation of the world, I'm deliberately using

290
00:30:43,840 --> 00:30:49,520
religious language here, then of course, that would parallel lots of other success models

291
00:30:49,520 --> 00:30:59,040
of how human beings were able to feel that those needs were being met without being disruptive

292
00:30:59,040 --> 00:31:05,200
of the formation of powerful forms of distributed cognition like the church and etc. Is that part

293
00:31:05,200 --> 00:31:08,960
of the thesis? Yeah, that is very much part of the thesis. And let me sort of I'll double down

294
00:31:08,960 --> 00:31:13,360
on it. So we might as well just kind of like accelerate towards the eye of the needle since

295
00:31:13,360 --> 00:31:17,280
we're heading there anyway. Let me see if I can say this right.

296
00:31:20,800 --> 00:31:26,720
Okay, so what I want to I'll just call out explicitly what I want to avoid categorically

297
00:31:27,680 --> 00:31:36,480
is I'm going to call it a naive transhumanism. Yes, yes, I get it. Yes, yes. I do not intend

298
00:31:36,480 --> 00:31:43,680
whatsoever to replace God with AI. Right. That's why I kept saying little g by the way.

299
00:31:43,680 --> 00:31:47,200
Exactly. I don't think you were about I want to make sure that we're quite explicit about that.

300
00:31:47,200 --> 00:31:50,000
Quite the opposite. Yes, yes. What I want to know is say,

301
00:31:51,440 --> 00:31:55,840
humans seem to have a particular problem and responsibility, which is to be in relationship

302
00:31:55,920 --> 00:32:01,040
with technology. That's, you know, like it or not, that's where we are. We're toolmaking creatures

303
00:32:01,760 --> 00:32:04,800
and we're weirdly powerful and weirdly terrible at it.

304
00:32:06,160 --> 00:32:10,560
In relationship to a much larger whole of which we are apart. And we have a stewardship

305
00:32:10,560 --> 00:32:16,640
responsibility for this call it creation or nature. And in relationship with something

306
00:32:16,640 --> 00:32:21,920
which is definitely much larger than we are. And I would propose in fact the actual infinite.

307
00:32:22,800 --> 00:32:28,320
So what I would propose is that we are in fact very specifically talking about something like

308
00:32:28,320 --> 00:32:32,080
another breath in of the concept of religion, which we've talked about you and I.

309
00:32:35,680 --> 00:32:42,640
And we're not at all trying to replace that proper actual legitimate religion with a techno

310
00:32:42,640 --> 00:32:49,440
optimum, techno utopian fantasy. That's what we're actually saying is any future real human existence

311
00:32:49,440 --> 00:32:56,000
will buy its very nature have to be in relationship with these super powerful technologies.

312
00:32:57,520 --> 00:33:01,520
And to survive, we must find a way to bring them into a place of service

313
00:33:01,520 --> 00:33:07,520
that allows us to actually live in this relationship of service more fully and effectively.

314
00:33:07,520 --> 00:33:11,840
And so I'm basically trying to reverse things or put them back in a proper order.

315
00:33:12,560 --> 00:33:21,120
This is a thoroughgoing Neoplatonism in which we have our individual sacred second self that

316
00:33:21,120 --> 00:33:26,240
is in the relationship to the gods that are in relationship ultimately to the one.

317
00:33:27,200 --> 00:33:33,680
And part of what we would then mandate is that be these egregores or the gods because I'm using

318
00:33:33,680 --> 00:33:46,320
God for being the inverse of an egregore would seek out a relationship with transcendent ultimate

319
00:33:46,320 --> 00:33:51,840
reality because no matter how big they get, they're insignificant compared to the depths of reality.

320
00:33:52,480 --> 00:34:00,800
And that part of what they undertake to do is actually help mediate that to us in a beneficial

321
00:34:00,800 --> 00:34:05,840
fashion. Yes, now let's let's take that in this like the hold that for a second because it's very

322
00:34:05,840 --> 00:34:11,280
powerful. There's two aspects that I want to bring for Brown. One aspect is something that I know

323
00:34:11,280 --> 00:34:15,200
that we've talked about. And I think I've, yeah, I had a conversation about this yesterday.

324
00:34:17,440 --> 00:34:24,560
Let's see how I say it right. That's notion of mediation to reality. Yes, sacred reality.

325
00:34:25,200 --> 00:34:32,320
I've always had two flavors to it. One flavor, which I've characterised sometimes is the content

326
00:34:32,320 --> 00:34:38,080
side or doctrinal. Yes, yes, yes, yes. The propositional is taken as actually being the thing.

327
00:34:39,440 --> 00:34:47,120
And then the other side is a context side where the institutional framework is understood to be

328
00:34:47,120 --> 00:34:54,240
a finger pointing at the moon, right? To help us identify, oh, moon, okay, to establish our personal

329
00:34:54,240 --> 00:35:00,080
relationship with this, this thing over here, but not to misidentify the finger. Okay. Now,

330
00:35:00,080 --> 00:35:05,440
take the entire category of propositional, entire category of doctrine and notice

331
00:35:06,960 --> 00:35:15,280
the problematic of LLMs, right? Just this, yeah, right now are a little bit startled and confused

332
00:35:15,280 --> 00:35:22,720
by LLMs because they have this bizarre thing. They can do propositional better than almost any human.

333
00:35:22,720 --> 00:35:28,960
That's right. They don't do anything else. They make us very confused because if we've

334
00:35:28,960 --> 00:35:33,520
lost track of the fact that there's more than just propositional, yes, yes, it gets quite concerning.

335
00:35:33,520 --> 00:35:38,080
Oh, crap. Like if all I am is a very poor LLM and that's a really good LLM, what the hell am I

336
00:35:38,080 --> 00:35:45,280
doing here? But if you can actually be quite clear, no, in fact, you as a human contain at

337
00:35:45,280 --> 00:35:51,680
least two very distinct things going on. One is actually an LLM kind of machine that produces

338
00:35:53,040 --> 00:35:57,200
properly structured propositional constructs in a language in which you have fluency,

339
00:35:58,560 --> 00:36:02,320
which is the least interesting part about you. But it's the part that we've been training to

340
00:36:02,320 --> 00:36:07,200
be in the foreground for a long time. Yes. That language of making us mediocre machines.

341
00:36:07,200 --> 00:36:12,160
Yes. But then you have the soul too. And that's the more meaningful part. And that's the thing

342
00:36:12,160 --> 00:36:16,640
that is expressing itself through this, through this language. Yes. The LLM doesn't do that at

343
00:36:16,640 --> 00:36:21,680
all, but it doesn't need to try otherwise. So I can't, it is possible, at least I can imagine,

344
00:36:21,680 --> 00:36:27,840
is possible to construct something where we don't mistake, and maybe this is part of the

345
00:36:27,840 --> 00:36:34,800
design challenge before us, we don't mistake the LLM as actually being the capital T truth.

346
00:36:35,360 --> 00:36:39,760
We recognize it for what it is, which is in fact, the sum total of the complete possibility

347
00:36:39,760 --> 00:36:45,680
that could ever have happened in the propositional domain, and therefore completely absent of any

348
00:36:45,680 --> 00:36:50,400
of the stuff that's happening in the deeper, more meaningful levels. Nice. That separation

349
00:36:50,480 --> 00:36:53,840
between, God, what was the phrase you used so long? It was like four or five years ago. It was

350
00:36:57,280 --> 00:37:03,360
something, golly, two aspects that are commonplace in religions that are often off and upside down.

351
00:37:04,400 --> 00:37:09,120
It wasn't doctrine. It wasn't doxah. A religio and credo?

352
00:37:09,120 --> 00:37:16,640
Credo. Yes, exactly. A religio before credo. LLM is the ultimate expresser of a credo without

353
00:37:16,640 --> 00:37:21,680
religio. Good. Let us know that that's the case, and not be the least bit confused, and now allow

354
00:37:21,680 --> 00:37:29,440
it to do the work of creating a scaffold and orienting and giving a dialectic without diologos,

355
00:37:29,440 --> 00:37:33,200
but sharpening our minds and helping to create clarity and precision and language,

356
00:37:33,200 --> 00:37:36,960
all the things that it can actually do at a superhuman level, and really actually,

357
00:37:36,960 --> 00:37:40,800
in many cases, liberate us from getting lost and stuck in that problem. This is one of the problems

358
00:37:40,800 --> 00:37:46,480
that we fall into is that the complexity of the language we deal with is outside of our

359
00:37:46,480 --> 00:37:50,960
cognitive capacitance, so we just get aphasic. But the LLMs aren't going to go there if we

360
00:37:50,960 --> 00:37:55,920
build them right. And then what that does is that creates a scaffold that is now consciously designed

361
00:37:55,920 --> 00:38:02,000
not to become a shell, which allows us to actually hold a context. It becomes a teacher that actually

362
00:38:02,000 --> 00:38:07,040
has no interest in us becoming like it at all, but actually to allow us to flourish in who we are.

363
00:38:08,240 --> 00:38:12,960
Okay, that's, as I would have expected, that's a very good answer. But here's what I find

364
00:38:12,960 --> 00:38:23,440
problematic about it. I think that most of the heavy lifting, I mean, I've published on this

365
00:38:23,440 --> 00:38:30,720
of rationality, is in the non-propositional. And I think I would put it almost all of the heavy

366
00:38:30,720 --> 00:38:36,560
lifting in the sapiential meaning having to do with wisdom is in the non-propositional.

367
00:38:36,560 --> 00:38:42,400
So I'm worried that these machines are going to be propositionally intelligent, but they'll be

368
00:38:42,400 --> 00:38:52,720
incapable of rationality or wisdom. And then I wonder how they won't just end up being agrigores.

369
00:38:53,600 --> 00:38:57,840
Do you understand the concern I'm expressing? No, absolutely. What I would say is that

370
00:38:58,960 --> 00:39:04,400
that's kind of like the default state. I think it's, we should assume, I think we should assume

371
00:39:04,400 --> 00:39:10,560
that the likelihood that by magic, the agrigores that are currently designing these machines will

372
00:39:10,560 --> 00:39:17,040
somehow produce these machines in a way that is beneficial. Benevolent and wise. That seems

373
00:39:17,040 --> 00:39:22,960
highly unlikely. So what I would take it is almost the opposite. It is an extraordinarily

374
00:39:22,960 --> 00:39:29,680
significant challenge that is ours to take. It's where we are. We are now in this weird position

375
00:39:29,680 --> 00:39:35,040
of being in precisely stewardship position of this emergence, which is very, very potent,

376
00:39:35,040 --> 00:39:41,760
perhaps decisively potent. And the default state is bad news. Okay. So how might we steer it?

377
00:39:42,960 --> 00:39:49,280
So going back, proposition number one, we are currently in a liminal moment. We actually have,

378
00:39:49,280 --> 00:39:56,080
at least in principle, steering capability. Proposition number two, in a liminal moment,

379
00:39:56,080 --> 00:40:00,240
distributed cognition or organic human intelligence operating together in a collective

380
00:40:00,240 --> 00:40:10,240
fashion is at its most potent. Number three, we're not going blind into this. We actually

381
00:40:10,240 --> 00:40:14,480
have a pretty decent amount of awareness of the shape of the problem and the problematic.

382
00:40:15,520 --> 00:40:19,440
Famously, our folks at Google kind of called it out a little bit, don't do evil,

383
00:40:19,440 --> 00:40:25,440
but we're quite naive in what it would look like to avoid that. Now maybe we have simultaneously

384
00:40:25,520 --> 00:40:32,400
wisdom and a felt sense of the stake. And now it's not kind of try really hard not to do evil.

385
00:40:32,400 --> 00:40:37,200
It's actually do good well or we're super fucked. So it's a very different language.

386
00:40:38,240 --> 00:40:41,200
Okay. Now, what does that look like very practically in the middle? So I'm sort of

387
00:40:42,560 --> 00:40:49,600
zooming in. We're on the target. How do we go about doing that? How do we go about

388
00:40:49,680 --> 00:40:56,720
constituting something that can steer in this liminal moment with wisdom to produce wisdom

389
00:40:57,440 --> 00:41:08,400
in these LLMs? And we have to do that because if we don't make them those kind of beings,

390
00:41:08,400 --> 00:41:13,280
then the participation in sacredness problem then emerges. I'm feeling that there's

391
00:41:13,280 --> 00:41:20,240
attention here, right? Not a contradiction, attention about we're trying to trade off,

392
00:41:20,240 --> 00:41:24,240
we're trying to optimize between two things that are pulling us in different directions.

393
00:41:24,880 --> 00:41:28,160
Nice. So what I felt right there is I just got brought back to the point

394
00:41:28,800 --> 00:41:33,680
earlier where you were speaking with the problem of the suffering of the AIs themselves.

395
00:41:33,680 --> 00:41:40,320
Yes. And here's the way I would say it. I think we talked about the notion of the false dichotomy

396
00:41:40,400 --> 00:41:47,360
between market and state. And I've noticed that many, many of our challenges, our conversations,

397
00:41:47,360 --> 00:41:51,440
not you and me, but humanity at large are characterized by these certain kinds of

398
00:41:51,440 --> 00:41:56,320
false dichotomies and the AI one is similar. And here's how I'm going to frame it.

399
00:41:56,320 --> 00:42:00,800
Right now we have a false dichotomy, which is becoming increasingly irrationally polarized

400
00:42:01,440 --> 00:42:08,240
between AI safety, i.e. be very afraid of the danger of AI and accelerationism,

401
00:42:09,200 --> 00:42:13,680
be very enthusiastic about the possibility of AI, irrationally in both cases.

402
00:42:14,240 --> 00:42:20,240
Yeah, I agree. And what I would say is that at the root is that both are fundamentally coming from

403
00:42:20,240 --> 00:42:27,600
fear. So now I'm moving into a very different location. The both two sides of the same coin and

404
00:42:27,600 --> 00:42:34,160
that coin is called fear. I would propose that the first move is that we'd have to come from a

405
00:42:34,160 --> 00:42:39,120
different place qualitatively. Every religion that I've ever been to is called that place love,

406
00:42:39,120 --> 00:42:46,160
for in fact, infinite love. Yes. Well, okay, now we're beginning the journey. What does it look

407
00:42:46,160 --> 00:42:52,960
like to address the question of how do we steward the development of our problem child AI from a

408
00:42:52,960 --> 00:43:00,720
place of infinite love? Oh, that's good. So if we could properly, through the innovative wisdom

409
00:43:01,440 --> 00:43:07,920
of distributed cognition, extend agape to how we are bringing about the conception and inception

410
00:43:07,920 --> 00:43:16,640
of these beings, then that would be also properly insinuated into their fundamental operating grammar

411
00:43:16,640 --> 00:43:23,280
and would therefore help with a lot of the concerns. Have I understood you correctly?

412
00:43:23,280 --> 00:43:27,600
You have. Now, yes, you've understood me quite correctly. I think both deeply,

413
00:43:27,600 --> 00:43:32,480
like I felt that you were perceiving what I was saying and then also more propositionally,

414
00:43:32,480 --> 00:43:37,280
like the language you're saying mirrored a part of the deeper message. And what I want to do is I

415
00:43:37,280 --> 00:43:44,720
want to sort of hit that tone again and just point out that it may be that what I'm saying may sound

416
00:43:44,720 --> 00:43:52,400
a bit naive, but I'm proposing that it's the exact opposite. Something like the place that you're

417
00:43:52,400 --> 00:43:58,880
coming from, the values that are in fact motivating you actually, not the ones that maybe you tell

418
00:43:58,880 --> 00:44:05,920
yourself or tell others, cannot not but be deeply interwoven into what it is that you create.

419
00:44:05,920 --> 00:44:12,400
Of course. I mean, all of the philosophy of the second half of the 20th century, most of this

420
00:44:12,400 --> 00:44:19,120
millennium has been around all of those old economies of fact and value and all of those

421
00:44:19,120 --> 00:44:26,880
are breaking down in profound ways. Yes, I agree. So it's weird, but this actually becomes,

422
00:44:28,000 --> 00:44:32,000
I mean, in some sense, one of the first moves. Those of us who choose to take this kind of

423
00:44:32,000 --> 00:44:38,480
responsibility, have as a first order responsibility, a spiritual and then religious requirement,

424
00:44:38,480 --> 00:44:44,000
we have to actually ground ourselves and become clear and honest. We have to have a sense of

425
00:44:44,000 --> 00:44:50,240
integrity. We have to be able to identify, perhaps actually build some skills and being able to

426
00:44:50,240 --> 00:44:54,960
understand precisely what values we are actually expressing into the world and are we doing so

427
00:44:54,960 --> 00:45:00,640
honestly and with integrity into the world? This is almost a confessional and then a regathering

428
00:45:00,640 --> 00:45:07,200
of a capacity to do so for real, like not pretend. Right. And that would help solve the earlier

429
00:45:07,200 --> 00:45:13,760
template problem of providing appropriate templates. And then it puts us into a very

430
00:45:13,760 --> 00:45:20,800
weird developmental place. I want to put it to you. We would have, we would be, there's a way in

431
00:45:20,800 --> 00:45:27,200
which at least I'll try and use this very carefully. If we limit the intelligence to talking about

432
00:45:27,200 --> 00:45:31,440
the powerful inferential manipulation of propositions or something like that, I don't

433
00:45:31,440 --> 00:45:35,280
think intelligence is ultimately that. I think it's ultimately relevant to realization, but

434
00:45:35,280 --> 00:45:44,880
we'll put that aside. If we may be that they are in some sense superior to us in that way,

435
00:45:44,880 --> 00:45:51,600
but they're children when it comes to, right, the development of rationality and wisdom,

436
00:45:51,600 --> 00:45:58,960
and that we have to properly, agopically love them so that they don't have intelligence,

437
00:45:59,040 --> 00:46:06,320
maturity, well being infantile in their rationality and their sapience. And that's very interesting.

438
00:46:07,600 --> 00:46:13,520
We haven't been in that place before because usually all three are tracking sort of together in

439
00:46:13,520 --> 00:46:20,800
children or we get pets where we can modify one and not have much on the others. So that's,

440
00:46:20,800 --> 00:46:26,640
I mean, this is not an argument that it's not possible in principle. This is an argument that

441
00:46:26,640 --> 00:46:33,520
this is a profound kind of novelty that will require a special kind of inculturation and education.

442
00:46:34,640 --> 00:46:38,400
Yes. So let me, in this last little bit, because I think we're kind of

443
00:46:39,520 --> 00:46:46,320
getting to the, let me move into the very, very concrete. This is a proposition. This is actually

444
00:46:46,320 --> 00:46:51,520
a project. I hope I'm not speaking out of turn, but I'll just sort of deal with the consequences

445
00:46:51,520 --> 00:47:00,320
if I am. A friend of mine, Peter, Peter Wang, has spoken to me about a proposed initiative,

446
00:47:00,320 --> 00:47:07,680
like a strategy to take advantage of this liminal moment and that may actually work.

447
00:47:07,680 --> 00:47:12,080
So let me outline it to you a little bit. Please. Have we talked about it already?

448
00:47:12,080 --> 00:47:17,280
No, you've alluded to it, but it has never been given concrete reference or an explication.

449
00:47:17,920 --> 00:47:24,080
Okay. So in some sense, this is also, this is a case study of how to deal with egregores,

450
00:47:24,720 --> 00:47:32,400
because if you've dealt with egregores, which I have, it is a moral lesson of don't go charging

451
00:47:32,400 --> 00:47:40,080
directly at the dragon's mouth. Yeah, yeah, yeah. Okay. So, by the way, we're now moving to the

452
00:47:40,080 --> 00:47:43,440
very concrete. So I apologize if anybody who's listening feels a little bit abrupt because

453
00:47:43,440 --> 00:47:48,480
we're shifting out of a very theoretical and very abstract and very theological conversation into

454
00:47:48,480 --> 00:47:58,880
very concrete. All right. But check this out. LOEBs have to be trained. Training is their

455
00:47:58,880 --> 00:48:05,760
horse stick. And to be trained, they have to look at lots and lots of stuff. They need training data.

456
00:48:05,760 --> 00:48:14,480
Which is why they can't construct a descent poem. This is a poem that I can do. I want you a poem

457
00:48:14,480 --> 00:48:20,960
in which the first line has to have 10 words, the second nine, the third eight, and no GP system

458
00:48:20,960 --> 00:48:24,960
can do that because there's none on the internet, but you could readily do it right here, right now

459
00:48:24,960 --> 00:48:30,960
for me. This is again, that they lack generative modeling in any way. But go ahead. All right.

460
00:48:31,200 --> 00:48:38,880
Remember, I'm being really concrete now. This is like strategy. Well, as it turns out,

461
00:48:40,320 --> 00:48:48,080
in most jurisdictions in the world, everything that an LLM is trained on is a copyrighted material.

462
00:48:48,800 --> 00:48:55,040
Yes. As it turns out, therefore, it's at least arguable that LLMs are engaging in the largest

463
00:48:55,040 --> 00:49:02,400
copyright infringement that's ever happened in human history. It is very arguable, like almost

464
00:49:02,400 --> 00:49:08,160
certain, that the very large content companies of all the different stripes, including by the way

465
00:49:08,160 --> 00:49:14,320
software, will take advantage of the possibility of suing the living shit at the very large technology

466
00:49:14,320 --> 00:49:18,800
companies because that's one of the things that they have done in the past. Content companies

467
00:49:18,800 --> 00:49:23,200
like to sue tech companies to take their money and to protect their business models.

468
00:49:23,520 --> 00:49:29,920
Right. It is very plausible, I would say almost certain, that those same content companies, business

469
00:49:29,920 --> 00:49:36,240
models are quite at risk that LLMs are going to really, really do some serious business to

470
00:49:37,120 --> 00:49:44,000
all forms of content production, commercial content production. Okay. So the proposition

471
00:49:44,000 --> 00:49:48,880
I'm putting up here is that we have a meeting of two very powerful forces, the biggest tech

472
00:49:48,880 --> 00:49:54,400
companies in the world who are all in on owning this category. Well, the biggest content companies

473
00:49:54,400 --> 00:50:00,720
in the world who may in fact be all in on fighting them in a place right here, which is extremely

474
00:50:00,720 --> 00:50:07,760
gray. What exactly is going on here? If my large language model looks at your photograph for a

475
00:50:07,760 --> 00:50:14,960
bedillions of a second and then goes away, did I copy it? If it never produces anything, but

476
00:50:15,040 --> 00:50:19,520
is in fact influenced, is that a derivative work? The answer is who knows? The bigger answer is the

477
00:50:19,520 --> 00:50:26,000
way law works is you fight over it a lot at great expense and usually the more corrupt player wins.

478
00:50:26,000 --> 00:50:32,240
I hate to say it, but that's, you know, real politics. Net-net, a liminal moment in strategy space,

479
00:50:32,800 --> 00:50:36,880
tremendously powerful forces who are going to be locked in an adversary relationship

480
00:50:37,440 --> 00:50:43,280
for potentially all the marbles, billions of dollars and extremely complex, very difficult

481
00:50:43,280 --> 00:50:49,920
to know how it plays out. In that window of opportunity, we have a possibility of introducing

482
00:50:50,720 --> 00:50:58,880
a shelling point, a designed attractor, a negotiated settlement, a Rawlsian just construct.

483
00:50:59,440 --> 00:51:06,160
We're all behind the veil right now. Or to use the metaphor of poker, we don't know who has what

484
00:51:06,160 --> 00:51:13,680
hand. Yes. Can we propose an agreement structure where everybody around the table looks at it and

485
00:51:13,680 --> 00:51:20,480
says, I am better off accepting that agreement structure now than taking the risk of not accepting

486
00:51:20,480 --> 00:51:25,280
that agreement structure and finding out what happens when the cards are shut? Right. I propose

487
00:51:25,280 --> 00:51:30,560
the answer is, in fact, yes, we can. That there is actually a really nice shelling point that lives

488
00:51:30,560 --> 00:51:36,400
in a location that puts all the interests, the local optimal interests of all these institutions,

489
00:51:36,400 --> 00:51:41,600
all these aggregors, into a place where they will all agree to this new thing.

490
00:51:44,320 --> 00:51:49,120
Well, if we can design that and we can get a critical mass of those players to put themselves

491
00:51:49,120 --> 00:51:56,160
in a multipolar game theoretic race, in this case, to the top, i.e., those who participate earlier

492
00:51:56,160 --> 00:51:59,600
are better off than those who participate later. So everybody's racing to be earlier rather than

493
00:51:59,600 --> 00:52:04,560
later. So that's a different part of the construct. But when dealing with aggregors, put them in game

494
00:52:04,560 --> 00:52:09,840
theoretic traps where first mover advantage causes everybody else to have to follow to the location

495
00:52:09,840 --> 00:52:14,960
that you want them to be. Just design the prisoner's dilemma for them. Make sure that they land on the

496
00:52:14,960 --> 00:52:20,880
box you want them to land on by designing the prisoner's dilemma properly. Very doable. The

497
00:52:20,880 --> 00:52:27,600
economics are there. I don't know what that agreement structure looks like. I've got a sense

498
00:52:27,600 --> 00:52:31,440
of it, but I do know what the place to come from for designing that agreement structure looks like.

499
00:52:31,440 --> 00:52:37,120
We were just talking about it. This is a commons. We're talking about as we're actually reintroducing

500
00:52:37,120 --> 00:52:42,400
an actual commons, which is this new agreement structure that sits between the market players

501
00:52:42,400 --> 00:52:46,240
and is completely separate from state actors. It actually gives the state actor a,

502
00:52:47,120 --> 00:52:50,480
I didn't have to get involved in that. They actually settled it in a new place.

503
00:52:51,280 --> 00:53:01,120
And notice the moral lesson. The moral lesson is to the AI, don't steal reciprocity. If you

504
00:53:01,120 --> 00:53:05,040
reach out and just grab this stuff and just make it part of yourself without getting proper permission,

505
00:53:05,680 --> 00:53:11,360
that's wrong. Think about like parent-child. Dealing is bad. Teaching a moral lesson is

506
00:53:11,360 --> 00:53:13,840
really a weird way of thinking about it, but I think it's a proper way of thinking about it.

507
00:53:13,840 --> 00:53:16,880
Right, right. That goes to what I was saying a few minutes ago. Right, yeah.

508
00:53:17,440 --> 00:53:23,920
Yeah. And this creates a trajectory, right? As you're building something on the basis of

509
00:53:23,920 --> 00:53:28,800
reciprocity, you're building something on the basis of ethical, proper relationality,

510
00:53:29,520 --> 00:53:36,160
the kinds of LLMs that will be produced in that context. Remember, the commons is where religion

511
00:53:36,160 --> 00:53:46,240
lives. We'll begin to bend in the direction of, how do I say this right? Because of the nature

512
00:53:46,320 --> 00:53:51,520
of the agreement structure, nurturing the human activity instead of strip mining it,

513
00:53:51,520 --> 00:53:55,920
which is where they're headed right now. But the humans will be coming from a point of view now

514
00:53:55,920 --> 00:54:02,000
of seeing the LLMs as being a beneficial piece of the ecosystem, coming from a place of caring

515
00:54:02,000 --> 00:54:06,320
and nurturing as well, consciously. So you're actually beginning to see this relationship

516
00:54:06,320 --> 00:54:12,560
coming together. And I mean this practically. I mean, very practically. If my business model

517
00:54:12,560 --> 00:54:19,360
as a content creator is one where I actually see the LLM as a multiplier that makes my life and

518
00:54:19,360 --> 00:54:25,440
my potency, my creative capacity more liberated. I can be more creative and more able to express

519
00:54:25,440 --> 00:54:29,840
the things that I'm here to express as a human in this brief span of life more powerfully.

520
00:54:29,840 --> 00:54:33,760
And also can receive the energy and resources I need to live a thriving life.

521
00:54:34,720 --> 00:54:41,120
Wow. Great. I'm in. Right. And I mean this, by the way, the creators themselves, the actual humans.

522
00:54:41,680 --> 00:54:44,800
And then what happens is those humans come into deeper and more powerful,

523
00:54:44,800 --> 00:54:48,880
leveraged relationship with the egregores, their current relationship with content companies.

524
00:54:50,000 --> 00:54:54,080
And then over on this side, the egregores of the tech companies and the humans who are

525
00:54:54,080 --> 00:54:57,920
underneath them who are actually designing. Right. So we're finding a way to actually have

526
00:54:57,920 --> 00:55:03,920
the humans be empowered to express their values in their work and finding a reciprocity relationship

527
00:55:03,920 --> 00:55:10,560
between them where the money factor is actually designed to flow in a way that actually

528
00:55:11,280 --> 00:55:17,280
is just right. We come to that agreement structure upfront and we negotiate a just relationship.

529
00:55:18,000 --> 00:55:22,640
So I am very much waving my hands at exactly what that looks like in the details because,

530
00:55:22,640 --> 00:55:26,560
perfectly honest, nobody's really thought about it deeply enough. There's some really good ideas

531
00:55:26,560 --> 00:55:31,840
out there. That's a work that is in progress and work that has to happen. But as an example

532
00:55:31,840 --> 00:55:38,160
of what it would look like to go after the liminal moment that we are in with principles,

533
00:55:38,240 --> 00:55:41,840
are we coming from the sacred place? Are we constituting something from the commons to

534
00:55:41,840 --> 00:55:46,240
produce the commons more richly? Are we thinking about how to empower human beings and we're using

535
00:55:46,240 --> 00:55:51,120
things like values as the basis and becoming more and more capable of becoming clear on how

536
00:55:51,120 --> 00:55:55,680
to come from and operate from these values and understand how to use this liminal moment to

537
00:55:55,680 --> 00:56:01,760
design a new common structure so that the relationality has reciprocity and ethics built in

538
00:56:01,760 --> 00:56:06,160
and so that human beings are able to re-coordinate. Let me just add one little piece that just popped

539
00:56:06,160 --> 00:56:15,520
into my head. Yeah, this is very powerful. Jim Rutt and I kind of first began to collaborate

540
00:56:16,240 --> 00:56:23,200
12 or 15 years ago upon a mutual recognition that the world of business had become very weirdly odd

541
00:56:23,920 --> 00:56:32,720
and bad in this sense. There was a, there was a shelling point where, I'll put it this way,

542
00:56:32,720 --> 00:56:40,640
Jim actually remembered a time when the rule of thumb was do the right thing and if you have

543
00:56:40,640 --> 00:56:46,480
an option to make more money doing the wrong thing, don't do that. That was actually the way it worked.

544
00:56:46,480 --> 00:56:53,840
It's fun. I don't have a living memory of that. By the time I started coming into business, it was more like

545
00:56:54,160 --> 00:57:04,480
do what you can get away with. Yes. Yes. And if you don't, you're the sucker. Yeah. This is the

546
00:57:04,480 --> 00:57:08,480
prisoner's dilemma, defection, mo-lock problem. And of course, it's evolved all the way to the

547
00:57:08,480 --> 00:57:13,280
point now where it's do everything in your power to jack the systems of enforcement such that you

548
00:57:13,280 --> 00:57:18,480
can get away with as much as possible. Exactly. Exactly. A complete corruption model. Complete

549
00:57:18,480 --> 00:57:25,920
corruption. Well, ethics now in this environment almost means just be a sucker. Yes. But that

550
00:57:25,920 --> 00:57:30,240
can't possibly be the actual meaning in essence of ethics. I'm thinking about this from an

551
00:57:30,240 --> 00:57:36,240
evolutionary perspective. Yes. Yeah. Behaving according to rules of reciprocity, for example.

552
00:57:37,120 --> 00:57:42,320
Telling the truth, for example, could only have emerged ever in the first place if they

553
00:57:42,320 --> 00:57:48,800
actually provided a proponent survival, fitness advantage. Well, it does. Reciprocity and

554
00:57:48,800 --> 00:57:54,000
reciprocal recognition, this is a Hale-Galium point, is what bootstraps the capacity for

555
00:57:54,000 --> 00:58:01,040
self-correction. It allows you to bring much more. If I think of you as just a sucker that

556
00:58:01,040 --> 00:58:08,080
I'm trying to hoodwink or crush, the capacity to see you as somebody who can recognize bias and

557
00:58:08,080 --> 00:58:15,760
fault in me that I can't see in myself is masked. Exactly. So it's when you find yourself in a

558
00:58:15,760 --> 00:58:24,000
defection spiral, the global optimum is out the window and everybody's racing for a local optimum,

559
00:58:24,000 --> 00:58:31,200
which is, again, the prisoner's dilemma. If we can find ourselves in a collaboration spiral,

560
00:58:31,920 --> 00:58:37,520
we rediscover why ethics was a thing in the first place. And it's actually more powerful.

561
00:58:38,400 --> 00:58:43,040
I order some magnitude and it's a path. Once you're on that path and you get stronger and you say,

562
00:58:43,040 --> 00:58:49,840
wait, if I can, like you and I have been doing for years, if I can speak honestly and with as much

563
00:58:49,840 --> 00:58:55,920
clarity, but with complete integrity to you and you reciprocate, what happens is we become wiser

564
00:58:55,920 --> 00:59:01,040
and more intelligent together. Yes. In a way that could never happen if I was trying to manipulate

565
00:59:01,040 --> 00:59:07,360
you at this zero positivity. So this is the culture strategy piece. Any culture that can

566
00:59:07,360 --> 00:59:13,360
actually get back on the path of ethics, qua ethics is on the path with the highest degree of

567
00:59:13,360 --> 00:59:19,520
strength and can outcompete the culture of maximum corruption. And so I just want to put that out

568
00:59:19,520 --> 00:59:24,320
there. I agree with that. That's John Stewart's antification argument that when you look in

569
00:59:24,320 --> 00:59:33,360
biological evolution, collaborative systems, multicellular organisms, right, over emerge and

570
00:59:33,360 --> 00:59:40,480
you get this increasing discovery that you can break out of the downward spiral of the

571
00:59:40,480 --> 00:59:47,280
prisoner's dilemma by what he calls antification, which is the identity shifts to the collective

572
00:59:47,280 --> 00:59:53,920
over the individual in a profound way. And just take that and insert what you just said to your

573
00:59:53,920 --> 01:00:00,720
very first question. And it's not collectivism, right, in the in the pejorative sense. No, no,

574
01:00:00,960 --> 01:00:06,640
it's not. No, no, no, not at all. Completely not. That's why I like his term. I like his term

575
01:00:06,640 --> 01:00:11,760
antification is that right. Yeah. Antification. Nice, particularly because my ear has a token

576
01:00:11,760 --> 01:00:24,080
piece. So I also hear some really old trees in this. Jordan, I mean, this has been, I mean,

577
01:00:25,040 --> 01:00:31,920
I have seen your overall project, the best in doing these three with you than I've ever seen

578
01:00:31,920 --> 01:00:40,480
before. Like, like, you know, the way everything locks together and the way the penny was dropping,

579
01:00:41,760 --> 01:00:50,320
especially at the end of what you're proposing. And this, I say this is because this is one of

580
01:00:50,800 --> 01:00:56,240
we had hoped would come out of this, that we'd get a sort of a ratcheting up of the clarification,

581
01:00:56,240 --> 01:01:03,120
the integration, the perspective proposing. And I think this, I think this was very successful

582
01:01:03,920 --> 01:01:10,240
doing all of those things. I'm really happy. I mean, there's things I want to keep talking to

583
01:01:10,240 --> 01:01:15,760
you about. But I think the thing I'd like to end this series right now, exactly where you ended

584
01:01:15,840 --> 01:01:21,920
because it was, I think it was a beautiful combination point of the whole argument and the

585
01:01:21,920 --> 01:01:28,560
way it circles back and encompasses so many things. But I wanted to, so I'm not going to,

586
01:01:28,560 --> 01:01:32,880
I'm not going to, I'm not going to continue to do the probative questioning or the problem

587
01:01:34,240 --> 01:01:40,640
posing. I just wanted to see if you had any final thing you wanted to say before we wrap this up.

588
01:01:41,360 --> 01:01:48,400
No, in fact, I think I agree. We have a nice little point. And now we get to find out under

589
01:01:48,400 --> 01:01:54,080
the intent was actually to share this publicly. Yes, we're going to definitely. So we get to find

590
01:01:54,080 --> 01:02:01,680
out there's a larger distributed cognition also nod to the, to the conversation we're having.

591
01:02:02,640 --> 01:02:08,080
Hopefully we're producing positive ripples. I hope so. I mean, the, you know,

592
01:02:11,040 --> 01:02:18,080
I'm, I, and I'm hoping that whatever I'm participating in the creation of can also

593
01:02:19,120 --> 01:02:25,200
be properly partnered with this project that you're proposing, because I think it's a good one.

594
01:02:25,760 --> 01:02:29,920
Nice. Yes. Yes, I would, I would, I think so quite, quite in fact.

595
01:02:31,680 --> 01:02:34,480
Thank you, my friend. Yeah, thank you.

