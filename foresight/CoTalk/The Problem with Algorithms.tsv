start	end	text
0	5000	This is going to sound a little bit crazy, but I think that the free speech debate is a complete distraction right now.
5000	8000	I think the real debate should be about free will.
8000	14000	And we feel it right now, because we are being programmed based on what we say we're interested in,
14000	18000	and we are told through these discovery mechanisms what is interesting.
18000	25000	And as we engage and interact with this content, the algorithm continues to build more and more of this bias.
25000	29000	But the algorithm, even if it's open source, is effectively a black box.
29000	34000	You cannot predict 100% of the time how it's going to work, what it's going to show you,
34000	37000	and it can be moved and changed at any time.
37000	43000	And because people become so dependent upon it, it's actually changing and impacting the free agency we have.
43000	47000	And I think the only answer to this is not to work harder at open source,
47000	51000	or making them more explainable about what they're doing and why they're doing it,
51000	57000	but to give people choice, give people choice of what algorithm they want to use from a party that they trust,
57000	62000	give people choice to build their own algorithm that they can plug in on top of these networks,
62000	66000	and give people choice to have really a marketplace.
66000	71000	Two tech billionaires, Elon Musk and Jack Dorsey, with access to powerful social media algorithms,
71000	76000	are publicly admitting that our free will is at stake, and it's time to take note.
76000	81000	The algorithms that we program are programming us.
81000	83000	So we know this.
83000	89000	Algorithms build a bias of interest based on its own suggestions while disregarding anomalies.
89000	94000	Like a snake eating its own tail, the more something is suggested, the more it is consumed.
94000	97000	And the more it is consumed, the more it's suggested.
97000	103000	Dorsey admits that algorithms are a black box, and the outcome of this is unpredictable.
103000	106000	We don't have much choice right now. You can only use what's baked into a product,
106000	110000	interact with products anonymously, or not use the products at all.
110000	115000	Dorsey suggests giving users a choice on which algorithms they want to use,
115000	118000	and that this would alleviate the threat of autonomy.
118000	120000	But is this actually enough?
120000	123000	In a public presentation from Google's former head of behavioral science,
123000	126000	Oxford PhD Maya Shankar said this.
126000	130000	So the first principle is called social identity priming.
130000	136000	And what it says is that people tend to act in ways that are consistent with the identities
136000	141000	that they either currently associate with or aspire to associate with.
141000	144000	The Red Cross ran an experiment that leveraged this insight.
144000	149000	They were sending letters to people who had previously donated to the Red Cross,
149000	153000	and were appealing to them and asking them whether they'd be willing to donate again.
153000	159000	And they ran this A-B test, and what they found is that when they simply reminded the recipients of this letter
159000	161000	that they had been previous donors,
161000	167000	primed them for identity, for their identity as charitable, generous people with golden hearts,
167000	173000	they found that that identity priming led to a 30% increase in charitable contributions.
173000	179000	And impressively, priming people for their identity as donors actually made them more generous.
179000	183000	They increased the magnitude of their charitable contributions.
183000	188000	If something is shown to you and it aligns enough with your identity or goals,
188000	194000	you will subconsciously prefer it, whether that's choosing a specific product or taking certain actions.
194000	199000	In essence, we choose the things we like, and then we like the things we choose.
199000	203000	This phenomenon is commonly referred to as the Ikea effect,
203000	208000	or a cognitive bias derived from being directly involved in a process.
208000	211000	And in the case of Ikea, it's building your own furniture.
211000	215000	You chose the table from this catalog, you worked hard to assemble it,
215000	217000	and obviously you did the job right.
217000	220000	So this table is actually awesome, right?
220000	223000	There's also other studies out there that confirm this.
223000	228000	There was one study that was done on school kids, two different groups taking the same test.
228000	233000	The only difference was that one group was told that the test would be very difficult.
233000	236000	The other group was told that the test would be a breeze.
236000	239000	And can you guess which group performed better?
239000	244000	It ended up being the group that was more relaxed because they were told the test was going to be easy.
244000	246000	And Facebook did a similar study.
246000	250000	One group of people was exposed repeatedly to only negative posts,
250000	253000	while the other group was exposed to only positive ones.
253000	254000	What do you think happened?
254000	257000	The negative exposure made people post more negatively,
257000	260000	and the positive exposure had a positive effect.
260000	264000	I guess the point I'm trying to make is that we are more susceptible to our environment
264000	267000	and the power of suggestion than we're comfortable admitting.
267000	273000	And now these large companies that don't really give a shit about us are in control of that.
273000	275000	We gave that autonomy away.
275000	276000	Does it piss you off?
276000	277000	Because it should.
277000	284000	British academic and author Hannah Fry suggests that algorithms are increasingly playing a huge role in our society.
284000	286000	From courtrooms to hospitals and schools,
286000	292000	algorithms are making decisions behind the scenes that affect the way our society operates.
292000	297000	But it's not necessarily the small decisions we give up in our daily lives that bothers us.
297000	301000	We're more than happy to automatically schedule meetings in our calendars
301000	306000	or let our phones handle color correction and autofocus on pictures.
306000	312000	But there's a much larger and more nefarious play happening on the internet's algorithm chessboard.
312000	318000	The way that we behave as humans, we like to think that we're sort of wandering around with free will, should we say.
318000	321000	We like to think that we're independent and we're making decisions for ourselves.
321000	325000	But I think what we're saying more and more, the more data that we're collecting,
325000	328000	is that there's just these very clear patterns in the way that people behave.
328000	336000	And those patterns can be exploited and really I think used to sort of slightly change the structures that you put in place around humans.
336000	339000	And I think that that's something that applies completely across the board.
339000	344000	Using maths and physics and whatever data and computer science to understand human behavior,
344000	347000	you are not looking at a physical system.
347000	353000	You are looking at something that has individuals with autonomy, like rich and varied lives.
353000	360000	So for example, in London where I live, I couldn't tell if my neighbor was going to go into work one morning or not.
360000	365000	I couldn't make that prediction. I couldn't make a prediction about what time they might jump on the tube, the underground.
365000	368000	But when you kind of scale out to the size of an entire city,
368000	375000	actually you can make really, really accurate predictions about how many people overall will choose a certain path,
375000	380000	will pick a particular route and if something changes how the whole population might end up reacting.
380000	386000	Back in 2002, after another day of analyzing credit card numbers and corresponding purchases,
386000	393000	the box store giant Target began to notice a spike in female customers buying unscented body lotion.
393000	399000	These same customers had historically bought up vitamins and supplements such as calcium and zinc.
399000	407000	And a little further along this retail timeline, it was eventually revealed that they would sign up for an in-store baby shower registry.
407000	413000	Target saw dollar signs. They ran an algorithm that would score its female customers on the likelihood that they were pregnant.
413000	422000	And if the probability score was high enough, they would send out a series of coupons for baby shower products to the expectant or unexpected mother-to-be.
426000	433000	This went on for a while until one day a man stormed angrily into a Minneapolis target location, demanding to speak with the manager.
433000	444000	His teenage daughter had been receiving pregnancy coupons in the mail and he was very upset that Target would be making assumptions about his daughter's extracurricular activities.
446000	454000	The manager of the store apologized profusely and called the man's home a few days later to reiterate the company's regret about the entire affair.
454000	458000	But by then, the father had an apology of his own to make.
459000	468000	I had a talk with my daughter, he told the manager, and it turns out there's been some activities in the house that I wasn't completely aware of.
468000	470000	She's due in August.
471000	481000	This story was published by The New York Times over 20 years ago, and since then you can imagine the sophistication of our algorithms and data capture have greatly improved.
481000	491000	Combining these methods with the emergence of AI learning models and you've got yourself a Molotov cocktail of hallucinating predictions and hyper-targeted marketing schemas.
491000	497000	If any of you have been on the internet lately, chances are your digital footprint has been left behind.
497000	503000	Every app, every device, every website, every interaction is logged and contributed as a data packet.
503000	509000	These data packets are then categorized and monetized by large data brokers.
509000	510000	Oracle is one of them.
510000	520000	Everything you are and do is made into a data profile of your digital self, your shadow, and it has been resold thousands of times.
520000	523000	Businesses in all industries are watching you.
523000	524000	They're learning from you.
524000	530000	They're adjusting their business models because of you, and they're constantly calculating your worth as a consumer.
532000	539000	Most of today's digital advertising takes place in the form of highly automated real-time auctions between publishers and advertisers.
539000	543000	This is often referred to as programmatic advertising.
543000	552000	When a person visits a website, it sends user data to a variety of third-party services, which then try to recognize a person and retrieve available profile information.
552000	560000	Advertisers interested in delivering an ad to this particular person due to certain attributes and behaviors make a bid.
560000	565000	And within milliseconds, the highest bidding advertiser wins and places the ad.
570000	579000	In whatever corner of the internet you've used, hiding in the background, these algorithms are trading information that you didn't know they had and never willingly offered.
579000	582000	They have made your most personal private secrets into a commodity.
582000	588000	Not only that, data profiling as a result of hyper-targeting has shown many biases.
588000	592000	Here's some real-world examples of biases as presented by IBM.
592000	600000	For example, computer-aided diagnosis or CAD systems have been found to return lower accuracy results for black patients than white patients.
600000	610000	Amazon stopped using a hiring algorithm after finding out that it favored applicants based on words like executed or captured, which were more commonly found on men's resumes than women's.
610000	615000	Google's online advertising system displayed high-paying positions to men more than women.
615000	624000	Academic research around image generation found that the application mid-journey showed bias when asked to create images of people in specialized professions.
624000	628000	It would show younger and older people, but the older people were always meant.
628000	636000	AI-powered predictive policing tools within the criminal justice system are supposed to identify areas in which crime is likely to occur.
636000	646000	However, they often rely on historical arrest data which can reinforce existing patterns of racial profiling and disproportionate targeting of minority communities.
646000	660000	So, as we consider the implication of commerce-driven algorithms, which also largely drive our social media consumption, the idea of what we like is actually being reinforced by what we chose, leading to similar offers of what we like.
660000	674000	This means that we can be influenced by others telling us what we should choose and if we're inclined to believe it, this could be enough to tip the scales on life-changing decisions such as whether or not to get an abortion or whether or not to attend flight school and become a pilot.
674000	677000	Or, go to the doctor and get that weird mole checked.
677000	686000	Not only that, companies are using your behavioral patterns to predict what you'll do next and they're ready to serve up your next favorite product or experience.
686000	692000	So, some of this seems harmless and well-meaning until we consider, again, those biases of the algorithms.
692000	702000	I don't want to live in a world where law enforcement makes non-evidence-based AI predictions on whether or not someone is a terrorist or is going to re-offend if given parole.
702000	707000	Algorithms on social media are siloing us into categories and trying to keep us there as long as possible.
707000	712000	By feeding us what it knows we already like, it's actually doing us a disservice.
712000	719000	So, as if the societal implications I've already listed aren't enough to scare you, what else are we missing out on?
719000	721000	Well, the short answer is anomalies.
721000	722000	Open-mindedness.
722000	726000	I'm speaking creatively now and putting some of that doom and gloom to the side.
726000	734000	We want new fresh content, new interesting and challenging information that contradicts our views on life as we know it.
734000	736000	Growth happens when we're uncomfortable.
736000	743000	And if we sit in our bubble and let an algorithm feed us what it knows we already like, then we are not in a growth zone.
743000	745000	We are not in a growth mindset.
745000	751000	We're sitting there waiting for data brokers to pin us down, label us, and offer us a product that they think we'll buy.
751000	752000	Don't be stagnant.
752000	756000	Be someone who listens to the opinions of others, even if you hate it.
756000	759000	Also, if you want to ditch the YouTube algorithm, go check out GreyJay.
759000	765000	It's still in development, but it was made by one of my favorite people, free-thinking evangelist, Louis Rossman,
765000	769000	who's a huge YouTuber, and if you haven't heard of him, go check out his channel and go subscribe.
769000	774000	And if you already know him, then you know that the app is going to be done correctly.
774000	777000	With that being said, algorithms themselves are not inherently bad.
777000	779000	It's all in how you use it.
779000	787000	After all, from the words of Hello World by Hannah Fry, GPS was invented to launch nuclear missiles, and now it helps deliver pizzas.
787000	789000	So go pick yourself up a copy.
789000	791000	That's where the target story came from.
791000	799000	And this book is also stocked to the brim with many other examples of mass surveillance, behavioral predictions, and it goes so, so much deeper than this video.
799000	800000	Amazon link is in the description.
800000	801000	I just like the book.
801000	805000	This is not an advertisement, and I do not get affiliate payments off of this whatsoever.
805000	806000	I'm just a fan.
806000	809000	And some final thoughts from the Oxford PhD Maya Shankar.
809000	824000	When people were allowed to tweak an algorithm even slightly, they were more satisfied with the results of the algorithm and actually opted to use that algorithm over other algorithms that they knew performed better but had not involved their input.
824000	826000	So it's pretty irrational behavior, right?
826000	829000	But it shows that there's some egocentricity here that matters.
829000	832000	Like, we really value our own contributions.
832000	838000	So does this mean we're really in control, or are we just satisfied with choosing the algorithm that controls us?
838000	843000	And will we start living in a post-AI age that is pre-deterministic?
843000	845000	If so, by how much?
845000	847000	I'm Austin with The Kotak Podcast over and out. Peace!
868000	870000	.
