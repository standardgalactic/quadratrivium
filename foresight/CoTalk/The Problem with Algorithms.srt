1
00:00:00,000 --> 00:00:05,000
This is going to sound a little bit crazy, but I think that the free speech debate is a complete distraction right now.

2
00:00:05,000 --> 00:00:08,000
I think the real debate should be about free will.

3
00:00:08,000 --> 00:00:14,000
And we feel it right now, because we are being programmed based on what we say we're interested in,

4
00:00:14,000 --> 00:00:18,000
and we are told through these discovery mechanisms what is interesting.

5
00:00:18,000 --> 00:00:25,000
And as we engage and interact with this content, the algorithm continues to build more and more of this bias.

6
00:00:25,000 --> 00:00:29,000
But the algorithm, even if it's open source, is effectively a black box.

7
00:00:29,000 --> 00:00:34,000
You cannot predict 100% of the time how it's going to work, what it's going to show you,

8
00:00:34,000 --> 00:00:37,000
and it can be moved and changed at any time.

9
00:00:37,000 --> 00:00:43,000
And because people become so dependent upon it, it's actually changing and impacting the free agency we have.

10
00:00:43,000 --> 00:00:47,000
And I think the only answer to this is not to work harder at open source,

11
00:00:47,000 --> 00:00:51,000
or making them more explainable about what they're doing and why they're doing it,

12
00:00:51,000 --> 00:00:57,000
but to give people choice, give people choice of what algorithm they want to use from a party that they trust,

13
00:00:57,000 --> 00:01:02,000
give people choice to build their own algorithm that they can plug in on top of these networks,

14
00:01:02,000 --> 00:01:06,000
and give people choice to have really a marketplace.

15
00:01:06,000 --> 00:01:11,000
Two tech billionaires, Elon Musk and Jack Dorsey, with access to powerful social media algorithms,

16
00:01:11,000 --> 00:01:16,000
are publicly admitting that our free will is at stake, and it's time to take note.

17
00:01:16,000 --> 00:01:21,000
The algorithms that we program are programming us.

18
00:01:21,000 --> 00:01:23,000
So we know this.

19
00:01:23,000 --> 00:01:29,000
Algorithms build a bias of interest based on its own suggestions while disregarding anomalies.

20
00:01:29,000 --> 00:01:34,000
Like a snake eating its own tail, the more something is suggested, the more it is consumed.

21
00:01:34,000 --> 00:01:37,000
And the more it is consumed, the more it's suggested.

22
00:01:37,000 --> 00:01:43,000
Dorsey admits that algorithms are a black box, and the outcome of this is unpredictable.

23
00:01:43,000 --> 00:01:46,000
We don't have much choice right now. You can only use what's baked into a product,

24
00:01:46,000 --> 00:01:50,000
interact with products anonymously, or not use the products at all.

25
00:01:50,000 --> 00:01:55,000
Dorsey suggests giving users a choice on which algorithms they want to use,

26
00:01:55,000 --> 00:01:58,000
and that this would alleviate the threat of autonomy.

27
00:01:58,000 --> 00:02:00,000
But is this actually enough?

28
00:02:00,000 --> 00:02:03,000
In a public presentation from Google's former head of behavioral science,

29
00:02:03,000 --> 00:02:06,000
Oxford PhD Maya Shankar said this.

30
00:02:06,000 --> 00:02:10,000
So the first principle is called social identity priming.

31
00:02:10,000 --> 00:02:16,000
And what it says is that people tend to act in ways that are consistent with the identities

32
00:02:16,000 --> 00:02:21,000
that they either currently associate with or aspire to associate with.

33
00:02:21,000 --> 00:02:24,000
The Red Cross ran an experiment that leveraged this insight.

34
00:02:24,000 --> 00:02:29,000
They were sending letters to people who had previously donated to the Red Cross,

35
00:02:29,000 --> 00:02:33,000
and were appealing to them and asking them whether they'd be willing to donate again.

36
00:02:33,000 --> 00:02:39,000
And they ran this A-B test, and what they found is that when they simply reminded the recipients of this letter

37
00:02:39,000 --> 00:02:41,000
that they had been previous donors,

38
00:02:41,000 --> 00:02:47,000
primed them for identity, for their identity as charitable, generous people with golden hearts,

39
00:02:47,000 --> 00:02:53,000
they found that that identity priming led to a 30% increase in charitable contributions.

40
00:02:53,000 --> 00:02:59,000
And impressively, priming people for their identity as donors actually made them more generous.

41
00:02:59,000 --> 00:03:03,000
They increased the magnitude of their charitable contributions.

42
00:03:03,000 --> 00:03:08,000
If something is shown to you and it aligns enough with your identity or goals,

43
00:03:08,000 --> 00:03:14,000
you will subconsciously prefer it, whether that's choosing a specific product or taking certain actions.

44
00:03:14,000 --> 00:03:19,000
In essence, we choose the things we like, and then we like the things we choose.

45
00:03:19,000 --> 00:03:23,000
This phenomenon is commonly referred to as the Ikea effect,

46
00:03:23,000 --> 00:03:28,000
or a cognitive bias derived from being directly involved in a process.

47
00:03:28,000 --> 00:03:31,000
And in the case of Ikea, it's building your own furniture.

48
00:03:31,000 --> 00:03:35,000
You chose the table from this catalog, you worked hard to assemble it,

49
00:03:35,000 --> 00:03:37,000
and obviously you did the job right.

50
00:03:37,000 --> 00:03:40,000
So this table is actually awesome, right?

51
00:03:40,000 --> 00:03:43,000
There's also other studies out there that confirm this.

52
00:03:43,000 --> 00:03:48,000
There was one study that was done on school kids, two different groups taking the same test.

53
00:03:48,000 --> 00:03:53,000
The only difference was that one group was told that the test would be very difficult.

54
00:03:53,000 --> 00:03:56,000
The other group was told that the test would be a breeze.

55
00:03:56,000 --> 00:03:59,000
And can you guess which group performed better?

56
00:03:59,000 --> 00:04:04,000
It ended up being the group that was more relaxed because they were told the test was going to be easy.

57
00:04:04,000 --> 00:04:06,000
And Facebook did a similar study.

58
00:04:06,000 --> 00:04:10,000
One group of people was exposed repeatedly to only negative posts,

59
00:04:10,000 --> 00:04:13,000
while the other group was exposed to only positive ones.

60
00:04:13,000 --> 00:04:14,000
What do you think happened?

61
00:04:14,000 --> 00:04:17,000
The negative exposure made people post more negatively,

62
00:04:17,000 --> 00:04:20,000
and the positive exposure had a positive effect.

63
00:04:20,000 --> 00:04:24,000
I guess the point I'm trying to make is that we are more susceptible to our environment

64
00:04:24,000 --> 00:04:27,000
and the power of suggestion than we're comfortable admitting.

65
00:04:27,000 --> 00:04:33,000
And now these large companies that don't really give a shit about us are in control of that.

66
00:04:33,000 --> 00:04:35,000
We gave that autonomy away.

67
00:04:35,000 --> 00:04:36,000
Does it piss you off?

68
00:04:36,000 --> 00:04:37,000
Because it should.

69
00:04:37,000 --> 00:04:44,000
British academic and author Hannah Fry suggests that algorithms are increasingly playing a huge role in our society.

70
00:04:44,000 --> 00:04:46,000
From courtrooms to hospitals and schools,

71
00:04:46,000 --> 00:04:52,000
algorithms are making decisions behind the scenes that affect the way our society operates.

72
00:04:52,000 --> 00:04:57,000
But it's not necessarily the small decisions we give up in our daily lives that bothers us.

73
00:04:57,000 --> 00:05:01,000
We're more than happy to automatically schedule meetings in our calendars

74
00:05:01,000 --> 00:05:06,000
or let our phones handle color correction and autofocus on pictures.

75
00:05:06,000 --> 00:05:12,000
But there's a much larger and more nefarious play happening on the internet's algorithm chessboard.

76
00:05:12,000 --> 00:05:18,000
The way that we behave as humans, we like to think that we're sort of wandering around with free will, should we say.

77
00:05:18,000 --> 00:05:21,000
We like to think that we're independent and we're making decisions for ourselves.

78
00:05:21,000 --> 00:05:25,000
But I think what we're saying more and more, the more data that we're collecting,

79
00:05:25,000 --> 00:05:28,000
is that there's just these very clear patterns in the way that people behave.

80
00:05:28,000 --> 00:05:36,000
And those patterns can be exploited and really I think used to sort of slightly change the structures that you put in place around humans.

81
00:05:36,000 --> 00:05:39,000
And I think that that's something that applies completely across the board.

82
00:05:39,000 --> 00:05:44,000
Using maths and physics and whatever data and computer science to understand human behavior,

83
00:05:44,000 --> 00:05:47,000
you are not looking at a physical system.

84
00:05:47,000 --> 00:05:53,000
You are looking at something that has individuals with autonomy, like rich and varied lives.

85
00:05:53,000 --> 00:06:00,000
So for example, in London where I live, I couldn't tell if my neighbor was going to go into work one morning or not.

86
00:06:00,000 --> 00:06:05,000
I couldn't make that prediction. I couldn't make a prediction about what time they might jump on the tube, the underground.

87
00:06:05,000 --> 00:06:08,000
But when you kind of scale out to the size of an entire city,

88
00:06:08,000 --> 00:06:15,000
actually you can make really, really accurate predictions about how many people overall will choose a certain path,

89
00:06:15,000 --> 00:06:20,000
will pick a particular route and if something changes how the whole population might end up reacting.

90
00:06:20,000 --> 00:06:26,000
Back in 2002, after another day of analyzing credit card numbers and corresponding purchases,

91
00:06:26,000 --> 00:06:33,000
the box store giant Target began to notice a spike in female customers buying unscented body lotion.

92
00:06:33,000 --> 00:06:39,000
These same customers had historically bought up vitamins and supplements such as calcium and zinc.

93
00:06:39,000 --> 00:06:47,000
And a little further along this retail timeline, it was eventually revealed that they would sign up for an in-store baby shower registry.

94
00:06:47,000 --> 00:06:53,000
Target saw dollar signs. They ran an algorithm that would score its female customers on the likelihood that they were pregnant.

95
00:06:53,000 --> 00:07:02,000
And if the probability score was high enough, they would send out a series of coupons for baby shower products to the expectant or unexpected mother-to-be.

96
00:07:06,000 --> 00:07:13,000
This went on for a while until one day a man stormed angrily into a Minneapolis target location, demanding to speak with the manager.

97
00:07:13,000 --> 00:07:24,000
His teenage daughter had been receiving pregnancy coupons in the mail and he was very upset that Target would be making assumptions about his daughter's extracurricular activities.

98
00:07:26,000 --> 00:07:34,000
The manager of the store apologized profusely and called the man's home a few days later to reiterate the company's regret about the entire affair.

99
00:07:34,000 --> 00:07:38,000
But by then, the father had an apology of his own to make.

100
00:07:39,000 --> 00:07:48,000
I had a talk with my daughter, he told the manager, and it turns out there's been some activities in the house that I wasn't completely aware of.

101
00:07:48,000 --> 00:07:50,000
She's due in August.

102
00:07:51,000 --> 00:08:01,000
This story was published by The New York Times over 20 years ago, and since then you can imagine the sophistication of our algorithms and data capture have greatly improved.

103
00:08:01,000 --> 00:08:11,000
Combining these methods with the emergence of AI learning models and you've got yourself a Molotov cocktail of hallucinating predictions and hyper-targeted marketing schemas.

104
00:08:11,000 --> 00:08:17,000
If any of you have been on the internet lately, chances are your digital footprint has been left behind.

105
00:08:17,000 --> 00:08:23,000
Every app, every device, every website, every interaction is logged and contributed as a data packet.

106
00:08:23,000 --> 00:08:29,000
These data packets are then categorized and monetized by large data brokers.

107
00:08:29,000 --> 00:08:30,000
Oracle is one of them.

108
00:08:30,000 --> 00:08:40,000
Everything you are and do is made into a data profile of your digital self, your shadow, and it has been resold thousands of times.

109
00:08:40,000 --> 00:08:43,000
Businesses in all industries are watching you.

110
00:08:43,000 --> 00:08:44,000
They're learning from you.

111
00:08:44,000 --> 00:08:50,000
They're adjusting their business models because of you, and they're constantly calculating your worth as a consumer.

112
00:08:52,000 --> 00:08:59,000
Most of today's digital advertising takes place in the form of highly automated real-time auctions between publishers and advertisers.

113
00:08:59,000 --> 00:09:03,000
This is often referred to as programmatic advertising.

114
00:09:03,000 --> 00:09:12,000
When a person visits a website, it sends user data to a variety of third-party services, which then try to recognize a person and retrieve available profile information.

115
00:09:12,000 --> 00:09:20,000
Advertisers interested in delivering an ad to this particular person due to certain attributes and behaviors make a bid.

116
00:09:20,000 --> 00:09:25,000
And within milliseconds, the highest bidding advertiser wins and places the ad.

117
00:09:30,000 --> 00:09:39,000
In whatever corner of the internet you've used, hiding in the background, these algorithms are trading information that you didn't know they had and never willingly offered.

118
00:09:39,000 --> 00:09:42,000
They have made your most personal private secrets into a commodity.

119
00:09:42,000 --> 00:09:48,000
Not only that, data profiling as a result of hyper-targeting has shown many biases.

120
00:09:48,000 --> 00:09:52,000
Here's some real-world examples of biases as presented by IBM.

121
00:09:52,000 --> 00:10:00,000
For example, computer-aided diagnosis or CAD systems have been found to return lower accuracy results for black patients than white patients.

122
00:10:00,000 --> 00:10:10,000
Amazon stopped using a hiring algorithm after finding out that it favored applicants based on words like executed or captured, which were more commonly found on men's resumes than women's.

123
00:10:10,000 --> 00:10:15,000
Google's online advertising system displayed high-paying positions to men more than women.

124
00:10:15,000 --> 00:10:24,000
Academic research around image generation found that the application mid-journey showed bias when asked to create images of people in specialized professions.

125
00:10:24,000 --> 00:10:28,000
It would show younger and older people, but the older people were always meant.

126
00:10:28,000 --> 00:10:36,000
AI-powered predictive policing tools within the criminal justice system are supposed to identify areas in which crime is likely to occur.

127
00:10:36,000 --> 00:10:46,000
However, they often rely on historical arrest data which can reinforce existing patterns of racial profiling and disproportionate targeting of minority communities.

128
00:10:46,000 --> 00:11:00,000
So, as we consider the implication of commerce-driven algorithms, which also largely drive our social media consumption, the idea of what we like is actually being reinforced by what we chose, leading to similar offers of what we like.

129
00:11:00,000 --> 00:11:14,000
This means that we can be influenced by others telling us what we should choose and if we're inclined to believe it, this could be enough to tip the scales on life-changing decisions such as whether or not to get an abortion or whether or not to attend flight school and become a pilot.

130
00:11:14,000 --> 00:11:17,000
Or, go to the doctor and get that weird mole checked.

131
00:11:17,000 --> 00:11:26,000
Not only that, companies are using your behavioral patterns to predict what you'll do next and they're ready to serve up your next favorite product or experience.

132
00:11:26,000 --> 00:11:32,000
So, some of this seems harmless and well-meaning until we consider, again, those biases of the algorithms.

133
00:11:32,000 --> 00:11:42,000
I don't want to live in a world where law enforcement makes non-evidence-based AI predictions on whether or not someone is a terrorist or is going to re-offend if given parole.

134
00:11:42,000 --> 00:11:47,000
Algorithms on social media are siloing us into categories and trying to keep us there as long as possible.

135
00:11:47,000 --> 00:11:52,000
By feeding us what it knows we already like, it's actually doing us a disservice.

136
00:11:52,000 --> 00:11:59,000
So, as if the societal implications I've already listed aren't enough to scare you, what else are we missing out on?

137
00:11:59,000 --> 00:12:01,000
Well, the short answer is anomalies.

138
00:12:01,000 --> 00:12:02,000
Open-mindedness.

139
00:12:02,000 --> 00:12:06,000
I'm speaking creatively now and putting some of that doom and gloom to the side.

140
00:12:06,000 --> 00:12:14,000
We want new fresh content, new interesting and challenging information that contradicts our views on life as we know it.

141
00:12:14,000 --> 00:12:16,000
Growth happens when we're uncomfortable.

142
00:12:16,000 --> 00:12:23,000
And if we sit in our bubble and let an algorithm feed us what it knows we already like, then we are not in a growth zone.

143
00:12:23,000 --> 00:12:25,000
We are not in a growth mindset.

144
00:12:25,000 --> 00:12:31,000
We're sitting there waiting for data brokers to pin us down, label us, and offer us a product that they think we'll buy.

145
00:12:31,000 --> 00:12:32,000
Don't be stagnant.

146
00:12:32,000 --> 00:12:36,000
Be someone who listens to the opinions of others, even if you hate it.

147
00:12:36,000 --> 00:12:39,000
Also, if you want to ditch the YouTube algorithm, go check out GreyJay.

148
00:12:39,000 --> 00:12:45,000
It's still in development, but it was made by one of my favorite people, free-thinking evangelist, Louis Rossman,

149
00:12:45,000 --> 00:12:49,000
who's a huge YouTuber, and if you haven't heard of him, go check out his channel and go subscribe.

150
00:12:49,000 --> 00:12:54,000
And if you already know him, then you know that the app is going to be done correctly.

151
00:12:54,000 --> 00:12:57,000
With that being said, algorithms themselves are not inherently bad.

152
00:12:57,000 --> 00:12:59,000
It's all in how you use it.

153
00:12:59,000 --> 00:13:07,000
After all, from the words of Hello World by Hannah Fry, GPS was invented to launch nuclear missiles, and now it helps deliver pizzas.

154
00:13:07,000 --> 00:13:09,000
So go pick yourself up a copy.

155
00:13:09,000 --> 00:13:11,000
That's where the target story came from.

156
00:13:11,000 --> 00:13:19,000
And this book is also stocked to the brim with many other examples of mass surveillance, behavioral predictions, and it goes so, so much deeper than this video.

157
00:13:19,000 --> 00:13:20,000
Amazon link is in the description.

158
00:13:20,000 --> 00:13:21,000
I just like the book.

159
00:13:21,000 --> 00:13:25,000
This is not an advertisement, and I do not get affiliate payments off of this whatsoever.

160
00:13:25,000 --> 00:13:26,000
I'm just a fan.

161
00:13:26,000 --> 00:13:29,000
And some final thoughts from the Oxford PhD Maya Shankar.

162
00:13:29,000 --> 00:13:44,000
When people were allowed to tweak an algorithm even slightly, they were more satisfied with the results of the algorithm and actually opted to use that algorithm over other algorithms that they knew performed better but had not involved their input.

163
00:13:44,000 --> 00:13:46,000
So it's pretty irrational behavior, right?

164
00:13:46,000 --> 00:13:49,000
But it shows that there's some egocentricity here that matters.

165
00:13:49,000 --> 00:13:52,000
Like, we really value our own contributions.

166
00:13:52,000 --> 00:13:58,000
So does this mean we're really in control, or are we just satisfied with choosing the algorithm that controls us?

167
00:13:58,000 --> 00:14:03,000
And will we start living in a post-AI age that is pre-deterministic?

168
00:14:03,000 --> 00:14:05,000
If so, by how much?

169
00:14:05,000 --> 00:14:07,000
I'm Austin with The Kotak Podcast over and out. Peace!

170
00:14:28,000 --> 00:14:30,000
.

