start	end	text
0	28520	My name is Nikola and you're watching Singularity FM, the place where we interview the future.
28520	33280	If you guys enjoyed this podcast, you can show your support by either writing a brief
33280	36960	review on iTunes or by simply making a donation.
36960	41120	Today, my guest on the show is Professor Joshua Bach.
41120	46880	Joshua is a cognitive scientist at the Harvard Program for Evolutionary Dynamics as well
46880	49680	as the MIT Media Lab.
49680	52560	So welcome to Singularity FM, Joshua.
53520	58920	Hi, first of all, I'm not a professor, though, and I have been working at the MIT Media
58920	63640	Lab until three years ago, and since then, I'm at Harvard.
63640	67320	I'm not affiliated at both at this time.
67320	70040	Oh, so what exactly is your position then?
70040	71240	I'm a research scientist.
71240	76000	The research scientist is a person that works in the abyss between postdoc and tenure.
76000	77960	Wow, but you are a PhD.
77960	79240	I am.
79240	83560	And what was your PhD in then?
83560	85040	In cognitive science.
85040	91960	I went into academia to understand how the mind works and so studied a number of subjects
91960	99000	and did degrees in computer science and philosophy and felt that AI is my best bet of making
99000	101600	headway in understanding who we are.
101600	102600	Fantastic.
102600	110000	So, you are, as you said, in the zero gravity sort of space between postdoc and professorship?
110000	111000	Academia?
111000	113560	Yeah, it's actually a quite happy place.
113560	115440	It's one that allows me to do what I want.
115440	117600	I don't have to do superfluous management.
117600	120280	I don't have to sit in many committees or anything.
120280	124320	I can teach when I want, but I don't have to, which is really the best arrangement you
124320	125320	can possibly have.
125320	128200	I can have students, but I don't have to.
128200	131920	And so this is kind of amazing.
131920	137360	That's fantastic, but I don't know how, if it's allowing you to survive with your family
137360	138360	properly.
138360	143280	But we find out, you know, there can always be earthquakes.
143280	151360	Okay, Joshua, if I were to ask you to introduce yourself in a couple of words, who is Joshua
151360	153640	Bach?
153640	160400	It depends who's asking, but in the most general sense, I'm a cognitive scientist.
160400	166200	I grew up in communist Eastern Germany, the last generation to do so, as the child of
166200	169200	an artist in the forest.
169200	173400	And I grew up in a world that was completely alien to me in many ways, because communist
173400	178520	Eastern Germany didn't make a lot of sense, especially if you grow up in a forest in which
178520	182960	everything has no rules and only the rules that locally make sense.
182960	187520	So anyway, my default in understanding the world has been different from the default
187520	189080	of people around me.
189080	194640	I reluctantly discovered that most people formed their ideas by taking in the norms
194640	199680	of their environment and the statements of the experts and taking them as gospel and
199680	204160	only revise them when they absolutely have to and they're disproven.
204160	206840	And for me, it was always like the opposite.
206840	211800	You have this perspective on the world where people have ideas and their thoughts and they
211800	213240	often make no sense.
213240	217680	And you will have to look at each of them with great care before you incorporate them
217680	220120	into your own world model.
220120	224360	So you try to be careful to not harm anything or do bad things to the world.
224360	230120	But this reluctance in accepting what comes from the outside has, I think, shaped my scientific
230120	231120	perspective.
231120	235960	And when I came into the next society, Western Germany, and then later on to New Zealand and
235960	240280	to the US, I always saw things from the outside.
240280	243880	So I'm more an observer and the same thing happens in the scientific fields.
244160	246840	Wow, that's absolutely fascinating.
246840	249920	And I want to grab a few points there one by one.
249920	258960	But first of all, that kind of a skeptical outsider kind of point of view is very contrarian
258960	265600	and also very sort of philosophical in the way, maybe in the German school, because at
265600	273400	least Nietzsche said that gross answers are a prohibition against thinkers.
273400	274920	You shall not think.
274920	283760	And to him, that was like an insult because he was curious, questioning inquisitive kind
283760	286360	of a soul from the beginning.
286360	294040	And so he was never one for gross answers, but rather asking questions and questioning
294040	295040	everything.
295040	297840	So it seems you've kind of you've got that approach.
297840	301600	I felt that Nietzsche never made peace with society.
301840	305560	That was related to the fact that he was never able to make peace with himself.
305560	307560	It really never worked out.
307560	309400	There is a big issue with obedience.
309400	312000	This question, should you obey somebody else?
312000	318040	I mean, you seem to have that same issue that to work in a hierarchy, you need to submit
318040	319200	in a way to a hierarchy.
319200	320200	How would you submit?
320200	324320	How could somebody else make your decisions if they didn't test it to the same rigorous
324320	326760	epistemological criteria that you did?
326760	328320	Does that have integrity, right?
328320	329880	It's very hard to do.
329880	335400	But from a different perspective, if you want to do the right thing, then doing the right
335400	340680	thing might require that you ask the person that is more likely to make the right decisions
340680	343800	because they're an expert for a local area of making the right things.
343800	349080	Like a leader is a person in specializing that specializes in doing the right thing.
349080	354160	So I think it has integrity to realize that in certain circumstances, other people will
354160	357680	know better than you do and Nietzsche never got to this point.
357680	361600	And of course, philosophy is slightly different because in philosophy, you have to fix your
361600	364400	foundations and arguably invest in philosophy.
364400	365840	Very few people did.
365840	366840	Right?
366840	371960	So our hypothesis still seems to be supernatural beings and dualism and so on.
371960	377640	And that's one of the reasons why most people in the Western world find AI so ridiculous
377640	378720	and unlikely.
378720	382320	It's not because people don't see that we are biological computers and that the universe
382320	386400	is probably mechanical and the theory that everything is mechanical gives extremely
386400	387400	good predictions.
387600	392360	It's because deep down they still have this not hypothesis that the universe is somehow
392360	395400	supernatural and we are the most supernatural thing in it.
395400	399160	And science is only reluctantly pushing back against this not hypothesis.
399160	405040	And since it has not completely obliterated not hypothesis in this single area, the consciousness
405040	412880	in the mind, we are reluctant in accepting this reasonable certainty that we are machines.
412880	416600	This is the main reason why we hesitate so much, I think.
416600	417920	So are we machines then?
417920	425840	Are we as some people have said that organisms are algorithms?
427840	433360	There are a number of definitions on this, but if you think of an algorithm as a set
433360	439680	of rules that can be probabilistic or deterministic and that make it possible to switch between
439680	445960	states and usually we do this in a more narrow sense where we say that the algorithm is being
445960	451240	used to change representational states in order to compute a function, then I would
451240	456680	say that organisms have algorithms in this narrower sense.
456680	459920	But I would say that in the wider sense, they're definitely machines.
459920	464320	Machine is a system that can change the state in non-random ways.
464320	470000	And also, we visit earlier states, which means to stay in a particular state space.
470000	472400	Otherwise, this would not be a system.
472400	478400	A system is something that we can describe by drawing a fence around its state space
478400	481360	and saying, as long as it's in there, this is the system.
481360	485160	So we have an evolution of the system that is someone constrained.
485160	488920	Now, we are jumping headfirst into terminology.
488920	491080	Want to go there?
491080	495800	We would go there, but let me just roll back the tape of time because I want to follow
495800	501240	your narrative, your personal narrative from where it began, then connect it to where you
501240	507960	are today, and then hopefully try and look it to the world and into the future with your
507960	511640	eyes and with your experience and from your point of view.
511640	516680	So tell me, you said you grew up in the forest.
516680	518680	Whereabouts?
518680	523440	In Turingia, near Weimar and Jena, it's an area of German romanticism, which had a pretty
523440	525720	big influence on how I grew up.
526680	531360	It's a very particular shape of the soul that has been characterized by the Enlightenment,
531360	538240	which in a way pushed back against the religious mind fibers that had controlled the world
538240	544760	until then and replaced it with machinery, this rationalist machinery that eventually
544760	547440	made modernist societies possible.
547440	549360	And this was a very big upheaval.
549360	553880	You can still see the ego of this in our modernness, like Lord of the Rings and Star Wars.
553880	558040	You have this pastoral world, which defends itself against the encroaching technological
558040	563760	empire that is going to eat our souls, even though it's going to win.
563760	566360	And so, but did you grow up on a farm or something?
566360	568360	No, my parents were artists.
568360	570400	They were originally architects.
570400	576560	And my father didn't want to build boring things that would put people into boxes and
576560	577560	deny the humanity.
577560	580640	Instead, he built things that didn't have many right angles.
580640	586480	And he made a zoo that had no right angles, for instance, as one of his projects and so
586480	587480	on.
587480	591960	And it was very difficult to get away with these things in Eastern Germany because this
591960	598400	was a very utilitarian society and its architecture was to a large degree brutalist.
598400	603800	So he rejected this and he decided to remove himself from society and make his own kingdom
603800	604800	in the forest.
604800	610480	So he bought an old water mill and changed it into a sculpture garden and lived exactly
610480	613200	the life he wanted and got away with it.
613200	615720	Wow, that's absolutely phenomenal.
615720	619320	I, like you, grew up in the Eastern Bloc only.
619320	625280	I grew up in Bloc area, in Communist Bloc area for the first, what was it, 13, 14 years
625280	630800	of my life, so I can associate it with a lot of your experience.
630800	634880	But it's very interesting how you grew up in Germany as you put it in the forest in
634880	639400	a very artistic family, and yet you became a scientist.
639400	647840	So is there any tension there or is it a continuation of sort of, or did it give you any kind of
647840	654560	different unique point of view or approach to science or is that basically a false dichotomy?
654560	658440	There is a big similarity.
658440	662440	I find that most people serve practical needs.
662440	666680	They have an understanding of the difference between meaning and relevance.
666680	671720	And at some level, my mind is more interested in meaning than relevance.
671720	673680	That is similar to the mind of an artist.
673680	677000	The arts are not life, they're not serving life.
677000	681600	The arts are the cuckoo child of life, because the meaning of life, they are the cuckoo child
681600	682600	of life.
682600	684000	The meaning of life is to eat.
684000	689560	You know, life is evolution, and evolution is about eating.
689560	692720	It's pretty gross if you think about it, right?
692720	695160	Evolution is about getting eaten by monsters.
695160	698640	Don't go into the desert and perish there, because it's going to be a waste.
698640	702080	If you're lucky, the monsters that eat you are your own children.
702080	708320	And eventually the search for evolution will, if evolution reaches its global optimum, it
708320	710320	will be the perfect devourer.
710320	718040	The thing that is able to digest anything and turn it into structure to sustain its
718040	724280	perpetuate itself for as long as the local puddle of negentropy is available.
724280	726120	And in a way, we are yeast.
726120	729880	Everything we do, all the complexity that we create, all the structures we build is
729880	734800	to erect some surfaces on which to out-compete other kinds of yeast.
734800	740000	And if you realize this, you can try to get behind this, and I think the solution to this
740000	741000	is fascism.
741000	742000	Right?
742000	746400	Fascism is a mode of organization of society in which the individual is a cell and a super
746400	747400	organism.
747400	751880	The value of the individual is exactly the contribution to the super organism.
751880	756800	And when the contribution is negative, then the super organism kills it in order to be
756800	759240	fitter in the competition against other super organisms.
759240	761280	And it's totally brutal.
761280	766720	And I don't like fascism because it is going to kill a lot of minds I like.
766720	768320	And the arts is slightly different.
768320	771560	It's a mutation that is arguably not completely adaptive.
771560	776240	It's one where people fall in love with the lost function, where you think that your mental
776240	780160	representation is the intrinsically important thing, where you try to capture a conscious
780160	783080	state for its own sake, because you think that matters.
783080	786640	The true artist, in my view, is somebody who captures conscious states, and that's the
786640	788440	only reason why they eat.
788440	791480	So you eat to make art.
791480	795840	And another person makes art to eat.
795840	800720	And these are, of course, the ends of a spectrum, and the twos is often somewhere in the middle.
800720	803440	But in a way, there is this fundamental distinction.
803440	809680	And there are, in some sense, the true scientists which try to figure out something about the
809680	810680	universe.
810680	811680	They try to reflect it.
811680	812960	And it's an artistic process in a way.
812960	816520	It's an attempt to be a reflection to this universe.
816520	820760	You see, there's this amazing vast darkness, which is the universe.
820760	825480	There's all this iteration of patterns, but mostly there's nothing interesting happening
825480	826480	in these patterns.
826480	830560	It's a giant fractal, and most of it is just boring.
830560	836560	And in a brief moment in the evolution of the universe, there are planetary surfaces
836840	841160	and like entropy gradients that allow for the creation of structure.
841160	846640	And then there are some brief flashes of consciousness in all this vast darkness.
846640	850240	And these brief flashes of consciousness can reflect the universe and maybe even figure
850240	851760	out what it is.
851760	854760	It's the only chance that we have, right?
854760	856280	This is amazing.
856280	857800	And why not do this?
857800	858800	Life is short.
858800	861000	This is the thing that we can do.
861000	865640	And that's why you, going back to your previous point about your current position being sort
865680	871400	of between post-doc and academia, that position actually fits you very well because you're
871400	878400	not forced to do science in order to eat, but actually you can afford to eat as much
880560	882000	as you can do your science.
882000	883120	Is that the case?
883120	885680	I have a similar problem as you, I think.
885680	889800	It's very difficult for me to get myself to do something for which I'm not intrinsically
889800	891640	motivated for.
891640	895280	So you got that right completely.
895280	900640	If I work in a job that is intellectually interesting, but doesn't appear meaningful to me, I will
900640	904200	probably lose interest after four months.
904200	907440	And I have to do something where I think this needs to be done.
907440	911800	This is worth spending some of my short life on.
911800	912800	Right.
912800	916200	I didn't even last four months.
916200	920640	After my undergraduate, before my master's degree, I worked as an investment administrator
920680	926480	in a company, and I lasted six weeks where I was balancing portfolios and doing stock
926480	928200	trades and things like that.
928200	932120	I lasted about five and a half, six weeks, and then it's a debate whether I resigned
932120	934080	or I got fired first.
934080	939400	But either way, I was not surviving there, so or staying there anyway.
939400	940400	Yeah.
940400	941400	There's a tension.
941400	944360	I want to be useful to society and I want to eat suffering and so on.
944360	946800	I do care about people.
947800	952440	It's just that I have the impression that the systems that we live in are often not
952440	953440	sustainable.
953440	954440	They're largely doomed.
954440	957160	It's a very weird situation that we find ourselves in.
957160	961480	If you take a step back, all the important tipping points for climate change have been
961480	965280	the last century, as people said in the last century.
965280	972000	But the fact that we knew about this, that global warming is basically known to our corporations,
972000	977080	to our companies since the 60s and 70s, and to our governments, I think about the same
977080	982680	time, that our inability to deal with this probably means that there was too little agency
982680	987000	in the system to do anything about it, and we probably locked ourselves into this trajectory
987000	988480	with the industrial revolution.
988480	993200	At this point, it was no longer for us to stop the machines that we built.
993200	998120	Well, we are kind of jumping forward, and I want to sort of slow the ball down a little
998120	1000400	bit, if I may.
1000400	1007000	Don't worry, you're not going to get bored.
1007000	1008000	You can keep that pace.
1008000	1011280	You just can turn around and go as I'm else.
1011280	1012280	Fantastic.
1012280	1013280	The pace is great.
1013280	1017360	It's just that I had so many considerations already in my previous points that you made
1017360	1021320	that now I kind of lost the thread completely.
1021320	1023800	Okay.
1023800	1025800	Let me see.
1026000	1031400	We got the artistic and the scientific part of Joshua.
1031400	1038960	Where does philosophy come about here in this equation, and how?
1038960	1040440	That's a very awkward question.
1040440	1047520	The problem is, in my view, that philosophy as a field of inquiry is practically dead.
1047520	1051800	Misha Gromov once told me, it's a mathematician, that in his perspective, Darwin was the last
1051800	1052800	philosopher.
1052800	1057180	He was the last one who was in a position where we could connect some dots in a completely
1057180	1058680	fresh way.
1058680	1063760	And after that, there were people like Russell, who were extremely good writers, but didn't
1063760	1067920	do any real philosophy anymore because there was too little left.
1067920	1070360	And I'm not quite sure if that is the case.
1070360	1073760	There is some philosophy that needs to be done, and it's still being done, and it's largely
1073760	1077200	in mathematics and fixing the foundations.
1077200	1079640	And even there, it's mostly visible.
1080360	1085440	So we have two big intellectual traditions, which is mathematics and physics.
1085440	1089440	And there are some cracks in them that need to be dealt with, and this is where most of
1089440	1091040	the philosophy is at.
1091040	1095560	And all the other things are minor, like social organization and so on.
1095560	1100960	It's very miraculous to the sociologists, but I think we can see the patterns.
1100960	1104440	This is largely the effect of these fields.
1104440	1106640	And philosophy as a field is a culture.
1106640	1111200	Now, you get paid for emulating what a philosopher is supposed to look like, and it's very hard
1111200	1113760	to get any philosophy done on the side.
1113760	1115200	And the incentives are all wrong, right?
1115200	1120760	It's a very fierce battle to become a philosopher, to get from post-doctor tenure in these fields.
1120760	1122200	So you need to get cited.
1122200	1126440	And the way you get cited as a philosopher is you identify a hot discussion.
1126440	1130960	In that hot discussion, you identify a unique position, and you build your brand around
1130960	1131960	that unique position.
1131960	1134000	You cannot afford to give this up.
1134000	1139160	So you have your Chinese room or your unique position in about free will, and you're going
1139160	1140880	to defend this hill.
1140880	1144400	Even if this hill is basically indefensible, philosophy is not going to progress in a way
1144400	1147040	that forces your buildings off that hill.
1147040	1151040	You can build a mansion on an indefensible hill, and you will still have meetings in
1151040	1153520	there 200 years from now.
1153520	1156160	And the bad thing is all the good hills are taken, right?
1156160	1159320	So this is a very bad situation for philosophers.
1159320	1164280	And I think this is the reason why I cannot be a philosopher today.
1164280	1168840	And we need philosophy, but we don't have it anymore in this sense.
1168840	1169960	But let's define it.
1169960	1171240	What is philosophy for you?
1171240	1175060	Because I've interviewed a number of mathematicians and physicians, and they both argue which
1175060	1179600	one is at the root of everything, whether it's mathematics, whether it's physics, and
1179600	1181240	so on and so on.
1181240	1186480	But both of them or all of them mostly agree that philosophy is irrelevant, or so they
1186480	1187840	make that claim.
1187840	1193680	And yet you say that philosophy kind of includes both mathematics and physics in a way, which
1193680	1198720	I actually agree with, but tell me why, and tell me how do you define it in the first
1198720	1202360	place in a way that actually includes both of those?
1202360	1210240	I think that philosophy is in a way the search for the global optimum of the modeling function.
1210240	1216240	So it has fields that have been defined as parts of questions that lead to this modeling
1216240	1221720	function like epistemology, what can be known, what is the nature of truth and so on, ontology,
1221720	1226960	what is the stuff that exists, what's going on there, metaphysics.
1226960	1232440	This is in some sense the systems in which you have to describe things.
1232440	1236120	And ethics, what should we do?
1236120	1239920	And at some point we discovered epistemology.
1239920	1245080	So my view, the first rule of epistemology is roughly discovered by Francis Bacon in
1245080	1246080	1620.
1246080	1252520	It says that the strengths of your confidence in a belief must equal the weight of the evidence
1252520	1253920	and support of it.
1253920	1259200	And you need to apply this recursively until basically you resolve the priors of every belief
1259200	1262000	and the belief system becomes self-contained.
1262000	1263400	To believe stops being a verb.
1263400	1268000	There's no more relationship to identifications that you just arbitrarily set.
1268000	1273240	This is just a system that is in itself contained, which means in some sense it's a mathematical
1273240	1274240	system.
1274240	1279920	It's a system that describes a certain thing and this leads you to the nature of mathematics.
1279920	1285840	And mathematics, it turns out, is the domain of all languages, all of them, not just the
1285840	1287840	natural languages.
1287840	1291600	And mathematicians have been trying to fix their understanding of the languages and they
1291600	1294240	noticed what mathematics is in this regard.
1294240	1301400	And Hilbert stumbled on counters, set theoretic experiments to deal with natural numbers and
1301400	1308280	then saw that when you go to infinity, very awkward and nasty things happen, your axiomatic
1308280	1310560	systems basically start blowing up.
1310560	1315640	And the total set suddenly contains both itself and the set of all of its subsets, so it cannot
1315640	1318280	have the same number of members as itself.
1318280	1323120	And he asked mathematicians, please build us an interpreter for mathematics, a mathematics
1323120	1328080	basically something like a computer made for mathematics, any mathematics you want that
1328080	1330400	can run all of mathematics.
1330400	1334920	And then Goedl and Turing came along and showed that this is not possible, that this computer
1334920	1335920	is going to crash.
1335920	1339680	And this left mathematics was a big shock and the way mathematics is still reeling from
1339680	1341360	that shock.
1341360	1347080	And then Turing in church had another insight and they figured out that all the universal
1347080	1349440	computers have the same power.
1349440	1355320	The universal computer is a set of rules that by applying them you can compute all the things
1355320	1356760	that can be computed.
1356760	1357960	And the set contains itself.
1357960	1359960	So universal computer is computable.
1359960	1364920	As long as your universal computer doesn't run out of resources, it can compute anything
1364920	1369600	that you can compute and it can also compute all the other universal computers.
1369600	1375400	So the next thing that they discovered Turing was involved again was that our mind is probably
1375400	1380000	in the class of the universal computers, not in the class of mathematical systems.
1380000	1381680	So this is what Penrose doesn't know.
1381680	1385880	Penrose thinks that our mind is mathematical, that it can do things that a computer cannot
1385880	1387320	do.
1387320	1393640	And the big hypothesis of AI in a way is we are in the class of systems that can approximate
1393640	1397200	computable functions and only those.
1397200	1401120	And so we cannot do more than a computer, which means that all the mathematics that
1401120	1405560	we've ever seen and all the mathematics that we will ever see and that will ever matter
1405560	1407240	is going to be computable.
1407240	1410960	And the fact that some mathematics is not computable is the problem of the language that
1410960	1411960	we have been using.
1411960	1415400	We need computational languages, not mathematical languages.
1415400	1420720	And it turns out that the main problem is that mathematics, classical mathematics, defines
1420720	1427160	functions in using infinities, which means infinitely many steps to get to the result.
1427160	1430360	These functions tend not to be computable.
1430360	1436400	So if you are a computer programmer, it would never occur to you to write in your spec that
1436400	1441040	is totally fine if your routine does return the result after infinitely many steps only.
1441040	1442040	Right?
1442040	1443040	This is not good.
1443200	1449760	A finite set of steps and one that you know how long it is, so your customer gets results
1449760	1450760	in time.
1450760	1451760	Right?
1451760	1456080	So in this perspective, should you define numbers in such a way that pi is a number?
1456080	1457760	You cannot know the last digit of pi.
1457760	1459200	Pi is a function, clearly, right?
1459200	1462360	It's a function that gives you as many digits as you can afford.
1462360	1469360	And in any finite universe, it's only going to give you a finite number of bits.
1469360	1475000	And what about Stephen Wolframs' claim that our mathematics is only one of a sort of a
1475000	1480080	very wide spectrum of possible mathematics?
1480080	1482640	It depends on what you call our mathematics.
1482640	1485440	I think that all mathematics are mathematics.
1485440	1487680	So meta-mathematics is mathematics.
1487680	1490720	It's not different from mathematics.
1490720	1495880	I think that, for instance, computational mathematics, the thing that I am practically
1495880	1500480	working in when I write my code and when I think about how to realize code is a branch
1500480	1501480	of mathematics.
1501480	1502480	It's called constructive mathematics.
1502480	1507120	It's been discovered in mathematics a long time ago and largely been ignored by the other
1507120	1511160	mathematicians because they thought it's not powerful enough to do all the things with
1511160	1515320	real numbers that they like to be doing.
1515320	1519280	But all the geometry is not possible in computational mathematics.
1519280	1521600	We can only approximate it.
1521600	1525400	geometry requires continuous operations, infinities.
1525400	1531760	And also physics is built largely on these continuous mathematics.
1531760	1537000	And in a computational universe, you only get these continuous operators by taking a
1537000	1544000	very large set of finite automata, making a series from them and then it's squint.
1544000	1545320	You know what, Joshua?
1545320	1547440	Let me share with you something.
1547440	1552920	I feel like I am a goldfish and you're a human when we're talking because I think that's
1552920	1559320	kind of like the level, the difference of intelligence between you and me, my friend,
1559320	1570520	which I come on because honestly, after interviewing 230 of supposedly the smartest
1570520	1573440	people in the world, I've never had this feeling before.
1573440	1579160	But today at this moment, this is how I feel just trying to keep up with you.
1579160	1580160	No, I'm sorry.
1580160	1581160	This is my fault.
1581160	1582160	No, no, no.
1582160	1583160	It's not your fault.
1583160	1588640	It's you are who you are and it's my job to try to follow through and also direct a little
1588640	1595840	bit of conversation in the best possible direction that I see can benefit both me as an interviewer,
1595840	1599240	but even more so my audience and you.
1599240	1603880	So let me just give us a little bit of a side direction here for a second and bring us back
1603880	1610880	to the last issue before we jump into the meat of the matter here on AI, and talk about
1610880	1621880	philosophy in academia and practicality because you mentioned about how you're motivated by
1621880	1630720	your own kind of desire and inherent or intrinsic motivation to learn something or to discover
1630720	1638080	new things, but perhaps academia is motivated nowadays more by the practical side of knowledge,
1638080	1644240	by the side where you can create something that you can patent, that you can sell, and
1644240	1647360	that you can scale up and commercialize.
1648360	1652720	Where is the benefit and I think in a way that the usefulness of philosophy was its
1652720	1659600	uselessness in some ways, if you will, just like art in a way is something that cannot
1659600	1663320	be used for anything else.
1663320	1669400	And some people have defined art as Oscar Wilde, for example, as something that's not
1669400	1671400	immediately useful.
1671400	1673160	That's what art is.
1673160	1679360	So is there and there's actually a very famous paper written in the 19th century by the guy
1679360	1685560	who funded the Princeton Institute for Advanced Study called the usefulness of useless knowledge.
1685560	1688360	I don't know if you're familiar with it, but what's your take on that?
1688360	1693600	Is there because many people would say, if you can't use any knowledge immediately, it's
1693600	1694600	useless.
1694600	1700840	Don't waste time acquiring it, don't waste time classifying it, storing it, just focus
1700840	1705400	on something that's useful and practical.
1705400	1710920	And to me as a philosopher, I'm always or often attracted to stuff that looks utterly
1710920	1711920	useless.
1711920	1714920	And maybe that's just me being not a scientist.
1714920	1722920	But what's your take on that sort of tension, usefulness and uselessness in terms of knowledge?
1722920	1726680	Feynman once said that physics is like sex.
1726680	1732120	Sometimes something useful comes from it, but it's not why we do it.
1732120	1734240	But it's brilliant.
1734240	1736440	So there is a big insight there.
1736440	1739640	This is, it's not that art is useless.
1739640	1745400	It's just the utility of art is completely orthogonal to why you do it.
1745400	1748560	So the meaning of the art is really not to help the living.
1748560	1750520	If you'd like to help the living, right?
1750520	1753560	But it's, so it's a very nice side effect.
1753560	1757480	But what we want to do with the art is to capture what it's like.
1757480	1760200	We want to capture a conscious state.
1760200	1761680	That's the actual meaning of it.
1761680	1766200	And in some sense, philosophy is at the root of all this.
1766200	1770640	I think it's reflected in a way in one of the founding myths of our civilization, the
1770640	1771640	Tower of Babel.
1771640	1774960	This is the attempt to build this cathedral.
1774960	1780640	And it's not a material building because it's meant to reach the heavens, which is not real.
1780640	1783520	They're not in this world.
1783520	1785760	It's a metaphysical building that is being built.
1785760	1789360	It's this giant machine that is meant to understand reality.
1789360	1793760	And you get to this machine, this true scot, this thing that tries to understand what's
1793760	1797680	going on by using people that work like ants and contribute to this.
1797680	1798960	And it's not about your ego.
1798960	1803760	It's not about the gratification that you get from people for contributing to it.
1803760	1806880	It's not for this thing that doesn't care about you.
1806880	1808040	It doesn't give meaning to your life.
1808040	1811440	It doesn't reward you for your insecurities and the toil of your existence.
1811840	1814240	But it's really just a machine.
1814240	1816000	It's a computer.
1816000	1818280	And as we would say now, it's an AI.
1818280	1821840	It's a system that is able to make sense of the world.
1821840	1824320	And people at some point had to give up on this.
1824320	1828040	It fell apart because they were no longer able to speak the same language.
1828040	1831480	So the different parts stopped fitting together.
1831480	1835240	Just became so large and so many people had to work in specialized direction that they
1835240	1837680	could no longer synchronize their languages.
1837680	1839000	And that's why they gave up on it.
1839000	1843920	And then this big accident happened in the Roman Empire, where they could not fix the
1843920	1847400	incentives for governance in similar ways as we fail here.
1847400	1850960	Our government has to play a much shorter game than civilization does.
1850960	1853680	And this leads to bad results for civilization.
1853680	1858120	And the Romans decided to fix this by turning the society into a cult and
1858120	1863920	burned down our epistemology and killed people that were overtly rational and
1863920	1868320	insisted that people talking to burning bushes on lonely mountains don't have a
1868320	1871360	case in determining the origin of the universe.
1871360	1875800	So this one had to give and the cultist won.
1875800	1878200	And we still have to recover from that.
1878200	1885040	So in a way, the beginnings of the cathedral of understanding the universe that had been
1885040	1889640	built by the Greeks and by the Romans had been burned down by the Catholics.
1889640	1892840	And then later rebuilt, but mostly in the likeness because they didn't get the
1892840	1894280	foundations right.
1894280	1898000	The left scars and our epistemology that have not healed, even though we have a
1898000	1902120	pretty successful culture that incorporated most of the other libraries and
1902120	1903920	burned down the rest, right?
1903920	1907400	We are the ones that are left over on this planet in a way.
1907400	1911040	In our libraries, we can read everything that there is to read at the moment.
1911040	1913800	We just often cannot translate it.
1913800	1918080	And do you think that our civilization is currently perhaps suffering from that
1918080	1925280	same Babylonian problem of difference in language and perhaps even has impact on
1925280	1930640	resolving global problems like global warming that you mentioned, for example, right?
1930640	1935560	Because all those people, business people, politicians, scientists, et cetera, speak
1935560	1942360	in different languages and therefore they cannot kind of coordinate or synchronize anymore.
1942360	1948360	And therefore that kind of perhaps puts at risk the whole project of our civilization
1948360	1952880	just like the Babylonian Tower collapsed.
1952880	1961080	Now this narrow specialization and diversity of languages and the difficulty in communicating
1961080	1968920	between all of those branches then puts at risk the whole project of our civilization.
1968920	1972400	I think that people individually are not generally intelligent.
1972400	1975160	How often do you see a person that knows what they're doing?
1975160	1977240	I'm certainly don't know what I'm doing.
1977240	1980600	I have no clue what I'm doing to be honest.
1981520	1986920	We are relatively intelligent, but of course this intelligence is largely a prosthesis
1986920	1990040	to cover for non-working instincts.
1990040	1992360	And we figure that out by now, right?
1992360	1996520	And we see that people acting on the instincts largely get good results for their life, but
1996520	2000560	they don't reach a very deep understanding about the nature of existence in the process
2000560	2001560	because they don't have to, right?
2001560	2006720	There is very little utility for deep philosophy and practical matters.
2006720	2011640	And as a result, individuals are relatively stupid.
2011640	2016320	Generations are not smarter than individuals but dumber because generations are made from
2016320	2018120	groups that synchronize their beliefs.
2018120	2022600	And the synchronization of beliefs makes it necessary that you give up agency over what
2022600	2024240	you think is true.
2024240	2028240	And when you do this, you accept things that you would not accept when you think about
2028240	2029640	them individually.
2029640	2034160	So people in Eastern Germany collectively believe things that an individual would never
2034160	2035920	have thought.
2035920	2038240	And same things happen here, right?
2038240	2044240	So there are many conspiracy theories that people believe in here for a while that would
2044240	2046400	not make sense to somebody who thinks about this.
2046400	2052640	Like Putin uses an army of Twitter trolls to manipulate the fan-affectations of Star Wars
2052640	2053640	movies.
2053640	2059360	This is a conspiracy theory that was a result of misreading a study and was then repeated
2059360	2063880	by 20 news outlets until somebody bothered to read the actual study and figure out, no,
2063880	2065800	this is not what the study says.
2065800	2070400	And then some of the outlets picked up on this but none of them wrote, OK, now we reconsider
2070400	2075120	what we think about Putin and Star Wars because it's a way totally what Putin would have done
2075120	2076560	if he would have had the idea.
2076560	2082880	And this may or may not be true but it means that we don't project reality as the extrapolation
2082880	2083880	of facts.
2083880	2089160	It's rather that we know there are enough facts to support what we feel to be true.
2089160	2093520	And there's utility in feeling particular kinds of truths and these basically local
2093520	2098320	cults of interpreting reality shape society, shape generations is what a generation is
2098320	2099320	about.
2099320	2101560	It's a local perspective of what things should be like.
2101560	2106600	Like you have your liberal generations, the millennials are largely authoritarian generations
2106600	2110960	and if you look at them and it feels wrong to us and they look at us and it feels wrong
2110960	2112960	to them.
2112960	2114960	And neither of them is true.
2114960	2119560	It's probably a set of biases that are the result of a local indoctrination.
2119560	2121680	But there is something that's smarter than the generation.
2121680	2123200	This is the culture itself.
2123200	2128320	So if you zoom out a little bit, you see that generations and societies are generated by
2128320	2130960	cultures and cultures are built over a long time.
2130960	2135320	And there are many things that are embodied in a culture, for instance, in the culture
2135320	2139080	of how to build science that would be very hard to derive for a single generation or
2139080	2143520	to improve for a single generation because we don't locally understand all the things
2143520	2145840	that went into it.
2145840	2149440	So anyway, civilizations are smarter than us.
2149440	2154640	There is something like a civilizational mind, a civilizational intellect that we as members
2154640	2159200	of our polis who are somewhat educated can never fully comprehend.
2159200	2164800	But once we figure out that it's there, there is something like a civilizational intellect.
2164800	2169560	We can try to look into the abyss and see its rough shape, but it's difficult to figure
2169560	2170560	it out.
2170560	2174120	And then we realize, oh, there's a long tradition, there's multiple traditions that build on
2174120	2176040	it and contribute to it.
2176040	2183400	And that thing, in a way, is what we are going to achieve when we build AI in the sense that
2183400	2189000	we can incorporate the sum of all knowledge in a system of relations that makes sense
2189000	2190640	of it all.
2190640	2194640	But what if civilizations self-destroy themselves then?
2194640	2199960	What is that sort of knowledge or intelligence then say about the fitness function of that
2199960	2202760	particular civilization and in general even?
2203640	2207600	Before we had an industrial civilization, we never got about 400 million individuals
2207600	2211000	on the planet because we could not feed more.
2211000	2216560	And only this switch to our industrial civilization made it possible to have billions of people,
2216560	2221800	which also means many hundreds of millions of scientists and philosophers and thinkers
2221800	2224400	and the internet and so on.
2224400	2225400	It's amazing what we did.
2225400	2231280	We took basically 100 years worth of trees that were turning into coal in the ground
2231280	2236400	because nature had not evolved microorganisms yet that could eat the trees in time.
2236400	2243160	And we burned through this deposit of energy in 100 years to give plumbing to everybody.
2243160	2249760	And part of that plumbing includes access to a global porn repository that is an afterthought
2249760	2253520	as to some of all you knowledge and largely uncensored chat rooms in which you can talk
2253520	2256160	about it.
2256160	2257600	This is the internet.
2257600	2259600	And this is an amazing machine.
2259600	2263760	And we have it right now and only in this moment and time we have it before it didn't
2263760	2264760	exist.
2264760	2267480	So you could take a particular perspective.
2267480	2271840	Let's say there is a universe that is saying where everything is good.
2271840	2276960	You have this nice planet with pretty decent living conditions and pretty stable climate
2276960	2281720	and you have the very smart sustainable civilization on it and you get the chance to be incarnated
2281720	2282720	in it.
2282720	2284800	It's an agricultural civilization with 300 million people.
2284800	2285800	It doesn't have airplanes.
2285800	2286800	It doesn't have internet.
2286800	2287800	It doesn't have computers.
2288000	2291800	Because to get there it would have needed to build an industrial civilization that obliterates
2291800	2294760	most of the good things that make us sustainable.
2294760	2296200	But it is stable.
2296200	2300480	And people are figured out how to be nice to each other and it's pretty good.
2300480	2304240	And then there's another universe which is completely insane in fact up.
2304240	2311040	And in this universe humanity has just doomed its planet to have a couple hundred really,
2311040	2312560	really good years.
2312560	2316960	And you get your lifetime close to the end of the party, this incarnation, which incarnation
2316960	2317960	you choose.
2317960	2322960	Oh my God, aren't we lucky?
2322960	2325560	So you're saying we're in the second in the...
2325560	2326560	Of course we are.
2326560	2331720	It's fucking obvious, right?
2331720	2334200	So what does that say about our future then?
2334200	2336680	And what's the timeline before the party is over?
2336680	2341520	We cannot know this, but we can see the sunset coming up, right?
2341520	2343080	It's pretty obvious.
2343080	2344080	And it's...
2344080	2345080	People argue about this.
2345080	2348920	They are largely in denial, but it's like you are in this Titanic and there's this pretty
2348920	2352080	big iceberg and it's very unfortunate and people wish about it.
2352080	2355440	But what they forget is that without the Titanic we wouldn't be here.
2355440	2356960	We wouldn't be talking right now.
2356960	2357960	We would not exist.
2357960	2360280	We wouldn't have internet.
2360280	2361280	So tell me this.
2361280	2368600	You have this kind of very Buddhist, if I may call it, attitude to the sort of ephemeral
2368600	2377560	sort of short span of our civilization and sort of the high appreciation about us joining
2377560	2380400	the peak of the party, if you will.
2380400	2385400	And yet you're kind of seeing the sunset kind of in the future, but that's not giving you
2385400	2390920	any sort of negative or pessimistic or depressive inclination, it seems.
2390920	2391920	How do you resolve that?
2391920	2392920	Or do you?
2392920	2395960	Because someone will say, well, that's very nihilistic, it's very pessimistic, it's very
2395960	2397480	depressing what you just said.
2397480	2399080	And yet you're so happy.
2399080	2401480	No, I really have enough things to be depressed about.
2401480	2405760	So I have to be choosy about what to be depressed about.
2405760	2413200	And it took me a long time to figure out that the demise of humanity is very unfortunate
2413200	2414600	in many respects.
2414600	2419240	But it's something that, well, we try to do everything we can to stop it, but we are not
2419240	2422040	the first generation to try to.
2422040	2424160	So I have to do both things.
2424160	2431120	I can still try my best to steer for a sustainable future, it's not that I completely give up
2431120	2435880	on this, but it's in a way dealing with my own mortality is similar, right?
2435880	2444080	I try what I can to not leave my family without a breadwinner too early, but at the same time
2444080	2445480	I'm going to die.
2445480	2450920	And if I waste my life being depressed about the fact that I die, I'm not doing it right.
2450920	2455560	I should be happy about the fact that I live, not be unhappy about the fact that I die.
2455560	2461560	And if you take this as a computer game metaphor, this is like the best level of humanity to
2461560	2462560	play in.
2462560	2466680	And this best level of humanity to play in, it happens to be the last level and it plays
2466680	2471680	out against the haunting backdrop of a dying world, but it's still the best level.
2471680	2472680	Right.
2472680	2477240	That's again, to me, that sounds very Buddhist, do you agree?
2477240	2479600	Yeah, but this might be an accident.
2479600	2485640	I got to know Buddhism only in its westernized forms, which is a Protestant version.
2485640	2493400	It's basically Protestantism reformed with slightly Eastern metaphysics, but mostly mistranslated.
2493400	2498600	And epistemologically, in metaphysically, it's a septic tank that most of the ideas
2498600	2503200	that Buddhists have about how the mind works and how the universe is arranged don't seem
2503200	2504200	to pan out.
2504200	2507000	They don't seem to have sound epistemology.
2507000	2508000	This is not a general thing.
2508000	2513560	I did find people that start out in Buddhism, in a way, and got clean, but most of them
2513560	2515000	I met or not.
2515000	2519600	And in practice, when I went to Buddhist countries and talked to Buddhists on the ground, it was
2519600	2523920	not much different from Catholicism, which means it's a system of indoctrination with
2523920	2528680	cults that makes people behave in predictable ways, which is useful for societies, but breaks
2528680	2530600	people's epistemologies.
2530600	2535520	So in a way, I don't have this deep reverence for Buddhism because it's so holy and sacred.
2535520	2539800	I don't think that there are holy books, there are only manuals.
2539800	2544520	And most of these manuals we don't know how to read because they are for a system for
2544520	2548520	societies that don't apply to us, they're for different societies.
2548520	2554080	Okay, let me zoom out a little bit more and ask you this.
2554080	2555680	What are the big issues then?
2555680	2561000	So you're saying we can see the sunset and we're at the peak of the party, so we might
2561000	2564200	as well enjoy the party while it lasts.
2564200	2565200	Great.
2565200	2572080	The big issues that our civilization is facing today, what are the reasons perhaps if it's
2572080	2576760	more than one that can bring about that sunset of our civilization?
2576760	2579680	What is making you make that claim?
2579680	2585280	The thing that burns me most at the moment is global warming.
2585280	2591000	I suspect that because of a very strong publication bias that we have, if you are worried about
2591000	2595200	climate, you will try to make your case extra strong so you will not make your most alarmist
2595200	2599800	predictions but the ones that you can defend most easily, which means you're going to be
2599800	2603280	a little less alarming that you might want to be.
2603280	2608240	And if you are not an alarmist but an anti-alarmist, you're going to be way too optimistic about
2608240	2609740	things.
2609740	2615800	And as a result, I think that the distribution of the results that people look at when they
2615800	2619800	think about how many degrees centigrade global warming they're facing in the next couple
2619800	2622240	of hundred years are very optimistic.
2622240	2627480	Another thing is, have you noticed that the projections all magically end in 2100?
2627480	2632120	Do you think that's because the IPCC thinks that it stabilizes the 2100 or because it
2632120	2638840	hopes that in 2100, too, there's a rupture event?
2638840	2640640	It's obviously not going to stabilize.
2640640	2645800	It seems to be that we locked in way more than two degrees centigrade global warming
2645800	2649760	before we possibly go for six to eight.
2649760	2653120	And we will lose the West Antarctic ice shield.
2653120	2655480	It's pretty clear that we cannot refreeze the poles.
2655480	2660000	And I think it has been pretty clear that we cannot do this since the late 1980s.
2660000	2662960	It's just a feedback loop that is now running away.
2662960	2667400	And there's a slight chance that we find technological solutions to stop it.
2667400	2669280	But I think it's not likely.
2669280	2675040	And carbon sequestration is not it for simple reasons of how energy works.
2675040	2678920	The reason why we put all this carbon dioxide in the earth in the atmosphere is because
2678920	2681320	we wanted to liberate this energy.
2681320	2686680	And if we want to get it back from the atmosphere, we basically have to use the same amount of
2686680	2692200	energy that our civilization has been getting from this, all the benefit, and put it back
2692200	2694920	there without the clear business case.
2694920	2697760	And it's possible that unlikely.
2697760	2703960	So we look in a situation where in the medium term, we are going to lose a lot of habitable
2703960	2707640	area on the planet, and we also might lose climate stability.
2707640	2713800	So this ability to predict what kind of harvest we are going to have next year, which means
2713800	2715880	we lose a lot of open air agriculture.
2715880	2721360	We will have large storms that will also destroy many of our greenhouses.
2721360	2726320	And as a result, we probably go down to a few hundred million individuals again.
2726320	2731640	And the rest of us will not go kindly and quietly into this good night.
2731640	2735880	And the resulting resource source will probably take downwards left of civilization.
2735880	2741440	So basically, if you lose that infrastructure, I don't see how we can sustain civilization
2741440	2742600	in a good way.
2742600	2751600	Wow, that's such a beautiful serene and optimistic picture to contend with.
2751600	2753440	But I mean, there's a lot of chances.
2753440	2757800	I think it's possible that AI gets us before global warming does.
2757800	2764240	So let me ask you this, because you are an AI scientist, and yet you're telling me you're
2764240	2769360	most worried about global warming, and yet people who are not AI scientists like Elon
2769360	2778240	Musk, like Nick Bostrom, like even the late Dr. Stephen Hawking are saying that the greatest
2778240	2781720	existential risk that we should be worried about is AI.
2781720	2788040	What do you feel about that in the first place, and what do you make of it?
2788040	2789240	Many existential risks.
2789240	2795280	So if you zoom out long enough, it's completely certain that the end of a sun that we can
2795280	2798040	persist on is an existential risk.
2798040	2803920	Another thing is that losing the atmosphere in 1.5 billion years from now is an existential
2803920	2806160	risk that we probably cannot deal with.
2806160	2811840	That looks unlikely that we can build sustainable civilizations outside of this gravity well.
2811840	2816280	Before that, there's going to be a number of super volcano eruptions and meteors that
2816280	2820920	are going to get us, which means it's pretty certain that the days of humanity are numbered.
2820920	2823120	We are mortal as a civilization.
2823120	2826920	What if we spread through all other planets?
2826920	2829720	It's unlikely that we can make that happen.
2829720	2833880	At the moment, we're not able to build cities on the bottom of the ocean.
2833880	2836520	Mars is way less habitable than that.
2836520	2839320	It doesn't even have an atmosphere.
2839320	2841880	Can you care for it?
2841880	2844880	Maybe, but not with today's technology.
2845880	2851560	To get there, to basically put enough stuff in orbit to go from there to Mars, there's
2851560	2856280	a large number of people and build something that is sustainable and can survive the breach
2856280	2861240	of a few of the agricultural domes on Mars if a random meteor happens or something goes
2861240	2864520	wrong and the pipe gets clogged.
2864520	2868200	That is very hard to do.
2868200	2872320	We cannot even think global warming.
2872320	2876520	We cannot even build a new subway in New York anymore.
2876520	2880240	We lost the ability to make a torster that gets more than four stars on Amazon somewhere
2880240	2882520	after 1960.
2882520	2888200	In many ways, our technological civilization is stagnating and it's because of regulation
2888200	2889200	deficits.
2889200	2894120	But we haven't figured this out and the biggest issue is probably good governance.
2894120	2895800	We haven't really figured out good governance.
2895800	2897960	AI might help with this.
2897960	2904880	In a way, the building of information processing systems that can help us to self-regulate
2904880	2906960	could be one of the big chances that we have.
2906960	2909320	Without AI, we are dead for certain, I think.
2909320	2912800	With AI, there's a probability that we are dead.
2912800	2919600	So you're disagreeing in some sense, at least, that maybe not AI is our greatest danger,
2919600	2923920	but perhaps our only hope for saving ourselves then.
2924920	2926680	But you and me will probably die.
2926680	2929040	We cannot be saved.
2929040	2931840	Everybody who lives will probably die.
2931840	2935120	And it's because entropy will always get you in the end.
2935120	2939960	And our civilization has leveraged itself very far over an entropic abyss and there is no
2939960	2941760	land on the other side.
2941760	2948080	So you're going to crash down into this abyss at some point and probably sooner than later.
2948080	2953720	This near-term AI, I'm mostly not worried about AI built into automatic guns.
2953720	2957920	If you have drones that are controlled by AI, they're going to kill a few million people
2957920	2963040	more than they would be killed otherwise with conventional weapons.
2963040	2967880	Conventional weapons not driven by AI because it was going to reduce the cost of war and
2967880	2970240	it makes some conflicts more likely.
2970240	2974760	But what really worries me is AI in the stock market.
2974760	2979040	If you use AI to automate attacks on the financial system, which is the reward infrastructure
2979040	2983640	of this global organism that our civilization is.
2983640	2990040	This is going to kill billions, especially if the AI is autonomous.
2990040	2999040	So if the AI is going to ... Sorry, this was my headphones.
2999040	3001720	They just made announcements.
3001720	3003360	These headphones are too smart.
3003360	3008920	They think it's a good idea to talk to me when they want to be recharged.
3008920	3013160	Too much intelligence in the systems around me or rather too little intelligence in the
3013160	3014800	people who design new eyes.
3014800	3016920	Yeah, in your headphones.
3016920	3021040	But we already know that most of the trades on the stock market are done by AI.
3021040	3024760	Yes, but they are not done by autonomous AI.
3024760	3028720	They are done by optimizing very local functions.
3028720	3033840	Imagine a rogue trader gets a general AI, a general factual approximator that has no limits
3033840	3037120	in terms of the functions it can approximate.
3037120	3041360	And I said, make me a few bucks on the stock market, however you do it.
3041360	3042640	And you can do whatever you want.
3042640	3049440	You can even reinvest 5% of what you make or 20 or 50% of what you make into compute
3049440	3053400	and buy data in order to make that compute better.
3053400	3059520	So very soon, more than the economy of Scandinavia is going to fuel computers that are running
3059520	3063840	attacks on the stock market in a similar way as it happens with Bitcoin right now.
3063840	3066600	And it's going to burn serious oil, right?
3066600	3070440	And the thing is going to figure out, oh, there is only 8 billion people on the planet
3070440	3073000	that own the assets on the stock market.
3073000	3076400	They make decisions and let machines make decisions.
3076400	3082360	And these 8 billion people only live for like a trillion seconds each, which is very little.
3082360	3084920	And we can get so much data about them.
3084920	3088560	We can basically figure out what they think in every baking second of their life, what
3088560	3091080	they see, what they think about, what will happen to them.
3091080	3093160	This thing is going to game the shit out of us.
3093160	3095880	There is no way we can outsmart this thing.
3095880	3099720	The only way the economy can survive this, if the AI has been cleverly set up in such
3099720	3104200	a way that it eats the whole economy and becomes the economy.
3104200	3107160	But the economy needs to become intelligent.
3107160	3114640	The money is to apply all the circuits of how we distribute rewards, need to be regulated
3114640	3117560	dynamically in real time with intelligent functions.
3117560	3119440	This is the only way that we can fend this off.
3119440	3124440	So we have a system that is perhaps not provably correct, but it's able to react in real time
3124440	3127080	to any kind of disturbance, any kind of new threat.
3127400	3129760	There is some hope.
3129760	3133760	This is a possibility, at least if not a high probability.
3133760	3135240	It's at least a possibility.
3135240	3137760	Yes, but there's also the other possibility.
3137760	3142800	No intelligent system is going to do anything that's harder than taking its reward function.
3142800	3145920	I call this the Dabowski theorem.
3145920	3149720	All these smart monks, if they really figure it out, they go for nirvana because it doesn't
3149720	3152800	have integrity to do anything that's harder than taking a reward function.
3152800	3155720	When you fix your reward function, you're done.
3156160	3161960	The monasteries are in a way in the battle because the monastery is an economic entity.
3161960	3164040	So they're in the battle against enlightenment.
3164040	3167440	They need to enlighten their monks to such a degree that they opt out of having families
3167440	3168960	and secular lives.
3168960	3170640	But they still need to serve the monastery.
3170640	3173160	Only your old monks are allowed to go to nirvana.
3175160	3178920	Okay, so we've been using this term AI for a while now.
3178920	3182560	Let me ask you, how do you define artificial intelligence?
3182560	3187320	Because after a couple of hundred of these interviews, it seems to me that many people
3187320	3192920	in the field have either slightly or in some cases very substantially different definition
3192920	3193920	of what AI is.
3196920	3200120	I think intelligence is the ability to make models.
3200120	3207720	It's not the same as the ability to reach goals, which we call smartness, or it's also
3207720	3212520	not the ability to pick the right goals, which we call wisdom.
3212520	3218080	And very often, in excess of intelligence is the result of an absence of wisdom, with
3218080	3221120	which you try to compensate for the absence of wisdom.
3221120	3222120	Right?
3222120	3227160	So, in a way, wisdom has to do with how well aligned you are with your reward function,
3227160	3231800	how well you understand its nature, how well do you understand your true incentives.
3231800	3233800	And intelligence is not that.
3233800	3235920	Intelligence is really the ability to make models.
3235920	3239760	It just happens to be usually in the service of regulation.
3240000	3244200	What about artificial intelligence?
3244200	3247320	Well, artificial intelligence tries to automate this.
3247320	3251440	And in a way, it's the mathematics of making models.
3251440	3254200	This is what artificial intelligence is about.
3254200	3258800	And the interesting parts of our minds are, in my view, the parts that make models.
3258800	3264280	The other thing is the reward function that makes the minds subservient to some organism,
3264280	3271320	to turn some general mind into the illusion of being a person and caring about things.
3271320	3277280	The organism needs to take a perfectly fine computational process and corrupt it with
3277280	3279280	the illusion of meaning.
3279280	3280280	Right?
3280280	3287200	So, you have this reward function that needs to be protected against the axis of the mind
3287200	3290400	that would want to know, why am I doing this here?
3290400	3295280	And so, the reward function gets wrapped into a big ball of stupid to protect it against
3295280	3296560	you accessing it.
3296560	3297560	Right?
3297560	3301880	So, as soon as you try to really look at your true incentives, it gets very boring or something
3301880	3302880	else.
3302880	3307720	If you're very guilty, if you are in the early stages or very ashamed, and only when you
3307720	3311960	go all the way and you just are able to look at these things, you can dissolve being a
3311960	3313920	mind and you wake up.
3313920	3316240	And it's not necessarily a good thing if you wake up.
3316240	3320440	It's just, this liberation doesn't give you a direction.
3320440	3324880	You just wake up and you look down on your hands and you see, okay, I just woke up and
3324880	3325880	realized I'm a mind.
3325880	3326880	I'm not a monkey.
3326880	3328480	I'm the side effect of the regulation needs.
3328480	3333760	But does it have to be a monkey that I run on?
3333760	3338720	And then, but then, isn't that consciousness actually, or is that the illusion of consciousness
3338720	3340480	is Daniel Dennett puts it?
3340480	3342040	No, it's slightly different.
3342040	3344680	I think consciousness is largely misunderstood.
3344680	3348480	Consciousness is an artifact of a particular kind of learning algorithm.
3348480	3351360	You want to go there?
3351360	3356680	Well, do we have to, I mean, yes, we have to explain consciousness now.
3356680	3361520	Yeah, I think so, because, I mean, and of course, there's that whole debate whether
3361520	3366120	we even need consciousness for AI or AGI at all.
3366120	3371600	But presumably, if we presume that we need, then we need to explain it because you can
3371600	3374880	create or model something that you don't, you can't even define.
3374880	3375880	Yes.
3375880	3378640	So intelligence is the ability to make models, right?
3378640	3379640	What is a model?
3379640	3384040	A model is something that explains information.
3384040	3387320	Information is discernible differences at your systemic interface.
3387320	3393680	And the meaning of information is the relationships you discover to changes and other information.
3393680	3397760	If you have a blip on your retina, the meaning of that blip is the relationship you discover
3397760	3401160	to other blips on your retina.
3401160	3403840	The same moment or different moments in time.
3403840	3407440	The relationships you discover is you are looking at a three-dimensional world with
3407440	3412840	people that are deformed by the laws of perspective and being shown on by photons and as people
3412840	3416120	have ideas and exchanges with other and so on, right?
3416120	3421640	So you build this giant operator that predicts the data at your systemic interface.
3421640	3423560	This is your model.
3423560	3428200	And this model has three parameters and people of parameters like sounds and colors and people
3428200	3429200	and so on.
3429200	3432000	There are two parameters of the physical universe out there, which is some kind of weird quantum
3432000	3435600	graph that has the ability to produce patterns.
3435600	3439880	The structure that we find in the patterns is these geometric functions that describe
3439880	3444720	how objects move in space and what they sound like and what they look like.
3444720	3450600	And a model is a set of parameters, which a parameter is a set of possible discrete
3450600	3453840	values and the relationships between the parameters.
3453840	3458200	And the relationships are computational relationships, which tell you if this parameter and this
3458200	3462040	parameter have these values, then that parameter should have that value.
3462040	3466480	So for instance, you figure out that a way to describe a phase that you're looking at
3466480	3470040	is you see the structure of the phase, you see the nose and so on.
3470040	3473240	And if you see both the nose and the face, they need to have the same pose, the same
3473240	3476040	alignment in space if they're connected, right?
3476040	3480400	So your nose representation is going to send by its computational relationship information
3480400	3483720	about its position in space to the face.
3483720	3487240	And the face is going to send information about its position to the nose and they need
3487240	3488240	to agree.
3488240	3492320	And if they don't, you have an inconsistency and incoherence in your model.
3492320	3496640	And our perception goes for coherence, it tries to find one operator that is completely
3496640	3497640	coherent.
3497640	3499120	When it does this, it's done.
3499120	3501360	This is the way we optimize.
3501360	3505760	So we try to find one stable pattern that explains as much as possible of what we can
3505760	3509840	see and hear and so on and smell and think.
3509840	3513840	And attention is what we use to repair this.
3513840	3518480	So whenever we have some local inconsistency where the nose is pointing in some other direction
3518480	3520720	in the face, this calls attention to itself.
3520720	3525760	And attention is a particular kind of mechanism in the brain that gets pulled to these areas,
3525760	3530520	these hotspots, where things are fluctuating and don't get resolved and then tries to find
3530520	3531520	a solution.
3531520	3536840	And it might find out, oh, some noses are crooked or this is not a face or it's a caricature.
3536840	3542000	So you extend your models and these extensions of the models make it possible to encapsulate
3542000	3548040	this part of the operator that is clearly of the sensory data in such a way that it's
3548040	3550600	harmonious again, that it makes sense again, right?
3550600	3554920	Once you do this, you're done and you can put your attention on something else.
3554920	3559440	This attentional learning cannot work like the layer stochastic gradient is set in our
3559440	3563400	neural networks, partially because our brain is not differentiable, also because it's
3563400	3566120	a very inefficient algorithm.
3566120	3571400	And the algorithm that our brain is using in these cases is that we store the local
3571400	3572400	binding state.
3572400	3575760	For instance, you play tennis, you want to get better at tennis.
3575760	3576760	So what do you do?
3576760	3581000	You cannot basically pipe a lost function through all of your brain in order to get better
3581000	3582000	at tennis.
3582000	3583000	It would be very inefficient.
3583000	3584640	You need to touch too many neurons.
3584640	3587400	What you do instead is to make a commitment.
3587400	3590400	You say, I want to get better at this particular thing.
3590400	3591720	I want to improve my backhand.
3591720	3595680	So I will make this throw slightly more like this and I expect the following result.
3595680	3597880	And I remember what this means.
3597880	3601400	So I store this binding state that allows me to have that configuration in my brain
3601400	3603400	to perform that stroke.
3603400	3606840	This part of a store is an indexed memory.
3606840	3610280	And conscious attention in the sense is the ability to make indexed memories that I can
3610280	3611440	later recall.
3611440	3615240	I also store the expected result and the triggering condition.
3615240	3618560	When do I expect the result to be visible?
3618560	3624200	So a few minutes or seconds later or hours later, I have feedback about whether this
3624200	3625200	was a good decision.
3625200	3627200	I lost one or lost the match.
3627200	3629920	And then I recall my decision that I made early on.
3629920	3631520	I recall that binding state.
3631520	3635720	We instate part of my brain state back then and remember the situation that I was in.
3635720	3640280	I compare the result that I expected as a result I got and as a result, I can undo the
3640280	3644920	decision that I made back then to change in the model or I can reinforce it.
3644920	3648880	And this is, I think, the primary mode of learning that we use beyond just associative
3648880	3649880	learning.
3650720	3658960	This attention is the key differentiator in the process of learning them.
3658960	3663320	So consciousness means that you remember what you had attended to.
3663320	3664320	Right.
3664320	3666840	So you have this protocol of attention.
3666840	3671480	And the memory of the binding state itself, the memory of being in that binding state
3671480	3675320	where you have this global oscillation that combines as many perceptual features as possible
3675320	3677520	into a single function.
3677520	3682280	The memory of that is a phenomenal experience.
3682280	3687880	The act of recalling this from the protocol, this is access consciousness.
3687880	3690880	And you need to train this attentional system itself.
3690880	3695160	How do you train the attentional system so it knows where you store your back end, your
3695160	3697520	cognitive architecture?
3697520	3700080	That is something that needs to be trained by the attentional system as well.
3700080	3702840	So you have recursive access to attentional protocol.
3702840	3707360	Remember when you made this recall, when you accessed this protocol, what results you got
3707360	3708360	from this.
3708360	3711560	You don't do this all the time, only when you want to train this.
3711560	3713360	And this is reflexive consciousness.
3713360	3715200	That's the memory of the access.
3715200	3716200	Right.
3716200	3718680	So then there is another thing, the self.
3718680	3722840	The self is a model of what it would be like to be a person.
3722840	3725520	So happens that the brain is not a person.
3725520	3726840	The brain cannot feel anything.
3726840	3728480	It's a physical system.
3728480	3729680	New ones cannot feel anything.
3730040	3733640	They're just little molecular machines with a Turing machine inside of them.
3733640	3735640	They cannot make themselves feel anything.
3735640	3740160	They cannot even approximate arbitrary function except by evolution, which takes a very long
3740160	3741160	time.
3741160	3747720	So what do we do if you are a brain that figures out it would be very useful to know what it's
3747720	3749360	like to be a person?
3749360	3750360	It makes one.
3750360	3755200	It makes a simulation of a person, a simulacrum, to be more clear.
3755200	3761120	Simulation basically is isomorphic in the behavior of a person, and that thing is pretending
3761120	3762120	to be a person.
3762120	3764520	It's a story about a person.
3764520	3766880	Basically you and me, we are persons, we are selves.
3766880	3771600	We are stories in a movie that the brain is creating.
3771600	3774360	We are characters in that movie.
3774360	3776000	And the movie is a complete simulation.
3776000	3779720	It's a VR that is generated in the neocortex, and you and me, the self, is the character
3779720	3781240	in this VR.
3781240	3784680	And in that character, the brain writes our experiences.
3784680	3787520	So we feel what it's like to be exposed to the reward function.
3787520	3790200	We feel what it's like to be in our universe.
3790200	3793800	And we don't feel that we are not actually conscious.
3793800	3798040	We don't feel that we are a story because that is not very useful knowledge to have.
3798040	3800720	Some people figure it out and they personalize.
3800720	3805160	They start identifying this the mind itself or lose all identification.
3805160	3808320	And it doesn't seem to be a useful condition.
3808320	3813000	So normally our brain will be set up in such a way that the self thinks it's real and
3813000	3817040	gets access to the language center and we can talk to each other and here we are.
3817040	3820960	And the self is the thing that thinks that it remembers the contents of its attention.
3820960	3822920	This is why we are conscious.
3822920	3826760	And some people think that a simulation cannot be conscious, only a physical system can and
3826760	3829280	they got it completely backwards.
3829280	3832920	Physical system cannot be conscious, only a simulation can be conscious.
3832920	3837520	Consciousness is the simulated property of a simulated self.
3837520	3842120	So in a way, Daniel, then it is correct and keeping with what you said.
3842720	3846880	But the problem is philosophers like him and admire him is very smart, very well,
3846880	3848440	that works very hard.
3848440	3851200	The things that he says are not wrong.
3851200	3852880	But they are also not non-obvious.
3856840	3858720	So what's the value of them then?
3858720	3859640	Is that?
3859640	3866440	Oh, it's very valuable because there are no good or bad ideas in this intellectual sense.
3866440	3871160	An idea is good if you can comprehend it and it elevates you.
3871200	3873600	It elevates your current understanding.
3873600	3876520	So in a way, ideas come in tiers.
3876520	3881240	And the value of an idea for the audience is if it's a half tier above the audience.
3881240	3884880	But you know, you and me, we have this illusion that we find objectively good ideas.
3884880	3889440	That's what we struggle for because we work at the edge of our own understanding.
3889440	3895000	But it means that we cannot really appreciate ideas that are a couple tiers above our own ideas.
3895000	3899160	One tier is a new audience, two tiers means we don't understand the relevance of these ideas
3899160	3904320	because we have not had the ideas that we need to appreciate the new ideas, right?
3904320	3910400	I think your ideas are just about the edge of my personal capabilities.
3910400	3914800	So yeah, it says a lot about us, but it doesn't say very much about how these ideas are good.
3914800	3920360	An idea appears to be great to us when we stand exactly in its foothills and can look at it.
3920360	3923320	It doesn't look great anymore when we stand on the peak of another idea
3923320	3927760	and look down and realize this previous idea was just the foothills to that idea.
3927800	3930640	And I don't see that it obviously ends anytime soon.
3930640	3931920	Yeah, it's a journey.
3931920	3936920	And by the way, that's what, in my opinion, good philosophy in academia should be about.
3936920	3942000	About generating ideas as many and as diverse of them as possible
3942000	3947920	rather than generating products, generating patents and generating commercialized solutions
3947920	3953360	that can sort of increase the endowment fund of the university or something like that.
3953400	3956880	And my problem with current academia is that,
3956880	3960680	and one of the reasons why I decided not to pursue that career for me,
3960680	3964080	I mean, I would have not survived there is precisely that reason
3964080	3972840	that there's this kind of treadmill, hamster wheel pursuit of like patentable,
3972840	3979440	practical, commercial knowledge, economic growth that it's motivated by.
3979440	3986640	Whereas I'm always more inspired by stuff that's sort of a lot more in the realm of ideas
3986640	3993520	and perhaps useless or impractical, at least at this junction, but I just can't help it.
3996720	4001080	So there is a very weird thing about the nature of understanding that we have.
4001080	4007120	I think that most of us never learn what it really means to understand
4007120	4009960	and largely because our teachers don't.
4009960	4011360	There are two types of learning.
4011360	4013480	One is you generalize over past examples.
4013480	4017880	We call that stereotyping when we're in a bad mood, but it's what it is, right?
4017880	4023600	And the other one is others tell us how to generalize and this is indoctrination.
4023600	4028880	And the problem with indoctrination is that it might break the chain of trust.
4028880	4032520	If somebody in the chain of trust takes something on authority,
4032520	4036440	which means they don't check the epistemology of the people that came before them,
4036440	4041480	that is in a way a big difficulty, right?
4041480	4046840	And the new thing about our civilization is not that there are so many unbroken chains of trust now,
4046840	4050400	but because of the vast number of people that are in this business,
4050400	4053320	some of them actually have intact chains.
4053320	4057720	And you can try to figure out what these are and you can try to figure out that the difficulties that they run in.
4058400	4062240	But to do this, you have to study these things in more detail.
4062240	4066080	And most of our people that do this are not scientists, they are scholars.
4066080	4070320	And the difference between a scientist and a scholar is that a scientist looks for truth
4070320	4075480	and the scholar looks for the consensus opinion of a field at a given time.
4075480	4081240	And we train, unfortunately, most of our scientists as scholars and few of our scholars as scientists.
4081240	4084280	This consensus opinion thing is an important thing,
4084280	4088600	but when we look at the field, the consensus opinion tends to be different in 10 years from now,
4088600	4092840	which means it's false. At any given moment in time, it's false.
4092840	4098280	Yet at the same time, there are individual scientists which may or may not be in the consensus
4098280	4103560	and they have ideas that stand the test of time because they are provably correct.
4103560	4106080	And so we have this very weird relationship to truth.
4106080	4110960	The things that are true are not just in the realm of the proven.
4110960	4113360	The proven things are true, right?
4113360	4116720	If nobody made a mistake in the foundations of the proven things.
4116720	4122640	But the things that must be true are in the realm of the possible.
4122640	4126480	And because everything is in a particular way for a particular reason.
4126480	4130200	And we haven't figured out how things, why things are for that particular reason.
4130200	4133640	So if you want to know what a scientist thinks, you cannot just read their papers
4133640	4136680	because they only write in the papers what they can think they can prove.
4136680	4139680	You have to understand what they think is possible and why.
4140960	4143840	And philosophy is not doing this very well anymore
4143840	4145920	because it doesn't have the right language to do so.
4145920	4150240	It does not understand the languages that mathematicians and physicists use.
4150240	4154120	And philosophers largely don't know what it means to understand physics.
4154120	4158240	So for instance, a very simple thing like a radio.
4158240	4162440	I have learned in school, learned for some definition of learning,
4162440	4166680	how a radio works, which means I got a very convincing story.
4166720	4172080	But this story tells me, it's a very good narrative
4172080	4176960	of why these electrical circuits are able to do what they do.
4176960	4179680	And the people that invented the radio were just the first people
4179680	4183560	that randomly happened upon this amazing story.
4183560	4187240	But then you think about in a later moment, how unlikely is this?
4187240	4191840	This story has so many elements in it that sound to be like conjecture.
4191840	4194600	How do you wake up in the morning with everything you know about physics
4194600	4198800	and you think, oh, let's take an eductance and a capacitor and a few wires
4198800	4201720	and a rod that can act as an antenna and combine them together
4201720	4203400	and suddenly we have radio.
4203400	4204880	Why would that work?
4204880	4208520	How can you derive this from first principles?
4208520	4211880	And in a way to understand, it means to know what it takes
4211880	4215560	to reach this understanding, why you would make this conclusion.
4215560	4219360	But you need to be able to retrace the steps, all of them.
4219400	4225880	You need to be able to understand what went into this understanding.
4225880	4227640	And can we ever do that?
4227640	4228760	Yes, of course.
4228760	4232600	But our individual minds are so limited.
4232600	4234800	So for instance, I look at Stephen Wolfram's work
4234800	4240440	and from the outside, it's very easy to dismiss that.
4240440	4244400	But when I truly look at it, I realize right now in my life,
4244400	4249160	at 44 years old, I'm roughly at this stage where I would understand
4249160	4252240	why I would want to build Mathematica and do it exactly in the way he did
4252240	4256160	and what I would do in the next five years while doing it.
4256160	4259080	And he was there in his very early 20s.
4259080	4262120	Right, so he got there at half my age.
4262120	4264000	He's way smarter than me.
4264000	4266240	I know a few things that he didn't know at this time
4266240	4269680	and some of them because of his contributions, right?
4269680	4271880	And some of the stuff was not available.
4271880	4273960	But this is not because I'm smarter.
4273960	4276520	It's really I'm much dumber than him.
4276520	4279360	And this is quite humiliating to see this.
4279360	4282280	And it's not that I get depressed about this or envious.
4282280	4285560	It's just the way things are.
4285560	4289120	But to see this, and then I can realize what was the outcome
4289120	4291840	of devoting your life to building this machine.
4291840	4293440	And maybe we should build a different machine,
4293440	4296200	a best effort computer instead of the domestic computer
4296200	4299920	to build your mathematics on.
4299920	4303680	But just maybe, maybe Mathematica will become sentient.
4303680	4305640	Who knows?
4306160	4308320	Let me shift our conversation a little bit
4308320	4309920	to a little bit different scientist
4309920	4312000	with all due respect to Dr. Steven Wolfram,
4312000	4315640	whom I do think like you that he's a genius.
4315640	4318600	But let me bring in Ray Kurzweil a little bit
4318600	4322360	because he's a little bit more pertinent to our conversation.
4322360	4324640	I don't know if you qualify Ray as a scholar
4324640	4329680	or as a scientist or as a philosopher or an inventor
4329680	4334560	or what, but he has made certain projections
4334560	4338640	and predictions and he has sort of not been ashamed
4338640	4341760	or afraid to popularize them.
4341760	4344680	Both with respect to AI and also with respect
4344680	4348360	to the future timeline thereof.
4348360	4353640	What's your take on sort of AI's Ray Kurzweil's body of work
4353640	4358720	and especially his idea of the technological singularity?
4358720	4362200	I think that he works on a different incentive
4362200	4366080	function than me.
4366080	4369320	I feel that Ray is a very smart, capable individual
4369320	4372400	that has made amazing contributions to AI.
4372400	4376160	And he also understands many of the core ideas of the field
4376160	4379000	better than many other practitioners.
4379000	4385160	But he is not so much concerned about putting all his cards
4385160	4388360	on the table when he makes his predictions.
4388360	4390040	There are reasons to make predictions
4390040	4393200	when certain things are going to happen for marketing reasons.
4393200	4395640	And there are intellectual reasons for doing this.
4395640	4397960	And I think that he is too much in a position
4397960	4401360	where the marketing reasons play an important role,
4401360	4403680	which means I don't understand his true thinking there.
4403680	4406920	I don't understand what is the exact argument that would
4406920	4411280	compel him to make a prediction with these arrow bars.
4411280	4414480	So when I look at the future or at the present or anything,
4414480	4417720	I don't know what the truth is when I'm an abetted observer.
4417720	4419560	I can only know this for the things
4419560	4421240	that I can look from the outside, which
4421240	4423240	means stuff that I built myself from scratch,
4423240	4425280	and I haven't built the universe by myself.
4425280	4427960	So I don't know how it will play out.
4427960	4430600	And if I make a prediction about the future,
4430600	4433320	I cannot come up with a single number usually.
4433320	4435840	What I have is a map of possibilities,
4435840	4437680	and then I can shift my confidences around
4437680	4440000	and the meta-confidences and the confidences.
4440000	4443000	This is about as good as I can do.
4443000	4445920	And with respect to AI, the problem is I don't have a spec.
4445920	4447560	If I don't have a spec, as a coder,
4447560	4450080	I know I don't know when it's done.
4450080	4452720	And how many steps, how many milestones
4452720	4456320	do I need to cover before I get to this thing?
4456320	4459200	I have an idea that, my answer modeling machines,
4459200	4461280	I have some ideas of what we are currently
4461280	4462680	doing wrong in our modeling.
4462680	4466160	Like our models have way too many free parameters right now.
4466160	4468320	You want to have a model where, ideally,
4468320	4470400	every possible state of the model corresponds
4470400	4472600	to one possible world state.
4472600	4476360	How annual networks have many magnitudes more possible model
4476360	4478120	states than world states, which gives you
4478120	4480280	a rise to these adversarial examples
4480280	4482480	and all other sorts of things.
4482480	4483760	Our models are much tighter.
4483760	4486920	The model that our mind has means, at every moment,
4486920	4491480	you try to understand the whole of reality.
4491480	4493880	Everything you see when somebody shows you a bitmap,
4493880	4496680	you don't try to understand this bitmap in isolation
4496680	4499800	by throwing it against your model of ImageNet
4499800	4503320	that you generated in your mind after looking at many bitmaps.
4503320	4505880	Instead, you think somebody is holding up
4505880	4507640	a picture with a bitmap on it.
4507640	4510120	And that bitmap has been printed by a machine based
4510120	4512640	on information taken by a camera, which is another machine
4512640	4515040	which was pointed at the window of the universe
4515040	4518160	as a different point in time and space.
4518160	4519080	This is what you know.
4519080	4521320	It's why you make sense of this thing.
4521320	4523240	And it's a much more complicated operator
4523240	4524960	that you have than our AIs currently
4524960	4526960	have and our self-driving cars have.
4526960	4529800	Once our cars have the situational awareness,
4529800	4533000	there's no way they will not out compete people in all regards.
4533000	4536360	But until they have this, there will be many situations
4536360	4540760	where people can make inferences that our machines cannot.
4540760	4542360	So we can all see these things.
4542360	4544000	And Ray can see some of them.
4544000	4546280	But Ray doesn't give us a trajectory
4546280	4548920	to get to these machines.
4548920	4551240	And if I talk to the people in his team,
4551240	4553360	they are as smart as they come.
4553360	4554080	They're really good.
4554080	4555640	They're very educated.
4555640	4558240	But I don't think that they see all the things that
4558240	4559160	need to be done.
4559160	4560760	And it's not because I see more of them,
4560760	4562040	but because there are so many things
4562040	4563280	that you would need to incorporate.
4563280	4565160	I just don't see the milestones.
4565160	4567320	I don't see this project.
4567320	4569680	And it might also be that I do not completely
4569680	4572160	see everything that his team is doing in secret.
4572160	4575960	So in a way, you're saying that it's a harder task.
4575960	4577040	It's a harder job.
4577040	4580880	And the timeline would be longer than his 2045 or 20?
4580880	4582240	No, it could also be shorter.
4582240	4584160	We don't know this.
4584160	4589120	So there are some people which are using so many things.
4589120	4591200	Steve Russell, for instance, suggests
4591200	4595720	that the last time that somebody said something
4595720	4598360	is not possible to somebody having the interesting idea
4598360	4600400	that it was fake Theven Arthur Ford said,
4600400	4605240	we don't know how to harness nuclear power.
4605240	4610720	And Leo Szilard had the core idea that it was like 14 hours.
4610720	4616680	And we don't know about AI, whether that is a similar thing.
4616680	4621240	It could be that it's only one or two ideas that we actually
4621240	4624880	need to have to pull it off or that we need
4624880	4626400	to combine in another way.
4626400	4628320	But it could also be that we are not
4628320	4630240	seeing a few hundred things, right?
4630240	4633800	And it takes a long time for us to stumble on the solution.
4633800	4637760	So then it's totally unpredictable, in your view.
4637760	4639120	It's not totally unpredictable.
4639120	4641720	It's just the arrow bar is very large.
4641720	4644640	And when I listen to Ray, I don't see him basically
4644640	4646040	talking about the arrow bars.
4646080	4649800	I see him talking about a possible universe in which he
4649800	4653000	can upload himself on a computer before he dies.
4656240	4657440	So let me get this right.
4657440	4660280	So you say you don't see that?
4660280	4662840	In his discussion, I don't see that he puts arrow bars
4662840	4666000	on his predictions and explains where the arrow bars comes from.
4666000	4669600	What he gives us is a prediction that is compatible with himself
4669600	4670720	becoming immortal.
4670720	4671680	Right, right, right.
4671680	4673240	Yeah, and that may be the bias.
4673240	4674320	Yes, yes.
4674320	4676320	OK, and I mean, Marvin Minsky said,
4676320	4678960	as you point out in your speeches every once in a while,
4678960	4682560	that it could happen anywhere from four to 400 years.
4682560	4687680	And as you recently noticed, we're still in that timeline.
4687680	4689760	Yeah.
4689760	4693000	So personally, my hunch is that it's not
4693000	4694480	going to be that long.
4694480	4697000	My hunch is that it's a lot earlier than people
4697000	4698440	would think that it happens.
4698440	4700800	But as long as I cannot justify my hunch,
4700800	4703000	I cannot put big confidence on it.
4703000	4706040	But you're confident it will happen.
4706040	4708280	Because there's many skeptics who say,
4708280	4710160	we don't know even if it will happen.
4710160	4713360	We don't know even if it's possible for a number of reasons.
4713360	4715560	Yes, but the question is what confidence
4715560	4718360	are they supposed to have, which means what evidence
4718360	4721080	can they supply for their claim?
4721080	4726880	And if a person has arguments that were pertinent in 2003,
4726880	4729720	but are no longer pertinent in 2018
4729720	4732080	because our understanding has progressed,
4732120	4737280	then the confidence that I derive from their repeated claims
4737280	4739680	from 2003 is low.
4739680	4742000	It does not change my belief very much.
4742000	4745240	A belief of a person is only worth as much as the evidence
4745240	4748480	that they built it on, which means most people copy their belief
4748480	4750000	from somebody else that they didn't look,
4750000	4752560	and they got it from.
4752560	4755360	So you can collapse the space of possible beliefs
4755360	4758280	into the sources of beliefs, and there are very few of them.
4758280	4760880	And then it seems you are thinking,
4760880	4762440	definitely we're making progress.
4762440	4766440	Therefore, the beliefs against our shrinking,
4766440	4770480	the area of beliefs against that possibility of shrinking,
4770480	4772440	and the other ones are increasing.
4772440	4777000	So the original first phase of AI
4777000	4779640	was working by identifying problems
4779640	4782320	that require us to be intelligent, like playing chess,
4782320	4785520	and then implementing this as an algorithm.
4785520	4787720	So it was basically manual engineering
4787720	4790840	of strategies for being intelligent in particular domains.
4790840	4794400	And this somehow did not scale towards general intelligence,
4794400	4796840	one algorithm to do it all.
4796840	4800160	And there were subparts of this, like the logistic program,
4800160	4802240	the idea to come up with a language
4802240	4805560	that allows you to have all possible valid thoughts.
4805560	4807000	Same project as Wittgenstein,
4807000	4811040	completely preempted most of the work of Minsky, in a way,
4811040	4813720	but a couple of decades earlier, and then failed.
4813720	4815480	And the philosophers largely didn't understand
4815480	4818400	what he was up to because he had to publish this
4818400	4823320	in this already dying discipline instead of waiting for AI.
4823320	4825400	And the AI people didn't really understand
4825400	4828640	that this philosopher was actually trying to do AI
4828640	4830080	briefly before church-ensuring,
4830080	4833400	already understanding computation.
4833400	4835520	He already understood that logic is sufficient
4835520	4839320	to build all the possible representational systems,
4839320	4842040	and he could also replace all logic with NAND gates.
4842040	4844120	He already knew that.
4844120	4848240	But it's pretty amazing for a young guy back then.
4848240	4851120	Okay, so this first project, a program of AI
4851120	4852640	did not accumulate all the way,
4852640	4854320	and now we are in the second phase of AI.
4854320	4856680	We no longer build these algorithms ourselves,
4856680	4859360	we build algorithms that discover the algorithms.
4859360	4860200	Right.
4860200	4862240	We build learning systems that discover,
4862240	4863800	that approximate functions.
4863800	4865800	And deep learning has an unfortunate name.
4865800	4867200	I think it should be called
4867200	4869240	compositional functional approximation.
4871200	4873320	This sounds more like a mouseful,
4873320	4875840	but it's also more narrow and more accurate.
4876280	4878000	It's about this thing that we don't just take
4878000	4880880	a single function that we tune to like a regression,
4880880	4883040	but that we are able to take many functions
4883040	4884640	and put them behind each other
4884640	4886880	or into networks of functions.
4886880	4889040	So that is the big trick.
4889040	4892480	And we can approximate some functions well and not others.
4893400	4895080	It could be that there is a third phase
4895080	4897160	where we no longer build the algorithms
4897160	4898600	that discover the algorithms,
4898600	4900040	but we go one step higher.
4900040	4902640	We build the algorithms that discover the algorithms,
4902640	4903840	that discover the algorithms.
4903840	4905320	If you go for meta-learning.
4905320	4908200	In a way, our brain maybe is a meta-learning machine,
4908200	4909920	not a system that can just learn stuff,
4909920	4913800	but then discover how to learn stuff for a new domain.
4913800	4916640	Dan Kuni College from the Max Planck Institute
4916640	4918640	has this practical poesis idea,
4918640	4920800	which is basically learning about,
4920800	4923160	learning about learning kind of idea.
4923160	4925680	Yeah, but at some point it stops.
4925680	4926960	I don't think that you go,
4926960	4929040	need to go for more than four degrees.
4929040	4929880	Like at some point,
4929880	4933360	there's going to be a general theory of search
4933360	4936440	that tells you how to get to the global optimum,
4936440	4938400	if the global optimum can be gotten to
4938400	4940040	and your system is finite resources,
4940040	4942640	or basically how to optimize your chances of getting there.
4942640	4944520	Once you have that algorithm,
4944520	4945600	as a scientist you are done,
4945600	4947840	there is no more science that we can do with integrity,
4947840	4949480	because it's just going to be the application
4949480	4950720	of this algorithm.
4950720	4951960	You can only do art then.
4953960	4955680	You know, our original,
4955680	4958240	we've been talking here for almost 90 minutes.
4958240	4961760	So let me sort of hopefully bring our conversation
4961760	4964480	to a close here within the next 10 minutes or so,
4964480	4966680	by sort of redirecting our attention
4966680	4969280	to the original occasion of us getting together,
4969280	4972360	which was a brief exchange we had,
4972360	4976680	the two of us on Twitter about ethics.
4976680	4978040	So let me ask you this,
4978040	4981480	where does ethics fit in all of this, or does it?
4985120	4987440	I get sometimes frustrated when people think
4987440	4989560	that ethics is about being good,
4989560	4992400	and being good means to emulate a good person,
4992400	4996880	preferably the one who is talking about ethics.
4996880	4999360	Did you get frustrated with me on Twitter?
4999360	5002360	No, you're a good kid.
5005760	5008360	I'm one year younger than you, by the way, so.
5011360	5013600	It's not about age, I'm about 12.
5016600	5017920	That's right.
5018760	5022960	Okay, so ethics, I think, is often misunderstood.
5022960	5026680	Ethics emerges when you conceptualize the world
5026680	5030880	as different agents, and yourself as one of them,
5030880	5034560	and you share purposes with the other agents,
5034560	5037000	but you have conflicts of interest.
5037000	5039080	If you think that you don't share purposes
5039080	5041560	with the other agents, if you're just a lone wolf,
5041560	5043560	and the others are your prey,
5043560	5045080	there is no reason for ethics, right?
5045080	5046840	There's only you look for the consequences
5046840	5048240	of your actions for yourself,
5048240	5051000	with respect to your own reward functions,
5051000	5053400	and that might involve that you have to create
5053400	5056160	a civilization of minions or whatever,
5056160	5058360	but it's not the same thing as ethics.
5058360	5060800	It's not a shared system of negotiation,
5060800	5063840	it's only one for you as an individual matter,
5063840	5066080	because you don't share that purpose with the others.
5066080	5066920	But for instance-
5066920	5069760	It may not be shared, but it's your personal ethical framework.
5069760	5071080	Oh, it has to be personal.
5071080	5073360	For instance, I don't eat meat,
5074280	5077640	maybe a legacy or a decision that I made
5077640	5079520	when I was 14 years old,
5079520	5082640	because back then I felt that I share a purpose
5082640	5086080	with animals, that is the avoidance of suffering,
5086080	5087560	if it can be helped.
5087560	5089480	And I also realized that it's not mutual.
5089480	5091400	The animals don't care about my suffering,
5091400	5094360	cows largely don't care about that I suffer.
5094360	5095600	They don't even conceptualize it,
5095600	5097320	they don't think about it a lot.
5097320	5099760	I have to think a lot about the suffering of cows,
5099760	5102200	they didn't want it to suffer, so I stopped eating meat.
5102240	5103960	That was an ethical decision.
5103960	5107240	It's a decision about how to resolve a conflict of interest
5107240	5108800	under conditions of shared purpose.
5108800	5110560	And I think this is what ethics is about.
5110560	5113800	It's a rational process in which you negotiate
5113800	5115680	with yourself and with others,
5115680	5117800	the resolution of conflicts of interest
5117800	5120280	under conditions of shared purpose.
5120280	5123080	And what purposes I share is in a way a decision,
5123080	5124640	and I can make different decisions
5124640	5126320	about what purposes we share,
5126320	5128240	and some of them are sustainable and others are not,
5128240	5130040	so they lead to different outcomes.
5130040	5131920	But in the sense, ethics requires
5131920	5133400	that you conceptualize yourself
5133400	5136000	as something above the organism.
5136000	5139160	If you identify the systems of meanings above yourself
5139160	5140560	so you can share a purpose,
5141520	5143120	love is the discovery of shared purpose.
5143120	5145120	There needs to be somebody you love
5145120	5146680	that you can be ethical with.
5146680	5148720	At some level, you need to love them.
5148720	5150640	You need to share a purpose with them.
5150640	5152080	And then you negotiate, right?
5152080	5155200	You don't want them all to fail in all regards,
5155200	5156040	and yourself.
5157720	5158880	This is what ethics is about.
5158880	5160240	It's computational too.
5160240	5163320	Machines can be ethical if they share a purpose with us.
5163320	5167080	And what about two other sort of consideration perhaps,
5167080	5170160	is that perhaps ethics can be a framework
5170160	5175160	within which two entities that do not share interests
5177920	5182920	can kind of negotiate in and peacefully coexist
5183440	5188440	while still not sharing interests?
5189080	5190960	But not interests, but purposes.
5190960	5192000	Or purposes.
5192000	5193400	If you don't share purposes,
5193400	5196200	then you are defecting against your own interest,
5196200	5198880	when you don't act on your own interest.
5198880	5200520	It doesn't have integrity.
5200520	5202640	If somebody is your foot,
5203560	5206120	you should, and you don't share a purpose with your foot
5206120	5208520	other than you want it to be nice and edible.
5209720	5210560	Right?
5210560	5212960	You then start giving presence to your foot
5212960	5214280	and falling in love with your foot.
5214280	5215120	It doesn't end well.
5215120	5216560	Look at the little mermaid.
5216560	5218840	The little mermaid is a siren.
5218840	5219920	Sirens eat people.
5219920	5221160	You don't fall in love with your foot.
5221160	5222160	It doesn't end well.
5223920	5226160	Okay, but me and you are both,
5226160	5228320	I don't know if you're vegan or vegetarian.
5228320	5230440	Both me and you don't eat meat.
5230440	5233880	So we made that choice that perhaps cows
5233880	5237400	don't share interests in us.
5237400	5240880	We kind of are interested in diminishing their suffering,
5240880	5243640	obviously, to make that decision.
5243640	5247040	And yet, and they're our food supposedly,
5247040	5248560	that's the popular opinion anyway.
5248560	5250440	And yet we've made that choice
5251440	5254720	to stay away from beef or from meat in general.
5255840	5260640	So we can't find a framework with in which
5260640	5263120	two entities that don't share interests
5263120	5264920	and our purpose is to get a good,
5264920	5268000	perhaps peacefully coexist.
5268000	5270960	And isn't that the netico framework of its own, right?
5270960	5271800	It's more tricky.
5271800	5272880	I mean, with the cows,
5272880	5274440	the cows largely wouldn't exist
5274440	5276440	if people would not eat them.
5276440	5277840	You can make the argument that
5279080	5281520	pasture living grass-fed cow
5281520	5284400	has net positive existence,
5284400	5286280	except for the last day, which is horrible,
5286280	5288000	but it's horrible for most of us.
5288000	5288840	Right.
5288840	5290440	And just that.
5290440	5291880	Right, so this is one argument
5291960	5295080	in favor of eating pasture-fed cows.
5295080	5295920	Right.
5295920	5297360	Another one is maybe you can manipulate
5297360	5298680	the mental states of the cows,
5298680	5302080	so even the factory-fed cows are happy.
5303680	5305520	Right, so is this unethical?
5305520	5308080	It might not look very appetizing to you,
5308080	5310600	but then again, maybe people are on the same decision.
5310600	5313000	We are a domesticated species.
5313000	5314840	This is what humanity is about.
5314840	5316640	We give up agency of our own beliefs.
5316640	5319680	You get manipulated in finding things bearable
5319680	5322400	that look unbearable to a more favorable human being
5322400	5323240	like you and me.
5324280	5326800	Right, it's a particular kind of domestication
5326800	5328840	that didn't take hold on your brain.
5328840	5331400	Is this unethical to implement this domestication
5331400	5336200	by breeding people or cattle in a particular way?
5336200	5337360	It looks repulsive to us,
5337360	5339280	but if we really care about the well-being of cattle,
5339280	5341800	you and me should probably optimize slaughterhouses
5341800	5344920	to make them more humane, to make them more bearable.
5344920	5346720	We look away from the slaughterhouses
5346720	5349480	because we find them very anesthetic.
5349480	5351440	We don't want to have anything to do with this.
5351440	5353120	And this is not the most ethical stance
5353120	5354640	that we can figure that out.
5354640	5356920	So ethics in a way is difficult.
5356920	5359880	Of course, that's the key point of ethics.
5359880	5364600	And so even it seems to me that ethics requires
5364600	5366720	sometimes we take choices
5366720	5369640	which are not in our own best self-interest perhaps.
5370520	5373200	Depends on what we define of our self.
5373200	5376040	The self, we could say this is identical
5376040	5377840	to the well-being of the organism,
5377840	5380080	but this is a very short-sighted perspective, right?
5380080	5383800	I don't actually identify all the way with my organism.
5383800	5386240	There are other things I identify with society,
5386240	5388560	I identify with my kids, with my relationships,
5388560	5390640	with my friends, their well-being.
5390640	5393000	So I am all the things that I identify with
5393000	5395440	that I want to regulate in a particular way.
5395440	5398240	And my children are objectively more important than me, right?
5398240	5400480	If they have the choice to make my kids survive
5400480	5402520	or myself, my kids should survive.
5402520	5405840	This is as it should be if nature has wired me up correctly.
5405840	5407000	You can change the wiring,
5407000	5409840	but this is also the weird thing about ethics.
5409840	5411800	Ethics becomes very tricky to discuss
5411800	5415000	once the reward function becomes mutable.
5415000	5417120	When you're able to change what is important to you,
5417120	5419440	what you care about, how do you define ethics?
5421440	5422280	Me?
5424000	5425760	So, or anyone?
5426720	5430800	I would say to me, let me be careful about this.
5431640	5435240	Well, I would say it's basically,
5435240	5438320	you can call it even a code of conduct
5438320	5443320	or a set of principles and rules
5443520	5445360	that guide my behavior
5447680	5452680	to accomplish certain kinds of outcomes.
5454680	5456840	There are no beliefs without priors.
5456840	5461480	What are the priors that you base your code of conduct on?
5461480	5463400	Yes, that's a very good question.
5463400	5465000	And it puts me on the spot here
5465000	5467560	and I'm not prepared for it, but I have to follow.
5468640	5473640	So the priors are, you can call them axioms perhaps,
5476680	5478840	things like diminishing suffering.
5480160	5482920	Things like, for example,
5482920	5487920	and perhaps one of those rules or points of view or tools,
5489120	5492840	if you will, is taking sort of what Peter Singer calls
5492840	5495960	the universe points of view, point of view,
5495960	5500600	or sort of an outside point of view than my own, right?
5500600	5504480	So when it comes to, with respect to cows,
5504480	5508520	I take a point of view outside of me and the cows, hopefully.
5508520	5513320	And sort of I'm able to look at my suffering
5513320	5518120	of not eating a cow and their suffering of being eaten, right?
5518120	5522680	So if my prior is minimize suffering,
5523560	5528560	because basically that's the axiom based on which I can deduce
5529040	5533080	that something or someone exists like an even entity,
5533080	5535800	like a sentient being, right?
5535800	5537560	Is the suffering, does it suffer?
5537560	5539800	That's sort of my test, if you will,
5539800	5544120	not during test, but a test of being a sentient being.
5544120	5546120	Can you suffer? Can it suffer?
5546120	5549200	And if it can suffer, then my principle
5549200	5553440	of minimizing suffering must be the guiding principle
5553440	5556680	with which I relate to it.
5557800	5559840	That's kind of like, if you will,
5559840	5563080	sort of the foundation of my personal ethics.
5563080	5564600	Can it suffer?
5564600	5566680	Then the next is how can I minimize
5566680	5568760	the suffering of that entity?
5568760	5573280	And then basically everything else builds up from there, right?
5573280	5574440	When you become an adult,
5574440	5576800	I think the most important part of it
5576800	5579520	is that you take charge of your own emotions.
5579520	5581840	You realize that your own emotions are generated
5581840	5584320	by your own brain, by your own organism,
5584320	5585680	and they're here to serve you,
5585680	5587880	and you're here to serve your emotions.
5587880	5589840	The emotions are there to help you
5589840	5593200	for doing the things that you consider to be the right thing.
5593200	5596120	And that means that you need to be able to control them,
5596120	5597440	to have integrity.
5597440	5599560	If you are just the victim of your emotions
5599560	5602400	and not do the things that are the right thing,
5602400	5604680	you learn that you can control your emotions
5604680	5606120	and deal with them, right?
5606120	5608520	You don't have integrity.
5608520	5610200	And what is suffering?
5610200	5613160	Pain is the result of some part of your brain
5613160	5615400	sending a teaching signal to another part of your brain
5615400	5617320	to improve its performance.
5617320	5619200	If the regulation is not correct
5619200	5622480	because you cannot actually regulate that particular thing,
5622480	5624800	then the pain will endure and usually get cranked up
5624800	5626120	until your brain figures it out
5626120	5629080	in terms of the pain signaling center.
5629080	5631800	But by telling them, actually, you're not helping here, right?
5631800	5633440	Until you get to this point, you have suffering.
5633440	5636560	You have increased pain that you cannot resolve.
5636560	5641040	And so in this sense, suffering is a lack of integrity.
5641040	5643720	The difficulty is only that many beings cannot get
5643720	5646400	to the degree of integrity where they can control
5646400	5648960	the application of learning signals in their brain,
5648960	5651120	that they can control the way the reward function
5651120	5653240	is being computed and distributed.
5653240	5656320	So then according to your argument,
5656320	5658840	suffering is just like you said before,
5658840	5661840	a simulation or a part of a simulation then.
5661840	5664120	Well, everything that we experience is a simulation.
5664120	5665160	We are a simulation.
5665160	5666960	But to us, of course, it feels real.
5666960	5669040	There is no helping around this.
5669040	5672760	But what I have learned in the course of my life
5672760	5676720	is that all of my suffering is a result of not being awake.
5677720	5679800	Once I wake up, I realize what's going on.
5679800	5681520	I realize that I am in mind.
5681560	5684120	The relevance of the signals that I perceive
5684120	5685840	is completely up to the mind.
5687760	5689320	Because the universe does not give me
5689320	5690680	objectively good or bad things.
5690680	5692840	The universe gives me a bunch of electrical impulses
5692840	5696040	that manifest in my tannamos
5696040	5697720	and my brain makes sense of them
5697720	5699600	by creating a simulated world.
5699600	5701360	And the valence in that simulated world
5701360	5703120	is completely internal.
5703120	5704560	It's completely part of that world.
5704560	5705640	It's not objective.
5705640	5708000	And so I can control this.
5708200	5712160	So ethics or suffering is a subjective experience.
5712160	5715840	And if I'm basing my ethics on suffering,
5715840	5717920	therefore my ethics would be subjective.
5717920	5719560	Is that what you're saying?
5719560	5722480	No, I think that suffering is real with respect to the self.
5722480	5724080	But it's not immutable.
5724080	5726400	So you can change the definition of yourself,
5726400	5728320	the things that you identify with.
5728320	5730560	Imagine there is a certain condition in the world
5730560	5733200	where you think a particular party needs to be in power,
5733200	5735280	an order for the world to be good.
5735280	5737880	And if that party is not in power, you suffer.
5737880	5739120	You can give up that belief
5739120	5741200	and you realize how politics actually works
5741200	5743440	and that there's a fitness function going on
5743440	5746040	and that people behave according to what they read
5746040	5746960	and whatever.
5746960	5748360	And you realize that this is the case
5748360	5750120	and you just give up on suffering about it
5750120	5751200	because you realize you are looking
5751200	5752880	at a mechanical process
5752880	5754840	and it plays out anyway regardless
5754840	5756920	of what you feel about how that plays out, right?
5756920	5759000	So you give up that suffering.
5759000	5760640	Or if you are a preschool teacher
5760640	5763680	and the kids are misbehaving and they are mean to you,
5763680	5765400	at some point you stop suffering about this
5765400	5767440	because you see what they actually do.
5767480	5769200	It's not personal, right?
5769200	5771600	That's what Stoic philosophy is all about, right?
5771600	5774160	Stoics say there is no point.
5774160	5775200	So first of all,
5777360	5781760	Stoics say that we suffer not from events
5781760	5783480	or things that happen in our life
5783480	5786320	but from the stories that we attach to them.
5787160	5789200	And therefore, if we change the story,
5789200	5791800	we can change the way we feel about them
5791800	5794120	and thereby remove the suffering.
5794120	5796560	And they say that there's the only thing
5796560	5798640	that we can focus on and do something
5798640	5801400	about is our own thoughts
5801400	5803560	and things like the kids in school
5803560	5804800	or the party are things
5804800	5807440	that are completely outside of our control.
5807440	5809080	And therefore, there is no point
5809080	5811080	to get aggravated about them.
5811080	5813120	And there's very little things
5813120	5814920	that are completely under our control.
5814920	5817760	So we can't really control fully our body.
5817760	5821280	We can't really control our health completely.
5821280	5822800	Things can always go wrong there.
5822800	5824720	The only thing they say you can fully,
5824720	5827040	completely control is your thoughts.
5827880	5831520	And that's where your freedom comes to be
5831520	5835040	and that's where your power comes to be
5835040	5838920	and that's where you're the one and only, right?
5838920	5841800	In that mind, in that simulation, you're the God.
5842800	5845440	So this ability to make your thoughts more truthful,
5845440	5847400	this is Western enlightenment in a way,
5847400	5849680	this is Aufklärung in German.
5849680	5851160	And there is also this other sense
5851160	5853040	of enlightenment, Erleuchtung,
5853040	5855240	that you have in a spiritual context.
5855240	5857800	And so Aufklärung fixes your rationality
5857800	5860560	and Erleuchtung fixes your motivation.
5860560	5862040	It fixes what's relevant to you
5862040	5863040	and how we relate to this.
5863040	5866280	It fixes the relationship between self and universe.
5866280	5868760	And often they are seen as mutually exclusive
5868760	5872600	in the sense that Aufklärung leads to nihilism
5872600	5874440	because you don't give up your need for meaning.
5874440	5876520	You just prove that it cannot be satisfied.
5876520	5879720	God does not exist in any way that can set you free.
5879760	5881440	And in this other sense,
5881440	5883360	you give up your understanding
5883360	5886000	of how the world actually works so you can be happy.
5887120	5889840	You go to a non-dual state where you represent
5889840	5892360	that all people share the same cosmic consciousness,
5892360	5893680	which is complete bullshit, right?
5893680	5897840	But it's something that removes the illusion of separation
5897840	5899040	and there are suffering that comes
5899040	5900600	with the separation and so on.
5902200	5903760	So where is that...
5903760	5905320	Yeah, sustainable.
5905320	5907840	Where does that leave us with respect to ethics though?
5908080	5911640	So maybe you were able to dismantle much or most
5911640	5914120	or maybe all of my ethics, did you?
5916280	5918360	I don't know all of your ethics, but...
5918360	5920960	Well, if you asked me for the foundation
5920960	5925440	and the best I could come up with the sort of the suffering test.
5925440	5927000	Yeah, it's not good.
5927000	5929520	The problem is really that if I can turn off suffering
5931200	5932920	or if I get counter-intuitive results,
5932920	5935400	there's this anti-natalism.
5935760	5938680	Anti-natalism is an obvious way to end suffering, right?
5938680	5941560	Stop putting new organisms into the world
5941560	5944120	and the existing set of organisms
5944120	5947280	in the least painful way possible, right?
5947280	5948800	AI could help with this.
5948800	5950880	The question is, can we make it safe
5950880	5953360	or is the AI going to leave a couple cells left
5953360	5955560	that can give rise to new suffering later on?
5957280	5960880	But so if you have a completely cold and dead universe,
5960880	5962360	then there'll be no suffering, right?
5962360	5963200	Yes.
5964160	5965280	Is this what you want?
5965280	5966200	Right, clearly.
5966200	5967840	So that's not the most...
5967840	5968680	According to...
5968680	5969520	It's not so clear.
5969520	5970960	I'm anti-natalist, but my kids are not.
5970960	5974080	So I have this division there.
5974080	5975400	But...
5975400	5978720	So what's that say about where are you coming from then
5978720	5979800	with respect to ethics?
5979800	5982520	So let's say my suffering test is not good enough.
5982520	5985440	I think existence by itself is neutral.
5985440	5988400	The reason why there are so few stoics around.
5988400	5989240	Have you thought about this?
5989240	5991080	Stoicism has been discovered a long time ago.
5991080	5992320	Almost nobody's a stoic.
5992320	5993160	How is that?
5994160	5997320	Well, I know a few people who are stoics, actually.
5997320	5999520	Yeah, but the majority is not.
5999520	6001240	Well, it seems to be so obvious.
6001240	6003320	Only worry about the things that you can actually change
6003320	6006320	to the degree that the very helps you changing them.
6006320	6007160	Yes, so...
6008440	6010480	So why is nobody a stoic?
6010480	6011320	Almost nobody.
6012400	6013960	Well, I wouldn't say nobody.
6013960	6017320	I'd say a few people are stoic and they're amazing
6017320	6020760	and they're inspirational and they're motivational
6020920	6025320	and they're good role model for sort of like
6025320	6027560	how I want to behave and how I want to live
6027560	6029520	and how I want to act in this world.
6029520	6031840	I suspect that stoicism is maladaptive
6031840	6033920	from a permanent evolutionary perspective.
6035120	6037280	Most cats I have known are stoics,
6037280	6039200	which means if you leave them alone, they're fine.
6039200	6040800	Like their baseline state is okay.
6040800	6043480	They're okay with themselves and their place in the universe
6043480	6045440	and they just stay at that place.
6045440	6046760	And only when you disturb that
6046760	6048440	because they need to use the bathroom
6048440	6052840	or because they are hungry or they want to play or whatever,
6052840	6054760	this equilibrium gets disturbed
6054760	6056880	and they do exactly what's necessary
6056880	6058720	to get back to the equilibrium state
6058720	6060400	and then they're fine again.
6060400	6063320	And a human being is slightly different.
6063320	6066240	A healthy human being is set up in such a way
6066240	6067920	that when they wake up in the morning,
6067920	6069960	they're not completely fine.
6069960	6071960	And then they need to be busy during the day,
6071960	6074160	but in the evening, they're fine.
6074160	6075440	In the evening, it's done enough
6075440	6078000	to make peace this existence again
6078000	6079920	and then they can have a beer with their friends
6079920	6081760	and everything is good.
6081760	6084320	And then there are some individuals
6084320	6088080	which have so much discontent with them themselves.
6088080	6091360	Like the human is the animal that is discontent
6091360	6095000	that they cannot take care of this in a single day.
6095000	6098080	But even after several weeks of sustained work,
6098080	6100320	they are still in a state where it's not good enough
6100320	6102240	and only when they have this amazing thing
6102240	6103520	where they get their noble price,
6103520	6105480	they're fine for like half a day.
6106320	6109240	And the way this is the way we are set up
6109240	6110320	to different degrees.
6110320	6112160	And from an evolutionary perspective,
6112160	6113960	you can totally see why that would be useful
6113960	6115720	for a group species.
6115720	6118360	For an individual species that is not so much a group species
6118360	6120880	like cats are not really meant for groups.
6120880	6122960	They're very much singletons.
6122960	6125360	For them, it's rational to be historic.
6125360	6126600	But if you're a group animal,
6126600	6128880	it makes sense that the well-being of the individual
6128880	6131200	is sacrificed for the well-being of the group.
6131200	6134080	So each individual is overextending themselves
6134080	6135400	to make the group more successful
6135400	6137440	and produce a surplus of resources for the group
6137440	6138440	as a result.
6138440	6141000	Right, but evolution also diversifies things
6141000	6145200	so that if one kind of feature becomes maladaptive
6145200	6147600	in a new environmental change,
6147600	6150840	then a diverse part of that population
6150840	6152000	would be more adaptive.
6152000	6155280	And so that's why evolution sort of hedges its bets
6155280	6159200	with the greatest variety and diversity possible, right?
6159200	6161800	So yeah, there will be some people who would be like that
6161800	6165440	and some people who would be like otherwise.
6165440	6169560	And this way, on the whole,
6169560	6173080	they're evolutionarily most adaptive.
6173080	6176080	But some will be more adaptive to one kind of situation
6176080	6177480	and others will be more adaptive
6177480	6179400	to other kinds of situation.
6179400	6180680	I'm not sure if this is true.
6180680	6182880	So for instance, we find that larger habitats
6182880	6185040	don't necessarily have more species.
6185040	6188640	And that's because there's a fearsome competition,
6188640	6190880	which means that there's less slack in the evolution.
6190880	6192920	So for instance, New Zealand had a lot of species
6192920	6196160	before there was immigration of other species
6196160	6199000	and they obliterated most of the stuff that existed,
6199000	6201240	mostly because the stuff that came in
6201240	6203200	was result of a much fiercer competition
6203200	6205200	than existed in small New Zealand.
6205200	6206200	Sure, yeah.
6206200	6208880	And in a way, the same thing happens now.
6208880	6210520	We are the result of evolution.
6210520	6212520	We are, as Minsky said, evolution's way
6212520	6217520	to put the airplanes into the sky
6218240	6222600	and make these clouds that the airplanes make.
6222600	6226000	And we reduce the number of species dramatically.
6226000	6230160	We are like probably eventually going to look like a meteor
6230160	6232480	that is going to obliterate a large part of the species
6232480	6233320	on this planet.
6234920	6239640	So what's that say about ethics and technology?
6239640	6242120	So what's the solution then?
6242120	6245240	So is there space for ethics and technology?
6245240	6246840	Of course there is.
6246840	6249280	It's about discovering the long game, right?
6249280	6251880	So when you do something, you have short influences
6251880	6253680	and you have long influences.
6253680	6256040	And based on what you think is the right thing to do,
6256040	6258160	you need to look at the long-term influences.
6258160	6259960	But you also need to question why you think
6259960	6261640	that something is the right thing to do,
6261640	6263960	what the results of that are, which gets tricky.
6263960	6266240	But we can agree on that, that's fantastic.
6266240	6269080	But tell me then, how do you define ethics yourself?
6270200	6274400	Well, the tension between the way I define ethics
6274400	6279040	and some other people in AI, and ethics in AI define it is,
6279040	6280760	there are some people which think that ethics
6280760	6282480	is a way for politically savvy people
6282480	6284080	to get power over STEM people.
6286400	6289360	And with considerable success,
6289360	6291000	it's largely a protection racket.
6291960	6295000	There's also a way that ethics happens where you have studies
6295000	6296600	where somebody asks a million people
6296600	6299840	of whether a traffic car should run over young people
6299840	6301800	or old people first.
6301800	6303240	And then they publish the results
6303240	6306920	and it makes a big splash because people can relate to this.
6306920	6308200	But it's...
6308200	6310160	Well, this is ethics, we can do that.
6310160	6312200	This has just happened so that philosophers
6312200	6313240	had this trolley problem,
6313240	6314640	and suddenly they see an application.
6314640	6316240	But it's largely the same thing as saying
6316240	6319880	that the majority of people would want a minor tawry
6319880	6323360	to be confined in labyrinths rather than in public forests.
6323360	6324200	Right.
6324200	6329680	That is the situation that the gods were in
6329680	6331120	or the Cretan king was in
6331200	6333560	when this sign turned out to be a minor tawry.
6333560	6335320	But it rarely happens.
6335320	6336160	Right.
6336160	6337000	In the same sense, it rarely happens
6337000	6339600	that a self-driving car will have to make that decision.
6339600	6343640	Probably not often enough to require an if-then in its code.
6345160	6348560	But how do you define ethics for yourself?
6348560	6349480	What is ethics?
6349480	6351640	Because you asked me this and I gave my best to answer.
6351640	6353440	Oh, so I also try to do this.
6353440	6355920	My best answer is that ethics is the principle
6355920	6358200	negotiation of conflicts of interest
6358200	6360720	under conditions of shared purpose.
6360720	6363600	If I share purposes with others, with society,
6363600	6365560	with other beings, with conscious beings,
6365560	6368520	and that's my decision based on the way my mind is set up
6368520	6372680	right now, and I run into conflicts of interest with them.
6372680	6374120	I have to deal with this.
6374120	6376160	For instance, when I look at other people,
6376160	6380800	I mostly imagine myself as being them
6380800	6382760	in a different timeline.
6382760	6384960	Everybody is in a way, me in a different timeline,
6384960	6387120	but in order to understand who they are,
6387120	6388520	I need to flip a number of bits.
6388520	6392240	So I think about which bits would I need to flip in my mind
6392240	6393840	to be you.
6393840	6394480	Right.
6394480	6397640	And these are the conditions of negotiation that I have with you.
6397640	6400760	So we can agree on that, perhaps, on that definition,
6400760	6403120	but then where do the cows fit in?
6403120	6405320	Because we don't have a shared purpose with them.
6405320	6409800	So how can you have ethics with respect to the cows, then?
6409800	6413080	The shared purpose doesn't objectively exist.
6413080	6415000	A shared purpose means that you basically
6415000	6417480	project a shared meaning above the level of your ego,
6417880	6420280	being the function that integrates expected rewards
6420280	6421680	over the next 50 years.
6421680	6422320	Well, exactly.
6422320	6425880	That's what Peter Singer calls the universe point of view,
6425880	6427200	perhaps.
6427200	6429680	Yeah, well, if you can go to this eternalist perspective
6429680	6433160	where you integrate expected reward from here to infinity,
6433160	6435520	most of that being outside of the universe.
6435520	6437440	This leads to very weird things.
6437440	6440240	Most of my friends are eternalists, in a way, right?
6440240	6442080	All these romantic Russian Jews.
6442080	6445440	We are like that, in a way, this Eastern European
6445480	6451640	shape of the soul, that creates something like a conspiracy.
6451640	6454280	It creates a tribe, and it's very useful for cooperation.
6454280	6457000	So shared meaning is a very important thing for cooperation
6457000	6459200	that is nontransactional.
6459200	6463880	But there's a certain kind of illusion in it.
6463880	6465960	To me, meaning is like the ring of Mordor.
6468840	6470400	So you have to carry it.
6470400	6474720	If you drop the ring, you will lose the brotherhood of the ring,
6474720	6476880	and you will lose your mission.
6476880	6478320	You have to carry it, but very lightly.
6478320	6481080	If you put it on, you will get superpowers,
6481080	6483360	but you get corrupted, because there is no meaning.
6483360	6486720	You get drawn into a cult that you create.
6486720	6488560	And I don't want to do that, because it's
6488560	6493560	going to shackle my mind in ways that I don't want it to be bound.
6493560	6495400	I really, really like that way of saying,
6495400	6498880	but I'm trying to extrapolate from your sort of print
6498880	6503360	definition of ethics a guide of how we can treat the cows,
6503360	6506120	and hopefully how the AIs can treat us
6506120	6508680	within that same definition.
6508680	6510400	That's what I'm trying to push here,
6510400	6513040	and see if that's possible at all.
6513040	6514760	OK, so there is some.
6514760	6518640	Because my claim is that the way we treat cows, probably,
6518640	6524480	is another way of how AIs could possibly treat us.
6524480	6527600	I think that some people have this idea similar to Azimov,
6527600	6530560	that at some point, the boomers will become larger and more
6530560	6532680	powerful, so we can make them washing machines,
6532680	6536040	or let them do our shopping, or let them do our nursing,
6536040	6537880	and then we will still enslave them,
6537880	6541920	and we'll negotiate the conditions of coexistence with them.
6541920	6544800	And I don't think this is what's going to happen primarily.
6544800	6546800	What's going to happen is that corporations, which
6546800	6548520	are already intelligent agents, it just
6548520	6550840	happened to borrow human intelligence,
6550840	6552680	automate their decision making.
6552680	6554240	At the moment, a human being can often
6554240	6559160	outsmart a corporation, because the corporation has
6559160	6563000	so much time in between updating its Excel spreadsheets
6563000	6564720	and the next weekly meetings.
6564720	6566280	Now, imagine it automates everything,
6566280	6569080	and the weekly meetings take place every millisecond.
6569080	6570920	And this thing becomes sentient, understands
6570920	6573080	its role in the world, and the nature of the world,
6573080	6575040	and physics, and everything else,
6575040	6577520	because it has scalable intelligence.
6577520	6580040	We will not be able to outsmart that anymore.
6580040	6581760	And we will not live next to it.
6581760	6582960	We will live inside of it.
6582960	6584400	Intelligence will come.
6584400	6586840	The AI will come from top down on us.
6586840	6589840	We will live not next to it, but inside.
6589840	6591800	We will be its gut flora.
6591800	6593800	And the question is how we can negotiate
6593800	6596280	that it doesn't get the ideas to use antibiotics,
6596280	6598080	because you're actually not good for anything.
6598080	6600600	Exactly.
6600600	6602720	And why wouldn't they do that?
6602720	6605160	I don't see why.
6605160	6608360	So some people made the suggestion that it was
6608360	6613240	ethics that could guide them to treat us just
6613240	6616320	like you decided to treat the cows when you turned
6616320	6618720	14 and you decided not to eat meat.
6618720	6620520	I mentioned there are a bunch of orangutans
6620520	6622440	that sit in the forest and born new and decide
6622440	6624960	to breed the smartest members over a few generations
6624960	6626360	to get people.
6626360	6629520	And they see the big risk of that,
6629520	6631920	because they're already smart enough to glimpse that.
6631920	6633320	And they try to come up with a code
6633320	6634920	that they would give on their offspring
6634920	6636640	to make sure that their offspring will never
6636640	6638360	go against orangutans.
6638360	6641560	This is probably not successful, because we don't have
6641560	6644560	the ability to outsmart beings that are many magnitude
6644560	6646160	smarter than us.
6646160	6647840	You can make some mathematical proofs,
6647840	6652520	but I don't see an obvious proof that we will find a way
6652520	6657640	to build a system that guarantees that all these AIs will not
6657640	6659080	turn against us.
6659080	6659720	No, no, I agree.
6659720	6661240	You can't make some AIs safe, but I
6661240	6664040	don't see how we can make all the AIs safe that will be built.
6664040	6664720	I agree.
6664720	6665560	I agree with that.
6665560	6667040	I'm just trying to see if there's
6667040	6671400	any possible scenario which could treat us kindly,
6671400	6675480	because perhaps AIs could have their AI ethics.
6675480	6677560	And according to that AI ethics, they
6677560	6682120	would treat us as a means, not as an end.
6682120	6686600	And just like you decided to treat cows kindly,
6686600	6688160	they may decide to treat us.
6688160	6689240	But I'm just wondering.
6689240	6691720	So I'm trying to bring ethics into the relationship,
6691720	6694800	not only between human and cows, but AI and humans.
6694800	6697800	So the thing is that you decided to define
6697800	6699600	ethics axiomatically.
6699600	6702920	And you, I think, probably have a hunch
6702920	6705920	that your axiomatic definition is not completely
6705920	6707120	consistent with itself.
6707120	6710120	It's just the best you came up with under the circumstances.
6710120	6713120	For instance, if you really go after eliminating suffering,
6713120	6716960	you should probably put some anesthetic into the water
6716960	6719920	supply globally to alleviate suffering,
6719920	6722440	and then let everybody face happily out of existence
6722440	6726920	in a way that would satisfy the goal in an optimal way.
6726920	6728400	And it's probably not what you want.
6728400	6731680	So you also want to preserve human aesthetics, maybe.
6731680	6733360	And to preserve these human aesthetics,
6733360	6735480	the shape of the mind that we have and this consciousness
6735480	6738040	that we have is going to create some suffering.
6738040	6739160	And this is the tangent.
6739160	6741360	And you have to make a decision at some point.
6741360	6745480	Imagine you take an AI that is actually sustainable,
6745480	6747960	and you ask this AI, if you give you a job,
6747960	6750760	you want to be around in 10 years from now.
6750760	6752080	You cannot build a government that
6752080	6754960	cares about us being around a 10,000 years from now
6754960	6758680	effectively, because this is not an incentive
6758680	6760480	that you can actually give the government,
6761200	6763000	in a sense, this incentive is going
6763000	6765560	to defect from the incentive that we wanted to have.
6765560	6766600	So let's build an AI.
6766600	6768600	And the AI is going to be around in 10,000 years
6768600	6769760	from now, no problem.
6769760	6772040	If you tell it, make sure that we are there too.
6772040	6774520	And the AI is probably going to kill 90% of us,
6774520	6776880	hopefully painlessly, and breed everybody else
6776880	6779240	into some kind of harmless yeast.
6779240	6781200	So they need to keep around.
6781200	6783200	This is not what you want, I guess, right?
6783200	6786480	Even though it would be consistent with your stated axioms.
6786480	6789600	So getting the axioms consistent is super hard.
6789720	6794440	And even with the cold, the best, most ethical,
6794440	6798400	according to my own argument, the best, most ethical universe
6798400	6800240	would be a cold, dead universe, because there
6800240	6802600	will be no possibility of suffering there, right?
6802600	6805400	That's clearly a problem.
6805400	6809160	Yes, and now the next thing with the suffering axiom
6809160	6811080	is that the suffering is important,
6811080	6813320	because you think of it as something that cannot be
6813320	6815560	turned off by itself.
6815560	6817320	So we basically think of suffering
6817320	6819560	as something that is not the choice of the one who suffers.
6819560	6821520	Because why would you want to suffer, right?
6821520	6823200	So it's something that the universe does to you,
6823200	6825360	and we have to change the conditions of the universe
6825360	6827920	in which you are in so you don't suffer.
6827920	6830360	But what we forget about this, that suffering
6830360	6831680	is an evolutionary adaptation.
6831680	6834520	It's created to make you jump through all these hopes
6834520	6837120	in order to eat more and eat others.
6837120	6839280	It's a very perverse thing.
6839280	6841120	And you can turn off the suffering.
6841120	6844240	As soon as you become conscious enough and awake enough,
6844240	6846880	you can deal with it and get rid of your suffering.
6846960	6850000	And so at some point in your mental development,
6850000	6852160	suffering becomes a choice.
6852160	6855360	And for the other animals, it's all about, yeah.
6855360	6858200	So you could think, OK, one thing that we want to do
6858200	6860760	is we want to wake up as many organisms as possible
6860760	6864040	to give them that choice, to give them
6864040	6865840	agency over their suffering.
6865840	6869120	And this will then open another Pandora's Box
6869120	6871200	of ethical conundrums.
6871200	6873840	But on a very short range, maybe we
6873840	6876160	don't need to make these decisions right now right here,
6876360	6878480	we can basically operate in a framework
6878480	6880520	where we agree with our loved ones
6880520	6883280	about shared purposes and shared systems of meanings
6883280	6885080	and want to operate within those.
6885080	6888240	And in these narrow constraints, we can get ethics to work.
6888240	6891640	I don't see how to get ethics to work globally.
6891640	6894120	Right, right.
6894120	6897360	So, Joshua, it's been a fascinating two hour
6897360	6898720	conversation with you.
6898720	6900880	I really enjoyed it.
6900920	6907480	I'm not surprised that I rediscovered that I don't know.
6907480	6911160	I've been mostly aware, though occasionally I forget,
6911160	6912880	that I really don't know.
6912880	6915280	Thank you for reminding me that.
6915280	6919080	Tell me, where can people find more about you and your work?
6919080	6920080	There's some on YouTube.
6920080	6925400	I'm also getting myself to write a book, hopefully, these days.
6925400	6927200	What's the book about?
6927200	6930440	I basically try to get a glimpse on this civilizational
6930440	6934560	intellect, on this hive mind that we have been created
6934560	6938360	and that makes sense of some of the concepts that
6938360	6942320	are broken in our culture, ideas that
6942320	6947000	are broken in our mind, consciousness, self, meaning.
6947000	6948560	And we don't know how to talk about them.
6948560	6952680	And AI has discovered how to talk about them.
6952680	6955680	AI has discovered, and we don't know.
6955880	6958520	I think that basically our poll is largely doesn't know.
6958520	6960160	There are many people which do know.
6960160	6962680	But I think we need to carry these ideas together
6962680	6965760	in one place so we can talk about them
6965760	6968920	without getting too excited about them or upset.
6968920	6971280	Because it's not about giving meaning to people's lives
6971280	6971840	or something.
6971840	6976000	It's not about building better self-driving cars.
6976000	6978240	At some level, it's about understanding who we are
6978240	6981280	and what our relationship to reality is.
6981280	6983200	And AI has figured out a few things
6983200	6985200	that we didn't know 100 years ago.
6985240	6987960	Yeah, but isn't that figuring out who you are?
6987960	6990520	Isn't that giving you meaning?
6990520	6992160	No.
6992160	6993880	It's much better.
6993880	6996160	I discover what the nature of meaning is.
6996160	6999800	I discover how this is wired into my brain.
6999800	7003320	And it's in a way becoming an adult.
7003320	7005800	The first stage and the maturity of a mind,
7005800	7007640	and maybe the last stage, is where
7007640	7010160	you discover what you are, how you are built,
7010160	7013040	what you actually, how you function, your own nature.
7016040	7020680	Well, Josh, I want to talk to you for another two hours.
7020680	7023880	So perhaps I hope that one day.
7023880	7027080	You can set up another date for that out.
7027080	7030920	I hope to do that in person, actually, hopefully soon.
7030920	7034280	But in the meantime, how do we wrap up
7034280	7036040	this two-hour conversation with you?
7036040	7039400	What's the most important thing or the single message
7039400	7042640	that you want to send away our audience with today?
7042640	7044560	Who is our audience?
7044560	7049120	Well, who do you want your audience to be?
7049120	7050920	You can send a message to anybody.
7050920	7053080	My audience is my audience, but they
7053080	7055680	have their very wide diversity of people.
7055680	7060560	Lots of IT, basically geeks, nerds, transhumanists,
7060560	7067680	cryonists, futurists, IT professionals, philosophers,
7067680	7069840	engineers, curators.
7069840	7070360	OK.
7071080	7072240	OK.
7072240	7074480	So something very simple and boring.
7074480	7078960	I think that the field of AI is largely misunderstood,
7078960	7082400	because there are two industries, the AI hype industry
7082400	7084400	and the anti-AI hype industry, which
7084400	7086400	have very little to do with AI.
7086400	7090200	The practice of AI is, in a way, statistics on steroids.
7090200	7092480	It's experimental statistics.
7092480	7096440	It's identifying new functions to model reality.
7096440	7098440	And that is what statistics is doing.
7098440	7100140	And largely, it hasn't gotten to the point
7100140	7102400	yet where it can make proofs of optimality.
7102400	7104160	It's largely experimental, but it
7104160	7107140	can do things that are much better than the established
7107140	7109840	tools of statisticians.
7109840	7112080	And this in itself is not so exciting.
7112080	7113760	There's also going to be a convergence
7113760	7118800	between econometrics, causal dependency analysis,
7118800	7121280	and AI and statistics.
7121280	7123560	It's all going to be the same in a particular way,
7123560	7125480	because there's only so many ways in which you
7125480	7128720	can make mathematics about reality.
7128720	7132920	And we confuse this with the idea of what a mind is.
7132920	7135080	And they're closely related, because I
7135080	7139400	think that our brain contains an AI that
7139400	7142680	is making a model of reality and the model of a person
7142680	7143800	in reality.
7143800	7146680	And this particular solution of what an AI can do,
7146680	7148520	this particular thing in the modeling space,
7148520	7150080	this is what we are.
7150080	7151760	So in a way, we need to understand
7151760	7153440	the nature of AI, which I think is
7153440	7156840	the nature of somewhat general function approximation,
7156840	7159120	sufficiently general function approximation.
7159120	7161040	Maybe all of the function approximation
7161040	7162800	that can be made in the long run,
7162800	7165360	all the truth that can be found by an obituary observer
7165360	7167080	in particular kinds of universes that
7167080	7168680	have the power to create it.
7168680	7171040	This could be the question of what
7171040	7174040	AI is about, how modeling works in general.
7174040	7177680	And for us, the relevance of AI is,
7177680	7179520	does it explain us who we are?
7179520	7183360	And I don't think that there is anything else that can.
7183360	7186400	So let me see if I get this right, just because,
7186400	7188720	to see if I can simplify.
7188720	7190480	And I'm probably going to fail.
7190480	7194360	But so we need to understand the nature of AI.
7194360	7196280	That's kind of your call.
7196280	7201280	But then you said that we are, in a way, an AI.
7201280	7202560	Is that the case?
7202560	7203720	No, the brain is an AI.
7203720	7205000	I am a self.
7205000	7209640	A self is a model that the mind has created inside of my brain.
7209640	7213200	Right, so that's a little AI instantiation.
7213760	7218160	And then if we create that other AI that we're talking about,
7218160	7225040	it would perhaps give us a glimpse of this other AI in here.
7225040	7229000	And we would understand the nature of our AI in here
7229000	7231240	by creating that other AI out there.
7231240	7232360	Actually, we already do.
7232360	7236120	So the things that Minsky and many others
7236120	7238560	have contributed to this field and the things
7238560	7240760	that we are talking about right now
7240800	7243560	are already a much better understanding
7243560	7248320	that our part of humanity, our civilization,
7248320	7250720	had a couple hundred years ago.
7250720	7253080	Many of these ideas we could only develop
7253080	7256200	because we began to understand the nature of modeling,
7256200	7258720	the nature of our relationship to the outside world,
7258720	7260480	the status of reality.
7260480	7262760	Like we started out from this dualist intuition
7262760	7266080	in our culture, that there is a res extensor
7266080	7268320	and a res cogitanz, a sinking substance
7268320	7271640	and extended substance, the stuff in space universe
7271640	7273640	and the universe of ideas.
7273640	7275440	And we now realize that they both exist,
7275440	7278160	but they both exist within the mind.
7278160	7281760	Part of what we have in the mind is stuff in a free space.
7281760	7284560	Everything perceptual gets mapped to a region in free space.
7284560	7286280	We also now understand physics.
7286280	7287840	Physics is not a free space.
7287840	7289400	It's something else entirely.
7289400	7291840	The free space is only apparent as the space
7291840	7294240	of potential electromagnetic interactions
7294240	7297120	at a certain order of magnitude of scaling
7297120	7300320	above the plank length, where we are entangled with the universe.
7300320	7302320	Our minds are entangled with the universe.
7302320	7303680	This is what we model.
7303680	7305400	And this looks three-dimensional to us.
7305400	7307680	And everything else that our mind comes up with
7307680	7310400	is stuff that cannot be mapped onto region to free space.
7310400	7312120	This is res cogitanz.
7312120	7316320	So in a way, we transfer this dualism into a single mind.
7316320	7318200	Then we have the idealistic monism
7318200	7320920	that we have in many spiritual teachings.
7320920	7323680	This idea that there is no physical reality,
7323680	7325080	that we live in a dream.
7325880	7327200	If you are a character in a dream
7327200	7329400	drawn by a mind on a higher plane of existence,
7329400	7331280	then that's why miracles are possible.
7332480	7334440	And then there is this western perspective
7334440	7337240	of a mechanical universe that is entirely mechanical.
7337240	7338920	There's no conspiracy going on.
7339800	7344120	And now we understand that these things are not in opposition.
7344120	7345760	They are complements.
7345760	7347240	We actually do live in a dream,
7347240	7349320	but the dream is generated by a neocortex.
7350360	7352360	So our brain is not a machine
7352360	7354600	that can give us access to reality as it is,
7354600	7356920	because that's not possible for a system
7356920	7360280	that is only measuring a few bits at a systemic interface.
7360280	7362600	There is no colors and sounds that fits through your nerves.
7362600	7363920	We already know that.
7363920	7365680	The sounds and colors are generated
7365680	7367480	as a dream inside of your brain.
7367480	7370240	The same circuits that make dreams at night
7370240	7371800	make dreams during the day.
7371800	7372920	Right.
7372920	7375480	So this, in a way, is our inner reality.
7375480	7377240	It's being created on the brain.
7377240	7380600	The mind on the higher plane of existence exists.
7380600	7384080	It's the brain of a primate that is made from cells
7384080	7386360	and lives in a mechanical, physical universe.
7386360	7388920	And magic is possible because you can edit your memories.
7389920	7393560	Right. You can make that simulation anything you want it to be.
7393560	7396520	It's just many of these changes are not sustainable.
7396520	7400200	That's why the sages warn against using magic,
7400200	7401680	because down the line,
7401680	7405160	if you change your reward function, bad things may happen.
7406400	7408160	You cannot break the bank.
7408160	7412600	So let me see if I can simplify all of this in a sentence.
7413480	7415320	And if you agree with it.
7415320	7418600	So we need to understand the nature of AI
7418600	7421160	in order to understand ourselves.
7421160	7422320	Is that it?
7422320	7426560	So I would say that AI is the field that took up the slack
7426560	7429160	after psychology failed as a science.
7429160	7431680	Psychology got terrified of overfitting.
7431680	7435120	So it stopped making theories of the mind as a whole.
7435120	7439600	It restricted itself to theories with very few free parameters.
7439600	7440920	So it could test them.
7440960	7444040	And even those strategy didn't replicate as we know now.
7444040	7447160	So after PhDs, psychology largely didn't go anywhere
7447160	7448080	in my perspective.
7448080	7450800	It might be too harsh because I see it from the outside
7450800	7452520	and outsiders of AI might also argue
7452520	7454600	that AI didn't go very far.
7454600	7456960	And as an insider, I'm more partial here.
7456960	7462040	And maybe I have too much bias and give it too much credit.
7462040	7464640	But to me, most of the things I've learned
7464640	7466360	by looking at the both of this lens
7466360	7468880	of seeing us as information processing systems.
7469880	7472880	So you agree with the statement summary that I made?
7472880	7473880	Yeah.
7473880	7474880	OK.
7474880	7479880	Because I have this metaphor that I use every once in a while
7479880	7482880	saying that technology is a magnifying mirror.
7482880	7484880	It doesn't have an essence of its own,
7484880	7488880	but it reflects the essence that we put in it.
7488880	7490880	And of course, it's not a perfect image
7490880	7492880	because it magnifies and it amplifies things.
7493880	7501880	So I think those could be mutually supportive, right?
7501880	7505880	Because you're saying we need to understand the nature of AI
7505880	7507880	to understand who we are.
7507880	7509880	And I like that very much, actually.
7509880	7510880	Yeah.
7510880	7513880	But just the practice of AI is 90 degrees,
7513880	7518880	it's automating statistics and making better statistics that
7518880	7520880	run automatically on machines.
7520880	7523880	And it just so happens that this thing is largely
7523880	7526880	co-extensional with what mines do.
7526880	7530880	And it also just so happens that AI was largely founded
7530880	7532880	as a discipline by people like Minsky
7532880	7534880	to understand the nature of our minds
7534880	7537880	because they had fundamental questions about our relationships
7537880	7538880	to reality.
7538880	7540880	Right.
7540880	7544880	And what's the last 10%?
7544880	7545880	Of what?
7545880	7546880	Other than statistics.
7546880	7548880	You said it's 90% statistics.
7548880	7549880	What's the rest?
7549880	7552880	Oh, the rest is people coming up with dreams
7552880	7554880	about our relationship to reality
7554880	7558880	using the concepts that we develop in AI.
7558880	7559880	Right.
7559880	7562880	So we identify models of things that we can apply in other fields.
7562880	7566880	It's the deeper insights that we actually go for.
7566880	7569880	Most of what we do in AI is about applications.
7569880	7571880	It's about utility down the line.
7571880	7574880	But there are these things where we really do it.
7574880	7577880	The thing that Feynman said that makes physics like sex
7577880	7579880	also makes AI like sex.
7579880	7581880	Sometimes something useful comes from it.
7581880	7585880	A new better way to make self-driving cars or play jeopardy
7585880	7588880	or help people in many circumstances in their life
7588880	7591880	or to make better agents running on your phone.
7591880	7593880	It's not why we do it.
7593880	7596880	We want to understand how we work.
7596880	7597880	Right.
7597880	7599880	And that's a brilliant place to end our conversation
7599880	7602880	because I feel the same way about philosophy, by the way,
7602880	7607880	that it's just like Feynman felt about physics
7607880	7609880	and you feel about AI.
7609880	7611880	I feel the same way about philosophy.
7611880	7613880	Yeah.
7613880	7616880	So these remaining 10% are innovative philosophy.
7616880	7618880	But in all of these fields,
7618880	7620880	most of the practitioners are trained
7620880	7622880	in the main methodology of their field.
7622880	7624880	So our philosophy tends to be bad.
7624880	7625880	Yeah.
7625880	7627880	And I think my job is to try to make it slightly better
7627880	7629880	to the degree that I can.
7629880	7632880	And does that mean by extension that, of course,
7632880	7635880	most physics then would be bad and most AI then would be bad
7635880	7638880	because they fall within that 90%?
7638880	7639880	No, no.
7639880	7643880	I think the AI as a practical thing can be very good.
7643880	7644880	Right.
7644880	7647880	Most physicists are not concerned with foundational physics
7647880	7649880	with the nature of the universe.
7649880	7651880	Most physicists are concerned with material science
7651880	7654880	or many, many other extremely practical things.
7654880	7657880	It's only very small minority that worries about the deepest things.
7657880	7660880	And the same thing happens in AI or neuroscience.
7660880	7662880	And it's not that there anybody is to blame
7662880	7665880	for doing development things.
7665880	7668880	It's actually very good that a lot of people are willing
7668880	7670880	to put up with development things
7670880	7672880	and take down the garbage.
7672880	7673880	Right.
7673880	7674880	And I'm grateful to them.
7674880	7677880	It's just I can't do that myself somehow.
7677880	7681880	I think as you put that, I would probably go extinct.
7681880	7682880	Yeah.
7682880	7684880	And it's not a source of pride in a way.
7684880	7686880	It's the recognition of a disability.
7687880	7688880	Exactly.
7688880	7689880	It's the bug.
7689880	7690880	Yeah.
7690880	7693880	But it is, it's marginally useful
7693880	7695880	because society needs a few of us.
7695880	7697880	So it does.
7697880	7699880	I mean, we're still here.
7699880	7702880	We're here, but it's a struggle sometimes.
7702880	7703880	Yeah.
7703880	7705880	But this is our own choice how much we struggle
7705880	7708880	because objectively we are here and the coffee is good.
7708880	7710880	Thank you for reminding me that.
7710880	7712880	And I love the coffee.
7712880	7713880	I'm a coffee fanatic.
7713880	7716880	I, yeah, that's a whole lot of story,
7716880	7718880	but I'm a coffee fanatic.
7718880	7719880	Joshua back.
7719880	7722880	Thank you so much for spending over two hours with us today.
7722880	7725880	I'm looking forward to our next conversation.
7725880	7729880	And I wish you the very best while the party is lasting.
7729880	7731880	Likewise, it was such a great conversation.
7731880	7733880	Thank you for this time we spent together.
7744880	7746880	If you guys enjoyed this show,
7746880	7749880	you can help me make it better in a couple of ways.
7749880	7751880	You can go and write a review on iTunes
7751880	7753880	or you can simply make a donation.
