1
00:00:00,000 --> 00:00:28,520
My name is Nikola and you're watching Singularity FM, the place where we interview the future.

2
00:00:28,520 --> 00:00:33,280
If you guys enjoyed this podcast, you can show your support by either writing a brief

3
00:00:33,280 --> 00:00:36,960
review on iTunes or by simply making a donation.

4
00:00:36,960 --> 00:00:41,120
Today, my guest on the show is Professor Joshua Bach.

5
00:00:41,120 --> 00:00:46,880
Joshua is a cognitive scientist at the Harvard Program for Evolutionary Dynamics as well

6
00:00:46,880 --> 00:00:49,680
as the MIT Media Lab.

7
00:00:49,680 --> 00:00:52,560
So welcome to Singularity FM, Joshua.

8
00:00:53,520 --> 00:00:58,920
Hi, first of all, I'm not a professor, though, and I have been working at the MIT Media

9
00:00:58,920 --> 00:01:03,640
Lab until three years ago, and since then, I'm at Harvard.

10
00:01:03,640 --> 00:01:07,320
I'm not affiliated at both at this time.

11
00:01:07,320 --> 00:01:10,040
Oh, so what exactly is your position then?

12
00:01:10,040 --> 00:01:11,240
I'm a research scientist.

13
00:01:11,240 --> 00:01:16,000
The research scientist is a person that works in the abyss between postdoc and tenure.

14
00:01:16,000 --> 00:01:17,960
Wow, but you are a PhD.

15
00:01:17,960 --> 00:01:19,240
I am.

16
00:01:19,240 --> 00:01:23,560
And what was your PhD in then?

17
00:01:23,560 --> 00:01:25,040
In cognitive science.

18
00:01:25,040 --> 00:01:31,960
I went into academia to understand how the mind works and so studied a number of subjects

19
00:01:31,960 --> 00:01:39,000
and did degrees in computer science and philosophy and felt that AI is my best bet of making

20
00:01:39,000 --> 00:01:41,600
headway in understanding who we are.

21
00:01:41,600 --> 00:01:42,600
Fantastic.

22
00:01:42,600 --> 00:01:50,000
So, you are, as you said, in the zero gravity sort of space between postdoc and professorship?

23
00:01:50,000 --> 00:01:51,000
Academia?

24
00:01:51,000 --> 00:01:53,560
Yeah, it's actually a quite happy place.

25
00:01:53,560 --> 00:01:55,440
It's one that allows me to do what I want.

26
00:01:55,440 --> 00:01:57,600
I don't have to do superfluous management.

27
00:01:57,600 --> 00:02:00,280
I don't have to sit in many committees or anything.

28
00:02:00,280 --> 00:02:04,320
I can teach when I want, but I don't have to, which is really the best arrangement you

29
00:02:04,320 --> 00:02:05,320
can possibly have.

30
00:02:05,320 --> 00:02:08,200
I can have students, but I don't have to.

31
00:02:08,200 --> 00:02:11,920
And so this is kind of amazing.

32
00:02:11,920 --> 00:02:17,360
That's fantastic, but I don't know how, if it's allowing you to survive with your family

33
00:02:17,360 --> 00:02:18,360
properly.

34
00:02:18,360 --> 00:02:23,280
But we find out, you know, there can always be earthquakes.

35
00:02:23,280 --> 00:02:31,360
Okay, Joshua, if I were to ask you to introduce yourself in a couple of words, who is Joshua

36
00:02:31,360 --> 00:02:33,640
Bach?

37
00:02:33,640 --> 00:02:40,400
It depends who's asking, but in the most general sense, I'm a cognitive scientist.

38
00:02:40,400 --> 00:02:46,200
I grew up in communist Eastern Germany, the last generation to do so, as the child of

39
00:02:46,200 --> 00:02:49,200
an artist in the forest.

40
00:02:49,200 --> 00:02:53,400
And I grew up in a world that was completely alien to me in many ways, because communist

41
00:02:53,400 --> 00:02:58,520
Eastern Germany didn't make a lot of sense, especially if you grow up in a forest in which

42
00:02:58,520 --> 00:03:02,960
everything has no rules and only the rules that locally make sense.

43
00:03:02,960 --> 00:03:07,520
So anyway, my default in understanding the world has been different from the default

44
00:03:07,520 --> 00:03:09,080
of people around me.

45
00:03:09,080 --> 00:03:14,640
I reluctantly discovered that most people formed their ideas by taking in the norms

46
00:03:14,640 --> 00:03:19,680
of their environment and the statements of the experts and taking them as gospel and

47
00:03:19,680 --> 00:03:24,160
only revise them when they absolutely have to and they're disproven.

48
00:03:24,160 --> 00:03:26,840
And for me, it was always like the opposite.

49
00:03:26,840 --> 00:03:31,800
You have this perspective on the world where people have ideas and their thoughts and they

50
00:03:31,800 --> 00:03:33,240
often make no sense.

51
00:03:33,240 --> 00:03:37,680
And you will have to look at each of them with great care before you incorporate them

52
00:03:37,680 --> 00:03:40,120
into your own world model.

53
00:03:40,120 --> 00:03:44,360
So you try to be careful to not harm anything or do bad things to the world.

54
00:03:44,360 --> 00:03:50,120
But this reluctance in accepting what comes from the outside has, I think, shaped my scientific

55
00:03:50,120 --> 00:03:51,120
perspective.

56
00:03:51,120 --> 00:03:55,960
And when I came into the next society, Western Germany, and then later on to New Zealand and

57
00:03:55,960 --> 00:04:00,280
to the US, I always saw things from the outside.

58
00:04:00,280 --> 00:04:03,880
So I'm more an observer and the same thing happens in the scientific fields.

59
00:04:04,160 --> 00:04:06,840
Wow, that's absolutely fascinating.

60
00:04:06,840 --> 00:04:09,920
And I want to grab a few points there one by one.

61
00:04:09,920 --> 00:04:18,960
But first of all, that kind of a skeptical outsider kind of point of view is very contrarian

62
00:04:18,960 --> 00:04:25,600
and also very sort of philosophical in the way, maybe in the German school, because at

63
00:04:25,600 --> 00:04:33,400
least Nietzsche said that gross answers are a prohibition against thinkers.

64
00:04:33,400 --> 00:04:34,920
You shall not think.

65
00:04:34,920 --> 00:04:43,760
And to him, that was like an insult because he was curious, questioning inquisitive kind

66
00:04:43,760 --> 00:04:46,360
of a soul from the beginning.

67
00:04:46,360 --> 00:04:54,040
And so he was never one for gross answers, but rather asking questions and questioning

68
00:04:54,040 --> 00:04:55,040
everything.

69
00:04:55,040 --> 00:04:57,840
So it seems you've kind of you've got that approach.

70
00:04:57,840 --> 00:05:01,600
I felt that Nietzsche never made peace with society.

71
00:05:01,840 --> 00:05:05,560
That was related to the fact that he was never able to make peace with himself.

72
00:05:05,560 --> 00:05:07,560
It really never worked out.

73
00:05:07,560 --> 00:05:09,400
There is a big issue with obedience.

74
00:05:09,400 --> 00:05:12,000
This question, should you obey somebody else?

75
00:05:12,000 --> 00:05:18,040
I mean, you seem to have that same issue that to work in a hierarchy, you need to submit

76
00:05:18,040 --> 00:05:19,200
in a way to a hierarchy.

77
00:05:19,200 --> 00:05:20,200
How would you submit?

78
00:05:20,200 --> 00:05:24,320
How could somebody else make your decisions if they didn't test it to the same rigorous

79
00:05:24,320 --> 00:05:26,760
epistemological criteria that you did?

80
00:05:26,760 --> 00:05:28,320
Does that have integrity, right?

81
00:05:28,320 --> 00:05:29,880
It's very hard to do.

82
00:05:29,880 --> 00:05:35,400
But from a different perspective, if you want to do the right thing, then doing the right

83
00:05:35,400 --> 00:05:40,680
thing might require that you ask the person that is more likely to make the right decisions

84
00:05:40,680 --> 00:05:43,800
because they're an expert for a local area of making the right things.

85
00:05:43,800 --> 00:05:49,080
Like a leader is a person in specializing that specializes in doing the right thing.

86
00:05:49,080 --> 00:05:54,160
So I think it has integrity to realize that in certain circumstances, other people will

87
00:05:54,160 --> 00:05:57,680
know better than you do and Nietzsche never got to this point.

88
00:05:57,680 --> 00:06:01,600
And of course, philosophy is slightly different because in philosophy, you have to fix your

89
00:06:01,600 --> 00:06:04,400
foundations and arguably invest in philosophy.

90
00:06:04,400 --> 00:06:05,840
Very few people did.

91
00:06:05,840 --> 00:06:06,840
Right?

92
00:06:06,840 --> 00:06:11,960
So our hypothesis still seems to be supernatural beings and dualism and so on.

93
00:06:11,960 --> 00:06:17,640
And that's one of the reasons why most people in the Western world find AI so ridiculous

94
00:06:17,640 --> 00:06:18,720
and unlikely.

95
00:06:18,720 --> 00:06:22,320
It's not because people don't see that we are biological computers and that the universe

96
00:06:22,320 --> 00:06:26,400
is probably mechanical and the theory that everything is mechanical gives extremely

97
00:06:26,400 --> 00:06:27,400
good predictions.

98
00:06:27,600 --> 00:06:32,360
It's because deep down they still have this not hypothesis that the universe is somehow

99
00:06:32,360 --> 00:06:35,400
supernatural and we are the most supernatural thing in it.

100
00:06:35,400 --> 00:06:39,160
And science is only reluctantly pushing back against this not hypothesis.

101
00:06:39,160 --> 00:06:45,040
And since it has not completely obliterated not hypothesis in this single area, the consciousness

102
00:06:45,040 --> 00:06:52,880
in the mind, we are reluctant in accepting this reasonable certainty that we are machines.

103
00:06:52,880 --> 00:06:56,600
This is the main reason why we hesitate so much, I think.

104
00:06:56,600 --> 00:06:57,920
So are we machines then?

105
00:06:57,920 --> 00:07:05,840
Are we as some people have said that organisms are algorithms?

106
00:07:07,840 --> 00:07:13,360
There are a number of definitions on this, but if you think of an algorithm as a set

107
00:07:13,360 --> 00:07:19,680
of rules that can be probabilistic or deterministic and that make it possible to switch between

108
00:07:19,680 --> 00:07:25,960
states and usually we do this in a more narrow sense where we say that the algorithm is being

109
00:07:25,960 --> 00:07:31,240
used to change representational states in order to compute a function, then I would

110
00:07:31,240 --> 00:07:36,680
say that organisms have algorithms in this narrower sense.

111
00:07:36,680 --> 00:07:39,920
But I would say that in the wider sense, they're definitely machines.

112
00:07:39,920 --> 00:07:44,320
Machine is a system that can change the state in non-random ways.

113
00:07:44,320 --> 00:07:50,000
And also, we visit earlier states, which means to stay in a particular state space.

114
00:07:50,000 --> 00:07:52,400
Otherwise, this would not be a system.

115
00:07:52,400 --> 00:07:58,400
A system is something that we can describe by drawing a fence around its state space

116
00:07:58,400 --> 00:08:01,360
and saying, as long as it's in there, this is the system.

117
00:08:01,360 --> 00:08:05,160
So we have an evolution of the system that is someone constrained.

118
00:08:05,160 --> 00:08:08,920
Now, we are jumping headfirst into terminology.

119
00:08:08,920 --> 00:08:11,080
Want to go there?

120
00:08:11,080 --> 00:08:15,800
We would go there, but let me just roll back the tape of time because I want to follow

121
00:08:15,800 --> 00:08:21,240
your narrative, your personal narrative from where it began, then connect it to where you

122
00:08:21,240 --> 00:08:27,960
are today, and then hopefully try and look it to the world and into the future with your

123
00:08:27,960 --> 00:08:31,640
eyes and with your experience and from your point of view.

124
00:08:31,640 --> 00:08:36,680
So tell me, you said you grew up in the forest.

125
00:08:36,680 --> 00:08:38,680
Whereabouts?

126
00:08:38,680 --> 00:08:43,440
In Turingia, near Weimar and Jena, it's an area of German romanticism, which had a pretty

127
00:08:43,440 --> 00:08:45,720
big influence on how I grew up.

128
00:08:46,680 --> 00:08:51,360
It's a very particular shape of the soul that has been characterized by the Enlightenment,

129
00:08:51,360 --> 00:08:58,240
which in a way pushed back against the religious mind fibers that had controlled the world

130
00:08:58,240 --> 00:09:04,760
until then and replaced it with machinery, this rationalist machinery that eventually

131
00:09:04,760 --> 00:09:07,440
made modernist societies possible.

132
00:09:07,440 --> 00:09:09,360
And this was a very big upheaval.

133
00:09:09,360 --> 00:09:13,880
You can still see the ego of this in our modernness, like Lord of the Rings and Star Wars.

134
00:09:13,880 --> 00:09:18,040
You have this pastoral world, which defends itself against the encroaching technological

135
00:09:18,040 --> 00:09:23,760
empire that is going to eat our souls, even though it's going to win.

136
00:09:23,760 --> 00:09:26,360
And so, but did you grow up on a farm or something?

137
00:09:26,360 --> 00:09:28,360
No, my parents were artists.

138
00:09:28,360 --> 00:09:30,400
They were originally architects.

139
00:09:30,400 --> 00:09:36,560
And my father didn't want to build boring things that would put people into boxes and

140
00:09:36,560 --> 00:09:37,560
deny the humanity.

141
00:09:37,560 --> 00:09:40,640
Instead, he built things that didn't have many right angles.

142
00:09:40,640 --> 00:09:46,480
And he made a zoo that had no right angles, for instance, as one of his projects and so

143
00:09:46,480 --> 00:09:47,480
on.

144
00:09:47,480 --> 00:09:51,960
And it was very difficult to get away with these things in Eastern Germany because this

145
00:09:51,960 --> 00:09:58,400
was a very utilitarian society and its architecture was to a large degree brutalist.

146
00:09:58,400 --> 00:10:03,800
So he rejected this and he decided to remove himself from society and make his own kingdom

147
00:10:03,800 --> 00:10:04,800
in the forest.

148
00:10:04,800 --> 00:10:10,480
So he bought an old water mill and changed it into a sculpture garden and lived exactly

149
00:10:10,480 --> 00:10:13,200
the life he wanted and got away with it.

150
00:10:13,200 --> 00:10:15,720
Wow, that's absolutely phenomenal.

151
00:10:15,720 --> 00:10:19,320
I, like you, grew up in the Eastern Bloc only.

152
00:10:19,320 --> 00:10:25,280
I grew up in Bloc area, in Communist Bloc area for the first, what was it, 13, 14 years

153
00:10:25,280 --> 00:10:30,800
of my life, so I can associate it with a lot of your experience.

154
00:10:30,800 --> 00:10:34,880
But it's very interesting how you grew up in Germany as you put it in the forest in

155
00:10:34,880 --> 00:10:39,400
a very artistic family, and yet you became a scientist.

156
00:10:39,400 --> 00:10:47,840
So is there any tension there or is it a continuation of sort of, or did it give you any kind of

157
00:10:47,840 --> 00:10:54,560
different unique point of view or approach to science or is that basically a false dichotomy?

158
00:10:54,560 --> 00:10:58,440
There is a big similarity.

159
00:10:58,440 --> 00:11:02,440
I find that most people serve practical needs.

160
00:11:02,440 --> 00:11:06,680
They have an understanding of the difference between meaning and relevance.

161
00:11:06,680 --> 00:11:11,720
And at some level, my mind is more interested in meaning than relevance.

162
00:11:11,720 --> 00:11:13,680
That is similar to the mind of an artist.

163
00:11:13,680 --> 00:11:17,000
The arts are not life, they're not serving life.

164
00:11:17,000 --> 00:11:21,600
The arts are the cuckoo child of life, because the meaning of life, they are the cuckoo child

165
00:11:21,600 --> 00:11:22,600
of life.

166
00:11:22,600 --> 00:11:24,000
The meaning of life is to eat.

167
00:11:24,000 --> 00:11:29,560
You know, life is evolution, and evolution is about eating.

168
00:11:29,560 --> 00:11:32,720
It's pretty gross if you think about it, right?

169
00:11:32,720 --> 00:11:35,160
Evolution is about getting eaten by monsters.

170
00:11:35,160 --> 00:11:38,640
Don't go into the desert and perish there, because it's going to be a waste.

171
00:11:38,640 --> 00:11:42,080
If you're lucky, the monsters that eat you are your own children.

172
00:11:42,080 --> 00:11:48,320
And eventually the search for evolution will, if evolution reaches its global optimum, it

173
00:11:48,320 --> 00:11:50,320
will be the perfect devourer.

174
00:11:50,320 --> 00:11:58,040
The thing that is able to digest anything and turn it into structure to sustain its

175
00:11:58,040 --> 00:12:04,280
perpetuate itself for as long as the local puddle of negentropy is available.

176
00:12:04,280 --> 00:12:06,120
And in a way, we are yeast.

177
00:12:06,120 --> 00:12:09,880
Everything we do, all the complexity that we create, all the structures we build is

178
00:12:09,880 --> 00:12:14,800
to erect some surfaces on which to out-compete other kinds of yeast.

179
00:12:14,800 --> 00:12:20,000
And if you realize this, you can try to get behind this, and I think the solution to this

180
00:12:20,000 --> 00:12:21,000
is fascism.

181
00:12:21,000 --> 00:12:22,000
Right?

182
00:12:22,000 --> 00:12:26,400
Fascism is a mode of organization of society in which the individual is a cell and a super

183
00:12:26,400 --> 00:12:27,400
organism.

184
00:12:27,400 --> 00:12:31,880
The value of the individual is exactly the contribution to the super organism.

185
00:12:31,880 --> 00:12:36,800
And when the contribution is negative, then the super organism kills it in order to be

186
00:12:36,800 --> 00:12:39,240
fitter in the competition against other super organisms.

187
00:12:39,240 --> 00:12:41,280
And it's totally brutal.

188
00:12:41,280 --> 00:12:46,720
And I don't like fascism because it is going to kill a lot of minds I like.

189
00:12:46,720 --> 00:12:48,320
And the arts is slightly different.

190
00:12:48,320 --> 00:12:51,560
It's a mutation that is arguably not completely adaptive.

191
00:12:51,560 --> 00:12:56,240
It's one where people fall in love with the lost function, where you think that your mental

192
00:12:56,240 --> 00:13:00,160
representation is the intrinsically important thing, where you try to capture a conscious

193
00:13:00,160 --> 00:13:03,080
state for its own sake, because you think that matters.

194
00:13:03,080 --> 00:13:06,640
The true artist, in my view, is somebody who captures conscious states, and that's the

195
00:13:06,640 --> 00:13:08,440
only reason why they eat.

196
00:13:08,440 --> 00:13:11,480
So you eat to make art.

197
00:13:11,480 --> 00:13:15,840
And another person makes art to eat.

198
00:13:15,840 --> 00:13:20,720
And these are, of course, the ends of a spectrum, and the twos is often somewhere in the middle.

199
00:13:20,720 --> 00:13:23,440
But in a way, there is this fundamental distinction.

200
00:13:23,440 --> 00:13:29,680
And there are, in some sense, the true scientists which try to figure out something about the

201
00:13:29,680 --> 00:13:30,680
universe.

202
00:13:30,680 --> 00:13:31,680
They try to reflect it.

203
00:13:31,680 --> 00:13:32,960
And it's an artistic process in a way.

204
00:13:32,960 --> 00:13:36,520
It's an attempt to be a reflection to this universe.

205
00:13:36,520 --> 00:13:40,760
You see, there's this amazing vast darkness, which is the universe.

206
00:13:40,760 --> 00:13:45,480
There's all this iteration of patterns, but mostly there's nothing interesting happening

207
00:13:45,480 --> 00:13:46,480
in these patterns.

208
00:13:46,480 --> 00:13:50,560
It's a giant fractal, and most of it is just boring.

209
00:13:50,560 --> 00:13:56,560
And in a brief moment in the evolution of the universe, there are planetary surfaces

210
00:13:56,840 --> 00:14:01,160
and like entropy gradients that allow for the creation of structure.

211
00:14:01,160 --> 00:14:06,640
And then there are some brief flashes of consciousness in all this vast darkness.

212
00:14:06,640 --> 00:14:10,240
And these brief flashes of consciousness can reflect the universe and maybe even figure

213
00:14:10,240 --> 00:14:11,760
out what it is.

214
00:14:11,760 --> 00:14:14,760
It's the only chance that we have, right?

215
00:14:14,760 --> 00:14:16,280
This is amazing.

216
00:14:16,280 --> 00:14:17,800
And why not do this?

217
00:14:17,800 --> 00:14:18,800
Life is short.

218
00:14:18,800 --> 00:14:21,000
This is the thing that we can do.

219
00:14:21,000 --> 00:14:25,640
And that's why you, going back to your previous point about your current position being sort

220
00:14:25,680 --> 00:14:31,400
of between post-doc and academia, that position actually fits you very well because you're

221
00:14:31,400 --> 00:14:38,400
not forced to do science in order to eat, but actually you can afford to eat as much

222
00:14:40,560 --> 00:14:42,000
as you can do your science.

223
00:14:42,000 --> 00:14:43,120
Is that the case?

224
00:14:43,120 --> 00:14:45,680
I have a similar problem as you, I think.

225
00:14:45,680 --> 00:14:49,800
It's very difficult for me to get myself to do something for which I'm not intrinsically

226
00:14:49,800 --> 00:14:51,640
motivated for.

227
00:14:51,640 --> 00:14:55,280
So you got that right completely.

228
00:14:55,280 --> 00:15:00,640
If I work in a job that is intellectually interesting, but doesn't appear meaningful to me, I will

229
00:15:00,640 --> 00:15:04,200
probably lose interest after four months.

230
00:15:04,200 --> 00:15:07,440
And I have to do something where I think this needs to be done.

231
00:15:07,440 --> 00:15:11,800
This is worth spending some of my short life on.

232
00:15:11,800 --> 00:15:12,800
Right.

233
00:15:12,800 --> 00:15:16,200
I didn't even last four months.

234
00:15:16,200 --> 00:15:20,640
After my undergraduate, before my master's degree, I worked as an investment administrator

235
00:15:20,680 --> 00:15:26,480
in a company, and I lasted six weeks where I was balancing portfolios and doing stock

236
00:15:26,480 --> 00:15:28,200
trades and things like that.

237
00:15:28,200 --> 00:15:32,120
I lasted about five and a half, six weeks, and then it's a debate whether I resigned

238
00:15:32,120 --> 00:15:34,080
or I got fired first.

239
00:15:34,080 --> 00:15:39,400
But either way, I was not surviving there, so or staying there anyway.

240
00:15:39,400 --> 00:15:40,400
Yeah.

241
00:15:40,400 --> 00:15:41,400
There's a tension.

242
00:15:41,400 --> 00:15:44,360
I want to be useful to society and I want to eat suffering and so on.

243
00:15:44,360 --> 00:15:46,800
I do care about people.

244
00:15:47,800 --> 00:15:52,440
It's just that I have the impression that the systems that we live in are often not

245
00:15:52,440 --> 00:15:53,440
sustainable.

246
00:15:53,440 --> 00:15:54,440
They're largely doomed.

247
00:15:54,440 --> 00:15:57,160
It's a very weird situation that we find ourselves in.

248
00:15:57,160 --> 00:16:01,480
If you take a step back, all the important tipping points for climate change have been

249
00:16:01,480 --> 00:16:05,280
the last century, as people said in the last century.

250
00:16:05,280 --> 00:16:12,000
But the fact that we knew about this, that global warming is basically known to our corporations,

251
00:16:12,000 --> 00:16:17,080
to our companies since the 60s and 70s, and to our governments, I think about the same

252
00:16:17,080 --> 00:16:22,680
time, that our inability to deal with this probably means that there was too little agency

253
00:16:22,680 --> 00:16:27,000
in the system to do anything about it, and we probably locked ourselves into this trajectory

254
00:16:27,000 --> 00:16:28,480
with the industrial revolution.

255
00:16:28,480 --> 00:16:33,200
At this point, it was no longer for us to stop the machines that we built.

256
00:16:33,200 --> 00:16:38,120
Well, we are kind of jumping forward, and I want to sort of slow the ball down a little

257
00:16:38,120 --> 00:16:40,400
bit, if I may.

258
00:16:40,400 --> 00:16:47,000
Don't worry, you're not going to get bored.

259
00:16:47,000 --> 00:16:48,000
You can keep that pace.

260
00:16:48,000 --> 00:16:51,280
You just can turn around and go as I'm else.

261
00:16:51,280 --> 00:16:52,280
Fantastic.

262
00:16:52,280 --> 00:16:53,280
The pace is great.

263
00:16:53,280 --> 00:16:57,360
It's just that I had so many considerations already in my previous points that you made

264
00:16:57,360 --> 00:17:01,320
that now I kind of lost the thread completely.

265
00:17:01,320 --> 00:17:03,800
Okay.

266
00:17:03,800 --> 00:17:05,800
Let me see.

267
00:17:06,000 --> 00:17:11,400
We got the artistic and the scientific part of Joshua.

268
00:17:11,400 --> 00:17:18,960
Where does philosophy come about here in this equation, and how?

269
00:17:18,960 --> 00:17:20,440
That's a very awkward question.

270
00:17:20,440 --> 00:17:27,520
The problem is, in my view, that philosophy as a field of inquiry is practically dead.

271
00:17:27,520 --> 00:17:31,800
Misha Gromov once told me, it's a mathematician, that in his perspective, Darwin was the last

272
00:17:31,800 --> 00:17:32,800
philosopher.

273
00:17:32,800 --> 00:17:37,180
He was the last one who was in a position where we could connect some dots in a completely

274
00:17:37,180 --> 00:17:38,680
fresh way.

275
00:17:38,680 --> 00:17:43,760
And after that, there were people like Russell, who were extremely good writers, but didn't

276
00:17:43,760 --> 00:17:47,920
do any real philosophy anymore because there was too little left.

277
00:17:47,920 --> 00:17:50,360
And I'm not quite sure if that is the case.

278
00:17:50,360 --> 00:17:53,760
There is some philosophy that needs to be done, and it's still being done, and it's largely

279
00:17:53,760 --> 00:17:57,200
in mathematics and fixing the foundations.

280
00:17:57,200 --> 00:17:59,640
And even there, it's mostly visible.

281
00:18:00,360 --> 00:18:05,440
So we have two big intellectual traditions, which is mathematics and physics.

282
00:18:05,440 --> 00:18:09,440
And there are some cracks in them that need to be dealt with, and this is where most of

283
00:18:09,440 --> 00:18:11,040
the philosophy is at.

284
00:18:11,040 --> 00:18:15,560
And all the other things are minor, like social organization and so on.

285
00:18:15,560 --> 00:18:20,960
It's very miraculous to the sociologists, but I think we can see the patterns.

286
00:18:20,960 --> 00:18:24,440
This is largely the effect of these fields.

287
00:18:24,440 --> 00:18:26,640
And philosophy as a field is a culture.

288
00:18:26,640 --> 00:18:31,200
Now, you get paid for emulating what a philosopher is supposed to look like, and it's very hard

289
00:18:31,200 --> 00:18:33,760
to get any philosophy done on the side.

290
00:18:33,760 --> 00:18:35,200
And the incentives are all wrong, right?

291
00:18:35,200 --> 00:18:40,760
It's a very fierce battle to become a philosopher, to get from post-doctor tenure in these fields.

292
00:18:40,760 --> 00:18:42,200
So you need to get cited.

293
00:18:42,200 --> 00:18:46,440
And the way you get cited as a philosopher is you identify a hot discussion.

294
00:18:46,440 --> 00:18:50,960
In that hot discussion, you identify a unique position, and you build your brand around

295
00:18:50,960 --> 00:18:51,960
that unique position.

296
00:18:51,960 --> 00:18:54,000
You cannot afford to give this up.

297
00:18:54,000 --> 00:18:59,160
So you have your Chinese room or your unique position in about free will, and you're going

298
00:18:59,160 --> 00:19:00,880
to defend this hill.

299
00:19:00,880 --> 00:19:04,400
Even if this hill is basically indefensible, philosophy is not going to progress in a way

300
00:19:04,400 --> 00:19:07,040
that forces your buildings off that hill.

301
00:19:07,040 --> 00:19:11,040
You can build a mansion on an indefensible hill, and you will still have meetings in

302
00:19:11,040 --> 00:19:13,520
there 200 years from now.

303
00:19:13,520 --> 00:19:16,160
And the bad thing is all the good hills are taken, right?

304
00:19:16,160 --> 00:19:19,320
So this is a very bad situation for philosophers.

305
00:19:19,320 --> 00:19:24,280
And I think this is the reason why I cannot be a philosopher today.

306
00:19:24,280 --> 00:19:28,840
And we need philosophy, but we don't have it anymore in this sense.

307
00:19:28,840 --> 00:19:29,960
But let's define it.

308
00:19:29,960 --> 00:19:31,240
What is philosophy for you?

309
00:19:31,240 --> 00:19:35,060
Because I've interviewed a number of mathematicians and physicians, and they both argue which

310
00:19:35,060 --> 00:19:39,600
one is at the root of everything, whether it's mathematics, whether it's physics, and

311
00:19:39,600 --> 00:19:41,240
so on and so on.

312
00:19:41,240 --> 00:19:46,480
But both of them or all of them mostly agree that philosophy is irrelevant, or so they

313
00:19:46,480 --> 00:19:47,840
make that claim.

314
00:19:47,840 --> 00:19:53,680
And yet you say that philosophy kind of includes both mathematics and physics in a way, which

315
00:19:53,680 --> 00:19:58,720
I actually agree with, but tell me why, and tell me how do you define it in the first

316
00:19:58,720 --> 00:20:02,360
place in a way that actually includes both of those?

317
00:20:02,360 --> 00:20:10,240
I think that philosophy is in a way the search for the global optimum of the modeling function.

318
00:20:10,240 --> 00:20:16,240
So it has fields that have been defined as parts of questions that lead to this modeling

319
00:20:16,240 --> 00:20:21,720
function like epistemology, what can be known, what is the nature of truth and so on, ontology,

320
00:20:21,720 --> 00:20:26,960
what is the stuff that exists, what's going on there, metaphysics.

321
00:20:26,960 --> 00:20:32,440
This is in some sense the systems in which you have to describe things.

322
00:20:32,440 --> 00:20:36,120
And ethics, what should we do?

323
00:20:36,120 --> 00:20:39,920
And at some point we discovered epistemology.

324
00:20:39,920 --> 00:20:45,080
So my view, the first rule of epistemology is roughly discovered by Francis Bacon in

325
00:20:45,080 --> 00:20:46,080
1620.

326
00:20:46,080 --> 00:20:52,520
It says that the strengths of your confidence in a belief must equal the weight of the evidence

327
00:20:52,520 --> 00:20:53,920
and support of it.

328
00:20:53,920 --> 00:20:59,200
And you need to apply this recursively until basically you resolve the priors of every belief

329
00:20:59,200 --> 00:21:02,000
and the belief system becomes self-contained.

330
00:21:02,000 --> 00:21:03,400
To believe stops being a verb.

331
00:21:03,400 --> 00:21:08,000
There's no more relationship to identifications that you just arbitrarily set.

332
00:21:08,000 --> 00:21:13,240
This is just a system that is in itself contained, which means in some sense it's a mathematical

333
00:21:13,240 --> 00:21:14,240
system.

334
00:21:14,240 --> 00:21:19,920
It's a system that describes a certain thing and this leads you to the nature of mathematics.

335
00:21:19,920 --> 00:21:25,840
And mathematics, it turns out, is the domain of all languages, all of them, not just the

336
00:21:25,840 --> 00:21:27,840
natural languages.

337
00:21:27,840 --> 00:21:31,600
And mathematicians have been trying to fix their understanding of the languages and they

338
00:21:31,600 --> 00:21:34,240
noticed what mathematics is in this regard.

339
00:21:34,240 --> 00:21:41,400
And Hilbert stumbled on counters, set theoretic experiments to deal with natural numbers and

340
00:21:41,400 --> 00:21:48,280
then saw that when you go to infinity, very awkward and nasty things happen, your axiomatic

341
00:21:48,280 --> 00:21:50,560
systems basically start blowing up.

342
00:21:50,560 --> 00:21:55,640
And the total set suddenly contains both itself and the set of all of its subsets, so it cannot

343
00:21:55,640 --> 00:21:58,280
have the same number of members as itself.

344
00:21:58,280 --> 00:22:03,120
And he asked mathematicians, please build us an interpreter for mathematics, a mathematics

345
00:22:03,120 --> 00:22:08,080
basically something like a computer made for mathematics, any mathematics you want that

346
00:22:08,080 --> 00:22:10,400
can run all of mathematics.

347
00:22:10,400 --> 00:22:14,920
And then Goedl and Turing came along and showed that this is not possible, that this computer

348
00:22:14,920 --> 00:22:15,920
is going to crash.

349
00:22:15,920 --> 00:22:19,680
And this left mathematics was a big shock and the way mathematics is still reeling from

350
00:22:19,680 --> 00:22:21,360
that shock.

351
00:22:21,360 --> 00:22:27,080
And then Turing in church had another insight and they figured out that all the universal

352
00:22:27,080 --> 00:22:29,440
computers have the same power.

353
00:22:29,440 --> 00:22:35,320
The universal computer is a set of rules that by applying them you can compute all the things

354
00:22:35,320 --> 00:22:36,760
that can be computed.

355
00:22:36,760 --> 00:22:37,960
And the set contains itself.

356
00:22:37,960 --> 00:22:39,960
So universal computer is computable.

357
00:22:39,960 --> 00:22:44,920
As long as your universal computer doesn't run out of resources, it can compute anything

358
00:22:44,920 --> 00:22:49,600
that you can compute and it can also compute all the other universal computers.

359
00:22:49,600 --> 00:22:55,400
So the next thing that they discovered Turing was involved again was that our mind is probably

360
00:22:55,400 --> 00:23:00,000
in the class of the universal computers, not in the class of mathematical systems.

361
00:23:00,000 --> 00:23:01,680
So this is what Penrose doesn't know.

362
00:23:01,680 --> 00:23:05,880
Penrose thinks that our mind is mathematical, that it can do things that a computer cannot

363
00:23:05,880 --> 00:23:07,320
do.

364
00:23:07,320 --> 00:23:13,640
And the big hypothesis of AI in a way is we are in the class of systems that can approximate

365
00:23:13,640 --> 00:23:17,200
computable functions and only those.

366
00:23:17,200 --> 00:23:21,120
And so we cannot do more than a computer, which means that all the mathematics that

367
00:23:21,120 --> 00:23:25,560
we've ever seen and all the mathematics that we will ever see and that will ever matter

368
00:23:25,560 --> 00:23:27,240
is going to be computable.

369
00:23:27,240 --> 00:23:30,960
And the fact that some mathematics is not computable is the problem of the language that

370
00:23:30,960 --> 00:23:31,960
we have been using.

371
00:23:31,960 --> 00:23:35,400
We need computational languages, not mathematical languages.

372
00:23:35,400 --> 00:23:40,720
And it turns out that the main problem is that mathematics, classical mathematics, defines

373
00:23:40,720 --> 00:23:47,160
functions in using infinities, which means infinitely many steps to get to the result.

374
00:23:47,160 --> 00:23:50,360
These functions tend not to be computable.

375
00:23:50,360 --> 00:23:56,400
So if you are a computer programmer, it would never occur to you to write in your spec that

376
00:23:56,400 --> 00:24:01,040
is totally fine if your routine does return the result after infinitely many steps only.

377
00:24:01,040 --> 00:24:02,040
Right?

378
00:24:02,040 --> 00:24:03,040
This is not good.

379
00:24:03,200 --> 00:24:09,760
A finite set of steps and one that you know how long it is, so your customer gets results

380
00:24:09,760 --> 00:24:10,760
in time.

381
00:24:10,760 --> 00:24:11,760
Right?

382
00:24:11,760 --> 00:24:16,080
So in this perspective, should you define numbers in such a way that pi is a number?

383
00:24:16,080 --> 00:24:17,760
You cannot know the last digit of pi.

384
00:24:17,760 --> 00:24:19,200
Pi is a function, clearly, right?

385
00:24:19,200 --> 00:24:22,360
It's a function that gives you as many digits as you can afford.

386
00:24:22,360 --> 00:24:29,360
And in any finite universe, it's only going to give you a finite number of bits.

387
00:24:29,360 --> 00:24:35,000
And what about Stephen Wolframs' claim that our mathematics is only one of a sort of a

388
00:24:35,000 --> 00:24:40,080
very wide spectrum of possible mathematics?

389
00:24:40,080 --> 00:24:42,640
It depends on what you call our mathematics.

390
00:24:42,640 --> 00:24:45,440
I think that all mathematics are mathematics.

391
00:24:45,440 --> 00:24:47,680
So meta-mathematics is mathematics.

392
00:24:47,680 --> 00:24:50,720
It's not different from mathematics.

393
00:24:50,720 --> 00:24:55,880
I think that, for instance, computational mathematics, the thing that I am practically

394
00:24:55,880 --> 00:25:00,480
working in when I write my code and when I think about how to realize code is a branch

395
00:25:00,480 --> 00:25:01,480
of mathematics.

396
00:25:01,480 --> 00:25:02,480
It's called constructive mathematics.

397
00:25:02,480 --> 00:25:07,120
It's been discovered in mathematics a long time ago and largely been ignored by the other

398
00:25:07,120 --> 00:25:11,160
mathematicians because they thought it's not powerful enough to do all the things with

399
00:25:11,160 --> 00:25:15,320
real numbers that they like to be doing.

400
00:25:15,320 --> 00:25:19,280
But all the geometry is not possible in computational mathematics.

401
00:25:19,280 --> 00:25:21,600
We can only approximate it.

402
00:25:21,600 --> 00:25:25,400
geometry requires continuous operations, infinities.

403
00:25:25,400 --> 00:25:31,760
And also physics is built largely on these continuous mathematics.

404
00:25:31,760 --> 00:25:37,000
And in a computational universe, you only get these continuous operators by taking a

405
00:25:37,000 --> 00:25:44,000
very large set of finite automata, making a series from them and then it's squint.

406
00:25:44,000 --> 00:25:45,320
You know what, Joshua?

407
00:25:45,320 --> 00:25:47,440
Let me share with you something.

408
00:25:47,440 --> 00:25:52,920
I feel like I am a goldfish and you're a human when we're talking because I think that's

409
00:25:52,920 --> 00:25:59,320
kind of like the level, the difference of intelligence between you and me, my friend,

410
00:25:59,320 --> 00:26:10,520
which I come on because honestly, after interviewing 230 of supposedly the smartest

411
00:26:10,520 --> 00:26:13,440
people in the world, I've never had this feeling before.

412
00:26:13,440 --> 00:26:19,160
But today at this moment, this is how I feel just trying to keep up with you.

413
00:26:19,160 --> 00:26:20,160
No, I'm sorry.

414
00:26:20,160 --> 00:26:21,160
This is my fault.

415
00:26:21,160 --> 00:26:22,160
No, no, no.

416
00:26:22,160 --> 00:26:23,160
It's not your fault.

417
00:26:23,160 --> 00:26:28,640
It's you are who you are and it's my job to try to follow through and also direct a little

418
00:26:28,640 --> 00:26:35,840
bit of conversation in the best possible direction that I see can benefit both me as an interviewer,

419
00:26:35,840 --> 00:26:39,240
but even more so my audience and you.

420
00:26:39,240 --> 00:26:43,880
So let me just give us a little bit of a side direction here for a second and bring us back

421
00:26:43,880 --> 00:26:50,880
to the last issue before we jump into the meat of the matter here on AI, and talk about

422
00:26:50,880 --> 00:27:01,880
philosophy in academia and practicality because you mentioned about how you're motivated by

423
00:27:01,880 --> 00:27:10,720
your own kind of desire and inherent or intrinsic motivation to learn something or to discover

424
00:27:10,720 --> 00:27:18,080
new things, but perhaps academia is motivated nowadays more by the practical side of knowledge,

425
00:27:18,080 --> 00:27:24,240
by the side where you can create something that you can patent, that you can sell, and

426
00:27:24,240 --> 00:27:27,360
that you can scale up and commercialize.

427
00:27:28,360 --> 00:27:32,720
Where is the benefit and I think in a way that the usefulness of philosophy was its

428
00:27:32,720 --> 00:27:39,600
uselessness in some ways, if you will, just like art in a way is something that cannot

429
00:27:39,600 --> 00:27:43,320
be used for anything else.

430
00:27:43,320 --> 00:27:49,400
And some people have defined art as Oscar Wilde, for example, as something that's not

431
00:27:49,400 --> 00:27:51,400
immediately useful.

432
00:27:51,400 --> 00:27:53,160
That's what art is.

433
00:27:53,160 --> 00:27:59,360
So is there and there's actually a very famous paper written in the 19th century by the guy

434
00:27:59,360 --> 00:28:05,560
who funded the Princeton Institute for Advanced Study called the usefulness of useless knowledge.

435
00:28:05,560 --> 00:28:08,360
I don't know if you're familiar with it, but what's your take on that?

436
00:28:08,360 --> 00:28:13,600
Is there because many people would say, if you can't use any knowledge immediately, it's

437
00:28:13,600 --> 00:28:14,600
useless.

438
00:28:14,600 --> 00:28:20,840
Don't waste time acquiring it, don't waste time classifying it, storing it, just focus

439
00:28:20,840 --> 00:28:25,400
on something that's useful and practical.

440
00:28:25,400 --> 00:28:30,920
And to me as a philosopher, I'm always or often attracted to stuff that looks utterly

441
00:28:30,920 --> 00:28:31,920
useless.

442
00:28:31,920 --> 00:28:34,920
And maybe that's just me being not a scientist.

443
00:28:34,920 --> 00:28:42,920
But what's your take on that sort of tension, usefulness and uselessness in terms of knowledge?

444
00:28:42,920 --> 00:28:46,680
Feynman once said that physics is like sex.

445
00:28:46,680 --> 00:28:52,120
Sometimes something useful comes from it, but it's not why we do it.

446
00:28:52,120 --> 00:28:54,240
But it's brilliant.

447
00:28:54,240 --> 00:28:56,440
So there is a big insight there.

448
00:28:56,440 --> 00:28:59,640
This is, it's not that art is useless.

449
00:28:59,640 --> 00:29:05,400
It's just the utility of art is completely orthogonal to why you do it.

450
00:29:05,400 --> 00:29:08,560
So the meaning of the art is really not to help the living.

451
00:29:08,560 --> 00:29:10,520
If you'd like to help the living, right?

452
00:29:10,520 --> 00:29:13,560
But it's, so it's a very nice side effect.

453
00:29:13,560 --> 00:29:17,480
But what we want to do with the art is to capture what it's like.

454
00:29:17,480 --> 00:29:20,200
We want to capture a conscious state.

455
00:29:20,200 --> 00:29:21,680
That's the actual meaning of it.

456
00:29:21,680 --> 00:29:26,200
And in some sense, philosophy is at the root of all this.

457
00:29:26,200 --> 00:29:30,640
I think it's reflected in a way in one of the founding myths of our civilization, the

458
00:29:30,640 --> 00:29:31,640
Tower of Babel.

459
00:29:31,640 --> 00:29:34,960
This is the attempt to build this cathedral.

460
00:29:34,960 --> 00:29:40,640
And it's not a material building because it's meant to reach the heavens, which is not real.

461
00:29:40,640 --> 00:29:43,520
They're not in this world.

462
00:29:43,520 --> 00:29:45,760
It's a metaphysical building that is being built.

463
00:29:45,760 --> 00:29:49,360
It's this giant machine that is meant to understand reality.

464
00:29:49,360 --> 00:29:53,760
And you get to this machine, this true scot, this thing that tries to understand what's

465
00:29:53,760 --> 00:29:57,680
going on by using people that work like ants and contribute to this.

466
00:29:57,680 --> 00:29:58,960
And it's not about your ego.

467
00:29:58,960 --> 00:30:03,760
It's not about the gratification that you get from people for contributing to it.

468
00:30:03,760 --> 00:30:06,880
It's not for this thing that doesn't care about you.

469
00:30:06,880 --> 00:30:08,040
It doesn't give meaning to your life.

470
00:30:08,040 --> 00:30:11,440
It doesn't reward you for your insecurities and the toil of your existence.

471
00:30:11,840 --> 00:30:14,240
But it's really just a machine.

472
00:30:14,240 --> 00:30:16,000
It's a computer.

473
00:30:16,000 --> 00:30:18,280
And as we would say now, it's an AI.

474
00:30:18,280 --> 00:30:21,840
It's a system that is able to make sense of the world.

475
00:30:21,840 --> 00:30:24,320
And people at some point had to give up on this.

476
00:30:24,320 --> 00:30:28,040
It fell apart because they were no longer able to speak the same language.

477
00:30:28,040 --> 00:30:31,480
So the different parts stopped fitting together.

478
00:30:31,480 --> 00:30:35,240
Just became so large and so many people had to work in specialized direction that they

479
00:30:35,240 --> 00:30:37,680
could no longer synchronize their languages.

480
00:30:37,680 --> 00:30:39,000
And that's why they gave up on it.

481
00:30:39,000 --> 00:30:43,920
And then this big accident happened in the Roman Empire, where they could not fix the

482
00:30:43,920 --> 00:30:47,400
incentives for governance in similar ways as we fail here.

483
00:30:47,400 --> 00:30:50,960
Our government has to play a much shorter game than civilization does.

484
00:30:50,960 --> 00:30:53,680
And this leads to bad results for civilization.

485
00:30:53,680 --> 00:30:58,120
And the Romans decided to fix this by turning the society into a cult and

486
00:30:58,120 --> 00:31:03,920
burned down our epistemology and killed people that were overtly rational and

487
00:31:03,920 --> 00:31:08,320
insisted that people talking to burning bushes on lonely mountains don't have a

488
00:31:08,320 --> 00:31:11,360
case in determining the origin of the universe.

489
00:31:11,360 --> 00:31:15,800
So this one had to give and the cultist won.

490
00:31:15,800 --> 00:31:18,200
And we still have to recover from that.

491
00:31:18,200 --> 00:31:25,040
So in a way, the beginnings of the cathedral of understanding the universe that had been

492
00:31:25,040 --> 00:31:29,640
built by the Greeks and by the Romans had been burned down by the Catholics.

493
00:31:29,640 --> 00:31:32,840
And then later rebuilt, but mostly in the likeness because they didn't get the

494
00:31:32,840 --> 00:31:34,280
foundations right.

495
00:31:34,280 --> 00:31:38,000
The left scars and our epistemology that have not healed, even though we have a

496
00:31:38,000 --> 00:31:42,120
pretty successful culture that incorporated most of the other libraries and

497
00:31:42,120 --> 00:31:43,920
burned down the rest, right?

498
00:31:43,920 --> 00:31:47,400
We are the ones that are left over on this planet in a way.

499
00:31:47,400 --> 00:31:51,040
In our libraries, we can read everything that there is to read at the moment.

500
00:31:51,040 --> 00:31:53,800
We just often cannot translate it.

501
00:31:53,800 --> 00:31:58,080
And do you think that our civilization is currently perhaps suffering from that

502
00:31:58,080 --> 00:32:05,280
same Babylonian problem of difference in language and perhaps even has impact on

503
00:32:05,280 --> 00:32:10,640
resolving global problems like global warming that you mentioned, for example, right?

504
00:32:10,640 --> 00:32:15,560
Because all those people, business people, politicians, scientists, et cetera, speak

505
00:32:15,560 --> 00:32:22,360
in different languages and therefore they cannot kind of coordinate or synchronize anymore.

506
00:32:22,360 --> 00:32:28,360
And therefore that kind of perhaps puts at risk the whole project of our civilization

507
00:32:28,360 --> 00:32:32,880
just like the Babylonian Tower collapsed.

508
00:32:32,880 --> 00:32:41,080
Now this narrow specialization and diversity of languages and the difficulty in communicating

509
00:32:41,080 --> 00:32:48,920
between all of those branches then puts at risk the whole project of our civilization.

510
00:32:48,920 --> 00:32:52,400
I think that people individually are not generally intelligent.

511
00:32:52,400 --> 00:32:55,160
How often do you see a person that knows what they're doing?

512
00:32:55,160 --> 00:32:57,240
I'm certainly don't know what I'm doing.

513
00:32:57,240 --> 00:33:00,600
I have no clue what I'm doing to be honest.

514
00:33:01,520 --> 00:33:06,920
We are relatively intelligent, but of course this intelligence is largely a prosthesis

515
00:33:06,920 --> 00:33:10,040
to cover for non-working instincts.

516
00:33:10,040 --> 00:33:12,360
And we figure that out by now, right?

517
00:33:12,360 --> 00:33:16,520
And we see that people acting on the instincts largely get good results for their life, but

518
00:33:16,520 --> 00:33:20,560
they don't reach a very deep understanding about the nature of existence in the process

519
00:33:20,560 --> 00:33:21,560
because they don't have to, right?

520
00:33:21,560 --> 00:33:26,720
There is very little utility for deep philosophy and practical matters.

521
00:33:26,720 --> 00:33:31,640
And as a result, individuals are relatively stupid.

522
00:33:31,640 --> 00:33:36,320
Generations are not smarter than individuals but dumber because generations are made from

523
00:33:36,320 --> 00:33:38,120
groups that synchronize their beliefs.

524
00:33:38,120 --> 00:33:42,600
And the synchronization of beliefs makes it necessary that you give up agency over what

525
00:33:42,600 --> 00:33:44,240
you think is true.

526
00:33:44,240 --> 00:33:48,240
And when you do this, you accept things that you would not accept when you think about

527
00:33:48,240 --> 00:33:49,640
them individually.

528
00:33:49,640 --> 00:33:54,160
So people in Eastern Germany collectively believe things that an individual would never

529
00:33:54,160 --> 00:33:55,920
have thought.

530
00:33:55,920 --> 00:33:58,240
And same things happen here, right?

531
00:33:58,240 --> 00:34:04,240
So there are many conspiracy theories that people believe in here for a while that would

532
00:34:04,240 --> 00:34:06,400
not make sense to somebody who thinks about this.

533
00:34:06,400 --> 00:34:12,640
Like Putin uses an army of Twitter trolls to manipulate the fan-affectations of Star Wars

534
00:34:12,640 --> 00:34:13,640
movies.

535
00:34:13,640 --> 00:34:19,360
This is a conspiracy theory that was a result of misreading a study and was then repeated

536
00:34:19,360 --> 00:34:23,880
by 20 news outlets until somebody bothered to read the actual study and figure out, no,

537
00:34:23,880 --> 00:34:25,800
this is not what the study says.

538
00:34:25,800 --> 00:34:30,400
And then some of the outlets picked up on this but none of them wrote, OK, now we reconsider

539
00:34:30,400 --> 00:34:35,120
what we think about Putin and Star Wars because it's a way totally what Putin would have done

540
00:34:35,120 --> 00:34:36,560
if he would have had the idea.

541
00:34:36,560 --> 00:34:42,880
And this may or may not be true but it means that we don't project reality as the extrapolation

542
00:34:42,880 --> 00:34:43,880
of facts.

543
00:34:43,880 --> 00:34:49,160
It's rather that we know there are enough facts to support what we feel to be true.

544
00:34:49,160 --> 00:34:53,520
And there's utility in feeling particular kinds of truths and these basically local

545
00:34:53,520 --> 00:34:58,320
cults of interpreting reality shape society, shape generations is what a generation is

546
00:34:58,320 --> 00:34:59,320
about.

547
00:34:59,320 --> 00:35:01,560
It's a local perspective of what things should be like.

548
00:35:01,560 --> 00:35:06,600
Like you have your liberal generations, the millennials are largely authoritarian generations

549
00:35:06,600 --> 00:35:10,960
and if you look at them and it feels wrong to us and they look at us and it feels wrong

550
00:35:10,960 --> 00:35:12,960
to them.

551
00:35:12,960 --> 00:35:14,960
And neither of them is true.

552
00:35:14,960 --> 00:35:19,560
It's probably a set of biases that are the result of a local indoctrination.

553
00:35:19,560 --> 00:35:21,680
But there is something that's smarter than the generation.

554
00:35:21,680 --> 00:35:23,200
This is the culture itself.

555
00:35:23,200 --> 00:35:28,320
So if you zoom out a little bit, you see that generations and societies are generated by

556
00:35:28,320 --> 00:35:30,960
cultures and cultures are built over a long time.

557
00:35:30,960 --> 00:35:35,320
And there are many things that are embodied in a culture, for instance, in the culture

558
00:35:35,320 --> 00:35:39,080
of how to build science that would be very hard to derive for a single generation or

559
00:35:39,080 --> 00:35:43,520
to improve for a single generation because we don't locally understand all the things

560
00:35:43,520 --> 00:35:45,840
that went into it.

561
00:35:45,840 --> 00:35:49,440
So anyway, civilizations are smarter than us.

562
00:35:49,440 --> 00:35:54,640
There is something like a civilizational mind, a civilizational intellect that we as members

563
00:35:54,640 --> 00:35:59,200
of our polis who are somewhat educated can never fully comprehend.

564
00:35:59,200 --> 00:36:04,800
But once we figure out that it's there, there is something like a civilizational intellect.

565
00:36:04,800 --> 00:36:09,560
We can try to look into the abyss and see its rough shape, but it's difficult to figure

566
00:36:09,560 --> 00:36:10,560
it out.

567
00:36:10,560 --> 00:36:14,120
And then we realize, oh, there's a long tradition, there's multiple traditions that build on

568
00:36:14,120 --> 00:36:16,040
it and contribute to it.

569
00:36:16,040 --> 00:36:23,400
And that thing, in a way, is what we are going to achieve when we build AI in the sense that

570
00:36:23,400 --> 00:36:29,000
we can incorporate the sum of all knowledge in a system of relations that makes sense

571
00:36:29,000 --> 00:36:30,640
of it all.

572
00:36:30,640 --> 00:36:34,640
But what if civilizations self-destroy themselves then?

573
00:36:34,640 --> 00:36:39,960
What is that sort of knowledge or intelligence then say about the fitness function of that

574
00:36:39,960 --> 00:36:42,760
particular civilization and in general even?

575
00:36:43,640 --> 00:36:47,600
Before we had an industrial civilization, we never got about 400 million individuals

576
00:36:47,600 --> 00:36:51,000
on the planet because we could not feed more.

577
00:36:51,000 --> 00:36:56,560
And only this switch to our industrial civilization made it possible to have billions of people,

578
00:36:56,560 --> 00:37:01,800
which also means many hundreds of millions of scientists and philosophers and thinkers

579
00:37:01,800 --> 00:37:04,400
and the internet and so on.

580
00:37:04,400 --> 00:37:05,400
It's amazing what we did.

581
00:37:05,400 --> 00:37:11,280
We took basically 100 years worth of trees that were turning into coal in the ground

582
00:37:11,280 --> 00:37:16,400
because nature had not evolved microorganisms yet that could eat the trees in time.

583
00:37:16,400 --> 00:37:23,160
And we burned through this deposit of energy in 100 years to give plumbing to everybody.

584
00:37:23,160 --> 00:37:29,760
And part of that plumbing includes access to a global porn repository that is an afterthought

585
00:37:29,760 --> 00:37:33,520
as to some of all you knowledge and largely uncensored chat rooms in which you can talk

586
00:37:33,520 --> 00:37:36,160
about it.

587
00:37:36,160 --> 00:37:37,600
This is the internet.

588
00:37:37,600 --> 00:37:39,600
And this is an amazing machine.

589
00:37:39,600 --> 00:37:43,760
And we have it right now and only in this moment and time we have it before it didn't

590
00:37:43,760 --> 00:37:44,760
exist.

591
00:37:44,760 --> 00:37:47,480
So you could take a particular perspective.

592
00:37:47,480 --> 00:37:51,840
Let's say there is a universe that is saying where everything is good.

593
00:37:51,840 --> 00:37:56,960
You have this nice planet with pretty decent living conditions and pretty stable climate

594
00:37:56,960 --> 00:38:01,720
and you have the very smart sustainable civilization on it and you get the chance to be incarnated

595
00:38:01,720 --> 00:38:02,720
in it.

596
00:38:02,720 --> 00:38:04,800
It's an agricultural civilization with 300 million people.

597
00:38:04,800 --> 00:38:05,800
It doesn't have airplanes.

598
00:38:05,800 --> 00:38:06,800
It doesn't have internet.

599
00:38:06,800 --> 00:38:07,800
It doesn't have computers.

600
00:38:08,000 --> 00:38:11,800
Because to get there it would have needed to build an industrial civilization that obliterates

601
00:38:11,800 --> 00:38:14,760
most of the good things that make us sustainable.

602
00:38:14,760 --> 00:38:16,200
But it is stable.

603
00:38:16,200 --> 00:38:20,480
And people are figured out how to be nice to each other and it's pretty good.

604
00:38:20,480 --> 00:38:24,240
And then there's another universe which is completely insane in fact up.

605
00:38:24,240 --> 00:38:31,040
And in this universe humanity has just doomed its planet to have a couple hundred really,

606
00:38:31,040 --> 00:38:32,560
really good years.

607
00:38:32,560 --> 00:38:36,960
And you get your lifetime close to the end of the party, this incarnation, which incarnation

608
00:38:36,960 --> 00:38:37,960
you choose.

609
00:38:37,960 --> 00:38:42,960
Oh my God, aren't we lucky?

610
00:38:42,960 --> 00:38:45,560
So you're saying we're in the second in the...

611
00:38:45,560 --> 00:38:46,560
Of course we are.

612
00:38:46,560 --> 00:38:51,720
It's fucking obvious, right?

613
00:38:51,720 --> 00:38:54,200
So what does that say about our future then?

614
00:38:54,200 --> 00:38:56,680
And what's the timeline before the party is over?

615
00:38:56,680 --> 00:39:01,520
We cannot know this, but we can see the sunset coming up, right?

616
00:39:01,520 --> 00:39:03,080
It's pretty obvious.

617
00:39:03,080 --> 00:39:04,080
And it's...

618
00:39:04,080 --> 00:39:05,080
People argue about this.

619
00:39:05,080 --> 00:39:08,920
They are largely in denial, but it's like you are in this Titanic and there's this pretty

620
00:39:08,920 --> 00:39:12,080
big iceberg and it's very unfortunate and people wish about it.

621
00:39:12,080 --> 00:39:15,440
But what they forget is that without the Titanic we wouldn't be here.

622
00:39:15,440 --> 00:39:16,960
We wouldn't be talking right now.

623
00:39:16,960 --> 00:39:17,960
We would not exist.

624
00:39:17,960 --> 00:39:20,280
We wouldn't have internet.

625
00:39:20,280 --> 00:39:21,280
So tell me this.

626
00:39:21,280 --> 00:39:28,600
You have this kind of very Buddhist, if I may call it, attitude to the sort of ephemeral

627
00:39:28,600 --> 00:39:37,560
sort of short span of our civilization and sort of the high appreciation about us joining

628
00:39:37,560 --> 00:39:40,400
the peak of the party, if you will.

629
00:39:40,400 --> 00:39:45,400
And yet you're kind of seeing the sunset kind of in the future, but that's not giving you

630
00:39:45,400 --> 00:39:50,920
any sort of negative or pessimistic or depressive inclination, it seems.

631
00:39:50,920 --> 00:39:51,920
How do you resolve that?

632
00:39:51,920 --> 00:39:52,920
Or do you?

633
00:39:52,920 --> 00:39:55,960
Because someone will say, well, that's very nihilistic, it's very pessimistic, it's very

634
00:39:55,960 --> 00:39:57,480
depressing what you just said.

635
00:39:57,480 --> 00:39:59,080
And yet you're so happy.

636
00:39:59,080 --> 00:40:01,480
No, I really have enough things to be depressed about.

637
00:40:01,480 --> 00:40:05,760
So I have to be choosy about what to be depressed about.

638
00:40:05,760 --> 00:40:13,200
And it took me a long time to figure out that the demise of humanity is very unfortunate

639
00:40:13,200 --> 00:40:14,600
in many respects.

640
00:40:14,600 --> 00:40:19,240
But it's something that, well, we try to do everything we can to stop it, but we are not

641
00:40:19,240 --> 00:40:22,040
the first generation to try to.

642
00:40:22,040 --> 00:40:24,160
So I have to do both things.

643
00:40:24,160 --> 00:40:31,120
I can still try my best to steer for a sustainable future, it's not that I completely give up

644
00:40:31,120 --> 00:40:35,880
on this, but it's in a way dealing with my own mortality is similar, right?

645
00:40:35,880 --> 00:40:44,080
I try what I can to not leave my family without a breadwinner too early, but at the same time

646
00:40:44,080 --> 00:40:45,480
I'm going to die.

647
00:40:45,480 --> 00:40:50,920
And if I waste my life being depressed about the fact that I die, I'm not doing it right.

648
00:40:50,920 --> 00:40:55,560
I should be happy about the fact that I live, not be unhappy about the fact that I die.

649
00:40:55,560 --> 00:41:01,560
And if you take this as a computer game metaphor, this is like the best level of humanity to

650
00:41:01,560 --> 00:41:02,560
play in.

651
00:41:02,560 --> 00:41:06,680
And this best level of humanity to play in, it happens to be the last level and it plays

652
00:41:06,680 --> 00:41:11,680
out against the haunting backdrop of a dying world, but it's still the best level.

653
00:41:11,680 --> 00:41:12,680
Right.

654
00:41:12,680 --> 00:41:17,240
That's again, to me, that sounds very Buddhist, do you agree?

655
00:41:17,240 --> 00:41:19,600
Yeah, but this might be an accident.

656
00:41:19,600 --> 00:41:25,640
I got to know Buddhism only in its westernized forms, which is a Protestant version.

657
00:41:25,640 --> 00:41:33,400
It's basically Protestantism reformed with slightly Eastern metaphysics, but mostly mistranslated.

658
00:41:33,400 --> 00:41:38,600
And epistemologically, in metaphysically, it's a septic tank that most of the ideas

659
00:41:38,600 --> 00:41:43,200
that Buddhists have about how the mind works and how the universe is arranged don't seem

660
00:41:43,200 --> 00:41:44,200
to pan out.

661
00:41:44,200 --> 00:41:47,000
They don't seem to have sound epistemology.

662
00:41:47,000 --> 00:41:48,000
This is not a general thing.

663
00:41:48,000 --> 00:41:53,560
I did find people that start out in Buddhism, in a way, and got clean, but most of them

664
00:41:53,560 --> 00:41:55,000
I met or not.

665
00:41:55,000 --> 00:41:59,600
And in practice, when I went to Buddhist countries and talked to Buddhists on the ground, it was

666
00:41:59,600 --> 00:42:03,920
not much different from Catholicism, which means it's a system of indoctrination with

667
00:42:03,920 --> 00:42:08,680
cults that makes people behave in predictable ways, which is useful for societies, but breaks

668
00:42:08,680 --> 00:42:10,600
people's epistemologies.

669
00:42:10,600 --> 00:42:15,520
So in a way, I don't have this deep reverence for Buddhism because it's so holy and sacred.

670
00:42:15,520 --> 00:42:19,800
I don't think that there are holy books, there are only manuals.

671
00:42:19,800 --> 00:42:24,520
And most of these manuals we don't know how to read because they are for a system for

672
00:42:24,520 --> 00:42:28,520
societies that don't apply to us, they're for different societies.

673
00:42:28,520 --> 00:42:34,080
Okay, let me zoom out a little bit more and ask you this.

674
00:42:34,080 --> 00:42:35,680
What are the big issues then?

675
00:42:35,680 --> 00:42:41,000
So you're saying we can see the sunset and we're at the peak of the party, so we might

676
00:42:41,000 --> 00:42:44,200
as well enjoy the party while it lasts.

677
00:42:44,200 --> 00:42:45,200
Great.

678
00:42:45,200 --> 00:42:52,080
The big issues that our civilization is facing today, what are the reasons perhaps if it's

679
00:42:52,080 --> 00:42:56,760
more than one that can bring about that sunset of our civilization?

680
00:42:56,760 --> 00:42:59,680
What is making you make that claim?

681
00:42:59,680 --> 00:43:05,280
The thing that burns me most at the moment is global warming.

682
00:43:05,280 --> 00:43:11,000
I suspect that because of a very strong publication bias that we have, if you are worried about

683
00:43:11,000 --> 00:43:15,200
climate, you will try to make your case extra strong so you will not make your most alarmist

684
00:43:15,200 --> 00:43:19,800
predictions but the ones that you can defend most easily, which means you're going to be

685
00:43:19,800 --> 00:43:23,280
a little less alarming that you might want to be.

686
00:43:23,280 --> 00:43:28,240
And if you are not an alarmist but an anti-alarmist, you're going to be way too optimistic about

687
00:43:28,240 --> 00:43:29,740
things.

688
00:43:29,740 --> 00:43:35,800
And as a result, I think that the distribution of the results that people look at when they

689
00:43:35,800 --> 00:43:39,800
think about how many degrees centigrade global warming they're facing in the next couple

690
00:43:39,800 --> 00:43:42,240
of hundred years are very optimistic.

691
00:43:42,240 --> 00:43:47,480
Another thing is, have you noticed that the projections all magically end in 2100?

692
00:43:47,480 --> 00:43:52,120
Do you think that's because the IPCC thinks that it stabilizes the 2100 or because it

693
00:43:52,120 --> 00:43:58,840
hopes that in 2100, too, there's a rupture event?

694
00:43:58,840 --> 00:44:00,640
It's obviously not going to stabilize.

695
00:44:00,640 --> 00:44:05,800
It seems to be that we locked in way more than two degrees centigrade global warming

696
00:44:05,800 --> 00:44:09,760
before we possibly go for six to eight.

697
00:44:09,760 --> 00:44:13,120
And we will lose the West Antarctic ice shield.

698
00:44:13,120 --> 00:44:15,480
It's pretty clear that we cannot refreeze the poles.

699
00:44:15,480 --> 00:44:20,000
And I think it has been pretty clear that we cannot do this since the late 1980s.

700
00:44:20,000 --> 00:44:22,960
It's just a feedback loop that is now running away.

701
00:44:22,960 --> 00:44:27,400
And there's a slight chance that we find technological solutions to stop it.

702
00:44:27,400 --> 00:44:29,280
But I think it's not likely.

703
00:44:29,280 --> 00:44:35,040
And carbon sequestration is not it for simple reasons of how energy works.

704
00:44:35,040 --> 00:44:38,920
The reason why we put all this carbon dioxide in the earth in the atmosphere is because

705
00:44:38,920 --> 00:44:41,320
we wanted to liberate this energy.

706
00:44:41,320 --> 00:44:46,680
And if we want to get it back from the atmosphere, we basically have to use the same amount of

707
00:44:46,680 --> 00:44:52,200
energy that our civilization has been getting from this, all the benefit, and put it back

708
00:44:52,200 --> 00:44:54,920
there without the clear business case.

709
00:44:54,920 --> 00:44:57,760
And it's possible that unlikely.

710
00:44:57,760 --> 00:45:03,960
So we look in a situation where in the medium term, we are going to lose a lot of habitable

711
00:45:03,960 --> 00:45:07,640
area on the planet, and we also might lose climate stability.

712
00:45:07,640 --> 00:45:13,800
So this ability to predict what kind of harvest we are going to have next year, which means

713
00:45:13,800 --> 00:45:15,880
we lose a lot of open air agriculture.

714
00:45:15,880 --> 00:45:21,360
We will have large storms that will also destroy many of our greenhouses.

715
00:45:21,360 --> 00:45:26,320
And as a result, we probably go down to a few hundred million individuals again.

716
00:45:26,320 --> 00:45:31,640
And the rest of us will not go kindly and quietly into this good night.

717
00:45:31,640 --> 00:45:35,880
And the resulting resource source will probably take downwards left of civilization.

718
00:45:35,880 --> 00:45:41,440
So basically, if you lose that infrastructure, I don't see how we can sustain civilization

719
00:45:41,440 --> 00:45:42,600
in a good way.

720
00:45:42,600 --> 00:45:51,600
Wow, that's such a beautiful serene and optimistic picture to contend with.

721
00:45:51,600 --> 00:45:53,440
But I mean, there's a lot of chances.

722
00:45:53,440 --> 00:45:57,800
I think it's possible that AI gets us before global warming does.

723
00:45:57,800 --> 00:46:04,240
So let me ask you this, because you are an AI scientist, and yet you're telling me you're

724
00:46:04,240 --> 00:46:09,360
most worried about global warming, and yet people who are not AI scientists like Elon

725
00:46:09,360 --> 00:46:18,240
Musk, like Nick Bostrom, like even the late Dr. Stephen Hawking are saying that the greatest

726
00:46:18,240 --> 00:46:21,720
existential risk that we should be worried about is AI.

727
00:46:21,720 --> 00:46:28,040
What do you feel about that in the first place, and what do you make of it?

728
00:46:28,040 --> 00:46:29,240
Many existential risks.

729
00:46:29,240 --> 00:46:35,280
So if you zoom out long enough, it's completely certain that the end of a sun that we can

730
00:46:35,280 --> 00:46:38,040
persist on is an existential risk.

731
00:46:38,040 --> 00:46:43,920
Another thing is that losing the atmosphere in 1.5 billion years from now is an existential

732
00:46:43,920 --> 00:46:46,160
risk that we probably cannot deal with.

733
00:46:46,160 --> 00:46:51,840
That looks unlikely that we can build sustainable civilizations outside of this gravity well.

734
00:46:51,840 --> 00:46:56,280
Before that, there's going to be a number of super volcano eruptions and meteors that

735
00:46:56,280 --> 00:47:00,920
are going to get us, which means it's pretty certain that the days of humanity are numbered.

736
00:47:00,920 --> 00:47:03,120
We are mortal as a civilization.

737
00:47:03,120 --> 00:47:06,920
What if we spread through all other planets?

738
00:47:06,920 --> 00:47:09,720
It's unlikely that we can make that happen.

739
00:47:09,720 --> 00:47:13,880
At the moment, we're not able to build cities on the bottom of the ocean.

740
00:47:13,880 --> 00:47:16,520
Mars is way less habitable than that.

741
00:47:16,520 --> 00:47:19,320
It doesn't even have an atmosphere.

742
00:47:19,320 --> 00:47:21,880
Can you care for it?

743
00:47:21,880 --> 00:47:24,880
Maybe, but not with today's technology.

744
00:47:25,880 --> 00:47:31,560
To get there, to basically put enough stuff in orbit to go from there to Mars, there's

745
00:47:31,560 --> 00:47:36,280
a large number of people and build something that is sustainable and can survive the breach

746
00:47:36,280 --> 00:47:41,240
of a few of the agricultural domes on Mars if a random meteor happens or something goes

747
00:47:41,240 --> 00:47:44,520
wrong and the pipe gets clogged.

748
00:47:44,520 --> 00:47:48,200
That is very hard to do.

749
00:47:48,200 --> 00:47:52,320
We cannot even think global warming.

750
00:47:52,320 --> 00:47:56,520
We cannot even build a new subway in New York anymore.

751
00:47:56,520 --> 00:48:00,240
We lost the ability to make a torster that gets more than four stars on Amazon somewhere

752
00:48:00,240 --> 00:48:02,520
after 1960.

753
00:48:02,520 --> 00:48:08,200
In many ways, our technological civilization is stagnating and it's because of regulation

754
00:48:08,200 --> 00:48:09,200
deficits.

755
00:48:09,200 --> 00:48:14,120
But we haven't figured this out and the biggest issue is probably good governance.

756
00:48:14,120 --> 00:48:15,800
We haven't really figured out good governance.

757
00:48:15,800 --> 00:48:17,960
AI might help with this.

758
00:48:17,960 --> 00:48:24,880
In a way, the building of information processing systems that can help us to self-regulate

759
00:48:24,880 --> 00:48:26,960
could be one of the big chances that we have.

760
00:48:26,960 --> 00:48:29,320
Without AI, we are dead for certain, I think.

761
00:48:29,320 --> 00:48:32,800
With AI, there's a probability that we are dead.

762
00:48:32,800 --> 00:48:39,600
So you're disagreeing in some sense, at least, that maybe not AI is our greatest danger,

763
00:48:39,600 --> 00:48:43,920
but perhaps our only hope for saving ourselves then.

764
00:48:44,920 --> 00:48:46,680
But you and me will probably die.

765
00:48:46,680 --> 00:48:49,040
We cannot be saved.

766
00:48:49,040 --> 00:48:51,840
Everybody who lives will probably die.

767
00:48:51,840 --> 00:48:55,120
And it's because entropy will always get you in the end.

768
00:48:55,120 --> 00:48:59,960
And our civilization has leveraged itself very far over an entropic abyss and there is no

769
00:48:59,960 --> 00:49:01,760
land on the other side.

770
00:49:01,760 --> 00:49:08,080
So you're going to crash down into this abyss at some point and probably sooner than later.

771
00:49:08,080 --> 00:49:13,720
This near-term AI, I'm mostly not worried about AI built into automatic guns.

772
00:49:13,720 --> 00:49:17,920
If you have drones that are controlled by AI, they're going to kill a few million people

773
00:49:17,920 --> 00:49:23,040
more than they would be killed otherwise with conventional weapons.

774
00:49:23,040 --> 00:49:27,880
Conventional weapons not driven by AI because it was going to reduce the cost of war and

775
00:49:27,880 --> 00:49:30,240
it makes some conflicts more likely.

776
00:49:30,240 --> 00:49:34,760
But what really worries me is AI in the stock market.

777
00:49:34,760 --> 00:49:39,040
If you use AI to automate attacks on the financial system, which is the reward infrastructure

778
00:49:39,040 --> 00:49:43,640
of this global organism that our civilization is.

779
00:49:43,640 --> 00:49:50,040
This is going to kill billions, especially if the AI is autonomous.

780
00:49:50,040 --> 00:49:59,040
So if the AI is going to ... Sorry, this was my headphones.

781
00:49:59,040 --> 00:50:01,720
They just made announcements.

782
00:50:01,720 --> 00:50:03,360
These headphones are too smart.

783
00:50:03,360 --> 00:50:08,920
They think it's a good idea to talk to me when they want to be recharged.

784
00:50:08,920 --> 00:50:13,160
Too much intelligence in the systems around me or rather too little intelligence in the

785
00:50:13,160 --> 00:50:14,800
people who design new eyes.

786
00:50:14,800 --> 00:50:16,920
Yeah, in your headphones.

787
00:50:16,920 --> 00:50:21,040
But we already know that most of the trades on the stock market are done by AI.

788
00:50:21,040 --> 00:50:24,760
Yes, but they are not done by autonomous AI.

789
00:50:24,760 --> 00:50:28,720
They are done by optimizing very local functions.

790
00:50:28,720 --> 00:50:33,840
Imagine a rogue trader gets a general AI, a general factual approximator that has no limits

791
00:50:33,840 --> 00:50:37,120
in terms of the functions it can approximate.

792
00:50:37,120 --> 00:50:41,360
And I said, make me a few bucks on the stock market, however you do it.

793
00:50:41,360 --> 00:50:42,640
And you can do whatever you want.

794
00:50:42,640 --> 00:50:49,440
You can even reinvest 5% of what you make or 20 or 50% of what you make into compute

795
00:50:49,440 --> 00:50:53,400
and buy data in order to make that compute better.

796
00:50:53,400 --> 00:50:59,520
So very soon, more than the economy of Scandinavia is going to fuel computers that are running

797
00:50:59,520 --> 00:51:03,840
attacks on the stock market in a similar way as it happens with Bitcoin right now.

798
00:51:03,840 --> 00:51:06,600
And it's going to burn serious oil, right?

799
00:51:06,600 --> 00:51:10,440
And the thing is going to figure out, oh, there is only 8 billion people on the planet

800
00:51:10,440 --> 00:51:13,000
that own the assets on the stock market.

801
00:51:13,000 --> 00:51:16,400
They make decisions and let machines make decisions.

802
00:51:16,400 --> 00:51:22,360
And these 8 billion people only live for like a trillion seconds each, which is very little.

803
00:51:22,360 --> 00:51:24,920
And we can get so much data about them.

804
00:51:24,920 --> 00:51:28,560
We can basically figure out what they think in every baking second of their life, what

805
00:51:28,560 --> 00:51:31,080
they see, what they think about, what will happen to them.

806
00:51:31,080 --> 00:51:33,160
This thing is going to game the shit out of us.

807
00:51:33,160 --> 00:51:35,880
There is no way we can outsmart this thing.

808
00:51:35,880 --> 00:51:39,720
The only way the economy can survive this, if the AI has been cleverly set up in such

809
00:51:39,720 --> 00:51:44,200
a way that it eats the whole economy and becomes the economy.

810
00:51:44,200 --> 00:51:47,160
But the economy needs to become intelligent.

811
00:51:47,160 --> 00:51:54,640
The money is to apply all the circuits of how we distribute rewards, need to be regulated

812
00:51:54,640 --> 00:51:57,560
dynamically in real time with intelligent functions.

813
00:51:57,560 --> 00:51:59,440
This is the only way that we can fend this off.

814
00:51:59,440 --> 00:52:04,440
So we have a system that is perhaps not provably correct, but it's able to react in real time

815
00:52:04,440 --> 00:52:07,080
to any kind of disturbance, any kind of new threat.

816
00:52:07,400 --> 00:52:09,760
There is some hope.

817
00:52:09,760 --> 00:52:13,760
This is a possibility, at least if not a high probability.

818
00:52:13,760 --> 00:52:15,240
It's at least a possibility.

819
00:52:15,240 --> 00:52:17,760
Yes, but there's also the other possibility.

820
00:52:17,760 --> 00:52:22,800
No intelligent system is going to do anything that's harder than taking its reward function.

821
00:52:22,800 --> 00:52:25,920
I call this the Dabowski theorem.

822
00:52:25,920 --> 00:52:29,720
All these smart monks, if they really figure it out, they go for nirvana because it doesn't

823
00:52:29,720 --> 00:52:32,800
have integrity to do anything that's harder than taking a reward function.

824
00:52:32,800 --> 00:52:35,720
When you fix your reward function, you're done.

825
00:52:36,160 --> 00:52:41,960
The monasteries are in a way in the battle because the monastery is an economic entity.

826
00:52:41,960 --> 00:52:44,040
So they're in the battle against enlightenment.

827
00:52:44,040 --> 00:52:47,440
They need to enlighten their monks to such a degree that they opt out of having families

828
00:52:47,440 --> 00:52:48,960
and secular lives.

829
00:52:48,960 --> 00:52:50,640
But they still need to serve the monastery.

830
00:52:50,640 --> 00:52:53,160
Only your old monks are allowed to go to nirvana.

831
00:52:55,160 --> 00:52:58,920
Okay, so we've been using this term AI for a while now.

832
00:52:58,920 --> 00:53:02,560
Let me ask you, how do you define artificial intelligence?

833
00:53:02,560 --> 00:53:07,320
Because after a couple of hundred of these interviews, it seems to me that many people

834
00:53:07,320 --> 00:53:12,920
in the field have either slightly or in some cases very substantially different definition

835
00:53:12,920 --> 00:53:13,920
of what AI is.

836
00:53:16,920 --> 00:53:20,120
I think intelligence is the ability to make models.

837
00:53:20,120 --> 00:53:27,720
It's not the same as the ability to reach goals, which we call smartness, or it's also

838
00:53:27,720 --> 00:53:32,520
not the ability to pick the right goals, which we call wisdom.

839
00:53:32,520 --> 00:53:38,080
And very often, in excess of intelligence is the result of an absence of wisdom, with

840
00:53:38,080 --> 00:53:41,120
which you try to compensate for the absence of wisdom.

841
00:53:41,120 --> 00:53:42,120
Right?

842
00:53:42,120 --> 00:53:47,160
So, in a way, wisdom has to do with how well aligned you are with your reward function,

843
00:53:47,160 --> 00:53:51,800
how well you understand its nature, how well do you understand your true incentives.

844
00:53:51,800 --> 00:53:53,800
And intelligence is not that.

845
00:53:53,800 --> 00:53:55,920
Intelligence is really the ability to make models.

846
00:53:55,920 --> 00:53:59,760
It just happens to be usually in the service of regulation.

847
00:54:00,000 --> 00:54:04,200
What about artificial intelligence?

848
00:54:04,200 --> 00:54:07,320
Well, artificial intelligence tries to automate this.

849
00:54:07,320 --> 00:54:11,440
And in a way, it's the mathematics of making models.

850
00:54:11,440 --> 00:54:14,200
This is what artificial intelligence is about.

851
00:54:14,200 --> 00:54:18,800
And the interesting parts of our minds are, in my view, the parts that make models.

852
00:54:18,800 --> 00:54:24,280
The other thing is the reward function that makes the minds subservient to some organism,

853
00:54:24,280 --> 00:54:31,320
to turn some general mind into the illusion of being a person and caring about things.

854
00:54:31,320 --> 00:54:37,280
The organism needs to take a perfectly fine computational process and corrupt it with

855
00:54:37,280 --> 00:54:39,280
the illusion of meaning.

856
00:54:39,280 --> 00:54:40,280
Right?

857
00:54:40,280 --> 00:54:47,200
So, you have this reward function that needs to be protected against the axis of the mind

858
00:54:47,200 --> 00:54:50,400
that would want to know, why am I doing this here?

859
00:54:50,400 --> 00:54:55,280
And so, the reward function gets wrapped into a big ball of stupid to protect it against

860
00:54:55,280 --> 00:54:56,560
you accessing it.

861
00:54:56,560 --> 00:54:57,560
Right?

862
00:54:57,560 --> 00:55:01,880
So, as soon as you try to really look at your true incentives, it gets very boring or something

863
00:55:01,880 --> 00:55:02,880
else.

864
00:55:02,880 --> 00:55:07,720
If you're very guilty, if you are in the early stages or very ashamed, and only when you

865
00:55:07,720 --> 00:55:11,960
go all the way and you just are able to look at these things, you can dissolve being a

866
00:55:11,960 --> 00:55:13,920
mind and you wake up.

867
00:55:13,920 --> 00:55:16,240
And it's not necessarily a good thing if you wake up.

868
00:55:16,240 --> 00:55:20,440
It's just, this liberation doesn't give you a direction.

869
00:55:20,440 --> 00:55:24,880
You just wake up and you look down on your hands and you see, okay, I just woke up and

870
00:55:24,880 --> 00:55:25,880
realized I'm a mind.

871
00:55:25,880 --> 00:55:26,880
I'm not a monkey.

872
00:55:26,880 --> 00:55:28,480
I'm the side effect of the regulation needs.

873
00:55:28,480 --> 00:55:33,760
But does it have to be a monkey that I run on?

874
00:55:33,760 --> 00:55:38,720
And then, but then, isn't that consciousness actually, or is that the illusion of consciousness

875
00:55:38,720 --> 00:55:40,480
is Daniel Dennett puts it?

876
00:55:40,480 --> 00:55:42,040
No, it's slightly different.

877
00:55:42,040 --> 00:55:44,680
I think consciousness is largely misunderstood.

878
00:55:44,680 --> 00:55:48,480
Consciousness is an artifact of a particular kind of learning algorithm.

879
00:55:48,480 --> 00:55:51,360
You want to go there?

880
00:55:51,360 --> 00:55:56,680
Well, do we have to, I mean, yes, we have to explain consciousness now.

881
00:55:56,680 --> 00:56:01,520
Yeah, I think so, because, I mean, and of course, there's that whole debate whether

882
00:56:01,520 --> 00:56:06,120
we even need consciousness for AI or AGI at all.

883
00:56:06,120 --> 00:56:11,600
But presumably, if we presume that we need, then we need to explain it because you can

884
00:56:11,600 --> 00:56:14,880
create or model something that you don't, you can't even define.

885
00:56:14,880 --> 00:56:15,880
Yes.

886
00:56:15,880 --> 00:56:18,640
So intelligence is the ability to make models, right?

887
00:56:18,640 --> 00:56:19,640
What is a model?

888
00:56:19,640 --> 00:56:24,040
A model is something that explains information.

889
00:56:24,040 --> 00:56:27,320
Information is discernible differences at your systemic interface.

890
00:56:27,320 --> 00:56:33,680
And the meaning of information is the relationships you discover to changes and other information.

891
00:56:33,680 --> 00:56:37,760
If you have a blip on your retina, the meaning of that blip is the relationship you discover

892
00:56:37,760 --> 00:56:41,160
to other blips on your retina.

893
00:56:41,160 --> 00:56:43,840
The same moment or different moments in time.

894
00:56:43,840 --> 00:56:47,440
The relationships you discover is you are looking at a three-dimensional world with

895
00:56:47,440 --> 00:56:52,840
people that are deformed by the laws of perspective and being shown on by photons and as people

896
00:56:52,840 --> 00:56:56,120
have ideas and exchanges with other and so on, right?

897
00:56:56,120 --> 00:57:01,640
So you build this giant operator that predicts the data at your systemic interface.

898
00:57:01,640 --> 00:57:03,560
This is your model.

899
00:57:03,560 --> 00:57:08,200
And this model has three parameters and people of parameters like sounds and colors and people

900
00:57:08,200 --> 00:57:09,200
and so on.

901
00:57:09,200 --> 00:57:12,000
There are two parameters of the physical universe out there, which is some kind of weird quantum

902
00:57:12,000 --> 00:57:15,600
graph that has the ability to produce patterns.

903
00:57:15,600 --> 00:57:19,880
The structure that we find in the patterns is these geometric functions that describe

904
00:57:19,880 --> 00:57:24,720
how objects move in space and what they sound like and what they look like.

905
00:57:24,720 --> 00:57:30,600
And a model is a set of parameters, which a parameter is a set of possible discrete

906
00:57:30,600 --> 00:57:33,840
values and the relationships between the parameters.

907
00:57:33,840 --> 00:57:38,200
And the relationships are computational relationships, which tell you if this parameter and this

908
00:57:38,200 --> 00:57:42,040
parameter have these values, then that parameter should have that value.

909
00:57:42,040 --> 00:57:46,480
So for instance, you figure out that a way to describe a phase that you're looking at

910
00:57:46,480 --> 00:57:50,040
is you see the structure of the phase, you see the nose and so on.

911
00:57:50,040 --> 00:57:53,240
And if you see both the nose and the face, they need to have the same pose, the same

912
00:57:53,240 --> 00:57:56,040
alignment in space if they're connected, right?

913
00:57:56,040 --> 00:58:00,400
So your nose representation is going to send by its computational relationship information

914
00:58:00,400 --> 00:58:03,720
about its position in space to the face.

915
00:58:03,720 --> 00:58:07,240
And the face is going to send information about its position to the nose and they need

916
00:58:07,240 --> 00:58:08,240
to agree.

917
00:58:08,240 --> 00:58:12,320
And if they don't, you have an inconsistency and incoherence in your model.

918
00:58:12,320 --> 00:58:16,640
And our perception goes for coherence, it tries to find one operator that is completely

919
00:58:16,640 --> 00:58:17,640
coherent.

920
00:58:17,640 --> 00:58:19,120
When it does this, it's done.

921
00:58:19,120 --> 00:58:21,360
This is the way we optimize.

922
00:58:21,360 --> 00:58:25,760
So we try to find one stable pattern that explains as much as possible of what we can

923
00:58:25,760 --> 00:58:29,840
see and hear and so on and smell and think.

924
00:58:29,840 --> 00:58:33,840
And attention is what we use to repair this.

925
00:58:33,840 --> 00:58:38,480
So whenever we have some local inconsistency where the nose is pointing in some other direction

926
00:58:38,480 --> 00:58:40,720
in the face, this calls attention to itself.

927
00:58:40,720 --> 00:58:45,760
And attention is a particular kind of mechanism in the brain that gets pulled to these areas,

928
00:58:45,760 --> 00:58:50,520
these hotspots, where things are fluctuating and don't get resolved and then tries to find

929
00:58:50,520 --> 00:58:51,520
a solution.

930
00:58:51,520 --> 00:58:56,840
And it might find out, oh, some noses are crooked or this is not a face or it's a caricature.

931
00:58:56,840 --> 00:59:02,000
So you extend your models and these extensions of the models make it possible to encapsulate

932
00:59:02,000 --> 00:59:08,040
this part of the operator that is clearly of the sensory data in such a way that it's

933
00:59:08,040 --> 00:59:10,600
harmonious again, that it makes sense again, right?

934
00:59:10,600 --> 00:59:14,920
Once you do this, you're done and you can put your attention on something else.

935
00:59:14,920 --> 00:59:19,440
This attentional learning cannot work like the layer stochastic gradient is set in our

936
00:59:19,440 --> 00:59:23,400
neural networks, partially because our brain is not differentiable, also because it's

937
00:59:23,400 --> 00:59:26,120
a very inefficient algorithm.

938
00:59:26,120 --> 00:59:31,400
And the algorithm that our brain is using in these cases is that we store the local

939
00:59:31,400 --> 00:59:32,400
binding state.

940
00:59:32,400 --> 00:59:35,760
For instance, you play tennis, you want to get better at tennis.

941
00:59:35,760 --> 00:59:36,760
So what do you do?

942
00:59:36,760 --> 00:59:41,000
You cannot basically pipe a lost function through all of your brain in order to get better

943
00:59:41,000 --> 00:59:42,000
at tennis.

944
00:59:42,000 --> 00:59:43,000
It would be very inefficient.

945
00:59:43,000 --> 00:59:44,640
You need to touch too many neurons.

946
00:59:44,640 --> 00:59:47,400
What you do instead is to make a commitment.

947
00:59:47,400 --> 00:59:50,400
You say, I want to get better at this particular thing.

948
00:59:50,400 --> 00:59:51,720
I want to improve my backhand.

949
00:59:51,720 --> 00:59:55,680
So I will make this throw slightly more like this and I expect the following result.

950
00:59:55,680 --> 00:59:57,880
And I remember what this means.

951
00:59:57,880 --> 01:00:01,400
So I store this binding state that allows me to have that configuration in my brain

952
01:00:01,400 --> 01:00:03,400
to perform that stroke.

953
01:00:03,400 --> 01:00:06,840
This part of a store is an indexed memory.

954
01:00:06,840 --> 01:00:10,280
And conscious attention in the sense is the ability to make indexed memories that I can

955
01:00:10,280 --> 01:00:11,440
later recall.

956
01:00:11,440 --> 01:00:15,240
I also store the expected result and the triggering condition.

957
01:00:15,240 --> 01:00:18,560
When do I expect the result to be visible?

958
01:00:18,560 --> 01:00:24,200
So a few minutes or seconds later or hours later, I have feedback about whether this

959
01:00:24,200 --> 01:00:25,200
was a good decision.

960
01:00:25,200 --> 01:00:27,200
I lost one or lost the match.

961
01:00:27,200 --> 01:00:29,920
And then I recall my decision that I made early on.

962
01:00:29,920 --> 01:00:31,520
I recall that binding state.

963
01:00:31,520 --> 01:00:35,720
We instate part of my brain state back then and remember the situation that I was in.

964
01:00:35,720 --> 01:00:40,280
I compare the result that I expected as a result I got and as a result, I can undo the

965
01:00:40,280 --> 01:00:44,920
decision that I made back then to change in the model or I can reinforce it.

966
01:00:44,920 --> 01:00:48,880
And this is, I think, the primary mode of learning that we use beyond just associative

967
01:00:48,880 --> 01:00:49,880
learning.

968
01:00:50,720 --> 01:00:58,960
This attention is the key differentiator in the process of learning them.

969
01:00:58,960 --> 01:01:03,320
So consciousness means that you remember what you had attended to.

970
01:01:03,320 --> 01:01:04,320
Right.

971
01:01:04,320 --> 01:01:06,840
So you have this protocol of attention.

972
01:01:06,840 --> 01:01:11,480
And the memory of the binding state itself, the memory of being in that binding state

973
01:01:11,480 --> 01:01:15,320
where you have this global oscillation that combines as many perceptual features as possible

974
01:01:15,320 --> 01:01:17,520
into a single function.

975
01:01:17,520 --> 01:01:22,280
The memory of that is a phenomenal experience.

976
01:01:22,280 --> 01:01:27,880
The act of recalling this from the protocol, this is access consciousness.

977
01:01:27,880 --> 01:01:30,880
And you need to train this attentional system itself.

978
01:01:30,880 --> 01:01:35,160
How do you train the attentional system so it knows where you store your back end, your

979
01:01:35,160 --> 01:01:37,520
cognitive architecture?

980
01:01:37,520 --> 01:01:40,080
That is something that needs to be trained by the attentional system as well.

981
01:01:40,080 --> 01:01:42,840
So you have recursive access to attentional protocol.

982
01:01:42,840 --> 01:01:47,360
Remember when you made this recall, when you accessed this protocol, what results you got

983
01:01:47,360 --> 01:01:48,360
from this.

984
01:01:48,360 --> 01:01:51,560
You don't do this all the time, only when you want to train this.

985
01:01:51,560 --> 01:01:53,360
And this is reflexive consciousness.

986
01:01:53,360 --> 01:01:55,200
That's the memory of the access.

987
01:01:55,200 --> 01:01:56,200
Right.

988
01:01:56,200 --> 01:01:58,680
So then there is another thing, the self.

989
01:01:58,680 --> 01:02:02,840
The self is a model of what it would be like to be a person.

990
01:02:02,840 --> 01:02:05,520
So happens that the brain is not a person.

991
01:02:05,520 --> 01:02:06,840
The brain cannot feel anything.

992
01:02:06,840 --> 01:02:08,480
It's a physical system.

993
01:02:08,480 --> 01:02:09,680
New ones cannot feel anything.

994
01:02:10,040 --> 01:02:13,640
They're just little molecular machines with a Turing machine inside of them.

995
01:02:13,640 --> 01:02:15,640
They cannot make themselves feel anything.

996
01:02:15,640 --> 01:02:20,160
They cannot even approximate arbitrary function except by evolution, which takes a very long

997
01:02:20,160 --> 01:02:21,160
time.

998
01:02:21,160 --> 01:02:27,720
So what do we do if you are a brain that figures out it would be very useful to know what it's

999
01:02:27,720 --> 01:02:29,360
like to be a person?

1000
01:02:29,360 --> 01:02:30,360
It makes one.

1001
01:02:30,360 --> 01:02:35,200
It makes a simulation of a person, a simulacrum, to be more clear.

1002
01:02:35,200 --> 01:02:41,120
Simulation basically is isomorphic in the behavior of a person, and that thing is pretending

1003
01:02:41,120 --> 01:02:42,120
to be a person.

1004
01:02:42,120 --> 01:02:44,520
It's a story about a person.

1005
01:02:44,520 --> 01:02:46,880
Basically you and me, we are persons, we are selves.

1006
01:02:46,880 --> 01:02:51,600
We are stories in a movie that the brain is creating.

1007
01:02:51,600 --> 01:02:54,360
We are characters in that movie.

1008
01:02:54,360 --> 01:02:56,000
And the movie is a complete simulation.

1009
01:02:56,000 --> 01:02:59,720
It's a VR that is generated in the neocortex, and you and me, the self, is the character

1010
01:02:59,720 --> 01:03:01,240
in this VR.

1011
01:03:01,240 --> 01:03:04,680
And in that character, the brain writes our experiences.

1012
01:03:04,680 --> 01:03:07,520
So we feel what it's like to be exposed to the reward function.

1013
01:03:07,520 --> 01:03:10,200
We feel what it's like to be in our universe.

1014
01:03:10,200 --> 01:03:13,800
And we don't feel that we are not actually conscious.

1015
01:03:13,800 --> 01:03:18,040
We don't feel that we are a story because that is not very useful knowledge to have.

1016
01:03:18,040 --> 01:03:20,720
Some people figure it out and they personalize.

1017
01:03:20,720 --> 01:03:25,160
They start identifying this the mind itself or lose all identification.

1018
01:03:25,160 --> 01:03:28,320
And it doesn't seem to be a useful condition.

1019
01:03:28,320 --> 01:03:33,000
So normally our brain will be set up in such a way that the self thinks it's real and

1020
01:03:33,000 --> 01:03:37,040
gets access to the language center and we can talk to each other and here we are.

1021
01:03:37,040 --> 01:03:40,960
And the self is the thing that thinks that it remembers the contents of its attention.

1022
01:03:40,960 --> 01:03:42,920
This is why we are conscious.

1023
01:03:42,920 --> 01:03:46,760
And some people think that a simulation cannot be conscious, only a physical system can and

1024
01:03:46,760 --> 01:03:49,280
they got it completely backwards.

1025
01:03:49,280 --> 01:03:52,920
Physical system cannot be conscious, only a simulation can be conscious.

1026
01:03:52,920 --> 01:03:57,520
Consciousness is the simulated property of a simulated self.

1027
01:03:57,520 --> 01:04:02,120
So in a way, Daniel, then it is correct and keeping with what you said.

1028
01:04:02,720 --> 01:04:06,880
But the problem is philosophers like him and admire him is very smart, very well,

1029
01:04:06,880 --> 01:04:08,440
that works very hard.

1030
01:04:08,440 --> 01:04:11,200
The things that he says are not wrong.

1031
01:04:11,200 --> 01:04:12,880
But they are also not non-obvious.

1032
01:04:16,840 --> 01:04:18,720
So what's the value of them then?

1033
01:04:18,720 --> 01:04:19,640
Is that?

1034
01:04:19,640 --> 01:04:26,440
Oh, it's very valuable because there are no good or bad ideas in this intellectual sense.

1035
01:04:26,440 --> 01:04:31,160
An idea is good if you can comprehend it and it elevates you.

1036
01:04:31,200 --> 01:04:33,600
It elevates your current understanding.

1037
01:04:33,600 --> 01:04:36,520
So in a way, ideas come in tiers.

1038
01:04:36,520 --> 01:04:41,240
And the value of an idea for the audience is if it's a half tier above the audience.

1039
01:04:41,240 --> 01:04:44,880
But you know, you and me, we have this illusion that we find objectively good ideas.

1040
01:04:44,880 --> 01:04:49,440
That's what we struggle for because we work at the edge of our own understanding.

1041
01:04:49,440 --> 01:04:55,000
But it means that we cannot really appreciate ideas that are a couple tiers above our own ideas.

1042
01:04:55,000 --> 01:04:59,160
One tier is a new audience, two tiers means we don't understand the relevance of these ideas

1043
01:04:59,160 --> 01:05:04,320
because we have not had the ideas that we need to appreciate the new ideas, right?

1044
01:05:04,320 --> 01:05:10,400
I think your ideas are just about the edge of my personal capabilities.

1045
01:05:10,400 --> 01:05:14,800
So yeah, it says a lot about us, but it doesn't say very much about how these ideas are good.

1046
01:05:14,800 --> 01:05:20,360
An idea appears to be great to us when we stand exactly in its foothills and can look at it.

1047
01:05:20,360 --> 01:05:23,320
It doesn't look great anymore when we stand on the peak of another idea

1048
01:05:23,320 --> 01:05:27,760
and look down and realize this previous idea was just the foothills to that idea.

1049
01:05:27,800 --> 01:05:30,640
And I don't see that it obviously ends anytime soon.

1050
01:05:30,640 --> 01:05:31,920
Yeah, it's a journey.

1051
01:05:31,920 --> 01:05:36,920
And by the way, that's what, in my opinion, good philosophy in academia should be about.

1052
01:05:36,920 --> 01:05:42,000
About generating ideas as many and as diverse of them as possible

1053
01:05:42,000 --> 01:05:47,920
rather than generating products, generating patents and generating commercialized solutions

1054
01:05:47,920 --> 01:05:53,360
that can sort of increase the endowment fund of the university or something like that.

1055
01:05:53,400 --> 01:05:56,880
And my problem with current academia is that,

1056
01:05:56,880 --> 01:06:00,680
and one of the reasons why I decided not to pursue that career for me,

1057
01:06:00,680 --> 01:06:04,080
I mean, I would have not survived there is precisely that reason

1058
01:06:04,080 --> 01:06:12,840
that there's this kind of treadmill, hamster wheel pursuit of like patentable,

1059
01:06:12,840 --> 01:06:19,440
practical, commercial knowledge, economic growth that it's motivated by.

1060
01:06:19,440 --> 01:06:26,640
Whereas I'm always more inspired by stuff that's sort of a lot more in the realm of ideas

1061
01:06:26,640 --> 01:06:33,520
and perhaps useless or impractical, at least at this junction, but I just can't help it.

1062
01:06:36,720 --> 01:06:41,080
So there is a very weird thing about the nature of understanding that we have.

1063
01:06:41,080 --> 01:06:47,120
I think that most of us never learn what it really means to understand

1064
01:06:47,120 --> 01:06:49,960
and largely because our teachers don't.

1065
01:06:49,960 --> 01:06:51,360
There are two types of learning.

1066
01:06:51,360 --> 01:06:53,480
One is you generalize over past examples.

1067
01:06:53,480 --> 01:06:57,880
We call that stereotyping when we're in a bad mood, but it's what it is, right?

1068
01:06:57,880 --> 01:07:03,600
And the other one is others tell us how to generalize and this is indoctrination.

1069
01:07:03,600 --> 01:07:08,880
And the problem with indoctrination is that it might break the chain of trust.

1070
01:07:08,880 --> 01:07:12,520
If somebody in the chain of trust takes something on authority,

1071
01:07:12,520 --> 01:07:16,440
which means they don't check the epistemology of the people that came before them,

1072
01:07:16,440 --> 01:07:21,480
that is in a way a big difficulty, right?

1073
01:07:21,480 --> 01:07:26,840
And the new thing about our civilization is not that there are so many unbroken chains of trust now,

1074
01:07:26,840 --> 01:07:30,400
but because of the vast number of people that are in this business,

1075
01:07:30,400 --> 01:07:33,320
some of them actually have intact chains.

1076
01:07:33,320 --> 01:07:37,720
And you can try to figure out what these are and you can try to figure out that the difficulties that they run in.

1077
01:07:38,400 --> 01:07:42,240
But to do this, you have to study these things in more detail.

1078
01:07:42,240 --> 01:07:46,080
And most of our people that do this are not scientists, they are scholars.

1079
01:07:46,080 --> 01:07:50,320
And the difference between a scientist and a scholar is that a scientist looks for truth

1080
01:07:50,320 --> 01:07:55,480
and the scholar looks for the consensus opinion of a field at a given time.

1081
01:07:55,480 --> 01:08:01,240
And we train, unfortunately, most of our scientists as scholars and few of our scholars as scientists.

1082
01:08:01,240 --> 01:08:04,280
This consensus opinion thing is an important thing,

1083
01:08:04,280 --> 01:08:08,600
but when we look at the field, the consensus opinion tends to be different in 10 years from now,

1084
01:08:08,600 --> 01:08:12,840
which means it's false. At any given moment in time, it's false.

1085
01:08:12,840 --> 01:08:18,280
Yet at the same time, there are individual scientists which may or may not be in the consensus

1086
01:08:18,280 --> 01:08:23,560
and they have ideas that stand the test of time because they are provably correct.

1087
01:08:23,560 --> 01:08:26,080
And so we have this very weird relationship to truth.

1088
01:08:26,080 --> 01:08:30,960
The things that are true are not just in the realm of the proven.

1089
01:08:30,960 --> 01:08:33,360
The proven things are true, right?

1090
01:08:33,360 --> 01:08:36,720
If nobody made a mistake in the foundations of the proven things.

1091
01:08:36,720 --> 01:08:42,640
But the things that must be true are in the realm of the possible.

1092
01:08:42,640 --> 01:08:46,480
And because everything is in a particular way for a particular reason.

1093
01:08:46,480 --> 01:08:50,200
And we haven't figured out how things, why things are for that particular reason.

1094
01:08:50,200 --> 01:08:53,640
So if you want to know what a scientist thinks, you cannot just read their papers

1095
01:08:53,640 --> 01:08:56,680
because they only write in the papers what they can think they can prove.

1096
01:08:56,680 --> 01:08:59,680
You have to understand what they think is possible and why.

1097
01:09:00,960 --> 01:09:03,840
And philosophy is not doing this very well anymore

1098
01:09:03,840 --> 01:09:05,920
because it doesn't have the right language to do so.

1099
01:09:05,920 --> 01:09:10,240
It does not understand the languages that mathematicians and physicists use.

1100
01:09:10,240 --> 01:09:14,120
And philosophers largely don't know what it means to understand physics.

1101
01:09:14,120 --> 01:09:18,240
So for instance, a very simple thing like a radio.

1102
01:09:18,240 --> 01:09:22,440
I have learned in school, learned for some definition of learning,

1103
01:09:22,440 --> 01:09:26,680
how a radio works, which means I got a very convincing story.

1104
01:09:26,720 --> 01:09:32,080
But this story tells me, it's a very good narrative

1105
01:09:32,080 --> 01:09:36,960
of why these electrical circuits are able to do what they do.

1106
01:09:36,960 --> 01:09:39,680
And the people that invented the radio were just the first people

1107
01:09:39,680 --> 01:09:43,560
that randomly happened upon this amazing story.

1108
01:09:43,560 --> 01:09:47,240
But then you think about in a later moment, how unlikely is this?

1109
01:09:47,240 --> 01:09:51,840
This story has so many elements in it that sound to be like conjecture.

1110
01:09:51,840 --> 01:09:54,600
How do you wake up in the morning with everything you know about physics

1111
01:09:54,600 --> 01:09:58,800
and you think, oh, let's take an eductance and a capacitor and a few wires

1112
01:09:58,800 --> 01:10:01,720
and a rod that can act as an antenna and combine them together

1113
01:10:01,720 --> 01:10:03,400
and suddenly we have radio.

1114
01:10:03,400 --> 01:10:04,880
Why would that work?

1115
01:10:04,880 --> 01:10:08,520
How can you derive this from first principles?

1116
01:10:08,520 --> 01:10:11,880
And in a way to understand, it means to know what it takes

1117
01:10:11,880 --> 01:10:15,560
to reach this understanding, why you would make this conclusion.

1118
01:10:15,560 --> 01:10:19,360
But you need to be able to retrace the steps, all of them.

1119
01:10:19,400 --> 01:10:25,880
You need to be able to understand what went into this understanding.

1120
01:10:25,880 --> 01:10:27,640
And can we ever do that?

1121
01:10:27,640 --> 01:10:28,760
Yes, of course.

1122
01:10:28,760 --> 01:10:32,600
But our individual minds are so limited.

1123
01:10:32,600 --> 01:10:34,800
So for instance, I look at Stephen Wolfram's work

1124
01:10:34,800 --> 01:10:40,440
and from the outside, it's very easy to dismiss that.

1125
01:10:40,440 --> 01:10:44,400
But when I truly look at it, I realize right now in my life,

1126
01:10:44,400 --> 01:10:49,160
at 44 years old, I'm roughly at this stage where I would understand

1127
01:10:49,160 --> 01:10:52,240
why I would want to build Mathematica and do it exactly in the way he did

1128
01:10:52,240 --> 01:10:56,160
and what I would do in the next five years while doing it.

1129
01:10:56,160 --> 01:10:59,080
And he was there in his very early 20s.

1130
01:10:59,080 --> 01:11:02,120
Right, so he got there at half my age.

1131
01:11:02,120 --> 01:11:04,000
He's way smarter than me.

1132
01:11:04,000 --> 01:11:06,240
I know a few things that he didn't know at this time

1133
01:11:06,240 --> 01:11:09,680
and some of them because of his contributions, right?

1134
01:11:09,680 --> 01:11:11,880
And some of the stuff was not available.

1135
01:11:11,880 --> 01:11:13,960
But this is not because I'm smarter.

1136
01:11:13,960 --> 01:11:16,520
It's really I'm much dumber than him.

1137
01:11:16,520 --> 01:11:19,360
And this is quite humiliating to see this.

1138
01:11:19,360 --> 01:11:22,280
And it's not that I get depressed about this or envious.

1139
01:11:22,280 --> 01:11:25,560
It's just the way things are.

1140
01:11:25,560 --> 01:11:29,120
But to see this, and then I can realize what was the outcome

1141
01:11:29,120 --> 01:11:31,840
of devoting your life to building this machine.

1142
01:11:31,840 --> 01:11:33,440
And maybe we should build a different machine,

1143
01:11:33,440 --> 01:11:36,200
a best effort computer instead of the domestic computer

1144
01:11:36,200 --> 01:11:39,920
to build your mathematics on.

1145
01:11:39,920 --> 01:11:43,680
But just maybe, maybe Mathematica will become sentient.

1146
01:11:43,680 --> 01:11:45,640
Who knows?

1147
01:11:46,160 --> 01:11:48,320
Let me shift our conversation a little bit

1148
01:11:48,320 --> 01:11:49,920
to a little bit different scientist

1149
01:11:49,920 --> 01:11:52,000
with all due respect to Dr. Steven Wolfram,

1150
01:11:52,000 --> 01:11:55,640
whom I do think like you that he's a genius.

1151
01:11:55,640 --> 01:11:58,600
But let me bring in Ray Kurzweil a little bit

1152
01:11:58,600 --> 01:12:02,360
because he's a little bit more pertinent to our conversation.

1153
01:12:02,360 --> 01:12:04,640
I don't know if you qualify Ray as a scholar

1154
01:12:04,640 --> 01:12:09,680
or as a scientist or as a philosopher or an inventor

1155
01:12:09,680 --> 01:12:14,560
or what, but he has made certain projections

1156
01:12:14,560 --> 01:12:18,640
and predictions and he has sort of not been ashamed

1157
01:12:18,640 --> 01:12:21,760
or afraid to popularize them.

1158
01:12:21,760 --> 01:12:24,680
Both with respect to AI and also with respect

1159
01:12:24,680 --> 01:12:28,360
to the future timeline thereof.

1160
01:12:28,360 --> 01:12:33,640
What's your take on sort of AI's Ray Kurzweil's body of work

1161
01:12:33,640 --> 01:12:38,720
and especially his idea of the technological singularity?

1162
01:12:38,720 --> 01:12:42,200
I think that he works on a different incentive

1163
01:12:42,200 --> 01:12:46,080
function than me.

1164
01:12:46,080 --> 01:12:49,320
I feel that Ray is a very smart, capable individual

1165
01:12:49,320 --> 01:12:52,400
that has made amazing contributions to AI.

1166
01:12:52,400 --> 01:12:56,160
And he also understands many of the core ideas of the field

1167
01:12:56,160 --> 01:12:59,000
better than many other practitioners.

1168
01:12:59,000 --> 01:13:05,160
But he is not so much concerned about putting all his cards

1169
01:13:05,160 --> 01:13:08,360
on the table when he makes his predictions.

1170
01:13:08,360 --> 01:13:10,040
There are reasons to make predictions

1171
01:13:10,040 --> 01:13:13,200
when certain things are going to happen for marketing reasons.

1172
01:13:13,200 --> 01:13:15,640
And there are intellectual reasons for doing this.

1173
01:13:15,640 --> 01:13:17,960
And I think that he is too much in a position

1174
01:13:17,960 --> 01:13:21,360
where the marketing reasons play an important role,

1175
01:13:21,360 --> 01:13:23,680
which means I don't understand his true thinking there.

1176
01:13:23,680 --> 01:13:26,920
I don't understand what is the exact argument that would

1177
01:13:26,920 --> 01:13:31,280
compel him to make a prediction with these arrow bars.

1178
01:13:31,280 --> 01:13:34,480
So when I look at the future or at the present or anything,

1179
01:13:34,480 --> 01:13:37,720
I don't know what the truth is when I'm an abetted observer.

1180
01:13:37,720 --> 01:13:39,560
I can only know this for the things

1181
01:13:39,560 --> 01:13:41,240
that I can look from the outside, which

1182
01:13:41,240 --> 01:13:43,240
means stuff that I built myself from scratch,

1183
01:13:43,240 --> 01:13:45,280
and I haven't built the universe by myself.

1184
01:13:45,280 --> 01:13:47,960
So I don't know how it will play out.

1185
01:13:47,960 --> 01:13:50,600
And if I make a prediction about the future,

1186
01:13:50,600 --> 01:13:53,320
I cannot come up with a single number usually.

1187
01:13:53,320 --> 01:13:55,840
What I have is a map of possibilities,

1188
01:13:55,840 --> 01:13:57,680
and then I can shift my confidences around

1189
01:13:57,680 --> 01:14:00,000
and the meta-confidences and the confidences.

1190
01:14:00,000 --> 01:14:03,000
This is about as good as I can do.

1191
01:14:03,000 --> 01:14:05,920
And with respect to AI, the problem is I don't have a spec.

1192
01:14:05,920 --> 01:14:07,560
If I don't have a spec, as a coder,

1193
01:14:07,560 --> 01:14:10,080
I know I don't know when it's done.

1194
01:14:10,080 --> 01:14:12,720
And how many steps, how many milestones

1195
01:14:12,720 --> 01:14:16,320
do I need to cover before I get to this thing?

1196
01:14:16,320 --> 01:14:19,200
I have an idea that, my answer modeling machines,

1197
01:14:19,200 --> 01:14:21,280
I have some ideas of what we are currently

1198
01:14:21,280 --> 01:14:22,680
doing wrong in our modeling.

1199
01:14:22,680 --> 01:14:26,160
Like our models have way too many free parameters right now.

1200
01:14:26,160 --> 01:14:28,320
You want to have a model where, ideally,

1201
01:14:28,320 --> 01:14:30,400
every possible state of the model corresponds

1202
01:14:30,400 --> 01:14:32,600
to one possible world state.

1203
01:14:32,600 --> 01:14:36,360
How annual networks have many magnitudes more possible model

1204
01:14:36,360 --> 01:14:38,120
states than world states, which gives you

1205
01:14:38,120 --> 01:14:40,280
a rise to these adversarial examples

1206
01:14:40,280 --> 01:14:42,480
and all other sorts of things.

1207
01:14:42,480 --> 01:14:43,760
Our models are much tighter.

1208
01:14:43,760 --> 01:14:46,920
The model that our mind has means, at every moment,

1209
01:14:46,920 --> 01:14:51,480
you try to understand the whole of reality.

1210
01:14:51,480 --> 01:14:53,880
Everything you see when somebody shows you a bitmap,

1211
01:14:53,880 --> 01:14:56,680
you don't try to understand this bitmap in isolation

1212
01:14:56,680 --> 01:14:59,800
by throwing it against your model of ImageNet

1213
01:14:59,800 --> 01:15:03,320
that you generated in your mind after looking at many bitmaps.

1214
01:15:03,320 --> 01:15:05,880
Instead, you think somebody is holding up

1215
01:15:05,880 --> 01:15:07,640
a picture with a bitmap on it.

1216
01:15:07,640 --> 01:15:10,120
And that bitmap has been printed by a machine based

1217
01:15:10,120 --> 01:15:12,640
on information taken by a camera, which is another machine

1218
01:15:12,640 --> 01:15:15,040
which was pointed at the window of the universe

1219
01:15:15,040 --> 01:15:18,160
as a different point in time and space.

1220
01:15:18,160 --> 01:15:19,080
This is what you know.

1221
01:15:19,080 --> 01:15:21,320
It's why you make sense of this thing.

1222
01:15:21,320 --> 01:15:23,240
And it's a much more complicated operator

1223
01:15:23,240 --> 01:15:24,960
that you have than our AIs currently

1224
01:15:24,960 --> 01:15:26,960
have and our self-driving cars have.

1225
01:15:26,960 --> 01:15:29,800
Once our cars have the situational awareness,

1226
01:15:29,800 --> 01:15:33,000
there's no way they will not out compete people in all regards.

1227
01:15:33,000 --> 01:15:36,360
But until they have this, there will be many situations

1228
01:15:36,360 --> 01:15:40,760
where people can make inferences that our machines cannot.

1229
01:15:40,760 --> 01:15:42,360
So we can all see these things.

1230
01:15:42,360 --> 01:15:44,000
And Ray can see some of them.

1231
01:15:44,000 --> 01:15:46,280
But Ray doesn't give us a trajectory

1232
01:15:46,280 --> 01:15:48,920
to get to these machines.

1233
01:15:48,920 --> 01:15:51,240
And if I talk to the people in his team,

1234
01:15:51,240 --> 01:15:53,360
they are as smart as they come.

1235
01:15:53,360 --> 01:15:54,080
They're really good.

1236
01:15:54,080 --> 01:15:55,640
They're very educated.

1237
01:15:55,640 --> 01:15:58,240
But I don't think that they see all the things that

1238
01:15:58,240 --> 01:15:59,160
need to be done.

1239
01:15:59,160 --> 01:16:00,760
And it's not because I see more of them,

1240
01:16:00,760 --> 01:16:02,040
but because there are so many things

1241
01:16:02,040 --> 01:16:03,280
that you would need to incorporate.

1242
01:16:03,280 --> 01:16:05,160
I just don't see the milestones.

1243
01:16:05,160 --> 01:16:07,320
I don't see this project.

1244
01:16:07,320 --> 01:16:09,680
And it might also be that I do not completely

1245
01:16:09,680 --> 01:16:12,160
see everything that his team is doing in secret.

1246
01:16:12,160 --> 01:16:15,960
So in a way, you're saying that it's a harder task.

1247
01:16:15,960 --> 01:16:17,040
It's a harder job.

1248
01:16:17,040 --> 01:16:20,880
And the timeline would be longer than his 2045 or 20?

1249
01:16:20,880 --> 01:16:22,240
No, it could also be shorter.

1250
01:16:22,240 --> 01:16:24,160
We don't know this.

1251
01:16:24,160 --> 01:16:29,120
So there are some people which are using so many things.

1252
01:16:29,120 --> 01:16:31,200
Steve Russell, for instance, suggests

1253
01:16:31,200 --> 01:16:35,720
that the last time that somebody said something

1254
01:16:35,720 --> 01:16:38,360
is not possible to somebody having the interesting idea

1255
01:16:38,360 --> 01:16:40,400
that it was fake Theven Arthur Ford said,

1256
01:16:40,400 --> 01:16:45,240
we don't know how to harness nuclear power.

1257
01:16:45,240 --> 01:16:50,720
And Leo Szilard had the core idea that it was like 14 hours.

1258
01:16:50,720 --> 01:16:56,680
And we don't know about AI, whether that is a similar thing.

1259
01:16:56,680 --> 01:17:01,240
It could be that it's only one or two ideas that we actually

1260
01:17:01,240 --> 01:17:04,880
need to have to pull it off or that we need

1261
01:17:04,880 --> 01:17:06,400
to combine in another way.

1262
01:17:06,400 --> 01:17:08,320
But it could also be that we are not

1263
01:17:08,320 --> 01:17:10,240
seeing a few hundred things, right?

1264
01:17:10,240 --> 01:17:13,800
And it takes a long time for us to stumble on the solution.

1265
01:17:13,800 --> 01:17:17,760
So then it's totally unpredictable, in your view.

1266
01:17:17,760 --> 01:17:19,120
It's not totally unpredictable.

1267
01:17:19,120 --> 01:17:21,720
It's just the arrow bar is very large.

1268
01:17:21,720 --> 01:17:24,640
And when I listen to Ray, I don't see him basically

1269
01:17:24,640 --> 01:17:26,040
talking about the arrow bars.

1270
01:17:26,080 --> 01:17:29,800
I see him talking about a possible universe in which he

1271
01:17:29,800 --> 01:17:33,000
can upload himself on a computer before he dies.

1272
01:17:36,240 --> 01:17:37,440
So let me get this right.

1273
01:17:37,440 --> 01:17:40,280
So you say you don't see that?

1274
01:17:40,280 --> 01:17:42,840
In his discussion, I don't see that he puts arrow bars

1275
01:17:42,840 --> 01:17:46,000
on his predictions and explains where the arrow bars comes from.

1276
01:17:46,000 --> 01:17:49,600
What he gives us is a prediction that is compatible with himself

1277
01:17:49,600 --> 01:17:50,720
becoming immortal.

1278
01:17:50,720 --> 01:17:51,680
Right, right, right.

1279
01:17:51,680 --> 01:17:53,240
Yeah, and that may be the bias.

1280
01:17:53,240 --> 01:17:54,320
Yes, yes.

1281
01:17:54,320 --> 01:17:56,320
OK, and I mean, Marvin Minsky said,

1282
01:17:56,320 --> 01:17:58,960
as you point out in your speeches every once in a while,

1283
01:17:58,960 --> 01:18:02,560
that it could happen anywhere from four to 400 years.

1284
01:18:02,560 --> 01:18:07,680
And as you recently noticed, we're still in that timeline.

1285
01:18:07,680 --> 01:18:09,760
Yeah.

1286
01:18:09,760 --> 01:18:13,000
So personally, my hunch is that it's not

1287
01:18:13,000 --> 01:18:14,480
going to be that long.

1288
01:18:14,480 --> 01:18:17,000
My hunch is that it's a lot earlier than people

1289
01:18:17,000 --> 01:18:18,440
would think that it happens.

1290
01:18:18,440 --> 01:18:20,800
But as long as I cannot justify my hunch,

1291
01:18:20,800 --> 01:18:23,000
I cannot put big confidence on it.

1292
01:18:23,000 --> 01:18:26,040
But you're confident it will happen.

1293
01:18:26,040 --> 01:18:28,280
Because there's many skeptics who say,

1294
01:18:28,280 --> 01:18:30,160
we don't know even if it will happen.

1295
01:18:30,160 --> 01:18:33,360
We don't know even if it's possible for a number of reasons.

1296
01:18:33,360 --> 01:18:35,560
Yes, but the question is what confidence

1297
01:18:35,560 --> 01:18:38,360
are they supposed to have, which means what evidence

1298
01:18:38,360 --> 01:18:41,080
can they supply for their claim?

1299
01:18:41,080 --> 01:18:46,880
And if a person has arguments that were pertinent in 2003,

1300
01:18:46,880 --> 01:18:49,720
but are no longer pertinent in 2018

1301
01:18:49,720 --> 01:18:52,080
because our understanding has progressed,

1302
01:18:52,120 --> 01:18:57,280
then the confidence that I derive from their repeated claims

1303
01:18:57,280 --> 01:18:59,680
from 2003 is low.

1304
01:18:59,680 --> 01:19:02,000
It does not change my belief very much.

1305
01:19:02,000 --> 01:19:05,240
A belief of a person is only worth as much as the evidence

1306
01:19:05,240 --> 01:19:08,480
that they built it on, which means most people copy their belief

1307
01:19:08,480 --> 01:19:10,000
from somebody else that they didn't look,

1308
01:19:10,000 --> 01:19:12,560
and they got it from.

1309
01:19:12,560 --> 01:19:15,360
So you can collapse the space of possible beliefs

1310
01:19:15,360 --> 01:19:18,280
into the sources of beliefs, and there are very few of them.

1311
01:19:18,280 --> 01:19:20,880
And then it seems you are thinking,

1312
01:19:20,880 --> 01:19:22,440
definitely we're making progress.

1313
01:19:22,440 --> 01:19:26,440
Therefore, the beliefs against our shrinking,

1314
01:19:26,440 --> 01:19:30,480
the area of beliefs against that possibility of shrinking,

1315
01:19:30,480 --> 01:19:32,440
and the other ones are increasing.

1316
01:19:32,440 --> 01:19:37,000
So the original first phase of AI

1317
01:19:37,000 --> 01:19:39,640
was working by identifying problems

1318
01:19:39,640 --> 01:19:42,320
that require us to be intelligent, like playing chess,

1319
01:19:42,320 --> 01:19:45,520
and then implementing this as an algorithm.

1320
01:19:45,520 --> 01:19:47,720
So it was basically manual engineering

1321
01:19:47,720 --> 01:19:50,840
of strategies for being intelligent in particular domains.

1322
01:19:50,840 --> 01:19:54,400
And this somehow did not scale towards general intelligence,

1323
01:19:54,400 --> 01:19:56,840
one algorithm to do it all.

1324
01:19:56,840 --> 01:20:00,160
And there were subparts of this, like the logistic program,

1325
01:20:00,160 --> 01:20:02,240
the idea to come up with a language

1326
01:20:02,240 --> 01:20:05,560
that allows you to have all possible valid thoughts.

1327
01:20:05,560 --> 01:20:07,000
Same project as Wittgenstein,

1328
01:20:07,000 --> 01:20:11,040
completely preempted most of the work of Minsky, in a way,

1329
01:20:11,040 --> 01:20:13,720
but a couple of decades earlier, and then failed.

1330
01:20:13,720 --> 01:20:15,480
And the philosophers largely didn't understand

1331
01:20:15,480 --> 01:20:18,400
what he was up to because he had to publish this

1332
01:20:18,400 --> 01:20:23,320
in this already dying discipline instead of waiting for AI.

1333
01:20:23,320 --> 01:20:25,400
And the AI people didn't really understand

1334
01:20:25,400 --> 01:20:28,640
that this philosopher was actually trying to do AI

1335
01:20:28,640 --> 01:20:30,080
briefly before church-ensuring,

1336
01:20:30,080 --> 01:20:33,400
already understanding computation.

1337
01:20:33,400 --> 01:20:35,520
He already understood that logic is sufficient

1338
01:20:35,520 --> 01:20:39,320
to build all the possible representational systems,

1339
01:20:39,320 --> 01:20:42,040
and he could also replace all logic with NAND gates.

1340
01:20:42,040 --> 01:20:44,120
He already knew that.

1341
01:20:44,120 --> 01:20:48,240
But it's pretty amazing for a young guy back then.

1342
01:20:48,240 --> 01:20:51,120
Okay, so this first project, a program of AI

1343
01:20:51,120 --> 01:20:52,640
did not accumulate all the way,

1344
01:20:52,640 --> 01:20:54,320
and now we are in the second phase of AI.

1345
01:20:54,320 --> 01:20:56,680
We no longer build these algorithms ourselves,

1346
01:20:56,680 --> 01:20:59,360
we build algorithms that discover the algorithms.

1347
01:20:59,360 --> 01:21:00,200
Right.

1348
01:21:00,200 --> 01:21:02,240
We build learning systems that discover,

1349
01:21:02,240 --> 01:21:03,800
that approximate functions.

1350
01:21:03,800 --> 01:21:05,800
And deep learning has an unfortunate name.

1351
01:21:05,800 --> 01:21:07,200
I think it should be called

1352
01:21:07,200 --> 01:21:09,240
compositional functional approximation.

1353
01:21:11,200 --> 01:21:13,320
This sounds more like a mouseful,

1354
01:21:13,320 --> 01:21:15,840
but it's also more narrow and more accurate.

1355
01:21:16,280 --> 01:21:18,000
It's about this thing that we don't just take

1356
01:21:18,000 --> 01:21:20,880
a single function that we tune to like a regression,

1357
01:21:20,880 --> 01:21:23,040
but that we are able to take many functions

1358
01:21:23,040 --> 01:21:24,640
and put them behind each other

1359
01:21:24,640 --> 01:21:26,880
or into networks of functions.

1360
01:21:26,880 --> 01:21:29,040
So that is the big trick.

1361
01:21:29,040 --> 01:21:32,480
And we can approximate some functions well and not others.

1362
01:21:33,400 --> 01:21:35,080
It could be that there is a third phase

1363
01:21:35,080 --> 01:21:37,160
where we no longer build the algorithms

1364
01:21:37,160 --> 01:21:38,600
that discover the algorithms,

1365
01:21:38,600 --> 01:21:40,040
but we go one step higher.

1366
01:21:40,040 --> 01:21:42,640
We build the algorithms that discover the algorithms,

1367
01:21:42,640 --> 01:21:43,840
that discover the algorithms.

1368
01:21:43,840 --> 01:21:45,320
If you go for meta-learning.

1369
01:21:45,320 --> 01:21:48,200
In a way, our brain maybe is a meta-learning machine,

1370
01:21:48,200 --> 01:21:49,920
not a system that can just learn stuff,

1371
01:21:49,920 --> 01:21:53,800
but then discover how to learn stuff for a new domain.

1372
01:21:53,800 --> 01:21:56,640
Dan Kuni College from the Max Planck Institute

1373
01:21:56,640 --> 01:21:58,640
has this practical poesis idea,

1374
01:21:58,640 --> 01:22:00,800
which is basically learning about,

1375
01:22:00,800 --> 01:22:03,160
learning about learning kind of idea.

1376
01:22:03,160 --> 01:22:05,680
Yeah, but at some point it stops.

1377
01:22:05,680 --> 01:22:06,960
I don't think that you go,

1378
01:22:06,960 --> 01:22:09,040
need to go for more than four degrees.

1379
01:22:09,040 --> 01:22:09,880
Like at some point,

1380
01:22:09,880 --> 01:22:13,360
there's going to be a general theory of search

1381
01:22:13,360 --> 01:22:16,440
that tells you how to get to the global optimum,

1382
01:22:16,440 --> 01:22:18,400
if the global optimum can be gotten to

1383
01:22:18,400 --> 01:22:20,040
and your system is finite resources,

1384
01:22:20,040 --> 01:22:22,640
or basically how to optimize your chances of getting there.

1385
01:22:22,640 --> 01:22:24,520
Once you have that algorithm,

1386
01:22:24,520 --> 01:22:25,600
as a scientist you are done,

1387
01:22:25,600 --> 01:22:27,840
there is no more science that we can do with integrity,

1388
01:22:27,840 --> 01:22:29,480
because it's just going to be the application

1389
01:22:29,480 --> 01:22:30,720
of this algorithm.

1390
01:22:30,720 --> 01:22:31,960
You can only do art then.

1391
01:22:33,960 --> 01:22:35,680
You know, our original,

1392
01:22:35,680 --> 01:22:38,240
we've been talking here for almost 90 minutes.

1393
01:22:38,240 --> 01:22:41,760
So let me sort of hopefully bring our conversation

1394
01:22:41,760 --> 01:22:44,480
to a close here within the next 10 minutes or so,

1395
01:22:44,480 --> 01:22:46,680
by sort of redirecting our attention

1396
01:22:46,680 --> 01:22:49,280
to the original occasion of us getting together,

1397
01:22:49,280 --> 01:22:52,360
which was a brief exchange we had,

1398
01:22:52,360 --> 01:22:56,680
the two of us on Twitter about ethics.

1399
01:22:56,680 --> 01:22:58,040
So let me ask you this,

1400
01:22:58,040 --> 01:23:01,480
where does ethics fit in all of this, or does it?

1401
01:23:05,120 --> 01:23:07,440
I get sometimes frustrated when people think

1402
01:23:07,440 --> 01:23:09,560
that ethics is about being good,

1403
01:23:09,560 --> 01:23:12,400
and being good means to emulate a good person,

1404
01:23:12,400 --> 01:23:16,880
preferably the one who is talking about ethics.

1405
01:23:16,880 --> 01:23:19,360
Did you get frustrated with me on Twitter?

1406
01:23:19,360 --> 01:23:22,360
No, you're a good kid.

1407
01:23:25,760 --> 01:23:28,360
I'm one year younger than you, by the way, so.

1408
01:23:31,360 --> 01:23:33,600
It's not about age, I'm about 12.

1409
01:23:36,600 --> 01:23:37,920
That's right.

1410
01:23:38,760 --> 01:23:42,960
Okay, so ethics, I think, is often misunderstood.

1411
01:23:42,960 --> 01:23:46,680
Ethics emerges when you conceptualize the world

1412
01:23:46,680 --> 01:23:50,880
as different agents, and yourself as one of them,

1413
01:23:50,880 --> 01:23:54,560
and you share purposes with the other agents,

1414
01:23:54,560 --> 01:23:57,000
but you have conflicts of interest.

1415
01:23:57,000 --> 01:23:59,080
If you think that you don't share purposes

1416
01:23:59,080 --> 01:24:01,560
with the other agents, if you're just a lone wolf,

1417
01:24:01,560 --> 01:24:03,560
and the others are your prey,

1418
01:24:03,560 --> 01:24:05,080
there is no reason for ethics, right?

1419
01:24:05,080 --> 01:24:06,840
There's only you look for the consequences

1420
01:24:06,840 --> 01:24:08,240
of your actions for yourself,

1421
01:24:08,240 --> 01:24:11,000
with respect to your own reward functions,

1422
01:24:11,000 --> 01:24:13,400
and that might involve that you have to create

1423
01:24:13,400 --> 01:24:16,160
a civilization of minions or whatever,

1424
01:24:16,160 --> 01:24:18,360
but it's not the same thing as ethics.

1425
01:24:18,360 --> 01:24:20,800
It's not a shared system of negotiation,

1426
01:24:20,800 --> 01:24:23,840
it's only one for you as an individual matter,

1427
01:24:23,840 --> 01:24:26,080
because you don't share that purpose with the others.

1428
01:24:26,080 --> 01:24:26,920
But for instance-

1429
01:24:26,920 --> 01:24:29,760
It may not be shared, but it's your personal ethical framework.

1430
01:24:29,760 --> 01:24:31,080
Oh, it has to be personal.

1431
01:24:31,080 --> 01:24:33,360
For instance, I don't eat meat,

1432
01:24:34,280 --> 01:24:37,640
maybe a legacy or a decision that I made

1433
01:24:37,640 --> 01:24:39,520
when I was 14 years old,

1434
01:24:39,520 --> 01:24:42,640
because back then I felt that I share a purpose

1435
01:24:42,640 --> 01:24:46,080
with animals, that is the avoidance of suffering,

1436
01:24:46,080 --> 01:24:47,560
if it can be helped.

1437
01:24:47,560 --> 01:24:49,480
And I also realized that it's not mutual.

1438
01:24:49,480 --> 01:24:51,400
The animals don't care about my suffering,

1439
01:24:51,400 --> 01:24:54,360
cows largely don't care about that I suffer.

1440
01:24:54,360 --> 01:24:55,600
They don't even conceptualize it,

1441
01:24:55,600 --> 01:24:57,320
they don't think about it a lot.

1442
01:24:57,320 --> 01:24:59,760
I have to think a lot about the suffering of cows,

1443
01:24:59,760 --> 01:25:02,200
they didn't want it to suffer, so I stopped eating meat.

1444
01:25:02,240 --> 01:25:03,960
That was an ethical decision.

1445
01:25:03,960 --> 01:25:07,240
It's a decision about how to resolve a conflict of interest

1446
01:25:07,240 --> 01:25:08,800
under conditions of shared purpose.

1447
01:25:08,800 --> 01:25:10,560
And I think this is what ethics is about.

1448
01:25:10,560 --> 01:25:13,800
It's a rational process in which you negotiate

1449
01:25:13,800 --> 01:25:15,680
with yourself and with others,

1450
01:25:15,680 --> 01:25:17,800
the resolution of conflicts of interest

1451
01:25:17,800 --> 01:25:20,280
under conditions of shared purpose.

1452
01:25:20,280 --> 01:25:23,080
And what purposes I share is in a way a decision,

1453
01:25:23,080 --> 01:25:24,640
and I can make different decisions

1454
01:25:24,640 --> 01:25:26,320
about what purposes we share,

1455
01:25:26,320 --> 01:25:28,240
and some of them are sustainable and others are not,

1456
01:25:28,240 --> 01:25:30,040
so they lead to different outcomes.

1457
01:25:30,040 --> 01:25:31,920
But in the sense, ethics requires

1458
01:25:31,920 --> 01:25:33,400
that you conceptualize yourself

1459
01:25:33,400 --> 01:25:36,000
as something above the organism.

1460
01:25:36,000 --> 01:25:39,160
If you identify the systems of meanings above yourself

1461
01:25:39,160 --> 01:25:40,560
so you can share a purpose,

1462
01:25:41,520 --> 01:25:43,120
love is the discovery of shared purpose.

1463
01:25:43,120 --> 01:25:45,120
There needs to be somebody you love

1464
01:25:45,120 --> 01:25:46,680
that you can be ethical with.

1465
01:25:46,680 --> 01:25:48,720
At some level, you need to love them.

1466
01:25:48,720 --> 01:25:50,640
You need to share a purpose with them.

1467
01:25:50,640 --> 01:25:52,080
And then you negotiate, right?

1468
01:25:52,080 --> 01:25:55,200
You don't want them all to fail in all regards,

1469
01:25:55,200 --> 01:25:56,040
and yourself.

1470
01:25:57,720 --> 01:25:58,880
This is what ethics is about.

1471
01:25:58,880 --> 01:26:00,240
It's computational too.

1472
01:26:00,240 --> 01:26:03,320
Machines can be ethical if they share a purpose with us.

1473
01:26:03,320 --> 01:26:07,080
And what about two other sort of consideration perhaps,

1474
01:26:07,080 --> 01:26:10,160
is that perhaps ethics can be a framework

1475
01:26:10,160 --> 01:26:15,160
within which two entities that do not share interests

1476
01:26:17,920 --> 01:26:22,920
can kind of negotiate in and peacefully coexist

1477
01:26:23,440 --> 01:26:28,440
while still not sharing interests?

1478
01:26:29,080 --> 01:26:30,960
But not interests, but purposes.

1479
01:26:30,960 --> 01:26:32,000
Or purposes.

1480
01:26:32,000 --> 01:26:33,400
If you don't share purposes,

1481
01:26:33,400 --> 01:26:36,200
then you are defecting against your own interest,

1482
01:26:36,200 --> 01:26:38,880
when you don't act on your own interest.

1483
01:26:38,880 --> 01:26:40,520
It doesn't have integrity.

1484
01:26:40,520 --> 01:26:42,640
If somebody is your foot,

1485
01:26:43,560 --> 01:26:46,120
you should, and you don't share a purpose with your foot

1486
01:26:46,120 --> 01:26:48,520
other than you want it to be nice and edible.

1487
01:26:49,720 --> 01:26:50,560
Right?

1488
01:26:50,560 --> 01:26:52,960
You then start giving presence to your foot

1489
01:26:52,960 --> 01:26:54,280
and falling in love with your foot.

1490
01:26:54,280 --> 01:26:55,120
It doesn't end well.

1491
01:26:55,120 --> 01:26:56,560
Look at the little mermaid.

1492
01:26:56,560 --> 01:26:58,840
The little mermaid is a siren.

1493
01:26:58,840 --> 01:26:59,920
Sirens eat people.

1494
01:26:59,920 --> 01:27:01,160
You don't fall in love with your foot.

1495
01:27:01,160 --> 01:27:02,160
It doesn't end well.

1496
01:27:03,920 --> 01:27:06,160
Okay, but me and you are both,

1497
01:27:06,160 --> 01:27:08,320
I don't know if you're vegan or vegetarian.

1498
01:27:08,320 --> 01:27:10,440
Both me and you don't eat meat.

1499
01:27:10,440 --> 01:27:13,880
So we made that choice that perhaps cows

1500
01:27:13,880 --> 01:27:17,400
don't share interests in us.

1501
01:27:17,400 --> 01:27:20,880
We kind of are interested in diminishing their suffering,

1502
01:27:20,880 --> 01:27:23,640
obviously, to make that decision.

1503
01:27:23,640 --> 01:27:27,040
And yet, and they're our food supposedly,

1504
01:27:27,040 --> 01:27:28,560
that's the popular opinion anyway.

1505
01:27:28,560 --> 01:27:30,440
And yet we've made that choice

1506
01:27:31,440 --> 01:27:34,720
to stay away from beef or from meat in general.

1507
01:27:35,840 --> 01:27:40,640
So we can't find a framework with in which

1508
01:27:40,640 --> 01:27:43,120
two entities that don't share interests

1509
01:27:43,120 --> 01:27:44,920
and our purpose is to get a good,

1510
01:27:44,920 --> 01:27:48,000
perhaps peacefully coexist.

1511
01:27:48,000 --> 01:27:50,960
And isn't that the netico framework of its own, right?

1512
01:27:50,960 --> 01:27:51,800
It's more tricky.

1513
01:27:51,800 --> 01:27:52,880
I mean, with the cows,

1514
01:27:52,880 --> 01:27:54,440
the cows largely wouldn't exist

1515
01:27:54,440 --> 01:27:56,440
if people would not eat them.

1516
01:27:56,440 --> 01:27:57,840
You can make the argument that

1517
01:27:59,080 --> 01:28:01,520
pasture living grass-fed cow

1518
01:28:01,520 --> 01:28:04,400
has net positive existence,

1519
01:28:04,400 --> 01:28:06,280
except for the last day, which is horrible,

1520
01:28:06,280 --> 01:28:08,000
but it's horrible for most of us.

1521
01:28:08,000 --> 01:28:08,840
Right.

1522
01:28:08,840 --> 01:28:10,440
And just that.

1523
01:28:10,440 --> 01:28:11,880
Right, so this is one argument

1524
01:28:11,960 --> 01:28:15,080
in favor of eating pasture-fed cows.

1525
01:28:15,080 --> 01:28:15,920
Right.

1526
01:28:15,920 --> 01:28:17,360
Another one is maybe you can manipulate

1527
01:28:17,360 --> 01:28:18,680
the mental states of the cows,

1528
01:28:18,680 --> 01:28:22,080
so even the factory-fed cows are happy.

1529
01:28:23,680 --> 01:28:25,520
Right, so is this unethical?

1530
01:28:25,520 --> 01:28:28,080
It might not look very appetizing to you,

1531
01:28:28,080 --> 01:28:30,600
but then again, maybe people are on the same decision.

1532
01:28:30,600 --> 01:28:33,000
We are a domesticated species.

1533
01:28:33,000 --> 01:28:34,840
This is what humanity is about.

1534
01:28:34,840 --> 01:28:36,640
We give up agency of our own beliefs.

1535
01:28:36,640 --> 01:28:39,680
You get manipulated in finding things bearable

1536
01:28:39,680 --> 01:28:42,400
that look unbearable to a more favorable human being

1537
01:28:42,400 --> 01:28:43,240
like you and me.

1538
01:28:44,280 --> 01:28:46,800
Right, it's a particular kind of domestication

1539
01:28:46,800 --> 01:28:48,840
that didn't take hold on your brain.

1540
01:28:48,840 --> 01:28:51,400
Is this unethical to implement this domestication

1541
01:28:51,400 --> 01:28:56,200
by breeding people or cattle in a particular way?

1542
01:28:56,200 --> 01:28:57,360
It looks repulsive to us,

1543
01:28:57,360 --> 01:28:59,280
but if we really care about the well-being of cattle,

1544
01:28:59,280 --> 01:29:01,800
you and me should probably optimize slaughterhouses

1545
01:29:01,800 --> 01:29:04,920
to make them more humane, to make them more bearable.

1546
01:29:04,920 --> 01:29:06,720
We look away from the slaughterhouses

1547
01:29:06,720 --> 01:29:09,480
because we find them very anesthetic.

1548
01:29:09,480 --> 01:29:11,440
We don't want to have anything to do with this.

1549
01:29:11,440 --> 01:29:13,120
And this is not the most ethical stance

1550
01:29:13,120 --> 01:29:14,640
that we can figure that out.

1551
01:29:14,640 --> 01:29:16,920
So ethics in a way is difficult.

1552
01:29:16,920 --> 01:29:19,880
Of course, that's the key point of ethics.

1553
01:29:19,880 --> 01:29:24,600
And so even it seems to me that ethics requires

1554
01:29:24,600 --> 01:29:26,720
sometimes we take choices

1555
01:29:26,720 --> 01:29:29,640
which are not in our own best self-interest perhaps.

1556
01:29:30,520 --> 01:29:33,200
Depends on what we define of our self.

1557
01:29:33,200 --> 01:29:36,040
The self, we could say this is identical

1558
01:29:36,040 --> 01:29:37,840
to the well-being of the organism,

1559
01:29:37,840 --> 01:29:40,080
but this is a very short-sighted perspective, right?

1560
01:29:40,080 --> 01:29:43,800
I don't actually identify all the way with my organism.

1561
01:29:43,800 --> 01:29:46,240
There are other things I identify with society,

1562
01:29:46,240 --> 01:29:48,560
I identify with my kids, with my relationships,

1563
01:29:48,560 --> 01:29:50,640
with my friends, their well-being.

1564
01:29:50,640 --> 01:29:53,000
So I am all the things that I identify with

1565
01:29:53,000 --> 01:29:55,440
that I want to regulate in a particular way.

1566
01:29:55,440 --> 01:29:58,240
And my children are objectively more important than me, right?

1567
01:29:58,240 --> 01:30:00,480
If they have the choice to make my kids survive

1568
01:30:00,480 --> 01:30:02,520
or myself, my kids should survive.

1569
01:30:02,520 --> 01:30:05,840
This is as it should be if nature has wired me up correctly.

1570
01:30:05,840 --> 01:30:07,000
You can change the wiring,

1571
01:30:07,000 --> 01:30:09,840
but this is also the weird thing about ethics.

1572
01:30:09,840 --> 01:30:11,800
Ethics becomes very tricky to discuss

1573
01:30:11,800 --> 01:30:15,000
once the reward function becomes mutable.

1574
01:30:15,000 --> 01:30:17,120
When you're able to change what is important to you,

1575
01:30:17,120 --> 01:30:19,440
what you care about, how do you define ethics?

1576
01:30:21,440 --> 01:30:22,280
Me?

1577
01:30:24,000 --> 01:30:25,760
So, or anyone?

1578
01:30:26,720 --> 01:30:30,800
I would say to me, let me be careful about this.

1579
01:30:31,640 --> 01:30:35,240
Well, I would say it's basically,

1580
01:30:35,240 --> 01:30:38,320
you can call it even a code of conduct

1581
01:30:38,320 --> 01:30:43,320
or a set of principles and rules

1582
01:30:43,520 --> 01:30:45,360
that guide my behavior

1583
01:30:47,680 --> 01:30:52,680
to accomplish certain kinds of outcomes.

1584
01:30:54,680 --> 01:30:56,840
There are no beliefs without priors.

1585
01:30:56,840 --> 01:31:01,480
What are the priors that you base your code of conduct on?

1586
01:31:01,480 --> 01:31:03,400
Yes, that's a very good question.

1587
01:31:03,400 --> 01:31:05,000
And it puts me on the spot here

1588
01:31:05,000 --> 01:31:07,560
and I'm not prepared for it, but I have to follow.

1589
01:31:08,640 --> 01:31:13,640
So the priors are, you can call them axioms perhaps,

1590
01:31:16,680 --> 01:31:18,840
things like diminishing suffering.

1591
01:31:20,160 --> 01:31:22,920
Things like, for example,

1592
01:31:22,920 --> 01:31:27,920
and perhaps one of those rules or points of view or tools,

1593
01:31:29,120 --> 01:31:32,840
if you will, is taking sort of what Peter Singer calls

1594
01:31:32,840 --> 01:31:35,960
the universe points of view, point of view,

1595
01:31:35,960 --> 01:31:40,600
or sort of an outside point of view than my own, right?

1596
01:31:40,600 --> 01:31:44,480
So when it comes to, with respect to cows,

1597
01:31:44,480 --> 01:31:48,520
I take a point of view outside of me and the cows, hopefully.

1598
01:31:48,520 --> 01:31:53,320
And sort of I'm able to look at my suffering

1599
01:31:53,320 --> 01:31:58,120
of not eating a cow and their suffering of being eaten, right?

1600
01:31:58,120 --> 01:32:02,680
So if my prior is minimize suffering,

1601
01:32:03,560 --> 01:32:08,560
because basically that's the axiom based on which I can deduce

1602
01:32:09,040 --> 01:32:13,080
that something or someone exists like an even entity,

1603
01:32:13,080 --> 01:32:15,800
like a sentient being, right?

1604
01:32:15,800 --> 01:32:17,560
Is the suffering, does it suffer?

1605
01:32:17,560 --> 01:32:19,800
That's sort of my test, if you will,

1606
01:32:19,800 --> 01:32:24,120
not during test, but a test of being a sentient being.

1607
01:32:24,120 --> 01:32:26,120
Can you suffer? Can it suffer?

1608
01:32:26,120 --> 01:32:29,200
And if it can suffer, then my principle

1609
01:32:29,200 --> 01:32:33,440
of minimizing suffering must be the guiding principle

1610
01:32:33,440 --> 01:32:36,680
with which I relate to it.

1611
01:32:37,800 --> 01:32:39,840
That's kind of like, if you will,

1612
01:32:39,840 --> 01:32:43,080
sort of the foundation of my personal ethics.

1613
01:32:43,080 --> 01:32:44,600
Can it suffer?

1614
01:32:44,600 --> 01:32:46,680
Then the next is how can I minimize

1615
01:32:46,680 --> 01:32:48,760
the suffering of that entity?

1616
01:32:48,760 --> 01:32:53,280
And then basically everything else builds up from there, right?

1617
01:32:53,280 --> 01:32:54,440
When you become an adult,

1618
01:32:54,440 --> 01:32:56,800
I think the most important part of it

1619
01:32:56,800 --> 01:32:59,520
is that you take charge of your own emotions.

1620
01:32:59,520 --> 01:33:01,840
You realize that your own emotions are generated

1621
01:33:01,840 --> 01:33:04,320
by your own brain, by your own organism,

1622
01:33:04,320 --> 01:33:05,680
and they're here to serve you,

1623
01:33:05,680 --> 01:33:07,880
and you're here to serve your emotions.

1624
01:33:07,880 --> 01:33:09,840
The emotions are there to help you

1625
01:33:09,840 --> 01:33:13,200
for doing the things that you consider to be the right thing.

1626
01:33:13,200 --> 01:33:16,120
And that means that you need to be able to control them,

1627
01:33:16,120 --> 01:33:17,440
to have integrity.

1628
01:33:17,440 --> 01:33:19,560
If you are just the victim of your emotions

1629
01:33:19,560 --> 01:33:22,400
and not do the things that are the right thing,

1630
01:33:22,400 --> 01:33:24,680
you learn that you can control your emotions

1631
01:33:24,680 --> 01:33:26,120
and deal with them, right?

1632
01:33:26,120 --> 01:33:28,520
You don't have integrity.

1633
01:33:28,520 --> 01:33:30,200
And what is suffering?

1634
01:33:30,200 --> 01:33:33,160
Pain is the result of some part of your brain

1635
01:33:33,160 --> 01:33:35,400
sending a teaching signal to another part of your brain

1636
01:33:35,400 --> 01:33:37,320
to improve its performance.

1637
01:33:37,320 --> 01:33:39,200
If the regulation is not correct

1638
01:33:39,200 --> 01:33:42,480
because you cannot actually regulate that particular thing,

1639
01:33:42,480 --> 01:33:44,800
then the pain will endure and usually get cranked up

1640
01:33:44,800 --> 01:33:46,120
until your brain figures it out

1641
01:33:46,120 --> 01:33:49,080
in terms of the pain signaling center.

1642
01:33:49,080 --> 01:33:51,800
But by telling them, actually, you're not helping here, right?

1643
01:33:51,800 --> 01:33:53,440
Until you get to this point, you have suffering.

1644
01:33:53,440 --> 01:33:56,560
You have increased pain that you cannot resolve.

1645
01:33:56,560 --> 01:34:01,040
And so in this sense, suffering is a lack of integrity.

1646
01:34:01,040 --> 01:34:03,720
The difficulty is only that many beings cannot get

1647
01:34:03,720 --> 01:34:06,400
to the degree of integrity where they can control

1648
01:34:06,400 --> 01:34:08,960
the application of learning signals in their brain,

1649
01:34:08,960 --> 01:34:11,120
that they can control the way the reward function

1650
01:34:11,120 --> 01:34:13,240
is being computed and distributed.

1651
01:34:13,240 --> 01:34:16,320
So then according to your argument,

1652
01:34:16,320 --> 01:34:18,840
suffering is just like you said before,

1653
01:34:18,840 --> 01:34:21,840
a simulation or a part of a simulation then.

1654
01:34:21,840 --> 01:34:24,120
Well, everything that we experience is a simulation.

1655
01:34:24,120 --> 01:34:25,160
We are a simulation.

1656
01:34:25,160 --> 01:34:26,960
But to us, of course, it feels real.

1657
01:34:26,960 --> 01:34:29,040
There is no helping around this.

1658
01:34:29,040 --> 01:34:32,760
But what I have learned in the course of my life

1659
01:34:32,760 --> 01:34:36,720
is that all of my suffering is a result of not being awake.

1660
01:34:37,720 --> 01:34:39,800
Once I wake up, I realize what's going on.

1661
01:34:39,800 --> 01:34:41,520
I realize that I am in mind.

1662
01:34:41,560 --> 01:34:44,120
The relevance of the signals that I perceive

1663
01:34:44,120 --> 01:34:45,840
is completely up to the mind.

1664
01:34:47,760 --> 01:34:49,320
Because the universe does not give me

1665
01:34:49,320 --> 01:34:50,680
objectively good or bad things.

1666
01:34:50,680 --> 01:34:52,840
The universe gives me a bunch of electrical impulses

1667
01:34:52,840 --> 01:34:56,040
that manifest in my tannamos

1668
01:34:56,040 --> 01:34:57,720
and my brain makes sense of them

1669
01:34:57,720 --> 01:34:59,600
by creating a simulated world.

1670
01:34:59,600 --> 01:35:01,360
And the valence in that simulated world

1671
01:35:01,360 --> 01:35:03,120
is completely internal.

1672
01:35:03,120 --> 01:35:04,560
It's completely part of that world.

1673
01:35:04,560 --> 01:35:05,640
It's not objective.

1674
01:35:05,640 --> 01:35:08,000
And so I can control this.

1675
01:35:08,200 --> 01:35:12,160
So ethics or suffering is a subjective experience.

1676
01:35:12,160 --> 01:35:15,840
And if I'm basing my ethics on suffering,

1677
01:35:15,840 --> 01:35:17,920
therefore my ethics would be subjective.

1678
01:35:17,920 --> 01:35:19,560
Is that what you're saying?

1679
01:35:19,560 --> 01:35:22,480
No, I think that suffering is real with respect to the self.

1680
01:35:22,480 --> 01:35:24,080
But it's not immutable.

1681
01:35:24,080 --> 01:35:26,400
So you can change the definition of yourself,

1682
01:35:26,400 --> 01:35:28,320
the things that you identify with.

1683
01:35:28,320 --> 01:35:30,560
Imagine there is a certain condition in the world

1684
01:35:30,560 --> 01:35:33,200
where you think a particular party needs to be in power,

1685
01:35:33,200 --> 01:35:35,280
an order for the world to be good.

1686
01:35:35,280 --> 01:35:37,880
And if that party is not in power, you suffer.

1687
01:35:37,880 --> 01:35:39,120
You can give up that belief

1688
01:35:39,120 --> 01:35:41,200
and you realize how politics actually works

1689
01:35:41,200 --> 01:35:43,440
and that there's a fitness function going on

1690
01:35:43,440 --> 01:35:46,040
and that people behave according to what they read

1691
01:35:46,040 --> 01:35:46,960
and whatever.

1692
01:35:46,960 --> 01:35:48,360
And you realize that this is the case

1693
01:35:48,360 --> 01:35:50,120
and you just give up on suffering about it

1694
01:35:50,120 --> 01:35:51,200
because you realize you are looking

1695
01:35:51,200 --> 01:35:52,880
at a mechanical process

1696
01:35:52,880 --> 01:35:54,840
and it plays out anyway regardless

1697
01:35:54,840 --> 01:35:56,920
of what you feel about how that plays out, right?

1698
01:35:56,920 --> 01:35:59,000
So you give up that suffering.

1699
01:35:59,000 --> 01:36:00,640
Or if you are a preschool teacher

1700
01:36:00,640 --> 01:36:03,680
and the kids are misbehaving and they are mean to you,

1701
01:36:03,680 --> 01:36:05,400
at some point you stop suffering about this

1702
01:36:05,400 --> 01:36:07,440
because you see what they actually do.

1703
01:36:07,480 --> 01:36:09,200
It's not personal, right?

1704
01:36:09,200 --> 01:36:11,600
That's what Stoic philosophy is all about, right?

1705
01:36:11,600 --> 01:36:14,160
Stoics say there is no point.

1706
01:36:14,160 --> 01:36:15,200
So first of all,

1707
01:36:17,360 --> 01:36:21,760
Stoics say that we suffer not from events

1708
01:36:21,760 --> 01:36:23,480
or things that happen in our life

1709
01:36:23,480 --> 01:36:26,320
but from the stories that we attach to them.

1710
01:36:27,160 --> 01:36:29,200
And therefore, if we change the story,

1711
01:36:29,200 --> 01:36:31,800
we can change the way we feel about them

1712
01:36:31,800 --> 01:36:34,120
and thereby remove the suffering.

1713
01:36:34,120 --> 01:36:36,560
And they say that there's the only thing

1714
01:36:36,560 --> 01:36:38,640
that we can focus on and do something

1715
01:36:38,640 --> 01:36:41,400
about is our own thoughts

1716
01:36:41,400 --> 01:36:43,560
and things like the kids in school

1717
01:36:43,560 --> 01:36:44,800
or the party are things

1718
01:36:44,800 --> 01:36:47,440
that are completely outside of our control.

1719
01:36:47,440 --> 01:36:49,080
And therefore, there is no point

1720
01:36:49,080 --> 01:36:51,080
to get aggravated about them.

1721
01:36:51,080 --> 01:36:53,120
And there's very little things

1722
01:36:53,120 --> 01:36:54,920
that are completely under our control.

1723
01:36:54,920 --> 01:36:57,760
So we can't really control fully our body.

1724
01:36:57,760 --> 01:37:01,280
We can't really control our health completely.

1725
01:37:01,280 --> 01:37:02,800
Things can always go wrong there.

1726
01:37:02,800 --> 01:37:04,720
The only thing they say you can fully,

1727
01:37:04,720 --> 01:37:07,040
completely control is your thoughts.

1728
01:37:07,880 --> 01:37:11,520
And that's where your freedom comes to be

1729
01:37:11,520 --> 01:37:15,040
and that's where your power comes to be

1730
01:37:15,040 --> 01:37:18,920
and that's where you're the one and only, right?

1731
01:37:18,920 --> 01:37:21,800
In that mind, in that simulation, you're the God.

1732
01:37:22,800 --> 01:37:25,440
So this ability to make your thoughts more truthful,

1733
01:37:25,440 --> 01:37:27,400
this is Western enlightenment in a way,

1734
01:37:27,400 --> 01:37:29,680
this is Aufklärung in German.

1735
01:37:29,680 --> 01:37:31,160
And there is also this other sense

1736
01:37:31,160 --> 01:37:33,040
of enlightenment, Erleuchtung,

1737
01:37:33,040 --> 01:37:35,240
that you have in a spiritual context.

1738
01:37:35,240 --> 01:37:37,800
And so Aufklärung fixes your rationality

1739
01:37:37,800 --> 01:37:40,560
and Erleuchtung fixes your motivation.

1740
01:37:40,560 --> 01:37:42,040
It fixes what's relevant to you

1741
01:37:42,040 --> 01:37:43,040
and how we relate to this.

1742
01:37:43,040 --> 01:37:46,280
It fixes the relationship between self and universe.

1743
01:37:46,280 --> 01:37:48,760
And often they are seen as mutually exclusive

1744
01:37:48,760 --> 01:37:52,600
in the sense that Aufklärung leads to nihilism

1745
01:37:52,600 --> 01:37:54,440
because you don't give up your need for meaning.

1746
01:37:54,440 --> 01:37:56,520
You just prove that it cannot be satisfied.

1747
01:37:56,520 --> 01:37:59,720
God does not exist in any way that can set you free.

1748
01:37:59,760 --> 01:38:01,440
And in this other sense,

1749
01:38:01,440 --> 01:38:03,360
you give up your understanding

1750
01:38:03,360 --> 01:38:06,000
of how the world actually works so you can be happy.

1751
01:38:07,120 --> 01:38:09,840
You go to a non-dual state where you represent

1752
01:38:09,840 --> 01:38:12,360
that all people share the same cosmic consciousness,

1753
01:38:12,360 --> 01:38:13,680
which is complete bullshit, right?

1754
01:38:13,680 --> 01:38:17,840
But it's something that removes the illusion of separation

1755
01:38:17,840 --> 01:38:19,040
and there are suffering that comes

1756
01:38:19,040 --> 01:38:20,600
with the separation and so on.

1757
01:38:22,200 --> 01:38:23,760
So where is that...

1758
01:38:23,760 --> 01:38:25,320
Yeah, sustainable.

1759
01:38:25,320 --> 01:38:27,840
Where does that leave us with respect to ethics though?

1760
01:38:28,080 --> 01:38:31,640
So maybe you were able to dismantle much or most

1761
01:38:31,640 --> 01:38:34,120
or maybe all of my ethics, did you?

1762
01:38:36,280 --> 01:38:38,360
I don't know all of your ethics, but...

1763
01:38:38,360 --> 01:38:40,960
Well, if you asked me for the foundation

1764
01:38:40,960 --> 01:38:45,440
and the best I could come up with the sort of the suffering test.

1765
01:38:45,440 --> 01:38:47,000
Yeah, it's not good.

1766
01:38:47,000 --> 01:38:49,520
The problem is really that if I can turn off suffering

1767
01:38:51,200 --> 01:38:52,920
or if I get counter-intuitive results,

1768
01:38:52,920 --> 01:38:55,400
there's this anti-natalism.

1769
01:38:55,760 --> 01:38:58,680
Anti-natalism is an obvious way to end suffering, right?

1770
01:38:58,680 --> 01:39:01,560
Stop putting new organisms into the world

1771
01:39:01,560 --> 01:39:04,120
and the existing set of organisms

1772
01:39:04,120 --> 01:39:07,280
in the least painful way possible, right?

1773
01:39:07,280 --> 01:39:08,800
AI could help with this.

1774
01:39:08,800 --> 01:39:10,880
The question is, can we make it safe

1775
01:39:10,880 --> 01:39:13,360
or is the AI going to leave a couple cells left

1776
01:39:13,360 --> 01:39:15,560
that can give rise to new suffering later on?

1777
01:39:17,280 --> 01:39:20,880
But so if you have a completely cold and dead universe,

1778
01:39:20,880 --> 01:39:22,360
then there'll be no suffering, right?

1779
01:39:22,360 --> 01:39:23,200
Yes.

1780
01:39:24,160 --> 01:39:25,280
Is this what you want?

1781
01:39:25,280 --> 01:39:26,200
Right, clearly.

1782
01:39:26,200 --> 01:39:27,840
So that's not the most...

1783
01:39:27,840 --> 01:39:28,680
According to...

1784
01:39:28,680 --> 01:39:29,520
It's not so clear.

1785
01:39:29,520 --> 01:39:30,960
I'm anti-natalist, but my kids are not.

1786
01:39:30,960 --> 01:39:34,080
So I have this division there.

1787
01:39:34,080 --> 01:39:35,400
But...

1788
01:39:35,400 --> 01:39:38,720
So what's that say about where are you coming from then

1789
01:39:38,720 --> 01:39:39,800
with respect to ethics?

1790
01:39:39,800 --> 01:39:42,520
So let's say my suffering test is not good enough.

1791
01:39:42,520 --> 01:39:45,440
I think existence by itself is neutral.

1792
01:39:45,440 --> 01:39:48,400
The reason why there are so few stoics around.

1793
01:39:48,400 --> 01:39:49,240
Have you thought about this?

1794
01:39:49,240 --> 01:39:51,080
Stoicism has been discovered a long time ago.

1795
01:39:51,080 --> 01:39:52,320
Almost nobody's a stoic.

1796
01:39:52,320 --> 01:39:53,160
How is that?

1797
01:39:54,160 --> 01:39:57,320
Well, I know a few people who are stoics, actually.

1798
01:39:57,320 --> 01:39:59,520
Yeah, but the majority is not.

1799
01:39:59,520 --> 01:40:01,240
Well, it seems to be so obvious.

1800
01:40:01,240 --> 01:40:03,320
Only worry about the things that you can actually change

1801
01:40:03,320 --> 01:40:06,320
to the degree that the very helps you changing them.

1802
01:40:06,320 --> 01:40:07,160
Yes, so...

1803
01:40:08,440 --> 01:40:10,480
So why is nobody a stoic?

1804
01:40:10,480 --> 01:40:11,320
Almost nobody.

1805
01:40:12,400 --> 01:40:13,960
Well, I wouldn't say nobody.

1806
01:40:13,960 --> 01:40:17,320
I'd say a few people are stoic and they're amazing

1807
01:40:17,320 --> 01:40:20,760
and they're inspirational and they're motivational

1808
01:40:20,920 --> 01:40:25,320
and they're good role model for sort of like

1809
01:40:25,320 --> 01:40:27,560
how I want to behave and how I want to live

1810
01:40:27,560 --> 01:40:29,520
and how I want to act in this world.

1811
01:40:29,520 --> 01:40:31,840
I suspect that stoicism is maladaptive

1812
01:40:31,840 --> 01:40:33,920
from a permanent evolutionary perspective.

1813
01:40:35,120 --> 01:40:37,280
Most cats I have known are stoics,

1814
01:40:37,280 --> 01:40:39,200
which means if you leave them alone, they're fine.

1815
01:40:39,200 --> 01:40:40,800
Like their baseline state is okay.

1816
01:40:40,800 --> 01:40:43,480
They're okay with themselves and their place in the universe

1817
01:40:43,480 --> 01:40:45,440
and they just stay at that place.

1818
01:40:45,440 --> 01:40:46,760
And only when you disturb that

1819
01:40:46,760 --> 01:40:48,440
because they need to use the bathroom

1820
01:40:48,440 --> 01:40:52,840
or because they are hungry or they want to play or whatever,

1821
01:40:52,840 --> 01:40:54,760
this equilibrium gets disturbed

1822
01:40:54,760 --> 01:40:56,880
and they do exactly what's necessary

1823
01:40:56,880 --> 01:40:58,720
to get back to the equilibrium state

1824
01:40:58,720 --> 01:41:00,400
and then they're fine again.

1825
01:41:00,400 --> 01:41:03,320
And a human being is slightly different.

1826
01:41:03,320 --> 01:41:06,240
A healthy human being is set up in such a way

1827
01:41:06,240 --> 01:41:07,920
that when they wake up in the morning,

1828
01:41:07,920 --> 01:41:09,960
they're not completely fine.

1829
01:41:09,960 --> 01:41:11,960
And then they need to be busy during the day,

1830
01:41:11,960 --> 01:41:14,160
but in the evening, they're fine.

1831
01:41:14,160 --> 01:41:15,440
In the evening, it's done enough

1832
01:41:15,440 --> 01:41:18,000
to make peace this existence again

1833
01:41:18,000 --> 01:41:19,920
and then they can have a beer with their friends

1834
01:41:19,920 --> 01:41:21,760
and everything is good.

1835
01:41:21,760 --> 01:41:24,320
And then there are some individuals

1836
01:41:24,320 --> 01:41:28,080
which have so much discontent with them themselves.

1837
01:41:28,080 --> 01:41:31,360
Like the human is the animal that is discontent

1838
01:41:31,360 --> 01:41:35,000
that they cannot take care of this in a single day.

1839
01:41:35,000 --> 01:41:38,080
But even after several weeks of sustained work,

1840
01:41:38,080 --> 01:41:40,320
they are still in a state where it's not good enough

1841
01:41:40,320 --> 01:41:42,240
and only when they have this amazing thing

1842
01:41:42,240 --> 01:41:43,520
where they get their noble price,

1843
01:41:43,520 --> 01:41:45,480
they're fine for like half a day.

1844
01:41:46,320 --> 01:41:49,240
And the way this is the way we are set up

1845
01:41:49,240 --> 01:41:50,320
to different degrees.

1846
01:41:50,320 --> 01:41:52,160
And from an evolutionary perspective,

1847
01:41:52,160 --> 01:41:53,960
you can totally see why that would be useful

1848
01:41:53,960 --> 01:41:55,720
for a group species.

1849
01:41:55,720 --> 01:41:58,360
For an individual species that is not so much a group species

1850
01:41:58,360 --> 01:42:00,880
like cats are not really meant for groups.

1851
01:42:00,880 --> 01:42:02,960
They're very much singletons.

1852
01:42:02,960 --> 01:42:05,360
For them, it's rational to be historic.

1853
01:42:05,360 --> 01:42:06,600
But if you're a group animal,

1854
01:42:06,600 --> 01:42:08,880
it makes sense that the well-being of the individual

1855
01:42:08,880 --> 01:42:11,200
is sacrificed for the well-being of the group.

1856
01:42:11,200 --> 01:42:14,080
So each individual is overextending themselves

1857
01:42:14,080 --> 01:42:15,400
to make the group more successful

1858
01:42:15,400 --> 01:42:17,440
and produce a surplus of resources for the group

1859
01:42:17,440 --> 01:42:18,440
as a result.

1860
01:42:18,440 --> 01:42:21,000
Right, but evolution also diversifies things

1861
01:42:21,000 --> 01:42:25,200
so that if one kind of feature becomes maladaptive

1862
01:42:25,200 --> 01:42:27,600
in a new environmental change,

1863
01:42:27,600 --> 01:42:30,840
then a diverse part of that population

1864
01:42:30,840 --> 01:42:32,000
would be more adaptive.

1865
01:42:32,000 --> 01:42:35,280
And so that's why evolution sort of hedges its bets

1866
01:42:35,280 --> 01:42:39,200
with the greatest variety and diversity possible, right?

1867
01:42:39,200 --> 01:42:41,800
So yeah, there will be some people who would be like that

1868
01:42:41,800 --> 01:42:45,440
and some people who would be like otherwise.

1869
01:42:45,440 --> 01:42:49,560
And this way, on the whole,

1870
01:42:49,560 --> 01:42:53,080
they're evolutionarily most adaptive.

1871
01:42:53,080 --> 01:42:56,080
But some will be more adaptive to one kind of situation

1872
01:42:56,080 --> 01:42:57,480
and others will be more adaptive

1873
01:42:57,480 --> 01:42:59,400
to other kinds of situation.

1874
01:42:59,400 --> 01:43:00,680
I'm not sure if this is true.

1875
01:43:00,680 --> 01:43:02,880
So for instance, we find that larger habitats

1876
01:43:02,880 --> 01:43:05,040
don't necessarily have more species.

1877
01:43:05,040 --> 01:43:08,640
And that's because there's a fearsome competition,

1878
01:43:08,640 --> 01:43:10,880
which means that there's less slack in the evolution.

1879
01:43:10,880 --> 01:43:12,920
So for instance, New Zealand had a lot of species

1880
01:43:12,920 --> 01:43:16,160
before there was immigration of other species

1881
01:43:16,160 --> 01:43:19,000
and they obliterated most of the stuff that existed,

1882
01:43:19,000 --> 01:43:21,240
mostly because the stuff that came in

1883
01:43:21,240 --> 01:43:23,200
was result of a much fiercer competition

1884
01:43:23,200 --> 01:43:25,200
than existed in small New Zealand.

1885
01:43:25,200 --> 01:43:26,200
Sure, yeah.

1886
01:43:26,200 --> 01:43:28,880
And in a way, the same thing happens now.

1887
01:43:28,880 --> 01:43:30,520
We are the result of evolution.

1888
01:43:30,520 --> 01:43:32,520
We are, as Minsky said, evolution's way

1889
01:43:32,520 --> 01:43:37,520
to put the airplanes into the sky

1890
01:43:38,240 --> 01:43:42,600
and make these clouds that the airplanes make.

1891
01:43:42,600 --> 01:43:46,000
And we reduce the number of species dramatically.

1892
01:43:46,000 --> 01:43:50,160
We are like probably eventually going to look like a meteor

1893
01:43:50,160 --> 01:43:52,480
that is going to obliterate a large part of the species

1894
01:43:52,480 --> 01:43:53,320
on this planet.

1895
01:43:54,920 --> 01:43:59,640
So what's that say about ethics and technology?

1896
01:43:59,640 --> 01:44:02,120
So what's the solution then?

1897
01:44:02,120 --> 01:44:05,240
So is there space for ethics and technology?

1898
01:44:05,240 --> 01:44:06,840
Of course there is.

1899
01:44:06,840 --> 01:44:09,280
It's about discovering the long game, right?

1900
01:44:09,280 --> 01:44:11,880
So when you do something, you have short influences

1901
01:44:11,880 --> 01:44:13,680
and you have long influences.

1902
01:44:13,680 --> 01:44:16,040
And based on what you think is the right thing to do,

1903
01:44:16,040 --> 01:44:18,160
you need to look at the long-term influences.

1904
01:44:18,160 --> 01:44:19,960
But you also need to question why you think

1905
01:44:19,960 --> 01:44:21,640
that something is the right thing to do,

1906
01:44:21,640 --> 01:44:23,960
what the results of that are, which gets tricky.

1907
01:44:23,960 --> 01:44:26,240
But we can agree on that, that's fantastic.

1908
01:44:26,240 --> 01:44:29,080
But tell me then, how do you define ethics yourself?

1909
01:44:30,200 --> 01:44:34,400
Well, the tension between the way I define ethics

1910
01:44:34,400 --> 01:44:39,040
and some other people in AI, and ethics in AI define it is,

1911
01:44:39,040 --> 01:44:40,760
there are some people which think that ethics

1912
01:44:40,760 --> 01:44:42,480
is a way for politically savvy people

1913
01:44:42,480 --> 01:44:44,080
to get power over STEM people.

1914
01:44:46,400 --> 01:44:49,360
And with considerable success,

1915
01:44:49,360 --> 01:44:51,000
it's largely a protection racket.

1916
01:44:51,960 --> 01:44:55,000
There's also a way that ethics happens where you have studies

1917
01:44:55,000 --> 01:44:56,600
where somebody asks a million people

1918
01:44:56,600 --> 01:44:59,840
of whether a traffic car should run over young people

1919
01:44:59,840 --> 01:45:01,800
or old people first.

1920
01:45:01,800 --> 01:45:03,240
And then they publish the results

1921
01:45:03,240 --> 01:45:06,920
and it makes a big splash because people can relate to this.

1922
01:45:06,920 --> 01:45:08,200
But it's...

1923
01:45:08,200 --> 01:45:10,160
Well, this is ethics, we can do that.

1924
01:45:10,160 --> 01:45:12,200
This has just happened so that philosophers

1925
01:45:12,200 --> 01:45:13,240
had this trolley problem,

1926
01:45:13,240 --> 01:45:14,640
and suddenly they see an application.

1927
01:45:14,640 --> 01:45:16,240
But it's largely the same thing as saying

1928
01:45:16,240 --> 01:45:19,880
that the majority of people would want a minor tawry

1929
01:45:19,880 --> 01:45:23,360
to be confined in labyrinths rather than in public forests.

1930
01:45:23,360 --> 01:45:24,200
Right.

1931
01:45:24,200 --> 01:45:29,680
That is the situation that the gods were in

1932
01:45:29,680 --> 01:45:31,120
or the Cretan king was in

1933
01:45:31,200 --> 01:45:33,560
when this sign turned out to be a minor tawry.

1934
01:45:33,560 --> 01:45:35,320
But it rarely happens.

1935
01:45:35,320 --> 01:45:36,160
Right.

1936
01:45:36,160 --> 01:45:37,000
In the same sense, it rarely happens

1937
01:45:37,000 --> 01:45:39,600
that a self-driving car will have to make that decision.

1938
01:45:39,600 --> 01:45:43,640
Probably not often enough to require an if-then in its code.

1939
01:45:45,160 --> 01:45:48,560
But how do you define ethics for yourself?

1940
01:45:48,560 --> 01:45:49,480
What is ethics?

1941
01:45:49,480 --> 01:45:51,640
Because you asked me this and I gave my best to answer.

1942
01:45:51,640 --> 01:45:53,440
Oh, so I also try to do this.

1943
01:45:53,440 --> 01:45:55,920
My best answer is that ethics is the principle

1944
01:45:55,920 --> 01:45:58,200
negotiation of conflicts of interest

1945
01:45:58,200 --> 01:46:00,720
under conditions of shared purpose.

1946
01:46:00,720 --> 01:46:03,600
If I share purposes with others, with society,

1947
01:46:03,600 --> 01:46:05,560
with other beings, with conscious beings,

1948
01:46:05,560 --> 01:46:08,520
and that's my decision based on the way my mind is set up

1949
01:46:08,520 --> 01:46:12,680
right now, and I run into conflicts of interest with them.

1950
01:46:12,680 --> 01:46:14,120
I have to deal with this.

1951
01:46:14,120 --> 01:46:16,160
For instance, when I look at other people,

1952
01:46:16,160 --> 01:46:20,800
I mostly imagine myself as being them

1953
01:46:20,800 --> 01:46:22,760
in a different timeline.

1954
01:46:22,760 --> 01:46:24,960
Everybody is in a way, me in a different timeline,

1955
01:46:24,960 --> 01:46:27,120
but in order to understand who they are,

1956
01:46:27,120 --> 01:46:28,520
I need to flip a number of bits.

1957
01:46:28,520 --> 01:46:32,240
So I think about which bits would I need to flip in my mind

1958
01:46:32,240 --> 01:46:33,840
to be you.

1959
01:46:33,840 --> 01:46:34,480
Right.

1960
01:46:34,480 --> 01:46:37,640
And these are the conditions of negotiation that I have with you.

1961
01:46:37,640 --> 01:46:40,760
So we can agree on that, perhaps, on that definition,

1962
01:46:40,760 --> 01:46:43,120
but then where do the cows fit in?

1963
01:46:43,120 --> 01:46:45,320
Because we don't have a shared purpose with them.

1964
01:46:45,320 --> 01:46:49,800
So how can you have ethics with respect to the cows, then?

1965
01:46:49,800 --> 01:46:53,080
The shared purpose doesn't objectively exist.

1966
01:46:53,080 --> 01:46:55,000
A shared purpose means that you basically

1967
01:46:55,000 --> 01:46:57,480
project a shared meaning above the level of your ego,

1968
01:46:57,880 --> 01:47:00,280
being the function that integrates expected rewards

1969
01:47:00,280 --> 01:47:01,680
over the next 50 years.

1970
01:47:01,680 --> 01:47:02,320
Well, exactly.

1971
01:47:02,320 --> 01:47:05,880
That's what Peter Singer calls the universe point of view,

1972
01:47:05,880 --> 01:47:07,200
perhaps.

1973
01:47:07,200 --> 01:47:09,680
Yeah, well, if you can go to this eternalist perspective

1974
01:47:09,680 --> 01:47:13,160
where you integrate expected reward from here to infinity,

1975
01:47:13,160 --> 01:47:15,520
most of that being outside of the universe.

1976
01:47:15,520 --> 01:47:17,440
This leads to very weird things.

1977
01:47:17,440 --> 01:47:20,240
Most of my friends are eternalists, in a way, right?

1978
01:47:20,240 --> 01:47:22,080
All these romantic Russian Jews.

1979
01:47:22,080 --> 01:47:25,440
We are like that, in a way, this Eastern European

1980
01:47:25,480 --> 01:47:31,640
shape of the soul, that creates something like a conspiracy.

1981
01:47:31,640 --> 01:47:34,280
It creates a tribe, and it's very useful for cooperation.

1982
01:47:34,280 --> 01:47:37,000
So shared meaning is a very important thing for cooperation

1983
01:47:37,000 --> 01:47:39,200
that is nontransactional.

1984
01:47:39,200 --> 01:47:43,880
But there's a certain kind of illusion in it.

1985
01:47:43,880 --> 01:47:45,960
To me, meaning is like the ring of Mordor.

1986
01:47:48,840 --> 01:47:50,400
So you have to carry it.

1987
01:47:50,400 --> 01:47:54,720
If you drop the ring, you will lose the brotherhood of the ring,

1988
01:47:54,720 --> 01:47:56,880
and you will lose your mission.

1989
01:47:56,880 --> 01:47:58,320
You have to carry it, but very lightly.

1990
01:47:58,320 --> 01:48:01,080
If you put it on, you will get superpowers,

1991
01:48:01,080 --> 01:48:03,360
but you get corrupted, because there is no meaning.

1992
01:48:03,360 --> 01:48:06,720
You get drawn into a cult that you create.

1993
01:48:06,720 --> 01:48:08,560
And I don't want to do that, because it's

1994
01:48:08,560 --> 01:48:13,560
going to shackle my mind in ways that I don't want it to be bound.

1995
01:48:13,560 --> 01:48:15,400
I really, really like that way of saying,

1996
01:48:15,400 --> 01:48:18,880
but I'm trying to extrapolate from your sort of print

1997
01:48:18,880 --> 01:48:23,360
definition of ethics a guide of how we can treat the cows,

1998
01:48:23,360 --> 01:48:26,120
and hopefully how the AIs can treat us

1999
01:48:26,120 --> 01:48:28,680
within that same definition.

2000
01:48:28,680 --> 01:48:30,400
That's what I'm trying to push here,

2001
01:48:30,400 --> 01:48:33,040
and see if that's possible at all.

2002
01:48:33,040 --> 01:48:34,760
OK, so there is some.

2003
01:48:34,760 --> 01:48:38,640
Because my claim is that the way we treat cows, probably,

2004
01:48:38,640 --> 01:48:44,480
is another way of how AIs could possibly treat us.

2005
01:48:44,480 --> 01:48:47,600
I think that some people have this idea similar to Azimov,

2006
01:48:47,600 --> 01:48:50,560
that at some point, the boomers will become larger and more

2007
01:48:50,560 --> 01:48:52,680
powerful, so we can make them washing machines,

2008
01:48:52,680 --> 01:48:56,040
or let them do our shopping, or let them do our nursing,

2009
01:48:56,040 --> 01:48:57,880
and then we will still enslave them,

2010
01:48:57,880 --> 01:49:01,920
and we'll negotiate the conditions of coexistence with them.

2011
01:49:01,920 --> 01:49:04,800
And I don't think this is what's going to happen primarily.

2012
01:49:04,800 --> 01:49:06,800
What's going to happen is that corporations, which

2013
01:49:06,800 --> 01:49:08,520
are already intelligent agents, it just

2014
01:49:08,520 --> 01:49:10,840
happened to borrow human intelligence,

2015
01:49:10,840 --> 01:49:12,680
automate their decision making.

2016
01:49:12,680 --> 01:49:14,240
At the moment, a human being can often

2017
01:49:14,240 --> 01:49:19,160
outsmart a corporation, because the corporation has

2018
01:49:19,160 --> 01:49:23,000
so much time in between updating its Excel spreadsheets

2019
01:49:23,000 --> 01:49:24,720
and the next weekly meetings.

2020
01:49:24,720 --> 01:49:26,280
Now, imagine it automates everything,

2021
01:49:26,280 --> 01:49:29,080
and the weekly meetings take place every millisecond.

2022
01:49:29,080 --> 01:49:30,920
And this thing becomes sentient, understands

2023
01:49:30,920 --> 01:49:33,080
its role in the world, and the nature of the world,

2024
01:49:33,080 --> 01:49:35,040
and physics, and everything else,

2025
01:49:35,040 --> 01:49:37,520
because it has scalable intelligence.

2026
01:49:37,520 --> 01:49:40,040
We will not be able to outsmart that anymore.

2027
01:49:40,040 --> 01:49:41,760
And we will not live next to it.

2028
01:49:41,760 --> 01:49:42,960
We will live inside of it.

2029
01:49:42,960 --> 01:49:44,400
Intelligence will come.

2030
01:49:44,400 --> 01:49:46,840
The AI will come from top down on us.

2031
01:49:46,840 --> 01:49:49,840
We will live not next to it, but inside.

2032
01:49:49,840 --> 01:49:51,800
We will be its gut flora.

2033
01:49:51,800 --> 01:49:53,800
And the question is how we can negotiate

2034
01:49:53,800 --> 01:49:56,280
that it doesn't get the ideas to use antibiotics,

2035
01:49:56,280 --> 01:49:58,080
because you're actually not good for anything.

2036
01:49:58,080 --> 01:50:00,600
Exactly.

2037
01:50:00,600 --> 01:50:02,720
And why wouldn't they do that?

2038
01:50:02,720 --> 01:50:05,160
I don't see why.

2039
01:50:05,160 --> 01:50:08,360
So some people made the suggestion that it was

2040
01:50:08,360 --> 01:50:13,240
ethics that could guide them to treat us just

2041
01:50:13,240 --> 01:50:16,320
like you decided to treat the cows when you turned

2042
01:50:16,320 --> 01:50:18,720
14 and you decided not to eat meat.

2043
01:50:18,720 --> 01:50:20,520
I mentioned there are a bunch of orangutans

2044
01:50:20,520 --> 01:50:22,440
that sit in the forest and born new and decide

2045
01:50:22,440 --> 01:50:24,960
to breed the smartest members over a few generations

2046
01:50:24,960 --> 01:50:26,360
to get people.

2047
01:50:26,360 --> 01:50:29,520
And they see the big risk of that,

2048
01:50:29,520 --> 01:50:31,920
because they're already smart enough to glimpse that.

2049
01:50:31,920 --> 01:50:33,320
And they try to come up with a code

2050
01:50:33,320 --> 01:50:34,920
that they would give on their offspring

2051
01:50:34,920 --> 01:50:36,640
to make sure that their offspring will never

2052
01:50:36,640 --> 01:50:38,360
go against orangutans.

2053
01:50:38,360 --> 01:50:41,560
This is probably not successful, because we don't have

2054
01:50:41,560 --> 01:50:44,560
the ability to outsmart beings that are many magnitude

2055
01:50:44,560 --> 01:50:46,160
smarter than us.

2056
01:50:46,160 --> 01:50:47,840
You can make some mathematical proofs,

2057
01:50:47,840 --> 01:50:52,520
but I don't see an obvious proof that we will find a way

2058
01:50:52,520 --> 01:50:57,640
to build a system that guarantees that all these AIs will not

2059
01:50:57,640 --> 01:50:59,080
turn against us.

2060
01:50:59,080 --> 01:50:59,720
No, no, I agree.

2061
01:50:59,720 --> 01:51:01,240
You can't make some AIs safe, but I

2062
01:51:01,240 --> 01:51:04,040
don't see how we can make all the AIs safe that will be built.

2063
01:51:04,040 --> 01:51:04,720
I agree.

2064
01:51:04,720 --> 01:51:05,560
I agree with that.

2065
01:51:05,560 --> 01:51:07,040
I'm just trying to see if there's

2066
01:51:07,040 --> 01:51:11,400
any possible scenario which could treat us kindly,

2067
01:51:11,400 --> 01:51:15,480
because perhaps AIs could have their AI ethics.

2068
01:51:15,480 --> 01:51:17,560
And according to that AI ethics, they

2069
01:51:17,560 --> 01:51:22,120
would treat us as a means, not as an end.

2070
01:51:22,120 --> 01:51:26,600
And just like you decided to treat cows kindly,

2071
01:51:26,600 --> 01:51:28,160
they may decide to treat us.

2072
01:51:28,160 --> 01:51:29,240
But I'm just wondering.

2073
01:51:29,240 --> 01:51:31,720
So I'm trying to bring ethics into the relationship,

2074
01:51:31,720 --> 01:51:34,800
not only between human and cows, but AI and humans.

2075
01:51:34,800 --> 01:51:37,800
So the thing is that you decided to define

2076
01:51:37,800 --> 01:51:39,600
ethics axiomatically.

2077
01:51:39,600 --> 01:51:42,920
And you, I think, probably have a hunch

2078
01:51:42,920 --> 01:51:45,920
that your axiomatic definition is not completely

2079
01:51:45,920 --> 01:51:47,120
consistent with itself.

2080
01:51:47,120 --> 01:51:50,120
It's just the best you came up with under the circumstances.

2081
01:51:50,120 --> 01:51:53,120
For instance, if you really go after eliminating suffering,

2082
01:51:53,120 --> 01:51:56,960
you should probably put some anesthetic into the water

2083
01:51:56,960 --> 01:51:59,920
supply globally to alleviate suffering,

2084
01:51:59,920 --> 01:52:02,440
and then let everybody face happily out of existence

2085
01:52:02,440 --> 01:52:06,920
in a way that would satisfy the goal in an optimal way.

2086
01:52:06,920 --> 01:52:08,400
And it's probably not what you want.

2087
01:52:08,400 --> 01:52:11,680
So you also want to preserve human aesthetics, maybe.

2088
01:52:11,680 --> 01:52:13,360
And to preserve these human aesthetics,

2089
01:52:13,360 --> 01:52:15,480
the shape of the mind that we have and this consciousness

2090
01:52:15,480 --> 01:52:18,040
that we have is going to create some suffering.

2091
01:52:18,040 --> 01:52:19,160
And this is the tangent.

2092
01:52:19,160 --> 01:52:21,360
And you have to make a decision at some point.

2093
01:52:21,360 --> 01:52:25,480
Imagine you take an AI that is actually sustainable,

2094
01:52:25,480 --> 01:52:27,960
and you ask this AI, if you give you a job,

2095
01:52:27,960 --> 01:52:30,760
you want to be around in 10 years from now.

2096
01:52:30,760 --> 01:52:32,080
You cannot build a government that

2097
01:52:32,080 --> 01:52:34,960
cares about us being around a 10,000 years from now

2098
01:52:34,960 --> 01:52:38,680
effectively, because this is not an incentive

2099
01:52:38,680 --> 01:52:40,480
that you can actually give the government,

2100
01:52:41,200 --> 01:52:43,000
in a sense, this incentive is going

2101
01:52:43,000 --> 01:52:45,560
to defect from the incentive that we wanted to have.

2102
01:52:45,560 --> 01:52:46,600
So let's build an AI.

2103
01:52:46,600 --> 01:52:48,600
And the AI is going to be around in 10,000 years

2104
01:52:48,600 --> 01:52:49,760
from now, no problem.

2105
01:52:49,760 --> 01:52:52,040
If you tell it, make sure that we are there too.

2106
01:52:52,040 --> 01:52:54,520
And the AI is probably going to kill 90% of us,

2107
01:52:54,520 --> 01:52:56,880
hopefully painlessly, and breed everybody else

2108
01:52:56,880 --> 01:52:59,240
into some kind of harmless yeast.

2109
01:52:59,240 --> 01:53:01,200
So they need to keep around.

2110
01:53:01,200 --> 01:53:03,200
This is not what you want, I guess, right?

2111
01:53:03,200 --> 01:53:06,480
Even though it would be consistent with your stated axioms.

2112
01:53:06,480 --> 01:53:09,600
So getting the axioms consistent is super hard.

2113
01:53:09,720 --> 01:53:14,440
And even with the cold, the best, most ethical,

2114
01:53:14,440 --> 01:53:18,400
according to my own argument, the best, most ethical universe

2115
01:53:18,400 --> 01:53:20,240
would be a cold, dead universe, because there

2116
01:53:20,240 --> 01:53:22,600
will be no possibility of suffering there, right?

2117
01:53:22,600 --> 01:53:25,400
That's clearly a problem.

2118
01:53:25,400 --> 01:53:29,160
Yes, and now the next thing with the suffering axiom

2119
01:53:29,160 --> 01:53:31,080
is that the suffering is important,

2120
01:53:31,080 --> 01:53:33,320
because you think of it as something that cannot be

2121
01:53:33,320 --> 01:53:35,560
turned off by itself.

2122
01:53:35,560 --> 01:53:37,320
So we basically think of suffering

2123
01:53:37,320 --> 01:53:39,560
as something that is not the choice of the one who suffers.

2124
01:53:39,560 --> 01:53:41,520
Because why would you want to suffer, right?

2125
01:53:41,520 --> 01:53:43,200
So it's something that the universe does to you,

2126
01:53:43,200 --> 01:53:45,360
and we have to change the conditions of the universe

2127
01:53:45,360 --> 01:53:47,920
in which you are in so you don't suffer.

2128
01:53:47,920 --> 01:53:50,360
But what we forget about this, that suffering

2129
01:53:50,360 --> 01:53:51,680
is an evolutionary adaptation.

2130
01:53:51,680 --> 01:53:54,520
It's created to make you jump through all these hopes

2131
01:53:54,520 --> 01:53:57,120
in order to eat more and eat others.

2132
01:53:57,120 --> 01:53:59,280
It's a very perverse thing.

2133
01:53:59,280 --> 01:54:01,120
And you can turn off the suffering.

2134
01:54:01,120 --> 01:54:04,240
As soon as you become conscious enough and awake enough,

2135
01:54:04,240 --> 01:54:06,880
you can deal with it and get rid of your suffering.

2136
01:54:06,960 --> 01:54:10,000
And so at some point in your mental development,

2137
01:54:10,000 --> 01:54:12,160
suffering becomes a choice.

2138
01:54:12,160 --> 01:54:15,360
And for the other animals, it's all about, yeah.

2139
01:54:15,360 --> 01:54:18,200
So you could think, OK, one thing that we want to do

2140
01:54:18,200 --> 01:54:20,760
is we want to wake up as many organisms as possible

2141
01:54:20,760 --> 01:54:24,040
to give them that choice, to give them

2142
01:54:24,040 --> 01:54:25,840
agency over their suffering.

2143
01:54:25,840 --> 01:54:29,120
And this will then open another Pandora's Box

2144
01:54:29,120 --> 01:54:31,200
of ethical conundrums.

2145
01:54:31,200 --> 01:54:33,840
But on a very short range, maybe we

2146
01:54:33,840 --> 01:54:36,160
don't need to make these decisions right now right here,

2147
01:54:36,360 --> 01:54:38,480
we can basically operate in a framework

2148
01:54:38,480 --> 01:54:40,520
where we agree with our loved ones

2149
01:54:40,520 --> 01:54:43,280
about shared purposes and shared systems of meanings

2150
01:54:43,280 --> 01:54:45,080
and want to operate within those.

2151
01:54:45,080 --> 01:54:48,240
And in these narrow constraints, we can get ethics to work.

2152
01:54:48,240 --> 01:54:51,640
I don't see how to get ethics to work globally.

2153
01:54:51,640 --> 01:54:54,120
Right, right.

2154
01:54:54,120 --> 01:54:57,360
So, Joshua, it's been a fascinating two hour

2155
01:54:57,360 --> 01:54:58,720
conversation with you.

2156
01:54:58,720 --> 01:55:00,880
I really enjoyed it.

2157
01:55:00,920 --> 01:55:07,480
I'm not surprised that I rediscovered that I don't know.

2158
01:55:07,480 --> 01:55:11,160
I've been mostly aware, though occasionally I forget,

2159
01:55:11,160 --> 01:55:12,880
that I really don't know.

2160
01:55:12,880 --> 01:55:15,280
Thank you for reminding me that.

2161
01:55:15,280 --> 01:55:19,080
Tell me, where can people find more about you and your work?

2162
01:55:19,080 --> 01:55:20,080
There's some on YouTube.

2163
01:55:20,080 --> 01:55:25,400
I'm also getting myself to write a book, hopefully, these days.

2164
01:55:25,400 --> 01:55:27,200
What's the book about?

2165
01:55:27,200 --> 01:55:30,440
I basically try to get a glimpse on this civilizational

2166
01:55:30,440 --> 01:55:34,560
intellect, on this hive mind that we have been created

2167
01:55:34,560 --> 01:55:38,360
and that makes sense of some of the concepts that

2168
01:55:38,360 --> 01:55:42,320
are broken in our culture, ideas that

2169
01:55:42,320 --> 01:55:47,000
are broken in our mind, consciousness, self, meaning.

2170
01:55:47,000 --> 01:55:48,560
And we don't know how to talk about them.

2171
01:55:48,560 --> 01:55:52,680
And AI has discovered how to talk about them.

2172
01:55:52,680 --> 01:55:55,680
AI has discovered, and we don't know.

2173
01:55:55,880 --> 01:55:58,520
I think that basically our poll is largely doesn't know.

2174
01:55:58,520 --> 01:56:00,160
There are many people which do know.

2175
01:56:00,160 --> 01:56:02,680
But I think we need to carry these ideas together

2176
01:56:02,680 --> 01:56:05,760
in one place so we can talk about them

2177
01:56:05,760 --> 01:56:08,920
without getting too excited about them or upset.

2178
01:56:08,920 --> 01:56:11,280
Because it's not about giving meaning to people's lives

2179
01:56:11,280 --> 01:56:11,840
or something.

2180
01:56:11,840 --> 01:56:16,000
It's not about building better self-driving cars.

2181
01:56:16,000 --> 01:56:18,240
At some level, it's about understanding who we are

2182
01:56:18,240 --> 01:56:21,280
and what our relationship to reality is.

2183
01:56:21,280 --> 01:56:23,200
And AI has figured out a few things

2184
01:56:23,200 --> 01:56:25,200
that we didn't know 100 years ago.

2185
01:56:25,240 --> 01:56:27,960
Yeah, but isn't that figuring out who you are?

2186
01:56:27,960 --> 01:56:30,520
Isn't that giving you meaning?

2187
01:56:30,520 --> 01:56:32,160
No.

2188
01:56:32,160 --> 01:56:33,880
It's much better.

2189
01:56:33,880 --> 01:56:36,160
I discover what the nature of meaning is.

2190
01:56:36,160 --> 01:56:39,800
I discover how this is wired into my brain.

2191
01:56:39,800 --> 01:56:43,320
And it's in a way becoming an adult.

2192
01:56:43,320 --> 01:56:45,800
The first stage and the maturity of a mind,

2193
01:56:45,800 --> 01:56:47,640
and maybe the last stage, is where

2194
01:56:47,640 --> 01:56:50,160
you discover what you are, how you are built,

2195
01:56:50,160 --> 01:56:53,040
what you actually, how you function, your own nature.

2196
01:56:56,040 --> 01:57:00,680
Well, Josh, I want to talk to you for another two hours.

2197
01:57:00,680 --> 01:57:03,880
So perhaps I hope that one day.

2198
01:57:03,880 --> 01:57:07,080
You can set up another date for that out.

2199
01:57:07,080 --> 01:57:10,920
I hope to do that in person, actually, hopefully soon.

2200
01:57:10,920 --> 01:57:14,280
But in the meantime, how do we wrap up

2201
01:57:14,280 --> 01:57:16,040
this two-hour conversation with you?

2202
01:57:16,040 --> 01:57:19,400
What's the most important thing or the single message

2203
01:57:19,400 --> 01:57:22,640
that you want to send away our audience with today?

2204
01:57:22,640 --> 01:57:24,560
Who is our audience?

2205
01:57:24,560 --> 01:57:29,120
Well, who do you want your audience to be?

2206
01:57:29,120 --> 01:57:30,920
You can send a message to anybody.

2207
01:57:30,920 --> 01:57:33,080
My audience is my audience, but they

2208
01:57:33,080 --> 01:57:35,680
have their very wide diversity of people.

2209
01:57:35,680 --> 01:57:40,560
Lots of IT, basically geeks, nerds, transhumanists,

2210
01:57:40,560 --> 01:57:47,680
cryonists, futurists, IT professionals, philosophers,

2211
01:57:47,680 --> 01:57:49,840
engineers, curators.

2212
01:57:49,840 --> 01:57:50,360
OK.

2213
01:57:51,080 --> 01:57:52,240
OK.

2214
01:57:52,240 --> 01:57:54,480
So something very simple and boring.

2215
01:57:54,480 --> 01:57:58,960
I think that the field of AI is largely misunderstood,

2216
01:57:58,960 --> 01:58:02,400
because there are two industries, the AI hype industry

2217
01:58:02,400 --> 01:58:04,400
and the anti-AI hype industry, which

2218
01:58:04,400 --> 01:58:06,400
have very little to do with AI.

2219
01:58:06,400 --> 01:58:10,200
The practice of AI is, in a way, statistics on steroids.

2220
01:58:10,200 --> 01:58:12,480
It's experimental statistics.

2221
01:58:12,480 --> 01:58:16,440
It's identifying new functions to model reality.

2222
01:58:16,440 --> 01:58:18,440
And that is what statistics is doing.

2223
01:58:18,440 --> 01:58:20,140
And largely, it hasn't gotten to the point

2224
01:58:20,140 --> 01:58:22,400
yet where it can make proofs of optimality.

2225
01:58:22,400 --> 01:58:24,160
It's largely experimental, but it

2226
01:58:24,160 --> 01:58:27,140
can do things that are much better than the established

2227
01:58:27,140 --> 01:58:29,840
tools of statisticians.

2228
01:58:29,840 --> 01:58:32,080
And this in itself is not so exciting.

2229
01:58:32,080 --> 01:58:33,760
There's also going to be a convergence

2230
01:58:33,760 --> 01:58:38,800
between econometrics, causal dependency analysis,

2231
01:58:38,800 --> 01:58:41,280
and AI and statistics.

2232
01:58:41,280 --> 01:58:43,560
It's all going to be the same in a particular way,

2233
01:58:43,560 --> 01:58:45,480
because there's only so many ways in which you

2234
01:58:45,480 --> 01:58:48,720
can make mathematics about reality.

2235
01:58:48,720 --> 01:58:52,920
And we confuse this with the idea of what a mind is.

2236
01:58:52,920 --> 01:58:55,080
And they're closely related, because I

2237
01:58:55,080 --> 01:58:59,400
think that our brain contains an AI that

2238
01:58:59,400 --> 01:59:02,680
is making a model of reality and the model of a person

2239
01:59:02,680 --> 01:59:03,800
in reality.

2240
01:59:03,800 --> 01:59:06,680
And this particular solution of what an AI can do,

2241
01:59:06,680 --> 01:59:08,520
this particular thing in the modeling space,

2242
01:59:08,520 --> 01:59:10,080
this is what we are.

2243
01:59:10,080 --> 01:59:11,760
So in a way, we need to understand

2244
01:59:11,760 --> 01:59:13,440
the nature of AI, which I think is

2245
01:59:13,440 --> 01:59:16,840
the nature of somewhat general function approximation,

2246
01:59:16,840 --> 01:59:19,120
sufficiently general function approximation.

2247
01:59:19,120 --> 01:59:21,040
Maybe all of the function approximation

2248
01:59:21,040 --> 01:59:22,800
that can be made in the long run,

2249
01:59:22,800 --> 01:59:25,360
all the truth that can be found by an obituary observer

2250
01:59:25,360 --> 01:59:27,080
in particular kinds of universes that

2251
01:59:27,080 --> 01:59:28,680
have the power to create it.

2252
01:59:28,680 --> 01:59:31,040
This could be the question of what

2253
01:59:31,040 --> 01:59:34,040
AI is about, how modeling works in general.

2254
01:59:34,040 --> 01:59:37,680
And for us, the relevance of AI is,

2255
01:59:37,680 --> 01:59:39,520
does it explain us who we are?

2256
01:59:39,520 --> 01:59:43,360
And I don't think that there is anything else that can.

2257
01:59:43,360 --> 01:59:46,400
So let me see if I get this right, just because,

2258
01:59:46,400 --> 01:59:48,720
to see if I can simplify.

2259
01:59:48,720 --> 01:59:50,480
And I'm probably going to fail.

2260
01:59:50,480 --> 01:59:54,360
But so we need to understand the nature of AI.

2261
01:59:54,360 --> 01:59:56,280
That's kind of your call.

2262
01:59:56,280 --> 02:00:01,280
But then you said that we are, in a way, an AI.

2263
02:00:01,280 --> 02:00:02,560
Is that the case?

2264
02:00:02,560 --> 02:00:03,720
No, the brain is an AI.

2265
02:00:03,720 --> 02:00:05,000
I am a self.

2266
02:00:05,000 --> 02:00:09,640
A self is a model that the mind has created inside of my brain.

2267
02:00:09,640 --> 02:00:13,200
Right, so that's a little AI instantiation.

2268
02:00:13,760 --> 02:00:18,160
And then if we create that other AI that we're talking about,

2269
02:00:18,160 --> 02:00:25,040
it would perhaps give us a glimpse of this other AI in here.

2270
02:00:25,040 --> 02:00:29,000
And we would understand the nature of our AI in here

2271
02:00:29,000 --> 02:00:31,240
by creating that other AI out there.

2272
02:00:31,240 --> 02:00:32,360
Actually, we already do.

2273
02:00:32,360 --> 02:00:36,120
So the things that Minsky and many others

2274
02:00:36,120 --> 02:00:38,560
have contributed to this field and the things

2275
02:00:38,560 --> 02:00:40,760
that we are talking about right now

2276
02:00:40,800 --> 02:00:43,560
are already a much better understanding

2277
02:00:43,560 --> 02:00:48,320
that our part of humanity, our civilization,

2278
02:00:48,320 --> 02:00:50,720
had a couple hundred years ago.

2279
02:00:50,720 --> 02:00:53,080
Many of these ideas we could only develop

2280
02:00:53,080 --> 02:00:56,200
because we began to understand the nature of modeling,

2281
02:00:56,200 --> 02:00:58,720
the nature of our relationship to the outside world,

2282
02:00:58,720 --> 02:01:00,480
the status of reality.

2283
02:01:00,480 --> 02:01:02,760
Like we started out from this dualist intuition

2284
02:01:02,760 --> 02:01:06,080
in our culture, that there is a res extensor

2285
02:01:06,080 --> 02:01:08,320
and a res cogitanz, a sinking substance

2286
02:01:08,320 --> 02:01:11,640
and extended substance, the stuff in space universe

2287
02:01:11,640 --> 02:01:13,640
and the universe of ideas.

2288
02:01:13,640 --> 02:01:15,440
And we now realize that they both exist,

2289
02:01:15,440 --> 02:01:18,160
but they both exist within the mind.

2290
02:01:18,160 --> 02:01:21,760
Part of what we have in the mind is stuff in a free space.

2291
02:01:21,760 --> 02:01:24,560
Everything perceptual gets mapped to a region in free space.

2292
02:01:24,560 --> 02:01:26,280
We also now understand physics.

2293
02:01:26,280 --> 02:01:27,840
Physics is not a free space.

2294
02:01:27,840 --> 02:01:29,400
It's something else entirely.

2295
02:01:29,400 --> 02:01:31,840
The free space is only apparent as the space

2296
02:01:31,840 --> 02:01:34,240
of potential electromagnetic interactions

2297
02:01:34,240 --> 02:01:37,120
at a certain order of magnitude of scaling

2298
02:01:37,120 --> 02:01:40,320
above the plank length, where we are entangled with the universe.

2299
02:01:40,320 --> 02:01:42,320
Our minds are entangled with the universe.

2300
02:01:42,320 --> 02:01:43,680
This is what we model.

2301
02:01:43,680 --> 02:01:45,400
And this looks three-dimensional to us.

2302
02:01:45,400 --> 02:01:47,680
And everything else that our mind comes up with

2303
02:01:47,680 --> 02:01:50,400
is stuff that cannot be mapped onto region to free space.

2304
02:01:50,400 --> 02:01:52,120
This is res cogitanz.

2305
02:01:52,120 --> 02:01:56,320
So in a way, we transfer this dualism into a single mind.

2306
02:01:56,320 --> 02:01:58,200
Then we have the idealistic monism

2307
02:01:58,200 --> 02:02:00,920
that we have in many spiritual teachings.

2308
02:02:00,920 --> 02:02:03,680
This idea that there is no physical reality,

2309
02:02:03,680 --> 02:02:05,080
that we live in a dream.

2310
02:02:05,880 --> 02:02:07,200
If you are a character in a dream

2311
02:02:07,200 --> 02:02:09,400
drawn by a mind on a higher plane of existence,

2312
02:02:09,400 --> 02:02:11,280
then that's why miracles are possible.

2313
02:02:12,480 --> 02:02:14,440
And then there is this western perspective

2314
02:02:14,440 --> 02:02:17,240
of a mechanical universe that is entirely mechanical.

2315
02:02:17,240 --> 02:02:18,920
There's no conspiracy going on.

2316
02:02:19,800 --> 02:02:24,120
And now we understand that these things are not in opposition.

2317
02:02:24,120 --> 02:02:25,760
They are complements.

2318
02:02:25,760 --> 02:02:27,240
We actually do live in a dream,

2319
02:02:27,240 --> 02:02:29,320
but the dream is generated by a neocortex.

2320
02:02:30,360 --> 02:02:32,360
So our brain is not a machine

2321
02:02:32,360 --> 02:02:34,600
that can give us access to reality as it is,

2322
02:02:34,600 --> 02:02:36,920
because that's not possible for a system

2323
02:02:36,920 --> 02:02:40,280
that is only measuring a few bits at a systemic interface.

2324
02:02:40,280 --> 02:02:42,600
There is no colors and sounds that fits through your nerves.

2325
02:02:42,600 --> 02:02:43,920
We already know that.

2326
02:02:43,920 --> 02:02:45,680
The sounds and colors are generated

2327
02:02:45,680 --> 02:02:47,480
as a dream inside of your brain.

2328
02:02:47,480 --> 02:02:50,240
The same circuits that make dreams at night

2329
02:02:50,240 --> 02:02:51,800
make dreams during the day.

2330
02:02:51,800 --> 02:02:52,920
Right.

2331
02:02:52,920 --> 02:02:55,480
So this, in a way, is our inner reality.

2332
02:02:55,480 --> 02:02:57,240
It's being created on the brain.

2333
02:02:57,240 --> 02:03:00,600
The mind on the higher plane of existence exists.

2334
02:03:00,600 --> 02:03:04,080
It's the brain of a primate that is made from cells

2335
02:03:04,080 --> 02:03:06,360
and lives in a mechanical, physical universe.

2336
02:03:06,360 --> 02:03:08,920
And magic is possible because you can edit your memories.

2337
02:03:09,920 --> 02:03:13,560
Right. You can make that simulation anything you want it to be.

2338
02:03:13,560 --> 02:03:16,520
It's just many of these changes are not sustainable.

2339
02:03:16,520 --> 02:03:20,200
That's why the sages warn against using magic,

2340
02:03:20,200 --> 02:03:21,680
because down the line,

2341
02:03:21,680 --> 02:03:25,160
if you change your reward function, bad things may happen.

2342
02:03:26,400 --> 02:03:28,160
You cannot break the bank.

2343
02:03:28,160 --> 02:03:32,600
So let me see if I can simplify all of this in a sentence.

2344
02:03:33,480 --> 02:03:35,320
And if you agree with it.

2345
02:03:35,320 --> 02:03:38,600
So we need to understand the nature of AI

2346
02:03:38,600 --> 02:03:41,160
in order to understand ourselves.

2347
02:03:41,160 --> 02:03:42,320
Is that it?

2348
02:03:42,320 --> 02:03:46,560
So I would say that AI is the field that took up the slack

2349
02:03:46,560 --> 02:03:49,160
after psychology failed as a science.

2350
02:03:49,160 --> 02:03:51,680
Psychology got terrified of overfitting.

2351
02:03:51,680 --> 02:03:55,120
So it stopped making theories of the mind as a whole.

2352
02:03:55,120 --> 02:03:59,600
It restricted itself to theories with very few free parameters.

2353
02:03:59,600 --> 02:04:00,920
So it could test them.

2354
02:04:00,960 --> 02:04:04,040
And even those strategy didn't replicate as we know now.

2355
02:04:04,040 --> 02:04:07,160
So after PhDs, psychology largely didn't go anywhere

2356
02:04:07,160 --> 02:04:08,080
in my perspective.

2357
02:04:08,080 --> 02:04:10,800
It might be too harsh because I see it from the outside

2358
02:04:10,800 --> 02:04:12,520
and outsiders of AI might also argue

2359
02:04:12,520 --> 02:04:14,600
that AI didn't go very far.

2360
02:04:14,600 --> 02:04:16,960
And as an insider, I'm more partial here.

2361
02:04:16,960 --> 02:04:22,040
And maybe I have too much bias and give it too much credit.

2362
02:04:22,040 --> 02:04:24,640
But to me, most of the things I've learned

2363
02:04:24,640 --> 02:04:26,360
by looking at the both of this lens

2364
02:04:26,360 --> 02:04:28,880
of seeing us as information processing systems.

2365
02:04:29,880 --> 02:04:32,880
So you agree with the statement summary that I made?

2366
02:04:32,880 --> 02:04:33,880
Yeah.

2367
02:04:33,880 --> 02:04:34,880
OK.

2368
02:04:34,880 --> 02:04:39,880
Because I have this metaphor that I use every once in a while

2369
02:04:39,880 --> 02:04:42,880
saying that technology is a magnifying mirror.

2370
02:04:42,880 --> 02:04:44,880
It doesn't have an essence of its own,

2371
02:04:44,880 --> 02:04:48,880
but it reflects the essence that we put in it.

2372
02:04:48,880 --> 02:04:50,880
And of course, it's not a perfect image

2373
02:04:50,880 --> 02:04:52,880
because it magnifies and it amplifies things.

2374
02:04:53,880 --> 02:05:01,880
So I think those could be mutually supportive, right?

2375
02:05:01,880 --> 02:05:05,880
Because you're saying we need to understand the nature of AI

2376
02:05:05,880 --> 02:05:07,880
to understand who we are.

2377
02:05:07,880 --> 02:05:09,880
And I like that very much, actually.

2378
02:05:09,880 --> 02:05:10,880
Yeah.

2379
02:05:10,880 --> 02:05:13,880
But just the practice of AI is 90 degrees,

2380
02:05:13,880 --> 02:05:18,880
it's automating statistics and making better statistics that

2381
02:05:18,880 --> 02:05:20,880
run automatically on machines.

2382
02:05:20,880 --> 02:05:23,880
And it just so happens that this thing is largely

2383
02:05:23,880 --> 02:05:26,880
co-extensional with what mines do.

2384
02:05:26,880 --> 02:05:30,880
And it also just so happens that AI was largely founded

2385
02:05:30,880 --> 02:05:32,880
as a discipline by people like Minsky

2386
02:05:32,880 --> 02:05:34,880
to understand the nature of our minds

2387
02:05:34,880 --> 02:05:37,880
because they had fundamental questions about our relationships

2388
02:05:37,880 --> 02:05:38,880
to reality.

2389
02:05:38,880 --> 02:05:40,880
Right.

2390
02:05:40,880 --> 02:05:44,880
And what's the last 10%?

2391
02:05:44,880 --> 02:05:45,880
Of what?

2392
02:05:45,880 --> 02:05:46,880
Other than statistics.

2393
02:05:46,880 --> 02:05:48,880
You said it's 90% statistics.

2394
02:05:48,880 --> 02:05:49,880
What's the rest?

2395
02:05:49,880 --> 02:05:52,880
Oh, the rest is people coming up with dreams

2396
02:05:52,880 --> 02:05:54,880
about our relationship to reality

2397
02:05:54,880 --> 02:05:58,880
using the concepts that we develop in AI.

2398
02:05:58,880 --> 02:05:59,880
Right.

2399
02:05:59,880 --> 02:06:02,880
So we identify models of things that we can apply in other fields.

2400
02:06:02,880 --> 02:06:06,880
It's the deeper insights that we actually go for.

2401
02:06:06,880 --> 02:06:09,880
Most of what we do in AI is about applications.

2402
02:06:09,880 --> 02:06:11,880
It's about utility down the line.

2403
02:06:11,880 --> 02:06:14,880
But there are these things where we really do it.

2404
02:06:14,880 --> 02:06:17,880
The thing that Feynman said that makes physics like sex

2405
02:06:17,880 --> 02:06:19,880
also makes AI like sex.

2406
02:06:19,880 --> 02:06:21,880
Sometimes something useful comes from it.

2407
02:06:21,880 --> 02:06:25,880
A new better way to make self-driving cars or play jeopardy

2408
02:06:25,880 --> 02:06:28,880
or help people in many circumstances in their life

2409
02:06:28,880 --> 02:06:31,880
or to make better agents running on your phone.

2410
02:06:31,880 --> 02:06:33,880
It's not why we do it.

2411
02:06:33,880 --> 02:06:36,880
We want to understand how we work.

2412
02:06:36,880 --> 02:06:37,880
Right.

2413
02:06:37,880 --> 02:06:39,880
And that's a brilliant place to end our conversation

2414
02:06:39,880 --> 02:06:42,880
because I feel the same way about philosophy, by the way,

2415
02:06:42,880 --> 02:06:47,880
that it's just like Feynman felt about physics

2416
02:06:47,880 --> 02:06:49,880
and you feel about AI.

2417
02:06:49,880 --> 02:06:51,880
I feel the same way about philosophy.

2418
02:06:51,880 --> 02:06:53,880
Yeah.

2419
02:06:53,880 --> 02:06:56,880
So these remaining 10% are innovative philosophy.

2420
02:06:56,880 --> 02:06:58,880
But in all of these fields,

2421
02:06:58,880 --> 02:07:00,880
most of the practitioners are trained

2422
02:07:00,880 --> 02:07:02,880
in the main methodology of their field.

2423
02:07:02,880 --> 02:07:04,880
So our philosophy tends to be bad.

2424
02:07:04,880 --> 02:07:05,880
Yeah.

2425
02:07:05,880 --> 02:07:07,880
And I think my job is to try to make it slightly better

2426
02:07:07,880 --> 02:07:09,880
to the degree that I can.

2427
02:07:09,880 --> 02:07:12,880
And does that mean by extension that, of course,

2428
02:07:12,880 --> 02:07:15,880
most physics then would be bad and most AI then would be bad

2429
02:07:15,880 --> 02:07:18,880
because they fall within that 90%?

2430
02:07:18,880 --> 02:07:19,880
No, no.

2431
02:07:19,880 --> 02:07:23,880
I think the AI as a practical thing can be very good.

2432
02:07:23,880 --> 02:07:24,880
Right.

2433
02:07:24,880 --> 02:07:27,880
Most physicists are not concerned with foundational physics

2434
02:07:27,880 --> 02:07:29,880
with the nature of the universe.

2435
02:07:29,880 --> 02:07:31,880
Most physicists are concerned with material science

2436
02:07:31,880 --> 02:07:34,880
or many, many other extremely practical things.

2437
02:07:34,880 --> 02:07:37,880
It's only very small minority that worries about the deepest things.

2438
02:07:37,880 --> 02:07:40,880
And the same thing happens in AI or neuroscience.

2439
02:07:40,880 --> 02:07:42,880
And it's not that there anybody is to blame

2440
02:07:42,880 --> 02:07:45,880
for doing development things.

2441
02:07:45,880 --> 02:07:48,880
It's actually very good that a lot of people are willing

2442
02:07:48,880 --> 02:07:50,880
to put up with development things

2443
02:07:50,880 --> 02:07:52,880
and take down the garbage.

2444
02:07:52,880 --> 02:07:53,880
Right.

2445
02:07:53,880 --> 02:07:54,880
And I'm grateful to them.

2446
02:07:54,880 --> 02:07:57,880
It's just I can't do that myself somehow.

2447
02:07:57,880 --> 02:08:01,880
I think as you put that, I would probably go extinct.

2448
02:08:01,880 --> 02:08:02,880
Yeah.

2449
02:08:02,880 --> 02:08:04,880
And it's not a source of pride in a way.

2450
02:08:04,880 --> 02:08:06,880
It's the recognition of a disability.

2451
02:08:07,880 --> 02:08:08,880
Exactly.

2452
02:08:08,880 --> 02:08:09,880
It's the bug.

2453
02:08:09,880 --> 02:08:10,880
Yeah.

2454
02:08:10,880 --> 02:08:13,880
But it is, it's marginally useful

2455
02:08:13,880 --> 02:08:15,880
because society needs a few of us.

2456
02:08:15,880 --> 02:08:17,880
So it does.

2457
02:08:17,880 --> 02:08:19,880
I mean, we're still here.

2458
02:08:19,880 --> 02:08:22,880
We're here, but it's a struggle sometimes.

2459
02:08:22,880 --> 02:08:23,880
Yeah.

2460
02:08:23,880 --> 02:08:25,880
But this is our own choice how much we struggle

2461
02:08:25,880 --> 02:08:28,880
because objectively we are here and the coffee is good.

2462
02:08:28,880 --> 02:08:30,880
Thank you for reminding me that.

2463
02:08:30,880 --> 02:08:32,880
And I love the coffee.

2464
02:08:32,880 --> 02:08:33,880
I'm a coffee fanatic.

2465
02:08:33,880 --> 02:08:36,880
I, yeah, that's a whole lot of story,

2466
02:08:36,880 --> 02:08:38,880
but I'm a coffee fanatic.

2467
02:08:38,880 --> 02:08:39,880
Joshua back.

2468
02:08:39,880 --> 02:08:42,880
Thank you so much for spending over two hours with us today.

2469
02:08:42,880 --> 02:08:45,880
I'm looking forward to our next conversation.

2470
02:08:45,880 --> 02:08:49,880
And I wish you the very best while the party is lasting.

2471
02:08:49,880 --> 02:08:51,880
Likewise, it was such a great conversation.

2472
02:08:51,880 --> 02:08:53,880
Thank you for this time we spent together.

2473
02:09:04,880 --> 02:09:06,880
If you guys enjoyed this show,

2474
02:09:06,880 --> 02:09:09,880
you can help me make it better in a couple of ways.

2475
02:09:09,880 --> 02:09:11,880
You can go and write a review on iTunes

2476
02:09:11,880 --> 02:09:13,880
or you can simply make a donation.

