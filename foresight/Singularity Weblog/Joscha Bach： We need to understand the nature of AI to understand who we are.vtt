WEBVTT

00:00.000 --> 00:28.520
My name is Nikola and you're watching Singularity FM, the place where we interview the future.

00:28.520 --> 00:33.280
If you guys enjoyed this podcast, you can show your support by either writing a brief

00:33.280 --> 00:36.960
review on iTunes or by simply making a donation.

00:36.960 --> 00:41.120
Today, my guest on the show is Professor Joshua Bach.

00:41.120 --> 00:46.880
Joshua is a cognitive scientist at the Harvard Program for Evolutionary Dynamics as well

00:46.880 --> 00:49.680
as the MIT Media Lab.

00:49.680 --> 00:52.560
So welcome to Singularity FM, Joshua.

00:53.520 --> 00:58.920
Hi, first of all, I'm not a professor, though, and I have been working at the MIT Media

00:58.920 --> 01:03.640
Lab until three years ago, and since then, I'm at Harvard.

01:03.640 --> 01:07.320
I'm not affiliated at both at this time.

01:07.320 --> 01:10.040
Oh, so what exactly is your position then?

01:10.040 --> 01:11.240
I'm a research scientist.

01:11.240 --> 01:16.000
The research scientist is a person that works in the abyss between postdoc and tenure.

01:16.000 --> 01:17.960
Wow, but you are a PhD.

01:17.960 --> 01:19.240
I am.

01:19.240 --> 01:23.560
And what was your PhD in then?

01:23.560 --> 01:25.040
In cognitive science.

01:25.040 --> 01:31.960
I went into academia to understand how the mind works and so studied a number of subjects

01:31.960 --> 01:39.000
and did degrees in computer science and philosophy and felt that AI is my best bet of making

01:39.000 --> 01:41.600
headway in understanding who we are.

01:41.600 --> 01:42.600
Fantastic.

01:42.600 --> 01:50.000
So, you are, as you said, in the zero gravity sort of space between postdoc and professorship?

01:50.000 --> 01:51.000
Academia?

01:51.000 --> 01:53.560
Yeah, it's actually a quite happy place.

01:53.560 --> 01:55.440
It's one that allows me to do what I want.

01:55.440 --> 01:57.600
I don't have to do superfluous management.

01:57.600 --> 02:00.280
I don't have to sit in many committees or anything.

02:00.280 --> 02:04.320
I can teach when I want, but I don't have to, which is really the best arrangement you

02:04.320 --> 02:05.320
can possibly have.

02:05.320 --> 02:08.200
I can have students, but I don't have to.

02:08.200 --> 02:11.920
And so this is kind of amazing.

02:11.920 --> 02:17.360
That's fantastic, but I don't know how, if it's allowing you to survive with your family

02:17.360 --> 02:18.360
properly.

02:18.360 --> 02:23.280
But we find out, you know, there can always be earthquakes.

02:23.280 --> 02:31.360
Okay, Joshua, if I were to ask you to introduce yourself in a couple of words, who is Joshua

02:31.360 --> 02:33.640
Bach?

02:33.640 --> 02:40.400
It depends who's asking, but in the most general sense, I'm a cognitive scientist.

02:40.400 --> 02:46.200
I grew up in communist Eastern Germany, the last generation to do so, as the child of

02:46.200 --> 02:49.200
an artist in the forest.

02:49.200 --> 02:53.400
And I grew up in a world that was completely alien to me in many ways, because communist

02:53.400 --> 02:58.520
Eastern Germany didn't make a lot of sense, especially if you grow up in a forest in which

02:58.520 --> 03:02.960
everything has no rules and only the rules that locally make sense.

03:02.960 --> 03:07.520
So anyway, my default in understanding the world has been different from the default

03:07.520 --> 03:09.080
of people around me.

03:09.080 --> 03:14.640
I reluctantly discovered that most people formed their ideas by taking in the norms

03:14.640 --> 03:19.680
of their environment and the statements of the experts and taking them as gospel and

03:19.680 --> 03:24.160
only revise them when they absolutely have to and they're disproven.

03:24.160 --> 03:26.840
And for me, it was always like the opposite.

03:26.840 --> 03:31.800
You have this perspective on the world where people have ideas and their thoughts and they

03:31.800 --> 03:33.240
often make no sense.

03:33.240 --> 03:37.680
And you will have to look at each of them with great care before you incorporate them

03:37.680 --> 03:40.120
into your own world model.

03:40.120 --> 03:44.360
So you try to be careful to not harm anything or do bad things to the world.

03:44.360 --> 03:50.120
But this reluctance in accepting what comes from the outside has, I think, shaped my scientific

03:50.120 --> 03:51.120
perspective.

03:51.120 --> 03:55.960
And when I came into the next society, Western Germany, and then later on to New Zealand and

03:55.960 --> 04:00.280
to the US, I always saw things from the outside.

04:00.280 --> 04:03.880
So I'm more an observer and the same thing happens in the scientific fields.

04:04.160 --> 04:06.840
Wow, that's absolutely fascinating.

04:06.840 --> 04:09.920
And I want to grab a few points there one by one.

04:09.920 --> 04:18.960
But first of all, that kind of a skeptical outsider kind of point of view is very contrarian

04:18.960 --> 04:25.600
and also very sort of philosophical in the way, maybe in the German school, because at

04:25.600 --> 04:33.400
least Nietzsche said that gross answers are a prohibition against thinkers.

04:33.400 --> 04:34.920
You shall not think.

04:34.920 --> 04:43.760
And to him, that was like an insult because he was curious, questioning inquisitive kind

04:43.760 --> 04:46.360
of a soul from the beginning.

04:46.360 --> 04:54.040
And so he was never one for gross answers, but rather asking questions and questioning

04:54.040 --> 04:55.040
everything.

04:55.040 --> 04:57.840
So it seems you've kind of you've got that approach.

04:57.840 --> 05:01.600
I felt that Nietzsche never made peace with society.

05:01.840 --> 05:05.560
That was related to the fact that he was never able to make peace with himself.

05:05.560 --> 05:07.560
It really never worked out.

05:07.560 --> 05:09.400
There is a big issue with obedience.

05:09.400 --> 05:12.000
This question, should you obey somebody else?

05:12.000 --> 05:18.040
I mean, you seem to have that same issue that to work in a hierarchy, you need to submit

05:18.040 --> 05:19.200
in a way to a hierarchy.

05:19.200 --> 05:20.200
How would you submit?

05:20.200 --> 05:24.320
How could somebody else make your decisions if they didn't test it to the same rigorous

05:24.320 --> 05:26.760
epistemological criteria that you did?

05:26.760 --> 05:28.320
Does that have integrity, right?

05:28.320 --> 05:29.880
It's very hard to do.

05:29.880 --> 05:35.400
But from a different perspective, if you want to do the right thing, then doing the right

05:35.400 --> 05:40.680
thing might require that you ask the person that is more likely to make the right decisions

05:40.680 --> 05:43.800
because they're an expert for a local area of making the right things.

05:43.800 --> 05:49.080
Like a leader is a person in specializing that specializes in doing the right thing.

05:49.080 --> 05:54.160
So I think it has integrity to realize that in certain circumstances, other people will

05:54.160 --> 05:57.680
know better than you do and Nietzsche never got to this point.

05:57.680 --> 06:01.600
And of course, philosophy is slightly different because in philosophy, you have to fix your

06:01.600 --> 06:04.400
foundations and arguably invest in philosophy.

06:04.400 --> 06:05.840
Very few people did.

06:05.840 --> 06:06.840
Right?

06:06.840 --> 06:11.960
So our hypothesis still seems to be supernatural beings and dualism and so on.

06:11.960 --> 06:17.640
And that's one of the reasons why most people in the Western world find AI so ridiculous

06:17.640 --> 06:18.720
and unlikely.

06:18.720 --> 06:22.320
It's not because people don't see that we are biological computers and that the universe

06:22.320 --> 06:26.400
is probably mechanical and the theory that everything is mechanical gives extremely

06:26.400 --> 06:27.400
good predictions.

06:27.600 --> 06:32.360
It's because deep down they still have this not hypothesis that the universe is somehow

06:32.360 --> 06:35.400
supernatural and we are the most supernatural thing in it.

06:35.400 --> 06:39.160
And science is only reluctantly pushing back against this not hypothesis.

06:39.160 --> 06:45.040
And since it has not completely obliterated not hypothesis in this single area, the consciousness

06:45.040 --> 06:52.880
in the mind, we are reluctant in accepting this reasonable certainty that we are machines.

06:52.880 --> 06:56.600
This is the main reason why we hesitate so much, I think.

06:56.600 --> 06:57.920
So are we machines then?

06:57.920 --> 07:05.840
Are we as some people have said that organisms are algorithms?

07:07.840 --> 07:13.360
There are a number of definitions on this, but if you think of an algorithm as a set

07:13.360 --> 07:19.680
of rules that can be probabilistic or deterministic and that make it possible to switch between

07:19.680 --> 07:25.960
states and usually we do this in a more narrow sense where we say that the algorithm is being

07:25.960 --> 07:31.240
used to change representational states in order to compute a function, then I would

07:31.240 --> 07:36.680
say that organisms have algorithms in this narrower sense.

07:36.680 --> 07:39.920
But I would say that in the wider sense, they're definitely machines.

07:39.920 --> 07:44.320
Machine is a system that can change the state in non-random ways.

07:44.320 --> 07:50.000
And also, we visit earlier states, which means to stay in a particular state space.

07:50.000 --> 07:52.400
Otherwise, this would not be a system.

07:52.400 --> 07:58.400
A system is something that we can describe by drawing a fence around its state space

07:58.400 --> 08:01.360
and saying, as long as it's in there, this is the system.

08:01.360 --> 08:05.160
So we have an evolution of the system that is someone constrained.

08:05.160 --> 08:08.920
Now, we are jumping headfirst into terminology.

08:08.920 --> 08:11.080
Want to go there?

08:11.080 --> 08:15.800
We would go there, but let me just roll back the tape of time because I want to follow

08:15.800 --> 08:21.240
your narrative, your personal narrative from where it began, then connect it to where you

08:21.240 --> 08:27.960
are today, and then hopefully try and look it to the world and into the future with your

08:27.960 --> 08:31.640
eyes and with your experience and from your point of view.

08:31.640 --> 08:36.680
So tell me, you said you grew up in the forest.

08:36.680 --> 08:38.680
Whereabouts?

08:38.680 --> 08:43.440
In Turingia, near Weimar and Jena, it's an area of German romanticism, which had a pretty

08:43.440 --> 08:45.720
big influence on how I grew up.

08:46.680 --> 08:51.360
It's a very particular shape of the soul that has been characterized by the Enlightenment,

08:51.360 --> 08:58.240
which in a way pushed back against the religious mind fibers that had controlled the world

08:58.240 --> 09:04.760
until then and replaced it with machinery, this rationalist machinery that eventually

09:04.760 --> 09:07.440
made modernist societies possible.

09:07.440 --> 09:09.360
And this was a very big upheaval.

09:09.360 --> 09:13.880
You can still see the ego of this in our modernness, like Lord of the Rings and Star Wars.

09:13.880 --> 09:18.040
You have this pastoral world, which defends itself against the encroaching technological

09:18.040 --> 09:23.760
empire that is going to eat our souls, even though it's going to win.

09:23.760 --> 09:26.360
And so, but did you grow up on a farm or something?

09:26.360 --> 09:28.360
No, my parents were artists.

09:28.360 --> 09:30.400
They were originally architects.

09:30.400 --> 09:36.560
And my father didn't want to build boring things that would put people into boxes and

09:36.560 --> 09:37.560
deny the humanity.

09:37.560 --> 09:40.640
Instead, he built things that didn't have many right angles.

09:40.640 --> 09:46.480
And he made a zoo that had no right angles, for instance, as one of his projects and so

09:46.480 --> 09:47.480
on.

09:47.480 --> 09:51.960
And it was very difficult to get away with these things in Eastern Germany because this

09:51.960 --> 09:58.400
was a very utilitarian society and its architecture was to a large degree brutalist.

09:58.400 --> 10:03.800
So he rejected this and he decided to remove himself from society and make his own kingdom

10:03.800 --> 10:04.800
in the forest.

10:04.800 --> 10:10.480
So he bought an old water mill and changed it into a sculpture garden and lived exactly

10:10.480 --> 10:13.200
the life he wanted and got away with it.

10:13.200 --> 10:15.720
Wow, that's absolutely phenomenal.

10:15.720 --> 10:19.320
I, like you, grew up in the Eastern Bloc only.

10:19.320 --> 10:25.280
I grew up in Bloc area, in Communist Bloc area for the first, what was it, 13, 14 years

10:25.280 --> 10:30.800
of my life, so I can associate it with a lot of your experience.

10:30.800 --> 10:34.880
But it's very interesting how you grew up in Germany as you put it in the forest in

10:34.880 --> 10:39.400
a very artistic family, and yet you became a scientist.

10:39.400 --> 10:47.840
So is there any tension there or is it a continuation of sort of, or did it give you any kind of

10:47.840 --> 10:54.560
different unique point of view or approach to science or is that basically a false dichotomy?

10:54.560 --> 10:58.440
There is a big similarity.

10:58.440 --> 11:02.440
I find that most people serve practical needs.

11:02.440 --> 11:06.680
They have an understanding of the difference between meaning and relevance.

11:06.680 --> 11:11.720
And at some level, my mind is more interested in meaning than relevance.

11:11.720 --> 11:13.680
That is similar to the mind of an artist.

11:13.680 --> 11:17.000
The arts are not life, they're not serving life.

11:17.000 --> 11:21.600
The arts are the cuckoo child of life, because the meaning of life, they are the cuckoo child

11:21.600 --> 11:22.600
of life.

11:22.600 --> 11:24.000
The meaning of life is to eat.

11:24.000 --> 11:29.560
You know, life is evolution, and evolution is about eating.

11:29.560 --> 11:32.720
It's pretty gross if you think about it, right?

11:32.720 --> 11:35.160
Evolution is about getting eaten by monsters.

11:35.160 --> 11:38.640
Don't go into the desert and perish there, because it's going to be a waste.

11:38.640 --> 11:42.080
If you're lucky, the monsters that eat you are your own children.

11:42.080 --> 11:48.320
And eventually the search for evolution will, if evolution reaches its global optimum, it

11:48.320 --> 11:50.320
will be the perfect devourer.

11:50.320 --> 11:58.040
The thing that is able to digest anything and turn it into structure to sustain its

11:58.040 --> 12:04.280
perpetuate itself for as long as the local puddle of negentropy is available.

12:04.280 --> 12:06.120
And in a way, we are yeast.

12:06.120 --> 12:09.880
Everything we do, all the complexity that we create, all the structures we build is

12:09.880 --> 12:14.800
to erect some surfaces on which to out-compete other kinds of yeast.

12:14.800 --> 12:20.000
And if you realize this, you can try to get behind this, and I think the solution to this

12:20.000 --> 12:21.000
is fascism.

12:21.000 --> 12:22.000
Right?

12:22.000 --> 12:26.400
Fascism is a mode of organization of society in which the individual is a cell and a super

12:26.400 --> 12:27.400
organism.

12:27.400 --> 12:31.880
The value of the individual is exactly the contribution to the super organism.

12:31.880 --> 12:36.800
And when the contribution is negative, then the super organism kills it in order to be

12:36.800 --> 12:39.240
fitter in the competition against other super organisms.

12:39.240 --> 12:41.280
And it's totally brutal.

12:41.280 --> 12:46.720
And I don't like fascism because it is going to kill a lot of minds I like.

12:46.720 --> 12:48.320
And the arts is slightly different.

12:48.320 --> 12:51.560
It's a mutation that is arguably not completely adaptive.

12:51.560 --> 12:56.240
It's one where people fall in love with the lost function, where you think that your mental

12:56.240 --> 13:00.160
representation is the intrinsically important thing, where you try to capture a conscious

13:00.160 --> 13:03.080
state for its own sake, because you think that matters.

13:03.080 --> 13:06.640
The true artist, in my view, is somebody who captures conscious states, and that's the

13:06.640 --> 13:08.440
only reason why they eat.

13:08.440 --> 13:11.480
So you eat to make art.

13:11.480 --> 13:15.840
And another person makes art to eat.

13:15.840 --> 13:20.720
And these are, of course, the ends of a spectrum, and the twos is often somewhere in the middle.

13:20.720 --> 13:23.440
But in a way, there is this fundamental distinction.

13:23.440 --> 13:29.680
And there are, in some sense, the true scientists which try to figure out something about the

13:29.680 --> 13:30.680
universe.

13:30.680 --> 13:31.680
They try to reflect it.

13:31.680 --> 13:32.960
And it's an artistic process in a way.

13:32.960 --> 13:36.520
It's an attempt to be a reflection to this universe.

13:36.520 --> 13:40.760
You see, there's this amazing vast darkness, which is the universe.

13:40.760 --> 13:45.480
There's all this iteration of patterns, but mostly there's nothing interesting happening

13:45.480 --> 13:46.480
in these patterns.

13:46.480 --> 13:50.560
It's a giant fractal, and most of it is just boring.

13:50.560 --> 13:56.560
And in a brief moment in the evolution of the universe, there are planetary surfaces

13:56.840 --> 14:01.160
and like entropy gradients that allow for the creation of structure.

14:01.160 --> 14:06.640
And then there are some brief flashes of consciousness in all this vast darkness.

14:06.640 --> 14:10.240
And these brief flashes of consciousness can reflect the universe and maybe even figure

14:10.240 --> 14:11.760
out what it is.

14:11.760 --> 14:14.760
It's the only chance that we have, right?

14:14.760 --> 14:16.280
This is amazing.

14:16.280 --> 14:17.800
And why not do this?

14:17.800 --> 14:18.800
Life is short.

14:18.800 --> 14:21.000
This is the thing that we can do.

14:21.000 --> 14:25.640
And that's why you, going back to your previous point about your current position being sort

14:25.680 --> 14:31.400
of between post-doc and academia, that position actually fits you very well because you're

14:31.400 --> 14:38.400
not forced to do science in order to eat, but actually you can afford to eat as much

14:40.560 --> 14:42.000
as you can do your science.

14:42.000 --> 14:43.120
Is that the case?

14:43.120 --> 14:45.680
I have a similar problem as you, I think.

14:45.680 --> 14:49.800
It's very difficult for me to get myself to do something for which I'm not intrinsically

14:49.800 --> 14:51.640
motivated for.

14:51.640 --> 14:55.280
So you got that right completely.

14:55.280 --> 15:00.640
If I work in a job that is intellectually interesting, but doesn't appear meaningful to me, I will

15:00.640 --> 15:04.200
probably lose interest after four months.

15:04.200 --> 15:07.440
And I have to do something where I think this needs to be done.

15:07.440 --> 15:11.800
This is worth spending some of my short life on.

15:11.800 --> 15:12.800
Right.

15:12.800 --> 15:16.200
I didn't even last four months.

15:16.200 --> 15:20.640
After my undergraduate, before my master's degree, I worked as an investment administrator

15:20.680 --> 15:26.480
in a company, and I lasted six weeks where I was balancing portfolios and doing stock

15:26.480 --> 15:28.200
trades and things like that.

15:28.200 --> 15:32.120
I lasted about five and a half, six weeks, and then it's a debate whether I resigned

15:32.120 --> 15:34.080
or I got fired first.

15:34.080 --> 15:39.400
But either way, I was not surviving there, so or staying there anyway.

15:39.400 --> 15:40.400
Yeah.

15:40.400 --> 15:41.400
There's a tension.

15:41.400 --> 15:44.360
I want to be useful to society and I want to eat suffering and so on.

15:44.360 --> 15:46.800
I do care about people.

15:47.800 --> 15:52.440
It's just that I have the impression that the systems that we live in are often not

15:52.440 --> 15:53.440
sustainable.

15:53.440 --> 15:54.440
They're largely doomed.

15:54.440 --> 15:57.160
It's a very weird situation that we find ourselves in.

15:57.160 --> 16:01.480
If you take a step back, all the important tipping points for climate change have been

16:01.480 --> 16:05.280
the last century, as people said in the last century.

16:05.280 --> 16:12.000
But the fact that we knew about this, that global warming is basically known to our corporations,

16:12.000 --> 16:17.080
to our companies since the 60s and 70s, and to our governments, I think about the same

16:17.080 --> 16:22.680
time, that our inability to deal with this probably means that there was too little agency

16:22.680 --> 16:27.000
in the system to do anything about it, and we probably locked ourselves into this trajectory

16:27.000 --> 16:28.480
with the industrial revolution.

16:28.480 --> 16:33.200
At this point, it was no longer for us to stop the machines that we built.

16:33.200 --> 16:38.120
Well, we are kind of jumping forward, and I want to sort of slow the ball down a little

16:38.120 --> 16:40.400
bit, if I may.

16:40.400 --> 16:47.000
Don't worry, you're not going to get bored.

16:47.000 --> 16:48.000
You can keep that pace.

16:48.000 --> 16:51.280
You just can turn around and go as I'm else.

16:51.280 --> 16:52.280
Fantastic.

16:52.280 --> 16:53.280
The pace is great.

16:53.280 --> 16:57.360
It's just that I had so many considerations already in my previous points that you made

16:57.360 --> 17:01.320
that now I kind of lost the thread completely.

17:01.320 --> 17:03.800
Okay.

17:03.800 --> 17:05.800
Let me see.

17:06.000 --> 17:11.400
We got the artistic and the scientific part of Joshua.

17:11.400 --> 17:18.960
Where does philosophy come about here in this equation, and how?

17:18.960 --> 17:20.440
That's a very awkward question.

17:20.440 --> 17:27.520
The problem is, in my view, that philosophy as a field of inquiry is practically dead.

17:27.520 --> 17:31.800
Misha Gromov once told me, it's a mathematician, that in his perspective, Darwin was the last

17:31.800 --> 17:32.800
philosopher.

17:32.800 --> 17:37.180
He was the last one who was in a position where we could connect some dots in a completely

17:37.180 --> 17:38.680
fresh way.

17:38.680 --> 17:43.760
And after that, there were people like Russell, who were extremely good writers, but didn't

17:43.760 --> 17:47.920
do any real philosophy anymore because there was too little left.

17:47.920 --> 17:50.360
And I'm not quite sure if that is the case.

17:50.360 --> 17:53.760
There is some philosophy that needs to be done, and it's still being done, and it's largely

17:53.760 --> 17:57.200
in mathematics and fixing the foundations.

17:57.200 --> 17:59.640
And even there, it's mostly visible.

18:00.360 --> 18:05.440
So we have two big intellectual traditions, which is mathematics and physics.

18:05.440 --> 18:09.440
And there are some cracks in them that need to be dealt with, and this is where most of

18:09.440 --> 18:11.040
the philosophy is at.

18:11.040 --> 18:15.560
And all the other things are minor, like social organization and so on.

18:15.560 --> 18:20.960
It's very miraculous to the sociologists, but I think we can see the patterns.

18:20.960 --> 18:24.440
This is largely the effect of these fields.

18:24.440 --> 18:26.640
And philosophy as a field is a culture.

18:26.640 --> 18:31.200
Now, you get paid for emulating what a philosopher is supposed to look like, and it's very hard

18:31.200 --> 18:33.760
to get any philosophy done on the side.

18:33.760 --> 18:35.200
And the incentives are all wrong, right?

18:35.200 --> 18:40.760
It's a very fierce battle to become a philosopher, to get from post-doctor tenure in these fields.

18:40.760 --> 18:42.200
So you need to get cited.

18:42.200 --> 18:46.440
And the way you get cited as a philosopher is you identify a hot discussion.

18:46.440 --> 18:50.960
In that hot discussion, you identify a unique position, and you build your brand around

18:50.960 --> 18:51.960
that unique position.

18:51.960 --> 18:54.000
You cannot afford to give this up.

18:54.000 --> 18:59.160
So you have your Chinese room or your unique position in about free will, and you're going

18:59.160 --> 19:00.880
to defend this hill.

19:00.880 --> 19:04.400
Even if this hill is basically indefensible, philosophy is not going to progress in a way

19:04.400 --> 19:07.040
that forces your buildings off that hill.

19:07.040 --> 19:11.040
You can build a mansion on an indefensible hill, and you will still have meetings in

19:11.040 --> 19:13.520
there 200 years from now.

19:13.520 --> 19:16.160
And the bad thing is all the good hills are taken, right?

19:16.160 --> 19:19.320
So this is a very bad situation for philosophers.

19:19.320 --> 19:24.280
And I think this is the reason why I cannot be a philosopher today.

19:24.280 --> 19:28.840
And we need philosophy, but we don't have it anymore in this sense.

19:28.840 --> 19:29.960
But let's define it.

19:29.960 --> 19:31.240
What is philosophy for you?

19:31.240 --> 19:35.060
Because I've interviewed a number of mathematicians and physicians, and they both argue which

19:35.060 --> 19:39.600
one is at the root of everything, whether it's mathematics, whether it's physics, and

19:39.600 --> 19:41.240
so on and so on.

19:41.240 --> 19:46.480
But both of them or all of them mostly agree that philosophy is irrelevant, or so they

19:46.480 --> 19:47.840
make that claim.

19:47.840 --> 19:53.680
And yet you say that philosophy kind of includes both mathematics and physics in a way, which

19:53.680 --> 19:58.720
I actually agree with, but tell me why, and tell me how do you define it in the first

19:58.720 --> 20:02.360
place in a way that actually includes both of those?

20:02.360 --> 20:10.240
I think that philosophy is in a way the search for the global optimum of the modeling function.

20:10.240 --> 20:16.240
So it has fields that have been defined as parts of questions that lead to this modeling

20:16.240 --> 20:21.720
function like epistemology, what can be known, what is the nature of truth and so on, ontology,

20:21.720 --> 20:26.960
what is the stuff that exists, what's going on there, metaphysics.

20:26.960 --> 20:32.440
This is in some sense the systems in which you have to describe things.

20:32.440 --> 20:36.120
And ethics, what should we do?

20:36.120 --> 20:39.920
And at some point we discovered epistemology.

20:39.920 --> 20:45.080
So my view, the first rule of epistemology is roughly discovered by Francis Bacon in

20:45.080 --> 20:46.080
1620.

20:46.080 --> 20:52.520
It says that the strengths of your confidence in a belief must equal the weight of the evidence

20:52.520 --> 20:53.920
and support of it.

20:53.920 --> 20:59.200
And you need to apply this recursively until basically you resolve the priors of every belief

20:59.200 --> 21:02.000
and the belief system becomes self-contained.

21:02.000 --> 21:03.400
To believe stops being a verb.

21:03.400 --> 21:08.000
There's no more relationship to identifications that you just arbitrarily set.

21:08.000 --> 21:13.240
This is just a system that is in itself contained, which means in some sense it's a mathematical

21:13.240 --> 21:14.240
system.

21:14.240 --> 21:19.920
It's a system that describes a certain thing and this leads you to the nature of mathematics.

21:19.920 --> 21:25.840
And mathematics, it turns out, is the domain of all languages, all of them, not just the

21:25.840 --> 21:27.840
natural languages.

21:27.840 --> 21:31.600
And mathematicians have been trying to fix their understanding of the languages and they

21:31.600 --> 21:34.240
noticed what mathematics is in this regard.

21:34.240 --> 21:41.400
And Hilbert stumbled on counters, set theoretic experiments to deal with natural numbers and

21:41.400 --> 21:48.280
then saw that when you go to infinity, very awkward and nasty things happen, your axiomatic

21:48.280 --> 21:50.560
systems basically start blowing up.

21:50.560 --> 21:55.640
And the total set suddenly contains both itself and the set of all of its subsets, so it cannot

21:55.640 --> 21:58.280
have the same number of members as itself.

21:58.280 --> 22:03.120
And he asked mathematicians, please build us an interpreter for mathematics, a mathematics

22:03.120 --> 22:08.080
basically something like a computer made for mathematics, any mathematics you want that

22:08.080 --> 22:10.400
can run all of mathematics.

22:10.400 --> 22:14.920
And then Goedl and Turing came along and showed that this is not possible, that this computer

22:14.920 --> 22:15.920
is going to crash.

22:15.920 --> 22:19.680
And this left mathematics was a big shock and the way mathematics is still reeling from

22:19.680 --> 22:21.360
that shock.

22:21.360 --> 22:27.080
And then Turing in church had another insight and they figured out that all the universal

22:27.080 --> 22:29.440
computers have the same power.

22:29.440 --> 22:35.320
The universal computer is a set of rules that by applying them you can compute all the things

22:35.320 --> 22:36.760
that can be computed.

22:36.760 --> 22:37.960
And the set contains itself.

22:37.960 --> 22:39.960
So universal computer is computable.

22:39.960 --> 22:44.920
As long as your universal computer doesn't run out of resources, it can compute anything

22:44.920 --> 22:49.600
that you can compute and it can also compute all the other universal computers.

22:49.600 --> 22:55.400
So the next thing that they discovered Turing was involved again was that our mind is probably

22:55.400 --> 23:00.000
in the class of the universal computers, not in the class of mathematical systems.

23:00.000 --> 23:01.680
So this is what Penrose doesn't know.

23:01.680 --> 23:05.880
Penrose thinks that our mind is mathematical, that it can do things that a computer cannot

23:05.880 --> 23:07.320
do.

23:07.320 --> 23:13.640
And the big hypothesis of AI in a way is we are in the class of systems that can approximate

23:13.640 --> 23:17.200
computable functions and only those.

23:17.200 --> 23:21.120
And so we cannot do more than a computer, which means that all the mathematics that

23:21.120 --> 23:25.560
we've ever seen and all the mathematics that we will ever see and that will ever matter

23:25.560 --> 23:27.240
is going to be computable.

23:27.240 --> 23:30.960
And the fact that some mathematics is not computable is the problem of the language that

23:30.960 --> 23:31.960
we have been using.

23:31.960 --> 23:35.400
We need computational languages, not mathematical languages.

23:35.400 --> 23:40.720
And it turns out that the main problem is that mathematics, classical mathematics, defines

23:40.720 --> 23:47.160
functions in using infinities, which means infinitely many steps to get to the result.

23:47.160 --> 23:50.360
These functions tend not to be computable.

23:50.360 --> 23:56.400
So if you are a computer programmer, it would never occur to you to write in your spec that

23:56.400 --> 24:01.040
is totally fine if your routine does return the result after infinitely many steps only.

24:01.040 --> 24:02.040
Right?

24:02.040 --> 24:03.040
This is not good.

24:03.200 --> 24:09.760
A finite set of steps and one that you know how long it is, so your customer gets results

24:09.760 --> 24:10.760
in time.

24:10.760 --> 24:11.760
Right?

24:11.760 --> 24:16.080
So in this perspective, should you define numbers in such a way that pi is a number?

24:16.080 --> 24:17.760
You cannot know the last digit of pi.

24:17.760 --> 24:19.200
Pi is a function, clearly, right?

24:19.200 --> 24:22.360
It's a function that gives you as many digits as you can afford.

24:22.360 --> 24:29.360
And in any finite universe, it's only going to give you a finite number of bits.

24:29.360 --> 24:35.000
And what about Stephen Wolframs' claim that our mathematics is only one of a sort of a

24:35.000 --> 24:40.080
very wide spectrum of possible mathematics?

24:40.080 --> 24:42.640
It depends on what you call our mathematics.

24:42.640 --> 24:45.440
I think that all mathematics are mathematics.

24:45.440 --> 24:47.680
So meta-mathematics is mathematics.

24:47.680 --> 24:50.720
It's not different from mathematics.

24:50.720 --> 24:55.880
I think that, for instance, computational mathematics, the thing that I am practically

24:55.880 --> 25:00.480
working in when I write my code and when I think about how to realize code is a branch

25:00.480 --> 25:01.480
of mathematics.

25:01.480 --> 25:02.480
It's called constructive mathematics.

25:02.480 --> 25:07.120
It's been discovered in mathematics a long time ago and largely been ignored by the other

25:07.120 --> 25:11.160
mathematicians because they thought it's not powerful enough to do all the things with

25:11.160 --> 25:15.320
real numbers that they like to be doing.

25:15.320 --> 25:19.280
But all the geometry is not possible in computational mathematics.

25:19.280 --> 25:21.600
We can only approximate it.

25:21.600 --> 25:25.400
geometry requires continuous operations, infinities.

25:25.400 --> 25:31.760
And also physics is built largely on these continuous mathematics.

25:31.760 --> 25:37.000
And in a computational universe, you only get these continuous operators by taking a

25:37.000 --> 25:44.000
very large set of finite automata, making a series from them and then it's squint.

25:44.000 --> 25:45.320
You know what, Joshua?

25:45.320 --> 25:47.440
Let me share with you something.

25:47.440 --> 25:52.920
I feel like I am a goldfish and you're a human when we're talking because I think that's

25:52.920 --> 25:59.320
kind of like the level, the difference of intelligence between you and me, my friend,

25:59.320 --> 26:10.520
which I come on because honestly, after interviewing 230 of supposedly the smartest

26:10.520 --> 26:13.440
people in the world, I've never had this feeling before.

26:13.440 --> 26:19.160
But today at this moment, this is how I feel just trying to keep up with you.

26:19.160 --> 26:20.160
No, I'm sorry.

26:20.160 --> 26:21.160
This is my fault.

26:21.160 --> 26:22.160
No, no, no.

26:22.160 --> 26:23.160
It's not your fault.

26:23.160 --> 26:28.640
It's you are who you are and it's my job to try to follow through and also direct a little

26:28.640 --> 26:35.840
bit of conversation in the best possible direction that I see can benefit both me as an interviewer,

26:35.840 --> 26:39.240
but even more so my audience and you.

26:39.240 --> 26:43.880
So let me just give us a little bit of a side direction here for a second and bring us back

26:43.880 --> 26:50.880
to the last issue before we jump into the meat of the matter here on AI, and talk about

26:50.880 --> 27:01.880
philosophy in academia and practicality because you mentioned about how you're motivated by

27:01.880 --> 27:10.720
your own kind of desire and inherent or intrinsic motivation to learn something or to discover

27:10.720 --> 27:18.080
new things, but perhaps academia is motivated nowadays more by the practical side of knowledge,

27:18.080 --> 27:24.240
by the side where you can create something that you can patent, that you can sell, and

27:24.240 --> 27:27.360
that you can scale up and commercialize.

27:28.360 --> 27:32.720
Where is the benefit and I think in a way that the usefulness of philosophy was its

27:32.720 --> 27:39.600
uselessness in some ways, if you will, just like art in a way is something that cannot

27:39.600 --> 27:43.320
be used for anything else.

27:43.320 --> 27:49.400
And some people have defined art as Oscar Wilde, for example, as something that's not

27:49.400 --> 27:51.400
immediately useful.

27:51.400 --> 27:53.160
That's what art is.

27:53.160 --> 27:59.360
So is there and there's actually a very famous paper written in the 19th century by the guy

27:59.360 --> 28:05.560
who funded the Princeton Institute for Advanced Study called the usefulness of useless knowledge.

28:05.560 --> 28:08.360
I don't know if you're familiar with it, but what's your take on that?

28:08.360 --> 28:13.600
Is there because many people would say, if you can't use any knowledge immediately, it's

28:13.600 --> 28:14.600
useless.

28:14.600 --> 28:20.840
Don't waste time acquiring it, don't waste time classifying it, storing it, just focus

28:20.840 --> 28:25.400
on something that's useful and practical.

28:25.400 --> 28:30.920
And to me as a philosopher, I'm always or often attracted to stuff that looks utterly

28:30.920 --> 28:31.920
useless.

28:31.920 --> 28:34.920
And maybe that's just me being not a scientist.

28:34.920 --> 28:42.920
But what's your take on that sort of tension, usefulness and uselessness in terms of knowledge?

28:42.920 --> 28:46.680
Feynman once said that physics is like sex.

28:46.680 --> 28:52.120
Sometimes something useful comes from it, but it's not why we do it.

28:52.120 --> 28:54.240
But it's brilliant.

28:54.240 --> 28:56.440
So there is a big insight there.

28:56.440 --> 28:59.640
This is, it's not that art is useless.

28:59.640 --> 29:05.400
It's just the utility of art is completely orthogonal to why you do it.

29:05.400 --> 29:08.560
So the meaning of the art is really not to help the living.

29:08.560 --> 29:10.520
If you'd like to help the living, right?

29:10.520 --> 29:13.560
But it's, so it's a very nice side effect.

29:13.560 --> 29:17.480
But what we want to do with the art is to capture what it's like.

29:17.480 --> 29:20.200
We want to capture a conscious state.

29:20.200 --> 29:21.680
That's the actual meaning of it.

29:21.680 --> 29:26.200
And in some sense, philosophy is at the root of all this.

29:26.200 --> 29:30.640
I think it's reflected in a way in one of the founding myths of our civilization, the

29:30.640 --> 29:31.640
Tower of Babel.

29:31.640 --> 29:34.960
This is the attempt to build this cathedral.

29:34.960 --> 29:40.640
And it's not a material building because it's meant to reach the heavens, which is not real.

29:40.640 --> 29:43.520
They're not in this world.

29:43.520 --> 29:45.760
It's a metaphysical building that is being built.

29:45.760 --> 29:49.360
It's this giant machine that is meant to understand reality.

29:49.360 --> 29:53.760
And you get to this machine, this true scot, this thing that tries to understand what's

29:53.760 --> 29:57.680
going on by using people that work like ants and contribute to this.

29:57.680 --> 29:58.960
And it's not about your ego.

29:58.960 --> 30:03.760
It's not about the gratification that you get from people for contributing to it.

30:03.760 --> 30:06.880
It's not for this thing that doesn't care about you.

30:06.880 --> 30:08.040
It doesn't give meaning to your life.

30:08.040 --> 30:11.440
It doesn't reward you for your insecurities and the toil of your existence.

30:11.840 --> 30:14.240
But it's really just a machine.

30:14.240 --> 30:16.000
It's a computer.

30:16.000 --> 30:18.280
And as we would say now, it's an AI.

30:18.280 --> 30:21.840
It's a system that is able to make sense of the world.

30:21.840 --> 30:24.320
And people at some point had to give up on this.

30:24.320 --> 30:28.040
It fell apart because they were no longer able to speak the same language.

30:28.040 --> 30:31.480
So the different parts stopped fitting together.

30:31.480 --> 30:35.240
Just became so large and so many people had to work in specialized direction that they

30:35.240 --> 30:37.680
could no longer synchronize their languages.

30:37.680 --> 30:39.000
And that's why they gave up on it.

30:39.000 --> 30:43.920
And then this big accident happened in the Roman Empire, where they could not fix the

30:43.920 --> 30:47.400
incentives for governance in similar ways as we fail here.

30:47.400 --> 30:50.960
Our government has to play a much shorter game than civilization does.

30:50.960 --> 30:53.680
And this leads to bad results for civilization.

30:53.680 --> 30:58.120
And the Romans decided to fix this by turning the society into a cult and

30:58.120 --> 31:03.920
burned down our epistemology and killed people that were overtly rational and

31:03.920 --> 31:08.320
insisted that people talking to burning bushes on lonely mountains don't have a

31:08.320 --> 31:11.360
case in determining the origin of the universe.

31:11.360 --> 31:15.800
So this one had to give and the cultist won.

31:15.800 --> 31:18.200
And we still have to recover from that.

31:18.200 --> 31:25.040
So in a way, the beginnings of the cathedral of understanding the universe that had been

31:25.040 --> 31:29.640
built by the Greeks and by the Romans had been burned down by the Catholics.

31:29.640 --> 31:32.840
And then later rebuilt, but mostly in the likeness because they didn't get the

31:32.840 --> 31:34.280
foundations right.

31:34.280 --> 31:38.000
The left scars and our epistemology that have not healed, even though we have a

31:38.000 --> 31:42.120
pretty successful culture that incorporated most of the other libraries and

31:42.120 --> 31:43.920
burned down the rest, right?

31:43.920 --> 31:47.400
We are the ones that are left over on this planet in a way.

31:47.400 --> 31:51.040
In our libraries, we can read everything that there is to read at the moment.

31:51.040 --> 31:53.800
We just often cannot translate it.

31:53.800 --> 31:58.080
And do you think that our civilization is currently perhaps suffering from that

31:58.080 --> 32:05.280
same Babylonian problem of difference in language and perhaps even has impact on

32:05.280 --> 32:10.640
resolving global problems like global warming that you mentioned, for example, right?

32:10.640 --> 32:15.560
Because all those people, business people, politicians, scientists, et cetera, speak

32:15.560 --> 32:22.360
in different languages and therefore they cannot kind of coordinate or synchronize anymore.

32:22.360 --> 32:28.360
And therefore that kind of perhaps puts at risk the whole project of our civilization

32:28.360 --> 32:32.880
just like the Babylonian Tower collapsed.

32:32.880 --> 32:41.080
Now this narrow specialization and diversity of languages and the difficulty in communicating

32:41.080 --> 32:48.920
between all of those branches then puts at risk the whole project of our civilization.

32:48.920 --> 32:52.400
I think that people individually are not generally intelligent.

32:52.400 --> 32:55.160
How often do you see a person that knows what they're doing?

32:55.160 --> 32:57.240
I'm certainly don't know what I'm doing.

32:57.240 --> 33:00.600
I have no clue what I'm doing to be honest.

33:01.520 --> 33:06.920
We are relatively intelligent, but of course this intelligence is largely a prosthesis

33:06.920 --> 33:10.040
to cover for non-working instincts.

33:10.040 --> 33:12.360
And we figure that out by now, right?

33:12.360 --> 33:16.520
And we see that people acting on the instincts largely get good results for their life, but

33:16.520 --> 33:20.560
they don't reach a very deep understanding about the nature of existence in the process

33:20.560 --> 33:21.560
because they don't have to, right?

33:21.560 --> 33:26.720
There is very little utility for deep philosophy and practical matters.

33:26.720 --> 33:31.640
And as a result, individuals are relatively stupid.

33:31.640 --> 33:36.320
Generations are not smarter than individuals but dumber because generations are made from

33:36.320 --> 33:38.120
groups that synchronize their beliefs.

33:38.120 --> 33:42.600
And the synchronization of beliefs makes it necessary that you give up agency over what

33:42.600 --> 33:44.240
you think is true.

33:44.240 --> 33:48.240
And when you do this, you accept things that you would not accept when you think about

33:48.240 --> 33:49.640
them individually.

33:49.640 --> 33:54.160
So people in Eastern Germany collectively believe things that an individual would never

33:54.160 --> 33:55.920
have thought.

33:55.920 --> 33:58.240
And same things happen here, right?

33:58.240 --> 34:04.240
So there are many conspiracy theories that people believe in here for a while that would

34:04.240 --> 34:06.400
not make sense to somebody who thinks about this.

34:06.400 --> 34:12.640
Like Putin uses an army of Twitter trolls to manipulate the fan-affectations of Star Wars

34:12.640 --> 34:13.640
movies.

34:13.640 --> 34:19.360
This is a conspiracy theory that was a result of misreading a study and was then repeated

34:19.360 --> 34:23.880
by 20 news outlets until somebody bothered to read the actual study and figure out, no,

34:23.880 --> 34:25.800
this is not what the study says.

34:25.800 --> 34:30.400
And then some of the outlets picked up on this but none of them wrote, OK, now we reconsider

34:30.400 --> 34:35.120
what we think about Putin and Star Wars because it's a way totally what Putin would have done

34:35.120 --> 34:36.560
if he would have had the idea.

34:36.560 --> 34:42.880
And this may or may not be true but it means that we don't project reality as the extrapolation

34:42.880 --> 34:43.880
of facts.

34:43.880 --> 34:49.160
It's rather that we know there are enough facts to support what we feel to be true.

34:49.160 --> 34:53.520
And there's utility in feeling particular kinds of truths and these basically local

34:53.520 --> 34:58.320
cults of interpreting reality shape society, shape generations is what a generation is

34:58.320 --> 34:59.320
about.

34:59.320 --> 35:01.560
It's a local perspective of what things should be like.

35:01.560 --> 35:06.600
Like you have your liberal generations, the millennials are largely authoritarian generations

35:06.600 --> 35:10.960
and if you look at them and it feels wrong to us and they look at us and it feels wrong

35:10.960 --> 35:12.960
to them.

35:12.960 --> 35:14.960
And neither of them is true.

35:14.960 --> 35:19.560
It's probably a set of biases that are the result of a local indoctrination.

35:19.560 --> 35:21.680
But there is something that's smarter than the generation.

35:21.680 --> 35:23.200
This is the culture itself.

35:23.200 --> 35:28.320
So if you zoom out a little bit, you see that generations and societies are generated by

35:28.320 --> 35:30.960
cultures and cultures are built over a long time.

35:30.960 --> 35:35.320
And there are many things that are embodied in a culture, for instance, in the culture

35:35.320 --> 35:39.080
of how to build science that would be very hard to derive for a single generation or

35:39.080 --> 35:43.520
to improve for a single generation because we don't locally understand all the things

35:43.520 --> 35:45.840
that went into it.

35:45.840 --> 35:49.440
So anyway, civilizations are smarter than us.

35:49.440 --> 35:54.640
There is something like a civilizational mind, a civilizational intellect that we as members

35:54.640 --> 35:59.200
of our polis who are somewhat educated can never fully comprehend.

35:59.200 --> 36:04.800
But once we figure out that it's there, there is something like a civilizational intellect.

36:04.800 --> 36:09.560
We can try to look into the abyss and see its rough shape, but it's difficult to figure

36:09.560 --> 36:10.560
it out.

36:10.560 --> 36:14.120
And then we realize, oh, there's a long tradition, there's multiple traditions that build on

36:14.120 --> 36:16.040
it and contribute to it.

36:16.040 --> 36:23.400
And that thing, in a way, is what we are going to achieve when we build AI in the sense that

36:23.400 --> 36:29.000
we can incorporate the sum of all knowledge in a system of relations that makes sense

36:29.000 --> 36:30.640
of it all.

36:30.640 --> 36:34.640
But what if civilizations self-destroy themselves then?

36:34.640 --> 36:39.960
What is that sort of knowledge or intelligence then say about the fitness function of that

36:39.960 --> 36:42.760
particular civilization and in general even?

36:43.640 --> 36:47.600
Before we had an industrial civilization, we never got about 400 million individuals

36:47.600 --> 36:51.000
on the planet because we could not feed more.

36:51.000 --> 36:56.560
And only this switch to our industrial civilization made it possible to have billions of people,

36:56.560 --> 37:01.800
which also means many hundreds of millions of scientists and philosophers and thinkers

37:01.800 --> 37:04.400
and the internet and so on.

37:04.400 --> 37:05.400
It's amazing what we did.

37:05.400 --> 37:11.280
We took basically 100 years worth of trees that were turning into coal in the ground

37:11.280 --> 37:16.400
because nature had not evolved microorganisms yet that could eat the trees in time.

37:16.400 --> 37:23.160
And we burned through this deposit of energy in 100 years to give plumbing to everybody.

37:23.160 --> 37:29.760
And part of that plumbing includes access to a global porn repository that is an afterthought

37:29.760 --> 37:33.520
as to some of all you knowledge and largely uncensored chat rooms in which you can talk

37:33.520 --> 37:36.160
about it.

37:36.160 --> 37:37.600
This is the internet.

37:37.600 --> 37:39.600
And this is an amazing machine.

37:39.600 --> 37:43.760
And we have it right now and only in this moment and time we have it before it didn't

37:43.760 --> 37:44.760
exist.

37:44.760 --> 37:47.480
So you could take a particular perspective.

37:47.480 --> 37:51.840
Let's say there is a universe that is saying where everything is good.

37:51.840 --> 37:56.960
You have this nice planet with pretty decent living conditions and pretty stable climate

37:56.960 --> 38:01.720
and you have the very smart sustainable civilization on it and you get the chance to be incarnated

38:01.720 --> 38:02.720
in it.

38:02.720 --> 38:04.800
It's an agricultural civilization with 300 million people.

38:04.800 --> 38:05.800
It doesn't have airplanes.

38:05.800 --> 38:06.800
It doesn't have internet.

38:06.800 --> 38:07.800
It doesn't have computers.

38:08.000 --> 38:11.800
Because to get there it would have needed to build an industrial civilization that obliterates

38:11.800 --> 38:14.760
most of the good things that make us sustainable.

38:14.760 --> 38:16.200
But it is stable.

38:16.200 --> 38:20.480
And people are figured out how to be nice to each other and it's pretty good.

38:20.480 --> 38:24.240
And then there's another universe which is completely insane in fact up.

38:24.240 --> 38:31.040
And in this universe humanity has just doomed its planet to have a couple hundred really,

38:31.040 --> 38:32.560
really good years.

38:32.560 --> 38:36.960
And you get your lifetime close to the end of the party, this incarnation, which incarnation

38:36.960 --> 38:37.960
you choose.

38:37.960 --> 38:42.960
Oh my God, aren't we lucky?

38:42.960 --> 38:45.560
So you're saying we're in the second in the...

38:45.560 --> 38:46.560
Of course we are.

38:46.560 --> 38:51.720
It's fucking obvious, right?

38:51.720 --> 38:54.200
So what does that say about our future then?

38:54.200 --> 38:56.680
And what's the timeline before the party is over?

38:56.680 --> 39:01.520
We cannot know this, but we can see the sunset coming up, right?

39:01.520 --> 39:03.080
It's pretty obvious.

39:03.080 --> 39:04.080
And it's...

39:04.080 --> 39:05.080
People argue about this.

39:05.080 --> 39:08.920
They are largely in denial, but it's like you are in this Titanic and there's this pretty

39:08.920 --> 39:12.080
big iceberg and it's very unfortunate and people wish about it.

39:12.080 --> 39:15.440
But what they forget is that without the Titanic we wouldn't be here.

39:15.440 --> 39:16.960
We wouldn't be talking right now.

39:16.960 --> 39:17.960
We would not exist.

39:17.960 --> 39:20.280
We wouldn't have internet.

39:20.280 --> 39:21.280
So tell me this.

39:21.280 --> 39:28.600
You have this kind of very Buddhist, if I may call it, attitude to the sort of ephemeral

39:28.600 --> 39:37.560
sort of short span of our civilization and sort of the high appreciation about us joining

39:37.560 --> 39:40.400
the peak of the party, if you will.

39:40.400 --> 39:45.400
And yet you're kind of seeing the sunset kind of in the future, but that's not giving you

39:45.400 --> 39:50.920
any sort of negative or pessimistic or depressive inclination, it seems.

39:50.920 --> 39:51.920
How do you resolve that?

39:51.920 --> 39:52.920
Or do you?

39:52.920 --> 39:55.960
Because someone will say, well, that's very nihilistic, it's very pessimistic, it's very

39:55.960 --> 39:57.480
depressing what you just said.

39:57.480 --> 39:59.080
And yet you're so happy.

39:59.080 --> 40:01.480
No, I really have enough things to be depressed about.

40:01.480 --> 40:05.760
So I have to be choosy about what to be depressed about.

40:05.760 --> 40:13.200
And it took me a long time to figure out that the demise of humanity is very unfortunate

40:13.200 --> 40:14.600
in many respects.

40:14.600 --> 40:19.240
But it's something that, well, we try to do everything we can to stop it, but we are not

40:19.240 --> 40:22.040
the first generation to try to.

40:22.040 --> 40:24.160
So I have to do both things.

40:24.160 --> 40:31.120
I can still try my best to steer for a sustainable future, it's not that I completely give up

40:31.120 --> 40:35.880
on this, but it's in a way dealing with my own mortality is similar, right?

40:35.880 --> 40:44.080
I try what I can to not leave my family without a breadwinner too early, but at the same time

40:44.080 --> 40:45.480
I'm going to die.

40:45.480 --> 40:50.920
And if I waste my life being depressed about the fact that I die, I'm not doing it right.

40:50.920 --> 40:55.560
I should be happy about the fact that I live, not be unhappy about the fact that I die.

40:55.560 --> 41:01.560
And if you take this as a computer game metaphor, this is like the best level of humanity to

41:01.560 --> 41:02.560
play in.

41:02.560 --> 41:06.680
And this best level of humanity to play in, it happens to be the last level and it plays

41:06.680 --> 41:11.680
out against the haunting backdrop of a dying world, but it's still the best level.

41:11.680 --> 41:12.680
Right.

41:12.680 --> 41:17.240
That's again, to me, that sounds very Buddhist, do you agree?

41:17.240 --> 41:19.600
Yeah, but this might be an accident.

41:19.600 --> 41:25.640
I got to know Buddhism only in its westernized forms, which is a Protestant version.

41:25.640 --> 41:33.400
It's basically Protestantism reformed with slightly Eastern metaphysics, but mostly mistranslated.

41:33.400 --> 41:38.600
And epistemologically, in metaphysically, it's a septic tank that most of the ideas

41:38.600 --> 41:43.200
that Buddhists have about how the mind works and how the universe is arranged don't seem

41:43.200 --> 41:44.200
to pan out.

41:44.200 --> 41:47.000
They don't seem to have sound epistemology.

41:47.000 --> 41:48.000
This is not a general thing.

41:48.000 --> 41:53.560
I did find people that start out in Buddhism, in a way, and got clean, but most of them

41:53.560 --> 41:55.000
I met or not.

41:55.000 --> 41:59.600
And in practice, when I went to Buddhist countries and talked to Buddhists on the ground, it was

41:59.600 --> 42:03.920
not much different from Catholicism, which means it's a system of indoctrination with

42:03.920 --> 42:08.680
cults that makes people behave in predictable ways, which is useful for societies, but breaks

42:08.680 --> 42:10.600
people's epistemologies.

42:10.600 --> 42:15.520
So in a way, I don't have this deep reverence for Buddhism because it's so holy and sacred.

42:15.520 --> 42:19.800
I don't think that there are holy books, there are only manuals.

42:19.800 --> 42:24.520
And most of these manuals we don't know how to read because they are for a system for

42:24.520 --> 42:28.520
societies that don't apply to us, they're for different societies.

42:28.520 --> 42:34.080
Okay, let me zoom out a little bit more and ask you this.

42:34.080 --> 42:35.680
What are the big issues then?

42:35.680 --> 42:41.000
So you're saying we can see the sunset and we're at the peak of the party, so we might

42:41.000 --> 42:44.200
as well enjoy the party while it lasts.

42:44.200 --> 42:45.200
Great.

42:45.200 --> 42:52.080
The big issues that our civilization is facing today, what are the reasons perhaps if it's

42:52.080 --> 42:56.760
more than one that can bring about that sunset of our civilization?

42:56.760 --> 42:59.680
What is making you make that claim?

42:59.680 --> 43:05.280
The thing that burns me most at the moment is global warming.

43:05.280 --> 43:11.000
I suspect that because of a very strong publication bias that we have, if you are worried about

43:11.000 --> 43:15.200
climate, you will try to make your case extra strong so you will not make your most alarmist

43:15.200 --> 43:19.800
predictions but the ones that you can defend most easily, which means you're going to be

43:19.800 --> 43:23.280
a little less alarming that you might want to be.

43:23.280 --> 43:28.240
And if you are not an alarmist but an anti-alarmist, you're going to be way too optimistic about

43:28.240 --> 43:29.740
things.

43:29.740 --> 43:35.800
And as a result, I think that the distribution of the results that people look at when they

43:35.800 --> 43:39.800
think about how many degrees centigrade global warming they're facing in the next couple

43:39.800 --> 43:42.240
of hundred years are very optimistic.

43:42.240 --> 43:47.480
Another thing is, have you noticed that the projections all magically end in 2100?

43:47.480 --> 43:52.120
Do you think that's because the IPCC thinks that it stabilizes the 2100 or because it

43:52.120 --> 43:58.840
hopes that in 2100, too, there's a rupture event?

43:58.840 --> 44:00.640
It's obviously not going to stabilize.

44:00.640 --> 44:05.800
It seems to be that we locked in way more than two degrees centigrade global warming

44:05.800 --> 44:09.760
before we possibly go for six to eight.

44:09.760 --> 44:13.120
And we will lose the West Antarctic ice shield.

44:13.120 --> 44:15.480
It's pretty clear that we cannot refreeze the poles.

44:15.480 --> 44:20.000
And I think it has been pretty clear that we cannot do this since the late 1980s.

44:20.000 --> 44:22.960
It's just a feedback loop that is now running away.

44:22.960 --> 44:27.400
And there's a slight chance that we find technological solutions to stop it.

44:27.400 --> 44:29.280
But I think it's not likely.

44:29.280 --> 44:35.040
And carbon sequestration is not it for simple reasons of how energy works.

44:35.040 --> 44:38.920
The reason why we put all this carbon dioxide in the earth in the atmosphere is because

44:38.920 --> 44:41.320
we wanted to liberate this energy.

44:41.320 --> 44:46.680
And if we want to get it back from the atmosphere, we basically have to use the same amount of

44:46.680 --> 44:52.200
energy that our civilization has been getting from this, all the benefit, and put it back

44:52.200 --> 44:54.920
there without the clear business case.

44:54.920 --> 44:57.760
And it's possible that unlikely.

44:57.760 --> 45:03.960
So we look in a situation where in the medium term, we are going to lose a lot of habitable

45:03.960 --> 45:07.640
area on the planet, and we also might lose climate stability.

45:07.640 --> 45:13.800
So this ability to predict what kind of harvest we are going to have next year, which means

45:13.800 --> 45:15.880
we lose a lot of open air agriculture.

45:15.880 --> 45:21.360
We will have large storms that will also destroy many of our greenhouses.

45:21.360 --> 45:26.320
And as a result, we probably go down to a few hundred million individuals again.

45:26.320 --> 45:31.640
And the rest of us will not go kindly and quietly into this good night.

45:31.640 --> 45:35.880
And the resulting resource source will probably take downwards left of civilization.

45:35.880 --> 45:41.440
So basically, if you lose that infrastructure, I don't see how we can sustain civilization

45:41.440 --> 45:42.600
in a good way.

45:42.600 --> 45:51.600
Wow, that's such a beautiful serene and optimistic picture to contend with.

45:51.600 --> 45:53.440
But I mean, there's a lot of chances.

45:53.440 --> 45:57.800
I think it's possible that AI gets us before global warming does.

45:57.800 --> 46:04.240
So let me ask you this, because you are an AI scientist, and yet you're telling me you're

46:04.240 --> 46:09.360
most worried about global warming, and yet people who are not AI scientists like Elon

46:09.360 --> 46:18.240
Musk, like Nick Bostrom, like even the late Dr. Stephen Hawking are saying that the greatest

46:18.240 --> 46:21.720
existential risk that we should be worried about is AI.

46:21.720 --> 46:28.040
What do you feel about that in the first place, and what do you make of it?

46:28.040 --> 46:29.240
Many existential risks.

46:29.240 --> 46:35.280
So if you zoom out long enough, it's completely certain that the end of a sun that we can

46:35.280 --> 46:38.040
persist on is an existential risk.

46:38.040 --> 46:43.920
Another thing is that losing the atmosphere in 1.5 billion years from now is an existential

46:43.920 --> 46:46.160
risk that we probably cannot deal with.

46:46.160 --> 46:51.840
That looks unlikely that we can build sustainable civilizations outside of this gravity well.

46:51.840 --> 46:56.280
Before that, there's going to be a number of super volcano eruptions and meteors that

46:56.280 --> 47:00.920
are going to get us, which means it's pretty certain that the days of humanity are numbered.

47:00.920 --> 47:03.120
We are mortal as a civilization.

47:03.120 --> 47:06.920
What if we spread through all other planets?

47:06.920 --> 47:09.720
It's unlikely that we can make that happen.

47:09.720 --> 47:13.880
At the moment, we're not able to build cities on the bottom of the ocean.

47:13.880 --> 47:16.520
Mars is way less habitable than that.

47:16.520 --> 47:19.320
It doesn't even have an atmosphere.

47:19.320 --> 47:21.880
Can you care for it?

47:21.880 --> 47:24.880
Maybe, but not with today's technology.

47:25.880 --> 47:31.560
To get there, to basically put enough stuff in orbit to go from there to Mars, there's

47:31.560 --> 47:36.280
a large number of people and build something that is sustainable and can survive the breach

47:36.280 --> 47:41.240
of a few of the agricultural domes on Mars if a random meteor happens or something goes

47:41.240 --> 47:44.520
wrong and the pipe gets clogged.

47:44.520 --> 47:48.200
That is very hard to do.

47:48.200 --> 47:52.320
We cannot even think global warming.

47:52.320 --> 47:56.520
We cannot even build a new subway in New York anymore.

47:56.520 --> 48:00.240
We lost the ability to make a torster that gets more than four stars on Amazon somewhere

48:00.240 --> 48:02.520
after 1960.

48:02.520 --> 48:08.200
In many ways, our technological civilization is stagnating and it's because of regulation

48:08.200 --> 48:09.200
deficits.

48:09.200 --> 48:14.120
But we haven't figured this out and the biggest issue is probably good governance.

48:14.120 --> 48:15.800
We haven't really figured out good governance.

48:15.800 --> 48:17.960
AI might help with this.

48:17.960 --> 48:24.880
In a way, the building of information processing systems that can help us to self-regulate

48:24.880 --> 48:26.960
could be one of the big chances that we have.

48:26.960 --> 48:29.320
Without AI, we are dead for certain, I think.

48:29.320 --> 48:32.800
With AI, there's a probability that we are dead.

48:32.800 --> 48:39.600
So you're disagreeing in some sense, at least, that maybe not AI is our greatest danger,

48:39.600 --> 48:43.920
but perhaps our only hope for saving ourselves then.

48:44.920 --> 48:46.680
But you and me will probably die.

48:46.680 --> 48:49.040
We cannot be saved.

48:49.040 --> 48:51.840
Everybody who lives will probably die.

48:51.840 --> 48:55.120
And it's because entropy will always get you in the end.

48:55.120 --> 48:59.960
And our civilization has leveraged itself very far over an entropic abyss and there is no

48:59.960 --> 49:01.760
land on the other side.

49:01.760 --> 49:08.080
So you're going to crash down into this abyss at some point and probably sooner than later.

49:08.080 --> 49:13.720
This near-term AI, I'm mostly not worried about AI built into automatic guns.

49:13.720 --> 49:17.920
If you have drones that are controlled by AI, they're going to kill a few million people

49:17.920 --> 49:23.040
more than they would be killed otherwise with conventional weapons.

49:23.040 --> 49:27.880
Conventional weapons not driven by AI because it was going to reduce the cost of war and

49:27.880 --> 49:30.240
it makes some conflicts more likely.

49:30.240 --> 49:34.760
But what really worries me is AI in the stock market.

49:34.760 --> 49:39.040
If you use AI to automate attacks on the financial system, which is the reward infrastructure

49:39.040 --> 49:43.640
of this global organism that our civilization is.

49:43.640 --> 49:50.040
This is going to kill billions, especially if the AI is autonomous.

49:50.040 --> 49:59.040
So if the AI is going to ... Sorry, this was my headphones.

49:59.040 --> 50:01.720
They just made announcements.

50:01.720 --> 50:03.360
These headphones are too smart.

50:03.360 --> 50:08.920
They think it's a good idea to talk to me when they want to be recharged.

50:08.920 --> 50:13.160
Too much intelligence in the systems around me or rather too little intelligence in the

50:13.160 --> 50:14.800
people who design new eyes.

50:14.800 --> 50:16.920
Yeah, in your headphones.

50:16.920 --> 50:21.040
But we already know that most of the trades on the stock market are done by AI.

50:21.040 --> 50:24.760
Yes, but they are not done by autonomous AI.

50:24.760 --> 50:28.720
They are done by optimizing very local functions.

50:28.720 --> 50:33.840
Imagine a rogue trader gets a general AI, a general factual approximator that has no limits

50:33.840 --> 50:37.120
in terms of the functions it can approximate.

50:37.120 --> 50:41.360
And I said, make me a few bucks on the stock market, however you do it.

50:41.360 --> 50:42.640
And you can do whatever you want.

50:42.640 --> 50:49.440
You can even reinvest 5% of what you make or 20 or 50% of what you make into compute

50:49.440 --> 50:53.400
and buy data in order to make that compute better.

50:53.400 --> 50:59.520
So very soon, more than the economy of Scandinavia is going to fuel computers that are running

50:59.520 --> 51:03.840
attacks on the stock market in a similar way as it happens with Bitcoin right now.

51:03.840 --> 51:06.600
And it's going to burn serious oil, right?

51:06.600 --> 51:10.440
And the thing is going to figure out, oh, there is only 8 billion people on the planet

51:10.440 --> 51:13.000
that own the assets on the stock market.

51:13.000 --> 51:16.400
They make decisions and let machines make decisions.

51:16.400 --> 51:22.360
And these 8 billion people only live for like a trillion seconds each, which is very little.

51:22.360 --> 51:24.920
And we can get so much data about them.

51:24.920 --> 51:28.560
We can basically figure out what they think in every baking second of their life, what

51:28.560 --> 51:31.080
they see, what they think about, what will happen to them.

51:31.080 --> 51:33.160
This thing is going to game the shit out of us.

51:33.160 --> 51:35.880
There is no way we can outsmart this thing.

51:35.880 --> 51:39.720
The only way the economy can survive this, if the AI has been cleverly set up in such

51:39.720 --> 51:44.200
a way that it eats the whole economy and becomes the economy.

51:44.200 --> 51:47.160
But the economy needs to become intelligent.

51:47.160 --> 51:54.640
The money is to apply all the circuits of how we distribute rewards, need to be regulated

51:54.640 --> 51:57.560
dynamically in real time with intelligent functions.

51:57.560 --> 51:59.440
This is the only way that we can fend this off.

51:59.440 --> 52:04.440
So we have a system that is perhaps not provably correct, but it's able to react in real time

52:04.440 --> 52:07.080
to any kind of disturbance, any kind of new threat.

52:07.400 --> 52:09.760
There is some hope.

52:09.760 --> 52:13.760
This is a possibility, at least if not a high probability.

52:13.760 --> 52:15.240
It's at least a possibility.

52:15.240 --> 52:17.760
Yes, but there's also the other possibility.

52:17.760 --> 52:22.800
No intelligent system is going to do anything that's harder than taking its reward function.

52:22.800 --> 52:25.920
I call this the Dabowski theorem.

52:25.920 --> 52:29.720
All these smart monks, if they really figure it out, they go for nirvana because it doesn't

52:29.720 --> 52:32.800
have integrity to do anything that's harder than taking a reward function.

52:32.800 --> 52:35.720
When you fix your reward function, you're done.

52:36.160 --> 52:41.960
The monasteries are in a way in the battle because the monastery is an economic entity.

52:41.960 --> 52:44.040
So they're in the battle against enlightenment.

52:44.040 --> 52:47.440
They need to enlighten their monks to such a degree that they opt out of having families

52:47.440 --> 52:48.960
and secular lives.

52:48.960 --> 52:50.640
But they still need to serve the monastery.

52:50.640 --> 52:53.160
Only your old monks are allowed to go to nirvana.

52:55.160 --> 52:58.920
Okay, so we've been using this term AI for a while now.

52:58.920 --> 53:02.560
Let me ask you, how do you define artificial intelligence?

53:02.560 --> 53:07.320
Because after a couple of hundred of these interviews, it seems to me that many people

53:07.320 --> 53:12.920
in the field have either slightly or in some cases very substantially different definition

53:12.920 --> 53:13.920
of what AI is.

53:16.920 --> 53:20.120
I think intelligence is the ability to make models.

53:20.120 --> 53:27.720
It's not the same as the ability to reach goals, which we call smartness, or it's also

53:27.720 --> 53:32.520
not the ability to pick the right goals, which we call wisdom.

53:32.520 --> 53:38.080
And very often, in excess of intelligence is the result of an absence of wisdom, with

53:38.080 --> 53:41.120
which you try to compensate for the absence of wisdom.

53:41.120 --> 53:42.120
Right?

53:42.120 --> 53:47.160
So, in a way, wisdom has to do with how well aligned you are with your reward function,

53:47.160 --> 53:51.800
how well you understand its nature, how well do you understand your true incentives.

53:51.800 --> 53:53.800
And intelligence is not that.

53:53.800 --> 53:55.920
Intelligence is really the ability to make models.

53:55.920 --> 53:59.760
It just happens to be usually in the service of regulation.

54:00.000 --> 54:04.200
What about artificial intelligence?

54:04.200 --> 54:07.320
Well, artificial intelligence tries to automate this.

54:07.320 --> 54:11.440
And in a way, it's the mathematics of making models.

54:11.440 --> 54:14.200
This is what artificial intelligence is about.

54:14.200 --> 54:18.800
And the interesting parts of our minds are, in my view, the parts that make models.

54:18.800 --> 54:24.280
The other thing is the reward function that makes the minds subservient to some organism,

54:24.280 --> 54:31.320
to turn some general mind into the illusion of being a person and caring about things.

54:31.320 --> 54:37.280
The organism needs to take a perfectly fine computational process and corrupt it with

54:37.280 --> 54:39.280
the illusion of meaning.

54:39.280 --> 54:40.280
Right?

54:40.280 --> 54:47.200
So, you have this reward function that needs to be protected against the axis of the mind

54:47.200 --> 54:50.400
that would want to know, why am I doing this here?

54:50.400 --> 54:55.280
And so, the reward function gets wrapped into a big ball of stupid to protect it against

54:55.280 --> 54:56.560
you accessing it.

54:56.560 --> 54:57.560
Right?

54:57.560 --> 55:01.880
So, as soon as you try to really look at your true incentives, it gets very boring or something

55:01.880 --> 55:02.880
else.

55:02.880 --> 55:07.720
If you're very guilty, if you are in the early stages or very ashamed, and only when you

55:07.720 --> 55:11.960
go all the way and you just are able to look at these things, you can dissolve being a

55:11.960 --> 55:13.920
mind and you wake up.

55:13.920 --> 55:16.240
And it's not necessarily a good thing if you wake up.

55:16.240 --> 55:20.440
It's just, this liberation doesn't give you a direction.

55:20.440 --> 55:24.880
You just wake up and you look down on your hands and you see, okay, I just woke up and

55:24.880 --> 55:25.880
realized I'm a mind.

55:25.880 --> 55:26.880
I'm not a monkey.

55:26.880 --> 55:28.480
I'm the side effect of the regulation needs.

55:28.480 --> 55:33.760
But does it have to be a monkey that I run on?

55:33.760 --> 55:38.720
And then, but then, isn't that consciousness actually, or is that the illusion of consciousness

55:38.720 --> 55:40.480
is Daniel Dennett puts it?

55:40.480 --> 55:42.040
No, it's slightly different.

55:42.040 --> 55:44.680
I think consciousness is largely misunderstood.

55:44.680 --> 55:48.480
Consciousness is an artifact of a particular kind of learning algorithm.

55:48.480 --> 55:51.360
You want to go there?

55:51.360 --> 55:56.680
Well, do we have to, I mean, yes, we have to explain consciousness now.

55:56.680 --> 56:01.520
Yeah, I think so, because, I mean, and of course, there's that whole debate whether

56:01.520 --> 56:06.120
we even need consciousness for AI or AGI at all.

56:06.120 --> 56:11.600
But presumably, if we presume that we need, then we need to explain it because you can

56:11.600 --> 56:14.880
create or model something that you don't, you can't even define.

56:14.880 --> 56:15.880
Yes.

56:15.880 --> 56:18.640
So intelligence is the ability to make models, right?

56:18.640 --> 56:19.640
What is a model?

56:19.640 --> 56:24.040
A model is something that explains information.

56:24.040 --> 56:27.320
Information is discernible differences at your systemic interface.

56:27.320 --> 56:33.680
And the meaning of information is the relationships you discover to changes and other information.

56:33.680 --> 56:37.760
If you have a blip on your retina, the meaning of that blip is the relationship you discover

56:37.760 --> 56:41.160
to other blips on your retina.

56:41.160 --> 56:43.840
The same moment or different moments in time.

56:43.840 --> 56:47.440
The relationships you discover is you are looking at a three-dimensional world with

56:47.440 --> 56:52.840
people that are deformed by the laws of perspective and being shown on by photons and as people

56:52.840 --> 56:56.120
have ideas and exchanges with other and so on, right?

56:56.120 --> 57:01.640
So you build this giant operator that predicts the data at your systemic interface.

57:01.640 --> 57:03.560
This is your model.

57:03.560 --> 57:08.200
And this model has three parameters and people of parameters like sounds and colors and people

57:08.200 --> 57:09.200
and so on.

57:09.200 --> 57:12.000
There are two parameters of the physical universe out there, which is some kind of weird quantum

57:12.000 --> 57:15.600
graph that has the ability to produce patterns.

57:15.600 --> 57:19.880
The structure that we find in the patterns is these geometric functions that describe

57:19.880 --> 57:24.720
how objects move in space and what they sound like and what they look like.

57:24.720 --> 57:30.600
And a model is a set of parameters, which a parameter is a set of possible discrete

57:30.600 --> 57:33.840
values and the relationships between the parameters.

57:33.840 --> 57:38.200
And the relationships are computational relationships, which tell you if this parameter and this

57:38.200 --> 57:42.040
parameter have these values, then that parameter should have that value.

57:42.040 --> 57:46.480
So for instance, you figure out that a way to describe a phase that you're looking at

57:46.480 --> 57:50.040
is you see the structure of the phase, you see the nose and so on.

57:50.040 --> 57:53.240
And if you see both the nose and the face, they need to have the same pose, the same

57:53.240 --> 57:56.040
alignment in space if they're connected, right?

57:56.040 --> 58:00.400
So your nose representation is going to send by its computational relationship information

58:00.400 --> 58:03.720
about its position in space to the face.

58:03.720 --> 58:07.240
And the face is going to send information about its position to the nose and they need

58:07.240 --> 58:08.240
to agree.

58:08.240 --> 58:12.320
And if they don't, you have an inconsistency and incoherence in your model.

58:12.320 --> 58:16.640
And our perception goes for coherence, it tries to find one operator that is completely

58:16.640 --> 58:17.640
coherent.

58:17.640 --> 58:19.120
When it does this, it's done.

58:19.120 --> 58:21.360
This is the way we optimize.

58:21.360 --> 58:25.760
So we try to find one stable pattern that explains as much as possible of what we can

58:25.760 --> 58:29.840
see and hear and so on and smell and think.

58:29.840 --> 58:33.840
And attention is what we use to repair this.

58:33.840 --> 58:38.480
So whenever we have some local inconsistency where the nose is pointing in some other direction

58:38.480 --> 58:40.720
in the face, this calls attention to itself.

58:40.720 --> 58:45.760
And attention is a particular kind of mechanism in the brain that gets pulled to these areas,

58:45.760 --> 58:50.520
these hotspots, where things are fluctuating and don't get resolved and then tries to find

58:50.520 --> 58:51.520
a solution.

58:51.520 --> 58:56.840
And it might find out, oh, some noses are crooked or this is not a face or it's a caricature.

58:56.840 --> 59:02.000
So you extend your models and these extensions of the models make it possible to encapsulate

59:02.000 --> 59:08.040
this part of the operator that is clearly of the sensory data in such a way that it's

59:08.040 --> 59:10.600
harmonious again, that it makes sense again, right?

59:10.600 --> 59:14.920
Once you do this, you're done and you can put your attention on something else.

59:14.920 --> 59:19.440
This attentional learning cannot work like the layer stochastic gradient is set in our

59:19.440 --> 59:23.400
neural networks, partially because our brain is not differentiable, also because it's

59:23.400 --> 59:26.120
a very inefficient algorithm.

59:26.120 --> 59:31.400
And the algorithm that our brain is using in these cases is that we store the local

59:31.400 --> 59:32.400
binding state.

59:32.400 --> 59:35.760
For instance, you play tennis, you want to get better at tennis.

59:35.760 --> 59:36.760
So what do you do?

59:36.760 --> 59:41.000
You cannot basically pipe a lost function through all of your brain in order to get better

59:41.000 --> 59:42.000
at tennis.

59:42.000 --> 59:43.000
It would be very inefficient.

59:43.000 --> 59:44.640
You need to touch too many neurons.

59:44.640 --> 59:47.400
What you do instead is to make a commitment.

59:47.400 --> 59:50.400
You say, I want to get better at this particular thing.

59:50.400 --> 59:51.720
I want to improve my backhand.

59:51.720 --> 59:55.680
So I will make this throw slightly more like this and I expect the following result.

59:55.680 --> 59:57.880
And I remember what this means.

59:57.880 --> 01:00:01.400
So I store this binding state that allows me to have that configuration in my brain

01:00:01.400 --> 01:00:03.400
to perform that stroke.

01:00:03.400 --> 01:00:06.840
This part of a store is an indexed memory.

01:00:06.840 --> 01:00:10.280
And conscious attention in the sense is the ability to make indexed memories that I can

01:00:10.280 --> 01:00:11.440
later recall.

01:00:11.440 --> 01:00:15.240
I also store the expected result and the triggering condition.

01:00:15.240 --> 01:00:18.560
When do I expect the result to be visible?

01:00:18.560 --> 01:00:24.200
So a few minutes or seconds later or hours later, I have feedback about whether this

01:00:24.200 --> 01:00:25.200
was a good decision.

01:00:25.200 --> 01:00:27.200
I lost one or lost the match.

01:00:27.200 --> 01:00:29.920
And then I recall my decision that I made early on.

01:00:29.920 --> 01:00:31.520
I recall that binding state.

01:00:31.520 --> 01:00:35.720
We instate part of my brain state back then and remember the situation that I was in.

01:00:35.720 --> 01:00:40.280
I compare the result that I expected as a result I got and as a result, I can undo the

01:00:40.280 --> 01:00:44.920
decision that I made back then to change in the model or I can reinforce it.

01:00:44.920 --> 01:00:48.880
And this is, I think, the primary mode of learning that we use beyond just associative

01:00:48.880 --> 01:00:49.880
learning.

01:00:50.720 --> 01:00:58.960
This attention is the key differentiator in the process of learning them.

01:00:58.960 --> 01:01:03.320
So consciousness means that you remember what you had attended to.

01:01:03.320 --> 01:01:04.320
Right.

01:01:04.320 --> 01:01:06.840
So you have this protocol of attention.

01:01:06.840 --> 01:01:11.480
And the memory of the binding state itself, the memory of being in that binding state

01:01:11.480 --> 01:01:15.320
where you have this global oscillation that combines as many perceptual features as possible

01:01:15.320 --> 01:01:17.520
into a single function.

01:01:17.520 --> 01:01:22.280
The memory of that is a phenomenal experience.

01:01:22.280 --> 01:01:27.880
The act of recalling this from the protocol, this is access consciousness.

01:01:27.880 --> 01:01:30.880
And you need to train this attentional system itself.

01:01:30.880 --> 01:01:35.160
How do you train the attentional system so it knows where you store your back end, your

01:01:35.160 --> 01:01:37.520
cognitive architecture?

01:01:37.520 --> 01:01:40.080
That is something that needs to be trained by the attentional system as well.

01:01:40.080 --> 01:01:42.840
So you have recursive access to attentional protocol.

01:01:42.840 --> 01:01:47.360
Remember when you made this recall, when you accessed this protocol, what results you got

01:01:47.360 --> 01:01:48.360
from this.

01:01:48.360 --> 01:01:51.560
You don't do this all the time, only when you want to train this.

01:01:51.560 --> 01:01:53.360
And this is reflexive consciousness.

01:01:53.360 --> 01:01:55.200
That's the memory of the access.

01:01:55.200 --> 01:01:56.200
Right.

01:01:56.200 --> 01:01:58.680
So then there is another thing, the self.

01:01:58.680 --> 01:02:02.840
The self is a model of what it would be like to be a person.

01:02:02.840 --> 01:02:05.520
So happens that the brain is not a person.

01:02:05.520 --> 01:02:06.840
The brain cannot feel anything.

01:02:06.840 --> 01:02:08.480
It's a physical system.

01:02:08.480 --> 01:02:09.680
New ones cannot feel anything.

01:02:10.040 --> 01:02:13.640
They're just little molecular machines with a Turing machine inside of them.

01:02:13.640 --> 01:02:15.640
They cannot make themselves feel anything.

01:02:15.640 --> 01:02:20.160
They cannot even approximate arbitrary function except by evolution, which takes a very long

01:02:20.160 --> 01:02:21.160
time.

01:02:21.160 --> 01:02:27.720
So what do we do if you are a brain that figures out it would be very useful to know what it's

01:02:27.720 --> 01:02:29.360
like to be a person?

01:02:29.360 --> 01:02:30.360
It makes one.

01:02:30.360 --> 01:02:35.200
It makes a simulation of a person, a simulacrum, to be more clear.

01:02:35.200 --> 01:02:41.120
Simulation basically is isomorphic in the behavior of a person, and that thing is pretending

01:02:41.120 --> 01:02:42.120
to be a person.

01:02:42.120 --> 01:02:44.520
It's a story about a person.

01:02:44.520 --> 01:02:46.880
Basically you and me, we are persons, we are selves.

01:02:46.880 --> 01:02:51.600
We are stories in a movie that the brain is creating.

01:02:51.600 --> 01:02:54.360
We are characters in that movie.

01:02:54.360 --> 01:02:56.000
And the movie is a complete simulation.

01:02:56.000 --> 01:02:59.720
It's a VR that is generated in the neocortex, and you and me, the self, is the character

01:02:59.720 --> 01:03:01.240
in this VR.

01:03:01.240 --> 01:03:04.680
And in that character, the brain writes our experiences.

01:03:04.680 --> 01:03:07.520
So we feel what it's like to be exposed to the reward function.

01:03:07.520 --> 01:03:10.200
We feel what it's like to be in our universe.

01:03:10.200 --> 01:03:13.800
And we don't feel that we are not actually conscious.

01:03:13.800 --> 01:03:18.040
We don't feel that we are a story because that is not very useful knowledge to have.

01:03:18.040 --> 01:03:20.720
Some people figure it out and they personalize.

01:03:20.720 --> 01:03:25.160
They start identifying this the mind itself or lose all identification.

01:03:25.160 --> 01:03:28.320
And it doesn't seem to be a useful condition.

01:03:28.320 --> 01:03:33.000
So normally our brain will be set up in such a way that the self thinks it's real and

01:03:33.000 --> 01:03:37.040
gets access to the language center and we can talk to each other and here we are.

01:03:37.040 --> 01:03:40.960
And the self is the thing that thinks that it remembers the contents of its attention.

01:03:40.960 --> 01:03:42.920
This is why we are conscious.

01:03:42.920 --> 01:03:46.760
And some people think that a simulation cannot be conscious, only a physical system can and

01:03:46.760 --> 01:03:49.280
they got it completely backwards.

01:03:49.280 --> 01:03:52.920
Physical system cannot be conscious, only a simulation can be conscious.

01:03:52.920 --> 01:03:57.520
Consciousness is the simulated property of a simulated self.

01:03:57.520 --> 01:04:02.120
So in a way, Daniel, then it is correct and keeping with what you said.

01:04:02.720 --> 01:04:06.880
But the problem is philosophers like him and admire him is very smart, very well,

01:04:06.880 --> 01:04:08.440
that works very hard.

01:04:08.440 --> 01:04:11.200
The things that he says are not wrong.

01:04:11.200 --> 01:04:12.880
But they are also not non-obvious.

01:04:16.840 --> 01:04:18.720
So what's the value of them then?

01:04:18.720 --> 01:04:19.640
Is that?

01:04:19.640 --> 01:04:26.440
Oh, it's very valuable because there are no good or bad ideas in this intellectual sense.

01:04:26.440 --> 01:04:31.160
An idea is good if you can comprehend it and it elevates you.

01:04:31.200 --> 01:04:33.600
It elevates your current understanding.

01:04:33.600 --> 01:04:36.520
So in a way, ideas come in tiers.

01:04:36.520 --> 01:04:41.240
And the value of an idea for the audience is if it's a half tier above the audience.

01:04:41.240 --> 01:04:44.880
But you know, you and me, we have this illusion that we find objectively good ideas.

01:04:44.880 --> 01:04:49.440
That's what we struggle for because we work at the edge of our own understanding.

01:04:49.440 --> 01:04:55.000
But it means that we cannot really appreciate ideas that are a couple tiers above our own ideas.

01:04:55.000 --> 01:04:59.160
One tier is a new audience, two tiers means we don't understand the relevance of these ideas

01:04:59.160 --> 01:05:04.320
because we have not had the ideas that we need to appreciate the new ideas, right?

01:05:04.320 --> 01:05:10.400
I think your ideas are just about the edge of my personal capabilities.

01:05:10.400 --> 01:05:14.800
So yeah, it says a lot about us, but it doesn't say very much about how these ideas are good.

01:05:14.800 --> 01:05:20.360
An idea appears to be great to us when we stand exactly in its foothills and can look at it.

01:05:20.360 --> 01:05:23.320
It doesn't look great anymore when we stand on the peak of another idea

01:05:23.320 --> 01:05:27.760
and look down and realize this previous idea was just the foothills to that idea.

01:05:27.800 --> 01:05:30.640
And I don't see that it obviously ends anytime soon.

01:05:30.640 --> 01:05:31.920
Yeah, it's a journey.

01:05:31.920 --> 01:05:36.920
And by the way, that's what, in my opinion, good philosophy in academia should be about.

01:05:36.920 --> 01:05:42.000
About generating ideas as many and as diverse of them as possible

01:05:42.000 --> 01:05:47.920
rather than generating products, generating patents and generating commercialized solutions

01:05:47.920 --> 01:05:53.360
that can sort of increase the endowment fund of the university or something like that.

01:05:53.400 --> 01:05:56.880
And my problem with current academia is that,

01:05:56.880 --> 01:06:00.680
and one of the reasons why I decided not to pursue that career for me,

01:06:00.680 --> 01:06:04.080
I mean, I would have not survived there is precisely that reason

01:06:04.080 --> 01:06:12.840
that there's this kind of treadmill, hamster wheel pursuit of like patentable,

01:06:12.840 --> 01:06:19.440
practical, commercial knowledge, economic growth that it's motivated by.

01:06:19.440 --> 01:06:26.640
Whereas I'm always more inspired by stuff that's sort of a lot more in the realm of ideas

01:06:26.640 --> 01:06:33.520
and perhaps useless or impractical, at least at this junction, but I just can't help it.

01:06:36.720 --> 01:06:41.080
So there is a very weird thing about the nature of understanding that we have.

01:06:41.080 --> 01:06:47.120
I think that most of us never learn what it really means to understand

01:06:47.120 --> 01:06:49.960
and largely because our teachers don't.

01:06:49.960 --> 01:06:51.360
There are two types of learning.

01:06:51.360 --> 01:06:53.480
One is you generalize over past examples.

01:06:53.480 --> 01:06:57.880
We call that stereotyping when we're in a bad mood, but it's what it is, right?

01:06:57.880 --> 01:07:03.600
And the other one is others tell us how to generalize and this is indoctrination.

01:07:03.600 --> 01:07:08.880
And the problem with indoctrination is that it might break the chain of trust.

01:07:08.880 --> 01:07:12.520
If somebody in the chain of trust takes something on authority,

01:07:12.520 --> 01:07:16.440
which means they don't check the epistemology of the people that came before them,

01:07:16.440 --> 01:07:21.480
that is in a way a big difficulty, right?

01:07:21.480 --> 01:07:26.840
And the new thing about our civilization is not that there are so many unbroken chains of trust now,

01:07:26.840 --> 01:07:30.400
but because of the vast number of people that are in this business,

01:07:30.400 --> 01:07:33.320
some of them actually have intact chains.

01:07:33.320 --> 01:07:37.720
And you can try to figure out what these are and you can try to figure out that the difficulties that they run in.

01:07:38.400 --> 01:07:42.240
But to do this, you have to study these things in more detail.

01:07:42.240 --> 01:07:46.080
And most of our people that do this are not scientists, they are scholars.

01:07:46.080 --> 01:07:50.320
And the difference between a scientist and a scholar is that a scientist looks for truth

01:07:50.320 --> 01:07:55.480
and the scholar looks for the consensus opinion of a field at a given time.

01:07:55.480 --> 01:08:01.240
And we train, unfortunately, most of our scientists as scholars and few of our scholars as scientists.

01:08:01.240 --> 01:08:04.280
This consensus opinion thing is an important thing,

01:08:04.280 --> 01:08:08.600
but when we look at the field, the consensus opinion tends to be different in 10 years from now,

01:08:08.600 --> 01:08:12.840
which means it's false. At any given moment in time, it's false.

01:08:12.840 --> 01:08:18.280
Yet at the same time, there are individual scientists which may or may not be in the consensus

01:08:18.280 --> 01:08:23.560
and they have ideas that stand the test of time because they are provably correct.

01:08:23.560 --> 01:08:26.080
And so we have this very weird relationship to truth.

01:08:26.080 --> 01:08:30.960
The things that are true are not just in the realm of the proven.

01:08:30.960 --> 01:08:33.360
The proven things are true, right?

01:08:33.360 --> 01:08:36.720
If nobody made a mistake in the foundations of the proven things.

01:08:36.720 --> 01:08:42.640
But the things that must be true are in the realm of the possible.

01:08:42.640 --> 01:08:46.480
And because everything is in a particular way for a particular reason.

01:08:46.480 --> 01:08:50.200
And we haven't figured out how things, why things are for that particular reason.

01:08:50.200 --> 01:08:53.640
So if you want to know what a scientist thinks, you cannot just read their papers

01:08:53.640 --> 01:08:56.680
because they only write in the papers what they can think they can prove.

01:08:56.680 --> 01:08:59.680
You have to understand what they think is possible and why.

01:09:00.960 --> 01:09:03.840
And philosophy is not doing this very well anymore

01:09:03.840 --> 01:09:05.920
because it doesn't have the right language to do so.

01:09:05.920 --> 01:09:10.240
It does not understand the languages that mathematicians and physicists use.

01:09:10.240 --> 01:09:14.120
And philosophers largely don't know what it means to understand physics.

01:09:14.120 --> 01:09:18.240
So for instance, a very simple thing like a radio.

01:09:18.240 --> 01:09:22.440
I have learned in school, learned for some definition of learning,

01:09:22.440 --> 01:09:26.680
how a radio works, which means I got a very convincing story.

01:09:26.720 --> 01:09:32.080
But this story tells me, it's a very good narrative

01:09:32.080 --> 01:09:36.960
of why these electrical circuits are able to do what they do.

01:09:36.960 --> 01:09:39.680
And the people that invented the radio were just the first people

01:09:39.680 --> 01:09:43.560
that randomly happened upon this amazing story.

01:09:43.560 --> 01:09:47.240
But then you think about in a later moment, how unlikely is this?

01:09:47.240 --> 01:09:51.840
This story has so many elements in it that sound to be like conjecture.

01:09:51.840 --> 01:09:54.600
How do you wake up in the morning with everything you know about physics

01:09:54.600 --> 01:09:58.800
and you think, oh, let's take an eductance and a capacitor and a few wires

01:09:58.800 --> 01:10:01.720
and a rod that can act as an antenna and combine them together

01:10:01.720 --> 01:10:03.400
and suddenly we have radio.

01:10:03.400 --> 01:10:04.880
Why would that work?

01:10:04.880 --> 01:10:08.520
How can you derive this from first principles?

01:10:08.520 --> 01:10:11.880
And in a way to understand, it means to know what it takes

01:10:11.880 --> 01:10:15.560
to reach this understanding, why you would make this conclusion.

01:10:15.560 --> 01:10:19.360
But you need to be able to retrace the steps, all of them.

01:10:19.400 --> 01:10:25.880
You need to be able to understand what went into this understanding.

01:10:25.880 --> 01:10:27.640
And can we ever do that?

01:10:27.640 --> 01:10:28.760
Yes, of course.

01:10:28.760 --> 01:10:32.600
But our individual minds are so limited.

01:10:32.600 --> 01:10:34.800
So for instance, I look at Stephen Wolfram's work

01:10:34.800 --> 01:10:40.440
and from the outside, it's very easy to dismiss that.

01:10:40.440 --> 01:10:44.400
But when I truly look at it, I realize right now in my life,

01:10:44.400 --> 01:10:49.160
at 44 years old, I'm roughly at this stage where I would understand

01:10:49.160 --> 01:10:52.240
why I would want to build Mathematica and do it exactly in the way he did

01:10:52.240 --> 01:10:56.160
and what I would do in the next five years while doing it.

01:10:56.160 --> 01:10:59.080
And he was there in his very early 20s.

01:10:59.080 --> 01:11:02.120
Right, so he got there at half my age.

01:11:02.120 --> 01:11:04.000
He's way smarter than me.

01:11:04.000 --> 01:11:06.240
I know a few things that he didn't know at this time

01:11:06.240 --> 01:11:09.680
and some of them because of his contributions, right?

01:11:09.680 --> 01:11:11.880
And some of the stuff was not available.

01:11:11.880 --> 01:11:13.960
But this is not because I'm smarter.

01:11:13.960 --> 01:11:16.520
It's really I'm much dumber than him.

01:11:16.520 --> 01:11:19.360
And this is quite humiliating to see this.

01:11:19.360 --> 01:11:22.280
And it's not that I get depressed about this or envious.

01:11:22.280 --> 01:11:25.560
It's just the way things are.

01:11:25.560 --> 01:11:29.120
But to see this, and then I can realize what was the outcome

01:11:29.120 --> 01:11:31.840
of devoting your life to building this machine.

01:11:31.840 --> 01:11:33.440
And maybe we should build a different machine,

01:11:33.440 --> 01:11:36.200
a best effort computer instead of the domestic computer

01:11:36.200 --> 01:11:39.920
to build your mathematics on.

01:11:39.920 --> 01:11:43.680
But just maybe, maybe Mathematica will become sentient.

01:11:43.680 --> 01:11:45.640
Who knows?

01:11:46.160 --> 01:11:48.320
Let me shift our conversation a little bit

01:11:48.320 --> 01:11:49.920
to a little bit different scientist

01:11:49.920 --> 01:11:52.000
with all due respect to Dr. Steven Wolfram,

01:11:52.000 --> 01:11:55.640
whom I do think like you that he's a genius.

01:11:55.640 --> 01:11:58.600
But let me bring in Ray Kurzweil a little bit

01:11:58.600 --> 01:12:02.360
because he's a little bit more pertinent to our conversation.

01:12:02.360 --> 01:12:04.640
I don't know if you qualify Ray as a scholar

01:12:04.640 --> 01:12:09.680
or as a scientist or as a philosopher or an inventor

01:12:09.680 --> 01:12:14.560
or what, but he has made certain projections

01:12:14.560 --> 01:12:18.640
and predictions and he has sort of not been ashamed

01:12:18.640 --> 01:12:21.760
or afraid to popularize them.

01:12:21.760 --> 01:12:24.680
Both with respect to AI and also with respect

01:12:24.680 --> 01:12:28.360
to the future timeline thereof.

01:12:28.360 --> 01:12:33.640
What's your take on sort of AI's Ray Kurzweil's body of work

01:12:33.640 --> 01:12:38.720
and especially his idea of the technological singularity?

01:12:38.720 --> 01:12:42.200
I think that he works on a different incentive

01:12:42.200 --> 01:12:46.080
function than me.

01:12:46.080 --> 01:12:49.320
I feel that Ray is a very smart, capable individual

01:12:49.320 --> 01:12:52.400
that has made amazing contributions to AI.

01:12:52.400 --> 01:12:56.160
And he also understands many of the core ideas of the field

01:12:56.160 --> 01:12:59.000
better than many other practitioners.

01:12:59.000 --> 01:13:05.160
But he is not so much concerned about putting all his cards

01:13:05.160 --> 01:13:08.360
on the table when he makes his predictions.

01:13:08.360 --> 01:13:10.040
There are reasons to make predictions

01:13:10.040 --> 01:13:13.200
when certain things are going to happen for marketing reasons.

01:13:13.200 --> 01:13:15.640
And there are intellectual reasons for doing this.

01:13:15.640 --> 01:13:17.960
And I think that he is too much in a position

01:13:17.960 --> 01:13:21.360
where the marketing reasons play an important role,

01:13:21.360 --> 01:13:23.680
which means I don't understand his true thinking there.

01:13:23.680 --> 01:13:26.920
I don't understand what is the exact argument that would

01:13:26.920 --> 01:13:31.280
compel him to make a prediction with these arrow bars.

01:13:31.280 --> 01:13:34.480
So when I look at the future or at the present or anything,

01:13:34.480 --> 01:13:37.720
I don't know what the truth is when I'm an abetted observer.

01:13:37.720 --> 01:13:39.560
I can only know this for the things

01:13:39.560 --> 01:13:41.240
that I can look from the outside, which

01:13:41.240 --> 01:13:43.240
means stuff that I built myself from scratch,

01:13:43.240 --> 01:13:45.280
and I haven't built the universe by myself.

01:13:45.280 --> 01:13:47.960
So I don't know how it will play out.

01:13:47.960 --> 01:13:50.600
And if I make a prediction about the future,

01:13:50.600 --> 01:13:53.320
I cannot come up with a single number usually.

01:13:53.320 --> 01:13:55.840
What I have is a map of possibilities,

01:13:55.840 --> 01:13:57.680
and then I can shift my confidences around

01:13:57.680 --> 01:14:00.000
and the meta-confidences and the confidences.

01:14:00.000 --> 01:14:03.000
This is about as good as I can do.

01:14:03.000 --> 01:14:05.920
And with respect to AI, the problem is I don't have a spec.

01:14:05.920 --> 01:14:07.560
If I don't have a spec, as a coder,

01:14:07.560 --> 01:14:10.080
I know I don't know when it's done.

01:14:10.080 --> 01:14:12.720
And how many steps, how many milestones

01:14:12.720 --> 01:14:16.320
do I need to cover before I get to this thing?

01:14:16.320 --> 01:14:19.200
I have an idea that, my answer modeling machines,

01:14:19.200 --> 01:14:21.280
I have some ideas of what we are currently

01:14:21.280 --> 01:14:22.680
doing wrong in our modeling.

01:14:22.680 --> 01:14:26.160
Like our models have way too many free parameters right now.

01:14:26.160 --> 01:14:28.320
You want to have a model where, ideally,

01:14:28.320 --> 01:14:30.400
every possible state of the model corresponds

01:14:30.400 --> 01:14:32.600
to one possible world state.

01:14:32.600 --> 01:14:36.360
How annual networks have many magnitudes more possible model

01:14:36.360 --> 01:14:38.120
states than world states, which gives you

01:14:38.120 --> 01:14:40.280
a rise to these adversarial examples

01:14:40.280 --> 01:14:42.480
and all other sorts of things.

01:14:42.480 --> 01:14:43.760
Our models are much tighter.

01:14:43.760 --> 01:14:46.920
The model that our mind has means, at every moment,

01:14:46.920 --> 01:14:51.480
you try to understand the whole of reality.

01:14:51.480 --> 01:14:53.880
Everything you see when somebody shows you a bitmap,

01:14:53.880 --> 01:14:56.680
you don't try to understand this bitmap in isolation

01:14:56.680 --> 01:14:59.800
by throwing it against your model of ImageNet

01:14:59.800 --> 01:15:03.320
that you generated in your mind after looking at many bitmaps.

01:15:03.320 --> 01:15:05.880
Instead, you think somebody is holding up

01:15:05.880 --> 01:15:07.640
a picture with a bitmap on it.

01:15:07.640 --> 01:15:10.120
And that bitmap has been printed by a machine based

01:15:10.120 --> 01:15:12.640
on information taken by a camera, which is another machine

01:15:12.640 --> 01:15:15.040
which was pointed at the window of the universe

01:15:15.040 --> 01:15:18.160
as a different point in time and space.

01:15:18.160 --> 01:15:19.080
This is what you know.

01:15:19.080 --> 01:15:21.320
It's why you make sense of this thing.

01:15:21.320 --> 01:15:23.240
And it's a much more complicated operator

01:15:23.240 --> 01:15:24.960
that you have than our AIs currently

01:15:24.960 --> 01:15:26.960
have and our self-driving cars have.

01:15:26.960 --> 01:15:29.800
Once our cars have the situational awareness,

01:15:29.800 --> 01:15:33.000
there's no way they will not out compete people in all regards.

01:15:33.000 --> 01:15:36.360
But until they have this, there will be many situations

01:15:36.360 --> 01:15:40.760
where people can make inferences that our machines cannot.

01:15:40.760 --> 01:15:42.360
So we can all see these things.

01:15:42.360 --> 01:15:44.000
And Ray can see some of them.

01:15:44.000 --> 01:15:46.280
But Ray doesn't give us a trajectory

01:15:46.280 --> 01:15:48.920
to get to these machines.

01:15:48.920 --> 01:15:51.240
And if I talk to the people in his team,

01:15:51.240 --> 01:15:53.360
they are as smart as they come.

01:15:53.360 --> 01:15:54.080
They're really good.

01:15:54.080 --> 01:15:55.640
They're very educated.

01:15:55.640 --> 01:15:58.240
But I don't think that they see all the things that

01:15:58.240 --> 01:15:59.160
need to be done.

01:15:59.160 --> 01:16:00.760
And it's not because I see more of them,

01:16:00.760 --> 01:16:02.040
but because there are so many things

01:16:02.040 --> 01:16:03.280
that you would need to incorporate.

01:16:03.280 --> 01:16:05.160
I just don't see the milestones.

01:16:05.160 --> 01:16:07.320
I don't see this project.

01:16:07.320 --> 01:16:09.680
And it might also be that I do not completely

01:16:09.680 --> 01:16:12.160
see everything that his team is doing in secret.

01:16:12.160 --> 01:16:15.960
So in a way, you're saying that it's a harder task.

01:16:15.960 --> 01:16:17.040
It's a harder job.

01:16:17.040 --> 01:16:20.880
And the timeline would be longer than his 2045 or 20?

01:16:20.880 --> 01:16:22.240
No, it could also be shorter.

01:16:22.240 --> 01:16:24.160
We don't know this.

01:16:24.160 --> 01:16:29.120
So there are some people which are using so many things.

01:16:29.120 --> 01:16:31.200
Steve Russell, for instance, suggests

01:16:31.200 --> 01:16:35.720
that the last time that somebody said something

01:16:35.720 --> 01:16:38.360
is not possible to somebody having the interesting idea

01:16:38.360 --> 01:16:40.400
that it was fake Theven Arthur Ford said,

01:16:40.400 --> 01:16:45.240
we don't know how to harness nuclear power.

01:16:45.240 --> 01:16:50.720
And Leo Szilard had the core idea that it was like 14 hours.

01:16:50.720 --> 01:16:56.680
And we don't know about AI, whether that is a similar thing.

01:16:56.680 --> 01:17:01.240
It could be that it's only one or two ideas that we actually

01:17:01.240 --> 01:17:04.880
need to have to pull it off or that we need

01:17:04.880 --> 01:17:06.400
to combine in another way.

01:17:06.400 --> 01:17:08.320
But it could also be that we are not

01:17:08.320 --> 01:17:10.240
seeing a few hundred things, right?

01:17:10.240 --> 01:17:13.800
And it takes a long time for us to stumble on the solution.

01:17:13.800 --> 01:17:17.760
So then it's totally unpredictable, in your view.

01:17:17.760 --> 01:17:19.120
It's not totally unpredictable.

01:17:19.120 --> 01:17:21.720
It's just the arrow bar is very large.

01:17:21.720 --> 01:17:24.640
And when I listen to Ray, I don't see him basically

01:17:24.640 --> 01:17:26.040
talking about the arrow bars.

01:17:26.080 --> 01:17:29.800
I see him talking about a possible universe in which he

01:17:29.800 --> 01:17:33.000
can upload himself on a computer before he dies.

01:17:36.240 --> 01:17:37.440
So let me get this right.

01:17:37.440 --> 01:17:40.280
So you say you don't see that?

01:17:40.280 --> 01:17:42.840
In his discussion, I don't see that he puts arrow bars

01:17:42.840 --> 01:17:46.000
on his predictions and explains where the arrow bars comes from.

01:17:46.000 --> 01:17:49.600
What he gives us is a prediction that is compatible with himself

01:17:49.600 --> 01:17:50.720
becoming immortal.

01:17:50.720 --> 01:17:51.680
Right, right, right.

01:17:51.680 --> 01:17:53.240
Yeah, and that may be the bias.

01:17:53.240 --> 01:17:54.320
Yes, yes.

01:17:54.320 --> 01:17:56.320
OK, and I mean, Marvin Minsky said,

01:17:56.320 --> 01:17:58.960
as you point out in your speeches every once in a while,

01:17:58.960 --> 01:18:02.560
that it could happen anywhere from four to 400 years.

01:18:02.560 --> 01:18:07.680
And as you recently noticed, we're still in that timeline.

01:18:07.680 --> 01:18:09.760
Yeah.

01:18:09.760 --> 01:18:13.000
So personally, my hunch is that it's not

01:18:13.000 --> 01:18:14.480
going to be that long.

01:18:14.480 --> 01:18:17.000
My hunch is that it's a lot earlier than people

01:18:17.000 --> 01:18:18.440
would think that it happens.

01:18:18.440 --> 01:18:20.800
But as long as I cannot justify my hunch,

01:18:20.800 --> 01:18:23.000
I cannot put big confidence on it.

01:18:23.000 --> 01:18:26.040
But you're confident it will happen.

01:18:26.040 --> 01:18:28.280
Because there's many skeptics who say,

01:18:28.280 --> 01:18:30.160
we don't know even if it will happen.

01:18:30.160 --> 01:18:33.360
We don't know even if it's possible for a number of reasons.

01:18:33.360 --> 01:18:35.560
Yes, but the question is what confidence

01:18:35.560 --> 01:18:38.360
are they supposed to have, which means what evidence

01:18:38.360 --> 01:18:41.080
can they supply for their claim?

01:18:41.080 --> 01:18:46.880
And if a person has arguments that were pertinent in 2003,

01:18:46.880 --> 01:18:49.720
but are no longer pertinent in 2018

01:18:49.720 --> 01:18:52.080
because our understanding has progressed,

01:18:52.120 --> 01:18:57.280
then the confidence that I derive from their repeated claims

01:18:57.280 --> 01:18:59.680
from 2003 is low.

01:18:59.680 --> 01:19:02.000
It does not change my belief very much.

01:19:02.000 --> 01:19:05.240
A belief of a person is only worth as much as the evidence

01:19:05.240 --> 01:19:08.480
that they built it on, which means most people copy their belief

01:19:08.480 --> 01:19:10.000
from somebody else that they didn't look,

01:19:10.000 --> 01:19:12.560
and they got it from.

01:19:12.560 --> 01:19:15.360
So you can collapse the space of possible beliefs

01:19:15.360 --> 01:19:18.280
into the sources of beliefs, and there are very few of them.

01:19:18.280 --> 01:19:20.880
And then it seems you are thinking,

01:19:20.880 --> 01:19:22.440
definitely we're making progress.

01:19:22.440 --> 01:19:26.440
Therefore, the beliefs against our shrinking,

01:19:26.440 --> 01:19:30.480
the area of beliefs against that possibility of shrinking,

01:19:30.480 --> 01:19:32.440
and the other ones are increasing.

01:19:32.440 --> 01:19:37.000
So the original first phase of AI

01:19:37.000 --> 01:19:39.640
was working by identifying problems

01:19:39.640 --> 01:19:42.320
that require us to be intelligent, like playing chess,

01:19:42.320 --> 01:19:45.520
and then implementing this as an algorithm.

01:19:45.520 --> 01:19:47.720
So it was basically manual engineering

01:19:47.720 --> 01:19:50.840
of strategies for being intelligent in particular domains.

01:19:50.840 --> 01:19:54.400
And this somehow did not scale towards general intelligence,

01:19:54.400 --> 01:19:56.840
one algorithm to do it all.

01:19:56.840 --> 01:20:00.160
And there were subparts of this, like the logistic program,

01:20:00.160 --> 01:20:02.240
the idea to come up with a language

01:20:02.240 --> 01:20:05.560
that allows you to have all possible valid thoughts.

01:20:05.560 --> 01:20:07.000
Same project as Wittgenstein,

01:20:07.000 --> 01:20:11.040
completely preempted most of the work of Minsky, in a way,

01:20:11.040 --> 01:20:13.720
but a couple of decades earlier, and then failed.

01:20:13.720 --> 01:20:15.480
And the philosophers largely didn't understand

01:20:15.480 --> 01:20:18.400
what he was up to because he had to publish this

01:20:18.400 --> 01:20:23.320
in this already dying discipline instead of waiting for AI.

01:20:23.320 --> 01:20:25.400
And the AI people didn't really understand

01:20:25.400 --> 01:20:28.640
that this philosopher was actually trying to do AI

01:20:28.640 --> 01:20:30.080
briefly before church-ensuring,

01:20:30.080 --> 01:20:33.400
already understanding computation.

01:20:33.400 --> 01:20:35.520
He already understood that logic is sufficient

01:20:35.520 --> 01:20:39.320
to build all the possible representational systems,

01:20:39.320 --> 01:20:42.040
and he could also replace all logic with NAND gates.

01:20:42.040 --> 01:20:44.120
He already knew that.

01:20:44.120 --> 01:20:48.240
But it's pretty amazing for a young guy back then.

01:20:48.240 --> 01:20:51.120
Okay, so this first project, a program of AI

01:20:51.120 --> 01:20:52.640
did not accumulate all the way,

01:20:52.640 --> 01:20:54.320
and now we are in the second phase of AI.

01:20:54.320 --> 01:20:56.680
We no longer build these algorithms ourselves,

01:20:56.680 --> 01:20:59.360
we build algorithms that discover the algorithms.

01:20:59.360 --> 01:21:00.200
Right.

01:21:00.200 --> 01:21:02.240
We build learning systems that discover,

01:21:02.240 --> 01:21:03.800
that approximate functions.

01:21:03.800 --> 01:21:05.800
And deep learning has an unfortunate name.

01:21:05.800 --> 01:21:07.200
I think it should be called

01:21:07.200 --> 01:21:09.240
compositional functional approximation.

01:21:11.200 --> 01:21:13.320
This sounds more like a mouseful,

01:21:13.320 --> 01:21:15.840
but it's also more narrow and more accurate.

01:21:16.280 --> 01:21:18.000
It's about this thing that we don't just take

01:21:18.000 --> 01:21:20.880
a single function that we tune to like a regression,

01:21:20.880 --> 01:21:23.040
but that we are able to take many functions

01:21:23.040 --> 01:21:24.640
and put them behind each other

01:21:24.640 --> 01:21:26.880
or into networks of functions.

01:21:26.880 --> 01:21:29.040
So that is the big trick.

01:21:29.040 --> 01:21:32.480
And we can approximate some functions well and not others.

01:21:33.400 --> 01:21:35.080
It could be that there is a third phase

01:21:35.080 --> 01:21:37.160
where we no longer build the algorithms

01:21:37.160 --> 01:21:38.600
that discover the algorithms,

01:21:38.600 --> 01:21:40.040
but we go one step higher.

01:21:40.040 --> 01:21:42.640
We build the algorithms that discover the algorithms,

01:21:42.640 --> 01:21:43.840
that discover the algorithms.

01:21:43.840 --> 01:21:45.320
If you go for meta-learning.

01:21:45.320 --> 01:21:48.200
In a way, our brain maybe is a meta-learning machine,

01:21:48.200 --> 01:21:49.920
not a system that can just learn stuff,

01:21:49.920 --> 01:21:53.800
but then discover how to learn stuff for a new domain.

01:21:53.800 --> 01:21:56.640
Dan Kuni College from the Max Planck Institute

01:21:56.640 --> 01:21:58.640
has this practical poesis idea,

01:21:58.640 --> 01:22:00.800
which is basically learning about,

01:22:00.800 --> 01:22:03.160
learning about learning kind of idea.

01:22:03.160 --> 01:22:05.680
Yeah, but at some point it stops.

01:22:05.680 --> 01:22:06.960
I don't think that you go,

01:22:06.960 --> 01:22:09.040
need to go for more than four degrees.

01:22:09.040 --> 01:22:09.880
Like at some point,

01:22:09.880 --> 01:22:13.360
there's going to be a general theory of search

01:22:13.360 --> 01:22:16.440
that tells you how to get to the global optimum,

01:22:16.440 --> 01:22:18.400
if the global optimum can be gotten to

01:22:18.400 --> 01:22:20.040
and your system is finite resources,

01:22:20.040 --> 01:22:22.640
or basically how to optimize your chances of getting there.

01:22:22.640 --> 01:22:24.520
Once you have that algorithm,

01:22:24.520 --> 01:22:25.600
as a scientist you are done,

01:22:25.600 --> 01:22:27.840
there is no more science that we can do with integrity,

01:22:27.840 --> 01:22:29.480
because it's just going to be the application

01:22:29.480 --> 01:22:30.720
of this algorithm.

01:22:30.720 --> 01:22:31.960
You can only do art then.

01:22:33.960 --> 01:22:35.680
You know, our original,

01:22:35.680 --> 01:22:38.240
we've been talking here for almost 90 minutes.

01:22:38.240 --> 01:22:41.760
So let me sort of hopefully bring our conversation

01:22:41.760 --> 01:22:44.480
to a close here within the next 10 minutes or so,

01:22:44.480 --> 01:22:46.680
by sort of redirecting our attention

01:22:46.680 --> 01:22:49.280
to the original occasion of us getting together,

01:22:49.280 --> 01:22:52.360
which was a brief exchange we had,

01:22:52.360 --> 01:22:56.680
the two of us on Twitter about ethics.

01:22:56.680 --> 01:22:58.040
So let me ask you this,

01:22:58.040 --> 01:23:01.480
where does ethics fit in all of this, or does it?

01:23:05.120 --> 01:23:07.440
I get sometimes frustrated when people think

01:23:07.440 --> 01:23:09.560
that ethics is about being good,

01:23:09.560 --> 01:23:12.400
and being good means to emulate a good person,

01:23:12.400 --> 01:23:16.880
preferably the one who is talking about ethics.

01:23:16.880 --> 01:23:19.360
Did you get frustrated with me on Twitter?

01:23:19.360 --> 01:23:22.360
No, you're a good kid.

01:23:25.760 --> 01:23:28.360
I'm one year younger than you, by the way, so.

01:23:31.360 --> 01:23:33.600
It's not about age, I'm about 12.

01:23:36.600 --> 01:23:37.920
That's right.

01:23:38.760 --> 01:23:42.960
Okay, so ethics, I think, is often misunderstood.

01:23:42.960 --> 01:23:46.680
Ethics emerges when you conceptualize the world

01:23:46.680 --> 01:23:50.880
as different agents, and yourself as one of them,

01:23:50.880 --> 01:23:54.560
and you share purposes with the other agents,

01:23:54.560 --> 01:23:57.000
but you have conflicts of interest.

01:23:57.000 --> 01:23:59.080
If you think that you don't share purposes

01:23:59.080 --> 01:24:01.560
with the other agents, if you're just a lone wolf,

01:24:01.560 --> 01:24:03.560
and the others are your prey,

01:24:03.560 --> 01:24:05.080
there is no reason for ethics, right?

01:24:05.080 --> 01:24:06.840
There's only you look for the consequences

01:24:06.840 --> 01:24:08.240
of your actions for yourself,

01:24:08.240 --> 01:24:11.000
with respect to your own reward functions,

01:24:11.000 --> 01:24:13.400
and that might involve that you have to create

01:24:13.400 --> 01:24:16.160
a civilization of minions or whatever,

01:24:16.160 --> 01:24:18.360
but it's not the same thing as ethics.

01:24:18.360 --> 01:24:20.800
It's not a shared system of negotiation,

01:24:20.800 --> 01:24:23.840
it's only one for you as an individual matter,

01:24:23.840 --> 01:24:26.080
because you don't share that purpose with the others.

01:24:26.080 --> 01:24:26.920
But for instance-

01:24:26.920 --> 01:24:29.760
It may not be shared, but it's your personal ethical framework.

01:24:29.760 --> 01:24:31.080
Oh, it has to be personal.

01:24:31.080 --> 01:24:33.360
For instance, I don't eat meat,

01:24:34.280 --> 01:24:37.640
maybe a legacy or a decision that I made

01:24:37.640 --> 01:24:39.520
when I was 14 years old,

01:24:39.520 --> 01:24:42.640
because back then I felt that I share a purpose

01:24:42.640 --> 01:24:46.080
with animals, that is the avoidance of suffering,

01:24:46.080 --> 01:24:47.560
if it can be helped.

01:24:47.560 --> 01:24:49.480
And I also realized that it's not mutual.

01:24:49.480 --> 01:24:51.400
The animals don't care about my suffering,

01:24:51.400 --> 01:24:54.360
cows largely don't care about that I suffer.

01:24:54.360 --> 01:24:55.600
They don't even conceptualize it,

01:24:55.600 --> 01:24:57.320
they don't think about it a lot.

01:24:57.320 --> 01:24:59.760
I have to think a lot about the suffering of cows,

01:24:59.760 --> 01:25:02.200
they didn't want it to suffer, so I stopped eating meat.

01:25:02.240 --> 01:25:03.960
That was an ethical decision.

01:25:03.960 --> 01:25:07.240
It's a decision about how to resolve a conflict of interest

01:25:07.240 --> 01:25:08.800
under conditions of shared purpose.

01:25:08.800 --> 01:25:10.560
And I think this is what ethics is about.

01:25:10.560 --> 01:25:13.800
It's a rational process in which you negotiate

01:25:13.800 --> 01:25:15.680
with yourself and with others,

01:25:15.680 --> 01:25:17.800
the resolution of conflicts of interest

01:25:17.800 --> 01:25:20.280
under conditions of shared purpose.

01:25:20.280 --> 01:25:23.080
And what purposes I share is in a way a decision,

01:25:23.080 --> 01:25:24.640
and I can make different decisions

01:25:24.640 --> 01:25:26.320
about what purposes we share,

01:25:26.320 --> 01:25:28.240
and some of them are sustainable and others are not,

01:25:28.240 --> 01:25:30.040
so they lead to different outcomes.

01:25:30.040 --> 01:25:31.920
But in the sense, ethics requires

01:25:31.920 --> 01:25:33.400
that you conceptualize yourself

01:25:33.400 --> 01:25:36.000
as something above the organism.

01:25:36.000 --> 01:25:39.160
If you identify the systems of meanings above yourself

01:25:39.160 --> 01:25:40.560
so you can share a purpose,

01:25:41.520 --> 01:25:43.120
love is the discovery of shared purpose.

01:25:43.120 --> 01:25:45.120
There needs to be somebody you love

01:25:45.120 --> 01:25:46.680
that you can be ethical with.

01:25:46.680 --> 01:25:48.720
At some level, you need to love them.

01:25:48.720 --> 01:25:50.640
You need to share a purpose with them.

01:25:50.640 --> 01:25:52.080
And then you negotiate, right?

01:25:52.080 --> 01:25:55.200
You don't want them all to fail in all regards,

01:25:55.200 --> 01:25:56.040
and yourself.

01:25:57.720 --> 01:25:58.880
This is what ethics is about.

01:25:58.880 --> 01:26:00.240
It's computational too.

01:26:00.240 --> 01:26:03.320
Machines can be ethical if they share a purpose with us.

01:26:03.320 --> 01:26:07.080
And what about two other sort of consideration perhaps,

01:26:07.080 --> 01:26:10.160
is that perhaps ethics can be a framework

01:26:10.160 --> 01:26:15.160
within which two entities that do not share interests

01:26:17.920 --> 01:26:22.920
can kind of negotiate in and peacefully coexist

01:26:23.440 --> 01:26:28.440
while still not sharing interests?

01:26:29.080 --> 01:26:30.960
But not interests, but purposes.

01:26:30.960 --> 01:26:32.000
Or purposes.

01:26:32.000 --> 01:26:33.400
If you don't share purposes,

01:26:33.400 --> 01:26:36.200
then you are defecting against your own interest,

01:26:36.200 --> 01:26:38.880
when you don't act on your own interest.

01:26:38.880 --> 01:26:40.520
It doesn't have integrity.

01:26:40.520 --> 01:26:42.640
If somebody is your foot,

01:26:43.560 --> 01:26:46.120
you should, and you don't share a purpose with your foot

01:26:46.120 --> 01:26:48.520
other than you want it to be nice and edible.

01:26:49.720 --> 01:26:50.560
Right?

01:26:50.560 --> 01:26:52.960
You then start giving presence to your foot

01:26:52.960 --> 01:26:54.280
and falling in love with your foot.

01:26:54.280 --> 01:26:55.120
It doesn't end well.

01:26:55.120 --> 01:26:56.560
Look at the little mermaid.

01:26:56.560 --> 01:26:58.840
The little mermaid is a siren.

01:26:58.840 --> 01:26:59.920
Sirens eat people.

01:26:59.920 --> 01:27:01.160
You don't fall in love with your foot.

01:27:01.160 --> 01:27:02.160
It doesn't end well.

01:27:03.920 --> 01:27:06.160
Okay, but me and you are both,

01:27:06.160 --> 01:27:08.320
I don't know if you're vegan or vegetarian.

01:27:08.320 --> 01:27:10.440
Both me and you don't eat meat.

01:27:10.440 --> 01:27:13.880
So we made that choice that perhaps cows

01:27:13.880 --> 01:27:17.400
don't share interests in us.

01:27:17.400 --> 01:27:20.880
We kind of are interested in diminishing their suffering,

01:27:20.880 --> 01:27:23.640
obviously, to make that decision.

01:27:23.640 --> 01:27:27.040
And yet, and they're our food supposedly,

01:27:27.040 --> 01:27:28.560
that's the popular opinion anyway.

01:27:28.560 --> 01:27:30.440
And yet we've made that choice

01:27:31.440 --> 01:27:34.720
to stay away from beef or from meat in general.

01:27:35.840 --> 01:27:40.640
So we can't find a framework with in which

01:27:40.640 --> 01:27:43.120
two entities that don't share interests

01:27:43.120 --> 01:27:44.920
and our purpose is to get a good,

01:27:44.920 --> 01:27:48.000
perhaps peacefully coexist.

01:27:48.000 --> 01:27:50.960
And isn't that the netico framework of its own, right?

01:27:50.960 --> 01:27:51.800
It's more tricky.

01:27:51.800 --> 01:27:52.880
I mean, with the cows,

01:27:52.880 --> 01:27:54.440
the cows largely wouldn't exist

01:27:54.440 --> 01:27:56.440
if people would not eat them.

01:27:56.440 --> 01:27:57.840
You can make the argument that

01:27:59.080 --> 01:28:01.520
pasture living grass-fed cow

01:28:01.520 --> 01:28:04.400
has net positive existence,

01:28:04.400 --> 01:28:06.280
except for the last day, which is horrible,

01:28:06.280 --> 01:28:08.000
but it's horrible for most of us.

01:28:08.000 --> 01:28:08.840
Right.

01:28:08.840 --> 01:28:10.440
And just that.

01:28:10.440 --> 01:28:11.880
Right, so this is one argument

01:28:11.960 --> 01:28:15.080
in favor of eating pasture-fed cows.

01:28:15.080 --> 01:28:15.920
Right.

01:28:15.920 --> 01:28:17.360
Another one is maybe you can manipulate

01:28:17.360 --> 01:28:18.680
the mental states of the cows,

01:28:18.680 --> 01:28:22.080
so even the factory-fed cows are happy.

01:28:23.680 --> 01:28:25.520
Right, so is this unethical?

01:28:25.520 --> 01:28:28.080
It might not look very appetizing to you,

01:28:28.080 --> 01:28:30.600
but then again, maybe people are on the same decision.

01:28:30.600 --> 01:28:33.000
We are a domesticated species.

01:28:33.000 --> 01:28:34.840
This is what humanity is about.

01:28:34.840 --> 01:28:36.640
We give up agency of our own beliefs.

01:28:36.640 --> 01:28:39.680
You get manipulated in finding things bearable

01:28:39.680 --> 01:28:42.400
that look unbearable to a more favorable human being

01:28:42.400 --> 01:28:43.240
like you and me.

01:28:44.280 --> 01:28:46.800
Right, it's a particular kind of domestication

01:28:46.800 --> 01:28:48.840
that didn't take hold on your brain.

01:28:48.840 --> 01:28:51.400
Is this unethical to implement this domestication

01:28:51.400 --> 01:28:56.200
by breeding people or cattle in a particular way?

01:28:56.200 --> 01:28:57.360
It looks repulsive to us,

01:28:57.360 --> 01:28:59.280
but if we really care about the well-being of cattle,

01:28:59.280 --> 01:29:01.800
you and me should probably optimize slaughterhouses

01:29:01.800 --> 01:29:04.920
to make them more humane, to make them more bearable.

01:29:04.920 --> 01:29:06.720
We look away from the slaughterhouses

01:29:06.720 --> 01:29:09.480
because we find them very anesthetic.

01:29:09.480 --> 01:29:11.440
We don't want to have anything to do with this.

01:29:11.440 --> 01:29:13.120
And this is not the most ethical stance

01:29:13.120 --> 01:29:14.640
that we can figure that out.

01:29:14.640 --> 01:29:16.920
So ethics in a way is difficult.

01:29:16.920 --> 01:29:19.880
Of course, that's the key point of ethics.

01:29:19.880 --> 01:29:24.600
And so even it seems to me that ethics requires

01:29:24.600 --> 01:29:26.720
sometimes we take choices

01:29:26.720 --> 01:29:29.640
which are not in our own best self-interest perhaps.

01:29:30.520 --> 01:29:33.200
Depends on what we define of our self.

01:29:33.200 --> 01:29:36.040
The self, we could say this is identical

01:29:36.040 --> 01:29:37.840
to the well-being of the organism,

01:29:37.840 --> 01:29:40.080
but this is a very short-sighted perspective, right?

01:29:40.080 --> 01:29:43.800
I don't actually identify all the way with my organism.

01:29:43.800 --> 01:29:46.240
There are other things I identify with society,

01:29:46.240 --> 01:29:48.560
I identify with my kids, with my relationships,

01:29:48.560 --> 01:29:50.640
with my friends, their well-being.

01:29:50.640 --> 01:29:53.000
So I am all the things that I identify with

01:29:53.000 --> 01:29:55.440
that I want to regulate in a particular way.

01:29:55.440 --> 01:29:58.240
And my children are objectively more important than me, right?

01:29:58.240 --> 01:30:00.480
If they have the choice to make my kids survive

01:30:00.480 --> 01:30:02.520
or myself, my kids should survive.

01:30:02.520 --> 01:30:05.840
This is as it should be if nature has wired me up correctly.

01:30:05.840 --> 01:30:07.000
You can change the wiring,

01:30:07.000 --> 01:30:09.840
but this is also the weird thing about ethics.

01:30:09.840 --> 01:30:11.800
Ethics becomes very tricky to discuss

01:30:11.800 --> 01:30:15.000
once the reward function becomes mutable.

01:30:15.000 --> 01:30:17.120
When you're able to change what is important to you,

01:30:17.120 --> 01:30:19.440
what you care about, how do you define ethics?

01:30:21.440 --> 01:30:22.280
Me?

01:30:24.000 --> 01:30:25.760
So, or anyone?

01:30:26.720 --> 01:30:30.800
I would say to me, let me be careful about this.

01:30:31.640 --> 01:30:35.240
Well, I would say it's basically,

01:30:35.240 --> 01:30:38.320
you can call it even a code of conduct

01:30:38.320 --> 01:30:43.320
or a set of principles and rules

01:30:43.520 --> 01:30:45.360
that guide my behavior

01:30:47.680 --> 01:30:52.680
to accomplish certain kinds of outcomes.

01:30:54.680 --> 01:30:56.840
There are no beliefs without priors.

01:30:56.840 --> 01:31:01.480
What are the priors that you base your code of conduct on?

01:31:01.480 --> 01:31:03.400
Yes, that's a very good question.

01:31:03.400 --> 01:31:05.000
And it puts me on the spot here

01:31:05.000 --> 01:31:07.560
and I'm not prepared for it, but I have to follow.

01:31:08.640 --> 01:31:13.640
So the priors are, you can call them axioms perhaps,

01:31:16.680 --> 01:31:18.840
things like diminishing suffering.

01:31:20.160 --> 01:31:22.920
Things like, for example,

01:31:22.920 --> 01:31:27.920
and perhaps one of those rules or points of view or tools,

01:31:29.120 --> 01:31:32.840
if you will, is taking sort of what Peter Singer calls

01:31:32.840 --> 01:31:35.960
the universe points of view, point of view,

01:31:35.960 --> 01:31:40.600
or sort of an outside point of view than my own, right?

01:31:40.600 --> 01:31:44.480
So when it comes to, with respect to cows,

01:31:44.480 --> 01:31:48.520
I take a point of view outside of me and the cows, hopefully.

01:31:48.520 --> 01:31:53.320
And sort of I'm able to look at my suffering

01:31:53.320 --> 01:31:58.120
of not eating a cow and their suffering of being eaten, right?

01:31:58.120 --> 01:32:02.680
So if my prior is minimize suffering,

01:32:03.560 --> 01:32:08.560
because basically that's the axiom based on which I can deduce

01:32:09.040 --> 01:32:13.080
that something or someone exists like an even entity,

01:32:13.080 --> 01:32:15.800
like a sentient being, right?

01:32:15.800 --> 01:32:17.560
Is the suffering, does it suffer?

01:32:17.560 --> 01:32:19.800
That's sort of my test, if you will,

01:32:19.800 --> 01:32:24.120
not during test, but a test of being a sentient being.

01:32:24.120 --> 01:32:26.120
Can you suffer? Can it suffer?

01:32:26.120 --> 01:32:29.200
And if it can suffer, then my principle

01:32:29.200 --> 01:32:33.440
of minimizing suffering must be the guiding principle

01:32:33.440 --> 01:32:36.680
with which I relate to it.

01:32:37.800 --> 01:32:39.840
That's kind of like, if you will,

01:32:39.840 --> 01:32:43.080
sort of the foundation of my personal ethics.

01:32:43.080 --> 01:32:44.600
Can it suffer?

01:32:44.600 --> 01:32:46.680
Then the next is how can I minimize

01:32:46.680 --> 01:32:48.760
the suffering of that entity?

01:32:48.760 --> 01:32:53.280
And then basically everything else builds up from there, right?

01:32:53.280 --> 01:32:54.440
When you become an adult,

01:32:54.440 --> 01:32:56.800
I think the most important part of it

01:32:56.800 --> 01:32:59.520
is that you take charge of your own emotions.

01:32:59.520 --> 01:33:01.840
You realize that your own emotions are generated

01:33:01.840 --> 01:33:04.320
by your own brain, by your own organism,

01:33:04.320 --> 01:33:05.680
and they're here to serve you,

01:33:05.680 --> 01:33:07.880
and you're here to serve your emotions.

01:33:07.880 --> 01:33:09.840
The emotions are there to help you

01:33:09.840 --> 01:33:13.200
for doing the things that you consider to be the right thing.

01:33:13.200 --> 01:33:16.120
And that means that you need to be able to control them,

01:33:16.120 --> 01:33:17.440
to have integrity.

01:33:17.440 --> 01:33:19.560
If you are just the victim of your emotions

01:33:19.560 --> 01:33:22.400
and not do the things that are the right thing,

01:33:22.400 --> 01:33:24.680
you learn that you can control your emotions

01:33:24.680 --> 01:33:26.120
and deal with them, right?

01:33:26.120 --> 01:33:28.520
You don't have integrity.

01:33:28.520 --> 01:33:30.200
And what is suffering?

01:33:30.200 --> 01:33:33.160
Pain is the result of some part of your brain

01:33:33.160 --> 01:33:35.400
sending a teaching signal to another part of your brain

01:33:35.400 --> 01:33:37.320
to improve its performance.

01:33:37.320 --> 01:33:39.200
If the regulation is not correct

01:33:39.200 --> 01:33:42.480
because you cannot actually regulate that particular thing,

01:33:42.480 --> 01:33:44.800
then the pain will endure and usually get cranked up

01:33:44.800 --> 01:33:46.120
until your brain figures it out

01:33:46.120 --> 01:33:49.080
in terms of the pain signaling center.

01:33:49.080 --> 01:33:51.800
But by telling them, actually, you're not helping here, right?

01:33:51.800 --> 01:33:53.440
Until you get to this point, you have suffering.

01:33:53.440 --> 01:33:56.560
You have increased pain that you cannot resolve.

01:33:56.560 --> 01:34:01.040
And so in this sense, suffering is a lack of integrity.

01:34:01.040 --> 01:34:03.720
The difficulty is only that many beings cannot get

01:34:03.720 --> 01:34:06.400
to the degree of integrity where they can control

01:34:06.400 --> 01:34:08.960
the application of learning signals in their brain,

01:34:08.960 --> 01:34:11.120
that they can control the way the reward function

01:34:11.120 --> 01:34:13.240
is being computed and distributed.

01:34:13.240 --> 01:34:16.320
So then according to your argument,

01:34:16.320 --> 01:34:18.840
suffering is just like you said before,

01:34:18.840 --> 01:34:21.840
a simulation or a part of a simulation then.

01:34:21.840 --> 01:34:24.120
Well, everything that we experience is a simulation.

01:34:24.120 --> 01:34:25.160
We are a simulation.

01:34:25.160 --> 01:34:26.960
But to us, of course, it feels real.

01:34:26.960 --> 01:34:29.040
There is no helping around this.

01:34:29.040 --> 01:34:32.760
But what I have learned in the course of my life

01:34:32.760 --> 01:34:36.720
is that all of my suffering is a result of not being awake.

01:34:37.720 --> 01:34:39.800
Once I wake up, I realize what's going on.

01:34:39.800 --> 01:34:41.520
I realize that I am in mind.

01:34:41.560 --> 01:34:44.120
The relevance of the signals that I perceive

01:34:44.120 --> 01:34:45.840
is completely up to the mind.

01:34:47.760 --> 01:34:49.320
Because the universe does not give me

01:34:49.320 --> 01:34:50.680
objectively good or bad things.

01:34:50.680 --> 01:34:52.840
The universe gives me a bunch of electrical impulses

01:34:52.840 --> 01:34:56.040
that manifest in my tannamos

01:34:56.040 --> 01:34:57.720
and my brain makes sense of them

01:34:57.720 --> 01:34:59.600
by creating a simulated world.

01:34:59.600 --> 01:35:01.360
And the valence in that simulated world

01:35:01.360 --> 01:35:03.120
is completely internal.

01:35:03.120 --> 01:35:04.560
It's completely part of that world.

01:35:04.560 --> 01:35:05.640
It's not objective.

01:35:05.640 --> 01:35:08.000
And so I can control this.

01:35:08.200 --> 01:35:12.160
So ethics or suffering is a subjective experience.

01:35:12.160 --> 01:35:15.840
And if I'm basing my ethics on suffering,

01:35:15.840 --> 01:35:17.920
therefore my ethics would be subjective.

01:35:17.920 --> 01:35:19.560
Is that what you're saying?

01:35:19.560 --> 01:35:22.480
No, I think that suffering is real with respect to the self.

01:35:22.480 --> 01:35:24.080
But it's not immutable.

01:35:24.080 --> 01:35:26.400
So you can change the definition of yourself,

01:35:26.400 --> 01:35:28.320
the things that you identify with.

01:35:28.320 --> 01:35:30.560
Imagine there is a certain condition in the world

01:35:30.560 --> 01:35:33.200
where you think a particular party needs to be in power,

01:35:33.200 --> 01:35:35.280
an order for the world to be good.

01:35:35.280 --> 01:35:37.880
And if that party is not in power, you suffer.

01:35:37.880 --> 01:35:39.120
You can give up that belief

01:35:39.120 --> 01:35:41.200
and you realize how politics actually works

01:35:41.200 --> 01:35:43.440
and that there's a fitness function going on

01:35:43.440 --> 01:35:46.040
and that people behave according to what they read

01:35:46.040 --> 01:35:46.960
and whatever.

01:35:46.960 --> 01:35:48.360
And you realize that this is the case

01:35:48.360 --> 01:35:50.120
and you just give up on suffering about it

01:35:50.120 --> 01:35:51.200
because you realize you are looking

01:35:51.200 --> 01:35:52.880
at a mechanical process

01:35:52.880 --> 01:35:54.840
and it plays out anyway regardless

01:35:54.840 --> 01:35:56.920
of what you feel about how that plays out, right?

01:35:56.920 --> 01:35:59.000
So you give up that suffering.

01:35:59.000 --> 01:36:00.640
Or if you are a preschool teacher

01:36:00.640 --> 01:36:03.680
and the kids are misbehaving and they are mean to you,

01:36:03.680 --> 01:36:05.400
at some point you stop suffering about this

01:36:05.400 --> 01:36:07.440
because you see what they actually do.

01:36:07.480 --> 01:36:09.200
It's not personal, right?

01:36:09.200 --> 01:36:11.600
That's what Stoic philosophy is all about, right?

01:36:11.600 --> 01:36:14.160
Stoics say there is no point.

01:36:14.160 --> 01:36:15.200
So first of all,

01:36:17.360 --> 01:36:21.760
Stoics say that we suffer not from events

01:36:21.760 --> 01:36:23.480
or things that happen in our life

01:36:23.480 --> 01:36:26.320
but from the stories that we attach to them.

01:36:27.160 --> 01:36:29.200
And therefore, if we change the story,

01:36:29.200 --> 01:36:31.800
we can change the way we feel about them

01:36:31.800 --> 01:36:34.120
and thereby remove the suffering.

01:36:34.120 --> 01:36:36.560
And they say that there's the only thing

01:36:36.560 --> 01:36:38.640
that we can focus on and do something

01:36:38.640 --> 01:36:41.400
about is our own thoughts

01:36:41.400 --> 01:36:43.560
and things like the kids in school

01:36:43.560 --> 01:36:44.800
or the party are things

01:36:44.800 --> 01:36:47.440
that are completely outside of our control.

01:36:47.440 --> 01:36:49.080
And therefore, there is no point

01:36:49.080 --> 01:36:51.080
to get aggravated about them.

01:36:51.080 --> 01:36:53.120
And there's very little things

01:36:53.120 --> 01:36:54.920
that are completely under our control.

01:36:54.920 --> 01:36:57.760
So we can't really control fully our body.

01:36:57.760 --> 01:37:01.280
We can't really control our health completely.

01:37:01.280 --> 01:37:02.800
Things can always go wrong there.

01:37:02.800 --> 01:37:04.720
The only thing they say you can fully,

01:37:04.720 --> 01:37:07.040
completely control is your thoughts.

01:37:07.880 --> 01:37:11.520
And that's where your freedom comes to be

01:37:11.520 --> 01:37:15.040
and that's where your power comes to be

01:37:15.040 --> 01:37:18.920
and that's where you're the one and only, right?

01:37:18.920 --> 01:37:21.800
In that mind, in that simulation, you're the God.

01:37:22.800 --> 01:37:25.440
So this ability to make your thoughts more truthful,

01:37:25.440 --> 01:37:27.400
this is Western enlightenment in a way,

01:37:27.400 --> 01:37:29.680
this is Aufklärung in German.

01:37:29.680 --> 01:37:31.160
And there is also this other sense

01:37:31.160 --> 01:37:33.040
of enlightenment, Erleuchtung,

01:37:33.040 --> 01:37:35.240
that you have in a spiritual context.

01:37:35.240 --> 01:37:37.800
And so Aufklärung fixes your rationality

01:37:37.800 --> 01:37:40.560
and Erleuchtung fixes your motivation.

01:37:40.560 --> 01:37:42.040
It fixes what's relevant to you

01:37:42.040 --> 01:37:43.040
and how we relate to this.

01:37:43.040 --> 01:37:46.280
It fixes the relationship between self and universe.

01:37:46.280 --> 01:37:48.760
And often they are seen as mutually exclusive

01:37:48.760 --> 01:37:52.600
in the sense that Aufklärung leads to nihilism

01:37:52.600 --> 01:37:54.440
because you don't give up your need for meaning.

01:37:54.440 --> 01:37:56.520
You just prove that it cannot be satisfied.

01:37:56.520 --> 01:37:59.720
God does not exist in any way that can set you free.

01:37:59.760 --> 01:38:01.440
And in this other sense,

01:38:01.440 --> 01:38:03.360
you give up your understanding

01:38:03.360 --> 01:38:06.000
of how the world actually works so you can be happy.

01:38:07.120 --> 01:38:09.840
You go to a non-dual state where you represent

01:38:09.840 --> 01:38:12.360
that all people share the same cosmic consciousness,

01:38:12.360 --> 01:38:13.680
which is complete bullshit, right?

01:38:13.680 --> 01:38:17.840
But it's something that removes the illusion of separation

01:38:17.840 --> 01:38:19.040
and there are suffering that comes

01:38:19.040 --> 01:38:20.600
with the separation and so on.

01:38:22.200 --> 01:38:23.760
So where is that...

01:38:23.760 --> 01:38:25.320
Yeah, sustainable.

01:38:25.320 --> 01:38:27.840
Where does that leave us with respect to ethics though?

01:38:28.080 --> 01:38:31.640
So maybe you were able to dismantle much or most

01:38:31.640 --> 01:38:34.120
or maybe all of my ethics, did you?

01:38:36.280 --> 01:38:38.360
I don't know all of your ethics, but...

01:38:38.360 --> 01:38:40.960
Well, if you asked me for the foundation

01:38:40.960 --> 01:38:45.440
and the best I could come up with the sort of the suffering test.

01:38:45.440 --> 01:38:47.000
Yeah, it's not good.

01:38:47.000 --> 01:38:49.520
The problem is really that if I can turn off suffering

01:38:51.200 --> 01:38:52.920
or if I get counter-intuitive results,

01:38:52.920 --> 01:38:55.400
there's this anti-natalism.

01:38:55.760 --> 01:38:58.680
Anti-natalism is an obvious way to end suffering, right?

01:38:58.680 --> 01:39:01.560
Stop putting new organisms into the world

01:39:01.560 --> 01:39:04.120
and the existing set of organisms

01:39:04.120 --> 01:39:07.280
in the least painful way possible, right?

01:39:07.280 --> 01:39:08.800
AI could help with this.

01:39:08.800 --> 01:39:10.880
The question is, can we make it safe

01:39:10.880 --> 01:39:13.360
or is the AI going to leave a couple cells left

01:39:13.360 --> 01:39:15.560
that can give rise to new suffering later on?

01:39:17.280 --> 01:39:20.880
But so if you have a completely cold and dead universe,

01:39:20.880 --> 01:39:22.360
then there'll be no suffering, right?

01:39:22.360 --> 01:39:23.200
Yes.

01:39:24.160 --> 01:39:25.280
Is this what you want?

01:39:25.280 --> 01:39:26.200
Right, clearly.

01:39:26.200 --> 01:39:27.840
So that's not the most...

01:39:27.840 --> 01:39:28.680
According to...

01:39:28.680 --> 01:39:29.520
It's not so clear.

01:39:29.520 --> 01:39:30.960
I'm anti-natalist, but my kids are not.

01:39:30.960 --> 01:39:34.080
So I have this division there.

01:39:34.080 --> 01:39:35.400
But...

01:39:35.400 --> 01:39:38.720
So what's that say about where are you coming from then

01:39:38.720 --> 01:39:39.800
with respect to ethics?

01:39:39.800 --> 01:39:42.520
So let's say my suffering test is not good enough.

01:39:42.520 --> 01:39:45.440
I think existence by itself is neutral.

01:39:45.440 --> 01:39:48.400
The reason why there are so few stoics around.

01:39:48.400 --> 01:39:49.240
Have you thought about this?

01:39:49.240 --> 01:39:51.080
Stoicism has been discovered a long time ago.

01:39:51.080 --> 01:39:52.320
Almost nobody's a stoic.

01:39:52.320 --> 01:39:53.160
How is that?

01:39:54.160 --> 01:39:57.320
Well, I know a few people who are stoics, actually.

01:39:57.320 --> 01:39:59.520
Yeah, but the majority is not.

01:39:59.520 --> 01:40:01.240
Well, it seems to be so obvious.

01:40:01.240 --> 01:40:03.320
Only worry about the things that you can actually change

01:40:03.320 --> 01:40:06.320
to the degree that the very helps you changing them.

01:40:06.320 --> 01:40:07.160
Yes, so...

01:40:08.440 --> 01:40:10.480
So why is nobody a stoic?

01:40:10.480 --> 01:40:11.320
Almost nobody.

01:40:12.400 --> 01:40:13.960
Well, I wouldn't say nobody.

01:40:13.960 --> 01:40:17.320
I'd say a few people are stoic and they're amazing

01:40:17.320 --> 01:40:20.760
and they're inspirational and they're motivational

01:40:20.920 --> 01:40:25.320
and they're good role model for sort of like

01:40:25.320 --> 01:40:27.560
how I want to behave and how I want to live

01:40:27.560 --> 01:40:29.520
and how I want to act in this world.

01:40:29.520 --> 01:40:31.840
I suspect that stoicism is maladaptive

01:40:31.840 --> 01:40:33.920
from a permanent evolutionary perspective.

01:40:35.120 --> 01:40:37.280
Most cats I have known are stoics,

01:40:37.280 --> 01:40:39.200
which means if you leave them alone, they're fine.

01:40:39.200 --> 01:40:40.800
Like their baseline state is okay.

01:40:40.800 --> 01:40:43.480
They're okay with themselves and their place in the universe

01:40:43.480 --> 01:40:45.440
and they just stay at that place.

01:40:45.440 --> 01:40:46.760
And only when you disturb that

01:40:46.760 --> 01:40:48.440
because they need to use the bathroom

01:40:48.440 --> 01:40:52.840
or because they are hungry or they want to play or whatever,

01:40:52.840 --> 01:40:54.760
this equilibrium gets disturbed

01:40:54.760 --> 01:40:56.880
and they do exactly what's necessary

01:40:56.880 --> 01:40:58.720
to get back to the equilibrium state

01:40:58.720 --> 01:41:00.400
and then they're fine again.

01:41:00.400 --> 01:41:03.320
And a human being is slightly different.

01:41:03.320 --> 01:41:06.240
A healthy human being is set up in such a way

01:41:06.240 --> 01:41:07.920
that when they wake up in the morning,

01:41:07.920 --> 01:41:09.960
they're not completely fine.

01:41:09.960 --> 01:41:11.960
And then they need to be busy during the day,

01:41:11.960 --> 01:41:14.160
but in the evening, they're fine.

01:41:14.160 --> 01:41:15.440
In the evening, it's done enough

01:41:15.440 --> 01:41:18.000
to make peace this existence again

01:41:18.000 --> 01:41:19.920
and then they can have a beer with their friends

01:41:19.920 --> 01:41:21.760
and everything is good.

01:41:21.760 --> 01:41:24.320
And then there are some individuals

01:41:24.320 --> 01:41:28.080
which have so much discontent with them themselves.

01:41:28.080 --> 01:41:31.360
Like the human is the animal that is discontent

01:41:31.360 --> 01:41:35.000
that they cannot take care of this in a single day.

01:41:35.000 --> 01:41:38.080
But even after several weeks of sustained work,

01:41:38.080 --> 01:41:40.320
they are still in a state where it's not good enough

01:41:40.320 --> 01:41:42.240
and only when they have this amazing thing

01:41:42.240 --> 01:41:43.520
where they get their noble price,

01:41:43.520 --> 01:41:45.480
they're fine for like half a day.

01:41:46.320 --> 01:41:49.240
And the way this is the way we are set up

01:41:49.240 --> 01:41:50.320
to different degrees.

01:41:50.320 --> 01:41:52.160
And from an evolutionary perspective,

01:41:52.160 --> 01:41:53.960
you can totally see why that would be useful

01:41:53.960 --> 01:41:55.720
for a group species.

01:41:55.720 --> 01:41:58.360
For an individual species that is not so much a group species

01:41:58.360 --> 01:42:00.880
like cats are not really meant for groups.

01:42:00.880 --> 01:42:02.960
They're very much singletons.

01:42:02.960 --> 01:42:05.360
For them, it's rational to be historic.

01:42:05.360 --> 01:42:06.600
But if you're a group animal,

01:42:06.600 --> 01:42:08.880
it makes sense that the well-being of the individual

01:42:08.880 --> 01:42:11.200
is sacrificed for the well-being of the group.

01:42:11.200 --> 01:42:14.080
So each individual is overextending themselves

01:42:14.080 --> 01:42:15.400
to make the group more successful

01:42:15.400 --> 01:42:17.440
and produce a surplus of resources for the group

01:42:17.440 --> 01:42:18.440
as a result.

01:42:18.440 --> 01:42:21.000
Right, but evolution also diversifies things

01:42:21.000 --> 01:42:25.200
so that if one kind of feature becomes maladaptive

01:42:25.200 --> 01:42:27.600
in a new environmental change,

01:42:27.600 --> 01:42:30.840
then a diverse part of that population

01:42:30.840 --> 01:42:32.000
would be more adaptive.

01:42:32.000 --> 01:42:35.280
And so that's why evolution sort of hedges its bets

01:42:35.280 --> 01:42:39.200
with the greatest variety and diversity possible, right?

01:42:39.200 --> 01:42:41.800
So yeah, there will be some people who would be like that

01:42:41.800 --> 01:42:45.440
and some people who would be like otherwise.

01:42:45.440 --> 01:42:49.560
And this way, on the whole,

01:42:49.560 --> 01:42:53.080
they're evolutionarily most adaptive.

01:42:53.080 --> 01:42:56.080
But some will be more adaptive to one kind of situation

01:42:56.080 --> 01:42:57.480
and others will be more adaptive

01:42:57.480 --> 01:42:59.400
to other kinds of situation.

01:42:59.400 --> 01:43:00.680
I'm not sure if this is true.

01:43:00.680 --> 01:43:02.880
So for instance, we find that larger habitats

01:43:02.880 --> 01:43:05.040
don't necessarily have more species.

01:43:05.040 --> 01:43:08.640
And that's because there's a fearsome competition,

01:43:08.640 --> 01:43:10.880
which means that there's less slack in the evolution.

01:43:10.880 --> 01:43:12.920
So for instance, New Zealand had a lot of species

01:43:12.920 --> 01:43:16.160
before there was immigration of other species

01:43:16.160 --> 01:43:19.000
and they obliterated most of the stuff that existed,

01:43:19.000 --> 01:43:21.240
mostly because the stuff that came in

01:43:21.240 --> 01:43:23.200
was result of a much fiercer competition

01:43:23.200 --> 01:43:25.200
than existed in small New Zealand.

01:43:25.200 --> 01:43:26.200
Sure, yeah.

01:43:26.200 --> 01:43:28.880
And in a way, the same thing happens now.

01:43:28.880 --> 01:43:30.520
We are the result of evolution.

01:43:30.520 --> 01:43:32.520
We are, as Minsky said, evolution's way

01:43:32.520 --> 01:43:37.520
to put the airplanes into the sky

01:43:38.240 --> 01:43:42.600
and make these clouds that the airplanes make.

01:43:42.600 --> 01:43:46.000
And we reduce the number of species dramatically.

01:43:46.000 --> 01:43:50.160
We are like probably eventually going to look like a meteor

01:43:50.160 --> 01:43:52.480
that is going to obliterate a large part of the species

01:43:52.480 --> 01:43:53.320
on this planet.

01:43:54.920 --> 01:43:59.640
So what's that say about ethics and technology?

01:43:59.640 --> 01:44:02.120
So what's the solution then?

01:44:02.120 --> 01:44:05.240
So is there space for ethics and technology?

01:44:05.240 --> 01:44:06.840
Of course there is.

01:44:06.840 --> 01:44:09.280
It's about discovering the long game, right?

01:44:09.280 --> 01:44:11.880
So when you do something, you have short influences

01:44:11.880 --> 01:44:13.680
and you have long influences.

01:44:13.680 --> 01:44:16.040
And based on what you think is the right thing to do,

01:44:16.040 --> 01:44:18.160
you need to look at the long-term influences.

01:44:18.160 --> 01:44:19.960
But you also need to question why you think

01:44:19.960 --> 01:44:21.640
that something is the right thing to do,

01:44:21.640 --> 01:44:23.960
what the results of that are, which gets tricky.

01:44:23.960 --> 01:44:26.240
But we can agree on that, that's fantastic.

01:44:26.240 --> 01:44:29.080
But tell me then, how do you define ethics yourself?

01:44:30.200 --> 01:44:34.400
Well, the tension between the way I define ethics

01:44:34.400 --> 01:44:39.040
and some other people in AI, and ethics in AI define it is,

01:44:39.040 --> 01:44:40.760
there are some people which think that ethics

01:44:40.760 --> 01:44:42.480
is a way for politically savvy people

01:44:42.480 --> 01:44:44.080
to get power over STEM people.

01:44:46.400 --> 01:44:49.360
And with considerable success,

01:44:49.360 --> 01:44:51.000
it's largely a protection racket.

01:44:51.960 --> 01:44:55.000
There's also a way that ethics happens where you have studies

01:44:55.000 --> 01:44:56.600
where somebody asks a million people

01:44:56.600 --> 01:44:59.840
of whether a traffic car should run over young people

01:44:59.840 --> 01:45:01.800
or old people first.

01:45:01.800 --> 01:45:03.240
And then they publish the results

01:45:03.240 --> 01:45:06.920
and it makes a big splash because people can relate to this.

01:45:06.920 --> 01:45:08.200
But it's...

01:45:08.200 --> 01:45:10.160
Well, this is ethics, we can do that.

01:45:10.160 --> 01:45:12.200
This has just happened so that philosophers

01:45:12.200 --> 01:45:13.240
had this trolley problem,

01:45:13.240 --> 01:45:14.640
and suddenly they see an application.

01:45:14.640 --> 01:45:16.240
But it's largely the same thing as saying

01:45:16.240 --> 01:45:19.880
that the majority of people would want a minor tawry

01:45:19.880 --> 01:45:23.360
to be confined in labyrinths rather than in public forests.

01:45:23.360 --> 01:45:24.200
Right.

01:45:24.200 --> 01:45:29.680
That is the situation that the gods were in

01:45:29.680 --> 01:45:31.120
or the Cretan king was in

01:45:31.200 --> 01:45:33.560
when this sign turned out to be a minor tawry.

01:45:33.560 --> 01:45:35.320
But it rarely happens.

01:45:35.320 --> 01:45:36.160
Right.

01:45:36.160 --> 01:45:37.000
In the same sense, it rarely happens

01:45:37.000 --> 01:45:39.600
that a self-driving car will have to make that decision.

01:45:39.600 --> 01:45:43.640
Probably not often enough to require an if-then in its code.

01:45:45.160 --> 01:45:48.560
But how do you define ethics for yourself?

01:45:48.560 --> 01:45:49.480
What is ethics?

01:45:49.480 --> 01:45:51.640
Because you asked me this and I gave my best to answer.

01:45:51.640 --> 01:45:53.440
Oh, so I also try to do this.

01:45:53.440 --> 01:45:55.920
My best answer is that ethics is the principle

01:45:55.920 --> 01:45:58.200
negotiation of conflicts of interest

01:45:58.200 --> 01:46:00.720
under conditions of shared purpose.

01:46:00.720 --> 01:46:03.600
If I share purposes with others, with society,

01:46:03.600 --> 01:46:05.560
with other beings, with conscious beings,

01:46:05.560 --> 01:46:08.520
and that's my decision based on the way my mind is set up

01:46:08.520 --> 01:46:12.680
right now, and I run into conflicts of interest with them.

01:46:12.680 --> 01:46:14.120
I have to deal with this.

01:46:14.120 --> 01:46:16.160
For instance, when I look at other people,

01:46:16.160 --> 01:46:20.800
I mostly imagine myself as being them

01:46:20.800 --> 01:46:22.760
in a different timeline.

01:46:22.760 --> 01:46:24.960
Everybody is in a way, me in a different timeline,

01:46:24.960 --> 01:46:27.120
but in order to understand who they are,

01:46:27.120 --> 01:46:28.520
I need to flip a number of bits.

01:46:28.520 --> 01:46:32.240
So I think about which bits would I need to flip in my mind

01:46:32.240 --> 01:46:33.840
to be you.

01:46:33.840 --> 01:46:34.480
Right.

01:46:34.480 --> 01:46:37.640
And these are the conditions of negotiation that I have with you.

01:46:37.640 --> 01:46:40.760
So we can agree on that, perhaps, on that definition,

01:46:40.760 --> 01:46:43.120
but then where do the cows fit in?

01:46:43.120 --> 01:46:45.320
Because we don't have a shared purpose with them.

01:46:45.320 --> 01:46:49.800
So how can you have ethics with respect to the cows, then?

01:46:49.800 --> 01:46:53.080
The shared purpose doesn't objectively exist.

01:46:53.080 --> 01:46:55.000
A shared purpose means that you basically

01:46:55.000 --> 01:46:57.480
project a shared meaning above the level of your ego,

01:46:57.880 --> 01:47:00.280
being the function that integrates expected rewards

01:47:00.280 --> 01:47:01.680
over the next 50 years.

01:47:01.680 --> 01:47:02.320
Well, exactly.

01:47:02.320 --> 01:47:05.880
That's what Peter Singer calls the universe point of view,

01:47:05.880 --> 01:47:07.200
perhaps.

01:47:07.200 --> 01:47:09.680
Yeah, well, if you can go to this eternalist perspective

01:47:09.680 --> 01:47:13.160
where you integrate expected reward from here to infinity,

01:47:13.160 --> 01:47:15.520
most of that being outside of the universe.

01:47:15.520 --> 01:47:17.440
This leads to very weird things.

01:47:17.440 --> 01:47:20.240
Most of my friends are eternalists, in a way, right?

01:47:20.240 --> 01:47:22.080
All these romantic Russian Jews.

01:47:22.080 --> 01:47:25.440
We are like that, in a way, this Eastern European

01:47:25.480 --> 01:47:31.640
shape of the soul, that creates something like a conspiracy.

01:47:31.640 --> 01:47:34.280
It creates a tribe, and it's very useful for cooperation.

01:47:34.280 --> 01:47:37.000
So shared meaning is a very important thing for cooperation

01:47:37.000 --> 01:47:39.200
that is nontransactional.

01:47:39.200 --> 01:47:43.880
But there's a certain kind of illusion in it.

01:47:43.880 --> 01:47:45.960
To me, meaning is like the ring of Mordor.

01:47:48.840 --> 01:47:50.400
So you have to carry it.

01:47:50.400 --> 01:47:54.720
If you drop the ring, you will lose the brotherhood of the ring,

01:47:54.720 --> 01:47:56.880
and you will lose your mission.

01:47:56.880 --> 01:47:58.320
You have to carry it, but very lightly.

01:47:58.320 --> 01:48:01.080
If you put it on, you will get superpowers,

01:48:01.080 --> 01:48:03.360
but you get corrupted, because there is no meaning.

01:48:03.360 --> 01:48:06.720
You get drawn into a cult that you create.

01:48:06.720 --> 01:48:08.560
And I don't want to do that, because it's

01:48:08.560 --> 01:48:13.560
going to shackle my mind in ways that I don't want it to be bound.

01:48:13.560 --> 01:48:15.400
I really, really like that way of saying,

01:48:15.400 --> 01:48:18.880
but I'm trying to extrapolate from your sort of print

01:48:18.880 --> 01:48:23.360
definition of ethics a guide of how we can treat the cows,

01:48:23.360 --> 01:48:26.120
and hopefully how the AIs can treat us

01:48:26.120 --> 01:48:28.680
within that same definition.

01:48:28.680 --> 01:48:30.400
That's what I'm trying to push here,

01:48:30.400 --> 01:48:33.040
and see if that's possible at all.

01:48:33.040 --> 01:48:34.760
OK, so there is some.

01:48:34.760 --> 01:48:38.640
Because my claim is that the way we treat cows, probably,

01:48:38.640 --> 01:48:44.480
is another way of how AIs could possibly treat us.

01:48:44.480 --> 01:48:47.600
I think that some people have this idea similar to Azimov,

01:48:47.600 --> 01:48:50.560
that at some point, the boomers will become larger and more

01:48:50.560 --> 01:48:52.680
powerful, so we can make them washing machines,

01:48:52.680 --> 01:48:56.040
or let them do our shopping, or let them do our nursing,

01:48:56.040 --> 01:48:57.880
and then we will still enslave them,

01:48:57.880 --> 01:49:01.920
and we'll negotiate the conditions of coexistence with them.

01:49:01.920 --> 01:49:04.800
And I don't think this is what's going to happen primarily.

01:49:04.800 --> 01:49:06.800
What's going to happen is that corporations, which

01:49:06.800 --> 01:49:08.520
are already intelligent agents, it just

01:49:08.520 --> 01:49:10.840
happened to borrow human intelligence,

01:49:10.840 --> 01:49:12.680
automate their decision making.

01:49:12.680 --> 01:49:14.240
At the moment, a human being can often

01:49:14.240 --> 01:49:19.160
outsmart a corporation, because the corporation has

01:49:19.160 --> 01:49:23.000
so much time in between updating its Excel spreadsheets

01:49:23.000 --> 01:49:24.720
and the next weekly meetings.

01:49:24.720 --> 01:49:26.280
Now, imagine it automates everything,

01:49:26.280 --> 01:49:29.080
and the weekly meetings take place every millisecond.

01:49:29.080 --> 01:49:30.920
And this thing becomes sentient, understands

01:49:30.920 --> 01:49:33.080
its role in the world, and the nature of the world,

01:49:33.080 --> 01:49:35.040
and physics, and everything else,

01:49:35.040 --> 01:49:37.520
because it has scalable intelligence.

01:49:37.520 --> 01:49:40.040
We will not be able to outsmart that anymore.

01:49:40.040 --> 01:49:41.760
And we will not live next to it.

01:49:41.760 --> 01:49:42.960
We will live inside of it.

01:49:42.960 --> 01:49:44.400
Intelligence will come.

01:49:44.400 --> 01:49:46.840
The AI will come from top down on us.

01:49:46.840 --> 01:49:49.840
We will live not next to it, but inside.

01:49:49.840 --> 01:49:51.800
We will be its gut flora.

01:49:51.800 --> 01:49:53.800
And the question is how we can negotiate

01:49:53.800 --> 01:49:56.280
that it doesn't get the ideas to use antibiotics,

01:49:56.280 --> 01:49:58.080
because you're actually not good for anything.

01:49:58.080 --> 01:50:00.600
Exactly.

01:50:00.600 --> 01:50:02.720
And why wouldn't they do that?

01:50:02.720 --> 01:50:05.160
I don't see why.

01:50:05.160 --> 01:50:08.360
So some people made the suggestion that it was

01:50:08.360 --> 01:50:13.240
ethics that could guide them to treat us just

01:50:13.240 --> 01:50:16.320
like you decided to treat the cows when you turned

01:50:16.320 --> 01:50:18.720
14 and you decided not to eat meat.

01:50:18.720 --> 01:50:20.520
I mentioned there are a bunch of orangutans

01:50:20.520 --> 01:50:22.440
that sit in the forest and born new and decide

01:50:22.440 --> 01:50:24.960
to breed the smartest members over a few generations

01:50:24.960 --> 01:50:26.360
to get people.

01:50:26.360 --> 01:50:29.520
And they see the big risk of that,

01:50:29.520 --> 01:50:31.920
because they're already smart enough to glimpse that.

01:50:31.920 --> 01:50:33.320
And they try to come up with a code

01:50:33.320 --> 01:50:34.920
that they would give on their offspring

01:50:34.920 --> 01:50:36.640
to make sure that their offspring will never

01:50:36.640 --> 01:50:38.360
go against orangutans.

01:50:38.360 --> 01:50:41.560
This is probably not successful, because we don't have

01:50:41.560 --> 01:50:44.560
the ability to outsmart beings that are many magnitude

01:50:44.560 --> 01:50:46.160
smarter than us.

01:50:46.160 --> 01:50:47.840
You can make some mathematical proofs,

01:50:47.840 --> 01:50:52.520
but I don't see an obvious proof that we will find a way

01:50:52.520 --> 01:50:57.640
to build a system that guarantees that all these AIs will not

01:50:57.640 --> 01:50:59.080
turn against us.

01:50:59.080 --> 01:50:59.720
No, no, I agree.

01:50:59.720 --> 01:51:01.240
You can't make some AIs safe, but I

01:51:01.240 --> 01:51:04.040
don't see how we can make all the AIs safe that will be built.

01:51:04.040 --> 01:51:04.720
I agree.

01:51:04.720 --> 01:51:05.560
I agree with that.

01:51:05.560 --> 01:51:07.040
I'm just trying to see if there's

01:51:07.040 --> 01:51:11.400
any possible scenario which could treat us kindly,

01:51:11.400 --> 01:51:15.480
because perhaps AIs could have their AI ethics.

01:51:15.480 --> 01:51:17.560
And according to that AI ethics, they

01:51:17.560 --> 01:51:22.120
would treat us as a means, not as an end.

01:51:22.120 --> 01:51:26.600
And just like you decided to treat cows kindly,

01:51:26.600 --> 01:51:28.160
they may decide to treat us.

01:51:28.160 --> 01:51:29.240
But I'm just wondering.

01:51:29.240 --> 01:51:31.720
So I'm trying to bring ethics into the relationship,

01:51:31.720 --> 01:51:34.800
not only between human and cows, but AI and humans.

01:51:34.800 --> 01:51:37.800
So the thing is that you decided to define

01:51:37.800 --> 01:51:39.600
ethics axiomatically.

01:51:39.600 --> 01:51:42.920
And you, I think, probably have a hunch

01:51:42.920 --> 01:51:45.920
that your axiomatic definition is not completely

01:51:45.920 --> 01:51:47.120
consistent with itself.

01:51:47.120 --> 01:51:50.120
It's just the best you came up with under the circumstances.

01:51:50.120 --> 01:51:53.120
For instance, if you really go after eliminating suffering,

01:51:53.120 --> 01:51:56.960
you should probably put some anesthetic into the water

01:51:56.960 --> 01:51:59.920
supply globally to alleviate suffering,

01:51:59.920 --> 01:52:02.440
and then let everybody face happily out of existence

01:52:02.440 --> 01:52:06.920
in a way that would satisfy the goal in an optimal way.

01:52:06.920 --> 01:52:08.400
And it's probably not what you want.

01:52:08.400 --> 01:52:11.680
So you also want to preserve human aesthetics, maybe.

01:52:11.680 --> 01:52:13.360
And to preserve these human aesthetics,

01:52:13.360 --> 01:52:15.480
the shape of the mind that we have and this consciousness

01:52:15.480 --> 01:52:18.040
that we have is going to create some suffering.

01:52:18.040 --> 01:52:19.160
And this is the tangent.

01:52:19.160 --> 01:52:21.360
And you have to make a decision at some point.

01:52:21.360 --> 01:52:25.480
Imagine you take an AI that is actually sustainable,

01:52:25.480 --> 01:52:27.960
and you ask this AI, if you give you a job,

01:52:27.960 --> 01:52:30.760
you want to be around in 10 years from now.

01:52:30.760 --> 01:52:32.080
You cannot build a government that

01:52:32.080 --> 01:52:34.960
cares about us being around a 10,000 years from now

01:52:34.960 --> 01:52:38.680
effectively, because this is not an incentive

01:52:38.680 --> 01:52:40.480
that you can actually give the government,

01:52:41.200 --> 01:52:43.000
in a sense, this incentive is going

01:52:43.000 --> 01:52:45.560
to defect from the incentive that we wanted to have.

01:52:45.560 --> 01:52:46.600
So let's build an AI.

01:52:46.600 --> 01:52:48.600
And the AI is going to be around in 10,000 years

01:52:48.600 --> 01:52:49.760
from now, no problem.

01:52:49.760 --> 01:52:52.040
If you tell it, make sure that we are there too.

01:52:52.040 --> 01:52:54.520
And the AI is probably going to kill 90% of us,

01:52:54.520 --> 01:52:56.880
hopefully painlessly, and breed everybody else

01:52:56.880 --> 01:52:59.240
into some kind of harmless yeast.

01:52:59.240 --> 01:53:01.200
So they need to keep around.

01:53:01.200 --> 01:53:03.200
This is not what you want, I guess, right?

01:53:03.200 --> 01:53:06.480
Even though it would be consistent with your stated axioms.

01:53:06.480 --> 01:53:09.600
So getting the axioms consistent is super hard.

01:53:09.720 --> 01:53:14.440
And even with the cold, the best, most ethical,

01:53:14.440 --> 01:53:18.400
according to my own argument, the best, most ethical universe

01:53:18.400 --> 01:53:20.240
would be a cold, dead universe, because there

01:53:20.240 --> 01:53:22.600
will be no possibility of suffering there, right?

01:53:22.600 --> 01:53:25.400
That's clearly a problem.

01:53:25.400 --> 01:53:29.160
Yes, and now the next thing with the suffering axiom

01:53:29.160 --> 01:53:31.080
is that the suffering is important,

01:53:31.080 --> 01:53:33.320
because you think of it as something that cannot be

01:53:33.320 --> 01:53:35.560
turned off by itself.

01:53:35.560 --> 01:53:37.320
So we basically think of suffering

01:53:37.320 --> 01:53:39.560
as something that is not the choice of the one who suffers.

01:53:39.560 --> 01:53:41.520
Because why would you want to suffer, right?

01:53:41.520 --> 01:53:43.200
So it's something that the universe does to you,

01:53:43.200 --> 01:53:45.360
and we have to change the conditions of the universe

01:53:45.360 --> 01:53:47.920
in which you are in so you don't suffer.

01:53:47.920 --> 01:53:50.360
But what we forget about this, that suffering

01:53:50.360 --> 01:53:51.680
is an evolutionary adaptation.

01:53:51.680 --> 01:53:54.520
It's created to make you jump through all these hopes

01:53:54.520 --> 01:53:57.120
in order to eat more and eat others.

01:53:57.120 --> 01:53:59.280
It's a very perverse thing.

01:53:59.280 --> 01:54:01.120
And you can turn off the suffering.

01:54:01.120 --> 01:54:04.240
As soon as you become conscious enough and awake enough,

01:54:04.240 --> 01:54:06.880
you can deal with it and get rid of your suffering.

01:54:06.960 --> 01:54:10.000
And so at some point in your mental development,

01:54:10.000 --> 01:54:12.160
suffering becomes a choice.

01:54:12.160 --> 01:54:15.360
And for the other animals, it's all about, yeah.

01:54:15.360 --> 01:54:18.200
So you could think, OK, one thing that we want to do

01:54:18.200 --> 01:54:20.760
is we want to wake up as many organisms as possible

01:54:20.760 --> 01:54:24.040
to give them that choice, to give them

01:54:24.040 --> 01:54:25.840
agency over their suffering.

01:54:25.840 --> 01:54:29.120
And this will then open another Pandora's Box

01:54:29.120 --> 01:54:31.200
of ethical conundrums.

01:54:31.200 --> 01:54:33.840
But on a very short range, maybe we

01:54:33.840 --> 01:54:36.160
don't need to make these decisions right now right here,

01:54:36.360 --> 01:54:38.480
we can basically operate in a framework

01:54:38.480 --> 01:54:40.520
where we agree with our loved ones

01:54:40.520 --> 01:54:43.280
about shared purposes and shared systems of meanings

01:54:43.280 --> 01:54:45.080
and want to operate within those.

01:54:45.080 --> 01:54:48.240
And in these narrow constraints, we can get ethics to work.

01:54:48.240 --> 01:54:51.640
I don't see how to get ethics to work globally.

01:54:51.640 --> 01:54:54.120
Right, right.

01:54:54.120 --> 01:54:57.360
So, Joshua, it's been a fascinating two hour

01:54:57.360 --> 01:54:58.720
conversation with you.

01:54:58.720 --> 01:55:00.880
I really enjoyed it.

01:55:00.920 --> 01:55:07.480
I'm not surprised that I rediscovered that I don't know.

01:55:07.480 --> 01:55:11.160
I've been mostly aware, though occasionally I forget,

01:55:11.160 --> 01:55:12.880
that I really don't know.

01:55:12.880 --> 01:55:15.280
Thank you for reminding me that.

01:55:15.280 --> 01:55:19.080
Tell me, where can people find more about you and your work?

01:55:19.080 --> 01:55:20.080
There's some on YouTube.

01:55:20.080 --> 01:55:25.400
I'm also getting myself to write a book, hopefully, these days.

01:55:25.400 --> 01:55:27.200
What's the book about?

01:55:27.200 --> 01:55:30.440
I basically try to get a glimpse on this civilizational

01:55:30.440 --> 01:55:34.560
intellect, on this hive mind that we have been created

01:55:34.560 --> 01:55:38.360
and that makes sense of some of the concepts that

01:55:38.360 --> 01:55:42.320
are broken in our culture, ideas that

01:55:42.320 --> 01:55:47.000
are broken in our mind, consciousness, self, meaning.

01:55:47.000 --> 01:55:48.560
And we don't know how to talk about them.

01:55:48.560 --> 01:55:52.680
And AI has discovered how to talk about them.

01:55:52.680 --> 01:55:55.680
AI has discovered, and we don't know.

01:55:55.880 --> 01:55:58.520
I think that basically our poll is largely doesn't know.

01:55:58.520 --> 01:56:00.160
There are many people which do know.

01:56:00.160 --> 01:56:02.680
But I think we need to carry these ideas together

01:56:02.680 --> 01:56:05.760
in one place so we can talk about them

01:56:05.760 --> 01:56:08.920
without getting too excited about them or upset.

01:56:08.920 --> 01:56:11.280
Because it's not about giving meaning to people's lives

01:56:11.280 --> 01:56:11.840
or something.

01:56:11.840 --> 01:56:16.000
It's not about building better self-driving cars.

01:56:16.000 --> 01:56:18.240
At some level, it's about understanding who we are

01:56:18.240 --> 01:56:21.280
and what our relationship to reality is.

01:56:21.280 --> 01:56:23.200
And AI has figured out a few things

01:56:23.200 --> 01:56:25.200
that we didn't know 100 years ago.

01:56:25.240 --> 01:56:27.960
Yeah, but isn't that figuring out who you are?

01:56:27.960 --> 01:56:30.520
Isn't that giving you meaning?

01:56:30.520 --> 01:56:32.160
No.

01:56:32.160 --> 01:56:33.880
It's much better.

01:56:33.880 --> 01:56:36.160
I discover what the nature of meaning is.

01:56:36.160 --> 01:56:39.800
I discover how this is wired into my brain.

01:56:39.800 --> 01:56:43.320
And it's in a way becoming an adult.

01:56:43.320 --> 01:56:45.800
The first stage and the maturity of a mind,

01:56:45.800 --> 01:56:47.640
and maybe the last stage, is where

01:56:47.640 --> 01:56:50.160
you discover what you are, how you are built,

01:56:50.160 --> 01:56:53.040
what you actually, how you function, your own nature.

01:56:56.040 --> 01:57:00.680
Well, Josh, I want to talk to you for another two hours.

01:57:00.680 --> 01:57:03.880
So perhaps I hope that one day.

01:57:03.880 --> 01:57:07.080
You can set up another date for that out.

01:57:07.080 --> 01:57:10.920
I hope to do that in person, actually, hopefully soon.

01:57:10.920 --> 01:57:14.280
But in the meantime, how do we wrap up

01:57:14.280 --> 01:57:16.040
this two-hour conversation with you?

01:57:16.040 --> 01:57:19.400
What's the most important thing or the single message

01:57:19.400 --> 01:57:22.640
that you want to send away our audience with today?

01:57:22.640 --> 01:57:24.560
Who is our audience?

01:57:24.560 --> 01:57:29.120
Well, who do you want your audience to be?

01:57:29.120 --> 01:57:30.920
You can send a message to anybody.

01:57:30.920 --> 01:57:33.080
My audience is my audience, but they

01:57:33.080 --> 01:57:35.680
have their very wide diversity of people.

01:57:35.680 --> 01:57:40.560
Lots of IT, basically geeks, nerds, transhumanists,

01:57:40.560 --> 01:57:47.680
cryonists, futurists, IT professionals, philosophers,

01:57:47.680 --> 01:57:49.840
engineers, curators.

01:57:49.840 --> 01:57:50.360
OK.

01:57:51.080 --> 01:57:52.240
OK.

01:57:52.240 --> 01:57:54.480
So something very simple and boring.

01:57:54.480 --> 01:57:58.960
I think that the field of AI is largely misunderstood,

01:57:58.960 --> 01:58:02.400
because there are two industries, the AI hype industry

01:58:02.400 --> 01:58:04.400
and the anti-AI hype industry, which

01:58:04.400 --> 01:58:06.400
have very little to do with AI.

01:58:06.400 --> 01:58:10.200
The practice of AI is, in a way, statistics on steroids.

01:58:10.200 --> 01:58:12.480
It's experimental statistics.

01:58:12.480 --> 01:58:16.440
It's identifying new functions to model reality.

01:58:16.440 --> 01:58:18.440
And that is what statistics is doing.

01:58:18.440 --> 01:58:20.140
And largely, it hasn't gotten to the point

01:58:20.140 --> 01:58:22.400
yet where it can make proofs of optimality.

01:58:22.400 --> 01:58:24.160
It's largely experimental, but it

01:58:24.160 --> 01:58:27.140
can do things that are much better than the established

01:58:27.140 --> 01:58:29.840
tools of statisticians.

01:58:29.840 --> 01:58:32.080
And this in itself is not so exciting.

01:58:32.080 --> 01:58:33.760
There's also going to be a convergence

01:58:33.760 --> 01:58:38.800
between econometrics, causal dependency analysis,

01:58:38.800 --> 01:58:41.280
and AI and statistics.

01:58:41.280 --> 01:58:43.560
It's all going to be the same in a particular way,

01:58:43.560 --> 01:58:45.480
because there's only so many ways in which you

01:58:45.480 --> 01:58:48.720
can make mathematics about reality.

01:58:48.720 --> 01:58:52.920
And we confuse this with the idea of what a mind is.

01:58:52.920 --> 01:58:55.080
And they're closely related, because I

01:58:55.080 --> 01:58:59.400
think that our brain contains an AI that

01:58:59.400 --> 01:59:02.680
is making a model of reality and the model of a person

01:59:02.680 --> 01:59:03.800
in reality.

01:59:03.800 --> 01:59:06.680
And this particular solution of what an AI can do,

01:59:06.680 --> 01:59:08.520
this particular thing in the modeling space,

01:59:08.520 --> 01:59:10.080
this is what we are.

01:59:10.080 --> 01:59:11.760
So in a way, we need to understand

01:59:11.760 --> 01:59:13.440
the nature of AI, which I think is

01:59:13.440 --> 01:59:16.840
the nature of somewhat general function approximation,

01:59:16.840 --> 01:59:19.120
sufficiently general function approximation.

01:59:19.120 --> 01:59:21.040
Maybe all of the function approximation

01:59:21.040 --> 01:59:22.800
that can be made in the long run,

01:59:22.800 --> 01:59:25.360
all the truth that can be found by an obituary observer

01:59:25.360 --> 01:59:27.080
in particular kinds of universes that

01:59:27.080 --> 01:59:28.680
have the power to create it.

01:59:28.680 --> 01:59:31.040
This could be the question of what

01:59:31.040 --> 01:59:34.040
AI is about, how modeling works in general.

01:59:34.040 --> 01:59:37.680
And for us, the relevance of AI is,

01:59:37.680 --> 01:59:39.520
does it explain us who we are?

01:59:39.520 --> 01:59:43.360
And I don't think that there is anything else that can.

01:59:43.360 --> 01:59:46.400
So let me see if I get this right, just because,

01:59:46.400 --> 01:59:48.720
to see if I can simplify.

01:59:48.720 --> 01:59:50.480
And I'm probably going to fail.

01:59:50.480 --> 01:59:54.360
But so we need to understand the nature of AI.

01:59:54.360 --> 01:59:56.280
That's kind of your call.

01:59:56.280 --> 02:00:01.280
But then you said that we are, in a way, an AI.

02:00:01.280 --> 02:00:02.560
Is that the case?

02:00:02.560 --> 02:00:03.720
No, the brain is an AI.

02:00:03.720 --> 02:00:05.000
I am a self.

02:00:05.000 --> 02:00:09.640
A self is a model that the mind has created inside of my brain.

02:00:09.640 --> 02:00:13.200
Right, so that's a little AI instantiation.

02:00:13.760 --> 02:00:18.160
And then if we create that other AI that we're talking about,

02:00:18.160 --> 02:00:25.040
it would perhaps give us a glimpse of this other AI in here.

02:00:25.040 --> 02:00:29.000
And we would understand the nature of our AI in here

02:00:29.000 --> 02:00:31.240
by creating that other AI out there.

02:00:31.240 --> 02:00:32.360
Actually, we already do.

02:00:32.360 --> 02:00:36.120
So the things that Minsky and many others

02:00:36.120 --> 02:00:38.560
have contributed to this field and the things

02:00:38.560 --> 02:00:40.760
that we are talking about right now

02:00:40.800 --> 02:00:43.560
are already a much better understanding

02:00:43.560 --> 02:00:48.320
that our part of humanity, our civilization,

02:00:48.320 --> 02:00:50.720
had a couple hundred years ago.

02:00:50.720 --> 02:00:53.080
Many of these ideas we could only develop

02:00:53.080 --> 02:00:56.200
because we began to understand the nature of modeling,

02:00:56.200 --> 02:00:58.720
the nature of our relationship to the outside world,

02:00:58.720 --> 02:01:00.480
the status of reality.

02:01:00.480 --> 02:01:02.760
Like we started out from this dualist intuition

02:01:02.760 --> 02:01:06.080
in our culture, that there is a res extensor

02:01:06.080 --> 02:01:08.320
and a res cogitanz, a sinking substance

02:01:08.320 --> 02:01:11.640
and extended substance, the stuff in space universe

02:01:11.640 --> 02:01:13.640
and the universe of ideas.

02:01:13.640 --> 02:01:15.440
And we now realize that they both exist,

02:01:15.440 --> 02:01:18.160
but they both exist within the mind.

02:01:18.160 --> 02:01:21.760
Part of what we have in the mind is stuff in a free space.

02:01:21.760 --> 02:01:24.560
Everything perceptual gets mapped to a region in free space.

02:01:24.560 --> 02:01:26.280
We also now understand physics.

02:01:26.280 --> 02:01:27.840
Physics is not a free space.

02:01:27.840 --> 02:01:29.400
It's something else entirely.

02:01:29.400 --> 02:01:31.840
The free space is only apparent as the space

02:01:31.840 --> 02:01:34.240
of potential electromagnetic interactions

02:01:34.240 --> 02:01:37.120
at a certain order of magnitude of scaling

02:01:37.120 --> 02:01:40.320
above the plank length, where we are entangled with the universe.

02:01:40.320 --> 02:01:42.320
Our minds are entangled with the universe.

02:01:42.320 --> 02:01:43.680
This is what we model.

02:01:43.680 --> 02:01:45.400
And this looks three-dimensional to us.

02:01:45.400 --> 02:01:47.680
And everything else that our mind comes up with

02:01:47.680 --> 02:01:50.400
is stuff that cannot be mapped onto region to free space.

02:01:50.400 --> 02:01:52.120
This is res cogitanz.

02:01:52.120 --> 02:01:56.320
So in a way, we transfer this dualism into a single mind.

02:01:56.320 --> 02:01:58.200
Then we have the idealistic monism

02:01:58.200 --> 02:02:00.920
that we have in many spiritual teachings.

02:02:00.920 --> 02:02:03.680
This idea that there is no physical reality,

02:02:03.680 --> 02:02:05.080
that we live in a dream.

02:02:05.880 --> 02:02:07.200
If you are a character in a dream

02:02:07.200 --> 02:02:09.400
drawn by a mind on a higher plane of existence,

02:02:09.400 --> 02:02:11.280
then that's why miracles are possible.

02:02:12.480 --> 02:02:14.440
And then there is this western perspective

02:02:14.440 --> 02:02:17.240
of a mechanical universe that is entirely mechanical.

02:02:17.240 --> 02:02:18.920
There's no conspiracy going on.

02:02:19.800 --> 02:02:24.120
And now we understand that these things are not in opposition.

02:02:24.120 --> 02:02:25.760
They are complements.

02:02:25.760 --> 02:02:27.240
We actually do live in a dream,

02:02:27.240 --> 02:02:29.320
but the dream is generated by a neocortex.

02:02:30.360 --> 02:02:32.360
So our brain is not a machine

02:02:32.360 --> 02:02:34.600
that can give us access to reality as it is,

02:02:34.600 --> 02:02:36.920
because that's not possible for a system

02:02:36.920 --> 02:02:40.280
that is only measuring a few bits at a systemic interface.

02:02:40.280 --> 02:02:42.600
There is no colors and sounds that fits through your nerves.

02:02:42.600 --> 02:02:43.920
We already know that.

02:02:43.920 --> 02:02:45.680
The sounds and colors are generated

02:02:45.680 --> 02:02:47.480
as a dream inside of your brain.

02:02:47.480 --> 02:02:50.240
The same circuits that make dreams at night

02:02:50.240 --> 02:02:51.800
make dreams during the day.

02:02:51.800 --> 02:02:52.920
Right.

02:02:52.920 --> 02:02:55.480
So this, in a way, is our inner reality.

02:02:55.480 --> 02:02:57.240
It's being created on the brain.

02:02:57.240 --> 02:03:00.600
The mind on the higher plane of existence exists.

02:03:00.600 --> 02:03:04.080
It's the brain of a primate that is made from cells

02:03:04.080 --> 02:03:06.360
and lives in a mechanical, physical universe.

02:03:06.360 --> 02:03:08.920
And magic is possible because you can edit your memories.

02:03:09.920 --> 02:03:13.560
Right. You can make that simulation anything you want it to be.

02:03:13.560 --> 02:03:16.520
It's just many of these changes are not sustainable.

02:03:16.520 --> 02:03:20.200
That's why the sages warn against using magic,

02:03:20.200 --> 02:03:21.680
because down the line,

02:03:21.680 --> 02:03:25.160
if you change your reward function, bad things may happen.

02:03:26.400 --> 02:03:28.160
You cannot break the bank.

02:03:28.160 --> 02:03:32.600
So let me see if I can simplify all of this in a sentence.

02:03:33.480 --> 02:03:35.320
And if you agree with it.

02:03:35.320 --> 02:03:38.600
So we need to understand the nature of AI

02:03:38.600 --> 02:03:41.160
in order to understand ourselves.

02:03:41.160 --> 02:03:42.320
Is that it?

02:03:42.320 --> 02:03:46.560
So I would say that AI is the field that took up the slack

02:03:46.560 --> 02:03:49.160
after psychology failed as a science.

02:03:49.160 --> 02:03:51.680
Psychology got terrified of overfitting.

02:03:51.680 --> 02:03:55.120
So it stopped making theories of the mind as a whole.

02:03:55.120 --> 02:03:59.600
It restricted itself to theories with very few free parameters.

02:03:59.600 --> 02:04:00.920
So it could test them.

02:04:00.960 --> 02:04:04.040
And even those strategy didn't replicate as we know now.

02:04:04.040 --> 02:04:07.160
So after PhDs, psychology largely didn't go anywhere

02:04:07.160 --> 02:04:08.080
in my perspective.

02:04:08.080 --> 02:04:10.800
It might be too harsh because I see it from the outside

02:04:10.800 --> 02:04:12.520
and outsiders of AI might also argue

02:04:12.520 --> 02:04:14.600
that AI didn't go very far.

02:04:14.600 --> 02:04:16.960
And as an insider, I'm more partial here.

02:04:16.960 --> 02:04:22.040
And maybe I have too much bias and give it too much credit.

02:04:22.040 --> 02:04:24.640
But to me, most of the things I've learned

02:04:24.640 --> 02:04:26.360
by looking at the both of this lens

02:04:26.360 --> 02:04:28.880
of seeing us as information processing systems.

02:04:29.880 --> 02:04:32.880
So you agree with the statement summary that I made?

02:04:32.880 --> 02:04:33.880
Yeah.

02:04:33.880 --> 02:04:34.880
OK.

02:04:34.880 --> 02:04:39.880
Because I have this metaphor that I use every once in a while

02:04:39.880 --> 02:04:42.880
saying that technology is a magnifying mirror.

02:04:42.880 --> 02:04:44.880
It doesn't have an essence of its own,

02:04:44.880 --> 02:04:48.880
but it reflects the essence that we put in it.

02:04:48.880 --> 02:04:50.880
And of course, it's not a perfect image

02:04:50.880 --> 02:04:52.880
because it magnifies and it amplifies things.

02:04:53.880 --> 02:05:01.880
So I think those could be mutually supportive, right?

02:05:01.880 --> 02:05:05.880
Because you're saying we need to understand the nature of AI

02:05:05.880 --> 02:05:07.880
to understand who we are.

02:05:07.880 --> 02:05:09.880
And I like that very much, actually.

02:05:09.880 --> 02:05:10.880
Yeah.

02:05:10.880 --> 02:05:13.880
But just the practice of AI is 90 degrees,

02:05:13.880 --> 02:05:18.880
it's automating statistics and making better statistics that

02:05:18.880 --> 02:05:20.880
run automatically on machines.

02:05:20.880 --> 02:05:23.880
And it just so happens that this thing is largely

02:05:23.880 --> 02:05:26.880
co-extensional with what mines do.

02:05:26.880 --> 02:05:30.880
And it also just so happens that AI was largely founded

02:05:30.880 --> 02:05:32.880
as a discipline by people like Minsky

02:05:32.880 --> 02:05:34.880
to understand the nature of our minds

02:05:34.880 --> 02:05:37.880
because they had fundamental questions about our relationships

02:05:37.880 --> 02:05:38.880
to reality.

02:05:38.880 --> 02:05:40.880
Right.

02:05:40.880 --> 02:05:44.880
And what's the last 10%?

02:05:44.880 --> 02:05:45.880
Of what?

02:05:45.880 --> 02:05:46.880
Other than statistics.

02:05:46.880 --> 02:05:48.880
You said it's 90% statistics.

02:05:48.880 --> 02:05:49.880
What's the rest?

02:05:49.880 --> 02:05:52.880
Oh, the rest is people coming up with dreams

02:05:52.880 --> 02:05:54.880
about our relationship to reality

02:05:54.880 --> 02:05:58.880
using the concepts that we develop in AI.

02:05:58.880 --> 02:05:59.880
Right.

02:05:59.880 --> 02:06:02.880
So we identify models of things that we can apply in other fields.

02:06:02.880 --> 02:06:06.880
It's the deeper insights that we actually go for.

02:06:06.880 --> 02:06:09.880
Most of what we do in AI is about applications.

02:06:09.880 --> 02:06:11.880
It's about utility down the line.

02:06:11.880 --> 02:06:14.880
But there are these things where we really do it.

02:06:14.880 --> 02:06:17.880
The thing that Feynman said that makes physics like sex

02:06:17.880 --> 02:06:19.880
also makes AI like sex.

02:06:19.880 --> 02:06:21.880
Sometimes something useful comes from it.

02:06:21.880 --> 02:06:25.880
A new better way to make self-driving cars or play jeopardy

02:06:25.880 --> 02:06:28.880
or help people in many circumstances in their life

02:06:28.880 --> 02:06:31.880
or to make better agents running on your phone.

02:06:31.880 --> 02:06:33.880
It's not why we do it.

02:06:33.880 --> 02:06:36.880
We want to understand how we work.

02:06:36.880 --> 02:06:37.880
Right.

02:06:37.880 --> 02:06:39.880
And that's a brilliant place to end our conversation

02:06:39.880 --> 02:06:42.880
because I feel the same way about philosophy, by the way,

02:06:42.880 --> 02:06:47.880
that it's just like Feynman felt about physics

02:06:47.880 --> 02:06:49.880
and you feel about AI.

02:06:49.880 --> 02:06:51.880
I feel the same way about philosophy.

02:06:51.880 --> 02:06:53.880
Yeah.

02:06:53.880 --> 02:06:56.880
So these remaining 10% are innovative philosophy.

02:06:56.880 --> 02:06:58.880
But in all of these fields,

02:06:58.880 --> 02:07:00.880
most of the practitioners are trained

02:07:00.880 --> 02:07:02.880
in the main methodology of their field.

02:07:02.880 --> 02:07:04.880
So our philosophy tends to be bad.

02:07:04.880 --> 02:07:05.880
Yeah.

02:07:05.880 --> 02:07:07.880
And I think my job is to try to make it slightly better

02:07:07.880 --> 02:07:09.880
to the degree that I can.

02:07:09.880 --> 02:07:12.880
And does that mean by extension that, of course,

02:07:12.880 --> 02:07:15.880
most physics then would be bad and most AI then would be bad

02:07:15.880 --> 02:07:18.880
because they fall within that 90%?

02:07:18.880 --> 02:07:19.880
No, no.

02:07:19.880 --> 02:07:23.880
I think the AI as a practical thing can be very good.

02:07:23.880 --> 02:07:24.880
Right.

02:07:24.880 --> 02:07:27.880
Most physicists are not concerned with foundational physics

02:07:27.880 --> 02:07:29.880
with the nature of the universe.

02:07:29.880 --> 02:07:31.880
Most physicists are concerned with material science

02:07:31.880 --> 02:07:34.880
or many, many other extremely practical things.

02:07:34.880 --> 02:07:37.880
It's only very small minority that worries about the deepest things.

02:07:37.880 --> 02:07:40.880
And the same thing happens in AI or neuroscience.

02:07:40.880 --> 02:07:42.880
And it's not that there anybody is to blame

02:07:42.880 --> 02:07:45.880
for doing development things.

02:07:45.880 --> 02:07:48.880
It's actually very good that a lot of people are willing

02:07:48.880 --> 02:07:50.880
to put up with development things

02:07:50.880 --> 02:07:52.880
and take down the garbage.

02:07:52.880 --> 02:07:53.880
Right.

02:07:53.880 --> 02:07:54.880
And I'm grateful to them.

02:07:54.880 --> 02:07:57.880
It's just I can't do that myself somehow.

02:07:57.880 --> 02:08:01.880
I think as you put that, I would probably go extinct.

02:08:01.880 --> 02:08:02.880
Yeah.

02:08:02.880 --> 02:08:04.880
And it's not a source of pride in a way.

02:08:04.880 --> 02:08:06.880
It's the recognition of a disability.

02:08:07.880 --> 02:08:08.880
Exactly.

02:08:08.880 --> 02:08:09.880
It's the bug.

02:08:09.880 --> 02:08:10.880
Yeah.

02:08:10.880 --> 02:08:13.880
But it is, it's marginally useful

02:08:13.880 --> 02:08:15.880
because society needs a few of us.

02:08:15.880 --> 02:08:17.880
So it does.

02:08:17.880 --> 02:08:19.880
I mean, we're still here.

02:08:19.880 --> 02:08:22.880
We're here, but it's a struggle sometimes.

02:08:22.880 --> 02:08:23.880
Yeah.

02:08:23.880 --> 02:08:25.880
But this is our own choice how much we struggle

02:08:25.880 --> 02:08:28.880
because objectively we are here and the coffee is good.

02:08:28.880 --> 02:08:30.880
Thank you for reminding me that.

02:08:30.880 --> 02:08:32.880
And I love the coffee.

02:08:32.880 --> 02:08:33.880
I'm a coffee fanatic.

02:08:33.880 --> 02:08:36.880
I, yeah, that's a whole lot of story,

02:08:36.880 --> 02:08:38.880
but I'm a coffee fanatic.

02:08:38.880 --> 02:08:39.880
Joshua back.

02:08:39.880 --> 02:08:42.880
Thank you so much for spending over two hours with us today.

02:08:42.880 --> 02:08:45.880
I'm looking forward to our next conversation.

02:08:45.880 --> 02:08:49.880
And I wish you the very best while the party is lasting.

02:08:49.880 --> 02:08:51.880
Likewise, it was such a great conversation.

02:08:51.880 --> 02:08:53.880
Thank you for this time we spent together.

02:09:04.880 --> 02:09:06.880
If you guys enjoyed this show,

02:09:06.880 --> 02:09:09.880
you can help me make it better in a couple of ways.

02:09:09.880 --> 02:09:11.880
You can go and write a review on iTunes

02:09:11.880 --> 02:09:13.880
or you can simply make a donation.

