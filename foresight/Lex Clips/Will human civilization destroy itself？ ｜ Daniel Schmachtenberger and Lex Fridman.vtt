WEBVTT

00:00.000 --> 00:06.000
What flavors of catastrophic risk are we talking about here?

00:06.000 --> 00:09.000
What's your favorite flavor in terms of ice cream?

00:09.000 --> 00:12.000
Mine is coconut. Nobody seems to like coconut ice cream.

00:12.000 --> 00:20.000
So ice cream aside, what do you most worry about in terms of catastrophic risk

00:20.000 --> 00:29.000
that will help us kind of make concrete the discussion we're having about how to fix this whole thing?

00:29.000 --> 00:34.000
Yeah, I think it's worth taking a historical perspective briefly to just kind of orient everyone to it.

00:34.000 --> 00:39.000
We don't have to go all the way back to the aliens who've seen all of civilization,

00:39.000 --> 00:50.000
but to just recognize that for all of human history, as far as we're aware, there were existential risks to civilizations,

00:50.000 --> 00:55.000
and they happened, right? Like there were civilizations that were killed in war,

00:55.000 --> 00:58.000
and tribes that were killed in tribal warfare or whatever.

00:58.000 --> 01:02.000
So people faced existential risk to the group that they identified with.

01:02.000 --> 01:06.000
It's just those were local phenomena, right? It wasn't a fully global phenomena.

01:06.000 --> 01:10.000
So an empire could fall, and surrounding empires didn't fall.

01:10.000 --> 01:14.000
Maybe they came in and filled the space.

01:14.000 --> 01:19.000
The first time that we were able to think about catastrophic risk,

01:19.000 --> 01:22.000
not from like a solar flare or something that we couldn't control,

01:22.000 --> 01:27.000
but from something that humans would actually create at a global level was World War II in the bomb.

01:27.000 --> 01:33.000
Because it was the first time that we had tech big enough that could actually mess up everything at a global level,

01:33.000 --> 01:37.000
that could mess up habitability. We just weren't powerful enough to do that before.

01:37.000 --> 01:40.000
It's not that we didn't behave in ways that would have done it.

01:40.000 --> 01:43.000
We just only behaved in those ways at the scale we could affect.

01:43.000 --> 01:48.000
And so it's important to get that there's the entire world before World War II,

01:48.000 --> 01:53.000
where we don't have the ability to make a non-habitable biosphere, non-habitable for us.

01:53.000 --> 01:57.000
And then there's World War II and the beginning of a completely new phase

01:57.000 --> 02:02.000
where global human-induced catastrophic risk is now a real thing.

02:02.000 --> 02:07.000
And that was such a big deal that it changed the entire world in a really fundamental way,

02:07.000 --> 02:14.000
which is, you know, when you study history, it's amazing how big a percentage of history is studying war, right?

02:14.000 --> 02:17.000
And the history of wars, you study European history and whatever.

02:17.000 --> 02:20.000
It's generals and wars and empire expansions.

02:20.000 --> 02:25.000
And so the major empires near each other never had really long periods of time

02:25.000 --> 02:29.000
where they weren't engaged in war or preparation for war or something like that.

02:29.000 --> 02:34.000
Humans don't have a good precedent in the post-tribal phase,

02:34.000 --> 02:39.000
the civilization phase of being able to solve conflicts without war for very long.

02:39.000 --> 02:45.000
World War II was the first time where we could have a war that no one could win.

02:45.000 --> 02:49.000
And so the superpowers couldn't fight again. They couldn't do a real kinetic war.

02:49.000 --> 02:52.000
They could do diplomatic wars and Cold War type stuff

02:52.000 --> 02:56.000
and they could fight proxy wars through other countries that didn't have the big weapons.

02:56.000 --> 03:00.000
And so mutually assured destruction and like coming out of World War II,

03:00.000 --> 03:04.000
we actually realized that nation states couldn't prevent world war.

03:04.000 --> 03:08.000
And so we needed a new type of supervening government in addition to nation states,

03:08.000 --> 03:13.000
which was the whole Bretton Woods world, the United Nations, the World Bank, the IMF,

03:13.000 --> 03:18.000
the globalization trade type agreements, mutually assured destruction,

03:18.000 --> 03:23.000
that was how do we have some coordination beyond just nation states between them

03:23.000 --> 03:26.000
since we have to stop war between at least the superpowers.

03:26.000 --> 03:35.000
And it was pretty successful, given that we've had like 75 years of no superpower on superpower war.

03:35.000 --> 03:40.000
We've had lots of proxy wars during that time. We've had, you know, Cold War.

03:40.000 --> 03:48.000
And I would say we're in a new phase now where the Bretton Woods solution is basically over, almost over.

03:48.000 --> 03:50.000
Can you describe the Bretton Woods solution?

03:50.000 --> 04:01.000
Yeah, so the Bretton Woods, the series of agreements for how the nations would be able to engage with each other

04:02.000 --> 04:09.000
was these IGOs, these intergovernmental organizations, and was the idea of globalization.

04:09.000 --> 04:12.000
Since we could have global effects, we needed to be able to think about things globally,

04:12.000 --> 04:18.000
where we had trade relationships with each other, where it would not be profitable to war with each other.

04:18.000 --> 04:20.000
It would be more profitable to actually be able to trade with each other.

04:20.000 --> 04:25.000
So our own self-interest was, you know, going to drive our non-war interest.

04:26.000 --> 04:32.000
And so this started to look like, and obviously this couldn't have happened that much earlier either,

04:32.000 --> 04:37.000
because industrialization hadn't gotten far enough to be able to do massive global industrial supply chains

04:37.000 --> 04:39.000
and ship stuff around, you know, quickly.

04:39.000 --> 04:45.000
But like we were mentioning earlier, almost all the electronics that we use today, just basic, cheap stuff for us,

04:45.000 --> 04:47.000
is made on six continents, made in many countries.

04:47.000 --> 04:51.000
There's no single country in the world that could actually make many of the things that we have,

04:51.000 --> 04:57.000
from the raw material extraction to the plastics and polymers and the, you know, et cetera.

04:57.000 --> 05:03.000
And so the idea that we made a world that could do that kind of trade and create massive GDP growth,

05:03.000 --> 05:08.000
we could all work together to be able to mine natural resources and grow stuff.

05:08.000 --> 05:12.000
With the rapid GDP growth, there was the idea that everybody could keep having more,

05:12.000 --> 05:15.000
without having to take each other's stuff.

05:15.000 --> 05:20.000
And so that was part of kind of the Bretton Woods post-World War II model.

05:20.000 --> 05:25.000
The other was that we would be so economically interdependent that blowing each other up would never make sense.

05:25.000 --> 05:28.000
That worked for a while.

05:28.000 --> 05:34.000
Now, it also brought us up into planetary boundaries faster,

05:34.000 --> 05:41.000
the unrenovable use of resource and turning those resources into pollution on the other side of the supply chain.

05:41.000 --> 05:47.000
So obviously that faster GDP growth meant the overfishing of the oceans

05:47.000 --> 05:53.000
and the cutting down of the trees and the climate change and the mining, toxic mining tailings going into the water

05:53.000 --> 05:56.000
and the mountaintop removal mining and all those types of things.

05:56.000 --> 06:00.000
That's the consumption side of the rest that we're talking about.

06:00.000 --> 06:07.000
And so the answer of let's do positive GDP is the answer rapidly and exponentially

06:07.000 --> 06:11.000
obviously accelerated the planetary boundary side.

06:11.000 --> 06:20.000
And that was thought about for a long time, but it started to be modeled with the Club of Rome and limits of growth.

06:20.000 --> 06:27.000
But it's just very obvious to say, if you have a linear materials economy where you take stuff out of the earth faster,

06:27.000 --> 06:34.000
whether it's fish or trees or oil, you take it out of the earth faster than it can replenish itself.

06:34.000 --> 06:37.000
And you turn it into trash after using it for a short period of time.

06:37.000 --> 06:41.000
You put the trash in the environment faster than it can process itself.

06:41.000 --> 06:44.000
And there's toxicity associated with both sides of this.

06:44.000 --> 06:49.000
You can't run an exponentially growing linear materials economy on a finite planet forever.

06:49.000 --> 06:51.000
That's not a hard thing to figure out.

06:51.000 --> 06:58.000
And it has to be exponential if there's an exponentiation in the monetary supply because of interest and then fractional reserve banking

06:58.000 --> 07:02.000
and to then be able to keep up with the growing monetary supply.

07:02.000 --> 07:03.000
You have to have growth of goods and services.

07:04.000 --> 07:09.000
So that's that kind of thing that has happened.

07:09.000 --> 07:15.000
But you also see that when you get these supply chains that are so interconnected across the world, you get increased fragility

07:15.000 --> 07:22.000
because a collapse or a problem in one area then affects the whole world in a much bigger area as opposed to the issues being local.

07:22.000 --> 07:30.000
So we got to see with COVID and an issue that started in one part of China affecting the whole world so much more rapidly

07:30.000 --> 07:33.000
than would have happened before Bretton Woods, right?

07:33.000 --> 07:37.000
Before international travel supply chains, you know, that whole kind of thing.

07:37.000 --> 07:40.000
And with a bunch of second and third order effects that people wouldn't have predicted, okay?

07:40.000 --> 07:44.000
We have to stop certain kinds of travel because of viral contaminants.

07:44.000 --> 07:50.000
But the countries doing agriculture depend upon fertilizer they don't produce that is shipped into them

07:50.000 --> 07:52.000
and depend upon pesticides they don't produce.

07:52.000 --> 07:58.000
So we got both crop failures and crops being eaten by locusts in scale in Northern Africa and Iran and things like that

07:58.000 --> 08:00.000
because they couldn't get the supplies of stuff in.

08:00.000 --> 08:06.000
So then you get massive starvation or future kind of hunger issues because of supply chain shutdowns.

08:06.000 --> 08:13.000
So you get this increased fragility and cascade dynamics where a small problem can end up leading to cascade effects.

08:13.000 --> 08:25.000
And also we went from two superpowers with one catastrophe weapon to now that same catastrophe weapon

08:25.000 --> 08:31.000
is there's more countries that have it, eight or nine countries that have it,

08:31.000 --> 08:34.000
and there's a lot more types of catastrophe weapons.

08:34.000 --> 08:40.000
We now have catastrophe weapons with weaponized drones that can hit infrastructure targets with bio

08:40.000 --> 08:44.000
with, in fact, every new type of tech has created an arms race.

08:44.000 --> 08:49.000
So we have not, with the UN or the other kind of intergovernmental organizations,

08:49.000 --> 08:52.000
we haven't been able to really do nuclear deproliferation.

08:52.000 --> 08:59.000
We've actually had more countries get nukes and keep getting faster nukes, the race to hypersonics and things like that.

08:59.000 --> 09:04.000
And every new type of technology that has emerged has created an arms race.

09:04.000 --> 09:11.000
And so you can't do mutually assured destruction with multiple agents the way you can with two agents.

09:11.000 --> 09:16.000
Two agents, it's much easier to create a stable Nash equilibrium that's forced.

09:16.000 --> 09:21.000
But the ability to modern say, if these guys shoot, who do I shoot? Do I shoot them? Do I shoot everybody? Do I?

09:21.000 --> 09:25.000
And so you get a three-body problem. You get a very complex type of thing when you have multiple agents

09:25.000 --> 09:30.000
and multiple different types of catastrophe weapons, including ones that can be much more easily produced than nukes.

09:30.000 --> 09:33.000
Nukes are really hard to produce. There's only uranium in a few areas.

09:33.000 --> 09:36.000
Uranium enrichment is hard. ICBMs are hard.

09:36.000 --> 09:40.000
But weaponized drones hitting smart targets is not so hard.

09:40.000 --> 09:47.000
There's a lot of other things where basically the scale at being able to manufacture them is going way, way down to where even non-state actors can have them.

09:47.000 --> 09:54.000
And so when we talk about exponential tech and the decentralization of exponential tech,

09:54.000 --> 09:58.000
what that means is decentralized catastrophe weapon capacity.

09:58.000 --> 10:06.000
And especially in a world of increasing numbers of people feeling disenfranchised, frantic, whatever, for different reasons.

10:06.000 --> 10:14.000
So I would say the Bretton Woods world doesn't prepare us to be able to deal with lots of different agents,

10:14.000 --> 10:18.000
having lots of different types of catastrophe weapons you can't put mutually assured destruction on,

10:18.000 --> 10:25.000
where you can't keep doing growth of the materials economy in the same way because of hitting planetary boundaries

10:25.000 --> 10:31.000
and where the fragility dynamics are actually now their own source of catastrophic risk.

10:31.000 --> 10:34.000
So now we're, so like there was all the world until World War II,

10:34.000 --> 10:39.000
and World War II is just from a civilization timescale point of view as just a second ago.

10:39.000 --> 10:41.000
It seems like a long time, but it is really not.

10:41.000 --> 10:44.000
We get a short period of relative peace at the level of superpowers

10:44.000 --> 10:48.000
while building up the military capacity for much, much, much worse war the entire time.

10:48.000 --> 10:55.000
And then now we're at this new phase where the things that allowed us to make it through the nuclear power

10:55.000 --> 10:59.000
are not the same systems that will let us make it through the next stage.

10:59.000 --> 11:02.000
So what is this next post Bretton Woods?

11:02.000 --> 11:10.000
How do we become safe vessels, safe stewards of many different types of exponential technology

11:10.000 --> 11:14.000
is a key question when we're thinking about X-Risk.

11:14.000 --> 11:22.000
Okay, so, and I'd like to try to answer the how a few ways,

11:22.000 --> 11:25.000
but first on the mutually assured destruction,

11:26.000 --> 11:31.000
do you give credit to the idea of two superpowers

11:31.000 --> 11:38.000
not blowing each other up with nuclear weapons to the simple game theoretic model of mutually assured destruction

11:38.000 --> 11:43.000
or something you've said previously, this idea of inverse correlation,

11:43.000 --> 11:50.000
which I tend to believe between the, now you were talking about tech,

11:50.000 --> 11:55.000
but I think it's maybe broadly true the inverse correlation

11:55.000 --> 11:58.000
between competence and propensity for destruction.

11:58.000 --> 12:07.000
So the better the bigger your weapons, not because you're afraid of mutually assured self-destruction,

12:07.000 --> 12:13.000
but because we're human beings and there's a deep moral fortitude there

12:13.000 --> 12:16.000
that somehow aligned with competence and being good at your job.

12:17.000 --> 12:26.000
It's very hard to be a psychopath and be good at killing at scale.

12:28.000 --> 12:30.000
Do you share any of that intuition?

12:30.000 --> 12:31.000
Kind of.

12:33.000 --> 12:39.000
I think most people would say that Alexander the Great and Genghis Khan and Napoleon were effective people

12:39.000 --> 12:45.000
that were good at their job, that were actually maybe asymmetrically good.

12:46.000 --> 12:50.000
At being able to organize people and do certain kinds of things

12:50.000 --> 12:54.000
that were pretty oriented towards certain types of destruction

12:54.000 --> 13:00.000
or pretty willing to, maybe they would say they were oriented towards empire expansion,

13:00.000 --> 13:04.000
but pretty willing to commit certain acts of destruction in the name of it.

13:04.000 --> 13:11.000
What are you worried about, the Genghis Khan or you could argue he's not a psychopath,

13:12.000 --> 13:16.000
are you worried about Genghis Khan, are you worried about Hitler,

13:16.000 --> 13:22.000
are you worried about a terrorist who has a very different ethic,

13:22.000 --> 13:31.000
which is not even for, it's not trying to preserve and build and expand my community.

13:31.000 --> 13:35.000
It's more about just destruction in itself is the goal.

13:35.000 --> 13:39.000
I think the thing that you're looking at that I do agree with

13:39.000 --> 13:44.000
is that there's a psychological disposition towards construction

13:44.000 --> 13:47.000
and a psychological disposition more towards destruction.

13:47.000 --> 13:50.000
Obviously everybody has both and can toggle between both

13:50.000 --> 13:53.000
and oftentimes one is willing to destroy certain things.

13:53.000 --> 13:55.000
We have this idea of creative destruction, right?

13:55.000 --> 13:58.000
Willing to destroy certain things to create other things

13:58.000 --> 14:02.000
and utilitarianism and trolley problems are all about exploring that space

14:02.000 --> 14:05.000
and the idea of war is all about that.

14:05.000 --> 14:09.000
I am trying to create something for our people and it requires destroying some other people.

14:13.000 --> 14:17.000
Sociopathy is a funny topic because it's possible to have very high fealty to your in-group

14:17.000 --> 14:22.000
and work on perfecting the methods of torture to the out-group at the same time

14:22.000 --> 14:25.000
because you can dehumanize and then remove empathy.

14:29.000 --> 14:32.000
I would also say that there are types.

14:33.000 --> 14:38.000
The thing that gives hope about the orientation towards construction

14:38.000 --> 14:41.000
and destruction being a little different in psychology

14:41.000 --> 14:45.000
is what it takes to build really catastrophic tech

14:45.000 --> 14:50.000
even today where it doesn't take what it took to make a small group of people could do it

14:50.000 --> 14:55.000
takes still some real technical knowledge that required having studied for a while

14:55.000 --> 14:58.000
and some then building capacity.

14:58.000 --> 15:02.000
And there's a question of is that psychologically inversely correlated

15:02.000 --> 15:06.000
with the desire to damage civilization, meaningfully?

15:08.000 --> 15:11.000
A little bit, a little bit, I think.

15:12.000 --> 15:13.000
I think a lot.

15:13.000 --> 15:17.000
I think it's actually, I mean, this is the conversation I had with,

15:17.000 --> 15:21.000
I think offline with Dan Carlin, which is like,

15:21.000 --> 15:25.000
it's pretty easy to come up with ways that any competent,

15:25.000 --> 15:29.000
I can come up with a lot of ways to hurt a lot of people.

15:29.000 --> 15:32.000
And it's pretty easy, like I alone could do it.

15:35.000 --> 15:39.000
There's a lot of people as smarter, smarter than me,

15:39.000 --> 15:42.000
at least in their creation of explosives.

15:42.000 --> 15:47.000
Why are we not seeing more insane mass murder?

15:47.000 --> 15:53.000
I think there's something fascinating and beautiful about this.

15:53.000 --> 15:59.000
And it does have to do with some deeply pro-social types of characteristics in humans.

16:02.000 --> 16:05.000
But when you're dealing with very large numbers,

16:05.000 --> 16:08.000
you don't need a whole lot of a phenomena.

16:08.000 --> 16:09.000
And so then you start to say,

16:09.000 --> 16:12.000
well, what's the probability that X won't happen this year,

16:12.000 --> 16:14.000
then won't happen in the next two years, three years, four years.

16:14.000 --> 16:18.000
And then how many people are doing destructive things with lower tech

16:18.000 --> 16:21.000
and then how many of them can get access to higher tech

16:21.000 --> 16:24.000
because they didn't have to figure out how to build.

16:24.000 --> 16:28.000
So when I can get commercial tech,

16:28.000 --> 16:31.000
and maybe I don't understand tech very well,

16:31.000 --> 16:34.000
but I understand it well enough to utilize it, not to create it,

16:34.000 --> 16:36.000
and I can repurpose it,

16:36.000 --> 16:41.000
when we saw that commercial drone with a homemade thermite bomb

16:41.000 --> 16:43.000
hit the Ukrainian munitions factory

16:43.000 --> 16:47.000
and do the equivalent of an incendiary bomb level of damage,

16:47.000 --> 16:49.000
that was just home tech.

16:49.000 --> 16:51.000
It was a simple kind of thing.

16:51.000 --> 16:55.000
And so the question is not

16:55.000 --> 16:59.000
does it stay being a small percentage of the population?

16:59.000 --> 17:04.000
The question is, can you bind that phenomena nearly completely?

17:06.000 --> 17:10.000
And especially now as you start to get into bigger things,

17:10.000 --> 17:15.000
CRISPR gene drive technologies and various things like that,

17:15.000 --> 17:18.000
can you bind it completely long term?

17:19.000 --> 17:20.000
Over what period of time?

17:20.000 --> 17:21.000
Not perfectly though.

17:21.000 --> 17:22.000
That's the thing.

17:22.000 --> 17:27.000
I'm trying to say that there is some, let's call it,

17:27.000 --> 17:32.000
a random word, love that's inherent in,

17:32.000 --> 17:35.000
that's core to human nature,

17:35.000 --> 17:38.000
that's preventing destruction at scale.

17:38.000 --> 17:42.000
And you're saying, yeah, but there's a lot of humans.

17:42.000 --> 17:44.000
There's going to be eight plus billion,

17:44.000 --> 17:47.000
and then there's a lot of seconds in the day to come up with stuff.

17:47.000 --> 17:51.000
There's a lot of pain in the world that can lead to a distorted view

17:51.000 --> 17:55.000
of the world such that you want to channel that pain into the destruction,

17:55.000 --> 17:56.000
all those kinds of things.

17:56.000 --> 18:00.000
And it's only a matter of time that anyone individual can do large damage,

18:00.000 --> 18:06.000
especially as we create more and more democratized, decentralized ways

18:06.000 --> 18:10.000
to deliver that damage even if you don't know how to build the initial weapon.

18:10.000 --> 18:21.000
But the thing is, it seems like it's a race between the cheapening of destructive weapons

18:21.000 --> 18:27.000
and the capacity of humans to express their love towards each other.

18:27.000 --> 18:34.000
And it's a race that so far, I know on Twitter, it's not popular to say,

18:34.000 --> 18:36.000
but love is winning, okay?

18:36.000 --> 18:39.000
So what is the argument that love is going to lose here?

18:39.000 --> 18:46.000
Against nuclear weapons, a biotech, and AI, and drones?

18:46.000 --> 18:51.000
Okay, I'm going to comment the end of this to a how love wins.

18:51.000 --> 18:53.000
So I just want you to know that that's where I'm oriented.

18:53.000 --> 18:55.000
That's the end, okay.

18:55.000 --> 19:03.000
But I'm going to argue against why that is a given, because it's not a given.

19:03.000 --> 19:04.000
I don't believe.

19:04.000 --> 19:08.000
This is like a good romantic comedy, so you're going to create drama right now.

19:08.000 --> 19:11.000
But it will end in a happy ending.

19:11.000 --> 19:15.000
Well, it's because it's only a happy ending if we actually understand the issues well enough

19:15.000 --> 19:17.000
and take responsibility to shift it.

19:17.000 --> 19:21.000
Do I believe, like, there's a reason why there's so much more dystopic sci-fi

19:21.000 --> 19:26.000
than protopic sci-fi, and the sum of protopic sci-fi usually requires magic,

19:26.000 --> 19:33.000
is because, or at least magical tech, right, dilithium crystals and warp drives and stuff,

19:33.000 --> 19:40.000
because it's very hard to imagine people like the people we have been in the history books

19:40.000 --> 19:48.000
with exponential type technology and power that don't eventually blow themselves up,

19:48.000 --> 19:52.000
that make good enough choices as stewards of their environment and their commons

19:52.000 --> 19:54.000
and each other and etc.

19:54.000 --> 19:57.000
So, like, it's easier to think of scenarios where we blow ourselves up

19:57.000 --> 20:01.000
than it is to think of scenarios where we avoid every single scenario where we blow ourselves up.

20:01.000 --> 20:06.000
And when I say blow ourselves up, I mean the environmental versions, the terrorist versions,

20:06.000 --> 20:10.000
the war versions, the cumulative externalities versions.

20:10.000 --> 20:17.000
Can I, and I'm sorry if I'm interrupting your flow of thought, but why is it easier?

20:17.000 --> 20:25.000
Could it be a weird psychological thing where we either are just more capable to visualize explosions and destruction

20:25.000 --> 20:30.000
and then the sicker thought, which is like we kind of enjoy for some weird reason

20:30.000 --> 20:34.000
thinking about that kind of stuff, even though we wouldn't actually act on it.

20:34.000 --> 20:40.000
It's almost like some weird, like I love playing shooter games, you know, first person shooters.

20:40.000 --> 20:46.000
And like, especially if it's like murdering, doing a doom, you're shooting demons.

20:46.000 --> 20:50.000
I play one of my favorite games, Diablo, is like slashing through different monsters

20:50.000 --> 20:53.000
and the screaming pain and the hellfire.

20:53.000 --> 20:58.000
And then I go out into the real world to eat my coconut ice cream and I'm all about love.

20:58.000 --> 21:07.000
So can we trust our ability to visualize how it all goes to shit as an actual rational way of thinking?

21:07.000 --> 21:16.000
I think it's a fair question to say to what degree is there just kind of perverse fantasy and morbid exploration

21:16.000 --> 21:21.000
and whatever else that happens in our imagination.

21:21.000 --> 21:23.000
But I don't think that's the whole of it.

21:23.000 --> 21:28.000
I think there is also a reality to the combinatorial possibility space

21:28.000 --> 21:37.000
and the difference in the probabilities that there's a lot of ways I could try to put the 70 trillion cells of your body together that don't make you.

21:37.000 --> 21:40.000
There's not that many ways I can put them together that make you.

21:40.000 --> 21:47.000
There's a lot of ways I could try to connect the organs together that make some weird kind of group of organs on a desk

21:47.000 --> 21:50.000
but that doesn't actually make a functioning human.

21:50.000 --> 21:54.000
And you can kill an adult human in a second but you can't get one in a second.

21:54.000 --> 21:57.000
It takes 20 years to grow one and a lot of things to happen, right?

21:57.000 --> 22:05.000
I could destroy this building in a couple of minutes with demolition but it took a year or a couple of years to build it.

22:05.000 --> 22:07.000
There is...

22:07.000 --> 22:12.000
Calm down, Cole. This is just an example. It doesn't mean it.

22:12.000 --> 22:16.000
There's a gradient where entropy is easier

22:16.000 --> 22:21.000
and there's a lot more ways to put a set of things together that don't work

22:21.000 --> 22:24.000
than the few that really do produce higher order synergies.

22:24.000 --> 22:31.000
And so when we look at a history of war

22:31.000 --> 22:36.000
and then we look at exponentially more powerful warfare,

22:36.000 --> 22:38.000
an arms race that drives out in all these directions,

22:38.000 --> 22:40.000
and we look at a history of environmental destruction

22:40.000 --> 22:44.000
and exponentially more powerful tech that makes exponential externalities

22:44.000 --> 22:48.000
multiplied by the total number of agents that are doing it in the cumulative effects,

22:48.000 --> 22:52.000
there's a lot of ways the whole thing can break, like a lot of different ways.

22:52.000 --> 22:56.000
And for it to get ahead, it has to have none of those happen.

22:56.000 --> 23:01.000
And so there's just a probability space where it's easier to imagine that thing.

23:01.000 --> 23:05.000
So to say how do we have a pro-topic future, we have to say,

23:05.000 --> 23:10.000
well, one criteria must be that it avoids all of the catastrophic risks.

23:10.000 --> 23:13.000
So can we inventory all the catastrophic risks?

23:13.000 --> 23:16.000
Can we inventory the patterns of human behavior that give rise to them?

23:16.000 --> 23:19.000
And could we try to solve for that?

23:19.000 --> 23:23.000
And could we have that be the essence of the social technology

23:23.000 --> 23:27.000
that we're thinking about to be able to guide, bind, and direct a new physical technology?

23:27.000 --> 23:31.000
Because so far our physical technology, like we were talking about the Genghis Kahn's

23:31.000 --> 23:36.000
and like that, that obviously use certain kinds of physical technology and armaments

23:36.000 --> 23:42.000
and also social technology and unconventional warfare for a particular set of purposes.

23:42.000 --> 23:48.000
But we have things that don't look like warfare, like Rockefeller and Standard Oil.

23:48.000 --> 23:56.000
And it looked like a constructive mindset to be able to bring this new energy resource to the world.

23:56.000 --> 23:58.000
And it did.

23:58.000 --> 24:03.000
And the second order effects of that are climate change

24:03.000 --> 24:08.000
and all of the oil spills that have happened and will happen

24:08.000 --> 24:12.000
and all of the wars in the Middle East over the oil that have been there

24:12.000 --> 24:18.000
and the massive political clusterfuck and human life issues that are associated with it

24:18.000 --> 24:21.000
and on and on, right?

24:21.000 --> 24:28.000
And so it's also not just the orientation to construct a thing

24:28.000 --> 24:30.000
can have a narrow focus on what I'm trying to construct

24:30.000 --> 24:33.000
but be affecting a lot of other things through second and third order effects

24:33.000 --> 24:35.000
I'm not taking responsibility for.

24:35.000 --> 24:41.000
And you often on another tangent mentioned second, third, and fourth order effects.

24:41.000 --> 24:43.000
And third order.

24:43.000 --> 24:44.000
Cascading.

24:44.000 --> 24:46.000
Which is really fascinating.

24:46.000 --> 24:53.000
Like starting with the third order plus, it gets really interesting.

24:53.000 --> 24:56.000
Because we don't even acknowledge like the second order effects.

24:56.000 --> 24:57.000
Right.

24:57.000 --> 25:01.000
But like thinking because those it could get bigger and bigger and bigger

25:01.000 --> 25:03.000
in ways we're not anticipating.

25:03.000 --> 25:04.000
So how do we make those?

25:04.000 --> 25:10.000
So it sounds like part of the part of the thing that you are thinking through

25:10.000 --> 25:16.000
in terms of a solution, how to create an anti fragile, a resilient society

25:16.000 --> 25:20.000
is to make explicit.

25:20.000 --> 25:24.000
Acknowledge, understand the externalities.

25:24.000 --> 25:29.000
The second order, third order, fourth order and the order effects.

25:29.000 --> 25:32.000
How do we start to think about those effects?

25:32.000 --> 25:37.000
Yeah, the war application is harm we're trying to cause or that we're aware we're causing, right?

25:37.000 --> 25:42.000
The externality is harm that at least supposedly we're not aware we're causing

25:42.000 --> 25:44.000
or at minimum it's not our intention, right?

25:44.000 --> 25:47.000
Maybe we're either totally unaware of it or we're aware of it

25:47.000 --> 25:49.000
but it is a side effect of what our intention is.

25:49.000 --> 25:51.000
It's not the intention itself.

25:51.000 --> 25:53.000
There are catastrophic risks from both types.

25:53.000 --> 26:00.000
The direct application of increased technological power to a rival risk intent

26:00.000 --> 26:04.000
which is going to cause harm for some out group for some in group to win

26:04.000 --> 26:06.000
but the out group is also working on growing the tech

26:06.000 --> 26:10.000
and if they don't lose completely they reverse engineer the tech upregulated

26:10.000 --> 26:12.000
come back with more capacity.

26:12.000 --> 26:18.000
So there's the exponential tech arms race side of in group out group rivalry

26:18.000 --> 26:20.000
using exponential tech that is one set of risks.

26:20.000 --> 26:27.000
And the other set of risks is the application of exponentially more powerful tech

26:27.000 --> 26:31.000
not intentionally to try and beat an out group

26:31.000 --> 26:33.000
but to try to achieve some goal that we have

26:33.000 --> 26:39.000
but to produce a second and third order effects that do have harm to the commons

26:39.000 --> 26:42.000
to other people to environment to other groups

26:42.000 --> 26:46.000
that might actually be bigger problems

26:46.000 --> 26:49.000
than the problem we were originally trying to solve with the thing we were building.

26:49.000 --> 26:54.000
When Facebook was building a dating app

26:54.000 --> 26:57.000
and then building a social app where people could tag pictures

26:57.000 --> 27:02.000
they weren't trying to build a democracy destroying app

27:02.000 --> 27:09.000
that would maximize time on site as part of its ad model

27:09.000 --> 27:12.000
through AI optimization of a news feed

27:12.000 --> 27:14.000
to the thing that made people spend most time on site

27:14.000 --> 27:18.000
which is usually them being limbically hijacked more than something else

27:18.000 --> 27:21.000
and it's a pretty powerful second order effect

27:21.000 --> 27:24.000
and a pretty fast one

27:24.000 --> 27:28.000
because the rate of tech is obviously able to get distributed to much larger scale

27:28.000 --> 27:33.000
much faster and with a bigger jump in terms of total vertical capacity

27:33.000 --> 27:37.000
then that's what it means to get to the verticalizing part of an exponential curve.

27:37.000 --> 27:43.000
So that's what it means to get to the verticalizing part of an exponential curve

27:43.000 --> 27:48.000
so just like we can see that oil had these second order environmental effects

27:48.000 --> 27:50.000
and also social and political effects

27:50.000 --> 27:53.000
war and so much of the whole

27:53.000 --> 27:59.000
like the total amount of oil used has a proportionality to total global GDP

27:59.000 --> 28:02.000
and this is why we have this, you know, the petrodollar

28:02.000 --> 28:09.000
and so the oil thing also had the exponential effect

28:09.000 --> 28:14.000
and so the oil thing also had the externalities of a major aspect

28:14.000 --> 28:18.000
of what happened with military industrial complex and things like that

28:18.000 --> 28:22.000
but we can see the same thing with more current technologies

28:22.000 --> 28:25.000
with Facebook and Google and other things

28:25.000 --> 28:29.000
so I don't think we can run

28:29.000 --> 28:31.000
and the more powerful the tech is

28:31.000 --> 28:35.000
we build it for reason X, whatever reason X is

28:35.000 --> 28:38.000
maybe X is three things, maybe it's one thing, right?

28:38.000 --> 28:42.000
We're doing the oil thing because we want to make cars

28:42.000 --> 28:44.000
because it's a better method of individual transportation

28:44.000 --> 28:48.000
we're building the Facebook thing because we're going to connect people socially in the personal sphere

28:48.000 --> 28:53.000
but it interacts with complex systems

28:53.000 --> 28:57.000
with ecologies, economies, psychologies, cultures

28:57.000 --> 29:00.000
and so it has effects on other than the thing we're intending

29:00.000 --> 29:04.000
some of those effects can end up being negative effects

29:04.000 --> 29:08.000
but because this technology, if we make it to solve a problem

29:08.000 --> 29:10.000
it has to overcome the problem

29:10.000 --> 29:13.000
the problem's been around for a while, it's going to overcome in a short period of time

29:13.000 --> 29:17.000
so it usually has greater scale, greater rate of magnitude in some way

29:17.000 --> 29:21.000
that also means that the externalities that it creates might be bigger problems

29:21.000 --> 29:24.000
and you can say, well, but then that's the new problem

29:24.000 --> 29:26.000
and humanity will innovate its way out of that

29:26.000 --> 29:31.000
well, I don't think that's paying attention to the fact that we can't keep up with exponential curves

29:31.000 --> 29:36.000
like that, nor do finite spaces allow exponential externalities forever

29:36.000 --> 29:41.000
and this is why a lot of the smartest people thinking about this are thinking

29:41.000 --> 29:44.000
well, no, I think we're totally screwed

29:44.000 --> 29:48.000
unless we can make a benevolent AI singleton that rules all of us

29:48.000 --> 29:52.000
you know, guys like Bostrom and others thinking in those directions

29:52.000 --> 29:56.000
because they're like, how do humans try to do

29:56.000 --> 29:59.000
multi-polarity and make it work

29:59.000 --> 30:03.000
and I have a different answer of what I think it looks like

30:03.000 --> 30:08.000
that does have more to do with the love but some applied social tech align with love

30:08.000 --> 30:11.000
because I have a bunch of really dumb ideas

30:11.000 --> 30:13.000
I'd prefer to hear less

30:13.000 --> 30:15.000
I'd like to hear some of them first

30:26.000 --> 30:29.000
you

