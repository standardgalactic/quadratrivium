1
00:00:00,000 --> 00:00:06,000
What flavors of catastrophic risk are we talking about here?

2
00:00:06,000 --> 00:00:09,000
What's your favorite flavor in terms of ice cream?

3
00:00:09,000 --> 00:00:12,000
Mine is coconut. Nobody seems to like coconut ice cream.

4
00:00:12,000 --> 00:00:20,000
So ice cream aside, what do you most worry about in terms of catastrophic risk

5
00:00:20,000 --> 00:00:29,000
that will help us kind of make concrete the discussion we're having about how to fix this whole thing?

6
00:00:29,000 --> 00:00:34,000
Yeah, I think it's worth taking a historical perspective briefly to just kind of orient everyone to it.

7
00:00:34,000 --> 00:00:39,000
We don't have to go all the way back to the aliens who've seen all of civilization,

8
00:00:39,000 --> 00:00:50,000
but to just recognize that for all of human history, as far as we're aware, there were existential risks to civilizations,

9
00:00:50,000 --> 00:00:55,000
and they happened, right? Like there were civilizations that were killed in war,

10
00:00:55,000 --> 00:00:58,000
and tribes that were killed in tribal warfare or whatever.

11
00:00:58,000 --> 00:01:02,000
So people faced existential risk to the group that they identified with.

12
00:01:02,000 --> 00:01:06,000
It's just those were local phenomena, right? It wasn't a fully global phenomena.

13
00:01:06,000 --> 00:01:10,000
So an empire could fall, and surrounding empires didn't fall.

14
00:01:10,000 --> 00:01:14,000
Maybe they came in and filled the space.

15
00:01:14,000 --> 00:01:19,000
The first time that we were able to think about catastrophic risk,

16
00:01:19,000 --> 00:01:22,000
not from like a solar flare or something that we couldn't control,

17
00:01:22,000 --> 00:01:27,000
but from something that humans would actually create at a global level was World War II in the bomb.

18
00:01:27,000 --> 00:01:33,000
Because it was the first time that we had tech big enough that could actually mess up everything at a global level,

19
00:01:33,000 --> 00:01:37,000
that could mess up habitability. We just weren't powerful enough to do that before.

20
00:01:37,000 --> 00:01:40,000
It's not that we didn't behave in ways that would have done it.

21
00:01:40,000 --> 00:01:43,000
We just only behaved in those ways at the scale we could affect.

22
00:01:43,000 --> 00:01:48,000
And so it's important to get that there's the entire world before World War II,

23
00:01:48,000 --> 00:01:53,000
where we don't have the ability to make a non-habitable biosphere, non-habitable for us.

24
00:01:53,000 --> 00:01:57,000
And then there's World War II and the beginning of a completely new phase

25
00:01:57,000 --> 00:02:02,000
where global human-induced catastrophic risk is now a real thing.

26
00:02:02,000 --> 00:02:07,000
And that was such a big deal that it changed the entire world in a really fundamental way,

27
00:02:07,000 --> 00:02:14,000
which is, you know, when you study history, it's amazing how big a percentage of history is studying war, right?

28
00:02:14,000 --> 00:02:17,000
And the history of wars, you study European history and whatever.

29
00:02:17,000 --> 00:02:20,000
It's generals and wars and empire expansions.

30
00:02:20,000 --> 00:02:25,000
And so the major empires near each other never had really long periods of time

31
00:02:25,000 --> 00:02:29,000
where they weren't engaged in war or preparation for war or something like that.

32
00:02:29,000 --> 00:02:34,000
Humans don't have a good precedent in the post-tribal phase,

33
00:02:34,000 --> 00:02:39,000
the civilization phase of being able to solve conflicts without war for very long.

34
00:02:39,000 --> 00:02:45,000
World War II was the first time where we could have a war that no one could win.

35
00:02:45,000 --> 00:02:49,000
And so the superpowers couldn't fight again. They couldn't do a real kinetic war.

36
00:02:49,000 --> 00:02:52,000
They could do diplomatic wars and Cold War type stuff

37
00:02:52,000 --> 00:02:56,000
and they could fight proxy wars through other countries that didn't have the big weapons.

38
00:02:56,000 --> 00:03:00,000
And so mutually assured destruction and like coming out of World War II,

39
00:03:00,000 --> 00:03:04,000
we actually realized that nation states couldn't prevent world war.

40
00:03:04,000 --> 00:03:08,000
And so we needed a new type of supervening government in addition to nation states,

41
00:03:08,000 --> 00:03:13,000
which was the whole Bretton Woods world, the United Nations, the World Bank, the IMF,

42
00:03:13,000 --> 00:03:18,000
the globalization trade type agreements, mutually assured destruction,

43
00:03:18,000 --> 00:03:23,000
that was how do we have some coordination beyond just nation states between them

44
00:03:23,000 --> 00:03:26,000
since we have to stop war between at least the superpowers.

45
00:03:26,000 --> 00:03:35,000
And it was pretty successful, given that we've had like 75 years of no superpower on superpower war.

46
00:03:35,000 --> 00:03:40,000
We've had lots of proxy wars during that time. We've had, you know, Cold War.

47
00:03:40,000 --> 00:03:48,000
And I would say we're in a new phase now where the Bretton Woods solution is basically over, almost over.

48
00:03:48,000 --> 00:03:50,000
Can you describe the Bretton Woods solution?

49
00:03:50,000 --> 00:04:01,000
Yeah, so the Bretton Woods, the series of agreements for how the nations would be able to engage with each other

50
00:04:02,000 --> 00:04:09,000
was these IGOs, these intergovernmental organizations, and was the idea of globalization.

51
00:04:09,000 --> 00:04:12,000
Since we could have global effects, we needed to be able to think about things globally,

52
00:04:12,000 --> 00:04:18,000
where we had trade relationships with each other, where it would not be profitable to war with each other.

53
00:04:18,000 --> 00:04:20,000
It would be more profitable to actually be able to trade with each other.

54
00:04:20,000 --> 00:04:25,000
So our own self-interest was, you know, going to drive our non-war interest.

55
00:04:26,000 --> 00:04:32,000
And so this started to look like, and obviously this couldn't have happened that much earlier either,

56
00:04:32,000 --> 00:04:37,000
because industrialization hadn't gotten far enough to be able to do massive global industrial supply chains

57
00:04:37,000 --> 00:04:39,000
and ship stuff around, you know, quickly.

58
00:04:39,000 --> 00:04:45,000
But like we were mentioning earlier, almost all the electronics that we use today, just basic, cheap stuff for us,

59
00:04:45,000 --> 00:04:47,000
is made on six continents, made in many countries.

60
00:04:47,000 --> 00:04:51,000
There's no single country in the world that could actually make many of the things that we have,

61
00:04:51,000 --> 00:04:57,000
from the raw material extraction to the plastics and polymers and the, you know, et cetera.

62
00:04:57,000 --> 00:05:03,000
And so the idea that we made a world that could do that kind of trade and create massive GDP growth,

63
00:05:03,000 --> 00:05:08,000
we could all work together to be able to mine natural resources and grow stuff.

64
00:05:08,000 --> 00:05:12,000
With the rapid GDP growth, there was the idea that everybody could keep having more,

65
00:05:12,000 --> 00:05:15,000
without having to take each other's stuff.

66
00:05:15,000 --> 00:05:20,000
And so that was part of kind of the Bretton Woods post-World War II model.

67
00:05:20,000 --> 00:05:25,000
The other was that we would be so economically interdependent that blowing each other up would never make sense.

68
00:05:25,000 --> 00:05:28,000
That worked for a while.

69
00:05:28,000 --> 00:05:34,000
Now, it also brought us up into planetary boundaries faster,

70
00:05:34,000 --> 00:05:41,000
the unrenovable use of resource and turning those resources into pollution on the other side of the supply chain.

71
00:05:41,000 --> 00:05:47,000
So obviously that faster GDP growth meant the overfishing of the oceans

72
00:05:47,000 --> 00:05:53,000
and the cutting down of the trees and the climate change and the mining, toxic mining tailings going into the water

73
00:05:53,000 --> 00:05:56,000
and the mountaintop removal mining and all those types of things.

74
00:05:56,000 --> 00:06:00,000
That's the consumption side of the rest that we're talking about.

75
00:06:00,000 --> 00:06:07,000
And so the answer of let's do positive GDP is the answer rapidly and exponentially

76
00:06:07,000 --> 00:06:11,000
obviously accelerated the planetary boundary side.

77
00:06:11,000 --> 00:06:20,000
And that was thought about for a long time, but it started to be modeled with the Club of Rome and limits of growth.

78
00:06:20,000 --> 00:06:27,000
But it's just very obvious to say, if you have a linear materials economy where you take stuff out of the earth faster,

79
00:06:27,000 --> 00:06:34,000
whether it's fish or trees or oil, you take it out of the earth faster than it can replenish itself.

80
00:06:34,000 --> 00:06:37,000
And you turn it into trash after using it for a short period of time.

81
00:06:37,000 --> 00:06:41,000
You put the trash in the environment faster than it can process itself.

82
00:06:41,000 --> 00:06:44,000
And there's toxicity associated with both sides of this.

83
00:06:44,000 --> 00:06:49,000
You can't run an exponentially growing linear materials economy on a finite planet forever.

84
00:06:49,000 --> 00:06:51,000
That's not a hard thing to figure out.

85
00:06:51,000 --> 00:06:58,000
And it has to be exponential if there's an exponentiation in the monetary supply because of interest and then fractional reserve banking

86
00:06:58,000 --> 00:07:02,000
and to then be able to keep up with the growing monetary supply.

87
00:07:02,000 --> 00:07:03,000
You have to have growth of goods and services.

88
00:07:04,000 --> 00:07:09,000
So that's that kind of thing that has happened.

89
00:07:09,000 --> 00:07:15,000
But you also see that when you get these supply chains that are so interconnected across the world, you get increased fragility

90
00:07:15,000 --> 00:07:22,000
because a collapse or a problem in one area then affects the whole world in a much bigger area as opposed to the issues being local.

91
00:07:22,000 --> 00:07:30,000
So we got to see with COVID and an issue that started in one part of China affecting the whole world so much more rapidly

92
00:07:30,000 --> 00:07:33,000
than would have happened before Bretton Woods, right?

93
00:07:33,000 --> 00:07:37,000
Before international travel supply chains, you know, that whole kind of thing.

94
00:07:37,000 --> 00:07:40,000
And with a bunch of second and third order effects that people wouldn't have predicted, okay?

95
00:07:40,000 --> 00:07:44,000
We have to stop certain kinds of travel because of viral contaminants.

96
00:07:44,000 --> 00:07:50,000
But the countries doing agriculture depend upon fertilizer they don't produce that is shipped into them

97
00:07:50,000 --> 00:07:52,000
and depend upon pesticides they don't produce.

98
00:07:52,000 --> 00:07:58,000
So we got both crop failures and crops being eaten by locusts in scale in Northern Africa and Iran and things like that

99
00:07:58,000 --> 00:08:00,000
because they couldn't get the supplies of stuff in.

100
00:08:00,000 --> 00:08:06,000
So then you get massive starvation or future kind of hunger issues because of supply chain shutdowns.

101
00:08:06,000 --> 00:08:13,000
So you get this increased fragility and cascade dynamics where a small problem can end up leading to cascade effects.

102
00:08:13,000 --> 00:08:25,000
And also we went from two superpowers with one catastrophe weapon to now that same catastrophe weapon

103
00:08:25,000 --> 00:08:31,000
is there's more countries that have it, eight or nine countries that have it,

104
00:08:31,000 --> 00:08:34,000
and there's a lot more types of catastrophe weapons.

105
00:08:34,000 --> 00:08:40,000
We now have catastrophe weapons with weaponized drones that can hit infrastructure targets with bio

106
00:08:40,000 --> 00:08:44,000
with, in fact, every new type of tech has created an arms race.

107
00:08:44,000 --> 00:08:49,000
So we have not, with the UN or the other kind of intergovernmental organizations,

108
00:08:49,000 --> 00:08:52,000
we haven't been able to really do nuclear deproliferation.

109
00:08:52,000 --> 00:08:59,000
We've actually had more countries get nukes and keep getting faster nukes, the race to hypersonics and things like that.

110
00:08:59,000 --> 00:09:04,000
And every new type of technology that has emerged has created an arms race.

111
00:09:04,000 --> 00:09:11,000
And so you can't do mutually assured destruction with multiple agents the way you can with two agents.

112
00:09:11,000 --> 00:09:16,000
Two agents, it's much easier to create a stable Nash equilibrium that's forced.

113
00:09:16,000 --> 00:09:21,000
But the ability to modern say, if these guys shoot, who do I shoot? Do I shoot them? Do I shoot everybody? Do I?

114
00:09:21,000 --> 00:09:25,000
And so you get a three-body problem. You get a very complex type of thing when you have multiple agents

115
00:09:25,000 --> 00:09:30,000
and multiple different types of catastrophe weapons, including ones that can be much more easily produced than nukes.

116
00:09:30,000 --> 00:09:33,000
Nukes are really hard to produce. There's only uranium in a few areas.

117
00:09:33,000 --> 00:09:36,000
Uranium enrichment is hard. ICBMs are hard.

118
00:09:36,000 --> 00:09:40,000
But weaponized drones hitting smart targets is not so hard.

119
00:09:40,000 --> 00:09:47,000
There's a lot of other things where basically the scale at being able to manufacture them is going way, way down to where even non-state actors can have them.

120
00:09:47,000 --> 00:09:54,000
And so when we talk about exponential tech and the decentralization of exponential tech,

121
00:09:54,000 --> 00:09:58,000
what that means is decentralized catastrophe weapon capacity.

122
00:09:58,000 --> 00:10:06,000
And especially in a world of increasing numbers of people feeling disenfranchised, frantic, whatever, for different reasons.

123
00:10:06,000 --> 00:10:14,000
So I would say the Bretton Woods world doesn't prepare us to be able to deal with lots of different agents,

124
00:10:14,000 --> 00:10:18,000
having lots of different types of catastrophe weapons you can't put mutually assured destruction on,

125
00:10:18,000 --> 00:10:25,000
where you can't keep doing growth of the materials economy in the same way because of hitting planetary boundaries

126
00:10:25,000 --> 00:10:31,000
and where the fragility dynamics are actually now their own source of catastrophic risk.

127
00:10:31,000 --> 00:10:34,000
So now we're, so like there was all the world until World War II,

128
00:10:34,000 --> 00:10:39,000
and World War II is just from a civilization timescale point of view as just a second ago.

129
00:10:39,000 --> 00:10:41,000
It seems like a long time, but it is really not.

130
00:10:41,000 --> 00:10:44,000
We get a short period of relative peace at the level of superpowers

131
00:10:44,000 --> 00:10:48,000
while building up the military capacity for much, much, much worse war the entire time.

132
00:10:48,000 --> 00:10:55,000
And then now we're at this new phase where the things that allowed us to make it through the nuclear power

133
00:10:55,000 --> 00:10:59,000
are not the same systems that will let us make it through the next stage.

134
00:10:59,000 --> 00:11:02,000
So what is this next post Bretton Woods?

135
00:11:02,000 --> 00:11:10,000
How do we become safe vessels, safe stewards of many different types of exponential technology

136
00:11:10,000 --> 00:11:14,000
is a key question when we're thinking about X-Risk.

137
00:11:14,000 --> 00:11:22,000
Okay, so, and I'd like to try to answer the how a few ways,

138
00:11:22,000 --> 00:11:25,000
but first on the mutually assured destruction,

139
00:11:26,000 --> 00:11:31,000
do you give credit to the idea of two superpowers

140
00:11:31,000 --> 00:11:38,000
not blowing each other up with nuclear weapons to the simple game theoretic model of mutually assured destruction

141
00:11:38,000 --> 00:11:43,000
or something you've said previously, this idea of inverse correlation,

142
00:11:43,000 --> 00:11:50,000
which I tend to believe between the, now you were talking about tech,

143
00:11:50,000 --> 00:11:55,000
but I think it's maybe broadly true the inverse correlation

144
00:11:55,000 --> 00:11:58,000
between competence and propensity for destruction.

145
00:11:58,000 --> 00:12:07,000
So the better the bigger your weapons, not because you're afraid of mutually assured self-destruction,

146
00:12:07,000 --> 00:12:13,000
but because we're human beings and there's a deep moral fortitude there

147
00:12:13,000 --> 00:12:16,000
that somehow aligned with competence and being good at your job.

148
00:12:17,000 --> 00:12:26,000
It's very hard to be a psychopath and be good at killing at scale.

149
00:12:28,000 --> 00:12:30,000
Do you share any of that intuition?

150
00:12:30,000 --> 00:12:31,000
Kind of.

151
00:12:33,000 --> 00:12:39,000
I think most people would say that Alexander the Great and Genghis Khan and Napoleon were effective people

152
00:12:39,000 --> 00:12:45,000
that were good at their job, that were actually maybe asymmetrically good.

153
00:12:46,000 --> 00:12:50,000
At being able to organize people and do certain kinds of things

154
00:12:50,000 --> 00:12:54,000
that were pretty oriented towards certain types of destruction

155
00:12:54,000 --> 00:13:00,000
or pretty willing to, maybe they would say they were oriented towards empire expansion,

156
00:13:00,000 --> 00:13:04,000
but pretty willing to commit certain acts of destruction in the name of it.

157
00:13:04,000 --> 00:13:11,000
What are you worried about, the Genghis Khan or you could argue he's not a psychopath,

158
00:13:12,000 --> 00:13:16,000
are you worried about Genghis Khan, are you worried about Hitler,

159
00:13:16,000 --> 00:13:22,000
are you worried about a terrorist who has a very different ethic,

160
00:13:22,000 --> 00:13:31,000
which is not even for, it's not trying to preserve and build and expand my community.

161
00:13:31,000 --> 00:13:35,000
It's more about just destruction in itself is the goal.

162
00:13:35,000 --> 00:13:39,000
I think the thing that you're looking at that I do agree with

163
00:13:39,000 --> 00:13:44,000
is that there's a psychological disposition towards construction

164
00:13:44,000 --> 00:13:47,000
and a psychological disposition more towards destruction.

165
00:13:47,000 --> 00:13:50,000
Obviously everybody has both and can toggle between both

166
00:13:50,000 --> 00:13:53,000
and oftentimes one is willing to destroy certain things.

167
00:13:53,000 --> 00:13:55,000
We have this idea of creative destruction, right?

168
00:13:55,000 --> 00:13:58,000
Willing to destroy certain things to create other things

169
00:13:58,000 --> 00:14:02,000
and utilitarianism and trolley problems are all about exploring that space

170
00:14:02,000 --> 00:14:05,000
and the idea of war is all about that.

171
00:14:05,000 --> 00:14:09,000
I am trying to create something for our people and it requires destroying some other people.

172
00:14:13,000 --> 00:14:17,000
Sociopathy is a funny topic because it's possible to have very high fealty to your in-group

173
00:14:17,000 --> 00:14:22,000
and work on perfecting the methods of torture to the out-group at the same time

174
00:14:22,000 --> 00:14:25,000
because you can dehumanize and then remove empathy.

175
00:14:29,000 --> 00:14:32,000
I would also say that there are types.

176
00:14:33,000 --> 00:14:38,000
The thing that gives hope about the orientation towards construction

177
00:14:38,000 --> 00:14:41,000
and destruction being a little different in psychology

178
00:14:41,000 --> 00:14:45,000
is what it takes to build really catastrophic tech

179
00:14:45,000 --> 00:14:50,000
even today where it doesn't take what it took to make a small group of people could do it

180
00:14:50,000 --> 00:14:55,000
takes still some real technical knowledge that required having studied for a while

181
00:14:55,000 --> 00:14:58,000
and some then building capacity.

182
00:14:58,000 --> 00:15:02,000
And there's a question of is that psychologically inversely correlated

183
00:15:02,000 --> 00:15:06,000
with the desire to damage civilization, meaningfully?

184
00:15:08,000 --> 00:15:11,000
A little bit, a little bit, I think.

185
00:15:12,000 --> 00:15:13,000
I think a lot.

186
00:15:13,000 --> 00:15:17,000
I think it's actually, I mean, this is the conversation I had with,

187
00:15:17,000 --> 00:15:21,000
I think offline with Dan Carlin, which is like,

188
00:15:21,000 --> 00:15:25,000
it's pretty easy to come up with ways that any competent,

189
00:15:25,000 --> 00:15:29,000
I can come up with a lot of ways to hurt a lot of people.

190
00:15:29,000 --> 00:15:32,000
And it's pretty easy, like I alone could do it.

191
00:15:35,000 --> 00:15:39,000
There's a lot of people as smarter, smarter than me,

192
00:15:39,000 --> 00:15:42,000
at least in their creation of explosives.

193
00:15:42,000 --> 00:15:47,000
Why are we not seeing more insane mass murder?

194
00:15:47,000 --> 00:15:53,000
I think there's something fascinating and beautiful about this.

195
00:15:53,000 --> 00:15:59,000
And it does have to do with some deeply pro-social types of characteristics in humans.

196
00:16:02,000 --> 00:16:05,000
But when you're dealing with very large numbers,

197
00:16:05,000 --> 00:16:08,000
you don't need a whole lot of a phenomena.

198
00:16:08,000 --> 00:16:09,000
And so then you start to say,

199
00:16:09,000 --> 00:16:12,000
well, what's the probability that X won't happen this year,

200
00:16:12,000 --> 00:16:14,000
then won't happen in the next two years, three years, four years.

201
00:16:14,000 --> 00:16:18,000
And then how many people are doing destructive things with lower tech

202
00:16:18,000 --> 00:16:21,000
and then how many of them can get access to higher tech

203
00:16:21,000 --> 00:16:24,000
because they didn't have to figure out how to build.

204
00:16:24,000 --> 00:16:28,000
So when I can get commercial tech,

205
00:16:28,000 --> 00:16:31,000
and maybe I don't understand tech very well,

206
00:16:31,000 --> 00:16:34,000
but I understand it well enough to utilize it, not to create it,

207
00:16:34,000 --> 00:16:36,000
and I can repurpose it,

208
00:16:36,000 --> 00:16:41,000
when we saw that commercial drone with a homemade thermite bomb

209
00:16:41,000 --> 00:16:43,000
hit the Ukrainian munitions factory

210
00:16:43,000 --> 00:16:47,000
and do the equivalent of an incendiary bomb level of damage,

211
00:16:47,000 --> 00:16:49,000
that was just home tech.

212
00:16:49,000 --> 00:16:51,000
It was a simple kind of thing.

213
00:16:51,000 --> 00:16:55,000
And so the question is not

214
00:16:55,000 --> 00:16:59,000
does it stay being a small percentage of the population?

215
00:16:59,000 --> 00:17:04,000
The question is, can you bind that phenomena nearly completely?

216
00:17:06,000 --> 00:17:10,000
And especially now as you start to get into bigger things,

217
00:17:10,000 --> 00:17:15,000
CRISPR gene drive technologies and various things like that,

218
00:17:15,000 --> 00:17:18,000
can you bind it completely long term?

219
00:17:19,000 --> 00:17:20,000
Over what period of time?

220
00:17:20,000 --> 00:17:21,000
Not perfectly though.

221
00:17:21,000 --> 00:17:22,000
That's the thing.

222
00:17:22,000 --> 00:17:27,000
I'm trying to say that there is some, let's call it,

223
00:17:27,000 --> 00:17:32,000
a random word, love that's inherent in,

224
00:17:32,000 --> 00:17:35,000
that's core to human nature,

225
00:17:35,000 --> 00:17:38,000
that's preventing destruction at scale.

226
00:17:38,000 --> 00:17:42,000
And you're saying, yeah, but there's a lot of humans.

227
00:17:42,000 --> 00:17:44,000
There's going to be eight plus billion,

228
00:17:44,000 --> 00:17:47,000
and then there's a lot of seconds in the day to come up with stuff.

229
00:17:47,000 --> 00:17:51,000
There's a lot of pain in the world that can lead to a distorted view

230
00:17:51,000 --> 00:17:55,000
of the world such that you want to channel that pain into the destruction,

231
00:17:55,000 --> 00:17:56,000
all those kinds of things.

232
00:17:56,000 --> 00:18:00,000
And it's only a matter of time that anyone individual can do large damage,

233
00:18:00,000 --> 00:18:06,000
especially as we create more and more democratized, decentralized ways

234
00:18:06,000 --> 00:18:10,000
to deliver that damage even if you don't know how to build the initial weapon.

235
00:18:10,000 --> 00:18:21,000
But the thing is, it seems like it's a race between the cheapening of destructive weapons

236
00:18:21,000 --> 00:18:27,000
and the capacity of humans to express their love towards each other.

237
00:18:27,000 --> 00:18:34,000
And it's a race that so far, I know on Twitter, it's not popular to say,

238
00:18:34,000 --> 00:18:36,000
but love is winning, okay?

239
00:18:36,000 --> 00:18:39,000
So what is the argument that love is going to lose here?

240
00:18:39,000 --> 00:18:46,000
Against nuclear weapons, a biotech, and AI, and drones?

241
00:18:46,000 --> 00:18:51,000
Okay, I'm going to comment the end of this to a how love wins.

242
00:18:51,000 --> 00:18:53,000
So I just want you to know that that's where I'm oriented.

243
00:18:53,000 --> 00:18:55,000
That's the end, okay.

244
00:18:55,000 --> 00:19:03,000
But I'm going to argue against why that is a given, because it's not a given.

245
00:19:03,000 --> 00:19:04,000
I don't believe.

246
00:19:04,000 --> 00:19:08,000
This is like a good romantic comedy, so you're going to create drama right now.

247
00:19:08,000 --> 00:19:11,000
But it will end in a happy ending.

248
00:19:11,000 --> 00:19:15,000
Well, it's because it's only a happy ending if we actually understand the issues well enough

249
00:19:15,000 --> 00:19:17,000
and take responsibility to shift it.

250
00:19:17,000 --> 00:19:21,000
Do I believe, like, there's a reason why there's so much more dystopic sci-fi

251
00:19:21,000 --> 00:19:26,000
than protopic sci-fi, and the sum of protopic sci-fi usually requires magic,

252
00:19:26,000 --> 00:19:33,000
is because, or at least magical tech, right, dilithium crystals and warp drives and stuff,

253
00:19:33,000 --> 00:19:40,000
because it's very hard to imagine people like the people we have been in the history books

254
00:19:40,000 --> 00:19:48,000
with exponential type technology and power that don't eventually blow themselves up,

255
00:19:48,000 --> 00:19:52,000
that make good enough choices as stewards of their environment and their commons

256
00:19:52,000 --> 00:19:54,000
and each other and etc.

257
00:19:54,000 --> 00:19:57,000
So, like, it's easier to think of scenarios where we blow ourselves up

258
00:19:57,000 --> 00:20:01,000
than it is to think of scenarios where we avoid every single scenario where we blow ourselves up.

259
00:20:01,000 --> 00:20:06,000
And when I say blow ourselves up, I mean the environmental versions, the terrorist versions,

260
00:20:06,000 --> 00:20:10,000
the war versions, the cumulative externalities versions.

261
00:20:10,000 --> 00:20:17,000
Can I, and I'm sorry if I'm interrupting your flow of thought, but why is it easier?

262
00:20:17,000 --> 00:20:25,000
Could it be a weird psychological thing where we either are just more capable to visualize explosions and destruction

263
00:20:25,000 --> 00:20:30,000
and then the sicker thought, which is like we kind of enjoy for some weird reason

264
00:20:30,000 --> 00:20:34,000
thinking about that kind of stuff, even though we wouldn't actually act on it.

265
00:20:34,000 --> 00:20:40,000
It's almost like some weird, like I love playing shooter games, you know, first person shooters.

266
00:20:40,000 --> 00:20:46,000
And like, especially if it's like murdering, doing a doom, you're shooting demons.

267
00:20:46,000 --> 00:20:50,000
I play one of my favorite games, Diablo, is like slashing through different monsters

268
00:20:50,000 --> 00:20:53,000
and the screaming pain and the hellfire.

269
00:20:53,000 --> 00:20:58,000
And then I go out into the real world to eat my coconut ice cream and I'm all about love.

270
00:20:58,000 --> 00:21:07,000
So can we trust our ability to visualize how it all goes to shit as an actual rational way of thinking?

271
00:21:07,000 --> 00:21:16,000
I think it's a fair question to say to what degree is there just kind of perverse fantasy and morbid exploration

272
00:21:16,000 --> 00:21:21,000
and whatever else that happens in our imagination.

273
00:21:21,000 --> 00:21:23,000
But I don't think that's the whole of it.

274
00:21:23,000 --> 00:21:28,000
I think there is also a reality to the combinatorial possibility space

275
00:21:28,000 --> 00:21:37,000
and the difference in the probabilities that there's a lot of ways I could try to put the 70 trillion cells of your body together that don't make you.

276
00:21:37,000 --> 00:21:40,000
There's not that many ways I can put them together that make you.

277
00:21:40,000 --> 00:21:47,000
There's a lot of ways I could try to connect the organs together that make some weird kind of group of organs on a desk

278
00:21:47,000 --> 00:21:50,000
but that doesn't actually make a functioning human.

279
00:21:50,000 --> 00:21:54,000
And you can kill an adult human in a second but you can't get one in a second.

280
00:21:54,000 --> 00:21:57,000
It takes 20 years to grow one and a lot of things to happen, right?

281
00:21:57,000 --> 00:22:05,000
I could destroy this building in a couple of minutes with demolition but it took a year or a couple of years to build it.

282
00:22:05,000 --> 00:22:07,000
There is...

283
00:22:07,000 --> 00:22:12,000
Calm down, Cole. This is just an example. It doesn't mean it.

284
00:22:12,000 --> 00:22:16,000
There's a gradient where entropy is easier

285
00:22:16,000 --> 00:22:21,000
and there's a lot more ways to put a set of things together that don't work

286
00:22:21,000 --> 00:22:24,000
than the few that really do produce higher order synergies.

287
00:22:24,000 --> 00:22:31,000
And so when we look at a history of war

288
00:22:31,000 --> 00:22:36,000
and then we look at exponentially more powerful warfare,

289
00:22:36,000 --> 00:22:38,000
an arms race that drives out in all these directions,

290
00:22:38,000 --> 00:22:40,000
and we look at a history of environmental destruction

291
00:22:40,000 --> 00:22:44,000
and exponentially more powerful tech that makes exponential externalities

292
00:22:44,000 --> 00:22:48,000
multiplied by the total number of agents that are doing it in the cumulative effects,

293
00:22:48,000 --> 00:22:52,000
there's a lot of ways the whole thing can break, like a lot of different ways.

294
00:22:52,000 --> 00:22:56,000
And for it to get ahead, it has to have none of those happen.

295
00:22:56,000 --> 00:23:01,000
And so there's just a probability space where it's easier to imagine that thing.

296
00:23:01,000 --> 00:23:05,000
So to say how do we have a pro-topic future, we have to say,

297
00:23:05,000 --> 00:23:10,000
well, one criteria must be that it avoids all of the catastrophic risks.

298
00:23:10,000 --> 00:23:13,000
So can we inventory all the catastrophic risks?

299
00:23:13,000 --> 00:23:16,000
Can we inventory the patterns of human behavior that give rise to them?

300
00:23:16,000 --> 00:23:19,000
And could we try to solve for that?

301
00:23:19,000 --> 00:23:23,000
And could we have that be the essence of the social technology

302
00:23:23,000 --> 00:23:27,000
that we're thinking about to be able to guide, bind, and direct a new physical technology?

303
00:23:27,000 --> 00:23:31,000
Because so far our physical technology, like we were talking about the Genghis Kahn's

304
00:23:31,000 --> 00:23:36,000
and like that, that obviously use certain kinds of physical technology and armaments

305
00:23:36,000 --> 00:23:42,000
and also social technology and unconventional warfare for a particular set of purposes.

306
00:23:42,000 --> 00:23:48,000
But we have things that don't look like warfare, like Rockefeller and Standard Oil.

307
00:23:48,000 --> 00:23:56,000
And it looked like a constructive mindset to be able to bring this new energy resource to the world.

308
00:23:56,000 --> 00:23:58,000
And it did.

309
00:23:58,000 --> 00:24:03,000
And the second order effects of that are climate change

310
00:24:03,000 --> 00:24:08,000
and all of the oil spills that have happened and will happen

311
00:24:08,000 --> 00:24:12,000
and all of the wars in the Middle East over the oil that have been there

312
00:24:12,000 --> 00:24:18,000
and the massive political clusterfuck and human life issues that are associated with it

313
00:24:18,000 --> 00:24:21,000
and on and on, right?

314
00:24:21,000 --> 00:24:28,000
And so it's also not just the orientation to construct a thing

315
00:24:28,000 --> 00:24:30,000
can have a narrow focus on what I'm trying to construct

316
00:24:30,000 --> 00:24:33,000
but be affecting a lot of other things through second and third order effects

317
00:24:33,000 --> 00:24:35,000
I'm not taking responsibility for.

318
00:24:35,000 --> 00:24:41,000
And you often on another tangent mentioned second, third, and fourth order effects.

319
00:24:41,000 --> 00:24:43,000
And third order.

320
00:24:43,000 --> 00:24:44,000
Cascading.

321
00:24:44,000 --> 00:24:46,000
Which is really fascinating.

322
00:24:46,000 --> 00:24:53,000
Like starting with the third order plus, it gets really interesting.

323
00:24:53,000 --> 00:24:56,000
Because we don't even acknowledge like the second order effects.

324
00:24:56,000 --> 00:24:57,000
Right.

325
00:24:57,000 --> 00:25:01,000
But like thinking because those it could get bigger and bigger and bigger

326
00:25:01,000 --> 00:25:03,000
in ways we're not anticipating.

327
00:25:03,000 --> 00:25:04,000
So how do we make those?

328
00:25:04,000 --> 00:25:10,000
So it sounds like part of the part of the thing that you are thinking through

329
00:25:10,000 --> 00:25:16,000
in terms of a solution, how to create an anti fragile, a resilient society

330
00:25:16,000 --> 00:25:20,000
is to make explicit.

331
00:25:20,000 --> 00:25:24,000
Acknowledge, understand the externalities.

332
00:25:24,000 --> 00:25:29,000
The second order, third order, fourth order and the order effects.

333
00:25:29,000 --> 00:25:32,000
How do we start to think about those effects?

334
00:25:32,000 --> 00:25:37,000
Yeah, the war application is harm we're trying to cause or that we're aware we're causing, right?

335
00:25:37,000 --> 00:25:42,000
The externality is harm that at least supposedly we're not aware we're causing

336
00:25:42,000 --> 00:25:44,000
or at minimum it's not our intention, right?

337
00:25:44,000 --> 00:25:47,000
Maybe we're either totally unaware of it or we're aware of it

338
00:25:47,000 --> 00:25:49,000
but it is a side effect of what our intention is.

339
00:25:49,000 --> 00:25:51,000
It's not the intention itself.

340
00:25:51,000 --> 00:25:53,000
There are catastrophic risks from both types.

341
00:25:53,000 --> 00:26:00,000
The direct application of increased technological power to a rival risk intent

342
00:26:00,000 --> 00:26:04,000
which is going to cause harm for some out group for some in group to win

343
00:26:04,000 --> 00:26:06,000
but the out group is also working on growing the tech

344
00:26:06,000 --> 00:26:10,000
and if they don't lose completely they reverse engineer the tech upregulated

345
00:26:10,000 --> 00:26:12,000
come back with more capacity.

346
00:26:12,000 --> 00:26:18,000
So there's the exponential tech arms race side of in group out group rivalry

347
00:26:18,000 --> 00:26:20,000
using exponential tech that is one set of risks.

348
00:26:20,000 --> 00:26:27,000
And the other set of risks is the application of exponentially more powerful tech

349
00:26:27,000 --> 00:26:31,000
not intentionally to try and beat an out group

350
00:26:31,000 --> 00:26:33,000
but to try to achieve some goal that we have

351
00:26:33,000 --> 00:26:39,000
but to produce a second and third order effects that do have harm to the commons

352
00:26:39,000 --> 00:26:42,000
to other people to environment to other groups

353
00:26:42,000 --> 00:26:46,000
that might actually be bigger problems

354
00:26:46,000 --> 00:26:49,000
than the problem we were originally trying to solve with the thing we were building.

355
00:26:49,000 --> 00:26:54,000
When Facebook was building a dating app

356
00:26:54,000 --> 00:26:57,000
and then building a social app where people could tag pictures

357
00:26:57,000 --> 00:27:02,000
they weren't trying to build a democracy destroying app

358
00:27:02,000 --> 00:27:09,000
that would maximize time on site as part of its ad model

359
00:27:09,000 --> 00:27:12,000
through AI optimization of a news feed

360
00:27:12,000 --> 00:27:14,000
to the thing that made people spend most time on site

361
00:27:14,000 --> 00:27:18,000
which is usually them being limbically hijacked more than something else

362
00:27:18,000 --> 00:27:21,000
and it's a pretty powerful second order effect

363
00:27:21,000 --> 00:27:24,000
and a pretty fast one

364
00:27:24,000 --> 00:27:28,000
because the rate of tech is obviously able to get distributed to much larger scale

365
00:27:28,000 --> 00:27:33,000
much faster and with a bigger jump in terms of total vertical capacity

366
00:27:33,000 --> 00:27:37,000
then that's what it means to get to the verticalizing part of an exponential curve.

367
00:27:37,000 --> 00:27:43,000
So that's what it means to get to the verticalizing part of an exponential curve

368
00:27:43,000 --> 00:27:48,000
so just like we can see that oil had these second order environmental effects

369
00:27:48,000 --> 00:27:50,000
and also social and political effects

370
00:27:50,000 --> 00:27:53,000
war and so much of the whole

371
00:27:53,000 --> 00:27:59,000
like the total amount of oil used has a proportionality to total global GDP

372
00:27:59,000 --> 00:28:02,000
and this is why we have this, you know, the petrodollar

373
00:28:02,000 --> 00:28:09,000
and so the oil thing also had the exponential effect

374
00:28:09,000 --> 00:28:14,000
and so the oil thing also had the externalities of a major aspect

375
00:28:14,000 --> 00:28:18,000
of what happened with military industrial complex and things like that

376
00:28:18,000 --> 00:28:22,000
but we can see the same thing with more current technologies

377
00:28:22,000 --> 00:28:25,000
with Facebook and Google and other things

378
00:28:25,000 --> 00:28:29,000
so I don't think we can run

379
00:28:29,000 --> 00:28:31,000
and the more powerful the tech is

380
00:28:31,000 --> 00:28:35,000
we build it for reason X, whatever reason X is

381
00:28:35,000 --> 00:28:38,000
maybe X is three things, maybe it's one thing, right?

382
00:28:38,000 --> 00:28:42,000
We're doing the oil thing because we want to make cars

383
00:28:42,000 --> 00:28:44,000
because it's a better method of individual transportation

384
00:28:44,000 --> 00:28:48,000
we're building the Facebook thing because we're going to connect people socially in the personal sphere

385
00:28:48,000 --> 00:28:53,000
but it interacts with complex systems

386
00:28:53,000 --> 00:28:57,000
with ecologies, economies, psychologies, cultures

387
00:28:57,000 --> 00:29:00,000
and so it has effects on other than the thing we're intending

388
00:29:00,000 --> 00:29:04,000
some of those effects can end up being negative effects

389
00:29:04,000 --> 00:29:08,000
but because this technology, if we make it to solve a problem

390
00:29:08,000 --> 00:29:10,000
it has to overcome the problem

391
00:29:10,000 --> 00:29:13,000
the problem's been around for a while, it's going to overcome in a short period of time

392
00:29:13,000 --> 00:29:17,000
so it usually has greater scale, greater rate of magnitude in some way

393
00:29:17,000 --> 00:29:21,000
that also means that the externalities that it creates might be bigger problems

394
00:29:21,000 --> 00:29:24,000
and you can say, well, but then that's the new problem

395
00:29:24,000 --> 00:29:26,000
and humanity will innovate its way out of that

396
00:29:26,000 --> 00:29:31,000
well, I don't think that's paying attention to the fact that we can't keep up with exponential curves

397
00:29:31,000 --> 00:29:36,000
like that, nor do finite spaces allow exponential externalities forever

398
00:29:36,000 --> 00:29:41,000
and this is why a lot of the smartest people thinking about this are thinking

399
00:29:41,000 --> 00:29:44,000
well, no, I think we're totally screwed

400
00:29:44,000 --> 00:29:48,000
unless we can make a benevolent AI singleton that rules all of us

401
00:29:48,000 --> 00:29:52,000
you know, guys like Bostrom and others thinking in those directions

402
00:29:52,000 --> 00:29:56,000
because they're like, how do humans try to do

403
00:29:56,000 --> 00:29:59,000
multi-polarity and make it work

404
00:29:59,000 --> 00:30:03,000
and I have a different answer of what I think it looks like

405
00:30:03,000 --> 00:30:08,000
that does have more to do with the love but some applied social tech align with love

406
00:30:08,000 --> 00:30:11,000
because I have a bunch of really dumb ideas

407
00:30:11,000 --> 00:30:13,000
I'd prefer to hear less

408
00:30:13,000 --> 00:30:15,000
I'd like to hear some of them first

409
00:30:26,000 --> 00:30:29,000
you

