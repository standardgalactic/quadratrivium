start	end	text
0	6000	What flavors of catastrophic risk are we talking about here?
6000	9000	What's your favorite flavor in terms of ice cream?
9000	12000	Mine is coconut. Nobody seems to like coconut ice cream.
12000	20000	So ice cream aside, what do you most worry about in terms of catastrophic risk
20000	29000	that will help us kind of make concrete the discussion we're having about how to fix this whole thing?
29000	34000	Yeah, I think it's worth taking a historical perspective briefly to just kind of orient everyone to it.
34000	39000	We don't have to go all the way back to the aliens who've seen all of civilization,
39000	50000	but to just recognize that for all of human history, as far as we're aware, there were existential risks to civilizations,
50000	55000	and they happened, right? Like there were civilizations that were killed in war,
55000	58000	and tribes that were killed in tribal warfare or whatever.
58000	62000	So people faced existential risk to the group that they identified with.
62000	66000	It's just those were local phenomena, right? It wasn't a fully global phenomena.
66000	70000	So an empire could fall, and surrounding empires didn't fall.
70000	74000	Maybe they came in and filled the space.
74000	79000	The first time that we were able to think about catastrophic risk,
79000	82000	not from like a solar flare or something that we couldn't control,
82000	87000	but from something that humans would actually create at a global level was World War II in the bomb.
87000	93000	Because it was the first time that we had tech big enough that could actually mess up everything at a global level,
93000	97000	that could mess up habitability. We just weren't powerful enough to do that before.
97000	100000	It's not that we didn't behave in ways that would have done it.
100000	103000	We just only behaved in those ways at the scale we could affect.
103000	108000	And so it's important to get that there's the entire world before World War II,
108000	113000	where we don't have the ability to make a non-habitable biosphere, non-habitable for us.
113000	117000	And then there's World War II and the beginning of a completely new phase
117000	122000	where global human-induced catastrophic risk is now a real thing.
122000	127000	And that was such a big deal that it changed the entire world in a really fundamental way,
127000	134000	which is, you know, when you study history, it's amazing how big a percentage of history is studying war, right?
134000	137000	And the history of wars, you study European history and whatever.
137000	140000	It's generals and wars and empire expansions.
140000	145000	And so the major empires near each other never had really long periods of time
145000	149000	where they weren't engaged in war or preparation for war or something like that.
149000	154000	Humans don't have a good precedent in the post-tribal phase,
154000	159000	the civilization phase of being able to solve conflicts without war for very long.
159000	165000	World War II was the first time where we could have a war that no one could win.
165000	169000	And so the superpowers couldn't fight again. They couldn't do a real kinetic war.
169000	172000	They could do diplomatic wars and Cold War type stuff
172000	176000	and they could fight proxy wars through other countries that didn't have the big weapons.
176000	180000	And so mutually assured destruction and like coming out of World War II,
180000	184000	we actually realized that nation states couldn't prevent world war.
184000	188000	And so we needed a new type of supervening government in addition to nation states,
188000	193000	which was the whole Bretton Woods world, the United Nations, the World Bank, the IMF,
193000	198000	the globalization trade type agreements, mutually assured destruction,
198000	203000	that was how do we have some coordination beyond just nation states between them
203000	206000	since we have to stop war between at least the superpowers.
206000	215000	And it was pretty successful, given that we've had like 75 years of no superpower on superpower war.
215000	220000	We've had lots of proxy wars during that time. We've had, you know, Cold War.
220000	228000	And I would say we're in a new phase now where the Bretton Woods solution is basically over, almost over.
228000	230000	Can you describe the Bretton Woods solution?
230000	241000	Yeah, so the Bretton Woods, the series of agreements for how the nations would be able to engage with each other
242000	249000	was these IGOs, these intergovernmental organizations, and was the idea of globalization.
249000	252000	Since we could have global effects, we needed to be able to think about things globally,
252000	258000	where we had trade relationships with each other, where it would not be profitable to war with each other.
258000	260000	It would be more profitable to actually be able to trade with each other.
260000	265000	So our own self-interest was, you know, going to drive our non-war interest.
266000	272000	And so this started to look like, and obviously this couldn't have happened that much earlier either,
272000	277000	because industrialization hadn't gotten far enough to be able to do massive global industrial supply chains
277000	279000	and ship stuff around, you know, quickly.
279000	285000	But like we were mentioning earlier, almost all the electronics that we use today, just basic, cheap stuff for us,
285000	287000	is made on six continents, made in many countries.
287000	291000	There's no single country in the world that could actually make many of the things that we have,
291000	297000	from the raw material extraction to the plastics and polymers and the, you know, et cetera.
297000	303000	And so the idea that we made a world that could do that kind of trade and create massive GDP growth,
303000	308000	we could all work together to be able to mine natural resources and grow stuff.
308000	312000	With the rapid GDP growth, there was the idea that everybody could keep having more,
312000	315000	without having to take each other's stuff.
315000	320000	And so that was part of kind of the Bretton Woods post-World War II model.
320000	325000	The other was that we would be so economically interdependent that blowing each other up would never make sense.
325000	328000	That worked for a while.
328000	334000	Now, it also brought us up into planetary boundaries faster,
334000	341000	the unrenovable use of resource and turning those resources into pollution on the other side of the supply chain.
341000	347000	So obviously that faster GDP growth meant the overfishing of the oceans
347000	353000	and the cutting down of the trees and the climate change and the mining, toxic mining tailings going into the water
353000	356000	and the mountaintop removal mining and all those types of things.
356000	360000	That's the consumption side of the rest that we're talking about.
360000	367000	And so the answer of let's do positive GDP is the answer rapidly and exponentially
367000	371000	obviously accelerated the planetary boundary side.
371000	380000	And that was thought about for a long time, but it started to be modeled with the Club of Rome and limits of growth.
380000	387000	But it's just very obvious to say, if you have a linear materials economy where you take stuff out of the earth faster,
387000	394000	whether it's fish or trees or oil, you take it out of the earth faster than it can replenish itself.
394000	397000	And you turn it into trash after using it for a short period of time.
397000	401000	You put the trash in the environment faster than it can process itself.
401000	404000	And there's toxicity associated with both sides of this.
404000	409000	You can't run an exponentially growing linear materials economy on a finite planet forever.
409000	411000	That's not a hard thing to figure out.
411000	418000	And it has to be exponential if there's an exponentiation in the monetary supply because of interest and then fractional reserve banking
418000	422000	and to then be able to keep up with the growing monetary supply.
422000	423000	You have to have growth of goods and services.
424000	429000	So that's that kind of thing that has happened.
429000	435000	But you also see that when you get these supply chains that are so interconnected across the world, you get increased fragility
435000	442000	because a collapse or a problem in one area then affects the whole world in a much bigger area as opposed to the issues being local.
442000	450000	So we got to see with COVID and an issue that started in one part of China affecting the whole world so much more rapidly
450000	453000	than would have happened before Bretton Woods, right?
453000	457000	Before international travel supply chains, you know, that whole kind of thing.
457000	460000	And with a bunch of second and third order effects that people wouldn't have predicted, okay?
460000	464000	We have to stop certain kinds of travel because of viral contaminants.
464000	470000	But the countries doing agriculture depend upon fertilizer they don't produce that is shipped into them
470000	472000	and depend upon pesticides they don't produce.
472000	478000	So we got both crop failures and crops being eaten by locusts in scale in Northern Africa and Iran and things like that
478000	480000	because they couldn't get the supplies of stuff in.
480000	486000	So then you get massive starvation or future kind of hunger issues because of supply chain shutdowns.
486000	493000	So you get this increased fragility and cascade dynamics where a small problem can end up leading to cascade effects.
493000	505000	And also we went from two superpowers with one catastrophe weapon to now that same catastrophe weapon
505000	511000	is there's more countries that have it, eight or nine countries that have it,
511000	514000	and there's a lot more types of catastrophe weapons.
514000	520000	We now have catastrophe weapons with weaponized drones that can hit infrastructure targets with bio
520000	524000	with, in fact, every new type of tech has created an arms race.
524000	529000	So we have not, with the UN or the other kind of intergovernmental organizations,
529000	532000	we haven't been able to really do nuclear deproliferation.
532000	539000	We've actually had more countries get nukes and keep getting faster nukes, the race to hypersonics and things like that.
539000	544000	And every new type of technology that has emerged has created an arms race.
544000	551000	And so you can't do mutually assured destruction with multiple agents the way you can with two agents.
551000	556000	Two agents, it's much easier to create a stable Nash equilibrium that's forced.
556000	561000	But the ability to modern say, if these guys shoot, who do I shoot? Do I shoot them? Do I shoot everybody? Do I?
561000	565000	And so you get a three-body problem. You get a very complex type of thing when you have multiple agents
565000	570000	and multiple different types of catastrophe weapons, including ones that can be much more easily produced than nukes.
570000	573000	Nukes are really hard to produce. There's only uranium in a few areas.
573000	576000	Uranium enrichment is hard. ICBMs are hard.
576000	580000	But weaponized drones hitting smart targets is not so hard.
580000	587000	There's a lot of other things where basically the scale at being able to manufacture them is going way, way down to where even non-state actors can have them.
587000	594000	And so when we talk about exponential tech and the decentralization of exponential tech,
594000	598000	what that means is decentralized catastrophe weapon capacity.
598000	606000	And especially in a world of increasing numbers of people feeling disenfranchised, frantic, whatever, for different reasons.
606000	614000	So I would say the Bretton Woods world doesn't prepare us to be able to deal with lots of different agents,
614000	618000	having lots of different types of catastrophe weapons you can't put mutually assured destruction on,
618000	625000	where you can't keep doing growth of the materials economy in the same way because of hitting planetary boundaries
625000	631000	and where the fragility dynamics are actually now their own source of catastrophic risk.
631000	634000	So now we're, so like there was all the world until World War II,
634000	639000	and World War II is just from a civilization timescale point of view as just a second ago.
639000	641000	It seems like a long time, but it is really not.
641000	644000	We get a short period of relative peace at the level of superpowers
644000	648000	while building up the military capacity for much, much, much worse war the entire time.
648000	655000	And then now we're at this new phase where the things that allowed us to make it through the nuclear power
655000	659000	are not the same systems that will let us make it through the next stage.
659000	662000	So what is this next post Bretton Woods?
662000	670000	How do we become safe vessels, safe stewards of many different types of exponential technology
670000	674000	is a key question when we're thinking about X-Risk.
674000	682000	Okay, so, and I'd like to try to answer the how a few ways,
682000	685000	but first on the mutually assured destruction,
686000	691000	do you give credit to the idea of two superpowers
691000	698000	not blowing each other up with nuclear weapons to the simple game theoretic model of mutually assured destruction
698000	703000	or something you've said previously, this idea of inverse correlation,
703000	710000	which I tend to believe between the, now you were talking about tech,
710000	715000	but I think it's maybe broadly true the inverse correlation
715000	718000	between competence and propensity for destruction.
718000	727000	So the better the bigger your weapons, not because you're afraid of mutually assured self-destruction,
727000	733000	but because we're human beings and there's a deep moral fortitude there
733000	736000	that somehow aligned with competence and being good at your job.
737000	746000	It's very hard to be a psychopath and be good at killing at scale.
748000	750000	Do you share any of that intuition?
750000	751000	Kind of.
753000	759000	I think most people would say that Alexander the Great and Genghis Khan and Napoleon were effective people
759000	765000	that were good at their job, that were actually maybe asymmetrically good.
766000	770000	At being able to organize people and do certain kinds of things
770000	774000	that were pretty oriented towards certain types of destruction
774000	780000	or pretty willing to, maybe they would say they were oriented towards empire expansion,
780000	784000	but pretty willing to commit certain acts of destruction in the name of it.
784000	791000	What are you worried about, the Genghis Khan or you could argue he's not a psychopath,
792000	796000	are you worried about Genghis Khan, are you worried about Hitler,
796000	802000	are you worried about a terrorist who has a very different ethic,
802000	811000	which is not even for, it's not trying to preserve and build and expand my community.
811000	815000	It's more about just destruction in itself is the goal.
815000	819000	I think the thing that you're looking at that I do agree with
819000	824000	is that there's a psychological disposition towards construction
824000	827000	and a psychological disposition more towards destruction.
827000	830000	Obviously everybody has both and can toggle between both
830000	833000	and oftentimes one is willing to destroy certain things.
833000	835000	We have this idea of creative destruction, right?
835000	838000	Willing to destroy certain things to create other things
838000	842000	and utilitarianism and trolley problems are all about exploring that space
842000	845000	and the idea of war is all about that.
845000	849000	I am trying to create something for our people and it requires destroying some other people.
853000	857000	Sociopathy is a funny topic because it's possible to have very high fealty to your in-group
857000	862000	and work on perfecting the methods of torture to the out-group at the same time
862000	865000	because you can dehumanize and then remove empathy.
869000	872000	I would also say that there are types.
873000	878000	The thing that gives hope about the orientation towards construction
878000	881000	and destruction being a little different in psychology
881000	885000	is what it takes to build really catastrophic tech
885000	890000	even today where it doesn't take what it took to make a small group of people could do it
890000	895000	takes still some real technical knowledge that required having studied for a while
895000	898000	and some then building capacity.
898000	902000	And there's a question of is that psychologically inversely correlated
902000	906000	with the desire to damage civilization, meaningfully?
908000	911000	A little bit, a little bit, I think.
912000	913000	I think a lot.
913000	917000	I think it's actually, I mean, this is the conversation I had with,
917000	921000	I think offline with Dan Carlin, which is like,
921000	925000	it's pretty easy to come up with ways that any competent,
925000	929000	I can come up with a lot of ways to hurt a lot of people.
929000	932000	And it's pretty easy, like I alone could do it.
935000	939000	There's a lot of people as smarter, smarter than me,
939000	942000	at least in their creation of explosives.
942000	947000	Why are we not seeing more insane mass murder?
947000	953000	I think there's something fascinating and beautiful about this.
953000	959000	And it does have to do with some deeply pro-social types of characteristics in humans.
962000	965000	But when you're dealing with very large numbers,
965000	968000	you don't need a whole lot of a phenomena.
968000	969000	And so then you start to say,
969000	972000	well, what's the probability that X won't happen this year,
972000	974000	then won't happen in the next two years, three years, four years.
974000	978000	And then how many people are doing destructive things with lower tech
978000	981000	and then how many of them can get access to higher tech
981000	984000	because they didn't have to figure out how to build.
984000	988000	So when I can get commercial tech,
988000	991000	and maybe I don't understand tech very well,
991000	994000	but I understand it well enough to utilize it, not to create it,
994000	996000	and I can repurpose it,
996000	1001000	when we saw that commercial drone with a homemade thermite bomb
1001000	1003000	hit the Ukrainian munitions factory
1003000	1007000	and do the equivalent of an incendiary bomb level of damage,
1007000	1009000	that was just home tech.
1009000	1011000	It was a simple kind of thing.
1011000	1015000	And so the question is not
1015000	1019000	does it stay being a small percentage of the population?
1019000	1024000	The question is, can you bind that phenomena nearly completely?
1026000	1030000	And especially now as you start to get into bigger things,
1030000	1035000	CRISPR gene drive technologies and various things like that,
1035000	1038000	can you bind it completely long term?
1039000	1040000	Over what period of time?
1040000	1041000	Not perfectly though.
1041000	1042000	That's the thing.
1042000	1047000	I'm trying to say that there is some, let's call it,
1047000	1052000	a random word, love that's inherent in,
1052000	1055000	that's core to human nature,
1055000	1058000	that's preventing destruction at scale.
1058000	1062000	And you're saying, yeah, but there's a lot of humans.
1062000	1064000	There's going to be eight plus billion,
1064000	1067000	and then there's a lot of seconds in the day to come up with stuff.
1067000	1071000	There's a lot of pain in the world that can lead to a distorted view
1071000	1075000	of the world such that you want to channel that pain into the destruction,
1075000	1076000	all those kinds of things.
1076000	1080000	And it's only a matter of time that anyone individual can do large damage,
1080000	1086000	especially as we create more and more democratized, decentralized ways
1086000	1090000	to deliver that damage even if you don't know how to build the initial weapon.
1090000	1101000	But the thing is, it seems like it's a race between the cheapening of destructive weapons
1101000	1107000	and the capacity of humans to express their love towards each other.
1107000	1114000	And it's a race that so far, I know on Twitter, it's not popular to say,
1114000	1116000	but love is winning, okay?
1116000	1119000	So what is the argument that love is going to lose here?
1119000	1126000	Against nuclear weapons, a biotech, and AI, and drones?
1126000	1131000	Okay, I'm going to comment the end of this to a how love wins.
1131000	1133000	So I just want you to know that that's where I'm oriented.
1133000	1135000	That's the end, okay.
1135000	1143000	But I'm going to argue against why that is a given, because it's not a given.
1143000	1144000	I don't believe.
1144000	1148000	This is like a good romantic comedy, so you're going to create drama right now.
1148000	1151000	But it will end in a happy ending.
1151000	1155000	Well, it's because it's only a happy ending if we actually understand the issues well enough
1155000	1157000	and take responsibility to shift it.
1157000	1161000	Do I believe, like, there's a reason why there's so much more dystopic sci-fi
1161000	1166000	than protopic sci-fi, and the sum of protopic sci-fi usually requires magic,
1166000	1173000	is because, or at least magical tech, right, dilithium crystals and warp drives and stuff,
1173000	1180000	because it's very hard to imagine people like the people we have been in the history books
1180000	1188000	with exponential type technology and power that don't eventually blow themselves up,
1188000	1192000	that make good enough choices as stewards of their environment and their commons
1192000	1194000	and each other and etc.
1194000	1197000	So, like, it's easier to think of scenarios where we blow ourselves up
1197000	1201000	than it is to think of scenarios where we avoid every single scenario where we blow ourselves up.
1201000	1206000	And when I say blow ourselves up, I mean the environmental versions, the terrorist versions,
1206000	1210000	the war versions, the cumulative externalities versions.
1210000	1217000	Can I, and I'm sorry if I'm interrupting your flow of thought, but why is it easier?
1217000	1225000	Could it be a weird psychological thing where we either are just more capable to visualize explosions and destruction
1225000	1230000	and then the sicker thought, which is like we kind of enjoy for some weird reason
1230000	1234000	thinking about that kind of stuff, even though we wouldn't actually act on it.
1234000	1240000	It's almost like some weird, like I love playing shooter games, you know, first person shooters.
1240000	1246000	And like, especially if it's like murdering, doing a doom, you're shooting demons.
1246000	1250000	I play one of my favorite games, Diablo, is like slashing through different monsters
1250000	1253000	and the screaming pain and the hellfire.
1253000	1258000	And then I go out into the real world to eat my coconut ice cream and I'm all about love.
1258000	1267000	So can we trust our ability to visualize how it all goes to shit as an actual rational way of thinking?
1267000	1276000	I think it's a fair question to say to what degree is there just kind of perverse fantasy and morbid exploration
1276000	1281000	and whatever else that happens in our imagination.
1281000	1283000	But I don't think that's the whole of it.
1283000	1288000	I think there is also a reality to the combinatorial possibility space
1288000	1297000	and the difference in the probabilities that there's a lot of ways I could try to put the 70 trillion cells of your body together that don't make you.
1297000	1300000	There's not that many ways I can put them together that make you.
1300000	1307000	There's a lot of ways I could try to connect the organs together that make some weird kind of group of organs on a desk
1307000	1310000	but that doesn't actually make a functioning human.
1310000	1314000	And you can kill an adult human in a second but you can't get one in a second.
1314000	1317000	It takes 20 years to grow one and a lot of things to happen, right?
1317000	1325000	I could destroy this building in a couple of minutes with demolition but it took a year or a couple of years to build it.
1325000	1327000	There is...
1327000	1332000	Calm down, Cole. This is just an example. It doesn't mean it.
1332000	1336000	There's a gradient where entropy is easier
1336000	1341000	and there's a lot more ways to put a set of things together that don't work
1341000	1344000	than the few that really do produce higher order synergies.
1344000	1351000	And so when we look at a history of war
1351000	1356000	and then we look at exponentially more powerful warfare,
1356000	1358000	an arms race that drives out in all these directions,
1358000	1360000	and we look at a history of environmental destruction
1360000	1364000	and exponentially more powerful tech that makes exponential externalities
1364000	1368000	multiplied by the total number of agents that are doing it in the cumulative effects,
1368000	1372000	there's a lot of ways the whole thing can break, like a lot of different ways.
1372000	1376000	And for it to get ahead, it has to have none of those happen.
1376000	1381000	And so there's just a probability space where it's easier to imagine that thing.
1381000	1385000	So to say how do we have a pro-topic future, we have to say,
1385000	1390000	well, one criteria must be that it avoids all of the catastrophic risks.
1390000	1393000	So can we inventory all the catastrophic risks?
1393000	1396000	Can we inventory the patterns of human behavior that give rise to them?
1396000	1399000	And could we try to solve for that?
1399000	1403000	And could we have that be the essence of the social technology
1403000	1407000	that we're thinking about to be able to guide, bind, and direct a new physical technology?
1407000	1411000	Because so far our physical technology, like we were talking about the Genghis Kahn's
1411000	1416000	and like that, that obviously use certain kinds of physical technology and armaments
1416000	1422000	and also social technology and unconventional warfare for a particular set of purposes.
1422000	1428000	But we have things that don't look like warfare, like Rockefeller and Standard Oil.
1428000	1436000	And it looked like a constructive mindset to be able to bring this new energy resource to the world.
1436000	1438000	And it did.
1438000	1443000	And the second order effects of that are climate change
1443000	1448000	and all of the oil spills that have happened and will happen
1448000	1452000	and all of the wars in the Middle East over the oil that have been there
1452000	1458000	and the massive political clusterfuck and human life issues that are associated with it
1458000	1461000	and on and on, right?
1461000	1468000	And so it's also not just the orientation to construct a thing
1468000	1470000	can have a narrow focus on what I'm trying to construct
1470000	1473000	but be affecting a lot of other things through second and third order effects
1473000	1475000	I'm not taking responsibility for.
1475000	1481000	And you often on another tangent mentioned second, third, and fourth order effects.
1481000	1483000	And third order.
1483000	1484000	Cascading.
1484000	1486000	Which is really fascinating.
1486000	1493000	Like starting with the third order plus, it gets really interesting.
1493000	1496000	Because we don't even acknowledge like the second order effects.
1496000	1497000	Right.
1497000	1501000	But like thinking because those it could get bigger and bigger and bigger
1501000	1503000	in ways we're not anticipating.
1503000	1504000	So how do we make those?
1504000	1510000	So it sounds like part of the part of the thing that you are thinking through
1510000	1516000	in terms of a solution, how to create an anti fragile, a resilient society
1516000	1520000	is to make explicit.
1520000	1524000	Acknowledge, understand the externalities.
1524000	1529000	The second order, third order, fourth order and the order effects.
1529000	1532000	How do we start to think about those effects?
1532000	1537000	Yeah, the war application is harm we're trying to cause or that we're aware we're causing, right?
1537000	1542000	The externality is harm that at least supposedly we're not aware we're causing
1542000	1544000	or at minimum it's not our intention, right?
1544000	1547000	Maybe we're either totally unaware of it or we're aware of it
1547000	1549000	but it is a side effect of what our intention is.
1549000	1551000	It's not the intention itself.
1551000	1553000	There are catastrophic risks from both types.
1553000	1560000	The direct application of increased technological power to a rival risk intent
1560000	1564000	which is going to cause harm for some out group for some in group to win
1564000	1566000	but the out group is also working on growing the tech
1566000	1570000	and if they don't lose completely they reverse engineer the tech upregulated
1570000	1572000	come back with more capacity.
1572000	1578000	So there's the exponential tech arms race side of in group out group rivalry
1578000	1580000	using exponential tech that is one set of risks.
1580000	1587000	And the other set of risks is the application of exponentially more powerful tech
1587000	1591000	not intentionally to try and beat an out group
1591000	1593000	but to try to achieve some goal that we have
1593000	1599000	but to produce a second and third order effects that do have harm to the commons
1599000	1602000	to other people to environment to other groups
1602000	1606000	that might actually be bigger problems
1606000	1609000	than the problem we were originally trying to solve with the thing we were building.
1609000	1614000	When Facebook was building a dating app
1614000	1617000	and then building a social app where people could tag pictures
1617000	1622000	they weren't trying to build a democracy destroying app
1622000	1629000	that would maximize time on site as part of its ad model
1629000	1632000	through AI optimization of a news feed
1632000	1634000	to the thing that made people spend most time on site
1634000	1638000	which is usually them being limbically hijacked more than something else
1638000	1641000	and it's a pretty powerful second order effect
1641000	1644000	and a pretty fast one
1644000	1648000	because the rate of tech is obviously able to get distributed to much larger scale
1648000	1653000	much faster and with a bigger jump in terms of total vertical capacity
1653000	1657000	then that's what it means to get to the verticalizing part of an exponential curve.
1657000	1663000	So that's what it means to get to the verticalizing part of an exponential curve
1663000	1668000	so just like we can see that oil had these second order environmental effects
1668000	1670000	and also social and political effects
1670000	1673000	war and so much of the whole
1673000	1679000	like the total amount of oil used has a proportionality to total global GDP
1679000	1682000	and this is why we have this, you know, the petrodollar
1682000	1689000	and so the oil thing also had the exponential effect
1689000	1694000	and so the oil thing also had the externalities of a major aspect
1694000	1698000	of what happened with military industrial complex and things like that
1698000	1702000	but we can see the same thing with more current technologies
1702000	1705000	with Facebook and Google and other things
1705000	1709000	so I don't think we can run
1709000	1711000	and the more powerful the tech is
1711000	1715000	we build it for reason X, whatever reason X is
1715000	1718000	maybe X is three things, maybe it's one thing, right?
1718000	1722000	We're doing the oil thing because we want to make cars
1722000	1724000	because it's a better method of individual transportation
1724000	1728000	we're building the Facebook thing because we're going to connect people socially in the personal sphere
1728000	1733000	but it interacts with complex systems
1733000	1737000	with ecologies, economies, psychologies, cultures
1737000	1740000	and so it has effects on other than the thing we're intending
1740000	1744000	some of those effects can end up being negative effects
1744000	1748000	but because this technology, if we make it to solve a problem
1748000	1750000	it has to overcome the problem
1750000	1753000	the problem's been around for a while, it's going to overcome in a short period of time
1753000	1757000	so it usually has greater scale, greater rate of magnitude in some way
1757000	1761000	that also means that the externalities that it creates might be bigger problems
1761000	1764000	and you can say, well, but then that's the new problem
1764000	1766000	and humanity will innovate its way out of that
1766000	1771000	well, I don't think that's paying attention to the fact that we can't keep up with exponential curves
1771000	1776000	like that, nor do finite spaces allow exponential externalities forever
1776000	1781000	and this is why a lot of the smartest people thinking about this are thinking
1781000	1784000	well, no, I think we're totally screwed
1784000	1788000	unless we can make a benevolent AI singleton that rules all of us
1788000	1792000	you know, guys like Bostrom and others thinking in those directions
1792000	1796000	because they're like, how do humans try to do
1796000	1799000	multi-polarity and make it work
1799000	1803000	and I have a different answer of what I think it looks like
1803000	1808000	that does have more to do with the love but some applied social tech align with love
1808000	1811000	because I have a bunch of really dumb ideas
1811000	1813000	I'd prefer to hear less
1813000	1815000	I'd like to hear some of them first
1826000	1829000	you
