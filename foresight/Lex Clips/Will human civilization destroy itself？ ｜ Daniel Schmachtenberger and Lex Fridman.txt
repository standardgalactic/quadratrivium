What flavors of catastrophic risk are we talking about here?
What's your favorite flavor in terms of ice cream?
Mine is coconut. Nobody seems to like coconut ice cream.
So ice cream aside, what do you most worry about in terms of catastrophic risk
that will help us kind of make concrete the discussion we're having about how to fix this whole thing?
Yeah, I think it's worth taking a historical perspective briefly to just kind of orient everyone to it.
We don't have to go all the way back to the aliens who've seen all of civilization,
but to just recognize that for all of human history, as far as we're aware, there were existential risks to civilizations,
and they happened, right? Like there were civilizations that were killed in war,
and tribes that were killed in tribal warfare or whatever.
So people faced existential risk to the group that they identified with.
It's just those were local phenomena, right? It wasn't a fully global phenomena.
So an empire could fall, and surrounding empires didn't fall.
Maybe they came in and filled the space.
The first time that we were able to think about catastrophic risk,
not from like a solar flare or something that we couldn't control,
but from something that humans would actually create at a global level was World War II in the bomb.
Because it was the first time that we had tech big enough that could actually mess up everything at a global level,
that could mess up habitability. We just weren't powerful enough to do that before.
It's not that we didn't behave in ways that would have done it.
We just only behaved in those ways at the scale we could affect.
And so it's important to get that there's the entire world before World War II,
where we don't have the ability to make a non-habitable biosphere, non-habitable for us.
And then there's World War II and the beginning of a completely new phase
where global human-induced catastrophic risk is now a real thing.
And that was such a big deal that it changed the entire world in a really fundamental way,
which is, you know, when you study history, it's amazing how big a percentage of history is studying war, right?
And the history of wars, you study European history and whatever.
It's generals and wars and empire expansions.
And so the major empires near each other never had really long periods of time
where they weren't engaged in war or preparation for war or something like that.
Humans don't have a good precedent in the post-tribal phase,
the civilization phase of being able to solve conflicts without war for very long.
World War II was the first time where we could have a war that no one could win.
And so the superpowers couldn't fight again. They couldn't do a real kinetic war.
They could do diplomatic wars and Cold War type stuff
and they could fight proxy wars through other countries that didn't have the big weapons.
And so mutually assured destruction and like coming out of World War II,
we actually realized that nation states couldn't prevent world war.
And so we needed a new type of supervening government in addition to nation states,
which was the whole Bretton Woods world, the United Nations, the World Bank, the IMF,
the globalization trade type agreements, mutually assured destruction,
that was how do we have some coordination beyond just nation states between them
since we have to stop war between at least the superpowers.
And it was pretty successful, given that we've had like 75 years of no superpower on superpower war.
We've had lots of proxy wars during that time. We've had, you know, Cold War.
And I would say we're in a new phase now where the Bretton Woods solution is basically over, almost over.
Can you describe the Bretton Woods solution?
Yeah, so the Bretton Woods, the series of agreements for how the nations would be able to engage with each other
was these IGOs, these intergovernmental organizations, and was the idea of globalization.
Since we could have global effects, we needed to be able to think about things globally,
where we had trade relationships with each other, where it would not be profitable to war with each other.
It would be more profitable to actually be able to trade with each other.
So our own self-interest was, you know, going to drive our non-war interest.
And so this started to look like, and obviously this couldn't have happened that much earlier either,
because industrialization hadn't gotten far enough to be able to do massive global industrial supply chains
and ship stuff around, you know, quickly.
But like we were mentioning earlier, almost all the electronics that we use today, just basic, cheap stuff for us,
is made on six continents, made in many countries.
There's no single country in the world that could actually make many of the things that we have,
from the raw material extraction to the plastics and polymers and the, you know, et cetera.
And so the idea that we made a world that could do that kind of trade and create massive GDP growth,
we could all work together to be able to mine natural resources and grow stuff.
With the rapid GDP growth, there was the idea that everybody could keep having more,
without having to take each other's stuff.
And so that was part of kind of the Bretton Woods post-World War II model.
The other was that we would be so economically interdependent that blowing each other up would never make sense.
That worked for a while.
Now, it also brought us up into planetary boundaries faster,
the unrenovable use of resource and turning those resources into pollution on the other side of the supply chain.
So obviously that faster GDP growth meant the overfishing of the oceans
and the cutting down of the trees and the climate change and the mining, toxic mining tailings going into the water
and the mountaintop removal mining and all those types of things.
That's the consumption side of the rest that we're talking about.
And so the answer of let's do positive GDP is the answer rapidly and exponentially
obviously accelerated the planetary boundary side.
And that was thought about for a long time, but it started to be modeled with the Club of Rome and limits of growth.
But it's just very obvious to say, if you have a linear materials economy where you take stuff out of the earth faster,
whether it's fish or trees or oil, you take it out of the earth faster than it can replenish itself.
And you turn it into trash after using it for a short period of time.
You put the trash in the environment faster than it can process itself.
And there's toxicity associated with both sides of this.
You can't run an exponentially growing linear materials economy on a finite planet forever.
That's not a hard thing to figure out.
And it has to be exponential if there's an exponentiation in the monetary supply because of interest and then fractional reserve banking
and to then be able to keep up with the growing monetary supply.
You have to have growth of goods and services.
So that's that kind of thing that has happened.
But you also see that when you get these supply chains that are so interconnected across the world, you get increased fragility
because a collapse or a problem in one area then affects the whole world in a much bigger area as opposed to the issues being local.
So we got to see with COVID and an issue that started in one part of China affecting the whole world so much more rapidly
than would have happened before Bretton Woods, right?
Before international travel supply chains, you know, that whole kind of thing.
And with a bunch of second and third order effects that people wouldn't have predicted, okay?
We have to stop certain kinds of travel because of viral contaminants.
But the countries doing agriculture depend upon fertilizer they don't produce that is shipped into them
and depend upon pesticides they don't produce.
So we got both crop failures and crops being eaten by locusts in scale in Northern Africa and Iran and things like that
because they couldn't get the supplies of stuff in.
So then you get massive starvation or future kind of hunger issues because of supply chain shutdowns.
So you get this increased fragility and cascade dynamics where a small problem can end up leading to cascade effects.
And also we went from two superpowers with one catastrophe weapon to now that same catastrophe weapon
is there's more countries that have it, eight or nine countries that have it,
and there's a lot more types of catastrophe weapons.
We now have catastrophe weapons with weaponized drones that can hit infrastructure targets with bio
with, in fact, every new type of tech has created an arms race.
So we have not, with the UN or the other kind of intergovernmental organizations,
we haven't been able to really do nuclear deproliferation.
We've actually had more countries get nukes and keep getting faster nukes, the race to hypersonics and things like that.
And every new type of technology that has emerged has created an arms race.
And so you can't do mutually assured destruction with multiple agents the way you can with two agents.
Two agents, it's much easier to create a stable Nash equilibrium that's forced.
But the ability to modern say, if these guys shoot, who do I shoot? Do I shoot them? Do I shoot everybody? Do I?
And so you get a three-body problem. You get a very complex type of thing when you have multiple agents
and multiple different types of catastrophe weapons, including ones that can be much more easily produced than nukes.
Nukes are really hard to produce. There's only uranium in a few areas.
Uranium enrichment is hard. ICBMs are hard.
But weaponized drones hitting smart targets is not so hard.
There's a lot of other things where basically the scale at being able to manufacture them is going way, way down to where even non-state actors can have them.
And so when we talk about exponential tech and the decentralization of exponential tech,
what that means is decentralized catastrophe weapon capacity.
And especially in a world of increasing numbers of people feeling disenfranchised, frantic, whatever, for different reasons.
So I would say the Bretton Woods world doesn't prepare us to be able to deal with lots of different agents,
having lots of different types of catastrophe weapons you can't put mutually assured destruction on,
where you can't keep doing growth of the materials economy in the same way because of hitting planetary boundaries
and where the fragility dynamics are actually now their own source of catastrophic risk.
So now we're, so like there was all the world until World War II,
and World War II is just from a civilization timescale point of view as just a second ago.
It seems like a long time, but it is really not.
We get a short period of relative peace at the level of superpowers
while building up the military capacity for much, much, much worse war the entire time.
And then now we're at this new phase where the things that allowed us to make it through the nuclear power
are not the same systems that will let us make it through the next stage.
So what is this next post Bretton Woods?
How do we become safe vessels, safe stewards of many different types of exponential technology
is a key question when we're thinking about X-Risk.
Okay, so, and I'd like to try to answer the how a few ways,
but first on the mutually assured destruction,
do you give credit to the idea of two superpowers
not blowing each other up with nuclear weapons to the simple game theoretic model of mutually assured destruction
or something you've said previously, this idea of inverse correlation,
which I tend to believe between the, now you were talking about tech,
but I think it's maybe broadly true the inverse correlation
between competence and propensity for destruction.
So the better the bigger your weapons, not because you're afraid of mutually assured self-destruction,
but because we're human beings and there's a deep moral fortitude there
that somehow aligned with competence and being good at your job.
It's very hard to be a psychopath and be good at killing at scale.
Do you share any of that intuition?
Kind of.
I think most people would say that Alexander the Great and Genghis Khan and Napoleon were effective people
that were good at their job, that were actually maybe asymmetrically good.
At being able to organize people and do certain kinds of things
that were pretty oriented towards certain types of destruction
or pretty willing to, maybe they would say they were oriented towards empire expansion,
but pretty willing to commit certain acts of destruction in the name of it.
What are you worried about, the Genghis Khan or you could argue he's not a psychopath,
are you worried about Genghis Khan, are you worried about Hitler,
are you worried about a terrorist who has a very different ethic,
which is not even for, it's not trying to preserve and build and expand my community.
It's more about just destruction in itself is the goal.
I think the thing that you're looking at that I do agree with
is that there's a psychological disposition towards construction
and a psychological disposition more towards destruction.
Obviously everybody has both and can toggle between both
and oftentimes one is willing to destroy certain things.
We have this idea of creative destruction, right?
Willing to destroy certain things to create other things
and utilitarianism and trolley problems are all about exploring that space
and the idea of war is all about that.
I am trying to create something for our people and it requires destroying some other people.
Sociopathy is a funny topic because it's possible to have very high fealty to your in-group
and work on perfecting the methods of torture to the out-group at the same time
because you can dehumanize and then remove empathy.
I would also say that there are types.
The thing that gives hope about the orientation towards construction
and destruction being a little different in psychology
is what it takes to build really catastrophic tech
even today where it doesn't take what it took to make a small group of people could do it
takes still some real technical knowledge that required having studied for a while
and some then building capacity.
And there's a question of is that psychologically inversely correlated
with the desire to damage civilization, meaningfully?
A little bit, a little bit, I think.
I think a lot.
I think it's actually, I mean, this is the conversation I had with,
I think offline with Dan Carlin, which is like,
it's pretty easy to come up with ways that any competent,
I can come up with a lot of ways to hurt a lot of people.
And it's pretty easy, like I alone could do it.
There's a lot of people as smarter, smarter than me,
at least in their creation of explosives.
Why are we not seeing more insane mass murder?
I think there's something fascinating and beautiful about this.
And it does have to do with some deeply pro-social types of characteristics in humans.
But when you're dealing with very large numbers,
you don't need a whole lot of a phenomena.
And so then you start to say,
well, what's the probability that X won't happen this year,
then won't happen in the next two years, three years, four years.
And then how many people are doing destructive things with lower tech
and then how many of them can get access to higher tech
because they didn't have to figure out how to build.
So when I can get commercial tech,
and maybe I don't understand tech very well,
but I understand it well enough to utilize it, not to create it,
and I can repurpose it,
when we saw that commercial drone with a homemade thermite bomb
hit the Ukrainian munitions factory
and do the equivalent of an incendiary bomb level of damage,
that was just home tech.
It was a simple kind of thing.
And so the question is not
does it stay being a small percentage of the population?
The question is, can you bind that phenomena nearly completely?
And especially now as you start to get into bigger things,
CRISPR gene drive technologies and various things like that,
can you bind it completely long term?
Over what period of time?
Not perfectly though.
That's the thing.
I'm trying to say that there is some, let's call it,
a random word, love that's inherent in,
that's core to human nature,
that's preventing destruction at scale.
And you're saying, yeah, but there's a lot of humans.
There's going to be eight plus billion,
and then there's a lot of seconds in the day to come up with stuff.
There's a lot of pain in the world that can lead to a distorted view
of the world such that you want to channel that pain into the destruction,
all those kinds of things.
And it's only a matter of time that anyone individual can do large damage,
especially as we create more and more democratized, decentralized ways
to deliver that damage even if you don't know how to build the initial weapon.
But the thing is, it seems like it's a race between the cheapening of destructive weapons
and the capacity of humans to express their love towards each other.
And it's a race that so far, I know on Twitter, it's not popular to say,
but love is winning, okay?
So what is the argument that love is going to lose here?
Against nuclear weapons, a biotech, and AI, and drones?
Okay, I'm going to comment the end of this to a how love wins.
So I just want you to know that that's where I'm oriented.
That's the end, okay.
But I'm going to argue against why that is a given, because it's not a given.
I don't believe.
This is like a good romantic comedy, so you're going to create drama right now.
But it will end in a happy ending.
Well, it's because it's only a happy ending if we actually understand the issues well enough
and take responsibility to shift it.
Do I believe, like, there's a reason why there's so much more dystopic sci-fi
than protopic sci-fi, and the sum of protopic sci-fi usually requires magic,
is because, or at least magical tech, right, dilithium crystals and warp drives and stuff,
because it's very hard to imagine people like the people we have been in the history books
with exponential type technology and power that don't eventually blow themselves up,
that make good enough choices as stewards of their environment and their commons
and each other and etc.
So, like, it's easier to think of scenarios where we blow ourselves up
than it is to think of scenarios where we avoid every single scenario where we blow ourselves up.
And when I say blow ourselves up, I mean the environmental versions, the terrorist versions,
the war versions, the cumulative externalities versions.
Can I, and I'm sorry if I'm interrupting your flow of thought, but why is it easier?
Could it be a weird psychological thing where we either are just more capable to visualize explosions and destruction
and then the sicker thought, which is like we kind of enjoy for some weird reason
thinking about that kind of stuff, even though we wouldn't actually act on it.
It's almost like some weird, like I love playing shooter games, you know, first person shooters.
And like, especially if it's like murdering, doing a doom, you're shooting demons.
I play one of my favorite games, Diablo, is like slashing through different monsters
and the screaming pain and the hellfire.
And then I go out into the real world to eat my coconut ice cream and I'm all about love.
So can we trust our ability to visualize how it all goes to shit as an actual rational way of thinking?
I think it's a fair question to say to what degree is there just kind of perverse fantasy and morbid exploration
and whatever else that happens in our imagination.
But I don't think that's the whole of it.
I think there is also a reality to the combinatorial possibility space
and the difference in the probabilities that there's a lot of ways I could try to put the 70 trillion cells of your body together that don't make you.
There's not that many ways I can put them together that make you.
There's a lot of ways I could try to connect the organs together that make some weird kind of group of organs on a desk
but that doesn't actually make a functioning human.
And you can kill an adult human in a second but you can't get one in a second.
It takes 20 years to grow one and a lot of things to happen, right?
I could destroy this building in a couple of minutes with demolition but it took a year or a couple of years to build it.
There is...
Calm down, Cole. This is just an example. It doesn't mean it.
There's a gradient where entropy is easier
and there's a lot more ways to put a set of things together that don't work
than the few that really do produce higher order synergies.
And so when we look at a history of war
and then we look at exponentially more powerful warfare,
an arms race that drives out in all these directions,
and we look at a history of environmental destruction
and exponentially more powerful tech that makes exponential externalities
multiplied by the total number of agents that are doing it in the cumulative effects,
there's a lot of ways the whole thing can break, like a lot of different ways.
And for it to get ahead, it has to have none of those happen.
And so there's just a probability space where it's easier to imagine that thing.
So to say how do we have a pro-topic future, we have to say,
well, one criteria must be that it avoids all of the catastrophic risks.
So can we inventory all the catastrophic risks?
Can we inventory the patterns of human behavior that give rise to them?
And could we try to solve for that?
And could we have that be the essence of the social technology
that we're thinking about to be able to guide, bind, and direct a new physical technology?
Because so far our physical technology, like we were talking about the Genghis Kahn's
and like that, that obviously use certain kinds of physical technology and armaments
and also social technology and unconventional warfare for a particular set of purposes.
But we have things that don't look like warfare, like Rockefeller and Standard Oil.
And it looked like a constructive mindset to be able to bring this new energy resource to the world.
And it did.
And the second order effects of that are climate change
and all of the oil spills that have happened and will happen
and all of the wars in the Middle East over the oil that have been there
and the massive political clusterfuck and human life issues that are associated with it
and on and on, right?
And so it's also not just the orientation to construct a thing
can have a narrow focus on what I'm trying to construct
but be affecting a lot of other things through second and third order effects
I'm not taking responsibility for.
And you often on another tangent mentioned second, third, and fourth order effects.
And third order.
Cascading.
Which is really fascinating.
Like starting with the third order plus, it gets really interesting.
Because we don't even acknowledge like the second order effects.
Right.
But like thinking because those it could get bigger and bigger and bigger
in ways we're not anticipating.
So how do we make those?
So it sounds like part of the part of the thing that you are thinking through
in terms of a solution, how to create an anti fragile, a resilient society
is to make explicit.
Acknowledge, understand the externalities.
The second order, third order, fourth order and the order effects.
How do we start to think about those effects?
Yeah, the war application is harm we're trying to cause or that we're aware we're causing, right?
The externality is harm that at least supposedly we're not aware we're causing
or at minimum it's not our intention, right?
Maybe we're either totally unaware of it or we're aware of it
but it is a side effect of what our intention is.
It's not the intention itself.
There are catastrophic risks from both types.
The direct application of increased technological power to a rival risk intent
which is going to cause harm for some out group for some in group to win
but the out group is also working on growing the tech
and if they don't lose completely they reverse engineer the tech upregulated
come back with more capacity.
So there's the exponential tech arms race side of in group out group rivalry
using exponential tech that is one set of risks.
And the other set of risks is the application of exponentially more powerful tech
not intentionally to try and beat an out group
but to try to achieve some goal that we have
but to produce a second and third order effects that do have harm to the commons
to other people to environment to other groups
that might actually be bigger problems
than the problem we were originally trying to solve with the thing we were building.
When Facebook was building a dating app
and then building a social app where people could tag pictures
they weren't trying to build a democracy destroying app
that would maximize time on site as part of its ad model
through AI optimization of a news feed
to the thing that made people spend most time on site
which is usually them being limbically hijacked more than something else
and it's a pretty powerful second order effect
and a pretty fast one
because the rate of tech is obviously able to get distributed to much larger scale
much faster and with a bigger jump in terms of total vertical capacity
then that's what it means to get to the verticalizing part of an exponential curve.
So that's what it means to get to the verticalizing part of an exponential curve
so just like we can see that oil had these second order environmental effects
and also social and political effects
war and so much of the whole
like the total amount of oil used has a proportionality to total global GDP
and this is why we have this, you know, the petrodollar
and so the oil thing also had the exponential effect
and so the oil thing also had the externalities of a major aspect
of what happened with military industrial complex and things like that
but we can see the same thing with more current technologies
with Facebook and Google and other things
so I don't think we can run
and the more powerful the tech is
we build it for reason X, whatever reason X is
maybe X is three things, maybe it's one thing, right?
We're doing the oil thing because we want to make cars
because it's a better method of individual transportation
we're building the Facebook thing because we're going to connect people socially in the personal sphere
but it interacts with complex systems
with ecologies, economies, psychologies, cultures
and so it has effects on other than the thing we're intending
some of those effects can end up being negative effects
but because this technology, if we make it to solve a problem
it has to overcome the problem
the problem's been around for a while, it's going to overcome in a short period of time
so it usually has greater scale, greater rate of magnitude in some way
that also means that the externalities that it creates might be bigger problems
and you can say, well, but then that's the new problem
and humanity will innovate its way out of that
well, I don't think that's paying attention to the fact that we can't keep up with exponential curves
like that, nor do finite spaces allow exponential externalities forever
and this is why a lot of the smartest people thinking about this are thinking
well, no, I think we're totally screwed
unless we can make a benevolent AI singleton that rules all of us
you know, guys like Bostrom and others thinking in those directions
because they're like, how do humans try to do
multi-polarity and make it work
and I have a different answer of what I think it looks like
that does have more to do with the love but some applied social tech align with love
because I have a bunch of really dumb ideas
I'd prefer to hear less
I'd like to hear some of them first
you
