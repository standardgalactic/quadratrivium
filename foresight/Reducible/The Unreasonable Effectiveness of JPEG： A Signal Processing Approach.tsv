start	end	text
0	5200	What you see here is one part of a really elaborate process that defines the universally
5200	12160	used JPEG image compression format. JPEG is rather complex and in this video the majority
12160	17200	of the focus will be on understanding how computer scientists came up with an algorithmic
17200	22160	and mathematical framework for solving the complex problems that image compression presents.
23200	28720	But to understand the motivation behind the ideas in JPEG we'll have to dive into the inner
28720	33920	workings of the many components involved. The perspectives we'll take will be a little bit
33920	39120	unorthodox but my hope is that you come away with a better understanding of the big themes
39120	44880	in image compression which apply to other compression related problems. It's not an
44880	50880	exaggeration to say that these concepts are used every time you open an image, play a video or
50880	56800	listen to some music. As we go through JPEG we'll interact with the wide variety of beautiful ideas
56800	62000	in the world of data compression and signal processing that make the technology around us
62000	70000	possible. Before we dive into JPEG let's talk about how computers represent images. The standard
70000	76960	color space that computers use is the RGB model. Every pixel of an image stores three values from
76960	82240	0 to 255 with higher values representing a larger weighting of the respective color.
82800	89280	So assuming each color component is expressed in 8 bits or a single byte of memory an image has
89280	97280	3 bytes per pixel. Here's an image with a little more than 5 million pixels. Based on our assumptions
97280	104400	the total size of this image should be about 15 megabytes but with JPEG compression the actual
104640	114640	file is only 0.8 megabytes. Same number of pixels but 5% of the expected size and the image looks
114640	123040	absolutely beautiful. This is the real magic of JPEG compression. JPEG aggressively takes advantage
123040	128880	of several clever ideas to achieve seemingly ridiculous amounts of compression with minimal
129600	135920	effects on the quality of the original image. One of the primary reasons JPEG works so well
135920	142080	is it uses lossy compression. To understand what that means let's think about compression from a
142080	148960	big picture perspective. We start with an RGB representation of an image and then we encode
148960	155760	it using a compression algorithm. This is what we store in memory and it's more compact but quite
155760	162720	different than our original RGB representation. So part of a compression scheme requires also
162720	169760	defining a decoding component that converts the stored representation of our data into the RGB
169760	176480	format that a computer can render as an image. Part of the JPEG standard is defining how both
176480	183600	the encoding and decoding work. A key point in JPEG is that the final decoded image is not going
183600	190880	to be the same as the original uncompressed image. That's why we call it lossy compression. In the
190880	197440	compression part of the pipeline we are going to deliberately lose information. To get compression
197440	203200	on the levels of 5% there's really no other option other than to actually discard some
203200	210240	information from our original image. Now the fun question to ask is what sort of information
210240	216960	from an image can we get rid of and how do we get rid of it? Answering this question is going to be
216960	223760	the primary focus of our journey into understanding JPEG. Here's an interesting image for you. If I
223760	229360	were to ask you what colors the squares of A and B were I imagine most of you would quickly say that
229360	235840	A is a darker shade of gray than B. But what if I told you that A and B were actually the same color?
236640	242160	It's okay if you don't see that. This picture is designed to trick our visual system. But once
242160	247760	we have a connector of the common color between the two squares it's much easier for us to see
247760	254480	that they are in fact the same color. So what's going on here? Over the years scientists have
254480	260000	developed a human visual system model through the study of our eyes and one incredibly interesting
260000	265840	finding through experimentation is that our eyes are much more sensitive to brightness than they
265840	272960	are to colors. And part of the JPEG compression scheme can take advantage of this. But to understand
272960	280880	how we have to dive into the world of color spaces. As we've discussed the RGB color space is a
280880	286400	combination of red, green, and blue color components. If we put each value on a separate
286400	292320	axis in a three-dimensional space we can see how all the possible colors are just a point on this
292320	298880	cube. One aspect of RGB color space is as you progress on the diagonal from the origin to the
298880	307440	color 255, 255, 255 you get gradually brighter colors. And in fact the exact line between these
307440	312400	points defines all possible gray scale colors which are a direct measure for brightness.
313120	320720	This idea of separating brightness is core to another color space called YCBCR. YCBCR
320720	326640	stands for Y, chroma blue, and chroma red. Our Y component is going to measure the luma or brightness
326640	332720	of an image and our CB and CR components are going to encode the colors. If we look at the
332720	338800	color space the Y can be thought of as a single vertical axis with larger values encoding more
338800	346400	brightness. Every cross section of the space defines a range of colors at that particular
346400	353040	brightness. For our purposes with JPEG using the color space gives us direct access to the part
353040	359600	of color that our eyes perceive best. As a result of being more sensitive to brightness than colors
359600	366480	one idea to compress our original image involves sampling less of the CBCR components and keeping
366480	372480	all of the luma components. The technique is referred to as chroma down sampling or more commonly
372480	380800	chroma subsampling. Suppose I have this 8x8 image which has the following Y, CB, and CR components.
380800	386640	The key idea of chroma down sampling or subsampling is to take fewer samples from CB
386640	391600	and CR components since their eyes are less sensitive to them. Here's one approach that
391600	400800	defines a 4-2-0 chroma down sampling scheme. We go through our original 8x8 image in 2x2 blocks
400800	406480	and simply average the group of pixels to get a shared value of the four pixels in the original
406480	411360	image. Averaging the pixels by the way is all down sampling really means.
413600	419600	Chroma subsampling is the same exact idea but instead of averaging we just choose one of the
419600	424880	samples usually the top left pixel to be the color of the entire 2x2 block.
429120	434640	Once we have these fewer samples from the color components we can merge them with the luma component
434640	440400	which will retain the original 16 pixels and this gives us our subsampled image.
440400	446640	In this case you can see quite a difference since our 8x8 pixel image is significantly scaled up
446720	451920	but in real world images it's often hard to see any changes after subsampling.
453200	459360	By merging 2x2 blocks on the CB and CR channels into one color we are left with a quarter of the
459360	466640	original data in each color channel shrinking the total file size by 50%. We're still quite far from
466640	473200	the 5% levels that we saw in JPEG so we're going to have to exploit more than just human perception
473200	480480	of brightness. For the following components of JPEG let's focus on the Y channel which
480480	486000	essentially defines grayscale images. The principles we'll discuss from here on out
486000	492560	will also apply to the color components of an image. The next clever idea in JPEG requires
492560	497520	looking at images in a completely different perspective, one that can be a little bit
497520	504480	counterintuitive. One way to think about images is treating them as signals. If I slice a particular
504480	511280	row of an image I essentially have a row of pixels each with some value between 0 and 255.
512240	518800	If we plot these values we can get an approximation of a signal. Visualizing an image as a signal
518800	524320	allows us to talk about frequency components within an image. Higher frequency components
524320	530400	correspond to rapid changes between pixels while lower frequency components are related to smoother
530400	536640	changes between pixels. There are two key aspects of frequencies within images that are incredibly
536640	542800	important to JPEG compression. The first is that a lot of real world images shot from cameras are
542800	548640	mostly composed of lower frequency components. In other words if I take a random portion of a
548640	554160	realistic image it's pretty likely that the pixels in that area do not change that rapidly.
554960	561120	And the second key fact is that from a variety of experiments the human visual system is generally
561120	567680	less sensitive to higher frequency detail in images. JPEG takes advantage of these ideas by
567680	573760	strategically removing less important and less common higher frequency components from an image
573760	581760	to achieve even more compression. But there's one big problem. How do we get frequency components
581760	587760	from an image? This is where some particularly clever and beautiful math comes into play.
587760	593920	The answer to this question lies in a special operation called the discrete cosine transform
593920	601920	or the DCT. The DCT works for any size input but to simplify things let's focus on an input
601920	608720	of eight pixels. Just as we did earlier let's suppose these eight pixels form some sort of signal.
609440	614480	We'll never be sure what exactly the signal looks like since we only have eight points
615040	622400	but the clever and definitely not obvious idea of the DCT is to represent these eight points as
622400	629360	sums of sampled points from cosine waves. And I really want to emphasize the fact that we only
629360	635760	care about the discrete samples. Visually I think it's nice to see the continuous signals and cosine
635760	642000	waves but throughout a discussion the only values that really matter are the sampled points from
642000	649680	these functions. The DCT takes an input of sampled points from our original signal and gives us an
649680	658800	output of the same size. We'll refer to the outputs of the DCT as coefficients. These coefficients
658800	664960	represent the weights of cosine waves of different frequencies that contribute to the original signal.
666000	672800	A nice analogy is to think of this as unraveling a complex signal into a weighted sum of simpler
672800	679440	cosine waves. If you've never interacted with this type of idea before it's natural to be confused.
680560	686560	What cosine waves do we even use? How do cosine waves relate to pixels on an image?
687520	693200	None of it makes any sense. Don't worry these are important questions that we will answer.
695360	702400	Let's start simple and talk about cosine waves. Here's a graph of cosine from zero to pi. I've
702400	707680	given you this general notion that the DCT is supposed to tell us how much of a specific
707680	715600	cosine wave is contained in a signal. So let's test this out. What happens if I provide an
715600	721520	actual cosine wave as the input signal to the DCT? What do we expect to happen?
722720	728720	Okay we can try this but there's a problem. To follow our existing example we need eight
728720	734960	sampled points from the cosine wave to make this work. How exactly should we sample the cosine wave?
735600	741200	Well there are a few options but let me present to you the most common one. What we can do is
741200	747760	split our cosine waves domain into eight even slices and then we take the midpoint of each of
747760	754560	these slices. This gives us the following input points which we can generalize for any number of
754560	764240	points. But for our purposes we'll stick with the smaller n equals 8 example. So going back to our
764240	770480	question what should we expect the output to be when we pass in sampled points from a standard
770480	778960	cosine function? This is an interesting experiment. When we pass these points from a cosine wave into
778960	786800	a DCT transform we get the following output. Only one coefficient has a nonzero value meaning
786800	793440	there's only one cosine wave that contributes to our input. And that seems to make some sense since
793440	800400	the input is literally from a cosine wave. In this case the first index is the only coefficient
800400	807040	with a nonzero value. When trying to understand complex ideas it really helps to play around
807040	812560	with these simple examples. A cool follow-up to our experiment is to see what happens when
812560	819440	we change the amplitude of this cosine wave. The first index DCT coefficient increases.
820080	825600	If we flip the cosine wave by multiplying negative one the DCT coefficient changes sign.
826560	833280	It's exactly acting like a weight for a cosine wave. When the amplitude of the input cosine
833280	841520	wave changes the weight correspondingly reflects that change. So taking a step back
841520	848480	how does this relate to images? Well just as we took images and represented them as signals
848480	855920	the reverse also works. Standard grayscale images have pixels ranging from 0 to 255.
856560	862320	The intuition with cosine waves to images makes more sense when we shift the range of pixel values
862320	870160	by 128. With pixel values from negative 128 to 127 we can see a better mapping between this
870160	876640	cosine wave to an actual set of eight pixels. This particular wave is a nice way to represent
876640	883120	a row of gradually decreasing pixel values. And the magnitude of that change as well as the
883120	889840	direction of the change is reflected in the amplitude of the original cosine wave and consequently
889840	896080	the DCT coefficient. So let's continue this experiment to see what else we can uncover
896080	901920	about the DCT. We've messed with the amplitude of a cosine wave. What other parameters could we
901920	909760	change? A simple one is to just shift the cosine wave up or down. Let's see what happens when we
909760	917440	try that. It looks like shifting up or down the signal only affects the zeroth index coefficient.
918320	923520	That's an interesting data point that we'll come back to. Another parameter of cosine waves
923520	929840	is the frequency. What we're going to do now is show the DCT coefficients as we wind up the
929840	936080	frequency of this cosine wave. I'll keep the sampling strategy we discussed earlier consistent
936080	943040	among all frequencies. Let's see what happens. As we increase the frequencies we get a few
943040	949920	different DCT coefficients for the respective cosine wave. That is until we get to this cosine wave.
950560	955840	For this particular cosine wave only the second index has a non-zero coefficient.
955840	962880	This cosine wave is actually just double the frequency of the previous cosine wave. This is
962880	969760	super interesting. The first index of the output seems to nicely correspond with the cosine wave
969760	975760	of frequency one while the second index correlates with a cosine wave of frequency two.
977280	982480	Let's continue this experiment of increasing frequencies but before I continue see if you
982480	987840	can take a guess at what frequencies the other coefficients will correspond to. Here we go.
988400	994800	We slowly increase the frequency and boom the index three coefficient corresponds to a cosine
994800	1002800	wave of frequency three. Then frequency four comes next and this pattern continues until we get to a
1002800	1011360	cosine wave of frequency seven. Pretty insane right? So for the coefficients indexed one to seven
1011360	1018000	it looks like they represent the weight on a cosine wave with the frequency that matches the index.
1020000	1025840	So what about the remaining index zero? We saw shifting cosine waves up and down led to a change
1025840	1032800	in the zeroth index. What cosine wave does that represent? Some of you have probably figured it
1032800	1038880	out but if you think about what a zero frequency cosine wave is it's just a constant signal.
1039840	1045600	What that means in terms of images is it gives us a measure of the overall brightness of a set of
1045600	1053760	pixels. Brighter images will have a larger zeroth coefficient than darker images. This is why shifting
1053760	1060640	up a cosine wave only impacts the zeroth coefficient. Putting this all together each
1060640	1066880	of these frequencies correspond to a different pattern of images and what the core DCT does
1066880	1072320	is break down how each of these fundamental patterns contribute to the original image.
1073600	1080400	And it turns out that all possible combinations of eight pixel values can be represented as a sum
1080400	1087360	of these eight cosine waves. Why that's true is not at all obvious but we can begin to understand it
1087360	1094960	once we translate this intuition to the actual math behind the DCT. If you look at the mathematical
1094960	1101520	definition of the DCT we usually have a vector definition of the original signal and the output
1101520	1109200	coefficients. We want to define the kth index of the coefficient vector mathematically. What you
1109200	1115360	often see is something that looks like the following and with the intuition that we just built up
1115360	1121840	we'll see that this equation is doing exactly what we want. Let's start with the cosine term.
1121840	1128000	This function should be familiar. It's the exact representation of a sampled point from a cosine
1128000	1133120	wave using our earlier sampling scheme and it incorporates the frequency of the cosine wave
1133120	1140400	as well. Now what's interesting is in order to get the kth index we are actually summing over a
1140400	1146880	product of each sampled point with samples from the cosine wave. Why does that make sense?
1147840	1152640	This type of expression might look vaguely familiar to a lot of you. Let me rewrite this
1152640	1158480	another way. We know that the original signal points can be represented as a vector but what
1158480	1164800	if we rewrote the sampled points from the cosine wave as a vector as well? What does this expression
1164800	1172320	mean in the context of these two vectors? It's a dot product and what we know about dot products is
1172400	1179520	there a nice way to measure similarity between two vectors. That's why when we pass in sampled
1179520	1187200	points from a cosine wave of frequency k as the input to the DCT we got large values at the kth
1187200	1193840	index coefficient. These two vectors were just scaled versions of each other so the dot product
1193840	1200080	was maximized and this perspective reveals what I think is truly the most surprising and elegant
1200080	1207280	part of the DCT. By picking the points through the sampling method we can think of the entire DCT
1207280	1214080	as a matrix vector product. All we're doing here is a linear transformation. The rows of the matrix
1214080	1219280	are the sampled points from the cosine waves of the respective frequencies and what's truly
1219280	1225680	astounding is that all row vectors in this matrix are orthogonal to each other. What I mean by that
1225680	1231920	is if you take the dot product of any two row vectors representing cosine waves you will get
1231920	1238880	zero if they are different rows of the matrix. Intuitively this is why in our earlier experiments
1238880	1246000	when we pass in a cosine wave of a particular frequency as an input into the DCT we didn't get
1246000	1252240	a contribution from any of the other coefficients which represented different frequency cosine waves.
1252880	1258800	The orthogonality of the sampled points from different cosine waves generates this behavior.
1259360	1265600	It's really quite beautiful. Another great property of the DCT that follows from these
1265600	1272320	facts is invertibility. I've talked about the DCT as a way of decomposing a signal into a
1272320	1278880	coefficient representation of weights associated with cosine waves. We can also reverse this process.
1279680	1285040	If I take my coefficient representation of the signal I can apply what's called the inverse
1285040	1293440	DCT to get back the original signal and it is the exact same signal. No information is lost
1293440	1300080	in this step. How we do that is by multiplying our coefficient representation with the inverse
1300080	1306640	of the matrix. What's cool about this is that because of the orthogonality of the vectors
1306640	1312960	the inverse is just the transpose of our original matrix with some additional normalization constants.
1314800	1321200	Now there's a super nice interpretation of the inverse DCT. The sample cosine wave points are
1321200	1327200	now column vectors so what the inverse DCT is doing is essentially summing over a weighted
1327200	1333600	combination of cosine waves directly to get the original signal. And because these columns are
1333600	1339360	orthogonal to each other that's what allows us to represent any set of eight points with these
1339360	1345680	eight cosine waves. Absolutely incredible. I know we spent some time and went through some fairly
1345680	1351360	complex math to get here but it's precisely these details that are the most fundamental part of not
1351360	1359040	only the DCT but many other similar transforms in the world of signal processing. Now that we have
1359040	1365440	a good intuition on the one-dimensional DCT let's talk about how JPEG specifically uses it.
1366480	1371840	JPEG takes an image and splits it into eight by eight blocks and then centers their values
1371840	1381040	around zero by subtracting 128. Then we take the block and apply the DCT to each row of the block
1381040	1389200	giving us eight sets of DCT coefficients. We then apply the DCT to each column of the block.
1393280	1396880	This process is what defines the two-dimensional DCT.
1399120	1405760	So in the end we have 64 coefficients each of which are await on a specific eight by eight pattern.
1406400	1412000	Notice the first row and column correspond to the earlier one-dimensional patterns and the other
1412000	1419040	elements are compositions of these patterns. And just like in the one-dimensional case the big idea
1419040	1426960	here is that we can build up any eight by eight image using these 64 fundamental patterns. The
1426960	1432960	same signal perspective we talked about earlier also applies here except now with 2D waveforms.
1432960	1438480	What's going on here is we are plotting the pixel value on the z-axis with brighter pixels having
1438480	1445200	larger values. What's fun to play around with is seeing how the waveform and image come together
1445200	1453200	as we slowly put together the 64 coefficients in increasing frequencies. Seeing this in action
1453200	1458640	makes you realize that one interesting property is that by the time we incorporate a small portion
1458720	1463920	of the coefficients our signal and image already look pretty close to the original versions.
1465360	1470080	There's an even more direct experiment we can run to quantify this notion.
1471760	1476480	This particular eight by eight block was randomly picked out of the original image.
1477200	1482560	If we map out the magnitude of the DCT coefficients on this block we see that most
1482560	1486400	of the largest values are in the upper left section which corresponds to the original
1486560	1492240	upper left section which corresponds to lower frequency components. And what's even more
1492240	1498560	interesting if I take any eight by eight block on this image almost all of them have the same
1498560	1504800	property. This property of the DCT is what's commonly referred to as energy compaction.
1506240	1512320	After applying the DCT most of the largest values are concentrated in a few low frequency
1512320	1519040	coefficients and this holds true in a lot of real world images. The concept of energy compaction
1519040	1524880	is incredibly important in image compression. As we will see it's exactly the property that
1524880	1530400	will allow us to aggressively compress images while still retaining high visual quality.
1533280	1539280	Fun fact the original discovery the DCT centered around approximating other transforms
1539280	1545840	that had better energy compaction properties but were too expensive to carry out. The DCT is just
1545840	1551120	one example of a transform that has this property for real world images and we use it because it's
1551120	1557920	quite easy to compute. There's a lot of complexity involved here but one of my goals in this discussion
1557920	1563600	of the DCT and JPEG was directly interacting with these deep and important ideas through
1563600	1570000	questions and visual experiments. Interactivity is a core part of learning and a website that
1570000	1576560	does a fantastic job of interactive explanations is Brilliant the sponsor for this video. From the
1576560	1581200	basics of mathematics and algorithmic thinking to more complex ideas and deep learning and
1581200	1586480	probability Brilliant offers a variety of courses and learning paths for those interested in getting
1586480	1592000	hands on practice. Our discussions of JPEG interacted with some linear algebra in the
1592000	1597680	application of image compression and Brilliant has an entire linear algebra module that goes
1597680	1602400	through the fundamentals and even shows applications of these ideas in image compression,
1602400	1609120	cryptography, error correcting codes and much more. When I was a student I really enjoyed their
1609120	1613920	computer science fundamentals course which has engaging visualizations of concepts and great
1613920	1618800	practice problems that helped me solidify my foundations. You can get started for free by
1618800	1624720	going to brilliant.org slash reducible which is linked in the description below. Brilliant is
1624720	1629840	providing a special offer through this channel where the first 200 members to sign up get 20%
1629840	1634960	off the annual subscription. It's a great way to learn more about the topics in these videos
1634960	1640480	and also a good way to support this channel. Big thanks to Brilliant for sponsoring this video.
1642720	1647360	Let's put everything we've discussed with the DCT together in one more experiment.
1648160	1652800	We'll split our image into eight by eight blocks and then basically rebuild the image
1652800	1659440	with each block having only a certain number of DCT coefficients. We're going to start off with
1659440	1666720	zero coefficients and slowly build up the image. After one coefficient we end up with basically a
1666720	1674080	blur of the original image and as we add DCT coefficients slowly notice how quickly the
1674080	1681600	image starts looking like the original. By the time we get to less than 25% of the DCT coefficients
1681600	1687680	you almost can't even tell the difference between the two images. This confirms the key aspects of
1687680	1694480	why JPEG works for this particular image. Almost all the blocks are composed of the lowest frequency
1694480	1700800	components and we are generally less sensitive to changes in high frequency details. So at this
1700800	1706400	point we know we can eliminate higher frequency components from the DCT but the next natural
1706400	1713520	question is how we actually do this. The process for eliminating higher frequency components in JPEG
1713520	1721280	is called quantization. Quantization is a simple idea. Given an eight by eight matrix of frequency
1721280	1727920	coefficients from the DCT what we're going to do is basically divide each element by a scalar value
1727920	1734720	and round it to an integer. These values are defined in terms of a quantization table. Notice
1734720	1739840	larger values in the bottom right of the table leading to zero values in the higher frequency
1739840	1747520	components. In the decoding stage of JPEG we'll actually be multiplying this result by the same
1747520	1754480	quantization matrix element by element and as you can see the final coefficient matrix will be quite
1754480	1760320	different from the original one. So what that means is we're purposely losing information in this step
1761600	1767600	but the key idea here is most of the lower frequency components will be retained. This is
1767600	1774400	why the energy compaction property of the DCT is so useful when the largest values lie in the lowest
1774400	1780240	frequencies we will end up with a lot of zeros in the less important high frequency components.
1780480	1787760	These quantization tables are provided by the JPEG standard from visual experiments
1787760	1794640	and are the main way for JPEG to define quality of compression. High quality compression parameters
1794640	1801760	can be translated to lower quantization table values. In practice JPEG also defines a separate
1801760	1808800	quantization table for both the luma and color channels. Notice that in the color channels
1808880	1816640	quantization can be even more aggressive. After performing quantization we have a matrix of quantized
1816640	1822160	DCT coefficients where we can now exploit redundancy to get even more compression.
1824000	1830160	The last part of JPEG encoding involves a combination of run length encoding and Huffman
1830160	1836800	encoding. One clever trick is that a JPEG encoder will order the coefficients in a zigzag manner
1836800	1843360	to maximize the chance of a large sequence of zeros in order. Classic run length encoding can
1843360	1849600	compress this fairly easily. All that's going on here is we are compressing every sequence of zeros
1849600	1855760	into a count of the occurrences in a continuous sequence. JPEG actually performs something a
1855760	1861920	little bit more sophisticated by keeping track of a triplet. For every coefficient this triplet
1861920	1867680	encodes the number of preceding zeros, the number of bits required to encode the coefficient,
1867680	1874080	and finally the actual coefficient value. We also have an end of block value to signal that
1874080	1883120	everything from here on out will be zeros. This particular scheme works well with Huffman
1883120	1890160	coding to further exploit redundancy. The big idea of Huffman codes is that more frequently
1890240	1897040	used data can be encoded with fewer bits, and it turns out especially with the nature of quantization,
1897040	1902240	these triplets can be further compressed since some of these values will be more frequent than others.
1904000	1909440	However, I'm purposefully not going to go into the details of how JPEG uses Huffman codes to
1909440	1915280	compress the data because it really does get quite tricky. To give you some sense of the problems,
1915280	1920320	we have to deal with the encoding signs of coefficients as well as triplets for all eight
1920320	1927120	by eight blocks. Most encoders also encode the top left coefficient separate from all the other
1927120	1933360	coefficients. And when you handle that, you have to deal with this on both luma and color channels.
1934320	1938720	And when you eventually get that working, a good chunk of your logic will break when you
1938720	1944480	have to deal with the different types of chroma subsampling. Implementing an optimized fully
1944480	1950640	functional JPEG encoder and decoder is no joke. I wouldn't give that task to even my worst enemies.
1951840	1956720	But in terms of the big picture, all that's going on in this component is taking advantage
1956720	1963920	of the redundancy that quantization creates. A JPEG decoder will be able to use the Huffman code
1963920	1970720	data in the files to get back all quantized DCT coefficients that were encoded. This part
1970720	1979600	of the JPEG algorithm does not lose any information. JPEG as a whole brings about an interesting
1979600	1985600	discussion on the philosophy of data compression. The classic and most straightforward way to
1985600	1992240	compress data is by taking advantage of redundancy. This is the basis of losses image compression
1992240	1998880	algorithms such as those found in PNG file formats. In fact, for images where it's really important
1998880	2005680	not to lose any information, PNG format is recommended over JPEG. But on most real world
2005680	2011520	images, being aware of the medium of presentation introduces another really powerful perspective.
2012640	2017760	A lot of innovation in JPEG compression comes from experiments and understanding of human
2017760	2023440	visual systems. It's from these experiments that we realized human eyes are less sensitive to color
2023440	2028560	and also less sensitive to higher frequencies. So we can remove that information without a
2028560	2036080	significant visual impact. This is why JPEG is so much more effective at compressing images than
2036080	2042560	lossless formats. You'll find these same types of techniques used in audio and video compression
2042560	2048640	where algorithms use our perceptions of sound and motion respectively to remove less relevant data.
2049520	2054560	In fact, variations of the discrete cosine transform and quantization show up in both
2054560	2061440	audio and video compression. It really is incredible to me how people in these fields came up with the
2061440	2067520	mathematical and algorithmic framework to utilize the way we actually perceive the digital technology
2067520	2073760	around us. There's so much depth to these topics that I can never hope to cover in just one video,
2073760	2078960	but I do hope this gives you a sense and appreciation for the complexity of the technology
2078960	2087520	around us that we use on a daily basis. Thanks for watching and I'll see you all in the next one.
