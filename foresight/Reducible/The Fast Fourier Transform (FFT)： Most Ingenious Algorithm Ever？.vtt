WEBVTT

00:00.000 --> 00:05.680
The world of algorithms is vast, but we can often split them into two distinct classes.

00:05.680 --> 00:10.080
The first class is those that are inherently useful. Think of your standard graph algorithms

00:10.080 --> 00:15.280
like DFS or BFS. These algorithms show up all over the place, they are efficient,

00:15.280 --> 00:19.360
and as a result we like to understand them. The second class of algorithms we study are

00:19.360 --> 00:24.400
ones that are just purely beautiful. Think of the first time you saw the incredibly simple

00:24.400 --> 00:30.080
recursive implementation of towers of annoy. If you have a soul, you feel a sense of wonder,

00:30.080 --> 00:35.440
a sense of awe at the elegance of such an algorithm. It happens to not actually be that

00:35.440 --> 00:40.400
useful or efficient as a matter of fact, but we still study it just as we like to read a work

00:40.400 --> 00:46.720
of fiction. It inspires us and motivates out-of-the-box thinking. Today I want to talk about an algorithm

00:46.720 --> 00:52.160
that rightfully belongs in both classes and my personal favorite algorithm, the fast Fourier

00:52.160 --> 00:59.120
transform. The fast Fourier transform or FFT is without exaggeration one of the most important

00:59.120 --> 01:03.840
algorithms created in the last century. So much of the modern technology that we have today such as

01:03.840 --> 01:09.520
wireless communication, GPS, and in fact anything related to the vast field of signal processing

01:09.520 --> 01:16.640
relies on the insights of the FFT. But it's also one of the most beautiful algorithms you ever see.

01:16.640 --> 01:21.600
The depth and sheer number of brilliant ideas that went into it is just astounding.

01:22.960 --> 01:28.160
It's easy to miss the beauty aspect of the FFT since it's often introduced in fairly complex

01:28.160 --> 01:32.960
settings that require a lot of prerequisite knowledge such as the discrete Fourier transform,

01:32.960 --> 01:39.040
time domain to frequency domain conversions, and much more. And to be fair, to get a full

01:39.040 --> 01:43.520
appreciation of the applications of the FFT you can't really avoid any of these ideas.

01:44.080 --> 01:48.880
But I want to do something a little different. We'll take a discovery-based approach to learning

01:48.880 --> 01:54.480
about the FFT in a context that you are all familiar with, polynomial multiplication.

01:54.480 --> 01:59.360
The way I see the structure of this video, it's all about starting with some common ground and

01:59.360 --> 02:05.120
then slowly asking questions that will hopefully prompt you to discover the truly ingenious ideas

02:05.200 --> 02:08.800
behind the FFT. Alright, let's get started.

02:18.960 --> 02:24.400
The setup here is simple. We're given two polynomials and want to find the product.

02:24.400 --> 02:29.120
Our task will be to design an efficient algorithm for this problem. Mathematically,

02:29.120 --> 02:34.640
we know one algorithm to multiply polynomials by repeatedly applying the distributed property.

02:34.640 --> 02:38.800
All of you have perhaps instinctively been applying this algorithm since any algebra class.

02:39.600 --> 02:44.240
Before we try this idea though, the first question we have to address is representation

02:44.240 --> 02:50.240
of polynomials in a computer. The most natural way to represent them is through coefficients,

02:50.240 --> 02:55.840
where we map coefficients to lists. It helps to arrange our coefficients in the following order,

02:55.840 --> 03:01.520
mainly because now the coefficient of the kth index is mapped to the coefficient of the kth

03:01.520 --> 03:07.200
degree term. We will refer to this representation as the coefficient representation of polynomials.

03:11.520 --> 03:17.120
In general, given 2d degree polynomials, the product will have a degree of 2 times d.

03:17.120 --> 03:21.840
And the running time of this algorithm, if we actually went about implementing it with the most

03:21.840 --> 03:28.320
natural distributed property approach, will be O of d squared, since each term in polynomial a will

03:28.320 --> 03:34.480
have to be multiplied by all terms in polynomial b. The question now is, can we do better?

03:36.080 --> 03:41.120
The first really clever idea comes from thinking about polynomials a little bit differently.

03:41.760 --> 03:46.000
To see the key inside here, let's take a look at one of the simplest polynomials,

03:46.000 --> 03:51.760
a degree 1 polynomial or a linear function. We can represent any line with exactly two

03:51.760 --> 03:57.760
coefficients, one for the degree 0 term and one for the degree 1 term. The key part that makes

03:57.760 --> 04:04.000
this representation valid is that every representation has a one to one mapping to a unique line.

04:04.000 --> 04:09.120
What other representations of a line have this property? There are actually several

04:09.120 --> 04:14.960
reasonable representations, but the one that we are going to use is the two point representation.

04:14.960 --> 04:19.280
We know from geometry that any line can be represented by two distinct points.

04:19.280 --> 04:23.920
And turns out that there is a neat extension of this for general polynomials, which I will state here.

04:24.800 --> 04:31.600
Any polynomial of degree d can be uniquely represented by d plus one points. For example,

04:31.600 --> 04:37.040
if I gave you three random points, this means that there is exactly one quadratic function

04:37.040 --> 04:42.560
that goes through all three of these points. If I give you four points, there is exactly one

04:42.560 --> 04:47.200
cubic function that goes through all these points. This statement is perhaps a little surprising,

04:47.200 --> 04:53.280
so it deserves a proof. Suppose I give you d plus one unique points of a d degree polynomial p of x.

04:53.280 --> 04:57.280
We want to show that for these set of points, there is only one set of coefficients.

04:57.840 --> 05:01.760
So if we actually evaluate our polynomial at each of these points,

05:01.760 --> 05:06.960
we get the following set of system of equations. Whenever you have a system of equations,

05:06.960 --> 05:11.040
writing it as a matrix vector product is almost always helpful for analysis.

05:12.240 --> 05:16.880
One nice property of this matrix is that if each of our original points is unique,

05:16.880 --> 05:20.560
as it is in this case, the matrix will always be invertible.

05:20.560 --> 05:26.160
The easiest way to show this is by calculating the determinant, which you will find is non-zero,

05:26.160 --> 05:31.440
but I'll link a nice linear algebraic proof of this fact in the description as well for those

05:31.440 --> 05:37.600
interested. Anyways, what this means is that for every set of points, there exists a unique set

05:37.600 --> 05:45.200
of coefficients and consequently a unique polynomial. Taking a step back, what this means

05:45.200 --> 05:50.240
is that there are actually two ways to represent polynomials of degree d, the first of which is

05:50.240 --> 05:56.000
the coefficient representation and the second with just d plus one points, which we will call

05:56.000 --> 06:03.040
the value representation. A nice property of using the value representation is multiplication

06:03.040 --> 06:09.280
of polynomials is much easier. Let's say I ask you to multiply these two polynomials a of x

06:09.280 --> 06:16.240
and b of x. We know the resulting polynomial c of x will be of degree four, so we'll need five

06:16.240 --> 06:22.320
points to uniquely represent the product. What we can now do is take five points from each of the

06:22.320 --> 06:27.600
two polynomials and then simply multiply the function values one by one to get the value

06:27.600 --> 06:34.480
representation of the product of the two polynomials. Following our earlier rule,

06:34.480 --> 06:39.520
there is only one degree four polynomial that passes through these points. That polynomial

06:39.520 --> 06:44.160
happens to be the following in the coefficient representation and this is indeed the product

06:44.160 --> 06:50.640
of our original a of x and b of x polynomials. With the multiplication operation being performed

06:50.640 --> 06:56.160
using the value representation, we've now reduced the time for multiplication from our original

06:56.160 --> 07:04.720
d squared operations to the order of only degree d operations. Okay, so let's propose a plan for

07:04.720 --> 07:09.840
an improvement to polynomial multiplication. We are given two polynomials in the coefficient

07:09.840 --> 07:15.920
representation of degree d each. We know multiplication is faster using the value representation,

07:15.920 --> 07:21.680
so what we'll do is evaluate each of these polynomials at 2d plus one points, multiply each

07:21.680 --> 07:26.480
of these points pairwise to get the value representation of the product polynomial,

07:26.480 --> 07:32.720
and then finally somehow convert the value representation back to the final coefficient

07:32.720 --> 07:37.920
representation. This is the grand plan, but there are several pieces of the puzzle we haven't figured

07:37.920 --> 07:43.360
out. What we're missing is really some sort of magic box that could take polynomials in the

07:43.360 --> 07:49.120
coefficient representation and convert them to the value representation and then vice versa.

07:49.120 --> 07:54.800
That magic box my friends and trust me it is truly magical is the fast Fourier transform.

07:56.240 --> 08:01.280
Let's first focus on taking polynomials from the coefficient representation to the value

08:01.280 --> 08:07.680
representation, which we will call evaluation. We have a degree d polynomial and we want to

08:07.680 --> 08:13.280
evaluate the polynomial at n points where n is some number greater than d plus one.

08:13.280 --> 08:18.880
Let's think about the most straightforward way to do this. We can pick n random x coordinates

08:18.880 --> 08:25.040
and simply calculate the respective y coordinate. This works, but when we deconstruct what's

08:25.040 --> 08:31.760
actually going on here, we run into our old foe. Each evaluation will take o of d operations,

08:31.760 --> 08:37.920
making this method run in o of n times d time, which ends up being o of d squared to evaluate

08:37.920 --> 08:43.600
all n points. So we're back to where we started. Can we find a way to optimize this?

08:45.040 --> 08:50.000
Let's try to simplify the problem. Let's say instead of considering a general polynomial,

08:50.000 --> 08:54.720
we wanted to instead just evaluate a simple polynomial like p of x is equal to x squared

08:54.720 --> 09:00.400
at eight points. The question now is, which points should we pick? Is there any set of

09:00.400 --> 09:04.960
points when knowing the value of one point immediately implies the value of another?

09:06.480 --> 09:11.600
In fact, there is. If we pick the point x equals one, we immediately know the value

09:11.600 --> 09:17.360
of the point x equals negative one. Similarly, if we pick x equals two, we automatically know x

09:17.360 --> 09:22.800
equals negative two will have the same value. Extending this idea, the key property we want

09:22.800 --> 09:28.160
here is that our eight points should be positive negative pairs. The reason this works is due to

09:28.160 --> 09:33.120
the property of even functions where a function evaluated at negative x is going to equal the

09:33.120 --> 09:39.760
function evaluated at positive x. Okay, but then the next immediate question is, what about functions

09:39.760 --> 09:46.240
like x cubed? Does the same trick work? It actually kind of does, but with one caveat.

09:46.240 --> 09:51.680
Each positive x value will have the same value as the negative x value, but with the sign flipped.

09:52.560 --> 09:58.080
So in these two cases of odd and even degree single term polynomials, instead of evaluating

09:58.080 --> 10:04.400
eight individual points, we can actually get away with evaluating exactly four positive points,

10:04.400 --> 10:08.480
from which we immediately know the value of the respective negative points.

10:10.080 --> 10:13.680
Let's see if we can extend this idea to a more general polynomial,

10:14.320 --> 10:19.360
taking inspiration from our early exploration, let's split our polynomial into even and odd

10:19.360 --> 10:25.760
degree terms. If we factor an x from the odd degree terms, we end up having two new polynomials,

10:25.760 --> 10:30.560
where these new polynomials have only even degree terms. Let's actually give these

10:30.560 --> 10:36.000
polynomials formal names, the first representing the even terms and the second representing

10:36.000 --> 10:42.480
odd terms after factoring out x. This fact allows us to recycle a lot of computation between positive

10:42.480 --> 10:47.920
and negative pairs of points. A bonus fact is that since these even and odd polynomials are

10:47.920 --> 10:53.280
functions of x squared, they are polynomials of degree two, which is a much simpler problem than

10:53.280 --> 11:00.800
our original degree five polynomial. Generalizing these observations, if we have an n minus one

11:00.800 --> 11:07.120
degree polynomial that we want to evaluate at n points, we can split the polynomial into even

11:07.120 --> 11:13.520
and odd terms with these two smaller polynomials now having degree n over two minus one. So,

11:13.520 --> 11:18.160
how do we evaluate these polynomials with half the degree of our original polynomial?

11:18.160 --> 11:23.760
Well, what's beautiful here is that this is just another evaluation problem, but this time

11:23.760 --> 11:29.600
we need to evaluate the polynomials at each of our original inputs squared. And this works out

11:29.600 --> 11:35.440
nicely since our original points were positive negative pairs, so if we originally had n points,

11:35.520 --> 11:40.960
we now only end up having n over two points. This is starting to smell like the start of

11:40.960 --> 11:47.600
a recursive algorithm. Let's take a look at the bigger picture. We want to evaluate a polynomial

11:47.600 --> 11:54.960
p of x at n points where the n points are positive negative paired. We split the polynomial into odd

11:54.960 --> 12:01.120
and even degree components where we now have two simpler polynomials of degree n over two minus one

12:01.120 --> 12:07.280
that only need n over two points to evaluate. Once we recursively evaluate these smaller

12:07.280 --> 12:12.080
polynomials, we can then go through every point in our original set of n points and calculate the

12:12.080 --> 12:17.200
respective values by utilizing the relationship between the positive and negative paired points.

12:18.640 --> 12:25.040
This gives us the value representation of our original polynomial. If we can get this to work,

12:25.040 --> 12:30.960
this means we have an O of n log n recursive algorithm since the two recursive sub problems

12:30.960 --> 12:36.640
have half the size of the original problem and take linear time to evaluate the n points.

12:36.640 --> 12:40.320
This would be a huge improvement from our earlier quadratic running time,

12:40.320 --> 12:43.760
but there is one major problem. Can you spot the issue?

12:46.000 --> 12:51.120
The problem occurs at the recursive step. The entire scheme relies on the fact

12:51.120 --> 12:55.600
that the polynomial will have positive and negative paired points for evaluation.

12:56.160 --> 13:01.520
This works at the top level, but the next level we are evaluating n over two points where each

13:01.520 --> 13:08.320
point is a squared value. These all end up being positive so the recursion breaks. So then the natural

13:08.320 --> 13:15.040
question is, can we make these new set of points positive negative paired? Some of you may already

13:15.040 --> 13:20.400
see it, but this actually leads to the third absolutely ingenious idea behind the FFT.

13:20.960 --> 13:27.680
The only way this is possible is if we expand the domain of possible initial points to include

13:27.680 --> 13:34.160
complex numbers. For a special choice of complex numbers, the recursive relation works perfectly

13:34.160 --> 13:38.400
where every subsequent set of points will contain positive negative pairs.

13:39.360 --> 13:42.480
What possible set of initial n points has this property?

13:43.360 --> 13:47.440
This is a hard question and to answer it we are going to do a little bit of reverse

13:47.440 --> 13:52.720
engineering with an example. Let's say we have a degree 3 polynomial which requires at least

13:52.720 --> 13:58.400
n equals 4 points for its value representation. These points need to be positive negative pairs

13:58.400 --> 14:05.520
so we can write them as x1, negative x1, x2, and negative x2. We know that the recursive step

14:05.520 --> 14:10.800
will require that we evaluate the odd and even splits of the polynomial at two points,

14:10.880 --> 14:17.200
x1 squared and x2 squared. Now the key constraint here is that for the recursion to work,

14:17.200 --> 14:23.520
these two points also have to be positive negative pairs. So we have an equivalence between x2

14:23.520 --> 14:30.320
squared and negative x1 squared. At the bottom level of the recursion we'll have a single point

14:30.320 --> 14:37.120
x1 to the power of 4. Now what's nice is that we get to choose these points. Let's see what happens

14:37.120 --> 14:44.480
if we pick our initial x1 to be 1. This means two of our initial points are 1 and negative 1

14:44.480 --> 14:50.160
which at the next level of recursion means that x1 squared and negative x1 squared also have to be

14:50.160 --> 14:56.800
1 and negative 1 respectively. And at the bottom layer we have only one point which ends up being

14:56.800 --> 15:04.240
1. Now the question becomes what x2 should we choose so that when we square x2 we end up with

15:04.240 --> 15:09.520
negative 1. The answer to that is the complex number i which means that the four points that

15:09.520 --> 15:15.040
we need to evaluate this polynomial at are 1, negative 1, i and negative i.

15:17.280 --> 15:22.480
An alternate perspective to what we just did here is that we essentially just solved the equation

15:22.480 --> 15:27.920
x to the power of 4 equals 1. Since at the bottom layer of the recursion the value of any of our

15:27.920 --> 15:35.360
original points to the power of 4 was 1. We know this equation has four solutions all of which

15:35.360 --> 15:42.000
are encompassed by a special set of points called the fourth roots of unity. Let's see if this

15:42.000 --> 15:49.120
generalizes. If given a degree 5 polynomial we'll need n is greater than or equal to six points.

15:49.120 --> 15:54.000
Since our recursive method is splitting each problem in half it's convenient to just pick

15:54.000 --> 15:59.440
a power of 2 so let's pick n equals 8. Now what we need to do is to find eight points that are

15:59.440 --> 16:05.360
positive negative paired such that each of these points when raised to the eighth power is equal

16:05.360 --> 16:12.480
to 1. We see that the right points are the eighth roots of unity. Generalizing this to any d degree

16:12.480 --> 16:18.480
polynomial what we will do is pick n is greater than or equal to d plus 1 points such that n is

16:18.480 --> 16:24.560
the power of 2 and the points that we should choose are the nth roots of unity. This fact

16:24.560 --> 16:30.320
deserves a little bit more explanation. Why does this work? Before we answer that question let's

16:30.320 --> 16:35.440
formalize a few things. The nth roots of unity are the solution to the following equation

16:35.440 --> 16:40.320
and they are best visualized as equally spaced points on the unit circle. The angle between

16:40.320 --> 16:47.200
these points is 2 pi over n. With this fact a nice way to compactly write these points is with

16:47.200 --> 16:52.960
complex exponential notation through Euler's formula. One standard way to define the roots of

16:52.960 --> 16:59.760
unity is by defining this omega term as e to the power of 2 pi i over n and then what this allows

16:59.760 --> 17:05.360
us to do is define individual roots of unity quite compactly. Here are some examples.

17:08.240 --> 17:13.920
So now when we say we want to evaluate a polynomial at the nth roots of unity what that really means

17:13.920 --> 17:19.520
is we want to evaluate it at omega to the power of 0, omega to the power of 1, so on and so forth

17:19.520 --> 17:27.680
until omega to the power of n minus 1. So going back to our original question of why evaluating

17:27.680 --> 17:33.520
the polynomial p of x at the nth roots of unity works for a recursive algorithm there are two

17:33.520 --> 17:39.360
key properties at play here. For one our original set of points are positive negative paired where

17:39.360 --> 17:45.520
for the jth root omega to the power of j omega to the power of j plus n over 2 is going to be the

17:45.520 --> 17:51.920
corresponding pair. Now in our recursive step we will be squaring each of these points and passing

17:51.920 --> 17:57.920
them on to the even and odd degree polynomials. This is what happens when we square our original

17:57.920 --> 18:04.800
nth roots of unity. This reveals the second key property of the nth roots of unity. When we square

18:04.800 --> 18:10.720
the nth roots of unity we end up with the n over 2 roots of unity which are also positive negative

18:10.720 --> 18:16.080
paired and are just the right number of points for the two new polynomials of half the degree.

18:16.080 --> 18:21.440
This same pattern holds at every level of the recursion until we end up with just one point.

18:21.440 --> 18:22.560
How beautiful is that?

18:28.480 --> 18:33.600
All right we are now ready to outline the core fast Fourier transform algorithm. The

18:33.600 --> 18:39.440
FFT will take in a coefficient representation of a degree n minus one polynomial where n is the power

18:39.440 --> 18:46.800
of two. We will define omega as e to the power of two pi i over n to allow us to define roots of

18:46.800 --> 18:52.640
unity easily. The first case we need to handle is the base case which is going to be when n is equal

18:52.640 --> 18:59.040
to one. All this means is that we are evaluating the polynomial at one point. Our recursive case is

18:59.040 --> 19:05.760
two calls to the FFT. One on even degree terms and one on odd degree terms. The intention is that

19:05.760 --> 19:11.120
these polynomials are now half the degree of our original polynomial so they only need to be evaluated

19:11.120 --> 19:16.800
at n over two roots of unity. Assuming the recursion works the output of these calls will be the

19:16.800 --> 19:22.400
corresponding value representation of these even and odd degree term polynomials which we will label

19:22.400 --> 19:29.760
as y e and y o. Now on to the tricky part which is to take the output from these recursive calls

19:29.760 --> 19:36.000
and combine them to get the value representation of our original degree n minus one polynomial.

19:36.640 --> 19:41.760
We saw earlier that the key idea was to use the relationship between positive and negative pairs

19:41.760 --> 19:47.760
but now we have to slightly modify this logic for our roots of unity inputs. As a quick note

19:47.760 --> 19:52.800
yes I did modify the indexing to zero indexing because we're getting ready to write some code.

19:52.800 --> 19:58.080
We know the jth input point will correspond to jth root of unity which results in the following

19:58.080 --> 20:05.600
relationship. We also saw earlier that the paired point negative omega to the power of j is equal

20:05.600 --> 20:11.680
to omega to the power of j plus n over two due to the properties of the roots of unity. Using this

20:11.680 --> 20:18.320
fact we can modify the second equation as follows. And lastly one more fact that's nice is that the

20:18.320 --> 20:26.560
jth index in our y e and y o list correspond to the even and odd polynomials evaluated at omega

20:26.560 --> 20:32.560
to the power of two times j. What this allows us to do is rewrite our equations as follows

20:32.560 --> 20:38.240
which makes it much easier to implement code. As mentioned this part is tricky so I encourage

20:38.240 --> 20:44.800
you to take your time and verify that each of these steps is indeed true. The final step in the

20:44.800 --> 20:51.360
FFT algorithm is to then return the values of a polynomial p evaluated at the nth roots of unity.

20:52.000 --> 20:58.400
Let's now translate this outlined logic into code. Our function FFT will take an input p which is

20:58.400 --> 21:05.680
the coefficient representation of a polynomial p. We first define n as the length of p and we will

21:05.680 --> 21:12.240
assume that n is a power of two. Just to be clear there are implementations of the FFT that can handle

21:12.240 --> 21:18.800
n not being a power of two but those are way more complicated. The power of two cases encompass the

21:18.800 --> 21:24.800
core ideas of the algorithm. We now handle the base case which is just a matter of returning our

21:24.800 --> 21:31.360
original p. This makes sense since we only have one element making p a degree zero polynomial

21:31.360 --> 21:37.440
or constant. Otherwise we define omega as we have outlined and then proceed with the recursive step.

21:37.440 --> 21:42.240
The first part of the recursive step requires splitting the polynomial into even and odd

21:42.240 --> 21:47.600
degree terms which is quite easy to do. Then we recursively call our FFT function on these

21:47.600 --> 21:52.880
polynomials that now have half the degree of our original polynomial. We denote the outputs as

21:52.880 --> 21:59.920
y e and y o as we have done in the outline. Now it's time to put this all together. We initialize

21:59.920 --> 22:07.440
our output list which will contain the final value representation. Then for all j up to n over two

22:07.440 --> 22:13.440
we calculate the value representations as we have outlined. After populating all values in our list

22:13.440 --> 22:19.840
we then return that list and that's the FFT. Overall pretty crazy how all the ideas we talked

22:19.840 --> 22:26.960
about end up coming together in eleven lines of truly elegant code. Let's now take a larger

22:26.960 --> 22:31.760
look at our original problem of polynomial multiplication and see where we are. We now

22:31.760 --> 22:38.080
have a way to convert coefficient representations to value representations efficiently using the FFT.

22:38.080 --> 22:43.280
So now the only missing piece is the reverse process of converting from value representations

22:43.280 --> 22:48.880
to coefficient representations which is formally called interpolation. This is where things get

22:48.880 --> 22:56.400
really wild. On the surface the idea of reversing evaluation feels like a significantly harder task.

22:56.400 --> 22:59.760
Let's take a step back and look at this problem from another perspective.

23:00.400 --> 23:05.920
Evaluation and interpolation are closely connected and as we saw earlier we can express

23:05.920 --> 23:13.680
evaluation as a matrix vector product. We have a vector of coefficients multiplied by a matrix

23:13.680 --> 23:20.320
of our evaluation points to give us the value representation. Now in the FFT algorithm the

23:20.320 --> 23:26.320
kth evaluation point was a corresponding root of unity which allows us to rewrite the matrix

23:26.320 --> 23:33.600
vector product as follows. This particular matrix has a special name the discrete Fourier transform

23:33.600 --> 23:41.520
or DFT matrix. In most textbooks and references the FFT at its core is an algorithm for calculating

23:41.520 --> 23:48.000
these types of matrix vector products efficiently. Polynomial evaluation at the roots of unity happens

23:48.000 --> 23:53.200
to be one case where this type of matrix vector product shows up so that's why we can use the

23:53.200 --> 23:59.840
FFT. Anyways the nice fact about the FFT and evaluation in this context is that interpolation

23:59.840 --> 24:07.520
is much easier to understand. Interpolation requires inversing this DFT matrix. For interpolation we are

24:07.520 --> 24:13.360
given a value representation of our polynomial and we want to find the coefficient representation

24:13.360 --> 24:19.120
which means we have to multiply the value representation by the inverse of the DFT matrix.

24:20.000 --> 24:24.560
So let me show you what the inverse of this matrix looks like. I'm purposefully skipping

24:24.560 --> 24:29.440
a lot of important linear algebra facts here since that would be an entirely different video

24:29.440 --> 24:33.520
but given that this is the inverse matrix what stands out to you?

24:36.160 --> 24:41.200
It's really quite amazing but this inverse matrix looks almost the same as our original

24:41.200 --> 24:47.840
DFT matrix. In fact the only difference is that every single omega in our original DFT matrix

24:47.840 --> 24:54.000
is now just replaced with omega to the power of negative 1 with a normalization factor of 1 over

24:54.000 --> 25:00.400
n. This indicates a potential to reuse the FFT logic for interpolation since the matrix structure

25:00.400 --> 25:06.400
is basically the same. Let's formalize this suspicion by doing a direct comparison. In

25:06.400 --> 25:12.240
evaluation which involved the FFT we are given a set of coefficients and evaluate the polynomial

25:12.240 --> 25:17.680
at the roots of unity to get a value representation. This involved the following matrix vector product

25:17.680 --> 25:24.320
where we define omega as e to the power of 2 pi i divided by n. Looking at interpolation we now

25:24.320 --> 25:30.800
want to define what is formally called the inverse FFT algorithm. The inverse FFT will take a value

25:30.880 --> 25:36.160
representation where each value was evaluated at the roots of unity and gives you a set of

25:36.160 --> 25:41.360
coefficients for the original polynomial basically reversing what the original FFT did.

25:41.360 --> 25:47.280
As we just saw this requires multiplying by the inverse of the DFT matrix. We noted that each

25:47.280 --> 25:54.000
omega in our original DFT matrix now corresponds to 1 over n times omega to the power of negative 1.

25:54.000 --> 26:00.000
Now the punchline here is that what this means is we can define the inverse FFT

26:00.000 --> 26:07.440
as the same FFT function but now called on the value representation with omega defined as 1 over

26:07.440 --> 26:14.640
n times e to the negative 2 pi i divided by n. That's it. With those small changes we have

26:14.640 --> 26:21.040
an inverse FFT that performs interpolation. Just so we are super clear on what sorcery just happened

26:21.040 --> 26:25.840
let me remind you of the original FFT implementation and now let me show you the

26:25.840 --> 26:30.640
inverse FFT implementation which takes the value representation as an input.

26:31.920 --> 26:37.680
What we literally do is copy our FFT implementation, change the name of the recursive calls to

26:37.680 --> 26:43.680
match and then literally change one line of code. One line and that's all there is to it.

26:44.640 --> 26:49.680
So if your mind isn't blown you haven't been paying attention. Let's take a look at what we just

26:49.680 --> 26:55.440
did. We motivated the FFT through the problem of polynomial multiplication where the first

26:55.440 --> 27:00.640
brilliant idea came from representing and multiplying polynomials using the value representation.

27:01.360 --> 27:05.680
Converting polynomials to a value representation required us to come up with an appropriate

27:05.680 --> 27:11.680
set of evaluation points. Our first attempts at solving this problem inspired the clever idea of

27:11.680 --> 27:17.440
using positive negative pairs but the recursion didn't quite work unless we expanded the domain

27:17.440 --> 27:23.120
to complex numbers. The next brilliant idea came from using the nth roots of unity where the points

27:23.120 --> 27:28.480
at every level of recursion are positive negative paired. This evaluation scheme using the roots

27:28.480 --> 27:33.920
of unity encompassed the essence of the FFT algorithm. When confronted with the problem

27:33.920 --> 27:39.040
of reversing the process using interpolation we discovered something truly astounding.

27:39.040 --> 27:46.000
The inverse FFT is the same algorithm but with one minor adjustment. So if we take a look at what

27:46.000 --> 27:52.240
we just did here there's not one, not two, not three, but four absolutely mind-blowing ideas

27:52.240 --> 27:57.680
that come together to make this work. Do I really need to say more on why this is my favorite algorithm?

28:01.280 --> 28:05.360
That's all for this video and thanks for watching. If you enjoyed the content please

28:05.360 --> 28:10.000
hit the like button so that this content will be recommended to more people. If you want to

28:10.000 --> 28:14.480
see more content like this please don't forget to hit the subscribe button and if you want to

28:14.480 --> 28:18.960
more directly support the work of this channel please check out the Patreon page linked in the

28:18.960 --> 28:22.240
description below. I'll see you all in the next video.

