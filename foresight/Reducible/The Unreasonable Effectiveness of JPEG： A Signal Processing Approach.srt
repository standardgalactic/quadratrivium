1
00:00:00,000 --> 00:00:05,200
What you see here is one part of a really elaborate process that defines the universally

2
00:00:05,200 --> 00:00:12,160
used JPEG image compression format. JPEG is rather complex and in this video the majority

3
00:00:12,160 --> 00:00:17,200
of the focus will be on understanding how computer scientists came up with an algorithmic

4
00:00:17,200 --> 00:00:22,160
and mathematical framework for solving the complex problems that image compression presents.

5
00:00:23,200 --> 00:00:28,720
But to understand the motivation behind the ideas in JPEG we'll have to dive into the inner

6
00:00:28,720 --> 00:00:33,920
workings of the many components involved. The perspectives we'll take will be a little bit

7
00:00:33,920 --> 00:00:39,120
unorthodox but my hope is that you come away with a better understanding of the big themes

8
00:00:39,120 --> 00:00:44,880
in image compression which apply to other compression related problems. It's not an

9
00:00:44,880 --> 00:00:50,880
exaggeration to say that these concepts are used every time you open an image, play a video or

10
00:00:50,880 --> 00:00:56,800
listen to some music. As we go through JPEG we'll interact with the wide variety of beautiful ideas

11
00:00:56,800 --> 00:01:02,000
in the world of data compression and signal processing that make the technology around us

12
00:01:02,000 --> 00:01:10,000
possible. Before we dive into JPEG let's talk about how computers represent images. The standard

13
00:01:10,000 --> 00:01:16,960
color space that computers use is the RGB model. Every pixel of an image stores three values from

14
00:01:16,960 --> 00:01:22,240
0 to 255 with higher values representing a larger weighting of the respective color.

15
00:01:22,800 --> 00:01:29,280
So assuming each color component is expressed in 8 bits or a single byte of memory an image has

16
00:01:29,280 --> 00:01:37,280
3 bytes per pixel. Here's an image with a little more than 5 million pixels. Based on our assumptions

17
00:01:37,280 --> 00:01:44,400
the total size of this image should be about 15 megabytes but with JPEG compression the actual

18
00:01:44,640 --> 00:01:54,640
file is only 0.8 megabytes. Same number of pixels but 5% of the expected size and the image looks

19
00:01:54,640 --> 00:02:03,040
absolutely beautiful. This is the real magic of JPEG compression. JPEG aggressively takes advantage

20
00:02:03,040 --> 00:02:08,880
of several clever ideas to achieve seemingly ridiculous amounts of compression with minimal

21
00:02:09,600 --> 00:02:15,920
effects on the quality of the original image. One of the primary reasons JPEG works so well

22
00:02:15,920 --> 00:02:22,080
is it uses lossy compression. To understand what that means let's think about compression from a

23
00:02:22,080 --> 00:02:28,960
big picture perspective. We start with an RGB representation of an image and then we encode

24
00:02:28,960 --> 00:02:35,760
it using a compression algorithm. This is what we store in memory and it's more compact but quite

25
00:02:35,760 --> 00:02:42,720
different than our original RGB representation. So part of a compression scheme requires also

26
00:02:42,720 --> 00:02:49,760
defining a decoding component that converts the stored representation of our data into the RGB

27
00:02:49,760 --> 00:02:56,480
format that a computer can render as an image. Part of the JPEG standard is defining how both

28
00:02:56,480 --> 00:03:03,600
the encoding and decoding work. A key point in JPEG is that the final decoded image is not going

29
00:03:03,600 --> 00:03:10,880
to be the same as the original uncompressed image. That's why we call it lossy compression. In the

30
00:03:10,880 --> 00:03:17,440
compression part of the pipeline we are going to deliberately lose information. To get compression

31
00:03:17,440 --> 00:03:23,200
on the levels of 5% there's really no other option other than to actually discard some

32
00:03:23,200 --> 00:03:30,240
information from our original image. Now the fun question to ask is what sort of information

33
00:03:30,240 --> 00:03:36,960
from an image can we get rid of and how do we get rid of it? Answering this question is going to be

34
00:03:36,960 --> 00:03:43,760
the primary focus of our journey into understanding JPEG. Here's an interesting image for you. If I

35
00:03:43,760 --> 00:03:49,360
were to ask you what colors the squares of A and B were I imagine most of you would quickly say that

36
00:03:49,360 --> 00:03:55,840
A is a darker shade of gray than B. But what if I told you that A and B were actually the same color?

37
00:03:56,640 --> 00:04:02,160
It's okay if you don't see that. This picture is designed to trick our visual system. But once

38
00:04:02,160 --> 00:04:07,760
we have a connector of the common color between the two squares it's much easier for us to see

39
00:04:07,760 --> 00:04:14,480
that they are in fact the same color. So what's going on here? Over the years scientists have

40
00:04:14,480 --> 00:04:20,000
developed a human visual system model through the study of our eyes and one incredibly interesting

41
00:04:20,000 --> 00:04:25,840
finding through experimentation is that our eyes are much more sensitive to brightness than they

42
00:04:25,840 --> 00:04:32,960
are to colors. And part of the JPEG compression scheme can take advantage of this. But to understand

43
00:04:32,960 --> 00:04:40,880
how we have to dive into the world of color spaces. As we've discussed the RGB color space is a

44
00:04:40,880 --> 00:04:46,400
combination of red, green, and blue color components. If we put each value on a separate

45
00:04:46,400 --> 00:04:52,320
axis in a three-dimensional space we can see how all the possible colors are just a point on this

46
00:04:52,320 --> 00:04:58,880
cube. One aspect of RGB color space is as you progress on the diagonal from the origin to the

47
00:04:58,880 --> 00:05:07,440
color 255, 255, 255 you get gradually brighter colors. And in fact the exact line between these

48
00:05:07,440 --> 00:05:12,400
points defines all possible gray scale colors which are a direct measure for brightness.

49
00:05:13,120 --> 00:05:20,720
This idea of separating brightness is core to another color space called YCBCR. YCBCR

50
00:05:20,720 --> 00:05:26,640
stands for Y, chroma blue, and chroma red. Our Y component is going to measure the luma or brightness

51
00:05:26,640 --> 00:05:32,720
of an image and our CB and CR components are going to encode the colors. If we look at the

52
00:05:32,720 --> 00:05:38,800
color space the Y can be thought of as a single vertical axis with larger values encoding more

53
00:05:38,800 --> 00:05:46,400
brightness. Every cross section of the space defines a range of colors at that particular

54
00:05:46,400 --> 00:05:53,040
brightness. For our purposes with JPEG using the color space gives us direct access to the part

55
00:05:53,040 --> 00:05:59,600
of color that our eyes perceive best. As a result of being more sensitive to brightness than colors

56
00:05:59,600 --> 00:06:06,480
one idea to compress our original image involves sampling less of the CBCR components and keeping

57
00:06:06,480 --> 00:06:12,480
all of the luma components. The technique is referred to as chroma down sampling or more commonly

58
00:06:12,480 --> 00:06:20,800
chroma subsampling. Suppose I have this 8x8 image which has the following Y, CB, and CR components.

59
00:06:20,800 --> 00:06:26,640
The key idea of chroma down sampling or subsampling is to take fewer samples from CB

60
00:06:26,640 --> 00:06:31,600
and CR components since their eyes are less sensitive to them. Here's one approach that

61
00:06:31,600 --> 00:06:40,800
defines a 4-2-0 chroma down sampling scheme. We go through our original 8x8 image in 2x2 blocks

62
00:06:40,800 --> 00:06:46,480
and simply average the group of pixels to get a shared value of the four pixels in the original

63
00:06:46,480 --> 00:06:51,360
image. Averaging the pixels by the way is all down sampling really means.

64
00:06:53,600 --> 00:06:59,600
Chroma subsampling is the same exact idea but instead of averaging we just choose one of the

65
00:06:59,600 --> 00:07:04,880
samples usually the top left pixel to be the color of the entire 2x2 block.

66
00:07:09,120 --> 00:07:14,640
Once we have these fewer samples from the color components we can merge them with the luma component

67
00:07:14,640 --> 00:07:20,400
which will retain the original 16 pixels and this gives us our subsampled image.

68
00:07:20,400 --> 00:07:26,640
In this case you can see quite a difference since our 8x8 pixel image is significantly scaled up

69
00:07:26,720 --> 00:07:31,920
but in real world images it's often hard to see any changes after subsampling.

70
00:07:33,200 --> 00:07:39,360
By merging 2x2 blocks on the CB and CR channels into one color we are left with a quarter of the

71
00:07:39,360 --> 00:07:46,640
original data in each color channel shrinking the total file size by 50%. We're still quite far from

72
00:07:46,640 --> 00:07:53,200
the 5% levels that we saw in JPEG so we're going to have to exploit more than just human perception

73
00:07:53,200 --> 00:08:00,480
of brightness. For the following components of JPEG let's focus on the Y channel which

74
00:08:00,480 --> 00:08:06,000
essentially defines grayscale images. The principles we'll discuss from here on out

75
00:08:06,000 --> 00:08:12,560
will also apply to the color components of an image. The next clever idea in JPEG requires

76
00:08:12,560 --> 00:08:17,520
looking at images in a completely different perspective, one that can be a little bit

77
00:08:17,520 --> 00:08:24,480
counterintuitive. One way to think about images is treating them as signals. If I slice a particular

78
00:08:24,480 --> 00:08:31,280
row of an image I essentially have a row of pixels each with some value between 0 and 255.

79
00:08:32,240 --> 00:08:38,800
If we plot these values we can get an approximation of a signal. Visualizing an image as a signal

80
00:08:38,800 --> 00:08:44,320
allows us to talk about frequency components within an image. Higher frequency components

81
00:08:44,320 --> 00:08:50,400
correspond to rapid changes between pixels while lower frequency components are related to smoother

82
00:08:50,400 --> 00:08:56,640
changes between pixels. There are two key aspects of frequencies within images that are incredibly

83
00:08:56,640 --> 00:09:02,800
important to JPEG compression. The first is that a lot of real world images shot from cameras are

84
00:09:02,800 --> 00:09:08,640
mostly composed of lower frequency components. In other words if I take a random portion of a

85
00:09:08,640 --> 00:09:14,160
realistic image it's pretty likely that the pixels in that area do not change that rapidly.

86
00:09:14,960 --> 00:09:21,120
And the second key fact is that from a variety of experiments the human visual system is generally

87
00:09:21,120 --> 00:09:27,680
less sensitive to higher frequency detail in images. JPEG takes advantage of these ideas by

88
00:09:27,680 --> 00:09:33,760
strategically removing less important and less common higher frequency components from an image

89
00:09:33,760 --> 00:09:41,760
to achieve even more compression. But there's one big problem. How do we get frequency components

90
00:09:41,760 --> 00:09:47,760
from an image? This is where some particularly clever and beautiful math comes into play.

91
00:09:47,760 --> 00:09:53,920
The answer to this question lies in a special operation called the discrete cosine transform

92
00:09:53,920 --> 00:10:01,920
or the DCT. The DCT works for any size input but to simplify things let's focus on an input

93
00:10:01,920 --> 00:10:08,720
of eight pixels. Just as we did earlier let's suppose these eight pixels form some sort of signal.

94
00:10:09,440 --> 00:10:14,480
We'll never be sure what exactly the signal looks like since we only have eight points

95
00:10:15,040 --> 00:10:22,400
but the clever and definitely not obvious idea of the DCT is to represent these eight points as

96
00:10:22,400 --> 00:10:29,360
sums of sampled points from cosine waves. And I really want to emphasize the fact that we only

97
00:10:29,360 --> 00:10:35,760
care about the discrete samples. Visually I think it's nice to see the continuous signals and cosine

98
00:10:35,760 --> 00:10:42,000
waves but throughout a discussion the only values that really matter are the sampled points from

99
00:10:42,000 --> 00:10:49,680
these functions. The DCT takes an input of sampled points from our original signal and gives us an

100
00:10:49,680 --> 00:10:58,800
output of the same size. We'll refer to the outputs of the DCT as coefficients. These coefficients

101
00:10:58,800 --> 00:11:04,960
represent the weights of cosine waves of different frequencies that contribute to the original signal.

102
00:11:06,000 --> 00:11:12,800
A nice analogy is to think of this as unraveling a complex signal into a weighted sum of simpler

103
00:11:12,800 --> 00:11:19,440
cosine waves. If you've never interacted with this type of idea before it's natural to be confused.

104
00:11:20,560 --> 00:11:26,560
What cosine waves do we even use? How do cosine waves relate to pixels on an image?

105
00:11:27,520 --> 00:11:33,200
None of it makes any sense. Don't worry these are important questions that we will answer.

106
00:11:35,360 --> 00:11:42,400
Let's start simple and talk about cosine waves. Here's a graph of cosine from zero to pi. I've

107
00:11:42,400 --> 00:11:47,680
given you this general notion that the DCT is supposed to tell us how much of a specific

108
00:11:47,680 --> 00:11:55,600
cosine wave is contained in a signal. So let's test this out. What happens if I provide an

109
00:11:55,600 --> 00:12:01,520
actual cosine wave as the input signal to the DCT? What do we expect to happen?

110
00:12:02,720 --> 00:12:08,720
Okay we can try this but there's a problem. To follow our existing example we need eight

111
00:12:08,720 --> 00:12:14,960
sampled points from the cosine wave to make this work. How exactly should we sample the cosine wave?

112
00:12:15,600 --> 00:12:21,200
Well there are a few options but let me present to you the most common one. What we can do is

113
00:12:21,200 --> 00:12:27,760
split our cosine waves domain into eight even slices and then we take the midpoint of each of

114
00:12:27,760 --> 00:12:34,560
these slices. This gives us the following input points which we can generalize for any number of

115
00:12:34,560 --> 00:12:44,240
points. But for our purposes we'll stick with the smaller n equals 8 example. So going back to our

116
00:12:44,240 --> 00:12:50,480
question what should we expect the output to be when we pass in sampled points from a standard

117
00:12:50,480 --> 00:12:58,960
cosine function? This is an interesting experiment. When we pass these points from a cosine wave into

118
00:12:58,960 --> 00:13:06,800
a DCT transform we get the following output. Only one coefficient has a nonzero value meaning

119
00:13:06,800 --> 00:13:13,440
there's only one cosine wave that contributes to our input. And that seems to make some sense since

120
00:13:13,440 --> 00:13:20,400
the input is literally from a cosine wave. In this case the first index is the only coefficient

121
00:13:20,400 --> 00:13:27,040
with a nonzero value. When trying to understand complex ideas it really helps to play around

122
00:13:27,040 --> 00:13:32,560
with these simple examples. A cool follow-up to our experiment is to see what happens when

123
00:13:32,560 --> 00:13:39,440
we change the amplitude of this cosine wave. The first index DCT coefficient increases.

124
00:13:40,080 --> 00:13:45,600
If we flip the cosine wave by multiplying negative one the DCT coefficient changes sign.

125
00:13:46,560 --> 00:13:53,280
It's exactly acting like a weight for a cosine wave. When the amplitude of the input cosine

126
00:13:53,280 --> 00:14:01,520
wave changes the weight correspondingly reflects that change. So taking a step back

127
00:14:01,520 --> 00:14:08,480
how does this relate to images? Well just as we took images and represented them as signals

128
00:14:08,480 --> 00:14:15,920
the reverse also works. Standard grayscale images have pixels ranging from 0 to 255.

129
00:14:16,560 --> 00:14:22,320
The intuition with cosine waves to images makes more sense when we shift the range of pixel values

130
00:14:22,320 --> 00:14:30,160
by 128. With pixel values from negative 128 to 127 we can see a better mapping between this

131
00:14:30,160 --> 00:14:36,640
cosine wave to an actual set of eight pixels. This particular wave is a nice way to represent

132
00:14:36,640 --> 00:14:43,120
a row of gradually decreasing pixel values. And the magnitude of that change as well as the

133
00:14:43,120 --> 00:14:49,840
direction of the change is reflected in the amplitude of the original cosine wave and consequently

134
00:14:49,840 --> 00:14:56,080
the DCT coefficient. So let's continue this experiment to see what else we can uncover

135
00:14:56,080 --> 00:15:01,920
about the DCT. We've messed with the amplitude of a cosine wave. What other parameters could we

136
00:15:01,920 --> 00:15:09,760
change? A simple one is to just shift the cosine wave up or down. Let's see what happens when we

137
00:15:09,760 --> 00:15:17,440
try that. It looks like shifting up or down the signal only affects the zeroth index coefficient.

138
00:15:18,320 --> 00:15:23,520
That's an interesting data point that we'll come back to. Another parameter of cosine waves

139
00:15:23,520 --> 00:15:29,840
is the frequency. What we're going to do now is show the DCT coefficients as we wind up the

140
00:15:29,840 --> 00:15:36,080
frequency of this cosine wave. I'll keep the sampling strategy we discussed earlier consistent

141
00:15:36,080 --> 00:15:43,040
among all frequencies. Let's see what happens. As we increase the frequencies we get a few

142
00:15:43,040 --> 00:15:49,920
different DCT coefficients for the respective cosine wave. That is until we get to this cosine wave.

143
00:15:50,560 --> 00:15:55,840
For this particular cosine wave only the second index has a non-zero coefficient.

144
00:15:55,840 --> 00:16:02,880
This cosine wave is actually just double the frequency of the previous cosine wave. This is

145
00:16:02,880 --> 00:16:09,760
super interesting. The first index of the output seems to nicely correspond with the cosine wave

146
00:16:09,760 --> 00:16:15,760
of frequency one while the second index correlates with a cosine wave of frequency two.

147
00:16:17,280 --> 00:16:22,480
Let's continue this experiment of increasing frequencies but before I continue see if you

148
00:16:22,480 --> 00:16:27,840
can take a guess at what frequencies the other coefficients will correspond to. Here we go.

149
00:16:28,400 --> 00:16:34,800
We slowly increase the frequency and boom the index three coefficient corresponds to a cosine

150
00:16:34,800 --> 00:16:42,800
wave of frequency three. Then frequency four comes next and this pattern continues until we get to a

151
00:16:42,800 --> 00:16:51,360
cosine wave of frequency seven. Pretty insane right? So for the coefficients indexed one to seven

152
00:16:51,360 --> 00:16:58,000
it looks like they represent the weight on a cosine wave with the frequency that matches the index.

153
00:17:00,000 --> 00:17:05,840
So what about the remaining index zero? We saw shifting cosine waves up and down led to a change

154
00:17:05,840 --> 00:17:12,800
in the zeroth index. What cosine wave does that represent? Some of you have probably figured it

155
00:17:12,800 --> 00:17:18,880
out but if you think about what a zero frequency cosine wave is it's just a constant signal.

156
00:17:19,840 --> 00:17:25,600
What that means in terms of images is it gives us a measure of the overall brightness of a set of

157
00:17:25,600 --> 00:17:33,760
pixels. Brighter images will have a larger zeroth coefficient than darker images. This is why shifting

158
00:17:33,760 --> 00:17:40,640
up a cosine wave only impacts the zeroth coefficient. Putting this all together each

159
00:17:40,640 --> 00:17:46,880
of these frequencies correspond to a different pattern of images and what the core DCT does

160
00:17:46,880 --> 00:17:52,320
is break down how each of these fundamental patterns contribute to the original image.

161
00:17:53,600 --> 00:18:00,400
And it turns out that all possible combinations of eight pixel values can be represented as a sum

162
00:18:00,400 --> 00:18:07,360
of these eight cosine waves. Why that's true is not at all obvious but we can begin to understand it

163
00:18:07,360 --> 00:18:14,960
once we translate this intuition to the actual math behind the DCT. If you look at the mathematical

164
00:18:14,960 --> 00:18:21,520
definition of the DCT we usually have a vector definition of the original signal and the output

165
00:18:21,520 --> 00:18:29,200
coefficients. We want to define the kth index of the coefficient vector mathematically. What you

166
00:18:29,200 --> 00:18:35,360
often see is something that looks like the following and with the intuition that we just built up

167
00:18:35,360 --> 00:18:41,840
we'll see that this equation is doing exactly what we want. Let's start with the cosine term.

168
00:18:41,840 --> 00:18:48,000
This function should be familiar. It's the exact representation of a sampled point from a cosine

169
00:18:48,000 --> 00:18:53,120
wave using our earlier sampling scheme and it incorporates the frequency of the cosine wave

170
00:18:53,120 --> 00:19:00,400
as well. Now what's interesting is in order to get the kth index we are actually summing over a

171
00:19:00,400 --> 00:19:06,880
product of each sampled point with samples from the cosine wave. Why does that make sense?

172
00:19:07,840 --> 00:19:12,640
This type of expression might look vaguely familiar to a lot of you. Let me rewrite this

173
00:19:12,640 --> 00:19:18,480
another way. We know that the original signal points can be represented as a vector but what

174
00:19:18,480 --> 00:19:24,800
if we rewrote the sampled points from the cosine wave as a vector as well? What does this expression

175
00:19:24,800 --> 00:19:32,320
mean in the context of these two vectors? It's a dot product and what we know about dot products is

176
00:19:32,400 --> 00:19:39,520
there a nice way to measure similarity between two vectors. That's why when we pass in sampled

177
00:19:39,520 --> 00:19:47,200
points from a cosine wave of frequency k as the input to the DCT we got large values at the kth

178
00:19:47,200 --> 00:19:53,840
index coefficient. These two vectors were just scaled versions of each other so the dot product

179
00:19:53,840 --> 00:20:00,080
was maximized and this perspective reveals what I think is truly the most surprising and elegant

180
00:20:00,080 --> 00:20:07,280
part of the DCT. By picking the points through the sampling method we can think of the entire DCT

181
00:20:07,280 --> 00:20:14,080
as a matrix vector product. All we're doing here is a linear transformation. The rows of the matrix

182
00:20:14,080 --> 00:20:19,280
are the sampled points from the cosine waves of the respective frequencies and what's truly

183
00:20:19,280 --> 00:20:25,680
astounding is that all row vectors in this matrix are orthogonal to each other. What I mean by that

184
00:20:25,680 --> 00:20:31,920
is if you take the dot product of any two row vectors representing cosine waves you will get

185
00:20:31,920 --> 00:20:38,880
zero if they are different rows of the matrix. Intuitively this is why in our earlier experiments

186
00:20:38,880 --> 00:20:46,000
when we pass in a cosine wave of a particular frequency as an input into the DCT we didn't get

187
00:20:46,000 --> 00:20:52,240
a contribution from any of the other coefficients which represented different frequency cosine waves.

188
00:20:52,880 --> 00:20:58,800
The orthogonality of the sampled points from different cosine waves generates this behavior.

189
00:20:59,360 --> 00:21:05,600
It's really quite beautiful. Another great property of the DCT that follows from these

190
00:21:05,600 --> 00:21:12,320
facts is invertibility. I've talked about the DCT as a way of decomposing a signal into a

191
00:21:12,320 --> 00:21:18,880
coefficient representation of weights associated with cosine waves. We can also reverse this process.

192
00:21:19,680 --> 00:21:25,040
If I take my coefficient representation of the signal I can apply what's called the inverse

193
00:21:25,040 --> 00:21:33,440
DCT to get back the original signal and it is the exact same signal. No information is lost

194
00:21:33,440 --> 00:21:40,080
in this step. How we do that is by multiplying our coefficient representation with the inverse

195
00:21:40,080 --> 00:21:46,640
of the matrix. What's cool about this is that because of the orthogonality of the vectors

196
00:21:46,640 --> 00:21:52,960
the inverse is just the transpose of our original matrix with some additional normalization constants.

197
00:21:54,800 --> 00:22:01,200
Now there's a super nice interpretation of the inverse DCT. The sample cosine wave points are

198
00:22:01,200 --> 00:22:07,200
now column vectors so what the inverse DCT is doing is essentially summing over a weighted

199
00:22:07,200 --> 00:22:13,600
combination of cosine waves directly to get the original signal. And because these columns are

200
00:22:13,600 --> 00:22:19,360
orthogonal to each other that's what allows us to represent any set of eight points with these

201
00:22:19,360 --> 00:22:25,680
eight cosine waves. Absolutely incredible. I know we spent some time and went through some fairly

202
00:22:25,680 --> 00:22:31,360
complex math to get here but it's precisely these details that are the most fundamental part of not

203
00:22:31,360 --> 00:22:39,040
only the DCT but many other similar transforms in the world of signal processing. Now that we have

204
00:22:39,040 --> 00:22:45,440
a good intuition on the one-dimensional DCT let's talk about how JPEG specifically uses it.

205
00:22:46,480 --> 00:22:51,840
JPEG takes an image and splits it into eight by eight blocks and then centers their values

206
00:22:51,840 --> 00:23:01,040
around zero by subtracting 128. Then we take the block and apply the DCT to each row of the block

207
00:23:01,040 --> 00:23:09,200
giving us eight sets of DCT coefficients. We then apply the DCT to each column of the block.

208
00:23:13,280 --> 00:23:16,880
This process is what defines the two-dimensional DCT.

209
00:23:19,120 --> 00:23:25,760
So in the end we have 64 coefficients each of which are await on a specific eight by eight pattern.

210
00:23:26,400 --> 00:23:32,000
Notice the first row and column correspond to the earlier one-dimensional patterns and the other

211
00:23:32,000 --> 00:23:39,040
elements are compositions of these patterns. And just like in the one-dimensional case the big idea

212
00:23:39,040 --> 00:23:46,960
here is that we can build up any eight by eight image using these 64 fundamental patterns. The

213
00:23:46,960 --> 00:23:52,960
same signal perspective we talked about earlier also applies here except now with 2D waveforms.

214
00:23:52,960 --> 00:23:58,480
What's going on here is we are plotting the pixel value on the z-axis with brighter pixels having

215
00:23:58,480 --> 00:24:05,200
larger values. What's fun to play around with is seeing how the waveform and image come together

216
00:24:05,200 --> 00:24:13,200
as we slowly put together the 64 coefficients in increasing frequencies. Seeing this in action

217
00:24:13,200 --> 00:24:18,640
makes you realize that one interesting property is that by the time we incorporate a small portion

218
00:24:18,720 --> 00:24:23,920
of the coefficients our signal and image already look pretty close to the original versions.

219
00:24:25,360 --> 00:24:30,080
There's an even more direct experiment we can run to quantify this notion.

220
00:24:31,760 --> 00:24:36,480
This particular eight by eight block was randomly picked out of the original image.

221
00:24:37,200 --> 00:24:42,560
If we map out the magnitude of the DCT coefficients on this block we see that most

222
00:24:42,560 --> 00:24:46,400
of the largest values are in the upper left section which corresponds to the original

223
00:24:46,560 --> 00:24:52,240
upper left section which corresponds to lower frequency components. And what's even more

224
00:24:52,240 --> 00:24:58,560
interesting if I take any eight by eight block on this image almost all of them have the same

225
00:24:58,560 --> 00:25:04,800
property. This property of the DCT is what's commonly referred to as energy compaction.

226
00:25:06,240 --> 00:25:12,320
After applying the DCT most of the largest values are concentrated in a few low frequency

227
00:25:12,320 --> 00:25:19,040
coefficients and this holds true in a lot of real world images. The concept of energy compaction

228
00:25:19,040 --> 00:25:24,880
is incredibly important in image compression. As we will see it's exactly the property that

229
00:25:24,880 --> 00:25:30,400
will allow us to aggressively compress images while still retaining high visual quality.

230
00:25:33,280 --> 00:25:39,280
Fun fact the original discovery the DCT centered around approximating other transforms

231
00:25:39,280 --> 00:25:45,840
that had better energy compaction properties but were too expensive to carry out. The DCT is just

232
00:25:45,840 --> 00:25:51,120
one example of a transform that has this property for real world images and we use it because it's

233
00:25:51,120 --> 00:25:57,920
quite easy to compute. There's a lot of complexity involved here but one of my goals in this discussion

234
00:25:57,920 --> 00:26:03,600
of the DCT and JPEG was directly interacting with these deep and important ideas through

235
00:26:03,600 --> 00:26:10,000
questions and visual experiments. Interactivity is a core part of learning and a website that

236
00:26:10,000 --> 00:26:16,560
does a fantastic job of interactive explanations is Brilliant the sponsor for this video. From the

237
00:26:16,560 --> 00:26:21,200
basics of mathematics and algorithmic thinking to more complex ideas and deep learning and

238
00:26:21,200 --> 00:26:26,480
probability Brilliant offers a variety of courses and learning paths for those interested in getting

239
00:26:26,480 --> 00:26:32,000
hands on practice. Our discussions of JPEG interacted with some linear algebra in the

240
00:26:32,000 --> 00:26:37,680
application of image compression and Brilliant has an entire linear algebra module that goes

241
00:26:37,680 --> 00:26:42,400
through the fundamentals and even shows applications of these ideas in image compression,

242
00:26:42,400 --> 00:26:49,120
cryptography, error correcting codes and much more. When I was a student I really enjoyed their

243
00:26:49,120 --> 00:26:53,920
computer science fundamentals course which has engaging visualizations of concepts and great

244
00:26:53,920 --> 00:26:58,800
practice problems that helped me solidify my foundations. You can get started for free by

245
00:26:58,800 --> 00:27:04,720
going to brilliant.org slash reducible which is linked in the description below. Brilliant is

246
00:27:04,720 --> 00:27:09,840
providing a special offer through this channel where the first 200 members to sign up get 20%

247
00:27:09,840 --> 00:27:14,960
off the annual subscription. It's a great way to learn more about the topics in these videos

248
00:27:14,960 --> 00:27:20,480
and also a good way to support this channel. Big thanks to Brilliant for sponsoring this video.

249
00:27:22,720 --> 00:27:27,360
Let's put everything we've discussed with the DCT together in one more experiment.

250
00:27:28,160 --> 00:27:32,800
We'll split our image into eight by eight blocks and then basically rebuild the image

251
00:27:32,800 --> 00:27:39,440
with each block having only a certain number of DCT coefficients. We're going to start off with

252
00:27:39,440 --> 00:27:46,720
zero coefficients and slowly build up the image. After one coefficient we end up with basically a

253
00:27:46,720 --> 00:27:54,080
blur of the original image and as we add DCT coefficients slowly notice how quickly the

254
00:27:54,080 --> 00:28:01,600
image starts looking like the original. By the time we get to less than 25% of the DCT coefficients

255
00:28:01,600 --> 00:28:07,680
you almost can't even tell the difference between the two images. This confirms the key aspects of

256
00:28:07,680 --> 00:28:14,480
why JPEG works for this particular image. Almost all the blocks are composed of the lowest frequency

257
00:28:14,480 --> 00:28:20,800
components and we are generally less sensitive to changes in high frequency details. So at this

258
00:28:20,800 --> 00:28:26,400
point we know we can eliminate higher frequency components from the DCT but the next natural

259
00:28:26,400 --> 00:28:33,520
question is how we actually do this. The process for eliminating higher frequency components in JPEG

260
00:28:33,520 --> 00:28:41,280
is called quantization. Quantization is a simple idea. Given an eight by eight matrix of frequency

261
00:28:41,280 --> 00:28:47,920
coefficients from the DCT what we're going to do is basically divide each element by a scalar value

262
00:28:47,920 --> 00:28:54,720
and round it to an integer. These values are defined in terms of a quantization table. Notice

263
00:28:54,720 --> 00:28:59,840
larger values in the bottom right of the table leading to zero values in the higher frequency

264
00:28:59,840 --> 00:29:07,520
components. In the decoding stage of JPEG we'll actually be multiplying this result by the same

265
00:29:07,520 --> 00:29:14,480
quantization matrix element by element and as you can see the final coefficient matrix will be quite

266
00:29:14,480 --> 00:29:20,320
different from the original one. So what that means is we're purposely losing information in this step

267
00:29:21,600 --> 00:29:27,600
but the key idea here is most of the lower frequency components will be retained. This is

268
00:29:27,600 --> 00:29:34,400
why the energy compaction property of the DCT is so useful when the largest values lie in the lowest

269
00:29:34,400 --> 00:29:40,240
frequencies we will end up with a lot of zeros in the less important high frequency components.

270
00:29:40,480 --> 00:29:47,760
These quantization tables are provided by the JPEG standard from visual experiments

271
00:29:47,760 --> 00:29:54,640
and are the main way for JPEG to define quality of compression. High quality compression parameters

272
00:29:54,640 --> 00:30:01,760
can be translated to lower quantization table values. In practice JPEG also defines a separate

273
00:30:01,760 --> 00:30:08,800
quantization table for both the luma and color channels. Notice that in the color channels

274
00:30:08,880 --> 00:30:16,640
quantization can be even more aggressive. After performing quantization we have a matrix of quantized

275
00:30:16,640 --> 00:30:22,160
DCT coefficients where we can now exploit redundancy to get even more compression.

276
00:30:24,000 --> 00:30:30,160
The last part of JPEG encoding involves a combination of run length encoding and Huffman

277
00:30:30,160 --> 00:30:36,800
encoding. One clever trick is that a JPEG encoder will order the coefficients in a zigzag manner

278
00:30:36,800 --> 00:30:43,360
to maximize the chance of a large sequence of zeros in order. Classic run length encoding can

279
00:30:43,360 --> 00:30:49,600
compress this fairly easily. All that's going on here is we are compressing every sequence of zeros

280
00:30:49,600 --> 00:30:55,760
into a count of the occurrences in a continuous sequence. JPEG actually performs something a

281
00:30:55,760 --> 00:31:01,920
little bit more sophisticated by keeping track of a triplet. For every coefficient this triplet

282
00:31:01,920 --> 00:31:07,680
encodes the number of preceding zeros, the number of bits required to encode the coefficient,

283
00:31:07,680 --> 00:31:14,080
and finally the actual coefficient value. We also have an end of block value to signal that

284
00:31:14,080 --> 00:31:23,120
everything from here on out will be zeros. This particular scheme works well with Huffman

285
00:31:23,120 --> 00:31:30,160
coding to further exploit redundancy. The big idea of Huffman codes is that more frequently

286
00:31:30,240 --> 00:31:37,040
used data can be encoded with fewer bits, and it turns out especially with the nature of quantization,

287
00:31:37,040 --> 00:31:42,240
these triplets can be further compressed since some of these values will be more frequent than others.

288
00:31:44,000 --> 00:31:49,440
However, I'm purposefully not going to go into the details of how JPEG uses Huffman codes to

289
00:31:49,440 --> 00:31:55,280
compress the data because it really does get quite tricky. To give you some sense of the problems,

290
00:31:55,280 --> 00:32:00,320
we have to deal with the encoding signs of coefficients as well as triplets for all eight

291
00:32:00,320 --> 00:32:07,120
by eight blocks. Most encoders also encode the top left coefficient separate from all the other

292
00:32:07,120 --> 00:32:13,360
coefficients. And when you handle that, you have to deal with this on both luma and color channels.

293
00:32:14,320 --> 00:32:18,720
And when you eventually get that working, a good chunk of your logic will break when you

294
00:32:18,720 --> 00:32:24,480
have to deal with the different types of chroma subsampling. Implementing an optimized fully

295
00:32:24,480 --> 00:32:30,640
functional JPEG encoder and decoder is no joke. I wouldn't give that task to even my worst enemies.

296
00:32:31,840 --> 00:32:36,720
But in terms of the big picture, all that's going on in this component is taking advantage

297
00:32:36,720 --> 00:32:43,920
of the redundancy that quantization creates. A JPEG decoder will be able to use the Huffman code

298
00:32:43,920 --> 00:32:50,720
data in the files to get back all quantized DCT coefficients that were encoded. This part

299
00:32:50,720 --> 00:32:59,600
of the JPEG algorithm does not lose any information. JPEG as a whole brings about an interesting

300
00:32:59,600 --> 00:33:05,600
discussion on the philosophy of data compression. The classic and most straightforward way to

301
00:33:05,600 --> 00:33:12,240
compress data is by taking advantage of redundancy. This is the basis of losses image compression

302
00:33:12,240 --> 00:33:18,880
algorithms such as those found in PNG file formats. In fact, for images where it's really important

303
00:33:18,880 --> 00:33:25,680
not to lose any information, PNG format is recommended over JPEG. But on most real world

304
00:33:25,680 --> 00:33:31,520
images, being aware of the medium of presentation introduces another really powerful perspective.

305
00:33:32,640 --> 00:33:37,760
A lot of innovation in JPEG compression comes from experiments and understanding of human

306
00:33:37,760 --> 00:33:43,440
visual systems. It's from these experiments that we realized human eyes are less sensitive to color

307
00:33:43,440 --> 00:33:48,560
and also less sensitive to higher frequencies. So we can remove that information without a

308
00:33:48,560 --> 00:33:56,080
significant visual impact. This is why JPEG is so much more effective at compressing images than

309
00:33:56,080 --> 00:34:02,560
lossless formats. You'll find these same types of techniques used in audio and video compression

310
00:34:02,560 --> 00:34:08,640
where algorithms use our perceptions of sound and motion respectively to remove less relevant data.

311
00:34:09,520 --> 00:34:14,560
In fact, variations of the discrete cosine transform and quantization show up in both

312
00:34:14,560 --> 00:34:21,440
audio and video compression. It really is incredible to me how people in these fields came up with the

313
00:34:21,440 --> 00:34:27,520
mathematical and algorithmic framework to utilize the way we actually perceive the digital technology

314
00:34:27,520 --> 00:34:33,760
around us. There's so much depth to these topics that I can never hope to cover in just one video,

315
00:34:33,760 --> 00:34:38,960
but I do hope this gives you a sense and appreciation for the complexity of the technology

316
00:34:38,960 --> 00:34:47,520
around us that we use on a daily basis. Thanks for watching and I'll see you all in the next one.

