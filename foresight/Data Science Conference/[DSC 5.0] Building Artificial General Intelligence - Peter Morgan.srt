1
00:00:00,000 --> 00:00:14,560
So, hi everybody, as a guy gentleman just said, I'm the CEO of a consulting company

2
00:00:14,560 --> 00:00:19,520
in London called Deep Learning Partnership and I'm going to talk to you today about

3
00:00:19,520 --> 00:00:24,880
something I don't really do on my day-to-day job, it doesn't pay my rent, but it affords

4
00:00:24,880 --> 00:00:34,800
me nice trips to exotic places. So, who here has heard of artificial general intelligence?

5
00:00:35,840 --> 00:00:43,520
Yeah, so quite a few. And so, what we label today is AI, isn't really AI, it's matrix

6
00:00:43,520 --> 00:00:48,240
multiplication. So, I'm going to talk to you today about how we might actually get to AGI

7
00:00:49,120 --> 00:00:54,320
and it's more of a neuroscience based approach. It's how the brain works, it's how biology does it.

8
00:00:55,040 --> 00:00:59,840
So, it's okay, everything that we've heard the speakers talk about so far,

9
00:01:01,760 --> 00:01:06,160
which has been labeled AI, it's really deep learning or artificial neural networks

10
00:01:06,160 --> 00:01:11,520
and the early creators of that said this is nothing to do with how the brain works, right?

11
00:01:11,520 --> 00:01:17,600
It's just it kind of works to classify cats and dogs and that's about it. We do a lot more than

12
00:01:17,600 --> 00:01:23,920
that. So, how do we get there? Is it even possible to build an artificial brain basically

13
00:01:23,920 --> 00:01:29,840
how biology does it? And I'm here to tell you, yes it is, I'll just give it away,

14
00:01:30,480 --> 00:01:36,320
you can go now, but if you're interested, I'll walk you through how that might look and how that

15
00:01:36,320 --> 00:01:42,720
might be done. Okay, so here's a little book I wrote for Riley, it's a booklet. There's my

16
00:01:42,720 --> 00:01:48,160
company website, there's my Twitter handle, you can follow me, I tweet a little bit about AGI and

17
00:01:48,240 --> 00:01:54,320
sometimes quantum computing, my other hobby or interest. So, this is what we're going to

18
00:01:54,320 --> 00:02:00,560
outline just briefly. We're going to look at physical systems, how biology does intelligence,

19
00:02:00,560 --> 00:02:07,280
and where we are with the non-biological, I've kind of just explained that in a broad sense,

20
00:02:07,280 --> 00:02:14,880
but we'll look at some of the, well, we'll go through the slides, deep learning recap and then

21
00:02:14,880 --> 00:02:21,520
we'll look at how we might build AGI and some of the theories and in particular one called

22
00:02:21,520 --> 00:02:29,760
active inference that I find particularly compelling to how the brain might work. Okay, so at that

23
00:02:29,760 --> 00:02:34,560
point it's sort of theoretical neuroscience, so I hope I don't lose you. I try to keep this kind of

24
00:02:34,560 --> 00:02:39,760
high level and not too much code or anything, although there is code and math for this stuff.

25
00:02:40,400 --> 00:02:48,080
And then we'll wrap it up by saying is it possible, how long might it take? Okay, so why do we even

26
00:02:48,080 --> 00:02:56,640
want to build AGI? Why don't we just stop here with the cats and dogs and TensorFlow? Well,

27
00:02:56,640 --> 00:03:01,680
if we do, like DeepMind says, if we can solve intelligence and we can use it to solve everything

28
00:03:01,680 --> 00:03:07,040
else. I mean, we're quite good at stuff, we've got here today, we've had Einstein's theory of

29
00:03:07,040 --> 00:03:14,160
relativity, we've had quantum physics, we've had classical physics, Newton, all the big names,

30
00:03:14,160 --> 00:03:20,880
why do we need any help? Well, clearly if we do manage to figure this out and build an artificially

31
00:03:20,880 --> 00:03:28,160
general intelligent system, then we can accelerate progress, which is probably, we can all agree,

32
00:03:28,160 --> 00:03:34,400
is a good thing. Although some people like Elon must say that, let's not build it because it will

33
00:03:34,400 --> 00:03:44,560
straight away kill us all, right? Well, we don't know, but I don't think it will. Okay, so I'm

34
00:03:44,560 --> 00:03:50,800
the anti-Elon Musk, okay? I'm the opposite. Okay, so I'm an optimist, he's a pessimist.

35
00:03:51,440 --> 00:03:57,040
So he's a good entrepreneur anyway. So what about all of these things here? AlphaGo,

36
00:03:58,000 --> 00:04:05,680
this is DeepMind in London, I'm sure we all saw on TV, AlphaGo beating the best go player in the

37
00:04:05,680 --> 00:04:16,720
world, was that AI, was that AGI? AlphaStar, it's a new project to beat the world's best video game

38
00:04:16,720 --> 00:04:21,920
players, a little bit more general one might argue than just playing a board game. DeepBlue,

39
00:04:21,920 --> 00:04:27,680
IBM's DeepBlue and Chess, Landmark, and IBM Watson, that's actually natural language processing,

40
00:04:27,680 --> 00:04:32,800
so that's quite clever, that's quite getting a little closer to AGI maybe, because it's having

41
00:04:32,800 --> 00:04:38,160
to understand language, but was it just doing it statistically? Was it just a massive computer

42
00:04:39,120 --> 00:04:45,680
hardware system that could crunch numbers really well and just do everything statistically? Is

43
00:04:45,760 --> 00:04:53,200
it how we learn language and understand language? I would argue no. So actually, all of those are

44
00:04:53,200 --> 00:04:59,920
just narrow AI, they're just statistical mathematical analysis of big data sets, okay? So it's not really

45
00:04:59,920 --> 00:05:07,120
how the brain does anything. If it were, I mean we do all of that in that little three pound thing

46
00:05:07,120 --> 00:05:12,880
above our sitting, above our shoulders, right? We don't need huge data centers, so clearly biology

47
00:05:12,880 --> 00:05:18,480
is doing it in a slightly different way, okay? So how is it doing it and how is it different?

48
00:05:19,200 --> 00:05:23,600
So first of all, to build something, we have to understand what we're building. What is

49
00:05:23,600 --> 00:05:28,960
intelligence, general intelligence? Well, it's all these things, right? So we don't just classify

50
00:05:28,960 --> 00:05:35,840
things, we do all of that. I won't read them out, because I've only got 30 minutes and sometimes

51
00:05:35,840 --> 00:05:40,640
I give this talk, it's an hour, but there's basically nine types of intelligence there,

52
00:05:41,360 --> 00:05:46,480
maybe even 10. And this guy at Harvard, Howard Gardner, just mapped it all out. See,

53
00:05:47,040 --> 00:05:56,320
existential thinking language, we can do math using ANNs, but the rest of it is pretty much

54
00:05:56,320 --> 00:06:03,360
music, creating art, spatial navigation, that's all. We can do all of that in three pound. We

55
00:06:03,360 --> 00:06:09,200
don't need a huge data center, hardware cluster of GPUs. So what's different? What are we missing?

56
00:06:09,280 --> 00:06:14,400
Okay. Well, first of all, how far have we come? Like I said, the mathematical part,

57
00:06:14,400 --> 00:06:18,240
we're like 50%, we're pretty good, we're not bad, we're about halfway there, linguistic,

58
00:06:18,240 --> 00:06:25,120
but it's only statistical analysis. Spatial navigation, self-driving cars, robots who can

59
00:06:25,120 --> 00:06:32,480
navigate through systems. I'd say we're about something called SLAM. I'd say we're about 50%

60
00:06:32,480 --> 00:06:41,440
there, that's the algorithm, simultaneous mapping and I forgot what the LAM stands for,

61
00:06:41,440 --> 00:06:51,040
but don't worry. We're about 50% there. You've probably seen simultaneous localization and mapping.

62
00:06:51,040 --> 00:06:56,560
You've probably seen the Atlas, you know, robot that do flips and run around outside and do good

63
00:06:56,640 --> 00:07:04,880
stuff, busts and dynamics. So that's why I'm giving it a generous 50%. We do a lot more than that.

64
00:07:04,880 --> 00:07:10,960
We can do gymnastics, we can run around, we learn stuff very quickly, we don't need millions of

65
00:07:10,960 --> 00:07:16,400
datasets to train, but as we go through that list of nine types of intelligence, it gets really,

66
00:07:16,400 --> 00:07:24,560
it just gets worse, right? Robots understanding each other or even algorithms running on a

67
00:07:24,560 --> 00:07:30,160
computer virtual sort of intelligence. Intrapersonal understanding themselves and existential,

68
00:07:30,960 --> 00:07:36,560
you know, show me an algorithm or a robot that kind of questions why it's there,

69
00:07:36,560 --> 00:07:42,000
or even understands that it exists, it's kind of zero. So we haven't figured that intelligence

70
00:07:42,000 --> 00:07:50,720
out at all. So the brain is pretty special, right? Biology is kind of special. We could argue chimpanzees,

71
00:07:50,720 --> 00:07:56,720
self-aware, well, yes, right, a dog, cats, probably. And as you go, you know, snails,

72
00:07:56,720 --> 00:08:03,440
maybe not, you know, bacteria, no. So there's clearly a continuum in what makes us a little

73
00:08:03,440 --> 00:08:08,800
different is our neocortex, that sort of outer layer of the brain. So we kind of have to figure

74
00:08:08,800 --> 00:08:13,200
out how that works, basically. But we've got all this other stuff in the brain like the amygdala

75
00:08:13,200 --> 00:08:18,400
for emotional intelligence, you know, the reptilian brain and there's sort of layers, you can trace it.

76
00:08:18,400 --> 00:08:23,200
In fact, you know, as babies are born and embryos develop over the nine months,

77
00:08:24,640 --> 00:08:28,880
you can see, you know, evolution, it starts off, you know, with the, you know,

78
00:08:28,880 --> 00:08:36,720
reptilian brain and it goes out monkey and then, you know, humans as we develop in the womb. So,

79
00:08:36,720 --> 00:08:42,160
you know, it's all, all of that evolution is still captured, all that information is actually

80
00:08:42,160 --> 00:08:48,960
captured by biology. It's just that last layer of the neocortex that makes us be able to come up

81
00:08:48,960 --> 00:08:55,280
with, you know, science, special theory, relativity, whatever, you know, that, that. So what are we

82
00:08:55,280 --> 00:09:02,960
missing? Why can't we do that yet in TensorFlow? Okay, why can't Google or DeepMind do that yet?

83
00:09:02,960 --> 00:09:08,320
So how will we get there? Well, it takes a village to create a child and I'm going to argue it takes,

84
00:09:08,320 --> 00:09:11,920
it's going to take a big community to create AGI. We're going to need computer scientists,

85
00:09:12,800 --> 00:09:16,560
physicists to understand the physics of the brain, neuroscientists, clearly,

86
00:09:17,600 --> 00:09:22,800
and also psychologists. So what I'm seeing really at the moment where these teams trying to build

87
00:09:23,440 --> 00:09:27,840
AGI or AI are just computer scientists, you know, just computer scientists, you know,

88
00:09:27,840 --> 00:09:32,560
you're clever people, they have PhDs and computer science, which involves a lot of math and

89
00:09:32,560 --> 00:09:38,000
understanding hardware and algorithm, algorithmic complexity. But it's not really

90
00:09:38,000 --> 00:09:44,240
understanding how much about biology at all. Okay, so we're going to need a whole bunch of people.

91
00:09:44,800 --> 00:09:49,920
And so I'll speed up a little bit now. I've kind of laid the foundations for what we might need.

92
00:09:50,720 --> 00:09:56,960
So this is basically what we've got in hardware. So this is how biology does it in hardware. So,

93
00:09:56,960 --> 00:10:03,520
you know, like I mentioned, we have a continuum from bacteria. That's a simple, C elegance is the

94
00:10:03,520 --> 00:10:09,760
simplest biological system with a central nervous system, it has, I think, about 300 neurons, then

95
00:10:09,760 --> 00:10:14,880
the bumblebee, the humble bumblebee with about a million, a few million neurons, and then us with

96
00:10:14,880 --> 00:10:23,200
about 80 billion or 100 billion neurons. Okay, so what, you know, what basically, maybe if we

97
00:10:23,200 --> 00:10:28,160
start at the simple, the C elegance and then build a bumblebee, then maybe one day, you know, we'll

98
00:10:28,160 --> 00:10:35,120
get to us. That's one sort of thought. The other, the other observation really is that nature is

99
00:10:35,120 --> 00:10:41,280
very hierarchical. You know, we built neurons, built of, they're not magic, you know, the built

100
00:10:41,280 --> 00:10:47,280
of atoms, molecules. And then if we just go up the different layers, we have synapses, neurons,

101
00:10:47,280 --> 00:10:53,600
and collection of neurons as connectome. So these 80 billion neurons are organized into some kind of

102
00:10:53,680 --> 00:11:00,000
structure. Maybe, you know, we should start there. So the question here is, you know, what level do

103
00:11:00,000 --> 00:11:05,760
we start at? Do we have to start at molecules or can we start at neurons or should we start a little

104
00:11:05,760 --> 00:11:12,080
more holistically at the connectome? You know, and on and on up until the central nervous system,

105
00:11:12,080 --> 00:11:18,400
us, which is about a meter in dimensions. So we're going from 10 to the minus 10 up to a meter.

106
00:11:18,400 --> 00:11:22,960
So it's a hard problem. It's kind of no wonder we haven't solved it yet. There's a lot of physics

107
00:11:22,960 --> 00:11:30,640
in there. And so that's what it looks like. This is how nature has done it. It's complicated,

108
00:11:30,640 --> 00:11:36,480
isn't it? You know, I mean, do we need all that little tiny microstructure? Probably not, I might

109
00:11:36,480 --> 00:11:43,840
argue. So you know, this is how, you know, what do we for in a half billion years of evolution

110
00:11:43,840 --> 00:11:50,880
on the earth, maybe a billion since the first sort of organisms, bacteria, whatever, the first,

111
00:11:50,880 --> 00:11:56,400
you know, primitive life. So over that billion year, this is what this is what nature's ended

112
00:11:56,400 --> 00:12:02,480
up with, right? This is what makes us intelligent, these things, right? You couldn't guess that.

113
00:12:02,480 --> 00:12:07,520
You start with a blank piece of paper, but this is just the universe we live in, the laws of physics.

114
00:12:07,520 --> 00:12:13,040
This is this is how we work. Okay. So there's no magic, but there's certainly a lot of

115
00:12:13,040 --> 00:12:19,440
complication. It's a complicated looking thing. That's a neuron. Okay. And so the next level of

116
00:12:19,440 --> 00:12:24,000
the hierarchy, we see that these neurons are kind of organized into some kind of structure

117
00:12:24,000 --> 00:12:31,120
called cortical columns, which each have about two, two million neurons. So maybe this is a

118
00:12:31,120 --> 00:12:36,160
good level to start. Maybe we don't have to go right down to all that complicated structure,

119
00:12:36,160 --> 00:12:40,960
maybe here. So that's kind of an interesting thing that we know. And then up onto the connectome,

120
00:12:41,600 --> 00:12:45,840
and then the central nervous system. If we want to build a robot, we need that those flips,

121
00:12:45,840 --> 00:12:50,240
we'll probably need all of that connected to the brain at the top. Okay. So

122
00:12:50,880 --> 00:12:56,400
and then we have societies where we're organized into societies. Bees are, you know,

123
00:12:56,400 --> 00:13:02,480
in their swarms and humans in their villages, towns, cities, countries, and then the global

124
00:13:02,480 --> 00:13:08,080
national community. So there's all these layers of hierarchy, which are intelligent, right? So,

125
00:13:08,080 --> 00:13:13,760
you know, we have this kind of macro meta intelligence built into communities and systems

126
00:13:13,760 --> 00:13:18,640
of communities, which has enabled us to, you know, survive reproduced into the next generation.

127
00:13:20,080 --> 00:13:25,200
So, okay. So that's biology. That's how it works. That's how it does it. That's what we know,

128
00:13:25,200 --> 00:13:28,000
plus a whole lot of other details, which I haven't really covered.

129
00:13:30,160 --> 00:13:37,040
So what have we done? How far are we along the line with hardware? So basically, the talks

130
00:13:37,040 --> 00:13:44,240
that we'll see here, you know, talking about CPUs, HGO, GPUs, that's what they use, sparkling water

131
00:13:44,240 --> 00:13:51,600
all runs on these things. There's a new sort of endeavor to build ASICs, which are specialized

132
00:13:51,600 --> 00:13:57,680
for deep learning. But again, it's just narrow AIS, it's matrix multiplication. But is it grok

133
00:13:57,680 --> 00:14:04,480
and cerebrus, graph core, Intel, Nirvana, they're all coming out with these amazing things that

134
00:14:04,480 --> 00:14:10,800
can do, you know, petaflops operations, which is more than the brain does. It's actually more

135
00:14:10,800 --> 00:14:16,000
than biology does. But all they're doing is multiplying matrices, right? They're incredible

136
00:14:16,000 --> 00:14:20,800
feats of micro engineering, incredible. These fabs cost, you know, billions of dollars to build.

137
00:14:21,600 --> 00:14:27,760
They've already, you know, graph core just raised $200 million. And they're impressive things,

138
00:14:27,760 --> 00:14:35,120
right? And that's what I look like that CPU GPU. There's a Google TPU, which is an ASIC,

139
00:14:35,120 --> 00:14:39,920
and runs in their data centers, you can hire them, run them in the cloud. And then there's the graph

140
00:14:39,920 --> 00:14:45,520
core IPU. And they all look the same, but you know, the micro architecture is a little different.

141
00:14:46,240 --> 00:14:51,920
So that, you know, this is all digital classical. If you do a computer science, you'd understand,

142
00:14:51,920 --> 00:14:57,360
you know, the details of how bits move around in there, how much energy it takes, how the memory

143
00:14:57,360 --> 00:15:05,680
in the processor, you know, interact together. But they're not intelligent, right? They're just

144
00:15:05,680 --> 00:15:12,800
very good mathematical processors. Okay, that's what a hundred petaflops looks like. And you can,

145
00:15:12,800 --> 00:15:18,160
you know, log in today to these things. The Google data center, this is the most powerful,

146
00:15:18,160 --> 00:15:22,720
you know, three exaflops, we're going to say that that's 1000 million, almost a million times more

147
00:15:22,720 --> 00:15:26,640
powerful than the brain. But it says dumb as a brick, it's the sun, it's incredibly impressive

148
00:15:26,720 --> 00:15:33,440
engineering, it takes up a football field, it's a data center size, but you can't do much except

149
00:15:33,440 --> 00:15:36,880
for model plane numbers together, it's not intelligent. Okay, so clearly we're heading

150
00:15:36,880 --> 00:15:42,880
into a dead end, if we just follow that computer science classical digital hardware route.

151
00:15:43,680 --> 00:15:48,480
See the brain does all of that and all the other types of intelligence in that little three pound

152
00:15:49,040 --> 00:15:55,600
mass. So clearly, it's not just about speed of compute, right? It's not just about horsepower.

153
00:15:55,600 --> 00:16:03,440
So what is it about? Okay, so recently, some of you, who's heard of neuromorphic computing?

154
00:16:03,440 --> 00:16:09,520
Who's heard of that? Yeah, maybe a few, right? A couple. So there's a project called Spinnaker

155
00:16:09,520 --> 00:16:17,920
in the UK. There's, there's IBM True North, Intel, the OHE. There's, you know, a few different

156
00:16:17,920 --> 00:16:27,040
projects that are trying to do mimic how biology in hardware, you know, does, does computation.

157
00:16:27,040 --> 00:16:35,200
Okay, it's called neuromorphic computing. So it's based on the brain. And so there's Steve Furber,

158
00:16:35,200 --> 00:16:43,600
he's one of the guys who come up, invented, founded the ARM computer processor company.

159
00:16:43,600 --> 00:16:51,760
And so he, 20 years ago, left ARM and started on this neuromorphic journey. He's based at

160
00:16:51,760 --> 00:16:59,120
University of Manchester. And recently, the human brain project has folded. It's called Spinnaker

161
00:16:59,120 --> 00:17:05,440
into itself because it's biopausable hardware. So they have lots of nice funding now, again,

162
00:17:05,440 --> 00:17:10,960
as IBM True North and Intel. I mean, these companies have a lot of money too. So that's

163
00:17:10,960 --> 00:17:18,000
what it looks like. So there's five racks there. That is about as powerful as the brain of a mouse.

164
00:17:18,960 --> 00:17:24,320
So it's not human level yet, but it can do things that can go through a maze, it can solve Sudoku

165
00:17:24,320 --> 00:17:28,240
and stuff like that, just like we do. So it's kind of getting there. And I would argue, you know,

166
00:17:28,240 --> 00:17:35,760
to build intelligence, rather than just simulating it on classical digital hardware, this is a more

167
00:17:35,760 --> 00:17:41,920
intelligent route to take. Okay. So, and there's a little bit of detail. So this is an important,

168
00:17:41,920 --> 00:17:48,560
you know, technology and a new path that not many have heard of here, you know, which is very

169
00:17:48,560 --> 00:17:53,680
interesting. It's probably like we were in the 50s with digital computing. You know, there's just a

170
00:17:53,680 --> 00:17:59,440
few main frames in the world right now, a few of these devices, but they actually exist. And you

171
00:17:59,440 --> 00:18:03,840
can actually log into Spinnaker. It's part of the human brain project. You can create an account,

172
00:18:03,840 --> 00:18:09,760
log in and watch it do stuff in real time. So they're not these magical, mystical things in a

173
00:18:09,760 --> 00:18:19,440
lab. They're kind of getting in real. And let's see. So that's okay. That's another initiative

174
00:18:19,440 --> 00:18:23,920
from Heidelberg, the brain scales project, which is also part of the human brain project.

175
00:18:23,920 --> 00:18:30,080
So the one on the left, I would argue, will get us to general intelligence. The one on the right,

176
00:18:30,080 --> 00:18:35,040
no matter how impressive that looks, won't. Okay. So, and then there's quantum computing,

177
00:18:35,040 --> 00:18:40,000
how about it? Is the brain quantum? No, it's warm, it's squidgy, nothing to do with quantum

178
00:18:40,720 --> 00:18:48,720
at all. Okay. Although birds navigate and plants photosynthesize using quantum processes,

179
00:18:48,720 --> 00:18:55,200
but it's not really generally how we do it. Okay. But the pictures look really nice. I decided

180
00:18:55,200 --> 00:18:59,760
to include a couple and it is what a quantum circuit looks like. I also quite like quantum

181
00:18:59,760 --> 00:19:05,200
computing because I have a physical background, but it's probably we don't need to worry about that

182
00:19:05,920 --> 00:19:11,040
if we're building AGI. It might help some of the computations initially, but ultimately we're not

183
00:19:11,040 --> 00:19:17,280
a quantum computer up here. It's a neuromorphic computer. Okay. So that's the sort of four different

184
00:19:17,280 --> 00:19:22,320
types of hardware. And I'd argue the one on the top right, neuromorphic will get us there. And

185
00:19:22,320 --> 00:19:26,960
that's how they look in a little bit more detail. And so, you know, it's not magic. It's just really

186
00:19:26,960 --> 00:19:31,840
hard engineering, which is why we haven't built it yet. It's just hard. Okay. It's small. It's,

187
00:19:31,840 --> 00:19:36,960
you know, and to understand it's difficult. Do we have a mathematical theory of the brain? Well,

188
00:19:36,960 --> 00:19:43,680
I would argue sort of before I go there. So the data center of the future right now is just full

189
00:19:43,680 --> 00:19:51,280
of those very clever chips we saw the graph core, the, you know, TPU GPU CPUs, very, you know,

190
00:19:51,360 --> 00:19:56,000
amazing engineering, but soon we'll see them being filled up with neuromorphic and quantum as well.

191
00:19:56,000 --> 00:20:02,800
So Google announced quantum supremacy last week, a couple of weeks ago, which means that a quantum

192
00:20:02,800 --> 00:20:08,640
computer, it can do something faster than the biggest classical computer. So the data center

193
00:20:08,640 --> 00:20:13,760
of future will look like this. Okay. So we're interesting. We're entering a very interesting

194
00:20:13,760 --> 00:20:18,480
time in human history, I think, in terms of computation, because we haven't just got classical

195
00:20:18,480 --> 00:20:23,520
computing, we have neuromorphic and quantum as well coming online. It's been 80 years, right,

196
00:20:23,520 --> 00:20:29,280
since we first started doing a computation. So at a very interesting time, I think in

197
00:20:29,280 --> 00:20:34,240
the history of computation with these other two major types of computation coming online.

198
00:20:35,520 --> 00:20:40,640
So deep learning, I've argued that won't get us anywhere near intelligence. It will classify

199
00:20:40,640 --> 00:20:44,880
stuff really well and do a little bit of statistical language processing, but it doesn't

200
00:20:44,880 --> 00:20:51,280
do the nine types of intelligence. Okay. And so the most popular framework by far as TensorFlow,

201
00:20:51,280 --> 00:21:00,160
PyTorch is kind of also increasing in the last little while, but you know,

202
00:21:01,520 --> 00:21:06,960
general intelligence, how biology does it, it doesn't do matrix multiplication. So deep learning

203
00:21:06,960 --> 00:21:14,320
is not AGI. So we won't talk about that, but we will talk about more neuroscience stuff. Okay.

204
00:21:14,960 --> 00:21:20,160
Sorry to be so harsh, but you know, it's reality. All that clever stuff isn't actually AGI, but

205
00:21:20,160 --> 00:21:27,600
it's good at certain things. Okay. So what do we do? I mean, where do we even start, right,

206
00:21:28,480 --> 00:21:32,560
with the general theory of intelligence? Well, you know, the brain is a physical system, right?

207
00:21:32,560 --> 00:21:38,400
It's just three pounds of physics, just, okay. So what do we know about physics? Well, we've

208
00:21:38,400 --> 00:21:43,920
pretty much discovered all of the laws of physics. I think dark matter is kind of a new one. It's a

209
00:21:43,920 --> 00:21:50,080
bit of a bold statement. I'm a physicist. I'm allowed to say that. So what are, I mean, what

210
00:21:50,080 --> 00:21:57,360
out of there do we use? Okay. You know, probably thermodynamics and electromagnetism in a complex

211
00:21:57,360 --> 00:22:02,480
system, right? So it's nothing magic. Now the underlying theory of all of this, all of that

212
00:22:03,120 --> 00:22:07,120
can be described in something called the principle of least action, which is that, and it's just

213
00:22:07,120 --> 00:22:12,640
been a book published last year. Surprisingly, it wasn't one before that sort of says, okay,

214
00:22:12,640 --> 00:22:18,240
all of physics, that's, you know, classical quantum, everything can be, you know, derived

215
00:22:18,240 --> 00:22:25,360
starting with this principle of least action. Now, if anyone's who's got a PhD in physics here,

216
00:22:25,360 --> 00:22:31,520
yeah, a few people. And so we've all, this is what you get at grad school in physics, right? We do

217
00:22:31,520 --> 00:22:35,360
Lagrangians and Hamiltonians and principles of least action. So everything we learn,

218
00:22:35,360 --> 00:22:40,240
if equals ma and undergrad, we're told, yeah, that's very nice and quaint. But really, this is

219
00:22:40,240 --> 00:22:45,440
the powerful machinery that's been developed. But we don't, you know, that we sort of, we get,

220
00:22:45,440 --> 00:22:50,960
we get there, but I think we're not really taught that, you know, it's very unifying principle.

221
00:22:52,080 --> 00:22:57,120
Okay, so I'm saying, okay, probably that's a nice place to start if we want to try to come

222
00:22:57,120 --> 00:23:01,840
up with a general theory of intelligence. Okay, it's a big statement and bold statement, but,

223
00:23:02,400 --> 00:23:07,040
you know, the brain is not doing anything that physical systems can't do, even though it feels

224
00:23:07,040 --> 00:23:12,160
kind of, it is different, right? Because we have consciousness and, you know, that subjective

225
00:23:12,160 --> 00:23:16,800
experience and self-awareness and that kind of stuff. But I'm going to argue it's all just physics.

226
00:23:17,600 --> 00:23:22,720
There are people in the room and elsewhere who will say, no, there's something beyond physics,

227
00:23:22,720 --> 00:23:27,680
metaphysics. I'm not one of those people. Okay, so I'm just going to keep it really simple.

228
00:23:28,400 --> 00:23:36,080
So, you know, what can we do? Well, we can explain and understand what we see. We can imagine things,

229
00:23:36,080 --> 00:23:41,200
you know, imagination, imagine that. Problem solving, planning actions to make these things

230
00:23:41,200 --> 00:23:46,320
real, we can do all of that. That's what intelligence is really. And we can build new models as we

231
00:23:46,320 --> 00:23:56,240
learn about the world. So, probably, okay, we've got five minutes. So, these are the sort of major,

232
00:23:56,240 --> 00:24:00,400
some of the major theories of general intelligence, mathematical theory. These are all smart people

233
00:24:01,360 --> 00:24:05,920
who have spent their whole lives dedicating to this. You can look them up later. There's

234
00:24:05,920 --> 00:24:11,600
a guy called Carl Friston at UCL in London. And so, his theory of active inference is the one

235
00:24:11,600 --> 00:24:17,040
that I think is particularly compelling. And I won't spend any time going through the details,

236
00:24:17,040 --> 00:24:20,960
because it's pretty hard math, actually, but he uses something called a free energy principle.

237
00:24:20,960 --> 00:24:26,000
It's based on physics and information theory. It's nothing kind of new and crazy. It's just

238
00:24:26,000 --> 00:24:30,880
applying it to the brain. And he is a theoretical neuroscientist. And so, here he is.

239
00:24:33,680 --> 00:24:38,560
And we don't have time to play it. So, the slides will be made available, I think,

240
00:24:38,560 --> 00:24:43,520
so you can listen to him. And he's on YouTube, if you want to Google. And so, that's how he

241
00:24:43,520 --> 00:24:48,800
sets it up. There's internal, there's us agents acting in an environment. That's the same if we're

242
00:24:48,800 --> 00:24:54,160
bacteria, monkeys, birds, human beings. It's a system, right? A physical system. We act in our

243
00:24:54,160 --> 00:24:59,600
environment. The environment acts back on us. So, there it is, bacteria to brains.

244
00:25:00,720 --> 00:25:05,280
And the math gets really ugly, because it is statistical, probabilistic, and physical all

245
00:25:05,280 --> 00:25:13,120
at the same time. It's complex information theory, essentially. But the ultimate question is,

246
00:25:13,840 --> 00:25:19,280
can we build general intelligence? So, I would argue yes, because we have theories. We have very

247
00:25:19,360 --> 00:25:23,040
compelling theories. All of those are, you know, hundreds and hundreds and hundreds,

248
00:25:23,040 --> 00:25:27,440
literally thousands of papers written deep mathematical theory, which keeps hundreds of

249
00:25:27,440 --> 00:25:32,960
graduate students in PhDs. So, yes, we have the theory. We have candidate theories. We have the

250
00:25:32,960 --> 00:25:38,560
algorithms and software. We have the hardware, neuromorphic in particular, and we have data

251
00:25:38,560 --> 00:25:45,040
sets nowadays. So, yes. And I think we're kind of on our way with these projects like Spinnaker and

252
00:25:45,040 --> 00:25:51,440
IBM True North. So, we're starting to build the hardware. It's just a sort of, it's always

253
00:25:51,440 --> 00:26:00,480
just an engineering problem, right? Yeah. So, you know, what else? So, should we? That's an

254
00:26:00,480 --> 00:26:05,120
ethics question. We won't go into that, because we could spend the rest of our lives arguing

255
00:26:05,120 --> 00:26:13,600
should we do it or not. We're going to do it anyway. So, there are some projects. DeepMind is

256
00:26:13,600 --> 00:26:19,280
trying to, although, you know, all the go and stuff isn't general. It's just specific. But

257
00:26:19,280 --> 00:26:25,200
there's a whole bunch of AGI projects and all super interesting things. I definitely encourage

258
00:26:25,200 --> 00:26:31,280
you to look and explore if you're interested in this subject at all on the internet. In conclusion,

259
00:26:32,160 --> 00:26:37,760
so, it's obvious that deep learning is just statistical. It's not based on physics at all.

260
00:26:37,760 --> 00:26:41,680
There's like zero physics in there. So, it's definitely not going to get us anywhere towards

261
00:26:41,680 --> 00:26:47,600
general intelligence. But research groups are, many are looking into biopausable models. That's

262
00:26:47,600 --> 00:26:53,440
both on the theoretical and the hardware side. We're sort of getting the glimpse of the very

263
00:26:53,440 --> 00:26:58,160
first systems. We have the rat brain there with Spinnaker. So, we're sort of at the very

264
00:26:58,800 --> 00:27:04,960
beginning of a steep exponential. So, maybe that will improve and in 10 years we'll have a human

265
00:27:04,960 --> 00:27:10,880
level brain, which will be conscious and self-aware and subjective experience, which is kind of

266
00:27:10,880 --> 00:27:15,840
spooky and creepy, right? There's nothing. I'm trying to argue there's nothing that mysterious

267
00:27:15,840 --> 00:27:22,000
about the brain. It's just a complex system. And so, what do we do then? Well, we'll figure it out

268
00:27:22,000 --> 00:27:30,080
as we go along, as we have done everything. And so, the future of AI, Jeff Hinton, he's the

269
00:27:30,080 --> 00:27:38,400
godfather of AI. He's in Canada and he, let him have the final word, except there's no volume.

270
00:27:38,400 --> 00:27:45,440
Okay. So, he's just basically saying that the brain is super, everything he's done in his whole

271
00:27:45,440 --> 00:27:53,040
life. He's just clearly admits he wasn't working on general intelligence and to do, to figure out

272
00:27:53,040 --> 00:27:56,480
how it works, we're going to have to look at neuroscience. That's pretty much his message.

273
00:27:56,480 --> 00:28:01,440
And it's pretty much my message. And so, I hope you've enjoyed my talk. It's been a little bit

274
00:28:01,440 --> 00:28:05,040
different from everyone else's, but thanks. Yeah. And we have questions, of course.

275
00:28:05,040 --> 00:28:18,480
Okay. One interesting question is, could AGI be achieved without emulating human brain but

276
00:28:18,480 --> 00:28:26,720
building a completely different model? No. That was a short one. You kind of answered this one,

277
00:28:26,720 --> 00:28:31,920
but maybe you can elaborate a little bit more. When do you think AGI will arrive? And I can

278
00:28:31,920 --> 00:28:37,600
follow with another one. As you said, you're at 50% of achieving AGI. What are the main

279
00:28:37,600 --> 00:28:42,480
challenges you are facing to get to 100% what technology breakthroughs are needed?

280
00:28:42,480 --> 00:28:48,880
Yeah. So, it's basically a hardware engineering problem. So, I did show the Spinnaker and there

281
00:28:48,880 --> 00:28:54,000
are the hardware projects around the world doing this, like IBM and Intel. And so,

282
00:28:54,000 --> 00:28:58,080
with that's the mouse brain right there. So, basically, it's a scaling problem now. I mean,

283
00:28:58,080 --> 00:29:05,360
we started at one transistor at one point with classical computing and then companies like Intel

284
00:29:05,360 --> 00:29:11,760
were formed and AMD and NVIDIA and the rest is history, right? It'll go on its own Moore's law.

285
00:29:11,760 --> 00:29:16,640
So, right now, we're at the kind of like 10 transistor stage that we were with classical. So,

286
00:29:16,640 --> 00:29:22,880
we'll just see that scale. I'm going to say something really bold. It's just a scaling

287
00:29:22,880 --> 00:29:27,040
problem at this point in time. So, watch this space. So, what will you say?

288
00:29:29,360 --> 00:29:36,720
Well, it'll go mouse and then it will go Bumblebee and then mouse and then monkey and then human.

289
00:29:36,720 --> 00:29:41,760
So, we're on a curve. So, human in 10 years, that's my prediction.

290
00:29:41,760 --> 00:29:47,120
They're too optimistic. I mean, in 1969, when people flew to moon, they thought that they

291
00:29:47,360 --> 00:29:52,560
would be living on the moon by now. Okay. I'm an optimist. I'm saying 10. Yeah.

292
00:29:53,680 --> 00:29:58,640
Okay. But I appreciate that. The last one, a comment about the Soul Machines Baby X project

293
00:29:58,640 --> 00:30:04,480
and the potential to get closer to general AI. What's that? Say that again? Yeah. I haven't heard

294
00:30:04,480 --> 00:30:10,560
about the Soul Machines Baby X project. Oh, yeah. I have. Yeah. Okay. Well, certainly. What was the

295
00:30:10,560 --> 00:30:17,760
question? The Baby X? I mean, your comment about it and the potential to get closer to general AI.

296
00:30:17,760 --> 00:30:23,200
Yeah. I didn't actually mention Baby X, but that's a nice project in New Zealand. And it's another

297
00:30:24,080 --> 00:30:30,240
AGI project. But basically, it's going to come back to neuroscience. You know, we saw the mass,

298
00:30:30,240 --> 00:30:36,240
we saw some of the mathematics involved. It's going to be people working at that level from a

299
00:30:36,240 --> 00:30:40,560
theoretical point of view. And other people like Steve Furber and the Human Brain Project,

300
00:30:41,280 --> 00:30:45,840
you know, brilliant engineers basically working on the hardware level, and these people talking

301
00:30:45,840 --> 00:30:51,520
to each other. And I know Carl Friston and Steve Furber do talk to each other and building this

302
00:30:51,520 --> 00:30:56,560
very complex system. Okay, Peter, thank you. Thank you. I will present you with a certificate.

303
00:30:56,560 --> 00:31:04,560
Thank you, Peter. Thank you, sir. Thank you, sir. Thank you, sir.

304
00:31:04,560 --> 00:31:08,560
Thank you. We, we got to, we got to fill in with the certificate.

