start	end	text
0	1380	Tako dobro.
15820	17340	Tko se dobro.
26000	27780	Thank You so for long session.
27780	32380	Google is in statistics, decision making and machine learning.
32380	39380	Today, she leads data scientific and elite agency known for assisting global leaders and executives
39380	41380	in optimizing critical decisions.
41380	47380	So, Casey, may I ask you, whose job does AI automate?
47380	62380	Thank you for that introduction, and yes, I have the answer, but we'll get there in a sort of meandering way.
62380	69380	Good morning, Belgrade, I am so happy to be here with you today, and I have a quick question for the audience.
69380	75380	Who here identifies themselves as an AI professional?
75380	77380	Sure of hands, sure of hands.
77380	79380	OK, I see some hands.
79380	83380	How about who does not identify as an AI professional?
83380	86380	All right, a little more hands, a little more hands.
86380	98380	Friends among the AI professionals, do you know what year was the first year that the term artificial intelligence was used?
98380	99380	Anybody?
99380	102380	We're gonna have to do a little history here, I think.
102380	114380	The answer is 1955-ish, 1956 for sure, because in 1955 there was a fun grant proposal being made,
114380	119380	and in this grant proposal was the first time that this term was used.
119380	124380	So, I think that was written in 1955, but it was for a summer workshop in 1956.
124380	126380	So, what was the grant proposal for?
126380	132380	It was for a summer school session at Dartmouth in America.
132380	141380	That's a university and a bunch of postdocs, led by postdocs or assistant professors, I forget what they were at the time,
141380	144380	led by a fellow at MIT, John McCarthy.
144380	154380	They wanted to get the funding to get together 10 of them to spend two months working on what they called in this grant proposal, artificial intelligence.
155380	161380	Now, there are two fun facts about this situation, which you might enjoy.
161380	167380	The first fun fact, and I actually only learned this myself in October,
167380	172380	from someone who used to work directly with one of those 10 attendees.
172380	176380	The first fun fact is why that term?
176380	179380	Is it because it is truly intelligent?
179380	183380	Because we do get a lot of questions, like, are these things intelligent?
184380	190380	That name is a curse that comes from the grant proposal,
190380	199380	where it turns out that John McCarthy wrote that term in order to scare the US government into giving funding.
199380	204380	So, that term was picked not because anybody particularly knew what intelligence was,
204380	209380	or how the human brain worked, but rather because that term sounded intimidating.
209380	213380	And we're still living with some of the legacy and baggage of that.
213380	219380	Sometimes it stops organizations from being sensible in what to expect.
219380	231380	The other fun fact about the name artificial intelligence is that in that grant proposal, where it was first used,
231380	237380	the writers of the proposal were planning to solve all of artificial intelligence
237380	241380	with 10 people in two months in 1956.
241380	249380	So, there's a legacy in this field of big talk, which does not necessarily meet reality.
249380	254380	So, maybe we should level-set a little bit and reorient ourselves.
254380	262380	Maybe what we should have called it, if we wanted AI as an acronym, is automated inspiration.
262380	265380	Maybe that would have been a better term.
265380	270380	Or amplified impact, which we're seeing a lot today.
270380	275380	Or this year's move towards augmented individuals.
275380	278380	But really, this is a story of automation.
278380	283380	So, let's see automation at its most basic.
283380	285380	So, what are we doing with a computer?
285380	286380	We're automating digitally.
286380	290380	We're turning inputs into outputs via a recipe.
290380	291380	So, what's a recipe?
291380	292380	It's some code.
292380	293380	It's a model.
293380	295380	Those are all fancy words for a recipe.
295380	297380	Takes the inputs, turns them into outputs.
297380	300380	But we're going to need some hardware here to do it.
300380	301380	The recipe is not enough.
301380	302380	We're going to need a computer.
302380	305380	And you are probably imagining this kind of computer.
305380	307380	But I want to show you a different kind of computer.
307380	309380	That's also a computer.
309380	311380	Meet Dora.
311380	313380	And Dora is a computer.
313380	315380	Now, how do I know this?
315380	320380	Because Dora happens to be my friend's wife's aunt.
320380	323380	And Dora was actually a computer.
323380	328380	Here is her marriage certificate from 1950.
328380	334380	And you can see very clearly there that her profession is listed as computer.
334380	336380	So, she is a real computer.
336380	339380	So, we're going to have her in this example.
339380	345380	And what she's supposed to do is take an input and turn it into an output via some recipe.
345380	347380	Now, how does she know what to do?
347380	348380	Right?
348380	353380	Those of you who already work with computers, those of you who are developers,
353380	356380	you know that a computer needs something very important,
356380	359380	which is an engineer to program the computer, right?
359380	361380	And luckily Dora has an engineer also,
361380	366380	because there you can see her husband's profession is listed as engineer.
366380	373380	So, this is one of those early days in history where a computer has managed to marry her engineer.
373380	378380	Now, let's consider Dora and her engineer husband.
378380	382380	Back then in 1950, we want to get Dora to do a task,
382380	386380	like recognize whether an image has a cat in it or not.
386380	390380	This was a very classic computer vision task, right?
390380	392380	The cat not cat task.
393380	396380	Now, one way in which we could do this
396380	403380	is to figure out the exact instructions to give Dora of what to do with the input.
403380	405380	Now, consider what the input is.
405380	411380	The input is a bunch of pixel color values, so it's a bunch of pixels.
411380	416380	And the question is, what should you do with each pixel in an image
416380	419380	to get the answer cat not cat, right?
419380	421380	What is that recipe supposed to be?
421380	424380	And now you're thinking philosophical things,
424380	426380	like what makes a cat a cat?
426380	428380	Difficult question, right?
428380	432380	Like, what do you do with the top left-hand pixel and the one next to it?
432380	434380	And what are you looking for in the image?
434380	438380	Are you looking for triangles, maybe two triangles, maybe some ovals for eyes?
438380	442380	This is a hard recipe to come up with.
442380	447380	And so, in order to program a computer with instructions,
447380	449380	do this, then do that, then do that,
449380	451380	first, you have to know how to do the task,
451380	455380	which, you know, how are you actually doing the task, though?
455380	458380	You're doing it, your brain is doing it,
458380	461380	but what are you actually doing with the pixels?
461380	464380	That is so hard to put into words.
464380	466380	You don't even know what you're doing.
466380	470380	So how on earth are you going to program with the instructions
470380	473380	of what to look for in each pixel?
473380	474380	Very difficult.
474380	477380	Now, there's a different way that we could go about this.
477380	480380	Instead of explaining what to do with each pixel,
480380	484380	you could instead explain your wishes with examples.
484380	486380	Here are a bunch of examples of cat.
486380	488380	Here are a bunch of examples of not cat.
488380	491380	You go find the patterns,
491380	495380	and then make a recipe automatically from those patterns
495380	499380	so that you can take yourself from input to output.
499380	503380	This examples versus instructions thing.
503380	507380	This is the essence of the difference between machine learning
507380	510380	slash AI and traditional programming.
510380	513380	And notice that we already do this with one another as humans.
513380	516380	Sometimes we explain our wishes with examples.
516380	519380	Sometimes we explain our wishes with instructions.
519380	522380	So we already teach one another one of these two ways.
522380	526380	Now we are able to do the same thing with machines,
526380	530380	except we need fancy words for examples and instructions.
530380	536380	So we've got code as instructions and examples map to data.
536380	538380	So this really is the difference
538380	541380	between the traditional software programming approach
541380	546380	and the AI slash machine learning programming approach.
546380	550380	So let's make sure that we warm up this room
550380	554380	and look at doing this cat not cat task together.
554380	559380	So I give you a bunch of inputs with their appropriate labels.
559380	565380	And then something in your brain figures out
565380	569380	what those patterns are, turns that into a recipe.
569380	572380	And then when the next one comes in, you're going to take it
572380	574380	and you're going to convert it to the output I'm looking for.
574380	577380	So to wake ourselves up, we're going to play this game together.
577380	582380	I need each and every one of you to shout cat or not cat
582380	584380	when I show you an input.
584380	585380	Do you think you can do it?
585380	586380	Yes?
586380	588380	You're not loud enough for me.
588380	589380	Yes?
589380	590380	Yes, OK.
590380	591380	Good.
591380	592380	Right.
592380	593380	So here comes the first one.
593380	594380	Cat.
594380	595380	Someone said yes.
595380	596380	Excellent.
596380	597380	Working as intended.
597380	600380	So cat, I agree with you.
600380	602380	Cat, see computers also make mistakes.
602380	604380	This is what we will see as a theme here.
604380	606380	Right, this one, I agree with you.
606380	607380	Cat.
607380	608380	Next.
608380	609380	Not cat.
609380	611380	Not cat.
611380	612380	Not cat.
612380	613380	Cat.
613380	614380	Cat.
614380	615380	Cat.
615380	616380	Cat.
616380	617380	Cat.
617380	619700	Good afternoon, judges and judges.
619700	624780	We've had a very laugh, we could have lep on you.
624780	626420	OK, here we go.
626420	630420	As for judges, we'll start.
630420	632940	No, do you have miel in dazz?
632940	633940	vog.
633980	636660	Now, now, the thing I wanted to do.
636660	645340	At the moment, I knew this chair in Nanаш and I didn't know the chair in Hanse.
645340	646340	Wanlax.
646420	647260	��重要!
647260	649160	Sen si je jestvo invented nachodnout s n夠.
649560	652400	Komand marched amerika je menarem za no doc,
652400	654400	pri konar 모 trenz si jeрал.
656000	656980	Spotn BOLE
658000	660900	postaende idemko u vlada supermarketu,
661340	663920	res prese баг always teber.
664200	666380	The right answer here depends very much
666380	668360	on the purpose of the system.
668360	670360	What does it exist for?
671320	673040	In so, I guess I'd better fill those big boots
673040	674800	and I'll tell you
674800	677240	that this is supposed to be a pet recommendations system.
677260	681300	iz temi direktosni signedcrossne od v pagesenati.
681980	684100	Jedob連 bomo se pristratil.
686380	687260	N Community.
687260	688440	EVON.
689820	704940	ADVORTED V
704940	708240	nismo visoki inserted, izmonživati,學om,
708560	711880	lagske stipiot Lars van pri otro Los 2.
712080	714600	Tudi sem bent ihtim eskatakaj.
714800	716820	Jaz pri vsojeve providilo
717020	719940	vse ovo, inverse drženje v taki,
720140	722140	i kaj mi dobro komens,
722340	724540	da jste ti s da žine.
724740	726940	Tako bojteške investiula,
727140	728700	je biloboardo.
728900	730660	Toto rada ne vem za.
730860	733300	Zvousite,To ga se od Economija Kot.
733300	738300	in zelo je zelo tako zelo tako zelo, if we just say what this is,
738300	743300	which is examples written down in electronic form, text books, essentially,
743300	746300	for the machine student to learn from.
746300	749300	And data quality is everything.
749300	752300	If you're going to teach someone with examples,
752300	756300	the quality of those examples matters so much.
756300	763300	And none of this is purely objective, same meaning and answer every time,
763300	767300	as you can see, depending on the purpose of the system,
767300	772300	whether the right answer is cat or not cat, changes.
772300	777300	So data is a bunch of scraps of information that we happen to write down,
777300	783300	that we put in a textbook for a machine to learn from.
783800	789300	Like human textbooks, normal textbooks for human students,
789300	793300	machine textbooks, data sets have human authors.
793300	799300	They don't arrive from aliens, they don't come from nowhere, from the universe.
799300	803300	They are collected by us, and they fit the sensibilities
803300	809300	of whoever is in charge of the data problem.
809300	816300	And the trouble with them, of course, is that they reflect
816300	821300	unconscious things we might not even have considered
821300	824300	could be a problem when we were authoring our textbooks.
824300	827300	So when you think about really old textbooks,
827300	830300	and you think whether you would want to teach your children
830300	835300	from these really old textbooks from 200 years ago, 300 years ago,
835300	838300	you're thinking absolutely not.
838300	843300	It doesn't matter what the title on the cover of that textbook is,
843300	847300	chances are, if you try to learn from it,
847300	854300	you're going to pick up some habits that are not good or useful habits.
854300	859300	So thinking about the quality of your textbook is really, really important.
859300	862300	And can you complete the following sentence,
862300	864300	just to make sure that we're all on the same page?
864300	867300	Garbage in, garbage out, you know this.
867300	871300	So what I find very interesting about the data space,
871300	874300	the data professions, data science,
874300	876300	I find fascinating about data science.
876300	880300	So when you go to a data conference or a data science conference,
880300	882300	so sometimes I hang out with people,
882300	884300	I'll hang out with people after this as well
884300	886300	if you want to hang out out there.
886300	888300	And sometimes folks come, they hang out,
888300	890300	and they've got all different job roles,
890300	893300	so I'll ask them, you know, what do you do professionally?
893300	896300	One will say statistician, and that one will be data engineer,
896300	899300	and that one will be clinical researcher, and so on.
899300	901300	And then I'd like to do another round,
901300	903300	and I'd like to ask them, okay,
903300	907300	who in your organization is responsible for data quality?
907300	911300	Who is in charge of it from data design,
911300	914300	documentation, the cleanup,
914300	920300	all the way through to the part where it starts hitting
920300	922300	the pipelines that the data engineers have built.
922300	925300	So who shapes the data set?
925300	929300	And what I love to hear here is that as we go around,
929300	933300	we have a very high correlation with whatever they said
933300	935300	their own job title was.
935300	938300	So the statistician says statisticians are responsible for it,
938300	941300	the researchers say researchers are responsible for it,
941300	943300	data engineers say data engineers are responsible for it.
943300	945300	You know what that sounds like?
945300	948300	That sounds like a situation where it's everybody's job
948300	951300	and therefore nobody's job.
951300	954300	In order to automate with data,
954300	956300	you need good and appropriate data.
956300	959300	In order to get good and appropriate data,
959300	963300	you need an expertise in a bunch of different topics.
963300	966300	You don't need to be a full expert in statistics, for example,
966300	969300	but you need some expertise in statistics.
969300	972300	You need some understanding of data engineering,
972300	974300	some understanding of survey design,
974300	978300	human psychology if the data sets are about humans.
978300	981300	You need some user experience design
981300	984300	if you are gathering that information online
984300	988300	and how is the way that you are presenting the questions,
988300	990300	influencing the answers that you get back.
990300	993300	There's a lot of expertise you need.
993300	996300	And yet where is the job role for this?
996300	999300	Where is the profession that takes this seriously?
999300	1003300	And I had a really terrible aha moment with this.
1003300	1007300	I was hanging out with a data science influencer, as one does.
1008300	1012300	And I was talking about how this is a problem,
1012300	1017300	that we are building our disciplines on a foundation of data,
1017300	1022300	and yet the quality of that data is no one's job.
1022300	1024300	I was saying this is so important,
1024300	1029300	maybe instead of over focusing on this last mile thing,
1029300	1034300	maybe we should put more effort as a profession,
1034300	1038300	in the industry in the first bit, the actual data quality.
1038300	1041300	I was saying we need to encourage university graduates
1041300	1043300	to study this and to take this seriously.
1043300	1045300	There needs to be a career progression,
1045300	1049300	a way that motivates you to actually want to learn
1049300	1051300	all those things, to do it professionally,
1051300	1054300	because there's a lot to learn.
1054300	1057300	And then I ask this friend of mine,
1057300	1060300	and we're live streaming, this is what makes it best,
1060300	1063300	we're live streaming in this moment, I'm talking about it.
1063300	1066300	And I ask, so what do you think we should,
1066300	1069300	what should this be called, what is this called?
1069300	1074300	And my friend goes, oh that sounds like a data janitor.
1079300	1083300	Is this how we're going to motivate our undergraduates
1083300	1086300	to go to university, and they're picking their major,
1086300	1088300	they're deciding what to study,
1088300	1090300	and then they call their parents and they say,
1090300	1092300	I've picked one.
1092300	1096300	I would like to go through a hard grueling training program
1096300	1098300	to be a data janitor.
1098300	1101300	Are you proud of me, mom and dad?
1101300	1104300	Right, that's not a good start.
1104300	1106300	And it's not a good start
1106300	1109300	when data seems to be everybody else's job.
1109300	1113300	So we have quite a brittle profession here,
1113300	1116300	because a lot of it is based on the hope
1116300	1118300	that someone is going to do a job
1118300	1120300	that they didn't train for,
1120300	1123300	and that we're almost surely not paying them properly for.
1123300	1125300	We should worry about this.
1125300	1127300	And we should also remember
1127300	1129300	that a lot of the data that we wish we had,
1129300	1132300	or the quality that we wish we had it at,
1132300	1137300	won't exist if we have this basic problem of economics.
1137300	1139300	So that's our first thing.
1139300	1143300	Data are not objective, they are subjective.
1143300	1145300	The design matters.
1146300	1150300	And even though we rely so much on data for automation,
1150300	1154300	there's not that good of a plan in the data professions.
1154300	1156300	So data quality is everything.
1156300	1160300	That is the first point I really want to hammer home here.
1160300	1162300	And I know a lot of you in the audience
1162300	1165300	think you've heard this all before and you get it.
1165300	1168300	But if you did,
1168300	1173300	wouldn't there be better progress in the industry
1173300	1176300	to motivate, fund and compensate people
1176300	1180300	whose job the data quality actually is?
1180300	1184300	And then let's talk a little bit about the internet
1184300	1187300	as a data source.
1187300	1191300	Like that is a source of mirrors, isn't it?
1191300	1194300	Kind of reflects reality a little bit,
1194300	1196300	but you get a skewed perspective.
1196300	1200300	Never forget that the internet is not reality.
1200300	1204300	How you behave online isn't how you behave
1204300	1207300	in your natural surroundings.
1207300	1210300	And we know what kind of stuff lives on the internet
1210300	1213300	in the parts where people can be anonymous, right?
1213300	1217300	It's not necessarily bringing the best of us to anything.
1217300	1219300	So we need to be quite careful
1219300	1222300	with what we allow ourselves to do
1222300	1225300	on the basis of wild type data.
1225300	1229300	Now back to our question of whose job AI automates.
1229300	1234300	Let's look again at the broader category of AI.
1234300	1236300	By the way, I'm using AI and machine learning
1236300	1239300	somewhat interchangeably here, because I've given up.
1239300	1242300	Once upon a time, AI used to be the superset,
1242300	1244300	then machine learning was the subset,
1244300	1247300	then at some point machine learning was the superset
1247300	1250300	and AI was the subset, something, something.
1250300	1252300	If it's deep learning, then it's AI.
1252300	1254300	I give up.
1254300	1257300	Honestly, there was a set of cycles
1257300	1259300	in funding and disappointment,
1259300	1262300	where you got the funding if you said AI
1262300	1264300	and then things didn't work out,
1264300	1267300	so then you started saying something else, machine learning,
1267300	1269300	and then the funding didn't work out there,
1269300	1271300	and so we went up and down in these cycles.
1271300	1273300	Like bell bottoms and skinny jeans, right?
1273300	1276300	Like it's the fashion of what we're gonna call it.
1276300	1278300	So actually my favorite definition
1278300	1281300	of the difference between AI and machine learning
1281300	1285300	is if it is written in Python,
1285300	1287300	it's probably machine learning,
1287300	1290300	and if it's written in PowerPoint, it's probably AI.
1290300	1293300	So, I'm using them interchangeably, the hell with it.
1293300	1295300	So whose job does AI automate?
1295300	1297300	Let's look carefully at what this is
1297300	1299300	as an automation proposition.
1299300	1302300	When I'm automating the traditional way,
1302300	1305300	first, I have to know how to do the task
1305300	1307300	so that I can explain to you
1307300	1309300	what precisely you need to do with each input
1309300	1311300	to get the output.
1311300	1314300	Second, I have to think about every little instruction
1314300	1316300	and then I have to write it down,
1316300	1319300	and first I can write it down for myself in pseudocode
1319300	1322300	or, you know, English or whatever language I speak,
1322300	1324300	and then I have to translate it into something
1324300	1326300	the computer can understand.
1326300	1328300	But I have to deal with every single line,
1328300	1330300	and maybe it takes 10,000 lines,
1330300	1332300	maybe it takes 100,000 lines of code
1332300	1335300	to automate my task, I can write each one down.
1335300	1338300	And you might be saying, oh no, maybe it's,
1338300	1341300	maybe I just get a package somewhere,
1341300	1344300	I find some library, I install something,
1344300	1346300	and then I just pull from there
1346300	1349300	and I don't have to write the code by hand myself.
1349300	1352300	Sure, but some member of our species
1352300	1354300	had to do it.
1354300	1356300	So some human is responsible
1356300	1359300	for having thought through all those instructions,
1359300	1362300	whereas with machine learning and AI,
1362300	1367300	there are just two lines.
1367300	1370300	Optimize this goal on that dataset
1370300	1372300	go.
1372300	1374300	Now those of you who raised your hand for AI professional,
1374300	1378300	you know there's a lot more code that you're writing.
1378300	1382300	But that is because the tools are nasty.
1382300	1386300	At the core, there are only these two lines of instructions.
1386300	1389300	What does success look like?
1389300	1393300	What data should we point this pattern finding thingy at,
1393300	1395300	and off we go.
1395300	1398300	And really, if you had the ability to brute force it,
1398300	1400300	and had enough computing power,
1400300	1402300	you could try every known algorithm
1402300	1404300	with every permutation of it pretty much,
1404300	1406300	quickly eliminate some,
1406300	1408300	try everything out,
1408300	1410300	subject to just the two important lines of instruction,
1410300	1412300	optimize this goal on that dataset.
1412300	1416300	And as the tools become easier and easier,
1416300	1419300	we will strip away all the huffing and puffing
1419300	1422300	and the difficulty of forcing a dataset in this format
1422300	1424300	to be taken up by an algorithm
1424300	1427300	that was designed over there.
1427300	1430300	You know, some of these tools are really,
1430300	1433300	only a mother could love them.
1433300	1436300	And you're left with just these two lines,
1436300	1439300	which means that almost anyone then
1439300	1445300	will be able to automate a task.
1445300	1447300	Ha!
1447300	1450300	What do we see here?
1450300	1455300	First, two very subjective lines.
1455300	1457300	What is the goal of the system?
1457300	1459300	What does success look like?
1459300	1461300	Why am I building this pet classifier
1461300	1463300	that does cat not cat?
1463300	1467300	And why should Tiger be labeled cat versus not cat?
1467300	1469300	Well, vice versa.
1469300	1472300	There's no one single right way to do that.
1472300	1474300	What about scoring mistakes?
1474300	1477300	That's all part of how we're gonna express our goal.
1477300	1480300	Which mistakes are worse than which other mistakes?
1480300	1482300	Again, highly subjective.
1482300	1486300	So, which textbook shall we learn from?
1486300	1489300	There are a lot of different textbook choices you could use.
1489300	1491300	You could also edit and modify
1491300	1493300	and get different versions of the textbooks.
1493300	1496300	And all of this is highly subjective.
1496300	1499300	But now, available,
1499300	1502300	just two lines and you can automate your task.
1502300	1505300	How wonderful and how terrifying simultaneously.
1505300	1509300	This is both the peril and the promise of AI.
1509300	1513300	The promise is if I'm doing a little task myself,
1513300	1517300	I can now automate it very quickly.
1517300	1518300	How great for me?
1518300	1523300	I don't have to go and write everything from scratch.
1523300	1525300	But at the same time,
1525300	1527300	what if I am automating something
1527300	1530300	on behalf of millions or billions of people?
1530300	1533300	What if my code's gonna touch a lot of lives?
1533300	1535300	Well, then I can, again,
1535300	1537300	without thinking too hard about it,
1537300	1539300	get it automated.
1539300	1541300	So, we have a thoughtlessness enabler here.
1541300	1543300	We can be more thoughtless
1543300	1545300	and we can automate thoughtlessly.
1545300	1548300	Which is great when it only affects you.
1548300	1551300	But when we start scaling that up,
1551300	1553300	we can do damage.
1553300	1556300	This is like a proliferation of magic lamps.
1556300	1558300	Lamps with genies.
1558300	1561300	And knowing how to make a wish responsibly
1561300	1563300	is a very important skill.
1563300	1565300	It's the skill of decision leadership.
1565300	1568300	We're not even talking about this, though.
1568300	1570300	We're not asking ourselves,
1570300	1572300	who is it?
1572300	1574300	Who has the skills on our team
1574300	1577300	to figure out what success should look like?
1577300	1581300	How do we carefully state what we're looking for?
1581300	1584300	What do we actually want to create in the world?
1584300	1586300	And what would be the consequences
1586300	1588300	if we got what we asked for?
1588300	1590300	And which data is appropriate and why?
1590300	1592300	And what would need to be true about that data
1592300	1594300	for us to wanna use it?
1594300	1597300	Very, very subjective questions
1597300	1600300	that very few people are trained to answer.
1600300	1602300	So you should worry
1602300	1604300	who is actually being tasked
1604300	1607300	with doing this for massive systems.
1607300	1610300	Do they have the skills to do it?
1610300	1612300	And as you see,
1612300	1614300	the tools get easier and easier,
1614300	1617300	you'll see a shift from a focus on
1617300	1620300	huffing and puffing and actually getting
1620300	1623300	the data to be taken up by the algorithm
1623300	1625300	and then deployed to production,
1625300	1627300	and a lot more focus on
1627300	1629300	how do we put 10,000 lines
1629300	1631300	or 100,000 lines of thought
1631300	1634300	back into these two lines.
1634300	1637300	We've allowed ourselves to be thoughtless,
1637300	1639300	but on some things, that's not okay.
1639300	1641300	So how do we put that thought back in?
1641300	1643300	How do we very carefully design
1643300	1646300	systems that can affect society at scale?
1646300	1648300	But back to the question
1648300	1651300	of whose job are we actually automating here?
1651300	1654300	Well, it is the developer's job.
1654300	1657300	We're going from having to write instructions
1657300	1659300	to now being able to say,
1659300	1661300	instead of knowing how to do the task,
1661300	1665300	here's the objective, here's the data, go.
1665300	1668300	That said, it's not like we're putting
1668300	1670300	software developers out of business.
1670300	1672300	First, there's still a lot of huffing
1672300	1674300	and puffing to do to get the algorithms
1674300	1677300	to accept those instructions
1677300	1679300	and the data.
1679300	1682300	Second, we are actually unlocking
1682300	1685300	a whole class of new applications.
1685300	1689300	And all the old approaches
1689300	1693300	are still going to be very economically necessary.
1693300	1695300	Why?
1695300	1697300	If you are able to automate your task
1697300	1699300	with instructions,
1699300	1701300	that is how you should do it.
1701300	1703300	That's how you get the most control.
1703300	1705300	If you're able to say what needs to be done
1705300	1707300	in what order,
1707300	1709300	and you give those instructions to your machine
1709300	1712300	or to your human employee,
1712300	1715300	you can be sure of what that person
1715300	1717300	is going to do next
1717300	1719300	if they're following the instructions.
1719300	1721300	Exactly what you've told them to do.
1721300	1723300	No guessing.
1723300	1726300	No surprise ways that they interpreted anything.
1726300	1729300	Just follow those instructions.
1729300	1731300	Whereas, if you know how to give the instructions,
1731300	1734300	but instead you give a few examples,
1734300	1736300	who knows what they're going to learn
1736300	1738300	in those examples?
1738300	1740300	Maybe they'll learn the right thing,
1740300	1742300	maybe they won't.
1742300	1744300	And mistakes are possible.
1744300	1746300	That's true with humans,
1746300	1748300	that's also true with these AI systems.
1748300	1750300	So why are we using them?
1750300	1753300	To automate things we can't automate the other way.
1753300	1756300	So we're not putting developers out of business.
1756300	1758300	Everything developers used to do
1758300	1760300	and used to be able to do,
1760300	1762300	you're still going to want to do that
1762300	1764300	in the old traditional way.
1764300	1767300	But now we've got a whole new class of applications.
1767300	1770300	And let's talk about a new new class of applications.
1770300	1772300	The two different AIs.
1772300	1775300	So this year we're talking a lot about AI.
1775300	1777300	We tend,
1777300	1779300	when we find ourselves hanging out with friends
1779300	1781300	and having a glass of wine
1781300	1784300	and talking about all these new things in AI in 2023,
1784300	1787300	we tend to be talking about generative AI.
1787300	1790300	So let's remind ourselves very quickly of the difference.
1790300	1792300	So discriminative AI, the old one,
1792300	1795300	the one you're used to from last decade,
1795300	1798300	that is all about applying a label.
1798300	1800300	So this is a thing labeler.
1800300	1802300	We had the cat not cat example of that.
1802300	1806300	Here's another classic again with vision.
1806300	1808300	So I really like this tweet.
1808300	1810300	It comes from BJM,
1810300	1812300	who complained that he was locked out
1812300	1815300	because his smart front door lock,
1815300	1818300	his nest camera system locked him out
1818300	1822300	and he was protecting him from Batman.
1822300	1824300	It didn't want to let Batman in the house,
1824300	1826300	so it locked poor BJM out.
1826300	1828300	So it's supposed to find the right answer.
1828300	1830300	It doesn't always work correctly.
1830300	1832300	These systems do make mistakes.
1832300	1835300	And it's really important for designers to remember this,
1835300	1838300	because imagine what would have happened to poor BJM
1838300	1840300	if there wasn't a plan for mistakes.
1840300	1843300	He wouldn't have gotten back into his house.
1843300	1846300	Instead he can put user pin to get himself in
1846300	1849300	because the engineers built that safety net
1849300	1851300	and knew that mistakes were possible.
1851300	1853300	So that's thing labelers.
1853300	1855300	On the other hand, generative AI
1855300	1857300	is about creating a plausible exemplar.
1857300	1858300	What are we learning?
1858300	1860300	Not a label, but a distribution.
1860300	1862300	And what can you do with a distribution
1862300	1865300	is create a really good fake.
1865300	1867300	This is a fake maker.
1867300	1869300	So as you all know,
1869300	1872300	Picasso is very famous for his paintings of laptops.
1872300	1874300	So I have some examples for you there
1874300	1876300	in the top right-hand corner.
1876300	1879300	This is where I'm using an image generation tool
1879300	1881300	called Mid Journey.
1881300	1883300	Mid Journey is my favorite casino.
1883300	1885300	I really enjoy playing with Mid Journey.
1885300	1887300	It gives you, you put in a prompt,
1887300	1890300	it gives you four options back,
1890300	1891300	and maybe you like it,
1891300	1893300	maybe you rerun the prompt
1893300	1895300	until you get something that you like.
1895300	1899300	I've also created some fake Gucci sunglasses for you.
1899300	1902300	So both of these aren't,
1902300	1904300	you know, it's not real Picasso.
1904300	1907300	This doesn't really exist out there.
1907300	1912300	We are generating from a distribution of plausible Picasso type
1912300	1915300	and laptop type things
1915300	1919300	to get this lovely fake for you.
1919300	1923300	So it's a game of plausible exemplars.
1923300	1927300	And people ask a question that bugs me so much
1927300	1929300	when they see this stuff.
1929300	1931300	They ask, can AI be creative?
1931300	1933300	Does this mean that AI is the artist?
1933300	1937300	Is AI making art?
1937300	1940300	And then I have to remind folks
1940300	1943300	of an entire century of art history.
1943300	1945300	Because if you're asking questions like this,
1945300	1947300	you must have missed something
1947300	1950300	from art history from the 20th century.
1950300	1952300	So let's go back to 1917.
1952300	1955300	Marcel Duchamp found this iconic piece.
1955300	1958300	This is considered iconic in the art world.
1958300	1960300	This is a urinal.
1960300	1962300	He signed some name on it,
1962300	1964300	not even his own name.
1964300	1967300	He took it to the exhibition when that's art.
1967300	1971300	And it was, we consider this a very interesting piece.
1971300	1973300	It's worth a lot.
1976300	1977300	Is it art?
1977300	1978300	Sure.
1978300	1979300	Why?
1979300	1981300	Because art is a conversation
1981300	1983300	that humanity is having with itself
1983300	1985300	and has been having for millennia.
1985300	1986300	And to make art,
1986300	1988300	we consider the next sentence
1988300	1991300	in this grand conversation.
1991300	1993300	But who is the artist?
1993300	1996300	I would say Duchamp, let's give him credit.
1996300	1998300	Because otherwise, where are we going to put the credit?
1998300	2000300	On the porcelain makers,
2000300	2001300	the toilet company,
2001300	2003300	that doesn't make any sense to me.
2003300	2005300	And should we penalize him
2005300	2008300	because he didn't sculpt it from scratch,
2008300	2011300	mixing his own materials,
2011300	2012300	creating his own porcelain?
2012300	2013300	Not at all.
2013300	2015300	This cut out a lot of toil
2015300	2018300	because he didn't know what he wanted to say much faster.
2018300	2021300	Generative AI plays the same role.
2021300	2023300	Like a paintbrush,
2023300	2026300	it's a tool for you to be able to say
2026300	2028300	what you need to say faster and better.
2028300	2033300	And the secret behind a lot of these generative AI art things
2033300	2035300	is that it's very rare
2035300	2039300	that the first one is the one that's presented to the audience.
2039300	2043300	So AI made art that won some art competition.
2043300	2046300	The 8,000th iteration,
2046300	2050300	a person cranked the handle on these tools
2050300	2052300	8,000 times,
2052300	2055300	500 times, however many times it took
2055300	2058300	to get the one that they were looking for,
2058300	2060300	to express what they wanted to express.
2060300	2062300	So where is the creativity?
2062300	2063300	It's in the human.
2063300	2066300	But the human can go a little bit faster.
2066300	2068300	They don't need to mix paints.
2068300	2070300	We don't penalize artists today
2070300	2072300	for not mixing their own paints
2072300	2074300	the way that they would have in the Middle Ages.
2074300	2076300	Making their own paintbrushes
2076300	2078300	out of horse hair or whatever it is.
2078300	2080300	You go to the shop, you buy some paintbrushes,
2080300	2082300	you buy some paint, and you go paint.
2082300	2083300	That's great.
2083300	2085300	That lets us get there faster.
2085300	2088300	And that's what generative AI allows you to do as well.
2088300	2090300	What are some other things you can do?
2090300	2093300	So good old open AI all over the news.
2093300	2095300	Open chat GPT.
2095300	2098300	I have in audiences like this,
2098300	2101300	I have asked who here has never used chat GPT.
2101300	2103300	And so I'm going to stop embarrassing audiences
2103300	2105300	because there does tend to be one hand.
2105300	2108300	And then I ask, OK, who here has never read about chat GPT
2108300	2110300	and that no hands go up?
2110300	2112300	And I think what a strange equation.
2112300	2115300	It is faster to try it than to read about it.
2115300	2118300	So why did my one hand, one or two hands
2118300	2120300	read about it without trying it?
2120300	2121300	You may as well just try it.
2121300	2122300	The interface is super easy.
2122300	2124300	It's like sending a text message
2124300	2126300	and you type whatever you want to type.
2126300	2130300	This is when we asked CEOs
2130300	2133300	what they personally use chat GPT for the most.
2133300	2137300	One of the favorite answers was to write a retirement poem
2137300	2138300	or a birthday poem.
2138300	2143300	So it's really getting used for its top applications.
2143300	2146300	But here I've asked it to write a funny retirement poem
2146300	2148300	for your CEO in the style of Dr. Seuss.
2148300	2149300	And what does it give us back?
2149300	2151300	You've been the big cheese, the head of the pack.
2151300	2154300	Now it's your time to kick back and slack.
2154300	2161300	So definitely the highest in what we could possibly want
2161300	2162300	out of our technology.
2162300	2163300	Let's try another one.
2163300	2167300	This is an application that OpenAI found
2167300	2173300	and noticed was a statement perhaps about the human condition.
2173300	2176300	So a lot of people like to take bullet points
2176300	2182300	and then ask OpenAI, chat GPT, to turn that into an email,
2182300	2183300	a full email.
2183300	2186300	So here's some summaries, expand that out into an email.
2186300	2189300	And what OpenAI found was that this was a popular use case,
2189300	2192300	as was this other use case,
2192300	2195300	which was summarize this email as bullet points
2195300	2197300	back into the original.
2197300	2201300	So I think that does tell us something a little bit sad
2201300	2203300	and funny about how humans work.
2203300	2205300	Wouldn't it be great if all of our emails
2205300	2209300	could just be bullet points in the first place?
2209300	2213300	But what we're seeing here with generative AI
2213300	2216300	is a new kind of user interaction.
2216300	2219300	It is AI as a raw material.
2219300	2221300	AI, for AI's sake,
2221300	2226300	given to you the user to do anything you want with Next.
2226300	2230300	So we have the ability to find the right distributions,
2230300	2233300	to pull laptops by Picasso out,
2233300	2236300	or Gucci sunglasses, or whatever else you want.
2236300	2238300	But now we're giving you the tool,
2238300	2241300	you figure out what you want to shape it into.
2241300	2246300	And when we talk about regulating generative AI,
2246300	2251300	I hope you can appreciate now how hard this is,
2251300	2255300	because we are not good at regulating raw materials,
2255300	2257300	even physical raw materials.
2257300	2259300	Whose fault is it?
2259300	2263300	If I invent a phenomenal anti-gravity material,
2263300	2265300	that could be pretty cool for humanity,
2265300	2268300	but some idiot's gonna make skis out of it.
2268300	2270300	So whose fault is it, then,
2270300	2273300	when they go and ski in anti-gravity skis and hurt themselves?
2273300	2276300	Was it me for making the material?
2276300	2279300	Was it whoever helped them fashion the skis?
2279300	2282300	Or was it, then, the user of those skis?
2282300	2284300	Who is responsible?
2284300	2287300	How do we limit what the uses of it are
2287300	2290300	that would be okay versus not okay?
2290300	2293300	This is a hard problem, hard with physical materials
2293300	2295300	when it comes to digital raw materials.
2295300	2297300	Good luck.
2297300	2299300	Really hard to figure out how to regulate.
2299300	2304300	And when I hear that a problem is really hard,
2304300	2308300	the last thing that I want is for us, then,
2308300	2311300	to solve it in some dumb way just to say we've solved it
2311300	2315300	so we can move on in our to-do list.
2315300	2318300	Solving AI regulation here is difficult,
2318300	2321300	which means that maybe we shouldn't get ahead of ourselves
2321300	2324300	and make a bunch of laws we haven't thought through,
2324300	2327300	maybe go slowly and think about the consequences
2327300	2330300	of regulating, maybe request a bunch of information
2330300	2333300	that would help you regulate it later.
2333300	2336300	So, it is a very, very difficult problem.
2336300	2340300	Then, another kind of application
2340300	2343300	is all kinds of translation-type stuff.
2343300	2346300	So here I have asked it to write the FORTRAN code
2346300	2349300	for generating the Fibonacci sequence.
2349300	2352300	And I do not speak or understand FORTRAN,
2352300	2355300	so, hopefully, someone in the room can look at this
2355300	2358300	and see if it's right or not.
2358300	2361300	What I can do is I know what the Fibonacci sequence is
2361300	2363300	and I can write out those instructions.
2363300	2366300	I could also have written them out the way that I want
2366300	2368300	and ask for it to translate that to FORTRAN.
2368300	2371300	But here's a little quick bit of trouble.
2371300	2373300	Anyone here going to fess up,
2373300	2377300	going to confess with me that you don't speak FORTRAN?
2378300	2380300	Right, so, I'm seeing some hands.
2380300	2384300	So, imagine that we asked for this lovely FORTRAN code
2384300	2387300	for generating the Fibonacci sequence and we get it.
2387300	2391300	Do we take this and plug this directly into our codebase
2391300	2395300	when we don't understand what the hell it is?
2395300	2397300	Terrifying.
2397300	2402300	So, with this one, OK, maybe we know how to be software engineers.
2402300	2404300	I would figure out how to make some unit tests here.
2404300	2407300	Maybe I would line by line try to figure out what I'm looking at.
2407300	2411300	But you can see the more code that I generate with this
2411300	2414300	in languages that I don't speak or understand,
2414300	2418300	the more space I'm leaving for potentially catastrophic disasters
2418300	2424300	as I plug this in to an already complicated system.
2424300	2428300	This is why people are saying that good developers
2428300	2431300	are becoming much better, the estimates coming from McKinsey
2431300	2434300	of 50% better, if you're a good engineer,
2434300	2438300	then this can cut out a lot of drudgery.
2438300	2442300	But bad engineers are becoming worse.
2442300	2446300	And bad teams are reducing corporate productivity
2446300	2450300	because they're plugging all kinds of nonsense into their systems.
2450300	2456300	So, in general here, people who are already highly productive
2456300	2459300	are making themselves more productive.
2459300	2461300	They understand the output.
2461300	2464300	They understand how to put it together into solutions.
2464300	2467300	But those who are on the less productive side
2467300	2469300	or they don't know what they're working with
2469300	2471300	or they just believe AI is magic
2471300	2473300	and they plug things in where they shouldn't,
2473300	2476300	they are reducing everybody's productivity.
2476300	2478300	So, that's a point worth thinking about.
2478300	2481300	And I wonder if it would surprise anybody here
2481300	2484300	that I do not, in fact, speak Serbian.
2484300	2488300	So, imagine if I had a really, really important email
2488300	2494300	that I needed to write to someone in Serbian
2494300	2497300	where I really care about my reputation and that relationship.
2497300	2500300	So, I just go straight to open AI.
2500300	2503300	I ask for that email to be translated.
2503300	2507300	I get something out of there and I send it.
2507300	2509300	What a disaster that could be.
2509300	2511300	Maybe it's correct.
2511300	2513300	But if I have no way of checking it,
2513300	2515300	I'm gonna have problems.
2515300	2518300	And this brings us to why it's so difficult to scale
2518300	2521300	generative AI in the enterprise.
2521300	2523300	To use it for personal productivity
2523300	2527300	and to make an individual responsible for the output
2527300	2529300	is quite an easy one
2529300	2533300	if you already have a smart and productive person.
2533300	2538300	But when you think about taking the person out of the loop
2538300	2541300	and then at scale automating some processes,
2541300	2546300	remember, these systems do make mistakes.
2546300	2550300	It may be hard for humans to check.
2550300	2554300	It may be hard to even define whether that output was right or not.
2554300	2561300	So, how do you set up at scale this kind of automation?
2561300	2563300	From now on, we're automatically going to write
2563300	2566300	all our emails in Serbian with this system.
2566300	2569300	You're gonna have to test the hell out of it.
2569300	2572300	And that's what a lot of companies don't know how to do.
2572300	2576300	So, no wonder we're getting this bottleneck in enterprise automation.
2576300	2580300	And so, when should you trust an AI system?
2580300	2582300	There are two paths to trust.
2582300	2584300	The human in the loop model.
2584300	2587300	So, make the human individually responsible
2587300	2590300	and treat that as individual productivity increases
2590300	2595300	or a hell of a lot of safety testing and safety nets.
2595300	2601300	And as always, it is safest to have both.
2601300	2606300	We're still so excited in what you potentially could do
2606300	2612300	that we end up in organizations with death by a thousand pilots,
2612300	2614300	get mandates from the top saying,
2614300	2617300	everybody go find two, three use cases
2617300	2620300	and everybody go try plug this into your business.
2620300	2624300	And then take the human out of the loop too.
2625300	2627300	They don't know how to do testing though.
2627300	2629300	They don't know how to build the safety nets.
2629300	2631300	They've removed the human in the loop
2631300	2634300	and then they wonder why their use cases don't work.
2634300	2639300	I do think that we're gonna have a lot more generative AI at scale,
2639300	2641300	but this is the part to figure out.
2641300	2643300	In fact, I'm a recovering statistician.
2643300	2645300	And recovering also,
2645300	2649300	because we are the grumpiest of the data scientist
2649300	2653300	and we've sort of been folded into the data science profession for a bit.
2653300	2661300	Definitively the less popular sibling in that family for now.
2661300	2664300	And I cackled to myself slightly,
2664300	2669300	because statisticians are gonna have to be back, aren't they?
2669300	2671300	Someone's gonna have to figure out,
2671300	2674300	does this thing in fact work the way that we think it works?
2674300	2676300	Someone's gonna have to figure out the testing.
2676300	2680300	Testing is still the most difficult part, especially in generative AI.
2681300	2684300	And here is a little analogy
2684300	2690300	that I encourage folks who are not in this business to really internalize.
2690300	2693300	The data are like the ingredients.
2693300	2696300	The algorithms are like appliances in a kitchen.
2696300	2701300	When we were talking about doing applied AI last decade,
2701300	2705300	we were talking about innovating in models, in recipes.
2705300	2710300	So creating a croissant that is sugar-free and gluten-free
2710300	2713300	and dairy-free and delicious.
2713300	2715300	How do you go about making a recipe like that?
2715300	2717300	You get a bunch of ingredients, you get the appliances,
2717300	2720300	and then you tinker, you doubly play, you hope.
2720300	2722300	You do a bunch of taste testing,
2722300	2725300	you get your croissant or cookie or whatever it is,
2725300	2729300	you start being able to produce them at scale, how wonderful.
2729300	2733300	Now, with the generative AI revolution,
2733300	2735300	it's equivalent to saying, instead of,
2735300	2737300	I give all of you the cookie that I've made
2737300	2740300	and you can eat it or not eat it right here, right now,
2740300	2743300	and you don't have to worry about where it comes from.
2743300	2748300	Instead, I'm saying, here, get access to my cookie maker,
2748300	2751300	the cookies, and take them and turn them into whatever you want.
2751300	2755300	Anything creative, Valentine's Day, baskets, spray-painted gold,
2755300	2758300	call it art, whatever you want to do.
2758300	2762300	But you see that we do lose an interesting layer of control there
2762300	2767300	because we have a separation in this system
2767300	2771300	from whatever the dish was to however you're going to use it.
2771300	2776300	And the users down the line of these products,
2776300	2781300	you are not told very much about where those ingredients came from,
2781300	2784300	how they got processed, what their quality was,
2784300	2788300	what were those appliances, what even went on in that kitchen.
2788300	2790300	Is it poisonous, isn't it?
2790300	2793300	But you're going to have to test everything yourself again from scratch
2793300	2796300	if you're a downstream user, trying to do this
2796300	2801300	at enterprise scale for tasks that matter.
2801300	2804300	For some tasks, though, truly, take a thing, spray-painted gold,
2804300	2806300	what do you care what the ingredients were?
2806300	2808300	It looks about right, now it's gold
2808300	2812300	and it still looks about right, everything's good.
2812300	2815300	But where the ingredients might have mattered,
2815300	2817300	where the quality might have mattered,
2818300	2820300	it's up to you to figure out the testing.
2820300	2825300	And finally, leaving you with the thought of whose job AI
2825300	2827300	will automate as a consequence.
2827300	2830300	Yes, it is about engineering and automation,
2830300	2833300	but as a consequence of these technologies,
2833300	2834300	what are we going to see?
2834300	2839300	In the US, what we are seeing is the second quartile
2839300	2845300	having the most effect, taking the most economic damage
2845300	2847300	in these technologies, the second quartile
2847300	2850300	of skill level and income.
2850300	2852300	And this is puzzling economists.
2852300	2855300	Why not the people at the very bottom,
2855300	2857300	the least skilled labor?
2857300	2860300	And why not the most skilled labor?
2860300	2862300	Well, I have a guess.
2862300	2866300	What's happening in the second quartile
2866300	2870300	is the tasks that are most repetitive and most digitized.
2870300	2874300	If it's repetitive and digitized, if it's copy-pasting,
2874300	2879300	if it's doing things on mental screensaver with a computer,
2879300	2885300	those are the tasks that are most likely to be rendered
2885300	2889300	no longer necessary for humans to do.
2889300	2893300	Whereas the difficult skills of having taste,
2893300	2896300	of being creative, of thinking, of problem solving,
2896300	2899300	of being a great engineer,
2900300	2905300	those skills and those jobs are still extremely important,
2905300	2909300	no matter how good your tools get.
2909300	2913300	But there is a small economic issue here,
2913300	2915300	which could become a very big one.
2915300	2918300	What we take for granted is how our young people
2918300	2922300	become our senior trusted leaders, artists,
2922300	2925300	developers, managers, et cetera.
2925300	2929300	And nobody trusts someone fresh out of university.
2929300	2931300	I don't know, do you?
2931300	2934300	So what does that path look like
2934300	2938300	from newly graduated to senior and trusted?
2938300	2943300	Often a bunch of easy-to-measure output.
2943300	2946300	That's pretty repetitive, a bit mindless.
2946300	2950300	You understand the person's character by working with them.
2950300	2952300	You see if they go a little outside the box,
2952300	2955300	you give them very low trust tasks.
2955300	2959300	Exactly the kind of tasks that are going to get automated here.
2959300	2961300	And what you will see is that bosses, leaders,
2961300	2963300	are now going to absorb a lot of the work
2963300	2967300	that their junior staff members would have done.
2967300	2970300	Great in the short term.
2970300	2973300	How are you going to get the next generation of bosses, though?
2973300	2977300	What is your plan for developing your talent
2977300	2979300	to a point where you can trust them?
2979300	2982300	Because the skills that are most likely to get left
2982300	2985300	are those skills where you need to apply some taste,
2985300	2986300	where you need to be trusted,
2986300	2990300	where you need to have judgment and good sense and expertise.
2990300	2993300	And where, when you look at the code,
2993300	2997300	where you look at the output, where you look at the art,
2997300	3000300	you are able to judge whether that's what you're looking for.
3000300	3002300	If I never learn how to speak Serbian,
3002300	3006300	how am I going to deal with that email output in Serbian?
3006300	3010300	How are we going to incentivize me to learn the language,
3010300	3014300	whether it's a programming one or a human one,
3014300	3018300	to get to the point where I'm able to supervise
3018300	3021300	the functioning of these systems rather than do the task myself?
3021300	3028300	This is a big piece that is missing in our economic plans globally.
3028300	3031300	And so as you, if you are beginning to automate
3031300	3034300	with these systems in your organizations,
3034300	3036300	as you're taking responsibility for this,
3036300	3039300	please pay attention to the three big topics from this talk.
3039300	3043300	One, data quality matters, and today it's nobody's job.
3043300	3045300	That's a problem.
3045300	3049300	Two, testing these things is difficult,
3049300	3053300	especially when we're talking about generative AI at scale
3053300	3055300	without humans in the loop.
3055300	3060300	And three, it's going to be up to you to create that plan
3060300	3063300	for how you're going to be able to train your staff
3063300	3065300	and not leave anyone out,
3065300	3067300	because your most valued staff members,
3067300	3070300	the ones you really do want to have around,
3070300	3075300	often need that path of going through a gauntlet
3075300	3081300	that these tools may render less attractive in the short term.
3081300	3085300	And that's why we have a term like secret cyborgs in America,
3085300	3087300	so people who want to use these tools,
3087300	3091300	they don't want their managers to know.
3091300	3093300	That should be a warning.
3093300	3096300	It should be a warning that we don't trust managers
3096300	3102300	to handle that transition fairly, carefully, and responsibly.
3102300	3108300	And so how do we in this room think about championing,
3108300	3112300	a gentle and also effective and intelligent
3112300	3115300	way through the challenges of today
3115300	3120300	to get to a highly productive economy and workforce tomorrow?
3121300	3123300	Thank you very much.
3126300	3128300	Thank you.
