WEBVTT

00:00.000 --> 00:01.380
Tako dobro.

00:15.820 --> 00:17.340
Tko se dobro.

00:26.000 --> 00:27.780
Thank You so for long session.

00:27.780 --> 00:32.380
Google is in statistics, decision making and machine learning.

00:32.380 --> 00:39.380
Today, she leads data scientific and elite agency known for assisting global leaders and executives

00:39.380 --> 00:41.380
in optimizing critical decisions.

00:41.380 --> 00:47.380
So, Casey, may I ask you, whose job does AI automate?

00:47.380 --> 01:02.380
Thank you for that introduction, and yes, I have the answer, but we'll get there in a sort of meandering way.

01:02.380 --> 01:09.380
Good morning, Belgrade, I am so happy to be here with you today, and I have a quick question for the audience.

01:09.380 --> 01:15.380
Who here identifies themselves as an AI professional?

01:15.380 --> 01:17.380
Sure of hands, sure of hands.

01:17.380 --> 01:19.380
OK, I see some hands.

01:19.380 --> 01:23.380
How about who does not identify as an AI professional?

01:23.380 --> 01:26.380
All right, a little more hands, a little more hands.

01:26.380 --> 01:38.380
Friends among the AI professionals, do you know what year was the first year that the term artificial intelligence was used?

01:38.380 --> 01:39.380
Anybody?

01:39.380 --> 01:42.380
We're gonna have to do a little history here, I think.

01:42.380 --> 01:54.380
The answer is 1955-ish, 1956 for sure, because in 1955 there was a fun grant proposal being made,

01:54.380 --> 01:59.380
and in this grant proposal was the first time that this term was used.

01:59.380 --> 02:04.380
So, I think that was written in 1955, but it was for a summer workshop in 1956.

02:04.380 --> 02:06.380
So, what was the grant proposal for?

02:06.380 --> 02:12.380
It was for a summer school session at Dartmouth in America.

02:12.380 --> 02:21.380
That's a university and a bunch of postdocs, led by postdocs or assistant professors, I forget what they were at the time,

02:21.380 --> 02:24.380
led by a fellow at MIT, John McCarthy.

02:24.380 --> 02:34.380
They wanted to get the funding to get together 10 of them to spend two months working on what they called in this grant proposal, artificial intelligence.

02:35.380 --> 02:41.380
Now, there are two fun facts about this situation, which you might enjoy.

02:41.380 --> 02:47.380
The first fun fact, and I actually only learned this myself in October,

02:47.380 --> 02:52.380
from someone who used to work directly with one of those 10 attendees.

02:52.380 --> 02:56.380
The first fun fact is why that term?

02:56.380 --> 02:59.380
Is it because it is truly intelligent?

02:59.380 --> 03:03.380
Because we do get a lot of questions, like, are these things intelligent?

03:04.380 --> 03:10.380
That name is a curse that comes from the grant proposal,

03:10.380 --> 03:19.380
where it turns out that John McCarthy wrote that term in order to scare the US government into giving funding.

03:19.380 --> 03:24.380
So, that term was picked not because anybody particularly knew what intelligence was,

03:24.380 --> 03:29.380
or how the human brain worked, but rather because that term sounded intimidating.

03:29.380 --> 03:33.380
And we're still living with some of the legacy and baggage of that.

03:33.380 --> 03:39.380
Sometimes it stops organizations from being sensible in what to expect.

03:39.380 --> 03:51.380
The other fun fact about the name artificial intelligence is that in that grant proposal, where it was first used,

03:51.380 --> 03:57.380
the writers of the proposal were planning to solve all of artificial intelligence

03:57.380 --> 04:01.380
with 10 people in two months in 1956.

04:01.380 --> 04:09.380
So, there's a legacy in this field of big talk, which does not necessarily meet reality.

04:09.380 --> 04:14.380
So, maybe we should level-set a little bit and reorient ourselves.

04:14.380 --> 04:22.380
Maybe what we should have called it, if we wanted AI as an acronym, is automated inspiration.

04:22.380 --> 04:25.380
Maybe that would have been a better term.

04:25.380 --> 04:30.380
Or amplified impact, which we're seeing a lot today.

04:30.380 --> 04:35.380
Or this year's move towards augmented individuals.

04:35.380 --> 04:38.380
But really, this is a story of automation.

04:38.380 --> 04:43.380
So, let's see automation at its most basic.

04:43.380 --> 04:45.380
So, what are we doing with a computer?

04:45.380 --> 04:46.380
We're automating digitally.

04:46.380 --> 04:50.380
We're turning inputs into outputs via a recipe.

04:50.380 --> 04:51.380
So, what's a recipe?

04:51.380 --> 04:52.380
It's some code.

04:52.380 --> 04:53.380
It's a model.

04:53.380 --> 04:55.380
Those are all fancy words for a recipe.

04:55.380 --> 04:57.380
Takes the inputs, turns them into outputs.

04:57.380 --> 05:00.380
But we're going to need some hardware here to do it.

05:00.380 --> 05:01.380
The recipe is not enough.

05:01.380 --> 05:02.380
We're going to need a computer.

05:02.380 --> 05:05.380
And you are probably imagining this kind of computer.

05:05.380 --> 05:07.380
But I want to show you a different kind of computer.

05:07.380 --> 05:09.380
That's also a computer.

05:09.380 --> 05:11.380
Meet Dora.

05:11.380 --> 05:13.380
And Dora is a computer.

05:13.380 --> 05:15.380
Now, how do I know this?

05:15.380 --> 05:20.380
Because Dora happens to be my friend's wife's aunt.

05:20.380 --> 05:23.380
And Dora was actually a computer.

05:23.380 --> 05:28.380
Here is her marriage certificate from 1950.

05:28.380 --> 05:34.380
And you can see very clearly there that her profession is listed as computer.

05:34.380 --> 05:36.380
So, she is a real computer.

05:36.380 --> 05:39.380
So, we're going to have her in this example.

05:39.380 --> 05:45.380
And what she's supposed to do is take an input and turn it into an output via some recipe.

05:45.380 --> 05:47.380
Now, how does she know what to do?

05:47.380 --> 05:48.380
Right?

05:48.380 --> 05:53.380
Those of you who already work with computers, those of you who are developers,

05:53.380 --> 05:56.380
you know that a computer needs something very important,

05:56.380 --> 05:59.380
which is an engineer to program the computer, right?

05:59.380 --> 06:01.380
And luckily Dora has an engineer also,

06:01.380 --> 06:06.380
because there you can see her husband's profession is listed as engineer.

06:06.380 --> 06:13.380
So, this is one of those early days in history where a computer has managed to marry her engineer.

06:13.380 --> 06:18.380
Now, let's consider Dora and her engineer husband.

06:18.380 --> 06:22.380
Back then in 1950, we want to get Dora to do a task,

06:22.380 --> 06:26.380
like recognize whether an image has a cat in it or not.

06:26.380 --> 06:30.380
This was a very classic computer vision task, right?

06:30.380 --> 06:32.380
The cat not cat task.

06:33.380 --> 06:36.380
Now, one way in which we could do this

06:36.380 --> 06:43.380
is to figure out the exact instructions to give Dora of what to do with the input.

06:43.380 --> 06:45.380
Now, consider what the input is.

06:45.380 --> 06:51.380
The input is a bunch of pixel color values, so it's a bunch of pixels.

06:51.380 --> 06:56.380
And the question is, what should you do with each pixel in an image

06:56.380 --> 06:59.380
to get the answer cat not cat, right?

06:59.380 --> 07:01.380
What is that recipe supposed to be?

07:01.380 --> 07:04.380
And now you're thinking philosophical things,

07:04.380 --> 07:06.380
like what makes a cat a cat?

07:06.380 --> 07:08.380
Difficult question, right?

07:08.380 --> 07:12.380
Like, what do you do with the top left-hand pixel and the one next to it?

07:12.380 --> 07:14.380
And what are you looking for in the image?

07:14.380 --> 07:18.380
Are you looking for triangles, maybe two triangles, maybe some ovals for eyes?

07:18.380 --> 07:22.380
This is a hard recipe to come up with.

07:22.380 --> 07:27.380
And so, in order to program a computer with instructions,

07:27.380 --> 07:29.380
do this, then do that, then do that,

07:29.380 --> 07:31.380
first, you have to know how to do the task,

07:31.380 --> 07:35.380
which, you know, how are you actually doing the task, though?

07:35.380 --> 07:38.380
You're doing it, your brain is doing it,

07:38.380 --> 07:41.380
but what are you actually doing with the pixels?

07:41.380 --> 07:44.380
That is so hard to put into words.

07:44.380 --> 07:46.380
You don't even know what you're doing.

07:46.380 --> 07:50.380
So how on earth are you going to program with the instructions

07:50.380 --> 07:53.380
of what to look for in each pixel?

07:53.380 --> 07:54.380
Very difficult.

07:54.380 --> 07:57.380
Now, there's a different way that we could go about this.

07:57.380 --> 08:00.380
Instead of explaining what to do with each pixel,

08:00.380 --> 08:04.380
you could instead explain your wishes with examples.

08:04.380 --> 08:06.380
Here are a bunch of examples of cat.

08:06.380 --> 08:08.380
Here are a bunch of examples of not cat.

08:08.380 --> 08:11.380
You go find the patterns,

08:11.380 --> 08:15.380
and then make a recipe automatically from those patterns

08:15.380 --> 08:19.380
so that you can take yourself from input to output.

08:19.380 --> 08:23.380
This examples versus instructions thing.

08:23.380 --> 08:27.380
This is the essence of the difference between machine learning

08:27.380 --> 08:30.380
slash AI and traditional programming.

08:30.380 --> 08:33.380
And notice that we already do this with one another as humans.

08:33.380 --> 08:36.380
Sometimes we explain our wishes with examples.

08:36.380 --> 08:39.380
Sometimes we explain our wishes with instructions.

08:39.380 --> 08:42.380
So we already teach one another one of these two ways.

08:42.380 --> 08:46.380
Now we are able to do the same thing with machines,

08:46.380 --> 08:50.380
except we need fancy words for examples and instructions.

08:50.380 --> 08:56.380
So we've got code as instructions and examples map to data.

08:56.380 --> 08:58.380
So this really is the difference

08:58.380 --> 09:01.380
between the traditional software programming approach

09:01.380 --> 09:06.380
and the AI slash machine learning programming approach.

09:06.380 --> 09:10.380
So let's make sure that we warm up this room

09:10.380 --> 09:14.380
and look at doing this cat not cat task together.

09:14.380 --> 09:19.380
So I give you a bunch of inputs with their appropriate labels.

09:19.380 --> 09:25.380
And then something in your brain figures out

09:25.380 --> 09:29.380
what those patterns are, turns that into a recipe.

09:29.380 --> 09:32.380
And then when the next one comes in, you're going to take it

09:32.380 --> 09:34.380
and you're going to convert it to the output I'm looking for.

09:34.380 --> 09:37.380
So to wake ourselves up, we're going to play this game together.

09:37.380 --> 09:42.380
I need each and every one of you to shout cat or not cat

09:42.380 --> 09:44.380
when I show you an input.

09:44.380 --> 09:45.380
Do you think you can do it?

09:45.380 --> 09:46.380
Yes?

09:46.380 --> 09:48.380
You're not loud enough for me.

09:48.380 --> 09:49.380
Yes?

09:49.380 --> 09:50.380
Yes, OK.

09:50.380 --> 09:51.380
Good.

09:51.380 --> 09:52.380
Right.

09:52.380 --> 09:53.380
So here comes the first one.

09:53.380 --> 09:54.380
Cat.

09:54.380 --> 09:55.380
Someone said yes.

09:55.380 --> 09:56.380
Excellent.

09:56.380 --> 09:57.380
Working as intended.

09:57.380 --> 10:00.380
So cat, I agree with you.

10:00.380 --> 10:02.380
Cat, see computers also make mistakes.

10:02.380 --> 10:04.380
This is what we will see as a theme here.

10:04.380 --> 10:06.380
Right, this one, I agree with you.

10:06.380 --> 10:07.380
Cat.

10:07.380 --> 10:08.380
Next.

10:08.380 --> 10:09.380
Not cat.

10:09.380 --> 10:11.380
Not cat.

10:11.380 --> 10:12.380
Not cat.

10:12.380 --> 10:13.380
Cat.

10:13.380 --> 10:14.380
Cat.

10:14.380 --> 10:15.380
Cat.

10:15.380 --> 10:16.380
Cat.

10:16.380 --> 10:17.380
Cat.

10:17.380 --> 10:19.700
Good afternoon, judges and judges.

10:19.700 --> 10:24.780
We've had a very laugh, we could have lep on you.

10:24.780 --> 10:26.420
OK, here we go.

10:26.420 --> 10:30.420
As for judges, we'll start.

10:30.420 --> 10:32.940
No, do you have miel in dazz?

10:32.940 --> 10:33.940
vog.

10:33.980 --> 10:36.660
Now, now, the thing I wanted to do.

10:36.660 --> 10:45.340
At the moment, I knew this chair in Nanаш and I didn't know the chair in Hanse.

10:45.340 --> 10:46.340
Wanlax.

10:46.420 --> 10:47.260
��重要!

10:47.260 --> 10:49.160
Sen si je jestvo invented nachodnout s n夠.

10:49.560 --> 10:52.400
Komand marched amerika je menarem za no doc,

10:52.400 --> 10:54.400
pri konar 모 trenz si jeрал.

10:56.000 --> 10:56.980
Spotn BOLE

10:58.000 --> 11:00.900
postaende idemko u vlada supermarketu,

11:01.340 --> 11:03.920
res prese баг always teber.

11:04.200 --> 11:06.380
The right answer here depends very much

11:06.380 --> 11:08.360
on the purpose of the system.

11:08.360 --> 11:10.360
What does it exist for?

11:11.320 --> 11:13.040
In so, I guess I'd better fill those big boots

11:13.040 --> 11:14.800
and I'll tell you

11:14.800 --> 11:17.240
that this is supposed to be a pet recommendations system.

11:17.260 --> 11:21.300
iz temi direktosni signedcrossne od v pagesenati.

11:21.980 --> 11:24.100
Jedob連 bomo se pristratil.

11:26.380 --> 11:27.260
N Community.

11:27.260 --> 11:28.440
EVON.

11:29.820 --> 11:44.940
ADVORTED V

11:44.940 --> 11:48.240
nismo visoki inserted, izmonživati,學om,

11:48.560 --> 11:51.880
lagske stipiot Lars van pri otro Los 2.

11:52.080 --> 11:54.600
Tudi sem bent ihtim eskatakaj.

11:54.800 --> 11:56.820
Jaz pri vsojeve providilo

11:57.020 --> 11:59.940
vse ovo, inverse drženje v taki,

12:00.140 --> 12:02.140
i kaj mi dobro komens,

12:02.340 --> 12:04.540
da jste ti s da žine.

12:04.740 --> 12:06.940
Tako bojteške investiula,

12:07.140 --> 12:08.700
je biloboardo.

12:08.900 --> 12:10.660
Toto rada ne vem za.

12:10.860 --> 12:13.300
Zvousite,To ga se od Economija Kot.

12:13.300 --> 12:18.300
in zelo je zelo tako zelo tako zelo, if we just say what this is,

12:18.300 --> 12:23.300
which is examples written down in electronic form, text books, essentially,

12:23.300 --> 12:26.300
for the machine student to learn from.

12:26.300 --> 12:29.300
And data quality is everything.

12:29.300 --> 12:32.300
If you're going to teach someone with examples,

12:32.300 --> 12:36.300
the quality of those examples matters so much.

12:36.300 --> 12:43.300
And none of this is purely objective, same meaning and answer every time,

12:43.300 --> 12:47.300
as you can see, depending on the purpose of the system,

12:47.300 --> 12:52.300
whether the right answer is cat or not cat, changes.

12:52.300 --> 12:57.300
So data is a bunch of scraps of information that we happen to write down,

12:57.300 --> 13:03.300
that we put in a textbook for a machine to learn from.

13:03.800 --> 13:09.300
Like human textbooks, normal textbooks for human students,

13:09.300 --> 13:13.300
machine textbooks, data sets have human authors.

13:13.300 --> 13:19.300
They don't arrive from aliens, they don't come from nowhere, from the universe.

13:19.300 --> 13:23.300
They are collected by us, and they fit the sensibilities

13:23.300 --> 13:29.300
of whoever is in charge of the data problem.

13:29.300 --> 13:36.300
And the trouble with them, of course, is that they reflect

13:36.300 --> 13:41.300
unconscious things we might not even have considered

13:41.300 --> 13:44.300
could be a problem when we were authoring our textbooks.

13:44.300 --> 13:47.300
So when you think about really old textbooks,

13:47.300 --> 13:50.300
and you think whether you would want to teach your children

13:50.300 --> 13:55.300
from these really old textbooks from 200 years ago, 300 years ago,

13:55.300 --> 13:58.300
you're thinking absolutely not.

13:58.300 --> 14:03.300
It doesn't matter what the title on the cover of that textbook is,

14:03.300 --> 14:07.300
chances are, if you try to learn from it,

14:07.300 --> 14:14.300
you're going to pick up some habits that are not good or useful habits.

14:14.300 --> 14:19.300
So thinking about the quality of your textbook is really, really important.

14:19.300 --> 14:22.300
And can you complete the following sentence,

14:22.300 --> 14:24.300
just to make sure that we're all on the same page?

14:24.300 --> 14:27.300
Garbage in, garbage out, you know this.

14:27.300 --> 14:31.300
So what I find very interesting about the data space,

14:31.300 --> 14:34.300
the data professions, data science,

14:34.300 --> 14:36.300
I find fascinating about data science.

14:36.300 --> 14:40.300
So when you go to a data conference or a data science conference,

14:40.300 --> 14:42.300
so sometimes I hang out with people,

14:42.300 --> 14:44.300
I'll hang out with people after this as well

14:44.300 --> 14:46.300
if you want to hang out out there.

14:46.300 --> 14:48.300
And sometimes folks come, they hang out,

14:48.300 --> 14:50.300
and they've got all different job roles,

14:50.300 --> 14:53.300
so I'll ask them, you know, what do you do professionally?

14:53.300 --> 14:56.300
One will say statistician, and that one will be data engineer,

14:56.300 --> 14:59.300
and that one will be clinical researcher, and so on.

14:59.300 --> 15:01.300
And then I'd like to do another round,

15:01.300 --> 15:03.300
and I'd like to ask them, okay,

15:03.300 --> 15:07.300
who in your organization is responsible for data quality?

15:07.300 --> 15:11.300
Who is in charge of it from data design,

15:11.300 --> 15:14.300
documentation, the cleanup,

15:14.300 --> 15:20.300
all the way through to the part where it starts hitting

15:20.300 --> 15:22.300
the pipelines that the data engineers have built.

15:22.300 --> 15:25.300
So who shapes the data set?

15:25.300 --> 15:29.300
And what I love to hear here is that as we go around,

15:29.300 --> 15:33.300
we have a very high correlation with whatever they said

15:33.300 --> 15:35.300
their own job title was.

15:35.300 --> 15:38.300
So the statistician says statisticians are responsible for it,

15:38.300 --> 15:41.300
the researchers say researchers are responsible for it,

15:41.300 --> 15:43.300
data engineers say data engineers are responsible for it.

15:43.300 --> 15:45.300
You know what that sounds like?

15:45.300 --> 15:48.300
That sounds like a situation where it's everybody's job

15:48.300 --> 15:51.300
and therefore nobody's job.

15:51.300 --> 15:54.300
In order to automate with data,

15:54.300 --> 15:56.300
you need good and appropriate data.

15:56.300 --> 15:59.300
In order to get good and appropriate data,

15:59.300 --> 16:03.300
you need an expertise in a bunch of different topics.

16:03.300 --> 16:06.300
You don't need to be a full expert in statistics, for example,

16:06.300 --> 16:09.300
but you need some expertise in statistics.

16:09.300 --> 16:12.300
You need some understanding of data engineering,

16:12.300 --> 16:14.300
some understanding of survey design,

16:14.300 --> 16:18.300
human psychology if the data sets are about humans.

16:18.300 --> 16:21.300
You need some user experience design

16:21.300 --> 16:24.300
if you are gathering that information online

16:24.300 --> 16:28.300
and how is the way that you are presenting the questions,

16:28.300 --> 16:30.300
influencing the answers that you get back.

16:30.300 --> 16:33.300
There's a lot of expertise you need.

16:33.300 --> 16:36.300
And yet where is the job role for this?

16:36.300 --> 16:39.300
Where is the profession that takes this seriously?

16:39.300 --> 16:43.300
And I had a really terrible aha moment with this.

16:43.300 --> 16:47.300
I was hanging out with a data science influencer, as one does.

16:48.300 --> 16:52.300
And I was talking about how this is a problem,

16:52.300 --> 16:57.300
that we are building our disciplines on a foundation of data,

16:57.300 --> 17:02.300
and yet the quality of that data is no one's job.

17:02.300 --> 17:04.300
I was saying this is so important,

17:04.300 --> 17:09.300
maybe instead of over focusing on this last mile thing,

17:09.300 --> 17:14.300
maybe we should put more effort as a profession,

17:14.300 --> 17:18.300
in the industry in the first bit, the actual data quality.

17:18.300 --> 17:21.300
I was saying we need to encourage university graduates

17:21.300 --> 17:23.300
to study this and to take this seriously.

17:23.300 --> 17:25.300
There needs to be a career progression,

17:25.300 --> 17:29.300
a way that motivates you to actually want to learn

17:29.300 --> 17:31.300
all those things, to do it professionally,

17:31.300 --> 17:34.300
because there's a lot to learn.

17:34.300 --> 17:37.300
And then I ask this friend of mine,

17:37.300 --> 17:40.300
and we're live streaming, this is what makes it best,

17:40.300 --> 17:43.300
we're live streaming in this moment, I'm talking about it.

17:43.300 --> 17:46.300
And I ask, so what do you think we should,

17:46.300 --> 17:49.300
what should this be called, what is this called?

17:49.300 --> 17:54.300
And my friend goes, oh that sounds like a data janitor.

17:59.300 --> 18:03.300
Is this how we're going to motivate our undergraduates

18:03.300 --> 18:06.300
to go to university, and they're picking their major,

18:06.300 --> 18:08.300
they're deciding what to study,

18:08.300 --> 18:10.300
and then they call their parents and they say,

18:10.300 --> 18:12.300
I've picked one.

18:12.300 --> 18:16.300
I would like to go through a hard grueling training program

18:16.300 --> 18:18.300
to be a data janitor.

18:18.300 --> 18:21.300
Are you proud of me, mom and dad?

18:21.300 --> 18:24.300
Right, that's not a good start.

18:24.300 --> 18:26.300
And it's not a good start

18:26.300 --> 18:29.300
when data seems to be everybody else's job.

18:29.300 --> 18:33.300
So we have quite a brittle profession here,

18:33.300 --> 18:36.300
because a lot of it is based on the hope

18:36.300 --> 18:38.300
that someone is going to do a job

18:38.300 --> 18:40.300
that they didn't train for,

18:40.300 --> 18:43.300
and that we're almost surely not paying them properly for.

18:43.300 --> 18:45.300
We should worry about this.

18:45.300 --> 18:47.300
And we should also remember

18:47.300 --> 18:49.300
that a lot of the data that we wish we had,

18:49.300 --> 18:52.300
or the quality that we wish we had it at,

18:52.300 --> 18:57.300
won't exist if we have this basic problem of economics.

18:57.300 --> 18:59.300
So that's our first thing.

18:59.300 --> 19:03.300
Data are not objective, they are subjective.

19:03.300 --> 19:05.300
The design matters.

19:06.300 --> 19:10.300
And even though we rely so much on data for automation,

19:10.300 --> 19:14.300
there's not that good of a plan in the data professions.

19:14.300 --> 19:16.300
So data quality is everything.

19:16.300 --> 19:20.300
That is the first point I really want to hammer home here.

19:20.300 --> 19:22.300
And I know a lot of you in the audience

19:22.300 --> 19:25.300
think you've heard this all before and you get it.

19:25.300 --> 19:28.300
But if you did,

19:28.300 --> 19:33.300
wouldn't there be better progress in the industry

19:33.300 --> 19:36.300
to motivate, fund and compensate people

19:36.300 --> 19:40.300
whose job the data quality actually is?

19:40.300 --> 19:44.300
And then let's talk a little bit about the internet

19:44.300 --> 19:47.300
as a data source.

19:47.300 --> 19:51.300
Like that is a source of mirrors, isn't it?

19:51.300 --> 19:54.300
Kind of reflects reality a little bit,

19:54.300 --> 19:56.300
but you get a skewed perspective.

19:56.300 --> 20:00.300
Never forget that the internet is not reality.

20:00.300 --> 20:04.300
How you behave online isn't how you behave

20:04.300 --> 20:07.300
in your natural surroundings.

20:07.300 --> 20:10.300
And we know what kind of stuff lives on the internet

20:10.300 --> 20:13.300
in the parts where people can be anonymous, right?

20:13.300 --> 20:17.300
It's not necessarily bringing the best of us to anything.

20:17.300 --> 20:19.300
So we need to be quite careful

20:19.300 --> 20:22.300
with what we allow ourselves to do

20:22.300 --> 20:25.300
on the basis of wild type data.

20:25.300 --> 20:29.300
Now back to our question of whose job AI automates.

20:29.300 --> 20:34.300
Let's look again at the broader category of AI.

20:34.300 --> 20:36.300
By the way, I'm using AI and machine learning

20:36.300 --> 20:39.300
somewhat interchangeably here, because I've given up.

20:39.300 --> 20:42.300
Once upon a time, AI used to be the superset,

20:42.300 --> 20:44.300
then machine learning was the subset,

20:44.300 --> 20:47.300
then at some point machine learning was the superset

20:47.300 --> 20:50.300
and AI was the subset, something, something.

20:50.300 --> 20:52.300
If it's deep learning, then it's AI.

20:52.300 --> 20:54.300
I give up.

20:54.300 --> 20:57.300
Honestly, there was a set of cycles

20:57.300 --> 20:59.300
in funding and disappointment,

20:59.300 --> 21:02.300
where you got the funding if you said AI

21:02.300 --> 21:04.300
and then things didn't work out,

21:04.300 --> 21:07.300
so then you started saying something else, machine learning,

21:07.300 --> 21:09.300
and then the funding didn't work out there,

21:09.300 --> 21:11.300
and so we went up and down in these cycles.

21:11.300 --> 21:13.300
Like bell bottoms and skinny jeans, right?

21:13.300 --> 21:16.300
Like it's the fashion of what we're gonna call it.

21:16.300 --> 21:18.300
So actually my favorite definition

21:18.300 --> 21:21.300
of the difference between AI and machine learning

21:21.300 --> 21:25.300
is if it is written in Python,

21:25.300 --> 21:27.300
it's probably machine learning,

21:27.300 --> 21:30.300
and if it's written in PowerPoint, it's probably AI.

21:30.300 --> 21:33.300
So, I'm using them interchangeably, the hell with it.

21:33.300 --> 21:35.300
So whose job does AI automate?

21:35.300 --> 21:37.300
Let's look carefully at what this is

21:37.300 --> 21:39.300
as an automation proposition.

21:39.300 --> 21:42.300
When I'm automating the traditional way,

21:42.300 --> 21:45.300
first, I have to know how to do the task

21:45.300 --> 21:47.300
so that I can explain to you

21:47.300 --> 21:49.300
what precisely you need to do with each input

21:49.300 --> 21:51.300
to get the output.

21:51.300 --> 21:54.300
Second, I have to think about every little instruction

21:54.300 --> 21:56.300
and then I have to write it down,

21:56.300 --> 21:59.300
and first I can write it down for myself in pseudocode

21:59.300 --> 22:02.300
or, you know, English or whatever language I speak,

22:02.300 --> 22:04.300
and then I have to translate it into something

22:04.300 --> 22:06.300
the computer can understand.

22:06.300 --> 22:08.300
But I have to deal with every single line,

22:08.300 --> 22:10.300
and maybe it takes 10,000 lines,

22:10.300 --> 22:12.300
maybe it takes 100,000 lines of code

22:12.300 --> 22:15.300
to automate my task, I can write each one down.

22:15.300 --> 22:18.300
And you might be saying, oh no, maybe it's,

22:18.300 --> 22:21.300
maybe I just get a package somewhere,

22:21.300 --> 22:24.300
I find some library, I install something,

22:24.300 --> 22:26.300
and then I just pull from there

22:26.300 --> 22:29.300
and I don't have to write the code by hand myself.

22:29.300 --> 22:32.300
Sure, but some member of our species

22:32.300 --> 22:34.300
had to do it.

22:34.300 --> 22:36.300
So some human is responsible

22:36.300 --> 22:39.300
for having thought through all those instructions,

22:39.300 --> 22:42.300
whereas with machine learning and AI,

22:42.300 --> 22:47.300
there are just two lines.

22:47.300 --> 22:50.300
Optimize this goal on that dataset

22:50.300 --> 22:52.300
go.

22:52.300 --> 22:54.300
Now those of you who raised your hand for AI professional,

22:54.300 --> 22:58.300
you know there's a lot more code that you're writing.

22:58.300 --> 23:02.300
But that is because the tools are nasty.

23:02.300 --> 23:06.300
At the core, there are only these two lines of instructions.

23:06.300 --> 23:09.300
What does success look like?

23:09.300 --> 23:13.300
What data should we point this pattern finding thingy at,

23:13.300 --> 23:15.300
and off we go.

23:15.300 --> 23:18.300
And really, if you had the ability to brute force it,

23:18.300 --> 23:20.300
and had enough computing power,

23:20.300 --> 23:22.300
you could try every known algorithm

23:22.300 --> 23:24.300
with every permutation of it pretty much,

23:24.300 --> 23:26.300
quickly eliminate some,

23:26.300 --> 23:28.300
try everything out,

23:28.300 --> 23:30.300
subject to just the two important lines of instruction,

23:30.300 --> 23:32.300
optimize this goal on that dataset.

23:32.300 --> 23:36.300
And as the tools become easier and easier,

23:36.300 --> 23:39.300
we will strip away all the huffing and puffing

23:39.300 --> 23:42.300
and the difficulty of forcing a dataset in this format

23:42.300 --> 23:44.300
to be taken up by an algorithm

23:44.300 --> 23:47.300
that was designed over there.

23:47.300 --> 23:50.300
You know, some of these tools are really,

23:50.300 --> 23:53.300
only a mother could love them.

23:53.300 --> 23:56.300
And you're left with just these two lines,

23:56.300 --> 23:59.300
which means that almost anyone then

23:59.300 --> 24:05.300
will be able to automate a task.

24:05.300 --> 24:07.300
Ha!

24:07.300 --> 24:10.300
What do we see here?

24:10.300 --> 24:15.300
First, two very subjective lines.

24:15.300 --> 24:17.300
What is the goal of the system?

24:17.300 --> 24:19.300
What does success look like?

24:19.300 --> 24:21.300
Why am I building this pet classifier

24:21.300 --> 24:23.300
that does cat not cat?

24:23.300 --> 24:27.300
And why should Tiger be labeled cat versus not cat?

24:27.300 --> 24:29.300
Well, vice versa.

24:29.300 --> 24:32.300
There's no one single right way to do that.

24:32.300 --> 24:34.300
What about scoring mistakes?

24:34.300 --> 24:37.300
That's all part of how we're gonna express our goal.

24:37.300 --> 24:40.300
Which mistakes are worse than which other mistakes?

24:40.300 --> 24:42.300
Again, highly subjective.

24:42.300 --> 24:46.300
So, which textbook shall we learn from?

24:46.300 --> 24:49.300
There are a lot of different textbook choices you could use.

24:49.300 --> 24:51.300
You could also edit and modify

24:51.300 --> 24:53.300
and get different versions of the textbooks.

24:53.300 --> 24:56.300
And all of this is highly subjective.

24:56.300 --> 24:59.300
But now, available,

24:59.300 --> 25:02.300
just two lines and you can automate your task.

25:02.300 --> 25:05.300
How wonderful and how terrifying simultaneously.

25:05.300 --> 25:09.300
This is both the peril and the promise of AI.

25:09.300 --> 25:13.300
The promise is if I'm doing a little task myself,

25:13.300 --> 25:17.300
I can now automate it very quickly.

25:17.300 --> 25:18.300
How great for me?

25:18.300 --> 25:23.300
I don't have to go and write everything from scratch.

25:23.300 --> 25:25.300
But at the same time,

25:25.300 --> 25:27.300
what if I am automating something

25:27.300 --> 25:30.300
on behalf of millions or billions of people?

25:30.300 --> 25:33.300
What if my code's gonna touch a lot of lives?

25:33.300 --> 25:35.300
Well, then I can, again,

25:35.300 --> 25:37.300
without thinking too hard about it,

25:37.300 --> 25:39.300
get it automated.

25:39.300 --> 25:41.300
So, we have a thoughtlessness enabler here.

25:41.300 --> 25:43.300
We can be more thoughtless

25:43.300 --> 25:45.300
and we can automate thoughtlessly.

25:45.300 --> 25:48.300
Which is great when it only affects you.

25:48.300 --> 25:51.300
But when we start scaling that up,

25:51.300 --> 25:53.300
we can do damage.

25:53.300 --> 25:56.300
This is like a proliferation of magic lamps.

25:56.300 --> 25:58.300
Lamps with genies.

25:58.300 --> 26:01.300
And knowing how to make a wish responsibly

26:01.300 --> 26:03.300
is a very important skill.

26:03.300 --> 26:05.300
It's the skill of decision leadership.

26:05.300 --> 26:08.300
We're not even talking about this, though.

26:08.300 --> 26:10.300
We're not asking ourselves,

26:10.300 --> 26:12.300
who is it?

26:12.300 --> 26:14.300
Who has the skills on our team

26:14.300 --> 26:17.300
to figure out what success should look like?

26:17.300 --> 26:21.300
How do we carefully state what we're looking for?

26:21.300 --> 26:24.300
What do we actually want to create in the world?

26:24.300 --> 26:26.300
And what would be the consequences

26:26.300 --> 26:28.300
if we got what we asked for?

26:28.300 --> 26:30.300
And which data is appropriate and why?

26:30.300 --> 26:32.300
And what would need to be true about that data

26:32.300 --> 26:34.300
for us to wanna use it?

26:34.300 --> 26:37.300
Very, very subjective questions

26:37.300 --> 26:40.300
that very few people are trained to answer.

26:40.300 --> 26:42.300
So you should worry

26:42.300 --> 26:44.300
who is actually being tasked

26:44.300 --> 26:47.300
with doing this for massive systems.

26:47.300 --> 26:50.300
Do they have the skills to do it?

26:50.300 --> 26:52.300
And as you see,

26:52.300 --> 26:54.300
the tools get easier and easier,

26:54.300 --> 26:57.300
you'll see a shift from a focus on

26:57.300 --> 27:00.300
huffing and puffing and actually getting

27:00.300 --> 27:03.300
the data to be taken up by the algorithm

27:03.300 --> 27:05.300
and then deployed to production,

27:05.300 --> 27:07.300
and a lot more focus on

27:07.300 --> 27:09.300
how do we put 10,000 lines

27:09.300 --> 27:11.300
or 100,000 lines of thought

27:11.300 --> 27:14.300
back into these two lines.

27:14.300 --> 27:17.300
We've allowed ourselves to be thoughtless,

27:17.300 --> 27:19.300
but on some things, that's not okay.

27:19.300 --> 27:21.300
So how do we put that thought back in?

27:21.300 --> 27:23.300
How do we very carefully design

27:23.300 --> 27:26.300
systems that can affect society at scale?

27:26.300 --> 27:28.300
But back to the question

27:28.300 --> 27:31.300
of whose job are we actually automating here?

27:31.300 --> 27:34.300
Well, it is the developer's job.

27:34.300 --> 27:37.300
We're going from having to write instructions

27:37.300 --> 27:39.300
to now being able to say,

27:39.300 --> 27:41.300
instead of knowing how to do the task,

27:41.300 --> 27:45.300
here's the objective, here's the data, go.

27:45.300 --> 27:48.300
That said, it's not like we're putting

27:48.300 --> 27:50.300
software developers out of business.

27:50.300 --> 27:52.300
First, there's still a lot of huffing

27:52.300 --> 27:54.300
and puffing to do to get the algorithms

27:54.300 --> 27:57.300
to accept those instructions

27:57.300 --> 27:59.300
and the data.

27:59.300 --> 28:02.300
Second, we are actually unlocking

28:02.300 --> 28:05.300
a whole class of new applications.

28:05.300 --> 28:09.300
And all the old approaches

28:09.300 --> 28:13.300
are still going to be very economically necessary.

28:13.300 --> 28:15.300
Why?

28:15.300 --> 28:17.300
If you are able to automate your task

28:17.300 --> 28:19.300
with instructions,

28:19.300 --> 28:21.300
that is how you should do it.

28:21.300 --> 28:23.300
That's how you get the most control.

28:23.300 --> 28:25.300
If you're able to say what needs to be done

28:25.300 --> 28:27.300
in what order,

28:27.300 --> 28:29.300
and you give those instructions to your machine

28:29.300 --> 28:32.300
or to your human employee,

28:32.300 --> 28:35.300
you can be sure of what that person

28:35.300 --> 28:37.300
is going to do next

28:37.300 --> 28:39.300
if they're following the instructions.

28:39.300 --> 28:41.300
Exactly what you've told them to do.

28:41.300 --> 28:43.300
No guessing.

28:43.300 --> 28:46.300
No surprise ways that they interpreted anything.

28:46.300 --> 28:49.300
Just follow those instructions.

28:49.300 --> 28:51.300
Whereas, if you know how to give the instructions,

28:51.300 --> 28:54.300
but instead you give a few examples,

28:54.300 --> 28:56.300
who knows what they're going to learn

28:56.300 --> 28:58.300
in those examples?

28:58.300 --> 29:00.300
Maybe they'll learn the right thing,

29:00.300 --> 29:02.300
maybe they won't.

29:02.300 --> 29:04.300
And mistakes are possible.

29:04.300 --> 29:06.300
That's true with humans,

29:06.300 --> 29:08.300
that's also true with these AI systems.

29:08.300 --> 29:10.300
So why are we using them?

29:10.300 --> 29:13.300
To automate things we can't automate the other way.

29:13.300 --> 29:16.300
So we're not putting developers out of business.

29:16.300 --> 29:18.300
Everything developers used to do

29:18.300 --> 29:20.300
and used to be able to do,

29:20.300 --> 29:22.300
you're still going to want to do that

29:22.300 --> 29:24.300
in the old traditional way.

29:24.300 --> 29:27.300
But now we've got a whole new class of applications.

29:27.300 --> 29:30.300
And let's talk about a new new class of applications.

29:30.300 --> 29:32.300
The two different AIs.

29:32.300 --> 29:35.300
So this year we're talking a lot about AI.

29:35.300 --> 29:37.300
We tend,

29:37.300 --> 29:39.300
when we find ourselves hanging out with friends

29:39.300 --> 29:41.300
and having a glass of wine

29:41.300 --> 29:44.300
and talking about all these new things in AI in 2023,

29:44.300 --> 29:47.300
we tend to be talking about generative AI.

29:47.300 --> 29:50.300
So let's remind ourselves very quickly of the difference.

29:50.300 --> 29:52.300
So discriminative AI, the old one,

29:52.300 --> 29:55.300
the one you're used to from last decade,

29:55.300 --> 29:58.300
that is all about applying a label.

29:58.300 --> 30:00.300
So this is a thing labeler.

30:00.300 --> 30:02.300
We had the cat not cat example of that.

30:02.300 --> 30:06.300
Here's another classic again with vision.

30:06.300 --> 30:08.300
So I really like this tweet.

30:08.300 --> 30:10.300
It comes from BJM,

30:10.300 --> 30:12.300
who complained that he was locked out

30:12.300 --> 30:15.300
because his smart front door lock,

30:15.300 --> 30:18.300
his nest camera system locked him out

30:18.300 --> 30:22.300
and he was protecting him from Batman.

30:22.300 --> 30:24.300
It didn't want to let Batman in the house,

30:24.300 --> 30:26.300
so it locked poor BJM out.

30:26.300 --> 30:28.300
So it's supposed to find the right answer.

30:28.300 --> 30:30.300
It doesn't always work correctly.

30:30.300 --> 30:32.300
These systems do make mistakes.

30:32.300 --> 30:35.300
And it's really important for designers to remember this,

30:35.300 --> 30:38.300
because imagine what would have happened to poor BJM

30:38.300 --> 30:40.300
if there wasn't a plan for mistakes.

30:40.300 --> 30:43.300
He wouldn't have gotten back into his house.

30:43.300 --> 30:46.300
Instead he can put user pin to get himself in

30:46.300 --> 30:49.300
because the engineers built that safety net

30:49.300 --> 30:51.300
and knew that mistakes were possible.

30:51.300 --> 30:53.300
So that's thing labelers.

30:53.300 --> 30:55.300
On the other hand, generative AI

30:55.300 --> 30:57.300
is about creating a plausible exemplar.

30:57.300 --> 30:58.300
What are we learning?

30:58.300 --> 31:00.300
Not a label, but a distribution.

31:00.300 --> 31:02.300
And what can you do with a distribution

31:02.300 --> 31:05.300
is create a really good fake.

31:05.300 --> 31:07.300
This is a fake maker.

31:07.300 --> 31:09.300
So as you all know,

31:09.300 --> 31:12.300
Picasso is very famous for his paintings of laptops.

31:12.300 --> 31:14.300
So I have some examples for you there

31:14.300 --> 31:16.300
in the top right-hand corner.

31:16.300 --> 31:19.300
This is where I'm using an image generation tool

31:19.300 --> 31:21.300
called Mid Journey.

31:21.300 --> 31:23.300
Mid Journey is my favorite casino.

31:23.300 --> 31:25.300
I really enjoy playing with Mid Journey.

31:25.300 --> 31:27.300
It gives you, you put in a prompt,

31:27.300 --> 31:30.300
it gives you four options back,

31:30.300 --> 31:31.300
and maybe you like it,

31:31.300 --> 31:33.300
maybe you rerun the prompt

31:33.300 --> 31:35.300
until you get something that you like.

31:35.300 --> 31:39.300
I've also created some fake Gucci sunglasses for you.

31:39.300 --> 31:42.300
So both of these aren't,

31:42.300 --> 31:44.300
you know, it's not real Picasso.

31:44.300 --> 31:47.300
This doesn't really exist out there.

31:47.300 --> 31:52.300
We are generating from a distribution of plausible Picasso type

31:52.300 --> 31:55.300
and laptop type things

31:55.300 --> 31:59.300
to get this lovely fake for you.

31:59.300 --> 32:03.300
So it's a game of plausible exemplars.

32:03.300 --> 32:07.300
And people ask a question that bugs me so much

32:07.300 --> 32:09.300
when they see this stuff.

32:09.300 --> 32:11.300
They ask, can AI be creative?

32:11.300 --> 32:13.300
Does this mean that AI is the artist?

32:13.300 --> 32:17.300
Is AI making art?

32:17.300 --> 32:20.300
And then I have to remind folks

32:20.300 --> 32:23.300
of an entire century of art history.

32:23.300 --> 32:25.300
Because if you're asking questions like this,

32:25.300 --> 32:27.300
you must have missed something

32:27.300 --> 32:30.300
from art history from the 20th century.

32:30.300 --> 32:32.300
So let's go back to 1917.

32:32.300 --> 32:35.300
Marcel Duchamp found this iconic piece.

32:35.300 --> 32:38.300
This is considered iconic in the art world.

32:38.300 --> 32:40.300
This is a urinal.

32:40.300 --> 32:42.300
He signed some name on it,

32:42.300 --> 32:44.300
not even his own name.

32:44.300 --> 32:47.300
He took it to the exhibition when that's art.

32:47.300 --> 32:51.300
And it was, we consider this a very interesting piece.

32:51.300 --> 32:53.300
It's worth a lot.

32:56.300 --> 32:57.300
Is it art?

32:57.300 --> 32:58.300
Sure.

32:58.300 --> 32:59.300
Why?

32:59.300 --> 33:01.300
Because art is a conversation

33:01.300 --> 33:03.300
that humanity is having with itself

33:03.300 --> 33:05.300
and has been having for millennia.

33:05.300 --> 33:06.300
And to make art,

33:06.300 --> 33:08.300
we consider the next sentence

33:08.300 --> 33:11.300
in this grand conversation.

33:11.300 --> 33:13.300
But who is the artist?

33:13.300 --> 33:16.300
I would say Duchamp, let's give him credit.

33:16.300 --> 33:18.300
Because otherwise, where are we going to put the credit?

33:18.300 --> 33:20.300
On the porcelain makers,

33:20.300 --> 33:21.300
the toilet company,

33:21.300 --> 33:23.300
that doesn't make any sense to me.

33:23.300 --> 33:25.300
And should we penalize him

33:25.300 --> 33:28.300
because he didn't sculpt it from scratch,

33:28.300 --> 33:31.300
mixing his own materials,

33:31.300 --> 33:32.300
creating his own porcelain?

33:32.300 --> 33:33.300
Not at all.

33:33.300 --> 33:35.300
This cut out a lot of toil

33:35.300 --> 33:38.300
because he didn't know what he wanted to say much faster.

33:38.300 --> 33:41.300
Generative AI plays the same role.

33:41.300 --> 33:43.300
Like a paintbrush,

33:43.300 --> 33:46.300
it's a tool for you to be able to say

33:46.300 --> 33:48.300
what you need to say faster and better.

33:48.300 --> 33:53.300
And the secret behind a lot of these generative AI art things

33:53.300 --> 33:55.300
is that it's very rare

33:55.300 --> 33:59.300
that the first one is the one that's presented to the audience.

33:59.300 --> 34:03.300
So AI made art that won some art competition.

34:03.300 --> 34:06.300
The 8,000th iteration,

34:06.300 --> 34:10.300
a person cranked the handle on these tools

34:10.300 --> 34:12.300
8,000 times,

34:12.300 --> 34:15.300
500 times, however many times it took

34:15.300 --> 34:18.300
to get the one that they were looking for,

34:18.300 --> 34:20.300
to express what they wanted to express.

34:20.300 --> 34:22.300
So where is the creativity?

34:22.300 --> 34:23.300
It's in the human.

34:23.300 --> 34:26.300
But the human can go a little bit faster.

34:26.300 --> 34:28.300
They don't need to mix paints.

34:28.300 --> 34:30.300
We don't penalize artists today

34:30.300 --> 34:32.300
for not mixing their own paints

34:32.300 --> 34:34.300
the way that they would have in the Middle Ages.

34:34.300 --> 34:36.300
Making their own paintbrushes

34:36.300 --> 34:38.300
out of horse hair or whatever it is.

34:38.300 --> 34:40.300
You go to the shop, you buy some paintbrushes,

34:40.300 --> 34:42.300
you buy some paint, and you go paint.

34:42.300 --> 34:43.300
That's great.

34:43.300 --> 34:45.300
That lets us get there faster.

34:45.300 --> 34:48.300
And that's what generative AI allows you to do as well.

34:48.300 --> 34:50.300
What are some other things you can do?

34:50.300 --> 34:53.300
So good old open AI all over the news.

34:53.300 --> 34:55.300
Open chat GPT.

34:55.300 --> 34:58.300
I have in audiences like this,

34:58.300 --> 35:01.300
I have asked who here has never used chat GPT.

35:01.300 --> 35:03.300
And so I'm going to stop embarrassing audiences

35:03.300 --> 35:05.300
because there does tend to be one hand.

35:05.300 --> 35:08.300
And then I ask, OK, who here has never read about chat GPT

35:08.300 --> 35:10.300
and that no hands go up?

35:10.300 --> 35:12.300
And I think what a strange equation.

35:12.300 --> 35:15.300
It is faster to try it than to read about it.

35:15.300 --> 35:18.300
So why did my one hand, one or two hands

35:18.300 --> 35:20.300
read about it without trying it?

35:20.300 --> 35:21.300
You may as well just try it.

35:21.300 --> 35:22.300
The interface is super easy.

35:22.300 --> 35:24.300
It's like sending a text message

35:24.300 --> 35:26.300
and you type whatever you want to type.

35:26.300 --> 35:30.300
This is when we asked CEOs

35:30.300 --> 35:33.300
what they personally use chat GPT for the most.

35:33.300 --> 35:37.300
One of the favorite answers was to write a retirement poem

35:37.300 --> 35:38.300
or a birthday poem.

35:38.300 --> 35:43.300
So it's really getting used for its top applications.

35:43.300 --> 35:46.300
But here I've asked it to write a funny retirement poem

35:46.300 --> 35:48.300
for your CEO in the style of Dr. Seuss.

35:48.300 --> 35:49.300
And what does it give us back?

35:49.300 --> 35:51.300
You've been the big cheese, the head of the pack.

35:51.300 --> 35:54.300
Now it's your time to kick back and slack.

35:54.300 --> 36:01.300
So definitely the highest in what we could possibly want

36:01.300 --> 36:02.300
out of our technology.

36:02.300 --> 36:03.300
Let's try another one.

36:03.300 --> 36:07.300
This is an application that OpenAI found

36:07.300 --> 36:13.300
and noticed was a statement perhaps about the human condition.

36:13.300 --> 36:16.300
So a lot of people like to take bullet points

36:16.300 --> 36:22.300
and then ask OpenAI, chat GPT, to turn that into an email,

36:22.300 --> 36:23.300
a full email.

36:23.300 --> 36:26.300
So here's some summaries, expand that out into an email.

36:26.300 --> 36:29.300
And what OpenAI found was that this was a popular use case,

36:29.300 --> 36:32.300
as was this other use case,

36:32.300 --> 36:35.300
which was summarize this email as bullet points

36:35.300 --> 36:37.300
back into the original.

36:37.300 --> 36:41.300
So I think that does tell us something a little bit sad

36:41.300 --> 36:43.300
and funny about how humans work.

36:43.300 --> 36:45.300
Wouldn't it be great if all of our emails

36:45.300 --> 36:49.300
could just be bullet points in the first place?

36:49.300 --> 36:53.300
But what we're seeing here with generative AI

36:53.300 --> 36:56.300
is a new kind of user interaction.

36:56.300 --> 36:59.300
It is AI as a raw material.

36:59.300 --> 37:01.300
AI, for AI's sake,

37:01.300 --> 37:06.300
given to you the user to do anything you want with Next.

37:06.300 --> 37:10.300
So we have the ability to find the right distributions,

37:10.300 --> 37:13.300
to pull laptops by Picasso out,

37:13.300 --> 37:16.300
or Gucci sunglasses, or whatever else you want.

37:16.300 --> 37:18.300
But now we're giving you the tool,

37:18.300 --> 37:21.300
you figure out what you want to shape it into.

37:21.300 --> 37:26.300
And when we talk about regulating generative AI,

37:26.300 --> 37:31.300
I hope you can appreciate now how hard this is,

37:31.300 --> 37:35.300
because we are not good at regulating raw materials,

37:35.300 --> 37:37.300
even physical raw materials.

37:37.300 --> 37:39.300
Whose fault is it?

37:39.300 --> 37:43.300
If I invent a phenomenal anti-gravity material,

37:43.300 --> 37:45.300
that could be pretty cool for humanity,

37:45.300 --> 37:48.300
but some idiot's gonna make skis out of it.

37:48.300 --> 37:50.300
So whose fault is it, then,

37:50.300 --> 37:53.300
when they go and ski in anti-gravity skis and hurt themselves?

37:53.300 --> 37:56.300
Was it me for making the material?

37:56.300 --> 37:59.300
Was it whoever helped them fashion the skis?

37:59.300 --> 38:02.300
Or was it, then, the user of those skis?

38:02.300 --> 38:04.300
Who is responsible?

38:04.300 --> 38:07.300
How do we limit what the uses of it are

38:07.300 --> 38:10.300
that would be okay versus not okay?

38:10.300 --> 38:13.300
This is a hard problem, hard with physical materials

38:13.300 --> 38:15.300
when it comes to digital raw materials.

38:15.300 --> 38:17.300
Good luck.

38:17.300 --> 38:19.300
Really hard to figure out how to regulate.

38:19.300 --> 38:24.300
And when I hear that a problem is really hard,

38:24.300 --> 38:28.300
the last thing that I want is for us, then,

38:28.300 --> 38:31.300
to solve it in some dumb way just to say we've solved it

38:31.300 --> 38:35.300
so we can move on in our to-do list.

38:35.300 --> 38:38.300
Solving AI regulation here is difficult,

38:38.300 --> 38:41.300
which means that maybe we shouldn't get ahead of ourselves

38:41.300 --> 38:44.300
and make a bunch of laws we haven't thought through,

38:44.300 --> 38:47.300
maybe go slowly and think about the consequences

38:47.300 --> 38:50.300
of regulating, maybe request a bunch of information

38:50.300 --> 38:53.300
that would help you regulate it later.

38:53.300 --> 38:56.300
So, it is a very, very difficult problem.

38:56.300 --> 39:00.300
Then, another kind of application

39:00.300 --> 39:03.300
is all kinds of translation-type stuff.

39:03.300 --> 39:06.300
So here I have asked it to write the FORTRAN code

39:06.300 --> 39:09.300
for generating the Fibonacci sequence.

39:09.300 --> 39:12.300
And I do not speak or understand FORTRAN,

39:12.300 --> 39:15.300
so, hopefully, someone in the room can look at this

39:15.300 --> 39:18.300
and see if it's right or not.

39:18.300 --> 39:21.300
What I can do is I know what the Fibonacci sequence is

39:21.300 --> 39:23.300
and I can write out those instructions.

39:23.300 --> 39:26.300
I could also have written them out the way that I want

39:26.300 --> 39:28.300
and ask for it to translate that to FORTRAN.

39:28.300 --> 39:31.300
But here's a little quick bit of trouble.

39:31.300 --> 39:33.300
Anyone here going to fess up,

39:33.300 --> 39:37.300
going to confess with me that you don't speak FORTRAN?

39:38.300 --> 39:40.300
Right, so, I'm seeing some hands.

39:40.300 --> 39:44.300
So, imagine that we asked for this lovely FORTRAN code

39:44.300 --> 39:47.300
for generating the Fibonacci sequence and we get it.

39:47.300 --> 39:51.300
Do we take this and plug this directly into our codebase

39:51.300 --> 39:55.300
when we don't understand what the hell it is?

39:55.300 --> 39:57.300
Terrifying.

39:57.300 --> 40:02.300
So, with this one, OK, maybe we know how to be software engineers.

40:02.300 --> 40:04.300
I would figure out how to make some unit tests here.

40:04.300 --> 40:07.300
Maybe I would line by line try to figure out what I'm looking at.

40:07.300 --> 40:11.300
But you can see the more code that I generate with this

40:11.300 --> 40:14.300
in languages that I don't speak or understand,

40:14.300 --> 40:18.300
the more space I'm leaving for potentially catastrophic disasters

40:18.300 --> 40:24.300
as I plug this in to an already complicated system.

40:24.300 --> 40:28.300
This is why people are saying that good developers

40:28.300 --> 40:31.300
are becoming much better, the estimates coming from McKinsey

40:31.300 --> 40:34.300
of 50% better, if you're a good engineer,

40:34.300 --> 40:38.300
then this can cut out a lot of drudgery.

40:38.300 --> 40:42.300
But bad engineers are becoming worse.

40:42.300 --> 40:46.300
And bad teams are reducing corporate productivity

40:46.300 --> 40:50.300
because they're plugging all kinds of nonsense into their systems.

40:50.300 --> 40:56.300
So, in general here, people who are already highly productive

40:56.300 --> 40:59.300
are making themselves more productive.

40:59.300 --> 41:01.300
They understand the output.

41:01.300 --> 41:04.300
They understand how to put it together into solutions.

41:04.300 --> 41:07.300
But those who are on the less productive side

41:07.300 --> 41:09.300
or they don't know what they're working with

41:09.300 --> 41:11.300
or they just believe AI is magic

41:11.300 --> 41:13.300
and they plug things in where they shouldn't,

41:13.300 --> 41:16.300
they are reducing everybody's productivity.

41:16.300 --> 41:18.300
So, that's a point worth thinking about.

41:18.300 --> 41:21.300
And I wonder if it would surprise anybody here

41:21.300 --> 41:24.300
that I do not, in fact, speak Serbian.

41:24.300 --> 41:28.300
So, imagine if I had a really, really important email

41:28.300 --> 41:34.300
that I needed to write to someone in Serbian

41:34.300 --> 41:37.300
where I really care about my reputation and that relationship.

41:37.300 --> 41:40.300
So, I just go straight to open AI.

41:40.300 --> 41:43.300
I ask for that email to be translated.

41:43.300 --> 41:47.300
I get something out of there and I send it.

41:47.300 --> 41:49.300
What a disaster that could be.

41:49.300 --> 41:51.300
Maybe it's correct.

41:51.300 --> 41:53.300
But if I have no way of checking it,

41:53.300 --> 41:55.300
I'm gonna have problems.

41:55.300 --> 41:58.300
And this brings us to why it's so difficult to scale

41:58.300 --> 42:01.300
generative AI in the enterprise.

42:01.300 --> 42:03.300
To use it for personal productivity

42:03.300 --> 42:07.300
and to make an individual responsible for the output

42:07.300 --> 42:09.300
is quite an easy one

42:09.300 --> 42:13.300
if you already have a smart and productive person.

42:13.300 --> 42:18.300
But when you think about taking the person out of the loop

42:18.300 --> 42:21.300
and then at scale automating some processes,

42:21.300 --> 42:26.300
remember, these systems do make mistakes.

42:26.300 --> 42:30.300
It may be hard for humans to check.

42:30.300 --> 42:34.300
It may be hard to even define whether that output was right or not.

42:34.300 --> 42:41.300
So, how do you set up at scale this kind of automation?

42:41.300 --> 42:43.300
From now on, we're automatically going to write

42:43.300 --> 42:46.300
all our emails in Serbian with this system.

42:46.300 --> 42:49.300
You're gonna have to test the hell out of it.

42:49.300 --> 42:52.300
And that's what a lot of companies don't know how to do.

42:52.300 --> 42:56.300
So, no wonder we're getting this bottleneck in enterprise automation.

42:56.300 --> 43:00.300
And so, when should you trust an AI system?

43:00.300 --> 43:02.300
There are two paths to trust.

43:02.300 --> 43:04.300
The human in the loop model.

43:04.300 --> 43:07.300
So, make the human individually responsible

43:07.300 --> 43:10.300
and treat that as individual productivity increases

43:10.300 --> 43:15.300
or a hell of a lot of safety testing and safety nets.

43:15.300 --> 43:21.300
And as always, it is safest to have both.

43:21.300 --> 43:26.300
We're still so excited in what you potentially could do

43:26.300 --> 43:32.300
that we end up in organizations with death by a thousand pilots,

43:32.300 --> 43:34.300
get mandates from the top saying,

43:34.300 --> 43:37.300
everybody go find two, three use cases

43:37.300 --> 43:40.300
and everybody go try plug this into your business.

43:40.300 --> 43:44.300
And then take the human out of the loop too.

43:45.300 --> 43:47.300
They don't know how to do testing though.

43:47.300 --> 43:49.300
They don't know how to build the safety nets.

43:49.300 --> 43:51.300
They've removed the human in the loop

43:51.300 --> 43:54.300
and then they wonder why their use cases don't work.

43:54.300 --> 43:59.300
I do think that we're gonna have a lot more generative AI at scale,

43:59.300 --> 44:01.300
but this is the part to figure out.

44:01.300 --> 44:03.300
In fact, I'm a recovering statistician.

44:03.300 --> 44:05.300
And recovering also,

44:05.300 --> 44:09.300
because we are the grumpiest of the data scientist

44:09.300 --> 44:13.300
and we've sort of been folded into the data science profession for a bit.

44:13.300 --> 44:21.300
Definitively the less popular sibling in that family for now.

44:21.300 --> 44:24.300
And I cackled to myself slightly,

44:24.300 --> 44:29.300
because statisticians are gonna have to be back, aren't they?

44:29.300 --> 44:31.300
Someone's gonna have to figure out,

44:31.300 --> 44:34.300
does this thing in fact work the way that we think it works?

44:34.300 --> 44:36.300
Someone's gonna have to figure out the testing.

44:36.300 --> 44:40.300
Testing is still the most difficult part, especially in generative AI.

44:41.300 --> 44:44.300
And here is a little analogy

44:44.300 --> 44:50.300
that I encourage folks who are not in this business to really internalize.

44:50.300 --> 44:53.300
The data are like the ingredients.

44:53.300 --> 44:56.300
The algorithms are like appliances in a kitchen.

44:56.300 --> 45:01.300
When we were talking about doing applied AI last decade,

45:01.300 --> 45:05.300
we were talking about innovating in models, in recipes.

45:05.300 --> 45:10.300
So creating a croissant that is sugar-free and gluten-free

45:10.300 --> 45:13.300
and dairy-free and delicious.

45:13.300 --> 45:15.300
How do you go about making a recipe like that?

45:15.300 --> 45:17.300
You get a bunch of ingredients, you get the appliances,

45:17.300 --> 45:20.300
and then you tinker, you doubly play, you hope.

45:20.300 --> 45:22.300
You do a bunch of taste testing,

45:22.300 --> 45:25.300
you get your croissant or cookie or whatever it is,

45:25.300 --> 45:29.300
you start being able to produce them at scale, how wonderful.

45:29.300 --> 45:33.300
Now, with the generative AI revolution,

45:33.300 --> 45:35.300
it's equivalent to saying, instead of,

45:35.300 --> 45:37.300
I give all of you the cookie that I've made

45:37.300 --> 45:40.300
and you can eat it or not eat it right here, right now,

45:40.300 --> 45:43.300
and you don't have to worry about where it comes from.

45:43.300 --> 45:48.300
Instead, I'm saying, here, get access to my cookie maker,

45:48.300 --> 45:51.300
the cookies, and take them and turn them into whatever you want.

45:51.300 --> 45:55.300
Anything creative, Valentine's Day, baskets, spray-painted gold,

45:55.300 --> 45:58.300
call it art, whatever you want to do.

45:58.300 --> 46:02.300
But you see that we do lose an interesting layer of control there

46:02.300 --> 46:07.300
because we have a separation in this system

46:07.300 --> 46:11.300
from whatever the dish was to however you're going to use it.

46:11.300 --> 46:16.300
And the users down the line of these products,

46:16.300 --> 46:21.300
you are not told very much about where those ingredients came from,

46:21.300 --> 46:24.300
how they got processed, what their quality was,

46:24.300 --> 46:28.300
what were those appliances, what even went on in that kitchen.

46:28.300 --> 46:30.300
Is it poisonous, isn't it?

46:30.300 --> 46:33.300
But you're going to have to test everything yourself again from scratch

46:33.300 --> 46:36.300
if you're a downstream user, trying to do this

46:36.300 --> 46:41.300
at enterprise scale for tasks that matter.

46:41.300 --> 46:44.300
For some tasks, though, truly, take a thing, spray-painted gold,

46:44.300 --> 46:46.300
what do you care what the ingredients were?

46:46.300 --> 46:48.300
It looks about right, now it's gold

46:48.300 --> 46:52.300
and it still looks about right, everything's good.

46:52.300 --> 46:55.300
But where the ingredients might have mattered,

46:55.300 --> 46:57.300
where the quality might have mattered,

46:58.300 --> 47:00.300
it's up to you to figure out the testing.

47:00.300 --> 47:05.300
And finally, leaving you with the thought of whose job AI

47:05.300 --> 47:07.300
will automate as a consequence.

47:07.300 --> 47:10.300
Yes, it is about engineering and automation,

47:10.300 --> 47:13.300
but as a consequence of these technologies,

47:13.300 --> 47:14.300
what are we going to see?

47:14.300 --> 47:19.300
In the US, what we are seeing is the second quartile

47:19.300 --> 47:25.300
having the most effect, taking the most economic damage

47:25.300 --> 47:27.300
in these technologies, the second quartile

47:27.300 --> 47:30.300
of skill level and income.

47:30.300 --> 47:32.300
And this is puzzling economists.

47:32.300 --> 47:35.300
Why not the people at the very bottom,

47:35.300 --> 47:37.300
the least skilled labor?

47:37.300 --> 47:40.300
And why not the most skilled labor?

47:40.300 --> 47:42.300
Well, I have a guess.

47:42.300 --> 47:46.300
What's happening in the second quartile

47:46.300 --> 47:50.300
is the tasks that are most repetitive and most digitized.

47:50.300 --> 47:54.300
If it's repetitive and digitized, if it's copy-pasting,

47:54.300 --> 47:59.300
if it's doing things on mental screensaver with a computer,

47:59.300 --> 48:05.300
those are the tasks that are most likely to be rendered

48:05.300 --> 48:09.300
no longer necessary for humans to do.

48:09.300 --> 48:13.300
Whereas the difficult skills of having taste,

48:13.300 --> 48:16.300
of being creative, of thinking, of problem solving,

48:16.300 --> 48:19.300
of being a great engineer,

48:20.300 --> 48:25.300
those skills and those jobs are still extremely important,

48:25.300 --> 48:29.300
no matter how good your tools get.

48:29.300 --> 48:33.300
But there is a small economic issue here,

48:33.300 --> 48:35.300
which could become a very big one.

48:35.300 --> 48:38.300
What we take for granted is how our young people

48:38.300 --> 48:42.300
become our senior trusted leaders, artists,

48:42.300 --> 48:45.300
developers, managers, et cetera.

48:45.300 --> 48:49.300
And nobody trusts someone fresh out of university.

48:49.300 --> 48:51.300
I don't know, do you?

48:51.300 --> 48:54.300
So what does that path look like

48:54.300 --> 48:58.300
from newly graduated to senior and trusted?

48:58.300 --> 49:03.300
Often a bunch of easy-to-measure output.

49:03.300 --> 49:06.300
That's pretty repetitive, a bit mindless.

49:06.300 --> 49:10.300
You understand the person's character by working with them.

49:10.300 --> 49:12.300
You see if they go a little outside the box,

49:12.300 --> 49:15.300
you give them very low trust tasks.

49:15.300 --> 49:19.300
Exactly the kind of tasks that are going to get automated here.

49:19.300 --> 49:21.300
And what you will see is that bosses, leaders,

49:21.300 --> 49:23.300
are now going to absorb a lot of the work

49:23.300 --> 49:27.300
that their junior staff members would have done.

49:27.300 --> 49:30.300
Great in the short term.

49:30.300 --> 49:33.300
How are you going to get the next generation of bosses, though?

49:33.300 --> 49:37.300
What is your plan for developing your talent

49:37.300 --> 49:39.300
to a point where you can trust them?

49:39.300 --> 49:42.300
Because the skills that are most likely to get left

49:42.300 --> 49:45.300
are those skills where you need to apply some taste,

49:45.300 --> 49:46.300
where you need to be trusted,

49:46.300 --> 49:50.300
where you need to have judgment and good sense and expertise.

49:50.300 --> 49:53.300
And where, when you look at the code,

49:53.300 --> 49:57.300
where you look at the output, where you look at the art,

49:57.300 --> 50:00.300
you are able to judge whether that's what you're looking for.

50:00.300 --> 50:02.300
If I never learn how to speak Serbian,

50:02.300 --> 50:06.300
how am I going to deal with that email output in Serbian?

50:06.300 --> 50:10.300
How are we going to incentivize me to learn the language,

50:10.300 --> 50:14.300
whether it's a programming one or a human one,

50:14.300 --> 50:18.300
to get to the point where I'm able to supervise

50:18.300 --> 50:21.300
the functioning of these systems rather than do the task myself?

50:21.300 --> 50:28.300
This is a big piece that is missing in our economic plans globally.

50:28.300 --> 50:31.300
And so as you, if you are beginning to automate

50:31.300 --> 50:34.300
with these systems in your organizations,

50:34.300 --> 50:36.300
as you're taking responsibility for this,

50:36.300 --> 50:39.300
please pay attention to the three big topics from this talk.

50:39.300 --> 50:43.300
One, data quality matters, and today it's nobody's job.

50:43.300 --> 50:45.300
That's a problem.

50:45.300 --> 50:49.300
Two, testing these things is difficult,

50:49.300 --> 50:53.300
especially when we're talking about generative AI at scale

50:53.300 --> 50:55.300
without humans in the loop.

50:55.300 --> 51:00.300
And three, it's going to be up to you to create that plan

51:00.300 --> 51:03.300
for how you're going to be able to train your staff

51:03.300 --> 51:05.300
and not leave anyone out,

51:05.300 --> 51:07.300
because your most valued staff members,

51:07.300 --> 51:10.300
the ones you really do want to have around,

51:10.300 --> 51:15.300
often need that path of going through a gauntlet

51:15.300 --> 51:21.300
that these tools may render less attractive in the short term.

51:21.300 --> 51:25.300
And that's why we have a term like secret cyborgs in America,

51:25.300 --> 51:27.300
so people who want to use these tools,

51:27.300 --> 51:31.300
they don't want their managers to know.

51:31.300 --> 51:33.300
That should be a warning.

51:33.300 --> 51:36.300
It should be a warning that we don't trust managers

51:36.300 --> 51:42.300
to handle that transition fairly, carefully, and responsibly.

51:42.300 --> 51:48.300
And so how do we in this room think about championing,

51:48.300 --> 51:52.300
a gentle and also effective and intelligent

51:52.300 --> 51:55.300
way through the challenges of today

51:55.300 --> 52:00.300
to get to a highly productive economy and workforce tomorrow?

52:01.300 --> 52:03.300
Thank you very much.

52:06.300 --> 52:08.300
Thank you.

