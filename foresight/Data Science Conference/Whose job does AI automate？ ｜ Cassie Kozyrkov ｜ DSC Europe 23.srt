1
00:00:00,000 --> 00:00:01,380
Tako dobro.

2
00:00:15,820 --> 00:00:17,340
Tko se dobro.

3
00:00:26,000 --> 00:00:27,780
Thank You so for long session.

4
00:00:27,780 --> 00:00:32,380
Google is in statistics, decision making and machine learning.

5
00:00:32,380 --> 00:00:39,380
Today, she leads data scientific and elite agency known for assisting global leaders and executives

6
00:00:39,380 --> 00:00:41,380
in optimizing critical decisions.

7
00:00:41,380 --> 00:00:47,380
So, Casey, may I ask you, whose job does AI automate?

8
00:00:47,380 --> 00:01:02,380
Thank you for that introduction, and yes, I have the answer, but we'll get there in a sort of meandering way.

9
00:01:02,380 --> 00:01:09,380
Good morning, Belgrade, I am so happy to be here with you today, and I have a quick question for the audience.

10
00:01:09,380 --> 00:01:15,380
Who here identifies themselves as an AI professional?

11
00:01:15,380 --> 00:01:17,380
Sure of hands, sure of hands.

12
00:01:17,380 --> 00:01:19,380
OK, I see some hands.

13
00:01:19,380 --> 00:01:23,380
How about who does not identify as an AI professional?

14
00:01:23,380 --> 00:01:26,380
All right, a little more hands, a little more hands.

15
00:01:26,380 --> 00:01:38,380
Friends among the AI professionals, do you know what year was the first year that the term artificial intelligence was used?

16
00:01:38,380 --> 00:01:39,380
Anybody?

17
00:01:39,380 --> 00:01:42,380
We're gonna have to do a little history here, I think.

18
00:01:42,380 --> 00:01:54,380
The answer is 1955-ish, 1956 for sure, because in 1955 there was a fun grant proposal being made,

19
00:01:54,380 --> 00:01:59,380
and in this grant proposal was the first time that this term was used.

20
00:01:59,380 --> 00:02:04,380
So, I think that was written in 1955, but it was for a summer workshop in 1956.

21
00:02:04,380 --> 00:02:06,380
So, what was the grant proposal for?

22
00:02:06,380 --> 00:02:12,380
It was for a summer school session at Dartmouth in America.

23
00:02:12,380 --> 00:02:21,380
That's a university and a bunch of postdocs, led by postdocs or assistant professors, I forget what they were at the time,

24
00:02:21,380 --> 00:02:24,380
led by a fellow at MIT, John McCarthy.

25
00:02:24,380 --> 00:02:34,380
They wanted to get the funding to get together 10 of them to spend two months working on what they called in this grant proposal, artificial intelligence.

26
00:02:35,380 --> 00:02:41,380
Now, there are two fun facts about this situation, which you might enjoy.

27
00:02:41,380 --> 00:02:47,380
The first fun fact, and I actually only learned this myself in October,

28
00:02:47,380 --> 00:02:52,380
from someone who used to work directly with one of those 10 attendees.

29
00:02:52,380 --> 00:02:56,380
The first fun fact is why that term?

30
00:02:56,380 --> 00:02:59,380
Is it because it is truly intelligent?

31
00:02:59,380 --> 00:03:03,380
Because we do get a lot of questions, like, are these things intelligent?

32
00:03:04,380 --> 00:03:10,380
That name is a curse that comes from the grant proposal,

33
00:03:10,380 --> 00:03:19,380
where it turns out that John McCarthy wrote that term in order to scare the US government into giving funding.

34
00:03:19,380 --> 00:03:24,380
So, that term was picked not because anybody particularly knew what intelligence was,

35
00:03:24,380 --> 00:03:29,380
or how the human brain worked, but rather because that term sounded intimidating.

36
00:03:29,380 --> 00:03:33,380
And we're still living with some of the legacy and baggage of that.

37
00:03:33,380 --> 00:03:39,380
Sometimes it stops organizations from being sensible in what to expect.

38
00:03:39,380 --> 00:03:51,380
The other fun fact about the name artificial intelligence is that in that grant proposal, where it was first used,

39
00:03:51,380 --> 00:03:57,380
the writers of the proposal were planning to solve all of artificial intelligence

40
00:03:57,380 --> 00:04:01,380
with 10 people in two months in 1956.

41
00:04:01,380 --> 00:04:09,380
So, there's a legacy in this field of big talk, which does not necessarily meet reality.

42
00:04:09,380 --> 00:04:14,380
So, maybe we should level-set a little bit and reorient ourselves.

43
00:04:14,380 --> 00:04:22,380
Maybe what we should have called it, if we wanted AI as an acronym, is automated inspiration.

44
00:04:22,380 --> 00:04:25,380
Maybe that would have been a better term.

45
00:04:25,380 --> 00:04:30,380
Or amplified impact, which we're seeing a lot today.

46
00:04:30,380 --> 00:04:35,380
Or this year's move towards augmented individuals.

47
00:04:35,380 --> 00:04:38,380
But really, this is a story of automation.

48
00:04:38,380 --> 00:04:43,380
So, let's see automation at its most basic.

49
00:04:43,380 --> 00:04:45,380
So, what are we doing with a computer?

50
00:04:45,380 --> 00:04:46,380
We're automating digitally.

51
00:04:46,380 --> 00:04:50,380
We're turning inputs into outputs via a recipe.

52
00:04:50,380 --> 00:04:51,380
So, what's a recipe?

53
00:04:51,380 --> 00:04:52,380
It's some code.

54
00:04:52,380 --> 00:04:53,380
It's a model.

55
00:04:53,380 --> 00:04:55,380
Those are all fancy words for a recipe.

56
00:04:55,380 --> 00:04:57,380
Takes the inputs, turns them into outputs.

57
00:04:57,380 --> 00:05:00,380
But we're going to need some hardware here to do it.

58
00:05:00,380 --> 00:05:01,380
The recipe is not enough.

59
00:05:01,380 --> 00:05:02,380
We're going to need a computer.

60
00:05:02,380 --> 00:05:05,380
And you are probably imagining this kind of computer.

61
00:05:05,380 --> 00:05:07,380
But I want to show you a different kind of computer.

62
00:05:07,380 --> 00:05:09,380
That's also a computer.

63
00:05:09,380 --> 00:05:11,380
Meet Dora.

64
00:05:11,380 --> 00:05:13,380
And Dora is a computer.

65
00:05:13,380 --> 00:05:15,380
Now, how do I know this?

66
00:05:15,380 --> 00:05:20,380
Because Dora happens to be my friend's wife's aunt.

67
00:05:20,380 --> 00:05:23,380
And Dora was actually a computer.

68
00:05:23,380 --> 00:05:28,380
Here is her marriage certificate from 1950.

69
00:05:28,380 --> 00:05:34,380
And you can see very clearly there that her profession is listed as computer.

70
00:05:34,380 --> 00:05:36,380
So, she is a real computer.

71
00:05:36,380 --> 00:05:39,380
So, we're going to have her in this example.

72
00:05:39,380 --> 00:05:45,380
And what she's supposed to do is take an input and turn it into an output via some recipe.

73
00:05:45,380 --> 00:05:47,380
Now, how does she know what to do?

74
00:05:47,380 --> 00:05:48,380
Right?

75
00:05:48,380 --> 00:05:53,380
Those of you who already work with computers, those of you who are developers,

76
00:05:53,380 --> 00:05:56,380
you know that a computer needs something very important,

77
00:05:56,380 --> 00:05:59,380
which is an engineer to program the computer, right?

78
00:05:59,380 --> 00:06:01,380
And luckily Dora has an engineer also,

79
00:06:01,380 --> 00:06:06,380
because there you can see her husband's profession is listed as engineer.

80
00:06:06,380 --> 00:06:13,380
So, this is one of those early days in history where a computer has managed to marry her engineer.

81
00:06:13,380 --> 00:06:18,380
Now, let's consider Dora and her engineer husband.

82
00:06:18,380 --> 00:06:22,380
Back then in 1950, we want to get Dora to do a task,

83
00:06:22,380 --> 00:06:26,380
like recognize whether an image has a cat in it or not.

84
00:06:26,380 --> 00:06:30,380
This was a very classic computer vision task, right?

85
00:06:30,380 --> 00:06:32,380
The cat not cat task.

86
00:06:33,380 --> 00:06:36,380
Now, one way in which we could do this

87
00:06:36,380 --> 00:06:43,380
is to figure out the exact instructions to give Dora of what to do with the input.

88
00:06:43,380 --> 00:06:45,380
Now, consider what the input is.

89
00:06:45,380 --> 00:06:51,380
The input is a bunch of pixel color values, so it's a bunch of pixels.

90
00:06:51,380 --> 00:06:56,380
And the question is, what should you do with each pixel in an image

91
00:06:56,380 --> 00:06:59,380
to get the answer cat not cat, right?

92
00:06:59,380 --> 00:07:01,380
What is that recipe supposed to be?

93
00:07:01,380 --> 00:07:04,380
And now you're thinking philosophical things,

94
00:07:04,380 --> 00:07:06,380
like what makes a cat a cat?

95
00:07:06,380 --> 00:07:08,380
Difficult question, right?

96
00:07:08,380 --> 00:07:12,380
Like, what do you do with the top left-hand pixel and the one next to it?

97
00:07:12,380 --> 00:07:14,380
And what are you looking for in the image?

98
00:07:14,380 --> 00:07:18,380
Are you looking for triangles, maybe two triangles, maybe some ovals for eyes?

99
00:07:18,380 --> 00:07:22,380
This is a hard recipe to come up with.

100
00:07:22,380 --> 00:07:27,380
And so, in order to program a computer with instructions,

101
00:07:27,380 --> 00:07:29,380
do this, then do that, then do that,

102
00:07:29,380 --> 00:07:31,380
first, you have to know how to do the task,

103
00:07:31,380 --> 00:07:35,380
which, you know, how are you actually doing the task, though?

104
00:07:35,380 --> 00:07:38,380
You're doing it, your brain is doing it,

105
00:07:38,380 --> 00:07:41,380
but what are you actually doing with the pixels?

106
00:07:41,380 --> 00:07:44,380
That is so hard to put into words.

107
00:07:44,380 --> 00:07:46,380
You don't even know what you're doing.

108
00:07:46,380 --> 00:07:50,380
So how on earth are you going to program with the instructions

109
00:07:50,380 --> 00:07:53,380
of what to look for in each pixel?

110
00:07:53,380 --> 00:07:54,380
Very difficult.

111
00:07:54,380 --> 00:07:57,380
Now, there's a different way that we could go about this.

112
00:07:57,380 --> 00:08:00,380
Instead of explaining what to do with each pixel,

113
00:08:00,380 --> 00:08:04,380
you could instead explain your wishes with examples.

114
00:08:04,380 --> 00:08:06,380
Here are a bunch of examples of cat.

115
00:08:06,380 --> 00:08:08,380
Here are a bunch of examples of not cat.

116
00:08:08,380 --> 00:08:11,380
You go find the patterns,

117
00:08:11,380 --> 00:08:15,380
and then make a recipe automatically from those patterns

118
00:08:15,380 --> 00:08:19,380
so that you can take yourself from input to output.

119
00:08:19,380 --> 00:08:23,380
This examples versus instructions thing.

120
00:08:23,380 --> 00:08:27,380
This is the essence of the difference between machine learning

121
00:08:27,380 --> 00:08:30,380
slash AI and traditional programming.

122
00:08:30,380 --> 00:08:33,380
And notice that we already do this with one another as humans.

123
00:08:33,380 --> 00:08:36,380
Sometimes we explain our wishes with examples.

124
00:08:36,380 --> 00:08:39,380
Sometimes we explain our wishes with instructions.

125
00:08:39,380 --> 00:08:42,380
So we already teach one another one of these two ways.

126
00:08:42,380 --> 00:08:46,380
Now we are able to do the same thing with machines,

127
00:08:46,380 --> 00:08:50,380
except we need fancy words for examples and instructions.

128
00:08:50,380 --> 00:08:56,380
So we've got code as instructions and examples map to data.

129
00:08:56,380 --> 00:08:58,380
So this really is the difference

130
00:08:58,380 --> 00:09:01,380
between the traditional software programming approach

131
00:09:01,380 --> 00:09:06,380
and the AI slash machine learning programming approach.

132
00:09:06,380 --> 00:09:10,380
So let's make sure that we warm up this room

133
00:09:10,380 --> 00:09:14,380
and look at doing this cat not cat task together.

134
00:09:14,380 --> 00:09:19,380
So I give you a bunch of inputs with their appropriate labels.

135
00:09:19,380 --> 00:09:25,380
And then something in your brain figures out

136
00:09:25,380 --> 00:09:29,380
what those patterns are, turns that into a recipe.

137
00:09:29,380 --> 00:09:32,380
And then when the next one comes in, you're going to take it

138
00:09:32,380 --> 00:09:34,380
and you're going to convert it to the output I'm looking for.

139
00:09:34,380 --> 00:09:37,380
So to wake ourselves up, we're going to play this game together.

140
00:09:37,380 --> 00:09:42,380
I need each and every one of you to shout cat or not cat

141
00:09:42,380 --> 00:09:44,380
when I show you an input.

142
00:09:44,380 --> 00:09:45,380
Do you think you can do it?

143
00:09:45,380 --> 00:09:46,380
Yes?

144
00:09:46,380 --> 00:09:48,380
You're not loud enough for me.

145
00:09:48,380 --> 00:09:49,380
Yes?

146
00:09:49,380 --> 00:09:50,380
Yes, OK.

147
00:09:50,380 --> 00:09:51,380
Good.

148
00:09:51,380 --> 00:09:52,380
Right.

149
00:09:52,380 --> 00:09:53,380
So here comes the first one.

150
00:09:53,380 --> 00:09:54,380
Cat.

151
00:09:54,380 --> 00:09:55,380
Someone said yes.

152
00:09:55,380 --> 00:09:56,380
Excellent.

153
00:09:56,380 --> 00:09:57,380
Working as intended.

154
00:09:57,380 --> 00:10:00,380
So cat, I agree with you.

155
00:10:00,380 --> 00:10:02,380
Cat, see computers also make mistakes.

156
00:10:02,380 --> 00:10:04,380
This is what we will see as a theme here.

157
00:10:04,380 --> 00:10:06,380
Right, this one, I agree with you.

158
00:10:06,380 --> 00:10:07,380
Cat.

159
00:10:07,380 --> 00:10:08,380
Next.

160
00:10:08,380 --> 00:10:09,380
Not cat.

161
00:10:09,380 --> 00:10:11,380
Not cat.

162
00:10:11,380 --> 00:10:12,380
Not cat.

163
00:10:12,380 --> 00:10:13,380
Cat.

164
00:10:13,380 --> 00:10:14,380
Cat.

165
00:10:14,380 --> 00:10:15,380
Cat.

166
00:10:15,380 --> 00:10:16,380
Cat.

167
00:10:16,380 --> 00:10:17,380
Cat.

168
00:10:17,380 --> 00:10:19,700
Good afternoon, judges and judges.

169
00:10:19,700 --> 00:10:24,780
We've had a very laugh, we could have lep on you.

170
00:10:24,780 --> 00:10:26,420
OK, here we go.

171
00:10:26,420 --> 00:10:30,420
As for judges, we'll start.

172
00:10:30,420 --> 00:10:32,940
No, do you have miel in dazz?

173
00:10:32,940 --> 00:10:33,940
vog.

174
00:10:33,980 --> 00:10:36,660
Now, now, the thing I wanted to do.

175
00:10:36,660 --> 00:10:45,340
At the moment, I knew this chair in Nanаш and I didn't know the chair in Hanse.

176
00:10:45,340 --> 00:10:46,340
Wanlax.

177
00:10:46,420 --> 00:10:47,260
��重要!

178
00:10:47,260 --> 00:10:49,160
Sen si je jestvo invented nachodnout s n夠.

179
00:10:49,560 --> 00:10:52,400
Komand marched amerika je menarem za no doc,

180
00:10:52,400 --> 00:10:54,400
pri konar 모 trenz si jeрал.

181
00:10:56,000 --> 00:10:56,980
Spotn BOLE

182
00:10:58,000 --> 00:11:00,900
postaende idemko u vlada supermarketu,

183
00:11:01,340 --> 00:11:03,920
res prese баг always teber.

184
00:11:04,200 --> 00:11:06,380
The right answer here depends very much

185
00:11:06,380 --> 00:11:08,360
on the purpose of the system.

186
00:11:08,360 --> 00:11:10,360
What does it exist for?

187
00:11:11,320 --> 00:11:13,040
In so, I guess I'd better fill those big boots

188
00:11:13,040 --> 00:11:14,800
and I'll tell you

189
00:11:14,800 --> 00:11:17,240
that this is supposed to be a pet recommendations system.

190
00:11:17,260 --> 00:11:21,300
iz temi direktosni signedcrossne od v pagesenati.

191
00:11:21,980 --> 00:11:24,100
Jedob連 bomo se pristratil.

192
00:11:26,380 --> 00:11:27,260
N Community.

193
00:11:27,260 --> 00:11:28,440
EVON.

194
00:11:29,820 --> 00:11:44,940
ADVORTED V

195
00:11:44,940 --> 00:11:48,240
nismo visoki inserted, izmonživati,學om,

196
00:11:48,560 --> 00:11:51,880
lagske stipiot Lars van pri otro Los 2.

197
00:11:52,080 --> 00:11:54,600
Tudi sem bent ihtim eskatakaj.

198
00:11:54,800 --> 00:11:56,820
Jaz pri vsojeve providilo

199
00:11:57,020 --> 00:11:59,940
vse ovo, inverse drženje v taki,

200
00:12:00,140 --> 00:12:02,140
i kaj mi dobro komens,

201
00:12:02,340 --> 00:12:04,540
da jste ti s da žine.

202
00:12:04,740 --> 00:12:06,940
Tako bojteške investiula,

203
00:12:07,140 --> 00:12:08,700
je biloboardo.

204
00:12:08,900 --> 00:12:10,660
Toto rada ne vem za.

205
00:12:10,860 --> 00:12:13,300
Zvousite,To ga se od Economija Kot.

206
00:12:13,300 --> 00:12:18,300
in zelo je zelo tako zelo tako zelo, if we just say what this is,

207
00:12:18,300 --> 00:12:23,300
which is examples written down in electronic form, text books, essentially,

208
00:12:23,300 --> 00:12:26,300
for the machine student to learn from.

209
00:12:26,300 --> 00:12:29,300
And data quality is everything.

210
00:12:29,300 --> 00:12:32,300
If you're going to teach someone with examples,

211
00:12:32,300 --> 00:12:36,300
the quality of those examples matters so much.

212
00:12:36,300 --> 00:12:43,300
And none of this is purely objective, same meaning and answer every time,

213
00:12:43,300 --> 00:12:47,300
as you can see, depending on the purpose of the system,

214
00:12:47,300 --> 00:12:52,300
whether the right answer is cat or not cat, changes.

215
00:12:52,300 --> 00:12:57,300
So data is a bunch of scraps of information that we happen to write down,

216
00:12:57,300 --> 00:13:03,300
that we put in a textbook for a machine to learn from.

217
00:13:03,800 --> 00:13:09,300
Like human textbooks, normal textbooks for human students,

218
00:13:09,300 --> 00:13:13,300
machine textbooks, data sets have human authors.

219
00:13:13,300 --> 00:13:19,300
They don't arrive from aliens, they don't come from nowhere, from the universe.

220
00:13:19,300 --> 00:13:23,300
They are collected by us, and they fit the sensibilities

221
00:13:23,300 --> 00:13:29,300
of whoever is in charge of the data problem.

222
00:13:29,300 --> 00:13:36,300
And the trouble with them, of course, is that they reflect

223
00:13:36,300 --> 00:13:41,300
unconscious things we might not even have considered

224
00:13:41,300 --> 00:13:44,300
could be a problem when we were authoring our textbooks.

225
00:13:44,300 --> 00:13:47,300
So when you think about really old textbooks,

226
00:13:47,300 --> 00:13:50,300
and you think whether you would want to teach your children

227
00:13:50,300 --> 00:13:55,300
from these really old textbooks from 200 years ago, 300 years ago,

228
00:13:55,300 --> 00:13:58,300
you're thinking absolutely not.

229
00:13:58,300 --> 00:14:03,300
It doesn't matter what the title on the cover of that textbook is,

230
00:14:03,300 --> 00:14:07,300
chances are, if you try to learn from it,

231
00:14:07,300 --> 00:14:14,300
you're going to pick up some habits that are not good or useful habits.

232
00:14:14,300 --> 00:14:19,300
So thinking about the quality of your textbook is really, really important.

233
00:14:19,300 --> 00:14:22,300
And can you complete the following sentence,

234
00:14:22,300 --> 00:14:24,300
just to make sure that we're all on the same page?

235
00:14:24,300 --> 00:14:27,300
Garbage in, garbage out, you know this.

236
00:14:27,300 --> 00:14:31,300
So what I find very interesting about the data space,

237
00:14:31,300 --> 00:14:34,300
the data professions, data science,

238
00:14:34,300 --> 00:14:36,300
I find fascinating about data science.

239
00:14:36,300 --> 00:14:40,300
So when you go to a data conference or a data science conference,

240
00:14:40,300 --> 00:14:42,300
so sometimes I hang out with people,

241
00:14:42,300 --> 00:14:44,300
I'll hang out with people after this as well

242
00:14:44,300 --> 00:14:46,300
if you want to hang out out there.

243
00:14:46,300 --> 00:14:48,300
And sometimes folks come, they hang out,

244
00:14:48,300 --> 00:14:50,300
and they've got all different job roles,

245
00:14:50,300 --> 00:14:53,300
so I'll ask them, you know, what do you do professionally?

246
00:14:53,300 --> 00:14:56,300
One will say statistician, and that one will be data engineer,

247
00:14:56,300 --> 00:14:59,300
and that one will be clinical researcher, and so on.

248
00:14:59,300 --> 00:15:01,300
And then I'd like to do another round,

249
00:15:01,300 --> 00:15:03,300
and I'd like to ask them, okay,

250
00:15:03,300 --> 00:15:07,300
who in your organization is responsible for data quality?

251
00:15:07,300 --> 00:15:11,300
Who is in charge of it from data design,

252
00:15:11,300 --> 00:15:14,300
documentation, the cleanup,

253
00:15:14,300 --> 00:15:20,300
all the way through to the part where it starts hitting

254
00:15:20,300 --> 00:15:22,300
the pipelines that the data engineers have built.

255
00:15:22,300 --> 00:15:25,300
So who shapes the data set?

256
00:15:25,300 --> 00:15:29,300
And what I love to hear here is that as we go around,

257
00:15:29,300 --> 00:15:33,300
we have a very high correlation with whatever they said

258
00:15:33,300 --> 00:15:35,300
their own job title was.

259
00:15:35,300 --> 00:15:38,300
So the statistician says statisticians are responsible for it,

260
00:15:38,300 --> 00:15:41,300
the researchers say researchers are responsible for it,

261
00:15:41,300 --> 00:15:43,300
data engineers say data engineers are responsible for it.

262
00:15:43,300 --> 00:15:45,300
You know what that sounds like?

263
00:15:45,300 --> 00:15:48,300
That sounds like a situation where it's everybody's job

264
00:15:48,300 --> 00:15:51,300
and therefore nobody's job.

265
00:15:51,300 --> 00:15:54,300
In order to automate with data,

266
00:15:54,300 --> 00:15:56,300
you need good and appropriate data.

267
00:15:56,300 --> 00:15:59,300
In order to get good and appropriate data,

268
00:15:59,300 --> 00:16:03,300
you need an expertise in a bunch of different topics.

269
00:16:03,300 --> 00:16:06,300
You don't need to be a full expert in statistics, for example,

270
00:16:06,300 --> 00:16:09,300
but you need some expertise in statistics.

271
00:16:09,300 --> 00:16:12,300
You need some understanding of data engineering,

272
00:16:12,300 --> 00:16:14,300
some understanding of survey design,

273
00:16:14,300 --> 00:16:18,300
human psychology if the data sets are about humans.

274
00:16:18,300 --> 00:16:21,300
You need some user experience design

275
00:16:21,300 --> 00:16:24,300
if you are gathering that information online

276
00:16:24,300 --> 00:16:28,300
and how is the way that you are presenting the questions,

277
00:16:28,300 --> 00:16:30,300
influencing the answers that you get back.

278
00:16:30,300 --> 00:16:33,300
There's a lot of expertise you need.

279
00:16:33,300 --> 00:16:36,300
And yet where is the job role for this?

280
00:16:36,300 --> 00:16:39,300
Where is the profession that takes this seriously?

281
00:16:39,300 --> 00:16:43,300
And I had a really terrible aha moment with this.

282
00:16:43,300 --> 00:16:47,300
I was hanging out with a data science influencer, as one does.

283
00:16:48,300 --> 00:16:52,300
And I was talking about how this is a problem,

284
00:16:52,300 --> 00:16:57,300
that we are building our disciplines on a foundation of data,

285
00:16:57,300 --> 00:17:02,300
and yet the quality of that data is no one's job.

286
00:17:02,300 --> 00:17:04,300
I was saying this is so important,

287
00:17:04,300 --> 00:17:09,300
maybe instead of over focusing on this last mile thing,

288
00:17:09,300 --> 00:17:14,300
maybe we should put more effort as a profession,

289
00:17:14,300 --> 00:17:18,300
in the industry in the first bit, the actual data quality.

290
00:17:18,300 --> 00:17:21,300
I was saying we need to encourage university graduates

291
00:17:21,300 --> 00:17:23,300
to study this and to take this seriously.

292
00:17:23,300 --> 00:17:25,300
There needs to be a career progression,

293
00:17:25,300 --> 00:17:29,300
a way that motivates you to actually want to learn

294
00:17:29,300 --> 00:17:31,300
all those things, to do it professionally,

295
00:17:31,300 --> 00:17:34,300
because there's a lot to learn.

296
00:17:34,300 --> 00:17:37,300
And then I ask this friend of mine,

297
00:17:37,300 --> 00:17:40,300
and we're live streaming, this is what makes it best,

298
00:17:40,300 --> 00:17:43,300
we're live streaming in this moment, I'm talking about it.

299
00:17:43,300 --> 00:17:46,300
And I ask, so what do you think we should,

300
00:17:46,300 --> 00:17:49,300
what should this be called, what is this called?

301
00:17:49,300 --> 00:17:54,300
And my friend goes, oh that sounds like a data janitor.

302
00:17:59,300 --> 00:18:03,300
Is this how we're going to motivate our undergraduates

303
00:18:03,300 --> 00:18:06,300
to go to university, and they're picking their major,

304
00:18:06,300 --> 00:18:08,300
they're deciding what to study,

305
00:18:08,300 --> 00:18:10,300
and then they call their parents and they say,

306
00:18:10,300 --> 00:18:12,300
I've picked one.

307
00:18:12,300 --> 00:18:16,300
I would like to go through a hard grueling training program

308
00:18:16,300 --> 00:18:18,300
to be a data janitor.

309
00:18:18,300 --> 00:18:21,300
Are you proud of me, mom and dad?

310
00:18:21,300 --> 00:18:24,300
Right, that's not a good start.

311
00:18:24,300 --> 00:18:26,300
And it's not a good start

312
00:18:26,300 --> 00:18:29,300
when data seems to be everybody else's job.

313
00:18:29,300 --> 00:18:33,300
So we have quite a brittle profession here,

314
00:18:33,300 --> 00:18:36,300
because a lot of it is based on the hope

315
00:18:36,300 --> 00:18:38,300
that someone is going to do a job

316
00:18:38,300 --> 00:18:40,300
that they didn't train for,

317
00:18:40,300 --> 00:18:43,300
and that we're almost surely not paying them properly for.

318
00:18:43,300 --> 00:18:45,300
We should worry about this.

319
00:18:45,300 --> 00:18:47,300
And we should also remember

320
00:18:47,300 --> 00:18:49,300
that a lot of the data that we wish we had,

321
00:18:49,300 --> 00:18:52,300
or the quality that we wish we had it at,

322
00:18:52,300 --> 00:18:57,300
won't exist if we have this basic problem of economics.

323
00:18:57,300 --> 00:18:59,300
So that's our first thing.

324
00:18:59,300 --> 00:19:03,300
Data are not objective, they are subjective.

325
00:19:03,300 --> 00:19:05,300
The design matters.

326
00:19:06,300 --> 00:19:10,300
And even though we rely so much on data for automation,

327
00:19:10,300 --> 00:19:14,300
there's not that good of a plan in the data professions.

328
00:19:14,300 --> 00:19:16,300
So data quality is everything.

329
00:19:16,300 --> 00:19:20,300
That is the first point I really want to hammer home here.

330
00:19:20,300 --> 00:19:22,300
And I know a lot of you in the audience

331
00:19:22,300 --> 00:19:25,300
think you've heard this all before and you get it.

332
00:19:25,300 --> 00:19:28,300
But if you did,

333
00:19:28,300 --> 00:19:33,300
wouldn't there be better progress in the industry

334
00:19:33,300 --> 00:19:36,300
to motivate, fund and compensate people

335
00:19:36,300 --> 00:19:40,300
whose job the data quality actually is?

336
00:19:40,300 --> 00:19:44,300
And then let's talk a little bit about the internet

337
00:19:44,300 --> 00:19:47,300
as a data source.

338
00:19:47,300 --> 00:19:51,300
Like that is a source of mirrors, isn't it?

339
00:19:51,300 --> 00:19:54,300
Kind of reflects reality a little bit,

340
00:19:54,300 --> 00:19:56,300
but you get a skewed perspective.

341
00:19:56,300 --> 00:20:00,300
Never forget that the internet is not reality.

342
00:20:00,300 --> 00:20:04,300
How you behave online isn't how you behave

343
00:20:04,300 --> 00:20:07,300
in your natural surroundings.

344
00:20:07,300 --> 00:20:10,300
And we know what kind of stuff lives on the internet

345
00:20:10,300 --> 00:20:13,300
in the parts where people can be anonymous, right?

346
00:20:13,300 --> 00:20:17,300
It's not necessarily bringing the best of us to anything.

347
00:20:17,300 --> 00:20:19,300
So we need to be quite careful

348
00:20:19,300 --> 00:20:22,300
with what we allow ourselves to do

349
00:20:22,300 --> 00:20:25,300
on the basis of wild type data.

350
00:20:25,300 --> 00:20:29,300
Now back to our question of whose job AI automates.

351
00:20:29,300 --> 00:20:34,300
Let's look again at the broader category of AI.

352
00:20:34,300 --> 00:20:36,300
By the way, I'm using AI and machine learning

353
00:20:36,300 --> 00:20:39,300
somewhat interchangeably here, because I've given up.

354
00:20:39,300 --> 00:20:42,300
Once upon a time, AI used to be the superset,

355
00:20:42,300 --> 00:20:44,300
then machine learning was the subset,

356
00:20:44,300 --> 00:20:47,300
then at some point machine learning was the superset

357
00:20:47,300 --> 00:20:50,300
and AI was the subset, something, something.

358
00:20:50,300 --> 00:20:52,300
If it's deep learning, then it's AI.

359
00:20:52,300 --> 00:20:54,300
I give up.

360
00:20:54,300 --> 00:20:57,300
Honestly, there was a set of cycles

361
00:20:57,300 --> 00:20:59,300
in funding and disappointment,

362
00:20:59,300 --> 00:21:02,300
where you got the funding if you said AI

363
00:21:02,300 --> 00:21:04,300
and then things didn't work out,

364
00:21:04,300 --> 00:21:07,300
so then you started saying something else, machine learning,

365
00:21:07,300 --> 00:21:09,300
and then the funding didn't work out there,

366
00:21:09,300 --> 00:21:11,300
and so we went up and down in these cycles.

367
00:21:11,300 --> 00:21:13,300
Like bell bottoms and skinny jeans, right?

368
00:21:13,300 --> 00:21:16,300
Like it's the fashion of what we're gonna call it.

369
00:21:16,300 --> 00:21:18,300
So actually my favorite definition

370
00:21:18,300 --> 00:21:21,300
of the difference between AI and machine learning

371
00:21:21,300 --> 00:21:25,300
is if it is written in Python,

372
00:21:25,300 --> 00:21:27,300
it's probably machine learning,

373
00:21:27,300 --> 00:21:30,300
and if it's written in PowerPoint, it's probably AI.

374
00:21:30,300 --> 00:21:33,300
So, I'm using them interchangeably, the hell with it.

375
00:21:33,300 --> 00:21:35,300
So whose job does AI automate?

376
00:21:35,300 --> 00:21:37,300
Let's look carefully at what this is

377
00:21:37,300 --> 00:21:39,300
as an automation proposition.

378
00:21:39,300 --> 00:21:42,300
When I'm automating the traditional way,

379
00:21:42,300 --> 00:21:45,300
first, I have to know how to do the task

380
00:21:45,300 --> 00:21:47,300
so that I can explain to you

381
00:21:47,300 --> 00:21:49,300
what precisely you need to do with each input

382
00:21:49,300 --> 00:21:51,300
to get the output.

383
00:21:51,300 --> 00:21:54,300
Second, I have to think about every little instruction

384
00:21:54,300 --> 00:21:56,300
and then I have to write it down,

385
00:21:56,300 --> 00:21:59,300
and first I can write it down for myself in pseudocode

386
00:21:59,300 --> 00:22:02,300
or, you know, English or whatever language I speak,

387
00:22:02,300 --> 00:22:04,300
and then I have to translate it into something

388
00:22:04,300 --> 00:22:06,300
the computer can understand.

389
00:22:06,300 --> 00:22:08,300
But I have to deal with every single line,

390
00:22:08,300 --> 00:22:10,300
and maybe it takes 10,000 lines,

391
00:22:10,300 --> 00:22:12,300
maybe it takes 100,000 lines of code

392
00:22:12,300 --> 00:22:15,300
to automate my task, I can write each one down.

393
00:22:15,300 --> 00:22:18,300
And you might be saying, oh no, maybe it's,

394
00:22:18,300 --> 00:22:21,300
maybe I just get a package somewhere,

395
00:22:21,300 --> 00:22:24,300
I find some library, I install something,

396
00:22:24,300 --> 00:22:26,300
and then I just pull from there

397
00:22:26,300 --> 00:22:29,300
and I don't have to write the code by hand myself.

398
00:22:29,300 --> 00:22:32,300
Sure, but some member of our species

399
00:22:32,300 --> 00:22:34,300
had to do it.

400
00:22:34,300 --> 00:22:36,300
So some human is responsible

401
00:22:36,300 --> 00:22:39,300
for having thought through all those instructions,

402
00:22:39,300 --> 00:22:42,300
whereas with machine learning and AI,

403
00:22:42,300 --> 00:22:47,300
there are just two lines.

404
00:22:47,300 --> 00:22:50,300
Optimize this goal on that dataset

405
00:22:50,300 --> 00:22:52,300
go.

406
00:22:52,300 --> 00:22:54,300
Now those of you who raised your hand for AI professional,

407
00:22:54,300 --> 00:22:58,300
you know there's a lot more code that you're writing.

408
00:22:58,300 --> 00:23:02,300
But that is because the tools are nasty.

409
00:23:02,300 --> 00:23:06,300
At the core, there are only these two lines of instructions.

410
00:23:06,300 --> 00:23:09,300
What does success look like?

411
00:23:09,300 --> 00:23:13,300
What data should we point this pattern finding thingy at,

412
00:23:13,300 --> 00:23:15,300
and off we go.

413
00:23:15,300 --> 00:23:18,300
And really, if you had the ability to brute force it,

414
00:23:18,300 --> 00:23:20,300
and had enough computing power,

415
00:23:20,300 --> 00:23:22,300
you could try every known algorithm

416
00:23:22,300 --> 00:23:24,300
with every permutation of it pretty much,

417
00:23:24,300 --> 00:23:26,300
quickly eliminate some,

418
00:23:26,300 --> 00:23:28,300
try everything out,

419
00:23:28,300 --> 00:23:30,300
subject to just the two important lines of instruction,

420
00:23:30,300 --> 00:23:32,300
optimize this goal on that dataset.

421
00:23:32,300 --> 00:23:36,300
And as the tools become easier and easier,

422
00:23:36,300 --> 00:23:39,300
we will strip away all the huffing and puffing

423
00:23:39,300 --> 00:23:42,300
and the difficulty of forcing a dataset in this format

424
00:23:42,300 --> 00:23:44,300
to be taken up by an algorithm

425
00:23:44,300 --> 00:23:47,300
that was designed over there.

426
00:23:47,300 --> 00:23:50,300
You know, some of these tools are really,

427
00:23:50,300 --> 00:23:53,300
only a mother could love them.

428
00:23:53,300 --> 00:23:56,300
And you're left with just these two lines,

429
00:23:56,300 --> 00:23:59,300
which means that almost anyone then

430
00:23:59,300 --> 00:24:05,300
will be able to automate a task.

431
00:24:05,300 --> 00:24:07,300
Ha!

432
00:24:07,300 --> 00:24:10,300
What do we see here?

433
00:24:10,300 --> 00:24:15,300
First, two very subjective lines.

434
00:24:15,300 --> 00:24:17,300
What is the goal of the system?

435
00:24:17,300 --> 00:24:19,300
What does success look like?

436
00:24:19,300 --> 00:24:21,300
Why am I building this pet classifier

437
00:24:21,300 --> 00:24:23,300
that does cat not cat?

438
00:24:23,300 --> 00:24:27,300
And why should Tiger be labeled cat versus not cat?

439
00:24:27,300 --> 00:24:29,300
Well, vice versa.

440
00:24:29,300 --> 00:24:32,300
There's no one single right way to do that.

441
00:24:32,300 --> 00:24:34,300
What about scoring mistakes?

442
00:24:34,300 --> 00:24:37,300
That's all part of how we're gonna express our goal.

443
00:24:37,300 --> 00:24:40,300
Which mistakes are worse than which other mistakes?

444
00:24:40,300 --> 00:24:42,300
Again, highly subjective.

445
00:24:42,300 --> 00:24:46,300
So, which textbook shall we learn from?

446
00:24:46,300 --> 00:24:49,300
There are a lot of different textbook choices you could use.

447
00:24:49,300 --> 00:24:51,300
You could also edit and modify

448
00:24:51,300 --> 00:24:53,300
and get different versions of the textbooks.

449
00:24:53,300 --> 00:24:56,300
And all of this is highly subjective.

450
00:24:56,300 --> 00:24:59,300
But now, available,

451
00:24:59,300 --> 00:25:02,300
just two lines and you can automate your task.

452
00:25:02,300 --> 00:25:05,300
How wonderful and how terrifying simultaneously.

453
00:25:05,300 --> 00:25:09,300
This is both the peril and the promise of AI.

454
00:25:09,300 --> 00:25:13,300
The promise is if I'm doing a little task myself,

455
00:25:13,300 --> 00:25:17,300
I can now automate it very quickly.

456
00:25:17,300 --> 00:25:18,300
How great for me?

457
00:25:18,300 --> 00:25:23,300
I don't have to go and write everything from scratch.

458
00:25:23,300 --> 00:25:25,300
But at the same time,

459
00:25:25,300 --> 00:25:27,300
what if I am automating something

460
00:25:27,300 --> 00:25:30,300
on behalf of millions or billions of people?

461
00:25:30,300 --> 00:25:33,300
What if my code's gonna touch a lot of lives?

462
00:25:33,300 --> 00:25:35,300
Well, then I can, again,

463
00:25:35,300 --> 00:25:37,300
without thinking too hard about it,

464
00:25:37,300 --> 00:25:39,300
get it automated.

465
00:25:39,300 --> 00:25:41,300
So, we have a thoughtlessness enabler here.

466
00:25:41,300 --> 00:25:43,300
We can be more thoughtless

467
00:25:43,300 --> 00:25:45,300
and we can automate thoughtlessly.

468
00:25:45,300 --> 00:25:48,300
Which is great when it only affects you.

469
00:25:48,300 --> 00:25:51,300
But when we start scaling that up,

470
00:25:51,300 --> 00:25:53,300
we can do damage.

471
00:25:53,300 --> 00:25:56,300
This is like a proliferation of magic lamps.

472
00:25:56,300 --> 00:25:58,300
Lamps with genies.

473
00:25:58,300 --> 00:26:01,300
And knowing how to make a wish responsibly

474
00:26:01,300 --> 00:26:03,300
is a very important skill.

475
00:26:03,300 --> 00:26:05,300
It's the skill of decision leadership.

476
00:26:05,300 --> 00:26:08,300
We're not even talking about this, though.

477
00:26:08,300 --> 00:26:10,300
We're not asking ourselves,

478
00:26:10,300 --> 00:26:12,300
who is it?

479
00:26:12,300 --> 00:26:14,300
Who has the skills on our team

480
00:26:14,300 --> 00:26:17,300
to figure out what success should look like?

481
00:26:17,300 --> 00:26:21,300
How do we carefully state what we're looking for?

482
00:26:21,300 --> 00:26:24,300
What do we actually want to create in the world?

483
00:26:24,300 --> 00:26:26,300
And what would be the consequences

484
00:26:26,300 --> 00:26:28,300
if we got what we asked for?

485
00:26:28,300 --> 00:26:30,300
And which data is appropriate and why?

486
00:26:30,300 --> 00:26:32,300
And what would need to be true about that data

487
00:26:32,300 --> 00:26:34,300
for us to wanna use it?

488
00:26:34,300 --> 00:26:37,300
Very, very subjective questions

489
00:26:37,300 --> 00:26:40,300
that very few people are trained to answer.

490
00:26:40,300 --> 00:26:42,300
So you should worry

491
00:26:42,300 --> 00:26:44,300
who is actually being tasked

492
00:26:44,300 --> 00:26:47,300
with doing this for massive systems.

493
00:26:47,300 --> 00:26:50,300
Do they have the skills to do it?

494
00:26:50,300 --> 00:26:52,300
And as you see,

495
00:26:52,300 --> 00:26:54,300
the tools get easier and easier,

496
00:26:54,300 --> 00:26:57,300
you'll see a shift from a focus on

497
00:26:57,300 --> 00:27:00,300
huffing and puffing and actually getting

498
00:27:00,300 --> 00:27:03,300
the data to be taken up by the algorithm

499
00:27:03,300 --> 00:27:05,300
and then deployed to production,

500
00:27:05,300 --> 00:27:07,300
and a lot more focus on

501
00:27:07,300 --> 00:27:09,300
how do we put 10,000 lines

502
00:27:09,300 --> 00:27:11,300
or 100,000 lines of thought

503
00:27:11,300 --> 00:27:14,300
back into these two lines.

504
00:27:14,300 --> 00:27:17,300
We've allowed ourselves to be thoughtless,

505
00:27:17,300 --> 00:27:19,300
but on some things, that's not okay.

506
00:27:19,300 --> 00:27:21,300
So how do we put that thought back in?

507
00:27:21,300 --> 00:27:23,300
How do we very carefully design

508
00:27:23,300 --> 00:27:26,300
systems that can affect society at scale?

509
00:27:26,300 --> 00:27:28,300
But back to the question

510
00:27:28,300 --> 00:27:31,300
of whose job are we actually automating here?

511
00:27:31,300 --> 00:27:34,300
Well, it is the developer's job.

512
00:27:34,300 --> 00:27:37,300
We're going from having to write instructions

513
00:27:37,300 --> 00:27:39,300
to now being able to say,

514
00:27:39,300 --> 00:27:41,300
instead of knowing how to do the task,

515
00:27:41,300 --> 00:27:45,300
here's the objective, here's the data, go.

516
00:27:45,300 --> 00:27:48,300
That said, it's not like we're putting

517
00:27:48,300 --> 00:27:50,300
software developers out of business.

518
00:27:50,300 --> 00:27:52,300
First, there's still a lot of huffing

519
00:27:52,300 --> 00:27:54,300
and puffing to do to get the algorithms

520
00:27:54,300 --> 00:27:57,300
to accept those instructions

521
00:27:57,300 --> 00:27:59,300
and the data.

522
00:27:59,300 --> 00:28:02,300
Second, we are actually unlocking

523
00:28:02,300 --> 00:28:05,300
a whole class of new applications.

524
00:28:05,300 --> 00:28:09,300
And all the old approaches

525
00:28:09,300 --> 00:28:13,300
are still going to be very economically necessary.

526
00:28:13,300 --> 00:28:15,300
Why?

527
00:28:15,300 --> 00:28:17,300
If you are able to automate your task

528
00:28:17,300 --> 00:28:19,300
with instructions,

529
00:28:19,300 --> 00:28:21,300
that is how you should do it.

530
00:28:21,300 --> 00:28:23,300
That's how you get the most control.

531
00:28:23,300 --> 00:28:25,300
If you're able to say what needs to be done

532
00:28:25,300 --> 00:28:27,300
in what order,

533
00:28:27,300 --> 00:28:29,300
and you give those instructions to your machine

534
00:28:29,300 --> 00:28:32,300
or to your human employee,

535
00:28:32,300 --> 00:28:35,300
you can be sure of what that person

536
00:28:35,300 --> 00:28:37,300
is going to do next

537
00:28:37,300 --> 00:28:39,300
if they're following the instructions.

538
00:28:39,300 --> 00:28:41,300
Exactly what you've told them to do.

539
00:28:41,300 --> 00:28:43,300
No guessing.

540
00:28:43,300 --> 00:28:46,300
No surprise ways that they interpreted anything.

541
00:28:46,300 --> 00:28:49,300
Just follow those instructions.

542
00:28:49,300 --> 00:28:51,300
Whereas, if you know how to give the instructions,

543
00:28:51,300 --> 00:28:54,300
but instead you give a few examples,

544
00:28:54,300 --> 00:28:56,300
who knows what they're going to learn

545
00:28:56,300 --> 00:28:58,300
in those examples?

546
00:28:58,300 --> 00:29:00,300
Maybe they'll learn the right thing,

547
00:29:00,300 --> 00:29:02,300
maybe they won't.

548
00:29:02,300 --> 00:29:04,300
And mistakes are possible.

549
00:29:04,300 --> 00:29:06,300
That's true with humans,

550
00:29:06,300 --> 00:29:08,300
that's also true with these AI systems.

551
00:29:08,300 --> 00:29:10,300
So why are we using them?

552
00:29:10,300 --> 00:29:13,300
To automate things we can't automate the other way.

553
00:29:13,300 --> 00:29:16,300
So we're not putting developers out of business.

554
00:29:16,300 --> 00:29:18,300
Everything developers used to do

555
00:29:18,300 --> 00:29:20,300
and used to be able to do,

556
00:29:20,300 --> 00:29:22,300
you're still going to want to do that

557
00:29:22,300 --> 00:29:24,300
in the old traditional way.

558
00:29:24,300 --> 00:29:27,300
But now we've got a whole new class of applications.

559
00:29:27,300 --> 00:29:30,300
And let's talk about a new new class of applications.

560
00:29:30,300 --> 00:29:32,300
The two different AIs.

561
00:29:32,300 --> 00:29:35,300
So this year we're talking a lot about AI.

562
00:29:35,300 --> 00:29:37,300
We tend,

563
00:29:37,300 --> 00:29:39,300
when we find ourselves hanging out with friends

564
00:29:39,300 --> 00:29:41,300
and having a glass of wine

565
00:29:41,300 --> 00:29:44,300
and talking about all these new things in AI in 2023,

566
00:29:44,300 --> 00:29:47,300
we tend to be talking about generative AI.

567
00:29:47,300 --> 00:29:50,300
So let's remind ourselves very quickly of the difference.

568
00:29:50,300 --> 00:29:52,300
So discriminative AI, the old one,

569
00:29:52,300 --> 00:29:55,300
the one you're used to from last decade,

570
00:29:55,300 --> 00:29:58,300
that is all about applying a label.

571
00:29:58,300 --> 00:30:00,300
So this is a thing labeler.

572
00:30:00,300 --> 00:30:02,300
We had the cat not cat example of that.

573
00:30:02,300 --> 00:30:06,300
Here's another classic again with vision.

574
00:30:06,300 --> 00:30:08,300
So I really like this tweet.

575
00:30:08,300 --> 00:30:10,300
It comes from BJM,

576
00:30:10,300 --> 00:30:12,300
who complained that he was locked out

577
00:30:12,300 --> 00:30:15,300
because his smart front door lock,

578
00:30:15,300 --> 00:30:18,300
his nest camera system locked him out

579
00:30:18,300 --> 00:30:22,300
and he was protecting him from Batman.

580
00:30:22,300 --> 00:30:24,300
It didn't want to let Batman in the house,

581
00:30:24,300 --> 00:30:26,300
so it locked poor BJM out.

582
00:30:26,300 --> 00:30:28,300
So it's supposed to find the right answer.

583
00:30:28,300 --> 00:30:30,300
It doesn't always work correctly.

584
00:30:30,300 --> 00:30:32,300
These systems do make mistakes.

585
00:30:32,300 --> 00:30:35,300
And it's really important for designers to remember this,

586
00:30:35,300 --> 00:30:38,300
because imagine what would have happened to poor BJM

587
00:30:38,300 --> 00:30:40,300
if there wasn't a plan for mistakes.

588
00:30:40,300 --> 00:30:43,300
He wouldn't have gotten back into his house.

589
00:30:43,300 --> 00:30:46,300
Instead he can put user pin to get himself in

590
00:30:46,300 --> 00:30:49,300
because the engineers built that safety net

591
00:30:49,300 --> 00:30:51,300
and knew that mistakes were possible.

592
00:30:51,300 --> 00:30:53,300
So that's thing labelers.

593
00:30:53,300 --> 00:30:55,300
On the other hand, generative AI

594
00:30:55,300 --> 00:30:57,300
is about creating a plausible exemplar.

595
00:30:57,300 --> 00:30:58,300
What are we learning?

596
00:30:58,300 --> 00:31:00,300
Not a label, but a distribution.

597
00:31:00,300 --> 00:31:02,300
And what can you do with a distribution

598
00:31:02,300 --> 00:31:05,300
is create a really good fake.

599
00:31:05,300 --> 00:31:07,300
This is a fake maker.

600
00:31:07,300 --> 00:31:09,300
So as you all know,

601
00:31:09,300 --> 00:31:12,300
Picasso is very famous for his paintings of laptops.

602
00:31:12,300 --> 00:31:14,300
So I have some examples for you there

603
00:31:14,300 --> 00:31:16,300
in the top right-hand corner.

604
00:31:16,300 --> 00:31:19,300
This is where I'm using an image generation tool

605
00:31:19,300 --> 00:31:21,300
called Mid Journey.

606
00:31:21,300 --> 00:31:23,300
Mid Journey is my favorite casino.

607
00:31:23,300 --> 00:31:25,300
I really enjoy playing with Mid Journey.

608
00:31:25,300 --> 00:31:27,300
It gives you, you put in a prompt,

609
00:31:27,300 --> 00:31:30,300
it gives you four options back,

610
00:31:30,300 --> 00:31:31,300
and maybe you like it,

611
00:31:31,300 --> 00:31:33,300
maybe you rerun the prompt

612
00:31:33,300 --> 00:31:35,300
until you get something that you like.

613
00:31:35,300 --> 00:31:39,300
I've also created some fake Gucci sunglasses for you.

614
00:31:39,300 --> 00:31:42,300
So both of these aren't,

615
00:31:42,300 --> 00:31:44,300
you know, it's not real Picasso.

616
00:31:44,300 --> 00:31:47,300
This doesn't really exist out there.

617
00:31:47,300 --> 00:31:52,300
We are generating from a distribution of plausible Picasso type

618
00:31:52,300 --> 00:31:55,300
and laptop type things

619
00:31:55,300 --> 00:31:59,300
to get this lovely fake for you.

620
00:31:59,300 --> 00:32:03,300
So it's a game of plausible exemplars.

621
00:32:03,300 --> 00:32:07,300
And people ask a question that bugs me so much

622
00:32:07,300 --> 00:32:09,300
when they see this stuff.

623
00:32:09,300 --> 00:32:11,300
They ask, can AI be creative?

624
00:32:11,300 --> 00:32:13,300
Does this mean that AI is the artist?

625
00:32:13,300 --> 00:32:17,300
Is AI making art?

626
00:32:17,300 --> 00:32:20,300
And then I have to remind folks

627
00:32:20,300 --> 00:32:23,300
of an entire century of art history.

628
00:32:23,300 --> 00:32:25,300
Because if you're asking questions like this,

629
00:32:25,300 --> 00:32:27,300
you must have missed something

630
00:32:27,300 --> 00:32:30,300
from art history from the 20th century.

631
00:32:30,300 --> 00:32:32,300
So let's go back to 1917.

632
00:32:32,300 --> 00:32:35,300
Marcel Duchamp found this iconic piece.

633
00:32:35,300 --> 00:32:38,300
This is considered iconic in the art world.

634
00:32:38,300 --> 00:32:40,300
This is a urinal.

635
00:32:40,300 --> 00:32:42,300
He signed some name on it,

636
00:32:42,300 --> 00:32:44,300
not even his own name.

637
00:32:44,300 --> 00:32:47,300
He took it to the exhibition when that's art.

638
00:32:47,300 --> 00:32:51,300
And it was, we consider this a very interesting piece.

639
00:32:51,300 --> 00:32:53,300
It's worth a lot.

640
00:32:56,300 --> 00:32:57,300
Is it art?

641
00:32:57,300 --> 00:32:58,300
Sure.

642
00:32:58,300 --> 00:32:59,300
Why?

643
00:32:59,300 --> 00:33:01,300
Because art is a conversation

644
00:33:01,300 --> 00:33:03,300
that humanity is having with itself

645
00:33:03,300 --> 00:33:05,300
and has been having for millennia.

646
00:33:05,300 --> 00:33:06,300
And to make art,

647
00:33:06,300 --> 00:33:08,300
we consider the next sentence

648
00:33:08,300 --> 00:33:11,300
in this grand conversation.

649
00:33:11,300 --> 00:33:13,300
But who is the artist?

650
00:33:13,300 --> 00:33:16,300
I would say Duchamp, let's give him credit.

651
00:33:16,300 --> 00:33:18,300
Because otherwise, where are we going to put the credit?

652
00:33:18,300 --> 00:33:20,300
On the porcelain makers,

653
00:33:20,300 --> 00:33:21,300
the toilet company,

654
00:33:21,300 --> 00:33:23,300
that doesn't make any sense to me.

655
00:33:23,300 --> 00:33:25,300
And should we penalize him

656
00:33:25,300 --> 00:33:28,300
because he didn't sculpt it from scratch,

657
00:33:28,300 --> 00:33:31,300
mixing his own materials,

658
00:33:31,300 --> 00:33:32,300
creating his own porcelain?

659
00:33:32,300 --> 00:33:33,300
Not at all.

660
00:33:33,300 --> 00:33:35,300
This cut out a lot of toil

661
00:33:35,300 --> 00:33:38,300
because he didn't know what he wanted to say much faster.

662
00:33:38,300 --> 00:33:41,300
Generative AI plays the same role.

663
00:33:41,300 --> 00:33:43,300
Like a paintbrush,

664
00:33:43,300 --> 00:33:46,300
it's a tool for you to be able to say

665
00:33:46,300 --> 00:33:48,300
what you need to say faster and better.

666
00:33:48,300 --> 00:33:53,300
And the secret behind a lot of these generative AI art things

667
00:33:53,300 --> 00:33:55,300
is that it's very rare

668
00:33:55,300 --> 00:33:59,300
that the first one is the one that's presented to the audience.

669
00:33:59,300 --> 00:34:03,300
So AI made art that won some art competition.

670
00:34:03,300 --> 00:34:06,300
The 8,000th iteration,

671
00:34:06,300 --> 00:34:10,300
a person cranked the handle on these tools

672
00:34:10,300 --> 00:34:12,300
8,000 times,

673
00:34:12,300 --> 00:34:15,300
500 times, however many times it took

674
00:34:15,300 --> 00:34:18,300
to get the one that they were looking for,

675
00:34:18,300 --> 00:34:20,300
to express what they wanted to express.

676
00:34:20,300 --> 00:34:22,300
So where is the creativity?

677
00:34:22,300 --> 00:34:23,300
It's in the human.

678
00:34:23,300 --> 00:34:26,300
But the human can go a little bit faster.

679
00:34:26,300 --> 00:34:28,300
They don't need to mix paints.

680
00:34:28,300 --> 00:34:30,300
We don't penalize artists today

681
00:34:30,300 --> 00:34:32,300
for not mixing their own paints

682
00:34:32,300 --> 00:34:34,300
the way that they would have in the Middle Ages.

683
00:34:34,300 --> 00:34:36,300
Making their own paintbrushes

684
00:34:36,300 --> 00:34:38,300
out of horse hair or whatever it is.

685
00:34:38,300 --> 00:34:40,300
You go to the shop, you buy some paintbrushes,

686
00:34:40,300 --> 00:34:42,300
you buy some paint, and you go paint.

687
00:34:42,300 --> 00:34:43,300
That's great.

688
00:34:43,300 --> 00:34:45,300
That lets us get there faster.

689
00:34:45,300 --> 00:34:48,300
And that's what generative AI allows you to do as well.

690
00:34:48,300 --> 00:34:50,300
What are some other things you can do?

691
00:34:50,300 --> 00:34:53,300
So good old open AI all over the news.

692
00:34:53,300 --> 00:34:55,300
Open chat GPT.

693
00:34:55,300 --> 00:34:58,300
I have in audiences like this,

694
00:34:58,300 --> 00:35:01,300
I have asked who here has never used chat GPT.

695
00:35:01,300 --> 00:35:03,300
And so I'm going to stop embarrassing audiences

696
00:35:03,300 --> 00:35:05,300
because there does tend to be one hand.

697
00:35:05,300 --> 00:35:08,300
And then I ask, OK, who here has never read about chat GPT

698
00:35:08,300 --> 00:35:10,300
and that no hands go up?

699
00:35:10,300 --> 00:35:12,300
And I think what a strange equation.

700
00:35:12,300 --> 00:35:15,300
It is faster to try it than to read about it.

701
00:35:15,300 --> 00:35:18,300
So why did my one hand, one or two hands

702
00:35:18,300 --> 00:35:20,300
read about it without trying it?

703
00:35:20,300 --> 00:35:21,300
You may as well just try it.

704
00:35:21,300 --> 00:35:22,300
The interface is super easy.

705
00:35:22,300 --> 00:35:24,300
It's like sending a text message

706
00:35:24,300 --> 00:35:26,300
and you type whatever you want to type.

707
00:35:26,300 --> 00:35:30,300
This is when we asked CEOs

708
00:35:30,300 --> 00:35:33,300
what they personally use chat GPT for the most.

709
00:35:33,300 --> 00:35:37,300
One of the favorite answers was to write a retirement poem

710
00:35:37,300 --> 00:35:38,300
or a birthday poem.

711
00:35:38,300 --> 00:35:43,300
So it's really getting used for its top applications.

712
00:35:43,300 --> 00:35:46,300
But here I've asked it to write a funny retirement poem

713
00:35:46,300 --> 00:35:48,300
for your CEO in the style of Dr. Seuss.

714
00:35:48,300 --> 00:35:49,300
And what does it give us back?

715
00:35:49,300 --> 00:35:51,300
You've been the big cheese, the head of the pack.

716
00:35:51,300 --> 00:35:54,300
Now it's your time to kick back and slack.

717
00:35:54,300 --> 00:36:01,300
So definitely the highest in what we could possibly want

718
00:36:01,300 --> 00:36:02,300
out of our technology.

719
00:36:02,300 --> 00:36:03,300
Let's try another one.

720
00:36:03,300 --> 00:36:07,300
This is an application that OpenAI found

721
00:36:07,300 --> 00:36:13,300
and noticed was a statement perhaps about the human condition.

722
00:36:13,300 --> 00:36:16,300
So a lot of people like to take bullet points

723
00:36:16,300 --> 00:36:22,300
and then ask OpenAI, chat GPT, to turn that into an email,

724
00:36:22,300 --> 00:36:23,300
a full email.

725
00:36:23,300 --> 00:36:26,300
So here's some summaries, expand that out into an email.

726
00:36:26,300 --> 00:36:29,300
And what OpenAI found was that this was a popular use case,

727
00:36:29,300 --> 00:36:32,300
as was this other use case,

728
00:36:32,300 --> 00:36:35,300
which was summarize this email as bullet points

729
00:36:35,300 --> 00:36:37,300
back into the original.

730
00:36:37,300 --> 00:36:41,300
So I think that does tell us something a little bit sad

731
00:36:41,300 --> 00:36:43,300
and funny about how humans work.

732
00:36:43,300 --> 00:36:45,300
Wouldn't it be great if all of our emails

733
00:36:45,300 --> 00:36:49,300
could just be bullet points in the first place?

734
00:36:49,300 --> 00:36:53,300
But what we're seeing here with generative AI

735
00:36:53,300 --> 00:36:56,300
is a new kind of user interaction.

736
00:36:56,300 --> 00:36:59,300
It is AI as a raw material.

737
00:36:59,300 --> 00:37:01,300
AI, for AI's sake,

738
00:37:01,300 --> 00:37:06,300
given to you the user to do anything you want with Next.

739
00:37:06,300 --> 00:37:10,300
So we have the ability to find the right distributions,

740
00:37:10,300 --> 00:37:13,300
to pull laptops by Picasso out,

741
00:37:13,300 --> 00:37:16,300
or Gucci sunglasses, or whatever else you want.

742
00:37:16,300 --> 00:37:18,300
But now we're giving you the tool,

743
00:37:18,300 --> 00:37:21,300
you figure out what you want to shape it into.

744
00:37:21,300 --> 00:37:26,300
And when we talk about regulating generative AI,

745
00:37:26,300 --> 00:37:31,300
I hope you can appreciate now how hard this is,

746
00:37:31,300 --> 00:37:35,300
because we are not good at regulating raw materials,

747
00:37:35,300 --> 00:37:37,300
even physical raw materials.

748
00:37:37,300 --> 00:37:39,300
Whose fault is it?

749
00:37:39,300 --> 00:37:43,300
If I invent a phenomenal anti-gravity material,

750
00:37:43,300 --> 00:37:45,300
that could be pretty cool for humanity,

751
00:37:45,300 --> 00:37:48,300
but some idiot's gonna make skis out of it.

752
00:37:48,300 --> 00:37:50,300
So whose fault is it, then,

753
00:37:50,300 --> 00:37:53,300
when they go and ski in anti-gravity skis and hurt themselves?

754
00:37:53,300 --> 00:37:56,300
Was it me for making the material?

755
00:37:56,300 --> 00:37:59,300
Was it whoever helped them fashion the skis?

756
00:37:59,300 --> 00:38:02,300
Or was it, then, the user of those skis?

757
00:38:02,300 --> 00:38:04,300
Who is responsible?

758
00:38:04,300 --> 00:38:07,300
How do we limit what the uses of it are

759
00:38:07,300 --> 00:38:10,300
that would be okay versus not okay?

760
00:38:10,300 --> 00:38:13,300
This is a hard problem, hard with physical materials

761
00:38:13,300 --> 00:38:15,300
when it comes to digital raw materials.

762
00:38:15,300 --> 00:38:17,300
Good luck.

763
00:38:17,300 --> 00:38:19,300
Really hard to figure out how to regulate.

764
00:38:19,300 --> 00:38:24,300
And when I hear that a problem is really hard,

765
00:38:24,300 --> 00:38:28,300
the last thing that I want is for us, then,

766
00:38:28,300 --> 00:38:31,300
to solve it in some dumb way just to say we've solved it

767
00:38:31,300 --> 00:38:35,300
so we can move on in our to-do list.

768
00:38:35,300 --> 00:38:38,300
Solving AI regulation here is difficult,

769
00:38:38,300 --> 00:38:41,300
which means that maybe we shouldn't get ahead of ourselves

770
00:38:41,300 --> 00:38:44,300
and make a bunch of laws we haven't thought through,

771
00:38:44,300 --> 00:38:47,300
maybe go slowly and think about the consequences

772
00:38:47,300 --> 00:38:50,300
of regulating, maybe request a bunch of information

773
00:38:50,300 --> 00:38:53,300
that would help you regulate it later.

774
00:38:53,300 --> 00:38:56,300
So, it is a very, very difficult problem.

775
00:38:56,300 --> 00:39:00,300
Then, another kind of application

776
00:39:00,300 --> 00:39:03,300
is all kinds of translation-type stuff.

777
00:39:03,300 --> 00:39:06,300
So here I have asked it to write the FORTRAN code

778
00:39:06,300 --> 00:39:09,300
for generating the Fibonacci sequence.

779
00:39:09,300 --> 00:39:12,300
And I do not speak or understand FORTRAN,

780
00:39:12,300 --> 00:39:15,300
so, hopefully, someone in the room can look at this

781
00:39:15,300 --> 00:39:18,300
and see if it's right or not.

782
00:39:18,300 --> 00:39:21,300
What I can do is I know what the Fibonacci sequence is

783
00:39:21,300 --> 00:39:23,300
and I can write out those instructions.

784
00:39:23,300 --> 00:39:26,300
I could also have written them out the way that I want

785
00:39:26,300 --> 00:39:28,300
and ask for it to translate that to FORTRAN.

786
00:39:28,300 --> 00:39:31,300
But here's a little quick bit of trouble.

787
00:39:31,300 --> 00:39:33,300
Anyone here going to fess up,

788
00:39:33,300 --> 00:39:37,300
going to confess with me that you don't speak FORTRAN?

789
00:39:38,300 --> 00:39:40,300
Right, so, I'm seeing some hands.

790
00:39:40,300 --> 00:39:44,300
So, imagine that we asked for this lovely FORTRAN code

791
00:39:44,300 --> 00:39:47,300
for generating the Fibonacci sequence and we get it.

792
00:39:47,300 --> 00:39:51,300
Do we take this and plug this directly into our codebase

793
00:39:51,300 --> 00:39:55,300
when we don't understand what the hell it is?

794
00:39:55,300 --> 00:39:57,300
Terrifying.

795
00:39:57,300 --> 00:40:02,300
So, with this one, OK, maybe we know how to be software engineers.

796
00:40:02,300 --> 00:40:04,300
I would figure out how to make some unit tests here.

797
00:40:04,300 --> 00:40:07,300
Maybe I would line by line try to figure out what I'm looking at.

798
00:40:07,300 --> 00:40:11,300
But you can see the more code that I generate with this

799
00:40:11,300 --> 00:40:14,300
in languages that I don't speak or understand,

800
00:40:14,300 --> 00:40:18,300
the more space I'm leaving for potentially catastrophic disasters

801
00:40:18,300 --> 00:40:24,300
as I plug this in to an already complicated system.

802
00:40:24,300 --> 00:40:28,300
This is why people are saying that good developers

803
00:40:28,300 --> 00:40:31,300
are becoming much better, the estimates coming from McKinsey

804
00:40:31,300 --> 00:40:34,300
of 50% better, if you're a good engineer,

805
00:40:34,300 --> 00:40:38,300
then this can cut out a lot of drudgery.

806
00:40:38,300 --> 00:40:42,300
But bad engineers are becoming worse.

807
00:40:42,300 --> 00:40:46,300
And bad teams are reducing corporate productivity

808
00:40:46,300 --> 00:40:50,300
because they're plugging all kinds of nonsense into their systems.

809
00:40:50,300 --> 00:40:56,300
So, in general here, people who are already highly productive

810
00:40:56,300 --> 00:40:59,300
are making themselves more productive.

811
00:40:59,300 --> 00:41:01,300
They understand the output.

812
00:41:01,300 --> 00:41:04,300
They understand how to put it together into solutions.

813
00:41:04,300 --> 00:41:07,300
But those who are on the less productive side

814
00:41:07,300 --> 00:41:09,300
or they don't know what they're working with

815
00:41:09,300 --> 00:41:11,300
or they just believe AI is magic

816
00:41:11,300 --> 00:41:13,300
and they plug things in where they shouldn't,

817
00:41:13,300 --> 00:41:16,300
they are reducing everybody's productivity.

818
00:41:16,300 --> 00:41:18,300
So, that's a point worth thinking about.

819
00:41:18,300 --> 00:41:21,300
And I wonder if it would surprise anybody here

820
00:41:21,300 --> 00:41:24,300
that I do not, in fact, speak Serbian.

821
00:41:24,300 --> 00:41:28,300
So, imagine if I had a really, really important email

822
00:41:28,300 --> 00:41:34,300
that I needed to write to someone in Serbian

823
00:41:34,300 --> 00:41:37,300
where I really care about my reputation and that relationship.

824
00:41:37,300 --> 00:41:40,300
So, I just go straight to open AI.

825
00:41:40,300 --> 00:41:43,300
I ask for that email to be translated.

826
00:41:43,300 --> 00:41:47,300
I get something out of there and I send it.

827
00:41:47,300 --> 00:41:49,300
What a disaster that could be.

828
00:41:49,300 --> 00:41:51,300
Maybe it's correct.

829
00:41:51,300 --> 00:41:53,300
But if I have no way of checking it,

830
00:41:53,300 --> 00:41:55,300
I'm gonna have problems.

831
00:41:55,300 --> 00:41:58,300
And this brings us to why it's so difficult to scale

832
00:41:58,300 --> 00:42:01,300
generative AI in the enterprise.

833
00:42:01,300 --> 00:42:03,300
To use it for personal productivity

834
00:42:03,300 --> 00:42:07,300
and to make an individual responsible for the output

835
00:42:07,300 --> 00:42:09,300
is quite an easy one

836
00:42:09,300 --> 00:42:13,300
if you already have a smart and productive person.

837
00:42:13,300 --> 00:42:18,300
But when you think about taking the person out of the loop

838
00:42:18,300 --> 00:42:21,300
and then at scale automating some processes,

839
00:42:21,300 --> 00:42:26,300
remember, these systems do make mistakes.

840
00:42:26,300 --> 00:42:30,300
It may be hard for humans to check.

841
00:42:30,300 --> 00:42:34,300
It may be hard to even define whether that output was right or not.

842
00:42:34,300 --> 00:42:41,300
So, how do you set up at scale this kind of automation?

843
00:42:41,300 --> 00:42:43,300
From now on, we're automatically going to write

844
00:42:43,300 --> 00:42:46,300
all our emails in Serbian with this system.

845
00:42:46,300 --> 00:42:49,300
You're gonna have to test the hell out of it.

846
00:42:49,300 --> 00:42:52,300
And that's what a lot of companies don't know how to do.

847
00:42:52,300 --> 00:42:56,300
So, no wonder we're getting this bottleneck in enterprise automation.

848
00:42:56,300 --> 00:43:00,300
And so, when should you trust an AI system?

849
00:43:00,300 --> 00:43:02,300
There are two paths to trust.

850
00:43:02,300 --> 00:43:04,300
The human in the loop model.

851
00:43:04,300 --> 00:43:07,300
So, make the human individually responsible

852
00:43:07,300 --> 00:43:10,300
and treat that as individual productivity increases

853
00:43:10,300 --> 00:43:15,300
or a hell of a lot of safety testing and safety nets.

854
00:43:15,300 --> 00:43:21,300
And as always, it is safest to have both.

855
00:43:21,300 --> 00:43:26,300
We're still so excited in what you potentially could do

856
00:43:26,300 --> 00:43:32,300
that we end up in organizations with death by a thousand pilots,

857
00:43:32,300 --> 00:43:34,300
get mandates from the top saying,

858
00:43:34,300 --> 00:43:37,300
everybody go find two, three use cases

859
00:43:37,300 --> 00:43:40,300
and everybody go try plug this into your business.

860
00:43:40,300 --> 00:43:44,300
And then take the human out of the loop too.

861
00:43:45,300 --> 00:43:47,300
They don't know how to do testing though.

862
00:43:47,300 --> 00:43:49,300
They don't know how to build the safety nets.

863
00:43:49,300 --> 00:43:51,300
They've removed the human in the loop

864
00:43:51,300 --> 00:43:54,300
and then they wonder why their use cases don't work.

865
00:43:54,300 --> 00:43:59,300
I do think that we're gonna have a lot more generative AI at scale,

866
00:43:59,300 --> 00:44:01,300
but this is the part to figure out.

867
00:44:01,300 --> 00:44:03,300
In fact, I'm a recovering statistician.

868
00:44:03,300 --> 00:44:05,300
And recovering also,

869
00:44:05,300 --> 00:44:09,300
because we are the grumpiest of the data scientist

870
00:44:09,300 --> 00:44:13,300
and we've sort of been folded into the data science profession for a bit.

871
00:44:13,300 --> 00:44:21,300
Definitively the less popular sibling in that family for now.

872
00:44:21,300 --> 00:44:24,300
And I cackled to myself slightly,

873
00:44:24,300 --> 00:44:29,300
because statisticians are gonna have to be back, aren't they?

874
00:44:29,300 --> 00:44:31,300
Someone's gonna have to figure out,

875
00:44:31,300 --> 00:44:34,300
does this thing in fact work the way that we think it works?

876
00:44:34,300 --> 00:44:36,300
Someone's gonna have to figure out the testing.

877
00:44:36,300 --> 00:44:40,300
Testing is still the most difficult part, especially in generative AI.

878
00:44:41,300 --> 00:44:44,300
And here is a little analogy

879
00:44:44,300 --> 00:44:50,300
that I encourage folks who are not in this business to really internalize.

880
00:44:50,300 --> 00:44:53,300
The data are like the ingredients.

881
00:44:53,300 --> 00:44:56,300
The algorithms are like appliances in a kitchen.

882
00:44:56,300 --> 00:45:01,300
When we were talking about doing applied AI last decade,

883
00:45:01,300 --> 00:45:05,300
we were talking about innovating in models, in recipes.

884
00:45:05,300 --> 00:45:10,300
So creating a croissant that is sugar-free and gluten-free

885
00:45:10,300 --> 00:45:13,300
and dairy-free and delicious.

886
00:45:13,300 --> 00:45:15,300
How do you go about making a recipe like that?

887
00:45:15,300 --> 00:45:17,300
You get a bunch of ingredients, you get the appliances,

888
00:45:17,300 --> 00:45:20,300
and then you tinker, you doubly play, you hope.

889
00:45:20,300 --> 00:45:22,300
You do a bunch of taste testing,

890
00:45:22,300 --> 00:45:25,300
you get your croissant or cookie or whatever it is,

891
00:45:25,300 --> 00:45:29,300
you start being able to produce them at scale, how wonderful.

892
00:45:29,300 --> 00:45:33,300
Now, with the generative AI revolution,

893
00:45:33,300 --> 00:45:35,300
it's equivalent to saying, instead of,

894
00:45:35,300 --> 00:45:37,300
I give all of you the cookie that I've made

895
00:45:37,300 --> 00:45:40,300
and you can eat it or not eat it right here, right now,

896
00:45:40,300 --> 00:45:43,300
and you don't have to worry about where it comes from.

897
00:45:43,300 --> 00:45:48,300
Instead, I'm saying, here, get access to my cookie maker,

898
00:45:48,300 --> 00:45:51,300
the cookies, and take them and turn them into whatever you want.

899
00:45:51,300 --> 00:45:55,300
Anything creative, Valentine's Day, baskets, spray-painted gold,

900
00:45:55,300 --> 00:45:58,300
call it art, whatever you want to do.

901
00:45:58,300 --> 00:46:02,300
But you see that we do lose an interesting layer of control there

902
00:46:02,300 --> 00:46:07,300
because we have a separation in this system

903
00:46:07,300 --> 00:46:11,300
from whatever the dish was to however you're going to use it.

904
00:46:11,300 --> 00:46:16,300
And the users down the line of these products,

905
00:46:16,300 --> 00:46:21,300
you are not told very much about where those ingredients came from,

906
00:46:21,300 --> 00:46:24,300
how they got processed, what their quality was,

907
00:46:24,300 --> 00:46:28,300
what were those appliances, what even went on in that kitchen.

908
00:46:28,300 --> 00:46:30,300
Is it poisonous, isn't it?

909
00:46:30,300 --> 00:46:33,300
But you're going to have to test everything yourself again from scratch

910
00:46:33,300 --> 00:46:36,300
if you're a downstream user, trying to do this

911
00:46:36,300 --> 00:46:41,300
at enterprise scale for tasks that matter.

912
00:46:41,300 --> 00:46:44,300
For some tasks, though, truly, take a thing, spray-painted gold,

913
00:46:44,300 --> 00:46:46,300
what do you care what the ingredients were?

914
00:46:46,300 --> 00:46:48,300
It looks about right, now it's gold

915
00:46:48,300 --> 00:46:52,300
and it still looks about right, everything's good.

916
00:46:52,300 --> 00:46:55,300
But where the ingredients might have mattered,

917
00:46:55,300 --> 00:46:57,300
where the quality might have mattered,

918
00:46:58,300 --> 00:47:00,300
it's up to you to figure out the testing.

919
00:47:00,300 --> 00:47:05,300
And finally, leaving you with the thought of whose job AI

920
00:47:05,300 --> 00:47:07,300
will automate as a consequence.

921
00:47:07,300 --> 00:47:10,300
Yes, it is about engineering and automation,

922
00:47:10,300 --> 00:47:13,300
but as a consequence of these technologies,

923
00:47:13,300 --> 00:47:14,300
what are we going to see?

924
00:47:14,300 --> 00:47:19,300
In the US, what we are seeing is the second quartile

925
00:47:19,300 --> 00:47:25,300
having the most effect, taking the most economic damage

926
00:47:25,300 --> 00:47:27,300
in these technologies, the second quartile

927
00:47:27,300 --> 00:47:30,300
of skill level and income.

928
00:47:30,300 --> 00:47:32,300
And this is puzzling economists.

929
00:47:32,300 --> 00:47:35,300
Why not the people at the very bottom,

930
00:47:35,300 --> 00:47:37,300
the least skilled labor?

931
00:47:37,300 --> 00:47:40,300
And why not the most skilled labor?

932
00:47:40,300 --> 00:47:42,300
Well, I have a guess.

933
00:47:42,300 --> 00:47:46,300
What's happening in the second quartile

934
00:47:46,300 --> 00:47:50,300
is the tasks that are most repetitive and most digitized.

935
00:47:50,300 --> 00:47:54,300
If it's repetitive and digitized, if it's copy-pasting,

936
00:47:54,300 --> 00:47:59,300
if it's doing things on mental screensaver with a computer,

937
00:47:59,300 --> 00:48:05,300
those are the tasks that are most likely to be rendered

938
00:48:05,300 --> 00:48:09,300
no longer necessary for humans to do.

939
00:48:09,300 --> 00:48:13,300
Whereas the difficult skills of having taste,

940
00:48:13,300 --> 00:48:16,300
of being creative, of thinking, of problem solving,

941
00:48:16,300 --> 00:48:19,300
of being a great engineer,

942
00:48:20,300 --> 00:48:25,300
those skills and those jobs are still extremely important,

943
00:48:25,300 --> 00:48:29,300
no matter how good your tools get.

944
00:48:29,300 --> 00:48:33,300
But there is a small economic issue here,

945
00:48:33,300 --> 00:48:35,300
which could become a very big one.

946
00:48:35,300 --> 00:48:38,300
What we take for granted is how our young people

947
00:48:38,300 --> 00:48:42,300
become our senior trusted leaders, artists,

948
00:48:42,300 --> 00:48:45,300
developers, managers, et cetera.

949
00:48:45,300 --> 00:48:49,300
And nobody trusts someone fresh out of university.

950
00:48:49,300 --> 00:48:51,300
I don't know, do you?

951
00:48:51,300 --> 00:48:54,300
So what does that path look like

952
00:48:54,300 --> 00:48:58,300
from newly graduated to senior and trusted?

953
00:48:58,300 --> 00:49:03,300
Often a bunch of easy-to-measure output.

954
00:49:03,300 --> 00:49:06,300
That's pretty repetitive, a bit mindless.

955
00:49:06,300 --> 00:49:10,300
You understand the person's character by working with them.

956
00:49:10,300 --> 00:49:12,300
You see if they go a little outside the box,

957
00:49:12,300 --> 00:49:15,300
you give them very low trust tasks.

958
00:49:15,300 --> 00:49:19,300
Exactly the kind of tasks that are going to get automated here.

959
00:49:19,300 --> 00:49:21,300
And what you will see is that bosses, leaders,

960
00:49:21,300 --> 00:49:23,300
are now going to absorb a lot of the work

961
00:49:23,300 --> 00:49:27,300
that their junior staff members would have done.

962
00:49:27,300 --> 00:49:30,300
Great in the short term.

963
00:49:30,300 --> 00:49:33,300
How are you going to get the next generation of bosses, though?

964
00:49:33,300 --> 00:49:37,300
What is your plan for developing your talent

965
00:49:37,300 --> 00:49:39,300
to a point where you can trust them?

966
00:49:39,300 --> 00:49:42,300
Because the skills that are most likely to get left

967
00:49:42,300 --> 00:49:45,300
are those skills where you need to apply some taste,

968
00:49:45,300 --> 00:49:46,300
where you need to be trusted,

969
00:49:46,300 --> 00:49:50,300
where you need to have judgment and good sense and expertise.

970
00:49:50,300 --> 00:49:53,300
And where, when you look at the code,

971
00:49:53,300 --> 00:49:57,300
where you look at the output, where you look at the art,

972
00:49:57,300 --> 00:50:00,300
you are able to judge whether that's what you're looking for.

973
00:50:00,300 --> 00:50:02,300
If I never learn how to speak Serbian,

974
00:50:02,300 --> 00:50:06,300
how am I going to deal with that email output in Serbian?

975
00:50:06,300 --> 00:50:10,300
How are we going to incentivize me to learn the language,

976
00:50:10,300 --> 00:50:14,300
whether it's a programming one or a human one,

977
00:50:14,300 --> 00:50:18,300
to get to the point where I'm able to supervise

978
00:50:18,300 --> 00:50:21,300
the functioning of these systems rather than do the task myself?

979
00:50:21,300 --> 00:50:28,300
This is a big piece that is missing in our economic plans globally.

980
00:50:28,300 --> 00:50:31,300
And so as you, if you are beginning to automate

981
00:50:31,300 --> 00:50:34,300
with these systems in your organizations,

982
00:50:34,300 --> 00:50:36,300
as you're taking responsibility for this,

983
00:50:36,300 --> 00:50:39,300
please pay attention to the three big topics from this talk.

984
00:50:39,300 --> 00:50:43,300
One, data quality matters, and today it's nobody's job.

985
00:50:43,300 --> 00:50:45,300
That's a problem.

986
00:50:45,300 --> 00:50:49,300
Two, testing these things is difficult,

987
00:50:49,300 --> 00:50:53,300
especially when we're talking about generative AI at scale

988
00:50:53,300 --> 00:50:55,300
without humans in the loop.

989
00:50:55,300 --> 00:51:00,300
And three, it's going to be up to you to create that plan

990
00:51:00,300 --> 00:51:03,300
for how you're going to be able to train your staff

991
00:51:03,300 --> 00:51:05,300
and not leave anyone out,

992
00:51:05,300 --> 00:51:07,300
because your most valued staff members,

993
00:51:07,300 --> 00:51:10,300
the ones you really do want to have around,

994
00:51:10,300 --> 00:51:15,300
often need that path of going through a gauntlet

995
00:51:15,300 --> 00:51:21,300
that these tools may render less attractive in the short term.

996
00:51:21,300 --> 00:51:25,300
And that's why we have a term like secret cyborgs in America,

997
00:51:25,300 --> 00:51:27,300
so people who want to use these tools,

998
00:51:27,300 --> 00:51:31,300
they don't want their managers to know.

999
00:51:31,300 --> 00:51:33,300
That should be a warning.

1000
00:51:33,300 --> 00:51:36,300
It should be a warning that we don't trust managers

1001
00:51:36,300 --> 00:51:42,300
to handle that transition fairly, carefully, and responsibly.

1002
00:51:42,300 --> 00:51:48,300
And so how do we in this room think about championing,

1003
00:51:48,300 --> 00:51:52,300
a gentle and also effective and intelligent

1004
00:51:52,300 --> 00:51:55,300
way through the challenges of today

1005
00:51:55,300 --> 00:52:00,300
to get to a highly productive economy and workforce tomorrow?

1006
00:52:01,300 --> 00:52:03,300
Thank you very much.

1007
00:52:06,300 --> 00:52:08,300
Thank you.

