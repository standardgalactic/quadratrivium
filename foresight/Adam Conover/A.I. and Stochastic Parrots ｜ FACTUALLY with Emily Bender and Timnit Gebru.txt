Hello, and welcome to Factually. I'm Adam Conover. Thank you so much for joining me once again as I
talked to an incredible expert about all the amazing things that they know that I don't know and that
you might not know. Both of our minds are going to get blown together and we're going to have
so much fun doing it. I want to remind you that if you are watching this podcast on YouTube,
go subscribe to the podcast and your favorite podcast player if you want to hear it every week.
If you're listening on your podcast player, go check out the video episode on YouTube. Now this
week we're talking about AI. Just a few weeks ago, I released a YouTube video called AI is BS in
which I argued that AI has become a marketing term that tech companies are using to hype up
software that cannot do what they claim it does and in which in many cases could be dangerous if
they jam it into mainstream products that it is not ready for. In fact, these companies are hyping
up AI to such an extent that they're trying to convince people that software like chat GPT is
a step on the road to some kind of godlike artificial general intelligence when in reality
what they actually made is a text generator that can write some pretty cool fanfic and help you
program and you know if they had marketed that way in the first place they said hey we made a tool
that'll output a recipe that tastes bad if you try to cook it. I mean that would be pretty neat. We
could think of a lot of uses for a text generator like that but a step on the road to true artificial
intelligence it is not and it is kind of fucked up to tell people that it is. Now that video got a
somewhat let's say divisive response because a lot of people on the internet have drunk the AI
hype Kool-Aid. These tech companies have succeeded in confusing the issue of AI so much that a lot
of the time when we say AI most of us don't even know what we're referring to. We don't understand
how the software works and we don't understand how that's connected to the science fiction fantasies
that the companies are peddling us and because of that confusion a lot of weird shit is happening.
For instance while I was in the process of editing my video 1600 major AI researchers
as well as people like Elon Musk and Steve Wozniak came out and asked for a six month
pause on AI research but they didn't ask for that pause because of you know AI giving out
misinformation or the fact that it just recycles the copyrighted work of artists like myself and
others no they asked for that pause because they were worried that it could create an AI
superintelligence again the bullshit hype science fiction claim so a bunch of other AI researchers
came out against this letter saying that we do not have a problem with AI because of the super
intelligence thing we have a problem because you are exploiting the work of real people and making
the world a worse place right now so I don't blame the people in my comments for being confused
even AI researchers themselves do not agree entirely on what the problems are so for that
reason we are going to spend a couple episodes of this podcast talking to some of those AI
researchers about what the problems are and how we might go about fixing them and on the show
today we have two incredible guests their names are Emily Bender who's a professor at the University
of Washington and Timney Gebru who's the executive director of the distributed artificial intelligence
research institute and you might recognize the name Timney Gebru because she is the researcher who
was famously fired by google for raising AI ethics concerns in that famous paper I am so excited to
talk to them because they are two of the sharpest minds on AI and how the problems with it are not
what the tech companies have been telling you but before we get to that interview I want to remind
you that if you want to support this show please head to patreon.com slash adam conover you can
get every episode of this podcast ad free and get a bunch of other goodies and even more importantly
please come see me on tour this summer I'm taking my brand new hour of stand-up to San Francisco,
San Antonio, Tempe, Arizona, Batavia, Illinois just outside Chicago, Baltimore, Maryland and St.
Louis, Missouri head to adamconover.net for tickets come see me I'd love to give you a hug in the
meet and greet line after the show and now without further ado let's get to my interview with Emily
Bender and Timney Gebru Timney and Emily thank you so much for being on the show super happy to be here
thank you for having me it's an honor to have both of you considering you know I've read your work
I've talked about it in my last youtube video all about AI you're some of the foremost researchers
on the topic some of the foremost critics of how the tech industry has been employing AI
so I'd love to hear from you first of all Emily we last talked it was might have been close to a
year ago back when AI was very much an active research subject we were hearing a lot about it
but it wasn't something that the average person was using in the year since AI has become radically
mainstreamed a lot of these companies have just shoved it into consumer products without you
know any concern for what the results are as two as two people who follow the field extremely
closely what has your reaction been to the last you know six months or so of rapid development in
the industry blank faces here it's been a lot of oh come on again more seriously yeah that's
that's how I feel I mean I just I am dumbfounded by the number of people I thought were more reasonable
than this kind of jumping on a bandwagon of what seems to be mania so I don't know that's how I feel
what do you think the greatest you know potential harms of this are in your paper your very famous
paper on the dangers of stochastic parents you talked about many dangers that you know these would
reify you know discriminatory materials in the training data by repeating it out to people that
people would take it to literally you know would take the pronouncements of a large language model
as fact uh a lot of those have seemed to come directly true do you feel validated that your
that your criticisms have come to pass no no because those were predictions those were warnings
like don't do it you know we don't want to get there and then we got there and then some right
like I just I definitely don't feel validated I feel upset I'm sad about it um yeah how do you
feel Emily you know I think that some of the biggest problems that maybe I didn't understand
because it's it's sort of half an economic problem um is the way in which people would say hey this
looks like it could be a robo lawyer this looks like it could be a robo therapist and look at all
those people who can't afford real lawyers and real therapists so let's give them this instead
and like that jump from um you've identified a real problem in the world it's a problem that
mental health resources are inaccessible and it's a problem that legal representation is inaccessible
but then try to fill that hole with something that is just a joke and can directly cause harm
when deployed in those cases I think even when we were writing the paper and saying you know it
would be bad if this was set up in such a place where people might believe it or believe it knew
what it was talking about I don't think I was in a position to predict that that's a direction that
it would go in at the time we wrote the paper I was just you know seeing this whole mind is bigger
than yours kind of race and just being very confused why is this the thing that anybody
everybody just wants to be the biggest one and now and now you have not just that text to text
models but text to image text to video video or whatever you know and so I didn't I didn't imagine
that in such a short time that kind of explosion of synthetic media into the world would happen
and I also didn't think about I would say what the content moderation demands and issues would be
with that much like explosion of synthetic media you know like the um the what is the fix
Clark's world shutting down submissions because they got a little bit devious yeah stuff like that
is something I didn't predict Emily you talked about just now the you know the sort of thing
happened that happens a lot of technology once it's released people come up with new uses for it
that nobody predicted and one of the uses that people have started you know uh do you talk about
robo lawyers uh I've seen people say that they're using uh chatbots as therapists um or as relationship
surrogates and what are the dangers of of those types of uses because I'm certainly seeing those
promoted all over the there's a lot of folks saying hey if you don't have access to xyz and
ai can do that for you in all sorts of fields and and you said that these are a joke what makes them
a joke for that purpose so they're a joke because with they're all form and no content right so
what these systems are really good at is mimicking the form or the style of something so it absolutely
can write something that looks like a legal contract for you but if your purpose in drafting
up a legal contract is anything other than intimidating the other party with legalese
then the specific content and the way that it maps into your situation really matters yeah and it
might be that there's some sort of template type situations where it's like okay yeah this is a
contract for you know the rights to use a piece of music um and um I want this right assigned and
that went not and it's going to be paid for this much and here's like you could answer a few questions
and get something out from a template that would work reasonably well but that's not what they're
doing right they're saying what's a plausible next word what's a plausible next word given this context
and you know who knows where that's going to be um so for the the legal case you know you're asking
for that because you are not a lawyer and you can't afford a lawyer you're not going to be in a position
to tell if it's good or not but it'll look impressive right right it it sort of will can it
will create a convincing imitation of a piece of text that will most readily convince someone
who knows nothing about the field like a lot of you know I work in television writing and there's
a lot of talk in you know oh can can studios use chat gpt to write scripts and when I use one of
these services to uh write you know to output text I'm like yes this superficially looks like a script
but it's missing so many of the things that you would need to film a script and someone might say
well what if the technology gets better and it's not a matter of aping something even more correctly
it's a matter of to successfully write a piece of screenwriting you need information about the
rest of the world that a no algorithm or ai program could ever have you need to understand
uh you know what is physically possible to produce you need to under you need to talk to
a department full of people who say uh a very good example I use of this is I didn't realize until
I started writing television that you can never have someone jump into a pool on television if you
watch tv and someone jumps into a pool it'll always happen off camera and I had because I had a scene
where I someone jumped into a pool my line producer told me they you need to remove that and I said
why it seems not that hard they said because we need to film every take five times so that means
we need five pairs of wardrobe because we need to film them one after another and we need to dry
the person off and do their makeup and it's going to take all day and so as a result people never
get wet on television or when they do it's very expensive okay interesting and you'd have no way
of knowing that uh without real life experience and even if an ai could eventually figure that out
there's also a million other things like that that are specific to the particular production
hey it's going to be cloudy on wednesday we need you to rewrite the scene you know there's there's
so many details that are that are fundamentally about humans communicating with each other
and that's the same thing with with a lawyer doesn't just output text a lawyer talks to
like knows what the other side might do in response knows how aggressive they'd be if you're
trying to sue laws are yeah exactly if you're trying to evict a tenant they have they're going
to have a much different response than if you're trying to sue the church of Scientology right
who are very aggressive like and knowing what the laws are too so to me this is it's very obvious
when i actually look at how they're used but it's it is it a problem with the technology or is it a
problem with humans not understanding how our own society works to to not realize that these that
these tools are going to be effective the hype too the hype yeah so it's a problem with the task
technology fit so what is it that we need and how does the technology fit into it and and timmy
and i want to bring up your wonderful line about how these things are unscoped technologies
and then maybe you could elaborate on that a little bit yeah i mean i was going to bring up
all your work on hype but which i think um and i really add them i mean it just like when i'm
talking to you i'm like yeah we live in the same planet and you know we're having the same
conversation in the same language i i am not that's not the language that we're speaking with the
other researchers in ai or machine learning or whatever it is i am i'm so confused what's going
on but because you know um scoping systems is a very basic engineering concept right when
you're building something you want to know what you're building it for and then see if what you're
building it for is actually being fulfilled whereas in this case the way they're advertising
their systems is that they're building it to accomplish anything for everybody anywhere
write code speak whatever language you know write scripts movie scripts protein folding
whatever it is and that's a fundamentally unscoped system that i don't even know how
we can make sure um can be safe or work um to add to the notion of being an unscoped system
right which to me is a basic um engineering concept when you're trying to build something
you ask what am i building uh to accomplish what are the tasks that i want to accomplish
under which scenarios under which conditions and in this case when you see the kinds of things that
they are um advertising um all of these companies metta talked about this um uh large language model
based system they had called galacticon they said oh it's gonna write code just protein folding stuff
and write scientific papers and more you know and with opening eyes chat gps like write movies
replace artists do this do that and more you know and so already you just haven't
have built a system that we don't even know what it's supposed to use for to be used for and how
do we even test whether it is actually accomplishing its um task the task that it's supposed to be
built for and one one problem here is that uh open ai is not at all open about how these things
are trained and so not only is it not tested in specific contexts where you can say okay
here here is the range of um here are the safety parameters of the system here's how well it has
been tested to work in these contexts we don't have that information we also don't know what
is training data or training regimen was and according to open ai this is somehow for safety
which makes no sense at all because one of the very first things that was worked out about
responsible development of these kinds of systems is provide documentation of the underlying dataset
and of the parameters of sort of safe use of the model and that's kind of the first place that
tim meet and i got to know each other independently right we were both working on this um separately
yeah and then and then we connected through that sort of related work and that was you know really
really fortunate it was 2017 i think there was just something in the air where a whole bunch of
groups said we got to document these things so that we could figure out how we could use them
and open ai while claiming to be doing this for safety is flat out refusing to do that
and what's the danger of that if they are refusing to release the or make the model transparent
so you can't make any decisions about whether it would be good to use the model or not if it's
not transparent like let's say i want to use it for writing computer code well i can try it a few
times and see if it seems to work well and then maybe get some confidence but i don't know what
its training data looks like and for programming languages from what i've heard again they're not
open about this but part of the training data is literally sort of english descriptions together
with executable codes there's a lot of paired stuff in there that helps it do well a lot of the time
the other thing about programming languages is that they are specifically designed to be unambiguous
which is in stark contrast to natural languages where ambiguity is sort of a fundamental design
feature everything is ambiguous and so the fact that it does well with this more constrained
universe of programming languages kind of makes sense but again we can't really know because we
don't know how it's trained but imagine like you're happy with this performance in helping you
generate code and you've even got some like computer security buffs on your team and they
look at that say yeah i don't see anything frightening coming out here but they keep changing
the system and then all of a sudden it's maybe putting that into your code but there's no information
about what version you're using you can't say no i want to keep using the version you know from
december of 2022 because that's gone all you have is the open it uh the open ai um api is what i was
trying to say that um allows you to connect with whatever they've put up for you to connect with
and a lot of tools are being built on their api right now there's uh if you open the app store
any kind of app store you'll find countless tools that are ai xyz you know ai help you write code
ai help you write a movie script ai therapist and they're really just hooking into open ai's model
and paying them a couple pennies per however many requests uh and people are now starting to use
those tools to do real things without knowing what is where the the output is coming from or
what the model is it does kind of remind me a little bit you're talking about models like i've
talked to plenty of climatologists on the show and like climb you know climate models are a huge
part of our understanding of how the climate works but we also know how those models work and we
can compare them and we have a lot of information about them so that we know when they predict
something you can go back to the what the source was but in this case it's both by design but also
by corporate structure a black box because they're not telling us anything about it i was just
thinking about something even more basic like how do we know we're not still they're not stealing
people's work um to profit off of it so you know there were all these um lawsuits by artists to
devian r i stability ai and mid journey right but not open ai not dally because we don't know
what training data they were used so we don't know if they were copyright violations or not if they
constantly you know compensated anybody versus not but they can do that right they there's nothing
that is like preventing them from doing that right now yeah and it makes me wonder so look i can go to
into chat gpt i talked about this in my youtube video i can go into chat gpt and say write an
episode of adam ruins everything about xyz and it'll output something that looks like a pros
version of adam ruins everything adam walks into the room and says blah blah blah about dogs and
i'm like where is this coming from because i don't believe it has access to my shooting scripts
because those are not public anywhere so i'm trying to figure out where is it getting the
information from and again this is based on my own copyrighted work that i spent a lot of time
putting together is the character i created and i would think that you know i would if it's being
used for profit i would like to be paid for it in some respect um i think that's a pretty fundamental
feature of our uh how capitalism is currently arranged and i would like it to follow those rules
but it's difficult for me to tell i'm like it could just be getting it all from like fan fiction
like i feel like it's really scraped like our archive archive of our own and all these big
fan fiction sites um but it's really it's really really difficult to tell let's spend a second
though and talk about open ai as an organization and the sort of ideology behind it because this is
you know the organization that is most in the news pushing uh ai forward and it was incorporated
originally as a nonprofit right and made a lot of noise about how the point is to make
sure they're going to do it responsibly it's research it's not about profit but it has recently
i believe changed its incorporation status to be a for-profit company completely changed its tune
and they now say it wouldn't be safe if we were to open it up to all of you but meanwhile you've
got the founder sam altman going on all sorts of podcasts talking to the news about how he has to
do all this because ai is really really dangerous but then the dangers they're always talking about
are always the science fiction kind where how the robot takes over the spaceship or you know from
isek azimov kind of style of science fiction where a super powerful intelligence you know takes over
the universe kind of thing they're never talking about the harms that we're talking about that
people might use it to write a legal document it shouldn't write or that it might rip off somebody's
copyrighted work or anything like that so what is your view of you know this this organization and
it's uh supposed altruism is this just a faint to sort of trick us all into thinking that they
have our best interests at heart or is as a corruption happened or what so this is the
test grill question and i'm gonna let tim need to find test grill but just my take on this very
quickly is i think they believe they are being altruistic and working in the best interests of
people but their view of who counts as a person is very narrow and sort of leaves out of you all
of the people who are being harmed now or just sees those harms as inconsequential compared to
what they're worrying about which is in this science fiction universe it's hard to say the
phrase science fiction fantasy because to me those are two genres of wonderful speculative
fiction and you don't want to bucket this into that yeah you're just so i you know i can say so
much about them i've been on them for a long time so in 2015 when they were announced i wrote a open
letter that i didn't end up sending to anybody i just kept it to myself because i was a phd
student back then and people were like uh people will know that it's you because i was so angry
by the tone so i don't think it's a corruption or they've changed their tone or whatever to me
they've like stayed exactly the same um and initially they um said they exactly they talked
to they wrote you know they talked about it as if they were gonna save humanity peter teal and
illa musk always as usual just on the ball to save humanity of course that's always what they've
been doing in the world and um and all the whole media was talking about it like oh this
nonprofit is starting they're gonna save humanity from ai because back then what happened is that
they had invested in deep mind that they also wanted to create you know a gi artificial general
intelligence which is a system that none of us know what it even is supposed to do this is the
acronym for a for a super intelligent uh uh sounds like a god sounds like a god to me um and so
they were all very much trying to develop this thing which i don't even know what it is and um
deep you know invested in deep mind deep mind got bought by google 2015 the future of life institute
had a similar letter asked to the one that we see the pause letter um and then they you know found
open ai put bill you know hundreds of millions of dollars into it because they say they're gonna
save humanity and all of that and create this agi thing fast forward right they realize they need
a lot more money uh they're now essentially bought by um microsoft and then now they have a competition
so they need to be closed and all of that so to me really it wasn't like a pivot or anything i
never believed that they were gonna you know save humanity or anything like that and in terms of
emily's um q about the test grill bundle um so you know i i have been really so irritated by the
whole crew because i've been around them for a long time i went to school with some of them
been around this you know agi community for a while so recently i teamed up with um a collaborator
of ours um whose name is emil torrez who used to be a long termist and so a long termism is this
weird you know the future of life institute behind the pause letter is a long termist institute and
so they literally think that our um job as humans is to maximize the number of future humans who
colonize space and digitally upload their minds and like live in the matrix kind of thing right
this is a real thing like it's not an exaggeration that's what they want yeah i've read i've read a
lot of that philosophy that you know the idea that we need to be thinking about how do we maximize
the future uh happiness and well-being of humans 10 000 years from now if you could why save one
life today when you could save 10 million lives uh 10 000 years from now now i'd say how the
fuck do you know that what you're gonna do is gonna have any effect on people that far in the
future it's the height of hubris to think that you can project that far into the future at all
well they give you some random numbers they pull some number numbers out of their asses like oh
my god we didn't know sam bankman freed was gonna be doing this but we will know what 10 000 years
or now point zero zero one probability that you know what i mean it's absolutely ridiculous but
anyhow so the test grill bundle is a bunch of ideologies that are all sort of descendants of
the first wave eugenics um eugenics movement and you know when you hear this word about human
flourishing maximizing our potential through both positive and negative eugenics positive would be
the ones who are desirable you want them to breed you want them to you know
multiply right and the ones who are negative the ones who are undesirable you want to kind of get
rid of them because they don't help you with this human flourishing thing so the transhumanists
you know were very much um that that ideology was very much developed by 20th century eugenics
um and nick bostrom who is also a long-termist you know it's also very famous very prominent
transhumanists right and so um we traced how these ideologies transhumanism extropianism the
singularity people who say singularity is coming because of ai the cosmos who are actually the
people who wrote the first book on age artificial general intelligence in 2007 the effective altruism
the long-termists and how they're all in this circle kind of learning from each other networking
with each other lots of money going into them and they're all sort of either selling agi utopia
or agi apocalypse right if we do it right it's going to bring us utopia it is out human flourishing
we need to do it if we do it wrong we're going to have an apocalypse because it's going to take
over the world or china is going to do the devil kind of agi and you need to let us do this utopian
kind because we're vanguards of humanity so it's obviously a very kind of convenient
ideology for the billionaires because you know they're saying give us all the money
we'll do the utopian kind um and but we're super worried about it because it might be
super powerful but we're careful because you can trust us right and so sam altman is kind of is
doing that thing right and to me he's in the same sort of camp as the future of life people
because that's the same thing they're selling yeah i've the connection to eugenics is not
theoretical i've i've seen it myself if you look at nick bostrom's writing and the writing of a lot
of folks who write extensively about ai or agi the future you know super ai that could control the
world um a lot they also write overtly about eugenics they have charts and tables about if we
what if we started a human breeding program and only allowed people in the top percent
of intelligence to breed and then they would have super babies and the babies would be super
smart and it's like this was tried in the 40s in a country in europe you know this this is um
this is very these are very old ideas by the way you can just look at a the interview i did a
couple weeks ago about intelligence to to learn about whether intelligence is actually heritable
in the in that way it's not um but so these the proximity of these ideas to each other is not
theoretical these are you know the same folks promoting neo eugenics and promoting um ai
catastrophism uh i want to refer though to this pause letter that you mentioned a couple times
so that folks know what it is um a couple weeks ago actually as i was editing my ai video um a
whole bunch of ai researchers from many many different organizations signed a letter suggesting
a six month pause on ai after the release of gpt4 and they said well this is very dangerous
we need to evaluate it um and etc etc and some folks who i you know i i have read and and enjoyed
as ai researchers are signed to the letter um and it sounds on the face of it that that might rhyme
with some of what you folks are saying um but you took objection to the letter and so i'd love
a little bit of explanation from you about about exactly what your uh issue with that is like what
what did that letter get wrong do we do you feel we need to pause or do we need to pause for a
different reason or what so i think pause is unrealistic i think six months is unrealistic
i think the letter makes it sound like these researchers are just now noticing that this
might be harmful despite you know years and years and years of work of people saying hey there's
harms here um and the letter itself is basically saying oh no we've built something too powerful
better be careful so it's it's what livencel calls pretty good yeah we're too good gotta stop
so it's it's basically helping to sell the technology um i have to say that i found out
about the letter a little bit before it dropped because there was a um a journalist who contacted
me asking if i was going to sign it and would i comment and i'm like haven't seen it not going
to comment on what i haven't seen and then like i think later that day it came out and i was busy
and then finally evening i sat down to read it and i thought oh god i have to i have to react to this
so i put out a tweet thread um and then you know media you know craziness about it and so i said
to tim neat and the other two listed authors of the stochastic parents paper let's put together
a statement coming from us so that we can point the media at that for one thing but also to have
sort of a joint statement here and and tim neat sort of took everybody's remarks including my
relatively snarky twitter thread and like pulled together our first draft that we then worked on
and where we start with that is with the observation that they cite us in their first footnote number
one yeah they say your stochastic parents paper number one and number two is nick boston yeah
wow oh wow okay they got they went all the way from alpha to omega there huh they're they're
inciting everybody but they didn't ask you to sign the paper that's interesting oh they would know i
would never yeah okay so their sentences ai systems with human competitive intelligence
can pose profound risks to society and humanity as shown by extensive research footnote one
and acknowledged by top ai labs footnote two and that footnote one has us embostrum and some other
people but the stochastic parents paper was not a paper about ai systems with human competitive
intelligence it was a paper about large language models which are not ai systems with human
competitive intelligence as we note so many times in the paper like that was the thing we said yeah
so like right off the bat it was just infuriating and there's like one or two things in here that
i think do rhyme as you say so you know we need regulation and that regulation should involve
things like watermarking so that we can tell when we're encountering synthetic media and
um you know liability for ai caused harm that sounds good that liability should sit with the
companies that are creating and deploying the ai they don't say that um but then there's a bunch
of really weird stuff in here ai research and development should be focused on making today's
powerful state-of-the-art systems more accurate safe interpretable transparent robust okay i'm
all right with all those aligned is a keyword for this weird one trustworthy yeah but then the last
one the last one is loyal a loyal loyal to who oh wait no no sorry they started doing Boy Scouts
a scout is trustworthy brave reverent kind obedient or whatever i quit Boy Scouts when i was like 10
but there's like a whole list of things in one of them's reverence so i'm surprised they didn't
include reverence shouldn't it go to church yeah but here's the thing and ai i mean so
there's the first question that timnit raises of okay loyal to whom whose interest is it serving
but also these are not the kinds of things that can be loyal to be loyal is to experience
certain feelings to have certain commitments and large language models are just text synthesis
machines yeah so they predict what comes next or they're i hear a really great description i heard
of them is to think of them as word calculators they they do a good job of you give it a bunch of
words and it can turn them into other words that are derived from the first words and that can be
a useful thing to do sometimes particularly if you're a computer programmer or someone else who
like is manipulating text on that sort of level but that's not a it's it's not a thing that has
an ethical drive such as loyalty yeah yeah exactly so so this this letter you know got a lot of
attention partially because of who signed it and you know we've we've had so we we pushed back pretty
quickly and then we were getting reactions like oh you're squandering the opportunity this is gary
marcus complaining about us going out for blood coming out what is what did he say coming out
they went for blood we went for blood or something like that or it's just like okay and sure and
basically it's like they supposedly created an opportunity for regulation that would maybe get
the six month pause whatever that means it's it's all completely unfounded right pause on systems
more powerful than gpt4 well we don't have the specs on gpt4 so that's a unmeasurable undefined
thing anyway um and as someone was pointing out and i'm sorry i don't have the source for this
a lot of the work in creating new systems is actually in the data preparation
and gathering these enormous amounts of data and a six months pause on training the systems
wouldn't prevent anybody from going and collecting more data we need to prevent that in other ways
to prevent data theft but that's a separate question right um and then you get people out
there saying well why can't the so-called ai safety and ai ethics people get along so the ai
safety people are the long-termists who want to prevent the agi from taking over the world
and ai ethics is sometimes used to refer to the people who are concerned with the problems in
the here and now in the ways that yeah they basically created the term ai ethic i'm ai safety
to separate their themselves from us is how i feel about it because like we have the same
technical expertise we have other expertise also but like it doesn't mean that you know
so i feel like they they they named that that field or whatever it is to explicitly separate
themselves from kind of our crew right yeah yeah so by answer to why can't we get along it's like
well if why can't we find common cause if the ai safety people wanted to find common cause with
those of us working in ethics they would cite us they would go to the you know to to tim needs work
they would go to the work of syphia nobel and ruha benjamin and kathie o'neal two past guests on
the show by the way just want to ding ding ding ding excellent um and you know build on that and
lend some of their money and resources to making that happen but of course they don't want to
because they're aligned with corporate interests and to really push back and to really reduce the
harms here we need regulation that reigns in the corporations like why would ilan musk sign like
everybody has to ask why does ilan musk an advisor uh end funder a future of life institute someone
who pumped hundreds of millions of dollars into open ai and deep mind or whatever and whatever
why is he so interested in like caution and whatever as long as it doesn't touch him sure
you know if we're talking about regulating tesla or looking at the racial the largest racial
discrimination um lost in history in california that's not what he wants us to talk about right
like he doesn't want us to talk about any of those things and whatever he's doing with twitter
we have to think about oh my god like this super powerful science fictiony thing that's going on
and it's just so disappointing to see that number of people who went along with it i think for us
we wanted to make sure we wanted to make it clear that we are not aligned with this vision
of ai safety they hold eugenics roots um there's emily has a thing she always says always read
the food notes we write the food notes and they have a food note that says you know if we don't
do x y and z um we ai systems might have might be potentially catastrophic like other potentially
catastrophic things like eugenics and we wanted to be like eugenics is not just potentially
catastrophic it has been catastrophic you know what i mean so we just want to make sure that
they can't they should not be able to launder um people's reputations to make themselves mainstream
and appear reasonable i also think that there there's a huge number of unexamined assumptions
in that letter that they are using the letter to promote to the public that are essentially myths
about ai and i want to get into some of those and and ask you to react to them and maybe debunk
them but we have to take a really quick break we'll be right back when we're emily bender and tim
nick ebru folks friends family members you know you should be backing up your computer right you
know that you are just one glitch or one macbook in the toilet away from losing all your beloved data
and you're like maybe it's stored in the cloud somewhere but also you keep getting that little
notification saying you're running out of space pay more money and you're like but even if i pay
them more money it's still going to tell me i'm running out of space so is my stuff saved is it not
saved i don't know i just hope you know i don't drop my laptop in a river all right i understand
you i've been there before okay but i'm here to tell you there is a solution you can put those fears
to rest for just seven bucks a month if you sign up for back plays now look i know a lot of podcast
hosts will tell you that you use the product but in my case i actually do i have been a back place
subscriber for years before even having the opportunity to read this ad when the folks at
my podcast ad company asked me do you want to do a back plays ad i said hell yeah i do this is
actually a good product that i really use here's how it works you install it on your computer it
runs in the background and it uploads all of your stuff to their server encrypted okay you will never
see a message on back plays that says you ran out of space no matter how big your hard drive is you
got a 20 gig hard drive it'll back all that stuff up okay here's how it works you install it it runs
in the background and without you even noticing it uploads all of your data and backs it up securely
and with back plays you will never see that message saying you just ran out of space because you
cannot run out of space with back plays it backs up all your stuff okay i've got like 20 terabytes
of stuff on back plays and i sleep securely at night knowing that if something goes wrong i can
get all that back you can restore it anyway you want you can download one file you can have them
send you a zip of a bunch of files you can even get a hard drive from them in the mail with all
your stuff on it so you can put it all on a new computer it is such a seamless and secure system
i love using it and it helps me feel better and it'll help you feel better too they have restored
55 billion files for their customers that's almost two trillion gigabytes that they currently
have under storage and with their web panel you can restore your data anywhere in the world
they've been recommended by the new york times mac world pc world lifewire but more importantly
they're being recommended to you by me right now and get this they have a 15 day no credit card
required free trial if you want to see how it works at backblaze.com slash factually that's
plenty of time for you to upload some stuff downloaded figure out how the system works before
you even pay them a penny they don't even ask for your credit card first all right so seriously
back your stuff up check out backblaze.com slash factually the no hassle online backup it's a no
brainer decision according to pc world and according to me adam conover that's backblaze.com slash factually
okay we're back with emily bender and tim nick ebru so we were talking about the ai pause letter
and i was starting to talk about how it seems to have a lot of assumptions built into it about
how ai works and how it's going to progress that the people who wrote it and the people who found
it open ai and the people in the tech industry have really pushed onto the public and i see those
assumptions actually in my youtube comments i'll see them in the comments to this video when we post
it on youtube um and in the comments to my last one people say well ai is progressing so quickly
it's unstoppable it's progressing every single day and so this idea of the pause seems to like
build you know connect to that idea where oh my god this is a runaway train and all we can do
is try to steer it in a direction when you know we could be questioning like these are just humans
like making these things like they can do whatever they want at any time um and a and b is it maybe
not a foregone conclusion that it's going to progress in the direction that they say it will
like it seems to me that the large language models are designed to make you think oh this is a step
on the road to general intelligence to a literal thinking computer but uh and if you play with
it for five minutes you might think that if you play with it for you know tens of hours as i have
you stop thinking that and you realize it's just mashing text up um i'm curious if you know if we
could dig into some of that is ai something that is constantly going to keep improving no matter
what we do and we just need to like control it and make sure it's not going to destroy us
so i have a lot to say on this good so there's a um wonderful uh explanation that comes from
beth singler about how combination of like looking back in the in what's happened in science and
technology to date combined with science fiction and imaginings of the future makes us think that
there is a path that we are just racing along and it's only a question of how fast do we get
there who's going to get there first and that's not how science happens right science is exploration
it's communication it's choosing things to work on or not um i think there's some interesting
stuff in the history of nuclear power and how the interests of building nuclear weapons shaped
the decisions we made about what kind of nuclear power to work on for example um and you know it's
all as you're saying choices that we can make and we don't really know what's possible in the future
um but because of this idea of you know ai that's given to us from science fiction and
to say i'm a huge fan of speculative fiction um but i mean for yeah it's cool um but i'm largely in it
for the exploration of what happens to the human condition given these different settings like
that's what i see the point of science fiction to be and a lot of this seems to come from this
idea of no the point of science fiction is the cool spaceships and the teleport devices and the robots
right and yeah those are cool but like that doesn't mean that it's going to exist so this notion
of a path that we're just racing along as fast as we can is false and we don't have to buy it um
and another part of it is when they say and you repeat um ai is just progressing that makes it
sound like ai is doing it on its own and no what's happened is a lot of corporations and individual
billionaires have put a lot of money into gathering big piles of data and doing some clever engineering
about how to manage that data and then build these learning systems that compress it into
something that can do the word calculator thing um and that happened quickly way more quickly than
we thought it was like too many are both quite surprised by how fast this happened not because
the tech got incredibly cool incredibly quickly but because it sort of got out into the world
that quickly and what i see there isn't rapid scientific progress i see a lot of money and a
lot of hype and all of a sudden someone's got the money to set up this thing so that anybody
can access it apparently for free although every time you do that you're doing some work for open
ai just by the way um so it's not really free but chat gpt is less a technological advance and
more of a product that was created it was a way of taking something that already existed
and opening it up to people and prompting it in a way to maximize the sort of public
shock of it and to make people think that it was extremely capable and to sort of further
this narrative that uh things are progressing so quickly but it's there's a little bit of a comparison
to you know steve jobs invented the iphone well steve jobs didn't invent any technology
he combined a lot of technologies some of which were invented by the federal government 30 years
earlier um and like put them into a very well marketed product with a with a shiny wrapper
and like a really nice clean store you could buy it in um there's maybe a little bit of a comparison
there yeah yeah i think so um the the big thing with chat gpt the reason it just exploded all over
the media was anybody could play with it which was a brilliant pr move on open ai's part because
meant that everybody was doing their height for them right i mean i i would say you only need to
try to talk play with it in any other language for like two minutes i like to agree it doesn't even
it's complete gibberish you know what i mean so i'm like well i guess the agi speaks english we've
already assumed that you know what i mean like but it's you know it's so crazy how many people um
i've been talking to who are engineers and and researchers and they and and stuff say parrot
exactly the talking points of open ai and you know anthropic and similar organizations that are
making this point that everything is going to be built on top of it and it's going to trump
like any other kind of development so most of gdp is going to be dependent on that and so
whoever is not you know on top of whoever does not have that technology by like 2025 or something
like that they're not going to be able to catch up because it's just going to be accelerating
so fast you know what i mean this is the kind of stuff a lot of people are saying you hear this
argument that in response to the pause letter you heard people say well if we pause china's
going to keep making the ai and they're going to use it to kill us and like what are they going to
use the devil kind yeah gpt for they're going to write more shitty fan fiction with it are they
going to you know output more bad recipes like it's it's unclear what it's being presented as
some threat to national security when the actual capabilities of these large language models
they're cool they're very cool tech there's some cool stuff you can do with them but this is not
launching nuclear warheads or like you know fight like waging national security battles
but i'm sorry please continue your point now i was just that's basically it that's all i was
going to say and it's that it's it's a few the really surprising thing to me and what's been a
huge lesson i guess in history or current affairs whatever you want to call it is how few people
can drive this is is really what is unbelievable to me few billionaires a few billionaires and a few
people in in the space of deep learning together just can drive this entire thing the whole media
eco chamber the chamber the whole research direction the entire you know silicon valley
ecosystem and you know it's it's been extremely surprising to see that yeah and disappointing
to see microsoft and google and like i've got criticisms of these corporations but they were
pretty staid and stodgy especially microsoft like jumping on this um we should maybe talk about the
sparks do you want to talk about the spark yeah i was gonna say i wanted to cue you to talk about
the sparks of agi paper what's the sparks of agi anyway what is the sparks of agi paper this is
fascinating i'm the host emily what is the sparks of agi paper this is a uh something that takes the
form of a research paper it's not peer reviewed it was just thrown up on archive which is this place
that was initially developed i think by physicists to help disseminate research faster and what's
happened in machine learning and computer science more generally is that it's become this place to
like put things up as if they were research papers and just bypass peer review entirely and so there's
there's a whole problem over there so sparks of agi is one of those papers published by microsoft
research a big group of people there including the head of research was an author author to air
coordinates yeah did not notice that and it's um they took an intermediate version of gpt4 because
you know microsoft is in bed with open air and this is this is microsoft can't be above the fray
here they are they are part of this they've their funding was like ten billion dollars and then
gpt4 is now driving the bing chat thing and remember that they have a search engine and it's called bing
yeah right yeah and now it's and now it's got a super clippy embedded in it where it talks back
to you yeah right super clippy um and so they have as researchers at microsoft access to
a sort of an interim version of gpt4 and they use a whole bunch of these benchmarks on it
that were developed by different people trying to test natural language processing systems
generally without like really good construct validity that is what is this thing supposed to
be testing and how do we know it's actually testing that especially given a large language model as
the thing taking the test um and it's a 154 page thing where they they try gpt4 and all these things
and they say yeah um you know it looks like we have the first sparks of artificial general
intelligence here and that's that's what the paper is but it gets worse all right so um my
first comment on seeing this was remember when you used to go to um microsoft for stodgy but
basically functional software and the bookstore for science fiction well you know now we've got
this like maybe it's like a fan fiction to gpt4 that's been published as if it were a research
paper out of microsoft well so so what is so ludicrous about the idea that large language models
are a step on the way to agi or or the sparks of agi because you know as you point out in the
stochastic parrots paper it looks like agi to us right it passes if you want to loosely interpret
the turing test right can it fool so can it fool a human into thinking they're talking to another
human yes it can do that you could trick somebody using its output um and so for a lot of people
that's what they were taught to believe is a step on the way to agi that's like the the version you
learn in college um and so and it certainly seems that way to people um so what what is you know
what are the barriers that stop it from being that can you start with the first page the first the
first sentence yeah exactly paper um i have to get the paper up so that i can do that for you um
but the the first problem with with it being the first steps to um to agi is that agi is undefined
it's what tim meet was describing before as an unscoped technology so that's first steps to nowhere
number one number two we know what a language model is it's a word calculator as you say it right
so the fact that it seems to be giving us something coherent that's all us like gladly
interpreting it as if it were inherent a coherent and nothing on the side of the actual system um
so i was trying i'm like really trying hard to get you to talk about what they cite you know i'm
i'm getting to it okay because it is so it is so atrocious and i have to get my um the i love
how much fun you guys have with this roasting these papers is i i love it i love it when academics
get spicy and you guys are delivering the goods that's right you know we just read the footnotes
that's how we get spicy no but yeah it's always read the footnotes so so the um the pause letter
by the way cites the sparks of agi this is one of its academic sources for the danger that's coming
right but it's not peer reviewed and it's and it's fanfiction to a machine right all right so sentence
one intelligence is a multifaceted and elusive concept that has long challenged psychologists
philosophers and computer scientists sentence two is where it is to meet an attempt to capture its
essence was made in 1994 by a group of 52 psychologists who signed on to a broad definition
published in an editorial about the science of intelligence so i thought hmm let's go look at
what this definition is and where this came from that editorial was published in reaction to the
public outcry and discussion about the book called the bell curve do you remember this book
by charles mary uh-huh yeah i remember this book this is a bunch of psychologists yeah now please
go ahead no yeah a bunch of psychologists who are saying okay we've got to wait in here because this
discussion has gotten out of hand and i'm like okay okay what are they saying needs to be established
what they say in this terrible editorial is no no no iq is real these measures of it are good
they are not racist and yes there are group level differences in iq where jews and asians are the
smartest but we don't know exactly how much and then you've got the white people centered around 100
and then they say but the black people are centered around 85 and like this is flat out what is in
that editorial that this group of researchers at microsoft decided to use as the basis for their
definition of intelligence so they can say yes gbt4 is the first steps on the way to artificial
intelligence using this definition so it's it's totally foundational and it is shocking to me
that nobody in that group of authors thought maybe we shouldn't be pointing to race science and just
like flat out racism posted in the wall street journal as the basis of what we're doing and
like the more charitable interpretation here is none of them actually read what they were citing
like that would be better than reading it going yeah this seems okay but you know it's eugenics
all the way down i mean so yeah how do they know that chat gbt is or that gbt4 is a is intelligent
then are they checking to see if it's jewish or or what are they like if that's what they're citing
and they're and they're citing a paper that says that you know asian people and jews are more
intelligent then they that's a pretty easy thing to tell they could just test if the ai
circumcised i'm sorry i'm a comedian i apologize the chatbot circumcised yeah
but so i mean in addition to the to the shoddy research though like what is it about these language
models that fail so profoundly like one one thing to me is that i keep coming back to and i even wish
i had put more clearly in my own youtube video on this subject is that no ai that we have has any
kind of like understanding that other minds exist you know like and that's like a foundational part
of intelligence as you and i are talking to each other you know us three and we each have our own
minds and they're interacting and they're communicating um and when you are communicating
with chat gbt you are imputing a mind to it it's almost impossible to use it without imagining
that there's a mind in there even though there isn't but it is not imagining a mind talking
back to it it's just chopping up words and phrases and uh you know like a self-driving car what is the
foundational problem with self-driving cars they can't communicate they can't make an eye contact
with another person and go you know i had a whole interaction with a car the other day where i was
like oh this car needs to pass i was walking in the street where there's there's no sidewalk this
car needs to pass me i'm gonna go stand where in the parking part you know where where the other
cars are parking and then i look back at the car it hasn't gone past i look back and the lady points
and she goes no actually i wanted to park there and i said oh now i'm in your way i need to go
get in the street because you were trying to use the parking lot you're trying to use the the the
shoulder there's no way for an ai to have a communication with a person like that to know
that there's a person with intent who i need to deal with in order to decide what the machine should
do um and that that seems to me to be like a fundamental extremely fundamental part of intelligence
that no level of hey let's make the language model better is ever going to accommodate
because it's it's all it is is a thing that you put words in one end and more come out the other
end i i imagine you might have more examples though of like what would actually constitute
intelligence that these fail at or maybe not like octopus octopi she has a whole paper on
on our yes yeah so so i have a paper from 2020 co-authored with alexander coller which is uh
has the octopus thought experiment which is why i'm wearing my octopus earrings here
purchased from an artist on etsy by the way um and um the what we were talking about there is um
basically showing it doesn't matter how intelligent the thing is it's not going to learn to understand
if all it has access to is the form of the language so to make that point we put together
this thought experiment with a hyper intelligent deep-sea octopus um and credit for it being an
octopus goes to my co-author alexander i was thinking dolphin and he's like no octopuses are
inherently funnier and also um that makes the environment more distinct from where the humans
are so hyper intelligent deep-sea octopus two humans stranded on two separate desert islands
that happen to be connected by a telegraph cable the humans figure this out and they start doing
morse code to each other english as encoded in morse code the octopus remember hyper intelligent
we're not doubting its intelligence taps into that cable and starts listening to the patterns
of the dots on the dashes and then after a while it cuts the cable and it starts sending dots and
dashes back based on the patterns that it's seeing and it can get away with this because a lot of
the communication is you know just sort of keeping each other company and so if something comes back
that's good enough right but then we have this point where the um uh one of the people on the
island says oh no i'm being chased by a bear because the thought experiment right spherical
cows and all that bear shows up on the desert island all i have are these two sticks what
am i going to do um and of course the the octopus can't provide anything useful because the octopus
hasn't understood has no model of the people's world even though we've posited it to be hyper
intelligent right so flipping that around to what people are seeing in the language models
our primary evidence such as it is that these things are intelligent is their apparent ability
to understand and create coherent text that's the only evidence that they're intelligent yeah
but in fact we know because of the octopus's thought experiment that it can't be that it's just
coming up with plausible next words something that looks like an answer um and so there's no
evidence for intelligence there at all now i frequently get asked by people okay emily so
what's the test that would convince you that one of these things is actually intelligent
twitch my answer is that's not my job i'm not trying to build one of these things why why do
we want to build those things that's what i don't understand who who wants to do that and what what
is it supposed to accomplish right a g i and then what like no more climate change like clean water
what i don't i don't understand the connection at all that's the weird thing because they say
it's coming we got to get ready we we need to be ready for it when it comes but it's like wait why
you're making it people are making it if the if if anyone's going to make it it's the people who
are telling you to be worried about it if anyone's going to create an agi and why are they what's the
purpose and so actually this leads me to a good question to end on um because uh if it occurs to
me a lot that look i love new technology i think chat gpt is super cool i played with it a ton it
's like the kind of technology i love to play with i love to play with see what i can what kind of
output i can get if i mess around with it if they had advertised it as this is a word calculator
here's what it does you put words in one end and it'll make the most it'll make a plausible
sounding answer to any question you ask it or you can you know it'll imitate any uh you know
you give it input and it'll imitate a plausible output that would have been really cool and they
could have come up with a lot of very plausible narrow uses for that such as computer programming
or other things of that nature but instead the industry made a marketing decision to say that
this was a step on the road to a super intelligent ai that we have to protect ourselves from and so
the question i keep wrestling with is why what was the purpose of misleading people of about
what the technology can do what were they trying to accomplish do you have any idea i that's the
paper i just so that's that's what we've been i personally you just wrote a paper on this oh
my god you're the perfect person to ask that question that's why i that's why i've been trying
to figure out like why when did people decide like we have to do a gi because when we're thinking
about large language models for instance i was telling emily uh i didn't have a problem with
like birth that was a large language model and i didn't really have a problem with you know them
being used in components to do various things whatever it was when opening i came in the scene
and started talking about these things like they are this huge super you know intelligent thing and
we're going to do a gi and we're gonna you know it's going to be like uh either amazing or utopia
or apocalypse and we have to focus on the future stuff that's what really started driving this whole
thing and our paper with emil which is like the one i was talking about was um you know tracing back
these test real ideologies back to first wave eugenesis and all of that it basically talks about
how this whole movement came about because you know so when we were talking about the connection
to eugenics it definitely was not theoretical it was for instance you know the chair of the
british eugenics society talking about transhumanism right transcending humanity and the cause the people
who first they call it they say christened the term agi in 2007 in a whole book that they wrote
the way they described it was transhuman agi you know they think that the agi will help humanity
transcend being human and become post-human colonize space you know and and live in like
digital mind so when you see sam altman's writings if you just read his blogs right now
he says we're gonna have unlimited energy and intelligence before the decade is out
he writes in his blog post that we have something by human flourishing and the cosmos the universe
you know and so that that was why i had to write the paper because it was like why and the the the
final conclusion was precisely what you were saying we were saying that you know there's been
ai winters and such because and at some point people doing various things like natural language
processing computer vision etc didn't call themselves ai whatever right we're just like oh i'm doing um
nlp i don't want to be associated with these ai weirdos who always talk about building a god
because they over promise like that and then you know people see through it and then it crashes
and then it comes back right and so now it's back and there's this whole agi thing and i really would
love to play this game where i ask people is this from like 1962 or 2022 right like we're like oh my
god you will be astonished to see what we have built you know and in the next 20 years people
will not have to work right and so this is what's going on and the thing is um as emily was saying
earlier that it is super aligned with this this super you mentioned Scientology i want them to
build the church of test grill you know ideologies or something like that it because it really is
like that um they have very much like religious characteristics of like the end of times kind
of things you know apocalyptic and utopian but it also is super aligned with corporate interests
right you know if you build this like one model that can do anything for anyone and everybody
just pays you and you can steal everybody's data and say that you're actually like saving humanity
and creating a god oh and you shouldn't be regulated because otherwise china is going to
do the devil kind and you don't want that you want us to do the good kind like it is super in line
with like corporate interests so yeah that's the conclusion that we've come uh to with our
paper that we're hoping to publish at some point after a peer review so it is corporate
interest but it also these the people who are doing this actually have a definite ideology of
100 of eugenics of that they they come from they write this they write this stuff yeah yeah and
unfortunately the money's behind them and so it's becoming everybody's problem instead of this niche
little research community that could just go be weirdos on their own yeah well how do you suggest
that you know for the public who's watching this and being flooded with misinformation with hype
about ai uh how can they gird themselves against it and you know like what what's the best way to
resist it and to think a little bit more critically the next time they're confronted with it so i
think i think we the public have a big job to do here around pushing for appropriate legislation
not the ai pause but something that actually is governance of collection of data and you know
synthetic media that is built through consultation with the people who are bearing the front of this
right now so the people who are being exploited in developing the systems the people whose data is
being stolen the people who are getting misinformation said about them and all of this so so developing
regulation collectively um but also resisting misinformation and the non-information so with
chat gpt being set up it looks to me that it's sort of like the um the oil spill in the Gulf of
Mexico when that um that the oil rig was broken and there was just oil going and going and going
right and BP was you know eventually saying look at all the birds we cleaned right um chat gpt is
polluting our information ecosystem in the same way with non-information and i've had people say to
me well you know hasn't the horse left the barn on that like it's out there and my answer to that is
we used to have lead in gasoline and we discovered that was bad news so we made some regulation
and now we don't like we don't just have to live with this we can regulate um and one thing that
i would love to see regulation lies like my my wish list item on this is uh corporations are
accountable for the actual literal output of their text synthesis machines it's libeling someone you
get sued for libel it's putting out bad medical information people get hurt you're liable i would
love to see it set up that way i don't know if that's something that works policy wise but that's
an idea in terms of just on an individual level how do you resist the ai hype um i think the questions
to ask are okay what's the actual task here what's the evidence that this machine is well matched to
that task how is it evaluated can i see the data that it was evaluated on and who's really benefiting
by using this system and who gets hurt um when it's wrong who gets hurt when it's right but people
you know are sort of using it um as a shortcut and so on that's a wonderful answer and uh i think i
think that last question is who is benefiting and who is actually getting hurt right now
is one of the most important questions we can always ask ourselves about the world but especially
in this case uh i can't thank both of you enough for coming on and and you're i mean you're just the
the perfect people to to speak to this topic and it was an honor to have you where can people
follow your work and and what's the most important thing of yours you think they should read is it
is it the on the dangers of stochastic pair parents your favorite your famous paper um that's
that's worth a read we are in the process of creating an an audio paper of that we've recorded
it and i have to edit it together i'm sorry i haven't done that i have to release a recording of our
event stochastic parents day yeah um so probably the best way to find me is probably my faculty
webpage at the university of washington and from there you can see links to everything i do in the
media my papers and stuff like that i am for the moment still on twitter and also on mastodon and
that's available through my web page so if you search emily bender university of washington
you'll find me and tim need how about you yeah um so dare dare institute website which is
not that much information right now but in a couple of days revamped we'll see much more
information there with a lot of our work and other things i'm also on twitter so if you want
to hear me rant i'm there but also a mastodon i'm a huge fan of the fediverse these days because i
i don't know i'm not worried that some random billionaire is going to take over that anytime
soon i ventured into linkedin and i'm trying to stay there but it's kind of difficult but i'm there
too awesome to me and emily thank you so much for coming on it's it's been a true honor to have you
thank you for having us and for raising these issues
well thank you once again to emily bender and tim need gebru for coming on the show i hope you
loved that conversation as much as i did and if you did i hope you will consider supporting the
show on patreon head to patreon.com slash adam conover and join our wonderful community including
folks who back this show at the fifteen dollar a month level and i'd love to read a couple of
your names we got hydrochloric victor densmore francis amadar kill me ink christina mendez akash
thakar frank f cling robin dumblap jeffery mcconnell nissy pods brian tobone leslie kokeshawn
garrison raghav kaushik olwiz sonny and ashley malini dias thank you folks so much for your
support and if you want to join him head to patreon.com slash adam conover to get every
episode of the show ad free and a bunch of other goodies as well we even do a live book club would
love to see you there i want to thank our producer sam rodman our engineer kyle magraw and the fine
folks at fucking northwest for building me a wonderful custom gaming pc that i record every
episode of the show on you can find me online at adam conover dot net you can find my tour dates at
adam conover dot net slash tour dates come see me do stand up all across the country and of course
you can find me on social media at adam conover wherever you get your social media thank you so
much for listening and we'll see you next time on factually
