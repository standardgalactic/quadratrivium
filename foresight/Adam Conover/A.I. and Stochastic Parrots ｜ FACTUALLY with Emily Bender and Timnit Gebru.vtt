WEBVTT

00:00.000 --> 00:06.240
Hello, and welcome to Factually. I'm Adam Conover. Thank you so much for joining me once again as I

00:06.240 --> 00:10.880
talked to an incredible expert about all the amazing things that they know that I don't know and that

00:10.880 --> 00:14.480
you might not know. Both of our minds are going to get blown together and we're going to have

00:14.480 --> 00:19.280
so much fun doing it. I want to remind you that if you are watching this podcast on YouTube,

00:19.280 --> 00:22.960
go subscribe to the podcast and your favorite podcast player if you want to hear it every week.

00:22.960 --> 00:27.680
If you're listening on your podcast player, go check out the video episode on YouTube. Now this

00:27.680 --> 00:34.400
week we're talking about AI. Just a few weeks ago, I released a YouTube video called AI is BS in

00:34.400 --> 00:40.480
which I argued that AI has become a marketing term that tech companies are using to hype up

00:40.480 --> 00:46.320
software that cannot do what they claim it does and in which in many cases could be dangerous if

00:46.320 --> 00:51.760
they jam it into mainstream products that it is not ready for. In fact, these companies are hyping

00:51.760 --> 00:57.280
up AI to such an extent that they're trying to convince people that software like chat GPT is

00:57.280 --> 01:03.600
a step on the road to some kind of godlike artificial general intelligence when in reality

01:03.600 --> 01:08.720
what they actually made is a text generator that can write some pretty cool fanfic and help you

01:08.720 --> 01:12.880
program and you know if they had marketed that way in the first place they said hey we made a tool

01:12.880 --> 01:17.840
that'll output a recipe that tastes bad if you try to cook it. I mean that would be pretty neat. We

01:17.840 --> 01:23.760
could think of a lot of uses for a text generator like that but a step on the road to true artificial

01:23.840 --> 01:30.160
intelligence it is not and it is kind of fucked up to tell people that it is. Now that video got a

01:30.160 --> 01:36.320
somewhat let's say divisive response because a lot of people on the internet have drunk the AI

01:36.320 --> 01:42.960
hype Kool-Aid. These tech companies have succeeded in confusing the issue of AI so much that a lot

01:42.960 --> 01:48.160
of the time when we say AI most of us don't even know what we're referring to. We don't understand

01:48.160 --> 01:53.360
how the software works and we don't understand how that's connected to the science fiction fantasies

01:53.440 --> 01:59.120
that the companies are peddling us and because of that confusion a lot of weird shit is happening.

01:59.120 --> 02:04.960
For instance while I was in the process of editing my video 1600 major AI researchers

02:04.960 --> 02:10.160
as well as people like Elon Musk and Steve Wozniak came out and asked for a six month

02:10.160 --> 02:15.040
pause on AI research but they didn't ask for that pause because of you know AI giving out

02:15.040 --> 02:19.760
misinformation or the fact that it just recycles the copyrighted work of artists like myself and

02:19.760 --> 02:24.480
others no they asked for that pause because they were worried that it could create an AI

02:24.480 --> 02:30.800
superintelligence again the bullshit hype science fiction claim so a bunch of other AI researchers

02:30.800 --> 02:35.440
came out against this letter saying that we do not have a problem with AI because of the super

02:35.440 --> 02:40.880
intelligence thing we have a problem because you are exploiting the work of real people and making

02:40.880 --> 02:46.560
the world a worse place right now so I don't blame the people in my comments for being confused

02:46.560 --> 02:52.480
even AI researchers themselves do not agree entirely on what the problems are so for that

02:52.480 --> 02:56.960
reason we are going to spend a couple episodes of this podcast talking to some of those AI

02:56.960 --> 03:02.160
researchers about what the problems are and how we might go about fixing them and on the show

03:02.160 --> 03:08.160
today we have two incredible guests their names are Emily Bender who's a professor at the University

03:08.160 --> 03:13.360
of Washington and Timney Gebru who's the executive director of the distributed artificial intelligence

03:13.360 --> 03:17.680
research institute and you might recognize the name Timney Gebru because she is the researcher who

03:17.680 --> 03:25.040
was famously fired by google for raising AI ethics concerns in that famous paper I am so excited to

03:25.040 --> 03:30.480
talk to them because they are two of the sharpest minds on AI and how the problems with it are not

03:30.480 --> 03:34.960
what the tech companies have been telling you but before we get to that interview I want to remind

03:34.960 --> 03:40.480
you that if you want to support this show please head to patreon.com slash adam conover you can

03:40.560 --> 03:46.400
get every episode of this podcast ad free and get a bunch of other goodies and even more importantly

03:46.400 --> 03:51.680
please come see me on tour this summer I'm taking my brand new hour of stand-up to San Francisco,

03:51.680 --> 03:57.360
San Antonio, Tempe, Arizona, Batavia, Illinois just outside Chicago, Baltimore, Maryland and St.

03:57.360 --> 04:03.280
Louis, Missouri head to adamconover.net for tickets come see me I'd love to give you a hug in the

04:03.280 --> 04:08.400
meet and greet line after the show and now without further ado let's get to my interview with Emily

04:08.400 --> 04:16.640
Bender and Timney Gebru Timney and Emily thank you so much for being on the show super happy to be here

04:17.280 --> 04:22.240
thank you for having me it's an honor to have both of you considering you know I've read your work

04:22.240 --> 04:28.720
I've talked about it in my last youtube video all about AI you're some of the foremost researchers

04:28.720 --> 04:34.480
on the topic some of the foremost critics of how the tech industry has been employing AI

04:34.480 --> 04:38.640
so I'd love to hear from you first of all Emily we last talked it was might have been close to a

04:38.640 --> 04:46.640
year ago back when AI was very much an active research subject we were hearing a lot about it

04:46.640 --> 04:52.240
but it wasn't something that the average person was using in the year since AI has become radically

04:52.240 --> 04:57.200
mainstreamed a lot of these companies have just shoved it into consumer products without you

04:57.200 --> 05:02.480
know any concern for what the results are as two as two people who follow the field extremely

05:02.480 --> 05:08.480
closely what has your reaction been to the last you know six months or so of rapid development in

05:08.480 --> 05:20.320
the industry blank faces here it's been a lot of oh come on again more seriously yeah that's

05:20.320 --> 05:29.760
that's how I feel I mean I just I am dumbfounded by the number of people I thought were more reasonable

05:29.760 --> 05:38.160
than this kind of jumping on a bandwagon of what seems to be mania so I don't know that's how I feel

05:39.840 --> 05:44.240
what do you think the greatest you know potential harms of this are in your paper your very famous

05:44.240 --> 05:49.200
paper on the dangers of stochastic parents you talked about many dangers that you know these would

05:50.000 --> 05:54.560
reify you know discriminatory materials in the training data by repeating it out to people that

05:54.560 --> 06:00.320
people would take it to literally you know would take the pronouncements of a large language model

06:00.320 --> 06:06.560
as fact uh a lot of those have seemed to come directly true do you feel validated that your

06:06.560 --> 06:13.600
that your criticisms have come to pass no no because those were predictions those were warnings

06:14.640 --> 06:20.000
like don't do it you know we don't want to get there and then we got there and then some right

06:20.080 --> 06:28.400
like I just I definitely don't feel validated I feel upset I'm sad about it um yeah how do you

06:28.400 --> 06:33.600
feel Emily you know I think that some of the biggest problems that maybe I didn't understand

06:33.600 --> 06:39.600
because it's it's sort of half an economic problem um is the way in which people would say hey this

06:39.600 --> 06:44.720
looks like it could be a robo lawyer this looks like it could be a robo therapist and look at all

06:44.720 --> 06:48.960
those people who can't afford real lawyers and real therapists so let's give them this instead

06:49.040 --> 06:55.440
and like that jump from um you've identified a real problem in the world it's a problem that

06:55.440 --> 06:59.760
mental health resources are inaccessible and it's a problem that legal representation is inaccessible

07:00.400 --> 07:07.200
but then try to fill that hole with something that is just a joke and can directly cause harm

07:07.200 --> 07:12.000
when deployed in those cases I think even when we were writing the paper and saying you know it

07:12.000 --> 07:16.080
would be bad if this was set up in such a place where people might believe it or believe it knew

07:16.080 --> 07:20.320
what it was talking about I don't think I was in a position to predict that that's a direction that

07:20.320 --> 07:26.000
it would go in at the time we wrote the paper I was just you know seeing this whole mind is bigger

07:26.000 --> 07:30.800
than yours kind of race and just being very confused why is this the thing that anybody

07:30.800 --> 07:36.560
everybody just wants to be the biggest one and now and now you have not just that text to text

07:37.280 --> 07:43.440
models but text to image text to video video or whatever you know and so I didn't I didn't imagine

07:43.440 --> 07:49.520
that in such a short time that kind of explosion of synthetic media into the world would happen

07:50.080 --> 07:57.280
and I also didn't think about I would say what the content moderation demands and issues would be

07:57.280 --> 08:04.320
with that much like explosion of synthetic media you know like the um the what is the fix

08:04.320 --> 08:09.920
Clark's world shutting down submissions because they got a little bit devious yeah stuff like that

08:09.920 --> 08:16.400
is something I didn't predict Emily you talked about just now the you know the sort of thing

08:16.400 --> 08:21.040
happened that happens a lot of technology once it's released people come up with new uses for it

08:21.040 --> 08:26.560
that nobody predicted and one of the uses that people have started you know uh do you talk about

08:26.560 --> 08:34.240
robo lawyers uh I've seen people say that they're using uh chatbots as therapists um or as relationship

08:34.240 --> 08:39.760
surrogates and what are the dangers of of those types of uses because I'm certainly seeing those

08:39.760 --> 08:45.600
promoted all over the there's a lot of folks saying hey if you don't have access to xyz and

08:45.600 --> 08:50.720
ai can do that for you in all sorts of fields and and you said that these are a joke what makes them

08:50.720 --> 08:57.040
a joke for that purpose so they're a joke because with they're all form and no content right so

08:57.040 --> 09:02.640
what these systems are really good at is mimicking the form or the style of something so it absolutely

09:02.640 --> 09:06.880
can write something that looks like a legal contract for you but if your purpose in drafting

09:06.880 --> 09:11.360
up a legal contract is anything other than intimidating the other party with legalese

09:11.360 --> 09:16.480
then the specific content and the way that it maps into your situation really matters yeah and it

09:16.480 --> 09:22.720
might be that there's some sort of template type situations where it's like okay yeah this is a

09:22.720 --> 09:30.720
contract for you know the rights to use a piece of music um and um I want this right assigned and

09:30.800 --> 09:34.560
that went not and it's going to be paid for this much and here's like you could answer a few questions

09:34.560 --> 09:38.960
and get something out from a template that would work reasonably well but that's not what they're

09:38.960 --> 09:44.400
doing right they're saying what's a plausible next word what's a plausible next word given this context

09:44.400 --> 09:48.960
and you know who knows where that's going to be um so for the the legal case you know you're asking

09:48.960 --> 09:52.960
for that because you are not a lawyer and you can't afford a lawyer you're not going to be in a position

09:52.960 --> 10:00.080
to tell if it's good or not but it'll look impressive right right it it sort of will can it

10:00.160 --> 10:06.160
will create a convincing imitation of a piece of text that will most readily convince someone

10:06.160 --> 10:12.240
who knows nothing about the field like a lot of you know I work in television writing and there's

10:12.240 --> 10:19.520
a lot of talk in you know oh can can studios use chat gpt to write scripts and when I use one of

10:19.520 --> 10:26.160
these services to uh write you know to output text I'm like yes this superficially looks like a script

10:26.160 --> 10:31.760
but it's missing so many of the things that you would need to film a script and someone might say

10:31.760 --> 10:37.760
well what if the technology gets better and it's not a matter of aping something even more correctly

10:37.760 --> 10:43.440
it's a matter of to successfully write a piece of screenwriting you need information about the

10:43.440 --> 10:50.160
rest of the world that a no algorithm or ai program could ever have you need to understand

10:50.160 --> 10:55.040
uh you know what is physically possible to produce you need to under you need to talk to

10:55.040 --> 11:00.400
a department full of people who say uh a very good example I use of this is I didn't realize until

11:00.400 --> 11:05.120
I started writing television that you can never have someone jump into a pool on television if you

11:05.120 --> 11:10.080
watch tv and someone jumps into a pool it'll always happen off camera and I had because I had a scene

11:10.080 --> 11:14.640
where I someone jumped into a pool my line producer told me they you need to remove that and I said

11:14.640 --> 11:19.200
why it seems not that hard they said because we need to film every take five times so that means

11:19.200 --> 11:23.280
we need five pairs of wardrobe because we need to film them one after another and we need to dry

11:23.280 --> 11:27.840
the person off and do their makeup and it's going to take all day and so as a result people never

11:27.840 --> 11:33.200
get wet on television or when they do it's very expensive okay interesting and you'd have no way

11:33.200 --> 11:38.400
of knowing that uh without real life experience and even if an ai could eventually figure that out

11:38.400 --> 11:41.840
there's also a million other things like that that are specific to the particular production

11:41.840 --> 11:45.680
hey it's going to be cloudy on wednesday we need you to rewrite the scene you know there's there's

11:45.680 --> 11:50.400
so many details that are that are fundamentally about humans communicating with each other

11:50.400 --> 11:56.560
and that's the same thing with with a lawyer doesn't just output text a lawyer talks to

11:56.560 --> 12:00.640
like knows what the other side might do in response knows how aggressive they'd be if you're

12:00.640 --> 12:05.920
trying to sue laws are yeah exactly if you're trying to evict a tenant they have they're going

12:05.920 --> 12:09.920
to have a much different response than if you're trying to sue the church of Scientology right

12:09.920 --> 12:17.440
who are very aggressive like and knowing what the laws are too so to me this is it's very obvious

12:17.440 --> 12:23.280
when i actually look at how they're used but it's it is it a problem with the technology or is it a

12:23.280 --> 12:29.280
problem with humans not understanding how our own society works to to not realize that these that

12:29.280 --> 12:35.440
these tools are going to be effective the hype too the hype yeah so it's a problem with the task

12:35.440 --> 12:41.200
technology fit so what is it that we need and how does the technology fit into it and and timmy

12:41.200 --> 12:45.760
and i want to bring up your wonderful line about how these things are unscoped technologies

12:45.840 --> 12:49.680
and then maybe you could elaborate on that a little bit yeah i mean i was going to bring up

12:49.680 --> 12:55.040
all your work on hype but which i think um and i really add them i mean it just like when i'm

12:55.040 --> 12:59.440
talking to you i'm like yeah we live in the same planet and you know we're having the same

12:59.440 --> 13:05.760
conversation in the same language i i am not that's not the language that we're speaking with the

13:05.760 --> 13:11.440
other researchers in ai or machine learning or whatever it is i am i'm so confused what's going

13:11.440 --> 13:19.520
on but because you know um scoping systems is a very basic engineering concept right when

13:19.520 --> 13:26.640
you're building something you want to know what you're building it for and then see if what you're

13:26.640 --> 13:31.280
building it for is actually being fulfilled whereas in this case the way they're advertising

13:31.280 --> 13:36.560
their systems is that they're building it to accomplish anything for everybody anywhere

13:36.560 --> 13:41.600
write code speak whatever language you know write scripts movie scripts protein folding

13:41.600 --> 13:45.840
whatever it is and that's a fundamentally unscoped system that i don't even know how

13:45.840 --> 13:52.560
we can make sure um can be safe or work um to add to the notion of being an unscoped system

13:52.560 --> 13:58.960
right which to me is a basic um engineering concept when you're trying to build something

13:58.960 --> 14:04.960
you ask what am i building uh to accomplish what are the tasks that i want to accomplish

14:05.040 --> 14:10.480
under which scenarios under which conditions and in this case when you see the kinds of things that

14:10.480 --> 14:17.520
they are um advertising um all of these companies metta talked about this um uh large language model

14:17.520 --> 14:24.000
based system they had called galacticon they said oh it's gonna write code just protein folding stuff

14:24.000 --> 14:30.320
and write scientific papers and more you know and with opening eyes chat gps like write movies

14:30.320 --> 14:35.200
replace artists do this do that and more you know and so already you just haven't

14:35.200 --> 14:39.920
have built a system that we don't even know what it's supposed to use for to be used for and how

14:39.920 --> 14:45.840
do we even test whether it is actually accomplishing its um task the task that it's supposed to be

14:45.840 --> 14:53.040
built for and one one problem here is that uh open ai is not at all open about how these things

14:53.040 --> 14:58.960
are trained and so not only is it not tested in specific contexts where you can say okay

14:58.960 --> 15:04.240
here here is the range of um here are the safety parameters of the system here's how well it has

15:04.240 --> 15:07.680
been tested to work in these contexts we don't have that information we also don't know what

15:07.680 --> 15:12.560
is training data or training regimen was and according to open ai this is somehow for safety

15:12.560 --> 15:16.960
which makes no sense at all because one of the very first things that was worked out about

15:17.760 --> 15:24.000
responsible development of these kinds of systems is provide documentation of the underlying dataset

15:24.000 --> 15:29.520
and of the parameters of sort of safe use of the model and that's kind of the first place that

15:29.520 --> 15:35.120
tim meet and i got to know each other independently right we were both working on this um separately

15:35.840 --> 15:41.440
yeah and then and then we connected through that sort of related work and that was you know really

15:41.440 --> 15:45.200
really fortunate it was 2017 i think there was just something in the air where a whole bunch of

15:45.200 --> 15:49.440
groups said we got to document these things so that we could figure out how we could use them

15:49.440 --> 15:54.560
and open ai while claiming to be doing this for safety is flat out refusing to do that

15:55.120 --> 15:59.840
and what's the danger of that if they are refusing to release the or make the model transparent

16:00.400 --> 16:05.520
so you can't make any decisions about whether it would be good to use the model or not if it's

16:05.520 --> 16:10.320
not transparent like let's say i want to use it for writing computer code well i can try it a few

16:10.320 --> 16:15.760
times and see if it seems to work well and then maybe get some confidence but i don't know what

16:15.760 --> 16:20.240
its training data looks like and for programming languages from what i've heard again they're not

16:20.240 --> 16:24.960
open about this but part of the training data is literally sort of english descriptions together

16:24.960 --> 16:30.720
with executable codes there's a lot of paired stuff in there that helps it do well a lot of the time

16:30.720 --> 16:34.720
the other thing about programming languages is that they are specifically designed to be unambiguous

16:35.520 --> 16:40.720
which is in stark contrast to natural languages where ambiguity is sort of a fundamental design

16:40.720 --> 16:48.080
feature everything is ambiguous and so the fact that it does well with this more constrained

16:48.080 --> 16:52.400
universe of programming languages kind of makes sense but again we can't really know because we

16:52.400 --> 16:56.560
don't know how it's trained but imagine like you're happy with this performance in helping you

16:56.560 --> 17:00.880
generate code and you've even got some like computer security buffs on your team and they

17:00.880 --> 17:05.520
look at that say yeah i don't see anything frightening coming out here but they keep changing

17:05.520 --> 17:10.080
the system and then all of a sudden it's maybe putting that into your code but there's no information

17:10.080 --> 17:14.320
about what version you're using you can't say no i want to keep using the version you know from

17:14.320 --> 17:20.800
december of 2022 because that's gone all you have is the open it uh the open ai um api is what i was

17:20.800 --> 17:25.520
trying to say that um allows you to connect with whatever they've put up for you to connect with

17:26.320 --> 17:32.800
and a lot of tools are being built on their api right now there's uh if you open the app store

17:32.800 --> 17:38.800
any kind of app store you'll find countless tools that are ai xyz you know ai help you write code

17:38.800 --> 17:45.600
ai help you write a movie script ai therapist and they're really just hooking into open ai's model

17:45.600 --> 17:51.840
and paying them a couple pennies per however many requests uh and people are now starting to use

17:51.840 --> 17:58.720
those tools to do real things without knowing what is where the the output is coming from or

17:58.720 --> 18:03.120
what the model is it does kind of remind me a little bit you're talking about models like i've

18:03.120 --> 18:07.760
talked to plenty of climatologists on the show and like climb you know climate models are a huge

18:07.760 --> 18:12.800
part of our understanding of how the climate works but we also know how those models work and we

18:12.800 --> 18:16.800
can compare them and we have a lot of information about them so that we know when they predict

18:16.800 --> 18:23.840
something you can go back to the what the source was but in this case it's both by design but also

18:23.840 --> 18:28.080
by corporate structure a black box because they're not telling us anything about it i was just

18:28.080 --> 18:32.640
thinking about something even more basic like how do we know we're not still they're not stealing

18:32.640 --> 18:39.600
people's work um to profit off of it so you know there were all these um lawsuits by artists to

18:39.600 --> 18:46.960
devian r i stability ai and mid journey right but not open ai not dally because we don't know

18:47.600 --> 18:52.320
what training data they were used so we don't know if they were copyright violations or not if they

18:52.320 --> 18:58.240
constantly you know compensated anybody versus not but they can do that right they there's nothing

18:58.240 --> 19:04.320
that is like preventing them from doing that right now yeah and it makes me wonder so look i can go to

19:04.320 --> 19:08.720
into chat gpt i talked about this in my youtube video i can go into chat gpt and say write an

19:08.720 --> 19:15.280
episode of adam ruins everything about xyz and it'll output something that looks like a pros

19:15.280 --> 19:20.800
version of adam ruins everything adam walks into the room and says blah blah blah about dogs and

19:20.800 --> 19:26.080
i'm like where is this coming from because i don't believe it has access to my shooting scripts

19:26.080 --> 19:30.080
because those are not public anywhere so i'm trying to figure out where is it getting the

19:30.080 --> 19:36.000
information from and again this is based on my own copyrighted work that i spent a lot of time

19:36.000 --> 19:40.560
putting together is the character i created and i would think that you know i would if it's being

19:40.560 --> 19:45.200
used for profit i would like to be paid for it in some respect um i think that's a pretty fundamental

19:45.200 --> 19:50.240
feature of our uh how capitalism is currently arranged and i would like it to follow those rules

19:50.240 --> 19:54.400
but it's difficult for me to tell i'm like it could just be getting it all from like fan fiction

19:54.400 --> 19:58.800
like i feel like it's really scraped like our archive archive of our own and all these big

19:58.800 --> 20:03.840
fan fiction sites um but it's really it's really really difficult to tell let's spend a second

20:03.840 --> 20:09.920
though and talk about open ai as an organization and the sort of ideology behind it because this is

20:09.920 --> 20:15.120
you know the organization that is most in the news pushing uh ai forward and it was incorporated

20:15.120 --> 20:20.080
originally as a nonprofit right and made a lot of noise about how the point is to make

20:20.080 --> 20:25.280
sure they're going to do it responsibly it's research it's not about profit but it has recently

20:25.280 --> 20:30.560
i believe changed its incorporation status to be a for-profit company completely changed its tune

20:31.120 --> 20:36.640
and they now say it wouldn't be safe if we were to open it up to all of you but meanwhile you've

20:36.640 --> 20:41.200
got the founder sam altman going on all sorts of podcasts talking to the news about how he has to

20:41.200 --> 20:45.680
do all this because ai is really really dangerous but then the dangers they're always talking about

20:45.760 --> 20:50.960
are always the science fiction kind where how the robot takes over the spaceship or you know from

20:50.960 --> 20:56.240
isek azimov kind of style of science fiction where a super powerful intelligence you know takes over

20:56.240 --> 21:00.160
the universe kind of thing they're never talking about the harms that we're talking about that

21:00.160 --> 21:04.160
people might use it to write a legal document it shouldn't write or that it might rip off somebody's

21:04.160 --> 21:10.160
copyrighted work or anything like that so what is your view of you know this this organization and

21:10.160 --> 21:15.680
it's uh supposed altruism is this just a faint to sort of trick us all into thinking that they

21:15.680 --> 21:21.680
have our best interests at heart or is as a corruption happened or what so this is the

21:21.680 --> 21:26.960
test grill question and i'm gonna let tim need to find test grill but just my take on this very

21:26.960 --> 21:33.440
quickly is i think they believe they are being altruistic and working in the best interests of

21:33.440 --> 21:42.240
people but their view of who counts as a person is very narrow and sort of leaves out of you all

21:42.240 --> 21:47.440
of the people who are being harmed now or just sees those harms as inconsequential compared to

21:47.440 --> 21:53.200
what they're worrying about which is in this science fiction universe it's hard to say the

21:53.200 --> 21:56.960
phrase science fiction fantasy because to me those are two genres of wonderful speculative

21:57.040 --> 22:06.400
fiction and you don't want to bucket this into that yeah you're just so i you know i can say so

22:06.400 --> 22:12.320
much about them i've been on them for a long time so in 2015 when they were announced i wrote a open

22:12.320 --> 22:18.480
letter that i didn't end up sending to anybody i just kept it to myself because i was a phd

22:18.480 --> 22:22.880
student back then and people were like uh people will know that it's you because i was so angry

22:23.440 --> 22:29.280
by the tone so i don't think it's a corruption or they've changed their tone or whatever to me

22:29.280 --> 22:37.440
they've like stayed exactly the same um and initially they um said they exactly they talked

22:37.440 --> 22:42.320
to they wrote you know they talked about it as if they were gonna save humanity peter teal and

22:42.320 --> 22:48.320
illa musk always as usual just on the ball to save humanity of course that's always what they've

22:48.400 --> 22:54.400
been doing in the world and um and all the whole media was talking about it like oh this

22:54.400 --> 23:00.560
nonprofit is starting they're gonna save humanity from ai because back then what happened is that

23:00.560 --> 23:06.560
they had invested in deep mind that they also wanted to create you know a gi artificial general

23:06.560 --> 23:12.160
intelligence which is a system that none of us know what it even is supposed to do this is the

23:12.160 --> 23:19.840
acronym for a for a super intelligent uh uh sounds like a god sounds like a god to me um and so

23:19.840 --> 23:25.680
they were all very much trying to develop this thing which i don't even know what it is and um

23:25.680 --> 23:31.760
deep you know invested in deep mind deep mind got bought by google 2015 the future of life institute

23:31.760 --> 23:38.640
had a similar letter asked to the one that we see the pause letter um and then they you know found

23:38.720 --> 23:43.680
open ai put bill you know hundreds of millions of dollars into it because they say they're gonna

23:43.680 --> 23:49.360
save humanity and all of that and create this agi thing fast forward right they realize they need

23:49.360 --> 23:55.840
a lot more money uh they're now essentially bought by um microsoft and then now they have a competition

23:55.840 --> 24:01.840
so they need to be closed and all of that so to me really it wasn't like a pivot or anything i

24:01.840 --> 24:07.120
never believed that they were gonna you know save humanity or anything like that and in terms of

24:07.120 --> 24:17.280
emily's um q about the test grill bundle um so you know i i have been really so irritated by the

24:17.280 --> 24:21.360
whole crew because i've been around them for a long time i went to school with some of them

24:21.360 --> 24:29.120
been around this you know agi community for a while so recently i teamed up with um a collaborator

24:29.120 --> 24:35.760
of ours um whose name is emil torrez who used to be a long termist and so a long termism is this

24:35.840 --> 24:41.520
weird you know the future of life institute behind the pause letter is a long termist institute and

24:41.520 --> 24:50.080
so they literally think that our um job as humans is to maximize the number of future humans who

24:50.080 --> 24:55.440
colonize space and digitally upload their minds and like live in the matrix kind of thing right

24:55.440 --> 25:02.480
this is a real thing like it's not an exaggeration that's what they want yeah i've read i've read a

25:02.480 --> 25:07.360
lot of that philosophy that you know the idea that we need to be thinking about how do we maximize

25:07.360 --> 25:13.920
the future uh happiness and well-being of humans 10 000 years from now if you could why save one

25:13.920 --> 25:18.960
life today when you could save 10 million lives uh 10 000 years from now now i'd say how the

25:18.960 --> 25:22.880
fuck do you know that what you're gonna do is gonna have any effect on people that far in the

25:22.880 --> 25:26.960
future it's the height of hubris to think that you can project that far into the future at all

25:26.960 --> 25:32.160
well they give you some random numbers they pull some number numbers out of their asses like oh

25:32.160 --> 25:37.440
my god we didn't know sam bankman freed was gonna be doing this but we will know what 10 000 years

25:37.440 --> 25:42.640
or now point zero zero one probability that you know what i mean it's absolutely ridiculous but

25:42.640 --> 25:49.120
anyhow so the test grill bundle is a bunch of ideologies that are all sort of descendants of

25:49.120 --> 25:54.800
the first wave eugenics um eugenics movement and you know when you hear this word about human

25:54.800 --> 26:01.360
flourishing maximizing our potential through both positive and negative eugenics positive would be

26:01.440 --> 26:05.040
the ones who are desirable you want them to breed you want them to you know

26:05.680 --> 26:10.640
multiply right and the ones who are negative the ones who are undesirable you want to kind of get

26:10.640 --> 26:16.960
rid of them because they don't help you with this human flourishing thing so the transhumanists

26:16.960 --> 26:22.160
you know were very much um that that ideology was very much developed by 20th century eugenics

26:22.800 --> 26:29.200
um and nick bostrom who is also a long-termist you know it's also very famous very prominent

26:29.200 --> 26:36.240
transhumanists right and so um we traced how these ideologies transhumanism extropianism the

26:36.240 --> 26:42.240
singularity people who say singularity is coming because of ai the cosmos who are actually the

26:42.240 --> 26:48.400
people who wrote the first book on age artificial general intelligence in 2007 the effective altruism

26:48.400 --> 26:54.240
the long-termists and how they're all in this circle kind of learning from each other networking

26:54.240 --> 27:00.560
with each other lots of money going into them and they're all sort of either selling agi utopia

27:00.560 --> 27:07.040
or agi apocalypse right if we do it right it's going to bring us utopia it is out human flourishing

27:07.040 --> 27:11.920
we need to do it if we do it wrong we're going to have an apocalypse because it's going to take

27:11.920 --> 27:18.240
over the world or china is going to do the devil kind of agi and you need to let us do this utopian

27:18.320 --> 27:25.120
kind because we're vanguards of humanity so it's obviously a very kind of convenient

27:25.680 --> 27:28.720
ideology for the billionaires because you know they're saying give us all the money

27:29.280 --> 27:34.080
we'll do the utopian kind um and but we're super worried about it because it might be

27:34.080 --> 27:39.360
super powerful but we're careful because you can trust us right and so sam altman is kind of is

27:39.360 --> 27:44.000
doing that thing right and to me he's in the same sort of camp as the future of life people

27:44.800 --> 27:50.400
because that's the same thing they're selling yeah i've the connection to eugenics is not

27:50.400 --> 27:54.640
theoretical i've i've seen it myself if you look at nick bostrom's writing and the writing of a lot

27:54.640 --> 28:01.120
of folks who write extensively about ai or agi the future you know super ai that could control the

28:01.120 --> 28:08.480
world um a lot they also write overtly about eugenics they have charts and tables about if we

28:08.480 --> 28:13.040
what if we started a human breeding program and only allowed people in the top percent

28:13.760 --> 28:17.840
of intelligence to breed and then they would have super babies and the babies would be super

28:17.840 --> 28:23.760
smart and it's like this was tried in the 40s in a country in europe you know this this is um

28:23.760 --> 28:28.400
this is very these are very old ideas by the way you can just look at a the interview i did a

28:28.400 --> 28:32.800
couple weeks ago about intelligence to to learn about whether intelligence is actually heritable

28:32.800 --> 28:39.440
in the in that way it's not um but so these the proximity of these ideas to each other is not

28:39.440 --> 28:45.360
theoretical these are you know the same folks promoting neo eugenics and promoting um ai

28:45.360 --> 28:50.960
catastrophism uh i want to refer though to this pause letter that you mentioned a couple times

28:50.960 --> 28:57.440
so that folks know what it is um a couple weeks ago actually as i was editing my ai video um a

28:57.440 --> 29:02.480
whole bunch of ai researchers from many many different organizations signed a letter suggesting

29:02.480 --> 29:08.560
a six month pause on ai after the release of gpt4 and they said well this is very dangerous

29:08.560 --> 29:16.000
we need to evaluate it um and etc etc and some folks who i you know i i have read and and enjoyed

29:16.000 --> 29:21.360
as ai researchers are signed to the letter um and it sounds on the face of it that that might rhyme

29:21.360 --> 29:26.320
with some of what you folks are saying um but you took objection to the letter and so i'd love

29:26.320 --> 29:31.840
a little bit of explanation from you about about exactly what your uh issue with that is like what

29:31.840 --> 29:36.400
what did that letter get wrong do we do you feel we need to pause or do we need to pause for a

29:36.400 --> 29:44.320
different reason or what so i think pause is unrealistic i think six months is unrealistic

29:44.320 --> 29:49.360
i think the letter makes it sound like these researchers are just now noticing that this

29:49.360 --> 29:55.680
might be harmful despite you know years and years and years of work of people saying hey there's

29:55.680 --> 30:02.560
harms here um and the letter itself is basically saying oh no we've built something too powerful

30:02.560 --> 30:08.480
better be careful so it's it's what livencel calls pretty good yeah we're too good gotta stop

30:08.480 --> 30:12.640
so it's it's basically helping to sell the technology um i have to say that i found out

30:12.640 --> 30:17.280
about the letter a little bit before it dropped because there was a um a journalist who contacted

30:17.280 --> 30:21.680
me asking if i was going to sign it and would i comment and i'm like haven't seen it not going

30:21.680 --> 30:27.120
to comment on what i haven't seen and then like i think later that day it came out and i was busy

30:27.120 --> 30:31.440
and then finally evening i sat down to read it and i thought oh god i have to i have to react to this

30:31.440 --> 30:39.600
so i put out a tweet thread um and then you know media you know craziness about it and so i said

30:39.600 --> 30:44.800
to tim neat and the other two listed authors of the stochastic parents paper let's put together

30:44.800 --> 30:49.600
a statement coming from us so that we can point the media at that for one thing but also to have

30:49.600 --> 30:54.400
sort of a joint statement here and and tim neat sort of took everybody's remarks including my

30:54.400 --> 30:58.960
relatively snarky twitter thread and like pulled together our first draft that we then worked on

30:58.960 --> 31:05.120
and where we start with that is with the observation that they cite us in their first footnote number

31:05.120 --> 31:10.720
one yeah they say your stochastic parents paper number one and number two is nick boston yeah

31:10.720 --> 31:15.760
wow oh wow okay they got they went all the way from alpha to omega there huh they're they're

31:15.760 --> 31:21.280
inciting everybody but they didn't ask you to sign the paper that's interesting oh they would know i

31:21.280 --> 31:27.760
would never yeah okay so their sentences ai systems with human competitive intelligence

31:27.760 --> 31:34.080
can pose profound risks to society and humanity as shown by extensive research footnote one

31:34.080 --> 31:39.280
and acknowledged by top ai labs footnote two and that footnote one has us embostrum and some other

31:39.280 --> 31:46.640
people but the stochastic parents paper was not a paper about ai systems with human competitive

31:46.640 --> 31:51.920
intelligence it was a paper about large language models which are not ai systems with human

31:52.000 --> 31:58.640
competitive intelligence as we note so many times in the paper like that was the thing we said yeah

31:59.680 --> 32:04.160
so like right off the bat it was just infuriating and there's like one or two things in here that

32:04.160 --> 32:12.080
i think do rhyme as you say so you know we need regulation and that regulation should involve

32:12.080 --> 32:17.840
things like watermarking so that we can tell when we're encountering synthetic media and

32:17.840 --> 32:22.960
um you know liability for ai caused harm that sounds good that liability should sit with the

32:22.960 --> 32:27.760
companies that are creating and deploying the ai they don't say that um but then there's a bunch

32:27.760 --> 32:32.240
of really weird stuff in here ai research and development should be focused on making today's

32:32.240 --> 32:39.920
powerful state-of-the-art systems more accurate safe interpretable transparent robust okay i'm

32:39.920 --> 32:47.680
all right with all those aligned is a keyword for this weird one trustworthy yeah but then the last

32:47.680 --> 32:57.200
one the last one is loyal a loyal loyal to who oh wait no no sorry they started doing Boy Scouts

32:57.200 --> 33:04.080
a scout is trustworthy brave reverent kind obedient or whatever i quit Boy Scouts when i was like 10

33:04.080 --> 33:07.760
but there's like a whole list of things in one of them's reverence so i'm surprised they didn't

33:07.760 --> 33:13.920
include reverence shouldn't it go to church yeah but here's the thing and ai i mean so

33:13.920 --> 33:18.400
there's the first question that timnit raises of okay loyal to whom whose interest is it serving

33:18.400 --> 33:23.840
but also these are not the kinds of things that can be loyal to be loyal is to experience

33:23.840 --> 33:29.840
certain feelings to have certain commitments and large language models are just text synthesis

33:29.840 --> 33:37.200
machines yeah so they predict what comes next or they're i hear a really great description i heard

33:37.200 --> 33:42.240
of them is to think of them as word calculators they they do a good job of you give it a bunch of

33:42.240 --> 33:46.480
words and it can turn them into other words that are derived from the first words and that can be

33:46.480 --> 33:50.400
a useful thing to do sometimes particularly if you're a computer programmer or someone else who

33:50.400 --> 33:57.600
like is manipulating text on that sort of level but that's not a it's it's not a thing that has

33:57.600 --> 34:05.200
an ethical drive such as loyalty yeah yeah exactly so so this this letter you know got a lot of

34:05.200 --> 34:11.200
attention partially because of who signed it and you know we've we've had so we we pushed back pretty

34:11.200 --> 34:17.920
quickly and then we were getting reactions like oh you're squandering the opportunity this is gary

34:17.920 --> 34:22.960
marcus complaining about us going out for blood coming out what is what did he say coming out

34:22.960 --> 34:28.160
they went for blood we went for blood or something like that or it's just like okay and sure and

34:28.880 --> 34:33.440
basically it's like they supposedly created an opportunity for regulation that would maybe get

34:33.440 --> 34:38.080
the six month pause whatever that means it's it's all completely unfounded right pause on systems

34:38.080 --> 34:43.920
more powerful than gpt4 well we don't have the specs on gpt4 so that's a unmeasurable undefined

34:43.920 --> 34:48.160
thing anyway um and as someone was pointing out and i'm sorry i don't have the source for this

34:48.160 --> 34:51.920
a lot of the work in creating new systems is actually in the data preparation

34:52.720 --> 34:57.920
and gathering these enormous amounts of data and a six months pause on training the systems

34:57.920 --> 35:02.240
wouldn't prevent anybody from going and collecting more data we need to prevent that in other ways

35:02.240 --> 35:07.600
to prevent data theft but that's a separate question right um and then you get people out

35:07.600 --> 35:14.240
there saying well why can't the so-called ai safety and ai ethics people get along so the ai

35:14.240 --> 35:18.800
safety people are the long-termists who want to prevent the agi from taking over the world

35:18.800 --> 35:22.560
and ai ethics is sometimes used to refer to the people who are concerned with the problems in

35:22.560 --> 35:28.800
the here and now in the ways that yeah they basically created the term ai ethic i'm ai safety

35:28.800 --> 35:33.520
to separate their themselves from us is how i feel about it because like we have the same

35:33.520 --> 35:38.720
technical expertise we have other expertise also but like it doesn't mean that you know

35:38.720 --> 35:45.280
so i feel like they they they named that that field or whatever it is to explicitly separate

35:45.280 --> 35:49.920
themselves from kind of our crew right yeah yeah so by answer to why can't we get along it's like

35:49.920 --> 35:55.680
well if why can't we find common cause if the ai safety people wanted to find common cause with

35:55.680 --> 36:00.800
those of us working in ethics they would cite us they would go to the you know to to tim needs work

36:00.800 --> 36:06.000
they would go to the work of syphia nobel and ruha benjamin and kathie o'neal two past guests on

36:06.000 --> 36:12.640
the show by the way just want to ding ding ding ding excellent um and you know build on that and

36:12.640 --> 36:17.600
lend some of their money and resources to making that happen but of course they don't want to

36:17.600 --> 36:22.480
because they're aligned with corporate interests and to really push back and to really reduce the

36:22.560 --> 36:28.960
harms here we need regulation that reigns in the corporations like why would ilan musk sign like

36:28.960 --> 36:35.600
everybody has to ask why does ilan musk an advisor uh end funder a future of life institute someone

36:35.600 --> 36:40.480
who pumped hundreds of millions of dollars into open ai and deep mind or whatever and whatever

36:40.480 --> 36:45.840
why is he so interested in like caution and whatever as long as it doesn't touch him sure

36:45.840 --> 36:51.120
you know if we're talking about regulating tesla or looking at the racial the largest racial

36:51.120 --> 36:56.480
discrimination um lost in history in california that's not what he wants us to talk about right

36:56.480 --> 37:01.120
like he doesn't want us to talk about any of those things and whatever he's doing with twitter

37:01.120 --> 37:06.480
we have to think about oh my god like this super powerful science fictiony thing that's going on

37:07.280 --> 37:12.640
and it's just so disappointing to see that number of people who went along with it i think for us

37:12.640 --> 37:20.720
we wanted to make sure we wanted to make it clear that we are not aligned with this vision

37:20.720 --> 37:27.920
of ai safety they hold eugenics roots um there's emily has a thing she always says always read

37:27.920 --> 37:33.840
the food notes we write the food notes and they have a food note that says you know if we don't

37:33.840 --> 37:42.480
do x y and z um we ai systems might have might be potentially catastrophic like other potentially

37:42.480 --> 37:47.200
catastrophic things like eugenics and we wanted to be like eugenics is not just potentially

37:47.200 --> 37:51.680
catastrophic it has been catastrophic you know what i mean so we just want to make sure that

37:51.680 --> 37:57.760
they can't they should not be able to launder um people's reputations to make themselves mainstream

37:57.760 --> 38:04.480
and appear reasonable i also think that there there's a huge number of unexamined assumptions

38:04.480 --> 38:09.840
in that letter that they are using the letter to promote to the public that are essentially myths

38:09.840 --> 38:14.960
about ai and i want to get into some of those and and ask you to react to them and maybe debunk

38:15.040 --> 38:18.800
them but we have to take a really quick break we'll be right back when we're emily bender and tim

38:18.800 --> 38:25.600
nick ebru folks friends family members you know you should be backing up your computer right you

38:25.600 --> 38:32.800
know that you are just one glitch or one macbook in the toilet away from losing all your beloved data

38:32.800 --> 38:37.920
and you're like maybe it's stored in the cloud somewhere but also you keep getting that little

38:37.920 --> 38:42.240
notification saying you're running out of space pay more money and you're like but even if i pay

38:42.240 --> 38:47.120
them more money it's still going to tell me i'm running out of space so is my stuff saved is it not

38:47.120 --> 38:53.200
saved i don't know i just hope you know i don't drop my laptop in a river all right i understand

38:53.200 --> 38:58.160
you i've been there before okay but i'm here to tell you there is a solution you can put those fears

38:58.160 --> 39:03.520
to rest for just seven bucks a month if you sign up for back plays now look i know a lot of podcast

39:03.520 --> 39:08.800
hosts will tell you that you use the product but in my case i actually do i have been a back place

39:08.800 --> 39:13.760
subscriber for years before even having the opportunity to read this ad when the folks at

39:13.760 --> 39:18.320
my podcast ad company asked me do you want to do a back plays ad i said hell yeah i do this is

39:18.320 --> 39:23.760
actually a good product that i really use here's how it works you install it on your computer it

39:23.760 --> 39:30.560
runs in the background and it uploads all of your stuff to their server encrypted okay you will never

39:30.560 --> 39:35.360
see a message on back plays that says you ran out of space no matter how big your hard drive is you

39:35.360 --> 39:40.400
got a 20 gig hard drive it'll back all that stuff up okay here's how it works you install it it runs

39:40.400 --> 39:47.280
in the background and without you even noticing it uploads all of your data and backs it up securely

39:47.280 --> 39:51.200
and with back plays you will never see that message saying you just ran out of space because you

39:51.200 --> 39:57.040
cannot run out of space with back plays it backs up all your stuff okay i've got like 20 terabytes

39:57.040 --> 40:02.080
of stuff on back plays and i sleep securely at night knowing that if something goes wrong i can

40:02.080 --> 40:06.240
get all that back you can restore it anyway you want you can download one file you can have them

40:06.240 --> 40:11.440
send you a zip of a bunch of files you can even get a hard drive from them in the mail with all

40:11.440 --> 40:17.440
your stuff on it so you can put it all on a new computer it is such a seamless and secure system

40:17.440 --> 40:22.720
i love using it and it helps me feel better and it'll help you feel better too they have restored

40:22.720 --> 40:28.960
55 billion files for their customers that's almost two trillion gigabytes that they currently

40:28.960 --> 40:33.920
have under storage and with their web panel you can restore your data anywhere in the world

40:33.920 --> 40:38.240
they've been recommended by the new york times mac world pc world lifewire but more importantly

40:38.240 --> 40:44.800
they're being recommended to you by me right now and get this they have a 15 day no credit card

40:44.800 --> 40:50.000
required free trial if you want to see how it works at backblaze.com slash factually that's

40:50.000 --> 40:54.000
plenty of time for you to upload some stuff downloaded figure out how the system works before

40:54.000 --> 40:59.600
you even pay them a penny they don't even ask for your credit card first all right so seriously

40:59.600 --> 41:06.320
back your stuff up check out backblaze.com slash factually the no hassle online backup it's a no

41:06.320 --> 41:12.640
brainer decision according to pc world and according to me adam conover that's backblaze.com slash factually

41:15.600 --> 41:20.080
okay we're back with emily bender and tim nick ebru so we were talking about the ai pause letter

41:20.080 --> 41:25.440
and i was starting to talk about how it seems to have a lot of assumptions built into it about

41:25.440 --> 41:30.720
how ai works and how it's going to progress that the people who wrote it and the people who found

41:30.720 --> 41:36.320
it open ai and the people in the tech industry have really pushed onto the public and i see those

41:36.320 --> 41:40.720
assumptions actually in my youtube comments i'll see them in the comments to this video when we post

41:40.720 --> 41:47.920
it on youtube um and in the comments to my last one people say well ai is progressing so quickly

41:47.920 --> 41:54.480
it's unstoppable it's progressing every single day and so this idea of the pause seems to like

41:54.480 --> 41:59.280
build you know connect to that idea where oh my god this is a runaway train and all we can do

41:59.280 --> 42:05.840
is try to steer it in a direction when you know we could be questioning like these are just humans

42:05.840 --> 42:13.200
like making these things like they can do whatever they want at any time um and a and b is it maybe

42:13.200 --> 42:17.360
not a foregone conclusion that it's going to progress in the direction that they say it will

42:17.360 --> 42:23.280
like it seems to me that the large language models are designed to make you think oh this is a step

42:23.280 --> 42:29.440
on the road to general intelligence to a literal thinking computer but uh and if you play with

42:29.440 --> 42:33.680
it for five minutes you might think that if you play with it for you know tens of hours as i have

42:33.680 --> 42:39.520
you stop thinking that and you realize it's just mashing text up um i'm curious if you know if we

42:39.520 --> 42:45.280
could dig into some of that is ai something that is constantly going to keep improving no matter

42:45.280 --> 42:48.800
what we do and we just need to like control it and make sure it's not going to destroy us

42:49.600 --> 42:56.320
so i have a lot to say on this good so there's a um wonderful uh explanation that comes from

42:56.320 --> 43:01.920
beth singler about how combination of like looking back in the in what's happened in science and

43:01.920 --> 43:08.240
technology to date combined with science fiction and imaginings of the future makes us think that

43:08.240 --> 43:13.920
there is a path that we are just racing along and it's only a question of how fast do we get

43:13.920 --> 43:20.080
there who's going to get there first and that's not how science happens right science is exploration

43:20.080 --> 43:24.880
it's communication it's choosing things to work on or not um i think there's some interesting

43:24.880 --> 43:30.000
stuff in the history of nuclear power and how the interests of building nuclear weapons shaped

43:30.000 --> 43:35.040
the decisions we made about what kind of nuclear power to work on for example um and you know it's

43:35.040 --> 43:40.880
all as you're saying choices that we can make and we don't really know what's possible in the future

43:41.440 --> 43:47.760
um but because of this idea of you know ai that's given to us from science fiction and

43:47.760 --> 43:53.680
to say i'm a huge fan of speculative fiction um but i mean for yeah it's cool um but i'm largely in it

43:53.680 --> 43:59.760
for the exploration of what happens to the human condition given these different settings like

43:59.760 --> 44:03.680
that's what i see the point of science fiction to be and a lot of this seems to come from this

44:03.680 --> 44:07.760
idea of no the point of science fiction is the cool spaceships and the teleport devices and the robots

44:08.320 --> 44:13.840
right and yeah those are cool but like that doesn't mean that it's going to exist so this notion

44:13.840 --> 44:20.400
of a path that we're just racing along as fast as we can is false and we don't have to buy it um

44:20.400 --> 44:27.280
and another part of it is when they say and you repeat um ai is just progressing that makes it

44:27.280 --> 44:33.840
sound like ai is doing it on its own and no what's happened is a lot of corporations and individual

44:33.840 --> 44:39.680
billionaires have put a lot of money into gathering big piles of data and doing some clever engineering

44:39.680 --> 44:45.040
about how to manage that data and then build these learning systems that compress it into

44:45.040 --> 44:51.200
something that can do the word calculator thing um and that happened quickly way more quickly than

44:51.200 --> 44:56.640
we thought it was like too many are both quite surprised by how fast this happened not because

44:57.280 --> 45:03.440
the tech got incredibly cool incredibly quickly but because it sort of got out into the world

45:03.520 --> 45:09.200
that quickly and what i see there isn't rapid scientific progress i see a lot of money and a

45:09.200 --> 45:13.760
lot of hype and all of a sudden someone's got the money to set up this thing so that anybody

45:13.760 --> 45:17.920
can access it apparently for free although every time you do that you're doing some work for open

45:17.920 --> 45:25.120
ai just by the way um so it's not really free but chat gpt is less a technological advance and

45:25.120 --> 45:30.560
more of a product that was created it was a way of taking something that already existed

45:30.560 --> 45:36.240
and opening it up to people and prompting it in a way to maximize the sort of public

45:36.240 --> 45:41.440
shock of it and to make people think that it was extremely capable and to sort of further

45:41.440 --> 45:47.680
this narrative that uh things are progressing so quickly but it's there's a little bit of a comparison

45:47.680 --> 45:52.640
to you know steve jobs invented the iphone well steve jobs didn't invent any technology

45:52.640 --> 45:56.800
he combined a lot of technologies some of which were invented by the federal government 30 years

45:56.800 --> 46:03.360
earlier um and like put them into a very well marketed product with a with a shiny wrapper

46:03.360 --> 46:07.520
and like a really nice clean store you could buy it in um there's maybe a little bit of a comparison

46:07.520 --> 46:13.920
there yeah yeah i think so um the the big thing with chat gpt the reason it just exploded all over

46:13.920 --> 46:19.360
the media was anybody could play with it which was a brilliant pr move on open ai's part because

46:19.360 --> 46:26.000
meant that everybody was doing their height for them right i mean i i would say you only need to

46:26.960 --> 46:33.600
try to talk play with it in any other language for like two minutes i like to agree it doesn't even

46:33.600 --> 46:39.120
it's complete gibberish you know what i mean so i'm like well i guess the agi speaks english we've

46:39.120 --> 46:46.160
already assumed that you know what i mean like but it's you know it's so crazy how many people um

46:46.160 --> 46:52.800
i've been talking to who are engineers and and researchers and they and and stuff say parrot

46:52.800 --> 46:58.880
exactly the talking points of open ai and you know anthropic and similar organizations that are

46:58.880 --> 47:04.880
making this point that everything is going to be built on top of it and it's going to trump

47:04.880 --> 47:13.200
like any other kind of development so most of gdp is going to be dependent on that and so

47:13.200 --> 47:20.720
whoever is not you know on top of whoever does not have that technology by like 2025 or something

47:20.720 --> 47:23.920
like that they're not going to be able to catch up because it's just going to be accelerating

47:23.920 --> 47:27.840
so fast you know what i mean this is the kind of stuff a lot of people are saying you hear this

47:27.840 --> 47:33.360
argument that in response to the pause letter you heard people say well if we pause china's

47:33.360 --> 47:38.160
going to keep making the ai and they're going to use it to kill us and like what are they going to

47:38.160 --> 47:43.600
use the devil kind yeah gpt for they're going to write more shitty fan fiction with it are they

47:43.600 --> 47:50.160
going to you know output more bad recipes like it's it's unclear what it's being presented as

47:50.160 --> 47:55.840
some threat to national security when the actual capabilities of these large language models

47:55.840 --> 48:01.440
they're cool they're very cool tech there's some cool stuff you can do with them but this is not

48:01.440 --> 48:07.280
launching nuclear warheads or like you know fight like waging national security battles

48:07.280 --> 48:12.240
but i'm sorry please continue your point now i was just that's basically it that's all i was

48:12.240 --> 48:17.200
going to say and it's that it's it's a few the really surprising thing to me and what's been a

48:17.200 --> 48:23.840
huge lesson i guess in history or current affairs whatever you want to call it is how few people

48:23.840 --> 48:31.200
can drive this is is really what is unbelievable to me few billionaires a few billionaires and a few

48:31.200 --> 48:38.640
people in in the space of deep learning together just can drive this entire thing the whole media

48:38.640 --> 48:44.080
eco chamber the chamber the whole research direction the entire you know silicon valley

48:44.080 --> 48:50.560
ecosystem and you know it's it's been extremely surprising to see that yeah and disappointing

48:50.560 --> 48:55.120
to see microsoft and google and like i've got criticisms of these corporations but they were

48:55.120 --> 49:01.760
pretty staid and stodgy especially microsoft like jumping on this um we should maybe talk about the

49:01.760 --> 49:05.920
sparks do you want to talk about the spark yeah i was gonna say i wanted to cue you to talk about

49:05.920 --> 49:11.200
the sparks of agi paper what's the sparks of agi anyway what is the sparks of agi paper this is

49:11.200 --> 49:19.680
fascinating i'm the host emily what is the sparks of agi paper this is a uh something that takes the

49:19.680 --> 49:24.000
form of a research paper it's not peer reviewed it was just thrown up on archive which is this place

49:24.000 --> 49:28.560
that was initially developed i think by physicists to help disseminate research faster and what's

49:28.560 --> 49:33.040
happened in machine learning and computer science more generally is that it's become this place to

49:33.040 --> 49:38.000
like put things up as if they were research papers and just bypass peer review entirely and so there's

49:38.000 --> 49:43.680
there's a whole problem over there so sparks of agi is one of those papers published by microsoft

49:43.680 --> 49:48.320
research a big group of people there including the head of research was an author author to air

49:48.320 --> 49:56.960
coordinates yeah did not notice that and it's um they took an intermediate version of gpt4 because

49:56.960 --> 50:02.160
you know microsoft is in bed with open air and this is this is microsoft can't be above the fray

50:02.160 --> 50:07.520
here they are they are part of this they've their funding was like ten billion dollars and then

50:08.240 --> 50:13.280
gpt4 is now driving the bing chat thing and remember that they have a search engine and it's called bing

50:13.920 --> 50:20.160
yeah right yeah and now it's and now it's got a super clippy embedded in it where it talks back

50:20.160 --> 50:26.880
to you yeah right super clippy um and so they have as researchers at microsoft access to

50:26.960 --> 50:32.160
a sort of an interim version of gpt4 and they use a whole bunch of these benchmarks on it

50:32.160 --> 50:36.400
that were developed by different people trying to test natural language processing systems

50:37.200 --> 50:43.520
generally without like really good construct validity that is what is this thing supposed to

50:43.520 --> 50:47.360
be testing and how do we know it's actually testing that especially given a large language model as

50:47.360 --> 50:53.280
the thing taking the test um and it's a 154 page thing where they they try gpt4 and all these things

50:53.280 --> 50:58.080
and they say yeah um you know it looks like we have the first sparks of artificial general

50:58.080 --> 51:05.600
intelligence here and that's that's what the paper is but it gets worse all right so um my

51:05.600 --> 51:12.960
first comment on seeing this was remember when you used to go to um microsoft for stodgy but

51:12.960 --> 51:21.120
basically functional software and the bookstore for science fiction well you know now we've got

51:21.120 --> 51:26.000
this like maybe it's like a fan fiction to gpt4 that's been published as if it were a research

51:26.000 --> 51:32.160
paper out of microsoft well so so what is so ludicrous about the idea that large language models

51:32.160 --> 51:38.640
are a step on the way to agi or or the sparks of agi because you know as you point out in the

51:38.640 --> 51:44.560
stochastic parrots paper it looks like agi to us right it passes if you want to loosely interpret

51:44.560 --> 51:49.040
the turing test right can it fool so can it fool a human into thinking they're talking to another

51:49.040 --> 51:54.880
human yes it can do that you could trick somebody using its output um and so for a lot of people

51:54.880 --> 51:59.680
that's what they were taught to believe is a step on the way to agi that's like the the version you

51:59.680 --> 52:07.760
learn in college um and so and it certainly seems that way to people um so what what is you know

52:07.760 --> 52:13.200
what are the barriers that stop it from being that can you start with the first page the first the

52:13.280 --> 52:20.080
first sentence yeah exactly paper um i have to get the paper up so that i can do that for you um

52:20.080 --> 52:27.280
but the the first problem with with it being the first steps to um to agi is that agi is undefined

52:27.280 --> 52:33.360
it's what tim meet was describing before as an unscoped technology so that's first steps to nowhere

52:33.360 --> 52:39.200
number one number two we know what a language model is it's a word calculator as you say it right

52:39.200 --> 52:46.160
so the fact that it seems to be giving us something coherent that's all us like gladly

52:46.160 --> 52:52.880
interpreting it as if it were inherent a coherent and nothing on the side of the actual system um

52:52.880 --> 53:00.000
so i was trying i'm like really trying hard to get you to talk about what they cite you know i'm

53:00.000 --> 53:08.080
i'm getting to it okay because it is so it is so atrocious and i have to get my um the i love

53:08.080 --> 53:14.000
how much fun you guys have with this roasting these papers is i i love it i love it when academics

53:14.000 --> 53:19.200
get spicy and you guys are delivering the goods that's right you know we just read the footnotes

53:19.200 --> 53:26.720
that's how we get spicy no but yeah it's always read the footnotes so so the um the pause letter

53:26.720 --> 53:33.040
by the way cites the sparks of agi this is one of its academic sources for the danger that's coming

53:33.040 --> 53:38.800
right but it's not peer reviewed and it's and it's fanfiction to a machine right all right so sentence

53:38.800 --> 53:44.480
one intelligence is a multifaceted and elusive concept that has long challenged psychologists

53:44.480 --> 53:50.320
philosophers and computer scientists sentence two is where it is to meet an attempt to capture its

53:50.320 --> 53:56.960
essence was made in 1994 by a group of 52 psychologists who signed on to a broad definition

53:56.960 --> 54:03.040
published in an editorial about the science of intelligence so i thought hmm let's go look at

54:03.040 --> 54:10.400
what this definition is and where this came from that editorial was published in reaction to the

54:10.400 --> 54:16.000
public outcry and discussion about the book called the bell curve do you remember this book

54:16.080 --> 54:21.840
by charles mary uh-huh yeah i remember this book this is a bunch of psychologists yeah now please

54:21.840 --> 54:28.400
go ahead no yeah a bunch of psychologists who are saying okay we've got to wait in here because this

54:28.400 --> 54:34.960
discussion has gotten out of hand and i'm like okay okay what are they saying needs to be established

54:34.960 --> 54:42.480
what they say in this terrible editorial is no no no iq is real these measures of it are good

54:42.480 --> 54:49.280
they are not racist and yes there are group level differences in iq where jews and asians are the

54:49.280 --> 54:53.920
smartest but we don't know exactly how much and then you've got the white people centered around 100

54:53.920 --> 54:59.040
and then they say but the black people are centered around 85 and like this is flat out what is in

54:59.040 --> 55:05.600
that editorial that this group of researchers at microsoft decided to use as the basis for their

55:05.600 --> 55:11.600
definition of intelligence so they can say yes gbt4 is the first steps on the way to artificial

55:11.600 --> 55:19.360
intelligence using this definition so it's it's totally foundational and it is shocking to me

55:19.360 --> 55:27.280
that nobody in that group of authors thought maybe we shouldn't be pointing to race science and just

55:27.280 --> 55:31.680
like flat out racism posted in the wall street journal as the basis of what we're doing and

55:31.680 --> 55:36.800
like the more charitable interpretation here is none of them actually read what they were citing

55:37.120 --> 55:46.320
like that would be better than reading it going yeah this seems okay but you know it's eugenics

55:46.320 --> 55:53.840
all the way down i mean so yeah how do they know that chat gbt is or that gbt4 is a is intelligent

55:53.840 --> 55:58.080
then are they checking to see if it's jewish or or what are they like if that's what they're citing

55:58.720 --> 56:02.880
and they're and they're citing a paper that says that you know asian people and jews are more

56:02.880 --> 56:07.280
intelligent then they that's a pretty easy thing to tell they could just test if the ai

56:07.280 --> 56:13.600
circumcised i'm sorry i'm a comedian i apologize the chatbot circumcised yeah

56:16.480 --> 56:24.800
but so i mean in addition to the to the shoddy research though like what is it about these language

56:24.800 --> 56:31.200
models that fail so profoundly like one one thing to me is that i keep coming back to and i even wish

56:31.280 --> 56:36.720
i had put more clearly in my own youtube video on this subject is that no ai that we have has any

56:36.720 --> 56:41.120
kind of like understanding that other minds exist you know like and that's like a foundational part

56:41.120 --> 56:46.480
of intelligence as you and i are talking to each other you know us three and we each have our own

56:46.480 --> 56:50.880
minds and they're interacting and they're communicating um and when you are communicating

56:50.880 --> 56:56.400
with chat gbt you are imputing a mind to it it's almost impossible to use it without imagining

56:56.400 --> 57:00.880
that there's a mind in there even though there isn't but it is not imagining a mind talking

57:00.880 --> 57:07.040
back to it it's just chopping up words and phrases and uh you know like a self-driving car what is the

57:07.040 --> 57:11.360
foundational problem with self-driving cars they can't communicate they can't make an eye contact

57:11.360 --> 57:15.920
with another person and go you know i had a whole interaction with a car the other day where i was

57:15.920 --> 57:19.680
like oh this car needs to pass i was walking in the street where there's there's no sidewalk this

57:19.680 --> 57:24.400
car needs to pass me i'm gonna go stand where in the parking part you know where where the other

57:24.400 --> 57:28.720
cars are parking and then i look back at the car it hasn't gone past i look back and the lady points

57:28.720 --> 57:33.520
and she goes no actually i wanted to park there and i said oh now i'm in your way i need to go

57:33.520 --> 57:37.440
get in the street because you were trying to use the parking lot you're trying to use the the the

57:37.440 --> 57:41.920
shoulder there's no way for an ai to have a communication with a person like that to know

57:41.920 --> 57:47.120
that there's a person with intent who i need to deal with in order to decide what the machine should

57:47.120 --> 57:53.040
do um and that that seems to me to be like a fundamental extremely fundamental part of intelligence

57:53.120 --> 57:58.800
that no level of hey let's make the language model better is ever going to accommodate

57:58.800 --> 58:04.880
because it's it's all it is is a thing that you put words in one end and more come out the other

58:04.880 --> 58:09.600
end i i imagine you might have more examples though of like what would actually constitute

58:10.160 --> 58:15.520
intelligence that these fail at or maybe not like octopus octopi she has a whole paper on

58:16.160 --> 58:21.840
on our yes yeah so so i have a paper from 2020 co-authored with alexander coller which is uh

58:21.840 --> 58:24.960
has the octopus thought experiment which is why i'm wearing my octopus earrings here

58:26.400 --> 58:33.440
purchased from an artist on etsy by the way um and um the what we were talking about there is um

58:33.440 --> 58:39.120
basically showing it doesn't matter how intelligent the thing is it's not going to learn to understand

58:39.120 --> 58:43.200
if all it has access to is the form of the language so to make that point we put together

58:43.200 --> 58:48.640
this thought experiment with a hyper intelligent deep-sea octopus um and credit for it being an

58:48.640 --> 58:53.280
octopus goes to my co-author alexander i was thinking dolphin and he's like no octopuses are

58:53.280 --> 58:57.920
inherently funnier and also um that makes the environment more distinct from where the humans

58:57.920 --> 59:02.400
are so hyper intelligent deep-sea octopus two humans stranded on two separate desert islands

59:02.400 --> 59:06.560
that happen to be connected by a telegraph cable the humans figure this out and they start doing

59:06.560 --> 59:11.840
morse code to each other english as encoded in morse code the octopus remember hyper intelligent

59:11.840 --> 59:16.640
we're not doubting its intelligence taps into that cable and starts listening to the patterns

59:16.640 --> 59:21.760
of the dots on the dashes and then after a while it cuts the cable and it starts sending dots and

59:21.760 --> 59:28.080
dashes back based on the patterns that it's seeing and it can get away with this because a lot of

59:28.080 --> 59:32.000
the communication is you know just sort of keeping each other company and so if something comes back

59:32.000 --> 59:37.680
that's good enough right but then we have this point where the um uh one of the people on the

59:37.680 --> 59:42.560
island says oh no i'm being chased by a bear because the thought experiment right spherical

59:42.560 --> 59:46.720
cows and all that bear shows up on the desert island all i have are these two sticks what

59:46.720 --> 59:53.360
am i going to do um and of course the the octopus can't provide anything useful because the octopus

59:53.360 --> 59:59.760
hasn't understood has no model of the people's world even though we've posited it to be hyper

59:59.760 --> 01:00:04.800
intelligent right so flipping that around to what people are seeing in the language models

01:00:04.800 --> 01:00:10.160
our primary evidence such as it is that these things are intelligent is their apparent ability

01:00:10.160 --> 01:00:15.680
to understand and create coherent text that's the only evidence that they're intelligent yeah

01:00:15.680 --> 01:00:22.000
but in fact we know because of the octopus's thought experiment that it can't be that it's just

01:00:22.000 --> 01:00:27.680
coming up with plausible next words something that looks like an answer um and so there's no

01:00:27.680 --> 01:00:33.280
evidence for intelligence there at all now i frequently get asked by people okay emily so

01:00:33.840 --> 01:00:37.680
what's the test that would convince you that one of these things is actually intelligent

01:00:37.760 --> 01:00:43.600
twitch my answer is that's not my job i'm not trying to build one of these things why why do

01:00:43.600 --> 01:00:50.160
we want to build those things that's what i don't understand who who wants to do that and what what

01:00:50.160 --> 01:00:56.640
is it supposed to accomplish right a g i and then what like no more climate change like clean water

01:00:57.200 --> 01:01:02.000
what i don't i don't understand the connection at all that's the weird thing because they say

01:01:02.000 --> 01:01:08.560
it's coming we got to get ready we we need to be ready for it when it comes but it's like wait why

01:01:09.200 --> 01:01:14.320
you're making it people are making it if the if if anyone's going to make it it's the people who

01:01:14.320 --> 01:01:20.080
are telling you to be worried about it if anyone's going to create an agi and why are they what's the

01:01:20.080 --> 01:01:26.320
purpose and so actually this leads me to a good question to end on um because uh if it occurs to

01:01:26.320 --> 01:01:31.920
me a lot that look i love new technology i think chat gpt is super cool i played with it a ton it

01:01:31.920 --> 01:01:36.400
's like the kind of technology i love to play with i love to play with see what i can what kind of

01:01:36.400 --> 01:01:42.240
output i can get if i mess around with it if they had advertised it as this is a word calculator

01:01:42.240 --> 01:01:47.360
here's what it does you put words in one end and it'll make the most it'll make a plausible

01:01:47.360 --> 01:01:53.760
sounding answer to any question you ask it or you can you know it'll imitate any uh you know

01:01:53.760 --> 01:01:58.880
you give it input and it'll imitate a plausible output that would have been really cool and they

01:01:58.880 --> 01:02:03.840
could have come up with a lot of very plausible narrow uses for that such as computer programming

01:02:03.840 --> 01:02:10.000
or other things of that nature but instead the industry made a marketing decision to say that

01:02:10.000 --> 01:02:16.160
this was a step on the road to a super intelligent ai that we have to protect ourselves from and so

01:02:16.720 --> 01:02:24.160
the question i keep wrestling with is why what was the purpose of misleading people of about

01:02:24.160 --> 01:02:29.760
what the technology can do what were they trying to accomplish do you have any idea i that's the

01:02:29.760 --> 01:02:35.200
paper i just so that's that's what we've been i personally you just wrote a paper on this oh

01:02:35.200 --> 01:02:39.120
my god you're the perfect person to ask that question that's why i that's why i've been trying

01:02:39.120 --> 01:02:45.200
to figure out like why when did people decide like we have to do a gi because when we're thinking

01:02:45.200 --> 01:02:50.480
about large language models for instance i was telling emily uh i didn't have a problem with

01:02:50.480 --> 01:02:56.080
like birth that was a large language model and i didn't really have a problem with you know them

01:02:56.080 --> 01:03:01.840
being used in components to do various things whatever it was when opening i came in the scene

01:03:01.840 --> 01:03:07.760
and started talking about these things like they are this huge super you know intelligent thing and

01:03:07.760 --> 01:03:12.800
we're going to do a gi and we're gonna you know it's going to be like uh either amazing or utopia

01:03:12.800 --> 01:03:18.320
or apocalypse and we have to focus on the future stuff that's what really started driving this whole

01:03:18.320 --> 01:03:25.360
thing and our paper with emil which is like the one i was talking about was um you know tracing back

01:03:25.360 --> 01:03:32.000
these test real ideologies back to first wave eugenesis and all of that it basically talks about

01:03:32.000 --> 01:03:37.120
how this whole movement came about because you know so when we were talking about the connection

01:03:37.120 --> 01:03:43.520
to eugenics it definitely was not theoretical it was for instance you know the chair of the

01:03:43.520 --> 01:03:51.440
british eugenics society talking about transhumanism right transcending humanity and the cause the people

01:03:51.440 --> 01:03:58.240
who first they call it they say christened the term agi in 2007 in a whole book that they wrote

01:03:58.240 --> 01:04:06.000
the way they described it was transhuman agi you know they think that the agi will help humanity

01:04:06.000 --> 01:04:12.720
transcend being human and become post-human colonize space you know and and live in like

01:04:12.720 --> 01:04:17.840
digital mind so when you see sam altman's writings if you just read his blogs right now

01:04:17.840 --> 01:04:22.320
he says we're gonna have unlimited energy and intelligence before the decade is out

01:04:22.320 --> 01:04:28.800
he writes in his blog post that we have something by human flourishing and the cosmos the universe

01:04:28.800 --> 01:04:36.720
you know and so that that was why i had to write the paper because it was like why and the the the

01:04:36.720 --> 01:04:42.560
final conclusion was precisely what you were saying we were saying that you know there's been

01:04:42.560 --> 01:04:48.240
ai winters and such because and at some point people doing various things like natural language

01:04:48.240 --> 01:04:54.960
processing computer vision etc didn't call themselves ai whatever right we're just like oh i'm doing um

01:04:54.960 --> 01:05:00.320
nlp i don't want to be associated with these ai weirdos who always talk about building a god

01:05:00.320 --> 01:05:05.280
because they over promise like that and then you know people see through it and then it crashes

01:05:05.280 --> 01:05:11.440
and then it comes back right and so now it's back and there's this whole agi thing and i really would

01:05:11.440 --> 01:05:20.640
love to play this game where i ask people is this from like 1962 or 2022 right like we're like oh my

01:05:20.640 --> 01:05:25.680
god you will be astonished to see what we have built you know and in the next 20 years people

01:05:25.680 --> 01:05:31.120
will not have to work right and so this is what's going on and the thing is um as emily was saying

01:05:31.120 --> 01:05:37.680
earlier that it is super aligned with this this super you mentioned Scientology i want them to

01:05:37.680 --> 01:05:42.320
build the church of test grill you know ideologies or something like that it because it really is

01:05:42.320 --> 01:05:48.080
like that um they have very much like religious characteristics of like the end of times kind

01:05:48.080 --> 01:05:54.080
of things you know apocalyptic and utopian but it also is super aligned with corporate interests

01:05:54.080 --> 01:05:58.880
right you know if you build this like one model that can do anything for anyone and everybody

01:05:58.880 --> 01:06:03.840
just pays you and you can steal everybody's data and say that you're actually like saving humanity

01:06:03.840 --> 01:06:07.520
and creating a god oh and you shouldn't be regulated because otherwise china is going to

01:06:07.520 --> 01:06:12.960
do the devil kind and you don't want that you want us to do the good kind like it is super in line

01:06:13.760 --> 01:06:19.840
with like corporate interests so yeah that's the conclusion that we've come uh to with our

01:06:20.640 --> 01:06:26.320
paper that we're hoping to publish at some point after a peer review so it is corporate

01:06:26.320 --> 01:06:32.960
interest but it also these the people who are doing this actually have a definite ideology of

01:06:33.520 --> 01:06:41.520
100 of eugenics of that they they come from they write this they write this stuff yeah yeah and

01:06:41.520 --> 01:06:46.400
unfortunately the money's behind them and so it's becoming everybody's problem instead of this niche

01:06:46.400 --> 01:06:52.800
little research community that could just go be weirdos on their own yeah well how do you suggest

01:06:52.800 --> 01:06:59.040
that you know for the public who's watching this and being flooded with misinformation with hype

01:06:59.040 --> 01:07:05.840
about ai uh how can they gird themselves against it and you know like what what's the best way to

01:07:05.840 --> 01:07:10.800
resist it and to think a little bit more critically the next time they're confronted with it so i

01:07:10.800 --> 01:07:15.600
think i think we the public have a big job to do here around pushing for appropriate legislation

01:07:15.600 --> 01:07:21.840
not the ai pause but something that actually is governance of collection of data and you know

01:07:21.840 --> 01:07:27.200
synthetic media that is built through consultation with the people who are bearing the front of this

01:07:27.200 --> 01:07:31.520
right now so the people who are being exploited in developing the systems the people whose data is

01:07:31.520 --> 01:07:37.680
being stolen the people who are getting misinformation said about them and all of this so so developing

01:07:37.680 --> 01:07:44.720
regulation collectively um but also resisting misinformation and the non-information so with

01:07:44.720 --> 01:07:49.920
chat gpt being set up it looks to me that it's sort of like the um the oil spill in the Gulf of

01:07:49.920 --> 01:07:55.600
Mexico when that um that the oil rig was broken and there was just oil going and going and going

01:07:55.600 --> 01:08:01.680
right and BP was you know eventually saying look at all the birds we cleaned right um chat gpt is

01:08:01.680 --> 01:08:07.440
polluting our information ecosystem in the same way with non-information and i've had people say to

01:08:07.440 --> 01:08:13.440
me well you know hasn't the horse left the barn on that like it's out there and my answer to that is

01:08:13.440 --> 01:08:18.240
we used to have lead in gasoline and we discovered that was bad news so we made some regulation

01:08:18.240 --> 01:08:24.720
and now we don't like we don't just have to live with this we can regulate um and one thing that

01:08:24.720 --> 01:08:30.400
i would love to see regulation lies like my my wish list item on this is uh corporations are

01:08:30.400 --> 01:08:36.240
accountable for the actual literal output of their text synthesis machines it's libeling someone you

01:08:36.240 --> 01:08:40.960
get sued for libel it's putting out bad medical information people get hurt you're liable i would

01:08:40.960 --> 01:08:45.840
love to see it set up that way i don't know if that's something that works policy wise but that's

01:08:45.840 --> 01:08:51.440
an idea in terms of just on an individual level how do you resist the ai hype um i think the questions

01:08:51.440 --> 01:08:57.440
to ask are okay what's the actual task here what's the evidence that this machine is well matched to

01:08:57.440 --> 01:09:04.480
that task how is it evaluated can i see the data that it was evaluated on and who's really benefiting

01:09:04.480 --> 01:09:10.080
by using this system and who gets hurt um when it's wrong who gets hurt when it's right but people

01:09:10.080 --> 01:09:19.200
you know are sort of using it um as a shortcut and so on that's a wonderful answer and uh i think i

01:09:19.280 --> 01:09:23.440
think that last question is who is benefiting and who is actually getting hurt right now

01:09:24.000 --> 01:09:27.840
is one of the most important questions we can always ask ourselves about the world but especially

01:09:27.840 --> 01:09:33.120
in this case uh i can't thank both of you enough for coming on and and you're i mean you're just the

01:09:33.120 --> 01:09:38.320
the perfect people to to speak to this topic and it was an honor to have you where can people

01:09:38.320 --> 01:09:43.280
follow your work and and what's the most important thing of yours you think they should read is it

01:09:43.280 --> 01:09:49.040
is it the on the dangers of stochastic pair parents your favorite your famous paper um that's

01:09:49.040 --> 01:09:53.680
that's worth a read we are in the process of creating an an audio paper of that we've recorded

01:09:53.680 --> 01:09:58.640
it and i have to edit it together i'm sorry i haven't done that i have to release a recording of our

01:09:58.640 --> 01:10:05.360
event stochastic parents day yeah um so probably the best way to find me is probably my faculty

01:10:05.360 --> 01:10:08.880
webpage at the university of washington and from there you can see links to everything i do in the

01:10:08.960 --> 01:10:15.040
media my papers and stuff like that i am for the moment still on twitter and also on mastodon and

01:10:15.040 --> 01:10:20.400
that's available through my web page so if you search emily bender university of washington

01:10:20.400 --> 01:10:28.160
you'll find me and tim need how about you yeah um so dare dare institute website which is

01:10:28.160 --> 01:10:32.800
not that much information right now but in a couple of days revamped we'll see much more

01:10:32.800 --> 01:10:38.240
information there with a lot of our work and other things i'm also on twitter so if you want

01:10:38.320 --> 01:10:44.960
to hear me rant i'm there but also a mastodon i'm a huge fan of the fediverse these days because i

01:10:44.960 --> 01:10:48.800
i don't know i'm not worried that some random billionaire is going to take over that anytime

01:10:48.800 --> 01:10:56.560
soon i ventured into linkedin and i'm trying to stay there but it's kind of difficult but i'm there

01:10:56.560 --> 01:11:02.960
too awesome to me and emily thank you so much for coming on it's it's been a true honor to have you

01:11:03.920 --> 01:11:06.160
thank you for having us and for raising these issues

01:11:08.320 --> 01:11:12.240
well thank you once again to emily bender and tim need gebru for coming on the show i hope you

01:11:12.240 --> 01:11:16.640
loved that conversation as much as i did and if you did i hope you will consider supporting the

01:11:16.640 --> 01:11:23.040
show on patreon head to patreon.com slash adam conover and join our wonderful community including

01:11:23.040 --> 01:11:27.040
folks who back this show at the fifteen dollar a month level and i'd love to read a couple of

01:11:27.040 --> 01:11:33.440
your names we got hydrochloric victor densmore francis amadar kill me ink christina mendez akash

01:11:33.440 --> 01:11:39.360
thakar frank f cling robin dumblap jeffery mcconnell nissy pods brian tobone leslie kokeshawn

01:11:39.360 --> 01:11:45.200
garrison raghav kaushik olwiz sonny and ashley malini dias thank you folks so much for your

01:11:45.200 --> 01:11:49.760
support and if you want to join him head to patreon.com slash adam conover to get every

01:11:49.760 --> 01:11:54.640
episode of the show ad free and a bunch of other goodies as well we even do a live book club would

01:11:54.640 --> 01:12:00.160
love to see you there i want to thank our producer sam rodman our engineer kyle magraw and the fine

01:12:00.160 --> 01:12:04.080
folks at fucking northwest for building me a wonderful custom gaming pc that i record every

01:12:04.080 --> 01:12:09.520
episode of the show on you can find me online at adam conover dot net you can find my tour dates at

01:12:09.520 --> 01:12:14.960
adam conover dot net slash tour dates come see me do stand up all across the country and of course

01:12:14.960 --> 01:12:19.840
you can find me on social media at adam conover wherever you get your social media thank you so

01:12:19.840 --> 01:12:25.040
much for listening and we'll see you next time on factually

