start	end	text
0	6240	Hello, and welcome to Factually. I'm Adam Conover. Thank you so much for joining me once again as I
6240	10880	talked to an incredible expert about all the amazing things that they know that I don't know and that
10880	14480	you might not know. Both of our minds are going to get blown together and we're going to have
14480	19280	so much fun doing it. I want to remind you that if you are watching this podcast on YouTube,
19280	22960	go subscribe to the podcast and your favorite podcast player if you want to hear it every week.
22960	27680	If you're listening on your podcast player, go check out the video episode on YouTube. Now this
27680	34400	week we're talking about AI. Just a few weeks ago, I released a YouTube video called AI is BS in
34400	40480	which I argued that AI has become a marketing term that tech companies are using to hype up
40480	46320	software that cannot do what they claim it does and in which in many cases could be dangerous if
46320	51760	they jam it into mainstream products that it is not ready for. In fact, these companies are hyping
51760	57280	up AI to such an extent that they're trying to convince people that software like chat GPT is
57280	63600	a step on the road to some kind of godlike artificial general intelligence when in reality
63600	68720	what they actually made is a text generator that can write some pretty cool fanfic and help you
68720	72880	program and you know if they had marketed that way in the first place they said hey we made a tool
72880	77840	that'll output a recipe that tastes bad if you try to cook it. I mean that would be pretty neat. We
77840	83760	could think of a lot of uses for a text generator like that but a step on the road to true artificial
83840	90160	intelligence it is not and it is kind of fucked up to tell people that it is. Now that video got a
90160	96320	somewhat let's say divisive response because a lot of people on the internet have drunk the AI
96320	102960	hype Kool-Aid. These tech companies have succeeded in confusing the issue of AI so much that a lot
102960	108160	of the time when we say AI most of us don't even know what we're referring to. We don't understand
108160	113360	how the software works and we don't understand how that's connected to the science fiction fantasies
113440	119120	that the companies are peddling us and because of that confusion a lot of weird shit is happening.
119120	124960	For instance while I was in the process of editing my video 1600 major AI researchers
124960	130160	as well as people like Elon Musk and Steve Wozniak came out and asked for a six month
130160	135040	pause on AI research but they didn't ask for that pause because of you know AI giving out
135040	139760	misinformation or the fact that it just recycles the copyrighted work of artists like myself and
139760	144480	others no they asked for that pause because they were worried that it could create an AI
144480	150800	superintelligence again the bullshit hype science fiction claim so a bunch of other AI researchers
150800	155440	came out against this letter saying that we do not have a problem with AI because of the super
155440	160880	intelligence thing we have a problem because you are exploiting the work of real people and making
160880	166560	the world a worse place right now so I don't blame the people in my comments for being confused
166560	172480	even AI researchers themselves do not agree entirely on what the problems are so for that
172480	176960	reason we are going to spend a couple episodes of this podcast talking to some of those AI
176960	182160	researchers about what the problems are and how we might go about fixing them and on the show
182160	188160	today we have two incredible guests their names are Emily Bender who's a professor at the University
188160	193360	of Washington and Timney Gebru who's the executive director of the distributed artificial intelligence
193360	197680	research institute and you might recognize the name Timney Gebru because she is the researcher who
197680	205040	was famously fired by google for raising AI ethics concerns in that famous paper I am so excited to
205040	210480	talk to them because they are two of the sharpest minds on AI and how the problems with it are not
210480	214960	what the tech companies have been telling you but before we get to that interview I want to remind
214960	220480	you that if you want to support this show please head to patreon.com slash adam conover you can
220560	226400	get every episode of this podcast ad free and get a bunch of other goodies and even more importantly
226400	231680	please come see me on tour this summer I'm taking my brand new hour of stand-up to San Francisco,
231680	237360	San Antonio, Tempe, Arizona, Batavia, Illinois just outside Chicago, Baltimore, Maryland and St.
237360	243280	Louis, Missouri head to adamconover.net for tickets come see me I'd love to give you a hug in the
243280	248400	meet and greet line after the show and now without further ado let's get to my interview with Emily
248400	256640	Bender and Timney Gebru Timney and Emily thank you so much for being on the show super happy to be here
257280	262240	thank you for having me it's an honor to have both of you considering you know I've read your work
262240	268720	I've talked about it in my last youtube video all about AI you're some of the foremost researchers
268720	274480	on the topic some of the foremost critics of how the tech industry has been employing AI
274480	278640	so I'd love to hear from you first of all Emily we last talked it was might have been close to a
278640	286640	year ago back when AI was very much an active research subject we were hearing a lot about it
286640	292240	but it wasn't something that the average person was using in the year since AI has become radically
292240	297200	mainstreamed a lot of these companies have just shoved it into consumer products without you
297200	302480	know any concern for what the results are as two as two people who follow the field extremely
302480	308480	closely what has your reaction been to the last you know six months or so of rapid development in
308480	320320	the industry blank faces here it's been a lot of oh come on again more seriously yeah that's
320320	329760	that's how I feel I mean I just I am dumbfounded by the number of people I thought were more reasonable
329760	338160	than this kind of jumping on a bandwagon of what seems to be mania so I don't know that's how I feel
339840	344240	what do you think the greatest you know potential harms of this are in your paper your very famous
344240	349200	paper on the dangers of stochastic parents you talked about many dangers that you know these would
350000	354560	reify you know discriminatory materials in the training data by repeating it out to people that
354560	360320	people would take it to literally you know would take the pronouncements of a large language model
360320	366560	as fact uh a lot of those have seemed to come directly true do you feel validated that your
366560	373600	that your criticisms have come to pass no no because those were predictions those were warnings
374640	380000	like don't do it you know we don't want to get there and then we got there and then some right
380080	388400	like I just I definitely don't feel validated I feel upset I'm sad about it um yeah how do you
388400	393600	feel Emily you know I think that some of the biggest problems that maybe I didn't understand
393600	399600	because it's it's sort of half an economic problem um is the way in which people would say hey this
399600	404720	looks like it could be a robo lawyer this looks like it could be a robo therapist and look at all
404720	408960	those people who can't afford real lawyers and real therapists so let's give them this instead
409040	415440	and like that jump from um you've identified a real problem in the world it's a problem that
415440	419760	mental health resources are inaccessible and it's a problem that legal representation is inaccessible
420400	427200	but then try to fill that hole with something that is just a joke and can directly cause harm
427200	432000	when deployed in those cases I think even when we were writing the paper and saying you know it
432000	436080	would be bad if this was set up in such a place where people might believe it or believe it knew
436080	440320	what it was talking about I don't think I was in a position to predict that that's a direction that
440320	446000	it would go in at the time we wrote the paper I was just you know seeing this whole mind is bigger
446000	450800	than yours kind of race and just being very confused why is this the thing that anybody
450800	456560	everybody just wants to be the biggest one and now and now you have not just that text to text
457280	463440	models but text to image text to video video or whatever you know and so I didn't I didn't imagine
463440	469520	that in such a short time that kind of explosion of synthetic media into the world would happen
470080	477280	and I also didn't think about I would say what the content moderation demands and issues would be
477280	484320	with that much like explosion of synthetic media you know like the um the what is the fix
484320	489920	Clark's world shutting down submissions because they got a little bit devious yeah stuff like that
489920	496400	is something I didn't predict Emily you talked about just now the you know the sort of thing
496400	501040	happened that happens a lot of technology once it's released people come up with new uses for it
501040	506560	that nobody predicted and one of the uses that people have started you know uh do you talk about
506560	514240	robo lawyers uh I've seen people say that they're using uh chatbots as therapists um or as relationship
514240	519760	surrogates and what are the dangers of of those types of uses because I'm certainly seeing those
519760	525600	promoted all over the there's a lot of folks saying hey if you don't have access to xyz and
525600	530720	ai can do that for you in all sorts of fields and and you said that these are a joke what makes them
530720	537040	a joke for that purpose so they're a joke because with they're all form and no content right so
537040	542640	what these systems are really good at is mimicking the form or the style of something so it absolutely
542640	546880	can write something that looks like a legal contract for you but if your purpose in drafting
546880	551360	up a legal contract is anything other than intimidating the other party with legalese
551360	556480	then the specific content and the way that it maps into your situation really matters yeah and it
556480	562720	might be that there's some sort of template type situations where it's like okay yeah this is a
562720	570720	contract for you know the rights to use a piece of music um and um I want this right assigned and
570800	574560	that went not and it's going to be paid for this much and here's like you could answer a few questions
574560	578960	and get something out from a template that would work reasonably well but that's not what they're
578960	584400	doing right they're saying what's a plausible next word what's a plausible next word given this context
584400	588960	and you know who knows where that's going to be um so for the the legal case you know you're asking
588960	592960	for that because you are not a lawyer and you can't afford a lawyer you're not going to be in a position
592960	600080	to tell if it's good or not but it'll look impressive right right it it sort of will can it
600160	606160	will create a convincing imitation of a piece of text that will most readily convince someone
606160	612240	who knows nothing about the field like a lot of you know I work in television writing and there's
612240	619520	a lot of talk in you know oh can can studios use chat gpt to write scripts and when I use one of
619520	626160	these services to uh write you know to output text I'm like yes this superficially looks like a script
626160	631760	but it's missing so many of the things that you would need to film a script and someone might say
631760	637760	well what if the technology gets better and it's not a matter of aping something even more correctly
637760	643440	it's a matter of to successfully write a piece of screenwriting you need information about the
643440	650160	rest of the world that a no algorithm or ai program could ever have you need to understand
650160	655040	uh you know what is physically possible to produce you need to under you need to talk to
655040	660400	a department full of people who say uh a very good example I use of this is I didn't realize until
660400	665120	I started writing television that you can never have someone jump into a pool on television if you
665120	670080	watch tv and someone jumps into a pool it'll always happen off camera and I had because I had a scene
670080	674640	where I someone jumped into a pool my line producer told me they you need to remove that and I said
674640	679200	why it seems not that hard they said because we need to film every take five times so that means
679200	683280	we need five pairs of wardrobe because we need to film them one after another and we need to dry
683280	687840	the person off and do their makeup and it's going to take all day and so as a result people never
687840	693200	get wet on television or when they do it's very expensive okay interesting and you'd have no way
693200	698400	of knowing that uh without real life experience and even if an ai could eventually figure that out
698400	701840	there's also a million other things like that that are specific to the particular production
701840	705680	hey it's going to be cloudy on wednesday we need you to rewrite the scene you know there's there's
705680	710400	so many details that are that are fundamentally about humans communicating with each other
710400	716560	and that's the same thing with with a lawyer doesn't just output text a lawyer talks to
716560	720640	like knows what the other side might do in response knows how aggressive they'd be if you're
720640	725920	trying to sue laws are yeah exactly if you're trying to evict a tenant they have they're going
725920	729920	to have a much different response than if you're trying to sue the church of Scientology right
729920	737440	who are very aggressive like and knowing what the laws are too so to me this is it's very obvious
737440	743280	when i actually look at how they're used but it's it is it a problem with the technology or is it a
743280	749280	problem with humans not understanding how our own society works to to not realize that these that
749280	755440	these tools are going to be effective the hype too the hype yeah so it's a problem with the task
755440	761200	technology fit so what is it that we need and how does the technology fit into it and and timmy
761200	765760	and i want to bring up your wonderful line about how these things are unscoped technologies
765840	769680	and then maybe you could elaborate on that a little bit yeah i mean i was going to bring up
769680	775040	all your work on hype but which i think um and i really add them i mean it just like when i'm
775040	779440	talking to you i'm like yeah we live in the same planet and you know we're having the same
779440	785760	conversation in the same language i i am not that's not the language that we're speaking with the
785760	791440	other researchers in ai or machine learning or whatever it is i am i'm so confused what's going
791440	799520	on but because you know um scoping systems is a very basic engineering concept right when
799520	806640	you're building something you want to know what you're building it for and then see if what you're
806640	811280	building it for is actually being fulfilled whereas in this case the way they're advertising
811280	816560	their systems is that they're building it to accomplish anything for everybody anywhere
816560	821600	write code speak whatever language you know write scripts movie scripts protein folding
821600	825840	whatever it is and that's a fundamentally unscoped system that i don't even know how
825840	832560	we can make sure um can be safe or work um to add to the notion of being an unscoped system
832560	838960	right which to me is a basic um engineering concept when you're trying to build something
838960	844960	you ask what am i building uh to accomplish what are the tasks that i want to accomplish
845040	850480	under which scenarios under which conditions and in this case when you see the kinds of things that
850480	857520	they are um advertising um all of these companies metta talked about this um uh large language model
857520	864000	based system they had called galacticon they said oh it's gonna write code just protein folding stuff
864000	870320	and write scientific papers and more you know and with opening eyes chat gps like write movies
870320	875200	replace artists do this do that and more you know and so already you just haven't
875200	879920	have built a system that we don't even know what it's supposed to use for to be used for and how
879920	885840	do we even test whether it is actually accomplishing its um task the task that it's supposed to be
885840	893040	built for and one one problem here is that uh open ai is not at all open about how these things
893040	898960	are trained and so not only is it not tested in specific contexts where you can say okay
898960	904240	here here is the range of um here are the safety parameters of the system here's how well it has
904240	907680	been tested to work in these contexts we don't have that information we also don't know what
907680	912560	is training data or training regimen was and according to open ai this is somehow for safety
912560	916960	which makes no sense at all because one of the very first things that was worked out about
917760	924000	responsible development of these kinds of systems is provide documentation of the underlying dataset
924000	929520	and of the parameters of sort of safe use of the model and that's kind of the first place that
929520	935120	tim meet and i got to know each other independently right we were both working on this um separately
935840	941440	yeah and then and then we connected through that sort of related work and that was you know really
941440	945200	really fortunate it was 2017 i think there was just something in the air where a whole bunch of
945200	949440	groups said we got to document these things so that we could figure out how we could use them
949440	954560	and open ai while claiming to be doing this for safety is flat out refusing to do that
955120	959840	and what's the danger of that if they are refusing to release the or make the model transparent
960400	965520	so you can't make any decisions about whether it would be good to use the model or not if it's
965520	970320	not transparent like let's say i want to use it for writing computer code well i can try it a few
970320	975760	times and see if it seems to work well and then maybe get some confidence but i don't know what
975760	980240	its training data looks like and for programming languages from what i've heard again they're not
980240	984960	open about this but part of the training data is literally sort of english descriptions together
984960	990720	with executable codes there's a lot of paired stuff in there that helps it do well a lot of the time
990720	994720	the other thing about programming languages is that they are specifically designed to be unambiguous
995520	1000720	which is in stark contrast to natural languages where ambiguity is sort of a fundamental design
1000720	1008080	feature everything is ambiguous and so the fact that it does well with this more constrained
1008080	1012400	universe of programming languages kind of makes sense but again we can't really know because we
1012400	1016560	don't know how it's trained but imagine like you're happy with this performance in helping you
1016560	1020880	generate code and you've even got some like computer security buffs on your team and they
1020880	1025520	look at that say yeah i don't see anything frightening coming out here but they keep changing
1025520	1030080	the system and then all of a sudden it's maybe putting that into your code but there's no information
1030080	1034320	about what version you're using you can't say no i want to keep using the version you know from
1034320	1040800	december of 2022 because that's gone all you have is the open it uh the open ai um api is what i was
1040800	1045520	trying to say that um allows you to connect with whatever they've put up for you to connect with
1046320	1052800	and a lot of tools are being built on their api right now there's uh if you open the app store
1052800	1058800	any kind of app store you'll find countless tools that are ai xyz you know ai help you write code
1058800	1065600	ai help you write a movie script ai therapist and they're really just hooking into open ai's model
1065600	1071840	and paying them a couple pennies per however many requests uh and people are now starting to use
1071840	1078720	those tools to do real things without knowing what is where the the output is coming from or
1078720	1083120	what the model is it does kind of remind me a little bit you're talking about models like i've
1083120	1087760	talked to plenty of climatologists on the show and like climb you know climate models are a huge
1087760	1092800	part of our understanding of how the climate works but we also know how those models work and we
1092800	1096800	can compare them and we have a lot of information about them so that we know when they predict
1096800	1103840	something you can go back to the what the source was but in this case it's both by design but also
1103840	1108080	by corporate structure a black box because they're not telling us anything about it i was just
1108080	1112640	thinking about something even more basic like how do we know we're not still they're not stealing
1112640	1119600	people's work um to profit off of it so you know there were all these um lawsuits by artists to
1119600	1126960	devian r i stability ai and mid journey right but not open ai not dally because we don't know
1127600	1132320	what training data they were used so we don't know if they were copyright violations or not if they
1132320	1138240	constantly you know compensated anybody versus not but they can do that right they there's nothing
1138240	1144320	that is like preventing them from doing that right now yeah and it makes me wonder so look i can go to
1144320	1148720	into chat gpt i talked about this in my youtube video i can go into chat gpt and say write an
1148720	1155280	episode of adam ruins everything about xyz and it'll output something that looks like a pros
1155280	1160800	version of adam ruins everything adam walks into the room and says blah blah blah about dogs and
1160800	1166080	i'm like where is this coming from because i don't believe it has access to my shooting scripts
1166080	1170080	because those are not public anywhere so i'm trying to figure out where is it getting the
1170080	1176000	information from and again this is based on my own copyrighted work that i spent a lot of time
1176000	1180560	putting together is the character i created and i would think that you know i would if it's being
1180560	1185200	used for profit i would like to be paid for it in some respect um i think that's a pretty fundamental
1185200	1190240	feature of our uh how capitalism is currently arranged and i would like it to follow those rules
1190240	1194400	but it's difficult for me to tell i'm like it could just be getting it all from like fan fiction
1194400	1198800	like i feel like it's really scraped like our archive archive of our own and all these big
1198800	1203840	fan fiction sites um but it's really it's really really difficult to tell let's spend a second
1203840	1209920	though and talk about open ai as an organization and the sort of ideology behind it because this is
1209920	1215120	you know the organization that is most in the news pushing uh ai forward and it was incorporated
1215120	1220080	originally as a nonprofit right and made a lot of noise about how the point is to make
1220080	1225280	sure they're going to do it responsibly it's research it's not about profit but it has recently
1225280	1230560	i believe changed its incorporation status to be a for-profit company completely changed its tune
1231120	1236640	and they now say it wouldn't be safe if we were to open it up to all of you but meanwhile you've
1236640	1241200	got the founder sam altman going on all sorts of podcasts talking to the news about how he has to
1241200	1245680	do all this because ai is really really dangerous but then the dangers they're always talking about
1245760	1250960	are always the science fiction kind where how the robot takes over the spaceship or you know from
1250960	1256240	isek azimov kind of style of science fiction where a super powerful intelligence you know takes over
1256240	1260160	the universe kind of thing they're never talking about the harms that we're talking about that
1260160	1264160	people might use it to write a legal document it shouldn't write or that it might rip off somebody's
1264160	1270160	copyrighted work or anything like that so what is your view of you know this this organization and
1270160	1275680	it's uh supposed altruism is this just a faint to sort of trick us all into thinking that they
1275680	1281680	have our best interests at heart or is as a corruption happened or what so this is the
1281680	1286960	test grill question and i'm gonna let tim need to find test grill but just my take on this very
1286960	1293440	quickly is i think they believe they are being altruistic and working in the best interests of
1293440	1302240	people but their view of who counts as a person is very narrow and sort of leaves out of you all
1302240	1307440	of the people who are being harmed now or just sees those harms as inconsequential compared to
1307440	1313200	what they're worrying about which is in this science fiction universe it's hard to say the
1313200	1316960	phrase science fiction fantasy because to me those are two genres of wonderful speculative
1317040	1326400	fiction and you don't want to bucket this into that yeah you're just so i you know i can say so
1326400	1332320	much about them i've been on them for a long time so in 2015 when they were announced i wrote a open
1332320	1338480	letter that i didn't end up sending to anybody i just kept it to myself because i was a phd
1338480	1342880	student back then and people were like uh people will know that it's you because i was so angry
1343440	1349280	by the tone so i don't think it's a corruption or they've changed their tone or whatever to me
1349280	1357440	they've like stayed exactly the same um and initially they um said they exactly they talked
1357440	1362320	to they wrote you know they talked about it as if they were gonna save humanity peter teal and
1362320	1368320	illa musk always as usual just on the ball to save humanity of course that's always what they've
1368400	1374400	been doing in the world and um and all the whole media was talking about it like oh this
1374400	1380560	nonprofit is starting they're gonna save humanity from ai because back then what happened is that
1380560	1386560	they had invested in deep mind that they also wanted to create you know a gi artificial general
1386560	1392160	intelligence which is a system that none of us know what it even is supposed to do this is the
1392160	1399840	acronym for a for a super intelligent uh uh sounds like a god sounds like a god to me um and so
1399840	1405680	they were all very much trying to develop this thing which i don't even know what it is and um
1405680	1411760	deep you know invested in deep mind deep mind got bought by google 2015 the future of life institute
1411760	1418640	had a similar letter asked to the one that we see the pause letter um and then they you know found
1418720	1423680	open ai put bill you know hundreds of millions of dollars into it because they say they're gonna
1423680	1429360	save humanity and all of that and create this agi thing fast forward right they realize they need
1429360	1435840	a lot more money uh they're now essentially bought by um microsoft and then now they have a competition
1435840	1441840	so they need to be closed and all of that so to me really it wasn't like a pivot or anything i
1441840	1447120	never believed that they were gonna you know save humanity or anything like that and in terms of
1447120	1457280	emily's um q about the test grill bundle um so you know i i have been really so irritated by the
1457280	1461360	whole crew because i've been around them for a long time i went to school with some of them
1461360	1469120	been around this you know agi community for a while so recently i teamed up with um a collaborator
1469120	1475760	of ours um whose name is emil torrez who used to be a long termist and so a long termism is this
1475840	1481520	weird you know the future of life institute behind the pause letter is a long termist institute and
1481520	1490080	so they literally think that our um job as humans is to maximize the number of future humans who
1490080	1495440	colonize space and digitally upload their minds and like live in the matrix kind of thing right
1495440	1502480	this is a real thing like it's not an exaggeration that's what they want yeah i've read i've read a
1502480	1507360	lot of that philosophy that you know the idea that we need to be thinking about how do we maximize
1507360	1513920	the future uh happiness and well-being of humans 10 000 years from now if you could why save one
1513920	1518960	life today when you could save 10 million lives uh 10 000 years from now now i'd say how the
1518960	1522880	fuck do you know that what you're gonna do is gonna have any effect on people that far in the
1522880	1526960	future it's the height of hubris to think that you can project that far into the future at all
1526960	1532160	well they give you some random numbers they pull some number numbers out of their asses like oh
1532160	1537440	my god we didn't know sam bankman freed was gonna be doing this but we will know what 10 000 years
1537440	1542640	or now point zero zero one probability that you know what i mean it's absolutely ridiculous but
1542640	1549120	anyhow so the test grill bundle is a bunch of ideologies that are all sort of descendants of
1549120	1554800	the first wave eugenics um eugenics movement and you know when you hear this word about human
1554800	1561360	flourishing maximizing our potential through both positive and negative eugenics positive would be
1561440	1565040	the ones who are desirable you want them to breed you want them to you know
1565680	1570640	multiply right and the ones who are negative the ones who are undesirable you want to kind of get
1570640	1576960	rid of them because they don't help you with this human flourishing thing so the transhumanists
1576960	1582160	you know were very much um that that ideology was very much developed by 20th century eugenics
1582800	1589200	um and nick bostrom who is also a long-termist you know it's also very famous very prominent
1589200	1596240	transhumanists right and so um we traced how these ideologies transhumanism extropianism the
1596240	1602240	singularity people who say singularity is coming because of ai the cosmos who are actually the
1602240	1608400	people who wrote the first book on age artificial general intelligence in 2007 the effective altruism
1608400	1614240	the long-termists and how they're all in this circle kind of learning from each other networking
1614240	1620560	with each other lots of money going into them and they're all sort of either selling agi utopia
1620560	1627040	or agi apocalypse right if we do it right it's going to bring us utopia it is out human flourishing
1627040	1631920	we need to do it if we do it wrong we're going to have an apocalypse because it's going to take
1631920	1638240	over the world or china is going to do the devil kind of agi and you need to let us do this utopian
1638320	1645120	kind because we're vanguards of humanity so it's obviously a very kind of convenient
1645680	1648720	ideology for the billionaires because you know they're saying give us all the money
1649280	1654080	we'll do the utopian kind um and but we're super worried about it because it might be
1654080	1659360	super powerful but we're careful because you can trust us right and so sam altman is kind of is
1659360	1664000	doing that thing right and to me he's in the same sort of camp as the future of life people
1664800	1670400	because that's the same thing they're selling yeah i've the connection to eugenics is not
1670400	1674640	theoretical i've i've seen it myself if you look at nick bostrom's writing and the writing of a lot
1674640	1681120	of folks who write extensively about ai or agi the future you know super ai that could control the
1681120	1688480	world um a lot they also write overtly about eugenics they have charts and tables about if we
1688480	1693040	what if we started a human breeding program and only allowed people in the top percent
1693760	1697840	of intelligence to breed and then they would have super babies and the babies would be super
1697840	1703760	smart and it's like this was tried in the 40s in a country in europe you know this this is um
1703760	1708400	this is very these are very old ideas by the way you can just look at a the interview i did a
1708400	1712800	couple weeks ago about intelligence to to learn about whether intelligence is actually heritable
1712800	1719440	in the in that way it's not um but so these the proximity of these ideas to each other is not
1719440	1725360	theoretical these are you know the same folks promoting neo eugenics and promoting um ai
1725360	1730960	catastrophism uh i want to refer though to this pause letter that you mentioned a couple times
1730960	1737440	so that folks know what it is um a couple weeks ago actually as i was editing my ai video um a
1737440	1742480	whole bunch of ai researchers from many many different organizations signed a letter suggesting
1742480	1748560	a six month pause on ai after the release of gpt4 and they said well this is very dangerous
1748560	1756000	we need to evaluate it um and etc etc and some folks who i you know i i have read and and enjoyed
1756000	1761360	as ai researchers are signed to the letter um and it sounds on the face of it that that might rhyme
1761360	1766320	with some of what you folks are saying um but you took objection to the letter and so i'd love
1766320	1771840	a little bit of explanation from you about about exactly what your uh issue with that is like what
1771840	1776400	what did that letter get wrong do we do you feel we need to pause or do we need to pause for a
1776400	1784320	different reason or what so i think pause is unrealistic i think six months is unrealistic
1784320	1789360	i think the letter makes it sound like these researchers are just now noticing that this
1789360	1795680	might be harmful despite you know years and years and years of work of people saying hey there's
1795680	1802560	harms here um and the letter itself is basically saying oh no we've built something too powerful
1802560	1808480	better be careful so it's it's what livencel calls pretty good yeah we're too good gotta stop
1808480	1812640	so it's it's basically helping to sell the technology um i have to say that i found out
1812640	1817280	about the letter a little bit before it dropped because there was a um a journalist who contacted
1817280	1821680	me asking if i was going to sign it and would i comment and i'm like haven't seen it not going
1821680	1827120	to comment on what i haven't seen and then like i think later that day it came out and i was busy
1827120	1831440	and then finally evening i sat down to read it and i thought oh god i have to i have to react to this
1831440	1839600	so i put out a tweet thread um and then you know media you know craziness about it and so i said
1839600	1844800	to tim neat and the other two listed authors of the stochastic parents paper let's put together
1844800	1849600	a statement coming from us so that we can point the media at that for one thing but also to have
1849600	1854400	sort of a joint statement here and and tim neat sort of took everybody's remarks including my
1854400	1858960	relatively snarky twitter thread and like pulled together our first draft that we then worked on
1858960	1865120	and where we start with that is with the observation that they cite us in their first footnote number
1865120	1870720	one yeah they say your stochastic parents paper number one and number two is nick boston yeah
1870720	1875760	wow oh wow okay they got they went all the way from alpha to omega there huh they're they're
1875760	1881280	inciting everybody but they didn't ask you to sign the paper that's interesting oh they would know i
1881280	1887760	would never yeah okay so their sentences ai systems with human competitive intelligence
1887760	1894080	can pose profound risks to society and humanity as shown by extensive research footnote one
1894080	1899280	and acknowledged by top ai labs footnote two and that footnote one has us embostrum and some other
1899280	1906640	people but the stochastic parents paper was not a paper about ai systems with human competitive
1906640	1911920	intelligence it was a paper about large language models which are not ai systems with human
1912000	1918640	competitive intelligence as we note so many times in the paper like that was the thing we said yeah
1919680	1924160	so like right off the bat it was just infuriating and there's like one or two things in here that
1924160	1932080	i think do rhyme as you say so you know we need regulation and that regulation should involve
1932080	1937840	things like watermarking so that we can tell when we're encountering synthetic media and
1937840	1942960	um you know liability for ai caused harm that sounds good that liability should sit with the
1942960	1947760	companies that are creating and deploying the ai they don't say that um but then there's a bunch
1947760	1952240	of really weird stuff in here ai research and development should be focused on making today's
1952240	1959920	powerful state-of-the-art systems more accurate safe interpretable transparent robust okay i'm
1959920	1967680	all right with all those aligned is a keyword for this weird one trustworthy yeah but then the last
1967680	1977200	one the last one is loyal a loyal loyal to who oh wait no no sorry they started doing Boy Scouts
1977200	1984080	a scout is trustworthy brave reverent kind obedient or whatever i quit Boy Scouts when i was like 10
1984080	1987760	but there's like a whole list of things in one of them's reverence so i'm surprised they didn't
1987760	1993920	include reverence shouldn't it go to church yeah but here's the thing and ai i mean so
1993920	1998400	there's the first question that timnit raises of okay loyal to whom whose interest is it serving
1998400	2003840	but also these are not the kinds of things that can be loyal to be loyal is to experience
2003840	2009840	certain feelings to have certain commitments and large language models are just text synthesis
2009840	2017200	machines yeah so they predict what comes next or they're i hear a really great description i heard
2017200	2022240	of them is to think of them as word calculators they they do a good job of you give it a bunch of
2022240	2026480	words and it can turn them into other words that are derived from the first words and that can be
2026480	2030400	a useful thing to do sometimes particularly if you're a computer programmer or someone else who
2030400	2037600	like is manipulating text on that sort of level but that's not a it's it's not a thing that has
2037600	2045200	an ethical drive such as loyalty yeah yeah exactly so so this this letter you know got a lot of
2045200	2051200	attention partially because of who signed it and you know we've we've had so we we pushed back pretty
2051200	2057920	quickly and then we were getting reactions like oh you're squandering the opportunity this is gary
2057920	2062960	marcus complaining about us going out for blood coming out what is what did he say coming out
2062960	2068160	they went for blood we went for blood or something like that or it's just like okay and sure and
2068880	2073440	basically it's like they supposedly created an opportunity for regulation that would maybe get
2073440	2078080	the six month pause whatever that means it's it's all completely unfounded right pause on systems
2078080	2083920	more powerful than gpt4 well we don't have the specs on gpt4 so that's a unmeasurable undefined
2083920	2088160	thing anyway um and as someone was pointing out and i'm sorry i don't have the source for this
2088160	2091920	a lot of the work in creating new systems is actually in the data preparation
2092720	2097920	and gathering these enormous amounts of data and a six months pause on training the systems
2097920	2102240	wouldn't prevent anybody from going and collecting more data we need to prevent that in other ways
2102240	2107600	to prevent data theft but that's a separate question right um and then you get people out
2107600	2114240	there saying well why can't the so-called ai safety and ai ethics people get along so the ai
2114240	2118800	safety people are the long-termists who want to prevent the agi from taking over the world
2118800	2122560	and ai ethics is sometimes used to refer to the people who are concerned with the problems in
2122560	2128800	the here and now in the ways that yeah they basically created the term ai ethic i'm ai safety
2128800	2133520	to separate their themselves from us is how i feel about it because like we have the same
2133520	2138720	technical expertise we have other expertise also but like it doesn't mean that you know
2138720	2145280	so i feel like they they they named that that field or whatever it is to explicitly separate
2145280	2149920	themselves from kind of our crew right yeah yeah so by answer to why can't we get along it's like
2149920	2155680	well if why can't we find common cause if the ai safety people wanted to find common cause with
2155680	2160800	those of us working in ethics they would cite us they would go to the you know to to tim needs work
2160800	2166000	they would go to the work of syphia nobel and ruha benjamin and kathie o'neal two past guests on
2166000	2172640	the show by the way just want to ding ding ding ding excellent um and you know build on that and
2172640	2177600	lend some of their money and resources to making that happen but of course they don't want to
2177600	2182480	because they're aligned with corporate interests and to really push back and to really reduce the
2182560	2188960	harms here we need regulation that reigns in the corporations like why would ilan musk sign like
2188960	2195600	everybody has to ask why does ilan musk an advisor uh end funder a future of life institute someone
2195600	2200480	who pumped hundreds of millions of dollars into open ai and deep mind or whatever and whatever
2200480	2205840	why is he so interested in like caution and whatever as long as it doesn't touch him sure
2205840	2211120	you know if we're talking about regulating tesla or looking at the racial the largest racial
2211120	2216480	discrimination um lost in history in california that's not what he wants us to talk about right
2216480	2221120	like he doesn't want us to talk about any of those things and whatever he's doing with twitter
2221120	2226480	we have to think about oh my god like this super powerful science fictiony thing that's going on
2227280	2232640	and it's just so disappointing to see that number of people who went along with it i think for us
2232640	2240720	we wanted to make sure we wanted to make it clear that we are not aligned with this vision
2240720	2247920	of ai safety they hold eugenics roots um there's emily has a thing she always says always read
2247920	2253840	the food notes we write the food notes and they have a food note that says you know if we don't
2253840	2262480	do x y and z um we ai systems might have might be potentially catastrophic like other potentially
2262480	2267200	catastrophic things like eugenics and we wanted to be like eugenics is not just potentially
2267200	2271680	catastrophic it has been catastrophic you know what i mean so we just want to make sure that
2271680	2277760	they can't they should not be able to launder um people's reputations to make themselves mainstream
2277760	2284480	and appear reasonable i also think that there there's a huge number of unexamined assumptions
2284480	2289840	in that letter that they are using the letter to promote to the public that are essentially myths
2289840	2294960	about ai and i want to get into some of those and and ask you to react to them and maybe debunk
2295040	2298800	them but we have to take a really quick break we'll be right back when we're emily bender and tim
2298800	2305600	nick ebru folks friends family members you know you should be backing up your computer right you
2305600	2312800	know that you are just one glitch or one macbook in the toilet away from losing all your beloved data
2312800	2317920	and you're like maybe it's stored in the cloud somewhere but also you keep getting that little
2317920	2322240	notification saying you're running out of space pay more money and you're like but even if i pay
2322240	2327120	them more money it's still going to tell me i'm running out of space so is my stuff saved is it not
2327120	2333200	saved i don't know i just hope you know i don't drop my laptop in a river all right i understand
2333200	2338160	you i've been there before okay but i'm here to tell you there is a solution you can put those fears
2338160	2343520	to rest for just seven bucks a month if you sign up for back plays now look i know a lot of podcast
2343520	2348800	hosts will tell you that you use the product but in my case i actually do i have been a back place
2348800	2353760	subscriber for years before even having the opportunity to read this ad when the folks at
2353760	2358320	my podcast ad company asked me do you want to do a back plays ad i said hell yeah i do this is
2358320	2363760	actually a good product that i really use here's how it works you install it on your computer it
2363760	2370560	runs in the background and it uploads all of your stuff to their server encrypted okay you will never
2370560	2375360	see a message on back plays that says you ran out of space no matter how big your hard drive is you
2375360	2380400	got a 20 gig hard drive it'll back all that stuff up okay here's how it works you install it it runs
2380400	2387280	in the background and without you even noticing it uploads all of your data and backs it up securely
2387280	2391200	and with back plays you will never see that message saying you just ran out of space because you
2391200	2397040	cannot run out of space with back plays it backs up all your stuff okay i've got like 20 terabytes
2397040	2402080	of stuff on back plays and i sleep securely at night knowing that if something goes wrong i can
2402080	2406240	get all that back you can restore it anyway you want you can download one file you can have them
2406240	2411440	send you a zip of a bunch of files you can even get a hard drive from them in the mail with all
2411440	2417440	your stuff on it so you can put it all on a new computer it is such a seamless and secure system
2417440	2422720	i love using it and it helps me feel better and it'll help you feel better too they have restored
2422720	2428960	55 billion files for their customers that's almost two trillion gigabytes that they currently
2428960	2433920	have under storage and with their web panel you can restore your data anywhere in the world
2433920	2438240	they've been recommended by the new york times mac world pc world lifewire but more importantly
2438240	2444800	they're being recommended to you by me right now and get this they have a 15 day no credit card
2444800	2450000	required free trial if you want to see how it works at backblaze.com slash factually that's
2450000	2454000	plenty of time for you to upload some stuff downloaded figure out how the system works before
2454000	2459600	you even pay them a penny they don't even ask for your credit card first all right so seriously
2459600	2466320	back your stuff up check out backblaze.com slash factually the no hassle online backup it's a no
2466320	2472640	brainer decision according to pc world and according to me adam conover that's backblaze.com slash factually
2475600	2480080	okay we're back with emily bender and tim nick ebru so we were talking about the ai pause letter
2480080	2485440	and i was starting to talk about how it seems to have a lot of assumptions built into it about
2485440	2490720	how ai works and how it's going to progress that the people who wrote it and the people who found
2490720	2496320	it open ai and the people in the tech industry have really pushed onto the public and i see those
2496320	2500720	assumptions actually in my youtube comments i'll see them in the comments to this video when we post
2500720	2507920	it on youtube um and in the comments to my last one people say well ai is progressing so quickly
2507920	2514480	it's unstoppable it's progressing every single day and so this idea of the pause seems to like
2514480	2519280	build you know connect to that idea where oh my god this is a runaway train and all we can do
2519280	2525840	is try to steer it in a direction when you know we could be questioning like these are just humans
2525840	2533200	like making these things like they can do whatever they want at any time um and a and b is it maybe
2533200	2537360	not a foregone conclusion that it's going to progress in the direction that they say it will
2537360	2543280	like it seems to me that the large language models are designed to make you think oh this is a step
2543280	2549440	on the road to general intelligence to a literal thinking computer but uh and if you play with
2549440	2553680	it for five minutes you might think that if you play with it for you know tens of hours as i have
2553680	2559520	you stop thinking that and you realize it's just mashing text up um i'm curious if you know if we
2559520	2565280	could dig into some of that is ai something that is constantly going to keep improving no matter
2565280	2568800	what we do and we just need to like control it and make sure it's not going to destroy us
2569600	2576320	so i have a lot to say on this good so there's a um wonderful uh explanation that comes from
2576320	2581920	beth singler about how combination of like looking back in the in what's happened in science and
2581920	2588240	technology to date combined with science fiction and imaginings of the future makes us think that
2588240	2593920	there is a path that we are just racing along and it's only a question of how fast do we get
2593920	2600080	there who's going to get there first and that's not how science happens right science is exploration
2600080	2604880	it's communication it's choosing things to work on or not um i think there's some interesting
2604880	2610000	stuff in the history of nuclear power and how the interests of building nuclear weapons shaped
2610000	2615040	the decisions we made about what kind of nuclear power to work on for example um and you know it's
2615040	2620880	all as you're saying choices that we can make and we don't really know what's possible in the future
2621440	2627760	um but because of this idea of you know ai that's given to us from science fiction and
2627760	2633680	to say i'm a huge fan of speculative fiction um but i mean for yeah it's cool um but i'm largely in it
2633680	2639760	for the exploration of what happens to the human condition given these different settings like
2639760	2643680	that's what i see the point of science fiction to be and a lot of this seems to come from this
2643680	2647760	idea of no the point of science fiction is the cool spaceships and the teleport devices and the robots
2648320	2653840	right and yeah those are cool but like that doesn't mean that it's going to exist so this notion
2653840	2660400	of a path that we're just racing along as fast as we can is false and we don't have to buy it um
2660400	2667280	and another part of it is when they say and you repeat um ai is just progressing that makes it
2667280	2673840	sound like ai is doing it on its own and no what's happened is a lot of corporations and individual
2673840	2679680	billionaires have put a lot of money into gathering big piles of data and doing some clever engineering
2679680	2685040	about how to manage that data and then build these learning systems that compress it into
2685040	2691200	something that can do the word calculator thing um and that happened quickly way more quickly than
2691200	2696640	we thought it was like too many are both quite surprised by how fast this happened not because
2697280	2703440	the tech got incredibly cool incredibly quickly but because it sort of got out into the world
2703520	2709200	that quickly and what i see there isn't rapid scientific progress i see a lot of money and a
2709200	2713760	lot of hype and all of a sudden someone's got the money to set up this thing so that anybody
2713760	2717920	can access it apparently for free although every time you do that you're doing some work for open
2717920	2725120	ai just by the way um so it's not really free but chat gpt is less a technological advance and
2725120	2730560	more of a product that was created it was a way of taking something that already existed
2730560	2736240	and opening it up to people and prompting it in a way to maximize the sort of public
2736240	2741440	shock of it and to make people think that it was extremely capable and to sort of further
2741440	2747680	this narrative that uh things are progressing so quickly but it's there's a little bit of a comparison
2747680	2752640	to you know steve jobs invented the iphone well steve jobs didn't invent any technology
2752640	2756800	he combined a lot of technologies some of which were invented by the federal government 30 years
2756800	2763360	earlier um and like put them into a very well marketed product with a with a shiny wrapper
2763360	2767520	and like a really nice clean store you could buy it in um there's maybe a little bit of a comparison
2767520	2773920	there yeah yeah i think so um the the big thing with chat gpt the reason it just exploded all over
2773920	2779360	the media was anybody could play with it which was a brilliant pr move on open ai's part because
2779360	2786000	meant that everybody was doing their height for them right i mean i i would say you only need to
2786960	2793600	try to talk play with it in any other language for like two minutes i like to agree it doesn't even
2793600	2799120	it's complete gibberish you know what i mean so i'm like well i guess the agi speaks english we've
2799120	2806160	already assumed that you know what i mean like but it's you know it's so crazy how many people um
2806160	2812800	i've been talking to who are engineers and and researchers and they and and stuff say parrot
2812800	2818880	exactly the talking points of open ai and you know anthropic and similar organizations that are
2818880	2824880	making this point that everything is going to be built on top of it and it's going to trump
2824880	2833200	like any other kind of development so most of gdp is going to be dependent on that and so
2833200	2840720	whoever is not you know on top of whoever does not have that technology by like 2025 or something
2840720	2843920	like that they're not going to be able to catch up because it's just going to be accelerating
2843920	2847840	so fast you know what i mean this is the kind of stuff a lot of people are saying you hear this
2847840	2853360	argument that in response to the pause letter you heard people say well if we pause china's
2853360	2858160	going to keep making the ai and they're going to use it to kill us and like what are they going to
2858160	2863600	use the devil kind yeah gpt for they're going to write more shitty fan fiction with it are they
2863600	2870160	going to you know output more bad recipes like it's it's unclear what it's being presented as
2870160	2875840	some threat to national security when the actual capabilities of these large language models
2875840	2881440	they're cool they're very cool tech there's some cool stuff you can do with them but this is not
2881440	2887280	launching nuclear warheads or like you know fight like waging national security battles
2887280	2892240	but i'm sorry please continue your point now i was just that's basically it that's all i was
2892240	2897200	going to say and it's that it's it's a few the really surprising thing to me and what's been a
2897200	2903840	huge lesson i guess in history or current affairs whatever you want to call it is how few people
2903840	2911200	can drive this is is really what is unbelievable to me few billionaires a few billionaires and a few
2911200	2918640	people in in the space of deep learning together just can drive this entire thing the whole media
2918640	2924080	eco chamber the chamber the whole research direction the entire you know silicon valley
2924080	2930560	ecosystem and you know it's it's been extremely surprising to see that yeah and disappointing
2930560	2935120	to see microsoft and google and like i've got criticisms of these corporations but they were
2935120	2941760	pretty staid and stodgy especially microsoft like jumping on this um we should maybe talk about the
2941760	2945920	sparks do you want to talk about the spark yeah i was gonna say i wanted to cue you to talk about
2945920	2951200	the sparks of agi paper what's the sparks of agi anyway what is the sparks of agi paper this is
2951200	2959680	fascinating i'm the host emily what is the sparks of agi paper this is a uh something that takes the
2959680	2964000	form of a research paper it's not peer reviewed it was just thrown up on archive which is this place
2964000	2968560	that was initially developed i think by physicists to help disseminate research faster and what's
2968560	2973040	happened in machine learning and computer science more generally is that it's become this place to
2973040	2978000	like put things up as if they were research papers and just bypass peer review entirely and so there's
2978000	2983680	there's a whole problem over there so sparks of agi is one of those papers published by microsoft
2983680	2988320	research a big group of people there including the head of research was an author author to air
2988320	2996960	coordinates yeah did not notice that and it's um they took an intermediate version of gpt4 because
2996960	3002160	you know microsoft is in bed with open air and this is this is microsoft can't be above the fray
3002160	3007520	here they are they are part of this they've their funding was like ten billion dollars and then
3008240	3013280	gpt4 is now driving the bing chat thing and remember that they have a search engine and it's called bing
3013920	3020160	yeah right yeah and now it's and now it's got a super clippy embedded in it where it talks back
3020160	3026880	to you yeah right super clippy um and so they have as researchers at microsoft access to
3026960	3032160	a sort of an interim version of gpt4 and they use a whole bunch of these benchmarks on it
3032160	3036400	that were developed by different people trying to test natural language processing systems
3037200	3043520	generally without like really good construct validity that is what is this thing supposed to
3043520	3047360	be testing and how do we know it's actually testing that especially given a large language model as
3047360	3053280	the thing taking the test um and it's a 154 page thing where they they try gpt4 and all these things
3053280	3058080	and they say yeah um you know it looks like we have the first sparks of artificial general
3058080	3065600	intelligence here and that's that's what the paper is but it gets worse all right so um my
3065600	3072960	first comment on seeing this was remember when you used to go to um microsoft for stodgy but
3072960	3081120	basically functional software and the bookstore for science fiction well you know now we've got
3081120	3086000	this like maybe it's like a fan fiction to gpt4 that's been published as if it were a research
3086000	3092160	paper out of microsoft well so so what is so ludicrous about the idea that large language models
3092160	3098640	are a step on the way to agi or or the sparks of agi because you know as you point out in the
3098640	3104560	stochastic parrots paper it looks like agi to us right it passes if you want to loosely interpret
3104560	3109040	the turing test right can it fool so can it fool a human into thinking they're talking to another
3109040	3114880	human yes it can do that you could trick somebody using its output um and so for a lot of people
3114880	3119680	that's what they were taught to believe is a step on the way to agi that's like the the version you
3119680	3127760	learn in college um and so and it certainly seems that way to people um so what what is you know
3127760	3133200	what are the barriers that stop it from being that can you start with the first page the first the
3133280	3140080	first sentence yeah exactly paper um i have to get the paper up so that i can do that for you um
3140080	3147280	but the the first problem with with it being the first steps to um to agi is that agi is undefined
3147280	3153360	it's what tim meet was describing before as an unscoped technology so that's first steps to nowhere
3153360	3159200	number one number two we know what a language model is it's a word calculator as you say it right
3159200	3166160	so the fact that it seems to be giving us something coherent that's all us like gladly
3166160	3172880	interpreting it as if it were inherent a coherent and nothing on the side of the actual system um
3172880	3180000	so i was trying i'm like really trying hard to get you to talk about what they cite you know i'm
3180000	3188080	i'm getting to it okay because it is so it is so atrocious and i have to get my um the i love
3188080	3194000	how much fun you guys have with this roasting these papers is i i love it i love it when academics
3194000	3199200	get spicy and you guys are delivering the goods that's right you know we just read the footnotes
3199200	3206720	that's how we get spicy no but yeah it's always read the footnotes so so the um the pause letter
3206720	3213040	by the way cites the sparks of agi this is one of its academic sources for the danger that's coming
3213040	3218800	right but it's not peer reviewed and it's and it's fanfiction to a machine right all right so sentence
3218800	3224480	one intelligence is a multifaceted and elusive concept that has long challenged psychologists
3224480	3230320	philosophers and computer scientists sentence two is where it is to meet an attempt to capture its
3230320	3236960	essence was made in 1994 by a group of 52 psychologists who signed on to a broad definition
3236960	3243040	published in an editorial about the science of intelligence so i thought hmm let's go look at
3243040	3250400	what this definition is and where this came from that editorial was published in reaction to the
3250400	3256000	public outcry and discussion about the book called the bell curve do you remember this book
3256080	3261840	by charles mary uh-huh yeah i remember this book this is a bunch of psychologists yeah now please
3261840	3268400	go ahead no yeah a bunch of psychologists who are saying okay we've got to wait in here because this
3268400	3274960	discussion has gotten out of hand and i'm like okay okay what are they saying needs to be established
3274960	3282480	what they say in this terrible editorial is no no no iq is real these measures of it are good
3282480	3289280	they are not racist and yes there are group level differences in iq where jews and asians are the
3289280	3293920	smartest but we don't know exactly how much and then you've got the white people centered around 100
3293920	3299040	and then they say but the black people are centered around 85 and like this is flat out what is in
3299040	3305600	that editorial that this group of researchers at microsoft decided to use as the basis for their
3305600	3311600	definition of intelligence so they can say yes gbt4 is the first steps on the way to artificial
3311600	3319360	intelligence using this definition so it's it's totally foundational and it is shocking to me
3319360	3327280	that nobody in that group of authors thought maybe we shouldn't be pointing to race science and just
3327280	3331680	like flat out racism posted in the wall street journal as the basis of what we're doing and
3331680	3336800	like the more charitable interpretation here is none of them actually read what they were citing
3337120	3346320	like that would be better than reading it going yeah this seems okay but you know it's eugenics
3346320	3353840	all the way down i mean so yeah how do they know that chat gbt is or that gbt4 is a is intelligent
3353840	3358080	then are they checking to see if it's jewish or or what are they like if that's what they're citing
3358720	3362880	and they're and they're citing a paper that says that you know asian people and jews are more
3362880	3367280	intelligent then they that's a pretty easy thing to tell they could just test if the ai
3367280	3373600	circumcised i'm sorry i'm a comedian i apologize the chatbot circumcised yeah
3376480	3384800	but so i mean in addition to the to the shoddy research though like what is it about these language
3384800	3391200	models that fail so profoundly like one one thing to me is that i keep coming back to and i even wish
3391280	3396720	i had put more clearly in my own youtube video on this subject is that no ai that we have has any
3396720	3401120	kind of like understanding that other minds exist you know like and that's like a foundational part
3401120	3406480	of intelligence as you and i are talking to each other you know us three and we each have our own
3406480	3410880	minds and they're interacting and they're communicating um and when you are communicating
3410880	3416400	with chat gbt you are imputing a mind to it it's almost impossible to use it without imagining
3416400	3420880	that there's a mind in there even though there isn't but it is not imagining a mind talking
3420880	3427040	back to it it's just chopping up words and phrases and uh you know like a self-driving car what is the
3427040	3431360	foundational problem with self-driving cars they can't communicate they can't make an eye contact
3431360	3435920	with another person and go you know i had a whole interaction with a car the other day where i was
3435920	3439680	like oh this car needs to pass i was walking in the street where there's there's no sidewalk this
3439680	3444400	car needs to pass me i'm gonna go stand where in the parking part you know where where the other
3444400	3448720	cars are parking and then i look back at the car it hasn't gone past i look back and the lady points
3448720	3453520	and she goes no actually i wanted to park there and i said oh now i'm in your way i need to go
3453520	3457440	get in the street because you were trying to use the parking lot you're trying to use the the the
3457440	3461920	shoulder there's no way for an ai to have a communication with a person like that to know
3461920	3467120	that there's a person with intent who i need to deal with in order to decide what the machine should
3467120	3473040	do um and that that seems to me to be like a fundamental extremely fundamental part of intelligence
3473120	3478800	that no level of hey let's make the language model better is ever going to accommodate
3478800	3484880	because it's it's all it is is a thing that you put words in one end and more come out the other
3484880	3489600	end i i imagine you might have more examples though of like what would actually constitute
3490160	3495520	intelligence that these fail at or maybe not like octopus octopi she has a whole paper on
3496160	3501840	on our yes yeah so so i have a paper from 2020 co-authored with alexander coller which is uh
3501840	3504960	has the octopus thought experiment which is why i'm wearing my octopus earrings here
3506400	3513440	purchased from an artist on etsy by the way um and um the what we were talking about there is um
3513440	3519120	basically showing it doesn't matter how intelligent the thing is it's not going to learn to understand
3519120	3523200	if all it has access to is the form of the language so to make that point we put together
3523200	3528640	this thought experiment with a hyper intelligent deep-sea octopus um and credit for it being an
3528640	3533280	octopus goes to my co-author alexander i was thinking dolphin and he's like no octopuses are
3533280	3537920	inherently funnier and also um that makes the environment more distinct from where the humans
3537920	3542400	are so hyper intelligent deep-sea octopus two humans stranded on two separate desert islands
3542400	3546560	that happen to be connected by a telegraph cable the humans figure this out and they start doing
3546560	3551840	morse code to each other english as encoded in morse code the octopus remember hyper intelligent
3551840	3556640	we're not doubting its intelligence taps into that cable and starts listening to the patterns
3556640	3561760	of the dots on the dashes and then after a while it cuts the cable and it starts sending dots and
3561760	3568080	dashes back based on the patterns that it's seeing and it can get away with this because a lot of
3568080	3572000	the communication is you know just sort of keeping each other company and so if something comes back
3572000	3577680	that's good enough right but then we have this point where the um uh one of the people on the
3577680	3582560	island says oh no i'm being chased by a bear because the thought experiment right spherical
3582560	3586720	cows and all that bear shows up on the desert island all i have are these two sticks what
3586720	3593360	am i going to do um and of course the the octopus can't provide anything useful because the octopus
3593360	3599760	hasn't understood has no model of the people's world even though we've posited it to be hyper
3599760	3604800	intelligent right so flipping that around to what people are seeing in the language models
3604800	3610160	our primary evidence such as it is that these things are intelligent is their apparent ability
3610160	3615680	to understand and create coherent text that's the only evidence that they're intelligent yeah
3615680	3622000	but in fact we know because of the octopus's thought experiment that it can't be that it's just
3622000	3627680	coming up with plausible next words something that looks like an answer um and so there's no
3627680	3633280	evidence for intelligence there at all now i frequently get asked by people okay emily so
3633840	3637680	what's the test that would convince you that one of these things is actually intelligent
3637760	3643600	twitch my answer is that's not my job i'm not trying to build one of these things why why do
3643600	3650160	we want to build those things that's what i don't understand who who wants to do that and what what
3650160	3656640	is it supposed to accomplish right a g i and then what like no more climate change like clean water
3657200	3662000	what i don't i don't understand the connection at all that's the weird thing because they say
3662000	3668560	it's coming we got to get ready we we need to be ready for it when it comes but it's like wait why
3669200	3674320	you're making it people are making it if the if if anyone's going to make it it's the people who
3674320	3680080	are telling you to be worried about it if anyone's going to create an agi and why are they what's the
3680080	3686320	purpose and so actually this leads me to a good question to end on um because uh if it occurs to
3686320	3691920	me a lot that look i love new technology i think chat gpt is super cool i played with it a ton it
3691920	3696400	's like the kind of technology i love to play with i love to play with see what i can what kind of
3696400	3702240	output i can get if i mess around with it if they had advertised it as this is a word calculator
3702240	3707360	here's what it does you put words in one end and it'll make the most it'll make a plausible
3707360	3713760	sounding answer to any question you ask it or you can you know it'll imitate any uh you know
3713760	3718880	you give it input and it'll imitate a plausible output that would have been really cool and they
3718880	3723840	could have come up with a lot of very plausible narrow uses for that such as computer programming
3723840	3730000	or other things of that nature but instead the industry made a marketing decision to say that
3730000	3736160	this was a step on the road to a super intelligent ai that we have to protect ourselves from and so
3736720	3744160	the question i keep wrestling with is why what was the purpose of misleading people of about
3744160	3749760	what the technology can do what were they trying to accomplish do you have any idea i that's the
3749760	3755200	paper i just so that's that's what we've been i personally you just wrote a paper on this oh
3755200	3759120	my god you're the perfect person to ask that question that's why i that's why i've been trying
3759120	3765200	to figure out like why when did people decide like we have to do a gi because when we're thinking
3765200	3770480	about large language models for instance i was telling emily uh i didn't have a problem with
3770480	3776080	like birth that was a large language model and i didn't really have a problem with you know them
3776080	3781840	being used in components to do various things whatever it was when opening i came in the scene
3781840	3787760	and started talking about these things like they are this huge super you know intelligent thing and
3787760	3792800	we're going to do a gi and we're gonna you know it's going to be like uh either amazing or utopia
3792800	3798320	or apocalypse and we have to focus on the future stuff that's what really started driving this whole
3798320	3805360	thing and our paper with emil which is like the one i was talking about was um you know tracing back
3805360	3812000	these test real ideologies back to first wave eugenesis and all of that it basically talks about
3812000	3817120	how this whole movement came about because you know so when we were talking about the connection
3817120	3823520	to eugenics it definitely was not theoretical it was for instance you know the chair of the
3823520	3831440	british eugenics society talking about transhumanism right transcending humanity and the cause the people
3831440	3838240	who first they call it they say christened the term agi in 2007 in a whole book that they wrote
3838240	3846000	the way they described it was transhuman agi you know they think that the agi will help humanity
3846000	3852720	transcend being human and become post-human colonize space you know and and live in like
3852720	3857840	digital mind so when you see sam altman's writings if you just read his blogs right now
3857840	3862320	he says we're gonna have unlimited energy and intelligence before the decade is out
3862320	3868800	he writes in his blog post that we have something by human flourishing and the cosmos the universe
3868800	3876720	you know and so that that was why i had to write the paper because it was like why and the the the
3876720	3882560	final conclusion was precisely what you were saying we were saying that you know there's been
3882560	3888240	ai winters and such because and at some point people doing various things like natural language
3888240	3894960	processing computer vision etc didn't call themselves ai whatever right we're just like oh i'm doing um
3894960	3900320	nlp i don't want to be associated with these ai weirdos who always talk about building a god
3900320	3905280	because they over promise like that and then you know people see through it and then it crashes
3905280	3911440	and then it comes back right and so now it's back and there's this whole agi thing and i really would
3911440	3920640	love to play this game where i ask people is this from like 1962 or 2022 right like we're like oh my
3920640	3925680	god you will be astonished to see what we have built you know and in the next 20 years people
3925680	3931120	will not have to work right and so this is what's going on and the thing is um as emily was saying
3931120	3937680	earlier that it is super aligned with this this super you mentioned Scientology i want them to
3937680	3942320	build the church of test grill you know ideologies or something like that it because it really is
3942320	3948080	like that um they have very much like religious characteristics of like the end of times kind
3948080	3954080	of things you know apocalyptic and utopian but it also is super aligned with corporate interests
3954080	3958880	right you know if you build this like one model that can do anything for anyone and everybody
3958880	3963840	just pays you and you can steal everybody's data and say that you're actually like saving humanity
3963840	3967520	and creating a god oh and you shouldn't be regulated because otherwise china is going to
3967520	3972960	do the devil kind and you don't want that you want us to do the good kind like it is super in line
3973760	3979840	with like corporate interests so yeah that's the conclusion that we've come uh to with our
3980640	3986320	paper that we're hoping to publish at some point after a peer review so it is corporate
3986320	3992960	interest but it also these the people who are doing this actually have a definite ideology of
3993520	4001520	100 of eugenics of that they they come from they write this they write this stuff yeah yeah and
4001520	4006400	unfortunately the money's behind them and so it's becoming everybody's problem instead of this niche
4006400	4012800	little research community that could just go be weirdos on their own yeah well how do you suggest
4012800	4019040	that you know for the public who's watching this and being flooded with misinformation with hype
4019040	4025840	about ai uh how can they gird themselves against it and you know like what what's the best way to
4025840	4030800	resist it and to think a little bit more critically the next time they're confronted with it so i
4030800	4035600	think i think we the public have a big job to do here around pushing for appropriate legislation
4035600	4041840	not the ai pause but something that actually is governance of collection of data and you know
4041840	4047200	synthetic media that is built through consultation with the people who are bearing the front of this
4047200	4051520	right now so the people who are being exploited in developing the systems the people whose data is
4051520	4057680	being stolen the people who are getting misinformation said about them and all of this so so developing
4057680	4064720	regulation collectively um but also resisting misinformation and the non-information so with
4064720	4069920	chat gpt being set up it looks to me that it's sort of like the um the oil spill in the Gulf of
4069920	4075600	Mexico when that um that the oil rig was broken and there was just oil going and going and going
4075600	4081680	right and BP was you know eventually saying look at all the birds we cleaned right um chat gpt is
4081680	4087440	polluting our information ecosystem in the same way with non-information and i've had people say to
4087440	4093440	me well you know hasn't the horse left the barn on that like it's out there and my answer to that is
4093440	4098240	we used to have lead in gasoline and we discovered that was bad news so we made some regulation
4098240	4104720	and now we don't like we don't just have to live with this we can regulate um and one thing that
4104720	4110400	i would love to see regulation lies like my my wish list item on this is uh corporations are
4110400	4116240	accountable for the actual literal output of their text synthesis machines it's libeling someone you
4116240	4120960	get sued for libel it's putting out bad medical information people get hurt you're liable i would
4120960	4125840	love to see it set up that way i don't know if that's something that works policy wise but that's
4125840	4131440	an idea in terms of just on an individual level how do you resist the ai hype um i think the questions
4131440	4137440	to ask are okay what's the actual task here what's the evidence that this machine is well matched to
4137440	4144480	that task how is it evaluated can i see the data that it was evaluated on and who's really benefiting
4144480	4150080	by using this system and who gets hurt um when it's wrong who gets hurt when it's right but people
4150080	4159200	you know are sort of using it um as a shortcut and so on that's a wonderful answer and uh i think i
4159280	4163440	think that last question is who is benefiting and who is actually getting hurt right now
4164000	4167840	is one of the most important questions we can always ask ourselves about the world but especially
4167840	4173120	in this case uh i can't thank both of you enough for coming on and and you're i mean you're just the
4173120	4178320	the perfect people to to speak to this topic and it was an honor to have you where can people
4178320	4183280	follow your work and and what's the most important thing of yours you think they should read is it
4183280	4189040	is it the on the dangers of stochastic pair parents your favorite your famous paper um that's
4189040	4193680	that's worth a read we are in the process of creating an an audio paper of that we've recorded
4193680	4198640	it and i have to edit it together i'm sorry i haven't done that i have to release a recording of our
4198640	4205360	event stochastic parents day yeah um so probably the best way to find me is probably my faculty
4205360	4208880	webpage at the university of washington and from there you can see links to everything i do in the
4208960	4215040	media my papers and stuff like that i am for the moment still on twitter and also on mastodon and
4215040	4220400	that's available through my web page so if you search emily bender university of washington
4220400	4228160	you'll find me and tim need how about you yeah um so dare dare institute website which is
4228160	4232800	not that much information right now but in a couple of days revamped we'll see much more
4232800	4238240	information there with a lot of our work and other things i'm also on twitter so if you want
4238320	4244960	to hear me rant i'm there but also a mastodon i'm a huge fan of the fediverse these days because i
4244960	4248800	i don't know i'm not worried that some random billionaire is going to take over that anytime
4248800	4256560	soon i ventured into linkedin and i'm trying to stay there but it's kind of difficult but i'm there
4256560	4262960	too awesome to me and emily thank you so much for coming on it's it's been a true honor to have you
4263920	4266160	thank you for having us and for raising these issues
4268320	4272240	well thank you once again to emily bender and tim need gebru for coming on the show i hope you
4272240	4276640	loved that conversation as much as i did and if you did i hope you will consider supporting the
4276640	4283040	show on patreon head to patreon.com slash adam conover and join our wonderful community including
4283040	4287040	folks who back this show at the fifteen dollar a month level and i'd love to read a couple of
4287040	4293440	your names we got hydrochloric victor densmore francis amadar kill me ink christina mendez akash
4293440	4299360	thakar frank f cling robin dumblap jeffery mcconnell nissy pods brian tobone leslie kokeshawn
4299360	4305200	garrison raghav kaushik olwiz sonny and ashley malini dias thank you folks so much for your
4305200	4309760	support and if you want to join him head to patreon.com slash adam conover to get every
4309760	4314640	episode of the show ad free and a bunch of other goodies as well we even do a live book club would
4314640	4320160	love to see you there i want to thank our producer sam rodman our engineer kyle magraw and the fine
4320160	4324080	folks at fucking northwest for building me a wonderful custom gaming pc that i record every
4324080	4329520	episode of the show on you can find me online at adam conover dot net you can find my tour dates at
4329520	4334960	adam conover dot net slash tour dates come see me do stand up all across the country and of course
4334960	4339840	you can find me on social media at adam conover wherever you get your social media thank you so
4339840	4345040	much for listening and we'll see you next time on factually
