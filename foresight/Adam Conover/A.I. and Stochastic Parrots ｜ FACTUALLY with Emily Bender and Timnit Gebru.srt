1
00:00:00,000 --> 00:00:06,240
Hello, and welcome to Factually. I'm Adam Conover. Thank you so much for joining me once again as I

2
00:00:06,240 --> 00:00:10,880
talked to an incredible expert about all the amazing things that they know that I don't know and that

3
00:00:10,880 --> 00:00:14,480
you might not know. Both of our minds are going to get blown together and we're going to have

4
00:00:14,480 --> 00:00:19,280
so much fun doing it. I want to remind you that if you are watching this podcast on YouTube,

5
00:00:19,280 --> 00:00:22,960
go subscribe to the podcast and your favorite podcast player if you want to hear it every week.

6
00:00:22,960 --> 00:00:27,680
If you're listening on your podcast player, go check out the video episode on YouTube. Now this

7
00:00:27,680 --> 00:00:34,400
week we're talking about AI. Just a few weeks ago, I released a YouTube video called AI is BS in

8
00:00:34,400 --> 00:00:40,480
which I argued that AI has become a marketing term that tech companies are using to hype up

9
00:00:40,480 --> 00:00:46,320
software that cannot do what they claim it does and in which in many cases could be dangerous if

10
00:00:46,320 --> 00:00:51,760
they jam it into mainstream products that it is not ready for. In fact, these companies are hyping

11
00:00:51,760 --> 00:00:57,280
up AI to such an extent that they're trying to convince people that software like chat GPT is

12
00:00:57,280 --> 00:01:03,600
a step on the road to some kind of godlike artificial general intelligence when in reality

13
00:01:03,600 --> 00:01:08,720
what they actually made is a text generator that can write some pretty cool fanfic and help you

14
00:01:08,720 --> 00:01:12,880
program and you know if they had marketed that way in the first place they said hey we made a tool

15
00:01:12,880 --> 00:01:17,840
that'll output a recipe that tastes bad if you try to cook it. I mean that would be pretty neat. We

16
00:01:17,840 --> 00:01:23,760
could think of a lot of uses for a text generator like that but a step on the road to true artificial

17
00:01:23,840 --> 00:01:30,160
intelligence it is not and it is kind of fucked up to tell people that it is. Now that video got a

18
00:01:30,160 --> 00:01:36,320
somewhat let's say divisive response because a lot of people on the internet have drunk the AI

19
00:01:36,320 --> 00:01:42,960
hype Kool-Aid. These tech companies have succeeded in confusing the issue of AI so much that a lot

20
00:01:42,960 --> 00:01:48,160
of the time when we say AI most of us don't even know what we're referring to. We don't understand

21
00:01:48,160 --> 00:01:53,360
how the software works and we don't understand how that's connected to the science fiction fantasies

22
00:01:53,440 --> 00:01:59,120
that the companies are peddling us and because of that confusion a lot of weird shit is happening.

23
00:01:59,120 --> 00:02:04,960
For instance while I was in the process of editing my video 1600 major AI researchers

24
00:02:04,960 --> 00:02:10,160
as well as people like Elon Musk and Steve Wozniak came out and asked for a six month

25
00:02:10,160 --> 00:02:15,040
pause on AI research but they didn't ask for that pause because of you know AI giving out

26
00:02:15,040 --> 00:02:19,760
misinformation or the fact that it just recycles the copyrighted work of artists like myself and

27
00:02:19,760 --> 00:02:24,480
others no they asked for that pause because they were worried that it could create an AI

28
00:02:24,480 --> 00:02:30,800
superintelligence again the bullshit hype science fiction claim so a bunch of other AI researchers

29
00:02:30,800 --> 00:02:35,440
came out against this letter saying that we do not have a problem with AI because of the super

30
00:02:35,440 --> 00:02:40,880
intelligence thing we have a problem because you are exploiting the work of real people and making

31
00:02:40,880 --> 00:02:46,560
the world a worse place right now so I don't blame the people in my comments for being confused

32
00:02:46,560 --> 00:02:52,480
even AI researchers themselves do not agree entirely on what the problems are so for that

33
00:02:52,480 --> 00:02:56,960
reason we are going to spend a couple episodes of this podcast talking to some of those AI

34
00:02:56,960 --> 00:03:02,160
researchers about what the problems are and how we might go about fixing them and on the show

35
00:03:02,160 --> 00:03:08,160
today we have two incredible guests their names are Emily Bender who's a professor at the University

36
00:03:08,160 --> 00:03:13,360
of Washington and Timney Gebru who's the executive director of the distributed artificial intelligence

37
00:03:13,360 --> 00:03:17,680
research institute and you might recognize the name Timney Gebru because she is the researcher who

38
00:03:17,680 --> 00:03:25,040
was famously fired by google for raising AI ethics concerns in that famous paper I am so excited to

39
00:03:25,040 --> 00:03:30,480
talk to them because they are two of the sharpest minds on AI and how the problems with it are not

40
00:03:30,480 --> 00:03:34,960
what the tech companies have been telling you but before we get to that interview I want to remind

41
00:03:34,960 --> 00:03:40,480
you that if you want to support this show please head to patreon.com slash adam conover you can

42
00:03:40,560 --> 00:03:46,400
get every episode of this podcast ad free and get a bunch of other goodies and even more importantly

43
00:03:46,400 --> 00:03:51,680
please come see me on tour this summer I'm taking my brand new hour of stand-up to San Francisco,

44
00:03:51,680 --> 00:03:57,360
San Antonio, Tempe, Arizona, Batavia, Illinois just outside Chicago, Baltimore, Maryland and St.

45
00:03:57,360 --> 00:04:03,280
Louis, Missouri head to adamconover.net for tickets come see me I'd love to give you a hug in the

46
00:04:03,280 --> 00:04:08,400
meet and greet line after the show and now without further ado let's get to my interview with Emily

47
00:04:08,400 --> 00:04:16,640
Bender and Timney Gebru Timney and Emily thank you so much for being on the show super happy to be here

48
00:04:17,280 --> 00:04:22,240
thank you for having me it's an honor to have both of you considering you know I've read your work

49
00:04:22,240 --> 00:04:28,720
I've talked about it in my last youtube video all about AI you're some of the foremost researchers

50
00:04:28,720 --> 00:04:34,480
on the topic some of the foremost critics of how the tech industry has been employing AI

51
00:04:34,480 --> 00:04:38,640
so I'd love to hear from you first of all Emily we last talked it was might have been close to a

52
00:04:38,640 --> 00:04:46,640
year ago back when AI was very much an active research subject we were hearing a lot about it

53
00:04:46,640 --> 00:04:52,240
but it wasn't something that the average person was using in the year since AI has become radically

54
00:04:52,240 --> 00:04:57,200
mainstreamed a lot of these companies have just shoved it into consumer products without you

55
00:04:57,200 --> 00:05:02,480
know any concern for what the results are as two as two people who follow the field extremely

56
00:05:02,480 --> 00:05:08,480
closely what has your reaction been to the last you know six months or so of rapid development in

57
00:05:08,480 --> 00:05:20,320
the industry blank faces here it's been a lot of oh come on again more seriously yeah that's

58
00:05:20,320 --> 00:05:29,760
that's how I feel I mean I just I am dumbfounded by the number of people I thought were more reasonable

59
00:05:29,760 --> 00:05:38,160
than this kind of jumping on a bandwagon of what seems to be mania so I don't know that's how I feel

60
00:05:39,840 --> 00:05:44,240
what do you think the greatest you know potential harms of this are in your paper your very famous

61
00:05:44,240 --> 00:05:49,200
paper on the dangers of stochastic parents you talked about many dangers that you know these would

62
00:05:50,000 --> 00:05:54,560
reify you know discriminatory materials in the training data by repeating it out to people that

63
00:05:54,560 --> 00:06:00,320
people would take it to literally you know would take the pronouncements of a large language model

64
00:06:00,320 --> 00:06:06,560
as fact uh a lot of those have seemed to come directly true do you feel validated that your

65
00:06:06,560 --> 00:06:13,600
that your criticisms have come to pass no no because those were predictions those were warnings

66
00:06:14,640 --> 00:06:20,000
like don't do it you know we don't want to get there and then we got there and then some right

67
00:06:20,080 --> 00:06:28,400
like I just I definitely don't feel validated I feel upset I'm sad about it um yeah how do you

68
00:06:28,400 --> 00:06:33,600
feel Emily you know I think that some of the biggest problems that maybe I didn't understand

69
00:06:33,600 --> 00:06:39,600
because it's it's sort of half an economic problem um is the way in which people would say hey this

70
00:06:39,600 --> 00:06:44,720
looks like it could be a robo lawyer this looks like it could be a robo therapist and look at all

71
00:06:44,720 --> 00:06:48,960
those people who can't afford real lawyers and real therapists so let's give them this instead

72
00:06:49,040 --> 00:06:55,440
and like that jump from um you've identified a real problem in the world it's a problem that

73
00:06:55,440 --> 00:06:59,760
mental health resources are inaccessible and it's a problem that legal representation is inaccessible

74
00:07:00,400 --> 00:07:07,200
but then try to fill that hole with something that is just a joke and can directly cause harm

75
00:07:07,200 --> 00:07:12,000
when deployed in those cases I think even when we were writing the paper and saying you know it

76
00:07:12,000 --> 00:07:16,080
would be bad if this was set up in such a place where people might believe it or believe it knew

77
00:07:16,080 --> 00:07:20,320
what it was talking about I don't think I was in a position to predict that that's a direction that

78
00:07:20,320 --> 00:07:26,000
it would go in at the time we wrote the paper I was just you know seeing this whole mind is bigger

79
00:07:26,000 --> 00:07:30,800
than yours kind of race and just being very confused why is this the thing that anybody

80
00:07:30,800 --> 00:07:36,560
everybody just wants to be the biggest one and now and now you have not just that text to text

81
00:07:37,280 --> 00:07:43,440
models but text to image text to video video or whatever you know and so I didn't I didn't imagine

82
00:07:43,440 --> 00:07:49,520
that in such a short time that kind of explosion of synthetic media into the world would happen

83
00:07:50,080 --> 00:07:57,280
and I also didn't think about I would say what the content moderation demands and issues would be

84
00:07:57,280 --> 00:08:04,320
with that much like explosion of synthetic media you know like the um the what is the fix

85
00:08:04,320 --> 00:08:09,920
Clark's world shutting down submissions because they got a little bit devious yeah stuff like that

86
00:08:09,920 --> 00:08:16,400
is something I didn't predict Emily you talked about just now the you know the sort of thing

87
00:08:16,400 --> 00:08:21,040
happened that happens a lot of technology once it's released people come up with new uses for it

88
00:08:21,040 --> 00:08:26,560
that nobody predicted and one of the uses that people have started you know uh do you talk about

89
00:08:26,560 --> 00:08:34,240
robo lawyers uh I've seen people say that they're using uh chatbots as therapists um or as relationship

90
00:08:34,240 --> 00:08:39,760
surrogates and what are the dangers of of those types of uses because I'm certainly seeing those

91
00:08:39,760 --> 00:08:45,600
promoted all over the there's a lot of folks saying hey if you don't have access to xyz and

92
00:08:45,600 --> 00:08:50,720
ai can do that for you in all sorts of fields and and you said that these are a joke what makes them

93
00:08:50,720 --> 00:08:57,040
a joke for that purpose so they're a joke because with they're all form and no content right so

94
00:08:57,040 --> 00:09:02,640
what these systems are really good at is mimicking the form or the style of something so it absolutely

95
00:09:02,640 --> 00:09:06,880
can write something that looks like a legal contract for you but if your purpose in drafting

96
00:09:06,880 --> 00:09:11,360
up a legal contract is anything other than intimidating the other party with legalese

97
00:09:11,360 --> 00:09:16,480
then the specific content and the way that it maps into your situation really matters yeah and it

98
00:09:16,480 --> 00:09:22,720
might be that there's some sort of template type situations where it's like okay yeah this is a

99
00:09:22,720 --> 00:09:30,720
contract for you know the rights to use a piece of music um and um I want this right assigned and

100
00:09:30,800 --> 00:09:34,560
that went not and it's going to be paid for this much and here's like you could answer a few questions

101
00:09:34,560 --> 00:09:38,960
and get something out from a template that would work reasonably well but that's not what they're

102
00:09:38,960 --> 00:09:44,400
doing right they're saying what's a plausible next word what's a plausible next word given this context

103
00:09:44,400 --> 00:09:48,960
and you know who knows where that's going to be um so for the the legal case you know you're asking

104
00:09:48,960 --> 00:09:52,960
for that because you are not a lawyer and you can't afford a lawyer you're not going to be in a position

105
00:09:52,960 --> 00:10:00,080
to tell if it's good or not but it'll look impressive right right it it sort of will can it

106
00:10:00,160 --> 00:10:06,160
will create a convincing imitation of a piece of text that will most readily convince someone

107
00:10:06,160 --> 00:10:12,240
who knows nothing about the field like a lot of you know I work in television writing and there's

108
00:10:12,240 --> 00:10:19,520
a lot of talk in you know oh can can studios use chat gpt to write scripts and when I use one of

109
00:10:19,520 --> 00:10:26,160
these services to uh write you know to output text I'm like yes this superficially looks like a script

110
00:10:26,160 --> 00:10:31,760
but it's missing so many of the things that you would need to film a script and someone might say

111
00:10:31,760 --> 00:10:37,760
well what if the technology gets better and it's not a matter of aping something even more correctly

112
00:10:37,760 --> 00:10:43,440
it's a matter of to successfully write a piece of screenwriting you need information about the

113
00:10:43,440 --> 00:10:50,160
rest of the world that a no algorithm or ai program could ever have you need to understand

114
00:10:50,160 --> 00:10:55,040
uh you know what is physically possible to produce you need to under you need to talk to

115
00:10:55,040 --> 00:11:00,400
a department full of people who say uh a very good example I use of this is I didn't realize until

116
00:11:00,400 --> 00:11:05,120
I started writing television that you can never have someone jump into a pool on television if you

117
00:11:05,120 --> 00:11:10,080
watch tv and someone jumps into a pool it'll always happen off camera and I had because I had a scene

118
00:11:10,080 --> 00:11:14,640
where I someone jumped into a pool my line producer told me they you need to remove that and I said

119
00:11:14,640 --> 00:11:19,200
why it seems not that hard they said because we need to film every take five times so that means

120
00:11:19,200 --> 00:11:23,280
we need five pairs of wardrobe because we need to film them one after another and we need to dry

121
00:11:23,280 --> 00:11:27,840
the person off and do their makeup and it's going to take all day and so as a result people never

122
00:11:27,840 --> 00:11:33,200
get wet on television or when they do it's very expensive okay interesting and you'd have no way

123
00:11:33,200 --> 00:11:38,400
of knowing that uh without real life experience and even if an ai could eventually figure that out

124
00:11:38,400 --> 00:11:41,840
there's also a million other things like that that are specific to the particular production

125
00:11:41,840 --> 00:11:45,680
hey it's going to be cloudy on wednesday we need you to rewrite the scene you know there's there's

126
00:11:45,680 --> 00:11:50,400
so many details that are that are fundamentally about humans communicating with each other

127
00:11:50,400 --> 00:11:56,560
and that's the same thing with with a lawyer doesn't just output text a lawyer talks to

128
00:11:56,560 --> 00:12:00,640
like knows what the other side might do in response knows how aggressive they'd be if you're

129
00:12:00,640 --> 00:12:05,920
trying to sue laws are yeah exactly if you're trying to evict a tenant they have they're going

130
00:12:05,920 --> 00:12:09,920
to have a much different response than if you're trying to sue the church of Scientology right

131
00:12:09,920 --> 00:12:17,440
who are very aggressive like and knowing what the laws are too so to me this is it's very obvious

132
00:12:17,440 --> 00:12:23,280
when i actually look at how they're used but it's it is it a problem with the technology or is it a

133
00:12:23,280 --> 00:12:29,280
problem with humans not understanding how our own society works to to not realize that these that

134
00:12:29,280 --> 00:12:35,440
these tools are going to be effective the hype too the hype yeah so it's a problem with the task

135
00:12:35,440 --> 00:12:41,200
technology fit so what is it that we need and how does the technology fit into it and and timmy

136
00:12:41,200 --> 00:12:45,760
and i want to bring up your wonderful line about how these things are unscoped technologies

137
00:12:45,840 --> 00:12:49,680
and then maybe you could elaborate on that a little bit yeah i mean i was going to bring up

138
00:12:49,680 --> 00:12:55,040
all your work on hype but which i think um and i really add them i mean it just like when i'm

139
00:12:55,040 --> 00:12:59,440
talking to you i'm like yeah we live in the same planet and you know we're having the same

140
00:12:59,440 --> 00:13:05,760
conversation in the same language i i am not that's not the language that we're speaking with the

141
00:13:05,760 --> 00:13:11,440
other researchers in ai or machine learning or whatever it is i am i'm so confused what's going

142
00:13:11,440 --> 00:13:19,520
on but because you know um scoping systems is a very basic engineering concept right when

143
00:13:19,520 --> 00:13:26,640
you're building something you want to know what you're building it for and then see if what you're

144
00:13:26,640 --> 00:13:31,280
building it for is actually being fulfilled whereas in this case the way they're advertising

145
00:13:31,280 --> 00:13:36,560
their systems is that they're building it to accomplish anything for everybody anywhere

146
00:13:36,560 --> 00:13:41,600
write code speak whatever language you know write scripts movie scripts protein folding

147
00:13:41,600 --> 00:13:45,840
whatever it is and that's a fundamentally unscoped system that i don't even know how

148
00:13:45,840 --> 00:13:52,560
we can make sure um can be safe or work um to add to the notion of being an unscoped system

149
00:13:52,560 --> 00:13:58,960
right which to me is a basic um engineering concept when you're trying to build something

150
00:13:58,960 --> 00:14:04,960
you ask what am i building uh to accomplish what are the tasks that i want to accomplish

151
00:14:05,040 --> 00:14:10,480
under which scenarios under which conditions and in this case when you see the kinds of things that

152
00:14:10,480 --> 00:14:17,520
they are um advertising um all of these companies metta talked about this um uh large language model

153
00:14:17,520 --> 00:14:24,000
based system they had called galacticon they said oh it's gonna write code just protein folding stuff

154
00:14:24,000 --> 00:14:30,320
and write scientific papers and more you know and with opening eyes chat gps like write movies

155
00:14:30,320 --> 00:14:35,200
replace artists do this do that and more you know and so already you just haven't

156
00:14:35,200 --> 00:14:39,920
have built a system that we don't even know what it's supposed to use for to be used for and how

157
00:14:39,920 --> 00:14:45,840
do we even test whether it is actually accomplishing its um task the task that it's supposed to be

158
00:14:45,840 --> 00:14:53,040
built for and one one problem here is that uh open ai is not at all open about how these things

159
00:14:53,040 --> 00:14:58,960
are trained and so not only is it not tested in specific contexts where you can say okay

160
00:14:58,960 --> 00:15:04,240
here here is the range of um here are the safety parameters of the system here's how well it has

161
00:15:04,240 --> 00:15:07,680
been tested to work in these contexts we don't have that information we also don't know what

162
00:15:07,680 --> 00:15:12,560
is training data or training regimen was and according to open ai this is somehow for safety

163
00:15:12,560 --> 00:15:16,960
which makes no sense at all because one of the very first things that was worked out about

164
00:15:17,760 --> 00:15:24,000
responsible development of these kinds of systems is provide documentation of the underlying dataset

165
00:15:24,000 --> 00:15:29,520
and of the parameters of sort of safe use of the model and that's kind of the first place that

166
00:15:29,520 --> 00:15:35,120
tim meet and i got to know each other independently right we were both working on this um separately

167
00:15:35,840 --> 00:15:41,440
yeah and then and then we connected through that sort of related work and that was you know really

168
00:15:41,440 --> 00:15:45,200
really fortunate it was 2017 i think there was just something in the air where a whole bunch of

169
00:15:45,200 --> 00:15:49,440
groups said we got to document these things so that we could figure out how we could use them

170
00:15:49,440 --> 00:15:54,560
and open ai while claiming to be doing this for safety is flat out refusing to do that

171
00:15:55,120 --> 00:15:59,840
and what's the danger of that if they are refusing to release the or make the model transparent

172
00:16:00,400 --> 00:16:05,520
so you can't make any decisions about whether it would be good to use the model or not if it's

173
00:16:05,520 --> 00:16:10,320
not transparent like let's say i want to use it for writing computer code well i can try it a few

174
00:16:10,320 --> 00:16:15,760
times and see if it seems to work well and then maybe get some confidence but i don't know what

175
00:16:15,760 --> 00:16:20,240
its training data looks like and for programming languages from what i've heard again they're not

176
00:16:20,240 --> 00:16:24,960
open about this but part of the training data is literally sort of english descriptions together

177
00:16:24,960 --> 00:16:30,720
with executable codes there's a lot of paired stuff in there that helps it do well a lot of the time

178
00:16:30,720 --> 00:16:34,720
the other thing about programming languages is that they are specifically designed to be unambiguous

179
00:16:35,520 --> 00:16:40,720
which is in stark contrast to natural languages where ambiguity is sort of a fundamental design

180
00:16:40,720 --> 00:16:48,080
feature everything is ambiguous and so the fact that it does well with this more constrained

181
00:16:48,080 --> 00:16:52,400
universe of programming languages kind of makes sense but again we can't really know because we

182
00:16:52,400 --> 00:16:56,560
don't know how it's trained but imagine like you're happy with this performance in helping you

183
00:16:56,560 --> 00:17:00,880
generate code and you've even got some like computer security buffs on your team and they

184
00:17:00,880 --> 00:17:05,520
look at that say yeah i don't see anything frightening coming out here but they keep changing

185
00:17:05,520 --> 00:17:10,080
the system and then all of a sudden it's maybe putting that into your code but there's no information

186
00:17:10,080 --> 00:17:14,320
about what version you're using you can't say no i want to keep using the version you know from

187
00:17:14,320 --> 00:17:20,800
december of 2022 because that's gone all you have is the open it uh the open ai um api is what i was

188
00:17:20,800 --> 00:17:25,520
trying to say that um allows you to connect with whatever they've put up for you to connect with

189
00:17:26,320 --> 00:17:32,800
and a lot of tools are being built on their api right now there's uh if you open the app store

190
00:17:32,800 --> 00:17:38,800
any kind of app store you'll find countless tools that are ai xyz you know ai help you write code

191
00:17:38,800 --> 00:17:45,600
ai help you write a movie script ai therapist and they're really just hooking into open ai's model

192
00:17:45,600 --> 00:17:51,840
and paying them a couple pennies per however many requests uh and people are now starting to use

193
00:17:51,840 --> 00:17:58,720
those tools to do real things without knowing what is where the the output is coming from or

194
00:17:58,720 --> 00:18:03,120
what the model is it does kind of remind me a little bit you're talking about models like i've

195
00:18:03,120 --> 00:18:07,760
talked to plenty of climatologists on the show and like climb you know climate models are a huge

196
00:18:07,760 --> 00:18:12,800
part of our understanding of how the climate works but we also know how those models work and we

197
00:18:12,800 --> 00:18:16,800
can compare them and we have a lot of information about them so that we know when they predict

198
00:18:16,800 --> 00:18:23,840
something you can go back to the what the source was but in this case it's both by design but also

199
00:18:23,840 --> 00:18:28,080
by corporate structure a black box because they're not telling us anything about it i was just

200
00:18:28,080 --> 00:18:32,640
thinking about something even more basic like how do we know we're not still they're not stealing

201
00:18:32,640 --> 00:18:39,600
people's work um to profit off of it so you know there were all these um lawsuits by artists to

202
00:18:39,600 --> 00:18:46,960
devian r i stability ai and mid journey right but not open ai not dally because we don't know

203
00:18:47,600 --> 00:18:52,320
what training data they were used so we don't know if they were copyright violations or not if they

204
00:18:52,320 --> 00:18:58,240
constantly you know compensated anybody versus not but they can do that right they there's nothing

205
00:18:58,240 --> 00:19:04,320
that is like preventing them from doing that right now yeah and it makes me wonder so look i can go to

206
00:19:04,320 --> 00:19:08,720
into chat gpt i talked about this in my youtube video i can go into chat gpt and say write an

207
00:19:08,720 --> 00:19:15,280
episode of adam ruins everything about xyz and it'll output something that looks like a pros

208
00:19:15,280 --> 00:19:20,800
version of adam ruins everything adam walks into the room and says blah blah blah about dogs and

209
00:19:20,800 --> 00:19:26,080
i'm like where is this coming from because i don't believe it has access to my shooting scripts

210
00:19:26,080 --> 00:19:30,080
because those are not public anywhere so i'm trying to figure out where is it getting the

211
00:19:30,080 --> 00:19:36,000
information from and again this is based on my own copyrighted work that i spent a lot of time

212
00:19:36,000 --> 00:19:40,560
putting together is the character i created and i would think that you know i would if it's being

213
00:19:40,560 --> 00:19:45,200
used for profit i would like to be paid for it in some respect um i think that's a pretty fundamental

214
00:19:45,200 --> 00:19:50,240
feature of our uh how capitalism is currently arranged and i would like it to follow those rules

215
00:19:50,240 --> 00:19:54,400
but it's difficult for me to tell i'm like it could just be getting it all from like fan fiction

216
00:19:54,400 --> 00:19:58,800
like i feel like it's really scraped like our archive archive of our own and all these big

217
00:19:58,800 --> 00:20:03,840
fan fiction sites um but it's really it's really really difficult to tell let's spend a second

218
00:20:03,840 --> 00:20:09,920
though and talk about open ai as an organization and the sort of ideology behind it because this is

219
00:20:09,920 --> 00:20:15,120
you know the organization that is most in the news pushing uh ai forward and it was incorporated

220
00:20:15,120 --> 00:20:20,080
originally as a nonprofit right and made a lot of noise about how the point is to make

221
00:20:20,080 --> 00:20:25,280
sure they're going to do it responsibly it's research it's not about profit but it has recently

222
00:20:25,280 --> 00:20:30,560
i believe changed its incorporation status to be a for-profit company completely changed its tune

223
00:20:31,120 --> 00:20:36,640
and they now say it wouldn't be safe if we were to open it up to all of you but meanwhile you've

224
00:20:36,640 --> 00:20:41,200
got the founder sam altman going on all sorts of podcasts talking to the news about how he has to

225
00:20:41,200 --> 00:20:45,680
do all this because ai is really really dangerous but then the dangers they're always talking about

226
00:20:45,760 --> 00:20:50,960
are always the science fiction kind where how the robot takes over the spaceship or you know from

227
00:20:50,960 --> 00:20:56,240
isek azimov kind of style of science fiction where a super powerful intelligence you know takes over

228
00:20:56,240 --> 00:21:00,160
the universe kind of thing they're never talking about the harms that we're talking about that

229
00:21:00,160 --> 00:21:04,160
people might use it to write a legal document it shouldn't write or that it might rip off somebody's

230
00:21:04,160 --> 00:21:10,160
copyrighted work or anything like that so what is your view of you know this this organization and

231
00:21:10,160 --> 00:21:15,680
it's uh supposed altruism is this just a faint to sort of trick us all into thinking that they

232
00:21:15,680 --> 00:21:21,680
have our best interests at heart or is as a corruption happened or what so this is the

233
00:21:21,680 --> 00:21:26,960
test grill question and i'm gonna let tim need to find test grill but just my take on this very

234
00:21:26,960 --> 00:21:33,440
quickly is i think they believe they are being altruistic and working in the best interests of

235
00:21:33,440 --> 00:21:42,240
people but their view of who counts as a person is very narrow and sort of leaves out of you all

236
00:21:42,240 --> 00:21:47,440
of the people who are being harmed now or just sees those harms as inconsequential compared to

237
00:21:47,440 --> 00:21:53,200
what they're worrying about which is in this science fiction universe it's hard to say the

238
00:21:53,200 --> 00:21:56,960
phrase science fiction fantasy because to me those are two genres of wonderful speculative

239
00:21:57,040 --> 00:22:06,400
fiction and you don't want to bucket this into that yeah you're just so i you know i can say so

240
00:22:06,400 --> 00:22:12,320
much about them i've been on them for a long time so in 2015 when they were announced i wrote a open

241
00:22:12,320 --> 00:22:18,480
letter that i didn't end up sending to anybody i just kept it to myself because i was a phd

242
00:22:18,480 --> 00:22:22,880
student back then and people were like uh people will know that it's you because i was so angry

243
00:22:23,440 --> 00:22:29,280
by the tone so i don't think it's a corruption or they've changed their tone or whatever to me

244
00:22:29,280 --> 00:22:37,440
they've like stayed exactly the same um and initially they um said they exactly they talked

245
00:22:37,440 --> 00:22:42,320
to they wrote you know they talked about it as if they were gonna save humanity peter teal and

246
00:22:42,320 --> 00:22:48,320
illa musk always as usual just on the ball to save humanity of course that's always what they've

247
00:22:48,400 --> 00:22:54,400
been doing in the world and um and all the whole media was talking about it like oh this

248
00:22:54,400 --> 00:23:00,560
nonprofit is starting they're gonna save humanity from ai because back then what happened is that

249
00:23:00,560 --> 00:23:06,560
they had invested in deep mind that they also wanted to create you know a gi artificial general

250
00:23:06,560 --> 00:23:12,160
intelligence which is a system that none of us know what it even is supposed to do this is the

251
00:23:12,160 --> 00:23:19,840
acronym for a for a super intelligent uh uh sounds like a god sounds like a god to me um and so

252
00:23:19,840 --> 00:23:25,680
they were all very much trying to develop this thing which i don't even know what it is and um

253
00:23:25,680 --> 00:23:31,760
deep you know invested in deep mind deep mind got bought by google 2015 the future of life institute

254
00:23:31,760 --> 00:23:38,640
had a similar letter asked to the one that we see the pause letter um and then they you know found

255
00:23:38,720 --> 00:23:43,680
open ai put bill you know hundreds of millions of dollars into it because they say they're gonna

256
00:23:43,680 --> 00:23:49,360
save humanity and all of that and create this agi thing fast forward right they realize they need

257
00:23:49,360 --> 00:23:55,840
a lot more money uh they're now essentially bought by um microsoft and then now they have a competition

258
00:23:55,840 --> 00:24:01,840
so they need to be closed and all of that so to me really it wasn't like a pivot or anything i

259
00:24:01,840 --> 00:24:07,120
never believed that they were gonna you know save humanity or anything like that and in terms of

260
00:24:07,120 --> 00:24:17,280
emily's um q about the test grill bundle um so you know i i have been really so irritated by the

261
00:24:17,280 --> 00:24:21,360
whole crew because i've been around them for a long time i went to school with some of them

262
00:24:21,360 --> 00:24:29,120
been around this you know agi community for a while so recently i teamed up with um a collaborator

263
00:24:29,120 --> 00:24:35,760
of ours um whose name is emil torrez who used to be a long termist and so a long termism is this

264
00:24:35,840 --> 00:24:41,520
weird you know the future of life institute behind the pause letter is a long termist institute and

265
00:24:41,520 --> 00:24:50,080
so they literally think that our um job as humans is to maximize the number of future humans who

266
00:24:50,080 --> 00:24:55,440
colonize space and digitally upload their minds and like live in the matrix kind of thing right

267
00:24:55,440 --> 00:25:02,480
this is a real thing like it's not an exaggeration that's what they want yeah i've read i've read a

268
00:25:02,480 --> 00:25:07,360
lot of that philosophy that you know the idea that we need to be thinking about how do we maximize

269
00:25:07,360 --> 00:25:13,920
the future uh happiness and well-being of humans 10 000 years from now if you could why save one

270
00:25:13,920 --> 00:25:18,960
life today when you could save 10 million lives uh 10 000 years from now now i'd say how the

271
00:25:18,960 --> 00:25:22,880
fuck do you know that what you're gonna do is gonna have any effect on people that far in the

272
00:25:22,880 --> 00:25:26,960
future it's the height of hubris to think that you can project that far into the future at all

273
00:25:26,960 --> 00:25:32,160
well they give you some random numbers they pull some number numbers out of their asses like oh

274
00:25:32,160 --> 00:25:37,440
my god we didn't know sam bankman freed was gonna be doing this but we will know what 10 000 years

275
00:25:37,440 --> 00:25:42,640
or now point zero zero one probability that you know what i mean it's absolutely ridiculous but

276
00:25:42,640 --> 00:25:49,120
anyhow so the test grill bundle is a bunch of ideologies that are all sort of descendants of

277
00:25:49,120 --> 00:25:54,800
the first wave eugenics um eugenics movement and you know when you hear this word about human

278
00:25:54,800 --> 00:26:01,360
flourishing maximizing our potential through both positive and negative eugenics positive would be

279
00:26:01,440 --> 00:26:05,040
the ones who are desirable you want them to breed you want them to you know

280
00:26:05,680 --> 00:26:10,640
multiply right and the ones who are negative the ones who are undesirable you want to kind of get

281
00:26:10,640 --> 00:26:16,960
rid of them because they don't help you with this human flourishing thing so the transhumanists

282
00:26:16,960 --> 00:26:22,160
you know were very much um that that ideology was very much developed by 20th century eugenics

283
00:26:22,800 --> 00:26:29,200
um and nick bostrom who is also a long-termist you know it's also very famous very prominent

284
00:26:29,200 --> 00:26:36,240
transhumanists right and so um we traced how these ideologies transhumanism extropianism the

285
00:26:36,240 --> 00:26:42,240
singularity people who say singularity is coming because of ai the cosmos who are actually the

286
00:26:42,240 --> 00:26:48,400
people who wrote the first book on age artificial general intelligence in 2007 the effective altruism

287
00:26:48,400 --> 00:26:54,240
the long-termists and how they're all in this circle kind of learning from each other networking

288
00:26:54,240 --> 00:27:00,560
with each other lots of money going into them and they're all sort of either selling agi utopia

289
00:27:00,560 --> 00:27:07,040
or agi apocalypse right if we do it right it's going to bring us utopia it is out human flourishing

290
00:27:07,040 --> 00:27:11,920
we need to do it if we do it wrong we're going to have an apocalypse because it's going to take

291
00:27:11,920 --> 00:27:18,240
over the world or china is going to do the devil kind of agi and you need to let us do this utopian

292
00:27:18,320 --> 00:27:25,120
kind because we're vanguards of humanity so it's obviously a very kind of convenient

293
00:27:25,680 --> 00:27:28,720
ideology for the billionaires because you know they're saying give us all the money

294
00:27:29,280 --> 00:27:34,080
we'll do the utopian kind um and but we're super worried about it because it might be

295
00:27:34,080 --> 00:27:39,360
super powerful but we're careful because you can trust us right and so sam altman is kind of is

296
00:27:39,360 --> 00:27:44,000
doing that thing right and to me he's in the same sort of camp as the future of life people

297
00:27:44,800 --> 00:27:50,400
because that's the same thing they're selling yeah i've the connection to eugenics is not

298
00:27:50,400 --> 00:27:54,640
theoretical i've i've seen it myself if you look at nick bostrom's writing and the writing of a lot

299
00:27:54,640 --> 00:28:01,120
of folks who write extensively about ai or agi the future you know super ai that could control the

300
00:28:01,120 --> 00:28:08,480
world um a lot they also write overtly about eugenics they have charts and tables about if we

301
00:28:08,480 --> 00:28:13,040
what if we started a human breeding program and only allowed people in the top percent

302
00:28:13,760 --> 00:28:17,840
of intelligence to breed and then they would have super babies and the babies would be super

303
00:28:17,840 --> 00:28:23,760
smart and it's like this was tried in the 40s in a country in europe you know this this is um

304
00:28:23,760 --> 00:28:28,400
this is very these are very old ideas by the way you can just look at a the interview i did a

305
00:28:28,400 --> 00:28:32,800
couple weeks ago about intelligence to to learn about whether intelligence is actually heritable

306
00:28:32,800 --> 00:28:39,440
in the in that way it's not um but so these the proximity of these ideas to each other is not

307
00:28:39,440 --> 00:28:45,360
theoretical these are you know the same folks promoting neo eugenics and promoting um ai

308
00:28:45,360 --> 00:28:50,960
catastrophism uh i want to refer though to this pause letter that you mentioned a couple times

309
00:28:50,960 --> 00:28:57,440
so that folks know what it is um a couple weeks ago actually as i was editing my ai video um a

310
00:28:57,440 --> 00:29:02,480
whole bunch of ai researchers from many many different organizations signed a letter suggesting

311
00:29:02,480 --> 00:29:08,560
a six month pause on ai after the release of gpt4 and they said well this is very dangerous

312
00:29:08,560 --> 00:29:16,000
we need to evaluate it um and etc etc and some folks who i you know i i have read and and enjoyed

313
00:29:16,000 --> 00:29:21,360
as ai researchers are signed to the letter um and it sounds on the face of it that that might rhyme

314
00:29:21,360 --> 00:29:26,320
with some of what you folks are saying um but you took objection to the letter and so i'd love

315
00:29:26,320 --> 00:29:31,840
a little bit of explanation from you about about exactly what your uh issue with that is like what

316
00:29:31,840 --> 00:29:36,400
what did that letter get wrong do we do you feel we need to pause or do we need to pause for a

317
00:29:36,400 --> 00:29:44,320
different reason or what so i think pause is unrealistic i think six months is unrealistic

318
00:29:44,320 --> 00:29:49,360
i think the letter makes it sound like these researchers are just now noticing that this

319
00:29:49,360 --> 00:29:55,680
might be harmful despite you know years and years and years of work of people saying hey there's

320
00:29:55,680 --> 00:30:02,560
harms here um and the letter itself is basically saying oh no we've built something too powerful

321
00:30:02,560 --> 00:30:08,480
better be careful so it's it's what livencel calls pretty good yeah we're too good gotta stop

322
00:30:08,480 --> 00:30:12,640
so it's it's basically helping to sell the technology um i have to say that i found out

323
00:30:12,640 --> 00:30:17,280
about the letter a little bit before it dropped because there was a um a journalist who contacted

324
00:30:17,280 --> 00:30:21,680
me asking if i was going to sign it and would i comment and i'm like haven't seen it not going

325
00:30:21,680 --> 00:30:27,120
to comment on what i haven't seen and then like i think later that day it came out and i was busy

326
00:30:27,120 --> 00:30:31,440
and then finally evening i sat down to read it and i thought oh god i have to i have to react to this

327
00:30:31,440 --> 00:30:39,600
so i put out a tweet thread um and then you know media you know craziness about it and so i said

328
00:30:39,600 --> 00:30:44,800
to tim neat and the other two listed authors of the stochastic parents paper let's put together

329
00:30:44,800 --> 00:30:49,600
a statement coming from us so that we can point the media at that for one thing but also to have

330
00:30:49,600 --> 00:30:54,400
sort of a joint statement here and and tim neat sort of took everybody's remarks including my

331
00:30:54,400 --> 00:30:58,960
relatively snarky twitter thread and like pulled together our first draft that we then worked on

332
00:30:58,960 --> 00:31:05,120
and where we start with that is with the observation that they cite us in their first footnote number

333
00:31:05,120 --> 00:31:10,720
one yeah they say your stochastic parents paper number one and number two is nick boston yeah

334
00:31:10,720 --> 00:31:15,760
wow oh wow okay they got they went all the way from alpha to omega there huh they're they're

335
00:31:15,760 --> 00:31:21,280
inciting everybody but they didn't ask you to sign the paper that's interesting oh they would know i

336
00:31:21,280 --> 00:31:27,760
would never yeah okay so their sentences ai systems with human competitive intelligence

337
00:31:27,760 --> 00:31:34,080
can pose profound risks to society and humanity as shown by extensive research footnote one

338
00:31:34,080 --> 00:31:39,280
and acknowledged by top ai labs footnote two and that footnote one has us embostrum and some other

339
00:31:39,280 --> 00:31:46,640
people but the stochastic parents paper was not a paper about ai systems with human competitive

340
00:31:46,640 --> 00:31:51,920
intelligence it was a paper about large language models which are not ai systems with human

341
00:31:52,000 --> 00:31:58,640
competitive intelligence as we note so many times in the paper like that was the thing we said yeah

342
00:31:59,680 --> 00:32:04,160
so like right off the bat it was just infuriating and there's like one or two things in here that

343
00:32:04,160 --> 00:32:12,080
i think do rhyme as you say so you know we need regulation and that regulation should involve

344
00:32:12,080 --> 00:32:17,840
things like watermarking so that we can tell when we're encountering synthetic media and

345
00:32:17,840 --> 00:32:22,960
um you know liability for ai caused harm that sounds good that liability should sit with the

346
00:32:22,960 --> 00:32:27,760
companies that are creating and deploying the ai they don't say that um but then there's a bunch

347
00:32:27,760 --> 00:32:32,240
of really weird stuff in here ai research and development should be focused on making today's

348
00:32:32,240 --> 00:32:39,920
powerful state-of-the-art systems more accurate safe interpretable transparent robust okay i'm

349
00:32:39,920 --> 00:32:47,680
all right with all those aligned is a keyword for this weird one trustworthy yeah but then the last

350
00:32:47,680 --> 00:32:57,200
one the last one is loyal a loyal loyal to who oh wait no no sorry they started doing Boy Scouts

351
00:32:57,200 --> 00:33:04,080
a scout is trustworthy brave reverent kind obedient or whatever i quit Boy Scouts when i was like 10

352
00:33:04,080 --> 00:33:07,760
but there's like a whole list of things in one of them's reverence so i'm surprised they didn't

353
00:33:07,760 --> 00:33:13,920
include reverence shouldn't it go to church yeah but here's the thing and ai i mean so

354
00:33:13,920 --> 00:33:18,400
there's the first question that timnit raises of okay loyal to whom whose interest is it serving

355
00:33:18,400 --> 00:33:23,840
but also these are not the kinds of things that can be loyal to be loyal is to experience

356
00:33:23,840 --> 00:33:29,840
certain feelings to have certain commitments and large language models are just text synthesis

357
00:33:29,840 --> 00:33:37,200
machines yeah so they predict what comes next or they're i hear a really great description i heard

358
00:33:37,200 --> 00:33:42,240
of them is to think of them as word calculators they they do a good job of you give it a bunch of

359
00:33:42,240 --> 00:33:46,480
words and it can turn them into other words that are derived from the first words and that can be

360
00:33:46,480 --> 00:33:50,400
a useful thing to do sometimes particularly if you're a computer programmer or someone else who

361
00:33:50,400 --> 00:33:57,600
like is manipulating text on that sort of level but that's not a it's it's not a thing that has

362
00:33:57,600 --> 00:34:05,200
an ethical drive such as loyalty yeah yeah exactly so so this this letter you know got a lot of

363
00:34:05,200 --> 00:34:11,200
attention partially because of who signed it and you know we've we've had so we we pushed back pretty

364
00:34:11,200 --> 00:34:17,920
quickly and then we were getting reactions like oh you're squandering the opportunity this is gary

365
00:34:17,920 --> 00:34:22,960
marcus complaining about us going out for blood coming out what is what did he say coming out

366
00:34:22,960 --> 00:34:28,160
they went for blood we went for blood or something like that or it's just like okay and sure and

367
00:34:28,880 --> 00:34:33,440
basically it's like they supposedly created an opportunity for regulation that would maybe get

368
00:34:33,440 --> 00:34:38,080
the six month pause whatever that means it's it's all completely unfounded right pause on systems

369
00:34:38,080 --> 00:34:43,920
more powerful than gpt4 well we don't have the specs on gpt4 so that's a unmeasurable undefined

370
00:34:43,920 --> 00:34:48,160
thing anyway um and as someone was pointing out and i'm sorry i don't have the source for this

371
00:34:48,160 --> 00:34:51,920
a lot of the work in creating new systems is actually in the data preparation

372
00:34:52,720 --> 00:34:57,920
and gathering these enormous amounts of data and a six months pause on training the systems

373
00:34:57,920 --> 00:35:02,240
wouldn't prevent anybody from going and collecting more data we need to prevent that in other ways

374
00:35:02,240 --> 00:35:07,600
to prevent data theft but that's a separate question right um and then you get people out

375
00:35:07,600 --> 00:35:14,240
there saying well why can't the so-called ai safety and ai ethics people get along so the ai

376
00:35:14,240 --> 00:35:18,800
safety people are the long-termists who want to prevent the agi from taking over the world

377
00:35:18,800 --> 00:35:22,560
and ai ethics is sometimes used to refer to the people who are concerned with the problems in

378
00:35:22,560 --> 00:35:28,800
the here and now in the ways that yeah they basically created the term ai ethic i'm ai safety

379
00:35:28,800 --> 00:35:33,520
to separate their themselves from us is how i feel about it because like we have the same

380
00:35:33,520 --> 00:35:38,720
technical expertise we have other expertise also but like it doesn't mean that you know

381
00:35:38,720 --> 00:35:45,280
so i feel like they they they named that that field or whatever it is to explicitly separate

382
00:35:45,280 --> 00:35:49,920
themselves from kind of our crew right yeah yeah so by answer to why can't we get along it's like

383
00:35:49,920 --> 00:35:55,680
well if why can't we find common cause if the ai safety people wanted to find common cause with

384
00:35:55,680 --> 00:36:00,800
those of us working in ethics they would cite us they would go to the you know to to tim needs work

385
00:36:00,800 --> 00:36:06,000
they would go to the work of syphia nobel and ruha benjamin and kathie o'neal two past guests on

386
00:36:06,000 --> 00:36:12,640
the show by the way just want to ding ding ding ding excellent um and you know build on that and

387
00:36:12,640 --> 00:36:17,600
lend some of their money and resources to making that happen but of course they don't want to

388
00:36:17,600 --> 00:36:22,480
because they're aligned with corporate interests and to really push back and to really reduce the

389
00:36:22,560 --> 00:36:28,960
harms here we need regulation that reigns in the corporations like why would ilan musk sign like

390
00:36:28,960 --> 00:36:35,600
everybody has to ask why does ilan musk an advisor uh end funder a future of life institute someone

391
00:36:35,600 --> 00:36:40,480
who pumped hundreds of millions of dollars into open ai and deep mind or whatever and whatever

392
00:36:40,480 --> 00:36:45,840
why is he so interested in like caution and whatever as long as it doesn't touch him sure

393
00:36:45,840 --> 00:36:51,120
you know if we're talking about regulating tesla or looking at the racial the largest racial

394
00:36:51,120 --> 00:36:56,480
discrimination um lost in history in california that's not what he wants us to talk about right

395
00:36:56,480 --> 00:37:01,120
like he doesn't want us to talk about any of those things and whatever he's doing with twitter

396
00:37:01,120 --> 00:37:06,480
we have to think about oh my god like this super powerful science fictiony thing that's going on

397
00:37:07,280 --> 00:37:12,640
and it's just so disappointing to see that number of people who went along with it i think for us

398
00:37:12,640 --> 00:37:20,720
we wanted to make sure we wanted to make it clear that we are not aligned with this vision

399
00:37:20,720 --> 00:37:27,920
of ai safety they hold eugenics roots um there's emily has a thing she always says always read

400
00:37:27,920 --> 00:37:33,840
the food notes we write the food notes and they have a food note that says you know if we don't

401
00:37:33,840 --> 00:37:42,480
do x y and z um we ai systems might have might be potentially catastrophic like other potentially

402
00:37:42,480 --> 00:37:47,200
catastrophic things like eugenics and we wanted to be like eugenics is not just potentially

403
00:37:47,200 --> 00:37:51,680
catastrophic it has been catastrophic you know what i mean so we just want to make sure that

404
00:37:51,680 --> 00:37:57,760
they can't they should not be able to launder um people's reputations to make themselves mainstream

405
00:37:57,760 --> 00:38:04,480
and appear reasonable i also think that there there's a huge number of unexamined assumptions

406
00:38:04,480 --> 00:38:09,840
in that letter that they are using the letter to promote to the public that are essentially myths

407
00:38:09,840 --> 00:38:14,960
about ai and i want to get into some of those and and ask you to react to them and maybe debunk

408
00:38:15,040 --> 00:38:18,800
them but we have to take a really quick break we'll be right back when we're emily bender and tim

409
00:38:18,800 --> 00:38:25,600
nick ebru folks friends family members you know you should be backing up your computer right you

410
00:38:25,600 --> 00:38:32,800
know that you are just one glitch or one macbook in the toilet away from losing all your beloved data

411
00:38:32,800 --> 00:38:37,920
and you're like maybe it's stored in the cloud somewhere but also you keep getting that little

412
00:38:37,920 --> 00:38:42,240
notification saying you're running out of space pay more money and you're like but even if i pay

413
00:38:42,240 --> 00:38:47,120
them more money it's still going to tell me i'm running out of space so is my stuff saved is it not

414
00:38:47,120 --> 00:38:53,200
saved i don't know i just hope you know i don't drop my laptop in a river all right i understand

415
00:38:53,200 --> 00:38:58,160
you i've been there before okay but i'm here to tell you there is a solution you can put those fears

416
00:38:58,160 --> 00:39:03,520
to rest for just seven bucks a month if you sign up for back plays now look i know a lot of podcast

417
00:39:03,520 --> 00:39:08,800
hosts will tell you that you use the product but in my case i actually do i have been a back place

418
00:39:08,800 --> 00:39:13,760
subscriber for years before even having the opportunity to read this ad when the folks at

419
00:39:13,760 --> 00:39:18,320
my podcast ad company asked me do you want to do a back plays ad i said hell yeah i do this is

420
00:39:18,320 --> 00:39:23,760
actually a good product that i really use here's how it works you install it on your computer it

421
00:39:23,760 --> 00:39:30,560
runs in the background and it uploads all of your stuff to their server encrypted okay you will never

422
00:39:30,560 --> 00:39:35,360
see a message on back plays that says you ran out of space no matter how big your hard drive is you

423
00:39:35,360 --> 00:39:40,400
got a 20 gig hard drive it'll back all that stuff up okay here's how it works you install it it runs

424
00:39:40,400 --> 00:39:47,280
in the background and without you even noticing it uploads all of your data and backs it up securely

425
00:39:47,280 --> 00:39:51,200
and with back plays you will never see that message saying you just ran out of space because you

426
00:39:51,200 --> 00:39:57,040
cannot run out of space with back plays it backs up all your stuff okay i've got like 20 terabytes

427
00:39:57,040 --> 00:40:02,080
of stuff on back plays and i sleep securely at night knowing that if something goes wrong i can

428
00:40:02,080 --> 00:40:06,240
get all that back you can restore it anyway you want you can download one file you can have them

429
00:40:06,240 --> 00:40:11,440
send you a zip of a bunch of files you can even get a hard drive from them in the mail with all

430
00:40:11,440 --> 00:40:17,440
your stuff on it so you can put it all on a new computer it is such a seamless and secure system

431
00:40:17,440 --> 00:40:22,720
i love using it and it helps me feel better and it'll help you feel better too they have restored

432
00:40:22,720 --> 00:40:28,960
55 billion files for their customers that's almost two trillion gigabytes that they currently

433
00:40:28,960 --> 00:40:33,920
have under storage and with their web panel you can restore your data anywhere in the world

434
00:40:33,920 --> 00:40:38,240
they've been recommended by the new york times mac world pc world lifewire but more importantly

435
00:40:38,240 --> 00:40:44,800
they're being recommended to you by me right now and get this they have a 15 day no credit card

436
00:40:44,800 --> 00:40:50,000
required free trial if you want to see how it works at backblaze.com slash factually that's

437
00:40:50,000 --> 00:40:54,000
plenty of time for you to upload some stuff downloaded figure out how the system works before

438
00:40:54,000 --> 00:40:59,600
you even pay them a penny they don't even ask for your credit card first all right so seriously

439
00:40:59,600 --> 00:41:06,320
back your stuff up check out backblaze.com slash factually the no hassle online backup it's a no

440
00:41:06,320 --> 00:41:12,640
brainer decision according to pc world and according to me adam conover that's backblaze.com slash factually

441
00:41:15,600 --> 00:41:20,080
okay we're back with emily bender and tim nick ebru so we were talking about the ai pause letter

442
00:41:20,080 --> 00:41:25,440
and i was starting to talk about how it seems to have a lot of assumptions built into it about

443
00:41:25,440 --> 00:41:30,720
how ai works and how it's going to progress that the people who wrote it and the people who found

444
00:41:30,720 --> 00:41:36,320
it open ai and the people in the tech industry have really pushed onto the public and i see those

445
00:41:36,320 --> 00:41:40,720
assumptions actually in my youtube comments i'll see them in the comments to this video when we post

446
00:41:40,720 --> 00:41:47,920
it on youtube um and in the comments to my last one people say well ai is progressing so quickly

447
00:41:47,920 --> 00:41:54,480
it's unstoppable it's progressing every single day and so this idea of the pause seems to like

448
00:41:54,480 --> 00:41:59,280
build you know connect to that idea where oh my god this is a runaway train and all we can do

449
00:41:59,280 --> 00:42:05,840
is try to steer it in a direction when you know we could be questioning like these are just humans

450
00:42:05,840 --> 00:42:13,200
like making these things like they can do whatever they want at any time um and a and b is it maybe

451
00:42:13,200 --> 00:42:17,360
not a foregone conclusion that it's going to progress in the direction that they say it will

452
00:42:17,360 --> 00:42:23,280
like it seems to me that the large language models are designed to make you think oh this is a step

453
00:42:23,280 --> 00:42:29,440
on the road to general intelligence to a literal thinking computer but uh and if you play with

454
00:42:29,440 --> 00:42:33,680
it for five minutes you might think that if you play with it for you know tens of hours as i have

455
00:42:33,680 --> 00:42:39,520
you stop thinking that and you realize it's just mashing text up um i'm curious if you know if we

456
00:42:39,520 --> 00:42:45,280
could dig into some of that is ai something that is constantly going to keep improving no matter

457
00:42:45,280 --> 00:42:48,800
what we do and we just need to like control it and make sure it's not going to destroy us

458
00:42:49,600 --> 00:42:56,320
so i have a lot to say on this good so there's a um wonderful uh explanation that comes from

459
00:42:56,320 --> 00:43:01,920
beth singler about how combination of like looking back in the in what's happened in science and

460
00:43:01,920 --> 00:43:08,240
technology to date combined with science fiction and imaginings of the future makes us think that

461
00:43:08,240 --> 00:43:13,920
there is a path that we are just racing along and it's only a question of how fast do we get

462
00:43:13,920 --> 00:43:20,080
there who's going to get there first and that's not how science happens right science is exploration

463
00:43:20,080 --> 00:43:24,880
it's communication it's choosing things to work on or not um i think there's some interesting

464
00:43:24,880 --> 00:43:30,000
stuff in the history of nuclear power and how the interests of building nuclear weapons shaped

465
00:43:30,000 --> 00:43:35,040
the decisions we made about what kind of nuclear power to work on for example um and you know it's

466
00:43:35,040 --> 00:43:40,880
all as you're saying choices that we can make and we don't really know what's possible in the future

467
00:43:41,440 --> 00:43:47,760
um but because of this idea of you know ai that's given to us from science fiction and

468
00:43:47,760 --> 00:43:53,680
to say i'm a huge fan of speculative fiction um but i mean for yeah it's cool um but i'm largely in it

469
00:43:53,680 --> 00:43:59,760
for the exploration of what happens to the human condition given these different settings like

470
00:43:59,760 --> 00:44:03,680
that's what i see the point of science fiction to be and a lot of this seems to come from this

471
00:44:03,680 --> 00:44:07,760
idea of no the point of science fiction is the cool spaceships and the teleport devices and the robots

472
00:44:08,320 --> 00:44:13,840
right and yeah those are cool but like that doesn't mean that it's going to exist so this notion

473
00:44:13,840 --> 00:44:20,400
of a path that we're just racing along as fast as we can is false and we don't have to buy it um

474
00:44:20,400 --> 00:44:27,280
and another part of it is when they say and you repeat um ai is just progressing that makes it

475
00:44:27,280 --> 00:44:33,840
sound like ai is doing it on its own and no what's happened is a lot of corporations and individual

476
00:44:33,840 --> 00:44:39,680
billionaires have put a lot of money into gathering big piles of data and doing some clever engineering

477
00:44:39,680 --> 00:44:45,040
about how to manage that data and then build these learning systems that compress it into

478
00:44:45,040 --> 00:44:51,200
something that can do the word calculator thing um and that happened quickly way more quickly than

479
00:44:51,200 --> 00:44:56,640
we thought it was like too many are both quite surprised by how fast this happened not because

480
00:44:57,280 --> 00:45:03,440
the tech got incredibly cool incredibly quickly but because it sort of got out into the world

481
00:45:03,520 --> 00:45:09,200
that quickly and what i see there isn't rapid scientific progress i see a lot of money and a

482
00:45:09,200 --> 00:45:13,760
lot of hype and all of a sudden someone's got the money to set up this thing so that anybody

483
00:45:13,760 --> 00:45:17,920
can access it apparently for free although every time you do that you're doing some work for open

484
00:45:17,920 --> 00:45:25,120
ai just by the way um so it's not really free but chat gpt is less a technological advance and

485
00:45:25,120 --> 00:45:30,560
more of a product that was created it was a way of taking something that already existed

486
00:45:30,560 --> 00:45:36,240
and opening it up to people and prompting it in a way to maximize the sort of public

487
00:45:36,240 --> 00:45:41,440
shock of it and to make people think that it was extremely capable and to sort of further

488
00:45:41,440 --> 00:45:47,680
this narrative that uh things are progressing so quickly but it's there's a little bit of a comparison

489
00:45:47,680 --> 00:45:52,640
to you know steve jobs invented the iphone well steve jobs didn't invent any technology

490
00:45:52,640 --> 00:45:56,800
he combined a lot of technologies some of which were invented by the federal government 30 years

491
00:45:56,800 --> 00:46:03,360
earlier um and like put them into a very well marketed product with a with a shiny wrapper

492
00:46:03,360 --> 00:46:07,520
and like a really nice clean store you could buy it in um there's maybe a little bit of a comparison

493
00:46:07,520 --> 00:46:13,920
there yeah yeah i think so um the the big thing with chat gpt the reason it just exploded all over

494
00:46:13,920 --> 00:46:19,360
the media was anybody could play with it which was a brilliant pr move on open ai's part because

495
00:46:19,360 --> 00:46:26,000
meant that everybody was doing their height for them right i mean i i would say you only need to

496
00:46:26,960 --> 00:46:33,600
try to talk play with it in any other language for like two minutes i like to agree it doesn't even

497
00:46:33,600 --> 00:46:39,120
it's complete gibberish you know what i mean so i'm like well i guess the agi speaks english we've

498
00:46:39,120 --> 00:46:46,160
already assumed that you know what i mean like but it's you know it's so crazy how many people um

499
00:46:46,160 --> 00:46:52,800
i've been talking to who are engineers and and researchers and they and and stuff say parrot

500
00:46:52,800 --> 00:46:58,880
exactly the talking points of open ai and you know anthropic and similar organizations that are

501
00:46:58,880 --> 00:47:04,880
making this point that everything is going to be built on top of it and it's going to trump

502
00:47:04,880 --> 00:47:13,200
like any other kind of development so most of gdp is going to be dependent on that and so

503
00:47:13,200 --> 00:47:20,720
whoever is not you know on top of whoever does not have that technology by like 2025 or something

504
00:47:20,720 --> 00:47:23,920
like that they're not going to be able to catch up because it's just going to be accelerating

505
00:47:23,920 --> 00:47:27,840
so fast you know what i mean this is the kind of stuff a lot of people are saying you hear this

506
00:47:27,840 --> 00:47:33,360
argument that in response to the pause letter you heard people say well if we pause china's

507
00:47:33,360 --> 00:47:38,160
going to keep making the ai and they're going to use it to kill us and like what are they going to

508
00:47:38,160 --> 00:47:43,600
use the devil kind yeah gpt for they're going to write more shitty fan fiction with it are they

509
00:47:43,600 --> 00:47:50,160
going to you know output more bad recipes like it's it's unclear what it's being presented as

510
00:47:50,160 --> 00:47:55,840
some threat to national security when the actual capabilities of these large language models

511
00:47:55,840 --> 00:48:01,440
they're cool they're very cool tech there's some cool stuff you can do with them but this is not

512
00:48:01,440 --> 00:48:07,280
launching nuclear warheads or like you know fight like waging national security battles

513
00:48:07,280 --> 00:48:12,240
but i'm sorry please continue your point now i was just that's basically it that's all i was

514
00:48:12,240 --> 00:48:17,200
going to say and it's that it's it's a few the really surprising thing to me and what's been a

515
00:48:17,200 --> 00:48:23,840
huge lesson i guess in history or current affairs whatever you want to call it is how few people

516
00:48:23,840 --> 00:48:31,200
can drive this is is really what is unbelievable to me few billionaires a few billionaires and a few

517
00:48:31,200 --> 00:48:38,640
people in in the space of deep learning together just can drive this entire thing the whole media

518
00:48:38,640 --> 00:48:44,080
eco chamber the chamber the whole research direction the entire you know silicon valley

519
00:48:44,080 --> 00:48:50,560
ecosystem and you know it's it's been extremely surprising to see that yeah and disappointing

520
00:48:50,560 --> 00:48:55,120
to see microsoft and google and like i've got criticisms of these corporations but they were

521
00:48:55,120 --> 00:49:01,760
pretty staid and stodgy especially microsoft like jumping on this um we should maybe talk about the

522
00:49:01,760 --> 00:49:05,920
sparks do you want to talk about the spark yeah i was gonna say i wanted to cue you to talk about

523
00:49:05,920 --> 00:49:11,200
the sparks of agi paper what's the sparks of agi anyway what is the sparks of agi paper this is

524
00:49:11,200 --> 00:49:19,680
fascinating i'm the host emily what is the sparks of agi paper this is a uh something that takes the

525
00:49:19,680 --> 00:49:24,000
form of a research paper it's not peer reviewed it was just thrown up on archive which is this place

526
00:49:24,000 --> 00:49:28,560
that was initially developed i think by physicists to help disseminate research faster and what's

527
00:49:28,560 --> 00:49:33,040
happened in machine learning and computer science more generally is that it's become this place to

528
00:49:33,040 --> 00:49:38,000
like put things up as if they were research papers and just bypass peer review entirely and so there's

529
00:49:38,000 --> 00:49:43,680
there's a whole problem over there so sparks of agi is one of those papers published by microsoft

530
00:49:43,680 --> 00:49:48,320
research a big group of people there including the head of research was an author author to air

531
00:49:48,320 --> 00:49:56,960
coordinates yeah did not notice that and it's um they took an intermediate version of gpt4 because

532
00:49:56,960 --> 00:50:02,160
you know microsoft is in bed with open air and this is this is microsoft can't be above the fray

533
00:50:02,160 --> 00:50:07,520
here they are they are part of this they've their funding was like ten billion dollars and then

534
00:50:08,240 --> 00:50:13,280
gpt4 is now driving the bing chat thing and remember that they have a search engine and it's called bing

535
00:50:13,920 --> 00:50:20,160
yeah right yeah and now it's and now it's got a super clippy embedded in it where it talks back

536
00:50:20,160 --> 00:50:26,880
to you yeah right super clippy um and so they have as researchers at microsoft access to

537
00:50:26,960 --> 00:50:32,160
a sort of an interim version of gpt4 and they use a whole bunch of these benchmarks on it

538
00:50:32,160 --> 00:50:36,400
that were developed by different people trying to test natural language processing systems

539
00:50:37,200 --> 00:50:43,520
generally without like really good construct validity that is what is this thing supposed to

540
00:50:43,520 --> 00:50:47,360
be testing and how do we know it's actually testing that especially given a large language model as

541
00:50:47,360 --> 00:50:53,280
the thing taking the test um and it's a 154 page thing where they they try gpt4 and all these things

542
00:50:53,280 --> 00:50:58,080
and they say yeah um you know it looks like we have the first sparks of artificial general

543
00:50:58,080 --> 00:51:05,600
intelligence here and that's that's what the paper is but it gets worse all right so um my

544
00:51:05,600 --> 00:51:12,960
first comment on seeing this was remember when you used to go to um microsoft for stodgy but

545
00:51:12,960 --> 00:51:21,120
basically functional software and the bookstore for science fiction well you know now we've got

546
00:51:21,120 --> 00:51:26,000
this like maybe it's like a fan fiction to gpt4 that's been published as if it were a research

547
00:51:26,000 --> 00:51:32,160
paper out of microsoft well so so what is so ludicrous about the idea that large language models

548
00:51:32,160 --> 00:51:38,640
are a step on the way to agi or or the sparks of agi because you know as you point out in the

549
00:51:38,640 --> 00:51:44,560
stochastic parrots paper it looks like agi to us right it passes if you want to loosely interpret

550
00:51:44,560 --> 00:51:49,040
the turing test right can it fool so can it fool a human into thinking they're talking to another

551
00:51:49,040 --> 00:51:54,880
human yes it can do that you could trick somebody using its output um and so for a lot of people

552
00:51:54,880 --> 00:51:59,680
that's what they were taught to believe is a step on the way to agi that's like the the version you

553
00:51:59,680 --> 00:52:07,760
learn in college um and so and it certainly seems that way to people um so what what is you know

554
00:52:07,760 --> 00:52:13,200
what are the barriers that stop it from being that can you start with the first page the first the

555
00:52:13,280 --> 00:52:20,080
first sentence yeah exactly paper um i have to get the paper up so that i can do that for you um

556
00:52:20,080 --> 00:52:27,280
but the the first problem with with it being the first steps to um to agi is that agi is undefined

557
00:52:27,280 --> 00:52:33,360
it's what tim meet was describing before as an unscoped technology so that's first steps to nowhere

558
00:52:33,360 --> 00:52:39,200
number one number two we know what a language model is it's a word calculator as you say it right

559
00:52:39,200 --> 00:52:46,160
so the fact that it seems to be giving us something coherent that's all us like gladly

560
00:52:46,160 --> 00:52:52,880
interpreting it as if it were inherent a coherent and nothing on the side of the actual system um

561
00:52:52,880 --> 00:53:00,000
so i was trying i'm like really trying hard to get you to talk about what they cite you know i'm

562
00:53:00,000 --> 00:53:08,080
i'm getting to it okay because it is so it is so atrocious and i have to get my um the i love

563
00:53:08,080 --> 00:53:14,000
how much fun you guys have with this roasting these papers is i i love it i love it when academics

564
00:53:14,000 --> 00:53:19,200
get spicy and you guys are delivering the goods that's right you know we just read the footnotes

565
00:53:19,200 --> 00:53:26,720
that's how we get spicy no but yeah it's always read the footnotes so so the um the pause letter

566
00:53:26,720 --> 00:53:33,040
by the way cites the sparks of agi this is one of its academic sources for the danger that's coming

567
00:53:33,040 --> 00:53:38,800
right but it's not peer reviewed and it's and it's fanfiction to a machine right all right so sentence

568
00:53:38,800 --> 00:53:44,480
one intelligence is a multifaceted and elusive concept that has long challenged psychologists

569
00:53:44,480 --> 00:53:50,320
philosophers and computer scientists sentence two is where it is to meet an attempt to capture its

570
00:53:50,320 --> 00:53:56,960
essence was made in 1994 by a group of 52 psychologists who signed on to a broad definition

571
00:53:56,960 --> 00:54:03,040
published in an editorial about the science of intelligence so i thought hmm let's go look at

572
00:54:03,040 --> 00:54:10,400
what this definition is and where this came from that editorial was published in reaction to the

573
00:54:10,400 --> 00:54:16,000
public outcry and discussion about the book called the bell curve do you remember this book

574
00:54:16,080 --> 00:54:21,840
by charles mary uh-huh yeah i remember this book this is a bunch of psychologists yeah now please

575
00:54:21,840 --> 00:54:28,400
go ahead no yeah a bunch of psychologists who are saying okay we've got to wait in here because this

576
00:54:28,400 --> 00:54:34,960
discussion has gotten out of hand and i'm like okay okay what are they saying needs to be established

577
00:54:34,960 --> 00:54:42,480
what they say in this terrible editorial is no no no iq is real these measures of it are good

578
00:54:42,480 --> 00:54:49,280
they are not racist and yes there are group level differences in iq where jews and asians are the

579
00:54:49,280 --> 00:54:53,920
smartest but we don't know exactly how much and then you've got the white people centered around 100

580
00:54:53,920 --> 00:54:59,040
and then they say but the black people are centered around 85 and like this is flat out what is in

581
00:54:59,040 --> 00:55:05,600
that editorial that this group of researchers at microsoft decided to use as the basis for their

582
00:55:05,600 --> 00:55:11,600
definition of intelligence so they can say yes gbt4 is the first steps on the way to artificial

583
00:55:11,600 --> 00:55:19,360
intelligence using this definition so it's it's totally foundational and it is shocking to me

584
00:55:19,360 --> 00:55:27,280
that nobody in that group of authors thought maybe we shouldn't be pointing to race science and just

585
00:55:27,280 --> 00:55:31,680
like flat out racism posted in the wall street journal as the basis of what we're doing and

586
00:55:31,680 --> 00:55:36,800
like the more charitable interpretation here is none of them actually read what they were citing

587
00:55:37,120 --> 00:55:46,320
like that would be better than reading it going yeah this seems okay but you know it's eugenics

588
00:55:46,320 --> 00:55:53,840
all the way down i mean so yeah how do they know that chat gbt is or that gbt4 is a is intelligent

589
00:55:53,840 --> 00:55:58,080
then are they checking to see if it's jewish or or what are they like if that's what they're citing

590
00:55:58,720 --> 00:56:02,880
and they're and they're citing a paper that says that you know asian people and jews are more

591
00:56:02,880 --> 00:56:07,280
intelligent then they that's a pretty easy thing to tell they could just test if the ai

592
00:56:07,280 --> 00:56:13,600
circumcised i'm sorry i'm a comedian i apologize the chatbot circumcised yeah

593
00:56:16,480 --> 00:56:24,800
but so i mean in addition to the to the shoddy research though like what is it about these language

594
00:56:24,800 --> 00:56:31,200
models that fail so profoundly like one one thing to me is that i keep coming back to and i even wish

595
00:56:31,280 --> 00:56:36,720
i had put more clearly in my own youtube video on this subject is that no ai that we have has any

596
00:56:36,720 --> 00:56:41,120
kind of like understanding that other minds exist you know like and that's like a foundational part

597
00:56:41,120 --> 00:56:46,480
of intelligence as you and i are talking to each other you know us three and we each have our own

598
00:56:46,480 --> 00:56:50,880
minds and they're interacting and they're communicating um and when you are communicating

599
00:56:50,880 --> 00:56:56,400
with chat gbt you are imputing a mind to it it's almost impossible to use it without imagining

600
00:56:56,400 --> 00:57:00,880
that there's a mind in there even though there isn't but it is not imagining a mind talking

601
00:57:00,880 --> 00:57:07,040
back to it it's just chopping up words and phrases and uh you know like a self-driving car what is the

602
00:57:07,040 --> 00:57:11,360
foundational problem with self-driving cars they can't communicate they can't make an eye contact

603
00:57:11,360 --> 00:57:15,920
with another person and go you know i had a whole interaction with a car the other day where i was

604
00:57:15,920 --> 00:57:19,680
like oh this car needs to pass i was walking in the street where there's there's no sidewalk this

605
00:57:19,680 --> 00:57:24,400
car needs to pass me i'm gonna go stand where in the parking part you know where where the other

606
00:57:24,400 --> 00:57:28,720
cars are parking and then i look back at the car it hasn't gone past i look back and the lady points

607
00:57:28,720 --> 00:57:33,520
and she goes no actually i wanted to park there and i said oh now i'm in your way i need to go

608
00:57:33,520 --> 00:57:37,440
get in the street because you were trying to use the parking lot you're trying to use the the the

609
00:57:37,440 --> 00:57:41,920
shoulder there's no way for an ai to have a communication with a person like that to know

610
00:57:41,920 --> 00:57:47,120
that there's a person with intent who i need to deal with in order to decide what the machine should

611
00:57:47,120 --> 00:57:53,040
do um and that that seems to me to be like a fundamental extremely fundamental part of intelligence

612
00:57:53,120 --> 00:57:58,800
that no level of hey let's make the language model better is ever going to accommodate

613
00:57:58,800 --> 00:58:04,880
because it's it's all it is is a thing that you put words in one end and more come out the other

614
00:58:04,880 --> 00:58:09,600
end i i imagine you might have more examples though of like what would actually constitute

615
00:58:10,160 --> 00:58:15,520
intelligence that these fail at or maybe not like octopus octopi she has a whole paper on

616
00:58:16,160 --> 00:58:21,840
on our yes yeah so so i have a paper from 2020 co-authored with alexander coller which is uh

617
00:58:21,840 --> 00:58:24,960
has the octopus thought experiment which is why i'm wearing my octopus earrings here

618
00:58:26,400 --> 00:58:33,440
purchased from an artist on etsy by the way um and um the what we were talking about there is um

619
00:58:33,440 --> 00:58:39,120
basically showing it doesn't matter how intelligent the thing is it's not going to learn to understand

620
00:58:39,120 --> 00:58:43,200
if all it has access to is the form of the language so to make that point we put together

621
00:58:43,200 --> 00:58:48,640
this thought experiment with a hyper intelligent deep-sea octopus um and credit for it being an

622
00:58:48,640 --> 00:58:53,280
octopus goes to my co-author alexander i was thinking dolphin and he's like no octopuses are

623
00:58:53,280 --> 00:58:57,920
inherently funnier and also um that makes the environment more distinct from where the humans

624
00:58:57,920 --> 00:59:02,400
are so hyper intelligent deep-sea octopus two humans stranded on two separate desert islands

625
00:59:02,400 --> 00:59:06,560
that happen to be connected by a telegraph cable the humans figure this out and they start doing

626
00:59:06,560 --> 00:59:11,840
morse code to each other english as encoded in morse code the octopus remember hyper intelligent

627
00:59:11,840 --> 00:59:16,640
we're not doubting its intelligence taps into that cable and starts listening to the patterns

628
00:59:16,640 --> 00:59:21,760
of the dots on the dashes and then after a while it cuts the cable and it starts sending dots and

629
00:59:21,760 --> 00:59:28,080
dashes back based on the patterns that it's seeing and it can get away with this because a lot of

630
00:59:28,080 --> 00:59:32,000
the communication is you know just sort of keeping each other company and so if something comes back

631
00:59:32,000 --> 00:59:37,680
that's good enough right but then we have this point where the um uh one of the people on the

632
00:59:37,680 --> 00:59:42,560
island says oh no i'm being chased by a bear because the thought experiment right spherical

633
00:59:42,560 --> 00:59:46,720
cows and all that bear shows up on the desert island all i have are these two sticks what

634
00:59:46,720 --> 00:59:53,360
am i going to do um and of course the the octopus can't provide anything useful because the octopus

635
00:59:53,360 --> 00:59:59,760
hasn't understood has no model of the people's world even though we've posited it to be hyper

636
00:59:59,760 --> 01:00:04,800
intelligent right so flipping that around to what people are seeing in the language models

637
01:00:04,800 --> 01:00:10,160
our primary evidence such as it is that these things are intelligent is their apparent ability

638
01:00:10,160 --> 01:00:15,680
to understand and create coherent text that's the only evidence that they're intelligent yeah

639
01:00:15,680 --> 01:00:22,000
but in fact we know because of the octopus's thought experiment that it can't be that it's just

640
01:00:22,000 --> 01:00:27,680
coming up with plausible next words something that looks like an answer um and so there's no

641
01:00:27,680 --> 01:00:33,280
evidence for intelligence there at all now i frequently get asked by people okay emily so

642
01:00:33,840 --> 01:00:37,680
what's the test that would convince you that one of these things is actually intelligent

643
01:00:37,760 --> 01:00:43,600
twitch my answer is that's not my job i'm not trying to build one of these things why why do

644
01:00:43,600 --> 01:00:50,160
we want to build those things that's what i don't understand who who wants to do that and what what

645
01:00:50,160 --> 01:00:56,640
is it supposed to accomplish right a g i and then what like no more climate change like clean water

646
01:00:57,200 --> 01:01:02,000
what i don't i don't understand the connection at all that's the weird thing because they say

647
01:01:02,000 --> 01:01:08,560
it's coming we got to get ready we we need to be ready for it when it comes but it's like wait why

648
01:01:09,200 --> 01:01:14,320
you're making it people are making it if the if if anyone's going to make it it's the people who

649
01:01:14,320 --> 01:01:20,080
are telling you to be worried about it if anyone's going to create an agi and why are they what's the

650
01:01:20,080 --> 01:01:26,320
purpose and so actually this leads me to a good question to end on um because uh if it occurs to

651
01:01:26,320 --> 01:01:31,920
me a lot that look i love new technology i think chat gpt is super cool i played with it a ton it

652
01:01:31,920 --> 01:01:36,400
's like the kind of technology i love to play with i love to play with see what i can what kind of

653
01:01:36,400 --> 01:01:42,240
output i can get if i mess around with it if they had advertised it as this is a word calculator

654
01:01:42,240 --> 01:01:47,360
here's what it does you put words in one end and it'll make the most it'll make a plausible

655
01:01:47,360 --> 01:01:53,760
sounding answer to any question you ask it or you can you know it'll imitate any uh you know

656
01:01:53,760 --> 01:01:58,880
you give it input and it'll imitate a plausible output that would have been really cool and they

657
01:01:58,880 --> 01:02:03,840
could have come up with a lot of very plausible narrow uses for that such as computer programming

658
01:02:03,840 --> 01:02:10,000
or other things of that nature but instead the industry made a marketing decision to say that

659
01:02:10,000 --> 01:02:16,160
this was a step on the road to a super intelligent ai that we have to protect ourselves from and so

660
01:02:16,720 --> 01:02:24,160
the question i keep wrestling with is why what was the purpose of misleading people of about

661
01:02:24,160 --> 01:02:29,760
what the technology can do what were they trying to accomplish do you have any idea i that's the

662
01:02:29,760 --> 01:02:35,200
paper i just so that's that's what we've been i personally you just wrote a paper on this oh

663
01:02:35,200 --> 01:02:39,120
my god you're the perfect person to ask that question that's why i that's why i've been trying

664
01:02:39,120 --> 01:02:45,200
to figure out like why when did people decide like we have to do a gi because when we're thinking

665
01:02:45,200 --> 01:02:50,480
about large language models for instance i was telling emily uh i didn't have a problem with

666
01:02:50,480 --> 01:02:56,080
like birth that was a large language model and i didn't really have a problem with you know them

667
01:02:56,080 --> 01:03:01,840
being used in components to do various things whatever it was when opening i came in the scene

668
01:03:01,840 --> 01:03:07,760
and started talking about these things like they are this huge super you know intelligent thing and

669
01:03:07,760 --> 01:03:12,800
we're going to do a gi and we're gonna you know it's going to be like uh either amazing or utopia

670
01:03:12,800 --> 01:03:18,320
or apocalypse and we have to focus on the future stuff that's what really started driving this whole

671
01:03:18,320 --> 01:03:25,360
thing and our paper with emil which is like the one i was talking about was um you know tracing back

672
01:03:25,360 --> 01:03:32,000
these test real ideologies back to first wave eugenesis and all of that it basically talks about

673
01:03:32,000 --> 01:03:37,120
how this whole movement came about because you know so when we were talking about the connection

674
01:03:37,120 --> 01:03:43,520
to eugenics it definitely was not theoretical it was for instance you know the chair of the

675
01:03:43,520 --> 01:03:51,440
british eugenics society talking about transhumanism right transcending humanity and the cause the people

676
01:03:51,440 --> 01:03:58,240
who first they call it they say christened the term agi in 2007 in a whole book that they wrote

677
01:03:58,240 --> 01:04:06,000
the way they described it was transhuman agi you know they think that the agi will help humanity

678
01:04:06,000 --> 01:04:12,720
transcend being human and become post-human colonize space you know and and live in like

679
01:04:12,720 --> 01:04:17,840
digital mind so when you see sam altman's writings if you just read his blogs right now

680
01:04:17,840 --> 01:04:22,320
he says we're gonna have unlimited energy and intelligence before the decade is out

681
01:04:22,320 --> 01:04:28,800
he writes in his blog post that we have something by human flourishing and the cosmos the universe

682
01:04:28,800 --> 01:04:36,720
you know and so that that was why i had to write the paper because it was like why and the the the

683
01:04:36,720 --> 01:04:42,560
final conclusion was precisely what you were saying we were saying that you know there's been

684
01:04:42,560 --> 01:04:48,240
ai winters and such because and at some point people doing various things like natural language

685
01:04:48,240 --> 01:04:54,960
processing computer vision etc didn't call themselves ai whatever right we're just like oh i'm doing um

686
01:04:54,960 --> 01:05:00,320
nlp i don't want to be associated with these ai weirdos who always talk about building a god

687
01:05:00,320 --> 01:05:05,280
because they over promise like that and then you know people see through it and then it crashes

688
01:05:05,280 --> 01:05:11,440
and then it comes back right and so now it's back and there's this whole agi thing and i really would

689
01:05:11,440 --> 01:05:20,640
love to play this game where i ask people is this from like 1962 or 2022 right like we're like oh my

690
01:05:20,640 --> 01:05:25,680
god you will be astonished to see what we have built you know and in the next 20 years people

691
01:05:25,680 --> 01:05:31,120
will not have to work right and so this is what's going on and the thing is um as emily was saying

692
01:05:31,120 --> 01:05:37,680
earlier that it is super aligned with this this super you mentioned Scientology i want them to

693
01:05:37,680 --> 01:05:42,320
build the church of test grill you know ideologies or something like that it because it really is

694
01:05:42,320 --> 01:05:48,080
like that um they have very much like religious characteristics of like the end of times kind

695
01:05:48,080 --> 01:05:54,080
of things you know apocalyptic and utopian but it also is super aligned with corporate interests

696
01:05:54,080 --> 01:05:58,880
right you know if you build this like one model that can do anything for anyone and everybody

697
01:05:58,880 --> 01:06:03,840
just pays you and you can steal everybody's data and say that you're actually like saving humanity

698
01:06:03,840 --> 01:06:07,520
and creating a god oh and you shouldn't be regulated because otherwise china is going to

699
01:06:07,520 --> 01:06:12,960
do the devil kind and you don't want that you want us to do the good kind like it is super in line

700
01:06:13,760 --> 01:06:19,840
with like corporate interests so yeah that's the conclusion that we've come uh to with our

701
01:06:20,640 --> 01:06:26,320
paper that we're hoping to publish at some point after a peer review so it is corporate

702
01:06:26,320 --> 01:06:32,960
interest but it also these the people who are doing this actually have a definite ideology of

703
01:06:33,520 --> 01:06:41,520
100 of eugenics of that they they come from they write this they write this stuff yeah yeah and

704
01:06:41,520 --> 01:06:46,400
unfortunately the money's behind them and so it's becoming everybody's problem instead of this niche

705
01:06:46,400 --> 01:06:52,800
little research community that could just go be weirdos on their own yeah well how do you suggest

706
01:06:52,800 --> 01:06:59,040
that you know for the public who's watching this and being flooded with misinformation with hype

707
01:06:59,040 --> 01:07:05,840
about ai uh how can they gird themselves against it and you know like what what's the best way to

708
01:07:05,840 --> 01:07:10,800
resist it and to think a little bit more critically the next time they're confronted with it so i

709
01:07:10,800 --> 01:07:15,600
think i think we the public have a big job to do here around pushing for appropriate legislation

710
01:07:15,600 --> 01:07:21,840
not the ai pause but something that actually is governance of collection of data and you know

711
01:07:21,840 --> 01:07:27,200
synthetic media that is built through consultation with the people who are bearing the front of this

712
01:07:27,200 --> 01:07:31,520
right now so the people who are being exploited in developing the systems the people whose data is

713
01:07:31,520 --> 01:07:37,680
being stolen the people who are getting misinformation said about them and all of this so so developing

714
01:07:37,680 --> 01:07:44,720
regulation collectively um but also resisting misinformation and the non-information so with

715
01:07:44,720 --> 01:07:49,920
chat gpt being set up it looks to me that it's sort of like the um the oil spill in the Gulf of

716
01:07:49,920 --> 01:07:55,600
Mexico when that um that the oil rig was broken and there was just oil going and going and going

717
01:07:55,600 --> 01:08:01,680
right and BP was you know eventually saying look at all the birds we cleaned right um chat gpt is

718
01:08:01,680 --> 01:08:07,440
polluting our information ecosystem in the same way with non-information and i've had people say to

719
01:08:07,440 --> 01:08:13,440
me well you know hasn't the horse left the barn on that like it's out there and my answer to that is

720
01:08:13,440 --> 01:08:18,240
we used to have lead in gasoline and we discovered that was bad news so we made some regulation

721
01:08:18,240 --> 01:08:24,720
and now we don't like we don't just have to live with this we can regulate um and one thing that

722
01:08:24,720 --> 01:08:30,400
i would love to see regulation lies like my my wish list item on this is uh corporations are

723
01:08:30,400 --> 01:08:36,240
accountable for the actual literal output of their text synthesis machines it's libeling someone you

724
01:08:36,240 --> 01:08:40,960
get sued for libel it's putting out bad medical information people get hurt you're liable i would

725
01:08:40,960 --> 01:08:45,840
love to see it set up that way i don't know if that's something that works policy wise but that's

726
01:08:45,840 --> 01:08:51,440
an idea in terms of just on an individual level how do you resist the ai hype um i think the questions

727
01:08:51,440 --> 01:08:57,440
to ask are okay what's the actual task here what's the evidence that this machine is well matched to

728
01:08:57,440 --> 01:09:04,480
that task how is it evaluated can i see the data that it was evaluated on and who's really benefiting

729
01:09:04,480 --> 01:09:10,080
by using this system and who gets hurt um when it's wrong who gets hurt when it's right but people

730
01:09:10,080 --> 01:09:19,200
you know are sort of using it um as a shortcut and so on that's a wonderful answer and uh i think i

731
01:09:19,280 --> 01:09:23,440
think that last question is who is benefiting and who is actually getting hurt right now

732
01:09:24,000 --> 01:09:27,840
is one of the most important questions we can always ask ourselves about the world but especially

733
01:09:27,840 --> 01:09:33,120
in this case uh i can't thank both of you enough for coming on and and you're i mean you're just the

734
01:09:33,120 --> 01:09:38,320
the perfect people to to speak to this topic and it was an honor to have you where can people

735
01:09:38,320 --> 01:09:43,280
follow your work and and what's the most important thing of yours you think they should read is it

736
01:09:43,280 --> 01:09:49,040
is it the on the dangers of stochastic pair parents your favorite your famous paper um that's

737
01:09:49,040 --> 01:09:53,680
that's worth a read we are in the process of creating an an audio paper of that we've recorded

738
01:09:53,680 --> 01:09:58,640
it and i have to edit it together i'm sorry i haven't done that i have to release a recording of our

739
01:09:58,640 --> 01:10:05,360
event stochastic parents day yeah um so probably the best way to find me is probably my faculty

740
01:10:05,360 --> 01:10:08,880
webpage at the university of washington and from there you can see links to everything i do in the

741
01:10:08,960 --> 01:10:15,040
media my papers and stuff like that i am for the moment still on twitter and also on mastodon and

742
01:10:15,040 --> 01:10:20,400
that's available through my web page so if you search emily bender university of washington

743
01:10:20,400 --> 01:10:28,160
you'll find me and tim need how about you yeah um so dare dare institute website which is

744
01:10:28,160 --> 01:10:32,800
not that much information right now but in a couple of days revamped we'll see much more

745
01:10:32,800 --> 01:10:38,240
information there with a lot of our work and other things i'm also on twitter so if you want

746
01:10:38,320 --> 01:10:44,960
to hear me rant i'm there but also a mastodon i'm a huge fan of the fediverse these days because i

747
01:10:44,960 --> 01:10:48,800
i don't know i'm not worried that some random billionaire is going to take over that anytime

748
01:10:48,800 --> 01:10:56,560
soon i ventured into linkedin and i'm trying to stay there but it's kind of difficult but i'm there

749
01:10:56,560 --> 01:11:02,960
too awesome to me and emily thank you so much for coming on it's it's been a true honor to have you

750
01:11:03,920 --> 01:11:06,160
thank you for having us and for raising these issues

751
01:11:08,320 --> 01:11:12,240
well thank you once again to emily bender and tim need gebru for coming on the show i hope you

752
01:11:12,240 --> 01:11:16,640
loved that conversation as much as i did and if you did i hope you will consider supporting the

753
01:11:16,640 --> 01:11:23,040
show on patreon head to patreon.com slash adam conover and join our wonderful community including

754
01:11:23,040 --> 01:11:27,040
folks who back this show at the fifteen dollar a month level and i'd love to read a couple of

755
01:11:27,040 --> 01:11:33,440
your names we got hydrochloric victor densmore francis amadar kill me ink christina mendez akash

756
01:11:33,440 --> 01:11:39,360
thakar frank f cling robin dumblap jeffery mcconnell nissy pods brian tobone leslie kokeshawn

757
01:11:39,360 --> 01:11:45,200
garrison raghav kaushik olwiz sonny and ashley malini dias thank you folks so much for your

758
01:11:45,200 --> 01:11:49,760
support and if you want to join him head to patreon.com slash adam conover to get every

759
01:11:49,760 --> 01:11:54,640
episode of the show ad free and a bunch of other goodies as well we even do a live book club would

760
01:11:54,640 --> 01:12:00,160
love to see you there i want to thank our producer sam rodman our engineer kyle magraw and the fine

761
01:12:00,160 --> 01:12:04,080
folks at fucking northwest for building me a wonderful custom gaming pc that i record every

762
01:12:04,080 --> 01:12:09,520
episode of the show on you can find me online at adam conover dot net you can find my tour dates at

763
01:12:09,520 --> 01:12:14,960
adam conover dot net slash tour dates come see me do stand up all across the country and of course

764
01:12:14,960 --> 01:12:19,840
you can find me on social media at adam conover wherever you get your social media thank you so

765
01:12:19,840 --> 01:12:25,040
much for listening and we'll see you next time on factually

