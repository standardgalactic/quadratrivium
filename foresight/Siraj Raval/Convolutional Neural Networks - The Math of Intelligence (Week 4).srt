1
00:00:00,000 --> 00:00:05,840
Hello world, it's Siraj and we're going to build a convolutional network using no libraries,

2
00:00:05,840 --> 00:00:11,520
I mean just NumPy, but no libraries, no TensorFlow, no PyTorch, none of it. We're going to look at

3
00:00:11,520 --> 00:00:17,040
the math behind it and we're going to build it with just NumPy for matrix math in Python.

4
00:00:17,040 --> 00:00:21,520
Okay and what it's going to be able to do, let me just start off with this demo to start off with,

5
00:00:21,520 --> 00:00:26,720
what it's going to be able to do is recognize any character that you type in or not type in but draw

6
00:00:26,800 --> 00:00:32,400
in with your mouse. So you could draw a six like that and then hit submit, it'll start working

7
00:00:33,040 --> 00:00:36,640
and then it'll say it's a six and then if you don't want to use a six you could say a letter like

8
00:00:36,640 --> 00:00:42,320
a any number or letter it's going to be able to detect slash predict. So it's going to be really

9
00:00:42,320 --> 00:00:48,080
cool because we basically were wrapping it into a web app using the Flask web framework. So it's

10
00:00:48,080 --> 00:00:51,680
going to be it's going to be super awesome. Okay so that's what we're going to do today

11
00:00:51,680 --> 00:00:56,160
and this is our first neural network that we're building in this course from scratch.

12
00:00:56,160 --> 00:01:01,040
I mean we made one in the weekly video but this is the real you know hardcore convolutional network

13
00:01:01,040 --> 00:01:06,640
with all the layers all the functions everything. Okay so let's start off with what it's inspired by.

14
00:01:06,640 --> 00:01:12,960
Well it's inspired by Jan Lacoon, the genius, no it's not. So Jan Lacoon is a director of AI at

15
00:01:12,960 --> 00:01:20,480
Facebook. He's a total G, he is awesome because he was inspired by these original two guys right here

16
00:01:21,360 --> 00:01:28,320
who published a paper in I think 68 or early 60s or 70s but the paper was on the mammalian

17
00:01:28,320 --> 00:01:33,280
visual cortex and the idea they had was and so here's a great image of it let me make it a lot

18
00:01:33,280 --> 00:01:40,800
bigger this has to be a lot bigger. So the idea they had was that mammals all see in a very similar

19
00:01:40,800 --> 00:01:46,400
way and that way is hierarchical. So you have a collection of cells and these cells are neurons

20
00:01:46,400 --> 00:01:51,520
and these cells cluster and and these clusters represent different features that are learned.

21
00:01:51,520 --> 00:01:57,360
Okay so here in terms of neuroscience they call these clusters v1 v2 you know they have names

22
00:01:57,360 --> 00:02:02,480
for all these clusters in the brain these clusters of neurons before it posterior all this

23
00:02:02,480 --> 00:02:07,440
neuroscience terminology but what we need to know is that at a high level what's happening is every

24
00:02:07,440 --> 00:02:15,680
time you see something a a series of clusters or layers of neurons are being activated whenever

25
00:02:15,680 --> 00:02:20,960
you see something whenever you detect something to be more accurate. So if I detect a dog or a

26
00:02:20,960 --> 00:02:26,640
you know face or whatever it's going to be a series of layers or clusters of neurons that fire

27
00:02:26,640 --> 00:02:31,840
and each of these clusters are going to detect a set of features okay and these features are going

28
00:02:31,840 --> 00:02:37,600
to be more abstract the the higher up the hierarchy of clusters you could think of it as a vertical

29
00:02:37,600 --> 00:02:42,160
hierarchy or even a horizontal hierarchy what it doesn't matter but the idea is that there is a

30
00:02:42,160 --> 00:02:47,520
hierarchy of features and at the start these features are very simple they're lines and edges

31
00:02:47,520 --> 00:02:53,120
but then they get more abstract then they become shapes and then they become more complex shapes

32
00:02:53,120 --> 00:02:58,240
and then eventually at the at the highest level at the highest cluster level exist the entire face

33
00:02:58,240 --> 00:03:05,680
or the entire dog or whatever it is and this is how the mammalian visual cortex worked and so

34
00:03:05,680 --> 00:03:12,240
what Yanlacun said and his team in 98 when they published probably the landmark paper of convolutional

35
00:03:12,240 --> 00:03:17,520
nets which is kind of arguable I guess because Khrtchevsky's ImageNet paper was pretty good in

36
00:03:18,080 --> 00:03:24,880
in I think 2012 but anyway Yanlacun is a G I just wanted to say that he had the idea to be inspired

37
00:03:24,880 --> 00:03:31,520
by three things three features of the human or the mammalian visual cortex local connections

38
00:03:31,520 --> 00:03:36,640
and that means the clusters between neurons how each neuron each set of neurons in a cluster

39
00:03:36,640 --> 00:03:41,120
cluster are connected to each other and they represent some set of features and then the

40
00:03:41,120 --> 00:03:48,240
idea of layering how these how there's a hierarchy of features that are learned and spatial invariance

41
00:03:48,240 --> 00:03:53,040
what does this mean this word spatial invariance it means that whenever you or I detect something

42
00:03:53,040 --> 00:03:58,080
whether it's let's say we're detecting a shoe right we if you see a shoe you know it's a shoe

43
00:03:58,080 --> 00:04:03,520
right if it's a Yeezy if it's uh you know Adidas whatever it is you know it's a shoe

44
00:04:03,520 --> 00:04:09,120
it could be shaped this way or this way it could be rotated or transformed no matter how it varies

45
00:04:09,920 --> 00:04:15,920
we still can detect that it's a shoe we know it's a shoe so we are it is the way its position is

46
00:04:15,920 --> 00:04:23,520
it's spatially invariant we can still detect what it is and so those three concepts were what inspired

47
00:04:23,520 --> 00:04:28,720
the birth of convolutional neural networks programmatic neural networks designed to mimic

48
00:04:28,720 --> 00:04:34,960
the mammalian visual cortex how cool is that that's so cool so how does this thing work

49
00:04:34,960 --> 00:04:40,480
let's look at how this works so we have a set of layers okay and we'll talk about what these layers

50
00:04:40,480 --> 00:04:48,480
mean right what is layer a layer in each case is a series it's a series of operations that we're

51
00:04:49,040 --> 00:04:53,360
okay so let's let's talk about this right so we have some input image so let's see

52
00:04:54,080 --> 00:04:58,480
this is the orange that's the image and you'll notice by the way that this image this is a

53
00:04:58,480 --> 00:05:02,960
convolutional network by the way this is what we're building okay you'll notice that this image

54
00:05:02,960 --> 00:05:07,760
right here or this image of the convolutional network isn't what you normally look at when

55
00:05:07,760 --> 00:05:11,280
you think of neural network right you always see that image of the circles and everything's

56
00:05:11,280 --> 00:05:17,520
connected so why is it different for convolutional networks because every layer in a convolutional

57
00:05:17,520 --> 00:05:24,080
network isn't connected to every so every neuron in every layer isn't connected to every other neuron

58
00:05:24,080 --> 00:05:30,480
in the next layer why because that would be too computationally expensive i'll go over that in

59
00:05:30,480 --> 00:05:35,360
a second but the idea is that if you if you see here there is a part of the image that is connected

60
00:05:35,360 --> 00:05:40,880
it's this little square of that orange and that is called the receptive field okay i'm going to go

61
00:05:40,880 --> 00:05:43,840
over all this it's going to make more and more sense you're going to be more confused it's going

62
00:05:44,160 --> 00:05:48,800
it's going to make more and more sense as as i go further and further in depth here so so so stay

63
00:05:48,800 --> 00:05:54,880
with me here so we have a receptive field okay that is some part of the image that we are focused on

64
00:05:54,880 --> 00:06:00,320
we are by focused i mean that is the part of the image that we apply a convolution operation to

65
00:06:00,960 --> 00:06:06,320
okay and we take that receptive field and we slide it across the image okay you're going to see

66
00:06:06,320 --> 00:06:10,720
exactly what i'm talking about in a second i'm just going to go over at a high level we slide

67
00:06:10,720 --> 00:06:16,480
over the image we are applying a dot product between our weight matrix at a layer and every

68
00:06:16,480 --> 00:06:22,160
part of that image iteratively okay and so that the reason that they look different the convolutional

69
00:06:22,160 --> 00:06:28,000
networks look different is two reasons really the first reason is that not every neuron in each

70
00:06:28,000 --> 00:06:32,720
layer is connected to every other neuron in the next layer it's only a part of that because it

71
00:06:32,720 --> 00:06:39,040
would be a to borrow from discrete math a combinatorial explosion to connect every single pixel

72
00:06:39,040 --> 00:06:45,040
value in an image to every single pixel value in the next layer of features right it would be

73
00:06:45,040 --> 00:06:50,880
just a huge amount so what we do instead is we take a part of that image and we iteratively slide

74
00:06:50,880 --> 00:06:55,360
over it okay so at a high level you understand the sliding part right think of it as a flashlight

75
00:06:55,360 --> 00:07:00,560
okay think of it think of the uh the filter at each layer that shines over the receptive field

76
00:07:00,560 --> 00:07:06,080
that box as a flashlight and you're shining over the image and you're and you're applying dot products

77
00:07:06,080 --> 00:07:10,800
to all of these numbers okay just like that okay i'm going to keep going into this that was just

78
00:07:10,800 --> 00:07:14,080
the highest level you're not supposed to understand it all yet okay that was that was very high level

79
00:07:14,080 --> 00:07:18,800
we're still going deeper we're going deep we're going deep okay so check out this beautiful image

80
00:07:18,800 --> 00:07:22,880
right here isn't it beautiful it's very beautiful also you're beautiful for watching this so thank

81
00:07:22,880 --> 00:07:29,280
you for watching this okay so i love my fans so much seriously you guys are amazing seriously

82
00:07:29,280 --> 00:07:35,280
you guys are the reason i do this every week okay so i by the way i want to say one more thing

83
00:07:35,280 --> 00:07:41,120
to go on a tangent the people who subscribe to my channel no one thought they existed we are

84
00:07:41,120 --> 00:07:47,680
programmers who are smart and we are also cool no one thought these people existed but we exist

85
00:07:47,680 --> 00:07:53,760
okay we are smart and we are cool so you are amazing okay anyway back to this what this is

86
00:07:53,760 --> 00:07:57,120
is another way of looking at the network right we're just looking at different ways we're looking

87
00:07:57,120 --> 00:08:03,120
at different ways so we can build a spatially invariant image in our head of what a convolutional

88
00:08:03,120 --> 00:08:08,800
network is like right no matter what that image is we're going to learn to recognize a convolutional

89
00:08:08,800 --> 00:08:13,440
network when we see one i'm just trying to you know meta applying this logic to what we're learning

90
00:08:13,440 --> 00:08:18,640
so what happens is that each layer we are applying a series of dot products between the

91
00:08:18,640 --> 00:08:25,040
weight matrices and the input matrix okay and so what happens is let's look at a third image okay

92
00:08:25,040 --> 00:08:30,960
so this is a third image what happens is we perform a series of operations okay at each layer

93
00:08:31,040 --> 00:08:36,000
and so we could think of of different we could think of splitting up a convolutional network

94
00:08:36,000 --> 00:08:41,760
into two separate categories the first category is feature learning and that's what's happening at the

95
00:08:41,760 --> 00:08:48,000
at the at the head of the the head to the middle to almost the tail end of the network and at the

96
00:08:48,000 --> 00:08:53,200
very tail end is classification so there's two parts there's the feature learning part and then

97
00:08:53,200 --> 00:08:58,240
there's the classification part and so for the feature learning part what happens are three

98
00:08:58,240 --> 00:09:04,480
operations over and over and over again and we can call them convolutional blocks let's just call

99
00:09:04,480 --> 00:09:10,080
them convolutional blocks i'm coining the term so what happens is we first apply convolution

100
00:09:10,080 --> 00:09:17,040
then we apply relu or any kind of activation and then we apply pooling and we repeat that that's

101
00:09:17,040 --> 00:09:23,360
that's a single block three operations in a single convolutional block okay so convolution relu pooling

102
00:09:23,360 --> 00:09:29,360
repeat convolution relu pooling repeat convolution relu pooling okay and usually you know you have

103
00:09:29,360 --> 00:09:34,560
three blocks at least unless you're building inception by google then you have 15 15 of these

104
00:09:35,440 --> 00:09:39,520
but you you know you have these convolutional blocks and at the very end then you flatten

105
00:09:39,520 --> 00:09:45,680
that output into a smaller dimensional vector and then you apply a fully connected layer to it so

106
00:09:45,680 --> 00:09:50,880
that means that you then connect all the neurons in one layer to the next one just because we want to

107
00:09:50,880 --> 00:09:56,160
then harness all of the learnings that we've learned so far that's why we fully connect at the end

108
00:09:56,160 --> 00:10:02,080
and then we take those learnings and we squash it into a set of probability values with our last

109
00:10:02,080 --> 00:10:07,600
softmax function and then we take the max value of those probabilities and each of these probabilities

110
00:10:07,600 --> 00:10:13,440
is a probability for a for specific class that it could be and we take the max value let's say 72

111
00:10:13,440 --> 00:10:19,440
percent as and we'll say okay well 72 percent for banana and now we know it's a banana okay so

112
00:10:20,160 --> 00:10:24,480
hopefully you get some of it but it's very confusing still i know we're about to go even

113
00:10:24,480 --> 00:10:29,760
deeper okay so get ready for this i haven't even started yet so i haven't even started yet okay so

114
00:10:29,760 --> 00:10:37,040
anyway step one so for step one we are preparing a data set of images right so when you think of an

115
00:10:37,040 --> 00:10:42,240
image you think of a matrix hopefully a matrix of pixel values if you don't think of it that way

116
00:10:42,240 --> 00:10:46,960
think of it think of it that way now you're thinking of an image as a matrix of pixel values

117
00:10:46,960 --> 00:10:50,640
rowed by columns and each of these um each of these uh

118
00:10:53,040 --> 00:11:00,080
points in the matrix represent a pixel right between 0 and 255 but it's actually better

119
00:11:00,080 --> 00:11:04,560
in terms of convolutional networks to think of an an image as a three-dimensional matrix

120
00:11:04,560 --> 00:11:09,600
and you're like what no what it's no so it's three dimensions so the first dimension is the length

121
00:11:09,600 --> 00:11:14,320
of the image the second dimension is the width and the third dimension is the depth so wait what

122
00:11:14,320 --> 00:11:20,080
is the depth because the depth represents the channels and there are three channels for images

123
00:11:20,080 --> 00:11:24,320
red green and blue unless you're talking about gray scale then there's black then there's you

124
00:11:24,320 --> 00:11:28,480
know black and white but we're talking about color images okay so there are three channels and you

125
00:11:28,480 --> 00:11:34,560
have these dimensions for each of the channels so these values in each of these um in each of these

126
00:11:34,560 --> 00:11:42,720
2d matrices for and there are three of them represent the the amount of redness or the amount of

127
00:11:42,800 --> 00:11:48,560
greenness or the amount of blueness between 0 and 255 so in terms of convolutional nets we think of

128
00:11:48,560 --> 00:11:53,600
images as three-dimensional pixels okay so i wanted to say that part okay so that's that's

129
00:11:53,600 --> 00:11:59,200
that's what we think of our image as our input image and it has an associated label right we're

130
00:11:59,200 --> 00:12:03,360
talking about supervised learning learning the mapping between the input data and the output

131
00:12:03,360 --> 00:12:08,880
label dog image dog label learn the mapping given a new dog image what is a label well you just

132
00:12:08,880 --> 00:12:14,320
learned it right so and we learn it through back propagation back propagate to update

133
00:12:14,320 --> 00:12:19,280
weights remember the rhyme you know what it is hey i haven't wrapped yet in the series but i will

134
00:12:19,280 --> 00:12:25,360
don't worry it's coming anyway so every image is a matrix of pixel values we know this we know this

135
00:12:25,360 --> 00:12:33,360
between 0 and 255 and we can use several training data sets there are two really popular ones there's

136
00:12:33,440 --> 00:12:38,400
seafar and there's cocoa and there's a bunch of other ones as well but basically these are

137
00:12:38,400 --> 00:12:44,000
huge data sets and you can find smaller versions of them and each of these images they're dogs

138
00:12:44,000 --> 00:12:49,200
they're cars they're airplanes they're people whatever they all have labels for them handmade

139
00:12:49,200 --> 00:12:54,960
labels by humans which is great for us okay so that's that's it that's step one step one is to

140
00:12:54,960 --> 00:13:00,160
get your training data which is your images which are your images step two is to perform convolution

141
00:13:00,160 --> 00:13:05,360
now you might be asking what is convolution well i'm here to tell you that convolution is an

142
00:13:05,360 --> 00:13:10,800
operation that is dope as f here's why it's dope because it's not just used in computer science

143
00:13:10,800 --> 00:13:15,680
and machine learning it's used in almost every field of engineering think of convolution as two

144
00:13:15,680 --> 00:13:20,400
paint buckets you have one paint bucket which is red another one which is blue and what you do

145
00:13:20,400 --> 00:13:24,720
is just smear it all over yourself no you don't do that what you do is you take these two paint

146
00:13:24,720 --> 00:13:29,360
buckets and you combine them into one paint bucket and that new paint bucket is going to be a new

147
00:13:29,360 --> 00:13:36,160
color whatever that combination of colors is that's convolution convolution is taking two separate

148
00:13:36,160 --> 00:13:42,800
types of data or two matrices and then apply and then it's an operation that combines them so you

149
00:13:42,800 --> 00:13:47,680
could think of convolution as synonymous to combination okay and why do we apply why do we

150
00:13:47,680 --> 00:13:54,160
say that for convolutional networks because what we're doing is we are combining the values for

151
00:13:54,160 --> 00:13:59,840
each of these layers with the input matrix so think of the input as that matrix right and so

152
00:13:59,840 --> 00:14:04,640
well it's a three-dimensional it's a it's a it's a it's a 3d tensor right but we're applying it to

153
00:14:04,640 --> 00:14:08,800
each of these dimensions right so three of them so just think of it as a matrix for right now

154
00:14:08,800 --> 00:14:16,800
and so what we do is we take this so at each layer at each layer there is a weight so by the way

155
00:14:16,800 --> 00:14:21,920
okay so there's a lot of interchangeable terms in machine learning and it's easy to get confused

156
00:14:21,920 --> 00:14:28,080
here but i want to set the record straight for a second weight is the same as feature matrix is

157
00:14:28,080 --> 00:14:35,600
the same as feature map is the same as a filter in this case in for convolutional networks so you

158
00:14:35,600 --> 00:14:40,480
see these or even kernel kernel is a different one there's actually five interchangeable terms so i

159
00:14:40,480 --> 00:14:46,560
can see how it can be confusing but if you get the basic idea of you have an input matrix which is

160
00:14:46,560 --> 00:14:52,880
your image and then you have a set of matrices which are your features that are learned you know

161
00:14:52,880 --> 00:15:01,360
edges shapes more abstract shapes that's it that's that's all it is matrix dot product matrices that

162
00:15:01,360 --> 00:15:05,760
are being multiplied by matrices all the way through that's that's all it is matrices that are

163
00:15:05,760 --> 00:15:09,680
being multiplied by matrices all the way through just a chain of them okay so what happens for

164
00:15:09,680 --> 00:15:15,360
convolution is we take a matrix and we multiply it by all the values in this matrix at a certain

165
00:15:15,360 --> 00:15:19,520
region right and so this is what i was talking about when i was saying we have a receptive field

166
00:15:19,520 --> 00:15:24,320
because we don't just multiply it all at once we multiply by a little part of it okay the receptive

167
00:15:24,320 --> 00:15:29,120
field and we slide it and we can define what that interval is that sliding window i know i'm talking

168
00:15:29,120 --> 00:15:33,040
a lot without coding the coding is coming believe me the coding is coming but just check this out

169
00:15:33,040 --> 00:15:39,920
for a second we got to learn this uh conceptually first so we are multiplying the the feature matrix

170
00:15:39,920 --> 00:15:44,000
by that input image just for every row and every column we're just multiplying multiply

171
00:15:44,000 --> 00:15:49,440
multiply and what happens is we have this new matrix that results the output and that output

172
00:15:49,440 --> 00:15:54,880
is considered the convolved feature okay and so what we do is we use that output as the input

173
00:15:54,880 --> 00:15:59,360
for the to the next layer and we repeat the process over and over and over again obviously

174
00:15:59,360 --> 00:16:04,480
there's two more parts here there's the activation the relu and then there's the pooling which i'll

175
00:16:04,480 --> 00:16:09,040
talk about as well but that's the basic idea between convolution and that's why we call it

176
00:16:09,120 --> 00:16:15,760
convolution because we are combining or convolving the weight matrix or filter or kernel whatever

177
00:16:15,760 --> 00:16:20,640
you want to call it feature map by that input we're combining it using the out and using that

178
00:16:20,640 --> 00:16:26,800
output as the input for the next layer after activating it and and pooling it okay so that's

179
00:16:26,800 --> 00:16:33,680
convolution and also um right so we apply it to all of those dimensions for that for that input

180
00:16:33,680 --> 00:16:39,040
matrix okay and that gives us our activation map or feature map or filter right so many different

181
00:16:39,040 --> 00:16:45,120
interchangeable terms here so anyway so it's computed using the dot product so you might be

182
00:16:45,120 --> 00:16:50,720
thinking well okay i see how there is a dot product i see how there's matrix multiplication

183
00:16:50,720 --> 00:16:54,960
but how does that really tell us what features there are i still you're still not making the

184
00:16:54,960 --> 00:17:02,240
connection probably why understandably why this these series of matrix operations help us detect

185
00:17:02,240 --> 00:17:07,600
features well here's what happens what happens is this and here's the great thing about matrices

186
00:17:08,320 --> 00:17:17,360
and having several of them when we learn a filter or a weight whatever you want to call it well this

187
00:17:17,360 --> 00:17:20,960
you know what moving forward let's just call it filter okay i'm just saying let's just call it

188
00:17:20,960 --> 00:17:25,280
filter moving forward for the rest of this video when we learn a filter over time by

189
00:17:25,280 --> 00:17:29,600
training it on mouse mouth pictures for example a filter is going to look like this at let's say

190
00:17:29,680 --> 00:17:33,440
at the first layer we we learn a filter for detecting a curve that looks like this right

191
00:17:33,440 --> 00:17:36,960
this curve right here and so what's what this filter is going to look like for detecting

192
00:17:36,960 --> 00:17:41,840
the specific type of curve is it's going to be a very sparse filter that means there's a lot of

193
00:17:41,840 --> 00:17:47,920
zeros except so there's all these zeros except for right here you see this 30 30 30 30 and notice

194
00:17:47,920 --> 00:17:54,160
that these values represent the shape they go in this direction of a shape and so what happens is

195
00:17:54,160 --> 00:18:00,720
when we take this filter and perform the dot product you know we convolve it with whatever part

196
00:18:00,720 --> 00:18:08,560
of the mouse if it's over a part of the mouse that matches that feature exactly then we when we

197
00:18:08,560 --> 00:18:13,760
multiply all of those uh when we when we perform the dot product between all those values and sum

198
00:18:13,760 --> 00:18:20,000
them up that's the convolution operation right there okay just it's going to be a big number

199
00:18:20,000 --> 00:18:24,320
okay and so then we know that we've detected a feature because we've we've multiplied it sum it

200
00:18:24,320 --> 00:18:30,160
up and there's a large number and if there's not if we multiply if let's let's say we have that

201
00:18:30,160 --> 00:18:35,040
receptive field over a different part of the mouse and that that curve doesn't exist then it's going

202
00:18:35,040 --> 00:18:40,640
to be zero right because if you look between these 30 30 30 values and that the equivalent

203
00:18:41,360 --> 00:18:47,360
locations on this pixel representation of the mouse image these are zeros and so what happens

204
00:18:47,360 --> 00:18:54,000
when you multiply zero by 30 you get zero right so that's why it's important to make the rest of the

205
00:18:54,000 --> 00:19:00,320
so the data that's irrelevant we want it to be zero right in the in the feature maps or in the

206
00:19:00,320 --> 00:19:04,960
filters that we learn in the filters that we learn we want the irrelevant parts to be zero

207
00:19:04,960 --> 00:19:14,240
and in the images okay and and in the input images so I so I could actually go even more into

208
00:19:14,240 --> 00:19:18,800
convolution but it's not really necessary but it's it is super dope it is super dope though

209
00:19:18,800 --> 00:19:23,200
this is a great blog post by the way I definitely encourage you to read this blog post it's linked

210
00:19:23,200 --> 00:19:28,720
in the notebook but this dude Tim Tim he goes into these this idea of convolution and he talks about

211
00:19:28,720 --> 00:19:37,920
how it's applied to all these different engineering fields and he goes into the formula the formula

212
00:19:37,920 --> 00:19:42,880
for the convolutional theorem is what he called is what it's called okay and I'm just going to go

213
00:19:42,880 --> 00:19:49,200
over this at a high level but the convolution theorem is this general theorem for discrete well

214
00:19:49,200 --> 00:19:54,240
there's a discrete version and a continuous version right discrete is if there's you know one or zero

215
00:19:54,240 --> 00:19:59,600
black or white you know definite classes that something could be whereas continuous is if it

216
00:19:59,600 --> 00:20:05,520
could be an infinite amount of values between zero and one point five point two five you know

217
00:20:05,520 --> 00:20:10,160
point seven infinity in that direction but here's the here's the formula for it and so

218
00:20:10,880 --> 00:20:14,720
let me make it bigger just really quickly and then we'll get back to it because it's

219
00:20:14,720 --> 00:20:22,000
it's really cool but the convolution theorem states that we and so in it's a general theorem

220
00:20:22,000 --> 00:20:29,360
that can be applied to any any any set of problems but in terms of what's relevant to us is is the

221
00:20:29,360 --> 00:20:36,320
convolutional theorem applied to matrix operations so what we can do is we can say what it what it

222
00:20:36,320 --> 00:20:41,440
says is it's the input times the kernel and it's the dot product it's a dot product between

223
00:20:41,440 --> 00:20:46,560
two different matrices and we perform that for every value in all of those matrices and we do that

224
00:20:46,560 --> 00:20:50,480
for all of the values that we have and we sum them up together and that's what the sigma term

225
00:20:50,480 --> 00:20:55,280
represents and we and we actually express that right here right this operation right here this

226
00:20:55,280 --> 00:20:59,680
multiplication and summation it's the same thing but it's a more complex way of looking at it or

227
00:20:59,680 --> 00:21:06,640
more mathematically accurate way and also the fast Fourier transform is is brought up by this and

228
00:21:07,840 --> 00:21:13,040
the fast Fourier transform takes some spatial data and it converts it into Fourier space which is

229
00:21:13,040 --> 00:21:18,880
like a waveform and you see this a lot in your day-to-day life whenever you're looking at some

230
00:21:18,880 --> 00:21:23,120
sound you know you're listening to some sound and you look at your mp3 player and you see the waves

231
00:21:23,120 --> 00:21:27,360
that's a that's a Fourier transform happening but i won't go into that that's that's for sound and

232
00:21:27,360 --> 00:21:31,840
audio but anyway it's a really cool blog post definitely check it out okay so back to this

233
00:21:33,920 --> 00:21:38,560
so we talked about convolution now we're going to talk about pooling right so what is pooling so

234
00:21:39,280 --> 00:21:43,440
whenever we apply convolution to some image what's going to happen at every layer

235
00:21:43,440 --> 00:21:49,360
is we're going to get a series of feature of so each of the weights are going to consist of

236
00:21:49,360 --> 00:21:56,480
multiple images and each of these images are going to be at every layer there's going to be more

237
00:21:56,480 --> 00:22:02,160
and smaller images so the first few layers are going to be these huge images right and then at the

238
00:22:02,160 --> 00:22:04,880
next few layers are going to be more of those but they're going to be smaller and it's just going to

239
00:22:04,880 --> 00:22:09,600
get just like that okay and at the end we squash it with some fully connected layer so we get some

240
00:22:09,600 --> 00:22:16,560
probability values with a softmax but anyway what pooling does is it is it dense is it makes

241
00:22:16,560 --> 00:22:24,000
the matrix the matrices that we learn more dense here's what i mean so if you if you perform convolution

242
00:22:24,080 --> 00:22:31,680
between an input and a feature matrix or a weight matrix or filter it's going to result in a matrix

243
00:22:31,680 --> 00:22:36,480
right but this matrix is going to be pretty big it's going to be a pretty big matrix what we can do

244
00:22:36,480 --> 00:22:42,480
is we can take the most important parts of that matrix and pass that on and what that's going to

245
00:22:42,480 --> 00:22:48,240
do is it's going to reduce the computational complexity of our model okay so that's what pooling

246
00:22:48,240 --> 00:22:53,440
is all about it's a pooling set so there's different types of pooling max pooling is the most used

247
00:22:53,440 --> 00:23:00,160
type of pooling by the way so basically multiply so what happens is we we strive we have some we

248
00:23:00,160 --> 00:23:05,520
define some window size and then some stride size so how what are the intervals that we look at

249
00:23:05,520 --> 00:23:11,600
and we say okay so for each of these windows let's take the max value so for so for uh this one right

250
00:23:11,600 --> 00:23:16,720
here four six zero eight the max value would be eight and so for one three twelve nine it'd be

251
00:23:16,720 --> 00:23:20,720
twelve right so we just take the biggest number it's really simple actually we just take the biggest

252
00:23:20,720 --> 00:23:25,120
number and we just do that for all of them and so that that's what pooling is all about and so

253
00:23:25,120 --> 00:23:31,680
it's going to just give us that the most relevant parts of the image and if you if you think of these

254
00:23:31,680 --> 00:23:38,800
these very these values in in the in the matrix as pixel intensities by taking the maximum intense

255
00:23:38,800 --> 00:23:43,520
the the pixel with the most intensity or the the highest intensity we're getting that feature that

256
00:23:43,520 --> 00:23:48,960
is the most relevant if you see what I'm saying it's a least opaque feature to use a term from

257
00:23:49,280 --> 00:23:57,680
image um math anyway so we so we talked about pooling and we talked about uh we talked about

258
00:23:58,400 --> 00:24:00,160
activation and so now

259
00:24:02,880 --> 00:24:06,800
no we talked about convolution and we talked about pooling and so now the third part

260
00:24:06,800 --> 00:24:12,960
is normalization or activation so remember how I said how it would be it's so important that we

261
00:24:12,960 --> 00:24:18,480
have these values that are not related to our image be zero we want it to be zero so the result

262
00:24:18,480 --> 00:24:24,960
is zero if the if the feature is not detected well the way we do that is using relu and so relu

263
00:24:24,960 --> 00:24:31,120
stands for rectified linear unit it's an activation function it's an activation function okay we use

264
00:24:31,120 --> 00:24:36,960
activation functions throughout neural networks and we use them because it is you can also call

265
00:24:36,960 --> 00:24:43,920
them non-linearities because they they make our model able to learn non-linear functions not just

266
00:24:43,920 --> 00:24:48,880
linear functions but non-linear functions so any kind of function right the universal function

267
00:24:48,880 --> 00:24:54,400
approximation theorem we talked about that activation functions help make this happen and

268
00:24:54,400 --> 00:24:59,760
so relu is a is a special kind of activation function that turns all negative numbers into

269
00:25:00,720 --> 00:25:04,960
zero so that's why it's going to make the math easier it won't make the math break

270
00:25:04,960 --> 00:25:09,760
for our convolutional network so we'll apply a relu so basically what we do is for every single pixel

271
00:25:09,760 --> 00:25:15,120
value in the in the input to this relu activation function we turn it if it's a negative we just

272
00:25:15,120 --> 00:25:19,200
say make it zero it's super simple it'll be one line of code you'll see exactly what i'm talking about

273
00:25:20,400 --> 00:25:24,880
okay so that's that's those are our blocks so that's how our convolutional blocks work

274
00:25:25,760 --> 00:25:30,320
however there is another step that I didn't talk about that is a nice to have and state of the

275
00:25:30,320 --> 00:25:35,920
our convolutional networks always use it and that's called dropout so Jeffrey Hinton the guy who invented

276
00:25:36,480 --> 00:25:42,800
neural networks invented a feature invented a technique called dropout and what dropout is

277
00:25:42,800 --> 00:25:49,200
is a good analogy is old people or not old people but people who are stuck in their ways let me let

278
00:25:49,200 --> 00:25:55,520
me okay so what dropout does is it turns neurons on and off randomly what do I mean by that that I

279
00:25:55,520 --> 00:26:02,080
mean the the matrices for each weight value is converted to zero randomly at some layer of the

280
00:26:02,080 --> 00:26:07,600
network and so what happens is by doing this our network is forced to learn new representations

281
00:26:07,600 --> 00:26:12,880
for the data new pathways that that data has to flow through it can't always flow through this neuron

282
00:26:12,880 --> 00:26:17,680
and the reason we use it is to prevent overfitting right we want to prevent overfitting we want to

283
00:26:17,680 --> 00:26:22,560
prevent being too fit to the data think of it as you know the older you get the more set in your

284
00:26:22,560 --> 00:26:28,240
ways of thinking your you are right and so it's harder to think of new ways of of of thinking

285
00:26:28,240 --> 00:26:33,360
right because you're so set in some ways so a way to prevent that is to have a novel crazy

286
00:26:33,360 --> 00:26:38,160
experience whether it's skydiving or ticking psychedelics or whatever it is and what that

287
00:26:38,160 --> 00:26:43,280
does is it creates new pathways so you're not so you're kind of forced your brain is forced to make

288
00:26:43,280 --> 00:26:50,480
new pathways and this increases your generalization ability and you're not so overfit that's a very

289
00:26:50,480 --> 00:26:56,240
rough abstract analogy but basically dropout is not as complex as that sounds dropout can be

290
00:26:56,240 --> 00:27:01,040
done in three lines of code so definitely check out this blog post as well that I've linked

291
00:27:01,040 --> 00:27:07,600
but what it does is it just randomly picks some neurons in a layer to set to zero right so it's

292
00:27:07,600 --> 00:27:12,240
just it's just three lines okay and you can look at it in this notebook right so that's and then our

293
00:27:12,240 --> 00:27:17,760
last step is probability conversion so we've got this huge set of values right all these little

294
00:27:17,760 --> 00:27:22,960
small images that are represented by this huge output matrix and we want to take this huge set

295
00:27:22,960 --> 00:27:27,920
of values and make some sense out of it we want to make probabilities out of it and the way we do

296
00:27:27,920 --> 00:27:33,440
that is using a soft max at the end a soft max is a type of function and it looks like this this

297
00:27:33,440 --> 00:27:38,160
this is a soft max function right here but what we do is we plug these values into the soft max

298
00:27:38,160 --> 00:27:43,600
function and it's going to output a set of probability values discrete probability values

299
00:27:43,600 --> 00:27:48,240
for each of the classes that we're trying to predict okay and then what we'll do is given

300
00:27:48,240 --> 00:27:54,560
all those probability values will pick the biggest one using arg max the arg max function in numpy

301
00:27:54,560 --> 00:28:00,160
and that's going to give us the most likely class okay those are the seven steps of a

302
00:28:00,160 --> 00:28:06,880
full forward pass through a convolutional network looks like that and so now you might be wondering

303
00:28:06,880 --> 00:28:13,040
well okay so how do we train this thing well using gradient descent right and when applied to neural

304
00:28:13,040 --> 00:28:20,320
networks gradient gradient descent is called back propagation exactly i hope you got that right

305
00:28:20,320 --> 00:28:25,920
anyway okay so how do we learn these magic numbers right how do we learn what these weight values

306
00:28:25,920 --> 00:28:31,600
should be what the feature should be back propagation is how we do it right and so we've talked quite a

307
00:28:31,600 --> 00:28:36,480
bit about back propagation and gradient descent but i'll do a little i'll go over it again um

308
00:28:37,440 --> 00:28:42,160
but the idea is that we have some error that we're computing right this is super this is

309
00:28:42,160 --> 00:28:47,440
supervised learning we have a we have a human label right for some data so we put in a dog image

310
00:28:47,440 --> 00:28:52,160
or a bicycle image to look at the summit to to relate to this image here we put in a bicycle

311
00:28:52,160 --> 00:28:57,040
image and the bike label we pass it through the each layer dot product dot product dot you know

312
00:28:57,040 --> 00:29:02,960
dot product activation function pool dot product repeat repeat soft max or squash into probability

313
00:29:02,960 --> 00:29:07,680
values pick the biggest one and we have some prediction value and what we do is we compare

314
00:29:07,680 --> 00:29:12,480
the prediction value to the out the actual value and we get an error and we take our error

315
00:29:12,480 --> 00:29:16,720
and we compute the partial derivative of the error with respect to each weight value

316
00:29:16,720 --> 00:29:23,280
going backwards in the network okay like this okay and so for regression we use the mean squared

317
00:29:23,280 --> 00:29:28,960
error if we're using linear regression regression and for classification we use the softmax function

318
00:29:28,960 --> 00:29:33,440
so remember how in the first neural network we built and in their linear regression example

319
00:29:33,440 --> 00:29:39,920
we used a uh we use mean squared error to compute the error and now we're using the softmax

320
00:29:39,920 --> 00:29:43,760
so we'll take the so we'll take the partial derivative of the error with respect to our

321
00:29:43,760 --> 00:29:48,480
weights and then that's going to give us the gradient value that we then update each of those

322
00:29:48,480 --> 00:29:54,000
weight values recursively going backward in the network and that's how it learns what this features

323
00:29:54,000 --> 00:29:59,440
what the ideal feature the weight matrix value should be but what about the other

324
00:30:00,560 --> 00:30:04,560
what about the other magic numbers what about the number of neurons and the number of features

325
00:30:04,560 --> 00:30:09,200
and the size of those features and the pooling window size and the window stride well those that

326
00:30:09,200 --> 00:30:14,560
is an active area of research there are best practices for values that you should use for those

327
00:30:14,560 --> 00:30:20,560
for those hyper parameters right the tuning knobs of our network and andrew karpathy has some great

328
00:30:20,560 --> 00:30:24,320
material on this and he's probably the leading source for convolutional networks right now in

329
00:30:24,320 --> 00:30:31,520
terms of um written contents and uh yeah i mean this is an active area of research finding out

330
00:30:31,520 --> 00:30:35,360
what the ideal hyper parameters for our neural network should be and we're still learning what

331
00:30:35,360 --> 00:30:40,560
it should be what what what what how we can get them rather than just guessing and checking which

332
00:30:40,560 --> 00:30:46,480
is what we do right now which is kind of like you know not it's not as optimal right so anyway

333
00:30:47,280 --> 00:30:50,800
last two things and then we're gonna get started with the code when is a good time to use this

334
00:30:50,800 --> 00:30:54,880
well we know to classify images we've talked about that but you can also use them to generate

335
00:30:54,880 --> 00:30:59,520
images and that's for later on that's a little more advanced but to give you a little spoiler

336
00:30:59,520 --> 00:31:03,760
a little teaser in fact this is in my intro to deep learning playlist you take a convolutional

337
00:31:03,760 --> 00:31:07,680
network you flip it and then you call it a deconvolutional network and then you can take

338
00:31:07,680 --> 00:31:15,040
some text and create an image out of text how crazy is that okay there's also generative models

339
00:31:15,040 --> 00:31:18,800
where you have two networks fighting each other and you can generate new images a whole bunch

340
00:31:18,800 --> 00:31:24,320
of really cool crazy stuff you can do but anyway when should you use a convolutional network

341
00:31:24,320 --> 00:31:30,640
anytime you have spatial 2d or 3d data what do i mean well obviously images are spatial the word

342
00:31:30,640 --> 00:31:37,760
spatial implies that the space the positioning of the data matters so sound you can apply to sound

343
00:31:37,760 --> 00:31:44,080
images or text where the the the position of the text matters right because we have a flashlight

344
00:31:44,160 --> 00:31:50,000
our filter and we're convolving over an image right but if you have some data like say customer

345
00:31:50,000 --> 00:31:54,400
data where if you were to just flip the rows and columns it doesn't matter what order they're in

346
00:31:54,400 --> 00:31:59,840
they're still you know they're still features so a good rule of thumb is if you swap out the rows

347
00:31:59,840 --> 00:32:05,600
and columns of your data set and it's just as useful like the space doesn't matter then you

348
00:32:05,600 --> 00:32:10,720
don't want to use a cnn else you do okay and a great and last thing the great example of using

349
00:32:10,720 --> 00:32:15,760
cnn's are for robot learning you can use a cnn for object detection and you can use a cnn for

350
00:32:15,760 --> 00:32:20,560
grasp learning and combine the two and then you can get a robot that cooks which is really cool

351
00:32:20,560 --> 00:32:25,680
i've got a great tensorflow example and a great adversarial network example okay let's go into

352
00:32:25,680 --> 00:32:31,760
the code now and so what i'm going to do is i'm going to look at the class for the convolutional

353
00:32:31,760 --> 00:32:36,880
network in numpy as well as the prediction class there's two classes here okay so these are our

354
00:32:36,880 --> 00:32:43,280
three inputs pickle is for saving and loading our serialized model what do i mean pickle is

355
00:32:43,280 --> 00:32:48,880
python's way of having a platform or language agnostic way of saving data so you can load it

356
00:32:48,880 --> 00:32:54,160
up later tensorflow uses it a bunch of other libraries use it as well numpy is for matrix math

357
00:32:54,160 --> 00:32:58,400
and we've got our own little custom class for pre-processing the data because we don't care

358
00:32:58,400 --> 00:33:04,480
about that part we care about the machine learning part okay so let's talk about our light ocr or

359
00:33:04,560 --> 00:33:09,360
object optical character recognition class in our initialized function we're going to load the

360
00:33:09,360 --> 00:33:13,840
weights from the pickle file and then store and then store all the labels that we've loaded

361
00:33:13,840 --> 00:33:19,040
we'll define how many rows and columns in an image load up our convolutional network using the light

362
00:33:19,040 --> 00:33:24,240
cnn function with our saved weights so assuming we've already trained our network we load it with

363
00:33:24,240 --> 00:33:29,840
the saved weights from the pickle file and then we define the number of pooling layers okay so once

364
00:33:29,840 --> 00:33:35,200
we have that then we can use this predict function so given some new image we'll reshape the image so

365
00:33:35,200 --> 00:33:40,480
it's in the correct size to perform the dot product between that image and the first layer of our

366
00:33:40,480 --> 00:33:46,880
convolutional network and we'll we'll we'll put it we'll feed it into our network and it's going to

367
00:33:46,880 --> 00:33:51,600
output a prediction probability for a class and we'll return it okay super high level we haven't

368
00:33:51,600 --> 00:33:57,280
even coded our cnn that's that's our first class that's our prediction class now now we're going

369
00:33:57,280 --> 00:34:01,200
to look at the convolutional network class and what i'm going to do is i'm going to i'm going to

370
00:34:01,200 --> 00:34:08,240
go over the code and i'm going to code some parts of it so now we'll look at our convolutional

371
00:34:08,240 --> 00:34:14,160
network class okay so in our initialized function we'll initialize two lists one to store the layers

372
00:34:14,160 --> 00:34:19,520
that we've learned the the weights of each layer and then the size of the pooling area for max pooling

373
00:34:19,520 --> 00:34:25,360
okay we'll load up our weights from our pickle file just like this and then we have our predict

374
00:34:25,360 --> 00:34:29,520
function now in our predict function that's where the real magic is happening right let's

375
00:34:29,520 --> 00:34:34,080
code what this looks like so given some input x we're going to feed it through all of these

376
00:34:34,640 --> 00:34:42,880
layers right so what happens is we will say okay so the first layer is going to be a convolutional

377
00:34:42,880 --> 00:34:47,280
layer okay and we're going to define what all of these functions look like look like but the first

378
00:34:47,280 --> 00:34:51,680
layer is going to be that convolutional layer we'll feed in that first image and we'll say okay

379
00:34:51,680 --> 00:34:56,640
well this is the first layer so it's a zeroth layer we'll say border mode equals full and i'll

380
00:34:56,640 --> 00:35:02,000
talk about that part later on but that's it for that and so what happens is x equals this layer

381
00:35:02,000 --> 00:35:07,760
okay so that's our first layer and then our next layer is going to be relu so we'll say okay now

382
00:35:07,760 --> 00:35:14,400
let's apply an activation to the output of the previous layer okay and then we'll set it equal

383
00:35:14,400 --> 00:35:20,080
to that okay so we'll set the output from the previous layer equal to the input of this layer

384
00:35:20,080 --> 00:35:25,840
and then we keep going we say okay so we've got another uh cnn we have another convolutional layer

385
00:35:25,840 --> 00:35:31,440
and we do the same thing here we say okay take the in output from the previous layer we'll define

386
00:35:31,440 --> 00:35:36,080
what the uh name of this layer is as well as the border mode which i'll talk about the very end of

387
00:35:36,080 --> 00:35:43,600
this we have a border mode which is valid and then we say okay well we'll set the output of that

388
00:35:43,600 --> 00:35:48,800
equal to the input of this and just keep repeating now it's time for us to apply a

389
00:35:49,600 --> 00:35:54,720
another non-linearity so we'll just go ahead and apply our non-linearity again remember these are

390
00:35:54,720 --> 00:36:02,000
convolutional blocks oh and we also want to pool so also the the order with which you can do this

391
00:36:02,000 --> 00:36:06,960
varies right you can do this in different ways and yeah so i'm doing it a certain way right now

392
00:36:06,960 --> 00:36:11,840
you know we could change it around it would change our result but the order map the ordering within

393
00:36:11,840 --> 00:36:18,000
the block can be can be different okay so right so we're going to pool it we're going to pick the

394
00:36:18,000 --> 00:36:25,840
the most relevant features from from that uh from that output and then we're going to perform drop

395
00:36:25,840 --> 00:36:31,680
out to prevent overfitting and we're going to say there's going to be a 0.25 chance that a neuron

396
00:36:31,680 --> 00:36:36,960
is going to be deactivated that will turn it off set it to zero and that's our dropout probability

397
00:36:36,960 --> 00:36:44,880
value and then now we're getting into our our the second category of our network not the feature

398
00:36:44,880 --> 00:36:48,640
learning part but the classification part and we'll say okay so let's flatten this layer let's

399
00:36:48,640 --> 00:36:56,000
reduce the dimensionality of all of that that data so it's something that we can then learn from

400
00:36:56,000 --> 00:37:02,160
and we'll say well let's let's set it equal to seven and then we'll say once again turn that output

401
00:37:02,160 --> 00:37:10,240
into our uh inputs here okay and so then we have another dense layer we just we just keep going

402
00:37:10,240 --> 00:37:14,640
with or our first dense layer and that means we are going to it's a fully connected layer so we're

403
00:37:14,640 --> 00:37:19,760
combining everything that we've learned because we're getting really close to squashing these values

404
00:37:19,760 --> 00:37:24,800
into a set of probability values so we want to take all of our learnings and combine them

405
00:37:24,800 --> 00:37:30,000
with a fully connected layer and so we'll combine them with a fully connected layer

406
00:37:30,720 --> 00:37:39,760
and then uh we'll squash it now with our sigmoid or no not our sigmoid our softmax function okay

407
00:37:39,760 --> 00:37:44,480
and then that's going to give us our output probability and then we're going to say well

408
00:37:44,480 --> 00:37:49,520
which of the probabilities do we want we want the max one right we want the max probability

409
00:37:49,520 --> 00:37:55,440
and we'll classify it just like that and return that value okay that's the highest level and so if

410
00:37:55,440 --> 00:37:59,840
you were using keras or one of these high level libraries this is all your code would look like

411
00:37:59,840 --> 00:38:04,000
but what we're going to do is we're going to look at these functions as well okay so let's look at

412
00:38:04,000 --> 00:38:10,800
these functions so we'll start off with the convolutional layer function and have your

413
00:38:10,800 --> 00:38:15,120
notebook open with me as well so you could go over this the the link is in the description if you

414
00:38:15,120 --> 00:38:20,400
don't know now you know if you don't know now you know so for our convolutional layer given some

415
00:38:20,400 --> 00:38:25,120
input image we're going to say well we'll store our feature maps and the bias value in these two

416
00:38:25,120 --> 00:38:30,480
variables features and bias will define how big our filter or patch is going to be how many features

417
00:38:30,480 --> 00:38:36,720
do we want how big is our image how many channels rgb so three and then how many images do we have

418
00:38:36,720 --> 00:38:42,720
so given those values we'll define a border mode so a border mode so is so when you apply

419
00:38:42,720 --> 00:38:47,760
fold to border mode in this case it means that the filter has to go outside the bounds of the input

420
00:38:47,760 --> 00:38:53,120
by filter size divided by two the area outside of the input is normally padded with zeros and

421
00:38:53,120 --> 00:38:57,680
the border mode valid is when you get an output that is smaller than the input because the convolution

422
00:38:57,680 --> 00:39:03,680
is only computed where the input and the filter fully overlap okay and they'll give us different

423
00:39:04,720 --> 00:39:10,400
they'll give us different classification results accuracy results and it's good to test both

424
00:39:10,400 --> 00:39:15,680
options so what we'll do is we'll initialize our feature matrix for this layer as conv as convolve

425
00:39:15,680 --> 00:39:20,000
zeros it's going to be a bunch of zeros and then we'll say okay so for every image that we have

426
00:39:20,000 --> 00:39:25,120
for every feature in that image let's initialize a convolved image as empty and then for each

427
00:39:25,120 --> 00:39:29,840
channel so we're doing this for each of the three channels let's extract a feature from our feature

428
00:39:29,840 --> 00:39:35,760
map define a channel specific part of our image and then perform convolution on our image using

429
00:39:35,760 --> 00:39:41,280
that given feature filter so notice this convolved 2d function it's where the actual convolution

430
00:39:41,280 --> 00:39:47,520
operation is happening this is more of a wrapper for that actual mathematical operation so once

431
00:39:47,520 --> 00:39:52,160
we have that we'll add a bias and a bias acts as our anchor for our network it's kind of like the

432
00:39:52,160 --> 00:39:58,080
y intercept it's kind of like a starting point for our model to exist and then we'll add it to our

433
00:39:58,080 --> 00:40:02,960
list of convolved features for this for this layer okay and we'll return that as the as our feature

434
00:40:02,960 --> 00:40:09,440
map our set of filter values our weight matrices and so let's look at this convolved 2d function so

435
00:40:09,440 --> 00:40:14,320
in our convolved 2d function we'll define the tensor dimension of the image and the feature

436
00:40:14,320 --> 00:40:22,960
we'll get a target dimension and then these two lines perform this this operation this convolutional

437
00:40:22,960 --> 00:40:26,960
theorem that we defined right here we're performing the dot product between the input

438
00:40:26,960 --> 00:40:34,480
and the kernel or feature for for all of those weight values and then we're summing them all up

439
00:40:34,480 --> 00:40:40,720
and that's going to be our output and so the fast Fourier function in numpy does this very well

440
00:40:40,720 --> 00:40:46,240
and so we can just use that as fft2 but that's it's a multiplication and a summation operation

441
00:40:46,800 --> 00:40:52,480
okay and so then we have our target value and then once we have our target value we could say

442
00:40:52,480 --> 00:40:56,800
okay let's have a starting point and an ending point and our target value is going to be within

443
00:40:56,800 --> 00:41:02,880
that range of what we want to return as the convolved feature right so we have some bounding

444
00:41:02,880 --> 00:41:08,320
box that we want to apply this to okay so then so we have that so what else do we have so we start

445
00:41:08,320 --> 00:41:15,440
off with our convolutional layer and then we had our relu so what is relu relu super simple relu

446
00:41:16,000 --> 00:41:23,360
relu is just forgive so for for some matrix of zeros it will go through every single pixel value

447
00:41:23,360 --> 00:41:28,560
in the input matrix and if it's a negative number we just turn it into zero that's it that's relu okay

448
00:41:29,360 --> 00:41:33,360
and then so we have we have talked about relu we've talked about convolution we have to talk

449
00:41:33,360 --> 00:41:39,440
about pooling so what does max pooling look like so given our learned features and our images

450
00:41:39,440 --> 00:41:44,640
let's initialize our more dense feature list as empty and so here's what we do we're going to

451
00:41:44,640 --> 00:41:49,680
we're going to take the max values of all of those parts of the input image right so we're

452
00:41:49,680 --> 00:41:54,240
going to say we're going to say for each image and for each feature map begin by the row define a

453
00:41:54,240 --> 00:41:59,840
starting and ending point okay which we define with our pool size hyper parameter and so for each

454
00:41:59,920 --> 00:42:04,800
column so we've got a set of rows and columns for each image there's a notice a lot of nesting

455
00:42:04,800 --> 00:42:09,120
happening here we're going to define starting end points for the columns as well and then we're

456
00:42:09,120 --> 00:42:14,800
going to say define a patch given our defined starting and ending point so some some bounding box

457
00:42:14,800 --> 00:42:21,360
and then take the max value from that patch using nmp.max and that patch is what moves around right

458
00:42:22,800 --> 00:42:27,520
for all parts of that image and then we return that and we're going to store all of that in our

459
00:42:27,520 --> 00:42:33,520
pooled features matrix right here and we return that as the output and that's what we pass on in

460
00:42:33,520 --> 00:42:39,200
the convolutional network okay so that's what max pooling is okay so we've talked about convolution

461
00:42:39,200 --> 00:42:47,920
relu max pooling and then dropout so for dropout right we have our probability value that we define

462
00:42:47,920 --> 00:42:53,440
as 0.25 and we just multiply it by the input okay and that what that's going to do is it's going to

463
00:42:53,440 --> 00:42:58,960
turn on or off some part of the matrix into so by on and off I mean zero it'll make it either

464
00:42:58,960 --> 00:43:04,640
zero or not zero so it'll so then our data will have to learn to either be multiplied by it or

465
00:43:04,640 --> 00:43:10,800
find a different pathway and that's for dropout and then we talked about dropout and convolution

466
00:43:11,360 --> 00:43:17,120
flattening dense and softmax so for flattening it's just a it's a tensor transformation we just

467
00:43:17,200 --> 00:43:22,560
reduce the dimensionality of the input okay and then for our

468
00:43:25,440 --> 00:43:30,720
dense layer our denses are fully connected layer now this is the generic layer that you would see

469
00:43:30,720 --> 00:43:36,560
in a feedforward network input times weight uh and then you add a bias right which is the dot

470
00:43:36,560 --> 00:43:40,720
product right here this is this is a dense layer we just take our input times our weight at a bias

471
00:43:40,720 --> 00:43:45,200
so that means we we just perform the dot product between the full weight matrix and the full weight

472
00:43:45,200 --> 00:43:50,000
matrix instead of doing it at all the layers because that would be way too computationally

473
00:43:50,000 --> 00:43:56,080
expensive for image data we perform it at one fully one fully connected or dense layer at the end

474
00:43:56,080 --> 00:43:59,360
and that's a way for us to combine all of our learnings together so we can then

475
00:43:59,920 --> 00:44:08,880
promptly squash it with a softmax function okay so then for our softmax layer and then we have

476
00:44:08,880 --> 00:44:16,160
classify so for our softmax layer we will uh so this is the this is the formula for softmax

477
00:44:16,160 --> 00:44:21,440
programmatically speaking uh but what it does is going to output a set of probability values

478
00:44:21,440 --> 00:44:25,680
and then we'll classify those values by taking the arg max the largest probability

479
00:44:25,680 --> 00:44:34,160
and that is our output okay so that is our forward pass through the network okay and so

480
00:44:34,160 --> 00:44:39,600
yes that is our forward pass through the network

481
00:44:47,760 --> 00:44:52,480
so back so back propagation works pretty much the same way as i've talked about before several

482
00:44:52,480 --> 00:44:56,640
times gradient descent back propagation works the same way we take the partial derivative of our

483
00:44:56,640 --> 00:45:00,960
error with respect to our weights and then recursively update our weights using that gradient

484
00:45:00,960 --> 00:45:06,080
value that we gradient equals partial derivative equals delta interchangeable words but here's

485
00:45:06,080 --> 00:45:12,400
a great simple example right here where we after the forward pass we do the same thing in reverse

486
00:45:12,400 --> 00:45:16,560
order so we calculate the gradient of those weights and then back and then multiply them by

487
00:45:16,560 --> 00:45:23,520
the previous layer and then for our javascript portion we are taking the drawing from the user

488
00:45:23,520 --> 00:45:28,640
here's the main code for that paint window in a canvas and we are going to say capture the

489
00:45:28,640 --> 00:45:33,520
mouse's positions capture all those points in that image with an event listener and then we're

490
00:45:33,520 --> 00:45:37,920
going to say on paint so whenever the user actually starts moving that painting whenever that mouse

491
00:45:37,920 --> 00:45:42,560
stops clicking and then the user hits the submit button we'll save that snapshot of that image and

492
00:45:42,560 --> 00:45:47,920
then feed that into the network and that's our flask app we'll define two routes one for our home

493
00:45:47,920 --> 00:45:52,080
and then one for that image for the network we can deploy to the web there's a heroku app you

494
00:45:52,080 --> 00:45:56,400
could definitely check out the link link is in the description as well check out the notebook

495
00:45:56,400 --> 00:45:59,760
and yeah that's it please subscribe for more programming videos and for now

496
00:45:59,760 --> 00:46:03,440
i've got to do a 4a transform so thanks for watching

