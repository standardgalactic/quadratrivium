So, what I'm going to do in this course is discuss mostly ideas that are already in the
book called The Emotion Machine, I'm sorry I used that title, and the older book called
The Society of Mind, which are, the books are not quite the same, they overlap a bit
in material, but they're sort of complementary, I like the old one better because the chapters
are all one page long, and they're moderately independent, so if you don't like one you
can skip it. The new book is much denser and has a smaller number of long chapters, and
I think it's, over the years I got lots of reactions from young people in high school,
for example, almost all of whom like The Society of Mind and found it easy to read and seemed
to understand it, there are lots of criticisms by older people who maybe some of them found
it harder to put so many fragments together, who knows, but most of this class, most of
the things I'd like to say are in those books, so it's really like a big seminar and I'll,
my hope is that everyone who comes to this class would have a couple of questions that
they'd like to discuss, and if I can't answer them maybe some others if you can, so I'd
like to think of this as a super seminar, and normally I don't prepare lectures, and
I just start off asking if there are any questions, and if they're not I get really pissed off
because, but anyway I'm going to start with a series of slides. So why do we need machines,
and partly there are a lot of problems, unlike most species or kinds of animals, humans have
only been around a few million years, and they're very clever compared to other animals,
but it's not clear how long they will last, and when we go we might take all the others
with us, so there are a whole set of serious problems that are arising because there are
so many humans, and here's just a little list of things. There's a better list in a book
by the astronomer royal Martin Lise of England, anybody know the title? Yes, our final hour,
it's a slightly scary title, and when I was a teenager, World War II came to an end with
the dropping of two atomic bombs on Japan, and I didn't believe the first one was real
because it was in Hiroshima, so I assumed that the U.S. had somehow made a big underground
underwater tanker with 20,000 tons of TNT and some few grams of radium or something and
blown it up in the harbor, and first it flew an airplane over dropping some little thing,
and this was to fool the Japanese into thinking that we have an atomic bomb, but when they
did it again over Nagasaki that wasn't feasible, so, and when I was in grade school, sometimes
if I said something very bright, I would hear a teacher saying, maybe he's another J. Robert
Oppenheimer, because that was the name of a scientist who had been head of the Manhattan
Project, and he was, I think, three or four years earlier in grade school than I was, and I thought
it was very strange for a person to have a first name as just being a letter rather than a name,
and many years later, when I was at Princeton in graduate school, I met the Robert Oppenheimer,
and that was a great pleasure, and in fact he took me to lunch with a couple of other people I
admired, namely Gertl and Einstein, which was very exciting, except I couldn't understand
Einstein because I wasn't used to people with a strong German accent, but I understood Gertl
just fine, and after that lunch was over, I went and spent about a year learning about Turing
machines and trying to prove theorems about them and so forth. So anyway, in the course of these
talks, we'll run across a few of these people, and here's a big list of the people that I'm mostly
indebted to for the ideas in the Society of Mind and the Emotion Machine. The ones in blue are
people I've actually met. It would be nice to have met Aristotle because no one really knows much
about him, but you really should read, just skim through some of that and you'll find that this is a
really smart guy. We don't know if he wrote this stuff or if it were compiled by his students,
like a lot of Feynman's writing is, and von Neumann's writing is edited from notes by their
students. Anyway, the astonishing thing about Aristotle is that he seems to be slightly more
imaginative than most cognitive scientists you'll run into in the present day. It would have been
nice to know Spinoza and Kant and the others. Also, Freud wrote 30 or 40 books, so did he fall off this
list? There he is. I just made this list the other day and I was looking up these people to find their
birthdays and stuff. Yes? Because they're religious, as far as I can see. Well, who would you, would you
say Buddha? Name one. Maybe I never heard of them. Confucius. Well, I only know of them through
aphorisms. Single Proverbs, but I don't know that Confucius had a theory of thinking. Well, I've
looked at Buddhist theories and they're, I don't think they would get a C plus. And one problem is
that there are cultures, there's something about Greek culture because it had science, it had
experiments. Somebody has a theory and they say, and like Epimenides Lucretius, somewhere in the society
of mind, I think I quoted Lucretius about translucent objects. And he says they have the
particular appearance because the rays of light bounce many times before they get to the surface, so
you can't tell where they started. And I don't find in Eastern philosophy theories that say, here's what
I think and here's a reason why. I've looked at Buddhist stuff and it's strange lists of psychological
principles, every one of which is looks pretty wrong. And they make nice two dimensional diagrams,
but no evidence for any of them. So I don't know whether to take it seriously.
Well, what can't be tested?
Then why, if they can't be tested, why should one look at it twice?
Okay, I think this is a serious argument. It seems to me that science began a little bit in China,
a little bit in India. In the Arabic world, they got up to the middle of high school algebra. But then what?
Well, but this wasn't as good as Archimedes, who got to the beginning of calculus. So if you look at most
cultures, they never got to the critical point of getting theories, doing experiments, discussing them
and then throwing them out. And so if you look at Buddhist philosophy, it's 2,500 years old.
If you look at Greek physics, yes, Archimedes almost got calculus and he got lots of nice principles. And
Buddha mentions, at some point, if you want to weigh an elephant,
put him in a boat and then take the elephant out and put rocks in till the boat sinks at the same level. So there
you see a good idea. But if you look at the history of the culture, if people still say this 1000 year old
stuff is good, then you should say, no, it's not.
Sure.
No, the question is why did it stop? Why did it stop?
Ancient wisdom is generally not very good. And we shouldn't respect it for too long. And that's
no, everybody. No, we, we got rid of alchemy, we got rid of what do you call it? What's caloric?
You, you use you jump off their shoulder. You don't stay on them.
So it's good to know history. But if the history doesn't get anywhere, then you don't want to admire it too
much because you have to ask, why did it stop when it went wrong? And usually went wrong because
barbarians came in and, well, you know, it happened to Archimedes. Some, some Roman killed him. But
anyway, no, it's, it's good. It's a good question. Why, why didn't science happen a million years ago?
Because humans are five million years old. So what took it so long?
No, it's more. Sure. Okay. Do you have a theory of why science didn't develop for
so long? In most cultures, it might be religion, which is a sort of science that doesn't use
evidence. And in fact, kills people who try to get it. And so they're, they're systematic reasons why
most cultures failed. And maybe somebody has written, is there a book on why science disappeared,
except once? It's rather remarkable, isn't it? After all, the idea, if somebody says something,
and somebody else says, okay, let's do an experiment to see if that's right. You don't have to very,
be very bright. So how come it didn't happen all the time everywhere? Period.
He's speculating, even in Europe, there did happen, was it a fluke? And I think he gives the example of,
suppose an asteroid or a common crash in Paris in, you know, family year, he gives me 1150 or 1200 or
something. Then what? Whether it's with science that they never developed anywhere. He just sort of
raised it himself as a thought problem. Well, history is called the flukes. I'm trying to remember who
wrote that nice book about the plague. Some woman. And she mentions that this was spread by rats and
fleas or something. And 30 or 40% of the population of many countries in Europe died. And the next
generation had a lot of furniture. The standard of living went way up. So anyway, here's a list of
disasters and Martin Reese is the Royal Astronomer and has that book about the last hour or whatever. And I'm
making another longer list, but he has lots of obvious disasters like some high school student looks up the
genetic sequence for smallpox virus has been published. And now you can write a list of nucleotides and send
it somewhere. And they'll make it for about 50 cents or a dollar per nucleotide for so for a couple of
hundred dollars, you can make a virus or a few hundred. And so one possibility is that some high school
student makes some smallpox only gets it wrong and it kills everyone. So there are lots of lots of
disasters like that. And no one knows what to do about that because the the DNA synthesis machinery is
becoming less and less expensive and probably the average rich private high school could afford one. So
there are lots of other things that could happen. But one particular one is this graph which I just made up.
An interesting fact is that since 1950, when the first antibiotics started to appear, as I mentioned, I was a kid
in the 1940s, and penicillin had just hit the stands. And there wasn't much of it. And there was a researcher
lived a few blocks from us whose dog had cancer. And so its father, I don't know what you call the owner of a
dog, sneaked some penicillin out of the lab and gave it to the dog who died anyway. But he said, Well, nobody's
tried penicillin on cancer yet. Maybe it will work. And a lot of people were mad at him because he probably
cost some human its life. But he said he might have saved a billion humans their lives. So ethics, ethicists are
people who give reasons not to do things. And I'm not saying they're wrong. But it's a funny job.
So anyway, since that sort of thing happened, and medicine began to advance, people have been living one year
longer every 12. So it's 60 years since 1950. Another. So that's five of those six. So they're living six or seven
years longer now than they were when I was born. And somebody mentioned that that curve stopped the last few years.
For other reasons. But anyway, if you extrapolated that, you find that the lifespan is going to keep increasing.
How much we don't know. Another problem is that you might discover enough about genetics to get rid of most of the
serious diseases. Maybe just 20 or 30 genes are responsible for most deaths right now. And if you could fix those,
which we can't do yet, there's no way to change a gene in a person. Because invading all the cells is pretty
massive intervention. But we'll get around that. And then it might be that people suddenly start living 200 or 300
years. No, at some point, the population has to slow down. And so you have, you can only reach equilibrium with one child per
family, and probably less than that. And so all the work has to be done by two or 300 year olds. And let's hope they're good
and healthy. So anyway, I think it's very important that we get smart robots, because we're going to have to stem the
population. And I hope people will live longer and blah, blah, blah. And so these robots have to be smart enough to replace
most people. And how do you make something smart? Well, artificial intelligence is the field whose goal has been to make
machines that do things that we regard as smart or intelligent or whatever you want to call it. And the idea of seriously making
machines smart has roots that go back to a few pioneers like Leibniz who wrote about automata and that sort of thing. But the idea of
the general purpose computer didn't appear till the 1930s and 40s in some sense. The first form of the general purpose computer appears in
really in the 1920s and 30s with the work of a mathematician who, Amel Post at NYU, who I happen to never meet. But we had some friends in
common. And he had the idea of production rules and basically rule based systems and proved various theorems about them. Then Kurt Gertl showed
that if you had something like a computer or a procedure that had the right kinds of rules, it could compute all sorts of things. But there were
some things it couldn't compute, unsolvable problems. And that became an exciting branch of mathematics. And the star thinker in that field was
Alan Turing, who invented a very simple kind of universal general purpose computer. Instead of random access memory, it just had a tape which it
could write on and read and change symbols and would go back and forth. And if it's in state X at C symbol Y, it will print symbol Z over the
X and move to the left or right. And just a bunch of rules like that were enough to make a universal computer. And so from about 1936, it was sort of clear to a large
mathematical community that these were great things. And a couple of general purpose like computers, very simple ones, were built in the 1930s and more in the 1940s. And in the 1950s, big
companies started to make big computers, which were rooms full of equipment. But as you know, most programs could only do some particular thing. And none of them were very smart.
Whereas a human can handle lots of kinds of situations. And if you have one that that you've never seen before, there's a good chance you'll think of a new way to deal with that, and so forth. And so how do you make a machine that doesn't get stuck almost all the time?
And I like to use the word resourcefulness, although I left an R out of that one. Is there a shorter word? So here's a good example.
My favorite example of a situation where a person is born, or less, with a dozen different ways of dealing with something. And the problem that I imagined that you're dealing with is this. My favorite example is I'm thirsty.
So I see that glass of water, and I do that and get it. Actually, I am. On the other hand, if I were here, I would never in a whole lifetime do this. You never walk out a window by mistake.
We're incredibly reliable. So how do I know how far it is? And that slide shows you 12 different ways that your vision system, that's only your vision system, has to measure distances.
So gradients, if things are sort of blurry, then they must be pretty far away. That's sort of a foggy day outside. Here's a situation. If you assume those are both chairs of the same size, and you know that this chair is about twice as far away as that, although you don't well,
and you know how far away they are pretty much by the absolute size.
If you have two eyes that work well, then if something is less than 30 feet away, you can make a pretty good estimate of its distance by focusing both eyes on some feature, and your brain can tell how far apart your eyes are looking.
So there's 12 different things. It's more than you need. Lots of people are missing half of those. Lots of people have very poor vision in one eye.
Some people cannot fuse stereo images, even though both eyes have 20-20 vision, and in some cases, nobody knows why they can't do that.
I think I once took a test for being a pilot, and they wanted to be sure you could do stereo vision, which seemed very strange, because if you're an airplane and you're less than 30 or 40 feet away from something, it's too...
Then yes, you can't use stereo, you could use stereo, but it's too late.
So anyway, that's interesting. See if you can think of an example where a person has even more 12 of these, but it's pretty amazing, isn't it? That's more redundancy.
This is too hard to read, but somehow I found in an Aristotle essay the idea that you should represent things in multiple ways.
One person might describe a house as a shelter against destruction by wind, rain, and heat. Another might describe it as a construction of stones, bricks, and timbers, but a third possible description would say it was in that form, in that material, with that purpose.
So you see there's two different descriptions. One is the functional description. It's a shelter. The second one is a structural description of how it's made, and Aristotle says which is the better description, and he dismisses the material one or the functional one is not rather the person who combines both in a single statement.
And then I found a paragraph by Feynman who says every theoretical physicist who is any good knows six or seven different ways to represent exactly the same physics, and you know that they're all equivalent,
but you keep them all in your head hoping that they will give you different ideas for guessing. I should put more dots.
Anyway, that whole argument is to say that the interesting thing about people is that they have so many ways to do things and perceive things and think of things.
And in some cases we even know that there are different parts of the brain that are involved in one aspect or another of constructing those different representations or descriptions.
If you look at the, one of my favorite books weighs about 20 pounds. It's the book on the nervous system by Candle and Schwartz, and the index to that book is quite a lot of pages long, and it mentions 400 different structures in the brain.
So the brain is not like the, well, I shouldn't make fun of the liver, because for all I know, the liver has 400 different mini processes for doing things, but the brain has distinguishable areas that seem to perform several hundred different functions,
and with a microscope, at first they all look pretty much the same, but if you look closely you see different, slightly different patterns of how the most layers of the cortex of the brain, most parts of it have six layers,
and each has a population of different kinds of cells. There are a lot of cross connections up and down, and sideways to other, there are engine columns of between 400 and 1,000 cells, and you have a couple of million of those, and there are lots of differences between the columns in different areas, and we know some of the functions.
Most cases we don't know much about how any of them actually work, with the main exception of vision, where the functions of the cells in the visual cortex are fairly well understood at low levels, so we know how that part of the brain finds the edges and boundaries of different areas and textures and regions of the visual field.
But we do not know even a little bit about how the brain recognizes something as a chair and an overhead projector and CRT screen, and that sort of thing.
So, the kind of question that I got interested in was, how can you have a system which has a very large number of different kinds of computers, each of which by itself might be relatively simple or might not, I suppose,
and how could you put them together into a larger system which could do things like learn language and prove theorems and convince people to do things that they would never have dreamed of doing five minutes earlier and stuff like that.
Now, the first sort of things I was interested in was, in fact, how to make, how to simulate simple kinds of nerve cells, because in the 1950s, there was about almost 100 years, really more like 50 years,
of science discovering things about neurons and nerve cells, the axons and dendrites that they used to communicate with other neurons.
So, if you go back to 1890, you find a few anatomists discovering some of the functions of, or connections of neurons in the brain, and you find a few experimental physicists.
There was no oscilloscope yet, but there were very high-yane galvanometers which could detect pulses going along a nerve fiber, and by 1900, it was pretty clear that part of the activity in a nerve cell was chemical,
and part was electrical, and by 1920 or 1930, with the cathode ray tube appearing mostly because of television, but it became possible to do a lot of neurophysiology by sticking needles in brains.
The vacuum tube appears around 1900, and you can make amplifiers that can see millivolts and then microvolts, so in the beginning of the 20th century, there was lots of progress.
By 1950, we knew a lot about the nervous system, but we still don't know much about how you learn something in the brain.
It's quite clear that the things called synapses are involved, connections between two neurons become better at conducting nerve impulses under some conditions.
But no one knows how higher-level knowledge is represented in the brain yet, and the Society of Mind Book had a lot of theories about that, and in particular, there was a theory called K-Lines,
K-Lines, knowledge lines or something, that came partly from me and partly from a couple of other researchers named David Walts and Jordan Pollack, and that's a sort of nice theory of how neural networks might remember higher-level concepts.
And for some reason, although that kind of work is from around 1980, which is 30 years ago, it has not hit the neuroscience community.
So if you look at the Emotion Machine Book or the Society of Mind, in Amazon, you might run across a review by a neurologist named Richard Restak, who says that Minsky makes up a lot of concepts like K-Lines and micro-neems and stuff like that that nobody's ever heard of, and there's no evidence for them,
and he ignores the possibility that it isn't the nerve cells in the brain that are important, but the supporting tissues called glia, which hold the neurons up and feed them, and he goes on for a couple of insane paragraphs.
It's very interesting because it doesn't occur to him that you can't look for something until you have the idea of it, and so here's this 30-year-old idea of K-Lines, and go and ask your favorite neuroscientist what it is, and he said,
oh, I think that's some AI thing, but where's the evidence for it?
What do you suppose is my reaction to that?
Who's supposed to get the evidence?
So it seems to me that there's a strange field in neuroscience, which is that it doesn't want new ideas unless you've proved them.
So I try to have conversations with them, but get somewhat tired of it.
Anyway, but in this course, I'm taking the opposite approach, which is that we don't want a theory of thinking.
We want a lot of them because probably psychology is not like physics.
What's the most wonderful thing about physics?
The most wonderful thing is that they have unified theories.
There wasn't much of a unified theory until Newton, and he got these three laws.
Wonderful laws.
One was the gravitational idea that things, bodies attract each other with a force that's the inverse square of the distance between them.
Another is that kinetic energy is conserved.
Equal reaction is equal and opposite.
If two things collide, they transfer equal amount of momentum to both.
There was a little problem up to Newton's time.
Galileo got some of those ideas.
My impression from reading him is that he has a dim idea that there are two things around.
There's kinetic energy, which is mv, and there's momentum is mv, and there's kinetic energy, which is mv squared.
He doesn't have the clear idea that there are two different things here, and you can't blame him.
You wouldn't think that two quantities would combine in two different ways to make two important different concepts.
Well, that got clear to Newton somehow, and Galileo is a bit muddled, although he gets almost all the consequences of those things right.
But he doesn't get the orbits and things to come out.
Anyway, what happened in artificial intelligence, like most fields, is that people said,
well, let's try to understand thinking and psychology, and let's use physics as our model.
And so what we want is to get a very small number of universal laws, and a lot of psychologists struggled around to do that.
And then they gradually separated so that there were some psychologists, like Bill Estes,
who worked out some very nice mathematical rules for reinforcement-based learning.
Got a simple rule, if you designed an experiment right, it predicted pretty well how many trials it would take,
a rat or a pigeon or a dog or whatever, to learn a certain thing from trial and error.
And Estes got a set of four or five rules, which looked like Newton's laws.
And if you designed your experiment very carefully and shielded the animal from noise and everything else,
which is what a physicist would do for a physics experiment, the reinforcement theories got some pretty good models of how to make a machine learn.
But they weren't good enough.
So here's a whole list of things that happened in the early years of cognitive psychology,
when people were trying to make theories of thinking, and they were imitating the physicists.
By physics envy to borrow a term of Freud, the idea is, can you find a few simple rules that will apply to very broad classes of psychological phenomena?
And this led to various kinds of projects.
Lots of neural network and reinforcement and statistical-based methods led to learning machines that were pretty good at learning in some kinds of situations.
And they're becoming very popular, but I don't like them because if you have a lot of variables, like 50 or 100,
then to use a probabilistic analysis, you have to think of all combinations of those variables.
Because if two of them are combined in something like an exclusive or a manner, I just put the light pen in a pocket.
It's either in a left pocket or a right pocket. Can't be both. That's an XOR.
That will cause a lot of trouble through a learning machine, and if there are 100 variables, there's no way you could decide which of the two to the 100th Boolean combinations of those variables you should think about.
And so lots of statistical learning systems are good for lots of applications, but they just won't cut it to solve hard problems where the hypothesis is a little bit complicated and has seven or eight variables with complicated interactions.
So most statistical learning people assume that if you get a lot of partial ones, then you can look for combinations of ones that have high correlations with the result.
Then you can start combining them and things get better and better.
However, if mathematically, if in effect you're looking for, it depends on the exclusive or of several variables, there's no way to approach that by successive approximations.
If any one of the variables is missing, there won't be any correlation of the phenomenon with the others.
Anyway, that's a long story, but I think it's worth complaining about because almost all young people who start working on artificial intelligence look around and say, what's popular?
Statistical learning. So I'll do that. That's exactly the way to kill yourself scientifically.
You don't want to get the most popular thing. You want to see what am I really good at that's different and what are the chances that that would provide another thing.
So you end of long speech.
Another problem in the last 30 years, and I'm sort of, as you'll see during my lectures, I think a lot of wonderful things happened between 1950 when the idea of AI first got articulated in the 1950s.
And then the 20 years after that from 1960 to 1980, a lot of early experiments, and I'll show you some of them, looked very promising.
In fact, they may be, here we go.
1961. Jim Slagle was a young graduate student here at MIT.
He was blind. He had gotten some retinal degeneration thing in his first or second year of high school. He was told that he would lose all his vision and there was no treatment or hope.
So he learned Braille while he could still see. And when he got to MIT, he was completely blind.
But there was a nice big parking lot in Technology Square, and he would ride a bicycle and people like Sussman and Winston and whoever was around would yell at him,
telling him where the next obstacle would be, and Jim got better and better at that, and nothing would stop him.
And he decided he would write a program that, oh, I wrote a program that would take any formula and find its derivative.
It was really easy because there are just about five rules.
Like if there's a product UV, then you compute U times the derivative of V plus V times, you know, U, T, V plus V, D, U.
So I wrote a 20-line list program that did all the algebraic expressions.
And what it would do is put D's in and then in the right place, and then it would go back through the expression again.
Wherever it saw a D, it would do the derivative of the thing after that, and nothing to it.
So Slagle said, well, I'll do integrals. And we all said, well, that's very hard. Nobody knows how to do it.
And in fact, in Providence at the home of the American Mathematical Society, there is a big library called the Bateman Manuscript Project,
which has been collecting all known integrals for 100 years.
And when anybody finds a new integral that they can integrate in closed form,
they send the formulas to the Bateman Manuscript Project, and some hackers there have developed ways to index it.
So if you had an integral and you didn't know how to integrate it, you could look it up.
And that was pretty big.
I should say that Slagle succeeded in writing a program that managed to do all of the kinds of integrals that one usually found on the first year calculus course at MIT,
and got an A in those, couldn't do word problems.
And the uncanny thing is that if it was a problem that usually took an MIT student five or 10 minutes, Slagle's program would take five or 10 minutes.
It's running on a IBM 701 with 20 millisecond cycle time.
It's incredibly slow.
You can type almost that fast.
And 16K of words of memory.
So there's no significance whatever to this accident of time.
It would now take a microsecond or so, be a thousand million times faster than a student.
Quite remarkable.
I don't have a slide.
Joel Moses then, Slagle went and graduated.
Joel Moses was another student who was, is he a provost now or what?
What?
He got tired of it.
A terrific student, and he set up a project called Maxima for Project Max Symbolic, something or algebra.
And got people, several people all over the country, working on integration.
And at some point, a couple of them, Bobby Kavanis and forget the other one, found a procedure that could in fact integrate everything,
every algebraic expression that has a, can be integrated in closed form.
I forget the couple of constraints on it.
And that became a widely used system.
It ultimately got replaced by Stephen Wolfram's Mathematica.
But Maxima was sort of the world class symbolic mathematician for quite a few years.
And Moses mentioned to me, he had read Slagle's program thesis.
And it took him a couple of weeks to understand the two pages of, or three pages of Lisp that Slagle had written.
Because being blind, Slagle had tried to get the thing into as compact a form as possible.
But that's symbolic.
It's too easy.
It wasn't a more ambitious one, which was three years later.
Dan Bobrow, who is now a vice president doing something at Xerox.
And it solved problems like this.
The gas consumption of my car is 15 miles per gallon.
The distance between Boston and New York is 250 miles.
What is the number of gallons used on a trip between Boston and New York?
And it chomps away and solves that.
It has about 100 rules.
It doesn't really know what any of those words mean.
But it thinks that the word is is equals.
The distance between doesn't care what Boston and New York is.
It has a format thing, which says the distance between two things.
And it never bothers to, you see, because the phrase Boston and New York occurs twice in the example.
It just replaces that by some symbol.
It was fairly remarkable.
And generally, if you had an algebra problem and you told it to Bobrow,
Bobrow could type something in and it would solve it.
If you typed it in, it probably wouldn't.
But it was, you know, it had more than half a chance or less, about half a chance.
So it was pretty good.
And if you look at an out-of-print book, I compiled called Semantic Information Processing.
Most of Bobrow's program is in that.
So that's 1964.
I'll skip Winograd, which is perhaps the most interesting program.
This was a program where you could talk to a robot that...
I don't have a good picture on the slide.
But there are a bunch of blocks of different colors.
They're all cubes in the rectangular blocks.
And you can say, which is the largest block on top of the big blue block?
And it would answer you.
And you could say, put the large red block on top of the small green block.
And it would do that.
And Winograd's program was, of course, a symbolic one.
We actually built a robot, and I guess we built it second.
Our friends at Stanford built a robot, and they imported Winograd's program.
And they had the robot actually performing these operations that you told it to do by typing.
And it was pretty exciting.
My favorite program in that period was this one, because it's so psychological.
This is called a geometrical analogy test, and it's on some IQ tests.
A is to be as C is to which of the following five.
And Evans wrote a set of rules, which were pretty good at this.
It did as well as 16-year-olds, and it picks this one.
And if you ask it why, it says something like...
I don't have a reason that...
It moves the largest object down or something like that.
It makes up different reasons, but...
So, you see, in some sense, we're going backwards in age,
because we're going from calculus to algebra to simple analogies.
Oh, there it is.
That's one where the largest object moves down.
I don't know why I have two of them.
These are for another lecture.
Okay.
So that was a period in which we picked problems that people considered hard,
because they were mathematical.
But when you think about it more, you see, well, those math things are just procedures,
and once you know what Laplace and Gauss and those mathematicians Newton and people did,
you can write down systematic procedures for integrating
or for solving simultaneous algebraic constraint equations or things like that.
And so there's very little to it.
So in some sense, if you look at what you're doing in math in high school, in education,
you're going from hard to easy.
It's just that people aren't...
Most people aren't very good at obeying really simple rules,
because it's so hideously boring or something.
So we gradually started to ask, well, why can't we make machines understand everyday things
and the things that everyone regards as common sense,
and people can do so you don't need machines to do them?
One of my favorite examples is, why can you pull something with a string but not push?
And there's been a lot of publicity recently about that interesting program
that I've written a group at IBM called Watson,
which is good at finding facts about sports people and celebrities and politics and so forth.
But there's no way it could understand why you could pull something with a string but not push.
And I don't know of any program that has that concept or way of dealing with it.
So that's what I got interested in.
And starting around maybe the middle 1970s or late 1970s,
several of us started to stop doing the easy stuff
and try to make theories of how you would do the kinds of things that people are uniquely good at.
I don't know of animals.
Well, I don't know.
I'm sure a monkey wouldn't try to push anything with a string.
Maybe it does it very quickly and you don't notice.
And one aspect of common sense thinking is going right back to that idea of vision having a dozen different systems.
What I think is that whatever a person normally is doing,
they are probably representing it in several different ways.
And here's an actual scene of two kids named Julie and Henry who are playing with blocks.
It's pretty hard to see those blocks.
And you can think that Julie is thinking seven thoughts.
I'd like to see a longer list, maybe a good essay would be to take a few examples and say,
what are the most common micro worlds?
See physical, social, emotional, mental, instrumental, whatever that is, visual, tactile, spatial.
She's thinking all these things. What if I pulled out that bottom block?
You can't see the tower very well.
Should I help him or knock his tower down?
How would he react?
I forgot where I left the arch shaped block.
That was real.
It's somewhere over here.
But I don't think maybe it's that.
I don't know.
I remember when it happened, she mentioned that she reached around and it wasn't where she thought it was.
So common sense thinking involves this.
In most cases, I think several representations.
I don't know if it's as many as seven or maybe 20 or what, but that's the kind of thing we want to know how to do.
Okay, I think I'll stop and we'll discuss things.
But in the next lecture, I'll talk about a model of how I think thinking works.
What's the difference between us and our ancestors?
We know we have a larger brain.
But if you think about it, if you took the brain that you already had in say,
I remember the name of the little monkey that looks like a squirrel, jumps around in trees.
Anybody know it?
What?
Maybe.
It's a squirrel like thing.
I didn't know it was a monkey till you took a close look.
Maybe.
Lemur?
I don't, I forget.
Anyway, if you just made the brain bigger, then the poor animal would be slower and heavier and would need more food and take longer to reproduce.
The joke about difficulty to give birth.
I don't know if any animal has the problem that humans have.
A lot of people die.
And so on.
So how did we evolve new ways to think and so forth?
And my first book, The Society of Mind, had this theory that maybe we evolved in a series of higher and higher levels or management structures built on the earlier ones.
And this particular picture suggests that I got this idea from Sigmund Freud's early theories.
There's been a lot of Freud bashing recently.
You can look on the web.
I forget the authors, but there are a couple of books saying that he made up all his data.
And there's no evidence that he ever cured anyone and that he lied about all the data mentioned in his 30 or 40 books and so forth.
Yes, right.
But the funny part is that if you look at his first major book, 1895, called The Interpretation of Dreams, it sort of outlines this theory that most of thinking is unconscious and it's processes you can't get access to.
And it has a little bit about sex, but that's not a major feature.
And it's just full of great ideas that the cognitive psychologists finally began to get in the 1960s again and never give credit to Freud.
So he may well have made up his data, but if you have a very good theory and nobody will listen to you, what can you do?
His friend Rudolf Fleece listened to him.
And there was another paper on how the neurons might be involved in thinking, which was also written around 1895, but never got published till 1950 by, forget who, called Project for a Scientific Psychology.
And it's full of ideas that if they had been published, might have changed everything.
Because anyway, what's on your mind?
What would you like to hear about?
Who has another theory?
Great.
So earlier you talked a little bit about how we don't really see the neuroscience, all the things like K-Lines, etc.
Do you think it's because they're just really hard to find or no one's actually looking for them?
S. Dex Review says he uses vague ill-defined terms like K-Line and Microneme and a couple of others, and Frame and so forth.
They're very well-defined.
When he talks about neurotransmitters, it's as though he thinks that chemical has some real significance.
Any chemical would have the same function as any other one, provided there's another receptor that causes something to happen in the cell membrane.
So you don't want to regard acetylcholine or epinephrine as having mental significance.
It's just another pulse, but very low resolution.
And yes, a neurochemical might affect all the neurons a little bit and raise the average amount of activity of some big population of cells and reduce the average activity of some others.
But that's nothing like thinking.
That's like saying, in order to understand how a car works, what's the most insulting thing I could say?
Or to understand how a computer works, you have to understand the arsenic and phosphorus and or what's the other one?
You have to understand these atoms that are, what?
Well, that's the matrix.
So there are these one part in a million impurities, and that's what's important about a computer, isn't it?
The fact that the transistor has gain and so forth.
Well, no.
The trouble with the computer is the transistors.
That's why practically every transistor in the computer is mated to another one in opposite phase to form a flip-flop whose properties are exactly the same except one in a quadrillion times.
In other words, everything chemical about a computer is irrelevant.
And I suspect that almost everything chemical about the brain is unimportant except that it causes, it helps to make the columns in the cortex which are complicated arrangements of several hundred cells work reliably.
Whereas the neuroscientist is looking for the secret in the sodium.
When a neuron fires, the important thing is that that lets the sodium in and the potassium out or vice versa.
I forget which.
At 500 millivolts.
Really quite a colossal event.
But it has no significant.
It's only when it's attached to a flip-flop or to something like a K line which has an encoder and decoder of a digital sort every few microns of its length that you get something functional.
So the trouble is the poor neuroscientist started out with too much knowledge about the wrong thing.
The chemistry of the neuron firing is very interesting and complicated and cute.
And in the case of the electric eel, you know what happened there.
The neuron synapse, it got rid of the next neuron and it just, in the electric eel, you have a bunch of synapses or motor end plates, they're called, in series.
So instead of half a volt, if you have 300 of those, you get 150 volts.
I think the electric shock that an electric eel can give you is about 300 volts.
And this can cause you to drown promptly if you are in the wrong way when it happens to bump into you.
I don't know why I'm rambling this way.
You're welcome to study neuroscience, but please try to help them instead of learn from them.
And they just don't know what a K line is.
And that's a paper that's been widely read, published in 1980, and RESTAC says, ill-defined.
And I guess he couldn't understand it.
Yeah, yeah.
Why instead of trying to make the neuroscientists, like trying to find this in the human mind,
why don't we just, like, as computer scientists, program, like, the K lines and try to prove that all this is the human mind and the claim of the producer?
Like, why is that not widely spread into the computer scientist field?
Well, I'm surprised how little has been done.
Mike Travers has a thesis, Tony Hearn.
There are three master's theses on K lines.
They sort of got them to work to solve some simple problems.
But I'd go further.
I've never met a neuroscientist who knows the pioneering work of Newell and Simon in the late 1950s.
So there's something wrong with that community.
They're just ignorant.
They're proud of it.
Oh, well.
I spent some time learning neuroscience when I was...
I once had a great stroke of luck when I was a...
I guess I was a junior at Harvard and there was a great new biology building that was just constructed.
You probably know it's a great big thing with two rhinoceroses.
What are those two huge animals?
So this building was just finished and half occupied because it was made with the future.
So I wandered over there and I met a professor named John Welsh.
And I said, I'd like to learn neurology.
And he said, great, well, I have an extra lab.
Why don't you study the crayfish claw?
And I said, great.
So he gave me this lab, which had four rooms and a dark room.
And a lot of equipment and nobody there.
And he had worked on crayfish.
So there was somebody who went every week up to Walden Ponder somewhere and caught crayfish and bringing them back.
And I was a radio amateur hacker at the time.
So I was good at electronics.
So I got my crayfish and Welsh showed me how to.
The great thing about this preparation is you can take the crayfish and if you claw and if you hold it just right, go snap.
It comes off, grows another one.
It takes a couple of years.
And then there's this white thing hanging out, which is the nerve.
And it turns out it's six nerves, one big one and a few little ones.
And if you keep it in ringer solution, whatever that is, it can live for several days.
So I got a lot of switches and little inductors and things and made a gadget and mounted this thing with six wires going to these nerves.
And then I programmed it to reach down and pick up a pencil like that and wave it around.
Well, that's obviously completely trivial.
And all the neuroscientists came around and gasped and said, that's incredible.
How did you do that?
They had never thought of putting the thing back together and making it work.
Anyway, it was, I always reminding myself that I'm the luckiest person in the world because every time I wanted to do something, I just happened to find the right person.
And they'd give me a lab.
I got an idea for a microscope and there was this great professor, Purcell, who got the Nobel Prize after a while.
And he said, that sounds like it would work.
Why don't you take this lab?
It was in the Jefferson.
Anyway, yeah.
Part of the reason that you don't see experimental neuroscience on things like K-lines is that neurons are long and thin.
So if you want to do an experiment to actually measure a real neural network, you have to trace structures with roughly maybe tens of nanometer resolution.
But you need to trace them over what might be a couple or even tens of millimeters to charge a week.
And you need to do this for thousands and thousands of neurons before you could get to the point of seeing something like a K-line and understanding it.
So it's just a massive data acquisition and processing problem.
But they're doing that.
They're trying to try to.
But they don't know what to look for.
Maybe you don't have to do so much.
Maybe you just have to do a few sections here and there and say, well, look, there were 400 of these here.
Now there's only 200.
It looks like this is the same kind.
Maybe you don't have to do the whole brain.
Even getting a single neuron is because you might get down to, you need to be looking at electron micrographs of grains that are sliced at about 30 nanometer slices.
So even just having a single person reconstruct a single neuron might take weeks.
Well, I don't know.
Maybe a bundle of K-lines is a half a millimeter thick.
Oh, so you actually do some larger scale structure to start looking at.
Yeah.
Why not?
I don't think they have no idea what to look for.
I could give you 20 of those in five minutes, but nobody's listening.
What scale?
I don't know.
I mean, they know what the neurons look like.
So you know what to look for if you're saying there's a neuron that level.
I'm saying you may only have to look at the white matter.
Oh, yeah.
Ignore the neurons because the point of K-lines is where do these go and what goes into them and out.
I don't know.
It's just this idea, let's map the whole brain, 100 billion things.
And then people, I rest exit.
Oh, and there's a thousand supporting cells for each neuron.
He's just glaring in the obscurity of it rather than trying to contribute something.
Anyway, if you run into him, give him my regards.
I really wonder how somebody can write something like that.
Yes.
Excuse my ignorance, but what is a K-line?
The idea is that suppose one part of the brain is doing something and it's in some particular state
that's very important, like I don't know, like I've just seen a glass of water.
Then another part of the brain would like to know there's a glass of water in the environment.
And I've been looking for one, so I should try to take over and do something about that.
Now at the moment, there's no theory of what happens in different parts of the brain
for a simple thing like that to happen.
No theory at all, except they use the word association or they talk about what are the purposeful neurons.
Goal, forget.
Okay, so my theory is that there are a bunch of things which are massive collections of nerve fibers,
maybe a few hundred or a few thousand.
And when the visual system sees an apple, it turns on 50 of those wires.
And when it sees a pear, it turns on a different hundred or 50 of those wires,
but about 20 of them are the same, so forth.
In other words, it's like the edge of a punched card.
Have you ever seen a card-based retrieval system?
If you have a book that has, suppose it's about physics and biology and Sumatra.
And a typical five-by-eight card has 80 holes in the top edge.
So what you do is, if it's Sumatra, you punch eight of these holes at random, a particular set.
They're assigned to the Sumatra, and then if it's, I forget what my first two examples were,
but you punch eight or ten holes for each of the other two words.
So now there are 24 punches.
Only probably four or five of them are duplicates, so you're punching about 20 holes.
And now, if something is looking for the cards that were punched for those three things,
even if there are 30 or 40 other holes punched in the card,
you stick your 20 wires through the whole deck and lift it up,
and only cards fall out that had those three categories punched for.
So you see, even though you had 80 holes,
you could punch combinations of up to a million different categories into that,
and if you have to put a bunch of wires through,
you'll get all of the ones that were punched for those categories, the categories you're looking for,
and you might get three or four other cards that will come down also
because all of the eight holes were punched for some category by accident.
Do you get the picture? I'll send you a reference.
It was invented in the early 1940s by a Cambridge scientist here named Calvin Moores,
and was widely used in libraries for information retrieval until computers came along.
But anyway, that's the sort of thing you could look for in a brain
if you had the concept in your head of Zato coding,
but I've never met a neuroscientist who ever heard of such a thing.
So you have this whole community which doesn't have a set of very clear ideas
about different ways that knowledge or symbols could be represented in neural activity.
So good luck to them when they get their big map.
They'll still have to say, what do I do with a hundred billion of these intricate accounts?
Yeah?
What are your thoughts about the current artificial intelligence research at MIT,
such as Winston's Legendes project?
Winston is just about one of the best ones in the whole world.
I don't know any other projects that are trying to do things on that higher level of common sense knowledge.
He's just lost a lot of funding, so one problem is how do you support a project like that?
Have you followed it? I don't know if there's a recent summary of what they're doing.
We used to write a new book every year called The Progress Report.
The nice thing is that we had a very good form of support from ARPA, or DARPA,
which was every year we'd tell them what we had done.
They didn't want to hear what we wanted to do, and things have turned the opposite.
So what would happen is every year we'd say we did these great things, and we might do some more.
It went on for about 20 years, and then it fell apart.
One thing that's a nice story, there was a great liberal senator, Mike Mansfield,
and unfortunately he got the idea that the Defense Department was getting too big and influential.
So he got Congress to pass a law that ARPA shouldn't be allowed to support anything that didn't have direct military application.
Congress went for this, and all of a sudden a lot of research disappeared, basic research.
It didn't bother us much because we made up applications and said,
well, this will make a military robot that will go out and do something bad.
I don't remember ever writing anything at all, but anyway around 1980 the funding for that sort of thing just dried up because of this political accident.
It was just an accident that ARPA, mainly through the Office of Naval Research, was funding basic research.
That was a bit of history.
If you look back at the year 1900 or so, you see people like Einstein making these nice theories.
But Einstein wasn't a very abstract mathematician, so he had a mathematician named Hermann Weill polishing his tensors and things for him.
And Hermann Weill's son, Joe, was at the Office of Naval Research in my early time.
That office had spent a lot of secret money getting scientists out of Europe while Hitler was marching around and sending them to places like Princeton and other forms of heaven in Cambridge.
And again, one of the reasons I was lucky is that I was here and all these, you know, if you had a mathematical question you could find the best mathematician in the world down the block somewhere.
And Joe Weill was partly responsible for that, and the ONR was piping all that money to us for work on early AI.
So it was a very sad thing that maybe the most influential liberal in the US government actually ruined everything by accident.
ARPA changed its name to DARPA. It was Advanced Research Projects Agency, and it had to call itself Defense Advanced Research Projects Agency.
Well, Christianity wiped out science. That might happen tomorrow. Only choose your religion.
It's a hard problem. The number of people working on advanced ideas in AI has gotten smaller and smaller as the, right now the around 1980 rule-based systems became popular.
There are lots of things to do. Right now, statistical-based inference systems are becoming popular. And as I said, these things are tremendously useful, but the problem is if you have a statistical system, the important part is guessing what are the plausible hypotheses and then making up the, then finding out how many instances of that are correlated with such and such.
So it's a nice idea, but the hard problem is the abstract symbolic problem of what sets of variables are worth considering at all when there are a lot of them.
So to me, the most exciting projects are the kind that Winston is developing for reasoning about real-life situations. And the one that Henry Lieberman, would you stand up, Henry?
Lieberman runs a world-class group that's working on common sense knowledge and informal reasoning. And it seems to me that that's the critical thing that all the other systems will need.
In the meantime, there are people working on logical inference, which has the same problem that statistical inference has, namely, how do you guess which combinations of variables are worth thinking about?
Then it seems to me that the statistics isn't so important. In fact, there's a great researcher named Douglas Lenat in Austin, Texas, who once made an interesting AI system that was good at making predictions and guessing explanations for things.
And it was sort of like a probabilistic system. It had a lot of hypotheses, and every time one of them was useful in solving a problem, it moved it up one on the list.
So Lenat's thing never used any numbers. It didn't say, this is successful .73 of the time, and now it's successful .7364825 of the time.
What it would do is, if something was useful, it would move it up past another hypothesis, every now and then, it would put a new one in.
Well, if you're trying to solve a problem, what do you need to know? You want to know what's the most likely to be useful one, and try that.
You don't care how likely it is to be useful, as long as it's the most, right?
I mean, if it's one in a million, maybe you should say, I'm getting out of here. I shouldn't be working in this field at all, or get a better problem.
But Lenat's thing did rather wonderfully at making theories by just changing the ranking of the hypotheses that it was considered. No numbers.
It did something very cute.
It gave it examples of arithmetic, and it actually, this is a rather long effort, and it actually learned to do some arithmetic, and it invented the idea of division, and the idea of prime number, which was some number that wasn't divisible by anything.
It decided that nine was a prime, didn't do much harm, and it crept along, and it got better and better, and it invented modular arithmetic by accident at some point, and it's a PhD thesis.
A lot of people didn't believe this PhD thesis because Lenat lost the program tape.
So he was under some cloud of suspicion for people thinking he might have faked it, but who cares?
Anyway, I think there's a lesson there, which is that let's start with something that works, and then if it's really good, then hire a mathematician who might be able to optimize it a little.
But the important thing was the order, and a good statistical one might waste a lot of time because here's this one that's 0.78, and here's this one that's 0.56, and it's the next one down, and you get a lot of experience, and it goes up to 0.57 and 0.58, and it never, you know, might be a long time before it gets past the other one because you're doing arithmetic.
Whereas in Lenats, it would just pop up past the other one, and then it would get tried right away, and if it were no good, it would get knocked down again. Who cares?
So it's a real question of, I don't know, mathematics is great, and I love it, and a lot of you do, but there should be a name for when it's actually slowing you down and wasting your time because there's a better way that's not formal.
There are people who know the price of everything and the value of nothing.
Yes, that's very nice.
I know you're also a musician, so I have a music related question. What do you think is the role of music? Why do all cultures have it?
I have a paper about it. Oh, okay.
I've been trying to revise it, actually, but it's a strange question because there is music everywhere.
On the other hand, I have several friends who are A musical, and so when I have this theory that music is a way of teaching you to represent things in their orderly fashion and stuff like that,
well, I have three of my colleagues who aren't musical, but they dance, so it may be that I don't know the answer.
The first theory in my paper is that when you have a lot of complicated things happening, then the only way to learn is to represent things that happen and then look at the differences between things that are similar and then try to explain the differences.
Right? I mean, what else is there? Maybe there's something else.
So in order to become intelligent and understand things, you have to be able to compare things, and to me the most important feature of what's called music is that it's divided into measures.
Ba-ba-ba, ba-ba-ba, ba-ba-ba, ba-ba-ba.
And measures are the same number of beats or whatever they are, and so now you can say da-da-da-da, da-da-da, da-da-da, da-da-da.
What's the difference? You change the eighth notes in the second one, the last four eighth notes, no, the two before last to a quarter note.
So you're taking things that were in different times, and you're superimposing those times, and now you can see the difference.
And the reason you can see the difference is that you have things called measures, and the measures have things called beats, and so things get knocked into very good frames.
Now there's some Indian music which has 14 measures for a phrase, and some of the measures go seven and five, and I can make no sense of that stuff whatever, and I've tried fairly hard but not very.
So I don't understand how Indians can think.
Any of you can handle Indian music?
I just want to add on what you said about this.
My favorite quote from your paper on music-minded meaning is the one about what good is music, about how kids play with blocks to learn about space, and people play with music to learn about time.
And I think in that sense both music and dance are different ways that people can arrange things in time.
And in the sense like in proposatory music and in proposatory movement are both ways of different blocks if you will in time as opposed to space.
Yeah, my friends who seem a-musically, maybe there's something different about their cochlea, or maybe they have absolute pitch in some sense, which is a bad thing to have.
Because if you're listening to a piece composed by a composer who doesn't have absolute pitch, then you're reading all sorts of things into the music that shouldn't be there, and the opposite would be true.
I read music criticism sometimes, and maybe the reviewer says, and after the second and third movement, he finally returns to the initial key of E flat major. What a relief.
Well, I once had absolute pitch for a couple of weeks, because I ran a tuning fork in my room for a month.
And I didn't like it, because you can't listen to Bach anymore.
Oh, well. It's a good question. Why do people like music? And I don't know any other paper like mine. If you ever find one, I'd like to see it, because if you go to a big library, there are thousands of books about music.
And if you open one, it's mostly Berlioz complaining that somebody wouldn't give him enough money to hire a big enough chorus.
I've found very few books about music itself. Yes.
Do you think that having a body is a necessary component of having a mind?
Could you do just as well a simulated creature?
Oh, sure. You could have all things.
Simulation? I think a mind that's not in a world wouldn't have much to do. It would have to invent the world.
And I don't see why it couldn't, but you might have to give it a start, like the idea of three or four dimensions.
Can't you? What happens if you sit back and just think for a while?
You wouldn't know if your body had disappeared, would you?
There are also some strange ideas about existence and...
Why do you think there's a world?
One of the things that bugs me is people say, well, who created it?
And that can't make any sense, because this is just a possible world.
Suppose there are a whole lot of possible worlds, and there's one real one.
How could you ever, how could you possibly know which one you're in?
And then you could say, well, didn't someone have to make it?
And what's the next thing you'd ask?
Well, who made the maker?
So the body-mind thing seems to me that once you have a computer, it can be its own world.
The program can spend half the time simulating a world and half the time thinking about what it's like to be in it.
Yeah?
Yes, it's an empty concept.
It's all right to say this bottle exists because let's say this bottle is in this universe.
But what would it mean to say the universe exists?
The universe is in the universe?
So there's something wrong with thinking about, so there are only possible worlds.
There's no, it doesn't make any sense to pick one of them out and say that's the real one.
Yeah, but existence is relative.
Yeah, you don't say this is the world I'm in, but you shouldn't say that doesn't mean it exists.
Like, two is in the set of even numbers.
What's the set of even numbers in?
It doesn't stop anywhere.
Yeah, lots of words.
So is mathematics or is it only worlds?
But physics explains the current world or are there no variables to use at this time?
Well, you can't tell because five minutes from now, everything might change.
So nothing ever explains anything.
You just have to take what you've got and make the best of it.
Yeah?
Solutions are between systems, knowledge, and visual intelligence.
Which knowledge?
Like systems, basically, in general.
Well, there are people who talk about systems theory, but I'm not sure that it's well-defined.
Artificial intelligence means, to me, making a system that is resourceful and doesn't get stuck.
And so if you have a system, and also it's a, how do you put it?
Some definitions are not stationary.
Like, what's popular?
Popular is what's popular now.
There isn't any such thing as popular music in terms of the music.
I know there were, there was once a little department called Systems Analysis at Tufts,
which had a couple of rather good philosophers trying to make general theory of everything.
And they were writing nice little papers and it got, it moved along.
But then there was a Senator McCarthy you've probably heard of, and he announced that,
he had evidence that the, one of the principal investigators had slept with his wife before they were married.
Well, Tufts was very frightened at this and abolished that department.
And Bill Schutts went to California and started Eselin and had a good time for the next 50 years.
I don't know, more stories.
Yeah.
Kind of was an extension of the body and mind question.
It seems to me like we as humans, we learn a lot from just interacting with the environment.
Like language and hearing, being spoken, we speak it, you know, we see things, we touch things.
But it, as far as I know, a lot of the efforts in artificial intelligence so far have been
confined to the computer that does not go out into the real world,
and direct doesn't kind of turn to bitlessly learn new things.
Well, here's the problem.
I look over at Carnegie Mellon and there are some nice projects.
And the most popular one is robot soccer.
And here are these little robots kicking a ball around.
They're Sony, what are they called?
Yes, the Sony iBos.
Sony stopped making the iBos, but it respected Carnegie and it made a little secret stash of iBos to send to Carnegie when the present ones break.
But my impression of AI projects that have robots is that they do less, less, less than projects that don't.
The reason is, if you have a robot like ASIMO, made by Sony, no, Honda,
ASIMO can get in the backseat of a car with some effort, usually falls over.
However, if you simulate a stick figure in a computer getting into a stick figure of a car,
then you can make it learn to do that and get better and better.
And so all AI projects without robots are way ahead of all AI projects with robots.
And the profound reason is that robots are usually expensive and they're always being fixed.
We have five students and the robot is being fixed.
I don't know what they're doing, but they have to wait.
Whereas if you have a stick figure robot, then you can just run it on this,
although it might be a little slower than your mainframe.
Probably not.
Here's the theory that I just thought of.
The idea of the body as a scene in abstract, basically a mechanism for input output.
It's a set of sensors from which our brains can get information about the world
and a set of actuators in which we can display our state.
In that light, it's almost as if our brains are really independent on our body itself.
It can adapt to any sort of body if we haven't hooked it up that way.
It just so happens that we've been hooked up to this body since birth,
that we have such good mental models of how to use this body.
I guess an example from experiments that support this theory might be
how when people have limbs amputated,
it takes them a while to forget that they have the limb
because their mental models still exist and their mental models don't go away overnight.
Also, I guess they train monkeys to control robot arms with their brains.
Sure.
Well, but it just seems to me that a large amount of our brain is involved
with highly evolved locomotion mechanisms.
And as I said, when you're sitting back with your eyes closed in a chair
thinking about something, then it's not clear how much of that machinery is important.
But it might be that I have a strange paper on...
I don't know if it's...
I'm trying to remember its name. It's called...
I can actually get a...
I can't remember the name of the title.
Oh, I give up.
The idea is that maybe in the older theories of psychology,
everything is learned by experience in the real world.
So conditioning and reinforcement and so forth.
In this theory, I call internal grounding.
I make a conjecture.
Suppose the brain has a little piece of nerve tissue,
which consists of a few neurons arranged to make...
not a flip-flop, but a...
what would you call a three or a four-flop?
A flip-flop with three or four states.
Let's say three.
If you put a certain input, it goes from...
I couldn't find the chalk.
So here are three states.
And here's a certain input.
That means if you're in that state, you go to this.
And if you pop that input again, it does this.
And if you say, go counterclockwise, it goes...
So three of them get you back where you were.
But if I go this, this, and that,
that would mean to go like this, this, and back.
So this would be...
that means that's equivalent to just going one.
Get the idea?
Imagine that there's a little world inside your brain,
which is very small and only has three states.
And you have actions that you can perform on it.
And you have an inner eye,
which can see which of the three points of that triangle you're on.
Then you could learn by experience
that if you go left, left, left, you're back where you were.
But if you go left, right, left, right, you're back where you are.
And if you go left, left, right, that's like going one left.
In other words, you could imagine a brain
that starts out before it connects itself to the real world.
It starts by having the top level of the brain
connected to a little internal world,
which just has three or four states.
And you get very good at manipulating that.
Then you add more sensory systems to the outer world,
and you get to learn ways to get around in the real world.
So I call that the internal grounding hypothesis.
And my suggestion is maybe somewhere in the human brain,
there's a little structure that's somewhat like that,
which is used by the frontal part of the cortex
to make very abstract ideas.
You understand, the more abstract an idea is,
the simpler and more stupid an elementary it is.
Abstract doesn't mean hard.
Abstract means stupid.
Real things like this are infinitely complicated.
So we might have, and I wouldn't dare suggest this to a neuroscientist,
there might be some little brain center somewhere near the frontal cortex
that allows the frontal cortex to do some predicting and planning
and induction about very simple,
few simple finite state arrangements.
Who knows?
Would you look for it?
Well, if you were a neuroscientist, you could say,
oh, that's completely different from anything I ever heard.
Let's look for it.
And if you're wrong, you're wasted a year,
and if you're right, then you become the new Ramoni Kahal or someone.
Who's the currently best neuroscientist?
Maybe it's late.
One more question.
One last question.
This is Cynthia Salman, who's one of the great developers of the logo language.
Hey.
Yes?
Maybe it's that question for me.
What do you think about theories such as Ramoni's theories
that speak of no same thing?
Completely weird.
Obviously, those theories have nothing to do with human thinking,
but they're very good for making stupid robots,
and the vacuum cleaner is one of the great achievements of the century.
However, his projects, what was it called?
Cog disappeared without a trace.
That theory was so wrong that it got a national award,
and it corrupted AI research in Japan for several years.
I can't understand.
Brooks became popular because he said,
maybe the important things about thinking is that there's no internal representation.
You're just reacting to situations,
and you have a big library of how to react to each situation.
Well, David Hume had that idea,
and he was a popular philosopher for hundreds of years,
but it went nowhere, and it's gone, and so is Rod.
However, he is one of the great robot designers,
and he may be the instrumental in fixing the great Japanese nuclear meltdown,
because they're shipping some of his robots out there.
The problem is, can it open the door?
So far, no robot can open the door, even though it's not locked.
I usually start by asking if there are any questions,
but I thought I'd say a few things about chapter one,
and then see if there are any questions.
I can't see the pointer.
Oh, anybody remember how to get word to make its pointer not disappear?
Maybe I mentioned this in the first lecture,
but I was taken by this cute poem by Dorothy Parker,
because the first chapter was about love and stuff like that.
So I tried to get the rights to reproduce it,
and it turned out that she was angry at all her friends.
She must have been a perpetually pissed off person,
and so she left all her literary rights to the NAACP,
and I called them up for hours, and they couldn't find the rights.
So finally, so it's in the version of the Emotion Machine on the web,
but I had to resort to Shakespeare to replace her.
Shakespeare's a slightly better poet,
he's not as funny as Dorothy Parker.
So the first chapter starts out,
it's mostly about all the things we don't understand about the mind,
which is almost everything,
and the first discussion is, well the whole chapter is making fun of the most popular ideas,
and the most popular idea of the mind is that people think that they're not doing the thinking,
but there's something inside them that's doing the thinking.
And it's this idea that there's a self is embedded in just about everything we say and think,
and really it's hard to see how you would do without it.
But if you ask what is the self,
then since this idea is so popular,
people begin to believe that there is such a thing and it takes all sorts of various forms,
and the most dangerous form maybe is the one that religions exploit,
which is that inside a person with all their complications,
there's a little pure essence called the soul or the self or whatever you want to call it,
and it's impossible to describe it or explain it in physical terms,
and so that is one of the reasons why we think there are two worlds,
a physical world and lots of other kinds of worlds.
Each of us has some imaginary model of what they are and what they're in,
and philosophers talk about it and existentialists and so on.
So there are lots of problems about our ideas, about ourselves,
and in reading around for half my life,
I was puzzled at the strange ideas that are around,
and in Aristotle, I find the first intelligible theories of mind and emotions.
So if you look at particular Aristotle's, there are a number of books,
and one of them is called Rhetoric,
and it's full of theories about how people reason and influence each other,
and I'll show you some quotes from that,
because when I look at the history that I've encountered about psychology,
Aristotle stands out as being the first and among the best,
and as far as I can see, there were no psychologists nearly as good as him.
Of course, we don't know whether there was a him exactly,
because what we have of Aristotle is a lot of writing,
but it's all cobbled together by students from all sorts of manuscripts by other people
and people who took notes, and Aristotle claims to have learned a lot from Plato.
We have very little writing from him,
and so there you go, three centuries before the Christian era, as it's called,
and then a couple of thousand years later,
we start to find people like Spinoza and Kant and John Locke and David Hume
who start to make psychology theories,
very little of which is as good as the ones that Aristotle has in all his fragments.
So one question that frequently bothers me and should bother everyone is
why did science disappear for a thousand years,
and the standard explanation is the rise of the great religions,
and why did it come back, and you see with the first signs of anything like modern science,
at least in my view, with Galileo and Newton.
There are a couple of people before that.
There are some people in the Muslim world who invented some high school algebra,
and they make a big fuss about that.
It looks like Archimedes, in a very recently discovered manuscript,
computed in integral.
He found the volume of a cone, which is, what is it, 1, 6th, B, H, I forget.
Anyway, so why did science disappear,
and why did psychology appear so late?
Because there isn't much psychology in the modern sense until 1900,
or the late 1800s, with Francis Galton and William James lived around here,
and Rudolf Fleiss, who, Sigmund Freud, starts writing in 1895.
People make fun of Freud, as I mentioned last week,
but in fact, among other things,
how many of you have read the recent criticisms of Freud,
which claim that he was complete faker and never cured a single patient?
This is popular stuff.
I don't believe Freud ever really claimed to cure a single patient.
So the critics, who are really very ferocious,
claim that he made up all his data and so forth.
But most of what Freud says is that psychoanalysis might be a good way
to find out what you're really thinking and discover more about yourself
and your goals and so forth.
I had the good or bad luck to be introduced to L. Ron Hubbard when I was an undergraduate.
John Campbell was the great editor of the, I think it was called,
The Stounding Science Fiction in those days.
What a marvelous title.
And this fairly mediocre science fiction writer L. Ron Hubbard
invented a new form of psychiatry called...
What was it called?
Dianetics.
It's pretty good.
And I'll tell you that story another time.
But John Campbell had Thanksgiving in the Commander Hotel every year
and invited a bunch of friends.
And I don't remember if that's how I got to meet Asimov and Heinlein and other people.
But anyway, I did and science fiction had a big influence on me from my...
Actually, early years, but starting in college, it got very serious.
Anyway, so chapter one starts to talk about this phenomenon of psychology
and one of the funny parts is this little section three, 1.3,
of trying to say what are emotions.
And I looked up emotions in dictionaries and can you all read that?
I don't feel like reading aloud.
There's lots of discussion of emotions and how mysterious and complex they are.
And then the marvelous thing is how many words there are for emotional states.
I think I got 300, but I don't remember.
Anyway, here's from A to D.
And I don't recall how I found those, but I think...
But that's a lot.
How many words for ways to think are there?
Now, that's a serious question because I complained maybe on the next page.
No, I didn't.
I found myself complaining that there were very few words for ways to think.
And then this afternoon when I was pruning these slides,
it occurred to me that I didn't really try.
So maybe I just didn't think enough.
So if there are a couple of hundred words for everyday emotions,
if any of you can find me a list of 10 or 20 common words for styles of thinking,
I'd appreciate it because I wonder if there are a lot and if not, why not?
So here's a list of typical situations, grieving for a lost child,
panic at being in an enclosed space.
I'm not sure any of the words in the list of 300 standard emotions
are good enough to describe how you feel for any of these not unusual states.
Have you ever lost control of your car at high speed?
No, but when I first learned to drive, I couldn't believe that you could read signs
at the same time as...
Well, anyway, one of the very best psychologists in history,
or a pair of psychologists, aren't even cold psychologists.
These are two guys named Conrad Lorenz and Nico Tinbergen,
and somebody made up the word ethology.
What they study is the behavior of animals,
and in some sense, presumably they're studying the psychology of animals
because just as with a person, when somebody flies into a rage,
you're not describing their mental state,
you're describing something about how they behave.
So the ethologists, too, are psychologists,
and Tinbergen and Lorenz, starting around 1920s,
started to analyze the behavior of animals in great detail.
So here's an example of how a certain fish behaves.
I actually forgot which fish it is, but there's a picture of it in the book.
And at different points in its life, it's in different phases,
and this is just one diagram of a dozen for this particular fish,
and its reproduction, which involves an environment with plants and other things,
and he divides its behavior into parenting, courtship, nesting and fighting,
and then you see each of those has a lot of subdivisions.
And Tinbergen and Lorenz and some students discovered all these things
by sitting in front of fish tanks and watching the fish for months and years.
Tinbergen also spent years on some beach watching seagulls,
and so he has a diagram like this for a particular class of seagulls.
When I came to Boston, my friends and I used to go to Nahant,
and look at these tide pools there where there are a lot of activities,
and it was very interesting, and I got a big fish tank
and imported all sorts of little animals and plants from the tide pools in Nahant,
and I watched them for about a year and didn't learn anything.
That was before I read Tinbergen, and then I realized there's something about those people,
which is they could watch a fish and recognize all sorts of behaviors,
and I would just watch a fish and wonder whether it was hungry,
or wouldn't you get bored swimming back and forth for three years in this.
Anyway, so here are the great psychologists of our day, Aristotle,
two thousand years ago, and Lorenz and Tinbergen in the 1930s,
and Sigmund Freud and Galton and William James around 1900,
and then what went wrong?
There's almost no good psychology between then and 1950s
when something called cognitive psychology started,
and it was partly due to people who said,
let's make psychology more scientific,
and you've probably all heard of Pavlov or Watson,
and what happened is around 1900, some psychologists said,
well, these Galton's and Freud's and William James are very poetic
and expressive and literary, and they write much better than we do
and tell good stories, but they're not scientists,
and they don't do reproducible experiments.
So what we have to do is simplify the situation to find the basic laws of behavior.
So let's take a pigeon and put it in a vacuum in the dark.
Well, they didn't go that far, but they did put it in the dark,
and there were two illuminated levers to work,
and you could make a sound, and the sound could be very annoying,
or you could have a right, annoying flashing light or something,
and the animal would push one of two levers,
and one of them would make the stimulus even more annoying,
and one of them would make it go away,
and you'd plot curves of how often the animal pressed these levers,
so you would get a quantitative theory of how much it learned
and how much it could remember and how many trials it would have to do,
and then instead of just looking at reactions to stimuli,
quickly switch to trying to teach the animal things by giving it two alternatives,
turn left or turn right, or push this button or that or whatever,
and if they pushed the one you approve of, then you'd give them a little pellet of food,
and there was a lot of engineering so that you would make sure that the food got to them right away,
because if there were a 10-second delay between an action and a reward,
the pigeon or a squirrel or a rat or a cat or a dog would learn much less quickly
than if there was a one-second delay,
and anyway, that went on for 50 years, starting around 1900, Pavlov and his dogs,
and there's a great movie that some guy came around with that had been taken of Pavlov's lab,
and it shows sort of like a great dictator or something.
There's this room with a lot of cages and dogs and mostly dogs in this case,
and Pavlov comes in and there are a bunch of lackeys who sort of bow and scrape,
because he's a lord, and he comes in,
and all the dogs run into the corner of their cage and yelp.
So the Pavlovians tried pretty hard to get that movie suppressed,
and I haven't seen it in recent years,
but anyway, Fred Skinner, who was a professor when I was an undergraduate at Harvard,
was the first one to really automate this experimental psychology,
and he invented what's called a Skinner box,
but it's just a soundproof, lightproof, well ventilated and thermally regulated cage,
and you can put a rat or a pigeon, those are the most common animals.
They're very inexpensive because they're free.
No one knows much about dolphins.
They've been studied for 50 years, whenever, John, what's his name?
Remember the name of the great dolphin?
Lily, thanks.
He discovered a lot about dolphins, and a certain amount about their communication,
and a little bit about whales, but there's an interesting mystery.
I forget which whales, but some whales have a 20-minute song,
and they repeat it for a whole season, and next year that song is a little bit different,
but it goes essentially without repeating, it's very complicated for 20 minutes,
and people have studied that a lot, and no one has the slightest idea of what it means,
and nobody even has any good conjectures, which bothers me.
What I think it probably means is this, when there's a whole bunch of fish somewhere for one of these whales,
it might be 200 miles away, and whales eat a lot,
and it's very important to find where the fish are,
and I believe this message, which changes a bit during the season,
might be telling you where the food is on the Atlantic or Pacific coastline,
in great detail, because if somebody finds a lot of fish somewhere,
you have to swim 300 miles, and if they're not there.
So anyway, it's interesting that John Lily got a lot of publicity,
but he didn't discover squat, and finally the dolphin studyers gave up because nothing happened.
Anybody have heard anything? I haven't paid any attention for 20 years.
Have you heard of anybody discovering anything about dolphins?
Except they're very good at solving a lot of physical problems.
Anyway, that's unbothered by the mystery of why was there some psychology in Aristotle's time,
and why didn't it get anywhere till 1950 when there was regular psychology,
but it was afflicted by what I call physics envy.
Namely, you run into people like Estes, and well, he was pretty good actually,
but there are a lot of psychologists who made up things like Maxwell's equations for how animals learn,
and there were generally three or four laws, and if there's a sequence of events,
then animals remember a lot about the first few and the last few in the sequence.
They don't remember much about the middle, and of course the reliability of their memory depends a lot on how recent it was
and on how powerful the reward was and blah, blah, and so they get these little sets of rules that look like Newton's laws,
and that was the kind of sort of psychological physics that the so-called behaviorists were mostly looking for.
This was not what Tinbergen and Lorenz did, because they wrote books with extensive descriptions of what the animals did
and made little diagrammatic guesses about the structure of the subroutines and substructures.
Anyway, end of history, but it's a nice question.
Why do some sciences grow and why was psychology just about the last one?
I suspect it could have been earlier, but people tried to imitate the physicists and tried to say maybe there's something like Newton's
or Maxwell's laws for the mind, and they found a few, but they weren't enough to explain much.
There are a lot of questions.
When Seymour Papert and I started thinking about these things, which was really around 1960,
I had been working on some ideas about AI in the late 50s, and my PhD thesis was a theory of neural networks,
which was sort of interesting, but never really went anywhere.
I went to a meeting in London somewhere and gave a talk about a theory of learning that was based on some neural network ideas,
and there was this person from South Africa named Seymour Papert who gave the same paper.
I hope this happens to you someday.
Find somebody who thinks so much like you, only different enough that it's worth it,
and that you only have to say about three words a day and some whole new thing starts because we really did write the same paper
and it had the same equation in it.
He had been working for Piaget, who was the first great child psychologist.
I should have mentioned Piaget, who probably discovered more things about psychology than any other single person in history,
and there are lots of people now who are saying he was wrong about 0.73,
because children learn that at the age of two and a half instead of two and three quarters.
I'm parodying the Piaget critic community, but it's pretty bad.
I think those poor guys are uncomfortable because Jean Piaget published 20 books full of observations about children
that, as far as I know, no one had made systematically before,
and in his later years he started courting algebraic mathematicians because he wanted more formal theories,
and in my view he wanted to make his theories worse, and nothing much happened,
but he did visit here a couple of times and it was really exciting to meet the starter of a whole new field.
Anyway, Papert and I discussed lots of things, and somehow or other we kept finding other more ideas about psychology,
and it finally gelled into the idea that, well, if you look at the brain, you know that there are several hundred different brain centers.
What's all that stuff for? And how could it possibly make any sense to try to explain what it does in terms of four laws like Newton?
Like, how does a car work? Is there a magical force inside the engine that causes the wheels to turn?
No. There's this funny thing in the back to cause a differential so that if the car isn't going in a straight line,
the two wheels going at different speeds won't rip the...
If the two wheels were going at the same speed, the tread would come off in five minutes.
You ever wonder what a differential is for?
So, most of the car is fixing bugs in the other parts.
Most of the brain is because we started out as fish, or lizards or whatever you like,
and making the brain bigger wouldn't help much,
because you'd just get a heavier lizard that had to eat more and would think more slowly.
So, size is bad, but on the other hand, if you need another cubic inch of brain to fix the bugs in the other part,
then the evolutionary advantage of being smarter had better make you able to catch a little more food per hour.
So, each person is an ecology of these different processes,
and the brain reached its present size about a million years ago, I guess.
What's the current guess? Anybody been tracked?
They keep discovering new ancestors of humans, and I don't have the patience to read about them,
because you know that next week somebody will say, oh, that isn't in the main line,
you were just unlucky to discover that skeleton.
So anyway, Papert and I and a lot of students gradually developed this picture that the mind is made of lots of processes,
or agents, or resources, or whatever you want to call them, and it's anybody's guess what they are.
If you look at the anatomy of the brain, you know that people label regions,
so it's very clear that this occipital lobe back here is largely concerned with vision,
and I forget where the one for hearing is.
If you destroy the part of the brain for hearing in some animals,
you get a little bit of increased function in some part of the visual system
that seems to enable the animal to hear a little bit and make some reactions.
And there's a whole lot of hype, I think you have to call it, about the flexibility of the nervous system.
That is, if certain brain areas get destroyed, other parts take over.
They almost never are as good, and mostly many functions never get taken over at all,
but are replaced by ones that superficially seem similar.
And so there's a whole lot of, I guess, wishful thinking that the brain is immensely resourceful
and error-correcting and repairing.
I think there was some idea that it was a general phenomenon,
but if you do some arithmetic, you get an interesting result.
Suppose that each function in the brain occurred in ten different places at random.
Then if you removed half the brain, how many functions would you lose?
Well, almost nowhere arithmetic tells you you would lose about one part in the thousand.
And so, in fact, you would never be able to detect it.
So this idea that the brain has enormous redundancy will now change that number to five.
Suppose each function is somewhat supported in five different parts of the brain.
Then if you take off half the brain, then what am I saying?
One part in 32, chance of losing some significant function.
So probably lots of things that we do are supported in several parts of the brain.
Apparently the language center is pretty unique and some others,
be careful about the conclusions you read from optimistic neuroscientists.
Anyway, Papert and I worked on this idea of how could these large numbers of different processes be organized
and we made various theories about it and then around, I guess, the late 1970s,
we stopped working together and Papert developed his revolutionary ideas about education,
which certainly have had a lot of influence although they didn't sweep the world in the way we had hoped.
And I kept working on the Society of Mind Theory and we didn't work together so much,
but we still did plenty of criticizing and supporting of each other.
Anyway, my theory ended up with this idea that it's sort of based on Freud.
I don't know if I kept a picture of his here.
Freud concluded that the mind was an interesting arena, sort of,
and he had the mind divided into three parts.
There's at one end of the mind, which we inherited from most other animals,
is called the id, which is a bunch of instinctive, mainly built-in behavioral mechanisms.
And a second part of the mind is what he called the superego, which is a collection of critics.
So in Freud's first image, the brain is in two parts.
One is a set of instinctive, built-in behaviors and the other is a set of critics,
which actually are associated with a culture and a tradition.
And you learn from other people things that are good to do and things that are bad to do.
And that's called the superego.
This is your set of values and standards and tests for suitable behavior.
And the middle is this strange object called the ego, which is not what people think it is.
At least Freud's word, the ego, is a kind of big neutral battleground where the instinctive behaviors,
I keep wanting to, oh, you can see that arrow if I take my finger off it.
And then gradually as I kept trying to figure out where, how problems are solved
and what kind of processes might be involved, I got this picture which has six layers.
And various people come around and say, I don't think you need to distinguish between layers four and five
or why don't you just lump all the three top layers into one.
And I sort of laugh quietly and say, these people are trying to find a physics like unified minimal theory of psychology.
And they're probably right in one sense, but if they do that they'll get stuck
because if you get a new idea there'll be no place to put it.
So if you have something that's very mysterious, don't imitate the physicists
because if you make a theory that's exactly right and just accounts for the data
and there's nothing extra and nothing loose,
even when you notice a new phenomenon like dark matter, then the physicists don't know what to do.
Should they regard dark matter as some obscure feature of space-time
or does it have something to do with this universe being near another one
that you can't otherwise communicate with and it's all very puzzling.
But there are lots of things that don't fit into Newton's laws these days.
And I'm not suggesting a six-layer theory of physics, but it might be worth a try.
Okay, so I made up some nice slides, but I think I'll stop.
So who has some questions and what would you like to see in the theory of psychology?
What do you want to be explained?
A lot of people are convinced that there are some really serious problems and mysteries
like what is consciousness?
And if you look at chapter four, my feeling is consciousness is an etymological accident
that people got a word which is a suitcase for all of the things they don't understand about the mind and more.
But once you've got a word and it goes in the culture,
consider the word consciousness for a minute from a legal point of view.
Suppose that you happen to be walking along and you're carrying something.
Where is that pointer?
And it happens to stick somebody's eye out.
Then it's very important when they sue you to establish whether you meant to do it or whether it was an accident.
Did you consciously plan to...
I can't think of the English word for putting somebody's eye out.
There's be heading and also to gouge is a good word.
So anyway, it's very important for social reasons to have a word for whether an action was deliberately violating the rules
as opposed to accidentally violating the rules.
Like if you tripped on the stairs and landed on somebody and broke their neck,
that's not a crime unless you were so clever as to make it appear that it was an accident.
Anyway, you see what I mean?
So we need a whole system of fairness and ethics and social responsibility
is based on the distinction between whether an action was deliberate or not.
And so did he do that consciously is a word for that.
And somehow the idea of conscious became elevated.
Well, that's a very superficial.
You can probably think of 10 other reasons why a word like that.
Yes.
Sometimes in your writing, it seems to me that there's both sides of it.
Some of you argue both sides of it.
It seems like in that kind of...
I can well imagine there's that kind of representation.
You have representation of self and representation of your mind,
but then you say there's no self or no consciousness.
Why can't you think about consciousness as just a process that is reasoning about your own mind?
Well, I...
I mean, I understand you don't want to talk about self as that's a real discussion, but...
No, but sometimes when you say conscious, that is, do you remember doing it?
Which is...
Yeah, but don't want to load up my process that asks me what I think about my own mind
and then I retrieve that and I say, yeah, I do remember it.
Well, you're right.
Actually, I went to a lot of trouble to find 25 or 30 different uses
for the word consciousness, and probably if I...
or if one of us worked harder, we could take those and condense them into five or six
much better ones that account for more stuff than the 30.
That might be just right for something.
Who knows?
Yes, well...
I think that's a great criticism of one reason why people don't like these theories quite so much
because I propose too many things.
I really should reprint that criticism from RESTAC or hand it out
because this neurologist who says,
why is he telling us all these things about K-lines and representations and so forth?
The answer is he's from a community that doesn't have enough variety yet.
And I'd be the first to admit that I try to go overboard
and think of five more things than are in the literature.
But that emotion thing is nice.
Remember that that was a serious challenge because when I made that list,
I do have a laser pointer somewhere in my jacket probably.
How many words for at least noticeably distinct ways of thinking or reasoning
or figuring out or solving problems can you think of?
Maybe there are 20 or 30.
I just realized this afternoon that I never looked.
I don't remember where I got this list.
Can I ask about your perception of free will?
I got a lot of readings that you don't have a strong sense of free will, so what is that?
I think it's the same as the one for consciousness, namely it's a legal concept.
The idea of free will is completely obscene, isn't it?
What could it possibly mean if you did something for no reason?
So it's a thoroughly empty idea, isn't it?
Or what do you mean by it?
Do you mean there's nobody ordering you around so you're free to do whatever you want?
But of course you're not.
You can only do what your computer computes you to do.
In the same way that you show the fish diagram, the fish's actions are products of the environment
and its current state, and it's essentially a turning machine.
Well, it's some kind of machine, yes, sure.
And so would you argue that we are also turning machines that are just turning out in the world?
Sure.
I've never heard of any even interesting alternative.
In other words, people who insist on free will appear to me to be like people who believe
that there must be a God who created the world.
What's the next step?
Who created the God?
They don't take that step.
So if your will is free, okay, then who's controlling it?
There's nothing there.
But legally it's great, because if somebody stole some money of their own free will.
But suppose you were a peculiar kind of epileptic, and every now and then when you go by,
your hand goes out and steals things.
Then they, what do they do?
They put you on parole?
This is very strange.
But if you look at religions, you see that people make money on them.
13% of the world's product goes to people who make their living on concepts like free will and consciousness.
So it's a big money thing.
It's not just an accident.
It's an industry.
So both of those are concepts of society?
They're parasitic.
Imagine a society without the concept of consciousness of free will.
But those are requirements for being free will.
I don't think you could.
You'd have to make up something to keep people in check and under control and to train them.
It's like the rat being, the rat needs somebody to press the reward or punish button.
And we have it built into our, a culture works because you build into people's head the machinery for suppressing doubt.
And it's very clever.
But you should think of it as an industry rather than an explicable phenomenon.
How much money goes into, yes?
How many ways of thinking can you think of right now?
That's my challenge, in fact.
There's probably a big list in some chapter or other, but there's nothing like this.
What's the trick? Three, if I go like that.
Actually, Dragon has a thing so you can tell it make things bigger.
How many of you use the new Dragon program speech thing?
I can't believe how good it is.
I was talking to Henry Lieberman about it earlier.
Yes?
So I'm wondering, personally, you would say that the side of mind here is both humans and animals.
Is it just that we have higher levels of organization than them?
And so where would you draw that line?
Like, do animals not have a notion of the cell that we describe in the book?
That's a great question and it'd be interesting to think about ways to investigate it.
People are, researchers are often, in fact, raising that question of,
do animals have a representation of themselves?
And there's a famous experiment, but I can't remember what its current status is,
where you put a red dot on a chimpanzee's head,
and when the chimp passes a mirror and sees that, the chimp might go like that.
Whereas, I don't think a dog, when it passes a mirror, would rub its forehead to see if it has a red spot.
I had a cat who walked past mirrors, because we have some wool full-sized mirrors around the house.
And the cat walks by and there's this other cat in the mirror, and she pays no attention to it, whatever.
So, of course, I don't know what happened the first three times she walked by that mirror.
Because if you see another cat going by, you'd think it would...
Anyway, it's a good question and people ask that and there's some evidence that elephants have a model of themselves.
And maybe dolphins, and I don't know where...
Have any of you heard any stories of other animals that can recognize, for example, when they've been painted?
Which?
Is it elephants?
Yes, I think elephants.
There's a famous child psychology experiment where, if you're less than a couple months old, you actually fail this test.
So, it's actually something that comes as a sign of your child progressing.
So, it might not be intrinsic to humanity, but it might be so.
So, I'm sure that the chimp can see how it can do that.
This is off the topic, but I once had a great email correspondence with some woman who was getting a PhD in France about how babies recognize their mothers.
And she concluded with...
You did experiments after having other people walk into the room with a mask of the mother or a different hairdo and so forth.
And for the first two months or three months, I think, it turned out that the baby recognizes the mother by the hairdo, which had not been known.
And then, I think, after three months, it's recognizing the mother by a face.
And at that point, she's doing experiments where you get another woman wearing a copy of the mother's face.
And so, now there's two mothers and the baby is absolutely delighted.
And then, as I can't remember, then I think at four or five months, when two copies of the mother comes in, the baby gets really panicked.
So, I lost track of her.
If you have a baby, let me know.
She got her PhD for this, and I haven't heard anything since. Yes?
Here's sort of a formulated thought that I just thought of regarding ways of thinking and ways of feeling.
So, it just occurred to me that it seems like, if you go back to the list of feelings, it seems like when we talk about feeling, we're talking about a state that the brain is in.
So, it might be a complicated state that's like some combination of a lot of different parameters, but it's a state that you can stay in for like an arbitrary amount of time.
But I think thinking is something that's more sequential, as in like, when you're thinking, you're necessarily changing the state of your brain all the time because you're moving bits around.
No, I think that's right.
I think when we talk about intelligent behavior, you're absolutely right.
What you've got is a process that's criticizing itself and seeing when you got stuck in finding things to do.
And I suppose in each emotional state, you're certainly also thinking, so that's going on, but maybe it's more restricted.
If you're confronting somebody and there's a sort of conflict, then almost all your thoughts are constrained to that subject and it's not as resourceful, but I'm just improvising.
Okay, I think what I'm talking about in this context is sort of extreme forms where the person changes into another machine and it's like an angry person won't listen to reason.
Or it's very hard to deflect them, so it's this kind of rigid thing.
But humans are generally, are rarely in such extreme states where nothing gets through.
The whole point of that was that, and I just realized that maybe it was just too lazy, that we have this huge vocabulary of nuances of emotional activity.
And people think these are, also people think that these are hard to explain and mysterious and non-physical and blah, blah, blah.
But why do we wait till next week and see if somebody comes up with, or see if we can come up with a set of 30 or 40 words about intellectual states, curiosity.
I just don't know how many there are and I haven't thought of any in the last few minutes.
Yes, has anybody thought of a couple of...
What's the word?
Flow, like just in the high, when you're really engaged in some activity that you're doing and you're really in the zone.
Yes, there's this, there's a state of keeping other things out so you can focus.
Not being interuptable, yes.
So in this side of mind, you talked about agents and how they divide between themselves.
But I don't see anywhere about how evolution modified a lot agents.
Like I believe that evolution modified the way we think right now.
I don't know if you saw, there's a paper like wrote like two years ago.
It's called The Region of Behavior.
And this guy tried to explain like how we make some decisions.
Evolutionary, like in a point of view of like our speech.
We are maximizing the probability of reproducing ourselves.
But individually we are not kind of increasing these efficiencies.
And like I think somehow like these agents would be like the decisions of agents or resources
would be like very determined by evolution since we have like a very long time to like of evolution of the human being.
And somehow we have like hard wire to make some decisions.
So for example, like he gives the example of like a guy.
Well, for lizards that's certainly true.
But why do humans keep changing their environment?
So like for example, yeah, so he gives this example of like tossing a point.
The guy says that like this coin is unfair.
And like there is seven, he doesn't say the probabilities, but there is 75% of getting heads and 25% of getting tails.
And like we like the subjects, they choose run only 25% of the time the tails.
Even though they like they can take account of the number of the times that you put heads.
And even though if you choose always hands, you would get more money or whatever you would make it the right decision.
That's called probability matching.
And it's not a good strategy.
Yeah, but humans do that.
No, well, they do it if the psychologist rigs the experiment very carefully.
It turns out that the best things, what do you think is the optimal strategy?
Always.
No, the optimal strategy turns out it's the square roots of the probabilities normalized to add up to one.
And I'll, I'll give you a proof next time.
This theorem is due to Ray Solomon off who invented inductive probability theory.
So evolution, if evolution did probability matching, then it would be wrong.
And I bet you'll find out that those experiments are wrong.
You have to see how did he rig the experiment so that people, if it's probability 25%, they guess that 25%.
I don't know.
There was a theory about why you would expect it.
It's a good question.
I don't think people use probabilities though.
So even if an experiment shows some, I would look for a flaw in the design of the experiment.
Yes.
You talked about a lot of emotions today, I'm sure.
And my understanding is that for someone who has a specific personality, they might have a predisposition to feel certain emotions, like anger, depression, or whatnot.
My question is that if you had any kind of insight or theories on to what extent our personality is affected by events or influences that happened to us over the course of a lifetime.
And to what extent is it impacted by sort of, you know, chemical violence when you wake up or carbohydrate?
Well you're asking what things do, what do people learn?
We don't care if it's chemical or...
See if it's chemical, it's still physical.
I mean, I reckon we'll go to that sort of treatment for your depression.
And the argument is that a lot of reasons for your depression is because of some sort of chemical or biological way of operating is constructed.
Well there's lots of complicated things about the brain.
One feature of the brain that I don't know if everybody...
You know that there are inhibitory and excitatory synapses.
When one neuron connects to another, the impulse that goes along the axon to the target neuron may reduce the probability or the strength of its firing or increase it.
So that's called inhibiting or...
There's no...
Or exciting, that's not quite the right word is it?
Now generally in the nervous system has a rule but not always.
If you follow a chain of activity, it goes inhibiting, exciting, inhibiting, exciting.
If you had too many excitatory things and there was a loop, then it would explode and it would wear itself out and jig time.
So there is this general feature of the anatomy that you alternate.
So when somebody talks about a drug having an inhibitory effect, that's sort of weird because it's inhibiting half the neurons and therefore lowering the thresholds.
Of the ones they're connected to and so on.
So I think the best thing is until you have a diagram of the functional relations between different brain centers, it might be best not to try to make generalizations about how the chemistry works.
It's easy but people think of adrenaline as a stimulant but in the nervous epinephrine but in the nervous system it's locally it may be inhibiting things that are inhibiting something else.
And so it appears to be exciting.
Yes.
I have a problem understanding the difference between thoughts and emotions and knowing something might be as simple but since the only thing that I can separate in my mind is that thoughts, so let's call it a time constant, I can change it kind of rapidly.
Emotions, time constant is long and I can control it again much less.
But since there is no, at least in this class, there is no preview, how can I make a decision on where the dividing line between these two entities is?
Oh, I think it's a waste of time.
As far as I can see, emotional mechanisms are generally lower level simpler ones than the ones that involve several layers of the, more layers of the brain.
So it's just a relative thing.
It's not that some states are emotional.
You're always having some high level thoughts and low level thoughts.
And the distinction, I just don't know why the distinction has occupied so much tension.
I think it's because, and that goes back to having more words for, or asking how many words do we have for ways to think.
It seems to me that in popular culture, there are very few words for ways to think and lots of words for emotions and so they're very prominent.
Maybe you have to be smarter to distinguish between ways to think and people generally are dumb.
Not because they're inherently dumb, but they come from cultures which bully you if you, you know, what happens if you're in third grade and you're smart.
You get it beaten out of you and you learn not to show it.
Kind of looked like that question of why science happened earlier and why we have more ways to describe different ways to think.
And that sort of puzzled me, but can we just not reflect it as much on different thinking states or different approaches?
I wonder if the Greeks had more, but we...
I think they did. I think they had also more concept of ideas and different states and their approaches.
Who has a theory of that? What's your theory of the Middle Ages?
How could things get dark for so, so dark for so long?
Well, I do have a theory.
And are we about to have one?
I think it has this much to do with the channels in which one can communicate ideas to other people, whether they exist or not, whether the arteries are over their clothes.
The Middle Ages were characterized by scientific discoveries being kept as family secrets.
Ah, Cardan knew how to take cube roots and he wouldn't tell anyone.
Well, the classic student example is baby talk, which for 300 years made a single Italian family very rich.
You know, using tunnels to extract the baby and crop earth increased success rate in difficult births by about 10% they say.
Wow.
And that was enough to build a family fortune until some servant finally stole the gene.
So, who has a theory of the Middle Ages?
Is there a standard theory? Yes.
Well, the concept of the Middle Ages as the dark ages is something that emerged mostly in the Renaissance,
when people in the 14th, 15th, 16th century tried to present themselves as going back to the classical age of scholarship of ancient Greece and Rome,
and as being better than their predecessor for the last few years.
This mostly happened because of the discovery of manuscripts that were translated from the ancient Greek,
and in certain cases Latin by muslims who at the time were sort of receding from Europe.
So, the entire concept of the Middle Ages might be a fabrication of the Renaissance.
There were some significant discoveries at the time.
That's a nice idea.
In other words, when was St. Patrick?
St. Patrick.
I'm told that he popularized a lot of technical manuscripts, brought them back into Europe.
He has two achievements.
One was bringing scientific culture back, and the other was getting snakes out of England or something.
Ireland.
I don't know which he was sainted for.
Don't you have to do three miracles, or is it?
What's, what's, yeah?
Yeah, my theory is that the Middle Ages ended around 2100.
Because they'll say, after the Middle Ages ended, they'll say, you know, those guys back in the 21st century,
they had no idea how thinking worked.
They couldn't even think of a few ways to think.
They had poverty, they had wars.
You know, those guys were barely out of their winecloths.
Right.
I just read a history of AI.
I forget who wrote it.
But it had this section saying, it mentioned the Newell Simon.
There was a thing called general problem solver, which I mentioned a couple of times in the book.
And it's the idea that the way to solve a problem is to find, it's a symbolic servo.
Find the difference between what you have and what you want.
And look in your memory for something that can reduce that difference.
Keep doing that.
And of course, it's, it's important to pay attention to the more important differences first and so forth.
And I'll send you this article.
This article is saying that they made a terrible mistake and this was a trivial theory.
And that's why nobody uses it anymore.
And it was interesting how many AI people fell for that idea in the 1960s.
My complaint has been that if you look in a modern textbook on, you must have some in your first volume, Pat.
Didn't you have some GPS things?
If you want to keep up with AI, you should read Patrick's textbook, even though he's,
people are starting to use this new one, which doesn't have any AI in it.
By, who's it by?
Russell and Norvig.
Russell and Norvig.
It's probably pretty good technically, but I leafed through it and it didn't have any, never mind.
It's probably better than I think because I'm jealous.
Yes.
It's kind of a different topic.
In Society of Learning, you're talking about the amnesia of infancy when you forget what you learned
and things that were once difficult to become common sense.
You can't even remember how it wasn't like that.
So it's just kind of wondering about the reverse of that process when you try and say,
do something to somebody that you turn on so that people can get your quick awareness on it
and bring back up the different levels.
I was wondering about, is that itself another way of relearning the things that you learned?
That's sure an interesting question.
When I first learned about programming, I had the idea that maybe babies think in machine language
and then after a while they start to think in FORTRAN.
And then finally when they're a little older, they think in ALGO or something.
But when they switch from machine language to FORTRAN, then they can't remember their earlier thoughts.
And there's almost no evidence of people finding genuine recollections from two-year-olds at later ages.
Now almost everybody thinks they remember something,
but there's the problem that you might have rehearsed it and translated it into the FORTRAN
and the ALGO and the LISP and the logo, whatever it is.
I had one of my greatest influences was a great mathematician named Andrew Gleason at Harvard,
who I met practically the first day I got to Harvard.
And he would always talk about things I didn't understand and I would go home and look them up and try to.
Anyway, one day we were talking about number forms.
And number forms are a psychological phenomenon which about 30% of people have.
And it was first described by Francis Galton.
And the phenomenon is if I ask you, close your eyes and tell me where is the number three?
How many of you have a place for the number three?
Well, that's a few.
So typically, if you imagine the visual field, that's a windshield, I guess.
So there are these numbers and they're nowhere in particular except that it's usually like that for an older child.
So here are these numbers and what's more in some people they're colored.
So I was talking to Andrew Gleason, I had read this Galton paper which was 1890 or 1885 or something.
And so I was asking people if they had number forms.
And he said, oh yes, he has one.
And he sketched it for me.
And he said, and they're colored too.
Oh, and his went way up and the prime numbers were bright.
What am I doing?
Maybe the composite numbers were something was bright.
And they were colored.
So I wrote this down and I, over the next couple of years, I look in antique stores for old children's blocks.
And I found a set of blocks that matched that.
And Andrew Gleason said that he knows when he acquired this thing.
And it was about four years old and he had a window which in this house and there was a hill.
And he could just see over the sill and he imagined these numbers on the side of that hill.
Blah, blah, blah.
Anyway, people who don't have a number form don't know what I'm talking about.
And I don't know if the 30% is still true, but it's an interesting phenomenon.
And in most cases of early childhood, well, that's, you can't find out because children do remember details of a house they lived in,
but you don't know if they've copied it.
So what was the original question?
How much can you remember from infancy?
Elran Hubbard thought you could go back to before you were born.
And you could remember people talking about you when you're still in the womb.
So anyway, John Campbell said you should look into this.
And a few of us made an expedition.
We went down to Elizabeth, New Jersey to visit the just starting up Dianetics Center.
And I met this Elran Hubbard who had green eyes and was quite hypnotic looking.
And the end of the story is he had been writing about how if you took this treatment of Dianetics,
then you could memorize an entire newspaper in five minutes and do all sorts of miraculous things like that.
Once your mind has been cleared of aberrations and obstacles.
And I became a big industry and turned into later Scientology.
I'm sure you've all heard about that.
So we asked Hubbard to look at a newspaper and tell us what was in it.
And he explained he was so busy training the other people to be cleared that he hadn't had time to go through the procedure himself.
And I never saw him again.
Yes?
What are your thoughts on memes and the fact that we're just, but because our thoughts are actually all about the good.
Of memes?
You mean Dawkins?
I didn't quite get the whole question.
The idea that...
So for example the way we talk and the way we all talk,
how we very much mix the way our parents talk to the people around us.
And possibly the way we think as well.
So how does that relate to how our mind develops?
Are we actually creative original characters?
Well of course it's both because you learn things from your culture and then you might just mainly repeat things
or you might get the knack of making new ideas.
I'm trying to remember what Dawkins main ideas are.
He invented the word meme to say that
ideas that people have might be considered to be somewhat similar to the genes in our heredity
and that societies are systems in which these memes which are conceptual units of meaning or knowledge
propagate around and self-reproduce and mutate and spread.
And I don't know what to say about it except that it's obviously true that every now and then someone gets a new idea
and tells people and for one reason or another they either forget it or tell someone else
and after a while it spreads and some of them fill up the whole culture and some just die out.
And whatever else Dawkins says, he's a very smart guy,
but almost everything then in my mind is that he's explaining that religions are mostly made of these memes
and they're very bad and cost the world a great deal in progress and productivity.
In other words, he's a militant atheist and there are about five best sellers in that business today.
But I don't know what else to say about memes. It's an obviously generally correct idea.
But the great thing about genes is we know the four amino acids or four nucleic acids they're made of
and how they're rope together and all that.
And I don't think Dawkins' theory develops anywhere nearly as elegantly as modern genetics.
So it would be nice if it could, but a really good theory of good ideas would be nice to have.
What would it look like?
Someday we'll have an AI that just punches them out.
I think it would look like a really good language.
Oh right.
Robert Heinlein has some stories in which the super intelligent people have a language that's so dense
that in five syllables they can explain something that would take you a half hour.
I think, I forget what it's, log land.
Anyway, if you need a good idea, read Robert Heinlein.
Sure.
I thought you already mentioned this.
When you talk about how geniuses, they might have just come up with better ways to think better,
like better ways to think, better ways to learn about how to better learn from better learning.
But why do you think they never mention it?
And why hasn't they propagated them better and learning about how better to learn from it?
That's a great question.
Do you think people have a concept of an idea that improves better learning?
There's a couple of phenomena that, like, how come there were so many geniuses in Athens?
And then some of the best mathematicians came from some high school in some little country next to the Baltic.
Bulgaria?
Romania.
Yes, there was some high school in Romania that not only produced von Neumann,
but about five or six other world-class mathematicians.
I don't remember the details.
So that's a nice question.
How come, if there are these great memes, how come there aren't more big pockets of them?
But there are a lot of cultures which were very inventive in other than intellectual fields.
How come Paris got all those artists?
How many of you saw the Woody Allen movie?
What's it called?
Paris at Midnight?
So funny.
Like, would you be able to say no one?
Say one idea that would improve, you know, your improvement of learning.
Right.
That's a good question for each of us.
What's your very best idea?
And stop fussing with the other ones and get that one out.
There must be some people who are very quiet and only speak once in a long time.
We should watch them carefully.
Yes?
I guess along with like, do you ever feel restricted by language?
Wait.
It's the sound I can't hear.
Am I strong enough to lift you?
I'm sure someone's done it.
Sorry about that.
Do you ever feel restricted by language and that you must represent your theory of mind or any theory of language?
No, I don't.
But I once was jealous when Papert explained that he got some idea and he explained he gets ideas like that when he thinks in French.
What?
You draw pictures too.
Oh, yeah.
So it's not just language.
That's a language.
Graphics.
So we ought to have devices within the next few years that draw pictures when you think.
It's so funny.
You know, we had cyborgs.
What were they called?
I mentioned them last time.
We had Steve Mann and who's the other one?
Yeah.
So there's these two guys around the Media Lab wearing various things on their head and they're always typing and you ask them a question and they've searched Google.
And when was that?
1990.
But it's all gone.
Nobody walks around with direct connection to the web.
Yes.
Anyway, I certainly expected it to turn up and something wrong.
So anyway, you should be able to buy one one of these days.
And what's your name?
Who is that nice woman who had the EEG thing?
Do you remember a Ted?
Forgot her name.
Anyway, she had this sort of helmet which had about 20 electrodes and she induced me to put it on.
I was on a stage with about a thousand people there, which was rather funny.
And there's a little spot on a CRT and I get to think about it moving one way or the other and rewarding it when it did the right thing for only about half a minute.
And then I could steer it around.
So here was a nice primitive gadget where you could sort of almost draw just by thinking this spot.
And then she started a company and hasn't sent me one.
Because maybe it was just beginners luck.
Beginners luck or something.
What?
Her name is Holly.
Yes, Lee.
Did you find the company?
Do you think there would be lots of people wearing stuff?
With your keyboard, right?
Why don't we take a five minute break?
I don't know.
Well, I hate to interrupt because I see ten different productive discussions.
I know.
Yes, I see.
I saw quite a few apparently productive discussions.
Maybe that's what the class needs.
But anybody come to a conclusion?
Yeah.
See if you can knock the wall down.
It's not a conclusion, but it's a question.
It's a thought experiment.
So if you had a black box that could sort of replace part of your brain.
And let's say you could replace like five percent at a time.
If you assume that there's a self entity.
At what point would you lose yourself?
If you change the question is, are you how how much do you have in common with with the you of yesterday as compared to when you graduated grade school?
So this question of the idea of identity is very, very fuzzy.
Yeah, I read a science fiction novel by Robert Sawyer.
And if you know of him, Canadian writer and it has to do with somebody who has a fatal disease.
So he's going to die soon.
But the technology is around where you can make a duplicate of him.
And so he has a duplicate made and he is sent to the moon for some reasons.
I can't remember, which is a kind of nursing home for I think people who are enfeebled and do much better with one seventh of gravity.
So there's some reason why anyway the living the original copy is sent to the moon and the substitute takes over.
But then our hero is miraculously cured by eating the right stem cells or what I don't remember.
So he wants to come back and the question is who gets the car?
So I can't remember the title of the novel except that it has alien in it.
Alienable rights is not something like that, but I wrote an article called alienable rights.
Anyway, so are you the same as you were five minutes ago or five years ago or whatever?
And as far as I'm concerned, the answer is who cares?
It's a sort of silly question because no two things are exactly the same ever anyway.
But again, a lot of these questions which look philosophical are legal.
The joke of that novel is that who owns the car is what matters to decide who is the real original and who's the copy.
Frederick Paul wrote a similar story much longer ago where people are copied and the copy is sent on a one way trip to some planet to fix a broken reactor.
And they always die and you get a million dollars for providing this copy.
But one of the copies survives so it's the same plot.
I can't remember that.
If you're looking for a good idea, if you go to 1950s science fiction, look for A.E. Van Voter, Frederick Paul or all those wonderful writers.
That was before it was necessary to describe really good characters.
And science fiction got better and better for the literary critics and generally worse and worse for the science fiction fans.
Do we really have any more questions?
Yes.
We're in a time age where sharing information between many people on a short amount of time is quite easy.
Do you think that this will change?
This will bring up many more ideas. Do you think that this will hinder?
Because before this time people had problems with sharing information.
And also now it's touching to hear from a large group of people working on one thing.
Do you think that this will change the way that we think and do you think that this will make us better?
It's a tough one. Bad things can happen and good things can happen.
Funny, because that reminds me again of science fiction.
Because in science fiction many, many years ago some writers got the idea that there would be something like an internet.
And some people realized there would be flash crowds.
And now there are flash crowds.
And I remember even as a kid talking to people who said,
why not wire up the voting machines so that they're always there?
And so if somebody in the government wants to know should we do this or that?
Should we bomb China or not?
You could get 100 million people to run up to the keyboard and say yes or no.
Presumably when the, what do they call those?
That great crowd of Jeffersons and Franklins and the Founding Fathers did a lot of things to prevent that.
And the one that they focused on, which was one of the most effective, was called the Electoral College.
And the United States is different from other places because we don't elect congressmen or presidents.
We elect smart people from the community who then get together and decide who should be president.
And of course now if anybody, now they belong to parties and if any of them voted for the other party's candidate,
they would be held to pay.
But it was a great idea because the Founding Fathers realized that if you had instant feedback, which is what Hitler got,
then you could say something really exciting and everybody would press the yes button and then you kill all the Jews.
And then the next speech you kill all the black people and all the yellow people and all the people whose last name doesn't begin with M.
And so what you don't want is instant feedback.
Now the new social networks are getting us close to that.
And the question is, is it time to have, is it time to stop that?
Is it getting dangerous? I don't know.
But there must be a lot of people who are recognizing that this thing is creeping up on us and you might be able to get 50 million people
to do something reckless in a few minutes if you don't put some limits.
I don't think we could get the electoral college back because you'd have to get a majority to, what does it take to fix the Constitution?
Two thirds?
We'll never see two thirds again.
It's the end of America.
Well, we have three minutes.
Yes?
If you would design a direction for the field of psychology, obviously one word is a set of debugging tools.
Why don't you say they should be doing it?
They should read Patrick Winston's thesis.
The psychologists now have disappeared into the tar pit of statistics and they don't have the idea that knowledge needs complicated representations.
And I don't care whether you assign probabilities to them or put them in the order in which you thought of them or, you know,
or do what Doug Lenet did in his thesis of swapping things when one worked better than another, but I forget the question.
But I think we've got to get better ideas about representation of knowledge.
I don't know where they're going to come from now that the whole AI community is drifting into these ways of avoiding representations.
I haven't read the Norvig, Russell, book.
Does he, can anybody summarize what it says about knowledge representation?
Who's read it?
There's a chapter on logic for sort of logic.
That's so funny.
First order of logic is what Newell and Simon thought of in 1956 before they thought of the so-called GPS thing.
Logic can't make analogies.
It's a very bad thing to get stuck with.
Zero or one.
Maybe one of our papers should be on what should AI do next year.
So really what my main concern it has been for quite a few years is to make some theory of how,
what makes people able to solve so many kinds of problems.
I guess if you ran through the spectrum of all the animals,
you'd find lots of problems that some animals can solve and people can't,
like how many of you could build a beaver dam or a termite nest.
So there are all sorts of things that evolution manages to produce.
But maybe the most impressive one is what the human infant can do just by hanging around for 10 or 20 or 30 years
and watching what other humans can do.
So we can solve all sorts of problems.
And my quarrel with the rest of the, most of the artificial intelligence community,
has been that the great success of science in the last 500 years really has been in physics
and it's been rewarded by finding little sets of rules like Newton's three laws and Maxwell's four laws
and Einstein's one law or two that explained a huge range of everyday phenomena.
Of course in the 1920s and 30s that apple cart got upset because actually Einstein himself
who had discovered the first quantum phenomena, namely the quantization of photons,
had produced various scientific laboratory observations that were inexplicable
in terms of either Maxwell or Newton or Einstein's earlier formulations.
So my picture of the history is that in the 19th century
and a little bit earlier going back to Locke and Spinoza and Hume and a few of those philosophers, even Immanuel Kant,
they had some pretty good psychological ideas.
And as I mentioned the other day, I suspect that Aristotle was more like a modern cognitive psychologist
and had even better ideas but we've probably lost a lot of them because there are no tape recorders.
Who knows what Aristotle and Plato said that their students didn't write down because it sounded silly.
So the idea that we developed around here mostly Seymour Papert and a lot of students,
Pat Winston was one of the great stars of that period was the idea that to get anything like human intellectual abilities
you're going to have to have all sorts of high level representations.
So one has to say the old conditioned reflex of stimulus versus stimulus producing a response isn't good enough.
The stimulus has to be represented by some kind of semantic structure somewhere in the brain or mind.
And so far as I know it's only in the theories of not even modern artificial intelligence but the AI of the 60s and 70s and 80s
that people thought about what could be the internal representation of the kinds of things that we think about.
And even more important, if one of those representations you see something or you remember some incident
and your brain represents it in some way and if that way doesn't work you take a breath and you sort of stumble around
and find another way to represent it.
Maybe when the original event first happened you represented it in three or four ways.
And so we're beginning to see, did anybody hear Faroochi's talk?
The Watson guy was up here a couple of days ago, I missed it,
but they haven't made a technical publication as far as I know of how this Watson program works
but it sounds like it's something of an interesting society of mind-like structure and it would be nice if they would...
Has anybody read any long paper on it?
There have been a lot of press reports.
Have you seen anything, Pat?
So anyway they seem to have done some sorts of common sense reasoning.
As I said the other day, I doubt that Watson could understand why you can pull something with a string but you can't push.
And actually I don't know if any existing program can understand that yet.
I saw some amazing demonstrations yesterday by...
Or Monday by Steve Wolfram of his Wolfram Alpha,
which doesn't do much common sense reasoning but it has...
What it does do is, if you put it in a sentence, it finds five or ten different representations,
anything you can find that's sort of mathematical.
And so when you ask it a question it gives you ten answers and it's much better than previous systems because it doesn't...
Well, Google gives you a quarter million answers but that's too many.
Anyway, I'm just going to talk a little bit more and just...
Everybody should be trying to think of a question that the rest of the class might answer.
So there are lots of different kinds of problems that people can solve going back to the first one,
like which moving object out there is my mother and which might be a potential threat.
So there are a lot of kinds of problems that we solve.
And I've never seen any discussion in psychology books of what are the activities, principal activities of common sense thinking.
Somehow they don't have...
Before computers there really wasn't any way to think about high-level thinking
because there weren't any technically usable ways to describe complicated processes.
The idea of a conditional expression was barely on the threshold of psychology.
So what kinds of problems do we have?
And if you take some particular problem like I find these days I can't get the top off bottles.
So how do I solve that?
There are lots of answers.
One is you look for somebody who looks really strong or you reach into your pocket and you probably have one of these.
And so on.
There must be some way to put it on the floor and step on it and kick it with the other foot.
So there are lots of problems that we're facing every day.
And if you look in traditional cognitive psychology you're...
Well, what's the worst theory?
The worst and the best theory got popular in the 1980s.
It was called rule-based systems and you just have a big library which says if you have a soda bottle
and you can't get the cap off then do this or that or the other.
And so some people decided well that's really all you need.
Rod Brooks in the 1980s sort of said we don't need those fancy theories
that people like Minsky and Papert and Winston are working on.
Why not just say for each situation in the outer world have a rule that says how to deal with that situation?
Let's make a hierarchy of them.
And he described a system that sort of looked like the priority interrupt system in a computer.
And he won all sorts of prizes for this really bad idea that's spread around the world.
But it solved a lot of problems.
There are things about priority interrupt that aren't obvious.
Like suppose you have...
In the first computers there was some problem because what should you do
if there are several signals coming into the computer and you want to respond to them.
And some of the signals are very fast and very short.
Then you might think well I should give the highest priority to the signal that's going to be there the shortest time.
Something like that.
The funny part is that when you made such a system the result was that if you had a computer
that was responding to some signal that's coming in at a...
I'm talking about the days when computers were only working at a few kilohertz.
A few thousand operations a second.
God that's slow.
A billion times shorter than what you have in your pocket.
And if you give priority to the signals that have to be reacted to very fast
then what happens if you type to those computers it would never see them because it's always...
I saw this happening once.
And finally somebody realized that you should give the highest priority to the inputs that come in most...
least frequently.
Because otherwise if there's something coming in very frequently you'll just always be responding to it.
Any of you run into this?
It took me a while to figure out why.
Anyway there are lots of kinds of problems.
And the other day I was complaining that we didn't have enough ways to...
We had hundreds of words for emotions and here's a couple of dozen.
There are chapters seven and eight actually most of these.
So here's a bunch of words for describing ways to think but they're not very technical.
So you can talk about remorse and sorrow and blah blah blah.
Hundreds and hundreds of words for feelings and it's a lot of effort to find a dozen words for...
for intellectual, for what should I call them, problem-solving processes.
So it's curious to me that the great field called cognitive psychology has not focused in that direction.
Anyway here's about 20 or 30 of them and you'll find them scattered through chapters seven and eight.
Here's my favorite one and I don't know of any proper name for it but if you're trying to solve a problem and you're stuck...
and the example that comes to my mind is if I'm trying to remember someone's name I can tell when it's hopeless.
And the reason is that for somehow or other I know that there's a huge tree of choices.
That's one way to represent what's going on and I might know that I'm sure that that letter, that name has a Z in it.
So you search around and try everything you can.
But of course it doesn't have a Z.
So the way to solve that problem is to give up and then a couple of minutes later the name occurs to you and you have no idea how it happened and so forth.
So anyway the long story is that Papert and I and lots of really great students in the 60s and 70s spent a lot of time making little models of problem solvers that didn't work.
We discovered that you needed something else and we put that in.
Other people would come and say that's hopeless.
You're putting in more things than you need and my conclusion is that wow it's the opposite of physics and physics you're always trying to find.
You don't want to what is it called Occam's razor never have more structure than you need because because what well it'll waste your time.
But my feeling was I never have less than you'll need but you don't know how many you'll need.
So what I did I had four of these and then I forced myself to put in two more and people ask what's the difference between self models and self conscious processes and I don't care.
What's the difference between self conscious and reflective.
I don't care.
And the reason is that wow it's nice to have a box that isn't full yet.
So if you find something that your previous theory going back to Brooks.
He was so successful getting simple robots to work that he concluded that the things didn't need any internal representations at all.
And for some mysterious reason the artificial intelligence society gave him their annual big prize for for this very wrong idea.
And it caused a research to sort of half collapse in places like Japan and said oh rule based systems is all we need.
Anybody want to defend him.
The odd thing is if you talk to Brooks he's one of the best philosophers you'll ever meet and he says oh yes of course that's wrong.
But it helps people do research and get things done.
As I think I mentioned the other day when the Three Mile Island thing happened there was no way to get into the reactor.
That was 1980 and 30 years later when the what's how do you pronounce it Fukushima accident happened.
There was no robot that could go in and open a door and I don't know who to blame for that maybe us.
But my picture of the history is that the places that did research on robotics.
There are quite a few places and for example Carnegie Mellon was very impressive in getting the Sony dogs to play soccer.
And they're still at it and I think I mentioned that Sony still has a stock of what's it called.
Say it again.
FIBO.
Right.
I Bose.
Right.
But the trouble is they're always broken and we had a there was a robot here called Cog that Brooks made and it sometimes worked.
But usually it wasn't working and so only one student at a time could experiment with the robot.
What was that wonderful project of trying to make a walking machine for four years in.
There was a project to make a robot walk and there was only one of it.
So first only one student at a time can do research on it and most of the time it's something's broken and you're fixing it.
And so you end up that you sort of get five or ten hours a week on your laboratory physical robot at the same time.
Ed Fredkin had a student who tried to make a walking robot and it was a stick figure on the screen and I forgot the student's name.
But anyway he simulated gravity and a few other things and in a couple of weeks he had a pretty good robot that could walk and go around turns and bank.
And if you put if you simulated an oily floor it could slip and fall which we considered the high point of the demo actually.
So anyway I've sort of asked you to read my two books for this course.
But those are not the only good texts about artificial intelligence.
If you want to dig deeper it might be a good idea to go to the web and type in Aaron Sloman S L O M A N and you'll get to his website which is something like that.
And Sloman is a sort of philosopher who can program.
There are a handful of them in the world and he has lots of interesting ideas that nobody's gotten to carry out.
And so I recommend.
Who else is.
Pat do you ever recommend anyone else.
I'm trying to think.
I mean if you're looking for a lot for philosophers.
Dan Dennett has a lot of ideas but but Sloman is the only person I'd say is a sort of real professional philosopher who tries to program.
At least some of his ideas and he has successful students who have made larger systems work.
So if you get tired of me and you ought to then go look at this guy and see who he recommends.
So OK any who has a good question to ask.
It's a mystery but.
I spent most of the couple of days making this list bigger.
But these aren't you know these are things that you do when you're thinking you make analogies.
If you have multiple goals you try to pick the most important one or in some cases if you have several goals maybe you should try to achieve the easiest one.
And there's a chance that it'll lead you into what to do about the harder ones but.
A lot of people think that mostly in England.
That logic is a good way to do reasoning.
And that's completely wrong.
Because in logic first of all you can't do analogies at all except at a very high level.
It takes four or five nested quantifiers to say A is to B is C is to which of the following five or.
So I've never seen anyone do analogical thinking.
Using formal logic first order or higher order predicate calculus.
What's logic good for.
It's great after you've solved a problem.
Because then you can formalize what you did and see if.
Some of the things you did weren't necessary.
In other words after you've got the solution to a problem which you got by going through a big search you finally found a path from A to Z.
And now you can.
See if the assumptions that you had to make to bridge all these various little gaps were all essential or not so.
Yes.
What example would you say that logic can do analogies like water is for water was like containment or like why.
Why.
Well.
Because you have to make a list of hypotheses.
And then let me see if I can find Evans.
The trouble is.
Don Evans name is in a picture.
And word can't look inside its pictures.
Can power can PowerPoint find words in its illustrations.
Why don't I use PowerPoint.
Because I've discovered that PowerPoint can't read.
Pictures made by other programs in the Microsoft Word suite.
The drawing program in Word is pretty good.
And then there's an operation in Word which will make a PowerPoint out of what you drew.
And.
It's 25 years since Microsoft hasn't fixed the fatal errors that it makes when you do that.
In other words I don't think that the PowerPoint and Word people communicate and they both make a lot of money.
So that might be that might be the reason.
Where was I.
Well you can do anything in logic if you try hard enough but.
But.
A is to be a C is to X.
Is a four part relation and.
You'd need a whole pile of quantifiers and.
How would you know what to do next.
Yes.
About the situation in which we are able to perform some sort of action like really fluently and really well.
But we cannot describe what we're doing.
And the example I give is say I'm like an expert African drummer from Africa.
And I can make these like really complicated rhythms.
But if you ask me what did you just do like I have no idea how to describe it.
And in that case do you think the person is capable of like.
Or I guess do you think the person we can say that the person understands this even though they cannot explain it.
Well.
But if you take an extreme form of that.
You can't explain why you used us any particular word for anything.
There's no reason.
It's remarkable how well people can do in everyday life.
To tell people how they got an idea but when you look at it.
It doesn't say how you would program a machine to do it.
So something very peculiar about.
The idea that.
Goes back to this.
This idea that people have free will and so forth.
Suppose I say.
Look at this and say.
This has a constriction at this point.
Why did I say constriction.
How do you get any how do you decide what word to use for something.
You have no idea.
So it's a very general question.
It's not clear that.
The different parts of the.
That the frontal lobes which might have something to do with.
Making plans and analyzing certain kinds of situations.
Have any access to what happens in the.
Broca or what's the.
What's the speech production area.
Broca and.
I'm trying to find the name of the other one.
It's connected by a cable that's.
About a quarter inch thick.
Yeah.
We have no idea.
How those work.
As far as I've never seen.
Any publication.
In neuroscience.
That says here's a theory of what happens in Wernicke's area.
Have any of you ever seen one.
What do those people think about.
But they'll tell you about.
I was reading something which said it's going to be very hard to understand these areas.
Because each neuron is connected to a hundred thousand little fibers.
Well, some of them are.
And I bet they don't do much except sort of set the bias for.
Some large.
Collection of other neurons.
But.
But we if you ask somebody how did you think of such a word.
They will tell you some story or anecdote.
But they won't be able to describe some sort of procedure.
Which is.
Say in terms of a language like Lisbon.
Say I const this and that and I took the cutter.
Of this and the car of that and I.
Put them in this register and then I.
Swap that with.
You don't see theories of how the mind works.
In psychology today.
The only parts are they know a little bit about.
Some aspects of vision because you can track.
The paths of images from the retina to the.
What to call to the.
Primary visual cortex and.
People have been able to figure out what's what some of those cortical columns do.
And.
If you go back to an animal like the frog then.
Researchers like bitsy and others.
Have figured out how the equivalent of the cerebellum in the frog.
They've got almost the whole circuit of how.
When the frog sees a fly.
It manages to turn its head that way and stick its tongue out and catch it.
But in the case of a human.
I've never seen any theory of how any person thinks of anything.
So.
There's artificial intelligence which has high level theories of.
Semantic representations and there's new neuroscience.
Which has good theories of local some parts of locomotion and some parts of sensory systems.
And.
To this day there's nothing much in between.
So.
David here has decided to go from one to the other and.
Former student of mine Bob her and.
Has done a little bit on both and.
I bet there are 20 or 30 people around the country who have.
Trying to bridge the gap between symbolic artificial intelligence and.
Mapping of the nervous system.
But.
It's very rare and.
I don't know who you could ask to get support to work on a problem like that for five years.
Yeah.
Presumably to build a human like artificial intelligence.
We need to.
Like we need to perfectly model our own intelligence which means that.
We're the system.
We ourselves are the system that we're trying to understand.
Well it doesn't have to be exact.
I mean people are different.
And.
Typical person has looks like they have 400.
Different brain centers doing slightly different things are very different things.
And.
We have these examples in many cases if you lose a lot of your brain.
You're very badly damaged and in other cases.
You recover and become.
Just about as smart as you were.
Probably a few cases where you got rid of something that was holding you back but.
It's hard to prove that.
So we don't need to.
We don't need a theory of.
How people work yet.
And the nice thing about AI is that.
We could get we could.
Eventually get models which are pretty good at solving.
What people call everyday common sense problems.
And.
Probably in many respects they're not the way the human mind works.
It doesn't matter.
But once you've got.
If I had a program which was pretty good at understanding why you can.
Pull with a string but not push.
Then there's a fair chance you could say well.
That seems to resemble what people do I'll do this.
A few psychological experiments and see.
What's wrong with that theory and how to change it.
So at some point.
There'll be people making AI systems.
Comparing them to two particular people.
And trying to make them fit.
The trouble is nowadays it takes a few months to.
If you get a really good new idea to program it.
I think there's something wrong with programming languages.
And what we need is a.
We need a programming language which.
Where the instructions.
Describe goals and then sub goals.
And then finally you might say well let's represent.
This concept by a number or a semantic.
Network of some sort.
But.
Yes.
Programming.
Is there a goal oriented language.
So.
There's kind of one if you think about it.
If you swim hard enough.
Something like SQL.
Where you tell him here you know I want to find.
You know the top 10.
People like my database with this high value.
And then you don't worry about how the system goes about doing that.
In a sense that's.
We're defining the goal when it goes.
What's it called.
SQL.
Oh right yes I guess database query languages are on the track.
But.
Wolfram alpha seems.
To be better than I thought.
Well he was running it.
And he.
Steve Wolfram was giving this demo a meeting we were at.
On Monday.
And he'd say well maybe I'll just say this and.
And it always worked so.
So maybe.
Either the language is better than I thought or.
Wolfram is better than I thought at something.
Remarkable guy.
Yes.
So.
I like this example of.
You only remember a game after your.
You remember a game after you've given up consciously trying to think about it.
Do you think this is a matter of us being able to set up background.
Processes and then.
Either.
There's some delay like we give up.
There's some delay in the process or we don't have the ability to correct me.
Terminate.
Processes.
Do you think this only works for memory or could work for.
Other things.
Starting.
Well there's a lot of nice questions about.
Things like that how many processes can you run at once in your brain and.
I was having a sort of argument.
The other day about.
Music.
And.
I was wondering if.
I see a big difference between Bach and.
The.
And.
The composers who do counterpoint.
Counterpoint.
You usually have several versions of a very similar idea.
Maybe there's one theme.
And you.
Have it playing and then another voice comes in.
And it has that theme upside down or a variation of it.
In some cases exactly the same.
And then it's called a cannon.
And.
So the tour de force in classical music.
Is when you have two or three or four.
Versions of the same thought going on at.
Once in different times.
And my feeling was that in.
Popular music or red.
If you take it to a typical band.
Then.
There might be four people.
And they're doing different things at the same time.
Usually not the same.
Musical tunes.
But there's a rhythm and there's.
A timpani and there's various instruments doing different things.
But there you don't have several doing the same thing.
I might might be wrong and.
Somebody said well some some popular music.
Has a lot of counterpoint.
I'm just not familiar with it.
But I think that's.
If you're trying to solve a hard problem.
It's fairly easy to look at the problem in several different ways.
But what's hard is to look at in several almost the same ways that are slightly different.
Because probably.
If you believe that the brain is made of agents or resources or whatever.
You probably don't have duplicate copies of.
Ones that do important things because.
That would take up too much real estate.
Anyway I might be completely wrong about.
Jazz somebody.
Maybe they have.
Just as complicated overlapping things.
As.
Buck and.
The Contrapuntal Composers did but.
Yeah.
What is the ultimate goal of artificial intelligence?
Is it some sort of application or is it more philosophical?
Oh everyone has different goals or ones.
In your opinion.
I think we're going to need it because.
The disaster that we're working our way toward is that.
People are going to live longer.
And.
They'll become slightly less able.
And so you'll have.
Billions of 200 year old people who can barely get around.
And there won't be enough.
People to import from underdeveloped countries.
To.
Or they won't be able to afford them.
So we're going to have to have machines that take care of us.
Of course that's just a transient because.
At some point then you'll download your brain into.
A machine and fix everything that's wrong.
So we'll need robots for a few hundred years.
Or a few decades and.
Then we'll be them and we won't need them anymore.
But it's an important problem.
What's going to happen in the next hundred years.
You're going to have.
Twenty billion two hundred year olds and nobody to take care of them.
Unless we get a I.
Nobody seems particularly sad about that.
How long.
Oh another anecdote.
Because once giving a lecture and.
Talking about people living a long time.
And nobody in the audience seemed interested in.
I'd say well suppose you could live 400 years and.
Most of the people I then I asked what.
What was the trouble and said wouldn't it be boring.
So then I tried it again in a couple of other lectures.
And.
If you ask a bunch of scientists.
How would you like to live 400 years.
Everyone says yay.
And you ask them why and they say.
Well I'm working on a problem that.
I might not have time to solve but if I.
If I had 400 years I bet I could get somewhere on it.
And the other people don't have any goal.
That's my cold-blooded view of the.
Typical non-scientist.
There's nothing for them to do in the long run.
Who can think of what should people do.
What's your goal.
How many of you want to live 400 years.
Wow.
Must be scientists here.
Try it on some crowd and let me know what happens.
Are people really afraid yeah.
Differentiating factors whether or not you're.
400 years it's just going to be the repetition.
Of 100 years experience or that will start to like.
Take off.
Well progress.
Right.
I've seen 30.
Issues of the big bang and I don't look forward to.
The next one anymore because they're all this.
They're getting to be all the same.
Well it's the only thing on TV that has.
Scientists.
Seriously I hardly read anything except journals and science fiction.
Because.
Yeah.
I can't think of any advantages except that medicine has.
Isn't getting.
The the age of.
Unhandy cap people.
Went up at one year every four.
Since the late 1940s.
So life span is.
So that's 60 years so people are living 15 years longer.
On the average then.
They did when I was born.
Or even more than that.
But it's leveled off lately.
Now I suspect that you only have to fix a dozen genes or.
Who knows nobody.
Really has a good estimate.
But you can probably double the lifespan if you could.
Fix.
Nobody knows but maybe there's just a dozen processes that.
That would fix a lot of things and.
Then you could live longer without deteriorating.
And lots of people might.
Get bored.
But.
They'll self select.
I don't know.
What's your answer.
I.
I hear that.
Creating I know that.
And more.
I mean the goal is not to help take care of people but to compliment.
What we already have.
You could also look at them as our descendants and.
They we will have them replace us.
And.
Just as a lot of people consider their children to be.
The next generation of them.
And I know a lot of people who don't.
So it's.
It's not a universal.
But.
What's the point of anything I don't want to get in.
We might be the only intelligent life in the universe and.
In that case it's very important that we.
Solve all our problems and make sure.
That something intelligent persists.
Carl Sagan had some argument of that sort.
If you were sure that there were lots of others.
Then.
Then it wouldn't seem so important.
Who's the new Carl Sagan.
Is there any who's the who's the note.
Is there a public scientist.
Who.
All the time.
Tyson.
He's very good.
Tyson is the astrophysicist.
Brian Green is a great actor.
Quite impressive.
Yeah.
Machine.
Well I think that's a funny question because.
If we're programming it.
We can make sure that the machine has a.
Very good.
Abstract.
But correct model of how it works.
Which people don't.
So people have a sense of self.
But it's only a sense of self and it's.
It's just plain wrong in almost every.
Every respect.
And.
So it's a really funny question because.
When you make a machine.
That really has a.
Good useful representation of.
What it is and how it works.
It might be quite different.
Have different attitudes than a person does.
Like it might not consider itself very valuable and say.
It's a oh I could make something that's even better than me and jump into that.
And so it wouldn't have the.
It might not have any self protective.
Reaction because.
If you could improve yourself then you don't want not to.
Whereas we're in a state where there's nothing much we can do except.
Try to keep living and.
We don't have any alternative.
If.
Stupid.
Thing to say.
I can't imagine getting tired of living but.
Lots of people do.
Yeah.
Think about creative thinking and the way of thinking and.
Where does this.
I had a little section about that somewhere that I wrote.
Which was the difference between artists and scientists or engineers and.
Engineers are in.
Have a very nice situation because.
They know what they want.
Because somebody's ordered them to make a.
In the last month three times I've walked away.
From my computer and.
How many of you have a Mac with the magnetic.
Thing.
And three times I pulled it with by tripping on this and it fell to the floor and didn't break.
And I've had Max for 20 odd years or since 1980.
When did they start.
30 years and.
They used to they have the regular jack power supply in the old days.
And I don't remember and usually when you pull the cord it comes out.
Here's this cord that Steve Jobs and everybody designed.
Very carefully so that when you pull it nothing bad would happen.
But it does.
How do you account for that.
Yeah.
Yeah.
Well it's quite a wide angle.
Yeah.
Well what it needs is a little ramp so that it would slide out.
I mean it would only take a minute to file it down so that it would slide out.
But they didn't.
I forget why I mentioned that but.
Right.
So what's the difference between an artist and engineer.
Well when you do a painting.
It seems to me if you're already good at painting.
The nine tenths of the problem is what should I paint.
So you can think of an artist as.
10% skill and 90% trying to figure out what the problem is to solve.
Whereas for the engineer.
Somebody's told him what to do make a better cable connector.
And so he's going to spend 90% of his time actually solving the problem.
And only 10% of the time.
Trying to decide what problem to solve.
So I don't see any difference between artists and engineers.
Except that.
The artist has more problems to solve than it could possibly solve.
And usually ends up by picking a really dumb one.
Like.
Let's have a saint in three angels.
Where will I put the third angel.
The engineering part.
Just just improvising.
So.
To me the media lab makes sense.
The artists and were semi artists and the scientists.
Are doing almost the same thing.
And if you look at the more arty people.
They're a little more concerned with human social relations and.
And others are more concerned with.
Very technical specific aspects of signal processing or.
Semantic representations.
So what.
But.
So I don't see much difference.
Between the arts and the sciences.
And then.
Of course the great moments are when you run into.
People like Leonardo and Michelangelo who.
Get some idea that requires a great new technical.
Innovation that.
Nobody it's ever done and.
It's hard to separate them.
I think there's some place where Leonardo.
Realizes that the lens in the eye would mean.
That the image is upside down.
On the retina and he couldn't stand that.
So there's a diagram he has.
Where the corny is curved enough.
To invert the image.
And then the lens inverts it back again.
Which is contrary to fact with.
He has a sketch showing that he was worried about.
If.
If the image were upside down on the retina.
Wouldn't things look upside down.
But.
Yeah.
I don't know if they have a question.
Did you ever heard of.
Hiring.
High air.
Temporal memory.
Temporal.
Temporal memory.
Like.
There's a system that I've had.
Right.
That's right.
Great.
That's right.
How.
Great.
Requires them.
They have a company called.
And they're going to release.
At the end of this year.
On it.
And there's like some research.
They have paper.
Well, I'm not sure what.
This is Jeff Hawkins project.
I don't know.
Yes.
I haven't heard about 10 years ago he said Hawkins.
Yeah.
Hawkins.
Yeah.
Well he was talking about 10 years ago how.
Great it was and I haven't heard a word of any progress.
Is there some.
Anybody heard it.
There's a couple of books about it but.
I've never seen any claim of that it works.
They wrote a ferocious review of the society of mind.
In.
Which came out in 1986.
And.
The Hawkins group existed then and had this.
Talk about a hierarchical memory system.
But.
So if I can tell it's all bluff nothing happened.
I've never seen a report.
That they have a machine which solved the problem.
Let me know if you find one.
Because.
Oh well.
Hawkins got really mad at me for pointing this out.
But.
I was really mad at him for.
Having four of his.
Assistants write a bad book review of my book so.
I hope we were even.
If anybody can find out whether I forget what it's called you remember its name.
The game.
Well let's find out if it can do anything yet.
Hawkins is wealthy enough to support it for a long time so.
It should be good by now.
Yes.
People first start out with some sort of.
Classification in their head of the kind of problem it is for.
Is that not necessary.
Yes.
That's.
Well.
There's this huge book called human problem solving.
Which was.
I don't know how many of you know.
The names of Newell and Simon.
Originally was Newell Shaw and Simon.
And in the.
Believe it or not in the late 1950s.
They did some of the first.
Really productive AI research.
And.
Then I think.
In 1970.
So that's.
After 12 years of.
Discovering interesting things.
Their main discovery was.
The gadget that they called GPS.
Which is not global positioning satellite but.
General problem solver.
And.
You can look it up in the index.
Of my book and there's a sort of.
One or two page description.
If you ever get some spare time.
Search the web for their early.
Early paper by Newell and Simon on.
How GPS worked because it's really fascinating.
What it did is it looked at a problem and found some features of it.
And then looked up in a table saying that.
If there's this difference between what you have and what you.
Want.
Use such and such a method.
So it was so what I called it.
We named it a difference engine.
As a sort of joke.
Because the first computer.
In history was.
The one called the difference engine but.
It was for.
Predicting tides and things.
Anyway.
They did some beautiful work.
And there's this big book.
Which I think is about 1970.
Called human problem solving.
And.
What they did is.
Got some people to solve problems.
And they trained the people to talk while they're solving the problem.
So some of them were little cryptograms like.
If each letter stands for.
For a digit.
I've forgotten it.
Pat, do you remember the name one of those problems.
John plus Joe.
John plus Jane equals Robert or something.
That I'm sure that has no solution.
But those are called crypt arithmetic.
And so they had dozens or hundreds of people.
Who would be trained to talk aloud while they're solving.
Little puzzles like that.
And then.
What they did was.
Get exactly what the people said and how long they took and.
In some cases where they moved their eyes.
They had an eye tracking machine.
And then they wrote programs.
That showed how this guy solved a couple of these crypt arithmetic problems.
Then they ran the program on a new one.
And in some rare cases it actually solved the.
The other problem so this is a book.
Which.
Looks at human behavior and makes a theory of what it's doing.
And the output is a rule based system.
So it's.
It's not a very exciting theory, but.
But.
There had never been anything like it.
Inside.
You know, it was like Pavlov discovering conditioned reflexes for rats.
Or dogs.
And Newland Simon.
Discovering some.
Rather higher level.
Almost a Rodney Brooks like system.
For how humans solve some problems that most people find pretty hard.
See.
Anyway.
What there hasn't been as much.
I don't know of any follow up they spent years.
Perfecting those experiments and.
Writing about.
Results.
And.
Anybody know anything like that.
What psychology is to try to make.
Real models of real people solving.
Point problems.
My.
It has a green light.
It has a green light, but the switch was up.
Boo.
Oh, it doesn't.
Yes.
Did that study.
Try and see when.
A person gave up on a particular problem solving method.
How they switched another which one is which to based on.
It has it has inexplicable.
Points at which.
The person suddenly gives up on that representation.
And he says, oh, well, I guess.
I guess are must be three.
Did I erase well.
Yes, it's got episodes.
And they can't account for the.
These little jerks in the script.
Where the model changes.
And.
Sorry.
And they announced those to be mysteries.
And say, here's a place where the.
Person has decided the strategy isn't working.
And starts over or is changing something.
The amazing part is that their model.
Sometimes fits what the person says.
For 50 or even 100 steps.
The guy saying, oh, I think two must be.
Z must be two and P must be seven.
And that means he plus he is nine.
And I wonder what's nine.
And.
So their model fits for very long.
Strings maybe two minutes of the person mumbling.
To themselves.
And then.
It breaks and then.
Is another sequence.
So new will actually spent.
More than a year after doing it verbally.
At tracking the person's eye motions.
And trying to correlate the person's eye motions with.
What the person was talking about.
And guess what.
None.
It was almost as though.
You look at something.
And then to think about it, you look away.
It was a.
Newell was quite distressed because.
He spent about a year.
Crawling over this data trying to.
Figure out what kinds of mental events caused the eyes to.
Change what they were looking at.
But when the problem got hard, you would look at a blank part of the thing more.
More often than.
The place where the problem.
Turned up.
So conclusion.
That didn't work.
When I was a very young.
Student in college I had a.
Friend named Marcus Singer who.
Was trying to figure out how the.
Nerve in the four limb of a frog worked.
And so he was operating on tadpoles and.
He spent about six weeks.
Moving this sciatic nerve.
From the leg up to the arm.
Of this tadpole.
And then they all got some fungus and died.
So I said what are you going to do and he said well.
I guess I'll have to do it again.
I switch from biology to mathematics.
But in fact he discovered the.
Growth hormone that.
He thought came from the nerve and made that.
If you cut off the limb but of a tadpole.
It'll grow another one and grow a whole.
It was a nude I'm sorry it's salamander.
It'll grow a new hand.
If you wait till it's got a.
Substantial hand it won't grow a new one.
But he discovered the hormone that.
Makes it do that.
Yeah.
Questions from the homework that.
Relates to the problem solving.
A common theme is having multiple ways to react to the same problem.
Which.
So we have a whole lot of if thens and we have to.
Choose which if.
I don't think I have a good theory of that.
Yes if you have a huge rule based system and they're.
What does Randy Davis do.
What if you have a rule based system and.
A whole lot of rules fit.
Ifs fit the condition.
Do you just take the one that's most often worked.
Or.
If nothing seems to be working do you.
You certainly don't want to keep trying the same one.
I think I mentioned Doug Lenet's rule.
Some people will assign probabilities to things.
To behaviors.
And then pick.
The way to react in proportional to the probability that.
That thing has worked in the past.
And Doug Lenette.
Thought of doing that but instead he just put the things in a list.
And whenever a hypothesis.
Worked better than another one.
He would raise it.
Push it toward the front of the list.
And then whenever there was a choice it would pick.
If all the rules that fit it would pick the one at the top of the list.
And if that didn't work it would get demoted.
So.
That's when I became an anti probability.
Person.
That is.
If just sorting the things on the list worked pretty well.
Our probabilities going to do much better.
No.
Because if you do probability matching.
You're worse off then.
Then what.
Ray Solomon off discovered that.
If you have.
A set of probabilities that something will work.
And you have no memory.
So that each time you comments.
Try the.
I think I mentioned that the other day but.
It's worth emphasizing because.
Nobody in the world seems to know it.
Suppose you have a list of things.
He.
Equals this.
Or that.
Or that.
And.
In other words suppose is a hundred boxes here.
And.
One of them has a.
Gold brick in it.
And the others don't.
And so for each box.
Suppose the probability is.
Point nine.
That this one has the gold brick.
And.
This one has point oh one.
And this has point oh one.
Let's see how many of them.
So there's ten of these.
That makes.
Now what should you do suppose you're allowed to.
Keep choosing a box.
And.
You want to get your gold brick as soon as possible.
What's the smart thing to do.
Should you.
But you have no memory.
Maybe the gold brick is decreasing in value.
I don't care.
But.
So should you keep trying point nine.
If you have no memory.
Of course not.
Because if you don't get it the first time you'll never get it.
If you tried them at random.
Each time.
Then you'd have point nine chance of getting it.
So in.
In two trials.
You'd have.
What am I saying.
In a hundred trials.
You're pretty sure to get it.
But.
In a hundred trials.
Almost certain.
So if you don't have any memory.
Then probability matching is not a good idea.
Certainly picking the highest probability.
Is not a good idea.
Because.
If you don't get it the first trial you'll never get it.
If you keep using the probabilities.
At.
What am I saying.
Anyway what do you think is the best thing to do.
It's to take the square roots of those probabilities.
And then divide them by the sum of the square roots.
So it adds up to one.
So a lot of psychologists.
Design experiments until they get the rat.
To match the probability.
And then they publish it.
Sort of like the.
But if the animal is optimal and doesn't have much memory.
It shouldn't match the probability of the unknown.
It should.
End of story.
Every now and then I search.
Every few years.
To see if anybody has noticed this thing which.
And I've never found it on the web.
Yeah.
So earlier in the course.
I didn't don't mean to say they don't work.
Rule based methods are great.
For some kinds of problems so.
Most.
Most systems make money and.
You know if.
If you're trying to.
Make hotel reservations and things.
This business of rule based systems.
It has a nice history.
A couple of AI researchers really.
Notably Ed Feigenbaum.
Who was a.
Student of Newell and Simon.
Started a company for making rule based systems.
And.
Company did pretty well for a while.
Until.
And they maintained that.
Only an expert in artificial intelligence.
Could be really good at making rule based systems.
And so they had a lot of customers and.
Quite a bit of success for a year or two.
And then some people at Arthur D little.
Said oh we can do that and.
They made some systems that worked fine.
And.
The market disappeared.
Because it turned out that.
You didn't have to be.
Good at anything in particular.
To make rule based systems work.
But.
For doing.
Harder problems like translating from one language to another.
You really needed to have more structure and.
You couldn't just take the probabilities of.
Words being in a sentence that you.
Had to look for diagrams and trigrams and.
Have some grammar theory and so forth so.
But generally if you have a.
Ordinary data processing problem.
Try rule based system first because.
If you understand what's going on good chance you'll.
Get things to work I'm sure that's what the.
Hawkins.
Thing started out as.
I don't have any questions.
Sure.
Machines use relatively few electronic components to run a.
Different type of thought operations.
All that changes is data over which the operation runs.
In the critic selector model resources different.
Of data with different physical parts to bring.
Which model.
Critics left.
Oh actually.
I've never seen a.
I've never seen a large scale theory of how the brain.
Connects it's.
There doesn't seem to be a global model anywhere.
Anybody.
Read any.
Of science books lately.
I mean.
I just don't know of any.
Any big diagrams.
Here's this wonderful behavioral diagram so.
How many of you have run across the word ethology.
Just a few.
There's a branch of.
The psychology of animals.
Which is.
Thanks.
Which is called ethology.
And it's the study of instinctive behavior.
So these.
And the most famous people in that field.
Who.
Well Tim Bergen Nico Tim Bergen and Conrad Lorenz.
Are the most famous.
I've just lost the name of the.
Guy around the 1900.
Who wrote.
A lot about the behavior of ants.
Anybody.
Ring a bell.
So he was the sort of the first ethologist.
And these people don't study learning because it's hard to.
I don't know why.
But.
So they're studying instinctive behavior which is.
What are the things that all fish do.
Of a certain species.
And.
You get these big diagrams.
And.
This is from a little book.
Which you really should read.
Called the study of instinct.
And.
It's a beautiful book.
And if that's not enough.
Then there's a two volume.
Similar book by Conrad Lorenz.
Who.
Was.
Austrian researcher.
They report.
They did a lot of stuff together.
These two.
People.
And it's full of.
Diagrams showing.
The.
Main behaviors that they were able to observe.
Of various.
Low cost animals.
I think I.
Mentioned that I had some fish and I.
Watched the fish tanks.
What they were doing for.
A very long time.
And.
Came to no conclusions at all.
And.
When I finally read.
Timbergen and Lorenz.
I realized that.
Just it never occurred to me.
To.
To guess what to look for.
My favorite one was.
That whenever a fire engine went by.
Lorenz's sticklebacks the male sticklebacks.
Would go crazy and look for a female.
Because when the females in heat or whatever it's called.
Estrus.
The lower abdomen turns red.
I think fire engines have turned yellow recently.
No one.
I don't know what the.
Sticklebacks do about that.
So if you're just an AI.
You really should look at.
At least one of these people.
Because.
It's the first appearance of rule based.
Systems in great detail in psychology.
There weren't any computers yet.
There must be 20 questions left.
Yeah.
So.
I know that.
Early on.
People were kind of.
They're careful not to apply.
Ecology of humans.
Till about 60s.
With.
Sociobiology.
So if you're thinking on that.
Maybe.
Around this area.
I don't know.
I sort of grew up with Ed Wilson because we.
Had the same fellowship at Harvard for three years.
But he was almost never there.
Because he was.
Out in the jungle in some little.
Telephone booth watching the.
Birds or.
Bees or.
You also had a 26 year old aunt.
Not aunt.
Aunt.
A and T.
I'm not sure what the controversy would have been but.
Of course.
There would be humanists who would say.
People aren't animals but.
But then what the devil are they.
Why aren't they better than they.
You've got to read this it's a fairly short book.
And you'll never.
See an animal is the same again.
Because.
I swear.
You start to notice.
All these little things you're probably wrong.
But.
You start picking up little pieces of behavior and trying to.
Figure out.
What what part of the instinct system is it and.
Lawrence was particularly I think in chapter two of the.
A motion machine I have some quotes from.
These guys and.
Lawrence was particularly interested in.
In how.
Animals got attached to their.
Parents that is.
For those animals that do get attached to.
Like alligator babies.
Live in the alligator's mouth.
For quite a while.
It's a good safe place.
And.
Lawrence would.
Catch birds.
Just when they're hatching.
And within the first day or so.
Some baby birds get attached to whatever large moving.
Object is nearby.
And he that was often Conrad Lawrence.
Rather than the birds.
Mother who's.
Supposed to be sitting on the egg when it hatches and.
The bird gets attached to the mother most.
Most birds do because.
They have to stay around and get fed.
So.
It is said that wherever Lawrence went.
In Vienna.
There were some ducks or whatever.
Birds that had gotten imprinted on him.
Would come out of the sky and land on his shoulder and.
And on no one else.
And he has various theories of.
How they recognize him.
But.
But you could do that too.
Anyway that was quite a field.
This thing called ethology.
And.
Between 1920 and 1950.
1930 I guess.
There were lots of people studying.
The behavior of animals and.
Ed Wilson is.
Probably the.
Most.
Well known successor to.
Lorenz and Timbergen.
And I think he just wrote a book is that.
Anybody seen it.
He has a huge book called socio biology.
Which is too heavy to read.
I've run out of things.
Yes.
Thank you.
Society of mind.
Ideas.
That book.
Had the machinery from it.
What would the initial state of the machine be.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You.
You you.
You.
I guess it depends whether you want it to be a person
or a marmoset or chicken or something.
Are there some animals that don't learn anything?
Must be.
What are the ones that Sidney Brenner studied?
See, I'm gonna say like, very simple associations.
The little worms?
There was a rumor that if you fed them RNA, was it them or was it some slightly higher animal?
It was worms.
What?
RNA interference.
Is that what you're talking about?
Yeah.
There was one that if you taught a worm to turn left when there was a bright light or
right and put some of its RNA into another worm, that worm would copy that reaction even
though it hadn't been trained.
And this was…
That's just a worm.
Is that what it is?
Slugs.
Slugs.
I think it was, yeah.
Yes.
It's a lazy-art scarex or something.
It's a little snail-like thing.
And nobody was ever able to replicate it.
So it's that rumor spread around the world quite happily and there was a great science
fiction story trying to remember in which somebody got to eat some of an alien's RNA
and got magical powers.
I think it's Larry Niven who is wonderful at taking little scientific ideas and making
a novel out of them.
And his wife Marilyn was a undergraduate here.
So she introduced me to Larry Niven and I once got to write an article.
I once gave a lecture and he wrote it up.
It was one of the big thrills because Niven was one of my heroes.
Imagine writing a book with a good idea in every paragraph.
Bernie Vingy and Larry Niven and Frederick Poe seem to be able to do that or at least
on every page.
I don't know about every paragraph.
Yeah.
To follow up on that question, it seems to me that you almost were saying that if this
was the difference between these sort of animals and depending on the start of the day, we
could either create a chicken or a human.
Well no, I don't think that I don't think that most animals have scripts.
I might, but I'd say that I don't know where most animals are, but I sort of make these
six levels and I'd say that none of the animals have this top self-reflective layer except
for all we know, dolphins and chimpanzees and whatever, it would be nice to know more
about octopuses because they do so much of wonderful things with their eight legs.
What kind of, how does it manage?
Have you seen pictures of an octopus picking up a shell and walking to some quiet place
that can, there's some movies of this on the web and then it drops the shell and climbs
under it and disappears.
It's hard to imagine programming a robot to do that.
Yeah.
So I've noticed both your books in the lecture, a lot of your models and diagrams seem to
have a very hierarchical structure to them, but as you mentioned in your book, other places,
passing between levels, feedback and self-reference are all very important to diligence.
So I'm curious if you could discuss some of the uses of these very hierarchical models,
why you represent so many things in that way and some implementation theory?
Well, it's probably very hard to debug things that aren't, so we need a sort of meta theory.
One thing is that, for example, it looks like that all neurons are almost the same.
Now, there's lots of difference in geometric features of them, but they all use the same
one or two transmitters and every now and then you run across people saying, oh, neurons
are incredibly complicated.
They have 100,000 connections.
You can find it if you just look up neuron on the web and get these essays, explaining
that nobody will ever understand them because typically a neuron is connected to 100,000
others and blah, blah, blah.
So it must be something inside the neuron that figures out all this stuff.
As far as I can see, it looks almost the opposite, namely probably the neuron hasn't changed
for half a billion years very much, except in sort of superficial ways in which it grows
because if you changed any of the genes controlling its metabolism or the way it propagates impulses,
then the animal would die before it was born.
That's why the embryology of all mammals is almost identical.
You can't make a change at that level after the first generations of cell divisions or
everything would be clobbered, the architecture would be all screwed up.
So I suspect that the people who say, well, maybe the important memories of a neuron are inside it
because there's so many fibers and things.
I bet it's sort of like saying the important memory in a computer is in the arsenic and phosphorus atoms
of the semiconductor.
So I think things have to be hierarchical in evolution because if you're building later stuff on earlier stuff,
then it's very hard to make any changes in the earlier stuff.
So as far as I know, the neurons in sea anemones are almost identical to the neurons in mammals
except for the later stages of growth and the way the fibers ramify.
Who knows, but there are many people who want to find the secret of the brain in what's inside the neurons
rather than outside.
It would be nice to get a textbook on neurology from 50 years in the future,
see how much of that stuff mattered.
Where are time machines?
Most systems have a state that they prefer to be in, like a state that they're most comfortable in.
Do you think the mind has such a state or would it tend to certain places?
That's interesting. How does that apply to living things?
I mean this bottle would rather be here than here, but I'm not sure what you mean.
Okay, so apparently in Professor Tannenbaum's class he shows this example of a number game.
He'll give you a sequence of numbers and he'll ask you to find a pattern in it.
For example, if you had a pattern like 10, 40, 50, and 55,
he kind of asks the class to come up with different things that could be describing the sequence.
Between the choice of, oh this sequence is a sequence of the multiples of 5
versus a sequence of the multiples of 10 or multiples of 11.
He says something like the multiples of 5 would have a higher private probability.
So that got me thinking, why would that be?
Would our minds have a preference for having as few categories as possible
and trying to view the world around us?
Trying to categorize things and as few things as possible?
Sounds very strange to me, but certainly if you're going to generate hypotheses,
you have to have the way you do it depends on what does this problem remind you of.
So I don't see how you could make a general...
If you look at the history of psychology, there are so many efforts to find four laws,
three laws of motion like Newton's.
Is he trying to do that?
And here you're talking about people with language and high level semantics.
Let's ask him what he meant.
Yeah, it's more of a social question, but there's always this debate about
how if AI gets to a point where it can take care of humans, will it ever destroy humanity?
And do you think that's something that we should fear?
And if so, is there some way we can prevent it?
If you judge by what's happened in AI since 1980, it's hard to imagine anything to fear.
Funny you should mention that I'm just trying to organize a conference sometime next year
about disasters.
And there's a nice book about disasters by...
What's his name?
The Royal... The Astronomer Royal.
What?
Martin Rees.
So he has a nice book which I just ordered from Amazon and it came the next day.
And it has about ten disasters like a big meteor coming and hitting the earth.
I forget the other ten, but I have it in here somewhere.
So I generated another list of ten to go with it.
So there are lots of bad things that could happen.
But I think right now that's not on the top of the list of disasters.
Eventually some hacker ought to be able to stop the net from working because it's not very secure.
And while you're at it, you could probably knock out all of the navigation satellites and maybe set off a few nuclear reactors.
But I don't think AI is the principal thing to worry about.
But it should very suddenly get to be a problem.
And there are lots of good science fiction stories.
My favorite is the Colossus series by DF Jones.
Anybody know?
There was a movie called The Forbidden Project.
And it's about somebody who builds an AI and it's trained to do some learning.
And it's also the early days of the web and it starts talking to another computer in Russia.
And suddenly it gets faster and faster and takes over all the computers in the world and gets control of all the missiles.
Because they're linked to the network.
And it says, I will destroy all the cities in the world unless you clear off some island and start building the following machine.
I think it's Sardinia or someplace.
So they get bulldozers.
And it starts building another machine which it calls Colossus 2.
And they ask, what's it going to do?
And Colossus says, well, you see, I have detected that there's a really bad AI out in space and it's coming this way.
And I have to make myself smarter than it really quick.
Anyway, see if you can order the sequel to Colossus.
That's the second volume where the invader actually arrives and I forget what happens.
And then there's a third one which was an anticlimax because I guess D.F. Jones couldn't think of anything worse that could happen.
But Martin Rees can.
Yeah.
Going back to her question, an example, if my mind has a state, would that example be more of a pattern recognition example?
So instead of 10, 40, 50, 55, it wasn't physical.
Good, fine, great.
And you have to come up with a word that could potentially fit in that pattern.
And then that pattern could be ways to answer it.
How are we?
Let's do an experiment.
How many of you have a resting state?
Sometimes when I have nothing else to do, I try to think of twinkle, twinkle, little star happening with the second one starting in the second measure.
And then the third one starts up the third measure.
And when that happens, I start losing the first one.
And ever since I was a baby, when I have nothing else to do, which is almost never, I try to think of three versions of the same tune at once and usually fail.
What do you do when you have nothing else to do?
Any volunteers?
What's yours?
I don't have to think anything at all.
All right, not two or two?
Not two.
Isn't that a sort of Buddhist thing?
Yes, sir.
Do you ever succeed?
How do you get out of it?
You have to think, well, enough of this, nothingness.
If you succeeded, wouldn't you be dead?
We're stuck.
Eventually some stimulus will appear that is too interesting to ignore.
Right, and threshold goes down until even the most boring thing is fascinated.
Make a good short story.
There was actually a movie that really got to me when I was little.
These aliens were trying to infiltrate people's brains and like their thoughts.
To keep the aliens from infiltrating your thoughts, you had to think of a wall,
which didn't make any sense at all, but now whenever I try to think of nothing,
I just end up thinking of a wall.
These awful psychoses and about every five years,
I get an email from someone who says that,
please help me, there's some people who are putting these terrible ideas in my head.
Have you ever gotten one, Pat?
And they're sort of scary because you realize that maybe the person
will suddenly figure out that it's you who's doing it.
I remember there was once,
one of them came to visit, actually showed up and he came to visit Norbert Wiener,
who is famous for, I mean he's the cybernetics person of the world.
And this person came in and he got between Wiener and the door
and started explaining that somebody was putting dirty words in his head
and making the grass on their lawn die.
And he was sure it was someone in the government and this was getting pretty scary.
I was near the door, so I went and got lethin.
It's a true story because nearby and I got lethin to come in
and lethin actually took this guy down and took him by the arm and went somewhere
and I don't know what happened, but Wiener was really scared
because the guy kept keeping him from going out.
Lethin was big. Wiener is not very big.
Anyway, that keeps happening every few years.
I get one and I don't answer them.
He's probably sending it to several people and I'm sure one of them
is much better at it than we are.
How many of you have ever had to deal with an obsessed person?
How did they find you?
I don't know. They found a number of people in the media lab, actually.
Don't answer anything.
But if they actually come, then it's not clear what to do.
Last question.
Thanks for coming.
Thank you.
