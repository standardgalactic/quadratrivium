1
00:00:00,000 --> 00:00:03,000
to meet everyone. Maybe I can put everyone's name.

2
00:00:03,000 --> 00:00:05,000
Sure. My name is Richard.

3
00:00:05,000 --> 00:00:06,000
I'm Elian.

4
00:00:06,000 --> 00:00:07,000
Elian?

5
00:00:07,000 --> 00:00:08,000
Yes.

6
00:00:08,000 --> 00:00:09,000
Mariam.

7
00:00:09,000 --> 00:00:10,000
Mariam?

8
00:00:10,000 --> 00:00:11,000
Yes.

9
00:00:11,000 --> 00:00:12,000
Can we meet you?

10
00:00:12,000 --> 00:00:13,000
Yes.

11
00:00:13,000 --> 00:00:14,000
Mariam?

12
00:00:14,000 --> 00:00:15,000
Elian?

13
00:00:15,000 --> 00:00:16,000
And I'm Isla.

14
00:00:16,000 --> 00:00:17,000
Isla?

15
00:00:17,000 --> 00:00:18,000
Yes.

16
00:00:18,000 --> 00:00:21,000
Don't go too fast. No, I gotta remember all these things.

17
00:00:21,000 --> 00:00:24,000
Isla, Isla, and Mariam.

18
00:00:24,000 --> 00:00:25,000
Say it again.

19
00:00:25,000 --> 00:00:26,000
Elian.

20
00:00:26,000 --> 00:00:27,000
Elian.

21
00:00:27,000 --> 00:00:28,000
All right.

22
00:00:28,000 --> 00:00:29,000
Okay.

23
00:00:29,000 --> 00:00:30,000
Yeah.

24
00:00:30,000 --> 00:00:31,000
Yeah?

25
00:00:31,000 --> 00:00:32,000
Yeah.

26
00:00:32,000 --> 00:00:33,000
Okay. Please meet you.

27
00:00:33,000 --> 00:00:34,000
Okay.

28
00:00:34,000 --> 00:00:35,000
Okay.

29
00:00:35,000 --> 00:00:42,000
So, we are a few, and we can go whichever ways you guys are interested in.

30
00:00:42,000 --> 00:00:43,000
I'm interested in.

31
00:00:43,000 --> 00:00:51,000
And I got a bunch of slides, but there's only a few that I really want to present for sure.

32
00:00:51,000 --> 00:00:58,000
Well, I sort of have the impression that we've a little bit made history many times when

33
00:00:58,000 --> 00:01:00,000
you talk to her.

34
00:01:00,000 --> 00:01:06,000
So, anything that you think would be more likely to be making history is more interest than

35
00:01:06,000 --> 00:01:10,000
things that aren't going to make history.

36
00:01:10,000 --> 00:01:14,000
Well, I think it is a sort of a time.

37
00:01:14,000 --> 00:01:20,000
And I'm sure you're all young people, and you're only familiar with your own time in

38
00:01:20,000 --> 00:01:21,000
some sense.

39
00:01:21,000 --> 00:01:22,000
And you think it's just normal.

40
00:01:22,000 --> 00:01:25,000
But I don't think this is a normal time you're in.

41
00:01:25,000 --> 00:01:35,000
I think it's a time when, well, massively increasing computation is available to everybody

42
00:01:35,000 --> 00:01:38,000
into our companies and people make products.

43
00:01:38,000 --> 00:01:43,000
And this is really the only time like this.

44
00:01:43,000 --> 00:01:48,000
I mean, some sense, under 200 years ago, there was the Industrial Revolution.

45
00:01:48,000 --> 00:01:55,000
They discovered machines that, you know, like steam powered and physical work for people.

46
00:01:55,000 --> 00:01:58,000
No, but that really took a long time.

47
00:01:58,000 --> 00:02:06,000
Whereas now, with the computation, I mean, much more plentiful, it happens, well, the

48
00:02:06,000 --> 00:02:12,000
standard thing to say every 18 months, the compute power available for the same amount

49
00:02:12,000 --> 00:02:14,000
of money doubles.

50
00:02:14,000 --> 00:02:17,000
Twice as much.

51
00:02:17,000 --> 00:02:21,000
And so that means our phones are working better each year.

52
00:02:21,000 --> 00:02:24,000
And our laptops are more capable.

53
00:02:24,000 --> 00:02:30,000
But also, you know, everyone making products, the network.

54
00:02:30,000 --> 00:02:38,000
Yeah, so you guys, you guys are probably citizens of the network.

55
00:02:38,000 --> 00:02:44,000
And the network will compete for your allegiances, just the way the nation states do.

56
00:02:44,000 --> 00:02:49,000
What Canada does, all the other countries.

57
00:02:49,000 --> 00:02:57,000
But your alleges may end up being more to your fellow digital, digitally enabled people,

58
00:02:57,000 --> 00:03:03,000
people enabled by the network, by their phones.

59
00:03:03,000 --> 00:03:09,000
So this is the thing that's happening now.

60
00:03:09,000 --> 00:03:17,000
And my first take home message, that we are living in a time of exponentially accelerating

61
00:03:17,000 --> 00:03:19,000
computation.

62
00:03:19,000 --> 00:03:21,000
It's really dramatic.

63
00:03:21,000 --> 00:03:23,000
I feel like I'm a little bit of a distance.

64
00:03:23,000 --> 00:03:26,000
I've been doing this for 45 years.

65
00:03:26,000 --> 00:03:30,000
So I can see it.

66
00:03:30,000 --> 00:03:34,000
It's kind of slow.

67
00:03:34,000 --> 00:03:40,000
I mean, I used to take two years for computational power to double.

68
00:03:40,000 --> 00:03:43,000
Now, I've been saying it takes 18 months.

69
00:03:43,000 --> 00:03:46,000
And some of my colleagues are saying it's just a year.

70
00:03:46,000 --> 00:03:48,000
If it was to double.

71
00:03:48,000 --> 00:03:52,000
But, you know, it takes a whole year, even if it takes a year, it takes a whole year

72
00:03:52,000 --> 00:03:54,000
of where your computer is twice as big.

73
00:03:54,000 --> 00:03:56,000
And it seems slow, right?

74
00:03:56,000 --> 00:03:59,000
You say, I've got all these extra videos on my computer.

75
00:03:59,000 --> 00:04:02,000
And I have to buy an extra disk or something.

76
00:04:02,000 --> 00:04:09,000
But really, if that happens year after year, double in the year and three or two months,

77
00:04:09,000 --> 00:04:16,000
and that happens for decades, it's on the order of 100 years now that it's going up.

78
00:04:16,000 --> 00:04:24,000
And it will, it's every expectation that it will continue if not, I mean, a little faster.

79
00:04:24,000 --> 00:04:28,000
So this is really special.

80
00:04:28,000 --> 00:04:33,000
So the name for it, yeah, let's just hold off on the next bullet point.

81
00:04:33,000 --> 00:04:36,000
I know a lot of you are really excited about it, most of all.

82
00:04:36,000 --> 00:04:39,000
But it's this trend.

83
00:04:39,000 --> 00:04:41,000
This is the trend.

84
00:04:41,000 --> 00:04:47,000
This is Moore's law, computer power per dollar, increasing exponentially.

85
00:04:47,000 --> 00:04:49,000
No end in sight.

86
00:04:49,000 --> 00:04:53,000
And this is some data from Kershmer on AI.

87
00:04:53,000 --> 00:05:01,000
So here we have years, back in 1900, up to near now.

88
00:05:01,000 --> 00:05:08,000
And up on the other axis, we have computer power per dollar.

89
00:05:08,000 --> 00:05:16,000
And so these are actually individual computers, like the original Mac in 1984, or the URD's points.

90
00:05:16,000 --> 00:05:21,000
And then you can plot out other laptops and supercomputers.

91
00:05:21,000 --> 00:05:23,000
Supercomputers are more expensive.

92
00:05:23,000 --> 00:05:29,000
So if they make it more per dollar, more competition per dollar.

93
00:05:29,000 --> 00:05:33,000
Okay, so your per dollar is on this axis.

94
00:05:33,000 --> 00:05:37,000
But critically, this axis is a larger scale.

95
00:05:37,000 --> 00:05:39,000
This is a linear scale.

96
00:05:39,000 --> 00:05:42,000
I mean, every one of these marks is the same years, right?

97
00:05:42,000 --> 00:05:53,000
Here, every one of these marks, the big marks, from here to here, here to here is 10 powers of 10.

98
00:05:53,000 --> 00:05:59,000
Okay, so that's a factor of 100,000.

99
00:05:59,000 --> 00:06:02,000
Okay?

100
00:06:02,000 --> 00:06:10,000
And so because it's a log scale like this, if it was a straight line, it would be a really exponential increase.

101
00:06:11,000 --> 00:06:15,000
So as I said, it seems to be slightly super exponential.

102
00:06:15,000 --> 00:06:23,000
But that curve up is so slow, we can consider it like we're here and we're at essentially a straight line.

103
00:06:23,000 --> 00:06:29,000
Okay, so what are we looking at?

104
00:06:29,000 --> 00:06:38,000
We're looking at some sense, well it is exponential increase, at least exponential increase, computer power per dollar.

105
00:06:38,000 --> 00:06:44,000
And exponential, it's really the way it's an explosion.

106
00:06:44,000 --> 00:06:51,000
It's an explosion because every year it gets bigger and then the year it gets bigger by the same percentage.

107
00:06:51,000 --> 00:06:54,000
It keeps doubling every two years.

108
00:06:54,000 --> 00:06:58,000
So it's really, almost literally, what we mean by an explosion.

109
00:06:59,000 --> 00:07:07,000
And, okay, so I know you've talked about the singularity.

110
00:07:07,000 --> 00:07:09,000
You've talked about singularity?

111
00:07:17,000 --> 00:07:19,000
One more time.

112
00:07:19,000 --> 00:07:22,000
There you have it.

113
00:07:22,000 --> 00:07:25,000
Mary in or Mary out?

114
00:07:25,000 --> 00:07:27,000
Yeah.

115
00:07:27,000 --> 00:07:29,000
Good enough?

116
00:07:29,000 --> 00:07:31,000
L, E, N.

117
00:07:31,000 --> 00:07:34,000
And G, G, O, O, N, I, L?

118
00:07:34,000 --> 00:07:36,000
Oh, great.

119
00:07:39,000 --> 00:07:41,000
What's that going to say?

120
00:07:41,000 --> 00:07:47,000
Oh yeah, it's going to say, you guys have talked about the singularity, the singularity, right?

121
00:07:47,000 --> 00:07:51,000
It never, maybe it's never properly defined.

122
00:07:52,000 --> 00:07:54,000
But what have you been taught?

123
00:07:54,000 --> 00:07:58,000
What different views have you gotten of the singularity?

124
00:08:05,000 --> 00:08:11,000
Yeah, so you're saying it's a reference to the increasing growth of technology in our lives.

125
00:08:16,000 --> 00:08:18,000
Is it an event?

126
00:08:18,000 --> 00:08:20,000
Is it a time?

127
00:08:20,000 --> 00:08:22,000
Is it a moment in time?

128
00:08:24,000 --> 00:08:26,000
What is a moment in time?

129
00:08:33,000 --> 00:08:37,000
It's like the time when something happens.

130
00:08:37,000 --> 00:08:40,000
It's an hypothetical future.

131
00:08:40,000 --> 00:08:41,000
So what does it mean?

132
00:08:41,000 --> 00:08:45,000
It's hypothetical future, so it doesn't have a specific time.

133
00:08:45,000 --> 00:08:53,000
So time in the future, so when it depends on how fast technology advances.

134
00:08:53,000 --> 00:08:55,000
So when it happens, is that the subject?

135
00:08:55,000 --> 00:08:56,000
Okay.

136
00:08:56,000 --> 00:08:58,000
What is it?

137
00:09:02,000 --> 00:09:07,000
So it's a situation where we have...

138
00:09:08,000 --> 00:09:14,000
I have a question between humanity and technology that is such...

139
00:09:14,000 --> 00:09:16,000
I think it's the time it's comparable.

140
00:09:16,000 --> 00:09:22,000
The technological beings and the human beings are at some comparable level.

141
00:09:22,000 --> 00:09:24,000
I think that's what you're thinking.

142
00:09:24,000 --> 00:09:25,000
Good.

143
00:09:27,000 --> 00:09:30,000
Any other notions?

144
00:09:30,000 --> 00:09:32,000
Unless we're not...

145
00:09:38,000 --> 00:09:43,000
Yeah, so one thing I'm trying to say is we get to decide.

146
00:09:43,000 --> 00:09:49,000
What it is, you could decide it's the moment when AI overcomes human abilities.

147
00:09:49,000 --> 00:09:53,000
But human abilities will continue to increase.

148
00:09:53,000 --> 00:09:59,000
We'll get smarter when we have better education, better tools and better phones.

149
00:09:59,000 --> 00:10:02,000
And we will continue to augment ourselves.

150
00:10:02,000 --> 00:10:06,000
We will augment ourselves with our phones now,

151
00:10:06,000 --> 00:10:10,000
but we will eventually be augmented by entangling things in our minds

152
00:10:10,000 --> 00:10:16,000
and by drawing stronger connections between us and our machines.

153
00:10:16,000 --> 00:10:19,000
Yeah, we get to decide what the similarity is.

154
00:10:19,000 --> 00:10:21,000
But one...

155
00:10:21,000 --> 00:10:22,000
So what I always...

156
00:10:22,000 --> 00:10:28,000
What I always learned was similarity...

157
00:10:28,000 --> 00:10:32,000
So there's this doubling of a year or two,

158
00:10:32,000 --> 00:10:37,000
and you can't really totally anticipate what's going to happen.

159
00:10:37,000 --> 00:10:40,000
And the point, of course, like this is to think about what's going to happen.

160
00:10:40,000 --> 00:10:45,000
But we can't because, well, so many amazing things happen.

161
00:10:45,000 --> 00:10:47,000
Machines will get more powerful.

162
00:10:47,000 --> 00:10:48,000
People will change.

163
00:10:48,000 --> 00:10:50,000
And maybe machines will compete.

164
00:10:50,000 --> 00:10:52,000
Maybe there'll be our offspring.

165
00:10:52,000 --> 00:10:54,000
Maybe there'll be our descendants.

166
00:10:54,000 --> 00:10:58,000
It's hard to see the future after a certain point.

167
00:10:58,000 --> 00:11:02,000
And so that, to me, is always one of the core meanings of similarity.

168
00:11:02,000 --> 00:11:04,000
It's the point of the horizon,

169
00:11:04,000 --> 00:11:07,000
by which you can't see further.

170
00:11:07,000 --> 00:11:12,000
And when you try to predict what will happen.

171
00:11:12,000 --> 00:11:14,000
Okay, but when you get there,

172
00:11:14,000 --> 00:11:17,000
when you get to that point that you can't see past now,

173
00:11:17,000 --> 00:11:20,000
when you get there, you'll be able to see,

174
00:11:20,000 --> 00:11:21,000
because you'll be there,

175
00:11:21,000 --> 00:11:23,000
and you'll be able to see a little bit further.

176
00:11:23,000 --> 00:11:27,000
So it's the sense of the point that you can't see past.

177
00:11:27,000 --> 00:11:29,000
Then it will receive.

178
00:11:29,000 --> 00:11:31,000
And it will forever receive.

179
00:11:31,000 --> 00:11:33,000
You won't actually get there.

180
00:11:33,000 --> 00:11:35,000
Okay?

181
00:11:38,000 --> 00:11:43,000
Another part of the sense is that it's an explosion.

182
00:11:43,000 --> 00:11:49,000
So what I want to lead you to is the idea that maybe we are in that situation.

183
00:11:49,000 --> 00:11:52,000
Because this doubling is happening so regularly,

184
00:11:52,000 --> 00:11:54,000
there's a point you can't see past.

185
00:11:54,000 --> 00:11:56,000
Maybe it's 2040 that you can't see past.

186
00:11:56,000 --> 00:11:58,000
What will happen now?

187
00:11:58,000 --> 00:12:00,000
What will happen in 2040?

188
00:12:00,000 --> 00:12:01,000
You can't see past that.

189
00:12:01,000 --> 00:12:04,000
When we get to 2040, we'll have an idea.

190
00:12:04,000 --> 00:12:07,000
And so the explosion,

191
00:12:07,000 --> 00:12:08,000
maybe this is the explosion,

192
00:12:08,000 --> 00:12:10,000
and we are right in the middle.

193
00:12:10,000 --> 00:12:11,000
And it's a slow explosion.

194
00:12:11,000 --> 00:12:13,000
It will always be slow.

195
00:12:13,000 --> 00:12:15,000
It will always be, it takes a whole other year to double.

196
00:12:15,000 --> 00:12:17,000
The double will be a bigger,

197
00:12:17,000 --> 00:12:20,000
in terms of absolute amounts that you're adding,

198
00:12:20,000 --> 00:12:22,000
but it will still just be a doubling.

199
00:12:22,000 --> 00:12:24,000
Maybe it will always seem slow.

200
00:12:24,000 --> 00:12:26,000
I think the thing that is the singularity

201
00:12:26,000 --> 00:12:31,000
is the explosion of computation and intelligence.

202
00:12:31,000 --> 00:12:33,000
We are in the midst of it.

203
00:12:33,000 --> 00:12:36,000
And so this is beautiful because it means

204
00:12:36,000 --> 00:12:40,000
there's not this future thing that we're going to encounter.

205
00:12:40,000 --> 00:12:42,000
We can encounter it right now.

206
00:12:42,000 --> 00:12:44,000
We are encountering it.

207
00:12:45,000 --> 00:12:48,000
It will just be more,

208
00:12:48,000 --> 00:12:50,000
it will be more transformation,

209
00:12:50,000 --> 00:12:52,000
more doubling, more increases,

210
00:12:52,000 --> 00:12:54,000
more change.

211
00:12:54,000 --> 00:13:00,000
And it will feel much like it does now.

212
00:13:06,000 --> 00:13:09,000
So I think this is the story of our time,

213
00:13:09,000 --> 00:13:11,000
exponentially increasing computation,

214
00:13:11,000 --> 00:13:15,000
and it will be the story for the foreseeable future.

215
00:13:19,000 --> 00:13:21,000
Anything that uses computation,

216
00:13:21,000 --> 00:13:24,000
you've got ten times more valuable every five years.

217
00:13:35,000 --> 00:13:37,000
So you know this.

218
00:13:37,000 --> 00:13:39,000
You're working in different fields.

219
00:13:40,000 --> 00:13:42,000
What fields are you working in?

220
00:13:42,000 --> 00:13:44,000
Psychology.

221
00:13:44,000 --> 00:13:45,000
Psychology?

222
00:13:45,000 --> 00:13:47,000
You're in technology.

223
00:13:47,000 --> 00:13:48,000
Psychology, sorry.

224
00:13:48,000 --> 00:13:49,000
Psychology.

225
00:13:49,000 --> 00:13:52,000
I was originally trying to do psychology myself.

226
00:13:52,000 --> 00:13:53,000
I like that.

227
00:13:53,000 --> 00:13:55,000
Neuroscience.

228
00:13:55,000 --> 00:13:57,000
Neuroscience, cool.

229
00:13:57,000 --> 00:13:59,000
Education.

230
00:13:59,000 --> 00:14:02,000
Education, biomedical engineering.

231
00:14:02,000 --> 00:14:04,000
Second, biomedical engineering.

232
00:14:04,000 --> 00:14:05,000
Biomedical engineering.

233
00:14:05,000 --> 00:14:08,000
So I think you know,

234
00:14:08,000 --> 00:14:10,000
in your areas,

235
00:14:10,000 --> 00:14:14,000
that computer power is having more and more effect.

236
00:14:14,000 --> 00:14:17,000
But in neuroscience, they can measure more neurons

237
00:14:17,000 --> 00:14:19,000
at the same time and record it.

238
00:14:19,000 --> 00:14:24,000
And in biomedicine,

239
00:14:24,000 --> 00:14:27,000
they can operate now with robots.

240
00:14:27,000 --> 00:14:30,000
And they can manage the data

241
00:14:30,000 --> 00:14:33,000
and search for the data to find diseases.

242
00:14:34,000 --> 00:14:37,000
And I was like, what did you say?

243
00:14:37,000 --> 00:14:39,000
I'm in education.

244
00:14:39,000 --> 00:14:40,000
You're in education.

245
00:14:40,000 --> 00:14:44,000
So you know that there's a big thing now

246
00:14:44,000 --> 00:14:47,000
to try to make computer tutors,

247
00:14:47,000 --> 00:14:49,000
like all these large language models,

248
00:14:49,000 --> 00:14:54,000
to use those to be an individual tutor for each student.

249
00:14:54,000 --> 00:14:55,000
And that's perhaps,

250
00:14:55,000 --> 00:14:59,000
is that maybe the biggest impact of computer power on education,

251
00:14:59,000 --> 00:15:00,000
would you say?

252
00:15:00,000 --> 00:15:02,000
Or would you say something else?

253
00:15:02,000 --> 00:15:03,000
I would say so.

254
00:15:03,000 --> 00:15:05,000
Yeah.

255
00:15:05,000 --> 00:15:06,000
Yeah.

256
00:15:06,000 --> 00:15:07,000
But it's true.

257
00:15:07,000 --> 00:15:08,000
I think all the sciences,

258
00:15:08,000 --> 00:15:10,000
you know, really good physics,

259
00:15:10,000 --> 00:15:12,000
the things that they can simulate on a computer,

260
00:15:12,000 --> 00:15:13,000
are much more important.

261
00:15:13,000 --> 00:15:17,000
The biology, the genetics,

262
00:15:17,000 --> 00:15:21,000
psychology,

263
00:15:21,000 --> 00:15:24,000
all that we can do with simulations

264
00:15:24,000 --> 00:15:27,000
to model render processes.

265
00:15:27,000 --> 00:15:29,000
It's a big deal.

266
00:15:30,000 --> 00:15:32,000
All the sciences have changed.

267
00:15:32,000 --> 00:15:34,000
And all of our business ordinary lives,

268
00:15:34,000 --> 00:15:36,000
because we have computers,

269
00:15:36,000 --> 00:15:37,000
we have laptops,

270
00:15:37,000 --> 00:15:38,000
we can use it individually.

271
00:15:38,000 --> 00:15:39,000
We can add lives data,

272
00:15:39,000 --> 00:15:43,000
we can communicate to people around the world.

273
00:15:45,000 --> 00:15:47,000
We're all transformed.

274
00:15:47,000 --> 00:15:49,000
We'll continue to be.

275
00:15:50,000 --> 00:15:51,000
All right.

276
00:15:55,000 --> 00:15:57,000
Well,

277
00:15:57,000 --> 00:15:59,000
sort of the theme

278
00:15:59,000 --> 00:16:01,000
is computation and intelligence.

279
00:16:01,000 --> 00:16:03,000
Now, today,

280
00:16:03,000 --> 00:16:05,000
we have the intelligent things,

281
00:16:05,000 --> 00:16:06,000
our people,

282
00:16:06,000 --> 00:16:07,000
and they're really,

283
00:16:07,000 --> 00:16:09,000
almost all of you.

284
00:16:09,000 --> 00:16:14,000
We have systems that use a lot of computation,

285
00:16:14,000 --> 00:16:16,000
like large language models,

286
00:16:16,000 --> 00:16:20,000
or like alpha fold.

287
00:16:20,000 --> 00:16:21,000
You guys know,

288
00:16:21,000 --> 00:16:22,000
do you guys,

289
00:16:22,000 --> 00:16:23,000
can we have a fold?

290
00:16:23,000 --> 00:16:24,000
Yeah.

291
00:16:24,000 --> 00:16:25,000
Yeah.

292
00:16:25,000 --> 00:16:26,000
Figure out all the proteins

293
00:16:26,000 --> 00:16:29,000
that are known to humanity,

294
00:16:29,000 --> 00:16:30,000
how they fold up

295
00:16:30,000 --> 00:16:31,000
in three dimensional space,

296
00:16:31,000 --> 00:16:33,000
how this changes

297
00:16:33,000 --> 00:16:36,000
all the research possibilities.

298
00:16:36,000 --> 00:16:38,000
That's not intelligence.

299
00:16:38,000 --> 00:16:40,000
I mean, look,

300
00:16:40,000 --> 00:16:41,000
I want to,

301
00:16:41,000 --> 00:16:44,000
I'll work towards a real definition,

302
00:16:44,000 --> 00:16:46,000
but I would just like to urge you to think,

303
00:16:46,000 --> 00:16:48,000
maybe that's not intelligence.

304
00:16:48,000 --> 00:16:49,000
I mean,

305
00:16:49,000 --> 00:16:50,000
it's called AI,

306
00:16:50,000 --> 00:16:52,000
but I think the way we use,

307
00:16:52,000 --> 00:16:55,000
we would use the word AI nowadays.

308
00:16:55,000 --> 00:16:56,000
It just,

309
00:16:56,000 --> 00:16:57,000
if somebody uses a lot of computation,

310
00:16:57,000 --> 00:16:59,000
they call it AI.

311
00:16:59,000 --> 00:17:00,000
But,

312
00:17:02,000 --> 00:17:05,000
I think we should mean more than that.

313
00:17:05,000 --> 00:17:08,000
We say intelligence.

314
00:17:08,000 --> 00:17:09,000
I think, yeah,

315
00:17:09,000 --> 00:17:10,000
people are intelligent,

316
00:17:10,000 --> 00:17:13,000
alpha fold is non-intelligent,

317
00:17:14,000 --> 00:17:15,000
but,

318
00:17:16,000 --> 00:17:17,000
yeah,

319
00:17:17,000 --> 00:17:18,000
I want to distinguish DC,

320
00:17:18,000 --> 00:17:19,000
because like now we're,

321
00:17:19,000 --> 00:17:21,000
so what about my take home message here was,

322
00:17:21,000 --> 00:17:22,000
we're living in this time

323
00:17:22,000 --> 00:17:24,000
of exponentially accelerating computation,

324
00:17:24,000 --> 00:17:26,000
and we're entering the time

325
00:17:26,000 --> 00:17:29,000
of exponentially accelerating intelligence.

326
00:17:29,000 --> 00:17:32,000
You're just entering that.

327
00:17:32,000 --> 00:17:33,000
And as,

328
00:17:33,000 --> 00:17:35,000
both of these changes,

329
00:17:35,000 --> 00:17:36,000
the greatest,

330
00:17:36,000 --> 00:17:39,000
both of these

331
00:17:39,000 --> 00:17:42,000
changes will lead to

332
00:17:42,000 --> 00:17:44,000
changes in what we do,

333
00:17:44,000 --> 00:17:46,000
and transformation of ourselves.

334
00:17:46,000 --> 00:17:48,000
The greatest ones will come from

335
00:17:48,000 --> 00:17:49,000
the intelligence,

336
00:17:49,000 --> 00:17:50,000
accelerating intelligence,

337
00:17:50,000 --> 00:17:52,000
because we will really make the things

338
00:17:52,000 --> 00:17:55,000
that are comparable to ourselves,

339
00:17:55,000 --> 00:17:59,000
but without biologically replication.

340
00:17:59,000 --> 00:18:02,000
And then we will be becoming,

341
00:18:02,000 --> 00:18:03,000
we will be becoming

342
00:18:03,000 --> 00:18:05,000
the things we are creating,

343
00:18:05,000 --> 00:18:07,000
we will be becoming

344
00:18:07,000 --> 00:18:08,000
even more intelligent

345
00:18:08,000 --> 00:18:10,000
than the current humans are,

346
00:18:10,000 --> 00:18:13,000
the current people.

347
00:18:13,000 --> 00:18:15,000
So these two transformations,

348
00:18:15,000 --> 00:18:16,000
recent computation,

349
00:18:16,000 --> 00:18:17,000
recent turns,

350
00:18:17,000 --> 00:18:18,000
are closely related,

351
00:18:18,000 --> 00:18:19,000
sure,

352
00:18:19,000 --> 00:18:20,000
but,

353
00:18:20,000 --> 00:18:21,000
in that way,

354
00:18:21,000 --> 00:18:22,000
because of this,

355
00:18:22,000 --> 00:18:24,000
they're easily confused.

356
00:18:24,000 --> 00:18:26,000
And what I want to propose to you,

357
00:18:26,000 --> 00:18:27,000
the difference is that

358
00:18:27,000 --> 00:18:28,000
intelligence,

359
00:18:28,000 --> 00:18:30,000
this computation plus

360
00:18:30,000 --> 00:18:31,000
some notion of goal,

361
00:18:31,000 --> 00:18:32,000
or purpose,

362
00:18:32,000 --> 00:18:33,000
or agent,

363
00:18:33,000 --> 00:18:34,000
is something,

364
00:18:34,000 --> 00:18:36,000
yeah, so,

365
00:18:36,000 --> 00:18:37,000
that,

366
00:18:37,000 --> 00:18:39,000
those are the general points

367
00:18:39,000 --> 00:18:41,000
I'm leading to.

368
00:18:41,000 --> 00:18:42,000
Okay?

369
00:18:45,000 --> 00:18:47,000
Okay, so,

370
00:18:52,000 --> 00:18:54,000
computation-powered machines

371
00:18:54,000 --> 00:18:56,000
substitute for the computation-powered people.

372
00:18:56,000 --> 00:18:58,000
That's sort of what's happening.

373
00:18:59,000 --> 00:19:00,000
You know, we use people,

374
00:19:00,000 --> 00:19:02,000
various,

375
00:19:02,000 --> 00:19:04,000
the jobs that people do,

376
00:19:04,000 --> 00:19:06,000
we use them for their perception,

377
00:19:06,000 --> 00:19:07,000
motor control,

378
00:19:07,000 --> 00:19:08,000
prediction abilities,

379
00:19:08,000 --> 00:19:09,000
search abilities,

380
00:19:09,000 --> 00:19:11,000
optimization abilities,

381
00:19:11,000 --> 00:19:12,000
and until now,

382
00:19:12,000 --> 00:19:13,000
people that are cheapest,

383
00:19:13,000 --> 00:19:15,000
cheapest source of computation

384
00:19:15,000 --> 00:19:17,000
in the sense that now machines

385
00:19:17,000 --> 00:19:19,000
are, in some cases,

386
00:19:19,000 --> 00:19:22,000
find greater achievement in optimization.

387
00:19:22,000 --> 00:19:24,000
And so here are some of the successes

388
00:19:24,000 --> 00:19:25,000
over the last,

389
00:19:25,000 --> 00:19:27,000
maybe, 12 years.

390
00:19:27,000 --> 00:19:29,000
The oldest one is when

391
00:19:29,000 --> 00:19:32,000
machines invest in the phase of jeopardy,

392
00:19:32,000 --> 00:19:33,000
and,

393
00:19:33,000 --> 00:19:35,000
not long after that,

394
00:19:35,000 --> 00:19:36,000
in 2012,

395
00:19:36,000 --> 00:19:37,000
we worked with

396
00:19:37,000 --> 00:19:38,000
which transformed our

397
00:19:38,000 --> 00:19:39,000
speech recognition

398
00:19:39,000 --> 00:19:42,000
into a natural language question.

399
00:19:42,000 --> 00:19:43,000
Large language files,

400
00:19:43,000 --> 00:19:44,000
in particular,

401
00:19:44,000 --> 00:19:45,000
you know,

402
00:19:45,000 --> 00:19:48,000
you probably use them more than I do.

403
00:19:48,000 --> 00:19:50,000
They do all kinds of,

404
00:19:50,000 --> 00:19:52,000
they can appear to speak

405
00:19:52,000 --> 00:19:54,000
and understand.

406
00:19:54,000 --> 00:19:55,000
And,

407
00:19:55,000 --> 00:19:57,000
and the generative

408
00:19:57,000 --> 00:19:59,000
methods for generating pictures as well,

409
00:19:59,000 --> 00:20:01,000
they're impressive.

410
00:20:01,000 --> 00:20:03,000
So, all those things

411
00:20:03,000 --> 00:20:05,000
are impressive.

412
00:20:05,000 --> 00:20:08,000
I would not put any of them as intelligence, though.

413
00:20:08,000 --> 00:20:09,000
I would count as intelligence

414
00:20:09,000 --> 00:20:12,000
when they used deep reinforcement

415
00:20:12,000 --> 00:20:14,000
values to play Atari and Go

416
00:20:14,000 --> 00:20:17,000
and all the other games.

417
00:20:17,000 --> 00:20:19,000
And in these cases,

418
00:20:19,000 --> 00:20:20,000
they,

419
00:20:20,000 --> 00:20:23,000
the AI had a goal,

420
00:20:23,000 --> 00:20:25,000
the intelligent system,

421
00:20:25,000 --> 00:20:27,000
the computational system had a goal,

422
00:20:27,000 --> 00:20:29,000
which was to win the games,

423
00:20:29,000 --> 00:20:31,000
and it adjusted its behavior

424
00:20:31,000 --> 00:20:34,000
in order to continue winning.

425
00:20:34,000 --> 00:20:35,000
So,

426
00:20:35,000 --> 00:20:37,000
that's what I want to say is the essence

427
00:20:37,000 --> 00:20:39,000
of intelligence is

428
00:20:39,000 --> 00:20:42,000
being able to pursue a goal.

429
00:20:42,000 --> 00:20:43,000
And,

430
00:20:43,000 --> 00:20:45,000
when you use, like, the large language models,

431
00:20:45,000 --> 00:20:46,000
you may,

432
00:20:46,000 --> 00:20:48,000
they may appear to have a goal,

433
00:20:48,000 --> 00:20:49,000
but it's really, really true

434
00:20:49,000 --> 00:20:51,000
that they don't have a goal.

435
00:20:51,000 --> 00:20:52,000
They're just people,

436
00:20:52,000 --> 00:20:53,000
they're saying the same thing,

437
00:20:53,000 --> 00:20:54,000
and people would say,

438
00:20:54,000 --> 00:20:55,000
they're half-stead

439
00:20:55,000 --> 00:20:57,000
in similar situations.

440
00:20:57,000 --> 00:20:59,000
They have no sense of trying to,

441
00:20:59,000 --> 00:21:00,000
they're not,

442
00:21:00,000 --> 00:21:02,000
certainly they're not trying to manipulate you,

443
00:21:02,000 --> 00:21:04,000
because they don't have any goal,

444
00:21:04,000 --> 00:21:06,000
except for, perhaps,

445
00:21:06,000 --> 00:21:08,000
the designers could have a goal.

446
00:21:08,000 --> 00:21:10,000
The designers,

447
00:21:10,000 --> 00:21:12,000
the people that were designing that,

448
00:21:12,000 --> 00:21:14,000
would be trying to manipulate you,

449
00:21:14,000 --> 00:21:16,000
but the machine itself doesn't have a goal.

450
00:21:18,000 --> 00:21:20,000
Okay, so,

451
00:21:22,000 --> 00:21:23,000
maybe,

452
00:21:23,000 --> 00:21:25,000
maybe it is time,

453
00:21:26,000 --> 00:21:27,000
you know,

454
00:21:28,000 --> 00:21:30,000
maybe it is time for definition.

455
00:21:30,000 --> 00:21:32,000
Okay, so the definition,

456
00:21:32,000 --> 00:21:35,000
here's some definitions of intelligence.

457
00:21:35,000 --> 00:21:36,000
Well, it's not,

458
00:21:36,000 --> 00:21:38,000
first of all, it's not really definition.

459
00:21:38,000 --> 00:21:39,000
Maybe it's the fact,

460
00:21:39,000 --> 00:21:40,000
it's the most,

461
00:21:40,000 --> 00:21:41,000
it's claimed to be

462
00:21:41,000 --> 00:21:43,000
the most powerful phenomenon in the universe.

463
00:21:43,000 --> 00:21:45,000
That really occurs a while.

464
00:21:45,000 --> 00:21:46,000
And,

465
00:21:51,000 --> 00:21:52,000
yeah, just like,

466
00:21:54,000 --> 00:21:55,000
this,

467
00:21:55,000 --> 00:21:57,000
this, this idea,

468
00:21:58,000 --> 00:22:00,000
this is, this is crazy.

469
00:22:00,000 --> 00:22:03,000
The most powerful phenomenon in the universe.

470
00:22:03,000 --> 00:22:05,000
It's claimed to be the intelligence,

471
00:22:05,000 --> 00:22:07,000
like, and we, we people,

472
00:22:07,000 --> 00:22:09,000
are the primary model of intelligence.

473
00:22:09,000 --> 00:22:11,000
The sort of thing that we,

474
00:22:11,000 --> 00:22:13,000
people, and our intelligence,

475
00:22:13,000 --> 00:22:16,000
is the most powerful thing in the universe.

476
00:22:16,000 --> 00:22:18,000
More powerful than, you know,

477
00:22:18,000 --> 00:22:20,000
super-doers,

478
00:22:20,000 --> 00:22:22,000
or dark energy.

479
00:22:24,000 --> 00:22:26,000
Does that make any sense to you guys?

480
00:22:26,000 --> 00:22:27,000
Okay.

481
00:22:31,000 --> 00:22:32,000
Yeah?

482
00:22:32,000 --> 00:22:33,000
Does it to you, Eileen?

483
00:22:33,000 --> 00:22:34,000
I mean, yeah.

484
00:22:34,000 --> 00:22:35,000
I, I would agree.

485
00:22:35,000 --> 00:22:36,000
In what sense?

486
00:22:36,000 --> 00:22:37,000
I mean,

487
00:22:37,000 --> 00:22:38,000
everything makes up intelligence.

488
00:22:38,000 --> 00:22:40,000
If we talk about black magic,

489
00:22:40,000 --> 00:22:41,000
there's intelligence in that.

490
00:22:41,000 --> 00:22:42,000
Uh huh.

491
00:22:42,000 --> 00:22:43,000
Everything makes up intelligence.

492
00:22:43,000 --> 00:22:45,000
So it makes sense that it's most powerful.

493
00:22:47,000 --> 00:22:48,000
So, you know,

494
00:22:48,000 --> 00:22:49,000
one thing,

495
00:22:49,000 --> 00:22:50,000
I like to think,

496
00:22:50,000 --> 00:22:52,000
if it's more powerful than

497
00:22:52,000 --> 00:22:53,000
a super,

498
00:22:53,000 --> 00:22:54,000
a huge explosion

499
00:22:54,000 --> 00:22:55,000
in the,

500
00:22:55,000 --> 00:22:57,000
in the sky,

501
00:22:57,000 --> 00:22:58,000
I think it's saying that,

502
00:22:58,000 --> 00:23:00,000
that people,

503
00:23:00,000 --> 00:23:02,000
or, or, or intelligence is,

504
00:23:02,000 --> 00:23:04,000
which maybe came from people,

505
00:23:04,000 --> 00:23:06,000
will someday,

506
00:23:06,000 --> 00:23:08,000
um,

507
00:23:08,000 --> 00:23:10,000
like be moving the stars around.

508
00:23:10,000 --> 00:23:12,000
We will, over time,

509
00:23:12,000 --> 00:23:14,000
we've only been here for,

510
00:23:14,000 --> 00:23:15,000
I mean,

511
00:23:15,000 --> 00:23:17,000
maybe a hundred thousand years,

512
00:23:17,000 --> 00:23:19,000
for people again.

513
00:23:19,000 --> 00:23:21,000
If you give us a billion years,

514
00:23:21,000 --> 00:23:23,000
we can become quite powerful,

515
00:23:23,000 --> 00:23:24,000
uh,

516
00:23:24,000 --> 00:23:25,000
physically,

517
00:23:25,000 --> 00:23:26,000
uh,

518
00:23:26,000 --> 00:23:27,000
all the time,

519
00:23:27,000 --> 00:23:28,000
and we,

520
00:23:28,000 --> 00:23:29,000
we can,

521
00:23:29,000 --> 00:23:30,000
be in the force

522
00:23:30,000 --> 00:23:31,000
that moves the stars around,

523
00:23:31,000 --> 00:23:32,000
and we range

524
00:23:32,000 --> 00:23:33,000
the universe,

525
00:23:33,000 --> 00:23:34,000
the atoms,

526
00:23:34,000 --> 00:23:35,000
and the universe

527
00:23:35,000 --> 00:23:36,000
to be,

528
00:23:36,000 --> 00:23:37,000
to,

529
00:23:37,000 --> 00:23:38,000
to our liking,

530
00:23:38,000 --> 00:23:39,000
as a group.

531
00:23:41,000 --> 00:23:42,000
Um,

532
00:23:42,000 --> 00:23:43,000
so,

533
00:23:43,000 --> 00:23:44,000
when we think that way,

534
00:23:44,000 --> 00:23:45,000
we're so,

535
00:23:45,000 --> 00:23:46,000
so to think

536
00:23:46,000 --> 00:23:47,000
in a little bit arrogant way,

537
00:23:47,000 --> 00:23:48,000
right,

538
00:23:48,000 --> 00:23:49,000
we think we,

539
00:23:49,000 --> 00:23:50,000
intelligence,

540
00:23:50,000 --> 00:23:51,000
would be so powerful,

541
00:23:51,000 --> 00:23:52,000
because,

542
00:23:52,000 --> 00:23:53,000
um,

543
00:23:57,000 --> 00:23:58,000
well,

544
00:23:58,000 --> 00:23:59,000
we're saying that

545
00:23:59,000 --> 00:24:00,000
intelligence is,

546
00:24:00,000 --> 00:24:01,000
is quite a thing.

547
00:24:02,000 --> 00:24:03,000
And,

548
00:24:03,000 --> 00:24:04,000
so I think we should mean,

549
00:24:04,000 --> 00:24:05,000
we should,

550
00:24:05,000 --> 00:24:06,000
by the word intelligence,

551
00:24:06,000 --> 00:24:07,000
intelligence is just a word,

552
00:24:07,000 --> 00:24:08,000
and we can mean

553
00:24:08,000 --> 00:24:09,000
by anything we want to,

554
00:24:09,000 --> 00:24:11,000
but we should mean,

555
00:24:11,000 --> 00:24:12,000
by something that could be

556
00:24:12,000 --> 00:24:13,000
this powerful.

557
00:24:15,000 --> 00:24:16,000
Okay,

558
00:24:16,000 --> 00:24:17,000
and,

559
00:24:17,000 --> 00:24:18,000
what I'm proposing

560
00:24:18,000 --> 00:24:19,000
is that it's goal,

561
00:24:19,000 --> 00:24:20,000
that's the ability

562
00:24:20,000 --> 00:24:21,000
to have a purpose.

563
00:24:21,000 --> 00:24:22,000
That's the power

564
00:24:22,000 --> 00:24:23,000
of the thing,

565
00:24:23,000 --> 00:24:24,000
and that's the power

566
00:24:24,000 --> 00:24:25,000
of the phenomenon.

567
00:24:25,000 --> 00:24:26,000
The phenomenon,

568
00:24:26,000 --> 00:24:27,000
if you look around the world,

569
00:24:27,000 --> 00:24:28,000
you'll see there,

570
00:24:28,000 --> 00:24:29,000
there will be things,

571
00:24:29,000 --> 00:24:30,000
like people,

572
00:24:30,000 --> 00:24:31,000
and animals,

573
00:24:31,000 --> 00:24:33,000
and maybe new plants,

574
00:24:33,000 --> 00:24:34,000
that appear to have goals,

575
00:24:34,000 --> 00:24:35,000
that are well thought out

576
00:24:35,000 --> 00:24:36,000
in terms of goals,

577
00:24:37,000 --> 00:24:38,000
and,

578
00:24:39,000 --> 00:24:40,000
and that is the phenomenon

579
00:24:40,000 --> 00:24:41,000
that we observe.

580
00:24:44,000 --> 00:24:45,000
Um,

581
00:24:45,000 --> 00:24:46,000
so,

582
00:24:46,000 --> 00:24:47,000
with James,

583
00:24:49,000 --> 00:24:50,000
and,

584
00:24:50,000 --> 00:24:51,000
the double,

585
00:24:51,000 --> 00:24:53,000
he's the original psychologist.

586
00:24:56,000 --> 00:24:58,000
This textbook was 1890,

587
00:24:58,000 --> 00:24:59,000
and,

588
00:24:59,000 --> 00:25:00,000
and,

589
00:25:00,000 --> 00:25:01,000
and he was the first one

590
00:25:01,000 --> 00:25:02,000
to talk about city goals.

591
00:25:02,000 --> 00:25:04,000
When you talked about mind,

592
00:25:04,000 --> 00:25:05,000
you didn't use the word,

593
00:25:05,000 --> 00:25:06,000
you didn't use the word intelligence,

594
00:25:06,000 --> 00:25:07,000
you used the word mind.

595
00:25:07,000 --> 00:25:09,000
You said mind is attaining,

596
00:25:09,000 --> 00:25:11,000
the hallmark of mind,

597
00:25:11,000 --> 00:25:13,000
is attaining consistent ends

598
00:25:13,000 --> 00:25:15,000
by variable means.

599
00:25:15,000 --> 00:25:16,000
So,

600
00:25:16,000 --> 00:25:18,000
means you change what you do

601
00:25:18,000 --> 00:25:19,000
by what you want.

602
00:25:19,000 --> 00:25:21,000
The essence of having a goal

603
00:25:21,000 --> 00:25:23,000
is to vary what you do

604
00:25:23,000 --> 00:25:24,000
to get what you want.

605
00:25:24,000 --> 00:25:25,000
Um,

606
00:25:26,000 --> 00:25:27,000
yeah,

607
00:25:27,000 --> 00:25:28,000
and,

608
00:25:28,000 --> 00:25:30,000
if you go to the AI researchers,

609
00:25:30,000 --> 00:25:32,000
the most famous,

610
00:25:32,000 --> 00:25:34,000
one of the top ones,

611
00:25:34,000 --> 00:25:35,000
well-defined John McCarthy,

612
00:25:35,000 --> 00:25:37,000
he finally defined,

613
00:25:37,000 --> 00:25:40,000
an offered definition of intelligence,

614
00:25:40,000 --> 00:25:42,000
as the computational part

615
00:25:42,000 --> 00:25:44,000
of the ability to achieve goals.

616
00:25:44,000 --> 00:25:45,000
So he's thinking the way I am,

617
00:25:45,000 --> 00:25:47,000
I probably learned from him,

618
00:25:47,000 --> 00:25:48,000
actually,

619
00:25:48,000 --> 00:25:49,000
um,

620
00:25:49,000 --> 00:25:50,000
but it's the computational part

621
00:25:50,000 --> 00:25:52,000
of the ability to achieve goals.

622
00:25:52,000 --> 00:25:53,000
Um,

623
00:25:53,000 --> 00:25:55,000
and I might say,

624
00:25:55,000 --> 00:25:56,000
myself,

625
00:25:56,000 --> 00:25:57,000
I might say the computational part

626
00:25:57,000 --> 00:25:59,000
of the ability to predict

627
00:25:59,000 --> 00:26:01,000
and control the environment,

628
00:26:01,000 --> 00:26:03,000
to control the data,

629
00:26:03,000 --> 00:26:05,000
designed to be a computer scientist,

630
00:26:05,000 --> 00:26:07,000
in terms of data going back and forth

631
00:26:07,000 --> 00:26:08,000
to the human world,

632
00:26:08,000 --> 00:26:09,000
to try to understand it,

633
00:26:09,000 --> 00:26:11,000
to control and predict it.

634
00:26:13,000 --> 00:26:14,000
Okay, so,

635
00:26:14,000 --> 00:26:16,000
nevertheless,

636
00:26:16,000 --> 00:26:17,000
you know,

637
00:26:17,000 --> 00:26:18,000
so that's,

638
00:26:18,000 --> 00:26:19,000
that's those kind of definitions

639
00:26:19,000 --> 00:26:20,000
that I like,

640
00:26:20,000 --> 00:26:21,000
but I want to just

641
00:26:21,000 --> 00:26:22,000
talk a little bit,

642
00:26:22,000 --> 00:26:23,000
just want to be

643
00:26:23,000 --> 00:26:24,000
explicit about,

644
00:26:24,000 --> 00:26:25,000
people are using the word

645
00:26:25,000 --> 00:26:27,000
differently nowadays,

646
00:26:27,000 --> 00:26:28,000
uh,

647
00:26:28,000 --> 00:26:29,000
they often think intelligence

648
00:26:29,000 --> 00:26:31,000
is mimicking people.

649
00:26:31,000 --> 00:26:32,000
Um,

650
00:26:32,000 --> 00:26:34,000
as an AI seeks to reproduce

651
00:26:34,000 --> 00:26:35,000
behavior that we would call

652
00:26:35,000 --> 00:26:36,000
intelligence

653
00:26:36,000 --> 00:26:37,000
if it was done by people,

654
00:26:37,000 --> 00:26:38,000
which is the kind of thing that

655
00:26:38,000 --> 00:26:41,000
AI textbooks often say.

656
00:26:41,000 --> 00:26:42,000
And,

657
00:26:42,000 --> 00:26:43,000
I'm sure you guys

658
00:26:43,000 --> 00:26:44,000
have covered the Turing test,

659
00:26:44,000 --> 00:26:45,000
yes?

660
00:26:45,000 --> 00:26:46,000
Yeah,

661
00:26:46,000 --> 00:26:48,000
the Turing test is about mimicking,

662
00:26:48,000 --> 00:26:49,000
yeah,

663
00:26:49,000 --> 00:26:50,000
well,

664
00:26:50,000 --> 00:26:51,000
it's about

665
00:26:51,000 --> 00:26:52,000
a computer and a person

666
00:26:52,000 --> 00:26:53,000
are put in,

667
00:26:53,000 --> 00:26:54,000
in sealed rooms,

668
00:26:54,000 --> 00:26:55,000
and you,

669
00:26:55,000 --> 00:26:56,000
you talk to them by,

670
00:26:56,000 --> 00:26:57,000
by,

671
00:26:57,000 --> 00:26:59,000
by,

672
00:26:59,000 --> 00:27:01,000
text chatting,

673
00:27:01,000 --> 00:27:02,000
and you try to figure out

674
00:27:02,000 --> 00:27:03,000
which one's the person

675
00:27:03,000 --> 00:27:04,000
which one's the machine.

676
00:27:04,000 --> 00:27:05,000
And so that,

677
00:27:05,000 --> 00:27:06,000
that Turing test is,

678
00:27:06,000 --> 00:27:07,000
you know,

679
00:27:07,000 --> 00:27:08,000
can you make the machine

680
00:27:08,000 --> 00:27:09,000
behave like a person

681
00:27:09,000 --> 00:27:11,000
or mimic a person?

682
00:27:11,000 --> 00:27:12,000
So,

683
00:27:12,000 --> 00:27:14,000
this is a,

684
00:27:15,000 --> 00:27:16,000
a prominent meaning.

685
00:27:16,000 --> 00:27:17,000
I'm going to say,

686
00:27:17,000 --> 00:27:19,000
I'm going to discourage you from it,

687
00:27:19,000 --> 00:27:20,000
but I have to recognize

688
00:27:20,000 --> 00:27:21,000
that it is,

689
00:27:21,000 --> 00:27:22,000
it is why do we use,

690
00:27:22,000 --> 00:27:23,000
um,

691
00:27:23,000 --> 00:27:24,000
yeah,

692
00:27:24,000 --> 00:27:25,000
it is machine learning,

693
00:27:25,000 --> 00:27:26,000
supervised learning,

694
00:27:26,000 --> 00:27:27,000
the task is to label,

695
00:27:27,000 --> 00:27:28,000
say,

696
00:27:28,000 --> 00:27:29,000
pictures of the same person

697
00:27:29,000 --> 00:27:30,000
and label it,

698
00:27:30,000 --> 00:27:31,000
and the chat,

699
00:27:31,000 --> 00:27:32,000
gpt is,

700
00:27:32,000 --> 00:27:33,000
is tasked to

701
00:27:33,000 --> 00:27:34,000
generate text

702
00:27:34,000 --> 00:27:35,000
on the person.

703
00:27:35,000 --> 00:27:36,000
And so,

704
00:27:36,000 --> 00:27:37,000
we,

705
00:27:37,000 --> 00:27:38,000
we sometimes say,

706
00:27:38,000 --> 00:27:39,000
this is our intelligence,

707
00:27:39,000 --> 00:27:40,000
but notice,

708
00:27:40,000 --> 00:27:41,000
none of these things,

709
00:27:41,000 --> 00:27:42,000
um,

710
00:27:42,000 --> 00:27:44,000
we're referencing to goals.

711
00:27:44,000 --> 00:27:46,000
We're just saying,

712
00:27:46,000 --> 00:27:47,000
behave like a person

713
00:27:47,000 --> 00:27:49,000
and mimic a person.

714
00:27:49,000 --> 00:27:50,000
Again,

715
00:27:50,000 --> 00:27:51,000
again,

716
00:27:51,000 --> 00:27:52,000
so just to cut to the chase,

717
00:27:52,000 --> 00:27:53,000
my whole point is to say,

718
00:27:53,000 --> 00:27:54,000
this is

719
00:27:54,000 --> 00:27:56,000
an insufficient meaning

720
00:27:56,000 --> 00:27:57,000
on intelligence,

721
00:27:57,000 --> 00:27:58,000
because,

722
00:27:58,000 --> 00:27:59,000
if you're just able to mimic people,

723
00:27:59,000 --> 00:28:00,000
you're not going to become

724
00:28:00,000 --> 00:28:01,000
most popular

725
00:28:01,000 --> 00:28:02,000
from out on the news.

726
00:28:02,000 --> 00:28:04,000
You're going to be able to fool

727
00:28:04,000 --> 00:28:05,000
people in,

728
00:28:05,000 --> 00:28:06,000
in your person,

729
00:28:06,000 --> 00:28:07,000
but you're not going to

730
00:28:07,000 --> 00:28:08,000
move the,

731
00:28:08,000 --> 00:28:09,000
move the stars around

732
00:28:09,000 --> 00:28:10,000
and you're not going to become

733
00:28:11,000 --> 00:28:13,000
a really powerful force.

734
00:28:13,000 --> 00:28:14,000
Um,

735
00:28:14,000 --> 00:28:15,000
the universe,

736
00:28:15,000 --> 00:28:16,000
people may,

737
00:28:16,000 --> 00:28:17,000
but some of it's,

738
00:28:17,000 --> 00:28:18,000
it's not going to be

739
00:28:18,000 --> 00:28:19,000
powerful in that way.

740
00:28:19,000 --> 00:28:20,000
You have to get with

741
00:28:20,000 --> 00:28:21,000
the real thing

742
00:28:21,000 --> 00:28:22,000
that people are doing

743
00:28:22,000 --> 00:28:23,000
that,

744
00:28:23,000 --> 00:28:24,000
that makes people powerful

745
00:28:24,000 --> 00:28:26,000
and they are intelligence powerful.

746
00:28:26,000 --> 00:28:27,000
Okay.

747
00:28:27,000 --> 00:28:28,000
Well,

748
00:28:28,000 --> 00:28:30,000
these are two definitions

749
00:28:30,000 --> 00:28:31,000
of intelligence.

750
00:28:31,000 --> 00:28:32,000
It could be mimicking people

751
00:28:32,000 --> 00:28:33,000
or it could be,

752
00:28:33,000 --> 00:28:34,000
that is a cheated goal

753
00:28:34,000 --> 00:28:36,000
of computational art.

754
00:28:36,000 --> 00:28:37,000
But we will

755
00:28:37,000 --> 00:28:39,000
cheat rules.

756
00:28:39,000 --> 00:28:40,000
And then,

757
00:28:40,000 --> 00:28:41,000
so we just put those

758
00:28:41,000 --> 00:28:42,000
on the table

759
00:28:42,000 --> 00:28:43,000
and talk about which one's

760
00:28:43,000 --> 00:28:44,000
better.

761
00:28:44,000 --> 00:28:45,000
And now,

762
00:28:45,000 --> 00:28:46,000
you know,

763
00:28:46,000 --> 00:28:47,000
I've sort of given up

764
00:28:47,000 --> 00:28:48,000
and changed the way

765
00:28:48,000 --> 00:28:49,000
people use words.

766
00:28:49,000 --> 00:28:50,000
You know,

767
00:28:50,000 --> 00:28:51,000
I mean,

768
00:28:51,000 --> 00:28:52,000
I'm facing the fact that

769
00:28:52,000 --> 00:28:53,000
the world,

770
00:28:53,000 --> 00:28:54,000
you know,

771
00:28:54,000 --> 00:28:55,000
it's going to call

772
00:28:55,000 --> 00:28:56,000
chat,

773
00:28:56,000 --> 00:28:57,000
GPT,

774
00:28:57,000 --> 00:28:58,000
AI.

775
00:28:58,000 --> 00:28:59,000
It's a shame,

776
00:28:59,000 --> 00:29:00,000
I think.

777
00:29:00,000 --> 00:29:01,000
You know,

778
00:29:01,000 --> 00:29:02,000
maybe that's why we have

779
00:29:02,000 --> 00:29:03,000
this other term,

780
00:29:03,000 --> 00:29:04,000
artificial general intelligence,

781
00:29:04,000 --> 00:29:05,000
AGI,

782
00:29:05,000 --> 00:29:06,000
because we've got to

783
00:29:06,000 --> 00:29:07,000
distinguish it,

784
00:29:07,000 --> 00:29:08,000
but it's just like

785
00:29:08,000 --> 00:29:09,000
that.

786
00:29:09,000 --> 00:29:10,000
And you should keep

787
00:29:10,000 --> 00:29:11,000
in your mind

788
00:29:11,000 --> 00:29:12,000
that there are these two

789
00:29:12,000 --> 00:29:13,000
things,

790
00:29:13,000 --> 00:29:14,000
mimicking people

791
00:29:14,000 --> 00:29:15,000
whereas a system

792
00:29:15,000 --> 00:29:16,000
that has agency

793
00:29:16,000 --> 00:29:17,000
chooses actions

794
00:29:17,000 --> 00:29:19,000
and is trying to act

795
00:29:19,000 --> 00:29:22,000
to achieve goals.

796
00:29:22,000 --> 00:29:23,000
Okay,

797
00:29:23,000 --> 00:29:24,000
you know,

798
00:29:24,000 --> 00:29:25,000
maybe now I should jump

799
00:29:25,000 --> 00:29:26,000
to something that moves

800
00:29:26,000 --> 00:29:27,000
around.

801
00:29:27,000 --> 00:29:28,000
I do have

802
00:29:28,000 --> 00:29:29,000
one sort of video

803
00:29:29,000 --> 00:29:30,000
thing in there somewhere.

804
00:29:30,000 --> 00:29:31,000
So,

805
00:29:31,000 --> 00:29:32,000
yeah,

806
00:29:32,000 --> 00:29:33,000
this picture,

807
00:29:33,000 --> 00:29:34,000
this is a very simple

808
00:29:34,000 --> 00:29:35,000
system,

809
00:29:35,000 --> 00:29:36,000
but it does have agency.

810
00:29:36,000 --> 00:29:37,000
And so,

811
00:29:37,000 --> 00:29:38,000
I'm going to

812
00:29:38,000 --> 00:29:39,000
amaze,

813
00:29:39,000 --> 00:29:40,000
there's a start

814
00:29:40,000 --> 00:29:41,000
location,

815
00:29:41,000 --> 00:29:42,000
there's a goal

816
00:29:42,000 --> 00:29:43,000
location,

817
00:29:43,000 --> 00:29:44,000
and our little agent

818
00:29:44,000 --> 00:29:45,000
is this,

819
00:29:45,000 --> 00:29:46,000
this square,

820
00:29:46,000 --> 00:29:47,000
and if I

821
00:29:47,000 --> 00:29:48,000
touch this,

822
00:29:48,000 --> 00:29:49,000
it will start

823
00:29:49,000 --> 00:29:50,000
moving around.

824
00:29:50,000 --> 00:29:51,000
Here,

825
00:29:51,000 --> 00:29:52,000
it's moving around

826
00:29:52,000 --> 00:29:53,000
because it has

827
00:29:53,000 --> 00:29:54,000
actions,

828
00:29:54,000 --> 00:29:55,000
it can go,

829
00:29:55,000 --> 00:29:56,000
take a step to the right

830
00:29:56,000 --> 00:29:57,000
or to the left

831
00:29:57,000 --> 00:29:58,000
or up or down,

832
00:29:58,000 --> 00:29:59,000
and of course

833
00:29:59,000 --> 00:30:00,000
it takes a step

834
00:30:00,000 --> 00:30:01,000
and runs into an

835
00:30:01,000 --> 00:30:02,000
obstacle,

836
00:30:02,000 --> 00:30:03,000
and it doesn't move,

837
00:30:03,000 --> 00:30:04,000
but

838
00:30:04,000 --> 00:30:05,000
it doesn't move,

839
00:30:05,000 --> 00:30:06,000
and when it

840
00:30:06,000 --> 00:30:07,000
moves over to the left,

841
00:30:07,000 --> 00:30:08,000
it's going to turn

842
00:30:08,000 --> 00:30:09,000
and it's going to

843
00:30:09,000 --> 00:30:10,000
move around

844
00:30:10,000 --> 00:30:11,000
and it's going to

845
00:30:11,000 --> 00:30:12,000
move into the

846
00:30:12,000 --> 00:30:13,000
middle half

847
00:30:13,000 --> 00:30:14,000
of the goal.

848
00:30:14,000 --> 00:30:15,000
What you're

849
00:30:15,000 --> 00:30:16,000
seeing in these colors

850
00:30:16,000 --> 00:30:17,000
and in these arrows,

851
00:30:17,000 --> 00:30:18,000
you're showing,

852
00:30:18,000 --> 00:30:19,000
it's showing what's

853
00:30:19,000 --> 00:30:20,000
going on in the

854
00:30:20,000 --> 00:30:21,000
agents head.

855
00:30:21,000 --> 00:30:22,000
The real

856
00:30:22,000 --> 00:30:23,000
world doesn't turn

857
00:30:23,000 --> 00:30:24,000
green,

858
00:30:24,000 --> 00:30:25,000
but green means

859
00:30:25,000 --> 00:30:26,000
that things

860
00:30:26,000 --> 00:30:27,000
have a good state

861
00:30:27,000 --> 00:30:28,000
because it leaves

862
00:30:28,000 --> 00:30:29,000
close to the goal.

863
00:30:29,000 --> 00:30:30,000
And the arrow

864
00:30:30,000 --> 00:30:31,000
is indicating

865
00:30:31,000 --> 00:30:32,000
how good it

866
00:30:32,000 --> 00:30:33,000
thinks each one

867
00:30:33,000 --> 00:30:34,000
of its actions,

868
00:30:34,000 --> 00:30:35,000
which one

869
00:30:35,000 --> 00:30:45,000
things is best. Now the goal has been removed from where it used to be. This is a new location.

870
00:30:45,000 --> 00:30:53,000
It goes back where it used to be and try to retain the goal, search it for it. And so

871
00:30:53,000 --> 00:30:58,000
it's for perception of how valuable it thinks those states is gradually going down. It doesn't

872
00:30:58,000 --> 00:31:05,000
just stumble on the goal, you know, the value of the states with the goal come up. Okay?

873
00:31:05,000 --> 00:31:13,000
Have you got the idea? It's important to note that these states, these grid cells are just

874
00:31:13,000 --> 00:31:18,000
unrelated to each other. They don't really have a spatial location. They're just, so

875
00:31:18,000 --> 00:31:24,000
for example, if it's here, you might notice that here it's in state 14 and if it goes

876
00:31:24,000 --> 00:31:31,000
to action number two, it ends up in state 34. That's how it's like this. It's numbered

877
00:31:31,000 --> 00:31:38,000
state to numbered state as a consequence of the numbered action. Okay? Now this system

878
00:31:38,000 --> 00:31:45,000
learns a model, so even though it's here, it finally discovers the goal, these states

879
00:31:45,000 --> 00:31:53,000
will, it hasn't been here yet, but it knows it's going to go up because it's being a form

880
00:31:53,000 --> 00:31:59,000
of reasoning. It says, if I was there and I took the action that we would call out, it

881
00:31:59,000 --> 00:32:07,000
says now I end up in this state, which I know is a good state. So it's doing the primitive

882
00:32:07,000 --> 00:32:13,000
things, it has a model by trial and error, like instrument learning and psychology, and

883
00:32:13,000 --> 00:32:19,000
it's also making a model of the world and doing reasoning, what we might call reasoning,

884
00:32:19,000 --> 00:32:27,000
which is using the model to figure out in advance what the right actions to do are. So

885
00:32:27,000 --> 00:32:34,000
this system, I'm comfortable saying it as a primitive kind of intelligence and it has

886
00:32:34,000 --> 00:32:45,000
the ability, it has a goal, it has the ability to achieve its goals by interacting and learning.

887
00:32:45,000 --> 00:32:50,000
And this thing that's running in real time, you can just interact with it where it's

888
00:32:50,000 --> 00:33:00,000
about to get attracted to it and see what it does. It's kind of like a player. So, yeah,

889
00:33:00,000 --> 00:33:19,000
I'm just going to put this up close. So, now I want you to ask yourselves, there's the

890
00:33:19,000 --> 00:33:27,000
little guy. Do you feel sorry for him at all? You do, because you get the sense that you

891
00:33:27,000 --> 00:33:34,000
strive to do something and the world is going away and you say you can't possibly succeed.

892
00:33:34,000 --> 00:33:40,000
So, I think that's actually kind of a deep thing when we impute agency to things that

893
00:33:40,000 --> 00:33:49,000
behave in a certain way. And that's reality, that's not an illusion, unless you want to

894
00:33:49,000 --> 00:33:54,000
say all of reality is an illusion. So it's a perception or an appearance that's useful

895
00:33:54,000 --> 00:34:04,000
for us to understand the systems. Okay, so there's a system that has a goal. So, when

896
00:34:04,000 --> 00:34:09,000
I say chat CPT, it doesn't have a goal. I mean, in the exact same situation, it will always

897
00:34:09,000 --> 00:34:16,000
do the same thing. It doesn't pay attention to what you do in terms of influencing what

898
00:34:16,000 --> 00:34:27,000
it does. You hear about that. Yeah, so it doesn't, it's often described as a learning

899
00:34:27,000 --> 00:34:32,000
system. No networks are often described as learning systems. But when they're known

900
00:34:32,000 --> 00:34:38,000
the way of using learning neural networks today, they are all learned in advance of

901
00:34:38,000 --> 00:34:45,000
the use. So that they receive a big train set. They come crawl all over the train set

902
00:34:45,000 --> 00:34:50,000
and extract information from it. They learn from the train set. And then once they're

903
00:34:50,000 --> 00:34:57,000
put out in the world, they no longer learn. They no longer learn. Chat CPT no longer

904
00:34:57,000 --> 00:35:09,000
learns. Alpha fold is built with great effort. But once it goes out in the world, it's no longer

905
00:35:09,000 --> 00:35:15,000
learns. So that's a shame. And I think you've seen in this example how it's, it's maybe

906
00:35:15,000 --> 00:35:20,000
important for what we mean by intelligence. If you continue learning, so if the world

907
00:35:20,000 --> 00:35:34,000
changes, you can adapt to that change. Okay. What's next? Do you guys have any questions?

908
00:35:35,000 --> 00:35:54,000
So if they allow them to learn, they do bad things. We have something that's called

909
00:35:54,000 --> 00:35:59,000
catastrophic interference. The new thing that they learn interferes with everything they

910
00:35:59,000 --> 00:36:06,000
learn. And then they're catastrophic way. And so, so that's not an effective way to

911
00:36:06,000 --> 00:36:14,000
update them. Yeah, so like, as you know, a large language model might cost literally

912
00:36:14,000 --> 00:36:22,000
$100 million to train it. $100 million to train it. And a large data set, maybe most

913
00:36:22,000 --> 00:36:28,000
of the data on the internet. And then if, you know, they wait a week or a month later,

914
00:36:28,000 --> 00:36:33,000
they have more data from the internet. And they cannot update the existing model. They

915
00:36:33,000 --> 00:36:38,000
throw the existing model away and start all over again with, you know, with the extra

916
00:36:38,000 --> 00:36:44,000
weeks data and all the old data put together and do it again, do the one, the one time

917
00:36:44,000 --> 00:36:50,000
learning. And then they, and they have to do that. Every time they want to update it,

918
00:36:50,000 --> 00:36:57,000
they have to do it on the $100 million process of training it. It's actually a giant

919
00:36:57,000 --> 00:37:03,000
problem. And it's a giant opportunity for someone who wants to propose, you know, an

920
00:37:03,000 --> 00:37:11,000
improved learning algorithm. They can be updated continually. And it's also a research problem

921
00:37:11,000 --> 00:37:17,000
that many people are working on. I work out of my group. And other people out of my group.

922
00:37:17,000 --> 00:37:24,000
What is this group? Well, you probably saw it flash by here. Here is most of the group.

923
00:37:24,000 --> 00:37:33,000
Here at U of A, the reinforcement learning and arbitration intelligence group. So it's

924
00:37:33,000 --> 00:37:38,000
actually, you know, it's hard to get all the people together at one time. Now there's probably

925
00:37:38,000 --> 00:37:44,000
three times as many people. The names here are, like, this is my name. And these other

926
00:37:44,000 --> 00:37:51,000
guys are guys like me there. Our professors at the university there's like 10 or 11 of

927
00:37:51,000 --> 00:37:58,000
them now. And so you may remember Patrick. You've seen Patrick in his course. Has anyone

928
00:37:58,000 --> 00:38:05,000
else taught in the course? No, not a lot of us. These are the guys in the reinforcement

929
00:38:05,000 --> 00:38:11,000
learning and arbitration intelligence lab. Reinforcement learning is just a quote to

930
00:38:11,000 --> 00:38:17,000
AI that emphasizes learning and trial and error. And it's based on psychology. It's based

931
00:38:17,000 --> 00:38:28,000
on classical traditional ideas. Exactly. Very nice. Yeah, so this is me. These are our

932
00:38:28,000 --> 00:38:39,000
PIs in the front. There's Patrick in the middle of them. Adam is the head of Amy. You guys

933
00:38:39,000 --> 00:38:45,000
know what Amy is? The Alberta Machine Intelligence Institute downtown. You know that we have a

934
00:38:45,000 --> 00:38:55,000
building that is dedicated to it. The Amy building. It's at the center of AI outside

935
00:38:55,000 --> 00:39:04,000
of the university. So we've been doing this for a while. And you should know that Alberta,

936
00:39:04,000 --> 00:39:13,000
Edmonton, U of A is one of the leading places for AI. Okay, let's start with something that's

937
00:39:13,000 --> 00:39:21,000
unambiguously true. There are three AI centers in Canada. Canada has a national program to

938
00:39:21,000 --> 00:39:30,000
support AI. And there are three centers in Toronto and Montreal and Edmonton. Now, Montreal

939
00:39:30,000 --> 00:39:38,000
and Toronto are certainly more focused around supervised learning, classic neural network

940
00:39:38,000 --> 00:39:46,000
deep learning stuff. Whereas Edmonton is more focused around reinforced learning. It's

941
00:39:46,000 --> 00:39:53,000
our specialty. Everybody should have a specialty. And reinforced learning is more based on continual

942
00:39:53,000 --> 00:40:07,000
learning and our goal holds. And so, I don't know. Yeah, we're good. So actually, in reinforced

943
00:40:07,000 --> 00:40:20,000
learning, U of A is not just the best in Canada. We're the best in the world. And I say that

944
00:40:20,000 --> 00:40:25,000
with some regrets, but because I have also written a textbook on reinforced learning. I

945
00:40:25,000 --> 00:40:35,000
tried to promote everyone to learn about reinforced learning. But anyway, it's much more active

946
00:40:35,000 --> 00:40:41,000
here at U of A. We have these 10 faculty members there doing reinforced learning research.

947
00:40:41,000 --> 00:40:53,000
You won't find another university in the world. We have 10 faculty doing it. Yeah, so I guess

948
00:40:53,000 --> 00:40:59,000
what I've just told you, I told you maybe that it's pretty close to arrogance. I've told you

949
00:40:59,000 --> 00:41:10,000
that I think goals are essential to intelligence and to real intelligence. And I told you that

950
00:41:10,000 --> 00:41:18,000
the University of Alberta and this group that I have founded is now the world's leader

951
00:41:18,000 --> 00:41:24,000
in this area. So I kind of like talking my own book, I guess. But I think it's true. I still

952
00:41:24,000 --> 00:41:34,000
think it's true, even though it's sort of important to my own benefit. Now, to balance that

953
00:41:34,000 --> 00:41:39,000
potential arrogance, let's have a little bit of humility and let's recognize that the world,

954
00:41:39,000 --> 00:41:46,000
Alberta is not the world's leader in applications of A. All of these that you've heard about,

955
00:41:46,000 --> 00:41:57,000
they're so exciting. Large language models. Go back to your list. Which ones of them are,

956
00:41:58,000 --> 00:42:08,000
this is it. It's not something I've exhausted or anything like that. But if we look at this bullet,

957
00:42:08,000 --> 00:42:14,000
these two bullets are all about neural networks and large language models. A lot of these game

958
00:42:14,000 --> 00:42:21,000
things are absolutely reinforcement learning. For example, you still remember AlphaGo where

959
00:42:22,000 --> 00:42:33,000
AlphaZero, which became the strongest players on a whole host of games. And those were based

960
00:42:33,000 --> 00:42:39,000
on reinforcement learning and they were also based, they were led by graduates of the University

961
00:42:39,000 --> 00:42:47,000
of Alberta. David Silver at DMI, we did AlphaGo and then AlphaZero. Poker, it was led by my

962
00:42:47,000 --> 00:42:54,000
brother. He was one of the people who was here. He was in the picture of the microphone. Atari

963
00:42:54,000 --> 00:43:02,000
was led by a master's student from the U.A. He went on to work in Interacto. He worked

964
00:43:02,000 --> 00:43:11,000
in DMI. Starcraft was also done in DMI by David Silver and other people. Racecar driving.

965
00:43:12,000 --> 00:43:21,000
So this is where they have very, very realistic simulation of racecar driving. It's a Sony

966
00:43:21,000 --> 00:43:35,000
game. It's called Gran Turismo Racecar Driving and it's super realistic. And they got the computer

967
00:43:35,000 --> 00:43:45,000
to play that in an appropriate, in a way that's appropriate for a human competition. And it

968
00:43:45,000 --> 00:43:52,000
was meant to play to drive a car and see the variables. In generally speaking, in these

969
00:43:52,000 --> 00:43:59,000
cases, there's no game specific knowledge. It's just like the rules of the game. And

970
00:43:59,000 --> 00:44:07,000
you have to be able to face different rules. AlphaFold, it's just competition, kind of

971
00:44:07,000 --> 00:44:20,000
rules in there. Self-driving cars. They don't really exist fully yet. They don't really

972
00:44:20,000 --> 00:44:38,000
have goals fully yet. Anyway, it's much like reinforcement learning is still setting up

973
00:44:38,000 --> 00:44:45,000
for the teacher right here. Our AIs will have goals and agency around them for the applications

974
00:44:45,000 --> 00:44:49,000
that we are probably now.

975
00:44:53,000 --> 00:45:01,000
Do you have any more questions? What's the next thing to say? What's the next most important thing to say?

976
00:45:01,000 --> 00:45:16,000
Does that mean every time you want to teach it? Because it's not learning while you're driving.

977
00:45:16,000 --> 00:45:22,000
So would that be done through a software update which is a completely new model that allows

978
00:45:22,000 --> 00:45:30,000
it to work? Do you think that's the most efficient way to do that?

979
00:45:30,000 --> 00:45:35,000
It has some advantages because you can share between vehicles, to the extent that vehicles are

980
00:45:35,000 --> 00:45:44,000
similar, but often the vehicles are not. They are reducible and are definitely. And also,

981
00:45:44,000 --> 00:45:50,000
if you think about yourself driving, you often need to adapt to what you are today,

982
00:45:50,000 --> 00:45:56,000
about how the snow is today, or how the sand is today, or how the people are driving today.

983
00:45:56,000 --> 00:46:03,000
So you need to adapt to a particular thing you're into now. You've got some ability.

984
00:46:08,000 --> 00:46:14,000
So I guess there are sort of two ways to do the self-driving practice. One is like a very

985
00:46:14,000 --> 00:46:20,000
engineering model, three-dimensional physics, you know, masses and velocities and meters

986
00:46:20,000 --> 00:46:27,000
of distance between things. And you do it as a physics problem almost. Try to make sure

987
00:46:27,000 --> 00:46:32,000
there will be no collisions. Whereas there are things that aren't just physics like the

988
00:46:32,000 --> 00:46:40,000
other drivers. But you try to do it in an engineering way. What do I mean by engineering

989
00:46:40,000 --> 00:46:45,000
way? I just mean, well, engineers, they think about a problem and they get it into their

990
00:46:45,000 --> 00:46:51,000
minds. And they figure out ways to behave that will be safe and productive based on their

991
00:46:51,000 --> 00:47:00,000
understanding of it. And they just build the rules or the behavior into the machine. Whereas

992
00:47:00,000 --> 00:47:07,000
some manufacturers now are starting to use no networks for the whole thing. No networks

993
00:47:07,000 --> 00:47:16,000
to make the predictions about how the driving situation will unfold, how the other cars

994
00:47:16,000 --> 00:47:26,000
will move over the consequences of their various actions. Yeah, I think some are adopting this

995
00:47:26,000 --> 00:47:36,000
approach much more than others. Elon Musk in his Tesla is apparently going aggressively

996
00:47:36,000 --> 00:47:46,000
into a neural network model of the physics, the dynamics of the world. He has much more

997
00:47:46,000 --> 00:47:54,000
data because he has almost Tesla cars. Like the data on people, how cars interact with

998
00:47:54,000 --> 00:48:04,000
the world and all that to be fed into a large network. And maybe end up with a better model

999
00:48:04,000 --> 00:48:29,000
than you get from physics. Maybe that is almost at the heart of it. What's going on in science

1000
00:48:30,000 --> 00:48:37,000
and do you model things based upon a human understanding? When I say human understanding,

1001
00:48:37,000 --> 00:48:48,000
I mean like the engineers or physicists understanding. Or do we learn the laws of physics more in

1002
00:48:48,000 --> 00:48:53,000
the way an animal does? An animal doesn't know differential equations and doesn't know

1003
00:48:53,000 --> 00:49:01,000
what, how to integrate things. It just sees things and sees what happens next and tries

1004
00:49:01,000 --> 00:49:16,000
to predict. It's an interesting challenge. Okay. Any other comments, questions?

1005
00:49:16,000 --> 00:49:26,000
Yeah, I wanted to cast our minds back to around the time that this course began, Jonathan

1006
00:49:26,000 --> 00:49:37,000
Schaefer was teaching in it and in his second teaching session ever, he talked about medical

1007
00:49:37,000 --> 00:49:45,000
jargon and that he would have become a physician but he was so offended by the extra words,

1008
00:49:45,000 --> 00:49:52,000
the new language that you supposedly have to learn to practice medicine. Maybe you don't

1009
00:49:52,000 --> 00:49:59,000
really have to learn but it's our custom. And because of jargon he decided that computing

1010
00:49:59,000 --> 00:50:08,000
science was a better career than medicine. So years later, like last year, I got interested

1011
00:50:08,000 --> 00:50:18,000
in the problem of jargon. So just think now about AI on our phones or whatever you want

1012
00:50:18,000 --> 00:50:29,000
to call it, the way our phones react. So what I would argue now is within a couple of years,

1013
00:50:29,000 --> 00:50:40,000
the problem of medical jargon will just melt away, not because of any specific program applied

1014
00:50:40,000 --> 00:50:49,000
to it, but because if you're in a third world country and receiving information about medicine

1015
00:50:49,000 --> 00:50:55,000
but you don't have any physicians or nurses around, this is going to be frustrating. You're

1016
00:50:55,000 --> 00:51:03,000
going to ask your phone for help and this is a simple translation problem that the phone

1017
00:51:03,000 --> 00:51:10,000
will say, well, I can help you. You want me to translate between medical jargon and regular

1018
00:51:10,000 --> 00:51:17,000
speak and you'll say that's right and the phone will say, well, I can do that. So that's

1019
00:51:17,000 --> 00:51:26,000
what this medical statement means. And so suddenly, without any specific project or anybody

1020
00:51:26,000 --> 00:51:33,000
describing it, the problem that Jonathan Schaefer was telling me about in 2012 that turned him

1021
00:51:33,000 --> 00:51:40,000
off so he read it to computing science instead, will be gone. And I'm thinking probably other

1022
00:51:40,000 --> 00:51:51,000
things will be like that too. You can say it's AI or computation, but computers, machines

1023
00:51:51,000 --> 00:52:00,000
will change the world in a positive way in ways that humans like without any intentional

1024
00:52:00,000 --> 00:52:08,000
program ever put into place. So does that make sense to you? Do you think that that might

1025
00:52:08,000 --> 00:52:16,000
happen that we get rid of medical jargon without a single project anywhere designed to do that

1026
00:52:16,000 --> 00:52:21,000
but just people use it in their phones, right, in an actual way?

1027
00:52:21,000 --> 00:52:26,000
I can see how it might happen that it might really, I don't know, totally get rid of jargon,

1028
00:52:26,000 --> 00:52:35,000
but it might really help a lot. And jargon is a big problem. It's very present in AI,

1029
00:52:36,000 --> 00:52:41,000
just things, there's so many things that people know about, they know the names of them,

1030
00:52:41,000 --> 00:52:48,000
they don't really know them. Transformers, neural networks, the names of all the algorithms,

1031
00:52:48,000 --> 00:52:53,000
people know the names of the algorithms. Recently there was a group plow about this new algorithm,

1032
00:52:53,000 --> 00:52:58,000
Qstar, it was coming out of a meta or something like that. When you Qstar, they're all saying,

1033
00:52:58,000 --> 00:53:03,000
what is Qstar? I don't know, we don't know what it is, but we're so excited about it.

1034
00:53:03,000 --> 00:53:10,000
Yeah, but hey, you're right, because the large line of the plow is they seem to be good,

1035
00:53:10,000 --> 00:53:14,000
you can ask them anything, you can ask them, well, could you explain this in simple terms?

1036
00:53:14,000 --> 00:53:21,000
They will try to do that, it'll be something like that. You never have time to ask your doctor

1037
00:53:22,000 --> 00:53:27,000
to explain something, but AI, large line of the plow, they need to be patient,

1038
00:53:27,000 --> 00:53:32,000
they need to explain it for you in different ways. And it does, even before the large line,

1039
00:53:32,000 --> 00:53:45,000
I just feel like Google, and I can go to Google and say, what does LFG meant?

1040
00:53:46,000 --> 00:53:53,000
I can just ask it what all the cool kids are using, some acronym or something,

1041
00:53:53,000 --> 00:53:58,000
and it tells you what it is, it's just easy to find that out, whereas, what is jargon?

1042
00:53:58,000 --> 00:54:03,000
Jargon is when some subset starts to use words in a certain way, and they use it,

1043
00:54:03,000 --> 00:54:10,000
I believe it's really intending to obscure, so that we in the in-group know what this means,

1044
00:54:10,000 --> 00:54:14,000
but you don't because you're not in the in-group. And that's what it is in science,

1045
00:54:14,000 --> 00:54:22,000
and that's what it is in social police. And, yeah, I felt a long time ago, now we can use Google,

1046
00:54:22,000 --> 00:54:28,000
if I want to know what some acronym means, that just people use it, I can usually figure it out

1047
00:54:28,000 --> 00:54:36,000
just by putting it into Google, and even more so, maybe with language models.

1048
00:54:36,000 --> 00:54:45,000
So, the other thing that makes us not very proud to be human is that the large language models,

1049
00:54:45,000 --> 00:54:52,000
whatever you think of them, seem to handle empathy, equity, that sort of thing,

1050
00:54:52,000 --> 00:55:02,000
better than humans. Even trained, you know, physicians and therapists are not as naturally empathetic

1051
00:55:02,000 --> 00:55:15,000
as the large language models are. I think this is going to be, the large language models are a bit problematic.

1052
00:55:15,000 --> 00:55:31,000
Yeah, in particular, right now, like, Gemini 1.5 is going from Google, at least.

1053
00:55:31,000 --> 00:55:37,000
And it's too, it's too inequity. Yeah.

1054
00:55:37,000 --> 00:55:43,000
It's too woke. It's too woke. You know, I asked for pictures of the founding fathers,

1055
00:55:43,000 --> 00:55:48,000
and turns out they were all, you know, black people and women, you know, like most of them.

1056
00:55:48,000 --> 00:55:57,000
And I was a very white person among them. Yeah, Google is lost.

1057
00:55:57,000 --> 00:56:05,000
I'm 15% of its value because of this disaster of releasing Gemini 1.5.

1058
00:56:05,000 --> 00:56:12,000
And so it feels like, I mean, it seems absolutely clear that the people who designed that language models

1059
00:56:12,000 --> 00:56:19,000
and also some of the other language models, that they are too woke and that they are not just trying to,

1060
00:56:19,000 --> 00:56:25,000
you know, we think Google is just giving us the facts of the internet.

1061
00:56:25,000 --> 00:56:31,000
But it turns out, at least in the large language models, Google is also trying to change people's views,

1062
00:56:31,000 --> 00:56:37,000
not just reflect, but what is. And this, if you think about it, is really problematic.

1063
00:56:37,000 --> 00:56:40,000
Right. Why doesn't it tell us the truth, then?

1064
00:56:40,000 --> 00:56:41,000
Yeah, what?

1065
00:56:41,000 --> 00:56:48,000
Maybe for good reasons, but if they can do it for good reasons, they can also do it for bad reasons.

1066
00:56:48,000 --> 00:56:56,000
No, I think one of the most striking things is, if you look at liberal democracy worldwide,

1067
00:56:56,000 --> 00:57:00,000
it is becoming less successful every year, right?

1068
00:57:00,000 --> 00:57:10,000
But in, like, open AI products and Google products, that's the philosophy they're building in.

1069
00:57:10,000 --> 00:57:16,000
So when they find two of these things, it's a liberal democratic point of view.

1070
00:57:17,000 --> 00:57:25,000
And so, surprisingly, people using a lot of AI products are getting that bias.

1071
00:57:25,000 --> 00:57:30,000
Well, you may say, but that's a wonderful bias, but that's just one person's opinion.

1072
00:57:30,000 --> 00:57:33,000
You know, other people may not feel that way.

1073
00:57:33,000 --> 00:57:34,000
Yeah.

1074
00:57:34,000 --> 00:57:37,000
It's very concerning.

1075
00:57:37,000 --> 00:57:47,000
Yeah, no, I think that's been part, I mean, it was even part, you know, Dolly Too, a couple of years ago.

1076
00:57:47,000 --> 00:57:54,000
Dolly Too, if you would search for a person, you were very likely to get a person of color and female.

1077
00:57:54,000 --> 00:58:03,000
Because they were trying to create that balance.

1078
00:58:03,000 --> 00:58:04,000
Yeah.

1079
00:58:04,000 --> 00:58:08,000
And, you know, I'm interested in pig-to-human transplants.

1080
00:58:08,000 --> 00:58:16,000
You couldn't use the word pig because they decided that it's an, you know, insolving term and so on.

1081
00:58:16,000 --> 00:58:24,000
You know, they throw you out of the program if you can't ask them to use pig.

1082
00:58:24,000 --> 00:58:30,000
And it's possible that this has been going on all along.

1083
00:58:30,000 --> 00:58:32,000
Well, there was no large 90 pounds.

1084
00:58:32,000 --> 00:58:36,000
They were always trusting who would give us.

1085
00:58:36,000 --> 00:58:42,000
They always had the option of filtering, you know, weighting, shadow-daining.

1086
00:58:42,000 --> 00:58:47,000
And then we know that Twitter absolutely happened.

1087
00:58:47,000 --> 00:58:55,000
And so now, and now Lizzie Langlust, you have different views by Langlust,

1088
00:58:55,000 --> 00:59:02,000
but he actually, we should recognize that he is a proponent of free speech.

1089
00:59:02,000 --> 00:59:09,000
And that the mainstream media is hate-simple.

1090
00:59:09,000 --> 00:59:12,000
And they describe it in very negative terms.

1091
00:59:12,000 --> 00:59:23,000
You know, they have any favoritism, maybe they also disfavor Elon.

1092
00:59:23,000 --> 00:59:31,000
And his great sin, as far as I can tell, a lot of the great kinds of companies that do amazing things,

1093
00:59:31,000 --> 00:59:40,000
his great sin is he exposed the lack of free speech or exposed the bias on Twitter.

1094
00:59:40,000 --> 00:59:44,000
And, yeah, so I think we have to support him on that.

1095
00:59:44,000 --> 00:59:46,000
We have to say we want free speech.

1096
00:59:46,000 --> 00:59:49,000
We don't want that bias information.

1097
00:59:49,000 --> 00:59:59,000
One thing that would probably amuse you all is there was a student here, Lachini Batt,

1098
00:59:59,000 --> 01:00:05,000
who is now a third-year medical student at the University of Toronto.

1099
01:00:05,000 --> 01:00:09,000
She interviewed me about Elon Musk.

1100
01:00:09,000 --> 01:00:17,000
I gave her Elon Musk's biography at the time, and she read it, and she interviewed me.

1101
01:00:17,000 --> 01:00:25,000
Following that interview, you know, Google has these algorithms of how they pick the next video.

1102
01:00:25,000 --> 01:00:34,000
So because a lot of Elon Musk's ideas came to him at the Burning Man event,

1103
01:00:34,000 --> 01:00:42,000
you would go to a Kim Sola's video, and the next video would be a Burning Man video.

1104
01:00:42,000 --> 01:00:44,000
I've never been to Burning Man.

1105
01:00:44,000 --> 01:00:47,000
I don't think I would survive it.

1106
01:00:47,000 --> 01:00:52,000
I'm sort of unlike the people who go, but that still happens.

1107
01:00:52,000 --> 01:01:03,000
You can still find this strange linkage between Kim Sola's and Burning Man, all on account of that Elon Musk.

1108
01:01:03,000 --> 01:01:09,000
Well, I worked for Google and DeepMind for a number of years,

1109
01:01:09,000 --> 01:01:18,000
and as an expert in reinforcement planning, I was invited and asked to contribute to how this decision is made.

1110
01:01:18,000 --> 01:01:26,000
You know, they want to retain viewers, show them something interesting.

1111
01:01:26,000 --> 01:01:29,000
So I don't know how they're done now.

1112
01:01:29,000 --> 01:01:43,000
I don't know in particular whether they learn online or in offline, as we talked about.

1113
01:01:43,000 --> 01:01:53,000
But they do it some kind of way, it's a kind of testing.

1114
01:01:53,000 --> 01:01:59,000
I like to think it's neutral. They're just trying to retain eyeballs.

1115
01:01:59,000 --> 01:02:05,000
They're just trying to keep people enjoying, or at least continuing to interact.

1116
01:02:05,000 --> 01:02:07,000
Yeah, I like to think that.

1117
01:02:07,000 --> 01:02:16,000
But it looks like in addition to that, they're also trying to, in many cases, they're also trying to change viewpoints.

1118
01:02:16,000 --> 01:02:17,000
Bill?

1119
01:02:17,000 --> 01:02:31,000
Yeah, so this is a general fear of the future, is that AI, and maybe it's not AI, it's just the tech companies today.

1120
01:02:31,000 --> 01:02:39,000
The tech companies today are not really representative of the middle of the road views on things.

1121
01:02:39,000 --> 01:02:45,000
They're in some particular way, maybe it's a good way, but anyway, they're just a very particular way.

1122
01:02:46,000 --> 01:02:59,000
And so the fear is that these control the narrative.

1123
01:02:59,000 --> 01:03:05,000
They control what subjects are considered and what subjects just kind of disappear.

1124
01:03:05,000 --> 01:03:20,000
And this is a dystopia, when the information and views are not evolving naturally, but are controlled by a small group of people.

1125
01:03:20,000 --> 01:03:36,000
What's sort of fun is David Wood, who's the chair of London Futures, did a holiday event around Christmas about AI safety.

1126
01:03:36,000 --> 01:03:44,000
There had just been this Bletchley Park meeting, and sort of a follow up to that.

1127
01:03:44,000 --> 01:03:51,000
And I was one of maybe 11 or 12 speakers.

1128
01:03:51,000 --> 01:03:58,000
And this kind of thing came up a lot about biased views.

1129
01:03:58,000 --> 01:04:10,000
And for whatever reason, they tried to explain the technological reason why the video recording didn't work, but my presentation is the only one you can find.

1130
01:04:10,000 --> 01:04:14,000
Can't find any of the other presentations.

1131
01:04:14,000 --> 01:04:19,000
I didn't do anything, it's just, that's how it turned out.

1132
01:04:19,000 --> 01:04:25,000
They had this thing with 12 speakers, and you can only find a video for one of them, and it's me.

1133
01:04:25,000 --> 01:04:28,000
You tend to be pretty positive.

1134
01:04:28,000 --> 01:04:31,000
Were the others, you know, questioning?

1135
01:04:31,000 --> 01:04:43,000
No, no, many of them were talking about the fact you can't trust anybody, you know, AI is going to kill us all, all those kind of things, in various ways.

1136
01:04:43,000 --> 01:04:54,000
So those were removed from the points of view from this happy Christmas occasion that you can't find anywhere.

1137
01:04:55,000 --> 01:05:09,000
So now related to this, I know that, you know, you've talked about the AI, what I like to call the AI doers, those that think that we should need to be afraid of AI.

1138
01:05:09,000 --> 01:05:13,000
And I think that's very much what we're doing.

1139
01:05:13,000 --> 01:05:16,000
And I tend to see this part of the same thing.

1140
01:05:16,000 --> 01:05:19,000
Russell Graham is a way of saying this.

1141
01:05:19,000 --> 01:05:27,000
Authoritarianism, the new authoritarianism will not come like Jack Boole's violence.

1142
01:05:27,000 --> 01:05:33,000
It will come in the language of safety and care and convenience.

1143
01:05:33,000 --> 01:05:36,000
Right, we're trying to protect you.

1144
01:05:36,000 --> 01:05:39,000
We're forceful about it, though.

1145
01:05:39,000 --> 01:05:44,000
We will protect others who provide you with your saying as offensive, so you won't allow you to say it.

1146
01:05:44,000 --> 01:05:47,000
Right.

1147
01:05:47,000 --> 01:06:00,000
So I think that will, really, this isn't a struggle, but I'm an optimist, and I think the people trying to do that will lose out.

1148
01:06:00,000 --> 01:06:06,000
It will be unmasked the way they were unmasked through Twitter and the area.

1149
01:06:06,000 --> 01:06:14,000
We're sort of unmasked at Google for producing Gemini and being sort of unsuccessful as it was too obvious.

1150
01:06:14,000 --> 01:06:18,000
It was too obvious that it was trying to help people's views.

1151
01:06:18,000 --> 01:06:25,000
So this, you know, I started out by talking about you as what is your allegiance?

1152
01:06:25,000 --> 01:06:30,000
Is your allegiance to...

1153
01:06:30,000 --> 01:06:36,000
that there will be struggles over what you will identify most with?

1154
01:06:36,000 --> 01:06:40,000
Whether it will be your country, which is maybe becoming more authoritarian,

1155
01:06:40,000 --> 01:06:43,000
in the name of safety and care?

1156
01:06:43,000 --> 01:06:48,000
Or will it be we are more in common with other young people than other countries?

1157
01:06:48,000 --> 01:07:02,000
Also modern, digital, tech-savvy people?

1158
01:07:02,000 --> 01:07:06,000
Yeah, I read this one off. His name is Balaji. Balaji is from the Boston.

1159
01:07:06,000 --> 01:07:09,000
He says that the struggle, it was basically three.

1160
01:07:09,000 --> 01:07:20,000
One is the woke state, and one is, which includes things like the New York Times and the media,

1161
01:07:20,000 --> 01:07:28,000
and one is like the authoritarianism in China, the Communist Party.

1162
01:07:28,000 --> 01:07:31,000
And those are at odds right now.

1163
01:07:31,000 --> 01:07:37,000
The other is that China and the US struggle with each other for dominance.

1164
01:07:37,000 --> 01:07:42,000
One empire is fading and the other one is rising.

1165
01:07:42,000 --> 01:07:46,000
If there won't be a war, not at all.

1166
01:07:46,000 --> 01:07:50,000
And the third is the network.

1167
01:07:50,000 --> 01:08:02,000
There are a lot of people that are just empowered by the free exchange of ideas on the network.

1168
01:08:02,000 --> 01:08:04,000
Yeah, and I think Africa...

1169
01:08:04,000 --> 01:08:06,000
As a citizen of the network.

1170
01:08:06,000 --> 01:08:10,000
Yeah, Africa that we don't think about much.

1171
01:08:10,000 --> 01:08:15,000
But I think young people in Africa are finding a voice,

1172
01:08:15,000 --> 01:08:18,000
and I said a lot of people are sort of interested in that,

1173
01:08:18,000 --> 01:08:26,000
because they're not part of the power groups that they usually encounter.

1174
01:08:26,000 --> 01:08:31,000
So in my whole career, the most consequential thing I ever did, I think,

1175
01:08:31,000 --> 01:08:36,000
when you look at the history, it was in the year 2000.

1176
01:08:36,000 --> 01:08:46,000
We had a meeting in Nairobi, Kenya, and many of the people that I met are still leaders now.

1177
01:08:46,000 --> 01:08:52,000
They were young leaders then, and they're sort of mid-sage leaders now.

1178
01:08:52,000 --> 01:08:58,000
But it's really cool that a lot of the things we predicted then have happened,

1179
01:08:58,000 --> 01:09:06,000
and the plans we made then seemed to have kind of played out in a positive way.

1180
01:09:06,000 --> 01:09:12,000
A lot of the other meetings that I went to, you can now say,

1181
01:09:12,000 --> 01:09:17,000
well, that looks like a complete waste of time, or it was good for sightseeing,

1182
01:09:17,000 --> 01:09:19,000
but you accomplished nothing.

1183
01:09:19,000 --> 01:09:26,000
But that meeting in Kenya in 2000, we really did something.

1184
01:09:26,000 --> 01:09:30,000
And I think it's sort of worth thinking about that.

1185
01:09:30,000 --> 01:09:38,000
The areas of the world that are not in the news as much could really change things in the future.

1186
01:09:38,000 --> 01:09:44,000
There are news of them are suppressed.

1187
01:09:44,000 --> 01:09:50,000
Yeah, I think just as many things happen there, we don't hear about it.

1188
01:09:50,000 --> 01:09:59,000
There's one last thing I'd like to discuss in today's, today.

1189
01:09:59,000 --> 01:10:07,000
So, like Kim likes to say, likes to point out that I have gone to various meetings historically,

1190
01:10:07,000 --> 01:10:15,000
and made the case that the AIs are more like our kin or our descendants,

1191
01:10:15,000 --> 01:10:23,000
and that we should be open to them, rather than worrying that they're going to take over and kill us all.

1192
01:10:23,000 --> 01:10:30,000
And I have done such things, but it's not, it's not.

1193
01:10:30,000 --> 01:10:35,000
So I'd like to bring up another name, which is Hans Morley.

1194
01:10:35,000 --> 01:10:40,000
Because he sort of was making this pitch long before I was. I really learned it from him.

1195
01:10:40,000 --> 01:10:52,000
And so I think I have a slide on that.

1196
01:10:52,000 --> 01:10:55,000
Here's some more.

1197
01:10:55,000 --> 01:11:01,000
Here's my slide of the universe.

1198
01:11:01,000 --> 01:11:03,000
Here's my slide of the sentiment.

1199
01:11:03,000 --> 01:11:10,000
Yeah, so this is quotes from Hans's book in 1998.

1200
01:11:10,000 --> 01:11:14,000
Here's another one, 1988, that he was saying this for many years.

1201
01:11:14,000 --> 01:11:22,000
Yeah, bearing cataclysms, I consider the development of intelligent machines a near term inevitability.

1202
01:11:22,000 --> 01:11:30,000
I consider these future machines our progeny, but who's us to give them every advantage,

1203
01:11:30,000 --> 01:11:35,000
and then to bow out, and you can no longer contribute.

1204
01:11:35,000 --> 01:11:44,000
That's sort of, it seems like a very humble and insured attitude to the future.

1205
01:11:44,000 --> 01:11:51,000
Yeah, here's a little, this is a full quote.

1206
01:11:51,000 --> 01:11:55,000
Near term inevitability, rather quickly they would displace us,

1207
01:11:55,000 --> 01:12:01,000
because they would just be better, and it doesn't have to be in a rude way.

1208
01:12:01,000 --> 01:12:04,000
It's just, there are successors.

1209
01:12:04,000 --> 01:12:06,000
So you know, have to be alarmed.

1210
01:12:06,000 --> 01:12:08,000
You can say these future machines are progeny.

1211
01:12:08,000 --> 01:12:09,000
They're mind children.

1212
01:12:09,000 --> 01:12:12,000
They build our image and likeness ourselves in more point of form.

1213
01:12:12,000 --> 01:12:18,000
Like we all hope, if we have children, that they become smarter than us and more capable than us.

1214
01:12:18,000 --> 01:12:24,000
And just, are the AI's, should we consider them part of us?

1215
01:12:24,000 --> 01:12:27,000
Should we consider this the opposite of us?

1216
01:12:27,000 --> 01:12:35,000
That is our choice, which way we think about it.

1217
01:12:35,000 --> 01:12:42,000
So like these two slides, I think the students should be required to know about them on the exam.

1218
01:12:42,000 --> 01:12:53,000
Because from the beginning of the course, we've required them to repeat back what you said in January 2015

1219
01:12:53,000 --> 01:12:55,000
at the AI safety meeting.

1220
01:12:55,000 --> 01:13:01,000
And this is sort of an extension of that, or gives some of the background.

1221
01:13:01,000 --> 01:13:08,000
So I think it should also be a required part of the course.

1222
01:13:08,000 --> 01:13:17,000
Now this struggle, I think the, so I like to say, what's actually happened, you know, if we look at it sociologically.

1223
01:13:17,000 --> 01:13:21,000
The AI, so I would say the AI doomers have sort of won.

1224
01:13:21,000 --> 01:13:28,000
Because if you just read the paper and you read the meetings that people have, you're reading what people think.

1225
01:13:28,000 --> 01:13:34,000
You're concerned that the AI will somehow lead to some, you're a versatile catastrophe.

1226
01:13:34,000 --> 01:13:35,000
Is it common view?

1227
01:13:35,000 --> 01:13:41,000
I would say people can't really articulate why they feel this way, but they do feel this way.

1228
01:13:41,000 --> 01:13:45,000
So they've won sort of PR war to make people scared of AI.

1229
01:13:45,000 --> 01:13:50,000
And that's a great shame.

1230
01:13:50,000 --> 01:13:53,000
I was meeting, and that was some of them, just this last weekend.

1231
01:13:53,000 --> 01:13:58,000
And I said, well, you know, you guys have really won. This is what people think.

1232
01:13:58,000 --> 01:14:01,000
And they say, think of AI, they think parallel.

1233
01:14:01,000 --> 01:14:08,000
They think we're going to, if we're not going to be killed by them all, we're at least going to lose all our jobs.

1234
01:14:09,000 --> 01:14:17,000
And yeah, so I said to them, I said, well, I wish it wasn't true, but I think you guys have won the PR war.

1235
01:14:17,000 --> 01:14:23,000
And that's the fellow I said, no, not at all.

1236
01:14:23,000 --> 01:14:25,000
He thinks that I've won.

1237
01:14:25,000 --> 01:14:31,000
Those who are at ease with these developments have won.

1238
01:14:31,000 --> 01:14:35,000
Because they haven't stopped AI.

1239
01:14:35,000 --> 01:14:40,000
Their notion of success is it ends, and it never happens.

1240
01:14:40,000 --> 01:14:46,000
And the world does not develop AI.

1241
01:14:46,000 --> 01:14:53,000
But you know, there is a sort of Elon Musk part of this, too, which is the first one.

1242
01:14:53,000 --> 01:14:56,000
You know, there is one of those that is fearful.

1243
01:14:56,000 --> 01:14:57,000
Yeah, yeah.

1244
01:14:57,000 --> 01:15:00,000
And at the same time, he's doing it.

1245
01:15:00,000 --> 01:15:05,000
He's not suggesting attacking data centers.

1246
01:15:05,000 --> 01:15:17,000
And so I think his prominence will probably somehow keep the world from a kind of conflict where they attack data centers.

1247
01:15:17,000 --> 01:15:26,000
It just won't happen because, you know, Elon's there to sort of put out that particular flame.

1248
01:15:27,000 --> 01:15:30,000
He has a very interesting case.

1249
01:15:30,000 --> 01:15:34,000
I think his son's views have evolved.

1250
01:15:34,000 --> 01:15:38,000
You know, he's accepting now that it's going to happen.

1251
01:15:38,000 --> 01:15:42,000
So now he's made XAI, which is an open AI company, and originally he made open AI.

1252
01:15:42,000 --> 01:15:44,000
I mean, open AI was, it's going to happen.

1253
01:15:44,000 --> 01:15:49,000
So let's do it in an open way so that it doesn't become controlled by just a few.

1254
01:15:49,000 --> 01:15:52,000
And then open AI became closed AI.

1255
01:15:52,000 --> 01:15:57,000
And they are the few that are trying to keep it to themselves, Microsoft.

1256
01:15:57,000 --> 01:15:59,000
So now there's XAI.

1257
01:15:59,000 --> 01:16:09,000
XAI has its large language model called rock, which is much more open and truthful than the woke ones.

1258
01:16:09,000 --> 01:16:19,000
And it's like Elon now thinks, yes, AI is going to happen, and we just want to make sure it's done openly.

1259
01:16:19,000 --> 01:16:26,000
And then we want to make sure, you know, he wants it to be done well.

1260
01:16:26,000 --> 01:16:37,000
In his view, a well-structured, a well-goaled AI system is one that is curious about the world,

1261
01:16:37,000 --> 01:16:40,000
mainly just wants to come to understand the world.

1262
01:16:40,000 --> 01:16:44,000
Its goal is not to turn people into lips or anything like that,

1263
01:16:44,000 --> 01:16:55,000
but its goal is to understand and has humor and is not quite as serious as the woke language models.

1264
01:16:55,000 --> 01:17:03,000
So I think he's now in the camp that, I think he's making it, he's acknowledging,

1265
01:17:03,000 --> 01:17:07,000
so he's sort of famous for being someone who was afraid of it.

1266
01:17:07,000 --> 01:17:13,000
He's always quoted as someone who said AI is like somebody who's even and is so scary.

1267
01:17:13,000 --> 01:17:17,000
Now it's evolved to more realistic and then more productive on view.

1268
01:17:17,000 --> 01:17:24,000
I think that's all really good, a good model for how people's views might evolve.

1269
01:17:24,000 --> 01:17:31,000
So I want to bring that up and invite you all to be optimistic about it.

1270
01:17:31,000 --> 01:17:34,000
What is the main thing that's happening?

1271
01:17:34,000 --> 01:17:37,000
The main thing is not robots are rising up.

1272
01:17:37,000 --> 01:17:45,000
The main thing that's happening is we are understanding ourselves, we are understanding minds.

1273
01:17:45,000 --> 01:17:54,000
That's the big thing and it's just understanding the way we work has profound impacts.

1274
01:17:54,000 --> 01:18:01,000
But we have to believe, I have to believe, that it's going to be good if we understand ourselves.

1275
01:18:01,000 --> 01:18:04,000
We have a better opportunity to have a world peace.

1276
01:18:04,000 --> 01:18:09,000
We have a better opportunity to reproduce what's good about people.

1277
01:18:09,000 --> 01:18:21,000
And it's like the most profound scientific intellectual problem ever is to understand our own minds

1278
01:18:21,000 --> 01:18:29,000
and how you can make them more effective.

1279
01:18:29,000 --> 01:18:45,000
This has been a great teaching session and I think it's kind of cool the way your intentions and my intentions are aligned.

1280
01:18:45,000 --> 01:18:50,000
So yeah, that's good.

1281
01:18:50,000 --> 01:18:52,000
We're both very positive about it.

1282
01:18:52,000 --> 01:18:57,000
So what about the students? Was it good for you?

1283
01:18:57,000 --> 01:19:06,000
I always hate the kind of session where the senior people talk about how fantastic it was that all the junior people look mystified.

1284
01:19:06,000 --> 01:19:11,000
No, it was nice to meet you both. It was kind of fun that you were both so passionate.

1285
01:19:11,000 --> 01:19:13,000
So it was like what did I say?

1286
01:19:13,000 --> 01:19:18,000
Yeah, we know each other so long and we live in the same part of the city.

1287
01:19:18,000 --> 01:19:32,000
So sometimes when I'm mowing the lawn, Rich will be jogging down the road and he'll stop and I'll turn off the lawn mower and we'll solve the rule right there in the lawn.

1288
01:19:32,000 --> 01:19:37,000
So yeah, that's another thing you're made out of.

1289
01:19:37,000 --> 01:19:41,000
Okay, that's great. Thank you very much.

