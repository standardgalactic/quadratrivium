to meet everyone. Maybe I can put everyone's name.
Sure. My name is Richard.
I'm Elian.
Elian?
Yes.
Mariam.
Mariam?
Yes.
Can we meet you?
Yes.
Mariam?
Elian?
And I'm Isla.
Isla?
Yes.
Don't go too fast. No, I gotta remember all these things.
Isla, Isla, and Mariam.
Say it again.
Elian.
Elian.
All right.
Okay.
Yeah.
Yeah?
Yeah.
Okay. Please meet you.
Okay.
Okay.
So, we are a few, and we can go whichever ways you guys are interested in.
I'm interested in.
And I got a bunch of slides, but there's only a few that I really want to present for sure.
Well, I sort of have the impression that we've a little bit made history many times when
you talk to her.
So, anything that you think would be more likely to be making history is more interest than
things that aren't going to make history.
Well, I think it is a sort of a time.
And I'm sure you're all young people, and you're only familiar with your own time in
some sense.
And you think it's just normal.
But I don't think this is a normal time you're in.
I think it's a time when, well, massively increasing computation is available to everybody
into our companies and people make products.
And this is really the only time like this.
I mean, some sense, under 200 years ago, there was the Industrial Revolution.
They discovered machines that, you know, like steam powered and physical work for people.
No, but that really took a long time.
Whereas now, with the computation, I mean, much more plentiful, it happens, well, the
standard thing to say every 18 months, the compute power available for the same amount
of money doubles.
Twice as much.
And so that means our phones are working better each year.
And our laptops are more capable.
But also, you know, everyone making products, the network.
Yeah, so you guys, you guys are probably citizens of the network.
And the network will compete for your allegiances, just the way the nation states do.
What Canada does, all the other countries.
But your alleges may end up being more to your fellow digital, digitally enabled people,
people enabled by the network, by their phones.
So this is the thing that's happening now.
And my first take home message, that we are living in a time of exponentially accelerating
computation.
It's really dramatic.
I feel like I'm a little bit of a distance.
I've been doing this for 45 years.
So I can see it.
It's kind of slow.
I mean, I used to take two years for computational power to double.
Now, I've been saying it takes 18 months.
And some of my colleagues are saying it's just a year.
If it was to double.
But, you know, it takes a whole year, even if it takes a year, it takes a whole year
of where your computer is twice as big.
And it seems slow, right?
You say, I've got all these extra videos on my computer.
And I have to buy an extra disk or something.
But really, if that happens year after year, double in the year and three or two months,
and that happens for decades, it's on the order of 100 years now that it's going up.
And it will, it's every expectation that it will continue if not, I mean, a little faster.
So this is really special.
So the name for it, yeah, let's just hold off on the next bullet point.
I know a lot of you are really excited about it, most of all.
But it's this trend.
This is the trend.
This is Moore's law, computer power per dollar, increasing exponentially.
No end in sight.
And this is some data from Kershmer on AI.
So here we have years, back in 1900, up to near now.
And up on the other axis, we have computer power per dollar.
And so these are actually individual computers, like the original Mac in 1984, or the URD's points.
And then you can plot out other laptops and supercomputers.
Supercomputers are more expensive.
So if they make it more per dollar, more competition per dollar.
Okay, so your per dollar is on this axis.
But critically, this axis is a larger scale.
This is a linear scale.
I mean, every one of these marks is the same years, right?
Here, every one of these marks, the big marks, from here to here, here to here is 10 powers of 10.
Okay, so that's a factor of 100,000.
Okay?
And so because it's a log scale like this, if it was a straight line, it would be a really exponential increase.
So as I said, it seems to be slightly super exponential.
But that curve up is so slow, we can consider it like we're here and we're at essentially a straight line.
Okay, so what are we looking at?
We're looking at some sense, well it is exponential increase, at least exponential increase, computer power per dollar.
And exponential, it's really the way it's an explosion.
It's an explosion because every year it gets bigger and then the year it gets bigger by the same percentage.
It keeps doubling every two years.
So it's really, almost literally, what we mean by an explosion.
And, okay, so I know you've talked about the singularity.
You've talked about singularity?
One more time.
There you have it.
Mary in or Mary out?
Yeah.
Good enough?
L, E, N.
And G, G, O, O, N, I, L?
Oh, great.
What's that going to say?
Oh yeah, it's going to say, you guys have talked about the singularity, the singularity, right?
It never, maybe it's never properly defined.
But what have you been taught?
What different views have you gotten of the singularity?
Yeah, so you're saying it's a reference to the increasing growth of technology in our lives.
Is it an event?
Is it a time?
Is it a moment in time?
What is a moment in time?
It's like the time when something happens.
It's an hypothetical future.
So what does it mean?
It's hypothetical future, so it doesn't have a specific time.
So time in the future, so when it depends on how fast technology advances.
So when it happens, is that the subject?
Okay.
What is it?
So it's a situation where we have...
I have a question between humanity and technology that is such...
I think it's the time it's comparable.
The technological beings and the human beings are at some comparable level.
I think that's what you're thinking.
Good.
Any other notions?
Unless we're not...
Yeah, so one thing I'm trying to say is we get to decide.
What it is, you could decide it's the moment when AI overcomes human abilities.
But human abilities will continue to increase.
We'll get smarter when we have better education, better tools and better phones.
And we will continue to augment ourselves.
We will augment ourselves with our phones now,
but we will eventually be augmented by entangling things in our minds
and by drawing stronger connections between us and our machines.
Yeah, we get to decide what the similarity is.
But one...
So what I always...
What I always learned was similarity...
So there's this doubling of a year or two,
and you can't really totally anticipate what's going to happen.
And the point, of course, like this is to think about what's going to happen.
But we can't because, well, so many amazing things happen.
Machines will get more powerful.
People will change.
And maybe machines will compete.
Maybe there'll be our offspring.
Maybe there'll be our descendants.
It's hard to see the future after a certain point.
And so that, to me, is always one of the core meanings of similarity.
It's the point of the horizon,
by which you can't see further.
And when you try to predict what will happen.
Okay, but when you get there,
when you get to that point that you can't see past now,
when you get there, you'll be able to see,
because you'll be there,
and you'll be able to see a little bit further.
So it's the sense of the point that you can't see past.
Then it will receive.
And it will forever receive.
You won't actually get there.
Okay?
Another part of the sense is that it's an explosion.
So what I want to lead you to is the idea that maybe we are in that situation.
Because this doubling is happening so regularly,
there's a point you can't see past.
Maybe it's 2040 that you can't see past.
What will happen now?
What will happen in 2040?
You can't see past that.
When we get to 2040, we'll have an idea.
And so the explosion,
maybe this is the explosion,
and we are right in the middle.
And it's a slow explosion.
It will always be slow.
It will always be, it takes a whole other year to double.
The double will be a bigger,
in terms of absolute amounts that you're adding,
but it will still just be a doubling.
Maybe it will always seem slow.
I think the thing that is the singularity
is the explosion of computation and intelligence.
We are in the midst of it.
And so this is beautiful because it means
there's not this future thing that we're going to encounter.
We can encounter it right now.
We are encountering it.
It will just be more,
it will be more transformation,
more doubling, more increases,
more change.
And it will feel much like it does now.
So I think this is the story of our time,
exponentially increasing computation,
and it will be the story for the foreseeable future.
Anything that uses computation,
you've got ten times more valuable every five years.
So you know this.
You're working in different fields.
What fields are you working in?
Psychology.
Psychology?
You're in technology.
Psychology, sorry.
Psychology.
I was originally trying to do psychology myself.
I like that.
Neuroscience.
Neuroscience, cool.
Education.
Education, biomedical engineering.
Second, biomedical engineering.
Biomedical engineering.
So I think you know,
in your areas,
that computer power is having more and more effect.
But in neuroscience, they can measure more neurons
at the same time and record it.
And in biomedicine,
they can operate now with robots.
And they can manage the data
and search for the data to find diseases.
And I was like, what did you say?
I'm in education.
You're in education.
So you know that there's a big thing now
to try to make computer tutors,
like all these large language models,
to use those to be an individual tutor for each student.
And that's perhaps,
is that maybe the biggest impact of computer power on education,
would you say?
Or would you say something else?
I would say so.
Yeah.
Yeah.
But it's true.
I think all the sciences,
you know, really good physics,
the things that they can simulate on a computer,
are much more important.
The biology, the genetics,
psychology,
all that we can do with simulations
to model render processes.
It's a big deal.
All the sciences have changed.
And all of our business ordinary lives,
because we have computers,
we have laptops,
we can use it individually.
We can add lives data,
we can communicate to people around the world.
We're all transformed.
We'll continue to be.
All right.
Well,
sort of the theme
is computation and intelligence.
Now, today,
we have the intelligent things,
our people,
and they're really,
almost all of you.
We have systems that use a lot of computation,
like large language models,
or like alpha fold.
You guys know,
do you guys,
can we have a fold?
Yeah.
Yeah.
Figure out all the proteins
that are known to humanity,
how they fold up
in three dimensional space,
how this changes
all the research possibilities.
That's not intelligence.
I mean, look,
I want to,
I'll work towards a real definition,
but I would just like to urge you to think,
maybe that's not intelligence.
I mean,
it's called AI,
but I think the way we use,
we would use the word AI nowadays.
It just,
if somebody uses a lot of computation,
they call it AI.
But,
I think we should mean more than that.
We say intelligence.
I think, yeah,
people are intelligent,
alpha fold is non-intelligent,
but,
yeah,
I want to distinguish DC,
because like now we're,
so what about my take home message here was,
we're living in this time
of exponentially accelerating computation,
and we're entering the time
of exponentially accelerating intelligence.
You're just entering that.
And as,
both of these changes,
the greatest,
both of these
changes will lead to
changes in what we do,
and transformation of ourselves.
The greatest ones will come from
the intelligence,
accelerating intelligence,
because we will really make the things
that are comparable to ourselves,
but without biologically replication.
And then we will be becoming,
we will be becoming
the things we are creating,
we will be becoming
even more intelligent
than the current humans are,
the current people.
So these two transformations,
recent computation,
recent turns,
are closely related,
sure,
but,
in that way,
because of this,
they're easily confused.
And what I want to propose to you,
the difference is that
intelligence,
this computation plus
some notion of goal,
or purpose,
or agent,
is something,
yeah, so,
that,
those are the general points
I'm leading to.
Okay?
Okay, so,
computation-powered machines
substitute for the computation-powered people.
That's sort of what's happening.
You know, we use people,
various,
the jobs that people do,
we use them for their perception,
motor control,
prediction abilities,
search abilities,
optimization abilities,
and until now,
people that are cheapest,
cheapest source of computation
in the sense that now machines
are, in some cases,
find greater achievement in optimization.
And so here are some of the successes
over the last,
maybe, 12 years.
The oldest one is when
machines invest in the phase of jeopardy,
and,
not long after that,
in 2012,
we worked with
which transformed our
speech recognition
into a natural language question.
Large language files,
in particular,
you know,
you probably use them more than I do.
They do all kinds of,
they can appear to speak
and understand.
And,
and the generative
methods for generating pictures as well,
they're impressive.
So, all those things
are impressive.
I would not put any of them as intelligence, though.
I would count as intelligence
when they used deep reinforcement
values to play Atari and Go
and all the other games.
And in these cases,
they,
the AI had a goal,
the intelligent system,
the computational system had a goal,
which was to win the games,
and it adjusted its behavior
in order to continue winning.
So,
that's what I want to say is the essence
of intelligence is
being able to pursue a goal.
And,
when you use, like, the large language models,
you may,
they may appear to have a goal,
but it's really, really true
that they don't have a goal.
They're just people,
they're saying the same thing,
and people would say,
they're half-stead
in similar situations.
They have no sense of trying to,
they're not,
certainly they're not trying to manipulate you,
because they don't have any goal,
except for, perhaps,
the designers could have a goal.
The designers,
the people that were designing that,
would be trying to manipulate you,
but the machine itself doesn't have a goal.
Okay, so,
maybe,
maybe it is time,
you know,
maybe it is time for definition.
Okay, so the definition,
here's some definitions of intelligence.
Well, it's not,
first of all, it's not really definition.
Maybe it's the fact,
it's the most,
it's claimed to be
the most powerful phenomenon in the universe.
That really occurs a while.
And,
yeah, just like,
this,
this, this idea,
this is, this is crazy.
The most powerful phenomenon in the universe.
It's claimed to be the intelligence,
like, and we, we people,
are the primary model of intelligence.
The sort of thing that we,
people, and our intelligence,
is the most powerful thing in the universe.
More powerful than, you know,
super-doers,
or dark energy.
Does that make any sense to you guys?
Okay.
Yeah?
Does it to you, Eileen?
I mean, yeah.
I, I would agree.
In what sense?
I mean,
everything makes up intelligence.
If we talk about black magic,
there's intelligence in that.
Uh huh.
Everything makes up intelligence.
So it makes sense that it's most powerful.
So, you know,
one thing,
I like to think,
if it's more powerful than
a super,
a huge explosion
in the,
in the sky,
I think it's saying that,
that people,
or, or, or intelligence is,
which maybe came from people,
will someday,
um,
like be moving the stars around.
We will, over time,
we've only been here for,
I mean,
maybe a hundred thousand years,
for people again.
If you give us a billion years,
we can become quite powerful,
uh,
physically,
uh,
all the time,
and we,
we can,
be in the force
that moves the stars around,
and we range
the universe,
the atoms,
and the universe
to be,
to,
to our liking,
as a group.
Um,
so,
when we think that way,
we're so,
so to think
in a little bit arrogant way,
right,
we think we,
intelligence,
would be so powerful,
because,
um,
well,
we're saying that
intelligence is,
is quite a thing.
And,
so I think we should mean,
we should,
by the word intelligence,
intelligence is just a word,
and we can mean
by anything we want to,
but we should mean,
by something that could be
this powerful.
Okay,
and,
what I'm proposing
is that it's goal,
that's the ability
to have a purpose.
That's the power
of the thing,
and that's the power
of the phenomenon.
The phenomenon,
if you look around the world,
you'll see there,
there will be things,
like people,
and animals,
and maybe new plants,
that appear to have goals,
that are well thought out
in terms of goals,
and,
and that is the phenomenon
that we observe.
Um,
so,
with James,
and,
the double,
he's the original psychologist.
This textbook was 1890,
and,
and,
and he was the first one
to talk about city goals.
When you talked about mind,
you didn't use the word,
you didn't use the word intelligence,
you used the word mind.
You said mind is attaining,
the hallmark of mind,
is attaining consistent ends
by variable means.
So,
means you change what you do
by what you want.
The essence of having a goal
is to vary what you do
to get what you want.
Um,
yeah,
and,
if you go to the AI researchers,
the most famous,
one of the top ones,
well-defined John McCarthy,
he finally defined,
an offered definition of intelligence,
as the computational part
of the ability to achieve goals.
So he's thinking the way I am,
I probably learned from him,
actually,
um,
but it's the computational part
of the ability to achieve goals.
Um,
and I might say,
myself,
I might say the computational part
of the ability to predict
and control the environment,
to control the data,
designed to be a computer scientist,
in terms of data going back and forth
to the human world,
to try to understand it,
to control and predict it.
Okay, so,
nevertheless,
you know,
so that's,
that's those kind of definitions
that I like,
but I want to just
talk a little bit,
just want to be
explicit about,
people are using the word
differently nowadays,
uh,
they often think intelligence
is mimicking people.
Um,
as an AI seeks to reproduce
behavior that we would call
intelligence
if it was done by people,
which is the kind of thing that
AI textbooks often say.
And,
I'm sure you guys
have covered the Turing test,
yes?
Yeah,
the Turing test is about mimicking,
yeah,
well,
it's about
a computer and a person
are put in,
in sealed rooms,
and you,
you talk to them by,
by,
by,
text chatting,
and you try to figure out
which one's the person
which one's the machine.
And so that,
that Turing test is,
you know,
can you make the machine
behave like a person
or mimic a person?
So,
this is a,
a prominent meaning.
I'm going to say,
I'm going to discourage you from it,
but I have to recognize
that it is,
it is why do we use,
um,
yeah,
it is machine learning,
supervised learning,
the task is to label,
say,
pictures of the same person
and label it,
and the chat,
gpt is,
is tasked to
generate text
on the person.
And so,
we,
we sometimes say,
this is our intelligence,
but notice,
none of these things,
um,
we're referencing to goals.
We're just saying,
behave like a person
and mimic a person.
Again,
again,
so just to cut to the chase,
my whole point is to say,
this is
an insufficient meaning
on intelligence,
because,
if you're just able to mimic people,
you're not going to become
most popular
from out on the news.
You're going to be able to fool
people in,
in your person,
but you're not going to
move the,
move the stars around
and you're not going to become
a really powerful force.
Um,
the universe,
people may,
but some of it's,
it's not going to be
powerful in that way.
You have to get with
the real thing
that people are doing
that,
that makes people powerful
and they are intelligence powerful.
Okay.
Well,
these are two definitions
of intelligence.
It could be mimicking people
or it could be,
that is a cheated goal
of computational art.
But we will
cheat rules.
And then,
so we just put those
on the table
and talk about which one's
better.
And now,
you know,
I've sort of given up
and changed the way
people use words.
You know,
I mean,
I'm facing the fact that
the world,
you know,
it's going to call
chat,
GPT,
AI.
It's a shame,
I think.
You know,
maybe that's why we have
this other term,
artificial general intelligence,
AGI,
because we've got to
distinguish it,
but it's just like
that.
And you should keep
in your mind
that there are these two
things,
mimicking people
whereas a system
that has agency
chooses actions
and is trying to act
to achieve goals.
Okay,
you know,
maybe now I should jump
to something that moves
around.
I do have
one sort of video
thing in there somewhere.
So,
yeah,
this picture,
this is a very simple
system,
but it does have agency.
And so,
I'm going to
amaze,
there's a start
location,
there's a goal
location,
and our little agent
is this,
this square,
and if I
touch this,
it will start
moving around.
Here,
it's moving around
because it has
actions,
it can go,
take a step to the right
or to the left
or up or down,
and of course
it takes a step
and runs into an
obstacle,
and it doesn't move,
but
it doesn't move,
and when it
moves over to the left,
it's going to turn
and it's going to
move around
and it's going to
move into the
middle half
of the goal.
What you're
seeing in these colors
and in these arrows,
you're showing,
it's showing what's
going on in the
agents head.
The real
world doesn't turn
green,
but green means
that things
have a good state
because it leaves
close to the goal.
And the arrow
is indicating
how good it
thinks each one
of its actions,
which one
things is best. Now the goal has been removed from where it used to be. This is a new location.
It goes back where it used to be and try to retain the goal, search it for it. And so
it's for perception of how valuable it thinks those states is gradually going down. It doesn't
just stumble on the goal, you know, the value of the states with the goal come up. Okay?
Have you got the idea? It's important to note that these states, these grid cells are just
unrelated to each other. They don't really have a spatial location. They're just, so
for example, if it's here, you might notice that here it's in state 14 and if it goes
to action number two, it ends up in state 34. That's how it's like this. It's numbered
state to numbered state as a consequence of the numbered action. Okay? Now this system
learns a model, so even though it's here, it finally discovers the goal, these states
will, it hasn't been here yet, but it knows it's going to go up because it's being a form
of reasoning. It says, if I was there and I took the action that we would call out, it
says now I end up in this state, which I know is a good state. So it's doing the primitive
things, it has a model by trial and error, like instrument learning and psychology, and
it's also making a model of the world and doing reasoning, what we might call reasoning,
which is using the model to figure out in advance what the right actions to do are. So
this system, I'm comfortable saying it as a primitive kind of intelligence and it has
the ability, it has a goal, it has the ability to achieve its goals by interacting and learning.
And this thing that's running in real time, you can just interact with it where it's
about to get attracted to it and see what it does. It's kind of like a player. So, yeah,
I'm just going to put this up close. So, now I want you to ask yourselves, there's the
little guy. Do you feel sorry for him at all? You do, because you get the sense that you
strive to do something and the world is going away and you say you can't possibly succeed.
So, I think that's actually kind of a deep thing when we impute agency to things that
behave in a certain way. And that's reality, that's not an illusion, unless you want to
say all of reality is an illusion. So it's a perception or an appearance that's useful
for us to understand the systems. Okay, so there's a system that has a goal. So, when
I say chat CPT, it doesn't have a goal. I mean, in the exact same situation, it will always
do the same thing. It doesn't pay attention to what you do in terms of influencing what
it does. You hear about that. Yeah, so it doesn't, it's often described as a learning
system. No networks are often described as learning systems. But when they're known
the way of using learning neural networks today, they are all learned in advance of
the use. So that they receive a big train set. They come crawl all over the train set
and extract information from it. They learn from the train set. And then once they're
put out in the world, they no longer learn. They no longer learn. Chat CPT no longer
learns. Alpha fold is built with great effort. But once it goes out in the world, it's no longer
learns. So that's a shame. And I think you've seen in this example how it's, it's maybe
important for what we mean by intelligence. If you continue learning, so if the world
changes, you can adapt to that change. Okay. What's next? Do you guys have any questions?
So if they allow them to learn, they do bad things. We have something that's called
catastrophic interference. The new thing that they learn interferes with everything they
learn. And then they're catastrophic way. And so, so that's not an effective way to
update them. Yeah, so like, as you know, a large language model might cost literally
$100 million to train it. $100 million to train it. And a large data set, maybe most
of the data on the internet. And then if, you know, they wait a week or a month later,
they have more data from the internet. And they cannot update the existing model. They
throw the existing model away and start all over again with, you know, with the extra
weeks data and all the old data put together and do it again, do the one, the one time
learning. And then they, and they have to do that. Every time they want to update it,
they have to do it on the $100 million process of training it. It's actually a giant
problem. And it's a giant opportunity for someone who wants to propose, you know, an
improved learning algorithm. They can be updated continually. And it's also a research problem
that many people are working on. I work out of my group. And other people out of my group.
What is this group? Well, you probably saw it flash by here. Here is most of the group.
Here at U of A, the reinforcement learning and arbitration intelligence group. So it's
actually, you know, it's hard to get all the people together at one time. Now there's probably
three times as many people. The names here are, like, this is my name. And these other
guys are guys like me there. Our professors at the university there's like 10 or 11 of
them now. And so you may remember Patrick. You've seen Patrick in his course. Has anyone
else taught in the course? No, not a lot of us. These are the guys in the reinforcement
learning and arbitration intelligence lab. Reinforcement learning is just a quote to
AI that emphasizes learning and trial and error. And it's based on psychology. It's based
on classical traditional ideas. Exactly. Very nice. Yeah, so this is me. These are our
PIs in the front. There's Patrick in the middle of them. Adam is the head of Amy. You guys
know what Amy is? The Alberta Machine Intelligence Institute downtown. You know that we have a
building that is dedicated to it. The Amy building. It's at the center of AI outside
of the university. So we've been doing this for a while. And you should know that Alberta,
Edmonton, U of A is one of the leading places for AI. Okay, let's start with something that's
unambiguously true. There are three AI centers in Canada. Canada has a national program to
support AI. And there are three centers in Toronto and Montreal and Edmonton. Now, Montreal
and Toronto are certainly more focused around supervised learning, classic neural network
deep learning stuff. Whereas Edmonton is more focused around reinforced learning. It's
our specialty. Everybody should have a specialty. And reinforced learning is more based on continual
learning and our goal holds. And so, I don't know. Yeah, we're good. So actually, in reinforced
learning, U of A is not just the best in Canada. We're the best in the world. And I say that
with some regrets, but because I have also written a textbook on reinforced learning. I
tried to promote everyone to learn about reinforced learning. But anyway, it's much more active
here at U of A. We have these 10 faculty members there doing reinforced learning research.
You won't find another university in the world. We have 10 faculty doing it. Yeah, so I guess
what I've just told you, I told you maybe that it's pretty close to arrogance. I've told you
that I think goals are essential to intelligence and to real intelligence. And I told you that
the University of Alberta and this group that I have founded is now the world's leader
in this area. So I kind of like talking my own book, I guess. But I think it's true. I still
think it's true, even though it's sort of important to my own benefit. Now, to balance that
potential arrogance, let's have a little bit of humility and let's recognize that the world,
Alberta is not the world's leader in applications of A. All of these that you've heard about,
they're so exciting. Large language models. Go back to your list. Which ones of them are,
this is it. It's not something I've exhausted or anything like that. But if we look at this bullet,
these two bullets are all about neural networks and large language models. A lot of these game
things are absolutely reinforcement learning. For example, you still remember AlphaGo where
AlphaZero, which became the strongest players on a whole host of games. And those were based
on reinforcement learning and they were also based, they were led by graduates of the University
of Alberta. David Silver at DMI, we did AlphaGo and then AlphaZero. Poker, it was led by my
brother. He was one of the people who was here. He was in the picture of the microphone. Atari
was led by a master's student from the U.A. He went on to work in Interacto. He worked
in DMI. Starcraft was also done in DMI by David Silver and other people. Racecar driving.
So this is where they have very, very realistic simulation of racecar driving. It's a Sony
game. It's called Gran Turismo Racecar Driving and it's super realistic. And they got the computer
to play that in an appropriate, in a way that's appropriate for a human competition. And it
was meant to play to drive a car and see the variables. In generally speaking, in these
cases, there's no game specific knowledge. It's just like the rules of the game. And
you have to be able to face different rules. AlphaFold, it's just competition, kind of
rules in there. Self-driving cars. They don't really exist fully yet. They don't really
have goals fully yet. Anyway, it's much like reinforcement learning is still setting up
for the teacher right here. Our AIs will have goals and agency around them for the applications
that we are probably now.
Do you have any more questions? What's the next thing to say? What's the next most important thing to say?
Does that mean every time you want to teach it? Because it's not learning while you're driving.
So would that be done through a software update which is a completely new model that allows
it to work? Do you think that's the most efficient way to do that?
It has some advantages because you can share between vehicles, to the extent that vehicles are
similar, but often the vehicles are not. They are reducible and are definitely. And also,
if you think about yourself driving, you often need to adapt to what you are today,
about how the snow is today, or how the sand is today, or how the people are driving today.
So you need to adapt to a particular thing you're into now. You've got some ability.
So I guess there are sort of two ways to do the self-driving practice. One is like a very
engineering model, three-dimensional physics, you know, masses and velocities and meters
of distance between things. And you do it as a physics problem almost. Try to make sure
there will be no collisions. Whereas there are things that aren't just physics like the
other drivers. But you try to do it in an engineering way. What do I mean by engineering
way? I just mean, well, engineers, they think about a problem and they get it into their
minds. And they figure out ways to behave that will be safe and productive based on their
understanding of it. And they just build the rules or the behavior into the machine. Whereas
some manufacturers now are starting to use no networks for the whole thing. No networks
to make the predictions about how the driving situation will unfold, how the other cars
will move over the consequences of their various actions. Yeah, I think some are adopting this
approach much more than others. Elon Musk in his Tesla is apparently going aggressively
into a neural network model of the physics, the dynamics of the world. He has much more
data because he has almost Tesla cars. Like the data on people, how cars interact with
the world and all that to be fed into a large network. And maybe end up with a better model
than you get from physics. Maybe that is almost at the heart of it. What's going on in science
and do you model things based upon a human understanding? When I say human understanding,
I mean like the engineers or physicists understanding. Or do we learn the laws of physics more in
the way an animal does? An animal doesn't know differential equations and doesn't know
what, how to integrate things. It just sees things and sees what happens next and tries
to predict. It's an interesting challenge. Okay. Any other comments, questions?
Yeah, I wanted to cast our minds back to around the time that this course began, Jonathan
Schaefer was teaching in it and in his second teaching session ever, he talked about medical
jargon and that he would have become a physician but he was so offended by the extra words,
the new language that you supposedly have to learn to practice medicine. Maybe you don't
really have to learn but it's our custom. And because of jargon he decided that computing
science was a better career than medicine. So years later, like last year, I got interested
in the problem of jargon. So just think now about AI on our phones or whatever you want
to call it, the way our phones react. So what I would argue now is within a couple of years,
the problem of medical jargon will just melt away, not because of any specific program applied
to it, but because if you're in a third world country and receiving information about medicine
but you don't have any physicians or nurses around, this is going to be frustrating. You're
going to ask your phone for help and this is a simple translation problem that the phone
will say, well, I can help you. You want me to translate between medical jargon and regular
speak and you'll say that's right and the phone will say, well, I can do that. So that's
what this medical statement means. And so suddenly, without any specific project or anybody
describing it, the problem that Jonathan Schaefer was telling me about in 2012 that turned him
off so he read it to computing science instead, will be gone. And I'm thinking probably other
things will be like that too. You can say it's AI or computation, but computers, machines
will change the world in a positive way in ways that humans like without any intentional
program ever put into place. So does that make sense to you? Do you think that that might
happen that we get rid of medical jargon without a single project anywhere designed to do that
but just people use it in their phones, right, in an actual way?
I can see how it might happen that it might really, I don't know, totally get rid of jargon,
but it might really help a lot. And jargon is a big problem. It's very present in AI,
just things, there's so many things that people know about, they know the names of them,
they don't really know them. Transformers, neural networks, the names of all the algorithms,
people know the names of the algorithms. Recently there was a group plow about this new algorithm,
Qstar, it was coming out of a meta or something like that. When you Qstar, they're all saying,
what is Qstar? I don't know, we don't know what it is, but we're so excited about it.
Yeah, but hey, you're right, because the large line of the plow is they seem to be good,
you can ask them anything, you can ask them, well, could you explain this in simple terms?
They will try to do that, it'll be something like that. You never have time to ask your doctor
to explain something, but AI, large line of the plow, they need to be patient,
they need to explain it for you in different ways. And it does, even before the large line,
I just feel like Google, and I can go to Google and say, what does LFG meant?
I can just ask it what all the cool kids are using, some acronym or something,
and it tells you what it is, it's just easy to find that out, whereas, what is jargon?
Jargon is when some subset starts to use words in a certain way, and they use it,
I believe it's really intending to obscure, so that we in the in-group know what this means,
but you don't because you're not in the in-group. And that's what it is in science,
and that's what it is in social police. And, yeah, I felt a long time ago, now we can use Google,
if I want to know what some acronym means, that just people use it, I can usually figure it out
just by putting it into Google, and even more so, maybe with language models.
So, the other thing that makes us not very proud to be human is that the large language models,
whatever you think of them, seem to handle empathy, equity, that sort of thing,
better than humans. Even trained, you know, physicians and therapists are not as naturally empathetic
as the large language models are. I think this is going to be, the large language models are a bit problematic.
Yeah, in particular, right now, like, Gemini 1.5 is going from Google, at least.
And it's too, it's too inequity. Yeah.
It's too woke. It's too woke. You know, I asked for pictures of the founding fathers,
and turns out they were all, you know, black people and women, you know, like most of them.
And I was a very white person among them. Yeah, Google is lost.
I'm 15% of its value because of this disaster of releasing Gemini 1.5.
And so it feels like, I mean, it seems absolutely clear that the people who designed that language models
and also some of the other language models, that they are too woke and that they are not just trying to,
you know, we think Google is just giving us the facts of the internet.
But it turns out, at least in the large language models, Google is also trying to change people's views,
not just reflect, but what is. And this, if you think about it, is really problematic.
Right. Why doesn't it tell us the truth, then?
Yeah, what?
Maybe for good reasons, but if they can do it for good reasons, they can also do it for bad reasons.
No, I think one of the most striking things is, if you look at liberal democracy worldwide,
it is becoming less successful every year, right?
But in, like, open AI products and Google products, that's the philosophy they're building in.
So when they find two of these things, it's a liberal democratic point of view.
And so, surprisingly, people using a lot of AI products are getting that bias.
Well, you may say, but that's a wonderful bias, but that's just one person's opinion.
You know, other people may not feel that way.
Yeah.
It's very concerning.
Yeah, no, I think that's been part, I mean, it was even part, you know, Dolly Too, a couple of years ago.
Dolly Too, if you would search for a person, you were very likely to get a person of color and female.
Because they were trying to create that balance.
Yeah.
And, you know, I'm interested in pig-to-human transplants.
You couldn't use the word pig because they decided that it's an, you know, insolving term and so on.
You know, they throw you out of the program if you can't ask them to use pig.
And it's possible that this has been going on all along.
Well, there was no large 90 pounds.
They were always trusting who would give us.
They always had the option of filtering, you know, weighting, shadow-daining.
And then we know that Twitter absolutely happened.
And so now, and now Lizzie Langlust, you have different views by Langlust,
but he actually, we should recognize that he is a proponent of free speech.
And that the mainstream media is hate-simple.
And they describe it in very negative terms.
You know, they have any favoritism, maybe they also disfavor Elon.
And his great sin, as far as I can tell, a lot of the great kinds of companies that do amazing things,
his great sin is he exposed the lack of free speech or exposed the bias on Twitter.
And, yeah, so I think we have to support him on that.
We have to say we want free speech.
We don't want that bias information.
One thing that would probably amuse you all is there was a student here, Lachini Batt,
who is now a third-year medical student at the University of Toronto.
She interviewed me about Elon Musk.
I gave her Elon Musk's biography at the time, and she read it, and she interviewed me.
Following that interview, you know, Google has these algorithms of how they pick the next video.
So because a lot of Elon Musk's ideas came to him at the Burning Man event,
you would go to a Kim Sola's video, and the next video would be a Burning Man video.
I've never been to Burning Man.
I don't think I would survive it.
I'm sort of unlike the people who go, but that still happens.
You can still find this strange linkage between Kim Sola's and Burning Man, all on account of that Elon Musk.
Well, I worked for Google and DeepMind for a number of years,
and as an expert in reinforcement planning, I was invited and asked to contribute to how this decision is made.
You know, they want to retain viewers, show them something interesting.
So I don't know how they're done now.
I don't know in particular whether they learn online or in offline, as we talked about.
But they do it some kind of way, it's a kind of testing.
I like to think it's neutral. They're just trying to retain eyeballs.
They're just trying to keep people enjoying, or at least continuing to interact.
Yeah, I like to think that.
But it looks like in addition to that, they're also trying to, in many cases, they're also trying to change viewpoints.
Bill?
Yeah, so this is a general fear of the future, is that AI, and maybe it's not AI, it's just the tech companies today.
The tech companies today are not really representative of the middle of the road views on things.
They're in some particular way, maybe it's a good way, but anyway, they're just a very particular way.
And so the fear is that these control the narrative.
They control what subjects are considered and what subjects just kind of disappear.
And this is a dystopia, when the information and views are not evolving naturally, but are controlled by a small group of people.
What's sort of fun is David Wood, who's the chair of London Futures, did a holiday event around Christmas about AI safety.
There had just been this Bletchley Park meeting, and sort of a follow up to that.
And I was one of maybe 11 or 12 speakers.
And this kind of thing came up a lot about biased views.
And for whatever reason, they tried to explain the technological reason why the video recording didn't work, but my presentation is the only one you can find.
Can't find any of the other presentations.
I didn't do anything, it's just, that's how it turned out.
They had this thing with 12 speakers, and you can only find a video for one of them, and it's me.
You tend to be pretty positive.
Were the others, you know, questioning?
No, no, many of them were talking about the fact you can't trust anybody, you know, AI is going to kill us all, all those kind of things, in various ways.
So those were removed from the points of view from this happy Christmas occasion that you can't find anywhere.
So now related to this, I know that, you know, you've talked about the AI, what I like to call the AI doers, those that think that we should need to be afraid of AI.
And I think that's very much what we're doing.
And I tend to see this part of the same thing.
Russell Graham is a way of saying this.
Authoritarianism, the new authoritarianism will not come like Jack Boole's violence.
It will come in the language of safety and care and convenience.
Right, we're trying to protect you.
We're forceful about it, though.
We will protect others who provide you with your saying as offensive, so you won't allow you to say it.
Right.
So I think that will, really, this isn't a struggle, but I'm an optimist, and I think the people trying to do that will lose out.
It will be unmasked the way they were unmasked through Twitter and the area.
We're sort of unmasked at Google for producing Gemini and being sort of unsuccessful as it was too obvious.
It was too obvious that it was trying to help people's views.
So this, you know, I started out by talking about you as what is your allegiance?
Is your allegiance to...
that there will be struggles over what you will identify most with?
Whether it will be your country, which is maybe becoming more authoritarian,
in the name of safety and care?
Or will it be we are more in common with other young people than other countries?
Also modern, digital, tech-savvy people?
Yeah, I read this one off. His name is Balaji. Balaji is from the Boston.
He says that the struggle, it was basically three.
One is the woke state, and one is, which includes things like the New York Times and the media,
and one is like the authoritarianism in China, the Communist Party.
And those are at odds right now.
The other is that China and the US struggle with each other for dominance.
One empire is fading and the other one is rising.
If there won't be a war, not at all.
And the third is the network.
There are a lot of people that are just empowered by the free exchange of ideas on the network.
Yeah, and I think Africa...
As a citizen of the network.
Yeah, Africa that we don't think about much.
But I think young people in Africa are finding a voice,
and I said a lot of people are sort of interested in that,
because they're not part of the power groups that they usually encounter.
So in my whole career, the most consequential thing I ever did, I think,
when you look at the history, it was in the year 2000.
We had a meeting in Nairobi, Kenya, and many of the people that I met are still leaders now.
They were young leaders then, and they're sort of mid-sage leaders now.
But it's really cool that a lot of the things we predicted then have happened,
and the plans we made then seemed to have kind of played out in a positive way.
A lot of the other meetings that I went to, you can now say,
well, that looks like a complete waste of time, or it was good for sightseeing,
but you accomplished nothing.
But that meeting in Kenya in 2000, we really did something.
And I think it's sort of worth thinking about that.
The areas of the world that are not in the news as much could really change things in the future.
There are news of them are suppressed.
Yeah, I think just as many things happen there, we don't hear about it.
There's one last thing I'd like to discuss in today's, today.
So, like Kim likes to say, likes to point out that I have gone to various meetings historically,
and made the case that the AIs are more like our kin or our descendants,
and that we should be open to them, rather than worrying that they're going to take over and kill us all.
And I have done such things, but it's not, it's not.
So I'd like to bring up another name, which is Hans Morley.
Because he sort of was making this pitch long before I was. I really learned it from him.
And so I think I have a slide on that.
Here's some more.
Here's my slide of the universe.
Here's my slide of the sentiment.
Yeah, so this is quotes from Hans's book in 1998.
Here's another one, 1988, that he was saying this for many years.
Yeah, bearing cataclysms, I consider the development of intelligent machines a near term inevitability.
I consider these future machines our progeny, but who's us to give them every advantage,
and then to bow out, and you can no longer contribute.
That's sort of, it seems like a very humble and insured attitude to the future.
Yeah, here's a little, this is a full quote.
Near term inevitability, rather quickly they would displace us,
because they would just be better, and it doesn't have to be in a rude way.
It's just, there are successors.
So you know, have to be alarmed.
You can say these future machines are progeny.
They're mind children.
They build our image and likeness ourselves in more point of form.
Like we all hope, if we have children, that they become smarter than us and more capable than us.
And just, are the AI's, should we consider them part of us?
Should we consider this the opposite of us?
That is our choice, which way we think about it.
So like these two slides, I think the students should be required to know about them on the exam.
Because from the beginning of the course, we've required them to repeat back what you said in January 2015
at the AI safety meeting.
And this is sort of an extension of that, or gives some of the background.
So I think it should also be a required part of the course.
Now this struggle, I think the, so I like to say, what's actually happened, you know, if we look at it sociologically.
The AI, so I would say the AI doomers have sort of won.
Because if you just read the paper and you read the meetings that people have, you're reading what people think.
You're concerned that the AI will somehow lead to some, you're a versatile catastrophe.
Is it common view?
I would say people can't really articulate why they feel this way, but they do feel this way.
So they've won sort of PR war to make people scared of AI.
And that's a great shame.
I was meeting, and that was some of them, just this last weekend.
And I said, well, you know, you guys have really won. This is what people think.
And they say, think of AI, they think parallel.
They think we're going to, if we're not going to be killed by them all, we're at least going to lose all our jobs.
And yeah, so I said to them, I said, well, I wish it wasn't true, but I think you guys have won the PR war.
And that's the fellow I said, no, not at all.
He thinks that I've won.
Those who are at ease with these developments have won.
Because they haven't stopped AI.
Their notion of success is it ends, and it never happens.
And the world does not develop AI.
But you know, there is a sort of Elon Musk part of this, too, which is the first one.
You know, there is one of those that is fearful.
Yeah, yeah.
And at the same time, he's doing it.
He's not suggesting attacking data centers.
And so I think his prominence will probably somehow keep the world from a kind of conflict where they attack data centers.
It just won't happen because, you know, Elon's there to sort of put out that particular flame.
He has a very interesting case.
I think his son's views have evolved.
You know, he's accepting now that it's going to happen.
So now he's made XAI, which is an open AI company, and originally he made open AI.
I mean, open AI was, it's going to happen.
So let's do it in an open way so that it doesn't become controlled by just a few.
And then open AI became closed AI.
And they are the few that are trying to keep it to themselves, Microsoft.
So now there's XAI.
XAI has its large language model called rock, which is much more open and truthful than the woke ones.
And it's like Elon now thinks, yes, AI is going to happen, and we just want to make sure it's done openly.
And then we want to make sure, you know, he wants it to be done well.
In his view, a well-structured, a well-goaled AI system is one that is curious about the world,
mainly just wants to come to understand the world.
Its goal is not to turn people into lips or anything like that,
but its goal is to understand and has humor and is not quite as serious as the woke language models.
So I think he's now in the camp that, I think he's making it, he's acknowledging,
so he's sort of famous for being someone who was afraid of it.
He's always quoted as someone who said AI is like somebody who's even and is so scary.
Now it's evolved to more realistic and then more productive on view.
I think that's all really good, a good model for how people's views might evolve.
So I want to bring that up and invite you all to be optimistic about it.
What is the main thing that's happening?
The main thing is not robots are rising up.
The main thing that's happening is we are understanding ourselves, we are understanding minds.
That's the big thing and it's just understanding the way we work has profound impacts.
But we have to believe, I have to believe, that it's going to be good if we understand ourselves.
We have a better opportunity to have a world peace.
We have a better opportunity to reproduce what's good about people.
And it's like the most profound scientific intellectual problem ever is to understand our own minds
and how you can make them more effective.
This has been a great teaching session and I think it's kind of cool the way your intentions and my intentions are aligned.
So yeah, that's good.
We're both very positive about it.
So what about the students? Was it good for you?
I always hate the kind of session where the senior people talk about how fantastic it was that all the junior people look mystified.
No, it was nice to meet you both. It was kind of fun that you were both so passionate.
So it was like what did I say?
Yeah, we know each other so long and we live in the same part of the city.
So sometimes when I'm mowing the lawn, Rich will be jogging down the road and he'll stop and I'll turn off the lawn mower and we'll solve the rule right there in the lawn.
So yeah, that's another thing you're made out of.
Okay, that's great. Thank you very much.
