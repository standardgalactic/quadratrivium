WEBVTT

00:00.000 --> 00:03.000
to meet everyone. Maybe I can put everyone's name.

00:03.000 --> 00:05.000
Sure. My name is Richard.

00:05.000 --> 00:06.000
I'm Elian.

00:06.000 --> 00:07.000
Elian?

00:07.000 --> 00:08.000
Yes.

00:08.000 --> 00:09.000
Mariam.

00:09.000 --> 00:10.000
Mariam?

00:10.000 --> 00:11.000
Yes.

00:11.000 --> 00:12.000
Can we meet you?

00:12.000 --> 00:13.000
Yes.

00:13.000 --> 00:14.000
Mariam?

00:14.000 --> 00:15.000
Elian?

00:15.000 --> 00:16.000
And I'm Isla.

00:16.000 --> 00:17.000
Isla?

00:17.000 --> 00:18.000
Yes.

00:18.000 --> 00:21.000
Don't go too fast. No, I gotta remember all these things.

00:21.000 --> 00:24.000
Isla, Isla, and Mariam.

00:24.000 --> 00:25.000
Say it again.

00:25.000 --> 00:26.000
Elian.

00:26.000 --> 00:27.000
Elian.

00:27.000 --> 00:28.000
All right.

00:28.000 --> 00:29.000
Okay.

00:29.000 --> 00:30.000
Yeah.

00:30.000 --> 00:31.000
Yeah?

00:31.000 --> 00:32.000
Yeah.

00:32.000 --> 00:33.000
Okay. Please meet you.

00:33.000 --> 00:34.000
Okay.

00:34.000 --> 00:35.000
Okay.

00:35.000 --> 00:42.000
So, we are a few, and we can go whichever ways you guys are interested in.

00:42.000 --> 00:43.000
I'm interested in.

00:43.000 --> 00:51.000
And I got a bunch of slides, but there's only a few that I really want to present for sure.

00:51.000 --> 00:58.000
Well, I sort of have the impression that we've a little bit made history many times when

00:58.000 --> 01:00.000
you talk to her.

01:00.000 --> 01:06.000
So, anything that you think would be more likely to be making history is more interest than

01:06.000 --> 01:10.000
things that aren't going to make history.

01:10.000 --> 01:14.000
Well, I think it is a sort of a time.

01:14.000 --> 01:20.000
And I'm sure you're all young people, and you're only familiar with your own time in

01:20.000 --> 01:21.000
some sense.

01:21.000 --> 01:22.000
And you think it's just normal.

01:22.000 --> 01:25.000
But I don't think this is a normal time you're in.

01:25.000 --> 01:35.000
I think it's a time when, well, massively increasing computation is available to everybody

01:35.000 --> 01:38.000
into our companies and people make products.

01:38.000 --> 01:43.000
And this is really the only time like this.

01:43.000 --> 01:48.000
I mean, some sense, under 200 years ago, there was the Industrial Revolution.

01:48.000 --> 01:55.000
They discovered machines that, you know, like steam powered and physical work for people.

01:55.000 --> 01:58.000
No, but that really took a long time.

01:58.000 --> 02:06.000
Whereas now, with the computation, I mean, much more plentiful, it happens, well, the

02:06.000 --> 02:12.000
standard thing to say every 18 months, the compute power available for the same amount

02:12.000 --> 02:14.000
of money doubles.

02:14.000 --> 02:17.000
Twice as much.

02:17.000 --> 02:21.000
And so that means our phones are working better each year.

02:21.000 --> 02:24.000
And our laptops are more capable.

02:24.000 --> 02:30.000
But also, you know, everyone making products, the network.

02:30.000 --> 02:38.000
Yeah, so you guys, you guys are probably citizens of the network.

02:38.000 --> 02:44.000
And the network will compete for your allegiances, just the way the nation states do.

02:44.000 --> 02:49.000
What Canada does, all the other countries.

02:49.000 --> 02:57.000
But your alleges may end up being more to your fellow digital, digitally enabled people,

02:57.000 --> 03:03.000
people enabled by the network, by their phones.

03:03.000 --> 03:09.000
So this is the thing that's happening now.

03:09.000 --> 03:17.000
And my first take home message, that we are living in a time of exponentially accelerating

03:17.000 --> 03:19.000
computation.

03:19.000 --> 03:21.000
It's really dramatic.

03:21.000 --> 03:23.000
I feel like I'm a little bit of a distance.

03:23.000 --> 03:26.000
I've been doing this for 45 years.

03:26.000 --> 03:30.000
So I can see it.

03:30.000 --> 03:34.000
It's kind of slow.

03:34.000 --> 03:40.000
I mean, I used to take two years for computational power to double.

03:40.000 --> 03:43.000
Now, I've been saying it takes 18 months.

03:43.000 --> 03:46.000
And some of my colleagues are saying it's just a year.

03:46.000 --> 03:48.000
If it was to double.

03:48.000 --> 03:52.000
But, you know, it takes a whole year, even if it takes a year, it takes a whole year

03:52.000 --> 03:54.000
of where your computer is twice as big.

03:54.000 --> 03:56.000
And it seems slow, right?

03:56.000 --> 03:59.000
You say, I've got all these extra videos on my computer.

03:59.000 --> 04:02.000
And I have to buy an extra disk or something.

04:02.000 --> 04:09.000
But really, if that happens year after year, double in the year and three or two months,

04:09.000 --> 04:16.000
and that happens for decades, it's on the order of 100 years now that it's going up.

04:16.000 --> 04:24.000
And it will, it's every expectation that it will continue if not, I mean, a little faster.

04:24.000 --> 04:28.000
So this is really special.

04:28.000 --> 04:33.000
So the name for it, yeah, let's just hold off on the next bullet point.

04:33.000 --> 04:36.000
I know a lot of you are really excited about it, most of all.

04:36.000 --> 04:39.000
But it's this trend.

04:39.000 --> 04:41.000
This is the trend.

04:41.000 --> 04:47.000
This is Moore's law, computer power per dollar, increasing exponentially.

04:47.000 --> 04:49.000
No end in sight.

04:49.000 --> 04:53.000
And this is some data from Kershmer on AI.

04:53.000 --> 05:01.000
So here we have years, back in 1900, up to near now.

05:01.000 --> 05:08.000
And up on the other axis, we have computer power per dollar.

05:08.000 --> 05:16.000
And so these are actually individual computers, like the original Mac in 1984, or the URD's points.

05:16.000 --> 05:21.000
And then you can plot out other laptops and supercomputers.

05:21.000 --> 05:23.000
Supercomputers are more expensive.

05:23.000 --> 05:29.000
So if they make it more per dollar, more competition per dollar.

05:29.000 --> 05:33.000
Okay, so your per dollar is on this axis.

05:33.000 --> 05:37.000
But critically, this axis is a larger scale.

05:37.000 --> 05:39.000
This is a linear scale.

05:39.000 --> 05:42.000
I mean, every one of these marks is the same years, right?

05:42.000 --> 05:53.000
Here, every one of these marks, the big marks, from here to here, here to here is 10 powers of 10.

05:53.000 --> 05:59.000
Okay, so that's a factor of 100,000.

05:59.000 --> 06:02.000
Okay?

06:02.000 --> 06:10.000
And so because it's a log scale like this, if it was a straight line, it would be a really exponential increase.

06:11.000 --> 06:15.000
So as I said, it seems to be slightly super exponential.

06:15.000 --> 06:23.000
But that curve up is so slow, we can consider it like we're here and we're at essentially a straight line.

06:23.000 --> 06:29.000
Okay, so what are we looking at?

06:29.000 --> 06:38.000
We're looking at some sense, well it is exponential increase, at least exponential increase, computer power per dollar.

06:38.000 --> 06:44.000
And exponential, it's really the way it's an explosion.

06:44.000 --> 06:51.000
It's an explosion because every year it gets bigger and then the year it gets bigger by the same percentage.

06:51.000 --> 06:54.000
It keeps doubling every two years.

06:54.000 --> 06:58.000
So it's really, almost literally, what we mean by an explosion.

06:59.000 --> 07:07.000
And, okay, so I know you've talked about the singularity.

07:07.000 --> 07:09.000
You've talked about singularity?

07:17.000 --> 07:19.000
One more time.

07:19.000 --> 07:22.000
There you have it.

07:22.000 --> 07:25.000
Mary in or Mary out?

07:25.000 --> 07:27.000
Yeah.

07:27.000 --> 07:29.000
Good enough?

07:29.000 --> 07:31.000
L, E, N.

07:31.000 --> 07:34.000
And G, G, O, O, N, I, L?

07:34.000 --> 07:36.000
Oh, great.

07:39.000 --> 07:41.000
What's that going to say?

07:41.000 --> 07:47.000
Oh yeah, it's going to say, you guys have talked about the singularity, the singularity, right?

07:47.000 --> 07:51.000
It never, maybe it's never properly defined.

07:52.000 --> 07:54.000
But what have you been taught?

07:54.000 --> 07:58.000
What different views have you gotten of the singularity?

08:05.000 --> 08:11.000
Yeah, so you're saying it's a reference to the increasing growth of technology in our lives.

08:16.000 --> 08:18.000
Is it an event?

08:18.000 --> 08:20.000
Is it a time?

08:20.000 --> 08:22.000
Is it a moment in time?

08:24.000 --> 08:26.000
What is a moment in time?

08:33.000 --> 08:37.000
It's like the time when something happens.

08:37.000 --> 08:40.000
It's an hypothetical future.

08:40.000 --> 08:41.000
So what does it mean?

08:41.000 --> 08:45.000
It's hypothetical future, so it doesn't have a specific time.

08:45.000 --> 08:53.000
So time in the future, so when it depends on how fast technology advances.

08:53.000 --> 08:55.000
So when it happens, is that the subject?

08:55.000 --> 08:56.000
Okay.

08:56.000 --> 08:58.000
What is it?

09:02.000 --> 09:07.000
So it's a situation where we have...

09:08.000 --> 09:14.000
I have a question between humanity and technology that is such...

09:14.000 --> 09:16.000
I think it's the time it's comparable.

09:16.000 --> 09:22.000
The technological beings and the human beings are at some comparable level.

09:22.000 --> 09:24.000
I think that's what you're thinking.

09:24.000 --> 09:25.000
Good.

09:27.000 --> 09:30.000
Any other notions?

09:30.000 --> 09:32.000
Unless we're not...

09:38.000 --> 09:43.000
Yeah, so one thing I'm trying to say is we get to decide.

09:43.000 --> 09:49.000
What it is, you could decide it's the moment when AI overcomes human abilities.

09:49.000 --> 09:53.000
But human abilities will continue to increase.

09:53.000 --> 09:59.000
We'll get smarter when we have better education, better tools and better phones.

09:59.000 --> 10:02.000
And we will continue to augment ourselves.

10:02.000 --> 10:06.000
We will augment ourselves with our phones now,

10:06.000 --> 10:10.000
but we will eventually be augmented by entangling things in our minds

10:10.000 --> 10:16.000
and by drawing stronger connections between us and our machines.

10:16.000 --> 10:19.000
Yeah, we get to decide what the similarity is.

10:19.000 --> 10:21.000
But one...

10:21.000 --> 10:22.000
So what I always...

10:22.000 --> 10:28.000
What I always learned was similarity...

10:28.000 --> 10:32.000
So there's this doubling of a year or two,

10:32.000 --> 10:37.000
and you can't really totally anticipate what's going to happen.

10:37.000 --> 10:40.000
And the point, of course, like this is to think about what's going to happen.

10:40.000 --> 10:45.000
But we can't because, well, so many amazing things happen.

10:45.000 --> 10:47.000
Machines will get more powerful.

10:47.000 --> 10:48.000
People will change.

10:48.000 --> 10:50.000
And maybe machines will compete.

10:50.000 --> 10:52.000
Maybe there'll be our offspring.

10:52.000 --> 10:54.000
Maybe there'll be our descendants.

10:54.000 --> 10:58.000
It's hard to see the future after a certain point.

10:58.000 --> 11:02.000
And so that, to me, is always one of the core meanings of similarity.

11:02.000 --> 11:04.000
It's the point of the horizon,

11:04.000 --> 11:07.000
by which you can't see further.

11:07.000 --> 11:12.000
And when you try to predict what will happen.

11:12.000 --> 11:14.000
Okay, but when you get there,

11:14.000 --> 11:17.000
when you get to that point that you can't see past now,

11:17.000 --> 11:20.000
when you get there, you'll be able to see,

11:20.000 --> 11:21.000
because you'll be there,

11:21.000 --> 11:23.000
and you'll be able to see a little bit further.

11:23.000 --> 11:27.000
So it's the sense of the point that you can't see past.

11:27.000 --> 11:29.000
Then it will receive.

11:29.000 --> 11:31.000
And it will forever receive.

11:31.000 --> 11:33.000
You won't actually get there.

11:33.000 --> 11:35.000
Okay?

11:38.000 --> 11:43.000
Another part of the sense is that it's an explosion.

11:43.000 --> 11:49.000
So what I want to lead you to is the idea that maybe we are in that situation.

11:49.000 --> 11:52.000
Because this doubling is happening so regularly,

11:52.000 --> 11:54.000
there's a point you can't see past.

11:54.000 --> 11:56.000
Maybe it's 2040 that you can't see past.

11:56.000 --> 11:58.000
What will happen now?

11:58.000 --> 12:00.000
What will happen in 2040?

12:00.000 --> 12:01.000
You can't see past that.

12:01.000 --> 12:04.000
When we get to 2040, we'll have an idea.

12:04.000 --> 12:07.000
And so the explosion,

12:07.000 --> 12:08.000
maybe this is the explosion,

12:08.000 --> 12:10.000
and we are right in the middle.

12:10.000 --> 12:11.000
And it's a slow explosion.

12:11.000 --> 12:13.000
It will always be slow.

12:13.000 --> 12:15.000
It will always be, it takes a whole other year to double.

12:15.000 --> 12:17.000
The double will be a bigger,

12:17.000 --> 12:20.000
in terms of absolute amounts that you're adding,

12:20.000 --> 12:22.000
but it will still just be a doubling.

12:22.000 --> 12:24.000
Maybe it will always seem slow.

12:24.000 --> 12:26.000
I think the thing that is the singularity

12:26.000 --> 12:31.000
is the explosion of computation and intelligence.

12:31.000 --> 12:33.000
We are in the midst of it.

12:33.000 --> 12:36.000
And so this is beautiful because it means

12:36.000 --> 12:40.000
there's not this future thing that we're going to encounter.

12:40.000 --> 12:42.000
We can encounter it right now.

12:42.000 --> 12:44.000
We are encountering it.

12:45.000 --> 12:48.000
It will just be more,

12:48.000 --> 12:50.000
it will be more transformation,

12:50.000 --> 12:52.000
more doubling, more increases,

12:52.000 --> 12:54.000
more change.

12:54.000 --> 13:00.000
And it will feel much like it does now.

13:06.000 --> 13:09.000
So I think this is the story of our time,

13:09.000 --> 13:11.000
exponentially increasing computation,

13:11.000 --> 13:15.000
and it will be the story for the foreseeable future.

13:19.000 --> 13:21.000
Anything that uses computation,

13:21.000 --> 13:24.000
you've got ten times more valuable every five years.

13:35.000 --> 13:37.000
So you know this.

13:37.000 --> 13:39.000
You're working in different fields.

13:40.000 --> 13:42.000
What fields are you working in?

13:42.000 --> 13:44.000
Psychology.

13:44.000 --> 13:45.000
Psychology?

13:45.000 --> 13:47.000
You're in technology.

13:47.000 --> 13:48.000
Psychology, sorry.

13:48.000 --> 13:49.000
Psychology.

13:49.000 --> 13:52.000
I was originally trying to do psychology myself.

13:52.000 --> 13:53.000
I like that.

13:53.000 --> 13:55.000
Neuroscience.

13:55.000 --> 13:57.000
Neuroscience, cool.

13:57.000 --> 13:59.000
Education.

13:59.000 --> 14:02.000
Education, biomedical engineering.

14:02.000 --> 14:04.000
Second, biomedical engineering.

14:04.000 --> 14:05.000
Biomedical engineering.

14:05.000 --> 14:08.000
So I think you know,

14:08.000 --> 14:10.000
in your areas,

14:10.000 --> 14:14.000
that computer power is having more and more effect.

14:14.000 --> 14:17.000
But in neuroscience, they can measure more neurons

14:17.000 --> 14:19.000
at the same time and record it.

14:19.000 --> 14:24.000
And in biomedicine,

14:24.000 --> 14:27.000
they can operate now with robots.

14:27.000 --> 14:30.000
And they can manage the data

14:30.000 --> 14:33.000
and search for the data to find diseases.

14:34.000 --> 14:37.000
And I was like, what did you say?

14:37.000 --> 14:39.000
I'm in education.

14:39.000 --> 14:40.000
You're in education.

14:40.000 --> 14:44.000
So you know that there's a big thing now

14:44.000 --> 14:47.000
to try to make computer tutors,

14:47.000 --> 14:49.000
like all these large language models,

14:49.000 --> 14:54.000
to use those to be an individual tutor for each student.

14:54.000 --> 14:55.000
And that's perhaps,

14:55.000 --> 14:59.000
is that maybe the biggest impact of computer power on education,

14:59.000 --> 15:00.000
would you say?

15:00.000 --> 15:02.000
Or would you say something else?

15:02.000 --> 15:03.000
I would say so.

15:03.000 --> 15:05.000
Yeah.

15:05.000 --> 15:06.000
Yeah.

15:06.000 --> 15:07.000
But it's true.

15:07.000 --> 15:08.000
I think all the sciences,

15:08.000 --> 15:10.000
you know, really good physics,

15:10.000 --> 15:12.000
the things that they can simulate on a computer,

15:12.000 --> 15:13.000
are much more important.

15:13.000 --> 15:17.000
The biology, the genetics,

15:17.000 --> 15:21.000
psychology,

15:21.000 --> 15:24.000
all that we can do with simulations

15:24.000 --> 15:27.000
to model render processes.

15:27.000 --> 15:29.000
It's a big deal.

15:30.000 --> 15:32.000
All the sciences have changed.

15:32.000 --> 15:34.000
And all of our business ordinary lives,

15:34.000 --> 15:36.000
because we have computers,

15:36.000 --> 15:37.000
we have laptops,

15:37.000 --> 15:38.000
we can use it individually.

15:38.000 --> 15:39.000
We can add lives data,

15:39.000 --> 15:43.000
we can communicate to people around the world.

15:45.000 --> 15:47.000
We're all transformed.

15:47.000 --> 15:49.000
We'll continue to be.

15:50.000 --> 15:51.000
All right.

15:55.000 --> 15:57.000
Well,

15:57.000 --> 15:59.000
sort of the theme

15:59.000 --> 16:01.000
is computation and intelligence.

16:01.000 --> 16:03.000
Now, today,

16:03.000 --> 16:05.000
we have the intelligent things,

16:05.000 --> 16:06.000
our people,

16:06.000 --> 16:07.000
and they're really,

16:07.000 --> 16:09.000
almost all of you.

16:09.000 --> 16:14.000
We have systems that use a lot of computation,

16:14.000 --> 16:16.000
like large language models,

16:16.000 --> 16:20.000
or like alpha fold.

16:20.000 --> 16:21.000
You guys know,

16:21.000 --> 16:22.000
do you guys,

16:22.000 --> 16:23.000
can we have a fold?

16:23.000 --> 16:24.000
Yeah.

16:24.000 --> 16:25.000
Yeah.

16:25.000 --> 16:26.000
Figure out all the proteins

16:26.000 --> 16:29.000
that are known to humanity,

16:29.000 --> 16:30.000
how they fold up

16:30.000 --> 16:31.000
in three dimensional space,

16:31.000 --> 16:33.000
how this changes

16:33.000 --> 16:36.000
all the research possibilities.

16:36.000 --> 16:38.000
That's not intelligence.

16:38.000 --> 16:40.000
I mean, look,

16:40.000 --> 16:41.000
I want to,

16:41.000 --> 16:44.000
I'll work towards a real definition,

16:44.000 --> 16:46.000
but I would just like to urge you to think,

16:46.000 --> 16:48.000
maybe that's not intelligence.

16:48.000 --> 16:49.000
I mean,

16:49.000 --> 16:50.000
it's called AI,

16:50.000 --> 16:52.000
but I think the way we use,

16:52.000 --> 16:55.000
we would use the word AI nowadays.

16:55.000 --> 16:56.000
It just,

16:56.000 --> 16:57.000
if somebody uses a lot of computation,

16:57.000 --> 16:59.000
they call it AI.

16:59.000 --> 17:00.000
But,

17:02.000 --> 17:05.000
I think we should mean more than that.

17:05.000 --> 17:08.000
We say intelligence.

17:08.000 --> 17:09.000
I think, yeah,

17:09.000 --> 17:10.000
people are intelligent,

17:10.000 --> 17:13.000
alpha fold is non-intelligent,

17:14.000 --> 17:15.000
but,

17:16.000 --> 17:17.000
yeah,

17:17.000 --> 17:18.000
I want to distinguish DC,

17:18.000 --> 17:19.000
because like now we're,

17:19.000 --> 17:21.000
so what about my take home message here was,

17:21.000 --> 17:22.000
we're living in this time

17:22.000 --> 17:24.000
of exponentially accelerating computation,

17:24.000 --> 17:26.000
and we're entering the time

17:26.000 --> 17:29.000
of exponentially accelerating intelligence.

17:29.000 --> 17:32.000
You're just entering that.

17:32.000 --> 17:33.000
And as,

17:33.000 --> 17:35.000
both of these changes,

17:35.000 --> 17:36.000
the greatest,

17:36.000 --> 17:39.000
both of these

17:39.000 --> 17:42.000
changes will lead to

17:42.000 --> 17:44.000
changes in what we do,

17:44.000 --> 17:46.000
and transformation of ourselves.

17:46.000 --> 17:48.000
The greatest ones will come from

17:48.000 --> 17:49.000
the intelligence,

17:49.000 --> 17:50.000
accelerating intelligence,

17:50.000 --> 17:52.000
because we will really make the things

17:52.000 --> 17:55.000
that are comparable to ourselves,

17:55.000 --> 17:59.000
but without biologically replication.

17:59.000 --> 18:02.000
And then we will be becoming,

18:02.000 --> 18:03.000
we will be becoming

18:03.000 --> 18:05.000
the things we are creating,

18:05.000 --> 18:07.000
we will be becoming

18:07.000 --> 18:08.000
even more intelligent

18:08.000 --> 18:10.000
than the current humans are,

18:10.000 --> 18:13.000
the current people.

18:13.000 --> 18:15.000
So these two transformations,

18:15.000 --> 18:16.000
recent computation,

18:16.000 --> 18:17.000
recent turns,

18:17.000 --> 18:18.000
are closely related,

18:18.000 --> 18:19.000
sure,

18:19.000 --> 18:20.000
but,

18:20.000 --> 18:21.000
in that way,

18:21.000 --> 18:22.000
because of this,

18:22.000 --> 18:24.000
they're easily confused.

18:24.000 --> 18:26.000
And what I want to propose to you,

18:26.000 --> 18:27.000
the difference is that

18:27.000 --> 18:28.000
intelligence,

18:28.000 --> 18:30.000
this computation plus

18:30.000 --> 18:31.000
some notion of goal,

18:31.000 --> 18:32.000
or purpose,

18:32.000 --> 18:33.000
or agent,

18:33.000 --> 18:34.000
is something,

18:34.000 --> 18:36.000
yeah, so,

18:36.000 --> 18:37.000
that,

18:37.000 --> 18:39.000
those are the general points

18:39.000 --> 18:41.000
I'm leading to.

18:41.000 --> 18:42.000
Okay?

18:45.000 --> 18:47.000
Okay, so,

18:52.000 --> 18:54.000
computation-powered machines

18:54.000 --> 18:56.000
substitute for the computation-powered people.

18:56.000 --> 18:58.000
That's sort of what's happening.

18:59.000 --> 19:00.000
You know, we use people,

19:00.000 --> 19:02.000
various,

19:02.000 --> 19:04.000
the jobs that people do,

19:04.000 --> 19:06.000
we use them for their perception,

19:06.000 --> 19:07.000
motor control,

19:07.000 --> 19:08.000
prediction abilities,

19:08.000 --> 19:09.000
search abilities,

19:09.000 --> 19:11.000
optimization abilities,

19:11.000 --> 19:12.000
and until now,

19:12.000 --> 19:13.000
people that are cheapest,

19:13.000 --> 19:15.000
cheapest source of computation

19:15.000 --> 19:17.000
in the sense that now machines

19:17.000 --> 19:19.000
are, in some cases,

19:19.000 --> 19:22.000
find greater achievement in optimization.

19:22.000 --> 19:24.000
And so here are some of the successes

19:24.000 --> 19:25.000
over the last,

19:25.000 --> 19:27.000
maybe, 12 years.

19:27.000 --> 19:29.000
The oldest one is when

19:29.000 --> 19:32.000
machines invest in the phase of jeopardy,

19:32.000 --> 19:33.000
and,

19:33.000 --> 19:35.000
not long after that,

19:35.000 --> 19:36.000
in 2012,

19:36.000 --> 19:37.000
we worked with

19:37.000 --> 19:38.000
which transformed our

19:38.000 --> 19:39.000
speech recognition

19:39.000 --> 19:42.000
into a natural language question.

19:42.000 --> 19:43.000
Large language files,

19:43.000 --> 19:44.000
in particular,

19:44.000 --> 19:45.000
you know,

19:45.000 --> 19:48.000
you probably use them more than I do.

19:48.000 --> 19:50.000
They do all kinds of,

19:50.000 --> 19:52.000
they can appear to speak

19:52.000 --> 19:54.000
and understand.

19:54.000 --> 19:55.000
And,

19:55.000 --> 19:57.000
and the generative

19:57.000 --> 19:59.000
methods for generating pictures as well,

19:59.000 --> 20:01.000
they're impressive.

20:01.000 --> 20:03.000
So, all those things

20:03.000 --> 20:05.000
are impressive.

20:05.000 --> 20:08.000
I would not put any of them as intelligence, though.

20:08.000 --> 20:09.000
I would count as intelligence

20:09.000 --> 20:12.000
when they used deep reinforcement

20:12.000 --> 20:14.000
values to play Atari and Go

20:14.000 --> 20:17.000
and all the other games.

20:17.000 --> 20:19.000
And in these cases,

20:19.000 --> 20:20.000
they,

20:20.000 --> 20:23.000
the AI had a goal,

20:23.000 --> 20:25.000
the intelligent system,

20:25.000 --> 20:27.000
the computational system had a goal,

20:27.000 --> 20:29.000
which was to win the games,

20:29.000 --> 20:31.000
and it adjusted its behavior

20:31.000 --> 20:34.000
in order to continue winning.

20:34.000 --> 20:35.000
So,

20:35.000 --> 20:37.000
that's what I want to say is the essence

20:37.000 --> 20:39.000
of intelligence is

20:39.000 --> 20:42.000
being able to pursue a goal.

20:42.000 --> 20:43.000
And,

20:43.000 --> 20:45.000
when you use, like, the large language models,

20:45.000 --> 20:46.000
you may,

20:46.000 --> 20:48.000
they may appear to have a goal,

20:48.000 --> 20:49.000
but it's really, really true

20:49.000 --> 20:51.000
that they don't have a goal.

20:51.000 --> 20:52.000
They're just people,

20:52.000 --> 20:53.000
they're saying the same thing,

20:53.000 --> 20:54.000
and people would say,

20:54.000 --> 20:55.000
they're half-stead

20:55.000 --> 20:57.000
in similar situations.

20:57.000 --> 20:59.000
They have no sense of trying to,

20:59.000 --> 21:00.000
they're not,

21:00.000 --> 21:02.000
certainly they're not trying to manipulate you,

21:02.000 --> 21:04.000
because they don't have any goal,

21:04.000 --> 21:06.000
except for, perhaps,

21:06.000 --> 21:08.000
the designers could have a goal.

21:08.000 --> 21:10.000
The designers,

21:10.000 --> 21:12.000
the people that were designing that,

21:12.000 --> 21:14.000
would be trying to manipulate you,

21:14.000 --> 21:16.000
but the machine itself doesn't have a goal.

21:18.000 --> 21:20.000
Okay, so,

21:22.000 --> 21:23.000
maybe,

21:23.000 --> 21:25.000
maybe it is time,

21:26.000 --> 21:27.000
you know,

21:28.000 --> 21:30.000
maybe it is time for definition.

21:30.000 --> 21:32.000
Okay, so the definition,

21:32.000 --> 21:35.000
here's some definitions of intelligence.

21:35.000 --> 21:36.000
Well, it's not,

21:36.000 --> 21:38.000
first of all, it's not really definition.

21:38.000 --> 21:39.000
Maybe it's the fact,

21:39.000 --> 21:40.000
it's the most,

21:40.000 --> 21:41.000
it's claimed to be

21:41.000 --> 21:43.000
the most powerful phenomenon in the universe.

21:43.000 --> 21:45.000
That really occurs a while.

21:45.000 --> 21:46.000
And,

21:51.000 --> 21:52.000
yeah, just like,

21:54.000 --> 21:55.000
this,

21:55.000 --> 21:57.000
this, this idea,

21:58.000 --> 22:00.000
this is, this is crazy.

22:00.000 --> 22:03.000
The most powerful phenomenon in the universe.

22:03.000 --> 22:05.000
It's claimed to be the intelligence,

22:05.000 --> 22:07.000
like, and we, we people,

22:07.000 --> 22:09.000
are the primary model of intelligence.

22:09.000 --> 22:11.000
The sort of thing that we,

22:11.000 --> 22:13.000
people, and our intelligence,

22:13.000 --> 22:16.000
is the most powerful thing in the universe.

22:16.000 --> 22:18.000
More powerful than, you know,

22:18.000 --> 22:20.000
super-doers,

22:20.000 --> 22:22.000
or dark energy.

22:24.000 --> 22:26.000
Does that make any sense to you guys?

22:26.000 --> 22:27.000
Okay.

22:31.000 --> 22:32.000
Yeah?

22:32.000 --> 22:33.000
Does it to you, Eileen?

22:33.000 --> 22:34.000
I mean, yeah.

22:34.000 --> 22:35.000
I, I would agree.

22:35.000 --> 22:36.000
In what sense?

22:36.000 --> 22:37.000
I mean,

22:37.000 --> 22:38.000
everything makes up intelligence.

22:38.000 --> 22:40.000
If we talk about black magic,

22:40.000 --> 22:41.000
there's intelligence in that.

22:41.000 --> 22:42.000
Uh huh.

22:42.000 --> 22:43.000
Everything makes up intelligence.

22:43.000 --> 22:45.000
So it makes sense that it's most powerful.

22:47.000 --> 22:48.000
So, you know,

22:48.000 --> 22:49.000
one thing,

22:49.000 --> 22:50.000
I like to think,

22:50.000 --> 22:52.000
if it's more powerful than

22:52.000 --> 22:53.000
a super,

22:53.000 --> 22:54.000
a huge explosion

22:54.000 --> 22:55.000
in the,

22:55.000 --> 22:57.000
in the sky,

22:57.000 --> 22:58.000
I think it's saying that,

22:58.000 --> 23:00.000
that people,

23:00.000 --> 23:02.000
or, or, or intelligence is,

23:02.000 --> 23:04.000
which maybe came from people,

23:04.000 --> 23:06.000
will someday,

23:06.000 --> 23:08.000
um,

23:08.000 --> 23:10.000
like be moving the stars around.

23:10.000 --> 23:12.000
We will, over time,

23:12.000 --> 23:14.000
we've only been here for,

23:14.000 --> 23:15.000
I mean,

23:15.000 --> 23:17.000
maybe a hundred thousand years,

23:17.000 --> 23:19.000
for people again.

23:19.000 --> 23:21.000
If you give us a billion years,

23:21.000 --> 23:23.000
we can become quite powerful,

23:23.000 --> 23:24.000
uh,

23:24.000 --> 23:25.000
physically,

23:25.000 --> 23:26.000
uh,

23:26.000 --> 23:27.000
all the time,

23:27.000 --> 23:28.000
and we,

23:28.000 --> 23:29.000
we can,

23:29.000 --> 23:30.000
be in the force

23:30.000 --> 23:31.000
that moves the stars around,

23:31.000 --> 23:32.000
and we range

23:32.000 --> 23:33.000
the universe,

23:33.000 --> 23:34.000
the atoms,

23:34.000 --> 23:35.000
and the universe

23:35.000 --> 23:36.000
to be,

23:36.000 --> 23:37.000
to,

23:37.000 --> 23:38.000
to our liking,

23:38.000 --> 23:39.000
as a group.

23:41.000 --> 23:42.000
Um,

23:42.000 --> 23:43.000
so,

23:43.000 --> 23:44.000
when we think that way,

23:44.000 --> 23:45.000
we're so,

23:45.000 --> 23:46.000
so to think

23:46.000 --> 23:47.000
in a little bit arrogant way,

23:47.000 --> 23:48.000
right,

23:48.000 --> 23:49.000
we think we,

23:49.000 --> 23:50.000
intelligence,

23:50.000 --> 23:51.000
would be so powerful,

23:51.000 --> 23:52.000
because,

23:52.000 --> 23:53.000
um,

23:57.000 --> 23:58.000
well,

23:58.000 --> 23:59.000
we're saying that

23:59.000 --> 24:00.000
intelligence is,

24:00.000 --> 24:01.000
is quite a thing.

24:02.000 --> 24:03.000
And,

24:03.000 --> 24:04.000
so I think we should mean,

24:04.000 --> 24:05.000
we should,

24:05.000 --> 24:06.000
by the word intelligence,

24:06.000 --> 24:07.000
intelligence is just a word,

24:07.000 --> 24:08.000
and we can mean

24:08.000 --> 24:09.000
by anything we want to,

24:09.000 --> 24:11.000
but we should mean,

24:11.000 --> 24:12.000
by something that could be

24:12.000 --> 24:13.000
this powerful.

24:15.000 --> 24:16.000
Okay,

24:16.000 --> 24:17.000
and,

24:17.000 --> 24:18.000
what I'm proposing

24:18.000 --> 24:19.000
is that it's goal,

24:19.000 --> 24:20.000
that's the ability

24:20.000 --> 24:21.000
to have a purpose.

24:21.000 --> 24:22.000
That's the power

24:22.000 --> 24:23.000
of the thing,

24:23.000 --> 24:24.000
and that's the power

24:24.000 --> 24:25.000
of the phenomenon.

24:25.000 --> 24:26.000
The phenomenon,

24:26.000 --> 24:27.000
if you look around the world,

24:27.000 --> 24:28.000
you'll see there,

24:28.000 --> 24:29.000
there will be things,

24:29.000 --> 24:30.000
like people,

24:30.000 --> 24:31.000
and animals,

24:31.000 --> 24:33.000
and maybe new plants,

24:33.000 --> 24:34.000
that appear to have goals,

24:34.000 --> 24:35.000
that are well thought out

24:35.000 --> 24:36.000
in terms of goals,

24:37.000 --> 24:38.000
and,

24:39.000 --> 24:40.000
and that is the phenomenon

24:40.000 --> 24:41.000
that we observe.

24:44.000 --> 24:45.000
Um,

24:45.000 --> 24:46.000
so,

24:46.000 --> 24:47.000
with James,

24:49.000 --> 24:50.000
and,

24:50.000 --> 24:51.000
the double,

24:51.000 --> 24:53.000
he's the original psychologist.

24:56.000 --> 24:58.000
This textbook was 1890,

24:58.000 --> 24:59.000
and,

24:59.000 --> 25:00.000
and,

25:00.000 --> 25:01.000
and he was the first one

25:01.000 --> 25:02.000
to talk about city goals.

25:02.000 --> 25:04.000
When you talked about mind,

25:04.000 --> 25:05.000
you didn't use the word,

25:05.000 --> 25:06.000
you didn't use the word intelligence,

25:06.000 --> 25:07.000
you used the word mind.

25:07.000 --> 25:09.000
You said mind is attaining,

25:09.000 --> 25:11.000
the hallmark of mind,

25:11.000 --> 25:13.000
is attaining consistent ends

25:13.000 --> 25:15.000
by variable means.

25:15.000 --> 25:16.000
So,

25:16.000 --> 25:18.000
means you change what you do

25:18.000 --> 25:19.000
by what you want.

25:19.000 --> 25:21.000
The essence of having a goal

25:21.000 --> 25:23.000
is to vary what you do

25:23.000 --> 25:24.000
to get what you want.

25:24.000 --> 25:25.000
Um,

25:26.000 --> 25:27.000
yeah,

25:27.000 --> 25:28.000
and,

25:28.000 --> 25:30.000
if you go to the AI researchers,

25:30.000 --> 25:32.000
the most famous,

25:32.000 --> 25:34.000
one of the top ones,

25:34.000 --> 25:35.000
well-defined John McCarthy,

25:35.000 --> 25:37.000
he finally defined,

25:37.000 --> 25:40.000
an offered definition of intelligence,

25:40.000 --> 25:42.000
as the computational part

25:42.000 --> 25:44.000
of the ability to achieve goals.

25:44.000 --> 25:45.000
So he's thinking the way I am,

25:45.000 --> 25:47.000
I probably learned from him,

25:47.000 --> 25:48.000
actually,

25:48.000 --> 25:49.000
um,

25:49.000 --> 25:50.000
but it's the computational part

25:50.000 --> 25:52.000
of the ability to achieve goals.

25:52.000 --> 25:53.000
Um,

25:53.000 --> 25:55.000
and I might say,

25:55.000 --> 25:56.000
myself,

25:56.000 --> 25:57.000
I might say the computational part

25:57.000 --> 25:59.000
of the ability to predict

25:59.000 --> 26:01.000
and control the environment,

26:01.000 --> 26:03.000
to control the data,

26:03.000 --> 26:05.000
designed to be a computer scientist,

26:05.000 --> 26:07.000
in terms of data going back and forth

26:07.000 --> 26:08.000
to the human world,

26:08.000 --> 26:09.000
to try to understand it,

26:09.000 --> 26:11.000
to control and predict it.

26:13.000 --> 26:14.000
Okay, so,

26:14.000 --> 26:16.000
nevertheless,

26:16.000 --> 26:17.000
you know,

26:17.000 --> 26:18.000
so that's,

26:18.000 --> 26:19.000
that's those kind of definitions

26:19.000 --> 26:20.000
that I like,

26:20.000 --> 26:21.000
but I want to just

26:21.000 --> 26:22.000
talk a little bit,

26:22.000 --> 26:23.000
just want to be

26:23.000 --> 26:24.000
explicit about,

26:24.000 --> 26:25.000
people are using the word

26:25.000 --> 26:27.000
differently nowadays,

26:27.000 --> 26:28.000
uh,

26:28.000 --> 26:29.000
they often think intelligence

26:29.000 --> 26:31.000
is mimicking people.

26:31.000 --> 26:32.000
Um,

26:32.000 --> 26:34.000
as an AI seeks to reproduce

26:34.000 --> 26:35.000
behavior that we would call

26:35.000 --> 26:36.000
intelligence

26:36.000 --> 26:37.000
if it was done by people,

26:37.000 --> 26:38.000
which is the kind of thing that

26:38.000 --> 26:41.000
AI textbooks often say.

26:41.000 --> 26:42.000
And,

26:42.000 --> 26:43.000
I'm sure you guys

26:43.000 --> 26:44.000
have covered the Turing test,

26:44.000 --> 26:45.000
yes?

26:45.000 --> 26:46.000
Yeah,

26:46.000 --> 26:48.000
the Turing test is about mimicking,

26:48.000 --> 26:49.000
yeah,

26:49.000 --> 26:50.000
well,

26:50.000 --> 26:51.000
it's about

26:51.000 --> 26:52.000
a computer and a person

26:52.000 --> 26:53.000
are put in,

26:53.000 --> 26:54.000
in sealed rooms,

26:54.000 --> 26:55.000
and you,

26:55.000 --> 26:56.000
you talk to them by,

26:56.000 --> 26:57.000
by,

26:57.000 --> 26:59.000
by,

26:59.000 --> 27:01.000
text chatting,

27:01.000 --> 27:02.000
and you try to figure out

27:02.000 --> 27:03.000
which one's the person

27:03.000 --> 27:04.000
which one's the machine.

27:04.000 --> 27:05.000
And so that,

27:05.000 --> 27:06.000
that Turing test is,

27:06.000 --> 27:07.000
you know,

27:07.000 --> 27:08.000
can you make the machine

27:08.000 --> 27:09.000
behave like a person

27:09.000 --> 27:11.000
or mimic a person?

27:11.000 --> 27:12.000
So,

27:12.000 --> 27:14.000
this is a,

27:15.000 --> 27:16.000
a prominent meaning.

27:16.000 --> 27:17.000
I'm going to say,

27:17.000 --> 27:19.000
I'm going to discourage you from it,

27:19.000 --> 27:20.000
but I have to recognize

27:20.000 --> 27:21.000
that it is,

27:21.000 --> 27:22.000
it is why do we use,

27:22.000 --> 27:23.000
um,

27:23.000 --> 27:24.000
yeah,

27:24.000 --> 27:25.000
it is machine learning,

27:25.000 --> 27:26.000
supervised learning,

27:26.000 --> 27:27.000
the task is to label,

27:27.000 --> 27:28.000
say,

27:28.000 --> 27:29.000
pictures of the same person

27:29.000 --> 27:30.000
and label it,

27:30.000 --> 27:31.000
and the chat,

27:31.000 --> 27:32.000
gpt is,

27:32.000 --> 27:33.000
is tasked to

27:33.000 --> 27:34.000
generate text

27:34.000 --> 27:35.000
on the person.

27:35.000 --> 27:36.000
And so,

27:36.000 --> 27:37.000
we,

27:37.000 --> 27:38.000
we sometimes say,

27:38.000 --> 27:39.000
this is our intelligence,

27:39.000 --> 27:40.000
but notice,

27:40.000 --> 27:41.000
none of these things,

27:41.000 --> 27:42.000
um,

27:42.000 --> 27:44.000
we're referencing to goals.

27:44.000 --> 27:46.000
We're just saying,

27:46.000 --> 27:47.000
behave like a person

27:47.000 --> 27:49.000
and mimic a person.

27:49.000 --> 27:50.000
Again,

27:50.000 --> 27:51.000
again,

27:51.000 --> 27:52.000
so just to cut to the chase,

27:52.000 --> 27:53.000
my whole point is to say,

27:53.000 --> 27:54.000
this is

27:54.000 --> 27:56.000
an insufficient meaning

27:56.000 --> 27:57.000
on intelligence,

27:57.000 --> 27:58.000
because,

27:58.000 --> 27:59.000
if you're just able to mimic people,

27:59.000 --> 28:00.000
you're not going to become

28:00.000 --> 28:01.000
most popular

28:01.000 --> 28:02.000
from out on the news.

28:02.000 --> 28:04.000
You're going to be able to fool

28:04.000 --> 28:05.000
people in,

28:05.000 --> 28:06.000
in your person,

28:06.000 --> 28:07.000
but you're not going to

28:07.000 --> 28:08.000
move the,

28:08.000 --> 28:09.000
move the stars around

28:09.000 --> 28:10.000
and you're not going to become

28:11.000 --> 28:13.000
a really powerful force.

28:13.000 --> 28:14.000
Um,

28:14.000 --> 28:15.000
the universe,

28:15.000 --> 28:16.000
people may,

28:16.000 --> 28:17.000
but some of it's,

28:17.000 --> 28:18.000
it's not going to be

28:18.000 --> 28:19.000
powerful in that way.

28:19.000 --> 28:20.000
You have to get with

28:20.000 --> 28:21.000
the real thing

28:21.000 --> 28:22.000
that people are doing

28:22.000 --> 28:23.000
that,

28:23.000 --> 28:24.000
that makes people powerful

28:24.000 --> 28:26.000
and they are intelligence powerful.

28:26.000 --> 28:27.000
Okay.

28:27.000 --> 28:28.000
Well,

28:28.000 --> 28:30.000
these are two definitions

28:30.000 --> 28:31.000
of intelligence.

28:31.000 --> 28:32.000
It could be mimicking people

28:32.000 --> 28:33.000
or it could be,

28:33.000 --> 28:34.000
that is a cheated goal

28:34.000 --> 28:36.000
of computational art.

28:36.000 --> 28:37.000
But we will

28:37.000 --> 28:39.000
cheat rules.

28:39.000 --> 28:40.000
And then,

28:40.000 --> 28:41.000
so we just put those

28:41.000 --> 28:42.000
on the table

28:42.000 --> 28:43.000
and talk about which one's

28:43.000 --> 28:44.000
better.

28:44.000 --> 28:45.000
And now,

28:45.000 --> 28:46.000
you know,

28:46.000 --> 28:47.000
I've sort of given up

28:47.000 --> 28:48.000
and changed the way

28:48.000 --> 28:49.000
people use words.

28:49.000 --> 28:50.000
You know,

28:50.000 --> 28:51.000
I mean,

28:51.000 --> 28:52.000
I'm facing the fact that

28:52.000 --> 28:53.000
the world,

28:53.000 --> 28:54.000
you know,

28:54.000 --> 28:55.000
it's going to call

28:55.000 --> 28:56.000
chat,

28:56.000 --> 28:57.000
GPT,

28:57.000 --> 28:58.000
AI.

28:58.000 --> 28:59.000
It's a shame,

28:59.000 --> 29:00.000
I think.

29:00.000 --> 29:01.000
You know,

29:01.000 --> 29:02.000
maybe that's why we have

29:02.000 --> 29:03.000
this other term,

29:03.000 --> 29:04.000
artificial general intelligence,

29:04.000 --> 29:05.000
AGI,

29:05.000 --> 29:06.000
because we've got to

29:06.000 --> 29:07.000
distinguish it,

29:07.000 --> 29:08.000
but it's just like

29:08.000 --> 29:09.000
that.

29:09.000 --> 29:10.000
And you should keep

29:10.000 --> 29:11.000
in your mind

29:11.000 --> 29:12.000
that there are these two

29:12.000 --> 29:13.000
things,

29:13.000 --> 29:14.000
mimicking people

29:14.000 --> 29:15.000
whereas a system

29:15.000 --> 29:16.000
that has agency

29:16.000 --> 29:17.000
chooses actions

29:17.000 --> 29:19.000
and is trying to act

29:19.000 --> 29:22.000
to achieve goals.

29:22.000 --> 29:23.000
Okay,

29:23.000 --> 29:24.000
you know,

29:24.000 --> 29:25.000
maybe now I should jump

29:25.000 --> 29:26.000
to something that moves

29:26.000 --> 29:27.000
around.

29:27.000 --> 29:28.000
I do have

29:28.000 --> 29:29.000
one sort of video

29:29.000 --> 29:30.000
thing in there somewhere.

29:30.000 --> 29:31.000
So,

29:31.000 --> 29:32.000
yeah,

29:32.000 --> 29:33.000
this picture,

29:33.000 --> 29:34.000
this is a very simple

29:34.000 --> 29:35.000
system,

29:35.000 --> 29:36.000
but it does have agency.

29:36.000 --> 29:37.000
And so,

29:37.000 --> 29:38.000
I'm going to

29:38.000 --> 29:39.000
amaze,

29:39.000 --> 29:40.000
there's a start

29:40.000 --> 29:41.000
location,

29:41.000 --> 29:42.000
there's a goal

29:42.000 --> 29:43.000
location,

29:43.000 --> 29:44.000
and our little agent

29:44.000 --> 29:45.000
is this,

29:45.000 --> 29:46.000
this square,

29:46.000 --> 29:47.000
and if I

29:47.000 --> 29:48.000
touch this,

29:48.000 --> 29:49.000
it will start

29:49.000 --> 29:50.000
moving around.

29:50.000 --> 29:51.000
Here,

29:51.000 --> 29:52.000
it's moving around

29:52.000 --> 29:53.000
because it has

29:53.000 --> 29:54.000
actions,

29:54.000 --> 29:55.000
it can go,

29:55.000 --> 29:56.000
take a step to the right

29:56.000 --> 29:57.000
or to the left

29:57.000 --> 29:58.000
or up or down,

29:58.000 --> 29:59.000
and of course

29:59.000 --> 30:00.000
it takes a step

30:00.000 --> 30:01.000
and runs into an

30:01.000 --> 30:02.000
obstacle,

30:02.000 --> 30:03.000
and it doesn't move,

30:03.000 --> 30:04.000
but

30:04.000 --> 30:05.000
it doesn't move,

30:05.000 --> 30:06.000
and when it

30:06.000 --> 30:07.000
moves over to the left,

30:07.000 --> 30:08.000
it's going to turn

30:08.000 --> 30:09.000
and it's going to

30:09.000 --> 30:10.000
move around

30:10.000 --> 30:11.000
and it's going to

30:11.000 --> 30:12.000
move into the

30:12.000 --> 30:13.000
middle half

30:13.000 --> 30:14.000
of the goal.

30:14.000 --> 30:15.000
What you're

30:15.000 --> 30:16.000
seeing in these colors

30:16.000 --> 30:17.000
and in these arrows,

30:17.000 --> 30:18.000
you're showing,

30:18.000 --> 30:19.000
it's showing what's

30:19.000 --> 30:20.000
going on in the

30:20.000 --> 30:21.000
agents head.

30:21.000 --> 30:22.000
The real

30:22.000 --> 30:23.000
world doesn't turn

30:23.000 --> 30:24.000
green,

30:24.000 --> 30:25.000
but green means

30:25.000 --> 30:26.000
that things

30:26.000 --> 30:27.000
have a good state

30:27.000 --> 30:28.000
because it leaves

30:28.000 --> 30:29.000
close to the goal.

30:29.000 --> 30:30.000
And the arrow

30:30.000 --> 30:31.000
is indicating

30:31.000 --> 30:32.000
how good it

30:32.000 --> 30:33.000
thinks each one

30:33.000 --> 30:34.000
of its actions,

30:34.000 --> 30:35.000
which one

30:35.000 --> 30:45.000
things is best. Now the goal has been removed from where it used to be. This is a new location.

30:45.000 --> 30:53.000
It goes back where it used to be and try to retain the goal, search it for it. And so

30:53.000 --> 30:58.000
it's for perception of how valuable it thinks those states is gradually going down. It doesn't

30:58.000 --> 31:05.000
just stumble on the goal, you know, the value of the states with the goal come up. Okay?

31:05.000 --> 31:13.000
Have you got the idea? It's important to note that these states, these grid cells are just

31:13.000 --> 31:18.000
unrelated to each other. They don't really have a spatial location. They're just, so

31:18.000 --> 31:24.000
for example, if it's here, you might notice that here it's in state 14 and if it goes

31:24.000 --> 31:31.000
to action number two, it ends up in state 34. That's how it's like this. It's numbered

31:31.000 --> 31:38.000
state to numbered state as a consequence of the numbered action. Okay? Now this system

31:38.000 --> 31:45.000
learns a model, so even though it's here, it finally discovers the goal, these states

31:45.000 --> 31:53.000
will, it hasn't been here yet, but it knows it's going to go up because it's being a form

31:53.000 --> 31:59.000
of reasoning. It says, if I was there and I took the action that we would call out, it

31:59.000 --> 32:07.000
says now I end up in this state, which I know is a good state. So it's doing the primitive

32:07.000 --> 32:13.000
things, it has a model by trial and error, like instrument learning and psychology, and

32:13.000 --> 32:19.000
it's also making a model of the world and doing reasoning, what we might call reasoning,

32:19.000 --> 32:27.000
which is using the model to figure out in advance what the right actions to do are. So

32:27.000 --> 32:34.000
this system, I'm comfortable saying it as a primitive kind of intelligence and it has

32:34.000 --> 32:45.000
the ability, it has a goal, it has the ability to achieve its goals by interacting and learning.

32:45.000 --> 32:50.000
And this thing that's running in real time, you can just interact with it where it's

32:50.000 --> 33:00.000
about to get attracted to it and see what it does. It's kind of like a player. So, yeah,

33:00.000 --> 33:19.000
I'm just going to put this up close. So, now I want you to ask yourselves, there's the

33:19.000 --> 33:27.000
little guy. Do you feel sorry for him at all? You do, because you get the sense that you

33:27.000 --> 33:34.000
strive to do something and the world is going away and you say you can't possibly succeed.

33:34.000 --> 33:40.000
So, I think that's actually kind of a deep thing when we impute agency to things that

33:40.000 --> 33:49.000
behave in a certain way. And that's reality, that's not an illusion, unless you want to

33:49.000 --> 33:54.000
say all of reality is an illusion. So it's a perception or an appearance that's useful

33:54.000 --> 34:04.000
for us to understand the systems. Okay, so there's a system that has a goal. So, when

34:04.000 --> 34:09.000
I say chat CPT, it doesn't have a goal. I mean, in the exact same situation, it will always

34:09.000 --> 34:16.000
do the same thing. It doesn't pay attention to what you do in terms of influencing what

34:16.000 --> 34:27.000
it does. You hear about that. Yeah, so it doesn't, it's often described as a learning

34:27.000 --> 34:32.000
system. No networks are often described as learning systems. But when they're known

34:32.000 --> 34:38.000
the way of using learning neural networks today, they are all learned in advance of

34:38.000 --> 34:45.000
the use. So that they receive a big train set. They come crawl all over the train set

34:45.000 --> 34:50.000
and extract information from it. They learn from the train set. And then once they're

34:50.000 --> 34:57.000
put out in the world, they no longer learn. They no longer learn. Chat CPT no longer

34:57.000 --> 35:09.000
learns. Alpha fold is built with great effort. But once it goes out in the world, it's no longer

35:09.000 --> 35:15.000
learns. So that's a shame. And I think you've seen in this example how it's, it's maybe

35:15.000 --> 35:20.000
important for what we mean by intelligence. If you continue learning, so if the world

35:20.000 --> 35:34.000
changes, you can adapt to that change. Okay. What's next? Do you guys have any questions?

35:35.000 --> 35:54.000
So if they allow them to learn, they do bad things. We have something that's called

35:54.000 --> 35:59.000
catastrophic interference. The new thing that they learn interferes with everything they

35:59.000 --> 36:06.000
learn. And then they're catastrophic way. And so, so that's not an effective way to

36:06.000 --> 36:14.000
update them. Yeah, so like, as you know, a large language model might cost literally

36:14.000 --> 36:22.000
$100 million to train it. $100 million to train it. And a large data set, maybe most

36:22.000 --> 36:28.000
of the data on the internet. And then if, you know, they wait a week or a month later,

36:28.000 --> 36:33.000
they have more data from the internet. And they cannot update the existing model. They

36:33.000 --> 36:38.000
throw the existing model away and start all over again with, you know, with the extra

36:38.000 --> 36:44.000
weeks data and all the old data put together and do it again, do the one, the one time

36:44.000 --> 36:50.000
learning. And then they, and they have to do that. Every time they want to update it,

36:50.000 --> 36:57.000
they have to do it on the $100 million process of training it. It's actually a giant

36:57.000 --> 37:03.000
problem. And it's a giant opportunity for someone who wants to propose, you know, an

37:03.000 --> 37:11.000
improved learning algorithm. They can be updated continually. And it's also a research problem

37:11.000 --> 37:17.000
that many people are working on. I work out of my group. And other people out of my group.

37:17.000 --> 37:24.000
What is this group? Well, you probably saw it flash by here. Here is most of the group.

37:24.000 --> 37:33.000
Here at U of A, the reinforcement learning and arbitration intelligence group. So it's

37:33.000 --> 37:38.000
actually, you know, it's hard to get all the people together at one time. Now there's probably

37:38.000 --> 37:44.000
three times as many people. The names here are, like, this is my name. And these other

37:44.000 --> 37:51.000
guys are guys like me there. Our professors at the university there's like 10 or 11 of

37:51.000 --> 37:58.000
them now. And so you may remember Patrick. You've seen Patrick in his course. Has anyone

37:58.000 --> 38:05.000
else taught in the course? No, not a lot of us. These are the guys in the reinforcement

38:05.000 --> 38:11.000
learning and arbitration intelligence lab. Reinforcement learning is just a quote to

38:11.000 --> 38:17.000
AI that emphasizes learning and trial and error. And it's based on psychology. It's based

38:17.000 --> 38:28.000
on classical traditional ideas. Exactly. Very nice. Yeah, so this is me. These are our

38:28.000 --> 38:39.000
PIs in the front. There's Patrick in the middle of them. Adam is the head of Amy. You guys

38:39.000 --> 38:45.000
know what Amy is? The Alberta Machine Intelligence Institute downtown. You know that we have a

38:45.000 --> 38:55.000
building that is dedicated to it. The Amy building. It's at the center of AI outside

38:55.000 --> 39:04.000
of the university. So we've been doing this for a while. And you should know that Alberta,

39:04.000 --> 39:13.000
Edmonton, U of A is one of the leading places for AI. Okay, let's start with something that's

39:13.000 --> 39:21.000
unambiguously true. There are three AI centers in Canada. Canada has a national program to

39:21.000 --> 39:30.000
support AI. And there are three centers in Toronto and Montreal and Edmonton. Now, Montreal

39:30.000 --> 39:38.000
and Toronto are certainly more focused around supervised learning, classic neural network

39:38.000 --> 39:46.000
deep learning stuff. Whereas Edmonton is more focused around reinforced learning. It's

39:46.000 --> 39:53.000
our specialty. Everybody should have a specialty. And reinforced learning is more based on continual

39:53.000 --> 40:07.000
learning and our goal holds. And so, I don't know. Yeah, we're good. So actually, in reinforced

40:07.000 --> 40:20.000
learning, U of A is not just the best in Canada. We're the best in the world. And I say that

40:20.000 --> 40:25.000
with some regrets, but because I have also written a textbook on reinforced learning. I

40:25.000 --> 40:35.000
tried to promote everyone to learn about reinforced learning. But anyway, it's much more active

40:35.000 --> 40:41.000
here at U of A. We have these 10 faculty members there doing reinforced learning research.

40:41.000 --> 40:53.000
You won't find another university in the world. We have 10 faculty doing it. Yeah, so I guess

40:53.000 --> 40:59.000
what I've just told you, I told you maybe that it's pretty close to arrogance. I've told you

40:59.000 --> 41:10.000
that I think goals are essential to intelligence and to real intelligence. And I told you that

41:10.000 --> 41:18.000
the University of Alberta and this group that I have founded is now the world's leader

41:18.000 --> 41:24.000
in this area. So I kind of like talking my own book, I guess. But I think it's true. I still

41:24.000 --> 41:34.000
think it's true, even though it's sort of important to my own benefit. Now, to balance that

41:34.000 --> 41:39.000
potential arrogance, let's have a little bit of humility and let's recognize that the world,

41:39.000 --> 41:46.000
Alberta is not the world's leader in applications of A. All of these that you've heard about,

41:46.000 --> 41:57.000
they're so exciting. Large language models. Go back to your list. Which ones of them are,

41:58.000 --> 42:08.000
this is it. It's not something I've exhausted or anything like that. But if we look at this bullet,

42:08.000 --> 42:14.000
these two bullets are all about neural networks and large language models. A lot of these game

42:14.000 --> 42:21.000
things are absolutely reinforcement learning. For example, you still remember AlphaGo where

42:22.000 --> 42:33.000
AlphaZero, which became the strongest players on a whole host of games. And those were based

42:33.000 --> 42:39.000
on reinforcement learning and they were also based, they were led by graduates of the University

42:39.000 --> 42:47.000
of Alberta. David Silver at DMI, we did AlphaGo and then AlphaZero. Poker, it was led by my

42:47.000 --> 42:54.000
brother. He was one of the people who was here. He was in the picture of the microphone. Atari

42:54.000 --> 43:02.000
was led by a master's student from the U.A. He went on to work in Interacto. He worked

43:02.000 --> 43:11.000
in DMI. Starcraft was also done in DMI by David Silver and other people. Racecar driving.

43:12.000 --> 43:21.000
So this is where they have very, very realistic simulation of racecar driving. It's a Sony

43:21.000 --> 43:35.000
game. It's called Gran Turismo Racecar Driving and it's super realistic. And they got the computer

43:35.000 --> 43:45.000
to play that in an appropriate, in a way that's appropriate for a human competition. And it

43:45.000 --> 43:52.000
was meant to play to drive a car and see the variables. In generally speaking, in these

43:52.000 --> 43:59.000
cases, there's no game specific knowledge. It's just like the rules of the game. And

43:59.000 --> 44:07.000
you have to be able to face different rules. AlphaFold, it's just competition, kind of

44:07.000 --> 44:20.000
rules in there. Self-driving cars. They don't really exist fully yet. They don't really

44:20.000 --> 44:38.000
have goals fully yet. Anyway, it's much like reinforcement learning is still setting up

44:38.000 --> 44:45.000
for the teacher right here. Our AIs will have goals and agency around them for the applications

44:45.000 --> 44:49.000
that we are probably now.

44:53.000 --> 45:01.000
Do you have any more questions? What's the next thing to say? What's the next most important thing to say?

45:01.000 --> 45:16.000
Does that mean every time you want to teach it? Because it's not learning while you're driving.

45:16.000 --> 45:22.000
So would that be done through a software update which is a completely new model that allows

45:22.000 --> 45:30.000
it to work? Do you think that's the most efficient way to do that?

45:30.000 --> 45:35.000
It has some advantages because you can share between vehicles, to the extent that vehicles are

45:35.000 --> 45:44.000
similar, but often the vehicles are not. They are reducible and are definitely. And also,

45:44.000 --> 45:50.000
if you think about yourself driving, you often need to adapt to what you are today,

45:50.000 --> 45:56.000
about how the snow is today, or how the sand is today, or how the people are driving today.

45:56.000 --> 46:03.000
So you need to adapt to a particular thing you're into now. You've got some ability.

46:08.000 --> 46:14.000
So I guess there are sort of two ways to do the self-driving practice. One is like a very

46:14.000 --> 46:20.000
engineering model, three-dimensional physics, you know, masses and velocities and meters

46:20.000 --> 46:27.000
of distance between things. And you do it as a physics problem almost. Try to make sure

46:27.000 --> 46:32.000
there will be no collisions. Whereas there are things that aren't just physics like the

46:32.000 --> 46:40.000
other drivers. But you try to do it in an engineering way. What do I mean by engineering

46:40.000 --> 46:45.000
way? I just mean, well, engineers, they think about a problem and they get it into their

46:45.000 --> 46:51.000
minds. And they figure out ways to behave that will be safe and productive based on their

46:51.000 --> 47:00.000
understanding of it. And they just build the rules or the behavior into the machine. Whereas

47:00.000 --> 47:07.000
some manufacturers now are starting to use no networks for the whole thing. No networks

47:07.000 --> 47:16.000
to make the predictions about how the driving situation will unfold, how the other cars

47:16.000 --> 47:26.000
will move over the consequences of their various actions. Yeah, I think some are adopting this

47:26.000 --> 47:36.000
approach much more than others. Elon Musk in his Tesla is apparently going aggressively

47:36.000 --> 47:46.000
into a neural network model of the physics, the dynamics of the world. He has much more

47:46.000 --> 47:54.000
data because he has almost Tesla cars. Like the data on people, how cars interact with

47:54.000 --> 48:04.000
the world and all that to be fed into a large network. And maybe end up with a better model

48:04.000 --> 48:29.000
than you get from physics. Maybe that is almost at the heart of it. What's going on in science

48:30.000 --> 48:37.000
and do you model things based upon a human understanding? When I say human understanding,

48:37.000 --> 48:48.000
I mean like the engineers or physicists understanding. Or do we learn the laws of physics more in

48:48.000 --> 48:53.000
the way an animal does? An animal doesn't know differential equations and doesn't know

48:53.000 --> 49:01.000
what, how to integrate things. It just sees things and sees what happens next and tries

49:01.000 --> 49:16.000
to predict. It's an interesting challenge. Okay. Any other comments, questions?

49:16.000 --> 49:26.000
Yeah, I wanted to cast our minds back to around the time that this course began, Jonathan

49:26.000 --> 49:37.000
Schaefer was teaching in it and in his second teaching session ever, he talked about medical

49:37.000 --> 49:45.000
jargon and that he would have become a physician but he was so offended by the extra words,

49:45.000 --> 49:52.000
the new language that you supposedly have to learn to practice medicine. Maybe you don't

49:52.000 --> 49:59.000
really have to learn but it's our custom. And because of jargon he decided that computing

49:59.000 --> 50:08.000
science was a better career than medicine. So years later, like last year, I got interested

50:08.000 --> 50:18.000
in the problem of jargon. So just think now about AI on our phones or whatever you want

50:18.000 --> 50:29.000
to call it, the way our phones react. So what I would argue now is within a couple of years,

50:29.000 --> 50:40.000
the problem of medical jargon will just melt away, not because of any specific program applied

50:40.000 --> 50:49.000
to it, but because if you're in a third world country and receiving information about medicine

50:49.000 --> 50:55.000
but you don't have any physicians or nurses around, this is going to be frustrating. You're

50:55.000 --> 51:03.000
going to ask your phone for help and this is a simple translation problem that the phone

51:03.000 --> 51:10.000
will say, well, I can help you. You want me to translate between medical jargon and regular

51:10.000 --> 51:17.000
speak and you'll say that's right and the phone will say, well, I can do that. So that's

51:17.000 --> 51:26.000
what this medical statement means. And so suddenly, without any specific project or anybody

51:26.000 --> 51:33.000
describing it, the problem that Jonathan Schaefer was telling me about in 2012 that turned him

51:33.000 --> 51:40.000
off so he read it to computing science instead, will be gone. And I'm thinking probably other

51:40.000 --> 51:51.000
things will be like that too. You can say it's AI or computation, but computers, machines

51:51.000 --> 52:00.000
will change the world in a positive way in ways that humans like without any intentional

52:00.000 --> 52:08.000
program ever put into place. So does that make sense to you? Do you think that that might

52:08.000 --> 52:16.000
happen that we get rid of medical jargon without a single project anywhere designed to do that

52:16.000 --> 52:21.000
but just people use it in their phones, right, in an actual way?

52:21.000 --> 52:26.000
I can see how it might happen that it might really, I don't know, totally get rid of jargon,

52:26.000 --> 52:35.000
but it might really help a lot. And jargon is a big problem. It's very present in AI,

52:36.000 --> 52:41.000
just things, there's so many things that people know about, they know the names of them,

52:41.000 --> 52:48.000
they don't really know them. Transformers, neural networks, the names of all the algorithms,

52:48.000 --> 52:53.000
people know the names of the algorithms. Recently there was a group plow about this new algorithm,

52:53.000 --> 52:58.000
Qstar, it was coming out of a meta or something like that. When you Qstar, they're all saying,

52:58.000 --> 53:03.000
what is Qstar? I don't know, we don't know what it is, but we're so excited about it.

53:03.000 --> 53:10.000
Yeah, but hey, you're right, because the large line of the plow is they seem to be good,

53:10.000 --> 53:14.000
you can ask them anything, you can ask them, well, could you explain this in simple terms?

53:14.000 --> 53:21.000
They will try to do that, it'll be something like that. You never have time to ask your doctor

53:22.000 --> 53:27.000
to explain something, but AI, large line of the plow, they need to be patient,

53:27.000 --> 53:32.000
they need to explain it for you in different ways. And it does, even before the large line,

53:32.000 --> 53:45.000
I just feel like Google, and I can go to Google and say, what does LFG meant?

53:46.000 --> 53:53.000
I can just ask it what all the cool kids are using, some acronym or something,

53:53.000 --> 53:58.000
and it tells you what it is, it's just easy to find that out, whereas, what is jargon?

53:58.000 --> 54:03.000
Jargon is when some subset starts to use words in a certain way, and they use it,

54:03.000 --> 54:10.000
I believe it's really intending to obscure, so that we in the in-group know what this means,

54:10.000 --> 54:14.000
but you don't because you're not in the in-group. And that's what it is in science,

54:14.000 --> 54:22.000
and that's what it is in social police. And, yeah, I felt a long time ago, now we can use Google,

54:22.000 --> 54:28.000
if I want to know what some acronym means, that just people use it, I can usually figure it out

54:28.000 --> 54:36.000
just by putting it into Google, and even more so, maybe with language models.

54:36.000 --> 54:45.000
So, the other thing that makes us not very proud to be human is that the large language models,

54:45.000 --> 54:52.000
whatever you think of them, seem to handle empathy, equity, that sort of thing,

54:52.000 --> 55:02.000
better than humans. Even trained, you know, physicians and therapists are not as naturally empathetic

55:02.000 --> 55:15.000
as the large language models are. I think this is going to be, the large language models are a bit problematic.

55:15.000 --> 55:31.000
Yeah, in particular, right now, like, Gemini 1.5 is going from Google, at least.

55:31.000 --> 55:37.000
And it's too, it's too inequity. Yeah.

55:37.000 --> 55:43.000
It's too woke. It's too woke. You know, I asked for pictures of the founding fathers,

55:43.000 --> 55:48.000
and turns out they were all, you know, black people and women, you know, like most of them.

55:48.000 --> 55:57.000
And I was a very white person among them. Yeah, Google is lost.

55:57.000 --> 56:05.000
I'm 15% of its value because of this disaster of releasing Gemini 1.5.

56:05.000 --> 56:12.000
And so it feels like, I mean, it seems absolutely clear that the people who designed that language models

56:12.000 --> 56:19.000
and also some of the other language models, that they are too woke and that they are not just trying to,

56:19.000 --> 56:25.000
you know, we think Google is just giving us the facts of the internet.

56:25.000 --> 56:31.000
But it turns out, at least in the large language models, Google is also trying to change people's views,

56:31.000 --> 56:37.000
not just reflect, but what is. And this, if you think about it, is really problematic.

56:37.000 --> 56:40.000
Right. Why doesn't it tell us the truth, then?

56:40.000 --> 56:41.000
Yeah, what?

56:41.000 --> 56:48.000
Maybe for good reasons, but if they can do it for good reasons, they can also do it for bad reasons.

56:48.000 --> 56:56.000
No, I think one of the most striking things is, if you look at liberal democracy worldwide,

56:56.000 --> 57:00.000
it is becoming less successful every year, right?

57:00.000 --> 57:10.000
But in, like, open AI products and Google products, that's the philosophy they're building in.

57:10.000 --> 57:16.000
So when they find two of these things, it's a liberal democratic point of view.

57:17.000 --> 57:25.000
And so, surprisingly, people using a lot of AI products are getting that bias.

57:25.000 --> 57:30.000
Well, you may say, but that's a wonderful bias, but that's just one person's opinion.

57:30.000 --> 57:33.000
You know, other people may not feel that way.

57:33.000 --> 57:34.000
Yeah.

57:34.000 --> 57:37.000
It's very concerning.

57:37.000 --> 57:47.000
Yeah, no, I think that's been part, I mean, it was even part, you know, Dolly Too, a couple of years ago.

57:47.000 --> 57:54.000
Dolly Too, if you would search for a person, you were very likely to get a person of color and female.

57:54.000 --> 58:03.000
Because they were trying to create that balance.

58:03.000 --> 58:04.000
Yeah.

58:04.000 --> 58:08.000
And, you know, I'm interested in pig-to-human transplants.

58:08.000 --> 58:16.000
You couldn't use the word pig because they decided that it's an, you know, insolving term and so on.

58:16.000 --> 58:24.000
You know, they throw you out of the program if you can't ask them to use pig.

58:24.000 --> 58:30.000
And it's possible that this has been going on all along.

58:30.000 --> 58:32.000
Well, there was no large 90 pounds.

58:32.000 --> 58:36.000
They were always trusting who would give us.

58:36.000 --> 58:42.000
They always had the option of filtering, you know, weighting, shadow-daining.

58:42.000 --> 58:47.000
And then we know that Twitter absolutely happened.

58:47.000 --> 58:55.000
And so now, and now Lizzie Langlust, you have different views by Langlust,

58:55.000 --> 59:02.000
but he actually, we should recognize that he is a proponent of free speech.

59:02.000 --> 59:09.000
And that the mainstream media is hate-simple.

59:09.000 --> 59:12.000
And they describe it in very negative terms.

59:12.000 --> 59:23.000
You know, they have any favoritism, maybe they also disfavor Elon.

59:23.000 --> 59:31.000
And his great sin, as far as I can tell, a lot of the great kinds of companies that do amazing things,

59:31.000 --> 59:40.000
his great sin is he exposed the lack of free speech or exposed the bias on Twitter.

59:40.000 --> 59:44.000
And, yeah, so I think we have to support him on that.

59:44.000 --> 59:46.000
We have to say we want free speech.

59:46.000 --> 59:49.000
We don't want that bias information.

59:49.000 --> 59:59.000
One thing that would probably amuse you all is there was a student here, Lachini Batt,

59:59.000 --> 01:00:05.000
who is now a third-year medical student at the University of Toronto.

01:00:05.000 --> 01:00:09.000
She interviewed me about Elon Musk.

01:00:09.000 --> 01:00:17.000
I gave her Elon Musk's biography at the time, and she read it, and she interviewed me.

01:00:17.000 --> 01:00:25.000
Following that interview, you know, Google has these algorithms of how they pick the next video.

01:00:25.000 --> 01:00:34.000
So because a lot of Elon Musk's ideas came to him at the Burning Man event,

01:00:34.000 --> 01:00:42.000
you would go to a Kim Sola's video, and the next video would be a Burning Man video.

01:00:42.000 --> 01:00:44.000
I've never been to Burning Man.

01:00:44.000 --> 01:00:47.000
I don't think I would survive it.

01:00:47.000 --> 01:00:52.000
I'm sort of unlike the people who go, but that still happens.

01:00:52.000 --> 01:01:03.000
You can still find this strange linkage between Kim Sola's and Burning Man, all on account of that Elon Musk.

01:01:03.000 --> 01:01:09.000
Well, I worked for Google and DeepMind for a number of years,

01:01:09.000 --> 01:01:18.000
and as an expert in reinforcement planning, I was invited and asked to contribute to how this decision is made.

01:01:18.000 --> 01:01:26.000
You know, they want to retain viewers, show them something interesting.

01:01:26.000 --> 01:01:29.000
So I don't know how they're done now.

01:01:29.000 --> 01:01:43.000
I don't know in particular whether they learn online or in offline, as we talked about.

01:01:43.000 --> 01:01:53.000
But they do it some kind of way, it's a kind of testing.

01:01:53.000 --> 01:01:59.000
I like to think it's neutral. They're just trying to retain eyeballs.

01:01:59.000 --> 01:02:05.000
They're just trying to keep people enjoying, or at least continuing to interact.

01:02:05.000 --> 01:02:07.000
Yeah, I like to think that.

01:02:07.000 --> 01:02:16.000
But it looks like in addition to that, they're also trying to, in many cases, they're also trying to change viewpoints.

01:02:16.000 --> 01:02:17.000
Bill?

01:02:17.000 --> 01:02:31.000
Yeah, so this is a general fear of the future, is that AI, and maybe it's not AI, it's just the tech companies today.

01:02:31.000 --> 01:02:39.000
The tech companies today are not really representative of the middle of the road views on things.

01:02:39.000 --> 01:02:45.000
They're in some particular way, maybe it's a good way, but anyway, they're just a very particular way.

01:02:46.000 --> 01:02:59.000
And so the fear is that these control the narrative.

01:02:59.000 --> 01:03:05.000
They control what subjects are considered and what subjects just kind of disappear.

01:03:05.000 --> 01:03:20.000
And this is a dystopia, when the information and views are not evolving naturally, but are controlled by a small group of people.

01:03:20.000 --> 01:03:36.000
What's sort of fun is David Wood, who's the chair of London Futures, did a holiday event around Christmas about AI safety.

01:03:36.000 --> 01:03:44.000
There had just been this Bletchley Park meeting, and sort of a follow up to that.

01:03:44.000 --> 01:03:51.000
And I was one of maybe 11 or 12 speakers.

01:03:51.000 --> 01:03:58.000
And this kind of thing came up a lot about biased views.

01:03:58.000 --> 01:04:10.000
And for whatever reason, they tried to explain the technological reason why the video recording didn't work, but my presentation is the only one you can find.

01:04:10.000 --> 01:04:14.000
Can't find any of the other presentations.

01:04:14.000 --> 01:04:19.000
I didn't do anything, it's just, that's how it turned out.

01:04:19.000 --> 01:04:25.000
They had this thing with 12 speakers, and you can only find a video for one of them, and it's me.

01:04:25.000 --> 01:04:28.000
You tend to be pretty positive.

01:04:28.000 --> 01:04:31.000
Were the others, you know, questioning?

01:04:31.000 --> 01:04:43.000
No, no, many of them were talking about the fact you can't trust anybody, you know, AI is going to kill us all, all those kind of things, in various ways.

01:04:43.000 --> 01:04:54.000
So those were removed from the points of view from this happy Christmas occasion that you can't find anywhere.

01:04:55.000 --> 01:05:09.000
So now related to this, I know that, you know, you've talked about the AI, what I like to call the AI doers, those that think that we should need to be afraid of AI.

01:05:09.000 --> 01:05:13.000
And I think that's very much what we're doing.

01:05:13.000 --> 01:05:16.000
And I tend to see this part of the same thing.

01:05:16.000 --> 01:05:19.000
Russell Graham is a way of saying this.

01:05:19.000 --> 01:05:27.000
Authoritarianism, the new authoritarianism will not come like Jack Boole's violence.

01:05:27.000 --> 01:05:33.000
It will come in the language of safety and care and convenience.

01:05:33.000 --> 01:05:36.000
Right, we're trying to protect you.

01:05:36.000 --> 01:05:39.000
We're forceful about it, though.

01:05:39.000 --> 01:05:44.000
We will protect others who provide you with your saying as offensive, so you won't allow you to say it.

01:05:44.000 --> 01:05:47.000
Right.

01:05:47.000 --> 01:06:00.000
So I think that will, really, this isn't a struggle, but I'm an optimist, and I think the people trying to do that will lose out.

01:06:00.000 --> 01:06:06.000
It will be unmasked the way they were unmasked through Twitter and the area.

01:06:06.000 --> 01:06:14.000
We're sort of unmasked at Google for producing Gemini and being sort of unsuccessful as it was too obvious.

01:06:14.000 --> 01:06:18.000
It was too obvious that it was trying to help people's views.

01:06:18.000 --> 01:06:25.000
So this, you know, I started out by talking about you as what is your allegiance?

01:06:25.000 --> 01:06:30.000
Is your allegiance to...

01:06:30.000 --> 01:06:36.000
that there will be struggles over what you will identify most with?

01:06:36.000 --> 01:06:40.000
Whether it will be your country, which is maybe becoming more authoritarian,

01:06:40.000 --> 01:06:43.000
in the name of safety and care?

01:06:43.000 --> 01:06:48.000
Or will it be we are more in common with other young people than other countries?

01:06:48.000 --> 01:07:02.000
Also modern, digital, tech-savvy people?

01:07:02.000 --> 01:07:06.000
Yeah, I read this one off. His name is Balaji. Balaji is from the Boston.

01:07:06.000 --> 01:07:09.000
He says that the struggle, it was basically three.

01:07:09.000 --> 01:07:20.000
One is the woke state, and one is, which includes things like the New York Times and the media,

01:07:20.000 --> 01:07:28.000
and one is like the authoritarianism in China, the Communist Party.

01:07:28.000 --> 01:07:31.000
And those are at odds right now.

01:07:31.000 --> 01:07:37.000
The other is that China and the US struggle with each other for dominance.

01:07:37.000 --> 01:07:42.000
One empire is fading and the other one is rising.

01:07:42.000 --> 01:07:46.000
If there won't be a war, not at all.

01:07:46.000 --> 01:07:50.000
And the third is the network.

01:07:50.000 --> 01:08:02.000
There are a lot of people that are just empowered by the free exchange of ideas on the network.

01:08:02.000 --> 01:08:04.000
Yeah, and I think Africa...

01:08:04.000 --> 01:08:06.000
As a citizen of the network.

01:08:06.000 --> 01:08:10.000
Yeah, Africa that we don't think about much.

01:08:10.000 --> 01:08:15.000
But I think young people in Africa are finding a voice,

01:08:15.000 --> 01:08:18.000
and I said a lot of people are sort of interested in that,

01:08:18.000 --> 01:08:26.000
because they're not part of the power groups that they usually encounter.

01:08:26.000 --> 01:08:31.000
So in my whole career, the most consequential thing I ever did, I think,

01:08:31.000 --> 01:08:36.000
when you look at the history, it was in the year 2000.

01:08:36.000 --> 01:08:46.000
We had a meeting in Nairobi, Kenya, and many of the people that I met are still leaders now.

01:08:46.000 --> 01:08:52.000
They were young leaders then, and they're sort of mid-sage leaders now.

01:08:52.000 --> 01:08:58.000
But it's really cool that a lot of the things we predicted then have happened,

01:08:58.000 --> 01:09:06.000
and the plans we made then seemed to have kind of played out in a positive way.

01:09:06.000 --> 01:09:12.000
A lot of the other meetings that I went to, you can now say,

01:09:12.000 --> 01:09:17.000
well, that looks like a complete waste of time, or it was good for sightseeing,

01:09:17.000 --> 01:09:19.000
but you accomplished nothing.

01:09:19.000 --> 01:09:26.000
But that meeting in Kenya in 2000, we really did something.

01:09:26.000 --> 01:09:30.000
And I think it's sort of worth thinking about that.

01:09:30.000 --> 01:09:38.000
The areas of the world that are not in the news as much could really change things in the future.

01:09:38.000 --> 01:09:44.000
There are news of them are suppressed.

01:09:44.000 --> 01:09:50.000
Yeah, I think just as many things happen there, we don't hear about it.

01:09:50.000 --> 01:09:59.000
There's one last thing I'd like to discuss in today's, today.

01:09:59.000 --> 01:10:07.000
So, like Kim likes to say, likes to point out that I have gone to various meetings historically,

01:10:07.000 --> 01:10:15.000
and made the case that the AIs are more like our kin or our descendants,

01:10:15.000 --> 01:10:23.000
and that we should be open to them, rather than worrying that they're going to take over and kill us all.

01:10:23.000 --> 01:10:30.000
And I have done such things, but it's not, it's not.

01:10:30.000 --> 01:10:35.000
So I'd like to bring up another name, which is Hans Morley.

01:10:35.000 --> 01:10:40.000
Because he sort of was making this pitch long before I was. I really learned it from him.

01:10:40.000 --> 01:10:52.000
And so I think I have a slide on that.

01:10:52.000 --> 01:10:55.000
Here's some more.

01:10:55.000 --> 01:11:01.000
Here's my slide of the universe.

01:11:01.000 --> 01:11:03.000
Here's my slide of the sentiment.

01:11:03.000 --> 01:11:10.000
Yeah, so this is quotes from Hans's book in 1998.

01:11:10.000 --> 01:11:14.000
Here's another one, 1988, that he was saying this for many years.

01:11:14.000 --> 01:11:22.000
Yeah, bearing cataclysms, I consider the development of intelligent machines a near term inevitability.

01:11:22.000 --> 01:11:30.000
I consider these future machines our progeny, but who's us to give them every advantage,

01:11:30.000 --> 01:11:35.000
and then to bow out, and you can no longer contribute.

01:11:35.000 --> 01:11:44.000
That's sort of, it seems like a very humble and insured attitude to the future.

01:11:44.000 --> 01:11:51.000
Yeah, here's a little, this is a full quote.

01:11:51.000 --> 01:11:55.000
Near term inevitability, rather quickly they would displace us,

01:11:55.000 --> 01:12:01.000
because they would just be better, and it doesn't have to be in a rude way.

01:12:01.000 --> 01:12:04.000
It's just, there are successors.

01:12:04.000 --> 01:12:06.000
So you know, have to be alarmed.

01:12:06.000 --> 01:12:08.000
You can say these future machines are progeny.

01:12:08.000 --> 01:12:09.000
They're mind children.

01:12:09.000 --> 01:12:12.000
They build our image and likeness ourselves in more point of form.

01:12:12.000 --> 01:12:18.000
Like we all hope, if we have children, that they become smarter than us and more capable than us.

01:12:18.000 --> 01:12:24.000
And just, are the AI's, should we consider them part of us?

01:12:24.000 --> 01:12:27.000
Should we consider this the opposite of us?

01:12:27.000 --> 01:12:35.000
That is our choice, which way we think about it.

01:12:35.000 --> 01:12:42.000
So like these two slides, I think the students should be required to know about them on the exam.

01:12:42.000 --> 01:12:53.000
Because from the beginning of the course, we've required them to repeat back what you said in January 2015

01:12:53.000 --> 01:12:55.000
at the AI safety meeting.

01:12:55.000 --> 01:13:01.000
And this is sort of an extension of that, or gives some of the background.

01:13:01.000 --> 01:13:08.000
So I think it should also be a required part of the course.

01:13:08.000 --> 01:13:17.000
Now this struggle, I think the, so I like to say, what's actually happened, you know, if we look at it sociologically.

01:13:17.000 --> 01:13:21.000
The AI, so I would say the AI doomers have sort of won.

01:13:21.000 --> 01:13:28.000
Because if you just read the paper and you read the meetings that people have, you're reading what people think.

01:13:28.000 --> 01:13:34.000
You're concerned that the AI will somehow lead to some, you're a versatile catastrophe.

01:13:34.000 --> 01:13:35.000
Is it common view?

01:13:35.000 --> 01:13:41.000
I would say people can't really articulate why they feel this way, but they do feel this way.

01:13:41.000 --> 01:13:45.000
So they've won sort of PR war to make people scared of AI.

01:13:45.000 --> 01:13:50.000
And that's a great shame.

01:13:50.000 --> 01:13:53.000
I was meeting, and that was some of them, just this last weekend.

01:13:53.000 --> 01:13:58.000
And I said, well, you know, you guys have really won. This is what people think.

01:13:58.000 --> 01:14:01.000
And they say, think of AI, they think parallel.

01:14:01.000 --> 01:14:08.000
They think we're going to, if we're not going to be killed by them all, we're at least going to lose all our jobs.

01:14:09.000 --> 01:14:17.000
And yeah, so I said to them, I said, well, I wish it wasn't true, but I think you guys have won the PR war.

01:14:17.000 --> 01:14:23.000
And that's the fellow I said, no, not at all.

01:14:23.000 --> 01:14:25.000
He thinks that I've won.

01:14:25.000 --> 01:14:31.000
Those who are at ease with these developments have won.

01:14:31.000 --> 01:14:35.000
Because they haven't stopped AI.

01:14:35.000 --> 01:14:40.000
Their notion of success is it ends, and it never happens.

01:14:40.000 --> 01:14:46.000
And the world does not develop AI.

01:14:46.000 --> 01:14:53.000
But you know, there is a sort of Elon Musk part of this, too, which is the first one.

01:14:53.000 --> 01:14:56.000
You know, there is one of those that is fearful.

01:14:56.000 --> 01:14:57.000
Yeah, yeah.

01:14:57.000 --> 01:15:00.000
And at the same time, he's doing it.

01:15:00.000 --> 01:15:05.000
He's not suggesting attacking data centers.

01:15:05.000 --> 01:15:17.000
And so I think his prominence will probably somehow keep the world from a kind of conflict where they attack data centers.

01:15:17.000 --> 01:15:26.000
It just won't happen because, you know, Elon's there to sort of put out that particular flame.

01:15:27.000 --> 01:15:30.000
He has a very interesting case.

01:15:30.000 --> 01:15:34.000
I think his son's views have evolved.

01:15:34.000 --> 01:15:38.000
You know, he's accepting now that it's going to happen.

01:15:38.000 --> 01:15:42.000
So now he's made XAI, which is an open AI company, and originally he made open AI.

01:15:42.000 --> 01:15:44.000
I mean, open AI was, it's going to happen.

01:15:44.000 --> 01:15:49.000
So let's do it in an open way so that it doesn't become controlled by just a few.

01:15:49.000 --> 01:15:52.000
And then open AI became closed AI.

01:15:52.000 --> 01:15:57.000
And they are the few that are trying to keep it to themselves, Microsoft.

01:15:57.000 --> 01:15:59.000
So now there's XAI.

01:15:59.000 --> 01:16:09.000
XAI has its large language model called rock, which is much more open and truthful than the woke ones.

01:16:09.000 --> 01:16:19.000
And it's like Elon now thinks, yes, AI is going to happen, and we just want to make sure it's done openly.

01:16:19.000 --> 01:16:26.000
And then we want to make sure, you know, he wants it to be done well.

01:16:26.000 --> 01:16:37.000
In his view, a well-structured, a well-goaled AI system is one that is curious about the world,

01:16:37.000 --> 01:16:40.000
mainly just wants to come to understand the world.

01:16:40.000 --> 01:16:44.000
Its goal is not to turn people into lips or anything like that,

01:16:44.000 --> 01:16:55.000
but its goal is to understand and has humor and is not quite as serious as the woke language models.

01:16:55.000 --> 01:17:03.000
So I think he's now in the camp that, I think he's making it, he's acknowledging,

01:17:03.000 --> 01:17:07.000
so he's sort of famous for being someone who was afraid of it.

01:17:07.000 --> 01:17:13.000
He's always quoted as someone who said AI is like somebody who's even and is so scary.

01:17:13.000 --> 01:17:17.000
Now it's evolved to more realistic and then more productive on view.

01:17:17.000 --> 01:17:24.000
I think that's all really good, a good model for how people's views might evolve.

01:17:24.000 --> 01:17:31.000
So I want to bring that up and invite you all to be optimistic about it.

01:17:31.000 --> 01:17:34.000
What is the main thing that's happening?

01:17:34.000 --> 01:17:37.000
The main thing is not robots are rising up.

01:17:37.000 --> 01:17:45.000
The main thing that's happening is we are understanding ourselves, we are understanding minds.

01:17:45.000 --> 01:17:54.000
That's the big thing and it's just understanding the way we work has profound impacts.

01:17:54.000 --> 01:18:01.000
But we have to believe, I have to believe, that it's going to be good if we understand ourselves.

01:18:01.000 --> 01:18:04.000
We have a better opportunity to have a world peace.

01:18:04.000 --> 01:18:09.000
We have a better opportunity to reproduce what's good about people.

01:18:09.000 --> 01:18:21.000
And it's like the most profound scientific intellectual problem ever is to understand our own minds

01:18:21.000 --> 01:18:29.000
and how you can make them more effective.

01:18:29.000 --> 01:18:45.000
This has been a great teaching session and I think it's kind of cool the way your intentions and my intentions are aligned.

01:18:45.000 --> 01:18:50.000
So yeah, that's good.

01:18:50.000 --> 01:18:52.000
We're both very positive about it.

01:18:52.000 --> 01:18:57.000
So what about the students? Was it good for you?

01:18:57.000 --> 01:19:06.000
I always hate the kind of session where the senior people talk about how fantastic it was that all the junior people look mystified.

01:19:06.000 --> 01:19:11.000
No, it was nice to meet you both. It was kind of fun that you were both so passionate.

01:19:11.000 --> 01:19:13.000
So it was like what did I say?

01:19:13.000 --> 01:19:18.000
Yeah, we know each other so long and we live in the same part of the city.

01:19:18.000 --> 01:19:32.000
So sometimes when I'm mowing the lawn, Rich will be jogging down the road and he'll stop and I'll turn off the lawn mower and we'll solve the rule right there in the lawn.

01:19:32.000 --> 01:19:37.000
So yeah, that's another thing you're made out of.

01:19:37.000 --> 01:19:41.000
Okay, that's great. Thank you very much.

