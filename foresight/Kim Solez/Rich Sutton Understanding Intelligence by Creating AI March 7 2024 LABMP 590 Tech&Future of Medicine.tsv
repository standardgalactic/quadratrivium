start	end	text
0	3000	to meet everyone. Maybe I can put everyone's name.
3000	5000	Sure. My name is Richard.
5000	6000	I'm Elian.
6000	7000	Elian?
7000	8000	Yes.
8000	9000	Mariam.
9000	10000	Mariam?
10000	11000	Yes.
11000	12000	Can we meet you?
12000	13000	Yes.
13000	14000	Mariam?
14000	15000	Elian?
15000	16000	And I'm Isla.
16000	17000	Isla?
17000	18000	Yes.
18000	21000	Don't go too fast. No, I gotta remember all these things.
21000	24000	Isla, Isla, and Mariam.
24000	25000	Say it again.
25000	26000	Elian.
26000	27000	Elian.
27000	28000	All right.
28000	29000	Okay.
29000	30000	Yeah.
30000	31000	Yeah?
31000	32000	Yeah.
32000	33000	Okay. Please meet you.
33000	34000	Okay.
34000	35000	Okay.
35000	42000	So, we are a few, and we can go whichever ways you guys are interested in.
42000	43000	I'm interested in.
43000	51000	And I got a bunch of slides, but there's only a few that I really want to present for sure.
51000	58000	Well, I sort of have the impression that we've a little bit made history many times when
58000	60000	you talk to her.
60000	66000	So, anything that you think would be more likely to be making history is more interest than
66000	70000	things that aren't going to make history.
70000	74000	Well, I think it is a sort of a time.
74000	80000	And I'm sure you're all young people, and you're only familiar with your own time in
80000	81000	some sense.
81000	82000	And you think it's just normal.
82000	85000	But I don't think this is a normal time you're in.
85000	95000	I think it's a time when, well, massively increasing computation is available to everybody
95000	98000	into our companies and people make products.
98000	103000	And this is really the only time like this.
103000	108000	I mean, some sense, under 200 years ago, there was the Industrial Revolution.
108000	115000	They discovered machines that, you know, like steam powered and physical work for people.
115000	118000	No, but that really took a long time.
118000	126000	Whereas now, with the computation, I mean, much more plentiful, it happens, well, the
126000	132000	standard thing to say every 18 months, the compute power available for the same amount
132000	134000	of money doubles.
134000	137000	Twice as much.
137000	141000	And so that means our phones are working better each year.
141000	144000	And our laptops are more capable.
144000	150000	But also, you know, everyone making products, the network.
150000	158000	Yeah, so you guys, you guys are probably citizens of the network.
158000	164000	And the network will compete for your allegiances, just the way the nation states do.
164000	169000	What Canada does, all the other countries.
169000	177000	But your alleges may end up being more to your fellow digital, digitally enabled people,
177000	183000	people enabled by the network, by their phones.
183000	189000	So this is the thing that's happening now.
189000	197000	And my first take home message, that we are living in a time of exponentially accelerating
197000	199000	computation.
199000	201000	It's really dramatic.
201000	203000	I feel like I'm a little bit of a distance.
203000	206000	I've been doing this for 45 years.
206000	210000	So I can see it.
210000	214000	It's kind of slow.
214000	220000	I mean, I used to take two years for computational power to double.
220000	223000	Now, I've been saying it takes 18 months.
223000	226000	And some of my colleagues are saying it's just a year.
226000	228000	If it was to double.
228000	232000	But, you know, it takes a whole year, even if it takes a year, it takes a whole year
232000	234000	of where your computer is twice as big.
234000	236000	And it seems slow, right?
236000	239000	You say, I've got all these extra videos on my computer.
239000	242000	And I have to buy an extra disk or something.
242000	249000	But really, if that happens year after year, double in the year and three or two months,
249000	256000	and that happens for decades, it's on the order of 100 years now that it's going up.
256000	264000	And it will, it's every expectation that it will continue if not, I mean, a little faster.
264000	268000	So this is really special.
268000	273000	So the name for it, yeah, let's just hold off on the next bullet point.
273000	276000	I know a lot of you are really excited about it, most of all.
276000	279000	But it's this trend.
279000	281000	This is the trend.
281000	287000	This is Moore's law, computer power per dollar, increasing exponentially.
287000	289000	No end in sight.
289000	293000	And this is some data from Kershmer on AI.
293000	301000	So here we have years, back in 1900, up to near now.
301000	308000	And up on the other axis, we have computer power per dollar.
308000	316000	And so these are actually individual computers, like the original Mac in 1984, or the URD's points.
316000	321000	And then you can plot out other laptops and supercomputers.
321000	323000	Supercomputers are more expensive.
323000	329000	So if they make it more per dollar, more competition per dollar.
329000	333000	Okay, so your per dollar is on this axis.
333000	337000	But critically, this axis is a larger scale.
337000	339000	This is a linear scale.
339000	342000	I mean, every one of these marks is the same years, right?
342000	353000	Here, every one of these marks, the big marks, from here to here, here to here is 10 powers of 10.
353000	359000	Okay, so that's a factor of 100,000.
359000	362000	Okay?
362000	370000	And so because it's a log scale like this, if it was a straight line, it would be a really exponential increase.
371000	375000	So as I said, it seems to be slightly super exponential.
375000	383000	But that curve up is so slow, we can consider it like we're here and we're at essentially a straight line.
383000	389000	Okay, so what are we looking at?
389000	398000	We're looking at some sense, well it is exponential increase, at least exponential increase, computer power per dollar.
398000	404000	And exponential, it's really the way it's an explosion.
404000	411000	It's an explosion because every year it gets bigger and then the year it gets bigger by the same percentage.
411000	414000	It keeps doubling every two years.
414000	418000	So it's really, almost literally, what we mean by an explosion.
419000	427000	And, okay, so I know you've talked about the singularity.
427000	429000	You've talked about singularity?
437000	439000	One more time.
439000	442000	There you have it.
442000	445000	Mary in or Mary out?
445000	447000	Yeah.
447000	449000	Good enough?
449000	451000	L, E, N.
451000	454000	And G, G, O, O, N, I, L?
454000	456000	Oh, great.
459000	461000	What's that going to say?
461000	467000	Oh yeah, it's going to say, you guys have talked about the singularity, the singularity, right?
467000	471000	It never, maybe it's never properly defined.
472000	474000	But what have you been taught?
474000	478000	What different views have you gotten of the singularity?
485000	491000	Yeah, so you're saying it's a reference to the increasing growth of technology in our lives.
496000	498000	Is it an event?
498000	500000	Is it a time?
500000	502000	Is it a moment in time?
504000	506000	What is a moment in time?
513000	517000	It's like the time when something happens.
517000	520000	It's an hypothetical future.
520000	521000	So what does it mean?
521000	525000	It's hypothetical future, so it doesn't have a specific time.
525000	533000	So time in the future, so when it depends on how fast technology advances.
533000	535000	So when it happens, is that the subject?
535000	536000	Okay.
536000	538000	What is it?
542000	547000	So it's a situation where we have...
548000	554000	I have a question between humanity and technology that is such...
554000	556000	I think it's the time it's comparable.
556000	562000	The technological beings and the human beings are at some comparable level.
562000	564000	I think that's what you're thinking.
564000	565000	Good.
567000	570000	Any other notions?
570000	572000	Unless we're not...
578000	583000	Yeah, so one thing I'm trying to say is we get to decide.
583000	589000	What it is, you could decide it's the moment when AI overcomes human abilities.
589000	593000	But human abilities will continue to increase.
593000	599000	We'll get smarter when we have better education, better tools and better phones.
599000	602000	And we will continue to augment ourselves.
602000	606000	We will augment ourselves with our phones now,
606000	610000	but we will eventually be augmented by entangling things in our minds
610000	616000	and by drawing stronger connections between us and our machines.
616000	619000	Yeah, we get to decide what the similarity is.
619000	621000	But one...
621000	622000	So what I always...
622000	628000	What I always learned was similarity...
628000	632000	So there's this doubling of a year or two,
632000	637000	and you can't really totally anticipate what's going to happen.
637000	640000	And the point, of course, like this is to think about what's going to happen.
640000	645000	But we can't because, well, so many amazing things happen.
645000	647000	Machines will get more powerful.
647000	648000	People will change.
648000	650000	And maybe machines will compete.
650000	652000	Maybe there'll be our offspring.
652000	654000	Maybe there'll be our descendants.
654000	658000	It's hard to see the future after a certain point.
658000	662000	And so that, to me, is always one of the core meanings of similarity.
662000	664000	It's the point of the horizon,
664000	667000	by which you can't see further.
667000	672000	And when you try to predict what will happen.
672000	674000	Okay, but when you get there,
674000	677000	when you get to that point that you can't see past now,
677000	680000	when you get there, you'll be able to see,
680000	681000	because you'll be there,
681000	683000	and you'll be able to see a little bit further.
683000	687000	So it's the sense of the point that you can't see past.
687000	689000	Then it will receive.
689000	691000	And it will forever receive.
691000	693000	You won't actually get there.
693000	695000	Okay?
698000	703000	Another part of the sense is that it's an explosion.
703000	709000	So what I want to lead you to is the idea that maybe we are in that situation.
709000	712000	Because this doubling is happening so regularly,
712000	714000	there's a point you can't see past.
714000	716000	Maybe it's 2040 that you can't see past.
716000	718000	What will happen now?
718000	720000	What will happen in 2040?
720000	721000	You can't see past that.
721000	724000	When we get to 2040, we'll have an idea.
724000	727000	And so the explosion,
727000	728000	maybe this is the explosion,
728000	730000	and we are right in the middle.
730000	731000	And it's a slow explosion.
731000	733000	It will always be slow.
733000	735000	It will always be, it takes a whole other year to double.
735000	737000	The double will be a bigger,
737000	740000	in terms of absolute amounts that you're adding,
740000	742000	but it will still just be a doubling.
742000	744000	Maybe it will always seem slow.
744000	746000	I think the thing that is the singularity
746000	751000	is the explosion of computation and intelligence.
751000	753000	We are in the midst of it.
753000	756000	And so this is beautiful because it means
756000	760000	there's not this future thing that we're going to encounter.
760000	762000	We can encounter it right now.
762000	764000	We are encountering it.
765000	768000	It will just be more,
768000	770000	it will be more transformation,
770000	772000	more doubling, more increases,
772000	774000	more change.
774000	780000	And it will feel much like it does now.
786000	789000	So I think this is the story of our time,
789000	791000	exponentially increasing computation,
791000	795000	and it will be the story for the foreseeable future.
799000	801000	Anything that uses computation,
801000	804000	you've got ten times more valuable every five years.
815000	817000	So you know this.
817000	819000	You're working in different fields.
820000	822000	What fields are you working in?
822000	824000	Psychology.
824000	825000	Psychology?
825000	827000	You're in technology.
827000	828000	Psychology, sorry.
828000	829000	Psychology.
829000	832000	I was originally trying to do psychology myself.
832000	833000	I like that.
833000	835000	Neuroscience.
835000	837000	Neuroscience, cool.
837000	839000	Education.
839000	842000	Education, biomedical engineering.
842000	844000	Second, biomedical engineering.
844000	845000	Biomedical engineering.
845000	848000	So I think you know,
848000	850000	in your areas,
850000	854000	that computer power is having more and more effect.
854000	857000	But in neuroscience, they can measure more neurons
857000	859000	at the same time and record it.
859000	864000	And in biomedicine,
864000	867000	they can operate now with robots.
867000	870000	And they can manage the data
870000	873000	and search for the data to find diseases.
874000	877000	And I was like, what did you say?
877000	879000	I'm in education.
879000	880000	You're in education.
880000	884000	So you know that there's a big thing now
884000	887000	to try to make computer tutors,
887000	889000	like all these large language models,
889000	894000	to use those to be an individual tutor for each student.
894000	895000	And that's perhaps,
895000	899000	is that maybe the biggest impact of computer power on education,
899000	900000	would you say?
900000	902000	Or would you say something else?
902000	903000	I would say so.
903000	905000	Yeah.
905000	906000	Yeah.
906000	907000	But it's true.
907000	908000	I think all the sciences,
908000	910000	you know, really good physics,
910000	912000	the things that they can simulate on a computer,
912000	913000	are much more important.
913000	917000	The biology, the genetics,
917000	921000	psychology,
921000	924000	all that we can do with simulations
924000	927000	to model render processes.
927000	929000	It's a big deal.
930000	932000	All the sciences have changed.
932000	934000	And all of our business ordinary lives,
934000	936000	because we have computers,
936000	937000	we have laptops,
937000	938000	we can use it individually.
938000	939000	We can add lives data,
939000	943000	we can communicate to people around the world.
945000	947000	We're all transformed.
947000	949000	We'll continue to be.
950000	951000	All right.
955000	957000	Well,
957000	959000	sort of the theme
959000	961000	is computation and intelligence.
961000	963000	Now, today,
963000	965000	we have the intelligent things,
965000	966000	our people,
966000	967000	and they're really,
967000	969000	almost all of you.
969000	974000	We have systems that use a lot of computation,
974000	976000	like large language models,
976000	980000	or like alpha fold.
980000	981000	You guys know,
981000	982000	do you guys,
982000	983000	can we have a fold?
983000	984000	Yeah.
984000	985000	Yeah.
985000	986000	Figure out all the proteins
986000	989000	that are known to humanity,
989000	990000	how they fold up
990000	991000	in three dimensional space,
991000	993000	how this changes
993000	996000	all the research possibilities.
996000	998000	That's not intelligence.
998000	1000000	I mean, look,
1000000	1001000	I want to,
1001000	1004000	I'll work towards a real definition,
1004000	1006000	but I would just like to urge you to think,
1006000	1008000	maybe that's not intelligence.
1008000	1009000	I mean,
1009000	1010000	it's called AI,
1010000	1012000	but I think the way we use,
1012000	1015000	we would use the word AI nowadays.
1015000	1016000	It just,
1016000	1017000	if somebody uses a lot of computation,
1017000	1019000	they call it AI.
1019000	1020000	But,
1022000	1025000	I think we should mean more than that.
1025000	1028000	We say intelligence.
1028000	1029000	I think, yeah,
1029000	1030000	people are intelligent,
1030000	1033000	alpha fold is non-intelligent,
1034000	1035000	but,
1036000	1037000	yeah,
1037000	1038000	I want to distinguish DC,
1038000	1039000	because like now we're,
1039000	1041000	so what about my take home message here was,
1041000	1042000	we're living in this time
1042000	1044000	of exponentially accelerating computation,
1044000	1046000	and we're entering the time
1046000	1049000	of exponentially accelerating intelligence.
1049000	1052000	You're just entering that.
1052000	1053000	And as,
1053000	1055000	both of these changes,
1055000	1056000	the greatest,
1056000	1059000	both of these
1059000	1062000	changes will lead to
1062000	1064000	changes in what we do,
1064000	1066000	and transformation of ourselves.
1066000	1068000	The greatest ones will come from
1068000	1069000	the intelligence,
1069000	1070000	accelerating intelligence,
1070000	1072000	because we will really make the things
1072000	1075000	that are comparable to ourselves,
1075000	1079000	but without biologically replication.
1079000	1082000	And then we will be becoming,
1082000	1083000	we will be becoming
1083000	1085000	the things we are creating,
1085000	1087000	we will be becoming
1087000	1088000	even more intelligent
1088000	1090000	than the current humans are,
1090000	1093000	the current people.
1093000	1095000	So these two transformations,
1095000	1096000	recent computation,
1096000	1097000	recent turns,
1097000	1098000	are closely related,
1098000	1099000	sure,
1099000	1100000	but,
1100000	1101000	in that way,
1101000	1102000	because of this,
1102000	1104000	they're easily confused.
1104000	1106000	And what I want to propose to you,
1106000	1107000	the difference is that
1107000	1108000	intelligence,
1108000	1110000	this computation plus
1110000	1111000	some notion of goal,
1111000	1112000	or purpose,
1112000	1113000	or agent,
1113000	1114000	is something,
1114000	1116000	yeah, so,
1116000	1117000	that,
1117000	1119000	those are the general points
1119000	1121000	I'm leading to.
1121000	1122000	Okay?
1125000	1127000	Okay, so,
1132000	1134000	computation-powered machines
1134000	1136000	substitute for the computation-powered people.
1136000	1138000	That's sort of what's happening.
1139000	1140000	You know, we use people,
1140000	1142000	various,
1142000	1144000	the jobs that people do,
1144000	1146000	we use them for their perception,
1146000	1147000	motor control,
1147000	1148000	prediction abilities,
1148000	1149000	search abilities,
1149000	1151000	optimization abilities,
1151000	1152000	and until now,
1152000	1153000	people that are cheapest,
1153000	1155000	cheapest source of computation
1155000	1157000	in the sense that now machines
1157000	1159000	are, in some cases,
1159000	1162000	find greater achievement in optimization.
1162000	1164000	And so here are some of the successes
1164000	1165000	over the last,
1165000	1167000	maybe, 12 years.
1167000	1169000	The oldest one is when
1169000	1172000	machines invest in the phase of jeopardy,
1172000	1173000	and,
1173000	1175000	not long after that,
1175000	1176000	in 2012,
1176000	1177000	we worked with
1177000	1178000	which transformed our
1178000	1179000	speech recognition
1179000	1182000	into a natural language question.
1182000	1183000	Large language files,
1183000	1184000	in particular,
1184000	1185000	you know,
1185000	1188000	you probably use them more than I do.
1188000	1190000	They do all kinds of,
1190000	1192000	they can appear to speak
1192000	1194000	and understand.
1194000	1195000	And,
1195000	1197000	and the generative
1197000	1199000	methods for generating pictures as well,
1199000	1201000	they're impressive.
1201000	1203000	So, all those things
1203000	1205000	are impressive.
1205000	1208000	I would not put any of them as intelligence, though.
1208000	1209000	I would count as intelligence
1209000	1212000	when they used deep reinforcement
1212000	1214000	values to play Atari and Go
1214000	1217000	and all the other games.
1217000	1219000	And in these cases,
1219000	1220000	they,
1220000	1223000	the AI had a goal,
1223000	1225000	the intelligent system,
1225000	1227000	the computational system had a goal,
1227000	1229000	which was to win the games,
1229000	1231000	and it adjusted its behavior
1231000	1234000	in order to continue winning.
1234000	1235000	So,
1235000	1237000	that's what I want to say is the essence
1237000	1239000	of intelligence is
1239000	1242000	being able to pursue a goal.
1242000	1243000	And,
1243000	1245000	when you use, like, the large language models,
1245000	1246000	you may,
1246000	1248000	they may appear to have a goal,
1248000	1249000	but it's really, really true
1249000	1251000	that they don't have a goal.
1251000	1252000	They're just people,
1252000	1253000	they're saying the same thing,
1253000	1254000	and people would say,
1254000	1255000	they're half-stead
1255000	1257000	in similar situations.
1257000	1259000	They have no sense of trying to,
1259000	1260000	they're not,
1260000	1262000	certainly they're not trying to manipulate you,
1262000	1264000	because they don't have any goal,
1264000	1266000	except for, perhaps,
1266000	1268000	the designers could have a goal.
1268000	1270000	The designers,
1270000	1272000	the people that were designing that,
1272000	1274000	would be trying to manipulate you,
1274000	1276000	but the machine itself doesn't have a goal.
1278000	1280000	Okay, so,
1282000	1283000	maybe,
1283000	1285000	maybe it is time,
1286000	1287000	you know,
1288000	1290000	maybe it is time for definition.
1290000	1292000	Okay, so the definition,
1292000	1295000	here's some definitions of intelligence.
1295000	1296000	Well, it's not,
1296000	1298000	first of all, it's not really definition.
1298000	1299000	Maybe it's the fact,
1299000	1300000	it's the most,
1300000	1301000	it's claimed to be
1301000	1303000	the most powerful phenomenon in the universe.
1303000	1305000	That really occurs a while.
1305000	1306000	And,
1311000	1312000	yeah, just like,
1314000	1315000	this,
1315000	1317000	this, this idea,
1318000	1320000	this is, this is crazy.
1320000	1323000	The most powerful phenomenon in the universe.
1323000	1325000	It's claimed to be the intelligence,
1325000	1327000	like, and we, we people,
1327000	1329000	are the primary model of intelligence.
1329000	1331000	The sort of thing that we,
1331000	1333000	people, and our intelligence,
1333000	1336000	is the most powerful thing in the universe.
1336000	1338000	More powerful than, you know,
1338000	1340000	super-doers,
1340000	1342000	or dark energy.
1344000	1346000	Does that make any sense to you guys?
1346000	1347000	Okay.
1351000	1352000	Yeah?
1352000	1353000	Does it to you, Eileen?
1353000	1354000	I mean, yeah.
1354000	1355000	I, I would agree.
1355000	1356000	In what sense?
1356000	1357000	I mean,
1357000	1358000	everything makes up intelligence.
1358000	1360000	If we talk about black magic,
1360000	1361000	there's intelligence in that.
1361000	1362000	Uh huh.
1362000	1363000	Everything makes up intelligence.
1363000	1365000	So it makes sense that it's most powerful.
1367000	1368000	So, you know,
1368000	1369000	one thing,
1369000	1370000	I like to think,
1370000	1372000	if it's more powerful than
1372000	1373000	a super,
1373000	1374000	a huge explosion
1374000	1375000	in the,
1375000	1377000	in the sky,
1377000	1378000	I think it's saying that,
1378000	1380000	that people,
1380000	1382000	or, or, or intelligence is,
1382000	1384000	which maybe came from people,
1384000	1386000	will someday,
1386000	1388000	um,
1388000	1390000	like be moving the stars around.
1390000	1392000	We will, over time,
1392000	1394000	we've only been here for,
1394000	1395000	I mean,
1395000	1397000	maybe a hundred thousand years,
1397000	1399000	for people again.
1399000	1401000	If you give us a billion years,
1401000	1403000	we can become quite powerful,
1403000	1404000	uh,
1404000	1405000	physically,
1405000	1406000	uh,
1406000	1407000	all the time,
1407000	1408000	and we,
1408000	1409000	we can,
1409000	1410000	be in the force
1410000	1411000	that moves the stars around,
1411000	1412000	and we range
1412000	1413000	the universe,
1413000	1414000	the atoms,
1414000	1415000	and the universe
1415000	1416000	to be,
1416000	1417000	to,
1417000	1418000	to our liking,
1418000	1419000	as a group.
1421000	1422000	Um,
1422000	1423000	so,
1423000	1424000	when we think that way,
1424000	1425000	we're so,
1425000	1426000	so to think
1426000	1427000	in a little bit arrogant way,
1427000	1428000	right,
1428000	1429000	we think we,
1429000	1430000	intelligence,
1430000	1431000	would be so powerful,
1431000	1432000	because,
1432000	1433000	um,
1437000	1438000	well,
1438000	1439000	we're saying that
1439000	1440000	intelligence is,
1440000	1441000	is quite a thing.
1442000	1443000	And,
1443000	1444000	so I think we should mean,
1444000	1445000	we should,
1445000	1446000	by the word intelligence,
1446000	1447000	intelligence is just a word,
1447000	1448000	and we can mean
1448000	1449000	by anything we want to,
1449000	1451000	but we should mean,
1451000	1452000	by something that could be
1452000	1453000	this powerful.
1455000	1456000	Okay,
1456000	1457000	and,
1457000	1458000	what I'm proposing
1458000	1459000	is that it's goal,
1459000	1460000	that's the ability
1460000	1461000	to have a purpose.
1461000	1462000	That's the power
1462000	1463000	of the thing,
1463000	1464000	and that's the power
1464000	1465000	of the phenomenon.
1465000	1466000	The phenomenon,
1466000	1467000	if you look around the world,
1467000	1468000	you'll see there,
1468000	1469000	there will be things,
1469000	1470000	like people,
1470000	1471000	and animals,
1471000	1473000	and maybe new plants,
1473000	1474000	that appear to have goals,
1474000	1475000	that are well thought out
1475000	1476000	in terms of goals,
1477000	1478000	and,
1479000	1480000	and that is the phenomenon
1480000	1481000	that we observe.
1484000	1485000	Um,
1485000	1486000	so,
1486000	1487000	with James,
1489000	1490000	and,
1490000	1491000	the double,
1491000	1493000	he's the original psychologist.
1496000	1498000	This textbook was 1890,
1498000	1499000	and,
1499000	1500000	and,
1500000	1501000	and he was the first one
1501000	1502000	to talk about city goals.
1502000	1504000	When you talked about mind,
1504000	1505000	you didn't use the word,
1505000	1506000	you didn't use the word intelligence,
1506000	1507000	you used the word mind.
1507000	1509000	You said mind is attaining,
1509000	1511000	the hallmark of mind,
1511000	1513000	is attaining consistent ends
1513000	1515000	by variable means.
1515000	1516000	So,
1516000	1518000	means you change what you do
1518000	1519000	by what you want.
1519000	1521000	The essence of having a goal
1521000	1523000	is to vary what you do
1523000	1524000	to get what you want.
1524000	1525000	Um,
1526000	1527000	yeah,
1527000	1528000	and,
1528000	1530000	if you go to the AI researchers,
1530000	1532000	the most famous,
1532000	1534000	one of the top ones,
1534000	1535000	well-defined John McCarthy,
1535000	1537000	he finally defined,
1537000	1540000	an offered definition of intelligence,
1540000	1542000	as the computational part
1542000	1544000	of the ability to achieve goals.
1544000	1545000	So he's thinking the way I am,
1545000	1547000	I probably learned from him,
1547000	1548000	actually,
1548000	1549000	um,
1549000	1550000	but it's the computational part
1550000	1552000	of the ability to achieve goals.
1552000	1553000	Um,
1553000	1555000	and I might say,
1555000	1556000	myself,
1556000	1557000	I might say the computational part
1557000	1559000	of the ability to predict
1559000	1561000	and control the environment,
1561000	1563000	to control the data,
1563000	1565000	designed to be a computer scientist,
1565000	1567000	in terms of data going back and forth
1567000	1568000	to the human world,
1568000	1569000	to try to understand it,
1569000	1571000	to control and predict it.
1573000	1574000	Okay, so,
1574000	1576000	nevertheless,
1576000	1577000	you know,
1577000	1578000	so that's,
1578000	1579000	that's those kind of definitions
1579000	1580000	that I like,
1580000	1581000	but I want to just
1581000	1582000	talk a little bit,
1582000	1583000	just want to be
1583000	1584000	explicit about,
1584000	1585000	people are using the word
1585000	1587000	differently nowadays,
1587000	1588000	uh,
1588000	1589000	they often think intelligence
1589000	1591000	is mimicking people.
1591000	1592000	Um,
1592000	1594000	as an AI seeks to reproduce
1594000	1595000	behavior that we would call
1595000	1596000	intelligence
1596000	1597000	if it was done by people,
1597000	1598000	which is the kind of thing that
1598000	1601000	AI textbooks often say.
1601000	1602000	And,
1602000	1603000	I'm sure you guys
1603000	1604000	have covered the Turing test,
1604000	1605000	yes?
1605000	1606000	Yeah,
1606000	1608000	the Turing test is about mimicking,
1608000	1609000	yeah,
1609000	1610000	well,
1610000	1611000	it's about
1611000	1612000	a computer and a person
1612000	1613000	are put in,
1613000	1614000	in sealed rooms,
1614000	1615000	and you,
1615000	1616000	you talk to them by,
1616000	1617000	by,
1617000	1619000	by,
1619000	1621000	text chatting,
1621000	1622000	and you try to figure out
1622000	1623000	which one's the person
1623000	1624000	which one's the machine.
1624000	1625000	And so that,
1625000	1626000	that Turing test is,
1626000	1627000	you know,
1627000	1628000	can you make the machine
1628000	1629000	behave like a person
1629000	1631000	or mimic a person?
1631000	1632000	So,
1632000	1634000	this is a,
1635000	1636000	a prominent meaning.
1636000	1637000	I'm going to say,
1637000	1639000	I'm going to discourage you from it,
1639000	1640000	but I have to recognize
1640000	1641000	that it is,
1641000	1642000	it is why do we use,
1642000	1643000	um,
1643000	1644000	yeah,
1644000	1645000	it is machine learning,
1645000	1646000	supervised learning,
1646000	1647000	the task is to label,
1647000	1648000	say,
1648000	1649000	pictures of the same person
1649000	1650000	and label it,
1650000	1651000	and the chat,
1651000	1652000	gpt is,
1652000	1653000	is tasked to
1653000	1654000	generate text
1654000	1655000	on the person.
1655000	1656000	And so,
1656000	1657000	we,
1657000	1658000	we sometimes say,
1658000	1659000	this is our intelligence,
1659000	1660000	but notice,
1660000	1661000	none of these things,
1661000	1662000	um,
1662000	1664000	we're referencing to goals.
1664000	1666000	We're just saying,
1666000	1667000	behave like a person
1667000	1669000	and mimic a person.
1669000	1670000	Again,
1670000	1671000	again,
1671000	1672000	so just to cut to the chase,
1672000	1673000	my whole point is to say,
1673000	1674000	this is
1674000	1676000	an insufficient meaning
1676000	1677000	on intelligence,
1677000	1678000	because,
1678000	1679000	if you're just able to mimic people,
1679000	1680000	you're not going to become
1680000	1681000	most popular
1681000	1682000	from out on the news.
1682000	1684000	You're going to be able to fool
1684000	1685000	people in,
1685000	1686000	in your person,
1686000	1687000	but you're not going to
1687000	1688000	move the,
1688000	1689000	move the stars around
1689000	1690000	and you're not going to become
1691000	1693000	a really powerful force.
1693000	1694000	Um,
1694000	1695000	the universe,
1695000	1696000	people may,
1696000	1697000	but some of it's,
1697000	1698000	it's not going to be
1698000	1699000	powerful in that way.
1699000	1700000	You have to get with
1700000	1701000	the real thing
1701000	1702000	that people are doing
1702000	1703000	that,
1703000	1704000	that makes people powerful
1704000	1706000	and they are intelligence powerful.
1706000	1707000	Okay.
1707000	1708000	Well,
1708000	1710000	these are two definitions
1710000	1711000	of intelligence.
1711000	1712000	It could be mimicking people
1712000	1713000	or it could be,
1713000	1714000	that is a cheated goal
1714000	1716000	of computational art.
1716000	1717000	But we will
1717000	1719000	cheat rules.
1719000	1720000	And then,
1720000	1721000	so we just put those
1721000	1722000	on the table
1722000	1723000	and talk about which one's
1723000	1724000	better.
1724000	1725000	And now,
1725000	1726000	you know,
1726000	1727000	I've sort of given up
1727000	1728000	and changed the way
1728000	1729000	people use words.
1729000	1730000	You know,
1730000	1731000	I mean,
1731000	1732000	I'm facing the fact that
1732000	1733000	the world,
1733000	1734000	you know,
1734000	1735000	it's going to call
1735000	1736000	chat,
1736000	1737000	GPT,
1737000	1738000	AI.
1738000	1739000	It's a shame,
1739000	1740000	I think.
1740000	1741000	You know,
1741000	1742000	maybe that's why we have
1742000	1743000	this other term,
1743000	1744000	artificial general intelligence,
1744000	1745000	AGI,
1745000	1746000	because we've got to
1746000	1747000	distinguish it,
1747000	1748000	but it's just like
1748000	1749000	that.
1749000	1750000	And you should keep
1750000	1751000	in your mind
1751000	1752000	that there are these two
1752000	1753000	things,
1753000	1754000	mimicking people
1754000	1755000	whereas a system
1755000	1756000	that has agency
1756000	1757000	chooses actions
1757000	1759000	and is trying to act
1759000	1762000	to achieve goals.
1762000	1763000	Okay,
1763000	1764000	you know,
1764000	1765000	maybe now I should jump
1765000	1766000	to something that moves
1766000	1767000	around.
1767000	1768000	I do have
1768000	1769000	one sort of video
1769000	1770000	thing in there somewhere.
1770000	1771000	So,
1771000	1772000	yeah,
1772000	1773000	this picture,
1773000	1774000	this is a very simple
1774000	1775000	system,
1775000	1776000	but it does have agency.
1776000	1777000	And so,
1777000	1778000	I'm going to
1778000	1779000	amaze,
1779000	1780000	there's a start
1780000	1781000	location,
1781000	1782000	there's a goal
1782000	1783000	location,
1783000	1784000	and our little agent
1784000	1785000	is this,
1785000	1786000	this square,
1786000	1787000	and if I
1787000	1788000	touch this,
1788000	1789000	it will start
1789000	1790000	moving around.
1790000	1791000	Here,
1791000	1792000	it's moving around
1792000	1793000	because it has
1793000	1794000	actions,
1794000	1795000	it can go,
1795000	1796000	take a step to the right
1796000	1797000	or to the left
1797000	1798000	or up or down,
1798000	1799000	and of course
1799000	1800000	it takes a step
1800000	1801000	and runs into an
1801000	1802000	obstacle,
1802000	1803000	and it doesn't move,
1803000	1804000	but
1804000	1805000	it doesn't move,
1805000	1806000	and when it
1806000	1807000	moves over to the left,
1807000	1808000	it's going to turn
1808000	1809000	and it's going to
1809000	1810000	move around
1810000	1811000	and it's going to
1811000	1812000	move into the
1812000	1813000	middle half
1813000	1814000	of the goal.
1814000	1815000	What you're
1815000	1816000	seeing in these colors
1816000	1817000	and in these arrows,
1817000	1818000	you're showing,
1818000	1819000	it's showing what's
1819000	1820000	going on in the
1820000	1821000	agents head.
1821000	1822000	The real
1822000	1823000	world doesn't turn
1823000	1824000	green,
1824000	1825000	but green means
1825000	1826000	that things
1826000	1827000	have a good state
1827000	1828000	because it leaves
1828000	1829000	close to the goal.
1829000	1830000	And the arrow
1830000	1831000	is indicating
1831000	1832000	how good it
1832000	1833000	thinks each one
1833000	1834000	of its actions,
1834000	1835000	which one
1835000	1845000	things is best. Now the goal has been removed from where it used to be. This is a new location.
1845000	1853000	It goes back where it used to be and try to retain the goal, search it for it. And so
1853000	1858000	it's for perception of how valuable it thinks those states is gradually going down. It doesn't
1858000	1865000	just stumble on the goal, you know, the value of the states with the goal come up. Okay?
1865000	1873000	Have you got the idea? It's important to note that these states, these grid cells are just
1873000	1878000	unrelated to each other. They don't really have a spatial location. They're just, so
1878000	1884000	for example, if it's here, you might notice that here it's in state 14 and if it goes
1884000	1891000	to action number two, it ends up in state 34. That's how it's like this. It's numbered
1891000	1898000	state to numbered state as a consequence of the numbered action. Okay? Now this system
1898000	1905000	learns a model, so even though it's here, it finally discovers the goal, these states
1905000	1913000	will, it hasn't been here yet, but it knows it's going to go up because it's being a form
1913000	1919000	of reasoning. It says, if I was there and I took the action that we would call out, it
1919000	1927000	says now I end up in this state, which I know is a good state. So it's doing the primitive
1927000	1933000	things, it has a model by trial and error, like instrument learning and psychology, and
1933000	1939000	it's also making a model of the world and doing reasoning, what we might call reasoning,
1939000	1947000	which is using the model to figure out in advance what the right actions to do are. So
1947000	1954000	this system, I'm comfortable saying it as a primitive kind of intelligence and it has
1954000	1965000	the ability, it has a goal, it has the ability to achieve its goals by interacting and learning.
1965000	1970000	And this thing that's running in real time, you can just interact with it where it's
1970000	1980000	about to get attracted to it and see what it does. It's kind of like a player. So, yeah,
1980000	1999000	I'm just going to put this up close. So, now I want you to ask yourselves, there's the
1999000	2007000	little guy. Do you feel sorry for him at all? You do, because you get the sense that you
2007000	2014000	strive to do something and the world is going away and you say you can't possibly succeed.
2014000	2020000	So, I think that's actually kind of a deep thing when we impute agency to things that
2020000	2029000	behave in a certain way. And that's reality, that's not an illusion, unless you want to
2029000	2034000	say all of reality is an illusion. So it's a perception or an appearance that's useful
2034000	2044000	for us to understand the systems. Okay, so there's a system that has a goal. So, when
2044000	2049000	I say chat CPT, it doesn't have a goal. I mean, in the exact same situation, it will always
2049000	2056000	do the same thing. It doesn't pay attention to what you do in terms of influencing what
2056000	2067000	it does. You hear about that. Yeah, so it doesn't, it's often described as a learning
2067000	2072000	system. No networks are often described as learning systems. But when they're known
2072000	2078000	the way of using learning neural networks today, they are all learned in advance of
2078000	2085000	the use. So that they receive a big train set. They come crawl all over the train set
2085000	2090000	and extract information from it. They learn from the train set. And then once they're
2090000	2097000	put out in the world, they no longer learn. They no longer learn. Chat CPT no longer
2097000	2109000	learns. Alpha fold is built with great effort. But once it goes out in the world, it's no longer
2109000	2115000	learns. So that's a shame. And I think you've seen in this example how it's, it's maybe
2115000	2120000	important for what we mean by intelligence. If you continue learning, so if the world
2120000	2134000	changes, you can adapt to that change. Okay. What's next? Do you guys have any questions?
2135000	2154000	So if they allow them to learn, they do bad things. We have something that's called
2154000	2159000	catastrophic interference. The new thing that they learn interferes with everything they
2159000	2166000	learn. And then they're catastrophic way. And so, so that's not an effective way to
2166000	2174000	update them. Yeah, so like, as you know, a large language model might cost literally
2174000	2182000	$100 million to train it. $100 million to train it. And a large data set, maybe most
2182000	2188000	of the data on the internet. And then if, you know, they wait a week or a month later,
2188000	2193000	they have more data from the internet. And they cannot update the existing model. They
2193000	2198000	throw the existing model away and start all over again with, you know, with the extra
2198000	2204000	weeks data and all the old data put together and do it again, do the one, the one time
2204000	2210000	learning. And then they, and they have to do that. Every time they want to update it,
2210000	2217000	they have to do it on the $100 million process of training it. It's actually a giant
2217000	2223000	problem. And it's a giant opportunity for someone who wants to propose, you know, an
2223000	2231000	improved learning algorithm. They can be updated continually. And it's also a research problem
2231000	2237000	that many people are working on. I work out of my group. And other people out of my group.
2237000	2244000	What is this group? Well, you probably saw it flash by here. Here is most of the group.
2244000	2253000	Here at U of A, the reinforcement learning and arbitration intelligence group. So it's
2253000	2258000	actually, you know, it's hard to get all the people together at one time. Now there's probably
2258000	2264000	three times as many people. The names here are, like, this is my name. And these other
2264000	2271000	guys are guys like me there. Our professors at the university there's like 10 or 11 of
2271000	2278000	them now. And so you may remember Patrick. You've seen Patrick in his course. Has anyone
2278000	2285000	else taught in the course? No, not a lot of us. These are the guys in the reinforcement
2285000	2291000	learning and arbitration intelligence lab. Reinforcement learning is just a quote to
2291000	2297000	AI that emphasizes learning and trial and error. And it's based on psychology. It's based
2297000	2308000	on classical traditional ideas. Exactly. Very nice. Yeah, so this is me. These are our
2308000	2319000	PIs in the front. There's Patrick in the middle of them. Adam is the head of Amy. You guys
2319000	2325000	know what Amy is? The Alberta Machine Intelligence Institute downtown. You know that we have a
2325000	2335000	building that is dedicated to it. The Amy building. It's at the center of AI outside
2335000	2344000	of the university. So we've been doing this for a while. And you should know that Alberta,
2344000	2353000	Edmonton, U of A is one of the leading places for AI. Okay, let's start with something that's
2353000	2361000	unambiguously true. There are three AI centers in Canada. Canada has a national program to
2361000	2370000	support AI. And there are three centers in Toronto and Montreal and Edmonton. Now, Montreal
2370000	2378000	and Toronto are certainly more focused around supervised learning, classic neural network
2378000	2386000	deep learning stuff. Whereas Edmonton is more focused around reinforced learning. It's
2386000	2393000	our specialty. Everybody should have a specialty. And reinforced learning is more based on continual
2393000	2407000	learning and our goal holds. And so, I don't know. Yeah, we're good. So actually, in reinforced
2407000	2420000	learning, U of A is not just the best in Canada. We're the best in the world. And I say that
2420000	2425000	with some regrets, but because I have also written a textbook on reinforced learning. I
2425000	2435000	tried to promote everyone to learn about reinforced learning. But anyway, it's much more active
2435000	2441000	here at U of A. We have these 10 faculty members there doing reinforced learning research.
2441000	2453000	You won't find another university in the world. We have 10 faculty doing it. Yeah, so I guess
2453000	2459000	what I've just told you, I told you maybe that it's pretty close to arrogance. I've told you
2459000	2470000	that I think goals are essential to intelligence and to real intelligence. And I told you that
2470000	2478000	the University of Alberta and this group that I have founded is now the world's leader
2478000	2484000	in this area. So I kind of like talking my own book, I guess. But I think it's true. I still
2484000	2494000	think it's true, even though it's sort of important to my own benefit. Now, to balance that
2494000	2499000	potential arrogance, let's have a little bit of humility and let's recognize that the world,
2499000	2506000	Alberta is not the world's leader in applications of A. All of these that you've heard about,
2506000	2517000	they're so exciting. Large language models. Go back to your list. Which ones of them are,
2518000	2528000	this is it. It's not something I've exhausted or anything like that. But if we look at this bullet,
2528000	2534000	these two bullets are all about neural networks and large language models. A lot of these game
2534000	2541000	things are absolutely reinforcement learning. For example, you still remember AlphaGo where
2542000	2553000	AlphaZero, which became the strongest players on a whole host of games. And those were based
2553000	2559000	on reinforcement learning and they were also based, they were led by graduates of the University
2559000	2567000	of Alberta. David Silver at DMI, we did AlphaGo and then AlphaZero. Poker, it was led by my
2567000	2574000	brother. He was one of the people who was here. He was in the picture of the microphone. Atari
2574000	2582000	was led by a master's student from the U.A. He went on to work in Interacto. He worked
2582000	2591000	in DMI. Starcraft was also done in DMI by David Silver and other people. Racecar driving.
2592000	2601000	So this is where they have very, very realistic simulation of racecar driving. It's a Sony
2601000	2615000	game. It's called Gran Turismo Racecar Driving and it's super realistic. And they got the computer
2615000	2625000	to play that in an appropriate, in a way that's appropriate for a human competition. And it
2625000	2632000	was meant to play to drive a car and see the variables. In generally speaking, in these
2632000	2639000	cases, there's no game specific knowledge. It's just like the rules of the game. And
2639000	2647000	you have to be able to face different rules. AlphaFold, it's just competition, kind of
2647000	2660000	rules in there. Self-driving cars. They don't really exist fully yet. They don't really
2660000	2678000	have goals fully yet. Anyway, it's much like reinforcement learning is still setting up
2678000	2685000	for the teacher right here. Our AIs will have goals and agency around them for the applications
2685000	2689000	that we are probably now.
2693000	2701000	Do you have any more questions? What's the next thing to say? What's the next most important thing to say?
2701000	2716000	Does that mean every time you want to teach it? Because it's not learning while you're driving.
2716000	2722000	So would that be done through a software update which is a completely new model that allows
2722000	2730000	it to work? Do you think that's the most efficient way to do that?
2730000	2735000	It has some advantages because you can share between vehicles, to the extent that vehicles are
2735000	2744000	similar, but often the vehicles are not. They are reducible and are definitely. And also,
2744000	2750000	if you think about yourself driving, you often need to adapt to what you are today,
2750000	2756000	about how the snow is today, or how the sand is today, or how the people are driving today.
2756000	2763000	So you need to adapt to a particular thing you're into now. You've got some ability.
2768000	2774000	So I guess there are sort of two ways to do the self-driving practice. One is like a very
2774000	2780000	engineering model, three-dimensional physics, you know, masses and velocities and meters
2780000	2787000	of distance between things. And you do it as a physics problem almost. Try to make sure
2787000	2792000	there will be no collisions. Whereas there are things that aren't just physics like the
2792000	2800000	other drivers. But you try to do it in an engineering way. What do I mean by engineering
2800000	2805000	way? I just mean, well, engineers, they think about a problem and they get it into their
2805000	2811000	minds. And they figure out ways to behave that will be safe and productive based on their
2811000	2820000	understanding of it. And they just build the rules or the behavior into the machine. Whereas
2820000	2827000	some manufacturers now are starting to use no networks for the whole thing. No networks
2827000	2836000	to make the predictions about how the driving situation will unfold, how the other cars
2836000	2846000	will move over the consequences of their various actions. Yeah, I think some are adopting this
2846000	2856000	approach much more than others. Elon Musk in his Tesla is apparently going aggressively
2856000	2866000	into a neural network model of the physics, the dynamics of the world. He has much more
2866000	2874000	data because he has almost Tesla cars. Like the data on people, how cars interact with
2874000	2884000	the world and all that to be fed into a large network. And maybe end up with a better model
2884000	2909000	than you get from physics. Maybe that is almost at the heart of it. What's going on in science
2910000	2917000	and do you model things based upon a human understanding? When I say human understanding,
2917000	2928000	I mean like the engineers or physicists understanding. Or do we learn the laws of physics more in
2928000	2933000	the way an animal does? An animal doesn't know differential equations and doesn't know
2933000	2941000	what, how to integrate things. It just sees things and sees what happens next and tries
2941000	2956000	to predict. It's an interesting challenge. Okay. Any other comments, questions?
2956000	2966000	Yeah, I wanted to cast our minds back to around the time that this course began, Jonathan
2966000	2977000	Schaefer was teaching in it and in his second teaching session ever, he talked about medical
2977000	2985000	jargon and that he would have become a physician but he was so offended by the extra words,
2985000	2992000	the new language that you supposedly have to learn to practice medicine. Maybe you don't
2992000	2999000	really have to learn but it's our custom. And because of jargon he decided that computing
2999000	3008000	science was a better career than medicine. So years later, like last year, I got interested
3008000	3018000	in the problem of jargon. So just think now about AI on our phones or whatever you want
3018000	3029000	to call it, the way our phones react. So what I would argue now is within a couple of years,
3029000	3040000	the problem of medical jargon will just melt away, not because of any specific program applied
3040000	3049000	to it, but because if you're in a third world country and receiving information about medicine
3049000	3055000	but you don't have any physicians or nurses around, this is going to be frustrating. You're
3055000	3063000	going to ask your phone for help and this is a simple translation problem that the phone
3063000	3070000	will say, well, I can help you. You want me to translate between medical jargon and regular
3070000	3077000	speak and you'll say that's right and the phone will say, well, I can do that. So that's
3077000	3086000	what this medical statement means. And so suddenly, without any specific project or anybody
3086000	3093000	describing it, the problem that Jonathan Schaefer was telling me about in 2012 that turned him
3093000	3100000	off so he read it to computing science instead, will be gone. And I'm thinking probably other
3100000	3111000	things will be like that too. You can say it's AI or computation, but computers, machines
3111000	3120000	will change the world in a positive way in ways that humans like without any intentional
3120000	3128000	program ever put into place. So does that make sense to you? Do you think that that might
3128000	3136000	happen that we get rid of medical jargon without a single project anywhere designed to do that
3136000	3141000	but just people use it in their phones, right, in an actual way?
3141000	3146000	I can see how it might happen that it might really, I don't know, totally get rid of jargon,
3146000	3155000	but it might really help a lot. And jargon is a big problem. It's very present in AI,
3156000	3161000	just things, there's so many things that people know about, they know the names of them,
3161000	3168000	they don't really know them. Transformers, neural networks, the names of all the algorithms,
3168000	3173000	people know the names of the algorithms. Recently there was a group plow about this new algorithm,
3173000	3178000	Qstar, it was coming out of a meta or something like that. When you Qstar, they're all saying,
3178000	3183000	what is Qstar? I don't know, we don't know what it is, but we're so excited about it.
3183000	3190000	Yeah, but hey, you're right, because the large line of the plow is they seem to be good,
3190000	3194000	you can ask them anything, you can ask them, well, could you explain this in simple terms?
3194000	3201000	They will try to do that, it'll be something like that. You never have time to ask your doctor
3202000	3207000	to explain something, but AI, large line of the plow, they need to be patient,
3207000	3212000	they need to explain it for you in different ways. And it does, even before the large line,
3212000	3225000	I just feel like Google, and I can go to Google and say, what does LFG meant?
3226000	3233000	I can just ask it what all the cool kids are using, some acronym or something,
3233000	3238000	and it tells you what it is, it's just easy to find that out, whereas, what is jargon?
3238000	3243000	Jargon is when some subset starts to use words in a certain way, and they use it,
3243000	3250000	I believe it's really intending to obscure, so that we in the in-group know what this means,
3250000	3254000	but you don't because you're not in the in-group. And that's what it is in science,
3254000	3262000	and that's what it is in social police. And, yeah, I felt a long time ago, now we can use Google,
3262000	3268000	if I want to know what some acronym means, that just people use it, I can usually figure it out
3268000	3276000	just by putting it into Google, and even more so, maybe with language models.
3276000	3285000	So, the other thing that makes us not very proud to be human is that the large language models,
3285000	3292000	whatever you think of them, seem to handle empathy, equity, that sort of thing,
3292000	3302000	better than humans. Even trained, you know, physicians and therapists are not as naturally empathetic
3302000	3315000	as the large language models are. I think this is going to be, the large language models are a bit problematic.
3315000	3331000	Yeah, in particular, right now, like, Gemini 1.5 is going from Google, at least.
3331000	3337000	And it's too, it's too inequity. Yeah.
3337000	3343000	It's too woke. It's too woke. You know, I asked for pictures of the founding fathers,
3343000	3348000	and turns out they were all, you know, black people and women, you know, like most of them.
3348000	3357000	And I was a very white person among them. Yeah, Google is lost.
3357000	3365000	I'm 15% of its value because of this disaster of releasing Gemini 1.5.
3365000	3372000	And so it feels like, I mean, it seems absolutely clear that the people who designed that language models
3372000	3379000	and also some of the other language models, that they are too woke and that they are not just trying to,
3379000	3385000	you know, we think Google is just giving us the facts of the internet.
3385000	3391000	But it turns out, at least in the large language models, Google is also trying to change people's views,
3391000	3397000	not just reflect, but what is. And this, if you think about it, is really problematic.
3397000	3400000	Right. Why doesn't it tell us the truth, then?
3400000	3401000	Yeah, what?
3401000	3408000	Maybe for good reasons, but if they can do it for good reasons, they can also do it for bad reasons.
3408000	3416000	No, I think one of the most striking things is, if you look at liberal democracy worldwide,
3416000	3420000	it is becoming less successful every year, right?
3420000	3430000	But in, like, open AI products and Google products, that's the philosophy they're building in.
3430000	3436000	So when they find two of these things, it's a liberal democratic point of view.
3437000	3445000	And so, surprisingly, people using a lot of AI products are getting that bias.
3445000	3450000	Well, you may say, but that's a wonderful bias, but that's just one person's opinion.
3450000	3453000	You know, other people may not feel that way.
3453000	3454000	Yeah.
3454000	3457000	It's very concerning.
3457000	3467000	Yeah, no, I think that's been part, I mean, it was even part, you know, Dolly Too, a couple of years ago.
3467000	3474000	Dolly Too, if you would search for a person, you were very likely to get a person of color and female.
3474000	3483000	Because they were trying to create that balance.
3483000	3484000	Yeah.
3484000	3488000	And, you know, I'm interested in pig-to-human transplants.
3488000	3496000	You couldn't use the word pig because they decided that it's an, you know, insolving term and so on.
3496000	3504000	You know, they throw you out of the program if you can't ask them to use pig.
3504000	3510000	And it's possible that this has been going on all along.
3510000	3512000	Well, there was no large 90 pounds.
3512000	3516000	They were always trusting who would give us.
3516000	3522000	They always had the option of filtering, you know, weighting, shadow-daining.
3522000	3527000	And then we know that Twitter absolutely happened.
3527000	3535000	And so now, and now Lizzie Langlust, you have different views by Langlust,
3535000	3542000	but he actually, we should recognize that he is a proponent of free speech.
3542000	3549000	And that the mainstream media is hate-simple.
3549000	3552000	And they describe it in very negative terms.
3552000	3563000	You know, they have any favoritism, maybe they also disfavor Elon.
3563000	3571000	And his great sin, as far as I can tell, a lot of the great kinds of companies that do amazing things,
3571000	3580000	his great sin is he exposed the lack of free speech or exposed the bias on Twitter.
3580000	3584000	And, yeah, so I think we have to support him on that.
3584000	3586000	We have to say we want free speech.
3586000	3589000	We don't want that bias information.
3589000	3599000	One thing that would probably amuse you all is there was a student here, Lachini Batt,
3599000	3605000	who is now a third-year medical student at the University of Toronto.
3605000	3609000	She interviewed me about Elon Musk.
3609000	3617000	I gave her Elon Musk's biography at the time, and she read it, and she interviewed me.
3617000	3625000	Following that interview, you know, Google has these algorithms of how they pick the next video.
3625000	3634000	So because a lot of Elon Musk's ideas came to him at the Burning Man event,
3634000	3642000	you would go to a Kim Sola's video, and the next video would be a Burning Man video.
3642000	3644000	I've never been to Burning Man.
3644000	3647000	I don't think I would survive it.
3647000	3652000	I'm sort of unlike the people who go, but that still happens.
3652000	3663000	You can still find this strange linkage between Kim Sola's and Burning Man, all on account of that Elon Musk.
3663000	3669000	Well, I worked for Google and DeepMind for a number of years,
3669000	3678000	and as an expert in reinforcement planning, I was invited and asked to contribute to how this decision is made.
3678000	3686000	You know, they want to retain viewers, show them something interesting.
3686000	3689000	So I don't know how they're done now.
3689000	3703000	I don't know in particular whether they learn online or in offline, as we talked about.
3703000	3713000	But they do it some kind of way, it's a kind of testing.
3713000	3719000	I like to think it's neutral. They're just trying to retain eyeballs.
3719000	3725000	They're just trying to keep people enjoying, or at least continuing to interact.
3725000	3727000	Yeah, I like to think that.
3727000	3736000	But it looks like in addition to that, they're also trying to, in many cases, they're also trying to change viewpoints.
3736000	3737000	Bill?
3737000	3751000	Yeah, so this is a general fear of the future, is that AI, and maybe it's not AI, it's just the tech companies today.
3751000	3759000	The tech companies today are not really representative of the middle of the road views on things.
3759000	3765000	They're in some particular way, maybe it's a good way, but anyway, they're just a very particular way.
3766000	3779000	And so the fear is that these control the narrative.
3779000	3785000	They control what subjects are considered and what subjects just kind of disappear.
3785000	3800000	And this is a dystopia, when the information and views are not evolving naturally, but are controlled by a small group of people.
3800000	3816000	What's sort of fun is David Wood, who's the chair of London Futures, did a holiday event around Christmas about AI safety.
3816000	3824000	There had just been this Bletchley Park meeting, and sort of a follow up to that.
3824000	3831000	And I was one of maybe 11 or 12 speakers.
3831000	3838000	And this kind of thing came up a lot about biased views.
3838000	3850000	And for whatever reason, they tried to explain the technological reason why the video recording didn't work, but my presentation is the only one you can find.
3850000	3854000	Can't find any of the other presentations.
3854000	3859000	I didn't do anything, it's just, that's how it turned out.
3859000	3865000	They had this thing with 12 speakers, and you can only find a video for one of them, and it's me.
3865000	3868000	You tend to be pretty positive.
3868000	3871000	Were the others, you know, questioning?
3871000	3883000	No, no, many of them were talking about the fact you can't trust anybody, you know, AI is going to kill us all, all those kind of things, in various ways.
3883000	3894000	So those were removed from the points of view from this happy Christmas occasion that you can't find anywhere.
3895000	3909000	So now related to this, I know that, you know, you've talked about the AI, what I like to call the AI doers, those that think that we should need to be afraid of AI.
3909000	3913000	And I think that's very much what we're doing.
3913000	3916000	And I tend to see this part of the same thing.
3916000	3919000	Russell Graham is a way of saying this.
3919000	3927000	Authoritarianism, the new authoritarianism will not come like Jack Boole's violence.
3927000	3933000	It will come in the language of safety and care and convenience.
3933000	3936000	Right, we're trying to protect you.
3936000	3939000	We're forceful about it, though.
3939000	3944000	We will protect others who provide you with your saying as offensive, so you won't allow you to say it.
3944000	3947000	Right.
3947000	3960000	So I think that will, really, this isn't a struggle, but I'm an optimist, and I think the people trying to do that will lose out.
3960000	3966000	It will be unmasked the way they were unmasked through Twitter and the area.
3966000	3974000	We're sort of unmasked at Google for producing Gemini and being sort of unsuccessful as it was too obvious.
3974000	3978000	It was too obvious that it was trying to help people's views.
3978000	3985000	So this, you know, I started out by talking about you as what is your allegiance?
3985000	3990000	Is your allegiance to...
3990000	3996000	that there will be struggles over what you will identify most with?
3996000	4000000	Whether it will be your country, which is maybe becoming more authoritarian,
4000000	4003000	in the name of safety and care?
4003000	4008000	Or will it be we are more in common with other young people than other countries?
4008000	4022000	Also modern, digital, tech-savvy people?
4022000	4026000	Yeah, I read this one off. His name is Balaji. Balaji is from the Boston.
4026000	4029000	He says that the struggle, it was basically three.
4029000	4040000	One is the woke state, and one is, which includes things like the New York Times and the media,
4040000	4048000	and one is like the authoritarianism in China, the Communist Party.
4048000	4051000	And those are at odds right now.
4051000	4057000	The other is that China and the US struggle with each other for dominance.
4057000	4062000	One empire is fading and the other one is rising.
4062000	4066000	If there won't be a war, not at all.
4066000	4070000	And the third is the network.
4070000	4082000	There are a lot of people that are just empowered by the free exchange of ideas on the network.
4082000	4084000	Yeah, and I think Africa...
4084000	4086000	As a citizen of the network.
4086000	4090000	Yeah, Africa that we don't think about much.
4090000	4095000	But I think young people in Africa are finding a voice,
4095000	4098000	and I said a lot of people are sort of interested in that,
4098000	4106000	because they're not part of the power groups that they usually encounter.
4106000	4111000	So in my whole career, the most consequential thing I ever did, I think,
4111000	4116000	when you look at the history, it was in the year 2000.
4116000	4126000	We had a meeting in Nairobi, Kenya, and many of the people that I met are still leaders now.
4126000	4132000	They were young leaders then, and they're sort of mid-sage leaders now.
4132000	4138000	But it's really cool that a lot of the things we predicted then have happened,
4138000	4146000	and the plans we made then seemed to have kind of played out in a positive way.
4146000	4152000	A lot of the other meetings that I went to, you can now say,
4152000	4157000	well, that looks like a complete waste of time, or it was good for sightseeing,
4157000	4159000	but you accomplished nothing.
4159000	4166000	But that meeting in Kenya in 2000, we really did something.
4166000	4170000	And I think it's sort of worth thinking about that.
4170000	4178000	The areas of the world that are not in the news as much could really change things in the future.
4178000	4184000	There are news of them are suppressed.
4184000	4190000	Yeah, I think just as many things happen there, we don't hear about it.
4190000	4199000	There's one last thing I'd like to discuss in today's, today.
4199000	4207000	So, like Kim likes to say, likes to point out that I have gone to various meetings historically,
4207000	4215000	and made the case that the AIs are more like our kin or our descendants,
4215000	4223000	and that we should be open to them, rather than worrying that they're going to take over and kill us all.
4223000	4230000	And I have done such things, but it's not, it's not.
4230000	4235000	So I'd like to bring up another name, which is Hans Morley.
4235000	4240000	Because he sort of was making this pitch long before I was. I really learned it from him.
4240000	4252000	And so I think I have a slide on that.
4252000	4255000	Here's some more.
4255000	4261000	Here's my slide of the universe.
4261000	4263000	Here's my slide of the sentiment.
4263000	4270000	Yeah, so this is quotes from Hans's book in 1998.
4270000	4274000	Here's another one, 1988, that he was saying this for many years.
4274000	4282000	Yeah, bearing cataclysms, I consider the development of intelligent machines a near term inevitability.
4282000	4290000	I consider these future machines our progeny, but who's us to give them every advantage,
4290000	4295000	and then to bow out, and you can no longer contribute.
4295000	4304000	That's sort of, it seems like a very humble and insured attitude to the future.
4304000	4311000	Yeah, here's a little, this is a full quote.
4311000	4315000	Near term inevitability, rather quickly they would displace us,
4315000	4321000	because they would just be better, and it doesn't have to be in a rude way.
4321000	4324000	It's just, there are successors.
4324000	4326000	So you know, have to be alarmed.
4326000	4328000	You can say these future machines are progeny.
4328000	4329000	They're mind children.
4329000	4332000	They build our image and likeness ourselves in more point of form.
4332000	4338000	Like we all hope, if we have children, that they become smarter than us and more capable than us.
4338000	4344000	And just, are the AI's, should we consider them part of us?
4344000	4347000	Should we consider this the opposite of us?
4347000	4355000	That is our choice, which way we think about it.
4355000	4362000	So like these two slides, I think the students should be required to know about them on the exam.
4362000	4373000	Because from the beginning of the course, we've required them to repeat back what you said in January 2015
4373000	4375000	at the AI safety meeting.
4375000	4381000	And this is sort of an extension of that, or gives some of the background.
4381000	4388000	So I think it should also be a required part of the course.
4388000	4397000	Now this struggle, I think the, so I like to say, what's actually happened, you know, if we look at it sociologically.
4397000	4401000	The AI, so I would say the AI doomers have sort of won.
4401000	4408000	Because if you just read the paper and you read the meetings that people have, you're reading what people think.
4408000	4414000	You're concerned that the AI will somehow lead to some, you're a versatile catastrophe.
4414000	4415000	Is it common view?
4415000	4421000	I would say people can't really articulate why they feel this way, but they do feel this way.
4421000	4425000	So they've won sort of PR war to make people scared of AI.
4425000	4430000	And that's a great shame.
4430000	4433000	I was meeting, and that was some of them, just this last weekend.
4433000	4438000	And I said, well, you know, you guys have really won. This is what people think.
4438000	4441000	And they say, think of AI, they think parallel.
4441000	4448000	They think we're going to, if we're not going to be killed by them all, we're at least going to lose all our jobs.
4449000	4457000	And yeah, so I said to them, I said, well, I wish it wasn't true, but I think you guys have won the PR war.
4457000	4463000	And that's the fellow I said, no, not at all.
4463000	4465000	He thinks that I've won.
4465000	4471000	Those who are at ease with these developments have won.
4471000	4475000	Because they haven't stopped AI.
4475000	4480000	Their notion of success is it ends, and it never happens.
4480000	4486000	And the world does not develop AI.
4486000	4493000	But you know, there is a sort of Elon Musk part of this, too, which is the first one.
4493000	4496000	You know, there is one of those that is fearful.
4496000	4497000	Yeah, yeah.
4497000	4500000	And at the same time, he's doing it.
4500000	4505000	He's not suggesting attacking data centers.
4505000	4517000	And so I think his prominence will probably somehow keep the world from a kind of conflict where they attack data centers.
4517000	4526000	It just won't happen because, you know, Elon's there to sort of put out that particular flame.
4527000	4530000	He has a very interesting case.
4530000	4534000	I think his son's views have evolved.
4534000	4538000	You know, he's accepting now that it's going to happen.
4538000	4542000	So now he's made XAI, which is an open AI company, and originally he made open AI.
4542000	4544000	I mean, open AI was, it's going to happen.
4544000	4549000	So let's do it in an open way so that it doesn't become controlled by just a few.
4549000	4552000	And then open AI became closed AI.
4552000	4557000	And they are the few that are trying to keep it to themselves, Microsoft.
4557000	4559000	So now there's XAI.
4559000	4569000	XAI has its large language model called rock, which is much more open and truthful than the woke ones.
4569000	4579000	And it's like Elon now thinks, yes, AI is going to happen, and we just want to make sure it's done openly.
4579000	4586000	And then we want to make sure, you know, he wants it to be done well.
4586000	4597000	In his view, a well-structured, a well-goaled AI system is one that is curious about the world,
4597000	4600000	mainly just wants to come to understand the world.
4600000	4604000	Its goal is not to turn people into lips or anything like that,
4604000	4615000	but its goal is to understand and has humor and is not quite as serious as the woke language models.
4615000	4623000	So I think he's now in the camp that, I think he's making it, he's acknowledging,
4623000	4627000	so he's sort of famous for being someone who was afraid of it.
4627000	4633000	He's always quoted as someone who said AI is like somebody who's even and is so scary.
4633000	4637000	Now it's evolved to more realistic and then more productive on view.
4637000	4644000	I think that's all really good, a good model for how people's views might evolve.
4644000	4651000	So I want to bring that up and invite you all to be optimistic about it.
4651000	4654000	What is the main thing that's happening?
4654000	4657000	The main thing is not robots are rising up.
4657000	4665000	The main thing that's happening is we are understanding ourselves, we are understanding minds.
4665000	4674000	That's the big thing and it's just understanding the way we work has profound impacts.
4674000	4681000	But we have to believe, I have to believe, that it's going to be good if we understand ourselves.
4681000	4684000	We have a better opportunity to have a world peace.
4684000	4689000	We have a better opportunity to reproduce what's good about people.
4689000	4701000	And it's like the most profound scientific intellectual problem ever is to understand our own minds
4701000	4709000	and how you can make them more effective.
4709000	4725000	This has been a great teaching session and I think it's kind of cool the way your intentions and my intentions are aligned.
4725000	4730000	So yeah, that's good.
4730000	4732000	We're both very positive about it.
4732000	4737000	So what about the students? Was it good for you?
4737000	4746000	I always hate the kind of session where the senior people talk about how fantastic it was that all the junior people look mystified.
4746000	4751000	No, it was nice to meet you both. It was kind of fun that you were both so passionate.
4751000	4753000	So it was like what did I say?
4753000	4758000	Yeah, we know each other so long and we live in the same part of the city.
4758000	4772000	So sometimes when I'm mowing the lawn, Rich will be jogging down the road and he'll stop and I'll turn off the lawn mower and we'll solve the rule right there in the lawn.
4772000	4777000	So yeah, that's another thing you're made out of.
4777000	4781000	Okay, that's great. Thank you very much.
