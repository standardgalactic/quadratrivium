1
00:00:00,000 --> 00:00:04,720
Hi, I think we're ready to start, so my name is Paul Franklin and I'm a Neuroscientist

2
00:00:04,720 --> 00:00:09,120
here at Sick Kids, and also I'm Program Chair for the Canadian Neuroscience Meeting that

3
00:00:09,120 --> 00:00:12,680
takes place in Toronto all this week.

4
00:00:12,680 --> 00:00:19,080
So the traditional curtain raiser for the Neuroscience Meeting, the CAN meeting, is the public lecture.

5
00:00:19,080 --> 00:00:25,760
And this year we decided to focus on the interface between neuroscience and AI.

6
00:00:25,760 --> 00:00:27,760
We did that for two reasons.

7
00:00:27,760 --> 00:00:33,520
The first reason is that AI, or Toronto, is one of the main hubs in the world for AI research.

8
00:00:33,520 --> 00:00:39,320
And the second reason is that it's also home to one of the true pioneers in this field,

9
00:00:39,320 --> 00:00:42,800
also known as the godfather of deep learning, Jeff Hinton.

10
00:00:42,800 --> 00:00:48,080
And so when we asked Jeff if he'd participate in this event, I think a year and a half ago

11
00:00:48,080 --> 00:00:51,600
we asked him and he said, yes, we were super excited.

12
00:00:51,600 --> 00:00:58,000
So at this point I want to hand over to Blake Richards, and Blake Richards is an associate

13
00:00:58,000 --> 00:01:03,240
professor at University of Toronto Scarborough, and he's going to host this evening's event.

14
00:01:03,240 --> 00:01:04,240
Blake?

15
00:01:04,240 --> 00:01:06,000
Thanks, Paul.

16
00:01:06,000 --> 00:01:12,040
So just as a brief introduction, I wanted to tell you a little bit more about Jeff.

17
00:01:12,040 --> 00:01:18,640
So you might be surprised to learn that the godfather of deep learning, associated mostly

18
00:01:18,680 --> 00:01:25,560
with AI, got his BA in experimental psychology from Cambridge originally.

19
00:01:25,560 --> 00:01:32,840
He then went on to do his PhD in artificial intelligence in Edinburgh, so he did get

20
00:01:32,840 --> 00:01:35,000
started relatively early.

21
00:01:35,000 --> 00:01:41,040
But throughout his early career, he really contributed to the first wave of what was

22
00:01:41,040 --> 00:01:46,520
known at the time as parallel distributed processing or connectionism, which really brought back

23
00:01:46,520 --> 00:01:53,520
into the fore the idea of using neural networks for both models of the mind and for artificial

24
00:01:53,520 --> 00:01:55,360
intelligence.

25
00:01:55,360 --> 00:02:03,000
Now Jeff got his first tenure track position at Carnegie Mellon in the 80s, but we were

26
00:02:03,000 --> 00:02:09,480
able to steal him away from them in the late 80s, which I understand was largely because

27
00:02:09,480 --> 00:02:12,560
of his ethical objections to DARPA funding.

28
00:02:12,560 --> 00:02:22,040
So once again, Canada's political bent has helped us in our research endeavors.

29
00:02:22,040 --> 00:02:27,320
Over the course of the 90s, Jeff continued to really push neural networks and machine

30
00:02:27,320 --> 00:02:29,320
learning forward.

31
00:02:29,320 --> 00:02:31,200
And sorry about that.

32
00:02:31,200 --> 00:02:36,320
In 98, he actually went to University College London to found the Gatsby computational

33
00:02:36,320 --> 00:02:42,480
neuroscience unit, and we might have lost him, but thankfully we pulled him back again.

34
00:02:42,480 --> 00:02:48,360
He came back to Toronto in 2001, where he became a university professor in 2006 and

35
00:02:48,360 --> 00:02:51,600
then an emeritus professor in 2014.

36
00:02:51,600 --> 00:02:59,080
Now I think that you all know that Jeff is a monumental figure within artificial intelligence

37
00:02:59,080 --> 00:03:05,040
and machine learning, and he's been critical to the founding of the Vector Institute here

38
00:03:05,040 --> 00:03:09,400
in Toronto and putting Toronto on the map for AI.

39
00:03:09,400 --> 00:03:14,440
And certainly he's had incredible recognitions of his work, most recently the Turing Award,

40
00:03:14,440 --> 00:03:20,640
which he shared with Joshua Bengio and Yann LeCun, as well as the Order of Canada.

41
00:03:20,640 --> 00:03:25,200
And he's a distinguished fellow of the Canadian Institute for Advanced Research, which I highlight

42
00:03:25,200 --> 00:03:30,520
because they were one of the people who continued to support neural networks throughout the

43
00:03:30,520 --> 00:03:33,440
time when it wasn't as faddish.

44
00:03:33,440 --> 00:03:37,440
But for all his successes and his technical endeavors, I think one of the things that's

45
00:03:37,440 --> 00:03:42,360
most important to understand about Jeff is the impact that he's had on other scientists.

46
00:03:42,360 --> 00:03:46,840
Jeff has really molded the career of so many people and changed the way that they think

47
00:03:46,840 --> 00:03:49,560
about things.

48
00:03:49,560 --> 00:03:54,560
When you look at the people who have been his graduate students or postdocs, it really

49
00:03:54,560 --> 00:03:57,840
is the who's who in artificial intelligence.

50
00:03:57,840 --> 00:04:07,320
It includes people like Max Welling, Yann LeCun, and you know, the phrase I used to describe

51
00:04:07,400 --> 00:04:12,160
it is, have you drunk Jeff's Kool-Aid?

52
00:04:12,160 --> 00:04:15,360
Because once you've drunk Jeff's Kool-Aid, there is no going back.

53
00:04:15,360 --> 00:04:22,200
You see neural networks, you see AI differently, and I would argue you also see neuroscience

54
00:04:22,200 --> 00:04:23,480
differently.

55
00:04:23,480 --> 00:04:31,320
And for me, my understanding of the brain has been largely shaped by Jeff and his work.

56
00:04:31,320 --> 00:04:36,080
But you know, we're at the point now where computer science has drunk Jeff's Kool-Aid.

57
00:04:36,080 --> 00:04:42,160
So he's got an H index of 145, and according to Google Scholar, his work has been cited

58
00:04:42,160 --> 00:04:49,040
270,000 times, which is more than Einstein, Ramoni, Cajal, and Alan Tern combined.

59
00:04:49,040 --> 00:04:51,520
But that's largely from computer scientists.

60
00:04:51,520 --> 00:04:58,040
And if my prediction is correct, neuroscience 30, 40 years from now will also have drunk

61
00:04:58,040 --> 00:05:01,480
Jeff's Kool-Aid, and maybe you're going to get your first taste tonight.

62
00:05:01,480 --> 00:05:04,040
So with that, I hand you over to Jeffrey Hinton.

63
00:05:06,080 --> 00:05:14,920
So thank you very much, Blake.

64
00:05:14,920 --> 00:05:17,920
I can give you some more Kool-Aid today.

65
00:05:17,920 --> 00:05:21,360
It's Kool-Aid produced by one of my former students, Ilya Sutskava.

66
00:05:21,360 --> 00:05:26,480
First, I want to tell you a little bit about the history of deep learning in AI.

67
00:05:26,480 --> 00:05:30,600
Can I just ask before I start, how many people here know what the back propagation algorithm

68
00:05:30,600 --> 00:05:31,600
is?

69
00:05:31,600 --> 00:05:33,880
Put your hands up.

70
00:05:33,880 --> 00:05:35,680
So some people don't.

71
00:05:35,680 --> 00:05:39,640
I'll explain it very quickly, and I'll explain it in such a way that you'll be able to explain

72
00:05:39,640 --> 00:05:40,640
to other people.

73
00:05:40,640 --> 00:05:43,960
So if you do know what it is, you follow the explanation from the point of view of how

74
00:05:43,960 --> 00:05:44,960
you explain it.

75
00:05:44,960 --> 00:05:45,960
Okay.

76
00:05:45,960 --> 00:05:48,320
There was a war between two paradigms for AI.

77
00:05:48,320 --> 00:05:53,320
There were people who thought that the essence of intelligence was reasoning, and logic is

78
00:05:53,320 --> 00:05:58,880
what does reasoning, so we should base artificial intelligence on taking strings of symbols and

79
00:05:58,880 --> 00:06:02,360
manipulating them to arrive at conclusions.

80
00:06:02,360 --> 00:06:05,840
And then there were other people who looked at the brain and said, no, no, intelligence

81
00:06:05,840 --> 00:06:10,200
is all about adapting connections in the brain to get smarter.

82
00:06:10,200 --> 00:06:17,080
And this war went on for a long time, and eventually, people who were trying to figure

83
00:06:17,080 --> 00:06:23,880
out how to change connections between fake neurons to make these networks smarter got

84
00:06:23,880 --> 00:06:28,920
to be able to do things that the people doing symbolic AI just couldn't do at all.

85
00:06:28,920 --> 00:06:33,320
And now there's a different way of getting a computer to do what you want.

86
00:06:33,320 --> 00:06:38,240
Instead of programming it, which is tedious, you just showed examples, and it figures it

87
00:06:38,240 --> 00:06:39,240
out.

88
00:06:39,240 --> 00:06:42,440
Now, of course, you have to write the program that figures it out, but that's just one program

89
00:06:42,440 --> 00:06:45,960
that will then do everything.

90
00:06:45,960 --> 00:06:50,000
And this is an example of what it can do.

91
00:06:50,000 --> 00:06:52,880
So the image, just think of the numbers.

92
00:06:52,880 --> 00:06:57,200
They're RGB values of pixels, and that's the input to the computer.

93
00:06:57,200 --> 00:07:02,640
Lots of values of pixels, just real numbers, saying how bright the red channel is.

94
00:07:02,640 --> 00:07:07,960
And you have to turn those numbers into a string of words that says a close-up of a

95
00:07:07,960 --> 00:07:09,920
child holding a stuffed animal.

96
00:07:09,920 --> 00:07:11,440
And imagine writing that program.

97
00:07:11,440 --> 00:07:14,960
Well, people in conventional AI had tried to write that program and they couldn't, partly

98
00:07:14,960 --> 00:07:17,760
because they didn't know how we did it.

99
00:07:17,760 --> 00:07:21,760
We still don't know how we do it, but we can get artificial neural networks to do it now

100
00:07:21,760 --> 00:07:24,320
and do a pretty good job.

101
00:07:24,440 --> 00:07:31,160
My prediction is, within 10 years, if you go and get a CT scan, what will happen is

102
00:07:31,160 --> 00:07:36,840
a computer will look at the CT scan, and a computer will produce the written report

103
00:07:36,840 --> 00:07:39,960
that the radiologist currently produces.

104
00:07:39,960 --> 00:07:43,120
Radiologists don't like this idea.

105
00:07:43,120 --> 00:07:44,120
Okay.

106
00:07:44,120 --> 00:07:46,120
Here's a simplified model of a neuron.

107
00:07:46,120 --> 00:07:47,120
It's very simple.

108
00:07:47,120 --> 00:07:51,520
It gets some input, which is just the activity on the input lines times the weights, adds

109
00:07:51,520 --> 00:07:52,520
it all up.

110
00:07:52,720 --> 00:07:54,560
That's called the depolarization.

111
00:07:54,560 --> 00:08:00,960
And then it gives an output that's proportional to how much input it gets as long as it gets

112
00:08:00,960 --> 00:08:02,480
enough input.

113
00:08:02,480 --> 00:08:04,360
And so, to begin with, we won't have spiking neurons.

114
00:08:04,360 --> 00:08:08,960
These are just going to be neurons that send real values in just the way neurons don't.

115
00:08:08,960 --> 00:08:13,880
We're going to make networks of them by hooking them up into layers.

116
00:08:13,880 --> 00:08:16,880
And you could put some pixels on the input neurons.

117
00:08:16,880 --> 00:08:17,880
Look.

118
00:08:17,880 --> 00:08:21,080
They're the input neurons.

119
00:08:21,080 --> 00:08:25,440
And you go forwards through the net until you get outputs.

120
00:08:25,440 --> 00:08:28,200
And then you compare those outputs with what you ought to have got, so you have to know

121
00:08:28,200 --> 00:08:30,440
what the right answer is.

122
00:08:30,440 --> 00:08:36,400
And what we'd like to do is train the weights, these red and green dots, so that it gives

123
00:08:36,400 --> 00:08:37,400
the right output.

124
00:08:37,400 --> 00:08:42,360
Now, I'm going to show you a way of training the weights that everybody can understand,

125
00:08:42,360 --> 00:08:47,120
and everybody is thought of, basically.

126
00:08:47,120 --> 00:08:54,280
What you do is you start with random weights, you show it some inputs, you measure how well

127
00:08:54,280 --> 00:08:58,200
it does, then you change one weight a tiny bit.

128
00:08:58,200 --> 00:09:02,680
So I take that weight there, and I just change it a tiny bit, and then I show it the same

129
00:09:02,680 --> 00:09:05,280
inputs again and see if it does better or worse.

130
00:09:05,280 --> 00:09:06,680
If it does better, I keep the change.

131
00:09:06,680 --> 00:09:10,920
If it does worse, maybe I keep the change in the opposite direction.

132
00:09:10,920 --> 00:09:12,400
That's an easy algorithm to understand.

133
00:09:12,400 --> 00:09:13,400
And that algorithm works.

134
00:09:13,400 --> 00:09:14,400
It's just incredibly slow.

135
00:09:14,400 --> 00:09:19,480
You have to show it lots of examples, change your weight, and then show it lots more examples,

136
00:09:19,480 --> 00:09:20,480
change another weight.

137
00:09:20,480 --> 00:09:22,480
And every weight has to be changed many times.

138
00:09:22,480 --> 00:09:28,840
So if you use calculus, you can go millions of times faster.

139
00:09:28,840 --> 00:09:32,320
So the trick of this algorithm, the sort of mutation algorithm, is you have to measure

140
00:09:32,320 --> 00:09:34,600
the effect of the weight change on the performance.

141
00:09:34,600 --> 00:09:42,080
But you don't really need to measure it, because when I change one of these weights, the effect

142
00:09:42,080 --> 00:09:45,440
that it has on the output is determined by the network.

143
00:09:45,440 --> 00:09:47,600
It just depends on the other weights in the network.

144
00:09:47,600 --> 00:09:51,000
It's not like normal evolution, where the effect of a gene depends on the environment

145
00:09:51,000 --> 00:09:52,000
you're in.

146
00:09:52,000 --> 00:09:54,000
This is all kind of internal to the brain.

147
00:09:54,000 --> 00:09:57,720
And so changing one of these weights has an effect that's predictable here.

148
00:09:57,720 --> 00:10:03,160
So I ought to be able to predict how changing the weight will help get the right output.

149
00:10:03,160 --> 00:10:08,920
And so what back propagation does is basically says, I'm going to compute using an algorithm,

150
00:10:08,920 --> 00:10:12,400
the details of which I won't tell you, and compute for every weight, all at the same

151
00:10:12,400 --> 00:10:17,320
time, how changing that weight would improve the output.

152
00:10:17,320 --> 00:10:19,920
And then I'm going to change all the weights a little bit.

153
00:10:19,920 --> 00:10:22,840
So every weight changes in direction to improve the output, and the output improves quite

154
00:10:22,840 --> 00:10:26,440
a bit, and then I do it all again.

155
00:10:26,440 --> 00:10:33,760
Now that allows me to compute for every weight what direction I'd like to change it in.

156
00:10:33,760 --> 00:10:38,560
And the question is, should I, when I show examples, show all of the examples, and then

157
00:10:38,560 --> 00:10:41,960
update the weights, so should you live your whole life with the synapse strengths you're

158
00:10:41,960 --> 00:10:45,600
born with, then update your weights a little bit, then live your life again, and update

159
00:10:45,600 --> 00:10:54,120
the weights a little bit more, that doesn't seem very good, or should you take one case

160
00:10:54,120 --> 00:10:58,120
or a few cases, figure out how you'd like to update the weights, update them, and then

161
00:10:58,120 --> 00:10:59,120
take more cases.

162
00:10:59,120 --> 00:11:02,960
That's the online algorithm, and that's what we do.

163
00:11:02,960 --> 00:11:04,920
And the amazing thing is, it works.

164
00:11:04,920 --> 00:11:08,120
You can take one case at a time, or you can take small batch of cases, you update the

165
00:11:08,120 --> 00:11:11,640
weights, and these networks get better.

166
00:11:11,640 --> 00:11:15,200
And it's very surprising how well it works on big data sets.

167
00:11:15,200 --> 00:11:20,880
So for a long time, people thought, you're never going to be able to learn something

168
00:11:20,880 --> 00:11:26,560
complicated, like, for example, take a string of words in English, feed them into a neural

169
00:11:26,560 --> 00:11:31,320
net, and output a string of words in French that mean the same thing.

170
00:11:31,320 --> 00:11:33,600
You're never going to be able to do that if you start with a big neural net with just

171
00:11:33,600 --> 00:11:34,600
random weights.

172
00:11:34,600 --> 00:11:38,080
It's just asking too much for the neural net to organize itself so it can do transactions

173
00:11:39,040 --> 00:11:43,120
because you have to kind of understand what the English says.

174
00:11:43,120 --> 00:11:47,920
And people predicted this was completely impossible, but you'd have to put in lots of prior knowledge.

175
00:11:47,920 --> 00:11:52,680
Well, they were wrong.

176
00:11:52,680 --> 00:12:00,200
So in 2009, my students in Toronto showed that you could actually improve speech recognizers

177
00:12:00,200 --> 00:12:02,800
using these neural nets that had random weights.

178
00:12:02,800 --> 00:12:07,520
They were just trying to predict in a spectrogram which piece of which phoneme you were trying

179
00:12:07,520 --> 00:12:09,760
to say in the middle of the spectrogram.

180
00:12:09,760 --> 00:12:13,440
And then there was more to the system that wasn't neural nets.

181
00:12:13,440 --> 00:12:17,640
Now what we've done is we've got rid of all the stuff that wasn't neural nets, and now

182
00:12:17,640 --> 00:12:23,960
you can take sound waves coming in and you have transcriptions coming out, or even better,

183
00:12:23,960 --> 00:12:27,600
you have sound waves coming in and you have sound waves coming out in another language

184
00:12:27,600 --> 00:12:29,960
with the same accent.

185
00:12:29,960 --> 00:12:31,800
They can do that now.

186
00:12:31,800 --> 00:12:34,000
That's speech recognition done.

187
00:12:34,000 --> 00:12:40,040
And in 2012, two of my students took a big database of images and used essentially the

188
00:12:40,040 --> 00:12:45,960
same algorithm, the few clever tricks, to say what was in the image.

189
00:12:45,960 --> 00:12:51,320
Not a full caption, just the class of the most obvious object, and they did much better

190
00:12:51,320 --> 00:12:55,720
than conventional computer vision, which had been going for many years, and since then

191
00:12:55,720 --> 00:12:58,600
all the best recognizers have used neural nets.

192
00:12:58,600 --> 00:13:03,880
In 2011, you couldn't publish a paper out neural nets in the standard computer vision

193
00:13:03,880 --> 00:13:06,240
conference because they said they were rubbish.

194
00:13:06,240 --> 00:13:11,720
In 2014, you couldn't publish a paper that wasn't about neural nets.

195
00:13:11,720 --> 00:13:15,000
And in 2014, they did something that I didn't expect.

196
00:13:15,000 --> 00:13:20,120
This was done by people at Google, not me, and Joshua Benjo in his group in Montreal,

197
00:13:20,120 --> 00:13:24,000
particularly by a guy called Bardenao and Cho.

198
00:13:24,000 --> 00:13:31,360
They managed to get a neural net, so you feed in actually fragments of words in one language.

199
00:13:31,360 --> 00:13:33,560
You have 32,000 possible fragments.

200
00:13:33,560 --> 00:13:36,960
So the word the in English would be one of the fragments, but so with things like ing

201
00:13:36,960 --> 00:13:38,760
and un.

202
00:13:38,760 --> 00:13:42,360
And what comes out in another language is fragments of words in that other language, and it's

203
00:13:42,360 --> 00:13:46,800
a pretty good translation, and that's how Google now does translation.

204
00:13:46,800 --> 00:13:53,240
So it did translation better than symbolic AI.

205
00:13:53,240 --> 00:13:56,560
So what changed between 1986 and 2009?

206
00:13:56,560 --> 00:13:58,520
And it was basically computers got faster.

207
00:13:58,520 --> 00:13:59,520
That was the main change.

208
00:13:59,520 --> 00:14:00,960
Data sets got bigger.

209
00:14:00,960 --> 00:14:05,440
We developed some clever tricks, and we like to emphasize those, but it was really the

210
00:14:05,440 --> 00:14:07,480
computers getting faster and data sets getting bigger.

211
00:14:07,480 --> 00:14:10,520
But I'll emphasize the clever tricks nonetheless.

212
00:14:10,520 --> 00:14:11,920
And I can tell you about two clever tricks.

213
00:14:11,920 --> 00:14:15,280
I can tell you about transformers, and I can tell you about better ways of stopping neural

214
00:14:15,280 --> 00:14:19,320
networks from overfitting.

215
00:14:19,320 --> 00:14:25,520
But first I want to show you an example of what neural nets can do now.

216
00:14:25,520 --> 00:14:32,400
So a team at OpenAI took work on transformers that was originally done at Google.

217
00:14:32,400 --> 00:14:36,440
They developed it a little bit further, and they applied it to big neural nets that have

218
00:14:36,440 --> 00:14:41,480
1.5 billion learnable connection strengths.

219
00:14:41,480 --> 00:14:43,560
So they're learning 1.5 billion numbers.

220
00:14:43,560 --> 00:14:45,280
That's the knowledge of the system.

221
00:14:45,280 --> 00:14:50,920
And they train it up on billions of words of English text, and all the net's trying

222
00:14:50,920 --> 00:14:53,920
to do is predict the next word.

223
00:14:53,920 --> 00:14:58,800
So what the net will do, or fragment of word, the net will give you probabilities for the

224
00:14:58,800 --> 00:14:59,800
next word.

225
00:14:59,800 --> 00:15:03,960
So if you give it some words, a lead-in, it'll give you probabilities for the next word.

226
00:15:03,960 --> 00:15:08,040
And once the net's trained, what you can do is you can look at those probabilities,

227
00:15:08,040 --> 00:15:13,800
and if it says there's a probability of 0.4 that the next word is the, you pick the with

228
00:15:13,800 --> 00:15:19,080
probability 9.4, and if it says fish with probability 0.01, you pick fish with probability

229
00:15:19,080 --> 00:15:20,080
0.01.

230
00:15:20,080 --> 00:15:24,160
And so you just pick from its distribution, and then you tell the neural net, okay, the

231
00:15:24,160 --> 00:15:27,840
one I picked was the next word, what do you think comes after that?

232
00:15:27,840 --> 00:15:31,920
And this way you can get it to sort of reveal what it really believes about the world.

233
00:15:31,920 --> 00:15:34,840
So you're getting it to predict words one at a time, and every time it makes a prediction

234
00:15:34,840 --> 00:15:39,360
you say you were right, and it just gets more and more carried away.

235
00:15:39,360 --> 00:15:45,520
So they initiated it with some interesting text.

236
00:15:45,520 --> 00:15:49,560
And the question is, will the neural net then produce stuff that's sort of related to that?

237
00:15:49,560 --> 00:15:52,400
I mean, the first question is, will it produce English words?

238
00:15:52,400 --> 00:15:54,520
Will the words have decent syntax?

239
00:15:54,520 --> 00:15:56,000
Will it have any meaning?

240
00:15:56,000 --> 00:15:58,280
Will it be related to this?

241
00:15:58,280 --> 00:16:02,560
If you're really optimistic, you might say, will they sort of relate to the fundamental

242
00:16:02,560 --> 00:16:05,400
problem here, which is how these unicorns can speak English?

243
00:16:05,400 --> 00:16:07,360
Okay, so here goes.

244
00:16:07,360 --> 00:16:08,960
This is what the neural net produced.

245
00:16:08,960 --> 00:16:10,240
Now this was cherry-picked.

246
00:16:10,240 --> 00:16:15,360
This was one of their better examples.

247
00:16:15,360 --> 00:16:21,720
The neural net just made this up, right?

248
00:16:21,720 --> 00:16:27,560
It made up Dr. Jorge Perez, there is no such person at the University of La Paz, but it's

249
00:16:27,560 --> 00:16:32,160
pretty plausible because it's South America, and I believe La Paz has a university.

250
00:16:32,160 --> 00:16:37,960
Okay, so that's the first bit of what it made up, and it carries on and it gets better.

251
00:16:38,960 --> 00:16:42,960
The next bit sounds a bit like one of those fantasy games.

252
00:16:50,960 --> 00:16:53,440
So it's remembered about unicorns and herds of unicorns, right?

253
00:16:53,440 --> 00:16:56,880
So they walk up and there's this strange valley, and it's a very strange valley, and they found

254
00:16:56,880 --> 00:17:03,360
the herds of unicorns.

255
00:17:03,360 --> 00:17:06,040
And it has something about seeing them from the air and being able to touch them, which

256
00:17:06,040 --> 00:17:07,320
isn't quite right.

257
00:17:07,320 --> 00:17:10,880
So people in Symbolicae leap on this and say, you see, it doesn't understand.

258
00:17:10,880 --> 00:17:16,080
Well, sure, there's little bits that it doesn't get right.

259
00:17:16,080 --> 00:17:20,400
But notice, it's remembered that these unicorns have to speak English, and so it tells you

260
00:17:20,400 --> 00:17:25,560
about, you know, they spoke some fairly regular English.

261
00:17:25,560 --> 00:17:30,200
It doesn't know the difference between dialect and dialectic, but my kids don't know that

262
00:17:30,200 --> 00:17:31,200
either.

263
00:17:31,200 --> 00:17:37,280
In fact, I'm not sure I know.

264
00:17:37,280 --> 00:17:44,800
It's a tribute to unicorns to Argentina, even though Dr. Perez comes from Bolivia.

265
00:17:44,800 --> 00:17:48,000
And it actually understands about magic realism.

266
00:17:48,000 --> 00:17:54,720
So they're descendants of a lost race, and I love the bit at the end where it says, in

267
00:17:54,720 --> 00:17:59,120
South America, such incidents seem to be quite common.

268
00:17:59,120 --> 00:18:04,600
This has an ability to just make up something that fits your prejudices and sounds moderately

269
00:18:04,600 --> 00:18:12,760
plausible, like a certain president.

270
00:18:12,760 --> 00:18:17,560
And it finally gets to the point which is, if you really want to know whether these unicorns

271
00:18:17,560 --> 00:18:23,520
were used by breeding with these strange lost race of people, you ought to do a DNA test.

272
00:18:23,520 --> 00:18:24,520
Okay?

273
00:18:24,520 --> 00:18:26,000
It understands that.

274
00:18:26,000 --> 00:18:27,000
Okay.

275
00:18:27,000 --> 00:18:30,440
So that's what neural nets can do now.

276
00:18:30,440 --> 00:18:38,120
This was a neural net with 1.5 billion connections that was trained on Google's, actually, I

277
00:18:38,120 --> 00:18:44,280
withdraw that, 1.5 billion connections is trained on a lot of hardware.

278
00:18:44,280 --> 00:18:50,160
And we look at what it says, and we sort of laugh at how, you know, it's pretty good,

279
00:18:50,160 --> 00:18:52,800
but it hasn't got it quite right, but it's pretty good.

280
00:18:52,800 --> 00:18:53,800
Okay.

281
00:18:53,800 --> 00:18:59,720
What they've done now is they've trained a neural net with 50 billion connections on

282
00:18:59,720 --> 00:19:05,720
Google's latest cloud hardware, which is, it's like having several of the world's biggest

283
00:19:05,720 --> 00:19:09,840
supercomputers going for you for months.

284
00:19:09,840 --> 00:19:14,520
The net with 50 billion connections, I haven't seen any text from it yet, but my prediction

285
00:19:14,520 --> 00:19:19,600
is it's sitting around laughing at how cute what we produce is.

286
00:19:19,600 --> 00:19:21,160
Okay.

287
00:19:21,160 --> 00:19:29,240
So one thing about that net is it's clearly very well aware of the initial context.

288
00:19:29,240 --> 00:19:34,720
These unicorns in a valley that speak English, and it's remembering this initial context

289
00:19:34,720 --> 00:19:37,480
a long time later, and a recurrent neural net can't do that.

290
00:19:37,480 --> 00:19:40,480
A recurrent neural net would have forgotten about the initial stuff and wouldn't produce

291
00:19:40,480 --> 00:19:43,560
such good context-dependent stuff.

292
00:19:43,560 --> 00:19:52,040
So the way this works is the word comes in, the neural net activates some hidden units.

293
00:19:52,040 --> 00:19:57,520
That pattern of activity in the hidden units goes and compares itself with previous patterns,

294
00:19:57,960 --> 00:20:02,560
your point of view, with previous patterns at earlier times.

295
00:20:02,560 --> 00:20:06,720
And when it finds a pattern at an earlier time that's a bit similar, it says that we'll

296
00:20:06,720 --> 00:20:11,880
take advice from that previous hidden pattern about how to affect the next layer.

297
00:20:11,880 --> 00:20:19,640
And so actually a word comes in and how one pattern of activity in the bottom layer of

298
00:20:19,640 --> 00:20:26,800
the hidden neurons affects the next layer is dependent on what happened previously.

299
00:20:26,800 --> 00:20:31,040
Now it's dependent in quite a complicated way, and this seems very implausible for a

300
00:20:31,040 --> 00:20:36,800
brain because what's happening in the computer is you're storing all these activity patterns

301
00:20:36,800 --> 00:20:39,800
that are meant to be neural activity patterns or light neural activity patterns, and you're

302
00:20:39,800 --> 00:20:43,320
comparing, and this looks hopeless.

303
00:20:43,320 --> 00:20:49,080
But actually all you need to do is every time you have an activity pattern, and you use

304
00:20:49,080 --> 00:20:53,760
the outgoing weights to affect the next layer, just change the weight slightly with heavy

305
00:20:53,760 --> 00:20:55,560
and learning.

306
00:20:55,560 --> 00:21:01,480
So now what's going to happen is that weight matrix that comes out of that activity pattern

307
00:21:01,480 --> 00:21:03,200
is going to be modified slightly.

308
00:21:03,200 --> 00:21:08,200
Now when I get a new activity pattern, if the activity pattern is orthogonal to the previous

309
00:21:08,200 --> 00:21:13,160
activity pattern, then any modifications you made in the weight matrix due to that previous

310
00:21:13,160 --> 00:21:15,400
activity pattern won't make any difference.

311
00:21:15,400 --> 00:21:19,760
But if it lines up with the previous activity pattern, if it's similar, the modifications

312
00:21:19,760 --> 00:21:25,040
you made in the weights back there, the temporary modifications, will cause this new activity

313
00:21:25,040 --> 00:21:27,360
pattern to have a different effect here.

314
00:21:27,360 --> 00:21:31,880
So you'll get that long temporal context, and the way to store a long temporal context

315
00:21:31,880 --> 00:21:39,280
is not to keep copies of neural activity patterns, it's to take your weights and to have temporary

316
00:21:39,280 --> 00:21:41,600
changes to the weights, which I call fast weights.

317
00:21:41,600 --> 00:21:48,640
So you temporarily change them, and these changes decay over time, so you'll have a memory.

318
00:21:48,640 --> 00:21:52,800
So if you ask, where in your brain is your memory of what I said a few minutes ago?

319
00:21:53,360 --> 00:21:56,160
I'll ask the younger people this, because for the older people it's nowhere.

320
00:21:56,160 --> 00:22:01,720
But for the younger people, it's somewhere you can, if I were to say something I said

321
00:22:01,720 --> 00:22:07,080
a few minutes ago, like these big neural nets are now laughing at us, you remember I said

322
00:22:07,080 --> 00:22:09,520
that, where was that memory?

323
00:22:09,520 --> 00:22:14,040
I think it's in the temporary changes to the weights, because that's got much bigger capacity

324
00:22:14,040 --> 00:22:20,080
than activities of neurons, and you don't need to use up neurons just sitting there remembering.

325
00:22:20,080 --> 00:22:23,360
And those temporary changes don't need to be driven by back propagation, they can just

326
00:22:23,360 --> 00:22:25,360
be heavier.

327
00:22:25,360 --> 00:22:33,800
Okay, so I've tried to relate these wonderful nets that can make up stories with an idea

328
00:22:33,800 --> 00:22:37,800
about where short-term memory is in the brain.

329
00:22:37,800 --> 00:22:41,520
And now I'll talk about where the cortex can do back propagation.

330
00:22:41,520 --> 00:22:48,560
So neuroscientists, 20 years ago neuroscientists said don't be ridiculous, of course the brain

331
00:22:48,560 --> 00:22:52,960
can't do back propagation, and they'd interpret it very literally as sending signals backwards

332
00:22:52,960 --> 00:23:03,600
down the same axons and saying neurons don't do that, no thanks.

333
00:23:03,600 --> 00:23:08,760
But now we know that back propagation works really well for solving tough practical problems.

334
00:23:08,760 --> 00:23:12,320
So that's rather changed the balance, because when back propagation was just a theory of

335
00:23:12,320 --> 00:23:17,560
how you might get computers to learn something, and when it learns some simple things, it

336
00:23:17,560 --> 00:23:20,800
wasn't sort of imperative to understand whether the brain did it.

337
00:23:20,800 --> 00:23:24,920
But now we know that you can do all these things with back propagation.

338
00:23:24,920 --> 00:23:29,000
What's more, we know that back propagation is the right thing to do, but if you have

339
00:23:29,000 --> 00:23:35,320
a sensory pathway, and you want to take the early feature detectors so that their outputs

340
00:23:35,320 --> 00:23:41,600
are more helpful for making the right decision later on in the system, then what you really

341
00:23:41,600 --> 00:23:46,560
need to do is ask the question, how should I change the receptive field of this early

342
00:23:46,560 --> 00:23:52,760
detector so that what is output helps with the decision?

343
00:23:52,760 --> 00:23:55,480
And what you have to do is do back propagation to compute that, that's the efficient way

344
00:23:55,480 --> 00:23:56,480
to compute it.

345
00:23:56,480 --> 00:24:02,280
And I think it'd be crazy if the brain wasn't somehow doing this.

346
00:24:02,280 --> 00:24:05,480
So why do neuroscientists think it's impossible?

347
00:24:05,480 --> 00:24:11,240
Apart from silly objections like things don't go backwards down axons, at least not at the

348
00:24:11,240 --> 00:24:20,240
right speed.

349
00:24:20,240 --> 00:24:22,240
He wants me to update things.

350
00:24:22,240 --> 00:24:30,280
Oh, it's just died.

351
00:24:30,280 --> 00:24:33,640
I'm going to go out and present them out.

352
00:24:34,640 --> 00:24:41,640
I'm going to go back in to present them out.

353
00:24:41,640 --> 00:24:46,800
Okay, so here's some reasons why the brain can't do back propagation.

354
00:24:46,800 --> 00:24:52,240
The first reason is they say, well, it doesn't get the supervision signal.

355
00:24:52,240 --> 00:24:55,720
And they're imagining that the supervision signal is like you take a micro pipette and

356
00:24:55,720 --> 00:25:00,320
you put it into the infrotemporal cortex and you inject the right answer and the brain

357
00:25:00,320 --> 00:25:03,240
doesn't have anything like that, right?

358
00:25:03,240 --> 00:25:08,440
But actually, if you take that language model, it didn't need label data, it was just trying

359
00:25:08,440 --> 00:25:09,960
to predict the next word.

360
00:25:09,960 --> 00:25:14,480
So you can often use part of the input, maybe a future part of the input, or maybe a small

361
00:25:14,480 --> 00:25:20,040
part of an image as the right answer, and so you can get supervision signals easily.

362
00:25:20,040 --> 00:25:22,800
So there's no problem about supervision signals.

363
00:25:22,800 --> 00:25:27,800
The second reason is neurons don't send real-valued activities, they send spikes.

364
00:25:27,800 --> 00:25:32,360
And back propagation is using these real-valued activities so you can get nice, smooth derivatives.

365
00:25:32,360 --> 00:25:36,040
So back propagation can't possibly be what's going on in the brain.

366
00:25:36,040 --> 00:25:39,000
The third objection is, neurons have to send two signals.

367
00:25:39,000 --> 00:25:45,240
They have to send the activity forwards and they have to send error derivatives backwards.

368
00:25:45,240 --> 00:25:50,440
The signal they have to send backwards is, how sensitive am I to changes in my input?

369
00:25:50,440 --> 00:25:58,640
Or rather, if you change my input, how much does that help with the final answer?

370
00:25:58,640 --> 00:26:03,000
And the last thing is about neurons having reciprocal connections because you have to,

371
00:26:03,000 --> 00:26:08,480
when you send things backwards, if you use a different neuron, you have to use the same

372
00:26:08,480 --> 00:26:11,520
weight as the forward weight.

373
00:26:11,520 --> 00:26:17,000
I'm not going to tell you how you can overcome that, but you can easily.

374
00:26:17,000 --> 00:26:21,760
So supervision signals isn't really a problem, there's many ways to get a supervision signal.

375
00:26:21,760 --> 00:26:27,000
The simplest is predicting what comes next.

376
00:26:27,000 --> 00:26:31,960
Now the question of can neurons communicate real values?

377
00:26:31,960 --> 00:26:36,560
Well the first thing to notice about back propagation is, if you have very noisy estimates

378
00:26:36,560 --> 00:26:39,920
of the gradient, it works just as well.

379
00:26:39,920 --> 00:26:43,760
It's very, very tolerant of noise as long as it's unbiased noise.

380
00:26:43,760 --> 00:26:48,000
So for example, the signal you send forwards can be one bit, one stochastic bit, and the

381
00:26:48,000 --> 00:26:50,520
signal you send backwards can be two bits.

382
00:26:50,520 --> 00:26:55,120
If they have the right average value, if their expected values are correct, then they're

383
00:26:55,120 --> 00:27:02,320
just this expected value plus some noise, and the whole system still works fine.

384
00:27:02,320 --> 00:27:06,800
So in the brain, you have a neuron.

385
00:27:06,800 --> 00:27:12,320
At any instant, the neuron has an underlying firing rate, and it produces spikes, and for

386
00:27:12,320 --> 00:27:17,440
now let's just suppose it produces spikes according to a Poisson process.

387
00:27:17,440 --> 00:27:20,920
So it's probability of producing a spike in a small interval, which is the underlying

388
00:27:20,920 --> 00:27:23,320
firing rate.

389
00:27:23,520 --> 00:27:29,640
The question is, suppose we treated it as if it could send that underlying firing rate.

390
00:27:29,640 --> 00:27:33,160
When it sends a Poisson spike, it's just a very noisy version of the underlying firing

391
00:27:33,160 --> 00:27:35,160
rate.

392
00:27:35,160 --> 00:27:40,520
It's a one or a zero, but its expected value is the underlying firing rate.

393
00:27:40,520 --> 00:27:47,280
So how well do neural networks work if we send very noisy signals?

394
00:27:47,280 --> 00:27:52,880
So I'm going to have a statistics digression.

395
00:27:52,880 --> 00:27:57,400
If you do statistics 101, they tell you you shouldn't have more parameters in your model

396
00:27:57,400 --> 00:27:59,280
than you have data points.

397
00:27:59,280 --> 00:28:03,080
You really ought to have quite a few data points for each parameter.

398
00:28:03,080 --> 00:28:06,400
It turns out this is completely wrong.

399
00:28:06,400 --> 00:28:11,520
The Bayesians knew it was wrong, actually.

400
00:28:11,520 --> 00:28:15,280
The brain is not in the same regime of statistics 101.

401
00:28:15,280 --> 00:28:19,160
In the brain, you're fitting about 10 to the 14 parameters, and you have about 10 to the

402
00:28:19,160 --> 00:28:21,280
nine seconds.

403
00:28:21,320 --> 00:28:27,040
So even if you have sort of 10 experiences per second, so even if you take 100 milliseconds

404
00:28:27,040 --> 00:28:33,880
is the time for an experience, that's the kind of backward masking time, you have like

405
00:28:33,880 --> 00:28:38,920
10,000 synapses per 100 milliseconds of your life.

406
00:28:38,920 --> 00:28:42,680
You're throwing a lot of parameters.

407
00:28:42,680 --> 00:28:46,920
So if your mother just kept saying, good, bad, good, bad, good, bad, she couldn't possibly

408
00:28:46,960 --> 00:28:51,960
provide enough information to learn all those 10 to the 14 parameters.

409
00:28:51,960 --> 00:28:58,960
And here's what they teach you wrong in statistics.

410
00:29:01,200 --> 00:29:04,960
Everybody knows that if you've got a given size model with a given number of parameters,

411
00:29:04,960 --> 00:29:07,800
the more data you have, the better you'll generalize.

412
00:29:07,800 --> 00:29:10,760
So for a given size of model, it's always better to have more data.

413
00:29:10,760 --> 00:29:14,360
In fact, the best thing you can do is get more data.

414
00:29:15,160 --> 00:29:19,520
Okay, but that doesn't mean that if you've got a fixed amount of data, you should make

415
00:29:19,520 --> 00:29:21,600
it look like a lot by having a small model.

416
00:29:21,600 --> 00:29:24,200
That's what they tell you in statistics 101.

417
00:29:24,200 --> 00:29:26,200
Okay?

418
00:29:26,200 --> 00:29:33,480
Big models are good if you regularize them, if you stop them doing crazy things.

419
00:29:33,480 --> 00:29:36,920
We can see that using a lot of parameters is good, that you can always win by having

420
00:29:36,920 --> 00:29:37,920
more parameters.

421
00:29:37,920 --> 00:29:39,920
And the way you do that is say, I'm going to have a committee.

422
00:29:39,920 --> 00:29:43,000
I'm going to learn lots of different little neural nets.

423
00:29:43,000 --> 00:29:45,960
If you give me more parameters, I'll learn more different neural nets.

424
00:29:45,960 --> 00:29:48,240
And then I'll average what they all say.

425
00:29:48,240 --> 00:29:49,680
And you'll always win.

426
00:29:49,680 --> 00:29:56,000
It's a sort of declining win, but if you have enough of them, you'll win by having more.

427
00:29:56,000 --> 00:30:00,840
So it's always better to have more parameters.

428
00:30:00,840 --> 00:30:07,840
It turns out if you have a fixed amount of data and you have enough computation power,

429
00:30:07,920 --> 00:30:13,680
the brain has, you should always use such a big model that the amount of data looks

430
00:30:13,680 --> 00:30:14,680
small.

431
00:30:14,680 --> 00:30:17,760
That's the regime you ought to be in for a fixed amount of data.

432
00:30:17,760 --> 00:30:22,880
That is, if you take the limit when the amount of data is fixed and you have unlimited computation

433
00:30:22,880 --> 00:30:28,040
and ask now how big would you like your model to be, you'd like your model to be much bigger

434
00:30:28,040 --> 00:30:30,680
than the data.

435
00:30:30,680 --> 00:30:33,360
Okay.

436
00:30:34,080 --> 00:30:36,840
That only works if you have a good regularizer.

437
00:30:36,840 --> 00:30:44,480
And I'm now going to tell you a very good regularizer called Dropout.

438
00:30:44,480 --> 00:30:47,520
So this is to use in neural networks where you have a lot more parameters than you have

439
00:30:47,520 --> 00:30:50,080
data points to train them on.

440
00:30:50,080 --> 00:30:53,840
And you could learn an ensemble of little models, and this is a way of learning an ensemble

441
00:30:53,840 --> 00:30:59,880
of many more models, but the models in the ensemble can share things with each other.

442
00:30:59,920 --> 00:31:05,640
So the idea is, if we just have one hidden layer in a neural net, we put the data in

443
00:31:05,640 --> 00:31:11,640
and each time we show the data vector, we randomly remove half the neurons.

444
00:31:11,640 --> 00:31:19,400
So we randomly get rid of half the neurons in your brain and only use the ones that remain.

445
00:31:19,400 --> 00:31:22,880
And it's a different subset we remove each time.

446
00:31:22,880 --> 00:31:27,440
Now when we do use a neuron, we use it with the same weights each time.

447
00:31:27,480 --> 00:31:31,080
So what you've got is if you've got h hidden neurons, you've got two to the h different

448
00:31:31,080 --> 00:31:33,240
subsets of neurons you might use.

449
00:31:33,240 --> 00:31:37,920
So you actually have two to the h different models, exponentially many models.

450
00:31:37,920 --> 00:31:39,440
Most of the models are never used.

451
00:31:39,440 --> 00:31:44,360
A few of the models will see one example, a small fraction of them will see one example.

452
00:31:44,360 --> 00:31:48,200
No models will see two examples.

453
00:31:48,200 --> 00:31:50,960
And yet they can learn because they're all sharing parameters.

454
00:31:50,960 --> 00:31:54,400
So this idea of sharing parameters in a neural network is very effective.

455
00:31:54,400 --> 00:31:59,240
So really you've got all these different models that are sharing parameters and you train

456
00:31:59,240 --> 00:32:02,560
it up and it generalizes really well.

457
00:32:06,600 --> 00:32:08,360
So I said that.

458
00:32:08,360 --> 00:32:14,160
So what we know is if you get rid of a fraction of the neurons each time and treat it as though

459
00:32:14,160 --> 00:32:18,240
they weren't there, it works really well.

460
00:32:18,240 --> 00:32:21,200
That's just a form of noise.

461
00:32:21,200 --> 00:32:26,360
And basically this is just an example of if you have a very big model and you add a lot

462
00:32:26,360 --> 00:32:34,880
of noise, the noise allows it to generalize well and it's better to have a big model with

463
00:32:34,880 --> 00:32:38,720
a lot of noise than to have a small model with no noise.

464
00:32:38,720 --> 00:32:42,240
And so what the brain wants, because it's got such a big model compared with the amount

465
00:32:42,240 --> 00:32:45,760
of data it operates on, it wants a lot of noise.

466
00:32:45,760 --> 00:32:48,880
And so now a Poisson neuron is kind of ideal.

467
00:32:48,880 --> 00:32:54,000
It's got a firing rate and now it adds a whole lot of noise to that and either sends a one

468
00:32:54,000 --> 00:32:59,880
or a zero and that actually makes it generalize much better.

469
00:32:59,880 --> 00:33:04,000
So the argument is the reason neurons don't send real values is they don't want to.

470
00:33:04,000 --> 00:33:08,280
They want to send things with a lot of noise in and that's making them generalize better.

471
00:33:08,280 --> 00:33:11,120
So that's not an argument against backpropagation.

472
00:33:11,120 --> 00:33:15,440
These dropout models are trained with backpropagation.

473
00:33:15,440 --> 00:33:24,520
So the random spikes are really just a way of adding noise to the signal to get better generalization.

474
00:33:24,520 --> 00:33:28,520
And now the last thing I'm going to address, I'm going to keep going till Blake stops me

475
00:33:28,520 --> 00:33:34,920
and I figure I've got about another five minutes before he gets really ratty.

476
00:33:34,920 --> 00:33:42,960
So the output of a neuron represents the presence of a feature in the current input.

477
00:33:43,000 --> 00:33:48,000
So it's obvious the same output can't represent the error derivative, right?

478
00:33:48,000 --> 00:33:51,880
You couldn't have a neuron that said to higher layers, this is the value of my feature

479
00:33:51,880 --> 00:33:55,120
and said to lower layers, this is my error derivative.

480
00:33:55,120 --> 00:33:56,520
It couldn't be done.

481
00:33:56,520 --> 00:34:02,720
So the neurons that go backwards need to be different neurons except that that's nonsense.

482
00:34:02,720 --> 00:34:07,320
So here's my claim.

483
00:34:07,320 --> 00:34:08,720
Joshua Benjo picked up on this later.

484
00:34:08,720 --> 00:34:10,760
I made this claim first in 2007.

485
00:34:10,760 --> 00:34:15,360
Actually, I made it first in the ground proposal.

486
00:34:15,360 --> 00:34:19,120
And I still believe this claim even though nobody's managed to make it work really well

487
00:34:19,120 --> 00:34:21,160
in the neural net yet.

488
00:34:21,160 --> 00:34:26,960
The idea is a neuron has a firing rate.

489
00:34:26,960 --> 00:34:34,240
That's the firing rate is its real output, which is communicated stochasticly by a spike.

490
00:34:34,240 --> 00:34:41,520
And that firing rate is actually changing over time, the underlying firing rate.

491
00:34:41,520 --> 00:34:47,360
And the rate of change of the firing rate is used to represent the error derivative.

492
00:34:47,360 --> 00:34:51,400
Now the nice thing about a rate of change is it can be positive or negative.

493
00:34:51,400 --> 00:34:56,560
So we can represent positive or negative derivatives without a neuron having to change

494
00:34:56,560 --> 00:35:00,560
the sort of signs of its synapses.

495
00:35:00,760 --> 00:35:04,000
And what it's representing, the derivative it's representing is the derivative of the

496
00:35:04,000 --> 00:35:06,920
error with respect to the input to the neuron.

497
00:35:06,920 --> 00:35:08,400
And that gets sent back to earlier neurons.

498
00:35:08,400 --> 00:35:11,800
And if I had enough time, I could show you a whole bunch of slides about how this will

499
00:35:11,800 --> 00:35:14,400
do back prop.

500
00:35:14,400 --> 00:35:17,960
But I want to show you one consequence of this.

501
00:35:17,960 --> 00:35:21,960
So that's, look, here we have a nice equation because it's got Leibniz on one side and Newton

502
00:35:21,960 --> 00:35:25,040
on the other side.

503
00:35:25,040 --> 00:35:28,400
That's Leibniz's notation for derivatives because they're not derivatives with respect

504
00:35:28,400 --> 00:35:29,720
to time.

505
00:35:29,720 --> 00:35:34,640
And this is Newton's notation because that was for derivatives with respect to time, okay?

506
00:35:34,640 --> 00:35:41,480
And what we're saying is the output of neuron J, which is yj, is the output of neuron J.

507
00:35:41,480 --> 00:35:46,280
But how fast that's changing over a short time interval is the error derivative.

508
00:35:46,280 --> 00:35:49,600
This is just a hypothesis, you understand?

509
00:35:49,600 --> 00:35:54,440
But it's true.

510
00:35:54,440 --> 00:35:58,680
Jay McClellan and I first used a version of this in 1988 before we knew about its back

511
00:35:58,720 --> 00:36:00,040
time dependent plasticity.

512
00:36:00,040 --> 00:36:03,200
I'm not sure it would be discovered then.

513
00:36:03,200 --> 00:36:06,200
Where you take, this is where I need the cursor.

514
00:36:06,200 --> 00:36:10,200
Yes, that one.

515
00:36:10,200 --> 00:36:12,280
Yes.

516
00:36:12,280 --> 00:36:14,120
You take some input.

517
00:36:14,120 --> 00:36:17,640
You send it to some hidden units, which send it to more hidden units by the green connections

518
00:36:17,640 --> 00:36:18,640
and sends it to more hidden units.

519
00:36:18,640 --> 00:36:21,840
It comes back to the input, so you reconstruct the input.

520
00:36:21,840 --> 00:36:25,840
And then you send it around again, not all the way around, but up to there and up to

521
00:36:25,840 --> 00:36:28,640
there and up to there using the right connections.

522
00:36:28,640 --> 00:36:33,760
And then the learning rule, which you'll notice doesn't involve explicit back propagation,

523
00:36:33,760 --> 00:36:42,280
is to say for these neurons, for example, I change the incoming weights by the activity

524
00:36:42,280 --> 00:36:49,360
of the presynaptic neuron down here times the difference between what I got on the green

525
00:36:49,360 --> 00:36:52,880
activation and on the red activation, first time round and second time round.

526
00:36:52,880 --> 00:36:57,760
So the rate of change of the activation of the neuron is what's used to communicate an

527
00:36:57,760 --> 00:36:58,760
error derivative.

528
00:36:58,760 --> 00:37:06,400
Now, unfortunately, this thing has the wrong sign, but later on we fixed that.

529
00:37:06,400 --> 00:37:14,240
And so here's a theory from 2007 that still hasn't been conclusively proved wrong.

530
00:37:14,240 --> 00:37:18,200
And it sort of works, but it doesn't work quite as well as we hoped, about how you could

531
00:37:18,200 --> 00:37:21,280
get a brain to do back propagation.

532
00:37:21,280 --> 00:37:25,120
What you first do is you learn a stack of autoencoders, that is you learn to get each

533
00:37:25,120 --> 00:37:31,280
layer to activate features in the layer above from which you can reconstruct the layer below.

534
00:37:31,280 --> 00:37:34,080
So you learn some features that can reconstruct this layer.

535
00:37:34,080 --> 00:37:37,880
Then you treat those features as data and learn some features that can reconstruct them.

536
00:37:37,880 --> 00:37:40,080
You build a big stack of autoencoders like that.

537
00:37:40,080 --> 00:37:41,080
Okay.

538
00:37:41,080 --> 00:37:45,720
Once you build the stack of autoencoders, then each layer can activity in a layer can

539
00:37:45,720 --> 00:37:49,420
reconstruct the activity in the layer below.

540
00:37:49,420 --> 00:37:52,640
And then you do two top down passes.

541
00:37:52,640 --> 00:37:57,360
You do a top down pass from the thing you predicted at the output.

542
00:37:57,360 --> 00:38:00,960
So you put in input, activity goes forward through the layers, you predict something

543
00:38:00,960 --> 00:38:01,960
at the output.

544
00:38:01,960 --> 00:38:07,040
And now you do a top down pass and you get reconstructed activities everywhere.

545
00:38:07,040 --> 00:38:15,280
And then you take your output and you change it to be more like the desired output.

546
00:38:15,280 --> 00:38:21,200
And now you do a top down pass and you'll get slightly different reconstructions.

547
00:38:21,200 --> 00:38:28,120
And the difference between those two reconstructions is actually the signal you need for back propagation.

548
00:38:28,120 --> 00:38:33,080
And so if you do that, the learning rule is that you should change a synapse by the

549
00:38:33,080 --> 00:38:39,200
pre-synaptic activity in the layer below times the rate of change of the activity in the

550
00:38:39,200 --> 00:38:41,280
layer above in the post-synaptic neuron.

551
00:38:41,280 --> 00:38:44,920
So it's a very simple learning rule.

552
00:38:45,120 --> 00:38:50,920
So let's change the weight in proportion to the pre-synaptic activity times the rate

553
00:38:50,920 --> 00:38:55,040
of change of the post-synaptic activity.

554
00:38:55,040 --> 00:39:00,640
Now it turns out if you're using spiking neurons, what that amounts to that are representing

555
00:39:00,640 --> 00:39:07,480
underlying firing rates that are changing, that amounts to a learning rule that looks

556
00:39:07,480 --> 00:39:13,080
like this.

557
00:39:13,080 --> 00:39:18,880
What you do is you take a pre-synaptic spike and you ask whether the post-synaptic spike

558
00:39:18,880 --> 00:39:21,920
came before or after it.

559
00:39:21,920 --> 00:39:27,480
Because what you're interested in is the rate of change of the post-synaptic firing rate

560
00:39:27,480 --> 00:39:33,000
around the time of the pre-synaptic spike.

561
00:39:33,000 --> 00:39:41,080
And if the post-synaptic spike occurs often just after it and seldom just before it, that

562
00:39:41,080 --> 00:39:43,520
suggests the firing rate is going up.

563
00:39:43,520 --> 00:39:47,840
And if the post-synaptic spike occurs often just before the pre-synaptic one and less

564
00:39:47,840 --> 00:39:51,160
often just after it, that means the firing rate of the post-synaptic neuron is going

565
00:39:51,160 --> 00:39:53,280
down.

566
00:39:53,280 --> 00:39:58,240
So if you want your learning rule to be the pre-synaptic activity, well, you'll only learn

567
00:39:58,240 --> 00:40:00,360
when you get a pre-synaptic spike.

568
00:40:00,360 --> 00:40:04,400
And then what you'll do is you'll say, did the post-synaptic spike occur afterwards

569
00:40:04,400 --> 00:40:05,400
or before?

570
00:40:05,400 --> 00:40:09,120
If it occurred afterwards, I should raise the weight and if it occurred before, I should

571
00:40:09,120 --> 00:40:10,600
lower the weight.

572
00:40:10,640 --> 00:40:18,720
And so your learning rule will look like this and this thing is actually a derivative filter.

573
00:40:18,720 --> 00:40:23,720
It's centered at zero and what this is really doing is measuring the rate of change of the

574
00:40:23,720 --> 00:40:25,880
post-synaptic firing rate.

575
00:40:25,880 --> 00:40:28,040
And of course it's sampling it.

576
00:40:28,040 --> 00:40:31,680
So you have a post-synaptic firing rate, there's these spike trains and are the other spikes

577
00:40:31,680 --> 00:40:34,040
getting closer together or further apart?

578
00:40:34,040 --> 00:40:36,720
Well this is a way to measure that.

579
00:40:36,720 --> 00:40:42,440
And of course you can do the learning on individual spikes and the learning rule would then be

580
00:40:42,440 --> 00:40:46,440
the implementation of this idea that the rate of change of the post-synaptic firing rate

581
00:40:46,440 --> 00:40:48,920
is the error signal.

582
00:40:48,920 --> 00:40:52,800
The learning rule would just be if the post-synaptic spike goes after the pre-synaptic one, increase

583
00:40:52,800 --> 00:40:57,720
the strength, otherwise decrease it and have that whole effect fall off as the spikes get

584
00:40:57,720 --> 00:41:01,920
further away because we're really only interested in the rate of change of the firing rate around

585
00:41:01,920 --> 00:41:05,760
the time of the pre-synaptic spike.

586
00:41:05,800 --> 00:41:11,120
Now there's one consequence of that which is that if you're going to use the rate of

587
00:41:11,120 --> 00:41:17,560
change of a neuron to represent not what the neuron is representing but to represent an

588
00:41:17,560 --> 00:41:22,520
error derivative, you've basically used up temporal derivatives for communicating error

589
00:41:22,520 --> 00:41:23,800
derivatives.

590
00:41:23,800 --> 00:41:28,640
So you cannot use temporal derivatives to communicate the temporal derivatives of what the neuron

591
00:41:28,640 --> 00:41:29,640
represents.

592
00:41:29,640 --> 00:41:34,080
So however I have a neuron that represents position, I can't use how fast that's changing

593
00:41:34,080 --> 00:41:37,480
to represent velocity and that's true of neurons.

594
00:41:37,480 --> 00:41:42,720
If you want to represent velocity, you have to have a neuron whose output represents velocity.

595
00:41:42,720 --> 00:41:46,000
You can't do it with the rate of change of a position neuron.

596
00:41:46,000 --> 00:41:52,680
If I kill the velocity neurons and keep the position neurons and I watch a car moving,

597
00:41:52,680 --> 00:41:56,400
the position neurons will change but I won't see any motion.

598
00:41:56,400 --> 00:42:01,480
Similarly, you can't use the rate of change of a velocity neuron to represent acceleration.

599
00:42:02,280 --> 00:42:08,120
Okay, so I think the fact that you can't use the rate of change of a representation to

600
00:42:08,120 --> 00:42:13,680
represent that that stuff in the world is changing is more evident since the board of

601
00:42:13,680 --> 00:42:20,120
the idea temporal derivatives of neurons are used up in representing error derivatives.

602
00:42:20,120 --> 00:42:24,040
So now I'll summarize.

603
00:42:24,040 --> 00:42:28,880
The main arguments against back propagation, the fact that they spent neurons and spikes

604
00:42:28,880 --> 00:42:34,480
rather than real numbers, well that's just because a lot of noise regularizes things.

605
00:42:34,480 --> 00:42:39,120
You can represent error derivatives as temporal derivatives so the same neuron can send temporal

606
00:42:39,120 --> 00:42:46,440
derivatives backwards, communicate those backwards and communicate activities forwards and the

607
00:42:46,440 --> 00:42:51,040
fact that in the brain you do get back to independent plasticity seems to be evidence

608
00:42:51,040 --> 00:42:55,560
in favor of that representation of error derivatives and now I'm done.

609
00:42:58,880 --> 00:43:13,880
Thank you Jeff for a great talk, sorry that was a Twitter joke, got it, anyway.

610
00:43:13,880 --> 00:43:21,920
So now what we're going to do is a brief Q&A between myself and Jeff and then after

611
00:43:21,920 --> 00:43:27,000
I've had my chance to ask some questions I'm going to open it up to you guys.

612
00:43:27,000 --> 00:43:34,560
Now I had originally sent Jeff a few questions which I'll rely on partially but his talk

613
00:43:34,560 --> 00:43:39,040
has made me want to ask a few others so I'm sorry I'm going to throw a few loops at you

614
00:43:39,040 --> 00:43:43,400
as well but let's start with some of the ones that I told you I wouldn't give you.

615
00:43:43,400 --> 00:43:48,560
There is something funny with my mic, I don't know if the AV guy is there, I just won't

616
00:43:48,560 --> 00:43:49,560
look down.

617
00:43:49,560 --> 00:44:01,760
Okay so the first question, yeah it's okay, I'm going off script anyway.

618
00:44:01,760 --> 00:44:07,080
The first question which I would like to ask just because it's something that I spend far

619
00:44:07,080 --> 00:44:16,080
too long arguing with people online is essentially you know so you're in the computer science

620
00:44:16,080 --> 00:44:20,760
department, you've come here, you've given us a talk that's largely about brains but

621
00:44:20,760 --> 00:44:26,160
many people seem to object to the idea that computers have anything to tell us about brains

622
00:44:26,160 --> 00:44:30,920
or indeed the idea that the brain is a computer despite the fact that neuroscientists often

623
00:44:30,920 --> 00:44:33,160
refer to computation in the brain.

624
00:44:33,160 --> 00:44:39,840
So my question is to you, is the brain a computer, I don't know I'll just hand that over to

625
00:44:39,840 --> 00:44:40,840
you first.

626
00:44:41,600 --> 00:44:42,600
Yes?

627
00:44:42,600 --> 00:44:45,120
Good, okay.

628
00:44:45,120 --> 00:44:50,880
And for the record I didn't tell him to say that if anyone from Twitter is watching.

629
00:44:50,880 --> 00:44:56,160
And B, can you just maybe give an intuitive understanding of why the answer is yes despite

630
00:44:56,160 --> 00:45:01,000
the fact that obviously our brains are very different from our laptops or our cell phones

631
00:45:01,000 --> 00:45:03,600
and stuff like that.

632
00:45:03,600 --> 00:45:10,640
So there's many ways you can do computation with physical stuff and you could get some

633
00:45:10,640 --> 00:45:15,920
silicon and make transistors and then run them at very high voltage much higher than

634
00:45:15,920 --> 00:45:21,800
needed to make them be digital and then you could if you wanted to represent a number

635
00:45:21,800 --> 00:45:26,920
you could have bits and you could and so on and you could create multipliers and adders

636
00:45:26,920 --> 00:45:31,160
and then you could put all that together and you could have some bits that tell you where

637
00:45:31,200 --> 00:45:38,240
in memory to find stuff and you could make a conventional computer or you could make

638
00:45:38,240 --> 00:45:44,280
little devices that have some input lines that are hardwired with input lines and you

639
00:45:44,280 --> 00:45:46,080
could have adaptive weights on the input lines.

640
00:45:46,080 --> 00:45:53,520
So early neural nets, Marvin Minsky made neural nets out of feedback controllers that were

641
00:45:53,520 --> 00:45:59,080
used in I think B-52 bombers or B-29 bombers or something, B-27 I don't know, some kind

642
00:45:59,080 --> 00:46:05,960
of bomber, it was America and so you can make computers in lots of different ways.

643
00:46:05,960 --> 00:46:11,480
When I was a kid I used to make computers by you take a six inch nail and you saw the

644
00:46:11,480 --> 00:46:17,760
head off and then you wrap copper wire around it and then you take a razor blade and you

645
00:46:17,760 --> 00:46:24,240
break it in half so that it's a nice flexible thing like this and you wrap a bit of copper

646
00:46:24,240 --> 00:46:28,560
wire around the razor blade and then when the current goes through the nail it'll make

647
00:46:28,640 --> 00:46:32,600
the razor blade go down and you'll make a contact so now you've got a relay and then

648
00:46:32,600 --> 00:46:35,240
you can put a bunch of those together and make logic gates.

649
00:46:35,240 --> 00:46:40,480
I never got more than about two logic gates that way but yeah you can make computers in

650
00:46:40,480 --> 00:46:47,920
lots of different ways and the brain is clearly made in a different way from the normal computers

651
00:46:47,920 --> 00:46:54,800
which has some different strengths and weaknesses so it's much slower but on the other hand

652
00:46:54,800 --> 00:46:57,760
you can make it much more parallel.

653
00:46:57,760 --> 00:47:02,600
It has one special property which I think is what makes us mortal which is that every

654
00:47:02,600 --> 00:47:09,800
brain is different so I can't take the weights from my brain and put them in Blake's brain

655
00:47:09,800 --> 00:47:12,920
and hope that it'll work because he just doesn't have connections in the same places.

656
00:47:12,920 --> 00:47:13,920
You've tried.

657
00:47:13,920 --> 00:47:14,920
Right.

658
00:47:14,920 --> 00:47:20,240
Well there's a way of doing it where you take the weights in my brain I turn it into strings

659
00:47:20,240 --> 00:47:21,240
of words.

660
00:47:21,240 --> 00:47:27,440
Blake absorbs these strings of words and puts different weights in his brain.

661
00:47:27,440 --> 00:47:31,520
It's pretty lucky all our brains are different because otherwise rich people will grab poor

662
00:47:31,520 --> 00:47:42,200
people's brains so they could live forever but quite okay.

663
00:47:42,200 --> 00:47:51,840
So I think I want to ask you then following on that what do you think about some of the

664
00:47:51,840 --> 00:47:57,760
quests to fully characterize the brains connectome do you think that is a scientifically worthwhile

665
00:47:57,760 --> 00:47:58,760
endeavor.

666
00:47:58,760 --> 00:48:08,320
Yes I do partly because some of the people doing it are my friends.

667
00:48:08,320 --> 00:48:11,480
Ignoring your loyalty to Sebastian.

668
00:48:11,760 --> 00:48:17,120
Not well in that case no.

669
00:48:17,120 --> 00:48:23,200
It seems to me it is very worth doing but you don't have to do that in order to begin

670
00:48:23,200 --> 00:48:25,040
to understand the principles.

671
00:48:25,040 --> 00:48:26,040
Very good.

672
00:48:26,040 --> 00:48:30,040
But for things like the retina which has a lot of hardwired stuff in it I think it's

673
00:48:30,040 --> 00:48:32,240
really important to do that.

674
00:48:32,240 --> 00:48:34,800
So that actually leads on to my next question.

675
00:48:34,800 --> 00:48:37,440
I wanted to ask you about hardwiring.

676
00:48:37,440 --> 00:48:44,080
So another thing that I think many people who study the brain find difficult about artificial

677
00:48:44,080 --> 00:48:49,120
neural networks as a model for the brain is that as you say you start with random weights

678
00:48:49,120 --> 00:48:52,200
and you train it on a lot of data and you get these things out.

679
00:48:52,200 --> 00:48:58,040
But we know that there are some pre-wired things in many brains so the classic examples

680
00:48:58,040 --> 00:49:04,160
are a horse can run pretty much right out of the womb but even within humans arguably

681
00:49:04,160 --> 00:49:08,160
there are some things that we find easier to learn than others.

682
00:49:08,160 --> 00:49:15,480
And so what do you think is the place for innate behavior within neural networks as

683
00:49:15,480 --> 00:49:17,160
a model of cognition?

684
00:49:17,160 --> 00:49:22,760
Okay so it used to be when I was a student if you were interested in language people

685
00:49:22,760 --> 00:49:28,200
would tell you that it was all innate and it just kind of matured as you got older and

686
00:49:28,200 --> 00:49:32,760
maybe you learned like 12 parameters that characterised your particular language whether

687
00:49:32,840 --> 00:49:38,280
it was subject verb object or some other way.

688
00:49:38,280 --> 00:49:43,080
In fact there's a nova that I saw, it was probably made about 20 years ago and it has

689
00:49:43,080 --> 00:49:48,560
all the leading linguists all of whom were educated by Chomsky and they look straight

690
00:49:48,560 --> 00:49:52,720
at the camera and they say there's a lot we don't know about language but one thing

691
00:49:52,720 --> 00:49:57,600
we know for sure is that it's not learned.

692
00:49:57,600 --> 00:49:59,720
So Chomsky had really good gulag.

693
00:49:59,720 --> 00:50:02,720
Yeah he did.

694
00:50:02,720 --> 00:50:09,680
But it's over because we know now if you want to translate you just learn it all.

695
00:50:09,680 --> 00:50:15,360
The number of linguists required to get a system that can turn a string of symbols in

696
00:50:15,360 --> 00:50:20,000
English into a string of symbols in French is roughly zero.

697
00:50:20,000 --> 00:50:23,480
I mean linguists involved in preparing the databases for training and making sure you

698
00:50:23,480 --> 00:50:29,000
get sort of a variety of grammatic structures and things but basically you don't need linguists,

699
00:50:29,000 --> 00:50:32,360
you just need data.

700
00:50:32,360 --> 00:50:34,680
So you don't need much innate structure.

701
00:50:34,680 --> 00:50:38,640
The issue of what is innate, it doesn't, it seems to me there's not much point putting

702
00:50:38,640 --> 00:50:43,640
in stuff innately if you can learn it quickly.

703
00:50:43,640 --> 00:50:50,680
So for example the ability to move and get 3D structure from motion, that's actually

704
00:50:50,680 --> 00:50:55,200
very easy to learn so I don't believe that's innate even though a child can do it at like

705
00:50:55,200 --> 00:50:57,560
two days.

706
00:50:57,560 --> 00:51:05,280
You show them a sort of W made of paper and you rotate it in a consistent way and they

707
00:51:05,280 --> 00:51:09,120
get bored and as soon as you rotate it in a way, move it in a way that's not consistent

708
00:51:09,120 --> 00:51:14,320
with the rotation, the interest perks up.

709
00:51:14,320 --> 00:51:16,240
But I think they can learn it in two days.

710
00:51:16,240 --> 00:51:19,280
It's really easy to learn.

711
00:51:19,280 --> 00:51:21,280
Okay interesting.

712
00:51:21,280 --> 00:51:29,760
Now I want to ask you, I know partially what your answer is going to be but when I remember

713
00:51:29,760 --> 00:51:36,920
long ago you told me that one of your career goals, at least earlier in your career, was

714
00:51:36,920 --> 00:51:43,320
to prove that everything that psychologists thought about the brain was wrong.

715
00:51:43,320 --> 00:51:48,880
And so my question is what was it that they had wrong, are they still getting it wrong

716
00:51:48,880 --> 00:51:52,880
and is neuroscience getting that same thing wrong?

717
00:51:52,880 --> 00:51:58,680
It was mainly to do with this conviction that psychologists had partly based on Chomsky

718
00:51:58,680 --> 00:52:04,200
that there was an awful lot of innate stuff there and that you couldn't just learn a whole

719
00:52:04,200 --> 00:52:06,760
bunch of representations from scratch.

720
00:52:06,760 --> 00:52:11,800
There was this innate framework and there was a little bit of tuning of this innate knowledge

721
00:52:11,800 --> 00:52:16,920
and that's what learning was and that's just, I think that's a completely wrong headed approach.

722
00:52:16,920 --> 00:52:23,640
In fact, I want to go the other way and I want to say the stuff in the brain that's innate

723
00:52:23,640 --> 00:52:26,320
wasn't discovered by evolution.

724
00:52:26,320 --> 00:52:31,960
The stuff in the brain that's innate was discovered by learning.

725
00:52:31,960 --> 00:52:34,600
Do I have time to do that digression?

726
00:52:34,600 --> 00:52:35,600
Yeah.

727
00:52:35,600 --> 00:52:46,040
Okay, so imagine we have a little neural network and it's got 20 connections in it and each

728
00:52:46,040 --> 00:52:50,000
of those connections has a switch that could be on or off.

729
00:52:50,000 --> 00:52:52,560
So it can let stuff through or not.

730
00:52:52,560 --> 00:52:54,800
So you've got to make 20 binary decisions.

731
00:52:54,800 --> 00:52:59,640
So your chance of making them by chance is one in a million and making the correct decisions.

732
00:52:59,640 --> 00:53:06,040
Now this little neural network circuit is a mating circuit and so the neural net goes

733
00:53:06,040 --> 00:53:12,920
into a singles bar and it runs this circuit and if it's got the connections right it has

734
00:53:12,920 --> 00:53:17,680
lots of offspring and if it hasn't got the connections right it doesn't have offspring

735
00:53:17,680 --> 00:53:20,400
or doesn't have so many offspring.

736
00:53:20,400 --> 00:53:27,480
Okay, so let's start off with the connections being, if they were just kind of random and

737
00:53:27,480 --> 00:53:32,640
you did mutation, what would happen is you'd have to build about a million organisms before

738
00:53:32,640 --> 00:53:37,680
you got a good one and if you had sexual combination of the organisms, let's have a really simple

739
00:53:37,680 --> 00:53:43,080
biology in which each connection has its own gene and this gene has two alleles for

740
00:53:43,080 --> 00:53:45,960
on and off, okay?

741
00:53:45,960 --> 00:53:52,160
If you do mating now, you might have an organism that got all 20 connections right and it mates

742
00:53:52,160 --> 00:53:55,840
with one that has a few wrong and it gets a few wrong ones and now it's wiped out.

743
00:53:55,840 --> 00:53:58,920
It doesn't have lots of offspring anymore, that's it.

744
00:53:58,920 --> 00:54:06,080
So it seems like a complete disaster and it would obviously take you at least a million

745
00:54:06,080 --> 00:54:10,200
organisms to expect to get a good one even if you have pathogenesis where you didn't

746
00:54:10,200 --> 00:54:12,200
have sexual reproduction.

747
00:54:12,200 --> 00:54:18,560
Now I will show you how to build a good organism in only 30,000 tries and the way you do it

748
00:54:18,560 --> 00:54:24,920
is this, you, for each connection you have three alleles, you have turn the connection

749
00:54:24,920 --> 00:54:33,400
on genetically, turn the connection off genetically or leave the connection to learning, okay?

750
00:54:33,400 --> 00:54:35,760
So that's the third alley.

751
00:54:35,760 --> 00:54:43,240
And now you start off with a population in which about half of the connections are genetically

752
00:54:43,240 --> 00:54:48,360
determined and the other half are left to learning.

753
00:54:48,360 --> 00:54:52,760
So that's 10 connections that are definitely determined, so there's a 1 in 1,000 chance

754
00:54:52,760 --> 00:54:56,440
that you'll luck out genetically, you'll get those 10 right.

755
00:54:56,440 --> 00:55:00,080
And then during the organism's lifetime, let's have a really dumb learning algorithm where

756
00:55:00,080 --> 00:55:04,320
it just randomly fit like the one I talked about, it randomly fiddles with the connections,

757
00:55:04,320 --> 00:55:08,440
just randomly flips connections of the 10 that are left to learning.

758
00:55:08,440 --> 00:55:12,960
And it'll take you to about 1,000 trials and it'll get the combination right.

759
00:55:12,960 --> 00:55:17,440
But the point is it can do those trials without building a whole organism, it can just go

760
00:55:17,440 --> 00:55:23,560
into the singles bar and sort of fiddle around a bit with its connections and it'll bang.

761
00:55:23,560 --> 00:55:31,560
So what we've done is we've replaced a million trials of evolution, building a million organisms

762
00:55:31,560 --> 00:55:38,200
with built 1,000 organisms and then let's build 30,000 organisms, just to be safe.

763
00:55:38,200 --> 00:55:46,120
And then each of those fiddles around with its connections and it'll do this search.

764
00:55:46,120 --> 00:55:49,840
The whole search is the same, you have to try a million combinations, but the way you

765
00:55:49,840 --> 00:55:55,200
get the million combinations is 1,000 organisms each does 1,000 learning trials and so almost

766
00:55:55,200 --> 00:55:57,920
all the work is done by learning.

767
00:55:57,920 --> 00:56:05,080
Now if genetically an organism happens to have more things set, like it's got 12 of

768
00:56:05,080 --> 00:56:08,120
them set right, it'll learn faster.

769
00:56:08,120 --> 00:56:15,720
And so this genetic pressure for, if you mate organisms now, this genetic pressure to get

770
00:56:15,720 --> 00:56:19,440
more and more of these alleles set genetically.

771
00:56:19,440 --> 00:56:25,000
But the pressure only comes because the learning can get all 20 set right so this thing can

772
00:56:25,000 --> 00:56:27,000
mate and have lots of offspring.

773
00:56:27,000 --> 00:56:32,840
So the fact that the learning can find a solution creates genetic pressure to hardwire these

774
00:56:32,840 --> 00:56:34,840
things.

775
00:56:34,840 --> 00:56:41,360
So what's happening there is the search, a thousand things were done by evolution, a

776
00:56:41,360 --> 00:56:46,560
million minus a thousand things were done by learning and that created a landscape for

777
00:56:46,560 --> 00:56:51,440
evolution that allowed evolution to gradually hardwire in more and more, that these things

778
00:56:51,440 --> 00:56:54,480
were first found by learning.

779
00:56:54,480 --> 00:57:02,160
So I think a lot of the structure in the brain that's hardwired is first found by learning

780
00:57:02,160 --> 00:57:05,560
and gradually it gets backed up into the hardwiring.

781
00:57:05,560 --> 00:57:09,040
But to get the evolutionary pressure to say that's good, you have to be able to do the

782
00:57:09,040 --> 00:57:10,040
learning.

783
00:57:10,080 --> 00:57:13,280
First hardwired things, you'd never find anything that was good.

784
00:57:13,280 --> 00:57:14,280
Okay.

785
00:57:14,280 --> 00:57:15,280
Great.

786
00:57:15,280 --> 00:57:16,280
Thank you.

787
00:57:16,280 --> 00:57:17,280
Now...

788
00:57:17,280 --> 00:57:19,200
That's called the Baldwin effect, by the way.

789
00:57:19,200 --> 00:57:20,200
Yes.

790
00:57:20,200 --> 00:57:26,880
Yeah, it's called after a psychology professor at the University of Toronto in the 1890s called

791
00:57:26,880 --> 00:57:29,520
Baldwin who invented this effect.

792
00:57:29,520 --> 00:57:34,480
He didn't do any computer simulations though.

793
00:57:34,480 --> 00:57:38,960
So I want to do one follow-up question on that and then ask my final question before

794
00:57:38,960 --> 00:57:40,640
handing it to the audience.

795
00:57:40,640 --> 00:57:49,040
So my follow-up to that is, you know, I think one of the things that is unclear in terms

796
00:57:49,040 --> 00:57:55,400
of the success of deep learning is exactly how much it was purely the compute or some

797
00:57:55,400 --> 00:57:56,880
clever things.

798
00:57:56,880 --> 00:58:02,920
Now I've seen both cases argued and you today kind of suggested that it was just the compute.

799
00:58:02,920 --> 00:58:07,840
But I want to ask you about this following on your last point, which is that we know

800
00:58:08,080 --> 00:58:12,280
that if you build networks with particular architectures and with particular learning

801
00:58:12,280 --> 00:58:18,440
rules, you are effectively making learning faster if you do it right.

802
00:58:18,440 --> 00:58:23,280
And arguably a lot of the success of deep learning has actually been as a result of

803
00:58:23,280 --> 00:58:28,560
people thinking about good designs for their networks and good ways of making learning

804
00:58:28,560 --> 00:58:29,560
faster.

805
00:58:29,560 --> 00:58:30,560
Yep.

806
00:58:30,560 --> 00:58:38,880
So would you potentially say that we have seen that process that you just described

807
00:58:38,880 --> 00:58:43,960
actually occur with NAI over the last ten years of the learning kind of backing up into

808
00:58:43,960 --> 00:58:45,680
the hardwiring?

809
00:58:45,680 --> 00:58:49,200
I see.

810
00:58:49,200 --> 00:58:51,280
I need to think about that.

811
00:58:51,280 --> 00:58:54,520
What we've seen, I mean, Jan Lecaun invented convolutional neural nets.

812
00:58:54,520 --> 00:58:55,520
Right.

813
00:58:55,520 --> 00:58:56,520
Yes.

814
00:58:56,520 --> 00:58:57,520
For example.

815
00:58:57,520 --> 00:59:00,240
So the computer was invented in the 1980s, but computers weren't fast enough to really

816
00:59:00,240 --> 00:59:01,680
do a lot with them.

817
00:59:01,680 --> 00:59:04,880
So they were used for handwriting recognition and they were used for reading 10% of all the

818
00:59:04,880 --> 00:59:06,600
checks in America.

819
00:59:06,600 --> 00:59:09,120
But they didn't really take off.

820
00:59:09,120 --> 00:59:14,080
They really took off when the computer hardware came along to make them really efficient.

821
00:59:14,080 --> 00:59:18,720
So that's a case where the ideas were had first, but without the hardware they didn't

822
00:59:18,720 --> 00:59:19,720
work.

823
00:59:19,720 --> 00:59:20,720
You've obviously got to have both.

824
00:59:20,720 --> 00:59:21,720
Right.

825
00:59:21,720 --> 00:59:22,720
Yes.

826
00:59:22,720 --> 00:59:23,720
Great.

827
00:59:23,720 --> 00:59:24,720
Okay.

828
00:59:25,000 --> 00:59:29,160
So now my last question before I hand it to the audience is just, what do you see as

829
00:59:29,160 --> 00:59:33,920
being the future of the interaction between neuroscience and AI?

830
00:59:33,920 --> 00:59:39,680
Do you think that there is space for a sort of new cognitive science where we study general

831
00:59:39,680 --> 00:59:46,560
intelligence, but with brain-centric models rather than logic-based models?

832
00:59:46,560 --> 00:59:49,880
Or will we see the two streams depart over the next few decades?

833
00:59:49,880 --> 00:59:52,960
The way I like to think of it is we'd like to understand, if you'd like to understand

834
00:59:53,000 --> 00:59:59,520
how the brain does computation, you've got brains in your computation.

835
00:59:59,520 --> 01:00:02,800
And they look, like you said, they look pretty different to begin with because there's many

836
01:00:02,800 --> 01:00:05,000
different ways to do computation.

837
01:00:05,000 --> 01:00:10,680
And with a conventional digital computer, you can get it to pretend to be anything.

838
01:00:10,680 --> 01:00:14,400
So we're getting it to pretend to be some other kind of computer, an artificial neural

839
01:00:14,400 --> 01:00:15,800
net.

840
01:00:15,800 --> 01:00:21,680
And we'd like to sort of bridge this huge gap between brains and what we can simulate

841
01:00:21,680 --> 01:00:23,520
on computers.

842
01:00:23,520 --> 01:00:28,120
And so neuroscientists are sort of doing experiments.

843
01:00:28,120 --> 01:00:32,520
And good computation neuroscientists are sort of doing experiments to try and sort of see

844
01:00:32,520 --> 01:00:34,880
how you could do the computation.

845
01:00:34,880 --> 01:00:40,440
And I think of myself at this end as doing, simulating things with artificial neural nets

846
01:00:40,440 --> 01:00:42,000
to see how you can make it more biological.

847
01:00:42,000 --> 01:00:44,440
And we're trying to build a bridge.

848
01:00:44,440 --> 01:00:48,840
And so the computational neuroscientists, most of them are building from this end.

849
01:00:48,920 --> 01:00:50,560
I'm building from the other end.

850
01:00:50,560 --> 01:00:53,320
But obviously, if you want to build a bridge to somewhere, you need to look at where you're

851
01:00:53,320 --> 01:00:54,160
going.

852
01:00:54,160 --> 01:00:57,560
And so I'm trying to build a bridge that does computation more and more like the brain does

853
01:00:57,560 --> 01:01:02,520
it, or like, I guess the brain does it from what my neuroscientist friends tell me.

854
01:01:02,520 --> 01:01:05,360
And then there's conventional AI, which is trying to build a bridge like that.

855
01:01:05,360 --> 01:01:10,320
Great, OK, thank you.

856
01:01:10,320 --> 01:01:14,200
So now I'm going to open up to questions from the audience.

857
01:01:14,240 --> 01:01:19,120
Now, for this, we've got this kind of interesting system here.

858
01:01:19,120 --> 01:01:26,000
So rather than you putting up your hands and me selecting you, you can actually nominate

859
01:01:26,000 --> 01:01:30,640
yourself to ask a question by pressing on the button on your microphone.

860
01:01:30,640 --> 01:01:32,880
And it is a first come, first serve basis.

861
01:01:32,880 --> 01:01:35,240
So you're going to be queued up.

862
01:01:35,240 --> 01:01:39,320
And so you're now first on the queue.

863
01:01:39,320 --> 01:01:41,920
And by now, it's too late to be able to ask a question.

864
01:01:41,920 --> 01:01:42,800
Yes.

865
01:01:42,800 --> 01:01:44,840
And one last thing about that, though.

866
01:01:44,840 --> 01:01:49,520
When you are done asking your question, please turn off your microphone,

867
01:01:49,520 --> 01:01:54,560
because that will open up this slot for the next person in the queue.

868
01:01:54,560 --> 01:01:58,520
OK, go ahead.

869
01:01:58,520 --> 01:01:59,960
I've got a red light here, does that?

870
01:01:59,960 --> 01:02:02,760
Yeah, if your red light is flashing, that means you're on.

871
01:02:02,760 --> 01:02:03,760
You get to ask a question.

872
01:02:07,000 --> 01:02:11,160
If your red light is flashing, you're on.

873
01:02:11,160 --> 01:02:12,520
I've got a solid light, please.

874
01:02:12,560 --> 01:02:13,440
Oh, solid.

875
01:02:13,440 --> 01:02:14,360
Sorry.

876
01:02:14,360 --> 01:02:15,680
OK, I've got a solid light.

877
01:02:15,680 --> 01:02:17,000
You were faster.

878
01:02:17,000 --> 01:02:18,320
OK.

879
01:02:18,320 --> 01:02:22,880
So this is a Clifton suspension bridge analogy for your interest here.

880
01:02:22,880 --> 01:02:26,080
So you mentioned briefly, Hebbian synapses.

881
01:02:26,080 --> 01:02:29,840
As neuroscientists, we have a good understanding of how they work at a molecular level.

882
01:02:29,840 --> 01:02:34,640
So my question is, to what extent are the understanding of biological memory

883
01:02:34,640 --> 01:02:39,040
mechanisms, i.e. Hebbian synapses, implemented by AI for deep learning

884
01:02:39,040 --> 01:02:42,080
and the sorts of systems that you're describing?

885
01:02:42,120 --> 01:02:49,520
So at present, people don't use Hebbian synapses for most deep learning.

886
01:02:49,520 --> 01:02:51,360
They're using back propagation.

887
01:02:51,360 --> 01:02:56,000
So it's an error correction rule, as opposed to something where,

888
01:02:56,000 --> 01:02:59,120
if you just use it, it gets stronger.

889
01:02:59,120 --> 01:03:02,200
But if you want a short-term memory for things like transformers

890
01:03:02,200 --> 01:03:06,520
to remember a temporal context, just a simple Hebbian synapses is a good thing to have.

891
01:03:06,520 --> 01:03:10,480
Yeah, but Hebbian synapses can code memories in humans that can last a lifetime.

892
01:03:10,480 --> 01:03:13,280
So is this something that AI is working towards using,

893
01:03:13,280 --> 01:03:18,000
or are they just going to bypass Hebbian synapses and come up with something superior?

894
01:03:18,000 --> 01:03:23,600
OK. So if you think about what's been successful in the last few years,

895
01:03:23,600 --> 01:03:27,600
it's using error correction learning with either labeled data

896
01:03:27,600 --> 01:03:33,240
or trying to predict what comes next, and not Hebbian synapses.

897
01:03:33,680 --> 01:03:40,960
Now, people like me who sort of do this kind of learning

898
01:03:40,960 --> 01:03:44,200
but are interested in the brain know this isn't right.

899
01:03:44,200 --> 01:03:46,400
We're much more interested in unsupervised learning.

900
01:03:46,400 --> 01:03:49,560
We just can't make it work very well yet.

901
01:03:49,560 --> 01:03:55,720
And I would love to be able to get learning to work,

902
01:03:55,720 --> 01:03:58,600
as well as it does when you do back propagation,

903
01:03:58,600 --> 01:04:01,480
without using biologically implausible things.

904
01:04:02,480 --> 01:04:06,880
And one place we can do that is with temporary memories.

905
01:04:06,880 --> 01:04:10,120
So if you say synapses have a fast component,

906
01:04:10,120 --> 01:04:12,600
you can use Hebbian learning for that fast component,

907
01:04:12,600 --> 01:04:14,760
and that will actually help neural networks work better,

908
01:04:14,760 --> 01:04:18,760
even if you're using back propagation for the slow component.

909
01:04:18,760 --> 01:04:21,760
That didn't really answer your question, but you know, it filled the time.

910
01:04:21,760 --> 01:04:25,560
LAUGHTER

911
01:04:25,560 --> 01:04:27,560
Hello.

912
01:04:28,560 --> 01:04:33,560
I read in the Reinforcement Learning Book that dopamine is used

913
01:04:33,560 --> 01:04:36,560
as a reward prediction error signal.

914
01:04:36,560 --> 01:04:40,560
So I was wondering, do you see it used as a supervisory signal,

915
01:04:40,560 --> 01:04:43,560
like you mentioned earlier?

916
01:04:43,560 --> 01:04:46,560
OK, so for reinforcement learning,

917
01:04:46,560 --> 01:04:49,560
there is some lovely work done by Peter Diane,

918
01:04:49,560 --> 01:04:53,560
who is the theoretician, and some experimentalists,

919
01:04:53,560 --> 01:04:56,560
showing that the real data from neuroscience

920
01:04:56,560 --> 01:05:00,560
fits in with a theory that was started with Rich Sutton.

921
01:05:00,560 --> 01:05:06,560
And Peter Diane did the work of showing that dopamine

922
01:05:06,560 --> 01:05:09,560
corresponds to something in a particular learning algorithm.

923
01:05:09,560 --> 01:05:12,560
And it doesn't correspond to the reward,

924
01:05:12,560 --> 01:05:16,560
it corresponds to the difference between the reward you're expecting

925
01:05:16,560 --> 01:05:18,560
and the reward you get.

926
01:05:18,560 --> 01:05:22,560
So if you're a monkey and you're expecting a grape

927
01:05:22,560 --> 01:05:26,560
and I give you a piece of cucumber, that's negative reward,

928
01:05:26,560 --> 01:05:30,560
and that will be a big negative hit at the dopamine.

929
01:05:33,560 --> 01:05:38,560
So that's not the kind of learning that's been really successful so far.

930
01:05:38,560 --> 01:05:41,560
If you're willing to burn a lot of computer time,

931
01:05:41,560 --> 01:05:45,560
Reinforcement Learning will solve some problems,

932
01:05:45,560 --> 01:05:49,560
but it's not the kind of learning that's been most successful in AI.

933
01:05:52,560 --> 01:05:55,560
So the difference is in Reinforcement Learning,

934
01:05:55,560 --> 01:05:58,560
you get a single scalar, you get one number,

935
01:05:58,560 --> 01:06:00,560
whereas in error correction learning,

936
01:06:00,560 --> 01:06:03,560
you typically get a whole vector of numbers.

937
01:06:05,560 --> 01:06:07,560
Right here.

938
01:06:07,560 --> 01:06:12,560
So you mentioned that your goal, kind of the bridge analogy,

939
01:06:12,560 --> 01:06:16,560
is your goal is to go from computers and try to get to the brain.

940
01:06:16,560 --> 01:06:19,560
So let's just say that kind of makes sense to think,

941
01:06:19,560 --> 01:06:21,560
okay, let's get to more general AI,

942
01:06:21,560 --> 01:06:23,560
because I'd say humans are decently general.

943
01:06:23,560 --> 01:06:26,560
And neuroscientists are trying to get from the other bridge,

944
01:06:26,560 --> 01:06:28,560
the brain, to generally AI.

945
01:06:28,560 --> 01:06:32,560
So you have these two kind of debates,

946
01:06:32,560 --> 01:06:34,560
and this happens quite often,

947
01:06:34,560 --> 01:06:37,560
where is it correct to go from generally AI to the brain,

948
01:06:37,560 --> 01:06:39,560
first understand generally AI, then understand the brain,

949
01:06:39,560 --> 01:06:41,560
or brain to generally AI.

950
01:06:41,560 --> 01:06:44,560
And so what would you say is the most practical way

951
01:06:44,560 --> 01:06:48,560
to problematize generally AI?

952
01:06:50,560 --> 01:06:53,560
I don't like the phrase general AI.

953
01:06:53,560 --> 01:06:57,560
I don't think, if you want intelligent devices,

954
01:06:57,560 --> 01:07:02,560
I don't think you want to produce a sort of general purpose Android.

955
01:07:02,560 --> 01:07:05,560
I think you want to produce different devices

956
01:07:05,560 --> 01:07:08,560
that are smart in different ways.

957
01:07:08,560 --> 01:07:11,560
So basically if you want intelligent machines that do things,

958
01:07:11,560 --> 01:07:14,560
you have a vacuum cleaner and you have a backhoe.

959
01:07:14,560 --> 01:07:18,560
You don't try to make one thing that's a vacuum cleaner and a backhoe.

960
01:07:18,560 --> 01:07:20,560
It doesn't make sense.

961
01:07:20,560 --> 01:07:22,560
What about connecting them through

962
01:07:22,560 --> 01:07:26,560
kind of like different cognition areas in the brain?

963
01:07:26,560 --> 01:07:28,560
Yeah, but I think it's the same with cognition too.

964
01:07:28,560 --> 01:07:31,560
I think the neural net that does machine translation

965
01:07:31,560 --> 01:07:36,560
isn't the same neural net as does vision.

966
01:07:36,560 --> 01:07:42,560
I think, yeah, my guess is that people are thinking too much

967
01:07:42,560 --> 01:07:45,560
about making one neural net that does everything,

968
01:07:45,560 --> 01:07:50,560
and not thinking enough about making more modular neural nets

969
01:07:50,560 --> 01:07:52,560
that are good at different things.

970
01:07:52,560 --> 01:07:54,560
Some are more universal than others,

971
01:07:54,560 --> 01:07:56,560
but I think that's how progress has been made.

972
01:07:56,560 --> 01:07:58,560
That's how progress has been made so far.

973
01:07:58,560 --> 01:08:00,560
Not by the people talking about general AI.

974
01:08:00,560 --> 01:08:03,560
It's being made by people looking at saying,

975
01:08:03,560 --> 01:08:05,560
how can I get a neural net to do vision,

976
01:08:05,560 --> 01:08:08,560
or how can I get it to do machine translation?

977
01:08:08,560 --> 01:08:10,560
Thank you.

978
01:08:15,560 --> 01:08:19,560
Hi, I'm just wondering about the role of hierarchy in general.

979
01:08:19,560 --> 01:08:21,560
There are different types of hierarchy.

980
01:08:21,560 --> 01:08:23,560
There's different layers of neural network.

981
01:08:23,560 --> 01:08:26,560
As you mentioned, there's fast memory and slow memory.

982
01:08:26,560 --> 01:08:31,560
Then I wonder, are there more ways to add hierarchy to neural networks

983
01:08:31,560 --> 01:08:35,560
to make them more useful or emulate actual brain?

984
01:08:38,560 --> 01:08:40,560
Yes, probably.

985
01:08:41,560 --> 01:08:45,560
In vision, for example,

986
01:08:45,560 --> 01:08:50,560
you have multiple layers,

987
01:08:50,560 --> 01:08:53,560
that is, you have multiple cortical areas in the visual pathway.

988
01:08:53,560 --> 01:08:56,560
That's a very different kind of hierarchy

989
01:08:56,560 --> 01:09:00,560
from what you need for dealing with the sort of structural reality.

990
01:09:00,560 --> 01:09:03,560
In reality, there's the universe.

991
01:09:03,560 --> 01:09:06,560
There may be many of them, but that's one's enough.

992
01:09:06,560 --> 01:09:09,560
Then there's galaxies.

993
01:09:09,560 --> 01:09:12,560
Then there's probably things above galaxies.

994
01:09:12,560 --> 01:09:15,560
Then there's in the galaxies the stars,

995
01:09:15,560 --> 01:09:17,560
and then there's solar systems,

996
01:09:17,560 --> 01:09:20,560
and then there's planets, and so on.

997
01:09:20,560 --> 01:09:22,560
We can do that all the way down to atoms.

998
01:09:22,560 --> 01:09:24,560
You can imagine all that.

999
01:09:24,560 --> 01:09:27,560
You can represent all that in your brain.

1000
01:09:27,560 --> 01:09:30,560
Clearly, what's going on is,

1001
01:09:30,560 --> 01:09:33,560
out in the world, there's this hierarchy

1002
01:09:33,560 --> 01:09:36,560
that goes over many, many orders of magnitude

1003
01:09:36,560 --> 01:09:39,560
from the universe down to quarks,

1004
01:09:39,560 --> 01:09:42,560
or whatever the smallest thing is now.

1005
01:09:44,560 --> 01:09:47,560
You don't want that kind of hierarchy in your brain.

1006
01:09:47,560 --> 01:09:50,560
What you've got in your brain is the ability to deal

1007
01:09:50,560 --> 01:09:52,560
with a little window of hierarchy,

1008
01:09:52,560 --> 01:09:55,560
where there's an object in its parts.

1009
01:09:55,560 --> 01:09:57,560
To deal with the whole universe,

1010
01:09:57,560 --> 01:09:59,560
you can take this window,

1011
01:09:59,560 --> 01:10:02,560
and you can map at a scale of the universe,

1012
01:10:02,560 --> 01:10:04,560
and there's the universe, and there's the galaxies,

1013
01:10:04,560 --> 01:10:07,560
or there's the galaxies, and there's the stars,

1014
01:10:07,560 --> 01:10:10,560
or there's the atom, and there's the electrons.

1015
01:10:12,560 --> 01:10:15,560
You're using the same neural hardware,

1016
01:10:15,560 --> 01:10:18,560
but mapping reality onto it differently.

1017
01:10:18,560 --> 01:10:21,560
I think whenever we have to deal with anything complicated,

1018
01:10:21,560 --> 01:10:23,560
we use hierarchies.

1019
01:10:24,560 --> 01:10:27,560
The way the brain uses them is by varying the mapping

1020
01:10:27,560 --> 01:10:31,560
from reality onto the brain.

1021
01:10:31,560 --> 01:10:34,560
It can only operate with a small window on a hierarchy,

1022
01:10:34,560 --> 01:10:36,560
which you can move up and down.

1023
01:10:36,560 --> 01:10:39,560
Much like you only have a small region of high resolution,

1024
01:10:39,560 --> 01:10:41,560
which you move around.

1025
01:10:41,560 --> 01:10:42,560
So logarithm?

1026
01:10:42,560 --> 01:10:43,560
Sorry?

1027
01:10:43,560 --> 01:10:45,560
Like logarithm.

1028
01:10:45,560 --> 01:10:47,560
What about logarithms?

1029
01:10:47,560 --> 01:10:49,560
That's what you're talking about, right?

1030
01:10:49,560 --> 01:10:52,560
Compressing a big range into something that is much manageable.

1031
01:10:52,560 --> 01:10:54,560
I wasn't thinking of it like that.

1032
01:10:54,560 --> 01:10:57,560
I was thinking of it as you have some fixed hardware,

1033
01:10:57,560 --> 01:11:01,560
and when I'm thinking about the solar system,

1034
01:11:01,560 --> 01:11:04,560
my fixed hardware couldn't possibly deal with the universe.

1035
01:11:04,560 --> 01:11:07,560
That's much too big, and it couldn't possibly deal with an atom.

1036
01:11:07,560 --> 01:11:09,560
That's much too small.

1037
01:11:09,560 --> 01:11:12,560
But it's fine dealing with the sun and some planets,

1038
01:11:12,560 --> 01:11:14,560
and maybe a moon or two.

1039
01:11:15,560 --> 01:11:19,560
What I'm trying to get at is we need to make a big distinction

1040
01:11:19,560 --> 01:11:23,560
between the hierarchy in the real world,

1041
01:11:23,560 --> 01:11:26,560
hierarchical structures in the real world,

1042
01:11:26,560 --> 01:11:30,560
and how we deal with them cognitively,

1043
01:11:30,560 --> 01:11:32,560
where we use attention,

1044
01:11:32,560 --> 01:11:35,560
and we only ever deal with a bit of the hierarchy at a time.

1045
01:11:35,560 --> 01:11:39,560
That's not the same for, say, aspects of language,

1046
01:11:39,560 --> 01:11:40,560
where you have...

1047
01:11:40,560 --> 01:11:42,560
Notice that with vision,

1048
01:11:42,560 --> 01:11:46,560
I can use the same neurons for representing the sun

1049
01:11:46,560 --> 01:11:48,560
and for representing a nucleus.

1050
01:11:49,560 --> 01:11:52,560
It's just an analogy, but it's the same neurons I'm using.

1051
01:11:53,560 --> 01:11:56,560
Now, if I'm processing language,

1052
01:11:56,560 --> 01:11:59,560
I've got things that find me phonemes

1053
01:11:59,560 --> 01:12:01,560
and things that turn phonemes into words

1054
01:12:01,560 --> 01:12:03,560
and things that turn words into sentences,

1055
01:12:03,560 --> 01:12:04,560
and those...

1056
01:12:04,560 --> 01:12:06,560
I can't move a window like that.

1057
01:12:06,560 --> 01:12:07,560
That's a fixed hierarchy.

1058
01:12:07,560 --> 01:12:09,560
There's phonemes, and there's words,

1059
01:12:09,560 --> 01:12:11,560
and there's phrases, and there's sentences,

1060
01:12:11,560 --> 01:12:13,560
and that's all sort of fixed in the brain.

1061
01:12:13,560 --> 01:12:15,560
That's not a flexible matter.

1062
01:12:15,560 --> 01:12:17,560
You can't kind of move the sentences down

1063
01:12:17,560 --> 01:12:19,560
so they're where the words were,

1064
01:12:19,560 --> 01:12:21,560
move the words down so they're where the phonemes were.

1065
01:12:21,560 --> 01:12:23,560
That doesn't work.

1066
01:12:23,560 --> 01:12:26,560
So there's some hierarchies that really do relate

1067
01:12:26,560 --> 01:12:30,560
to sets of neurons in the brain.

1068
01:12:30,560 --> 01:12:33,560
They're like the layers in the connections models.

1069
01:12:33,560 --> 01:12:35,560
There's other hierarchies,

1070
01:12:35,560 --> 01:12:37,560
like the whole spatial structure of the universe,

1071
01:12:37,560 --> 01:12:40,560
where what's in the brain is a window

1072
01:12:40,560 --> 01:12:42,560
you move over that hierarchy.

1073
01:12:42,560 --> 01:12:44,560
Thank you.

1074
01:12:47,560 --> 01:12:50,560
Thanks, Dr. Hinton, for excellent talk

1075
01:12:50,560 --> 01:12:53,560
and excellent ideas about the feasibility of back propagation.

1076
01:12:53,560 --> 01:12:55,560
My question's maybe more boring

1077
01:12:55,560 --> 01:12:58,560
about the statistical comments that you made.

1078
01:12:58,560 --> 01:13:00,560
Is that Dale?

1079
01:13:00,560 --> 01:13:01,560
Pardon me?

1080
01:13:01,560 --> 01:13:02,560
Are you Dale?

1081
01:13:02,560 --> 01:13:04,560
No, I'm Kyle.

1082
01:13:06,560 --> 01:13:08,560
You sound like Dale Shermans.

1083
01:13:08,560 --> 01:13:11,560
I'm at the University of Alberta.

1084
01:13:12,560 --> 01:13:13,560
Hi.

1085
01:13:13,560 --> 01:13:15,560
Well, that's a good instance,

1086
01:13:15,560 --> 01:13:17,560
that you sound just like Dale Shermans

1087
01:13:17,560 --> 01:13:19,560
and you're at the University of Alberta.

1088
01:13:19,560 --> 01:13:23,560
Are you a student of Dale's?

1089
01:13:23,560 --> 01:13:24,560
No.

1090
01:13:27,560 --> 01:13:29,560
I think I've only met a voice.

1091
01:13:29,560 --> 01:13:31,560
If you're a student of Dale's, I need to watch out

1092
01:13:31,560 --> 01:13:33,560
because it's going to be a very tricky question.

1093
01:13:33,560 --> 01:13:38,560
The question is that I was trained with this intuition

1094
01:13:38,560 --> 01:13:40,560
that you can't overparameterize your models,

1095
01:13:40,560 --> 01:13:44,560
that if you're trying to fit a line that you need two points,

1096
01:13:44,560 --> 01:13:46,560
if you're trying to fit a curve you need three and so on

1097
01:13:46,560 --> 01:13:48,560
and that scales up and you should always have

1098
01:13:48,560 --> 01:13:50,560
a little bit less data points.

1099
01:13:50,560 --> 01:13:54,560
I know that you have shown clearly

1100
01:13:54,560 --> 01:13:56,560
and the field has shown that that's not true.

1101
01:13:56,560 --> 01:13:59,560
What were the statisticians getting wrong

1102
01:13:59,560 --> 01:14:01,560
in their logic to be convinced?

1103
01:14:01,560 --> 01:14:03,560
It's to do with regularization,

1104
01:14:03,560 --> 01:14:05,560
that you need it to be highly regularized.

1105
01:14:05,560 --> 01:14:07,560
But first of all, I'll show you

1106
01:14:07,560 --> 01:14:13,560
that if you want to fit two data points,

1107
01:14:13,560 --> 01:14:15,560
well, let's take three.

1108
01:14:15,560 --> 01:14:17,560
If you want to fit three data points,

1109
01:14:17,560 --> 01:14:21,560
you would have told me you want a polynomial

1110
01:14:21,560 --> 01:14:23,560
with only three degrees of freedom,

1111
01:14:23,560 --> 01:14:26,560
so you want a constant and a slope and a curvature

1112
01:14:26,560 --> 01:14:29,560
and that's all you can afford with three data points.

1113
01:14:29,560 --> 01:14:31,560
That's wrong.

1114
01:14:31,560 --> 01:14:33,560
Now, this is where we need a pen.

1115
01:14:33,560 --> 01:14:35,560
Oh, sorry.

1116
01:14:37,560 --> 01:14:38,560
They don't work.

1117
01:14:38,560 --> 01:14:40,560
I tried them all and none of them work.

1118
01:14:41,560 --> 01:14:42,560
Okay, did they work?

1119
01:14:42,560 --> 01:14:44,560
Oh, well done.

1120
01:14:44,560 --> 01:14:46,560
Extra points.

1121
01:14:48,560 --> 01:14:50,560
Okay, yes.

1122
01:14:50,560 --> 01:14:52,560
Can you see that?

1123
01:14:52,560 --> 01:14:53,560
Yeah.

1124
01:14:53,560 --> 01:14:55,560
Okay, and we're going to have three data points.

1125
01:15:03,560 --> 01:15:07,560
And actually, if you're a statistician,

1126
01:15:07,560 --> 01:15:09,560
you'd probably say for three data points

1127
01:15:09,560 --> 01:15:12,560
you'd probably ought to fit a straight line like this.

1128
01:15:12,560 --> 01:15:15,560
Because I could fit a parabola

1129
01:15:15,560 --> 01:15:17,560
and the parabola would fit exactly

1130
01:15:17,560 --> 01:15:20,560
and that's a bit suspicious.

1131
01:15:20,560 --> 01:15:23,560
In other words, the parabola fits exactly,

1132
01:15:23,560 --> 01:15:29,560
but do you really believe that if you were to ask

1133
01:15:29,560 --> 01:15:31,560
when x is zero, what's the value of y?

1134
01:15:31,560 --> 01:15:33,560
Do you really believe that value for y?

1135
01:15:33,560 --> 01:15:36,560
Because a straight line is far more conservative.

1136
01:15:36,560 --> 01:15:38,560
So a statistician would probably say

1137
01:15:38,560 --> 01:15:40,560
fit a straight line.

1138
01:15:40,560 --> 01:15:43,560
However, that would be a frequentist statistician.

1139
01:15:43,560 --> 01:15:46,560
If you took a Bayesian statistician,

1140
01:15:46,560 --> 01:15:49,560
and this is in Chris Fisher's machine learning textbook,

1141
01:15:49,560 --> 01:15:51,560
there's a nice picture of it, I think, somewhere.

1142
01:15:51,560 --> 01:15:53,560
Think it's that book.

1143
01:15:53,560 --> 01:15:55,560
A Bayesian statistician would say,

1144
01:15:55,560 --> 01:15:59,560
okay, let's try fitting fifth-order polynomials.

1145
01:15:59,560 --> 01:16:02,560
And fifth-order polynomials,

1146
01:16:02,560 --> 01:16:05,560
we might even fit ones that don't exactly go through the data,

1147
01:16:05,560 --> 01:16:07,560
but for now let's make them go through the data.

1148
01:16:07,560 --> 01:16:09,560
So we fit a fifth-order polynomial

1149
01:16:09,560 --> 01:16:12,560
that goes kind of...

1150
01:16:15,560 --> 01:16:16,560
One, two, three...

1151
01:16:16,560 --> 01:16:18,560
Well, you know, some order.

1152
01:16:18,560 --> 01:16:20,560
And we fit another one.

1153
01:16:20,560 --> 01:16:23,560
Oh, that didn't go through the data.

1154
01:16:27,560 --> 01:16:29,560
And we keep fitting these guys,

1155
01:16:29,560 --> 01:16:31,560
and we fit a gazillion of them.

1156
01:16:31,560 --> 01:16:34,560
And what you see at the end is that

1157
01:16:34,560 --> 01:16:37,560
these gazillion ones, in between the data points,

1158
01:16:37,560 --> 01:16:39,560
they're kind of all over the place,

1159
01:16:39,560 --> 01:16:43,560
and their average is in a sensible place like here,

1160
01:16:43,560 --> 01:16:45,560
but their variance is big.

1161
01:16:45,560 --> 01:16:47,560
And what they're telling you is,

1162
01:16:47,560 --> 01:16:49,560
if you give me this x-coordinate,

1163
01:16:49,560 --> 01:16:51,560
I'm rather uncertain about this y-coordinate,

1164
01:16:51,560 --> 01:16:53,560
but this is a good bet.

1165
01:16:53,560 --> 01:16:55,560
And similarly here,

1166
01:16:55,560 --> 01:16:57,560
and if you go out here,

1167
01:16:57,560 --> 01:16:59,560
these polynomials are just all over the place,

1168
01:16:59,560 --> 01:17:01,560
and they'll tell you,

1169
01:17:01,560 --> 01:17:03,560
if you give me this x-value,

1170
01:17:03,560 --> 01:17:06,560
then it could be pretty much anything.

1171
01:17:06,560 --> 01:17:08,560
That's not a bad bet,

1172
01:17:08,560 --> 01:17:10,560
but it could be pretty much anything.

1173
01:17:10,560 --> 01:17:12,560
And that's a much better answer

1174
01:17:12,560 --> 01:17:14,560
than you get from a straight line.

1175
01:17:14,560 --> 01:17:16,560
So by fitting a very large number

1176
01:17:16,560 --> 01:17:18,560
of different polynomials,

1177
01:17:18,560 --> 01:17:20,560
and then averaging,

1178
01:17:20,560 --> 01:17:22,560
you get good, mean answers,

1179
01:17:22,560 --> 01:17:24,560
and you also get a sense of the variance.

1180
01:17:24,560 --> 01:17:26,560
Now, Drop-Out is doing something like that.

1181
01:17:26,560 --> 01:17:28,560
Yes, and that's brilliant.

1182
01:17:28,560 --> 01:17:31,560
Thank you for coming up with Drop-Out.

1183
01:17:32,560 --> 01:17:35,560
Many of us here

1184
01:17:35,560 --> 01:17:38,560
are working in a regime of sparse data,

1185
01:17:38,560 --> 01:17:40,560
and so we have a couple channels,

1186
01:17:40,560 --> 01:17:42,560
a couple signals, a couple voxels,

1187
01:17:42,560 --> 01:17:46,560
and you've convinced us that we need more,

1188
01:17:46,560 --> 01:17:50,560
but is there a way forward in AI

1189
01:17:50,560 --> 01:17:53,560
that can manage with more sparse data,

1190
01:17:53,560 --> 01:17:55,560
or is this the only regime

1191
01:17:55,560 --> 01:17:57,560
that's going to be able to make success?

1192
01:17:57,560 --> 01:18:00,560
So the really big successes

1193
01:18:00,560 --> 01:18:02,560
have been on big databases,

1194
01:18:02,560 --> 01:18:05,560
and I think we should be using

1195
01:18:05,560 --> 01:18:07,560
even bigger models,

1196
01:18:07,560 --> 01:18:10,560
but you can't get away from the fact that actually,

1197
01:18:10,560 --> 01:18:12,560
if you're going to have something

1198
01:18:12,560 --> 01:18:14,560
that starts off random

1199
01:18:14,560 --> 01:18:16,560
and sucks all its knowledge from the data,

1200
01:18:16,560 --> 01:18:17,560
you'd better have enough data

1201
01:18:17,560 --> 01:18:19,560
to suck all that knowledge from.

1202
01:18:19,560 --> 01:18:21,560
The bigger your model, the better,

1203
01:18:21,560 --> 01:18:22,560
if you regularize it,

1204
01:18:22,560 --> 01:18:24,560
but you still need a lot of data.

1205
01:18:24,560 --> 01:18:26,560
So the way you should think about it is this.

1206
01:18:26,560 --> 01:18:28,560
If you've got 100,000 data points,

1207
01:18:28,560 --> 01:18:30,560
that's small.

1208
01:18:30,560 --> 01:18:32,560
I know that's very depressing

1209
01:18:32,560 --> 01:18:33,560
if you're a neuroscientist.

1210
01:18:33,560 --> 01:18:34,560
Well, it's not depressing.

1211
01:18:34,560 --> 01:18:36,560
It seems impossible.

1212
01:18:36,560 --> 01:18:38,560
If you want to personalize medicine

1213
01:18:38,560 --> 01:18:39,560
for one individual

1214
01:18:39,560 --> 01:18:41,560
and you want to train a model

1215
01:18:41,560 --> 01:18:43,560
on their data from their brain,

1216
01:18:43,560 --> 01:18:45,560
it seems like there's going to be a disconnect

1217
01:18:45,560 --> 01:18:47,560
between what these models can do

1218
01:18:47,560 --> 01:18:49,560
and how they might help someone in the future.

1219
01:18:49,560 --> 01:18:50,560
Yes and no.

1220
01:18:50,560 --> 01:18:52,560
If I train a model

1221
01:18:52,560 --> 01:18:54,560
on a very large number of people

1222
01:18:54,560 --> 01:18:56,560
and then apply that model to one person,

1223
01:18:56,560 --> 01:18:58,560
that's the form of personalize medicine

1224
01:18:58,560 --> 01:19:00,560
that really works.

1225
01:19:00,560 --> 01:19:02,560
Great, thanks.

1226
01:19:11,560 --> 01:19:13,560
Oh, and say hi to Dale for me.

1227
01:19:18,560 --> 01:19:21,560
Thank you for the presentation.

1228
01:19:21,560 --> 01:19:23,560
My last question is about this dropout.

1229
01:19:23,560 --> 01:19:25,560
The thing is that you randomly just drop

1230
01:19:25,560 --> 01:19:27,560
some parts of the network

1231
01:19:27,560 --> 01:19:29,560
and then you say, okay,

1232
01:19:29,560 --> 01:19:31,560
that it works better so I would accept it.

1233
01:19:31,560 --> 01:19:33,560
But do we have any, like, intuition

1234
01:19:33,560 --> 01:19:35,560
why, for example, some parts of it work better

1235
01:19:35,560 --> 01:19:37,560
or if we try to embed this, like,

1236
01:19:37,560 --> 01:19:39,560
network into, like,

1237
01:19:39,560 --> 01:19:41,560
is it more graph isomorphism?

1238
01:19:41,560 --> 01:19:43,560
What are these two different graphs,

1239
01:19:43,560 --> 01:19:45,560
different graphs that we took?

1240
01:19:45,560 --> 01:19:47,560
Are there any similarities

1241
01:19:47,560 --> 01:19:49,560
between them or just with randomly?

1242
01:19:49,560 --> 01:19:51,560
I guess the problem with the randomness,

1243
01:19:51,560 --> 01:19:53,560
I guess we are trying to put

1244
01:19:53,560 --> 01:19:55,560
the burden of prediction

1245
01:19:55,560 --> 01:19:57,560
on the random part of the computer desk.

1246
01:19:57,560 --> 01:19:59,560
So I didn't hear

1247
01:19:59,560 --> 01:20:01,560
the whole question,

1248
01:20:01,560 --> 01:20:03,560
but certainly in dropout what we do

1249
01:20:03,560 --> 01:20:05,560
is we randomly leave out units.

1250
01:20:05,560 --> 01:20:07,560
Now you can also do block dropout.

1251
01:20:07,560 --> 01:20:09,560
You can take groups of units

1252
01:20:09,560 --> 01:20:11,560
and randomly leave out the groups.

1253
01:20:11,560 --> 01:20:13,560
And what that does is it allows the units

1254
01:20:13,560 --> 01:20:15,560
within a group to collaborate with one another,

1255
01:20:15,560 --> 01:20:17,560
and then between groups

1256
01:20:17,560 --> 01:20:19,560
they have to be fairly independent.

1257
01:20:19,560 --> 01:20:21,560
And that's called block dropout

1258
01:20:21,560 --> 01:20:23,560
but I didn't really hear

1259
01:20:23,560 --> 01:20:25,560
the rest of your question.

1260
01:20:25,560 --> 01:20:27,560
Okay.

1261
01:20:27,560 --> 01:20:29,560
The question was about

1262
01:20:29,560 --> 01:20:31,560
that, okay, imagine that you...

1263
01:20:31,560 --> 01:20:33,560
Can you talk closer to the microphone

1264
01:20:33,560 --> 01:20:35,560
because I'm partially dead?

1265
01:20:35,560 --> 01:20:37,560
The question is that

1266
01:20:37,560 --> 01:20:39,560
imagine that you have a dropout of 50%

1267
01:20:39,560 --> 01:20:41,560
and you're trying to get rid

1268
01:20:41,560 --> 01:20:43,560
of, like, 50% of your nodes

1269
01:20:43,560 --> 01:20:45,560
and the nodes in the network.

1270
01:20:45,560 --> 01:20:47,560
And the question is, okay,

1271
01:20:47,560 --> 01:20:49,560
whether we have any similarity between the types

1272
01:20:49,560 --> 01:20:51,560
if we do it iteratively,

1273
01:20:51,560 --> 01:20:53,560
whether we would find any similarity

1274
01:20:53,560 --> 01:20:55,560
on the structure of the network

1275
01:20:55,560 --> 01:20:57,560
that would produce the best results

1276
01:20:57,560 --> 01:20:59,560
and if it's so, whether it would correspond

1277
01:20:59,560 --> 01:21:01,560
to something physical, like, for example,

1278
01:21:01,560 --> 01:21:03,560
if you're doing a vision thing,

1279
01:21:03,560 --> 01:21:05,560
whether it would correspond to something in brain or not, I guess.

1280
01:21:05,560 --> 01:21:07,560
Yeah.

1281
01:21:07,560 --> 01:21:09,560
Lots of people have thought about whether you can do better

1282
01:21:09,560 --> 01:21:11,560
than random in dropout.

1283
01:21:11,560 --> 01:21:13,560
And there's some work on that, like,

1284
01:21:13,560 --> 01:21:15,560
block dropout that works, can work for some things.

1285
01:21:15,560 --> 01:21:17,560
But I don't really have much to say

1286
01:21:17,560 --> 01:21:19,560
about...

1287
01:21:19,560 --> 01:21:21,560
I don't really know the answer to

1288
01:21:21,560 --> 01:21:23,560
sort of, is there something much

1289
01:21:23,560 --> 01:21:25,560
more sensible than dropout

1290
01:21:25,560 --> 01:21:27,560
that's a lot more structured?

1291
01:21:27,560 --> 01:21:29,560
There might well be, but I...

1292
01:21:29,560 --> 01:21:31,560
Thank you.

1293
01:21:31,560 --> 01:21:33,560
Okay, so with that, we're going to have to end.

1294
01:21:33,560 --> 01:21:35,560
So please join me in

1295
01:21:35,560 --> 01:21:37,560
thanking Jeff for...

1296
01:21:37,560 --> 01:21:39,560
Thank you.

1297
01:21:39,560 --> 01:21:41,560
APPLAUSE

1298
01:21:41,560 --> 01:21:43,560
And

1299
01:21:43,560 --> 01:21:45,560
I wanted also

1300
01:21:45,560 --> 01:21:47,560
to thank Blake

1301
01:21:47,560 --> 01:21:49,560
for hosting this event,

1302
01:21:49,560 --> 01:21:51,560
and I felt the questions

1303
01:21:51,560 --> 01:21:53,560
could have gone on all night,

1304
01:21:53,560 --> 01:21:55,560
but the tip-off isn't half an hour,

1305
01:21:55,560 --> 01:21:57,560
so some of us have to move on.

1306
01:21:57,560 --> 01:21:59,560
So thank you, Blake.

1307
01:21:59,560 --> 01:22:01,560
APPLAUSE

