Hi, I think we're ready to start, so my name is Paul Franklin and I'm a Neuroscientist
here at Sick Kids, and also I'm Program Chair for the Canadian Neuroscience Meeting that
takes place in Toronto all this week.
So the traditional curtain raiser for the Neuroscience Meeting, the CAN meeting, is the public lecture.
And this year we decided to focus on the interface between neuroscience and AI.
We did that for two reasons.
The first reason is that AI, or Toronto, is one of the main hubs in the world for AI research.
And the second reason is that it's also home to one of the true pioneers in this field,
also known as the godfather of deep learning, Jeff Hinton.
And so when we asked Jeff if he'd participate in this event, I think a year and a half ago
we asked him and he said, yes, we were super excited.
So at this point I want to hand over to Blake Richards, and Blake Richards is an associate
professor at University of Toronto Scarborough, and he's going to host this evening's event.
Blake?
Thanks, Paul.
So just as a brief introduction, I wanted to tell you a little bit more about Jeff.
So you might be surprised to learn that the godfather of deep learning, associated mostly
with AI, got his BA in experimental psychology from Cambridge originally.
He then went on to do his PhD in artificial intelligence in Edinburgh, so he did get
started relatively early.
But throughout his early career, he really contributed to the first wave of what was
known at the time as parallel distributed processing or connectionism, which really brought back
into the fore the idea of using neural networks for both models of the mind and for artificial
intelligence.
Now Jeff got his first tenure track position at Carnegie Mellon in the 80s, but we were
able to steal him away from them in the late 80s, which I understand was largely because
of his ethical objections to DARPA funding.
So once again, Canada's political bent has helped us in our research endeavors.
Over the course of the 90s, Jeff continued to really push neural networks and machine
learning forward.
And sorry about that.
In 98, he actually went to University College London to found the Gatsby computational
neuroscience unit, and we might have lost him, but thankfully we pulled him back again.
He came back to Toronto in 2001, where he became a university professor in 2006 and
then an emeritus professor in 2014.
Now I think that you all know that Jeff is a monumental figure within artificial intelligence
and machine learning, and he's been critical to the founding of the Vector Institute here
in Toronto and putting Toronto on the map for AI.
And certainly he's had incredible recognitions of his work, most recently the Turing Award,
which he shared with Joshua Bengio and Yann LeCun, as well as the Order of Canada.
And he's a distinguished fellow of the Canadian Institute for Advanced Research, which I highlight
because they were one of the people who continued to support neural networks throughout the
time when it wasn't as faddish.
But for all his successes and his technical endeavors, I think one of the things that's
most important to understand about Jeff is the impact that he's had on other scientists.
Jeff has really molded the career of so many people and changed the way that they think
about things.
When you look at the people who have been his graduate students or postdocs, it really
is the who's who in artificial intelligence.
It includes people like Max Welling, Yann LeCun, and you know, the phrase I used to describe
it is, have you drunk Jeff's Kool-Aid?
Because once you've drunk Jeff's Kool-Aid, there is no going back.
You see neural networks, you see AI differently, and I would argue you also see neuroscience
differently.
And for me, my understanding of the brain has been largely shaped by Jeff and his work.
But you know, we're at the point now where computer science has drunk Jeff's Kool-Aid.
So he's got an H index of 145, and according to Google Scholar, his work has been cited
270,000 times, which is more than Einstein, Ramoni, Cajal, and Alan Tern combined.
But that's largely from computer scientists.
And if my prediction is correct, neuroscience 30, 40 years from now will also have drunk
Jeff's Kool-Aid, and maybe you're going to get your first taste tonight.
So with that, I hand you over to Jeffrey Hinton.
So thank you very much, Blake.
I can give you some more Kool-Aid today.
It's Kool-Aid produced by one of my former students, Ilya Sutskava.
First, I want to tell you a little bit about the history of deep learning in AI.
Can I just ask before I start, how many people here know what the back propagation algorithm
is?
Put your hands up.
So some people don't.
I'll explain it very quickly, and I'll explain it in such a way that you'll be able to explain
to other people.
So if you do know what it is, you follow the explanation from the point of view of how
you explain it.
Okay.
There was a war between two paradigms for AI.
There were people who thought that the essence of intelligence was reasoning, and logic is
what does reasoning, so we should base artificial intelligence on taking strings of symbols and
manipulating them to arrive at conclusions.
And then there were other people who looked at the brain and said, no, no, intelligence
is all about adapting connections in the brain to get smarter.
And this war went on for a long time, and eventually, people who were trying to figure
out how to change connections between fake neurons to make these networks smarter got
to be able to do things that the people doing symbolic AI just couldn't do at all.
And now there's a different way of getting a computer to do what you want.
Instead of programming it, which is tedious, you just showed examples, and it figures it
out.
Now, of course, you have to write the program that figures it out, but that's just one program
that will then do everything.
And this is an example of what it can do.
So the image, just think of the numbers.
They're RGB values of pixels, and that's the input to the computer.
Lots of values of pixels, just real numbers, saying how bright the red channel is.
And you have to turn those numbers into a string of words that says a close-up of a
child holding a stuffed animal.
And imagine writing that program.
Well, people in conventional AI had tried to write that program and they couldn't, partly
because they didn't know how we did it.
We still don't know how we do it, but we can get artificial neural networks to do it now
and do a pretty good job.
My prediction is, within 10 years, if you go and get a CT scan, what will happen is
a computer will look at the CT scan, and a computer will produce the written report
that the radiologist currently produces.
Radiologists don't like this idea.
Okay.
Here's a simplified model of a neuron.
It's very simple.
It gets some input, which is just the activity on the input lines times the weights, adds
it all up.
That's called the depolarization.
And then it gives an output that's proportional to how much input it gets as long as it gets
enough input.
And so, to begin with, we won't have spiking neurons.
These are just going to be neurons that send real values in just the way neurons don't.
We're going to make networks of them by hooking them up into layers.
And you could put some pixels on the input neurons.
Look.
They're the input neurons.
And you go forwards through the net until you get outputs.
And then you compare those outputs with what you ought to have got, so you have to know
what the right answer is.
And what we'd like to do is train the weights, these red and green dots, so that it gives
the right output.
Now, I'm going to show you a way of training the weights that everybody can understand,
and everybody is thought of, basically.
What you do is you start with random weights, you show it some inputs, you measure how well
it does, then you change one weight a tiny bit.
So I take that weight there, and I just change it a tiny bit, and then I show it the same
inputs again and see if it does better or worse.
If it does better, I keep the change.
If it does worse, maybe I keep the change in the opposite direction.
That's an easy algorithm to understand.
And that algorithm works.
It's just incredibly slow.
You have to show it lots of examples, change your weight, and then show it lots more examples,
change another weight.
And every weight has to be changed many times.
So if you use calculus, you can go millions of times faster.
So the trick of this algorithm, the sort of mutation algorithm, is you have to measure
the effect of the weight change on the performance.
But you don't really need to measure it, because when I change one of these weights, the effect
that it has on the output is determined by the network.
It just depends on the other weights in the network.
It's not like normal evolution, where the effect of a gene depends on the environment
you're in.
This is all kind of internal to the brain.
And so changing one of these weights has an effect that's predictable here.
So I ought to be able to predict how changing the weight will help get the right output.
And so what back propagation does is basically says, I'm going to compute using an algorithm,
the details of which I won't tell you, and compute for every weight, all at the same
time, how changing that weight would improve the output.
And then I'm going to change all the weights a little bit.
So every weight changes in direction to improve the output, and the output improves quite
a bit, and then I do it all again.
Now that allows me to compute for every weight what direction I'd like to change it in.
And the question is, should I, when I show examples, show all of the examples, and then
update the weights, so should you live your whole life with the synapse strengths you're
born with, then update your weights a little bit, then live your life again, and update
the weights a little bit more, that doesn't seem very good, or should you take one case
or a few cases, figure out how you'd like to update the weights, update them, and then
take more cases.
That's the online algorithm, and that's what we do.
And the amazing thing is, it works.
You can take one case at a time, or you can take small batch of cases, you update the
weights, and these networks get better.
And it's very surprising how well it works on big data sets.
So for a long time, people thought, you're never going to be able to learn something
complicated, like, for example, take a string of words in English, feed them into a neural
net, and output a string of words in French that mean the same thing.
You're never going to be able to do that if you start with a big neural net with just
random weights.
It's just asking too much for the neural net to organize itself so it can do transactions
because you have to kind of understand what the English says.
And people predicted this was completely impossible, but you'd have to put in lots of prior knowledge.
Well, they were wrong.
So in 2009, my students in Toronto showed that you could actually improve speech recognizers
using these neural nets that had random weights.
They were just trying to predict in a spectrogram which piece of which phoneme you were trying
to say in the middle of the spectrogram.
And then there was more to the system that wasn't neural nets.
Now what we've done is we've got rid of all the stuff that wasn't neural nets, and now
you can take sound waves coming in and you have transcriptions coming out, or even better,
you have sound waves coming in and you have sound waves coming out in another language
with the same accent.
They can do that now.
That's speech recognition done.
And in 2012, two of my students took a big database of images and used essentially the
same algorithm, the few clever tricks, to say what was in the image.
Not a full caption, just the class of the most obvious object, and they did much better
than conventional computer vision, which had been going for many years, and since then
all the best recognizers have used neural nets.
In 2011, you couldn't publish a paper out neural nets in the standard computer vision
conference because they said they were rubbish.
In 2014, you couldn't publish a paper that wasn't about neural nets.
And in 2014, they did something that I didn't expect.
This was done by people at Google, not me, and Joshua Benjo in his group in Montreal,
particularly by a guy called Bardenao and Cho.
They managed to get a neural net, so you feed in actually fragments of words in one language.
You have 32,000 possible fragments.
So the word the in English would be one of the fragments, but so with things like ing
and un.
And what comes out in another language is fragments of words in that other language, and it's
a pretty good translation, and that's how Google now does translation.
So it did translation better than symbolic AI.
So what changed between 1986 and 2009?
And it was basically computers got faster.
That was the main change.
Data sets got bigger.
We developed some clever tricks, and we like to emphasize those, but it was really the
computers getting faster and data sets getting bigger.
But I'll emphasize the clever tricks nonetheless.
And I can tell you about two clever tricks.
I can tell you about transformers, and I can tell you about better ways of stopping neural
networks from overfitting.
But first I want to show you an example of what neural nets can do now.
So a team at OpenAI took work on transformers that was originally done at Google.
They developed it a little bit further, and they applied it to big neural nets that have
1.5 billion learnable connection strengths.
So they're learning 1.5 billion numbers.
That's the knowledge of the system.
And they train it up on billions of words of English text, and all the net's trying
to do is predict the next word.
So what the net will do, or fragment of word, the net will give you probabilities for the
next word.
So if you give it some words, a lead-in, it'll give you probabilities for the next word.
And once the net's trained, what you can do is you can look at those probabilities,
and if it says there's a probability of 0.4 that the next word is the, you pick the with
probability 9.4, and if it says fish with probability 0.01, you pick fish with probability
0.01.
And so you just pick from its distribution, and then you tell the neural net, okay, the
one I picked was the next word, what do you think comes after that?
And this way you can get it to sort of reveal what it really believes about the world.
So you're getting it to predict words one at a time, and every time it makes a prediction
you say you were right, and it just gets more and more carried away.
So they initiated it with some interesting text.
And the question is, will the neural net then produce stuff that's sort of related to that?
I mean, the first question is, will it produce English words?
Will the words have decent syntax?
Will it have any meaning?
Will it be related to this?
If you're really optimistic, you might say, will they sort of relate to the fundamental
problem here, which is how these unicorns can speak English?
Okay, so here goes.
This is what the neural net produced.
Now this was cherry-picked.
This was one of their better examples.
The neural net just made this up, right?
It made up Dr. Jorge Perez, there is no such person at the University of La Paz, but it's
pretty plausible because it's South America, and I believe La Paz has a university.
Okay, so that's the first bit of what it made up, and it carries on and it gets better.
The next bit sounds a bit like one of those fantasy games.
So it's remembered about unicorns and herds of unicorns, right?
So they walk up and there's this strange valley, and it's a very strange valley, and they found
the herds of unicorns.
And it has something about seeing them from the air and being able to touch them, which
isn't quite right.
So people in Symbolicae leap on this and say, you see, it doesn't understand.
Well, sure, there's little bits that it doesn't get right.
But notice, it's remembered that these unicorns have to speak English, and so it tells you
about, you know, they spoke some fairly regular English.
It doesn't know the difference between dialect and dialectic, but my kids don't know that
either.
In fact, I'm not sure I know.
It's a tribute to unicorns to Argentina, even though Dr. Perez comes from Bolivia.
And it actually understands about magic realism.
So they're descendants of a lost race, and I love the bit at the end where it says, in
South America, such incidents seem to be quite common.
This has an ability to just make up something that fits your prejudices and sounds moderately
plausible, like a certain president.
And it finally gets to the point which is, if you really want to know whether these unicorns
were used by breeding with these strange lost race of people, you ought to do a DNA test.
Okay?
It understands that.
Okay.
So that's what neural nets can do now.
This was a neural net with 1.5 billion connections that was trained on Google's, actually, I
withdraw that, 1.5 billion connections is trained on a lot of hardware.
And we look at what it says, and we sort of laugh at how, you know, it's pretty good,
but it hasn't got it quite right, but it's pretty good.
Okay.
What they've done now is they've trained a neural net with 50 billion connections on
Google's latest cloud hardware, which is, it's like having several of the world's biggest
supercomputers going for you for months.
The net with 50 billion connections, I haven't seen any text from it yet, but my prediction
is it's sitting around laughing at how cute what we produce is.
Okay.
So one thing about that net is it's clearly very well aware of the initial context.
These unicorns in a valley that speak English, and it's remembering this initial context
a long time later, and a recurrent neural net can't do that.
A recurrent neural net would have forgotten about the initial stuff and wouldn't produce
such good context-dependent stuff.
So the way this works is the word comes in, the neural net activates some hidden units.
That pattern of activity in the hidden units goes and compares itself with previous patterns,
your point of view, with previous patterns at earlier times.
And when it finds a pattern at an earlier time that's a bit similar, it says that we'll
take advice from that previous hidden pattern about how to affect the next layer.
And so actually a word comes in and how one pattern of activity in the bottom layer of
the hidden neurons affects the next layer is dependent on what happened previously.
Now it's dependent in quite a complicated way, and this seems very implausible for a
brain because what's happening in the computer is you're storing all these activity patterns
that are meant to be neural activity patterns or light neural activity patterns, and you're
comparing, and this looks hopeless.
But actually all you need to do is every time you have an activity pattern, and you use
the outgoing weights to affect the next layer, just change the weight slightly with heavy
and learning.
So now what's going to happen is that weight matrix that comes out of that activity pattern
is going to be modified slightly.
Now when I get a new activity pattern, if the activity pattern is orthogonal to the previous
activity pattern, then any modifications you made in the weight matrix due to that previous
activity pattern won't make any difference.
But if it lines up with the previous activity pattern, if it's similar, the modifications
you made in the weights back there, the temporary modifications, will cause this new activity
pattern to have a different effect here.
So you'll get that long temporal context, and the way to store a long temporal context
is not to keep copies of neural activity patterns, it's to take your weights and to have temporary
changes to the weights, which I call fast weights.
So you temporarily change them, and these changes decay over time, so you'll have a memory.
So if you ask, where in your brain is your memory of what I said a few minutes ago?
I'll ask the younger people this, because for the older people it's nowhere.
But for the younger people, it's somewhere you can, if I were to say something I said
a few minutes ago, like these big neural nets are now laughing at us, you remember I said
that, where was that memory?
I think it's in the temporary changes to the weights, because that's got much bigger capacity
than activities of neurons, and you don't need to use up neurons just sitting there remembering.
And those temporary changes don't need to be driven by back propagation, they can just
be heavier.
Okay, so I've tried to relate these wonderful nets that can make up stories with an idea
about where short-term memory is in the brain.
And now I'll talk about where the cortex can do back propagation.
So neuroscientists, 20 years ago neuroscientists said don't be ridiculous, of course the brain
can't do back propagation, and they'd interpret it very literally as sending signals backwards
down the same axons and saying neurons don't do that, no thanks.
But now we know that back propagation works really well for solving tough practical problems.
So that's rather changed the balance, because when back propagation was just a theory of
how you might get computers to learn something, and when it learns some simple things, it
wasn't sort of imperative to understand whether the brain did it.
But now we know that you can do all these things with back propagation.
What's more, we know that back propagation is the right thing to do, but if you have
a sensory pathway, and you want to take the early feature detectors so that their outputs
are more helpful for making the right decision later on in the system, then what you really
need to do is ask the question, how should I change the receptive field of this early
detector so that what is output helps with the decision?
And what you have to do is do back propagation to compute that, that's the efficient way
to compute it.
And I think it'd be crazy if the brain wasn't somehow doing this.
So why do neuroscientists think it's impossible?
Apart from silly objections like things don't go backwards down axons, at least not at the
right speed.
He wants me to update things.
Oh, it's just died.
I'm going to go out and present them out.
I'm going to go back in to present them out.
Okay, so here's some reasons why the brain can't do back propagation.
The first reason is they say, well, it doesn't get the supervision signal.
And they're imagining that the supervision signal is like you take a micro pipette and
you put it into the infrotemporal cortex and you inject the right answer and the brain
doesn't have anything like that, right?
But actually, if you take that language model, it didn't need label data, it was just trying
to predict the next word.
So you can often use part of the input, maybe a future part of the input, or maybe a small
part of an image as the right answer, and so you can get supervision signals easily.
So there's no problem about supervision signals.
The second reason is neurons don't send real-valued activities, they send spikes.
And back propagation is using these real-valued activities so you can get nice, smooth derivatives.
So back propagation can't possibly be what's going on in the brain.
The third objection is, neurons have to send two signals.
They have to send the activity forwards and they have to send error derivatives backwards.
The signal they have to send backwards is, how sensitive am I to changes in my input?
Or rather, if you change my input, how much does that help with the final answer?
And the last thing is about neurons having reciprocal connections because you have to,
when you send things backwards, if you use a different neuron, you have to use the same
weight as the forward weight.
I'm not going to tell you how you can overcome that, but you can easily.
So supervision signals isn't really a problem, there's many ways to get a supervision signal.
The simplest is predicting what comes next.
Now the question of can neurons communicate real values?
Well the first thing to notice about back propagation is, if you have very noisy estimates
of the gradient, it works just as well.
It's very, very tolerant of noise as long as it's unbiased noise.
So for example, the signal you send forwards can be one bit, one stochastic bit, and the
signal you send backwards can be two bits.
If they have the right average value, if their expected values are correct, then they're
just this expected value plus some noise, and the whole system still works fine.
So in the brain, you have a neuron.
At any instant, the neuron has an underlying firing rate, and it produces spikes, and for
now let's just suppose it produces spikes according to a Poisson process.
So it's probability of producing a spike in a small interval, which is the underlying
firing rate.
The question is, suppose we treated it as if it could send that underlying firing rate.
When it sends a Poisson spike, it's just a very noisy version of the underlying firing
rate.
It's a one or a zero, but its expected value is the underlying firing rate.
So how well do neural networks work if we send very noisy signals?
So I'm going to have a statistics digression.
If you do statistics 101, they tell you you shouldn't have more parameters in your model
than you have data points.
You really ought to have quite a few data points for each parameter.
It turns out this is completely wrong.
The Bayesians knew it was wrong, actually.
The brain is not in the same regime of statistics 101.
In the brain, you're fitting about 10 to the 14 parameters, and you have about 10 to the
nine seconds.
So even if you have sort of 10 experiences per second, so even if you take 100 milliseconds
is the time for an experience, that's the kind of backward masking time, you have like
10,000 synapses per 100 milliseconds of your life.
You're throwing a lot of parameters.
So if your mother just kept saying, good, bad, good, bad, good, bad, she couldn't possibly
provide enough information to learn all those 10 to the 14 parameters.
And here's what they teach you wrong in statistics.
Everybody knows that if you've got a given size model with a given number of parameters,
the more data you have, the better you'll generalize.
So for a given size of model, it's always better to have more data.
In fact, the best thing you can do is get more data.
Okay, but that doesn't mean that if you've got a fixed amount of data, you should make
it look like a lot by having a small model.
That's what they tell you in statistics 101.
Okay?
Big models are good if you regularize them, if you stop them doing crazy things.
We can see that using a lot of parameters is good, that you can always win by having
more parameters.
And the way you do that is say, I'm going to have a committee.
I'm going to learn lots of different little neural nets.
If you give me more parameters, I'll learn more different neural nets.
And then I'll average what they all say.
And you'll always win.
It's a sort of declining win, but if you have enough of them, you'll win by having more.
So it's always better to have more parameters.
It turns out if you have a fixed amount of data and you have enough computation power,
the brain has, you should always use such a big model that the amount of data looks
small.
That's the regime you ought to be in for a fixed amount of data.
That is, if you take the limit when the amount of data is fixed and you have unlimited computation
and ask now how big would you like your model to be, you'd like your model to be much bigger
than the data.
Okay.
That only works if you have a good regularizer.
And I'm now going to tell you a very good regularizer called Dropout.
So this is to use in neural networks where you have a lot more parameters than you have
data points to train them on.
And you could learn an ensemble of little models, and this is a way of learning an ensemble
of many more models, but the models in the ensemble can share things with each other.
So the idea is, if we just have one hidden layer in a neural net, we put the data in
and each time we show the data vector, we randomly remove half the neurons.
So we randomly get rid of half the neurons in your brain and only use the ones that remain.
And it's a different subset we remove each time.
Now when we do use a neuron, we use it with the same weights each time.
So what you've got is if you've got h hidden neurons, you've got two to the h different
subsets of neurons you might use.
So you actually have two to the h different models, exponentially many models.
Most of the models are never used.
A few of the models will see one example, a small fraction of them will see one example.
No models will see two examples.
And yet they can learn because they're all sharing parameters.
So this idea of sharing parameters in a neural network is very effective.
So really you've got all these different models that are sharing parameters and you train
it up and it generalizes really well.
So I said that.
So what we know is if you get rid of a fraction of the neurons each time and treat it as though
they weren't there, it works really well.
That's just a form of noise.
And basically this is just an example of if you have a very big model and you add a lot
of noise, the noise allows it to generalize well and it's better to have a big model with
a lot of noise than to have a small model with no noise.
And so what the brain wants, because it's got such a big model compared with the amount
of data it operates on, it wants a lot of noise.
And so now a Poisson neuron is kind of ideal.
It's got a firing rate and now it adds a whole lot of noise to that and either sends a one
or a zero and that actually makes it generalize much better.
So the argument is the reason neurons don't send real values is they don't want to.
They want to send things with a lot of noise in and that's making them generalize better.
So that's not an argument against backpropagation.
These dropout models are trained with backpropagation.
So the random spikes are really just a way of adding noise to the signal to get better generalization.
And now the last thing I'm going to address, I'm going to keep going till Blake stops me
and I figure I've got about another five minutes before he gets really ratty.
So the output of a neuron represents the presence of a feature in the current input.
So it's obvious the same output can't represent the error derivative, right?
You couldn't have a neuron that said to higher layers, this is the value of my feature
and said to lower layers, this is my error derivative.
It couldn't be done.
So the neurons that go backwards need to be different neurons except that that's nonsense.
So here's my claim.
Joshua Benjo picked up on this later.
I made this claim first in 2007.
Actually, I made it first in the ground proposal.
And I still believe this claim even though nobody's managed to make it work really well
in the neural net yet.
The idea is a neuron has a firing rate.
That's the firing rate is its real output, which is communicated stochasticly by a spike.
And that firing rate is actually changing over time, the underlying firing rate.
And the rate of change of the firing rate is used to represent the error derivative.
Now the nice thing about a rate of change is it can be positive or negative.
So we can represent positive or negative derivatives without a neuron having to change
the sort of signs of its synapses.
And what it's representing, the derivative it's representing is the derivative of the
error with respect to the input to the neuron.
And that gets sent back to earlier neurons.
And if I had enough time, I could show you a whole bunch of slides about how this will
do back prop.
But I want to show you one consequence of this.
So that's, look, here we have a nice equation because it's got Leibniz on one side and Newton
on the other side.
That's Leibniz's notation for derivatives because they're not derivatives with respect
to time.
And this is Newton's notation because that was for derivatives with respect to time, okay?
And what we're saying is the output of neuron J, which is yj, is the output of neuron J.
But how fast that's changing over a short time interval is the error derivative.
This is just a hypothesis, you understand?
But it's true.
Jay McClellan and I first used a version of this in 1988 before we knew about its back
time dependent plasticity.
I'm not sure it would be discovered then.
Where you take, this is where I need the cursor.
Yes, that one.
Yes.
You take some input.
You send it to some hidden units, which send it to more hidden units by the green connections
and sends it to more hidden units.
It comes back to the input, so you reconstruct the input.
And then you send it around again, not all the way around, but up to there and up to
there and up to there using the right connections.
And then the learning rule, which you'll notice doesn't involve explicit back propagation,
is to say for these neurons, for example, I change the incoming weights by the activity
of the presynaptic neuron down here times the difference between what I got on the green
activation and on the red activation, first time round and second time round.
So the rate of change of the activation of the neuron is what's used to communicate an
error derivative.
Now, unfortunately, this thing has the wrong sign, but later on we fixed that.
And so here's a theory from 2007 that still hasn't been conclusively proved wrong.
And it sort of works, but it doesn't work quite as well as we hoped, about how you could
get a brain to do back propagation.
What you first do is you learn a stack of autoencoders, that is you learn to get each
layer to activate features in the layer above from which you can reconstruct the layer below.
So you learn some features that can reconstruct this layer.
Then you treat those features as data and learn some features that can reconstruct them.
You build a big stack of autoencoders like that.
Okay.
Once you build the stack of autoencoders, then each layer can activity in a layer can
reconstruct the activity in the layer below.
And then you do two top down passes.
You do a top down pass from the thing you predicted at the output.
So you put in input, activity goes forward through the layers, you predict something
at the output.
And now you do a top down pass and you get reconstructed activities everywhere.
And then you take your output and you change it to be more like the desired output.
And now you do a top down pass and you'll get slightly different reconstructions.
And the difference between those two reconstructions is actually the signal you need for back propagation.
And so if you do that, the learning rule is that you should change a synapse by the
pre-synaptic activity in the layer below times the rate of change of the activity in the
layer above in the post-synaptic neuron.
So it's a very simple learning rule.
So let's change the weight in proportion to the pre-synaptic activity times the rate
of change of the post-synaptic activity.
Now it turns out if you're using spiking neurons, what that amounts to that are representing
underlying firing rates that are changing, that amounts to a learning rule that looks
like this.
What you do is you take a pre-synaptic spike and you ask whether the post-synaptic spike
came before or after it.
Because what you're interested in is the rate of change of the post-synaptic firing rate
around the time of the pre-synaptic spike.
And if the post-synaptic spike occurs often just after it and seldom just before it, that
suggests the firing rate is going up.
And if the post-synaptic spike occurs often just before the pre-synaptic one and less
often just after it, that means the firing rate of the post-synaptic neuron is going
down.
So if you want your learning rule to be the pre-synaptic activity, well, you'll only learn
when you get a pre-synaptic spike.
And then what you'll do is you'll say, did the post-synaptic spike occur afterwards
or before?
If it occurred afterwards, I should raise the weight and if it occurred before, I should
lower the weight.
And so your learning rule will look like this and this thing is actually a derivative filter.
It's centered at zero and what this is really doing is measuring the rate of change of the
post-synaptic firing rate.
And of course it's sampling it.
So you have a post-synaptic firing rate, there's these spike trains and are the other spikes
getting closer together or further apart?
Well this is a way to measure that.
And of course you can do the learning on individual spikes and the learning rule would then be
the implementation of this idea that the rate of change of the post-synaptic firing rate
is the error signal.
The learning rule would just be if the post-synaptic spike goes after the pre-synaptic one, increase
the strength, otherwise decrease it and have that whole effect fall off as the spikes get
further away because we're really only interested in the rate of change of the firing rate around
the time of the pre-synaptic spike.
Now there's one consequence of that which is that if you're going to use the rate of
change of a neuron to represent not what the neuron is representing but to represent an
error derivative, you've basically used up temporal derivatives for communicating error
derivatives.
So you cannot use temporal derivatives to communicate the temporal derivatives of what the neuron
represents.
So however I have a neuron that represents position, I can't use how fast that's changing
to represent velocity and that's true of neurons.
If you want to represent velocity, you have to have a neuron whose output represents velocity.
You can't do it with the rate of change of a position neuron.
If I kill the velocity neurons and keep the position neurons and I watch a car moving,
the position neurons will change but I won't see any motion.
Similarly, you can't use the rate of change of a velocity neuron to represent acceleration.
Okay, so I think the fact that you can't use the rate of change of a representation to
represent that that stuff in the world is changing is more evident since the board of
the idea temporal derivatives of neurons are used up in representing error derivatives.
So now I'll summarize.
The main arguments against back propagation, the fact that they spent neurons and spikes
rather than real numbers, well that's just because a lot of noise regularizes things.
You can represent error derivatives as temporal derivatives so the same neuron can send temporal
derivatives backwards, communicate those backwards and communicate activities forwards and the
fact that in the brain you do get back to independent plasticity seems to be evidence
in favor of that representation of error derivatives and now I'm done.
Thank you Jeff for a great talk, sorry that was a Twitter joke, got it, anyway.
So now what we're going to do is a brief Q&A between myself and Jeff and then after
I've had my chance to ask some questions I'm going to open it up to you guys.
Now I had originally sent Jeff a few questions which I'll rely on partially but his talk
has made me want to ask a few others so I'm sorry I'm going to throw a few loops at you
as well but let's start with some of the ones that I told you I wouldn't give you.
There is something funny with my mic, I don't know if the AV guy is there, I just won't
look down.
Okay so the first question, yeah it's okay, I'm going off script anyway.
The first question which I would like to ask just because it's something that I spend far
too long arguing with people online is essentially you know so you're in the computer science
department, you've come here, you've given us a talk that's largely about brains but
many people seem to object to the idea that computers have anything to tell us about brains
or indeed the idea that the brain is a computer despite the fact that neuroscientists often
refer to computation in the brain.
So my question is to you, is the brain a computer, I don't know I'll just hand that over to
you first.
Yes?
Good, okay.
And for the record I didn't tell him to say that if anyone from Twitter is watching.
And B, can you just maybe give an intuitive understanding of why the answer is yes despite
the fact that obviously our brains are very different from our laptops or our cell phones
and stuff like that.
So there's many ways you can do computation with physical stuff and you could get some
silicon and make transistors and then run them at very high voltage much higher than
needed to make them be digital and then you could if you wanted to represent a number
you could have bits and you could and so on and you could create multipliers and adders
and then you could put all that together and you could have some bits that tell you where
in memory to find stuff and you could make a conventional computer or you could make
little devices that have some input lines that are hardwired with input lines and you
could have adaptive weights on the input lines.
So early neural nets, Marvin Minsky made neural nets out of feedback controllers that were
used in I think B-52 bombers or B-29 bombers or something, B-27 I don't know, some kind
of bomber, it was America and so you can make computers in lots of different ways.
When I was a kid I used to make computers by you take a six inch nail and you saw the
head off and then you wrap copper wire around it and then you take a razor blade and you
break it in half so that it's a nice flexible thing like this and you wrap a bit of copper
wire around the razor blade and then when the current goes through the nail it'll make
the razor blade go down and you'll make a contact so now you've got a relay and then
you can put a bunch of those together and make logic gates.
I never got more than about two logic gates that way but yeah you can make computers in
lots of different ways and the brain is clearly made in a different way from the normal computers
which has some different strengths and weaknesses so it's much slower but on the other hand
you can make it much more parallel.
It has one special property which I think is what makes us mortal which is that every
brain is different so I can't take the weights from my brain and put them in Blake's brain
and hope that it'll work because he just doesn't have connections in the same places.
You've tried.
Right.
Well there's a way of doing it where you take the weights in my brain I turn it into strings
of words.
Blake absorbs these strings of words and puts different weights in his brain.
It's pretty lucky all our brains are different because otherwise rich people will grab poor
people's brains so they could live forever but quite okay.
So I think I want to ask you then following on that what do you think about some of the
quests to fully characterize the brains connectome do you think that is a scientifically worthwhile
endeavor.
Yes I do partly because some of the people doing it are my friends.
Ignoring your loyalty to Sebastian.
Not well in that case no.
It seems to me it is very worth doing but you don't have to do that in order to begin
to understand the principles.
Very good.
But for things like the retina which has a lot of hardwired stuff in it I think it's
really important to do that.
So that actually leads on to my next question.
I wanted to ask you about hardwiring.
So another thing that I think many people who study the brain find difficult about artificial
neural networks as a model for the brain is that as you say you start with random weights
and you train it on a lot of data and you get these things out.
But we know that there are some pre-wired things in many brains so the classic examples
are a horse can run pretty much right out of the womb but even within humans arguably
there are some things that we find easier to learn than others.
And so what do you think is the place for innate behavior within neural networks as
a model of cognition?
Okay so it used to be when I was a student if you were interested in language people
would tell you that it was all innate and it just kind of matured as you got older and
maybe you learned like 12 parameters that characterised your particular language whether
it was subject verb object or some other way.
In fact there's a nova that I saw, it was probably made about 20 years ago and it has
all the leading linguists all of whom were educated by Chomsky and they look straight
at the camera and they say there's a lot we don't know about language but one thing
we know for sure is that it's not learned.
So Chomsky had really good gulag.
Yeah he did.
But it's over because we know now if you want to translate you just learn it all.
The number of linguists required to get a system that can turn a string of symbols in
English into a string of symbols in French is roughly zero.
I mean linguists involved in preparing the databases for training and making sure you
get sort of a variety of grammatic structures and things but basically you don't need linguists,
you just need data.
So you don't need much innate structure.
The issue of what is innate, it doesn't, it seems to me there's not much point putting
in stuff innately if you can learn it quickly.
So for example the ability to move and get 3D structure from motion, that's actually
very easy to learn so I don't believe that's innate even though a child can do it at like
two days.
You show them a sort of W made of paper and you rotate it in a consistent way and they
get bored and as soon as you rotate it in a way, move it in a way that's not consistent
with the rotation, the interest perks up.
But I think they can learn it in two days.
It's really easy to learn.
Okay interesting.
Now I want to ask you, I know partially what your answer is going to be but when I remember
long ago you told me that one of your career goals, at least earlier in your career, was
to prove that everything that psychologists thought about the brain was wrong.
And so my question is what was it that they had wrong, are they still getting it wrong
and is neuroscience getting that same thing wrong?
It was mainly to do with this conviction that psychologists had partly based on Chomsky
that there was an awful lot of innate stuff there and that you couldn't just learn a whole
bunch of representations from scratch.
There was this innate framework and there was a little bit of tuning of this innate knowledge
and that's what learning was and that's just, I think that's a completely wrong headed approach.
In fact, I want to go the other way and I want to say the stuff in the brain that's innate
wasn't discovered by evolution.
The stuff in the brain that's innate was discovered by learning.
Do I have time to do that digression?
Yeah.
Okay, so imagine we have a little neural network and it's got 20 connections in it and each
of those connections has a switch that could be on or off.
So it can let stuff through or not.
So you've got to make 20 binary decisions.
So your chance of making them by chance is one in a million and making the correct decisions.
Now this little neural network circuit is a mating circuit and so the neural net goes
into a singles bar and it runs this circuit and if it's got the connections right it has
lots of offspring and if it hasn't got the connections right it doesn't have offspring
or doesn't have so many offspring.
Okay, so let's start off with the connections being, if they were just kind of random and
you did mutation, what would happen is you'd have to build about a million organisms before
you got a good one and if you had sexual combination of the organisms, let's have a really simple
biology in which each connection has its own gene and this gene has two alleles for
on and off, okay?
If you do mating now, you might have an organism that got all 20 connections right and it mates
with one that has a few wrong and it gets a few wrong ones and now it's wiped out.
It doesn't have lots of offspring anymore, that's it.
So it seems like a complete disaster and it would obviously take you at least a million
organisms to expect to get a good one even if you have pathogenesis where you didn't
have sexual reproduction.
Now I will show you how to build a good organism in only 30,000 tries and the way you do it
is this, you, for each connection you have three alleles, you have turn the connection
on genetically, turn the connection off genetically or leave the connection to learning, okay?
So that's the third alley.
And now you start off with a population in which about half of the connections are genetically
determined and the other half are left to learning.
So that's 10 connections that are definitely determined, so there's a 1 in 1,000 chance
that you'll luck out genetically, you'll get those 10 right.
And then during the organism's lifetime, let's have a really dumb learning algorithm where
it just randomly fit like the one I talked about, it randomly fiddles with the connections,
just randomly flips connections of the 10 that are left to learning.
And it'll take you to about 1,000 trials and it'll get the combination right.
But the point is it can do those trials without building a whole organism, it can just go
into the singles bar and sort of fiddle around a bit with its connections and it'll bang.
So what we've done is we've replaced a million trials of evolution, building a million organisms
with built 1,000 organisms and then let's build 30,000 organisms, just to be safe.
And then each of those fiddles around with its connections and it'll do this search.
The whole search is the same, you have to try a million combinations, but the way you
get the million combinations is 1,000 organisms each does 1,000 learning trials and so almost
all the work is done by learning.
Now if genetically an organism happens to have more things set, like it's got 12 of
them set right, it'll learn faster.
And so this genetic pressure for, if you mate organisms now, this genetic pressure to get
more and more of these alleles set genetically.
But the pressure only comes because the learning can get all 20 set right so this thing can
mate and have lots of offspring.
So the fact that the learning can find a solution creates genetic pressure to hardwire these
things.
So what's happening there is the search, a thousand things were done by evolution, a
million minus a thousand things were done by learning and that created a landscape for
evolution that allowed evolution to gradually hardwire in more and more, that these things
were first found by learning.
So I think a lot of the structure in the brain that's hardwired is first found by learning
and gradually it gets backed up into the hardwiring.
But to get the evolutionary pressure to say that's good, you have to be able to do the
learning.
First hardwired things, you'd never find anything that was good.
Okay.
Great.
Thank you.
Now...
That's called the Baldwin effect, by the way.
Yes.
Yeah, it's called after a psychology professor at the University of Toronto in the 1890s called
Baldwin who invented this effect.
He didn't do any computer simulations though.
So I want to do one follow-up question on that and then ask my final question before
handing it to the audience.
So my follow-up to that is, you know, I think one of the things that is unclear in terms
of the success of deep learning is exactly how much it was purely the compute or some
clever things.
Now I've seen both cases argued and you today kind of suggested that it was just the compute.
But I want to ask you about this following on your last point, which is that we know
that if you build networks with particular architectures and with particular learning
rules, you are effectively making learning faster if you do it right.
And arguably a lot of the success of deep learning has actually been as a result of
people thinking about good designs for their networks and good ways of making learning
faster.
Yep.
So would you potentially say that we have seen that process that you just described
actually occur with NAI over the last ten years of the learning kind of backing up into
the hardwiring?
I see.
I need to think about that.
What we've seen, I mean, Jan Lecaun invented convolutional neural nets.
Right.
Yes.
For example.
So the computer was invented in the 1980s, but computers weren't fast enough to really
do a lot with them.
So they were used for handwriting recognition and they were used for reading 10% of all the
checks in America.
But they didn't really take off.
They really took off when the computer hardware came along to make them really efficient.
So that's a case where the ideas were had first, but without the hardware they didn't
work.
You've obviously got to have both.
Right.
Yes.
Great.
Okay.
So now my last question before I hand it to the audience is just, what do you see as
being the future of the interaction between neuroscience and AI?
Do you think that there is space for a sort of new cognitive science where we study general
intelligence, but with brain-centric models rather than logic-based models?
Or will we see the two streams depart over the next few decades?
The way I like to think of it is we'd like to understand, if you'd like to understand
how the brain does computation, you've got brains in your computation.
And they look, like you said, they look pretty different to begin with because there's many
different ways to do computation.
And with a conventional digital computer, you can get it to pretend to be anything.
So we're getting it to pretend to be some other kind of computer, an artificial neural
net.
And we'd like to sort of bridge this huge gap between brains and what we can simulate
on computers.
And so neuroscientists are sort of doing experiments.
And good computation neuroscientists are sort of doing experiments to try and sort of see
how you could do the computation.
And I think of myself at this end as doing, simulating things with artificial neural nets
to see how you can make it more biological.
And we're trying to build a bridge.
And so the computational neuroscientists, most of them are building from this end.
I'm building from the other end.
But obviously, if you want to build a bridge to somewhere, you need to look at where you're
going.
And so I'm trying to build a bridge that does computation more and more like the brain does
it, or like, I guess the brain does it from what my neuroscientist friends tell me.
And then there's conventional AI, which is trying to build a bridge like that.
Great, OK, thank you.
So now I'm going to open up to questions from the audience.
Now, for this, we've got this kind of interesting system here.
So rather than you putting up your hands and me selecting you, you can actually nominate
yourself to ask a question by pressing on the button on your microphone.
And it is a first come, first serve basis.
So you're going to be queued up.
And so you're now first on the queue.
And by now, it's too late to be able to ask a question.
Yes.
And one last thing about that, though.
When you are done asking your question, please turn off your microphone,
because that will open up this slot for the next person in the queue.
OK, go ahead.
I've got a red light here, does that?
Yeah, if your red light is flashing, that means you're on.
You get to ask a question.
If your red light is flashing, you're on.
I've got a solid light, please.
Oh, solid.
Sorry.
OK, I've got a solid light.
You were faster.
OK.
So this is a Clifton suspension bridge analogy for your interest here.
So you mentioned briefly, Hebbian synapses.
As neuroscientists, we have a good understanding of how they work at a molecular level.
So my question is, to what extent are the understanding of biological memory
mechanisms, i.e. Hebbian synapses, implemented by AI for deep learning
and the sorts of systems that you're describing?
So at present, people don't use Hebbian synapses for most deep learning.
They're using back propagation.
So it's an error correction rule, as opposed to something where,
if you just use it, it gets stronger.
But if you want a short-term memory for things like transformers
to remember a temporal context, just a simple Hebbian synapses is a good thing to have.
Yeah, but Hebbian synapses can code memories in humans that can last a lifetime.
So is this something that AI is working towards using,
or are they just going to bypass Hebbian synapses and come up with something superior?
OK. So if you think about what's been successful in the last few years,
it's using error correction learning with either labeled data
or trying to predict what comes next, and not Hebbian synapses.
Now, people like me who sort of do this kind of learning
but are interested in the brain know this isn't right.
We're much more interested in unsupervised learning.
We just can't make it work very well yet.
And I would love to be able to get learning to work,
as well as it does when you do back propagation,
without using biologically implausible things.
And one place we can do that is with temporary memories.
So if you say synapses have a fast component,
you can use Hebbian learning for that fast component,
and that will actually help neural networks work better,
even if you're using back propagation for the slow component.
That didn't really answer your question, but you know, it filled the time.
LAUGHTER
Hello.
I read in the Reinforcement Learning Book that dopamine is used
as a reward prediction error signal.
So I was wondering, do you see it used as a supervisory signal,
like you mentioned earlier?
OK, so for reinforcement learning,
there is some lovely work done by Peter Diane,
who is the theoretician, and some experimentalists,
showing that the real data from neuroscience
fits in with a theory that was started with Rich Sutton.
And Peter Diane did the work of showing that dopamine
corresponds to something in a particular learning algorithm.
And it doesn't correspond to the reward,
it corresponds to the difference between the reward you're expecting
and the reward you get.
So if you're a monkey and you're expecting a grape
and I give you a piece of cucumber, that's negative reward,
and that will be a big negative hit at the dopamine.
So that's not the kind of learning that's been really successful so far.
If you're willing to burn a lot of computer time,
Reinforcement Learning will solve some problems,
but it's not the kind of learning that's been most successful in AI.
So the difference is in Reinforcement Learning,
you get a single scalar, you get one number,
whereas in error correction learning,
you typically get a whole vector of numbers.
Right here.
So you mentioned that your goal, kind of the bridge analogy,
is your goal is to go from computers and try to get to the brain.
So let's just say that kind of makes sense to think,
okay, let's get to more general AI,
because I'd say humans are decently general.
And neuroscientists are trying to get from the other bridge,
the brain, to generally AI.
So you have these two kind of debates,
and this happens quite often,
where is it correct to go from generally AI to the brain,
first understand generally AI, then understand the brain,
or brain to generally AI.
And so what would you say is the most practical way
to problematize generally AI?
I don't like the phrase general AI.
I don't think, if you want intelligent devices,
I don't think you want to produce a sort of general purpose Android.
I think you want to produce different devices
that are smart in different ways.
So basically if you want intelligent machines that do things,
you have a vacuum cleaner and you have a backhoe.
You don't try to make one thing that's a vacuum cleaner and a backhoe.
It doesn't make sense.
What about connecting them through
kind of like different cognition areas in the brain?
Yeah, but I think it's the same with cognition too.
I think the neural net that does machine translation
isn't the same neural net as does vision.
I think, yeah, my guess is that people are thinking too much
about making one neural net that does everything,
and not thinking enough about making more modular neural nets
that are good at different things.
Some are more universal than others,
but I think that's how progress has been made.
That's how progress has been made so far.
Not by the people talking about general AI.
It's being made by people looking at saying,
how can I get a neural net to do vision,
or how can I get it to do machine translation?
Thank you.
Hi, I'm just wondering about the role of hierarchy in general.
There are different types of hierarchy.
There's different layers of neural network.
As you mentioned, there's fast memory and slow memory.
Then I wonder, are there more ways to add hierarchy to neural networks
to make them more useful or emulate actual brain?
Yes, probably.
In vision, for example,
you have multiple layers,
that is, you have multiple cortical areas in the visual pathway.
That's a very different kind of hierarchy
from what you need for dealing with the sort of structural reality.
In reality, there's the universe.
There may be many of them, but that's one's enough.
Then there's galaxies.
Then there's probably things above galaxies.
Then there's in the galaxies the stars,
and then there's solar systems,
and then there's planets, and so on.
We can do that all the way down to atoms.
You can imagine all that.
You can represent all that in your brain.
Clearly, what's going on is,
out in the world, there's this hierarchy
that goes over many, many orders of magnitude
from the universe down to quarks,
or whatever the smallest thing is now.
You don't want that kind of hierarchy in your brain.
What you've got in your brain is the ability to deal
with a little window of hierarchy,
where there's an object in its parts.
To deal with the whole universe,
you can take this window,
and you can map at a scale of the universe,
and there's the universe, and there's the galaxies,
or there's the galaxies, and there's the stars,
or there's the atom, and there's the electrons.
You're using the same neural hardware,
but mapping reality onto it differently.
I think whenever we have to deal with anything complicated,
we use hierarchies.
The way the brain uses them is by varying the mapping
from reality onto the brain.
It can only operate with a small window on a hierarchy,
which you can move up and down.
Much like you only have a small region of high resolution,
which you move around.
So logarithm?
Sorry?
Like logarithm.
What about logarithms?
That's what you're talking about, right?
Compressing a big range into something that is much manageable.
I wasn't thinking of it like that.
I was thinking of it as you have some fixed hardware,
and when I'm thinking about the solar system,
my fixed hardware couldn't possibly deal with the universe.
That's much too big, and it couldn't possibly deal with an atom.
That's much too small.
But it's fine dealing with the sun and some planets,
and maybe a moon or two.
What I'm trying to get at is we need to make a big distinction
between the hierarchy in the real world,
hierarchical structures in the real world,
and how we deal with them cognitively,
where we use attention,
and we only ever deal with a bit of the hierarchy at a time.
That's not the same for, say, aspects of language,
where you have...
Notice that with vision,
I can use the same neurons for representing the sun
and for representing a nucleus.
It's just an analogy, but it's the same neurons I'm using.
Now, if I'm processing language,
I've got things that find me phonemes
and things that turn phonemes into words
and things that turn words into sentences,
and those...
I can't move a window like that.
That's a fixed hierarchy.
There's phonemes, and there's words,
and there's phrases, and there's sentences,
and that's all sort of fixed in the brain.
That's not a flexible matter.
You can't kind of move the sentences down
so they're where the words were,
move the words down so they're where the phonemes were.
That doesn't work.
So there's some hierarchies that really do relate
to sets of neurons in the brain.
They're like the layers in the connections models.
There's other hierarchies,
like the whole spatial structure of the universe,
where what's in the brain is a window
you move over that hierarchy.
Thank you.
Thanks, Dr. Hinton, for excellent talk
and excellent ideas about the feasibility of back propagation.
My question's maybe more boring
about the statistical comments that you made.
Is that Dale?
Pardon me?
Are you Dale?
No, I'm Kyle.
You sound like Dale Shermans.
I'm at the University of Alberta.
Hi.
Well, that's a good instance,
that you sound just like Dale Shermans
and you're at the University of Alberta.
Are you a student of Dale's?
No.
I think I've only met a voice.
If you're a student of Dale's, I need to watch out
because it's going to be a very tricky question.
The question is that I was trained with this intuition
that you can't overparameterize your models,
that if you're trying to fit a line that you need two points,
if you're trying to fit a curve you need three and so on
and that scales up and you should always have
a little bit less data points.
I know that you have shown clearly
and the field has shown that that's not true.
What were the statisticians getting wrong
in their logic to be convinced?
It's to do with regularization,
that you need it to be highly regularized.
But first of all, I'll show you
that if you want to fit two data points,
well, let's take three.
If you want to fit three data points,
you would have told me you want a polynomial
with only three degrees of freedom,
so you want a constant and a slope and a curvature
and that's all you can afford with three data points.
That's wrong.
Now, this is where we need a pen.
Oh, sorry.
They don't work.
I tried them all and none of them work.
Okay, did they work?
Oh, well done.
Extra points.
Okay, yes.
Can you see that?
Yeah.
Okay, and we're going to have three data points.
And actually, if you're a statistician,
you'd probably say for three data points
you'd probably ought to fit a straight line like this.
Because I could fit a parabola
and the parabola would fit exactly
and that's a bit suspicious.
In other words, the parabola fits exactly,
but do you really believe that if you were to ask
when x is zero, what's the value of y?
Do you really believe that value for y?
Because a straight line is far more conservative.
So a statistician would probably say
fit a straight line.
However, that would be a frequentist statistician.
If you took a Bayesian statistician,
and this is in Chris Fisher's machine learning textbook,
there's a nice picture of it, I think, somewhere.
Think it's that book.
A Bayesian statistician would say,
okay, let's try fitting fifth-order polynomials.
And fifth-order polynomials,
we might even fit ones that don't exactly go through the data,
but for now let's make them go through the data.
So we fit a fifth-order polynomial
that goes kind of...
One, two, three...
Well, you know, some order.
And we fit another one.
Oh, that didn't go through the data.
And we keep fitting these guys,
and we fit a gazillion of them.
And what you see at the end is that
these gazillion ones, in between the data points,
they're kind of all over the place,
and their average is in a sensible place like here,
but their variance is big.
And what they're telling you is,
if you give me this x-coordinate,
I'm rather uncertain about this y-coordinate,
but this is a good bet.
And similarly here,
and if you go out here,
these polynomials are just all over the place,
and they'll tell you,
if you give me this x-value,
then it could be pretty much anything.
That's not a bad bet,
but it could be pretty much anything.
And that's a much better answer
than you get from a straight line.
So by fitting a very large number
of different polynomials,
and then averaging,
you get good, mean answers,
and you also get a sense of the variance.
Now, Drop-Out is doing something like that.
Yes, and that's brilliant.
Thank you for coming up with Drop-Out.
Many of us here
are working in a regime of sparse data,
and so we have a couple channels,
a couple signals, a couple voxels,
and you've convinced us that we need more,
but is there a way forward in AI
that can manage with more sparse data,
or is this the only regime
that's going to be able to make success?
So the really big successes
have been on big databases,
and I think we should be using
even bigger models,
but you can't get away from the fact that actually,
if you're going to have something
that starts off random
and sucks all its knowledge from the data,
you'd better have enough data
to suck all that knowledge from.
The bigger your model, the better,
if you regularize it,
but you still need a lot of data.
So the way you should think about it is this.
If you've got 100,000 data points,
that's small.
I know that's very depressing
if you're a neuroscientist.
Well, it's not depressing.
It seems impossible.
If you want to personalize medicine
for one individual
and you want to train a model
on their data from their brain,
it seems like there's going to be a disconnect
between what these models can do
and how they might help someone in the future.
Yes and no.
If I train a model
on a very large number of people
and then apply that model to one person,
that's the form of personalize medicine
that really works.
Great, thanks.
Oh, and say hi to Dale for me.
Thank you for the presentation.
My last question is about this dropout.
The thing is that you randomly just drop
some parts of the network
and then you say, okay,
that it works better so I would accept it.
But do we have any, like, intuition
why, for example, some parts of it work better
or if we try to embed this, like,
network into, like,
is it more graph isomorphism?
What are these two different graphs,
different graphs that we took?
Are there any similarities
between them or just with randomly?
I guess the problem with the randomness,
I guess we are trying to put
the burden of prediction
on the random part of the computer desk.
So I didn't hear
the whole question,
but certainly in dropout what we do
is we randomly leave out units.
Now you can also do block dropout.
You can take groups of units
and randomly leave out the groups.
And what that does is it allows the units
within a group to collaborate with one another,
and then between groups
they have to be fairly independent.
And that's called block dropout
but I didn't really hear
the rest of your question.
Okay.
The question was about
that, okay, imagine that you...
Can you talk closer to the microphone
because I'm partially dead?
The question is that
imagine that you have a dropout of 50%
and you're trying to get rid
of, like, 50% of your nodes
and the nodes in the network.
And the question is, okay,
whether we have any similarity between the types
if we do it iteratively,
whether we would find any similarity
on the structure of the network
that would produce the best results
and if it's so, whether it would correspond
to something physical, like, for example,
if you're doing a vision thing,
whether it would correspond to something in brain or not, I guess.
Yeah.
Lots of people have thought about whether you can do better
than random in dropout.
And there's some work on that, like,
block dropout that works, can work for some things.
But I don't really have much to say
about...
I don't really know the answer to
sort of, is there something much
more sensible than dropout
that's a lot more structured?
There might well be, but I...
Thank you.
Okay, so with that, we're going to have to end.
So please join me in
thanking Jeff for...
Thank you.
APPLAUSE
And
I wanted also
to thank Blake
for hosting this event,
and I felt the questions
could have gone on all night,
but the tip-off isn't half an hour,
so some of us have to move on.
So thank you, Blake.
APPLAUSE
