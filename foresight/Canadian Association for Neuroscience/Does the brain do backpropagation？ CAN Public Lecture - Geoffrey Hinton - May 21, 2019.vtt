WEBVTT

00:00.000 --> 00:04.720
Hi, I think we're ready to start, so my name is Paul Franklin and I'm a Neuroscientist

00:04.720 --> 00:09.120
here at Sick Kids, and also I'm Program Chair for the Canadian Neuroscience Meeting that

00:09.120 --> 00:12.680
takes place in Toronto all this week.

00:12.680 --> 00:19.080
So the traditional curtain raiser for the Neuroscience Meeting, the CAN meeting, is the public lecture.

00:19.080 --> 00:25.760
And this year we decided to focus on the interface between neuroscience and AI.

00:25.760 --> 00:27.760
We did that for two reasons.

00:27.760 --> 00:33.520
The first reason is that AI, or Toronto, is one of the main hubs in the world for AI research.

00:33.520 --> 00:39.320
And the second reason is that it's also home to one of the true pioneers in this field,

00:39.320 --> 00:42.800
also known as the godfather of deep learning, Jeff Hinton.

00:42.800 --> 00:48.080
And so when we asked Jeff if he'd participate in this event, I think a year and a half ago

00:48.080 --> 00:51.600
we asked him and he said, yes, we were super excited.

00:51.600 --> 00:58.000
So at this point I want to hand over to Blake Richards, and Blake Richards is an associate

00:58.000 --> 01:03.240
professor at University of Toronto Scarborough, and he's going to host this evening's event.

01:03.240 --> 01:04.240
Blake?

01:04.240 --> 01:06.000
Thanks, Paul.

01:06.000 --> 01:12.040
So just as a brief introduction, I wanted to tell you a little bit more about Jeff.

01:12.040 --> 01:18.640
So you might be surprised to learn that the godfather of deep learning, associated mostly

01:18.680 --> 01:25.560
with AI, got his BA in experimental psychology from Cambridge originally.

01:25.560 --> 01:32.840
He then went on to do his PhD in artificial intelligence in Edinburgh, so he did get

01:32.840 --> 01:35.000
started relatively early.

01:35.000 --> 01:41.040
But throughout his early career, he really contributed to the first wave of what was

01:41.040 --> 01:46.520
known at the time as parallel distributed processing or connectionism, which really brought back

01:46.520 --> 01:53.520
into the fore the idea of using neural networks for both models of the mind and for artificial

01:53.520 --> 01:55.360
intelligence.

01:55.360 --> 02:03.000
Now Jeff got his first tenure track position at Carnegie Mellon in the 80s, but we were

02:03.000 --> 02:09.480
able to steal him away from them in the late 80s, which I understand was largely because

02:09.480 --> 02:12.560
of his ethical objections to DARPA funding.

02:12.560 --> 02:22.040
So once again, Canada's political bent has helped us in our research endeavors.

02:22.040 --> 02:27.320
Over the course of the 90s, Jeff continued to really push neural networks and machine

02:27.320 --> 02:29.320
learning forward.

02:29.320 --> 02:31.200
And sorry about that.

02:31.200 --> 02:36.320
In 98, he actually went to University College London to found the Gatsby computational

02:36.320 --> 02:42.480
neuroscience unit, and we might have lost him, but thankfully we pulled him back again.

02:42.480 --> 02:48.360
He came back to Toronto in 2001, where he became a university professor in 2006 and

02:48.360 --> 02:51.600
then an emeritus professor in 2014.

02:51.600 --> 02:59.080
Now I think that you all know that Jeff is a monumental figure within artificial intelligence

02:59.080 --> 03:05.040
and machine learning, and he's been critical to the founding of the Vector Institute here

03:05.040 --> 03:09.400
in Toronto and putting Toronto on the map for AI.

03:09.400 --> 03:14.440
And certainly he's had incredible recognitions of his work, most recently the Turing Award,

03:14.440 --> 03:20.640
which he shared with Joshua Bengio and Yann LeCun, as well as the Order of Canada.

03:20.640 --> 03:25.200
And he's a distinguished fellow of the Canadian Institute for Advanced Research, which I highlight

03:25.200 --> 03:30.520
because they were one of the people who continued to support neural networks throughout the

03:30.520 --> 03:33.440
time when it wasn't as faddish.

03:33.440 --> 03:37.440
But for all his successes and his technical endeavors, I think one of the things that's

03:37.440 --> 03:42.360
most important to understand about Jeff is the impact that he's had on other scientists.

03:42.360 --> 03:46.840
Jeff has really molded the career of so many people and changed the way that they think

03:46.840 --> 03:49.560
about things.

03:49.560 --> 03:54.560
When you look at the people who have been his graduate students or postdocs, it really

03:54.560 --> 03:57.840
is the who's who in artificial intelligence.

03:57.840 --> 04:07.320
It includes people like Max Welling, Yann LeCun, and you know, the phrase I used to describe

04:07.400 --> 04:12.160
it is, have you drunk Jeff's Kool-Aid?

04:12.160 --> 04:15.360
Because once you've drunk Jeff's Kool-Aid, there is no going back.

04:15.360 --> 04:22.200
You see neural networks, you see AI differently, and I would argue you also see neuroscience

04:22.200 --> 04:23.480
differently.

04:23.480 --> 04:31.320
And for me, my understanding of the brain has been largely shaped by Jeff and his work.

04:31.320 --> 04:36.080
But you know, we're at the point now where computer science has drunk Jeff's Kool-Aid.

04:36.080 --> 04:42.160
So he's got an H index of 145, and according to Google Scholar, his work has been cited

04:42.160 --> 04:49.040
270,000 times, which is more than Einstein, Ramoni, Cajal, and Alan Tern combined.

04:49.040 --> 04:51.520
But that's largely from computer scientists.

04:51.520 --> 04:58.040
And if my prediction is correct, neuroscience 30, 40 years from now will also have drunk

04:58.040 --> 05:01.480
Jeff's Kool-Aid, and maybe you're going to get your first taste tonight.

05:01.480 --> 05:04.040
So with that, I hand you over to Jeffrey Hinton.

05:06.080 --> 05:14.920
So thank you very much, Blake.

05:14.920 --> 05:17.920
I can give you some more Kool-Aid today.

05:17.920 --> 05:21.360
It's Kool-Aid produced by one of my former students, Ilya Sutskava.

05:21.360 --> 05:26.480
First, I want to tell you a little bit about the history of deep learning in AI.

05:26.480 --> 05:30.600
Can I just ask before I start, how many people here know what the back propagation algorithm

05:30.600 --> 05:31.600
is?

05:31.600 --> 05:33.880
Put your hands up.

05:33.880 --> 05:35.680
So some people don't.

05:35.680 --> 05:39.640
I'll explain it very quickly, and I'll explain it in such a way that you'll be able to explain

05:39.640 --> 05:40.640
to other people.

05:40.640 --> 05:43.960
So if you do know what it is, you follow the explanation from the point of view of how

05:43.960 --> 05:44.960
you explain it.

05:44.960 --> 05:45.960
Okay.

05:45.960 --> 05:48.320
There was a war between two paradigms for AI.

05:48.320 --> 05:53.320
There were people who thought that the essence of intelligence was reasoning, and logic is

05:53.320 --> 05:58.880
what does reasoning, so we should base artificial intelligence on taking strings of symbols and

05:58.880 --> 06:02.360
manipulating them to arrive at conclusions.

06:02.360 --> 06:05.840
And then there were other people who looked at the brain and said, no, no, intelligence

06:05.840 --> 06:10.200
is all about adapting connections in the brain to get smarter.

06:10.200 --> 06:17.080
And this war went on for a long time, and eventually, people who were trying to figure

06:17.080 --> 06:23.880
out how to change connections between fake neurons to make these networks smarter got

06:23.880 --> 06:28.920
to be able to do things that the people doing symbolic AI just couldn't do at all.

06:28.920 --> 06:33.320
And now there's a different way of getting a computer to do what you want.

06:33.320 --> 06:38.240
Instead of programming it, which is tedious, you just showed examples, and it figures it

06:38.240 --> 06:39.240
out.

06:39.240 --> 06:42.440
Now, of course, you have to write the program that figures it out, but that's just one program

06:42.440 --> 06:45.960
that will then do everything.

06:45.960 --> 06:50.000
And this is an example of what it can do.

06:50.000 --> 06:52.880
So the image, just think of the numbers.

06:52.880 --> 06:57.200
They're RGB values of pixels, and that's the input to the computer.

06:57.200 --> 07:02.640
Lots of values of pixels, just real numbers, saying how bright the red channel is.

07:02.640 --> 07:07.960
And you have to turn those numbers into a string of words that says a close-up of a

07:07.960 --> 07:09.920
child holding a stuffed animal.

07:09.920 --> 07:11.440
And imagine writing that program.

07:11.440 --> 07:14.960
Well, people in conventional AI had tried to write that program and they couldn't, partly

07:14.960 --> 07:17.760
because they didn't know how we did it.

07:17.760 --> 07:21.760
We still don't know how we do it, but we can get artificial neural networks to do it now

07:21.760 --> 07:24.320
and do a pretty good job.

07:24.440 --> 07:31.160
My prediction is, within 10 years, if you go and get a CT scan, what will happen is

07:31.160 --> 07:36.840
a computer will look at the CT scan, and a computer will produce the written report

07:36.840 --> 07:39.960
that the radiologist currently produces.

07:39.960 --> 07:43.120
Radiologists don't like this idea.

07:43.120 --> 07:44.120
Okay.

07:44.120 --> 07:46.120
Here's a simplified model of a neuron.

07:46.120 --> 07:47.120
It's very simple.

07:47.120 --> 07:51.520
It gets some input, which is just the activity on the input lines times the weights, adds

07:51.520 --> 07:52.520
it all up.

07:52.720 --> 07:54.560
That's called the depolarization.

07:54.560 --> 08:00.960
And then it gives an output that's proportional to how much input it gets as long as it gets

08:00.960 --> 08:02.480
enough input.

08:02.480 --> 08:04.360
And so, to begin with, we won't have spiking neurons.

08:04.360 --> 08:08.960
These are just going to be neurons that send real values in just the way neurons don't.

08:08.960 --> 08:13.880
We're going to make networks of them by hooking them up into layers.

08:13.880 --> 08:16.880
And you could put some pixels on the input neurons.

08:16.880 --> 08:17.880
Look.

08:17.880 --> 08:21.080
They're the input neurons.

08:21.080 --> 08:25.440
And you go forwards through the net until you get outputs.

08:25.440 --> 08:28.200
And then you compare those outputs with what you ought to have got, so you have to know

08:28.200 --> 08:30.440
what the right answer is.

08:30.440 --> 08:36.400
And what we'd like to do is train the weights, these red and green dots, so that it gives

08:36.400 --> 08:37.400
the right output.

08:37.400 --> 08:42.360
Now, I'm going to show you a way of training the weights that everybody can understand,

08:42.360 --> 08:47.120
and everybody is thought of, basically.

08:47.120 --> 08:54.280
What you do is you start with random weights, you show it some inputs, you measure how well

08:54.280 --> 08:58.200
it does, then you change one weight a tiny bit.

08:58.200 --> 09:02.680
So I take that weight there, and I just change it a tiny bit, and then I show it the same

09:02.680 --> 09:05.280
inputs again and see if it does better or worse.

09:05.280 --> 09:06.680
If it does better, I keep the change.

09:06.680 --> 09:10.920
If it does worse, maybe I keep the change in the opposite direction.

09:10.920 --> 09:12.400
That's an easy algorithm to understand.

09:12.400 --> 09:13.400
And that algorithm works.

09:13.400 --> 09:14.400
It's just incredibly slow.

09:14.400 --> 09:19.480
You have to show it lots of examples, change your weight, and then show it lots more examples,

09:19.480 --> 09:20.480
change another weight.

09:20.480 --> 09:22.480
And every weight has to be changed many times.

09:22.480 --> 09:28.840
So if you use calculus, you can go millions of times faster.

09:28.840 --> 09:32.320
So the trick of this algorithm, the sort of mutation algorithm, is you have to measure

09:32.320 --> 09:34.600
the effect of the weight change on the performance.

09:34.600 --> 09:42.080
But you don't really need to measure it, because when I change one of these weights, the effect

09:42.080 --> 09:45.440
that it has on the output is determined by the network.

09:45.440 --> 09:47.600
It just depends on the other weights in the network.

09:47.600 --> 09:51.000
It's not like normal evolution, where the effect of a gene depends on the environment

09:51.000 --> 09:52.000
you're in.

09:52.000 --> 09:54.000
This is all kind of internal to the brain.

09:54.000 --> 09:57.720
And so changing one of these weights has an effect that's predictable here.

09:57.720 --> 10:03.160
So I ought to be able to predict how changing the weight will help get the right output.

10:03.160 --> 10:08.920
And so what back propagation does is basically says, I'm going to compute using an algorithm,

10:08.920 --> 10:12.400
the details of which I won't tell you, and compute for every weight, all at the same

10:12.400 --> 10:17.320
time, how changing that weight would improve the output.

10:17.320 --> 10:19.920
And then I'm going to change all the weights a little bit.

10:19.920 --> 10:22.840
So every weight changes in direction to improve the output, and the output improves quite

10:22.840 --> 10:26.440
a bit, and then I do it all again.

10:26.440 --> 10:33.760
Now that allows me to compute for every weight what direction I'd like to change it in.

10:33.760 --> 10:38.560
And the question is, should I, when I show examples, show all of the examples, and then

10:38.560 --> 10:41.960
update the weights, so should you live your whole life with the synapse strengths you're

10:41.960 --> 10:45.600
born with, then update your weights a little bit, then live your life again, and update

10:45.600 --> 10:54.120
the weights a little bit more, that doesn't seem very good, or should you take one case

10:54.120 --> 10:58.120
or a few cases, figure out how you'd like to update the weights, update them, and then

10:58.120 --> 10:59.120
take more cases.

10:59.120 --> 11:02.960
That's the online algorithm, and that's what we do.

11:02.960 --> 11:04.920
And the amazing thing is, it works.

11:04.920 --> 11:08.120
You can take one case at a time, or you can take small batch of cases, you update the

11:08.120 --> 11:11.640
weights, and these networks get better.

11:11.640 --> 11:15.200
And it's very surprising how well it works on big data sets.

11:15.200 --> 11:20.880
So for a long time, people thought, you're never going to be able to learn something

11:20.880 --> 11:26.560
complicated, like, for example, take a string of words in English, feed them into a neural

11:26.560 --> 11:31.320
net, and output a string of words in French that mean the same thing.

11:31.320 --> 11:33.600
You're never going to be able to do that if you start with a big neural net with just

11:33.600 --> 11:34.600
random weights.

11:34.600 --> 11:38.080
It's just asking too much for the neural net to organize itself so it can do transactions

11:39.040 --> 11:43.120
because you have to kind of understand what the English says.

11:43.120 --> 11:47.920
And people predicted this was completely impossible, but you'd have to put in lots of prior knowledge.

11:47.920 --> 11:52.680
Well, they were wrong.

11:52.680 --> 12:00.200
So in 2009, my students in Toronto showed that you could actually improve speech recognizers

12:00.200 --> 12:02.800
using these neural nets that had random weights.

12:02.800 --> 12:07.520
They were just trying to predict in a spectrogram which piece of which phoneme you were trying

12:07.520 --> 12:09.760
to say in the middle of the spectrogram.

12:09.760 --> 12:13.440
And then there was more to the system that wasn't neural nets.

12:13.440 --> 12:17.640
Now what we've done is we've got rid of all the stuff that wasn't neural nets, and now

12:17.640 --> 12:23.960
you can take sound waves coming in and you have transcriptions coming out, or even better,

12:23.960 --> 12:27.600
you have sound waves coming in and you have sound waves coming out in another language

12:27.600 --> 12:29.960
with the same accent.

12:29.960 --> 12:31.800
They can do that now.

12:31.800 --> 12:34.000
That's speech recognition done.

12:34.000 --> 12:40.040
And in 2012, two of my students took a big database of images and used essentially the

12:40.040 --> 12:45.960
same algorithm, the few clever tricks, to say what was in the image.

12:45.960 --> 12:51.320
Not a full caption, just the class of the most obvious object, and they did much better

12:51.320 --> 12:55.720
than conventional computer vision, which had been going for many years, and since then

12:55.720 --> 12:58.600
all the best recognizers have used neural nets.

12:58.600 --> 13:03.880
In 2011, you couldn't publish a paper out neural nets in the standard computer vision

13:03.880 --> 13:06.240
conference because they said they were rubbish.

13:06.240 --> 13:11.720
In 2014, you couldn't publish a paper that wasn't about neural nets.

13:11.720 --> 13:15.000
And in 2014, they did something that I didn't expect.

13:15.000 --> 13:20.120
This was done by people at Google, not me, and Joshua Benjo in his group in Montreal,

13:20.120 --> 13:24.000
particularly by a guy called Bardenao and Cho.

13:24.000 --> 13:31.360
They managed to get a neural net, so you feed in actually fragments of words in one language.

13:31.360 --> 13:33.560
You have 32,000 possible fragments.

13:33.560 --> 13:36.960
So the word the in English would be one of the fragments, but so with things like ing

13:36.960 --> 13:38.760
and un.

13:38.760 --> 13:42.360
And what comes out in another language is fragments of words in that other language, and it's

13:42.360 --> 13:46.800
a pretty good translation, and that's how Google now does translation.

13:46.800 --> 13:53.240
So it did translation better than symbolic AI.

13:53.240 --> 13:56.560
So what changed between 1986 and 2009?

13:56.560 --> 13:58.520
And it was basically computers got faster.

13:58.520 --> 13:59.520
That was the main change.

13:59.520 --> 14:00.960
Data sets got bigger.

14:00.960 --> 14:05.440
We developed some clever tricks, and we like to emphasize those, but it was really the

14:05.440 --> 14:07.480
computers getting faster and data sets getting bigger.

14:07.480 --> 14:10.520
But I'll emphasize the clever tricks nonetheless.

14:10.520 --> 14:11.920
And I can tell you about two clever tricks.

14:11.920 --> 14:15.280
I can tell you about transformers, and I can tell you about better ways of stopping neural

14:15.280 --> 14:19.320
networks from overfitting.

14:19.320 --> 14:25.520
But first I want to show you an example of what neural nets can do now.

14:25.520 --> 14:32.400
So a team at OpenAI took work on transformers that was originally done at Google.

14:32.400 --> 14:36.440
They developed it a little bit further, and they applied it to big neural nets that have

14:36.440 --> 14:41.480
1.5 billion learnable connection strengths.

14:41.480 --> 14:43.560
So they're learning 1.5 billion numbers.

14:43.560 --> 14:45.280
That's the knowledge of the system.

14:45.280 --> 14:50.920
And they train it up on billions of words of English text, and all the net's trying

14:50.920 --> 14:53.920
to do is predict the next word.

14:53.920 --> 14:58.800
So what the net will do, or fragment of word, the net will give you probabilities for the

14:58.800 --> 14:59.800
next word.

14:59.800 --> 15:03.960
So if you give it some words, a lead-in, it'll give you probabilities for the next word.

15:03.960 --> 15:08.040
And once the net's trained, what you can do is you can look at those probabilities,

15:08.040 --> 15:13.800
and if it says there's a probability of 0.4 that the next word is the, you pick the with

15:13.800 --> 15:19.080
probability 9.4, and if it says fish with probability 0.01, you pick fish with probability

15:19.080 --> 15:20.080
0.01.

15:20.080 --> 15:24.160
And so you just pick from its distribution, and then you tell the neural net, okay, the

15:24.160 --> 15:27.840
one I picked was the next word, what do you think comes after that?

15:27.840 --> 15:31.920
And this way you can get it to sort of reveal what it really believes about the world.

15:31.920 --> 15:34.840
So you're getting it to predict words one at a time, and every time it makes a prediction

15:34.840 --> 15:39.360
you say you were right, and it just gets more and more carried away.

15:39.360 --> 15:45.520
So they initiated it with some interesting text.

15:45.520 --> 15:49.560
And the question is, will the neural net then produce stuff that's sort of related to that?

15:49.560 --> 15:52.400
I mean, the first question is, will it produce English words?

15:52.400 --> 15:54.520
Will the words have decent syntax?

15:54.520 --> 15:56.000
Will it have any meaning?

15:56.000 --> 15:58.280
Will it be related to this?

15:58.280 --> 16:02.560
If you're really optimistic, you might say, will they sort of relate to the fundamental

16:02.560 --> 16:05.400
problem here, which is how these unicorns can speak English?

16:05.400 --> 16:07.360
Okay, so here goes.

16:07.360 --> 16:08.960
This is what the neural net produced.

16:08.960 --> 16:10.240
Now this was cherry-picked.

16:10.240 --> 16:15.360
This was one of their better examples.

16:15.360 --> 16:21.720
The neural net just made this up, right?

16:21.720 --> 16:27.560
It made up Dr. Jorge Perez, there is no such person at the University of La Paz, but it's

16:27.560 --> 16:32.160
pretty plausible because it's South America, and I believe La Paz has a university.

16:32.160 --> 16:37.960
Okay, so that's the first bit of what it made up, and it carries on and it gets better.

16:38.960 --> 16:42.960
The next bit sounds a bit like one of those fantasy games.

16:50.960 --> 16:53.440
So it's remembered about unicorns and herds of unicorns, right?

16:53.440 --> 16:56.880
So they walk up and there's this strange valley, and it's a very strange valley, and they found

16:56.880 --> 17:03.360
the herds of unicorns.

17:03.360 --> 17:06.040
And it has something about seeing them from the air and being able to touch them, which

17:06.040 --> 17:07.320
isn't quite right.

17:07.320 --> 17:10.880
So people in Symbolicae leap on this and say, you see, it doesn't understand.

17:10.880 --> 17:16.080
Well, sure, there's little bits that it doesn't get right.

17:16.080 --> 17:20.400
But notice, it's remembered that these unicorns have to speak English, and so it tells you

17:20.400 --> 17:25.560
about, you know, they spoke some fairly regular English.

17:25.560 --> 17:30.200
It doesn't know the difference between dialect and dialectic, but my kids don't know that

17:30.200 --> 17:31.200
either.

17:31.200 --> 17:37.280
In fact, I'm not sure I know.

17:37.280 --> 17:44.800
It's a tribute to unicorns to Argentina, even though Dr. Perez comes from Bolivia.

17:44.800 --> 17:48.000
And it actually understands about magic realism.

17:48.000 --> 17:54.720
So they're descendants of a lost race, and I love the bit at the end where it says, in

17:54.720 --> 17:59.120
South America, such incidents seem to be quite common.

17:59.120 --> 18:04.600
This has an ability to just make up something that fits your prejudices and sounds moderately

18:04.600 --> 18:12.760
plausible, like a certain president.

18:12.760 --> 18:17.560
And it finally gets to the point which is, if you really want to know whether these unicorns

18:17.560 --> 18:23.520
were used by breeding with these strange lost race of people, you ought to do a DNA test.

18:23.520 --> 18:24.520
Okay?

18:24.520 --> 18:26.000
It understands that.

18:26.000 --> 18:27.000
Okay.

18:27.000 --> 18:30.440
So that's what neural nets can do now.

18:30.440 --> 18:38.120
This was a neural net with 1.5 billion connections that was trained on Google's, actually, I

18:38.120 --> 18:44.280
withdraw that, 1.5 billion connections is trained on a lot of hardware.

18:44.280 --> 18:50.160
And we look at what it says, and we sort of laugh at how, you know, it's pretty good,

18:50.160 --> 18:52.800
but it hasn't got it quite right, but it's pretty good.

18:52.800 --> 18:53.800
Okay.

18:53.800 --> 18:59.720
What they've done now is they've trained a neural net with 50 billion connections on

18:59.720 --> 19:05.720
Google's latest cloud hardware, which is, it's like having several of the world's biggest

19:05.720 --> 19:09.840
supercomputers going for you for months.

19:09.840 --> 19:14.520
The net with 50 billion connections, I haven't seen any text from it yet, but my prediction

19:14.520 --> 19:19.600
is it's sitting around laughing at how cute what we produce is.

19:19.600 --> 19:21.160
Okay.

19:21.160 --> 19:29.240
So one thing about that net is it's clearly very well aware of the initial context.

19:29.240 --> 19:34.720
These unicorns in a valley that speak English, and it's remembering this initial context

19:34.720 --> 19:37.480
a long time later, and a recurrent neural net can't do that.

19:37.480 --> 19:40.480
A recurrent neural net would have forgotten about the initial stuff and wouldn't produce

19:40.480 --> 19:43.560
such good context-dependent stuff.

19:43.560 --> 19:52.040
So the way this works is the word comes in, the neural net activates some hidden units.

19:52.040 --> 19:57.520
That pattern of activity in the hidden units goes and compares itself with previous patterns,

19:57.960 --> 20:02.560
your point of view, with previous patterns at earlier times.

20:02.560 --> 20:06.720
And when it finds a pattern at an earlier time that's a bit similar, it says that we'll

20:06.720 --> 20:11.880
take advice from that previous hidden pattern about how to affect the next layer.

20:11.880 --> 20:19.640
And so actually a word comes in and how one pattern of activity in the bottom layer of

20:19.640 --> 20:26.800
the hidden neurons affects the next layer is dependent on what happened previously.

20:26.800 --> 20:31.040
Now it's dependent in quite a complicated way, and this seems very implausible for a

20:31.040 --> 20:36.800
brain because what's happening in the computer is you're storing all these activity patterns

20:36.800 --> 20:39.800
that are meant to be neural activity patterns or light neural activity patterns, and you're

20:39.800 --> 20:43.320
comparing, and this looks hopeless.

20:43.320 --> 20:49.080
But actually all you need to do is every time you have an activity pattern, and you use

20:49.080 --> 20:53.760
the outgoing weights to affect the next layer, just change the weight slightly with heavy

20:53.760 --> 20:55.560
and learning.

20:55.560 --> 21:01.480
So now what's going to happen is that weight matrix that comes out of that activity pattern

21:01.480 --> 21:03.200
is going to be modified slightly.

21:03.200 --> 21:08.200
Now when I get a new activity pattern, if the activity pattern is orthogonal to the previous

21:08.200 --> 21:13.160
activity pattern, then any modifications you made in the weight matrix due to that previous

21:13.160 --> 21:15.400
activity pattern won't make any difference.

21:15.400 --> 21:19.760
But if it lines up with the previous activity pattern, if it's similar, the modifications

21:19.760 --> 21:25.040
you made in the weights back there, the temporary modifications, will cause this new activity

21:25.040 --> 21:27.360
pattern to have a different effect here.

21:27.360 --> 21:31.880
So you'll get that long temporal context, and the way to store a long temporal context

21:31.880 --> 21:39.280
is not to keep copies of neural activity patterns, it's to take your weights and to have temporary

21:39.280 --> 21:41.600
changes to the weights, which I call fast weights.

21:41.600 --> 21:48.640
So you temporarily change them, and these changes decay over time, so you'll have a memory.

21:48.640 --> 21:52.800
So if you ask, where in your brain is your memory of what I said a few minutes ago?

21:53.360 --> 21:56.160
I'll ask the younger people this, because for the older people it's nowhere.

21:56.160 --> 22:01.720
But for the younger people, it's somewhere you can, if I were to say something I said

22:01.720 --> 22:07.080
a few minutes ago, like these big neural nets are now laughing at us, you remember I said

22:07.080 --> 22:09.520
that, where was that memory?

22:09.520 --> 22:14.040
I think it's in the temporary changes to the weights, because that's got much bigger capacity

22:14.040 --> 22:20.080
than activities of neurons, and you don't need to use up neurons just sitting there remembering.

22:20.080 --> 22:23.360
And those temporary changes don't need to be driven by back propagation, they can just

22:23.360 --> 22:25.360
be heavier.

22:25.360 --> 22:33.800
Okay, so I've tried to relate these wonderful nets that can make up stories with an idea

22:33.800 --> 22:37.800
about where short-term memory is in the brain.

22:37.800 --> 22:41.520
And now I'll talk about where the cortex can do back propagation.

22:41.520 --> 22:48.560
So neuroscientists, 20 years ago neuroscientists said don't be ridiculous, of course the brain

22:48.560 --> 22:52.960
can't do back propagation, and they'd interpret it very literally as sending signals backwards

22:52.960 --> 23:03.600
down the same axons and saying neurons don't do that, no thanks.

23:03.600 --> 23:08.760
But now we know that back propagation works really well for solving tough practical problems.

23:08.760 --> 23:12.320
So that's rather changed the balance, because when back propagation was just a theory of

23:12.320 --> 23:17.560
how you might get computers to learn something, and when it learns some simple things, it

23:17.560 --> 23:20.800
wasn't sort of imperative to understand whether the brain did it.

23:20.800 --> 23:24.920
But now we know that you can do all these things with back propagation.

23:24.920 --> 23:29.000
What's more, we know that back propagation is the right thing to do, but if you have

23:29.000 --> 23:35.320
a sensory pathway, and you want to take the early feature detectors so that their outputs

23:35.320 --> 23:41.600
are more helpful for making the right decision later on in the system, then what you really

23:41.600 --> 23:46.560
need to do is ask the question, how should I change the receptive field of this early

23:46.560 --> 23:52.760
detector so that what is output helps with the decision?

23:52.760 --> 23:55.480
And what you have to do is do back propagation to compute that, that's the efficient way

23:55.480 --> 23:56.480
to compute it.

23:56.480 --> 24:02.280
And I think it'd be crazy if the brain wasn't somehow doing this.

24:02.280 --> 24:05.480
So why do neuroscientists think it's impossible?

24:05.480 --> 24:11.240
Apart from silly objections like things don't go backwards down axons, at least not at the

24:11.240 --> 24:20.240
right speed.

24:20.240 --> 24:22.240
He wants me to update things.

24:22.240 --> 24:30.280
Oh, it's just died.

24:30.280 --> 24:33.640
I'm going to go out and present them out.

24:34.640 --> 24:41.640
I'm going to go back in to present them out.

24:41.640 --> 24:46.800
Okay, so here's some reasons why the brain can't do back propagation.

24:46.800 --> 24:52.240
The first reason is they say, well, it doesn't get the supervision signal.

24:52.240 --> 24:55.720
And they're imagining that the supervision signal is like you take a micro pipette and

24:55.720 --> 25:00.320
you put it into the infrotemporal cortex and you inject the right answer and the brain

25:00.320 --> 25:03.240
doesn't have anything like that, right?

25:03.240 --> 25:08.440
But actually, if you take that language model, it didn't need label data, it was just trying

25:08.440 --> 25:09.960
to predict the next word.

25:09.960 --> 25:14.480
So you can often use part of the input, maybe a future part of the input, or maybe a small

25:14.480 --> 25:20.040
part of an image as the right answer, and so you can get supervision signals easily.

25:20.040 --> 25:22.800
So there's no problem about supervision signals.

25:22.800 --> 25:27.800
The second reason is neurons don't send real-valued activities, they send spikes.

25:27.800 --> 25:32.360
And back propagation is using these real-valued activities so you can get nice, smooth derivatives.

25:32.360 --> 25:36.040
So back propagation can't possibly be what's going on in the brain.

25:36.040 --> 25:39.000
The third objection is, neurons have to send two signals.

25:39.000 --> 25:45.240
They have to send the activity forwards and they have to send error derivatives backwards.

25:45.240 --> 25:50.440
The signal they have to send backwards is, how sensitive am I to changes in my input?

25:50.440 --> 25:58.640
Or rather, if you change my input, how much does that help with the final answer?

25:58.640 --> 26:03.000
And the last thing is about neurons having reciprocal connections because you have to,

26:03.000 --> 26:08.480
when you send things backwards, if you use a different neuron, you have to use the same

26:08.480 --> 26:11.520
weight as the forward weight.

26:11.520 --> 26:17.000
I'm not going to tell you how you can overcome that, but you can easily.

26:17.000 --> 26:21.760
So supervision signals isn't really a problem, there's many ways to get a supervision signal.

26:21.760 --> 26:27.000
The simplest is predicting what comes next.

26:27.000 --> 26:31.960
Now the question of can neurons communicate real values?

26:31.960 --> 26:36.560
Well the first thing to notice about back propagation is, if you have very noisy estimates

26:36.560 --> 26:39.920
of the gradient, it works just as well.

26:39.920 --> 26:43.760
It's very, very tolerant of noise as long as it's unbiased noise.

26:43.760 --> 26:48.000
So for example, the signal you send forwards can be one bit, one stochastic bit, and the

26:48.000 --> 26:50.520
signal you send backwards can be two bits.

26:50.520 --> 26:55.120
If they have the right average value, if their expected values are correct, then they're

26:55.120 --> 27:02.320
just this expected value plus some noise, and the whole system still works fine.

27:02.320 --> 27:06.800
So in the brain, you have a neuron.

27:06.800 --> 27:12.320
At any instant, the neuron has an underlying firing rate, and it produces spikes, and for

27:12.320 --> 27:17.440
now let's just suppose it produces spikes according to a Poisson process.

27:17.440 --> 27:20.920
So it's probability of producing a spike in a small interval, which is the underlying

27:20.920 --> 27:23.320
firing rate.

27:23.520 --> 27:29.640
The question is, suppose we treated it as if it could send that underlying firing rate.

27:29.640 --> 27:33.160
When it sends a Poisson spike, it's just a very noisy version of the underlying firing

27:33.160 --> 27:35.160
rate.

27:35.160 --> 27:40.520
It's a one or a zero, but its expected value is the underlying firing rate.

27:40.520 --> 27:47.280
So how well do neural networks work if we send very noisy signals?

27:47.280 --> 27:52.880
So I'm going to have a statistics digression.

27:52.880 --> 27:57.400
If you do statistics 101, they tell you you shouldn't have more parameters in your model

27:57.400 --> 27:59.280
than you have data points.

27:59.280 --> 28:03.080
You really ought to have quite a few data points for each parameter.

28:03.080 --> 28:06.400
It turns out this is completely wrong.

28:06.400 --> 28:11.520
The Bayesians knew it was wrong, actually.

28:11.520 --> 28:15.280
The brain is not in the same regime of statistics 101.

28:15.280 --> 28:19.160
In the brain, you're fitting about 10 to the 14 parameters, and you have about 10 to the

28:19.160 --> 28:21.280
nine seconds.

28:21.320 --> 28:27.040
So even if you have sort of 10 experiences per second, so even if you take 100 milliseconds

28:27.040 --> 28:33.880
is the time for an experience, that's the kind of backward masking time, you have like

28:33.880 --> 28:38.920
10,000 synapses per 100 milliseconds of your life.

28:38.920 --> 28:42.680
You're throwing a lot of parameters.

28:42.680 --> 28:46.920
So if your mother just kept saying, good, bad, good, bad, good, bad, she couldn't possibly

28:46.960 --> 28:51.960
provide enough information to learn all those 10 to the 14 parameters.

28:51.960 --> 28:58.960
And here's what they teach you wrong in statistics.

29:01.200 --> 29:04.960
Everybody knows that if you've got a given size model with a given number of parameters,

29:04.960 --> 29:07.800
the more data you have, the better you'll generalize.

29:07.800 --> 29:10.760
So for a given size of model, it's always better to have more data.

29:10.760 --> 29:14.360
In fact, the best thing you can do is get more data.

29:15.160 --> 29:19.520
Okay, but that doesn't mean that if you've got a fixed amount of data, you should make

29:19.520 --> 29:21.600
it look like a lot by having a small model.

29:21.600 --> 29:24.200
That's what they tell you in statistics 101.

29:24.200 --> 29:26.200
Okay?

29:26.200 --> 29:33.480
Big models are good if you regularize them, if you stop them doing crazy things.

29:33.480 --> 29:36.920
We can see that using a lot of parameters is good, that you can always win by having

29:36.920 --> 29:37.920
more parameters.

29:37.920 --> 29:39.920
And the way you do that is say, I'm going to have a committee.

29:39.920 --> 29:43.000
I'm going to learn lots of different little neural nets.

29:43.000 --> 29:45.960
If you give me more parameters, I'll learn more different neural nets.

29:45.960 --> 29:48.240
And then I'll average what they all say.

29:48.240 --> 29:49.680
And you'll always win.

29:49.680 --> 29:56.000
It's a sort of declining win, but if you have enough of them, you'll win by having more.

29:56.000 --> 30:00.840
So it's always better to have more parameters.

30:00.840 --> 30:07.840
It turns out if you have a fixed amount of data and you have enough computation power,

30:07.920 --> 30:13.680
the brain has, you should always use such a big model that the amount of data looks

30:13.680 --> 30:14.680
small.

30:14.680 --> 30:17.760
That's the regime you ought to be in for a fixed amount of data.

30:17.760 --> 30:22.880
That is, if you take the limit when the amount of data is fixed and you have unlimited computation

30:22.880 --> 30:28.040
and ask now how big would you like your model to be, you'd like your model to be much bigger

30:28.040 --> 30:30.680
than the data.

30:30.680 --> 30:33.360
Okay.

30:34.080 --> 30:36.840
That only works if you have a good regularizer.

30:36.840 --> 30:44.480
And I'm now going to tell you a very good regularizer called Dropout.

30:44.480 --> 30:47.520
So this is to use in neural networks where you have a lot more parameters than you have

30:47.520 --> 30:50.080
data points to train them on.

30:50.080 --> 30:53.840
And you could learn an ensemble of little models, and this is a way of learning an ensemble

30:53.840 --> 30:59.880
of many more models, but the models in the ensemble can share things with each other.

30:59.920 --> 31:05.640
So the idea is, if we just have one hidden layer in a neural net, we put the data in

31:05.640 --> 31:11.640
and each time we show the data vector, we randomly remove half the neurons.

31:11.640 --> 31:19.400
So we randomly get rid of half the neurons in your brain and only use the ones that remain.

31:19.400 --> 31:22.880
And it's a different subset we remove each time.

31:22.880 --> 31:27.440
Now when we do use a neuron, we use it with the same weights each time.

31:27.480 --> 31:31.080
So what you've got is if you've got h hidden neurons, you've got two to the h different

31:31.080 --> 31:33.240
subsets of neurons you might use.

31:33.240 --> 31:37.920
So you actually have two to the h different models, exponentially many models.

31:37.920 --> 31:39.440
Most of the models are never used.

31:39.440 --> 31:44.360
A few of the models will see one example, a small fraction of them will see one example.

31:44.360 --> 31:48.200
No models will see two examples.

31:48.200 --> 31:50.960
And yet they can learn because they're all sharing parameters.

31:50.960 --> 31:54.400
So this idea of sharing parameters in a neural network is very effective.

31:54.400 --> 31:59.240
So really you've got all these different models that are sharing parameters and you train

31:59.240 --> 32:02.560
it up and it generalizes really well.

32:06.600 --> 32:08.360
So I said that.

32:08.360 --> 32:14.160
So what we know is if you get rid of a fraction of the neurons each time and treat it as though

32:14.160 --> 32:18.240
they weren't there, it works really well.

32:18.240 --> 32:21.200
That's just a form of noise.

32:21.200 --> 32:26.360
And basically this is just an example of if you have a very big model and you add a lot

32:26.360 --> 32:34.880
of noise, the noise allows it to generalize well and it's better to have a big model with

32:34.880 --> 32:38.720
a lot of noise than to have a small model with no noise.

32:38.720 --> 32:42.240
And so what the brain wants, because it's got such a big model compared with the amount

32:42.240 --> 32:45.760
of data it operates on, it wants a lot of noise.

32:45.760 --> 32:48.880
And so now a Poisson neuron is kind of ideal.

32:48.880 --> 32:54.000
It's got a firing rate and now it adds a whole lot of noise to that and either sends a one

32:54.000 --> 32:59.880
or a zero and that actually makes it generalize much better.

32:59.880 --> 33:04.000
So the argument is the reason neurons don't send real values is they don't want to.

33:04.000 --> 33:08.280
They want to send things with a lot of noise in and that's making them generalize better.

33:08.280 --> 33:11.120
So that's not an argument against backpropagation.

33:11.120 --> 33:15.440
These dropout models are trained with backpropagation.

33:15.440 --> 33:24.520
So the random spikes are really just a way of adding noise to the signal to get better generalization.

33:24.520 --> 33:28.520
And now the last thing I'm going to address, I'm going to keep going till Blake stops me

33:28.520 --> 33:34.920
and I figure I've got about another five minutes before he gets really ratty.

33:34.920 --> 33:42.960
So the output of a neuron represents the presence of a feature in the current input.

33:43.000 --> 33:48.000
So it's obvious the same output can't represent the error derivative, right?

33:48.000 --> 33:51.880
You couldn't have a neuron that said to higher layers, this is the value of my feature

33:51.880 --> 33:55.120
and said to lower layers, this is my error derivative.

33:55.120 --> 33:56.520
It couldn't be done.

33:56.520 --> 34:02.720
So the neurons that go backwards need to be different neurons except that that's nonsense.

34:02.720 --> 34:07.320
So here's my claim.

34:07.320 --> 34:08.720
Joshua Benjo picked up on this later.

34:08.720 --> 34:10.760
I made this claim first in 2007.

34:10.760 --> 34:15.360
Actually, I made it first in the ground proposal.

34:15.360 --> 34:19.120
And I still believe this claim even though nobody's managed to make it work really well

34:19.120 --> 34:21.160
in the neural net yet.

34:21.160 --> 34:26.960
The idea is a neuron has a firing rate.

34:26.960 --> 34:34.240
That's the firing rate is its real output, which is communicated stochasticly by a spike.

34:34.240 --> 34:41.520
And that firing rate is actually changing over time, the underlying firing rate.

34:41.520 --> 34:47.360
And the rate of change of the firing rate is used to represent the error derivative.

34:47.360 --> 34:51.400
Now the nice thing about a rate of change is it can be positive or negative.

34:51.400 --> 34:56.560
So we can represent positive or negative derivatives without a neuron having to change

34:56.560 --> 35:00.560
the sort of signs of its synapses.

35:00.760 --> 35:04.000
And what it's representing, the derivative it's representing is the derivative of the

35:04.000 --> 35:06.920
error with respect to the input to the neuron.

35:06.920 --> 35:08.400
And that gets sent back to earlier neurons.

35:08.400 --> 35:11.800
And if I had enough time, I could show you a whole bunch of slides about how this will

35:11.800 --> 35:14.400
do back prop.

35:14.400 --> 35:17.960
But I want to show you one consequence of this.

35:17.960 --> 35:21.960
So that's, look, here we have a nice equation because it's got Leibniz on one side and Newton

35:21.960 --> 35:25.040
on the other side.

35:25.040 --> 35:28.400
That's Leibniz's notation for derivatives because they're not derivatives with respect

35:28.400 --> 35:29.720
to time.

35:29.720 --> 35:34.640
And this is Newton's notation because that was for derivatives with respect to time, okay?

35:34.640 --> 35:41.480
And what we're saying is the output of neuron J, which is yj, is the output of neuron J.

35:41.480 --> 35:46.280
But how fast that's changing over a short time interval is the error derivative.

35:46.280 --> 35:49.600
This is just a hypothesis, you understand?

35:49.600 --> 35:54.440
But it's true.

35:54.440 --> 35:58.680
Jay McClellan and I first used a version of this in 1988 before we knew about its back

35:58.720 --> 36:00.040
time dependent plasticity.

36:00.040 --> 36:03.200
I'm not sure it would be discovered then.

36:03.200 --> 36:06.200
Where you take, this is where I need the cursor.

36:06.200 --> 36:10.200
Yes, that one.

36:10.200 --> 36:12.280
Yes.

36:12.280 --> 36:14.120
You take some input.

36:14.120 --> 36:17.640
You send it to some hidden units, which send it to more hidden units by the green connections

36:17.640 --> 36:18.640
and sends it to more hidden units.

36:18.640 --> 36:21.840
It comes back to the input, so you reconstruct the input.

36:21.840 --> 36:25.840
And then you send it around again, not all the way around, but up to there and up to

36:25.840 --> 36:28.640
there and up to there using the right connections.

36:28.640 --> 36:33.760
And then the learning rule, which you'll notice doesn't involve explicit back propagation,

36:33.760 --> 36:42.280
is to say for these neurons, for example, I change the incoming weights by the activity

36:42.280 --> 36:49.360
of the presynaptic neuron down here times the difference between what I got on the green

36:49.360 --> 36:52.880
activation and on the red activation, first time round and second time round.

36:52.880 --> 36:57.760
So the rate of change of the activation of the neuron is what's used to communicate an

36:57.760 --> 36:58.760
error derivative.

36:58.760 --> 37:06.400
Now, unfortunately, this thing has the wrong sign, but later on we fixed that.

37:06.400 --> 37:14.240
And so here's a theory from 2007 that still hasn't been conclusively proved wrong.

37:14.240 --> 37:18.200
And it sort of works, but it doesn't work quite as well as we hoped, about how you could

37:18.200 --> 37:21.280
get a brain to do back propagation.

37:21.280 --> 37:25.120
What you first do is you learn a stack of autoencoders, that is you learn to get each

37:25.120 --> 37:31.280
layer to activate features in the layer above from which you can reconstruct the layer below.

37:31.280 --> 37:34.080
So you learn some features that can reconstruct this layer.

37:34.080 --> 37:37.880
Then you treat those features as data and learn some features that can reconstruct them.

37:37.880 --> 37:40.080
You build a big stack of autoencoders like that.

37:40.080 --> 37:41.080
Okay.

37:41.080 --> 37:45.720
Once you build the stack of autoencoders, then each layer can activity in a layer can

37:45.720 --> 37:49.420
reconstruct the activity in the layer below.

37:49.420 --> 37:52.640
And then you do two top down passes.

37:52.640 --> 37:57.360
You do a top down pass from the thing you predicted at the output.

37:57.360 --> 38:00.960
So you put in input, activity goes forward through the layers, you predict something

38:00.960 --> 38:01.960
at the output.

38:01.960 --> 38:07.040
And now you do a top down pass and you get reconstructed activities everywhere.

38:07.040 --> 38:15.280
And then you take your output and you change it to be more like the desired output.

38:15.280 --> 38:21.200
And now you do a top down pass and you'll get slightly different reconstructions.

38:21.200 --> 38:28.120
And the difference between those two reconstructions is actually the signal you need for back propagation.

38:28.120 --> 38:33.080
And so if you do that, the learning rule is that you should change a synapse by the

38:33.080 --> 38:39.200
pre-synaptic activity in the layer below times the rate of change of the activity in the

38:39.200 --> 38:41.280
layer above in the post-synaptic neuron.

38:41.280 --> 38:44.920
So it's a very simple learning rule.

38:45.120 --> 38:50.920
So let's change the weight in proportion to the pre-synaptic activity times the rate

38:50.920 --> 38:55.040
of change of the post-synaptic activity.

38:55.040 --> 39:00.640
Now it turns out if you're using spiking neurons, what that amounts to that are representing

39:00.640 --> 39:07.480
underlying firing rates that are changing, that amounts to a learning rule that looks

39:07.480 --> 39:13.080
like this.

39:13.080 --> 39:18.880
What you do is you take a pre-synaptic spike and you ask whether the post-synaptic spike

39:18.880 --> 39:21.920
came before or after it.

39:21.920 --> 39:27.480
Because what you're interested in is the rate of change of the post-synaptic firing rate

39:27.480 --> 39:33.000
around the time of the pre-synaptic spike.

39:33.000 --> 39:41.080
And if the post-synaptic spike occurs often just after it and seldom just before it, that

39:41.080 --> 39:43.520
suggests the firing rate is going up.

39:43.520 --> 39:47.840
And if the post-synaptic spike occurs often just before the pre-synaptic one and less

39:47.840 --> 39:51.160
often just after it, that means the firing rate of the post-synaptic neuron is going

39:51.160 --> 39:53.280
down.

39:53.280 --> 39:58.240
So if you want your learning rule to be the pre-synaptic activity, well, you'll only learn

39:58.240 --> 40:00.360
when you get a pre-synaptic spike.

40:00.360 --> 40:04.400
And then what you'll do is you'll say, did the post-synaptic spike occur afterwards

40:04.400 --> 40:05.400
or before?

40:05.400 --> 40:09.120
If it occurred afterwards, I should raise the weight and if it occurred before, I should

40:09.120 --> 40:10.600
lower the weight.

40:10.640 --> 40:18.720
And so your learning rule will look like this and this thing is actually a derivative filter.

40:18.720 --> 40:23.720
It's centered at zero and what this is really doing is measuring the rate of change of the

40:23.720 --> 40:25.880
post-synaptic firing rate.

40:25.880 --> 40:28.040
And of course it's sampling it.

40:28.040 --> 40:31.680
So you have a post-synaptic firing rate, there's these spike trains and are the other spikes

40:31.680 --> 40:34.040
getting closer together or further apart?

40:34.040 --> 40:36.720
Well this is a way to measure that.

40:36.720 --> 40:42.440
And of course you can do the learning on individual spikes and the learning rule would then be

40:42.440 --> 40:46.440
the implementation of this idea that the rate of change of the post-synaptic firing rate

40:46.440 --> 40:48.920
is the error signal.

40:48.920 --> 40:52.800
The learning rule would just be if the post-synaptic spike goes after the pre-synaptic one, increase

40:52.800 --> 40:57.720
the strength, otherwise decrease it and have that whole effect fall off as the spikes get

40:57.720 --> 41:01.920
further away because we're really only interested in the rate of change of the firing rate around

41:01.920 --> 41:05.760
the time of the pre-synaptic spike.

41:05.800 --> 41:11.120
Now there's one consequence of that which is that if you're going to use the rate of

41:11.120 --> 41:17.560
change of a neuron to represent not what the neuron is representing but to represent an

41:17.560 --> 41:22.520
error derivative, you've basically used up temporal derivatives for communicating error

41:22.520 --> 41:23.800
derivatives.

41:23.800 --> 41:28.640
So you cannot use temporal derivatives to communicate the temporal derivatives of what the neuron

41:28.640 --> 41:29.640
represents.

41:29.640 --> 41:34.080
So however I have a neuron that represents position, I can't use how fast that's changing

41:34.080 --> 41:37.480
to represent velocity and that's true of neurons.

41:37.480 --> 41:42.720
If you want to represent velocity, you have to have a neuron whose output represents velocity.

41:42.720 --> 41:46.000
You can't do it with the rate of change of a position neuron.

41:46.000 --> 41:52.680
If I kill the velocity neurons and keep the position neurons and I watch a car moving,

41:52.680 --> 41:56.400
the position neurons will change but I won't see any motion.

41:56.400 --> 42:01.480
Similarly, you can't use the rate of change of a velocity neuron to represent acceleration.

42:02.280 --> 42:08.120
Okay, so I think the fact that you can't use the rate of change of a representation to

42:08.120 --> 42:13.680
represent that that stuff in the world is changing is more evident since the board of

42:13.680 --> 42:20.120
the idea temporal derivatives of neurons are used up in representing error derivatives.

42:20.120 --> 42:24.040
So now I'll summarize.

42:24.040 --> 42:28.880
The main arguments against back propagation, the fact that they spent neurons and spikes

42:28.880 --> 42:34.480
rather than real numbers, well that's just because a lot of noise regularizes things.

42:34.480 --> 42:39.120
You can represent error derivatives as temporal derivatives so the same neuron can send temporal

42:39.120 --> 42:46.440
derivatives backwards, communicate those backwards and communicate activities forwards and the

42:46.440 --> 42:51.040
fact that in the brain you do get back to independent plasticity seems to be evidence

42:51.040 --> 42:55.560
in favor of that representation of error derivatives and now I'm done.

42:58.880 --> 43:13.880
Thank you Jeff for a great talk, sorry that was a Twitter joke, got it, anyway.

43:13.880 --> 43:21.920
So now what we're going to do is a brief Q&A between myself and Jeff and then after

43:21.920 --> 43:27.000
I've had my chance to ask some questions I'm going to open it up to you guys.

43:27.000 --> 43:34.560
Now I had originally sent Jeff a few questions which I'll rely on partially but his talk

43:34.560 --> 43:39.040
has made me want to ask a few others so I'm sorry I'm going to throw a few loops at you

43:39.040 --> 43:43.400
as well but let's start with some of the ones that I told you I wouldn't give you.

43:43.400 --> 43:48.560
There is something funny with my mic, I don't know if the AV guy is there, I just won't

43:48.560 --> 43:49.560
look down.

43:49.560 --> 44:01.760
Okay so the first question, yeah it's okay, I'm going off script anyway.

44:01.760 --> 44:07.080
The first question which I would like to ask just because it's something that I spend far

44:07.080 --> 44:16.080
too long arguing with people online is essentially you know so you're in the computer science

44:16.080 --> 44:20.760
department, you've come here, you've given us a talk that's largely about brains but

44:20.760 --> 44:26.160
many people seem to object to the idea that computers have anything to tell us about brains

44:26.160 --> 44:30.920
or indeed the idea that the brain is a computer despite the fact that neuroscientists often

44:30.920 --> 44:33.160
refer to computation in the brain.

44:33.160 --> 44:39.840
So my question is to you, is the brain a computer, I don't know I'll just hand that over to

44:39.840 --> 44:40.840
you first.

44:41.600 --> 44:42.600
Yes?

44:42.600 --> 44:45.120
Good, okay.

44:45.120 --> 44:50.880
And for the record I didn't tell him to say that if anyone from Twitter is watching.

44:50.880 --> 44:56.160
And B, can you just maybe give an intuitive understanding of why the answer is yes despite

44:56.160 --> 45:01.000
the fact that obviously our brains are very different from our laptops or our cell phones

45:01.000 --> 45:03.600
and stuff like that.

45:03.600 --> 45:10.640
So there's many ways you can do computation with physical stuff and you could get some

45:10.640 --> 45:15.920
silicon and make transistors and then run them at very high voltage much higher than

45:15.920 --> 45:21.800
needed to make them be digital and then you could if you wanted to represent a number

45:21.800 --> 45:26.920
you could have bits and you could and so on and you could create multipliers and adders

45:26.920 --> 45:31.160
and then you could put all that together and you could have some bits that tell you where

45:31.200 --> 45:38.240
in memory to find stuff and you could make a conventional computer or you could make

45:38.240 --> 45:44.280
little devices that have some input lines that are hardwired with input lines and you

45:44.280 --> 45:46.080
could have adaptive weights on the input lines.

45:46.080 --> 45:53.520
So early neural nets, Marvin Minsky made neural nets out of feedback controllers that were

45:53.520 --> 45:59.080
used in I think B-52 bombers or B-29 bombers or something, B-27 I don't know, some kind

45:59.080 --> 46:05.960
of bomber, it was America and so you can make computers in lots of different ways.

46:05.960 --> 46:11.480
When I was a kid I used to make computers by you take a six inch nail and you saw the

46:11.480 --> 46:17.760
head off and then you wrap copper wire around it and then you take a razor blade and you

46:17.760 --> 46:24.240
break it in half so that it's a nice flexible thing like this and you wrap a bit of copper

46:24.240 --> 46:28.560
wire around the razor blade and then when the current goes through the nail it'll make

46:28.640 --> 46:32.600
the razor blade go down and you'll make a contact so now you've got a relay and then

46:32.600 --> 46:35.240
you can put a bunch of those together and make logic gates.

46:35.240 --> 46:40.480
I never got more than about two logic gates that way but yeah you can make computers in

46:40.480 --> 46:47.920
lots of different ways and the brain is clearly made in a different way from the normal computers

46:47.920 --> 46:54.800
which has some different strengths and weaknesses so it's much slower but on the other hand

46:54.800 --> 46:57.760
you can make it much more parallel.

46:57.760 --> 47:02.600
It has one special property which I think is what makes us mortal which is that every

47:02.600 --> 47:09.800
brain is different so I can't take the weights from my brain and put them in Blake's brain

47:09.800 --> 47:12.920
and hope that it'll work because he just doesn't have connections in the same places.

47:12.920 --> 47:13.920
You've tried.

47:13.920 --> 47:14.920
Right.

47:14.920 --> 47:20.240
Well there's a way of doing it where you take the weights in my brain I turn it into strings

47:20.240 --> 47:21.240
of words.

47:21.240 --> 47:27.440
Blake absorbs these strings of words and puts different weights in his brain.

47:27.440 --> 47:31.520
It's pretty lucky all our brains are different because otherwise rich people will grab poor

47:31.520 --> 47:42.200
people's brains so they could live forever but quite okay.

47:42.200 --> 47:51.840
So I think I want to ask you then following on that what do you think about some of the

47:51.840 --> 47:57.760
quests to fully characterize the brains connectome do you think that is a scientifically worthwhile

47:57.760 --> 47:58.760
endeavor.

47:58.760 --> 48:08.320
Yes I do partly because some of the people doing it are my friends.

48:08.320 --> 48:11.480
Ignoring your loyalty to Sebastian.

48:11.760 --> 48:17.120
Not well in that case no.

48:17.120 --> 48:23.200
It seems to me it is very worth doing but you don't have to do that in order to begin

48:23.200 --> 48:25.040
to understand the principles.

48:25.040 --> 48:26.040
Very good.

48:26.040 --> 48:30.040
But for things like the retina which has a lot of hardwired stuff in it I think it's

48:30.040 --> 48:32.240
really important to do that.

48:32.240 --> 48:34.800
So that actually leads on to my next question.

48:34.800 --> 48:37.440
I wanted to ask you about hardwiring.

48:37.440 --> 48:44.080
So another thing that I think many people who study the brain find difficult about artificial

48:44.080 --> 48:49.120
neural networks as a model for the brain is that as you say you start with random weights

48:49.120 --> 48:52.200
and you train it on a lot of data and you get these things out.

48:52.200 --> 48:58.040
But we know that there are some pre-wired things in many brains so the classic examples

48:58.040 --> 49:04.160
are a horse can run pretty much right out of the womb but even within humans arguably

49:04.160 --> 49:08.160
there are some things that we find easier to learn than others.

49:08.160 --> 49:15.480
And so what do you think is the place for innate behavior within neural networks as

49:15.480 --> 49:17.160
a model of cognition?

49:17.160 --> 49:22.760
Okay so it used to be when I was a student if you were interested in language people

49:22.760 --> 49:28.200
would tell you that it was all innate and it just kind of matured as you got older and

49:28.200 --> 49:32.760
maybe you learned like 12 parameters that characterised your particular language whether

49:32.840 --> 49:38.280
it was subject verb object or some other way.

49:38.280 --> 49:43.080
In fact there's a nova that I saw, it was probably made about 20 years ago and it has

49:43.080 --> 49:48.560
all the leading linguists all of whom were educated by Chomsky and they look straight

49:48.560 --> 49:52.720
at the camera and they say there's a lot we don't know about language but one thing

49:52.720 --> 49:57.600
we know for sure is that it's not learned.

49:57.600 --> 49:59.720
So Chomsky had really good gulag.

49:59.720 --> 50:02.720
Yeah he did.

50:02.720 --> 50:09.680
But it's over because we know now if you want to translate you just learn it all.

50:09.680 --> 50:15.360
The number of linguists required to get a system that can turn a string of symbols in

50:15.360 --> 50:20.000
English into a string of symbols in French is roughly zero.

50:20.000 --> 50:23.480
I mean linguists involved in preparing the databases for training and making sure you

50:23.480 --> 50:29.000
get sort of a variety of grammatic structures and things but basically you don't need linguists,

50:29.000 --> 50:32.360
you just need data.

50:32.360 --> 50:34.680
So you don't need much innate structure.

50:34.680 --> 50:38.640
The issue of what is innate, it doesn't, it seems to me there's not much point putting

50:38.640 --> 50:43.640
in stuff innately if you can learn it quickly.

50:43.640 --> 50:50.680
So for example the ability to move and get 3D structure from motion, that's actually

50:50.680 --> 50:55.200
very easy to learn so I don't believe that's innate even though a child can do it at like

50:55.200 --> 50:57.560
two days.

50:57.560 --> 51:05.280
You show them a sort of W made of paper and you rotate it in a consistent way and they

51:05.280 --> 51:09.120
get bored and as soon as you rotate it in a way, move it in a way that's not consistent

51:09.120 --> 51:14.320
with the rotation, the interest perks up.

51:14.320 --> 51:16.240
But I think they can learn it in two days.

51:16.240 --> 51:19.280
It's really easy to learn.

51:19.280 --> 51:21.280
Okay interesting.

51:21.280 --> 51:29.760
Now I want to ask you, I know partially what your answer is going to be but when I remember

51:29.760 --> 51:36.920
long ago you told me that one of your career goals, at least earlier in your career, was

51:36.920 --> 51:43.320
to prove that everything that psychologists thought about the brain was wrong.

51:43.320 --> 51:48.880
And so my question is what was it that they had wrong, are they still getting it wrong

51:48.880 --> 51:52.880
and is neuroscience getting that same thing wrong?

51:52.880 --> 51:58.680
It was mainly to do with this conviction that psychologists had partly based on Chomsky

51:58.680 --> 52:04.200
that there was an awful lot of innate stuff there and that you couldn't just learn a whole

52:04.200 --> 52:06.760
bunch of representations from scratch.

52:06.760 --> 52:11.800
There was this innate framework and there was a little bit of tuning of this innate knowledge

52:11.800 --> 52:16.920
and that's what learning was and that's just, I think that's a completely wrong headed approach.

52:16.920 --> 52:23.640
In fact, I want to go the other way and I want to say the stuff in the brain that's innate

52:23.640 --> 52:26.320
wasn't discovered by evolution.

52:26.320 --> 52:31.960
The stuff in the brain that's innate was discovered by learning.

52:31.960 --> 52:34.600
Do I have time to do that digression?

52:34.600 --> 52:35.600
Yeah.

52:35.600 --> 52:46.040
Okay, so imagine we have a little neural network and it's got 20 connections in it and each

52:46.040 --> 52:50.000
of those connections has a switch that could be on or off.

52:50.000 --> 52:52.560
So it can let stuff through or not.

52:52.560 --> 52:54.800
So you've got to make 20 binary decisions.

52:54.800 --> 52:59.640
So your chance of making them by chance is one in a million and making the correct decisions.

52:59.640 --> 53:06.040
Now this little neural network circuit is a mating circuit and so the neural net goes

53:06.040 --> 53:12.920
into a singles bar and it runs this circuit and if it's got the connections right it has

53:12.920 --> 53:17.680
lots of offspring and if it hasn't got the connections right it doesn't have offspring

53:17.680 --> 53:20.400
or doesn't have so many offspring.

53:20.400 --> 53:27.480
Okay, so let's start off with the connections being, if they were just kind of random and

53:27.480 --> 53:32.640
you did mutation, what would happen is you'd have to build about a million organisms before

53:32.640 --> 53:37.680
you got a good one and if you had sexual combination of the organisms, let's have a really simple

53:37.680 --> 53:43.080
biology in which each connection has its own gene and this gene has two alleles for

53:43.080 --> 53:45.960
on and off, okay?

53:45.960 --> 53:52.160
If you do mating now, you might have an organism that got all 20 connections right and it mates

53:52.160 --> 53:55.840
with one that has a few wrong and it gets a few wrong ones and now it's wiped out.

53:55.840 --> 53:58.920
It doesn't have lots of offspring anymore, that's it.

53:58.920 --> 54:06.080
So it seems like a complete disaster and it would obviously take you at least a million

54:06.080 --> 54:10.200
organisms to expect to get a good one even if you have pathogenesis where you didn't

54:10.200 --> 54:12.200
have sexual reproduction.

54:12.200 --> 54:18.560
Now I will show you how to build a good organism in only 30,000 tries and the way you do it

54:18.560 --> 54:24.920
is this, you, for each connection you have three alleles, you have turn the connection

54:24.920 --> 54:33.400
on genetically, turn the connection off genetically or leave the connection to learning, okay?

54:33.400 --> 54:35.760
So that's the third alley.

54:35.760 --> 54:43.240
And now you start off with a population in which about half of the connections are genetically

54:43.240 --> 54:48.360
determined and the other half are left to learning.

54:48.360 --> 54:52.760
So that's 10 connections that are definitely determined, so there's a 1 in 1,000 chance

54:52.760 --> 54:56.440
that you'll luck out genetically, you'll get those 10 right.

54:56.440 --> 55:00.080
And then during the organism's lifetime, let's have a really dumb learning algorithm where

55:00.080 --> 55:04.320
it just randomly fit like the one I talked about, it randomly fiddles with the connections,

55:04.320 --> 55:08.440
just randomly flips connections of the 10 that are left to learning.

55:08.440 --> 55:12.960
And it'll take you to about 1,000 trials and it'll get the combination right.

55:12.960 --> 55:17.440
But the point is it can do those trials without building a whole organism, it can just go

55:17.440 --> 55:23.560
into the singles bar and sort of fiddle around a bit with its connections and it'll bang.

55:23.560 --> 55:31.560
So what we've done is we've replaced a million trials of evolution, building a million organisms

55:31.560 --> 55:38.200
with built 1,000 organisms and then let's build 30,000 organisms, just to be safe.

55:38.200 --> 55:46.120
And then each of those fiddles around with its connections and it'll do this search.

55:46.120 --> 55:49.840
The whole search is the same, you have to try a million combinations, but the way you

55:49.840 --> 55:55.200
get the million combinations is 1,000 organisms each does 1,000 learning trials and so almost

55:55.200 --> 55:57.920
all the work is done by learning.

55:57.920 --> 56:05.080
Now if genetically an organism happens to have more things set, like it's got 12 of

56:05.080 --> 56:08.120
them set right, it'll learn faster.

56:08.120 --> 56:15.720
And so this genetic pressure for, if you mate organisms now, this genetic pressure to get

56:15.720 --> 56:19.440
more and more of these alleles set genetically.

56:19.440 --> 56:25.000
But the pressure only comes because the learning can get all 20 set right so this thing can

56:25.000 --> 56:27.000
mate and have lots of offspring.

56:27.000 --> 56:32.840
So the fact that the learning can find a solution creates genetic pressure to hardwire these

56:32.840 --> 56:34.840
things.

56:34.840 --> 56:41.360
So what's happening there is the search, a thousand things were done by evolution, a

56:41.360 --> 56:46.560
million minus a thousand things were done by learning and that created a landscape for

56:46.560 --> 56:51.440
evolution that allowed evolution to gradually hardwire in more and more, that these things

56:51.440 --> 56:54.480
were first found by learning.

56:54.480 --> 57:02.160
So I think a lot of the structure in the brain that's hardwired is first found by learning

57:02.160 --> 57:05.560
and gradually it gets backed up into the hardwiring.

57:05.560 --> 57:09.040
But to get the evolutionary pressure to say that's good, you have to be able to do the

57:09.040 --> 57:10.040
learning.

57:10.080 --> 57:13.280
First hardwired things, you'd never find anything that was good.

57:13.280 --> 57:14.280
Okay.

57:14.280 --> 57:15.280
Great.

57:15.280 --> 57:16.280
Thank you.

57:16.280 --> 57:17.280
Now...

57:17.280 --> 57:19.200
That's called the Baldwin effect, by the way.

57:19.200 --> 57:20.200
Yes.

57:20.200 --> 57:26.880
Yeah, it's called after a psychology professor at the University of Toronto in the 1890s called

57:26.880 --> 57:29.520
Baldwin who invented this effect.

57:29.520 --> 57:34.480
He didn't do any computer simulations though.

57:34.480 --> 57:38.960
So I want to do one follow-up question on that and then ask my final question before

57:38.960 --> 57:40.640
handing it to the audience.

57:40.640 --> 57:49.040
So my follow-up to that is, you know, I think one of the things that is unclear in terms

57:49.040 --> 57:55.400
of the success of deep learning is exactly how much it was purely the compute or some

57:55.400 --> 57:56.880
clever things.

57:56.880 --> 58:02.920
Now I've seen both cases argued and you today kind of suggested that it was just the compute.

58:02.920 --> 58:07.840
But I want to ask you about this following on your last point, which is that we know

58:08.080 --> 58:12.280
that if you build networks with particular architectures and with particular learning

58:12.280 --> 58:18.440
rules, you are effectively making learning faster if you do it right.

58:18.440 --> 58:23.280
And arguably a lot of the success of deep learning has actually been as a result of

58:23.280 --> 58:28.560
people thinking about good designs for their networks and good ways of making learning

58:28.560 --> 58:29.560
faster.

58:29.560 --> 58:30.560
Yep.

58:30.560 --> 58:38.880
So would you potentially say that we have seen that process that you just described

58:38.880 --> 58:43.960
actually occur with NAI over the last ten years of the learning kind of backing up into

58:43.960 --> 58:45.680
the hardwiring?

58:45.680 --> 58:49.200
I see.

58:49.200 --> 58:51.280
I need to think about that.

58:51.280 --> 58:54.520
What we've seen, I mean, Jan Lecaun invented convolutional neural nets.

58:54.520 --> 58:55.520
Right.

58:55.520 --> 58:56.520
Yes.

58:56.520 --> 58:57.520
For example.

58:57.520 --> 59:00.240
So the computer was invented in the 1980s, but computers weren't fast enough to really

59:00.240 --> 59:01.680
do a lot with them.

59:01.680 --> 59:04.880
So they were used for handwriting recognition and they were used for reading 10% of all the

59:04.880 --> 59:06.600
checks in America.

59:06.600 --> 59:09.120
But they didn't really take off.

59:09.120 --> 59:14.080
They really took off when the computer hardware came along to make them really efficient.

59:14.080 --> 59:18.720
So that's a case where the ideas were had first, but without the hardware they didn't

59:18.720 --> 59:19.720
work.

59:19.720 --> 59:20.720
You've obviously got to have both.

59:20.720 --> 59:21.720
Right.

59:21.720 --> 59:22.720
Yes.

59:22.720 --> 59:23.720
Great.

59:23.720 --> 59:24.720
Okay.

59:25.000 --> 59:29.160
So now my last question before I hand it to the audience is just, what do you see as

59:29.160 --> 59:33.920
being the future of the interaction between neuroscience and AI?

59:33.920 --> 59:39.680
Do you think that there is space for a sort of new cognitive science where we study general

59:39.680 --> 59:46.560
intelligence, but with brain-centric models rather than logic-based models?

59:46.560 --> 59:49.880
Or will we see the two streams depart over the next few decades?

59:49.880 --> 59:52.960
The way I like to think of it is we'd like to understand, if you'd like to understand

59:53.000 --> 59:59.520
how the brain does computation, you've got brains in your computation.

59:59.520 --> 01:00:02.800
And they look, like you said, they look pretty different to begin with because there's many

01:00:02.800 --> 01:00:05.000
different ways to do computation.

01:00:05.000 --> 01:00:10.680
And with a conventional digital computer, you can get it to pretend to be anything.

01:00:10.680 --> 01:00:14.400
So we're getting it to pretend to be some other kind of computer, an artificial neural

01:00:14.400 --> 01:00:15.800
net.

01:00:15.800 --> 01:00:21.680
And we'd like to sort of bridge this huge gap between brains and what we can simulate

01:00:21.680 --> 01:00:23.520
on computers.

01:00:23.520 --> 01:00:28.120
And so neuroscientists are sort of doing experiments.

01:00:28.120 --> 01:00:32.520
And good computation neuroscientists are sort of doing experiments to try and sort of see

01:00:32.520 --> 01:00:34.880
how you could do the computation.

01:00:34.880 --> 01:00:40.440
And I think of myself at this end as doing, simulating things with artificial neural nets

01:00:40.440 --> 01:00:42.000
to see how you can make it more biological.

01:00:42.000 --> 01:00:44.440
And we're trying to build a bridge.

01:00:44.440 --> 01:00:48.840
And so the computational neuroscientists, most of them are building from this end.

01:00:48.920 --> 01:00:50.560
I'm building from the other end.

01:00:50.560 --> 01:00:53.320
But obviously, if you want to build a bridge to somewhere, you need to look at where you're

01:00:53.320 --> 01:00:54.160
going.

01:00:54.160 --> 01:00:57.560
And so I'm trying to build a bridge that does computation more and more like the brain does

01:00:57.560 --> 01:01:02.520
it, or like, I guess the brain does it from what my neuroscientist friends tell me.

01:01:02.520 --> 01:01:05.360
And then there's conventional AI, which is trying to build a bridge like that.

01:01:05.360 --> 01:01:10.320
Great, OK, thank you.

01:01:10.320 --> 01:01:14.200
So now I'm going to open up to questions from the audience.

01:01:14.240 --> 01:01:19.120
Now, for this, we've got this kind of interesting system here.

01:01:19.120 --> 01:01:26.000
So rather than you putting up your hands and me selecting you, you can actually nominate

01:01:26.000 --> 01:01:30.640
yourself to ask a question by pressing on the button on your microphone.

01:01:30.640 --> 01:01:32.880
And it is a first come, first serve basis.

01:01:32.880 --> 01:01:35.240
So you're going to be queued up.

01:01:35.240 --> 01:01:39.320
And so you're now first on the queue.

01:01:39.320 --> 01:01:41.920
And by now, it's too late to be able to ask a question.

01:01:41.920 --> 01:01:42.800
Yes.

01:01:42.800 --> 01:01:44.840
And one last thing about that, though.

01:01:44.840 --> 01:01:49.520
When you are done asking your question, please turn off your microphone,

01:01:49.520 --> 01:01:54.560
because that will open up this slot for the next person in the queue.

01:01:54.560 --> 01:01:58.520
OK, go ahead.

01:01:58.520 --> 01:01:59.960
I've got a red light here, does that?

01:01:59.960 --> 01:02:02.760
Yeah, if your red light is flashing, that means you're on.

01:02:02.760 --> 01:02:03.760
You get to ask a question.

01:02:07.000 --> 01:02:11.160
If your red light is flashing, you're on.

01:02:11.160 --> 01:02:12.520
I've got a solid light, please.

01:02:12.560 --> 01:02:13.440
Oh, solid.

01:02:13.440 --> 01:02:14.360
Sorry.

01:02:14.360 --> 01:02:15.680
OK, I've got a solid light.

01:02:15.680 --> 01:02:17.000
You were faster.

01:02:17.000 --> 01:02:18.320
OK.

01:02:18.320 --> 01:02:22.880
So this is a Clifton suspension bridge analogy for your interest here.

01:02:22.880 --> 01:02:26.080
So you mentioned briefly, Hebbian synapses.

01:02:26.080 --> 01:02:29.840
As neuroscientists, we have a good understanding of how they work at a molecular level.

01:02:29.840 --> 01:02:34.640
So my question is, to what extent are the understanding of biological memory

01:02:34.640 --> 01:02:39.040
mechanisms, i.e. Hebbian synapses, implemented by AI for deep learning

01:02:39.040 --> 01:02:42.080
and the sorts of systems that you're describing?

01:02:42.120 --> 01:02:49.520
So at present, people don't use Hebbian synapses for most deep learning.

01:02:49.520 --> 01:02:51.360
They're using back propagation.

01:02:51.360 --> 01:02:56.000
So it's an error correction rule, as opposed to something where,

01:02:56.000 --> 01:02:59.120
if you just use it, it gets stronger.

01:02:59.120 --> 01:03:02.200
But if you want a short-term memory for things like transformers

01:03:02.200 --> 01:03:06.520
to remember a temporal context, just a simple Hebbian synapses is a good thing to have.

01:03:06.520 --> 01:03:10.480
Yeah, but Hebbian synapses can code memories in humans that can last a lifetime.

01:03:10.480 --> 01:03:13.280
So is this something that AI is working towards using,

01:03:13.280 --> 01:03:18.000
or are they just going to bypass Hebbian synapses and come up with something superior?

01:03:18.000 --> 01:03:23.600
OK. So if you think about what's been successful in the last few years,

01:03:23.600 --> 01:03:27.600
it's using error correction learning with either labeled data

01:03:27.600 --> 01:03:33.240
or trying to predict what comes next, and not Hebbian synapses.

01:03:33.680 --> 01:03:40.960
Now, people like me who sort of do this kind of learning

01:03:40.960 --> 01:03:44.200
but are interested in the brain know this isn't right.

01:03:44.200 --> 01:03:46.400
We're much more interested in unsupervised learning.

01:03:46.400 --> 01:03:49.560
We just can't make it work very well yet.

01:03:49.560 --> 01:03:55.720
And I would love to be able to get learning to work,

01:03:55.720 --> 01:03:58.600
as well as it does when you do back propagation,

01:03:58.600 --> 01:04:01.480
without using biologically implausible things.

01:04:02.480 --> 01:04:06.880
And one place we can do that is with temporary memories.

01:04:06.880 --> 01:04:10.120
So if you say synapses have a fast component,

01:04:10.120 --> 01:04:12.600
you can use Hebbian learning for that fast component,

01:04:12.600 --> 01:04:14.760
and that will actually help neural networks work better,

01:04:14.760 --> 01:04:18.760
even if you're using back propagation for the slow component.

01:04:18.760 --> 01:04:21.760
That didn't really answer your question, but you know, it filled the time.

01:04:21.760 --> 01:04:25.560
LAUGHTER

01:04:25.560 --> 01:04:27.560
Hello.

01:04:28.560 --> 01:04:33.560
I read in the Reinforcement Learning Book that dopamine is used

01:04:33.560 --> 01:04:36.560
as a reward prediction error signal.

01:04:36.560 --> 01:04:40.560
So I was wondering, do you see it used as a supervisory signal,

01:04:40.560 --> 01:04:43.560
like you mentioned earlier?

01:04:43.560 --> 01:04:46.560
OK, so for reinforcement learning,

01:04:46.560 --> 01:04:49.560
there is some lovely work done by Peter Diane,

01:04:49.560 --> 01:04:53.560
who is the theoretician, and some experimentalists,

01:04:53.560 --> 01:04:56.560
showing that the real data from neuroscience

01:04:56.560 --> 01:05:00.560
fits in with a theory that was started with Rich Sutton.

01:05:00.560 --> 01:05:06.560
And Peter Diane did the work of showing that dopamine

01:05:06.560 --> 01:05:09.560
corresponds to something in a particular learning algorithm.

01:05:09.560 --> 01:05:12.560
And it doesn't correspond to the reward,

01:05:12.560 --> 01:05:16.560
it corresponds to the difference between the reward you're expecting

01:05:16.560 --> 01:05:18.560
and the reward you get.

01:05:18.560 --> 01:05:22.560
So if you're a monkey and you're expecting a grape

01:05:22.560 --> 01:05:26.560
and I give you a piece of cucumber, that's negative reward,

01:05:26.560 --> 01:05:30.560
and that will be a big negative hit at the dopamine.

01:05:33.560 --> 01:05:38.560
So that's not the kind of learning that's been really successful so far.

01:05:38.560 --> 01:05:41.560
If you're willing to burn a lot of computer time,

01:05:41.560 --> 01:05:45.560
Reinforcement Learning will solve some problems,

01:05:45.560 --> 01:05:49.560
but it's not the kind of learning that's been most successful in AI.

01:05:52.560 --> 01:05:55.560
So the difference is in Reinforcement Learning,

01:05:55.560 --> 01:05:58.560
you get a single scalar, you get one number,

01:05:58.560 --> 01:06:00.560
whereas in error correction learning,

01:06:00.560 --> 01:06:03.560
you typically get a whole vector of numbers.

01:06:05.560 --> 01:06:07.560
Right here.

01:06:07.560 --> 01:06:12.560
So you mentioned that your goal, kind of the bridge analogy,

01:06:12.560 --> 01:06:16.560
is your goal is to go from computers and try to get to the brain.

01:06:16.560 --> 01:06:19.560
So let's just say that kind of makes sense to think,

01:06:19.560 --> 01:06:21.560
okay, let's get to more general AI,

01:06:21.560 --> 01:06:23.560
because I'd say humans are decently general.

01:06:23.560 --> 01:06:26.560
And neuroscientists are trying to get from the other bridge,

01:06:26.560 --> 01:06:28.560
the brain, to generally AI.

01:06:28.560 --> 01:06:32.560
So you have these two kind of debates,

01:06:32.560 --> 01:06:34.560
and this happens quite often,

01:06:34.560 --> 01:06:37.560
where is it correct to go from generally AI to the brain,

01:06:37.560 --> 01:06:39.560
first understand generally AI, then understand the brain,

01:06:39.560 --> 01:06:41.560
or brain to generally AI.

01:06:41.560 --> 01:06:44.560
And so what would you say is the most practical way

01:06:44.560 --> 01:06:48.560
to problematize generally AI?

01:06:50.560 --> 01:06:53.560
I don't like the phrase general AI.

01:06:53.560 --> 01:06:57.560
I don't think, if you want intelligent devices,

01:06:57.560 --> 01:07:02.560
I don't think you want to produce a sort of general purpose Android.

01:07:02.560 --> 01:07:05.560
I think you want to produce different devices

01:07:05.560 --> 01:07:08.560
that are smart in different ways.

01:07:08.560 --> 01:07:11.560
So basically if you want intelligent machines that do things,

01:07:11.560 --> 01:07:14.560
you have a vacuum cleaner and you have a backhoe.

01:07:14.560 --> 01:07:18.560
You don't try to make one thing that's a vacuum cleaner and a backhoe.

01:07:18.560 --> 01:07:20.560
It doesn't make sense.

01:07:20.560 --> 01:07:22.560
What about connecting them through

01:07:22.560 --> 01:07:26.560
kind of like different cognition areas in the brain?

01:07:26.560 --> 01:07:28.560
Yeah, but I think it's the same with cognition too.

01:07:28.560 --> 01:07:31.560
I think the neural net that does machine translation

01:07:31.560 --> 01:07:36.560
isn't the same neural net as does vision.

01:07:36.560 --> 01:07:42.560
I think, yeah, my guess is that people are thinking too much

01:07:42.560 --> 01:07:45.560
about making one neural net that does everything,

01:07:45.560 --> 01:07:50.560
and not thinking enough about making more modular neural nets

01:07:50.560 --> 01:07:52.560
that are good at different things.

01:07:52.560 --> 01:07:54.560
Some are more universal than others,

01:07:54.560 --> 01:07:56.560
but I think that's how progress has been made.

01:07:56.560 --> 01:07:58.560
That's how progress has been made so far.

01:07:58.560 --> 01:08:00.560
Not by the people talking about general AI.

01:08:00.560 --> 01:08:03.560
It's being made by people looking at saying,

01:08:03.560 --> 01:08:05.560
how can I get a neural net to do vision,

01:08:05.560 --> 01:08:08.560
or how can I get it to do machine translation?

01:08:08.560 --> 01:08:10.560
Thank you.

01:08:15.560 --> 01:08:19.560
Hi, I'm just wondering about the role of hierarchy in general.

01:08:19.560 --> 01:08:21.560
There are different types of hierarchy.

01:08:21.560 --> 01:08:23.560
There's different layers of neural network.

01:08:23.560 --> 01:08:26.560
As you mentioned, there's fast memory and slow memory.

01:08:26.560 --> 01:08:31.560
Then I wonder, are there more ways to add hierarchy to neural networks

01:08:31.560 --> 01:08:35.560
to make them more useful or emulate actual brain?

01:08:38.560 --> 01:08:40.560
Yes, probably.

01:08:41.560 --> 01:08:45.560
In vision, for example,

01:08:45.560 --> 01:08:50.560
you have multiple layers,

01:08:50.560 --> 01:08:53.560
that is, you have multiple cortical areas in the visual pathway.

01:08:53.560 --> 01:08:56.560
That's a very different kind of hierarchy

01:08:56.560 --> 01:09:00.560
from what you need for dealing with the sort of structural reality.

01:09:00.560 --> 01:09:03.560
In reality, there's the universe.

01:09:03.560 --> 01:09:06.560
There may be many of them, but that's one's enough.

01:09:06.560 --> 01:09:09.560
Then there's galaxies.

01:09:09.560 --> 01:09:12.560
Then there's probably things above galaxies.

01:09:12.560 --> 01:09:15.560
Then there's in the galaxies the stars,

01:09:15.560 --> 01:09:17.560
and then there's solar systems,

01:09:17.560 --> 01:09:20.560
and then there's planets, and so on.

01:09:20.560 --> 01:09:22.560
We can do that all the way down to atoms.

01:09:22.560 --> 01:09:24.560
You can imagine all that.

01:09:24.560 --> 01:09:27.560
You can represent all that in your brain.

01:09:27.560 --> 01:09:30.560
Clearly, what's going on is,

01:09:30.560 --> 01:09:33.560
out in the world, there's this hierarchy

01:09:33.560 --> 01:09:36.560
that goes over many, many orders of magnitude

01:09:36.560 --> 01:09:39.560
from the universe down to quarks,

01:09:39.560 --> 01:09:42.560
or whatever the smallest thing is now.

01:09:44.560 --> 01:09:47.560
You don't want that kind of hierarchy in your brain.

01:09:47.560 --> 01:09:50.560
What you've got in your brain is the ability to deal

01:09:50.560 --> 01:09:52.560
with a little window of hierarchy,

01:09:52.560 --> 01:09:55.560
where there's an object in its parts.

01:09:55.560 --> 01:09:57.560
To deal with the whole universe,

01:09:57.560 --> 01:09:59.560
you can take this window,

01:09:59.560 --> 01:10:02.560
and you can map at a scale of the universe,

01:10:02.560 --> 01:10:04.560
and there's the universe, and there's the galaxies,

01:10:04.560 --> 01:10:07.560
or there's the galaxies, and there's the stars,

01:10:07.560 --> 01:10:10.560
or there's the atom, and there's the electrons.

01:10:12.560 --> 01:10:15.560
You're using the same neural hardware,

01:10:15.560 --> 01:10:18.560
but mapping reality onto it differently.

01:10:18.560 --> 01:10:21.560
I think whenever we have to deal with anything complicated,

01:10:21.560 --> 01:10:23.560
we use hierarchies.

01:10:24.560 --> 01:10:27.560
The way the brain uses them is by varying the mapping

01:10:27.560 --> 01:10:31.560
from reality onto the brain.

01:10:31.560 --> 01:10:34.560
It can only operate with a small window on a hierarchy,

01:10:34.560 --> 01:10:36.560
which you can move up and down.

01:10:36.560 --> 01:10:39.560
Much like you only have a small region of high resolution,

01:10:39.560 --> 01:10:41.560
which you move around.

01:10:41.560 --> 01:10:42.560
So logarithm?

01:10:42.560 --> 01:10:43.560
Sorry?

01:10:43.560 --> 01:10:45.560
Like logarithm.

01:10:45.560 --> 01:10:47.560
What about logarithms?

01:10:47.560 --> 01:10:49.560
That's what you're talking about, right?

01:10:49.560 --> 01:10:52.560
Compressing a big range into something that is much manageable.

01:10:52.560 --> 01:10:54.560
I wasn't thinking of it like that.

01:10:54.560 --> 01:10:57.560
I was thinking of it as you have some fixed hardware,

01:10:57.560 --> 01:11:01.560
and when I'm thinking about the solar system,

01:11:01.560 --> 01:11:04.560
my fixed hardware couldn't possibly deal with the universe.

01:11:04.560 --> 01:11:07.560
That's much too big, and it couldn't possibly deal with an atom.

01:11:07.560 --> 01:11:09.560
That's much too small.

01:11:09.560 --> 01:11:12.560
But it's fine dealing with the sun and some planets,

01:11:12.560 --> 01:11:14.560
and maybe a moon or two.

01:11:15.560 --> 01:11:19.560
What I'm trying to get at is we need to make a big distinction

01:11:19.560 --> 01:11:23.560
between the hierarchy in the real world,

01:11:23.560 --> 01:11:26.560
hierarchical structures in the real world,

01:11:26.560 --> 01:11:30.560
and how we deal with them cognitively,

01:11:30.560 --> 01:11:32.560
where we use attention,

01:11:32.560 --> 01:11:35.560
and we only ever deal with a bit of the hierarchy at a time.

01:11:35.560 --> 01:11:39.560
That's not the same for, say, aspects of language,

01:11:39.560 --> 01:11:40.560
where you have...

01:11:40.560 --> 01:11:42.560
Notice that with vision,

01:11:42.560 --> 01:11:46.560
I can use the same neurons for representing the sun

01:11:46.560 --> 01:11:48.560
and for representing a nucleus.

01:11:49.560 --> 01:11:52.560
It's just an analogy, but it's the same neurons I'm using.

01:11:53.560 --> 01:11:56.560
Now, if I'm processing language,

01:11:56.560 --> 01:11:59.560
I've got things that find me phonemes

01:11:59.560 --> 01:12:01.560
and things that turn phonemes into words

01:12:01.560 --> 01:12:03.560
and things that turn words into sentences,

01:12:03.560 --> 01:12:04.560
and those...

01:12:04.560 --> 01:12:06.560
I can't move a window like that.

01:12:06.560 --> 01:12:07.560
That's a fixed hierarchy.

01:12:07.560 --> 01:12:09.560
There's phonemes, and there's words,

01:12:09.560 --> 01:12:11.560
and there's phrases, and there's sentences,

01:12:11.560 --> 01:12:13.560
and that's all sort of fixed in the brain.

01:12:13.560 --> 01:12:15.560
That's not a flexible matter.

01:12:15.560 --> 01:12:17.560
You can't kind of move the sentences down

01:12:17.560 --> 01:12:19.560
so they're where the words were,

01:12:19.560 --> 01:12:21.560
move the words down so they're where the phonemes were.

01:12:21.560 --> 01:12:23.560
That doesn't work.

01:12:23.560 --> 01:12:26.560
So there's some hierarchies that really do relate

01:12:26.560 --> 01:12:30.560
to sets of neurons in the brain.

01:12:30.560 --> 01:12:33.560
They're like the layers in the connections models.

01:12:33.560 --> 01:12:35.560
There's other hierarchies,

01:12:35.560 --> 01:12:37.560
like the whole spatial structure of the universe,

01:12:37.560 --> 01:12:40.560
where what's in the brain is a window

01:12:40.560 --> 01:12:42.560
you move over that hierarchy.

01:12:42.560 --> 01:12:44.560
Thank you.

01:12:47.560 --> 01:12:50.560
Thanks, Dr. Hinton, for excellent talk

01:12:50.560 --> 01:12:53.560
and excellent ideas about the feasibility of back propagation.

01:12:53.560 --> 01:12:55.560
My question's maybe more boring

01:12:55.560 --> 01:12:58.560
about the statistical comments that you made.

01:12:58.560 --> 01:13:00.560
Is that Dale?

01:13:00.560 --> 01:13:01.560
Pardon me?

01:13:01.560 --> 01:13:02.560
Are you Dale?

01:13:02.560 --> 01:13:04.560
No, I'm Kyle.

01:13:06.560 --> 01:13:08.560
You sound like Dale Shermans.

01:13:08.560 --> 01:13:11.560
I'm at the University of Alberta.

01:13:12.560 --> 01:13:13.560
Hi.

01:13:13.560 --> 01:13:15.560
Well, that's a good instance,

01:13:15.560 --> 01:13:17.560
that you sound just like Dale Shermans

01:13:17.560 --> 01:13:19.560
and you're at the University of Alberta.

01:13:19.560 --> 01:13:23.560
Are you a student of Dale's?

01:13:23.560 --> 01:13:24.560
No.

01:13:27.560 --> 01:13:29.560
I think I've only met a voice.

01:13:29.560 --> 01:13:31.560
If you're a student of Dale's, I need to watch out

01:13:31.560 --> 01:13:33.560
because it's going to be a very tricky question.

01:13:33.560 --> 01:13:38.560
The question is that I was trained with this intuition

01:13:38.560 --> 01:13:40.560
that you can't overparameterize your models,

01:13:40.560 --> 01:13:44.560
that if you're trying to fit a line that you need two points,

01:13:44.560 --> 01:13:46.560
if you're trying to fit a curve you need three and so on

01:13:46.560 --> 01:13:48.560
and that scales up and you should always have

01:13:48.560 --> 01:13:50.560
a little bit less data points.

01:13:50.560 --> 01:13:54.560
I know that you have shown clearly

01:13:54.560 --> 01:13:56.560
and the field has shown that that's not true.

01:13:56.560 --> 01:13:59.560
What were the statisticians getting wrong

01:13:59.560 --> 01:14:01.560
in their logic to be convinced?

01:14:01.560 --> 01:14:03.560
It's to do with regularization,

01:14:03.560 --> 01:14:05.560
that you need it to be highly regularized.

01:14:05.560 --> 01:14:07.560
But first of all, I'll show you

01:14:07.560 --> 01:14:13.560
that if you want to fit two data points,

01:14:13.560 --> 01:14:15.560
well, let's take three.

01:14:15.560 --> 01:14:17.560
If you want to fit three data points,

01:14:17.560 --> 01:14:21.560
you would have told me you want a polynomial

01:14:21.560 --> 01:14:23.560
with only three degrees of freedom,

01:14:23.560 --> 01:14:26.560
so you want a constant and a slope and a curvature

01:14:26.560 --> 01:14:29.560
and that's all you can afford with three data points.

01:14:29.560 --> 01:14:31.560
That's wrong.

01:14:31.560 --> 01:14:33.560
Now, this is where we need a pen.

01:14:33.560 --> 01:14:35.560
Oh, sorry.

01:14:37.560 --> 01:14:38.560
They don't work.

01:14:38.560 --> 01:14:40.560
I tried them all and none of them work.

01:14:41.560 --> 01:14:42.560
Okay, did they work?

01:14:42.560 --> 01:14:44.560
Oh, well done.

01:14:44.560 --> 01:14:46.560
Extra points.

01:14:48.560 --> 01:14:50.560
Okay, yes.

01:14:50.560 --> 01:14:52.560
Can you see that?

01:14:52.560 --> 01:14:53.560
Yeah.

01:14:53.560 --> 01:14:55.560
Okay, and we're going to have three data points.

01:15:03.560 --> 01:15:07.560
And actually, if you're a statistician,

01:15:07.560 --> 01:15:09.560
you'd probably say for three data points

01:15:09.560 --> 01:15:12.560
you'd probably ought to fit a straight line like this.

01:15:12.560 --> 01:15:15.560
Because I could fit a parabola

01:15:15.560 --> 01:15:17.560
and the parabola would fit exactly

01:15:17.560 --> 01:15:20.560
and that's a bit suspicious.

01:15:20.560 --> 01:15:23.560
In other words, the parabola fits exactly,

01:15:23.560 --> 01:15:29.560
but do you really believe that if you were to ask

01:15:29.560 --> 01:15:31.560
when x is zero, what's the value of y?

01:15:31.560 --> 01:15:33.560
Do you really believe that value for y?

01:15:33.560 --> 01:15:36.560
Because a straight line is far more conservative.

01:15:36.560 --> 01:15:38.560
So a statistician would probably say

01:15:38.560 --> 01:15:40.560
fit a straight line.

01:15:40.560 --> 01:15:43.560
However, that would be a frequentist statistician.

01:15:43.560 --> 01:15:46.560
If you took a Bayesian statistician,

01:15:46.560 --> 01:15:49.560
and this is in Chris Fisher's machine learning textbook,

01:15:49.560 --> 01:15:51.560
there's a nice picture of it, I think, somewhere.

01:15:51.560 --> 01:15:53.560
Think it's that book.

01:15:53.560 --> 01:15:55.560
A Bayesian statistician would say,

01:15:55.560 --> 01:15:59.560
okay, let's try fitting fifth-order polynomials.

01:15:59.560 --> 01:16:02.560
And fifth-order polynomials,

01:16:02.560 --> 01:16:05.560
we might even fit ones that don't exactly go through the data,

01:16:05.560 --> 01:16:07.560
but for now let's make them go through the data.

01:16:07.560 --> 01:16:09.560
So we fit a fifth-order polynomial

01:16:09.560 --> 01:16:12.560
that goes kind of...

01:16:15.560 --> 01:16:16.560
One, two, three...

01:16:16.560 --> 01:16:18.560
Well, you know, some order.

01:16:18.560 --> 01:16:20.560
And we fit another one.

01:16:20.560 --> 01:16:23.560
Oh, that didn't go through the data.

01:16:27.560 --> 01:16:29.560
And we keep fitting these guys,

01:16:29.560 --> 01:16:31.560
and we fit a gazillion of them.

01:16:31.560 --> 01:16:34.560
And what you see at the end is that

01:16:34.560 --> 01:16:37.560
these gazillion ones, in between the data points,

01:16:37.560 --> 01:16:39.560
they're kind of all over the place,

01:16:39.560 --> 01:16:43.560
and their average is in a sensible place like here,

01:16:43.560 --> 01:16:45.560
but their variance is big.

01:16:45.560 --> 01:16:47.560
And what they're telling you is,

01:16:47.560 --> 01:16:49.560
if you give me this x-coordinate,

01:16:49.560 --> 01:16:51.560
I'm rather uncertain about this y-coordinate,

01:16:51.560 --> 01:16:53.560
but this is a good bet.

01:16:53.560 --> 01:16:55.560
And similarly here,

01:16:55.560 --> 01:16:57.560
and if you go out here,

01:16:57.560 --> 01:16:59.560
these polynomials are just all over the place,

01:16:59.560 --> 01:17:01.560
and they'll tell you,

01:17:01.560 --> 01:17:03.560
if you give me this x-value,

01:17:03.560 --> 01:17:06.560
then it could be pretty much anything.

01:17:06.560 --> 01:17:08.560
That's not a bad bet,

01:17:08.560 --> 01:17:10.560
but it could be pretty much anything.

01:17:10.560 --> 01:17:12.560
And that's a much better answer

01:17:12.560 --> 01:17:14.560
than you get from a straight line.

01:17:14.560 --> 01:17:16.560
So by fitting a very large number

01:17:16.560 --> 01:17:18.560
of different polynomials,

01:17:18.560 --> 01:17:20.560
and then averaging,

01:17:20.560 --> 01:17:22.560
you get good, mean answers,

01:17:22.560 --> 01:17:24.560
and you also get a sense of the variance.

01:17:24.560 --> 01:17:26.560
Now, Drop-Out is doing something like that.

01:17:26.560 --> 01:17:28.560
Yes, and that's brilliant.

01:17:28.560 --> 01:17:31.560
Thank you for coming up with Drop-Out.

01:17:32.560 --> 01:17:35.560
Many of us here

01:17:35.560 --> 01:17:38.560
are working in a regime of sparse data,

01:17:38.560 --> 01:17:40.560
and so we have a couple channels,

01:17:40.560 --> 01:17:42.560
a couple signals, a couple voxels,

01:17:42.560 --> 01:17:46.560
and you've convinced us that we need more,

01:17:46.560 --> 01:17:50.560
but is there a way forward in AI

01:17:50.560 --> 01:17:53.560
that can manage with more sparse data,

01:17:53.560 --> 01:17:55.560
or is this the only regime

01:17:55.560 --> 01:17:57.560
that's going to be able to make success?

01:17:57.560 --> 01:18:00.560
So the really big successes

01:18:00.560 --> 01:18:02.560
have been on big databases,

01:18:02.560 --> 01:18:05.560
and I think we should be using

01:18:05.560 --> 01:18:07.560
even bigger models,

01:18:07.560 --> 01:18:10.560
but you can't get away from the fact that actually,

01:18:10.560 --> 01:18:12.560
if you're going to have something

01:18:12.560 --> 01:18:14.560
that starts off random

01:18:14.560 --> 01:18:16.560
and sucks all its knowledge from the data,

01:18:16.560 --> 01:18:17.560
you'd better have enough data

01:18:17.560 --> 01:18:19.560
to suck all that knowledge from.

01:18:19.560 --> 01:18:21.560
The bigger your model, the better,

01:18:21.560 --> 01:18:22.560
if you regularize it,

01:18:22.560 --> 01:18:24.560
but you still need a lot of data.

01:18:24.560 --> 01:18:26.560
So the way you should think about it is this.

01:18:26.560 --> 01:18:28.560
If you've got 100,000 data points,

01:18:28.560 --> 01:18:30.560
that's small.

01:18:30.560 --> 01:18:32.560
I know that's very depressing

01:18:32.560 --> 01:18:33.560
if you're a neuroscientist.

01:18:33.560 --> 01:18:34.560
Well, it's not depressing.

01:18:34.560 --> 01:18:36.560
It seems impossible.

01:18:36.560 --> 01:18:38.560
If you want to personalize medicine

01:18:38.560 --> 01:18:39.560
for one individual

01:18:39.560 --> 01:18:41.560
and you want to train a model

01:18:41.560 --> 01:18:43.560
on their data from their brain,

01:18:43.560 --> 01:18:45.560
it seems like there's going to be a disconnect

01:18:45.560 --> 01:18:47.560
between what these models can do

01:18:47.560 --> 01:18:49.560
and how they might help someone in the future.

01:18:49.560 --> 01:18:50.560
Yes and no.

01:18:50.560 --> 01:18:52.560
If I train a model

01:18:52.560 --> 01:18:54.560
on a very large number of people

01:18:54.560 --> 01:18:56.560
and then apply that model to one person,

01:18:56.560 --> 01:18:58.560
that's the form of personalize medicine

01:18:58.560 --> 01:19:00.560
that really works.

01:19:00.560 --> 01:19:02.560
Great, thanks.

01:19:11.560 --> 01:19:13.560
Oh, and say hi to Dale for me.

01:19:18.560 --> 01:19:21.560
Thank you for the presentation.

01:19:21.560 --> 01:19:23.560
My last question is about this dropout.

01:19:23.560 --> 01:19:25.560
The thing is that you randomly just drop

01:19:25.560 --> 01:19:27.560
some parts of the network

01:19:27.560 --> 01:19:29.560
and then you say, okay,

01:19:29.560 --> 01:19:31.560
that it works better so I would accept it.

01:19:31.560 --> 01:19:33.560
But do we have any, like, intuition

01:19:33.560 --> 01:19:35.560
why, for example, some parts of it work better

01:19:35.560 --> 01:19:37.560
or if we try to embed this, like,

01:19:37.560 --> 01:19:39.560
network into, like,

01:19:39.560 --> 01:19:41.560
is it more graph isomorphism?

01:19:41.560 --> 01:19:43.560
What are these two different graphs,

01:19:43.560 --> 01:19:45.560
different graphs that we took?

01:19:45.560 --> 01:19:47.560
Are there any similarities

01:19:47.560 --> 01:19:49.560
between them or just with randomly?

01:19:49.560 --> 01:19:51.560
I guess the problem with the randomness,

01:19:51.560 --> 01:19:53.560
I guess we are trying to put

01:19:53.560 --> 01:19:55.560
the burden of prediction

01:19:55.560 --> 01:19:57.560
on the random part of the computer desk.

01:19:57.560 --> 01:19:59.560
So I didn't hear

01:19:59.560 --> 01:20:01.560
the whole question,

01:20:01.560 --> 01:20:03.560
but certainly in dropout what we do

01:20:03.560 --> 01:20:05.560
is we randomly leave out units.

01:20:05.560 --> 01:20:07.560
Now you can also do block dropout.

01:20:07.560 --> 01:20:09.560
You can take groups of units

01:20:09.560 --> 01:20:11.560
and randomly leave out the groups.

01:20:11.560 --> 01:20:13.560
And what that does is it allows the units

01:20:13.560 --> 01:20:15.560
within a group to collaborate with one another,

01:20:15.560 --> 01:20:17.560
and then between groups

01:20:17.560 --> 01:20:19.560
they have to be fairly independent.

01:20:19.560 --> 01:20:21.560
And that's called block dropout

01:20:21.560 --> 01:20:23.560
but I didn't really hear

01:20:23.560 --> 01:20:25.560
the rest of your question.

01:20:25.560 --> 01:20:27.560
Okay.

01:20:27.560 --> 01:20:29.560
The question was about

01:20:29.560 --> 01:20:31.560
that, okay, imagine that you...

01:20:31.560 --> 01:20:33.560
Can you talk closer to the microphone

01:20:33.560 --> 01:20:35.560
because I'm partially dead?

01:20:35.560 --> 01:20:37.560
The question is that

01:20:37.560 --> 01:20:39.560
imagine that you have a dropout of 50%

01:20:39.560 --> 01:20:41.560
and you're trying to get rid

01:20:41.560 --> 01:20:43.560
of, like, 50% of your nodes

01:20:43.560 --> 01:20:45.560
and the nodes in the network.

01:20:45.560 --> 01:20:47.560
And the question is, okay,

01:20:47.560 --> 01:20:49.560
whether we have any similarity between the types

01:20:49.560 --> 01:20:51.560
if we do it iteratively,

01:20:51.560 --> 01:20:53.560
whether we would find any similarity

01:20:53.560 --> 01:20:55.560
on the structure of the network

01:20:55.560 --> 01:20:57.560
that would produce the best results

01:20:57.560 --> 01:20:59.560
and if it's so, whether it would correspond

01:20:59.560 --> 01:21:01.560
to something physical, like, for example,

01:21:01.560 --> 01:21:03.560
if you're doing a vision thing,

01:21:03.560 --> 01:21:05.560
whether it would correspond to something in brain or not, I guess.

01:21:05.560 --> 01:21:07.560
Yeah.

01:21:07.560 --> 01:21:09.560
Lots of people have thought about whether you can do better

01:21:09.560 --> 01:21:11.560
than random in dropout.

01:21:11.560 --> 01:21:13.560
And there's some work on that, like,

01:21:13.560 --> 01:21:15.560
block dropout that works, can work for some things.

01:21:15.560 --> 01:21:17.560
But I don't really have much to say

01:21:17.560 --> 01:21:19.560
about...

01:21:19.560 --> 01:21:21.560
I don't really know the answer to

01:21:21.560 --> 01:21:23.560
sort of, is there something much

01:21:23.560 --> 01:21:25.560
more sensible than dropout

01:21:25.560 --> 01:21:27.560
that's a lot more structured?

01:21:27.560 --> 01:21:29.560
There might well be, but I...

01:21:29.560 --> 01:21:31.560
Thank you.

01:21:31.560 --> 01:21:33.560
Okay, so with that, we're going to have to end.

01:21:33.560 --> 01:21:35.560
So please join me in

01:21:35.560 --> 01:21:37.560
thanking Jeff for...

01:21:37.560 --> 01:21:39.560
Thank you.

01:21:39.560 --> 01:21:41.560
APPLAUSE

01:21:41.560 --> 01:21:43.560
And

01:21:43.560 --> 01:21:45.560
I wanted also

01:21:45.560 --> 01:21:47.560
to thank Blake

01:21:47.560 --> 01:21:49.560
for hosting this event,

01:21:49.560 --> 01:21:51.560
and I felt the questions

01:21:51.560 --> 01:21:53.560
could have gone on all night,

01:21:53.560 --> 01:21:55.560
but the tip-off isn't half an hour,

01:21:55.560 --> 01:21:57.560
so some of us have to move on.

01:21:57.560 --> 01:21:59.560
So thank you, Blake.

01:21:59.560 --> 01:22:01.560
APPLAUSE

