{"text": " Hi, I think we're ready to start, so my name is Paul Franklin and I'm a Neuroscientist here at Sick Kids, and also I'm Program Chair for the Canadian Neuroscience Meeting that takes place in Toronto all this week. So the traditional curtain raiser for the Neuroscience Meeting, the CAN meeting, is the public lecture. And this year we decided to focus on the interface between neuroscience and AI. We did that for two reasons. The first reason is that AI, or Toronto, is one of the main hubs in the world for AI research. And the second reason is that it's also home to one of the true pioneers in this field, also known as the godfather of deep learning, Jeff Hinton. And so when we asked Jeff if he'd participate in this event, I think a year and a half ago we asked him and he said, yes, we were super excited. So at this point I want to hand over to Blake Richards, and Blake Richards is an associate professor at University of Toronto Scarborough, and he's going to host this evening's event. Blake? Thanks, Paul. So just as a brief introduction, I wanted to tell you a little bit more about Jeff. So you might be surprised to learn that the godfather of deep learning, associated mostly with AI, got his BA in experimental psychology from Cambridge originally. He then went on to do his PhD in artificial intelligence in Edinburgh, so he did get started relatively early. But throughout his early career, he really contributed to the first wave of what was known at the time as parallel distributed processing or connectionism, which really brought back into the fore the idea of using neural networks for both models of the mind and for artificial intelligence. Now Jeff got his first tenure track position at Carnegie Mellon in the 80s, but we were able to steal him away from them in the late 80s, which I understand was largely because of his ethical objections to DARPA funding. So once again, Canada's political bent has helped us in our research endeavors. Over the course of the 90s, Jeff continued to really push neural networks and machine learning forward. And sorry about that. In 98, he actually went to University College London to found the Gatsby computational neuroscience unit, and we might have lost him, but thankfully we pulled him back again. He came back to Toronto in 2001, where he became a university professor in 2006 and then an emeritus professor in 2014. Now I think that you all know that Jeff is a monumental figure within artificial intelligence and machine learning, and he's been critical to the founding of the Vector Institute here in Toronto and putting Toronto on the map for AI. And certainly he's had incredible recognitions of his work, most recently the Turing Award, which he shared with Joshua Bengio and Yann LeCun, as well as the Order of Canada. And he's a distinguished fellow of the Canadian Institute for Advanced Research, which I highlight because they were one of the people who continued to support neural networks throughout the time when it wasn't as faddish. But for all his successes and his technical endeavors, I think one of the things that's most important to understand about Jeff is the impact that he's had on other scientists. Jeff has really molded the career of so many people and changed the way that they think about things. When you look at the people who have been his graduate students or postdocs, it really is the who's who in artificial intelligence. It includes people like Max Welling, Yann LeCun, and you know, the phrase I used to describe it is, have you drunk Jeff's Kool-Aid? Because once you've drunk Jeff's Kool-Aid, there is no going back. You see neural networks, you see AI differently, and I would argue you also see neuroscience differently. And for me, my understanding of the brain has been largely shaped by Jeff and his work. But you know, we're at the point now where computer science has drunk Jeff's Kool-Aid. So he's got an H index of 145, and according to Google Scholar, his work has been cited 270,000 times, which is more than Einstein, Ramoni, Cajal, and Alan Tern combined. But that's largely from computer scientists. And if my prediction is correct, neuroscience 30, 40 years from now will also have drunk Jeff's Kool-Aid, and maybe you're going to get your first taste tonight. So with that, I hand you over to Jeffrey Hinton. So thank you very much, Blake. I can give you some more Kool-Aid today. It's Kool-Aid produced by one of my former students, Ilya Sutskava. First, I want to tell you a little bit about the history of deep learning in AI. Can I just ask before I start, how many people here know what the back propagation algorithm is? Put your hands up. So some people don't. I'll explain it very quickly, and I'll explain it in such a way that you'll be able to explain to other people. So if you do know what it is, you follow the explanation from the point of view of how you explain it. Okay. There was a war between two paradigms for AI. There were people who thought that the essence of intelligence was reasoning, and logic is what does reasoning, so we should base artificial intelligence on taking strings of symbols and manipulating them to arrive at conclusions. And then there were other people who looked at the brain and said, no, no, intelligence is all about adapting connections in the brain to get smarter. And this war went on for a long time, and eventually, people who were trying to figure out how to change connections between fake neurons to make these networks smarter got to be able to do things that the people doing symbolic AI just couldn't do at all. And now there's a different way of getting a computer to do what you want. Instead of programming it, which is tedious, you just showed examples, and it figures it out. Now, of course, you have to write the program that figures it out, but that's just one program that will then do everything. And this is an example of what it can do. So the image, just think of the numbers. They're RGB values of pixels, and that's the input to the computer. Lots of values of pixels, just real numbers, saying how bright the red channel is. And you have to turn those numbers into a string of words that says a close-up of a child holding a stuffed animal. And imagine writing that program. Well, people in conventional AI had tried to write that program and they couldn't, partly because they didn't know how we did it. We still don't know how we do it, but we can get artificial neural networks to do it now and do a pretty good job. My prediction is, within 10 years, if you go and get a CT scan, what will happen is a computer will look at the CT scan, and a computer will produce the written report that the radiologist currently produces. Radiologists don't like this idea. Okay. Here's a simplified model of a neuron. It's very simple. It gets some input, which is just the activity on the input lines times the weights, adds it all up. That's called the depolarization. And then it gives an output that's proportional to how much input it gets as long as it gets enough input. And so, to begin with, we won't have spiking neurons. These are just going to be neurons that send real values in just the way neurons don't. We're going to make networks of them by hooking them up into layers. And you could put some pixels on the input neurons. Look. They're the input neurons. And you go forwards through the net until you get outputs. And then you compare those outputs with what you ought to have got, so you have to know what the right answer is. And what we'd like to do is train the weights, these red and green dots, so that it gives the right output. Now, I'm going to show you a way of training the weights that everybody can understand, and everybody is thought of, basically. What you do is you start with random weights, you show it some inputs, you measure how well it does, then you change one weight a tiny bit. So I take that weight there, and I just change it a tiny bit, and then I show it the same inputs again and see if it does better or worse. If it does better, I keep the change. If it does worse, maybe I keep the change in the opposite direction. That's an easy algorithm to understand. And that algorithm works. It's just incredibly slow. You have to show it lots of examples, change your weight, and then show it lots more examples, change another weight. And every weight has to be changed many times. So if you use calculus, you can go millions of times faster. So the trick of this algorithm, the sort of mutation algorithm, is you have to measure the effect of the weight change on the performance. But you don't really need to measure it, because when I change one of these weights, the effect that it has on the output is determined by the network. It just depends on the other weights in the network. It's not like normal evolution, where the effect of a gene depends on the environment you're in. This is all kind of internal to the brain. And so changing one of these weights has an effect that's predictable here. So I ought to be able to predict how changing the weight will help get the right output. And so what back propagation does is basically says, I'm going to compute using an algorithm, the details of which I won't tell you, and compute for every weight, all at the same time, how changing that weight would improve the output. And then I'm going to change all the weights a little bit. So every weight changes in direction to improve the output, and the output improves quite a bit, and then I do it all again. Now that allows me to compute for every weight what direction I'd like to change it in. And the question is, should I, when I show examples, show all of the examples, and then update the weights, so should you live your whole life with the synapse strengths you're born with, then update your weights a little bit, then live your life again, and update the weights a little bit more, that doesn't seem very good, or should you take one case or a few cases, figure out how you'd like to update the weights, update them, and then take more cases. That's the online algorithm, and that's what we do. And the amazing thing is, it works. You can take one case at a time, or you can take small batch of cases, you update the weights, and these networks get better. And it's very surprising how well it works on big data sets. So for a long time, people thought, you're never going to be able to learn something complicated, like, for example, take a string of words in English, feed them into a neural net, and output a string of words in French that mean the same thing. You're never going to be able to do that if you start with a big neural net with just random weights. It's just asking too much for the neural net to organize itself so it can do transactions because you have to kind of understand what the English says. And people predicted this was completely impossible, but you'd have to put in lots of prior knowledge. Well, they were wrong. So in 2009, my students in Toronto showed that you could actually improve speech recognizers using these neural nets that had random weights. They were just trying to predict in a spectrogram which piece of which phoneme you were trying to say in the middle of the spectrogram. And then there was more to the system that wasn't neural nets. Now what we've done is we've got rid of all the stuff that wasn't neural nets, and now you can take sound waves coming in and you have transcriptions coming out, or even better, you have sound waves coming in and you have sound waves coming out in another language with the same accent. They can do that now. That's speech recognition done. And in 2012, two of my students took a big database of images and used essentially the same algorithm, the few clever tricks, to say what was in the image. Not a full caption, just the class of the most obvious object, and they did much better than conventional computer vision, which had been going for many years, and since then all the best recognizers have used neural nets. In 2011, you couldn't publish a paper out neural nets in the standard computer vision conference because they said they were rubbish. In 2014, you couldn't publish a paper that wasn't about neural nets. And in 2014, they did something that I didn't expect. This was done by people at Google, not me, and Joshua Benjo in his group in Montreal, particularly by a guy called Bardenao and Cho. They managed to get a neural net, so you feed in actually fragments of words in one language. You have 32,000 possible fragments. So the word the in English would be one of the fragments, but so with things like ing and un. And what comes out in another language is fragments of words in that other language, and it's a pretty good translation, and that's how Google now does translation. So it did translation better than symbolic AI. So what changed between 1986 and 2009? And it was basically computers got faster. That was the main change. Data sets got bigger. We developed some clever tricks, and we like to emphasize those, but it was really the computers getting faster and data sets getting bigger. But I'll emphasize the clever tricks nonetheless. And I can tell you about two clever tricks. I can tell you about transformers, and I can tell you about better ways of stopping neural networks from overfitting. But first I want to show you an example of what neural nets can do now. So a team at OpenAI took work on transformers that was originally done at Google. They developed it a little bit further, and they applied it to big neural nets that have 1.5 billion learnable connection strengths. So they're learning 1.5 billion numbers. That's the knowledge of the system. And they train it up on billions of words of English text, and all the net's trying to do is predict the next word. So what the net will do, or fragment of word, the net will give you probabilities for the next word. So if you give it some words, a lead-in, it'll give you probabilities for the next word. And once the net's trained, what you can do is you can look at those probabilities, and if it says there's a probability of 0.4 that the next word is the, you pick the with probability 9.4, and if it says fish with probability 0.01, you pick fish with probability 0.01. And so you just pick from its distribution, and then you tell the neural net, okay, the one I picked was the next word, what do you think comes after that? And this way you can get it to sort of reveal what it really believes about the world. So you're getting it to predict words one at a time, and every time it makes a prediction you say you were right, and it just gets more and more carried away. So they initiated it with some interesting text. And the question is, will the neural net then produce stuff that's sort of related to that? I mean, the first question is, will it produce English words? Will the words have decent syntax? Will it have any meaning? Will it be related to this? If you're really optimistic, you might say, will they sort of relate to the fundamental problem here, which is how these unicorns can speak English? Okay, so here goes. This is what the neural net produced. Now this was cherry-picked. This was one of their better examples. The neural net just made this up, right? It made up Dr. Jorge Perez, there is no such person at the University of La Paz, but it's pretty plausible because it's South America, and I believe La Paz has a university. Okay, so that's the first bit of what it made up, and it carries on and it gets better. The next bit sounds a bit like one of those fantasy games. So it's remembered about unicorns and herds of unicorns, right? So they walk up and there's this strange valley, and it's a very strange valley, and they found the herds of unicorns. And it has something about seeing them from the air and being able to touch them, which isn't quite right. So people in Symbolicae leap on this and say, you see, it doesn't understand. Well, sure, there's little bits that it doesn't get right. But notice, it's remembered that these unicorns have to speak English, and so it tells you about, you know, they spoke some fairly regular English. It doesn't know the difference between dialect and dialectic, but my kids don't know that either. In fact, I'm not sure I know. It's a tribute to unicorns to Argentina, even though Dr. Perez comes from Bolivia. And it actually understands about magic realism. So they're descendants of a lost race, and I love the bit at the end where it says, in South America, such incidents seem to be quite common. This has an ability to just make up something that fits your prejudices and sounds moderately plausible, like a certain president. And it finally gets to the point which is, if you really want to know whether these unicorns were used by breeding with these strange lost race of people, you ought to do a DNA test. Okay? It understands that. Okay. So that's what neural nets can do now. This was a neural net with 1.5 billion connections that was trained on Google's, actually, I withdraw that, 1.5 billion connections is trained on a lot of hardware. And we look at what it says, and we sort of laugh at how, you know, it's pretty good, but it hasn't got it quite right, but it's pretty good. Okay. What they've done now is they've trained a neural net with 50 billion connections on Google's latest cloud hardware, which is, it's like having several of the world's biggest supercomputers going for you for months. The net with 50 billion connections, I haven't seen any text from it yet, but my prediction is it's sitting around laughing at how cute what we produce is. Okay. So one thing about that net is it's clearly very well aware of the initial context. These unicorns in a valley that speak English, and it's remembering this initial context a long time later, and a recurrent neural net can't do that. A recurrent neural net would have forgotten about the initial stuff and wouldn't produce such good context-dependent stuff. So the way this works is the word comes in, the neural net activates some hidden units. That pattern of activity in the hidden units goes and compares itself with previous patterns, your point of view, with previous patterns at earlier times. And when it finds a pattern at an earlier time that's a bit similar, it says that we'll take advice from that previous hidden pattern about how to affect the next layer. And so actually a word comes in and how one pattern of activity in the bottom layer of the hidden neurons affects the next layer is dependent on what happened previously. Now it's dependent in quite a complicated way, and this seems very implausible for a brain because what's happening in the computer is you're storing all these activity patterns that are meant to be neural activity patterns or light neural activity patterns, and you're comparing, and this looks hopeless. But actually all you need to do is every time you have an activity pattern, and you use the outgoing weights to affect the next layer, just change the weight slightly with heavy and learning. So now what's going to happen is that weight matrix that comes out of that activity pattern is going to be modified slightly. Now when I get a new activity pattern, if the activity pattern is orthogonal to the previous activity pattern, then any modifications you made in the weight matrix due to that previous activity pattern won't make any difference. But if it lines up with the previous activity pattern, if it's similar, the modifications you made in the weights back there, the temporary modifications, will cause this new activity pattern to have a different effect here. So you'll get that long temporal context, and the way to store a long temporal context is not to keep copies of neural activity patterns, it's to take your weights and to have temporary changes to the weights, which I call fast weights. So you temporarily change them, and these changes decay over time, so you'll have a memory. So if you ask, where in your brain is your memory of what I said a few minutes ago? I'll ask the younger people this, because for the older people it's nowhere. But for the younger people, it's somewhere you can, if I were to say something I said a few minutes ago, like these big neural nets are now laughing at us, you remember I said that, where was that memory? I think it's in the temporary changes to the weights, because that's got much bigger capacity than activities of neurons, and you don't need to use up neurons just sitting there remembering. And those temporary changes don't need to be driven by back propagation, they can just be heavier. Okay, so I've tried to relate these wonderful nets that can make up stories with an idea about where short-term memory is in the brain. And now I'll talk about where the cortex can do back propagation. So neuroscientists, 20 years ago neuroscientists said don't be ridiculous, of course the brain can't do back propagation, and they'd interpret it very literally as sending signals backwards down the same axons and saying neurons don't do that, no thanks. But now we know that back propagation works really well for solving tough practical problems. So that's rather changed the balance, because when back propagation was just a theory of how you might get computers to learn something, and when it learns some simple things, it wasn't sort of imperative to understand whether the brain did it. But now we know that you can do all these things with back propagation. What's more, we know that back propagation is the right thing to do, but if you have a sensory pathway, and you want to take the early feature detectors so that their outputs are more helpful for making the right decision later on in the system, then what you really need to do is ask the question, how should I change the receptive field of this early detector so that what is output helps with the decision? And what you have to do is do back propagation to compute that, that's the efficient way to compute it. And I think it'd be crazy if the brain wasn't somehow doing this. So why do neuroscientists think it's impossible? Apart from silly objections like things don't go backwards down axons, at least not at the right speed. He wants me to update things. Oh, it's just died. I'm going to go out and present them out. I'm going to go back in to present them out. Okay, so here's some reasons why the brain can't do back propagation. The first reason is they say, well, it doesn't get the supervision signal. And they're imagining that the supervision signal is like you take a micro pipette and you put it into the infrotemporal cortex and you inject the right answer and the brain doesn't have anything like that, right? But actually, if you take that language model, it didn't need label data, it was just trying to predict the next word. So you can often use part of the input, maybe a future part of the input, or maybe a small part of an image as the right answer, and so you can get supervision signals easily. So there's no problem about supervision signals. The second reason is neurons don't send real-valued activities, they send spikes. And back propagation is using these real-valued activities so you can get nice, smooth derivatives. So back propagation can't possibly be what's going on in the brain. The third objection is, neurons have to send two signals. They have to send the activity forwards and they have to send error derivatives backwards. The signal they have to send backwards is, how sensitive am I to changes in my input? Or rather, if you change my input, how much does that help with the final answer? And the last thing is about neurons having reciprocal connections because you have to, when you send things backwards, if you use a different neuron, you have to use the same weight as the forward weight. I'm not going to tell you how you can overcome that, but you can easily. So supervision signals isn't really a problem, there's many ways to get a supervision signal. The simplest is predicting what comes next. Now the question of can neurons communicate real values? Well the first thing to notice about back propagation is, if you have very noisy estimates of the gradient, it works just as well. It's very, very tolerant of noise as long as it's unbiased noise. So for example, the signal you send forwards can be one bit, one stochastic bit, and the signal you send backwards can be two bits. If they have the right average value, if their expected values are correct, then they're just this expected value plus some noise, and the whole system still works fine. So in the brain, you have a neuron. At any instant, the neuron has an underlying firing rate, and it produces spikes, and for now let's just suppose it produces spikes according to a Poisson process. So it's probability of producing a spike in a small interval, which is the underlying firing rate. The question is, suppose we treated it as if it could send that underlying firing rate. When it sends a Poisson spike, it's just a very noisy version of the underlying firing rate. It's a one or a zero, but its expected value is the underlying firing rate. So how well do neural networks work if we send very noisy signals? So I'm going to have a statistics digression. If you do statistics 101, they tell you you shouldn't have more parameters in your model than you have data points. You really ought to have quite a few data points for each parameter. It turns out this is completely wrong. The Bayesians knew it was wrong, actually. The brain is not in the same regime of statistics 101. In the brain, you're fitting about 10 to the 14 parameters, and you have about 10 to the nine seconds. So even if you have sort of 10 experiences per second, so even if you take 100 milliseconds is the time for an experience, that's the kind of backward masking time, you have like 10,000 synapses per 100 milliseconds of your life. You're throwing a lot of parameters. So if your mother just kept saying, good, bad, good, bad, good, bad, she couldn't possibly provide enough information to learn all those 10 to the 14 parameters. And here's what they teach you wrong in statistics. Everybody knows that if you've got a given size model with a given number of parameters, the more data you have, the better you'll generalize. So for a given size of model, it's always better to have more data. In fact, the best thing you can do is get more data. Okay, but that doesn't mean that if you've got a fixed amount of data, you should make it look like a lot by having a small model. That's what they tell you in statistics 101. Okay? Big models are good if you regularize them, if you stop them doing crazy things. We can see that using a lot of parameters is good, that you can always win by having more parameters. And the way you do that is say, I'm going to have a committee. I'm going to learn lots of different little neural nets. If you give me more parameters, I'll learn more different neural nets. And then I'll average what they all say. And you'll always win. It's a sort of declining win, but if you have enough of them, you'll win by having more. So it's always better to have more parameters. It turns out if you have a fixed amount of data and you have enough computation power, the brain has, you should always use such a big model that the amount of data looks small. That's the regime you ought to be in for a fixed amount of data. That is, if you take the limit when the amount of data is fixed and you have unlimited computation and ask now how big would you like your model to be, you'd like your model to be much bigger than the data. Okay. That only works if you have a good regularizer. And I'm now going to tell you a very good regularizer called Dropout. So this is to use in neural networks where you have a lot more parameters than you have data points to train them on. And you could learn an ensemble of little models, and this is a way of learning an ensemble of many more models, but the models in the ensemble can share things with each other. So the idea is, if we just have one hidden layer in a neural net, we put the data in and each time we show the data vector, we randomly remove half the neurons. So we randomly get rid of half the neurons in your brain and only use the ones that remain. And it's a different subset we remove each time. Now when we do use a neuron, we use it with the same weights each time. So what you've got is if you've got h hidden neurons, you've got two to the h different subsets of neurons you might use. So you actually have two to the h different models, exponentially many models. Most of the models are never used. A few of the models will see one example, a small fraction of them will see one example. No models will see two examples. And yet they can learn because they're all sharing parameters. So this idea of sharing parameters in a neural network is very effective. So really you've got all these different models that are sharing parameters and you train it up and it generalizes really well. So I said that. So what we know is if you get rid of a fraction of the neurons each time and treat it as though they weren't there, it works really well. That's just a form of noise. And basically this is just an example of if you have a very big model and you add a lot of noise, the noise allows it to generalize well and it's better to have a big model with a lot of noise than to have a small model with no noise. And so what the brain wants, because it's got such a big model compared with the amount of data it operates on, it wants a lot of noise. And so now a Poisson neuron is kind of ideal. It's got a firing rate and now it adds a whole lot of noise to that and either sends a one or a zero and that actually makes it generalize much better. So the argument is the reason neurons don't send real values is they don't want to. They want to send things with a lot of noise in and that's making them generalize better. So that's not an argument against backpropagation. These dropout models are trained with backpropagation. So the random spikes are really just a way of adding noise to the signal to get better generalization. And now the last thing I'm going to address, I'm going to keep going till Blake stops me and I figure I've got about another five minutes before he gets really ratty. So the output of a neuron represents the presence of a feature in the current input. So it's obvious the same output can't represent the error derivative, right? You couldn't have a neuron that said to higher layers, this is the value of my feature and said to lower layers, this is my error derivative. It couldn't be done. So the neurons that go backwards need to be different neurons except that that's nonsense. So here's my claim. Joshua Benjo picked up on this later. I made this claim first in 2007. Actually, I made it first in the ground proposal. And I still believe this claim even though nobody's managed to make it work really well in the neural net yet. The idea is a neuron has a firing rate. That's the firing rate is its real output, which is communicated stochasticly by a spike. And that firing rate is actually changing over time, the underlying firing rate. And the rate of change of the firing rate is used to represent the error derivative. Now the nice thing about a rate of change is it can be positive or negative. So we can represent positive or negative derivatives without a neuron having to change the sort of signs of its synapses. And what it's representing, the derivative it's representing is the derivative of the error with respect to the input to the neuron. And that gets sent back to earlier neurons. And if I had enough time, I could show you a whole bunch of slides about how this will do back prop. But I want to show you one consequence of this. So that's, look, here we have a nice equation because it's got Leibniz on one side and Newton on the other side. That's Leibniz's notation for derivatives because they're not derivatives with respect to time. And this is Newton's notation because that was for derivatives with respect to time, okay? And what we're saying is the output of neuron J, which is yj, is the output of neuron J. But how fast that's changing over a short time interval is the error derivative. This is just a hypothesis, you understand? But it's true. Jay McClellan and I first used a version of this in 1988 before we knew about its back time dependent plasticity. I'm not sure it would be discovered then. Where you take, this is where I need the cursor. Yes, that one. Yes. You take some input. You send it to some hidden units, which send it to more hidden units by the green connections and sends it to more hidden units. It comes back to the input, so you reconstruct the input. And then you send it around again, not all the way around, but up to there and up to there and up to there using the right connections. And then the learning rule, which you'll notice doesn't involve explicit back propagation, is to say for these neurons, for example, I change the incoming weights by the activity of the presynaptic neuron down here times the difference between what I got on the green activation and on the red activation, first time round and second time round. So the rate of change of the activation of the neuron is what's used to communicate an error derivative. Now, unfortunately, this thing has the wrong sign, but later on we fixed that. And so here's a theory from 2007 that still hasn't been conclusively proved wrong. And it sort of works, but it doesn't work quite as well as we hoped, about how you could get a brain to do back propagation. What you first do is you learn a stack of autoencoders, that is you learn to get each layer to activate features in the layer above from which you can reconstruct the layer below. So you learn some features that can reconstruct this layer. Then you treat those features as data and learn some features that can reconstruct them. You build a big stack of autoencoders like that. Okay. Once you build the stack of autoencoders, then each layer can activity in a layer can reconstruct the activity in the layer below. And then you do two top down passes. You do a top down pass from the thing you predicted at the output. So you put in input, activity goes forward through the layers, you predict something at the output. And now you do a top down pass and you get reconstructed activities everywhere. And then you take your output and you change it to be more like the desired output. And now you do a top down pass and you'll get slightly different reconstructions. And the difference between those two reconstructions is actually the signal you need for back propagation. And so if you do that, the learning rule is that you should change a synapse by the pre-synaptic activity in the layer below times the rate of change of the activity in the layer above in the post-synaptic neuron. So it's a very simple learning rule. So let's change the weight in proportion to the pre-synaptic activity times the rate of change of the post-synaptic activity. Now it turns out if you're using spiking neurons, what that amounts to that are representing underlying firing rates that are changing, that amounts to a learning rule that looks like this. What you do is you take a pre-synaptic spike and you ask whether the post-synaptic spike came before or after it. Because what you're interested in is the rate of change of the post-synaptic firing rate around the time of the pre-synaptic spike. And if the post-synaptic spike occurs often just after it and seldom just before it, that suggests the firing rate is going up. And if the post-synaptic spike occurs often just before the pre-synaptic one and less often just after it, that means the firing rate of the post-synaptic neuron is going down. So if you want your learning rule to be the pre-synaptic activity, well, you'll only learn when you get a pre-synaptic spike. And then what you'll do is you'll say, did the post-synaptic spike occur afterwards or before? If it occurred afterwards, I should raise the weight and if it occurred before, I should lower the weight. And so your learning rule will look like this and this thing is actually a derivative filter. It's centered at zero and what this is really doing is measuring the rate of change of the post-synaptic firing rate. And of course it's sampling it. So you have a post-synaptic firing rate, there's these spike trains and are the other spikes getting closer together or further apart? Well this is a way to measure that. And of course you can do the learning on individual spikes and the learning rule would then be the implementation of this idea that the rate of change of the post-synaptic firing rate is the error signal. The learning rule would just be if the post-synaptic spike goes after the pre-synaptic one, increase the strength, otherwise decrease it and have that whole effect fall off as the spikes get further away because we're really only interested in the rate of change of the firing rate around the time of the pre-synaptic spike. Now there's one consequence of that which is that if you're going to use the rate of change of a neuron to represent not what the neuron is representing but to represent an error derivative, you've basically used up temporal derivatives for communicating error derivatives. So you cannot use temporal derivatives to communicate the temporal derivatives of what the neuron represents. So however I have a neuron that represents position, I can't use how fast that's changing to represent velocity and that's true of neurons. If you want to represent velocity, you have to have a neuron whose output represents velocity. You can't do it with the rate of change of a position neuron. If I kill the velocity neurons and keep the position neurons and I watch a car moving, the position neurons will change but I won't see any motion. Similarly, you can't use the rate of change of a velocity neuron to represent acceleration. Okay, so I think the fact that you can't use the rate of change of a representation to represent that that stuff in the world is changing is more evident since the board of the idea temporal derivatives of neurons are used up in representing error derivatives. So now I'll summarize. The main arguments against back propagation, the fact that they spent neurons and spikes rather than real numbers, well that's just because a lot of noise regularizes things. You can represent error derivatives as temporal derivatives so the same neuron can send temporal derivatives backwards, communicate those backwards and communicate activities forwards and the fact that in the brain you do get back to independent plasticity seems to be evidence in favor of that representation of error derivatives and now I'm done. Thank you Jeff for a great talk, sorry that was a Twitter joke, got it, anyway. So now what we're going to do is a brief Q&A between myself and Jeff and then after I've had my chance to ask some questions I'm going to open it up to you guys. Now I had originally sent Jeff a few questions which I'll rely on partially but his talk has made me want to ask a few others so I'm sorry I'm going to throw a few loops at you as well but let's start with some of the ones that I told you I wouldn't give you. There is something funny with my mic, I don't know if the AV guy is there, I just won't look down. Okay so the first question, yeah it's okay, I'm going off script anyway. The first question which I would like to ask just because it's something that I spend far too long arguing with people online is essentially you know so you're in the computer science department, you've come here, you've given us a talk that's largely about brains but many people seem to object to the idea that computers have anything to tell us about brains or indeed the idea that the brain is a computer despite the fact that neuroscientists often refer to computation in the brain. So my question is to you, is the brain a computer, I don't know I'll just hand that over to you first. Yes? Good, okay. And for the record I didn't tell him to say that if anyone from Twitter is watching. And B, can you just maybe give an intuitive understanding of why the answer is yes despite the fact that obviously our brains are very different from our laptops or our cell phones and stuff like that. So there's many ways you can do computation with physical stuff and you could get some silicon and make transistors and then run them at very high voltage much higher than needed to make them be digital and then you could if you wanted to represent a number you could have bits and you could and so on and you could create multipliers and adders and then you could put all that together and you could have some bits that tell you where in memory to find stuff and you could make a conventional computer or you could make little devices that have some input lines that are hardwired with input lines and you could have adaptive weights on the input lines. So early neural nets, Marvin Minsky made neural nets out of feedback controllers that were used in I think B-52 bombers or B-29 bombers or something, B-27 I don't know, some kind of bomber, it was America and so you can make computers in lots of different ways. When I was a kid I used to make computers by you take a six inch nail and you saw the head off and then you wrap copper wire around it and then you take a razor blade and you break it in half so that it's a nice flexible thing like this and you wrap a bit of copper wire around the razor blade and then when the current goes through the nail it'll make the razor blade go down and you'll make a contact so now you've got a relay and then you can put a bunch of those together and make logic gates. I never got more than about two logic gates that way but yeah you can make computers in lots of different ways and the brain is clearly made in a different way from the normal computers which has some different strengths and weaknesses so it's much slower but on the other hand you can make it much more parallel. It has one special property which I think is what makes us mortal which is that every brain is different so I can't take the weights from my brain and put them in Blake's brain and hope that it'll work because he just doesn't have connections in the same places. You've tried. Right. Well there's a way of doing it where you take the weights in my brain I turn it into strings of words. Blake absorbs these strings of words and puts different weights in his brain. It's pretty lucky all our brains are different because otherwise rich people will grab poor people's brains so they could live forever but quite okay. So I think I want to ask you then following on that what do you think about some of the quests to fully characterize the brains connectome do you think that is a scientifically worthwhile endeavor. Yes I do partly because some of the people doing it are my friends. Ignoring your loyalty to Sebastian. Not well in that case no. It seems to me it is very worth doing but you don't have to do that in order to begin to understand the principles. Very good. But for things like the retina which has a lot of hardwired stuff in it I think it's really important to do that. So that actually leads on to my next question. I wanted to ask you about hardwiring. So another thing that I think many people who study the brain find difficult about artificial neural networks as a model for the brain is that as you say you start with random weights and you train it on a lot of data and you get these things out. But we know that there are some pre-wired things in many brains so the classic examples are a horse can run pretty much right out of the womb but even within humans arguably there are some things that we find easier to learn than others. And so what do you think is the place for innate behavior within neural networks as a model of cognition? Okay so it used to be when I was a student if you were interested in language people would tell you that it was all innate and it just kind of matured as you got older and maybe you learned like 12 parameters that characterised your particular language whether it was subject verb object or some other way. In fact there's a nova that I saw, it was probably made about 20 years ago and it has all the leading linguists all of whom were educated by Chomsky and they look straight at the camera and they say there's a lot we don't know about language but one thing we know for sure is that it's not learned. So Chomsky had really good gulag. Yeah he did. But it's over because we know now if you want to translate you just learn it all. The number of linguists required to get a system that can turn a string of symbols in English into a string of symbols in French is roughly zero. I mean linguists involved in preparing the databases for training and making sure you get sort of a variety of grammatic structures and things but basically you don't need linguists, you just need data. So you don't need much innate structure. The issue of what is innate, it doesn't, it seems to me there's not much point putting in stuff innately if you can learn it quickly. So for example the ability to move and get 3D structure from motion, that's actually very easy to learn so I don't believe that's innate even though a child can do it at like two days. You show them a sort of W made of paper and you rotate it in a consistent way and they get bored and as soon as you rotate it in a way, move it in a way that's not consistent with the rotation, the interest perks up. But I think they can learn it in two days. It's really easy to learn. Okay interesting. Now I want to ask you, I know partially what your answer is going to be but when I remember long ago you told me that one of your career goals, at least earlier in your career, was to prove that everything that psychologists thought about the brain was wrong. And so my question is what was it that they had wrong, are they still getting it wrong and is neuroscience getting that same thing wrong? It was mainly to do with this conviction that psychologists had partly based on Chomsky that there was an awful lot of innate stuff there and that you couldn't just learn a whole bunch of representations from scratch. There was this innate framework and there was a little bit of tuning of this innate knowledge and that's what learning was and that's just, I think that's a completely wrong headed approach. In fact, I want to go the other way and I want to say the stuff in the brain that's innate wasn't discovered by evolution. The stuff in the brain that's innate was discovered by learning. Do I have time to do that digression? Yeah. Okay, so imagine we have a little neural network and it's got 20 connections in it and each of those connections has a switch that could be on or off. So it can let stuff through or not. So you've got to make 20 binary decisions. So your chance of making them by chance is one in a million and making the correct decisions. Now this little neural network circuit is a mating circuit and so the neural net goes into a singles bar and it runs this circuit and if it's got the connections right it has lots of offspring and if it hasn't got the connections right it doesn't have offspring or doesn't have so many offspring. Okay, so let's start off with the connections being, if they were just kind of random and you did mutation, what would happen is you'd have to build about a million organisms before you got a good one and if you had sexual combination of the organisms, let's have a really simple biology in which each connection has its own gene and this gene has two alleles for on and off, okay? If you do mating now, you might have an organism that got all 20 connections right and it mates with one that has a few wrong and it gets a few wrong ones and now it's wiped out. It doesn't have lots of offspring anymore, that's it. So it seems like a complete disaster and it would obviously take you at least a million organisms to expect to get a good one even if you have pathogenesis where you didn't have sexual reproduction. Now I will show you how to build a good organism in only 30,000 tries and the way you do it is this, you, for each connection you have three alleles, you have turn the connection on genetically, turn the connection off genetically or leave the connection to learning, okay? So that's the third alley. And now you start off with a population in which about half of the connections are genetically determined and the other half are left to learning. So that's 10 connections that are definitely determined, so there's a 1 in 1,000 chance that you'll luck out genetically, you'll get those 10 right. And then during the organism's lifetime, let's have a really dumb learning algorithm where it just randomly fit like the one I talked about, it randomly fiddles with the connections, just randomly flips connections of the 10 that are left to learning. And it'll take you to about 1,000 trials and it'll get the combination right. But the point is it can do those trials without building a whole organism, it can just go into the singles bar and sort of fiddle around a bit with its connections and it'll bang. So what we've done is we've replaced a million trials of evolution, building a million organisms with built 1,000 organisms and then let's build 30,000 organisms, just to be safe. And then each of those fiddles around with its connections and it'll do this search. The whole search is the same, you have to try a million combinations, but the way you get the million combinations is 1,000 organisms each does 1,000 learning trials and so almost all the work is done by learning. Now if genetically an organism happens to have more things set, like it's got 12 of them set right, it'll learn faster. And so this genetic pressure for, if you mate organisms now, this genetic pressure to get more and more of these alleles set genetically. But the pressure only comes because the learning can get all 20 set right so this thing can mate and have lots of offspring. So the fact that the learning can find a solution creates genetic pressure to hardwire these things. So what's happening there is the search, a thousand things were done by evolution, a million minus a thousand things were done by learning and that created a landscape for evolution that allowed evolution to gradually hardwire in more and more, that these things were first found by learning. So I think a lot of the structure in the brain that's hardwired is first found by learning and gradually it gets backed up into the hardwiring. But to get the evolutionary pressure to say that's good, you have to be able to do the learning. First hardwired things, you'd never find anything that was good. Okay. Great. Thank you. Now... That's called the Baldwin effect, by the way. Yes. Yeah, it's called after a psychology professor at the University of Toronto in the 1890s called Baldwin who invented this effect. He didn't do any computer simulations though. So I want to do one follow-up question on that and then ask my final question before handing it to the audience. So my follow-up to that is, you know, I think one of the things that is unclear in terms of the success of deep learning is exactly how much it was purely the compute or some clever things. Now I've seen both cases argued and you today kind of suggested that it was just the compute. But I want to ask you about this following on your last point, which is that we know that if you build networks with particular architectures and with particular learning rules, you are effectively making learning faster if you do it right. And arguably a lot of the success of deep learning has actually been as a result of people thinking about good designs for their networks and good ways of making learning faster. Yep. So would you potentially say that we have seen that process that you just described actually occur with NAI over the last ten years of the learning kind of backing up into the hardwiring? I see. I need to think about that. What we've seen, I mean, Jan Lecaun invented convolutional neural nets. Right. Yes. For example. So the computer was invented in the 1980s, but computers weren't fast enough to really do a lot with them. So they were used for handwriting recognition and they were used for reading 10% of all the checks in America. But they didn't really take off. They really took off when the computer hardware came along to make them really efficient. So that's a case where the ideas were had first, but without the hardware they didn't work. You've obviously got to have both. Right. Yes. Great. Okay. So now my last question before I hand it to the audience is just, what do you see as being the future of the interaction between neuroscience and AI? Do you think that there is space for a sort of new cognitive science where we study general intelligence, but with brain-centric models rather than logic-based models? Or will we see the two streams depart over the next few decades? The way I like to think of it is we'd like to understand, if you'd like to understand how the brain does computation, you've got brains in your computation. And they look, like you said, they look pretty different to begin with because there's many different ways to do computation. And with a conventional digital computer, you can get it to pretend to be anything. So we're getting it to pretend to be some other kind of computer, an artificial neural net. And we'd like to sort of bridge this huge gap between brains and what we can simulate on computers. And so neuroscientists are sort of doing experiments. And good computation neuroscientists are sort of doing experiments to try and sort of see how you could do the computation. And I think of myself at this end as doing, simulating things with artificial neural nets to see how you can make it more biological. And we're trying to build a bridge. And so the computational neuroscientists, most of them are building from this end. I'm building from the other end. But obviously, if you want to build a bridge to somewhere, you need to look at where you're going. And so I'm trying to build a bridge that does computation more and more like the brain does it, or like, I guess the brain does it from what my neuroscientist friends tell me. And then there's conventional AI, which is trying to build a bridge like that. Great, OK, thank you. So now I'm going to open up to questions from the audience. Now, for this, we've got this kind of interesting system here. So rather than you putting up your hands and me selecting you, you can actually nominate yourself to ask a question by pressing on the button on your microphone. And it is a first come, first serve basis. So you're going to be queued up. And so you're now first on the queue. And by now, it's too late to be able to ask a question. Yes. And one last thing about that, though. When you are done asking your question, please turn off your microphone, because that will open up this slot for the next person in the queue. OK, go ahead. I've got a red light here, does that? Yeah, if your red light is flashing, that means you're on. You get to ask a question. If your red light is flashing, you're on. I've got a solid light, please. Oh, solid. Sorry. OK, I've got a solid light. You were faster. OK. So this is a Clifton suspension bridge analogy for your interest here. So you mentioned briefly, Hebbian synapses. As neuroscientists, we have a good understanding of how they work at a molecular level. So my question is, to what extent are the understanding of biological memory mechanisms, i.e. Hebbian synapses, implemented by AI for deep learning and the sorts of systems that you're describing? So at present, people don't use Hebbian synapses for most deep learning. They're using back propagation. So it's an error correction rule, as opposed to something where, if you just use it, it gets stronger. But if you want a short-term memory for things like transformers to remember a temporal context, just a simple Hebbian synapses is a good thing to have. Yeah, but Hebbian synapses can code memories in humans that can last a lifetime. So is this something that AI is working towards using, or are they just going to bypass Hebbian synapses and come up with something superior? OK. So if you think about what's been successful in the last few years, it's using error correction learning with either labeled data or trying to predict what comes next, and not Hebbian synapses. Now, people like me who sort of do this kind of learning but are interested in the brain know this isn't right. We're much more interested in unsupervised learning. We just can't make it work very well yet. And I would love to be able to get learning to work, as well as it does when you do back propagation, without using biologically implausible things. And one place we can do that is with temporary memories. So if you say synapses have a fast component, you can use Hebbian learning for that fast component, and that will actually help neural networks work better, even if you're using back propagation for the slow component. That didn't really answer your question, but you know, it filled the time. LAUGHTER Hello. I read in the Reinforcement Learning Book that dopamine is used as a reward prediction error signal. So I was wondering, do you see it used as a supervisory signal, like you mentioned earlier? OK, so for reinforcement learning, there is some lovely work done by Peter Diane, who is the theoretician, and some experimentalists, showing that the real data from neuroscience fits in with a theory that was started with Rich Sutton. And Peter Diane did the work of showing that dopamine corresponds to something in a particular learning algorithm. And it doesn't correspond to the reward, it corresponds to the difference between the reward you're expecting and the reward you get. So if you're a monkey and you're expecting a grape and I give you a piece of cucumber, that's negative reward, and that will be a big negative hit at the dopamine. So that's not the kind of learning that's been really successful so far. If you're willing to burn a lot of computer time, Reinforcement Learning will solve some problems, but it's not the kind of learning that's been most successful in AI. So the difference is in Reinforcement Learning, you get a single scalar, you get one number, whereas in error correction learning, you typically get a whole vector of numbers. Right here. So you mentioned that your goal, kind of the bridge analogy, is your goal is to go from computers and try to get to the brain. So let's just say that kind of makes sense to think, okay, let's get to more general AI, because I'd say humans are decently general. And neuroscientists are trying to get from the other bridge, the brain, to generally AI. So you have these two kind of debates, and this happens quite often, where is it correct to go from generally AI to the brain, first understand generally AI, then understand the brain, or brain to generally AI. And so what would you say is the most practical way to problematize generally AI? I don't like the phrase general AI. I don't think, if you want intelligent devices, I don't think you want to produce a sort of general purpose Android. I think you want to produce different devices that are smart in different ways. So basically if you want intelligent machines that do things, you have a vacuum cleaner and you have a backhoe. You don't try to make one thing that's a vacuum cleaner and a backhoe. It doesn't make sense. What about connecting them through kind of like different cognition areas in the brain? Yeah, but I think it's the same with cognition too. I think the neural net that does machine translation isn't the same neural net as does vision. I think, yeah, my guess is that people are thinking too much about making one neural net that does everything, and not thinking enough about making more modular neural nets that are good at different things. Some are more universal than others, but I think that's how progress has been made. That's how progress has been made so far. Not by the people talking about general AI. It's being made by people looking at saying, how can I get a neural net to do vision, or how can I get it to do machine translation? Thank you. Hi, I'm just wondering about the role of hierarchy in general. There are different types of hierarchy. There's different layers of neural network. As you mentioned, there's fast memory and slow memory. Then I wonder, are there more ways to add hierarchy to neural networks to make them more useful or emulate actual brain? Yes, probably. In vision, for example, you have multiple layers, that is, you have multiple cortical areas in the visual pathway. That's a very different kind of hierarchy from what you need for dealing with the sort of structural reality. In reality, there's the universe. There may be many of them, but that's one's enough. Then there's galaxies. Then there's probably things above galaxies. Then there's in the galaxies the stars, and then there's solar systems, and then there's planets, and so on. We can do that all the way down to atoms. You can imagine all that. You can represent all that in your brain. Clearly, what's going on is, out in the world, there's this hierarchy that goes over many, many orders of magnitude from the universe down to quarks, or whatever the smallest thing is now. You don't want that kind of hierarchy in your brain. What you've got in your brain is the ability to deal with a little window of hierarchy, where there's an object in its parts. To deal with the whole universe, you can take this window, and you can map at a scale of the universe, and there's the universe, and there's the galaxies, or there's the galaxies, and there's the stars, or there's the atom, and there's the electrons. You're using the same neural hardware, but mapping reality onto it differently. I think whenever we have to deal with anything complicated, we use hierarchies. The way the brain uses them is by varying the mapping from reality onto the brain. It can only operate with a small window on a hierarchy, which you can move up and down. Much like you only have a small region of high resolution, which you move around. So logarithm? Sorry? Like logarithm. What about logarithms? That's what you're talking about, right? Compressing a big range into something that is much manageable. I wasn't thinking of it like that. I was thinking of it as you have some fixed hardware, and when I'm thinking about the solar system, my fixed hardware couldn't possibly deal with the universe. That's much too big, and it couldn't possibly deal with an atom. That's much too small. But it's fine dealing with the sun and some planets, and maybe a moon or two. What I'm trying to get at is we need to make a big distinction between the hierarchy in the real world, hierarchical structures in the real world, and how we deal with them cognitively, where we use attention, and we only ever deal with a bit of the hierarchy at a time. That's not the same for, say, aspects of language, where you have... Notice that with vision, I can use the same neurons for representing the sun and for representing a nucleus. It's just an analogy, but it's the same neurons I'm using. Now, if I'm processing language, I've got things that find me phonemes and things that turn phonemes into words and things that turn words into sentences, and those... I can't move a window like that. That's a fixed hierarchy. There's phonemes, and there's words, and there's phrases, and there's sentences, and that's all sort of fixed in the brain. That's not a flexible matter. You can't kind of move the sentences down so they're where the words were, move the words down so they're where the phonemes were. That doesn't work. So there's some hierarchies that really do relate to sets of neurons in the brain. They're like the layers in the connections models. There's other hierarchies, like the whole spatial structure of the universe, where what's in the brain is a window you move over that hierarchy. Thank you. Thanks, Dr. Hinton, for excellent talk and excellent ideas about the feasibility of back propagation. My question's maybe more boring about the statistical comments that you made. Is that Dale? Pardon me? Are you Dale? No, I'm Kyle. You sound like Dale Shermans. I'm at the University of Alberta. Hi. Well, that's a good instance, that you sound just like Dale Shermans and you're at the University of Alberta. Are you a student of Dale's? No. I think I've only met a voice. If you're a student of Dale's, I need to watch out because it's going to be a very tricky question. The question is that I was trained with this intuition that you can't overparameterize your models, that if you're trying to fit a line that you need two points, if you're trying to fit a curve you need three and so on and that scales up and you should always have a little bit less data points. I know that you have shown clearly and the field has shown that that's not true. What were the statisticians getting wrong in their logic to be convinced? It's to do with regularization, that you need it to be highly regularized. But first of all, I'll show you that if you want to fit two data points, well, let's take three. If you want to fit three data points, you would have told me you want a polynomial with only three degrees of freedom, so you want a constant and a slope and a curvature and that's all you can afford with three data points. That's wrong. Now, this is where we need a pen. Oh, sorry. They don't work. I tried them all and none of them work. Okay, did they work? Oh, well done. Extra points. Okay, yes. Can you see that? Yeah. Okay, and we're going to have three data points. And actually, if you're a statistician, you'd probably say for three data points you'd probably ought to fit a straight line like this. Because I could fit a parabola and the parabola would fit exactly and that's a bit suspicious. In other words, the parabola fits exactly, but do you really believe that if you were to ask when x is zero, what's the value of y? Do you really believe that value for y? Because a straight line is far more conservative. So a statistician would probably say fit a straight line. However, that would be a frequentist statistician. If you took a Bayesian statistician, and this is in Chris Fisher's machine learning textbook, there's a nice picture of it, I think, somewhere. Think it's that book. A Bayesian statistician would say, okay, let's try fitting fifth-order polynomials. And fifth-order polynomials, we might even fit ones that don't exactly go through the data, but for now let's make them go through the data. So we fit a fifth-order polynomial that goes kind of... One, two, three... Well, you know, some order. And we fit another one. Oh, that didn't go through the data. And we keep fitting these guys, and we fit a gazillion of them. And what you see at the end is that these gazillion ones, in between the data points, they're kind of all over the place, and their average is in a sensible place like here, but their variance is big. And what they're telling you is, if you give me this x-coordinate, I'm rather uncertain about this y-coordinate, but this is a good bet. And similarly here, and if you go out here, these polynomials are just all over the place, and they'll tell you, if you give me this x-value, then it could be pretty much anything. That's not a bad bet, but it could be pretty much anything. And that's a much better answer than you get from a straight line. So by fitting a very large number of different polynomials, and then averaging, you get good, mean answers, and you also get a sense of the variance. Now, Drop-Out is doing something like that. Yes, and that's brilliant. Thank you for coming up with Drop-Out. Many of us here are working in a regime of sparse data, and so we have a couple channels, a couple signals, a couple voxels, and you've convinced us that we need more, but is there a way forward in AI that can manage with more sparse data, or is this the only regime that's going to be able to make success? So the really big successes have been on big databases, and I think we should be using even bigger models, but you can't get away from the fact that actually, if you're going to have something that starts off random and sucks all its knowledge from the data, you'd better have enough data to suck all that knowledge from. The bigger your model, the better, if you regularize it, but you still need a lot of data. So the way you should think about it is this. If you've got 100,000 data points, that's small. I know that's very depressing if you're a neuroscientist. Well, it's not depressing. It seems impossible. If you want to personalize medicine for one individual and you want to train a model on their data from their brain, it seems like there's going to be a disconnect between what these models can do and how they might help someone in the future. Yes and no. If I train a model on a very large number of people and then apply that model to one person, that's the form of personalize medicine that really works. Great, thanks. Oh, and say hi to Dale for me. Thank you for the presentation. My last question is about this dropout. The thing is that you randomly just drop some parts of the network and then you say, okay, that it works better so I would accept it. But do we have any, like, intuition why, for example, some parts of it work better or if we try to embed this, like, network into, like, is it more graph isomorphism? What are these two different graphs, different graphs that we took? Are there any similarities between them or just with randomly? I guess the problem with the randomness, I guess we are trying to put the burden of prediction on the random part of the computer desk. So I didn't hear the whole question, but certainly in dropout what we do is we randomly leave out units. Now you can also do block dropout. You can take groups of units and randomly leave out the groups. And what that does is it allows the units within a group to collaborate with one another, and then between groups they have to be fairly independent. And that's called block dropout but I didn't really hear the rest of your question. Okay. The question was about that, okay, imagine that you... Can you talk closer to the microphone because I'm partially dead? The question is that imagine that you have a dropout of 50% and you're trying to get rid of, like, 50% of your nodes and the nodes in the network. And the question is, okay, whether we have any similarity between the types if we do it iteratively, whether we would find any similarity on the structure of the network that would produce the best results and if it's so, whether it would correspond to something physical, like, for example, if you're doing a vision thing, whether it would correspond to something in brain or not, I guess. Yeah. Lots of people have thought about whether you can do better than random in dropout. And there's some work on that, like, block dropout that works, can work for some things. But I don't really have much to say about... I don't really know the answer to sort of, is there something much more sensible than dropout that's a lot more structured? There might well be, but I... Thank you. Okay, so with that, we're going to have to end. So please join me in thanking Jeff for... Thank you. APPLAUSE And I wanted also to thank Blake for hosting this event, and I felt the questions could have gone on all night, but the tip-off isn't half an hour, so some of us have to move on. So thank you, Blake. APPLAUSE", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.72, "text": " Hi, I think we're ready to start, so my name is Paul Franklin and I'm a Neuroscientist", "tokens": [50364, 2421, 11, 286, 519, 321, 434, 1919, 281, 722, 11, 370, 452, 1315, 307, 4552, 22010, 293, 286, 478, 257, 1734, 8977, 5412, 468, 50600], "temperature": 0.0, "avg_logprob": -0.226112295728211, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.1732003390789032}, {"id": 1, "seek": 0, "start": 4.72, "end": 9.120000000000001, "text": " here at Sick Kids, and also I'm Program Chair for the Canadian Neuroscience Meeting that", "tokens": [50600, 510, 412, 43471, 15694, 11, 293, 611, 286, 478, 8338, 8678, 337, 264, 12641, 1734, 8977, 6699, 33217, 300, 50820], "temperature": 0.0, "avg_logprob": -0.226112295728211, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.1732003390789032}, {"id": 2, "seek": 0, "start": 9.120000000000001, "end": 12.68, "text": " takes place in Toronto all this week.", "tokens": [50820, 2516, 1081, 294, 14140, 439, 341, 1243, 13, 50998], "temperature": 0.0, "avg_logprob": -0.226112295728211, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.1732003390789032}, {"id": 3, "seek": 0, "start": 12.68, "end": 19.080000000000002, "text": " So the traditional curtain raiser for the Neuroscience Meeting, the CAN meeting, is the public lecture.", "tokens": [50998, 407, 264, 5164, 26789, 4000, 260, 337, 264, 1734, 8977, 6699, 33217, 11, 264, 22931, 3440, 11, 307, 264, 1908, 7991, 13, 51318], "temperature": 0.0, "avg_logprob": -0.226112295728211, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.1732003390789032}, {"id": 4, "seek": 0, "start": 19.080000000000002, "end": 25.76, "text": " And this year we decided to focus on the interface between neuroscience and AI.", "tokens": [51318, 400, 341, 1064, 321, 3047, 281, 1879, 322, 264, 9226, 1296, 42762, 293, 7318, 13, 51652], "temperature": 0.0, "avg_logprob": -0.226112295728211, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.1732003390789032}, {"id": 5, "seek": 0, "start": 25.76, "end": 27.76, "text": " We did that for two reasons.", "tokens": [51652, 492, 630, 300, 337, 732, 4112, 13, 51752], "temperature": 0.0, "avg_logprob": -0.226112295728211, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.1732003390789032}, {"id": 6, "seek": 2776, "start": 27.76, "end": 33.52, "text": " The first reason is that AI, or Toronto, is one of the main hubs in the world for AI research.", "tokens": [50364, 440, 700, 1778, 307, 300, 7318, 11, 420, 14140, 11, 307, 472, 295, 264, 2135, 46870, 294, 264, 1002, 337, 7318, 2132, 13, 50652], "temperature": 0.0, "avg_logprob": -0.14832227797735306, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.017572367563843727}, {"id": 7, "seek": 2776, "start": 33.52, "end": 39.32, "text": " And the second reason is that it's also home to one of the true pioneers in this field,", "tokens": [50652, 400, 264, 1150, 1778, 307, 300, 309, 311, 611, 1280, 281, 472, 295, 264, 2074, 47381, 294, 341, 2519, 11, 50942], "temperature": 0.0, "avg_logprob": -0.14832227797735306, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.017572367563843727}, {"id": 8, "seek": 2776, "start": 39.32, "end": 42.800000000000004, "text": " also known as the godfather of deep learning, Jeff Hinton.", "tokens": [50942, 611, 2570, 382, 264, 3044, 11541, 295, 2452, 2539, 11, 7506, 389, 12442, 13, 51116], "temperature": 0.0, "avg_logprob": -0.14832227797735306, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.017572367563843727}, {"id": 9, "seek": 2776, "start": 42.800000000000004, "end": 48.08, "text": " And so when we asked Jeff if he'd participate in this event, I think a year and a half ago", "tokens": [51116, 400, 370, 562, 321, 2351, 7506, 498, 415, 1116, 8197, 294, 341, 2280, 11, 286, 519, 257, 1064, 293, 257, 1922, 2057, 51380], "temperature": 0.0, "avg_logprob": -0.14832227797735306, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.017572367563843727}, {"id": 10, "seek": 2776, "start": 48.08, "end": 51.6, "text": " we asked him and he said, yes, we were super excited.", "tokens": [51380, 321, 2351, 796, 293, 415, 848, 11, 2086, 11, 321, 645, 1687, 2919, 13, 51556], "temperature": 0.0, "avg_logprob": -0.14832227797735306, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.017572367563843727}, {"id": 11, "seek": 5160, "start": 51.6, "end": 58.0, "text": " So at this point I want to hand over to Blake Richards, and Blake Richards is an associate", "tokens": [50364, 407, 412, 341, 935, 286, 528, 281, 1011, 670, 281, 23451, 33021, 11, 293, 23451, 33021, 307, 364, 14644, 50684], "temperature": 0.0, "avg_logprob": -0.15311172160696476, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.002746951300650835}, {"id": 12, "seek": 5160, "start": 58.0, "end": 63.24, "text": " professor at University of Toronto Scarborough, and he's going to host this evening's event.", "tokens": [50684, 8304, 412, 3535, 295, 14140, 23181, 24987, 11, 293, 415, 311, 516, 281, 3975, 341, 5634, 311, 2280, 13, 50946], "temperature": 0.0, "avg_logprob": -0.15311172160696476, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.002746951300650835}, {"id": 13, "seek": 5160, "start": 63.24, "end": 64.24000000000001, "text": " Blake?", "tokens": [50946, 23451, 30, 50996], "temperature": 0.0, "avg_logprob": -0.15311172160696476, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.002746951300650835}, {"id": 14, "seek": 5160, "start": 64.24000000000001, "end": 66.0, "text": " Thanks, Paul.", "tokens": [50996, 2561, 11, 4552, 13, 51084], "temperature": 0.0, "avg_logprob": -0.15311172160696476, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.002746951300650835}, {"id": 15, "seek": 5160, "start": 66.0, "end": 72.04, "text": " So just as a brief introduction, I wanted to tell you a little bit more about Jeff.", "tokens": [51084, 407, 445, 382, 257, 5353, 9339, 11, 286, 1415, 281, 980, 291, 257, 707, 857, 544, 466, 7506, 13, 51386], "temperature": 0.0, "avg_logprob": -0.15311172160696476, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.002746951300650835}, {"id": 16, "seek": 5160, "start": 72.04, "end": 78.64, "text": " So you might be surprised to learn that the godfather of deep learning, associated mostly", "tokens": [51386, 407, 291, 1062, 312, 6100, 281, 1466, 300, 264, 3044, 11541, 295, 2452, 2539, 11, 6615, 5240, 51716], "temperature": 0.0, "avg_logprob": -0.15311172160696476, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.002746951300650835}, {"id": 17, "seek": 7864, "start": 78.68, "end": 85.56, "text": " with AI, got his BA in experimental psychology from Cambridge originally.", "tokens": [50366, 365, 7318, 11, 658, 702, 21050, 294, 17069, 15105, 490, 24876, 7993, 13, 50710], "temperature": 0.0, "avg_logprob": -0.12345568741424175, "compression_ratio": 1.5378151260504203, "no_speech_prob": 0.038981955498456955}, {"id": 18, "seek": 7864, "start": 85.56, "end": 92.84, "text": " He then went on to do his PhD in artificial intelligence in Edinburgh, so he did get", "tokens": [50710, 634, 550, 1437, 322, 281, 360, 702, 14476, 294, 11677, 7599, 294, 41215, 11, 370, 415, 630, 483, 51074], "temperature": 0.0, "avg_logprob": -0.12345568741424175, "compression_ratio": 1.5378151260504203, "no_speech_prob": 0.038981955498456955}, {"id": 19, "seek": 7864, "start": 92.84, "end": 95.0, "text": " started relatively early.", "tokens": [51074, 1409, 7226, 2440, 13, 51182], "temperature": 0.0, "avg_logprob": -0.12345568741424175, "compression_ratio": 1.5378151260504203, "no_speech_prob": 0.038981955498456955}, {"id": 20, "seek": 7864, "start": 95.0, "end": 101.04, "text": " But throughout his early career, he really contributed to the first wave of what was", "tokens": [51182, 583, 3710, 702, 2440, 3988, 11, 415, 534, 18434, 281, 264, 700, 5772, 295, 437, 390, 51484], "temperature": 0.0, "avg_logprob": -0.12345568741424175, "compression_ratio": 1.5378151260504203, "no_speech_prob": 0.038981955498456955}, {"id": 21, "seek": 7864, "start": 101.04, "end": 106.52, "text": " known at the time as parallel distributed processing or connectionism, which really brought back", "tokens": [51484, 2570, 412, 264, 565, 382, 8952, 12631, 9007, 420, 4984, 1434, 11, 597, 534, 3038, 646, 51758], "temperature": 0.0, "avg_logprob": -0.12345568741424175, "compression_ratio": 1.5378151260504203, "no_speech_prob": 0.038981955498456955}, {"id": 22, "seek": 10652, "start": 106.52, "end": 113.52, "text": " into the fore the idea of using neural networks for both models of the mind and for artificial", "tokens": [50364, 666, 264, 2091, 264, 1558, 295, 1228, 18161, 9590, 337, 1293, 5245, 295, 264, 1575, 293, 337, 11677, 50714], "temperature": 0.0, "avg_logprob": -0.12846254419397424, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.0026308950036764145}, {"id": 23, "seek": 10652, "start": 113.52, "end": 115.36, "text": " intelligence.", "tokens": [50714, 7599, 13, 50806], "temperature": 0.0, "avg_logprob": -0.12846254419397424, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.0026308950036764145}, {"id": 24, "seek": 10652, "start": 115.36, "end": 123.0, "text": " Now Jeff got his first tenure track position at Carnegie Mellon in the 80s, but we were", "tokens": [50806, 823, 7506, 658, 702, 700, 32256, 2837, 2535, 412, 47301, 376, 898, 266, 294, 264, 4688, 82, 11, 457, 321, 645, 51188], "temperature": 0.0, "avg_logprob": -0.12846254419397424, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.0026308950036764145}, {"id": 25, "seek": 10652, "start": 123.0, "end": 129.48, "text": " able to steal him away from them in the late 80s, which I understand was largely because", "tokens": [51188, 1075, 281, 11009, 796, 1314, 490, 552, 294, 264, 3469, 4688, 82, 11, 597, 286, 1223, 390, 11611, 570, 51512], "temperature": 0.0, "avg_logprob": -0.12846254419397424, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.0026308950036764145}, {"id": 26, "seek": 10652, "start": 129.48, "end": 132.56, "text": " of his ethical objections to DARPA funding.", "tokens": [51512, 295, 702, 18890, 44649, 281, 49274, 10297, 6137, 13, 51666], "temperature": 0.0, "avg_logprob": -0.12846254419397424, "compression_ratio": 1.48868778280543, "no_speech_prob": 0.0026308950036764145}, {"id": 27, "seek": 13256, "start": 132.56, "end": 142.04, "text": " So once again, Canada's political bent has helped us in our research endeavors.", "tokens": [50364, 407, 1564, 797, 11, 6309, 311, 3905, 14075, 575, 4254, 505, 294, 527, 2132, 49608, 13, 50838], "temperature": 0.0, "avg_logprob": -0.1282261848449707, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.006685304455459118}, {"id": 28, "seek": 13256, "start": 142.04, "end": 147.32, "text": " Over the course of the 90s, Jeff continued to really push neural networks and machine", "tokens": [50838, 4886, 264, 1164, 295, 264, 4289, 82, 11, 7506, 7014, 281, 534, 2944, 18161, 9590, 293, 3479, 51102], "temperature": 0.0, "avg_logprob": -0.1282261848449707, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.006685304455459118}, {"id": 29, "seek": 13256, "start": 147.32, "end": 149.32, "text": " learning forward.", "tokens": [51102, 2539, 2128, 13, 51202], "temperature": 0.0, "avg_logprob": -0.1282261848449707, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.006685304455459118}, {"id": 30, "seek": 13256, "start": 149.32, "end": 151.2, "text": " And sorry about that.", "tokens": [51202, 400, 2597, 466, 300, 13, 51296], "temperature": 0.0, "avg_logprob": -0.1282261848449707, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.006685304455459118}, {"id": 31, "seek": 13256, "start": 151.2, "end": 156.32, "text": " In 98, he actually went to University College London to found the Gatsby computational", "tokens": [51296, 682, 20860, 11, 415, 767, 1437, 281, 3535, 6745, 7042, 281, 1352, 264, 460, 1720, 2322, 28270, 51552], "temperature": 0.0, "avg_logprob": -0.1282261848449707, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.006685304455459118}, {"id": 32, "seek": 13256, "start": 156.32, "end": 162.48000000000002, "text": " neuroscience unit, and we might have lost him, but thankfully we pulled him back again.", "tokens": [51552, 42762, 4985, 11, 293, 321, 1062, 362, 2731, 796, 11, 457, 27352, 321, 7373, 796, 646, 797, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1282261848449707, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.006685304455459118}, {"id": 33, "seek": 16248, "start": 162.48, "end": 168.35999999999999, "text": " He came back to Toronto in 2001, where he became a university professor in 2006 and", "tokens": [50364, 634, 1361, 646, 281, 14140, 294, 16382, 11, 689, 415, 3062, 257, 5454, 8304, 294, 14062, 293, 50658], "temperature": 0.0, "avg_logprob": -0.12309267462753667, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.008431687951087952}, {"id": 34, "seek": 16248, "start": 168.35999999999999, "end": 171.6, "text": " then an emeritus professor in 2014.", "tokens": [50658, 550, 364, 4345, 30973, 8304, 294, 8227, 13, 50820], "temperature": 0.0, "avg_logprob": -0.12309267462753667, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.008431687951087952}, {"id": 35, "seek": 16248, "start": 171.6, "end": 179.07999999999998, "text": " Now I think that you all know that Jeff is a monumental figure within artificial intelligence", "tokens": [50820, 823, 286, 519, 300, 291, 439, 458, 300, 7506, 307, 257, 43105, 2573, 1951, 11677, 7599, 51194], "temperature": 0.0, "avg_logprob": -0.12309267462753667, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.008431687951087952}, {"id": 36, "seek": 16248, "start": 179.07999999999998, "end": 185.04, "text": " and machine learning, and he's been critical to the founding of the Vector Institute here", "tokens": [51194, 293, 3479, 2539, 11, 293, 415, 311, 668, 4924, 281, 264, 22223, 295, 264, 691, 20814, 9446, 510, 51492], "temperature": 0.0, "avg_logprob": -0.12309267462753667, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.008431687951087952}, {"id": 37, "seek": 16248, "start": 185.04, "end": 189.39999999999998, "text": " in Toronto and putting Toronto on the map for AI.", "tokens": [51492, 294, 14140, 293, 3372, 14140, 322, 264, 4471, 337, 7318, 13, 51710], "temperature": 0.0, "avg_logprob": -0.12309267462753667, "compression_ratio": 1.5414847161572052, "no_speech_prob": 0.008431687951087952}, {"id": 38, "seek": 18940, "start": 189.4, "end": 194.44, "text": " And certainly he's had incredible recognitions of his work, most recently the Turing Award,", "tokens": [50364, 400, 3297, 415, 311, 632, 4651, 3068, 2451, 295, 702, 589, 11, 881, 3938, 264, 314, 1345, 13894, 11, 50616], "temperature": 0.0, "avg_logprob": -0.16126057168711785, "compression_ratio": 1.6059602649006623, "no_speech_prob": 0.014262307435274124}, {"id": 39, "seek": 18940, "start": 194.44, "end": 200.64000000000001, "text": " which he shared with Joshua Bengio and Yann LeCun, as well as the Order of Canada.", "tokens": [50616, 597, 415, 5507, 365, 24005, 3964, 17862, 293, 398, 969, 1456, 34, 409, 11, 382, 731, 382, 264, 16321, 295, 6309, 13, 50926], "temperature": 0.0, "avg_logprob": -0.16126057168711785, "compression_ratio": 1.6059602649006623, "no_speech_prob": 0.014262307435274124}, {"id": 40, "seek": 18940, "start": 200.64000000000001, "end": 205.20000000000002, "text": " And he's a distinguished fellow of the Canadian Institute for Advanced Research, which I highlight", "tokens": [50926, 400, 415, 311, 257, 21702, 7177, 295, 264, 12641, 9446, 337, 26951, 10303, 11, 597, 286, 5078, 51154], "temperature": 0.0, "avg_logprob": -0.16126057168711785, "compression_ratio": 1.6059602649006623, "no_speech_prob": 0.014262307435274124}, {"id": 41, "seek": 18940, "start": 205.20000000000002, "end": 210.52, "text": " because they were one of the people who continued to support neural networks throughout the", "tokens": [51154, 570, 436, 645, 472, 295, 264, 561, 567, 7014, 281, 1406, 18161, 9590, 3710, 264, 51420], "temperature": 0.0, "avg_logprob": -0.16126057168711785, "compression_ratio": 1.6059602649006623, "no_speech_prob": 0.014262307435274124}, {"id": 42, "seek": 18940, "start": 210.52, "end": 213.44, "text": " time when it wasn't as faddish.", "tokens": [51420, 565, 562, 309, 2067, 380, 382, 283, 25224, 742, 13, 51566], "temperature": 0.0, "avg_logprob": -0.16126057168711785, "compression_ratio": 1.6059602649006623, "no_speech_prob": 0.014262307435274124}, {"id": 43, "seek": 18940, "start": 213.44, "end": 217.44, "text": " But for all his successes and his technical endeavors, I think one of the things that's", "tokens": [51566, 583, 337, 439, 702, 26101, 293, 702, 6191, 49608, 11, 286, 519, 472, 295, 264, 721, 300, 311, 51766], "temperature": 0.0, "avg_logprob": -0.16126057168711785, "compression_ratio": 1.6059602649006623, "no_speech_prob": 0.014262307435274124}, {"id": 44, "seek": 21744, "start": 217.44, "end": 222.35999999999999, "text": " most important to understand about Jeff is the impact that he's had on other scientists.", "tokens": [50364, 881, 1021, 281, 1223, 466, 7506, 307, 264, 2712, 300, 415, 311, 632, 322, 661, 7708, 13, 50610], "temperature": 0.0, "avg_logprob": -0.12224726493542011, "compression_ratio": 1.6274509803921569, "no_speech_prob": 0.0025073240976780653}, {"id": 45, "seek": 21744, "start": 222.35999999999999, "end": 226.84, "text": " Jeff has really molded the career of so many people and changed the way that they think", "tokens": [50610, 7506, 575, 534, 11102, 292, 264, 3988, 295, 370, 867, 561, 293, 3105, 264, 636, 300, 436, 519, 50834], "temperature": 0.0, "avg_logprob": -0.12224726493542011, "compression_ratio": 1.6274509803921569, "no_speech_prob": 0.0025073240976780653}, {"id": 46, "seek": 21744, "start": 226.84, "end": 229.56, "text": " about things.", "tokens": [50834, 466, 721, 13, 50970], "temperature": 0.0, "avg_logprob": -0.12224726493542011, "compression_ratio": 1.6274509803921569, "no_speech_prob": 0.0025073240976780653}, {"id": 47, "seek": 21744, "start": 229.56, "end": 234.56, "text": " When you look at the people who have been his graduate students or postdocs, it really", "tokens": [50970, 1133, 291, 574, 412, 264, 561, 567, 362, 668, 702, 8080, 1731, 420, 2183, 39966, 82, 11, 309, 534, 51220], "temperature": 0.0, "avg_logprob": -0.12224726493542011, "compression_ratio": 1.6274509803921569, "no_speech_prob": 0.0025073240976780653}, {"id": 48, "seek": 21744, "start": 234.56, "end": 237.84, "text": " is the who's who in artificial intelligence.", "tokens": [51220, 307, 264, 567, 311, 567, 294, 11677, 7599, 13, 51384], "temperature": 0.0, "avg_logprob": -0.12224726493542011, "compression_ratio": 1.6274509803921569, "no_speech_prob": 0.0025073240976780653}, {"id": 49, "seek": 21744, "start": 237.84, "end": 247.32, "text": " It includes people like Max Welling, Yann LeCun, and you know, the phrase I used to describe", "tokens": [51384, 467, 5974, 561, 411, 7402, 1042, 278, 11, 398, 969, 1456, 34, 409, 11, 293, 291, 458, 11, 264, 9535, 286, 1143, 281, 6786, 51858], "temperature": 0.0, "avg_logprob": -0.12224726493542011, "compression_ratio": 1.6274509803921569, "no_speech_prob": 0.0025073240976780653}, {"id": 50, "seek": 24732, "start": 247.4, "end": 252.16, "text": " it is, have you drunk Jeff's Kool-Aid?", "tokens": [50368, 309, 307, 11, 362, 291, 11192, 7506, 311, 591, 1092, 12, 32, 327, 30, 50606], "temperature": 0.0, "avg_logprob": -0.12023354451590722, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.04330262541770935}, {"id": 51, "seek": 24732, "start": 252.16, "end": 255.35999999999999, "text": " Because once you've drunk Jeff's Kool-Aid, there is no going back.", "tokens": [50606, 1436, 1564, 291, 600, 11192, 7506, 311, 591, 1092, 12, 32, 327, 11, 456, 307, 572, 516, 646, 13, 50766], "temperature": 0.0, "avg_logprob": -0.12023354451590722, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.04330262541770935}, {"id": 52, "seek": 24732, "start": 255.35999999999999, "end": 262.2, "text": " You see neural networks, you see AI differently, and I would argue you also see neuroscience", "tokens": [50766, 509, 536, 18161, 9590, 11, 291, 536, 7318, 7614, 11, 293, 286, 576, 9695, 291, 611, 536, 42762, 51108], "temperature": 0.0, "avg_logprob": -0.12023354451590722, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.04330262541770935}, {"id": 53, "seek": 24732, "start": 262.2, "end": 263.48, "text": " differently.", "tokens": [51108, 7614, 13, 51172], "temperature": 0.0, "avg_logprob": -0.12023354451590722, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.04330262541770935}, {"id": 54, "seek": 24732, "start": 263.48, "end": 271.32, "text": " And for me, my understanding of the brain has been largely shaped by Jeff and his work.", "tokens": [51172, 400, 337, 385, 11, 452, 3701, 295, 264, 3567, 575, 668, 11611, 13475, 538, 7506, 293, 702, 589, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12023354451590722, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.04330262541770935}, {"id": 55, "seek": 24732, "start": 271.32, "end": 276.08, "text": " But you know, we're at the point now where computer science has drunk Jeff's Kool-Aid.", "tokens": [51564, 583, 291, 458, 11, 321, 434, 412, 264, 935, 586, 689, 3820, 3497, 575, 11192, 7506, 311, 591, 1092, 12, 32, 327, 13, 51802], "temperature": 0.0, "avg_logprob": -0.12023354451590722, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.04330262541770935}, {"id": 56, "seek": 27608, "start": 276.08, "end": 282.15999999999997, "text": " So he's got an H index of 145, and according to Google Scholar, his work has been cited", "tokens": [50364, 407, 415, 311, 658, 364, 389, 8186, 295, 3499, 20, 11, 293, 4650, 281, 3329, 2065, 15276, 11, 702, 589, 575, 668, 30134, 50668], "temperature": 0.0, "avg_logprob": -0.1737927188392447, "compression_ratio": 1.4689655172413794, "no_speech_prob": 0.0007552095921710134}, {"id": 57, "seek": 27608, "start": 282.15999999999997, "end": 289.03999999999996, "text": " 270,000 times, which is more than Einstein, Ramoni, Cajal, and Alan Tern combined.", "tokens": [50668, 40774, 11, 1360, 1413, 11, 597, 307, 544, 813, 23486, 11, 9078, 17049, 11, 383, 1805, 304, 11, 293, 16442, 314, 1248, 9354, 13, 51012], "temperature": 0.0, "avg_logprob": -0.1737927188392447, "compression_ratio": 1.4689655172413794, "no_speech_prob": 0.0007552095921710134}, {"id": 58, "seek": 27608, "start": 289.03999999999996, "end": 291.52, "text": " But that's largely from computer scientists.", "tokens": [51012, 583, 300, 311, 11611, 490, 3820, 7708, 13, 51136], "temperature": 0.0, "avg_logprob": -0.1737927188392447, "compression_ratio": 1.4689655172413794, "no_speech_prob": 0.0007552095921710134}, {"id": 59, "seek": 27608, "start": 291.52, "end": 298.03999999999996, "text": " And if my prediction is correct, neuroscience 30, 40 years from now will also have drunk", "tokens": [51136, 400, 498, 452, 17630, 307, 3006, 11, 42762, 2217, 11, 3356, 924, 490, 586, 486, 611, 362, 11192, 51462], "temperature": 0.0, "avg_logprob": -0.1737927188392447, "compression_ratio": 1.4689655172413794, "no_speech_prob": 0.0007552095921710134}, {"id": 60, "seek": 27608, "start": 298.03999999999996, "end": 301.47999999999996, "text": " Jeff's Kool-Aid, and maybe you're going to get your first taste tonight.", "tokens": [51462, 7506, 311, 591, 1092, 12, 32, 327, 11, 293, 1310, 291, 434, 516, 281, 483, 428, 700, 3939, 4440, 13, 51634], "temperature": 0.0, "avg_logprob": -0.1737927188392447, "compression_ratio": 1.4689655172413794, "no_speech_prob": 0.0007552095921710134}, {"id": 61, "seek": 27608, "start": 301.47999999999996, "end": 304.03999999999996, "text": " So with that, I hand you over to Jeffrey Hinton.", "tokens": [51634, 407, 365, 300, 11, 286, 1011, 291, 670, 281, 28721, 389, 12442, 13, 51762], "temperature": 0.0, "avg_logprob": -0.1737927188392447, "compression_ratio": 1.4689655172413794, "no_speech_prob": 0.0007552095921710134}, {"id": 62, "seek": 30608, "start": 306.08, "end": 314.91999999999996, "text": " So thank you very much, Blake.", "tokens": [50364, 407, 1309, 291, 588, 709, 11, 23451, 13, 50806], "temperature": 0.0, "avg_logprob": -0.22197123690768406, "compression_ratio": 1.4612244897959183, "no_speech_prob": 0.014804862439632416}, {"id": 63, "seek": 30608, "start": 314.91999999999996, "end": 317.91999999999996, "text": " I can give you some more Kool-Aid today.", "tokens": [50806, 286, 393, 976, 291, 512, 544, 591, 1092, 12, 32, 327, 965, 13, 50956], "temperature": 0.0, "avg_logprob": -0.22197123690768406, "compression_ratio": 1.4612244897959183, "no_speech_prob": 0.014804862439632416}, {"id": 64, "seek": 30608, "start": 317.91999999999996, "end": 321.36, "text": " It's Kool-Aid produced by one of my former students, Ilya Sutskava.", "tokens": [50956, 467, 311, 591, 1092, 12, 32, 327, 7126, 538, 472, 295, 452, 5819, 1731, 11, 286, 45106, 318, 3648, 74, 4061, 13, 51128], "temperature": 0.0, "avg_logprob": -0.22197123690768406, "compression_ratio": 1.4612244897959183, "no_speech_prob": 0.014804862439632416}, {"id": 65, "seek": 30608, "start": 321.36, "end": 326.47999999999996, "text": " First, I want to tell you a little bit about the history of deep learning in AI.", "tokens": [51128, 2386, 11, 286, 528, 281, 980, 291, 257, 707, 857, 466, 264, 2503, 295, 2452, 2539, 294, 7318, 13, 51384], "temperature": 0.0, "avg_logprob": -0.22197123690768406, "compression_ratio": 1.4612244897959183, "no_speech_prob": 0.014804862439632416}, {"id": 66, "seek": 30608, "start": 326.47999999999996, "end": 330.59999999999997, "text": " Can I just ask before I start, how many people here know what the back propagation algorithm", "tokens": [51384, 1664, 286, 445, 1029, 949, 286, 722, 11, 577, 867, 561, 510, 458, 437, 264, 646, 38377, 9284, 51590], "temperature": 0.0, "avg_logprob": -0.22197123690768406, "compression_ratio": 1.4612244897959183, "no_speech_prob": 0.014804862439632416}, {"id": 67, "seek": 30608, "start": 330.59999999999997, "end": 331.59999999999997, "text": " is?", "tokens": [51590, 307, 30, 51640], "temperature": 0.0, "avg_logprob": -0.22197123690768406, "compression_ratio": 1.4612244897959183, "no_speech_prob": 0.014804862439632416}, {"id": 68, "seek": 30608, "start": 331.59999999999997, "end": 333.88, "text": " Put your hands up.", "tokens": [51640, 4935, 428, 2377, 493, 13, 51754], "temperature": 0.0, "avg_logprob": -0.22197123690768406, "compression_ratio": 1.4612244897959183, "no_speech_prob": 0.014804862439632416}, {"id": 69, "seek": 30608, "start": 333.88, "end": 335.68, "text": " So some people don't.", "tokens": [51754, 407, 512, 561, 500, 380, 13, 51844], "temperature": 0.0, "avg_logprob": -0.22197123690768406, "compression_ratio": 1.4612244897959183, "no_speech_prob": 0.014804862439632416}, {"id": 70, "seek": 33568, "start": 335.68, "end": 339.64, "text": " I'll explain it very quickly, and I'll explain it in such a way that you'll be able to explain", "tokens": [50364, 286, 603, 2903, 309, 588, 2661, 11, 293, 286, 603, 2903, 309, 294, 1270, 257, 636, 300, 291, 603, 312, 1075, 281, 2903, 50562], "temperature": 0.0, "avg_logprob": -0.15377375387376355, "compression_ratio": 1.75, "no_speech_prob": 0.06298846751451492}, {"id": 71, "seek": 33568, "start": 339.64, "end": 340.64, "text": " to other people.", "tokens": [50562, 281, 661, 561, 13, 50612], "temperature": 0.0, "avg_logprob": -0.15377375387376355, "compression_ratio": 1.75, "no_speech_prob": 0.06298846751451492}, {"id": 72, "seek": 33568, "start": 340.64, "end": 343.96, "text": " So if you do know what it is, you follow the explanation from the point of view of how", "tokens": [50612, 407, 498, 291, 360, 458, 437, 309, 307, 11, 291, 1524, 264, 10835, 490, 264, 935, 295, 1910, 295, 577, 50778], "temperature": 0.0, "avg_logprob": -0.15377375387376355, "compression_ratio": 1.75, "no_speech_prob": 0.06298846751451492}, {"id": 73, "seek": 33568, "start": 343.96, "end": 344.96, "text": " you explain it.", "tokens": [50778, 291, 2903, 309, 13, 50828], "temperature": 0.0, "avg_logprob": -0.15377375387376355, "compression_ratio": 1.75, "no_speech_prob": 0.06298846751451492}, {"id": 74, "seek": 33568, "start": 344.96, "end": 345.96, "text": " Okay.", "tokens": [50828, 1033, 13, 50878], "temperature": 0.0, "avg_logprob": -0.15377375387376355, "compression_ratio": 1.75, "no_speech_prob": 0.06298846751451492}, {"id": 75, "seek": 33568, "start": 345.96, "end": 348.32, "text": " There was a war between two paradigms for AI.", "tokens": [50878, 821, 390, 257, 1516, 1296, 732, 13480, 328, 2592, 337, 7318, 13, 50996], "temperature": 0.0, "avg_logprob": -0.15377375387376355, "compression_ratio": 1.75, "no_speech_prob": 0.06298846751451492}, {"id": 76, "seek": 33568, "start": 348.32, "end": 353.32, "text": " There were people who thought that the essence of intelligence was reasoning, and logic is", "tokens": [50996, 821, 645, 561, 567, 1194, 300, 264, 12801, 295, 7599, 390, 21577, 11, 293, 9952, 307, 51246], "temperature": 0.0, "avg_logprob": -0.15377375387376355, "compression_ratio": 1.75, "no_speech_prob": 0.06298846751451492}, {"id": 77, "seek": 33568, "start": 353.32, "end": 358.88, "text": " what does reasoning, so we should base artificial intelligence on taking strings of symbols and", "tokens": [51246, 437, 775, 21577, 11, 370, 321, 820, 3096, 11677, 7599, 322, 1940, 13985, 295, 16944, 293, 51524], "temperature": 0.0, "avg_logprob": -0.15377375387376355, "compression_ratio": 1.75, "no_speech_prob": 0.06298846751451492}, {"id": 78, "seek": 33568, "start": 358.88, "end": 362.36, "text": " manipulating them to arrive at conclusions.", "tokens": [51524, 40805, 552, 281, 8881, 412, 22865, 13, 51698], "temperature": 0.0, "avg_logprob": -0.15377375387376355, "compression_ratio": 1.75, "no_speech_prob": 0.06298846751451492}, {"id": 79, "seek": 36236, "start": 362.36, "end": 365.84000000000003, "text": " And then there were other people who looked at the brain and said, no, no, intelligence", "tokens": [50364, 400, 550, 456, 645, 661, 561, 567, 2956, 412, 264, 3567, 293, 848, 11, 572, 11, 572, 11, 7599, 50538], "temperature": 0.0, "avg_logprob": -0.106637231927169, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.041453756392002106}, {"id": 80, "seek": 36236, "start": 365.84000000000003, "end": 370.2, "text": " is all about adapting connections in the brain to get smarter.", "tokens": [50538, 307, 439, 466, 34942, 9271, 294, 264, 3567, 281, 483, 20294, 13, 50756], "temperature": 0.0, "avg_logprob": -0.106637231927169, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.041453756392002106}, {"id": 81, "seek": 36236, "start": 370.2, "end": 377.08000000000004, "text": " And this war went on for a long time, and eventually, people who were trying to figure", "tokens": [50756, 400, 341, 1516, 1437, 322, 337, 257, 938, 565, 11, 293, 4728, 11, 561, 567, 645, 1382, 281, 2573, 51100], "temperature": 0.0, "avg_logprob": -0.106637231927169, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.041453756392002106}, {"id": 82, "seek": 36236, "start": 377.08000000000004, "end": 383.88, "text": " out how to change connections between fake neurons to make these networks smarter got", "tokens": [51100, 484, 577, 281, 1319, 9271, 1296, 7592, 22027, 281, 652, 613, 9590, 20294, 658, 51440], "temperature": 0.0, "avg_logprob": -0.106637231927169, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.041453756392002106}, {"id": 83, "seek": 36236, "start": 383.88, "end": 388.92, "text": " to be able to do things that the people doing symbolic AI just couldn't do at all.", "tokens": [51440, 281, 312, 1075, 281, 360, 721, 300, 264, 561, 884, 25755, 7318, 445, 2809, 380, 360, 412, 439, 13, 51692], "temperature": 0.0, "avg_logprob": -0.106637231927169, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.041453756392002106}, {"id": 84, "seek": 38892, "start": 388.92, "end": 393.32, "text": " And now there's a different way of getting a computer to do what you want.", "tokens": [50364, 400, 586, 456, 311, 257, 819, 636, 295, 1242, 257, 3820, 281, 360, 437, 291, 528, 13, 50584], "temperature": 0.0, "avg_logprob": -0.1318138043085734, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.08898666501045227}, {"id": 85, "seek": 38892, "start": 393.32, "end": 398.24, "text": " Instead of programming it, which is tedious, you just showed examples, and it figures it", "tokens": [50584, 7156, 295, 9410, 309, 11, 597, 307, 38284, 11, 291, 445, 4712, 5110, 11, 293, 309, 9624, 309, 50830], "temperature": 0.0, "avg_logprob": -0.1318138043085734, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.08898666501045227}, {"id": 86, "seek": 38892, "start": 398.24, "end": 399.24, "text": " out.", "tokens": [50830, 484, 13, 50880], "temperature": 0.0, "avg_logprob": -0.1318138043085734, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.08898666501045227}, {"id": 87, "seek": 38892, "start": 399.24, "end": 402.44, "text": " Now, of course, you have to write the program that figures it out, but that's just one program", "tokens": [50880, 823, 11, 295, 1164, 11, 291, 362, 281, 2464, 264, 1461, 300, 9624, 309, 484, 11, 457, 300, 311, 445, 472, 1461, 51040], "temperature": 0.0, "avg_logprob": -0.1318138043085734, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.08898666501045227}, {"id": 88, "seek": 38892, "start": 402.44, "end": 405.96000000000004, "text": " that will then do everything.", "tokens": [51040, 300, 486, 550, 360, 1203, 13, 51216], "temperature": 0.0, "avg_logprob": -0.1318138043085734, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.08898666501045227}, {"id": 89, "seek": 38892, "start": 405.96000000000004, "end": 410.0, "text": " And this is an example of what it can do.", "tokens": [51216, 400, 341, 307, 364, 1365, 295, 437, 309, 393, 360, 13, 51418], "temperature": 0.0, "avg_logprob": -0.1318138043085734, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.08898666501045227}, {"id": 90, "seek": 38892, "start": 410.0, "end": 412.88, "text": " So the image, just think of the numbers.", "tokens": [51418, 407, 264, 3256, 11, 445, 519, 295, 264, 3547, 13, 51562], "temperature": 0.0, "avg_logprob": -0.1318138043085734, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.08898666501045227}, {"id": 91, "seek": 38892, "start": 412.88, "end": 417.20000000000005, "text": " They're RGB values of pixels, and that's the input to the computer.", "tokens": [51562, 814, 434, 31231, 4190, 295, 18668, 11, 293, 300, 311, 264, 4846, 281, 264, 3820, 13, 51778], "temperature": 0.0, "avg_logprob": -0.1318138043085734, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.08898666501045227}, {"id": 92, "seek": 41720, "start": 417.2, "end": 422.64, "text": " Lots of values of pixels, just real numbers, saying how bright the red channel is.", "tokens": [50364, 15908, 295, 4190, 295, 18668, 11, 445, 957, 3547, 11, 1566, 577, 4730, 264, 2182, 2269, 307, 13, 50636], "temperature": 0.0, "avg_logprob": -0.18039641841765372, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.05138480290770531}, {"id": 93, "seek": 41720, "start": 422.64, "end": 427.96, "text": " And you have to turn those numbers into a string of words that says a close-up of a", "tokens": [50636, 400, 291, 362, 281, 1261, 729, 3547, 666, 257, 6798, 295, 2283, 300, 1619, 257, 1998, 12, 1010, 295, 257, 50902], "temperature": 0.0, "avg_logprob": -0.18039641841765372, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.05138480290770531}, {"id": 94, "seek": 41720, "start": 427.96, "end": 429.92, "text": " child holding a stuffed animal.", "tokens": [50902, 1440, 5061, 257, 24092, 5496, 13, 51000], "temperature": 0.0, "avg_logprob": -0.18039641841765372, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.05138480290770531}, {"id": 95, "seek": 41720, "start": 429.92, "end": 431.44, "text": " And imagine writing that program.", "tokens": [51000, 400, 3811, 3579, 300, 1461, 13, 51076], "temperature": 0.0, "avg_logprob": -0.18039641841765372, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.05138480290770531}, {"id": 96, "seek": 41720, "start": 431.44, "end": 434.96, "text": " Well, people in conventional AI had tried to write that program and they couldn't, partly", "tokens": [51076, 1042, 11, 561, 294, 16011, 7318, 632, 3031, 281, 2464, 300, 1461, 293, 436, 2809, 380, 11, 17031, 51252], "temperature": 0.0, "avg_logprob": -0.18039641841765372, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.05138480290770531}, {"id": 97, "seek": 41720, "start": 434.96, "end": 437.76, "text": " because they didn't know how we did it.", "tokens": [51252, 570, 436, 994, 380, 458, 577, 321, 630, 309, 13, 51392], "temperature": 0.0, "avg_logprob": -0.18039641841765372, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.05138480290770531}, {"id": 98, "seek": 41720, "start": 437.76, "end": 441.76, "text": " We still don't know how we do it, but we can get artificial neural networks to do it now", "tokens": [51392, 492, 920, 500, 380, 458, 577, 321, 360, 309, 11, 457, 321, 393, 483, 11677, 18161, 9590, 281, 360, 309, 586, 51592], "temperature": 0.0, "avg_logprob": -0.18039641841765372, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.05138480290770531}, {"id": 99, "seek": 41720, "start": 441.76, "end": 444.32, "text": " and do a pretty good job.", "tokens": [51592, 293, 360, 257, 1238, 665, 1691, 13, 51720], "temperature": 0.0, "avg_logprob": -0.18039641841765372, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.05138480290770531}, {"id": 100, "seek": 44432, "start": 444.44, "end": 451.15999999999997, "text": " My prediction is, within 10 years, if you go and get a CT scan, what will happen is", "tokens": [50370, 1222, 17630, 307, 11, 1951, 1266, 924, 11, 498, 291, 352, 293, 483, 257, 19529, 11049, 11, 437, 486, 1051, 307, 50706], "temperature": 0.0, "avg_logprob": -0.20469297681535994, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.009939194656908512}, {"id": 101, "seek": 44432, "start": 451.15999999999997, "end": 456.84, "text": " a computer will look at the CT scan, and a computer will produce the written report", "tokens": [50706, 257, 3820, 486, 574, 412, 264, 19529, 11049, 11, 293, 257, 3820, 486, 5258, 264, 3720, 2275, 50990], "temperature": 0.0, "avg_logprob": -0.20469297681535994, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.009939194656908512}, {"id": 102, "seek": 44432, "start": 456.84, "end": 459.96, "text": " that the radiologist currently produces.", "tokens": [50990, 300, 264, 16335, 9201, 4362, 14725, 13, 51146], "temperature": 0.0, "avg_logprob": -0.20469297681535994, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.009939194656908512}, {"id": 103, "seek": 44432, "start": 459.96, "end": 463.12, "text": " Radiologists don't like this idea.", "tokens": [51146, 37806, 12256, 500, 380, 411, 341, 1558, 13, 51304], "temperature": 0.0, "avg_logprob": -0.20469297681535994, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.009939194656908512}, {"id": 104, "seek": 44432, "start": 463.12, "end": 464.12, "text": " Okay.", "tokens": [51304, 1033, 13, 51354], "temperature": 0.0, "avg_logprob": -0.20469297681535994, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.009939194656908512}, {"id": 105, "seek": 44432, "start": 464.12, "end": 466.12, "text": " Here's a simplified model of a neuron.", "tokens": [51354, 1692, 311, 257, 26335, 2316, 295, 257, 34090, 13, 51454], "temperature": 0.0, "avg_logprob": -0.20469297681535994, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.009939194656908512}, {"id": 106, "seek": 44432, "start": 466.12, "end": 467.12, "text": " It's very simple.", "tokens": [51454, 467, 311, 588, 2199, 13, 51504], "temperature": 0.0, "avg_logprob": -0.20469297681535994, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.009939194656908512}, {"id": 107, "seek": 44432, "start": 467.12, "end": 471.52, "text": " It gets some input, which is just the activity on the input lines times the weights, adds", "tokens": [51504, 467, 2170, 512, 4846, 11, 597, 307, 445, 264, 5191, 322, 264, 4846, 3876, 1413, 264, 17443, 11, 10860, 51724], "temperature": 0.0, "avg_logprob": -0.20469297681535994, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.009939194656908512}, {"id": 108, "seek": 44432, "start": 471.52, "end": 472.52, "text": " it all up.", "tokens": [51724, 309, 439, 493, 13, 51774], "temperature": 0.0, "avg_logprob": -0.20469297681535994, "compression_ratio": 1.6215139442231075, "no_speech_prob": 0.009939194656908512}, {"id": 109, "seek": 47252, "start": 472.71999999999997, "end": 474.56, "text": " That's called the depolarization.", "tokens": [50374, 663, 311, 1219, 264, 1367, 15276, 2144, 13, 50466], "temperature": 0.0, "avg_logprob": -0.199213711420695, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.003911581821739674}, {"id": 110, "seek": 47252, "start": 474.56, "end": 480.96, "text": " And then it gives an output that's proportional to how much input it gets as long as it gets", "tokens": [50466, 400, 550, 309, 2709, 364, 5598, 300, 311, 24969, 281, 577, 709, 4846, 309, 2170, 382, 938, 382, 309, 2170, 50786], "temperature": 0.0, "avg_logprob": -0.199213711420695, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.003911581821739674}, {"id": 111, "seek": 47252, "start": 480.96, "end": 482.47999999999996, "text": " enough input.", "tokens": [50786, 1547, 4846, 13, 50862], "temperature": 0.0, "avg_logprob": -0.199213711420695, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.003911581821739674}, {"id": 112, "seek": 47252, "start": 482.47999999999996, "end": 484.35999999999996, "text": " And so, to begin with, we won't have spiking neurons.", "tokens": [50862, 400, 370, 11, 281, 1841, 365, 11, 321, 1582, 380, 362, 637, 13085, 22027, 13, 50956], "temperature": 0.0, "avg_logprob": -0.199213711420695, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.003911581821739674}, {"id": 113, "seek": 47252, "start": 484.35999999999996, "end": 488.96, "text": " These are just going to be neurons that send real values in just the way neurons don't.", "tokens": [50956, 1981, 366, 445, 516, 281, 312, 22027, 300, 2845, 957, 4190, 294, 445, 264, 636, 22027, 500, 380, 13, 51186], "temperature": 0.0, "avg_logprob": -0.199213711420695, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.003911581821739674}, {"id": 114, "seek": 47252, "start": 488.96, "end": 493.88, "text": " We're going to make networks of them by hooking them up into layers.", "tokens": [51186, 492, 434, 516, 281, 652, 9590, 295, 552, 538, 1106, 5953, 552, 493, 666, 7914, 13, 51432], "temperature": 0.0, "avg_logprob": -0.199213711420695, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.003911581821739674}, {"id": 115, "seek": 47252, "start": 493.88, "end": 496.88, "text": " And you could put some pixels on the input neurons.", "tokens": [51432, 400, 291, 727, 829, 512, 18668, 322, 264, 4846, 22027, 13, 51582], "temperature": 0.0, "avg_logprob": -0.199213711420695, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.003911581821739674}, {"id": 116, "seek": 47252, "start": 496.88, "end": 497.88, "text": " Look.", "tokens": [51582, 2053, 13, 51632], "temperature": 0.0, "avg_logprob": -0.199213711420695, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.003911581821739674}, {"id": 117, "seek": 47252, "start": 497.88, "end": 501.08, "text": " They're the input neurons.", "tokens": [51632, 814, 434, 264, 4846, 22027, 13, 51792], "temperature": 0.0, "avg_logprob": -0.199213711420695, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.003911581821739674}, {"id": 118, "seek": 50108, "start": 501.08, "end": 505.44, "text": " And you go forwards through the net until you get outputs.", "tokens": [50364, 400, 291, 352, 30126, 807, 264, 2533, 1826, 291, 483, 23930, 13, 50582], "temperature": 0.0, "avg_logprob": -0.15191999471412515, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.003653100226074457}, {"id": 119, "seek": 50108, "start": 505.44, "end": 508.2, "text": " And then you compare those outputs with what you ought to have got, so you have to know", "tokens": [50582, 400, 550, 291, 6794, 729, 23930, 365, 437, 291, 13416, 281, 362, 658, 11, 370, 291, 362, 281, 458, 50720], "temperature": 0.0, "avg_logprob": -0.15191999471412515, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.003653100226074457}, {"id": 120, "seek": 50108, "start": 508.2, "end": 510.44, "text": " what the right answer is.", "tokens": [50720, 437, 264, 558, 1867, 307, 13, 50832], "temperature": 0.0, "avg_logprob": -0.15191999471412515, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.003653100226074457}, {"id": 121, "seek": 50108, "start": 510.44, "end": 516.4, "text": " And what we'd like to do is train the weights, these red and green dots, so that it gives", "tokens": [50832, 400, 437, 321, 1116, 411, 281, 360, 307, 3847, 264, 17443, 11, 613, 2182, 293, 3092, 15026, 11, 370, 300, 309, 2709, 51130], "temperature": 0.0, "avg_logprob": -0.15191999471412515, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.003653100226074457}, {"id": 122, "seek": 50108, "start": 516.4, "end": 517.4, "text": " the right output.", "tokens": [51130, 264, 558, 5598, 13, 51180], "temperature": 0.0, "avg_logprob": -0.15191999471412515, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.003653100226074457}, {"id": 123, "seek": 50108, "start": 517.4, "end": 522.36, "text": " Now, I'm going to show you a way of training the weights that everybody can understand,", "tokens": [51180, 823, 11, 286, 478, 516, 281, 855, 291, 257, 636, 295, 3097, 264, 17443, 300, 2201, 393, 1223, 11, 51428], "temperature": 0.0, "avg_logprob": -0.15191999471412515, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.003653100226074457}, {"id": 124, "seek": 50108, "start": 522.36, "end": 527.12, "text": " and everybody is thought of, basically.", "tokens": [51428, 293, 2201, 307, 1194, 295, 11, 1936, 13, 51666], "temperature": 0.0, "avg_logprob": -0.15191999471412515, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.003653100226074457}, {"id": 125, "seek": 52712, "start": 527.12, "end": 534.28, "text": " What you do is you start with random weights, you show it some inputs, you measure how well", "tokens": [50364, 708, 291, 360, 307, 291, 722, 365, 4974, 17443, 11, 291, 855, 309, 512, 15743, 11, 291, 3481, 577, 731, 50722], "temperature": 0.0, "avg_logprob": -0.19190763326791616, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.062171127647161484}, {"id": 126, "seek": 52712, "start": 534.28, "end": 538.2, "text": " it does, then you change one weight a tiny bit.", "tokens": [50722, 309, 775, 11, 550, 291, 1319, 472, 3364, 257, 5870, 857, 13, 50918], "temperature": 0.0, "avg_logprob": -0.19190763326791616, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.062171127647161484}, {"id": 127, "seek": 52712, "start": 538.2, "end": 542.68, "text": " So I take that weight there, and I just change it a tiny bit, and then I show it the same", "tokens": [50918, 407, 286, 747, 300, 3364, 456, 11, 293, 286, 445, 1319, 309, 257, 5870, 857, 11, 293, 550, 286, 855, 309, 264, 912, 51142], "temperature": 0.0, "avg_logprob": -0.19190763326791616, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.062171127647161484}, {"id": 128, "seek": 52712, "start": 542.68, "end": 545.28, "text": " inputs again and see if it does better or worse.", "tokens": [51142, 15743, 797, 293, 536, 498, 309, 775, 1101, 420, 5324, 13, 51272], "temperature": 0.0, "avg_logprob": -0.19190763326791616, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.062171127647161484}, {"id": 129, "seek": 52712, "start": 545.28, "end": 546.68, "text": " If it does better, I keep the change.", "tokens": [51272, 759, 309, 775, 1101, 11, 286, 1066, 264, 1319, 13, 51342], "temperature": 0.0, "avg_logprob": -0.19190763326791616, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.062171127647161484}, {"id": 130, "seek": 52712, "start": 546.68, "end": 550.92, "text": " If it does worse, maybe I keep the change in the opposite direction.", "tokens": [51342, 759, 309, 775, 5324, 11, 1310, 286, 1066, 264, 1319, 294, 264, 6182, 3513, 13, 51554], "temperature": 0.0, "avg_logprob": -0.19190763326791616, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.062171127647161484}, {"id": 131, "seek": 52712, "start": 550.92, "end": 552.4, "text": " That's an easy algorithm to understand.", "tokens": [51554, 663, 311, 364, 1858, 9284, 281, 1223, 13, 51628], "temperature": 0.0, "avg_logprob": -0.19190763326791616, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.062171127647161484}, {"id": 132, "seek": 52712, "start": 552.4, "end": 553.4, "text": " And that algorithm works.", "tokens": [51628, 400, 300, 9284, 1985, 13, 51678], "temperature": 0.0, "avg_logprob": -0.19190763326791616, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.062171127647161484}, {"id": 133, "seek": 52712, "start": 553.4, "end": 554.4, "text": " It's just incredibly slow.", "tokens": [51678, 467, 311, 445, 6252, 2964, 13, 51728], "temperature": 0.0, "avg_logprob": -0.19190763326791616, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.062171127647161484}, {"id": 134, "seek": 55440, "start": 554.4, "end": 559.48, "text": " You have to show it lots of examples, change your weight, and then show it lots more examples,", "tokens": [50364, 509, 362, 281, 855, 309, 3195, 295, 5110, 11, 1319, 428, 3364, 11, 293, 550, 855, 309, 3195, 544, 5110, 11, 50618], "temperature": 0.0, "avg_logprob": -0.15567732693856223, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.04874519631266594}, {"id": 135, "seek": 55440, "start": 559.48, "end": 560.48, "text": " change another weight.", "tokens": [50618, 1319, 1071, 3364, 13, 50668], "temperature": 0.0, "avg_logprob": -0.15567732693856223, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.04874519631266594}, {"id": 136, "seek": 55440, "start": 560.48, "end": 562.48, "text": " And every weight has to be changed many times.", "tokens": [50668, 400, 633, 3364, 575, 281, 312, 3105, 867, 1413, 13, 50768], "temperature": 0.0, "avg_logprob": -0.15567732693856223, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.04874519631266594}, {"id": 137, "seek": 55440, "start": 562.48, "end": 568.84, "text": " So if you use calculus, you can go millions of times faster.", "tokens": [50768, 407, 498, 291, 764, 33400, 11, 291, 393, 352, 6803, 295, 1413, 4663, 13, 51086], "temperature": 0.0, "avg_logprob": -0.15567732693856223, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.04874519631266594}, {"id": 138, "seek": 55440, "start": 568.84, "end": 572.3199999999999, "text": " So the trick of this algorithm, the sort of mutation algorithm, is you have to measure", "tokens": [51086, 407, 264, 4282, 295, 341, 9284, 11, 264, 1333, 295, 27960, 9284, 11, 307, 291, 362, 281, 3481, 51260], "temperature": 0.0, "avg_logprob": -0.15567732693856223, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.04874519631266594}, {"id": 139, "seek": 55440, "start": 572.3199999999999, "end": 574.6, "text": " the effect of the weight change on the performance.", "tokens": [51260, 264, 1802, 295, 264, 3364, 1319, 322, 264, 3389, 13, 51374], "temperature": 0.0, "avg_logprob": -0.15567732693856223, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.04874519631266594}, {"id": 140, "seek": 55440, "start": 574.6, "end": 582.0799999999999, "text": " But you don't really need to measure it, because when I change one of these weights, the effect", "tokens": [51374, 583, 291, 500, 380, 534, 643, 281, 3481, 309, 11, 570, 562, 286, 1319, 472, 295, 613, 17443, 11, 264, 1802, 51748], "temperature": 0.0, "avg_logprob": -0.15567732693856223, "compression_ratio": 1.8775510204081634, "no_speech_prob": 0.04874519631266594}, {"id": 141, "seek": 58208, "start": 582.08, "end": 585.44, "text": " that it has on the output is determined by the network.", "tokens": [50364, 300, 309, 575, 322, 264, 5598, 307, 9540, 538, 264, 3209, 13, 50532], "temperature": 0.0, "avg_logprob": -0.1296162872314453, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.32679423689842224}, {"id": 142, "seek": 58208, "start": 585.44, "end": 587.6, "text": " It just depends on the other weights in the network.", "tokens": [50532, 467, 445, 5946, 322, 264, 661, 17443, 294, 264, 3209, 13, 50640], "temperature": 0.0, "avg_logprob": -0.1296162872314453, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.32679423689842224}, {"id": 143, "seek": 58208, "start": 587.6, "end": 591.0, "text": " It's not like normal evolution, where the effect of a gene depends on the environment", "tokens": [50640, 467, 311, 406, 411, 2710, 9303, 11, 689, 264, 1802, 295, 257, 12186, 5946, 322, 264, 2823, 50810], "temperature": 0.0, "avg_logprob": -0.1296162872314453, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.32679423689842224}, {"id": 144, "seek": 58208, "start": 591.0, "end": 592.0, "text": " you're in.", "tokens": [50810, 291, 434, 294, 13, 50860], "temperature": 0.0, "avg_logprob": -0.1296162872314453, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.32679423689842224}, {"id": 145, "seek": 58208, "start": 592.0, "end": 594.0, "text": " This is all kind of internal to the brain.", "tokens": [50860, 639, 307, 439, 733, 295, 6920, 281, 264, 3567, 13, 50960], "temperature": 0.0, "avg_logprob": -0.1296162872314453, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.32679423689842224}, {"id": 146, "seek": 58208, "start": 594.0, "end": 597.72, "text": " And so changing one of these weights has an effect that's predictable here.", "tokens": [50960, 400, 370, 4473, 472, 295, 613, 17443, 575, 364, 1802, 300, 311, 27737, 510, 13, 51146], "temperature": 0.0, "avg_logprob": -0.1296162872314453, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.32679423689842224}, {"id": 147, "seek": 58208, "start": 597.72, "end": 603.1600000000001, "text": " So I ought to be able to predict how changing the weight will help get the right output.", "tokens": [51146, 407, 286, 13416, 281, 312, 1075, 281, 6069, 577, 4473, 264, 3364, 486, 854, 483, 264, 558, 5598, 13, 51418], "temperature": 0.0, "avg_logprob": -0.1296162872314453, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.32679423689842224}, {"id": 148, "seek": 58208, "start": 603.1600000000001, "end": 608.9200000000001, "text": " And so what back propagation does is basically says, I'm going to compute using an algorithm,", "tokens": [51418, 400, 370, 437, 646, 38377, 775, 307, 1936, 1619, 11, 286, 478, 516, 281, 14722, 1228, 364, 9284, 11, 51706], "temperature": 0.0, "avg_logprob": -0.1296162872314453, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.32679423689842224}, {"id": 149, "seek": 60892, "start": 608.92, "end": 612.4, "text": " the details of which I won't tell you, and compute for every weight, all at the same", "tokens": [50364, 264, 4365, 295, 597, 286, 1582, 380, 980, 291, 11, 293, 14722, 337, 633, 3364, 11, 439, 412, 264, 912, 50538], "temperature": 0.0, "avg_logprob": -0.15403786301612854, "compression_ratio": 1.941860465116279, "no_speech_prob": 0.05274592712521553}, {"id": 150, "seek": 60892, "start": 612.4, "end": 617.3199999999999, "text": " time, how changing that weight would improve the output.", "tokens": [50538, 565, 11, 577, 4473, 300, 3364, 576, 3470, 264, 5598, 13, 50784], "temperature": 0.0, "avg_logprob": -0.15403786301612854, "compression_ratio": 1.941860465116279, "no_speech_prob": 0.05274592712521553}, {"id": 151, "seek": 60892, "start": 617.3199999999999, "end": 619.92, "text": " And then I'm going to change all the weights a little bit.", "tokens": [50784, 400, 550, 286, 478, 516, 281, 1319, 439, 264, 17443, 257, 707, 857, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15403786301612854, "compression_ratio": 1.941860465116279, "no_speech_prob": 0.05274592712521553}, {"id": 152, "seek": 60892, "start": 619.92, "end": 622.8399999999999, "text": " So every weight changes in direction to improve the output, and the output improves quite", "tokens": [50914, 407, 633, 3364, 2962, 294, 3513, 281, 3470, 264, 5598, 11, 293, 264, 5598, 24771, 1596, 51060], "temperature": 0.0, "avg_logprob": -0.15403786301612854, "compression_ratio": 1.941860465116279, "no_speech_prob": 0.05274592712521553}, {"id": 153, "seek": 60892, "start": 622.8399999999999, "end": 626.4399999999999, "text": " a bit, and then I do it all again.", "tokens": [51060, 257, 857, 11, 293, 550, 286, 360, 309, 439, 797, 13, 51240], "temperature": 0.0, "avg_logprob": -0.15403786301612854, "compression_ratio": 1.941860465116279, "no_speech_prob": 0.05274592712521553}, {"id": 154, "seek": 60892, "start": 626.4399999999999, "end": 633.76, "text": " Now that allows me to compute for every weight what direction I'd like to change it in.", "tokens": [51240, 823, 300, 4045, 385, 281, 14722, 337, 633, 3364, 437, 3513, 286, 1116, 411, 281, 1319, 309, 294, 13, 51606], "temperature": 0.0, "avg_logprob": -0.15403786301612854, "compression_ratio": 1.941860465116279, "no_speech_prob": 0.05274592712521553}, {"id": 155, "seek": 60892, "start": 633.76, "end": 638.56, "text": " And the question is, should I, when I show examples, show all of the examples, and then", "tokens": [51606, 400, 264, 1168, 307, 11, 820, 286, 11, 562, 286, 855, 5110, 11, 855, 439, 295, 264, 5110, 11, 293, 550, 51846], "temperature": 0.0, "avg_logprob": -0.15403786301612854, "compression_ratio": 1.941860465116279, "no_speech_prob": 0.05274592712521553}, {"id": 156, "seek": 63856, "start": 638.56, "end": 641.9599999999999, "text": " update the weights, so should you live your whole life with the synapse strengths you're", "tokens": [50364, 5623, 264, 17443, 11, 370, 820, 291, 1621, 428, 1379, 993, 365, 264, 5451, 11145, 16986, 291, 434, 50534], "temperature": 0.0, "avg_logprob": -0.16946228400810615, "compression_ratio": 1.9853479853479854, "no_speech_prob": 0.04454773664474487}, {"id": 157, "seek": 63856, "start": 641.9599999999999, "end": 645.5999999999999, "text": " born with, then update your weights a little bit, then live your life again, and update", "tokens": [50534, 4232, 365, 11, 550, 5623, 428, 17443, 257, 707, 857, 11, 550, 1621, 428, 993, 797, 11, 293, 5623, 50716], "temperature": 0.0, "avg_logprob": -0.16946228400810615, "compression_ratio": 1.9853479853479854, "no_speech_prob": 0.04454773664474487}, {"id": 158, "seek": 63856, "start": 645.5999999999999, "end": 654.1199999999999, "text": " the weights a little bit more, that doesn't seem very good, or should you take one case", "tokens": [50716, 264, 17443, 257, 707, 857, 544, 11, 300, 1177, 380, 1643, 588, 665, 11, 420, 820, 291, 747, 472, 1389, 51142], "temperature": 0.0, "avg_logprob": -0.16946228400810615, "compression_ratio": 1.9853479853479854, "no_speech_prob": 0.04454773664474487}, {"id": 159, "seek": 63856, "start": 654.1199999999999, "end": 658.1199999999999, "text": " or a few cases, figure out how you'd like to update the weights, update them, and then", "tokens": [51142, 420, 257, 1326, 3331, 11, 2573, 484, 577, 291, 1116, 411, 281, 5623, 264, 17443, 11, 5623, 552, 11, 293, 550, 51342], "temperature": 0.0, "avg_logprob": -0.16946228400810615, "compression_ratio": 1.9853479853479854, "no_speech_prob": 0.04454773664474487}, {"id": 160, "seek": 63856, "start": 658.1199999999999, "end": 659.1199999999999, "text": " take more cases.", "tokens": [51342, 747, 544, 3331, 13, 51392], "temperature": 0.0, "avg_logprob": -0.16946228400810615, "compression_ratio": 1.9853479853479854, "no_speech_prob": 0.04454773664474487}, {"id": 161, "seek": 63856, "start": 659.1199999999999, "end": 662.9599999999999, "text": " That's the online algorithm, and that's what we do.", "tokens": [51392, 663, 311, 264, 2950, 9284, 11, 293, 300, 311, 437, 321, 360, 13, 51584], "temperature": 0.0, "avg_logprob": -0.16946228400810615, "compression_ratio": 1.9853479853479854, "no_speech_prob": 0.04454773664474487}, {"id": 162, "seek": 63856, "start": 662.9599999999999, "end": 664.92, "text": " And the amazing thing is, it works.", "tokens": [51584, 400, 264, 2243, 551, 307, 11, 309, 1985, 13, 51682], "temperature": 0.0, "avg_logprob": -0.16946228400810615, "compression_ratio": 1.9853479853479854, "no_speech_prob": 0.04454773664474487}, {"id": 163, "seek": 63856, "start": 664.92, "end": 668.1199999999999, "text": " You can take one case at a time, or you can take small batch of cases, you update the", "tokens": [51682, 509, 393, 747, 472, 1389, 412, 257, 565, 11, 420, 291, 393, 747, 1359, 15245, 295, 3331, 11, 291, 5623, 264, 51842], "temperature": 0.0, "avg_logprob": -0.16946228400810615, "compression_ratio": 1.9853479853479854, "no_speech_prob": 0.04454773664474487}, {"id": 164, "seek": 66812, "start": 668.12, "end": 671.64, "text": " weights, and these networks get better.", "tokens": [50364, 17443, 11, 293, 613, 9590, 483, 1101, 13, 50540], "temperature": 0.0, "avg_logprob": -0.13729293206158807, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.0350588895380497}, {"id": 165, "seek": 66812, "start": 671.64, "end": 675.2, "text": " And it's very surprising how well it works on big data sets.", "tokens": [50540, 400, 309, 311, 588, 8830, 577, 731, 309, 1985, 322, 955, 1412, 6352, 13, 50718], "temperature": 0.0, "avg_logprob": -0.13729293206158807, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.0350588895380497}, {"id": 166, "seek": 66812, "start": 675.2, "end": 680.88, "text": " So for a long time, people thought, you're never going to be able to learn something", "tokens": [50718, 407, 337, 257, 938, 565, 11, 561, 1194, 11, 291, 434, 1128, 516, 281, 312, 1075, 281, 1466, 746, 51002], "temperature": 0.0, "avg_logprob": -0.13729293206158807, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.0350588895380497}, {"id": 167, "seek": 66812, "start": 680.88, "end": 686.5600000000001, "text": " complicated, like, for example, take a string of words in English, feed them into a neural", "tokens": [51002, 6179, 11, 411, 11, 337, 1365, 11, 747, 257, 6798, 295, 2283, 294, 3669, 11, 3154, 552, 666, 257, 18161, 51286], "temperature": 0.0, "avg_logprob": -0.13729293206158807, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.0350588895380497}, {"id": 168, "seek": 66812, "start": 686.5600000000001, "end": 691.32, "text": " net, and output a string of words in French that mean the same thing.", "tokens": [51286, 2533, 11, 293, 5598, 257, 6798, 295, 2283, 294, 5522, 300, 914, 264, 912, 551, 13, 51524], "temperature": 0.0, "avg_logprob": -0.13729293206158807, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.0350588895380497}, {"id": 169, "seek": 66812, "start": 691.32, "end": 693.6, "text": " You're never going to be able to do that if you start with a big neural net with just", "tokens": [51524, 509, 434, 1128, 516, 281, 312, 1075, 281, 360, 300, 498, 291, 722, 365, 257, 955, 18161, 2533, 365, 445, 51638], "temperature": 0.0, "avg_logprob": -0.13729293206158807, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.0350588895380497}, {"id": 170, "seek": 66812, "start": 693.6, "end": 694.6, "text": " random weights.", "tokens": [51638, 4974, 17443, 13, 51688], "temperature": 0.0, "avg_logprob": -0.13729293206158807, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.0350588895380497}, {"id": 171, "seek": 66812, "start": 694.6, "end": 698.08, "text": " It's just asking too much for the neural net to organize itself so it can do transactions", "tokens": [51688, 467, 311, 445, 3365, 886, 709, 337, 264, 18161, 2533, 281, 13859, 2564, 370, 309, 393, 360, 16856, 51862], "temperature": 0.0, "avg_logprob": -0.13729293206158807, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.0350588895380497}, {"id": 172, "seek": 69808, "start": 699.0400000000001, "end": 703.12, "text": " because you have to kind of understand what the English says.", "tokens": [50412, 570, 291, 362, 281, 733, 295, 1223, 437, 264, 3669, 1619, 13, 50616], "temperature": 0.0, "avg_logprob": -0.14976467405046737, "compression_ratio": 1.5703703703703704, "no_speech_prob": 0.03556283935904503}, {"id": 173, "seek": 69808, "start": 703.12, "end": 707.9200000000001, "text": " And people predicted this was completely impossible, but you'd have to put in lots of prior knowledge.", "tokens": [50616, 400, 561, 19147, 341, 390, 2584, 6243, 11, 457, 291, 1116, 362, 281, 829, 294, 3195, 295, 4059, 3601, 13, 50856], "temperature": 0.0, "avg_logprob": -0.14976467405046737, "compression_ratio": 1.5703703703703704, "no_speech_prob": 0.03556283935904503}, {"id": 174, "seek": 69808, "start": 707.9200000000001, "end": 712.6800000000001, "text": " Well, they were wrong.", "tokens": [50856, 1042, 11, 436, 645, 2085, 13, 51094], "temperature": 0.0, "avg_logprob": -0.14976467405046737, "compression_ratio": 1.5703703703703704, "no_speech_prob": 0.03556283935904503}, {"id": 175, "seek": 69808, "start": 712.6800000000001, "end": 720.2, "text": " So in 2009, my students in Toronto showed that you could actually improve speech recognizers", "tokens": [51094, 407, 294, 11453, 11, 452, 1731, 294, 14140, 4712, 300, 291, 727, 767, 3470, 6218, 3068, 22525, 51470], "temperature": 0.0, "avg_logprob": -0.14976467405046737, "compression_ratio": 1.5703703703703704, "no_speech_prob": 0.03556283935904503}, {"id": 176, "seek": 69808, "start": 720.2, "end": 722.8000000000001, "text": " using these neural nets that had random weights.", "tokens": [51470, 1228, 613, 18161, 36170, 300, 632, 4974, 17443, 13, 51600], "temperature": 0.0, "avg_logprob": -0.14976467405046737, "compression_ratio": 1.5703703703703704, "no_speech_prob": 0.03556283935904503}, {"id": 177, "seek": 69808, "start": 722.8000000000001, "end": 727.5200000000001, "text": " They were just trying to predict in a spectrogram which piece of which phoneme you were trying", "tokens": [51600, 814, 645, 445, 1382, 281, 6069, 294, 257, 6177, 340, 1342, 597, 2522, 295, 597, 30754, 5729, 291, 645, 1382, 51836], "temperature": 0.0, "avg_logprob": -0.14976467405046737, "compression_ratio": 1.5703703703703704, "no_speech_prob": 0.03556283935904503}, {"id": 178, "seek": 72752, "start": 727.52, "end": 729.76, "text": " to say in the middle of the spectrogram.", "tokens": [50364, 281, 584, 294, 264, 2808, 295, 264, 6177, 340, 1342, 13, 50476], "temperature": 0.0, "avg_logprob": -0.15174117455115685, "compression_ratio": 1.905579399141631, "no_speech_prob": 0.004797524306923151}, {"id": 179, "seek": 72752, "start": 729.76, "end": 733.4399999999999, "text": " And then there was more to the system that wasn't neural nets.", "tokens": [50476, 400, 550, 456, 390, 544, 281, 264, 1185, 300, 2067, 380, 18161, 36170, 13, 50660], "temperature": 0.0, "avg_logprob": -0.15174117455115685, "compression_ratio": 1.905579399141631, "no_speech_prob": 0.004797524306923151}, {"id": 180, "seek": 72752, "start": 733.4399999999999, "end": 737.64, "text": " Now what we've done is we've got rid of all the stuff that wasn't neural nets, and now", "tokens": [50660, 823, 437, 321, 600, 1096, 307, 321, 600, 658, 3973, 295, 439, 264, 1507, 300, 2067, 380, 18161, 36170, 11, 293, 586, 50870], "temperature": 0.0, "avg_logprob": -0.15174117455115685, "compression_ratio": 1.905579399141631, "no_speech_prob": 0.004797524306923151}, {"id": 181, "seek": 72752, "start": 737.64, "end": 743.96, "text": " you can take sound waves coming in and you have transcriptions coming out, or even better,", "tokens": [50870, 291, 393, 747, 1626, 9417, 1348, 294, 293, 291, 362, 24444, 626, 1348, 484, 11, 420, 754, 1101, 11, 51186], "temperature": 0.0, "avg_logprob": -0.15174117455115685, "compression_ratio": 1.905579399141631, "no_speech_prob": 0.004797524306923151}, {"id": 182, "seek": 72752, "start": 743.96, "end": 747.6, "text": " you have sound waves coming in and you have sound waves coming out in another language", "tokens": [51186, 291, 362, 1626, 9417, 1348, 294, 293, 291, 362, 1626, 9417, 1348, 484, 294, 1071, 2856, 51368], "temperature": 0.0, "avg_logprob": -0.15174117455115685, "compression_ratio": 1.905579399141631, "no_speech_prob": 0.004797524306923151}, {"id": 183, "seek": 72752, "start": 747.6, "end": 749.96, "text": " with the same accent.", "tokens": [51368, 365, 264, 912, 11982, 13, 51486], "temperature": 0.0, "avg_logprob": -0.15174117455115685, "compression_ratio": 1.905579399141631, "no_speech_prob": 0.004797524306923151}, {"id": 184, "seek": 72752, "start": 749.96, "end": 751.8, "text": " They can do that now.", "tokens": [51486, 814, 393, 360, 300, 586, 13, 51578], "temperature": 0.0, "avg_logprob": -0.15174117455115685, "compression_ratio": 1.905579399141631, "no_speech_prob": 0.004797524306923151}, {"id": 185, "seek": 72752, "start": 751.8, "end": 754.0, "text": " That's speech recognition done.", "tokens": [51578, 663, 311, 6218, 11150, 1096, 13, 51688], "temperature": 0.0, "avg_logprob": -0.15174117455115685, "compression_ratio": 1.905579399141631, "no_speech_prob": 0.004797524306923151}, {"id": 186, "seek": 75400, "start": 754.0, "end": 760.04, "text": " And in 2012, two of my students took a big database of images and used essentially the", "tokens": [50364, 400, 294, 9125, 11, 732, 295, 452, 1731, 1890, 257, 955, 8149, 295, 5267, 293, 1143, 4476, 264, 50666], "temperature": 0.0, "avg_logprob": -0.13720397110823745, "compression_ratio": 1.5491803278688525, "no_speech_prob": 0.014737228862941265}, {"id": 187, "seek": 75400, "start": 760.04, "end": 765.96, "text": " same algorithm, the few clever tricks, to say what was in the image.", "tokens": [50666, 912, 9284, 11, 264, 1326, 13494, 11733, 11, 281, 584, 437, 390, 294, 264, 3256, 13, 50962], "temperature": 0.0, "avg_logprob": -0.13720397110823745, "compression_ratio": 1.5491803278688525, "no_speech_prob": 0.014737228862941265}, {"id": 188, "seek": 75400, "start": 765.96, "end": 771.32, "text": " Not a full caption, just the class of the most obvious object, and they did much better", "tokens": [50962, 1726, 257, 1577, 31974, 11, 445, 264, 1508, 295, 264, 881, 6322, 2657, 11, 293, 436, 630, 709, 1101, 51230], "temperature": 0.0, "avg_logprob": -0.13720397110823745, "compression_ratio": 1.5491803278688525, "no_speech_prob": 0.014737228862941265}, {"id": 189, "seek": 75400, "start": 771.32, "end": 775.72, "text": " than conventional computer vision, which had been going for many years, and since then", "tokens": [51230, 813, 16011, 3820, 5201, 11, 597, 632, 668, 516, 337, 867, 924, 11, 293, 1670, 550, 51450], "temperature": 0.0, "avg_logprob": -0.13720397110823745, "compression_ratio": 1.5491803278688525, "no_speech_prob": 0.014737228862941265}, {"id": 190, "seek": 75400, "start": 775.72, "end": 778.6, "text": " all the best recognizers have used neural nets.", "tokens": [51450, 439, 264, 1151, 3068, 22525, 362, 1143, 18161, 36170, 13, 51594], "temperature": 0.0, "avg_logprob": -0.13720397110823745, "compression_ratio": 1.5491803278688525, "no_speech_prob": 0.014737228862941265}, {"id": 191, "seek": 77860, "start": 778.6, "end": 783.88, "text": " In 2011, you couldn't publish a paper out neural nets in the standard computer vision", "tokens": [50364, 682, 10154, 11, 291, 2809, 380, 11374, 257, 3035, 484, 18161, 36170, 294, 264, 3832, 3820, 5201, 50628], "temperature": 0.0, "avg_logprob": -0.2544706726074219, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.027251090854406357}, {"id": 192, "seek": 77860, "start": 783.88, "end": 786.24, "text": " conference because they said they were rubbish.", "tokens": [50628, 7586, 570, 436, 848, 436, 645, 29978, 13, 50746], "temperature": 0.0, "avg_logprob": -0.2544706726074219, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.027251090854406357}, {"id": 193, "seek": 77860, "start": 786.24, "end": 791.72, "text": " In 2014, you couldn't publish a paper that wasn't about neural nets.", "tokens": [50746, 682, 8227, 11, 291, 2809, 380, 11374, 257, 3035, 300, 2067, 380, 466, 18161, 36170, 13, 51020], "temperature": 0.0, "avg_logprob": -0.2544706726074219, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.027251090854406357}, {"id": 194, "seek": 77860, "start": 791.72, "end": 795.0, "text": " And in 2014, they did something that I didn't expect.", "tokens": [51020, 400, 294, 8227, 11, 436, 630, 746, 300, 286, 994, 380, 2066, 13, 51184], "temperature": 0.0, "avg_logprob": -0.2544706726074219, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.027251090854406357}, {"id": 195, "seek": 77860, "start": 795.0, "end": 800.12, "text": " This was done by people at Google, not me, and Joshua Benjo in his group in Montreal,", "tokens": [51184, 639, 390, 1096, 538, 561, 412, 3329, 11, 406, 385, 11, 293, 24005, 3964, 5134, 294, 702, 1594, 294, 34180, 11, 51440], "temperature": 0.0, "avg_logprob": -0.2544706726074219, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.027251090854406357}, {"id": 196, "seek": 77860, "start": 800.12, "end": 804.0, "text": " particularly by a guy called Bardenao and Cho.", "tokens": [51440, 4098, 538, 257, 2146, 1219, 26841, 4118, 78, 293, 12366, 13, 51634], "temperature": 0.0, "avg_logprob": -0.2544706726074219, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.027251090854406357}, {"id": 197, "seek": 80400, "start": 804.0, "end": 811.36, "text": " They managed to get a neural net, so you feed in actually fragments of words in one language.", "tokens": [50364, 814, 6453, 281, 483, 257, 18161, 2533, 11, 370, 291, 3154, 294, 767, 29197, 295, 2283, 294, 472, 2856, 13, 50732], "temperature": 0.0, "avg_logprob": -0.1990169949001736, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.05822822079062462}, {"id": 198, "seek": 80400, "start": 811.36, "end": 813.56, "text": " You have 32,000 possible fragments.", "tokens": [50732, 509, 362, 8858, 11, 1360, 1944, 29197, 13, 50842], "temperature": 0.0, "avg_logprob": -0.1990169949001736, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.05822822079062462}, {"id": 199, "seek": 80400, "start": 813.56, "end": 816.96, "text": " So the word the in English would be one of the fragments, but so with things like ing", "tokens": [50842, 407, 264, 1349, 264, 294, 3669, 576, 312, 472, 295, 264, 29197, 11, 457, 370, 365, 721, 411, 3957, 51012], "temperature": 0.0, "avg_logprob": -0.1990169949001736, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.05822822079062462}, {"id": 200, "seek": 80400, "start": 816.96, "end": 818.76, "text": " and un.", "tokens": [51012, 293, 517, 13, 51102], "temperature": 0.0, "avg_logprob": -0.1990169949001736, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.05822822079062462}, {"id": 201, "seek": 80400, "start": 818.76, "end": 822.36, "text": " And what comes out in another language is fragments of words in that other language, and it's", "tokens": [51102, 400, 437, 1487, 484, 294, 1071, 2856, 307, 29197, 295, 2283, 294, 300, 661, 2856, 11, 293, 309, 311, 51282], "temperature": 0.0, "avg_logprob": -0.1990169949001736, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.05822822079062462}, {"id": 202, "seek": 80400, "start": 822.36, "end": 826.8, "text": " a pretty good translation, and that's how Google now does translation.", "tokens": [51282, 257, 1238, 665, 12853, 11, 293, 300, 311, 577, 3329, 586, 775, 12853, 13, 51504], "temperature": 0.0, "avg_logprob": -0.1990169949001736, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.05822822079062462}, {"id": 203, "seek": 80400, "start": 826.8, "end": 833.24, "text": " So it did translation better than symbolic AI.", "tokens": [51504, 407, 309, 630, 12853, 1101, 813, 25755, 7318, 13, 51826], "temperature": 0.0, "avg_logprob": -0.1990169949001736, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.05822822079062462}, {"id": 204, "seek": 83324, "start": 833.24, "end": 836.5600000000001, "text": " So what changed between 1986 and 2009?", "tokens": [50364, 407, 437, 3105, 1296, 27895, 293, 11453, 30, 50530], "temperature": 0.0, "avg_logprob": -0.21846065521240235, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.024938080459833145}, {"id": 205, "seek": 83324, "start": 836.5600000000001, "end": 838.52, "text": " And it was basically computers got faster.", "tokens": [50530, 400, 309, 390, 1936, 10807, 658, 4663, 13, 50628], "temperature": 0.0, "avg_logprob": -0.21846065521240235, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.024938080459833145}, {"id": 206, "seek": 83324, "start": 838.52, "end": 839.52, "text": " That was the main change.", "tokens": [50628, 663, 390, 264, 2135, 1319, 13, 50678], "temperature": 0.0, "avg_logprob": -0.21846065521240235, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.024938080459833145}, {"id": 207, "seek": 83324, "start": 839.52, "end": 840.96, "text": " Data sets got bigger.", "tokens": [50678, 11888, 6352, 658, 3801, 13, 50750], "temperature": 0.0, "avg_logprob": -0.21846065521240235, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.024938080459833145}, {"id": 208, "seek": 83324, "start": 840.96, "end": 845.44, "text": " We developed some clever tricks, and we like to emphasize those, but it was really the", "tokens": [50750, 492, 4743, 512, 13494, 11733, 11, 293, 321, 411, 281, 16078, 729, 11, 457, 309, 390, 534, 264, 50974], "temperature": 0.0, "avg_logprob": -0.21846065521240235, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.024938080459833145}, {"id": 209, "seek": 83324, "start": 845.44, "end": 847.48, "text": " computers getting faster and data sets getting bigger.", "tokens": [50974, 10807, 1242, 4663, 293, 1412, 6352, 1242, 3801, 13, 51076], "temperature": 0.0, "avg_logprob": -0.21846065521240235, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.024938080459833145}, {"id": 210, "seek": 83324, "start": 847.48, "end": 850.52, "text": " But I'll emphasize the clever tricks nonetheless.", "tokens": [51076, 583, 286, 603, 16078, 264, 13494, 11733, 26756, 13, 51228], "temperature": 0.0, "avg_logprob": -0.21846065521240235, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.024938080459833145}, {"id": 211, "seek": 83324, "start": 850.52, "end": 851.92, "text": " And I can tell you about two clever tricks.", "tokens": [51228, 400, 286, 393, 980, 291, 466, 732, 13494, 11733, 13, 51298], "temperature": 0.0, "avg_logprob": -0.21846065521240235, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.024938080459833145}, {"id": 212, "seek": 83324, "start": 851.92, "end": 855.28, "text": " I can tell you about transformers, and I can tell you about better ways of stopping neural", "tokens": [51298, 286, 393, 980, 291, 466, 4088, 433, 11, 293, 286, 393, 980, 291, 466, 1101, 2098, 295, 12767, 18161, 51466], "temperature": 0.0, "avg_logprob": -0.21846065521240235, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.024938080459833145}, {"id": 213, "seek": 83324, "start": 855.28, "end": 859.32, "text": " networks from overfitting.", "tokens": [51466, 9590, 490, 670, 69, 2414, 13, 51668], "temperature": 0.0, "avg_logprob": -0.21846065521240235, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.024938080459833145}, {"id": 214, "seek": 85932, "start": 859.32, "end": 865.5200000000001, "text": " But first I want to show you an example of what neural nets can do now.", "tokens": [50364, 583, 700, 286, 528, 281, 855, 291, 364, 1365, 295, 437, 18161, 36170, 393, 360, 586, 13, 50674], "temperature": 0.0, "avg_logprob": -0.14432430267333984, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.007329869549721479}, {"id": 215, "seek": 85932, "start": 865.5200000000001, "end": 872.4000000000001, "text": " So a team at OpenAI took work on transformers that was originally done at Google.", "tokens": [50674, 407, 257, 1469, 412, 7238, 48698, 1890, 589, 322, 4088, 433, 300, 390, 7993, 1096, 412, 3329, 13, 51018], "temperature": 0.0, "avg_logprob": -0.14432430267333984, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.007329869549721479}, {"id": 216, "seek": 85932, "start": 872.4000000000001, "end": 876.44, "text": " They developed it a little bit further, and they applied it to big neural nets that have", "tokens": [51018, 814, 4743, 309, 257, 707, 857, 3052, 11, 293, 436, 6456, 309, 281, 955, 18161, 36170, 300, 362, 51220], "temperature": 0.0, "avg_logprob": -0.14432430267333984, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.007329869549721479}, {"id": 217, "seek": 85932, "start": 876.44, "end": 881.48, "text": " 1.5 billion learnable connection strengths.", "tokens": [51220, 502, 13, 20, 5218, 1466, 712, 4984, 16986, 13, 51472], "temperature": 0.0, "avg_logprob": -0.14432430267333984, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.007329869549721479}, {"id": 218, "seek": 85932, "start": 881.48, "end": 883.5600000000001, "text": " So they're learning 1.5 billion numbers.", "tokens": [51472, 407, 436, 434, 2539, 502, 13, 20, 5218, 3547, 13, 51576], "temperature": 0.0, "avg_logprob": -0.14432430267333984, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.007329869549721479}, {"id": 219, "seek": 85932, "start": 883.5600000000001, "end": 885.2800000000001, "text": " That's the knowledge of the system.", "tokens": [51576, 663, 311, 264, 3601, 295, 264, 1185, 13, 51662], "temperature": 0.0, "avg_logprob": -0.14432430267333984, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.007329869549721479}, {"id": 220, "seek": 88528, "start": 885.28, "end": 890.92, "text": " And they train it up on billions of words of English text, and all the net's trying", "tokens": [50364, 400, 436, 3847, 309, 493, 322, 17375, 295, 2283, 295, 3669, 2487, 11, 293, 439, 264, 2533, 311, 1382, 50646], "temperature": 0.0, "avg_logprob": -0.14652030886584566, "compression_ratio": 2.0340425531914894, "no_speech_prob": 0.061059385538101196}, {"id": 221, "seek": 88528, "start": 890.92, "end": 893.92, "text": " to do is predict the next word.", "tokens": [50646, 281, 360, 307, 6069, 264, 958, 1349, 13, 50796], "temperature": 0.0, "avg_logprob": -0.14652030886584566, "compression_ratio": 2.0340425531914894, "no_speech_prob": 0.061059385538101196}, {"id": 222, "seek": 88528, "start": 893.92, "end": 898.8, "text": " So what the net will do, or fragment of word, the net will give you probabilities for the", "tokens": [50796, 407, 437, 264, 2533, 486, 360, 11, 420, 26424, 295, 1349, 11, 264, 2533, 486, 976, 291, 33783, 337, 264, 51040], "temperature": 0.0, "avg_logprob": -0.14652030886584566, "compression_ratio": 2.0340425531914894, "no_speech_prob": 0.061059385538101196}, {"id": 223, "seek": 88528, "start": 898.8, "end": 899.8, "text": " next word.", "tokens": [51040, 958, 1349, 13, 51090], "temperature": 0.0, "avg_logprob": -0.14652030886584566, "compression_ratio": 2.0340425531914894, "no_speech_prob": 0.061059385538101196}, {"id": 224, "seek": 88528, "start": 899.8, "end": 903.9599999999999, "text": " So if you give it some words, a lead-in, it'll give you probabilities for the next word.", "tokens": [51090, 407, 498, 291, 976, 309, 512, 2283, 11, 257, 1477, 12, 259, 11, 309, 603, 976, 291, 33783, 337, 264, 958, 1349, 13, 51298], "temperature": 0.0, "avg_logprob": -0.14652030886584566, "compression_ratio": 2.0340425531914894, "no_speech_prob": 0.061059385538101196}, {"id": 225, "seek": 88528, "start": 903.9599999999999, "end": 908.04, "text": " And once the net's trained, what you can do is you can look at those probabilities,", "tokens": [51298, 400, 1564, 264, 2533, 311, 8895, 11, 437, 291, 393, 360, 307, 291, 393, 574, 412, 729, 33783, 11, 51502], "temperature": 0.0, "avg_logprob": -0.14652030886584566, "compression_ratio": 2.0340425531914894, "no_speech_prob": 0.061059385538101196}, {"id": 226, "seek": 88528, "start": 908.04, "end": 913.8, "text": " and if it says there's a probability of 0.4 that the next word is the, you pick the with", "tokens": [51502, 293, 498, 309, 1619, 456, 311, 257, 8482, 295, 1958, 13, 19, 300, 264, 958, 1349, 307, 264, 11, 291, 1888, 264, 365, 51790], "temperature": 0.0, "avg_logprob": -0.14652030886584566, "compression_ratio": 2.0340425531914894, "no_speech_prob": 0.061059385538101196}, {"id": 227, "seek": 91380, "start": 913.8, "end": 919.0799999999999, "text": " probability 9.4, and if it says fish with probability 0.01, you pick fish with probability", "tokens": [50364, 8482, 1722, 13, 19, 11, 293, 498, 309, 1619, 3506, 365, 8482, 1958, 13, 10607, 11, 291, 1888, 3506, 365, 8482, 50628], "temperature": 0.0, "avg_logprob": -0.1671158183704723, "compression_ratio": 1.7849462365591398, "no_speech_prob": 0.08335566520690918}, {"id": 228, "seek": 91380, "start": 919.0799999999999, "end": 920.0799999999999, "text": " 0.01.", "tokens": [50628, 1958, 13, 10607, 13, 50678], "temperature": 0.0, "avg_logprob": -0.1671158183704723, "compression_ratio": 1.7849462365591398, "no_speech_prob": 0.08335566520690918}, {"id": 229, "seek": 91380, "start": 920.0799999999999, "end": 924.16, "text": " And so you just pick from its distribution, and then you tell the neural net, okay, the", "tokens": [50678, 400, 370, 291, 445, 1888, 490, 1080, 7316, 11, 293, 550, 291, 980, 264, 18161, 2533, 11, 1392, 11, 264, 50882], "temperature": 0.0, "avg_logprob": -0.1671158183704723, "compression_ratio": 1.7849462365591398, "no_speech_prob": 0.08335566520690918}, {"id": 230, "seek": 91380, "start": 924.16, "end": 927.8399999999999, "text": " one I picked was the next word, what do you think comes after that?", "tokens": [50882, 472, 286, 6183, 390, 264, 958, 1349, 11, 437, 360, 291, 519, 1487, 934, 300, 30, 51066], "temperature": 0.0, "avg_logprob": -0.1671158183704723, "compression_ratio": 1.7849462365591398, "no_speech_prob": 0.08335566520690918}, {"id": 231, "seek": 91380, "start": 927.8399999999999, "end": 931.92, "text": " And this way you can get it to sort of reveal what it really believes about the world.", "tokens": [51066, 400, 341, 636, 291, 393, 483, 309, 281, 1333, 295, 10658, 437, 309, 534, 12307, 466, 264, 1002, 13, 51270], "temperature": 0.0, "avg_logprob": -0.1671158183704723, "compression_ratio": 1.7849462365591398, "no_speech_prob": 0.08335566520690918}, {"id": 232, "seek": 91380, "start": 931.92, "end": 934.8399999999999, "text": " So you're getting it to predict words one at a time, and every time it makes a prediction", "tokens": [51270, 407, 291, 434, 1242, 309, 281, 6069, 2283, 472, 412, 257, 565, 11, 293, 633, 565, 309, 1669, 257, 17630, 51416], "temperature": 0.0, "avg_logprob": -0.1671158183704723, "compression_ratio": 1.7849462365591398, "no_speech_prob": 0.08335566520690918}, {"id": 233, "seek": 91380, "start": 934.8399999999999, "end": 939.3599999999999, "text": " you say you were right, and it just gets more and more carried away.", "tokens": [51416, 291, 584, 291, 645, 558, 11, 293, 309, 445, 2170, 544, 293, 544, 9094, 1314, 13, 51642], "temperature": 0.0, "avg_logprob": -0.1671158183704723, "compression_ratio": 1.7849462365591398, "no_speech_prob": 0.08335566520690918}, {"id": 234, "seek": 93936, "start": 939.36, "end": 945.52, "text": " So they initiated it with some interesting text.", "tokens": [50364, 407, 436, 28578, 309, 365, 512, 1880, 2487, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1759248586801382, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.012152668088674545}, {"id": 235, "seek": 93936, "start": 945.52, "end": 949.5600000000001, "text": " And the question is, will the neural net then produce stuff that's sort of related to that?", "tokens": [50672, 400, 264, 1168, 307, 11, 486, 264, 18161, 2533, 550, 5258, 1507, 300, 311, 1333, 295, 4077, 281, 300, 30, 50874], "temperature": 0.0, "avg_logprob": -0.1759248586801382, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.012152668088674545}, {"id": 236, "seek": 93936, "start": 949.5600000000001, "end": 952.4, "text": " I mean, the first question is, will it produce English words?", "tokens": [50874, 286, 914, 11, 264, 700, 1168, 307, 11, 486, 309, 5258, 3669, 2283, 30, 51016], "temperature": 0.0, "avg_logprob": -0.1759248586801382, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.012152668088674545}, {"id": 237, "seek": 93936, "start": 952.4, "end": 954.52, "text": " Will the words have decent syntax?", "tokens": [51016, 3099, 264, 2283, 362, 8681, 28431, 30, 51122], "temperature": 0.0, "avg_logprob": -0.1759248586801382, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.012152668088674545}, {"id": 238, "seek": 93936, "start": 954.52, "end": 956.0, "text": " Will it have any meaning?", "tokens": [51122, 3099, 309, 362, 604, 3620, 30, 51196], "temperature": 0.0, "avg_logprob": -0.1759248586801382, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.012152668088674545}, {"id": 239, "seek": 93936, "start": 956.0, "end": 958.28, "text": " Will it be related to this?", "tokens": [51196, 3099, 309, 312, 4077, 281, 341, 30, 51310], "temperature": 0.0, "avg_logprob": -0.1759248586801382, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.012152668088674545}, {"id": 240, "seek": 93936, "start": 958.28, "end": 962.5600000000001, "text": " If you're really optimistic, you might say, will they sort of relate to the fundamental", "tokens": [51310, 759, 291, 434, 534, 19397, 11, 291, 1062, 584, 11, 486, 436, 1333, 295, 10961, 281, 264, 8088, 51524], "temperature": 0.0, "avg_logprob": -0.1759248586801382, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.012152668088674545}, {"id": 241, "seek": 93936, "start": 962.5600000000001, "end": 965.4, "text": " problem here, which is how these unicorns can speak English?", "tokens": [51524, 1154, 510, 11, 597, 307, 577, 613, 28122, 82, 393, 1710, 3669, 30, 51666], "temperature": 0.0, "avg_logprob": -0.1759248586801382, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.012152668088674545}, {"id": 242, "seek": 93936, "start": 965.4, "end": 967.36, "text": " Okay, so here goes.", "tokens": [51666, 1033, 11, 370, 510, 1709, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1759248586801382, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.012152668088674545}, {"id": 243, "seek": 93936, "start": 967.36, "end": 968.96, "text": " This is what the neural net produced.", "tokens": [51764, 639, 307, 437, 264, 18161, 2533, 7126, 13, 51844], "temperature": 0.0, "avg_logprob": -0.1759248586801382, "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.012152668088674545}, {"id": 244, "seek": 96896, "start": 968.96, "end": 970.24, "text": " Now this was cherry-picked.", "tokens": [50364, 823, 341, 390, 20164, 12, 79, 12598, 13, 50428], "temperature": 0.0, "avg_logprob": -0.21657051813034783, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.0009691269369795918}, {"id": 245, "seek": 96896, "start": 970.24, "end": 975.36, "text": " This was one of their better examples.", "tokens": [50428, 639, 390, 472, 295, 641, 1101, 5110, 13, 50684], "temperature": 0.0, "avg_logprob": -0.21657051813034783, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.0009691269369795918}, {"id": 246, "seek": 96896, "start": 975.36, "end": 981.72, "text": " The neural net just made this up, right?", "tokens": [50684, 440, 18161, 2533, 445, 1027, 341, 493, 11, 558, 30, 51002], "temperature": 0.0, "avg_logprob": -0.21657051813034783, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.0009691269369795918}, {"id": 247, "seek": 96896, "start": 981.72, "end": 987.5600000000001, "text": " It made up Dr. Jorge Perez, there is no such person at the University of La Paz, but it's", "tokens": [51002, 467, 1027, 493, 2491, 13, 36875, 47317, 11, 456, 307, 572, 1270, 954, 412, 264, 3535, 295, 2369, 430, 921, 11, 457, 309, 311, 51294], "temperature": 0.0, "avg_logprob": -0.21657051813034783, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.0009691269369795918}, {"id": 248, "seek": 96896, "start": 987.5600000000001, "end": 992.1600000000001, "text": " pretty plausible because it's South America, and I believe La Paz has a university.", "tokens": [51294, 1238, 39925, 570, 309, 311, 4242, 3374, 11, 293, 286, 1697, 2369, 430, 921, 575, 257, 5454, 13, 51524], "temperature": 0.0, "avg_logprob": -0.21657051813034783, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.0009691269369795918}, {"id": 249, "seek": 96896, "start": 992.1600000000001, "end": 997.96, "text": " Okay, so that's the first bit of what it made up, and it carries on and it gets better.", "tokens": [51524, 1033, 11, 370, 300, 311, 264, 700, 857, 295, 437, 309, 1027, 493, 11, 293, 309, 16402, 322, 293, 309, 2170, 1101, 13, 51814], "temperature": 0.0, "avg_logprob": -0.21657051813034783, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.0009691269369795918}, {"id": 250, "seek": 99796, "start": 998.96, "end": 1002.96, "text": " The next bit sounds a bit like one of those fantasy games.", "tokens": [50414, 440, 958, 857, 3263, 257, 857, 411, 472, 295, 729, 13861, 2813, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2087940818385074, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.033301401883363724}, {"id": 251, "seek": 99796, "start": 1010.96, "end": 1013.44, "text": " So it's remembered about unicorns and herds of unicorns, right?", "tokens": [51014, 407, 309, 311, 13745, 466, 28122, 82, 293, 720, 16063, 295, 28122, 82, 11, 558, 30, 51138], "temperature": 0.0, "avg_logprob": -0.2087940818385074, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.033301401883363724}, {"id": 252, "seek": 99796, "start": 1013.44, "end": 1016.88, "text": " So they walk up and there's this strange valley, and it's a very strange valley, and they found", "tokens": [51138, 407, 436, 1792, 493, 293, 456, 311, 341, 5861, 17636, 11, 293, 309, 311, 257, 588, 5861, 17636, 11, 293, 436, 1352, 51310], "temperature": 0.0, "avg_logprob": -0.2087940818385074, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.033301401883363724}, {"id": 253, "seek": 99796, "start": 1016.88, "end": 1023.36, "text": " the herds of unicorns.", "tokens": [51310, 264, 720, 16063, 295, 28122, 82, 13, 51634], "temperature": 0.0, "avg_logprob": -0.2087940818385074, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.033301401883363724}, {"id": 254, "seek": 99796, "start": 1023.36, "end": 1026.04, "text": " And it has something about seeing them from the air and being able to touch them, which", "tokens": [51634, 400, 309, 575, 746, 466, 2577, 552, 490, 264, 1988, 293, 885, 1075, 281, 2557, 552, 11, 597, 51768], "temperature": 0.0, "avg_logprob": -0.2087940818385074, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.033301401883363724}, {"id": 255, "seek": 99796, "start": 1026.04, "end": 1027.32, "text": " isn't quite right.", "tokens": [51768, 1943, 380, 1596, 558, 13, 51832], "temperature": 0.0, "avg_logprob": -0.2087940818385074, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.033301401883363724}, {"id": 256, "seek": 102732, "start": 1027.32, "end": 1030.8799999999999, "text": " So people in Symbolicae leap on this and say, you see, it doesn't understand.", "tokens": [50364, 407, 561, 294, 3902, 5612, 2262, 68, 19438, 322, 341, 293, 584, 11, 291, 536, 11, 309, 1177, 380, 1223, 13, 50542], "temperature": 0.0, "avg_logprob": -0.20736893363620923, "compression_ratio": 1.648, "no_speech_prob": 0.0011962370481342077}, {"id": 257, "seek": 102732, "start": 1030.8799999999999, "end": 1036.08, "text": " Well, sure, there's little bits that it doesn't get right.", "tokens": [50542, 1042, 11, 988, 11, 456, 311, 707, 9239, 300, 309, 1177, 380, 483, 558, 13, 50802], "temperature": 0.0, "avg_logprob": -0.20736893363620923, "compression_ratio": 1.648, "no_speech_prob": 0.0011962370481342077}, {"id": 258, "seek": 102732, "start": 1036.08, "end": 1040.3999999999999, "text": " But notice, it's remembered that these unicorns have to speak English, and so it tells you", "tokens": [50802, 583, 3449, 11, 309, 311, 13745, 300, 613, 28122, 82, 362, 281, 1710, 3669, 11, 293, 370, 309, 5112, 291, 51018], "temperature": 0.0, "avg_logprob": -0.20736893363620923, "compression_ratio": 1.648, "no_speech_prob": 0.0011962370481342077}, {"id": 259, "seek": 102732, "start": 1040.3999999999999, "end": 1045.56, "text": " about, you know, they spoke some fairly regular English.", "tokens": [51018, 466, 11, 291, 458, 11, 436, 7179, 512, 6457, 3890, 3669, 13, 51276], "temperature": 0.0, "avg_logprob": -0.20736893363620923, "compression_ratio": 1.648, "no_speech_prob": 0.0011962370481342077}, {"id": 260, "seek": 102732, "start": 1045.56, "end": 1050.2, "text": " It doesn't know the difference between dialect and dialectic, but my kids don't know that", "tokens": [51276, 467, 1177, 380, 458, 264, 2649, 1296, 24652, 293, 24652, 299, 11, 457, 452, 2301, 500, 380, 458, 300, 51508], "temperature": 0.0, "avg_logprob": -0.20736893363620923, "compression_ratio": 1.648, "no_speech_prob": 0.0011962370481342077}, {"id": 261, "seek": 102732, "start": 1050.2, "end": 1051.2, "text": " either.", "tokens": [51508, 2139, 13, 51558], "temperature": 0.0, "avg_logprob": -0.20736893363620923, "compression_ratio": 1.648, "no_speech_prob": 0.0011962370481342077}, {"id": 262, "seek": 102732, "start": 1051.2, "end": 1057.28, "text": " In fact, I'm not sure I know.", "tokens": [51558, 682, 1186, 11, 286, 478, 406, 988, 286, 458, 13, 51862], "temperature": 0.0, "avg_logprob": -0.20736893363620923, "compression_ratio": 1.648, "no_speech_prob": 0.0011962370481342077}, {"id": 263, "seek": 105728, "start": 1057.28, "end": 1064.8, "text": " It's a tribute to unicorns to Argentina, even though Dr. Perez comes from Bolivia.", "tokens": [50364, 467, 311, 257, 24722, 281, 28122, 82, 281, 18336, 11, 754, 1673, 2491, 13, 47317, 1487, 490, 14331, 18503, 13, 50740], "temperature": 0.0, "avg_logprob": -0.18572180611746653, "compression_ratio": 1.4738955823293172, "no_speech_prob": 0.002368177752941847}, {"id": 264, "seek": 105728, "start": 1064.8, "end": 1068.0, "text": " And it actually understands about magic realism.", "tokens": [50740, 400, 309, 767, 15146, 466, 5585, 38484, 13, 50900], "temperature": 0.0, "avg_logprob": -0.18572180611746653, "compression_ratio": 1.4738955823293172, "no_speech_prob": 0.002368177752941847}, {"id": 265, "seek": 105728, "start": 1068.0, "end": 1074.72, "text": " So they're descendants of a lost race, and I love the bit at the end where it says, in", "tokens": [50900, 407, 436, 434, 31693, 295, 257, 2731, 4569, 11, 293, 286, 959, 264, 857, 412, 264, 917, 689, 309, 1619, 11, 294, 51236], "temperature": 0.0, "avg_logprob": -0.18572180611746653, "compression_ratio": 1.4738955823293172, "no_speech_prob": 0.002368177752941847}, {"id": 266, "seek": 105728, "start": 1074.72, "end": 1079.12, "text": " South America, such incidents seem to be quite common.", "tokens": [51236, 4242, 3374, 11, 1270, 21139, 1643, 281, 312, 1596, 2689, 13, 51456], "temperature": 0.0, "avg_logprob": -0.18572180611746653, "compression_ratio": 1.4738955823293172, "no_speech_prob": 0.002368177752941847}, {"id": 267, "seek": 105728, "start": 1079.12, "end": 1084.6, "text": " This has an ability to just make up something that fits your prejudices and sounds moderately", "tokens": [51456, 639, 575, 364, 3485, 281, 445, 652, 493, 746, 300, 9001, 428, 23121, 1473, 293, 3263, 10494, 1592, 51730], "temperature": 0.0, "avg_logprob": -0.18572180611746653, "compression_ratio": 1.4738955823293172, "no_speech_prob": 0.002368177752941847}, {"id": 268, "seek": 108460, "start": 1084.6, "end": 1092.76, "text": " plausible, like a certain president.", "tokens": [50364, 39925, 11, 411, 257, 1629, 3868, 13, 50772], "temperature": 0.0, "avg_logprob": -0.22897037459008487, "compression_ratio": 1.455, "no_speech_prob": 0.15721286833286285}, {"id": 269, "seek": 108460, "start": 1092.76, "end": 1097.56, "text": " And it finally gets to the point which is, if you really want to know whether these unicorns", "tokens": [50772, 400, 309, 2721, 2170, 281, 264, 935, 597, 307, 11, 498, 291, 534, 528, 281, 458, 1968, 613, 28122, 82, 51012], "temperature": 0.0, "avg_logprob": -0.22897037459008487, "compression_ratio": 1.455, "no_speech_prob": 0.15721286833286285}, {"id": 270, "seek": 108460, "start": 1097.56, "end": 1103.52, "text": " were used by breeding with these strange lost race of people, you ought to do a DNA test.", "tokens": [51012, 645, 1143, 538, 26051, 365, 613, 5861, 2731, 4569, 295, 561, 11, 291, 13416, 281, 360, 257, 8272, 1500, 13, 51310], "temperature": 0.0, "avg_logprob": -0.22897037459008487, "compression_ratio": 1.455, "no_speech_prob": 0.15721286833286285}, {"id": 271, "seek": 108460, "start": 1103.52, "end": 1104.52, "text": " Okay?", "tokens": [51310, 1033, 30, 51360], "temperature": 0.0, "avg_logprob": -0.22897037459008487, "compression_ratio": 1.455, "no_speech_prob": 0.15721286833286285}, {"id": 272, "seek": 108460, "start": 1104.52, "end": 1106.0, "text": " It understands that.", "tokens": [51360, 467, 15146, 300, 13, 51434], "temperature": 0.0, "avg_logprob": -0.22897037459008487, "compression_ratio": 1.455, "no_speech_prob": 0.15721286833286285}, {"id": 273, "seek": 108460, "start": 1106.0, "end": 1107.0, "text": " Okay.", "tokens": [51434, 1033, 13, 51484], "temperature": 0.0, "avg_logprob": -0.22897037459008487, "compression_ratio": 1.455, "no_speech_prob": 0.15721286833286285}, {"id": 274, "seek": 108460, "start": 1107.0, "end": 1110.4399999999998, "text": " So that's what neural nets can do now.", "tokens": [51484, 407, 300, 311, 437, 18161, 36170, 393, 360, 586, 13, 51656], "temperature": 0.0, "avg_logprob": -0.22897037459008487, "compression_ratio": 1.455, "no_speech_prob": 0.15721286833286285}, {"id": 275, "seek": 111044, "start": 1110.44, "end": 1118.1200000000001, "text": " This was a neural net with 1.5 billion connections that was trained on Google's, actually, I", "tokens": [50364, 639, 390, 257, 18161, 2533, 365, 502, 13, 20, 5218, 9271, 300, 390, 8895, 322, 3329, 311, 11, 767, 11, 286, 50748], "temperature": 0.0, "avg_logprob": -0.1818689643789869, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.06479351967573166}, {"id": 276, "seek": 111044, "start": 1118.1200000000001, "end": 1124.28, "text": " withdraw that, 1.5 billion connections is trained on a lot of hardware.", "tokens": [50748, 14999, 300, 11, 502, 13, 20, 5218, 9271, 307, 8895, 322, 257, 688, 295, 8837, 13, 51056], "temperature": 0.0, "avg_logprob": -0.1818689643789869, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.06479351967573166}, {"id": 277, "seek": 111044, "start": 1124.28, "end": 1130.16, "text": " And we look at what it says, and we sort of laugh at how, you know, it's pretty good,", "tokens": [51056, 400, 321, 574, 412, 437, 309, 1619, 11, 293, 321, 1333, 295, 5801, 412, 577, 11, 291, 458, 11, 309, 311, 1238, 665, 11, 51350], "temperature": 0.0, "avg_logprob": -0.1818689643789869, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.06479351967573166}, {"id": 278, "seek": 111044, "start": 1130.16, "end": 1132.8, "text": " but it hasn't got it quite right, but it's pretty good.", "tokens": [51350, 457, 309, 6132, 380, 658, 309, 1596, 558, 11, 457, 309, 311, 1238, 665, 13, 51482], "temperature": 0.0, "avg_logprob": -0.1818689643789869, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.06479351967573166}, {"id": 279, "seek": 111044, "start": 1132.8, "end": 1133.8, "text": " Okay.", "tokens": [51482, 1033, 13, 51532], "temperature": 0.0, "avg_logprob": -0.1818689643789869, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.06479351967573166}, {"id": 280, "seek": 111044, "start": 1133.8, "end": 1139.72, "text": " What they've done now is they've trained a neural net with 50 billion connections on", "tokens": [51532, 708, 436, 600, 1096, 586, 307, 436, 600, 8895, 257, 18161, 2533, 365, 2625, 5218, 9271, 322, 51828], "temperature": 0.0, "avg_logprob": -0.1818689643789869, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.06479351967573166}, {"id": 281, "seek": 113972, "start": 1139.72, "end": 1145.72, "text": " Google's latest cloud hardware, which is, it's like having several of the world's biggest", "tokens": [50364, 3329, 311, 6792, 4588, 8837, 11, 597, 307, 11, 309, 311, 411, 1419, 2940, 295, 264, 1002, 311, 3880, 50664], "temperature": 0.0, "avg_logprob": -0.1427148679892222, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0021972416434437037}, {"id": 282, "seek": 113972, "start": 1145.72, "end": 1149.84, "text": " supercomputers going for you for months.", "tokens": [50664, 27839, 2582, 433, 516, 337, 291, 337, 2493, 13, 50870], "temperature": 0.0, "avg_logprob": -0.1427148679892222, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0021972416434437037}, {"id": 283, "seek": 113972, "start": 1149.84, "end": 1154.52, "text": " The net with 50 billion connections, I haven't seen any text from it yet, but my prediction", "tokens": [50870, 440, 2533, 365, 2625, 5218, 9271, 11, 286, 2378, 380, 1612, 604, 2487, 490, 309, 1939, 11, 457, 452, 17630, 51104], "temperature": 0.0, "avg_logprob": -0.1427148679892222, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0021972416434437037}, {"id": 284, "seek": 113972, "start": 1154.52, "end": 1159.6000000000001, "text": " is it's sitting around laughing at how cute what we produce is.", "tokens": [51104, 307, 309, 311, 3798, 926, 5059, 412, 577, 4052, 437, 321, 5258, 307, 13, 51358], "temperature": 0.0, "avg_logprob": -0.1427148679892222, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0021972416434437037}, {"id": 285, "seek": 113972, "start": 1159.6000000000001, "end": 1161.16, "text": " Okay.", "tokens": [51358, 1033, 13, 51436], "temperature": 0.0, "avg_logprob": -0.1427148679892222, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0021972416434437037}, {"id": 286, "seek": 113972, "start": 1161.16, "end": 1169.24, "text": " So one thing about that net is it's clearly very well aware of the initial context.", "tokens": [51436, 407, 472, 551, 466, 300, 2533, 307, 309, 311, 4448, 588, 731, 3650, 295, 264, 5883, 4319, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1427148679892222, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0021972416434437037}, {"id": 287, "seek": 116924, "start": 1169.24, "end": 1174.72, "text": " These unicorns in a valley that speak English, and it's remembering this initial context", "tokens": [50364, 1981, 28122, 82, 294, 257, 17636, 300, 1710, 3669, 11, 293, 309, 311, 20719, 341, 5883, 4319, 50638], "temperature": 0.0, "avg_logprob": -0.15861966475000921, "compression_ratio": 1.75, "no_speech_prob": 0.029057573527097702}, {"id": 288, "seek": 116924, "start": 1174.72, "end": 1177.48, "text": " a long time later, and a recurrent neural net can't do that.", "tokens": [50638, 257, 938, 565, 1780, 11, 293, 257, 18680, 1753, 18161, 2533, 393, 380, 360, 300, 13, 50776], "temperature": 0.0, "avg_logprob": -0.15861966475000921, "compression_ratio": 1.75, "no_speech_prob": 0.029057573527097702}, {"id": 289, "seek": 116924, "start": 1177.48, "end": 1180.48, "text": " A recurrent neural net would have forgotten about the initial stuff and wouldn't produce", "tokens": [50776, 316, 18680, 1753, 18161, 2533, 576, 362, 11832, 466, 264, 5883, 1507, 293, 2759, 380, 5258, 50926], "temperature": 0.0, "avg_logprob": -0.15861966475000921, "compression_ratio": 1.75, "no_speech_prob": 0.029057573527097702}, {"id": 290, "seek": 116924, "start": 1180.48, "end": 1183.56, "text": " such good context-dependent stuff.", "tokens": [50926, 1270, 665, 4319, 12, 36763, 317, 1507, 13, 51080], "temperature": 0.0, "avg_logprob": -0.15861966475000921, "compression_ratio": 1.75, "no_speech_prob": 0.029057573527097702}, {"id": 291, "seek": 116924, "start": 1183.56, "end": 1192.04, "text": " So the way this works is the word comes in, the neural net activates some hidden units.", "tokens": [51080, 407, 264, 636, 341, 1985, 307, 264, 1349, 1487, 294, 11, 264, 18161, 2533, 43869, 512, 7633, 6815, 13, 51504], "temperature": 0.0, "avg_logprob": -0.15861966475000921, "compression_ratio": 1.75, "no_speech_prob": 0.029057573527097702}, {"id": 292, "seek": 116924, "start": 1192.04, "end": 1197.52, "text": " That pattern of activity in the hidden units goes and compares itself with previous patterns,", "tokens": [51504, 663, 5102, 295, 5191, 294, 264, 7633, 6815, 1709, 293, 38334, 2564, 365, 3894, 8294, 11, 51778], "temperature": 0.0, "avg_logprob": -0.15861966475000921, "compression_ratio": 1.75, "no_speech_prob": 0.029057573527097702}, {"id": 293, "seek": 119752, "start": 1197.96, "end": 1202.56, "text": " your point of view, with previous patterns at earlier times.", "tokens": [50386, 428, 935, 295, 1910, 11, 365, 3894, 8294, 412, 3071, 1413, 13, 50616], "temperature": 0.0, "avg_logprob": -0.163906947426174, "compression_ratio": 1.7743362831858407, "no_speech_prob": 0.05351937562227249}, {"id": 294, "seek": 119752, "start": 1202.56, "end": 1206.72, "text": " And when it finds a pattern at an earlier time that's a bit similar, it says that we'll", "tokens": [50616, 400, 562, 309, 10704, 257, 5102, 412, 364, 3071, 565, 300, 311, 257, 857, 2531, 11, 309, 1619, 300, 321, 603, 50824], "temperature": 0.0, "avg_logprob": -0.163906947426174, "compression_ratio": 1.7743362831858407, "no_speech_prob": 0.05351937562227249}, {"id": 295, "seek": 119752, "start": 1206.72, "end": 1211.8799999999999, "text": " take advice from that previous hidden pattern about how to affect the next layer.", "tokens": [50824, 747, 5192, 490, 300, 3894, 7633, 5102, 466, 577, 281, 3345, 264, 958, 4583, 13, 51082], "temperature": 0.0, "avg_logprob": -0.163906947426174, "compression_ratio": 1.7743362831858407, "no_speech_prob": 0.05351937562227249}, {"id": 296, "seek": 119752, "start": 1211.8799999999999, "end": 1219.6399999999999, "text": " And so actually a word comes in and how one pattern of activity in the bottom layer of", "tokens": [51082, 400, 370, 767, 257, 1349, 1487, 294, 293, 577, 472, 5102, 295, 5191, 294, 264, 2767, 4583, 295, 51470], "temperature": 0.0, "avg_logprob": -0.163906947426174, "compression_ratio": 1.7743362831858407, "no_speech_prob": 0.05351937562227249}, {"id": 297, "seek": 119752, "start": 1219.6399999999999, "end": 1226.8, "text": " the hidden neurons affects the next layer is dependent on what happened previously.", "tokens": [51470, 264, 7633, 22027, 11807, 264, 958, 4583, 307, 12334, 322, 437, 2011, 8046, 13, 51828], "temperature": 0.0, "avg_logprob": -0.163906947426174, "compression_ratio": 1.7743362831858407, "no_speech_prob": 0.05351937562227249}, {"id": 298, "seek": 122680, "start": 1226.8, "end": 1231.04, "text": " Now it's dependent in quite a complicated way, and this seems very implausible for a", "tokens": [50364, 823, 309, 311, 12334, 294, 1596, 257, 6179, 636, 11, 293, 341, 2544, 588, 8484, 8463, 964, 337, 257, 50576], "temperature": 0.0, "avg_logprob": -0.18558836405256154, "compression_ratio": 1.8407407407407408, "no_speech_prob": 0.001447291113436222}, {"id": 299, "seek": 122680, "start": 1231.04, "end": 1236.8, "text": " brain because what's happening in the computer is you're storing all these activity patterns", "tokens": [50576, 3567, 570, 437, 311, 2737, 294, 264, 3820, 307, 291, 434, 26085, 439, 613, 5191, 8294, 50864], "temperature": 0.0, "avg_logprob": -0.18558836405256154, "compression_ratio": 1.8407407407407408, "no_speech_prob": 0.001447291113436222}, {"id": 300, "seek": 122680, "start": 1236.8, "end": 1239.8, "text": " that are meant to be neural activity patterns or light neural activity patterns, and you're", "tokens": [50864, 300, 366, 4140, 281, 312, 18161, 5191, 8294, 420, 1442, 18161, 5191, 8294, 11, 293, 291, 434, 51014], "temperature": 0.0, "avg_logprob": -0.18558836405256154, "compression_ratio": 1.8407407407407408, "no_speech_prob": 0.001447291113436222}, {"id": 301, "seek": 122680, "start": 1239.8, "end": 1243.32, "text": " comparing, and this looks hopeless.", "tokens": [51014, 15763, 11, 293, 341, 1542, 27317, 13, 51190], "temperature": 0.0, "avg_logprob": -0.18558836405256154, "compression_ratio": 1.8407407407407408, "no_speech_prob": 0.001447291113436222}, {"id": 302, "seek": 122680, "start": 1243.32, "end": 1249.08, "text": " But actually all you need to do is every time you have an activity pattern, and you use", "tokens": [51190, 583, 767, 439, 291, 643, 281, 360, 307, 633, 565, 291, 362, 364, 5191, 5102, 11, 293, 291, 764, 51478], "temperature": 0.0, "avg_logprob": -0.18558836405256154, "compression_ratio": 1.8407407407407408, "no_speech_prob": 0.001447291113436222}, {"id": 303, "seek": 122680, "start": 1249.08, "end": 1253.76, "text": " the outgoing weights to affect the next layer, just change the weight slightly with heavy", "tokens": [51478, 264, 41565, 17443, 281, 3345, 264, 958, 4583, 11, 445, 1319, 264, 3364, 4748, 365, 4676, 51712], "temperature": 0.0, "avg_logprob": -0.18558836405256154, "compression_ratio": 1.8407407407407408, "no_speech_prob": 0.001447291113436222}, {"id": 304, "seek": 122680, "start": 1253.76, "end": 1255.56, "text": " and learning.", "tokens": [51712, 293, 2539, 13, 51802], "temperature": 0.0, "avg_logprob": -0.18558836405256154, "compression_ratio": 1.8407407407407408, "no_speech_prob": 0.001447291113436222}, {"id": 305, "seek": 125556, "start": 1255.56, "end": 1261.48, "text": " So now what's going to happen is that weight matrix that comes out of that activity pattern", "tokens": [50364, 407, 586, 437, 311, 516, 281, 1051, 307, 300, 3364, 8141, 300, 1487, 484, 295, 300, 5191, 5102, 50660], "temperature": 0.0, "avg_logprob": -0.11821686211278883, "compression_ratio": 2.1781376518218623, "no_speech_prob": 0.001106912735849619}, {"id": 306, "seek": 125556, "start": 1261.48, "end": 1263.2, "text": " is going to be modified slightly.", "tokens": [50660, 307, 516, 281, 312, 15873, 4748, 13, 50746], "temperature": 0.0, "avg_logprob": -0.11821686211278883, "compression_ratio": 2.1781376518218623, "no_speech_prob": 0.001106912735849619}, {"id": 307, "seek": 125556, "start": 1263.2, "end": 1268.2, "text": " Now when I get a new activity pattern, if the activity pattern is orthogonal to the previous", "tokens": [50746, 823, 562, 286, 483, 257, 777, 5191, 5102, 11, 498, 264, 5191, 5102, 307, 41488, 281, 264, 3894, 50996], "temperature": 0.0, "avg_logprob": -0.11821686211278883, "compression_ratio": 2.1781376518218623, "no_speech_prob": 0.001106912735849619}, {"id": 308, "seek": 125556, "start": 1268.2, "end": 1273.1599999999999, "text": " activity pattern, then any modifications you made in the weight matrix due to that previous", "tokens": [50996, 5191, 5102, 11, 550, 604, 26881, 291, 1027, 294, 264, 3364, 8141, 3462, 281, 300, 3894, 51244], "temperature": 0.0, "avg_logprob": -0.11821686211278883, "compression_ratio": 2.1781376518218623, "no_speech_prob": 0.001106912735849619}, {"id": 309, "seek": 125556, "start": 1273.1599999999999, "end": 1275.3999999999999, "text": " activity pattern won't make any difference.", "tokens": [51244, 5191, 5102, 1582, 380, 652, 604, 2649, 13, 51356], "temperature": 0.0, "avg_logprob": -0.11821686211278883, "compression_ratio": 2.1781376518218623, "no_speech_prob": 0.001106912735849619}, {"id": 310, "seek": 125556, "start": 1275.3999999999999, "end": 1279.76, "text": " But if it lines up with the previous activity pattern, if it's similar, the modifications", "tokens": [51356, 583, 498, 309, 3876, 493, 365, 264, 3894, 5191, 5102, 11, 498, 309, 311, 2531, 11, 264, 26881, 51574], "temperature": 0.0, "avg_logprob": -0.11821686211278883, "compression_ratio": 2.1781376518218623, "no_speech_prob": 0.001106912735849619}, {"id": 311, "seek": 125556, "start": 1279.76, "end": 1285.04, "text": " you made in the weights back there, the temporary modifications, will cause this new activity", "tokens": [51574, 291, 1027, 294, 264, 17443, 646, 456, 11, 264, 13413, 26881, 11, 486, 3082, 341, 777, 5191, 51838], "temperature": 0.0, "avg_logprob": -0.11821686211278883, "compression_ratio": 2.1781376518218623, "no_speech_prob": 0.001106912735849619}, {"id": 312, "seek": 128504, "start": 1285.04, "end": 1287.36, "text": " pattern to have a different effect here.", "tokens": [50364, 5102, 281, 362, 257, 819, 1802, 510, 13, 50480], "temperature": 0.0, "avg_logprob": -0.14753430230276926, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.07531099021434784}, {"id": 313, "seek": 128504, "start": 1287.36, "end": 1291.8799999999999, "text": " So you'll get that long temporal context, and the way to store a long temporal context", "tokens": [50480, 407, 291, 603, 483, 300, 938, 30881, 4319, 11, 293, 264, 636, 281, 3531, 257, 938, 30881, 4319, 50706], "temperature": 0.0, "avg_logprob": -0.14753430230276926, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.07531099021434784}, {"id": 314, "seek": 128504, "start": 1291.8799999999999, "end": 1299.28, "text": " is not to keep copies of neural activity patterns, it's to take your weights and to have temporary", "tokens": [50706, 307, 406, 281, 1066, 14341, 295, 18161, 5191, 8294, 11, 309, 311, 281, 747, 428, 17443, 293, 281, 362, 13413, 51076], "temperature": 0.0, "avg_logprob": -0.14753430230276926, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.07531099021434784}, {"id": 315, "seek": 128504, "start": 1299.28, "end": 1301.6, "text": " changes to the weights, which I call fast weights.", "tokens": [51076, 2962, 281, 264, 17443, 11, 597, 286, 818, 2370, 17443, 13, 51192], "temperature": 0.0, "avg_logprob": -0.14753430230276926, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.07531099021434784}, {"id": 316, "seek": 128504, "start": 1301.6, "end": 1308.6399999999999, "text": " So you temporarily change them, and these changes decay over time, so you'll have a memory.", "tokens": [51192, 407, 291, 23750, 1319, 552, 11, 293, 613, 2962, 21039, 670, 565, 11, 370, 291, 603, 362, 257, 4675, 13, 51544], "temperature": 0.0, "avg_logprob": -0.14753430230276926, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.07531099021434784}, {"id": 317, "seek": 128504, "start": 1308.6399999999999, "end": 1312.8, "text": " So if you ask, where in your brain is your memory of what I said a few minutes ago?", "tokens": [51544, 407, 498, 291, 1029, 11, 689, 294, 428, 3567, 307, 428, 4675, 295, 437, 286, 848, 257, 1326, 2077, 2057, 30, 51752], "temperature": 0.0, "avg_logprob": -0.14753430230276926, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.07531099021434784}, {"id": 318, "seek": 131280, "start": 1313.36, "end": 1316.1599999999999, "text": " I'll ask the younger people this, because for the older people it's nowhere.", "tokens": [50392, 286, 603, 1029, 264, 7037, 561, 341, 11, 570, 337, 264, 4906, 561, 309, 311, 11159, 13, 50532], "temperature": 0.0, "avg_logprob": -0.2019869376873148, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.004794237203896046}, {"id": 319, "seek": 131280, "start": 1316.1599999999999, "end": 1321.72, "text": " But for the younger people, it's somewhere you can, if I were to say something I said", "tokens": [50532, 583, 337, 264, 7037, 561, 11, 309, 311, 4079, 291, 393, 11, 498, 286, 645, 281, 584, 746, 286, 848, 50810], "temperature": 0.0, "avg_logprob": -0.2019869376873148, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.004794237203896046}, {"id": 320, "seek": 131280, "start": 1321.72, "end": 1327.08, "text": " a few minutes ago, like these big neural nets are now laughing at us, you remember I said", "tokens": [50810, 257, 1326, 2077, 2057, 11, 411, 613, 955, 18161, 36170, 366, 586, 5059, 412, 505, 11, 291, 1604, 286, 848, 51078], "temperature": 0.0, "avg_logprob": -0.2019869376873148, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.004794237203896046}, {"id": 321, "seek": 131280, "start": 1327.08, "end": 1329.52, "text": " that, where was that memory?", "tokens": [51078, 300, 11, 689, 390, 300, 4675, 30, 51200], "temperature": 0.0, "avg_logprob": -0.2019869376873148, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.004794237203896046}, {"id": 322, "seek": 131280, "start": 1329.52, "end": 1334.04, "text": " I think it's in the temporary changes to the weights, because that's got much bigger capacity", "tokens": [51200, 286, 519, 309, 311, 294, 264, 13413, 2962, 281, 264, 17443, 11, 570, 300, 311, 658, 709, 3801, 6042, 51426], "temperature": 0.0, "avg_logprob": -0.2019869376873148, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.004794237203896046}, {"id": 323, "seek": 131280, "start": 1334.04, "end": 1340.08, "text": " than activities of neurons, and you don't need to use up neurons just sitting there remembering.", "tokens": [51426, 813, 5354, 295, 22027, 11, 293, 291, 500, 380, 643, 281, 764, 493, 22027, 445, 3798, 456, 20719, 13, 51728], "temperature": 0.0, "avg_logprob": -0.2019869376873148, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.004794237203896046}, {"id": 324, "seek": 134008, "start": 1340.08, "end": 1343.36, "text": " And those temporary changes don't need to be driven by back propagation, they can just", "tokens": [50364, 400, 729, 13413, 2962, 500, 380, 643, 281, 312, 9555, 538, 646, 38377, 11, 436, 393, 445, 50528], "temperature": 0.0, "avg_logprob": -0.15618199049824416, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.0011994761880487204}, {"id": 325, "seek": 134008, "start": 1343.36, "end": 1345.36, "text": " be heavier.", "tokens": [50528, 312, 18279, 13, 50628], "temperature": 0.0, "avg_logprob": -0.15618199049824416, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.0011994761880487204}, {"id": 326, "seek": 134008, "start": 1345.36, "end": 1353.8, "text": " Okay, so I've tried to relate these wonderful nets that can make up stories with an idea", "tokens": [50628, 1033, 11, 370, 286, 600, 3031, 281, 10961, 613, 3715, 36170, 300, 393, 652, 493, 3676, 365, 364, 1558, 51050], "temperature": 0.0, "avg_logprob": -0.15618199049824416, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.0011994761880487204}, {"id": 327, "seek": 134008, "start": 1353.8, "end": 1357.8, "text": " about where short-term memory is in the brain.", "tokens": [51050, 466, 689, 2099, 12, 7039, 4675, 307, 294, 264, 3567, 13, 51250], "temperature": 0.0, "avg_logprob": -0.15618199049824416, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.0011994761880487204}, {"id": 328, "seek": 134008, "start": 1357.8, "end": 1361.52, "text": " And now I'll talk about where the cortex can do back propagation.", "tokens": [51250, 400, 586, 286, 603, 751, 466, 689, 264, 33312, 393, 360, 646, 38377, 13, 51436], "temperature": 0.0, "avg_logprob": -0.15618199049824416, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.0011994761880487204}, {"id": 329, "seek": 134008, "start": 1361.52, "end": 1368.56, "text": " So neuroscientists, 20 years ago neuroscientists said don't be ridiculous, of course the brain", "tokens": [51436, 407, 28813, 5412, 1751, 11, 945, 924, 2057, 28813, 5412, 1751, 848, 500, 380, 312, 11083, 11, 295, 1164, 264, 3567, 51788], "temperature": 0.0, "avg_logprob": -0.15618199049824416, "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.0011994761880487204}, {"id": 330, "seek": 136856, "start": 1368.56, "end": 1372.96, "text": " can't do back propagation, and they'd interpret it very literally as sending signals backwards", "tokens": [50364, 393, 380, 360, 646, 38377, 11, 293, 436, 1116, 7302, 309, 588, 3736, 382, 7750, 12354, 12204, 50584], "temperature": 0.0, "avg_logprob": -0.18205928802490234, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0011534763034433126}, {"id": 331, "seek": 136856, "start": 1372.96, "end": 1383.6, "text": " down the same axons and saying neurons don't do that, no thanks.", "tokens": [50584, 760, 264, 912, 6360, 892, 293, 1566, 22027, 500, 380, 360, 300, 11, 572, 3231, 13, 51116], "temperature": 0.0, "avg_logprob": -0.18205928802490234, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0011534763034433126}, {"id": 332, "seek": 136856, "start": 1383.6, "end": 1388.76, "text": " But now we know that back propagation works really well for solving tough practical problems.", "tokens": [51116, 583, 586, 321, 458, 300, 646, 38377, 1985, 534, 731, 337, 12606, 4930, 8496, 2740, 13, 51374], "temperature": 0.0, "avg_logprob": -0.18205928802490234, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0011534763034433126}, {"id": 333, "seek": 136856, "start": 1388.76, "end": 1392.32, "text": " So that's rather changed the balance, because when back propagation was just a theory of", "tokens": [51374, 407, 300, 311, 2831, 3105, 264, 4772, 11, 570, 562, 646, 38377, 390, 445, 257, 5261, 295, 51552], "temperature": 0.0, "avg_logprob": -0.18205928802490234, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0011534763034433126}, {"id": 334, "seek": 136856, "start": 1392.32, "end": 1397.56, "text": " how you might get computers to learn something, and when it learns some simple things, it", "tokens": [51552, 577, 291, 1062, 483, 10807, 281, 1466, 746, 11, 293, 562, 309, 27152, 512, 2199, 721, 11, 309, 51814], "temperature": 0.0, "avg_logprob": -0.18205928802490234, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0011534763034433126}, {"id": 335, "seek": 139756, "start": 1397.56, "end": 1400.8, "text": " wasn't sort of imperative to understand whether the brain did it.", "tokens": [50364, 2067, 380, 1333, 295, 32490, 281, 1223, 1968, 264, 3567, 630, 309, 13, 50526], "temperature": 0.0, "avg_logprob": -0.13256547347359035, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.12386962026357651}, {"id": 336, "seek": 139756, "start": 1400.8, "end": 1404.9199999999998, "text": " But now we know that you can do all these things with back propagation.", "tokens": [50526, 583, 586, 321, 458, 300, 291, 393, 360, 439, 613, 721, 365, 646, 38377, 13, 50732], "temperature": 0.0, "avg_logprob": -0.13256547347359035, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.12386962026357651}, {"id": 337, "seek": 139756, "start": 1404.9199999999998, "end": 1409.0, "text": " What's more, we know that back propagation is the right thing to do, but if you have", "tokens": [50732, 708, 311, 544, 11, 321, 458, 300, 646, 38377, 307, 264, 558, 551, 281, 360, 11, 457, 498, 291, 362, 50936], "temperature": 0.0, "avg_logprob": -0.13256547347359035, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.12386962026357651}, {"id": 338, "seek": 139756, "start": 1409.0, "end": 1415.32, "text": " a sensory pathway, and you want to take the early feature detectors so that their outputs", "tokens": [50936, 257, 27233, 18590, 11, 293, 291, 528, 281, 747, 264, 2440, 4111, 46866, 370, 300, 641, 23930, 51252], "temperature": 0.0, "avg_logprob": -0.13256547347359035, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.12386962026357651}, {"id": 339, "seek": 139756, "start": 1415.32, "end": 1421.6, "text": " are more helpful for making the right decision later on in the system, then what you really", "tokens": [51252, 366, 544, 4961, 337, 1455, 264, 558, 3537, 1780, 322, 294, 264, 1185, 11, 550, 437, 291, 534, 51566], "temperature": 0.0, "avg_logprob": -0.13256547347359035, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.12386962026357651}, {"id": 340, "seek": 139756, "start": 1421.6, "end": 1426.56, "text": " need to do is ask the question, how should I change the receptive field of this early", "tokens": [51566, 643, 281, 360, 307, 1029, 264, 1168, 11, 577, 820, 286, 1319, 264, 45838, 2519, 295, 341, 2440, 51814], "temperature": 0.0, "avg_logprob": -0.13256547347359035, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.12386962026357651}, {"id": 341, "seek": 142656, "start": 1426.56, "end": 1432.76, "text": " detector so that what is output helps with the decision?", "tokens": [50364, 25712, 370, 300, 437, 307, 5598, 3665, 365, 264, 3537, 30, 50674], "temperature": 0.0, "avg_logprob": -0.18847587260794132, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.05380403622984886}, {"id": 342, "seek": 142656, "start": 1432.76, "end": 1435.48, "text": " And what you have to do is do back propagation to compute that, that's the efficient way", "tokens": [50674, 400, 437, 291, 362, 281, 360, 307, 360, 646, 38377, 281, 14722, 300, 11, 300, 311, 264, 7148, 636, 50810], "temperature": 0.0, "avg_logprob": -0.18847587260794132, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.05380403622984886}, {"id": 343, "seek": 142656, "start": 1435.48, "end": 1436.48, "text": " to compute it.", "tokens": [50810, 281, 14722, 309, 13, 50860], "temperature": 0.0, "avg_logprob": -0.18847587260794132, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.05380403622984886}, {"id": 344, "seek": 142656, "start": 1436.48, "end": 1442.28, "text": " And I think it'd be crazy if the brain wasn't somehow doing this.", "tokens": [50860, 400, 286, 519, 309, 1116, 312, 3219, 498, 264, 3567, 2067, 380, 6063, 884, 341, 13, 51150], "temperature": 0.0, "avg_logprob": -0.18847587260794132, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.05380403622984886}, {"id": 345, "seek": 142656, "start": 1442.28, "end": 1445.48, "text": " So why do neuroscientists think it's impossible?", "tokens": [51150, 407, 983, 360, 28813, 5412, 1751, 519, 309, 311, 6243, 30, 51310], "temperature": 0.0, "avg_logprob": -0.18847587260794132, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.05380403622984886}, {"id": 346, "seek": 142656, "start": 1445.48, "end": 1451.24, "text": " Apart from silly objections like things don't go backwards down axons, at least not at the", "tokens": [51310, 24111, 490, 11774, 44649, 411, 721, 500, 380, 352, 12204, 760, 6360, 892, 11, 412, 1935, 406, 412, 264, 51598], "temperature": 0.0, "avg_logprob": -0.18847587260794132, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.05380403622984886}, {"id": 347, "seek": 145124, "start": 1451.24, "end": 1460.24, "text": " right speed.", "tokens": [50364, 558, 3073, 13, 50814], "temperature": 0.0, "avg_logprob": -0.5642606835616263, "compression_ratio": 1.1555555555555554, "no_speech_prob": 0.3310433626174927}, {"id": 348, "seek": 145124, "start": 1460.24, "end": 1462.24, "text": " He wants me to update things.", "tokens": [50814, 634, 2738, 385, 281, 5623, 721, 13, 50914], "temperature": 0.0, "avg_logprob": -0.5642606835616263, "compression_ratio": 1.1555555555555554, "no_speech_prob": 0.3310433626174927}, {"id": 349, "seek": 145124, "start": 1462.24, "end": 1470.28, "text": " Oh, it's just died.", "tokens": [50914, 876, 11, 309, 311, 445, 4539, 13, 51316], "temperature": 0.0, "avg_logprob": -0.5642606835616263, "compression_ratio": 1.1555555555555554, "no_speech_prob": 0.3310433626174927}, {"id": 350, "seek": 145124, "start": 1470.28, "end": 1473.64, "text": " I'm going to go out and present them out.", "tokens": [51316, 286, 478, 516, 281, 352, 484, 293, 1974, 552, 484, 13, 51484], "temperature": 0.0, "avg_logprob": -0.5642606835616263, "compression_ratio": 1.1555555555555554, "no_speech_prob": 0.3310433626174927}, {"id": 351, "seek": 147364, "start": 1474.64, "end": 1481.64, "text": " I'm going to go back in to present them out.", "tokens": [50414, 286, 478, 516, 281, 352, 646, 294, 281, 1974, 552, 484, 13, 50764], "temperature": 0.0, "avg_logprob": -0.23664670724135178, "compression_ratio": 1.7076271186440677, "no_speech_prob": 0.06101295351982117}, {"id": 352, "seek": 147364, "start": 1481.64, "end": 1486.8000000000002, "text": " Okay, so here's some reasons why the brain can't do back propagation.", "tokens": [50764, 1033, 11, 370, 510, 311, 512, 4112, 983, 264, 3567, 393, 380, 360, 646, 38377, 13, 51022], "temperature": 0.0, "avg_logprob": -0.23664670724135178, "compression_ratio": 1.7076271186440677, "no_speech_prob": 0.06101295351982117}, {"id": 353, "seek": 147364, "start": 1486.8000000000002, "end": 1492.24, "text": " The first reason is they say, well, it doesn't get the supervision signal.", "tokens": [51022, 440, 700, 1778, 307, 436, 584, 11, 731, 11, 309, 1177, 380, 483, 264, 32675, 6358, 13, 51294], "temperature": 0.0, "avg_logprob": -0.23664670724135178, "compression_ratio": 1.7076271186440677, "no_speech_prob": 0.06101295351982117}, {"id": 354, "seek": 147364, "start": 1492.24, "end": 1495.72, "text": " And they're imagining that the supervision signal is like you take a micro pipette and", "tokens": [51294, 400, 436, 434, 27798, 300, 264, 32675, 6358, 307, 411, 291, 747, 257, 4532, 8489, 3007, 293, 51468], "temperature": 0.0, "avg_logprob": -0.23664670724135178, "compression_ratio": 1.7076271186440677, "no_speech_prob": 0.06101295351982117}, {"id": 355, "seek": 147364, "start": 1495.72, "end": 1500.3200000000002, "text": " you put it into the infrotemporal cortex and you inject the right answer and the brain", "tokens": [51468, 291, 829, 309, 666, 264, 1536, 10536, 11840, 304, 33312, 293, 291, 10711, 264, 558, 1867, 293, 264, 3567, 51698], "temperature": 0.0, "avg_logprob": -0.23664670724135178, "compression_ratio": 1.7076271186440677, "no_speech_prob": 0.06101295351982117}, {"id": 356, "seek": 147364, "start": 1500.3200000000002, "end": 1503.24, "text": " doesn't have anything like that, right?", "tokens": [51698, 1177, 380, 362, 1340, 411, 300, 11, 558, 30, 51844], "temperature": 0.0, "avg_logprob": -0.23664670724135178, "compression_ratio": 1.7076271186440677, "no_speech_prob": 0.06101295351982117}, {"id": 357, "seek": 150324, "start": 1503.24, "end": 1508.44, "text": " But actually, if you take that language model, it didn't need label data, it was just trying", "tokens": [50364, 583, 767, 11, 498, 291, 747, 300, 2856, 2316, 11, 309, 994, 380, 643, 7645, 1412, 11, 309, 390, 445, 1382, 50624], "temperature": 0.0, "avg_logprob": -0.13435972653902495, "compression_ratio": 1.8356643356643356, "no_speech_prob": 0.0065168230794370174}, {"id": 358, "seek": 150324, "start": 1508.44, "end": 1509.96, "text": " to predict the next word.", "tokens": [50624, 281, 6069, 264, 958, 1349, 13, 50700], "temperature": 0.0, "avg_logprob": -0.13435972653902495, "compression_ratio": 1.8356643356643356, "no_speech_prob": 0.0065168230794370174}, {"id": 359, "seek": 150324, "start": 1509.96, "end": 1514.48, "text": " So you can often use part of the input, maybe a future part of the input, or maybe a small", "tokens": [50700, 407, 291, 393, 2049, 764, 644, 295, 264, 4846, 11, 1310, 257, 2027, 644, 295, 264, 4846, 11, 420, 1310, 257, 1359, 50926], "temperature": 0.0, "avg_logprob": -0.13435972653902495, "compression_ratio": 1.8356643356643356, "no_speech_prob": 0.0065168230794370174}, {"id": 360, "seek": 150324, "start": 1514.48, "end": 1520.04, "text": " part of an image as the right answer, and so you can get supervision signals easily.", "tokens": [50926, 644, 295, 364, 3256, 382, 264, 558, 1867, 11, 293, 370, 291, 393, 483, 32675, 12354, 3612, 13, 51204], "temperature": 0.0, "avg_logprob": -0.13435972653902495, "compression_ratio": 1.8356643356643356, "no_speech_prob": 0.0065168230794370174}, {"id": 361, "seek": 150324, "start": 1520.04, "end": 1522.8, "text": " So there's no problem about supervision signals.", "tokens": [51204, 407, 456, 311, 572, 1154, 466, 32675, 12354, 13, 51342], "temperature": 0.0, "avg_logprob": -0.13435972653902495, "compression_ratio": 1.8356643356643356, "no_speech_prob": 0.0065168230794370174}, {"id": 362, "seek": 150324, "start": 1522.8, "end": 1527.8, "text": " The second reason is neurons don't send real-valued activities, they send spikes.", "tokens": [51342, 440, 1150, 1778, 307, 22027, 500, 380, 2845, 957, 12, 3337, 5827, 5354, 11, 436, 2845, 28997, 13, 51592], "temperature": 0.0, "avg_logprob": -0.13435972653902495, "compression_ratio": 1.8356643356643356, "no_speech_prob": 0.0065168230794370174}, {"id": 363, "seek": 150324, "start": 1527.8, "end": 1532.36, "text": " And back propagation is using these real-valued activities so you can get nice, smooth derivatives.", "tokens": [51592, 400, 646, 38377, 307, 1228, 613, 957, 12, 3337, 5827, 5354, 370, 291, 393, 483, 1481, 11, 5508, 33733, 13, 51820], "temperature": 0.0, "avg_logprob": -0.13435972653902495, "compression_ratio": 1.8356643356643356, "no_speech_prob": 0.0065168230794370174}, {"id": 364, "seek": 153236, "start": 1532.36, "end": 1536.04, "text": " So back propagation can't possibly be what's going on in the brain.", "tokens": [50364, 407, 646, 38377, 393, 380, 6264, 312, 437, 311, 516, 322, 294, 264, 3567, 13, 50548], "temperature": 0.0, "avg_logprob": -0.14860161401892222, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.004227656405419111}, {"id": 365, "seek": 153236, "start": 1536.04, "end": 1539.0, "text": " The third objection is, neurons have to send two signals.", "tokens": [50548, 440, 2636, 35756, 307, 11, 22027, 362, 281, 2845, 732, 12354, 13, 50696], "temperature": 0.0, "avg_logprob": -0.14860161401892222, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.004227656405419111}, {"id": 366, "seek": 153236, "start": 1539.0, "end": 1545.24, "text": " They have to send the activity forwards and they have to send error derivatives backwards.", "tokens": [50696, 814, 362, 281, 2845, 264, 5191, 30126, 293, 436, 362, 281, 2845, 6713, 33733, 12204, 13, 51008], "temperature": 0.0, "avg_logprob": -0.14860161401892222, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.004227656405419111}, {"id": 367, "seek": 153236, "start": 1545.24, "end": 1550.4399999999998, "text": " The signal they have to send backwards is, how sensitive am I to changes in my input?", "tokens": [51008, 440, 6358, 436, 362, 281, 2845, 12204, 307, 11, 577, 9477, 669, 286, 281, 2962, 294, 452, 4846, 30, 51268], "temperature": 0.0, "avg_logprob": -0.14860161401892222, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.004227656405419111}, {"id": 368, "seek": 153236, "start": 1550.4399999999998, "end": 1558.6399999999999, "text": " Or rather, if you change my input, how much does that help with the final answer?", "tokens": [51268, 1610, 2831, 11, 498, 291, 1319, 452, 4846, 11, 577, 709, 775, 300, 854, 365, 264, 2572, 1867, 30, 51678], "temperature": 0.0, "avg_logprob": -0.14860161401892222, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.004227656405419111}, {"id": 369, "seek": 155864, "start": 1558.64, "end": 1563.0, "text": " And the last thing is about neurons having reciprocal connections because you have to,", "tokens": [50364, 400, 264, 1036, 551, 307, 466, 22027, 1419, 46948, 9271, 570, 291, 362, 281, 11, 50582], "temperature": 0.0, "avg_logprob": -0.14032062376388396, "compression_ratio": 1.66, "no_speech_prob": 0.1057264655828476}, {"id": 370, "seek": 155864, "start": 1563.0, "end": 1568.48, "text": " when you send things backwards, if you use a different neuron, you have to use the same", "tokens": [50582, 562, 291, 2845, 721, 12204, 11, 498, 291, 764, 257, 819, 34090, 11, 291, 362, 281, 764, 264, 912, 50856], "temperature": 0.0, "avg_logprob": -0.14032062376388396, "compression_ratio": 1.66, "no_speech_prob": 0.1057264655828476}, {"id": 371, "seek": 155864, "start": 1568.48, "end": 1571.5200000000002, "text": " weight as the forward weight.", "tokens": [50856, 3364, 382, 264, 2128, 3364, 13, 51008], "temperature": 0.0, "avg_logprob": -0.14032062376388396, "compression_ratio": 1.66, "no_speech_prob": 0.1057264655828476}, {"id": 372, "seek": 155864, "start": 1571.5200000000002, "end": 1577.0, "text": " I'm not going to tell you how you can overcome that, but you can easily.", "tokens": [51008, 286, 478, 406, 516, 281, 980, 291, 577, 291, 393, 10473, 300, 11, 457, 291, 393, 3612, 13, 51282], "temperature": 0.0, "avg_logprob": -0.14032062376388396, "compression_ratio": 1.66, "no_speech_prob": 0.1057264655828476}, {"id": 373, "seek": 155864, "start": 1577.0, "end": 1581.76, "text": " So supervision signals isn't really a problem, there's many ways to get a supervision signal.", "tokens": [51282, 407, 32675, 12354, 1943, 380, 534, 257, 1154, 11, 456, 311, 867, 2098, 281, 483, 257, 32675, 6358, 13, 51520], "temperature": 0.0, "avg_logprob": -0.14032062376388396, "compression_ratio": 1.66, "no_speech_prob": 0.1057264655828476}, {"id": 374, "seek": 155864, "start": 1581.76, "end": 1587.0, "text": " The simplest is predicting what comes next.", "tokens": [51520, 440, 22811, 307, 32884, 437, 1487, 958, 13, 51782], "temperature": 0.0, "avg_logprob": -0.14032062376388396, "compression_ratio": 1.66, "no_speech_prob": 0.1057264655828476}, {"id": 375, "seek": 158700, "start": 1587.0, "end": 1591.96, "text": " Now the question of can neurons communicate real values?", "tokens": [50364, 823, 264, 1168, 295, 393, 22027, 7890, 957, 4190, 30, 50612], "temperature": 0.0, "avg_logprob": -0.15657924016316732, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0066771903075277805}, {"id": 376, "seek": 158700, "start": 1591.96, "end": 1596.56, "text": " Well the first thing to notice about back propagation is, if you have very noisy estimates", "tokens": [50612, 1042, 264, 700, 551, 281, 3449, 466, 646, 38377, 307, 11, 498, 291, 362, 588, 24518, 20561, 50842], "temperature": 0.0, "avg_logprob": -0.15657924016316732, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0066771903075277805}, {"id": 377, "seek": 158700, "start": 1596.56, "end": 1599.92, "text": " of the gradient, it works just as well.", "tokens": [50842, 295, 264, 16235, 11, 309, 1985, 445, 382, 731, 13, 51010], "temperature": 0.0, "avg_logprob": -0.15657924016316732, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0066771903075277805}, {"id": 378, "seek": 158700, "start": 1599.92, "end": 1603.76, "text": " It's very, very tolerant of noise as long as it's unbiased noise.", "tokens": [51010, 467, 311, 588, 11, 588, 45525, 295, 5658, 382, 938, 382, 309, 311, 517, 5614, 1937, 5658, 13, 51202], "temperature": 0.0, "avg_logprob": -0.15657924016316732, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0066771903075277805}, {"id": 379, "seek": 158700, "start": 1603.76, "end": 1608.0, "text": " So for example, the signal you send forwards can be one bit, one stochastic bit, and the", "tokens": [51202, 407, 337, 1365, 11, 264, 6358, 291, 2845, 30126, 393, 312, 472, 857, 11, 472, 342, 8997, 2750, 857, 11, 293, 264, 51414], "temperature": 0.0, "avg_logprob": -0.15657924016316732, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0066771903075277805}, {"id": 380, "seek": 158700, "start": 1608.0, "end": 1610.52, "text": " signal you send backwards can be two bits.", "tokens": [51414, 6358, 291, 2845, 12204, 393, 312, 732, 9239, 13, 51540], "temperature": 0.0, "avg_logprob": -0.15657924016316732, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0066771903075277805}, {"id": 381, "seek": 158700, "start": 1610.52, "end": 1615.12, "text": " If they have the right average value, if their expected values are correct, then they're", "tokens": [51540, 759, 436, 362, 264, 558, 4274, 2158, 11, 498, 641, 5176, 4190, 366, 3006, 11, 550, 436, 434, 51770], "temperature": 0.0, "avg_logprob": -0.15657924016316732, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0066771903075277805}, {"id": 382, "seek": 161512, "start": 1615.12, "end": 1622.32, "text": " just this expected value plus some noise, and the whole system still works fine.", "tokens": [50364, 445, 341, 5176, 2158, 1804, 512, 5658, 11, 293, 264, 1379, 1185, 920, 1985, 2489, 13, 50724], "temperature": 0.0, "avg_logprob": -0.14824539549807284, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0027797508519142866}, {"id": 383, "seek": 161512, "start": 1622.32, "end": 1626.8, "text": " So in the brain, you have a neuron.", "tokens": [50724, 407, 294, 264, 3567, 11, 291, 362, 257, 34090, 13, 50948], "temperature": 0.0, "avg_logprob": -0.14824539549807284, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0027797508519142866}, {"id": 384, "seek": 161512, "start": 1626.8, "end": 1632.32, "text": " At any instant, the neuron has an underlying firing rate, and it produces spikes, and for", "tokens": [50948, 1711, 604, 9836, 11, 264, 34090, 575, 364, 14217, 16045, 3314, 11, 293, 309, 14725, 28997, 11, 293, 337, 51224], "temperature": 0.0, "avg_logprob": -0.14824539549807284, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0027797508519142866}, {"id": 385, "seek": 161512, "start": 1632.32, "end": 1637.4399999999998, "text": " now let's just suppose it produces spikes according to a Poisson process.", "tokens": [51224, 586, 718, 311, 445, 7297, 309, 14725, 28997, 4650, 281, 257, 6165, 30472, 1399, 13, 51480], "temperature": 0.0, "avg_logprob": -0.14824539549807284, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0027797508519142866}, {"id": 386, "seek": 161512, "start": 1637.4399999999998, "end": 1640.9199999999998, "text": " So it's probability of producing a spike in a small interval, which is the underlying", "tokens": [51480, 407, 309, 311, 8482, 295, 10501, 257, 21053, 294, 257, 1359, 15035, 11, 597, 307, 264, 14217, 51654], "temperature": 0.0, "avg_logprob": -0.14824539549807284, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0027797508519142866}, {"id": 387, "seek": 161512, "start": 1640.9199999999998, "end": 1643.32, "text": " firing rate.", "tokens": [51654, 16045, 3314, 13, 51774], "temperature": 0.0, "avg_logprob": -0.14824539549807284, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0027797508519142866}, {"id": 388, "seek": 164332, "start": 1643.52, "end": 1649.6399999999999, "text": " The question is, suppose we treated it as if it could send that underlying firing rate.", "tokens": [50374, 440, 1168, 307, 11, 7297, 321, 8668, 309, 382, 498, 309, 727, 2845, 300, 14217, 16045, 3314, 13, 50680], "temperature": 0.0, "avg_logprob": -0.14348463217417398, "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.0017968129832297564}, {"id": 389, "seek": 164332, "start": 1649.6399999999999, "end": 1653.1599999999999, "text": " When it sends a Poisson spike, it's just a very noisy version of the underlying firing", "tokens": [50680, 1133, 309, 14790, 257, 6165, 30472, 21053, 11, 309, 311, 445, 257, 588, 24518, 3037, 295, 264, 14217, 16045, 50856], "temperature": 0.0, "avg_logprob": -0.14348463217417398, "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.0017968129832297564}, {"id": 390, "seek": 164332, "start": 1653.1599999999999, "end": 1655.1599999999999, "text": " rate.", "tokens": [50856, 3314, 13, 50956], "temperature": 0.0, "avg_logprob": -0.14348463217417398, "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.0017968129832297564}, {"id": 391, "seek": 164332, "start": 1655.1599999999999, "end": 1660.52, "text": " It's a one or a zero, but its expected value is the underlying firing rate.", "tokens": [50956, 467, 311, 257, 472, 420, 257, 4018, 11, 457, 1080, 5176, 2158, 307, 264, 14217, 16045, 3314, 13, 51224], "temperature": 0.0, "avg_logprob": -0.14348463217417398, "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.0017968129832297564}, {"id": 392, "seek": 164332, "start": 1660.52, "end": 1667.28, "text": " So how well do neural networks work if we send very noisy signals?", "tokens": [51224, 407, 577, 731, 360, 18161, 9590, 589, 498, 321, 2845, 588, 24518, 12354, 30, 51562], "temperature": 0.0, "avg_logprob": -0.14348463217417398, "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.0017968129832297564}, {"id": 393, "seek": 164332, "start": 1667.28, "end": 1672.8799999999999, "text": " So I'm going to have a statistics digression.", "tokens": [51562, 407, 286, 478, 516, 281, 362, 257, 12523, 2528, 2775, 13, 51842], "temperature": 0.0, "avg_logprob": -0.14348463217417398, "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.0017968129832297564}, {"id": 394, "seek": 167288, "start": 1672.88, "end": 1677.4, "text": " If you do statistics 101, they tell you you shouldn't have more parameters in your model", "tokens": [50364, 759, 291, 360, 12523, 21055, 11, 436, 980, 291, 291, 4659, 380, 362, 544, 9834, 294, 428, 2316, 50590], "temperature": 0.0, "avg_logprob": -0.17538314681869369, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.00044685788452625275}, {"id": 395, "seek": 167288, "start": 1677.4, "end": 1679.2800000000002, "text": " than you have data points.", "tokens": [50590, 813, 291, 362, 1412, 2793, 13, 50684], "temperature": 0.0, "avg_logprob": -0.17538314681869369, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.00044685788452625275}, {"id": 396, "seek": 167288, "start": 1679.2800000000002, "end": 1683.0800000000002, "text": " You really ought to have quite a few data points for each parameter.", "tokens": [50684, 509, 534, 13416, 281, 362, 1596, 257, 1326, 1412, 2793, 337, 1184, 13075, 13, 50874], "temperature": 0.0, "avg_logprob": -0.17538314681869369, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.00044685788452625275}, {"id": 397, "seek": 167288, "start": 1683.0800000000002, "end": 1686.4, "text": " It turns out this is completely wrong.", "tokens": [50874, 467, 4523, 484, 341, 307, 2584, 2085, 13, 51040], "temperature": 0.0, "avg_logprob": -0.17538314681869369, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.00044685788452625275}, {"id": 398, "seek": 167288, "start": 1686.4, "end": 1691.5200000000002, "text": " The Bayesians knew it was wrong, actually.", "tokens": [51040, 440, 7840, 279, 2567, 2586, 309, 390, 2085, 11, 767, 13, 51296], "temperature": 0.0, "avg_logprob": -0.17538314681869369, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.00044685788452625275}, {"id": 399, "seek": 167288, "start": 1691.5200000000002, "end": 1695.2800000000002, "text": " The brain is not in the same regime of statistics 101.", "tokens": [51296, 440, 3567, 307, 406, 294, 264, 912, 13120, 295, 12523, 21055, 13, 51484], "temperature": 0.0, "avg_logprob": -0.17538314681869369, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.00044685788452625275}, {"id": 400, "seek": 167288, "start": 1695.2800000000002, "end": 1699.16, "text": " In the brain, you're fitting about 10 to the 14 parameters, and you have about 10 to the", "tokens": [51484, 682, 264, 3567, 11, 291, 434, 15669, 466, 1266, 281, 264, 3499, 9834, 11, 293, 291, 362, 466, 1266, 281, 264, 51678], "temperature": 0.0, "avg_logprob": -0.17538314681869369, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.00044685788452625275}, {"id": 401, "seek": 167288, "start": 1699.16, "end": 1701.2800000000002, "text": " nine seconds.", "tokens": [51678, 4949, 3949, 13, 51784], "temperature": 0.0, "avg_logprob": -0.17538314681869369, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.00044685788452625275}, {"id": 402, "seek": 170128, "start": 1701.32, "end": 1707.04, "text": " So even if you have sort of 10 experiences per second, so even if you take 100 milliseconds", "tokens": [50366, 407, 754, 498, 291, 362, 1333, 295, 1266, 5235, 680, 1150, 11, 370, 754, 498, 291, 747, 2319, 34184, 50652], "temperature": 0.0, "avg_logprob": -0.16021305887322676, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.000926884647924453}, {"id": 403, "seek": 170128, "start": 1707.04, "end": 1713.8799999999999, "text": " is the time for an experience, that's the kind of backward masking time, you have like", "tokens": [50652, 307, 264, 565, 337, 364, 1752, 11, 300, 311, 264, 733, 295, 23897, 31226, 565, 11, 291, 362, 411, 50994], "temperature": 0.0, "avg_logprob": -0.16021305887322676, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.000926884647924453}, {"id": 404, "seek": 170128, "start": 1713.8799999999999, "end": 1718.92, "text": " 10,000 synapses per 100 milliseconds of your life.", "tokens": [50994, 1266, 11, 1360, 5451, 2382, 279, 680, 2319, 34184, 295, 428, 993, 13, 51246], "temperature": 0.0, "avg_logprob": -0.16021305887322676, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.000926884647924453}, {"id": 405, "seek": 170128, "start": 1718.92, "end": 1722.68, "text": " You're throwing a lot of parameters.", "tokens": [51246, 509, 434, 10238, 257, 688, 295, 9834, 13, 51434], "temperature": 0.0, "avg_logprob": -0.16021305887322676, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.000926884647924453}, {"id": 406, "seek": 170128, "start": 1722.68, "end": 1726.92, "text": " So if your mother just kept saying, good, bad, good, bad, good, bad, she couldn't possibly", "tokens": [51434, 407, 498, 428, 2895, 445, 4305, 1566, 11, 665, 11, 1578, 11, 665, 11, 1578, 11, 665, 11, 1578, 11, 750, 2809, 380, 6264, 51646], "temperature": 0.0, "avg_logprob": -0.16021305887322676, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.000926884647924453}, {"id": 407, "seek": 172692, "start": 1726.96, "end": 1731.96, "text": " provide enough information to learn all those 10 to the 14 parameters.", "tokens": [50366, 2893, 1547, 1589, 281, 1466, 439, 729, 1266, 281, 264, 3499, 9834, 13, 50616], "temperature": 0.0, "avg_logprob": -0.22022342681884766, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.003422628389671445}, {"id": 408, "seek": 172692, "start": 1731.96, "end": 1738.96, "text": " And here's what they teach you wrong in statistics.", "tokens": [50616, 400, 510, 311, 437, 436, 2924, 291, 2085, 294, 12523, 13, 50966], "temperature": 0.0, "avg_logprob": -0.22022342681884766, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.003422628389671445}, {"id": 409, "seek": 172692, "start": 1741.2, "end": 1744.96, "text": " Everybody knows that if you've got a given size model with a given number of parameters,", "tokens": [51078, 7646, 3255, 300, 498, 291, 600, 658, 257, 2212, 2744, 2316, 365, 257, 2212, 1230, 295, 9834, 11, 51266], "temperature": 0.0, "avg_logprob": -0.22022342681884766, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.003422628389671445}, {"id": 410, "seek": 172692, "start": 1744.96, "end": 1747.8000000000002, "text": " the more data you have, the better you'll generalize.", "tokens": [51266, 264, 544, 1412, 291, 362, 11, 264, 1101, 291, 603, 2674, 1125, 13, 51408], "temperature": 0.0, "avg_logprob": -0.22022342681884766, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.003422628389671445}, {"id": 411, "seek": 172692, "start": 1747.8000000000002, "end": 1750.76, "text": " So for a given size of model, it's always better to have more data.", "tokens": [51408, 407, 337, 257, 2212, 2744, 295, 2316, 11, 309, 311, 1009, 1101, 281, 362, 544, 1412, 13, 51556], "temperature": 0.0, "avg_logprob": -0.22022342681884766, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.003422628389671445}, {"id": 412, "seek": 172692, "start": 1750.76, "end": 1754.3600000000001, "text": " In fact, the best thing you can do is get more data.", "tokens": [51556, 682, 1186, 11, 264, 1151, 551, 291, 393, 360, 307, 483, 544, 1412, 13, 51736], "temperature": 0.0, "avg_logprob": -0.22022342681884766, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.003422628389671445}, {"id": 413, "seek": 175436, "start": 1755.1599999999999, "end": 1759.52, "text": " Okay, but that doesn't mean that if you've got a fixed amount of data, you should make", "tokens": [50404, 1033, 11, 457, 300, 1177, 380, 914, 300, 498, 291, 600, 658, 257, 6806, 2372, 295, 1412, 11, 291, 820, 652, 50622], "temperature": 0.0, "avg_logprob": -0.1957692598041735, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0010913952719420195}, {"id": 414, "seek": 175436, "start": 1759.52, "end": 1761.6, "text": " it look like a lot by having a small model.", "tokens": [50622, 309, 574, 411, 257, 688, 538, 1419, 257, 1359, 2316, 13, 50726], "temperature": 0.0, "avg_logprob": -0.1957692598041735, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0010913952719420195}, {"id": 415, "seek": 175436, "start": 1761.6, "end": 1764.1999999999998, "text": " That's what they tell you in statistics 101.", "tokens": [50726, 663, 311, 437, 436, 980, 291, 294, 12523, 21055, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1957692598041735, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0010913952719420195}, {"id": 416, "seek": 175436, "start": 1764.1999999999998, "end": 1766.1999999999998, "text": " Okay?", "tokens": [50856, 1033, 30, 50956], "temperature": 0.0, "avg_logprob": -0.1957692598041735, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0010913952719420195}, {"id": 417, "seek": 175436, "start": 1766.1999999999998, "end": 1773.4799999999998, "text": " Big models are good if you regularize them, if you stop them doing crazy things.", "tokens": [50956, 5429, 5245, 366, 665, 498, 291, 3890, 1125, 552, 11, 498, 291, 1590, 552, 884, 3219, 721, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1957692598041735, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0010913952719420195}, {"id": 418, "seek": 175436, "start": 1773.4799999999998, "end": 1776.9199999999998, "text": " We can see that using a lot of parameters is good, that you can always win by having", "tokens": [51320, 492, 393, 536, 300, 1228, 257, 688, 295, 9834, 307, 665, 11, 300, 291, 393, 1009, 1942, 538, 1419, 51492], "temperature": 0.0, "avg_logprob": -0.1957692598041735, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0010913952719420195}, {"id": 419, "seek": 175436, "start": 1776.9199999999998, "end": 1777.9199999999998, "text": " more parameters.", "tokens": [51492, 544, 9834, 13, 51542], "temperature": 0.0, "avg_logprob": -0.1957692598041735, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0010913952719420195}, {"id": 420, "seek": 175436, "start": 1777.9199999999998, "end": 1779.9199999999998, "text": " And the way you do that is say, I'm going to have a committee.", "tokens": [51542, 400, 264, 636, 291, 360, 300, 307, 584, 11, 286, 478, 516, 281, 362, 257, 7482, 13, 51642], "temperature": 0.0, "avg_logprob": -0.1957692598041735, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0010913952719420195}, {"id": 421, "seek": 175436, "start": 1779.9199999999998, "end": 1783.0, "text": " I'm going to learn lots of different little neural nets.", "tokens": [51642, 286, 478, 516, 281, 1466, 3195, 295, 819, 707, 18161, 36170, 13, 51796], "temperature": 0.0, "avg_logprob": -0.1957692598041735, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0010913952719420195}, {"id": 422, "seek": 178300, "start": 1783.0, "end": 1785.96, "text": " If you give me more parameters, I'll learn more different neural nets.", "tokens": [50364, 759, 291, 976, 385, 544, 9834, 11, 286, 603, 1466, 544, 819, 18161, 36170, 13, 50512], "temperature": 0.0, "avg_logprob": -0.15264088908831278, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0012404988519847393}, {"id": 423, "seek": 178300, "start": 1785.96, "end": 1788.24, "text": " And then I'll average what they all say.", "tokens": [50512, 400, 550, 286, 603, 4274, 437, 436, 439, 584, 13, 50626], "temperature": 0.0, "avg_logprob": -0.15264088908831278, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0012404988519847393}, {"id": 424, "seek": 178300, "start": 1788.24, "end": 1789.68, "text": " And you'll always win.", "tokens": [50626, 400, 291, 603, 1009, 1942, 13, 50698], "temperature": 0.0, "avg_logprob": -0.15264088908831278, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0012404988519847393}, {"id": 425, "seek": 178300, "start": 1789.68, "end": 1796.0, "text": " It's a sort of declining win, but if you have enough of them, you'll win by having more.", "tokens": [50698, 467, 311, 257, 1333, 295, 34298, 1942, 11, 457, 498, 291, 362, 1547, 295, 552, 11, 291, 603, 1942, 538, 1419, 544, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15264088908831278, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0012404988519847393}, {"id": 426, "seek": 178300, "start": 1796.0, "end": 1800.84, "text": " So it's always better to have more parameters.", "tokens": [51014, 407, 309, 311, 1009, 1101, 281, 362, 544, 9834, 13, 51256], "temperature": 0.0, "avg_logprob": -0.15264088908831278, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0012404988519847393}, {"id": 427, "seek": 178300, "start": 1800.84, "end": 1807.84, "text": " It turns out if you have a fixed amount of data and you have enough computation power,", "tokens": [51256, 467, 4523, 484, 498, 291, 362, 257, 6806, 2372, 295, 1412, 293, 291, 362, 1547, 24903, 1347, 11, 51606], "temperature": 0.0, "avg_logprob": -0.15264088908831278, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0012404988519847393}, {"id": 428, "seek": 180784, "start": 1807.9199999999998, "end": 1813.6799999999998, "text": " the brain has, you should always use such a big model that the amount of data looks", "tokens": [50368, 264, 3567, 575, 11, 291, 820, 1009, 764, 1270, 257, 955, 2316, 300, 264, 2372, 295, 1412, 1542, 50656], "temperature": 0.0, "avg_logprob": -0.20910770416259766, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.03739931806921959}, {"id": 429, "seek": 180784, "start": 1813.6799999999998, "end": 1814.6799999999998, "text": " small.", "tokens": [50656, 1359, 13, 50706], "temperature": 0.0, "avg_logprob": -0.20910770416259766, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.03739931806921959}, {"id": 430, "seek": 180784, "start": 1814.6799999999998, "end": 1817.76, "text": " That's the regime you ought to be in for a fixed amount of data.", "tokens": [50706, 663, 311, 264, 13120, 291, 13416, 281, 312, 294, 337, 257, 6806, 2372, 295, 1412, 13, 50860], "temperature": 0.0, "avg_logprob": -0.20910770416259766, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.03739931806921959}, {"id": 431, "seek": 180784, "start": 1817.76, "end": 1822.8799999999999, "text": " That is, if you take the limit when the amount of data is fixed and you have unlimited computation", "tokens": [50860, 663, 307, 11, 498, 291, 747, 264, 4948, 562, 264, 2372, 295, 1412, 307, 6806, 293, 291, 362, 21950, 24903, 51116], "temperature": 0.0, "avg_logprob": -0.20910770416259766, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.03739931806921959}, {"id": 432, "seek": 180784, "start": 1822.8799999999999, "end": 1828.04, "text": " and ask now how big would you like your model to be, you'd like your model to be much bigger", "tokens": [51116, 293, 1029, 586, 577, 955, 576, 291, 411, 428, 2316, 281, 312, 11, 291, 1116, 411, 428, 2316, 281, 312, 709, 3801, 51374], "temperature": 0.0, "avg_logprob": -0.20910770416259766, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.03739931806921959}, {"id": 433, "seek": 180784, "start": 1828.04, "end": 1830.6799999999998, "text": " than the data.", "tokens": [51374, 813, 264, 1412, 13, 51506], "temperature": 0.0, "avg_logprob": -0.20910770416259766, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.03739931806921959}, {"id": 434, "seek": 180784, "start": 1830.6799999999998, "end": 1833.36, "text": " Okay.", "tokens": [51506, 1033, 13, 51640], "temperature": 0.0, "avg_logprob": -0.20910770416259766, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.03739931806921959}, {"id": 435, "seek": 183336, "start": 1834.08, "end": 1836.84, "text": " That only works if you have a good regularizer.", "tokens": [50400, 663, 787, 1985, 498, 291, 362, 257, 665, 3890, 6545, 13, 50538], "temperature": 0.0, "avg_logprob": -0.14585343691018912, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.004187741316854954}, {"id": 436, "seek": 183336, "start": 1836.84, "end": 1844.4799999999998, "text": " And I'm now going to tell you a very good regularizer called Dropout.", "tokens": [50538, 400, 286, 478, 586, 516, 281, 980, 291, 257, 588, 665, 3890, 6545, 1219, 17675, 346, 13, 50920], "temperature": 0.0, "avg_logprob": -0.14585343691018912, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.004187741316854954}, {"id": 437, "seek": 183336, "start": 1844.4799999999998, "end": 1847.52, "text": " So this is to use in neural networks where you have a lot more parameters than you have", "tokens": [50920, 407, 341, 307, 281, 764, 294, 18161, 9590, 689, 291, 362, 257, 688, 544, 9834, 813, 291, 362, 51072], "temperature": 0.0, "avg_logprob": -0.14585343691018912, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.004187741316854954}, {"id": 438, "seek": 183336, "start": 1847.52, "end": 1850.08, "text": " data points to train them on.", "tokens": [51072, 1412, 2793, 281, 3847, 552, 322, 13, 51200], "temperature": 0.0, "avg_logprob": -0.14585343691018912, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.004187741316854954}, {"id": 439, "seek": 183336, "start": 1850.08, "end": 1853.84, "text": " And you could learn an ensemble of little models, and this is a way of learning an ensemble", "tokens": [51200, 400, 291, 727, 1466, 364, 19492, 295, 707, 5245, 11, 293, 341, 307, 257, 636, 295, 2539, 364, 19492, 51388], "temperature": 0.0, "avg_logprob": -0.14585343691018912, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.004187741316854954}, {"id": 440, "seek": 183336, "start": 1853.84, "end": 1859.8799999999999, "text": " of many more models, but the models in the ensemble can share things with each other.", "tokens": [51388, 295, 867, 544, 5245, 11, 457, 264, 5245, 294, 264, 19492, 393, 2073, 721, 365, 1184, 661, 13, 51690], "temperature": 0.0, "avg_logprob": -0.14585343691018912, "compression_ratio": 1.7801724137931034, "no_speech_prob": 0.004187741316854954}, {"id": 441, "seek": 185988, "start": 1859.92, "end": 1865.64, "text": " So the idea is, if we just have one hidden layer in a neural net, we put the data in", "tokens": [50366, 407, 264, 1558, 307, 11, 498, 321, 445, 362, 472, 7633, 4583, 294, 257, 18161, 2533, 11, 321, 829, 264, 1412, 294, 50652], "temperature": 0.0, "avg_logprob": -0.12278190766922151, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.0009426979813724756}, {"id": 442, "seek": 185988, "start": 1865.64, "end": 1871.64, "text": " and each time we show the data vector, we randomly remove half the neurons.", "tokens": [50652, 293, 1184, 565, 321, 855, 264, 1412, 8062, 11, 321, 16979, 4159, 1922, 264, 22027, 13, 50952], "temperature": 0.0, "avg_logprob": -0.12278190766922151, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.0009426979813724756}, {"id": 443, "seek": 185988, "start": 1871.64, "end": 1879.4, "text": " So we randomly get rid of half the neurons in your brain and only use the ones that remain.", "tokens": [50952, 407, 321, 16979, 483, 3973, 295, 1922, 264, 22027, 294, 428, 3567, 293, 787, 764, 264, 2306, 300, 6222, 13, 51340], "temperature": 0.0, "avg_logprob": -0.12278190766922151, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.0009426979813724756}, {"id": 444, "seek": 185988, "start": 1879.4, "end": 1882.88, "text": " And it's a different subset we remove each time.", "tokens": [51340, 400, 309, 311, 257, 819, 25993, 321, 4159, 1184, 565, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12278190766922151, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.0009426979813724756}, {"id": 445, "seek": 185988, "start": 1882.88, "end": 1887.44, "text": " Now when we do use a neuron, we use it with the same weights each time.", "tokens": [51514, 823, 562, 321, 360, 764, 257, 34090, 11, 321, 764, 309, 365, 264, 912, 17443, 1184, 565, 13, 51742], "temperature": 0.0, "avg_logprob": -0.12278190766922151, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.0009426979813724756}, {"id": 446, "seek": 188744, "start": 1887.48, "end": 1891.0800000000002, "text": " So what you've got is if you've got h hidden neurons, you've got two to the h different", "tokens": [50366, 407, 437, 291, 600, 658, 307, 498, 291, 600, 658, 276, 7633, 22027, 11, 291, 600, 658, 732, 281, 264, 276, 819, 50546], "temperature": 0.0, "avg_logprob": -0.1273559985622283, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0008875358034856617}, {"id": 447, "seek": 188744, "start": 1891.0800000000002, "end": 1893.24, "text": " subsets of neurons you might use.", "tokens": [50546, 2090, 1385, 295, 22027, 291, 1062, 764, 13, 50654], "temperature": 0.0, "avg_logprob": -0.1273559985622283, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0008875358034856617}, {"id": 448, "seek": 188744, "start": 1893.24, "end": 1897.92, "text": " So you actually have two to the h different models, exponentially many models.", "tokens": [50654, 407, 291, 767, 362, 732, 281, 264, 276, 819, 5245, 11, 37330, 867, 5245, 13, 50888], "temperature": 0.0, "avg_logprob": -0.1273559985622283, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0008875358034856617}, {"id": 449, "seek": 188744, "start": 1897.92, "end": 1899.44, "text": " Most of the models are never used.", "tokens": [50888, 4534, 295, 264, 5245, 366, 1128, 1143, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1273559985622283, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0008875358034856617}, {"id": 450, "seek": 188744, "start": 1899.44, "end": 1904.3600000000001, "text": " A few of the models will see one example, a small fraction of them will see one example.", "tokens": [50964, 316, 1326, 295, 264, 5245, 486, 536, 472, 1365, 11, 257, 1359, 14135, 295, 552, 486, 536, 472, 1365, 13, 51210], "temperature": 0.0, "avg_logprob": -0.1273559985622283, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0008875358034856617}, {"id": 451, "seek": 188744, "start": 1904.3600000000001, "end": 1908.2, "text": " No models will see two examples.", "tokens": [51210, 883, 5245, 486, 536, 732, 5110, 13, 51402], "temperature": 0.0, "avg_logprob": -0.1273559985622283, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0008875358034856617}, {"id": 452, "seek": 188744, "start": 1908.2, "end": 1910.96, "text": " And yet they can learn because they're all sharing parameters.", "tokens": [51402, 400, 1939, 436, 393, 1466, 570, 436, 434, 439, 5414, 9834, 13, 51540], "temperature": 0.0, "avg_logprob": -0.1273559985622283, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0008875358034856617}, {"id": 453, "seek": 188744, "start": 1910.96, "end": 1914.4, "text": " So this idea of sharing parameters in a neural network is very effective.", "tokens": [51540, 407, 341, 1558, 295, 5414, 9834, 294, 257, 18161, 3209, 307, 588, 4942, 13, 51712], "temperature": 0.0, "avg_logprob": -0.1273559985622283, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0008875358034856617}, {"id": 454, "seek": 191440, "start": 1914.4, "end": 1919.24, "text": " So really you've got all these different models that are sharing parameters and you train", "tokens": [50364, 407, 534, 291, 600, 658, 439, 613, 819, 5245, 300, 366, 5414, 9834, 293, 291, 3847, 50606], "temperature": 0.0, "avg_logprob": -0.19003799983433314, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0012979896273463964}, {"id": 455, "seek": 191440, "start": 1919.24, "end": 1922.5600000000002, "text": " it up and it generalizes really well.", "tokens": [50606, 309, 493, 293, 309, 2674, 5660, 534, 731, 13, 50772], "temperature": 0.0, "avg_logprob": -0.19003799983433314, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0012979896273463964}, {"id": 456, "seek": 191440, "start": 1926.6000000000001, "end": 1928.3600000000001, "text": " So I said that.", "tokens": [50974, 407, 286, 848, 300, 13, 51062], "temperature": 0.0, "avg_logprob": -0.19003799983433314, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0012979896273463964}, {"id": 457, "seek": 191440, "start": 1928.3600000000001, "end": 1934.16, "text": " So what we know is if you get rid of a fraction of the neurons each time and treat it as though", "tokens": [51062, 407, 437, 321, 458, 307, 498, 291, 483, 3973, 295, 257, 14135, 295, 264, 22027, 1184, 565, 293, 2387, 309, 382, 1673, 51352], "temperature": 0.0, "avg_logprob": -0.19003799983433314, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0012979896273463964}, {"id": 458, "seek": 191440, "start": 1934.16, "end": 1938.24, "text": " they weren't there, it works really well.", "tokens": [51352, 436, 4999, 380, 456, 11, 309, 1985, 534, 731, 13, 51556], "temperature": 0.0, "avg_logprob": -0.19003799983433314, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0012979896273463964}, {"id": 459, "seek": 191440, "start": 1938.24, "end": 1941.2, "text": " That's just a form of noise.", "tokens": [51556, 663, 311, 445, 257, 1254, 295, 5658, 13, 51704], "temperature": 0.0, "avg_logprob": -0.19003799983433314, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0012979896273463964}, {"id": 460, "seek": 194120, "start": 1941.2, "end": 1946.3600000000001, "text": " And basically this is just an example of if you have a very big model and you add a lot", "tokens": [50364, 400, 1936, 341, 307, 445, 364, 1365, 295, 498, 291, 362, 257, 588, 955, 2316, 293, 291, 909, 257, 688, 50622], "temperature": 0.0, "avg_logprob": -0.12469403089675228, "compression_ratio": 1.7974137931034482, "no_speech_prob": 0.0008621361339464784}, {"id": 461, "seek": 194120, "start": 1946.3600000000001, "end": 1954.88, "text": " of noise, the noise allows it to generalize well and it's better to have a big model with", "tokens": [50622, 295, 5658, 11, 264, 5658, 4045, 309, 281, 2674, 1125, 731, 293, 309, 311, 1101, 281, 362, 257, 955, 2316, 365, 51048], "temperature": 0.0, "avg_logprob": -0.12469403089675228, "compression_ratio": 1.7974137931034482, "no_speech_prob": 0.0008621361339464784}, {"id": 462, "seek": 194120, "start": 1954.88, "end": 1958.72, "text": " a lot of noise than to have a small model with no noise.", "tokens": [51048, 257, 688, 295, 5658, 813, 281, 362, 257, 1359, 2316, 365, 572, 5658, 13, 51240], "temperature": 0.0, "avg_logprob": -0.12469403089675228, "compression_ratio": 1.7974137931034482, "no_speech_prob": 0.0008621361339464784}, {"id": 463, "seek": 194120, "start": 1958.72, "end": 1962.24, "text": " And so what the brain wants, because it's got such a big model compared with the amount", "tokens": [51240, 400, 370, 437, 264, 3567, 2738, 11, 570, 309, 311, 658, 1270, 257, 955, 2316, 5347, 365, 264, 2372, 51416], "temperature": 0.0, "avg_logprob": -0.12469403089675228, "compression_ratio": 1.7974137931034482, "no_speech_prob": 0.0008621361339464784}, {"id": 464, "seek": 194120, "start": 1962.24, "end": 1965.76, "text": " of data it operates on, it wants a lot of noise.", "tokens": [51416, 295, 1412, 309, 22577, 322, 11, 309, 2738, 257, 688, 295, 5658, 13, 51592], "temperature": 0.0, "avg_logprob": -0.12469403089675228, "compression_ratio": 1.7974137931034482, "no_speech_prob": 0.0008621361339464784}, {"id": 465, "seek": 194120, "start": 1965.76, "end": 1968.88, "text": " And so now a Poisson neuron is kind of ideal.", "tokens": [51592, 400, 370, 586, 257, 6165, 30472, 34090, 307, 733, 295, 7157, 13, 51748], "temperature": 0.0, "avg_logprob": -0.12469403089675228, "compression_ratio": 1.7974137931034482, "no_speech_prob": 0.0008621361339464784}, {"id": 466, "seek": 196888, "start": 1968.88, "end": 1974.0, "text": " It's got a firing rate and now it adds a whole lot of noise to that and either sends a one", "tokens": [50364, 467, 311, 658, 257, 16045, 3314, 293, 586, 309, 10860, 257, 1379, 688, 295, 5658, 281, 300, 293, 2139, 14790, 257, 472, 50620], "temperature": 0.0, "avg_logprob": -0.1284656524658203, "compression_ratio": 1.8340425531914895, "no_speech_prob": 0.0014398294733837247}, {"id": 467, "seek": 196888, "start": 1974.0, "end": 1979.88, "text": " or a zero and that actually makes it generalize much better.", "tokens": [50620, 420, 257, 4018, 293, 300, 767, 1669, 309, 2674, 1125, 709, 1101, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1284656524658203, "compression_ratio": 1.8340425531914895, "no_speech_prob": 0.0014398294733837247}, {"id": 468, "seek": 196888, "start": 1979.88, "end": 1984.0, "text": " So the argument is the reason neurons don't send real values is they don't want to.", "tokens": [50914, 407, 264, 6770, 307, 264, 1778, 22027, 500, 380, 2845, 957, 4190, 307, 436, 500, 380, 528, 281, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1284656524658203, "compression_ratio": 1.8340425531914895, "no_speech_prob": 0.0014398294733837247}, {"id": 469, "seek": 196888, "start": 1984.0, "end": 1988.2800000000002, "text": " They want to send things with a lot of noise in and that's making them generalize better.", "tokens": [51120, 814, 528, 281, 2845, 721, 365, 257, 688, 295, 5658, 294, 293, 300, 311, 1455, 552, 2674, 1125, 1101, 13, 51334], "temperature": 0.0, "avg_logprob": -0.1284656524658203, "compression_ratio": 1.8340425531914895, "no_speech_prob": 0.0014398294733837247}, {"id": 470, "seek": 196888, "start": 1988.2800000000002, "end": 1991.1200000000001, "text": " So that's not an argument against backpropagation.", "tokens": [51334, 407, 300, 311, 406, 364, 6770, 1970, 646, 79, 1513, 559, 399, 13, 51476], "temperature": 0.0, "avg_logprob": -0.1284656524658203, "compression_ratio": 1.8340425531914895, "no_speech_prob": 0.0014398294733837247}, {"id": 471, "seek": 196888, "start": 1991.1200000000001, "end": 1995.44, "text": " These dropout models are trained with backpropagation.", "tokens": [51476, 1981, 3270, 346, 5245, 366, 8895, 365, 646, 79, 1513, 559, 399, 13, 51692], "temperature": 0.0, "avg_logprob": -0.1284656524658203, "compression_ratio": 1.8340425531914895, "no_speech_prob": 0.0014398294733837247}, {"id": 472, "seek": 199544, "start": 1995.44, "end": 2004.52, "text": " So the random spikes are really just a way of adding noise to the signal to get better generalization.", "tokens": [50364, 407, 264, 4974, 28997, 366, 534, 445, 257, 636, 295, 5127, 5658, 281, 264, 6358, 281, 483, 1101, 2674, 2144, 13, 50818], "temperature": 0.0, "avg_logprob": -0.13319473488386288, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00022901353077031672}, {"id": 473, "seek": 199544, "start": 2004.52, "end": 2008.52, "text": " And now the last thing I'm going to address, I'm going to keep going till Blake stops me", "tokens": [50818, 400, 586, 264, 1036, 551, 286, 478, 516, 281, 2985, 11, 286, 478, 516, 281, 1066, 516, 4288, 23451, 10094, 385, 51018], "temperature": 0.0, "avg_logprob": -0.13319473488386288, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00022901353077031672}, {"id": 474, "seek": 199544, "start": 2008.52, "end": 2014.92, "text": " and I figure I've got about another five minutes before he gets really ratty.", "tokens": [51018, 293, 286, 2573, 286, 600, 658, 466, 1071, 1732, 2077, 949, 415, 2170, 534, 5937, 874, 13, 51338], "temperature": 0.0, "avg_logprob": -0.13319473488386288, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00022901353077031672}, {"id": 475, "seek": 199544, "start": 2014.92, "end": 2022.96, "text": " So the output of a neuron represents the presence of a feature in the current input.", "tokens": [51338, 407, 264, 5598, 295, 257, 34090, 8855, 264, 6814, 295, 257, 4111, 294, 264, 2190, 4846, 13, 51740], "temperature": 0.0, "avg_logprob": -0.13319473488386288, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00022901353077031672}, {"id": 476, "seek": 202296, "start": 2023.0, "end": 2028.0, "text": " So it's obvious the same output can't represent the error derivative, right?", "tokens": [50366, 407, 309, 311, 6322, 264, 912, 5598, 393, 380, 2906, 264, 6713, 13760, 11, 558, 30, 50616], "temperature": 0.0, "avg_logprob": -0.15678861119725682, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.0017776505555957556}, {"id": 477, "seek": 202296, "start": 2028.0, "end": 2031.88, "text": " You couldn't have a neuron that said to higher layers, this is the value of my feature", "tokens": [50616, 509, 2809, 380, 362, 257, 34090, 300, 848, 281, 2946, 7914, 11, 341, 307, 264, 2158, 295, 452, 4111, 50810], "temperature": 0.0, "avg_logprob": -0.15678861119725682, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.0017776505555957556}, {"id": 478, "seek": 202296, "start": 2031.88, "end": 2035.1200000000001, "text": " and said to lower layers, this is my error derivative.", "tokens": [50810, 293, 848, 281, 3126, 7914, 11, 341, 307, 452, 6713, 13760, 13, 50972], "temperature": 0.0, "avg_logprob": -0.15678861119725682, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.0017776505555957556}, {"id": 479, "seek": 202296, "start": 2035.1200000000001, "end": 2036.52, "text": " It couldn't be done.", "tokens": [50972, 467, 2809, 380, 312, 1096, 13, 51042], "temperature": 0.0, "avg_logprob": -0.15678861119725682, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.0017776505555957556}, {"id": 480, "seek": 202296, "start": 2036.52, "end": 2042.72, "text": " So the neurons that go backwards need to be different neurons except that that's nonsense.", "tokens": [51042, 407, 264, 22027, 300, 352, 12204, 643, 281, 312, 819, 22027, 3993, 300, 300, 311, 14925, 13, 51352], "temperature": 0.0, "avg_logprob": -0.15678861119725682, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.0017776505555957556}, {"id": 481, "seek": 202296, "start": 2042.72, "end": 2047.32, "text": " So here's my claim.", "tokens": [51352, 407, 510, 311, 452, 3932, 13, 51582], "temperature": 0.0, "avg_logprob": -0.15678861119725682, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.0017776505555957556}, {"id": 482, "seek": 202296, "start": 2047.32, "end": 2048.7200000000003, "text": " Joshua Benjo picked up on this later.", "tokens": [51582, 24005, 3964, 5134, 6183, 493, 322, 341, 1780, 13, 51652], "temperature": 0.0, "avg_logprob": -0.15678861119725682, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.0017776505555957556}, {"id": 483, "seek": 202296, "start": 2048.7200000000003, "end": 2050.76, "text": " I made this claim first in 2007.", "tokens": [51652, 286, 1027, 341, 3932, 700, 294, 12656, 13, 51754], "temperature": 0.0, "avg_logprob": -0.15678861119725682, "compression_ratio": 1.6772908366533865, "no_speech_prob": 0.0017776505555957556}, {"id": 484, "seek": 205076, "start": 2050.76, "end": 2055.36, "text": " Actually, I made it first in the ground proposal.", "tokens": [50364, 5135, 11, 286, 1027, 309, 700, 294, 264, 2727, 11494, 13, 50594], "temperature": 0.0, "avg_logprob": -0.23368766091086648, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00034919733298011124}, {"id": 485, "seek": 205076, "start": 2055.36, "end": 2059.1200000000003, "text": " And I still believe this claim even though nobody's managed to make it work really well", "tokens": [50594, 400, 286, 920, 1697, 341, 3932, 754, 1673, 5079, 311, 6453, 281, 652, 309, 589, 534, 731, 50782], "temperature": 0.0, "avg_logprob": -0.23368766091086648, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00034919733298011124}, {"id": 486, "seek": 205076, "start": 2059.1200000000003, "end": 2061.1600000000003, "text": " in the neural net yet.", "tokens": [50782, 294, 264, 18161, 2533, 1939, 13, 50884], "temperature": 0.0, "avg_logprob": -0.23368766091086648, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00034919733298011124}, {"id": 487, "seek": 205076, "start": 2061.1600000000003, "end": 2066.96, "text": " The idea is a neuron has a firing rate.", "tokens": [50884, 440, 1558, 307, 257, 34090, 575, 257, 16045, 3314, 13, 51174], "temperature": 0.0, "avg_logprob": -0.23368766091086648, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00034919733298011124}, {"id": 488, "seek": 205076, "start": 2066.96, "end": 2074.2400000000002, "text": " That's the firing rate is its real output, which is communicated stochasticly by a spike.", "tokens": [51174, 663, 311, 264, 16045, 3314, 307, 1080, 957, 5598, 11, 597, 307, 34989, 342, 8997, 2750, 356, 538, 257, 21053, 13, 51538], "temperature": 0.0, "avg_logprob": -0.23368766091086648, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.00034919733298011124}, {"id": 489, "seek": 207424, "start": 2074.24, "end": 2081.52, "text": " And that firing rate is actually changing over time, the underlying firing rate.", "tokens": [50364, 400, 300, 16045, 3314, 307, 767, 4473, 670, 565, 11, 264, 14217, 16045, 3314, 13, 50728], "temperature": 0.0, "avg_logprob": -0.12318219577564912, "compression_ratio": 1.8860103626943006, "no_speech_prob": 0.0016885744407773018}, {"id": 490, "seek": 207424, "start": 2081.52, "end": 2087.3599999999997, "text": " And the rate of change of the firing rate is used to represent the error derivative.", "tokens": [50728, 400, 264, 3314, 295, 1319, 295, 264, 16045, 3314, 307, 1143, 281, 2906, 264, 6713, 13760, 13, 51020], "temperature": 0.0, "avg_logprob": -0.12318219577564912, "compression_ratio": 1.8860103626943006, "no_speech_prob": 0.0016885744407773018}, {"id": 491, "seek": 207424, "start": 2087.3599999999997, "end": 2091.3999999999996, "text": " Now the nice thing about a rate of change is it can be positive or negative.", "tokens": [51020, 823, 264, 1481, 551, 466, 257, 3314, 295, 1319, 307, 309, 393, 312, 3353, 420, 3671, 13, 51222], "temperature": 0.0, "avg_logprob": -0.12318219577564912, "compression_ratio": 1.8860103626943006, "no_speech_prob": 0.0016885744407773018}, {"id": 492, "seek": 207424, "start": 2091.3999999999996, "end": 2096.56, "text": " So we can represent positive or negative derivatives without a neuron having to change", "tokens": [51222, 407, 321, 393, 2906, 3353, 420, 3671, 33733, 1553, 257, 34090, 1419, 281, 1319, 51480], "temperature": 0.0, "avg_logprob": -0.12318219577564912, "compression_ratio": 1.8860103626943006, "no_speech_prob": 0.0016885744407773018}, {"id": 493, "seek": 207424, "start": 2096.56, "end": 2100.56, "text": " the sort of signs of its synapses.", "tokens": [51480, 264, 1333, 295, 7880, 295, 1080, 5451, 2382, 279, 13, 51680], "temperature": 0.0, "avg_logprob": -0.12318219577564912, "compression_ratio": 1.8860103626943006, "no_speech_prob": 0.0016885744407773018}, {"id": 494, "seek": 210056, "start": 2100.7599999999998, "end": 2104.0, "text": " And what it's representing, the derivative it's representing is the derivative of the", "tokens": [50374, 400, 437, 309, 311, 13460, 11, 264, 13760, 309, 311, 13460, 307, 264, 13760, 295, 264, 50536], "temperature": 0.0, "avg_logprob": -0.18361542930065747, "compression_ratio": 1.8413793103448275, "no_speech_prob": 0.009014843963086605}, {"id": 495, "seek": 210056, "start": 2104.0, "end": 2106.92, "text": " error with respect to the input to the neuron.", "tokens": [50536, 6713, 365, 3104, 281, 264, 4846, 281, 264, 34090, 13, 50682], "temperature": 0.0, "avg_logprob": -0.18361542930065747, "compression_ratio": 1.8413793103448275, "no_speech_prob": 0.009014843963086605}, {"id": 496, "seek": 210056, "start": 2106.92, "end": 2108.4, "text": " And that gets sent back to earlier neurons.", "tokens": [50682, 400, 300, 2170, 2279, 646, 281, 3071, 22027, 13, 50756], "temperature": 0.0, "avg_logprob": -0.18361542930065747, "compression_ratio": 1.8413793103448275, "no_speech_prob": 0.009014843963086605}, {"id": 497, "seek": 210056, "start": 2108.4, "end": 2111.7999999999997, "text": " And if I had enough time, I could show you a whole bunch of slides about how this will", "tokens": [50756, 400, 498, 286, 632, 1547, 565, 11, 286, 727, 855, 291, 257, 1379, 3840, 295, 9788, 466, 577, 341, 486, 50926], "temperature": 0.0, "avg_logprob": -0.18361542930065747, "compression_ratio": 1.8413793103448275, "no_speech_prob": 0.009014843963086605}, {"id": 498, "seek": 210056, "start": 2111.7999999999997, "end": 2114.4, "text": " do back prop.", "tokens": [50926, 360, 646, 2365, 13, 51056], "temperature": 0.0, "avg_logprob": -0.18361542930065747, "compression_ratio": 1.8413793103448275, "no_speech_prob": 0.009014843963086605}, {"id": 499, "seek": 210056, "start": 2114.4, "end": 2117.96, "text": " But I want to show you one consequence of this.", "tokens": [51056, 583, 286, 528, 281, 855, 291, 472, 18326, 295, 341, 13, 51234], "temperature": 0.0, "avg_logprob": -0.18361542930065747, "compression_ratio": 1.8413793103448275, "no_speech_prob": 0.009014843963086605}, {"id": 500, "seek": 210056, "start": 2117.96, "end": 2121.96, "text": " So that's, look, here we have a nice equation because it's got Leibniz on one side and Newton", "tokens": [51234, 407, 300, 311, 11, 574, 11, 510, 321, 362, 257, 1481, 5367, 570, 309, 311, 658, 1456, 897, 77, 590, 322, 472, 1252, 293, 19541, 51434], "temperature": 0.0, "avg_logprob": -0.18361542930065747, "compression_ratio": 1.8413793103448275, "no_speech_prob": 0.009014843963086605}, {"id": 501, "seek": 210056, "start": 2121.96, "end": 2125.04, "text": " on the other side.", "tokens": [51434, 322, 264, 661, 1252, 13, 51588], "temperature": 0.0, "avg_logprob": -0.18361542930065747, "compression_ratio": 1.8413793103448275, "no_speech_prob": 0.009014843963086605}, {"id": 502, "seek": 210056, "start": 2125.04, "end": 2128.4, "text": " That's Leibniz's notation for derivatives because they're not derivatives with respect", "tokens": [51588, 663, 311, 1456, 897, 77, 590, 311, 24657, 337, 33733, 570, 436, 434, 406, 33733, 365, 3104, 51756], "temperature": 0.0, "avg_logprob": -0.18361542930065747, "compression_ratio": 1.8413793103448275, "no_speech_prob": 0.009014843963086605}, {"id": 503, "seek": 210056, "start": 2128.4, "end": 2129.72, "text": " to time.", "tokens": [51756, 281, 565, 13, 51822], "temperature": 0.0, "avg_logprob": -0.18361542930065747, "compression_ratio": 1.8413793103448275, "no_speech_prob": 0.009014843963086605}, {"id": 504, "seek": 212972, "start": 2129.72, "end": 2134.64, "text": " And this is Newton's notation because that was for derivatives with respect to time, okay?", "tokens": [50364, 400, 341, 307, 19541, 311, 24657, 570, 300, 390, 337, 33733, 365, 3104, 281, 565, 11, 1392, 30, 50610], "temperature": 0.0, "avg_logprob": -0.24620541986429467, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0006344499415718019}, {"id": 505, "seek": 212972, "start": 2134.64, "end": 2141.48, "text": " And what we're saying is the output of neuron J, which is yj, is the output of neuron J.", "tokens": [50610, 400, 437, 321, 434, 1566, 307, 264, 5598, 295, 34090, 508, 11, 597, 307, 288, 73, 11, 307, 264, 5598, 295, 34090, 508, 13, 50952], "temperature": 0.0, "avg_logprob": -0.24620541986429467, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0006344499415718019}, {"id": 506, "seek": 212972, "start": 2141.48, "end": 2146.2799999999997, "text": " But how fast that's changing over a short time interval is the error derivative.", "tokens": [50952, 583, 577, 2370, 300, 311, 4473, 670, 257, 2099, 565, 15035, 307, 264, 6713, 13760, 13, 51192], "temperature": 0.0, "avg_logprob": -0.24620541986429467, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0006344499415718019}, {"id": 507, "seek": 212972, "start": 2146.2799999999997, "end": 2149.6, "text": " This is just a hypothesis, you understand?", "tokens": [51192, 639, 307, 445, 257, 17291, 11, 291, 1223, 30, 51358], "temperature": 0.0, "avg_logprob": -0.24620541986429467, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0006344499415718019}, {"id": 508, "seek": 212972, "start": 2149.6, "end": 2154.4399999999996, "text": " But it's true.", "tokens": [51358, 583, 309, 311, 2074, 13, 51600], "temperature": 0.0, "avg_logprob": -0.24620541986429467, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0006344499415718019}, {"id": 509, "seek": 212972, "start": 2154.4399999999996, "end": 2158.68, "text": " Jay McClellan and I first used a version of this in 1988 before we knew about its back", "tokens": [51600, 11146, 12061, 306, 32919, 293, 286, 700, 1143, 257, 3037, 295, 341, 294, 27816, 949, 321, 2586, 466, 1080, 646, 51812], "temperature": 0.0, "avg_logprob": -0.24620541986429467, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0006344499415718019}, {"id": 510, "seek": 215868, "start": 2158.72, "end": 2160.04, "text": " time dependent plasticity.", "tokens": [50366, 565, 12334, 5900, 507, 13, 50432], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 511, "seek": 215868, "start": 2160.04, "end": 2163.2, "text": " I'm not sure it would be discovered then.", "tokens": [50432, 286, 478, 406, 988, 309, 576, 312, 6941, 550, 13, 50590], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 512, "seek": 215868, "start": 2163.2, "end": 2166.2, "text": " Where you take, this is where I need the cursor.", "tokens": [50590, 2305, 291, 747, 11, 341, 307, 689, 286, 643, 264, 28169, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 513, "seek": 215868, "start": 2166.2, "end": 2170.2, "text": " Yes, that one.", "tokens": [50740, 1079, 11, 300, 472, 13, 50940], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 514, "seek": 215868, "start": 2170.2, "end": 2172.2799999999997, "text": " Yes.", "tokens": [50940, 1079, 13, 51044], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 515, "seek": 215868, "start": 2172.2799999999997, "end": 2174.12, "text": " You take some input.", "tokens": [51044, 509, 747, 512, 4846, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 516, "seek": 215868, "start": 2174.12, "end": 2177.64, "text": " You send it to some hidden units, which send it to more hidden units by the green connections", "tokens": [51136, 509, 2845, 309, 281, 512, 7633, 6815, 11, 597, 2845, 309, 281, 544, 7633, 6815, 538, 264, 3092, 9271, 51312], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 517, "seek": 215868, "start": 2177.64, "end": 2178.64, "text": " and sends it to more hidden units.", "tokens": [51312, 293, 14790, 309, 281, 544, 7633, 6815, 13, 51362], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 518, "seek": 215868, "start": 2178.64, "end": 2181.8399999999997, "text": " It comes back to the input, so you reconstruct the input.", "tokens": [51362, 467, 1487, 646, 281, 264, 4846, 11, 370, 291, 31499, 264, 4846, 13, 51522], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 519, "seek": 215868, "start": 2181.8399999999997, "end": 2185.8399999999997, "text": " And then you send it around again, not all the way around, but up to there and up to", "tokens": [51522, 400, 550, 291, 2845, 309, 926, 797, 11, 406, 439, 264, 636, 926, 11, 457, 493, 281, 456, 293, 493, 281, 51722], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 520, "seek": 215868, "start": 2185.8399999999997, "end": 2188.64, "text": " there and up to there using the right connections.", "tokens": [51722, 456, 293, 493, 281, 456, 1228, 264, 558, 9271, 13, 51862], "temperature": 0.0, "avg_logprob": -0.2636613134127944, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.04772314801812172}, {"id": 521, "seek": 218864, "start": 2188.64, "end": 2193.7599999999998, "text": " And then the learning rule, which you'll notice doesn't involve explicit back propagation,", "tokens": [50364, 400, 550, 264, 2539, 4978, 11, 597, 291, 603, 3449, 1177, 380, 9494, 13691, 646, 38377, 11, 50620], "temperature": 0.0, "avg_logprob": -0.1482784290506382, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.0004768686485476792}, {"id": 522, "seek": 218864, "start": 2193.7599999999998, "end": 2202.2799999999997, "text": " is to say for these neurons, for example, I change the incoming weights by the activity", "tokens": [50620, 307, 281, 584, 337, 613, 22027, 11, 337, 1365, 11, 286, 1319, 264, 22341, 17443, 538, 264, 5191, 51046], "temperature": 0.0, "avg_logprob": -0.1482784290506382, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.0004768686485476792}, {"id": 523, "seek": 218864, "start": 2202.2799999999997, "end": 2209.3599999999997, "text": " of the presynaptic neuron down here times the difference between what I got on the green", "tokens": [51046, 295, 264, 1183, 2534, 2796, 299, 34090, 760, 510, 1413, 264, 2649, 1296, 437, 286, 658, 322, 264, 3092, 51400], "temperature": 0.0, "avg_logprob": -0.1482784290506382, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.0004768686485476792}, {"id": 524, "seek": 218864, "start": 2209.3599999999997, "end": 2212.8799999999997, "text": " activation and on the red activation, first time round and second time round.", "tokens": [51400, 24433, 293, 322, 264, 2182, 24433, 11, 700, 565, 3098, 293, 1150, 565, 3098, 13, 51576], "temperature": 0.0, "avg_logprob": -0.1482784290506382, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.0004768686485476792}, {"id": 525, "seek": 218864, "start": 2212.8799999999997, "end": 2217.7599999999998, "text": " So the rate of change of the activation of the neuron is what's used to communicate an", "tokens": [51576, 407, 264, 3314, 295, 1319, 295, 264, 24433, 295, 264, 34090, 307, 437, 311, 1143, 281, 7890, 364, 51820], "temperature": 0.0, "avg_logprob": -0.1482784290506382, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.0004768686485476792}, {"id": 526, "seek": 221776, "start": 2217.76, "end": 2218.76, "text": " error derivative.", "tokens": [50364, 6713, 13760, 13, 50414], "temperature": 0.0, "avg_logprob": -0.17228867667061942, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.030592266470193863}, {"id": 527, "seek": 221776, "start": 2218.76, "end": 2226.4, "text": " Now, unfortunately, this thing has the wrong sign, but later on we fixed that.", "tokens": [50414, 823, 11, 7015, 11, 341, 551, 575, 264, 2085, 1465, 11, 457, 1780, 322, 321, 6806, 300, 13, 50796], "temperature": 0.0, "avg_logprob": -0.17228867667061942, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.030592266470193863}, {"id": 528, "seek": 221776, "start": 2226.4, "end": 2234.2400000000002, "text": " And so here's a theory from 2007 that still hasn't been conclusively proved wrong.", "tokens": [50796, 400, 370, 510, 311, 257, 5261, 490, 12656, 300, 920, 6132, 380, 668, 18646, 3413, 14617, 2085, 13, 51188], "temperature": 0.0, "avg_logprob": -0.17228867667061942, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.030592266470193863}, {"id": 529, "seek": 221776, "start": 2234.2400000000002, "end": 2238.2000000000003, "text": " And it sort of works, but it doesn't work quite as well as we hoped, about how you could", "tokens": [51188, 400, 309, 1333, 295, 1985, 11, 457, 309, 1177, 380, 589, 1596, 382, 731, 382, 321, 19737, 11, 466, 577, 291, 727, 51386], "temperature": 0.0, "avg_logprob": -0.17228867667061942, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.030592266470193863}, {"id": 530, "seek": 221776, "start": 2238.2000000000003, "end": 2241.28, "text": " get a brain to do back propagation.", "tokens": [51386, 483, 257, 3567, 281, 360, 646, 38377, 13, 51540], "temperature": 0.0, "avg_logprob": -0.17228867667061942, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.030592266470193863}, {"id": 531, "seek": 221776, "start": 2241.28, "end": 2245.1200000000003, "text": " What you first do is you learn a stack of autoencoders, that is you learn to get each", "tokens": [51540, 708, 291, 700, 360, 307, 291, 1466, 257, 8630, 295, 8399, 22660, 378, 433, 11, 300, 307, 291, 1466, 281, 483, 1184, 51732], "temperature": 0.0, "avg_logprob": -0.17228867667061942, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.030592266470193863}, {"id": 532, "seek": 224512, "start": 2245.12, "end": 2251.2799999999997, "text": " layer to activate features in the layer above from which you can reconstruct the layer below.", "tokens": [50364, 4583, 281, 13615, 4122, 294, 264, 4583, 3673, 490, 597, 291, 393, 31499, 264, 4583, 2507, 13, 50672], "temperature": 0.0, "avg_logprob": -0.17786993060195655, "compression_ratio": 2.1232876712328768, "no_speech_prob": 0.12593121826648712}, {"id": 533, "seek": 224512, "start": 2251.2799999999997, "end": 2254.08, "text": " So you learn some features that can reconstruct this layer.", "tokens": [50672, 407, 291, 1466, 512, 4122, 300, 393, 31499, 341, 4583, 13, 50812], "temperature": 0.0, "avg_logprob": -0.17786993060195655, "compression_ratio": 2.1232876712328768, "no_speech_prob": 0.12593121826648712}, {"id": 534, "seek": 224512, "start": 2254.08, "end": 2257.88, "text": " Then you treat those features as data and learn some features that can reconstruct them.", "tokens": [50812, 1396, 291, 2387, 729, 4122, 382, 1412, 293, 1466, 512, 4122, 300, 393, 31499, 552, 13, 51002], "temperature": 0.0, "avg_logprob": -0.17786993060195655, "compression_ratio": 2.1232876712328768, "no_speech_prob": 0.12593121826648712}, {"id": 535, "seek": 224512, "start": 2257.88, "end": 2260.08, "text": " You build a big stack of autoencoders like that.", "tokens": [51002, 509, 1322, 257, 955, 8630, 295, 8399, 22660, 378, 433, 411, 300, 13, 51112], "temperature": 0.0, "avg_logprob": -0.17786993060195655, "compression_ratio": 2.1232876712328768, "no_speech_prob": 0.12593121826648712}, {"id": 536, "seek": 224512, "start": 2260.08, "end": 2261.08, "text": " Okay.", "tokens": [51112, 1033, 13, 51162], "temperature": 0.0, "avg_logprob": -0.17786993060195655, "compression_ratio": 2.1232876712328768, "no_speech_prob": 0.12593121826648712}, {"id": 537, "seek": 224512, "start": 2261.08, "end": 2265.72, "text": " Once you build the stack of autoencoders, then each layer can activity in a layer can", "tokens": [51162, 3443, 291, 1322, 264, 8630, 295, 8399, 22660, 378, 433, 11, 550, 1184, 4583, 393, 5191, 294, 257, 4583, 393, 51394], "temperature": 0.0, "avg_logprob": -0.17786993060195655, "compression_ratio": 2.1232876712328768, "no_speech_prob": 0.12593121826648712}, {"id": 538, "seek": 224512, "start": 2265.72, "end": 2269.42, "text": " reconstruct the activity in the layer below.", "tokens": [51394, 31499, 264, 5191, 294, 264, 4583, 2507, 13, 51579], "temperature": 0.0, "avg_logprob": -0.17786993060195655, "compression_ratio": 2.1232876712328768, "no_speech_prob": 0.12593121826648712}, {"id": 539, "seek": 224512, "start": 2269.42, "end": 2272.64, "text": " And then you do two top down passes.", "tokens": [51579, 400, 550, 291, 360, 732, 1192, 760, 11335, 13, 51740], "temperature": 0.0, "avg_logprob": -0.17786993060195655, "compression_ratio": 2.1232876712328768, "no_speech_prob": 0.12593121826648712}, {"id": 540, "seek": 227264, "start": 2272.64, "end": 2277.3599999999997, "text": " You do a top down pass from the thing you predicted at the output.", "tokens": [50364, 509, 360, 257, 1192, 760, 1320, 490, 264, 551, 291, 19147, 412, 264, 5598, 13, 50600], "temperature": 0.0, "avg_logprob": -0.11514366149902344, "compression_ratio": 1.9712918660287082, "no_speech_prob": 0.004676405806094408}, {"id": 541, "seek": 227264, "start": 2277.3599999999997, "end": 2280.96, "text": " So you put in input, activity goes forward through the layers, you predict something", "tokens": [50600, 407, 291, 829, 294, 4846, 11, 5191, 1709, 2128, 807, 264, 7914, 11, 291, 6069, 746, 50780], "temperature": 0.0, "avg_logprob": -0.11514366149902344, "compression_ratio": 1.9712918660287082, "no_speech_prob": 0.004676405806094408}, {"id": 542, "seek": 227264, "start": 2280.96, "end": 2281.96, "text": " at the output.", "tokens": [50780, 412, 264, 5598, 13, 50830], "temperature": 0.0, "avg_logprob": -0.11514366149902344, "compression_ratio": 1.9712918660287082, "no_speech_prob": 0.004676405806094408}, {"id": 543, "seek": 227264, "start": 2281.96, "end": 2287.04, "text": " And now you do a top down pass and you get reconstructed activities everywhere.", "tokens": [50830, 400, 586, 291, 360, 257, 1192, 760, 1320, 293, 291, 483, 31499, 292, 5354, 5315, 13, 51084], "temperature": 0.0, "avg_logprob": -0.11514366149902344, "compression_ratio": 1.9712918660287082, "no_speech_prob": 0.004676405806094408}, {"id": 544, "seek": 227264, "start": 2287.04, "end": 2295.2799999999997, "text": " And then you take your output and you change it to be more like the desired output.", "tokens": [51084, 400, 550, 291, 747, 428, 5598, 293, 291, 1319, 309, 281, 312, 544, 411, 264, 14721, 5598, 13, 51496], "temperature": 0.0, "avg_logprob": -0.11514366149902344, "compression_ratio": 1.9712918660287082, "no_speech_prob": 0.004676405806094408}, {"id": 545, "seek": 227264, "start": 2295.2799999999997, "end": 2301.2, "text": " And now you do a top down pass and you'll get slightly different reconstructions.", "tokens": [51496, 400, 586, 291, 360, 257, 1192, 760, 1320, 293, 291, 603, 483, 4748, 819, 31499, 626, 13, 51792], "temperature": 0.0, "avg_logprob": -0.11514366149902344, "compression_ratio": 1.9712918660287082, "no_speech_prob": 0.004676405806094408}, {"id": 546, "seek": 230120, "start": 2301.2, "end": 2308.12, "text": " And the difference between those two reconstructions is actually the signal you need for back propagation.", "tokens": [50364, 400, 264, 2649, 1296, 729, 732, 31499, 626, 307, 767, 264, 6358, 291, 643, 337, 646, 38377, 13, 50710], "temperature": 0.0, "avg_logprob": -0.15747029885001804, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.03656116500496864}, {"id": 547, "seek": 230120, "start": 2308.12, "end": 2313.08, "text": " And so if you do that, the learning rule is that you should change a synapse by the", "tokens": [50710, 400, 370, 498, 291, 360, 300, 11, 264, 2539, 4978, 307, 300, 291, 820, 1319, 257, 5451, 11145, 538, 264, 50958], "temperature": 0.0, "avg_logprob": -0.15747029885001804, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.03656116500496864}, {"id": 548, "seek": 230120, "start": 2313.08, "end": 2319.2, "text": " pre-synaptic activity in the layer below times the rate of change of the activity in the", "tokens": [50958, 659, 12, 82, 2534, 2796, 299, 5191, 294, 264, 4583, 2507, 1413, 264, 3314, 295, 1319, 295, 264, 5191, 294, 264, 51264], "temperature": 0.0, "avg_logprob": -0.15747029885001804, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.03656116500496864}, {"id": 549, "seek": 230120, "start": 2319.2, "end": 2321.2799999999997, "text": " layer above in the post-synaptic neuron.", "tokens": [51264, 4583, 3673, 294, 264, 2183, 12, 82, 2534, 2796, 299, 34090, 13, 51368], "temperature": 0.0, "avg_logprob": -0.15747029885001804, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.03656116500496864}, {"id": 550, "seek": 230120, "start": 2321.2799999999997, "end": 2324.9199999999996, "text": " So it's a very simple learning rule.", "tokens": [51368, 407, 309, 311, 257, 588, 2199, 2539, 4978, 13, 51550], "temperature": 0.0, "avg_logprob": -0.15747029885001804, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.03656116500496864}, {"id": 551, "seek": 232492, "start": 2325.12, "end": 2330.92, "text": " So let's change the weight in proportion to the pre-synaptic activity times the rate", "tokens": [50374, 407, 718, 311, 1319, 264, 3364, 294, 16068, 281, 264, 659, 12, 82, 2534, 2796, 299, 5191, 1413, 264, 3314, 50664], "temperature": 0.0, "avg_logprob": -0.18139591923466436, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.021533111110329628}, {"id": 552, "seek": 232492, "start": 2330.92, "end": 2335.04, "text": " of change of the post-synaptic activity.", "tokens": [50664, 295, 1319, 295, 264, 2183, 12, 82, 2534, 2796, 299, 5191, 13, 50870], "temperature": 0.0, "avg_logprob": -0.18139591923466436, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.021533111110329628}, {"id": 553, "seek": 232492, "start": 2335.04, "end": 2340.64, "text": " Now it turns out if you're using spiking neurons, what that amounts to that are representing", "tokens": [50870, 823, 309, 4523, 484, 498, 291, 434, 1228, 637, 13085, 22027, 11, 437, 300, 11663, 281, 300, 366, 13460, 51150], "temperature": 0.0, "avg_logprob": -0.18139591923466436, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.021533111110329628}, {"id": 554, "seek": 232492, "start": 2340.64, "end": 2347.48, "text": " underlying firing rates that are changing, that amounts to a learning rule that looks", "tokens": [51150, 14217, 16045, 6846, 300, 366, 4473, 11, 300, 11663, 281, 257, 2539, 4978, 300, 1542, 51492], "temperature": 0.0, "avg_logprob": -0.18139591923466436, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.021533111110329628}, {"id": 555, "seek": 232492, "start": 2347.48, "end": 2353.08, "text": " like this.", "tokens": [51492, 411, 341, 13, 51772], "temperature": 0.0, "avg_logprob": -0.18139591923466436, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.021533111110329628}, {"id": 556, "seek": 235308, "start": 2353.08, "end": 2358.88, "text": " What you do is you take a pre-synaptic spike and you ask whether the post-synaptic spike", "tokens": [50364, 708, 291, 360, 307, 291, 747, 257, 659, 12, 82, 2534, 2796, 299, 21053, 293, 291, 1029, 1968, 264, 2183, 12, 82, 2534, 2796, 299, 21053, 50654], "temperature": 0.0, "avg_logprob": -0.11564774513244629, "compression_ratio": 1.8715083798882681, "no_speech_prob": 0.029039399698376656}, {"id": 557, "seek": 235308, "start": 2358.88, "end": 2361.92, "text": " came before or after it.", "tokens": [50654, 1361, 949, 420, 934, 309, 13, 50806], "temperature": 0.0, "avg_logprob": -0.11564774513244629, "compression_ratio": 1.8715083798882681, "no_speech_prob": 0.029039399698376656}, {"id": 558, "seek": 235308, "start": 2361.92, "end": 2367.48, "text": " Because what you're interested in is the rate of change of the post-synaptic firing rate", "tokens": [50806, 1436, 437, 291, 434, 3102, 294, 307, 264, 3314, 295, 1319, 295, 264, 2183, 12, 82, 2534, 2796, 299, 16045, 3314, 51084], "temperature": 0.0, "avg_logprob": -0.11564774513244629, "compression_ratio": 1.8715083798882681, "no_speech_prob": 0.029039399698376656}, {"id": 559, "seek": 235308, "start": 2367.48, "end": 2373.0, "text": " around the time of the pre-synaptic spike.", "tokens": [51084, 926, 264, 565, 295, 264, 659, 12, 82, 2534, 2796, 299, 21053, 13, 51360], "temperature": 0.0, "avg_logprob": -0.11564774513244629, "compression_ratio": 1.8715083798882681, "no_speech_prob": 0.029039399698376656}, {"id": 560, "seek": 235308, "start": 2373.0, "end": 2381.08, "text": " And if the post-synaptic spike occurs often just after it and seldom just before it, that", "tokens": [51360, 400, 498, 264, 2183, 12, 82, 2534, 2796, 299, 21053, 11843, 2049, 445, 934, 309, 293, 47717, 445, 949, 309, 11, 300, 51764], "temperature": 0.0, "avg_logprob": -0.11564774513244629, "compression_ratio": 1.8715083798882681, "no_speech_prob": 0.029039399698376656}, {"id": 561, "seek": 238108, "start": 2381.08, "end": 2383.52, "text": " suggests the firing rate is going up.", "tokens": [50364, 13409, 264, 16045, 3314, 307, 516, 493, 13, 50486], "temperature": 0.0, "avg_logprob": -0.10680316686630249, "compression_ratio": 2.1171875, "no_speech_prob": 0.07167277485132217}, {"id": 562, "seek": 238108, "start": 2383.52, "end": 2387.84, "text": " And if the post-synaptic spike occurs often just before the pre-synaptic one and less", "tokens": [50486, 400, 498, 264, 2183, 12, 82, 2534, 2796, 299, 21053, 11843, 2049, 445, 949, 264, 659, 12, 82, 2534, 2796, 299, 472, 293, 1570, 50702], "temperature": 0.0, "avg_logprob": -0.10680316686630249, "compression_ratio": 2.1171875, "no_speech_prob": 0.07167277485132217}, {"id": 563, "seek": 238108, "start": 2387.84, "end": 2391.16, "text": " often just after it, that means the firing rate of the post-synaptic neuron is going", "tokens": [50702, 2049, 445, 934, 309, 11, 300, 1355, 264, 16045, 3314, 295, 264, 2183, 12, 82, 2534, 2796, 299, 34090, 307, 516, 50868], "temperature": 0.0, "avg_logprob": -0.10680316686630249, "compression_ratio": 2.1171875, "no_speech_prob": 0.07167277485132217}, {"id": 564, "seek": 238108, "start": 2391.16, "end": 2393.2799999999997, "text": " down.", "tokens": [50868, 760, 13, 50974], "temperature": 0.0, "avg_logprob": -0.10680316686630249, "compression_ratio": 2.1171875, "no_speech_prob": 0.07167277485132217}, {"id": 565, "seek": 238108, "start": 2393.2799999999997, "end": 2398.24, "text": " So if you want your learning rule to be the pre-synaptic activity, well, you'll only learn", "tokens": [50974, 407, 498, 291, 528, 428, 2539, 4978, 281, 312, 264, 659, 12, 82, 2534, 2796, 299, 5191, 11, 731, 11, 291, 603, 787, 1466, 51222], "temperature": 0.0, "avg_logprob": -0.10680316686630249, "compression_ratio": 2.1171875, "no_speech_prob": 0.07167277485132217}, {"id": 566, "seek": 238108, "start": 2398.24, "end": 2400.36, "text": " when you get a pre-synaptic spike.", "tokens": [51222, 562, 291, 483, 257, 659, 12, 82, 2534, 2796, 299, 21053, 13, 51328], "temperature": 0.0, "avg_logprob": -0.10680316686630249, "compression_ratio": 2.1171875, "no_speech_prob": 0.07167277485132217}, {"id": 567, "seek": 238108, "start": 2400.36, "end": 2404.4, "text": " And then what you'll do is you'll say, did the post-synaptic spike occur afterwards", "tokens": [51328, 400, 550, 437, 291, 603, 360, 307, 291, 603, 584, 11, 630, 264, 2183, 12, 82, 2534, 2796, 299, 21053, 5160, 10543, 51530], "temperature": 0.0, "avg_logprob": -0.10680316686630249, "compression_ratio": 2.1171875, "no_speech_prob": 0.07167277485132217}, {"id": 568, "seek": 238108, "start": 2404.4, "end": 2405.4, "text": " or before?", "tokens": [51530, 420, 949, 30, 51580], "temperature": 0.0, "avg_logprob": -0.10680316686630249, "compression_ratio": 2.1171875, "no_speech_prob": 0.07167277485132217}, {"id": 569, "seek": 238108, "start": 2405.4, "end": 2409.12, "text": " If it occurred afterwards, I should raise the weight and if it occurred before, I should", "tokens": [51580, 759, 309, 11068, 10543, 11, 286, 820, 5300, 264, 3364, 293, 498, 309, 11068, 949, 11, 286, 820, 51766], "temperature": 0.0, "avg_logprob": -0.10680316686630249, "compression_ratio": 2.1171875, "no_speech_prob": 0.07167277485132217}, {"id": 570, "seek": 238108, "start": 2409.12, "end": 2410.6, "text": " lower the weight.", "tokens": [51766, 3126, 264, 3364, 13, 51840], "temperature": 0.0, "avg_logprob": -0.10680316686630249, "compression_ratio": 2.1171875, "no_speech_prob": 0.07167277485132217}, {"id": 571, "seek": 241060, "start": 2410.64, "end": 2418.72, "text": " And so your learning rule will look like this and this thing is actually a derivative filter.", "tokens": [50366, 400, 370, 428, 2539, 4978, 486, 574, 411, 341, 293, 341, 551, 307, 767, 257, 13760, 6608, 13, 50770], "temperature": 0.0, "avg_logprob": -0.19522801670459433, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0009949313243851066}, {"id": 572, "seek": 241060, "start": 2418.72, "end": 2423.72, "text": " It's centered at zero and what this is really doing is measuring the rate of change of the", "tokens": [50770, 467, 311, 18988, 412, 4018, 293, 437, 341, 307, 534, 884, 307, 13389, 264, 3314, 295, 1319, 295, 264, 51020], "temperature": 0.0, "avg_logprob": -0.19522801670459433, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0009949313243851066}, {"id": 573, "seek": 241060, "start": 2423.72, "end": 2425.88, "text": " post-synaptic firing rate.", "tokens": [51020, 2183, 12, 82, 2534, 2796, 299, 16045, 3314, 13, 51128], "temperature": 0.0, "avg_logprob": -0.19522801670459433, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0009949313243851066}, {"id": 574, "seek": 241060, "start": 2425.88, "end": 2428.04, "text": " And of course it's sampling it.", "tokens": [51128, 400, 295, 1164, 309, 311, 21179, 309, 13, 51236], "temperature": 0.0, "avg_logprob": -0.19522801670459433, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0009949313243851066}, {"id": 575, "seek": 241060, "start": 2428.04, "end": 2431.68, "text": " So you have a post-synaptic firing rate, there's these spike trains and are the other spikes", "tokens": [51236, 407, 291, 362, 257, 2183, 12, 82, 2534, 2796, 299, 16045, 3314, 11, 456, 311, 613, 21053, 16329, 293, 366, 264, 661, 28997, 51418], "temperature": 0.0, "avg_logprob": -0.19522801670459433, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0009949313243851066}, {"id": 576, "seek": 241060, "start": 2431.68, "end": 2434.04, "text": " getting closer together or further apart?", "tokens": [51418, 1242, 4966, 1214, 420, 3052, 4936, 30, 51536], "temperature": 0.0, "avg_logprob": -0.19522801670459433, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0009949313243851066}, {"id": 577, "seek": 241060, "start": 2434.04, "end": 2436.72, "text": " Well this is a way to measure that.", "tokens": [51536, 1042, 341, 307, 257, 636, 281, 3481, 300, 13, 51670], "temperature": 0.0, "avg_logprob": -0.19522801670459433, "compression_ratio": 1.7394957983193278, "no_speech_prob": 0.0009949313243851066}, {"id": 578, "seek": 243672, "start": 2436.72, "end": 2442.4399999999996, "text": " And of course you can do the learning on individual spikes and the learning rule would then be", "tokens": [50364, 400, 295, 1164, 291, 393, 360, 264, 2539, 322, 2609, 28997, 293, 264, 2539, 4978, 576, 550, 312, 50650], "temperature": 0.0, "avg_logprob": -0.12840404367088376, "compression_ratio": 1.9812734082397003, "no_speech_prob": 0.0009337495430372655}, {"id": 579, "seek": 243672, "start": 2442.4399999999996, "end": 2446.4399999999996, "text": " the implementation of this idea that the rate of change of the post-synaptic firing rate", "tokens": [50650, 264, 11420, 295, 341, 1558, 300, 264, 3314, 295, 1319, 295, 264, 2183, 12, 82, 2534, 2796, 299, 16045, 3314, 50850], "temperature": 0.0, "avg_logprob": -0.12840404367088376, "compression_ratio": 1.9812734082397003, "no_speech_prob": 0.0009337495430372655}, {"id": 580, "seek": 243672, "start": 2446.4399999999996, "end": 2448.9199999999996, "text": " is the error signal.", "tokens": [50850, 307, 264, 6713, 6358, 13, 50974], "temperature": 0.0, "avg_logprob": -0.12840404367088376, "compression_ratio": 1.9812734082397003, "no_speech_prob": 0.0009337495430372655}, {"id": 581, "seek": 243672, "start": 2448.9199999999996, "end": 2452.7999999999997, "text": " The learning rule would just be if the post-synaptic spike goes after the pre-synaptic one, increase", "tokens": [50974, 440, 2539, 4978, 576, 445, 312, 498, 264, 2183, 12, 82, 2534, 2796, 299, 21053, 1709, 934, 264, 659, 12, 82, 2534, 2796, 299, 472, 11, 3488, 51168], "temperature": 0.0, "avg_logprob": -0.12840404367088376, "compression_ratio": 1.9812734082397003, "no_speech_prob": 0.0009337495430372655}, {"id": 582, "seek": 243672, "start": 2452.7999999999997, "end": 2457.72, "text": " the strength, otherwise decrease it and have that whole effect fall off as the spikes get", "tokens": [51168, 264, 3800, 11, 5911, 11514, 309, 293, 362, 300, 1379, 1802, 2100, 766, 382, 264, 28997, 483, 51414], "temperature": 0.0, "avg_logprob": -0.12840404367088376, "compression_ratio": 1.9812734082397003, "no_speech_prob": 0.0009337495430372655}, {"id": 583, "seek": 243672, "start": 2457.72, "end": 2461.9199999999996, "text": " further away because we're really only interested in the rate of change of the firing rate around", "tokens": [51414, 3052, 1314, 570, 321, 434, 534, 787, 3102, 294, 264, 3314, 295, 1319, 295, 264, 16045, 3314, 926, 51624], "temperature": 0.0, "avg_logprob": -0.12840404367088376, "compression_ratio": 1.9812734082397003, "no_speech_prob": 0.0009337495430372655}, {"id": 584, "seek": 243672, "start": 2461.9199999999996, "end": 2465.7599999999998, "text": " the time of the pre-synaptic spike.", "tokens": [51624, 264, 565, 295, 264, 659, 12, 82, 2534, 2796, 299, 21053, 13, 51816], "temperature": 0.0, "avg_logprob": -0.12840404367088376, "compression_ratio": 1.9812734082397003, "no_speech_prob": 0.0009337495430372655}, {"id": 585, "seek": 246576, "start": 2465.8, "end": 2471.1200000000003, "text": " Now there's one consequence of that which is that if you're going to use the rate of", "tokens": [50366, 823, 456, 311, 472, 18326, 295, 300, 597, 307, 300, 498, 291, 434, 516, 281, 764, 264, 3314, 295, 50632], "temperature": 0.0, "avg_logprob": -0.174650510152181, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.0020471473690122366}, {"id": 586, "seek": 246576, "start": 2471.1200000000003, "end": 2477.5600000000004, "text": " change of a neuron to represent not what the neuron is representing but to represent an", "tokens": [50632, 1319, 295, 257, 34090, 281, 2906, 406, 437, 264, 34090, 307, 13460, 457, 281, 2906, 364, 50954], "temperature": 0.0, "avg_logprob": -0.174650510152181, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.0020471473690122366}, {"id": 587, "seek": 246576, "start": 2477.5600000000004, "end": 2482.5200000000004, "text": " error derivative, you've basically used up temporal derivatives for communicating error", "tokens": [50954, 6713, 13760, 11, 291, 600, 1936, 1143, 493, 30881, 33733, 337, 17559, 6713, 51202], "temperature": 0.0, "avg_logprob": -0.174650510152181, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.0020471473690122366}, {"id": 588, "seek": 246576, "start": 2482.5200000000004, "end": 2483.8, "text": " derivatives.", "tokens": [51202, 33733, 13, 51266], "temperature": 0.0, "avg_logprob": -0.174650510152181, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.0020471473690122366}, {"id": 589, "seek": 246576, "start": 2483.8, "end": 2488.6400000000003, "text": " So you cannot use temporal derivatives to communicate the temporal derivatives of what the neuron", "tokens": [51266, 407, 291, 2644, 764, 30881, 33733, 281, 7890, 264, 30881, 33733, 295, 437, 264, 34090, 51508], "temperature": 0.0, "avg_logprob": -0.174650510152181, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.0020471473690122366}, {"id": 590, "seek": 246576, "start": 2488.6400000000003, "end": 2489.6400000000003, "text": " represents.", "tokens": [51508, 8855, 13, 51558], "temperature": 0.0, "avg_logprob": -0.174650510152181, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.0020471473690122366}, {"id": 591, "seek": 246576, "start": 2489.6400000000003, "end": 2494.0800000000004, "text": " So however I have a neuron that represents position, I can't use how fast that's changing", "tokens": [51558, 407, 4461, 286, 362, 257, 34090, 300, 8855, 2535, 11, 286, 393, 380, 764, 577, 2370, 300, 311, 4473, 51780], "temperature": 0.0, "avg_logprob": -0.174650510152181, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.0020471473690122366}, {"id": 592, "seek": 249408, "start": 2494.08, "end": 2497.48, "text": " to represent velocity and that's true of neurons.", "tokens": [50364, 281, 2906, 9269, 293, 300, 311, 2074, 295, 22027, 13, 50534], "temperature": 0.0, "avg_logprob": -0.14480431574695515, "compression_ratio": 2.074418604651163, "no_speech_prob": 0.012808850966393948}, {"id": 593, "seek": 249408, "start": 2497.48, "end": 2502.72, "text": " If you want to represent velocity, you have to have a neuron whose output represents velocity.", "tokens": [50534, 759, 291, 528, 281, 2906, 9269, 11, 291, 362, 281, 362, 257, 34090, 6104, 5598, 8855, 9269, 13, 50796], "temperature": 0.0, "avg_logprob": -0.14480431574695515, "compression_ratio": 2.074418604651163, "no_speech_prob": 0.012808850966393948}, {"id": 594, "seek": 249408, "start": 2502.72, "end": 2506.0, "text": " You can't do it with the rate of change of a position neuron.", "tokens": [50796, 509, 393, 380, 360, 309, 365, 264, 3314, 295, 1319, 295, 257, 2535, 34090, 13, 50960], "temperature": 0.0, "avg_logprob": -0.14480431574695515, "compression_ratio": 2.074418604651163, "no_speech_prob": 0.012808850966393948}, {"id": 595, "seek": 249408, "start": 2506.0, "end": 2512.68, "text": " If I kill the velocity neurons and keep the position neurons and I watch a car moving,", "tokens": [50960, 759, 286, 1961, 264, 9269, 22027, 293, 1066, 264, 2535, 22027, 293, 286, 1159, 257, 1032, 2684, 11, 51294], "temperature": 0.0, "avg_logprob": -0.14480431574695515, "compression_ratio": 2.074418604651163, "no_speech_prob": 0.012808850966393948}, {"id": 596, "seek": 249408, "start": 2512.68, "end": 2516.4, "text": " the position neurons will change but I won't see any motion.", "tokens": [51294, 264, 2535, 22027, 486, 1319, 457, 286, 1582, 380, 536, 604, 5394, 13, 51480], "temperature": 0.0, "avg_logprob": -0.14480431574695515, "compression_ratio": 2.074418604651163, "no_speech_prob": 0.012808850966393948}, {"id": 597, "seek": 249408, "start": 2516.4, "end": 2521.48, "text": " Similarly, you can't use the rate of change of a velocity neuron to represent acceleration.", "tokens": [51480, 13157, 11, 291, 393, 380, 764, 264, 3314, 295, 1319, 295, 257, 9269, 34090, 281, 2906, 17162, 13, 51734], "temperature": 0.0, "avg_logprob": -0.14480431574695515, "compression_ratio": 2.074418604651163, "no_speech_prob": 0.012808850966393948}, {"id": 598, "seek": 252148, "start": 2522.28, "end": 2528.12, "text": " Okay, so I think the fact that you can't use the rate of change of a representation to", "tokens": [50404, 1033, 11, 370, 286, 519, 264, 1186, 300, 291, 393, 380, 764, 264, 3314, 295, 1319, 295, 257, 10290, 281, 50696], "temperature": 0.0, "avg_logprob": -0.2532987249902932, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.0007830390823073685}, {"id": 599, "seek": 252148, "start": 2528.12, "end": 2533.68, "text": " represent that that stuff in the world is changing is more evident since the board of", "tokens": [50696, 2906, 300, 300, 1507, 294, 264, 1002, 307, 4473, 307, 544, 16371, 1670, 264, 3150, 295, 50974], "temperature": 0.0, "avg_logprob": -0.2532987249902932, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.0007830390823073685}, {"id": 600, "seek": 252148, "start": 2533.68, "end": 2540.12, "text": " the idea temporal derivatives of neurons are used up in representing error derivatives.", "tokens": [50974, 264, 1558, 30881, 33733, 295, 22027, 366, 1143, 493, 294, 13460, 6713, 33733, 13, 51296], "temperature": 0.0, "avg_logprob": -0.2532987249902932, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.0007830390823073685}, {"id": 601, "seek": 252148, "start": 2540.12, "end": 2544.04, "text": " So now I'll summarize.", "tokens": [51296, 407, 586, 286, 603, 20858, 13, 51492], "temperature": 0.0, "avg_logprob": -0.2532987249902932, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.0007830390823073685}, {"id": 602, "seek": 252148, "start": 2544.04, "end": 2548.88, "text": " The main arguments against back propagation, the fact that they spent neurons and spikes", "tokens": [51492, 440, 2135, 12869, 1970, 646, 38377, 11, 264, 1186, 300, 436, 4418, 22027, 293, 28997, 51734], "temperature": 0.0, "avg_logprob": -0.2532987249902932, "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.0007830390823073685}, {"id": 603, "seek": 254888, "start": 2548.88, "end": 2554.48, "text": " rather than real numbers, well that's just because a lot of noise regularizes things.", "tokens": [50364, 2831, 813, 957, 3547, 11, 731, 300, 311, 445, 570, 257, 688, 295, 5658, 3890, 5660, 721, 13, 50644], "temperature": 0.0, "avg_logprob": -0.20578941257520653, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.005787636153399944}, {"id": 604, "seek": 254888, "start": 2554.48, "end": 2559.12, "text": " You can represent error derivatives as temporal derivatives so the same neuron can send temporal", "tokens": [50644, 509, 393, 2906, 6713, 33733, 382, 30881, 33733, 370, 264, 912, 34090, 393, 2845, 30881, 50876], "temperature": 0.0, "avg_logprob": -0.20578941257520653, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.005787636153399944}, {"id": 605, "seek": 254888, "start": 2559.12, "end": 2566.44, "text": " derivatives backwards, communicate those backwards and communicate activities forwards and the", "tokens": [50876, 33733, 12204, 11, 7890, 729, 12204, 293, 7890, 5354, 30126, 293, 264, 51242], "temperature": 0.0, "avg_logprob": -0.20578941257520653, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.005787636153399944}, {"id": 606, "seek": 254888, "start": 2566.44, "end": 2571.04, "text": " fact that in the brain you do get back to independent plasticity seems to be evidence", "tokens": [51242, 1186, 300, 294, 264, 3567, 291, 360, 483, 646, 281, 6695, 5900, 507, 2544, 281, 312, 4467, 51472], "temperature": 0.0, "avg_logprob": -0.20578941257520653, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.005787636153399944}, {"id": 607, "seek": 254888, "start": 2571.04, "end": 2575.56, "text": " in favor of that representation of error derivatives and now I'm done.", "tokens": [51472, 294, 2294, 295, 300, 10290, 295, 6713, 33733, 293, 586, 286, 478, 1096, 13, 51698], "temperature": 0.0, "avg_logprob": -0.20578941257520653, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.005787636153399944}, {"id": 608, "seek": 257888, "start": 2578.88, "end": 2593.88, "text": " Thank you Jeff for a great talk, sorry that was a Twitter joke, got it, anyway.", "tokens": [50364, 1044, 291, 7506, 337, 257, 869, 751, 11, 2597, 300, 390, 257, 5794, 7647, 11, 658, 309, 11, 4033, 13, 51114], "temperature": 0.0, "avg_logprob": -0.25740609652754187, "compression_ratio": 1.393063583815029, "no_speech_prob": 0.002082465449348092}, {"id": 609, "seek": 257888, "start": 2593.88, "end": 2601.92, "text": " So now what we're going to do is a brief Q&A between myself and Jeff and then after", "tokens": [51114, 407, 586, 437, 321, 434, 516, 281, 360, 307, 257, 5353, 1249, 5, 32, 1296, 2059, 293, 7506, 293, 550, 934, 51516], "temperature": 0.0, "avg_logprob": -0.25740609652754187, "compression_ratio": 1.393063583815029, "no_speech_prob": 0.002082465449348092}, {"id": 610, "seek": 257888, "start": 2601.92, "end": 2607.0, "text": " I've had my chance to ask some questions I'm going to open it up to you guys.", "tokens": [51516, 286, 600, 632, 452, 2931, 281, 1029, 512, 1651, 286, 478, 516, 281, 1269, 309, 493, 281, 291, 1074, 13, 51770], "temperature": 0.0, "avg_logprob": -0.25740609652754187, "compression_ratio": 1.393063583815029, "no_speech_prob": 0.002082465449348092}, {"id": 611, "seek": 260700, "start": 2607.0, "end": 2614.56, "text": " Now I had originally sent Jeff a few questions which I'll rely on partially but his talk", "tokens": [50364, 823, 286, 632, 7993, 2279, 7506, 257, 1326, 1651, 597, 286, 603, 10687, 322, 18886, 457, 702, 751, 50742], "temperature": 0.0, "avg_logprob": -0.22849288940429688, "compression_ratio": 1.5633187772925765, "no_speech_prob": 0.054941821843385696}, {"id": 612, "seek": 260700, "start": 2614.56, "end": 2619.04, "text": " has made me want to ask a few others so I'm sorry I'm going to throw a few loops at you", "tokens": [50742, 575, 1027, 385, 528, 281, 1029, 257, 1326, 2357, 370, 286, 478, 2597, 286, 478, 516, 281, 3507, 257, 1326, 16121, 412, 291, 50966], "temperature": 0.0, "avg_logprob": -0.22849288940429688, "compression_ratio": 1.5633187772925765, "no_speech_prob": 0.054941821843385696}, {"id": 613, "seek": 260700, "start": 2619.04, "end": 2623.4, "text": " as well but let's start with some of the ones that I told you I wouldn't give you.", "tokens": [50966, 382, 731, 457, 718, 311, 722, 365, 512, 295, 264, 2306, 300, 286, 1907, 291, 286, 2759, 380, 976, 291, 13, 51184], "temperature": 0.0, "avg_logprob": -0.22849288940429688, "compression_ratio": 1.5633187772925765, "no_speech_prob": 0.054941821843385696}, {"id": 614, "seek": 260700, "start": 2623.4, "end": 2628.56, "text": " There is something funny with my mic, I don't know if the AV guy is there, I just won't", "tokens": [51184, 821, 307, 746, 4074, 365, 452, 3123, 11, 286, 500, 380, 458, 498, 264, 30198, 2146, 307, 456, 11, 286, 445, 1582, 380, 51442], "temperature": 0.0, "avg_logprob": -0.22849288940429688, "compression_ratio": 1.5633187772925765, "no_speech_prob": 0.054941821843385696}, {"id": 615, "seek": 260700, "start": 2628.56, "end": 2629.56, "text": " look down.", "tokens": [51442, 574, 760, 13, 51492], "temperature": 0.0, "avg_logprob": -0.22849288940429688, "compression_ratio": 1.5633187772925765, "no_speech_prob": 0.054941821843385696}, {"id": 616, "seek": 262956, "start": 2629.56, "end": 2641.7599999999998, "text": " Okay so the first question, yeah it's okay, I'm going off script anyway.", "tokens": [50364, 1033, 370, 264, 700, 1168, 11, 1338, 309, 311, 1392, 11, 286, 478, 516, 766, 5755, 4033, 13, 50974], "temperature": 0.0, "avg_logprob": -0.20926225380819352, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.1965716928243637}, {"id": 617, "seek": 262956, "start": 2641.7599999999998, "end": 2647.08, "text": " The first question which I would like to ask just because it's something that I spend far", "tokens": [50974, 440, 700, 1168, 597, 286, 576, 411, 281, 1029, 445, 570, 309, 311, 746, 300, 286, 3496, 1400, 51240], "temperature": 0.0, "avg_logprob": -0.20926225380819352, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.1965716928243637}, {"id": 618, "seek": 262956, "start": 2647.08, "end": 2656.08, "text": " too long arguing with people online is essentially you know so you're in the computer science", "tokens": [51240, 886, 938, 19697, 365, 561, 2950, 307, 4476, 291, 458, 370, 291, 434, 294, 264, 3820, 3497, 51690], "temperature": 0.0, "avg_logprob": -0.20926225380819352, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.1965716928243637}, {"id": 619, "seek": 265608, "start": 2656.08, "end": 2660.7599999999998, "text": " department, you've come here, you've given us a talk that's largely about brains but", "tokens": [50364, 5882, 11, 291, 600, 808, 510, 11, 291, 600, 2212, 505, 257, 751, 300, 311, 11611, 466, 15442, 457, 50598], "temperature": 0.0, "avg_logprob": -0.1893922581392176, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.4529207944869995}, {"id": 620, "seek": 265608, "start": 2660.7599999999998, "end": 2666.16, "text": " many people seem to object to the idea that computers have anything to tell us about brains", "tokens": [50598, 867, 561, 1643, 281, 2657, 281, 264, 1558, 300, 10807, 362, 1340, 281, 980, 505, 466, 15442, 50868], "temperature": 0.0, "avg_logprob": -0.1893922581392176, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.4529207944869995}, {"id": 621, "seek": 265608, "start": 2666.16, "end": 2670.92, "text": " or indeed the idea that the brain is a computer despite the fact that neuroscientists often", "tokens": [50868, 420, 6451, 264, 1558, 300, 264, 3567, 307, 257, 3820, 7228, 264, 1186, 300, 28813, 5412, 1751, 2049, 51106], "temperature": 0.0, "avg_logprob": -0.1893922581392176, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.4529207944869995}, {"id": 622, "seek": 265608, "start": 2670.92, "end": 2673.16, "text": " refer to computation in the brain.", "tokens": [51106, 2864, 281, 24903, 294, 264, 3567, 13, 51218], "temperature": 0.0, "avg_logprob": -0.1893922581392176, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.4529207944869995}, {"id": 623, "seek": 265608, "start": 2673.16, "end": 2679.84, "text": " So my question is to you, is the brain a computer, I don't know I'll just hand that over to", "tokens": [51218, 407, 452, 1168, 307, 281, 291, 11, 307, 264, 3567, 257, 3820, 11, 286, 500, 380, 458, 286, 603, 445, 1011, 300, 670, 281, 51552], "temperature": 0.0, "avg_logprob": -0.1893922581392176, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.4529207944869995}, {"id": 624, "seek": 265608, "start": 2679.84, "end": 2680.84, "text": " you first.", "tokens": [51552, 291, 700, 13, 51602], "temperature": 0.0, "avg_logprob": -0.1893922581392176, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.4529207944869995}, {"id": 625, "seek": 268084, "start": 2681.6000000000004, "end": 2682.6000000000004, "text": " Yes?", "tokens": [50402, 1079, 30, 50452], "temperature": 0.0, "avg_logprob": -0.22357510297726363, "compression_ratio": 1.470873786407767, "no_speech_prob": 0.06054702401161194}, {"id": 626, "seek": 268084, "start": 2682.6000000000004, "end": 2685.1200000000003, "text": " Good, okay.", "tokens": [50452, 2205, 11, 1392, 13, 50578], "temperature": 0.0, "avg_logprob": -0.22357510297726363, "compression_ratio": 1.470873786407767, "no_speech_prob": 0.06054702401161194}, {"id": 627, "seek": 268084, "start": 2685.1200000000003, "end": 2690.88, "text": " And for the record I didn't tell him to say that if anyone from Twitter is watching.", "tokens": [50578, 400, 337, 264, 2136, 286, 994, 380, 980, 796, 281, 584, 300, 498, 2878, 490, 5794, 307, 1976, 13, 50866], "temperature": 0.0, "avg_logprob": -0.22357510297726363, "compression_ratio": 1.470873786407767, "no_speech_prob": 0.06054702401161194}, {"id": 628, "seek": 268084, "start": 2690.88, "end": 2696.1600000000003, "text": " And B, can you just maybe give an intuitive understanding of why the answer is yes despite", "tokens": [50866, 400, 363, 11, 393, 291, 445, 1310, 976, 364, 21769, 3701, 295, 983, 264, 1867, 307, 2086, 7228, 51130], "temperature": 0.0, "avg_logprob": -0.22357510297726363, "compression_ratio": 1.470873786407767, "no_speech_prob": 0.06054702401161194}, {"id": 629, "seek": 268084, "start": 2696.1600000000003, "end": 2701.0, "text": " the fact that obviously our brains are very different from our laptops or our cell phones", "tokens": [51130, 264, 1186, 300, 2745, 527, 15442, 366, 588, 819, 490, 527, 27642, 420, 527, 2815, 10216, 51372], "temperature": 0.0, "avg_logprob": -0.22357510297726363, "compression_ratio": 1.470873786407767, "no_speech_prob": 0.06054702401161194}, {"id": 630, "seek": 268084, "start": 2701.0, "end": 2703.6000000000004, "text": " and stuff like that.", "tokens": [51372, 293, 1507, 411, 300, 13, 51502], "temperature": 0.0, "avg_logprob": -0.22357510297726363, "compression_ratio": 1.470873786407767, "no_speech_prob": 0.06054702401161194}, {"id": 631, "seek": 270360, "start": 2703.6, "end": 2710.64, "text": " So there's many ways you can do computation with physical stuff and you could get some", "tokens": [50364, 407, 456, 311, 867, 2098, 291, 393, 360, 24903, 365, 4001, 1507, 293, 291, 727, 483, 512, 50716], "temperature": 0.0, "avg_logprob": -0.11264858823834044, "compression_ratio": 1.924778761061947, "no_speech_prob": 0.5857721567153931}, {"id": 632, "seek": 270360, "start": 2710.64, "end": 2715.92, "text": " silicon and make transistors and then run them at very high voltage much higher than", "tokens": [50716, 22848, 293, 652, 1145, 46976, 293, 550, 1190, 552, 412, 588, 1090, 8352, 709, 2946, 813, 50980], "temperature": 0.0, "avg_logprob": -0.11264858823834044, "compression_ratio": 1.924778761061947, "no_speech_prob": 0.5857721567153931}, {"id": 633, "seek": 270360, "start": 2715.92, "end": 2721.7999999999997, "text": " needed to make them be digital and then you could if you wanted to represent a number", "tokens": [50980, 2978, 281, 652, 552, 312, 4562, 293, 550, 291, 727, 498, 291, 1415, 281, 2906, 257, 1230, 51274], "temperature": 0.0, "avg_logprob": -0.11264858823834044, "compression_ratio": 1.924778761061947, "no_speech_prob": 0.5857721567153931}, {"id": 634, "seek": 270360, "start": 2721.7999999999997, "end": 2726.92, "text": " you could have bits and you could and so on and you could create multipliers and adders", "tokens": [51274, 291, 727, 362, 9239, 293, 291, 727, 293, 370, 322, 293, 291, 727, 1884, 12788, 4890, 293, 909, 433, 51530], "temperature": 0.0, "avg_logprob": -0.11264858823834044, "compression_ratio": 1.924778761061947, "no_speech_prob": 0.5857721567153931}, {"id": 635, "seek": 270360, "start": 2726.92, "end": 2731.16, "text": " and then you could put all that together and you could have some bits that tell you where", "tokens": [51530, 293, 550, 291, 727, 829, 439, 300, 1214, 293, 291, 727, 362, 512, 9239, 300, 980, 291, 689, 51742], "temperature": 0.0, "avg_logprob": -0.11264858823834044, "compression_ratio": 1.924778761061947, "no_speech_prob": 0.5857721567153931}, {"id": 636, "seek": 273116, "start": 2731.2, "end": 2738.24, "text": " in memory to find stuff and you could make a conventional computer or you could make", "tokens": [50366, 294, 4675, 281, 915, 1507, 293, 291, 727, 652, 257, 16011, 3820, 420, 291, 727, 652, 50718], "temperature": 0.0, "avg_logprob": -0.18307915913689995, "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.3207639753818512}, {"id": 637, "seek": 273116, "start": 2738.24, "end": 2744.2799999999997, "text": " little devices that have some input lines that are hardwired with input lines and you", "tokens": [50718, 707, 5759, 300, 362, 512, 4846, 3876, 300, 366, 1152, 86, 1824, 365, 4846, 3876, 293, 291, 51020], "temperature": 0.0, "avg_logprob": -0.18307915913689995, "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.3207639753818512}, {"id": 638, "seek": 273116, "start": 2744.2799999999997, "end": 2746.08, "text": " could have adaptive weights on the input lines.", "tokens": [51020, 727, 362, 27912, 17443, 322, 264, 4846, 3876, 13, 51110], "temperature": 0.0, "avg_logprob": -0.18307915913689995, "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.3207639753818512}, {"id": 639, "seek": 273116, "start": 2746.08, "end": 2753.52, "text": " So early neural nets, Marvin Minsky made neural nets out of feedback controllers that were", "tokens": [51110, 407, 2440, 18161, 36170, 11, 48722, 376, 44153, 1027, 18161, 36170, 484, 295, 5824, 26903, 300, 645, 51482], "temperature": 0.0, "avg_logprob": -0.18307915913689995, "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.3207639753818512}, {"id": 640, "seek": 273116, "start": 2753.52, "end": 2759.08, "text": " used in I think B-52 bombers or B-29 bombers or something, B-27 I don't know, some kind", "tokens": [51482, 1143, 294, 286, 519, 363, 12, 17602, 50055, 420, 363, 12, 11871, 50055, 420, 746, 11, 363, 12, 10076, 286, 500, 380, 458, 11, 512, 733, 51760], "temperature": 0.0, "avg_logprob": -0.18307915913689995, "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.3207639753818512}, {"id": 641, "seek": 275908, "start": 2759.08, "end": 2765.96, "text": " of bomber, it was America and so you can make computers in lots of different ways.", "tokens": [50364, 295, 44889, 11, 309, 390, 3374, 293, 370, 291, 393, 652, 10807, 294, 3195, 295, 819, 2098, 13, 50708], "temperature": 0.0, "avg_logprob": -0.12574271802549009, "compression_ratio": 1.8277310924369747, "no_speech_prob": 0.012629944831132889}, {"id": 642, "seek": 275908, "start": 2765.96, "end": 2771.48, "text": " When I was a kid I used to make computers by you take a six inch nail and you saw the", "tokens": [50708, 1133, 286, 390, 257, 1636, 286, 1143, 281, 652, 10807, 538, 291, 747, 257, 2309, 7227, 10173, 293, 291, 1866, 264, 50984], "temperature": 0.0, "avg_logprob": -0.12574271802549009, "compression_ratio": 1.8277310924369747, "no_speech_prob": 0.012629944831132889}, {"id": 643, "seek": 275908, "start": 2771.48, "end": 2777.7599999999998, "text": " head off and then you wrap copper wire around it and then you take a razor blade and you", "tokens": [50984, 1378, 766, 293, 550, 291, 7019, 15007, 6234, 926, 309, 293, 550, 291, 747, 257, 30478, 10959, 293, 291, 51298], "temperature": 0.0, "avg_logprob": -0.12574271802549009, "compression_ratio": 1.8277310924369747, "no_speech_prob": 0.012629944831132889}, {"id": 644, "seek": 275908, "start": 2777.7599999999998, "end": 2784.24, "text": " break it in half so that it's a nice flexible thing like this and you wrap a bit of copper", "tokens": [51298, 1821, 309, 294, 1922, 370, 300, 309, 311, 257, 1481, 11358, 551, 411, 341, 293, 291, 7019, 257, 857, 295, 15007, 51622], "temperature": 0.0, "avg_logprob": -0.12574271802549009, "compression_ratio": 1.8277310924369747, "no_speech_prob": 0.012629944831132889}, {"id": 645, "seek": 275908, "start": 2784.24, "end": 2788.56, "text": " wire around the razor blade and then when the current goes through the nail it'll make", "tokens": [51622, 6234, 926, 264, 30478, 10959, 293, 550, 562, 264, 2190, 1709, 807, 264, 10173, 309, 603, 652, 51838], "temperature": 0.0, "avg_logprob": -0.12574271802549009, "compression_ratio": 1.8277310924369747, "no_speech_prob": 0.012629944831132889}, {"id": 646, "seek": 278856, "start": 2788.64, "end": 2792.6, "text": " the razor blade go down and you'll make a contact so now you've got a relay and then", "tokens": [50368, 264, 30478, 10959, 352, 760, 293, 291, 603, 652, 257, 3385, 370, 586, 291, 600, 658, 257, 24214, 293, 550, 50566], "temperature": 0.0, "avg_logprob": -0.15124737486547354, "compression_ratio": 1.7957446808510638, "no_speech_prob": 0.0011130969505757093}, {"id": 647, "seek": 278856, "start": 2792.6, "end": 2795.24, "text": " you can put a bunch of those together and make logic gates.", "tokens": [50566, 291, 393, 829, 257, 3840, 295, 729, 1214, 293, 652, 9952, 19792, 13, 50698], "temperature": 0.0, "avg_logprob": -0.15124737486547354, "compression_ratio": 1.7957446808510638, "no_speech_prob": 0.0011130969505757093}, {"id": 648, "seek": 278856, "start": 2795.24, "end": 2800.48, "text": " I never got more than about two logic gates that way but yeah you can make computers in", "tokens": [50698, 286, 1128, 658, 544, 813, 466, 732, 9952, 19792, 300, 636, 457, 1338, 291, 393, 652, 10807, 294, 50960], "temperature": 0.0, "avg_logprob": -0.15124737486547354, "compression_ratio": 1.7957446808510638, "no_speech_prob": 0.0011130969505757093}, {"id": 649, "seek": 278856, "start": 2800.48, "end": 2807.92, "text": " lots of different ways and the brain is clearly made in a different way from the normal computers", "tokens": [50960, 3195, 295, 819, 2098, 293, 264, 3567, 307, 4448, 1027, 294, 257, 819, 636, 490, 264, 2710, 10807, 51332], "temperature": 0.0, "avg_logprob": -0.15124737486547354, "compression_ratio": 1.7957446808510638, "no_speech_prob": 0.0011130969505757093}, {"id": 650, "seek": 278856, "start": 2807.92, "end": 2814.7999999999997, "text": " which has some different strengths and weaknesses so it's much slower but on the other hand", "tokens": [51332, 597, 575, 512, 819, 16986, 293, 24381, 370, 309, 311, 709, 14009, 457, 322, 264, 661, 1011, 51676], "temperature": 0.0, "avg_logprob": -0.15124737486547354, "compression_ratio": 1.7957446808510638, "no_speech_prob": 0.0011130969505757093}, {"id": 651, "seek": 281480, "start": 2814.8, "end": 2817.76, "text": " you can make it much more parallel.", "tokens": [50364, 291, 393, 652, 309, 709, 544, 8952, 13, 50512], "temperature": 0.0, "avg_logprob": -0.18418767317286078, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.04065887629985809}, {"id": 652, "seek": 281480, "start": 2817.76, "end": 2822.6000000000004, "text": " It has one special property which I think is what makes us mortal which is that every", "tokens": [50512, 467, 575, 472, 2121, 4707, 597, 286, 519, 307, 437, 1669, 505, 27624, 597, 307, 300, 633, 50754], "temperature": 0.0, "avg_logprob": -0.18418767317286078, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.04065887629985809}, {"id": 653, "seek": 281480, "start": 2822.6000000000004, "end": 2829.8, "text": " brain is different so I can't take the weights from my brain and put them in Blake's brain", "tokens": [50754, 3567, 307, 819, 370, 286, 393, 380, 747, 264, 17443, 490, 452, 3567, 293, 829, 552, 294, 23451, 311, 3567, 51114], "temperature": 0.0, "avg_logprob": -0.18418767317286078, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.04065887629985809}, {"id": 654, "seek": 281480, "start": 2829.8, "end": 2832.92, "text": " and hope that it'll work because he just doesn't have connections in the same places.", "tokens": [51114, 293, 1454, 300, 309, 603, 589, 570, 415, 445, 1177, 380, 362, 9271, 294, 264, 912, 3190, 13, 51270], "temperature": 0.0, "avg_logprob": -0.18418767317286078, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.04065887629985809}, {"id": 655, "seek": 281480, "start": 2832.92, "end": 2833.92, "text": " You've tried.", "tokens": [51270, 509, 600, 3031, 13, 51320], "temperature": 0.0, "avg_logprob": -0.18418767317286078, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.04065887629985809}, {"id": 656, "seek": 281480, "start": 2833.92, "end": 2834.92, "text": " Right.", "tokens": [51320, 1779, 13, 51370], "temperature": 0.0, "avg_logprob": -0.18418767317286078, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.04065887629985809}, {"id": 657, "seek": 281480, "start": 2834.92, "end": 2840.2400000000002, "text": " Well there's a way of doing it where you take the weights in my brain I turn it into strings", "tokens": [51370, 1042, 456, 311, 257, 636, 295, 884, 309, 689, 291, 747, 264, 17443, 294, 452, 3567, 286, 1261, 309, 666, 13985, 51636], "temperature": 0.0, "avg_logprob": -0.18418767317286078, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.04065887629985809}, {"id": 658, "seek": 284024, "start": 2840.24, "end": 2841.24, "text": " of words.", "tokens": [50364, 295, 2283, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2561011401089755, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00478612445294857}, {"id": 659, "seek": 284024, "start": 2841.24, "end": 2847.4399999999996, "text": " Blake absorbs these strings of words and puts different weights in his brain.", "tokens": [50414, 23451, 40745, 613, 13985, 295, 2283, 293, 8137, 819, 17443, 294, 702, 3567, 13, 50724], "temperature": 0.0, "avg_logprob": -0.2561011401089755, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00478612445294857}, {"id": 660, "seek": 284024, "start": 2847.4399999999996, "end": 2851.52, "text": " It's pretty lucky all our brains are different because otherwise rich people will grab poor", "tokens": [50724, 467, 311, 1238, 6356, 439, 527, 15442, 366, 819, 570, 5911, 4593, 561, 486, 4444, 4716, 50928], "temperature": 0.0, "avg_logprob": -0.2561011401089755, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00478612445294857}, {"id": 661, "seek": 284024, "start": 2851.52, "end": 2862.2, "text": " people's brains so they could live forever but quite okay.", "tokens": [50928, 561, 311, 15442, 370, 436, 727, 1621, 5680, 457, 1596, 1392, 13, 51462], "temperature": 0.0, "avg_logprob": -0.2561011401089755, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.00478612445294857}, {"id": 662, "seek": 286220, "start": 2862.2, "end": 2871.8399999999997, "text": " So I think I want to ask you then following on that what do you think about some of the", "tokens": [50364, 407, 286, 519, 286, 528, 281, 1029, 291, 550, 3480, 322, 300, 437, 360, 291, 519, 466, 512, 295, 264, 50846], "temperature": 0.0, "avg_logprob": -0.25998584429423016, "compression_ratio": 1.5677083333333333, "no_speech_prob": 0.48703640699386597}, {"id": 663, "seek": 286220, "start": 2871.8399999999997, "end": 2877.7599999999998, "text": " quests to fully characterize the brains connectome do you think that is a scientifically worthwhile", "tokens": [50846, 34247, 281, 4498, 38463, 264, 15442, 1745, 423, 360, 291, 519, 300, 307, 257, 39719, 28159, 51142], "temperature": 0.0, "avg_logprob": -0.25998584429423016, "compression_ratio": 1.5677083333333333, "no_speech_prob": 0.48703640699386597}, {"id": 664, "seek": 286220, "start": 2877.7599999999998, "end": 2878.7599999999998, "text": " endeavor.", "tokens": [51142, 34975, 13, 51192], "temperature": 0.0, "avg_logprob": -0.25998584429423016, "compression_ratio": 1.5677083333333333, "no_speech_prob": 0.48703640699386597}, {"id": 665, "seek": 286220, "start": 2878.7599999999998, "end": 2888.3199999999997, "text": " Yes I do partly because some of the people doing it are my friends.", "tokens": [51192, 1079, 286, 360, 17031, 570, 512, 295, 264, 561, 884, 309, 366, 452, 1855, 13, 51670], "temperature": 0.0, "avg_logprob": -0.25998584429423016, "compression_ratio": 1.5677083333333333, "no_speech_prob": 0.48703640699386597}, {"id": 666, "seek": 286220, "start": 2888.3199999999997, "end": 2891.48, "text": " Ignoring your loyalty to Sebastian.", "tokens": [51670, 24754, 3662, 428, 22831, 281, 31102, 13, 51828], "temperature": 0.0, "avg_logprob": -0.25998584429423016, "compression_ratio": 1.5677083333333333, "no_speech_prob": 0.48703640699386597}, {"id": 667, "seek": 289148, "start": 2891.76, "end": 2897.12, "text": " Not well in that case no.", "tokens": [50378, 1726, 731, 294, 300, 1389, 572, 13, 50646], "temperature": 0.0, "avg_logprob": -0.22874852722766353, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.1197313442826271}, {"id": 668, "seek": 289148, "start": 2897.12, "end": 2903.2, "text": " It seems to me it is very worth doing but you don't have to do that in order to begin", "tokens": [50646, 467, 2544, 281, 385, 309, 307, 588, 3163, 884, 457, 291, 500, 380, 362, 281, 360, 300, 294, 1668, 281, 1841, 50950], "temperature": 0.0, "avg_logprob": -0.22874852722766353, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.1197313442826271}, {"id": 669, "seek": 289148, "start": 2903.2, "end": 2905.04, "text": " to understand the principles.", "tokens": [50950, 281, 1223, 264, 9156, 13, 51042], "temperature": 0.0, "avg_logprob": -0.22874852722766353, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.1197313442826271}, {"id": 670, "seek": 289148, "start": 2905.04, "end": 2906.04, "text": " Very good.", "tokens": [51042, 4372, 665, 13, 51092], "temperature": 0.0, "avg_logprob": -0.22874852722766353, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.1197313442826271}, {"id": 671, "seek": 289148, "start": 2906.04, "end": 2910.04, "text": " But for things like the retina which has a lot of hardwired stuff in it I think it's", "tokens": [51092, 583, 337, 721, 411, 264, 1533, 1426, 597, 575, 257, 688, 295, 1152, 86, 1824, 1507, 294, 309, 286, 519, 309, 311, 51292], "temperature": 0.0, "avg_logprob": -0.22874852722766353, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.1197313442826271}, {"id": 672, "seek": 289148, "start": 2910.04, "end": 2912.2400000000002, "text": " really important to do that.", "tokens": [51292, 534, 1021, 281, 360, 300, 13, 51402], "temperature": 0.0, "avg_logprob": -0.22874852722766353, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.1197313442826271}, {"id": 673, "seek": 289148, "start": 2912.2400000000002, "end": 2914.8, "text": " So that actually leads on to my next question.", "tokens": [51402, 407, 300, 767, 6689, 322, 281, 452, 958, 1168, 13, 51530], "temperature": 0.0, "avg_logprob": -0.22874852722766353, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.1197313442826271}, {"id": 674, "seek": 289148, "start": 2914.8, "end": 2917.44, "text": " I wanted to ask you about hardwiring.", "tokens": [51530, 286, 1415, 281, 1029, 291, 466, 1152, 86, 5057, 13, 51662], "temperature": 0.0, "avg_logprob": -0.22874852722766353, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.1197313442826271}, {"id": 675, "seek": 291744, "start": 2917.44, "end": 2924.08, "text": " So another thing that I think many people who study the brain find difficult about artificial", "tokens": [50364, 407, 1071, 551, 300, 286, 519, 867, 561, 567, 2979, 264, 3567, 915, 2252, 466, 11677, 50696], "temperature": 0.0, "avg_logprob": -0.13239599733936544, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00432805297896266}, {"id": 676, "seek": 291744, "start": 2924.08, "end": 2929.12, "text": " neural networks as a model for the brain is that as you say you start with random weights", "tokens": [50696, 18161, 9590, 382, 257, 2316, 337, 264, 3567, 307, 300, 382, 291, 584, 291, 722, 365, 4974, 17443, 50948], "temperature": 0.0, "avg_logprob": -0.13239599733936544, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00432805297896266}, {"id": 677, "seek": 291744, "start": 2929.12, "end": 2932.2000000000003, "text": " and you train it on a lot of data and you get these things out.", "tokens": [50948, 293, 291, 3847, 309, 322, 257, 688, 295, 1412, 293, 291, 483, 613, 721, 484, 13, 51102], "temperature": 0.0, "avg_logprob": -0.13239599733936544, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00432805297896266}, {"id": 678, "seek": 291744, "start": 2932.2000000000003, "end": 2938.04, "text": " But we know that there are some pre-wired things in many brains so the classic examples", "tokens": [51102, 583, 321, 458, 300, 456, 366, 512, 659, 12, 86, 1824, 721, 294, 867, 15442, 370, 264, 7230, 5110, 51394], "temperature": 0.0, "avg_logprob": -0.13239599733936544, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00432805297896266}, {"id": 679, "seek": 291744, "start": 2938.04, "end": 2944.16, "text": " are a horse can run pretty much right out of the womb but even within humans arguably", "tokens": [51394, 366, 257, 6832, 393, 1190, 1238, 709, 558, 484, 295, 264, 34310, 457, 754, 1951, 6255, 26771, 51700], "temperature": 0.0, "avg_logprob": -0.13239599733936544, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00432805297896266}, {"id": 680, "seek": 294416, "start": 2944.16, "end": 2948.16, "text": " there are some things that we find easier to learn than others.", "tokens": [50364, 456, 366, 512, 721, 300, 321, 915, 3571, 281, 1466, 813, 2357, 13, 50564], "temperature": 0.0, "avg_logprob": -0.19420334787079782, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.009383413940668106}, {"id": 681, "seek": 294416, "start": 2948.16, "end": 2955.48, "text": " And so what do you think is the place for innate behavior within neural networks as", "tokens": [50564, 400, 370, 437, 360, 291, 519, 307, 264, 1081, 337, 41766, 5223, 1951, 18161, 9590, 382, 50930], "temperature": 0.0, "avg_logprob": -0.19420334787079782, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.009383413940668106}, {"id": 682, "seek": 294416, "start": 2955.48, "end": 2957.16, "text": " a model of cognition?", "tokens": [50930, 257, 2316, 295, 46905, 30, 51014], "temperature": 0.0, "avg_logprob": -0.19420334787079782, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.009383413940668106}, {"id": 683, "seek": 294416, "start": 2957.16, "end": 2962.7599999999998, "text": " Okay so it used to be when I was a student if you were interested in language people", "tokens": [51014, 1033, 370, 309, 1143, 281, 312, 562, 286, 390, 257, 3107, 498, 291, 645, 3102, 294, 2856, 561, 51294], "temperature": 0.0, "avg_logprob": -0.19420334787079782, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.009383413940668106}, {"id": 684, "seek": 294416, "start": 2962.7599999999998, "end": 2968.2, "text": " would tell you that it was all innate and it just kind of matured as you got older and", "tokens": [51294, 576, 980, 291, 300, 309, 390, 439, 41766, 293, 309, 445, 733, 295, 14442, 67, 382, 291, 658, 4906, 293, 51566], "temperature": 0.0, "avg_logprob": -0.19420334787079782, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.009383413940668106}, {"id": 685, "seek": 294416, "start": 2968.2, "end": 2972.7599999999998, "text": " maybe you learned like 12 parameters that characterised your particular language whether", "tokens": [51566, 1310, 291, 3264, 411, 2272, 9834, 300, 2517, 2640, 428, 1729, 2856, 1968, 51794], "temperature": 0.0, "avg_logprob": -0.19420334787079782, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.009383413940668106}, {"id": 686, "seek": 297276, "start": 2972.84, "end": 2978.28, "text": " it was subject verb object or some other way.", "tokens": [50368, 309, 390, 3983, 9595, 2657, 420, 512, 661, 636, 13, 50640], "temperature": 0.0, "avg_logprob": -0.2300535722212358, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.01045838650316}, {"id": 687, "seek": 297276, "start": 2978.28, "end": 2983.0800000000004, "text": " In fact there's a nova that I saw, it was probably made about 20 years ago and it has", "tokens": [50640, 682, 1186, 456, 311, 257, 28265, 300, 286, 1866, 11, 309, 390, 1391, 1027, 466, 945, 924, 2057, 293, 309, 575, 50880], "temperature": 0.0, "avg_logprob": -0.2300535722212358, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.01045838650316}, {"id": 688, "seek": 297276, "start": 2983.0800000000004, "end": 2988.5600000000004, "text": " all the leading linguists all of whom were educated by Chomsky and they look straight", "tokens": [50880, 439, 264, 5775, 21766, 1751, 439, 295, 7101, 645, 15872, 538, 761, 4785, 4133, 293, 436, 574, 2997, 51154], "temperature": 0.0, "avg_logprob": -0.2300535722212358, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.01045838650316}, {"id": 689, "seek": 297276, "start": 2988.5600000000004, "end": 2992.7200000000003, "text": " at the camera and they say there's a lot we don't know about language but one thing", "tokens": [51154, 412, 264, 2799, 293, 436, 584, 456, 311, 257, 688, 321, 500, 380, 458, 466, 2856, 457, 472, 551, 51362], "temperature": 0.0, "avg_logprob": -0.2300535722212358, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.01045838650316}, {"id": 690, "seek": 297276, "start": 2992.7200000000003, "end": 2997.6000000000004, "text": " we know for sure is that it's not learned.", "tokens": [51362, 321, 458, 337, 988, 307, 300, 309, 311, 406, 3264, 13, 51606], "temperature": 0.0, "avg_logprob": -0.2300535722212358, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.01045838650316}, {"id": 691, "seek": 297276, "start": 2997.6000000000004, "end": 2999.7200000000003, "text": " So Chomsky had really good gulag.", "tokens": [51606, 407, 761, 4785, 4133, 632, 534, 665, 290, 425, 559, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2300535722212358, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.01045838650316}, {"id": 692, "seek": 297276, "start": 2999.7200000000003, "end": 3002.7200000000003, "text": " Yeah he did.", "tokens": [51712, 865, 415, 630, 13, 51862], "temperature": 0.0, "avg_logprob": -0.2300535722212358, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.01045838650316}, {"id": 693, "seek": 300272, "start": 3002.72, "end": 3009.68, "text": " But it's over because we know now if you want to translate you just learn it all.", "tokens": [50364, 583, 309, 311, 670, 570, 321, 458, 586, 498, 291, 528, 281, 13799, 291, 445, 1466, 309, 439, 13, 50712], "temperature": 0.0, "avg_logprob": -0.1873895589587758, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.005707212723791599}, {"id": 694, "seek": 300272, "start": 3009.68, "end": 3015.3599999999997, "text": " The number of linguists required to get a system that can turn a string of symbols in", "tokens": [50712, 440, 1230, 295, 21766, 1751, 4739, 281, 483, 257, 1185, 300, 393, 1261, 257, 6798, 295, 16944, 294, 50996], "temperature": 0.0, "avg_logprob": -0.1873895589587758, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.005707212723791599}, {"id": 695, "seek": 300272, "start": 3015.3599999999997, "end": 3020.0, "text": " English into a string of symbols in French is roughly zero.", "tokens": [50996, 3669, 666, 257, 6798, 295, 16944, 294, 5522, 307, 9810, 4018, 13, 51228], "temperature": 0.0, "avg_logprob": -0.1873895589587758, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.005707212723791599}, {"id": 696, "seek": 300272, "start": 3020.0, "end": 3023.48, "text": " I mean linguists involved in preparing the databases for training and making sure you", "tokens": [51228, 286, 914, 21766, 1751, 3288, 294, 10075, 264, 22380, 337, 3097, 293, 1455, 988, 291, 51402], "temperature": 0.0, "avg_logprob": -0.1873895589587758, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.005707212723791599}, {"id": 697, "seek": 300272, "start": 3023.48, "end": 3029.0, "text": " get sort of a variety of grammatic structures and things but basically you don't need linguists,", "tokens": [51402, 483, 1333, 295, 257, 5673, 295, 17570, 2399, 9227, 293, 721, 457, 1936, 291, 500, 380, 643, 21766, 1751, 11, 51678], "temperature": 0.0, "avg_logprob": -0.1873895589587758, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.005707212723791599}, {"id": 698, "seek": 300272, "start": 3029.0, "end": 3032.3599999999997, "text": " you just need data.", "tokens": [51678, 291, 445, 643, 1412, 13, 51846], "temperature": 0.0, "avg_logprob": -0.1873895589587758, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.005707212723791599}, {"id": 699, "seek": 303236, "start": 3032.36, "end": 3034.6800000000003, "text": " So you don't need much innate structure.", "tokens": [50364, 407, 291, 500, 380, 643, 709, 41766, 3877, 13, 50480], "temperature": 0.0, "avg_logprob": -0.15742483827256665, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.003065318800508976}, {"id": 700, "seek": 303236, "start": 3034.6800000000003, "end": 3038.6400000000003, "text": " The issue of what is innate, it doesn't, it seems to me there's not much point putting", "tokens": [50480, 440, 2734, 295, 437, 307, 41766, 11, 309, 1177, 380, 11, 309, 2544, 281, 385, 456, 311, 406, 709, 935, 3372, 50678], "temperature": 0.0, "avg_logprob": -0.15742483827256665, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.003065318800508976}, {"id": 701, "seek": 303236, "start": 3038.6400000000003, "end": 3043.6400000000003, "text": " in stuff innately if you can learn it quickly.", "tokens": [50678, 294, 1507, 7714, 1592, 498, 291, 393, 1466, 309, 2661, 13, 50928], "temperature": 0.0, "avg_logprob": -0.15742483827256665, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.003065318800508976}, {"id": 702, "seek": 303236, "start": 3043.6400000000003, "end": 3050.6800000000003, "text": " So for example the ability to move and get 3D structure from motion, that's actually", "tokens": [50928, 407, 337, 1365, 264, 3485, 281, 1286, 293, 483, 805, 35, 3877, 490, 5394, 11, 300, 311, 767, 51280], "temperature": 0.0, "avg_logprob": -0.15742483827256665, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.003065318800508976}, {"id": 703, "seek": 303236, "start": 3050.6800000000003, "end": 3055.2000000000003, "text": " very easy to learn so I don't believe that's innate even though a child can do it at like", "tokens": [51280, 588, 1858, 281, 1466, 370, 286, 500, 380, 1697, 300, 311, 41766, 754, 1673, 257, 1440, 393, 360, 309, 412, 411, 51506], "temperature": 0.0, "avg_logprob": -0.15742483827256665, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.003065318800508976}, {"id": 704, "seek": 303236, "start": 3055.2000000000003, "end": 3057.56, "text": " two days.", "tokens": [51506, 732, 1708, 13, 51624], "temperature": 0.0, "avg_logprob": -0.15742483827256665, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.003065318800508976}, {"id": 705, "seek": 305756, "start": 3057.56, "end": 3065.2799999999997, "text": " You show them a sort of W made of paper and you rotate it in a consistent way and they", "tokens": [50364, 509, 855, 552, 257, 1333, 295, 343, 1027, 295, 3035, 293, 291, 13121, 309, 294, 257, 8398, 636, 293, 436, 50750], "temperature": 0.0, "avg_logprob": -0.19913999424424284, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.011269774287939072}, {"id": 706, "seek": 305756, "start": 3065.2799999999997, "end": 3069.12, "text": " get bored and as soon as you rotate it in a way, move it in a way that's not consistent", "tokens": [50750, 483, 13521, 293, 382, 2321, 382, 291, 13121, 309, 294, 257, 636, 11, 1286, 309, 294, 257, 636, 300, 311, 406, 8398, 50942], "temperature": 0.0, "avg_logprob": -0.19913999424424284, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.011269774287939072}, {"id": 707, "seek": 305756, "start": 3069.12, "end": 3074.32, "text": " with the rotation, the interest perks up.", "tokens": [50942, 365, 264, 12447, 11, 264, 1179, 36991, 493, 13, 51202], "temperature": 0.0, "avg_logprob": -0.19913999424424284, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.011269774287939072}, {"id": 708, "seek": 305756, "start": 3074.32, "end": 3076.24, "text": " But I think they can learn it in two days.", "tokens": [51202, 583, 286, 519, 436, 393, 1466, 309, 294, 732, 1708, 13, 51298], "temperature": 0.0, "avg_logprob": -0.19913999424424284, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.011269774287939072}, {"id": 709, "seek": 305756, "start": 3076.24, "end": 3079.2799999999997, "text": " It's really easy to learn.", "tokens": [51298, 467, 311, 534, 1858, 281, 1466, 13, 51450], "temperature": 0.0, "avg_logprob": -0.19913999424424284, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.011269774287939072}, {"id": 710, "seek": 305756, "start": 3079.2799999999997, "end": 3081.2799999999997, "text": " Okay interesting.", "tokens": [51450, 1033, 1880, 13, 51550], "temperature": 0.0, "avg_logprob": -0.19913999424424284, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.011269774287939072}, {"id": 711, "seek": 308128, "start": 3081.28, "end": 3089.76, "text": " Now I want to ask you, I know partially what your answer is going to be but when I remember", "tokens": [50364, 823, 286, 528, 281, 1029, 291, 11, 286, 458, 18886, 437, 428, 1867, 307, 516, 281, 312, 457, 562, 286, 1604, 50788], "temperature": 0.0, "avg_logprob": -0.15860135871243763, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.11259812861680984}, {"id": 712, "seek": 308128, "start": 3089.76, "end": 3096.92, "text": " long ago you told me that one of your career goals, at least earlier in your career, was", "tokens": [50788, 938, 2057, 291, 1907, 385, 300, 472, 295, 428, 3988, 5493, 11, 412, 1935, 3071, 294, 428, 3988, 11, 390, 51146], "temperature": 0.0, "avg_logprob": -0.15860135871243763, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.11259812861680984}, {"id": 713, "seek": 308128, "start": 3096.92, "end": 3103.32, "text": " to prove that everything that psychologists thought about the brain was wrong.", "tokens": [51146, 281, 7081, 300, 1203, 300, 41562, 1194, 466, 264, 3567, 390, 2085, 13, 51466], "temperature": 0.0, "avg_logprob": -0.15860135871243763, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.11259812861680984}, {"id": 714, "seek": 308128, "start": 3103.32, "end": 3108.88, "text": " And so my question is what was it that they had wrong, are they still getting it wrong", "tokens": [51466, 400, 370, 452, 1168, 307, 437, 390, 309, 300, 436, 632, 2085, 11, 366, 436, 920, 1242, 309, 2085, 51744], "temperature": 0.0, "avg_logprob": -0.15860135871243763, "compression_ratio": 1.6398104265402844, "no_speech_prob": 0.11259812861680984}, {"id": 715, "seek": 310888, "start": 3108.88, "end": 3112.88, "text": " and is neuroscience getting that same thing wrong?", "tokens": [50364, 293, 307, 42762, 1242, 300, 912, 551, 2085, 30, 50564], "temperature": 0.0, "avg_logprob": -0.14800644846795832, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.015587644651532173}, {"id": 716, "seek": 310888, "start": 3112.88, "end": 3118.6800000000003, "text": " It was mainly to do with this conviction that psychologists had partly based on Chomsky", "tokens": [50564, 467, 390, 8704, 281, 360, 365, 341, 24837, 300, 41562, 632, 17031, 2361, 322, 761, 4785, 4133, 50854], "temperature": 0.0, "avg_logprob": -0.14800644846795832, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.015587644651532173}, {"id": 717, "seek": 310888, "start": 3118.6800000000003, "end": 3124.2000000000003, "text": " that there was an awful lot of innate stuff there and that you couldn't just learn a whole", "tokens": [50854, 300, 456, 390, 364, 11232, 688, 295, 41766, 1507, 456, 293, 300, 291, 2809, 380, 445, 1466, 257, 1379, 51130], "temperature": 0.0, "avg_logprob": -0.14800644846795832, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.015587644651532173}, {"id": 718, "seek": 310888, "start": 3124.2000000000003, "end": 3126.76, "text": " bunch of representations from scratch.", "tokens": [51130, 3840, 295, 33358, 490, 8459, 13, 51258], "temperature": 0.0, "avg_logprob": -0.14800644846795832, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.015587644651532173}, {"id": 719, "seek": 310888, "start": 3126.76, "end": 3131.8, "text": " There was this innate framework and there was a little bit of tuning of this innate knowledge", "tokens": [51258, 821, 390, 341, 41766, 8388, 293, 456, 390, 257, 707, 857, 295, 15164, 295, 341, 41766, 3601, 51510], "temperature": 0.0, "avg_logprob": -0.14800644846795832, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.015587644651532173}, {"id": 720, "seek": 310888, "start": 3131.8, "end": 3136.92, "text": " and that's what learning was and that's just, I think that's a completely wrong headed approach.", "tokens": [51510, 293, 300, 311, 437, 2539, 390, 293, 300, 311, 445, 11, 286, 519, 300, 311, 257, 2584, 2085, 12798, 3109, 13, 51766], "temperature": 0.0, "avg_logprob": -0.14800644846795832, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.015587644651532173}, {"id": 721, "seek": 313692, "start": 3136.92, "end": 3143.64, "text": " In fact, I want to go the other way and I want to say the stuff in the brain that's innate", "tokens": [50364, 682, 1186, 11, 286, 528, 281, 352, 264, 661, 636, 293, 286, 528, 281, 584, 264, 1507, 294, 264, 3567, 300, 311, 41766, 50700], "temperature": 0.0, "avg_logprob": -0.2165440862829035, "compression_ratio": 1.6735751295336787, "no_speech_prob": 0.007415079977363348}, {"id": 722, "seek": 313692, "start": 3143.64, "end": 3146.32, "text": " wasn't discovered by evolution.", "tokens": [50700, 2067, 380, 6941, 538, 9303, 13, 50834], "temperature": 0.0, "avg_logprob": -0.2165440862829035, "compression_ratio": 1.6735751295336787, "no_speech_prob": 0.007415079977363348}, {"id": 723, "seek": 313692, "start": 3146.32, "end": 3151.96, "text": " The stuff in the brain that's innate was discovered by learning.", "tokens": [50834, 440, 1507, 294, 264, 3567, 300, 311, 41766, 390, 6941, 538, 2539, 13, 51116], "temperature": 0.0, "avg_logprob": -0.2165440862829035, "compression_ratio": 1.6735751295336787, "no_speech_prob": 0.007415079977363348}, {"id": 724, "seek": 313692, "start": 3151.96, "end": 3154.6, "text": " Do I have time to do that digression?", "tokens": [51116, 1144, 286, 362, 565, 281, 360, 300, 2528, 2775, 30, 51248], "temperature": 0.0, "avg_logprob": -0.2165440862829035, "compression_ratio": 1.6735751295336787, "no_speech_prob": 0.007415079977363348}, {"id": 725, "seek": 313692, "start": 3154.6, "end": 3155.6, "text": " Yeah.", "tokens": [51248, 865, 13, 51298], "temperature": 0.0, "avg_logprob": -0.2165440862829035, "compression_ratio": 1.6735751295336787, "no_speech_prob": 0.007415079977363348}, {"id": 726, "seek": 313692, "start": 3155.6, "end": 3166.04, "text": " Okay, so imagine we have a little neural network and it's got 20 connections in it and each", "tokens": [51298, 1033, 11, 370, 3811, 321, 362, 257, 707, 18161, 3209, 293, 309, 311, 658, 945, 9271, 294, 309, 293, 1184, 51820], "temperature": 0.0, "avg_logprob": -0.2165440862829035, "compression_ratio": 1.6735751295336787, "no_speech_prob": 0.007415079977363348}, {"id": 727, "seek": 316604, "start": 3166.04, "end": 3170.0, "text": " of those connections has a switch that could be on or off.", "tokens": [50364, 295, 729, 9271, 575, 257, 3679, 300, 727, 312, 322, 420, 766, 13, 50562], "temperature": 0.0, "avg_logprob": -0.14715484619140626, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.008126402273774147}, {"id": 728, "seek": 316604, "start": 3170.0, "end": 3172.56, "text": " So it can let stuff through or not.", "tokens": [50562, 407, 309, 393, 718, 1507, 807, 420, 406, 13, 50690], "temperature": 0.0, "avg_logprob": -0.14715484619140626, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.008126402273774147}, {"id": 729, "seek": 316604, "start": 3172.56, "end": 3174.8, "text": " So you've got to make 20 binary decisions.", "tokens": [50690, 407, 291, 600, 658, 281, 652, 945, 17434, 5327, 13, 50802], "temperature": 0.0, "avg_logprob": -0.14715484619140626, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.008126402273774147}, {"id": 730, "seek": 316604, "start": 3174.8, "end": 3179.64, "text": " So your chance of making them by chance is one in a million and making the correct decisions.", "tokens": [50802, 407, 428, 2931, 295, 1455, 552, 538, 2931, 307, 472, 294, 257, 2459, 293, 1455, 264, 3006, 5327, 13, 51044], "temperature": 0.0, "avg_logprob": -0.14715484619140626, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.008126402273774147}, {"id": 731, "seek": 316604, "start": 3179.64, "end": 3186.04, "text": " Now this little neural network circuit is a mating circuit and so the neural net goes", "tokens": [51044, 823, 341, 707, 18161, 3209, 9048, 307, 257, 49955, 9048, 293, 370, 264, 18161, 2533, 1709, 51364], "temperature": 0.0, "avg_logprob": -0.14715484619140626, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.008126402273774147}, {"id": 732, "seek": 316604, "start": 3186.04, "end": 3192.92, "text": " into a singles bar and it runs this circuit and if it's got the connections right it has", "tokens": [51364, 666, 257, 36334, 2159, 293, 309, 6676, 341, 9048, 293, 498, 309, 311, 658, 264, 9271, 558, 309, 575, 51708], "temperature": 0.0, "avg_logprob": -0.14715484619140626, "compression_ratio": 1.780701754385965, "no_speech_prob": 0.008126402273774147}, {"id": 733, "seek": 319292, "start": 3192.92, "end": 3197.6800000000003, "text": " lots of offspring and if it hasn't got the connections right it doesn't have offspring", "tokens": [50364, 3195, 295, 36857, 293, 498, 309, 6132, 380, 658, 264, 9271, 558, 309, 1177, 380, 362, 36857, 50602], "temperature": 0.0, "avg_logprob": -0.16393553217252096, "compression_ratio": 1.7510917030567685, "no_speech_prob": 0.004752825014293194}, {"id": 734, "seek": 319292, "start": 3197.6800000000003, "end": 3200.4, "text": " or doesn't have so many offspring.", "tokens": [50602, 420, 1177, 380, 362, 370, 867, 36857, 13, 50738], "temperature": 0.0, "avg_logprob": -0.16393553217252096, "compression_ratio": 1.7510917030567685, "no_speech_prob": 0.004752825014293194}, {"id": 735, "seek": 319292, "start": 3200.4, "end": 3207.48, "text": " Okay, so let's start off with the connections being, if they were just kind of random and", "tokens": [50738, 1033, 11, 370, 718, 311, 722, 766, 365, 264, 9271, 885, 11, 498, 436, 645, 445, 733, 295, 4974, 293, 51092], "temperature": 0.0, "avg_logprob": -0.16393553217252096, "compression_ratio": 1.7510917030567685, "no_speech_prob": 0.004752825014293194}, {"id": 736, "seek": 319292, "start": 3207.48, "end": 3212.64, "text": " you did mutation, what would happen is you'd have to build about a million organisms before", "tokens": [51092, 291, 630, 27960, 11, 437, 576, 1051, 307, 291, 1116, 362, 281, 1322, 466, 257, 2459, 22110, 949, 51350], "temperature": 0.0, "avg_logprob": -0.16393553217252096, "compression_ratio": 1.7510917030567685, "no_speech_prob": 0.004752825014293194}, {"id": 737, "seek": 319292, "start": 3212.64, "end": 3217.6800000000003, "text": " you got a good one and if you had sexual combination of the organisms, let's have a really simple", "tokens": [51350, 291, 658, 257, 665, 472, 293, 498, 291, 632, 6701, 6562, 295, 264, 22110, 11, 718, 311, 362, 257, 534, 2199, 51602], "temperature": 0.0, "avg_logprob": -0.16393553217252096, "compression_ratio": 1.7510917030567685, "no_speech_prob": 0.004752825014293194}, {"id": 738, "seek": 321768, "start": 3217.68, "end": 3223.08, "text": " biology in which each connection has its own gene and this gene has two alleles for", "tokens": [50364, 14956, 294, 597, 1184, 4984, 575, 1080, 1065, 12186, 293, 341, 12186, 575, 732, 439, 6972, 337, 50634], "temperature": 0.0, "avg_logprob": -0.12856272414878564, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.06944076716899872}, {"id": 739, "seek": 321768, "start": 3223.08, "end": 3225.96, "text": " on and off, okay?", "tokens": [50634, 322, 293, 766, 11, 1392, 30, 50778], "temperature": 0.0, "avg_logprob": -0.12856272414878564, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.06944076716899872}, {"id": 740, "seek": 321768, "start": 3225.96, "end": 3232.16, "text": " If you do mating now, you might have an organism that got all 20 connections right and it mates", "tokens": [50778, 759, 291, 360, 49955, 586, 11, 291, 1062, 362, 364, 24128, 300, 658, 439, 945, 9271, 558, 293, 309, 31488, 51088], "temperature": 0.0, "avg_logprob": -0.12856272414878564, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.06944076716899872}, {"id": 741, "seek": 321768, "start": 3232.16, "end": 3235.8399999999997, "text": " with one that has a few wrong and it gets a few wrong ones and now it's wiped out.", "tokens": [51088, 365, 472, 300, 575, 257, 1326, 2085, 293, 309, 2170, 257, 1326, 2085, 2306, 293, 586, 309, 311, 26879, 484, 13, 51272], "temperature": 0.0, "avg_logprob": -0.12856272414878564, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.06944076716899872}, {"id": 742, "seek": 321768, "start": 3235.8399999999997, "end": 3238.9199999999996, "text": " It doesn't have lots of offspring anymore, that's it.", "tokens": [51272, 467, 1177, 380, 362, 3195, 295, 36857, 3602, 11, 300, 311, 309, 13, 51426], "temperature": 0.0, "avg_logprob": -0.12856272414878564, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.06944076716899872}, {"id": 743, "seek": 321768, "start": 3238.9199999999996, "end": 3246.08, "text": " So it seems like a complete disaster and it would obviously take you at least a million", "tokens": [51426, 407, 309, 2544, 411, 257, 3566, 11293, 293, 309, 576, 2745, 747, 291, 412, 1935, 257, 2459, 51784], "temperature": 0.0, "avg_logprob": -0.12856272414878564, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.06944076716899872}, {"id": 744, "seek": 324608, "start": 3246.08, "end": 3250.2, "text": " organisms to expect to get a good one even if you have pathogenesis where you didn't", "tokens": [50364, 22110, 281, 2066, 281, 483, 257, 665, 472, 754, 498, 291, 362, 3100, 8799, 9374, 689, 291, 994, 380, 50570], "temperature": 0.0, "avg_logprob": -0.16220468155881193, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.09889679402112961}, {"id": 745, "seek": 324608, "start": 3250.2, "end": 3252.2, "text": " have sexual reproduction.", "tokens": [50570, 362, 6701, 33934, 13, 50670], "temperature": 0.0, "avg_logprob": -0.16220468155881193, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.09889679402112961}, {"id": 746, "seek": 324608, "start": 3252.2, "end": 3258.56, "text": " Now I will show you how to build a good organism in only 30,000 tries and the way you do it", "tokens": [50670, 823, 286, 486, 855, 291, 577, 281, 1322, 257, 665, 24128, 294, 787, 2217, 11, 1360, 9898, 293, 264, 636, 291, 360, 309, 50988], "temperature": 0.0, "avg_logprob": -0.16220468155881193, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.09889679402112961}, {"id": 747, "seek": 324608, "start": 3258.56, "end": 3264.92, "text": " is this, you, for each connection you have three alleles, you have turn the connection", "tokens": [50988, 307, 341, 11, 291, 11, 337, 1184, 4984, 291, 362, 1045, 439, 6972, 11, 291, 362, 1261, 264, 4984, 51306], "temperature": 0.0, "avg_logprob": -0.16220468155881193, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.09889679402112961}, {"id": 748, "seek": 324608, "start": 3264.92, "end": 3273.4, "text": " on genetically, turn the connection off genetically or leave the connection to learning, okay?", "tokens": [51306, 322, 37582, 11, 1261, 264, 4984, 766, 37582, 420, 1856, 264, 4984, 281, 2539, 11, 1392, 30, 51730], "temperature": 0.0, "avg_logprob": -0.16220468155881193, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.09889679402112961}, {"id": 749, "seek": 327340, "start": 3273.4, "end": 3275.76, "text": " So that's the third alley.", "tokens": [50364, 407, 300, 311, 264, 2636, 26660, 13, 50482], "temperature": 0.0, "avg_logprob": -0.2291259380302044, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.10366782546043396}, {"id": 750, "seek": 327340, "start": 3275.76, "end": 3283.2400000000002, "text": " And now you start off with a population in which about half of the connections are genetically", "tokens": [50482, 400, 586, 291, 722, 766, 365, 257, 4415, 294, 597, 466, 1922, 295, 264, 9271, 366, 37582, 50856], "temperature": 0.0, "avg_logprob": -0.2291259380302044, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.10366782546043396}, {"id": 751, "seek": 327340, "start": 3283.2400000000002, "end": 3288.36, "text": " determined and the other half are left to learning.", "tokens": [50856, 9540, 293, 264, 661, 1922, 366, 1411, 281, 2539, 13, 51112], "temperature": 0.0, "avg_logprob": -0.2291259380302044, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.10366782546043396}, {"id": 752, "seek": 327340, "start": 3288.36, "end": 3292.76, "text": " So that's 10 connections that are definitely determined, so there's a 1 in 1,000 chance", "tokens": [51112, 407, 300, 311, 1266, 9271, 300, 366, 2138, 9540, 11, 370, 456, 311, 257, 502, 294, 502, 11, 1360, 2931, 51332], "temperature": 0.0, "avg_logprob": -0.2291259380302044, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.10366782546043396}, {"id": 753, "seek": 327340, "start": 3292.76, "end": 3296.44, "text": " that you'll luck out genetically, you'll get those 10 right.", "tokens": [51332, 300, 291, 603, 3668, 484, 37582, 11, 291, 603, 483, 729, 1266, 558, 13, 51516], "temperature": 0.0, "avg_logprob": -0.2291259380302044, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.10366782546043396}, {"id": 754, "seek": 327340, "start": 3296.44, "end": 3300.08, "text": " And then during the organism's lifetime, let's have a really dumb learning algorithm where", "tokens": [51516, 400, 550, 1830, 264, 24128, 311, 11364, 11, 718, 311, 362, 257, 534, 10316, 2539, 9284, 689, 51698], "temperature": 0.0, "avg_logprob": -0.2291259380302044, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.10366782546043396}, {"id": 755, "seek": 330008, "start": 3300.08, "end": 3304.3199999999997, "text": " it just randomly fit like the one I talked about, it randomly fiddles with the connections,", "tokens": [50364, 309, 445, 16979, 3318, 411, 264, 472, 286, 2825, 466, 11, 309, 16979, 283, 14273, 904, 365, 264, 9271, 11, 50576], "temperature": 0.0, "avg_logprob": -0.2074598293859982, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.027357598766684532}, {"id": 756, "seek": 330008, "start": 3304.3199999999997, "end": 3308.44, "text": " just randomly flips connections of the 10 that are left to learning.", "tokens": [50576, 445, 16979, 40249, 9271, 295, 264, 1266, 300, 366, 1411, 281, 2539, 13, 50782], "temperature": 0.0, "avg_logprob": -0.2074598293859982, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.027357598766684532}, {"id": 757, "seek": 330008, "start": 3308.44, "end": 3312.96, "text": " And it'll take you to about 1,000 trials and it'll get the combination right.", "tokens": [50782, 400, 309, 603, 747, 291, 281, 466, 502, 11, 1360, 12450, 293, 309, 603, 483, 264, 6562, 558, 13, 51008], "temperature": 0.0, "avg_logprob": -0.2074598293859982, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.027357598766684532}, {"id": 758, "seek": 330008, "start": 3312.96, "end": 3317.44, "text": " But the point is it can do those trials without building a whole organism, it can just go", "tokens": [51008, 583, 264, 935, 307, 309, 393, 360, 729, 12450, 1553, 2390, 257, 1379, 24128, 11, 309, 393, 445, 352, 51232], "temperature": 0.0, "avg_logprob": -0.2074598293859982, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.027357598766684532}, {"id": 759, "seek": 330008, "start": 3317.44, "end": 3323.56, "text": " into the singles bar and sort of fiddle around a bit with its connections and it'll bang.", "tokens": [51232, 666, 264, 36334, 2159, 293, 1333, 295, 24553, 2285, 926, 257, 857, 365, 1080, 9271, 293, 309, 603, 8550, 13, 51538], "temperature": 0.0, "avg_logprob": -0.2074598293859982, "compression_ratio": 1.8017241379310345, "no_speech_prob": 0.027357598766684532}, {"id": 760, "seek": 332356, "start": 3323.56, "end": 3331.56, "text": " So what we've done is we've replaced a million trials of evolution, building a million organisms", "tokens": [50364, 407, 437, 321, 600, 1096, 307, 321, 600, 10772, 257, 2459, 12450, 295, 9303, 11, 2390, 257, 2459, 22110, 50764], "temperature": 0.0, "avg_logprob": -0.16366372325203635, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03064860962331295}, {"id": 761, "seek": 332356, "start": 3331.56, "end": 3338.2, "text": " with built 1,000 organisms and then let's build 30,000 organisms, just to be safe.", "tokens": [50764, 365, 3094, 502, 11, 1360, 22110, 293, 550, 718, 311, 1322, 2217, 11, 1360, 22110, 11, 445, 281, 312, 3273, 13, 51096], "temperature": 0.0, "avg_logprob": -0.16366372325203635, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03064860962331295}, {"id": 762, "seek": 332356, "start": 3338.2, "end": 3346.12, "text": " And then each of those fiddles around with its connections and it'll do this search.", "tokens": [51096, 400, 550, 1184, 295, 729, 283, 14273, 904, 926, 365, 1080, 9271, 293, 309, 603, 360, 341, 3164, 13, 51492], "temperature": 0.0, "avg_logprob": -0.16366372325203635, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03064860962331295}, {"id": 763, "seek": 332356, "start": 3346.12, "end": 3349.84, "text": " The whole search is the same, you have to try a million combinations, but the way you", "tokens": [51492, 440, 1379, 3164, 307, 264, 912, 11, 291, 362, 281, 853, 257, 2459, 21267, 11, 457, 264, 636, 291, 51678], "temperature": 0.0, "avg_logprob": -0.16366372325203635, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.03064860962331295}, {"id": 764, "seek": 334984, "start": 3349.84, "end": 3355.2000000000003, "text": " get the million combinations is 1,000 organisms each does 1,000 learning trials and so almost", "tokens": [50364, 483, 264, 2459, 21267, 307, 502, 11, 1360, 22110, 1184, 775, 502, 11, 1360, 2539, 12450, 293, 370, 1920, 50632], "temperature": 0.0, "avg_logprob": -0.22541052954537527, "compression_ratio": 1.6519607843137254, "no_speech_prob": 0.29985764622688293}, {"id": 765, "seek": 334984, "start": 3355.2000000000003, "end": 3357.92, "text": " all the work is done by learning.", "tokens": [50632, 439, 264, 589, 307, 1096, 538, 2539, 13, 50768], "temperature": 0.0, "avg_logprob": -0.22541052954537527, "compression_ratio": 1.6519607843137254, "no_speech_prob": 0.29985764622688293}, {"id": 766, "seek": 334984, "start": 3357.92, "end": 3365.08, "text": " Now if genetically an organism happens to have more things set, like it's got 12 of", "tokens": [50768, 823, 498, 37582, 364, 24128, 2314, 281, 362, 544, 721, 992, 11, 411, 309, 311, 658, 2272, 295, 51126], "temperature": 0.0, "avg_logprob": -0.22541052954537527, "compression_ratio": 1.6519607843137254, "no_speech_prob": 0.29985764622688293}, {"id": 767, "seek": 334984, "start": 3365.08, "end": 3368.1200000000003, "text": " them set right, it'll learn faster.", "tokens": [51126, 552, 992, 558, 11, 309, 603, 1466, 4663, 13, 51278], "temperature": 0.0, "avg_logprob": -0.22541052954537527, "compression_ratio": 1.6519607843137254, "no_speech_prob": 0.29985764622688293}, {"id": 768, "seek": 334984, "start": 3368.1200000000003, "end": 3375.7200000000003, "text": " And so this genetic pressure for, if you mate organisms now, this genetic pressure to get", "tokens": [51278, 400, 370, 341, 12462, 3321, 337, 11, 498, 291, 11709, 22110, 586, 11, 341, 12462, 3321, 281, 483, 51658], "temperature": 0.0, "avg_logprob": -0.22541052954537527, "compression_ratio": 1.6519607843137254, "no_speech_prob": 0.29985764622688293}, {"id": 769, "seek": 337572, "start": 3375.72, "end": 3379.4399999999996, "text": " more and more of these alleles set genetically.", "tokens": [50364, 544, 293, 544, 295, 613, 439, 6972, 992, 37582, 13, 50550], "temperature": 0.0, "avg_logprob": -0.1437996043715366, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.03212258964776993}, {"id": 770, "seek": 337572, "start": 3379.4399999999996, "end": 3385.0, "text": " But the pressure only comes because the learning can get all 20 set right so this thing can", "tokens": [50550, 583, 264, 3321, 787, 1487, 570, 264, 2539, 393, 483, 439, 945, 992, 558, 370, 341, 551, 393, 50828], "temperature": 0.0, "avg_logprob": -0.1437996043715366, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.03212258964776993}, {"id": 771, "seek": 337572, "start": 3385.0, "end": 3387.0, "text": " mate and have lots of offspring.", "tokens": [50828, 11709, 293, 362, 3195, 295, 36857, 13, 50928], "temperature": 0.0, "avg_logprob": -0.1437996043715366, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.03212258964776993}, {"id": 772, "seek": 337572, "start": 3387.0, "end": 3392.8399999999997, "text": " So the fact that the learning can find a solution creates genetic pressure to hardwire these", "tokens": [50928, 407, 264, 1186, 300, 264, 2539, 393, 915, 257, 3827, 7829, 12462, 3321, 281, 1152, 42689, 613, 51220], "temperature": 0.0, "avg_logprob": -0.1437996043715366, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.03212258964776993}, {"id": 773, "seek": 337572, "start": 3392.8399999999997, "end": 3394.8399999999997, "text": " things.", "tokens": [51220, 721, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1437996043715366, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.03212258964776993}, {"id": 774, "seek": 337572, "start": 3394.8399999999997, "end": 3401.3599999999997, "text": " So what's happening there is the search, a thousand things were done by evolution, a", "tokens": [51320, 407, 437, 311, 2737, 456, 307, 264, 3164, 11, 257, 4714, 721, 645, 1096, 538, 9303, 11, 257, 51646], "temperature": 0.0, "avg_logprob": -0.1437996043715366, "compression_ratio": 1.7047619047619047, "no_speech_prob": 0.03212258964776993}, {"id": 775, "seek": 340136, "start": 3401.36, "end": 3406.56, "text": " million minus a thousand things were done by learning and that created a landscape for", "tokens": [50364, 2459, 3175, 257, 4714, 721, 645, 1096, 538, 2539, 293, 300, 2942, 257, 9661, 337, 50624], "temperature": 0.0, "avg_logprob": -0.14756244200247307, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.047762151807546616}, {"id": 776, "seek": 340136, "start": 3406.56, "end": 3411.44, "text": " evolution that allowed evolution to gradually hardwire in more and more, that these things", "tokens": [50624, 9303, 300, 4350, 9303, 281, 13145, 1152, 42689, 294, 544, 293, 544, 11, 300, 613, 721, 50868], "temperature": 0.0, "avg_logprob": -0.14756244200247307, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.047762151807546616}, {"id": 777, "seek": 340136, "start": 3411.44, "end": 3414.48, "text": " were first found by learning.", "tokens": [50868, 645, 700, 1352, 538, 2539, 13, 51020], "temperature": 0.0, "avg_logprob": -0.14756244200247307, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.047762151807546616}, {"id": 778, "seek": 340136, "start": 3414.48, "end": 3422.1600000000003, "text": " So I think a lot of the structure in the brain that's hardwired is first found by learning", "tokens": [51020, 407, 286, 519, 257, 688, 295, 264, 3877, 294, 264, 3567, 300, 311, 1152, 86, 1824, 307, 700, 1352, 538, 2539, 51404], "temperature": 0.0, "avg_logprob": -0.14756244200247307, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.047762151807546616}, {"id": 779, "seek": 340136, "start": 3422.1600000000003, "end": 3425.56, "text": " and gradually it gets backed up into the hardwiring.", "tokens": [51404, 293, 13145, 309, 2170, 20391, 493, 666, 264, 1152, 86, 5057, 13, 51574], "temperature": 0.0, "avg_logprob": -0.14756244200247307, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.047762151807546616}, {"id": 780, "seek": 340136, "start": 3425.56, "end": 3429.04, "text": " But to get the evolutionary pressure to say that's good, you have to be able to do the", "tokens": [51574, 583, 281, 483, 264, 27567, 3321, 281, 584, 300, 311, 665, 11, 291, 362, 281, 312, 1075, 281, 360, 264, 51748], "temperature": 0.0, "avg_logprob": -0.14756244200247307, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.047762151807546616}, {"id": 781, "seek": 340136, "start": 3429.04, "end": 3430.04, "text": " learning.", "tokens": [51748, 2539, 13, 51798], "temperature": 0.0, "avg_logprob": -0.14756244200247307, "compression_ratio": 1.8983050847457628, "no_speech_prob": 0.047762151807546616}, {"id": 782, "seek": 343004, "start": 3430.08, "end": 3433.2799999999997, "text": " First hardwired things, you'd never find anything that was good.", "tokens": [50366, 2386, 1152, 86, 1824, 721, 11, 291, 1116, 1128, 915, 1340, 300, 390, 665, 13, 50526], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 783, "seek": 343004, "start": 3433.2799999999997, "end": 3434.2799999999997, "text": " Okay.", "tokens": [50526, 1033, 13, 50576], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 784, "seek": 343004, "start": 3434.2799999999997, "end": 3435.2799999999997, "text": " Great.", "tokens": [50576, 3769, 13, 50626], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 785, "seek": 343004, "start": 3435.2799999999997, "end": 3436.2799999999997, "text": " Thank you.", "tokens": [50626, 1044, 291, 13, 50676], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 786, "seek": 343004, "start": 3436.2799999999997, "end": 3437.2799999999997, "text": " Now...", "tokens": [50676, 823, 485, 50726], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 787, "seek": 343004, "start": 3437.2799999999997, "end": 3439.2, "text": " That's called the Baldwin effect, by the way.", "tokens": [50726, 663, 311, 1219, 264, 46050, 1802, 11, 538, 264, 636, 13, 50822], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 788, "seek": 343004, "start": 3439.2, "end": 3440.2, "text": " Yes.", "tokens": [50822, 1079, 13, 50872], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 789, "seek": 343004, "start": 3440.2, "end": 3446.88, "text": " Yeah, it's called after a psychology professor at the University of Toronto in the 1890s called", "tokens": [50872, 865, 11, 309, 311, 1219, 934, 257, 15105, 8304, 412, 264, 3535, 295, 14140, 294, 264, 47725, 82, 1219, 51206], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 790, "seek": 343004, "start": 3446.88, "end": 3449.52, "text": " Baldwin who invented this effect.", "tokens": [51206, 46050, 567, 14479, 341, 1802, 13, 51338], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 791, "seek": 343004, "start": 3449.52, "end": 3454.48, "text": " He didn't do any computer simulations though.", "tokens": [51338, 634, 994, 380, 360, 604, 3820, 35138, 1673, 13, 51586], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 792, "seek": 343004, "start": 3454.48, "end": 3458.96, "text": " So I want to do one follow-up question on that and then ask my final question before", "tokens": [51586, 407, 286, 528, 281, 360, 472, 1524, 12, 1010, 1168, 322, 300, 293, 550, 1029, 452, 2572, 1168, 949, 51810], "temperature": 0.0, "avg_logprob": -0.2842732814320347, "compression_ratio": 1.5653846153846154, "no_speech_prob": 0.054853182286024094}, {"id": 793, "seek": 345896, "start": 3458.96, "end": 3460.64, "text": " handing it to the audience.", "tokens": [50364, 34774, 309, 281, 264, 4034, 13, 50448], "temperature": 0.0, "avg_logprob": -0.12257922864427753, "compression_ratio": 1.65, "no_speech_prob": 0.004067258443683386}, {"id": 794, "seek": 345896, "start": 3460.64, "end": 3469.04, "text": " So my follow-up to that is, you know, I think one of the things that is unclear in terms", "tokens": [50448, 407, 452, 1524, 12, 1010, 281, 300, 307, 11, 291, 458, 11, 286, 519, 472, 295, 264, 721, 300, 307, 25636, 294, 2115, 50868], "temperature": 0.0, "avg_logprob": -0.12257922864427753, "compression_ratio": 1.65, "no_speech_prob": 0.004067258443683386}, {"id": 795, "seek": 345896, "start": 3469.04, "end": 3475.4, "text": " of the success of deep learning is exactly how much it was purely the compute or some", "tokens": [50868, 295, 264, 2245, 295, 2452, 2539, 307, 2293, 577, 709, 309, 390, 17491, 264, 14722, 420, 512, 51186], "temperature": 0.0, "avg_logprob": -0.12257922864427753, "compression_ratio": 1.65, "no_speech_prob": 0.004067258443683386}, {"id": 796, "seek": 345896, "start": 3475.4, "end": 3476.88, "text": " clever things.", "tokens": [51186, 13494, 721, 13, 51260], "temperature": 0.0, "avg_logprob": -0.12257922864427753, "compression_ratio": 1.65, "no_speech_prob": 0.004067258443683386}, {"id": 797, "seek": 345896, "start": 3476.88, "end": 3482.92, "text": " Now I've seen both cases argued and you today kind of suggested that it was just the compute.", "tokens": [51260, 823, 286, 600, 1612, 1293, 3331, 20219, 293, 291, 965, 733, 295, 10945, 300, 309, 390, 445, 264, 14722, 13, 51562], "temperature": 0.0, "avg_logprob": -0.12257922864427753, "compression_ratio": 1.65, "no_speech_prob": 0.004067258443683386}, {"id": 798, "seek": 345896, "start": 3482.92, "end": 3487.84, "text": " But I want to ask you about this following on your last point, which is that we know", "tokens": [51562, 583, 286, 528, 281, 1029, 291, 466, 341, 3480, 322, 428, 1036, 935, 11, 597, 307, 300, 321, 458, 51808], "temperature": 0.0, "avg_logprob": -0.12257922864427753, "compression_ratio": 1.65, "no_speech_prob": 0.004067258443683386}, {"id": 799, "seek": 348784, "start": 3488.08, "end": 3492.28, "text": " that if you build networks with particular architectures and with particular learning", "tokens": [50376, 300, 498, 291, 1322, 9590, 365, 1729, 6331, 1303, 293, 365, 1729, 2539, 50586], "temperature": 0.0, "avg_logprob": -0.15164094222219368, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.014696699567139149}, {"id": 800, "seek": 348784, "start": 3492.28, "end": 3498.44, "text": " rules, you are effectively making learning faster if you do it right.", "tokens": [50586, 4474, 11, 291, 366, 8659, 1455, 2539, 4663, 498, 291, 360, 309, 558, 13, 50894], "temperature": 0.0, "avg_logprob": -0.15164094222219368, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.014696699567139149}, {"id": 801, "seek": 348784, "start": 3498.44, "end": 3503.28, "text": " And arguably a lot of the success of deep learning has actually been as a result of", "tokens": [50894, 400, 26771, 257, 688, 295, 264, 2245, 295, 2452, 2539, 575, 767, 668, 382, 257, 1874, 295, 51136], "temperature": 0.0, "avg_logprob": -0.15164094222219368, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.014696699567139149}, {"id": 802, "seek": 348784, "start": 3503.28, "end": 3508.56, "text": " people thinking about good designs for their networks and good ways of making learning", "tokens": [51136, 561, 1953, 466, 665, 11347, 337, 641, 9590, 293, 665, 2098, 295, 1455, 2539, 51400], "temperature": 0.0, "avg_logprob": -0.15164094222219368, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.014696699567139149}, {"id": 803, "seek": 348784, "start": 3508.56, "end": 3509.56, "text": " faster.", "tokens": [51400, 4663, 13, 51450], "temperature": 0.0, "avg_logprob": -0.15164094222219368, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.014696699567139149}, {"id": 804, "seek": 348784, "start": 3509.56, "end": 3510.56, "text": " Yep.", "tokens": [51450, 7010, 13, 51500], "temperature": 0.0, "avg_logprob": -0.15164094222219368, "compression_ratio": 1.7384615384615385, "no_speech_prob": 0.014696699567139149}, {"id": 805, "seek": 351056, "start": 3510.56, "end": 3518.88, "text": " So would you potentially say that we have seen that process that you just described", "tokens": [50364, 407, 576, 291, 7263, 584, 300, 321, 362, 1612, 300, 1399, 300, 291, 445, 7619, 50780], "temperature": 0.0, "avg_logprob": -0.2950677153884724, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.009095143526792526}, {"id": 806, "seek": 351056, "start": 3518.88, "end": 3523.96, "text": " actually occur with NAI over the last ten years of the learning kind of backing up into", "tokens": [50780, 767, 5160, 365, 16585, 40, 670, 264, 1036, 2064, 924, 295, 264, 2539, 733, 295, 19373, 493, 666, 51034], "temperature": 0.0, "avg_logprob": -0.2950677153884724, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.009095143526792526}, {"id": 807, "seek": 351056, "start": 3523.96, "end": 3525.68, "text": " the hardwiring?", "tokens": [51034, 264, 1152, 86, 5057, 30, 51120], "temperature": 0.0, "avg_logprob": -0.2950677153884724, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.009095143526792526}, {"id": 808, "seek": 351056, "start": 3525.68, "end": 3529.2, "text": " I see.", "tokens": [51120, 286, 536, 13, 51296], "temperature": 0.0, "avg_logprob": -0.2950677153884724, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.009095143526792526}, {"id": 809, "seek": 351056, "start": 3529.2, "end": 3531.2799999999997, "text": " I need to think about that.", "tokens": [51296, 286, 643, 281, 519, 466, 300, 13, 51400], "temperature": 0.0, "avg_logprob": -0.2950677153884724, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.009095143526792526}, {"id": 810, "seek": 351056, "start": 3531.2799999999997, "end": 3534.52, "text": " What we've seen, I mean, Jan Lecaun invented convolutional neural nets.", "tokens": [51400, 708, 321, 600, 1612, 11, 286, 914, 11, 4956, 1456, 496, 409, 14479, 45216, 304, 18161, 36170, 13, 51562], "temperature": 0.0, "avg_logprob": -0.2950677153884724, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.009095143526792526}, {"id": 811, "seek": 351056, "start": 3534.52, "end": 3535.52, "text": " Right.", "tokens": [51562, 1779, 13, 51612], "temperature": 0.0, "avg_logprob": -0.2950677153884724, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.009095143526792526}, {"id": 812, "seek": 351056, "start": 3535.52, "end": 3536.52, "text": " Yes.", "tokens": [51612, 1079, 13, 51662], "temperature": 0.0, "avg_logprob": -0.2950677153884724, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.009095143526792526}, {"id": 813, "seek": 351056, "start": 3536.52, "end": 3537.52, "text": " For example.", "tokens": [51662, 1171, 1365, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2950677153884724, "compression_ratio": 1.4906542056074767, "no_speech_prob": 0.009095143526792526}, {"id": 814, "seek": 353752, "start": 3537.52, "end": 3540.24, "text": " So the computer was invented in the 1980s, but computers weren't fast enough to really", "tokens": [50364, 407, 264, 3820, 390, 14479, 294, 264, 13626, 82, 11, 457, 10807, 4999, 380, 2370, 1547, 281, 534, 50500], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 815, "seek": 353752, "start": 3540.24, "end": 3541.68, "text": " do a lot with them.", "tokens": [50500, 360, 257, 688, 365, 552, 13, 50572], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 816, "seek": 353752, "start": 3541.68, "end": 3544.88, "text": " So they were used for handwriting recognition and they were used for reading 10% of all the", "tokens": [50572, 407, 436, 645, 1143, 337, 39179, 11150, 293, 436, 645, 1143, 337, 3760, 1266, 4, 295, 439, 264, 50732], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 817, "seek": 353752, "start": 3544.88, "end": 3546.6, "text": " checks in America.", "tokens": [50732, 13834, 294, 3374, 13, 50818], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 818, "seek": 353752, "start": 3546.6, "end": 3549.12, "text": " But they didn't really take off.", "tokens": [50818, 583, 436, 994, 380, 534, 747, 766, 13, 50944], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 819, "seek": 353752, "start": 3549.12, "end": 3554.08, "text": " They really took off when the computer hardware came along to make them really efficient.", "tokens": [50944, 814, 534, 1890, 766, 562, 264, 3820, 8837, 1361, 2051, 281, 652, 552, 534, 7148, 13, 51192], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 820, "seek": 353752, "start": 3554.08, "end": 3558.72, "text": " So that's a case where the ideas were had first, but without the hardware they didn't", "tokens": [51192, 407, 300, 311, 257, 1389, 689, 264, 3487, 645, 632, 700, 11, 457, 1553, 264, 8837, 436, 994, 380, 51424], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 821, "seek": 353752, "start": 3558.72, "end": 3559.72, "text": " work.", "tokens": [51424, 589, 13, 51474], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 822, "seek": 353752, "start": 3559.72, "end": 3560.72, "text": " You've obviously got to have both.", "tokens": [51474, 509, 600, 2745, 658, 281, 362, 1293, 13, 51524], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 823, "seek": 353752, "start": 3560.72, "end": 3561.72, "text": " Right.", "tokens": [51524, 1779, 13, 51574], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 824, "seek": 353752, "start": 3561.72, "end": 3562.72, "text": " Yes.", "tokens": [51574, 1079, 13, 51624], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 825, "seek": 353752, "start": 3562.72, "end": 3563.72, "text": " Great.", "tokens": [51624, 3769, 13, 51674], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 826, "seek": 353752, "start": 3563.72, "end": 3564.72, "text": " Okay.", "tokens": [51674, 1033, 13, 51724], "temperature": 0.2, "avg_logprob": -0.2795076158311632, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.23380643129348755}, {"id": 827, "seek": 356472, "start": 3565.0, "end": 3569.16, "text": " So now my last question before I hand it to the audience is just, what do you see as", "tokens": [50378, 407, 586, 452, 1036, 1168, 949, 286, 1011, 309, 281, 264, 4034, 307, 445, 11, 437, 360, 291, 536, 382, 50586], "temperature": 0.0, "avg_logprob": -0.1576721225164633, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.005983298644423485}, {"id": 828, "seek": 356472, "start": 3569.16, "end": 3573.9199999999996, "text": " being the future of the interaction between neuroscience and AI?", "tokens": [50586, 885, 264, 2027, 295, 264, 9285, 1296, 42762, 293, 7318, 30, 50824], "temperature": 0.0, "avg_logprob": -0.1576721225164633, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.005983298644423485}, {"id": 829, "seek": 356472, "start": 3573.9199999999996, "end": 3579.68, "text": " Do you think that there is space for a sort of new cognitive science where we study general", "tokens": [50824, 1144, 291, 519, 300, 456, 307, 1901, 337, 257, 1333, 295, 777, 15605, 3497, 689, 321, 2979, 2674, 51112], "temperature": 0.0, "avg_logprob": -0.1576721225164633, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.005983298644423485}, {"id": 830, "seek": 356472, "start": 3579.68, "end": 3586.56, "text": " intelligence, but with brain-centric models rather than logic-based models?", "tokens": [51112, 7599, 11, 457, 365, 3567, 12, 45300, 5245, 2831, 813, 9952, 12, 6032, 5245, 30, 51456], "temperature": 0.0, "avg_logprob": -0.1576721225164633, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.005983298644423485}, {"id": 831, "seek": 356472, "start": 3586.56, "end": 3589.8799999999997, "text": " Or will we see the two streams depart over the next few decades?", "tokens": [51456, 1610, 486, 321, 536, 264, 732, 15842, 9110, 670, 264, 958, 1326, 7878, 30, 51622], "temperature": 0.0, "avg_logprob": -0.1576721225164633, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.005983298644423485}, {"id": 832, "seek": 356472, "start": 3589.8799999999997, "end": 3592.9599999999996, "text": " The way I like to think of it is we'd like to understand, if you'd like to understand", "tokens": [51622, 440, 636, 286, 411, 281, 519, 295, 309, 307, 321, 1116, 411, 281, 1223, 11, 498, 291, 1116, 411, 281, 1223, 51776], "temperature": 0.0, "avg_logprob": -0.1576721225164633, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.005983298644423485}, {"id": 833, "seek": 359296, "start": 3593.0, "end": 3599.52, "text": " how the brain does computation, you've got brains in your computation.", "tokens": [50366, 577, 264, 3567, 775, 24903, 11, 291, 600, 658, 15442, 294, 428, 24903, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1637405222112482, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.024272816255688667}, {"id": 834, "seek": 359296, "start": 3599.52, "end": 3602.8, "text": " And they look, like you said, they look pretty different to begin with because there's many", "tokens": [50692, 400, 436, 574, 11, 411, 291, 848, 11, 436, 574, 1238, 819, 281, 1841, 365, 570, 456, 311, 867, 50856], "temperature": 0.0, "avg_logprob": -0.1637405222112482, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.024272816255688667}, {"id": 835, "seek": 359296, "start": 3602.8, "end": 3605.0, "text": " different ways to do computation.", "tokens": [50856, 819, 2098, 281, 360, 24903, 13, 50966], "temperature": 0.0, "avg_logprob": -0.1637405222112482, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.024272816255688667}, {"id": 836, "seek": 359296, "start": 3605.0, "end": 3610.68, "text": " And with a conventional digital computer, you can get it to pretend to be anything.", "tokens": [50966, 400, 365, 257, 16011, 4562, 3820, 11, 291, 393, 483, 309, 281, 11865, 281, 312, 1340, 13, 51250], "temperature": 0.0, "avg_logprob": -0.1637405222112482, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.024272816255688667}, {"id": 837, "seek": 359296, "start": 3610.68, "end": 3614.4, "text": " So we're getting it to pretend to be some other kind of computer, an artificial neural", "tokens": [51250, 407, 321, 434, 1242, 309, 281, 11865, 281, 312, 512, 661, 733, 295, 3820, 11, 364, 11677, 18161, 51436], "temperature": 0.0, "avg_logprob": -0.1637405222112482, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.024272816255688667}, {"id": 838, "seek": 359296, "start": 3614.4, "end": 3615.8, "text": " net.", "tokens": [51436, 2533, 13, 51506], "temperature": 0.0, "avg_logprob": -0.1637405222112482, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.024272816255688667}, {"id": 839, "seek": 359296, "start": 3615.8, "end": 3621.68, "text": " And we'd like to sort of bridge this huge gap between brains and what we can simulate", "tokens": [51506, 400, 321, 1116, 411, 281, 1333, 295, 7283, 341, 2603, 7417, 1296, 15442, 293, 437, 321, 393, 27817, 51800], "temperature": 0.0, "avg_logprob": -0.1637405222112482, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.024272816255688667}, {"id": 840, "seek": 362168, "start": 3621.68, "end": 3623.52, "text": " on computers.", "tokens": [50364, 322, 10807, 13, 50456], "temperature": 0.0, "avg_logprob": -0.15020037564364347, "compression_ratio": 2.018181818181818, "no_speech_prob": 0.003277488285675645}, {"id": 841, "seek": 362168, "start": 3623.52, "end": 3628.12, "text": " And so neuroscientists are sort of doing experiments.", "tokens": [50456, 400, 370, 28813, 5412, 1751, 366, 1333, 295, 884, 12050, 13, 50686], "temperature": 0.0, "avg_logprob": -0.15020037564364347, "compression_ratio": 2.018181818181818, "no_speech_prob": 0.003277488285675645}, {"id": 842, "seek": 362168, "start": 3628.12, "end": 3632.52, "text": " And good computation neuroscientists are sort of doing experiments to try and sort of see", "tokens": [50686, 400, 665, 24903, 28813, 5412, 1751, 366, 1333, 295, 884, 12050, 281, 853, 293, 1333, 295, 536, 50906], "temperature": 0.0, "avg_logprob": -0.15020037564364347, "compression_ratio": 2.018181818181818, "no_speech_prob": 0.003277488285675645}, {"id": 843, "seek": 362168, "start": 3632.52, "end": 3634.8799999999997, "text": " how you could do the computation.", "tokens": [50906, 577, 291, 727, 360, 264, 24903, 13, 51024], "temperature": 0.0, "avg_logprob": -0.15020037564364347, "compression_ratio": 2.018181818181818, "no_speech_prob": 0.003277488285675645}, {"id": 844, "seek": 362168, "start": 3634.8799999999997, "end": 3640.44, "text": " And I think of myself at this end as doing, simulating things with artificial neural nets", "tokens": [51024, 400, 286, 519, 295, 2059, 412, 341, 917, 382, 884, 11, 1034, 12162, 721, 365, 11677, 18161, 36170, 51302], "temperature": 0.0, "avg_logprob": -0.15020037564364347, "compression_ratio": 2.018181818181818, "no_speech_prob": 0.003277488285675645}, {"id": 845, "seek": 362168, "start": 3640.44, "end": 3642.0, "text": " to see how you can make it more biological.", "tokens": [51302, 281, 536, 577, 291, 393, 652, 309, 544, 13910, 13, 51380], "temperature": 0.0, "avg_logprob": -0.15020037564364347, "compression_ratio": 2.018181818181818, "no_speech_prob": 0.003277488285675645}, {"id": 846, "seek": 362168, "start": 3642.0, "end": 3644.44, "text": " And we're trying to build a bridge.", "tokens": [51380, 400, 321, 434, 1382, 281, 1322, 257, 7283, 13, 51502], "temperature": 0.0, "avg_logprob": -0.15020037564364347, "compression_ratio": 2.018181818181818, "no_speech_prob": 0.003277488285675645}, {"id": 847, "seek": 362168, "start": 3644.44, "end": 3648.8399999999997, "text": " And so the computational neuroscientists, most of them are building from this end.", "tokens": [51502, 400, 370, 264, 28270, 28813, 5412, 1751, 11, 881, 295, 552, 366, 2390, 490, 341, 917, 13, 51722], "temperature": 0.0, "avg_logprob": -0.15020037564364347, "compression_ratio": 2.018181818181818, "no_speech_prob": 0.003277488285675645}, {"id": 848, "seek": 364884, "start": 3648.92, "end": 3650.56, "text": " I'm building from the other end.", "tokens": [50368, 286, 478, 2390, 490, 264, 661, 917, 13, 50450], "temperature": 0.0, "avg_logprob": -0.16834924352450634, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.0005217700963839889}, {"id": 849, "seek": 364884, "start": 3650.56, "end": 3653.32, "text": " But obviously, if you want to build a bridge to somewhere, you need to look at where you're", "tokens": [50450, 583, 2745, 11, 498, 291, 528, 281, 1322, 257, 7283, 281, 4079, 11, 291, 643, 281, 574, 412, 689, 291, 434, 50588], "temperature": 0.0, "avg_logprob": -0.16834924352450634, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.0005217700963839889}, {"id": 850, "seek": 364884, "start": 3653.32, "end": 3654.1600000000003, "text": " going.", "tokens": [50588, 516, 13, 50630], "temperature": 0.0, "avg_logprob": -0.16834924352450634, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.0005217700963839889}, {"id": 851, "seek": 364884, "start": 3654.1600000000003, "end": 3657.56, "text": " And so I'm trying to build a bridge that does computation more and more like the brain does", "tokens": [50630, 400, 370, 286, 478, 1382, 281, 1322, 257, 7283, 300, 775, 24903, 544, 293, 544, 411, 264, 3567, 775, 50800], "temperature": 0.0, "avg_logprob": -0.16834924352450634, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.0005217700963839889}, {"id": 852, "seek": 364884, "start": 3657.56, "end": 3662.52, "text": " it, or like, I guess the brain does it from what my neuroscientist friends tell me.", "tokens": [50800, 309, 11, 420, 411, 11, 286, 2041, 264, 3567, 775, 309, 490, 437, 452, 28813, 5412, 468, 1855, 980, 385, 13, 51048], "temperature": 0.0, "avg_logprob": -0.16834924352450634, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.0005217700963839889}, {"id": 853, "seek": 364884, "start": 3662.52, "end": 3665.36, "text": " And then there's conventional AI, which is trying to build a bridge like that.", "tokens": [51048, 400, 550, 456, 311, 16011, 7318, 11, 597, 307, 1382, 281, 1322, 257, 7283, 411, 300, 13, 51190], "temperature": 0.0, "avg_logprob": -0.16834924352450634, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.0005217700963839889}, {"id": 854, "seek": 364884, "start": 3665.36, "end": 3670.32, "text": " Great, OK, thank you.", "tokens": [51190, 3769, 11, 2264, 11, 1309, 291, 13, 51438], "temperature": 0.0, "avg_logprob": -0.16834924352450634, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.0005217700963839889}, {"id": 855, "seek": 364884, "start": 3670.32, "end": 3674.2000000000003, "text": " So now I'm going to open up to questions from the audience.", "tokens": [51438, 407, 586, 286, 478, 516, 281, 1269, 493, 281, 1651, 490, 264, 4034, 13, 51632], "temperature": 0.0, "avg_logprob": -0.16834924352450634, "compression_ratio": 1.779467680608365, "no_speech_prob": 0.0005217700963839889}, {"id": 856, "seek": 367420, "start": 3674.24, "end": 3679.12, "text": " Now, for this, we've got this kind of interesting system here.", "tokens": [50366, 823, 11, 337, 341, 11, 321, 600, 658, 341, 733, 295, 1880, 1185, 510, 13, 50610], "temperature": 0.0, "avg_logprob": -0.130733473259106, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.01183372177183628}, {"id": 857, "seek": 367420, "start": 3679.12, "end": 3686.0, "text": " So rather than you putting up your hands and me selecting you, you can actually nominate", "tokens": [50610, 407, 2831, 813, 291, 3372, 493, 428, 2377, 293, 385, 18182, 291, 11, 291, 393, 767, 5369, 13923, 50954], "temperature": 0.0, "avg_logprob": -0.130733473259106, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.01183372177183628}, {"id": 858, "seek": 367420, "start": 3686.0, "end": 3690.64, "text": " yourself to ask a question by pressing on the button on your microphone.", "tokens": [50954, 1803, 281, 1029, 257, 1168, 538, 12417, 322, 264, 2960, 322, 428, 10952, 13, 51186], "temperature": 0.0, "avg_logprob": -0.130733473259106, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.01183372177183628}, {"id": 859, "seek": 367420, "start": 3690.64, "end": 3692.8799999999997, "text": " And it is a first come, first serve basis.", "tokens": [51186, 400, 309, 307, 257, 700, 808, 11, 700, 4596, 5143, 13, 51298], "temperature": 0.0, "avg_logprob": -0.130733473259106, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.01183372177183628}, {"id": 860, "seek": 367420, "start": 3692.8799999999997, "end": 3695.24, "text": " So you're going to be queued up.", "tokens": [51298, 407, 291, 434, 516, 281, 312, 631, 5827, 493, 13, 51416], "temperature": 0.0, "avg_logprob": -0.130733473259106, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.01183372177183628}, {"id": 861, "seek": 367420, "start": 3695.24, "end": 3699.3199999999997, "text": " And so you're now first on the queue.", "tokens": [51416, 400, 370, 291, 434, 586, 700, 322, 264, 18639, 13, 51620], "temperature": 0.0, "avg_logprob": -0.130733473259106, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.01183372177183628}, {"id": 862, "seek": 367420, "start": 3699.3199999999997, "end": 3701.9199999999996, "text": " And by now, it's too late to be able to ask a question.", "tokens": [51620, 400, 538, 586, 11, 309, 311, 886, 3469, 281, 312, 1075, 281, 1029, 257, 1168, 13, 51750], "temperature": 0.0, "avg_logprob": -0.130733473259106, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.01183372177183628}, {"id": 863, "seek": 367420, "start": 3701.9199999999996, "end": 3702.7999999999997, "text": " Yes.", "tokens": [51750, 1079, 13, 51794], "temperature": 0.0, "avg_logprob": -0.130733473259106, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.01183372177183628}, {"id": 864, "seek": 370280, "start": 3702.8, "end": 3704.84, "text": " And one last thing about that, though.", "tokens": [50364, 400, 472, 1036, 551, 466, 300, 11, 1673, 13, 50466], "temperature": 0.0, "avg_logprob": -0.1826931854774212, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0007430627010762691}, {"id": 865, "seek": 370280, "start": 3704.84, "end": 3709.52, "text": " When you are done asking your question, please turn off your microphone,", "tokens": [50466, 1133, 291, 366, 1096, 3365, 428, 1168, 11, 1767, 1261, 766, 428, 10952, 11, 50700], "temperature": 0.0, "avg_logprob": -0.1826931854774212, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0007430627010762691}, {"id": 866, "seek": 370280, "start": 3709.52, "end": 3714.5600000000004, "text": " because that will open up this slot for the next person in the queue.", "tokens": [50700, 570, 300, 486, 1269, 493, 341, 14747, 337, 264, 958, 954, 294, 264, 18639, 13, 50952], "temperature": 0.0, "avg_logprob": -0.1826931854774212, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0007430627010762691}, {"id": 867, "seek": 370280, "start": 3714.5600000000004, "end": 3718.52, "text": " OK, go ahead.", "tokens": [50952, 2264, 11, 352, 2286, 13, 51150], "temperature": 0.0, "avg_logprob": -0.1826931854774212, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0007430627010762691}, {"id": 868, "seek": 370280, "start": 3718.52, "end": 3719.96, "text": " I've got a red light here, does that?", "tokens": [51150, 286, 600, 658, 257, 2182, 1442, 510, 11, 775, 300, 30, 51222], "temperature": 0.0, "avg_logprob": -0.1826931854774212, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0007430627010762691}, {"id": 869, "seek": 370280, "start": 3719.96, "end": 3722.76, "text": " Yeah, if your red light is flashing, that means you're on.", "tokens": [51222, 865, 11, 498, 428, 2182, 1442, 307, 31049, 11, 300, 1355, 291, 434, 322, 13, 51362], "temperature": 0.0, "avg_logprob": -0.1826931854774212, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0007430627010762691}, {"id": 870, "seek": 370280, "start": 3722.76, "end": 3723.76, "text": " You get to ask a question.", "tokens": [51362, 509, 483, 281, 1029, 257, 1168, 13, 51412], "temperature": 0.0, "avg_logprob": -0.1826931854774212, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0007430627010762691}, {"id": 871, "seek": 370280, "start": 3727.0, "end": 3731.1600000000003, "text": " If your red light is flashing, you're on.", "tokens": [51574, 759, 428, 2182, 1442, 307, 31049, 11, 291, 434, 322, 13, 51782], "temperature": 0.0, "avg_logprob": -0.1826931854774212, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0007430627010762691}, {"id": 872, "seek": 370280, "start": 3731.1600000000003, "end": 3732.52, "text": " I've got a solid light, please.", "tokens": [51782, 286, 600, 658, 257, 5100, 1442, 11, 1767, 13, 51850], "temperature": 0.0, "avg_logprob": -0.1826931854774212, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0007430627010762691}, {"id": 873, "seek": 373252, "start": 3732.56, "end": 3733.44, "text": " Oh, solid.", "tokens": [50366, 876, 11, 5100, 13, 50410], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 874, "seek": 373252, "start": 3733.44, "end": 3734.36, "text": " Sorry.", "tokens": [50410, 4919, 13, 50456], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 875, "seek": 373252, "start": 3734.36, "end": 3735.68, "text": " OK, I've got a solid light.", "tokens": [50456, 2264, 11, 286, 600, 658, 257, 5100, 1442, 13, 50522], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 876, "seek": 373252, "start": 3735.68, "end": 3737.0, "text": " You were faster.", "tokens": [50522, 509, 645, 4663, 13, 50588], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 877, "seek": 373252, "start": 3737.0, "end": 3738.32, "text": " OK.", "tokens": [50588, 2264, 13, 50654], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 878, "seek": 373252, "start": 3738.32, "end": 3742.88, "text": " So this is a Clifton suspension bridge analogy for your interest here.", "tokens": [50654, 407, 341, 307, 257, 2033, 351, 1756, 15771, 7283, 21663, 337, 428, 1179, 510, 13, 50882], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 879, "seek": 373252, "start": 3742.88, "end": 3746.08, "text": " So you mentioned briefly, Hebbian synapses.", "tokens": [50882, 407, 291, 2835, 10515, 11, 634, 6692, 952, 5451, 2382, 279, 13, 51042], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 880, "seek": 373252, "start": 3746.08, "end": 3749.84, "text": " As neuroscientists, we have a good understanding of how they work at a molecular level.", "tokens": [51042, 1018, 28813, 5412, 1751, 11, 321, 362, 257, 665, 3701, 295, 577, 436, 589, 412, 257, 19046, 1496, 13, 51230], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 881, "seek": 373252, "start": 3749.84, "end": 3754.64, "text": " So my question is, to what extent are the understanding of biological memory", "tokens": [51230, 407, 452, 1168, 307, 11, 281, 437, 8396, 366, 264, 3701, 295, 13910, 4675, 51470], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 882, "seek": 373252, "start": 3754.64, "end": 3759.04, "text": " mechanisms, i.e. Hebbian synapses, implemented by AI for deep learning", "tokens": [51470, 15902, 11, 741, 13, 68, 13, 634, 6692, 952, 5451, 2382, 279, 11, 12270, 538, 7318, 337, 2452, 2539, 51690], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 883, "seek": 373252, "start": 3759.04, "end": 3762.08, "text": " and the sorts of systems that you're describing?", "tokens": [51690, 293, 264, 7527, 295, 3652, 300, 291, 434, 16141, 30, 51842], "temperature": 0.0, "avg_logprob": -0.15060441885421524, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.0012466937769204378}, {"id": 884, "seek": 376208, "start": 3762.12, "end": 3769.52, "text": " So at present, people don't use Hebbian synapses for most deep learning.", "tokens": [50366, 407, 412, 1974, 11, 561, 500, 380, 764, 634, 6692, 952, 5451, 2382, 279, 337, 881, 2452, 2539, 13, 50736], "temperature": 0.0, "avg_logprob": -0.12609112952366347, "compression_ratio": 1.6641509433962265, "no_speech_prob": 0.0006214220193214715}, {"id": 885, "seek": 376208, "start": 3769.52, "end": 3771.36, "text": " They're using back propagation.", "tokens": [50736, 814, 434, 1228, 646, 38377, 13, 50828], "temperature": 0.0, "avg_logprob": -0.12609112952366347, "compression_ratio": 1.6641509433962265, "no_speech_prob": 0.0006214220193214715}, {"id": 886, "seek": 376208, "start": 3771.36, "end": 3776.0, "text": " So it's an error correction rule, as opposed to something where,", "tokens": [50828, 407, 309, 311, 364, 6713, 19984, 4978, 11, 382, 8851, 281, 746, 689, 11, 51060], "temperature": 0.0, "avg_logprob": -0.12609112952366347, "compression_ratio": 1.6641509433962265, "no_speech_prob": 0.0006214220193214715}, {"id": 887, "seek": 376208, "start": 3776.0, "end": 3779.12, "text": " if you just use it, it gets stronger.", "tokens": [51060, 498, 291, 445, 764, 309, 11, 309, 2170, 7249, 13, 51216], "temperature": 0.0, "avg_logprob": -0.12609112952366347, "compression_ratio": 1.6641509433962265, "no_speech_prob": 0.0006214220193214715}, {"id": 888, "seek": 376208, "start": 3779.12, "end": 3782.2, "text": " But if you want a short-term memory for things like transformers", "tokens": [51216, 583, 498, 291, 528, 257, 2099, 12, 7039, 4675, 337, 721, 411, 4088, 433, 51370], "temperature": 0.0, "avg_logprob": -0.12609112952366347, "compression_ratio": 1.6641509433962265, "no_speech_prob": 0.0006214220193214715}, {"id": 889, "seek": 376208, "start": 3782.2, "end": 3786.52, "text": " to remember a temporal context, just a simple Hebbian synapses is a good thing to have.", "tokens": [51370, 281, 1604, 257, 30881, 4319, 11, 445, 257, 2199, 634, 6692, 952, 5451, 2382, 279, 307, 257, 665, 551, 281, 362, 13, 51586], "temperature": 0.0, "avg_logprob": -0.12609112952366347, "compression_ratio": 1.6641509433962265, "no_speech_prob": 0.0006214220193214715}, {"id": 890, "seek": 376208, "start": 3786.52, "end": 3790.48, "text": " Yeah, but Hebbian synapses can code memories in humans that can last a lifetime.", "tokens": [51586, 865, 11, 457, 634, 6692, 952, 5451, 2382, 279, 393, 3089, 8495, 294, 6255, 300, 393, 1036, 257, 11364, 13, 51784], "temperature": 0.0, "avg_logprob": -0.12609112952366347, "compression_ratio": 1.6641509433962265, "no_speech_prob": 0.0006214220193214715}, {"id": 891, "seek": 379048, "start": 3790.48, "end": 3793.28, "text": " So is this something that AI is working towards using,", "tokens": [50364, 407, 307, 341, 746, 300, 7318, 307, 1364, 3030, 1228, 11, 50504], "temperature": 0.0, "avg_logprob": -0.17239315208347364, "compression_ratio": 1.5767441860465117, "no_speech_prob": 0.0005429993034340441}, {"id": 892, "seek": 379048, "start": 3793.28, "end": 3798.0, "text": " or are they just going to bypass Hebbian synapses and come up with something superior?", "tokens": [50504, 420, 366, 436, 445, 516, 281, 24996, 634, 6692, 952, 5451, 2382, 279, 293, 808, 493, 365, 746, 13028, 30, 50740], "temperature": 0.0, "avg_logprob": -0.17239315208347364, "compression_ratio": 1.5767441860465117, "no_speech_prob": 0.0005429993034340441}, {"id": 893, "seek": 379048, "start": 3798.0, "end": 3803.6, "text": " OK. So if you think about what's been successful in the last few years,", "tokens": [50740, 2264, 13, 407, 498, 291, 519, 466, 437, 311, 668, 4406, 294, 264, 1036, 1326, 924, 11, 51020], "temperature": 0.0, "avg_logprob": -0.17239315208347364, "compression_ratio": 1.5767441860465117, "no_speech_prob": 0.0005429993034340441}, {"id": 894, "seek": 379048, "start": 3803.6, "end": 3807.6, "text": " it's using error correction learning with either labeled data", "tokens": [51020, 309, 311, 1228, 6713, 19984, 2539, 365, 2139, 21335, 1412, 51220], "temperature": 0.0, "avg_logprob": -0.17239315208347364, "compression_ratio": 1.5767441860465117, "no_speech_prob": 0.0005429993034340441}, {"id": 895, "seek": 379048, "start": 3807.6, "end": 3813.2400000000002, "text": " or trying to predict what comes next, and not Hebbian synapses.", "tokens": [51220, 420, 1382, 281, 6069, 437, 1487, 958, 11, 293, 406, 634, 6692, 952, 5451, 2382, 279, 13, 51502], "temperature": 0.0, "avg_logprob": -0.17239315208347364, "compression_ratio": 1.5767441860465117, "no_speech_prob": 0.0005429993034340441}, {"id": 896, "seek": 381324, "start": 3813.68, "end": 3820.9599999999996, "text": " Now, people like me who sort of do this kind of learning", "tokens": [50386, 823, 11, 561, 411, 385, 567, 1333, 295, 360, 341, 733, 295, 2539, 50750], "temperature": 0.0, "avg_logprob": -0.18050883213678995, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.03163912519812584}, {"id": 897, "seek": 381324, "start": 3820.9599999999996, "end": 3824.2, "text": " but are interested in the brain know this isn't right.", "tokens": [50750, 457, 366, 3102, 294, 264, 3567, 458, 341, 1943, 380, 558, 13, 50912], "temperature": 0.0, "avg_logprob": -0.18050883213678995, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.03163912519812584}, {"id": 898, "seek": 381324, "start": 3824.2, "end": 3826.3999999999996, "text": " We're much more interested in unsupervised learning.", "tokens": [50912, 492, 434, 709, 544, 3102, 294, 2693, 12879, 24420, 2539, 13, 51022], "temperature": 0.0, "avg_logprob": -0.18050883213678995, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.03163912519812584}, {"id": 899, "seek": 381324, "start": 3826.3999999999996, "end": 3829.56, "text": " We just can't make it work very well yet.", "tokens": [51022, 492, 445, 393, 380, 652, 309, 589, 588, 731, 1939, 13, 51180], "temperature": 0.0, "avg_logprob": -0.18050883213678995, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.03163912519812584}, {"id": 900, "seek": 381324, "start": 3829.56, "end": 3835.72, "text": " And I would love to be able to get learning to work,", "tokens": [51180, 400, 286, 576, 959, 281, 312, 1075, 281, 483, 2539, 281, 589, 11, 51488], "temperature": 0.0, "avg_logprob": -0.18050883213678995, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.03163912519812584}, {"id": 901, "seek": 381324, "start": 3835.72, "end": 3838.6, "text": " as well as it does when you do back propagation,", "tokens": [51488, 382, 731, 382, 309, 775, 562, 291, 360, 646, 38377, 11, 51632], "temperature": 0.0, "avg_logprob": -0.18050883213678995, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.03163912519812584}, {"id": 902, "seek": 381324, "start": 3838.6, "end": 3841.4799999999996, "text": " without using biologically implausible things.", "tokens": [51632, 1553, 1228, 3228, 17157, 8484, 8463, 964, 721, 13, 51776], "temperature": 0.0, "avg_logprob": -0.18050883213678995, "compression_ratio": 1.5990990990990992, "no_speech_prob": 0.03163912519812584}, {"id": 903, "seek": 384148, "start": 3842.48, "end": 3846.88, "text": " And one place we can do that is with temporary memories.", "tokens": [50414, 400, 472, 1081, 321, 393, 360, 300, 307, 365, 13413, 8495, 13, 50634], "temperature": 0.0, "avg_logprob": -0.2777627905209859, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.0022267380263656378}, {"id": 904, "seek": 384148, "start": 3846.88, "end": 3850.12, "text": " So if you say synapses have a fast component,", "tokens": [50634, 407, 498, 291, 584, 5451, 2382, 279, 362, 257, 2370, 6542, 11, 50796], "temperature": 0.0, "avg_logprob": -0.2777627905209859, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.0022267380263656378}, {"id": 905, "seek": 384148, "start": 3850.12, "end": 3852.6, "text": " you can use Hebbian learning for that fast component,", "tokens": [50796, 291, 393, 764, 634, 6692, 952, 2539, 337, 300, 2370, 6542, 11, 50920], "temperature": 0.0, "avg_logprob": -0.2777627905209859, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.0022267380263656378}, {"id": 906, "seek": 384148, "start": 3852.6, "end": 3854.76, "text": " and that will actually help neural networks work better,", "tokens": [50920, 293, 300, 486, 767, 854, 18161, 9590, 589, 1101, 11, 51028], "temperature": 0.0, "avg_logprob": -0.2777627905209859, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.0022267380263656378}, {"id": 907, "seek": 384148, "start": 3854.76, "end": 3858.76, "text": " even if you're using back propagation for the slow component.", "tokens": [51028, 754, 498, 291, 434, 1228, 646, 38377, 337, 264, 2964, 6542, 13, 51228], "temperature": 0.0, "avg_logprob": -0.2777627905209859, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.0022267380263656378}, {"id": 908, "seek": 384148, "start": 3858.76, "end": 3861.76, "text": " That didn't really answer your question, but you know, it filled the time.", "tokens": [51228, 663, 994, 380, 534, 1867, 428, 1168, 11, 457, 291, 458, 11, 309, 6412, 264, 565, 13, 51378], "temperature": 0.0, "avg_logprob": -0.2777627905209859, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.0022267380263656378}, {"id": 909, "seek": 384148, "start": 3861.76, "end": 3865.56, "text": " LAUGHTER", "tokens": [51378, 46760, 51568], "temperature": 0.0, "avg_logprob": -0.2777627905209859, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.0022267380263656378}, {"id": 910, "seek": 384148, "start": 3865.56, "end": 3867.56, "text": " Hello.", "tokens": [51568, 2425, 13, 51668], "temperature": 0.0, "avg_logprob": -0.2777627905209859, "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.0022267380263656378}, {"id": 911, "seek": 386756, "start": 3868.56, "end": 3873.56, "text": " I read in the Reinforcement Learning Book that dopamine is used", "tokens": [50414, 286, 1401, 294, 264, 42116, 9382, 15205, 9476, 300, 37219, 307, 1143, 50664], "temperature": 0.0, "avg_logprob": -0.2686305273146856, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.014555193483829498}, {"id": 912, "seek": 386756, "start": 3873.56, "end": 3876.56, "text": " as a reward prediction error signal.", "tokens": [50664, 382, 257, 7782, 17630, 6713, 6358, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2686305273146856, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.014555193483829498}, {"id": 913, "seek": 386756, "start": 3876.56, "end": 3880.56, "text": " So I was wondering, do you see it used as a supervisory signal,", "tokens": [50814, 407, 286, 390, 6359, 11, 360, 291, 536, 309, 1143, 382, 257, 34409, 827, 6358, 11, 51014], "temperature": 0.0, "avg_logprob": -0.2686305273146856, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.014555193483829498}, {"id": 914, "seek": 386756, "start": 3880.56, "end": 3883.56, "text": " like you mentioned earlier?", "tokens": [51014, 411, 291, 2835, 3071, 30, 51164], "temperature": 0.0, "avg_logprob": -0.2686305273146856, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.014555193483829498}, {"id": 915, "seek": 386756, "start": 3883.56, "end": 3886.56, "text": " OK, so for reinforcement learning,", "tokens": [51164, 2264, 11, 370, 337, 29280, 2539, 11, 51314], "temperature": 0.0, "avg_logprob": -0.2686305273146856, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.014555193483829498}, {"id": 916, "seek": 386756, "start": 3886.56, "end": 3889.56, "text": " there is some lovely work done by Peter Diane,", "tokens": [51314, 456, 307, 512, 7496, 589, 1096, 538, 6508, 30460, 11, 51464], "temperature": 0.0, "avg_logprob": -0.2686305273146856, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.014555193483829498}, {"id": 917, "seek": 386756, "start": 3889.56, "end": 3893.56, "text": " who is the theoretician, and some experimentalists,", "tokens": [51464, 567, 307, 264, 14308, 9027, 11, 293, 512, 17069, 1751, 11, 51664], "temperature": 0.0, "avg_logprob": -0.2686305273146856, "compression_ratio": 1.5377358490566038, "no_speech_prob": 0.014555193483829498}, {"id": 918, "seek": 389356, "start": 3893.56, "end": 3896.56, "text": " showing that the real data from neuroscience", "tokens": [50364, 4099, 300, 264, 957, 1412, 490, 42762, 50514], "temperature": 0.0, "avg_logprob": -0.11283310338070518, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.008146356791257858}, {"id": 919, "seek": 389356, "start": 3896.56, "end": 3900.56, "text": " fits in with a theory that was started with Rich Sutton.", "tokens": [50514, 9001, 294, 365, 257, 5261, 300, 390, 1409, 365, 6781, 40492, 1756, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11283310338070518, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.008146356791257858}, {"id": 920, "seek": 389356, "start": 3900.56, "end": 3906.56, "text": " And Peter Diane did the work of showing that dopamine", "tokens": [50714, 400, 6508, 30460, 630, 264, 589, 295, 4099, 300, 37219, 51014], "temperature": 0.0, "avg_logprob": -0.11283310338070518, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.008146356791257858}, {"id": 921, "seek": 389356, "start": 3906.56, "end": 3909.56, "text": " corresponds to something in a particular learning algorithm.", "tokens": [51014, 23249, 281, 746, 294, 257, 1729, 2539, 9284, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11283310338070518, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.008146356791257858}, {"id": 922, "seek": 389356, "start": 3909.56, "end": 3912.56, "text": " And it doesn't correspond to the reward,", "tokens": [51164, 400, 309, 1177, 380, 6805, 281, 264, 7782, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11283310338070518, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.008146356791257858}, {"id": 923, "seek": 389356, "start": 3912.56, "end": 3916.56, "text": " it corresponds to the difference between the reward you're expecting", "tokens": [51314, 309, 23249, 281, 264, 2649, 1296, 264, 7782, 291, 434, 9650, 51514], "temperature": 0.0, "avg_logprob": -0.11283310338070518, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.008146356791257858}, {"id": 924, "seek": 389356, "start": 3916.56, "end": 3918.56, "text": " and the reward you get.", "tokens": [51514, 293, 264, 7782, 291, 483, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11283310338070518, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.008146356791257858}, {"id": 925, "seek": 389356, "start": 3918.56, "end": 3922.56, "text": " So if you're a monkey and you're expecting a grape", "tokens": [51614, 407, 498, 291, 434, 257, 17847, 293, 291, 434, 9650, 257, 23978, 51814], "temperature": 0.0, "avg_logprob": -0.11283310338070518, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.008146356791257858}, {"id": 926, "seek": 392256, "start": 3922.56, "end": 3926.56, "text": " and I give you a piece of cucumber, that's negative reward,", "tokens": [50364, 293, 286, 976, 291, 257, 2522, 295, 28725, 11, 300, 311, 3671, 7782, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07742484585269467, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.0007756347768008709}, {"id": 927, "seek": 392256, "start": 3926.56, "end": 3930.56, "text": " and that will be a big negative hit at the dopamine.", "tokens": [50564, 293, 300, 486, 312, 257, 955, 3671, 2045, 412, 264, 37219, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07742484585269467, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.0007756347768008709}, {"id": 928, "seek": 392256, "start": 3933.56, "end": 3938.56, "text": " So that's not the kind of learning that's been really successful so far.", "tokens": [50914, 407, 300, 311, 406, 264, 733, 295, 2539, 300, 311, 668, 534, 4406, 370, 1400, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07742484585269467, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.0007756347768008709}, {"id": 929, "seek": 392256, "start": 3938.56, "end": 3941.56, "text": " If you're willing to burn a lot of computer time,", "tokens": [51164, 759, 291, 434, 4950, 281, 5064, 257, 688, 295, 3820, 565, 11, 51314], "temperature": 0.0, "avg_logprob": -0.07742484585269467, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.0007756347768008709}, {"id": 930, "seek": 392256, "start": 3941.56, "end": 3945.56, "text": " Reinforcement Learning will solve some problems,", "tokens": [51314, 42116, 9382, 15205, 486, 5039, 512, 2740, 11, 51514], "temperature": 0.0, "avg_logprob": -0.07742484585269467, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.0007756347768008709}, {"id": 931, "seek": 392256, "start": 3945.56, "end": 3949.56, "text": " but it's not the kind of learning that's been most successful in AI.", "tokens": [51514, 457, 309, 311, 406, 264, 733, 295, 2539, 300, 311, 668, 881, 4406, 294, 7318, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07742484585269467, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.0007756347768008709}, {"id": 932, "seek": 395256, "start": 3952.56, "end": 3955.56, "text": " So the difference is in Reinforcement Learning,", "tokens": [50364, 407, 264, 2649, 307, 294, 42116, 9382, 15205, 11, 50514], "temperature": 0.0, "avg_logprob": -0.12546642459168725, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.0006075484561733902}, {"id": 933, "seek": 395256, "start": 3955.56, "end": 3958.56, "text": " you get a single scalar, you get one number,", "tokens": [50514, 291, 483, 257, 2167, 39684, 11, 291, 483, 472, 1230, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12546642459168725, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.0006075484561733902}, {"id": 934, "seek": 395256, "start": 3958.56, "end": 3960.56, "text": " whereas in error correction learning,", "tokens": [50664, 9735, 294, 6713, 19984, 2539, 11, 50764], "temperature": 0.0, "avg_logprob": -0.12546642459168725, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.0006075484561733902}, {"id": 935, "seek": 395256, "start": 3960.56, "end": 3963.56, "text": " you typically get a whole vector of numbers.", "tokens": [50764, 291, 5850, 483, 257, 1379, 8062, 295, 3547, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12546642459168725, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.0006075484561733902}, {"id": 936, "seek": 395256, "start": 3965.56, "end": 3967.56, "text": " Right here.", "tokens": [51014, 1779, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12546642459168725, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.0006075484561733902}, {"id": 937, "seek": 395256, "start": 3967.56, "end": 3972.56, "text": " So you mentioned that your goal, kind of the bridge analogy,", "tokens": [51114, 407, 291, 2835, 300, 428, 3387, 11, 733, 295, 264, 7283, 21663, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12546642459168725, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.0006075484561733902}, {"id": 938, "seek": 395256, "start": 3972.56, "end": 3976.56, "text": " is your goal is to go from computers and try to get to the brain.", "tokens": [51364, 307, 428, 3387, 307, 281, 352, 490, 10807, 293, 853, 281, 483, 281, 264, 3567, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12546642459168725, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.0006075484561733902}, {"id": 939, "seek": 395256, "start": 3976.56, "end": 3979.56, "text": " So let's just say that kind of makes sense to think,", "tokens": [51564, 407, 718, 311, 445, 584, 300, 733, 295, 1669, 2020, 281, 519, 11, 51714], "temperature": 0.0, "avg_logprob": -0.12546642459168725, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.0006075484561733902}, {"id": 940, "seek": 397956, "start": 3979.56, "end": 3981.56, "text": " okay, let's get to more general AI,", "tokens": [50364, 1392, 11, 718, 311, 483, 281, 544, 2674, 7318, 11, 50464], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 941, "seek": 397956, "start": 3981.56, "end": 3983.56, "text": " because I'd say humans are decently general.", "tokens": [50464, 570, 286, 1116, 584, 6255, 366, 979, 2276, 2674, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 942, "seek": 397956, "start": 3983.56, "end": 3986.56, "text": " And neuroscientists are trying to get from the other bridge,", "tokens": [50564, 400, 28813, 5412, 1751, 366, 1382, 281, 483, 490, 264, 661, 7283, 11, 50714], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 943, "seek": 397956, "start": 3986.56, "end": 3988.56, "text": " the brain, to generally AI.", "tokens": [50714, 264, 3567, 11, 281, 5101, 7318, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 944, "seek": 397956, "start": 3988.56, "end": 3992.56, "text": " So you have these two kind of debates,", "tokens": [50814, 407, 291, 362, 613, 732, 733, 295, 24203, 11, 51014], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 945, "seek": 397956, "start": 3992.56, "end": 3994.56, "text": " and this happens quite often,", "tokens": [51014, 293, 341, 2314, 1596, 2049, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 946, "seek": 397956, "start": 3994.56, "end": 3997.56, "text": " where is it correct to go from generally AI to the brain,", "tokens": [51114, 689, 307, 309, 3006, 281, 352, 490, 5101, 7318, 281, 264, 3567, 11, 51264], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 947, "seek": 397956, "start": 3997.56, "end": 3999.56, "text": " first understand generally AI, then understand the brain,", "tokens": [51264, 700, 1223, 5101, 7318, 11, 550, 1223, 264, 3567, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 948, "seek": 397956, "start": 3999.56, "end": 4001.56, "text": " or brain to generally AI.", "tokens": [51364, 420, 3567, 281, 5101, 7318, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 949, "seek": 397956, "start": 4001.56, "end": 4004.56, "text": " And so what would you say is the most practical way", "tokens": [51464, 400, 370, 437, 576, 291, 584, 307, 264, 881, 8496, 636, 51614], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 950, "seek": 397956, "start": 4004.56, "end": 4008.56, "text": " to problematize generally AI?", "tokens": [51614, 281, 1154, 267, 1125, 5101, 7318, 30, 51814], "temperature": 0.0, "avg_logprob": -0.11558264923095703, "compression_ratio": 1.8046875, "no_speech_prob": 0.0020758192986249924}, {"id": 951, "seek": 400956, "start": 4010.56, "end": 4013.56, "text": " I don't like the phrase general AI.", "tokens": [50414, 286, 500, 380, 411, 264, 9535, 2674, 7318, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08880868089308433, "compression_ratio": 1.9951923076923077, "no_speech_prob": 0.0015661584911867976}, {"id": 952, "seek": 400956, "start": 4013.56, "end": 4017.56, "text": " I don't think, if you want intelligent devices,", "tokens": [50564, 286, 500, 380, 519, 11, 498, 291, 528, 13232, 5759, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08880868089308433, "compression_ratio": 1.9951923076923077, "no_speech_prob": 0.0015661584911867976}, {"id": 953, "seek": 400956, "start": 4017.56, "end": 4022.56, "text": " I don't think you want to produce a sort of general purpose Android.", "tokens": [50764, 286, 500, 380, 519, 291, 528, 281, 5258, 257, 1333, 295, 2674, 4334, 8853, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08880868089308433, "compression_ratio": 1.9951923076923077, "no_speech_prob": 0.0015661584911867976}, {"id": 954, "seek": 400956, "start": 4022.56, "end": 4025.56, "text": " I think you want to produce different devices", "tokens": [51014, 286, 519, 291, 528, 281, 5258, 819, 5759, 51164], "temperature": 0.0, "avg_logprob": -0.08880868089308433, "compression_ratio": 1.9951923076923077, "no_speech_prob": 0.0015661584911867976}, {"id": 955, "seek": 400956, "start": 4025.56, "end": 4028.56, "text": " that are smart in different ways.", "tokens": [51164, 300, 366, 4069, 294, 819, 2098, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08880868089308433, "compression_ratio": 1.9951923076923077, "no_speech_prob": 0.0015661584911867976}, {"id": 956, "seek": 400956, "start": 4028.56, "end": 4031.56, "text": " So basically if you want intelligent machines that do things,", "tokens": [51314, 407, 1936, 498, 291, 528, 13232, 8379, 300, 360, 721, 11, 51464], "temperature": 0.0, "avg_logprob": -0.08880868089308433, "compression_ratio": 1.9951923076923077, "no_speech_prob": 0.0015661584911867976}, {"id": 957, "seek": 400956, "start": 4031.56, "end": 4034.56, "text": " you have a vacuum cleaner and you have a backhoe.", "tokens": [51464, 291, 362, 257, 14224, 16532, 293, 291, 362, 257, 646, 33810, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08880868089308433, "compression_ratio": 1.9951923076923077, "no_speech_prob": 0.0015661584911867976}, {"id": 958, "seek": 400956, "start": 4034.56, "end": 4038.56, "text": " You don't try to make one thing that's a vacuum cleaner and a backhoe.", "tokens": [51614, 509, 500, 380, 853, 281, 652, 472, 551, 300, 311, 257, 14224, 16532, 293, 257, 646, 33810, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08880868089308433, "compression_ratio": 1.9951923076923077, "no_speech_prob": 0.0015661584911867976}, {"id": 959, "seek": 403856, "start": 4038.56, "end": 4040.56, "text": " It doesn't make sense.", "tokens": [50364, 467, 1177, 380, 652, 2020, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11322884863995492, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0048018633387982845}, {"id": 960, "seek": 403856, "start": 4040.56, "end": 4042.56, "text": " What about connecting them through", "tokens": [50464, 708, 466, 11015, 552, 807, 50564], "temperature": 0.0, "avg_logprob": -0.11322884863995492, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0048018633387982845}, {"id": 961, "seek": 403856, "start": 4042.56, "end": 4046.56, "text": " kind of like different cognition areas in the brain?", "tokens": [50564, 733, 295, 411, 819, 46905, 3179, 294, 264, 3567, 30, 50764], "temperature": 0.0, "avg_logprob": -0.11322884863995492, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0048018633387982845}, {"id": 962, "seek": 403856, "start": 4046.56, "end": 4048.56, "text": " Yeah, but I think it's the same with cognition too.", "tokens": [50764, 865, 11, 457, 286, 519, 309, 311, 264, 912, 365, 46905, 886, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11322884863995492, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0048018633387982845}, {"id": 963, "seek": 403856, "start": 4048.56, "end": 4051.56, "text": " I think the neural net that does machine translation", "tokens": [50864, 286, 519, 264, 18161, 2533, 300, 775, 3479, 12853, 51014], "temperature": 0.0, "avg_logprob": -0.11322884863995492, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0048018633387982845}, {"id": 964, "seek": 403856, "start": 4051.56, "end": 4056.56, "text": " isn't the same neural net as does vision.", "tokens": [51014, 1943, 380, 264, 912, 18161, 2533, 382, 775, 5201, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11322884863995492, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0048018633387982845}, {"id": 965, "seek": 403856, "start": 4056.56, "end": 4062.56, "text": " I think, yeah, my guess is that people are thinking too much", "tokens": [51264, 286, 519, 11, 1338, 11, 452, 2041, 307, 300, 561, 366, 1953, 886, 709, 51564], "temperature": 0.0, "avg_logprob": -0.11322884863995492, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0048018633387982845}, {"id": 966, "seek": 403856, "start": 4062.56, "end": 4065.56, "text": " about making one neural net that does everything,", "tokens": [51564, 466, 1455, 472, 18161, 2533, 300, 775, 1203, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11322884863995492, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0048018633387982845}, {"id": 967, "seek": 406556, "start": 4065.56, "end": 4070.56, "text": " and not thinking enough about making more modular neural nets", "tokens": [50364, 293, 406, 1953, 1547, 466, 1455, 544, 31111, 18161, 36170, 50614], "temperature": 0.0, "avg_logprob": -0.0920292854309082, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0017979098483920097}, {"id": 968, "seek": 406556, "start": 4070.56, "end": 4072.56, "text": " that are good at different things.", "tokens": [50614, 300, 366, 665, 412, 819, 721, 13, 50714], "temperature": 0.0, "avg_logprob": -0.0920292854309082, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0017979098483920097}, {"id": 969, "seek": 406556, "start": 4072.56, "end": 4074.56, "text": " Some are more universal than others,", "tokens": [50714, 2188, 366, 544, 11455, 813, 2357, 11, 50814], "temperature": 0.0, "avg_logprob": -0.0920292854309082, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0017979098483920097}, {"id": 970, "seek": 406556, "start": 4074.56, "end": 4076.56, "text": " but I think that's how progress has been made.", "tokens": [50814, 457, 286, 519, 300, 311, 577, 4205, 575, 668, 1027, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0920292854309082, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0017979098483920097}, {"id": 971, "seek": 406556, "start": 4076.56, "end": 4078.56, "text": " That's how progress has been made so far.", "tokens": [50914, 663, 311, 577, 4205, 575, 668, 1027, 370, 1400, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0920292854309082, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0017979098483920097}, {"id": 972, "seek": 406556, "start": 4078.56, "end": 4080.56, "text": " Not by the people talking about general AI.", "tokens": [51014, 1726, 538, 264, 561, 1417, 466, 2674, 7318, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0920292854309082, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0017979098483920097}, {"id": 973, "seek": 406556, "start": 4080.56, "end": 4083.56, "text": " It's being made by people looking at saying,", "tokens": [51114, 467, 311, 885, 1027, 538, 561, 1237, 412, 1566, 11, 51264], "temperature": 0.0, "avg_logprob": -0.0920292854309082, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0017979098483920097}, {"id": 974, "seek": 406556, "start": 4083.56, "end": 4085.56, "text": " how can I get a neural net to do vision,", "tokens": [51264, 577, 393, 286, 483, 257, 18161, 2533, 281, 360, 5201, 11, 51364], "temperature": 0.0, "avg_logprob": -0.0920292854309082, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0017979098483920097}, {"id": 975, "seek": 406556, "start": 4085.56, "end": 4088.56, "text": " or how can I get it to do machine translation?", "tokens": [51364, 420, 577, 393, 286, 483, 309, 281, 360, 3479, 12853, 30, 51514], "temperature": 0.0, "avg_logprob": -0.0920292854309082, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0017979098483920097}, {"id": 976, "seek": 406556, "start": 4088.56, "end": 4090.56, "text": " Thank you.", "tokens": [51514, 1044, 291, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0920292854309082, "compression_ratio": 1.7521367521367521, "no_speech_prob": 0.0017979098483920097}, {"id": 977, "seek": 409556, "start": 4095.56, "end": 4099.5599999999995, "text": " Hi, I'm just wondering about the role of hierarchy in general.", "tokens": [50364, 2421, 11, 286, 478, 445, 6359, 466, 264, 3090, 295, 22333, 294, 2674, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13942219229305491, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.004507765639573336}, {"id": 978, "seek": 409556, "start": 4099.5599999999995, "end": 4101.5599999999995, "text": " There are different types of hierarchy.", "tokens": [50564, 821, 366, 819, 3467, 295, 22333, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13942219229305491, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.004507765639573336}, {"id": 979, "seek": 409556, "start": 4101.5599999999995, "end": 4103.5599999999995, "text": " There's different layers of neural network.", "tokens": [50664, 821, 311, 819, 7914, 295, 18161, 3209, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13942219229305491, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.004507765639573336}, {"id": 980, "seek": 409556, "start": 4103.5599999999995, "end": 4106.5599999999995, "text": " As you mentioned, there's fast memory and slow memory.", "tokens": [50764, 1018, 291, 2835, 11, 456, 311, 2370, 4675, 293, 2964, 4675, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13942219229305491, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.004507765639573336}, {"id": 981, "seek": 409556, "start": 4106.5599999999995, "end": 4111.5599999999995, "text": " Then I wonder, are there more ways to add hierarchy to neural networks", "tokens": [50914, 1396, 286, 2441, 11, 366, 456, 544, 2098, 281, 909, 22333, 281, 18161, 9590, 51164], "temperature": 0.0, "avg_logprob": -0.13942219229305491, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.004507765639573336}, {"id": 982, "seek": 409556, "start": 4111.5599999999995, "end": 4115.5599999999995, "text": " to make them more useful or emulate actual brain?", "tokens": [51164, 281, 652, 552, 544, 4420, 420, 45497, 3539, 3567, 30, 51364], "temperature": 0.0, "avg_logprob": -0.13942219229305491, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.004507765639573336}, {"id": 983, "seek": 409556, "start": 4118.5599999999995, "end": 4120.5599999999995, "text": " Yes, probably.", "tokens": [51514, 1079, 11, 1391, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13942219229305491, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.004507765639573336}, {"id": 984, "seek": 412056, "start": 4121.56, "end": 4125.56, "text": " In vision, for example,", "tokens": [50414, 682, 5201, 11, 337, 1365, 11, 50614], "temperature": 0.0, "avg_logprob": -0.16453650757506655, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.001858516945503652}, {"id": 985, "seek": 412056, "start": 4125.56, "end": 4130.56, "text": " you have multiple layers,", "tokens": [50614, 291, 362, 3866, 7914, 11, 50864], "temperature": 0.0, "avg_logprob": -0.16453650757506655, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.001858516945503652}, {"id": 986, "seek": 412056, "start": 4130.56, "end": 4133.56, "text": " that is, you have multiple cortical areas in the visual pathway.", "tokens": [50864, 300, 307, 11, 291, 362, 3866, 11278, 804, 3179, 294, 264, 5056, 18590, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16453650757506655, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.001858516945503652}, {"id": 987, "seek": 412056, "start": 4133.56, "end": 4136.56, "text": " That's a very different kind of hierarchy", "tokens": [51014, 663, 311, 257, 588, 819, 733, 295, 22333, 51164], "temperature": 0.0, "avg_logprob": -0.16453650757506655, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.001858516945503652}, {"id": 988, "seek": 412056, "start": 4136.56, "end": 4140.56, "text": " from what you need for dealing with the sort of structural reality.", "tokens": [51164, 490, 437, 291, 643, 337, 6260, 365, 264, 1333, 295, 15067, 4103, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16453650757506655, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.001858516945503652}, {"id": 989, "seek": 412056, "start": 4140.56, "end": 4143.56, "text": " In reality, there's the universe.", "tokens": [51364, 682, 4103, 11, 456, 311, 264, 6445, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16453650757506655, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.001858516945503652}, {"id": 990, "seek": 412056, "start": 4143.56, "end": 4146.56, "text": " There may be many of them, but that's one's enough.", "tokens": [51514, 821, 815, 312, 867, 295, 552, 11, 457, 300, 311, 472, 311, 1547, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16453650757506655, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.001858516945503652}, {"id": 991, "seek": 412056, "start": 4146.56, "end": 4149.56, "text": " Then there's galaxies.", "tokens": [51664, 1396, 456, 311, 28755, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16453650757506655, "compression_ratio": 1.6243902439024391, "no_speech_prob": 0.001858516945503652}, {"id": 992, "seek": 414956, "start": 4149.56, "end": 4152.56, "text": " Then there's probably things above galaxies.", "tokens": [50364, 1396, 456, 311, 1391, 721, 3673, 28755, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11600998595908836, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.002085247775539756}, {"id": 993, "seek": 414956, "start": 4152.56, "end": 4155.56, "text": " Then there's in the galaxies the stars,", "tokens": [50514, 1396, 456, 311, 294, 264, 28755, 264, 6105, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11600998595908836, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.002085247775539756}, {"id": 994, "seek": 414956, "start": 4155.56, "end": 4157.56, "text": " and then there's solar systems,", "tokens": [50664, 293, 550, 456, 311, 7936, 3652, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11600998595908836, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.002085247775539756}, {"id": 995, "seek": 414956, "start": 4157.56, "end": 4160.56, "text": " and then there's planets, and so on.", "tokens": [50764, 293, 550, 456, 311, 15126, 11, 293, 370, 322, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11600998595908836, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.002085247775539756}, {"id": 996, "seek": 414956, "start": 4160.56, "end": 4162.56, "text": " We can do that all the way down to atoms.", "tokens": [50914, 492, 393, 360, 300, 439, 264, 636, 760, 281, 16871, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11600998595908836, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.002085247775539756}, {"id": 997, "seek": 414956, "start": 4162.56, "end": 4164.56, "text": " You can imagine all that.", "tokens": [51014, 509, 393, 3811, 439, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11600998595908836, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.002085247775539756}, {"id": 998, "seek": 414956, "start": 4164.56, "end": 4167.56, "text": " You can represent all that in your brain.", "tokens": [51114, 509, 393, 2906, 439, 300, 294, 428, 3567, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11600998595908836, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.002085247775539756}, {"id": 999, "seek": 414956, "start": 4167.56, "end": 4170.56, "text": " Clearly, what's going on is,", "tokens": [51264, 24120, 11, 437, 311, 516, 322, 307, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11600998595908836, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.002085247775539756}, {"id": 1000, "seek": 414956, "start": 4170.56, "end": 4173.56, "text": " out in the world, there's this hierarchy", "tokens": [51414, 484, 294, 264, 1002, 11, 456, 311, 341, 22333, 51564], "temperature": 0.0, "avg_logprob": -0.11600998595908836, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.002085247775539756}, {"id": 1001, "seek": 414956, "start": 4173.56, "end": 4176.56, "text": " that goes over many, many orders of magnitude", "tokens": [51564, 300, 1709, 670, 867, 11, 867, 9470, 295, 15668, 51714], "temperature": 0.0, "avg_logprob": -0.11600998595908836, "compression_ratio": 1.7710280373831775, "no_speech_prob": 0.002085247775539756}, {"id": 1002, "seek": 417656, "start": 4176.56, "end": 4179.56, "text": " from the universe down to quarks,", "tokens": [50364, 490, 264, 6445, 760, 281, 421, 20851, 11, 50514], "temperature": 0.0, "avg_logprob": -0.11951078414916992, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.011037305928766727}, {"id": 1003, "seek": 417656, "start": 4179.56, "end": 4182.56, "text": " or whatever the smallest thing is now.", "tokens": [50514, 420, 2035, 264, 16998, 551, 307, 586, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11951078414916992, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.011037305928766727}, {"id": 1004, "seek": 417656, "start": 4184.56, "end": 4187.56, "text": " You don't want that kind of hierarchy in your brain.", "tokens": [50764, 509, 500, 380, 528, 300, 733, 295, 22333, 294, 428, 3567, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11951078414916992, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.011037305928766727}, {"id": 1005, "seek": 417656, "start": 4187.56, "end": 4190.56, "text": " What you've got in your brain is the ability to deal", "tokens": [50914, 708, 291, 600, 658, 294, 428, 3567, 307, 264, 3485, 281, 2028, 51064], "temperature": 0.0, "avg_logprob": -0.11951078414916992, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.011037305928766727}, {"id": 1006, "seek": 417656, "start": 4190.56, "end": 4192.56, "text": " with a little window of hierarchy,", "tokens": [51064, 365, 257, 707, 4910, 295, 22333, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11951078414916992, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.011037305928766727}, {"id": 1007, "seek": 417656, "start": 4192.56, "end": 4195.56, "text": " where there's an object in its parts.", "tokens": [51164, 689, 456, 311, 364, 2657, 294, 1080, 3166, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11951078414916992, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.011037305928766727}, {"id": 1008, "seek": 417656, "start": 4195.56, "end": 4197.56, "text": " To deal with the whole universe,", "tokens": [51314, 1407, 2028, 365, 264, 1379, 6445, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11951078414916992, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.011037305928766727}, {"id": 1009, "seek": 417656, "start": 4197.56, "end": 4199.56, "text": " you can take this window,", "tokens": [51414, 291, 393, 747, 341, 4910, 11, 51514], "temperature": 0.0, "avg_logprob": -0.11951078414916992, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.011037305928766727}, {"id": 1010, "seek": 417656, "start": 4199.56, "end": 4202.56, "text": " and you can map at a scale of the universe,", "tokens": [51514, 293, 291, 393, 4471, 412, 257, 4373, 295, 264, 6445, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11951078414916992, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.011037305928766727}, {"id": 1011, "seek": 420256, "start": 4202.56, "end": 4204.56, "text": " and there's the universe, and there's the galaxies,", "tokens": [50364, 293, 456, 311, 264, 6445, 11, 293, 456, 311, 264, 28755, 11, 50464], "temperature": 0.0, "avg_logprob": -0.1026667723288903, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.002961711725220084}, {"id": 1012, "seek": 420256, "start": 4204.56, "end": 4207.56, "text": " or there's the galaxies, and there's the stars,", "tokens": [50464, 420, 456, 311, 264, 28755, 11, 293, 456, 311, 264, 6105, 11, 50614], "temperature": 0.0, "avg_logprob": -0.1026667723288903, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.002961711725220084}, {"id": 1013, "seek": 420256, "start": 4207.56, "end": 4210.56, "text": " or there's the atom, and there's the electrons.", "tokens": [50614, 420, 456, 311, 264, 12018, 11, 293, 456, 311, 264, 14265, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1026667723288903, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.002961711725220084}, {"id": 1014, "seek": 420256, "start": 4212.56, "end": 4215.56, "text": " You're using the same neural hardware,", "tokens": [50864, 509, 434, 1228, 264, 912, 18161, 8837, 11, 51014], "temperature": 0.0, "avg_logprob": -0.1026667723288903, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.002961711725220084}, {"id": 1015, "seek": 420256, "start": 4215.56, "end": 4218.56, "text": " but mapping reality onto it differently.", "tokens": [51014, 457, 18350, 4103, 3911, 309, 7614, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1026667723288903, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.002961711725220084}, {"id": 1016, "seek": 420256, "start": 4218.56, "end": 4221.56, "text": " I think whenever we have to deal with anything complicated,", "tokens": [51164, 286, 519, 5699, 321, 362, 281, 2028, 365, 1340, 6179, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1026667723288903, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.002961711725220084}, {"id": 1017, "seek": 420256, "start": 4221.56, "end": 4223.56, "text": " we use hierarchies.", "tokens": [51314, 321, 764, 35250, 530, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1026667723288903, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.002961711725220084}, {"id": 1018, "seek": 420256, "start": 4224.56, "end": 4227.56, "text": " The way the brain uses them is by varying the mapping", "tokens": [51464, 440, 636, 264, 3567, 4960, 552, 307, 538, 22984, 264, 18350, 51614], "temperature": 0.0, "avg_logprob": -0.1026667723288903, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.002961711725220084}, {"id": 1019, "seek": 420256, "start": 4227.56, "end": 4231.56, "text": " from reality onto the brain.", "tokens": [51614, 490, 4103, 3911, 264, 3567, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1026667723288903, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.002961711725220084}, {"id": 1020, "seek": 423156, "start": 4231.56, "end": 4234.56, "text": " It can only operate with a small window on a hierarchy,", "tokens": [50364, 467, 393, 787, 9651, 365, 257, 1359, 4910, 322, 257, 22333, 11, 50514], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1021, "seek": 423156, "start": 4234.56, "end": 4236.56, "text": " which you can move up and down.", "tokens": [50514, 597, 291, 393, 1286, 493, 293, 760, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1022, "seek": 423156, "start": 4236.56, "end": 4239.56, "text": " Much like you only have a small region of high resolution,", "tokens": [50614, 12313, 411, 291, 787, 362, 257, 1359, 4458, 295, 1090, 8669, 11, 50764], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1023, "seek": 423156, "start": 4239.56, "end": 4241.56, "text": " which you move around.", "tokens": [50764, 597, 291, 1286, 926, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1024, "seek": 423156, "start": 4241.56, "end": 4242.56, "text": " So logarithm?", "tokens": [50864, 407, 41473, 32674, 30, 50914], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1025, "seek": 423156, "start": 4242.56, "end": 4243.56, "text": " Sorry?", "tokens": [50914, 4919, 30, 50964], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1026, "seek": 423156, "start": 4243.56, "end": 4245.56, "text": " Like logarithm.", "tokens": [50964, 1743, 41473, 32674, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1027, "seek": 423156, "start": 4245.56, "end": 4247.56, "text": " What about logarithms?", "tokens": [51064, 708, 466, 41473, 355, 2592, 30, 51164], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1028, "seek": 423156, "start": 4247.56, "end": 4249.56, "text": " That's what you're talking about, right?", "tokens": [51164, 663, 311, 437, 291, 434, 1417, 466, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1029, "seek": 423156, "start": 4249.56, "end": 4252.56, "text": " Compressing a big range into something that is much manageable.", "tokens": [51264, 6620, 18605, 257, 955, 3613, 666, 746, 300, 307, 709, 38798, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1030, "seek": 423156, "start": 4252.56, "end": 4254.56, "text": " I wasn't thinking of it like that.", "tokens": [51414, 286, 2067, 380, 1953, 295, 309, 411, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1031, "seek": 423156, "start": 4254.56, "end": 4257.56, "text": " I was thinking of it as you have some fixed hardware,", "tokens": [51514, 286, 390, 1953, 295, 309, 382, 291, 362, 512, 6806, 8837, 11, 51664], "temperature": 0.0, "avg_logprob": -0.12211835579793962, "compression_ratio": 1.692, "no_speech_prob": 0.00045407243305817246}, {"id": 1032, "seek": 425756, "start": 4257.56, "end": 4261.56, "text": " and when I'm thinking about the solar system,", "tokens": [50364, 293, 562, 286, 478, 1953, 466, 264, 7936, 1185, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11946664658267941, "compression_ratio": 1.7863247863247864, "no_speech_prob": 0.008033453486859798}, {"id": 1033, "seek": 425756, "start": 4261.56, "end": 4264.56, "text": " my fixed hardware couldn't possibly deal with the universe.", "tokens": [50564, 452, 6806, 8837, 2809, 380, 6264, 2028, 365, 264, 6445, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11946664658267941, "compression_ratio": 1.7863247863247864, "no_speech_prob": 0.008033453486859798}, {"id": 1034, "seek": 425756, "start": 4264.56, "end": 4267.56, "text": " That's much too big, and it couldn't possibly deal with an atom.", "tokens": [50714, 663, 311, 709, 886, 955, 11, 293, 309, 2809, 380, 6264, 2028, 365, 364, 12018, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11946664658267941, "compression_ratio": 1.7863247863247864, "no_speech_prob": 0.008033453486859798}, {"id": 1035, "seek": 425756, "start": 4267.56, "end": 4269.56, "text": " That's much too small.", "tokens": [50864, 663, 311, 709, 886, 1359, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11946664658267941, "compression_ratio": 1.7863247863247864, "no_speech_prob": 0.008033453486859798}, {"id": 1036, "seek": 425756, "start": 4269.56, "end": 4272.56, "text": " But it's fine dealing with the sun and some planets,", "tokens": [50964, 583, 309, 311, 2489, 6260, 365, 264, 3295, 293, 512, 15126, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11946664658267941, "compression_ratio": 1.7863247863247864, "no_speech_prob": 0.008033453486859798}, {"id": 1037, "seek": 425756, "start": 4272.56, "end": 4274.56, "text": " and maybe a moon or two.", "tokens": [51114, 293, 1310, 257, 7135, 420, 732, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11946664658267941, "compression_ratio": 1.7863247863247864, "no_speech_prob": 0.008033453486859798}, {"id": 1038, "seek": 425756, "start": 4275.56, "end": 4279.56, "text": " What I'm trying to get at is we need to make a big distinction", "tokens": [51264, 708, 286, 478, 1382, 281, 483, 412, 307, 321, 643, 281, 652, 257, 955, 16844, 51464], "temperature": 0.0, "avg_logprob": -0.11946664658267941, "compression_ratio": 1.7863247863247864, "no_speech_prob": 0.008033453486859798}, {"id": 1039, "seek": 425756, "start": 4279.56, "end": 4283.56, "text": " between the hierarchy in the real world,", "tokens": [51464, 1296, 264, 22333, 294, 264, 957, 1002, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11946664658267941, "compression_ratio": 1.7863247863247864, "no_speech_prob": 0.008033453486859798}, {"id": 1040, "seek": 425756, "start": 4283.56, "end": 4286.56, "text": " hierarchical structures in the real world,", "tokens": [51664, 35250, 804, 9227, 294, 264, 957, 1002, 11, 51814], "temperature": 0.0, "avg_logprob": -0.11946664658267941, "compression_ratio": 1.7863247863247864, "no_speech_prob": 0.008033453486859798}, {"id": 1041, "seek": 428656, "start": 4286.56, "end": 4290.56, "text": " and how we deal with them cognitively,", "tokens": [50364, 293, 577, 321, 2028, 365, 552, 15605, 356, 11, 50564], "temperature": 0.0, "avg_logprob": -0.0842237750303398, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0011646617203950882}, {"id": 1042, "seek": 428656, "start": 4290.56, "end": 4292.56, "text": " where we use attention,", "tokens": [50564, 689, 321, 764, 3202, 11, 50664], "temperature": 0.0, "avg_logprob": -0.0842237750303398, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0011646617203950882}, {"id": 1043, "seek": 428656, "start": 4292.56, "end": 4295.56, "text": " and we only ever deal with a bit of the hierarchy at a time.", "tokens": [50664, 293, 321, 787, 1562, 2028, 365, 257, 857, 295, 264, 22333, 412, 257, 565, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0842237750303398, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0011646617203950882}, {"id": 1044, "seek": 428656, "start": 4295.56, "end": 4299.56, "text": " That's not the same for, say, aspects of language,", "tokens": [50814, 663, 311, 406, 264, 912, 337, 11, 584, 11, 7270, 295, 2856, 11, 51014], "temperature": 0.0, "avg_logprob": -0.0842237750303398, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0011646617203950882}, {"id": 1045, "seek": 428656, "start": 4299.56, "end": 4300.56, "text": " where you have...", "tokens": [51014, 689, 291, 362, 485, 51064], "temperature": 0.0, "avg_logprob": -0.0842237750303398, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0011646617203950882}, {"id": 1046, "seek": 428656, "start": 4300.56, "end": 4302.56, "text": " Notice that with vision,", "tokens": [51064, 13428, 300, 365, 5201, 11, 51164], "temperature": 0.0, "avg_logprob": -0.0842237750303398, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0011646617203950882}, {"id": 1047, "seek": 428656, "start": 4302.56, "end": 4306.56, "text": " I can use the same neurons for representing the sun", "tokens": [51164, 286, 393, 764, 264, 912, 22027, 337, 13460, 264, 3295, 51364], "temperature": 0.0, "avg_logprob": -0.0842237750303398, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0011646617203950882}, {"id": 1048, "seek": 428656, "start": 4306.56, "end": 4308.56, "text": " and for representing a nucleus.", "tokens": [51364, 293, 337, 13460, 257, 28055, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0842237750303398, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0011646617203950882}, {"id": 1049, "seek": 428656, "start": 4309.56, "end": 4312.56, "text": " It's just an analogy, but it's the same neurons I'm using.", "tokens": [51514, 467, 311, 445, 364, 21663, 11, 457, 309, 311, 264, 912, 22027, 286, 478, 1228, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0842237750303398, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0011646617203950882}, {"id": 1050, "seek": 431256, "start": 4313.56, "end": 4316.56, "text": " Now, if I'm processing language,", "tokens": [50414, 823, 11, 498, 286, 478, 9007, 2856, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1051, "seek": 431256, "start": 4316.56, "end": 4319.56, "text": " I've got things that find me phonemes", "tokens": [50564, 286, 600, 658, 721, 300, 915, 385, 30754, 443, 279, 50714], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1052, "seek": 431256, "start": 4319.56, "end": 4321.56, "text": " and things that turn phonemes into words", "tokens": [50714, 293, 721, 300, 1261, 30754, 443, 279, 666, 2283, 50814], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1053, "seek": 431256, "start": 4321.56, "end": 4323.56, "text": " and things that turn words into sentences,", "tokens": [50814, 293, 721, 300, 1261, 2283, 666, 16579, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1054, "seek": 431256, "start": 4323.56, "end": 4324.56, "text": " and those...", "tokens": [50914, 293, 729, 485, 50964], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1055, "seek": 431256, "start": 4324.56, "end": 4326.56, "text": " I can't move a window like that.", "tokens": [50964, 286, 393, 380, 1286, 257, 4910, 411, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1056, "seek": 431256, "start": 4326.56, "end": 4327.56, "text": " That's a fixed hierarchy.", "tokens": [51064, 663, 311, 257, 6806, 22333, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1057, "seek": 431256, "start": 4327.56, "end": 4329.56, "text": " There's phonemes, and there's words,", "tokens": [51114, 821, 311, 30754, 443, 279, 11, 293, 456, 311, 2283, 11, 51214], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1058, "seek": 431256, "start": 4329.56, "end": 4331.56, "text": " and there's phrases, and there's sentences,", "tokens": [51214, 293, 456, 311, 20312, 11, 293, 456, 311, 16579, 11, 51314], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1059, "seek": 431256, "start": 4331.56, "end": 4333.56, "text": " and that's all sort of fixed in the brain.", "tokens": [51314, 293, 300, 311, 439, 1333, 295, 6806, 294, 264, 3567, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1060, "seek": 431256, "start": 4333.56, "end": 4335.56, "text": " That's not a flexible matter.", "tokens": [51414, 663, 311, 406, 257, 11358, 1871, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1061, "seek": 431256, "start": 4335.56, "end": 4337.56, "text": " You can't kind of move the sentences down", "tokens": [51514, 509, 393, 380, 733, 295, 1286, 264, 16579, 760, 51614], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1062, "seek": 431256, "start": 4337.56, "end": 4339.56, "text": " so they're where the words were,", "tokens": [51614, 370, 436, 434, 689, 264, 2283, 645, 11, 51714], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1063, "seek": 431256, "start": 4339.56, "end": 4341.56, "text": " move the words down so they're where the phonemes were.", "tokens": [51714, 1286, 264, 2283, 760, 370, 436, 434, 689, 264, 30754, 443, 279, 645, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09645814833298229, "compression_ratio": 2.085714285714286, "no_speech_prob": 0.0058893803507089615}, {"id": 1064, "seek": 434156, "start": 4341.56, "end": 4343.56, "text": " That doesn't work.", "tokens": [50364, 663, 1177, 380, 589, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1370450817808813, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.0034890323877334595}, {"id": 1065, "seek": 434156, "start": 4343.56, "end": 4346.56, "text": " So there's some hierarchies that really do relate", "tokens": [50464, 407, 456, 311, 512, 35250, 530, 300, 534, 360, 10961, 50614], "temperature": 0.0, "avg_logprob": -0.1370450817808813, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.0034890323877334595}, {"id": 1066, "seek": 434156, "start": 4346.56, "end": 4350.56, "text": " to sets of neurons in the brain.", "tokens": [50614, 281, 6352, 295, 22027, 294, 264, 3567, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1370450817808813, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.0034890323877334595}, {"id": 1067, "seek": 434156, "start": 4350.56, "end": 4353.56, "text": " They're like the layers in the connections models.", "tokens": [50814, 814, 434, 411, 264, 7914, 294, 264, 9271, 5245, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1370450817808813, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.0034890323877334595}, {"id": 1068, "seek": 434156, "start": 4353.56, "end": 4355.56, "text": " There's other hierarchies,", "tokens": [50964, 821, 311, 661, 35250, 530, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1370450817808813, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.0034890323877334595}, {"id": 1069, "seek": 434156, "start": 4355.56, "end": 4357.56, "text": " like the whole spatial structure of the universe,", "tokens": [51064, 411, 264, 1379, 23598, 3877, 295, 264, 6445, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1370450817808813, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.0034890323877334595}, {"id": 1070, "seek": 434156, "start": 4357.56, "end": 4360.56, "text": " where what's in the brain is a window", "tokens": [51164, 689, 437, 311, 294, 264, 3567, 307, 257, 4910, 51314], "temperature": 0.0, "avg_logprob": -0.1370450817808813, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.0034890323877334595}, {"id": 1071, "seek": 434156, "start": 4360.56, "end": 4362.56, "text": " you move over that hierarchy.", "tokens": [51314, 291, 1286, 670, 300, 22333, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1370450817808813, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.0034890323877334595}, {"id": 1072, "seek": 434156, "start": 4362.56, "end": 4364.56, "text": " Thank you.", "tokens": [51414, 1044, 291, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1370450817808813, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.0034890323877334595}, {"id": 1073, "seek": 434156, "start": 4367.56, "end": 4370.56, "text": " Thanks, Dr. Hinton, for excellent talk", "tokens": [51664, 2561, 11, 2491, 13, 389, 12442, 11, 337, 7103, 751, 51814], "temperature": 0.0, "avg_logprob": -0.1370450817808813, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.0034890323877334595}, {"id": 1074, "seek": 437056, "start": 4370.56, "end": 4373.56, "text": " and excellent ideas about the feasibility of back propagation.", "tokens": [50364, 293, 7103, 3487, 466, 264, 21781, 2841, 295, 646, 38377, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1075, "seek": 437056, "start": 4373.56, "end": 4375.56, "text": " My question's maybe more boring", "tokens": [50514, 1222, 1168, 311, 1310, 544, 9989, 50614], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1076, "seek": 437056, "start": 4375.56, "end": 4378.56, "text": " about the statistical comments that you made.", "tokens": [50614, 466, 264, 22820, 3053, 300, 291, 1027, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1077, "seek": 437056, "start": 4378.56, "end": 4380.56, "text": " Is that Dale?", "tokens": [50764, 1119, 300, 31329, 30, 50864], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1078, "seek": 437056, "start": 4380.56, "end": 4381.56, "text": " Pardon me?", "tokens": [50864, 32392, 385, 30, 50914], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1079, "seek": 437056, "start": 4381.56, "end": 4382.56, "text": " Are you Dale?", "tokens": [50914, 2014, 291, 31329, 30, 50964], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1080, "seek": 437056, "start": 4382.56, "end": 4384.56, "text": " No, I'm Kyle.", "tokens": [50964, 883, 11, 286, 478, 18023, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1081, "seek": 437056, "start": 4386.56, "end": 4388.56, "text": " You sound like Dale Shermans.", "tokens": [51164, 509, 1626, 411, 31329, 11789, 44734, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1082, "seek": 437056, "start": 4388.56, "end": 4391.56, "text": " I'm at the University of Alberta.", "tokens": [51264, 286, 478, 412, 264, 3535, 295, 43279, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1083, "seek": 437056, "start": 4392.56, "end": 4393.56, "text": " Hi.", "tokens": [51464, 2421, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1084, "seek": 437056, "start": 4393.56, "end": 4395.56, "text": " Well, that's a good instance,", "tokens": [51514, 1042, 11, 300, 311, 257, 665, 5197, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1085, "seek": 437056, "start": 4395.56, "end": 4397.56, "text": " that you sound just like Dale Shermans", "tokens": [51614, 300, 291, 1626, 445, 411, 31329, 11789, 44734, 51714], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1086, "seek": 437056, "start": 4397.56, "end": 4399.56, "text": " and you're at the University of Alberta.", "tokens": [51714, 293, 291, 434, 412, 264, 3535, 295, 43279, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1202209506716047, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0026148364413529634}, {"id": 1087, "seek": 439956, "start": 4399.56, "end": 4403.56, "text": " Are you a student of Dale's?", "tokens": [50364, 2014, 291, 257, 3107, 295, 31329, 311, 30, 50564], "temperature": 0.0, "avg_logprob": -0.11071275628131369, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003650665457826108}, {"id": 1088, "seek": 439956, "start": 4403.56, "end": 4404.56, "text": " No.", "tokens": [50564, 883, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11071275628131369, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003650665457826108}, {"id": 1089, "seek": 439956, "start": 4407.56, "end": 4409.56, "text": " I think I've only met a voice.", "tokens": [50764, 286, 519, 286, 600, 787, 1131, 257, 3177, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11071275628131369, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003650665457826108}, {"id": 1090, "seek": 439956, "start": 4409.56, "end": 4411.56, "text": " If you're a student of Dale's, I need to watch out", "tokens": [50864, 759, 291, 434, 257, 3107, 295, 31329, 311, 11, 286, 643, 281, 1159, 484, 50964], "temperature": 0.0, "avg_logprob": -0.11071275628131369, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003650665457826108}, {"id": 1091, "seek": 439956, "start": 4411.56, "end": 4413.56, "text": " because it's going to be a very tricky question.", "tokens": [50964, 570, 309, 311, 516, 281, 312, 257, 588, 12414, 1168, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11071275628131369, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003650665457826108}, {"id": 1092, "seek": 439956, "start": 4413.56, "end": 4418.56, "text": " The question is that I was trained with this intuition", "tokens": [51064, 440, 1168, 307, 300, 286, 390, 8895, 365, 341, 24002, 51314], "temperature": 0.0, "avg_logprob": -0.11071275628131369, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003650665457826108}, {"id": 1093, "seek": 439956, "start": 4418.56, "end": 4420.56, "text": " that you can't overparameterize your models,", "tokens": [51314, 300, 291, 393, 380, 670, 2181, 335, 2398, 1125, 428, 5245, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11071275628131369, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003650665457826108}, {"id": 1094, "seek": 439956, "start": 4420.56, "end": 4424.56, "text": " that if you're trying to fit a line that you need two points,", "tokens": [51414, 300, 498, 291, 434, 1382, 281, 3318, 257, 1622, 300, 291, 643, 732, 2793, 11, 51614], "temperature": 0.0, "avg_logprob": -0.11071275628131369, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003650665457826108}, {"id": 1095, "seek": 439956, "start": 4424.56, "end": 4426.56, "text": " if you're trying to fit a curve you need three and so on", "tokens": [51614, 498, 291, 434, 1382, 281, 3318, 257, 7605, 291, 643, 1045, 293, 370, 322, 51714], "temperature": 0.0, "avg_logprob": -0.11071275628131369, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0003650665457826108}, {"id": 1096, "seek": 442656, "start": 4426.56, "end": 4428.56, "text": " and that scales up and you should always have", "tokens": [50364, 293, 300, 17408, 493, 293, 291, 820, 1009, 362, 50464], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1097, "seek": 442656, "start": 4428.56, "end": 4430.56, "text": " a little bit less data points.", "tokens": [50464, 257, 707, 857, 1570, 1412, 2793, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1098, "seek": 442656, "start": 4430.56, "end": 4434.56, "text": " I know that you have shown clearly", "tokens": [50564, 286, 458, 300, 291, 362, 4898, 4448, 50764], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1099, "seek": 442656, "start": 4434.56, "end": 4436.56, "text": " and the field has shown that that's not true.", "tokens": [50764, 293, 264, 2519, 575, 4898, 300, 300, 311, 406, 2074, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1100, "seek": 442656, "start": 4436.56, "end": 4439.56, "text": " What were the statisticians getting wrong", "tokens": [50864, 708, 645, 264, 29588, 2567, 1242, 2085, 51014], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1101, "seek": 442656, "start": 4439.56, "end": 4441.56, "text": " in their logic to be convinced?", "tokens": [51014, 294, 641, 9952, 281, 312, 12561, 30, 51114], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1102, "seek": 442656, "start": 4441.56, "end": 4443.56, "text": " It's to do with regularization,", "tokens": [51114, 467, 311, 281, 360, 365, 3890, 2144, 11, 51214], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1103, "seek": 442656, "start": 4443.56, "end": 4445.56, "text": " that you need it to be highly regularized.", "tokens": [51214, 300, 291, 643, 309, 281, 312, 5405, 3890, 1602, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1104, "seek": 442656, "start": 4445.56, "end": 4447.56, "text": " But first of all, I'll show you", "tokens": [51314, 583, 700, 295, 439, 11, 286, 603, 855, 291, 51414], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1105, "seek": 442656, "start": 4447.56, "end": 4453.56, "text": " that if you want to fit two data points,", "tokens": [51414, 300, 498, 291, 528, 281, 3318, 732, 1412, 2793, 11, 51714], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1106, "seek": 442656, "start": 4453.56, "end": 4455.56, "text": " well, let's take three.", "tokens": [51714, 731, 11, 718, 311, 747, 1045, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09238583466102337, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.007788649760186672}, {"id": 1107, "seek": 445556, "start": 4455.56, "end": 4457.56, "text": " If you want to fit three data points,", "tokens": [50364, 759, 291, 528, 281, 3318, 1045, 1412, 2793, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1108, "seek": 445556, "start": 4457.56, "end": 4461.56, "text": " you would have told me you want a polynomial", "tokens": [50464, 291, 576, 362, 1907, 385, 291, 528, 257, 26110, 50664], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1109, "seek": 445556, "start": 4461.56, "end": 4463.56, "text": " with only three degrees of freedom,", "tokens": [50664, 365, 787, 1045, 5310, 295, 5645, 11, 50764], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1110, "seek": 445556, "start": 4463.56, "end": 4466.56, "text": " so you want a constant and a slope and a curvature", "tokens": [50764, 370, 291, 528, 257, 5754, 293, 257, 13525, 293, 257, 37638, 50914], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1111, "seek": 445556, "start": 4466.56, "end": 4469.56, "text": " and that's all you can afford with three data points.", "tokens": [50914, 293, 300, 311, 439, 291, 393, 6157, 365, 1045, 1412, 2793, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1112, "seek": 445556, "start": 4469.56, "end": 4471.56, "text": " That's wrong.", "tokens": [51064, 663, 311, 2085, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1113, "seek": 445556, "start": 4471.56, "end": 4473.56, "text": " Now, this is where we need a pen.", "tokens": [51164, 823, 11, 341, 307, 689, 321, 643, 257, 3435, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1114, "seek": 445556, "start": 4473.56, "end": 4475.56, "text": " Oh, sorry.", "tokens": [51264, 876, 11, 2597, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1115, "seek": 445556, "start": 4477.56, "end": 4478.56, "text": " They don't work.", "tokens": [51464, 814, 500, 380, 589, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1116, "seek": 445556, "start": 4478.56, "end": 4480.56, "text": " I tried them all and none of them work.", "tokens": [51514, 286, 3031, 552, 439, 293, 6022, 295, 552, 589, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1117, "seek": 445556, "start": 4481.56, "end": 4482.56, "text": " Okay, did they work?", "tokens": [51664, 1033, 11, 630, 436, 589, 30, 51714], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1118, "seek": 445556, "start": 4482.56, "end": 4484.56, "text": " Oh, well done.", "tokens": [51714, 876, 11, 731, 1096, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09481629678758524, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.005802274215966463}, {"id": 1119, "seek": 448456, "start": 4484.56, "end": 4486.56, "text": " Extra points.", "tokens": [50364, 29429, 2793, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14453063140044342, "compression_ratio": 1.563758389261745, "no_speech_prob": 0.0012748746667057276}, {"id": 1120, "seek": 448456, "start": 4488.56, "end": 4490.56, "text": " Okay, yes.", "tokens": [50564, 1033, 11, 2086, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14453063140044342, "compression_ratio": 1.563758389261745, "no_speech_prob": 0.0012748746667057276}, {"id": 1121, "seek": 448456, "start": 4490.56, "end": 4492.56, "text": " Can you see that?", "tokens": [50664, 1664, 291, 536, 300, 30, 50764], "temperature": 0.0, "avg_logprob": -0.14453063140044342, "compression_ratio": 1.563758389261745, "no_speech_prob": 0.0012748746667057276}, {"id": 1122, "seek": 448456, "start": 4492.56, "end": 4493.56, "text": " Yeah.", "tokens": [50764, 865, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14453063140044342, "compression_ratio": 1.563758389261745, "no_speech_prob": 0.0012748746667057276}, {"id": 1123, "seek": 448456, "start": 4493.56, "end": 4495.56, "text": " Okay, and we're going to have three data points.", "tokens": [50814, 1033, 11, 293, 321, 434, 516, 281, 362, 1045, 1412, 2793, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14453063140044342, "compression_ratio": 1.563758389261745, "no_speech_prob": 0.0012748746667057276}, {"id": 1124, "seek": 448456, "start": 4503.56, "end": 4507.56, "text": " And actually, if you're a statistician,", "tokens": [51314, 400, 767, 11, 498, 291, 434, 257, 29588, 952, 11, 51514], "temperature": 0.0, "avg_logprob": -0.14453063140044342, "compression_ratio": 1.563758389261745, "no_speech_prob": 0.0012748746667057276}, {"id": 1125, "seek": 448456, "start": 4507.56, "end": 4509.56, "text": " you'd probably say for three data points", "tokens": [51514, 291, 1116, 1391, 584, 337, 1045, 1412, 2793, 51614], "temperature": 0.0, "avg_logprob": -0.14453063140044342, "compression_ratio": 1.563758389261745, "no_speech_prob": 0.0012748746667057276}, {"id": 1126, "seek": 448456, "start": 4509.56, "end": 4512.56, "text": " you'd probably ought to fit a straight line like this.", "tokens": [51614, 291, 1116, 1391, 13416, 281, 3318, 257, 2997, 1622, 411, 341, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14453063140044342, "compression_ratio": 1.563758389261745, "no_speech_prob": 0.0012748746667057276}, {"id": 1127, "seek": 451256, "start": 4512.56, "end": 4515.56, "text": " Because I could fit a parabola", "tokens": [50364, 1436, 286, 727, 3318, 257, 45729, 4711, 50514], "temperature": 0.0, "avg_logprob": -0.11499304141638414, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0015407187165692449}, {"id": 1128, "seek": 451256, "start": 4515.56, "end": 4517.56, "text": " and the parabola would fit exactly", "tokens": [50514, 293, 264, 45729, 4711, 576, 3318, 2293, 50614], "temperature": 0.0, "avg_logprob": -0.11499304141638414, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0015407187165692449}, {"id": 1129, "seek": 451256, "start": 4517.56, "end": 4520.56, "text": " and that's a bit suspicious.", "tokens": [50614, 293, 300, 311, 257, 857, 17931, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11499304141638414, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0015407187165692449}, {"id": 1130, "seek": 451256, "start": 4520.56, "end": 4523.56, "text": " In other words, the parabola fits exactly,", "tokens": [50764, 682, 661, 2283, 11, 264, 45729, 4711, 9001, 2293, 11, 50914], "temperature": 0.0, "avg_logprob": -0.11499304141638414, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0015407187165692449}, {"id": 1131, "seek": 451256, "start": 4523.56, "end": 4529.56, "text": " but do you really believe that if you were to ask", "tokens": [50914, 457, 360, 291, 534, 1697, 300, 498, 291, 645, 281, 1029, 51214], "temperature": 0.0, "avg_logprob": -0.11499304141638414, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0015407187165692449}, {"id": 1132, "seek": 451256, "start": 4529.56, "end": 4531.56, "text": " when x is zero, what's the value of y?", "tokens": [51214, 562, 2031, 307, 4018, 11, 437, 311, 264, 2158, 295, 288, 30, 51314], "temperature": 0.0, "avg_logprob": -0.11499304141638414, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0015407187165692449}, {"id": 1133, "seek": 451256, "start": 4531.56, "end": 4533.56, "text": " Do you really believe that value for y?", "tokens": [51314, 1144, 291, 534, 1697, 300, 2158, 337, 288, 30, 51414], "temperature": 0.0, "avg_logprob": -0.11499304141638414, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0015407187165692449}, {"id": 1134, "seek": 451256, "start": 4533.56, "end": 4536.56, "text": " Because a straight line is far more conservative.", "tokens": [51414, 1436, 257, 2997, 1622, 307, 1400, 544, 13780, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11499304141638414, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0015407187165692449}, {"id": 1135, "seek": 451256, "start": 4536.56, "end": 4538.56, "text": " So a statistician would probably say", "tokens": [51564, 407, 257, 29588, 952, 576, 1391, 584, 51664], "temperature": 0.0, "avg_logprob": -0.11499304141638414, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0015407187165692449}, {"id": 1136, "seek": 451256, "start": 4538.56, "end": 4540.56, "text": " fit a straight line.", "tokens": [51664, 3318, 257, 2997, 1622, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11499304141638414, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0015407187165692449}, {"id": 1137, "seek": 454056, "start": 4540.56, "end": 4543.56, "text": " However, that would be a frequentist statistician.", "tokens": [50364, 2908, 11, 300, 576, 312, 257, 18004, 468, 29588, 952, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1138, "seek": 454056, "start": 4543.56, "end": 4546.56, "text": " If you took a Bayesian statistician,", "tokens": [50514, 759, 291, 1890, 257, 7840, 42434, 29588, 952, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1139, "seek": 454056, "start": 4546.56, "end": 4549.56, "text": " and this is in Chris Fisher's machine learning textbook,", "tokens": [50664, 293, 341, 307, 294, 6688, 26676, 311, 3479, 2539, 25591, 11, 50814], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1140, "seek": 454056, "start": 4549.56, "end": 4551.56, "text": " there's a nice picture of it, I think, somewhere.", "tokens": [50814, 456, 311, 257, 1481, 3036, 295, 309, 11, 286, 519, 11, 4079, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1141, "seek": 454056, "start": 4551.56, "end": 4553.56, "text": " Think it's that book.", "tokens": [50914, 6557, 309, 311, 300, 1446, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1142, "seek": 454056, "start": 4553.56, "end": 4555.56, "text": " A Bayesian statistician would say,", "tokens": [51014, 316, 7840, 42434, 29588, 952, 576, 584, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1143, "seek": 454056, "start": 4555.56, "end": 4559.56, "text": " okay, let's try fitting fifth-order polynomials.", "tokens": [51114, 1392, 11, 718, 311, 853, 15669, 9266, 12, 4687, 22560, 12356, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1144, "seek": 454056, "start": 4559.56, "end": 4562.56, "text": " And fifth-order polynomials,", "tokens": [51314, 400, 9266, 12, 4687, 22560, 12356, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1145, "seek": 454056, "start": 4562.56, "end": 4565.56, "text": " we might even fit ones that don't exactly go through the data,", "tokens": [51464, 321, 1062, 754, 3318, 2306, 300, 500, 380, 2293, 352, 807, 264, 1412, 11, 51614], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1146, "seek": 454056, "start": 4565.56, "end": 4567.56, "text": " but for now let's make them go through the data.", "tokens": [51614, 457, 337, 586, 718, 311, 652, 552, 352, 807, 264, 1412, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1147, "seek": 454056, "start": 4567.56, "end": 4569.56, "text": " So we fit a fifth-order polynomial", "tokens": [51714, 407, 321, 3318, 257, 9266, 12, 4687, 26110, 51814], "temperature": 0.0, "avg_logprob": -0.11033277864809389, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.0017851339653134346}, {"id": 1148, "seek": 456956, "start": 4569.56, "end": 4572.56, "text": " that goes kind of...", "tokens": [50364, 300, 1709, 733, 295, 485, 50514], "temperature": 0.0, "avg_logprob": -0.14751391358427948, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0016762890154495835}, {"id": 1149, "seek": 456956, "start": 4575.56, "end": 4576.56, "text": " One, two, three...", "tokens": [50664, 1485, 11, 732, 11, 1045, 485, 50714], "temperature": 0.0, "avg_logprob": -0.14751391358427948, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0016762890154495835}, {"id": 1150, "seek": 456956, "start": 4576.56, "end": 4578.56, "text": " Well, you know, some order.", "tokens": [50714, 1042, 11, 291, 458, 11, 512, 1668, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14751391358427948, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0016762890154495835}, {"id": 1151, "seek": 456956, "start": 4578.56, "end": 4580.56, "text": " And we fit another one.", "tokens": [50814, 400, 321, 3318, 1071, 472, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14751391358427948, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0016762890154495835}, {"id": 1152, "seek": 456956, "start": 4580.56, "end": 4583.56, "text": " Oh, that didn't go through the data.", "tokens": [50914, 876, 11, 300, 994, 380, 352, 807, 264, 1412, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14751391358427948, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0016762890154495835}, {"id": 1153, "seek": 456956, "start": 4587.56, "end": 4589.56, "text": " And we keep fitting these guys,", "tokens": [51264, 400, 321, 1066, 15669, 613, 1074, 11, 51364], "temperature": 0.0, "avg_logprob": -0.14751391358427948, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0016762890154495835}, {"id": 1154, "seek": 456956, "start": 4589.56, "end": 4591.56, "text": " and we fit a gazillion of them.", "tokens": [51364, 293, 321, 3318, 257, 26232, 11836, 295, 552, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14751391358427948, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0016762890154495835}, {"id": 1155, "seek": 456956, "start": 4591.56, "end": 4594.56, "text": " And what you see at the end is that", "tokens": [51464, 400, 437, 291, 536, 412, 264, 917, 307, 300, 51614], "temperature": 0.0, "avg_logprob": -0.14751391358427948, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0016762890154495835}, {"id": 1156, "seek": 456956, "start": 4594.56, "end": 4597.56, "text": " these gazillion ones, in between the data points,", "tokens": [51614, 613, 26232, 11836, 2306, 11, 294, 1296, 264, 1412, 2793, 11, 51764], "temperature": 0.0, "avg_logprob": -0.14751391358427948, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0016762890154495835}, {"id": 1157, "seek": 459756, "start": 4597.56, "end": 4599.56, "text": " they're kind of all over the place,", "tokens": [50364, 436, 434, 733, 295, 439, 670, 264, 1081, 11, 50464], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1158, "seek": 459756, "start": 4599.56, "end": 4603.56, "text": " and their average is in a sensible place like here,", "tokens": [50464, 293, 641, 4274, 307, 294, 257, 25380, 1081, 411, 510, 11, 50664], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1159, "seek": 459756, "start": 4603.56, "end": 4605.56, "text": " but their variance is big.", "tokens": [50664, 457, 641, 21977, 307, 955, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1160, "seek": 459756, "start": 4605.56, "end": 4607.56, "text": " And what they're telling you is,", "tokens": [50764, 400, 437, 436, 434, 3585, 291, 307, 11, 50864], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1161, "seek": 459756, "start": 4607.56, "end": 4609.56, "text": " if you give me this x-coordinate,", "tokens": [50864, 498, 291, 976, 385, 341, 2031, 12, 1291, 37326, 11, 50964], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1162, "seek": 459756, "start": 4609.56, "end": 4611.56, "text": " I'm rather uncertain about this y-coordinate,", "tokens": [50964, 286, 478, 2831, 11308, 466, 341, 288, 12, 1291, 37326, 11, 51064], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1163, "seek": 459756, "start": 4611.56, "end": 4613.56, "text": " but this is a good bet.", "tokens": [51064, 457, 341, 307, 257, 665, 778, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1164, "seek": 459756, "start": 4613.56, "end": 4615.56, "text": " And similarly here,", "tokens": [51164, 400, 14138, 510, 11, 51264], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1165, "seek": 459756, "start": 4615.56, "end": 4617.56, "text": " and if you go out here,", "tokens": [51264, 293, 498, 291, 352, 484, 510, 11, 51364], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1166, "seek": 459756, "start": 4617.56, "end": 4619.56, "text": " these polynomials are just all over the place,", "tokens": [51364, 613, 22560, 12356, 366, 445, 439, 670, 264, 1081, 11, 51464], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1167, "seek": 459756, "start": 4619.56, "end": 4621.56, "text": " and they'll tell you,", "tokens": [51464, 293, 436, 603, 980, 291, 11, 51564], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1168, "seek": 459756, "start": 4621.56, "end": 4623.56, "text": " if you give me this x-value,", "tokens": [51564, 498, 291, 976, 385, 341, 2031, 12, 29155, 11, 51664], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1169, "seek": 459756, "start": 4623.56, "end": 4626.56, "text": " then it could be pretty much anything.", "tokens": [51664, 550, 309, 727, 312, 1238, 709, 1340, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06675853302229696, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.0046067857183516026}, {"id": 1170, "seek": 462656, "start": 4626.56, "end": 4628.56, "text": " That's not a bad bet,", "tokens": [50364, 663, 311, 406, 257, 1578, 778, 11, 50464], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1171, "seek": 462656, "start": 4628.56, "end": 4630.56, "text": " but it could be pretty much anything.", "tokens": [50464, 457, 309, 727, 312, 1238, 709, 1340, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1172, "seek": 462656, "start": 4630.56, "end": 4632.56, "text": " And that's a much better answer", "tokens": [50564, 400, 300, 311, 257, 709, 1101, 1867, 50664], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1173, "seek": 462656, "start": 4632.56, "end": 4634.56, "text": " than you get from a straight line.", "tokens": [50664, 813, 291, 483, 490, 257, 2997, 1622, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1174, "seek": 462656, "start": 4634.56, "end": 4636.56, "text": " So by fitting a very large number", "tokens": [50764, 407, 538, 15669, 257, 588, 2416, 1230, 50864], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1175, "seek": 462656, "start": 4636.56, "end": 4638.56, "text": " of different polynomials,", "tokens": [50864, 295, 819, 22560, 12356, 11, 50964], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1176, "seek": 462656, "start": 4638.56, "end": 4640.56, "text": " and then averaging,", "tokens": [50964, 293, 550, 47308, 11, 51064], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1177, "seek": 462656, "start": 4640.56, "end": 4642.56, "text": " you get good, mean answers,", "tokens": [51064, 291, 483, 665, 11, 914, 6338, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1178, "seek": 462656, "start": 4642.56, "end": 4644.56, "text": " and you also get a sense of the variance.", "tokens": [51164, 293, 291, 611, 483, 257, 2020, 295, 264, 21977, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1179, "seek": 462656, "start": 4644.56, "end": 4646.56, "text": " Now, Drop-Out is doing something like that.", "tokens": [51264, 823, 11, 17675, 12, 28353, 307, 884, 746, 411, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1180, "seek": 462656, "start": 4646.56, "end": 4648.56, "text": " Yes, and that's brilliant.", "tokens": [51364, 1079, 11, 293, 300, 311, 10248, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1181, "seek": 462656, "start": 4648.56, "end": 4651.56, "text": " Thank you for coming up with Drop-Out.", "tokens": [51464, 1044, 291, 337, 1348, 493, 365, 17675, 12, 28353, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10709539234128773, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.001162709086202085}, {"id": 1182, "seek": 465156, "start": 4652.56, "end": 4655.56, "text": " Many of us here", "tokens": [50414, 5126, 295, 505, 510, 50564], "temperature": 0.0, "avg_logprob": -0.09127615947349399, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.03204505145549774}, {"id": 1183, "seek": 465156, "start": 4655.56, "end": 4658.56, "text": " are working in a regime of sparse data,", "tokens": [50564, 366, 1364, 294, 257, 13120, 295, 637, 11668, 1412, 11, 50714], "temperature": 0.0, "avg_logprob": -0.09127615947349399, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.03204505145549774}, {"id": 1184, "seek": 465156, "start": 4658.56, "end": 4660.56, "text": " and so we have a couple channels,", "tokens": [50714, 293, 370, 321, 362, 257, 1916, 9235, 11, 50814], "temperature": 0.0, "avg_logprob": -0.09127615947349399, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.03204505145549774}, {"id": 1185, "seek": 465156, "start": 4660.56, "end": 4662.56, "text": " a couple signals, a couple voxels,", "tokens": [50814, 257, 1916, 12354, 11, 257, 1916, 1650, 87, 1625, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09127615947349399, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.03204505145549774}, {"id": 1186, "seek": 465156, "start": 4662.56, "end": 4666.56, "text": " and you've convinced us that we need more,", "tokens": [50914, 293, 291, 600, 12561, 505, 300, 321, 643, 544, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09127615947349399, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.03204505145549774}, {"id": 1187, "seek": 465156, "start": 4666.56, "end": 4670.56, "text": " but is there a way forward in AI", "tokens": [51114, 457, 307, 456, 257, 636, 2128, 294, 7318, 51314], "temperature": 0.0, "avg_logprob": -0.09127615947349399, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.03204505145549774}, {"id": 1188, "seek": 465156, "start": 4670.56, "end": 4673.56, "text": " that can manage with more sparse data,", "tokens": [51314, 300, 393, 3067, 365, 544, 637, 11668, 1412, 11, 51464], "temperature": 0.0, "avg_logprob": -0.09127615947349399, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.03204505145549774}, {"id": 1189, "seek": 465156, "start": 4673.56, "end": 4675.56, "text": " or is this the only regime", "tokens": [51464, 420, 307, 341, 264, 787, 13120, 51564], "temperature": 0.0, "avg_logprob": -0.09127615947349399, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.03204505145549774}, {"id": 1190, "seek": 465156, "start": 4675.56, "end": 4677.56, "text": " that's going to be able to make success?", "tokens": [51564, 300, 311, 516, 281, 312, 1075, 281, 652, 2245, 30, 51664], "temperature": 0.0, "avg_logprob": -0.09127615947349399, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.03204505145549774}, {"id": 1191, "seek": 465156, "start": 4677.56, "end": 4680.56, "text": " So the really big successes", "tokens": [51664, 407, 264, 534, 955, 26101, 51814], "temperature": 0.0, "avg_logprob": -0.09127615947349399, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.03204505145549774}, {"id": 1192, "seek": 468056, "start": 4680.56, "end": 4682.56, "text": " have been on big databases,", "tokens": [50364, 362, 668, 322, 955, 22380, 11, 50464], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1193, "seek": 468056, "start": 4682.56, "end": 4685.56, "text": " and I think we should be using", "tokens": [50464, 293, 286, 519, 321, 820, 312, 1228, 50614], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1194, "seek": 468056, "start": 4685.56, "end": 4687.56, "text": " even bigger models,", "tokens": [50614, 754, 3801, 5245, 11, 50714], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1195, "seek": 468056, "start": 4687.56, "end": 4690.56, "text": " but you can't get away from the fact that actually,", "tokens": [50714, 457, 291, 393, 380, 483, 1314, 490, 264, 1186, 300, 767, 11, 50864], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1196, "seek": 468056, "start": 4690.56, "end": 4692.56, "text": " if you're going to have something", "tokens": [50864, 498, 291, 434, 516, 281, 362, 746, 50964], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1197, "seek": 468056, "start": 4692.56, "end": 4694.56, "text": " that starts off random", "tokens": [50964, 300, 3719, 766, 4974, 51064], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1198, "seek": 468056, "start": 4694.56, "end": 4696.56, "text": " and sucks all its knowledge from the data,", "tokens": [51064, 293, 15846, 439, 1080, 3601, 490, 264, 1412, 11, 51164], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1199, "seek": 468056, "start": 4696.56, "end": 4697.56, "text": " you'd better have enough data", "tokens": [51164, 291, 1116, 1101, 362, 1547, 1412, 51214], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1200, "seek": 468056, "start": 4697.56, "end": 4699.56, "text": " to suck all that knowledge from.", "tokens": [51214, 281, 9967, 439, 300, 3601, 490, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1201, "seek": 468056, "start": 4699.56, "end": 4701.56, "text": " The bigger your model, the better,", "tokens": [51314, 440, 3801, 428, 2316, 11, 264, 1101, 11, 51414], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1202, "seek": 468056, "start": 4701.56, "end": 4702.56, "text": " if you regularize it,", "tokens": [51414, 498, 291, 3890, 1125, 309, 11, 51464], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1203, "seek": 468056, "start": 4702.56, "end": 4704.56, "text": " but you still need a lot of data.", "tokens": [51464, 457, 291, 920, 643, 257, 688, 295, 1412, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1204, "seek": 468056, "start": 4704.56, "end": 4706.56, "text": " So the way you should think about it is this.", "tokens": [51564, 407, 264, 636, 291, 820, 519, 466, 309, 307, 341, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1205, "seek": 468056, "start": 4706.56, "end": 4708.56, "text": " If you've got 100,000 data points,", "tokens": [51664, 759, 291, 600, 658, 2319, 11, 1360, 1412, 2793, 11, 51764], "temperature": 0.0, "avg_logprob": -0.0822124200708726, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.024130553007125854}, {"id": 1206, "seek": 470856, "start": 4708.56, "end": 4710.56, "text": " that's small.", "tokens": [50364, 300, 311, 1359, 13, 50464], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1207, "seek": 470856, "start": 4710.56, "end": 4712.56, "text": " I know that's very depressing", "tokens": [50464, 286, 458, 300, 311, 588, 36355, 50564], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1208, "seek": 470856, "start": 4712.56, "end": 4713.56, "text": " if you're a neuroscientist.", "tokens": [50564, 498, 291, 434, 257, 28813, 5412, 468, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1209, "seek": 470856, "start": 4713.56, "end": 4714.56, "text": " Well, it's not depressing.", "tokens": [50614, 1042, 11, 309, 311, 406, 36355, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1210, "seek": 470856, "start": 4714.56, "end": 4716.56, "text": " It seems impossible.", "tokens": [50664, 467, 2544, 6243, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1211, "seek": 470856, "start": 4716.56, "end": 4718.56, "text": " If you want to personalize medicine", "tokens": [50764, 759, 291, 528, 281, 2973, 1125, 7195, 50864], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1212, "seek": 470856, "start": 4718.56, "end": 4719.56, "text": " for one individual", "tokens": [50864, 337, 472, 2609, 50914], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1213, "seek": 470856, "start": 4719.56, "end": 4721.56, "text": " and you want to train a model", "tokens": [50914, 293, 291, 528, 281, 3847, 257, 2316, 51014], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1214, "seek": 470856, "start": 4721.56, "end": 4723.56, "text": " on their data from their brain,", "tokens": [51014, 322, 641, 1412, 490, 641, 3567, 11, 51114], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1215, "seek": 470856, "start": 4723.56, "end": 4725.56, "text": " it seems like there's going to be a disconnect", "tokens": [51114, 309, 2544, 411, 456, 311, 516, 281, 312, 257, 14299, 51214], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1216, "seek": 470856, "start": 4725.56, "end": 4727.56, "text": " between what these models can do", "tokens": [51214, 1296, 437, 613, 5245, 393, 360, 51314], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1217, "seek": 470856, "start": 4727.56, "end": 4729.56, "text": " and how they might help someone in the future.", "tokens": [51314, 293, 577, 436, 1062, 854, 1580, 294, 264, 2027, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1218, "seek": 470856, "start": 4729.56, "end": 4730.56, "text": " Yes and no.", "tokens": [51414, 1079, 293, 572, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1219, "seek": 470856, "start": 4730.56, "end": 4732.56, "text": " If I train a model", "tokens": [51464, 759, 286, 3847, 257, 2316, 51564], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1220, "seek": 470856, "start": 4732.56, "end": 4734.56, "text": " on a very large number of people", "tokens": [51564, 322, 257, 588, 2416, 1230, 295, 561, 51664], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1221, "seek": 470856, "start": 4734.56, "end": 4736.56, "text": " and then apply that model to one person,", "tokens": [51664, 293, 550, 3079, 300, 2316, 281, 472, 954, 11, 51764], "temperature": 0.0, "avg_logprob": -0.06191330370695695, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.006090946029871702}, {"id": 1222, "seek": 473656, "start": 4736.56, "end": 4738.56, "text": " that's the form of personalize medicine", "tokens": [50364, 300, 311, 264, 1254, 295, 2973, 1125, 7195, 50464], "temperature": 0.0, "avg_logprob": -0.19922115619365985, "compression_ratio": 1.4183006535947713, "no_speech_prob": 0.0016719826962798834}, {"id": 1223, "seek": 473656, "start": 4738.56, "end": 4740.56, "text": " that really works.", "tokens": [50464, 300, 534, 1985, 13, 50564], "temperature": 0.0, "avg_logprob": -0.19922115619365985, "compression_ratio": 1.4183006535947713, "no_speech_prob": 0.0016719826962798834}, {"id": 1224, "seek": 473656, "start": 4740.56, "end": 4742.56, "text": " Great, thanks.", "tokens": [50564, 3769, 11, 3231, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19922115619365985, "compression_ratio": 1.4183006535947713, "no_speech_prob": 0.0016719826962798834}, {"id": 1225, "seek": 473656, "start": 4751.56, "end": 4753.56, "text": " Oh, and say hi to Dale for me.", "tokens": [51114, 876, 11, 293, 584, 4879, 281, 31329, 337, 385, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19922115619365985, "compression_ratio": 1.4183006535947713, "no_speech_prob": 0.0016719826962798834}, {"id": 1226, "seek": 473656, "start": 4758.56, "end": 4761.56, "text": " Thank you for the presentation.", "tokens": [51464, 1044, 291, 337, 264, 5860, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19922115619365985, "compression_ratio": 1.4183006535947713, "no_speech_prob": 0.0016719826962798834}, {"id": 1227, "seek": 473656, "start": 4761.56, "end": 4763.56, "text": " My last question is about this dropout.", "tokens": [51614, 1222, 1036, 1168, 307, 466, 341, 3270, 346, 13, 51714], "temperature": 0.0, "avg_logprob": -0.19922115619365985, "compression_ratio": 1.4183006535947713, "no_speech_prob": 0.0016719826962798834}, {"id": 1228, "seek": 473656, "start": 4763.56, "end": 4765.56, "text": " The thing is that you randomly just drop", "tokens": [51714, 440, 551, 307, 300, 291, 16979, 445, 3270, 51814], "temperature": 0.0, "avg_logprob": -0.19922115619365985, "compression_ratio": 1.4183006535947713, "no_speech_prob": 0.0016719826962798834}, {"id": 1229, "seek": 476556, "start": 4765.56, "end": 4767.56, "text": " some parts of the network", "tokens": [50364, 512, 3166, 295, 264, 3209, 50464], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1230, "seek": 476556, "start": 4767.56, "end": 4769.56, "text": " and then you say, okay,", "tokens": [50464, 293, 550, 291, 584, 11, 1392, 11, 50564], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1231, "seek": 476556, "start": 4769.56, "end": 4771.56, "text": " that it works better so I would accept it.", "tokens": [50564, 300, 309, 1985, 1101, 370, 286, 576, 3241, 309, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1232, "seek": 476556, "start": 4771.56, "end": 4773.56, "text": " But do we have any, like, intuition", "tokens": [50664, 583, 360, 321, 362, 604, 11, 411, 11, 24002, 50764], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1233, "seek": 476556, "start": 4773.56, "end": 4775.56, "text": " why, for example, some parts of it work better", "tokens": [50764, 983, 11, 337, 1365, 11, 512, 3166, 295, 309, 589, 1101, 50864], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1234, "seek": 476556, "start": 4775.56, "end": 4777.56, "text": " or if we try to embed this, like,", "tokens": [50864, 420, 498, 321, 853, 281, 12240, 341, 11, 411, 11, 50964], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1235, "seek": 476556, "start": 4777.56, "end": 4779.56, "text": " network into, like,", "tokens": [50964, 3209, 666, 11, 411, 11, 51064], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1236, "seek": 476556, "start": 4779.56, "end": 4781.56, "text": " is it more graph isomorphism?", "tokens": [51064, 307, 309, 544, 4295, 307, 32702, 1434, 30, 51164], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1237, "seek": 476556, "start": 4781.56, "end": 4783.56, "text": " What are these two different graphs,", "tokens": [51164, 708, 366, 613, 732, 819, 24877, 11, 51264], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1238, "seek": 476556, "start": 4783.56, "end": 4785.56, "text": " different graphs that we took?", "tokens": [51264, 819, 24877, 300, 321, 1890, 30, 51364], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1239, "seek": 476556, "start": 4785.56, "end": 4787.56, "text": " Are there any similarities", "tokens": [51364, 2014, 456, 604, 24197, 51464], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1240, "seek": 476556, "start": 4787.56, "end": 4789.56, "text": " between them or just with randomly?", "tokens": [51464, 1296, 552, 420, 445, 365, 16979, 30, 51564], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1241, "seek": 476556, "start": 4789.56, "end": 4791.56, "text": " I guess the problem with the randomness,", "tokens": [51564, 286, 2041, 264, 1154, 365, 264, 4974, 1287, 11, 51664], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1242, "seek": 476556, "start": 4791.56, "end": 4793.56, "text": " I guess we are trying to put", "tokens": [51664, 286, 2041, 321, 366, 1382, 281, 829, 51764], "temperature": 0.0, "avg_logprob": -0.15235041865596063, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.018094360828399658}, {"id": 1243, "seek": 479356, "start": 4793.56, "end": 4795.56, "text": " the burden of prediction", "tokens": [50364, 264, 12578, 295, 17630, 50464], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1244, "seek": 479356, "start": 4795.56, "end": 4797.56, "text": " on the random part of the computer desk.", "tokens": [50464, 322, 264, 4974, 644, 295, 264, 3820, 10026, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1245, "seek": 479356, "start": 4797.56, "end": 4799.56, "text": " So I didn't hear", "tokens": [50564, 407, 286, 994, 380, 1568, 50664], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1246, "seek": 479356, "start": 4799.56, "end": 4801.56, "text": " the whole question,", "tokens": [50664, 264, 1379, 1168, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1247, "seek": 479356, "start": 4801.56, "end": 4803.56, "text": " but certainly in dropout what we do", "tokens": [50764, 457, 3297, 294, 3270, 346, 437, 321, 360, 50864], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1248, "seek": 479356, "start": 4803.56, "end": 4805.56, "text": " is we randomly leave out units.", "tokens": [50864, 307, 321, 16979, 1856, 484, 6815, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1249, "seek": 479356, "start": 4805.56, "end": 4807.56, "text": " Now you can also do block dropout.", "tokens": [50964, 823, 291, 393, 611, 360, 3461, 3270, 346, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1250, "seek": 479356, "start": 4807.56, "end": 4809.56, "text": " You can take groups of units", "tokens": [51064, 509, 393, 747, 3935, 295, 6815, 51164], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1251, "seek": 479356, "start": 4809.56, "end": 4811.56, "text": " and randomly leave out the groups.", "tokens": [51164, 293, 16979, 1856, 484, 264, 3935, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1252, "seek": 479356, "start": 4811.56, "end": 4813.56, "text": " And what that does is it allows the units", "tokens": [51264, 400, 437, 300, 775, 307, 309, 4045, 264, 6815, 51364], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1253, "seek": 479356, "start": 4813.56, "end": 4815.56, "text": " within a group to collaborate with one another,", "tokens": [51364, 1951, 257, 1594, 281, 18338, 365, 472, 1071, 11, 51464], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1254, "seek": 479356, "start": 4815.56, "end": 4817.56, "text": " and then between groups", "tokens": [51464, 293, 550, 1296, 3935, 51564], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1255, "seek": 479356, "start": 4817.56, "end": 4819.56, "text": " they have to be fairly independent.", "tokens": [51564, 436, 362, 281, 312, 6457, 6695, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1256, "seek": 479356, "start": 4819.56, "end": 4821.56, "text": " And that's called block dropout", "tokens": [51664, 400, 300, 311, 1219, 3461, 3270, 346, 51764], "temperature": 0.0, "avg_logprob": -0.10638369750976563, "compression_ratio": 1.754863813229572, "no_speech_prob": 0.0046386877074837685}, {"id": 1257, "seek": 482156, "start": 4821.56, "end": 4823.56, "text": " but I didn't really hear", "tokens": [50364, 457, 286, 994, 380, 534, 1568, 50464], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1258, "seek": 482156, "start": 4823.56, "end": 4825.56, "text": " the rest of your question.", "tokens": [50464, 264, 1472, 295, 428, 1168, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1259, "seek": 482156, "start": 4825.56, "end": 4827.56, "text": " Okay.", "tokens": [50564, 1033, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1260, "seek": 482156, "start": 4827.56, "end": 4829.56, "text": " The question was about", "tokens": [50664, 440, 1168, 390, 466, 50764], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1261, "seek": 482156, "start": 4829.56, "end": 4831.56, "text": " that, okay, imagine that you...", "tokens": [50764, 300, 11, 1392, 11, 3811, 300, 291, 485, 50864], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1262, "seek": 482156, "start": 4831.56, "end": 4833.56, "text": " Can you talk closer to the microphone", "tokens": [50864, 1664, 291, 751, 4966, 281, 264, 10952, 50964], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1263, "seek": 482156, "start": 4833.56, "end": 4835.56, "text": " because I'm partially dead?", "tokens": [50964, 570, 286, 478, 18886, 3116, 30, 51064], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1264, "seek": 482156, "start": 4835.56, "end": 4837.56, "text": " The question is that", "tokens": [51064, 440, 1168, 307, 300, 51164], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1265, "seek": 482156, "start": 4837.56, "end": 4839.56, "text": " imagine that you have a dropout of 50%", "tokens": [51164, 3811, 300, 291, 362, 257, 3270, 346, 295, 2625, 4, 51264], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1266, "seek": 482156, "start": 4839.56, "end": 4841.56, "text": " and you're trying to get rid", "tokens": [51264, 293, 291, 434, 1382, 281, 483, 3973, 51364], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1267, "seek": 482156, "start": 4841.56, "end": 4843.56, "text": " of, like, 50% of your nodes", "tokens": [51364, 295, 11, 411, 11, 2625, 4, 295, 428, 13891, 51464], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1268, "seek": 482156, "start": 4843.56, "end": 4845.56, "text": " and the nodes in the network.", "tokens": [51464, 293, 264, 13891, 294, 264, 3209, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1269, "seek": 482156, "start": 4845.56, "end": 4847.56, "text": " And the question is, okay,", "tokens": [51564, 400, 264, 1168, 307, 11, 1392, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1270, "seek": 482156, "start": 4847.56, "end": 4849.56, "text": " whether we have any similarity between the types", "tokens": [51664, 1968, 321, 362, 604, 32194, 1296, 264, 3467, 51764], "temperature": 0.0, "avg_logprob": -0.11559210532952931, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.003206313820555806}, {"id": 1271, "seek": 484956, "start": 4849.56, "end": 4851.56, "text": " if we do it iteratively,", "tokens": [50364, 498, 321, 360, 309, 17138, 19020, 11, 50464], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1272, "seek": 484956, "start": 4851.56, "end": 4853.56, "text": " whether we would find any similarity", "tokens": [50464, 1968, 321, 576, 915, 604, 32194, 50564], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1273, "seek": 484956, "start": 4853.56, "end": 4855.56, "text": " on the structure of the network", "tokens": [50564, 322, 264, 3877, 295, 264, 3209, 50664], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1274, "seek": 484956, "start": 4855.56, "end": 4857.56, "text": " that would produce the best results", "tokens": [50664, 300, 576, 5258, 264, 1151, 3542, 50764], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1275, "seek": 484956, "start": 4857.56, "end": 4859.56, "text": " and if it's so, whether it would correspond", "tokens": [50764, 293, 498, 309, 311, 370, 11, 1968, 309, 576, 6805, 50864], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1276, "seek": 484956, "start": 4859.56, "end": 4861.56, "text": " to something physical, like, for example,", "tokens": [50864, 281, 746, 4001, 11, 411, 11, 337, 1365, 11, 50964], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1277, "seek": 484956, "start": 4861.56, "end": 4863.56, "text": " if you're doing a vision thing,", "tokens": [50964, 498, 291, 434, 884, 257, 5201, 551, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1278, "seek": 484956, "start": 4863.56, "end": 4865.56, "text": " whether it would correspond to something in brain or not, I guess.", "tokens": [51064, 1968, 309, 576, 6805, 281, 746, 294, 3567, 420, 406, 11, 286, 2041, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1279, "seek": 484956, "start": 4865.56, "end": 4867.56, "text": " Yeah.", "tokens": [51164, 865, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1280, "seek": 484956, "start": 4867.56, "end": 4869.56, "text": " Lots of people have thought about whether you can do better", "tokens": [51264, 15908, 295, 561, 362, 1194, 466, 1968, 291, 393, 360, 1101, 51364], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1281, "seek": 484956, "start": 4869.56, "end": 4871.56, "text": " than random in dropout.", "tokens": [51364, 813, 4974, 294, 3270, 346, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1282, "seek": 484956, "start": 4871.56, "end": 4873.56, "text": " And there's some work on that, like,", "tokens": [51464, 400, 456, 311, 512, 589, 322, 300, 11, 411, 11, 51564], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1283, "seek": 484956, "start": 4873.56, "end": 4875.56, "text": " block dropout that works, can work for some things.", "tokens": [51564, 3461, 3270, 346, 300, 1985, 11, 393, 589, 337, 512, 721, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1284, "seek": 484956, "start": 4875.56, "end": 4877.56, "text": " But I don't really have much to say", "tokens": [51664, 583, 286, 500, 380, 534, 362, 709, 281, 584, 51764], "temperature": 0.0, "avg_logprob": -0.09651807889546433, "compression_ratio": 1.8116438356164384, "no_speech_prob": 0.0023294894490391016}, {"id": 1285, "seek": 487756, "start": 4877.56, "end": 4879.56, "text": " about...", "tokens": [50364, 466, 485, 50464], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1286, "seek": 487756, "start": 4879.56, "end": 4881.56, "text": " I don't really know the answer to", "tokens": [50464, 286, 500, 380, 534, 458, 264, 1867, 281, 50564], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1287, "seek": 487756, "start": 4881.56, "end": 4883.56, "text": " sort of, is there something much", "tokens": [50564, 1333, 295, 11, 307, 456, 746, 709, 50664], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1288, "seek": 487756, "start": 4883.56, "end": 4885.56, "text": " more sensible than dropout", "tokens": [50664, 544, 25380, 813, 3270, 346, 50764], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1289, "seek": 487756, "start": 4885.56, "end": 4887.56, "text": " that's a lot more structured?", "tokens": [50764, 300, 311, 257, 688, 544, 18519, 30, 50864], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1290, "seek": 487756, "start": 4887.56, "end": 4889.56, "text": " There might well be, but I...", "tokens": [50864, 821, 1062, 731, 312, 11, 457, 286, 485, 50964], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1291, "seek": 487756, "start": 4889.56, "end": 4891.56, "text": " Thank you.", "tokens": [50964, 1044, 291, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1292, "seek": 487756, "start": 4891.56, "end": 4893.56, "text": " Okay, so with that, we're going to have to end.", "tokens": [51064, 1033, 11, 370, 365, 300, 11, 321, 434, 516, 281, 362, 281, 917, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1293, "seek": 487756, "start": 4893.56, "end": 4895.56, "text": " So please join me in", "tokens": [51164, 407, 1767, 3917, 385, 294, 51264], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1294, "seek": 487756, "start": 4895.56, "end": 4897.56, "text": " thanking Jeff for...", "tokens": [51264, 30830, 7506, 337, 485, 51364], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1295, "seek": 487756, "start": 4897.56, "end": 4899.56, "text": " Thank you.", "tokens": [51364, 1044, 291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1296, "seek": 487756, "start": 4899.56, "end": 4901.56, "text": " APPLAUSE", "tokens": [51464, 35298, 51564], "temperature": 0.0, "avg_logprob": -0.13444183718773625, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.0037659110967069864}, {"id": 1297, "seek": 490156, "start": 4901.56, "end": 4903.56, "text": " And", "tokens": [50364, 400, 50464], "temperature": 0.0, "avg_logprob": -0.1348183552424113, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.014620860107243061}, {"id": 1298, "seek": 490156, "start": 4903.56, "end": 4905.56, "text": " I wanted also", "tokens": [50464, 286, 1415, 611, 50564], "temperature": 0.0, "avg_logprob": -0.1348183552424113, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.014620860107243061}, {"id": 1299, "seek": 490156, "start": 4905.56, "end": 4907.56, "text": " to thank Blake", "tokens": [50564, 281, 1309, 23451, 50664], "temperature": 0.0, "avg_logprob": -0.1348183552424113, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.014620860107243061}, {"id": 1300, "seek": 490156, "start": 4907.56, "end": 4909.56, "text": " for hosting this event,", "tokens": [50664, 337, 16058, 341, 2280, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1348183552424113, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.014620860107243061}, {"id": 1301, "seek": 490156, "start": 4909.56, "end": 4911.56, "text": " and I felt the questions", "tokens": [50764, 293, 286, 2762, 264, 1651, 50864], "temperature": 0.0, "avg_logprob": -0.1348183552424113, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.014620860107243061}, {"id": 1302, "seek": 490156, "start": 4911.56, "end": 4913.56, "text": " could have gone on all night,", "tokens": [50864, 727, 362, 2780, 322, 439, 1818, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1348183552424113, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.014620860107243061}, {"id": 1303, "seek": 490156, "start": 4913.56, "end": 4915.56, "text": " but the tip-off isn't half an hour,", "tokens": [50964, 457, 264, 4125, 12, 4506, 1943, 380, 1922, 364, 1773, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1348183552424113, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.014620860107243061}, {"id": 1304, "seek": 490156, "start": 4915.56, "end": 4917.56, "text": " so some of us have to move on.", "tokens": [51064, 370, 512, 295, 505, 362, 281, 1286, 322, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1348183552424113, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.014620860107243061}, {"id": 1305, "seek": 490156, "start": 4917.56, "end": 4919.56, "text": " So thank you, Blake.", "tokens": [51164, 407, 1309, 291, 11, 23451, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1348183552424113, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.014620860107243061}, {"id": 1306, "seek": 490156, "start": 4919.56, "end": 4921.56, "text": " APPLAUSE", "tokens": [51264, 35298, 51364], "temperature": 0.0, "avg_logprob": -0.1348183552424113, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.014620860107243061}], "language": "en"}