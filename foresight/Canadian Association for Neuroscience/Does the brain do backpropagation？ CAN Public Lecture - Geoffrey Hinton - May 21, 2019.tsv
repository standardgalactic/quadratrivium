start	end	text
0	4720	Hi, I think we're ready to start, so my name is Paul Franklin and I'm a Neuroscientist
4720	9120	here at Sick Kids, and also I'm Program Chair for the Canadian Neuroscience Meeting that
9120	12680	takes place in Toronto all this week.
12680	19080	So the traditional curtain raiser for the Neuroscience Meeting, the CAN meeting, is the public lecture.
19080	25760	And this year we decided to focus on the interface between neuroscience and AI.
25760	27760	We did that for two reasons.
27760	33520	The first reason is that AI, or Toronto, is one of the main hubs in the world for AI research.
33520	39320	And the second reason is that it's also home to one of the true pioneers in this field,
39320	42800	also known as the godfather of deep learning, Jeff Hinton.
42800	48080	And so when we asked Jeff if he'd participate in this event, I think a year and a half ago
48080	51600	we asked him and he said, yes, we were super excited.
51600	58000	So at this point I want to hand over to Blake Richards, and Blake Richards is an associate
58000	63240	professor at University of Toronto Scarborough, and he's going to host this evening's event.
63240	64240	Blake?
64240	66000	Thanks, Paul.
66000	72040	So just as a brief introduction, I wanted to tell you a little bit more about Jeff.
72040	78640	So you might be surprised to learn that the godfather of deep learning, associated mostly
78680	85560	with AI, got his BA in experimental psychology from Cambridge originally.
85560	92840	He then went on to do his PhD in artificial intelligence in Edinburgh, so he did get
92840	95000	started relatively early.
95000	101040	But throughout his early career, he really contributed to the first wave of what was
101040	106520	known at the time as parallel distributed processing or connectionism, which really brought back
106520	113520	into the fore the idea of using neural networks for both models of the mind and for artificial
113520	115360	intelligence.
115360	123000	Now Jeff got his first tenure track position at Carnegie Mellon in the 80s, but we were
123000	129480	able to steal him away from them in the late 80s, which I understand was largely because
129480	132560	of his ethical objections to DARPA funding.
132560	142040	So once again, Canada's political bent has helped us in our research endeavors.
142040	147320	Over the course of the 90s, Jeff continued to really push neural networks and machine
147320	149320	learning forward.
149320	151200	And sorry about that.
151200	156320	In 98, he actually went to University College London to found the Gatsby computational
156320	162480	neuroscience unit, and we might have lost him, but thankfully we pulled him back again.
162480	168360	He came back to Toronto in 2001, where he became a university professor in 2006 and
168360	171600	then an emeritus professor in 2014.
171600	179080	Now I think that you all know that Jeff is a monumental figure within artificial intelligence
179080	185040	and machine learning, and he's been critical to the founding of the Vector Institute here
185040	189400	in Toronto and putting Toronto on the map for AI.
189400	194440	And certainly he's had incredible recognitions of his work, most recently the Turing Award,
194440	200640	which he shared with Joshua Bengio and Yann LeCun, as well as the Order of Canada.
200640	205200	And he's a distinguished fellow of the Canadian Institute for Advanced Research, which I highlight
205200	210520	because they were one of the people who continued to support neural networks throughout the
210520	213440	time when it wasn't as faddish.
213440	217440	But for all his successes and his technical endeavors, I think one of the things that's
217440	222360	most important to understand about Jeff is the impact that he's had on other scientists.
222360	226840	Jeff has really molded the career of so many people and changed the way that they think
226840	229560	about things.
229560	234560	When you look at the people who have been his graduate students or postdocs, it really
234560	237840	is the who's who in artificial intelligence.
237840	247320	It includes people like Max Welling, Yann LeCun, and you know, the phrase I used to describe
247400	252160	it is, have you drunk Jeff's Kool-Aid?
252160	255360	Because once you've drunk Jeff's Kool-Aid, there is no going back.
255360	262200	You see neural networks, you see AI differently, and I would argue you also see neuroscience
262200	263480	differently.
263480	271320	And for me, my understanding of the brain has been largely shaped by Jeff and his work.
271320	276080	But you know, we're at the point now where computer science has drunk Jeff's Kool-Aid.
276080	282160	So he's got an H index of 145, and according to Google Scholar, his work has been cited
282160	289040	270,000 times, which is more than Einstein, Ramoni, Cajal, and Alan Tern combined.
289040	291520	But that's largely from computer scientists.
291520	298040	And if my prediction is correct, neuroscience 30, 40 years from now will also have drunk
298040	301480	Jeff's Kool-Aid, and maybe you're going to get your first taste tonight.
301480	304040	So with that, I hand you over to Jeffrey Hinton.
306080	314920	So thank you very much, Blake.
314920	317920	I can give you some more Kool-Aid today.
317920	321360	It's Kool-Aid produced by one of my former students, Ilya Sutskava.
321360	326480	First, I want to tell you a little bit about the history of deep learning in AI.
326480	330600	Can I just ask before I start, how many people here know what the back propagation algorithm
330600	331600	is?
331600	333880	Put your hands up.
333880	335680	So some people don't.
335680	339640	I'll explain it very quickly, and I'll explain it in such a way that you'll be able to explain
339640	340640	to other people.
340640	343960	So if you do know what it is, you follow the explanation from the point of view of how
343960	344960	you explain it.
344960	345960	Okay.
345960	348320	There was a war between two paradigms for AI.
348320	353320	There were people who thought that the essence of intelligence was reasoning, and logic is
353320	358880	what does reasoning, so we should base artificial intelligence on taking strings of symbols and
358880	362360	manipulating them to arrive at conclusions.
362360	365840	And then there were other people who looked at the brain and said, no, no, intelligence
365840	370200	is all about adapting connections in the brain to get smarter.
370200	377080	And this war went on for a long time, and eventually, people who were trying to figure
377080	383880	out how to change connections between fake neurons to make these networks smarter got
383880	388920	to be able to do things that the people doing symbolic AI just couldn't do at all.
388920	393320	And now there's a different way of getting a computer to do what you want.
393320	398240	Instead of programming it, which is tedious, you just showed examples, and it figures it
398240	399240	out.
399240	402440	Now, of course, you have to write the program that figures it out, but that's just one program
402440	405960	that will then do everything.
405960	410000	And this is an example of what it can do.
410000	412880	So the image, just think of the numbers.
412880	417200	They're RGB values of pixels, and that's the input to the computer.
417200	422640	Lots of values of pixels, just real numbers, saying how bright the red channel is.
422640	427960	And you have to turn those numbers into a string of words that says a close-up of a
427960	429920	child holding a stuffed animal.
429920	431440	And imagine writing that program.
431440	434960	Well, people in conventional AI had tried to write that program and they couldn't, partly
434960	437760	because they didn't know how we did it.
437760	441760	We still don't know how we do it, but we can get artificial neural networks to do it now
441760	444320	and do a pretty good job.
444440	451160	My prediction is, within 10 years, if you go and get a CT scan, what will happen is
451160	456840	a computer will look at the CT scan, and a computer will produce the written report
456840	459960	that the radiologist currently produces.
459960	463120	Radiologists don't like this idea.
463120	464120	Okay.
464120	466120	Here's a simplified model of a neuron.
466120	467120	It's very simple.
467120	471520	It gets some input, which is just the activity on the input lines times the weights, adds
471520	472520	it all up.
472720	474560	That's called the depolarization.
474560	480960	And then it gives an output that's proportional to how much input it gets as long as it gets
480960	482480	enough input.
482480	484360	And so, to begin with, we won't have spiking neurons.
484360	488960	These are just going to be neurons that send real values in just the way neurons don't.
488960	493880	We're going to make networks of them by hooking them up into layers.
493880	496880	And you could put some pixels on the input neurons.
496880	497880	Look.
497880	501080	They're the input neurons.
501080	505440	And you go forwards through the net until you get outputs.
505440	508200	And then you compare those outputs with what you ought to have got, so you have to know
508200	510440	what the right answer is.
510440	516400	And what we'd like to do is train the weights, these red and green dots, so that it gives
516400	517400	the right output.
517400	522360	Now, I'm going to show you a way of training the weights that everybody can understand,
522360	527120	and everybody is thought of, basically.
527120	534280	What you do is you start with random weights, you show it some inputs, you measure how well
534280	538200	it does, then you change one weight a tiny bit.
538200	542680	So I take that weight there, and I just change it a tiny bit, and then I show it the same
542680	545280	inputs again and see if it does better or worse.
545280	546680	If it does better, I keep the change.
546680	550920	If it does worse, maybe I keep the change in the opposite direction.
550920	552400	That's an easy algorithm to understand.
552400	553400	And that algorithm works.
553400	554400	It's just incredibly slow.
554400	559480	You have to show it lots of examples, change your weight, and then show it lots more examples,
559480	560480	change another weight.
560480	562480	And every weight has to be changed many times.
562480	568840	So if you use calculus, you can go millions of times faster.
568840	572320	So the trick of this algorithm, the sort of mutation algorithm, is you have to measure
572320	574600	the effect of the weight change on the performance.
574600	582080	But you don't really need to measure it, because when I change one of these weights, the effect
582080	585440	that it has on the output is determined by the network.
585440	587600	It just depends on the other weights in the network.
587600	591000	It's not like normal evolution, where the effect of a gene depends on the environment
591000	592000	you're in.
592000	594000	This is all kind of internal to the brain.
594000	597720	And so changing one of these weights has an effect that's predictable here.
597720	603160	So I ought to be able to predict how changing the weight will help get the right output.
603160	608920	And so what back propagation does is basically says, I'm going to compute using an algorithm,
608920	612400	the details of which I won't tell you, and compute for every weight, all at the same
612400	617320	time, how changing that weight would improve the output.
617320	619920	And then I'm going to change all the weights a little bit.
619920	622840	So every weight changes in direction to improve the output, and the output improves quite
622840	626440	a bit, and then I do it all again.
626440	633760	Now that allows me to compute for every weight what direction I'd like to change it in.
633760	638560	And the question is, should I, when I show examples, show all of the examples, and then
638560	641960	update the weights, so should you live your whole life with the synapse strengths you're
641960	645600	born with, then update your weights a little bit, then live your life again, and update
645600	654120	the weights a little bit more, that doesn't seem very good, or should you take one case
654120	658120	or a few cases, figure out how you'd like to update the weights, update them, and then
658120	659120	take more cases.
659120	662960	That's the online algorithm, and that's what we do.
662960	664920	And the amazing thing is, it works.
664920	668120	You can take one case at a time, or you can take small batch of cases, you update the
668120	671640	weights, and these networks get better.
671640	675200	And it's very surprising how well it works on big data sets.
675200	680880	So for a long time, people thought, you're never going to be able to learn something
680880	686560	complicated, like, for example, take a string of words in English, feed them into a neural
686560	691320	net, and output a string of words in French that mean the same thing.
691320	693600	You're never going to be able to do that if you start with a big neural net with just
693600	694600	random weights.
694600	698080	It's just asking too much for the neural net to organize itself so it can do transactions
699040	703120	because you have to kind of understand what the English says.
703120	707920	And people predicted this was completely impossible, but you'd have to put in lots of prior knowledge.
707920	712680	Well, they were wrong.
712680	720200	So in 2009, my students in Toronto showed that you could actually improve speech recognizers
720200	722800	using these neural nets that had random weights.
722800	727520	They were just trying to predict in a spectrogram which piece of which phoneme you were trying
727520	729760	to say in the middle of the spectrogram.
729760	733440	And then there was more to the system that wasn't neural nets.
733440	737640	Now what we've done is we've got rid of all the stuff that wasn't neural nets, and now
737640	743960	you can take sound waves coming in and you have transcriptions coming out, or even better,
743960	747600	you have sound waves coming in and you have sound waves coming out in another language
747600	749960	with the same accent.
749960	751800	They can do that now.
751800	754000	That's speech recognition done.
754000	760040	And in 2012, two of my students took a big database of images and used essentially the
760040	765960	same algorithm, the few clever tricks, to say what was in the image.
765960	771320	Not a full caption, just the class of the most obvious object, and they did much better
771320	775720	than conventional computer vision, which had been going for many years, and since then
775720	778600	all the best recognizers have used neural nets.
778600	783880	In 2011, you couldn't publish a paper out neural nets in the standard computer vision
783880	786240	conference because they said they were rubbish.
786240	791720	In 2014, you couldn't publish a paper that wasn't about neural nets.
791720	795000	And in 2014, they did something that I didn't expect.
795000	800120	This was done by people at Google, not me, and Joshua Benjo in his group in Montreal,
800120	804000	particularly by a guy called Bardenao and Cho.
804000	811360	They managed to get a neural net, so you feed in actually fragments of words in one language.
811360	813560	You have 32,000 possible fragments.
813560	816960	So the word the in English would be one of the fragments, but so with things like ing
816960	818760	and un.
818760	822360	And what comes out in another language is fragments of words in that other language, and it's
822360	826800	a pretty good translation, and that's how Google now does translation.
826800	833240	So it did translation better than symbolic AI.
833240	836560	So what changed between 1986 and 2009?
836560	838520	And it was basically computers got faster.
838520	839520	That was the main change.
839520	840960	Data sets got bigger.
840960	845440	We developed some clever tricks, and we like to emphasize those, but it was really the
845440	847480	computers getting faster and data sets getting bigger.
847480	850520	But I'll emphasize the clever tricks nonetheless.
850520	851920	And I can tell you about two clever tricks.
851920	855280	I can tell you about transformers, and I can tell you about better ways of stopping neural
855280	859320	networks from overfitting.
859320	865520	But first I want to show you an example of what neural nets can do now.
865520	872400	So a team at OpenAI took work on transformers that was originally done at Google.
872400	876440	They developed it a little bit further, and they applied it to big neural nets that have
876440	881480	1.5 billion learnable connection strengths.
881480	883560	So they're learning 1.5 billion numbers.
883560	885280	That's the knowledge of the system.
885280	890920	And they train it up on billions of words of English text, and all the net's trying
890920	893920	to do is predict the next word.
893920	898800	So what the net will do, or fragment of word, the net will give you probabilities for the
898800	899800	next word.
899800	903960	So if you give it some words, a lead-in, it'll give you probabilities for the next word.
903960	908040	And once the net's trained, what you can do is you can look at those probabilities,
908040	913800	and if it says there's a probability of 0.4 that the next word is the, you pick the with
913800	919080	probability 9.4, and if it says fish with probability 0.01, you pick fish with probability
919080	920080	0.01.
920080	924160	And so you just pick from its distribution, and then you tell the neural net, okay, the
924160	927840	one I picked was the next word, what do you think comes after that?
927840	931920	And this way you can get it to sort of reveal what it really believes about the world.
931920	934840	So you're getting it to predict words one at a time, and every time it makes a prediction
934840	939360	you say you were right, and it just gets more and more carried away.
939360	945520	So they initiated it with some interesting text.
945520	949560	And the question is, will the neural net then produce stuff that's sort of related to that?
949560	952400	I mean, the first question is, will it produce English words?
952400	954520	Will the words have decent syntax?
954520	956000	Will it have any meaning?
956000	958280	Will it be related to this?
958280	962560	If you're really optimistic, you might say, will they sort of relate to the fundamental
962560	965400	problem here, which is how these unicorns can speak English?
965400	967360	Okay, so here goes.
967360	968960	This is what the neural net produced.
968960	970240	Now this was cherry-picked.
970240	975360	This was one of their better examples.
975360	981720	The neural net just made this up, right?
981720	987560	It made up Dr. Jorge Perez, there is no such person at the University of La Paz, but it's
987560	992160	pretty plausible because it's South America, and I believe La Paz has a university.
992160	997960	Okay, so that's the first bit of what it made up, and it carries on and it gets better.
998960	1002960	The next bit sounds a bit like one of those fantasy games.
1010960	1013440	So it's remembered about unicorns and herds of unicorns, right?
1013440	1016880	So they walk up and there's this strange valley, and it's a very strange valley, and they found
1016880	1023360	the herds of unicorns.
1023360	1026040	And it has something about seeing them from the air and being able to touch them, which
1026040	1027320	isn't quite right.
1027320	1030880	So people in Symbolicae leap on this and say, you see, it doesn't understand.
1030880	1036080	Well, sure, there's little bits that it doesn't get right.
1036080	1040400	But notice, it's remembered that these unicorns have to speak English, and so it tells you
1040400	1045560	about, you know, they spoke some fairly regular English.
1045560	1050200	It doesn't know the difference between dialect and dialectic, but my kids don't know that
1050200	1051200	either.
1051200	1057280	In fact, I'm not sure I know.
1057280	1064800	It's a tribute to unicorns to Argentina, even though Dr. Perez comes from Bolivia.
1064800	1068000	And it actually understands about magic realism.
1068000	1074720	So they're descendants of a lost race, and I love the bit at the end where it says, in
1074720	1079120	South America, such incidents seem to be quite common.
1079120	1084600	This has an ability to just make up something that fits your prejudices and sounds moderately
1084600	1092760	plausible, like a certain president.
1092760	1097560	And it finally gets to the point which is, if you really want to know whether these unicorns
1097560	1103520	were used by breeding with these strange lost race of people, you ought to do a DNA test.
1103520	1104520	Okay?
1104520	1106000	It understands that.
1106000	1107000	Okay.
1107000	1110440	So that's what neural nets can do now.
1110440	1118120	This was a neural net with 1.5 billion connections that was trained on Google's, actually, I
1118120	1124280	withdraw that, 1.5 billion connections is trained on a lot of hardware.
1124280	1130160	And we look at what it says, and we sort of laugh at how, you know, it's pretty good,
1130160	1132800	but it hasn't got it quite right, but it's pretty good.
1132800	1133800	Okay.
1133800	1139720	What they've done now is they've trained a neural net with 50 billion connections on
1139720	1145720	Google's latest cloud hardware, which is, it's like having several of the world's biggest
1145720	1149840	supercomputers going for you for months.
1149840	1154520	The net with 50 billion connections, I haven't seen any text from it yet, but my prediction
1154520	1159600	is it's sitting around laughing at how cute what we produce is.
1159600	1161160	Okay.
1161160	1169240	So one thing about that net is it's clearly very well aware of the initial context.
1169240	1174720	These unicorns in a valley that speak English, and it's remembering this initial context
1174720	1177480	a long time later, and a recurrent neural net can't do that.
1177480	1180480	A recurrent neural net would have forgotten about the initial stuff and wouldn't produce
1180480	1183560	such good context-dependent stuff.
1183560	1192040	So the way this works is the word comes in, the neural net activates some hidden units.
1192040	1197520	That pattern of activity in the hidden units goes and compares itself with previous patterns,
1197960	1202560	your point of view, with previous patterns at earlier times.
1202560	1206720	And when it finds a pattern at an earlier time that's a bit similar, it says that we'll
1206720	1211880	take advice from that previous hidden pattern about how to affect the next layer.
1211880	1219640	And so actually a word comes in and how one pattern of activity in the bottom layer of
1219640	1226800	the hidden neurons affects the next layer is dependent on what happened previously.
1226800	1231040	Now it's dependent in quite a complicated way, and this seems very implausible for a
1231040	1236800	brain because what's happening in the computer is you're storing all these activity patterns
1236800	1239800	that are meant to be neural activity patterns or light neural activity patterns, and you're
1239800	1243320	comparing, and this looks hopeless.
1243320	1249080	But actually all you need to do is every time you have an activity pattern, and you use
1249080	1253760	the outgoing weights to affect the next layer, just change the weight slightly with heavy
1253760	1255560	and learning.
1255560	1261480	So now what's going to happen is that weight matrix that comes out of that activity pattern
1261480	1263200	is going to be modified slightly.
1263200	1268200	Now when I get a new activity pattern, if the activity pattern is orthogonal to the previous
1268200	1273160	activity pattern, then any modifications you made in the weight matrix due to that previous
1273160	1275400	activity pattern won't make any difference.
1275400	1279760	But if it lines up with the previous activity pattern, if it's similar, the modifications
1279760	1285040	you made in the weights back there, the temporary modifications, will cause this new activity
1285040	1287360	pattern to have a different effect here.
1287360	1291880	So you'll get that long temporal context, and the way to store a long temporal context
1291880	1299280	is not to keep copies of neural activity patterns, it's to take your weights and to have temporary
1299280	1301600	changes to the weights, which I call fast weights.
1301600	1308640	So you temporarily change them, and these changes decay over time, so you'll have a memory.
1308640	1312800	So if you ask, where in your brain is your memory of what I said a few minutes ago?
1313360	1316160	I'll ask the younger people this, because for the older people it's nowhere.
1316160	1321720	But for the younger people, it's somewhere you can, if I were to say something I said
1321720	1327080	a few minutes ago, like these big neural nets are now laughing at us, you remember I said
1327080	1329520	that, where was that memory?
1329520	1334040	I think it's in the temporary changes to the weights, because that's got much bigger capacity
1334040	1340080	than activities of neurons, and you don't need to use up neurons just sitting there remembering.
1340080	1343360	And those temporary changes don't need to be driven by back propagation, they can just
1343360	1345360	be heavier.
1345360	1353800	Okay, so I've tried to relate these wonderful nets that can make up stories with an idea
1353800	1357800	about where short-term memory is in the brain.
1357800	1361520	And now I'll talk about where the cortex can do back propagation.
1361520	1368560	So neuroscientists, 20 years ago neuroscientists said don't be ridiculous, of course the brain
1368560	1372960	can't do back propagation, and they'd interpret it very literally as sending signals backwards
1372960	1383600	down the same axons and saying neurons don't do that, no thanks.
1383600	1388760	But now we know that back propagation works really well for solving tough practical problems.
1388760	1392320	So that's rather changed the balance, because when back propagation was just a theory of
1392320	1397560	how you might get computers to learn something, and when it learns some simple things, it
1397560	1400800	wasn't sort of imperative to understand whether the brain did it.
1400800	1404920	But now we know that you can do all these things with back propagation.
1404920	1409000	What's more, we know that back propagation is the right thing to do, but if you have
1409000	1415320	a sensory pathway, and you want to take the early feature detectors so that their outputs
1415320	1421600	are more helpful for making the right decision later on in the system, then what you really
1421600	1426560	need to do is ask the question, how should I change the receptive field of this early
1426560	1432760	detector so that what is output helps with the decision?
1432760	1435480	And what you have to do is do back propagation to compute that, that's the efficient way
1435480	1436480	to compute it.
1436480	1442280	And I think it'd be crazy if the brain wasn't somehow doing this.
1442280	1445480	So why do neuroscientists think it's impossible?
1445480	1451240	Apart from silly objections like things don't go backwards down axons, at least not at the
1451240	1460240	right speed.
1460240	1462240	He wants me to update things.
1462240	1470280	Oh, it's just died.
1470280	1473640	I'm going to go out and present them out.
1474640	1481640	I'm going to go back in to present them out.
1481640	1486800	Okay, so here's some reasons why the brain can't do back propagation.
1486800	1492240	The first reason is they say, well, it doesn't get the supervision signal.
1492240	1495720	And they're imagining that the supervision signal is like you take a micro pipette and
1495720	1500320	you put it into the infrotemporal cortex and you inject the right answer and the brain
1500320	1503240	doesn't have anything like that, right?
1503240	1508440	But actually, if you take that language model, it didn't need label data, it was just trying
1508440	1509960	to predict the next word.
1509960	1514480	So you can often use part of the input, maybe a future part of the input, or maybe a small
1514480	1520040	part of an image as the right answer, and so you can get supervision signals easily.
1520040	1522800	So there's no problem about supervision signals.
1522800	1527800	The second reason is neurons don't send real-valued activities, they send spikes.
1527800	1532360	And back propagation is using these real-valued activities so you can get nice, smooth derivatives.
1532360	1536040	So back propagation can't possibly be what's going on in the brain.
1536040	1539000	The third objection is, neurons have to send two signals.
1539000	1545240	They have to send the activity forwards and they have to send error derivatives backwards.
1545240	1550440	The signal they have to send backwards is, how sensitive am I to changes in my input?
1550440	1558640	Or rather, if you change my input, how much does that help with the final answer?
1558640	1563000	And the last thing is about neurons having reciprocal connections because you have to,
1563000	1568480	when you send things backwards, if you use a different neuron, you have to use the same
1568480	1571520	weight as the forward weight.
1571520	1577000	I'm not going to tell you how you can overcome that, but you can easily.
1577000	1581760	So supervision signals isn't really a problem, there's many ways to get a supervision signal.
1581760	1587000	The simplest is predicting what comes next.
1587000	1591960	Now the question of can neurons communicate real values?
1591960	1596560	Well the first thing to notice about back propagation is, if you have very noisy estimates
1596560	1599920	of the gradient, it works just as well.
1599920	1603760	It's very, very tolerant of noise as long as it's unbiased noise.
1603760	1608000	So for example, the signal you send forwards can be one bit, one stochastic bit, and the
1608000	1610520	signal you send backwards can be two bits.
1610520	1615120	If they have the right average value, if their expected values are correct, then they're
1615120	1622320	just this expected value plus some noise, and the whole system still works fine.
1622320	1626800	So in the brain, you have a neuron.
1626800	1632320	At any instant, the neuron has an underlying firing rate, and it produces spikes, and for
1632320	1637440	now let's just suppose it produces spikes according to a Poisson process.
1637440	1640920	So it's probability of producing a spike in a small interval, which is the underlying
1640920	1643320	firing rate.
1643520	1649640	The question is, suppose we treated it as if it could send that underlying firing rate.
1649640	1653160	When it sends a Poisson spike, it's just a very noisy version of the underlying firing
1653160	1655160	rate.
1655160	1660520	It's a one or a zero, but its expected value is the underlying firing rate.
1660520	1667280	So how well do neural networks work if we send very noisy signals?
1667280	1672880	So I'm going to have a statistics digression.
1672880	1677400	If you do statistics 101, they tell you you shouldn't have more parameters in your model
1677400	1679280	than you have data points.
1679280	1683080	You really ought to have quite a few data points for each parameter.
1683080	1686400	It turns out this is completely wrong.
1686400	1691520	The Bayesians knew it was wrong, actually.
1691520	1695280	The brain is not in the same regime of statistics 101.
1695280	1699160	In the brain, you're fitting about 10 to the 14 parameters, and you have about 10 to the
1699160	1701280	nine seconds.
1701320	1707040	So even if you have sort of 10 experiences per second, so even if you take 100 milliseconds
1707040	1713880	is the time for an experience, that's the kind of backward masking time, you have like
1713880	1718920	10,000 synapses per 100 milliseconds of your life.
1718920	1722680	You're throwing a lot of parameters.
1722680	1726920	So if your mother just kept saying, good, bad, good, bad, good, bad, she couldn't possibly
1726960	1731960	provide enough information to learn all those 10 to the 14 parameters.
1731960	1738960	And here's what they teach you wrong in statistics.
1741200	1744960	Everybody knows that if you've got a given size model with a given number of parameters,
1744960	1747800	the more data you have, the better you'll generalize.
1747800	1750760	So for a given size of model, it's always better to have more data.
1750760	1754360	In fact, the best thing you can do is get more data.
1755160	1759520	Okay, but that doesn't mean that if you've got a fixed amount of data, you should make
1759520	1761600	it look like a lot by having a small model.
1761600	1764200	That's what they tell you in statistics 101.
1764200	1766200	Okay?
1766200	1773480	Big models are good if you regularize them, if you stop them doing crazy things.
1773480	1776920	We can see that using a lot of parameters is good, that you can always win by having
1776920	1777920	more parameters.
1777920	1779920	And the way you do that is say, I'm going to have a committee.
1779920	1783000	I'm going to learn lots of different little neural nets.
1783000	1785960	If you give me more parameters, I'll learn more different neural nets.
1785960	1788240	And then I'll average what they all say.
1788240	1789680	And you'll always win.
1789680	1796000	It's a sort of declining win, but if you have enough of them, you'll win by having more.
1796000	1800840	So it's always better to have more parameters.
1800840	1807840	It turns out if you have a fixed amount of data and you have enough computation power,
1807920	1813680	the brain has, you should always use such a big model that the amount of data looks
1813680	1814680	small.
1814680	1817760	That's the regime you ought to be in for a fixed amount of data.
1817760	1822880	That is, if you take the limit when the amount of data is fixed and you have unlimited computation
1822880	1828040	and ask now how big would you like your model to be, you'd like your model to be much bigger
1828040	1830680	than the data.
1830680	1833360	Okay.
1834080	1836840	That only works if you have a good regularizer.
1836840	1844480	And I'm now going to tell you a very good regularizer called Dropout.
1844480	1847520	So this is to use in neural networks where you have a lot more parameters than you have
1847520	1850080	data points to train them on.
1850080	1853840	And you could learn an ensemble of little models, and this is a way of learning an ensemble
1853840	1859880	of many more models, but the models in the ensemble can share things with each other.
1859920	1865640	So the idea is, if we just have one hidden layer in a neural net, we put the data in
1865640	1871640	and each time we show the data vector, we randomly remove half the neurons.
1871640	1879400	So we randomly get rid of half the neurons in your brain and only use the ones that remain.
1879400	1882880	And it's a different subset we remove each time.
1882880	1887440	Now when we do use a neuron, we use it with the same weights each time.
1887480	1891080	So what you've got is if you've got h hidden neurons, you've got two to the h different
1891080	1893240	subsets of neurons you might use.
1893240	1897920	So you actually have two to the h different models, exponentially many models.
1897920	1899440	Most of the models are never used.
1899440	1904360	A few of the models will see one example, a small fraction of them will see one example.
1904360	1908200	No models will see two examples.
1908200	1910960	And yet they can learn because they're all sharing parameters.
1910960	1914400	So this idea of sharing parameters in a neural network is very effective.
1914400	1919240	So really you've got all these different models that are sharing parameters and you train
1919240	1922560	it up and it generalizes really well.
1926600	1928360	So I said that.
1928360	1934160	So what we know is if you get rid of a fraction of the neurons each time and treat it as though
1934160	1938240	they weren't there, it works really well.
1938240	1941200	That's just a form of noise.
1941200	1946360	And basically this is just an example of if you have a very big model and you add a lot
1946360	1954880	of noise, the noise allows it to generalize well and it's better to have a big model with
1954880	1958720	a lot of noise than to have a small model with no noise.
1958720	1962240	And so what the brain wants, because it's got such a big model compared with the amount
1962240	1965760	of data it operates on, it wants a lot of noise.
1965760	1968880	And so now a Poisson neuron is kind of ideal.
1968880	1974000	It's got a firing rate and now it adds a whole lot of noise to that and either sends a one
1974000	1979880	or a zero and that actually makes it generalize much better.
1979880	1984000	So the argument is the reason neurons don't send real values is they don't want to.
1984000	1988280	They want to send things with a lot of noise in and that's making them generalize better.
1988280	1991120	So that's not an argument against backpropagation.
1991120	1995440	These dropout models are trained with backpropagation.
1995440	2004520	So the random spikes are really just a way of adding noise to the signal to get better generalization.
2004520	2008520	And now the last thing I'm going to address, I'm going to keep going till Blake stops me
2008520	2014920	and I figure I've got about another five minutes before he gets really ratty.
2014920	2022960	So the output of a neuron represents the presence of a feature in the current input.
2023000	2028000	So it's obvious the same output can't represent the error derivative, right?
2028000	2031880	You couldn't have a neuron that said to higher layers, this is the value of my feature
2031880	2035120	and said to lower layers, this is my error derivative.
2035120	2036520	It couldn't be done.
2036520	2042720	So the neurons that go backwards need to be different neurons except that that's nonsense.
2042720	2047320	So here's my claim.
2047320	2048720	Joshua Benjo picked up on this later.
2048720	2050760	I made this claim first in 2007.
2050760	2055360	Actually, I made it first in the ground proposal.
2055360	2059120	And I still believe this claim even though nobody's managed to make it work really well
2059120	2061160	in the neural net yet.
2061160	2066960	The idea is a neuron has a firing rate.
2066960	2074240	That's the firing rate is its real output, which is communicated stochasticly by a spike.
2074240	2081520	And that firing rate is actually changing over time, the underlying firing rate.
2081520	2087360	And the rate of change of the firing rate is used to represent the error derivative.
2087360	2091400	Now the nice thing about a rate of change is it can be positive or negative.
2091400	2096560	So we can represent positive or negative derivatives without a neuron having to change
2096560	2100560	the sort of signs of its synapses.
2100760	2104000	And what it's representing, the derivative it's representing is the derivative of the
2104000	2106920	error with respect to the input to the neuron.
2106920	2108400	And that gets sent back to earlier neurons.
2108400	2111800	And if I had enough time, I could show you a whole bunch of slides about how this will
2111800	2114400	do back prop.
2114400	2117960	But I want to show you one consequence of this.
2117960	2121960	So that's, look, here we have a nice equation because it's got Leibniz on one side and Newton
2121960	2125040	on the other side.
2125040	2128400	That's Leibniz's notation for derivatives because they're not derivatives with respect
2128400	2129720	to time.
2129720	2134640	And this is Newton's notation because that was for derivatives with respect to time, okay?
2134640	2141480	And what we're saying is the output of neuron J, which is yj, is the output of neuron J.
2141480	2146280	But how fast that's changing over a short time interval is the error derivative.
2146280	2149600	This is just a hypothesis, you understand?
2149600	2154440	But it's true.
2154440	2158680	Jay McClellan and I first used a version of this in 1988 before we knew about its back
2158720	2160040	time dependent plasticity.
2160040	2163200	I'm not sure it would be discovered then.
2163200	2166200	Where you take, this is where I need the cursor.
2166200	2170200	Yes, that one.
2170200	2172280	Yes.
2172280	2174120	You take some input.
2174120	2177640	You send it to some hidden units, which send it to more hidden units by the green connections
2177640	2178640	and sends it to more hidden units.
2178640	2181840	It comes back to the input, so you reconstruct the input.
2181840	2185840	And then you send it around again, not all the way around, but up to there and up to
2185840	2188640	there and up to there using the right connections.
2188640	2193760	And then the learning rule, which you'll notice doesn't involve explicit back propagation,
2193760	2202280	is to say for these neurons, for example, I change the incoming weights by the activity
2202280	2209360	of the presynaptic neuron down here times the difference between what I got on the green
2209360	2212880	activation and on the red activation, first time round and second time round.
2212880	2217760	So the rate of change of the activation of the neuron is what's used to communicate an
2217760	2218760	error derivative.
2218760	2226400	Now, unfortunately, this thing has the wrong sign, but later on we fixed that.
2226400	2234240	And so here's a theory from 2007 that still hasn't been conclusively proved wrong.
2234240	2238200	And it sort of works, but it doesn't work quite as well as we hoped, about how you could
2238200	2241280	get a brain to do back propagation.
2241280	2245120	What you first do is you learn a stack of autoencoders, that is you learn to get each
2245120	2251280	layer to activate features in the layer above from which you can reconstruct the layer below.
2251280	2254080	So you learn some features that can reconstruct this layer.
2254080	2257880	Then you treat those features as data and learn some features that can reconstruct them.
2257880	2260080	You build a big stack of autoencoders like that.
2260080	2261080	Okay.
2261080	2265720	Once you build the stack of autoencoders, then each layer can activity in a layer can
2265720	2269420	reconstruct the activity in the layer below.
2269420	2272640	And then you do two top down passes.
2272640	2277360	You do a top down pass from the thing you predicted at the output.
2277360	2280960	So you put in input, activity goes forward through the layers, you predict something
2280960	2281960	at the output.
2281960	2287040	And now you do a top down pass and you get reconstructed activities everywhere.
2287040	2295280	And then you take your output and you change it to be more like the desired output.
2295280	2301200	And now you do a top down pass and you'll get slightly different reconstructions.
2301200	2308120	And the difference between those two reconstructions is actually the signal you need for back propagation.
2308120	2313080	And so if you do that, the learning rule is that you should change a synapse by the
2313080	2319200	pre-synaptic activity in the layer below times the rate of change of the activity in the
2319200	2321280	layer above in the post-synaptic neuron.
2321280	2324920	So it's a very simple learning rule.
2325120	2330920	So let's change the weight in proportion to the pre-synaptic activity times the rate
2330920	2335040	of change of the post-synaptic activity.
2335040	2340640	Now it turns out if you're using spiking neurons, what that amounts to that are representing
2340640	2347480	underlying firing rates that are changing, that amounts to a learning rule that looks
2347480	2353080	like this.
2353080	2358880	What you do is you take a pre-synaptic spike and you ask whether the post-synaptic spike
2358880	2361920	came before or after it.
2361920	2367480	Because what you're interested in is the rate of change of the post-synaptic firing rate
2367480	2373000	around the time of the pre-synaptic spike.
2373000	2381080	And if the post-synaptic spike occurs often just after it and seldom just before it, that
2381080	2383520	suggests the firing rate is going up.
2383520	2387840	And if the post-synaptic spike occurs often just before the pre-synaptic one and less
2387840	2391160	often just after it, that means the firing rate of the post-synaptic neuron is going
2391160	2393280	down.
2393280	2398240	So if you want your learning rule to be the pre-synaptic activity, well, you'll only learn
2398240	2400360	when you get a pre-synaptic spike.
2400360	2404400	And then what you'll do is you'll say, did the post-synaptic spike occur afterwards
2404400	2405400	or before?
2405400	2409120	If it occurred afterwards, I should raise the weight and if it occurred before, I should
2409120	2410600	lower the weight.
2410640	2418720	And so your learning rule will look like this and this thing is actually a derivative filter.
2418720	2423720	It's centered at zero and what this is really doing is measuring the rate of change of the
2423720	2425880	post-synaptic firing rate.
2425880	2428040	And of course it's sampling it.
2428040	2431680	So you have a post-synaptic firing rate, there's these spike trains and are the other spikes
2431680	2434040	getting closer together or further apart?
2434040	2436720	Well this is a way to measure that.
2436720	2442440	And of course you can do the learning on individual spikes and the learning rule would then be
2442440	2446440	the implementation of this idea that the rate of change of the post-synaptic firing rate
2446440	2448920	is the error signal.
2448920	2452800	The learning rule would just be if the post-synaptic spike goes after the pre-synaptic one, increase
2452800	2457720	the strength, otherwise decrease it and have that whole effect fall off as the spikes get
2457720	2461920	further away because we're really only interested in the rate of change of the firing rate around
2461920	2465760	the time of the pre-synaptic spike.
2465800	2471120	Now there's one consequence of that which is that if you're going to use the rate of
2471120	2477560	change of a neuron to represent not what the neuron is representing but to represent an
2477560	2482520	error derivative, you've basically used up temporal derivatives for communicating error
2482520	2483800	derivatives.
2483800	2488640	So you cannot use temporal derivatives to communicate the temporal derivatives of what the neuron
2488640	2489640	represents.
2489640	2494080	So however I have a neuron that represents position, I can't use how fast that's changing
2494080	2497480	to represent velocity and that's true of neurons.
2497480	2502720	If you want to represent velocity, you have to have a neuron whose output represents velocity.
2502720	2506000	You can't do it with the rate of change of a position neuron.
2506000	2512680	If I kill the velocity neurons and keep the position neurons and I watch a car moving,
2512680	2516400	the position neurons will change but I won't see any motion.
2516400	2521480	Similarly, you can't use the rate of change of a velocity neuron to represent acceleration.
2522280	2528120	Okay, so I think the fact that you can't use the rate of change of a representation to
2528120	2533680	represent that that stuff in the world is changing is more evident since the board of
2533680	2540120	the idea temporal derivatives of neurons are used up in representing error derivatives.
2540120	2544040	So now I'll summarize.
2544040	2548880	The main arguments against back propagation, the fact that they spent neurons and spikes
2548880	2554480	rather than real numbers, well that's just because a lot of noise regularizes things.
2554480	2559120	You can represent error derivatives as temporal derivatives so the same neuron can send temporal
2559120	2566440	derivatives backwards, communicate those backwards and communicate activities forwards and the
2566440	2571040	fact that in the brain you do get back to independent plasticity seems to be evidence
2571040	2575560	in favor of that representation of error derivatives and now I'm done.
2578880	2593880	Thank you Jeff for a great talk, sorry that was a Twitter joke, got it, anyway.
2593880	2601920	So now what we're going to do is a brief Q&A between myself and Jeff and then after
2601920	2607000	I've had my chance to ask some questions I'm going to open it up to you guys.
2607000	2614560	Now I had originally sent Jeff a few questions which I'll rely on partially but his talk
2614560	2619040	has made me want to ask a few others so I'm sorry I'm going to throw a few loops at you
2619040	2623400	as well but let's start with some of the ones that I told you I wouldn't give you.
2623400	2628560	There is something funny with my mic, I don't know if the AV guy is there, I just won't
2628560	2629560	look down.
2629560	2641760	Okay so the first question, yeah it's okay, I'm going off script anyway.
2641760	2647080	The first question which I would like to ask just because it's something that I spend far
2647080	2656080	too long arguing with people online is essentially you know so you're in the computer science
2656080	2660760	department, you've come here, you've given us a talk that's largely about brains but
2660760	2666160	many people seem to object to the idea that computers have anything to tell us about brains
2666160	2670920	or indeed the idea that the brain is a computer despite the fact that neuroscientists often
2670920	2673160	refer to computation in the brain.
2673160	2679840	So my question is to you, is the brain a computer, I don't know I'll just hand that over to
2679840	2680840	you first.
2681600	2682600	Yes?
2682600	2685120	Good, okay.
2685120	2690880	And for the record I didn't tell him to say that if anyone from Twitter is watching.
2690880	2696160	And B, can you just maybe give an intuitive understanding of why the answer is yes despite
2696160	2701000	the fact that obviously our brains are very different from our laptops or our cell phones
2701000	2703600	and stuff like that.
2703600	2710640	So there's many ways you can do computation with physical stuff and you could get some
2710640	2715920	silicon and make transistors and then run them at very high voltage much higher than
2715920	2721800	needed to make them be digital and then you could if you wanted to represent a number
2721800	2726920	you could have bits and you could and so on and you could create multipliers and adders
2726920	2731160	and then you could put all that together and you could have some bits that tell you where
2731200	2738240	in memory to find stuff and you could make a conventional computer or you could make
2738240	2744280	little devices that have some input lines that are hardwired with input lines and you
2744280	2746080	could have adaptive weights on the input lines.
2746080	2753520	So early neural nets, Marvin Minsky made neural nets out of feedback controllers that were
2753520	2759080	used in I think B-52 bombers or B-29 bombers or something, B-27 I don't know, some kind
2759080	2765960	of bomber, it was America and so you can make computers in lots of different ways.
2765960	2771480	When I was a kid I used to make computers by you take a six inch nail and you saw the
2771480	2777760	head off and then you wrap copper wire around it and then you take a razor blade and you
2777760	2784240	break it in half so that it's a nice flexible thing like this and you wrap a bit of copper
2784240	2788560	wire around the razor blade and then when the current goes through the nail it'll make
2788640	2792600	the razor blade go down and you'll make a contact so now you've got a relay and then
2792600	2795240	you can put a bunch of those together and make logic gates.
2795240	2800480	I never got more than about two logic gates that way but yeah you can make computers in
2800480	2807920	lots of different ways and the brain is clearly made in a different way from the normal computers
2807920	2814800	which has some different strengths and weaknesses so it's much slower but on the other hand
2814800	2817760	you can make it much more parallel.
2817760	2822600	It has one special property which I think is what makes us mortal which is that every
2822600	2829800	brain is different so I can't take the weights from my brain and put them in Blake's brain
2829800	2832920	and hope that it'll work because he just doesn't have connections in the same places.
2832920	2833920	You've tried.
2833920	2834920	Right.
2834920	2840240	Well there's a way of doing it where you take the weights in my brain I turn it into strings
2840240	2841240	of words.
2841240	2847440	Blake absorbs these strings of words and puts different weights in his brain.
2847440	2851520	It's pretty lucky all our brains are different because otherwise rich people will grab poor
2851520	2862200	people's brains so they could live forever but quite okay.
2862200	2871840	So I think I want to ask you then following on that what do you think about some of the
2871840	2877760	quests to fully characterize the brains connectome do you think that is a scientifically worthwhile
2877760	2878760	endeavor.
2878760	2888320	Yes I do partly because some of the people doing it are my friends.
2888320	2891480	Ignoring your loyalty to Sebastian.
2891760	2897120	Not well in that case no.
2897120	2903200	It seems to me it is very worth doing but you don't have to do that in order to begin
2903200	2905040	to understand the principles.
2905040	2906040	Very good.
2906040	2910040	But for things like the retina which has a lot of hardwired stuff in it I think it's
2910040	2912240	really important to do that.
2912240	2914800	So that actually leads on to my next question.
2914800	2917440	I wanted to ask you about hardwiring.
2917440	2924080	So another thing that I think many people who study the brain find difficult about artificial
2924080	2929120	neural networks as a model for the brain is that as you say you start with random weights
2929120	2932200	and you train it on a lot of data and you get these things out.
2932200	2938040	But we know that there are some pre-wired things in many brains so the classic examples
2938040	2944160	are a horse can run pretty much right out of the womb but even within humans arguably
2944160	2948160	there are some things that we find easier to learn than others.
2948160	2955480	And so what do you think is the place for innate behavior within neural networks as
2955480	2957160	a model of cognition?
2957160	2962760	Okay so it used to be when I was a student if you were interested in language people
2962760	2968200	would tell you that it was all innate and it just kind of matured as you got older and
2968200	2972760	maybe you learned like 12 parameters that characterised your particular language whether
2972840	2978280	it was subject verb object or some other way.
2978280	2983080	In fact there's a nova that I saw, it was probably made about 20 years ago and it has
2983080	2988560	all the leading linguists all of whom were educated by Chomsky and they look straight
2988560	2992720	at the camera and they say there's a lot we don't know about language but one thing
2992720	2997600	we know for sure is that it's not learned.
2997600	2999720	So Chomsky had really good gulag.
2999720	3002720	Yeah he did.
3002720	3009680	But it's over because we know now if you want to translate you just learn it all.
3009680	3015360	The number of linguists required to get a system that can turn a string of symbols in
3015360	3020000	English into a string of symbols in French is roughly zero.
3020000	3023480	I mean linguists involved in preparing the databases for training and making sure you
3023480	3029000	get sort of a variety of grammatic structures and things but basically you don't need linguists,
3029000	3032360	you just need data.
3032360	3034680	So you don't need much innate structure.
3034680	3038640	The issue of what is innate, it doesn't, it seems to me there's not much point putting
3038640	3043640	in stuff innately if you can learn it quickly.
3043640	3050680	So for example the ability to move and get 3D structure from motion, that's actually
3050680	3055200	very easy to learn so I don't believe that's innate even though a child can do it at like
3055200	3057560	two days.
3057560	3065280	You show them a sort of W made of paper and you rotate it in a consistent way and they
3065280	3069120	get bored and as soon as you rotate it in a way, move it in a way that's not consistent
3069120	3074320	with the rotation, the interest perks up.
3074320	3076240	But I think they can learn it in two days.
3076240	3079280	It's really easy to learn.
3079280	3081280	Okay interesting.
3081280	3089760	Now I want to ask you, I know partially what your answer is going to be but when I remember
3089760	3096920	long ago you told me that one of your career goals, at least earlier in your career, was
3096920	3103320	to prove that everything that psychologists thought about the brain was wrong.
3103320	3108880	And so my question is what was it that they had wrong, are they still getting it wrong
3108880	3112880	and is neuroscience getting that same thing wrong?
3112880	3118680	It was mainly to do with this conviction that psychologists had partly based on Chomsky
3118680	3124200	that there was an awful lot of innate stuff there and that you couldn't just learn a whole
3124200	3126760	bunch of representations from scratch.
3126760	3131800	There was this innate framework and there was a little bit of tuning of this innate knowledge
3131800	3136920	and that's what learning was and that's just, I think that's a completely wrong headed approach.
3136920	3143640	In fact, I want to go the other way and I want to say the stuff in the brain that's innate
3143640	3146320	wasn't discovered by evolution.
3146320	3151960	The stuff in the brain that's innate was discovered by learning.
3151960	3154600	Do I have time to do that digression?
3154600	3155600	Yeah.
3155600	3166040	Okay, so imagine we have a little neural network and it's got 20 connections in it and each
3166040	3170000	of those connections has a switch that could be on or off.
3170000	3172560	So it can let stuff through or not.
3172560	3174800	So you've got to make 20 binary decisions.
3174800	3179640	So your chance of making them by chance is one in a million and making the correct decisions.
3179640	3186040	Now this little neural network circuit is a mating circuit and so the neural net goes
3186040	3192920	into a singles bar and it runs this circuit and if it's got the connections right it has
3192920	3197680	lots of offspring and if it hasn't got the connections right it doesn't have offspring
3197680	3200400	or doesn't have so many offspring.
3200400	3207480	Okay, so let's start off with the connections being, if they were just kind of random and
3207480	3212640	you did mutation, what would happen is you'd have to build about a million organisms before
3212640	3217680	you got a good one and if you had sexual combination of the organisms, let's have a really simple
3217680	3223080	biology in which each connection has its own gene and this gene has two alleles for
3223080	3225960	on and off, okay?
3225960	3232160	If you do mating now, you might have an organism that got all 20 connections right and it mates
3232160	3235840	with one that has a few wrong and it gets a few wrong ones and now it's wiped out.
3235840	3238920	It doesn't have lots of offspring anymore, that's it.
3238920	3246080	So it seems like a complete disaster and it would obviously take you at least a million
3246080	3250200	organisms to expect to get a good one even if you have pathogenesis where you didn't
3250200	3252200	have sexual reproduction.
3252200	3258560	Now I will show you how to build a good organism in only 30,000 tries and the way you do it
3258560	3264920	is this, you, for each connection you have three alleles, you have turn the connection
3264920	3273400	on genetically, turn the connection off genetically or leave the connection to learning, okay?
3273400	3275760	So that's the third alley.
3275760	3283240	And now you start off with a population in which about half of the connections are genetically
3283240	3288360	determined and the other half are left to learning.
3288360	3292760	So that's 10 connections that are definitely determined, so there's a 1 in 1,000 chance
3292760	3296440	that you'll luck out genetically, you'll get those 10 right.
3296440	3300080	And then during the organism's lifetime, let's have a really dumb learning algorithm where
3300080	3304320	it just randomly fit like the one I talked about, it randomly fiddles with the connections,
3304320	3308440	just randomly flips connections of the 10 that are left to learning.
3308440	3312960	And it'll take you to about 1,000 trials and it'll get the combination right.
3312960	3317440	But the point is it can do those trials without building a whole organism, it can just go
3317440	3323560	into the singles bar and sort of fiddle around a bit with its connections and it'll bang.
3323560	3331560	So what we've done is we've replaced a million trials of evolution, building a million organisms
3331560	3338200	with built 1,000 organisms and then let's build 30,000 organisms, just to be safe.
3338200	3346120	And then each of those fiddles around with its connections and it'll do this search.
3346120	3349840	The whole search is the same, you have to try a million combinations, but the way you
3349840	3355200	get the million combinations is 1,000 organisms each does 1,000 learning trials and so almost
3355200	3357920	all the work is done by learning.
3357920	3365080	Now if genetically an organism happens to have more things set, like it's got 12 of
3365080	3368120	them set right, it'll learn faster.
3368120	3375720	And so this genetic pressure for, if you mate organisms now, this genetic pressure to get
3375720	3379440	more and more of these alleles set genetically.
3379440	3385000	But the pressure only comes because the learning can get all 20 set right so this thing can
3385000	3387000	mate and have lots of offspring.
3387000	3392840	So the fact that the learning can find a solution creates genetic pressure to hardwire these
3392840	3394840	things.
3394840	3401360	So what's happening there is the search, a thousand things were done by evolution, a
3401360	3406560	million minus a thousand things were done by learning and that created a landscape for
3406560	3411440	evolution that allowed evolution to gradually hardwire in more and more, that these things
3411440	3414480	were first found by learning.
3414480	3422160	So I think a lot of the structure in the brain that's hardwired is first found by learning
3422160	3425560	and gradually it gets backed up into the hardwiring.
3425560	3429040	But to get the evolutionary pressure to say that's good, you have to be able to do the
3429040	3430040	learning.
3430080	3433280	First hardwired things, you'd never find anything that was good.
3433280	3434280	Okay.
3434280	3435280	Great.
3435280	3436280	Thank you.
3436280	3437280	Now...
3437280	3439200	That's called the Baldwin effect, by the way.
3439200	3440200	Yes.
3440200	3446880	Yeah, it's called after a psychology professor at the University of Toronto in the 1890s called
3446880	3449520	Baldwin who invented this effect.
3449520	3454480	He didn't do any computer simulations though.
3454480	3458960	So I want to do one follow-up question on that and then ask my final question before
3458960	3460640	handing it to the audience.
3460640	3469040	So my follow-up to that is, you know, I think one of the things that is unclear in terms
3469040	3475400	of the success of deep learning is exactly how much it was purely the compute or some
3475400	3476880	clever things.
3476880	3482920	Now I've seen both cases argued and you today kind of suggested that it was just the compute.
3482920	3487840	But I want to ask you about this following on your last point, which is that we know
3488080	3492280	that if you build networks with particular architectures and with particular learning
3492280	3498440	rules, you are effectively making learning faster if you do it right.
3498440	3503280	And arguably a lot of the success of deep learning has actually been as a result of
3503280	3508560	people thinking about good designs for their networks and good ways of making learning
3508560	3509560	faster.
3509560	3510560	Yep.
3510560	3518880	So would you potentially say that we have seen that process that you just described
3518880	3523960	actually occur with NAI over the last ten years of the learning kind of backing up into
3523960	3525680	the hardwiring?
3525680	3529200	I see.
3529200	3531280	I need to think about that.
3531280	3534520	What we've seen, I mean, Jan Lecaun invented convolutional neural nets.
3534520	3535520	Right.
3535520	3536520	Yes.
3536520	3537520	For example.
3537520	3540240	So the computer was invented in the 1980s, but computers weren't fast enough to really
3540240	3541680	do a lot with them.
3541680	3544880	So they were used for handwriting recognition and they were used for reading 10% of all the
3544880	3546600	checks in America.
3546600	3549120	But they didn't really take off.
3549120	3554080	They really took off when the computer hardware came along to make them really efficient.
3554080	3558720	So that's a case where the ideas were had first, but without the hardware they didn't
3558720	3559720	work.
3559720	3560720	You've obviously got to have both.
3560720	3561720	Right.
3561720	3562720	Yes.
3562720	3563720	Great.
3563720	3564720	Okay.
3565000	3569160	So now my last question before I hand it to the audience is just, what do you see as
3569160	3573920	being the future of the interaction between neuroscience and AI?
3573920	3579680	Do you think that there is space for a sort of new cognitive science where we study general
3579680	3586560	intelligence, but with brain-centric models rather than logic-based models?
3586560	3589880	Or will we see the two streams depart over the next few decades?
3589880	3592960	The way I like to think of it is we'd like to understand, if you'd like to understand
3593000	3599520	how the brain does computation, you've got brains in your computation.
3599520	3602800	And they look, like you said, they look pretty different to begin with because there's many
3602800	3605000	different ways to do computation.
3605000	3610680	And with a conventional digital computer, you can get it to pretend to be anything.
3610680	3614400	So we're getting it to pretend to be some other kind of computer, an artificial neural
3614400	3615800	net.
3615800	3621680	And we'd like to sort of bridge this huge gap between brains and what we can simulate
3621680	3623520	on computers.
3623520	3628120	And so neuroscientists are sort of doing experiments.
3628120	3632520	And good computation neuroscientists are sort of doing experiments to try and sort of see
3632520	3634880	how you could do the computation.
3634880	3640440	And I think of myself at this end as doing, simulating things with artificial neural nets
3640440	3642000	to see how you can make it more biological.
3642000	3644440	And we're trying to build a bridge.
3644440	3648840	And so the computational neuroscientists, most of them are building from this end.
3648920	3650560	I'm building from the other end.
3650560	3653320	But obviously, if you want to build a bridge to somewhere, you need to look at where you're
3653320	3654160	going.
3654160	3657560	And so I'm trying to build a bridge that does computation more and more like the brain does
3657560	3662520	it, or like, I guess the brain does it from what my neuroscientist friends tell me.
3662520	3665360	And then there's conventional AI, which is trying to build a bridge like that.
3665360	3670320	Great, OK, thank you.
3670320	3674200	So now I'm going to open up to questions from the audience.
3674240	3679120	Now, for this, we've got this kind of interesting system here.
3679120	3686000	So rather than you putting up your hands and me selecting you, you can actually nominate
3686000	3690640	yourself to ask a question by pressing on the button on your microphone.
3690640	3692880	And it is a first come, first serve basis.
3692880	3695240	So you're going to be queued up.
3695240	3699320	And so you're now first on the queue.
3699320	3701920	And by now, it's too late to be able to ask a question.
3701920	3702800	Yes.
3702800	3704840	And one last thing about that, though.
3704840	3709520	When you are done asking your question, please turn off your microphone,
3709520	3714560	because that will open up this slot for the next person in the queue.
3714560	3718520	OK, go ahead.
3718520	3719960	I've got a red light here, does that?
3719960	3722760	Yeah, if your red light is flashing, that means you're on.
3722760	3723760	You get to ask a question.
3727000	3731160	If your red light is flashing, you're on.
3731160	3732520	I've got a solid light, please.
3732560	3733440	Oh, solid.
3733440	3734360	Sorry.
3734360	3735680	OK, I've got a solid light.
3735680	3737000	You were faster.
3737000	3738320	OK.
3738320	3742880	So this is a Clifton suspension bridge analogy for your interest here.
3742880	3746080	So you mentioned briefly, Hebbian synapses.
3746080	3749840	As neuroscientists, we have a good understanding of how they work at a molecular level.
3749840	3754640	So my question is, to what extent are the understanding of biological memory
3754640	3759040	mechanisms, i.e. Hebbian synapses, implemented by AI for deep learning
3759040	3762080	and the sorts of systems that you're describing?
3762120	3769520	So at present, people don't use Hebbian synapses for most deep learning.
3769520	3771360	They're using back propagation.
3771360	3776000	So it's an error correction rule, as opposed to something where,
3776000	3779120	if you just use it, it gets stronger.
3779120	3782200	But if you want a short-term memory for things like transformers
3782200	3786520	to remember a temporal context, just a simple Hebbian synapses is a good thing to have.
3786520	3790480	Yeah, but Hebbian synapses can code memories in humans that can last a lifetime.
3790480	3793280	So is this something that AI is working towards using,
3793280	3798000	or are they just going to bypass Hebbian synapses and come up with something superior?
3798000	3803600	OK. So if you think about what's been successful in the last few years,
3803600	3807600	it's using error correction learning with either labeled data
3807600	3813240	or trying to predict what comes next, and not Hebbian synapses.
3813680	3820960	Now, people like me who sort of do this kind of learning
3820960	3824200	but are interested in the brain know this isn't right.
3824200	3826400	We're much more interested in unsupervised learning.
3826400	3829560	We just can't make it work very well yet.
3829560	3835720	And I would love to be able to get learning to work,
3835720	3838600	as well as it does when you do back propagation,
3838600	3841480	without using biologically implausible things.
3842480	3846880	And one place we can do that is with temporary memories.
3846880	3850120	So if you say synapses have a fast component,
3850120	3852600	you can use Hebbian learning for that fast component,
3852600	3854760	and that will actually help neural networks work better,
3854760	3858760	even if you're using back propagation for the slow component.
3858760	3861760	That didn't really answer your question, but you know, it filled the time.
3861760	3865560	LAUGHTER
3865560	3867560	Hello.
3868560	3873560	I read in the Reinforcement Learning Book that dopamine is used
3873560	3876560	as a reward prediction error signal.
3876560	3880560	So I was wondering, do you see it used as a supervisory signal,
3880560	3883560	like you mentioned earlier?
3883560	3886560	OK, so for reinforcement learning,
3886560	3889560	there is some lovely work done by Peter Diane,
3889560	3893560	who is the theoretician, and some experimentalists,
3893560	3896560	showing that the real data from neuroscience
3896560	3900560	fits in with a theory that was started with Rich Sutton.
3900560	3906560	And Peter Diane did the work of showing that dopamine
3906560	3909560	corresponds to something in a particular learning algorithm.
3909560	3912560	And it doesn't correspond to the reward,
3912560	3916560	it corresponds to the difference between the reward you're expecting
3916560	3918560	and the reward you get.
3918560	3922560	So if you're a monkey and you're expecting a grape
3922560	3926560	and I give you a piece of cucumber, that's negative reward,
3926560	3930560	and that will be a big negative hit at the dopamine.
3933560	3938560	So that's not the kind of learning that's been really successful so far.
3938560	3941560	If you're willing to burn a lot of computer time,
3941560	3945560	Reinforcement Learning will solve some problems,
3945560	3949560	but it's not the kind of learning that's been most successful in AI.
3952560	3955560	So the difference is in Reinforcement Learning,
3955560	3958560	you get a single scalar, you get one number,
3958560	3960560	whereas in error correction learning,
3960560	3963560	you typically get a whole vector of numbers.
3965560	3967560	Right here.
3967560	3972560	So you mentioned that your goal, kind of the bridge analogy,
3972560	3976560	is your goal is to go from computers and try to get to the brain.
3976560	3979560	So let's just say that kind of makes sense to think,
3979560	3981560	okay, let's get to more general AI,
3981560	3983560	because I'd say humans are decently general.
3983560	3986560	And neuroscientists are trying to get from the other bridge,
3986560	3988560	the brain, to generally AI.
3988560	3992560	So you have these two kind of debates,
3992560	3994560	and this happens quite often,
3994560	3997560	where is it correct to go from generally AI to the brain,
3997560	3999560	first understand generally AI, then understand the brain,
3999560	4001560	or brain to generally AI.
4001560	4004560	And so what would you say is the most practical way
4004560	4008560	to problematize generally AI?
4010560	4013560	I don't like the phrase general AI.
4013560	4017560	I don't think, if you want intelligent devices,
4017560	4022560	I don't think you want to produce a sort of general purpose Android.
4022560	4025560	I think you want to produce different devices
4025560	4028560	that are smart in different ways.
4028560	4031560	So basically if you want intelligent machines that do things,
4031560	4034560	you have a vacuum cleaner and you have a backhoe.
4034560	4038560	You don't try to make one thing that's a vacuum cleaner and a backhoe.
4038560	4040560	It doesn't make sense.
4040560	4042560	What about connecting them through
4042560	4046560	kind of like different cognition areas in the brain?
4046560	4048560	Yeah, but I think it's the same with cognition too.
4048560	4051560	I think the neural net that does machine translation
4051560	4056560	isn't the same neural net as does vision.
4056560	4062560	I think, yeah, my guess is that people are thinking too much
4062560	4065560	about making one neural net that does everything,
4065560	4070560	and not thinking enough about making more modular neural nets
4070560	4072560	that are good at different things.
4072560	4074560	Some are more universal than others,
4074560	4076560	but I think that's how progress has been made.
4076560	4078560	That's how progress has been made so far.
4078560	4080560	Not by the people talking about general AI.
4080560	4083560	It's being made by people looking at saying,
4083560	4085560	how can I get a neural net to do vision,
4085560	4088560	or how can I get it to do machine translation?
4088560	4090560	Thank you.
4095560	4099560	Hi, I'm just wondering about the role of hierarchy in general.
4099560	4101560	There are different types of hierarchy.
4101560	4103560	There's different layers of neural network.
4103560	4106560	As you mentioned, there's fast memory and slow memory.
4106560	4111560	Then I wonder, are there more ways to add hierarchy to neural networks
4111560	4115560	to make them more useful or emulate actual brain?
4118560	4120560	Yes, probably.
4121560	4125560	In vision, for example,
4125560	4130560	you have multiple layers,
4130560	4133560	that is, you have multiple cortical areas in the visual pathway.
4133560	4136560	That's a very different kind of hierarchy
4136560	4140560	from what you need for dealing with the sort of structural reality.
4140560	4143560	In reality, there's the universe.
4143560	4146560	There may be many of them, but that's one's enough.
4146560	4149560	Then there's galaxies.
4149560	4152560	Then there's probably things above galaxies.
4152560	4155560	Then there's in the galaxies the stars,
4155560	4157560	and then there's solar systems,
4157560	4160560	and then there's planets, and so on.
4160560	4162560	We can do that all the way down to atoms.
4162560	4164560	You can imagine all that.
4164560	4167560	You can represent all that in your brain.
4167560	4170560	Clearly, what's going on is,
4170560	4173560	out in the world, there's this hierarchy
4173560	4176560	that goes over many, many orders of magnitude
4176560	4179560	from the universe down to quarks,
4179560	4182560	or whatever the smallest thing is now.
4184560	4187560	You don't want that kind of hierarchy in your brain.
4187560	4190560	What you've got in your brain is the ability to deal
4190560	4192560	with a little window of hierarchy,
4192560	4195560	where there's an object in its parts.
4195560	4197560	To deal with the whole universe,
4197560	4199560	you can take this window,
4199560	4202560	and you can map at a scale of the universe,
4202560	4204560	and there's the universe, and there's the galaxies,
4204560	4207560	or there's the galaxies, and there's the stars,
4207560	4210560	or there's the atom, and there's the electrons.
4212560	4215560	You're using the same neural hardware,
4215560	4218560	but mapping reality onto it differently.
4218560	4221560	I think whenever we have to deal with anything complicated,
4221560	4223560	we use hierarchies.
4224560	4227560	The way the brain uses them is by varying the mapping
4227560	4231560	from reality onto the brain.
4231560	4234560	It can only operate with a small window on a hierarchy,
4234560	4236560	which you can move up and down.
4236560	4239560	Much like you only have a small region of high resolution,
4239560	4241560	which you move around.
4241560	4242560	So logarithm?
4242560	4243560	Sorry?
4243560	4245560	Like logarithm.
4245560	4247560	What about logarithms?
4247560	4249560	That's what you're talking about, right?
4249560	4252560	Compressing a big range into something that is much manageable.
4252560	4254560	I wasn't thinking of it like that.
4254560	4257560	I was thinking of it as you have some fixed hardware,
4257560	4261560	and when I'm thinking about the solar system,
4261560	4264560	my fixed hardware couldn't possibly deal with the universe.
4264560	4267560	That's much too big, and it couldn't possibly deal with an atom.
4267560	4269560	That's much too small.
4269560	4272560	But it's fine dealing with the sun and some planets,
4272560	4274560	and maybe a moon or two.
4275560	4279560	What I'm trying to get at is we need to make a big distinction
4279560	4283560	between the hierarchy in the real world,
4283560	4286560	hierarchical structures in the real world,
4286560	4290560	and how we deal with them cognitively,
4290560	4292560	where we use attention,
4292560	4295560	and we only ever deal with a bit of the hierarchy at a time.
4295560	4299560	That's not the same for, say, aspects of language,
4299560	4300560	where you have...
4300560	4302560	Notice that with vision,
4302560	4306560	I can use the same neurons for representing the sun
4306560	4308560	and for representing a nucleus.
4309560	4312560	It's just an analogy, but it's the same neurons I'm using.
4313560	4316560	Now, if I'm processing language,
4316560	4319560	I've got things that find me phonemes
4319560	4321560	and things that turn phonemes into words
4321560	4323560	and things that turn words into sentences,
4323560	4324560	and those...
4324560	4326560	I can't move a window like that.
4326560	4327560	That's a fixed hierarchy.
4327560	4329560	There's phonemes, and there's words,
4329560	4331560	and there's phrases, and there's sentences,
4331560	4333560	and that's all sort of fixed in the brain.
4333560	4335560	That's not a flexible matter.
4335560	4337560	You can't kind of move the sentences down
4337560	4339560	so they're where the words were,
4339560	4341560	move the words down so they're where the phonemes were.
4341560	4343560	That doesn't work.
4343560	4346560	So there's some hierarchies that really do relate
4346560	4350560	to sets of neurons in the brain.
4350560	4353560	They're like the layers in the connections models.
4353560	4355560	There's other hierarchies,
4355560	4357560	like the whole spatial structure of the universe,
4357560	4360560	where what's in the brain is a window
4360560	4362560	you move over that hierarchy.
4362560	4364560	Thank you.
4367560	4370560	Thanks, Dr. Hinton, for excellent talk
4370560	4373560	and excellent ideas about the feasibility of back propagation.
4373560	4375560	My question's maybe more boring
4375560	4378560	about the statistical comments that you made.
4378560	4380560	Is that Dale?
4380560	4381560	Pardon me?
4381560	4382560	Are you Dale?
4382560	4384560	No, I'm Kyle.
4386560	4388560	You sound like Dale Shermans.
4388560	4391560	I'm at the University of Alberta.
4392560	4393560	Hi.
4393560	4395560	Well, that's a good instance,
4395560	4397560	that you sound just like Dale Shermans
4397560	4399560	and you're at the University of Alberta.
4399560	4403560	Are you a student of Dale's?
4403560	4404560	No.
4407560	4409560	I think I've only met a voice.
4409560	4411560	If you're a student of Dale's, I need to watch out
4411560	4413560	because it's going to be a very tricky question.
4413560	4418560	The question is that I was trained with this intuition
4418560	4420560	that you can't overparameterize your models,
4420560	4424560	that if you're trying to fit a line that you need two points,
4424560	4426560	if you're trying to fit a curve you need three and so on
4426560	4428560	and that scales up and you should always have
4428560	4430560	a little bit less data points.
4430560	4434560	I know that you have shown clearly
4434560	4436560	and the field has shown that that's not true.
4436560	4439560	What were the statisticians getting wrong
4439560	4441560	in their logic to be convinced?
4441560	4443560	It's to do with regularization,
4443560	4445560	that you need it to be highly regularized.
4445560	4447560	But first of all, I'll show you
4447560	4453560	that if you want to fit two data points,
4453560	4455560	well, let's take three.
4455560	4457560	If you want to fit three data points,
4457560	4461560	you would have told me you want a polynomial
4461560	4463560	with only three degrees of freedom,
4463560	4466560	so you want a constant and a slope and a curvature
4466560	4469560	and that's all you can afford with three data points.
4469560	4471560	That's wrong.
4471560	4473560	Now, this is where we need a pen.
4473560	4475560	Oh, sorry.
4477560	4478560	They don't work.
4478560	4480560	I tried them all and none of them work.
4481560	4482560	Okay, did they work?
4482560	4484560	Oh, well done.
4484560	4486560	Extra points.
4488560	4490560	Okay, yes.
4490560	4492560	Can you see that?
4492560	4493560	Yeah.
4493560	4495560	Okay, and we're going to have three data points.
4503560	4507560	And actually, if you're a statistician,
4507560	4509560	you'd probably say for three data points
4509560	4512560	you'd probably ought to fit a straight line like this.
4512560	4515560	Because I could fit a parabola
4515560	4517560	and the parabola would fit exactly
4517560	4520560	and that's a bit suspicious.
4520560	4523560	In other words, the parabola fits exactly,
4523560	4529560	but do you really believe that if you were to ask
4529560	4531560	when x is zero, what's the value of y?
4531560	4533560	Do you really believe that value for y?
4533560	4536560	Because a straight line is far more conservative.
4536560	4538560	So a statistician would probably say
4538560	4540560	fit a straight line.
4540560	4543560	However, that would be a frequentist statistician.
4543560	4546560	If you took a Bayesian statistician,
4546560	4549560	and this is in Chris Fisher's machine learning textbook,
4549560	4551560	there's a nice picture of it, I think, somewhere.
4551560	4553560	Think it's that book.
4553560	4555560	A Bayesian statistician would say,
4555560	4559560	okay, let's try fitting fifth-order polynomials.
4559560	4562560	And fifth-order polynomials,
4562560	4565560	we might even fit ones that don't exactly go through the data,
4565560	4567560	but for now let's make them go through the data.
4567560	4569560	So we fit a fifth-order polynomial
4569560	4572560	that goes kind of...
4575560	4576560	One, two, three...
4576560	4578560	Well, you know, some order.
4578560	4580560	And we fit another one.
4580560	4583560	Oh, that didn't go through the data.
4587560	4589560	And we keep fitting these guys,
4589560	4591560	and we fit a gazillion of them.
4591560	4594560	And what you see at the end is that
4594560	4597560	these gazillion ones, in between the data points,
4597560	4599560	they're kind of all over the place,
4599560	4603560	and their average is in a sensible place like here,
4603560	4605560	but their variance is big.
4605560	4607560	And what they're telling you is,
4607560	4609560	if you give me this x-coordinate,
4609560	4611560	I'm rather uncertain about this y-coordinate,
4611560	4613560	but this is a good bet.
4613560	4615560	And similarly here,
4615560	4617560	and if you go out here,
4617560	4619560	these polynomials are just all over the place,
4619560	4621560	and they'll tell you,
4621560	4623560	if you give me this x-value,
4623560	4626560	then it could be pretty much anything.
4626560	4628560	That's not a bad bet,
4628560	4630560	but it could be pretty much anything.
4630560	4632560	And that's a much better answer
4632560	4634560	than you get from a straight line.
4634560	4636560	So by fitting a very large number
4636560	4638560	of different polynomials,
4638560	4640560	and then averaging,
4640560	4642560	you get good, mean answers,
4642560	4644560	and you also get a sense of the variance.
4644560	4646560	Now, Drop-Out is doing something like that.
4646560	4648560	Yes, and that's brilliant.
4648560	4651560	Thank you for coming up with Drop-Out.
4652560	4655560	Many of us here
4655560	4658560	are working in a regime of sparse data,
4658560	4660560	and so we have a couple channels,
4660560	4662560	a couple signals, a couple voxels,
4662560	4666560	and you've convinced us that we need more,
4666560	4670560	but is there a way forward in AI
4670560	4673560	that can manage with more sparse data,
4673560	4675560	or is this the only regime
4675560	4677560	that's going to be able to make success?
4677560	4680560	So the really big successes
4680560	4682560	have been on big databases,
4682560	4685560	and I think we should be using
4685560	4687560	even bigger models,
4687560	4690560	but you can't get away from the fact that actually,
4690560	4692560	if you're going to have something
4692560	4694560	that starts off random
4694560	4696560	and sucks all its knowledge from the data,
4696560	4697560	you'd better have enough data
4697560	4699560	to suck all that knowledge from.
4699560	4701560	The bigger your model, the better,
4701560	4702560	if you regularize it,
4702560	4704560	but you still need a lot of data.
4704560	4706560	So the way you should think about it is this.
4706560	4708560	If you've got 100,000 data points,
4708560	4710560	that's small.
4710560	4712560	I know that's very depressing
4712560	4713560	if you're a neuroscientist.
4713560	4714560	Well, it's not depressing.
4714560	4716560	It seems impossible.
4716560	4718560	If you want to personalize medicine
4718560	4719560	for one individual
4719560	4721560	and you want to train a model
4721560	4723560	on their data from their brain,
4723560	4725560	it seems like there's going to be a disconnect
4725560	4727560	between what these models can do
4727560	4729560	and how they might help someone in the future.
4729560	4730560	Yes and no.
4730560	4732560	If I train a model
4732560	4734560	on a very large number of people
4734560	4736560	and then apply that model to one person,
4736560	4738560	that's the form of personalize medicine
4738560	4740560	that really works.
4740560	4742560	Great, thanks.
4751560	4753560	Oh, and say hi to Dale for me.
4758560	4761560	Thank you for the presentation.
4761560	4763560	My last question is about this dropout.
4763560	4765560	The thing is that you randomly just drop
4765560	4767560	some parts of the network
4767560	4769560	and then you say, okay,
4769560	4771560	that it works better so I would accept it.
4771560	4773560	But do we have any, like, intuition
4773560	4775560	why, for example, some parts of it work better
4775560	4777560	or if we try to embed this, like,
4777560	4779560	network into, like,
4779560	4781560	is it more graph isomorphism?
4781560	4783560	What are these two different graphs,
4783560	4785560	different graphs that we took?
4785560	4787560	Are there any similarities
4787560	4789560	between them or just with randomly?
4789560	4791560	I guess the problem with the randomness,
4791560	4793560	I guess we are trying to put
4793560	4795560	the burden of prediction
4795560	4797560	on the random part of the computer desk.
4797560	4799560	So I didn't hear
4799560	4801560	the whole question,
4801560	4803560	but certainly in dropout what we do
4803560	4805560	is we randomly leave out units.
4805560	4807560	Now you can also do block dropout.
4807560	4809560	You can take groups of units
4809560	4811560	and randomly leave out the groups.
4811560	4813560	And what that does is it allows the units
4813560	4815560	within a group to collaborate with one another,
4815560	4817560	and then between groups
4817560	4819560	they have to be fairly independent.
4819560	4821560	And that's called block dropout
4821560	4823560	but I didn't really hear
4823560	4825560	the rest of your question.
4825560	4827560	Okay.
4827560	4829560	The question was about
4829560	4831560	that, okay, imagine that you...
4831560	4833560	Can you talk closer to the microphone
4833560	4835560	because I'm partially dead?
4835560	4837560	The question is that
4837560	4839560	imagine that you have a dropout of 50%
4839560	4841560	and you're trying to get rid
4841560	4843560	of, like, 50% of your nodes
4843560	4845560	and the nodes in the network.
4845560	4847560	And the question is, okay,
4847560	4849560	whether we have any similarity between the types
4849560	4851560	if we do it iteratively,
4851560	4853560	whether we would find any similarity
4853560	4855560	on the structure of the network
4855560	4857560	that would produce the best results
4857560	4859560	and if it's so, whether it would correspond
4859560	4861560	to something physical, like, for example,
4861560	4863560	if you're doing a vision thing,
4863560	4865560	whether it would correspond to something in brain or not, I guess.
4865560	4867560	Yeah.
4867560	4869560	Lots of people have thought about whether you can do better
4869560	4871560	than random in dropout.
4871560	4873560	And there's some work on that, like,
4873560	4875560	block dropout that works, can work for some things.
4875560	4877560	But I don't really have much to say
4877560	4879560	about...
4879560	4881560	I don't really know the answer to
4881560	4883560	sort of, is there something much
4883560	4885560	more sensible than dropout
4885560	4887560	that's a lot more structured?
4887560	4889560	There might well be, but I...
4889560	4891560	Thank you.
4891560	4893560	Okay, so with that, we're going to have to end.
4893560	4895560	So please join me in
4895560	4897560	thanking Jeff for...
4897560	4899560	Thank you.
4899560	4901560	APPLAUSE
4901560	4903560	And
4903560	4905560	I wanted also
4905560	4907560	to thank Blake
4907560	4909560	for hosting this event,
4909560	4911560	and I felt the questions
4911560	4913560	could have gone on all night,
4913560	4915560	but the tip-off isn't half an hour,
4915560	4917560	so some of us have to move on.
4917560	4919560	So thank you, Blake.
4919560	4921560	APPLAUSE
