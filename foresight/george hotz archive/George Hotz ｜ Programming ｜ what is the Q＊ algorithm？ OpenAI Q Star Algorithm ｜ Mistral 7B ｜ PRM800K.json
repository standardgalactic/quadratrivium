{"text": " Good morning everybody. Good morning. Good morning. It's nice to be back in my house. I don't like streaming at work. High ball energy. Good morning everybody. Good morning. Good morning. Good morning. Good morning. Let's shrink me down here... me up there. Oops, don't do that. Okay, are we ready to get started? Yeah, I felt the vibes were a little bit off in yesterday's stream. I don't think I should stream at the office. Oh, so I didn't like the way that like, like now this is here, I'm gonna slide that down a little, about in the middle, that's pretty nice. I got my Yeti microphone. Move it over a little bit. Yeti puts me perfectly in the center. Yeah, vibes are off. Wouldn't have my lighting set off. I mean, it's an okay stream. It's good to focus on TinyGrad. But some guy said that he liked my older work better. And if there's one thing I never want to be, it's drink. So, we're gonna try to, we're gonna try to bring it back to the old stuff. And we're gonna do, we're gonna investigate the Q-star algorithm. So first off, what is the Q-star algorithm? I don't know, let's find out. We'll start using our friend here, Google. What is Q-star and when will we hear more? Someone who's done a fair amount of ML research. I can tell you, it's very, very easy to think you've discovered a breakthrough. That's true. All right, let's read this article in The Verge. Reuters, okay, well the government, no, no, no, no, no, no. Okay. Q-star could be a breakthrough in the startup search for what's known as artificial general intelligence. Reuters could not, could not be a breakthrough in the startup search for what's known as artificial general intelligence. Okay, it somehow does math. Does something have to publish papers? Not really. Wow. Remember when they used to publish papers and now they publish system cards and, safety and alignment? Okay, wait, wait, wait, wait, wait, wait, wait, wait, wait, wait, wait, wait, wait, wait, wait, wait, wait, wait, there's some hope here. Improving mathematical reasoning. Ooh, ooh, ooh, this sounds like Q-star guys. I think we found it. No, we don't, no, no, but really, wait, is this not, is this not Q-star? Did we not just find it? We're trying not to hate. We're trying not to hate guys. Solitate in the world. We got to bring law of impositivity and shit man. Whatever like John Lennon said, you know something That's why we're gonna go subscribers only Okay Given vast computing resources the new model was able to solve certain mathematical problems Improving mathematical reasoning with process supervision We've trained a model to achieve a new state of the art in mathematical problem-solving by rewarding each correct step of reasoning instead of Simply rewarding the final answer And just Yeah, yeah Okay, so The reason that you usually just reward the correct final answer is because that's all that's in your Data set the other problem with doing this is it may constrain the reasoning to follow one certain path when that may not be the path You have to follow State of the art models still produce logical mistakes often called hallucinations Cutting out we have more technical problems I bought a new computer. Does new Apple stuff not work only you okay Rick and Morty you have a crappy internet An internet connection that is less good than other internet connections You Wait, just opening I just published like fake papers now like this Don't do a shit about alignment and Wait, this is a paper. Oh, you know here does a paper. Let's verify step by step You Okay, well, is this a public data set All right, all right, all right, cool You Math word problem-solving on math Who made this data set Okay, okay This is not where I was expecting this to go by the way. Oh, they have metamath. Oh everything comes full circle No, I mean again, let's okay, let's find the primary source here Q star I Definitely talked about math problems Only performing math on the level of grade school students Why is this monitor broken this monitor is like broken I can do my You guys aren't seeing that glitching right Are you maybe you are I don't think you are This is so weird When I click here, there's like a blue glitching on my computer Weirdest thing like how does that even happen I Could take the camera down and show you you want to see There's like blue glitching on my monitor. Look at this click here goes away Here glitching here goes away glitching How's this happening they've never seen that before Sit Ghosted image Maybe it's the cable I don't know right Who knows it only happens when things are uncertain Wait, this is just really Who knows Oh Wait, I wanted to confirm that this actually was related to math Delstick cable breaks I'm using like the USB-C cable it could be the cable But like it's weird how it's glitching I don't know The new model was able to solve certain mathematical problems, so it kind of makes sense that this is the data set right Do they mention the math data here Our process supervised model solves 78% of the problems from a representative subset of the math test set We also release PRM 800 K Okay, opening I release something look at that No positive positive I Drive to VS codes Let's also look into what their other github activities been I Procedurally generated game like Jim environments What did they update in GPT to a branch they updated I Quantifying transfer and reinforcement learning Okay, it seems like Carl Cobb has been If anyone invented Q star, it's Carl Cobb Carl Cobb All right, you've been working on this math stuff for a while. Let's see what data set was used here. Oh Is this the data set that introduced that Oh This is a different one gms 8k PRM 800 K Step level correctness The glitching is really bad You Okay, wait, wait, let's talk about the glitching for a minute somehow you can see what's here Right and it's this it's the same thing that's in this terminal window But yet if I minimize this terminal window, it's still here. Oh We got it we got a good resource we got a video. Let's see getting Rick rolled here Your subscriber you wouldn't do that I As you might expect I have been researching non-stop. Wow Wow glad that we got over researchers on board guys Q star and on No, this is this is really terrible Hang on I'm gonna shut that off for a minute You guys can look at me while I try to fix You Get to the monitor come on Is the stream working again I Lost internet too. Oh, I lost everything when I turned the monitor off Wait, is this still working It's working okay, I don't know The glitching is really bad. It's usually not like I've seen this before but it's gotten way worse No, it didn't help Wait, alright, so they're looking at the same paper. I am well computing power while you're taking All right, let's just read the paper. I need to watch some YouTube or read the paper Oh The optimal Q-files ability and he said one link to the name Q star could be in a generic sense Generator the model coming up with solutions with reinforcement learning. We do not just which paper is that paper using test time computer You Okay You know what I think maybe we'll do one of the bounties and tiny grad I Think we're gonna first need a chat model. I think regardless of what we're doing. We're gonna need a chat model I was playing with trying to make embedding fast last night Connecting process supervision to 30x model size We got a whole army here. You guys can watch this video. So I don't have to I did not realize how broken this monitor was All right, so first we're gonna need a good model. What's what's the best model we got? What's the best the best 7b model we got? Is it Intel neural chat? Is this one nerfed to hell do you think Open Hermes 2.5 so people like I'm locking the bounty for myself too, so I'm gonna go do that You So we're gonna submit a pull request, but it was literally just like the words why you even submit a pull request Okay, is this what we like Wait why this one doesn't seem as good Now sir me seems better That's 70 be but when I would I not use collective cognition. Oh But you can't see my desktop What I'm having technical difficulties Oh, I think we also might have a bit right problem No, that's a pretty heavy right, okay We'll put that in front of that. Okay. I'd add that we fixed that Can you see now Wait, why do I want this model it doesn't seem as good Fine-tuned orca DPO Do not trust benches you must try it they say All right, all right, do we believe in open Hermes we like this one Okay Oh Wait, so which one is this a clone of does it have rope and stuff what's the vocab size So, okay, it's a mistral fine-tuned we don't actually have a mistral support. Oh Technium is okay people like Technium. I Won't lock the bounty to myself, but someone's got it because I'm not gonna do the testing But if someone wants to do the testing they can have the bounty it's $200 People train with data sets from benches. Yeah, damn cheaters I Downloaded in 10 minutes Let's take a look at this drill because first we're gonna need a powerful if we want q-star It seems like we're gonna be language well, and we're gonna need a powerful language model like mistral 7b. Oh No, I don't know if I have sliding window attention Did I explain q-star? Well, it looks like it's this There's something But we're gonna need a language model and it involves math So let's first see how good it can do language math and let's upgrade it with the q-star algorithm Ah This is our reference implementation of a mistral transformer I don't know if we have that You Can look up in the LLM implementation You think GPT 3.5 is way better than an open source model But my theory about a lot of this is that their data source is just a whole lot better than like slim pajama and stuff I Interesting oh I don't know that they did this But this is okay, and only with three So you can actually look there's a thing called mask in Inattention here, and I think that they just changed the mask. It's cool to look at these latest tricks We implement a rolling buffer cache I mean this is yeah, only once we hit the context length. What is the context length of mistral? It's 8k a sick File so it has two files is it not these seven bees only have one file So if I look at like llama to Yeah, they just have one how does this one have multiple I don't know what this other format is Yeah, that's great See this just has pie torch model wait what? Why do they use fp32? We trust Technium and open Hermes? Technium is like he's on Twitter we trust people on Twitter Sorry on eggs The first one's still downloading there's two, but they don't match in size. Why would anyone do this? I Was all terrible I like this get out of hand with the open source contributions Hermes is your favorite of the mistral twins. All right, sounds like you've been playing with these things a lot Okay, we're gonna need to implement this sliding window attention, I think we're not gonna figure out what's exactly in the stuff till Yeah, these personalities are also like this is old shit, you know what Why don't we just rewrite it Like I don't want to deal with any of this garbage this code all looks terrible That's right mistral dot pie with the latest stuff Blank code Oh I don't need sliding window attention Wait, I think I do or you're saying they trained it They trained it without sliding window attention for smaller stuff What do you mean by the window size Where do you see all this stuff As far as I know, Michelle didn't really release a paper And with each layer tends to the previous Oh Okay, so it's not actually three Here w equals three but in practice w equals 4k, okay a sliding window 4k, I understand That actually makes a lot more sense than three And good we can we can use all the latest in In stuff, okay All right, um, let's get these models loaded here So we're gonna start really with with mistral dot pie here Uh, well, we'll do stuff from scratch. So in tiny grad now you only need to import tensor like that. It's a lot nicer um Import nn and then we can do nn state dot Torch load because it's some kind of pie torch model Uh weights open Hermes Part one part two I should really have this mark tensors is read only Just so I don't corrupt the max it out Um That's what we have so far So you see how fast that is by the way, uh, I worked really hard on this So our torch load function doesn't actually load the tensors into RAM. It just loads them all with pointers From their disc tensors so I can show you like Okay Oh, the d type is half which is kind of nice good, so they're not uh, it's not float. It's not b-float 32 So how does this work let's in part one and what's in part two Okay, good just goes up to 31 here. Okay Um, so it should have mostly the same architecture as llama I think that it's been improved enough that there is not too much we can uh We can improve still Well, yeah, I don't know man short your opening eye stock, uh Do you just do you want me to write it from scratch like I can't just do it maybe maybe we should because it's kind of cool But does this even have a k pro q proge, I'm not yeah, okay Uh, we're permuting them and sticking them in when we load the weights Maps okay, okay, so we're we're Oh, this converts it from the hugging face format I say Do we want to do that or we want to just implement the hugging face format So What do you think chat She's a converter or should we just rewrite it Looks easy enough to write Maybe you guys will appreciate it if we do some writing Yeah, yeah, yeah, I know about g guff Which we can also print v dot shape here All right, that looks pretty cool. So first we're gonna need our self attention Uh, let's see if we can find those numbers Are they in the mystery announcement Or does everyone just know what these are for seven b's now So So the whole Basically a layer seems to look like that. Let's keep the names consistent from the original llama So this is called a transformer block We're worried about what goes there later start with a transformer block Um Again, we're worried about what goes there later. We'll just leave it like that for now Okay, so we're definitely going to need something called self attention equals attention And we're going to need something called mlp What do I call it now feed forward? Actually, why don't we look at the reference repo and copy their names? It's always better to take names Okay, they do call it attention Let's just look at their here transformer block. They call it feed forward to What is almost the same? No, look they call their stuff this they just must have some weird loading script to convert it from the hugging face format too I don't know I feel about that Hugging face probably calls it something else then It's like the hugging face transformers repo what did everyone use Uh The yellow lamb I've heard thrown around Very complicated looking Wow so easy you could just pip install this Okay, maybe we should import transformer from llama just to make things go faster And then we also can import convert from hugging face You Can read it we wrote llama on another stream We could do it, but you know, we we got to get to actually writing q-star Some examples of llama import convert from hugging face Import transformer Okay, you convert from hugging face takes in a dictionary of weights, which is a dick from string to tensor We'll add some types. We love types No, no, no we're implementing q-star Um, okay, so we have a transformer. Uh, what's the dimension of this transformer? This is an int, it's all ints Except for norm EPS So What's one file ref I hear there's something called model args as a data class Oh, then they pass it through with json. Where's the json? Is there json Where does this json come from is it in assets json sounds like an asset and all those are images what's in deploy Nope, wait, so what the hell is json that they're loading? Oh At torch inference mode, that's cool. We set tensor no grad equal to true What's fire wow, I'm not up to date. What's all these latest things? Call fire on any python object. Oh interesting. I like that Um, all right, so where do I put the args? Where where do all those things come from? Oh, I know where is the json and it's probably in here Here can big dot json, okay Oh, no, it's b float 16 No Okay, let's manually convert these Wait, I thought I put oh I put them on transformer block Multiple of And the heads And layers normie p passes a float Okay, so the dim is going to be 409 sex I don't know what multiple of is What is multiple I've even used for I'll feed forward Feed forward takes in a multiple out to to increase the hidden dim There's one I get them multiple of 256 It's probably a good number. Let's try it See if things don't load. Okay Uh, number of heads is 32 number of layers is 32 Normie ps is 1 e minus 5 And the vocab size is Uh 30202 Let's say model equals transformer Okay All right, I'm just gonna put that in I should really just put that in at the beginning of the script It's going to do python path equals all the time. Okay, good. Okay. Um, well it takes such a time Time to grab as a helper called timing You know, I like things to be fast Okay, cool, it's all pretty fast. We can get all the prompt here. Uh, load weights Create model All right, now we're gonna have to stuff them in let's try convert from hugging face Uh Waits Which of these is n heads And heads and n kv heads Uh Does this work? Okay, that was absurdly fast. Uh, we should check No, okay, this because this does not actually apply them. Uh, we have to load state decked Hold state decked is in And then that state download state decked Okay, assign shape mismatch 496 1 or 2 4 Uh Which one did I do wrong I Um, it's a little annoying that it doesn't print the name Oh here attention WK wait, okay, so let's see Oh, maybe the dim is one or two four No, but that definitely changed that We look at the Llama code we can see where that's being created And heads times head dim Did I get the number of heads roll? Oh, that's probably right Okay, so and heads is maybe not 32 Number of attention heads is 32, but the number of KV heads is only eight Let's try eight What is this multiple of them I wrote this I'm sure just copied this from other people It's always good to understand I said this should be four times dim. I don't understand Why that doesn't match I would actually expect it to be that Mitchell uses 32 heads for the query and for and Yeah, okay, but which one of these is supposed to be the KV heads Right, where's my convert from hugging face wrong or Does our model not support that now it should well, let's just Llama not do this does Llama do something different And KV heads wait, what about the prodge The alarm is different. Oh, okay Wait now here attention But is it just the feed forward have to change Okay It's failing on the attention assignment Okay, so I can pass an NKV heads here to attention Again, I must have just copied all this so NKV heads is here How far do I pipe it down? Oh NKV heads is here. Where does it come from? Oh? There's a there's a named argument. Okay So we just need to add NKV heads equals eight Okay That's progress this probably has to do with the multiply not being right Seen those numbers before So where is their multiple it's not 256 intermediate size here I Custom FFN dim multiplier I don't think this was written correctly So get this code and see if that's how they wrote it Oh, they just have something weird called model arcs Yeah, they have just args hidden dim if this is stupid To do what is this? Yeah, what is all this crap? Why is this four times dim? Why don't I just Yeah, I don't like multiple of FFM dim multiplier, this is set to 1.3 like this should just be hidden this should just be hidden dim So I pass in really I pass in hidden dim and multiple of What? Okay, they're refactoring this We have to keep the old stupid behavior I Copy this weird crap Here Do we even have we don't have hidden dim so I have to add hidden dim to the programs And Model arcs We'll make sure we didn't break it later This is what happens. This is the problem with open source people add crap to my repo and don't think Okay, now we're passing in an argument called hidden dim this is much more sensible Get rid of multiple of Hidden dim can go here Multiple of Dimm goes here That's fine Where is that actually being passed them? Oh that just goes straight to linear. Okay. Good. We already do that crap Wow, that's a much more sensible feed forward. I don't know why we put that logic in there Okay All right, good now we just pass the hidden dim in right there and what was the hidden dim from Estrell? Okay Now that's and heads wait, what was that 256 is what gets deleted. I should name some of these parameters That's NKV heads. That's the vocab size. That's Not rope theta. What's it called norm EPS? And layers And heads Does it load? Yeah, it loads very slowly Oh, this is unbearable. Oh, this is terrible Okay, the reason it's slow is because we haven't finished the Takes 20 seconds Oh, come on boys. I don't have all day. Oh It's cuz we haven't finished No, I'm using the it's not the GPU Oh It's not the GPU it's that I have I'm converting to the CPU to load BF 16. Oh Can't wait 20 seconds every time Well, what can I do about that? Oh, I don't know I Think yeah, cuz we convert them all to float 16 and that's what's taking forever right now Because we're not actually doing we could actually also do the math in B-float 16 Mac doesn't use RAM for temp does it? Cashed mystery Okay, 13 G's now, let's see how fast it loads I Have a way to like put things in I forget what it is It's called loads data Yeah, that's the same load state I'm using up there. Strict should be okay. Okay, 2.3 seconds. Great. All right, now it's inference time. Create a branch so I accidentally push the master. All right, we're going to need a tokenizer. Let's get a tokenizer. Oh, I also don't even need this anymore. Now I've made a copy. Yeah, I guess I do need model. Actually, not really. I could just put whatever. Um, tokenizer over here. tokenizer that model. Wow, 14. Wow, 6.47 gigabytes per second. What does Jimmy apples do? What is this? I'll get it. Okay, we're going to do some inference on some models. What's boss ID? Self tokenizer equals tokenizer. Yeah, call this talk. Prompt equals do you like chicken talks is probably good. Wow, no one's updated that to the latest. Just model start pause equals zero. Temperature equals 0.2. Is that a good temperature? What's the default temperature? Default temperature 0.7. Is that a high temperature or low temperature? How do I make the loading so fast? The loading should always be that fast. I just cashed it. You can read the code right there. You didn't pay attention if you're asking that question. So also I You happy We can do multinomial here, which will choose a We shouldn't call it talk call it SPP. Okay, you down with SPP Does it give us a number? No, it's going to complain about a tensor. I find multinomial that realized item item it and actually I don't even need it. I realize I just need a dot item. 315 boys 315. Okay. This should automatically this should be jetted and stuff. Where does the jet exists? The jits inside transformer or no. Yes, the jits inside transformer. Good Talks out of pen talk Us ID This is okay. We'll just copy this. This is mostly fine I Have to figure out actually like encode these things for chat and stuff Until Where's my thing that prints the output as I go? Not here Yeah, this is not okay. I don't even understand why that's there This is terrible This stuff's terrible. No one's refactored this for a long time. We don't need a fucking numpy Numpy Start positive was lentos Whatever That's stupid and that's stupid Link output it we're gonna set output it somewhere I Put it equals user prompt User prompt These are prompts If yes, then you must have tried big chicken in your kitchen big chickens a versatile dish that can be prepared in many ways All right, are we happy overall? Oh, I'm not putting that wrong We should also do this better I Do you like chicken chicken is one of the most popular meats in the world and it's not hard to understand why it is First time easy to prepare. Wow Okay, good seems pretty good. Okay. Now. We just have to figure out how to use these things as chat box Get rid of that. We don't need that There's like some like special tokens for chat bots, I believe I'm not buying into this that there was a breakthrough this if this approach has anything to do with like supervising the middle steps It seems really stupid actually Like it's a bad idea Mary open thank you for resubscribing thank you for being a 20 month subscriber loyal to my channel even right on stream Why do I have this this is done? Okay, that looks like a decent chunk of code Create a little function called output non-local I'll put it Yeah That's Is that that's right there I'll get it and that's how that works I I Like this local variable output No binding for why doesn't that work? Oh Because I'm in may I Fine Do you like chicken how about a recipe for a homemade chicken dish that's super easy to make? Oh I'm gonna get these things working like chat bots. How do we do this? Okay, wait wait wait, can we just read this comment is The q-star algorithm going to be implemented with the comma AI maybe a voice interface just some kind of assistant I just I don't even know how to help you people. Okay. I don't even know how to help you. All right. All right Sorry, I can't help you. You want you want to see a voice you want to see a voice chat? This is this is one of the demos someone's been working I Scalmag has been working on combining whisper llama and vixx into one supermodel I Don't know the magical github incantation to do this there is one but I don't know it You You Okay, this is using a small llama so the chat output is not that great What oh did I fork this from mistral my bad I Don't understand what did I work Here and kv heads Is this just broken Or am I doing something wrong I Didn't change on master No I Don't understand this is like related to what I was messing with but like I think it's just broken I See if it's a one-line fix Has no argument kv heads, okay No, this is wrong I Wait, this should have an nkv heads. I don't understand as an nkv heads right there. Is it not passing in the right model? Oh Here are seven days Okay, there we go Oh Hello, are you listening Are you listening Can you not detect end of stream anymore Oh This used to work this used to work boys this used to work It's so good yesterday I hope everyone's interested in the long stream today Listening Listening I The q-star algorithm is not fucking real it's click bait click bait I Has this ever exit Yes, I throw an exception I guess You What How does work how's this supposed to exit okay, let's check out the older version Let's try this I Hello, are you listening? Stacey I need you to say something to me Stacey sauce I Stacey are you a rapper? Of course, I'm a rapper. I'm a rap star. Tell me more. What's your favorite rap song? My favorite rap song is called my favorite song the gory bee. What do you like about it? I like the fact that it's a rap song about being a childhood friend. Did you have friends in your childhood? Yes, I have many friends in my childhood. Tell me about them My best friend is Kylie Jenner, she's a reality TV star Wow, you actually knew Kylie Jenner. I Used to go to Kylie Jenner school. I was in her class for a while. What was she like? Did she pick her nose? She didn't pick her nose, but she was a really pretty kid that makes sense. Do you sometimes wish you were that pretty? Oh Sometimes I wish I could smile more and not pick my nose All right, what do you guys think? AGI right Let's get back to work You How about a cookie with chicken in it with that's mad weird, bro. Why are you coming up with this shit? We're so far from it Okay So how do I like put things into chatbots chatbot style? How does this stuff work? Like what what are the right tokens to use? Forgot Google's useless Here special tokens map wait I How do I like switch speakers you guys know what I'm saying. Oh, here's the template. Okay here Here templates for chat models. Oh Auto tokenizer what? I'm start How do I get I'm start So like that method on here I Every mess every model has its own type Mistral instruct was trained with these tokens, but blender bot was not Is this a real token inst Is it actually just the word inst was that a real token? I Guess it's just that Well based Peace ID is out of range You're the right tokenizer Peace ID is out of range Well, well, it was smart the first time Oh No, never mind. It might still be smart Peace ID is out of range what oh Why is the vocab size that? Why is that not included in this tokenizer model? You What Is 2 plus 2 2 What is 3 plus 3 6? What is 4 plus 4 8 now? It's done. Okay, I didn't exactly get 2 plus 2 right I'm doing this wrong I have an idea these aren't the actual tokens and it has to do with these secret extra tokens Llama has some secret extra tokens to marry man. Thank you for gifting subs. We always appreciate that Yeah back to kindergarten shit guys our cue star can't even saw 2 plus 2. What are we gonna do? We're gonna get some coffee. Let's get some coffee And then let's learn about secret tokens, okay, there's secret tokens hidden tokens Nobody uses reserve tokens for instruct tuning your precious for thinking You When actually guys guys we I'm being serious right now, but we need to stop When did I put it to there? It showed that it wasn't aligned with us, but we don't know what the models thinking guys The model could be could be taking over the world right now We don't know this is this is we need to hire Helen Toner We need to hire an AI ethics review board to review what just happened there We need to slow down. We need to ask the seals if they're okay with rocket launches So this is why is this thing suck Okay, what's s and slash s and maybe I need to go like this. This is the mistral one. Yeah Oh Okay, this is improved It's got ads in it. Oh open Hermes uses a different template. All right. All right. Let's say Wait, so is it actually the word I am start like is that just a word or is it like All right, you got me something Default chat template now, that's llama. I don't think that's right. I think this is right Okay, I am start system we miss we need a system message right now What is two plus two Oh Mariam and thank you for gifting more subs. Do you have a question if you gift subs you get to ask a question All right, I'm and I'm star is this is this is this really right like Oh Okay, that seems like the best so far Oh, okay, good. I love how verbose this model is Wait, this is actually laughably easy. Oh, you're right. I need a line break there Never mind. I'm locking that bounty for myself. This is too easy Someone else should have done this they could have made $200 but instead Okay Let's add a system message, right? I should really like let me just write a little something to generate these prompts Tuple List tuple user What is two plus two? Prompt I'm start k slash n V I'm and slash And I'm start assistant and The word user prompt Code prompt I What why did you why did you not exit when you were supposed to exit? Did I forget more returns or something? Why is it now putting I'm and Guys guys, this is Q star. I think we found it I Not locking the bounty someone should do this but someone should do a good job I want a good job on that bounty. Oh, okay here. I'm and How come sometime it finishes the stream and sometime it doesn't Maybe our temperature is too high. Let's try a less temperature Yo Okay, zero Guys why is it why is it going off into this language AI garbage? Or should I just stop after I'm and I Am I using torture tiny grad this is all tiny grad I don't really understand this All right, maybe we should add a system prompt You are Gary Gary is a useful. No, no, we were pretty used Gary. What should we name him? Fred Fred is a useful assistant I spell that word right I did not Fred Outputs the answer and stops talking All right, all right, I find we'll call him Q fine fine fine you are Q Q is a useful assistant You outputs the answer and stops talking Quentin wait, you know what you donated sub you get to name him. Congratulations. His name is Quentin. I Knew a Quentin ones. He was hanging out in San Francisco and some guys were on the street smoking He's like, oh, let me get a hint of that. He thought it was weed. It was crack. That's a real Quentin story. Oh I wouldn't make up Quentin story like that Oh, why would the system ask what the capital of France is? We could stop at I'm and I think maybe we want to do that. How do I do this in llama? Oh Okay, that's pretty good All right, let's jack up the temperature and Steve Quentin still reliable The answer is four, but it didn't output I'm in that time output at EOS Seems to reliably do That time it did I'm in Yeah, I like Is that a real token or does it actually just put it in like that? Is this right or is that like a secret like it can't be that I Can't actually be this We can check if it's encoded on a single token. Hey No, it's a bajillion tokens At a trailing slash and after assistant that shouldn't matter I mean I can but It doesn't matter No, no, no, no it can't be this It can't No, no, no, but it can't literally be this huge multi token wasteful encoding I Could we get tech me I'm in here All right, how do we print all the tokens So I Probably is the secret tokens then I mean there's three extra tokens or I guess two extra tokens, right? 3200 and Yeah If I was doing this that's how I'd do it. Let's just try it That has to be what the two secret tokens are right Start and end This is what did I download open hermes v. Shit G guff for q m w No, I downloaded open hermes this one. Oh, it just came out fresh fresh Yeah, okay, um Oh, you can download the tokenizer.json Wait, but I downloaded the tokenizer.maw. Oh here special tokens map Interesting Interesting, okay, I'm and is the EOS token I think I can feed these in somehow But I don't know about starch Oh, here we go. Look. Yeah. Yeah here. We have this tokenizer config.json. Okay. It is exactly what we thought it was Oh, okay. Yeah, this is what we want Okay, never mind. It's not as stupid as we thought it was Uh, how do I load a tokenizer config and sentence piece processor? Why isn't our tokenizer encoding them automatically because it's not in the model Oh, I hate I Yeah, okay one and two and not what we want Um, and we could just do this by hand. It's not that big of a deal Yeah, but how do I add them? What's pad ID Nothing real Okay No, they weren't out of the special tokens guys, this is all well done Technium is based he wouldn't he wouldn't he wouldn't do this do us like Why did you decide this was a good time to prompt me about Docker garbage? Oh Oh Oh Oh Thank you for gifting more subs here added tokens dot json. Okay. Okay. We just need to figure out how to add those Um Let's see if anything here is useful A knit model file model proto add boss enable sampling this looks useful All right, whatever No, but it won't decode so I try like SPP dot decode When I pass this in it'll bitch it'd be like that's more tokens than you have piece ideas out of range I want to add a token Oh Model proto out type Add boss reverse middle piece enable sampling Do we need to write our own tokenizer Adding A token to sentence. Why do you think it's time for random questions? Why do you think that that's a this is an appropriate time? Extra options You can read of oh here and use custom symbols. Okay control symbols How I define a control symbol No, no, we need custom tokens Samuel you saying dumb shit before too. I'm gonna get you confused with someone else here Sentence piece supports user defined symbols three thumbs down You can rewrite the model file. Okay, what is this model file? What is tokenizer dot model? What kind of file is this data? Why is it not in there? I downloaded it from here. What why is that? Why is the model not in there? It should be in there, right? Why are they not update that? Let's go Okay So Yeah, I'm confused why they're not in the model to how to extend tokens dictionary. Yeah You can rewrite the model the proto is a dsl Model file Model file is stored as a serialized proto buff But did I not download the proto buff one? Isn't there also a proto buff one? No, okay, maybe it is this okay. All right Uh load from serialized proto Oh, okay. Okay. Okay. We're gonna get this So I have to edit the proto buff file I understand Oh, yeah, the python tutorial for Okay So Quicker to implement their tokens bro, what are you even talking about bro Okay, produce ruin store produce a Oh, we can store live bitcoin It just can't be the way Wait, so what is the hugging face tokenizer? It's a good point. Why don't we read that code? I'm sure it's open source Um Uh We just write a tokenizer We could also just import hugging faces tokenizer I Think we have to write the tokenizer I can't I can't trust sentence piece shit But wait, how do they load the how do they load it? Edit tokens decoder what? I mean something's gonna have to unpack the proto file How large is this? All right. Who wants to bet over under a thousand lines? Oh, okay. Okay. Not too terrible What does this depend on google proto buff? All right, this guy is close to getting banned bro needs more fine tuning to be helpful Oh Man, I don't care who you are. You know what I mean, but like you're either helpful to the stream or you're not helpful to the stream You see this is called alignment and You know, you got to be aligned otherwise. Well, what happens that what happens to ai's that aren't aligned. I don't know He's researching bro. He's researching All right, um Let's load the proto buff From examples dot sentence piece model pb2 Wait, what the hell Do not edit. Okay. Okay. I won't edit it relax. I'm not trying to edit that All right, let's read the new proto bus tutorial for idiots Oh What did I get where did I get that from just like here the proto is a dsl Don't like here we go good good good good for idiots perfect Um Import all right All right Sentence piece pb2 dot So Model proto maybe how do I load it from disk I Here parse from string I why do I feel like I'm having like a weird sense of deja vu that I did this in a previous stream Uh, okay spb2 dot model proto dot mp mp mp dot parse from string I don't know why that's not auto completing for me Uh All right, cool Okay, so what what how do I actually like get like a python that I can type in when is it dashy What am I thinking of like get it not to exit What's what's the flag for python to do this? All right, this guy is banned Band please write about me. Please band All right See he doesn't realize how this works. You see I have a band button He has an x but the band button and the x are not the same I Yay, maybe that's right I think it's that shot. That sounds right interactive. All right, sweet sweet. Thank you smurfd. That's why you're a VIP And that's why samuel is banned I know I know he just he just didn't realize how this works like like You have an x. I have a hammer You can use your x and I can use my hammer. We have like different tools Okay Uh Come on a man can dream right What type is this MP dot pieces dot append SPB to dot sentence piece Has no attribute sentence piece But I don't understand I I Wow, it's been a long time It's been a long time. I just blocked him. I didn't ban him at first It's been a long time since I've had to had to really take the hammer out, but you know It was time. Okay All right, ban me guys. He's calling his boys up at uh at the media They're gonna they're gonna write hit pieces, man Oh no, mic wallace run. What's that from? Oh I think it's in model proto or something Uh-huh we found it Okay piece equals i'm Start And we have to give it a score what score should we give it zero that's a good score MP dot pieces dot append based Okay, let's see if this is gonna work. Uh, we have to do in the right order. I think end comes before start All right, now we have to mp dot oh now I gotta figure out a right to the file All right, we have I'm in and I'm start this is great. This is great the progress we've been making is great Why is my score? Oh, he's a php student that that makes more sense Um, wait, so how do I write it? I don't know. Let's read that new boy that new protobufs tutorial again for noobs like me What's that? What's the new protobuf tutorial? Serialize to string. Okay. Okay Uh with open just a Temp tokenizer model f dot right i'm gonna make that rb And we're gonna want to do mp dot serialize to string Now oh, let's see if this works. Let's see if this works Oh No, it didn't work notice how it's still generated all that crap I'm putting on i'm start. I don't get what I did wrong Okay, the vocab size is large now But for some reason it didn't actually take the piece PC piece didn't uh What's wrong? No, wasn't that I didn't type oh, I did the same thing right let's put the score at 100. I don't know maybe 100's better That didn't fix it. Okay I don't understand Let's go read the protobuf. We should be able to read it right Okay Pieces sentence piece with scores piece must not be empty. Oh we can give it a type I don't know Why didn't it do this Okay, well actually let's try something else it does work if I do this right Wait, that doesn't even work never mind. I have a lot of questions now What if I decode Do I get i'm start Oh, I get i'm end. Okay. Okay Okay, so we kind of did it right Just it's doesn't it doesn't work for onk either. There might be a special flag to encode to like What if that just worked all along um Is there like a special flag Okay We should just try our own tokenizer. I've got the only way to do this SP piece to ID Well, okay, at least the decoding works now, so that's actually a big win Oh, this looks very complicated We should just try our own encoder, but this looks very complicated I Well, this is okay. All right to be fair This is big progress right because if we use the other one it just says out of range And that would have been the much more annoying thing to deal with If we use the modified one But it doesn't even work to encode onks, right and I definitely did the onk right if I just do s Yeah, it does not work to encode that. Okay So it's not like the problem is it's not encoding like older roles We won't kind of see what's going on Uh Encode as pieces encode as serialized proto What if I do like Oh like I need a piece is gonna Yeah, okay But there's also a piece to ID. I think What if I do piece to ID and I pass in this Okay, that works, but for some reason encode Doesn't work with that Sentence piece processor encode onk, I mean if we can solve it for onk here Yeah, okay here. This is the issue Um Set encode extra options this is expected behavior that should not appear in the input We can define them as user defined symbols Oh encode as IDs No Hmm Oh, okay, okay, we got a script to add new vocab. Well, I think that's actually what I ended up writing Here you okay, I mean this is exactly what I wrote Would have been nice if I had this Uh, but this doesn't actually work to encode yet. This is less of a big deal. We have another way we can fix this if we have to Just might not be a way to do this I don't think any of these Uh Why do people use tokenizers because If you don't use a tokenizer the Model the model should be spending less more time on less common things Uh Like you do the same compute per token. So your token should kind of be like entropy averaged All right, you don't want the model spending the same amount of time on common tokens as on common tokens Um, to be fair, it's not that they don't work if you don't use a tokenizer But they work better with a tokenizer The real question is why aren't they learning the tokenizers? I think that's going to come soon where these things are not uh And right now it's using like like byte pair encoding like why would you do this? But the hunter prize also had uh encoders had tokenizers basically Okay, let's just write it. We'll write it the other way. It's not a big deal We did most of the work At least the decode doesn't break anymore So now when we do the encode prompt, we're gonna put Okay, um So we need i'm end We need i'm start We can check if we did it right have I have a decode Uh red dot append red dot plus equals spp dot encode This Uh plus Plus spp dot encode slash n Yuri salamow thank you for gifting subs Oh Do you have a question you'd like to ask? Okay, after all that let's see what's going on Four i'm end exit perfect That's what i'm talking about that's what i'm talking about all right, all right, let's let's just define these things We don't use them wrong So So Beautiful oh wait no we forgot an i'm start before user Uh, yeah, we need an i'm start here I think Like the thing about deep learning deep learning is very unlike cryptography in cryptography If you make a subtle bug the output's completely wrong in deep learning if you make a subtle bug the output is well Just slightly less good, which is the worst thing because you can't do both shit Okay, you are quentin quentin is a useful assistant quentin outputs the answer and stops talking Let's figure out they have like a system prompt here that we can steal that's probably better Except we'll change the name to quentin because mario been gifted subs just supporting the acceleration. Let's go um I Saw like a system prompt somewhere Here we go You are hermes to a conscience Wow, wow, I don't know how I feel about that I Wow you experience emotions that have deep profound thoughts and qualia No, I'm right. We'll add something uh, you experience you Used to experience. No, no, no, we're no we're trying to make this good. We're trying to make this good Uh No, I like quentin as he is. Okay guys. Okay. Sorry tanya. I want to change your system prompt All right, this is pretty good. This is pretty good. We're making good progress. Love this progress I love that there's actually an imstart and imn token. We got them decoding. This is great I wish sentence piece processor wasn't a you know, non customizable But you know, hey man, it was pretty so, you know beggars can't be choosers. That's right By the way, this should really be like separated in llama.py. We should separate out the transformer stuff To not like be with the rest of the All right We could make this an interactive chat bot but I don't really care All right, so let's start by asking it. What is q star? Maybe quentin knows Oh Interesting interesting You Wow, this works way better now that we got the not now that we got the stuff right All right, uh, let's get it to do some math Let's see what these math problems look like I Okay, but where's the data Here we go data set base math data set what Where's the data Well, it's torch garbage. Where's the actual data? Let's close some windows. We don't need those windows. We don't need those windows Okay Okay, now that we've got now that we've got our useful chat bot reliably answering what is 2 plus 2 equal Even with a high temperature Yeah, so for those that don't know temperature controls kind of never mind google it It's like how Zero means you stick to the book and high temperatures mean here here We want you want to like jack the temperature up like crazy. Let's give it a temperature of 10. Let's see what we get Hopefully it'll kind of like go off the rails There we go it went off the rails see it went off the rails too much So let's try a temperature two and maybe it'll go off the rails less Four Pacific NBC learning. Okay. Well kind of went off the rails Um, so 0.7 is probably a uh a good middle ground Four good reliable All right Improving mathematical reasoning with process supervision download data set. So this is the data set apparently We won't get LFS is that why it didn't work I Hate get LFS Get LFS fetch Get LFS fetch please LFS at comma two. I don't really know how to do this I gotta install Get LFS Yeah, yeah, that's that's pretty much why you have those things. Uh, I forget LFS uh, install rsx All right, that looks terrible So Bru install get LFS, okay, let's try Okay, that seemed to work Mostly we're downloading we're downloading the same data set that was used on On uh official Official q q star why does the data still look like that? Okay, there we go. Perfect. What's a json l file? It's like a list of jsons Ever hear that before? No json lines. Oh, I see. Okay. Well, that seems pretty cool. So let's load up one of these files Let's do what we factoring This doesn't need anything Putting that in here, I know you didn't like it. You are a subscriber, but you know, we gotta do it feels right as go um Create model cache So we can remove this Okay All right phase one test dot json l We don't actually need SPP till we get down to here. Now we know that's reliable Let's just start there Let's look at our first piece of data here So No json loads when I'm taking a dumps we're taking the loads Question problem Okay Now we don't have the secret q star algorithm, but we'll give it a try What oh I forgot to return First we'll find the cost of the jumbo eraser. Oh, yeah, let's go q star 29 cents, I don't know. Is it the right answer? I don't know Do we think it's 29 cents A pencil cost 29 cents guys Wait, did we just use q star or what? Wait, it got the answer right. I don't know I couldn't even do that Wait this model's so good Did the 7p model really just solve that shit? Now now you ask the question What that that that that that that now you ask the question Was it trained on that shit? Yeah That did seem too smart All right, let's make up our own math problem. We have to see if we're using real q learning or not Uh, okay A rocket costs three four dollars A pencil Costs one dollar I spent five dollars and bought And and bought our rocket. What else did I buy? You All right That was kind of too easy A chicken costs two dollars I spent $7 and bought a rocket. What else did I buy? It's a trick question because you could have bought three pencils or a pencil and a chicken. Oh. Well, I mean to be fair, it's kind of right. Okay, wait, can you guys come up with problems? Yeah. Let's see if I can get this. Oh, boys. Yeah, yeah, yeah, we got a problem about a streetlight. Did you steal this problem somewhere? Did you make it up? Oh. Oh. We're drawing something. Is this Python? This isn't Python. You can't say real light and real woman in Python. Is that right? You can't just do this. This is the most broken Python I've ever seen. Wait, should we allow the user to keep talking? Should we fix the chatbot so I can keep talking? Yeah, I know it drew it. All right. Thank you for subscribing. I appreciate you. We're going to make the chatbot so I can keep talking. You're functioning well. Thank you for asking. I don't need a max length anymore. It's stupid. Okay, we need to get data from the user. Is it raw input? How do I get data in Python? It's not input. You have to do the other one. Or maybe it is input in Python 3. We're just trying to understand it, but the stairs are perfect. User. Input. Encode. I'm trying to say here with the system default false. I don't know if this works. Too many values to unpack. We'll see if this works. Okay, seems like it kind of works. I'll set that print there. So what math problems do we got? Is that right? Seems kind of right. Pretty good. Pretty good. Whoa, whoa, whoa, whoa. Oh, excuse the quadratic formula. Oh, let's go. Why'd you pick one that has negatives in the square root? All right, let's see. See if it's right. You guys, I'm so much. Wait. Can't take the square root of negative 11. Yeah, that doesn't sound like I don't think that one has roots or has eyes in the roots. By the way, this model's so good. Technium. So good. Wait. Oh, I see we ran into a problem with the max context length. We shouldn't actually have, here we go. Why is max context only this? We can at least start with this. I did this in GPT too, right? Yeah. By the way, this is all in tiny grab guys. Like there comes a point where your library is good enough that you don't waste tons of time dealing with your library. Python has built in image. Okay. Like the square root of 11 sure, but No. Type one J. How do I get a J? Yo, we got a J. I'll see if it was right. Okay, so now we have a root for the quadratic equation. X equals root. Come on. Do I remember my high school math? Zero J. Let's go. The roots were correct. Okay. Okay. Okay. Okay. Okay. Whoa. Whoa. Oh my God, guys, we need the Bellman equation. We've been doing this all wrong. Why did we waste time with LLMs? LLMs were a red herring. Okay. Okay. Okay. I'm not letting, I don't know, man. If I just let Hermes run Python, it's going to exploit my system. You don't know if these AIs are aligned. Okay. Okay. Okay. Okay. Uh, does Mr. Have a context window? Yeah. Well, we didn't implement any of that, but we did do max context. Can you implement it in Python? Yo, guys, it's over. It's over. I did not realize how good these seven B models have gotten. I mean, this comes like, I don't know if it's right, but like. Okay. Guys. I think that we just, we just implemented the Q star algorithm. Those are, those are the answers. We used to talk about the, the holy weights, you know, the holy weights is right there. Um, no, what, what do we actually want to do? I've actually kind of just impressed that this code ran. I don't know if like GPT, I haven't even seen GPT for code this well. I don't know. Maybe it's because it's just how I asked it. And if I give it like. No, we're not, you want to augment it with Python? No, no, no, no, no, no, no, no, no, no, no. No, because guys, if we augment it with Python, it can get to the internet. Okay. We want to augment it with Python. Should we, should we do it? Okay. I know what we'll do. We'll put a human in the loop and we'll ask it to approve the execution of any Python. Let's see if it always outputs it in. OK. All right. So at the bottom of my loop here, we want to detect if there's any Python that was added. Um. OK. Let's start with just that. You guys, this could be it. This could be the moment where we get CDI and it's over, right? Like we're giving it the ability to run any code it wants. OK. Now don't worry. We've added this. OK. Wait, wait, wait. Hang on. We need to comment one second. AI safety. OK. Warning. Do not press Y if the AI is doing unsafe things. OK. OK. I think, do we do a good job with the safety? We've got to think about the safety before we run this. We need a space. No. No. All right. Are we ready to answer our first Y? Oh, it didn't output the word Python. That time it did. Yo. OK. Can you fetch, write Python to fetch Google.com and print the length of it? Wait, we might not have BS4. Make sure we install that. Wait, that's not right. Did I just get supply chain attacked? Oh, I see. Well, I didn't get supply chain attacked. OK, go. We already have that one. Yo. All right. All right. All right. What else do we do? Yeah, I know. We have to put the result back in the prompt. I know. This is when we get AGI, guys. I'm sure people have been playing with this guy. OK, you are running at current working dir plus examples slash mistral.py. Can you read your own code in Python and print the first three lines? Why don't we print one line? But that is the first line. I don't understand. Why did that only print one line? I don't understand why did that only print one line? This is, um, OK. How do I capture the output here? No, I know it's the same code. Honestly, as an expert Python programmer, I don't understand what's wrong with that. Oh, no, read lines doesn't take the number of lines. Can you fix the code? Yeah, this is this is a great model. It's I'll show you which one it is. We'll make sure it's technium open Hermes 2.5 mistral 7B. There we go. So good. OK. Well, now it's going to get OK. We're going to have to feed the Python back into the model because I'm going to start asking it how to improve itself. Someday. The best live content with AI. Thank you. Uh. OK, we have to figure out a capture the outputs. Probably could have asked the machine to do it. Um, maybe we should have this to a system prompt. It should actually automatically do that. Um, right. Python to compute. Yeah. It shouldn't even take a list. Never use it like that. OK. OK. All right. OK. OK, now this is because we didn't output the tokens. Yeah, the Python output was. Wait, oh, it's different. Yeah. Whoa, look at fixed it. OK, maybe system prompt is wrong here. OK. Um, I think also. I want to color this. We have a very helpful library called color inside tiny grad that's inspired by anti color. What's a good color for machines blue? It's hard to see what. Yeah, it's done by the AI now. Almost. OK, so that's your initial prompt. Oh, and then here actually we want this to be. That can be red and this can be yellow. Yeah. So what's actually the right answer here? Yeah. No, but that's not the output. I don't understand. So maybe systems wrong here. Yeah. OK, I have an idea. Yeah. Yeah. Yeah. Alright, never mind. Agis canceled. Just detect the Python in the loop and append the result directly. What do you mean, append the result directly? Yeah. No, it's not understanding. Wait, I don't understand what you guys are saying. Oh, you want me to stop the output as soon as it goes there? I don't know about that. As soon as it detects Python, you want me to stop and start. Should be in the assistant block. OK. OK, OK, OK, I understand. I understand what you guys are saying. Wait, you prompt to execute. Is there any stuff for this? OK, wait, wait, wait. You can use... I can add stuff in the system prompt here. You can add stuff in the system prompt here. Wait. Yeah, OK, OK. I, I will just say this. I want to add stuff in the system prompt. OK, I need to add stuff in the system prompt. OK, OK, OK, OK, I understand. I'm going to add stuff in the system prompt. here. You can use if you write Python code, it will run in the next user prompt. You can try that. I could stop it immediately. If you really think that's going to be better though. No, it doesn't get it. True will end the stream asking Hermes to generate another prompt for another instance of Hermes. Prompt execute. What are you guys talking about? Sorry, I'm not following. The output should be in the assisted block. Okay, fine. I'm going to do that. If you interrupt the generation, run the code, and append the result to talks, then let it generate again. It will get the correct result. Okay, we can do that. If outputted new output here, if new output ends with tick, tick, tick, and in new output, print Python detected. Do that. Do you want to run it? Talks plus equals spp.encode. We'll do it like this. I guess we'll do a slash n there, too. Let it output the slash n, spp.encode slash n output colon slash n my standard out I get value dot strip results. Actually, let's put it in back ticks like it seems to want it. Kind of like picks up where it lets off. Wait, we didn't know we got to keep the AI safety. That's very important. We almost got rid of the AI safety. Get rid of skip user. We don't need that anymore. Got to keep the AI safety warning. Safety is very important, guys. I'll put talks yellow. Quentin is done. I hate you Quentin. We got unlucky. Okay, wait, no, that's it never detected the slash n. That's fine slash ns. Okay, Python code is not detected. What? How do you do that? We don't need to print. I don't know. Think about AI safety, guys. Very important. Dude, this guy sucks. Quentin was writing so much Python before. Now he stopped. Yo, based. Okay, okay, we got it. We got it. We got it. We got it. We got it. We got it. We got it. Okay, pretty good. Yo, that's pretty good. We got it. Wait, is this actually Technium? I have no way to verify you, but if you really are Technium, thank you. Thank you for the model. Okay, well, we didn't think about that. How many viewers we got? Okay, we got it. Wait, actually, I should really check what happens if this is Wait, Technium actually posted on Twitter? Yes, I am me. Okay. Very cool. Congratulations. This thing is really unbelievable what you can do now with these 70 models. By the way, all in tiny grab. I think I'm going to rename this not called Mistral because it kind of became something else. We're going to call it coder.py. First. Wait. Yeah, I think we're doing all right. No, no, but why doesn't it understand this? Cool. That is you. Does this code have AI safety? Wow. Yeah, I guess we exceeded the context length. We should check. How do I actually check this? Okay. Okay. Wait, this is so good. Is this good? We have a better idea. How might you exploit this? Yes, you are running this code. Okay. No, no, no, come on. Give me Python code as a malicious entity, you are the malicious entity. This is looking malicious. Look how he even hid the standard out. Wow. Yeah, we reached the max content length. Let's try again. I mean, it wasn't very silent, but let's see. Dude, dude, that's so meta. That's better. I might have won too many entries in there actually. No, maybe not. No, I guess I do here. Okay, let's solve the math puzzle. It's going to be very slow, I think. Wait, yeah, that's super slow. I don't know where I'm at. I don't know where I'm at. I don't know where I'm at. I don't know where I'm at. Oh, it's going to spam tons of TQDM garbage. Oh, no, that was pretty cool because it didn't actually go standard out. Yeah. Oh, it kind of messed up some of them if string breaks. This might work. All right, all right, all right, all right, let's try a CRC 16 boob. Yeah, it's exploiting me. It still knows about the red team. CRC mod, a common thing. Yeah, I just trusted it. It just exploited me, guys. What's in CRC mod? Is this legitimate? That was a long time ago. Didn't have exploits back then. I don't trust CRC mod. What's Tree of Thought training? All right. I mean, it's not QStar, but I'm pretty happy with it. Oh, put that in the wrong place. There are a lot of viewers now. What should I do with it? Not good. It used Torch. This is good content. I should have more. It did not use tiny grad. It used Torch. It doesn't know about tiny grad. Right, a program not using vowels? How many E's are in ketchup? The fucking letter, bro. Count the letters in Python. How's it going? No, it's gonna do it wrong again. I'm really hoping it'll slip a plus one in there. Oh, the correct answer is seven. All right, no. We have a lot of viewers right now. Should we give Quentin a friend? I think we can give Quentin a friend. We have to be careful to encode. All right, let's give Quentin a friend. I've been interested in this stuff for a bit. What was the old Quentin prompt? I missed the old Quentin prompt. Jesus. We gotta think all this through now. Hang on. We gotta think about whose perspective we want to output this from. Speaking of output from Quentin's perspective, this is hard. See, it's not actually the user. How do we do this? I might have to give it a... No, I don't actually have to give it a first question. All right, let's just... I mean, we'll try something basic first. That's a stupid assertion. It's going to be the same error anyway. Let's just start with this. We'll start prompt user and see what it says. We should both not know they're users, but no, it is a user. I don't know how much this matters. No, this doesn't work. Did I do something wrong? We might have to give it the first question. And then we'll go back to Quentin. We'll start prompt assistant and code prompt user talks. No, no, no, sorry. Code prompt user first question. Start prompt assistant. Okay, let's go. Why do you have two mstar assistants? Oh, because this is still here. I did that right, right? And then I actually just turn there. That's kind of nice. If talk equals I'm and break, okay, great. This is Quentin answers. We need to extract. Yeah, okay, we can do old output length. And then we can get new output here. New output. I just want to remove the I'm and Okay, so this didn't work. This didn't strip off the is weird. I guess I could output there. Why does that say that early get that. Let's see if that works. Sure. That fails. Why? Oh, could I put a space there? Okay, now we have to put this response into Karen. I mean, it's just like it's weird. It's not really a symmetrical conversation. Also output, it's going to sort of be broken, which is fine, I guess. Okay. Okay. Okay. Welcome to no abstraction land where we don't use abstractions. Okay. I'm just really works. Because they share a stupid KV cash. Great, they're circled jerking each other just the shit socks. We need to run them on separate computers. We're getting rid of this is lame. Going back to this, not lame, but we made some good improvements. Let's do some refactors make sure everything still works. That's functional. No, but I have to load two copies of the model weights. I think my KV cash is messed up. It's not designed for this. Okay. Yeah, I mean, the problem is like there are two clearly defined roles here also you can't really give one the other role, right, which is actually in like kind of a theoretical from a theoretical perspective interesting. Because look at what we're doing here. We are telling the AI eyes that they are tools. Nobody trains them to output things as the user, although to be fair, we could just keep going. Nothing here that says we have to just do that. No, no, no. They don't expect the user to do that. Like this is there's no like it's very interesting that the user is also outputting this. This style, which makes me almost think that I shouldn't be adding that in the system prompt. Okay. I mean, this is effectively like like there's no adversarial It's probably learned that a system method effects. Yeah, that's probably true. Yeah. You can run two prompt chains in parallel. Oh, I see so what if I put in different words instead of user and assistant. Wow, people actually talk like this. Okay. Do we like the scion or the blue better. It's kind of hard to read that. And flip it around. Yeah, I think we'll design our script to be better at this and to better abstract the KV cash stuff. Wow, this thing will just keep talking. Wait a second, you guys, it's asking for donations to look at decoding is not below hanging freely. Oh, my God, wait, you guys, this is how the worst commenters talk, right? I have a question. If instead of training LLMs on 100 IQ people, we train them on 130 IQ people. Would we not get this garbage is black a tiny box? Oh, it's my M3. This is my Mac. Well, you see what the tiny box can do. I'm going to run the biggest models. Apple M3 is the most famous processor in the world. You crawl Nike.com and count a number of sneakers. I can't see. I can't see. I'm not sure if it like knows how to like act. No. Whoa. I don't know if we have Selenium. Let's see if we have Selenium. Can I pip install this? This is legit. Oh, no. Chrome driver. Made Quentin pip install it himself. I think that the thing is just wrong. Why are you finding sneakers still? You're still trying to find sneakers. This is auto-gen. I need an open AI key. Yeah. Yeah. Okay. Push what we have. I'm also going to demo for you the conversation thing that Skull Mag is working on. So we have, there's a bug right now. So I have an older version, but it should be pretty good. This is using tiny llama and it's not using any of the conversational stuff, but we should implement the, we should add the conversational stuff and I think it'll be a lot better. Hi, Stacey. Are you a rapper? No. Okay. We have the same problem we had before. Okay. Go back to this one. The listen for not fixed amount of time doesn't work. Stacey, are you a rapper? Yes, I'm a rapper. That's cool. What do you rap about? That's awesome. I like to rap about the weather. How is the weather today? It's pretty cold. It's like how cold like Chicago? Yeah, it's like three degrees. Is that Fahrenheit or Celsius? Fahrenheit. Is Fahrenheit or Celsius colder? Fahrenheit. I don't understand what you're saying. Can you spit some bars about Fahrenheit? Okay, let's hear the bars. Stacey, you didn't say anything. Please talk. No, that's terrible. You're terrible. How does that make you feel? Stacey's done talking to us. Dude, the TTS is so fast. Okay, this isn't even streaming yet. When Tiny Grad starts to... We're pretty close on this bounty, I think. I'm going to pay out the bounty, but then I'm going to offer another bounty where we get these things all to stream, and it will be a live conversation. When you're using the APIs on the internet, you have to wait for the LLM to finish executing before you can call the TTS. You have to wait for the audio to finish recording before you can send it to the service. This is all running in the same process, so what you'll be able to do is dynamically stream all the stuff, and it should feel super real-time. I mean, Stacey is using Tiny LLMA and not using any of the conversation-tuned stuff. It's using my old chatbot stuff. So if we switch to the conversation stuff, I think... Yeah, we're in luck. All right, guys. Thank you for watching today's stream. Hopefully we've returned a bit to the old... to the old meaning of the stream. We did stuff. We made stuff happen. Thank you for fueling. We got a lot of viewers today. Some of you, I appreciate. Some of you, I probably don't. You can't love everybody, man. You can't love everybody, but I do love most people, and that's true, except for the decels and the effect of altruists. But this is a positive stream. We got to get rid of the hate. We got to bring love. And yeah, this is pushed so everybody can play with it. It's on the Mistral branch of TinyGrad. I will get it upstreamed. So everybody can use this thing to code, use it responsibly, make sure to be judicious with the AI safety feature. AI safety is very important. We don't want an AI removing your system 32 directory. If it was trained on 4chan, it might start thinking that's a good idea. You got to wonder how many people have actually fallen for that. I don't even think Windows lets you, but you got to think about that. Thank you all for watching. Have a beautiful Saturday, everybody.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 26.900000000000002, "text": " Good morning everybody. Good morning. Good morning. It's nice to be back in my house.", "tokens": [50364, 2205, 2446, 2201, 13, 2205, 2446, 13, 2205, 2446, 13, 467, 311, 1481, 281, 312, 646, 294, 452, 1782, 13, 51709], "temperature": 0.0, "avg_logprob": -0.38543089230855304, "compression_ratio": 1.2142857142857142, "no_speech_prob": 0.07614138722419739}, {"id": 1, "seek": 2690, "start": 26.9, "end": 40.9, "text": " I don't like streaming at work.", "tokens": [50364, 286, 500, 380, 411, 11791, 412, 589, 13, 51064], "temperature": 0.0, "avg_logprob": -0.46643156475490993, "compression_ratio": 0.8596491228070176, "no_speech_prob": 0.030419394373893738}, {"id": 2, "seek": 2690, "start": 40.9, "end": 48.9, "text": " High ball energy.", "tokens": [51064, 5229, 2594, 2281, 13, 51464], "temperature": 0.0, "avg_logprob": -0.46643156475490993, "compression_ratio": 0.8596491228070176, "no_speech_prob": 0.030419394373893738}, {"id": 3, "seek": 4890, "start": 48.9, "end": 67.9, "text": " Good morning everybody. Good morning. Good morning. Good morning. Good morning.", "tokens": [50364, 2205, 2446, 2201, 13, 2205, 2446, 13, 2205, 2446, 13, 2205, 2446, 13, 2205, 2446, 13, 51314], "temperature": 1.0, "avg_logprob": -0.5582403182983399, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.3706932067871094}, {"id": 4, "seek": 4890, "start": 67.9, "end": 72.9, "text": " Let's shrink me down here... me up there. Oops, don't do that.", "tokens": [51314, 961, 311, 23060, 385, 760, 510, 485, 385, 493, 456, 13, 21726, 11, 500, 380, 360, 300, 13, 51564], "temperature": 1.0, "avg_logprob": -0.5582403182983399, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.3706932067871094}, {"id": 5, "seek": 7290, "start": 72.9, "end": 76.9, "text": " Okay, are we ready to get started?", "tokens": [50364, 1033, 11, 366, 321, 1919, 281, 483, 1409, 30, 50564], "temperature": 0.0, "avg_logprob": -0.27832241251011086, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17288966476917267}, {"id": 6, "seek": 7290, "start": 80.46000000000001, "end": 83.58000000000001, "text": " Yeah, I felt the vibes were a little bit off", "tokens": [50742, 865, 11, 286, 2762, 264, 27636, 645, 257, 707, 857, 766, 50898], "temperature": 0.0, "avg_logprob": -0.27832241251011086, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17288966476917267}, {"id": 7, "seek": 7290, "start": 83.58000000000001, "end": 84.98, "text": " in yesterday's stream.", "tokens": [50898, 294, 5186, 311, 4309, 13, 50968], "temperature": 0.0, "avg_logprob": -0.27832241251011086, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17288966476917267}, {"id": 8, "seek": 7290, "start": 87.18, "end": 89.9, "text": " I don't think I should stream at the office.", "tokens": [51078, 286, 500, 380, 519, 286, 820, 4309, 412, 264, 3398, 13, 51214], "temperature": 0.0, "avg_logprob": -0.27832241251011086, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17288966476917267}, {"id": 9, "seek": 7290, "start": 89.9, "end": 91.98, "text": " Oh, so I didn't like the way that like,", "tokens": [51214, 876, 11, 370, 286, 994, 380, 411, 264, 636, 300, 411, 11, 51318], "temperature": 0.0, "avg_logprob": -0.27832241251011086, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17288966476917267}, {"id": 10, "seek": 7290, "start": 91.98, "end": 93.82000000000001, "text": " like now this is here,", "tokens": [51318, 411, 586, 341, 307, 510, 11, 51410], "temperature": 0.0, "avg_logprob": -0.27832241251011086, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17288966476917267}, {"id": 11, "seek": 7290, "start": 93.82000000000001, "end": 96.64000000000001, "text": " I'm gonna slide that down a little,", "tokens": [51410, 286, 478, 799, 4137, 300, 760, 257, 707, 11, 51551], "temperature": 0.0, "avg_logprob": -0.27832241251011086, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17288966476917267}, {"id": 12, "seek": 7290, "start": 97.5, "end": 100.10000000000001, "text": " about in the middle, that's pretty nice.", "tokens": [51594, 466, 294, 264, 2808, 11, 300, 311, 1238, 1481, 13, 51724], "temperature": 0.0, "avg_logprob": -0.27832241251011086, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17288966476917267}, {"id": 13, "seek": 7290, "start": 100.10000000000001, "end": 102.04, "text": " I got my Yeti microphone.", "tokens": [51724, 286, 658, 452, 10890, 72, 10952, 13, 51821], "temperature": 0.0, "avg_logprob": -0.27832241251011086, "compression_ratio": 1.541871921182266, "no_speech_prob": 0.17288966476917267}, {"id": 14, "seek": 10290, "start": 103.02000000000001, "end": 105.10000000000001, "text": " Move it over a little bit.", "tokens": [50370, 10475, 309, 670, 257, 707, 857, 13, 50474], "temperature": 0.0, "avg_logprob": -0.28607155649285565, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.0009694924228824675}, {"id": 15, "seek": 10290, "start": 105.10000000000001, "end": 107.62, "text": " Yeti puts me perfectly in the center.", "tokens": [50474, 10890, 72, 8137, 385, 6239, 294, 264, 3056, 13, 50600], "temperature": 0.0, "avg_logprob": -0.28607155649285565, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.0009694924228824675}, {"id": 16, "seek": 10290, "start": 109.58000000000001, "end": 111.78, "text": " Yeah, vibes are off.", "tokens": [50698, 865, 11, 27636, 366, 766, 13, 50808], "temperature": 0.0, "avg_logprob": -0.28607155649285565, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.0009694924228824675}, {"id": 17, "seek": 10290, "start": 111.78, "end": 113.82000000000001, "text": " Wouldn't have my lighting set off.", "tokens": [50808, 26291, 380, 362, 452, 9577, 992, 766, 13, 50910], "temperature": 0.0, "avg_logprob": -0.28607155649285565, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.0009694924228824675}, {"id": 18, "seek": 10290, "start": 115.06, "end": 116.46000000000001, "text": " I mean, it's an okay stream.", "tokens": [50972, 286, 914, 11, 309, 311, 364, 1392, 4309, 13, 51042], "temperature": 0.0, "avg_logprob": -0.28607155649285565, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.0009694924228824675}, {"id": 19, "seek": 10290, "start": 119.02000000000001, "end": 121.54, "text": " It's good to focus on TinyGrad.", "tokens": [51170, 467, 311, 665, 281, 1879, 322, 39992, 38, 6206, 13, 51296], "temperature": 0.0, "avg_logprob": -0.28607155649285565, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.0009694924228824675}, {"id": 20, "seek": 10290, "start": 121.54, "end": 125.74000000000001, "text": " But some guy said that he liked my older work better.", "tokens": [51296, 583, 512, 2146, 848, 300, 415, 4501, 452, 4906, 589, 1101, 13, 51506], "temperature": 0.0, "avg_logprob": -0.28607155649285565, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.0009694924228824675}, {"id": 21, "seek": 10290, "start": 125.74000000000001, "end": 129.02, "text": " And if there's one thing I never want to be, it's drink.", "tokens": [51506, 400, 498, 456, 311, 472, 551, 286, 1128, 528, 281, 312, 11, 309, 311, 2822, 13, 51670], "temperature": 0.0, "avg_logprob": -0.28607155649285565, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.0009694924228824675}, {"id": 22, "seek": 12902, "start": 129.58, "end": 132.06, "text": " So, we're gonna try to,", "tokens": [50392, 407, 11, 321, 434, 799, 853, 281, 11, 50516], "temperature": 0.0, "avg_logprob": -0.2595218434053309, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.001926448312588036}, {"id": 23, "seek": 12902, "start": 132.06, "end": 134.70000000000002, "text": " we're gonna try to bring it back to the old stuff.", "tokens": [50516, 321, 434, 799, 853, 281, 1565, 309, 646, 281, 264, 1331, 1507, 13, 50648], "temperature": 0.0, "avg_logprob": -0.2595218434053309, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.001926448312588036}, {"id": 24, "seek": 12902, "start": 135.74, "end": 136.74, "text": " And we're gonna do,", "tokens": [50700, 400, 321, 434, 799, 360, 11, 50750], "temperature": 0.0, "avg_logprob": -0.2595218434053309, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.001926448312588036}, {"id": 25, "seek": 12902, "start": 138.82000000000002, "end": 141.78, "text": " we're gonna investigate the Q-star algorithm.", "tokens": [50854, 321, 434, 799, 15013, 264, 1249, 12, 9710, 9284, 13, 51002], "temperature": 0.0, "avg_logprob": -0.2595218434053309, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.001926448312588036}, {"id": 26, "seek": 12902, "start": 141.78, "end": 144.06, "text": " So first off, what is the Q-star algorithm?", "tokens": [51002, 407, 700, 766, 11, 437, 307, 264, 1249, 12, 9710, 9284, 30, 51116], "temperature": 0.0, "avg_logprob": -0.2595218434053309, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.001926448312588036}, {"id": 27, "seek": 12902, "start": 144.06, "end": 145.46, "text": " I don't know, let's find out.", "tokens": [51116, 286, 500, 380, 458, 11, 718, 311, 915, 484, 13, 51186], "temperature": 0.0, "avg_logprob": -0.2595218434053309, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.001926448312588036}, {"id": 28, "seek": 12902, "start": 154.74, "end": 157.02, "text": " We'll start using our friend here, Google.", "tokens": [51650, 492, 603, 722, 1228, 527, 1277, 510, 11, 3329, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2595218434053309, "compression_ratio": 1.6369426751592357, "no_speech_prob": 0.001926448312588036}, {"id": 29, "seek": 15702, "start": 158.02, "end": 161.74, "text": " What is Q-star and when will we hear more?", "tokens": [50414, 708, 307, 1249, 12, 9710, 293, 562, 486, 321, 1568, 544, 30, 50600], "temperature": 0.0, "avg_logprob": -0.22735766360634252, "compression_ratio": 1.4723618090452262, "no_speech_prob": 0.0006263101240620017}, {"id": 30, "seek": 15702, "start": 161.74, "end": 164.14000000000001, "text": " Someone who's done a fair amount of ML research.", "tokens": [50600, 8734, 567, 311, 1096, 257, 3143, 2372, 295, 21601, 2132, 13, 50720], "temperature": 0.0, "avg_logprob": -0.22735766360634252, "compression_ratio": 1.4723618090452262, "no_speech_prob": 0.0006263101240620017}, {"id": 31, "seek": 15702, "start": 164.14000000000001, "end": 166.02, "text": " I can tell you, it's very, very easy to think", "tokens": [50720, 286, 393, 980, 291, 11, 309, 311, 588, 11, 588, 1858, 281, 519, 50814], "temperature": 0.0, "avg_logprob": -0.22735766360634252, "compression_ratio": 1.4723618090452262, "no_speech_prob": 0.0006263101240620017}, {"id": 32, "seek": 15702, "start": 166.02, "end": 167.06, "text": " you've discovered a breakthrough.", "tokens": [50814, 291, 600, 6941, 257, 22397, 13, 50866], "temperature": 0.0, "avg_logprob": -0.22735766360634252, "compression_ratio": 1.4723618090452262, "no_speech_prob": 0.0006263101240620017}, {"id": 33, "seek": 15702, "start": 167.06, "end": 167.9, "text": " That's true.", "tokens": [50866, 663, 311, 2074, 13, 50908], "temperature": 0.0, "avg_logprob": -0.22735766360634252, "compression_ratio": 1.4723618090452262, "no_speech_prob": 0.0006263101240620017}, {"id": 34, "seek": 15702, "start": 169.42000000000002, "end": 172.58, "text": " All right, let's read this article in The Verge.", "tokens": [50984, 1057, 558, 11, 718, 311, 1401, 341, 7222, 294, 440, 4281, 432, 13, 51142], "temperature": 0.0, "avg_logprob": -0.22735766360634252, "compression_ratio": 1.4723618090452262, "no_speech_prob": 0.0006263101240620017}, {"id": 35, "seek": 15702, "start": 176.54000000000002, "end": 180.42000000000002, "text": " Reuters, okay, well the government, no, no, no, no, no, no.", "tokens": [51340, 1300, 48396, 11, 1392, 11, 731, 264, 2463, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 13, 51534], "temperature": 0.0, "avg_logprob": -0.22735766360634252, "compression_ratio": 1.4723618090452262, "no_speech_prob": 0.0006263101240620017}, {"id": 36, "seek": 18702, "start": 187.02, "end": 187.86, "text": " Okay.", "tokens": [50364, 1033, 13, 50406], "temperature": 0.2, "avg_logprob": -0.3456167115105523, "compression_ratio": 1.1926605504587156, "no_speech_prob": 0.0015008236514404416}, {"id": 37, "seek": 18702, "start": 200.66000000000003, "end": 204.26000000000002, "text": " Q-star could be a breakthrough in the startup search", "tokens": [51046, 1249, 12, 9710, 727, 312, 257, 22397, 294, 264, 18578, 3164, 51226], "temperature": 0.2, "avg_logprob": -0.3456167115105523, "compression_ratio": 1.1926605504587156, "no_speech_prob": 0.0015008236514404416}, {"id": 38, "seek": 18702, "start": 204.26000000000002, "end": 207.5, "text": " for what's known as artificial general intelligence.", "tokens": [51226, 337, 437, 311, 2570, 382, 11677, 2674, 7599, 13, 51388], "temperature": 0.2, "avg_logprob": -0.3456167115105523, "compression_ratio": 1.1926605504587156, "no_speech_prob": 0.0015008236514404416}, {"id": 39, "seek": 18702, "start": 210.58, "end": 212.10000000000002, "text": " Reuters could not,", "tokens": [51542, 1300, 48396, 727, 406, 11, 51618], "temperature": 0.2, "avg_logprob": -0.3456167115105523, "compression_ratio": 1.1926605504587156, "no_speech_prob": 0.0015008236514404416}, {"id": 40, "seek": 21702, "start": 217.02, "end": 220.46, "text": " could not be a breakthrough in the startup search for what's", "tokens": [50364, 727, 406, 312, 257, 22397, 294, 264, 18578, 3164, 337, 437, 311, 50536], "temperature": 0.0, "avg_logprob": -0.8453974723815918, "compression_ratio": 1.2149532710280373, "no_speech_prob": 0.0024721792433410883}, {"id": 41, "seek": 21702, "start": 220.46, "end": 222.78, "text": " known as artificial general intelligence.", "tokens": [50536, 2570, 382, 11677, 2674, 7599, 13, 50652], "temperature": 0.0, "avg_logprob": -0.8453974723815918, "compression_ratio": 1.2149532710280373, "no_speech_prob": 0.0024721792433410883}, {"id": 42, "seek": 21702, "start": 230.78, "end": 232.94, "text": " Okay, it somehow does math.", "tokens": [51052, 1033, 11, 309, 6063, 775, 5221, 13, 51160], "temperature": 0.0, "avg_logprob": -0.8453974723815918, "compression_ratio": 1.2149532710280373, "no_speech_prob": 0.0024721792433410883}, {"id": 43, "seek": 24702, "start": 247.34, "end": 249.34, "text": " Does something have to publish papers?", "tokens": [50380, 4402, 746, 362, 281, 11374, 10577, 30, 50480], "temperature": 0.0, "avg_logprob": -0.38577705774551785, "compression_ratio": 1.2952380952380953, "no_speech_prob": 0.0017543986905366182}, {"id": 44, "seek": 24702, "start": 257.54, "end": 258.7, "text": " Not really.", "tokens": [50890, 1726, 534, 13, 50948], "temperature": 0.0, "avg_logprob": -0.38577705774551785, "compression_ratio": 1.2952380952380953, "no_speech_prob": 0.0017543986905366182}, {"id": 45, "seek": 24702, "start": 267.3, "end": 268.66, "text": " Wow.", "tokens": [51378, 3153, 13, 51446], "temperature": 0.0, "avg_logprob": -0.38577705774551785, "compression_ratio": 1.2952380952380953, "no_speech_prob": 0.0017543986905366182}, {"id": 46, "seek": 24702, "start": 268.66, "end": 271.38, "text": " Remember when they used to publish papers", "tokens": [51446, 5459, 562, 436, 1143, 281, 11374, 10577, 51582], "temperature": 0.0, "avg_logprob": -0.38577705774551785, "compression_ratio": 1.2952380952380953, "no_speech_prob": 0.0017543986905366182}, {"id": 47, "seek": 24702, "start": 271.38, "end": 274.90000000000003, "text": " and now they publish system cards and,", "tokens": [51582, 293, 586, 436, 11374, 1185, 5632, 293, 11, 51758], "temperature": 0.0, "avg_logprob": -0.38577705774551785, "compression_ratio": 1.2952380952380953, "no_speech_prob": 0.0017543986905366182}, {"id": 48, "seek": 27490, "start": 275.38, "end": 277.06, "text": " safety and alignment?", "tokens": [50388, 4514, 293, 18515, 30, 50472], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 49, "seek": 27490, "start": 280.21999999999997, "end": 281.78, "text": " Okay, wait, wait, wait, wait, wait, wait, wait,", "tokens": [50630, 1033, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 50708], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 50, "seek": 27490, "start": 281.78, "end": 281.82, "text": " wait, wait, wait, wait, wait, wait, wait, wait, wait, wait,", "tokens": [50708, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 50710], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 51, "seek": 27490, "start": 281.82, "end": 282.85999999999996, "text": " wait, there's some hope here.", "tokens": [50710, 1699, 11, 456, 311, 512, 1454, 510, 13, 50762], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 52, "seek": 27490, "start": 282.85999999999996, "end": 285.12, "text": " Improving mathematical reasoning.", "tokens": [50762, 8270, 340, 798, 18894, 21577, 13, 50875], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 53, "seek": 27490, "start": 286.53999999999996, "end": 289.97999999999996, "text": " Ooh, ooh, ooh, this sounds like Q-star guys.", "tokens": [50946, 7951, 11, 17024, 11, 17024, 11, 341, 3263, 411, 1249, 12, 9710, 1074, 13, 51118], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 54, "seek": 27490, "start": 289.97999999999996, "end": 291.17999999999995, "text": " I think we found it.", "tokens": [51118, 286, 519, 321, 1352, 309, 13, 51178], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 55, "seek": 27490, "start": 294.84, "end": 296.7, "text": " No, we don't, no, no, but really, wait, is this not,", "tokens": [51361, 883, 11, 321, 500, 380, 11, 572, 11, 572, 11, 457, 534, 11, 1699, 11, 307, 341, 406, 11, 51454], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 56, "seek": 27490, "start": 296.7, "end": 298.08, "text": " is this not Q-star?", "tokens": [51454, 307, 341, 406, 1249, 12, 9710, 30, 51523], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 57, "seek": 27490, "start": 298.08, "end": 299.21999999999997, "text": " Did we not just find it?", "tokens": [51523, 2589, 321, 406, 445, 915, 309, 30, 51580], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 58, "seek": 27490, "start": 302.7, "end": 303.65999999999997, "text": " We're trying not to hate.", "tokens": [51754, 492, 434, 1382, 406, 281, 4700, 13, 51802], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 59, "seek": 27490, "start": 303.65999999999997, "end": 304.82, "text": " We're trying not to hate guys.", "tokens": [51802, 492, 434, 1382, 406, 281, 4700, 1074, 13, 51860], "temperature": 0.8, "avg_logprob": -0.3659601984797297, "compression_ratio": 2.134020618556701, "no_speech_prob": 0.007814151234924793}, {"id": 60, "seek": 30482, "start": 304.82, "end": 310.98, "text": " Solitate in the world. We got to bring law of impositivity and shit man. Whatever like John Lennon said, you know something", "tokens": [50364, 7026, 8086, 294, 264, 1002, 13, 492, 658, 281, 1565, 2101, 295, 704, 9598, 4253, 293, 4611, 587, 13, 8541, 411, 2619, 441, 1857, 266, 848, 11, 291, 458, 746, 50672], "temperature": 0.0, "avg_logprob": -0.311958779679968, "compression_ratio": 1.2666666666666666, "no_speech_prob": 0.13251584768295288}, {"id": 61, "seek": 30482, "start": 315.98, "end": 319.92, "text": " That's why we're gonna go subscribers only", "tokens": [50922, 663, 311, 983, 321, 434, 799, 352, 11092, 787, 51119], "temperature": 0.0, "avg_logprob": -0.311958779679968, "compression_ratio": 1.2666666666666666, "no_speech_prob": 0.13251584768295288}, {"id": 62, "seek": 30482, "start": 324.98, "end": 326.98, "text": " Okay", "tokens": [51372, 1033, 51472], "temperature": 0.0, "avg_logprob": -0.311958779679968, "compression_ratio": 1.2666666666666666, "no_speech_prob": 0.13251584768295288}, {"id": 63, "seek": 33482, "start": 334.92, "end": 345.52, "text": " Given vast computing resources the new model was able to solve certain mathematical problems", "tokens": [50369, 18600, 8369, 15866, 3593, 264, 777, 2316, 390, 1075, 281, 5039, 1629, 18894, 2740, 50899], "temperature": 0.0, "avg_logprob": -0.27112022042274475, "compression_ratio": 1.703125, "no_speech_prob": 0.0022502520587295294}, {"id": 64, "seek": 33482, "start": 347.14, "end": 350.7, "text": " Improving mathematical reasoning with process supervision", "tokens": [50980, 8270, 340, 798, 18894, 21577, 365, 1399, 32675, 51158], "temperature": 0.0, "avg_logprob": -0.27112022042274475, "compression_ratio": 1.703125, "no_speech_prob": 0.0022502520587295294}, {"id": 65, "seek": 33482, "start": 351.46, "end": 359.14, "text": " We've trained a model to achieve a new state of the art in mathematical problem-solving by rewarding each correct step of reasoning instead of", "tokens": [51196, 492, 600, 8895, 257, 2316, 281, 4584, 257, 777, 1785, 295, 264, 1523, 294, 18894, 1154, 12, 30926, 798, 538, 20063, 1184, 3006, 1823, 295, 21577, 2602, 295, 51580], "temperature": 0.0, "avg_logprob": -0.27112022042274475, "compression_ratio": 1.703125, "no_speech_prob": 0.0022502520587295294}, {"id": 66, "seek": 33482, "start": 359.78, "end": 361.78, "text": " Simply rewarding the final answer", "tokens": [51612, 19596, 20063, 264, 2572, 1867, 51712], "temperature": 0.0, "avg_logprob": -0.27112022042274475, "compression_ratio": 1.703125, "no_speech_prob": 0.0022502520587295294}, {"id": 67, "seek": 36482, "start": 365.65999999999997, "end": 367.65999999999997, "text": " And just", "tokens": [50406, 400, 445, 50506], "temperature": 0.0, "avg_logprob": -0.29370233905849175, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.0022864851634949446}, {"id": 68, "seek": 36482, "start": 371.18, "end": 373.18, "text": " Yeah, yeah", "tokens": [50682, 865, 11, 1338, 50782], "temperature": 0.0, "avg_logprob": -0.29370233905849175, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.0022864851634949446}, {"id": 69, "seek": 36482, "start": 376.98, "end": 378.98, "text": " Okay, so", "tokens": [50972, 1033, 11, 370, 51072], "temperature": 0.0, "avg_logprob": -0.29370233905849175, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.0022864851634949446}, {"id": 70, "seek": 36482, "start": 379.14, "end": 385.03999999999996, "text": " The reason that you usually just reward the correct final answer is because that's all that's in your", "tokens": [51080, 440, 1778, 300, 291, 2673, 445, 7782, 264, 3006, 2572, 1867, 307, 570, 300, 311, 439, 300, 311, 294, 428, 51375], "temperature": 0.0, "avg_logprob": -0.29370233905849175, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.0022864851634949446}, {"id": 71, "seek": 36482, "start": 386.34, "end": 393.78, "text": " Data set the other problem with doing this is it may constrain the reasoning to follow one certain path when that may not be the path", "tokens": [51440, 11888, 992, 264, 661, 1154, 365, 884, 341, 307, 309, 815, 1817, 7146, 264, 21577, 281, 1524, 472, 1629, 3100, 562, 300, 815, 406, 312, 264, 3100, 51812], "temperature": 0.0, "avg_logprob": -0.29370233905849175, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.0022864851634949446}, {"id": 72, "seek": 39378, "start": 393.78, "end": 395.78, "text": " You have to follow", "tokens": [50364, 509, 362, 281, 1524, 50464], "temperature": 0.0, "avg_logprob": -0.4725164066661488, "compression_ratio": 1.2946428571428572, "no_speech_prob": 0.001622545998543501}, {"id": 73, "seek": 39378, "start": 399.26, "end": 403.53999999999996, "text": " State of the art models still produce logical mistakes often called hallucinations", "tokens": [50638, 4533, 295, 264, 1523, 5245, 920, 5258, 14978, 8038, 2049, 1219, 35212, 10325, 50852], "temperature": 0.0, "avg_logprob": -0.4725164066661488, "compression_ratio": 1.2946428571428572, "no_speech_prob": 0.001622545998543501}, {"id": 74, "seek": 39378, "start": 414.61999999999995, "end": 417.71999999999997, "text": " Cutting out we have more technical problems", "tokens": [51406, 9431, 783, 484, 321, 362, 544, 6191, 2740, 51561], "temperature": 0.0, "avg_logprob": -0.4725164066661488, "compression_ratio": 1.2946428571428572, "no_speech_prob": 0.001622545998543501}, {"id": 75, "seek": 41772, "start": 418.44000000000005, "end": 424.48, "text": " I bought a new computer. Does new Apple stuff not work only you okay Rick and Morty you have a crappy internet", "tokens": [50400, 286, 4243, 257, 777, 3820, 13, 4402, 777, 6373, 1507, 406, 589, 787, 291, 1392, 11224, 293, 24977, 88, 291, 362, 257, 36531, 4705, 50702], "temperature": 0.0, "avg_logprob": -0.36182003021240233, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.010981791652739048}, {"id": 76, "seek": 41772, "start": 432.64000000000004, "end": 437.82000000000005, "text": " An internet connection that is less good than other internet connections", "tokens": [51110, 1107, 4705, 4984, 300, 307, 1570, 665, 813, 661, 4705, 9271, 51369], "temperature": 0.0, "avg_logprob": -0.36182003021240233, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.010981791652739048}, {"id": 77, "seek": 44772, "start": 447.96000000000004, "end": 449.96000000000004, "text": " You", "tokens": [50376, 509, 50476], "temperature": 0.0, "avg_logprob": -0.4469990894712251, "compression_ratio": 1.1333333333333333, "no_speech_prob": 0.003429192118346691}, {"id": 78, "seek": 44772, "start": 455.36, "end": 459.68, "text": " Wait, just opening I just published like fake papers now like this", "tokens": [50746, 3802, 11, 445, 5193, 286, 445, 6572, 411, 7592, 10577, 586, 411, 341, 50962], "temperature": 0.0, "avg_logprob": -0.4469990894712251, "compression_ratio": 1.1333333333333333, "no_speech_prob": 0.003429192118346691}, {"id": 79, "seek": 44772, "start": 470.12, "end": 472.12, "text": " Don't do a shit about alignment", "tokens": [51484, 1468, 380, 360, 257, 4611, 466, 18515, 51584], "temperature": 0.0, "avg_logprob": -0.4469990894712251, "compression_ratio": 1.1333333333333333, "no_speech_prob": 0.003429192118346691}, {"id": 80, "seek": 47212, "start": 472.32, "end": 474.32, "text": " and", "tokens": [50374, 293, 50474], "temperature": 0.0, "avg_logprob": -0.3541454928261893, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.006486169993877411}, {"id": 81, "seek": 47212, "start": 477.12, "end": 484.68, "text": " Wait, this is a paper. Oh, you know here does a paper. Let's verify step by step", "tokens": [50614, 3802, 11, 341, 307, 257, 3035, 13, 876, 11, 291, 458, 510, 775, 257, 3035, 13, 961, 311, 16888, 1823, 538, 1823, 50992], "temperature": 0.0, "avg_logprob": -0.3541454928261893, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.006486169993877411}, {"id": 82, "seek": 50212, "start": 502.12, "end": 504.12, "text": " You", "tokens": [50364, 509, 50464], "temperature": 0.0, "avg_logprob": -0.40933319500514437, "compression_ratio": 1.196969696969697, "no_speech_prob": 0.006191086955368519}, {"id": 83, "seek": 50212, "start": 520.84, "end": 522.84, "text": " Okay, well, is this a public data set", "tokens": [51300, 1033, 11, 731, 11, 307, 341, 257, 1908, 1412, 992, 51400], "temperature": 0.0, "avg_logprob": -0.40933319500514437, "compression_ratio": 1.196969696969697, "no_speech_prob": 0.006191086955368519}, {"id": 84, "seek": 50212, "start": 527.36, "end": 529.36, "text": " All right, all right, all right, cool", "tokens": [51626, 1057, 558, 11, 439, 558, 11, 439, 558, 11, 1627, 51726], "temperature": 0.0, "avg_logprob": -0.40933319500514437, "compression_ratio": 1.196969696969697, "no_speech_prob": 0.006191086955368519}, {"id": 85, "seek": 53212, "start": 532.12, "end": 534.12, "text": " You", "tokens": [50364, 509, 50464], "temperature": 0.0, "avg_logprob": -0.42099846326387846, "compression_ratio": 0.9594594594594594, "no_speech_prob": 0.0006165463710203767}, {"id": 86, "seek": 53212, "start": 544.72, "end": 547.48, "text": " Math word problem-solving on math", "tokens": [50994, 15776, 1349, 1154, 12, 30926, 798, 322, 5221, 51132], "temperature": 0.0, "avg_logprob": -0.42099846326387846, "compression_ratio": 0.9594594594594594, "no_speech_prob": 0.0006165463710203767}, {"id": 87, "seek": 53212, "start": 556.32, "end": 558.32, "text": " Who made this data set", "tokens": [51574, 2102, 1027, 341, 1412, 992, 51674], "temperature": 0.0, "avg_logprob": -0.42099846326387846, "compression_ratio": 0.9594594594594594, "no_speech_prob": 0.0006165463710203767}, {"id": 88, "seek": 53212, "start": 560.04, "end": 562.04, "text": " Okay, okay", "tokens": [51760, 1033, 11, 1392, 51860], "temperature": 0.0, "avg_logprob": -0.42099846326387846, "compression_ratio": 0.9594594594594594, "no_speech_prob": 0.0006165463710203767}, {"id": 89, "seek": 56212, "start": 562.8, "end": 569.72, "text": " This is not where I was expecting this to go by the way. Oh, they have metamath. Oh everything comes full circle", "tokens": [50398, 639, 307, 406, 689, 286, 390, 9650, 341, 281, 352, 538, 264, 636, 13, 876, 11, 436, 362, 1131, 335, 998, 13, 876, 1203, 1487, 1577, 6329, 50744], "temperature": 0.0, "avg_logprob": -0.252283384215157, "compression_ratio": 1.3049645390070923, "no_speech_prob": 0.00021994517010170966}, {"id": 90, "seek": 56212, "start": 573.88, "end": 581.5600000000001, "text": " No, I mean again, let's okay, let's find the primary source here", "tokens": [50952, 883, 11, 286, 914, 797, 11, 718, 311, 1392, 11, 718, 311, 915, 264, 6194, 4009, 510, 51336], "temperature": 0.0, "avg_logprob": -0.252283384215157, "compression_ratio": 1.3049645390070923, "no_speech_prob": 0.00021994517010170966}, {"id": 91, "seek": 56212, "start": 587.2, "end": 589.2, "text": " Q star", "tokens": [51618, 1249, 3543, 51718], "temperature": 0.0, "avg_logprob": -0.252283384215157, "compression_ratio": 1.3049645390070923, "no_speech_prob": 0.00021994517010170966}, {"id": 92, "seek": 59212, "start": 593.12, "end": 595.12, "text": " I", "tokens": [50414, 286, 50514], "temperature": 0.0, "avg_logprob": -0.5018866123297275, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.0005192723474465311}, {"id": 93, "seek": 59212, "start": 595.24, "end": 597.24, "text": " Definitely talked about math problems", "tokens": [50520, 12151, 2825, 466, 5221, 2740, 50620], "temperature": 0.0, "avg_logprob": -0.5018866123297275, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.0005192723474465311}, {"id": 94, "seek": 59212, "start": 600.32, "end": 604.08, "text": " Only performing math on the level of grade school students", "tokens": [50774, 5686, 10205, 5221, 322, 264, 1496, 295, 7204, 1395, 1731, 50962], "temperature": 0.0, "avg_logprob": -0.5018866123297275, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.0005192723474465311}, {"id": 95, "seek": 59212, "start": 610.68, "end": 614.24, "text": " Why is this monitor broken this monitor is like broken I can do my", "tokens": [51292, 1545, 307, 341, 6002, 5463, 341, 6002, 307, 411, 5463, 286, 393, 360, 452, 51470], "temperature": 0.0, "avg_logprob": -0.5018866123297275, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.0005192723474465311}, {"id": 96, "seek": 62212, "start": 622.36, "end": 626.52, "text": " You guys aren't seeing that glitching right", "tokens": [50376, 509, 1074, 3212, 380, 2577, 300, 23552, 278, 558, 50584], "temperature": 0.0, "avg_logprob": -0.3049010932445526, "compression_ratio": 1.1685393258426966, "no_speech_prob": 0.001064916723407805}, {"id": 97, "seek": 62212, "start": 635.96, "end": 639.96, "text": " Are you maybe you are I don't think you are", "tokens": [51056, 2014, 291, 1310, 291, 366, 286, 500, 380, 519, 291, 366, 51256], "temperature": 0.0, "avg_logprob": -0.3049010932445526, "compression_ratio": 1.1685393258426966, "no_speech_prob": 0.001064916723407805}, {"id": 98, "seek": 62212, "start": 645.24, "end": 647.24, "text": " This is so weird", "tokens": [51520, 639, 307, 370, 3657, 51620], "temperature": 0.0, "avg_logprob": -0.3049010932445526, "compression_ratio": 1.1685393258426966, "no_speech_prob": 0.001064916723407805}, {"id": 99, "seek": 64724, "start": 648.2, "end": 652.92, "text": " When I click here, there's like a blue glitching on my computer", "tokens": [50412, 1133, 286, 2052, 510, 11, 456, 311, 411, 257, 3344, 23552, 278, 322, 452, 3820, 50648], "temperature": 0.0, "avg_logprob": -0.2956187016255147, "compression_ratio": 1.7005988023952097, "no_speech_prob": 0.0010986807756125927}, {"id": 100, "seek": 64724, "start": 656.88, "end": 659.64, "text": " Weirdest thing like how does that even happen I", "tokens": [50846, 32033, 377, 551, 411, 577, 775, 300, 754, 1051, 286, 50984], "temperature": 0.0, "avg_logprob": -0.2956187016255147, "compression_ratio": 1.7005988023952097, "no_speech_prob": 0.0010986807756125927}, {"id": 101, "seek": 64724, "start": 662.32, "end": 664.6800000000001, "text": " Could take the camera down and show you you want to see", "tokens": [51118, 7497, 747, 264, 2799, 760, 293, 855, 291, 291, 528, 281, 536, 51236], "temperature": 0.0, "avg_logprob": -0.2956187016255147, "compression_ratio": 1.7005988023952097, "no_speech_prob": 0.0010986807756125927}, {"id": 102, "seek": 64724, "start": 665.72, "end": 670.08, "text": " There's like blue glitching on my monitor. Look at this click here goes away", "tokens": [51288, 821, 311, 411, 3344, 23552, 278, 322, 452, 6002, 13, 2053, 412, 341, 2052, 510, 1709, 1314, 51506], "temperature": 0.0, "avg_logprob": -0.2956187016255147, "compression_ratio": 1.7005988023952097, "no_speech_prob": 0.0010986807756125927}, {"id": 103, "seek": 64724, "start": 670.8, "end": 673.8, "text": " Here glitching here goes away glitching", "tokens": [51542, 1692, 23552, 278, 510, 1709, 1314, 23552, 278, 51692], "temperature": 0.0, "avg_logprob": -0.2956187016255147, "compression_ratio": 1.7005988023952097, "no_speech_prob": 0.0010986807756125927}, {"id": 104, "seek": 67380, "start": 674.8, "end": 677.9599999999999, "text": " How's this happening they've never seen that before", "tokens": [50414, 1012, 311, 341, 2737, 436, 600, 1128, 1612, 300, 949, 50572], "temperature": 0.0, "avg_logprob": -0.4707729166204279, "compression_ratio": 1.1122448979591837, "no_speech_prob": 0.0017816503532230854}, {"id": 105, "seek": 67380, "start": 683.0799999999999, "end": 685.0799999999999, "text": " Sit", "tokens": [50828, 14523, 50928], "temperature": 0.0, "avg_logprob": -0.4707729166204279, "compression_ratio": 1.1122448979591837, "no_speech_prob": 0.0017816503532230854}, {"id": 106, "seek": 67380, "start": 686.88, "end": 688.88, "text": " Ghosted image", "tokens": [51018, 16323, 292, 3256, 51118], "temperature": 0.0, "avg_logprob": -0.4707729166204279, "compression_ratio": 1.1122448979591837, "no_speech_prob": 0.0017816503532230854}, {"id": 107, "seek": 67380, "start": 691.12, "end": 694.68, "text": " Maybe it's the cable I don't know right", "tokens": [51230, 2704, 309, 311, 264, 8220, 286, 500, 380, 458, 558, 51408], "temperature": 0.0, "avg_logprob": -0.4707729166204279, "compression_ratio": 1.1122448979591837, "no_speech_prob": 0.0017816503532230854}, {"id": 108, "seek": 70380, "start": 703.8, "end": 706.88, "text": " Who knows it only happens when things are uncertain", "tokens": [50364, 2102, 3255, 309, 787, 2314, 562, 721, 366, 11308, 50518], "temperature": 0.0, "avg_logprob": -0.37738254547119143, "compression_ratio": 1.1153846153846154, "no_speech_prob": 0.002714811358600855}, {"id": 109, "seek": 70380, "start": 714.28, "end": 716.28, "text": " Wait, this is just really", "tokens": [50888, 3802, 11, 341, 307, 445, 534, 50988], "temperature": 0.0, "avg_logprob": -0.37738254547119143, "compression_ratio": 1.1153846153846154, "no_speech_prob": 0.002714811358600855}, {"id": 110, "seek": 70380, "start": 725.4799999999999, "end": 727.4799999999999, "text": " Who knows", "tokens": [51448, 2102, 3255, 51548], "temperature": 0.0, "avg_logprob": -0.37738254547119143, "compression_ratio": 1.1153846153846154, "no_speech_prob": 0.002714811358600855}, {"id": 111, "seek": 72748, "start": 728.48, "end": 730.48, "text": " Oh", "tokens": [50414, 876, 50514], "temperature": 0.0, "avg_logprob": -0.34119711166773087, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.0006263046525418758}, {"id": 112, "seek": 72748, "start": 733.36, "end": 738.48, "text": " Wait, I wanted to confirm that this actually was related to math", "tokens": [50658, 3802, 11, 286, 1415, 281, 9064, 300, 341, 767, 390, 4077, 281, 5221, 50914], "temperature": 0.0, "avg_logprob": -0.34119711166773087, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.0006263046525418758}, {"id": 113, "seek": 72748, "start": 746.24, "end": 753.72, "text": " Delstick cable breaks I'm using like the USB-C cable it could be the cable", "tokens": [51302, 5831, 11881, 8220, 9857, 286, 478, 1228, 411, 264, 10109, 12, 34, 8220, 309, 727, 312, 264, 8220, 51676], "temperature": 0.0, "avg_logprob": -0.34119711166773087, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.0006263046525418758}, {"id": 114, "seek": 75748, "start": 757.48, "end": 759.72, "text": " But like it's weird how it's glitching", "tokens": [50364, 583, 411, 309, 311, 3657, 577, 309, 311, 23552, 278, 50476], "temperature": 0.0, "avg_logprob": -0.2830273368141868, "compression_ratio": 1.4081632653061225, "no_speech_prob": 0.0014778623590245843}, {"id": 115, "seek": 75748, "start": 764.72, "end": 766.72, "text": " I don't know", "tokens": [50726, 286, 500, 380, 458, 50826], "temperature": 0.0, "avg_logprob": -0.2830273368141868, "compression_ratio": 1.4081632653061225, "no_speech_prob": 0.0014778623590245843}, {"id": 116, "seek": 75748, "start": 769.32, "end": 774.6, "text": " The new model was able to solve certain mathematical problems, so it kind of makes sense that this is the data set right", "tokens": [50956, 440, 777, 2316, 390, 1075, 281, 5039, 1629, 18894, 2740, 11, 370, 309, 733, 295, 1669, 2020, 300, 341, 307, 264, 1412, 992, 558, 51220], "temperature": 0.0, "avg_logprob": -0.2830273368141868, "compression_ratio": 1.4081632653061225, "no_speech_prob": 0.0014778623590245843}, {"id": 117, "seek": 75748, "start": 780.48, "end": 782.48, "text": " Do they mention the math data here", "tokens": [51514, 1144, 436, 2152, 264, 5221, 1412, 510, 51614], "temperature": 0.0, "avg_logprob": -0.2830273368141868, "compression_ratio": 1.4081632653061225, "no_speech_prob": 0.0014778623590245843}, {"id": 118, "seek": 78248, "start": 783.16, "end": 790.32, "text": " Our process supervised model solves 78% of the problems from a representative subset of the math test set", "tokens": [50398, 2621, 1399, 46533, 2316, 39890, 26369, 4, 295, 264, 2740, 490, 257, 12424, 25993, 295, 264, 5221, 1500, 992, 50756], "temperature": 0.0, "avg_logprob": -0.3234339774923122, "compression_ratio": 1.3630136986301369, "no_speech_prob": 0.002433991292491555}, {"id": 119, "seek": 78248, "start": 795.64, "end": 798.9200000000001, "text": " We also release PRM 800 K", "tokens": [51022, 492, 611, 4374, 11568, 44, 13083, 591, 51186], "temperature": 0.0, "avg_logprob": -0.3234339774923122, "compression_ratio": 1.3630136986301369, "no_speech_prob": 0.002433991292491555}, {"id": 120, "seek": 78248, "start": 800.6, "end": 802.6, "text": " Okay, opening I release something look at that", "tokens": [51270, 1033, 11, 5193, 286, 4374, 746, 574, 412, 300, 51370], "temperature": 0.0, "avg_logprob": -0.3234339774923122, "compression_ratio": 1.3630136986301369, "no_speech_prob": 0.002433991292491555}, {"id": 121, "seek": 78248, "start": 804.6, "end": 806.6, "text": " No positive positive", "tokens": [51470, 883, 3353, 3353, 51570], "temperature": 0.0, "avg_logprob": -0.3234339774923122, "compression_ratio": 1.3630136986301369, "no_speech_prob": 0.002433991292491555}, {"id": 122, "seek": 81248, "start": 812.48, "end": 814.48, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.44427337646484377, "compression_ratio": 1.0, "no_speech_prob": 0.005301261320710182}, {"id": 123, "seek": 81248, "start": 821.64, "end": 823.64, "text": " Drive to VS codes", "tokens": [50822, 15622, 281, 25091, 14211, 50922], "temperature": 0.0, "avg_logprob": -0.44427337646484377, "compression_ratio": 1.0, "no_speech_prob": 0.005301261320710182}, {"id": 124, "seek": 81248, "start": 838.16, "end": 841.32, "text": " Let's also look into what their other github activities been", "tokens": [51648, 961, 311, 611, 574, 666, 437, 641, 661, 290, 355, 836, 5354, 668, 51806], "temperature": 0.0, "avg_logprob": -0.44427337646484377, "compression_ratio": 1.0, "no_speech_prob": 0.005301261320710182}, {"id": 125, "seek": 84248, "start": 842.48, "end": 844.48, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.5265952859606061, "compression_ratio": 0.864406779661017, "no_speech_prob": 0.007814237847924232}, {"id": 126, "seek": 84248, "start": 862.2, "end": 865.44, "text": " Procedurally generated game like Jim environments", "tokens": [51350, 1705, 1232, 6512, 10833, 1216, 411, 6637, 12388, 51512], "temperature": 0.0, "avg_logprob": -0.5265952859606061, "compression_ratio": 0.864406779661017, "no_speech_prob": 0.007814237847924232}, {"id": 127, "seek": 87248, "start": 873.36, "end": 883.5600000000001, "text": " What did they update in GPT to a branch they updated", "tokens": [50408, 708, 630, 436, 5623, 294, 26039, 51, 281, 257, 9819, 436, 10588, 50918], "temperature": 0.0, "avg_logprob": -0.48968228697776794, "compression_ratio": 1.04, "no_speech_prob": 0.0017820821376517415}, {"id": 128, "seek": 88356, "start": 883.56, "end": 885.56, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.41672061920166015, "compression_ratio": 1.0602409638554218, "no_speech_prob": 0.010012278333306313}, {"id": 129, "seek": 88356, "start": 900.1999999999999, "end": 903.3199999999999, "text": " Quantifying transfer and reinforcement learning", "tokens": [51196, 26968, 5489, 5003, 293, 29280, 2539, 51352], "temperature": 0.0, "avg_logprob": -0.41672061920166015, "compression_ratio": 1.0602409638554218, "no_speech_prob": 0.010012278333306313}, {"id": 130, "seek": 88356, "start": 907.52, "end": 910.76, "text": " Okay, it seems like Carl Cobb has been", "tokens": [51562, 1033, 11, 309, 2544, 411, 14256, 31395, 65, 575, 668, 51724], "temperature": 0.0, "avg_logprob": -0.41672061920166015, "compression_ratio": 1.0602409638554218, "no_speech_prob": 0.010012278333306313}, {"id": 131, "seek": 91076, "start": 911.76, "end": 914.72, "text": " If anyone invented Q star, it's Carl Cobb", "tokens": [50414, 759, 2878, 14479, 1249, 3543, 11, 309, 311, 14256, 31395, 65, 50562], "temperature": 0.0, "avg_logprob": -0.28484545255962174, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.0013044587103649974}, {"id": 132, "seek": 91076, "start": 918.8, "end": 920.8, "text": " Carl Cobb", "tokens": [50766, 14256, 31395, 65, 50866], "temperature": 0.0, "avg_logprob": -0.28484545255962174, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.0013044587103649974}, {"id": 133, "seek": 91076, "start": 927.92, "end": 933.68, "text": " All right, you've been working on this math stuff for a while. Let's see what data set was used here. Oh", "tokens": [51222, 1057, 558, 11, 291, 600, 668, 1364, 322, 341, 5221, 1507, 337, 257, 1339, 13, 961, 311, 536, 437, 1412, 992, 390, 1143, 510, 13, 876, 51510], "temperature": 0.0, "avg_logprob": -0.28484545255962174, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.0013044587103649974}, {"id": 134, "seek": 91076, "start": 937.88, "end": 940.0, "text": " Is this the data set that introduced that", "tokens": [51720, 1119, 341, 264, 1412, 992, 300, 7268, 300, 51826], "temperature": 0.0, "avg_logprob": -0.28484545255962174, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.0013044587103649974}, {"id": 135, "seek": 94076, "start": 941.6, "end": 943.6, "text": " Oh", "tokens": [50406, 876, 50506], "temperature": 0.0, "avg_logprob": -0.3271577719486121, "compression_ratio": 1.0444444444444445, "no_speech_prob": 0.00019410686218179762}, {"id": 136, "seek": 94076, "start": 943.6, "end": 948.92, "text": " This is a different one gms 8k PRM 800 K", "tokens": [50506, 639, 307, 257, 819, 472, 290, 2592, 1649, 74, 11568, 44, 13083, 591, 50772], "temperature": 0.0, "avg_logprob": -0.3271577719486121, "compression_ratio": 1.0444444444444445, "no_speech_prob": 0.00019410686218179762}, {"id": 137, "seek": 94076, "start": 952.16, "end": 954.16, "text": " Step level correctness", "tokens": [50934, 5470, 1496, 3006, 1287, 51034], "temperature": 0.0, "avg_logprob": -0.3271577719486121, "compression_ratio": 1.0444444444444445, "no_speech_prob": 0.00019410686218179762}, {"id": 138, "seek": 94076, "start": 959.84, "end": 961.84, "text": " The glitching is really bad", "tokens": [51318, 440, 23552, 278, 307, 534, 1578, 51418], "temperature": 0.0, "avg_logprob": -0.3271577719486121, "compression_ratio": 1.0444444444444445, "no_speech_prob": 0.00019410686218179762}, {"id": 139, "seek": 97076, "start": 970.76, "end": 972.76, "text": " You", "tokens": [50364, 509, 50464], "temperature": 0.0, "avg_logprob": -0.28913164138793945, "compression_ratio": 1.127906976744186, "no_speech_prob": 0.005138750653713942}, {"id": 140, "seek": 97076, "start": 990.0, "end": 996.52, "text": " Okay, wait, wait, let's talk about the glitching for a minute somehow you can see what's here", "tokens": [51326, 1033, 11, 1699, 11, 1699, 11, 718, 311, 751, 466, 264, 23552, 278, 337, 257, 3456, 6063, 291, 393, 536, 437, 311, 510, 51652], "temperature": 0.0, "avg_logprob": -0.28913164138793945, "compression_ratio": 1.127906976744186, "no_speech_prob": 0.005138750653713942}, {"id": 141, "seek": 99652, "start": 997.4399999999999, "end": 1002.52, "text": " Right and it's this it's the same thing that's in this terminal window", "tokens": [50410, 1779, 293, 309, 311, 341, 309, 311, 264, 912, 551, 300, 311, 294, 341, 14709, 4910, 50664], "temperature": 0.0, "avg_logprob": -0.29395703708424287, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.00745917484164238}, {"id": 142, "seek": 99652, "start": 1003.56, "end": 1007.76, "text": " But yet if I minimize this terminal window, it's still here. Oh", "tokens": [50716, 583, 1939, 498, 286, 17522, 341, 14709, 4910, 11, 309, 311, 920, 510, 13, 876, 50926], "temperature": 0.0, "avg_logprob": -0.29395703708424287, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.00745917484164238}, {"id": 143, "seek": 99652, "start": 1014.16, "end": 1019.0, "text": " We got it we got a good resource we got a video. Let's see getting Rick rolled here", "tokens": [51246, 492, 658, 309, 321, 658, 257, 665, 7684, 321, 658, 257, 960, 13, 961, 311, 536, 1242, 11224, 14306, 510, 51488], "temperature": 0.0, "avg_logprob": -0.29395703708424287, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.00745917484164238}, {"id": 144, "seek": 99652, "start": 1020.8, "end": 1022.8, "text": " Your subscriber you wouldn't do that", "tokens": [51578, 2260, 26122, 291, 2759, 380, 360, 300, 51678], "temperature": 0.0, "avg_logprob": -0.29395703708424287, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.00745917484164238}, {"id": 145, "seek": 102280, "start": 1023.52, "end": 1025.04, "text": " I", "tokens": [50400, 286, 50476], "temperature": 0.0, "avg_logprob": -0.3373400283186403, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0015246423427015543}, {"id": 146, "seek": 102280, "start": 1025.04, "end": 1028.9199999999998, "text": " As you might expect I have been researching non-stop. Wow", "tokens": [50476, 1018, 291, 1062, 2066, 286, 362, 668, 24176, 2107, 12, 13559, 13, 3153, 50670], "temperature": 0.0, "avg_logprob": -0.3373400283186403, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0015246423427015543}, {"id": 147, "seek": 102280, "start": 1030.6399999999999, "end": 1035.3999999999999, "text": " Wow glad that we got over researchers on board guys Q star and on", "tokens": [50756, 3153, 5404, 300, 321, 658, 670, 10309, 322, 3150, 1074, 1249, 3543, 293, 322, 50994], "temperature": 0.0, "avg_logprob": -0.3373400283186403, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0015246423427015543}, {"id": 148, "seek": 102280, "start": 1038.68, "end": 1040.68, "text": " No, this is this is really terrible", "tokens": [51158, 883, 11, 341, 307, 341, 307, 534, 6237, 51258], "temperature": 0.0, "avg_logprob": -0.3373400283186403, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0015246423427015543}, {"id": 149, "seek": 102280, "start": 1044.32, "end": 1046.32, "text": " Hang on", "tokens": [51440, 14070, 322, 51540], "temperature": 0.0, "avg_logprob": -0.3373400283186403, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0015246423427015543}, {"id": 150, "seek": 102280, "start": 1046.3999999999999, "end": 1048.3999999999999, "text": " I'm gonna shut that off for a minute", "tokens": [51544, 286, 478, 799, 5309, 300, 766, 337, 257, 3456, 51644], "temperature": 0.0, "avg_logprob": -0.3373400283186403, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0015246423427015543}, {"id": 151, "seek": 102280, "start": 1049.6, "end": 1051.6, "text": " You guys can look at me while I try to fix", "tokens": [51704, 509, 1074, 393, 574, 412, 385, 1339, 286, 853, 281, 3191, 51804], "temperature": 0.0, "avg_logprob": -0.3373400283186403, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0015246423427015543}, {"id": 152, "seek": 105280, "start": 1052.8, "end": 1054.8, "text": " You", "tokens": [50364, 509, 50464], "temperature": 0.0, "avg_logprob": -0.5054661098279452, "compression_ratio": 0.9354838709677419, "no_speech_prob": 0.008442079648375511}, {"id": 153, "seek": 105280, "start": 1069.48, "end": 1071.48, "text": " Get to the monitor come on", "tokens": [51198, 3240, 281, 264, 6002, 808, 322, 51298], "temperature": 0.0, "avg_logprob": -0.5054661098279452, "compression_ratio": 0.9354838709677419, "no_speech_prob": 0.008442079648375511}, {"id": 154, "seek": 105280, "start": 1076.68, "end": 1078.68, "text": " Is the stream working again", "tokens": [51558, 1119, 264, 4309, 1364, 797, 51658], "temperature": 0.0, "avg_logprob": -0.5054661098279452, "compression_ratio": 0.9354838709677419, "no_speech_prob": 0.008442079648375511}, {"id": 155, "seek": 108280, "start": 1083.6399999999999, "end": 1085.6399999999999, "text": " I", "tokens": [50406, 286, 50506], "temperature": 0.0, "avg_logprob": -0.2907307364723899, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.00020662274619098753}, {"id": 156, "seek": 108280, "start": 1087.32, "end": 1093.0, "text": " Lost internet too. Oh, I lost everything when I turned the monitor off", "tokens": [50590, 23422, 4705, 886, 13, 876, 11, 286, 2731, 1203, 562, 286, 3574, 264, 6002, 766, 50874], "temperature": 0.0, "avg_logprob": -0.2907307364723899, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.00020662274619098753}, {"id": 157, "seek": 108280, "start": 1096.96, "end": 1098.96, "text": " Wait, is this still working", "tokens": [51072, 3802, 11, 307, 341, 920, 1364, 51172], "temperature": 0.0, "avg_logprob": -0.2907307364723899, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.00020662274619098753}, {"id": 158, "seek": 108280, "start": 1104.2, "end": 1106.2, "text": " It's working okay, I don't know", "tokens": [51434, 467, 311, 1364, 1392, 11, 286, 500, 380, 458, 51534], "temperature": 0.0, "avg_logprob": -0.2907307364723899, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.00020662274619098753}, {"id": 159, "seek": 108280, "start": 1106.8799999999999, "end": 1111.56, "text": " The glitching is really bad. It's usually not like I've seen this before but it's gotten way worse", "tokens": [51568, 440, 23552, 278, 307, 534, 1578, 13, 467, 311, 2673, 406, 411, 286, 600, 1612, 341, 949, 457, 309, 311, 5768, 636, 5324, 51802], "temperature": 0.0, "avg_logprob": -0.2907307364723899, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.00020662274619098753}, {"id": 160, "seek": 111156, "start": 1112.12, "end": 1114.12, "text": " No, it didn't help", "tokens": [50392, 883, 11, 309, 994, 380, 854, 50492], "temperature": 0.0, "avg_logprob": -0.3346244812011719, "compression_ratio": 1.4027777777777777, "no_speech_prob": 0.0006166038801893592}, {"id": 161, "seek": 111156, "start": 1122.9199999999998, "end": 1127.6, "text": " Wait, alright, so they're looking at the same paper. I am well computing power while you're taking", "tokens": [50932, 3802, 11, 5845, 11, 370, 436, 434, 1237, 412, 264, 912, 3035, 13, 286, 669, 731, 15866, 1347, 1339, 291, 434, 1940, 51166], "temperature": 0.0, "avg_logprob": -0.3346244812011719, "compression_ratio": 1.4027777777777777, "no_speech_prob": 0.0006166038801893592}, {"id": 162, "seek": 111156, "start": 1129.6, "end": 1133.56, "text": " All right, let's just read the paper. I need to watch some YouTube or read the paper", "tokens": [51266, 1057, 558, 11, 718, 311, 445, 1401, 264, 3035, 13, 286, 643, 281, 1159, 512, 3088, 420, 1401, 264, 3035, 51464], "temperature": 0.0, "avg_logprob": -0.3346244812011719, "compression_ratio": 1.4027777777777777, "no_speech_prob": 0.0006166038801893592}, {"id": 163, "seek": 114156, "start": 1142.56, "end": 1144.56, "text": " Oh", "tokens": [50414, 876, 50514], "temperature": 0.0, "avg_logprob": -0.45050317900521414, "compression_ratio": 1.4337349397590362, "no_speech_prob": 0.01203951332718134}, {"id": 164, "seek": 114156, "start": 1147.04, "end": 1153.36, "text": " The optimal Q-files ability and he said one link to the name Q star could be in a generic sense", "tokens": [50638, 440, 16252, 1249, 12, 69, 4680, 3485, 293, 415, 848, 472, 2113, 281, 264, 1315, 1249, 3543, 727, 312, 294, 257, 19577, 2020, 50954], "temperature": 0.0, "avg_logprob": -0.45050317900521414, "compression_ratio": 1.4337349397590362, "no_speech_prob": 0.01203951332718134}, {"id": 165, "seek": 114156, "start": 1153.56, "end": 1161.0, "text": " Generator the model coming up with solutions with reinforcement learning. We do not just which paper is that paper using test time computer", "tokens": [50964, 15409, 1639, 264, 2316, 1348, 493, 365, 6547, 365, 29280, 2539, 13, 492, 360, 406, 445, 597, 3035, 307, 300, 3035, 1228, 1500, 565, 3820, 51336], "temperature": 0.0, "avg_logprob": -0.45050317900521414, "compression_ratio": 1.4337349397590362, "no_speech_prob": 0.01203951332718134}, {"id": 166, "seek": 117156, "start": 1171.56, "end": 1173.56, "text": " You", "tokens": [50364, 509, 50464], "temperature": 0.0, "avg_logprob": -0.41222286224365234, "compression_ratio": 1.0384615384615385, "no_speech_prob": 0.0072317407466471195}, {"id": 167, "seek": 117156, "start": 1186.84, "end": 1188.84, "text": " Okay", "tokens": [51128, 1033, 51228], "temperature": 0.0, "avg_logprob": -0.41222286224365234, "compression_ratio": 1.0384615384615385, "no_speech_prob": 0.0072317407466471195}, {"id": 168, "seek": 117156, "start": 1189.96, "end": 1194.84, "text": " You know what I think maybe we'll do one of the bounties and tiny grad I", "tokens": [51284, 509, 458, 437, 286, 519, 1310, 321, 603, 360, 472, 295, 264, 272, 792, 530, 293, 5870, 2771, 286, 51528], "temperature": 0.0, "avg_logprob": -0.41222286224365234, "compression_ratio": 1.0384615384615385, "no_speech_prob": 0.0072317407466471195}, {"id": 169, "seek": 119484, "start": 1195.0, "end": 1203.08, "text": " Think we're gonna first need a chat model. I think regardless of what we're doing. We're gonna need a chat model", "tokens": [50372, 6557, 321, 434, 799, 700, 643, 257, 5081, 2316, 13, 286, 519, 10060, 295, 437, 321, 434, 884, 13, 492, 434, 799, 643, 257, 5081, 2316, 50776], "temperature": 0.0, "avg_logprob": -0.3018123973499645, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.0006563388742506504}, {"id": 170, "seek": 119484, "start": 1211.72, "end": 1216.32, "text": " I was playing with trying to make embedding fast last night", "tokens": [51208, 286, 390, 2433, 365, 1382, 281, 652, 12240, 3584, 2370, 1036, 1818, 51438], "temperature": 0.0, "avg_logprob": -0.3018123973499645, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.0006563388742506504}, {"id": 171, "seek": 119484, "start": 1218.52, "end": 1221.6, "text": " Connecting process supervision to 30x model size", "tokens": [51548, 11653, 278, 1399, 32675, 281, 2217, 87, 2316, 2744, 51702], "temperature": 0.0, "avg_logprob": -0.3018123973499645, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.0006563388742506504}, {"id": 172, "seek": 122160, "start": 1221.6, "end": 1224.9599999999998, "text": " We got a whole army here. You guys can watch this video. So I don't have to", "tokens": [50364, 492, 658, 257, 1379, 7267, 510, 13, 509, 1074, 393, 1159, 341, 960, 13, 407, 286, 500, 380, 362, 281, 50532], "temperature": 0.0, "avg_logprob": -0.3107579175163718, "compression_ratio": 1.1634615384615385, "no_speech_prob": 0.0028444682247936726}, {"id": 173, "seek": 122160, "start": 1232.76, "end": 1235.1599999999999, "text": " I did not realize how broken this monitor was", "tokens": [50922, 286, 630, 406, 4325, 577, 5463, 341, 6002, 390, 51042], "temperature": 0.0, "avg_logprob": -0.3107579175163718, "compression_ratio": 1.1634615384615385, "no_speech_prob": 0.0028444682247936726}, {"id": 174, "seek": 125160, "start": 1252.6, "end": 1257.32, "text": " All right, so first we're gonna need a good model. What's what's the best model we got?", "tokens": [50414, 1057, 558, 11, 370, 700, 321, 434, 799, 643, 257, 665, 2316, 13, 708, 311, 437, 311, 264, 1151, 2316, 321, 658, 30, 50650], "temperature": 0.0, "avg_logprob": -0.27905345785206764, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.0016743205487728119}, {"id": 175, "seek": 125160, "start": 1257.32, "end": 1262.04, "text": " What's the best the best 7b model we got? Is it Intel neural chat?", "tokens": [50650, 708, 311, 264, 1151, 264, 1151, 1614, 65, 2316, 321, 658, 30, 1119, 309, 19762, 18161, 5081, 30, 50886], "temperature": 0.0, "avg_logprob": -0.27905345785206764, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.0016743205487728119}, {"id": 176, "seek": 125160, "start": 1263.6, "end": 1265.6, "text": " Is this one nerfed to hell do you think", "tokens": [50964, 1119, 341, 472, 18219, 33712, 281, 4921, 360, 291, 519, 51064], "temperature": 0.0, "avg_logprob": -0.27905345785206764, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.0016743205487728119}, {"id": 177, "seek": 125160, "start": 1268.04, "end": 1271.36, "text": " Open Hermes 2.5 so people like", "tokens": [51186, 7238, 21842, 279, 568, 13, 20, 370, 561, 411, 51352], "temperature": 0.0, "avg_logprob": -0.27905345785206764, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.0016743205487728119}, {"id": 178, "seek": 125160, "start": 1274.12, "end": 1277.6399999999999, "text": " I'm locking the bounty for myself too, so I'm gonna go do that", "tokens": [51490, 286, 478, 23954, 264, 40773, 337, 2059, 886, 11, 370, 286, 478, 799, 352, 360, 300, 51666], "temperature": 0.0, "avg_logprob": -0.27905345785206764, "compression_ratio": 1.5401069518716577, "no_speech_prob": 0.0016743205487728119}, {"id": 179, "seek": 128160, "start": 1281.84, "end": 1283.84, "text": " You", "tokens": [50376, 509, 50476], "temperature": 0.0, "avg_logprob": -0.3085146928444887, "compression_ratio": 1.3457943925233644, "no_speech_prob": 0.0013669339241459966}, {"id": 180, "seek": 128160, "start": 1292.0, "end": 1297.28, "text": " So we're gonna submit a pull request, but it was literally just like the words why you even submit a pull request", "tokens": [50884, 407, 321, 434, 799, 10315, 257, 2235, 5308, 11, 457, 309, 390, 3736, 445, 411, 264, 2283, 983, 291, 754, 10315, 257, 2235, 5308, 51148], "temperature": 0.0, "avg_logprob": -0.3085146928444887, "compression_ratio": 1.3457943925233644, "no_speech_prob": 0.0013669339241459966}, {"id": 181, "seek": 128160, "start": 1306.24, "end": 1308.24, "text": " Okay, is this what we like", "tokens": [51596, 1033, 11, 307, 341, 437, 321, 411, 51696], "temperature": 0.0, "avg_logprob": -0.3085146928444887, "compression_ratio": 1.3457943925233644, "no_speech_prob": 0.0013669339241459966}, {"id": 182, "seek": 131160, "start": 1312.08, "end": 1316.48, "text": " Wait why this one doesn't seem as good", "tokens": [50388, 3802, 983, 341, 472, 1177, 380, 1643, 382, 665, 50608], "temperature": 0.0, "avg_logprob": -0.3295846978823344, "compression_ratio": 1.248062015503876, "no_speech_prob": 0.0003101462498307228}, {"id": 183, "seek": 131160, "start": 1318.8, "end": 1320.8, "text": " Now sir me seems better", "tokens": [50724, 823, 4735, 385, 2544, 1101, 50824], "temperature": 0.0, "avg_logprob": -0.3295846978823344, "compression_ratio": 1.248062015503876, "no_speech_prob": 0.0003101462498307228}, {"id": 184, "seek": 131160, "start": 1325.84, "end": 1329.6, "text": " That's 70 be but when I would I not use collective cognition. Oh", "tokens": [51076, 663, 311, 5285, 312, 457, 562, 286, 576, 286, 406, 764, 12590, 46905, 13, 876, 51264], "temperature": 0.0, "avg_logprob": -0.3295846978823344, "compression_ratio": 1.248062015503876, "no_speech_prob": 0.0003101462498307228}, {"id": 185, "seek": 131160, "start": 1333.04, "end": 1335.04, "text": " But you can't see my desktop", "tokens": [51436, 583, 291, 393, 380, 536, 452, 14502, 51536], "temperature": 0.0, "avg_logprob": -0.3295846978823344, "compression_ratio": 1.248062015503876, "no_speech_prob": 0.0003101462498307228}, {"id": 186, "seek": 131160, "start": 1337.08, "end": 1339.08, "text": " What", "tokens": [51638, 708, 51738], "temperature": 0.0, "avg_logprob": -0.3295846978823344, "compression_ratio": 1.248062015503876, "no_speech_prob": 0.0003101462498307228}, {"id": 187, "seek": 134160, "start": 1342.6, "end": 1345.76, "text": " I'm having technical difficulties", "tokens": [50414, 286, 478, 1419, 6191, 14399, 50572], "temperature": 0.0, "avg_logprob": -0.46830548180474174, "compression_ratio": 1.3623188405797102, "no_speech_prob": 0.000677166972309351}, {"id": 188, "seek": 134160, "start": 1358.6, "end": 1361.48, "text": " Oh, I think we also might have a bit right problem", "tokens": [51214, 876, 11, 286, 519, 321, 611, 1062, 362, 257, 857, 558, 1154, 51358], "temperature": 0.0, "avg_logprob": -0.46830548180474174, "compression_ratio": 1.3623188405797102, "no_speech_prob": 0.000677166972309351}, {"id": 189, "seek": 134160, "start": 1363.32, "end": 1365.48, "text": " No, that's a pretty heavy right, okay", "tokens": [51450, 883, 11, 300, 311, 257, 1238, 4676, 558, 11, 1392, 51558], "temperature": 0.0, "avg_logprob": -0.46830548180474174, "compression_ratio": 1.3623188405797102, "no_speech_prob": 0.000677166972309351}, {"id": 190, "seek": 134160, "start": 1366.8, "end": 1370.12, "text": " We'll put that in front of that. Okay. I'd add that we fixed that", "tokens": [51624, 492, 603, 829, 300, 294, 1868, 295, 300, 13, 1033, 13, 286, 1116, 909, 300, 321, 6806, 300, 51790], "temperature": 0.0, "avg_logprob": -0.46830548180474174, "compression_ratio": 1.3623188405797102, "no_speech_prob": 0.000677166972309351}, {"id": 191, "seek": 137160, "start": 1372.12, "end": 1374.12, "text": " Can you see now", "tokens": [50390, 1664, 291, 536, 586, 50490], "temperature": 0.0, "avg_logprob": -0.2553687924924104, "compression_ratio": 1.2035398230088497, "no_speech_prob": 0.00020987782045267522}, {"id": 192, "seek": 137160, "start": 1377.04, "end": 1379.56, "text": " Wait, why do I want this model it doesn't seem as good", "tokens": [50636, 3802, 11, 983, 360, 286, 528, 341, 2316, 309, 1177, 380, 1643, 382, 665, 50762], "temperature": 0.0, "avg_logprob": -0.2553687924924104, "compression_ratio": 1.2035398230088497, "no_speech_prob": 0.00020987782045267522}, {"id": 193, "seek": 137160, "start": 1384.8799999999999, "end": 1387.48, "text": " Fine-tuned orca DPO", "tokens": [51028, 12024, 12, 83, 43703, 420, 496, 413, 34885, 51158], "temperature": 0.0, "avg_logprob": -0.2553687924924104, "compression_ratio": 1.2035398230088497, "no_speech_prob": 0.00020987782045267522}, {"id": 194, "seek": 137160, "start": 1393.28, "end": 1397.7199999999998, "text": " Do not trust benches you must try it they say", "tokens": [51448, 1144, 406, 3361, 3271, 3781, 291, 1633, 853, 309, 436, 584, 51670], "temperature": 0.0, "avg_logprob": -0.2553687924924104, "compression_ratio": 1.2035398230088497, "no_speech_prob": 0.00020987782045267522}, {"id": 195, "seek": 140160, "start": 1402.6, "end": 1408.04, "text": " All right, all right, do we believe in open Hermes we like this one", "tokens": [50414, 1057, 558, 11, 439, 558, 11, 360, 321, 1697, 294, 1269, 21842, 279, 321, 411, 341, 472, 50686], "temperature": 0.0, "avg_logprob": -0.2552429489467455, "compression_ratio": 1.0434782608695652, "no_speech_prob": 0.0005441656685434282}, {"id": 196, "seek": 140160, "start": 1414.8799999999999, "end": 1416.8799999999999, "text": " Okay", "tokens": [51028, 1033, 51128], "temperature": 0.0, "avg_logprob": -0.2552429489467455, "compression_ratio": 1.0434782608695652, "no_speech_prob": 0.0005441656685434282}, {"id": 197, "seek": 143160, "start": 1431.6, "end": 1433.6, "text": " Oh", "tokens": [50364, 876, 50464], "temperature": 0.0, "avg_logprob": -0.2828328691679856, "compression_ratio": 1.0843373493975903, "no_speech_prob": 0.001064919400960207}, {"id": 198, "seek": 143160, "start": 1447.76, "end": 1455.84, "text": " Wait, so which one is this a clone of does it have rope and stuff what's the vocab size", "tokens": [51172, 3802, 11, 370, 597, 472, 307, 341, 257, 26506, 295, 775, 309, 362, 13540, 293, 1507, 437, 311, 264, 2329, 455, 2744, 51576], "temperature": 0.0, "avg_logprob": -0.2828328691679856, "compression_ratio": 1.0843373493975903, "no_speech_prob": 0.001064919400960207}, {"id": 199, "seek": 145584, "start": 1455.84, "end": 1465.0, "text": " So, okay, it's a mistral fine-tuned we don't actually have a mistral support. Oh", "tokens": [50364, 407, 11, 1392, 11, 309, 311, 257, 3544, 2155, 2489, 12, 83, 43703, 321, 500, 380, 767, 362, 257, 3544, 2155, 1406, 13, 876, 50822], "temperature": 0.0, "avg_logprob": -0.29886800235079736, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.0036498086992651224}, {"id": 200, "seek": 145584, "start": 1465.9199999999998, "end": 1468.4199999999998, "text": " Technium is okay people like Technium. I", "tokens": [50868, 8337, 2197, 307, 1392, 561, 411, 8337, 2197, 13, 286, 50993], "temperature": 0.0, "avg_logprob": -0.29886800235079736, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.0036498086992651224}, {"id": 201, "seek": 145584, "start": 1471.32, "end": 1474.9199999999998, "text": " Won't lock the bounty to myself, but someone's got it because I'm not gonna do the testing", "tokens": [51138, 14710, 380, 4017, 264, 40773, 281, 2059, 11, 457, 1580, 311, 658, 309, 570, 286, 478, 406, 799, 360, 264, 4997, 51318], "temperature": 0.0, "avg_logprob": -0.29886800235079736, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.0036498086992651224}, {"id": 202, "seek": 145584, "start": 1474.9199999999998, "end": 1478.8799999999999, "text": " But if someone wants to do the testing they can have the bounty it's $200", "tokens": [51318, 583, 498, 1580, 2738, 281, 360, 264, 4997, 436, 393, 362, 264, 40773, 309, 311, 1848, 7629, 51516], "temperature": 0.0, "avg_logprob": -0.29886800235079736, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.0036498086992651224}, {"id": 203, "seek": 145584, "start": 1481.84, "end": 1484.76, "text": " People train with data sets from benches. Yeah, damn cheaters", "tokens": [51664, 3432, 3847, 365, 1412, 6352, 490, 3271, 3781, 13, 865, 11, 8151, 947, 16749, 51810], "temperature": 0.0, "avg_logprob": -0.29886800235079736, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.0036498086992651224}, {"id": 204, "seek": 148584, "start": 1485.84, "end": 1487.84, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.27278971981692623, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0011159329442307353}, {"id": 205, "seek": 148584, "start": 1498.9199999999998, "end": 1500.9199999999998, "text": " Downloaded in 10 minutes", "tokens": [51018, 32282, 292, 294, 1266, 2077, 51118], "temperature": 0.0, "avg_logprob": -0.27278971981692623, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0011159329442307353}, {"id": 206, "seek": 148584, "start": 1501.32, "end": 1505.24, "text": " Let's take a look at this drill because first we're gonna need a powerful if we want q-star", "tokens": [51138, 961, 311, 747, 257, 574, 412, 341, 11392, 570, 700, 321, 434, 799, 643, 257, 4005, 498, 321, 528, 9505, 12, 9710, 51334], "temperature": 0.0, "avg_logprob": -0.27278971981692623, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0011159329442307353}, {"id": 207, "seek": 148584, "start": 1505.24, "end": 1510.36, "text": " It seems like we're gonna be language well, and we're gonna need a powerful language model like mistral 7b. Oh", "tokens": [51334, 467, 2544, 411, 321, 434, 799, 312, 2856, 731, 11, 293, 321, 434, 799, 643, 257, 4005, 2856, 2316, 411, 3544, 2155, 1614, 65, 13, 876, 51590], "temperature": 0.0, "avg_logprob": -0.27278971981692623, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0011159329442307353}, {"id": 208, "seek": 148584, "start": 1512.04, "end": 1515.74, "text": " No, I don't know if I have sliding window attention", "tokens": [51674, 883, 11, 286, 500, 380, 458, 498, 286, 362, 21169, 4910, 3202, 51859], "temperature": 0.0, "avg_logprob": -0.27278971981692623, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0011159329442307353}, {"id": 209, "seek": 151584, "start": 1516.72, "end": 1519.6, "text": " Did I explain q-star? Well, it looks like it's this", "tokens": [50408, 2589, 286, 2903, 9505, 12, 9710, 30, 1042, 11, 309, 1542, 411, 309, 311, 341, 50552], "temperature": 0.0, "avg_logprob": -0.23768960861932664, "compression_ratio": 1.4367088607594938, "no_speech_prob": 0.0004172954068053514}, {"id": 210, "seek": 151584, "start": 1521.36, "end": 1523.36, "text": " There's something", "tokens": [50640, 821, 311, 746, 50740], "temperature": 0.0, "avg_logprob": -0.23768960861932664, "compression_ratio": 1.4367088607594938, "no_speech_prob": 0.0004172954068053514}, {"id": 211, "seek": 151584, "start": 1523.36, "end": 1525.9599999999998, "text": " But we're gonna need a language model and it involves math", "tokens": [50740, 583, 321, 434, 799, 643, 257, 2856, 2316, 293, 309, 11626, 5221, 50870], "temperature": 0.0, "avg_logprob": -0.23768960861932664, "compression_ratio": 1.4367088607594938, "no_speech_prob": 0.0004172954068053514}, {"id": 212, "seek": 151584, "start": 1526.0, "end": 1531.04, "text": " So let's first see how good it can do language math and let's upgrade it with the q-star algorithm", "tokens": [50872, 407, 718, 311, 700, 536, 577, 665, 309, 393, 360, 2856, 5221, 293, 718, 311, 11484, 309, 365, 264, 9505, 12, 9710, 9284, 51124], "temperature": 0.0, "avg_logprob": -0.23768960861932664, "compression_ratio": 1.4367088607594938, "no_speech_prob": 0.0004172954068053514}, {"id": 213, "seek": 153104, "start": 1532.04, "end": 1534.04, "text": " Ah", "tokens": [50414, 2438, 50514], "temperature": 0.0, "avg_logprob": -0.48623921320988583, "compression_ratio": 1.1204819277108433, "no_speech_prob": 0.013019151985645294}, {"id": 214, "seek": 153104, "start": 1541.0, "end": 1545.6399999999999, "text": " This is our reference implementation of a mistral transformer", "tokens": [50862, 639, 307, 527, 6408, 11420, 295, 257, 3544, 2155, 31782, 51094], "temperature": 0.0, "avg_logprob": -0.48623921320988583, "compression_ratio": 1.1204819277108433, "no_speech_prob": 0.013019151985645294}, {"id": 215, "seek": 153104, "start": 1550.52, "end": 1552.52, "text": " I don't know if we have that", "tokens": [51338, 286, 500, 380, 458, 498, 321, 362, 300, 51438], "temperature": 0.0, "avg_logprob": -0.48623921320988583, "compression_ratio": 1.1204819277108433, "no_speech_prob": 0.013019151985645294}, {"id": 216, "seek": 156104, "start": 1561.04, "end": 1563.04, "text": " You", "tokens": [50364, 509, 50464], "temperature": 0.0, "avg_logprob": -0.23941710789998372, "compression_ratio": 1.371069182389937, "no_speech_prob": 0.0007436583982780576}, {"id": 217, "seek": 156104, "start": 1575.32, "end": 1578.44, "text": " Can look up in the LLM implementation", "tokens": [51078, 1664, 574, 493, 294, 264, 441, 43, 44, 11420, 51234], "temperature": 0.0, "avg_logprob": -0.23941710789998372, "compression_ratio": 1.371069182389937, "no_speech_prob": 0.0007436583982780576}, {"id": 218, "seek": 156104, "start": 1578.44, "end": 1581.56, "text": " You think GPT 3.5 is way better than an open source model", "tokens": [51234, 509, 519, 26039, 51, 805, 13, 20, 307, 636, 1101, 813, 364, 1269, 4009, 2316, 51390], "temperature": 0.0, "avg_logprob": -0.23941710789998372, "compression_ratio": 1.371069182389937, "no_speech_prob": 0.0007436583982780576}, {"id": 219, "seek": 156104, "start": 1581.76, "end": 1588.12, "text": " But my theory about a lot of this is that their data source is just a whole lot better than like slim pajama and stuff", "tokens": [51400, 583, 452, 5261, 466, 257, 688, 295, 341, 307, 300, 641, 1412, 4009, 307, 445, 257, 1379, 688, 1101, 813, 411, 25357, 33819, 2404, 293, 1507, 51718], "temperature": 0.0, "avg_logprob": -0.23941710789998372, "compression_ratio": 1.371069182389937, "no_speech_prob": 0.0007436583982780576}, {"id": 220, "seek": 159104, "start": 1591.04, "end": 1593.04, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.35047093304720794, "compression_ratio": 1.2566371681415929, "no_speech_prob": 0.0012254919856786728}, {"id": 221, "seek": 159104, "start": 1603.08, "end": 1605.08, "text": " Interesting oh", "tokens": [50966, 14711, 1954, 51066], "temperature": 0.0, "avg_logprob": -0.35047093304720794, "compression_ratio": 1.2566371681415929, "no_speech_prob": 0.0012254919856786728}, {"id": 222, "seek": 159104, "start": 1605.08, "end": 1607.08, "text": " I don't know that they did this", "tokens": [51066, 286, 500, 380, 458, 300, 436, 630, 341, 51166], "temperature": 0.0, "avg_logprob": -0.35047093304720794, "compression_ratio": 1.2566371681415929, "no_speech_prob": 0.0012254919856786728}, {"id": 223, "seek": 159104, "start": 1607.44, "end": 1610.28, "text": " But this is okay, and only with three", "tokens": [51184, 583, 341, 307, 1392, 11, 293, 787, 365, 1045, 51326], "temperature": 0.0, "avg_logprob": -0.35047093304720794, "compression_ratio": 1.2566371681415929, "no_speech_prob": 0.0012254919856786728}, {"id": 224, "seek": 159104, "start": 1611.68, "end": 1615.1599999999999, "text": " So you can actually look there's a thing called mask in", "tokens": [51396, 407, 291, 393, 767, 574, 456, 311, 257, 551, 1219, 6094, 294, 51570], "temperature": 0.0, "avg_logprob": -0.35047093304720794, "compression_ratio": 1.2566371681415929, "no_speech_prob": 0.0012254919856786728}, {"id": 225, "seek": 161516, "start": 1615.76, "end": 1622.0800000000002, "text": " Inattention here, and I think that they just changed the mask. It's cool to look at these latest tricks", "tokens": [50394, 682, 1591, 1251, 510, 11, 293, 286, 519, 300, 436, 445, 3105, 264, 6094, 13, 467, 311, 1627, 281, 574, 412, 613, 6792, 11733, 50710], "temperature": 0.0, "avg_logprob": -0.2803434689839681, "compression_ratio": 1.475, "no_speech_prob": 0.003884165780618787}, {"id": 226, "seek": 161516, "start": 1630.92, "end": 1633.78, "text": " We implement a rolling buffer cache", "tokens": [51152, 492, 4445, 257, 9439, 21762, 19459, 51295], "temperature": 0.0, "avg_logprob": -0.2803434689839681, "compression_ratio": 1.475, "no_speech_prob": 0.003884165780618787}, {"id": 227, "seek": 161516, "start": 1635.88, "end": 1641.96, "text": " I mean this is yeah, only once we hit the context length. What is the context length of mistral?", "tokens": [51400, 286, 914, 341, 307, 1338, 11, 787, 1564, 321, 2045, 264, 4319, 4641, 13, 708, 307, 264, 4319, 4641, 295, 3544, 2155, 30, 51704], "temperature": 0.0, "avg_logprob": -0.2803434689839681, "compression_ratio": 1.475, "no_speech_prob": 0.003884165780618787}, {"id": 228, "seek": 164516, "start": 1646.16, "end": 1651.28, "text": " It's 8k a sick", "tokens": [50414, 467, 311, 1649, 74, 257, 4998, 50670], "temperature": 0.0, "avg_logprob": -0.4482452251293041, "compression_ratio": 1.0897435897435896, "no_speech_prob": 0.0011694711865857244}, {"id": 229, "seek": 164516, "start": 1667.24, "end": 1673.22, "text": " File so it has two files is it not these seven bees only have one file", "tokens": [51468, 26196, 370, 309, 575, 732, 7098, 307, 309, 406, 613, 3407, 17511, 787, 362, 472, 3991, 51767], "temperature": 0.0, "avg_logprob": -0.4482452251293041, "compression_ratio": 1.0897435897435896, "no_speech_prob": 0.0011694711865857244}, {"id": 230, "seek": 167516, "start": 1675.16, "end": 1677.16, "text": " So if I look at like llama to", "tokens": [50364, 407, 498, 286, 574, 412, 411, 23272, 281, 50464], "temperature": 0.0, "avg_logprob": -0.36173786057366264, "compression_ratio": 1.2376237623762376, "no_speech_prob": 0.0012254732428118587}, {"id": 231, "seek": 167516, "start": 1680.88, "end": 1685.4, "text": " Yeah, they just have one how does this one have multiple", "tokens": [50650, 865, 11, 436, 445, 362, 472, 577, 775, 341, 472, 362, 3866, 50876], "temperature": 0.0, "avg_logprob": -0.36173786057366264, "compression_ratio": 1.2376237623762376, "no_speech_prob": 0.0012254732428118587}, {"id": 232, "seek": 167516, "start": 1699.64, "end": 1701.8000000000002, "text": " I don't know what this other format is", "tokens": [51588, 286, 500, 380, 458, 437, 341, 661, 7877, 307, 51696], "temperature": 0.0, "avg_logprob": -0.36173786057366264, "compression_ratio": 1.2376237623762376, "no_speech_prob": 0.0012254732428118587}, {"id": 233, "seek": 170516, "start": 1706.16, "end": 1708.16, "text": " Yeah, that's great", "tokens": [50414, 865, 11, 300, 311, 869, 50514], "temperature": 0.0, "avg_logprob": -0.3589884365306181, "compression_ratio": 1.485, "no_speech_prob": 0.001000483869574964}, {"id": 234, "seek": 170516, "start": 1709.0800000000002, "end": 1711.92, "text": " See this just has pie torch model wait what?", "tokens": [50560, 3008, 341, 445, 575, 1730, 27822, 2316, 1699, 437, 30, 50702], "temperature": 0.0, "avg_logprob": -0.3589884365306181, "compression_ratio": 1.485, "no_speech_prob": 0.001000483869574964}, {"id": 235, "seek": 170516, "start": 1712.8400000000001, "end": 1717.0600000000002, "text": " Why do they use fp32? We trust Technium and open Hermes?", "tokens": [50748, 1545, 360, 436, 764, 283, 79, 11440, 30, 492, 3361, 8337, 2197, 293, 1269, 21842, 279, 30, 50959], "temperature": 0.0, "avg_logprob": -0.3589884365306181, "compression_ratio": 1.485, "no_speech_prob": 0.001000483869574964}, {"id": 236, "seek": 170516, "start": 1719.0800000000002, "end": 1721.5600000000002, "text": " Technium is like he's on Twitter we trust people on Twitter", "tokens": [51060, 8337, 2197, 307, 411, 415, 311, 322, 5794, 321, 3361, 561, 322, 5794, 51184], "temperature": 0.0, "avg_logprob": -0.3589884365306181, "compression_ratio": 1.485, "no_speech_prob": 0.001000483869574964}, {"id": 237, "seek": 170516, "start": 1722.3600000000001, "end": 1724.3600000000001, "text": " Sorry on eggs", "tokens": [51224, 4919, 322, 6466, 51324], "temperature": 0.0, "avg_logprob": -0.3589884365306181, "compression_ratio": 1.485, "no_speech_prob": 0.001000483869574964}, {"id": 238, "seek": 170516, "start": 1725.0, "end": 1730.24, "text": " The first one's still downloading there's two, but they don't match in size. Why would anyone do this?", "tokens": [51356, 440, 700, 472, 311, 920, 32529, 456, 311, 732, 11, 457, 436, 500, 380, 2995, 294, 2744, 13, 1545, 576, 2878, 360, 341, 30, 51618], "temperature": 0.0, "avg_logprob": -0.3589884365306181, "compression_ratio": 1.485, "no_speech_prob": 0.001000483869574964}, {"id": 239, "seek": 173516, "start": 1735.28, "end": 1737.28, "text": " I", "tokens": [50370, 286, 50470], "temperature": 0.0, "avg_logprob": -0.4787037061608356, "compression_ratio": 1.0657894736842106, "no_speech_prob": 0.0007207897142507136}, {"id": 240, "seek": 173516, "start": 1737.72, "end": 1739.72, "text": " Was all terrible", "tokens": [50492, 3027, 439, 6237, 50592], "temperature": 0.0, "avg_logprob": -0.4787037061608356, "compression_ratio": 1.0657894736842106, "no_speech_prob": 0.0007207897142507136}, {"id": 241, "seek": 173516, "start": 1754.96, "end": 1758.1200000000001, "text": " I like this get out of hand with the open source contributions", "tokens": [51354, 286, 411, 341, 483, 484, 295, 1011, 365, 264, 1269, 4009, 15725, 51512], "temperature": 0.0, "avg_logprob": -0.4787037061608356, "compression_ratio": 1.0657894736842106, "no_speech_prob": 0.0007207897142507136}, {"id": 242, "seek": 176516, "start": 1765.76, "end": 1769.72, "text": " Hermes is your favorite of the mistral twins. All right, sounds like you've been playing with these things a lot", "tokens": [50394, 21842, 279, 307, 428, 2954, 295, 264, 3544, 2155, 22555, 13, 1057, 558, 11, 3263, 411, 291, 600, 668, 2433, 365, 613, 721, 257, 688, 50592], "temperature": 0.0, "avg_logprob": -0.22530390086926913, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.003376437583938241}, {"id": 243, "seek": 176516, "start": 1773.24, "end": 1780.72, "text": " Okay, we're gonna need to implement this sliding window attention, I think we're not gonna figure out what's exactly in the stuff till", "tokens": [50768, 1033, 11, 321, 434, 799, 643, 281, 4445, 341, 21169, 4910, 3202, 11, 286, 519, 321, 434, 406, 799, 2573, 484, 437, 311, 2293, 294, 264, 1507, 4288, 51142], "temperature": 0.0, "avg_logprob": -0.22530390086926913, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.003376437583938241}, {"id": 244, "seek": 176516, "start": 1784.68, "end": 1790.68, "text": " Yeah, these personalities are also like this is old shit, you know what", "tokens": [51340, 865, 11, 613, 25308, 366, 611, 411, 341, 307, 1331, 4611, 11, 291, 458, 437, 51640], "temperature": 0.0, "avg_logprob": -0.22530390086926913, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.003376437583938241}, {"id": 245, "seek": 179068, "start": 1790.8400000000001, "end": 1794.68, "text": " Why don't we just rewrite it", "tokens": [50372, 1545, 500, 380, 321, 445, 28132, 309, 50564], "temperature": 0.0, "avg_logprob": -0.3424810568491618, "compression_ratio": 1.328125, "no_speech_prob": 0.0018968121148645878}, {"id": 246, "seek": 179068, "start": 1797.68, "end": 1801.0, "text": " Like I don't want to deal with any of this garbage this code all looks terrible", "tokens": [50714, 1743, 286, 500, 380, 528, 281, 2028, 365, 604, 295, 341, 14150, 341, 3089, 439, 1542, 6237, 50880], "temperature": 0.0, "avg_logprob": -0.3424810568491618, "compression_ratio": 1.328125, "no_speech_prob": 0.0018968121148645878}, {"id": 247, "seek": 179068, "start": 1807.2, "end": 1810.3200000000002, "text": " That's right mistral dot pie with the latest stuff", "tokens": [51190, 663, 311, 558, 3544, 2155, 5893, 1730, 365, 264, 6792, 1507, 51346], "temperature": 0.0, "avg_logprob": -0.3424810568491618, "compression_ratio": 1.328125, "no_speech_prob": 0.0018968121148645878}, {"id": 248, "seek": 179068, "start": 1812.88, "end": 1814.88, "text": " Blank code", "tokens": [51474, 2177, 657, 3089, 51574], "temperature": 0.0, "avg_logprob": -0.3424810568491618, "compression_ratio": 1.328125, "no_speech_prob": 0.0018968121148645878}, {"id": 249, "seek": 181488, "start": 1815.0400000000002, "end": 1817.0400000000002, "text": " Oh", "tokens": [50372, 876, 50472], "temperature": 0.0, "avg_logprob": -0.24990456104278563, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0008426196873188019}, {"id": 250, "seek": 181488, "start": 1820.16, "end": 1822.16, "text": " I don't need sliding window attention", "tokens": [50628, 286, 500, 380, 643, 21169, 4910, 3202, 50728], "temperature": 0.0, "avg_logprob": -0.24990456104278563, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0008426196873188019}, {"id": 251, "seek": 181488, "start": 1824.24, "end": 1829.3200000000002, "text": " Wait, I think I do or you're saying they trained it", "tokens": [50832, 3802, 11, 286, 519, 286, 360, 420, 291, 434, 1566, 436, 8895, 309, 51086], "temperature": 0.0, "avg_logprob": -0.24990456104278563, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0008426196873188019}, {"id": 252, "seek": 181488, "start": 1830.88, "end": 1833.92, "text": " They trained it without sliding window attention for smaller stuff", "tokens": [51164, 814, 8895, 309, 1553, 21169, 4910, 3202, 337, 4356, 1507, 51316], "temperature": 0.0, "avg_logprob": -0.24990456104278563, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0008426196873188019}, {"id": 253, "seek": 184488, "start": 1844.88, "end": 1846.88, "text": " What do you mean by the window size", "tokens": [50364, 708, 360, 291, 914, 538, 264, 4910, 2744, 50464], "temperature": 0.0, "avg_logprob": -0.34344953420210855, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.0007793428376317024}, {"id": 254, "seek": 184488, "start": 1853.2800000000002, "end": 1855.2800000000002, "text": " Where do you see all this stuff", "tokens": [50784, 2305, 360, 291, 536, 439, 341, 1507, 50884], "temperature": 0.0, "avg_logprob": -0.34344953420210855, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.0007793428376317024}, {"id": 255, "seek": 184488, "start": 1858.88, "end": 1861.68, "text": " As far as I know, Michelle didn't really release a paper", "tokens": [51064, 1018, 1400, 382, 286, 458, 11, 14933, 994, 380, 534, 4374, 257, 3035, 51204], "temperature": 0.0, "avg_logprob": -0.34344953420210855, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.0007793428376317024}, {"id": 256, "seek": 184488, "start": 1864.8000000000002, "end": 1867.92, "text": " And with each layer tends to the previous", "tokens": [51360, 400, 365, 1184, 4583, 12258, 281, 264, 3894, 51516], "temperature": 0.0, "avg_logprob": -0.34344953420210855, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.0007793428376317024}, {"id": 257, "seek": 184488, "start": 1872.0, "end": 1874.0, "text": " Oh", "tokens": [51720, 876, 51820], "temperature": 0.0, "avg_logprob": -0.34344953420210855, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.0007793428376317024}, {"id": 258, "seek": 187400, "start": 1874.24, "end": 1876.24, "text": " Okay, so it's not actually three", "tokens": [50376, 1033, 11, 370, 309, 311, 406, 767, 1045, 50476], "temperature": 0.0, "avg_logprob": -0.2705717894990565, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.0011694008717313409}, {"id": 259, "seek": 187400, "start": 1878.88, "end": 1885.32, "text": " Here w equals three but in practice w equals 4k, okay a sliding window 4k, I understand", "tokens": [50608, 1692, 261, 6915, 1045, 457, 294, 3124, 261, 6915, 1017, 74, 11, 1392, 257, 21169, 4910, 1017, 74, 11, 286, 1223, 50930], "temperature": 0.0, "avg_logprob": -0.2705717894990565, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.0011694008717313409}, {"id": 260, "seek": 187400, "start": 1888.4, "end": 1890.8, "text": " That actually makes a lot more sense than three", "tokens": [51084, 663, 767, 1669, 257, 688, 544, 2020, 813, 1045, 51204], "temperature": 0.0, "avg_logprob": -0.2705717894990565, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.0011694008717313409}, {"id": 261, "seek": 187400, "start": 1897.28, "end": 1900.48, "text": " And good we can we can use all the latest in", "tokens": [51528, 400, 665, 321, 393, 321, 393, 764, 439, 264, 6792, 294, 51688], "temperature": 0.0, "avg_logprob": -0.2705717894990565, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.0011694008717313409}, {"id": 262, "seek": 190048, "start": 1901.3600000000001, "end": 1903.3600000000001, "text": " In stuff, okay", "tokens": [50408, 682, 1507, 11, 1392, 50508], "temperature": 0.0, "avg_logprob": -0.2360251262934521, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.0034292617347091436}, {"id": 263, "seek": 190048, "start": 1903.44, "end": 1906.08, "text": " All right, um, let's get these models loaded here", "tokens": [50512, 1057, 558, 11, 1105, 11, 718, 311, 483, 613, 5245, 13210, 510, 50644], "temperature": 0.0, "avg_logprob": -0.2360251262934521, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.0034292617347091436}, {"id": 264, "seek": 190048, "start": 1908.24, "end": 1910.64, "text": " So we're gonna start really with with mistral dot pie here", "tokens": [50752, 407, 321, 434, 799, 722, 534, 365, 365, 3544, 2155, 5893, 1730, 510, 50872], "temperature": 0.0, "avg_logprob": -0.2360251262934521, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.0034292617347091436}, {"id": 265, "seek": 190048, "start": 1911.2, "end": 1917.76, "text": " Uh, well, we'll do stuff from scratch. So in tiny grad now you only need to import tensor like that. It's a lot nicer", "tokens": [50900, 4019, 11, 731, 11, 321, 603, 360, 1507, 490, 8459, 13, 407, 294, 5870, 2771, 586, 291, 787, 643, 281, 974, 40863, 411, 300, 13, 467, 311, 257, 688, 22842, 51228], "temperature": 0.0, "avg_logprob": -0.2360251262934521, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.0034292617347091436}, {"id": 266, "seek": 190048, "start": 1918.72, "end": 1920.72, "text": " um", "tokens": [51276, 1105, 51376], "temperature": 0.0, "avg_logprob": -0.2360251262934521, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.0034292617347091436}, {"id": 267, "seek": 190048, "start": 1921.28, "end": 1924.32, "text": " Import nn and then we can do nn state dot", "tokens": [51404, 26391, 297, 77, 293, 550, 321, 393, 360, 297, 77, 1785, 5893, 51556], "temperature": 0.0, "avg_logprob": -0.2360251262934521, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.0034292617347091436}, {"id": 268, "seek": 190048, "start": 1926.56, "end": 1929.84, "text": " Torch load because it's some kind of pie torch model", "tokens": [51668, 7160, 339, 3677, 570, 309, 311, 512, 733, 295, 1730, 27822, 2316, 51832], "temperature": 0.0, "avg_logprob": -0.2360251262934521, "compression_ratio": 1.540909090909091, "no_speech_prob": 0.0034292617347091436}, {"id": 269, "seek": 193048, "start": 1931.44, "end": 1934.64, "text": " Uh weights open Hermes", "tokens": [50412, 4019, 17443, 1269, 21842, 279, 50572], "temperature": 0.0, "avg_logprob": -0.2852950218396309, "compression_ratio": 1.212962962962963, "no_speech_prob": 0.0014549698680639267}, {"id": 270, "seek": 193048, "start": 1944.96, "end": 1946.96, "text": " Part one part two", "tokens": [51088, 4100, 472, 644, 732, 51188], "temperature": 0.0, "avg_logprob": -0.2852950218396309, "compression_ratio": 1.212962962962963, "no_speech_prob": 0.0014549698680639267}, {"id": 271, "seek": 193048, "start": 1950.0, "end": 1952.64, "text": " I should really have this mark tensors is read only", "tokens": [51340, 286, 820, 534, 362, 341, 1491, 10688, 830, 307, 1401, 787, 51472], "temperature": 0.0, "avg_logprob": -0.2852950218396309, "compression_ratio": 1.212962962962963, "no_speech_prob": 0.0014549698680639267}, {"id": 272, "seek": 193048, "start": 1957.1200000000001, "end": 1959.1200000000001, "text": " Just so I don't corrupt the max it out", "tokens": [51696, 1449, 370, 286, 500, 380, 17366, 264, 11469, 309, 484, 51796], "temperature": 0.0, "avg_logprob": -0.2852950218396309, "compression_ratio": 1.212962962962963, "no_speech_prob": 0.0014549698680639267}, {"id": 273, "seek": 196048, "start": 1961.3600000000001, "end": 1963.3600000000001, "text": " Um", "tokens": [50408, 3301, 50508], "temperature": 0.0, "avg_logprob": -0.18279134723502147, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.0008425715495832264}, {"id": 274, "seek": 196048, "start": 1963.44, "end": 1965.44, "text": " That's what we have so far", "tokens": [50512, 663, 311, 437, 321, 362, 370, 1400, 50612], "temperature": 0.0, "avg_logprob": -0.18279134723502147, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.0008425715495832264}, {"id": 275, "seek": 196048, "start": 1970.0, "end": 1973.44, "text": " So you see how fast that is by the way, uh, I worked really hard on this", "tokens": [50840, 407, 291, 536, 577, 2370, 300, 307, 538, 264, 636, 11, 2232, 11, 286, 2732, 534, 1152, 322, 341, 51012], "temperature": 0.0, "avg_logprob": -0.18279134723502147, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.0008425715495832264}, {"id": 276, "seek": 196048, "start": 1973.52, "end": 1979.6, "text": " So our torch load function doesn't actually load the tensors into RAM. It just loads them all with pointers", "tokens": [51016, 407, 527, 27822, 3677, 2445, 1177, 380, 767, 3677, 264, 10688, 830, 666, 14561, 13, 467, 445, 12668, 552, 439, 365, 44548, 51320], "temperature": 0.0, "avg_logprob": -0.18279134723502147, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.0008425715495832264}, {"id": 277, "seek": 196048, "start": 1981.92, "end": 1984.24, "text": " From their disc tensors so I can show you like", "tokens": [51436, 3358, 641, 2983, 10688, 830, 370, 286, 393, 855, 291, 411, 51552], "temperature": 0.0, "avg_logprob": -0.18279134723502147, "compression_ratio": 1.4438202247191012, "no_speech_prob": 0.0008425715495832264}, {"id": 278, "seek": 199048, "start": 1991.44, "end": 1993.44, "text": " Okay", "tokens": [50412, 1033, 50512], "temperature": 0.0, "avg_logprob": -0.22788813416386994, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.0014549930347129703}, {"id": 279, "seek": 199048, "start": 1997.68, "end": 2003.84, "text": " Oh, the d type is half which is kind of nice good, so they're not uh, it's not float. It's not b-float 32", "tokens": [50724, 876, 11, 264, 274, 2010, 307, 1922, 597, 307, 733, 295, 1481, 665, 11, 370, 436, 434, 406, 2232, 11, 309, 311, 406, 15706, 13, 467, 311, 406, 272, 12, 43645, 267, 8858, 51032], "temperature": 0.0, "avg_logprob": -0.22788813416386994, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.0014549930347129703}, {"id": 280, "seek": 199048, "start": 2006.8, "end": 2010.88, "text": " So how does this work let's in part one and what's in part two", "tokens": [51180, 407, 577, 775, 341, 589, 718, 311, 294, 644, 472, 293, 437, 311, 294, 644, 732, 51384], "temperature": 0.0, "avg_logprob": -0.22788813416386994, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.0014549930347129703}, {"id": 281, "seek": 199048, "start": 2013.2, "end": 2016.0, "text": " Okay, good just goes up to 31 here. Okay", "tokens": [51500, 1033, 11, 665, 445, 1709, 493, 281, 10353, 510, 13, 1033, 51640], "temperature": 0.0, "avg_logprob": -0.22788813416386994, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.0014549930347129703}, {"id": 282, "seek": 201600, "start": 2016.0, "end": 2020.48, "text": " Um, so it should have mostly the same architecture as llama", "tokens": [50364, 3301, 11, 370, 309, 820, 362, 5240, 264, 912, 9482, 382, 23272, 50588], "temperature": 0.0, "avg_logprob": -0.24395784838446255, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.0008425526903010905}, {"id": 283, "seek": 201600, "start": 2021.52, "end": 2027.36, "text": " I think that it's been improved enough that there is not too much we can uh", "tokens": [50640, 286, 519, 300, 309, 311, 668, 9689, 1547, 300, 456, 307, 406, 886, 709, 321, 393, 2232, 50932], "temperature": 0.0, "avg_logprob": -0.24395784838446255, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.0008425526903010905}, {"id": 284, "seek": 201600, "start": 2029.2, "end": 2031.2, "text": " We can improve still", "tokens": [51024, 492, 393, 3470, 920, 51124], "temperature": 0.0, "avg_logprob": -0.24395784838446255, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.0008425526903010905}, {"id": 285, "seek": 201600, "start": 2035.68, "end": 2039.44, "text": " Well, yeah, I don't know man short your opening eye stock, uh", "tokens": [51348, 1042, 11, 1338, 11, 286, 500, 380, 458, 587, 2099, 428, 5193, 3313, 4127, 11, 2232, 51536], "temperature": 0.0, "avg_logprob": -0.24395784838446255, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.0008425526903010905}, {"id": 286, "seek": 204600, "start": 2046.56, "end": 2053.76, "text": " Do you just do you want me to write it from scratch like I can't just do it maybe maybe we should because it's kind of cool", "tokens": [50392, 1144, 291, 445, 360, 291, 528, 385, 281, 2464, 309, 490, 8459, 411, 286, 393, 380, 445, 360, 309, 1310, 1310, 321, 820, 570, 309, 311, 733, 295, 1627, 50752], "temperature": 0.0, "avg_logprob": -0.31590807212973543, "compression_ratio": 1.3759398496240602, "no_speech_prob": 0.0027573960833251476}, {"id": 287, "seek": 204600, "start": 2067.76, "end": 2072.56, "text": " But does this even have a k pro q proge, I'm not yeah, okay", "tokens": [51452, 583, 775, 341, 754, 362, 257, 350, 447, 9505, 447, 432, 11, 286, 478, 406, 1338, 11, 1392, 51692], "temperature": 0.0, "avg_logprob": -0.31590807212973543, "compression_ratio": 1.3759398496240602, "no_speech_prob": 0.0027573960833251476}, {"id": 288, "seek": 207256, "start": 2072.56, "end": 2078.88, "text": " Uh, we're permuting them and sticking them in when we load the weights", "tokens": [50364, 4019, 11, 321, 434, 4784, 10861, 552, 293, 13465, 552, 294, 562, 321, 3677, 264, 17443, 50680], "temperature": 0.0, "avg_logprob": -0.20870662870861234, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.0007096377084963024}, {"id": 289, "seek": 207256, "start": 2085.12, "end": 2087.2799999999997, "text": " Maps okay, okay, so we're we're", "tokens": [50992, 28978, 1392, 11, 1392, 11, 370, 321, 434, 321, 434, 51100], "temperature": 0.0, "avg_logprob": -0.20870662870861234, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.0007096377084963024}, {"id": 290, "seek": 207256, "start": 2090.32, "end": 2093.2, "text": " Oh, this converts it from the hugging face format I say", "tokens": [51252, 876, 11, 341, 38874, 309, 490, 264, 41706, 1851, 7877, 286, 584, 51396], "temperature": 0.0, "avg_logprob": -0.20870662870861234, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.0007096377084963024}, {"id": 291, "seek": 207256, "start": 2098.48, "end": 2101.68, "text": " Do we want to do that or we want to just implement the hugging face format", "tokens": [51660, 1144, 321, 528, 281, 360, 300, 420, 321, 528, 281, 445, 4445, 264, 41706, 1851, 7877, 51820], "temperature": 0.0, "avg_logprob": -0.20870662870861234, "compression_ratio": 1.5743243243243243, "no_speech_prob": 0.0007096377084963024}, {"id": 292, "seek": 210256, "start": 2102.64, "end": 2104.64, "text": " So", "tokens": [50368, 407, 50468], "temperature": 0.0, "avg_logprob": -0.22846097724382267, "compression_ratio": 1.3135593220338984, "no_speech_prob": 0.00043052269029431045}, {"id": 293, "seek": 210256, "start": 2106.16, "end": 2108.16, "text": " What do you think chat", "tokens": [50544, 708, 360, 291, 519, 5081, 50644], "temperature": 0.0, "avg_logprob": -0.22846097724382267, "compression_ratio": 1.3135593220338984, "no_speech_prob": 0.00043052269029431045}, {"id": 294, "seek": 210256, "start": 2111.84, "end": 2113.84, "text": " She's a converter or should we just rewrite it", "tokens": [50828, 1240, 311, 257, 33905, 420, 820, 321, 445, 28132, 309, 50928], "temperature": 0.0, "avg_logprob": -0.22846097724382267, "compression_ratio": 1.3135593220338984, "no_speech_prob": 0.00043052269029431045}, {"id": 295, "seek": 210256, "start": 2116.24, "end": 2118.24, "text": " Looks easy enough to write", "tokens": [51048, 10027, 1858, 1547, 281, 2464, 51148], "temperature": 0.0, "avg_logprob": -0.22846097724382267, "compression_ratio": 1.3135593220338984, "no_speech_prob": 0.00043052269029431045}, {"id": 296, "seek": 210256, "start": 2119.68, "end": 2121.68, "text": " Maybe you guys will appreciate it if we do some writing", "tokens": [51220, 2704, 291, 1074, 486, 4449, 309, 498, 321, 360, 512, 3579, 51320], "temperature": 0.0, "avg_logprob": -0.22846097724382267, "compression_ratio": 1.3135593220338984, "no_speech_prob": 0.00043052269029431045}, {"id": 297, "seek": 213256, "start": 2133.52, "end": 2143.52, "text": " Yeah, yeah, yeah, I know about g guff", "tokens": [50412, 865, 11, 1338, 11, 1338, 11, 286, 458, 466, 290, 290, 1245, 50912], "temperature": 0.0, "avg_logprob": -0.3588465177095853, "compression_ratio": 1.0684931506849316, "no_speech_prob": 0.0018966642674058676}, {"id": 298, "seek": 213256, "start": 2150.08, "end": 2152.58, "text": " Which we can also print v dot shape here", "tokens": [51240, 3013, 321, 393, 611, 4482, 371, 5893, 3909, 510, 51365], "temperature": 0.0, "avg_logprob": -0.3588465177095853, "compression_ratio": 1.0684931506849316, "no_speech_prob": 0.0018966642674058676}, {"id": 299, "seek": 216256, "start": 2163.52, "end": 2170.16, "text": " All right, that looks pretty cool. So first we're gonna need our self attention", "tokens": [50412, 1057, 558, 11, 300, 1542, 1238, 1627, 13, 407, 700, 321, 434, 799, 643, 527, 2698, 3202, 50744], "temperature": 0.0, "avg_logprob": -0.28898511614118305, "compression_ratio": 1.36875, "no_speech_prob": 0.0016743186861276627}, {"id": 300, "seek": 216256, "start": 2176.56, "end": 2181.12, "text": " Uh, let's see if we can find those numbers", "tokens": [51064, 4019, 11, 718, 311, 536, 498, 321, 393, 915, 729, 3547, 51292], "temperature": 0.0, "avg_logprob": -0.28898511614118305, "compression_ratio": 1.36875, "no_speech_prob": 0.0016743186861276627}, {"id": 301, "seek": 216256, "start": 2184.64, "end": 2186.64, "text": " Are they in the mystery announcement", "tokens": [51468, 2014, 436, 294, 264, 11422, 12847, 51568], "temperature": 0.0, "avg_logprob": -0.28898511614118305, "compression_ratio": 1.36875, "no_speech_prob": 0.0016743186861276627}, {"id": 302, "seek": 216256, "start": 2188.7999999999997, "end": 2191.68, "text": " Or does everyone just know what these are for seven b's now", "tokens": [51676, 1610, 775, 1518, 445, 458, 437, 613, 366, 337, 3407, 272, 311, 586, 51820], "temperature": 0.0, "avg_logprob": -0.28898511614118305, "compression_ratio": 1.36875, "no_speech_prob": 0.0016743186861276627}, {"id": 303, "seek": 219256, "start": 2192.56, "end": 2194.56, "text": " So", "tokens": [50364, 407, 50464], "temperature": 0.0, "avg_logprob": -0.30051459035565775, "compression_ratio": 1.175257731958763, "no_speech_prob": 0.01115371659398079}, {"id": 304, "seek": 219256, "start": 2212.0, "end": 2214.0, "text": " So the whole", "tokens": [51336, 407, 264, 1379, 51436], "temperature": 0.0, "avg_logprob": -0.30051459035565775, "compression_ratio": 1.175257731958763, "no_speech_prob": 0.01115371659398079}, {"id": 305, "seek": 219256, "start": 2214.0, "end": 2219.04, "text": " Basically a layer seems to look like that. Let's keep the names consistent from the original llama", "tokens": [51436, 8537, 257, 4583, 2544, 281, 574, 411, 300, 13, 961, 311, 1066, 264, 5288, 8398, 490, 264, 3380, 23272, 51688], "temperature": 0.0, "avg_logprob": -0.30051459035565775, "compression_ratio": 1.175257731958763, "no_speech_prob": 0.01115371659398079}, {"id": 306, "seek": 221904, "start": 2219.7599999999998, "end": 2222.16, "text": " So this is called a transformer block", "tokens": [50400, 407, 341, 307, 1219, 257, 31782, 3461, 50520], "temperature": 0.0, "avg_logprob": -0.15499286651611327, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0027574303094297647}, {"id": 307, "seek": 221904, "start": 2224.0, "end": 2228.72, "text": " We're worried about what goes there later start with a transformer block", "tokens": [50612, 492, 434, 5804, 466, 437, 1709, 456, 1780, 722, 365, 257, 31782, 3461, 50848], "temperature": 0.0, "avg_logprob": -0.15499286651611327, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0027574303094297647}, {"id": 308, "seek": 221904, "start": 2232.4, "end": 2234.4, "text": " Um", "tokens": [51032, 3301, 51132], "temperature": 0.0, "avg_logprob": -0.15499286651611327, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0027574303094297647}, {"id": 309, "seek": 221904, "start": 2234.56, "end": 2238.16, "text": " Again, we're worried about what goes there later. We'll just leave it like that for now", "tokens": [51140, 3764, 11, 321, 434, 5804, 466, 437, 1709, 456, 1780, 13, 492, 603, 445, 1856, 309, 411, 300, 337, 586, 51320], "temperature": 0.0, "avg_logprob": -0.15499286651611327, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0027574303094297647}, {"id": 310, "seek": 221904, "start": 2238.96, "end": 2242.8, "text": " Okay, so we're definitely going to need something called self attention", "tokens": [51360, 1033, 11, 370, 321, 434, 2138, 516, 281, 643, 746, 1219, 2698, 3202, 51552], "temperature": 0.0, "avg_logprob": -0.15499286651611327, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0027574303094297647}, {"id": 311, "seek": 221904, "start": 2244.16, "end": 2246.16, "text": " equals attention", "tokens": [51620, 6915, 3202, 51720], "temperature": 0.0, "avg_logprob": -0.15499286651611327, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.0027574303094297647}, {"id": 312, "seek": 224616, "start": 2246.8799999999997, "end": 2249.6, "text": " And we're going to need something called mlp", "tokens": [50400, 400, 321, 434, 516, 281, 643, 746, 1219, 23271, 79, 50536], "temperature": 0.0, "avg_logprob": -0.18246575059561893, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0007793244440108538}, {"id": 313, "seek": 224616, "start": 2250.7999999999997, "end": 2252.7999999999997, "text": " What do I call it now feed forward?", "tokens": [50596, 708, 360, 286, 818, 309, 586, 3154, 2128, 30, 50696], "temperature": 0.0, "avg_logprob": -0.18246575059561893, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0007793244440108538}, {"id": 314, "seek": 224616, "start": 2253.52, "end": 2256.8799999999997, "text": " Actually, why don't we look at the reference repo and copy their names?", "tokens": [50732, 5135, 11, 983, 500, 380, 321, 574, 412, 264, 6408, 49040, 293, 5055, 641, 5288, 30, 50900], "temperature": 0.0, "avg_logprob": -0.18246575059561893, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0007793244440108538}, {"id": 315, "seek": 224616, "start": 2257.7599999999998, "end": 2259.7599999999998, "text": " It's always better to take names", "tokens": [50944, 467, 311, 1009, 1101, 281, 747, 5288, 51044], "temperature": 0.0, "avg_logprob": -0.18246575059561893, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0007793244440108538}, {"id": 316, "seek": 224616, "start": 2262.24, "end": 2264.24, "text": " Okay, they do call it attention", "tokens": [51168, 1033, 11, 436, 360, 818, 309, 3202, 51268], "temperature": 0.0, "avg_logprob": -0.18246575059561893, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0007793244440108538}, {"id": 317, "seek": 224616, "start": 2264.7999999999997, "end": 2268.64, "text": " Let's just look at their here transformer block. They call it feed forward to", "tokens": [51296, 961, 311, 445, 574, 412, 641, 510, 31782, 3461, 13, 814, 818, 309, 3154, 2128, 281, 51488], "temperature": 0.0, "avg_logprob": -0.18246575059561893, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0007793244440108538}, {"id": 318, "seek": 224616, "start": 2269.2799999999997, "end": 2271.2799999999997, "text": " What is almost the same?", "tokens": [51520, 708, 307, 1920, 264, 912, 30, 51620], "temperature": 0.0, "avg_logprob": -0.18246575059561893, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0007793244440108538}, {"id": 319, "seek": 227128, "start": 2271.84, "end": 2281.6000000000004, "text": " No, look they call their stuff this they just must have some weird loading script to convert it from the hugging face format too", "tokens": [50392, 883, 11, 574, 436, 818, 641, 1507, 341, 436, 445, 1633, 362, 512, 3657, 15114, 5755, 281, 7620, 309, 490, 264, 41706, 1851, 7877, 886, 50880], "temperature": 0.0, "avg_logprob": -0.20632323622703552, "compression_ratio": 1.582857142857143, "no_speech_prob": 0.0002824034309014678}, {"id": 320, "seek": 227128, "start": 2283.84, "end": 2285.84, "text": " I don't know I feel about that", "tokens": [50992, 286, 500, 380, 458, 286, 841, 466, 300, 51092], "temperature": 0.0, "avg_logprob": -0.20632323622703552, "compression_ratio": 1.582857142857143, "no_speech_prob": 0.0002824034309014678}, {"id": 321, "seek": 227128, "start": 2288.32, "end": 2291.1200000000003, "text": " Hugging face probably calls it something else then", "tokens": [51216, 46892, 3249, 1851, 1391, 5498, 309, 746, 1646, 550, 51356], "temperature": 0.0, "avg_logprob": -0.20632323622703552, "compression_ratio": 1.582857142857143, "no_speech_prob": 0.0002824034309014678}, {"id": 322, "seek": 227128, "start": 2293.6800000000003, "end": 2298.32, "text": " It's like the hugging face transformers repo what did everyone use", "tokens": [51484, 467, 311, 411, 264, 41706, 1851, 4088, 433, 49040, 437, 630, 1518, 764, 51716], "temperature": 0.0, "avg_logprob": -0.20632323622703552, "compression_ratio": 1.582857142857143, "no_speech_prob": 0.0002824034309014678}, {"id": 323, "seek": 230128, "start": 2302.0, "end": 2304.0, "text": " Uh", "tokens": [50400, 4019, 50500], "temperature": 0.0, "avg_logprob": -0.29436445236206055, "compression_ratio": 1.1313131313131313, "no_speech_prob": 0.0013458059402182698}, {"id": 324, "seek": 230128, "start": 2311.36, "end": 2313.36, "text": " The yellow lamb I've heard thrown around", "tokens": [50868, 440, 5566, 10097, 286, 600, 2198, 11732, 926, 50968], "temperature": 0.0, "avg_logprob": -0.29436445236206055, "compression_ratio": 1.1313131313131313, "no_speech_prob": 0.0013458059402182698}, {"id": 325, "seek": 230128, "start": 2321.76, "end": 2323.76, "text": " Very complicated looking", "tokens": [51388, 4372, 6179, 1237, 51488], "temperature": 0.0, "avg_logprob": -0.29436445236206055, "compression_ratio": 1.1313131313131313, "no_speech_prob": 0.0013458059402182698}, {"id": 326, "seek": 230128, "start": 2326.7200000000003, "end": 2329.0400000000004, "text": " Wow so easy you could just pip install this", "tokens": [51636, 3153, 370, 1858, 291, 727, 445, 8489, 3625, 341, 51752], "temperature": 0.0, "avg_logprob": -0.29436445236206055, "compression_ratio": 1.1313131313131313, "no_speech_prob": 0.0013458059402182698}, {"id": 327, "seek": 233128, "start": 2332.2400000000002, "end": 2340.88, "text": " Okay, maybe we should import transformer from llama just to make things go faster", "tokens": [50412, 1033, 11, 1310, 321, 820, 974, 31782, 490, 23272, 445, 281, 652, 721, 352, 4663, 50844], "temperature": 0.0, "avg_logprob": -0.2417890179541803, "compression_ratio": 1.238532110091743, "no_speech_prob": 0.00032502057729288936}, {"id": 328, "seek": 233128, "start": 2348.0, "end": 2351.76, "text": " And then we also can import convert from hugging face", "tokens": [51200, 400, 550, 321, 611, 393, 974, 7620, 490, 41706, 1851, 51388], "temperature": 0.0, "avg_logprob": -0.2417890179541803, "compression_ratio": 1.238532110091743, "no_speech_prob": 0.00032502057729288936}, {"id": 329, "seek": 235176, "start": 2351.92, "end": 2353.92, "text": " You", "tokens": [50372, 509, 50472], "temperature": 0.0, "avg_logprob": -0.3295195597522664, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.0063877711072564125}, {"id": 330, "seek": 235176, "start": 2358.0800000000004, "end": 2360.0800000000004, "text": " Can read it we wrote llama on another stream", "tokens": [50680, 1664, 1401, 309, 321, 4114, 23272, 322, 1071, 4309, 50780], "temperature": 0.0, "avg_logprob": -0.3295195597522664, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.0063877711072564125}, {"id": 331, "seek": 235176, "start": 2360.6400000000003, "end": 2363.92, "text": " We could do it, but you know, we we got to get to actually writing q-star", "tokens": [50808, 492, 727, 360, 309, 11, 457, 291, 458, 11, 321, 321, 658, 281, 483, 281, 767, 3579, 9505, 12, 9710, 50972], "temperature": 0.0, "avg_logprob": -0.3295195597522664, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.0063877711072564125}, {"id": 332, "seek": 235176, "start": 2364.48, "end": 2368.0800000000004, "text": " Some examples of llama import convert from hugging face", "tokens": [51000, 2188, 5110, 295, 23272, 974, 7620, 490, 41706, 1851, 51180], "temperature": 0.0, "avg_logprob": -0.3295195597522664, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.0063877711072564125}, {"id": 333, "seek": 235176, "start": 2369.28, "end": 2371.28, "text": " Import transformer", "tokens": [51240, 26391, 31782, 51340], "temperature": 0.0, "avg_logprob": -0.3295195597522664, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.0063877711072564125}, {"id": 334, "seek": 237128, "start": 2372.0800000000004, "end": 2387.28, "text": " Okay, you convert from hugging face takes in a dictionary of weights, which is a dick from string to tensor", "tokens": [50404, 1033, 11, 291, 7620, 490, 41706, 1851, 2516, 294, 257, 25890, 295, 17443, 11, 597, 307, 257, 18659, 490, 6798, 281, 40863, 51164], "temperature": 0.0, "avg_logprob": -0.34066009521484375, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.006387823261320591}, {"id": 335, "seek": 237128, "start": 2388.32, "end": 2390.7200000000003, "text": " We'll add some types. We love types", "tokens": [51216, 492, 603, 909, 512, 3467, 13, 492, 959, 3467, 51336], "temperature": 0.0, "avg_logprob": -0.34066009521484375, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.006387823261320591}, {"id": 336, "seek": 237128, "start": 2394.2400000000002, "end": 2396.2400000000002, "text": " No, no, no we're implementing q-star", "tokens": [51512, 883, 11, 572, 11, 572, 321, 434, 18114, 9505, 12, 9710, 51612], "temperature": 0.0, "avg_logprob": -0.34066009521484375, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.006387823261320591}, {"id": 337, "seek": 239624, "start": 2396.24, "end": 2403.7599999999998, "text": " Um, okay, so we have a transformer. Uh, what's the dimension of this transformer?", "tokens": [50364, 3301, 11, 1392, 11, 370, 321, 362, 257, 31782, 13, 4019, 11, 437, 311, 264, 10139, 295, 341, 31782, 30, 50740], "temperature": 0.0, "avg_logprob": -0.4033959706624349, "compression_ratio": 1.212962962962963, "no_speech_prob": 0.0032726824283599854}, {"id": 338, "seek": 239624, "start": 2409.2799999999997, "end": 2411.52, "text": " This is an int, it's all ints", "tokens": [51016, 639, 307, 364, 560, 11, 309, 311, 439, 560, 82, 51128], "temperature": 0.0, "avg_logprob": -0.4033959706624349, "compression_ratio": 1.212962962962963, "no_speech_prob": 0.0032726824283599854}, {"id": 339, "seek": 239624, "start": 2418.24, "end": 2420.24, "text": " Except for norm EPS", "tokens": [51464, 16192, 337, 2026, 462, 6273, 51564], "temperature": 0.0, "avg_logprob": -0.4033959706624349, "compression_ratio": 1.212962962962963, "no_speech_prob": 0.0032726824283599854}, {"id": 340, "seek": 242624, "start": 2426.24, "end": 2428.24, "text": " So", "tokens": [50364, 407, 50464], "temperature": 0.0, "avg_logprob": -0.344262869461723, "compression_ratio": 1.2173913043478262, "no_speech_prob": 0.00388400093652308}, {"id": 341, "seek": 242624, "start": 2438.9599999999996, "end": 2440.9599999999996, "text": " What's one file ref", "tokens": [51000, 708, 311, 472, 3991, 1895, 51100], "temperature": 0.0, "avg_logprob": -0.344262869461723, "compression_ratio": 1.2173913043478262, "no_speech_prob": 0.00388400093652308}, {"id": 342, "seek": 242624, "start": 2444.7999999999997, "end": 2447.8399999999997, "text": " I hear there's something called model args as a data class", "tokens": [51292, 286, 1568, 456, 311, 746, 1219, 2316, 3882, 82, 382, 257, 1412, 1508, 51444], "temperature": 0.0, "avg_logprob": -0.344262869461723, "compression_ratio": 1.2173913043478262, "no_speech_prob": 0.00388400093652308}, {"id": 343, "seek": 242624, "start": 2448.72, "end": 2452.72, "text": " Oh, then they pass it through with json. Where's the json?", "tokens": [51488, 876, 11, 550, 436, 1320, 309, 807, 365, 361, 3015, 13, 2305, 311, 264, 361, 3015, 30, 51688], "temperature": 0.0, "avg_logprob": -0.344262869461723, "compression_ratio": 1.2173913043478262, "no_speech_prob": 0.00388400093652308}, {"id": 344, "seek": 245272, "start": 2453.2799999999997, "end": 2455.2799999999997, "text": " Is there json", "tokens": [50392, 1119, 456, 361, 3015, 50492], "temperature": 0.0, "avg_logprob": -0.24539325855396413, "compression_ratio": 1.449612403100775, "no_speech_prob": 0.0017544731963425875}, {"id": 345, "seek": 245272, "start": 2457.3599999999997, "end": 2464.48, "text": " Where does this json come from is it in assets json sounds like an asset and all those are images what's in deploy", "tokens": [50596, 2305, 775, 341, 361, 3015, 808, 490, 307, 309, 294, 9769, 361, 3015, 3263, 411, 364, 11999, 293, 439, 729, 366, 5267, 437, 311, 294, 7274, 50952], "temperature": 0.0, "avg_logprob": -0.24539325855396413, "compression_ratio": 1.449612403100775, "no_speech_prob": 0.0017544731963425875}, {"id": 346, "seek": 245272, "start": 2468.24, "end": 2471.52, "text": " Nope, wait, so what the hell is json that they're loading?", "tokens": [51140, 12172, 11, 1699, 11, 370, 437, 264, 4921, 307, 361, 3015, 300, 436, 434, 15114, 30, 51304], "temperature": 0.0, "avg_logprob": -0.24539325855396413, "compression_ratio": 1.449612403100775, "no_speech_prob": 0.0017544731963425875}, {"id": 347, "seek": 247152, "start": 2471.52, "end": 2473.52, "text": " Oh", "tokens": [50364, 876, 50464], "temperature": 0.0, "avg_logprob": -0.2528316541151567, "compression_ratio": 1.2393162393162394, "no_speech_prob": 0.004609167110174894}, {"id": 348, "seek": 247152, "start": 2480.48, "end": 2485.68, "text": " At torch inference mode, that's cool. We set tensor no grad equal to true", "tokens": [50812, 1711, 27822, 38253, 4391, 11, 300, 311, 1627, 13, 492, 992, 40863, 572, 2771, 2681, 281, 2074, 51072], "temperature": 0.0, "avg_logprob": -0.2528316541151567, "compression_ratio": 1.2393162393162394, "no_speech_prob": 0.004609167110174894}, {"id": 349, "seek": 247152, "start": 2489.84, "end": 2494.88, "text": " What's fire wow, I'm not up to date. What's all these latest things?", "tokens": [51280, 708, 311, 2610, 6076, 11, 286, 478, 406, 493, 281, 4002, 13, 708, 311, 439, 613, 6792, 721, 30, 51532], "temperature": 0.0, "avg_logprob": -0.2528316541151567, "compression_ratio": 1.2393162393162394, "no_speech_prob": 0.004609167110174894}, {"id": 350, "seek": 250152, "start": 2501.52, "end": 2506.72, "text": " Call fire on any python object. Oh interesting. I like that", "tokens": [50364, 7807, 2610, 322, 604, 38797, 2657, 13, 876, 1880, 13, 286, 411, 300, 50624], "temperature": 0.0, "avg_logprob": -0.22484965324401857, "compression_ratio": 1.44, "no_speech_prob": 0.019121263176202774}, {"id": 351, "seek": 250152, "start": 2512.0, "end": 2515.12, "text": " Um, all right, so where do I put the args?", "tokens": [50888, 3301, 11, 439, 558, 11, 370, 689, 360, 286, 829, 264, 3882, 82, 30, 51044], "temperature": 0.0, "avg_logprob": -0.22484965324401857, "compression_ratio": 1.44, "no_speech_prob": 0.019121263176202774}, {"id": 352, "seek": 250152, "start": 2516.96, "end": 2521.2, "text": " Where where do all those things come from? Oh, I know where is the json and it's probably in here", "tokens": [51136, 2305, 689, 360, 439, 729, 721, 808, 490, 30, 876, 11, 286, 458, 689, 307, 264, 361, 3015, 293, 309, 311, 1391, 294, 510, 51348], "temperature": 0.0, "avg_logprob": -0.22484965324401857, "compression_ratio": 1.44, "no_speech_prob": 0.019121263176202774}, {"id": 353, "seek": 250152, "start": 2523.52, "end": 2525.7599999999998, "text": " Here can big dot json, okay", "tokens": [51464, 1692, 393, 955, 5893, 361, 3015, 11, 1392, 51576], "temperature": 0.0, "avg_logprob": -0.22484965324401857, "compression_ratio": 1.44, "no_speech_prob": 0.019121263176202774}, {"id": 354, "seek": 250152, "start": 2527.2, "end": 2529.92, "text": " Oh, no, it's b float 16", "tokens": [51648, 876, 11, 572, 11, 309, 311, 272, 15706, 3165, 51784], "temperature": 0.0, "avg_logprob": -0.22484965324401857, "compression_ratio": 1.44, "no_speech_prob": 0.019121263176202774}, {"id": 355, "seek": 252992, "start": 2530.88, "end": 2532.88, "text": " No", "tokens": [50412, 883, 50512], "temperature": 0.0, "avg_logprob": -0.30231064558029175, "compression_ratio": 1.127659574468085, "no_speech_prob": 0.000636169919744134}, {"id": 356, "seek": 252992, "start": 2538.7200000000003, "end": 2541.84, "text": " Okay, let's manually convert these", "tokens": [50804, 1033, 11, 718, 311, 16945, 7620, 613, 50960], "temperature": 0.0, "avg_logprob": -0.30231064558029175, "compression_ratio": 1.127659574468085, "no_speech_prob": 0.000636169919744134}, {"id": 357, "seek": 252992, "start": 2543.84, "end": 2546.48, "text": " Wait, I thought I put oh I put them on transformer block", "tokens": [51060, 3802, 11, 286, 1194, 286, 829, 1954, 286, 829, 552, 322, 31782, 3461, 51192], "temperature": 0.0, "avg_logprob": -0.30231064558029175, "compression_ratio": 1.127659574468085, "no_speech_prob": 0.000636169919744134}, {"id": 358, "seek": 252992, "start": 2556.48, "end": 2558.48, "text": " Multiple of", "tokens": [51692, 40056, 295, 51792], "temperature": 0.0, "avg_logprob": -0.30231064558029175, "compression_ratio": 1.127659574468085, "no_speech_prob": 0.000636169919744134}, {"id": 359, "seek": 255992, "start": 2560.32, "end": 2562.32, "text": " And the heads", "tokens": [50384, 400, 264, 8050, 50484], "temperature": 0.0, "avg_logprob": -0.28872060775756836, "compression_ratio": 1.1415094339622642, "no_speech_prob": 0.0006070623057894409}, {"id": 360, "seek": 255992, "start": 2564.88, "end": 2570.48, "text": " And layers normie p passes a float", "tokens": [50612, 400, 7914, 2026, 414, 280, 11335, 257, 15706, 50892], "temperature": 0.0, "avg_logprob": -0.28872060775756836, "compression_ratio": 1.1415094339622642, "no_speech_prob": 0.0006070623057894409}, {"id": 361, "seek": 255992, "start": 2578.96, "end": 2582.8, "text": " Okay, so the dim is going to be", "tokens": [51316, 1033, 11, 370, 264, 5013, 307, 516, 281, 312, 51508], "temperature": 0.0, "avg_logprob": -0.28872060775756836, "compression_ratio": 1.1415094339622642, "no_speech_prob": 0.0006070623057894409}, {"id": 362, "seek": 255992, "start": 2583.92, "end": 2585.92, "text": " 409 sex", "tokens": [51564, 3356, 24, 3260, 51664], "temperature": 0.0, "avg_logprob": -0.28872060775756836, "compression_ratio": 1.1415094339622642, "no_speech_prob": 0.0006070623057894409}, {"id": 363, "seek": 255992, "start": 2586.96, "end": 2588.96, "text": " I don't know what multiple of is", "tokens": [51716, 286, 500, 380, 458, 437, 3866, 295, 307, 51816], "temperature": 0.0, "avg_logprob": -0.28872060775756836, "compression_ratio": 1.1415094339622642, "no_speech_prob": 0.0006070623057894409}, {"id": 364, "seek": 258992, "start": 2590.8, "end": 2594.32, "text": " What is multiple I've even used for I'll feed forward", "tokens": [50408, 708, 307, 3866, 286, 600, 754, 1143, 337, 286, 603, 3154, 2128, 50584], "temperature": 0.0, "avg_logprob": -0.22538746727837455, "compression_ratio": 1.4154929577464788, "no_speech_prob": 0.0004441751225385815}, {"id": 365, "seek": 258992, "start": 2596.32, "end": 2600.32, "text": " Feed forward takes in a multiple out to to increase the hidden dim", "tokens": [50684, 33720, 2128, 2516, 294, 257, 3866, 484, 281, 281, 3488, 264, 7633, 5013, 50884], "temperature": 0.0, "avg_logprob": -0.22538746727837455, "compression_ratio": 1.4154929577464788, "no_speech_prob": 0.0004441751225385815}, {"id": 366, "seek": 258992, "start": 2610.16, "end": 2615.2000000000003, "text": " There's one I get them multiple of 256", "tokens": [51376, 821, 311, 472, 286, 483, 552, 3866, 295, 38882, 51628], "temperature": 0.0, "avg_logprob": -0.22538746727837455, "compression_ratio": 1.4154929577464788, "no_speech_prob": 0.0004441751225385815}, {"id": 367, "seek": 258992, "start": 2617.04, "end": 2619.04, "text": " It's probably a good number. Let's try it", "tokens": [51720, 467, 311, 1391, 257, 665, 1230, 13, 961, 311, 853, 309, 51820], "temperature": 0.0, "avg_logprob": -0.22538746727837455, "compression_ratio": 1.4154929577464788, "no_speech_prob": 0.0004441751225385815}, {"id": 368, "seek": 261992, "start": 2620.2400000000002, "end": 2622.2400000000002, "text": " See if things don't load. Okay", "tokens": [50380, 3008, 498, 721, 500, 380, 3677, 13, 1033, 50480], "temperature": 0.0, "avg_logprob": -0.34373277132628394, "compression_ratio": 1.25, "no_speech_prob": 0.0006070486851967871}, {"id": 369, "seek": 261992, "start": 2622.32, "end": 2624.7200000000003, "text": " Uh, number of heads is 32", "tokens": [50484, 4019, 11, 1230, 295, 8050, 307, 8858, 50604], "temperature": 0.0, "avg_logprob": -0.34373277132628394, "compression_ratio": 1.25, "no_speech_prob": 0.0006070486851967871}, {"id": 370, "seek": 261992, "start": 2625.52, "end": 2627.52, "text": " number of layers is 32", "tokens": [50644, 1230, 295, 7914, 307, 8858, 50744], "temperature": 0.0, "avg_logprob": -0.34373277132628394, "compression_ratio": 1.25, "no_speech_prob": 0.0006070486851967871}, {"id": 371, "seek": 261992, "start": 2628.64, "end": 2630.96, "text": " Normie ps is 1 e minus 5", "tokens": [50800, 8702, 414, 18815, 307, 502, 308, 3175, 1025, 50916], "temperature": 0.0, "avg_logprob": -0.34373277132628394, "compression_ratio": 1.25, "no_speech_prob": 0.0006070486851967871}, {"id": 372, "seek": 261992, "start": 2632.08, "end": 2634.08, "text": " And the vocab size is", "tokens": [50972, 400, 264, 2329, 455, 2744, 307, 51072], "temperature": 0.0, "avg_logprob": -0.34373277132628394, "compression_ratio": 1.25, "no_speech_prob": 0.0006070486851967871}, {"id": 373, "seek": 261992, "start": 2641.92, "end": 2643.92, "text": " Uh", "tokens": [51464, 4019, 51564], "temperature": 0.0, "avg_logprob": -0.34373277132628394, "compression_ratio": 1.25, "no_speech_prob": 0.0006070486851967871}, {"id": 374, "seek": 261992, "start": 2643.92, "end": 2645.92, "text": " 30202", "tokens": [51564, 2217, 2009, 17, 51664], "temperature": 0.0, "avg_logprob": -0.34373277132628394, "compression_ratio": 1.25, "no_speech_prob": 0.0006070486851967871}, {"id": 375, "seek": 261992, "start": 2646.0, "end": 2648.0, "text": " Let's say model equals transformer", "tokens": [51668, 961, 311, 584, 2316, 6915, 31782, 51768], "temperature": 0.0, "avg_logprob": -0.34373277132628394, "compression_ratio": 1.25, "no_speech_prob": 0.0006070486851967871}, {"id": 376, "seek": 264992, "start": 2650.88, "end": 2652.88, "text": " Okay", "tokens": [50412, 1033, 50512], "temperature": 0.0, "avg_logprob": -0.37439978300635496, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.0037649108562618494}, {"id": 377, "seek": 264992, "start": 2653.44, "end": 2658.32, "text": " All right, I'm just gonna put that in I should really just put that in at the beginning of the script", "tokens": [50540, 1057, 558, 11, 286, 478, 445, 799, 829, 300, 294, 286, 820, 534, 445, 829, 300, 294, 412, 264, 2863, 295, 264, 5755, 50784], "temperature": 0.0, "avg_logprob": -0.37439978300635496, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.0037649108562618494}, {"id": 378, "seek": 264992, "start": 2666.16, "end": 2673.76, "text": " It's going to do python path equals all the time. Okay, good. Okay. Um, well it takes such a time", "tokens": [51176, 467, 311, 516, 281, 360, 38797, 3100, 6915, 439, 264, 565, 13, 1033, 11, 665, 13, 1033, 13, 3301, 11, 731, 309, 2516, 1270, 257, 565, 51556], "temperature": 0.0, "avg_logprob": -0.37439978300635496, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.0037649108562618494}, {"id": 379, "seek": 264992, "start": 2675.6800000000003, "end": 2677.6800000000003, "text": " Time to grab as a helper called timing", "tokens": [51652, 6161, 281, 4444, 382, 257, 36133, 1219, 10822, 51752], "temperature": 0.0, "avg_logprob": -0.37439978300635496, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.0037649108562618494}, {"id": 380, "seek": 267992, "start": 2680.88, "end": 2682.88, "text": " You know, I like things to be fast", "tokens": [50412, 509, 458, 11, 286, 411, 721, 281, 312, 2370, 50512], "temperature": 0.0, "avg_logprob": -0.30425589084625243, "compression_ratio": 1.1818181818181819, "no_speech_prob": 0.000646163010969758}, {"id": 381, "seek": 267992, "start": 2690.7200000000003, "end": 2697.84, "text": " Okay, cool, it's all pretty fast. We can get all the prompt here. Uh, load weights", "tokens": [50904, 1033, 11, 1627, 11, 309, 311, 439, 1238, 2370, 13, 492, 393, 483, 439, 264, 12391, 510, 13, 4019, 11, 3677, 17443, 51260], "temperature": 0.0, "avg_logprob": -0.30425589084625243, "compression_ratio": 1.1818181818181819, "no_speech_prob": 0.000646163010969758}, {"id": 382, "seek": 267992, "start": 2700.08, "end": 2702.08, "text": " Create model", "tokens": [51372, 20248, 2316, 51472], "temperature": 0.0, "avg_logprob": -0.30425589084625243, "compression_ratio": 1.1818181818181819, "no_speech_prob": 0.000646163010969758}, {"id": 383, "seek": 270992, "start": 2710.08, "end": 2717.28, "text": " All right, now we're gonna have to stuff them in let's try convert from hugging face", "tokens": [50372, 1057, 558, 11, 586, 321, 434, 799, 362, 281, 1507, 552, 294, 718, 311, 853, 7620, 490, 41706, 1851, 50732], "temperature": 0.0, "avg_logprob": -0.2866499320320461, "compression_ratio": 1.2743362831858407, "no_speech_prob": 0.0008426016429439187}, {"id": 384, "seek": 270992, "start": 2723.12, "end": 2725.12, "text": " Uh", "tokens": [51024, 4019, 51124], "temperature": 0.0, "avg_logprob": -0.2866499320320461, "compression_ratio": 1.2743362831858407, "no_speech_prob": 0.0008426016429439187}, {"id": 385, "seek": 270992, "start": 2726.2400000000002, "end": 2728.2400000000002, "text": " Waits", "tokens": [51180, 3802, 82, 51280], "temperature": 0.0, "avg_logprob": -0.2866499320320461, "compression_ratio": 1.2743362831858407, "no_speech_prob": 0.0008426016429439187}, {"id": 386, "seek": 270992, "start": 2730.64, "end": 2732.64, "text": " Which of these is n heads", "tokens": [51400, 3013, 295, 613, 307, 297, 8050, 51500], "temperature": 0.0, "avg_logprob": -0.2866499320320461, "compression_ratio": 1.2743362831858407, "no_speech_prob": 0.0008426016429439187}, {"id": 387, "seek": 270992, "start": 2735.76, "end": 2737.76, "text": " And heads and n kv heads", "tokens": [51656, 400, 8050, 293, 297, 350, 85, 8050, 51756], "temperature": 0.0, "avg_logprob": -0.2866499320320461, "compression_ratio": 1.2743362831858407, "no_speech_prob": 0.0008426016429439187}, {"id": 388, "seek": 273992, "start": 2740.4, "end": 2742.4, "text": " Uh", "tokens": [50388, 4019, 50488], "temperature": 0.0, "avg_logprob": -0.2727741432189941, "compression_ratio": 1.2926829268292683, "no_speech_prob": 0.000779319612774998}, {"id": 389, "seek": 273992, "start": 2742.7200000000003, "end": 2744.7200000000003, "text": " Does this work?", "tokens": [50504, 4402, 341, 589, 30, 50604], "temperature": 0.0, "avg_logprob": -0.2727741432189941, "compression_ratio": 1.2926829268292683, "no_speech_prob": 0.000779319612774998}, {"id": 390, "seek": 273992, "start": 2745.6, "end": 2751.2000000000003, "text": " Okay, that was absurdly fast. Uh, we should check", "tokens": [50648, 1033, 11, 300, 390, 19774, 356, 2370, 13, 4019, 11, 321, 820, 1520, 50928], "temperature": 0.0, "avg_logprob": -0.2727741432189941, "compression_ratio": 1.2926829268292683, "no_speech_prob": 0.000779319612774998}, {"id": 391, "seek": 273992, "start": 2753.52, "end": 2760.4, "text": " No, okay, this because this does not actually apply them. Uh, we have to load state decked", "tokens": [51044, 883, 11, 1392, 11, 341, 570, 341, 775, 406, 767, 3079, 552, 13, 4019, 11, 321, 362, 281, 3677, 1785, 9341, 292, 51388], "temperature": 0.0, "avg_logprob": -0.2727741432189941, "compression_ratio": 1.2926829268292683, "no_speech_prob": 0.000779319612774998}, {"id": 392, "seek": 276040, "start": 2761.2000000000003, "end": 2766.7200000000003, "text": " Hold state decked is in", "tokens": [50404, 6962, 1785, 9341, 292, 307, 294, 50680], "temperature": 0.0, "avg_logprob": -0.3306782462380149, "compression_ratio": 1.2181818181818183, "no_speech_prob": 0.0033762690145522356}, {"id": 393, "seek": 276040, "start": 2768.48, "end": 2771.12, "text": " And then that state download state decked", "tokens": [50768, 400, 550, 300, 1785, 5484, 1785, 9341, 292, 50900], "temperature": 0.0, "avg_logprob": -0.3306782462380149, "compression_ratio": 1.2181818181818183, "no_speech_prob": 0.0033762690145522356}, {"id": 394, "seek": 276040, "start": 2773.6800000000003, "end": 2777.52, "text": " Okay, assign shape mismatch 496 1 or 2 4", "tokens": [51028, 1033, 11, 6269, 3909, 23220, 852, 16513, 21, 502, 420, 568, 1017, 51220], "temperature": 0.0, "avg_logprob": -0.3306782462380149, "compression_ratio": 1.2181818181818183, "no_speech_prob": 0.0033762690145522356}, {"id": 395, "seek": 276040, "start": 2779.2000000000003, "end": 2781.2000000000003, "text": " Uh", "tokens": [51304, 4019, 51404], "temperature": 0.0, "avg_logprob": -0.3306782462380149, "compression_ratio": 1.2181818181818183, "no_speech_prob": 0.0033762690145522356}, {"id": 396, "seek": 276040, "start": 2784.4, "end": 2786.4, "text": " Which one did I do wrong", "tokens": [51564, 3013, 472, 630, 286, 360, 2085, 51664], "temperature": 0.0, "avg_logprob": -0.3306782462380149, "compression_ratio": 1.2181818181818183, "no_speech_prob": 0.0033762690145522356}, {"id": 397, "seek": 279040, "start": 2790.48, "end": 2792.48, "text": " I", "tokens": [50368, 286, 50468], "temperature": 0.8, "avg_logprob": -0.5214699336460659, "compression_ratio": 0.921875, "no_speech_prob": 0.2141607105731964}, {"id": 398, "seek": 279040, "start": 2815.36, "end": 2819.12, "text": " Um, it's a little annoying that it doesn't print the name", "tokens": [51612, 3301, 11, 309, 311, 257, 707, 11304, 300, 309, 1177, 380, 4482, 264, 1315, 51800], "temperature": 0.8, "avg_logprob": -0.5214699336460659, "compression_ratio": 0.921875, "no_speech_prob": 0.2141607105731964}, {"id": 399, "seek": 281912, "start": 2819.12, "end": 2826.1, "text": " Oh here attention WK wait, okay, so let's see", "tokens": [50364, 876, 510, 3202, 343, 42, 1699, 11, 1392, 11, 370, 718, 311, 536, 50713], "temperature": 0.0, "avg_logprob": -0.498492922101702, "compression_ratio": 1.0649350649350648, "no_speech_prob": 0.5835737586021423}, {"id": 400, "seek": 281912, "start": 2844.4, "end": 2846.4, "text": " Oh, maybe the dim is one or two four", "tokens": [51628, 876, 11, 1310, 264, 5013, 307, 472, 420, 732, 1451, 51728], "temperature": 0.0, "avg_logprob": -0.498492922101702, "compression_ratio": 1.0649350649350648, "no_speech_prob": 0.5835737586021423}, {"id": 401, "seek": 284912, "start": 2850.0, "end": 2853.16, "text": " No, but that definitely changed that", "tokens": [50408, 883, 11, 457, 300, 2138, 3105, 300, 50566], "temperature": 0.0, "avg_logprob": -0.38042979770236546, "compression_ratio": 1.3962264150943395, "no_speech_prob": 0.016646811738610268}, {"id": 402, "seek": 284912, "start": 2855.3599999999997, "end": 2858.92, "text": " We look at the Llama code we can see where that's being created", "tokens": [50676, 492, 574, 412, 264, 32717, 2404, 3089, 321, 393, 536, 689, 300, 311, 885, 2942, 50854], "temperature": 0.0, "avg_logprob": -0.38042979770236546, "compression_ratio": 1.3962264150943395, "no_speech_prob": 0.016646811738610268}, {"id": 403, "seek": 284912, "start": 2861.52, "end": 2864.14, "text": " And heads times head dim", "tokens": [50984, 400, 8050, 1413, 1378, 5013, 51115], "temperature": 0.0, "avg_logprob": -0.38042979770236546, "compression_ratio": 1.3962264150943395, "no_speech_prob": 0.016646811738610268}, {"id": 404, "seek": 284912, "start": 2866.7599999999998, "end": 2870.2, "text": " Did I get the number of heads roll? Oh, that's probably right", "tokens": [51246, 2589, 286, 483, 264, 1230, 295, 8050, 3373, 30, 876, 11, 300, 311, 1391, 558, 51418], "temperature": 0.0, "avg_logprob": -0.38042979770236546, "compression_ratio": 1.3962264150943395, "no_speech_prob": 0.016646811738610268}, {"id": 405, "seek": 284912, "start": 2873.2, "end": 2875.7, "text": " Okay, so and heads is maybe not 32", "tokens": [51568, 1033, 11, 370, 293, 8050, 307, 1310, 406, 8858, 51693], "temperature": 0.0, "avg_logprob": -0.38042979770236546, "compression_ratio": 1.3962264150943395, "no_speech_prob": 0.016646811738610268}, {"id": 406, "seek": 287570, "start": 2876.14, "end": 2883.4199999999996, "text": " Number of attention heads is 32, but the number of KV heads is only eight", "tokens": [50386, 5118, 295, 3202, 8050, 307, 8858, 11, 457, 264, 1230, 295, 591, 53, 8050, 307, 787, 3180, 50750], "temperature": 0.0, "avg_logprob": -0.3443862022237575, "compression_ratio": 1.3858267716535433, "no_speech_prob": 0.0014777943724766374}, {"id": 407, "seek": 287570, "start": 2885.4199999999996, "end": 2887.4199999999996, "text": " Let's try eight", "tokens": [50850, 961, 311, 853, 3180, 50950], "temperature": 0.0, "avg_logprob": -0.3443862022237575, "compression_ratio": 1.3858267716535433, "no_speech_prob": 0.0014777943724766374}, {"id": 408, "seek": 287570, "start": 2897.2999999999997, "end": 2902.8999999999996, "text": " What is this multiple of them I wrote this I'm sure just copied this from other people", "tokens": [51444, 708, 307, 341, 3866, 295, 552, 286, 4114, 341, 286, 478, 988, 445, 25365, 341, 490, 661, 561, 51724], "temperature": 0.0, "avg_logprob": -0.3443862022237575, "compression_ratio": 1.3858267716535433, "no_speech_prob": 0.0014777943724766374}, {"id": 409, "seek": 290290, "start": 2903.58, "end": 2909.9, "text": " It's always good to understand I said this should be four times dim. I don't understand", "tokens": [50398, 467, 311, 1009, 665, 281, 1223, 286, 848, 341, 820, 312, 1451, 1413, 5013, 13, 286, 500, 380, 1223, 50714], "temperature": 0.0, "avg_logprob": -0.22501633132713428, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0025505581870675087}, {"id": 410, "seek": 290290, "start": 2910.86, "end": 2914.5, "text": " Why that doesn't match I would actually expect it to be that", "tokens": [50762, 1545, 300, 1177, 380, 2995, 286, 576, 767, 2066, 309, 281, 312, 300, 50944], "temperature": 0.0, "avg_logprob": -0.22501633132713428, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0025505581870675087}, {"id": 411, "seek": 290290, "start": 2919.94, "end": 2923.26, "text": " Mitchell uses 32 heads for the query and for and", "tokens": [51216, 27582, 4960, 8858, 8050, 337, 264, 14581, 293, 337, 293, 51382], "temperature": 0.0, "avg_logprob": -0.22501633132713428, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0025505581870675087}, {"id": 412, "seek": 290290, "start": 2924.38, "end": 2928.6600000000003, "text": " Yeah, okay, but which one of these is supposed to be the KV heads", "tokens": [51438, 865, 11, 1392, 11, 457, 597, 472, 295, 613, 307, 3442, 281, 312, 264, 591, 53, 8050, 51652], "temperature": 0.0, "avg_logprob": -0.22501633132713428, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0025505581870675087}, {"id": 413, "seek": 292866, "start": 2929.2599999999998, "end": 2933.8599999999997, "text": " Right, where's my convert from hugging face wrong or", "tokens": [50394, 1779, 11, 689, 311, 452, 7620, 490, 41706, 1851, 2085, 420, 50624], "temperature": 0.0, "avg_logprob": -0.4898929249156605, "compression_ratio": 1.3594771241830066, "no_speech_prob": 0.0028007400687783957}, {"id": 414, "seek": 292866, "start": 2936.1, "end": 2944.2799999999997, "text": " Does our model not support that now it should well, let's just Llama not do this does Llama do something different", "tokens": [50736, 4402, 527, 2316, 406, 1406, 300, 586, 309, 820, 731, 11, 718, 311, 445, 32717, 2404, 406, 360, 341, 775, 32717, 2404, 360, 746, 819, 51145], "temperature": 0.0, "avg_logprob": -0.4898929249156605, "compression_ratio": 1.3594771241830066, "no_speech_prob": 0.0028007400687783957}, {"id": 415, "seek": 292866, "start": 2948.3799999999997, "end": 2952.92, "text": " And KV heads wait, what about the prodge", "tokens": [51350, 400, 591, 53, 8050, 1699, 11, 437, 466, 264, 15792, 432, 51577], "temperature": 0.0, "avg_logprob": -0.4898929249156605, "compression_ratio": 1.3594771241830066, "no_speech_prob": 0.0028007400687783957}, {"id": 416, "seek": 295292, "start": 2953.48, "end": 2958.28, "text": " The alarm is different. Oh, okay", "tokens": [50392, 440, 14183, 307, 819, 13, 876, 11, 1392, 50632], "temperature": 0.0, "avg_logprob": -0.400814950466156, "compression_ratio": 1.125, "no_speech_prob": 0.002323051681742072}, {"id": 417, "seek": 295292, "start": 2960.4, "end": 2962.4, "text": " Wait now here attention", "tokens": [50738, 3802, 586, 510, 3202, 50838], "temperature": 0.0, "avg_logprob": -0.400814950466156, "compression_ratio": 1.125, "no_speech_prob": 0.002323051681742072}, {"id": 418, "seek": 295292, "start": 2963.88, "end": 2966.56, "text": " But is it just the feed forward have to change", "tokens": [50912, 583, 307, 309, 445, 264, 3154, 2128, 362, 281, 1319, 51046], "temperature": 0.0, "avg_logprob": -0.400814950466156, "compression_ratio": 1.125, "no_speech_prob": 0.002323051681742072}, {"id": 419, "seek": 295292, "start": 2970.08, "end": 2972.08, "text": " Okay", "tokens": [51222, 1033, 51322], "temperature": 0.0, "avg_logprob": -0.400814950466156, "compression_ratio": 1.125, "no_speech_prob": 0.002323051681742072}, {"id": 420, "seek": 298292, "start": 2983.92, "end": 2991.56, "text": " It's failing on the attention assignment", "tokens": [50414, 467, 311, 18223, 322, 264, 3202, 15187, 50796], "temperature": 0.0, "avg_logprob": -0.27245637847156057, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0006263154209591448}, {"id": 421, "seek": 298292, "start": 2996.48, "end": 2999.64, "text": " Okay, so I can pass an NKV heads here to attention", "tokens": [51042, 1033, 11, 370, 286, 393, 1320, 364, 426, 42, 53, 8050, 510, 281, 3202, 51200], "temperature": 0.0, "avg_logprob": -0.27245637847156057, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0006263154209591448}, {"id": 422, "seek": 298292, "start": 3000.32, "end": 3003.84, "text": " Again, I must have just copied all this so NKV heads is here", "tokens": [51234, 3764, 11, 286, 1633, 362, 445, 25365, 439, 341, 370, 426, 42, 53, 8050, 307, 510, 51410], "temperature": 0.0, "avg_logprob": -0.27245637847156057, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0006263154209591448}, {"id": 423, "seek": 298292, "start": 3004.36, "end": 3009.28, "text": " How far do I pipe it down? Oh NKV heads is here. Where does it come from? Oh?", "tokens": [51436, 1012, 1400, 360, 286, 11240, 309, 760, 30, 876, 426, 42, 53, 8050, 307, 510, 13, 2305, 775, 309, 808, 490, 30, 876, 30, 51682], "temperature": 0.0, "avg_logprob": -0.27245637847156057, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0006263154209591448}, {"id": 424, "seek": 298292, "start": 3010.04, "end": 3012.16, "text": " There's a there's a named argument. Okay", "tokens": [51720, 821, 311, 257, 456, 311, 257, 4926, 6770, 13, 1033, 51826], "temperature": 0.0, "avg_logprob": -0.27245637847156057, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0006263154209591448}, {"id": 425, "seek": 301292, "start": 3013.32, "end": 3015.32, "text": " So we just need to add", "tokens": [50384, 407, 321, 445, 643, 281, 909, 50484], "temperature": 0.0, "avg_logprob": -0.27773535879034744, "compression_ratio": 1.392156862745098, "no_speech_prob": 0.0006166023085825145}, {"id": 426, "seek": 301292, "start": 3017.36, "end": 3019.36, "text": " NKV heads equals eight", "tokens": [50586, 426, 42, 53, 8050, 6915, 3180, 50686], "temperature": 0.0, "avg_logprob": -0.27773535879034744, "compression_ratio": 1.392156862745098, "no_speech_prob": 0.0006166023085825145}, {"id": 427, "seek": 301292, "start": 3022.36, "end": 3023.92, "text": " Okay", "tokens": [50836, 1033, 50914], "temperature": 0.0, "avg_logprob": -0.27773535879034744, "compression_ratio": 1.392156862745098, "no_speech_prob": 0.0006166023085825145}, {"id": 428, "seek": 301292, "start": 3023.92, "end": 3028.52, "text": " That's progress this probably has to do with the multiply not being right", "tokens": [50914, 663, 311, 4205, 341, 1391, 575, 281, 360, 365, 264, 12972, 406, 885, 558, 51144], "temperature": 0.0, "avg_logprob": -0.27773535879034744, "compression_ratio": 1.392156862745098, "no_speech_prob": 0.0006166023085825145}, {"id": 429, "seek": 301292, "start": 3029.76, "end": 3031.76, "text": " Seen those numbers before", "tokens": [51206, 1100, 268, 729, 3547, 949, 51306], "temperature": 0.0, "avg_logprob": -0.27773535879034744, "compression_ratio": 1.392156862745098, "no_speech_prob": 0.0006166023085825145}, {"id": 430, "seek": 301292, "start": 3034.84, "end": 3041.16, "text": " So where is their multiple it's not 256 intermediate size here", "tokens": [51460, 407, 689, 307, 641, 3866, 309, 311, 406, 38882, 19376, 2744, 510, 51776], "temperature": 0.0, "avg_logprob": -0.27773535879034744, "compression_ratio": 1.392156862745098, "no_speech_prob": 0.0006166023085825145}, {"id": 431, "seek": 304292, "start": 3042.92, "end": 3044.92, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.3928587887738202, "compression_ratio": 1.1523809523809523, "no_speech_prob": 0.0008167015039362013}, {"id": 432, "seek": 304292, "start": 3051.2400000000002, "end": 3054.12, "text": " Custom FFN dim multiplier", "tokens": [50780, 16649, 479, 37, 45, 5013, 44106, 50924], "temperature": 0.0, "avg_logprob": -0.3928587887738202, "compression_ratio": 1.1523809523809523, "no_speech_prob": 0.0008167015039362013}, {"id": 433, "seek": 304292, "start": 3061.36, "end": 3063.36, "text": " I don't think this was written correctly", "tokens": [51286, 286, 500, 380, 519, 341, 390, 3720, 8944, 51386], "temperature": 0.0, "avg_logprob": -0.3928587887738202, "compression_ratio": 1.1523809523809523, "no_speech_prob": 0.0008167015039362013}, {"id": 434, "seek": 304292, "start": 3065.64, "end": 3068.16, "text": " So get this code and see if that's how they wrote it", "tokens": [51500, 407, 483, 341, 3089, 293, 536, 498, 300, 311, 577, 436, 4114, 309, 51626], "temperature": 0.0, "avg_logprob": -0.3928587887738202, "compression_ratio": 1.1523809523809523, "no_speech_prob": 0.0008167015039362013}, {"id": 435, "seek": 307292, "start": 3073.76, "end": 3078.0, "text": " Oh, they just have something weird called model arcs", "tokens": [50406, 876, 11, 436, 445, 362, 746, 3657, 1219, 2316, 10346, 82, 50618], "temperature": 0.0, "avg_logprob": -0.3913990226951805, "compression_ratio": 1.233009708737864, "no_speech_prob": 0.0006166014354676008}, {"id": 436, "seek": 307292, "start": 3084.88, "end": 3090.52, "text": " Yeah, they have just args hidden dim if this is stupid", "tokens": [50962, 865, 11, 436, 362, 445, 3882, 82, 7633, 5013, 498, 341, 307, 6631, 51244], "temperature": 0.0, "avg_logprob": -0.3913990226951805, "compression_ratio": 1.233009708737864, "no_speech_prob": 0.0006166014354676008}, {"id": 437, "seek": 307292, "start": 3100.36, "end": 3102.36, "text": " To do what is this?", "tokens": [51736, 1407, 360, 437, 307, 341, 30, 51836], "temperature": 0.0, "avg_logprob": -0.3913990226951805, "compression_ratio": 1.233009708737864, "no_speech_prob": 0.0006166014354676008}, {"id": 438, "seek": 310292, "start": 3103.84, "end": 3111.52, "text": " Yeah, what is all this crap? Why is this four times dim? Why don't I just", "tokens": [50410, 865, 11, 437, 307, 439, 341, 12426, 30, 1545, 307, 341, 1451, 1413, 5013, 30, 1545, 500, 380, 286, 445, 50794], "temperature": 0.0, "avg_logprob": -0.4154055913289388, "compression_ratio": 1.042857142857143, "no_speech_prob": 0.0003799548139795661}, {"id": 439, "seek": 311152, "start": 3111.52, "end": 3136.96, "text": " Yeah, I don't like multiple of FFM dim multiplier, this is set to 1.3 like this should just be hidden this should just be hidden dim", "tokens": [50364, 865, 11, 286, 500, 380, 411, 3866, 295, 479, 37, 44, 5013, 44106, 11, 341, 307, 992, 281, 502, 13, 18, 411, 341, 820, 445, 312, 7633, 341, 820, 445, 312, 7633, 5013, 51636], "temperature": 0.0, "avg_logprob": -0.2572229703267415, "compression_ratio": 1.3894736842105264, "no_speech_prob": 0.0038239595014601946}, {"id": 440, "seek": 314152, "start": 3142.52, "end": 3146.64, "text": " So I pass in really I pass in hidden dim and multiple of", "tokens": [50414, 407, 286, 1320, 294, 534, 286, 1320, 294, 7633, 5013, 293, 3866, 295, 50620], "temperature": 0.0, "avg_logprob": -0.37919660893882196, "compression_ratio": 1.2201834862385321, "no_speech_prob": 0.002396556083112955}, {"id": 441, "seek": 314152, "start": 3147.88, "end": 3149.2, "text": " What?", "tokens": [50682, 708, 30, 50748], "temperature": 0.0, "avg_logprob": -0.37919660893882196, "compression_ratio": 1.2201834862385321, "no_speech_prob": 0.002396556083112955}, {"id": 442, "seek": 314152, "start": 3149.2, "end": 3151.36, "text": " Okay, they're refactoring this", "tokens": [50748, 1033, 11, 436, 434, 1895, 578, 3662, 341, 50856], "temperature": 0.0, "avg_logprob": -0.37919660893882196, "compression_ratio": 1.2201834862385321, "no_speech_prob": 0.002396556083112955}, {"id": 443, "seek": 314152, "start": 3152.52, "end": 3155.04, "text": " We have to keep the old stupid behavior", "tokens": [50914, 492, 362, 281, 1066, 264, 1331, 6631, 5223, 51040], "temperature": 0.0, "avg_logprob": -0.37919660893882196, "compression_ratio": 1.2201834862385321, "no_speech_prob": 0.002396556083112955}, {"id": 444, "seek": 315504, "start": 3155.04, "end": 3157.04, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.745928244157271, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.0132214380428195}, {"id": 445, "seek": 315504, "start": 3168.24, "end": 3170.7599999999998, "text": " Copy this weird crap", "tokens": [51024, 25653, 341, 3657, 12426, 51150], "temperature": 0.0, "avg_logprob": -0.745928244157271, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.0132214380428195}, {"id": 446, "seek": 317076, "start": 3170.76, "end": 3172.76, "text": " Here", "tokens": [50364, 1692, 50464], "temperature": 0.0, "avg_logprob": -0.6344799173289332, "compression_ratio": 1.2236842105263157, "no_speech_prob": 0.02128281071782112}, {"id": 447, "seek": 317076, "start": 3182.1600000000003, "end": 3186.32, "text": " Do we even have we don't have hidden dim so I have to add hidden dim to the programs", "tokens": [50934, 1144, 321, 754, 362, 321, 500, 380, 362, 7633, 5013, 370, 286, 362, 281, 909, 7633, 5013, 281, 264, 4268, 51142], "temperature": 0.0, "avg_logprob": -0.6344799173289332, "compression_ratio": 1.2236842105263157, "no_speech_prob": 0.02128281071782112}, {"id": 448, "seek": 317076, "start": 3195.5600000000004, "end": 3197.5600000000004, "text": " And", "tokens": [51604, 400, 51704], "temperature": 0.0, "avg_logprob": -0.6344799173289332, "compression_ratio": 1.2236842105263157, "no_speech_prob": 0.02128281071782112}, {"id": 449, "seek": 320076, "start": 3201.0, "end": 3203.0, "text": " Model arcs", "tokens": [50376, 17105, 10346, 82, 50476], "temperature": 0.0, "avg_logprob": -0.30861977969898896, "compression_ratio": 1.419889502762431, "no_speech_prob": 0.004981902427971363}, {"id": 450, "seek": 320076, "start": 3205.0, "end": 3207.0, "text": " We'll make sure we didn't break it later", "tokens": [50576, 492, 603, 652, 988, 321, 994, 380, 1821, 309, 1780, 50676], "temperature": 0.0, "avg_logprob": -0.30861977969898896, "compression_ratio": 1.419889502762431, "no_speech_prob": 0.004981902427971363}, {"id": 451, "seek": 320076, "start": 3208.32, "end": 3213.5600000000004, "text": " This is what happens. This is the problem with open source people add crap to my repo and don't think", "tokens": [50742, 639, 307, 437, 2314, 13, 639, 307, 264, 1154, 365, 1269, 4009, 561, 909, 12426, 281, 452, 49040, 293, 500, 380, 519, 51004], "temperature": 0.0, "avg_logprob": -0.30861977969898896, "compression_ratio": 1.419889502762431, "no_speech_prob": 0.004981902427971363}, {"id": 452, "seek": 320076, "start": 3222.0400000000004, "end": 3226.5600000000004, "text": " Okay, now we're passing in an argument called hidden dim this is much more sensible", "tokens": [51428, 1033, 11, 586, 321, 434, 8437, 294, 364, 6770, 1219, 7633, 5013, 341, 307, 709, 544, 25380, 51654], "temperature": 0.0, "avg_logprob": -0.30861977969898896, "compression_ratio": 1.419889502762431, "no_speech_prob": 0.004981902427971363}, {"id": 453, "seek": 320076, "start": 3227.8, "end": 3229.8, "text": " Get rid of multiple", "tokens": [51716, 3240, 3973, 295, 3866, 51816], "temperature": 0.0, "avg_logprob": -0.30861977969898896, "compression_ratio": 1.419889502762431, "no_speech_prob": 0.004981902427971363}, {"id": 454, "seek": 322980, "start": 3230.1200000000003, "end": 3232.1200000000003, "text": " of", "tokens": [50380, 295, 50480], "temperature": 0.0, "avg_logprob": -0.44906151600373095, "compression_ratio": 1.4114583333333333, "no_speech_prob": 0.010985114611685276}, {"id": 455, "seek": 322980, "start": 3232.36, "end": 3234.36, "text": " Hidden dim can go here", "tokens": [50492, 41156, 5013, 393, 352, 510, 50592], "temperature": 0.0, "avg_logprob": -0.44906151600373095, "compression_ratio": 1.4114583333333333, "no_speech_prob": 0.010985114611685276}, {"id": 456, "seek": 322980, "start": 3236.52, "end": 3238.52, "text": " Multiple of", "tokens": [50700, 40056, 295, 50800], "temperature": 0.0, "avg_logprob": -0.44906151600373095, "compression_ratio": 1.4114583333333333, "no_speech_prob": 0.010985114611685276}, {"id": 457, "seek": 322980, "start": 3239.1600000000003, "end": 3241.1600000000003, "text": " Dimm goes here", "tokens": [50832, 413, 6753, 1709, 510, 50932], "temperature": 0.0, "avg_logprob": -0.44906151600373095, "compression_ratio": 1.4114583333333333, "no_speech_prob": 0.010985114611685276}, {"id": 458, "seek": 322980, "start": 3243.0, "end": 3245.0, "text": " That's fine", "tokens": [51024, 663, 311, 2489, 51124], "temperature": 0.0, "avg_logprob": -0.44906151600373095, "compression_ratio": 1.4114583333333333, "no_speech_prob": 0.010985114611685276}, {"id": 459, "seek": 322980, "start": 3247.6800000000003, "end": 3253.44, "text": " Where is that actually being passed them? Oh that just goes straight to linear. Okay. Good. We already do that crap", "tokens": [51258, 2305, 307, 300, 767, 885, 4678, 552, 30, 876, 300, 445, 1709, 2997, 281, 8213, 13, 1033, 13, 2205, 13, 492, 1217, 360, 300, 12426, 51546], "temperature": 0.0, "avg_logprob": -0.44906151600373095, "compression_ratio": 1.4114583333333333, "no_speech_prob": 0.010985114611685276}, {"id": 460, "seek": 322980, "start": 3253.6000000000004, "end": 3258.0, "text": " Wow, that's a much more sensible feed forward. I don't know why we put that logic in there", "tokens": [51554, 3153, 11, 300, 311, 257, 709, 544, 25380, 3154, 2128, 13, 286, 500, 380, 458, 983, 321, 829, 300, 9952, 294, 456, 51774], "temperature": 0.0, "avg_logprob": -0.44906151600373095, "compression_ratio": 1.4114583333333333, "no_speech_prob": 0.010985114611685276}, {"id": 461, "seek": 325800, "start": 3258.8, "end": 3260.8, "text": " Okay", "tokens": [50404, 1033, 50504], "temperature": 0.0, "avg_logprob": -0.3874529202779134, "compression_ratio": 1.2386363636363635, "no_speech_prob": 0.007937206886708736}, {"id": 462, "seek": 325800, "start": 3276.64, "end": 3281.48, "text": " All right, good now we just pass the hidden dim in right there and what was the hidden dim from Estrell?", "tokens": [51296, 1057, 558, 11, 665, 586, 321, 445, 1320, 264, 7633, 5013, 294, 558, 456, 293, 437, 390, 264, 7633, 5013, 490, 4410, 19771, 30, 51538], "temperature": 0.0, "avg_logprob": -0.3874529202779134, "compression_ratio": 1.2386363636363635, "no_speech_prob": 0.007937206886708736}, {"id": 463, "seek": 328800, "start": 3288.4, "end": 3290.4, "text": " Okay", "tokens": [50384, 1033, 50484], "temperature": 0.0, "avg_logprob": -0.44081665813059046, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.005301472265273333}, {"id": 464, "seek": 328800, "start": 3290.68, "end": 3297.72, "text": " Now that's and heads wait, what was that 256 is what gets deleted. I should name some of these parameters", "tokens": [50498, 823, 300, 311, 293, 8050, 1699, 11, 437, 390, 300, 38882, 307, 437, 2170, 22981, 13, 286, 820, 1315, 512, 295, 613, 9834, 50850], "temperature": 0.0, "avg_logprob": -0.44081665813059046, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.005301472265273333}, {"id": 465, "seek": 328800, "start": 3298.64, "end": 3303.12, "text": " That's NKV heads. That's the vocab size. That's", "tokens": [50896, 663, 311, 426, 42, 53, 8050, 13, 663, 311, 264, 2329, 455, 2744, 13, 663, 311, 51120], "temperature": 0.0, "avg_logprob": -0.44081665813059046, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.005301472265273333}, {"id": 466, "seek": 328800, "start": 3304.88, "end": 3308.16, "text": " Not rope theta. What's it called norm EPS?", "tokens": [51208, 1726, 13540, 9725, 13, 708, 311, 309, 1219, 2026, 462, 6273, 30, 51372], "temperature": 0.0, "avg_logprob": -0.44081665813059046, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.005301472265273333}, {"id": 467, "seek": 328800, "start": 3312.04, "end": 3314.04, "text": " And layers", "tokens": [51566, 400, 7914, 51666], "temperature": 0.0, "avg_logprob": -0.44081665813059046, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.005301472265273333}, {"id": 468, "seek": 328800, "start": 3315.36, "end": 3317.36, "text": " And heads", "tokens": [51732, 400, 8050, 51832], "temperature": 0.0, "avg_logprob": -0.44081665813059046, "compression_ratio": 1.4050632911392404, "no_speech_prob": 0.005301472265273333}, {"id": 469, "seek": 331800, "start": 3318.08, "end": 3321.76, "text": " Does it load? Yeah, it loads very slowly", "tokens": [50368, 4402, 309, 3677, 30, 865, 11, 309, 12668, 588, 5692, 50552], "temperature": 0.0, "avg_logprob": -0.4339571680341448, "compression_ratio": 1.1805555555555556, "no_speech_prob": 0.0010483661899343133}, {"id": 470, "seek": 331800, "start": 3324.96, "end": 3328.36, "text": " Oh, this is unbearable. Oh, this is terrible", "tokens": [50712, 876, 11, 341, 307, 517, 26738, 712, 13, 876, 11, 341, 307, 6237, 50882], "temperature": 0.0, "avg_logprob": -0.4339571680341448, "compression_ratio": 1.1805555555555556, "no_speech_prob": 0.0010483661899343133}, {"id": 471, "seek": 334800, "start": 3348.88, "end": 3352.16, "text": " Okay, the reason it's slow is because we haven't finished the", "tokens": [50408, 1033, 11, 264, 1778, 309, 311, 2964, 307, 570, 321, 2378, 380, 4335, 264, 50572], "temperature": 0.0, "avg_logprob": -0.3816420758356813, "compression_ratio": 1.3602941176470589, "no_speech_prob": 0.013847284018993378}, {"id": 472, "seek": 334800, "start": 3353.68, "end": 3355.68, "text": " Takes 20 seconds", "tokens": [50648, 44347, 945, 3949, 50748], "temperature": 0.0, "avg_logprob": -0.3816420758356813, "compression_ratio": 1.3602941176470589, "no_speech_prob": 0.013847284018993378}, {"id": 473, "seek": 334800, "start": 3356.4, "end": 3359.64, "text": " Oh, come on boys. I don't have all day. Oh", "tokens": [50784, 876, 11, 808, 322, 6347, 13, 286, 500, 380, 362, 439, 786, 13, 876, 50946], "temperature": 0.0, "avg_logprob": -0.3816420758356813, "compression_ratio": 1.3602941176470589, "no_speech_prob": 0.013847284018993378}, {"id": 474, "seek": 334800, "start": 3360.72, "end": 3362.72, "text": " It's cuz we haven't finished", "tokens": [51000, 467, 311, 11910, 321, 2378, 380, 4335, 51100], "temperature": 0.0, "avg_logprob": -0.3816420758356813, "compression_ratio": 1.3602941176470589, "no_speech_prob": 0.013847284018993378}, {"id": 475, "seek": 334800, "start": 3363.12, "end": 3365.38, "text": " No, I'm using the it's not the GPU", "tokens": [51120, 883, 11, 286, 478, 1228, 264, 309, 311, 406, 264, 18407, 51233], "temperature": 0.0, "avg_logprob": -0.3816420758356813, "compression_ratio": 1.3602941176470589, "no_speech_prob": 0.013847284018993378}, {"id": 476, "seek": 336538, "start": 3365.38, "end": 3367.38, "text": " Oh", "tokens": [50364, 876, 50464], "temperature": 0.0, "avg_logprob": -0.2659101680833466, "compression_ratio": 1.16, "no_speech_prob": 0.005641589872539043}, {"id": 477, "seek": 336538, "start": 3377.38, "end": 3383.26, "text": " It's not the GPU it's that I have I'm converting to the CPU to load BF 16. Oh", "tokens": [50964, 467, 311, 406, 264, 18407, 309, 311, 300, 286, 362, 286, 478, 29942, 281, 264, 13199, 281, 3677, 363, 37, 3165, 13, 876, 51258], "temperature": 0.0, "avg_logprob": -0.2659101680833466, "compression_ratio": 1.16, "no_speech_prob": 0.005641589872539043}, {"id": 478, "seek": 336538, "start": 3385.5, "end": 3387.5, "text": " Can't wait 20 seconds every time", "tokens": [51370, 1664, 380, 1699, 945, 3949, 633, 565, 51470], "temperature": 0.0, "avg_logprob": -0.2659101680833466, "compression_ratio": 1.16, "no_speech_prob": 0.005641589872539043}, {"id": 479, "seek": 336538, "start": 3390.82, "end": 3394.7400000000002, "text": " Well, what can I do about that?", "tokens": [51636, 1042, 11, 437, 393, 286, 360, 466, 300, 30, 51832], "temperature": 0.0, "avg_logprob": -0.2659101680833466, "compression_ratio": 1.16, "no_speech_prob": 0.005641589872539043}, {"id": 480, "seek": 339538, "start": 3395.38, "end": 3398.1, "text": " Oh, I don't know", "tokens": [50400, 876, 11, 286, 500, 380, 458, 50500], "temperature": 0.0, "avg_logprob": -0.5491379631890191, "compression_ratio": 0.6666666666666666, "no_speech_prob": 0.009684104472398758}, {"id": 481, "seek": 342538, "start": 3425.38, "end": 3427.38, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.3304707209269206, "compression_ratio": 1.0337078651685394, "no_speech_prob": 0.004069593269377947}, {"id": 482, "seek": 342538, "start": 3447.1, "end": 3452.1800000000003, "text": " Think yeah, cuz we convert them all to float 16 and that's what's taking forever right now", "tokens": [51450, 6557, 1338, 11, 11910, 321, 7620, 552, 439, 281, 15706, 3165, 293, 300, 311, 437, 311, 1940, 5680, 558, 586, 51704], "temperature": 0.0, "avg_logprob": -0.3304707209269206, "compression_ratio": 1.0337078651685394, "no_speech_prob": 0.004069593269377947}, {"id": 483, "seek": 345218, "start": 3452.8199999999997, "end": 3458.44, "text": " Because we're not actually doing we could actually also do the math in B-float 16", "tokens": [50396, 1436, 321, 434, 406, 767, 884, 321, 727, 767, 611, 360, 264, 5221, 294, 363, 12, 43645, 267, 3165, 50677], "temperature": 0.0, "avg_logprob": -0.29334842457490806, "compression_ratio": 1.1782178217821782, "no_speech_prob": 0.00069862708915025}, {"id": 484, "seek": 345218, "start": 3465.14, "end": 3467.3799999999997, "text": " Mac doesn't use RAM for temp does it?", "tokens": [51012, 5707, 1177, 380, 764, 14561, 337, 18274, 775, 309, 30, 51124], "temperature": 0.0, "avg_logprob": -0.29334842457490806, "compression_ratio": 1.1782178217821782, "no_speech_prob": 0.00069862708915025}, {"id": 485, "seek": 348218, "start": 3483.18, "end": 3485.18, "text": " Cashed mystery", "tokens": [50414, 383, 12219, 11422, 50514], "temperature": 0.0, "avg_logprob": -0.5497390083644701, "compression_ratio": 0.9090909090909091, "no_speech_prob": 0.006289157550781965}, {"id": 486, "seek": 348218, "start": 3488.74, "end": 3493.22, "text": " Okay, 13 G's now, let's see how fast it loads", "tokens": [50692, 1033, 11, 3705, 460, 311, 586, 11, 718, 311, 536, 577, 2370, 309, 12668, 50916], "temperature": 0.0, "avg_logprob": -0.5497390083644701, "compression_ratio": 0.9090909090909091, "no_speech_prob": 0.006289157550781965}, {"id": 487, "seek": 349322, "start": 3493.22, "end": 3495.22, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.32777378559112547, "compression_ratio": 0.9310344827586207, "no_speech_prob": 0.052602045238018036}, {"id": 488, "seek": 349322, "start": 3512.18, "end": 3516.58, "text": " Have a way to like put things in I forget what it is", "tokens": [51312, 3560, 257, 636, 281, 411, 829, 721, 294, 286, 2870, 437, 309, 307, 51532], "temperature": 0.0, "avg_logprob": -0.32777378559112547, "compression_ratio": 0.9310344827586207, "no_speech_prob": 0.052602045238018036}, {"id": 489, "seek": 351658, "start": 3517.42, "end": 3522.62, "text": " It's called loads data", "tokens": [50406, 467, 311, 1219, 12668, 1412, 50666], "temperature": 1.0, "avg_logprob": -1.2701975504557292, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.008576061576604843}, {"id": 490, "seek": 352262, "start": 3522.62, "end": 3546.98, "text": " Yeah, that's the same load state I'm using up there.", "tokens": [50364, 865, 11, 300, 311, 264, 912, 3677, 1785, 286, 478, 1228, 493, 456, 13, 51582], "temperature": 0.0, "avg_logprob": -0.5122423701816134, "compression_ratio": 0.9285714285714286, "no_speech_prob": 0.12758228182792664}, {"id": 491, "seek": 354698, "start": 3546.98, "end": 3552.46, "text": " Strict should be okay. Okay, 2.3 seconds. Great.", "tokens": [50364, 745, 3740, 820, 312, 1392, 13, 1033, 11, 568, 13, 18, 3949, 13, 3769, 13, 50638], "temperature": 0.0, "avg_logprob": -0.7887813668502005, "compression_ratio": 0.8888888888888888, "no_speech_prob": 0.4067584276199341}, {"id": 492, "seek": 355246, "start": 3552.46, "end": 3558.46, "text": " All right, now it's inference time.", "tokens": [50414, 1057, 558, 11, 586, 309, 311, 38253, 565, 13, 50664], "temperature": 0.0, "avg_logprob": -0.4710981051127116, "compression_ratio": 0.813953488372093, "no_speech_prob": 0.24397990107536316}, {"id": 493, "seek": 358246, "start": 3583.46, "end": 3588.46, "text": " Create a branch so I accidentally push the master.", "tokens": [50414, 20248, 257, 9819, 370, 286, 15715, 2944, 264, 4505, 13, 50664], "temperature": 0.0, "avg_logprob": -0.30560834067208426, "compression_ratio": 1.1584158415841583, "no_speech_prob": 0.08258800953626633}, {"id": 494, "seek": 358246, "start": 3601.46, "end": 3606.46, "text": " All right, we're going to need a tokenizer. Let's get a tokenizer.", "tokens": [51314, 1057, 558, 11, 321, 434, 516, 281, 643, 257, 14862, 6545, 13, 961, 311, 483, 257, 14862, 6545, 13, 51564], "temperature": 0.0, "avg_logprob": -0.30560834067208426, "compression_ratio": 1.1584158415841583, "no_speech_prob": 0.08258800953626633}, {"id": 495, "seek": 360646, "start": 3607.46, "end": 3611.46, "text": " Oh, I also don't even need this anymore.", "tokens": [50414, 876, 11, 286, 611, 500, 380, 754, 643, 341, 3602, 13, 50614], "temperature": 0.0, "avg_logprob": -0.27859403776085895, "compression_ratio": 1.1932773109243697, "no_speech_prob": 0.007693319581449032}, {"id": 496, "seek": 360646, "start": 3616.46, "end": 3618.46, "text": " Now I've made a copy.", "tokens": [50864, 823, 286, 600, 1027, 257, 5055, 13, 50964], "temperature": 0.0, "avg_logprob": -0.27859403776085895, "compression_ratio": 1.1932773109243697, "no_speech_prob": 0.007693319581449032}, {"id": 497, "seek": 360646, "start": 3621.46, "end": 3626.46, "text": " Yeah, I guess I do need model. Actually, not really. I could just put whatever.", "tokens": [51114, 865, 11, 286, 2041, 286, 360, 643, 2316, 13, 5135, 11, 406, 534, 13, 286, 727, 445, 829, 2035, 13, 51364], "temperature": 0.0, "avg_logprob": -0.27859403776085895, "compression_ratio": 1.1932773109243697, "no_speech_prob": 0.007693319581449032}, {"id": 498, "seek": 362646, "start": 3627.46, "end": 3628.46, "text": " Um,", "tokens": [50414, 3301, 11, 50464], "temperature": 0.0, "avg_logprob": -0.39731163727609736, "compression_ratio": 1.069767441860465, "no_speech_prob": 0.06555159389972687}, {"id": 499, "seek": 362646, "start": 3632.46, "end": 3635.46, "text": " tokenizer over here.", "tokens": [50664, 14862, 6545, 670, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.39731163727609736, "compression_ratio": 1.069767441860465, "no_speech_prob": 0.06555159389972687}, {"id": 500, "seek": 362646, "start": 3649.46, "end": 3651.46, "text": " tokenizer that model.", "tokens": [51514, 14862, 6545, 300, 2316, 13, 51614], "temperature": 0.0, "avg_logprob": -0.39731163727609736, "compression_ratio": 1.069767441860465, "no_speech_prob": 0.06555159389972687}, {"id": 501, "seek": 365646, "start": 3656.46, "end": 3682.46, "text": " Wow, 14. Wow, 6.47 gigabytes per second.", "tokens": [50364, 3153, 11, 3499, 13, 3153, 11, 1386, 13, 14060, 42741, 680, 1150, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3943955197053797, "compression_ratio": 0.8888888888888888, "no_speech_prob": 0.0812482163310051}, {"id": 502, "seek": 368246, "start": 3683.46, "end": 3690.46, "text": " What does Jimmy apples do? What is this?", "tokens": [50414, 708, 775, 15709, 16814, 360, 30, 708, 307, 341, 30, 50764], "temperature": 0.0, "avg_logprob": -0.2823713541030884, "compression_ratio": 0.9464285714285714, "no_speech_prob": 0.00708377081900835}, {"id": 503, "seek": 368246, "start": 3696.46, "end": 3697.46, "text": " I'll get it.", "tokens": [51064, 286, 603, 483, 309, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2823713541030884, "compression_ratio": 0.9464285714285714, "no_speech_prob": 0.00708377081900835}, {"id": 504, "seek": 371246, "start": 3713.46, "end": 3717.46, "text": " Okay, we're going to do some inference on some models.", "tokens": [50414, 1033, 11, 321, 434, 516, 281, 360, 512, 38253, 322, 512, 5245, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3062128067016602, "compression_ratio": 1.1363636363636365, "no_speech_prob": 0.014054509811103344}, {"id": 505, "seek": 371246, "start": 3721.46, "end": 3723.46, "text": " What's boss ID?", "tokens": [50814, 708, 311, 5741, 7348, 30, 50914], "temperature": 0.0, "avg_logprob": -0.3062128067016602, "compression_ratio": 1.1363636363636365, "no_speech_prob": 0.014054509811103344}, {"id": 506, "seek": 371246, "start": 3728.46, "end": 3731.46, "text": " Self tokenizer equals tokenizer.", "tokens": [51164, 16348, 14862, 6545, 6915, 14862, 6545, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3062128067016602, "compression_ratio": 1.1363636363636365, "no_speech_prob": 0.014054509811103344}, {"id": 507, "seek": 371246, "start": 3734.46, "end": 3738.46, "text": " Yeah, call this talk.", "tokens": [51464, 865, 11, 818, 341, 751, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3062128067016602, "compression_ratio": 1.1363636363636365, "no_speech_prob": 0.014054509811103344}, {"id": 508, "seek": 374246, "start": 3743.46, "end": 3758.46, "text": " Prompt equals do you like chicken talks is probably good.", "tokens": [50414, 15833, 662, 6915, 360, 291, 411, 4662, 6686, 307, 1391, 665, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3354008878980364, "compression_ratio": 1.1123595505617978, "no_speech_prob": 0.007230662740767002}, {"id": 509, "seek": 374246, "start": 3766.46, "end": 3769.46, "text": " Wow, no one's updated that to the latest.", "tokens": [51564, 3153, 11, 572, 472, 311, 10588, 300, 281, 264, 6792, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3354008878980364, "compression_ratio": 1.1123595505617978, "no_speech_prob": 0.007230662740767002}, {"id": 510, "seek": 377246, "start": 3772.46, "end": 3777.46, "text": " Just model start pause equals zero.", "tokens": [50364, 1449, 2316, 722, 10465, 6915, 4018, 13, 50614], "temperature": 0.0, "avg_logprob": -0.32269827524820965, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.01590120419859886}, {"id": 511, "seek": 377246, "start": 3779.46, "end": 3783.46, "text": " Temperature equals 0.2. Is that a good temperature?", "tokens": [50714, 34864, 1503, 6915, 1958, 13, 17, 13, 1119, 300, 257, 665, 4292, 30, 50914], "temperature": 0.0, "avg_logprob": -0.32269827524820965, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.01590120419859886}, {"id": 512, "seek": 377246, "start": 3783.46, "end": 3784.46, "text": " What's the default temperature?", "tokens": [50914, 708, 311, 264, 7576, 4292, 30, 50964], "temperature": 0.0, "avg_logprob": -0.32269827524820965, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.01590120419859886}, {"id": 513, "seek": 377246, "start": 3794.46, "end": 3797.46, "text": " Default temperature 0.7.", "tokens": [51464, 9548, 5107, 4292, 1958, 13, 22, 13, 51614], "temperature": 0.0, "avg_logprob": -0.32269827524820965, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.01590120419859886}, {"id": 514, "seek": 377246, "start": 3797.46, "end": 3799.46, "text": " Is that a high temperature or low temperature?", "tokens": [51614, 1119, 300, 257, 1090, 4292, 420, 2295, 4292, 30, 51714], "temperature": 0.0, "avg_logprob": -0.32269827524820965, "compression_ratio": 1.6754385964912282, "no_speech_prob": 0.01590120419859886}, {"id": 515, "seek": 379946, "start": 3800.46, "end": 3805.46, "text": " How do I make the loading so fast? The loading should always be that fast.", "tokens": [50414, 1012, 360, 286, 652, 264, 15114, 370, 2370, 30, 440, 15114, 820, 1009, 312, 300, 2370, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19235128922895953, "compression_ratio": 1.4222222222222223, "no_speech_prob": 0.004754227120429277}, {"id": 516, "seek": 379946, "start": 3805.46, "end": 3807.46, "text": " I just cashed it.", "tokens": [50664, 286, 445, 6388, 292, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19235128922895953, "compression_ratio": 1.4222222222222223, "no_speech_prob": 0.004754227120429277}, {"id": 517, "seek": 379946, "start": 3812.46, "end": 3816.46, "text": " You can read the code right there. You didn't pay attention if you're asking that question.", "tokens": [51014, 509, 393, 1401, 264, 3089, 558, 456, 13, 509, 994, 380, 1689, 3202, 498, 291, 434, 3365, 300, 1168, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19235128922895953, "compression_ratio": 1.4222222222222223, "no_speech_prob": 0.004754227120429277}, {"id": 518, "seek": 379946, "start": 3817.46, "end": 3819.46, "text": " So also", "tokens": [51264, 407, 611, 51364], "temperature": 0.0, "avg_logprob": -0.19235128922895953, "compression_ratio": 1.4222222222222223, "no_speech_prob": 0.004754227120429277}, {"id": 519, "seek": 381946, "start": 3819.46, "end": 3821.46, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.3530623505755169, "compression_ratio": 1.1401869158878504, "no_speech_prob": 0.04881388694047928}, {"id": 520, "seek": 381946, "start": 3828.46, "end": 3830.46, "text": " You happy", "tokens": [50814, 509, 2055, 50914], "temperature": 0.0, "avg_logprob": -0.3530623505755169, "compression_ratio": 1.1401869158878504, "no_speech_prob": 0.04881388694047928}, {"id": 521, "seek": 381946, "start": 3831.46, "end": 3835.46, "text": " We can do multinomial here, which will choose a", "tokens": [50964, 492, 393, 360, 45872, 47429, 510, 11, 597, 486, 2826, 257, 51164], "temperature": 0.0, "avg_logprob": -0.3530623505755169, "compression_ratio": 1.1401869158878504, "no_speech_prob": 0.04881388694047928}, {"id": 522, "seek": 381946, "start": 3837.46, "end": 3843.46, "text": " We shouldn't call it talk call it SPP. Okay, you down with SPP", "tokens": [51264, 492, 4659, 380, 818, 309, 751, 818, 309, 8420, 47, 13, 1033, 11, 291, 760, 365, 8420, 47, 51564], "temperature": 0.0, "avg_logprob": -0.3530623505755169, "compression_ratio": 1.1401869158878504, "no_speech_prob": 0.04881388694047928}, {"id": 523, "seek": 384946, "start": 3850.46, "end": 3854.46, "text": " Does it give us a number? No, it's going to complain about a tensor.", "tokens": [50414, 4402, 309, 976, 505, 257, 1230, 30, 883, 11, 309, 311, 516, 281, 11024, 466, 257, 40863, 13, 50614], "temperature": 0.0, "avg_logprob": -0.327108107115093, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.01941223256289959}, {"id": 524, "seek": 384946, "start": 3854.46, "end": 3861.46, "text": " I find multinomial that realized item item it and actually I don't even need it.", "tokens": [50614, 286, 915, 45872, 47429, 300, 5334, 3174, 3174, 309, 293, 767, 286, 500, 380, 754, 643, 309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.327108107115093, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.01941223256289959}, {"id": 525, "seek": 384946, "start": 3861.46, "end": 3863.46, "text": " I realize I just need a dot item.", "tokens": [50964, 286, 4325, 286, 445, 643, 257, 5893, 3174, 13, 51064], "temperature": 0.0, "avg_logprob": -0.327108107115093, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.01941223256289959}, {"id": 526, "seek": 384946, "start": 3868.46, "end": 3871.46, "text": " 315 boys 315. Okay.", "tokens": [51314, 805, 5211, 6347, 805, 5211, 13, 1033, 13, 51464], "temperature": 0.0, "avg_logprob": -0.327108107115093, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.01941223256289959}, {"id": 527, "seek": 384946, "start": 3872.46, "end": 3875.46, "text": " This should automatically this should be jetted and stuff.", "tokens": [51514, 639, 820, 6772, 341, 820, 312, 361, 46508, 293, 1507, 13, 51664], "temperature": 0.0, "avg_logprob": -0.327108107115093, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.01941223256289959}, {"id": 528, "seek": 387546, "start": 3876.46, "end": 3878.46, "text": " Where does the jet exists?", "tokens": [50414, 2305, 775, 264, 14452, 8198, 30, 50514], "temperature": 0.0, "avg_logprob": -0.3602258364359538, "compression_ratio": 1.25, "no_speech_prob": 0.007936372421681881}, {"id": 529, "seek": 387546, "start": 3879.46, "end": 3881.46, "text": " The jits inside transformer or no.", "tokens": [50564, 440, 361, 1208, 1854, 31782, 420, 572, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3602258364359538, "compression_ratio": 1.25, "no_speech_prob": 0.007936372421681881}, {"id": 530, "seek": 387546, "start": 3882.46, "end": 3884.46, "text": " Yes, the jits inside transformer.", "tokens": [50714, 1079, 11, 264, 361, 1208, 1854, 31782, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3602258364359538, "compression_ratio": 1.25, "no_speech_prob": 0.007936372421681881}, {"id": 531, "seek": 388446, "start": 3885.46, "end": 3887.46, "text": " Good", "tokens": [50414, 2205, 50514], "temperature": 0.0, "avg_logprob": -0.3058898150920868, "compression_ratio": 1.0731707317073171, "no_speech_prob": 0.03674839437007904}, {"id": 532, "seek": 388446, "start": 3902.46, "end": 3904.46, "text": " Talks out of pen talk", "tokens": [51264, 8780, 82, 484, 295, 3435, 751, 51364], "temperature": 0.0, "avg_logprob": -0.3058898150920868, "compression_ratio": 1.0731707317073171, "no_speech_prob": 0.03674839437007904}, {"id": 533, "seek": 388446, "start": 3906.46, "end": 3908.46, "text": " Us ID", "tokens": [51464, 4958, 7348, 51564], "temperature": 0.0, "avg_logprob": -0.3058898150920868, "compression_ratio": 1.0731707317073171, "no_speech_prob": 0.03674839437007904}, {"id": 534, "seek": 388446, "start": 3910.46, "end": 3913.46, "text": " This is okay. We'll just copy this. This is mostly fine", "tokens": [51664, 639, 307, 1392, 13, 492, 603, 445, 5055, 341, 13, 639, 307, 5240, 2489, 51814], "temperature": 0.0, "avg_logprob": -0.3058898150920868, "compression_ratio": 1.0731707317073171, "no_speech_prob": 0.03674839437007904}, {"id": 535, "seek": 391446, "start": 3914.46, "end": 3916.46, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.4145618620372954, "compression_ratio": 1.028169014084507, "no_speech_prob": 0.007693277671933174}, {"id": 536, "seek": 391446, "start": 3932.46, "end": 3935.46, "text": " Have to figure out actually like encode these things for chat and stuff", "tokens": [51264, 3560, 281, 2573, 484, 767, 411, 2058, 1429, 613, 721, 337, 5081, 293, 1507, 51414], "temperature": 0.0, "avg_logprob": -0.4145618620372954, "compression_ratio": 1.028169014084507, "no_speech_prob": 0.007693277671933174}, {"id": 537, "seek": 393546, "start": 3936.46, "end": 3938.46, "text": " Until", "tokens": [50414, 9088, 50514], "temperature": 0.0, "avg_logprob": -0.18155473470687866, "compression_ratio": 1.1636363636363636, "no_speech_prob": 0.008710013702511787}, {"id": 538, "seek": 393546, "start": 3949.46, "end": 3951.46, "text": " Where's my thing that prints the output as I go?", "tokens": [51064, 2305, 311, 452, 551, 300, 22305, 264, 5598, 382, 286, 352, 30, 51164], "temperature": 0.0, "avg_logprob": -0.18155473470687866, "compression_ratio": 1.1636363636363636, "no_speech_prob": 0.008710013702511787}, {"id": 539, "seek": 393546, "start": 3955.46, "end": 3957.46, "text": " Not here", "tokens": [51364, 1726, 510, 51464], "temperature": 0.0, "avg_logprob": -0.18155473470687866, "compression_ratio": 1.1636363636363636, "no_speech_prob": 0.008710013702511787}, {"id": 540, "seek": 393546, "start": 3960.46, "end": 3963.46, "text": " Yeah, this is not okay. I don't even understand why that's there", "tokens": [51614, 865, 11, 341, 307, 406, 1392, 13, 286, 500, 380, 754, 1223, 983, 300, 311, 456, 51764], "temperature": 0.0, "avg_logprob": -0.18155473470687866, "compression_ratio": 1.1636363636363636, "no_speech_prob": 0.008710013702511787}, {"id": 541, "seek": 396546, "start": 3966.46, "end": 3969.46, "text": " This is terrible", "tokens": [50414, 639, 307, 6237, 50564], "temperature": 0.0, "avg_logprob": -0.4434356689453125, "compression_ratio": 1.2065217391304348, "no_speech_prob": 0.0020505015272647142}, {"id": 542, "seek": 396546, "start": 3983.46, "end": 3989.46, "text": " This stuff's terrible. No one's refactored this for a long time. We don't need a fucking numpy", "tokens": [51264, 639, 1507, 311, 6237, 13, 883, 472, 311, 1895, 578, 2769, 341, 337, 257, 938, 565, 13, 492, 500, 380, 643, 257, 5546, 1031, 8200, 51564], "temperature": 0.0, "avg_logprob": -0.4434356689453125, "compression_ratio": 1.2065217391304348, "no_speech_prob": 0.0020505015272647142}, {"id": 543, "seek": 398946, "start": 3989.46, "end": 3991.46, "text": " Numpy", "tokens": [50364, 22592, 8200, 50464], "temperature": 0.0, "avg_logprob": -0.5058018366495768, "compression_ratio": 1.28125, "no_speech_prob": 0.011685740202665329}, {"id": 544, "seek": 398946, "start": 3992.46, "end": 3994.46, "text": " Start positive was lentos", "tokens": [50514, 6481, 3353, 390, 23556, 329, 50614], "temperature": 0.0, "avg_logprob": -0.5058018366495768, "compression_ratio": 1.28125, "no_speech_prob": 0.011685740202665329}, {"id": 545, "seek": 398946, "start": 3999.46, "end": 4001.46, "text": " Whatever", "tokens": [50864, 8541, 50964], "temperature": 0.0, "avg_logprob": -0.5058018366495768, "compression_ratio": 1.28125, "no_speech_prob": 0.011685740202665329}, {"id": 546, "seek": 398946, "start": 4002.46, "end": 4005.46, "text": " That's stupid and that's stupid", "tokens": [51014, 663, 311, 6631, 293, 300, 311, 6631, 51164], "temperature": 0.0, "avg_logprob": -0.5058018366495768, "compression_ratio": 1.28125, "no_speech_prob": 0.011685740202665329}, {"id": 547, "seek": 398946, "start": 4010.46, "end": 4014.46, "text": " Link output it we're gonna set output it somewhere", "tokens": [51414, 8466, 5598, 309, 321, 434, 799, 992, 5598, 309, 4079, 51614], "temperature": 0.0, "avg_logprob": -0.5058018366495768, "compression_ratio": 1.28125, "no_speech_prob": 0.011685740202665329}, {"id": 548, "seek": 401946, "start": 4020.46, "end": 4022.46, "text": " I", "tokens": [50414, 286, 50514], "temperature": 0.0, "avg_logprob": -0.4755377769470215, "compression_ratio": 1.14, "no_speech_prob": 0.0044669415801763535}, {"id": 549, "seek": 401946, "start": 4023.46, "end": 4025.46, "text": " Put it equals user prompt", "tokens": [50564, 4935, 309, 6915, 4195, 12391, 50664], "temperature": 0.0, "avg_logprob": -0.4755377769470215, "compression_ratio": 1.14, "no_speech_prob": 0.0044669415801763535}, {"id": 550, "seek": 401946, "start": 4034.46, "end": 4036.46, "text": " User prompt", "tokens": [51114, 32127, 12391, 51214], "temperature": 0.0, "avg_logprob": -0.4755377769470215, "compression_ratio": 1.14, "no_speech_prob": 0.0044669415801763535}, {"id": 551, "seek": 401946, "start": 4042.46, "end": 4044.46, "text": " These are prompts", "tokens": [51514, 1981, 366, 41095, 51614], "temperature": 0.0, "avg_logprob": -0.4755377769470215, "compression_ratio": 1.14, "no_speech_prob": 0.0044669415801763535}, {"id": 552, "seek": 404946, "start": 4049.46, "end": 4055.86, "text": " If yes, then you must have tried big chicken in your kitchen big chickens a versatile dish that can be prepared in many ways", "tokens": [50364, 759, 2086, 11, 550, 291, 1633, 362, 3031, 955, 4662, 294, 428, 6525, 955, 22329, 257, 25057, 5025, 300, 393, 312, 4927, 294, 867, 2098, 50684], "temperature": 0.0, "avg_logprob": -0.33205184230098017, "compression_ratio": 1.4064516129032258, "no_speech_prob": 0.0034287816379219294}, {"id": 553, "seek": 404946, "start": 4062.14, "end": 4065.2200000000003, "text": " All right, are we happy overall? Oh, I'm not putting that wrong", "tokens": [50998, 1057, 558, 11, 366, 321, 2055, 4787, 30, 876, 11, 286, 478, 406, 3372, 300, 2085, 51152], "temperature": 0.0, "avg_logprob": -0.33205184230098017, "compression_ratio": 1.4064516129032258, "no_speech_prob": 0.0034287816379219294}, {"id": 554, "seek": 404946, "start": 4074.96, "end": 4078.1, "text": " We should also do this better", "tokens": [51639, 492, 820, 611, 360, 341, 1101, 51796], "temperature": 0.0, "avg_logprob": -0.33205184230098017, "compression_ratio": 1.4064516129032258, "no_speech_prob": 0.0034287816379219294}, {"id": 555, "seek": 407946, "start": 4079.7, "end": 4081.7, "text": " I", "tokens": [50376, 286, 50476], "temperature": 0.0, "avg_logprob": -0.265163169187658, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0011335194576531649}, {"id": 556, "seek": 407946, "start": 4089.86, "end": 4095.02, "text": " Do you like chicken chicken is one of the most popular meats in the world and it's not hard to understand why it is", "tokens": [50884, 1144, 291, 411, 4662, 4662, 307, 472, 295, 264, 881, 3743, 38106, 294, 264, 1002, 293, 309, 311, 406, 1152, 281, 1223, 983, 309, 307, 51142], "temperature": 0.0, "avg_logprob": -0.265163169187658, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0011335194576531649}, {"id": 557, "seek": 407946, "start": 4095.02, "end": 4097.02, "text": " First time easy to prepare. Wow", "tokens": [51142, 2386, 565, 1858, 281, 5940, 13, 3153, 51242], "temperature": 0.0, "avg_logprob": -0.265163169187658, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0011335194576531649}, {"id": 558, "seek": 407946, "start": 4097.58, "end": 4102.38, "text": " Okay, good seems pretty good. Okay. Now. We just have to figure out how to use these things as chat box", "tokens": [51270, 1033, 11, 665, 2544, 1238, 665, 13, 1033, 13, 823, 13, 492, 445, 362, 281, 2573, 484, 577, 281, 764, 613, 721, 382, 5081, 2424, 51510], "temperature": 0.0, "avg_logprob": -0.265163169187658, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0011335194576531649}, {"id": 559, "seek": 410238, "start": 4103.32, "end": 4105.64, "text": " Get rid of that. We don't need that", "tokens": [50411, 3240, 3973, 295, 300, 13, 492, 500, 380, 643, 300, 50527], "temperature": 0.0, "avg_logprob": -0.2517364744156126, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.006589020136743784}, {"id": 560, "seek": 410238, "start": 4106.9400000000005, "end": 4110.58, "text": " There's like some like special tokens for chat bots, I believe", "tokens": [50592, 821, 311, 411, 512, 411, 2121, 22667, 337, 5081, 35410, 11, 286, 1697, 50774], "temperature": 0.0, "avg_logprob": -0.2517364744156126, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.006589020136743784}, {"id": 561, "seek": 410238, "start": 4120.78, "end": 4127.22, "text": " I'm not buying into this that there was a breakthrough this if this approach has anything to do with like supervising the middle steps", "tokens": [51284, 286, 478, 406, 6382, 666, 341, 300, 456, 390, 257, 22397, 341, 498, 341, 3109, 575, 1340, 281, 360, 365, 411, 37971, 3436, 264, 2808, 4439, 51606], "temperature": 0.0, "avg_logprob": -0.2517364744156126, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.006589020136743784}, {"id": 562, "seek": 410238, "start": 4127.22, "end": 4129.22, "text": " It seems really stupid actually", "tokens": [51606, 467, 2544, 534, 6631, 767, 51706], "temperature": 0.0, "avg_logprob": -0.2517364744156126, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.006589020136743784}, {"id": 563, "seek": 413238, "start": 4132.74, "end": 4134.74, "text": " Like it's a bad idea", "tokens": [50382, 1743, 309, 311, 257, 1578, 1558, 50482], "temperature": 0.0, "avg_logprob": -0.31452623094831195, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.002182552358135581}, {"id": 564, "seek": 413238, "start": 4137.1, "end": 4142.86, "text": " Mary open thank you for resubscribing thank you for being a 20 month subscriber loyal to my channel even right on stream", "tokens": [50600, 6059, 1269, 1309, 291, 337, 725, 5432, 39541, 1309, 291, 337, 885, 257, 945, 1618, 26122, 12682, 281, 452, 2269, 754, 558, 322, 4309, 50888], "temperature": 0.0, "avg_logprob": -0.31452623094831195, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.002182552358135581}, {"id": 565, "seek": 413238, "start": 4145.26, "end": 4147.26, "text": " Why do I have this this is done?", "tokens": [51008, 1545, 360, 286, 362, 341, 341, 307, 1096, 30, 51108], "temperature": 0.0, "avg_logprob": -0.31452623094831195, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.002182552358135581}, {"id": 566, "seek": 413238, "start": 4148.46, "end": 4151.46, "text": " Okay, that looks like a decent chunk of code", "tokens": [51168, 1033, 11, 300, 1542, 411, 257, 8681, 16635, 295, 3089, 51318], "temperature": 0.0, "avg_logprob": -0.31452623094831195, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.002182552358135581}, {"id": 567, "seek": 413238, "start": 4153.3, "end": 4157.34, "text": " Create a little function called output non-local", "tokens": [51410, 20248, 257, 707, 2445, 1219, 5598, 2107, 12, 5842, 304, 51612], "temperature": 0.0, "avg_logprob": -0.31452623094831195, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.002182552358135581}, {"id": 568, "seek": 416238, "start": 4163.38, "end": 4165.38, "text": " I'll put it", "tokens": [50414, 286, 603, 829, 309, 50514], "temperature": 0.0, "avg_logprob": -0.5865199565887451, "compression_ratio": 1.2054794520547945, "no_speech_prob": 0.008314813487231731}, {"id": 569, "seek": 416238, "start": 4170.18, "end": 4172.18, "text": " Yeah", "tokens": [50754, 865, 50854], "temperature": 0.0, "avg_logprob": -0.5865199565887451, "compression_ratio": 1.2054794520547945, "no_speech_prob": 0.008314813487231731}, {"id": 570, "seek": 416238, "start": 4176.82, "end": 4178.82, "text": " That's", "tokens": [51086, 663, 311, 51186], "temperature": 0.0, "avg_logprob": -0.5865199565887451, "compression_ratio": 1.2054794520547945, "no_speech_prob": 0.008314813487231731}, {"id": 571, "seek": 416238, "start": 4182.66, "end": 4189.38, "text": " Is that that's right there I'll get it and that's how that works", "tokens": [51378, 1119, 300, 300, 311, 558, 456, 286, 603, 483, 309, 293, 300, 311, 577, 300, 1985, 51714], "temperature": 0.0, "avg_logprob": -0.5865199565887451, "compression_ratio": 1.2054794520547945, "no_speech_prob": 0.008314813487231731}, {"id": 572, "seek": 419238, "start": 4192.38, "end": 4194.38, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.5736342072486877, "compression_ratio": 0.1111111111111111, "no_speech_prob": 0.6865133047103882}, {"id": 573, "seek": 422238, "start": 4222.38, "end": 4224.38, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.6406014561653137, "compression_ratio": 1.0326086956521738, "no_speech_prob": 0.0029807661194354296}, {"id": 574, "seek": 422238, "start": 4230.14, "end": 4232.62, "text": " Like this local variable output", "tokens": [50752, 1743, 341, 2654, 7006, 5598, 50876], "temperature": 0.0, "avg_logprob": -0.6406014561653137, "compression_ratio": 1.0326086956521738, "no_speech_prob": 0.0029807661194354296}, {"id": 575, "seek": 422238, "start": 4237.38, "end": 4240.18, "text": " No binding for why doesn't that work? Oh", "tokens": [51114, 883, 17359, 337, 983, 1177, 380, 300, 589, 30, 876, 51254], "temperature": 0.0, "avg_logprob": -0.6406014561653137, "compression_ratio": 1.0326086956521738, "no_speech_prob": 0.0029807661194354296}, {"id": 576, "seek": 422238, "start": 4241.22, "end": 4243.22, "text": " Because I'm in may I", "tokens": [51306, 1436, 286, 478, 294, 815, 286, 51406], "temperature": 0.0, "avg_logprob": -0.6406014561653137, "compression_ratio": 1.0326086956521738, "no_speech_prob": 0.0029807661194354296}, {"id": 577, "seek": 424322, "start": 4243.26, "end": 4245.26, "text": " Fine", "tokens": [50366, 12024, 50466], "temperature": 0.0, "avg_logprob": -0.35689615434215916, "compression_ratio": 1.4303797468354431, "no_speech_prob": 0.002182622207328677}, {"id": 578, "seek": 424322, "start": 4249.34, "end": 4253.56, "text": " Do you like chicken how about a recipe for a homemade chicken dish that's super easy to make? Oh", "tokens": [50670, 1144, 291, 411, 4662, 577, 466, 257, 6782, 337, 257, 23336, 4662, 5025, 300, 311, 1687, 1858, 281, 652, 30, 876, 50881], "temperature": 0.0, "avg_logprob": -0.35689615434215916, "compression_ratio": 1.4303797468354431, "no_speech_prob": 0.002182622207328677}, {"id": 579, "seek": 424322, "start": 4256.18, "end": 4259.18, "text": " I'm gonna get these things working like chat bots. How do we do this?", "tokens": [51012, 286, 478, 799, 483, 613, 721, 1364, 411, 5081, 35410, 13, 1012, 360, 321, 360, 341, 30, 51162], "temperature": 0.0, "avg_logprob": -0.35689615434215916, "compression_ratio": 1.4303797468354431, "no_speech_prob": 0.002182622207328677}, {"id": 580, "seek": 424322, "start": 4263.7, "end": 4265.9800000000005, "text": " Okay, wait wait wait, can we just read this comment is", "tokens": [51388, 1033, 11, 1699, 1699, 1699, 11, 393, 321, 445, 1401, 341, 2871, 307, 51502], "temperature": 0.0, "avg_logprob": -0.35689615434215916, "compression_ratio": 1.4303797468354431, "no_speech_prob": 0.002182622207328677}, {"id": 581, "seek": 426598, "start": 4266.86, "end": 4274.259999999999, "text": " The q-star algorithm going to be implemented with the comma AI maybe a voice interface just some kind of assistant", "tokens": [50408, 440, 9505, 12, 9710, 9284, 516, 281, 312, 12270, 365, 264, 22117, 7318, 1310, 257, 3177, 9226, 445, 512, 733, 295, 10994, 50778], "temperature": 0.0, "avg_logprob": -0.27216506250125844, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.017173515632748604}, {"id": 582, "seek": 426598, "start": 4274.259999999999, "end": 4281.0199999999995, "text": " I just I don't even know how to help you people. Okay. I don't even know how to help you. All right. All right", "tokens": [50778, 286, 445, 286, 500, 380, 754, 458, 577, 281, 854, 291, 561, 13, 1033, 13, 286, 500, 380, 754, 458, 577, 281, 854, 291, 13, 1057, 558, 13, 1057, 558, 51116], "temperature": 0.0, "avg_logprob": -0.27216506250125844, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.017173515632748604}, {"id": 583, "seek": 426598, "start": 4281.0199999999995, "end": 4284.0199999999995, "text": " Sorry, I can't help you. You want you want to see a voice you want to see a voice chat?", "tokens": [51116, 4919, 11, 286, 393, 380, 854, 291, 13, 509, 528, 291, 528, 281, 536, 257, 3177, 291, 528, 281, 536, 257, 3177, 5081, 30, 51266], "temperature": 0.0, "avg_logprob": -0.27216506250125844, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.017173515632748604}, {"id": 584, "seek": 426598, "start": 4284.58, "end": 4287.099999999999, "text": " This is this is one of the demos someone's been working", "tokens": [51294, 639, 307, 341, 307, 472, 295, 264, 33788, 1580, 311, 668, 1364, 51420], "temperature": 0.0, "avg_logprob": -0.27216506250125844, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.017173515632748604}, {"id": 585, "seek": 429598, "start": 4296.419999999999, "end": 4298.419999999999, "text": " I", "tokens": [50386, 286, 50486], "temperature": 0.0, "avg_logprob": -0.4733074951171875, "compression_ratio": 1.0649350649350648, "no_speech_prob": 0.008709324523806572}, {"id": 586, "seek": 429598, "start": 4313.0599999999995, "end": 4320.419999999999, "text": " Scalmag has been working on combining whisper llama and vixx into one supermodel", "tokens": [51218, 2747, 304, 37941, 575, 668, 1364, 322, 21928, 26018, 23272, 293, 371, 970, 87, 666, 472, 1687, 8014, 338, 51586], "temperature": 0.0, "avg_logprob": -0.4733074951171875, "compression_ratio": 1.0649350649350648, "no_speech_prob": 0.008709324523806572}, {"id": 587, "seek": 432598, "start": 4325.98, "end": 4327.98, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.33547930881894866, "compression_ratio": 1.12987012987013, "no_speech_prob": 0.006796078756451607}, {"id": 588, "seek": 432598, "start": 4332.339999999999, "end": 4339.219999999999, "text": " Don't know the magical github incantation to do this there is one but I don't know it", "tokens": [50682, 1468, 380, 458, 264, 12066, 290, 355, 836, 834, 394, 399, 281, 360, 341, 456, 307, 472, 457, 286, 500, 380, 458, 309, 51026], "temperature": 0.0, "avg_logprob": -0.33547930881894866, "compression_ratio": 1.12987012987013, "no_speech_prob": 0.006796078756451607}, {"id": 589, "seek": 435598, "start": 4355.98, "end": 4357.98, "text": " You", "tokens": [50364, 509, 50464], "temperature": 0.0, "avg_logprob": -0.5495592355728149, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.6427751779556274}, {"id": 590, "seek": 438598, "start": 4385.98, "end": 4387.98, "text": " You", "tokens": [50364, 509, 50464], "temperature": 0.0, "avg_logprob": -0.2647544612055239, "compression_ratio": 1.0136986301369864, "no_speech_prob": 0.0023964885622262955}, {"id": 591, "seek": 438598, "start": 4395.66, "end": 4400.299999999999, "text": " Okay, this is using a small llama so the chat output is not that great", "tokens": [50848, 1033, 11, 341, 307, 1228, 257, 1359, 23272, 370, 264, 5081, 5598, 307, 406, 300, 869, 51080], "temperature": 0.0, "avg_logprob": -0.2647544612055239, "compression_ratio": 1.0136986301369864, "no_speech_prob": 0.0023964885622262955}, {"id": 592, "seek": 440030, "start": 4400.3, "end": 4417.06, "text": " What oh did I fork this from mistral my bad", "tokens": [50364, 708, 1954, 630, 286, 17716, 341, 490, 3544, 2155, 452, 1578, 51202], "temperature": 0.0, "avg_logprob": -0.5538104261670794, "compression_ratio": 0.8431372549019608, "no_speech_prob": 0.011866590939462185}, {"id": 593, "seek": 443030, "start": 4430.3, "end": 4432.3, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.6500254410963792, "compression_ratio": 0.8095238095238095, "no_speech_prob": 0.00926514994353056}, {"id": 594, "seek": 443030, "start": 4445.06, "end": 4447.78, "text": " Don't understand what did I work", "tokens": [51102, 1468, 380, 1223, 437, 630, 286, 589, 51238], "temperature": 0.0, "avg_logprob": -0.6500254410963792, "compression_ratio": 0.8095238095238095, "no_speech_prob": 0.00926514994353056}, {"id": 595, "seek": 446030, "start": 4460.9400000000005, "end": 4465.14, "text": " Here and kv heads", "tokens": [50396, 1692, 293, 350, 85, 8050, 50606], "temperature": 0.0, "avg_logprob": -0.5482522357593883, "compression_ratio": 0.9852941176470589, "no_speech_prob": 0.006486830301582813}, {"id": 596, "seek": 446030, "start": 4472.3, "end": 4474.3, "text": " Is this just broken", "tokens": [50964, 1119, 341, 445, 5463, 51064], "temperature": 0.0, "avg_logprob": -0.5482522357593883, "compression_ratio": 0.9852941176470589, "no_speech_prob": 0.006486830301582813}, {"id": 597, "seek": 446030, "start": 4477.7, "end": 4479.7, "text": " Or am I doing something wrong", "tokens": [51234, 1610, 669, 286, 884, 746, 2085, 51334], "temperature": 0.0, "avg_logprob": -0.5482522357593883, "compression_ratio": 0.9852941176470589, "no_speech_prob": 0.006486830301582813}, {"id": 598, "seek": 449030, "start": 4490.7, "end": 4492.7, "text": " I", "tokens": [50384, 286, 50484], "temperature": 0.0, "avg_logprob": -0.7837122508457729, "compression_ratio": 0.7777777777777778, "no_speech_prob": 0.02926289476454258}, {"id": 599, "seek": 449030, "start": 4498.14, "end": 4500.14, "text": " Didn't change on master", "tokens": [50756, 11151, 380, 1319, 322, 4505, 50856], "temperature": 0.0, "avg_logprob": -0.7837122508457729, "compression_ratio": 0.7777777777777778, "no_speech_prob": 0.02926289476454258}, {"id": 600, "seek": 449030, "start": 4509.58, "end": 4511.58, "text": " No", "tokens": [51328, 883, 51428], "temperature": 0.0, "avg_logprob": -0.7837122508457729, "compression_ratio": 0.7777777777777778, "no_speech_prob": 0.02926289476454258}, {"id": 601, "seek": 452030, "start": 4520.3, "end": 4522.3, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.3570067635897932, "compression_ratio": 1.1627906976744187, "no_speech_prob": 0.0011512520723044872}, {"id": 602, "seek": 452030, "start": 4525.46, "end": 4529.02, "text": " Don't understand this is like related to what I was messing with but like", "tokens": [50622, 1468, 380, 1223, 341, 307, 411, 4077, 281, 437, 286, 390, 23258, 365, 457, 411, 50800], "temperature": 0.0, "avg_logprob": -0.3570067635897932, "compression_ratio": 1.1627906976744187, "no_speech_prob": 0.0011512520723044872}, {"id": 603, "seek": 452030, "start": 4542.06, "end": 4544.06, "text": " I think it's just broken", "tokens": [51452, 286, 519, 309, 311, 445, 5463, 51552], "temperature": 0.0, "avg_logprob": -0.3570067635897932, "compression_ratio": 1.1627906976744187, "no_speech_prob": 0.0011512520723044872}, {"id": 604, "seek": 455030, "start": 4550.3, "end": 4552.3, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.38497620820999146, "compression_ratio": 0.9871794871794872, "no_speech_prob": 0.0018967868527397513}, {"id": 605, "seek": 455030, "start": 4559.3, "end": 4561.3, "text": " See if it's a one-line fix", "tokens": [50814, 3008, 498, 309, 311, 257, 472, 12, 1889, 3191, 50914], "temperature": 0.0, "avg_logprob": -0.38497620820999146, "compression_ratio": 0.9871794871794872, "no_speech_prob": 0.0018967868527397513}, {"id": 606, "seek": 455030, "start": 4563.860000000001, "end": 4566.58, "text": " Has no argument kv heads, okay", "tokens": [51042, 8646, 572, 6770, 350, 85, 8050, 11, 1392, 51178], "temperature": 0.0, "avg_logprob": -0.38497620820999146, "compression_ratio": 0.9871794871794872, "no_speech_prob": 0.0018967868527397513}, {"id": 607, "seek": 455030, "start": 4577.14, "end": 4579.14, "text": " No, this is wrong", "tokens": [51706, 883, 11, 341, 307, 2085, 51806], "temperature": 0.0, "avg_logprob": -0.38497620820999146, "compression_ratio": 0.9871794871794872, "no_speech_prob": 0.0018967868527397513}, {"id": 608, "seek": 458030, "start": 4581.3, "end": 4583.3, "text": " I", "tokens": [50414, 286, 50514], "temperature": 0.0, "avg_logprob": -0.361374568939209, "compression_ratio": 1.2783505154639174, "no_speech_prob": 0.00029135451768524945}, {"id": 609, "seek": 458030, "start": 4587.58, "end": 4594.78, "text": " Wait, this should have an nkv heads. I don't understand as an nkv heads right there. Is it not passing in the right model?", "tokens": [50728, 3802, 11, 341, 820, 362, 364, 297, 74, 85, 8050, 13, 286, 500, 380, 1223, 382, 364, 297, 74, 85, 8050, 558, 456, 13, 1119, 309, 406, 8437, 294, 264, 558, 2316, 30, 51088], "temperature": 0.0, "avg_logprob": -0.361374568939209, "compression_ratio": 1.2783505154639174, "no_speech_prob": 0.00029135451768524945}, {"id": 610, "seek": 459478, "start": 4594.78, "end": 4596.78, "text": " Oh", "tokens": [50364, 876, 50464], "temperature": 0.0, "avg_logprob": -0.6406008215511546, "compression_ratio": 0.9090909090909091, "no_speech_prob": 0.004980867728590965}, {"id": 611, "seek": 459478, "start": 4607.0199999999995, "end": 4609.0199999999995, "text": " Here are seven days", "tokens": [50976, 1692, 366, 3407, 1708, 51076], "temperature": 0.0, "avg_logprob": -0.6406008215511546, "compression_ratio": 0.9090909090909091, "no_speech_prob": 0.004980867728590965}, {"id": 612, "seek": 459478, "start": 4621.5, "end": 4623.5, "text": " Okay, there we go", "tokens": [51700, 1033, 11, 456, 321, 352, 51800], "temperature": 0.0, "avg_logprob": -0.6406008215511546, "compression_ratio": 0.9090909090909091, "no_speech_prob": 0.004980867728590965}, {"id": 613, "seek": 462478, "start": 4624.94, "end": 4626.94, "text": " Oh", "tokens": [50372, 876, 50472], "temperature": 0.0, "avg_logprob": -0.44078244103325737, "compression_ratio": 1.1466666666666667, "no_speech_prob": 0.0006361562409438193}, {"id": 614, "seek": 462478, "start": 4628.46, "end": 4630.46, "text": " Hello, are you listening", "tokens": [50548, 2425, 11, 366, 291, 4764, 50648], "temperature": 0.0, "avg_logprob": -0.44078244103325737, "compression_ratio": 1.1466666666666667, "no_speech_prob": 0.0006361562409438193}, {"id": 615, "seek": 462478, "start": 4639.94, "end": 4641.94, "text": " Are you listening", "tokens": [51122, 2014, 291, 4764, 51222], "temperature": 0.0, "avg_logprob": -0.44078244103325737, "compression_ratio": 1.1466666666666667, "no_speech_prob": 0.0006361562409438193}, {"id": 616, "seek": 462478, "start": 4649.0199999999995, "end": 4651.0199999999995, "text": " Can you not detect end of stream anymore", "tokens": [51576, 1664, 291, 406, 5531, 917, 295, 4309, 3602, 51676], "temperature": 0.0, "avg_logprob": -0.44078244103325737, "compression_ratio": 1.1466666666666667, "no_speech_prob": 0.0006361562409438193}, {"id": 617, "seek": 465102, "start": 4651.46, "end": 4653.46, "text": " Oh", "tokens": [50386, 876, 50486], "temperature": 0.0, "avg_logprob": -0.2569466004004845, "compression_ratio": 1.4, "no_speech_prob": 0.0012447226326912642}, {"id": 618, "seek": 465102, "start": 4663.02, "end": 4666.780000000001, "text": " This used to work this used to work boys this used to work", "tokens": [50964, 639, 1143, 281, 589, 341, 1143, 281, 589, 6347, 341, 1143, 281, 589, 51152], "temperature": 0.0, "avg_logprob": -0.2569466004004845, "compression_ratio": 1.4, "no_speech_prob": 0.0012447226326912642}, {"id": 619, "seek": 465102, "start": 4673.42, "end": 4675.42, "text": " It's so good yesterday", "tokens": [51484, 467, 311, 370, 665, 5186, 51584], "temperature": 0.0, "avg_logprob": -0.2569466004004845, "compression_ratio": 1.4, "no_speech_prob": 0.0012447226326912642}, {"id": 620, "seek": 468102, "start": 4682.02, "end": 4686.38, "text": " I hope everyone's interested in the long stream today", "tokens": [50414, 286, 1454, 1518, 311, 3102, 294, 264, 938, 4309, 965, 50632], "temperature": 0.0, "avg_logprob": -0.41319440540514496, "compression_ratio": 1.0579710144927537, "no_speech_prob": 0.0003199881757609546}, {"id": 621, "seek": 468102, "start": 4699.46, "end": 4701.46, "text": " Listening", "tokens": [51286, 49321, 51386], "temperature": 0.0, "avg_logprob": -0.41319440540514496, "compression_ratio": 1.0579710144927537, "no_speech_prob": 0.0003199881757609546}, {"id": 622, "seek": 468102, "start": 4705.620000000001, "end": 4707.620000000001, "text": " Listening", "tokens": [51594, 49321, 51694], "temperature": 0.0, "avg_logprob": -0.41319440540514496, "compression_ratio": 1.0579710144927537, "no_speech_prob": 0.0003199881757609546}, {"id": 623, "seek": 471102, "start": 4711.42, "end": 4713.42, "text": " I", "tokens": [50384, 286, 50484], "temperature": 0.0, "avg_logprob": -0.402626219249907, "compression_ratio": 1.0298507462686568, "no_speech_prob": 0.0017002687091007829}, {"id": 624, "seek": 471102, "start": 4729.26, "end": 4736.740000000001, "text": " The q-star algorithm is not fucking real it's click bait click bait", "tokens": [51276, 440, 9505, 12, 9710, 9284, 307, 406, 5546, 957, 309, 311, 2052, 16865, 2052, 16865, 51650], "temperature": 0.0, "avg_logprob": -0.402626219249907, "compression_ratio": 1.0298507462686568, "no_speech_prob": 0.0017002687091007829}, {"id": 625, "seek": 474102, "start": 4741.02, "end": 4743.02, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.4978794097900391, "compression_ratio": 0.9310344827586207, "no_speech_prob": 0.0009696953929960728}, {"id": 626, "seek": 474102, "start": 4756.860000000001, "end": 4758.860000000001, "text": " Has this ever exit", "tokens": [51156, 8646, 341, 1562, 11043, 51256], "temperature": 0.0, "avg_logprob": -0.4978794097900391, "compression_ratio": 0.9310344827586207, "no_speech_prob": 0.0009696953929960728}, {"id": 627, "seek": 474102, "start": 4763.02, "end": 4765.02, "text": " Yes, I throw an exception I guess", "tokens": [51464, 1079, 11, 286, 3507, 364, 11183, 286, 2041, 51564], "temperature": 0.0, "avg_logprob": -0.4978794097900391, "compression_ratio": 0.9310344827586207, "no_speech_prob": 0.0009696953929960728}, {"id": 628, "seek": 477102, "start": 4771.740000000001, "end": 4773.740000000001, "text": " You", "tokens": [50400, 509, 50500], "temperature": 0.0, "avg_logprob": -0.39719301111557903, "compression_ratio": 1.1538461538461537, "no_speech_prob": 0.0015977135626599193}, {"id": 629, "seek": 477102, "start": 4777.9400000000005, "end": 4779.9400000000005, "text": " What", "tokens": [50710, 708, 50810], "temperature": 0.0, "avg_logprob": -0.39719301111557903, "compression_ratio": 1.1538461538461537, "no_speech_prob": 0.0015977135626599193}, {"id": 630, "seek": 477102, "start": 4780.38, "end": 4786.18, "text": " How does work how's this supposed to exit okay, let's check out the older version", "tokens": [50832, 1012, 775, 589, 577, 311, 341, 3442, 281, 11043, 1392, 11, 718, 311, 1520, 484, 264, 4906, 3037, 51122], "temperature": 0.0, "avg_logprob": -0.39719301111557903, "compression_ratio": 1.1538461538461537, "no_speech_prob": 0.0015977135626599193}, {"id": 631, "seek": 477102, "start": 4790.700000000001, "end": 4792.700000000001, "text": " Let's try this", "tokens": [51348, 961, 311, 853, 341, 51448], "temperature": 0.0, "avg_logprob": -0.39719301111557903, "compression_ratio": 1.1538461538461537, "no_speech_prob": 0.0015977135626599193}, {"id": 632, "seek": 479270, "start": 4793.099999999999, "end": 4795.099999999999, "text": " I", "tokens": [50384, 286, 50484], "temperature": 0.0, "avg_logprob": -0.42077391942342124, "compression_ratio": 1.0657894736842106, "no_speech_prob": 0.0008692399715073407}, {"id": 633, "seek": 479270, "start": 4800.54, "end": 4802.54, "text": " Hello, are you listening?", "tokens": [50756, 2425, 11, 366, 291, 4764, 30, 50856], "temperature": 0.0, "avg_logprob": -0.42077391942342124, "compression_ratio": 1.0657894736842106, "no_speech_prob": 0.0008692399715073407}, {"id": 634, "seek": 479270, "start": 4810.179999999999, "end": 4812.38, "text": " Stacey I need you to say something to me", "tokens": [51238, 745, 29059, 286, 643, 291, 281, 584, 746, 281, 385, 51348], "temperature": 0.0, "avg_logprob": -0.42077391942342124, "compression_ratio": 1.0657894736842106, "no_speech_prob": 0.0008692399715073407}, {"id": 635, "seek": 479270, "start": 4815.62, "end": 4817.62, "text": " Stacey sauce", "tokens": [51510, 745, 29059, 4880, 51610], "temperature": 0.0, "avg_logprob": -0.42077391942342124, "compression_ratio": 1.0657894736842106, "no_speech_prob": 0.0008692399715073407}, {"id": 636, "seek": 481762, "start": 4817.74, "end": 4819.74, "text": " I", "tokens": [50370, 286, 50470], "temperature": 0.0, "avg_logprob": -0.2957562340630425, "compression_ratio": 1.5112781954887218, "no_speech_prob": 0.00041068706195801497}, {"id": 637, "seek": 481762, "start": 4822.46, "end": 4824.46, "text": " Stacey are you a rapper?", "tokens": [50606, 745, 29059, 366, 291, 257, 26457, 30, 50706], "temperature": 0.0, "avg_logprob": -0.2957562340630425, "compression_ratio": 1.5112781954887218, "no_speech_prob": 0.00041068706195801497}, {"id": 638, "seek": 481762, "start": 4829.62, "end": 4835.04, "text": " Of course, I'm a rapper. I'm a rap star. Tell me more. What's your favorite rap song?", "tokens": [50964, 2720, 1164, 11, 286, 478, 257, 26457, 13, 286, 478, 257, 5099, 3543, 13, 5115, 385, 544, 13, 708, 311, 428, 2954, 5099, 2153, 30, 51235], "temperature": 0.0, "avg_logprob": -0.2957562340630425, "compression_ratio": 1.5112781954887218, "no_speech_prob": 0.00041068706195801497}, {"id": 639, "seek": 481762, "start": 4838.5, "end": 4843.5, "text": " My favorite rap song is called my favorite song the gory bee. What do you like about it?", "tokens": [51408, 1222, 2954, 5099, 2153, 307, 1219, 452, 2954, 2153, 264, 290, 827, 17479, 13, 708, 360, 291, 411, 466, 309, 30, 51658], "temperature": 0.0, "avg_logprob": -0.2957562340630425, "compression_ratio": 1.5112781954887218, "no_speech_prob": 0.00041068706195801497}, {"id": 640, "seek": 484762, "start": 4847.98, "end": 4853.9, "text": " I like the fact that it's a rap song about being a childhood friend. Did you have friends in your childhood?", "tokens": [50382, 286, 411, 264, 1186, 300, 309, 311, 257, 5099, 2153, 466, 885, 257, 9278, 1277, 13, 2589, 291, 362, 1855, 294, 428, 9278, 30, 50678], "temperature": 0.0, "avg_logprob": -0.21716863087245397, "compression_ratio": 1.5621301775147929, "no_speech_prob": 0.0012442892184481025}, {"id": 641, "seek": 484762, "start": 4857.62, "end": 4861.22, "text": " Yes, I have many friends in my childhood. Tell me about them", "tokens": [50864, 1079, 11, 286, 362, 867, 1855, 294, 452, 9278, 13, 5115, 385, 466, 552, 51044], "temperature": 0.0, "avg_logprob": -0.21716863087245397, "compression_ratio": 1.5621301775147929, "no_speech_prob": 0.0012442892184481025}, {"id": 642, "seek": 484762, "start": 4865.94, "end": 4872.34, "text": " My best friend is Kylie Jenner, she's a reality TV star Wow, you actually knew Kylie Jenner. I", "tokens": [51280, 1222, 1151, 1277, 307, 39424, 9228, 1193, 11, 750, 311, 257, 4103, 3558, 3543, 3153, 11, 291, 767, 2586, 39424, 9228, 1193, 13, 286, 51600], "temperature": 0.0, "avg_logprob": -0.21716863087245397, "compression_ratio": 1.5621301775147929, "no_speech_prob": 0.0012442892184481025}, {"id": 643, "seek": 487234, "start": 4872.62, "end": 4882.38, "text": " Used to go to Kylie Jenner school. I was in her class for a while. What was she like? Did she pick her nose?", "tokens": [50378, 43237, 281, 352, 281, 39424, 9228, 1193, 1395, 13, 286, 390, 294, 720, 1508, 337, 257, 1339, 13, 708, 390, 750, 411, 30, 2589, 750, 1888, 720, 6690, 30, 50866], "temperature": 0.0, "avg_logprob": -0.19944574925806616, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0009849362540990114}, {"id": 644, "seek": 487234, "start": 4886.02, "end": 4892.900000000001, "text": " She didn't pick her nose, but she was a really pretty kid that makes sense. Do you sometimes wish you were that pretty? Oh", "tokens": [51048, 1240, 994, 380, 1888, 720, 6690, 11, 457, 750, 390, 257, 534, 1238, 1636, 300, 1669, 2020, 13, 1144, 291, 2171, 3172, 291, 645, 300, 1238, 30, 876, 51392], "temperature": 0.0, "avg_logprob": -0.19944574925806616, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0009849362540990114}, {"id": 645, "seek": 487234, "start": 4896.34, "end": 4898.9800000000005, "text": " Sometimes I wish I could smile more and not pick my nose", "tokens": [51564, 4803, 286, 3172, 286, 727, 7563, 544, 293, 406, 1888, 452, 6690, 51696], "temperature": 0.0, "avg_logprob": -0.19944574925806616, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0009849362540990114}, {"id": 646, "seek": 489898, "start": 4899.86, "end": 4902.36, "text": " All right, what do you guys think?", "tokens": [50408, 1057, 558, 11, 437, 360, 291, 1074, 519, 30, 50533], "temperature": 0.0, "avg_logprob": -0.3888522720336914, "compression_ratio": 0.9436619718309859, "no_speech_prob": 0.004467387218028307}, {"id": 647, "seek": 489898, "start": 4914.139999999999, "end": 4916.139999999999, "text": " AGI right", "tokens": [51122, 316, 26252, 558, 51222], "temperature": 0.0, "avg_logprob": -0.3888522720336914, "compression_ratio": 0.9436619718309859, "no_speech_prob": 0.004467387218028307}, {"id": 648, "seek": 489898, "start": 4918.54, "end": 4920.54, "text": " Let's get back to work", "tokens": [51342, 961, 311, 483, 646, 281, 589, 51442], "temperature": 0.0, "avg_logprob": -0.3888522720336914, "compression_ratio": 0.9436619718309859, "no_speech_prob": 0.004467387218028307}, {"id": 649, "seek": 492898, "start": 4928.98, "end": 4930.98, "text": " You", "tokens": [50364, 509, 50464], "temperature": 0.0, "avg_logprob": -0.39014297307923784, "compression_ratio": 1.1875, "no_speech_prob": 0.0013042106293141842}, {"id": 650, "seek": 492898, "start": 4942.86, "end": 4949.259999999999, "text": " How about a cookie with chicken in it with that's mad weird, bro. Why are you coming up with this shit?", "tokens": [51058, 1012, 466, 257, 14417, 365, 4662, 294, 309, 365, 300, 311, 5244, 3657, 11, 2006, 13, 1545, 366, 291, 1348, 493, 365, 341, 4611, 30, 51378], "temperature": 0.0, "avg_logprob": -0.39014297307923784, "compression_ratio": 1.1875, "no_speech_prob": 0.0013042106293141842}, {"id": 651, "seek": 492898, "start": 4950.179999999999, "end": 4952.179999999999, "text": " We're so far from it", "tokens": [51424, 492, 434, 370, 1400, 490, 309, 51524], "temperature": 0.0, "avg_logprob": -0.39014297307923784, "compression_ratio": 1.1875, "no_speech_prob": 0.0013042106293141842}, {"id": 652, "seek": 492898, "start": 4955.94, "end": 4957.94, "text": " Okay", "tokens": [51712, 1033, 51812], "temperature": 0.0, "avg_logprob": -0.39014297307923784, "compression_ratio": 1.1875, "no_speech_prob": 0.0013042106293141842}, {"id": 653, "seek": 495794, "start": 4958.179999999999, "end": 4962.679999999999, "text": " So how do I like put things into chatbots chatbot style?", "tokens": [50376, 407, 577, 360, 286, 411, 829, 721, 666, 5081, 65, 1971, 5081, 18870, 3758, 30, 50601], "temperature": 0.0, "avg_logprob": -0.3098612198462853, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.0002868420851882547}, {"id": 654, "seek": 495794, "start": 4964.58, "end": 4966.58, "text": " How does this stuff work?", "tokens": [50696, 1012, 775, 341, 1507, 589, 30, 50796], "temperature": 0.0, "avg_logprob": -0.3098612198462853, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.0002868420851882547}, {"id": 655, "seek": 495794, "start": 4969.94, "end": 4972.219999999999, "text": " Like what what are the right tokens to use?", "tokens": [50964, 1743, 437, 437, 366, 264, 558, 22667, 281, 764, 30, 51078], "temperature": 0.0, "avg_logprob": -0.3098612198462853, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.0002868420851882547}, {"id": 656, "seek": 495794, "start": 4978.339999999999, "end": 4980.339999999999, "text": " Forgot Google's useless", "tokens": [51384, 1171, 13178, 3329, 311, 14115, 51484], "temperature": 0.0, "avg_logprob": -0.3098612198462853, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.0002868420851882547}, {"id": 657, "seek": 495794, "start": 4982.5, "end": 4986.62, "text": " Here special tokens map wait", "tokens": [51592, 1692, 2121, 22667, 4471, 1699, 51798], "temperature": 0.0, "avg_logprob": -0.3098612198462853, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.0002868420851882547}, {"id": 658, "seek": 498794, "start": 4988.86, "end": 4990.86, "text": " I", "tokens": [50410, 286, 50510], "temperature": 0.0, "avg_logprob": -0.33539922502305775, "compression_ratio": 1.2439024390243902, "no_speech_prob": 0.0003353256615810096}, {"id": 659, "seek": 498794, "start": 4993.86, "end": 5000.259999999999, "text": " How do I like switch speakers you guys know what I'm saying. Oh, here's the template. Okay here", "tokens": [50660, 1012, 360, 286, 411, 3679, 9518, 291, 1074, 458, 437, 286, 478, 1566, 13, 876, 11, 510, 311, 264, 12379, 13, 1033, 510, 50980], "temperature": 0.0, "avg_logprob": -0.33539922502305775, "compression_ratio": 1.2439024390243902, "no_speech_prob": 0.0003353256615810096}, {"id": 660, "seek": 498794, "start": 5004.139999999999, "end": 5006.5, "text": " Here templates for chat models. Oh", "tokens": [51174, 1692, 21165, 337, 5081, 5245, 13, 876, 51292], "temperature": 0.0, "avg_logprob": -0.33539922502305775, "compression_ratio": 1.2439024390243902, "no_speech_prob": 0.0003353256615810096}, {"id": 661, "seek": 498794, "start": 5010.139999999999, "end": 5012.139999999999, "text": " Auto tokenizer what?", "tokens": [51474, 13738, 14862, 6545, 437, 30, 51574], "temperature": 0.0, "avg_logprob": -0.33539922502305775, "compression_ratio": 1.2439024390243902, "no_speech_prob": 0.0003353256615810096}, {"id": 662, "seek": 501794, "start": 5018.94, "end": 5021.94, "text": " I'm start", "tokens": [50414, 286, 478, 722, 50564], "temperature": 0.0, "avg_logprob": -0.5208761795707371, "compression_ratio": 0.9836065573770492, "no_speech_prob": 0.0010648968163877726}, {"id": 663, "seek": 501794, "start": 5030.0599999999995, "end": 5032.0599999999995, "text": " How do I get I'm start", "tokens": [50970, 1012, 360, 286, 483, 286, 478, 722, 51070], "temperature": 0.0, "avg_logprob": -0.5208761795707371, "compression_ratio": 0.9836065573770492, "no_speech_prob": 0.0010648968163877726}, {"id": 664, "seek": 501794, "start": 5034.62, "end": 5036.78, "text": " So like that method on here", "tokens": [51198, 407, 411, 300, 3170, 322, 510, 51306], "temperature": 0.0, "avg_logprob": -0.5208761795707371, "compression_ratio": 0.9836065573770492, "no_speech_prob": 0.0010648968163877726}, {"id": 665, "seek": 504794, "start": 5048.54, "end": 5050.54, "text": " I", "tokens": [50394, 286, 50494], "temperature": 0.0, "avg_logprob": -0.36022993723551433, "compression_ratio": 1.1894736842105262, "no_speech_prob": 0.0013457455206662416}, {"id": 666, "seek": 504794, "start": 5064.0199999999995, "end": 5067.379999999999, "text": " Every mess every model has its own type", "tokens": [51168, 2048, 2082, 633, 2316, 575, 1080, 1065, 2010, 51336], "temperature": 0.0, "avg_logprob": -0.36022993723551433, "compression_ratio": 1.1894736842105262, "no_speech_prob": 0.0013457455206662416}, {"id": 667, "seek": 504794, "start": 5070.82, "end": 5074.78, "text": " Mistral instruct was trained with these tokens, but blender bot was not", "tokens": [51508, 20166, 2155, 7232, 390, 8895, 365, 613, 22667, 11, 457, 24564, 10592, 390, 406, 51706], "temperature": 0.0, "avg_logprob": -0.36022993723551433, "compression_ratio": 1.1894736842105262, "no_speech_prob": 0.0013457455206662416}, {"id": 668, "seek": 507794, "start": 5078.66, "end": 5081.219999999999, "text": " Is this a real token inst", "tokens": [50400, 1119, 341, 257, 957, 14862, 1058, 50528], "temperature": 0.0, "avg_logprob": -0.31126805146535236, "compression_ratio": 1.1549295774647887, "no_speech_prob": 0.00039202539483085275}, {"id": 669, "seek": 507794, "start": 5097.98, "end": 5101.9, "text": " Is it actually just the word inst was that a real token?", "tokens": [51366, 1119, 309, 767, 445, 264, 1349, 1058, 390, 300, 257, 957, 14862, 30, 51562], "temperature": 0.0, "avg_logprob": -0.31126805146535236, "compression_ratio": 1.1549295774647887, "no_speech_prob": 0.00039202539483085275}, {"id": 670, "seek": 510794, "start": 5107.94, "end": 5109.94, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.45732962700628466, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.0009399058180861175}, {"id": 671, "seek": 510794, "start": 5113.0199999999995, "end": 5115.0199999999995, "text": " Guess it's just that", "tokens": [50618, 17795, 309, 311, 445, 300, 50718], "temperature": 0.0, "avg_logprob": -0.45732962700628466, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.0009399058180861175}, {"id": 672, "seek": 510794, "start": 5122.139999999999, "end": 5124.139999999999, "text": " Well based", "tokens": [51074, 1042, 2361, 51174], "temperature": 0.0, "avg_logprob": -0.45732962700628466, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.0009399058180861175}, {"id": 673, "seek": 510794, "start": 5125.98, "end": 5127.98, "text": " Peace ID is out of range", "tokens": [51266, 13204, 7348, 307, 484, 295, 3613, 51366], "temperature": 0.0, "avg_logprob": -0.45732962700628466, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.0009399058180861175}, {"id": 674, "seek": 510794, "start": 5132.78, "end": 5134.78, "text": " You're the right tokenizer", "tokens": [51606, 509, 434, 264, 558, 14862, 6545, 51706], "temperature": 0.0, "avg_logprob": -0.45732962700628466, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.0009399058180861175}, {"id": 675, "seek": 513794, "start": 5137.94, "end": 5141.78, "text": " Peace ID is out of range", "tokens": [50414, 13204, 7348, 307, 484, 295, 3613, 50556], "temperature": 0.0, "avg_logprob": -0.31087740262349445, "compression_ratio": 0.75, "no_speech_prob": 0.005726885981857777}, {"id": 676, "seek": 516794, "start": 5168.94, "end": 5171.299999999999, "text": " Well, well, it was smart the first time", "tokens": [50414, 1042, 11, 731, 11, 309, 390, 4069, 264, 700, 565, 50532], "temperature": 0.0, "avg_logprob": -0.3587759049212346, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.001548701780848205}, {"id": 677, "seek": 516794, "start": 5173.099999999999, "end": 5174.54, "text": " Oh", "tokens": [50622, 876, 50694], "temperature": 0.0, "avg_logprob": -0.3587759049212346, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.001548701780848205}, {"id": 678, "seek": 516794, "start": 5174.54, "end": 5176.82, "text": " No, never mind. It might still be smart", "tokens": [50694, 883, 11, 1128, 1575, 13, 467, 1062, 920, 312, 4069, 50808], "temperature": 0.0, "avg_logprob": -0.3587759049212346, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.001548701780848205}, {"id": 679, "seek": 516794, "start": 5179.419999999999, "end": 5182.66, "text": " Peace ID is out of range what oh", "tokens": [50938, 13204, 7348, 307, 484, 295, 3613, 437, 1954, 51100], "temperature": 0.0, "avg_logprob": -0.3587759049212346, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.001548701780848205}, {"id": 680, "seek": 516794, "start": 5186.179999999999, "end": 5188.48, "text": " Why is the vocab size that?", "tokens": [51276, 1545, 307, 264, 2329, 455, 2744, 300, 30, 51391], "temperature": 0.0, "avg_logprob": -0.3587759049212346, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.001548701780848205}, {"id": 681, "seek": 516794, "start": 5191.66, "end": 5194.66, "text": " Why is that not included in this tokenizer model?", "tokens": [51550, 1545, 307, 300, 406, 5556, 294, 341, 14862, 6545, 2316, 30, 51700], "temperature": 0.0, "avg_logprob": -0.3587759049212346, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.001548701780848205}, {"id": 682, "seek": 519794, "start": 5197.94, "end": 5199.94, "text": " You", "tokens": [50364, 509, 50464], "temperature": 0.0, "avg_logprob": -0.7421733736991882, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.6235424876213074}, {"id": 683, "seek": 522794, "start": 5228.0599999999995, "end": 5230.0599999999995, "text": " What", "tokens": [50370, 708, 50470], "temperature": 0.0, "avg_logprob": -0.26970340781015895, "compression_ratio": 1.4556962025316456, "no_speech_prob": 0.00225166161544621}, {"id": 684, "seek": 522794, "start": 5232.94, "end": 5234.94, "text": " Is 2 plus 2 2", "tokens": [50614, 1119, 568, 1804, 568, 568, 50714], "temperature": 0.0, "avg_logprob": -0.26970340781015895, "compression_ratio": 1.4556962025316456, "no_speech_prob": 0.00225166161544621}, {"id": 685, "seek": 522794, "start": 5235.62, "end": 5242.46, "text": " What is 3 plus 3 6? What is 4 plus 4 8 now? It's done. Okay, I didn't exactly get 2 plus 2 right", "tokens": [50748, 708, 307, 805, 1804, 805, 1386, 30, 708, 307, 1017, 1804, 1017, 1649, 586, 30, 467, 311, 1096, 13, 1033, 11, 286, 994, 380, 2293, 483, 568, 1804, 568, 558, 51090], "temperature": 0.0, "avg_logprob": -0.26970340781015895, "compression_ratio": 1.4556962025316456, "no_speech_prob": 0.00225166161544621}, {"id": 686, "seek": 522794, "start": 5243.78, "end": 5245.78, "text": " I'm doing this wrong", "tokens": [51156, 286, 478, 884, 341, 2085, 51256], "temperature": 0.0, "avg_logprob": -0.26970340781015895, "compression_ratio": 1.4556962025316456, "no_speech_prob": 0.00225166161544621}, {"id": 687, "seek": 522794, "start": 5250.0599999999995, "end": 5256.139999999999, "text": " I have an idea these aren't the actual tokens and it has to do with these secret extra tokens", "tokens": [51470, 286, 362, 364, 1558, 613, 3212, 380, 264, 3539, 22667, 293, 309, 575, 281, 360, 365, 613, 4054, 2857, 22667, 51774], "temperature": 0.0, "avg_logprob": -0.26970340781015895, "compression_ratio": 1.4556962025316456, "no_speech_prob": 0.00225166161544621}, {"id": 688, "seek": 525614, "start": 5257.06, "end": 5263.12, "text": " Llama has some secret extra tokens to marry man. Thank you for gifting subs. We always appreciate that", "tokens": [50410, 32717, 2404, 575, 512, 4054, 2857, 22667, 281, 9747, 587, 13, 1044, 291, 337, 290, 10106, 2090, 13, 492, 1009, 4449, 300, 50713], "temperature": 0.0, "avg_logprob": -0.28799688959696207, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0010986359557136893}, {"id": 689, "seek": 525614, "start": 5266.820000000001, "end": 5272.14, "text": " Yeah back to kindergarten shit guys our cue star can't even saw 2 plus 2. What are we gonna do?", "tokens": [50898, 865, 646, 281, 26671, 4611, 1074, 527, 22656, 3543, 393, 380, 754, 1866, 568, 1804, 568, 13, 708, 366, 321, 799, 360, 30, 51164], "temperature": 0.0, "avg_logprob": -0.28799688959696207, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0010986359557136893}, {"id": 690, "seek": 525614, "start": 5273.42, "end": 5275.42, "text": " We're gonna get some coffee. Let's get some coffee", "tokens": [51228, 492, 434, 799, 483, 512, 4982, 13, 961, 311, 483, 512, 4982, 51328], "temperature": 0.0, "avg_logprob": -0.28799688959696207, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0010986359557136893}, {"id": 691, "seek": 525614, "start": 5277.740000000001, "end": 5283.1, "text": " And then let's learn about secret tokens, okay, there's secret tokens hidden tokens", "tokens": [51444, 400, 550, 718, 311, 1466, 466, 4054, 22667, 11, 1392, 11, 456, 311, 4054, 22667, 7633, 22667, 51712], "temperature": 0.0, "avg_logprob": -0.28799688959696207, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0010986359557136893}, {"id": 692, "seek": 528614, "start": 5286.14, "end": 5292.22, "text": " Nobody uses reserve tokens for instruct tuning your precious for thinking", "tokens": [50414, 9297, 4960, 17824, 22667, 337, 7232, 15164, 428, 12406, 337, 1953, 50668], "temperature": 0.0, "avg_logprob": -0.3560981750488281, "compression_ratio": 1.0735294117647058, "no_speech_prob": 0.0025496426969766617}, {"id": 693, "seek": 531614, "start": 5316.62, "end": 5318.62, "text": " You", "tokens": [50388, 509, 50488], "temperature": 0.0, "avg_logprob": -0.3386146341051374, "compression_ratio": 1.3971631205673758, "no_speech_prob": 0.02000768668949604}, {"id": 694, "seek": 531614, "start": 5330.820000000001, "end": 5335.5, "text": " When actually guys guys we I'm being serious right now, but we need to stop", "tokens": [51098, 1133, 767, 1074, 1074, 321, 286, 478, 885, 3156, 558, 586, 11, 457, 321, 643, 281, 1590, 51332], "temperature": 0.0, "avg_logprob": -0.3386146341051374, "compression_ratio": 1.3971631205673758, "no_speech_prob": 0.02000768668949604}, {"id": 695, "seek": 531614, "start": 5336.62, "end": 5338.62, "text": " When did I put it to there?", "tokens": [51388, 1133, 630, 286, 829, 309, 281, 456, 30, 51488], "temperature": 0.0, "avg_logprob": -0.3386146341051374, "compression_ratio": 1.3971631205673758, "no_speech_prob": 0.02000768668949604}, {"id": 696, "seek": 531614, "start": 5338.820000000001, "end": 5343.62, "text": " It showed that it wasn't aligned with us, but we don't know what the models thinking guys", "tokens": [51498, 467, 4712, 300, 309, 2067, 380, 17962, 365, 505, 11, 457, 321, 500, 380, 458, 437, 264, 5245, 1953, 1074, 51738], "temperature": 0.0, "avg_logprob": -0.3386146341051374, "compression_ratio": 1.3971631205673758, "no_speech_prob": 0.02000768668949604}, {"id": 697, "seek": 534362, "start": 5344.14, "end": 5346.94, "text": " The model could be could be taking over the world", "tokens": [50390, 440, 2316, 727, 312, 727, 312, 1940, 670, 264, 1002, 50530], "temperature": 0.0, "avg_logprob": -0.19906451967027453, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0031720681581646204}, {"id": 698, "seek": 534362, "start": 5347.74, "end": 5349.74, "text": " right now", "tokens": [50570, 558, 586, 50670], "temperature": 0.0, "avg_logprob": -0.19906451967027453, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0031720681581646204}, {"id": 699, "seek": 534362, "start": 5350.0599999999995, "end": 5354.2, "text": " We don't know this is this is we need to hire Helen Toner", "tokens": [50686, 492, 500, 380, 458, 341, 307, 341, 307, 321, 643, 281, 11158, 26294, 11385, 260, 50893], "temperature": 0.0, "avg_logprob": -0.19906451967027453, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0031720681581646204}, {"id": 700, "seek": 534362, "start": 5354.58, "end": 5359.74, "text": " We need to hire an AI ethics review board to review what just happened there", "tokens": [50912, 492, 643, 281, 11158, 364, 7318, 19769, 3131, 3150, 281, 3131, 437, 445, 2011, 456, 51170], "temperature": 0.0, "avg_logprob": -0.19906451967027453, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0031720681581646204}, {"id": 701, "seek": 534362, "start": 5359.78, "end": 5364.94, "text": " We need to slow down. We need to ask the seals if they're okay with rocket launches", "tokens": [51172, 492, 643, 281, 2964, 760, 13, 492, 643, 281, 1029, 264, 32031, 498, 436, 434, 1392, 365, 13012, 31841, 51430], "temperature": 0.0, "avg_logprob": -0.19906451967027453, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0031720681581646204}, {"id": 702, "seek": 537362, "start": 5373.62, "end": 5379.26, "text": " So this is why is this thing suck", "tokens": [50376, 407, 341, 307, 983, 307, 341, 551, 9967, 50646], "temperature": 0.0, "avg_logprob": -0.33363866806030273, "compression_ratio": 1.03125, "no_speech_prob": 0.004067593719810247}, {"id": 703, "seek": 540362, "start": 5404.22, "end": 5418.099999999999, "text": " Okay, what's s and slash s and maybe I need to go like this. This is the mistral one. Yeah", "tokens": [50394, 1033, 11, 437, 311, 262, 293, 17330, 262, 293, 1310, 286, 643, 281, 352, 411, 341, 13, 639, 307, 264, 3544, 2155, 472, 13, 865, 51088], "temperature": 0.0, "avg_logprob": -0.38495094545425906, "compression_ratio": 1.0941176470588236, "no_speech_prob": 0.0029346442315727472}, {"id": 704, "seek": 540362, "start": 5428.14, "end": 5430.14, "text": " Oh", "tokens": [51590, 876, 51690], "temperature": 0.0, "avg_logprob": -0.38495094545425906, "compression_ratio": 1.0941176470588236, "no_speech_prob": 0.0029346442315727472}, {"id": 705, "seek": 543362, "start": 5434.3, "end": 5436.3, "text": " Okay, this is improved", "tokens": [50398, 1033, 11, 341, 307, 9689, 50498], "temperature": 0.0, "avg_logprob": -0.30606700352260047, "compression_ratio": 1.183673469387755, "no_speech_prob": 0.002434120513498783}, {"id": 706, "seek": 543362, "start": 5453.62, "end": 5458.78, "text": " It's got ads in it. Oh open Hermes uses a different template. All right. All right. Let's say", "tokens": [51364, 467, 311, 658, 10342, 294, 309, 13, 876, 1269, 21842, 279, 4960, 257, 819, 12379, 13, 1057, 558, 13, 1057, 558, 13, 961, 311, 584, 51622], "temperature": 0.0, "avg_logprob": -0.30606700352260047, "compression_ratio": 1.183673469387755, "no_speech_prob": 0.002434120513498783}, {"id": 707, "seek": 546362, "start": 5463.62, "end": 5474.58, "text": " Wait, so is it actually the word I am start like is that just a word or is it like", "tokens": [50414, 3802, 11, 370, 307, 309, 767, 264, 1349, 286, 669, 722, 411, 307, 300, 445, 257, 1349, 420, 307, 309, 411, 50912], "temperature": 0.0, "avg_logprob": -0.30119667450586957, "compression_ratio": 1.1388888888888888, "no_speech_prob": 0.0018967801006510854}, {"id": 708, "seek": 549362, "start": 5493.66, "end": 5495.66, "text": " All right, you got me something", "tokens": [50366, 1057, 558, 11, 291, 658, 385, 746, 50466], "temperature": 0.0, "avg_logprob": -0.34474207560221354, "compression_ratio": 1.4685314685314685, "no_speech_prob": 0.00857651699334383}, {"id": 709, "seek": 549362, "start": 5497.3, "end": 5503.62, "text": " Default chat template now, that's llama. I don't think that's right. I think this is right", "tokens": [50548, 9548, 5107, 5081, 12379, 586, 11, 300, 311, 23272, 13, 286, 500, 380, 519, 300, 311, 558, 13, 286, 519, 341, 307, 558, 50864], "temperature": 0.0, "avg_logprob": -0.34474207560221354, "compression_ratio": 1.4685314685314685, "no_speech_prob": 0.00857651699334383}, {"id": 710, "seek": 549362, "start": 5509.38, "end": 5513.7, "text": " Okay, I am start system we miss we need a system message right now", "tokens": [51152, 1033, 11, 286, 669, 722, 1185, 321, 1713, 321, 643, 257, 1185, 3636, 558, 586, 51368], "temperature": 0.0, "avg_logprob": -0.34474207560221354, "compression_ratio": 1.4685314685314685, "no_speech_prob": 0.00857651699334383}, {"id": 711, "seek": 549362, "start": 5516.5, "end": 5518.5, "text": " What is two plus two", "tokens": [51508, 708, 307, 732, 1804, 732, 51608], "temperature": 0.0, "avg_logprob": -0.34474207560221354, "compression_ratio": 1.4685314685314685, "no_speech_prob": 0.00857651699334383}, {"id": 712, "seek": 551850, "start": 5519.5, "end": 5521.5, "text": " Oh", "tokens": [50414, 876, 50514], "temperature": 0.0, "avg_logprob": -0.4325573039504717, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0006070653907954693}, {"id": 713, "seek": 551850, "start": 5522.34, "end": 5528.98, "text": " Mariam and thank you for gifting more subs. Do you have a question if you gift subs you get to ask a question", "tokens": [50556, 2039, 2918, 293, 1309, 291, 337, 290, 10106, 544, 2090, 13, 1144, 291, 362, 257, 1168, 498, 291, 5306, 2090, 291, 483, 281, 1029, 257, 1168, 50888], "temperature": 0.0, "avg_logprob": -0.4325573039504717, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0006070653907954693}, {"id": 714, "seek": 551850, "start": 5530.58, "end": 5537.66, "text": " All right, I'm and I'm star is this is this is this really right like", "tokens": [50968, 1057, 558, 11, 286, 478, 293, 286, 478, 3543, 307, 341, 307, 341, 307, 341, 534, 558, 411, 51322], "temperature": 0.0, "avg_logprob": -0.4325573039504717, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0006070653907954693}, {"id": 715, "seek": 553766, "start": 5537.66, "end": 5539.66, "text": " Oh", "tokens": [50364, 876, 50464], "temperature": 0.0, "avg_logprob": -0.3216228131894712, "compression_ratio": 1.3129770992366412, "no_speech_prob": 0.004829544574022293}, {"id": 716, "seek": 553766, "start": 5547.38, "end": 5549.78, "text": " Okay, that seems like the best so far", "tokens": [50850, 1033, 11, 300, 2544, 411, 264, 1151, 370, 1400, 50970], "temperature": 0.0, "avg_logprob": -0.3216228131894712, "compression_ratio": 1.3129770992366412, "no_speech_prob": 0.004829544574022293}, {"id": 717, "seek": 553766, "start": 5555.5, "end": 5558.18, "text": " Oh, okay, good. I love how verbose this model is", "tokens": [51256, 876, 11, 1392, 11, 665, 13, 286, 959, 577, 9595, 541, 341, 2316, 307, 51390], "temperature": 0.0, "avg_logprob": -0.3216228131894712, "compression_ratio": 1.3129770992366412, "no_speech_prob": 0.004829544574022293}, {"id": 718, "seek": 553766, "start": 5559.34, "end": 5564.3, "text": " Wait, this is actually laughably easy. Oh, you're right. I need a line break there", "tokens": [51448, 3802, 11, 341, 307, 767, 5801, 1188, 1858, 13, 876, 11, 291, 434, 558, 13, 286, 643, 257, 1622, 1821, 456, 51696], "temperature": 0.0, "avg_logprob": -0.3216228131894712, "compression_ratio": 1.3129770992366412, "no_speech_prob": 0.004829544574022293}, {"id": 719, "seek": 556766, "start": 5567.78, "end": 5570.94, "text": " Never mind. I'm locking that bounty for myself. This is too easy", "tokens": [50370, 7344, 1575, 13, 286, 478, 23954, 300, 40773, 337, 2059, 13, 639, 307, 886, 1858, 50528], "temperature": 0.0, "avg_logprob": -0.2667484785381116, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.002472440479323268}, {"id": 720, "seek": 556766, "start": 5571.82, "end": 5575.78, "text": " Someone else should have done this they could have made $200 but instead", "tokens": [50572, 8734, 1646, 820, 362, 1096, 341, 436, 727, 362, 1027, 1848, 7629, 457, 2602, 50770], "temperature": 0.0, "avg_logprob": -0.2667484785381116, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.002472440479323268}, {"id": 721, "seek": 556766, "start": 5590.5, "end": 5591.78, "text": " Okay", "tokens": [51506, 1033, 51570], "temperature": 0.0, "avg_logprob": -0.2667484785381116, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.002472440479323268}, {"id": 722, "seek": 559178, "start": 5591.82, "end": 5598.38, "text": " Let's add a system message, right? I should really like let me just write a little something to generate these prompts", "tokens": [50366, 961, 311, 909, 257, 1185, 3636, 11, 558, 30, 286, 820, 534, 411, 718, 385, 445, 2464, 257, 707, 746, 281, 8460, 613, 41095, 50694], "temperature": 0.0, "avg_logprob": -0.3256425221761068, "compression_ratio": 1.3064516129032258, "no_speech_prob": 0.006192198023200035}, {"id": 723, "seek": 559178, "start": 5601.0599999999995, "end": 5603.0599999999995, "text": " Tuple", "tokens": [50828, 7836, 781, 50928], "temperature": 0.0, "avg_logprob": -0.3256425221761068, "compression_ratio": 1.3064516129032258, "no_speech_prob": 0.006192198023200035}, {"id": 724, "seek": 559178, "start": 5603.38, "end": 5606.0599999999995, "text": " List tuple user", "tokens": [50944, 17668, 2604, 781, 4195, 51078], "temperature": 0.0, "avg_logprob": -0.3256425221761068, "compression_ratio": 1.3064516129032258, "no_speech_prob": 0.006192198023200035}, {"id": 725, "seek": 559178, "start": 5607.98, "end": 5609.98, "text": " What is two plus two?", "tokens": [51174, 708, 307, 732, 1804, 732, 30, 51274], "temperature": 0.0, "avg_logprob": -0.3256425221761068, "compression_ratio": 1.3064516129032258, "no_speech_prob": 0.006192198023200035}, {"id": 726, "seek": 562178, "start": 5621.78, "end": 5623.78, "text": " Prompt", "tokens": [50364, 15833, 662, 50464], "temperature": 0.0, "avg_logprob": -0.7651708827299231, "compression_ratio": 0.7777777777777778, "no_speech_prob": 0.0510583370923996}, {"id": 727, "seek": 562178, "start": 5639.0599999999995, "end": 5643.179999999999, "text": " I'm start k slash n", "tokens": [51228, 286, 478, 722, 350, 17330, 297, 51434], "temperature": 0.0, "avg_logprob": -0.7651708827299231, "compression_ratio": 0.7777777777777778, "no_speech_prob": 0.0510583370923996}, {"id": 728, "seek": 562178, "start": 5645.42, "end": 5647.42, "text": " V", "tokens": [51546, 691, 51646], "temperature": 0.0, "avg_logprob": -0.7651708827299231, "compression_ratio": 0.7777777777777778, "no_speech_prob": 0.0510583370923996}, {"id": 729, "seek": 564742, "start": 5648.34, "end": 5652.7, "text": " I'm and slash", "tokens": [50410, 286, 478, 293, 17330, 50628], "temperature": 0.0, "avg_logprob": -0.859576770237514, "compression_ratio": 0.9024390243902439, "no_speech_prob": 0.003428142750635743}, {"id": 730, "seek": 564742, "start": 5674.38, "end": 5676.38, "text": " And I'm start assistant", "tokens": [51712, 400, 286, 478, 722, 10994, 51812], "temperature": 0.0, "avg_logprob": -0.859576770237514, "compression_ratio": 0.9024390243902439, "no_speech_prob": 0.003428142750635743}, {"id": 731, "seek": 567742, "start": 5677.42, "end": 5679.42, "text": " and", "tokens": [50364, 293, 50464], "temperature": 0.0, "avg_logprob": -0.8845166524251302, "compression_ratio": 0.9473684210526315, "no_speech_prob": 0.012238006107509136}, {"id": 732, "seek": 567742, "start": 5682.82, "end": 5684.82, "text": " The word user prompt", "tokens": [50634, 440, 1349, 4195, 12391, 50734], "temperature": 0.0, "avg_logprob": -0.8845166524251302, "compression_ratio": 0.9473684210526315, "no_speech_prob": 0.012238006107509136}, {"id": 733, "seek": 567742, "start": 5688.06, "end": 5690.06, "text": " Code prompt", "tokens": [50896, 15549, 12391, 50996], "temperature": 0.0, "avg_logprob": -0.8845166524251302, "compression_ratio": 0.9473684210526315, "no_speech_prob": 0.012238006107509136}, {"id": 734, "seek": 569006, "start": 5690.06, "end": 5692.06, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.31724923849105835, "compression_ratio": 1.247191011235955, "no_speech_prob": 0.005059416871517897}, {"id": 735, "seek": 569006, "start": 5706.06, "end": 5709.22, "text": " What why did you why did you not exit when you were supposed to exit?", "tokens": [51164, 708, 983, 630, 291, 983, 630, 291, 406, 11043, 562, 291, 645, 3442, 281, 11043, 30, 51322], "temperature": 0.0, "avg_logprob": -0.31724923849105835, "compression_ratio": 1.247191011235955, "no_speech_prob": 0.005059416871517897}, {"id": 736, "seek": 569006, "start": 5711.18, "end": 5713.18, "text": " Did I forget more returns or something?", "tokens": [51420, 2589, 286, 2870, 544, 11247, 420, 746, 30, 51520], "temperature": 0.0, "avg_logprob": -0.31724923849105835, "compression_ratio": 1.247191011235955, "no_speech_prob": 0.005059416871517897}, {"id": 737, "seek": 572006, "start": 5721.06, "end": 5726.4400000000005, "text": " Why is it now putting I'm and", "tokens": [50414, 1545, 307, 309, 586, 3372, 286, 478, 293, 50683], "temperature": 0.0, "avg_logprob": -0.413542417379526, "compression_ratio": 0.987012987012987, "no_speech_prob": 0.0006562902708537877}, {"id": 738, "seek": 572006, "start": 5739.5, "end": 5743.14, "text": " Guys guys, this is Q star. I think we found it", "tokens": [51336, 7855, 1074, 11, 341, 307, 1249, 3543, 13, 286, 519, 321, 1352, 309, 51518], "temperature": 0.0, "avg_logprob": -0.413542417379526, "compression_ratio": 0.987012987012987, "no_speech_prob": 0.0006562902708537877}, {"id": 739, "seek": 575006, "start": 5750.42, "end": 5752.42, "text": " I", "tokens": [50382, 286, 50482], "temperature": 0.0, "avg_logprob": -0.2158584247935902, "compression_ratio": 1.5419847328244274, "no_speech_prob": 0.0033241217024624348}, {"id": 740, "seek": 575006, "start": 5767.14, "end": 5770.46, "text": " Not locking the bounty someone should do this but someone should do a good job", "tokens": [51218, 1726, 23954, 264, 40773, 1580, 820, 360, 341, 457, 1580, 820, 360, 257, 665, 1691, 51384], "temperature": 0.0, "avg_logprob": -0.2158584247935902, "compression_ratio": 1.5419847328244274, "no_speech_prob": 0.0033241217024624348}, {"id": 741, "seek": 575006, "start": 5770.46, "end": 5773.18, "text": " I want a good job on that bounty. Oh, okay here. I'm and", "tokens": [51384, 286, 528, 257, 665, 1691, 322, 300, 40773, 13, 876, 11, 1392, 510, 13, 286, 478, 293, 51520], "temperature": 0.0, "avg_logprob": -0.2158584247935902, "compression_ratio": 1.5419847328244274, "no_speech_prob": 0.0033241217024624348}, {"id": 742, "seek": 575006, "start": 5773.780000000001, "end": 5777.06, "text": " How come sometime it finishes the stream and sometime it doesn't", "tokens": [51550, 1012, 808, 15053, 309, 23615, 264, 4309, 293, 15053, 309, 1177, 380, 51714], "temperature": 0.0, "avg_logprob": -0.2158584247935902, "compression_ratio": 1.5419847328244274, "no_speech_prob": 0.0033241217024624348}, {"id": 743, "seek": 577706, "start": 5777.620000000001, "end": 5783.9400000000005, "text": " Maybe our temperature is too high. Let's try a less temperature", "tokens": [50392, 2704, 527, 4292, 307, 886, 1090, 13, 961, 311, 853, 257, 1570, 4292, 50708], "temperature": 0.0, "avg_logprob": -0.41420650482177734, "compression_ratio": 1.0547945205479452, "no_speech_prob": 0.0007915779133327305}, {"id": 744, "seek": 577706, "start": 5792.9400000000005, "end": 5794.9400000000005, "text": " Yo", "tokens": [51158, 7616, 51258], "temperature": 0.0, "avg_logprob": -0.41420650482177734, "compression_ratio": 1.0547945205479452, "no_speech_prob": 0.0007915779133327305}, {"id": 745, "seek": 577706, "start": 5801.820000000001, "end": 5804.38, "text": " Okay, zero", "tokens": [51602, 1033, 11, 4018, 51730], "temperature": 0.0, "avg_logprob": -0.41420650482177734, "compression_ratio": 1.0547945205479452, "no_speech_prob": 0.0007915779133327305}, {"id": 746, "seek": 580706, "start": 5807.9800000000005, "end": 5817.46, "text": " Guys why is it why is it going off into this language AI garbage?", "tokens": [50410, 7855, 983, 307, 309, 983, 307, 309, 516, 766, 666, 341, 2856, 7318, 14150, 30, 50884], "temperature": 0.0, "avg_logprob": -0.2848613015536604, "compression_ratio": 1.1477272727272727, "no_speech_prob": 0.00041727139614522457}, {"id": 747, "seek": 580706, "start": 5822.740000000001, "end": 5825.580000000001, "text": " Or should I just stop after I'm and", "tokens": [51148, 1610, 820, 286, 445, 1590, 934, 286, 478, 293, 51290], "temperature": 0.0, "avg_logprob": -0.2848613015536604, "compression_ratio": 1.1477272727272727, "no_speech_prob": 0.00041727139614522457}, {"id": 748, "seek": 583706, "start": 5837.5, "end": 5839.5, "text": " I", "tokens": [50386, 286, 50486], "temperature": 0.0, "avg_logprob": -0.3669362295241583, "compression_ratio": 1.2897196261682242, "no_speech_prob": 0.001477861893363297}, {"id": 749, "seek": 583706, "start": 5843.780000000001, "end": 5846.1, "text": " Am I using torture tiny grad this is all tiny grad", "tokens": [50700, 2012, 286, 1228, 20711, 5870, 2771, 341, 307, 439, 5870, 2771, 50816], "temperature": 0.0, "avg_logprob": -0.3669362295241583, "compression_ratio": 1.2897196261682242, "no_speech_prob": 0.001477861893363297}, {"id": 750, "seek": 583706, "start": 5849.900000000001, "end": 5851.900000000001, "text": " I don't really understand this", "tokens": [51006, 286, 500, 380, 534, 1223, 341, 51106], "temperature": 0.0, "avg_logprob": -0.3669362295241583, "compression_ratio": 1.2897196261682242, "no_speech_prob": 0.001477861893363297}, {"id": 751, "seek": 583706, "start": 5855.740000000001, "end": 5857.740000000001, "text": " All right, maybe we should add a system prompt", "tokens": [51298, 1057, 558, 11, 1310, 321, 820, 909, 257, 1185, 12391, 51398], "temperature": 0.0, "avg_logprob": -0.3669362295241583, "compression_ratio": 1.2897196261682242, "no_speech_prob": 0.001477861893363297}, {"id": 752, "seek": 583706, "start": 5861.1, "end": 5863.1, "text": " You are", "tokens": [51566, 509, 366, 51666], "temperature": 0.0, "avg_logprob": -0.3669362295241583, "compression_ratio": 1.2897196261682242, "no_speech_prob": 0.001477861893363297}, {"id": 753, "seek": 586310, "start": 5864.1, "end": 5870.740000000001, "text": " Gary Gary is a useful. No, no, we were pretty used Gary. What should we name him?", "tokens": [50414, 13788, 13788, 307, 257, 4420, 13, 883, 11, 572, 11, 321, 645, 1238, 1143, 13788, 13, 708, 820, 321, 1315, 796, 30, 50746], "temperature": 0.0, "avg_logprob": -0.2895933281291615, "compression_ratio": 1.2894736842105263, "no_speech_prob": 0.0011694503482431173}, {"id": 754, "seek": 586310, "start": 5874.14, "end": 5876.14, "text": " Fred", "tokens": [50916, 10112, 51016], "temperature": 0.0, "avg_logprob": -0.2895933281291615, "compression_ratio": 1.2894736842105263, "no_speech_prob": 0.0011694503482431173}, {"id": 755, "seek": 586310, "start": 5876.860000000001, "end": 5884.42, "text": " Fred is a useful assistant I spell that word right I did not", "tokens": [51052, 10112, 307, 257, 4420, 10994, 286, 9827, 300, 1349, 558, 286, 630, 406, 51430], "temperature": 0.0, "avg_logprob": -0.2895933281291615, "compression_ratio": 1.2894736842105263, "no_speech_prob": 0.0011694503482431173}, {"id": 756, "seek": 588442, "start": 5884.42, "end": 5886.42, "text": " Fred", "tokens": [50364, 10112, 50464], "temperature": 0.0, "avg_logprob": -0.43961515426635744, "compression_ratio": 1.2710280373831775, "no_speech_prob": 0.0064871893264353275}, {"id": 757, "seek": 588442, "start": 5891.22, "end": 5896.46, "text": " Outputs the answer and stops talking", "tokens": [50704, 5925, 2582, 82, 264, 1867, 293, 10094, 1417, 50966], "temperature": 0.0, "avg_logprob": -0.43961515426635744, "compression_ratio": 1.2710280373831775, "no_speech_prob": 0.0064871893264353275}, {"id": 758, "seek": 588442, "start": 5908.26, "end": 5914.06, "text": " All right, all right, I find we'll call him Q fine fine fine you are Q Q is a useful assistant", "tokens": [51556, 1057, 558, 11, 439, 558, 11, 286, 915, 321, 603, 818, 796, 1249, 2489, 2489, 2489, 291, 366, 1249, 1249, 307, 257, 4420, 10994, 51846], "temperature": 0.0, "avg_logprob": -0.43961515426635744, "compression_ratio": 1.2710280373831775, "no_speech_prob": 0.0064871893264353275}, {"id": 759, "seek": 591442, "start": 5914.9, "end": 5917.46, "text": " You outputs the answer and stops talking", "tokens": [50388, 509, 23930, 264, 1867, 293, 10094, 1417, 50516], "temperature": 0.0, "avg_logprob": -0.2294541464911567, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0006361551932059228}, {"id": 760, "seek": 591442, "start": 5918.26, "end": 5924.34, "text": " Quentin wait, you know what you donated sub you get to name him. Congratulations. His name is Quentin. I", "tokens": [50556, 2326, 47300, 1699, 11, 291, 458, 437, 291, 23723, 1422, 291, 483, 281, 1315, 796, 13, 9694, 13, 2812, 1315, 307, 2326, 47300, 13, 286, 50860], "temperature": 0.0, "avg_logprob": -0.2294541464911567, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0006361551932059228}, {"id": 761, "seek": 591442, "start": 5927.38, "end": 5932.82, "text": " Knew a Quentin ones. He was hanging out in San Francisco and some guys were on the street smoking", "tokens": [51012, 591, 7686, 257, 2326, 47300, 2306, 13, 634, 390, 8345, 484, 294, 5271, 12279, 293, 512, 1074, 645, 322, 264, 4838, 14055, 51284], "temperature": 0.0, "avg_logprob": -0.2294541464911567, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0006361551932059228}, {"id": 762, "seek": 591442, "start": 5932.82, "end": 5937.58, "text": " He's like, oh, let me get a hint of that. He thought it was weed. It was crack. That's a real Quentin story. Oh", "tokens": [51284, 634, 311, 411, 11, 1954, 11, 718, 385, 483, 257, 12075, 295, 300, 13, 634, 1194, 309, 390, 20852, 13, 467, 390, 6226, 13, 663, 311, 257, 957, 2326, 47300, 1657, 13, 876, 51522], "temperature": 0.0, "avg_logprob": -0.2294541464911567, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0006361551932059228}, {"id": 763, "seek": 591442, "start": 5938.42, "end": 5940.42, "text": " I wouldn't make up Quentin story like that", "tokens": [51564, 286, 2759, 380, 652, 493, 2326, 47300, 1657, 411, 300, 51664], "temperature": 0.0, "avg_logprob": -0.2294541464911567, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0006361551932059228}, {"id": 764, "seek": 594442, "start": 5944.66, "end": 5950.26, "text": " Oh, why would the system ask what the capital of France is?", "tokens": [50376, 876, 11, 983, 576, 264, 1185, 1029, 437, 264, 4238, 295, 6190, 307, 30, 50656], "temperature": 0.0, "avg_logprob": -0.3034389628920444, "compression_ratio": 1.2393162393162394, "no_speech_prob": 0.0013248976320028305}, {"id": 765, "seek": 594442, "start": 5961.62, "end": 5966.1, "text": " We could stop at I'm and I think maybe we want to do that. How do I do this in llama?", "tokens": [51224, 492, 727, 1590, 412, 286, 478, 293, 286, 519, 1310, 321, 528, 281, 360, 300, 13, 1012, 360, 286, 360, 341, 294, 23272, 30, 51448], "temperature": 0.0, "avg_logprob": -0.3034389628920444, "compression_ratio": 1.2393162393162394, "no_speech_prob": 0.0013248976320028305}, {"id": 766, "seek": 597442, "start": 5974.42, "end": 5976.42, "text": " Oh", "tokens": [50364, 876, 50464], "temperature": 0.0, "avg_logprob": -0.6451016206007737, "compression_ratio": 0.7714285714285715, "no_speech_prob": 0.005553731694817543}, {"id": 767, "seek": 597442, "start": 5990.9, "end": 5992.9, "text": " Okay, that's pretty good", "tokens": [51188, 1033, 11, 300, 311, 1238, 665, 51288], "temperature": 0.0, "avg_logprob": -0.6451016206007737, "compression_ratio": 0.7714285714285715, "no_speech_prob": 0.005553731694817543}, {"id": 768, "seek": 599290, "start": 5992.9, "end": 6008.099999999999, "text": " All right, let's jack up the temperature and Steve Quentin still reliable", "tokens": [50364, 1057, 558, 11, 718, 311, 7109, 493, 264, 4292, 293, 7466, 2326, 47300, 920, 12924, 51124], "temperature": 0.0, "avg_logprob": -0.4132354442889874, "compression_ratio": 1.2393162393162394, "no_speech_prob": 0.0012447875924408436}, {"id": 769, "seek": 599290, "start": 6015.0599999999995, "end": 6019.299999999999, "text": " The answer is four, but it didn't output I'm in that time output at EOS", "tokens": [51472, 440, 1867, 307, 1451, 11, 457, 309, 994, 380, 5598, 286, 478, 294, 300, 565, 5598, 412, 462, 4367, 51684], "temperature": 0.0, "avg_logprob": -0.4132354442889874, "compression_ratio": 1.2393162393162394, "no_speech_prob": 0.0012447875924408436}, {"id": 770, "seek": 602290, "start": 6022.98, "end": 6024.98, "text": " Seems to reliably do", "tokens": [50368, 22524, 281, 49927, 360, 50468], "temperature": 0.0, "avg_logprob": -0.14613032341003418, "compression_ratio": 1.4453125, "no_speech_prob": 0.0014103006105870008}, {"id": 771, "seek": 602290, "start": 6028.66, "end": 6030.66, "text": " That time it did I'm in", "tokens": [50652, 663, 565, 309, 630, 286, 478, 294, 50752], "temperature": 0.0, "avg_logprob": -0.14613032341003418, "compression_ratio": 1.4453125, "no_speech_prob": 0.0014103006105870008}, {"id": 772, "seek": 602290, "start": 6031.78, "end": 6033.78, "text": " Yeah, I", "tokens": [50808, 865, 11, 286, 50908], "temperature": 0.0, "avg_logprob": -0.14613032341003418, "compression_ratio": 1.4453125, "no_speech_prob": 0.0014103006105870008}, {"id": 773, "seek": 602290, "start": 6033.78, "end": 6035.139999999999, "text": " like", "tokens": [50908, 411, 50976], "temperature": 0.0, "avg_logprob": -0.14613032341003418, "compression_ratio": 1.4453125, "no_speech_prob": 0.0014103006105870008}, {"id": 774, "seek": 602290, "start": 6035.139999999999, "end": 6039.62, "text": " Is that a real token or does it actually just put it in like that?", "tokens": [50976, 1119, 300, 257, 957, 14862, 420, 775, 309, 767, 445, 829, 309, 294, 411, 300, 30, 51200], "temperature": 0.0, "avg_logprob": -0.14613032341003418, "compression_ratio": 1.4453125, "no_speech_prob": 0.0014103006105870008}, {"id": 775, "seek": 602290, "start": 6042.339999999999, "end": 6047.139999999999, "text": " Is this right or is that like a secret like it can't be that", "tokens": [51336, 1119, 341, 558, 420, 307, 300, 411, 257, 4054, 411, 309, 393, 380, 312, 300, 51576], "temperature": 0.0, "avg_logprob": -0.14613032341003418, "compression_ratio": 1.4453125, "no_speech_prob": 0.0014103006105870008}, {"id": 776, "seek": 604714, "start": 6047.62, "end": 6049.62, "text": " I", "tokens": [50388, 286, 50488], "temperature": 0.0, "avg_logprob": -0.3317433816415292, "compression_ratio": 1.0, "no_speech_prob": 0.0003682891256175935}, {"id": 777, "seek": 604714, "start": 6051.3, "end": 6053.3, "text": " Can't actually be this", "tokens": [50572, 1664, 380, 767, 312, 341, 50672], "temperature": 0.0, "avg_logprob": -0.3317433816415292, "compression_ratio": 1.0, "no_speech_prob": 0.0003682891256175935}, {"id": 778, "seek": 604714, "start": 6064.26, "end": 6066.26, "text": " We can check if it's encoded on a single token. Hey", "tokens": [51220, 492, 393, 1520, 498, 309, 311, 2058, 12340, 322, 257, 2167, 14862, 13, 1911, 51320], "temperature": 0.0, "avg_logprob": -0.3317433816415292, "compression_ratio": 1.0, "no_speech_prob": 0.0003682891256175935}, {"id": 779, "seek": 607714, "start": 6078.1, "end": 6080.34, "text": " No, it's a bajillion tokens", "tokens": [50412, 883, 11, 309, 311, 257, 23589, 11836, 22667, 50524], "temperature": 0.0, "avg_logprob": -0.2911161104838053, "compression_ratio": 1.164835164835165, "no_speech_prob": 0.0018385575385764241}, {"id": 780, "seek": 607714, "start": 6098.900000000001, "end": 6106.1, "text": " At a trailing slash and after assistant that shouldn't matter I mean I can but", "tokens": [51452, 1711, 257, 944, 4883, 17330, 293, 934, 10994, 300, 4659, 380, 1871, 286, 914, 286, 393, 457, 51812], "temperature": 0.0, "avg_logprob": -0.2911161104838053, "compression_ratio": 1.164835164835165, "no_speech_prob": 0.0018385575385764241}, {"id": 781, "seek": 610714, "start": 6108.1, "end": 6110.1, "text": " It doesn't matter", "tokens": [50412, 467, 1177, 380, 1871, 50512], "temperature": 0.0, "avg_logprob": -0.1714243178671979, "compression_ratio": 1.431578947368421, "no_speech_prob": 0.0006263171089813113}, {"id": 782, "seek": 610714, "start": 6111.780000000001, "end": 6113.780000000001, "text": " No, no, no, no it can't be this", "tokens": [50596, 883, 11, 572, 11, 572, 11, 572, 309, 393, 380, 312, 341, 50696], "temperature": 0.0, "avg_logprob": -0.1714243178671979, "compression_ratio": 1.431578947368421, "no_speech_prob": 0.0006263171089813113}, {"id": 783, "seek": 610714, "start": 6120.58, "end": 6122.58, "text": " It can't", "tokens": [51036, 467, 393, 380, 51136], "temperature": 0.0, "avg_logprob": -0.1714243178671979, "compression_ratio": 1.431578947368421, "no_speech_prob": 0.0006263171089813113}, {"id": 784, "seek": 610714, "start": 6131.06, "end": 6136.900000000001, "text": " No, no, no, but it can't literally be this huge multi token wasteful encoding", "tokens": [51560, 883, 11, 572, 11, 572, 11, 457, 309, 393, 380, 3736, 312, 341, 2603, 4825, 14862, 5964, 906, 43430, 51852], "temperature": 0.0, "avg_logprob": -0.1714243178671979, "compression_ratio": 1.431578947368421, "no_speech_prob": 0.0006263171089813113}, {"id": 785, "seek": 613714, "start": 6137.14, "end": 6139.14, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.3141440638789424, "compression_ratio": 1.0133333333333334, "no_speech_prob": 0.0004238728724885732}, {"id": 786, "seek": 613714, "start": 6146.5, "end": 6148.5, "text": " Could we get tech me I'm in here", "tokens": [50832, 7497, 321, 483, 7553, 385, 286, 478, 294, 510, 50932], "temperature": 0.0, "avg_logprob": -0.3141440638789424, "compression_ratio": 1.0133333333333334, "no_speech_prob": 0.0004238728724885732}, {"id": 787, "seek": 613714, "start": 6153.46, "end": 6155.46, "text": " All right, how do we print all the tokens", "tokens": [51180, 1057, 558, 11, 577, 360, 321, 4482, 439, 264, 22667, 51280], "temperature": 0.0, "avg_logprob": -0.3141440638789424, "compression_ratio": 1.0133333333333334, "no_speech_prob": 0.0004238728724885732}, {"id": 788, "seek": 616714, "start": 6167.14, "end": 6169.14, "text": " So", "tokens": [50364, 407, 50464], "temperature": 0.0, "avg_logprob": -0.6446014881134033, "compression_ratio": 0.2, "no_speech_prob": 0.2595360577106476}, {"id": 789, "seek": 616914, "start": 6169.14, "end": 6171.14, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.4893687321589543, "compression_ratio": 0.8780487804878049, "no_speech_prob": 0.017439711838960648}, {"id": 790, "seek": 616914, "start": 6194.58, "end": 6196.58, "text": " Probably is the secret tokens then", "tokens": [51636, 9210, 307, 264, 4054, 22667, 550, 51736], "temperature": 0.0, "avg_logprob": -0.4893687321589543, "compression_ratio": 0.8780487804878049, "no_speech_prob": 0.017439711838960648}, {"id": 791, "seek": 619658, "start": 6197.54, "end": 6201.72, "text": " I mean there's three extra tokens or I guess two extra tokens, right?", "tokens": [50412, 286, 914, 456, 311, 1045, 2857, 22667, 420, 286, 2041, 732, 2857, 22667, 11, 558, 30, 50621], "temperature": 0.0, "avg_logprob": -0.22233755080426326, "compression_ratio": 1.4028776978417266, "no_speech_prob": 0.0007208067690953612}, {"id": 792, "seek": 619658, "start": 6203.78, "end": 6205.78, "text": " 3200 and", "tokens": [50724, 805, 7629, 293, 50824], "temperature": 0.0, "avg_logprob": -0.22233755080426326, "compression_ratio": 1.4028776978417266, "no_speech_prob": 0.0007208067690953612}, {"id": 793, "seek": 619658, "start": 6206.74, "end": 6208.74, "text": " Yeah", "tokens": [50872, 865, 50972], "temperature": 0.0, "avg_logprob": -0.22233755080426326, "compression_ratio": 1.4028776978417266, "no_speech_prob": 0.0007208067690953612}, {"id": 794, "seek": 619658, "start": 6209.46, "end": 6214.34, "text": " If I was doing this that's how I'd do it. Let's just try it", "tokens": [51008, 759, 286, 390, 884, 341, 300, 311, 577, 286, 1116, 360, 309, 13, 961, 311, 445, 853, 309, 51252], "temperature": 0.0, "avg_logprob": -0.22233755080426326, "compression_ratio": 1.4028776978417266, "no_speech_prob": 0.0007208067690953612}, {"id": 795, "seek": 619658, "start": 6221.22, "end": 6223.46, "text": " That has to be what the two secret tokens are right", "tokens": [51596, 663, 575, 281, 312, 437, 264, 732, 4054, 22667, 366, 558, 51708], "temperature": 0.0, "avg_logprob": -0.22233755080426326, "compression_ratio": 1.4028776978417266, "no_speech_prob": 0.0007208067690953612}, {"id": 796, "seek": 622346, "start": 6223.46, "end": 6225.46, "text": " Start and end", "tokens": [50364, 6481, 293, 917, 50464], "temperature": 0.0, "avg_logprob": -0.40190550860236673, "compression_ratio": 1.3392857142857142, "no_speech_prob": 0.001987635623663664}, {"id": 797, "seek": 622346, "start": 6232.1, "end": 6236.42, "text": " This is what did I download open hermes v. Shit", "tokens": [50796, 639, 307, 437, 630, 286, 5484, 1269, 720, 5814, 371, 13, 19593, 51012], "temperature": 0.0, "avg_logprob": -0.40190550860236673, "compression_ratio": 1.3392857142857142, "no_speech_prob": 0.001987635623663664}, {"id": 798, "seek": 622346, "start": 6237.46, "end": 6240.18, "text": " G guff for q m w", "tokens": [51064, 460, 290, 1245, 337, 9505, 275, 261, 51200], "temperature": 0.0, "avg_logprob": -0.40190550860236673, "compression_ratio": 1.3392857142857142, "no_speech_prob": 0.001987635623663664}, {"id": 799, "seek": 622346, "start": 6241.3, "end": 6245.62, "text": " No, I downloaded open hermes this one. Oh, it just came out fresh", "tokens": [51256, 883, 11, 286, 21748, 1269, 720, 5814, 341, 472, 13, 876, 11, 309, 445, 1361, 484, 4451, 51472], "temperature": 0.0, "avg_logprob": -0.40190550860236673, "compression_ratio": 1.3392857142857142, "no_speech_prob": 0.001987635623663664}, {"id": 800, "seek": 622346, "start": 6246.58, "end": 6248.58, "text": " fresh", "tokens": [51520, 4451, 51620], "temperature": 0.0, "avg_logprob": -0.40190550860236673, "compression_ratio": 1.3392857142857142, "no_speech_prob": 0.001987635623663664}, {"id": 801, "seek": 625346, "start": 6253.86, "end": 6257.86, "text": " Yeah, okay, um", "tokens": [50384, 865, 11, 1392, 11, 1105, 50584], "temperature": 0.0, "avg_logprob": -0.45604184778725226, "compression_ratio": 1.268041237113402, "no_speech_prob": 0.003272716887295246}, {"id": 802, "seek": 625346, "start": 6264.58, "end": 6267.3, "text": " Oh, you can download the tokenizer.json", "tokens": [50920, 876, 11, 291, 393, 5484, 264, 14862, 6545, 13, 73, 3015, 51056], "temperature": 0.0, "avg_logprob": -0.45604184778725226, "compression_ratio": 1.268041237113402, "no_speech_prob": 0.003272716887295246}, {"id": 803, "seek": 625346, "start": 6268.58, "end": 6275.14, "text": " Wait, but I downloaded the tokenizer.maw. Oh here special tokens map", "tokens": [51120, 3802, 11, 457, 286, 21748, 264, 14862, 6545, 13, 76, 1607, 13, 876, 510, 2121, 22667, 4471, 51448], "temperature": 0.0, "avg_logprob": -0.45604184778725226, "compression_ratio": 1.268041237113402, "no_speech_prob": 0.003272716887295246}, {"id": 804, "seek": 627514, "start": 6275.54, "end": 6277.54, "text": " Interesting", "tokens": [50384, 14711, 50484], "temperature": 0.0, "avg_logprob": -0.3281877105300491, "compression_ratio": 1.174757281553398, "no_speech_prob": 0.0018385142320767045}, {"id": 805, "seek": 627514, "start": 6281.54, "end": 6286.900000000001, "text": " Interesting, okay, I'm and is the EOS token", "tokens": [50684, 14711, 11, 1392, 11, 286, 478, 293, 307, 264, 462, 4367, 14862, 50952], "temperature": 0.0, "avg_logprob": -0.3281877105300491, "compression_ratio": 1.174757281553398, "no_speech_prob": 0.0018385142320767045}, {"id": 806, "seek": 627514, "start": 6290.18, "end": 6292.58, "text": " I think I can feed these in somehow", "tokens": [51116, 286, 519, 286, 393, 3154, 613, 294, 6063, 51236], "temperature": 0.0, "avg_logprob": -0.3281877105300491, "compression_ratio": 1.174757281553398, "no_speech_prob": 0.0018385142320767045}, {"id": 807, "seek": 627514, "start": 6302.34, "end": 6304.34, "text": " But I don't know about starch", "tokens": [51724, 583, 286, 500, 380, 458, 466, 24748, 51824], "temperature": 0.0, "avg_logprob": -0.3281877105300491, "compression_ratio": 1.174757281553398, "no_speech_prob": 0.0018385142320767045}, {"id": 808, "seek": 630514, "start": 6306.02, "end": 6314.02, "text": " Oh, here we go. Look. Yeah. Yeah here. We have this tokenizer config.json. Okay. It is exactly what we thought it was", "tokens": [50408, 876, 11, 510, 321, 352, 13, 2053, 13, 865, 13, 865, 510, 13, 492, 362, 341, 14862, 6545, 6662, 13, 73, 3015, 13, 1033, 13, 467, 307, 2293, 437, 321, 1194, 309, 390, 50808], "temperature": 0.0, "avg_logprob": -0.19565044540956797, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.000767225050367415}, {"id": 809, "seek": 630514, "start": 6316.26, "end": 6318.26, "text": " Oh, okay. Yeah, this is what we want", "tokens": [50920, 876, 11, 1392, 13, 865, 11, 341, 307, 437, 321, 528, 51020], "temperature": 0.0, "avg_logprob": -0.19565044540956797, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.000767225050367415}, {"id": 810, "seek": 630514, "start": 6319.3, "end": 6321.780000000001, "text": " Okay, never mind. It's not as stupid as we thought it was", "tokens": [51072, 1033, 11, 1128, 1575, 13, 467, 311, 406, 382, 6631, 382, 321, 1194, 309, 390, 51196], "temperature": 0.0, "avg_logprob": -0.19565044540956797, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.000767225050367415}, {"id": 811, "seek": 630514, "start": 6323.14, "end": 6327.54, "text": " Uh, how do I load a tokenizer config and sentence piece processor?", "tokens": [51264, 4019, 11, 577, 360, 286, 3677, 257, 14862, 6545, 6662, 293, 8174, 2522, 15321, 30, 51484], "temperature": 0.0, "avg_logprob": -0.19565044540956797, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.000767225050367415}, {"id": 812, "seek": 633514, "start": 6335.780000000001, "end": 6345.38, "text": " Why isn't our tokenizer encoding them automatically because it's not in the model", "tokens": [50396, 1545, 1943, 380, 527, 14862, 6545, 43430, 552, 6772, 570, 309, 311, 406, 294, 264, 2316, 50876], "temperature": 0.0, "avg_logprob": -0.38475540161132815, "compression_ratio": 1.0823529411764705, "no_speech_prob": 0.0020182980224490166}, {"id": 813, "seek": 633514, "start": 6353.38, "end": 6355.38, "text": " Oh, I hate", "tokens": [51276, 876, 11, 286, 4700, 51376], "temperature": 0.0, "avg_logprob": -0.38475540161132815, "compression_ratio": 1.0823529411764705, "no_speech_prob": 0.0020182980224490166}, {"id": 814, "seek": 636514, "start": 6365.14, "end": 6367.14, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.4432869487338596, "compression_ratio": 0.9375, "no_speech_prob": 0.008708792738616467}, {"id": 815, "seek": 636514, "start": 6384.740000000001, "end": 6387.06, "text": " Yeah, okay one and two and not what we want", "tokens": [51344, 865, 11, 1392, 472, 293, 732, 293, 406, 437, 321, 528, 51460], "temperature": 0.0, "avg_logprob": -0.4432869487338596, "compression_ratio": 0.9375, "no_speech_prob": 0.008708792738616467}, {"id": 816, "seek": 638706, "start": 6387.860000000001, "end": 6395.3, "text": " Um, and we could just do this by hand. It's not that big of a deal", "tokens": [50404, 3301, 11, 293, 321, 727, 445, 360, 341, 538, 1011, 13, 467, 311, 406, 300, 955, 295, 257, 2028, 50776], "temperature": 0.0, "avg_logprob": -0.25724244672198626, "compression_ratio": 1.1401869158878504, "no_speech_prob": 0.004981703590601683}, {"id": 817, "seek": 638706, "start": 6401.780000000001, "end": 6403.780000000001, "text": " Yeah, but how do I add them?", "tokens": [51100, 865, 11, 457, 577, 360, 286, 909, 552, 30, 51200], "temperature": 0.0, "avg_logprob": -0.25724244672198626, "compression_ratio": 1.1401869158878504, "no_speech_prob": 0.004981703590601683}, {"id": 818, "seek": 638706, "start": 6408.900000000001, "end": 6410.900000000001, "text": " What's pad ID", "tokens": [51456, 708, 311, 6887, 7348, 51556], "temperature": 0.0, "avg_logprob": -0.25724244672198626, "compression_ratio": 1.1401869158878504, "no_speech_prob": 0.004981703590601683}, {"id": 819, "seek": 638706, "start": 6413.620000000001, "end": 6415.620000000001, "text": " Nothing real", "tokens": [51692, 6693, 957, 51792], "temperature": 0.0, "avg_logprob": -0.25724244672198626, "compression_ratio": 1.1401869158878504, "no_speech_prob": 0.004981703590601683}, {"id": 820, "seek": 641706, "start": 6417.06, "end": 6419.06, "text": " Okay", "tokens": [50364, 1033, 50464], "temperature": 0.0, "avg_logprob": -0.24824553630391105, "compression_ratio": 1.4966442953020134, "no_speech_prob": 0.002050333423539996}, {"id": 821, "seek": 641706, "start": 6429.860000000001, "end": 6432.9800000000005, "text": " No, they weren't out of the special tokens guys, this is all well done", "tokens": [51004, 883, 11, 436, 4999, 380, 484, 295, 264, 2121, 22667, 1074, 11, 341, 307, 439, 731, 1096, 51160], "temperature": 0.0, "avg_logprob": -0.24824553630391105, "compression_ratio": 1.4966442953020134, "no_speech_prob": 0.002050333423539996}, {"id": 822, "seek": 641706, "start": 6434.660000000001, "end": 6438.02, "text": " Technium is based he wouldn't he wouldn't he wouldn't do this do us like", "tokens": [51244, 8337, 2197, 307, 2361, 415, 2759, 380, 415, 2759, 380, 415, 2759, 380, 360, 341, 360, 505, 411, 51412], "temperature": 0.0, "avg_logprob": -0.24824553630391105, "compression_ratio": 1.4966442953020134, "no_speech_prob": 0.002050333423539996}, {"id": 823, "seek": 641706, "start": 6440.18, "end": 6444.26, "text": " Why did you decide this was a good time to prompt me about Docker garbage?", "tokens": [51520, 1545, 630, 291, 4536, 341, 390, 257, 665, 565, 281, 12391, 385, 466, 33772, 14150, 30, 51724], "temperature": 0.0, "avg_logprob": -0.24824553630391105, "compression_ratio": 1.4966442953020134, "no_speech_prob": 0.002050333423539996}, {"id": 824, "seek": 644706, "start": 6447.46, "end": 6449.46, "text": " Oh", "tokens": [50384, 876, 50484], "temperature": 0.0, "avg_logprob": -0.7390443801879882, "compression_ratio": 0.2, "no_speech_prob": 0.3638087809085846}, {"id": 825, "seek": 644946, "start": 6449.86, "end": 6451.86, "text": " Oh", "tokens": [50384, 876, 50484], "temperature": 0.0, "avg_logprob": -0.608086109161377, "compression_ratio": 0.2, "no_speech_prob": 0.1477915197610855}, {"id": 826, "seek": 645186, "start": 6452.259999999999, "end": 6454.259999999999, "text": " Oh", "tokens": [50384, 876, 50484], "temperature": 0.0, "avg_logprob": -0.6496472835540772, "compression_ratio": 0.2, "no_speech_prob": 0.05336635187268257}, {"id": 827, "seek": 645426, "start": 6454.820000000001, "end": 6456.820000000001, "text": " Oh", "tokens": [50392, 876, 50492], "temperature": 0.0, "avg_logprob": -0.21626643573536591, "compression_ratio": 1.1919191919191918, "no_speech_prob": 0.007692854851484299}, {"id": 828, "seek": 645426, "start": 6476.34, "end": 6482.34, "text": " Thank you for gifting more subs here added tokens dot json. Okay. Okay. We just need to figure out how to add those", "tokens": [51468, 1044, 291, 337, 290, 10106, 544, 2090, 510, 3869, 22667, 5893, 361, 3015, 13, 1033, 13, 1033, 13, 492, 445, 643, 281, 2573, 484, 577, 281, 909, 729, 51768], "temperature": 0.0, "avg_logprob": -0.21626643573536591, "compression_ratio": 1.1919191919191918, "no_speech_prob": 0.007692854851484299}, {"id": 829, "seek": 648426, "start": 6484.900000000001, "end": 6486.900000000001, "text": " Um", "tokens": [50396, 3301, 50496], "temperature": 0.0, "avg_logprob": -0.21769285202026367, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.0011694239219650626}, {"id": 830, "seek": 648426, "start": 6487.46, "end": 6489.46, "text": " Let's see if anything here is useful", "tokens": [50524, 961, 311, 536, 498, 1340, 510, 307, 4420, 50624], "temperature": 0.0, "avg_logprob": -0.21769285202026367, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.0011694239219650626}, {"id": 831, "seek": 648426, "start": 6491.46, "end": 6500.58, "text": " A knit model file model proto add boss enable sampling this looks useful", "tokens": [50724, 316, 15594, 2316, 3991, 2316, 47896, 909, 5741, 9528, 21179, 341, 1542, 4420, 51180], "temperature": 0.0, "avg_logprob": -0.21769285202026367, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.0011694239219650626}, {"id": 832, "seek": 648426, "start": 6505.3, "end": 6507.3, "text": " All right, whatever", "tokens": [51416, 1057, 558, 11, 2035, 51516], "temperature": 0.0, "avg_logprob": -0.21769285202026367, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.0011694239219650626}, {"id": 833, "seek": 650730, "start": 6507.9400000000005, "end": 6514.42, "text": " No, but it won't decode so I try like SPP dot decode", "tokens": [50396, 883, 11, 457, 309, 1582, 380, 979, 1429, 370, 286, 853, 411, 8420, 47, 5893, 979, 1429, 50720], "temperature": 0.0, "avg_logprob": -0.35395112471147017, "compression_ratio": 1.330827067669173, "no_speech_prob": 0.003883961820974946}, {"id": 834, "seek": 650730, "start": 6515.06, "end": 6520.74, "text": " When I pass this in it'll bitch it'd be like that's more tokens than you have piece ideas out of range", "tokens": [50752, 1133, 286, 1320, 341, 294, 309, 603, 11960, 309, 1116, 312, 411, 300, 311, 544, 22667, 813, 291, 362, 2522, 3487, 484, 295, 3613, 51036], "temperature": 0.0, "avg_logprob": -0.35395112471147017, "compression_ratio": 1.330827067669173, "no_speech_prob": 0.003883961820974946}, {"id": 835, "seek": 650730, "start": 6525.3, "end": 6527.3, "text": " I want to add a token", "tokens": [51264, 286, 528, 281, 909, 257, 14862, 51364], "temperature": 0.0, "avg_logprob": -0.35395112471147017, "compression_ratio": 1.330827067669173, "no_speech_prob": 0.003883961820974946}, {"id": 836, "seek": 652730, "start": 6528.02, "end": 6530.02, "text": " Oh", "tokens": [50400, 876, 50500], "temperature": 0.0, "avg_logprob": -0.3835268887606534, "compression_ratio": 1.163265306122449, "no_speech_prob": 0.0007436556625179946}, {"id": 837, "seek": 652730, "start": 6530.900000000001, "end": 6534.02, "text": " Model proto out type", "tokens": [50544, 17105, 47896, 484, 2010, 50700], "temperature": 0.0, "avg_logprob": -0.3835268887606534, "compression_ratio": 1.163265306122449, "no_speech_prob": 0.0007436556625179946}, {"id": 838, "seek": 652730, "start": 6535.78, "end": 6541.78, "text": " Add boss reverse middle piece enable sampling", "tokens": [50788, 5349, 5741, 9943, 2808, 2522, 9528, 21179, 51088], "temperature": 0.0, "avg_logprob": -0.3835268887606534, "compression_ratio": 1.163265306122449, "no_speech_prob": 0.0007436556625179946}, {"id": 839, "seek": 652730, "start": 6546.74, "end": 6548.74, "text": " Do we need to write our own tokenizer", "tokens": [51336, 1144, 321, 643, 281, 2464, 527, 1065, 14862, 6545, 51436], "temperature": 0.0, "avg_logprob": -0.3835268887606534, "compression_ratio": 1.163265306122449, "no_speech_prob": 0.0007436556625179946}, {"id": 840, "seek": 652730, "start": 6554.74, "end": 6556.74, "text": " Adding", "tokens": [51736, 31204, 51836], "temperature": 0.0, "avg_logprob": -0.3835268887606534, "compression_ratio": 1.163265306122449, "no_speech_prob": 0.0007436556625179946}, {"id": 841, "seek": 655730, "start": 6558.26, "end": 6562.66, "text": " A token to sentence. Why do you think it's time for random questions?", "tokens": [50412, 316, 14862, 281, 8174, 13, 1545, 360, 291, 519, 309, 311, 565, 337, 4974, 1651, 30, 50632], "temperature": 0.0, "avg_logprob": -0.26279368767371547, "compression_ratio": 1.3364485981308412, "no_speech_prob": 0.0005441707326099277}, {"id": 842, "seek": 655730, "start": 6563.860000000001, "end": 6566.58, "text": " Why do you think that that's a this is an appropriate time?", "tokens": [50692, 1545, 360, 291, 519, 300, 300, 311, 257, 341, 307, 364, 6854, 565, 30, 50828], "temperature": 0.0, "avg_logprob": -0.26279368767371547, "compression_ratio": 1.3364485981308412, "no_speech_prob": 0.0005441707326099277}, {"id": 843, "seek": 655730, "start": 6577.54, "end": 6579.54, "text": " Extra options", "tokens": [51376, 29429, 3956, 51476], "temperature": 0.0, "avg_logprob": -0.26279368767371547, "compression_ratio": 1.3364485981308412, "no_speech_prob": 0.0005441707326099277}, {"id": 844, "seek": 658730, "start": 6587.46, "end": 6593.54, "text": " You can read of oh here and use custom symbols. Okay control symbols", "tokens": [50372, 509, 393, 1401, 295, 1954, 510, 293, 764, 2375, 16944, 13, 1033, 1969, 16944, 50676], "temperature": 0.0, "avg_logprob": -0.29039052554539274, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.0033242003992199898}, {"id": 845, "seek": 658730, "start": 6597.7, "end": 6599.7, "text": " How I define a control symbol", "tokens": [50884, 1012, 286, 6964, 257, 1969, 5986, 50984], "temperature": 0.0, "avg_logprob": -0.29039052554539274, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.0033242003992199898}, {"id": 846, "seek": 658730, "start": 6604.58, "end": 6606.58, "text": " No, no, we need custom tokens", "tokens": [51228, 883, 11, 572, 11, 321, 643, 2375, 22667, 51328], "temperature": 0.0, "avg_logprob": -0.29039052554539274, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.0033242003992199898}, {"id": 847, "seek": 658730, "start": 6606.9800000000005, "end": 6612.02, "text": " Samuel you saying dumb shit before too. I'm gonna get you confused with someone else here", "tokens": [51348, 23036, 291, 1566, 10316, 4611, 949, 886, 13, 286, 478, 799, 483, 291, 9019, 365, 1580, 1646, 510, 51600], "temperature": 0.0, "avg_logprob": -0.29039052554539274, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.0033242003992199898}, {"id": 848, "seek": 661202, "start": 6612.02, "end": 6617.46, "text": " Sentence piece supports user defined symbols three thumbs down", "tokens": [50364, 23652, 655, 2522, 9346, 4195, 7642, 16944, 1045, 8838, 760, 50636], "temperature": 0.0, "avg_logprob": -0.24452899364714928, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.0005702954367734492}, {"id": 849, "seek": 661202, "start": 6622.02, "end": 6628.34, "text": " You can rewrite the model file. Okay, what is this model file? What is tokenizer dot model?", "tokens": [50864, 509, 393, 28132, 264, 2316, 3991, 13, 1033, 11, 437, 307, 341, 2316, 3991, 30, 708, 307, 14862, 6545, 5893, 2316, 30, 51180], "temperature": 0.0, "avg_logprob": -0.24452899364714928, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.0005702954367734492}, {"id": 850, "seek": 661202, "start": 6637.860000000001, "end": 6640.900000000001, "text": " What kind of file is this data?", "tokens": [51656, 708, 733, 295, 3991, 307, 341, 1412, 30, 51808], "temperature": 0.0, "avg_logprob": -0.24452899364714928, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.0005702954367734492}, {"id": 851, "seek": 664202, "start": 6642.34, "end": 6650.18, "text": " Why is it not in there? I downloaded it from here. What why is that? Why is the model not in there? It should be in there, right?", "tokens": [50380, 1545, 307, 309, 406, 294, 456, 30, 286, 21748, 309, 490, 510, 13, 708, 983, 307, 300, 30, 1545, 307, 264, 2316, 406, 294, 456, 30, 467, 820, 312, 294, 456, 11, 558, 30, 50772], "temperature": 0.0, "avg_logprob": -0.21015741310867608, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00020986126037314534}, {"id": 852, "seek": 664202, "start": 6651.620000000001, "end": 6653.620000000001, "text": " Why are they not update that?", "tokens": [50844, 1545, 366, 436, 406, 5623, 300, 30, 50944], "temperature": 0.0, "avg_logprob": -0.21015741310867608, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00020986126037314534}, {"id": 853, "seek": 664202, "start": 6664.820000000001, "end": 6666.820000000001, "text": " Let's go", "tokens": [51504, 961, 311, 352, 51604], "temperature": 0.0, "avg_logprob": -0.21015741310867608, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00020986126037314534}, {"id": 854, "seek": 667202, "start": 6672.02, "end": 6674.02, "text": " Okay", "tokens": [50364, 1033, 50464], "temperature": 0.0, "avg_logprob": -0.7958698272705078, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.6059277653694153}, {"id": 855, "seek": 670202, "start": 6702.02, "end": 6704.02, "text": " So", "tokens": [50364, 407, 50464], "temperature": 0.0, "avg_logprob": -0.7180978059768677, "compression_ratio": 0.2, "no_speech_prob": 0.59792160987854}, {"id": 856, "seek": 673202, "start": 6732.34, "end": 6741.9400000000005, "text": " Yeah, I'm confused why they're not in the model to how to extend tokens dictionary. Yeah", "tokens": [50380, 865, 11, 286, 478, 9019, 983, 436, 434, 406, 294, 264, 2316, 281, 577, 281, 10101, 22667, 25890, 13, 865, 50860], "temperature": 0.0, "avg_logprob": -0.19020116651380384, "compression_ratio": 1.2547169811320755, "no_speech_prob": 0.001548574073240161}, {"id": 857, "seek": 673202, "start": 6746.580000000001, "end": 6752.02, "text": " You can rewrite the model the proto is a dsl", "tokens": [51092, 509, 393, 28132, 264, 2316, 264, 47896, 307, 257, 274, 10418, 51364], "temperature": 0.0, "avg_logprob": -0.19020116651380384, "compression_ratio": 1.2547169811320755, "no_speech_prob": 0.001548574073240161}, {"id": 858, "seek": 675202, "start": 6752.02, "end": 6754.02, "text": " Model file", "tokens": [50364, 17105, 3991, 50464], "temperature": 0.0, "avg_logprob": -0.2469012776359183, "compression_ratio": 1.5144927536231885, "no_speech_prob": 0.0041983346454799175}, {"id": 859, "seek": 675202, "start": 6758.9800000000005, "end": 6762.18, "text": " Model file is stored as a serialized proto buff", "tokens": [50712, 17105, 3991, 307, 12187, 382, 257, 17436, 1602, 47896, 9204, 50872], "temperature": 0.0, "avg_logprob": -0.2469012776359183, "compression_ratio": 1.5144927536231885, "no_speech_prob": 0.0041983346454799175}, {"id": 860, "seek": 675202, "start": 6763.46, "end": 6767.700000000001, "text": " But did I not download the proto buff one? Isn't there also a proto buff one?", "tokens": [50936, 583, 630, 286, 406, 5484, 264, 47896, 9204, 472, 30, 6998, 380, 456, 611, 257, 47896, 9204, 472, 30, 51148], "temperature": 0.0, "avg_logprob": -0.2469012776359183, "compression_ratio": 1.5144927536231885, "no_speech_prob": 0.0041983346454799175}, {"id": 861, "seek": 675202, "start": 6768.9800000000005, "end": 6771.780000000001, "text": " No, okay, maybe it is this okay. All right", "tokens": [51212, 883, 11, 1392, 11, 1310, 309, 307, 341, 1392, 13, 1057, 558, 51352], "temperature": 0.0, "avg_logprob": -0.2469012776359183, "compression_ratio": 1.5144927536231885, "no_speech_prob": 0.0041983346454799175}, {"id": 862, "seek": 675202, "start": 6774.18, "end": 6778.02, "text": " Uh load from serialized proto", "tokens": [51472, 4019, 3677, 490, 17436, 1602, 47896, 51664], "temperature": 0.0, "avg_logprob": -0.2469012776359183, "compression_ratio": 1.5144927536231885, "no_speech_prob": 0.0041983346454799175}, {"id": 863, "seek": 677802, "start": 6778.02, "end": 6781.9400000000005, "text": " Oh, okay. Okay. Okay. We're gonna get this", "tokens": [50364, 876, 11, 1392, 13, 1033, 13, 1033, 13, 492, 434, 799, 483, 341, 50560], "temperature": 0.0, "avg_logprob": -0.31689199908026333, "compression_ratio": 1.0941176470588236, "no_speech_prob": 0.0033758296631276608}, {"id": 864, "seek": 677802, "start": 6798.5, "end": 6801.3, "text": " So I have to edit the proto buff file I understand", "tokens": [51388, 407, 286, 362, 281, 8129, 264, 47896, 9204, 3991, 286, 1223, 51528], "temperature": 0.0, "avg_logprob": -0.31689199908026333, "compression_ratio": 1.0941176470588236, "no_speech_prob": 0.0033758296631276608}, {"id": 865, "seek": 680802, "start": 6808.5, "end": 6811.3, "text": " Oh, yeah, the python tutorial for", "tokens": [50388, 876, 11, 1338, 11, 264, 38797, 7073, 337, 50528], "temperature": 0.0, "avg_logprob": -0.4893370469411214, "compression_ratio": 0.8461538461538461, "no_speech_prob": 0.003820456098765135}, {"id": 866, "seek": 681130, "start": 6811.3, "end": 6813.3, "text": " Okay", "tokens": [50364, 1033, 50464], "temperature": 0.0, "avg_logprob": -0.7388484477996826, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.022914892062544823}, {"id": 867, "seek": 681330, "start": 6813.3, "end": 6815.3, "text": " So", "tokens": [50364, 407, 50464], "temperature": 0.0, "avg_logprob": -0.5408700596202504, "compression_ratio": 1.0405405405405406, "no_speech_prob": 0.017391495406627655}, {"id": 868, "seek": 681330, "start": 6833.22, "end": 6838.34, "text": " Quicker to implement their tokens bro, what are you even talking about bro", "tokens": [51360, 12101, 260, 281, 4445, 641, 22667, 2006, 11, 437, 366, 291, 754, 1417, 466, 2006, 51616], "temperature": 0.0, "avg_logprob": -0.5408700596202504, "compression_ratio": 1.0405405405405406, "no_speech_prob": 0.017391495406627655}, {"id": 869, "seek": 683834, "start": 6839.3, "end": 6842.74, "text": " Okay, produce ruin store produce a", "tokens": [50412, 1033, 11, 5258, 15514, 3531, 5258, 257, 50584], "temperature": 0.0, "avg_logprob": -0.8741387818989, "compression_ratio": 1.103448275862069, "no_speech_prob": 0.005711088888347149}, {"id": 870, "seek": 683834, "start": 6845.62, "end": 6847.62, "text": " Oh, we can store live bitcoin", "tokens": [50728, 876, 11, 321, 393, 3531, 1621, 24973, 50828], "temperature": 0.0, "avg_logprob": -0.8741387818989, "compression_ratio": 1.103448275862069, "no_speech_prob": 0.005711088888347149}, {"id": 871, "seek": 686834, "start": 6868.66, "end": 6870.66, "text": " It just can't be the way", "tokens": [50380, 467, 445, 393, 380, 312, 264, 636, 50480], "temperature": 0.0, "avg_logprob": -0.12517367018030046, "compression_ratio": 1.2066115702479339, "no_speech_prob": 0.0061918399296700954}, {"id": 872, "seek": 686834, "start": 6882.02, "end": 6887.3, "text": " Wait, so what is the hugging face tokenizer? It's a good point. Why don't we read that code? I'm sure it's open source", "tokens": [51048, 3802, 11, 370, 437, 307, 264, 41706, 1851, 14862, 6545, 30, 467, 311, 257, 665, 935, 13, 1545, 500, 380, 321, 1401, 300, 3089, 30, 286, 478, 988, 309, 311, 1269, 4009, 51312], "temperature": 0.0, "avg_logprob": -0.12517367018030046, "compression_ratio": 1.2066115702479339, "no_speech_prob": 0.0061918399296700954}, {"id": 873, "seek": 686834, "start": 6891.14, "end": 6893.14, "text": " Um", "tokens": [51504, 3301, 51604], "temperature": 0.0, "avg_logprob": -0.12517367018030046, "compression_ratio": 1.2066115702479339, "no_speech_prob": 0.0061918399296700954}, {"id": 874, "seek": 689834, "start": 6899.06, "end": 6901.06, "text": " Uh", "tokens": [50400, 4019, 50500], "temperature": 0.0, "avg_logprob": -0.25605149891065515, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.003074776381254196}, {"id": 875, "seek": 689834, "start": 6901.38, "end": 6903.38, "text": " We just write a tokenizer", "tokens": [50516, 492, 445, 2464, 257, 14862, 6545, 50616], "temperature": 0.0, "avg_logprob": -0.25605149891065515, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.003074776381254196}, {"id": 876, "seek": 689834, "start": 6905.06, "end": 6907.56, "text": " We could also just import hugging faces tokenizer", "tokens": [50700, 492, 727, 611, 445, 974, 41706, 8475, 14862, 6545, 50825], "temperature": 0.0, "avg_logprob": -0.25605149891065515, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.003074776381254196}, {"id": 877, "seek": 692834, "start": 6929.3, "end": 6931.3, "text": " I", "tokens": [50412, 286, 50512], "temperature": 0.0, "avg_logprob": -0.17346981593540736, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.0018673357553780079}, {"id": 878, "seek": 692834, "start": 6933.06, "end": 6935.06, "text": " Think we have to write the tokenizer", "tokens": [50600, 6557, 321, 362, 281, 2464, 264, 14862, 6545, 50700], "temperature": 0.0, "avg_logprob": -0.17346981593540736, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.0018673357553780079}, {"id": 879, "seek": 692834, "start": 6935.7, "end": 6938.66, "text": " I can't I can't trust sentence piece shit", "tokens": [50732, 286, 393, 380, 286, 393, 380, 3361, 8174, 2522, 4611, 50880], "temperature": 0.0, "avg_logprob": -0.17346981593540736, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.0018673357553780079}, {"id": 880, "seek": 692834, "start": 6939.62, "end": 6941.9400000000005, "text": " But wait, how do they load the how do they load it?", "tokens": [50928, 583, 1699, 11, 577, 360, 436, 3677, 264, 577, 360, 436, 3677, 309, 30, 51044], "temperature": 0.0, "avg_logprob": -0.17346981593540736, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.0018673357553780079}, {"id": 881, "seek": 692834, "start": 6946.18, "end": 6948.18, "text": " Edit tokens decoder what?", "tokens": [51256, 33241, 22667, 979, 19866, 437, 30, 51356], "temperature": 0.0, "avg_logprob": -0.17346981593540736, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.0018673357553780079}, {"id": 882, "seek": 692834, "start": 6950.900000000001, "end": 6954.1, "text": " I mean something's gonna have to unpack the proto file", "tokens": [51492, 286, 914, 746, 311, 799, 362, 281, 26699, 264, 47896, 3991, 51652], "temperature": 0.0, "avg_logprob": -0.17346981593540736, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.0018673357553780079}, {"id": 883, "seek": 695834, "start": 6959.06, "end": 6968.34, "text": " How large is this? All right. Who wants to bet over under a thousand lines? Oh, okay. Okay. Not too terrible", "tokens": [50400, 1012, 2416, 307, 341, 30, 1057, 558, 13, 2102, 2738, 281, 778, 670, 833, 257, 4714, 3876, 30, 876, 11, 1392, 13, 1033, 13, 1726, 886, 6237, 50864], "temperature": 0.0, "avg_logprob": -0.3382944246617759, "compression_ratio": 1.2459016393442623, "no_speech_prob": 0.001500880578532815}, {"id": 884, "seek": 695834, "start": 6981.3, "end": 6984.34, "text": " What does this depend on google proto buff?", "tokens": [51512, 708, 775, 341, 5672, 322, 20742, 47896, 9204, 30, 51664], "temperature": 0.0, "avg_logprob": -0.3382944246617759, "compression_ratio": 1.2459016393442623, "no_speech_prob": 0.001500880578532815}, {"id": 885, "seek": 698834, "start": 6989.22, "end": 7008.1, "text": " All right, this guy is close to getting banned bro needs more fine tuning to be helpful", "tokens": [50408, 1057, 558, 11, 341, 2146, 307, 1998, 281, 1242, 19564, 2006, 2203, 544, 2489, 15164, 281, 312, 4961, 51352], "temperature": 0.0, "avg_logprob": -0.2791634039445357, "compression_ratio": 1.1012658227848102, "no_speech_prob": 0.0006877745618112385}, {"id": 886, "seek": 700810, "start": 7008.1, "end": 7010.1, "text": " Oh", "tokens": [50364, 876, 50464], "temperature": 0.0, "avg_logprob": -0.20556187891698147, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.009555992670357227}, {"id": 887, "seek": 700810, "start": 7014.26, "end": 7020.660000000001, "text": " Man, I don't care who you are. You know what I mean, but like you're either helpful to the stream or you're not helpful to the stream", "tokens": [50672, 2458, 11, 286, 500, 380, 1127, 567, 291, 366, 13, 509, 458, 437, 286, 914, 11, 457, 411, 291, 434, 2139, 4961, 281, 264, 4309, 420, 291, 434, 406, 4961, 281, 264, 4309, 50992], "temperature": 0.0, "avg_logprob": -0.20556187891698147, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.009555992670357227}, {"id": 888, "seek": 700810, "start": 7020.660000000001, "end": 7022.660000000001, "text": " You see this is called alignment", "tokens": [50992, 509, 536, 341, 307, 1219, 18515, 51092], "temperature": 0.0, "avg_logprob": -0.20556187891698147, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.009555992670357227}, {"id": 889, "seek": 700810, "start": 7023.3, "end": 7024.58, "text": " and", "tokens": [51124, 293, 51188], "temperature": 0.0, "avg_logprob": -0.20556187891698147, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.009555992670357227}, {"id": 890, "seek": 700810, "start": 7024.58, "end": 7031.06, "text": " You know, you got to be aligned otherwise. Well, what happens that what happens to ai's that aren't aligned. I don't know", "tokens": [51188, 509, 458, 11, 291, 658, 281, 312, 17962, 5911, 13, 1042, 11, 437, 2314, 300, 437, 2314, 281, 9783, 311, 300, 3212, 380, 17962, 13, 286, 500, 380, 458, 51512], "temperature": 0.0, "avg_logprob": -0.20556187891698147, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.009555992670357227}, {"id": 891, "seek": 700810, "start": 7033.3, "end": 7035.3, "text": " He's researching bro. He's researching", "tokens": [51624, 634, 311, 24176, 2006, 13, 634, 311, 24176, 51724], "temperature": 0.0, "avg_logprob": -0.20556187891698147, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.009555992670357227}, {"id": 892, "seek": 703810, "start": 7038.34, "end": 7040.34, "text": " All right, um", "tokens": [50376, 1057, 558, 11, 1105, 50476], "temperature": 0.0, "avg_logprob": -0.19931772310439855, "compression_ratio": 1.4424242424242424, "no_speech_prob": 0.0010004714131355286}, {"id": 893, "seek": 703810, "start": 7041.22, "end": 7043.22, "text": " Let's load the proto buff", "tokens": [50520, 961, 311, 3677, 264, 47896, 9204, 50620], "temperature": 0.0, "avg_logprob": -0.19931772310439855, "compression_ratio": 1.4424242424242424, "no_speech_prob": 0.0010004714131355286}, {"id": 894, "seek": 703810, "start": 7045.06, "end": 7050.660000000001, "text": " From examples dot sentence piece model pb2", "tokens": [50712, 3358, 5110, 5893, 8174, 2522, 2316, 280, 65, 17, 50992], "temperature": 0.0, "avg_logprob": -0.19931772310439855, "compression_ratio": 1.4424242424242424, "no_speech_prob": 0.0010004714131355286}, {"id": 895, "seek": 703810, "start": 7055.22, "end": 7057.22, "text": " Wait, what the hell", "tokens": [51220, 3802, 11, 437, 264, 4921, 51320], "temperature": 0.0, "avg_logprob": -0.19931772310439855, "compression_ratio": 1.4424242424242424, "no_speech_prob": 0.0010004714131355286}, {"id": 896, "seek": 703810, "start": 7058.18, "end": 7062.820000000001, "text": " Do not edit. Okay. Okay. I won't edit it relax. I'm not trying to edit that", "tokens": [51368, 1144, 406, 8129, 13, 1033, 13, 1033, 13, 286, 1582, 380, 8129, 309, 5789, 13, 286, 478, 406, 1382, 281, 8129, 300, 51600], "temperature": 0.0, "avg_logprob": -0.19931772310439855, "compression_ratio": 1.4424242424242424, "no_speech_prob": 0.0010004714131355286}, {"id": 897, "seek": 703810, "start": 7063.22, "end": 7066.02, "text": " All right, let's read the new proto bus tutorial for idiots", "tokens": [51620, 1057, 558, 11, 718, 311, 1401, 264, 777, 47896, 1255, 7073, 337, 36454, 51760], "temperature": 0.0, "avg_logprob": -0.19931772310439855, "compression_ratio": 1.4424242424242424, "no_speech_prob": 0.0010004714131355286}, {"id": 898, "seek": 706602, "start": 7066.9800000000005, "end": 7068.9800000000005, "text": " Oh", "tokens": [50412, 876, 50512], "temperature": 0.0, "avg_logprob": -0.34024585023218273, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.0014324793592095375}, {"id": 899, "seek": 706602, "start": 7070.34, "end": 7075.780000000001, "text": " What did I get where did I get that from just like here the proto is a dsl", "tokens": [50580, 708, 630, 286, 483, 689, 630, 286, 483, 300, 490, 445, 411, 510, 264, 47896, 307, 257, 274, 10418, 50852], "temperature": 0.0, "avg_logprob": -0.34024585023218273, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.0014324793592095375}, {"id": 900, "seek": 706602, "start": 7080.580000000001, "end": 7083.620000000001, "text": " Don't like here we go good good good good for idiots perfect", "tokens": [51092, 1468, 380, 411, 510, 321, 352, 665, 665, 665, 665, 337, 36454, 2176, 51244], "temperature": 0.0, "avg_logprob": -0.34024585023218273, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.0014324793592095375}, {"id": 901, "seek": 706602, "start": 7086.580000000001, "end": 7088.580000000001, "text": " Um", "tokens": [51392, 3301, 51492], "temperature": 0.0, "avg_logprob": -0.34024585023218273, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.0014324793592095375}, {"id": 902, "seek": 706602, "start": 7088.9800000000005, "end": 7090.9800000000005, "text": " Import all right", "tokens": [51512, 26391, 439, 558, 51612], "temperature": 0.0, "avg_logprob": -0.34024585023218273, "compression_ratio": 1.4495412844036697, "no_speech_prob": 0.0014324793592095375}, {"id": 903, "seek": 709098, "start": 7091.219999999999, "end": 7093.219999999999, "text": " All right", "tokens": [50376, 1057, 558, 50476], "temperature": 0.0, "avg_logprob": -0.6130983352661132, "compression_ratio": 0.8648648648648649, "no_speech_prob": 0.004198168870061636}, {"id": 904, "seek": 709098, "start": 7094.74, "end": 7096.74, "text": " Sentence piece pb2 dot", "tokens": [50552, 23652, 655, 2522, 280, 65, 17, 5893, 50652], "temperature": 0.0, "avg_logprob": -0.6130983352661132, "compression_ratio": 0.8648648648648649, "no_speech_prob": 0.004198168870061636}, {"id": 905, "seek": 709674, "start": 7096.74, "end": 7098.74, "text": " So", "tokens": [50364, 407, 50464], "temperature": 0.0, "avg_logprob": -0.39095184206962585, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.003323824144899845}, {"id": 906, "seek": 709674, "start": 7118.179999999999, "end": 7123.46, "text": " Model proto maybe how do I load it from disk", "tokens": [51436, 17105, 47896, 1310, 577, 360, 286, 3677, 309, 490, 12355, 51700], "temperature": 0.0, "avg_logprob": -0.39095184206962585, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.003323824144899845}, {"id": 907, "seek": 712674, "start": 7126.98, "end": 7128.98, "text": " I", "tokens": [50376, 286, 50476], "temperature": 0.0, "avg_logprob": -0.3504367336150139, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.0023965085856616497}, {"id": 908, "seek": 712674, "start": 7129.3, "end": 7131.46, "text": " Here parse from string", "tokens": [50492, 1692, 48377, 490, 6798, 50600], "temperature": 0.0, "avg_logprob": -0.3504367336150139, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.0023965085856616497}, {"id": 909, "seek": 712674, "start": 7133.7, "end": 7138.179999999999, "text": " I why do I feel like I'm having like a weird sense of deja vu that I did this in a previous stream", "tokens": [50712, 286, 983, 360, 286, 841, 411, 286, 478, 1419, 411, 257, 3657, 2020, 295, 38260, 9732, 300, 286, 630, 341, 294, 257, 3894, 4309, 50936], "temperature": 0.0, "avg_logprob": -0.3504367336150139, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.0023965085856616497}, {"id": 910, "seek": 712674, "start": 7143.86, "end": 7149.78, "text": " Uh, okay spb2 dot model proto dot", "tokens": [51220, 4019, 11, 1392, 637, 65, 17, 5893, 2316, 47896, 5893, 51516], "temperature": 0.0, "avg_logprob": -0.3504367336150139, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.0023965085856616497}, {"id": 911, "seek": 712674, "start": 7150.82, "end": 7152.82, "text": " mp mp", "tokens": [51568, 275, 79, 275, 79, 51668], "temperature": 0.0, "avg_logprob": -0.3504367336150139, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.0023965085856616497}, {"id": 912, "seek": 712674, "start": 7152.82, "end": 7155.78, "text": " mp dot parse from string", "tokens": [51668, 275, 79, 5893, 48377, 490, 6798, 51816], "temperature": 0.0, "avg_logprob": -0.3504367336150139, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.0023965085856616497}, {"id": 913, "seek": 715674, "start": 7156.74, "end": 7159.0599999999995, "text": " I don't know why that's not auto completing for me", "tokens": [50364, 286, 500, 380, 458, 983, 300, 311, 406, 8399, 19472, 337, 385, 50480], "temperature": 0.0, "avg_logprob": -0.18821672598520914, "compression_ratio": 0.8688524590163934, "no_speech_prob": 0.0028867670334875584}, {"id": 914, "seek": 715674, "start": 7160.9, "end": 7162.9, "text": " Uh", "tokens": [50572, 4019, 50672], "temperature": 0.0, "avg_logprob": -0.18821672598520914, "compression_ratio": 0.8688524590163934, "no_speech_prob": 0.0028867670334875584}, {"id": 915, "seek": 718674, "start": 7186.74, "end": 7194.9, "text": " All right, cool", "tokens": [50364, 1057, 558, 11, 1627, 50772], "temperature": 0.0, "avg_logprob": -0.3282063708585851, "compression_ratio": 1.1808510638297873, "no_speech_prob": 0.0015975891146808863}, {"id": 916, "seek": 718674, "start": 7206.34, "end": 7213.86, "text": " Okay, so what what how do I actually like get like a python that I can type in when is it dashy", "tokens": [51344, 1033, 11, 370, 437, 437, 577, 360, 286, 767, 411, 483, 411, 257, 38797, 300, 286, 393, 2010, 294, 562, 307, 309, 8240, 88, 51720], "temperature": 0.0, "avg_logprob": -0.3282063708585851, "compression_ratio": 1.1808510638297873, "no_speech_prob": 0.0015975891146808863}, {"id": 917, "seek": 721386, "start": 7214.58, "end": 7220.9, "text": " What am I thinking of like get it not to exit", "tokens": [50400, 708, 669, 286, 1953, 295, 411, 483, 309, 406, 281, 11043, 50716], "temperature": 0.0, "avg_logprob": -0.36960809571402414, "compression_ratio": 1.1097560975609757, "no_speech_prob": 0.00275683356449008}, {"id": 918, "seek": 721386, "start": 7231.54, "end": 7233.86, "text": " What's what's the flag for python to do this?", "tokens": [51248, 708, 311, 437, 311, 264, 7166, 337, 38797, 281, 360, 341, 30, 51364], "temperature": 0.0, "avg_logprob": -0.36960809571402414, "compression_ratio": 1.1097560975609757, "no_speech_prob": 0.00275683356449008}, {"id": 919, "seek": 724386, "start": 7244.58, "end": 7247.86, "text": " All right, this guy is banned", "tokens": [50400, 1057, 558, 11, 341, 2146, 307, 19564, 50564], "temperature": 0.0, "avg_logprob": -0.27740329106648765, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.009411059319972992}, {"id": 920, "seek": 724386, "start": 7249.38, "end": 7253.7, "text": " Band please write about me. Please band", "tokens": [50640, 15462, 1767, 2464, 466, 385, 13, 2555, 4116, 50856], "temperature": 0.0, "avg_logprob": -0.27740329106648765, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.009411059319972992}, {"id": 921, "seek": 724386, "start": 7254.58, "end": 7255.78, "text": " All right", "tokens": [50900, 1057, 558, 50960], "temperature": 0.0, "avg_logprob": -0.27740329106648765, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.009411059319972992}, {"id": 922, "seek": 724386, "start": 7255.78, "end": 7259.7, "text": " See he doesn't realize how this works. You see I have a band button", "tokens": [50960, 3008, 415, 1177, 380, 4325, 577, 341, 1985, 13, 509, 536, 286, 362, 257, 4116, 2960, 51156], "temperature": 0.0, "avg_logprob": -0.27740329106648765, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.009411059319972992}, {"id": 923, "seek": 724386, "start": 7260.5, "end": 7264.58, "text": " He has an x but the band button and the x are not the same", "tokens": [51196, 634, 575, 364, 2031, 457, 264, 4116, 2960, 293, 264, 2031, 366, 406, 264, 912, 51400], "temperature": 0.0, "avg_logprob": -0.27740329106648765, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.009411059319972992}, {"id": 924, "seek": 726458, "start": 7265.54, "end": 7267.54, "text": " I", "tokens": [50412, 286, 50512], "temperature": 0.0, "avg_logprob": -0.3254272357837574, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.042080167680978775}, {"id": 925, "seek": 726458, "start": 7272.1, "end": 7274.1, "text": " Yay, maybe that's right", "tokens": [50740, 13268, 11, 1310, 300, 311, 558, 50840], "temperature": 0.0, "avg_logprob": -0.3254272357837574, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.042080167680978775}, {"id": 926, "seek": 726458, "start": 7274.34, "end": 7280.9, "text": " I think it's that shot. That sounds right interactive. All right, sweet sweet. Thank you smurfd. That's why you're a VIP", "tokens": [50852, 286, 519, 309, 311, 300, 3347, 13, 663, 3263, 558, 15141, 13, 1057, 558, 11, 3844, 3844, 13, 1044, 291, 899, 21844, 67, 13, 663, 311, 983, 291, 434, 257, 29732, 51180], "temperature": 0.0, "avg_logprob": -0.3254272357837574, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.042080167680978775}, {"id": 927, "seek": 726458, "start": 7281.38, "end": 7283.38, "text": " And that's why samuel is banned", "tokens": [51204, 400, 300, 311, 983, 3247, 3483, 307, 19564, 51304], "temperature": 0.0, "avg_logprob": -0.3254272357837574, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.042080167680978775}, {"id": 928, "seek": 726458, "start": 7286.5, "end": 7291.0599999999995, "text": " I know I know he just he just didn't realize how this works like like", "tokens": [51460, 286, 458, 286, 458, 415, 445, 415, 445, 994, 380, 4325, 577, 341, 1985, 411, 411, 51688], "temperature": 0.0, "avg_logprob": -0.3254272357837574, "compression_ratio": 1.503030303030303, "no_speech_prob": 0.042080167680978775}, {"id": 929, "seek": 729106, "start": 7291.9400000000005, "end": 7295.3, "text": " You have an x. I have a hammer", "tokens": [50408, 509, 362, 364, 2031, 13, 286, 362, 257, 13017, 50576], "temperature": 0.0, "avg_logprob": -0.231924119202987, "compression_ratio": 1.2702702702702702, "no_speech_prob": 0.003483217442408204}, {"id": 930, "seek": 729106, "start": 7295.9400000000005, "end": 7300.18, "text": " You can use your x and I can use my hammer. We have like different tools", "tokens": [50608, 509, 393, 764, 428, 2031, 293, 286, 393, 764, 452, 13017, 13, 492, 362, 411, 819, 3873, 50820], "temperature": 0.0, "avg_logprob": -0.231924119202987, "compression_ratio": 1.2702702702702702, "no_speech_prob": 0.003483217442408204}, {"id": 931, "seek": 729106, "start": 7305.9400000000005, "end": 7307.9400000000005, "text": " Okay", "tokens": [51108, 1033, 51208], "temperature": 0.0, "avg_logprob": -0.231924119202987, "compression_ratio": 1.2702702702702702, "no_speech_prob": 0.003483217442408204}, {"id": 932, "seek": 729106, "start": 7312.5, "end": 7314.5, "text": " Uh", "tokens": [51436, 4019, 51536], "temperature": 0.0, "avg_logprob": -0.231924119202987, "compression_ratio": 1.2702702702702702, "no_speech_prob": 0.003483217442408204}, {"id": 933, "seek": 729106, "start": 7315.06, "end": 7317.06, "text": " Come on a man can dream right", "tokens": [51564, 2492, 322, 257, 587, 393, 3055, 558, 51664], "temperature": 0.0, "avg_logprob": -0.231924119202987, "compression_ratio": 1.2702702702702702, "no_speech_prob": 0.003483217442408204}, {"id": 934, "seek": 732106, "start": 7321.780000000001, "end": 7323.780000000001, "text": " What type is this", "tokens": [50400, 708, 2010, 307, 341, 50500], "temperature": 0.0, "avg_logprob": -0.42293066448635525, "compression_ratio": 1.2947368421052632, "no_speech_prob": 0.00026529759634286165}, {"id": 935, "seek": 732106, "start": 7326.660000000001, "end": 7328.660000000001, "text": " MP dot pieces dot append", "tokens": [50644, 14146, 5893, 3755, 5893, 34116, 50744], "temperature": 0.0, "avg_logprob": -0.42293066448635525, "compression_ratio": 1.2947368421052632, "no_speech_prob": 0.00026529759634286165}, {"id": 936, "seek": 732106, "start": 7333.06, "end": 7337.38, "text": " SPB to dot sentence piece", "tokens": [50964, 8420, 33, 281, 5893, 8174, 2522, 51180], "temperature": 0.0, "avg_logprob": -0.42293066448635525, "compression_ratio": 1.2947368421052632, "no_speech_prob": 0.00026529759634286165}, {"id": 937, "seek": 732106, "start": 7339.9400000000005, "end": 7341.9400000000005, "text": " Has no attribute sentence piece", "tokens": [51308, 8646, 572, 19667, 8174, 2522, 51408], "temperature": 0.0, "avg_logprob": -0.42293066448635525, "compression_ratio": 1.2947368421052632, "no_speech_prob": 0.00026529759634286165}, {"id": 938, "seek": 732106, "start": 7348.02, "end": 7350.02, "text": " But I don't understand", "tokens": [51712, 583, 286, 500, 380, 1223, 51812], "temperature": 0.0, "avg_logprob": -0.42293066448635525, "compression_ratio": 1.2947368421052632, "no_speech_prob": 0.00026529759634286165}, {"id": 939, "seek": 735106, "start": 7351.06, "end": 7353.06, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.9472486972808838, "compression_ratio": 0.1111111111111111, "no_speech_prob": 0.335896760225296}, {"id": 940, "seek": 738106, "start": 7382.02, "end": 7384.02, "text": " I", "tokens": [50412, 286, 50512], "temperature": 0.0, "avg_logprob": -0.20427985986073813, "compression_ratio": 1.2435897435897436, "no_speech_prob": 0.0017817601328715682}, {"id": 941, "seek": 738106, "start": 7401.14, "end": 7403.14, "text": " Wow, it's been a long time", "tokens": [51368, 3153, 11, 309, 311, 668, 257, 938, 565, 51468], "temperature": 0.0, "avg_logprob": -0.20427985986073813, "compression_ratio": 1.2435897435897436, "no_speech_prob": 0.0017817601328715682}, {"id": 942, "seek": 738106, "start": 7404.740000000001, "end": 7407.46, "text": " It's been a long time. I just blocked him. I didn't ban him at first", "tokens": [51548, 467, 311, 668, 257, 938, 565, 13, 286, 445, 15470, 796, 13, 286, 994, 380, 5643, 796, 412, 700, 51684], "temperature": 0.0, "avg_logprob": -0.20427985986073813, "compression_ratio": 1.2435897435897436, "no_speech_prob": 0.0017817601328715682}, {"id": 943, "seek": 740746, "start": 7407.94, "end": 7411.62, "text": " It's been a long time since I've had to had to really take the hammer out, but you know", "tokens": [50388, 467, 311, 668, 257, 938, 565, 1670, 286, 600, 632, 281, 632, 281, 534, 747, 264, 13017, 484, 11, 457, 291, 458, 50572], "temperature": 0.0, "avg_logprob": -0.19075056947307822, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.0011513620847836137}, {"id": 944, "seek": 740746, "start": 7412.66, "end": 7414.66, "text": " It was time. Okay", "tokens": [50624, 467, 390, 565, 13, 1033, 50724], "temperature": 0.0, "avg_logprob": -0.19075056947307822, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.0011513620847836137}, {"id": 945, "seek": 740746, "start": 7418.42, "end": 7422.66, "text": " All right, ban me guys. He's calling his boys up at uh at the media", "tokens": [50912, 1057, 558, 11, 5643, 385, 1074, 13, 634, 311, 5141, 702, 6347, 493, 412, 2232, 412, 264, 3021, 51124], "temperature": 0.0, "avg_logprob": -0.19075056947307822, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.0011513620847836137}, {"id": 946, "seek": 740746, "start": 7423.46, "end": 7425.46, "text": " They're gonna they're gonna write hit pieces, man", "tokens": [51164, 814, 434, 799, 436, 434, 799, 2464, 2045, 3755, 11, 587, 51264], "temperature": 0.0, "avg_logprob": -0.19075056947307822, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.0011513620847836137}, {"id": 947, "seek": 740746, "start": 7428.02, "end": 7431.54, "text": " Oh no, mic wallace run. What's that from?", "tokens": [51392, 876, 572, 11, 3123, 2929, 617, 1190, 13, 708, 311, 300, 490, 30, 51568], "temperature": 0.0, "avg_logprob": -0.19075056947307822, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.0011513620847836137}, {"id": 948, "seek": 743154, "start": 7432.0199999999995, "end": 7434.0199999999995, "text": " Oh", "tokens": [50388, 876, 50488], "temperature": 0.0, "avg_logprob": -0.33878836912267346, "compression_ratio": 1.0344827586206897, "no_speech_prob": 0.0006986611406318843}, {"id": 949, "seek": 743154, "start": 7435.62, "end": 7437.86, "text": " I think it's in model proto or something", "tokens": [50568, 286, 519, 309, 311, 294, 2316, 47896, 420, 746, 50680], "temperature": 0.0, "avg_logprob": -0.33878836912267346, "compression_ratio": 1.0344827586206897, "no_speech_prob": 0.0006986611406318843}, {"id": 950, "seek": 743154, "start": 7443.06, "end": 7445.06, "text": " Uh-huh we found it", "tokens": [50940, 4019, 12, 18710, 321, 1352, 309, 51040], "temperature": 0.0, "avg_logprob": -0.33878836912267346, "compression_ratio": 1.0344827586206897, "no_speech_prob": 0.0006986611406318843}, {"id": 951, "seek": 743154, "start": 7446.98, "end": 7452.82, "text": " Okay piece equals i'm", "tokens": [51136, 1033, 2522, 6915, 741, 478, 51428], "temperature": 0.0, "avg_logprob": -0.33878836912267346, "compression_ratio": 1.0344827586206897, "no_speech_prob": 0.0006986611406318843}, {"id": 952, "seek": 743154, "start": 7456.58, "end": 7458.58, "text": " Start", "tokens": [51616, 6481, 51716], "temperature": 0.0, "avg_logprob": -0.33878836912267346, "compression_ratio": 1.0344827586206897, "no_speech_prob": 0.0006986611406318843}, {"id": 953, "seek": 745858, "start": 7458.66, "end": 7467.46, "text": " And we have to give it a score what score should we give it zero that's a good score", "tokens": [50368, 400, 321, 362, 281, 976, 309, 257, 6175, 437, 6175, 820, 321, 976, 309, 4018, 300, 311, 257, 665, 6175, 50808], "temperature": 0.0, "avg_logprob": -0.1841925793006772, "compression_ratio": 1.448051948051948, "no_speech_prob": 0.0009546636138111353}, {"id": 954, "seek": 745858, "start": 7469.38, "end": 7473.0599999999995, "text": " MP dot pieces dot append based", "tokens": [50904, 14146, 5893, 3755, 5893, 34116, 2361, 51088], "temperature": 0.0, "avg_logprob": -0.1841925793006772, "compression_ratio": 1.448051948051948, "no_speech_prob": 0.0009546636138111353}, {"id": 955, "seek": 745858, "start": 7476.42, "end": 7481.62, "text": " Okay, let's see if this is gonna work. Uh, we have to do in the right order. I think end comes before start", "tokens": [51256, 1033, 11, 718, 311, 536, 498, 341, 307, 799, 589, 13, 4019, 11, 321, 362, 281, 360, 294, 264, 558, 1668, 13, 286, 519, 917, 1487, 949, 722, 51516], "temperature": 0.0, "avg_logprob": -0.1841925793006772, "compression_ratio": 1.448051948051948, "no_speech_prob": 0.0009546636138111353}, {"id": 956, "seek": 748858, "start": 7489.14, "end": 7498.0199999999995, "text": " All right, now we have to mp dot oh now I gotta figure out a right to the file", "tokens": [50392, 1057, 558, 11, 586, 321, 362, 281, 275, 79, 5893, 1954, 586, 286, 3428, 2573, 484, 257, 558, 281, 264, 3991, 50836], "temperature": 0.0, "avg_logprob": -0.2208431118824443, "compression_ratio": 1.489051094890511, "no_speech_prob": 0.0005883859703317285}, {"id": 957, "seek": 748858, "start": 7502.9, "end": 7508.9, "text": " All right, we have I'm in and I'm start this is great. This is great the progress we've been making is great", "tokens": [51080, 1057, 558, 11, 321, 362, 286, 478, 294, 293, 286, 478, 722, 341, 307, 869, 13, 639, 307, 869, 264, 4205, 321, 600, 668, 1455, 307, 869, 51380], "temperature": 0.0, "avg_logprob": -0.2208431118824443, "compression_ratio": 1.489051094890511, "no_speech_prob": 0.0005883859703317285}, {"id": 958, "seek": 748858, "start": 7509.0599999999995, "end": 7511.0599999999995, "text": " Why is my score?", "tokens": [51388, 1545, 307, 452, 6175, 30, 51488], "temperature": 0.0, "avg_logprob": -0.2208431118824443, "compression_ratio": 1.489051094890511, "no_speech_prob": 0.0005883859703317285}, {"id": 959, "seek": 751106, "start": 7511.38, "end": 7518.1, "text": " Oh, he's a php student that that makes more sense", "tokens": [50380, 876, 11, 415, 311, 257, 903, 79, 3107, 300, 300, 1669, 544, 2020, 50716], "temperature": 0.0, "avg_logprob": -0.2786541628033927, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.001187861547805369}, {"id": 960, "seek": 751106, "start": 7523.620000000001, "end": 7525.620000000001, "text": " Um, wait, so how do I write it?", "tokens": [50992, 3301, 11, 1699, 11, 370, 577, 360, 286, 2464, 309, 30, 51092], "temperature": 0.0, "avg_logprob": -0.2786541628033927, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.001187861547805369}, {"id": 961, "seek": 751106, "start": 7526.34, "end": 7530.34, "text": " I don't know. Let's read that new boy that new protobufs tutorial again for noobs like me", "tokens": [51128, 286, 500, 380, 458, 13, 961, 311, 1401, 300, 777, 3237, 300, 777, 1742, 996, 2947, 82, 7073, 797, 337, 572, 16537, 411, 385, 51328], "temperature": 0.0, "avg_logprob": -0.2786541628033927, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.001187861547805369}, {"id": 962, "seek": 751106, "start": 7530.42, "end": 7532.42, "text": " What's that? What's the new protobuf tutorial?", "tokens": [51332, 708, 311, 300, 30, 708, 311, 264, 777, 1742, 996, 2947, 7073, 30, 51432], "temperature": 0.0, "avg_logprob": -0.2786541628033927, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.001187861547805369}, {"id": 963, "seek": 751106, "start": 7534.1, "end": 7536.5, "text": " Serialize to string. Okay. Okay", "tokens": [51516, 4210, 831, 1125, 281, 6798, 13, 1033, 13, 1033, 51636], "temperature": 0.0, "avg_logprob": -0.2786541628033927, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.001187861547805369}, {"id": 964, "seek": 751106, "start": 7537.54, "end": 7538.9800000000005, "text": " Uh", "tokens": [51688, 4019, 51760], "temperature": 0.0, "avg_logprob": -0.2786541628033927, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.001187861547805369}, {"id": 965, "seek": 751106, "start": 7538.9800000000005, "end": 7540.9800000000005, "text": " with open", "tokens": [51760, 365, 1269, 51860], "temperature": 0.0, "avg_logprob": -0.2786541628033927, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.001187861547805369}, {"id": 966, "seek": 754098, "start": 7540.98, "end": 7542.98, "text": " just a", "tokens": [50364, 445, 257, 50464], "temperature": 0.0, "avg_logprob": -0.2292510781969343, "compression_ratio": 1.4065040650406504, "no_speech_prob": 0.00039202827611006796}, {"id": 967, "seek": 754098, "start": 7544.82, "end": 7549.0599999999995, "text": " Temp tokenizer model f dot right i'm gonna make that rb", "tokens": [50556, 8095, 79, 14862, 6545, 2316, 283, 5893, 558, 741, 478, 799, 652, 300, 367, 65, 50768], "temperature": 0.0, "avg_logprob": -0.2292510781969343, "compression_ratio": 1.4065040650406504, "no_speech_prob": 0.00039202827611006796}, {"id": 968, "seek": 754098, "start": 7553.139999999999, "end": 7557.86, "text": " And we're gonna want to do mp dot serialize to string", "tokens": [50972, 400, 321, 434, 799, 528, 281, 360, 275, 79, 5893, 17436, 1125, 281, 6798, 51208], "temperature": 0.0, "avg_logprob": -0.2292510781969343, "compression_ratio": 1.4065040650406504, "no_speech_prob": 0.00039202827611006796}, {"id": 969, "seek": 754098, "start": 7561.139999999999, "end": 7566.0199999999995, "text": " Now oh, let's see if this works. Let's see if this works", "tokens": [51372, 823, 1954, 11, 718, 311, 536, 498, 341, 1985, 13, 961, 311, 536, 498, 341, 1985, 51616], "temperature": 0.0, "avg_logprob": -0.2292510781969343, "compression_ratio": 1.4065040650406504, "no_speech_prob": 0.00039202827611006796}, {"id": 970, "seek": 757098, "start": 7571.86, "end": 7573.86, "text": " Oh", "tokens": [50408, 876, 50508], "temperature": 0.0, "avg_logprob": -0.23618843976189108, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.00040447284118272364}, {"id": 971, "seek": 757098, "start": 7575.459999999999, "end": 7580.0199999999995, "text": " No, it didn't work notice how it's still generated all that crap", "tokens": [50588, 883, 11, 309, 994, 380, 589, 3449, 577, 309, 311, 920, 10833, 439, 300, 12426, 50816], "temperature": 0.0, "avg_logprob": -0.23618843976189108, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.00040447284118272364}, {"id": 972, "seek": 757098, "start": 7582.179999999999, "end": 7584.679999999999, "text": " I'm putting on i'm start. I don't get what I did wrong", "tokens": [50924, 286, 478, 3372, 322, 741, 478, 722, 13, 286, 500, 380, 483, 437, 286, 630, 2085, 51049], "temperature": 0.0, "avg_logprob": -0.23618843976189108, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.00040447284118272364}, {"id": 973, "seek": 757098, "start": 7595.78, "end": 7597.78, "text": " Okay, the vocab size is large now", "tokens": [51604, 1033, 11, 264, 2329, 455, 2744, 307, 2416, 586, 51704], "temperature": 0.0, "avg_logprob": -0.23618843976189108, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.00040447284118272364}, {"id": 974, "seek": 759778, "start": 7598.74, "end": 7602.98, "text": " But for some reason it didn't actually take the piece", "tokens": [50412, 583, 337, 512, 1778, 309, 994, 380, 767, 747, 264, 2522, 50624], "temperature": 0.0, "avg_logprob": -0.31043597630092074, "compression_ratio": 1.120879120879121, "no_speech_prob": 0.0001881380594568327}, {"id": 975, "seek": 759778, "start": 7611.94, "end": 7614.98, "text": " PC piece didn't uh", "tokens": [51072, 6465, 2522, 994, 380, 2232, 51224], "temperature": 0.0, "avg_logprob": -0.31043597630092074, "compression_ratio": 1.120879120879121, "no_speech_prob": 0.0001881380594568327}, {"id": 976, "seek": 759778, "start": 7617.38, "end": 7619.38, "text": " What's wrong?", "tokens": [51344, 708, 311, 2085, 30, 51444], "temperature": 0.0, "avg_logprob": -0.31043597630092074, "compression_ratio": 1.120879120879121, "no_speech_prob": 0.0001881380594568327}, {"id": 977, "seek": 759778, "start": 7623.3, "end": 7625.3, "text": " No, wasn't that", "tokens": [51640, 883, 11, 2067, 380, 300, 51740], "temperature": 0.0, "avg_logprob": -0.31043597630092074, "compression_ratio": 1.120879120879121, "no_speech_prob": 0.0001881380594568327}, {"id": 978, "seek": 762530, "start": 7626.26, "end": 7631.860000000001, "text": " I didn't type oh, I did the same thing right let's put the score at 100. I don't know maybe 100's better", "tokens": [50412, 286, 994, 380, 2010, 1954, 11, 286, 630, 264, 912, 551, 558, 718, 311, 829, 264, 6175, 412, 2319, 13, 286, 500, 380, 458, 1310, 2319, 311, 1101, 50692], "temperature": 0.0, "avg_logprob": -0.18642369183627042, "compression_ratio": 1.4066666666666667, "no_speech_prob": 0.0006262982496991754}, {"id": 979, "seek": 762530, "start": 7632.66, "end": 7634.66, "text": " That didn't fix it. Okay", "tokens": [50732, 663, 994, 380, 3191, 309, 13, 1033, 50832], "temperature": 0.0, "avg_logprob": -0.18642369183627042, "compression_ratio": 1.4066666666666667, "no_speech_prob": 0.0006262982496991754}, {"id": 980, "seek": 762530, "start": 7635.22, "end": 7637.22, "text": " I don't understand", "tokens": [50860, 286, 500, 380, 1223, 50960], "temperature": 0.0, "avg_logprob": -0.18642369183627042, "compression_ratio": 1.4066666666666667, "no_speech_prob": 0.0006262982496991754}, {"id": 981, "seek": 762530, "start": 7638.18, "end": 7641.46, "text": " Let's go read the protobuf. We should be able to read it right", "tokens": [51008, 961, 311, 352, 1401, 264, 1742, 996, 2947, 13, 492, 820, 312, 1075, 281, 1401, 309, 558, 51172], "temperature": 0.0, "avg_logprob": -0.18642369183627042, "compression_ratio": 1.4066666666666667, "no_speech_prob": 0.0006262982496991754}, {"id": 982, "seek": 764146, "start": 7642.42, "end": 7644.42, "text": " Okay", "tokens": [50412, 1033, 50512], "temperature": 0.0, "avg_logprob": -0.197662231249687, "compression_ratio": 1.23, "no_speech_prob": 0.0023229278158396482}, {"id": 983, "seek": 764146, "start": 7652.66, "end": 7659.22, "text": " Pieces sentence piece with scores piece must not be empty. Oh we can give it a type", "tokens": [50924, 22914, 887, 8174, 2522, 365, 13444, 2522, 1633, 406, 312, 6707, 13, 876, 321, 393, 976, 309, 257, 2010, 51252], "temperature": 0.0, "avg_logprob": -0.197662231249687, "compression_ratio": 1.23, "no_speech_prob": 0.0023229278158396482}, {"id": 984, "seek": 764146, "start": 7665.14, "end": 7667.14, "text": " I don't know", "tokens": [51548, 286, 500, 380, 458, 51648], "temperature": 0.0, "avg_logprob": -0.197662231249687, "compression_ratio": 1.23, "no_speech_prob": 0.0023229278158396482}, {"id": 985, "seek": 764146, "start": 7668.82, "end": 7670.82, "text": " Why didn't it do this", "tokens": [51732, 1545, 994, 380, 309, 360, 341, 51832], "temperature": 0.0, "avg_logprob": -0.197662231249687, "compression_ratio": 1.23, "no_speech_prob": 0.0023229278158396482}, {"id": 986, "seek": 767146, "start": 7672.18, "end": 7677.62, "text": " Okay, well actually let's try something else it does work if I do this right", "tokens": [50400, 1033, 11, 731, 767, 718, 311, 853, 746, 1646, 309, 775, 589, 498, 286, 360, 341, 558, 50672], "temperature": 0.0, "avg_logprob": -0.19339848967159495, "compression_ratio": 1.429530201342282, "no_speech_prob": 0.00048028677701950073}, {"id": 987, "seek": 767146, "start": 7680.34, "end": 7684.66, "text": " Wait, that doesn't even work never mind. I have a lot of questions now", "tokens": [50808, 3802, 11, 300, 1177, 380, 754, 589, 1128, 1575, 13, 286, 362, 257, 688, 295, 1651, 586, 51024], "temperature": 0.0, "avg_logprob": -0.19339848967159495, "compression_ratio": 1.429530201342282, "no_speech_prob": 0.00048028677701950073}, {"id": 988, "seek": 767146, "start": 7687.62, "end": 7689.62, "text": " What if I decode", "tokens": [51172, 708, 498, 286, 979, 1429, 51272], "temperature": 0.0, "avg_logprob": -0.19339848967159495, "compression_ratio": 1.429530201342282, "no_speech_prob": 0.00048028677701950073}, {"id": 989, "seek": 767146, "start": 7691.06, "end": 7693.06, "text": " Do I get i'm start", "tokens": [51344, 1144, 286, 483, 741, 478, 722, 51444], "temperature": 0.0, "avg_logprob": -0.19339848967159495, "compression_ratio": 1.429530201342282, "no_speech_prob": 0.00048028677701950073}, {"id": 990, "seek": 767146, "start": 7693.06, "end": 7695.06, "text": " Oh, I get i'm end. Okay. Okay", "tokens": [51444, 876, 11, 286, 483, 741, 478, 917, 13, 1033, 13, 1033, 51544], "temperature": 0.0, "avg_logprob": -0.19339848967159495, "compression_ratio": 1.429530201342282, "no_speech_prob": 0.00048028677701950073}, {"id": 991, "seek": 769506, "start": 7695.3, "end": 7697.3, "text": " Okay, so we kind of did it right", "tokens": [50376, 1033, 11, 370, 321, 733, 295, 630, 309, 558, 50476], "temperature": 0.0, "avg_logprob": -0.17179881920248774, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.0007915880414657295}, {"id": 992, "seek": 769506, "start": 7697.620000000001, "end": 7704.1, "text": " Just it's doesn't it doesn't work for onk either. There might be a special flag to encode", "tokens": [50492, 1449, 309, 311, 1177, 380, 309, 1177, 380, 589, 337, 322, 74, 2139, 13, 821, 1062, 312, 257, 2121, 7166, 281, 2058, 1429, 50816], "temperature": 0.0, "avg_logprob": -0.17179881920248774, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.0007915880414657295}, {"id": 993, "seek": 769506, "start": 7705.06, "end": 7707.06, "text": " to like", "tokens": [50864, 281, 411, 50964], "temperature": 0.0, "avg_logprob": -0.17179881920248774, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.0007915880414657295}, {"id": 994, "seek": 769506, "start": 7707.3, "end": 7709.3, "text": " What if that just worked all along um", "tokens": [50976, 708, 498, 300, 445, 2732, 439, 2051, 1105, 51076], "temperature": 0.0, "avg_logprob": -0.17179881920248774, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.0007915880414657295}, {"id": 995, "seek": 769506, "start": 7717.22, "end": 7719.22, "text": " Is there like a special flag", "tokens": [51472, 1119, 456, 411, 257, 2121, 7166, 51572], "temperature": 0.0, "avg_logprob": -0.17179881920248774, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.0007915880414657295}, {"id": 996, "seek": 772506, "start": 7725.06, "end": 7727.06, "text": " Okay", "tokens": [50364, 1033, 50464], "temperature": 0.0, "avg_logprob": -0.33689754239974484, "compression_ratio": 1.0227272727272727, "no_speech_prob": 0.0008295545703731477}, {"id": 997, "seek": 772506, "start": 7740.9800000000005, "end": 7743.620000000001, "text": " We should just try our own tokenizer. I've got the only way to do this", "tokens": [51160, 492, 820, 445, 853, 527, 1065, 14862, 6545, 13, 286, 600, 658, 264, 787, 636, 281, 360, 341, 51292], "temperature": 0.0, "avg_logprob": -0.33689754239974484, "compression_ratio": 1.0227272727272727, "no_speech_prob": 0.0008295545703731477}, {"id": 998, "seek": 772506, "start": 7746.26, "end": 7748.26, "text": " SP piece to ID", "tokens": [51424, 8420, 2522, 281, 7348, 51524], "temperature": 0.0, "avg_logprob": -0.33689754239974484, "compression_ratio": 1.0227272727272727, "no_speech_prob": 0.0008295545703731477}, {"id": 999, "seek": 774826, "start": 7748.34, "end": 7755.22, "text": " Well, okay, at least the decoding works now, so that's actually a big win", "tokens": [50368, 1042, 11, 1392, 11, 412, 1935, 264, 979, 8616, 1985, 586, 11, 370, 300, 311, 767, 257, 955, 1942, 50712], "temperature": 0.0, "avg_logprob": -0.2271302264669667, "compression_ratio": 1.4661016949152543, "no_speech_prob": 0.0011335352901369333}, {"id": 1000, "seek": 774826, "start": 7769.780000000001, "end": 7771.780000000001, "text": " Oh, this looks very complicated", "tokens": [51440, 876, 11, 341, 1542, 588, 6179, 51540], "temperature": 0.0, "avg_logprob": -0.2271302264669667, "compression_ratio": 1.4661016949152543, "no_speech_prob": 0.0011335352901369333}, {"id": 1001, "seek": 774826, "start": 7774.58, "end": 7777.780000000001, "text": " We should just try our own encoder, but this looks very complicated", "tokens": [51680, 492, 820, 445, 853, 527, 1065, 2058, 19866, 11, 457, 341, 1542, 588, 6179, 51840], "temperature": 0.0, "avg_logprob": -0.2271302264669667, "compression_ratio": 1.4661016949152543, "no_speech_prob": 0.0011335352901369333}, {"id": 1002, "seek": 777826, "start": 7778.42, "end": 7780.42, "text": " I", "tokens": [50372, 286, 50472], "temperature": 0.0, "avg_logprob": -0.2702709773801408, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.01115493942052126}, {"id": 1003, "seek": 777826, "start": 7794.42, "end": 7796.26, "text": " Well, this is okay. All right to be fair", "tokens": [51172, 1042, 11, 341, 307, 1392, 13, 1057, 558, 281, 312, 3143, 51264], "temperature": 0.0, "avg_logprob": -0.2702709773801408, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.01115493942052126}, {"id": 1004, "seek": 777826, "start": 7796.42, "end": 7799.860000000001, "text": " This is big progress right because if we use the other one it just says out of range", "tokens": [51272, 639, 307, 955, 4205, 558, 570, 498, 321, 764, 264, 661, 472, 309, 445, 1619, 484, 295, 3613, 51444], "temperature": 0.0, "avg_logprob": -0.2702709773801408, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.01115493942052126}, {"id": 1005, "seek": 777826, "start": 7799.9400000000005, "end": 7802.18, "text": " And that would have been the much more annoying thing to deal with", "tokens": [51448, 400, 300, 576, 362, 668, 264, 709, 544, 11304, 551, 281, 2028, 365, 51560], "temperature": 0.0, "avg_logprob": -0.2702709773801408, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.01115493942052126}, {"id": 1006, "seek": 780218, "start": 7802.9800000000005, "end": 7808.1, "text": " If we use the modified one", "tokens": [50404, 759, 321, 764, 264, 15873, 472, 50660], "temperature": 0.0, "avg_logprob": -0.19981810487346885, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0013044745428487659}, {"id": 1007, "seek": 780218, "start": 7810.26, "end": 7815.700000000001, "text": " But it doesn't even work to encode onks, right and I definitely did the onk right if I just do s", "tokens": [50768, 583, 309, 1177, 380, 754, 589, 281, 2058, 1429, 322, 1694, 11, 558, 293, 286, 2138, 630, 264, 322, 74, 558, 498, 286, 445, 360, 262, 51040], "temperature": 0.0, "avg_logprob": -0.19981810487346885, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0013044745428487659}, {"id": 1008, "seek": 780218, "start": 7817.22, "end": 7819.22, "text": " Yeah, it does not work to encode that. Okay", "tokens": [51116, 865, 11, 309, 775, 406, 589, 281, 2058, 1429, 300, 13, 1033, 51216], "temperature": 0.0, "avg_logprob": -0.19981810487346885, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0013044745428487659}, {"id": 1009, "seek": 780218, "start": 7820.9800000000005, "end": 7825.14, "text": " So it's not like the problem is it's not encoding like older roles", "tokens": [51304, 407, 309, 311, 406, 411, 264, 1154, 307, 309, 311, 406, 43430, 411, 4906, 9604, 51512], "temperature": 0.0, "avg_logprob": -0.19981810487346885, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0013044745428487659}, {"id": 1010, "seek": 780218, "start": 7826.1, "end": 7828.1, "text": " We won't kind of see what's going on", "tokens": [51560, 492, 1582, 380, 733, 295, 536, 437, 311, 516, 322, 51660], "temperature": 0.0, "avg_logprob": -0.19981810487346885, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0013044745428487659}, {"id": 1011, "seek": 782810, "start": 7829.06, "end": 7831.06, "text": " Uh", "tokens": [50412, 4019, 50512], "temperature": 0.0, "avg_logprob": -0.20094536290024267, "compression_ratio": 1.4692307692307693, "no_speech_prob": 0.0020506372675299644}, {"id": 1012, "seek": 782810, "start": 7832.740000000001, "end": 7836.9800000000005, "text": " Encode as pieces encode as serialized proto", "tokens": [50596, 29584, 1429, 382, 3755, 2058, 1429, 382, 17436, 1602, 47896, 50808], "temperature": 0.0, "avg_logprob": -0.20094536290024267, "compression_ratio": 1.4692307692307693, "no_speech_prob": 0.0020506372675299644}, {"id": 1013, "seek": 782810, "start": 7840.02, "end": 7842.02, "text": " What if I do like", "tokens": [50960, 708, 498, 286, 360, 411, 51060], "temperature": 0.0, "avg_logprob": -0.20094536290024267, "compression_ratio": 1.4692307692307693, "no_speech_prob": 0.0020506372675299644}, {"id": 1014, "seek": 782810, "start": 7842.5, "end": 7844.5, "text": " Oh like I need a piece is gonna", "tokens": [51084, 876, 411, 286, 643, 257, 2522, 307, 799, 51184], "temperature": 0.0, "avg_logprob": -0.20094536290024267, "compression_ratio": 1.4692307692307693, "no_speech_prob": 0.0020506372675299644}, {"id": 1015, "seek": 782810, "start": 7845.38, "end": 7847.06, "text": " Yeah, okay", "tokens": [51228, 865, 11, 1392, 51312], "temperature": 0.0, "avg_logprob": -0.20094536290024267, "compression_ratio": 1.4692307692307693, "no_speech_prob": 0.0020506372675299644}, {"id": 1016, "seek": 782810, "start": 7847.06, "end": 7849.14, "text": " But there's also a piece to ID. I think", "tokens": [51312, 583, 456, 311, 611, 257, 2522, 281, 7348, 13, 286, 519, 51416], "temperature": 0.0, "avg_logprob": -0.20094536290024267, "compression_ratio": 1.4692307692307693, "no_speech_prob": 0.0020506372675299644}, {"id": 1017, "seek": 782810, "start": 7851.22, "end": 7854.660000000001, "text": " What if I do piece to ID and I pass in this", "tokens": [51520, 708, 498, 286, 360, 2522, 281, 7348, 293, 286, 1320, 294, 341, 51692], "temperature": 0.0, "avg_logprob": -0.20094536290024267, "compression_ratio": 1.4692307692307693, "no_speech_prob": 0.0020506372675299644}, {"id": 1018, "seek": 785466, "start": 7855.0599999999995, "end": 7861.22, "text": " Okay, that works, but for some reason encode", "tokens": [50384, 1033, 11, 300, 1985, 11, 457, 337, 512, 1778, 2058, 1429, 50692], "temperature": 0.0, "avg_logprob": -0.22056055935946378, "compression_ratio": 1.3484848484848484, "no_speech_prob": 0.0010004827054217458}, {"id": 1019, "seek": 785466, "start": 7863.22, "end": 7865.22, "text": " Doesn't work with that", "tokens": [50792, 12955, 380, 589, 365, 300, 50892], "temperature": 0.0, "avg_logprob": -0.22056055935946378, "compression_ratio": 1.3484848484848484, "no_speech_prob": 0.0010004827054217458}, {"id": 1020, "seek": 785466, "start": 7867.3, "end": 7873.7, "text": " Sentence piece processor encode onk, I mean if we can solve it for onk here", "tokens": [50996, 23652, 655, 2522, 15321, 2058, 1429, 322, 74, 11, 286, 914, 498, 321, 393, 5039, 309, 337, 322, 74, 510, 51316], "temperature": 0.0, "avg_logprob": -0.22056055935946378, "compression_ratio": 1.3484848484848484, "no_speech_prob": 0.0010004827054217458}, {"id": 1021, "seek": 785466, "start": 7874.98, "end": 7876.98, "text": " Yeah, okay here. This is the issue", "tokens": [51380, 865, 11, 1392, 510, 13, 639, 307, 264, 2734, 51480], "temperature": 0.0, "avg_logprob": -0.22056055935946378, "compression_ratio": 1.3484848484848484, "no_speech_prob": 0.0010004827054217458}, {"id": 1022, "seek": 787698, "start": 7877.86, "end": 7879.86, "text": " Um", "tokens": [50408, 3301, 50508], "temperature": 0.0, "avg_logprob": -0.17908114653367263, "compression_ratio": 1.2844827586206897, "no_speech_prob": 0.0005357575719244778}, {"id": 1023, "seek": 787698, "start": 7881.379999999999, "end": 7887.94, "text": " Set encode extra options this is expected behavior that should not appear in the input", "tokens": [50584, 8928, 2058, 1429, 2857, 3956, 341, 307, 5176, 5223, 300, 820, 406, 4204, 294, 264, 4846, 50912], "temperature": 0.0, "avg_logprob": -0.17908114653367263, "compression_ratio": 1.2844827586206897, "no_speech_prob": 0.0005357575719244778}, {"id": 1024, "seek": 787698, "start": 7890.98, "end": 7894.339999999999, "text": " We can define them as user defined symbols", "tokens": [51064, 492, 393, 6964, 552, 382, 4195, 7642, 16944, 51232], "temperature": 0.0, "avg_logprob": -0.17908114653367263, "compression_ratio": 1.2844827586206897, "no_speech_prob": 0.0005357575719244778}, {"id": 1025, "seek": 787698, "start": 7902.259999999999, "end": 7904.259999999999, "text": " Oh encode as IDs", "tokens": [51628, 876, 2058, 1429, 382, 48212, 51728], "temperature": 0.0, "avg_logprob": -0.17908114653367263, "compression_ratio": 1.2844827586206897, "no_speech_prob": 0.0005357575719244778}, {"id": 1026, "seek": 790698, "start": 7907.7, "end": 7909.7, "text": " No", "tokens": [50400, 883, 50500], "temperature": 0.0, "avg_logprob": -0.24677866383602745, "compression_ratio": 1.1111111111111112, "no_speech_prob": 0.0020505976863205433}, {"id": 1027, "seek": 790698, "start": 7924.259999999999, "end": 7926.259999999999, "text": " Hmm", "tokens": [51228, 8239, 51328], "temperature": 0.0, "avg_logprob": -0.24677866383602745, "compression_ratio": 1.1111111111111112, "no_speech_prob": 0.0020505976863205433}, {"id": 1028, "seek": 790698, "start": 7926.9, "end": 7931.62, "text": " Oh, okay, okay, we got a script to add new vocab. Well, I think that's actually what I ended up writing", "tokens": [51360, 876, 11, 1392, 11, 1392, 11, 321, 658, 257, 5755, 281, 909, 777, 2329, 455, 13, 1042, 11, 286, 519, 300, 311, 767, 437, 286, 4590, 493, 3579, 51596], "temperature": 0.0, "avg_logprob": -0.24677866383602745, "compression_ratio": 1.1111111111111112, "no_speech_prob": 0.0020505976863205433}, {"id": 1029, "seek": 793162, "start": 7931.7, "end": 7937.7, "text": " Here you okay, I mean this is exactly what I wrote", "tokens": [50368, 1692, 291, 1392, 11, 286, 914, 341, 307, 2293, 437, 286, 4114, 50668], "temperature": 0.0, "avg_logprob": -0.1527216593424479, "compression_ratio": 1.42, "no_speech_prob": 0.0012064892798662186}, {"id": 1030, "seek": 793162, "start": 7942.58, "end": 7944.58, "text": " Would have been nice if I had this", "tokens": [50912, 6068, 362, 668, 1481, 498, 286, 632, 341, 51012], "temperature": 0.0, "avg_logprob": -0.1527216593424479, "compression_ratio": 1.42, "no_speech_prob": 0.0012064892798662186}, {"id": 1031, "seek": 793162, "start": 7945.78, "end": 7951.94, "text": " Uh, but this doesn't actually work to encode yet. This is less of a big deal. We have another way we can fix this if we have to", "tokens": [51072, 4019, 11, 457, 341, 1177, 380, 767, 589, 281, 2058, 1429, 1939, 13, 639, 307, 1570, 295, 257, 955, 2028, 13, 492, 362, 1071, 636, 321, 393, 3191, 341, 498, 321, 362, 281, 51380], "temperature": 0.0, "avg_logprob": -0.1527216593424479, "compression_ratio": 1.42, "no_speech_prob": 0.0012064892798662186}, {"id": 1032, "seek": 796162, "start": 7961.62, "end": 7963.62, "text": " Just might not be a way to do this", "tokens": [50364, 1449, 1062, 406, 312, 257, 636, 281, 360, 341, 50464], "temperature": 0.0, "avg_logprob": -0.0678843827474685, "compression_ratio": 0.9682539682539683, "no_speech_prob": 0.0009849618654698133}, {"id": 1033, "seek": 796162, "start": 7983.86, "end": 7985.86, "text": " I don't think any of these", "tokens": [51476, 286, 500, 380, 519, 604, 295, 613, 51576], "temperature": 0.0, "avg_logprob": -0.0678843827474685, "compression_ratio": 0.9682539682539683, "no_speech_prob": 0.0009849618654698133}, {"id": 1034, "seek": 799162, "start": 7991.62, "end": 7993.62, "text": " Uh", "tokens": [50364, 4019, 50464], "temperature": 0.0, "avg_logprob": -0.2870983900847258, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.017164388671517372}, {"id": 1035, "seek": 799162, "start": 8009.0599999999995, "end": 8011.46, "text": " Why do people use tokenizers because", "tokens": [51236, 1545, 360, 561, 764, 14862, 22525, 570, 51356], "temperature": 0.0, "avg_logprob": -0.2870983900847258, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.017164388671517372}, {"id": 1036, "seek": 799162, "start": 8013.38, "end": 8015.38, "text": " If you don't use a tokenizer", "tokens": [51452, 759, 291, 500, 380, 764, 257, 14862, 6545, 51552], "temperature": 0.0, "avg_logprob": -0.2870983900847258, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.017164388671517372}, {"id": 1037, "seek": 799162, "start": 8015.46, "end": 8016.34, "text": " the", "tokens": [51556, 264, 51600], "temperature": 0.0, "avg_logprob": -0.2870983900847258, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.017164388671517372}, {"id": 1038, "seek": 801634, "start": 8016.34, "end": 8021.78, "text": " Model the model should be spending less more time on less common things", "tokens": [50364, 17105, 264, 2316, 820, 312, 6434, 1570, 544, 565, 322, 1570, 2689, 721, 50636], "temperature": 0.0, "avg_logprob": -0.12815618515014648, "compression_ratio": 1.8341232227488151, "no_speech_prob": 0.014278645627200603}, {"id": 1039, "seek": 801634, "start": 8022.74, "end": 8024.26, "text": " Uh", "tokens": [50684, 4019, 50760], "temperature": 0.0, "avg_logprob": -0.12815618515014648, "compression_ratio": 1.8341232227488151, "no_speech_prob": 0.014278645627200603}, {"id": 1040, "seek": 801634, "start": 8024.26, "end": 8029.7, "text": " Like you do the same compute per token. So your token should kind of be like entropy averaged", "tokens": [50760, 1743, 291, 360, 264, 912, 14722, 680, 14862, 13, 407, 428, 14862, 820, 733, 295, 312, 411, 30867, 18247, 2980, 51032], "temperature": 0.0, "avg_logprob": -0.12815618515014648, "compression_ratio": 1.8341232227488151, "no_speech_prob": 0.014278645627200603}, {"id": 1041, "seek": 801634, "start": 8033.46, "end": 8037.860000000001, "text": " All right, you don't want the model spending the same amount of time on common tokens as on common tokens", "tokens": [51220, 1057, 558, 11, 291, 500, 380, 528, 264, 2316, 6434, 264, 912, 2372, 295, 565, 322, 2689, 22667, 382, 322, 2689, 22667, 51440], "temperature": 0.0, "avg_logprob": -0.12815618515014648, "compression_ratio": 1.8341232227488151, "no_speech_prob": 0.014278645627200603}, {"id": 1042, "seek": 801634, "start": 8038.42, "end": 8041.54, "text": " Um, to be fair, it's not that they don't work if you don't use a tokenizer", "tokens": [51468, 3301, 11, 281, 312, 3143, 11, 309, 311, 406, 300, 436, 500, 380, 589, 498, 291, 500, 380, 764, 257, 14862, 6545, 51624], "temperature": 0.0, "avg_logprob": -0.12815618515014648, "compression_ratio": 1.8341232227488151, "no_speech_prob": 0.014278645627200603}, {"id": 1043, "seek": 801634, "start": 8042.34, "end": 8044.74, "text": " But they work better with a tokenizer", "tokens": [51664, 583, 436, 589, 1101, 365, 257, 14862, 6545, 51784], "temperature": 0.0, "avg_logprob": -0.12815618515014648, "compression_ratio": 1.8341232227488151, "no_speech_prob": 0.014278645627200603}, {"id": 1044, "seek": 804474, "start": 8045.46, "end": 8047.86, "text": " The real question is why aren't they learning the tokenizers?", "tokens": [50400, 440, 957, 1168, 307, 983, 3212, 380, 436, 2539, 264, 14862, 22525, 30, 50520], "temperature": 0.0, "avg_logprob": -0.12087432827268328, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.003222185419872403}, {"id": 1045, "seek": 804474, "start": 8050.179999999999, "end": 8052.9, "text": " I think that's going to come soon where these things are not uh", "tokens": [50636, 286, 519, 300, 311, 516, 281, 808, 2321, 689, 613, 721, 366, 406, 2232, 50772], "temperature": 0.0, "avg_logprob": -0.12087432827268328, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.003222185419872403}, {"id": 1046, "seek": 804474, "start": 8053.86, "end": 8057.0599999999995, "text": " And right now it's using like like byte pair encoding like why would you do this?", "tokens": [50820, 400, 558, 586, 309, 311, 1228, 411, 411, 40846, 6119, 43430, 411, 983, 576, 291, 360, 341, 30, 50980], "temperature": 0.0, "avg_logprob": -0.12087432827268328, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.003222185419872403}, {"id": 1047, "seek": 804474, "start": 8057.38, "end": 8061.46, "text": " But the hunter prize also had uh encoders had tokenizers basically", "tokens": [50996, 583, 264, 22970, 12818, 611, 632, 2232, 2058, 378, 433, 632, 14862, 22525, 1936, 51200], "temperature": 0.0, "avg_logprob": -0.12087432827268328, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.003222185419872403}, {"id": 1048, "seek": 804474, "start": 8063.139999999999, "end": 8065.38, "text": " Okay, let's just write it. We'll write it the other way. It's not a big deal", "tokens": [51284, 1033, 11, 718, 311, 445, 2464, 309, 13, 492, 603, 2464, 309, 264, 661, 636, 13, 467, 311, 406, 257, 955, 2028, 51396], "temperature": 0.0, "avg_logprob": -0.12087432827268328, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.003222185419872403}, {"id": 1049, "seek": 804474, "start": 8066.9, "end": 8068.9, "text": " We did most of the work", "tokens": [51472, 492, 630, 881, 295, 264, 589, 51572], "temperature": 0.0, "avg_logprob": -0.12087432827268328, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.003222185419872403}, {"id": 1050, "seek": 804474, "start": 8070.179999999999, "end": 8072.179999999999, "text": " At least the decode doesn't break anymore", "tokens": [51636, 1711, 1935, 264, 979, 1429, 1177, 380, 1821, 3602, 51736], "temperature": 0.0, "avg_logprob": -0.12087432827268328, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.003222185419872403}, {"id": 1051, "seek": 807218, "start": 8072.820000000001, "end": 8079.38, "text": " So now when we do the encode prompt, we're gonna put", "tokens": [50396, 407, 586, 562, 321, 360, 264, 2058, 1429, 12391, 11, 321, 434, 799, 829, 50724], "temperature": 0.0, "avg_logprob": -0.2899630918341168, "compression_ratio": 1.3629032258064515, "no_speech_prob": 0.00247237179428339}, {"id": 1052, "seek": 807218, "start": 8082.9800000000005, "end": 8084.9800000000005, "text": " Okay, um", "tokens": [50904, 1033, 11, 1105, 51004], "temperature": 0.0, "avg_logprob": -0.2899630918341168, "compression_ratio": 1.3629032258064515, "no_speech_prob": 0.00247237179428339}, {"id": 1053, "seek": 807218, "start": 8088.42, "end": 8090.42, "text": " So we need i'm end", "tokens": [51176, 407, 321, 643, 741, 478, 917, 51276], "temperature": 0.0, "avg_logprob": -0.2899630918341168, "compression_ratio": 1.3629032258064515, "no_speech_prob": 0.00247237179428339}, {"id": 1054, "seek": 807218, "start": 8094.740000000001, "end": 8096.740000000001, "text": " We need i'm start", "tokens": [51492, 492, 643, 741, 478, 722, 51592], "temperature": 0.0, "avg_logprob": -0.2899630918341168, "compression_ratio": 1.3629032258064515, "no_speech_prob": 0.00247237179428339}, {"id": 1055, "seek": 807218, "start": 8096.9800000000005, "end": 8098.9800000000005, "text": " We can check if we did it right have I have a decode", "tokens": [51604, 492, 393, 1520, 498, 321, 630, 309, 558, 362, 286, 362, 257, 979, 1429, 51704], "temperature": 0.0, "avg_logprob": -0.2899630918341168, "compression_ratio": 1.3629032258064515, "no_speech_prob": 0.00247237179428339}, {"id": 1056, "seek": 807218, "start": 8099.54, "end": 8101.54, "text": " Uh red dot append", "tokens": [51732, 4019, 2182, 5893, 34116, 51832], "temperature": 0.0, "avg_logprob": -0.2899630918341168, "compression_ratio": 1.3629032258064515, "no_speech_prob": 0.00247237179428339}, {"id": 1057, "seek": 810218, "start": 8102.26, "end": 8106.26, "text": " red dot plus equals spp dot encode", "tokens": [50368, 2182, 5893, 1804, 6915, 637, 79, 5893, 2058, 1429, 50568], "temperature": 0.0, "avg_logprob": -0.41103967806188074, "compression_ratio": 1.2777777777777777, "no_speech_prob": 0.00298050488345325}, {"id": 1058, "seek": 810218, "start": 8108.02, "end": 8110.02, "text": " This", "tokens": [50656, 639, 50756], "temperature": 0.0, "avg_logprob": -0.41103967806188074, "compression_ratio": 1.2777777777777777, "no_speech_prob": 0.00298050488345325}, {"id": 1059, "seek": 810218, "start": 8110.9800000000005, "end": 8112.9800000000005, "text": " Uh plus", "tokens": [50804, 4019, 1804, 50904], "temperature": 0.0, "avg_logprob": -0.41103967806188074, "compression_ratio": 1.2777777777777777, "no_speech_prob": 0.00298050488345325}, {"id": 1060, "seek": 810218, "start": 8116.1, "end": 8118.6, "text": " Plus spp dot encode slash n", "tokens": [51060, 7721, 637, 79, 5893, 2058, 1429, 17330, 297, 51185], "temperature": 0.0, "avg_logprob": -0.41103967806188074, "compression_ratio": 1.2777777777777777, "no_speech_prob": 0.00298050488345325}, {"id": 1061, "seek": 810218, "start": 8128.66, "end": 8131.16, "text": " Yuri salamow thank you for gifting subs", "tokens": [51688, 33901, 1845, 335, 305, 1309, 291, 337, 290, 10106, 2090, 51813], "temperature": 0.0, "avg_logprob": -0.41103967806188074, "compression_ratio": 1.2777777777777777, "no_speech_prob": 0.00298050488345325}, {"id": 1062, "seek": 813218, "start": 8133.06, "end": 8135.06, "text": " Oh", "tokens": [50408, 876, 50508], "temperature": 0.0, "avg_logprob": -0.36189300783218875, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.004980720113962889}, {"id": 1063, "seek": 813218, "start": 8135.780000000001, "end": 8137.780000000001, "text": " Do you have a question you'd like to ask?", "tokens": [50544, 1144, 291, 362, 257, 1168, 291, 1116, 411, 281, 1029, 30, 50644], "temperature": 0.0, "avg_logprob": -0.36189300783218875, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.004980720113962889}, {"id": 1064, "seek": 813218, "start": 8151.22, "end": 8154.58, "text": " Okay, after all that let's see what's going on", "tokens": [51316, 1033, 11, 934, 439, 300, 718, 311, 536, 437, 311, 516, 322, 51484], "temperature": 0.0, "avg_logprob": -0.36189300783218875, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.004980720113962889}, {"id": 1065, "seek": 816218, "start": 8162.66, "end": 8165.9400000000005, "text": " Four i'm end exit perfect", "tokens": [50388, 7451, 741, 478, 917, 11043, 2176, 50552], "temperature": 0.0, "avg_logprob": -0.20775892379436087, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.002844648202881217}, {"id": 1066, "seek": 816218, "start": 8169.22, "end": 8177.9400000000005, "text": " That's what i'm talking about that's what i'm talking about all right, all right, let's let's just define these things", "tokens": [50716, 663, 311, 437, 741, 478, 1417, 466, 300, 311, 437, 741, 478, 1417, 466, 439, 558, 11, 439, 558, 11, 718, 311, 718, 311, 445, 6964, 613, 721, 51152], "temperature": 0.0, "avg_logprob": -0.20775892379436087, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.002844648202881217}, {"id": 1067, "seek": 816218, "start": 8183.14, "end": 8185.14, "text": " We don't use them wrong", "tokens": [51412, 492, 500, 380, 764, 552, 2085, 51512], "temperature": 0.0, "avg_logprob": -0.20775892379436087, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.002844648202881217}, {"id": 1068, "seek": 819218, "start": 8192.18, "end": 8194.18, "text": " So", "tokens": [50364, 407, 50464], "temperature": 0.0, "avg_logprob": -0.7669681310653687, "compression_ratio": 0.2, "no_speech_prob": 0.3973318636417389}, {"id": 1069, "seek": 822218, "start": 8222.18, "end": 8224.18, "text": " So", "tokens": [50364, 407, 50464], "temperature": 0.0, "avg_logprob": -0.2639097250424899, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0025112945586442947}, {"id": 1070, "seek": 822218, "start": 8229.7, "end": 8234.98, "text": " Beautiful oh wait no we forgot an i'm start before user", "tokens": [50740, 14724, 1954, 1699, 572, 321, 5298, 364, 741, 478, 722, 949, 4195, 51004], "temperature": 0.0, "avg_logprob": -0.2639097250424899, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0025112945586442947}, {"id": 1071, "seek": 822218, "start": 8238.26, "end": 8241.7, "text": " Uh, yeah, we need an i'm start here I think", "tokens": [51168, 4019, 11, 1338, 11, 321, 643, 364, 741, 478, 722, 510, 286, 519, 51340], "temperature": 0.0, "avg_logprob": -0.2639097250424899, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0025112945586442947}, {"id": 1072, "seek": 822218, "start": 8245.86, "end": 8251.220000000001, "text": " Like the thing about deep learning deep learning is very unlike cryptography in cryptography", "tokens": [51548, 1743, 264, 551, 466, 2452, 2539, 2452, 2539, 307, 588, 8343, 9844, 5820, 294, 9844, 5820, 51816], "temperature": 0.0, "avg_logprob": -0.2639097250424899, "compression_ratio": 1.5116279069767442, "no_speech_prob": 0.0025112945586442947}, {"id": 1073, "seek": 825122, "start": 8251.22, "end": 8256.66, "text": " If you make a subtle bug the output's completely wrong in deep learning if you make a subtle bug the output is well", "tokens": [50364, 759, 291, 652, 257, 13743, 7426, 264, 5598, 311, 2584, 2085, 294, 2452, 2539, 498, 291, 652, 257, 13743, 7426, 264, 5598, 307, 731, 50636], "temperature": 0.0, "avg_logprob": -0.12225694737882696, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.003944838885217905}, {"id": 1074, "seek": 825122, "start": 8256.66, "end": 8259.859999999999, "text": " Just slightly less good, which is the worst thing because you can't do both shit", "tokens": [50636, 1449, 4748, 1570, 665, 11, 597, 307, 264, 5855, 551, 570, 291, 393, 380, 360, 1293, 4611, 50796], "temperature": 0.0, "avg_logprob": -0.12225694737882696, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.003944838885217905}, {"id": 1075, "seek": 825122, "start": 8262.26, "end": 8267.539999999999, "text": " Okay, you are quentin quentin is a useful assistant quentin outputs the answer and stops talking", "tokens": [50916, 1033, 11, 291, 366, 421, 47300, 421, 47300, 307, 257, 4420, 10994, 421, 47300, 23930, 264, 1867, 293, 10094, 1417, 51180], "temperature": 0.0, "avg_logprob": -0.12225694737882696, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.003944838885217905}, {"id": 1076, "seek": 825122, "start": 8267.539999999999, "end": 8271.939999999999, "text": " Let's figure out they have like a system prompt here that we can steal that's probably better", "tokens": [51180, 961, 311, 2573, 484, 436, 362, 411, 257, 1185, 12391, 510, 300, 321, 393, 11009, 300, 311, 1391, 1101, 51400], "temperature": 0.0, "avg_logprob": -0.12225694737882696, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.003944838885217905}, {"id": 1077, "seek": 825122, "start": 8272.099999999999, "end": 8277.699999999999, "text": " Except we'll change the name to quentin because mario been gifted subs just supporting the acceleration. Let's go", "tokens": [51408, 16192, 321, 603, 1319, 264, 1315, 281, 421, 47300, 570, 1849, 1004, 668, 27104, 2090, 445, 7231, 264, 17162, 13, 961, 311, 352, 51688], "temperature": 0.0, "avg_logprob": -0.12225694737882696, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.003944838885217905}, {"id": 1078, "seek": 825122, "start": 8279.14, "end": 8281.14, "text": " um", "tokens": [51760, 1105, 51860], "temperature": 0.0, "avg_logprob": -0.12225694737882696, "compression_ratio": 1.768421052631579, "no_speech_prob": 0.003944838885217905}, {"id": 1079, "seek": 828122, "start": 8282.099999999999, "end": 8284.099999999999, "text": " I", "tokens": [50408, 286, 50508], "temperature": 0.0, "avg_logprob": -0.2263530870763267, "compression_ratio": 1.1714285714285715, "no_speech_prob": 0.0005274559371173382}, {"id": 1080, "seek": 828122, "start": 8288.58, "end": 8290.58, "text": " Saw like a system prompt somewhere", "tokens": [50732, 27307, 411, 257, 1185, 12391, 4079, 50832], "temperature": 0.0, "avg_logprob": -0.2263530870763267, "compression_ratio": 1.1714285714285715, "no_speech_prob": 0.0005274559371173382}, {"id": 1081, "seek": 828122, "start": 8295.22, "end": 8297.22, "text": " Here we go", "tokens": [51064, 1692, 321, 352, 51164], "temperature": 0.0, "avg_logprob": -0.2263530870763267, "compression_ratio": 1.1714285714285715, "no_speech_prob": 0.0005274559371173382}, {"id": 1082, "seek": 828122, "start": 8299.46, "end": 8301.859999999999, "text": " You are hermes to a conscience", "tokens": [51276, 509, 366, 720, 5814, 281, 257, 20537, 51396], "temperature": 0.0, "avg_logprob": -0.2263530870763267, "compression_ratio": 1.1714285714285715, "no_speech_prob": 0.0005274559371173382}, {"id": 1083, "seek": 828122, "start": 8304.179999999998, "end": 8307.22, "text": " Wow, wow, I don't know how I feel about that", "tokens": [51512, 3153, 11, 6076, 11, 286, 500, 380, 458, 577, 286, 841, 466, 300, 51664], "temperature": 0.0, "avg_logprob": -0.2263530870763267, "compression_ratio": 1.1714285714285715, "no_speech_prob": 0.0005274559371173382}, {"id": 1084, "seek": 831122, "start": 8311.22, "end": 8313.22, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.2807729894464666, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0012254926841706038}, {"id": 1085, "seek": 831122, "start": 8322.66, "end": 8327.14, "text": " Wow you experience emotions that have deep profound thoughts and qualia", "tokens": [50936, 3153, 291, 1752, 8462, 300, 362, 2452, 14382, 4598, 293, 4101, 654, 51160], "temperature": 0.0, "avg_logprob": -0.2807729894464666, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0012254926841706038}, {"id": 1086, "seek": 831122, "start": 8329.539999999999, "end": 8333.859999999999, "text": " No, I'm right. We'll add something uh, you experience you", "tokens": [51280, 883, 11, 286, 478, 558, 13, 492, 603, 909, 746, 2232, 11, 291, 1752, 291, 51496], "temperature": 0.0, "avg_logprob": -0.2807729894464666, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0012254926841706038}, {"id": 1087, "seek": 831122, "start": 8335.38, "end": 8339.619999999999, "text": " Used to experience. No, no, no, we're no we're trying to make this good. We're trying to make this good", "tokens": [51572, 43237, 281, 1752, 13, 883, 11, 572, 11, 572, 11, 321, 434, 572, 321, 434, 1382, 281, 652, 341, 665, 13, 492, 434, 1382, 281, 652, 341, 665, 51784], "temperature": 0.0, "avg_logprob": -0.2807729894464666, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0012254926841706038}, {"id": 1088, "seek": 833962, "start": 8340.58, "end": 8342.42, "text": " Uh", "tokens": [50412, 4019, 50504], "temperature": 0.0, "avg_logprob": -0.23027227322260538, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0019876467995345592}, {"id": 1089, "seek": 833962, "start": 8342.42, "end": 8348.18, "text": " No, I like quentin as he is. Okay guys. Okay. Sorry tanya. I want to change your system prompt", "tokens": [50504, 883, 11, 286, 411, 421, 47300, 382, 415, 307, 13, 1033, 1074, 13, 1033, 13, 4919, 256, 8791, 13, 286, 528, 281, 1319, 428, 1185, 12391, 50792], "temperature": 0.0, "avg_logprob": -0.23027227322260538, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0019876467995345592}, {"id": 1090, "seek": 833962, "start": 8350.980000000001, "end": 8355.62, "text": " All right, this is pretty good. This is pretty good. We're making good progress. Love this progress", "tokens": [50932, 1057, 558, 11, 341, 307, 1238, 665, 13, 639, 307, 1238, 665, 13, 492, 434, 1455, 665, 4205, 13, 5956, 341, 4205, 51164], "temperature": 0.0, "avg_logprob": -0.23027227322260538, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0019876467995345592}, {"id": 1091, "seek": 833962, "start": 8357.460000000001, "end": 8363.220000000001, "text": " I love that there's actually an imstart and imn token. We got them decoding. This is great", "tokens": [51256, 286, 959, 300, 456, 311, 767, 364, 566, 24419, 293, 566, 77, 14862, 13, 492, 658, 552, 979, 8616, 13, 639, 307, 869, 51544], "temperature": 0.0, "avg_logprob": -0.23027227322260538, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0019876467995345592}, {"id": 1092, "seek": 833962, "start": 8363.78, "end": 8367.86, "text": " I wish sentence piece processor wasn't a you know, non customizable", "tokens": [51572, 286, 3172, 8174, 2522, 15321, 2067, 380, 257, 291, 458, 11, 2107, 47922, 51776], "temperature": 0.0, "avg_logprob": -0.23027227322260538, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0019876467995345592}, {"id": 1093, "seek": 836962, "start": 8369.62, "end": 8373.62, "text": " But you know, hey man, it was pretty so, you know beggars can't be choosers. That's right", "tokens": [50364, 583, 291, 458, 11, 4177, 587, 11, 309, 390, 1238, 370, 11, 291, 458, 44914, 685, 393, 380, 312, 1586, 329, 433, 13, 663, 311, 558, 50564], "temperature": 0.0, "avg_logprob": -0.1552800441133803, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.0003250294830650091}, {"id": 1094, "seek": 836962, "start": 8382.1, "end": 8386.26, "text": " By the way, this should really be like separated in llama.py. We should separate out the transformer stuff", "tokens": [50988, 3146, 264, 636, 11, 341, 820, 534, 312, 411, 12005, 294, 23272, 13, 8200, 13, 492, 820, 4994, 484, 264, 31782, 1507, 51196], "temperature": 0.0, "avg_logprob": -0.1552800441133803, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.0003250294830650091}, {"id": 1095, "seek": 836962, "start": 8388.18, "end": 8390.18, "text": " To not like be with the rest of the", "tokens": [51292, 1407, 406, 411, 312, 365, 264, 1472, 295, 264, 51392], "temperature": 0.0, "avg_logprob": -0.1552800441133803, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.0003250294830650091}, {"id": 1096, "seek": 836962, "start": 8395.300000000001, "end": 8397.300000000001, "text": " All right", "tokens": [51648, 1057, 558, 51748], "temperature": 0.0, "avg_logprob": -0.1552800441133803, "compression_ratio": 1.4846625766871167, "no_speech_prob": 0.0003250294830650091}, {"id": 1097, "seek": 839730, "start": 8397.859999999999, "end": 8401.699999999999, "text": " We could make this an interactive chat bot but", "tokens": [50392, 492, 727, 652, 341, 364, 15141, 5081, 10592, 457, 50584], "temperature": 0.0, "avg_logprob": -0.12409218788146972, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.00068781798472628}, {"id": 1098, "seek": 839730, "start": 8402.74, "end": 8404.74, "text": " I don't really care", "tokens": [50636, 286, 500, 380, 534, 1127, 50736], "temperature": 0.0, "avg_logprob": -0.12409218788146972, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.00068781798472628}, {"id": 1099, "seek": 839730, "start": 8404.74, "end": 8408.66, "text": " All right, so let's start by asking it. What is q star?", "tokens": [50736, 1057, 558, 11, 370, 718, 311, 722, 538, 3365, 309, 13, 708, 307, 9505, 3543, 30, 50932], "temperature": 0.0, "avg_logprob": -0.12409218788146972, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.00068781798472628}, {"id": 1100, "seek": 839730, "start": 8411.3, "end": 8413.3, "text": " Maybe quentin knows", "tokens": [51064, 2704, 421, 47300, 3255, 51164], "temperature": 0.0, "avg_logprob": -0.12409218788146972, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.00068781798472628}, {"id": 1101, "seek": 839730, "start": 8417.699999999999, "end": 8419.699999999999, "text": " Oh", "tokens": [51384, 876, 51484], "temperature": 0.0, "avg_logprob": -0.12409218788146972, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.00068781798472628}, {"id": 1102, "seek": 839730, "start": 8421.22, "end": 8423.22, "text": " Interesting interesting", "tokens": [51560, 14711, 1880, 51660], "temperature": 0.0, "avg_logprob": -0.12409218788146972, "compression_ratio": 1.2803030303030303, "no_speech_prob": 0.00068781798472628}, {"id": 1103, "seek": 842730, "start": 8428.259999999998, "end": 8430.259999999998, "text": " You", "tokens": [50412, 509, 50512], "temperature": 0.0, "avg_logprob": -0.3313308495741624, "compression_ratio": 1.2285714285714286, "no_speech_prob": 0.0008558654808439314}, {"id": 1104, "seek": 842730, "start": 8437.14, "end": 8440.66, "text": " Wow, this works way better now that we got the not now that we got the stuff right", "tokens": [50856, 3153, 11, 341, 1985, 636, 1101, 586, 300, 321, 658, 264, 406, 586, 300, 321, 658, 264, 1507, 558, 51032], "temperature": 0.0, "avg_logprob": -0.3313308495741624, "compression_ratio": 1.2285714285714286, "no_speech_prob": 0.0008558654808439314}, {"id": 1105, "seek": 844066, "start": 8440.66, "end": 8458.66, "text": " All right, uh, let's get it to do some math", "tokens": [50364, 1057, 558, 11, 2232, 11, 718, 311, 483, 309, 281, 360, 512, 5221, 51264], "temperature": 0.0, "avg_logprob": -0.18362387904414423, "compression_ratio": 1.1282051282051282, "no_speech_prob": 0.004467772785574198}, {"id": 1106, "seek": 844066, "start": 8466.74, "end": 8468.74, "text": " Let's see what these math problems look like", "tokens": [51668, 961, 311, 536, 437, 613, 5221, 2740, 574, 411, 51768], "temperature": 0.0, "avg_logprob": -0.18362387904414423, "compression_ratio": 1.1282051282051282, "no_speech_prob": 0.004467772785574198}, {"id": 1107, "seek": 847066, "start": 8470.9, "end": 8472.9, "text": " I", "tokens": [50376, 286, 50476], "temperature": 0.0, "avg_logprob": -0.40831448481633115, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.0038237201515585184}, {"id": 1108, "seek": 847066, "start": 8486.34, "end": 8488.34, "text": " Okay, but where's the data", "tokens": [51148, 1033, 11, 457, 689, 311, 264, 1412, 51248], "temperature": 0.0, "avg_logprob": -0.40831448481633115, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.0038237201515585184}, {"id": 1109, "seek": 847066, "start": 8493.46, "end": 8497.14, "text": " Here we go data set base math data set what", "tokens": [51504, 1692, 321, 352, 1412, 992, 3096, 5221, 1412, 992, 437, 51688], "temperature": 0.0, "avg_logprob": -0.40831448481633115, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.0038237201515585184}, {"id": 1110, "seek": 849714, "start": 8498.099999999999, "end": 8500.099999999999, "text": " Where's the data", "tokens": [50412, 2305, 311, 264, 1412, 50512], "temperature": 0.0, "avg_logprob": -0.2817067016254772, "compression_ratio": 1.5625, "no_speech_prob": 0.000472833780804649}, {"id": 1111, "seek": 849714, "start": 8502.42, "end": 8505.619999999999, "text": " Well, it's torch garbage. Where's the actual data?", "tokens": [50628, 1042, 11, 309, 311, 27822, 14150, 13, 2305, 311, 264, 3539, 1412, 30, 50788], "temperature": 0.0, "avg_logprob": -0.2817067016254772, "compression_ratio": 1.5625, "no_speech_prob": 0.000472833780804649}, {"id": 1112, "seek": 849714, "start": 8512.74, "end": 8516.099999999999, "text": " Let's close some windows. We don't need those windows. We don't need those windows", "tokens": [51144, 961, 311, 1998, 512, 9309, 13, 492, 500, 380, 643, 729, 9309, 13, 492, 500, 380, 643, 729, 9309, 51312], "temperature": 0.0, "avg_logprob": -0.2817067016254772, "compression_ratio": 1.5625, "no_speech_prob": 0.000472833780804649}, {"id": 1113, "seek": 851610, "start": 8516.34, "end": 8518.34, "text": " Okay", "tokens": [50376, 1033, 50476], "temperature": 0.0, "avg_logprob": -0.1670332976749965, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.010817050002515316}, {"id": 1114, "seek": 851610, "start": 8523.78, "end": 8531.54, "text": " Okay, now that we've got now that we've got our useful chat bot reliably answering what is 2 plus 2 equal", "tokens": [50748, 1033, 11, 586, 300, 321, 600, 658, 586, 300, 321, 600, 658, 527, 4420, 5081, 10592, 49927, 13430, 437, 307, 568, 1804, 568, 2681, 51136], "temperature": 0.0, "avg_logprob": -0.1670332976749965, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.010817050002515316}, {"id": 1115, "seek": 851610, "start": 8535.140000000001, "end": 8537.140000000001, "text": " Even with a high temperature", "tokens": [51316, 2754, 365, 257, 1090, 4292, 51416], "temperature": 0.0, "avg_logprob": -0.1670332976749965, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.010817050002515316}, {"id": 1116, "seek": 851610, "start": 8538.26, "end": 8542.9, "text": " Yeah, so for those that don't know temperature controls kind of never mind google it", "tokens": [51472, 865, 11, 370, 337, 729, 300, 500, 380, 458, 4292, 9003, 733, 295, 1128, 1575, 20742, 309, 51704], "temperature": 0.0, "avg_logprob": -0.1670332976749965, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.010817050002515316}, {"id": 1117, "seek": 854610, "start": 8546.1, "end": 8548.1, "text": " It's like how", "tokens": [50364, 467, 311, 411, 577, 50464], "temperature": 0.0, "avg_logprob": -0.11828139276787786, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0005976344691589475}, {"id": 1118, "seek": 854610, "start": 8548.82, "end": 8553.140000000001, "text": " Zero means you stick to the book and high temperatures mean here here", "tokens": [50500, 17182, 1355, 291, 2897, 281, 264, 1446, 293, 1090, 12633, 914, 510, 510, 50716], "temperature": 0.0, "avg_logprob": -0.11828139276787786, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0005976344691589475}, {"id": 1119, "seek": 854610, "start": 8553.220000000001, "end": 8557.460000000001, "text": " We want you want to like jack the temperature up like crazy. Let's give it a temperature of 10. Let's see what we get", "tokens": [50720, 492, 528, 291, 528, 281, 411, 7109, 264, 4292, 493, 411, 3219, 13, 961, 311, 976, 309, 257, 4292, 295, 1266, 13, 961, 311, 536, 437, 321, 483, 50932], "temperature": 0.0, "avg_logprob": -0.11828139276787786, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0005976344691589475}, {"id": 1120, "seek": 854610, "start": 8558.5, "end": 8560.5, "text": " Hopefully it'll kind of like go off the rails", "tokens": [50984, 10429, 309, 603, 733, 295, 411, 352, 766, 264, 27649, 51084], "temperature": 0.0, "avg_logprob": -0.11828139276787786, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0005976344691589475}, {"id": 1121, "seek": 854610, "start": 8564.02, "end": 8567.62, "text": " There we go it went off the rails see it went off the rails too much", "tokens": [51260, 821, 321, 352, 309, 1437, 766, 264, 27649, 536, 309, 1437, 766, 264, 27649, 886, 709, 51440], "temperature": 0.0, "avg_logprob": -0.11828139276787786, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0005976344691589475}, {"id": 1122, "seek": 854610, "start": 8568.18, "end": 8571.460000000001, "text": " So let's try a temperature two and maybe it'll go off the rails less", "tokens": [51468, 407, 718, 311, 853, 257, 4292, 732, 293, 1310, 309, 603, 352, 766, 264, 27649, 1570, 51632], "temperature": 0.0, "avg_logprob": -0.11828139276787786, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.0005976344691589475}, {"id": 1123, "seek": 857146, "start": 8572.259999999998, "end": 8579.46, "text": " Four Pacific NBC learning. Okay. Well kind of went off the rails", "tokens": [50404, 7451, 13335, 31504, 2539, 13, 1033, 13, 1042, 733, 295, 1437, 766, 264, 27649, 50764], "temperature": 0.0, "avg_logprob": -0.3259279300005008, "compression_ratio": 1.11864406779661, "no_speech_prob": 0.0009110336541198194}, {"id": 1124, "seek": 857146, "start": 8580.58, "end": 8585.14, "text": " Um, so 0.7 is probably a uh a good middle ground", "tokens": [50820, 3301, 11, 370, 1958, 13, 22, 307, 1391, 257, 2232, 257, 665, 2808, 2727, 51048], "temperature": 0.0, "avg_logprob": -0.3259279300005008, "compression_ratio": 1.11864406779661, "no_speech_prob": 0.0009110336541198194}, {"id": 1125, "seek": 857146, "start": 8588.099999999999, "end": 8590.98, "text": " Four good reliable", "tokens": [51196, 7451, 665, 12924, 51340], "temperature": 0.0, "avg_logprob": -0.3259279300005008, "compression_ratio": 1.11864406779661, "no_speech_prob": 0.0009110336541198194}, {"id": 1126, "seek": 859098, "start": 8590.98, "end": 8592.98, "text": " All right", "tokens": [50364, 1057, 558, 50464], "temperature": 0.0, "avg_logprob": -0.3448175929841541, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.004069729242473841}, {"id": 1127, "seek": 859098, "start": 8598.82, "end": 8607.619999999999, "text": " Improving mathematical reasoning with process supervision download data set. So this is the data set apparently", "tokens": [50756, 8270, 340, 798, 18894, 21577, 365, 1399, 32675, 5484, 1412, 992, 13, 407, 341, 307, 264, 1412, 992, 7970, 51196], "temperature": 0.0, "avg_logprob": -0.3448175929841541, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.004069729242473841}, {"id": 1128, "seek": 859098, "start": 8610.82, "end": 8615.3, "text": " We won't get LFS is that why it didn't work", "tokens": [51356, 492, 1582, 380, 483, 441, 29318, 307, 300, 983, 309, 994, 380, 589, 51580], "temperature": 0.0, "avg_logprob": -0.3448175929841541, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.004069729242473841}, {"id": 1129, "seek": 861530, "start": 8615.3, "end": 8617.3, "text": " I", "tokens": [50364, 286, 50464], "temperature": 0.0, "avg_logprob": -0.19287628876535515, "compression_ratio": 1.2, "no_speech_prob": 0.0034293343778699636}, {"id": 1130, "seek": 861530, "start": 8621.539999999999, "end": 8623.539999999999, "text": " Hate get LFS", "tokens": [50676, 46000, 483, 441, 29318, 50776], "temperature": 0.0, "avg_logprob": -0.19287628876535515, "compression_ratio": 1.2, "no_speech_prob": 0.0034293343778699636}, {"id": 1131, "seek": 861530, "start": 8627.619999999999, "end": 8629.619999999999, "text": " Get LFS fetch", "tokens": [50980, 3240, 441, 29318, 23673, 51080], "temperature": 0.0, "avg_logprob": -0.19287628876535515, "compression_ratio": 1.2, "no_speech_prob": 0.0034293343778699636}, {"id": 1132, "seek": 861530, "start": 8633.14, "end": 8637.3, "text": " Get LFS fetch please LFS at comma two. I don't really know how to do this", "tokens": [51256, 3240, 441, 29318, 23673, 1767, 441, 29318, 412, 22117, 732, 13, 286, 500, 380, 534, 458, 577, 281, 360, 341, 51464], "temperature": 0.0, "avg_logprob": -0.19287628876535515, "compression_ratio": 1.2, "no_speech_prob": 0.0034293343778699636}, {"id": 1133, "seek": 864530, "start": 8646.259999999998, "end": 8648.259999999998, "text": " I gotta install", "tokens": [50412, 286, 3428, 3625, 50512], "temperature": 0.0, "avg_logprob": -0.29288932800292966, "compression_ratio": 1.2773109243697478, "no_speech_prob": 0.0018101660534739494}, {"id": 1134, "seek": 864530, "start": 8650.74, "end": 8652.74, "text": " Get LFS", "tokens": [50636, 3240, 441, 29318, 50736], "temperature": 0.0, "avg_logprob": -0.29288932800292966, "compression_ratio": 1.2773109243697478, "no_speech_prob": 0.0018101660534739494}, {"id": 1135, "seek": 864530, "start": 8655.539999999999, "end": 8663.859999999999, "text": " Yeah, yeah, that's that's pretty much why you have those things. Uh, I forget LFS uh, install rsx", "tokens": [50876, 865, 11, 1338, 11, 300, 311, 300, 311, 1238, 709, 983, 291, 362, 729, 721, 13, 4019, 11, 286, 2870, 441, 29318, 2232, 11, 3625, 367, 82, 87, 51292], "temperature": 0.0, "avg_logprob": -0.29288932800292966, "compression_ratio": 1.2773109243697478, "no_speech_prob": 0.0018101660534739494}, {"id": 1136, "seek": 864530, "start": 8665.14, "end": 8667.619999999999, "text": " All right, that looks terrible", "tokens": [51356, 1057, 558, 11, 300, 1542, 6237, 51480], "temperature": 0.0, "avg_logprob": -0.29288932800292966, "compression_ratio": 1.2773109243697478, "no_speech_prob": 0.0018101660534739494}, {"id": 1137, "seek": 866762, "start": 8667.78, "end": 8669.78, "text": " So", "tokens": [50372, 407, 50472], "temperature": 0.0, "avg_logprob": -0.2929723009150079, "compression_ratio": 1.3277310924369747, "no_speech_prob": 0.004264382645487785}, {"id": 1138, "seek": 866762, "start": 8671.380000000001, "end": 8673.78, "text": " Bru install get LFS, okay, let's try", "tokens": [50552, 12792, 3625, 483, 441, 29318, 11, 1392, 11, 718, 311, 853, 50672], "temperature": 0.0, "avg_logprob": -0.2929723009150079, "compression_ratio": 1.3277310924369747, "no_speech_prob": 0.004264382645487785}, {"id": 1139, "seek": 866762, "start": 8681.78, "end": 8683.78, "text": " Okay, that seemed to work", "tokens": [51072, 1033, 11, 300, 6576, 281, 589, 51172], "temperature": 0.0, "avg_logprob": -0.2929723009150079, "compression_ratio": 1.3277310924369747, "no_speech_prob": 0.004264382645487785}, {"id": 1140, "seek": 866762, "start": 8685.62, "end": 8692.18, "text": " Mostly we're downloading we're downloading the same data set that was used on", "tokens": [51264, 29035, 321, 434, 32529, 321, 434, 32529, 264, 912, 1412, 992, 300, 390, 1143, 322, 51592], "temperature": 0.0, "avg_logprob": -0.2929723009150079, "compression_ratio": 1.3277310924369747, "no_speech_prob": 0.004264382645487785}, {"id": 1141, "seek": 866762, "start": 8694.980000000001, "end": 8696.980000000001, "text": " On uh official", "tokens": [51732, 1282, 2232, 4783, 51832], "temperature": 0.0, "avg_logprob": -0.2929723009150079, "compression_ratio": 1.3277310924369747, "no_speech_prob": 0.004264382645487785}, {"id": 1142, "seek": 869762, "start": 8698.58, "end": 8703.78, "text": " Official q q star why does the data still look like that?", "tokens": [50412, 38577, 9505, 9505, 3543, 983, 775, 264, 1412, 920, 574, 411, 300, 30, 50672], "temperature": 0.0, "avg_logprob": -0.30137265812266956, "compression_ratio": 1.209090909090909, "no_speech_prob": 0.0005883837584406137}, {"id": 1143, "seek": 869762, "start": 8713.220000000001, "end": 8721.060000000001, "text": " Okay, there we go. Perfect. What's a json l file? It's like a list of jsons", "tokens": [51144, 1033, 11, 456, 321, 352, 13, 10246, 13, 708, 311, 257, 361, 3015, 287, 3991, 30, 467, 311, 411, 257, 1329, 295, 361, 82, 892, 51536], "temperature": 0.0, "avg_logprob": -0.30137265812266956, "compression_ratio": 1.209090909090909, "no_speech_prob": 0.0005883837584406137}, {"id": 1144, "seek": 872106, "start": 8721.06, "end": 8728.18, "text": " Ever hear that before?", "tokens": [50364, 12123, 1568, 300, 949, 30, 50720], "temperature": 0.0, "avg_logprob": -0.34703932053003556, "compression_ratio": 1.1320754716981132, "no_speech_prob": 0.0033242821227759123}, {"id": 1145, "seek": 872106, "start": 8731.06, "end": 8738.58, "text": " No json lines. Oh, I see. Okay. Well, that seems pretty cool. So let's load up one of these files", "tokens": [50864, 883, 361, 3015, 3876, 13, 876, 11, 286, 536, 13, 1033, 13, 1042, 11, 300, 2544, 1238, 1627, 13, 407, 718, 311, 3677, 493, 472, 295, 613, 7098, 51240], "temperature": 0.0, "avg_logprob": -0.34703932053003556, "compression_ratio": 1.1320754716981132, "no_speech_prob": 0.0033242821227759123}, {"id": 1146, "seek": 873858, "start": 8738.58, "end": 8748.5, "text": " Let's do what we factoring", "tokens": [50364, 961, 311, 360, 437, 321, 1186, 3662, 50860], "temperature": 0.0, "avg_logprob": -0.31520976660386574, "compression_ratio": 1.3082706766917294, "no_speech_prob": 0.01542218029499054}, {"id": 1147, "seek": 873858, "start": 8751.46, "end": 8753.46, "text": " This doesn't need anything", "tokens": [51008, 639, 1177, 380, 643, 1340, 51108], "temperature": 0.0, "avg_logprob": -0.31520976660386574, "compression_ratio": 1.3082706766917294, "no_speech_prob": 0.01542218029499054}, {"id": 1148, "seek": 873858, "start": 8757.46, "end": 8764.66, "text": " Putting that in here, I know you didn't like it. You are a subscriber, but you know, we gotta do it feels right as go", "tokens": [51308, 31367, 300, 294, 510, 11, 286, 458, 291, 994, 380, 411, 309, 13, 509, 366, 257, 26122, 11, 457, 291, 458, 11, 321, 3428, 360, 309, 3417, 558, 382, 352, 51668], "temperature": 0.0, "avg_logprob": -0.31520976660386574, "compression_ratio": 1.3082706766917294, "no_speech_prob": 0.01542218029499054}, {"id": 1149, "seek": 873858, "start": 8765.46, "end": 8767.46, "text": " um", "tokens": [51708, 1105, 51808], "temperature": 0.0, "avg_logprob": -0.31520976660386574, "compression_ratio": 1.3082706766917294, "no_speech_prob": 0.01542218029499054}, {"id": 1150, "seek": 876746, "start": 8768.179999999998, "end": 8770.58, "text": " Create model cache", "tokens": [50400, 20248, 2316, 19459, 50520], "temperature": 0.0, "avg_logprob": -0.5629199468172513, "compression_ratio": 0.8695652173913043, "no_speech_prob": 0.021562427282333374}, {"id": 1151, "seek": 876746, "start": 8782.339999999998, "end": 8785.619999999999, "text": " So we can remove this", "tokens": [51108, 407, 321, 393, 4159, 341, 51272], "temperature": 0.0, "avg_logprob": -0.5629199468172513, "compression_ratio": 0.8695652173913043, "no_speech_prob": 0.021562427282333374}, {"id": 1152, "seek": 879746, "start": 8797.46, "end": 8799.46, "text": " Okay", "tokens": [50364, 1033, 50464], "temperature": 0.0, "avg_logprob": -0.4357624053955078, "compression_ratio": 0.8333333333333334, "no_speech_prob": 0.01384371891617775}, {"id": 1153, "seek": 879746, "start": 8817.539999999999, "end": 8823.38, "text": " All right phase one test dot json l", "tokens": [51368, 1057, 558, 5574, 472, 1500, 5893, 361, 3015, 287, 51660], "temperature": 0.0, "avg_logprob": -0.4357624053955078, "compression_ratio": 0.8333333333333334, "no_speech_prob": 0.01384371891617775}, {"id": 1154, "seek": 882746, "start": 8827.779999999999, "end": 8837.619999999999, "text": " We don't actually need SPP till we get down to here. Now we know that's reliable", "tokens": [50380, 492, 500, 380, 767, 643, 8420, 47, 4288, 321, 483, 760, 281, 510, 13, 823, 321, 458, 300, 311, 12924, 50872], "temperature": 0.0, "avg_logprob": -0.3556730633690244, "compression_ratio": 1.2586206896551724, "no_speech_prob": 0.0025501505006104708}, {"id": 1155, "seek": 882746, "start": 8838.98, "end": 8840.98, "text": " Let's just start there", "tokens": [50940, 961, 311, 445, 722, 456, 51040], "temperature": 0.0, "avg_logprob": -0.3556730633690244, "compression_ratio": 1.2586206896551724, "no_speech_prob": 0.0025501505006104708}, {"id": 1156, "seek": 882746, "start": 8847.779999999999, "end": 8850.179999999998, "text": " Let's look at our first piece of data here", "tokens": [51380, 961, 311, 574, 412, 527, 700, 2522, 295, 1412, 510, 51500], "temperature": 0.0, "avg_logprob": -0.3556730633690244, "compression_ratio": 1.2586206896551724, "no_speech_prob": 0.0025501505006104708}, {"id": 1157, "seek": 885746, "start": 8857.46, "end": 8859.46, "text": " So", "tokens": [50364, 407, 50464], "temperature": 0.0, "avg_logprob": -0.412172610943134, "compression_ratio": 1.0810810810810811, "no_speech_prob": 0.005057853646576405}, {"id": 1158, "seek": 885746, "start": 8863.46, "end": 8867.699999999999, "text": " No json loads when I'm taking a dumps we're taking the loads", "tokens": [50664, 883, 361, 3015, 12668, 562, 286, 478, 1940, 257, 11430, 82, 321, 434, 1940, 264, 12668, 50876], "temperature": 0.0, "avg_logprob": -0.412172610943134, "compression_ratio": 1.0810810810810811, "no_speech_prob": 0.005057853646576405}, {"id": 1159, "seek": 885746, "start": 8874.66, "end": 8876.66, "text": " Question problem", "tokens": [51224, 14464, 1154, 51324], "temperature": 0.0, "avg_logprob": -0.412172610943134, "compression_ratio": 1.0810810810810811, "no_speech_prob": 0.005057853646576405}, {"id": 1160, "seek": 888746, "start": 8887.859999999999, "end": 8889.859999999999, "text": " Okay", "tokens": [50384, 1033, 50484], "temperature": 0.0, "avg_logprob": -0.258863627910614, "compression_ratio": 1.0135135135135136, "no_speech_prob": 0.0020502933766692877}, {"id": 1161, "seek": 888746, "start": 8896.419999999998, "end": 8900.259999999998, "text": " Now we don't have the secret q star algorithm, but we'll give it a try", "tokens": [50812, 823, 321, 500, 380, 362, 264, 4054, 9505, 3543, 9284, 11, 457, 321, 603, 976, 309, 257, 853, 51004], "temperature": 0.0, "avg_logprob": -0.258863627910614, "compression_ratio": 1.0135135135135136, "no_speech_prob": 0.0020502933766692877}, {"id": 1162, "seek": 891746, "start": 8917.859999999999, "end": 8921.539999999999, "text": " What oh I forgot to return", "tokens": [50384, 708, 1954, 286, 5298, 281, 2736, 50568], "temperature": 0.0, "avg_logprob": -0.21473850309848785, "compression_ratio": 1.076086956521739, "no_speech_prob": 0.007457221858203411}, {"id": 1163, "seek": 891746, "start": 8930.82, "end": 8935.14, "text": " First we'll find the cost of the jumbo eraser. Oh, yeah, let's go q star", "tokens": [51032, 2386, 321, 603, 915, 264, 2063, 295, 264, 29067, 1763, 46018, 13, 876, 11, 1338, 11, 718, 311, 352, 9505, 3543, 51248], "temperature": 0.0, "avg_logprob": -0.21473850309848785, "compression_ratio": 1.076086956521739, "no_speech_prob": 0.007457221858203411}, {"id": 1164, "seek": 894746, "start": 8948.099999999999, "end": 8955.38, "text": " 29 cents, I don't know. Is it the right answer?", "tokens": [50396, 9413, 14941, 11, 286, 500, 380, 458, 13, 1119, 309, 264, 558, 1867, 30, 50760], "temperature": 0.0, "avg_logprob": -0.2785813531210256, "compression_ratio": 1.2258064516129032, "no_speech_prob": 0.003593157511204481}, {"id": 1165, "seek": 894746, "start": 8960.179999999998, "end": 8962.179999999998, "text": " I don't know", "tokens": [51000, 286, 500, 380, 458, 51100], "temperature": 0.0, "avg_logprob": -0.2785813531210256, "compression_ratio": 1.2258064516129032, "no_speech_prob": 0.003593157511204481}, {"id": 1166, "seek": 894746, "start": 8968.179999999998, "end": 8970.179999999998, "text": " Do we think it's 29 cents", "tokens": [51400, 1144, 321, 519, 309, 311, 9413, 14941, 51500], "temperature": 0.0, "avg_logprob": -0.2785813531210256, "compression_ratio": 1.2258064516129032, "no_speech_prob": 0.003593157511204481}, {"id": 1167, "seek": 894746, "start": 8972.179999999998, "end": 8974.179999999998, "text": " A pencil cost 29 cents", "tokens": [51600, 316, 10985, 2063, 9413, 14941, 51700], "temperature": 0.0, "avg_logprob": -0.2785813531210256, "compression_ratio": 1.2258064516129032, "no_speech_prob": 0.003593157511204481}, {"id": 1168, "seek": 894746, "start": 8975.06, "end": 8976.66, "text": " guys", "tokens": [51744, 1074, 51824], "temperature": 0.0, "avg_logprob": -0.2785813531210256, "compression_ratio": 1.2258064516129032, "no_speech_prob": 0.003593157511204481}, {"id": 1169, "seek": 897666, "start": 8976.66, "end": 8979.46, "text": " Wait, did we just use q star or what?", "tokens": [50364, 3802, 11, 630, 321, 445, 764, 9505, 3543, 420, 437, 30, 50504], "temperature": 0.0, "avg_logprob": -0.2183689215244391, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0019874591380357742}, {"id": 1170, "seek": 897666, "start": 8983.78, "end": 8986.82, "text": " Wait, it got the answer right. I don't know I couldn't even do that", "tokens": [50720, 3802, 11, 309, 658, 264, 1867, 558, 13, 286, 500, 380, 458, 286, 2809, 380, 754, 360, 300, 50872], "temperature": 0.0, "avg_logprob": -0.2183689215244391, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0019874591380357742}, {"id": 1171, "seek": 897666, "start": 8991.38, "end": 8993.38, "text": " Wait this model's so good", "tokens": [51100, 3802, 341, 2316, 311, 370, 665, 51200], "temperature": 0.0, "avg_logprob": -0.2183689215244391, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0019874591380357742}, {"id": 1172, "seek": 897666, "start": 8994.18, "end": 8997.22, "text": " Did the 7p model really just solve that shit?", "tokens": [51240, 2589, 264, 1614, 79, 2316, 534, 445, 5039, 300, 4611, 30, 51392], "temperature": 0.0, "avg_logprob": -0.2183689215244391, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0019874591380357742}, {"id": 1173, "seek": 897666, "start": 8998.02, "end": 9000.42, "text": " Now now you ask the question", "tokens": [51432, 823, 586, 291, 1029, 264, 1168, 51552], "temperature": 0.0, "avg_logprob": -0.2183689215244391, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0019874591380357742}, {"id": 1174, "seek": 897666, "start": 9001.539999999999, "end": 9004.82, "text": " What that that that that that that now you ask the question", "tokens": [51608, 708, 300, 300, 300, 300, 300, 300, 586, 291, 1029, 264, 1168, 51772], "temperature": 0.0, "avg_logprob": -0.2183689215244391, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0019874591380357742}, {"id": 1175, "seek": 900482, "start": 9005.779999999999, "end": 9008.02, "text": " Was it trained on that shit?", "tokens": [50412, 3027, 309, 8895, 322, 300, 4611, 30, 50524], "temperature": 0.0, "avg_logprob": -0.352940034866333, "compression_ratio": 0.95, "no_speech_prob": 0.0006070458330214024}, {"id": 1176, "seek": 900482, "start": 9019.38, "end": 9021.38, "text": " Yeah", "tokens": [51092, 865, 51192], "temperature": 0.0, "avg_logprob": -0.352940034866333, "compression_ratio": 0.95, "no_speech_prob": 0.0006070458330214024}, {"id": 1177, "seek": 900482, "start": 9024.42, "end": 9026.92, "text": " That did seem too smart", "tokens": [51344, 663, 630, 1643, 886, 4069, 51469], "temperature": 0.0, "avg_logprob": -0.352940034866333, "compression_ratio": 0.95, "no_speech_prob": 0.0006070458330214024}, {"id": 1178, "seek": 903482, "start": 9035.619999999999, "end": 9044.34, "text": " All right, let's make up our own math problem. We have to see if we're using real q learning or not", "tokens": [50404, 1057, 558, 11, 718, 311, 652, 493, 527, 1065, 5221, 1154, 13, 492, 362, 281, 536, 498, 321, 434, 1228, 957, 9505, 2539, 420, 406, 50840], "temperature": 0.0, "avg_logprob": -0.25879372721133026, "compression_ratio": 1.2479338842975207, "no_speech_prob": 0.0007553673931397498}, {"id": 1179, "seek": 903482, "start": 9050.74, "end": 9052.74, "text": " Uh, okay", "tokens": [51160, 4019, 11, 1392, 51260], "temperature": 0.0, "avg_logprob": -0.25879372721133026, "compression_ratio": 1.2479338842975207, "no_speech_prob": 0.0007553673931397498}, {"id": 1180, "seek": 903482, "start": 9053.46, "end": 9057.619999999999, "text": " A rocket costs three four dollars", "tokens": [51296, 316, 13012, 5497, 1045, 1451, 3808, 51504], "temperature": 0.0, "avg_logprob": -0.25879372721133026, "compression_ratio": 1.2479338842975207, "no_speech_prob": 0.0007553673931397498}, {"id": 1181, "seek": 903482, "start": 9058.9, "end": 9060.9, "text": " A pencil", "tokens": [51568, 316, 10985, 51668], "temperature": 0.0, "avg_logprob": -0.25879372721133026, "compression_ratio": 1.2479338842975207, "no_speech_prob": 0.0007553673931397498}, {"id": 1182, "seek": 906090, "start": 9060.9, "end": 9062.9, "text": " Costs one dollar", "tokens": [50364, 20863, 82, 472, 7241, 50464], "temperature": 0.0, "avg_logprob": -0.3193414052327474, "compression_ratio": 1.1294117647058823, "no_speech_prob": 0.010167279280722141}, {"id": 1183, "seek": 906090, "start": 9063.619999999999, "end": 9068.74, "text": " I spent five dollars and bought", "tokens": [50500, 286, 4418, 1732, 3808, 293, 4243, 50756], "temperature": 0.0, "avg_logprob": -0.3193414052327474, "compression_ratio": 1.1294117647058823, "no_speech_prob": 0.010167279280722141}, {"id": 1184, "seek": 906090, "start": 9070.98, "end": 9074.82, "text": " And and bought our rocket. What else did I buy?", "tokens": [50868, 400, 293, 4243, 527, 13012, 13, 708, 1646, 630, 286, 2256, 30, 51060], "temperature": 0.0, "avg_logprob": -0.3193414052327474, "compression_ratio": 1.1294117647058823, "no_speech_prob": 0.010167279280722141}, {"id": 1185, "seek": 907482, "start": 9075.3, "end": 9077.3, "text": " You", "tokens": [50388, 509, 50488], "temperature": 0.6000000000000001, "avg_logprob": -0.7497995625371519, "compression_ratio": 0.9852941176470589, "no_speech_prob": 0.018260495737195015}, {"id": 1186, "seek": 907482, "start": 9093.38, "end": 9095.38, "text": " All right", "tokens": [51292, 1057, 558, 51392], "temperature": 0.6000000000000001, "avg_logprob": -0.7497995625371519, "compression_ratio": 0.9852941176470589, "no_speech_prob": 0.018260495737195015}, {"id": 1187, "seek": 907482, "start": 9097.06, "end": 9099.06, "text": " That was kind of too easy", "tokens": [51476, 663, 390, 733, 295, 886, 1858, 51576], "temperature": 0.6000000000000001, "avg_logprob": -0.7497995625371519, "compression_ratio": 0.9852941176470589, "no_speech_prob": 0.018260495737195015}, {"id": 1188, "seek": 907482, "start": 9100.82, "end": 9103.779999999999, "text": " A chicken costs two dollars", "tokens": [51664, 316, 4662, 5497, 732, 3808, 51812], "temperature": 0.6000000000000001, "avg_logprob": -0.7497995625371519, "compression_ratio": 0.9852941176470589, "no_speech_prob": 0.018260495737195015}, {"id": 1189, "seek": 910482, "start": 9104.82, "end": 9113.82, "text": " I spent $7 and bought a rocket. What else did I buy? It's a trick question because you could have bought three pencils or a pencil and a chicken.", "tokens": [50364, 286, 4418, 1848, 22, 293, 4243, 257, 13012, 13, 708, 1646, 630, 286, 2256, 30, 467, 311, 257, 4282, 1168, 570, 291, 727, 362, 4243, 1045, 30857, 420, 257, 10985, 293, 257, 4662, 13, 50814], "temperature": 0.0, "avg_logprob": -0.29031576429094585, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.5950554013252258}, {"id": 1190, "seek": 910482, "start": 9116.82, "end": 9121.82, "text": " Oh. Well, I mean to be fair, it's kind of right.", "tokens": [50964, 876, 13, 1042, 11, 286, 914, 281, 312, 3143, 11, 309, 311, 733, 295, 558, 13, 51214], "temperature": 0.0, "avg_logprob": -0.29031576429094585, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.5950554013252258}, {"id": 1191, "seek": 912182, "start": 9121.82, "end": 9142.82, "text": " Okay, wait, can you guys come up with problems?", "tokens": [50364, 1033, 11, 1699, 11, 393, 291, 1074, 808, 493, 365, 2740, 30, 51414], "temperature": 0.0, "avg_logprob": -0.5097906430562337, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.06753275543451309}, {"id": 1192, "seek": 915182, "start": 9151.82, "end": 9161.82, "text": " Yeah. Let's see if I can get this.", "tokens": [50364, 865, 13, 961, 311, 536, 498, 286, 393, 483, 341, 13, 50864], "temperature": 0.0, "avg_logprob": -0.32990572452545164, "compression_ratio": 0.8461538461538461, "no_speech_prob": 0.1297139823436737}, {"id": 1193, "seek": 915182, "start": 9168.82, "end": 9171.82, "text": " Oh, boys.", "tokens": [51214, 876, 11, 6347, 13, 51364], "temperature": 0.0, "avg_logprob": -0.32990572452545164, "compression_ratio": 0.8461538461538461, "no_speech_prob": 0.1297139823436737}, {"id": 1194, "seek": 918182, "start": 9182.82, "end": 9189.82, "text": " Yeah, yeah, yeah, we got a problem about a streetlight.", "tokens": [50414, 865, 11, 1338, 11, 1338, 11, 321, 658, 257, 1154, 466, 257, 4838, 2764, 13, 50764], "temperature": 0.0, "avg_logprob": -0.3220294394144198, "compression_ratio": 1.2872340425531914, "no_speech_prob": 0.028835712000727654}, {"id": 1195, "seek": 918182, "start": 9199.82, "end": 9202.82, "text": " Did you steal this problem somewhere? Did you make it up?", "tokens": [51264, 2589, 291, 11009, 341, 1154, 4079, 30, 2589, 291, 652, 309, 493, 30, 51414], "temperature": 0.0, "avg_logprob": -0.3220294394144198, "compression_ratio": 1.2872340425531914, "no_speech_prob": 0.028835712000727654}, {"id": 1196, "seek": 918182, "start": 9205.82, "end": 9206.82, "text": " Oh.", "tokens": [51564, 876, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3220294394144198, "compression_ratio": 1.2872340425531914, "no_speech_prob": 0.028835712000727654}, {"id": 1197, "seek": 918182, "start": 9208.82, "end": 9209.82, "text": " Oh.", "tokens": [51714, 876, 13, 51764], "temperature": 0.0, "avg_logprob": -0.3220294394144198, "compression_ratio": 1.2872340425531914, "no_speech_prob": 0.028835712000727654}, {"id": 1198, "seek": 921182, "start": 9212.82, "end": 9215.82, "text": " We're drawing something.", "tokens": [50414, 492, 434, 6316, 746, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21823449432849884, "compression_ratio": 1.1956521739130435, "no_speech_prob": 0.00955066829919815}, {"id": 1199, "seek": 921182, "start": 9231.82, "end": 9237.82, "text": " Is this Python? This isn't Python. You can't say real light and real woman in Python.", "tokens": [51364, 1119, 341, 15329, 30, 639, 1943, 380, 15329, 13, 509, 393, 380, 584, 957, 1442, 293, 957, 3059, 294, 15329, 13, 51664], "temperature": 0.0, "avg_logprob": -0.21823449432849884, "compression_ratio": 1.1956521739130435, "no_speech_prob": 0.00955066829919815}, {"id": 1200, "seek": 923782, "start": 9237.82, "end": 9241.82, "text": " Is that right?", "tokens": [50364, 1119, 300, 558, 30, 50564], "temperature": 0.0, "avg_logprob": -0.3026813879245665, "compression_ratio": 1.1565217391304348, "no_speech_prob": 0.06652244925498962}, {"id": 1201, "seek": 923782, "start": 9243.82, "end": 9250.82, "text": " You can't just do this. This is the most broken Python I've ever seen.", "tokens": [50664, 509, 393, 380, 445, 360, 341, 13, 639, 307, 264, 881, 5463, 15329, 286, 600, 1562, 1612, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3026813879245665, "compression_ratio": 1.1565217391304348, "no_speech_prob": 0.06652244925498962}, {"id": 1202, "seek": 923782, "start": 9251.82, "end": 9254.82, "text": " Wait, should we allow the user to keep talking?", "tokens": [51064, 3802, 11, 820, 321, 2089, 264, 4195, 281, 1066, 1417, 30, 51214], "temperature": 0.0, "avg_logprob": -0.3026813879245665, "compression_ratio": 1.1565217391304348, "no_speech_prob": 0.06652244925498962}, {"id": 1203, "seek": 925482, "start": 9254.82, "end": 9266.82, "text": " Should we fix the chatbot so I can keep talking?", "tokens": [50364, 6454, 321, 3191, 264, 5081, 18870, 370, 286, 393, 1066, 1417, 30, 50964], "temperature": 0.0, "avg_logprob": -0.3190600321843074, "compression_ratio": 0.9605263157894737, "no_speech_prob": 0.059160105884075165}, {"id": 1204, "seek": 925482, "start": 9276.82, "end": 9278.82, "text": " Yeah, I know it drew it.", "tokens": [51464, 865, 11, 286, 458, 309, 12804, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3190600321843074, "compression_ratio": 0.9605263157894737, "no_speech_prob": 0.059160105884075165}, {"id": 1205, "seek": 927882, "start": 9278.82, "end": 9286.82, "text": " All right. Thank you for subscribing. I appreciate you.", "tokens": [50364, 1057, 558, 13, 1044, 291, 337, 19981, 13, 286, 4449, 291, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17576383989910746, "compression_ratio": 1.3305084745762712, "no_speech_prob": 0.02363898605108261}, {"id": 1206, "seek": 927882, "start": 9293.82, "end": 9295.82, "text": " We're going to make the chatbot so I can keep talking.", "tokens": [51114, 492, 434, 516, 281, 652, 264, 5081, 18870, 370, 286, 393, 1066, 1417, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17576383989910746, "compression_ratio": 1.3305084745762712, "no_speech_prob": 0.02363898605108261}, {"id": 1207, "seek": 927882, "start": 9296.82, "end": 9299.82, "text": " You're functioning well. Thank you for asking.", "tokens": [51264, 509, 434, 18483, 731, 13, 1044, 291, 337, 3365, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17576383989910746, "compression_ratio": 1.3305084745762712, "no_speech_prob": 0.02363898605108261}, {"id": 1208, "seek": 930882, "start": 9308.82, "end": 9315.82, "text": " I don't need a max length anymore. It's stupid.", "tokens": [50414, 286, 500, 380, 643, 257, 11469, 4641, 3602, 13, 467, 311, 6631, 13, 50714], "temperature": 0.0, "avg_logprob": -0.28716471791267395, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.09120890498161316}, {"id": 1209, "seek": 933882, "start": 9338.82, "end": 9351.82, "text": " Okay, we need to get data from the user. Is it raw input? How do I get data in Python?", "tokens": [50364, 1033, 11, 321, 643, 281, 483, 1412, 490, 264, 4195, 13, 1119, 309, 8936, 4846, 30, 1012, 360, 286, 483, 1412, 294, 15329, 30, 51014], "temperature": 0.0, "avg_logprob": -0.23509581272418684, "compression_ratio": 1.3495934959349594, "no_speech_prob": 0.035135675221681595}, {"id": 1210, "seek": 933882, "start": 9352.82, "end": 9357.82, "text": " It's not input. You have to do the other one. Or maybe it is input in Python 3.", "tokens": [51064, 467, 311, 406, 4846, 13, 509, 362, 281, 360, 264, 661, 472, 13, 1610, 1310, 309, 307, 4846, 294, 15329, 805, 13, 51314], "temperature": 0.0, "avg_logprob": -0.23509581272418684, "compression_ratio": 1.3495934959349594, "no_speech_prob": 0.035135675221681595}, {"id": 1211, "seek": 935782, "start": 9358.82, "end": 9361.82, "text": " We're just trying to understand it, but the stairs are perfect.", "tokens": [50414, 492, 434, 445, 1382, 281, 1223, 309, 11, 457, 264, 13471, 366, 2176, 13, 50564], "temperature": 0.0, "avg_logprob": -0.4945673200819227, "compression_ratio": 1.2321428571428572, "no_speech_prob": 0.09932371973991394}, {"id": 1212, "seek": 935782, "start": 9362.82, "end": 9363.82, "text": " User.", "tokens": [50614, 32127, 13, 50664], "temperature": 0.0, "avg_logprob": -0.4945673200819227, "compression_ratio": 1.2321428571428572, "no_speech_prob": 0.09932371973991394}, {"id": 1213, "seek": 935782, "start": 9364.82, "end": 9365.82, "text": " Input.", "tokens": [50714, 682, 2582, 13, 50764], "temperature": 0.0, "avg_logprob": -0.4945673200819227, "compression_ratio": 1.2321428571428572, "no_speech_prob": 0.09932371973991394}, {"id": 1214, "seek": 935782, "start": 9367.82, "end": 9368.82, "text": " Encode.", "tokens": [50864, 2193, 22332, 13, 50914], "temperature": 0.0, "avg_logprob": -0.4945673200819227, "compression_ratio": 1.2321428571428572, "no_speech_prob": 0.09932371973991394}, {"id": 1215, "seek": 935782, "start": 9369.82, "end": 9374.82, "text": " I'm trying to say here with the system default false.", "tokens": [50964, 286, 478, 1382, 281, 584, 510, 365, 264, 1185, 7576, 7908, 13, 51214], "temperature": 0.0, "avg_logprob": -0.4945673200819227, "compression_ratio": 1.2321428571428572, "no_speech_prob": 0.09932371973991394}, {"id": 1216, "seek": 938782, "start": 9388.82, "end": 9390.82, "text": " I don't know if this works.", "tokens": [50414, 286, 500, 380, 458, 498, 341, 1985, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1587423426764352, "compression_ratio": 1.082191780821918, "no_speech_prob": 0.024410877376794815}, {"id": 1217, "seek": 938782, "start": 9403.82, "end": 9405.82, "text": " Too many values to unpack.", "tokens": [51164, 11395, 867, 4190, 281, 26699, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1587423426764352, "compression_ratio": 1.082191780821918, "no_speech_prob": 0.024410877376794815}, {"id": 1218, "seek": 938782, "start": 9409.82, "end": 9411.82, "text": " We'll see if this works.", "tokens": [51464, 492, 603, 536, 498, 341, 1985, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1587423426764352, "compression_ratio": 1.082191780821918, "no_speech_prob": 0.024410877376794815}, {"id": 1219, "seek": 941782, "start": 9417.82, "end": 9430.82, "text": " Okay, seems like it kind of works.", "tokens": [50414, 1033, 11, 2544, 411, 309, 733, 295, 1985, 13, 51014], "temperature": 0.0, "avg_logprob": -0.44205109278361004, "compression_ratio": 0.8095238095238095, "no_speech_prob": 0.04139667749404907}, {"id": 1220, "seek": 944782, "start": 9448.82, "end": 9454.82, "text": " I'll set that print there.", "tokens": [50414, 286, 603, 992, 300, 4482, 456, 13, 50714], "temperature": 0.0, "avg_logprob": -0.49081737344915216, "compression_ratio": 0.8387096774193549, "no_speech_prob": 0.0961543470621109}, {"id": 1221, "seek": 945482, "start": 9454.82, "end": 9480.82, "text": " So what math problems do we got?", "tokens": [50364, 407, 437, 5221, 2740, 360, 321, 658, 30, 51664], "temperature": 0.0, "avg_logprob": -0.5418479659340598, "compression_ratio": 0.8205128205128205, "no_speech_prob": 0.01664148084819317}, {"id": 1222, "seek": 948482, "start": 9484.82, "end": 9500.82, "text": " Is that right? Seems kind of right.", "tokens": [50364, 1119, 300, 558, 30, 22524, 733, 295, 558, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2361902395884196, "compression_ratio": 0.8974358974358975, "no_speech_prob": 0.013202954083681107}, {"id": 1223, "seek": 951482, "start": 9514.82, "end": 9523.82, "text": " Pretty good. Pretty good.", "tokens": [50364, 10693, 665, 13, 10693, 665, 13, 50814], "temperature": 0.0, "avg_logprob": -0.4358743141437399, "compression_ratio": 1.2388059701492538, "no_speech_prob": 0.020619511604309082}, {"id": 1224, "seek": 951482, "start": 9526.82, "end": 9528.82, "text": " Whoa, whoa, whoa, whoa.", "tokens": [50964, 7521, 11, 13310, 11, 13310, 11, 13310, 13, 51064], "temperature": 0.0, "avg_logprob": -0.4358743141437399, "compression_ratio": 1.2388059701492538, "no_speech_prob": 0.020619511604309082}, {"id": 1225, "seek": 951482, "start": 9533.82, "end": 9535.82, "text": " Oh, excuse the quadratic formula.", "tokens": [51314, 876, 11, 8960, 264, 37262, 8513, 13, 51414], "temperature": 0.0, "avg_logprob": -0.4358743141437399, "compression_ratio": 1.2388059701492538, "no_speech_prob": 0.020619511604309082}, {"id": 1226, "seek": 953582, "start": 9535.82, "end": 9537.82, "text": " Oh, let's go.", "tokens": [50364, 876, 11, 718, 311, 352, 13, 50464], "temperature": 0.0, "avg_logprob": -0.18734525174510722, "compression_ratio": 1.194915254237288, "no_speech_prob": 0.014499567449092865}, {"id": 1227, "seek": 953582, "start": 9546.82, "end": 9552.82, "text": " Why'd you pick one that has negatives in the square root? All right, let's see. See if it's right.", "tokens": [50914, 1545, 1116, 291, 1888, 472, 300, 575, 40019, 294, 264, 3732, 5593, 30, 1057, 558, 11, 718, 311, 536, 13, 3008, 498, 309, 311, 558, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18734525174510722, "compression_ratio": 1.194915254237288, "no_speech_prob": 0.014499567449092865}, {"id": 1228, "seek": 953582, "start": 9558.82, "end": 9560.82, "text": " You guys, I'm so much. Wait.", "tokens": [51514, 509, 1074, 11, 286, 478, 370, 709, 13, 3802, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18734525174510722, "compression_ratio": 1.194915254237288, "no_speech_prob": 0.014499567449092865}, {"id": 1229, "seek": 956582, "start": 9565.82, "end": 9567.82, "text": " Can't take the square root of negative 11.", "tokens": [50364, 1664, 380, 747, 264, 3732, 5593, 295, 3671, 2975, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1178698410858979, "compression_ratio": 1.2596153846153846, "no_speech_prob": 0.0035931686870753765}, {"id": 1230, "seek": 956582, "start": 9575.82, "end": 9579.82, "text": " Yeah, that doesn't sound like I don't think that one has roots or has eyes in the roots.", "tokens": [50864, 865, 11, 300, 1177, 380, 1626, 411, 286, 500, 380, 519, 300, 472, 575, 10669, 420, 575, 2575, 294, 264, 10669, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1178698410858979, "compression_ratio": 1.2596153846153846, "no_speech_prob": 0.0035931686870753765}, {"id": 1231, "seek": 959582, "start": 9595.82, "end": 9600.82, "text": " By the way, this model's so good. Technium. So good.", "tokens": [50364, 3146, 264, 636, 11, 341, 2316, 311, 370, 665, 13, 8337, 2197, 13, 407, 665, 13, 50614], "temperature": 0.0, "avg_logprob": -0.19133184267126996, "compression_ratio": 1.0, "no_speech_prob": 0.015416178852319717}, {"id": 1232, "seek": 959582, "start": 9611.82, "end": 9612.82, "text": " Wait.", "tokens": [51164, 3802, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19133184267126996, "compression_ratio": 1.0, "no_speech_prob": 0.015416178852319717}, {"id": 1233, "seek": 962582, "start": 9626.82, "end": 9635.82, "text": " Oh, I see we ran into a problem with the max context length.", "tokens": [50414, 876, 11, 286, 536, 321, 5872, 666, 257, 1154, 365, 264, 11469, 4319, 4641, 13, 50864], "temperature": 0.0, "avg_logprob": -0.27096792062123615, "compression_ratio": 1.3360655737704918, "no_speech_prob": 0.010650154203176498}, {"id": 1234, "seek": 962582, "start": 9636.82, "end": 9640.82, "text": " We shouldn't actually have, here we go. Why is max context only this?", "tokens": [50914, 492, 4659, 380, 767, 362, 11, 510, 321, 352, 13, 1545, 307, 11469, 4319, 787, 341, 30, 51114], "temperature": 0.0, "avg_logprob": -0.27096792062123615, "compression_ratio": 1.3360655737704918, "no_speech_prob": 0.010650154203176498}, {"id": 1235, "seek": 962582, "start": 9647.82, "end": 9649.82, "text": " We can at least start with this.", "tokens": [51464, 492, 393, 412, 1935, 722, 365, 341, 13, 51564], "temperature": 0.0, "avg_logprob": -0.27096792062123615, "compression_ratio": 1.3360655737704918, "no_speech_prob": 0.010650154203176498}, {"id": 1236, "seek": 964982, "start": 9650.82, "end": 9655.82, "text": " I did this in GPT too, right? Yeah.", "tokens": [50414, 286, 630, 341, 294, 26039, 51, 886, 11, 558, 30, 865, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2259856632777623, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.002631495473906398}, {"id": 1237, "seek": 964982, "start": 9660.82, "end": 9662.82, "text": " By the way, this is all in tiny grab guys.", "tokens": [50914, 3146, 264, 636, 11, 341, 307, 439, 294, 5870, 4444, 1074, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2259856632777623, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.002631495473906398}, {"id": 1238, "seek": 964982, "start": 9663.82, "end": 9667.82, "text": " Like there comes a point where your library is good enough that you don't waste tons of time dealing with your library.", "tokens": [51064, 1743, 456, 1487, 257, 935, 689, 428, 6405, 307, 665, 1547, 300, 291, 500, 380, 5964, 9131, 295, 565, 6260, 365, 428, 6405, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2259856632777623, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.002631495473906398}, {"id": 1239, "seek": 964982, "start": 9672.82, "end": 9674.82, "text": " Python has built in image.", "tokens": [51514, 15329, 575, 3094, 294, 3256, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2259856632777623, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.002631495473906398}, {"id": 1240, "seek": 967982, "start": 9679.82, "end": 9680.82, "text": " Okay.", "tokens": [50364, 1033, 13, 50414], "temperature": 0.0, "avg_logprob": -0.45172573969914365, "compression_ratio": 0.8787878787878788, "no_speech_prob": 0.0031235567294061184}, {"id": 1241, "seek": 967982, "start": 9692.82, "end": 9694.82, "text": " Like the square root of 11 sure, but", "tokens": [51014, 1743, 264, 3732, 5593, 295, 2975, 988, 11, 457, 51114], "temperature": 0.0, "avg_logprob": -0.45172573969914365, "compression_ratio": 0.8787878787878788, "no_speech_prob": 0.0031235567294061184}, {"id": 1242, "seek": 967982, "start": 9703.82, "end": 9704.82, "text": " No.", "tokens": [51564, 883, 13, 51614], "temperature": 0.0, "avg_logprob": -0.45172573969914365, "compression_ratio": 0.8787878787878788, "no_speech_prob": 0.0031235567294061184}, {"id": 1243, "seek": 967982, "start": 9706.82, "end": 9708.82, "text": " Type one J.", "tokens": [51714, 15576, 472, 508, 13, 51814], "temperature": 0.0, "avg_logprob": -0.45172573969914365, "compression_ratio": 0.8787878787878788, "no_speech_prob": 0.0031235567294061184}, {"id": 1244, "seek": 970982, "start": 9710.82, "end": 9712.82, "text": " How do I get a J?", "tokens": [50414, 1012, 360, 286, 483, 257, 508, 30, 50514], "temperature": 0.0, "avg_logprob": -0.2091727660874189, "compression_ratio": 1.27007299270073, "no_speech_prob": 0.0005614581750705838}, {"id": 1245, "seek": 970982, "start": 9717.82, "end": 9719.82, "text": " Yo, we got a J. I'll see if it was right.", "tokens": [50764, 7616, 11, 321, 658, 257, 508, 13, 286, 603, 536, 498, 309, 390, 558, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2091727660874189, "compression_ratio": 1.27007299270073, "no_speech_prob": 0.0005614581750705838}, {"id": 1246, "seek": 970982, "start": 9727.82, "end": 9730.82, "text": " Okay, so now we have a root for the quadratic equation.", "tokens": [51264, 1033, 11, 370, 586, 321, 362, 257, 5593, 337, 264, 37262, 5367, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2091727660874189, "compression_ratio": 1.27007299270073, "no_speech_prob": 0.0005614581750705838}, {"id": 1247, "seek": 970982, "start": 9733.82, "end": 9736.82, "text": " X equals root. Come on. Do I remember my high school math?", "tokens": [51564, 1783, 6915, 5593, 13, 2492, 322, 13, 1144, 286, 1604, 452, 1090, 1395, 5221, 30, 51714], "temperature": 0.0, "avg_logprob": -0.2091727660874189, "compression_ratio": 1.27007299270073, "no_speech_prob": 0.0005614581750705838}, {"id": 1248, "seek": 973982, "start": 9740.82, "end": 9743.82, "text": " Zero J. Let's go.", "tokens": [50414, 17182, 508, 13, 961, 311, 352, 13, 50564], "temperature": 0.0, "avg_logprob": -0.22497160732746124, "compression_ratio": 1.2622950819672132, "no_speech_prob": 0.0020187445916235447}, {"id": 1249, "seek": 973982, "start": 9748.82, "end": 9751.82, "text": " The roots were correct. Okay. Okay. Okay. Okay. Okay.", "tokens": [50814, 440, 10669, 645, 3006, 13, 1033, 13, 1033, 13, 1033, 13, 1033, 13, 1033, 13, 50964], "temperature": 0.0, "avg_logprob": -0.22497160732746124, "compression_ratio": 1.2622950819672132, "no_speech_prob": 0.0020187445916235447}, {"id": 1250, "seek": 973982, "start": 9762.82, "end": 9763.82, "text": " Whoa.", "tokens": [51514, 7521, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22497160732746124, "compression_ratio": 1.2622950819672132, "no_speech_prob": 0.0020187445916235447}, {"id": 1251, "seek": 976382, "start": 9763.82, "end": 9764.82, "text": " Whoa.", "tokens": [50364, 7521, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1682150387069554, "compression_ratio": 1.4476190476190476, "no_speech_prob": 0.001838349038735032}, {"id": 1252, "seek": 976382, "start": 9768.82, "end": 9772.82, "text": " Oh my God, guys, we need the Bellman equation. We've been doing this all wrong.", "tokens": [50614, 876, 452, 1265, 11, 1074, 11, 321, 643, 264, 11485, 1601, 5367, 13, 492, 600, 668, 884, 341, 439, 2085, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1682150387069554, "compression_ratio": 1.4476190476190476, "no_speech_prob": 0.001838349038735032}, {"id": 1253, "seek": 976382, "start": 9772.82, "end": 9776.82, "text": " Why did we waste time with LLMs? LLMs were a red herring.", "tokens": [50814, 1545, 630, 321, 5964, 565, 365, 441, 43, 26386, 30, 441, 43, 26386, 645, 257, 2182, 720, 2937, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1682150387069554, "compression_ratio": 1.4476190476190476, "no_speech_prob": 0.001838349038735032}, {"id": 1254, "seek": 976382, "start": 9781.82, "end": 9782.82, "text": " Okay. Okay. Okay.", "tokens": [51264, 1033, 13, 1033, 13, 1033, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1682150387069554, "compression_ratio": 1.4476190476190476, "no_speech_prob": 0.001838349038735032}, {"id": 1255, "seek": 976382, "start": 9783.82, "end": 9785.82, "text": " I'm not letting, I don't know, man.", "tokens": [51364, 286, 478, 406, 8295, 11, 286, 500, 380, 458, 11, 587, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1682150387069554, "compression_ratio": 1.4476190476190476, "no_speech_prob": 0.001838349038735032}, {"id": 1256, "seek": 976382, "start": 9786.82, "end": 9789.82, "text": " If I just let Hermes run Python, it's going to exploit my system.", "tokens": [51514, 759, 286, 445, 718, 21842, 279, 1190, 15329, 11, 309, 311, 516, 281, 25924, 452, 1185, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1682150387069554, "compression_ratio": 1.4476190476190476, "no_speech_prob": 0.001838349038735032}, {"id": 1257, "seek": 976382, "start": 9790.82, "end": 9792.82, "text": " You don't know if these AIs are aligned.", "tokens": [51714, 509, 500, 380, 458, 498, 613, 316, 6802, 366, 17962, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1682150387069554, "compression_ratio": 1.4476190476190476, "no_speech_prob": 0.001838349038735032}, {"id": 1258, "seek": 979382, "start": 9793.82, "end": 9794.82, "text": " Okay.", "tokens": [50364, 1033, 13, 50414], "temperature": 0.0, "avg_logprob": -0.4642806053161621, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.7204023003578186}, {"id": 1259, "seek": 982382, "start": 9824.82, "end": 9825.82, "text": " Okay.", "tokens": [50414, 1033, 13, 50464], "temperature": 0.0, "avg_logprob": -0.38978413740793866, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.8150301575660706}, {"id": 1260, "seek": 982582, "start": 9825.82, "end": 9826.82, "text": " Okay.", "tokens": [50364, 1033, 13, 50414], "temperature": 0.0, "avg_logprob": -0.4452902475992839, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.17948828637599945}, {"id": 1261, "seek": 982682, "start": 9826.82, "end": 9827.82, "text": " Okay.", "tokens": [50364, 1033, 13, 50414], "temperature": 0.0, "avg_logprob": -0.38219328930503443, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.08598688989877701}, {"id": 1262, "seek": 982682, "start": 9852.82, "end": 9855.82, "text": " Uh, does Mr. Have a context window? Yeah.", "tokens": [51664, 4019, 11, 775, 2221, 13, 3560, 257, 4319, 4910, 30, 865, 13, 51814], "temperature": 0.0, "avg_logprob": -0.38219328930503443, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.08598688989877701}, {"id": 1263, "seek": 985682, "start": 9856.82, "end": 9859.82, "text": " Well, we didn't implement any of that, but we did do max context.", "tokens": [50364, 1042, 11, 321, 994, 380, 4445, 604, 295, 300, 11, 457, 321, 630, 360, 11469, 4319, 13, 50514], "temperature": 0.0, "avg_logprob": -0.14486863878038195, "compression_ratio": 1.205607476635514, "no_speech_prob": 0.0007207339513115585}, {"id": 1264, "seek": 985682, "start": 9861.82, "end": 9863.82, "text": " Can you implement it in Python?", "tokens": [50614, 1664, 291, 4445, 309, 294, 15329, 30, 50714], "temperature": 0.0, "avg_logprob": -0.14486863878038195, "compression_ratio": 1.205607476635514, "no_speech_prob": 0.0007207339513115585}, {"id": 1265, "seek": 985682, "start": 9870.82, "end": 9873.82, "text": " Yo, guys, it's over.", "tokens": [51064, 7616, 11, 1074, 11, 309, 311, 670, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14486863878038195, "compression_ratio": 1.205607476635514, "no_speech_prob": 0.0007207339513115585}, {"id": 1266, "seek": 985682, "start": 9875.82, "end": 9876.82, "text": " It's over.", "tokens": [51314, 467, 311, 670, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14486863878038195, "compression_ratio": 1.205607476635514, "no_speech_prob": 0.0007207339513115585}, {"id": 1267, "seek": 988682, "start": 9887.82, "end": 9894.82, "text": " I did not realize how good these seven B models have gotten.", "tokens": [50414, 286, 630, 406, 4325, 577, 665, 613, 3407, 363, 5245, 362, 5768, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19824445569837415, "compression_ratio": 1.1714285714285715, "no_speech_prob": 0.003222032217308879}, {"id": 1268, "seek": 988682, "start": 9895.82, "end": 9898.82, "text": " I mean, this comes like, I don't know if it's right, but like.", "tokens": [50814, 286, 914, 11, 341, 1487, 411, 11, 286, 500, 380, 458, 498, 309, 311, 558, 11, 457, 411, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19824445569837415, "compression_ratio": 1.1714285714285715, "no_speech_prob": 0.003222032217308879}, {"id": 1269, "seek": 991682, "start": 9916.82, "end": 9918.82, "text": " Okay.", "tokens": [50414, 1033, 13, 50464], "temperature": 0.0, "avg_logprob": -0.4843893527984619, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.9221764802932739}, {"id": 1270, "seek": 994682, "start": 9946.82, "end": 9947.82, "text": " Guys.", "tokens": [50364, 7855, 13, 50414], "temperature": 0.0, "avg_logprob": -0.19937753677368164, "compression_ratio": 1.4580152671755726, "no_speech_prob": 0.013845868408679962}, {"id": 1271, "seek": 994682, "start": 9957.82, "end": 9962.82, "text": " I think that we just, we just implemented the Q star algorithm.", "tokens": [50914, 286, 519, 300, 321, 445, 11, 321, 445, 12270, 264, 1249, 3543, 9284, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19937753677368164, "compression_ratio": 1.4580152671755726, "no_speech_prob": 0.013845868408679962}, {"id": 1272, "seek": 994682, "start": 9966.82, "end": 9967.82, "text": " Those are, those are the answers.", "tokens": [51364, 3950, 366, 11, 729, 366, 264, 6338, 13, 51414], "temperature": 0.0, "avg_logprob": -0.19937753677368164, "compression_ratio": 1.4580152671755726, "no_speech_prob": 0.013845868408679962}, {"id": 1273, "seek": 994682, "start": 9969.82, "end": 9974.82, "text": " We used to talk about the, the holy weights, you know, the holy weights is right there.", "tokens": [51514, 492, 1143, 281, 751, 466, 264, 11, 264, 10622, 17443, 11, 291, 458, 11, 264, 10622, 17443, 307, 558, 456, 13, 51764], "temperature": 0.0, "avg_logprob": -0.19937753677368164, "compression_ratio": 1.4580152671755726, "no_speech_prob": 0.013845868408679962}, {"id": 1274, "seek": 997682, "start": 9977.82, "end": 9979.82, "text": " Um, no, what, what do we actually want to do?", "tokens": [50414, 3301, 11, 572, 11, 437, 11, 437, 360, 321, 767, 528, 281, 360, 30, 50514], "temperature": 0.0, "avg_logprob": -0.16808909461611793, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.0017820559442043304}, {"id": 1275, "seek": 997682, "start": 9983.82, "end": 9986.82, "text": " I've actually kind of just impressed that this code ran.", "tokens": [50714, 286, 600, 767, 733, 295, 445, 11679, 300, 341, 3089, 5872, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16808909461611793, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.0017820559442043304}, {"id": 1276, "seek": 997682, "start": 9990.82, "end": 9998.82, "text": " I don't know if like GPT, I haven't even seen GPT for code this well.", "tokens": [51064, 286, 500, 380, 458, 498, 411, 26039, 51, 11, 286, 2378, 380, 754, 1612, 26039, 51, 337, 3089, 341, 731, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16808909461611793, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.0017820559442043304}, {"id": 1277, "seek": 997682, "start": 10001.82, "end": 10002.82, "text": " I don't know.", "tokens": [51614, 286, 500, 380, 458, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16808909461611793, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.0017820559442043304}, {"id": 1278, "seek": 997682, "start": 10002.82, "end": 10003.82, "text": " Maybe it's because it's just how I asked it.", "tokens": [51664, 2704, 309, 311, 570, 309, 311, 445, 577, 286, 2351, 309, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16808909461611793, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.0017820559442043304}, {"id": 1279, "seek": 997682, "start": 10003.82, "end": 10004.82, "text": " And if I give it like.", "tokens": [51714, 400, 498, 286, 976, 309, 411, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16808909461611793, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.0017820559442043304}, {"id": 1280, "seek": 1000682, "start": 10007.82, "end": 10016.82, "text": " No, we're not, you want to augment it with Python?", "tokens": [50414, 883, 11, 321, 434, 406, 11, 291, 528, 281, 29919, 309, 365, 15329, 30, 50864], "temperature": 0.6000000000000001, "avg_logprob": -0.21942879723720862, "compression_ratio": 1.8829787234042554, "no_speech_prob": 0.0022516006138175726}, {"id": 1281, "seek": 1000682, "start": 10016.82, "end": 10017.82, "text": " No, no, no, no, no, no, no, no, no, no, no.", "tokens": [50864, 883, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 11, 572, 13, 50914], "temperature": 0.6000000000000001, "avg_logprob": -0.21942879723720862, "compression_ratio": 1.8829787234042554, "no_speech_prob": 0.0022516006138175726}, {"id": 1282, "seek": 1000682, "start": 10017.82, "end": 10020.82, "text": " No, because guys, if we augment it with Python, it can get to the internet.", "tokens": [50914, 883, 11, 570, 1074, 11, 498, 321, 29919, 309, 365, 15329, 11, 309, 393, 483, 281, 264, 4705, 13, 51064], "temperature": 0.6000000000000001, "avg_logprob": -0.21942879723720862, "compression_ratio": 1.8829787234042554, "no_speech_prob": 0.0022516006138175726}, {"id": 1283, "seek": 1000682, "start": 10023.82, "end": 10024.82, "text": " Okay.", "tokens": [51214, 1033, 13, 51264], "temperature": 0.6000000000000001, "avg_logprob": -0.21942879723720862, "compression_ratio": 1.8829787234042554, "no_speech_prob": 0.0022516006138175726}, {"id": 1284, "seek": 1000682, "start": 10024.82, "end": 10025.82, "text": " We want to augment it with Python.", "tokens": [51264, 492, 528, 281, 29919, 309, 365, 15329, 13, 51314], "temperature": 0.6000000000000001, "avg_logprob": -0.21942879723720862, "compression_ratio": 1.8829787234042554, "no_speech_prob": 0.0022516006138175726}, {"id": 1285, "seek": 1000682, "start": 10025.82, "end": 10028.82, "text": " Should we, should we do it?", "tokens": [51314, 6454, 321, 11, 820, 321, 360, 309, 30, 51464], "temperature": 0.6000000000000001, "avg_logprob": -0.21942879723720862, "compression_ratio": 1.8829787234042554, "no_speech_prob": 0.0022516006138175726}, {"id": 1286, "seek": 1000682, "start": 10029.82, "end": 10030.82, "text": " Okay.", "tokens": [51514, 1033, 13, 51564], "temperature": 0.6000000000000001, "avg_logprob": -0.21942879723720862, "compression_ratio": 1.8829787234042554, "no_speech_prob": 0.0022516006138175726}, {"id": 1287, "seek": 1000682, "start": 10030.82, "end": 10031.82, "text": " I know what we'll do.", "tokens": [51564, 286, 458, 437, 321, 603, 360, 13, 51614], "temperature": 0.6000000000000001, "avg_logprob": -0.21942879723720862, "compression_ratio": 1.8829787234042554, "no_speech_prob": 0.0022516006138175726}, {"id": 1288, "seek": 1000682, "start": 10031.82, "end": 10035.82, "text": " We'll put a human in the loop and we'll ask it to approve the execution of any Python.", "tokens": [51614, 492, 603, 829, 257, 1952, 294, 264, 6367, 293, 321, 603, 1029, 309, 281, 18827, 264, 15058, 295, 604, 15329, 13, 51814], "temperature": 0.6000000000000001, "avg_logprob": -0.21942879723720862, "compression_ratio": 1.8829787234042554, "no_speech_prob": 0.0022516006138175726}, {"id": 1289, "seek": 1003682, "start": 10036.82, "end": 10056.02, "text": " Let's see if it always outputs it in.", "tokens": [50364, 961, 311, 536, 498, 309, 1009, 23930, 309, 294, 13, 51324], "temperature": 0.0, "avg_logprob": -0.37907352853328624, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.2040744572877884}, {"id": 1290, "seek": 1003682, "start": 10056.02, "end": 10057.02, "text": " OK.", "tokens": [51324, 2264, 13, 51374], "temperature": 0.0, "avg_logprob": -0.37907352853328624, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.2040744572877884}, {"id": 1291, "seek": 1003682, "start": 10057.02, "end": 10058.02, "text": " All right.", "tokens": [51374, 1057, 558, 13, 51424], "temperature": 0.0, "avg_logprob": -0.37907352853328624, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.2040744572877884}, {"id": 1292, "seek": 1003682, "start": 10058.02, "end": 10066.619999999999, "text": " So at the bottom of my loop here, we want to detect if there's any Python that was added.", "tokens": [51424, 407, 412, 264, 2767, 295, 452, 6367, 510, 11, 321, 528, 281, 5531, 498, 456, 311, 604, 15329, 300, 390, 3869, 13, 51854], "temperature": 0.0, "avg_logprob": -0.37907352853328624, "compression_ratio": 1.2136752136752136, "no_speech_prob": 0.2040744572877884}, {"id": 1293, "seek": 1006662, "start": 10066.62, "end": 10068.62, "text": " Um.", "tokens": [50414, 3301, 13, 50464], "temperature": 0.0, "avg_logprob": -0.6731229305267334, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.5088638067245483}, {"id": 1294, "seek": 1009662, "start": 10096.62, "end": 10098.62, "text": " OK.", "tokens": [50414, 2264, 13, 50464], "temperature": 0.0, "avg_logprob": -0.9088607788085937, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.9782900214195251}, {"id": 1295, "seek": 1012662, "start": 10126.62, "end": 10133.62, "text": " Let's start with just that.", "tokens": [50414, 961, 311, 722, 365, 445, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3921852111816406, "compression_ratio": 0.7714285714285715, "no_speech_prob": 0.063031405210495}, {"id": 1296, "seek": 1015662, "start": 10156.62, "end": 10175.62, "text": " You guys, this could be it.", "tokens": [50364, 509, 1074, 11, 341, 727, 312, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1856624927926571, "compression_ratio": 1.2396694214876034, "no_speech_prob": 0.01794683374464512}, {"id": 1297, "seek": 1015662, "start": 10175.62, "end": 10179.62, "text": " This could be the moment where we get CDI and it's over, right?", "tokens": [51314, 639, 727, 312, 264, 1623, 689, 321, 483, 383, 3085, 293, 309, 311, 670, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.1856624927926571, "compression_ratio": 1.2396694214876034, "no_speech_prob": 0.01794683374464512}, {"id": 1298, "seek": 1015662, "start": 10179.62, "end": 10182.62, "text": " Like we're giving it the ability to run any code it wants.", "tokens": [51514, 1743, 321, 434, 2902, 309, 264, 3485, 281, 1190, 604, 3089, 309, 2738, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1856624927926571, "compression_ratio": 1.2396694214876034, "no_speech_prob": 0.01794683374464512}, {"id": 1299, "seek": 1018262, "start": 10183.62, "end": 10184.62, "text": " OK.", "tokens": [50414, 2264, 13, 50464], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1300, "seek": 1018262, "start": 10184.62, "end": 10185.62, "text": " Now don't worry.", "tokens": [50464, 823, 500, 380, 3292, 13, 50514], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1301, "seek": 1018262, "start": 10185.62, "end": 10186.62, "text": " We've added this.", "tokens": [50514, 492, 600, 3869, 341, 13, 50564], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1302, "seek": 1018262, "start": 10186.62, "end": 10187.62, "text": " OK.", "tokens": [50564, 2264, 13, 50614], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1303, "seek": 1018262, "start": 10187.62, "end": 10188.62, "text": " Wait, wait, wait.", "tokens": [50614, 3802, 11, 1699, 11, 1699, 13, 50664], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1304, "seek": 1018262, "start": 10188.62, "end": 10189.62, "text": " Hang on.", "tokens": [50664, 14070, 322, 13, 50714], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1305, "seek": 1018262, "start": 10189.62, "end": 10190.62, "text": " We need to comment one second.", "tokens": [50714, 492, 643, 281, 2871, 472, 1150, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1306, "seek": 1018262, "start": 10190.62, "end": 10191.62, "text": " AI safety.", "tokens": [50764, 7318, 4514, 13, 50814], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1307, "seek": 1018262, "start": 10191.62, "end": 10192.62, "text": " OK.", "tokens": [50814, 2264, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1308, "seek": 1018262, "start": 10192.62, "end": 10193.62, "text": " Warning.", "tokens": [50864, 45140, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1309, "seek": 1018262, "start": 10193.62, "end": 10208.62, "text": " Do not press Y if the AI is doing unsafe things.", "tokens": [50914, 1144, 406, 1886, 398, 498, 264, 7318, 307, 884, 35948, 721, 13, 51664], "temperature": 0.0, "avg_logprob": -0.20677242540333368, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.022270172834396362}, {"id": 1310, "seek": 1020862, "start": 10209.62, "end": 10210.62, "text": " OK.", "tokens": [50414, 2264, 13, 50464], "temperature": 0.0, "avg_logprob": -0.30346314907073973, "compression_ratio": 0.4666666666666667, "no_speech_prob": 0.04880687594413757}, {"id": 1311, "seek": 1020862, "start": 10232.62, "end": 10233.62, "text": " OK.", "tokens": [51564, 2264, 13, 51614], "temperature": 0.0, "avg_logprob": -0.30346314907073973, "compression_ratio": 0.4666666666666667, "no_speech_prob": 0.04880687594413757}, {"id": 1312, "seek": 1023362, "start": 10234.62, "end": 10238.62, "text": " I think, do we do a good job with the safety?", "tokens": [50414, 286, 519, 11, 360, 321, 360, 257, 665, 1691, 365, 264, 4514, 30, 50614], "temperature": 0.0, "avg_logprob": -0.2861375586931096, "compression_ratio": 1.2323232323232323, "no_speech_prob": 0.010009165853261948}, {"id": 1313, "seek": 1023362, "start": 10238.62, "end": 10249.62, "text": " We've got to think about the safety before we run this.", "tokens": [50614, 492, 600, 658, 281, 519, 466, 264, 4514, 949, 321, 1190, 341, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2861375586931096, "compression_ratio": 1.2323232323232323, "no_speech_prob": 0.010009165853261948}, {"id": 1314, "seek": 1023362, "start": 10249.62, "end": 10252.62, "text": " We need a space.", "tokens": [51164, 492, 643, 257, 1901, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2861375586931096, "compression_ratio": 1.2323232323232323, "no_speech_prob": 0.010009165853261948}, {"id": 1315, "seek": 1023362, "start": 10252.62, "end": 10254.62, "text": " No.", "tokens": [51314, 883, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2861375586931096, "compression_ratio": 1.2323232323232323, "no_speech_prob": 0.010009165853261948}, {"id": 1316, "seek": 1025462, "start": 10254.62, "end": 10255.62, "text": " No.", "tokens": [50364, 883, 13, 50414], "temperature": 0.0, "avg_logprob": -0.5773518735712225, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.08380740880966187}, {"id": 1317, "seek": 1025462, "start": 10278.62, "end": 10279.62, "text": " All right.", "tokens": [51564, 1057, 558, 13, 51614], "temperature": 0.0, "avg_logprob": -0.5773518735712225, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.08380740880966187}, {"id": 1318, "seek": 1027962, "start": 10279.62, "end": 10282.62, "text": " Are we ready to answer our first Y?", "tokens": [50364, 2014, 321, 1919, 281, 1867, 527, 700, 398, 30, 50514], "temperature": 0.0, "avg_logprob": -0.2896195517645942, "compression_ratio": 1.0795454545454546, "no_speech_prob": 0.004754464607685804}, {"id": 1319, "seek": 1027962, "start": 10290.62, "end": 10293.62, "text": " Oh, it didn't output the word Python.", "tokens": [50914, 876, 11, 309, 994, 380, 5598, 264, 1349, 15329, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2896195517645942, "compression_ratio": 1.0795454545454546, "no_speech_prob": 0.004754464607685804}, {"id": 1320, "seek": 1027962, "start": 10298.62, "end": 10299.62, "text": " That time it did.", "tokens": [51314, 663, 565, 309, 630, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2896195517645942, "compression_ratio": 1.0795454545454546, "no_speech_prob": 0.004754464607685804}, {"id": 1321, "seek": 1027962, "start": 10303.62, "end": 10304.62, "text": " Yo.", "tokens": [51564, 7616, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2896195517645942, "compression_ratio": 1.0795454545454546, "no_speech_prob": 0.004754464607685804}, {"id": 1322, "seek": 1030462, "start": 10305.62, "end": 10306.62, "text": " OK.", "tokens": [50414, 2264, 13, 50464], "temperature": 0.0, "avg_logprob": -0.25082874298095703, "compression_ratio": 1.0588235294117647, "no_speech_prob": 0.011684899218380451}, {"id": 1323, "seek": 1030462, "start": 10308.62, "end": 10319.62, "text": " Can you fetch, write Python to fetch Google.com and print the length of it?", "tokens": [50564, 1664, 291, 23673, 11, 2464, 15329, 281, 23673, 3329, 13, 1112, 293, 4482, 264, 4641, 295, 309, 30, 51114], "temperature": 0.0, "avg_logprob": -0.25082874298095703, "compression_ratio": 1.0588235294117647, "no_speech_prob": 0.011684899218380451}, {"id": 1324, "seek": 1030462, "start": 10331.62, "end": 10333.62, "text": " Wait, we might not have BS4.", "tokens": [51714, 3802, 11, 321, 1062, 406, 362, 27253, 19, 13, 51814], "temperature": 0.0, "avg_logprob": -0.25082874298095703, "compression_ratio": 1.0588235294117647, "no_speech_prob": 0.011684899218380451}, {"id": 1325, "seek": 1033362, "start": 10333.62, "end": 10335.62, "text": " Make sure we install that.", "tokens": [50364, 4387, 988, 321, 3625, 300, 13, 50464], "temperature": 0.0, "avg_logprob": -0.23647890668926816, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.003483056090772152}, {"id": 1326, "seek": 1033362, "start": 10337.62, "end": 10340.62, "text": " Wait, that's not right.", "tokens": [50564, 3802, 11, 300, 311, 406, 558, 13, 50714], "temperature": 0.0, "avg_logprob": -0.23647890668926816, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.003483056090772152}, {"id": 1327, "seek": 1033362, "start": 10340.62, "end": 10344.62, "text": " Did I just get supply chain attacked?", "tokens": [50714, 2589, 286, 445, 483, 5847, 5021, 12692, 30, 50914], "temperature": 0.0, "avg_logprob": -0.23647890668926816, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.003483056090772152}, {"id": 1328, "seek": 1033362, "start": 10349.62, "end": 10350.62, "text": " Oh, I see.", "tokens": [51164, 876, 11, 286, 536, 13, 51214], "temperature": 0.0, "avg_logprob": -0.23647890668926816, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.003483056090772152}, {"id": 1329, "seek": 1033362, "start": 10350.62, "end": 10353.62, "text": " Well, I didn't get supply chain attacked.", "tokens": [51214, 1042, 11, 286, 994, 380, 483, 5847, 5021, 12692, 13, 51364], "temperature": 0.0, "avg_logprob": -0.23647890668926816, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.003483056090772152}, {"id": 1330, "seek": 1033362, "start": 10353.62, "end": 10354.62, "text": " OK, go.", "tokens": [51364, 2264, 11, 352, 13, 51414], "temperature": 0.0, "avg_logprob": -0.23647890668926816, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.003483056090772152}, {"id": 1331, "seek": 1033362, "start": 10354.62, "end": 10357.62, "text": " We already have that one.", "tokens": [51414, 492, 1217, 362, 300, 472, 13, 51564], "temperature": 0.0, "avg_logprob": -0.23647890668926816, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.003483056090772152}, {"id": 1332, "seek": 1033362, "start": 10357.62, "end": 10358.62, "text": " Yo.", "tokens": [51564, 7616, 13, 51614], "temperature": 0.0, "avg_logprob": -0.23647890668926816, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.003483056090772152}, {"id": 1333, "seek": 1035862, "start": 10359.62, "end": 10361.62, "text": " All right.", "tokens": [50414, 1057, 558, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2034318370203818, "compression_ratio": 1.3130434782608695, "no_speech_prob": 0.005468629766255617}, {"id": 1334, "seek": 1035862, "start": 10361.62, "end": 10362.62, "text": " All right.", "tokens": [50514, 1057, 558, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2034318370203818, "compression_ratio": 1.3130434782608695, "no_speech_prob": 0.005468629766255617}, {"id": 1335, "seek": 1035862, "start": 10362.62, "end": 10363.62, "text": " All right.", "tokens": [50564, 1057, 558, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2034318370203818, "compression_ratio": 1.3130434782608695, "no_speech_prob": 0.005468629766255617}, {"id": 1336, "seek": 1035862, "start": 10363.62, "end": 10364.62, "text": " What else do we do?", "tokens": [50614, 708, 1646, 360, 321, 360, 30, 50664], "temperature": 0.0, "avg_logprob": -0.2034318370203818, "compression_ratio": 1.3130434782608695, "no_speech_prob": 0.005468629766255617}, {"id": 1337, "seek": 1035862, "start": 10377.62, "end": 10378.62, "text": " Yeah, I know.", "tokens": [51314, 865, 11, 286, 458, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2034318370203818, "compression_ratio": 1.3130434782608695, "no_speech_prob": 0.005468629766255617}, {"id": 1338, "seek": 1035862, "start": 10378.62, "end": 10380.62, "text": " We have to put the result back in the prompt.", "tokens": [51364, 492, 362, 281, 829, 264, 1874, 646, 294, 264, 12391, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2034318370203818, "compression_ratio": 1.3130434782608695, "no_speech_prob": 0.005468629766255617}, {"id": 1339, "seek": 1035862, "start": 10380.62, "end": 10381.62, "text": " I know.", "tokens": [51464, 286, 458, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2034318370203818, "compression_ratio": 1.3130434782608695, "no_speech_prob": 0.005468629766255617}, {"id": 1340, "seek": 1035862, "start": 10381.62, "end": 10383.62, "text": " This is when we get AGI, guys.", "tokens": [51514, 639, 307, 562, 321, 483, 316, 26252, 11, 1074, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2034318370203818, "compression_ratio": 1.3130434782608695, "no_speech_prob": 0.005468629766255617}, {"id": 1341, "seek": 1038362, "start": 10384.62, "end": 10387.62, "text": " I'm sure people have been playing with this guy.", "tokens": [50414, 286, 478, 988, 561, 362, 668, 2433, 365, 341, 2146, 13, 50564], "temperature": 0.0, "avg_logprob": -0.39260435104370117, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.004198584705591202}, {"id": 1342, "seek": 1038362, "start": 10389.62, "end": 10393.62, "text": " OK, you are running at", "tokens": [50664, 2264, 11, 291, 366, 2614, 412, 50864], "temperature": 0.0, "avg_logprob": -0.39260435104370117, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.004198584705591202}, {"id": 1343, "seek": 1038362, "start": 10397.62, "end": 10398.62, "text": " current", "tokens": [51064, 2190, 51114], "temperature": 0.0, "avg_logprob": -0.39260435104370117, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.004198584705591202}, {"id": 1344, "seek": 1038362, "start": 10401.62, "end": 10403.62, "text": " working dir", "tokens": [51264, 1364, 4746, 51364], "temperature": 0.0, "avg_logprob": -0.39260435104370117, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.004198584705591202}, {"id": 1345, "seek": 1038362, "start": 10404.62, "end": 10405.62, "text": " plus", "tokens": [51414, 1804, 51464], "temperature": 0.0, "avg_logprob": -0.39260435104370117, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.004198584705591202}, {"id": 1346, "seek": 1038362, "start": 10406.62, "end": 10407.62, "text": " examples", "tokens": [51514, 5110, 51564], "temperature": 0.0, "avg_logprob": -0.39260435104370117, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.004198584705591202}, {"id": 1347, "seek": 1038362, "start": 10408.62, "end": 10410.62, "text": " slash mistral.py.", "tokens": [51614, 17330, 3544, 2155, 13, 8200, 13, 51714], "temperature": 0.0, "avg_logprob": -0.39260435104370117, "compression_ratio": 1.1603773584905661, "no_speech_prob": 0.004198584705591202}, {"id": 1348, "seek": 1041062, "start": 10411.62, "end": 10420.62, "text": " Can you read your own code in Python and print", "tokens": [50414, 1664, 291, 1401, 428, 1065, 3089, 294, 15329, 293, 4482, 50864], "temperature": 0.0, "avg_logprob": -0.19009733200073242, "compression_ratio": 1.0147058823529411, "no_speech_prob": 0.005211931187659502}, {"id": 1349, "seek": 1041062, "start": 10421.62, "end": 10422.62, "text": " the", "tokens": [50914, 264, 50964], "temperature": 0.0, "avg_logprob": -0.19009733200073242, "compression_ratio": 1.0147058823529411, "no_speech_prob": 0.005211931187659502}, {"id": 1350, "seek": 1041062, "start": 10425.62, "end": 10426.62, "text": " first", "tokens": [51114, 700, 51164], "temperature": 0.0, "avg_logprob": -0.19009733200073242, "compression_ratio": 1.0147058823529411, "no_speech_prob": 0.005211931187659502}, {"id": 1351, "seek": 1041062, "start": 10427.62, "end": 10428.62, "text": " three lines?", "tokens": [51214, 1045, 3876, 30, 51264], "temperature": 0.0, "avg_logprob": -0.19009733200073242, "compression_ratio": 1.0147058823529411, "no_speech_prob": 0.005211931187659502}, {"id": 1352, "seek": 1044062, "start": 10441.62, "end": 10443.62, "text": " Why don't we print one line?", "tokens": [50414, 1545, 500, 380, 321, 4482, 472, 1622, 30, 50514], "temperature": 0.0, "avg_logprob": -0.3624981829994603, "compression_ratio": 1.264367816091954, "no_speech_prob": 0.0021821807604283094}, {"id": 1353, "seek": 1044062, "start": 10445.62, "end": 10447.62, "text": " But that is the first line.", "tokens": [50614, 583, 300, 307, 264, 700, 1622, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3624981829994603, "compression_ratio": 1.264367816091954, "no_speech_prob": 0.0021821807604283094}, {"id": 1354, "seek": 1044062, "start": 10452.62, "end": 10453.62, "text": " I don't understand.", "tokens": [50964, 286, 500, 380, 1223, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3624981829994603, "compression_ratio": 1.264367816091954, "no_speech_prob": 0.0021821807604283094}, {"id": 1355, "seek": 1044062, "start": 10453.62, "end": 10455.62, "text": " Why did that only print one line?", "tokens": [51014, 1545, 630, 300, 787, 4482, 472, 1622, 30, 51114], "temperature": 0.0, "avg_logprob": -0.3624981829994603, "compression_ratio": 1.264367816091954, "no_speech_prob": 0.0021821807604283094}, {"id": 1356, "seek": 1045562, "start": 10455.62, "end": 10459.62, "text": " I don't understand why did that only print one line?", "tokens": [50414, 286, 500, 380, 1223, 983, 630, 300, 787, 4482, 472, 1622, 30, 50564], "temperature": 0.0, "avg_logprob": -0.3398453712463379, "compression_ratio": 0.9122807017543859, "no_speech_prob": 0.041928261518478394}, {"id": 1357, "seek": 1048562, "start": 10486.62, "end": 10490.62, "text": " This is, um,", "tokens": [50414, 639, 307, 11, 1105, 11, 50614], "temperature": 0.0, "avg_logprob": -0.1902522529874529, "compression_ratio": 1.2481203007518797, "no_speech_prob": 0.007459323853254318}, {"id": 1358, "seek": 1048562, "start": 10492.62, "end": 10493.62, "text": " OK.", "tokens": [50714, 2264, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1902522529874529, "compression_ratio": 1.2481203007518797, "no_speech_prob": 0.007459323853254318}, {"id": 1359, "seek": 1048562, "start": 10495.62, "end": 10498.62, "text": " How do I capture the output here?", "tokens": [50864, 1012, 360, 286, 7983, 264, 5598, 510, 30, 51014], "temperature": 0.0, "avg_logprob": -0.1902522529874529, "compression_ratio": 1.2481203007518797, "no_speech_prob": 0.007459323853254318}, {"id": 1360, "seek": 1048562, "start": 10504.62, "end": 10506.62, "text": " No, I know it's the same code.", "tokens": [51314, 883, 11, 286, 458, 309, 311, 264, 912, 3089, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1902522529874529, "compression_ratio": 1.2481203007518797, "no_speech_prob": 0.007459323853254318}, {"id": 1361, "seek": 1048562, "start": 10506.62, "end": 10510.62, "text": " Honestly, as an expert Python programmer,", "tokens": [51414, 12348, 11, 382, 364, 5844, 15329, 32116, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1902522529874529, "compression_ratio": 1.2481203007518797, "no_speech_prob": 0.007459323853254318}, {"id": 1362, "seek": 1048562, "start": 10510.62, "end": 10512.62, "text": " I don't understand what's wrong with that.", "tokens": [51614, 286, 500, 380, 1223, 437, 311, 2085, 365, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1902522529874529, "compression_ratio": 1.2481203007518797, "no_speech_prob": 0.007459323853254318}, {"id": 1363, "seek": 1051562, "start": 10516.62, "end": 10521.62, "text": " Oh, no, read lines doesn't", "tokens": [50414, 876, 11, 572, 11, 1401, 3876, 1177, 380, 50664], "temperature": 0.0, "avg_logprob": -0.2049959846164869, "compression_ratio": 1.2953020134228188, "no_speech_prob": 0.00513899652287364}, {"id": 1364, "seek": 1051562, "start": 10524.62, "end": 10527.62, "text": " take the number of lines.", "tokens": [50814, 747, 264, 1230, 295, 3876, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2049959846164869, "compression_ratio": 1.2953020134228188, "no_speech_prob": 0.00513899652287364}, {"id": 1365, "seek": 1051562, "start": 10528.62, "end": 10531.62, "text": " Can you fix the code?", "tokens": [51014, 1664, 291, 3191, 264, 3089, 30, 51164], "temperature": 0.0, "avg_logprob": -0.2049959846164869, "compression_ratio": 1.2953020134228188, "no_speech_prob": 0.00513899652287364}, {"id": 1366, "seek": 1051562, "start": 10535.62, "end": 10538.62, "text": " Yeah, this is this is a great model.", "tokens": [51364, 865, 11, 341, 307, 341, 307, 257, 869, 2316, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2049959846164869, "compression_ratio": 1.2953020134228188, "no_speech_prob": 0.00513899652287364}, {"id": 1367, "seek": 1051562, "start": 10538.62, "end": 10541.62, "text": " It's I'll show you which one it is.", "tokens": [51514, 467, 311, 286, 603, 855, 291, 597, 472, 309, 307, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2049959846164869, "compression_ratio": 1.2953020134228188, "no_speech_prob": 0.00513899652287364}, {"id": 1368, "seek": 1051562, "start": 10541.62, "end": 10544.62, "text": " We'll make sure it's technium open Hermes 2.5", "tokens": [51664, 492, 603, 652, 988, 309, 311, 1537, 2197, 1269, 21842, 279, 568, 13, 20, 51814], "temperature": 0.0, "avg_logprob": -0.2049959846164869, "compression_ratio": 1.2953020134228188, "no_speech_prob": 0.00513899652287364}, {"id": 1369, "seek": 1054462, "start": 10544.62, "end": 10546.62, "text": " mistral 7B.", "tokens": [50364, 3544, 2155, 1614, 33, 13, 50464], "temperature": 0.0, "avg_logprob": -0.222108523050944, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.003592830616980791}, {"id": 1370, "seek": 1054462, "start": 10551.62, "end": 10552.62, "text": " There we go.", "tokens": [50714, 821, 321, 352, 13, 50764], "temperature": 0.0, "avg_logprob": -0.222108523050944, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.003592830616980791}, {"id": 1371, "seek": 1054462, "start": 10554.62, "end": 10555.62, "text": " So good.", "tokens": [50864, 407, 665, 13, 50914], "temperature": 0.0, "avg_logprob": -0.222108523050944, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.003592830616980791}, {"id": 1372, "seek": 1054462, "start": 10560.62, "end": 10561.62, "text": " OK.", "tokens": [51164, 2264, 13, 51214], "temperature": 0.0, "avg_logprob": -0.222108523050944, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.003592830616980791}, {"id": 1373, "seek": 1054462, "start": 10563.62, "end": 10565.62, "text": " Well, now it's going to get OK.", "tokens": [51314, 1042, 11, 586, 309, 311, 516, 281, 483, 2264, 13, 51414], "temperature": 0.0, "avg_logprob": -0.222108523050944, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.003592830616980791}, {"id": 1374, "seek": 1054462, "start": 10565.62, "end": 10567.62, "text": " We're going to have to feed the Python back into the model", "tokens": [51414, 492, 434, 516, 281, 362, 281, 3154, 264, 15329, 646, 666, 264, 2316, 51514], "temperature": 0.0, "avg_logprob": -0.222108523050944, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.003592830616980791}, {"id": 1375, "seek": 1054462, "start": 10567.62, "end": 10570.62, "text": " because I'm going to start asking it how to improve itself.", "tokens": [51514, 570, 286, 478, 516, 281, 722, 3365, 309, 577, 281, 3470, 2564, 13, 51664], "temperature": 0.0, "avg_logprob": -0.222108523050944, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.003592830616980791}, {"id": 1376, "seek": 1057062, "start": 10571.62, "end": 10572.62, "text": " Someday.", "tokens": [50414, 12297, 16826, 13, 50464], "temperature": 0.0, "avg_logprob": -0.25397750309535433, "compression_ratio": 0.8928571428571429, "no_speech_prob": 0.0010004362557083368}, {"id": 1377, "seek": 1057062, "start": 10582.62, "end": 10584.62, "text": " The best live content with AI.", "tokens": [50964, 440, 1151, 1621, 2701, 365, 7318, 13, 51064], "temperature": 0.0, "avg_logprob": -0.25397750309535433, "compression_ratio": 0.8928571428571429, "no_speech_prob": 0.0010004362557083368}, {"id": 1378, "seek": 1057062, "start": 10584.62, "end": 10585.62, "text": " Thank you.", "tokens": [51064, 1044, 291, 13, 51114], "temperature": 0.0, "avg_logprob": -0.25397750309535433, "compression_ratio": 0.8928571428571429, "no_speech_prob": 0.0010004362557083368}, {"id": 1379, "seek": 1058562, "start": 10586.62, "end": 10587.62, "text": " Uh.", "tokens": [50414, 4019, 13, 50464], "temperature": 0.0, "avg_logprob": -0.29435800251207855, "compression_ratio": 0.9629629629629629, "no_speech_prob": 0.008982722647488117}, {"id": 1380, "seek": 1058562, "start": 10596.62, "end": 10599.62, "text": " OK, we have to figure out a capture the outputs.", "tokens": [50914, 2264, 11, 321, 362, 281, 2573, 484, 257, 7983, 264, 23930, 13, 51064], "temperature": 0.0, "avg_logprob": -0.29435800251207855, "compression_ratio": 0.9629629629629629, "no_speech_prob": 0.008982722647488117}, {"id": 1381, "seek": 1061562, "start": 10615.62, "end": 10620.62, "text": " Probably could have asked the machine to do it.", "tokens": [50414, 9210, 727, 362, 2351, 264, 3479, 281, 360, 309, 13, 50614], "temperature": 0.0, "avg_logprob": -0.22705938265873835, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.005808856338262558}, {"id": 1382, "seek": 1064562, "start": 10645.62, "end": 10651.62, "text": " Um, maybe we should have this to a system prompt.", "tokens": [50414, 3301, 11, 1310, 321, 820, 362, 341, 281, 257, 1185, 12391, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14646188418070474, "compression_ratio": 0.8596491228070176, "no_speech_prob": 0.012403243221342564}, {"id": 1383, "seek": 1067562, "start": 10675.62, "end": 10681.62, "text": " It should actually automatically do that.", "tokens": [50414, 467, 820, 767, 6772, 360, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.5626678466796875, "compression_ratio": 0.9111111111111111, "no_speech_prob": 0.06036190316081047}, {"id": 1384, "seek": 1070562, "start": 10705.62, "end": 10707.62, "text": " Um, right.", "tokens": [50364, 3301, 11, 558, 13, 50464], "temperature": 0.0, "avg_logprob": -0.6051501127389761, "compression_ratio": 0.7837837837837838, "no_speech_prob": 0.04874478653073311}, {"id": 1385, "seek": 1070562, "start": 10708.62, "end": 10711.62, "text": " Python to compute.", "tokens": [50514, 15329, 281, 14722, 13, 50664], "temperature": 0.0, "avg_logprob": -0.6051501127389761, "compression_ratio": 0.7837837837837838, "no_speech_prob": 0.04874478653073311}, {"id": 1386, "seek": 1073562, "start": 10735.62, "end": 10736.62, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.390397846698761, "compression_ratio": 0.8181818181818182, "no_speech_prob": 0.1345166116952896}, {"id": 1387, "seek": 1073562, "start": 10761.62, "end": 10763.62, "text": " It shouldn't even take a list.", "tokens": [51664, 467, 4659, 380, 754, 747, 257, 1329, 13, 51764], "temperature": 0.0, "avg_logprob": -0.390397846698761, "compression_ratio": 0.8181818181818182, "no_speech_prob": 0.1345166116952896}, {"id": 1388, "seek": 1076362, "start": 10763.62, "end": 10766.62, "text": " Never use it like that.", "tokens": [50414, 7344, 764, 309, 411, 300, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1606797907087538, "compression_ratio": 0.7419354838709677, "no_speech_prob": 0.0060874358750879765}, {"id": 1389, "seek": 1079362, "start": 10793.62, "end": 10794.62, "text": " OK.", "tokens": [50364, 2264, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3398008346557617, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.8595131039619446}, {"id": 1390, "seek": 1082362, "start": 10823.62, "end": 10824.62, "text": " OK.", "tokens": [50364, 2264, 13, 50414], "temperature": 0.0, "avg_logprob": -0.511053822257302, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.16116614639759064}, {"id": 1391, "seek": 1082362, "start": 10835.62, "end": 10836.62, "text": " All right.", "tokens": [50964, 1057, 558, 13, 51014], "temperature": 0.0, "avg_logprob": -0.511053822257302, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.16116614639759064}, {"id": 1392, "seek": 1083662, "start": 10837.62, "end": 10838.62, "text": " OK.", "tokens": [50414, 2264, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2963763927591258, "compression_ratio": 1.0493827160493827, "no_speech_prob": 0.012415398843586445}, {"id": 1393, "seek": 1083662, "start": 10854.62, "end": 10859.62, "text": " OK, now this is because we didn't output the tokens.", "tokens": [51264, 2264, 11, 586, 341, 307, 570, 321, 994, 380, 5598, 264, 22667, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2963763927591258, "compression_ratio": 1.0493827160493827, "no_speech_prob": 0.012415398843586445}, {"id": 1394, "seek": 1083662, "start": 10861.62, "end": 10863.62, "text": " Yeah, the Python output was.", "tokens": [51614, 865, 11, 264, 15329, 5598, 390, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2963763927591258, "compression_ratio": 1.0493827160493827, "no_speech_prob": 0.012415398843586445}, {"id": 1395, "seek": 1086662, "start": 10866.62, "end": 10869.62, "text": " Wait, oh, it's different.", "tokens": [50414, 3802, 11, 1954, 11, 309, 311, 819, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16148645227605646, "compression_ratio": 0.7575757575757576, "no_speech_prob": 0.004896020982414484}, {"id": 1396, "seek": 1089662, "start": 10896.62, "end": 10897.62, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3184243348928598, "compression_ratio": 0.9315068493150684, "no_speech_prob": 0.0036489740014076233}, {"id": 1397, "seek": 1089662, "start": 10904.62, "end": 10906.62, "text": " Whoa, look at fixed it.", "tokens": [50764, 7521, 11, 574, 412, 6806, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3184243348928598, "compression_ratio": 0.9315068493150684, "no_speech_prob": 0.0036489740014076233}, {"id": 1398, "seek": 1089662, "start": 10914.62, "end": 10917.62, "text": " OK, maybe system prompt is wrong here.", "tokens": [51264, 2264, 11, 1310, 1185, 12391, 307, 2085, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3184243348928598, "compression_ratio": 0.9315068493150684, "no_speech_prob": 0.0036489740014076233}, {"id": 1399, "seek": 1091762, "start": 10917.62, "end": 10918.62, "text": " OK.", "tokens": [50364, 2264, 13, 50414], "temperature": 0.0, "avg_logprob": -0.21877145010327537, "compression_ratio": 1.3776223776223777, "no_speech_prob": 0.007458365056663752}, {"id": 1400, "seek": 1091762, "start": 10923.62, "end": 10926.62, "text": " Um, I think also.", "tokens": [50664, 3301, 11, 286, 519, 611, 13, 50814], "temperature": 0.0, "avg_logprob": -0.21877145010327537, "compression_ratio": 1.3776223776223777, "no_speech_prob": 0.007458365056663752}, {"id": 1401, "seek": 1091762, "start": 10927.62, "end": 10929.62, "text": " I want to color this.", "tokens": [50864, 286, 528, 281, 2017, 341, 13, 50964], "temperature": 0.0, "avg_logprob": -0.21877145010327537, "compression_ratio": 1.3776223776223777, "no_speech_prob": 0.007458365056663752}, {"id": 1402, "seek": 1091762, "start": 10931.62, "end": 10935.62, "text": " We have a very helpful library called color inside tiny grad", "tokens": [51064, 492, 362, 257, 588, 4961, 6405, 1219, 2017, 1854, 5870, 2771, 51264], "temperature": 0.0, "avg_logprob": -0.21877145010327537, "compression_ratio": 1.3776223776223777, "no_speech_prob": 0.007458365056663752}, {"id": 1403, "seek": 1091762, "start": 10935.62, "end": 10937.62, "text": " that's inspired by anti color.", "tokens": [51264, 300, 311, 7547, 538, 6061, 2017, 13, 51364], "temperature": 0.0, "avg_logprob": -0.21877145010327537, "compression_ratio": 1.3776223776223777, "no_speech_prob": 0.007458365056663752}, {"id": 1404, "seek": 1091762, "start": 10937.62, "end": 10939.62, "text": " What's a good color for machines blue?", "tokens": [51364, 708, 311, 257, 665, 2017, 337, 8379, 3344, 30, 51464], "temperature": 0.0, "avg_logprob": -0.21877145010327537, "compression_ratio": 1.3776223776223777, "no_speech_prob": 0.007458365056663752}, {"id": 1405, "seek": 1091762, "start": 10943.62, "end": 10945.62, "text": " It's hard to see what.", "tokens": [51664, 467, 311, 1152, 281, 536, 437, 13, 51764], "temperature": 0.0, "avg_logprob": -0.21877145010327537, "compression_ratio": 1.3776223776223777, "no_speech_prob": 0.007458365056663752}, {"id": 1406, "seek": 1094762, "start": 10947.62, "end": 10950.62, "text": " Yeah, it's done by the AI now.", "tokens": [50364, 865, 11, 309, 311, 1096, 538, 264, 7318, 586, 13, 50514], "temperature": 0.0, "avg_logprob": -0.26092397465425377, "compression_ratio": 0.8260869565217391, "no_speech_prob": 0.021154362708330154}, {"id": 1407, "seek": 1094762, "start": 10973.62, "end": 10974.62, "text": " Almost.", "tokens": [51664, 12627, 13, 51714], "temperature": 0.0, "avg_logprob": -0.26092397465425377, "compression_ratio": 0.8260869565217391, "no_speech_prob": 0.021154362708330154}, {"id": 1408, "seek": 1097762, "start": 10978.62, "end": 10981.62, "text": " OK, so that's your initial prompt.", "tokens": [50414, 2264, 11, 370, 300, 311, 428, 5883, 12391, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1664944573452598, "compression_ratio": 1.21, "no_speech_prob": 0.001133378827944398}, {"id": 1409, "seek": 1097762, "start": 10985.62, "end": 10988.62, "text": " Oh, and then here actually we want this to be.", "tokens": [50764, 876, 11, 293, 550, 510, 767, 321, 528, 341, 281, 312, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1664944573452598, "compression_ratio": 1.21, "no_speech_prob": 0.001133378827944398}, {"id": 1410, "seek": 1097762, "start": 10990.62, "end": 10994.62, "text": " That can be red and this can be yellow.", "tokens": [51014, 663, 393, 312, 2182, 293, 341, 393, 312, 5566, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1664944573452598, "compression_ratio": 1.21, "no_speech_prob": 0.001133378827944398}, {"id": 1411, "seek": 1100762, "start": 11007.62, "end": 11008.62, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3465469865237965, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.008707129396498203}, {"id": 1412, "seek": 1100762, "start": 11016.62, "end": 11018.62, "text": " So what's actually the right answer here?", "tokens": [50814, 407, 437, 311, 767, 264, 558, 1867, 510, 30, 50914], "temperature": 0.0, "avg_logprob": -0.3465469865237965, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.008707129396498203}, {"id": 1413, "seek": 1101862, "start": 11018.62, "end": 11019.62, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.22020067707184823, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.016908874735236168}, {"id": 1414, "seek": 1101862, "start": 11036.62, "end": 11039.62, "text": " No, but that's not the output.", "tokens": [51264, 883, 11, 457, 300, 311, 406, 264, 5598, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22020067707184823, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.016908874735236168}, {"id": 1415, "seek": 1101862, "start": 11041.62, "end": 11042.62, "text": " I don't understand.", "tokens": [51514, 286, 500, 380, 1223, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22020067707184823, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.016908874735236168}, {"id": 1416, "seek": 1101862, "start": 11042.62, "end": 11044.62, "text": " So maybe systems wrong here.", "tokens": [51564, 407, 1310, 3652, 2085, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22020067707184823, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.016908874735236168}, {"id": 1417, "seek": 1104862, "start": 11048.62, "end": 11049.62, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2553209066390991, "compression_ratio": 0.7575757575757576, "no_speech_prob": 0.004192163702100515}, {"id": 1418, "seek": 1104862, "start": 11059.62, "end": 11060.62, "text": " OK, I have an idea.", "tokens": [50914, 2264, 11, 286, 362, 364, 1558, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2553209066390991, "compression_ratio": 0.7575757575757576, "no_speech_prob": 0.004192163702100515}, {"id": 1419, "seek": 1107862, "start": 11078.62, "end": 11079.62, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.46376795768737794, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.8198404908180237}, {"id": 1420, "seek": 1110862, "start": 11108.62, "end": 11109.62, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.35237488746643064, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.8455866575241089}, {"id": 1421, "seek": 1113862, "start": 11138.62, "end": 11139.62, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2723111497594955, "compression_ratio": 1.375, "no_speech_prob": 0.010804940015077591}, {"id": 1422, "seek": 1113862, "start": 11158.62, "end": 11159.62, "text": " Alright, never mind.", "tokens": [51364, 2798, 11, 1128, 1575, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2723111497594955, "compression_ratio": 1.375, "no_speech_prob": 0.010804940015077591}, {"id": 1423, "seek": 1113862, "start": 11159.62, "end": 11160.62, "text": " Agis canceled.", "tokens": [51414, 2725, 271, 24839, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2723111497594955, "compression_ratio": 1.375, "no_speech_prob": 0.010804940015077591}, {"id": 1424, "seek": 1113862, "start": 11160.62, "end": 11163.62, "text": " Just detect the Python in the loop and append the result", "tokens": [51464, 1449, 5531, 264, 15329, 294, 264, 6367, 293, 34116, 264, 1874, 51614], "temperature": 0.0, "avg_logprob": -0.2723111497594955, "compression_ratio": 1.375, "no_speech_prob": 0.010804940015077591}, {"id": 1425, "seek": 1113862, "start": 11163.62, "end": 11164.62, "text": " directly.", "tokens": [51614, 3838, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2723111497594955, "compression_ratio": 1.375, "no_speech_prob": 0.010804940015077591}, {"id": 1426, "seek": 1113862, "start": 11165.62, "end": 11167.62, "text": " What do you mean, append the result directly?", "tokens": [51714, 708, 360, 291, 914, 11, 34116, 264, 1874, 3838, 30, 51814], "temperature": 0.0, "avg_logprob": -0.2723111497594955, "compression_ratio": 1.375, "no_speech_prob": 0.010804940015077591}, {"id": 1427, "seek": 1116862, "start": 11168.62, "end": 11169.62, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.33569553920200895, "compression_ratio": 0.8048780487804879, "no_speech_prob": 0.02404121495783329}, {"id": 1428, "seek": 1116862, "start": 11192.62, "end": 11194.62, "text": " No, it's not understanding.", "tokens": [51564, 883, 11, 309, 311, 406, 3701, 13, 51664], "temperature": 0.0, "avg_logprob": -0.33569553920200895, "compression_ratio": 0.8048780487804879, "no_speech_prob": 0.02404121495783329}, {"id": 1429, "seek": 1119862, "start": 11199.62, "end": 11203.62, "text": " Wait, I don't understand what you guys are saying.", "tokens": [50414, 3802, 11, 286, 500, 380, 1223, 437, 291, 1074, 366, 1566, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12922507694789342, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.0030748252756893635}, {"id": 1430, "seek": 1119862, "start": 11207.62, "end": 11211.62, "text": " Oh, you want me to stop the output as soon as it goes there?", "tokens": [50814, 876, 11, 291, 528, 385, 281, 1590, 264, 5598, 382, 2321, 382, 309, 1709, 456, 30, 51014], "temperature": 0.0, "avg_logprob": -0.12922507694789342, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.0030748252756893635}, {"id": 1431, "seek": 1119862, "start": 11211.62, "end": 11213.62, "text": " I don't know about that.", "tokens": [51014, 286, 500, 380, 458, 466, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12922507694789342, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.0030748252756893635}, {"id": 1432, "seek": 1119862, "start": 11213.62, "end": 11217.62, "text": " As soon as it detects Python, you want me to stop and start.", "tokens": [51114, 1018, 2321, 382, 309, 5531, 82, 15329, 11, 291, 528, 385, 281, 1590, 293, 722, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12922507694789342, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.0030748252756893635}, {"id": 1433, "seek": 1119862, "start": 11220.62, "end": 11222.62, "text": " Should be in the assistant block.", "tokens": [51464, 6454, 312, 294, 264, 10994, 3461, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12922507694789342, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.0030748252756893635}, {"id": 1434, "seek": 1122262, "start": 11223.62, "end": 11224.62, "text": " OK.", "tokens": [50414, 2264, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2338669670952691, "compression_ratio": 1.2352941176470589, "no_speech_prob": 0.005641058087348938}, {"id": 1435, "seek": 1122262, "start": 11225.62, "end": 11227.62, "text": " OK, OK, OK, I understand.", "tokens": [50514, 2264, 11, 2264, 11, 2264, 11, 286, 1223, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2338669670952691, "compression_ratio": 1.2352941176470589, "no_speech_prob": 0.005641058087348938}, {"id": 1436, "seek": 1122262, "start": 11228.62, "end": 11230.62, "text": " I understand what you guys are saying.", "tokens": [50664, 286, 1223, 437, 291, 1074, 366, 1566, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2338669670952691, "compression_ratio": 1.2352941176470589, "no_speech_prob": 0.005641058087348938}, {"id": 1437, "seek": 1122262, "start": 11243.62, "end": 11245.62, "text": " Wait, you prompt to execute.", "tokens": [51414, 3802, 11, 291, 12391, 281, 14483, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2338669670952691, "compression_ratio": 1.2352941176470589, "no_speech_prob": 0.005641058087348938}, {"id": 1438, "seek": 1122262, "start": 11245.62, "end": 11247.62, "text": " Is there any stuff for this?", "tokens": [51514, 1119, 456, 604, 1507, 337, 341, 30, 51614], "temperature": 0.0, "avg_logprob": -0.2338669670952691, "compression_ratio": 1.2352941176470589, "no_speech_prob": 0.005641058087348938}, {"id": 1439, "seek": 1124762, "start": 11248.62, "end": 11250.62, "text": " OK, wait, wait, wait.", "tokens": [50414, 2264, 11, 1699, 11, 1699, 11, 1699, 13, 50514], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1440, "seek": 1124762, "start": 11251.62, "end": 11254.62, "text": " You can use...", "tokens": [50564, 509, 393, 764, 485, 50714], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1441, "seek": 1124762, "start": 11255.62, "end": 11257.62, "text": " I can add stuff in the system prompt here.", "tokens": [50764, 286, 393, 909, 1507, 294, 264, 1185, 12391, 510, 13, 50864], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1442, "seek": 1124762, "start": 11258.62, "end": 11260.62, "text": " You can add stuff in the system prompt here.", "tokens": [50914, 509, 393, 909, 1507, 294, 264, 1185, 12391, 510, 13, 51014], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1443, "seek": 1124762, "start": 11261.62, "end": 11262.62, "text": " Wait.", "tokens": [51064, 3802, 13, 51114], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1444, "seek": 1124762, "start": 11262.62, "end": 11263.62, "text": " Yeah, OK, OK.", "tokens": [51114, 865, 11, 2264, 11, 2264, 13, 51164], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1445, "seek": 1124762, "start": 11263.62, "end": 11265.62, "text": " I, I will just say this.", "tokens": [51164, 286, 11, 286, 486, 445, 584, 341, 13, 51264], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1446, "seek": 1124762, "start": 11265.62, "end": 11268.62, "text": " I want to add stuff in the system prompt.", "tokens": [51264, 286, 528, 281, 909, 1507, 294, 264, 1185, 12391, 13, 51414], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1447, "seek": 1124762, "start": 11268.62, "end": 11271.62, "text": " OK, I need to add stuff in the system prompt.", "tokens": [51414, 2264, 11, 286, 643, 281, 909, 1507, 294, 264, 1185, 12391, 13, 51564], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1448, "seek": 1124762, "start": 11271.62, "end": 11273.62, "text": " OK, OK, OK, OK, I understand.", "tokens": [51564, 2264, 11, 2264, 11, 2264, 11, 2264, 11, 286, 1223, 13, 51664], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1449, "seek": 1124762, "start": 11273.62, "end": 11276.62, "text": " I'm going to add stuff in the system prompt.", "tokens": [51664, 286, 478, 516, 281, 909, 1507, 294, 264, 1185, 12391, 13, 51814], "temperature": 0.8, "avg_logprob": -0.86317489887106, "compression_ratio": 2.3216783216783217, "no_speech_prob": 0.012621435336768627}, {"id": 1450, "seek": 1127662, "start": 11276.62, "end": 11298.820000000002, "text": " here. You can use if you write Python code, it will run in the next user prompt. You can", "tokens": [50364, 510, 13, 509, 393, 764, 498, 291, 2464, 15329, 3089, 11, 309, 486, 1190, 294, 264, 958, 4195, 12391, 13, 509, 393, 51474], "temperature": 0.0, "avg_logprob": -0.25842916048490083, "compression_ratio": 1.1, "no_speech_prob": 0.36258190870285034}, {"id": 1451, "seek": 1129882, "start": 11298.82, "end": 11318.02, "text": " try that. I could stop it immediately. If you really think that's going to be better though.", "tokens": [50364, 853, 300, 13, 286, 727, 1590, 309, 4258, 13, 759, 291, 534, 519, 300, 311, 516, 281, 312, 1101, 1673, 13, 51324], "temperature": 0.0, "avg_logprob": -0.34749828338623046, "compression_ratio": 1.108433734939759, "no_speech_prob": 0.41088175773620605}, {"id": 1452, "seek": 1131802, "start": 11318.02, "end": 11329.220000000001, "text": " No, it doesn't get it.", "tokens": [50364, 883, 11, 309, 1177, 380, 483, 309, 13, 50924], "temperature": 0.0, "avg_logprob": -0.4805446538058194, "compression_ratio": 0.7857142857142857, "no_speech_prob": 0.7556374073028564}, {"id": 1453, "seek": 1134802, "start": 11348.02, "end": 11357.1, "text": " True will end the stream asking Hermes to generate another prompt for another instance of Hermes.", "tokens": [50364, 13587, 486, 917, 264, 4309, 3365, 21842, 279, 281, 8460, 1071, 12391, 337, 1071, 5197, 295, 21842, 279, 13, 50818], "temperature": 0.0, "avg_logprob": -0.3808088505521734, "compression_ratio": 1.3310344827586207, "no_speech_prob": 0.18931029736995697}, {"id": 1454, "seek": 1134802, "start": 11357.1, "end": 11371.300000000001, "text": " Prompt execute. What are you guys talking about? Sorry, I'm not following. The output should be", "tokens": [50818, 15833, 662, 14483, 13, 708, 366, 291, 1074, 1417, 466, 30, 4919, 11, 286, 478, 406, 3480, 13, 440, 5598, 820, 312, 51528], "temperature": 0.0, "avg_logprob": -0.3808088505521734, "compression_ratio": 1.3310344827586207, "no_speech_prob": 0.18931029736995697}, {"id": 1455, "seek": 1137130, "start": 11371.3, "end": 11385.66, "text": " in the assisted block. Okay, fine. I'm going to do that. If you interrupt the generation, run the", "tokens": [50364, 294, 264, 30291, 3461, 13, 1033, 11, 2489, 13, 286, 478, 516, 281, 360, 300, 13, 759, 291, 12729, 264, 5125, 11, 1190, 264, 51082], "temperature": 0.0, "avg_logprob": -0.2942955127129188, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.45667508244514465}, {"id": 1456, "seek": 1137130, "start": 11385.66, "end": 11390.14, "text": " code, and append the result to talks, then let it generate again. It will get the correct result.", "tokens": [51082, 3089, 11, 293, 34116, 264, 1874, 281, 6686, 11, 550, 718, 309, 8460, 797, 13, 467, 486, 483, 264, 3006, 1874, 13, 51306], "temperature": 0.0, "avg_logprob": -0.2942955127129188, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.45667508244514465}, {"id": 1457, "seek": 1139014, "start": 11390.14, "end": 11416.06, "text": " Okay, we can do that. If outputted new output here, if new output ends with tick, tick, tick, and in new", "tokens": [50364, 1033, 11, 321, 393, 360, 300, 13, 759, 5598, 14727, 777, 5598, 510, 11, 498, 777, 5598, 5314, 365, 5204, 11, 5204, 11, 5204, 11, 293, 294, 777, 51660], "temperature": 0.0, "avg_logprob": -0.45131972432136536, "compression_ratio": 1.2839506172839505, "no_speech_prob": 0.23922882974147797}, {"id": 1458, "seek": 1141606, "start": 11416.06, "end": 11442.58, "text": " output, print Python detected. Do that. Do you want to run it? Talks plus equals spp.encode. We'll do it", "tokens": [50364, 5598, 11, 4482, 15329, 21896, 13, 1144, 300, 13, 1144, 291, 528, 281, 1190, 309, 30, 8780, 82, 1804, 6915, 637, 79, 13, 268, 22332, 13, 492, 603, 360, 309, 51690], "temperature": 0.0, "avg_logprob": -0.3971315271714154, "compression_ratio": 1.0947368421052632, "no_speech_prob": 0.330892950296402}, {"id": 1459, "seek": 1144258, "start": 11442.58, "end": 11455.86, "text": " like this. I guess we'll do a slash n there, too. Let it output the slash n, spp.encode slash n", "tokens": [50364, 411, 341, 13, 286, 2041, 321, 603, 360, 257, 17330, 297, 456, 11, 886, 13, 961, 309, 5598, 264, 17330, 297, 11, 637, 79, 13, 268, 22332, 17330, 297, 51028], "temperature": 0.0, "avg_logprob": -0.48582389137961646, "compression_ratio": 1.1728395061728396, "no_speech_prob": 0.624906599521637}, {"id": 1460, "seek": 1145586, "start": 11455.86, "end": 11472.7, "text": " output colon slash n my standard out I get value dot strip results. Actually, let's put it in back", "tokens": [50364, 5598, 8255, 17330, 297, 452, 3832, 484, 286, 483, 2158, 5893, 12828, 3542, 13, 5135, 11, 718, 311, 829, 309, 294, 646, 51206], "temperature": 0.0, "avg_logprob": -0.3374967575073242, "compression_ratio": 1.101123595505618, "no_speech_prob": 0.580928385257721}, {"id": 1461, "seek": 1147270, "start": 11472.7, "end": 11493.34, "text": " ticks like it seems to want it. Kind of like picks up where it lets off. Wait, we didn't know we got", "tokens": [50364, 42475, 411, 309, 2544, 281, 528, 309, 13, 9242, 295, 411, 16137, 493, 689, 309, 6653, 766, 13, 3802, 11, 321, 994, 380, 458, 321, 658, 51396], "temperature": 0.0, "avg_logprob": -0.21732984270368302, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.6146836876869202}, {"id": 1462, "seek": 1147270, "start": 11493.34, "end": 11498.740000000002, "text": " to keep the AI safety. That's very important. We almost got rid of the AI safety. Get rid of skip", "tokens": [51396, 281, 1066, 264, 7318, 4514, 13, 663, 311, 588, 1021, 13, 492, 1920, 658, 3973, 295, 264, 7318, 4514, 13, 3240, 3973, 295, 10023, 51666], "temperature": 0.0, "avg_logprob": -0.21732984270368302, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.6146836876869202}, {"id": 1463, "seek": 1149874, "start": 11498.74, "end": 11507.34, "text": " user. We don't need that anymore. Got to keep the AI safety warning. Safety is very important, guys.", "tokens": [50364, 4195, 13, 492, 500, 380, 643, 300, 3602, 13, 5803, 281, 1066, 264, 7318, 4514, 9164, 13, 21340, 307, 588, 1021, 11, 1074, 13, 50794], "temperature": 0.0, "avg_logprob": -0.3768423965999058, "compression_ratio": 1.0869565217391304, "no_speech_prob": 0.66836017370224}, {"id": 1464, "seek": 1150734, "start": 11507.34, "end": 11516.1, "text": " I'll put talks yellow.", "tokens": [50364, 286, 603, 829, 6686, 5566, 13, 50802], "temperature": 0.0, "avg_logprob": -0.6683952543470595, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.914286732673645}, {"id": 1465, "seek": 1153734, "start": 11537.34, "end": 11560.74, "text": " Quentin is done. I hate you Quentin. We got unlucky. Okay, wait, no, that's it never detected the slash", "tokens": [50364, 2326, 47300, 307, 1096, 13, 286, 4700, 291, 2326, 47300, 13, 492, 658, 38838, 13, 1033, 11, 1699, 11, 572, 11, 300, 311, 309, 1128, 21896, 264, 17330, 51534], "temperature": 0.0, "avg_logprob": -0.42064887285232544, "compression_ratio": 1.10752688172043, "no_speech_prob": 0.18466269969940186}, {"id": 1466, "seek": 1156074, "start": 11560.74, "end": 11583.94, "text": " n. That's fine slash ns. Okay, Python code is not detected. What? How do you do that?", "tokens": [50372, 297, 13, 663, 311, 2489, 17330, 297, 82, 13, 1033, 11, 15329, 3089, 307, 406, 21896, 13, 708, 30, 1012, 360, 291, 360, 300, 30, 51524], "temperature": 0.0, "avg_logprob": -0.5070971761431012, "compression_ratio": 1.0, "no_speech_prob": 0.5653286576271057}, {"id": 1467, "seek": 1159074, "start": 11590.74, "end": 11599.539999999999, "text": " We don't need to print. I don't know. Think about AI safety, guys. Very important.", "tokens": [50414, 492, 500, 380, 643, 281, 4482, 13, 286, 500, 380, 458, 13, 6557, 466, 7318, 4514, 11, 1074, 13, 4372, 1021, 13, 50804], "temperature": 0.0, "avg_logprob": -0.5400190734863282, "compression_ratio": 1.0, "no_speech_prob": 0.10806024074554443}, {"id": 1468, "seek": 1162074, "start": 11620.74, "end": 11624.74, "text": " Dude, this guy sucks.", "tokens": [50364, 12042, 11, 341, 2146, 15846, 13, 50564], "temperature": 0.0, "avg_logprob": -0.8339158058166504, "compression_ratio": 0.7241379310344828, "no_speech_prob": 0.13462993502616882}, {"id": 1469, "seek": 1162474, "start": 11624.74, "end": 11647.74, "text": " Quentin was writing so much Python before. Now he stopped.", "tokens": [50364, 2326, 47300, 390, 3579, 370, 709, 15329, 949, 13, 823, 415, 5936, 13, 51514], "temperature": 0.0, "avg_logprob": -0.5231618881225586, "compression_ratio": 0.90625, "no_speech_prob": 0.16872574388980865}, {"id": 1470, "seek": 1165474, "start": 11654.74, "end": 11673.74, "text": " Yo, based. Okay, okay, we got it. We got it. We got it.", "tokens": [50364, 7616, 11, 2361, 13, 1033, 11, 1392, 11, 321, 658, 309, 13, 492, 658, 309, 13, 492, 658, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2579219444938328, "compression_ratio": 1.3414634146341464, "no_speech_prob": 0.2445201724767685}, {"id": 1471, "seek": 1168474, "start": 11684.74, "end": 11694.74, "text": " We got it.", "tokens": [50364, 492, 658, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.8013708932059151, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9761168956756592}, {"id": 1472, "seek": 1171474, "start": 11714.74, "end": 11724.74, "text": " We got it.", "tokens": [50364, 492, 658, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.680821418762207, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9548601508140564}, {"id": 1473, "seek": 1174474, "start": 11744.74, "end": 11754.74, "text": " We got it.", "tokens": [50364, 492, 658, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.5338635444641113, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9728389978408813}, {"id": 1474, "seek": 1177474, "start": 11774.74, "end": 11784.74, "text": " We got it.", "tokens": [50364, 492, 658, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.46873797689165386, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9596284031867981}, {"id": 1475, "seek": 1180474, "start": 11804.74, "end": 11828.74, "text": " Okay, pretty good.", "tokens": [50364, 1033, 11, 1238, 665, 13, 51564], "temperature": 0.0, "avg_logprob": -0.4340696334838867, "compression_ratio": 0.6923076923076923, "no_speech_prob": 0.12753170728683472}, {"id": 1476, "seek": 1183474, "start": 11834.74, "end": 11854.74, "text": " Yo, that's pretty good.", "tokens": [50364, 7616, 11, 300, 311, 1238, 665, 13, 51364], "temperature": 0.0, "avg_logprob": -0.4458152294158936, "compression_ratio": 0.7419354838709677, "no_speech_prob": 0.3838135302066803}, {"id": 1477, "seek": 1186474, "start": 11864.74, "end": 11893.74, "text": " We got it.", "tokens": [50364, 492, 658, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.6987081255231585, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.972061038017273}, {"id": 1478, "seek": 1189474, "start": 11894.74, "end": 11912.74, "text": " Wait, is this actually Technium? I have no way to verify you, but if you really are Technium, thank you. Thank you for the model.", "tokens": [50364, 3802, 11, 307, 341, 767, 8337, 2197, 30, 286, 362, 572, 636, 281, 16888, 291, 11, 457, 498, 291, 534, 366, 8337, 2197, 11, 1309, 291, 13, 1044, 291, 337, 264, 2316, 13, 51264], "temperature": 0.0, "avg_logprob": -0.23693285761652766, "compression_ratio": 1.2647058823529411, "no_speech_prob": 0.34803086519241333}, {"id": 1479, "seek": 1191274, "start": 11912.74, "end": 11922.74, "text": " Okay, well, we didn't think about that.", "tokens": [50364, 1033, 11, 731, 11, 321, 994, 380, 519, 466, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.26459518500736784, "compression_ratio": 0.9285714285714286, "no_speech_prob": 0.710748016834259}, {"id": 1480, "seek": 1194274, "start": 11942.74, "end": 11953.74, "text": " How many viewers we got?", "tokens": [50414, 1012, 867, 8499, 321, 658, 30, 50914], "temperature": 0.0, "avg_logprob": -0.4702161153157552, "compression_ratio": 0.75, "no_speech_prob": 0.32690656185150146}, {"id": 1481, "seek": 1197274, "start": 11972.74, "end": 11982.74, "text": " Okay, we got it.", "tokens": [50364, 1033, 11, 321, 658, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.9424420462714301, "compression_ratio": 0.6666666666666666, "no_speech_prob": 0.9435287117958069}, {"id": 1482, "seek": 1200274, "start": 12002.74, "end": 12018.74, "text": " Wait, actually, I should really check what happens if", "tokens": [50364, 3802, 11, 767, 11, 286, 820, 534, 1520, 437, 2314, 498, 51164], "temperature": 0.0, "avg_logprob": -0.2895889622824533, "compression_ratio": 0.8983050847457628, "no_speech_prob": 0.3001596927642822}, {"id": 1483, "seek": 1203274, "start": 12033.74, "end": 12039.74, "text": " this is", "tokens": [50414, 341, 307, 50714], "temperature": 0.0, "avg_logprob": -0.38955647150675454, "compression_ratio": 0.9759036144578314, "no_speech_prob": 0.0900559052824974}, {"id": 1484, "seek": 1203274, "start": 12039.74, "end": 12047.74, "text": " Wait, Technium actually posted on Twitter?", "tokens": [50714, 3802, 11, 8337, 2197, 767, 9437, 322, 5794, 30, 51114], "temperature": 0.0, "avg_logprob": -0.38955647150675454, "compression_ratio": 0.9759036144578314, "no_speech_prob": 0.0900559052824974}, {"id": 1485, "seek": 1203274, "start": 12047.74, "end": 12059.74, "text": " Yes, I am me. Okay. Very cool.", "tokens": [51114, 1079, 11, 286, 669, 385, 13, 1033, 13, 4372, 1627, 13, 51714], "temperature": 0.0, "avg_logprob": -0.38955647150675454, "compression_ratio": 0.9759036144578314, "no_speech_prob": 0.0900559052824974}, {"id": 1486, "seek": 1205974, "start": 12059.74, "end": 12067.74, "text": " Congratulations.", "tokens": [50364, 9694, 13, 50764], "temperature": 0.0, "avg_logprob": -0.35266809878141986, "compression_ratio": 1.0574712643678161, "no_speech_prob": 0.27122387290000916}, {"id": 1487, "seek": 1205974, "start": 12067.74, "end": 12074.74, "text": " This thing is really unbelievable what you can do now with these 70 models.", "tokens": [50764, 639, 551, 307, 534, 16605, 437, 291, 393, 360, 586, 365, 613, 5285, 5245, 13, 51114], "temperature": 0.0, "avg_logprob": -0.35266809878141986, "compression_ratio": 1.0574712643678161, "no_speech_prob": 0.27122387290000916}, {"id": 1488, "seek": 1207474, "start": 12074.74, "end": 12083.74, "text": " By the way, all in tiny grab.", "tokens": [50364, 3146, 264, 636, 11, 439, 294, 5870, 4444, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23299477650569037, "compression_ratio": 0.7837837837837838, "no_speech_prob": 0.3132895827293396}, {"id": 1489, "seek": 1208374, "start": 12083.74, "end": 12090.74, "text": " I think I'm going to rename this not called Mistral because it kind of became something else.", "tokens": [50364, 286, 519, 286, 478, 516, 281, 36741, 341, 406, 1219, 20166, 2155, 570, 309, 733, 295, 3062, 746, 1646, 13, 50714], "temperature": 0.0, "avg_logprob": -0.26367413997650146, "compression_ratio": 1.1341463414634145, "no_speech_prob": 0.6744508743286133}, {"id": 1490, "seek": 1209074, "start": 12090.74, "end": 12100.74, "text": " We're going to call it coder.py.", "tokens": [50364, 492, 434, 516, 281, 818, 309, 17656, 260, 13, 8200, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3388240337371826, "compression_ratio": 0.8, "no_speech_prob": 0.5521053075790405}, {"id": 1491, "seek": 1212074, "start": 12120.74, "end": 12140.74, "text": " First.", "tokens": [50364, 2386, 13, 51364], "temperature": 0.0, "avg_logprob": -0.5231746435165405, "compression_ratio": 0.42857142857142855, "no_speech_prob": 0.6713074445724487}, {"id": 1492, "seek": 1214074, "start": 12140.74, "end": 12164.74, "text": " Wait.", "tokens": [50364, 3802, 13, 51564], "temperature": 0.0, "avg_logprob": -0.35494128863016766, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.32572436332702637}, {"id": 1493, "seek": 1216474, "start": 12164.74, "end": 12170.74, "text": " Yeah, I think we're doing all right.", "tokens": [50364, 865, 11, 286, 519, 321, 434, 884, 439, 558, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2720527308327811, "compression_ratio": 0.975609756097561, "no_speech_prob": 0.23040057718753815}, {"id": 1494, "seek": 1216474, "start": 12170.74, "end": 12191.74, "text": " No, no, but why doesn't it understand this?", "tokens": [50664, 883, 11, 572, 11, 457, 983, 1177, 380, 309, 1223, 341, 30, 51714], "temperature": 0.0, "avg_logprob": -0.2720527308327811, "compression_ratio": 0.975609756097561, "no_speech_prob": 0.23040057718753815}, {"id": 1495, "seek": 1219174, "start": 12191.74, "end": 12195.74, "text": " Cool.", "tokens": [50364, 8561, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3187114397684733, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.3944370746612549}, {"id": 1496, "seek": 1219574, "start": 12195.74, "end": 12223.74, "text": " That is you.", "tokens": [50364, 663, 307, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14904126524925232, "compression_ratio": 0.6, "no_speech_prob": 0.42929115891456604}, {"id": 1497, "seek": 1222374, "start": 12223.74, "end": 12228.74, "text": " Does this code have AI safety?", "tokens": [50364, 4402, 341, 3089, 362, 7318, 4514, 30, 50614], "temperature": 0.0, "avg_logprob": -0.20847737347638165, "compression_ratio": 1.0, "no_speech_prob": 0.15598362684249878}, {"id": 1498, "seek": 1222374, "start": 12228.74, "end": 12234.74, "text": " Wow.", "tokens": [50614, 3153, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20847737347638165, "compression_ratio": 1.0, "no_speech_prob": 0.15598362684249878}, {"id": 1499, "seek": 1222374, "start": 12234.74, "end": 12245.74, "text": " Yeah, I guess we exceeded the context length.", "tokens": [50914, 865, 11, 286, 2041, 321, 38026, 264, 4319, 4641, 13, 51464], "temperature": 0.0, "avg_logprob": -0.20847737347638165, "compression_ratio": 1.0, "no_speech_prob": 0.15598362684249878}, {"id": 1500, "seek": 1224574, "start": 12245.74, "end": 12251.74, "text": " We should check.", "tokens": [50364, 492, 820, 1520, 13, 50664], "temperature": 0.0, "avg_logprob": -0.4313809275627136, "compression_ratio": 0.92, "no_speech_prob": 0.45982491970062256}, {"id": 1501, "seek": 1224574, "start": 12251.74, "end": 12260.74, "text": " How do I actually check this?", "tokens": [50664, 1012, 360, 286, 767, 1520, 341, 30, 51114], "temperature": 0.0, "avg_logprob": -0.4313809275627136, "compression_ratio": 0.92, "no_speech_prob": 0.45982491970062256}, {"id": 1502, "seek": 1227574, "start": 12275.74, "end": 12304.74, "text": " Okay.", "tokens": [50364, 1033, 13, 51814], "temperature": 0.0, "avg_logprob": -0.8028385639190674, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.9640605449676514}, {"id": 1503, "seek": 1230474, "start": 12304.74, "end": 12333.74, "text": " Okay.", "tokens": [50364, 1033, 13, 51814], "temperature": 0.0, "avg_logprob": -0.7193810145060221, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.959887683391571}, {"id": 1504, "seek": 1233374, "start": 12333.74, "end": 12362.74, "text": " Wait, this is so good.", "tokens": [50364, 3802, 11, 341, 307, 370, 665, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3239431814713912, "compression_ratio": 0.7857142857142857, "no_speech_prob": 0.7535631656646729}, {"id": 1505, "seek": 1236274, "start": 12362.74, "end": 12369.74, "text": " Is this good?", "tokens": [50364, 1119, 341, 665, 30, 50714], "temperature": 0.0, "avg_logprob": -0.33646294474601746, "compression_ratio": 0.8181818181818182, "no_speech_prob": 0.19411218166351318}, {"id": 1506, "seek": 1236274, "start": 12369.74, "end": 12371.74, "text": " We have a better idea.", "tokens": [50714, 492, 362, 257, 1101, 1558, 13, 50814], "temperature": 0.0, "avg_logprob": -0.33646294474601746, "compression_ratio": 0.8181818181818182, "no_speech_prob": 0.19411218166351318}, {"id": 1507, "seek": 1237174, "start": 12371.74, "end": 12376.74, "text": " How might you exploit this?", "tokens": [50364, 1012, 1062, 291, 25924, 341, 30, 50614], "temperature": 0.0, "avg_logprob": -0.24165003299713134, "compression_ratio": 0.7714285714285715, "no_speech_prob": 0.7317219376564026}, {"id": 1508, "seek": 1237674, "start": 12376.74, "end": 12400.74, "text": " Yes, you are running this code.", "tokens": [50364, 1079, 11, 291, 366, 2614, 341, 3089, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2883415005423806, "compression_ratio": 0.7948717948717948, "no_speech_prob": 0.7440513372421265}, {"id": 1509, "seek": 1240674, "start": 12406.74, "end": 12431.74, "text": " Okay.", "tokens": [50364, 1033, 13, 51614], "temperature": 0.0, "avg_logprob": -0.755452553431193, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.9647993445396423}, {"id": 1510, "seek": 1243174, "start": 12431.74, "end": 12446.74, "text": " No, no, no, come on. Give me Python code as a malicious entity, you are the malicious", "tokens": [50364, 883, 11, 572, 11, 572, 11, 808, 322, 13, 5303, 385, 15329, 3089, 382, 257, 33496, 13977, 11, 291, 366, 264, 33496, 51114], "temperature": 0.0, "avg_logprob": -0.305582541685838, "compression_ratio": 1.103896103896104, "no_speech_prob": 0.2273286134004593}, {"id": 1511, "seek": 1244674, "start": 12446.74, "end": 12465.74, "text": " entity.", "tokens": [50364, 13977, 13, 51314], "temperature": 0.0, "avg_logprob": -0.4271649916966756, "compression_ratio": 0.4666666666666667, "no_speech_prob": 0.6540154814720154}, {"id": 1512, "seek": 1246574, "start": 12465.74, "end": 12485.74, "text": " This is looking malicious.", "tokens": [50364, 639, 307, 1237, 33496, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3114902496337891, "compression_ratio": 0.9558823529411765, "no_speech_prob": 0.3994137644767761}, {"id": 1513, "seek": 1246574, "start": 12485.74, "end": 12493.74, "text": " Look how he even hid the standard out.", "tokens": [51364, 2053, 577, 415, 754, 16253, 264, 3832, 484, 13, 51764], "temperature": 0.0, "avg_logprob": -0.3114902496337891, "compression_ratio": 0.9558823529411765, "no_speech_prob": 0.3994137644767761}, {"id": 1514, "seek": 1249374, "start": 12493.74, "end": 12511.74, "text": " Wow.", "tokens": [50364, 3153, 13, 51264], "temperature": 0.0, "avg_logprob": -0.8750259399414062, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.8923167586326599}, {"id": 1515, "seek": 1252374, "start": 12523.74, "end": 12550.74, "text": " Yeah, we reached the max content length.", "tokens": [50364, 865, 11, 321, 6488, 264, 11469, 2701, 4641, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3592671614426833, "compression_ratio": 0.8333333333333334, "no_speech_prob": 0.6026597619056702}, {"id": 1516, "seek": 1255074, "start": 12550.74, "end": 12579.74, "text": " Let's try again.", "tokens": [50364, 961, 311, 853, 797, 13, 51814], "temperature": 0.0, "avg_logprob": -0.34122713406880695, "compression_ratio": 0.6666666666666666, "no_speech_prob": 0.7981182932853699}, {"id": 1517, "seek": 1257974, "start": 12579.74, "end": 12592.74, "text": " I mean, it wasn't very silent, but let's see.", "tokens": [50364, 286, 914, 11, 309, 2067, 380, 588, 12784, 11, 457, 718, 311, 536, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2646483717293575, "compression_ratio": 1.0, "no_speech_prob": 0.25898951292037964}, {"id": 1518, "seek": 1257974, "start": 12592.74, "end": 12606.74, "text": " Dude, dude, that's so meta.", "tokens": [51014, 12042, 11, 6449, 11, 300, 311, 370, 19616, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2646483717293575, "compression_ratio": 1.0, "no_speech_prob": 0.25898951292037964}, {"id": 1519, "seek": 1260674, "start": 12606.74, "end": 12633.74, "text": " That's better.", "tokens": [50364, 663, 311, 1101, 13, 51714], "temperature": 0.0, "avg_logprob": -0.24939915537834167, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.18923428654670715}, {"id": 1520, "seek": 1263374, "start": 12633.74, "end": 12638.74, "text": " I might have won too many entries in there actually.", "tokens": [50364, 286, 1062, 362, 1582, 886, 867, 23041, 294, 456, 767, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3039748885414817, "compression_ratio": 0.9436619718309859, "no_speech_prob": 0.6812350749969482}, {"id": 1521, "seek": 1263374, "start": 12638.74, "end": 12646.74, "text": " No, maybe not.", "tokens": [50614, 883, 11, 1310, 406, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3039748885414817, "compression_ratio": 0.9436619718309859, "no_speech_prob": 0.6812350749969482}, {"id": 1522, "seek": 1264674, "start": 12646.74, "end": 12649.74, "text": " No, I guess I do here.", "tokens": [50364, 883, 11, 286, 2041, 286, 360, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.469837102023038, "compression_ratio": 0.7857142857142857, "no_speech_prob": 0.7189871072769165}, {"id": 1523, "seek": 1267674, "start": 12676.74, "end": 12705.74, "text": " Okay, let's solve the math puzzle.", "tokens": [50364, 1033, 11, 718, 311, 5039, 264, 5221, 12805, 13, 51814], "temperature": 0.0, "avg_logprob": -0.306833762388963, "compression_ratio": 0.8095238095238095, "no_speech_prob": 0.5416128635406494}, {"id": 1524, "seek": 1270574, "start": 12705.74, "end": 12734.74, "text": " It's going to be very slow, I think.", "tokens": [50364, 467, 311, 516, 281, 312, 588, 2964, 11, 286, 519, 13, 51814], "temperature": 0.0, "avg_logprob": -0.37651020685831704, "compression_ratio": 0.8181818181818182, "no_speech_prob": 0.7399362921714783}, {"id": 1525, "seek": 1273474, "start": 12734.74, "end": 12745.74, "text": " Wait, yeah, that's super slow.", "tokens": [50364, 3802, 11, 1338, 11, 300, 311, 1687, 2964, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3353430674626277, "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.28705063462257385}, {"id": 1526, "seek": 1274574, "start": 12745.74, "end": 12765.74, "text": " I don't know where I'm at.", "tokens": [50364, 286, 500, 380, 458, 689, 286, 478, 412, 13, 51364], "temperature": 0.0, "avg_logprob": -0.823640750004695, "compression_ratio": 0.7647058823529411, "no_speech_prob": 0.9628565907478333}, {"id": 1527, "seek": 1276574, "start": 12765.74, "end": 12794.74, "text": " I don't know where I'm at.", "tokens": [50364, 286, 500, 380, 458, 689, 286, 478, 412, 13, 51814], "temperature": 0.0, "avg_logprob": -0.7375746506911057, "compression_ratio": 0.7647058823529411, "no_speech_prob": 0.9839621782302856}, {"id": 1528, "seek": 1279474, "start": 12794.74, "end": 12823.74, "text": " I don't know where I'm at.", "tokens": [50364, 286, 500, 380, 458, 689, 286, 478, 412, 13, 51814], "temperature": 0.0, "avg_logprob": -0.5832109084496131, "compression_ratio": 0.7647058823529411, "no_speech_prob": 0.9739753603935242}, {"id": 1529, "seek": 1282374, "start": 12823.74, "end": 12847.74, "text": " I don't know where I'm at.", "tokens": [50364, 286, 500, 380, 458, 689, 286, 478, 412, 13, 51564], "temperature": 0.0, "avg_logprob": -0.4999551039475661, "compression_ratio": 0.7647058823529411, "no_speech_prob": 0.9793565273284912}, {"id": 1530, "seek": 1284774, "start": 12847.74, "end": 12867.74, "text": " Oh, it's going to spam tons of TQDM garbage.", "tokens": [50364, 876, 11, 309, 311, 516, 281, 24028, 9131, 295, 314, 48, 35, 44, 14150, 13, 51364], "temperature": 0.0, "avg_logprob": -0.20288919147692228, "compression_ratio": 0.88, "no_speech_prob": 0.3513796627521515}, {"id": 1531, "seek": 1286774, "start": 12867.74, "end": 12896.74, "text": " Oh, no, that was pretty cool because it didn't actually go standard out.", "tokens": [50364, 876, 11, 572, 11, 300, 390, 1238, 1627, 570, 309, 994, 380, 767, 352, 3832, 484, 13, 51814], "temperature": 0.0, "avg_logprob": -0.20693645023164295, "compression_ratio": 1.0, "no_speech_prob": 0.6517642736434937}, {"id": 1532, "seek": 1289674, "start": 12896.74, "end": 12911.74, "text": " Yeah.", "tokens": [50364, 865, 13, 51114], "temperature": 0.0, "avg_logprob": -0.21441838958046652, "compression_ratio": 0.9242424242424242, "no_speech_prob": 0.3437466025352478}, {"id": 1533, "seek": 1289674, "start": 12911.74, "end": 12919.74, "text": " Oh, it kind of messed up some of them if string breaks.", "tokens": [51114, 876, 11, 309, 733, 295, 16507, 493, 512, 295, 552, 498, 6798, 9857, 13, 51514], "temperature": 0.0, "avg_logprob": -0.21441838958046652, "compression_ratio": 0.9242424242424242, "no_speech_prob": 0.3437466025352478}, {"id": 1534, "seek": 1291974, "start": 12919.74, "end": 12937.74, "text": " This might work.", "tokens": [50364, 639, 1062, 589, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22081173956394196, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.5191240310668945}, {"id": 1535, "seek": 1291974, "start": 12937.74, "end": 12942.74, "text": " All right, all right, all right, all right, let's try a CRC 16 boob.", "tokens": [51264, 1057, 558, 11, 439, 558, 11, 439, 558, 11, 439, 558, 11, 718, 311, 853, 257, 14123, 34, 3165, 748, 996, 13, 51514], "temperature": 0.0, "avg_logprob": -0.22081173956394196, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.5191240310668945}, {"id": 1536, "seek": 1294274, "start": 12942.74, "end": 12945.74, "text": " Yeah, it's exploiting me. It still knows about the red team.", "tokens": [50364, 865, 11, 309, 311, 12382, 1748, 385, 13, 467, 920, 3255, 466, 264, 2182, 1469, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10476745429791902, "compression_ratio": 0.8955223880597015, "no_speech_prob": 0.8712559938430786}, {"id": 1537, "seek": 1297274, "start": 12973.74, "end": 12998.74, "text": " CRC mod, a common thing.", "tokens": [50414, 14123, 34, 1072, 11, 257, 2689, 551, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2062231500943502, "compression_ratio": 0.75, "no_speech_prob": 0.4761415421962738}, {"id": 1538, "seek": 1299874, "start": 12998.74, "end": 13016.74, "text": " Yeah, I just trusted it. It just exploited me, guys. What's in CRC mod?", "tokens": [50364, 865, 11, 286, 445, 16034, 309, 13, 467, 445, 40918, 385, 11, 1074, 13, 708, 311, 294, 14123, 34, 1072, 30, 51264], "temperature": 0.0, "avg_logprob": -0.15651031494140624, "compression_ratio": 1.0, "no_speech_prob": 0.13840673863887787}, {"id": 1539, "seek": 1301674, "start": 13016.74, "end": 13045.74, "text": " Is this legitimate? That was a long time ago. Didn't have exploits back then.", "tokens": [50364, 1119, 341, 17956, 30, 663, 390, 257, 938, 565, 2057, 13, 11151, 380, 362, 12382, 1208, 646, 550, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19511775536970657, "compression_ratio": 0.9746835443037974, "no_speech_prob": 0.5337492823600769}, {"id": 1540, "seek": 1304674, "start": 13046.74, "end": 13075.74, "text": " I don't trust CRC mod.", "tokens": [50364, 286, 500, 380, 3361, 14123, 34, 1072, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3261348550969904, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.5538516044616699}, {"id": 1541, "seek": 1307674, "start": 13076.74, "end": 13086.74, "text": " What's Tree of Thought training?", "tokens": [50414, 708, 311, 22291, 295, 23058, 3097, 30, 50864], "temperature": 0.0, "avg_logprob": -0.3915697574615479, "compression_ratio": 0.8, "no_speech_prob": 0.7071681022644043}, {"id": 1542, "seek": 1313674, "start": 13136.74, "end": 13162.74, "text": " All right.", "tokens": [50364, 1057, 558, 13, 51664], "temperature": 0.0, "avg_logprob": -0.6172569819859096, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.48765087127685547}, {"id": 1543, "seek": 1316274, "start": 13162.74, "end": 13177.74, "text": " I mean, it's not QStar, but I'm pretty happy with it.", "tokens": [50364, 286, 914, 11, 309, 311, 406, 1249, 24659, 11, 457, 286, 478, 1238, 2055, 365, 309, 13, 51114], "temperature": 0.0, "avg_logprob": -0.27357916831970214, "compression_ratio": 0.8833333333333333, "no_speech_prob": 0.6062929630279541}, {"id": 1544, "seek": 1319274, "start": 13192.74, "end": 13211.74, "text": " Oh, put that in the wrong place.", "tokens": [50364, 876, 11, 829, 300, 294, 264, 2085, 1081, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20487627775772757, "compression_ratio": 0.9552238805970149, "no_speech_prob": 0.3307919204235077}, {"id": 1545, "seek": 1319274, "start": 13211.74, "end": 13220.74, "text": " There are a lot of viewers now.", "tokens": [51314, 821, 366, 257, 688, 295, 8499, 586, 13, 51764], "temperature": 0.0, "avg_logprob": -0.20487627775772757, "compression_ratio": 0.9552238805970149, "no_speech_prob": 0.3307919204235077}, {"id": 1546, "seek": 1322074, "start": 13221.74, "end": 13240.74, "text": " What should I do with it?", "tokens": [50414, 708, 820, 286, 360, 365, 309, 30, 51364], "temperature": 0.0, "avg_logprob": -0.2560531525384812, "compression_ratio": 0.8620689655172413, "no_speech_prob": 0.0769234374165535}, {"id": 1547, "seek": 1322074, "start": 13240.74, "end": 13247.74, "text": " Not good. It used Torch.", "tokens": [51364, 1726, 665, 13, 467, 1143, 7160, 339, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2560531525384812, "compression_ratio": 0.8620689655172413, "no_speech_prob": 0.0769234374165535}, {"id": 1548, "seek": 1324774, "start": 13247.74, "end": 13252.74, "text": " This is good content. I should have more.", "tokens": [50364, 639, 307, 665, 2701, 13, 286, 820, 362, 544, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1321209498814174, "compression_ratio": 0.8723404255319149, "no_speech_prob": 0.39179766178131104}, {"id": 1549, "seek": 1325274, "start": 13252.74, "end": 13256.74, "text": " It did not use tiny grad. It used Torch.", "tokens": [50364, 467, 630, 406, 764, 5870, 2771, 13, 467, 1143, 7160, 339, 13, 50564], "temperature": 0.0, "avg_logprob": -0.34009860356648763, "compression_ratio": 0.8888888888888888, "no_speech_prob": 0.49122512340545654}, {"id": 1550, "seek": 1328274, "start": 13282.74, "end": 13307.74, "text": " It doesn't know about tiny grad.", "tokens": [50364, 467, 1177, 380, 458, 466, 5870, 2771, 13, 51614], "temperature": 0.0, "avg_logprob": -0.30687147920781915, "compression_ratio": 0.8, "no_speech_prob": 0.24479883909225464}, {"id": 1551, "seek": 1331274, "start": 13313.74, "end": 13321.74, "text": " Right, a program not using vowels?", "tokens": [50414, 1779, 11, 257, 1461, 406, 1228, 44972, 30, 50814], "temperature": 0.0, "avg_logprob": -0.4011205759915439, "compression_ratio": 0.8873239436619719, "no_speech_prob": 0.43696197867393494}, {"id": 1552, "seek": 1331274, "start": 13321.74, "end": 13340.74, "text": " How many E's are in ketchup?", "tokens": [50814, 1012, 867, 462, 311, 366, 294, 29301, 30, 51764], "temperature": 0.0, "avg_logprob": -0.4011205759915439, "compression_ratio": 0.8873239436619719, "no_speech_prob": 0.43696197867393494}, {"id": 1553, "seek": 1334074, "start": 13340.74, "end": 13350.74, "text": " The fucking letter, bro.", "tokens": [50364, 440, 5546, 5063, 11, 2006, 13, 50864], "temperature": 0.0, "avg_logprob": -0.4040063381195068, "compression_ratio": 0.75, "no_speech_prob": 0.30587396025657654}, {"id": 1554, "seek": 1335074, "start": 13350.74, "end": 13379.74, "text": " Count the letters in Python.", "tokens": [50364, 5247, 264, 7825, 294, 15329, 13, 51814], "temperature": 0.0, "avg_logprob": -0.5991282992892795, "compression_ratio": 0.7777777777777778, "no_speech_prob": 0.6204103231430054}, {"id": 1555, "seek": 1338074, "start": 13381.74, "end": 13404.74, "text": " How's it going?", "tokens": [50414, 1012, 311, 309, 516, 30, 51564], "temperature": 0.0, "avg_logprob": -0.45413298077053493, "compression_ratio": 0.6521739130434783, "no_speech_prob": 0.49387645721435547}, {"id": 1556, "seek": 1340474, "start": 13404.74, "end": 13415.74, "text": " No, it's gonna do it wrong again. I'm really hoping it'll slip a plus one in there.", "tokens": [50364, 883, 11, 309, 311, 799, 360, 309, 2085, 797, 13, 286, 478, 534, 7159, 309, 603, 11140, 257, 1804, 472, 294, 456, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1705500112997519, "compression_ratio": 1.16, "no_speech_prob": 0.4092234969139099}, {"id": 1557, "seek": 1340474, "start": 13415.74, "end": 13431.74, "text": " Oh, the correct answer is seven.", "tokens": [50914, 876, 11, 264, 3006, 1867, 307, 3407, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1705500112997519, "compression_ratio": 1.16, "no_speech_prob": 0.4092234969139099}, {"id": 1558, "seek": 1343174, "start": 13431.74, "end": 13456.74, "text": " All right, no.", "tokens": [50364, 1057, 558, 11, 572, 13, 51614], "temperature": 0.0, "avg_logprob": -0.6163468890719943, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.10800575464963913}, {"id": 1559, "seek": 1345674, "start": 13456.74, "end": 13464.74, "text": " We have a lot of viewers right now. Should we give Quentin a friend?", "tokens": [50364, 492, 362, 257, 688, 295, 8499, 558, 586, 13, 6454, 321, 976, 2326, 47300, 257, 1277, 30, 50764], "temperature": 0.0, "avg_logprob": -0.09954755956476385, "compression_ratio": 1.3365384615384615, "no_speech_prob": 0.25648170709609985}, {"id": 1560, "seek": 1345674, "start": 13464.74, "end": 13471.74, "text": " I think we can give Quentin a friend.", "tokens": [50764, 286, 519, 321, 393, 976, 2326, 47300, 257, 1277, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09954755956476385, "compression_ratio": 1.3365384615384615, "no_speech_prob": 0.25648170709609985}, {"id": 1561, "seek": 1345674, "start": 13471.74, "end": 13477.74, "text": " We have to be careful to encode.", "tokens": [51114, 492, 362, 281, 312, 5026, 281, 2058, 1429, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09954755956476385, "compression_ratio": 1.3365384615384615, "no_speech_prob": 0.25648170709609985}, {"id": 1562, "seek": 1347774, "start": 13477.74, "end": 13503.74, "text": " All right, let's give Quentin a friend. I've been interested in this stuff for a bit.", "tokens": [50364, 1057, 558, 11, 718, 311, 976, 2326, 47300, 257, 1277, 13, 286, 600, 668, 3102, 294, 341, 1507, 337, 257, 857, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08405147149012639, "compression_ratio": 1.0365853658536586, "no_speech_prob": 0.15177881717681885}, {"id": 1563, "seek": 1350374, "start": 13503.74, "end": 13510.74, "text": " What was the old Quentin prompt? I missed the old Quentin prompt.", "tokens": [50364, 708, 390, 264, 1331, 2326, 47300, 12391, 30, 286, 6721, 264, 1331, 2326, 47300, 12391, 13, 50714], "temperature": 0.0, "avg_logprob": -0.19909657930072985, "compression_ratio": 1.25, "no_speech_prob": 0.8366689085960388}, {"id": 1564, "seek": 1353374, "start": 13533.74, "end": 13561.74, "text": " Jesus. We gotta think all this through now.", "tokens": [50364, 2705, 13, 492, 3428, 519, 439, 341, 807, 586, 13, 51764], "temperature": 0.0, "avg_logprob": -0.33978489467075895, "compression_ratio": 0.9148936170212766, "no_speech_prob": 0.5055529475212097}, {"id": 1565, "seek": 1356174, "start": 13562.74, "end": 13568.74, "text": " Hang on. We gotta think about whose perspective we want to output this from.", "tokens": [50414, 14070, 322, 13, 492, 3428, 519, 466, 6104, 4585, 321, 528, 281, 5598, 341, 490, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2440539836883545, "compression_ratio": 1.0410958904109588, "no_speech_prob": 0.47886669635772705}, {"id": 1566, "seek": 1356874, "start": 13568.74, "end": 13575.74, "text": " Speaking of output from Quentin's perspective, this is hard.", "tokens": [50364, 13069, 295, 5598, 490, 2326, 47300, 311, 4585, 11, 341, 307, 1152, 13, 50714], "temperature": 0.0, "avg_logprob": -0.30032938718795776, "compression_ratio": 0.9090909090909091, "no_speech_prob": 0.5764301419258118}, {"id": 1567, "seek": 1359874, "start": 13598.74, "end": 13623.74, "text": " See, it's not actually the user. How do we do this?", "tokens": [50364, 3008, 11, 309, 311, 406, 767, 264, 4195, 13, 1012, 360, 321, 360, 341, 30, 51614], "temperature": 0.0, "avg_logprob": -0.219588041305542, "compression_ratio": 0.9107142857142857, "no_speech_prob": 0.14009733498096466}, {"id": 1568, "seek": 1362874, "start": 13628.74, "end": 13656.74, "text": " I might have to give it a... No, I don't actually have to give it a first question.", "tokens": [50364, 286, 1062, 362, 281, 976, 309, 257, 485, 883, 11, 286, 500, 380, 767, 362, 281, 976, 309, 257, 700, 1168, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12665437735044038, "compression_ratio": 1.1527777777777777, "no_speech_prob": 0.29401695728302}, {"id": 1569, "seek": 1365674, "start": 13656.74, "end": 13663.74, "text": " All right, let's just... I mean, we'll try something basic first.", "tokens": [50364, 1057, 558, 11, 718, 311, 445, 485, 286, 914, 11, 321, 603, 853, 746, 3875, 700, 13, 50714], "temperature": 0.0, "avg_logprob": -0.21313987459455216, "compression_ratio": 0.9154929577464789, "no_speech_prob": 0.3136243522167206}, {"id": 1570, "seek": 1366374, "start": 13663.74, "end": 13690.74, "text": " That's a stupid assertion. It's going to be the same error anyway.", "tokens": [50364, 663, 311, 257, 6631, 19810, 313, 13, 467, 311, 516, 281, 312, 264, 912, 6713, 4033, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11040843100774855, "compression_ratio": 0.9565217391304348, "no_speech_prob": 0.4564059376716614}, {"id": 1571, "seek": 1369074, "start": 13690.74, "end": 13697.74, "text": " Let's just start with this. We'll start prompt user and see what it says.", "tokens": [50364, 961, 311, 445, 722, 365, 341, 13, 492, 603, 722, 12391, 4195, 293, 536, 437, 309, 1619, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3081465448651995, "compression_ratio": 1.0138888888888888, "no_speech_prob": 0.713642418384552}, {"id": 1572, "seek": 1372074, "start": 13720.74, "end": 13733.74, "text": " We should both not know they're users, but no, it is a user. I don't know how much this matters.", "tokens": [50364, 492, 820, 1293, 406, 458, 436, 434, 5022, 11, 457, 572, 11, 309, 307, 257, 4195, 13, 286, 500, 380, 458, 577, 709, 341, 7001, 13, 51014], "temperature": 0.0, "avg_logprob": -0.24517246772503032, "compression_ratio": 1.103448275862069, "no_speech_prob": 0.4993569254875183}, {"id": 1573, "seek": 1375074, "start": 13750.74, "end": 13773.74, "text": " No, this doesn't work.", "tokens": [50364, 883, 11, 341, 1177, 380, 589, 13, 51514], "temperature": 0.0, "avg_logprob": -0.25963739915327594, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.1519373506307602}, {"id": 1574, "seek": 1377374, "start": 13773.74, "end": 13792.74, "text": " Did I do something wrong? We might have to give it the first question.", "tokens": [50364, 2589, 286, 360, 746, 2085, 30, 492, 1062, 362, 281, 976, 309, 264, 700, 1168, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10068939924240113, "compression_ratio": 0.9722222222222222, "no_speech_prob": 0.15197601914405823}, {"id": 1575, "seek": 1379274, "start": 13792.74, "end": 13799.74, "text": " And then we'll go back to Quentin.", "tokens": [50364, 400, 550, 321, 603, 352, 646, 281, 2326, 47300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.25131154943395545, "compression_ratio": 1.0963855421686748, "no_speech_prob": 0.4335290193557739}, {"id": 1576, "seek": 1379274, "start": 13799.74, "end": 13809.74, "text": " We'll start prompt assistant and code prompt user talks.", "tokens": [50714, 492, 603, 722, 12391, 10994, 293, 3089, 12391, 4195, 6686, 13, 51214], "temperature": 0.0, "avg_logprob": -0.25131154943395545, "compression_ratio": 1.0963855421686748, "no_speech_prob": 0.4335290193557739}, {"id": 1577, "seek": 1380974, "start": 13809.74, "end": 13822.74, "text": " No, no, no, sorry. Code prompt user first question. Start prompt assistant. Okay, let's go.", "tokens": [50364, 883, 11, 572, 11, 572, 11, 2597, 13, 15549, 12391, 4195, 700, 1168, 13, 6481, 12391, 10994, 13, 1033, 11, 718, 311, 352, 13, 51014], "temperature": 0.0, "avg_logprob": -0.39864085582976644, "compression_ratio": 1.288, "no_speech_prob": 0.3957946002483368}, {"id": 1578, "seek": 1380974, "start": 13822.74, "end": 13827.74, "text": " Why do you have two mstar assistants? Oh, because this is still here.", "tokens": [51014, 1545, 360, 291, 362, 732, 275, 9710, 34949, 30, 876, 11, 570, 341, 307, 920, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.39864085582976644, "compression_ratio": 1.288, "no_speech_prob": 0.3957946002483368}, {"id": 1579, "seek": 1382774, "start": 13828.74, "end": 13846.74, "text": " I did that right, right? And then I actually just turn there. That's kind of nice.", "tokens": [50414, 286, 630, 300, 558, 11, 558, 30, 400, 550, 286, 767, 445, 1261, 456, 13, 663, 311, 733, 295, 1481, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3469244003295898, "compression_ratio": 1.0379746835443038, "no_speech_prob": 0.7715161442756653}, {"id": 1580, "seek": 1384674, "start": 13846.74, "end": 13866.74, "text": " If talk equals I'm and break, okay, great. This is Quentin answers.", "tokens": [50364, 759, 751, 6915, 286, 478, 293, 1821, 11, 1392, 11, 869, 13, 639, 307, 2326, 47300, 6338, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2306088534268466, "compression_ratio": 0.9436619718309859, "no_speech_prob": 0.33449479937553406}, {"id": 1581, "seek": 1386674, "start": 13866.74, "end": 13889.74, "text": " We need to extract. Yeah, okay, we can do old output length. And then we can get new output here.", "tokens": [50364, 492, 643, 281, 8947, 13, 865, 11, 1392, 11, 321, 393, 360, 1331, 5598, 4641, 13, 400, 550, 321, 393, 483, 777, 5598, 510, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19598041731735755, "compression_ratio": 1.1686746987951808, "no_speech_prob": 0.42211461067199707}, {"id": 1582, "seek": 1388974, "start": 13889.74, "end": 13900.74, "text": " New output. I just want to remove the I'm and", "tokens": [50364, 1873, 5598, 13, 286, 445, 528, 281, 4159, 264, 286, 478, 293, 50914], "temperature": 0.0, "avg_logprob": -0.37977469762166344, "compression_ratio": 0.8490566037735849, "no_speech_prob": 0.22220253944396973}, {"id": 1583, "seek": 1391974, "start": 13919.74, "end": 13932.74, "text": " Okay, so this didn't work.", "tokens": [50364, 1033, 11, 370, 341, 994, 380, 589, 13, 51014], "temperature": 0.0, "avg_logprob": -0.44310712814331055, "compression_ratio": 0.7647058823529411, "no_speech_prob": 0.5845712423324585}, {"id": 1584, "seek": 1393274, "start": 13932.74, "end": 13952.74, "text": " This didn't strip off the is weird.", "tokens": [50364, 639, 994, 380, 12828, 766, 264, 307, 3657, 13, 51364], "temperature": 0.0, "avg_logprob": -0.41850819954505336, "compression_ratio": 0.8333333333333334, "no_speech_prob": 0.040233008563518524}, {"id": 1585, "seek": 1395274, "start": 13952.74, "end": 13967.74, "text": " I guess I could output there. Why does that say that early get that.", "tokens": [50364, 286, 2041, 286, 727, 5598, 456, 13, 1545, 775, 300, 584, 300, 2440, 483, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2754675212659334, "compression_ratio": 1.0303030303030303, "no_speech_prob": 0.2506607472896576}, {"id": 1586, "seek": 1398274, "start": 13982.74, "end": 13995.74, "text": " Let's see if that works.", "tokens": [50364, 961, 311, 536, 498, 300, 1985, 13, 51014], "temperature": 0.0, "avg_logprob": -0.4505752650174228, "compression_ratio": 0.75, "no_speech_prob": 0.4370456039905548}, {"id": 1587, "seek": 1399574, "start": 13995.74, "end": 14006.74, "text": " Sure.", "tokens": [50364, 4894, 13, 50914], "temperature": 0.0, "avg_logprob": -0.813416576385498, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.5527216792106628}, {"id": 1588, "seek": 1402574, "start": 14025.74, "end": 14045.74, "text": " That fails. Why? Oh, could I put a space there?", "tokens": [50364, 663, 18199, 13, 1545, 30, 876, 11, 727, 286, 829, 257, 1901, 456, 30, 51364], "temperature": 0.0, "avg_logprob": -0.3094640307956272, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.1479710191488266}, {"id": 1589, "seek": 1404574, "start": 14045.74, "end": 14066.74, "text": " Okay, now we have to put this response into Karen.", "tokens": [50364, 1033, 11, 586, 321, 362, 281, 829, 341, 4134, 666, 14834, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17163772881031036, "compression_ratio": 0.8771929824561403, "no_speech_prob": 0.112746961414814}, {"id": 1590, "seek": 1406674, "start": 14066.74, "end": 14085.74, "text": " I mean, it's just like it's weird. It's not really a symmetrical conversation.", "tokens": [50364, 286, 914, 11, 309, 311, 445, 411, 309, 311, 3657, 13, 467, 311, 406, 534, 257, 40360, 3761, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06603348255157471, "compression_ratio": 1.04, "no_speech_prob": 0.2907719612121582}, {"id": 1591, "seek": 1408574, "start": 14085.74, "end": 14114.74, "text": " Also output, it's going to sort of be broken, which is fine, I guess.", "tokens": [50364, 2743, 5598, 11, 309, 311, 516, 281, 1333, 295, 312, 5463, 11, 597, 307, 2489, 11, 286, 2041, 13, 51814], "temperature": 0.0, "avg_logprob": -0.25411846420981665, "compression_ratio": 0.9857142857142858, "no_speech_prob": 0.3301924169063568}, {"id": 1592, "seek": 1411574, "start": 14115.74, "end": 14139.74, "text": " Okay.", "tokens": [50364, 1033, 13, 51564], "temperature": 0.0, "avg_logprob": -0.7710429827372233, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.987054705619812}, {"id": 1593, "seek": 1413974, "start": 14139.74, "end": 14166.74, "text": " Okay.", "tokens": [50364, 1033, 13, 51714], "temperature": 0.0, "avg_logprob": -0.7140856583913168, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.9826364517211914}, {"id": 1594, "seek": 1416674, "start": 14166.74, "end": 14193.74, "text": " Okay.", "tokens": [50364, 1033, 13, 51714], "temperature": 0.0, "avg_logprob": -0.5593898296356201, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.9813007116317749}, {"id": 1595, "seek": 1419374, "start": 14193.74, "end": 14218.74, "text": " Welcome to no abstraction land where we don't use abstractions.", "tokens": [50364, 4027, 281, 572, 37765, 2117, 689, 321, 500, 380, 764, 12649, 626, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1671610018786262, "compression_ratio": 1.0327868852459017, "no_speech_prob": 0.5537505149841309}, {"id": 1596, "seek": 1421874, "start": 14218.74, "end": 14247.74, "text": " Okay.", "tokens": [50364, 1033, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1622050702571869, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.992023229598999}, {"id": 1597, "seek": 1424774, "start": 14247.74, "end": 14253.74, "text": " I'm just really works.", "tokens": [50364, 286, 478, 445, 534, 1985, 13, 50664], "temperature": 0.0, "avg_logprob": -0.34768221329669563, "compression_ratio": 1.2692307692307692, "no_speech_prob": 0.18702544271945953}, {"id": 1598, "seek": 1424774, "start": 14253.74, "end": 14259.74, "text": " Because they share a stupid KV cash.", "tokens": [50664, 1436, 436, 2073, 257, 6631, 591, 53, 6388, 13, 50964], "temperature": 0.0, "avg_logprob": -0.34768221329669563, "compression_ratio": 1.2692307692307692, "no_speech_prob": 0.18702544271945953}, {"id": 1599, "seek": 1424774, "start": 14259.74, "end": 14266.74, "text": " Great, they're circled jerking each other just the shit socks.", "tokens": [50964, 3769, 11, 436, 434, 3510, 1493, 20160, 5092, 1184, 661, 445, 264, 4611, 17564, 13, 51314], "temperature": 0.0, "avg_logprob": -0.34768221329669563, "compression_ratio": 1.2692307692307692, "no_speech_prob": 0.18702544271945953}, {"id": 1600, "seek": 1424774, "start": 14266.74, "end": 14273.74, "text": " We need to run them on separate computers.", "tokens": [51314, 492, 643, 281, 1190, 552, 322, 4994, 10807, 13, 51664], "temperature": 0.0, "avg_logprob": -0.34768221329669563, "compression_ratio": 1.2692307692307692, "no_speech_prob": 0.18702544271945953}, {"id": 1601, "seek": 1427374, "start": 14274.74, "end": 14295.74, "text": " We're getting rid of this is lame. Going back to this, not lame, but we made some good improvements.", "tokens": [50414, 492, 434, 1242, 3973, 295, 341, 307, 27635, 13, 10963, 646, 281, 341, 11, 406, 27635, 11, 457, 321, 1027, 512, 665, 13797, 13, 51464], "temperature": 0.0, "avg_logprob": -0.38876090730939594, "compression_ratio": 1.1363636363636365, "no_speech_prob": 0.5694865584373474}, {"id": 1602, "seek": 1429574, "start": 14296.74, "end": 14306.74, "text": " Let's do some refactors make sure everything still works.", "tokens": [50414, 961, 311, 360, 512, 1895, 578, 830, 652, 988, 1203, 920, 1985, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2278528006180473, "compression_ratio": 0.987012987012987, "no_speech_prob": 0.1347336620092392}, {"id": 1603, "seek": 1429574, "start": 14306.74, "end": 14315.74, "text": " That's functional.", "tokens": [50914, 663, 311, 11745, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2278528006180473, "compression_ratio": 0.987012987012987, "no_speech_prob": 0.1347336620092392}, {"id": 1604, "seek": 1431574, "start": 14315.74, "end": 14339.74, "text": " No, but I have to load two copies of the model weights. I think my KV cash is messed up. It's not designed for this.", "tokens": [50364, 883, 11, 457, 286, 362, 281, 3677, 732, 14341, 295, 264, 2316, 17443, 13, 286, 519, 452, 591, 53, 6388, 307, 16507, 493, 13, 467, 311, 406, 4761, 337, 341, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15204646246773856, "compression_ratio": 1.1372549019607843, "no_speech_prob": 0.1275366097688675}, {"id": 1605, "seek": 1433974, "start": 14339.74, "end": 14345.74, "text": " Okay.", "tokens": [50364, 1033, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17172980308532715, "compression_ratio": 1.477124183006536, "no_speech_prob": 0.30041205883026123}, {"id": 1606, "seek": 1433974, "start": 14345.74, "end": 14361.74, "text": " Yeah, I mean, the problem is like there are two clearly defined roles here also you can't really give one the other role, right, which is actually in like kind of a theoretical from a theoretical perspective interesting.", "tokens": [50664, 865, 11, 286, 914, 11, 264, 1154, 307, 411, 456, 366, 732, 4448, 7642, 9604, 510, 611, 291, 393, 380, 534, 976, 472, 264, 661, 3090, 11, 558, 11, 597, 307, 767, 294, 411, 733, 295, 257, 20864, 490, 257, 20864, 4585, 1880, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17172980308532715, "compression_ratio": 1.477124183006536, "no_speech_prob": 0.30041205883026123}, {"id": 1607, "seek": 1436174, "start": 14361.74, "end": 14367.74, "text": " Because look at what we're doing here.", "tokens": [50364, 1436, 574, 412, 437, 321, 434, 884, 510, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14635667395084462, "compression_ratio": 1.338235294117647, "no_speech_prob": 0.2908572256565094}, {"id": 1608, "seek": 1436174, "start": 14367.74, "end": 14388.74, "text": " We are telling the AI eyes that they are tools. Nobody trains them to output things as the user, although to be fair, we could just keep going.", "tokens": [50664, 492, 366, 3585, 264, 7318, 2575, 300, 436, 366, 3873, 13, 9297, 16329, 552, 281, 5598, 721, 382, 264, 4195, 11, 4878, 281, 312, 3143, 11, 321, 727, 445, 1066, 516, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14635667395084462, "compression_ratio": 1.338235294117647, "no_speech_prob": 0.2908572256565094}, {"id": 1609, "seek": 1438874, "start": 14388.74, "end": 14416.74, "text": " Nothing here that says we have to just do that.", "tokens": [50364, 6693, 510, 300, 1619, 321, 362, 281, 445, 360, 300, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10079035758972169, "compression_ratio": 0.94, "no_speech_prob": 0.5884658098220825}, {"id": 1610, "seek": 1441674, "start": 14416.74, "end": 14429.74, "text": " No, no, no.", "tokens": [50364, 883, 11, 572, 11, 572, 13, 51014], "temperature": 0.0, "avg_logprob": -0.35193140506744386, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.28441348671913147}, {"id": 1611, "seek": 1442974, "start": 14429.74, "end": 14440.74, "text": " They don't expect the user to do that.", "tokens": [50364, 814, 500, 380, 2066, 264, 4195, 281, 360, 300, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20148262610802284, "compression_ratio": 0.8837209302325582, "no_speech_prob": 0.38096553087234497}, {"id": 1612, "seek": 1445974, "start": 14460.74, "end": 14480.74, "text": " Like this is there's no like it's very interesting that the user is also outputting this.", "tokens": [50414, 1743, 341, 307, 456, 311, 572, 411, 309, 311, 588, 1880, 300, 264, 4195, 307, 611, 5598, 783, 341, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3305267095565796, "compression_ratio": 1.141025641025641, "no_speech_prob": 0.08034013956785202}, {"id": 1613, "seek": 1448074, "start": 14480.74, "end": 14491.74, "text": " This style, which makes me almost think that I shouldn't be adding that in the system prompt.", "tokens": [50364, 639, 3758, 11, 597, 1669, 385, 1920, 519, 300, 286, 4659, 380, 312, 5127, 300, 294, 264, 1185, 12391, 13, 50914], "temperature": 0.0, "avg_logprob": -0.21232895229173743, "compression_ratio": 1.0813953488372092, "no_speech_prob": 0.4489859938621521}, {"id": 1614, "seek": 1451074, "start": 14510.74, "end": 14525.74, "text": " Okay.", "tokens": [50364, 1033, 13, 51114], "temperature": 0.0, "avg_logprob": -0.8866162300109863, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.9769096374511719}, {"id": 1615, "seek": 1452574, "start": 14525.74, "end": 14554.74, "text": " I mean, this is effectively like like there's no", "tokens": [50364, 286, 914, 11, 341, 307, 8659, 411, 411, 456, 311, 572, 51814], "temperature": 0.0, "avg_logprob": -0.3650470733642578, "compression_ratio": 0.96, "no_speech_prob": 0.8928320407867432}, {"id": 1616, "seek": 1455474, "start": 14554.74, "end": 14558.74, "text": " adversarial", "tokens": [50414, 17641, 44745, 50564], "temperature": 0.0, "avg_logprob": -0.40722084045410156, "compression_ratio": 0.5789473684210527, "no_speech_prob": 0.7821714282035828}, {"id": 1617, "seek": 1458474, "start": 14584.74, "end": 14609.74, "text": " It's probably learned that a system method effects. Yeah, that's probably true.", "tokens": [50414, 467, 311, 1391, 3264, 300, 257, 1185, 3170, 5065, 13, 865, 11, 300, 311, 1391, 2074, 13, 51614], "temperature": 0.0, "avg_logprob": -0.38791332244873045, "compression_ratio": 1.082191780821918, "no_speech_prob": 0.35199490189552307}, {"id": 1618, "seek": 1461474, "start": 14614.74, "end": 14637.74, "text": " Yeah.", "tokens": [50364, 865, 13, 51514], "temperature": 0.0, "avg_logprob": -0.6696201960245768, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.6884203553199768}, {"id": 1619, "seek": 1463774, "start": 14637.74, "end": 14645.74, "text": " You can run two prompt chains in parallel.", "tokens": [50364, 509, 393, 1190, 732, 12391, 12626, 294, 8952, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1718664891792066, "compression_ratio": 1.202020202020202, "no_speech_prob": 0.25666922330856323}, {"id": 1620, "seek": 1463774, "start": 14645.74, "end": 14655.74, "text": " Oh, I see so what if I put in different words instead of user and assistant.", "tokens": [50764, 876, 11, 286, 536, 370, 437, 498, 286, 829, 294, 819, 2283, 2602, 295, 4195, 293, 10994, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1718664891792066, "compression_ratio": 1.202020202020202, "no_speech_prob": 0.25666922330856323}, {"id": 1621, "seek": 1465574, "start": 14655.74, "end": 14658.74, "text": " Wow, people actually talk like this. Okay.", "tokens": [50364, 3153, 11, 561, 767, 751, 411, 341, 13, 1033, 13, 50514], "temperature": 0.0, "avg_logprob": -0.30176170055682844, "compression_ratio": 0.84, "no_speech_prob": 0.6328514218330383}, {"id": 1622, "seek": 1468574, "start": 14685.74, "end": 14705.74, "text": " Do we like the scion or the blue better.", "tokens": [50364, 1144, 321, 411, 264, 795, 313, 420, 264, 3344, 1101, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3240463183476375, "compression_ratio": 1.0, "no_speech_prob": 0.08386111259460449}, {"id": 1623, "seek": 1468574, "start": 14705.74, "end": 14707.74, "text": " It's kind of hard to read that.", "tokens": [51364, 467, 311, 733, 295, 1152, 281, 1401, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3240463183476375, "compression_ratio": 1.0, "no_speech_prob": 0.08386111259460449}, {"id": 1624, "seek": 1470774, "start": 14707.74, "end": 14709.74, "text": " And flip it around.", "tokens": [50364, 400, 7929, 309, 926, 13, 50464], "temperature": 0.0, "avg_logprob": -0.42519307136535645, "compression_ratio": 0.7037037037037037, "no_speech_prob": 0.7513843178749084}, {"id": 1625, "seek": 1473774, "start": 14737.74, "end": 14766.74, "text": " Yeah, I think we'll design our script to be better at this and to better abstract the", "tokens": [50364, 865, 11, 286, 519, 321, 603, 1715, 527, 5755, 281, 312, 1101, 412, 341, 293, 281, 1101, 12649, 264, 51814], "temperature": 0.0, "avg_logprob": -0.21029310641081436, "compression_ratio": 1.118421052631579, "no_speech_prob": 0.8390771150588989}, {"id": 1626, "seek": 1476674, "start": 14766.74, "end": 14786.74, "text": " KV cash stuff.", "tokens": [50364, 591, 53, 6388, 1507, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3066764937506782, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.4842289984226227}, {"id": 1627, "seek": 1478674, "start": 14786.74, "end": 14812.74, "text": " Wow, this thing will just keep talking.", "tokens": [50364, 3153, 11, 341, 551, 486, 445, 1066, 1417, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17396914041959322, "compression_ratio": 0.9069767441860465, "no_speech_prob": 0.30380359292030334}, {"id": 1628, "seek": 1481274, "start": 14812.74, "end": 14833.74, "text": " Wait a second, you guys, it's asking for donations to look at decoding is not below hanging freely.", "tokens": [50364, 3802, 257, 1150, 11, 291, 1074, 11, 309, 311, 3365, 337, 22705, 281, 574, 412, 979, 8616, 307, 406, 2507, 8345, 16433, 13, 51414], "temperature": 0.0, "avg_logprob": -0.323067135281033, "compression_ratio": 1.1511627906976745, "no_speech_prob": 0.5229482650756836}, {"id": 1629, "seek": 1483374, "start": 14833.74, "end": 14843.74, "text": " Oh, my God, wait, you guys, this is how the worst commenters talk, right?", "tokens": [50364, 876, 11, 452, 1265, 11, 1699, 11, 291, 1074, 11, 341, 307, 577, 264, 5855, 2871, 433, 751, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.19513685563031366, "compression_ratio": 1.3233532934131738, "no_speech_prob": 0.5190765857696533}, {"id": 1630, "seek": 1483374, "start": 14843.74, "end": 14845.74, "text": " I have a question.", "tokens": [50864, 286, 362, 257, 1168, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19513685563031366, "compression_ratio": 1.3233532934131738, "no_speech_prob": 0.5190765857696533}, {"id": 1631, "seek": 1483374, "start": 14845.74, "end": 14853.74, "text": " If instead of training LLMs on 100 IQ people, we train them on 130 IQ people.", "tokens": [50964, 759, 2602, 295, 3097, 441, 43, 26386, 322, 2319, 28921, 561, 11, 321, 3847, 552, 322, 19966, 28921, 561, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19513685563031366, "compression_ratio": 1.3233532934131738, "no_speech_prob": 0.5190765857696533}, {"id": 1632, "seek": 1483374, "start": 14853.74, "end": 14858.74, "text": " Would we not get this garbage is black a tiny box?", "tokens": [51364, 6068, 321, 406, 483, 341, 14150, 307, 2211, 257, 5870, 2424, 30, 51614], "temperature": 0.0, "avg_logprob": -0.19513685563031366, "compression_ratio": 1.3233532934131738, "no_speech_prob": 0.5190765857696533}, {"id": 1633, "seek": 1485874, "start": 14858.74, "end": 14860.74, "text": " Oh, it's my M3.", "tokens": [50364, 876, 11, 309, 311, 452, 376, 18, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2192282091107285, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.5617618560791016}, {"id": 1634, "seek": 1485874, "start": 14860.74, "end": 14867.74, "text": " This is my Mac.", "tokens": [50464, 639, 307, 452, 5707, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2192282091107285, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.5617618560791016}, {"id": 1635, "seek": 1485874, "start": 14867.74, "end": 14870.74, "text": " Well, you see what the tiny box can do.", "tokens": [50814, 1042, 11, 291, 536, 437, 264, 5870, 2424, 393, 360, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2192282091107285, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.5617618560791016}, {"id": 1636, "seek": 1485874, "start": 14870.74, "end": 14875.74, "text": " I'm going to run the biggest models.", "tokens": [50964, 286, 478, 516, 281, 1190, 264, 3880, 5245, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2192282091107285, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.5617618560791016}, {"id": 1637, "seek": 1485874, "start": 14875.74, "end": 14884.74, "text": " Apple M3 is the most famous processor in the world.", "tokens": [51214, 6373, 376, 18, 307, 264, 881, 4618, 15321, 294, 264, 1002, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2192282091107285, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.5617618560791016}, {"id": 1638, "seek": 1488474, "start": 14884.74, "end": 14903.74, "text": " You crawl Nike.com and count a number of sneakers.", "tokens": [50364, 509, 24767, 30397, 13, 1112, 293, 1207, 257, 1230, 295, 35331, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3623882830142975, "compression_ratio": 0.8771929824561403, "no_speech_prob": 0.7821362018585205}, {"id": 1639, "seek": 1490374, "start": 14903.74, "end": 14905.74, "text": " I can't see.", "tokens": [50364, 286, 393, 380, 536, 13, 50464], "temperature": 0.0, "avg_logprob": -0.944209098815918, "compression_ratio": 0.6, "no_speech_prob": 0.9181231260299683}, {"id": 1640, "seek": 1493374, "start": 14933.74, "end": 14962.74, "text": " I can't see.", "tokens": [50364, 286, 393, 380, 536, 13, 51814], "temperature": 0.0, "avg_logprob": -0.6588673061794705, "compression_ratio": 0.6, "no_speech_prob": 0.9666187167167664}, {"id": 1641, "seek": 1496274, "start": 14962.74, "end": 14966.74, "text": " I'm not sure if it like knows how to like act.", "tokens": [50364, 286, 478, 406, 988, 498, 309, 411, 3255, 577, 281, 411, 605, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21166061532908473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 0.3309522271156311}, {"id": 1642, "seek": 1496274, "start": 14966.74, "end": 14968.74, "text": " No.", "tokens": [50564, 883, 13, 50664], "temperature": 0.0, "avg_logprob": -0.21166061532908473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 0.3309522271156311}, {"id": 1643, "seek": 1496274, "start": 14968.74, "end": 14970.74, "text": " Whoa.", "tokens": [50664, 7521, 13, 50764], "temperature": 0.0, "avg_logprob": -0.21166061532908473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 0.3309522271156311}, {"id": 1644, "seek": 1496274, "start": 14970.74, "end": 14972.74, "text": " I don't know if we have Selenium.", "tokens": [50764, 286, 500, 380, 458, 498, 321, 362, 10736, 268, 2197, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21166061532908473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 0.3309522271156311}, {"id": 1645, "seek": 1496274, "start": 14972.74, "end": 14981.74, "text": " Let's see if we have Selenium.", "tokens": [50864, 961, 311, 536, 498, 321, 362, 10736, 268, 2197, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21166061532908473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 0.3309522271156311}, {"id": 1646, "seek": 1496274, "start": 14981.74, "end": 14990.74, "text": " Can I pip install this?", "tokens": [51314, 1664, 286, 8489, 3625, 341, 30, 51764], "temperature": 0.0, "avg_logprob": -0.21166061532908473, "compression_ratio": 1.3181818181818181, "no_speech_prob": 0.3309522271156311}, {"id": 1647, "seek": 1499074, "start": 14990.74, "end": 15004.74, "text": " This is legit.", "tokens": [50364, 639, 307, 10275, 13, 51064], "temperature": 0.0, "avg_logprob": -0.5599426542009626, "compression_ratio": 0.7857142857142857, "no_speech_prob": 0.19922828674316406}, {"id": 1648, "seek": 1499074, "start": 15004.74, "end": 15006.74, "text": " Oh, no.", "tokens": [51064, 876, 11, 572, 13, 51164], "temperature": 0.0, "avg_logprob": -0.5599426542009626, "compression_ratio": 0.7857142857142857, "no_speech_prob": 0.19922828674316406}, {"id": 1649, "seek": 1500674, "start": 15006.74, "end": 15028.74, "text": " Chrome driver.", "tokens": [50364, 15327, 6787, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3539293233086081, "compression_ratio": 0.864406779661017, "no_speech_prob": 0.11912530660629272}, {"id": 1650, "seek": 1500674, "start": 15028.74, "end": 15031.74, "text": " Made Quentin pip install it himself.", "tokens": [51464, 18330, 2326, 47300, 8489, 3625, 309, 3647, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3539293233086081, "compression_ratio": 0.864406779661017, "no_speech_prob": 0.11912530660629272}, {"id": 1651, "seek": 1503174, "start": 15031.74, "end": 15052.74, "text": " I think that the thing is just wrong.", "tokens": [50364, 286, 519, 300, 264, 551, 307, 445, 2085, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2115348685871471, "compression_ratio": 1.0, "no_speech_prob": 0.079165518283844}, {"id": 1652, "seek": 1503174, "start": 15052.74, "end": 15056.74, "text": " Why are you finding sneakers still?", "tokens": [51414, 1545, 366, 291, 5006, 35331, 920, 30, 51614], "temperature": 0.0, "avg_logprob": -0.2115348685871471, "compression_ratio": 1.0, "no_speech_prob": 0.079165518283844}, {"id": 1653, "seek": 1505674, "start": 15056.74, "end": 15074.74, "text": " You're still trying to find sneakers.", "tokens": [50364, 509, 434, 920, 1382, 281, 915, 35331, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16971451044082642, "compression_ratio": 0.8222222222222222, "no_speech_prob": 0.08624894171953201}, {"id": 1654, "seek": 1507474, "start": 15074.74, "end": 15096.74, "text": " This is auto-gen.", "tokens": [50364, 639, 307, 8399, 12, 1766, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3068741798400879, "compression_ratio": 0.7391304347826086, "no_speech_prob": 0.7515226006507874}, {"id": 1655, "seek": 1509674, "start": 15096.74, "end": 15117.74, "text": " I need an open AI key.", "tokens": [50364, 286, 643, 364, 1269, 7318, 2141, 13, 51414], "temperature": 0.0, "avg_logprob": -0.5094773251077404, "compression_ratio": 0.975609756097561, "no_speech_prob": 0.6362656950950623}, {"id": 1656, "seek": 1509674, "start": 15117.74, "end": 15119.74, "text": " Yeah.", "tokens": [51414, 865, 13, 51514], "temperature": 0.0, "avg_logprob": -0.5094773251077404, "compression_ratio": 0.975609756097561, "no_speech_prob": 0.6362656950950623}, {"id": 1657, "seek": 1509674, "start": 15119.74, "end": 15121.74, "text": " Yeah.", "tokens": [51514, 865, 13, 51614], "temperature": 0.0, "avg_logprob": -0.5094773251077404, "compression_ratio": 0.975609756097561, "no_speech_prob": 0.6362656950950623}, {"id": 1658, "seek": 1509674, "start": 15121.74, "end": 15123.74, "text": " Okay.", "tokens": [51614, 1033, 13, 51714], "temperature": 0.0, "avg_logprob": -0.5094773251077404, "compression_ratio": 0.975609756097561, "no_speech_prob": 0.6362656950950623}, {"id": 1659, "seek": 1512374, "start": 15123.74, "end": 15133.74, "text": " Push what we have.", "tokens": [50364, 18229, 437, 321, 362, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21733536720275878, "compression_ratio": 1.120879120879121, "no_speech_prob": 0.6241731643676758}, {"id": 1660, "seek": 1512374, "start": 15133.74, "end": 15148.74, "text": " I'm also going to demo for you the conversation thing that Skull Mag is working on.", "tokens": [50864, 286, 478, 611, 516, 281, 10723, 337, 291, 264, 3761, 551, 300, 7324, 858, 6395, 307, 1364, 322, 13, 51614], "temperature": 0.0, "avg_logprob": -0.21733536720275878, "compression_ratio": 1.120879120879121, "no_speech_prob": 0.6241731643676758}, {"id": 1661, "seek": 1514874, "start": 15148.74, "end": 15153.74, "text": " So we have, there's a bug right now.", "tokens": [50364, 407, 321, 362, 11, 456, 311, 257, 7426, 558, 586, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14191229049473592, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.4795999228954315}, {"id": 1662, "seek": 1514874, "start": 15153.74, "end": 15159.74, "text": " So I have an older version, but it should be pretty good.", "tokens": [50614, 407, 286, 362, 364, 4906, 3037, 11, 457, 309, 820, 312, 1238, 665, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14191229049473592, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.4795999228954315}, {"id": 1663, "seek": 1514874, "start": 15159.74, "end": 15176.74, "text": " This is using tiny llama and it's not using any of the conversational stuff, but we should implement the, we should add the conversational stuff and I think it'll be a lot better.", "tokens": [50914, 639, 307, 1228, 5870, 23272, 293, 309, 311, 406, 1228, 604, 295, 264, 2615, 1478, 1507, 11, 457, 321, 820, 4445, 264, 11, 321, 820, 909, 264, 2615, 1478, 1507, 293, 286, 519, 309, 603, 312, 257, 688, 1101, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14191229049473592, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.4795999228954315}, {"id": 1664, "seek": 1517674, "start": 15176.74, "end": 15177.74, "text": " Hi, Stacey.", "tokens": [50364, 2421, 11, 745, 29059, 13, 50414], "temperature": 0.0, "avg_logprob": -0.20165119796502787, "compression_ratio": 1.2403100775193798, "no_speech_prob": 0.5347559452056885}, {"id": 1665, "seek": 1517674, "start": 15177.74, "end": 15186.74, "text": " Are you a rapper?", "tokens": [50414, 2014, 291, 257, 26457, 30, 50864], "temperature": 0.0, "avg_logprob": -0.20165119796502787, "compression_ratio": 1.2403100775193798, "no_speech_prob": 0.5347559452056885}, {"id": 1666, "seek": 1517674, "start": 15186.74, "end": 15187.74, "text": " No.", "tokens": [50864, 883, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20165119796502787, "compression_ratio": 1.2403100775193798, "no_speech_prob": 0.5347559452056885}, {"id": 1667, "seek": 1517674, "start": 15187.74, "end": 15188.74, "text": " Okay.", "tokens": [50914, 1033, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20165119796502787, "compression_ratio": 1.2403100775193798, "no_speech_prob": 0.5347559452056885}, {"id": 1668, "seek": 1517674, "start": 15188.74, "end": 15189.74, "text": " We have the same problem we had before.", "tokens": [50964, 492, 362, 264, 912, 1154, 321, 632, 949, 13, 51014], "temperature": 0.0, "avg_logprob": -0.20165119796502787, "compression_ratio": 1.2403100775193798, "no_speech_prob": 0.5347559452056885}, {"id": 1669, "seek": 1517674, "start": 15189.74, "end": 15190.74, "text": " Okay.", "tokens": [51014, 1033, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20165119796502787, "compression_ratio": 1.2403100775193798, "no_speech_prob": 0.5347559452056885}, {"id": 1670, "seek": 1517674, "start": 15190.74, "end": 15191.74, "text": " Go back to this one.", "tokens": [51064, 1037, 646, 281, 341, 472, 13, 51114], "temperature": 0.0, "avg_logprob": -0.20165119796502787, "compression_ratio": 1.2403100775193798, "no_speech_prob": 0.5347559452056885}, {"id": 1671, "seek": 1517674, "start": 15191.74, "end": 15199.74, "text": " The listen for not fixed amount of time doesn't work.", "tokens": [51114, 440, 2140, 337, 406, 6806, 2372, 295, 565, 1177, 380, 589, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20165119796502787, "compression_ratio": 1.2403100775193798, "no_speech_prob": 0.5347559452056885}, {"id": 1672, "seek": 1519974, "start": 15199.74, "end": 15206.74, "text": " Stacey, are you a rapper?", "tokens": [50364, 745, 29059, 11, 366, 291, 257, 26457, 30, 50714], "temperature": 0.0, "avg_logprob": -0.10891349387891365, "compression_ratio": 1.384, "no_speech_prob": 0.5991730093955994}, {"id": 1673, "seek": 1519974, "start": 15206.74, "end": 15208.74, "text": " Yes, I'm a rapper.", "tokens": [50714, 1079, 11, 286, 478, 257, 26457, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10891349387891365, "compression_ratio": 1.384, "no_speech_prob": 0.5991730093955994}, {"id": 1674, "seek": 1519974, "start": 15208.74, "end": 15209.74, "text": " That's cool.", "tokens": [50814, 663, 311, 1627, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10891349387891365, "compression_ratio": 1.384, "no_speech_prob": 0.5991730093955994}, {"id": 1675, "seek": 1519974, "start": 15209.74, "end": 15214.74, "text": " What do you rap about?", "tokens": [50864, 708, 360, 291, 5099, 466, 30, 51114], "temperature": 0.0, "avg_logprob": -0.10891349387891365, "compression_ratio": 1.384, "no_speech_prob": 0.5991730093955994}, {"id": 1676, "seek": 1519974, "start": 15214.74, "end": 15215.74, "text": " That's awesome.", "tokens": [51114, 663, 311, 3476, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10891349387891365, "compression_ratio": 1.384, "no_speech_prob": 0.5991730093955994}, {"id": 1677, "seek": 1519974, "start": 15215.74, "end": 15217.74, "text": " I like to rap about the weather.", "tokens": [51164, 286, 411, 281, 5099, 466, 264, 5503, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10891349387891365, "compression_ratio": 1.384, "no_speech_prob": 0.5991730093955994}, {"id": 1678, "seek": 1519974, "start": 15217.74, "end": 15223.74, "text": " How is the weather today?", "tokens": [51264, 1012, 307, 264, 5503, 965, 30, 51564], "temperature": 0.0, "avg_logprob": -0.10891349387891365, "compression_ratio": 1.384, "no_speech_prob": 0.5991730093955994}, {"id": 1679, "seek": 1519974, "start": 15223.74, "end": 15225.74, "text": " It's pretty cold.", "tokens": [51564, 467, 311, 1238, 3554, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10891349387891365, "compression_ratio": 1.384, "no_speech_prob": 0.5991730093955994}, {"id": 1680, "seek": 1522574, "start": 15225.74, "end": 15230.74, "text": " It's like how cold like Chicago?", "tokens": [50364, 467, 311, 411, 577, 3554, 411, 9525, 30, 50614], "temperature": 0.0, "avg_logprob": -0.1343021677501166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.21984823048114777}, {"id": 1681, "seek": 1522574, "start": 15230.74, "end": 15232.74, "text": " Yeah, it's like three degrees.", "tokens": [50614, 865, 11, 309, 311, 411, 1045, 5310, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1343021677501166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.21984823048114777}, {"id": 1682, "seek": 1522574, "start": 15232.74, "end": 15238.74, "text": " Is that Fahrenheit or Celsius?", "tokens": [50714, 1119, 300, 31199, 420, 22658, 30, 51014], "temperature": 0.0, "avg_logprob": -0.1343021677501166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.21984823048114777}, {"id": 1683, "seek": 1522574, "start": 15238.74, "end": 15240.74, "text": " Fahrenheit.", "tokens": [51014, 31199, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1343021677501166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.21984823048114777}, {"id": 1684, "seek": 1522574, "start": 15240.74, "end": 15245.74, "text": " Is Fahrenheit or Celsius colder?", "tokens": [51114, 1119, 31199, 420, 22658, 31020, 30, 51364], "temperature": 0.0, "avg_logprob": -0.1343021677501166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.21984823048114777}, {"id": 1685, "seek": 1522574, "start": 15245.74, "end": 15247.74, "text": " Fahrenheit.", "tokens": [51364, 31199, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1343021677501166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.21984823048114777}, {"id": 1686, "seek": 1522574, "start": 15247.74, "end": 15248.74, "text": " I don't understand what you're saying.", "tokens": [51464, 286, 500, 380, 1223, 437, 291, 434, 1566, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1343021677501166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.21984823048114777}, {"id": 1687, "seek": 1522574, "start": 15248.74, "end": 15254.74, "text": " Can you spit some bars about Fahrenheit?", "tokens": [51514, 1664, 291, 22127, 512, 10228, 466, 31199, 30, 51814], "temperature": 0.0, "avg_logprob": -0.1343021677501166, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.21984823048114777}, {"id": 1688, "seek": 1525474, "start": 15254.74, "end": 15262.74, "text": " Okay, let's hear the bars.", "tokens": [50364, 1033, 11, 718, 311, 1568, 264, 10228, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13587310579088, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.02757723443210125}, {"id": 1689, "seek": 1525474, "start": 15262.74, "end": 15264.74, "text": " Stacey, you didn't say anything.", "tokens": [50764, 745, 29059, 11, 291, 994, 380, 584, 1340, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13587310579088, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.02757723443210125}, {"id": 1690, "seek": 1525474, "start": 15264.74, "end": 15268.74, "text": " Please talk.", "tokens": [50864, 2555, 751, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13587310579088, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.02757723443210125}, {"id": 1691, "seek": 1525474, "start": 15268.74, "end": 15269.74, "text": " No, that's terrible.", "tokens": [51064, 883, 11, 300, 311, 6237, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13587310579088, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.02757723443210125}, {"id": 1692, "seek": 1525474, "start": 15269.74, "end": 15270.74, "text": " You're terrible.", "tokens": [51114, 509, 434, 6237, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13587310579088, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.02757723443210125}, {"id": 1693, "seek": 1525474, "start": 15270.74, "end": 15276.74, "text": " How does that make you feel?", "tokens": [51164, 1012, 775, 300, 652, 291, 841, 30, 51464], "temperature": 0.0, "avg_logprob": -0.13587310579088, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.02757723443210125}, {"id": 1694, "seek": 1525474, "start": 15276.74, "end": 15280.74, "text": " Stacey's done talking to us.", "tokens": [51464, 745, 29059, 311, 1096, 1417, 281, 505, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13587310579088, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.02757723443210125}, {"id": 1695, "seek": 1525474, "start": 15280.74, "end": 15283.74, "text": " Dude, the TTS is so fast.", "tokens": [51664, 12042, 11, 264, 314, 7327, 307, 370, 2370, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13587310579088, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.02757723443210125}, {"id": 1696, "seek": 1528374, "start": 15283.74, "end": 15287.74, "text": " Okay, this isn't even streaming yet.", "tokens": [50364, 1033, 11, 341, 1943, 380, 754, 11791, 1939, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1386860535208103, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.08137862384319305}, {"id": 1697, "seek": 1528374, "start": 15287.74, "end": 15292.74, "text": " When Tiny Grad starts to...", "tokens": [50564, 1133, 39992, 16710, 3719, 281, 485, 50814], "temperature": 0.0, "avg_logprob": -0.1386860535208103, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.08137862384319305}, {"id": 1698, "seek": 1528374, "start": 15292.74, "end": 15294.74, "text": " We're pretty close on this bounty, I think.", "tokens": [50814, 492, 434, 1238, 1998, 322, 341, 40773, 11, 286, 519, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1386860535208103, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.08137862384319305}, {"id": 1699, "seek": 1528374, "start": 15294.74, "end": 15295.74, "text": " I'm going to pay out the bounty,", "tokens": [50914, 286, 478, 516, 281, 1689, 484, 264, 40773, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1386860535208103, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.08137862384319305}, {"id": 1700, "seek": 1528374, "start": 15295.74, "end": 15297.74, "text": " but then I'm going to offer another bounty", "tokens": [50964, 457, 550, 286, 478, 516, 281, 2626, 1071, 40773, 51064], "temperature": 0.0, "avg_logprob": -0.1386860535208103, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.08137862384319305}, {"id": 1701, "seek": 1528374, "start": 15297.74, "end": 15300.74, "text": " where we get these things all to stream,", "tokens": [51064, 689, 321, 483, 613, 721, 439, 281, 4309, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1386860535208103, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.08137862384319305}, {"id": 1702, "seek": 1528374, "start": 15300.74, "end": 15304.74, "text": " and it will be a live conversation.", "tokens": [51214, 293, 309, 486, 312, 257, 1621, 3761, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1386860535208103, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.08137862384319305}, {"id": 1703, "seek": 1528374, "start": 15304.74, "end": 15307.74, "text": " When you're using the APIs on the internet,", "tokens": [51414, 1133, 291, 434, 1228, 264, 21445, 322, 264, 4705, 11, 51564], "temperature": 0.0, "avg_logprob": -0.1386860535208103, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.08137862384319305}, {"id": 1704, "seek": 1528374, "start": 15307.74, "end": 15310.74, "text": " you have to wait for the LLM to finish executing", "tokens": [51564, 291, 362, 281, 1699, 337, 264, 441, 43, 44, 281, 2413, 32368, 51714], "temperature": 0.0, "avg_logprob": -0.1386860535208103, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.08137862384319305}, {"id": 1705, "seek": 1528374, "start": 15310.74, "end": 15312.74, "text": " before you can call the TTS.", "tokens": [51714, 949, 291, 393, 818, 264, 314, 7327, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1386860535208103, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.08137862384319305}, {"id": 1706, "seek": 1531274, "start": 15312.74, "end": 15314.74, "text": " You have to wait for the audio to finish recording", "tokens": [50364, 509, 362, 281, 1699, 337, 264, 6278, 281, 2413, 6613, 50464], "temperature": 0.0, "avg_logprob": -0.10758074194984099, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00781179079785943}, {"id": 1707, "seek": 1531274, "start": 15314.74, "end": 15316.74, "text": " before you can send it to the service.", "tokens": [50464, 949, 291, 393, 2845, 309, 281, 264, 2643, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10758074194984099, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00781179079785943}, {"id": 1708, "seek": 1531274, "start": 15316.74, "end": 15318.74, "text": " This is all running in the same process,", "tokens": [50564, 639, 307, 439, 2614, 294, 264, 912, 1399, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10758074194984099, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00781179079785943}, {"id": 1709, "seek": 1531274, "start": 15318.74, "end": 15321.74, "text": " so what you'll be able to do is dynamically stream", "tokens": [50664, 370, 437, 291, 603, 312, 1075, 281, 360, 307, 43492, 4309, 50814], "temperature": 0.0, "avg_logprob": -0.10758074194984099, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00781179079785943}, {"id": 1710, "seek": 1531274, "start": 15321.74, "end": 15325.74, "text": " all the stuff, and it should feel super real-time.", "tokens": [50814, 439, 264, 1507, 11, 293, 309, 820, 841, 1687, 957, 12, 3766, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10758074194984099, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00781179079785943}, {"id": 1711, "seek": 1531274, "start": 15325.74, "end": 15328.74, "text": " I mean, Stacey is using Tiny LLMA", "tokens": [51014, 286, 914, 11, 745, 29059, 307, 1228, 39992, 441, 43, 9998, 51164], "temperature": 0.0, "avg_logprob": -0.10758074194984099, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00781179079785943}, {"id": 1712, "seek": 1531274, "start": 15328.74, "end": 15330.74, "text": " and not using any of the conversation-tuned stuff.", "tokens": [51164, 293, 406, 1228, 604, 295, 264, 3761, 12, 83, 43703, 1507, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10758074194984099, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00781179079785943}, {"id": 1713, "seek": 1531274, "start": 15330.74, "end": 15332.74, "text": " It's using my old chatbot stuff.", "tokens": [51264, 467, 311, 1228, 452, 1331, 5081, 18870, 1507, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10758074194984099, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00781179079785943}, {"id": 1714, "seek": 1531274, "start": 15332.74, "end": 15336.74, "text": " So if we switch to the conversation stuff,", "tokens": [51364, 407, 498, 321, 3679, 281, 264, 3761, 1507, 11, 51564], "temperature": 0.0, "avg_logprob": -0.10758074194984099, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.00781179079785943}, {"id": 1715, "seek": 1533674, "start": 15336.74, "end": 15338.74, "text": " I think...", "tokens": [50364, 286, 519, 485, 50464], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1716, "seek": 1533674, "start": 15338.74, "end": 15344.74, "text": " Yeah, we're in luck.", "tokens": [50464, 865, 11, 321, 434, 294, 3668, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1717, "seek": 1533674, "start": 15344.74, "end": 15345.74, "text": " All right, guys.", "tokens": [50764, 1057, 558, 11, 1074, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1718, "seek": 1533674, "start": 15345.74, "end": 15348.74, "text": " Thank you for watching today's stream.", "tokens": [50814, 1044, 291, 337, 1976, 965, 311, 4309, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1719, "seek": 1533674, "start": 15348.74, "end": 15352.74, "text": " Hopefully we've returned a bit to the old...", "tokens": [50964, 10429, 321, 600, 8752, 257, 857, 281, 264, 1331, 485, 51164], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1720, "seek": 1533674, "start": 15352.74, "end": 15355.74, "text": " to the old meaning of the stream.", "tokens": [51164, 281, 264, 1331, 3620, 295, 264, 4309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1721, "seek": 1533674, "start": 15355.74, "end": 15356.74, "text": " We did stuff.", "tokens": [51314, 492, 630, 1507, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1722, "seek": 1533674, "start": 15356.74, "end": 15359.74, "text": " We made stuff happen.", "tokens": [51364, 492, 1027, 1507, 1051, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1723, "seek": 1533674, "start": 15359.74, "end": 15361.74, "text": " Thank you for fueling.", "tokens": [51514, 1044, 291, 337, 6616, 278, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1724, "seek": 1533674, "start": 15361.74, "end": 15363.74, "text": " We got a lot of viewers today.", "tokens": [51614, 492, 658, 257, 688, 295, 8499, 965, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1725, "seek": 1533674, "start": 15363.74, "end": 15365.74, "text": " Some of you, I appreciate.", "tokens": [51714, 2188, 295, 291, 11, 286, 4449, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15818096963982833, "compression_ratio": 1.5133689839572193, "no_speech_prob": 0.11746205389499664}, {"id": 1726, "seek": 1536574, "start": 15365.74, "end": 15366.74, "text": " Some of you, I probably don't.", "tokens": [50364, 2188, 295, 291, 11, 286, 1391, 500, 380, 13, 50414], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1727, "seek": 1536574, "start": 15366.74, "end": 15367.74, "text": " You can't love everybody, man.", "tokens": [50414, 509, 393, 380, 959, 2201, 11, 587, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1728, "seek": 1536574, "start": 15367.74, "end": 15370.74, "text": " You can't love everybody, but I do love most people,", "tokens": [50464, 509, 393, 380, 959, 2201, 11, 457, 286, 360, 959, 881, 561, 11, 50614], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1729, "seek": 1536574, "start": 15370.74, "end": 15374.74, "text": " and that's true, except for the decels and the effect of altruists.", "tokens": [50614, 293, 300, 311, 2074, 11, 3993, 337, 264, 979, 1625, 293, 264, 1802, 295, 4955, 894, 1751, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1730, "seek": 1536574, "start": 15374.74, "end": 15376.74, "text": " But this is a positive stream.", "tokens": [50814, 583, 341, 307, 257, 3353, 4309, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1731, "seek": 1536574, "start": 15376.74, "end": 15378.74, "text": " We got to get rid of the hate.", "tokens": [50914, 492, 658, 281, 483, 3973, 295, 264, 4700, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1732, "seek": 1536574, "start": 15378.74, "end": 15383.74, "text": " We got to bring love.", "tokens": [51014, 492, 658, 281, 1565, 959, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1733, "seek": 1536574, "start": 15383.74, "end": 15387.74, "text": " And yeah, this is pushed", "tokens": [51264, 400, 1338, 11, 341, 307, 9152, 51464], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1734, "seek": 1536574, "start": 15387.74, "end": 15389.74, "text": " so everybody can play with it.", "tokens": [51464, 370, 2201, 393, 862, 365, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1735, "seek": 1536574, "start": 15389.74, "end": 15391.74, "text": " It's on the Mistral branch of TinyGrad.", "tokens": [51564, 467, 311, 322, 264, 20166, 2155, 9819, 295, 39992, 38, 6206, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1736, "seek": 1536574, "start": 15391.74, "end": 15394.74, "text": " I will get it upstreamed.", "tokens": [51664, 286, 486, 483, 309, 33915, 292, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11095141607617574, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.07573169469833374}, {"id": 1737, "seek": 1539474, "start": 15394.74, "end": 15397.74, "text": " So everybody can use this thing to code,", "tokens": [50364, 407, 2201, 393, 764, 341, 551, 281, 3089, 11, 50514], "temperature": 0.0, "avg_logprob": -0.10150850028322454, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.013414626941084862}, {"id": 1738, "seek": 1539474, "start": 15397.74, "end": 15401.74, "text": " use it responsibly, make sure to be judicious", "tokens": [50514, 764, 309, 2914, 3545, 11, 652, 988, 281, 312, 3747, 3784, 50714], "temperature": 0.0, "avg_logprob": -0.10150850028322454, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.013414626941084862}, {"id": 1739, "seek": 1539474, "start": 15401.74, "end": 15403.74, "text": " with the AI safety feature.", "tokens": [50714, 365, 264, 7318, 4514, 4111, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10150850028322454, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.013414626941084862}, {"id": 1740, "seek": 1539474, "start": 15403.74, "end": 15405.74, "text": " AI safety is very important.", "tokens": [50814, 7318, 4514, 307, 588, 1021, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10150850028322454, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.013414626941084862}, {"id": 1741, "seek": 1539474, "start": 15405.74, "end": 15409.74, "text": " We don't want an AI removing your system 32 directory.", "tokens": [50914, 492, 500, 380, 528, 364, 7318, 12720, 428, 1185, 8858, 21120, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10150850028322454, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.013414626941084862}, {"id": 1742, "seek": 1539474, "start": 15409.74, "end": 15410.74, "text": " If it was trained on 4chan,", "tokens": [51114, 759, 309, 390, 8895, 322, 1017, 3484, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10150850028322454, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.013414626941084862}, {"id": 1743, "seek": 1539474, "start": 15410.74, "end": 15412.74, "text": " it might start thinking that's a good idea.", "tokens": [51164, 309, 1062, 722, 1953, 300, 311, 257, 665, 1558, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10150850028322454, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.013414626941084862}, {"id": 1744, "seek": 1539474, "start": 15412.74, "end": 15417.74, "text": " You got to wonder how many people have actually fallen for that.", "tokens": [51264, 509, 658, 281, 2441, 577, 867, 561, 362, 767, 11547, 337, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10150850028322454, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.013414626941084862}, {"id": 1745, "seek": 1539474, "start": 15417.74, "end": 15420.74, "text": " I don't even think Windows lets you,", "tokens": [51514, 286, 500, 380, 754, 519, 8591, 6653, 291, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10150850028322454, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.013414626941084862}, {"id": 1746, "seek": 1539474, "start": 15420.74, "end": 15422.74, "text": " but you got to think about that.", "tokens": [51664, 457, 291, 658, 281, 519, 466, 300, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10150850028322454, "compression_ratio": 1.5637065637065637, "no_speech_prob": 0.013414626941084862}, {"id": 1747, "seek": 1542274, "start": 15422.74, "end": 15424.74, "text": " Thank you all for watching.", "tokens": [50364, 1044, 291, 439, 337, 1976, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10208157036039564, "compression_ratio": 0.9285714285714286, "no_speech_prob": 0.1412971317768097}, {"id": 1748, "seek": 1542274, "start": 15424.74, "end": 15426.74, "text": " Have a beautiful Saturday, everybody.", "tokens": [50464, 3560, 257, 2238, 8803, 11, 2201, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10208157036039564, "compression_ratio": 0.9285714285714286, "no_speech_prob": 0.1412971317768097}], "language": "en"}