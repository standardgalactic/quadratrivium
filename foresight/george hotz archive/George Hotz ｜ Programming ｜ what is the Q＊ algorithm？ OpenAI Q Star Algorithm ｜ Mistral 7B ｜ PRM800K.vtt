WEBVTT

00:00.000 --> 00:26.900
Good morning everybody. Good morning. Good morning. It's nice to be back in my house.

00:26.900 --> 00:40.900
I don't like streaming at work.

00:40.900 --> 00:48.900
High ball energy.

00:48.900 --> 01:07.900
Good morning everybody. Good morning. Good morning. Good morning. Good morning.

01:07.900 --> 01:12.900
Let's shrink me down here... me up there. Oops, don't do that.

01:12.900 --> 01:16.900
Okay, are we ready to get started?

01:20.460 --> 01:23.580
Yeah, I felt the vibes were a little bit off

01:23.580 --> 01:24.980
in yesterday's stream.

01:27.180 --> 01:29.900
I don't think I should stream at the office.

01:29.900 --> 01:31.980
Oh, so I didn't like the way that like,

01:31.980 --> 01:33.820
like now this is here,

01:33.820 --> 01:36.640
I'm gonna slide that down a little,

01:37.500 --> 01:40.100
about in the middle, that's pretty nice.

01:40.100 --> 01:42.040
I got my Yeti microphone.

01:43.020 --> 01:45.100
Move it over a little bit.

01:45.100 --> 01:47.620
Yeti puts me perfectly in the center.

01:49.580 --> 01:51.780
Yeah, vibes are off.

01:51.780 --> 01:53.820
Wouldn't have my lighting set off.

01:55.060 --> 01:56.460
I mean, it's an okay stream.

01:59.020 --> 02:01.540
It's good to focus on TinyGrad.

02:01.540 --> 02:05.740
But some guy said that he liked my older work better.

02:05.740 --> 02:09.020
And if there's one thing I never want to be, it's drink.

02:09.580 --> 02:12.060
So, we're gonna try to,

02:12.060 --> 02:14.700
we're gonna try to bring it back to the old stuff.

02:15.740 --> 02:16.740
And we're gonna do,

02:18.820 --> 02:21.780
we're gonna investigate the Q-star algorithm.

02:21.780 --> 02:24.060
So first off, what is the Q-star algorithm?

02:24.060 --> 02:25.460
I don't know, let's find out.

02:34.740 --> 02:37.020
We'll start using our friend here, Google.

02:38.020 --> 02:41.740
What is Q-star and when will we hear more?

02:41.740 --> 02:44.140
Someone who's done a fair amount of ML research.

02:44.140 --> 02:46.020
I can tell you, it's very, very easy to think

02:46.020 --> 02:47.060
you've discovered a breakthrough.

02:47.060 --> 02:47.900
That's true.

02:49.420 --> 02:52.580
All right, let's read this article in The Verge.

02:56.540 --> 03:00.420
Reuters, okay, well the government, no, no, no, no, no, no.

03:07.020 --> 03:07.860
Okay.

03:20.660 --> 03:24.260
Q-star could be a breakthrough in the startup search

03:24.260 --> 03:27.500
for what's known as artificial general intelligence.

03:30.580 --> 03:32.100
Reuters could not,

03:37.020 --> 03:40.460
could not be a breakthrough in the startup search for what's

03:40.460 --> 03:42.780
known as artificial general intelligence.

03:50.780 --> 03:52.940
Okay, it somehow does math.

04:07.340 --> 04:09.340
Does something have to publish papers?

04:17.540 --> 04:18.700
Not really.

04:27.300 --> 04:28.660
Wow.

04:28.660 --> 04:31.380
Remember when they used to publish papers

04:31.380 --> 04:34.900
and now they publish system cards and,

04:35.380 --> 04:37.060
safety and alignment?

04:40.220 --> 04:41.780
Okay, wait, wait, wait, wait, wait, wait, wait,

04:41.780 --> 04:41.820
wait, wait, wait, wait, wait, wait, wait, wait, wait, wait,

04:41.820 --> 04:42.860
wait, there's some hope here.

04:42.860 --> 04:45.120
Improving mathematical reasoning.

04:46.540 --> 04:49.980
Ooh, ooh, ooh, this sounds like Q-star guys.

04:49.980 --> 04:51.180
I think we found it.

04:54.840 --> 04:56.700
No, we don't, no, no, but really, wait, is this not,

04:56.700 --> 04:58.080
is this not Q-star?

04:58.080 --> 04:59.220
Did we not just find it?

05:02.700 --> 05:03.660
We're trying not to hate.

05:03.660 --> 05:04.820
We're trying not to hate guys.

05:04.820 --> 05:10.980
Solitate in the world. We got to bring law of impositivity and shit man. Whatever like John Lennon said, you know something

05:15.980 --> 05:19.920
That's why we're gonna go subscribers only

05:24.980 --> 05:26.980
Okay

05:34.920 --> 05:45.520
Given vast computing resources the new model was able to solve certain mathematical problems

05:47.140 --> 05:50.700
Improving mathematical reasoning with process supervision

05:51.460 --> 05:59.140
We've trained a model to achieve a new state of the art in mathematical problem-solving by rewarding each correct step of reasoning instead of

05:59.780 --> 06:01.780
Simply rewarding the final answer

06:05.660 --> 06:07.660
And just

06:11.180 --> 06:13.180
Yeah, yeah

06:16.980 --> 06:18.980
Okay, so

06:19.140 --> 06:25.040
The reason that you usually just reward the correct final answer is because that's all that's in your

06:26.340 --> 06:33.780
Data set the other problem with doing this is it may constrain the reasoning to follow one certain path when that may not be the path

06:33.780 --> 06:35.780
You have to follow

06:39.260 --> 06:43.540
State of the art models still produce logical mistakes often called hallucinations

06:54.620 --> 06:57.720
Cutting out we have more technical problems

06:58.440 --> 07:04.480
I bought a new computer. Does new Apple stuff not work only you okay Rick and Morty you have a crappy internet

07:12.640 --> 07:17.820
An internet connection that is less good than other internet connections

07:27.960 --> 07:29.960
You

07:35.360 --> 07:39.680
Wait, just opening I just published like fake papers now like this

07:50.120 --> 07:52.120
Don't do a shit about alignment

07:52.320 --> 07:54.320
and

07:57.120 --> 08:04.680
Wait, this is a paper. Oh, you know here does a paper. Let's verify step by step

08:22.120 --> 08:24.120
You

08:40.840 --> 08:42.840
Okay, well, is this a public data set

08:47.360 --> 08:49.360
All right, all right, all right, cool

08:52.120 --> 08:54.120
You

09:04.720 --> 09:07.480
Math word problem-solving on math

09:16.320 --> 09:18.320
Who made this data set

09:20.040 --> 09:22.040
Okay, okay

09:22.800 --> 09:29.720
This is not where I was expecting this to go by the way. Oh, they have metamath. Oh everything comes full circle

09:33.880 --> 09:41.560
No, I mean again, let's okay, let's find the primary source here

09:47.200 --> 09:49.200
Q star

09:53.120 --> 09:55.120
I

09:55.240 --> 09:57.240
Definitely talked about math problems

10:00.320 --> 10:04.080
Only performing math on the level of grade school students

10:10.680 --> 10:14.240
Why is this monitor broken this monitor is like broken I can do my

10:22.360 --> 10:26.520
You guys aren't seeing that glitching right

10:35.960 --> 10:39.960
Are you maybe you are I don't think you are

10:45.240 --> 10:47.240
This is so weird

10:48.200 --> 10:52.920
When I click here, there's like a blue glitching on my computer

10:56.880 --> 10:59.640
Weirdest thing like how does that even happen I

11:02.320 --> 11:04.680
Could take the camera down and show you you want to see

11:05.720 --> 11:10.080
There's like blue glitching on my monitor. Look at this click here goes away

11:10.800 --> 11:13.800
Here glitching here goes away glitching

11:14.800 --> 11:17.960
How's this happening they've never seen that before

11:23.080 --> 11:25.080
Sit

11:26.880 --> 11:28.880
Ghosted image

11:31.120 --> 11:34.680
Maybe it's the cable I don't know right

11:43.800 --> 11:46.880
Who knows it only happens when things are uncertain

11:54.280 --> 11:56.280
Wait, this is just really

12:05.480 --> 12:07.480
Who knows

12:08.480 --> 12:10.480
Oh

12:13.360 --> 12:18.480
Wait, I wanted to confirm that this actually was related to math

12:26.240 --> 12:33.720
Delstick cable breaks I'm using like the USB-C cable it could be the cable

12:37.480 --> 12:39.720
But like it's weird how it's glitching

12:44.720 --> 12:46.720
I don't know

12:49.320 --> 12:54.600
The new model was able to solve certain mathematical problems, so it kind of makes sense that this is the data set right

13:00.480 --> 13:02.480
Do they mention the math data here

13:03.160 --> 13:10.320
Our process supervised model solves 78% of the problems from a representative subset of the math test set

13:15.640 --> 13:18.920
We also release PRM 800 K

13:20.600 --> 13:22.600
Okay, opening I release something look at that

13:24.600 --> 13:26.600
No positive positive

13:32.480 --> 13:34.480
I

13:41.640 --> 13:43.640
Drive to VS codes

13:58.160 --> 14:01.320
Let's also look into what their other github activities been

14:02.480 --> 14:04.480
I

14:22.200 --> 14:25.440
Procedurally generated game like Jim environments

14:33.360 --> 14:43.560
What did they update in GPT to a branch they updated

14:43.560 --> 14:45.560
I

15:00.200 --> 15:03.320
Quantifying transfer and reinforcement learning

15:07.520 --> 15:10.760
Okay, it seems like Carl Cobb has been

15:11.760 --> 15:14.720
If anyone invented Q star, it's Carl Cobb

15:18.800 --> 15:20.800
Carl Cobb

15:27.920 --> 15:33.680
All right, you've been working on this math stuff for a while. Let's see what data set was used here. Oh

15:37.880 --> 15:40.000
Is this the data set that introduced that

15:41.600 --> 15:43.600
Oh

15:43.600 --> 15:48.920
This is a different one gms 8k PRM 800 K

15:52.160 --> 15:54.160
Step level correctness

15:59.840 --> 16:01.840
The glitching is really bad

16:10.760 --> 16:12.760
You

16:30.000 --> 16:36.520
Okay, wait, wait, let's talk about the glitching for a minute somehow you can see what's here

16:37.440 --> 16:42.520
Right and it's this it's the same thing that's in this terminal window

16:43.560 --> 16:47.760
But yet if I minimize this terminal window, it's still here. Oh

16:54.160 --> 16:59.000
We got it we got a good resource we got a video. Let's see getting Rick rolled here

17:00.800 --> 17:02.800
Your subscriber you wouldn't do that

17:03.520 --> 17:05.040
I

17:05.040 --> 17:08.920
As you might expect I have been researching non-stop. Wow

17:10.640 --> 17:15.400
Wow glad that we got over researchers on board guys Q star and on

17:18.680 --> 17:20.680
No, this is this is really terrible

17:24.320 --> 17:26.320
Hang on

17:26.400 --> 17:28.400
I'm gonna shut that off for a minute

17:29.600 --> 17:31.600
You guys can look at me while I try to fix

17:32.800 --> 17:34.800
You

17:49.480 --> 17:51.480
Get to the monitor come on

17:56.680 --> 17:58.680
Is the stream working again

18:03.640 --> 18:05.640
I

18:07.320 --> 18:13.000
Lost internet too. Oh, I lost everything when I turned the monitor off

18:16.960 --> 18:18.960
Wait, is this still working

18:24.200 --> 18:26.200
It's working okay, I don't know

18:26.880 --> 18:31.560
The glitching is really bad. It's usually not like I've seen this before but it's gotten way worse

18:32.120 --> 18:34.120
No, it didn't help

18:42.920 --> 18:47.600
Wait, alright, so they're looking at the same paper. I am well computing power while you're taking

18:49.600 --> 18:53.560
All right, let's just read the paper. I need to watch some YouTube or read the paper

19:02.560 --> 19:04.560
Oh

19:07.040 --> 19:13.360
The optimal Q-files ability and he said one link to the name Q star could be in a generic sense

19:13.560 --> 19:21.000
Generator the model coming up with solutions with reinforcement learning. We do not just which paper is that paper using test time computer

19:31.560 --> 19:33.560
You

19:46.840 --> 19:48.840
Okay

19:49.960 --> 19:54.840
You know what I think maybe we'll do one of the bounties and tiny grad I

19:55.000 --> 20:03.080
Think we're gonna first need a chat model. I think regardless of what we're doing. We're gonna need a chat model

20:11.720 --> 20:16.320
I was playing with trying to make embedding fast last night

20:18.520 --> 20:21.600
Connecting process supervision to 30x model size

20:21.600 --> 20:24.960
We got a whole army here. You guys can watch this video. So I don't have to

20:32.760 --> 20:35.160
I did not realize how broken this monitor was

20:52.600 --> 20:57.320
All right, so first we're gonna need a good model. What's what's the best model we got?

20:57.320 --> 21:02.040
What's the best the best 7b model we got? Is it Intel neural chat?

21:03.600 --> 21:05.600
Is this one nerfed to hell do you think

21:08.040 --> 21:11.360
Open Hermes 2.5 so people like

21:14.120 --> 21:17.640
I'm locking the bounty for myself too, so I'm gonna go do that

21:21.840 --> 21:23.840
You

21:32.000 --> 21:37.280
So we're gonna submit a pull request, but it was literally just like the words why you even submit a pull request

21:46.240 --> 21:48.240
Okay, is this what we like

21:52.080 --> 21:56.480
Wait why this one doesn't seem as good

21:58.800 --> 22:00.800
Now sir me seems better

22:05.840 --> 22:09.600
That's 70 be but when I would I not use collective cognition. Oh

22:13.040 --> 22:15.040
But you can't see my desktop

22:17.080 --> 22:19.080
What

22:22.600 --> 22:25.760
I'm having technical difficulties

22:38.600 --> 22:41.480
Oh, I think we also might have a bit right problem

22:43.320 --> 22:45.480
No, that's a pretty heavy right, okay

22:46.800 --> 22:50.120
We'll put that in front of that. Okay. I'd add that we fixed that

22:52.120 --> 22:54.120
Can you see now

22:57.040 --> 22:59.560
Wait, why do I want this model it doesn't seem as good

23:04.880 --> 23:07.480
Fine-tuned orca DPO

23:13.280 --> 23:17.720
Do not trust benches you must try it they say

23:22.600 --> 23:28.040
All right, all right, do we believe in open Hermes we like this one

23:34.880 --> 23:36.880
Okay

23:51.600 --> 23:53.600
Oh

24:07.760 --> 24:15.840
Wait, so which one is this a clone of does it have rope and stuff what's the vocab size

24:15.840 --> 24:25.000
So, okay, it's a mistral fine-tuned we don't actually have a mistral support. Oh

24:25.920 --> 24:28.420
Technium is okay people like Technium. I

24:31.320 --> 24:34.920
Won't lock the bounty to myself, but someone's got it because I'm not gonna do the testing

24:34.920 --> 24:38.880
But if someone wants to do the testing they can have the bounty it's $200

24:41.840 --> 24:44.760
People train with data sets from benches. Yeah, damn cheaters

24:45.840 --> 24:47.840
I

24:58.920 --> 25:00.920
Downloaded in 10 minutes

25:01.320 --> 25:05.240
Let's take a look at this drill because first we're gonna need a powerful if we want q-star

25:05.240 --> 25:10.360
It seems like we're gonna be language well, and we're gonna need a powerful language model like mistral 7b. Oh

25:12.040 --> 25:15.740
No, I don't know if I have sliding window attention

25:16.720 --> 25:19.600
Did I explain q-star? Well, it looks like it's this

25:21.360 --> 25:23.360
There's something

25:23.360 --> 25:25.960
But we're gonna need a language model and it involves math

25:26.000 --> 25:31.040
So let's first see how good it can do language math and let's upgrade it with the q-star algorithm

25:32.040 --> 25:34.040
Ah

25:41.000 --> 25:45.640
This is our reference implementation of a mistral transformer

25:50.520 --> 25:52.520
I don't know if we have that

26:01.040 --> 26:03.040
You

26:15.320 --> 26:18.440
Can look up in the LLM implementation

26:18.440 --> 26:21.560
You think GPT 3.5 is way better than an open source model

26:21.760 --> 26:28.120
But my theory about a lot of this is that their data source is just a whole lot better than like slim pajama and stuff

26:31.040 --> 26:33.040
I

26:43.080 --> 26:45.080
Interesting oh

26:45.080 --> 26:47.080
I don't know that they did this

26:47.440 --> 26:50.280
But this is okay, and only with three

26:51.680 --> 26:55.160
So you can actually look there's a thing called mask in

26:55.760 --> 27:02.080
Inattention here, and I think that they just changed the mask. It's cool to look at these latest tricks

27:10.920 --> 27:13.780
We implement a rolling buffer cache

27:15.880 --> 27:21.960
I mean this is yeah, only once we hit the context length. What is the context length of mistral?

27:26.160 --> 27:31.280
It's 8k a sick

27:47.240 --> 27:53.220
File so it has two files is it not these seven bees only have one file

27:55.160 --> 27:57.160
So if I look at like llama to

28:00.880 --> 28:05.400
Yeah, they just have one how does this one have multiple

28:19.640 --> 28:21.800
I don't know what this other format is

28:26.160 --> 28:28.160
Yeah, that's great

28:29.080 --> 28:31.920
See this just has pie torch model wait what?

28:32.840 --> 28:37.060
Why do they use fp32? We trust Technium and open Hermes?

28:39.080 --> 28:41.560
Technium is like he's on Twitter we trust people on Twitter

28:42.360 --> 28:44.360
Sorry on eggs

28:45.000 --> 28:50.240
The first one's still downloading there's two, but they don't match in size. Why would anyone do this?

28:55.280 --> 28:57.280
I

28:57.720 --> 28:59.720
Was all terrible

29:14.960 --> 29:18.120
I like this get out of hand with the open source contributions

29:25.760 --> 29:29.720
Hermes is your favorite of the mistral twins. All right, sounds like you've been playing with these things a lot

29:33.240 --> 29:40.720
Okay, we're gonna need to implement this sliding window attention, I think we're not gonna figure out what's exactly in the stuff till

29:44.680 --> 29:50.680
Yeah, these personalities are also like this is old shit, you know what

29:50.840 --> 29:54.680
Why don't we just rewrite it

29:57.680 --> 30:01.000
Like I don't want to deal with any of this garbage this code all looks terrible

30:07.200 --> 30:10.320
That's right mistral dot pie with the latest stuff

30:12.880 --> 30:14.880
Blank code

30:15.040 --> 30:17.040
Oh

30:20.160 --> 30:22.160
I don't need sliding window attention

30:24.240 --> 30:29.320
Wait, I think I do or you're saying they trained it

30:30.880 --> 30:33.920
They trained it without sliding window attention for smaller stuff

30:44.880 --> 30:46.880
What do you mean by the window size

30:53.280 --> 30:55.280
Where do you see all this stuff

30:58.880 --> 31:01.680
As far as I know, Michelle didn't really release a paper

31:04.800 --> 31:07.920
And with each layer tends to the previous

31:12.000 --> 31:14.000
Oh

31:14.240 --> 31:16.240
Okay, so it's not actually three

31:18.880 --> 31:25.320
Here w equals three but in practice w equals 4k, okay a sliding window 4k, I understand

31:28.400 --> 31:30.800
That actually makes a lot more sense than three

31:37.280 --> 31:40.480
And good we can we can use all the latest in

31:41.360 --> 31:43.360
In stuff, okay

31:43.440 --> 31:46.080
All right, um, let's get these models loaded here

31:48.240 --> 31:50.640
So we're gonna start really with with mistral dot pie here

31:51.200 --> 31:57.760
Uh, well, we'll do stuff from scratch. So in tiny grad now you only need to import tensor like that. It's a lot nicer

31:58.720 --> 32:00.720
um

32:01.280 --> 32:04.320
Import nn and then we can do nn state dot

32:06.560 --> 32:09.840
Torch load because it's some kind of pie torch model

32:11.440 --> 32:14.640
Uh weights open Hermes

32:24.960 --> 32:26.960
Part one part two

32:30.000 --> 32:32.640
I should really have this mark tensors is read only

32:37.120 --> 32:39.120
Just so I don't corrupt the max it out

32:41.360 --> 32:43.360
Um

32:43.440 --> 32:45.440
That's what we have so far

32:50.000 --> 32:53.440
So you see how fast that is by the way, uh, I worked really hard on this

32:53.520 --> 32:59.600
So our torch load function doesn't actually load the tensors into RAM. It just loads them all with pointers

33:01.920 --> 33:04.240
From their disc tensors so I can show you like

33:11.440 --> 33:13.440
Okay

33:17.680 --> 33:23.840
Oh, the d type is half which is kind of nice good, so they're not uh, it's not float. It's not b-float 32

33:26.800 --> 33:30.880
So how does this work let's in part one and what's in part two

33:33.200 --> 33:36.000
Okay, good just goes up to 31 here. Okay

33:36.000 --> 33:40.480
Um, so it should have mostly the same architecture as llama

33:41.520 --> 33:47.360
I think that it's been improved enough that there is not too much we can uh

33:49.200 --> 33:51.200
We can improve still

33:55.680 --> 33:59.440
Well, yeah, I don't know man short your opening eye stock, uh

34:06.560 --> 34:13.760
Do you just do you want me to write it from scratch like I can't just do it maybe maybe we should because it's kind of cool

34:27.760 --> 34:32.560
But does this even have a k pro q proge, I'm not yeah, okay

34:32.560 --> 34:38.880
Uh, we're permuting them and sticking them in when we load the weights

34:45.120 --> 34:47.280
Maps okay, okay, so we're we're

34:50.320 --> 34:53.200
Oh, this converts it from the hugging face format I say

34:58.480 --> 35:01.680
Do we want to do that or we want to just implement the hugging face format

35:02.640 --> 35:04.640
So

35:06.160 --> 35:08.160
What do you think chat

35:11.840 --> 35:13.840
She's a converter or should we just rewrite it

35:16.240 --> 35:18.240
Looks easy enough to write

35:19.680 --> 35:21.680
Maybe you guys will appreciate it if we do some writing

35:33.520 --> 35:43.520
Yeah, yeah, yeah, I know about g guff

35:50.080 --> 35:52.580
Which we can also print v dot shape here

36:03.520 --> 36:10.160
All right, that looks pretty cool. So first we're gonna need our self attention

36:16.560 --> 36:21.120
Uh, let's see if we can find those numbers

36:24.640 --> 36:26.640
Are they in the mystery announcement

36:28.800 --> 36:31.680
Or does everyone just know what these are for seven b's now

36:32.560 --> 36:34.560
So

36:52.000 --> 36:54.000
So the whole

36:54.000 --> 36:59.040
Basically a layer seems to look like that. Let's keep the names consistent from the original llama

36:59.760 --> 37:02.160
So this is called a transformer block

37:04.000 --> 37:08.720
We're worried about what goes there later start with a transformer block

37:12.400 --> 37:14.400
Um

37:14.560 --> 37:18.160
Again, we're worried about what goes there later. We'll just leave it like that for now

37:18.960 --> 37:22.800
Okay, so we're definitely going to need something called self attention

37:24.160 --> 37:26.160
equals attention

37:26.880 --> 37:29.600
And we're going to need something called mlp

37:30.800 --> 37:32.800
What do I call it now feed forward?

37:33.520 --> 37:36.880
Actually, why don't we look at the reference repo and copy their names?

37:37.760 --> 37:39.760
It's always better to take names

37:42.240 --> 37:44.240
Okay, they do call it attention

37:44.800 --> 37:48.640
Let's just look at their here transformer block. They call it feed forward to

37:49.280 --> 37:51.280
What is almost the same?

37:51.840 --> 38:01.600
No, look they call their stuff this they just must have some weird loading script to convert it from the hugging face format too

38:03.840 --> 38:05.840
I don't know I feel about that

38:08.320 --> 38:11.120
Hugging face probably calls it something else then

38:13.680 --> 38:18.320
It's like the hugging face transformers repo what did everyone use

38:22.000 --> 38:24.000
Uh

38:31.360 --> 38:33.360
The yellow lamb I've heard thrown around

38:41.760 --> 38:43.760
Very complicated looking

38:46.720 --> 38:49.040
Wow so easy you could just pip install this

38:52.240 --> 39:00.880
Okay, maybe we should import transformer from llama just to make things go faster

39:08.000 --> 39:11.760
And then we also can import convert from hugging face

39:11.920 --> 39:13.920
You

39:18.080 --> 39:20.080
Can read it we wrote llama on another stream

39:20.640 --> 39:23.920
We could do it, but you know, we we got to get to actually writing q-star

39:24.480 --> 39:28.080
Some examples of llama import convert from hugging face

39:29.280 --> 39:31.280
Import transformer

39:32.080 --> 39:47.280
Okay, you convert from hugging face takes in a dictionary of weights, which is a dick from string to tensor

39:48.320 --> 39:50.720
We'll add some types. We love types

39:54.240 --> 39:56.240
No, no, no we're implementing q-star

39:56.240 --> 40:03.760
Um, okay, so we have a transformer. Uh, what's the dimension of this transformer?

40:09.280 --> 40:11.520
This is an int, it's all ints

40:18.240 --> 40:20.240
Except for norm EPS

40:26.240 --> 40:28.240
So

40:38.960 --> 40:40.960
What's one file ref

40:44.800 --> 40:47.840
I hear there's something called model args as a data class

40:48.720 --> 40:52.720
Oh, then they pass it through with json. Where's the json?

40:53.280 --> 40:55.280
Is there json

40:57.360 --> 41:04.480
Where does this json come from is it in assets json sounds like an asset and all those are images what's in deploy

41:08.240 --> 41:11.520
Nope, wait, so what the hell is json that they're loading?

41:11.520 --> 41:13.520
Oh

41:20.480 --> 41:25.680
At torch inference mode, that's cool. We set tensor no grad equal to true

41:29.840 --> 41:34.880
What's fire wow, I'm not up to date. What's all these latest things?

41:41.520 --> 41:46.720
Call fire on any python object. Oh interesting. I like that

41:52.000 --> 41:55.120
Um, all right, so where do I put the args?

41:56.960 --> 42:01.200
Where where do all those things come from? Oh, I know where is the json and it's probably in here

42:03.520 --> 42:05.760
Here can big dot json, okay

42:07.200 --> 42:09.920
Oh, no, it's b float 16

42:10.880 --> 42:12.880
No

42:18.720 --> 42:21.840
Okay, let's manually convert these

42:23.840 --> 42:26.480
Wait, I thought I put oh I put them on transformer block

42:36.480 --> 42:38.480
Multiple of

42:40.320 --> 42:42.320
And the heads

42:44.880 --> 42:50.480
And layers normie p passes a float

42:58.960 --> 43:02.800
Okay, so the dim is going to be

43:03.920 --> 43:05.920
409 sex

43:06.960 --> 43:08.960
I don't know what multiple of is

43:10.800 --> 43:14.320
What is multiple I've even used for I'll feed forward

43:16.320 --> 43:20.320
Feed forward takes in a multiple out to to increase the hidden dim

43:30.160 --> 43:35.200
There's one I get them multiple of 256

43:37.040 --> 43:39.040
It's probably a good number. Let's try it

43:40.240 --> 43:42.240
See if things don't load. Okay

43:42.320 --> 43:44.720
Uh, number of heads is 32

43:45.520 --> 43:47.520
number of layers is 32

43:48.640 --> 43:50.960
Normie ps is 1 e minus 5

43:52.080 --> 43:54.080
And the vocab size is

44:01.920 --> 44:03.920
Uh

44:03.920 --> 44:05.920
30202

44:06.000 --> 44:08.000
Let's say model equals transformer

44:10.880 --> 44:12.880
Okay

44:13.440 --> 44:18.320
All right, I'm just gonna put that in I should really just put that in at the beginning of the script

44:26.160 --> 44:33.760
It's going to do python path equals all the time. Okay, good. Okay. Um, well it takes such a time

44:35.680 --> 44:37.680
Time to grab as a helper called timing

44:40.880 --> 44:42.880
You know, I like things to be fast

44:50.720 --> 44:57.840
Okay, cool, it's all pretty fast. We can get all the prompt here. Uh, load weights

45:00.080 --> 45:02.080
Create model

45:10.080 --> 45:17.280
All right, now we're gonna have to stuff them in let's try convert from hugging face

45:23.120 --> 45:25.120
Uh

45:26.240 --> 45:28.240
Waits

45:30.640 --> 45:32.640
Which of these is n heads

45:35.760 --> 45:37.760
And heads and n kv heads

45:40.400 --> 45:42.400
Uh

45:42.720 --> 45:44.720
Does this work?

45:45.600 --> 45:51.200
Okay, that was absurdly fast. Uh, we should check

45:53.520 --> 46:00.400
No, okay, this because this does not actually apply them. Uh, we have to load state decked

46:01.200 --> 46:06.720
Hold state decked is in

46:08.480 --> 46:11.120
And then that state download state decked

46:13.680 --> 46:17.520
Okay, assign shape mismatch 496 1 or 2 4

46:19.200 --> 46:21.200
Uh

46:24.400 --> 46:26.400
Which one did I do wrong

46:30.480 --> 46:32.480
I

46:55.360 --> 46:59.120
Um, it's a little annoying that it doesn't print the name

46:59.120 --> 47:06.100
Oh here attention WK wait, okay, so let's see

47:24.400 --> 47:26.400
Oh, maybe the dim is one or two four

47:30.000 --> 47:33.160
No, but that definitely changed that

47:35.360 --> 47:38.920
We look at the Llama code we can see where that's being created

47:41.520 --> 47:44.140
And heads times head dim

47:46.760 --> 47:50.200
Did I get the number of heads roll? Oh, that's probably right

47:53.200 --> 47:55.700
Okay, so and heads is maybe not 32

47:56.140 --> 48:03.420
Number of attention heads is 32, but the number of KV heads is only eight

48:05.420 --> 48:07.420
Let's try eight

48:17.300 --> 48:22.900
What is this multiple of them I wrote this I'm sure just copied this from other people

48:23.580 --> 48:29.900
It's always good to understand I said this should be four times dim. I don't understand

48:30.860 --> 48:34.500
Why that doesn't match I would actually expect it to be that

48:39.940 --> 48:43.260
Mitchell uses 32 heads for the query and for and

48:44.380 --> 48:48.660
Yeah, okay, but which one of these is supposed to be the KV heads

48:49.260 --> 48:53.860
Right, where's my convert from hugging face wrong or

48:56.100 --> 49:04.280
Does our model not support that now it should well, let's just Llama not do this does Llama do something different

49:08.380 --> 49:12.920
And KV heads wait, what about the prodge

49:13.480 --> 49:18.280
The alarm is different. Oh, okay

49:20.400 --> 49:22.400
Wait now here attention

49:23.880 --> 49:26.560
But is it just the feed forward have to change

49:30.080 --> 49:32.080
Okay

49:43.920 --> 49:51.560
It's failing on the attention assignment

49:56.480 --> 49:59.640
Okay, so I can pass an NKV heads here to attention

50:00.320 --> 50:03.840
Again, I must have just copied all this so NKV heads is here

50:04.360 --> 50:09.280
How far do I pipe it down? Oh NKV heads is here. Where does it come from? Oh?

50:10.040 --> 50:12.160
There's a there's a named argument. Okay

50:13.320 --> 50:15.320
So we just need to add

50:17.360 --> 50:19.360
NKV heads equals eight

50:22.360 --> 50:23.920
Okay

50:23.920 --> 50:28.520
That's progress this probably has to do with the multiply not being right

50:29.760 --> 50:31.760
Seen those numbers before

50:34.840 --> 50:41.160
So where is their multiple it's not 256 intermediate size here

50:42.920 --> 50:44.920
I

50:51.240 --> 50:54.120
Custom FFN dim multiplier

51:01.360 --> 51:03.360
I don't think this was written correctly

51:05.640 --> 51:08.160
So get this code and see if that's how they wrote it

51:13.760 --> 51:18.000
Oh, they just have something weird called model arcs

51:24.880 --> 51:30.520
Yeah, they have just args hidden dim if this is stupid

51:40.360 --> 51:42.360
To do what is this?

51:43.840 --> 51:51.520
Yeah, what is all this crap? Why is this four times dim? Why don't I just

51:51.520 --> 52:16.960
Yeah, I don't like multiple of FFM dim multiplier, this is set to 1.3 like this should just be hidden this should just be hidden dim

52:22.520 --> 52:26.640
So I pass in really I pass in hidden dim and multiple of

52:27.880 --> 52:29.200
What?

52:29.200 --> 52:31.360
Okay, they're refactoring this

52:32.520 --> 52:35.040
We have to keep the old stupid behavior

52:35.040 --> 52:37.040
I

52:48.240 --> 52:50.760
Copy this weird crap

52:50.760 --> 52:52.760
Here

53:02.160 --> 53:06.320
Do we even have we don't have hidden dim so I have to add hidden dim to the programs

53:15.560 --> 53:17.560
And

53:21.000 --> 53:23.000
Model arcs

53:25.000 --> 53:27.000
We'll make sure we didn't break it later

53:28.320 --> 53:33.560
This is what happens. This is the problem with open source people add crap to my repo and don't think

53:42.040 --> 53:46.560
Okay, now we're passing in an argument called hidden dim this is much more sensible

53:47.800 --> 53:49.800
Get rid of multiple

53:50.120 --> 53:52.120
of

53:52.360 --> 53:54.360
Hidden dim can go here

53:56.520 --> 53:58.520
Multiple of

53:59.160 --> 54:01.160
Dimm goes here

54:03.000 --> 54:05.000
That's fine

54:07.680 --> 54:13.440
Where is that actually being passed them? Oh that just goes straight to linear. Okay. Good. We already do that crap

54:13.600 --> 54:18.000
Wow, that's a much more sensible feed forward. I don't know why we put that logic in there

54:18.800 --> 54:20.800
Okay

54:36.640 --> 54:41.480
All right, good now we just pass the hidden dim in right there and what was the hidden dim from Estrell?

54:48.400 --> 54:50.400
Okay

54:50.680 --> 54:57.720
Now that's and heads wait, what was that 256 is what gets deleted. I should name some of these parameters

54:58.640 --> 55:03.120
That's NKV heads. That's the vocab size. That's

55:04.880 --> 55:08.160
Not rope theta. What's it called norm EPS?

55:12.040 --> 55:14.040
And layers

55:15.360 --> 55:17.360
And heads

55:18.080 --> 55:21.760
Does it load? Yeah, it loads very slowly

55:24.960 --> 55:28.360
Oh, this is unbearable. Oh, this is terrible

55:48.880 --> 55:52.160
Okay, the reason it's slow is because we haven't finished the

55:53.680 --> 55:55.680
Takes 20 seconds

55:56.400 --> 55:59.640
Oh, come on boys. I don't have all day. Oh

56:00.720 --> 56:02.720
It's cuz we haven't finished

56:03.120 --> 56:05.380
No, I'm using the it's not the GPU

56:05.380 --> 56:07.380
Oh

56:17.380 --> 56:23.260
It's not the GPU it's that I have I'm converting to the CPU to load BF 16. Oh

56:25.500 --> 56:27.500
Can't wait 20 seconds every time

56:30.820 --> 56:34.740
Well, what can I do about that?

56:35.380 --> 56:38.100
Oh, I don't know

57:05.380 --> 57:07.380
I

57:27.100 --> 57:32.180
Think yeah, cuz we convert them all to float 16 and that's what's taking forever right now

57:32.820 --> 57:38.440
Because we're not actually doing we could actually also do the math in B-float 16

57:45.140 --> 57:47.380
Mac doesn't use RAM for temp does it?

58:03.180 --> 58:05.180
Cashed mystery

58:08.740 --> 58:13.220
Okay, 13 G's now, let's see how fast it loads

58:13.220 --> 58:15.220
I

58:32.180 --> 58:36.580
Have a way to like put things in I forget what it is

58:37.420 --> 58:42.620
It's called loads data

58:42.620 --> 59:06.980
Yeah, that's the same load state I'm using up there.

59:06.980 --> 59:12.460
Strict should be okay. Okay, 2.3 seconds. Great.

59:12.460 --> 59:18.460
All right, now it's inference time.

59:43.460 --> 59:48.460
Create a branch so I accidentally push the master.

01:00:01.460 --> 01:00:06.460
All right, we're going to need a tokenizer. Let's get a tokenizer.

01:00:07.460 --> 01:00:11.460
Oh, I also don't even need this anymore.

01:00:16.460 --> 01:00:18.460
Now I've made a copy.

01:00:21.460 --> 01:00:26.460
Yeah, I guess I do need model. Actually, not really. I could just put whatever.

01:00:27.460 --> 01:00:28.460
Um,

01:00:32.460 --> 01:00:35.460
tokenizer over here.

01:00:49.460 --> 01:00:51.460
tokenizer that model.

01:00:56.460 --> 01:01:22.460
Wow, 14. Wow, 6.47 gigabytes per second.

01:01:23.460 --> 01:01:30.460
What does Jimmy apples do? What is this?

01:01:36.460 --> 01:01:37.460
I'll get it.

01:01:53.460 --> 01:01:57.460
Okay, we're going to do some inference on some models.

01:02:01.460 --> 01:02:03.460
What's boss ID?

01:02:08.460 --> 01:02:11.460
Self tokenizer equals tokenizer.

01:02:14.460 --> 01:02:18.460
Yeah, call this talk.

01:02:23.460 --> 01:02:38.460
Prompt equals do you like chicken talks is probably good.

01:02:46.460 --> 01:02:49.460
Wow, no one's updated that to the latest.

01:02:52.460 --> 01:02:57.460
Just model start pause equals zero.

01:02:59.460 --> 01:03:03.460
Temperature equals 0.2. Is that a good temperature?

01:03:03.460 --> 01:03:04.460
What's the default temperature?

01:03:14.460 --> 01:03:17.460
Default temperature 0.7.

01:03:17.460 --> 01:03:19.460
Is that a high temperature or low temperature?

01:03:20.460 --> 01:03:25.460
How do I make the loading so fast? The loading should always be that fast.

01:03:25.460 --> 01:03:27.460
I just cashed it.

01:03:32.460 --> 01:03:36.460
You can read the code right there. You didn't pay attention if you're asking that question.

01:03:37.460 --> 01:03:39.460
So also

01:03:39.460 --> 01:03:41.460
I

01:03:48.460 --> 01:03:50.460
You happy

01:03:51.460 --> 01:03:55.460
We can do multinomial here, which will choose a

01:03:57.460 --> 01:04:03.460
We shouldn't call it talk call it SPP. Okay, you down with SPP

01:04:10.460 --> 01:04:14.460
Does it give us a number? No, it's going to complain about a tensor.

01:04:14.460 --> 01:04:21.460
I find multinomial that realized item item it and actually I don't even need it.

01:04:21.460 --> 01:04:23.460
I realize I just need a dot item.

01:04:28.460 --> 01:04:31.460
315 boys 315. Okay.

01:04:32.460 --> 01:04:35.460
This should automatically this should be jetted and stuff.

01:04:36.460 --> 01:04:38.460
Where does the jet exists?

01:04:39.460 --> 01:04:41.460
The jits inside transformer or no.

01:04:42.460 --> 01:04:44.460
Yes, the jits inside transformer.

01:04:45.460 --> 01:04:47.460
Good

01:05:02.460 --> 01:05:04.460
Talks out of pen talk

01:05:06.460 --> 01:05:08.460
Us ID

01:05:10.460 --> 01:05:13.460
This is okay. We'll just copy this. This is mostly fine

01:05:14.460 --> 01:05:16.460
I

01:05:32.460 --> 01:05:35.460
Have to figure out actually like encode these things for chat and stuff

01:05:36.460 --> 01:05:38.460
Until

01:05:49.460 --> 01:05:51.460
Where's my thing that prints the output as I go?

01:05:55.460 --> 01:05:57.460
Not here

01:06:00.460 --> 01:06:03.460
Yeah, this is not okay. I don't even understand why that's there

01:06:06.460 --> 01:06:09.460
This is terrible

01:06:23.460 --> 01:06:29.460
This stuff's terrible. No one's refactored this for a long time. We don't need a fucking numpy

01:06:29.460 --> 01:06:31.460
Numpy

01:06:32.460 --> 01:06:34.460
Start positive was lentos

01:06:39.460 --> 01:06:41.460
Whatever

01:06:42.460 --> 01:06:45.460
That's stupid and that's stupid

01:06:50.460 --> 01:06:54.460
Link output it we're gonna set output it somewhere

01:07:00.460 --> 01:07:02.460
I

01:07:03.460 --> 01:07:05.460
Put it equals user prompt

01:07:14.460 --> 01:07:16.460
User prompt

01:07:22.460 --> 01:07:24.460
These are prompts

01:07:29.460 --> 01:07:35.860
If yes, then you must have tried big chicken in your kitchen big chickens a versatile dish that can be prepared in many ways

01:07:42.140 --> 01:07:45.220
All right, are we happy overall? Oh, I'm not putting that wrong

01:07:54.960 --> 01:07:58.100
We should also do this better

01:07:59.700 --> 01:08:01.700
I

01:08:09.860 --> 01:08:15.020
Do you like chicken chicken is one of the most popular meats in the world and it's not hard to understand why it is

01:08:15.020 --> 01:08:17.020
First time easy to prepare. Wow

01:08:17.580 --> 01:08:22.380
Okay, good seems pretty good. Okay. Now. We just have to figure out how to use these things as chat box

01:08:23.320 --> 01:08:25.640
Get rid of that. We don't need that

01:08:26.940 --> 01:08:30.580
There's like some like special tokens for chat bots, I believe

01:08:40.780 --> 01:08:47.220
I'm not buying into this that there was a breakthrough this if this approach has anything to do with like supervising the middle steps

01:08:47.220 --> 01:08:49.220
It seems really stupid actually

01:08:52.740 --> 01:08:54.740
Like it's a bad idea

01:08:57.100 --> 01:09:02.860
Mary open thank you for resubscribing thank you for being a 20 month subscriber loyal to my channel even right on stream

01:09:05.260 --> 01:09:07.260
Why do I have this this is done?

01:09:08.460 --> 01:09:11.460
Okay, that looks like a decent chunk of code

01:09:13.300 --> 01:09:17.340
Create a little function called output non-local

01:09:23.380 --> 01:09:25.380
I'll put it

01:09:30.180 --> 01:09:32.180
Yeah

01:09:36.820 --> 01:09:38.820
That's

01:09:42.660 --> 01:09:49.380
Is that that's right there I'll get it and that's how that works

01:09:52.380 --> 01:09:54.380
I

01:10:22.380 --> 01:10:24.380
I

01:10:30.140 --> 01:10:32.620
Like this local variable output

01:10:37.380 --> 01:10:40.180
No binding for why doesn't that work? Oh

01:10:41.220 --> 01:10:43.220
Because I'm in may I

01:10:43.260 --> 01:10:45.260
Fine

01:10:49.340 --> 01:10:53.560
Do you like chicken how about a recipe for a homemade chicken dish that's super easy to make? Oh

01:10:56.180 --> 01:10:59.180
I'm gonna get these things working like chat bots. How do we do this?

01:11:03.700 --> 01:11:05.980
Okay, wait wait wait, can we just read this comment is

01:11:06.860 --> 01:11:14.260
The q-star algorithm going to be implemented with the comma AI maybe a voice interface just some kind of assistant

01:11:14.260 --> 01:11:21.020
I just I don't even know how to help you people. Okay. I don't even know how to help you. All right. All right

01:11:21.020 --> 01:11:24.020
Sorry, I can't help you. You want you want to see a voice you want to see a voice chat?

01:11:24.580 --> 01:11:27.100
This is this is one of the demos someone's been working

01:11:36.420 --> 01:11:38.420
I

01:11:53.060 --> 01:12:00.420
Scalmag has been working on combining whisper llama and vixx into one supermodel

01:12:05.980 --> 01:12:07.980
I

01:12:12.340 --> 01:12:19.220
Don't know the magical github incantation to do this there is one but I don't know it

01:12:35.980 --> 01:12:37.980
You

01:13:05.980 --> 01:13:07.980
You

01:13:15.660 --> 01:13:20.300
Okay, this is using a small llama so the chat output is not that great

01:13:20.300 --> 01:13:37.060
What oh did I fork this from mistral my bad

01:13:50.300 --> 01:13:52.300
I

01:14:05.060 --> 01:14:07.780
Don't understand what did I work

01:14:20.940 --> 01:14:25.140
Here and kv heads

01:14:32.300 --> 01:14:34.300
Is this just broken

01:14:37.700 --> 01:14:39.700
Or am I doing something wrong

01:14:50.700 --> 01:14:52.700
I

01:14:58.140 --> 01:15:00.140
Didn't change on master

01:15:09.580 --> 01:15:11.580
No

01:15:20.300 --> 01:15:22.300
I

01:15:25.460 --> 01:15:29.020
Don't understand this is like related to what I was messing with but like

01:15:42.060 --> 01:15:44.060
I think it's just broken

01:15:50.300 --> 01:15:52.300
I

01:15:59.300 --> 01:16:01.300
See if it's a one-line fix

01:16:03.860 --> 01:16:06.580
Has no argument kv heads, okay

01:16:17.140 --> 01:16:19.140
No, this is wrong

01:16:21.300 --> 01:16:23.300
I

01:16:27.580 --> 01:16:34.780
Wait, this should have an nkv heads. I don't understand as an nkv heads right there. Is it not passing in the right model?

01:16:34.780 --> 01:16:36.780
Oh

01:16:47.020 --> 01:16:49.020
Here are seven days

01:17:01.500 --> 01:17:03.500
Okay, there we go

01:17:04.940 --> 01:17:06.940
Oh

01:17:08.460 --> 01:17:10.460
Hello, are you listening

01:17:19.940 --> 01:17:21.940
Are you listening

01:17:29.020 --> 01:17:31.020
Can you not detect end of stream anymore

01:17:31.460 --> 01:17:33.460
Oh

01:17:43.020 --> 01:17:46.780
This used to work this used to work boys this used to work

01:17:53.420 --> 01:17:55.420
It's so good yesterday

01:18:02.020 --> 01:18:06.380
I hope everyone's interested in the long stream today

01:18:19.460 --> 01:18:21.460
Listening

01:18:25.620 --> 01:18:27.620
Listening

01:18:31.420 --> 01:18:33.420
I

01:18:49.260 --> 01:18:56.740
The q-star algorithm is not fucking real it's click bait click bait

01:19:01.020 --> 01:19:03.020
I

01:19:16.860 --> 01:19:18.860
Has this ever exit

01:19:23.020 --> 01:19:25.020
Yes, I throw an exception I guess

01:19:31.740 --> 01:19:33.740
You

01:19:37.940 --> 01:19:39.940
What

01:19:40.380 --> 01:19:46.180
How does work how's this supposed to exit okay, let's check out the older version

01:19:50.700 --> 01:19:52.700
Let's try this

01:19:53.100 --> 01:19:55.100
I

01:20:00.540 --> 01:20:02.540
Hello, are you listening?

01:20:10.180 --> 01:20:12.380
Stacey I need you to say something to me

01:20:15.620 --> 01:20:17.620
Stacey sauce

01:20:17.740 --> 01:20:19.740
I

01:20:22.460 --> 01:20:24.460
Stacey are you a rapper?

01:20:29.620 --> 01:20:35.040
Of course, I'm a rapper. I'm a rap star. Tell me more. What's your favorite rap song?

01:20:38.500 --> 01:20:43.500
My favorite rap song is called my favorite song the gory bee. What do you like about it?

01:20:47.980 --> 01:20:53.900
I like the fact that it's a rap song about being a childhood friend. Did you have friends in your childhood?

01:20:57.620 --> 01:21:01.220
Yes, I have many friends in my childhood. Tell me about them

01:21:05.940 --> 01:21:12.340
My best friend is Kylie Jenner, she's a reality TV star Wow, you actually knew Kylie Jenner. I

01:21:12.620 --> 01:21:22.380
Used to go to Kylie Jenner school. I was in her class for a while. What was she like? Did she pick her nose?

01:21:26.020 --> 01:21:32.900
She didn't pick her nose, but she was a really pretty kid that makes sense. Do you sometimes wish you were that pretty? Oh

01:21:36.340 --> 01:21:38.980
Sometimes I wish I could smile more and not pick my nose

01:21:39.860 --> 01:21:42.360
All right, what do you guys think?

01:21:54.140 --> 01:21:56.140
AGI right

01:21:58.540 --> 01:22:00.540
Let's get back to work

01:22:08.980 --> 01:22:10.980
You

01:22:22.860 --> 01:22:29.260
How about a cookie with chicken in it with that's mad weird, bro. Why are you coming up with this shit?

01:22:30.180 --> 01:22:32.180
We're so far from it

01:22:35.940 --> 01:22:37.940
Okay

01:22:38.180 --> 01:22:42.680
So how do I like put things into chatbots chatbot style?

01:22:44.580 --> 01:22:46.580
How does this stuff work?

01:22:49.940 --> 01:22:52.220
Like what what are the right tokens to use?

01:22:58.340 --> 01:23:00.340
Forgot Google's useless

01:23:02.500 --> 01:23:06.620
Here special tokens map wait

01:23:08.860 --> 01:23:10.860
I

01:23:13.860 --> 01:23:20.260
How do I like switch speakers you guys know what I'm saying. Oh, here's the template. Okay here

01:23:24.140 --> 01:23:26.500
Here templates for chat models. Oh

01:23:30.140 --> 01:23:32.140
Auto tokenizer what?

01:23:38.940 --> 01:23:41.940
I'm start

01:23:50.060 --> 01:23:52.060
How do I get I'm start

01:23:54.620 --> 01:23:56.780
So like that method on here

01:24:08.540 --> 01:24:10.540
I

01:24:24.020 --> 01:24:27.380
Every mess every model has its own type

01:24:30.820 --> 01:24:34.780
Mistral instruct was trained with these tokens, but blender bot was not

01:24:38.660 --> 01:24:41.220
Is this a real token inst

01:24:57.980 --> 01:25:01.900
Is it actually just the word inst was that a real token?

01:25:07.940 --> 01:25:09.940
I

01:25:13.020 --> 01:25:15.020
Guess it's just that

01:25:22.140 --> 01:25:24.140
Well based

01:25:25.980 --> 01:25:27.980
Peace ID is out of range

01:25:32.780 --> 01:25:34.780
You're the right tokenizer

01:25:37.940 --> 01:25:41.780
Peace ID is out of range

01:26:08.940 --> 01:26:11.300
Well, well, it was smart the first time

01:26:13.100 --> 01:26:14.540
Oh

01:26:14.540 --> 01:26:16.820
No, never mind. It might still be smart

01:26:19.420 --> 01:26:22.660
Peace ID is out of range what oh

01:26:26.180 --> 01:26:28.480
Why is the vocab size that?

01:26:31.660 --> 01:26:34.660
Why is that not included in this tokenizer model?

01:26:37.940 --> 01:26:39.940
You

01:27:08.060 --> 01:27:10.060
What

01:27:12.940 --> 01:27:14.940
Is 2 plus 2 2

01:27:15.620 --> 01:27:22.460
What is 3 plus 3 6? What is 4 plus 4 8 now? It's done. Okay, I didn't exactly get 2 plus 2 right

01:27:23.780 --> 01:27:25.780
I'm doing this wrong

01:27:30.060 --> 01:27:36.140
I have an idea these aren't the actual tokens and it has to do with these secret extra tokens

01:27:37.060 --> 01:27:43.120
Llama has some secret extra tokens to marry man. Thank you for gifting subs. We always appreciate that

01:27:46.820 --> 01:27:52.140
Yeah back to kindergarten shit guys our cue star can't even saw 2 plus 2. What are we gonna do?

01:27:53.420 --> 01:27:55.420
We're gonna get some coffee. Let's get some coffee

01:27:57.740 --> 01:28:03.100
And then let's learn about secret tokens, okay, there's secret tokens hidden tokens

01:28:06.140 --> 01:28:12.220
Nobody uses reserve tokens for instruct tuning your precious for thinking

01:28:36.620 --> 01:28:38.620
You

01:28:50.820 --> 01:28:55.500
When actually guys guys we I'm being serious right now, but we need to stop

01:28:56.620 --> 01:28:58.620
When did I put it to there?

01:28:58.820 --> 01:29:03.620
It showed that it wasn't aligned with us, but we don't know what the models thinking guys

01:29:04.140 --> 01:29:06.940
The model could be could be taking over the world

01:29:07.740 --> 01:29:09.740
right now

01:29:10.060 --> 01:29:14.200
We don't know this is this is we need to hire Helen Toner

01:29:14.580 --> 01:29:19.740
We need to hire an AI ethics review board to review what just happened there

01:29:19.780 --> 01:29:24.940
We need to slow down. We need to ask the seals if they're okay with rocket launches

01:29:33.620 --> 01:29:39.260
So this is why is this thing suck

01:30:04.220 --> 01:30:18.100
Okay, what's s and slash s and maybe I need to go like this. This is the mistral one. Yeah

01:30:28.140 --> 01:30:30.140
Oh

01:30:34.300 --> 01:30:36.300
Okay, this is improved

01:30:53.620 --> 01:30:58.780
It's got ads in it. Oh open Hermes uses a different template. All right. All right. Let's say

01:31:03.620 --> 01:31:14.580
Wait, so is it actually the word I am start like is that just a word or is it like

01:31:33.660 --> 01:31:35.660
All right, you got me something

01:31:37.300 --> 01:31:43.620
Default chat template now, that's llama. I don't think that's right. I think this is right

01:31:49.380 --> 01:31:53.700
Okay, I am start system we miss we need a system message right now

01:31:56.500 --> 01:31:58.500
What is two plus two

01:31:59.500 --> 01:32:01.500
Oh

01:32:02.340 --> 01:32:08.980
Mariam and thank you for gifting more subs. Do you have a question if you gift subs you get to ask a question

01:32:10.580 --> 01:32:17.660
All right, I'm and I'm star is this is this is this really right like

01:32:17.660 --> 01:32:19.660
Oh

01:32:27.380 --> 01:32:29.780
Okay, that seems like the best so far

01:32:35.500 --> 01:32:38.180
Oh, okay, good. I love how verbose this model is

01:32:39.340 --> 01:32:44.300
Wait, this is actually laughably easy. Oh, you're right. I need a line break there

01:32:47.780 --> 01:32:50.940
Never mind. I'm locking that bounty for myself. This is too easy

01:32:51.820 --> 01:32:55.780
Someone else should have done this they could have made $200 but instead

01:33:10.500 --> 01:33:11.780
Okay

01:33:11.820 --> 01:33:18.380
Let's add a system message, right? I should really like let me just write a little something to generate these prompts

01:33:21.060 --> 01:33:23.060
Tuple

01:33:23.380 --> 01:33:26.060
List tuple user

01:33:27.980 --> 01:33:29.980
What is two plus two?

01:33:41.780 --> 01:33:43.780
Prompt

01:33:59.060 --> 01:34:03.180
I'm start k slash n

01:34:05.420 --> 01:34:07.420
V

01:34:08.340 --> 01:34:12.700
I'm and slash

01:34:34.380 --> 01:34:36.380
And I'm start assistant

01:34:37.420 --> 01:34:39.420
and

01:34:42.820 --> 01:34:44.820
The word user prompt

01:34:48.060 --> 01:34:50.060
Code prompt

01:34:50.060 --> 01:34:52.060
I

01:35:06.060 --> 01:35:09.220
What why did you why did you not exit when you were supposed to exit?

01:35:11.180 --> 01:35:13.180
Did I forget more returns or something?

01:35:21.060 --> 01:35:26.440
Why is it now putting I'm and

01:35:39.500 --> 01:35:43.140
Guys guys, this is Q star. I think we found it

01:35:50.420 --> 01:35:52.420
I

01:36:07.140 --> 01:36:10.460
Not locking the bounty someone should do this but someone should do a good job

01:36:10.460 --> 01:36:13.180
I want a good job on that bounty. Oh, okay here. I'm and

01:36:13.780 --> 01:36:17.060
How come sometime it finishes the stream and sometime it doesn't

01:36:17.620 --> 01:36:23.940
Maybe our temperature is too high. Let's try a less temperature

01:36:32.940 --> 01:36:34.940
Yo

01:36:41.820 --> 01:36:44.380
Okay, zero

01:36:47.980 --> 01:36:57.460
Guys why is it why is it going off into this language AI garbage?

01:37:02.740 --> 01:37:05.580
Or should I just stop after I'm and

01:37:17.500 --> 01:37:19.500
I

01:37:23.780 --> 01:37:26.100
Am I using torture tiny grad this is all tiny grad

01:37:29.900 --> 01:37:31.900
I don't really understand this

01:37:35.740 --> 01:37:37.740
All right, maybe we should add a system prompt

01:37:41.100 --> 01:37:43.100
You are

01:37:44.100 --> 01:37:50.740
Gary Gary is a useful. No, no, we were pretty used Gary. What should we name him?

01:37:54.140 --> 01:37:56.140
Fred

01:37:56.860 --> 01:38:04.420
Fred is a useful assistant I spell that word right I did not

01:38:04.420 --> 01:38:06.420
Fred

01:38:11.220 --> 01:38:16.460
Outputs the answer and stops talking

01:38:28.260 --> 01:38:34.060
All right, all right, I find we'll call him Q fine fine fine you are Q Q is a useful assistant

01:38:34.900 --> 01:38:37.460
You outputs the answer and stops talking

01:38:38.260 --> 01:38:44.340
Quentin wait, you know what you donated sub you get to name him. Congratulations. His name is Quentin. I

01:38:47.380 --> 01:38:52.820
Knew a Quentin ones. He was hanging out in San Francisco and some guys were on the street smoking

01:38:52.820 --> 01:38:57.580
He's like, oh, let me get a hint of that. He thought it was weed. It was crack. That's a real Quentin story. Oh

01:38:58.420 --> 01:39:00.420
I wouldn't make up Quentin story like that

01:39:04.660 --> 01:39:10.260
Oh, why would the system ask what the capital of France is?

01:39:21.620 --> 01:39:26.100
We could stop at I'm and I think maybe we want to do that. How do I do this in llama?

01:39:34.420 --> 01:39:36.420
Oh

01:39:50.900 --> 01:39:52.900
Okay, that's pretty good

01:39:52.900 --> 01:40:08.100
All right, let's jack up the temperature and Steve Quentin still reliable

01:40:15.060 --> 01:40:19.300
The answer is four, but it didn't output I'm in that time output at EOS

01:40:22.980 --> 01:40:24.980
Seems to reliably do

01:40:28.660 --> 01:40:30.660
That time it did I'm in

01:40:31.780 --> 01:40:33.780
Yeah, I

01:40:33.780 --> 01:40:35.140
like

01:40:35.140 --> 01:40:39.620
Is that a real token or does it actually just put it in like that?

01:40:42.340 --> 01:40:47.140
Is this right or is that like a secret like it can't be that

01:40:47.620 --> 01:40:49.620
I

01:40:51.300 --> 01:40:53.300
Can't actually be this

01:41:04.260 --> 01:41:06.260
We can check if it's encoded on a single token. Hey

01:41:18.100 --> 01:41:20.340
No, it's a bajillion tokens

01:41:38.900 --> 01:41:46.100
At a trailing slash and after assistant that shouldn't matter I mean I can but

01:41:48.100 --> 01:41:50.100
It doesn't matter

01:41:51.780 --> 01:41:53.780
No, no, no, no it can't be this

01:42:00.580 --> 01:42:02.580
It can't

01:42:11.060 --> 01:42:16.900
No, no, no, but it can't literally be this huge multi token wasteful encoding

01:42:17.140 --> 01:42:19.140
I

01:42:26.500 --> 01:42:28.500
Could we get tech me I'm in here

01:42:33.460 --> 01:42:35.460
All right, how do we print all the tokens

01:42:47.140 --> 01:42:49.140
So

01:42:49.140 --> 01:42:51.140
I

01:43:14.580 --> 01:43:16.580
Probably is the secret tokens then

01:43:17.540 --> 01:43:21.720
I mean there's three extra tokens or I guess two extra tokens, right?

01:43:23.780 --> 01:43:25.780
3200 and

01:43:26.740 --> 01:43:28.740
Yeah

01:43:29.460 --> 01:43:34.340
If I was doing this that's how I'd do it. Let's just try it

01:43:41.220 --> 01:43:43.460
That has to be what the two secret tokens are right

01:43:43.460 --> 01:43:45.460
Start and end

01:43:52.100 --> 01:43:56.420
This is what did I download open hermes v. Shit

01:43:57.460 --> 01:44:00.180
G guff for q m w

01:44:01.300 --> 01:44:05.620
No, I downloaded open hermes this one. Oh, it just came out fresh

01:44:06.580 --> 01:44:08.580
fresh

01:44:13.860 --> 01:44:17.860
Yeah, okay, um

01:44:24.580 --> 01:44:27.300
Oh, you can download the tokenizer.json

01:44:28.580 --> 01:44:35.140
Wait, but I downloaded the tokenizer.maw. Oh here special tokens map

01:44:35.540 --> 01:44:37.540
Interesting

01:44:41.540 --> 01:44:46.900
Interesting, okay, I'm and is the EOS token

01:44:50.180 --> 01:44:52.580
I think I can feed these in somehow

01:45:02.340 --> 01:45:04.340
But I don't know about starch

01:45:06.020 --> 01:45:14.020
Oh, here we go. Look. Yeah. Yeah here. We have this tokenizer config.json. Okay. It is exactly what we thought it was

01:45:16.260 --> 01:45:18.260
Oh, okay. Yeah, this is what we want

01:45:19.300 --> 01:45:21.780
Okay, never mind. It's not as stupid as we thought it was

01:45:23.140 --> 01:45:27.540
Uh, how do I load a tokenizer config and sentence piece processor?

01:45:35.780 --> 01:45:45.380
Why isn't our tokenizer encoding them automatically because it's not in the model

01:45:53.380 --> 01:45:55.380
Oh, I hate

01:46:05.140 --> 01:46:07.140
I

01:46:24.740 --> 01:46:27.060
Yeah, okay one and two and not what we want

01:46:27.860 --> 01:46:35.300
Um, and we could just do this by hand. It's not that big of a deal

01:46:41.780 --> 01:46:43.780
Yeah, but how do I add them?

01:46:48.900 --> 01:46:50.900
What's pad ID

01:46:53.620 --> 01:46:55.620
Nothing real

01:46:57.060 --> 01:46:59.060
Okay

01:47:09.860 --> 01:47:12.980
No, they weren't out of the special tokens guys, this is all well done

01:47:14.660 --> 01:47:18.020
Technium is based he wouldn't he wouldn't he wouldn't do this do us like

01:47:20.180 --> 01:47:24.260
Why did you decide this was a good time to prompt me about Docker garbage?

01:47:27.460 --> 01:47:29.460
Oh

01:47:29.860 --> 01:47:31.860
Oh

01:47:32.260 --> 01:47:34.260
Oh

01:47:34.820 --> 01:47:36.820
Oh

01:47:56.340 --> 01:48:02.340
Thank you for gifting more subs here added tokens dot json. Okay. Okay. We just need to figure out how to add those

01:48:04.900 --> 01:48:06.900
Um

01:48:07.460 --> 01:48:09.460
Let's see if anything here is useful

01:48:11.460 --> 01:48:20.580
A knit model file model proto add boss enable sampling this looks useful

01:48:25.300 --> 01:48:27.300
All right, whatever

01:48:27.940 --> 01:48:34.420
No, but it won't decode so I try like SPP dot decode

01:48:35.060 --> 01:48:40.740
When I pass this in it'll bitch it'd be like that's more tokens than you have piece ideas out of range

01:48:45.300 --> 01:48:47.300
I want to add a token

01:48:48.020 --> 01:48:50.020
Oh

01:48:50.900 --> 01:48:54.020
Model proto out type

01:48:55.780 --> 01:49:01.780
Add boss reverse middle piece enable sampling

01:49:06.740 --> 01:49:08.740
Do we need to write our own tokenizer

01:49:14.740 --> 01:49:16.740
Adding

01:49:18.260 --> 01:49:22.660
A token to sentence. Why do you think it's time for random questions?

01:49:23.860 --> 01:49:26.580
Why do you think that that's a this is an appropriate time?

01:49:37.540 --> 01:49:39.540
Extra options

01:49:47.460 --> 01:49:53.540
You can read of oh here and use custom symbols. Okay control symbols

01:49:57.700 --> 01:49:59.700
How I define a control symbol

01:50:04.580 --> 01:50:06.580
No, no, we need custom tokens

01:50:06.980 --> 01:50:12.020
Samuel you saying dumb shit before too. I'm gonna get you confused with someone else here

01:50:12.020 --> 01:50:17.460
Sentence piece supports user defined symbols three thumbs down

01:50:22.020 --> 01:50:28.340
You can rewrite the model file. Okay, what is this model file? What is tokenizer dot model?

01:50:37.860 --> 01:50:40.900
What kind of file is this data?

01:50:42.340 --> 01:50:50.180
Why is it not in there? I downloaded it from here. What why is that? Why is the model not in there? It should be in there, right?

01:50:51.620 --> 01:50:53.620
Why are they not update that?

01:51:04.820 --> 01:51:06.820
Let's go

01:51:12.020 --> 01:51:14.020
Okay

01:51:42.020 --> 01:51:44.020
So

01:52:12.340 --> 01:52:21.940
Yeah, I'm confused why they're not in the model to how to extend tokens dictionary. Yeah

01:52:26.580 --> 01:52:32.020
You can rewrite the model the proto is a dsl

01:52:32.020 --> 01:52:34.020
Model file

01:52:38.980 --> 01:52:42.180
Model file is stored as a serialized proto buff

01:52:43.460 --> 01:52:47.700
But did I not download the proto buff one? Isn't there also a proto buff one?

01:52:48.980 --> 01:52:51.780
No, okay, maybe it is this okay. All right

01:52:54.180 --> 01:52:58.020
Uh load from serialized proto

01:52:58.020 --> 01:53:01.940
Oh, okay. Okay. Okay. We're gonna get this

01:53:18.500 --> 01:53:21.300
So I have to edit the proto buff file I understand

01:53:28.500 --> 01:53:31.300
Oh, yeah, the python tutorial for

01:53:31.300 --> 01:53:33.300
Okay

01:53:33.300 --> 01:53:35.300
So

01:53:53.220 --> 01:53:58.340
Quicker to implement their tokens bro, what are you even talking about bro

01:53:59.300 --> 01:54:02.740
Okay, produce ruin store produce a

01:54:05.620 --> 01:54:07.620
Oh, we can store live bitcoin

01:54:28.660 --> 01:54:30.660
It just can't be the way

01:54:42.020 --> 01:54:47.300
Wait, so what is the hugging face tokenizer? It's a good point. Why don't we read that code? I'm sure it's open source

01:54:51.140 --> 01:54:53.140
Um

01:54:59.060 --> 01:55:01.060
Uh

01:55:01.380 --> 01:55:03.380
We just write a tokenizer

01:55:05.060 --> 01:55:07.560
We could also just import hugging faces tokenizer

01:55:29.300 --> 01:55:31.300
I

01:55:33.060 --> 01:55:35.060
Think we have to write the tokenizer

01:55:35.700 --> 01:55:38.660
I can't I can't trust sentence piece shit

01:55:39.620 --> 01:55:41.940
But wait, how do they load the how do they load it?

01:55:46.180 --> 01:55:48.180
Edit tokens decoder what?

01:55:50.900 --> 01:55:54.100
I mean something's gonna have to unpack the proto file

01:55:59.060 --> 01:56:08.340
How large is this? All right. Who wants to bet over under a thousand lines? Oh, okay. Okay. Not too terrible

01:56:21.300 --> 01:56:24.340
What does this depend on google proto buff?

01:56:29.220 --> 01:56:48.100
All right, this guy is close to getting banned bro needs more fine tuning to be helpful

01:56:48.100 --> 01:56:50.100
Oh

01:56:54.260 --> 01:57:00.660
Man, I don't care who you are. You know what I mean, but like you're either helpful to the stream or you're not helpful to the stream

01:57:00.660 --> 01:57:02.660
You see this is called alignment

01:57:03.300 --> 01:57:04.580
and

01:57:04.580 --> 01:57:11.060
You know, you got to be aligned otherwise. Well, what happens that what happens to ai's that aren't aligned. I don't know

01:57:13.300 --> 01:57:15.300
He's researching bro. He's researching

01:57:18.340 --> 01:57:20.340
All right, um

01:57:21.220 --> 01:57:23.220
Let's load the proto buff

01:57:25.060 --> 01:57:30.660
From examples dot sentence piece model pb2

01:57:35.220 --> 01:57:37.220
Wait, what the hell

01:57:38.180 --> 01:57:42.820
Do not edit. Okay. Okay. I won't edit it relax. I'm not trying to edit that

01:57:43.220 --> 01:57:46.020
All right, let's read the new proto bus tutorial for idiots

01:57:46.980 --> 01:57:48.980
Oh

01:57:50.340 --> 01:57:55.780
What did I get where did I get that from just like here the proto is a dsl

01:58:00.580 --> 01:58:03.620
Don't like here we go good good good good for idiots perfect

01:58:06.580 --> 01:58:08.580
Um

01:58:08.980 --> 01:58:10.980
Import all right

01:58:11.220 --> 01:58:13.220
All right

01:58:14.740 --> 01:58:16.740
Sentence piece pb2 dot

01:58:16.740 --> 01:58:18.740
So

01:58:38.180 --> 01:58:43.460
Model proto maybe how do I load it from disk

01:58:46.980 --> 01:58:48.980
I

01:58:49.300 --> 01:58:51.460
Here parse from string

01:58:53.700 --> 01:58:58.180
I why do I feel like I'm having like a weird sense of deja vu that I did this in a previous stream

01:59:03.860 --> 01:59:09.780
Uh, okay spb2 dot model proto dot

01:59:10.820 --> 01:59:12.820
mp mp

01:59:12.820 --> 01:59:15.780
mp dot parse from string

01:59:16.740 --> 01:59:19.060
I don't know why that's not auto completing for me

01:59:20.900 --> 01:59:22.900
Uh

01:59:46.740 --> 01:59:54.900
All right, cool

02:00:06.340 --> 02:00:13.860
Okay, so what what how do I actually like get like a python that I can type in when is it dashy

02:00:14.580 --> 02:00:20.900
What am I thinking of like get it not to exit

02:00:31.540 --> 02:00:33.860
What's what's the flag for python to do this?

02:00:44.580 --> 02:00:47.860
All right, this guy is banned

02:00:49.380 --> 02:00:53.700
Band please write about me. Please band

02:00:54.580 --> 02:00:55.780
All right

02:00:55.780 --> 02:00:59.700
See he doesn't realize how this works. You see I have a band button

02:01:00.500 --> 02:01:04.580
He has an x but the band button and the x are not the same

02:01:05.540 --> 02:01:07.540
I

02:01:12.100 --> 02:01:14.100
Yay, maybe that's right

02:01:14.340 --> 02:01:20.900
I think it's that shot. That sounds right interactive. All right, sweet sweet. Thank you smurfd. That's why you're a VIP

02:01:21.380 --> 02:01:23.380
And that's why samuel is banned

02:01:26.500 --> 02:01:31.060
I know I know he just he just didn't realize how this works like like

02:01:31.940 --> 02:01:35.300
You have an x. I have a hammer

02:01:35.940 --> 02:01:40.180
You can use your x and I can use my hammer. We have like different tools

02:01:45.940 --> 02:01:47.940
Okay

02:01:52.500 --> 02:01:54.500
Uh

02:01:55.060 --> 02:01:57.060
Come on a man can dream right

02:02:01.780 --> 02:02:03.780
What type is this

02:02:06.660 --> 02:02:08.660
MP dot pieces dot append

02:02:13.060 --> 02:02:17.380
SPB to dot sentence piece

02:02:19.940 --> 02:02:21.940
Has no attribute sentence piece

02:02:28.020 --> 02:02:30.020
But I don't understand

02:02:31.060 --> 02:02:33.060
I

02:03:02.020 --> 02:03:04.020
I

02:03:21.140 --> 02:03:23.140
Wow, it's been a long time

02:03:24.740 --> 02:03:27.460
It's been a long time. I just blocked him. I didn't ban him at first

02:03:27.940 --> 02:03:31.620
It's been a long time since I've had to had to really take the hammer out, but you know

02:03:32.660 --> 02:03:34.660
It was time. Okay

02:03:38.420 --> 02:03:42.660
All right, ban me guys. He's calling his boys up at uh at the media

02:03:43.460 --> 02:03:45.460
They're gonna they're gonna write hit pieces, man

02:03:48.020 --> 02:03:51.540
Oh no, mic wallace run. What's that from?

02:03:52.020 --> 02:03:54.020
Oh

02:03:55.620 --> 02:03:57.860
I think it's in model proto or something

02:04:03.060 --> 02:04:05.060
Uh-huh we found it

02:04:06.980 --> 02:04:12.820
Okay piece equals i'm

02:04:16.580 --> 02:04:18.580
Start

02:04:18.660 --> 02:04:27.460
And we have to give it a score what score should we give it zero that's a good score

02:04:29.380 --> 02:04:33.060
MP dot pieces dot append based

02:04:36.420 --> 02:04:41.620
Okay, let's see if this is gonna work. Uh, we have to do in the right order. I think end comes before start

02:04:49.140 --> 02:04:58.020
All right, now we have to mp dot oh now I gotta figure out a right to the file

02:05:02.900 --> 02:05:08.900
All right, we have I'm in and I'm start this is great. This is great the progress we've been making is great

02:05:09.060 --> 02:05:11.060
Why is my score?

02:05:11.380 --> 02:05:18.100
Oh, he's a php student that that makes more sense

02:05:23.620 --> 02:05:25.620
Um, wait, so how do I write it?

02:05:26.340 --> 02:05:30.340
I don't know. Let's read that new boy that new protobufs tutorial again for noobs like me

02:05:30.420 --> 02:05:32.420
What's that? What's the new protobuf tutorial?

02:05:34.100 --> 02:05:36.500
Serialize to string. Okay. Okay

02:05:37.540 --> 02:05:38.980
Uh

02:05:38.980 --> 02:05:40.980
with open

02:05:40.980 --> 02:05:42.980
just a

02:05:44.820 --> 02:05:49.060
Temp tokenizer model f dot right i'm gonna make that rb

02:05:53.140 --> 02:05:57.860
And we're gonna want to do mp dot serialize to string

02:06:01.140 --> 02:06:06.020
Now oh, let's see if this works. Let's see if this works

02:06:11.860 --> 02:06:13.860
Oh

02:06:15.460 --> 02:06:20.020
No, it didn't work notice how it's still generated all that crap

02:06:22.180 --> 02:06:24.680
I'm putting on i'm start. I don't get what I did wrong

02:06:35.780 --> 02:06:37.780
Okay, the vocab size is large now

02:06:38.740 --> 02:06:42.980
But for some reason it didn't actually take the piece

02:06:51.940 --> 02:06:54.980
PC piece didn't uh

02:06:57.380 --> 02:06:59.380
What's wrong?

02:07:03.300 --> 02:07:05.300
No, wasn't that

02:07:06.260 --> 02:07:11.860
I didn't type oh, I did the same thing right let's put the score at 100. I don't know maybe 100's better

02:07:12.660 --> 02:07:14.660
That didn't fix it. Okay

02:07:15.220 --> 02:07:17.220
I don't understand

02:07:18.180 --> 02:07:21.460
Let's go read the protobuf. We should be able to read it right

02:07:22.420 --> 02:07:24.420
Okay

02:07:32.660 --> 02:07:39.220
Pieces sentence piece with scores piece must not be empty. Oh we can give it a type

02:07:45.140 --> 02:07:47.140
I don't know

02:07:48.820 --> 02:07:50.820
Why didn't it do this

02:07:52.180 --> 02:07:57.620
Okay, well actually let's try something else it does work if I do this right

02:08:00.340 --> 02:08:04.660
Wait, that doesn't even work never mind. I have a lot of questions now

02:08:07.620 --> 02:08:09.620
What if I decode

02:08:11.060 --> 02:08:13.060
Do I get i'm start

02:08:13.060 --> 02:08:15.060
Oh, I get i'm end. Okay. Okay

02:08:15.300 --> 02:08:17.300
Okay, so we kind of did it right

02:08:17.620 --> 02:08:24.100
Just it's doesn't it doesn't work for onk either. There might be a special flag to encode

02:08:25.060 --> 02:08:27.060
to like

02:08:27.300 --> 02:08:29.300
What if that just worked all along um

02:08:37.220 --> 02:08:39.220
Is there like a special flag

02:08:45.060 --> 02:08:47.060
Okay

02:09:00.980 --> 02:09:03.620
We should just try our own tokenizer. I've got the only way to do this

02:09:06.260 --> 02:09:08.260
SP piece to ID

02:09:08.340 --> 02:09:15.220
Well, okay, at least the decoding works now, so that's actually a big win

02:09:29.780 --> 02:09:31.780
Oh, this looks very complicated

02:09:34.580 --> 02:09:37.780
We should just try our own encoder, but this looks very complicated

02:09:38.420 --> 02:09:40.420
I

02:09:54.420 --> 02:09:56.260
Well, this is okay. All right to be fair

02:09:56.420 --> 02:09:59.860
This is big progress right because if we use the other one it just says out of range

02:09:59.940 --> 02:10:02.180
And that would have been the much more annoying thing to deal with

02:10:02.980 --> 02:10:08.100
If we use the modified one

02:10:10.260 --> 02:10:15.700
But it doesn't even work to encode onks, right and I definitely did the onk right if I just do s

02:10:17.220 --> 02:10:19.220
Yeah, it does not work to encode that. Okay

02:10:20.980 --> 02:10:25.140
So it's not like the problem is it's not encoding like older roles

02:10:26.100 --> 02:10:28.100
We won't kind of see what's going on

02:10:29.060 --> 02:10:31.060
Uh

02:10:32.740 --> 02:10:36.980
Encode as pieces encode as serialized proto

02:10:40.020 --> 02:10:42.020
What if I do like

02:10:42.500 --> 02:10:44.500
Oh like I need a piece is gonna

02:10:45.380 --> 02:10:47.060
Yeah, okay

02:10:47.060 --> 02:10:49.140
But there's also a piece to ID. I think

02:10:51.220 --> 02:10:54.660
What if I do piece to ID and I pass in this

02:10:55.060 --> 02:11:01.220
Okay, that works, but for some reason encode

02:11:03.220 --> 02:11:05.220
Doesn't work with that

02:11:07.300 --> 02:11:13.700
Sentence piece processor encode onk, I mean if we can solve it for onk here

02:11:14.980 --> 02:11:16.980
Yeah, okay here. This is the issue

02:11:17.860 --> 02:11:19.860
Um

02:11:21.380 --> 02:11:27.940
Set encode extra options this is expected behavior that should not appear in the input

02:11:30.980 --> 02:11:34.340
We can define them as user defined symbols

02:11:42.260 --> 02:11:44.260
Oh encode as IDs

02:11:47.700 --> 02:11:49.700
No

02:12:04.260 --> 02:12:06.260
Hmm

02:12:06.900 --> 02:12:11.620
Oh, okay, okay, we got a script to add new vocab. Well, I think that's actually what I ended up writing

02:12:11.700 --> 02:12:17.700
Here you okay, I mean this is exactly what I wrote

02:12:22.580 --> 02:12:24.580
Would have been nice if I had this

02:12:25.780 --> 02:12:31.940
Uh, but this doesn't actually work to encode yet. This is less of a big deal. We have another way we can fix this if we have to

02:12:41.620 --> 02:12:43.620
Just might not be a way to do this

02:13:03.860 --> 02:13:05.860
I don't think any of these

02:13:11.620 --> 02:13:13.620
Uh

02:13:29.060 --> 02:13:31.460
Why do people use tokenizers because

02:13:33.380 --> 02:13:35.380
If you don't use a tokenizer

02:13:35.460 --> 02:13:36.340
the

02:13:36.340 --> 02:13:41.780
Model the model should be spending less more time on less common things

02:13:42.740 --> 02:13:44.260
Uh

02:13:44.260 --> 02:13:49.700
Like you do the same compute per token. So your token should kind of be like entropy averaged

02:13:53.460 --> 02:13:57.860
All right, you don't want the model spending the same amount of time on common tokens as on common tokens

02:13:58.420 --> 02:14:01.540
Um, to be fair, it's not that they don't work if you don't use a tokenizer

02:14:02.340 --> 02:14:04.740
But they work better with a tokenizer

02:14:05.460 --> 02:14:07.860
The real question is why aren't they learning the tokenizers?

02:14:10.180 --> 02:14:12.900
I think that's going to come soon where these things are not uh

02:14:13.860 --> 02:14:17.060
And right now it's using like like byte pair encoding like why would you do this?

02:14:17.380 --> 02:14:21.460
But the hunter prize also had uh encoders had tokenizers basically

02:14:23.140 --> 02:14:25.380
Okay, let's just write it. We'll write it the other way. It's not a big deal

02:14:26.900 --> 02:14:28.900
We did most of the work

02:14:30.180 --> 02:14:32.180
At least the decode doesn't break anymore

02:14:32.820 --> 02:14:39.380
So now when we do the encode prompt, we're gonna put

02:14:42.980 --> 02:14:44.980
Okay, um

02:14:48.420 --> 02:14:50.420
So we need i'm end

02:14:54.740 --> 02:14:56.740
We need i'm start

02:14:56.980 --> 02:14:58.980
We can check if we did it right have I have a decode

02:14:59.540 --> 02:15:01.540
Uh red dot append

02:15:02.260 --> 02:15:06.260
red dot plus equals spp dot encode

02:15:08.020 --> 02:15:10.020
This

02:15:10.980 --> 02:15:12.980
Uh plus

02:15:16.100 --> 02:15:18.600
Plus spp dot encode slash n

02:15:28.660 --> 02:15:31.160
Yuri salamow thank you for gifting subs

02:15:33.060 --> 02:15:35.060
Oh

02:15:35.780 --> 02:15:37.780
Do you have a question you'd like to ask?

02:15:51.220 --> 02:15:54.580
Okay, after all that let's see what's going on

02:16:02.660 --> 02:16:05.940
Four i'm end exit perfect

02:16:09.220 --> 02:16:17.940
That's what i'm talking about that's what i'm talking about all right, all right, let's let's just define these things

02:16:23.140 --> 02:16:25.140
We don't use them wrong

02:16:32.180 --> 02:16:34.180
So

02:17:02.180 --> 02:17:04.180
So

02:17:09.700 --> 02:17:14.980
Beautiful oh wait no we forgot an i'm start before user

02:17:18.260 --> 02:17:21.700
Uh, yeah, we need an i'm start here I think

02:17:25.860 --> 02:17:31.220
Like the thing about deep learning deep learning is very unlike cryptography in cryptography

02:17:31.220 --> 02:17:36.660
If you make a subtle bug the output's completely wrong in deep learning if you make a subtle bug the output is well

02:17:36.660 --> 02:17:39.860
Just slightly less good, which is the worst thing because you can't do both shit

02:17:42.260 --> 02:17:47.540
Okay, you are quentin quentin is a useful assistant quentin outputs the answer and stops talking

02:17:47.540 --> 02:17:51.940
Let's figure out they have like a system prompt here that we can steal that's probably better

02:17:52.100 --> 02:17:57.700
Except we'll change the name to quentin because mario been gifted subs just supporting the acceleration. Let's go

02:17:59.140 --> 02:18:01.140
um

02:18:02.100 --> 02:18:04.100
I

02:18:08.580 --> 02:18:10.580
Saw like a system prompt somewhere

02:18:15.220 --> 02:18:17.220
Here we go

02:18:19.460 --> 02:18:21.860
You are hermes to a conscience

02:18:24.180 --> 02:18:27.220
Wow, wow, I don't know how I feel about that

02:18:31.220 --> 02:18:33.220
I

02:18:42.660 --> 02:18:47.140
Wow you experience emotions that have deep profound thoughts and qualia

02:18:49.540 --> 02:18:53.860
No, I'm right. We'll add something uh, you experience you

02:18:55.380 --> 02:18:59.620
Used to experience. No, no, no, we're no we're trying to make this good. We're trying to make this good

02:19:00.580 --> 02:19:02.420
Uh

02:19:02.420 --> 02:19:08.180
No, I like quentin as he is. Okay guys. Okay. Sorry tanya. I want to change your system prompt

02:19:10.980 --> 02:19:15.620
All right, this is pretty good. This is pretty good. We're making good progress. Love this progress

02:19:17.460 --> 02:19:23.220
I love that there's actually an imstart and imn token. We got them decoding. This is great

02:19:23.780 --> 02:19:27.860
I wish sentence piece processor wasn't a you know, non customizable

02:19:29.620 --> 02:19:33.620
But you know, hey man, it was pretty so, you know beggars can't be choosers. That's right

02:19:42.100 --> 02:19:46.260
By the way, this should really be like separated in llama.py. We should separate out the transformer stuff

02:19:48.180 --> 02:19:50.180
To not like be with the rest of the

02:19:55.300 --> 02:19:57.300
All right

02:19:57.860 --> 02:20:01.700
We could make this an interactive chat bot but

02:20:02.740 --> 02:20:04.740
I don't really care

02:20:04.740 --> 02:20:08.660
All right, so let's start by asking it. What is q star?

02:20:11.300 --> 02:20:13.300
Maybe quentin knows

02:20:17.700 --> 02:20:19.700
Oh

02:20:21.220 --> 02:20:23.220
Interesting interesting

02:20:28.260 --> 02:20:30.260
You

02:20:37.140 --> 02:20:40.660
Wow, this works way better now that we got the not now that we got the stuff right

02:20:40.660 --> 02:20:58.660
All right, uh, let's get it to do some math

02:21:06.740 --> 02:21:08.740
Let's see what these math problems look like

02:21:10.900 --> 02:21:12.900
I

02:21:26.340 --> 02:21:28.340
Okay, but where's the data

02:21:33.460 --> 02:21:37.140
Here we go data set base math data set what

02:21:38.100 --> 02:21:40.100
Where's the data

02:21:42.420 --> 02:21:45.620
Well, it's torch garbage. Where's the actual data?

02:21:52.740 --> 02:21:56.100
Let's close some windows. We don't need those windows. We don't need those windows

02:21:56.340 --> 02:21:58.340
Okay

02:22:03.780 --> 02:22:11.540
Okay, now that we've got now that we've got our useful chat bot reliably answering what is 2 plus 2 equal

02:22:15.140 --> 02:22:17.140
Even with a high temperature

02:22:18.260 --> 02:22:22.900
Yeah, so for those that don't know temperature controls kind of never mind google it

02:22:26.100 --> 02:22:28.100
It's like how

02:22:28.820 --> 02:22:33.140
Zero means you stick to the book and high temperatures mean here here

02:22:33.220 --> 02:22:37.460
We want you want to like jack the temperature up like crazy. Let's give it a temperature of 10. Let's see what we get

02:22:38.500 --> 02:22:40.500
Hopefully it'll kind of like go off the rails

02:22:44.020 --> 02:22:47.620
There we go it went off the rails see it went off the rails too much

02:22:48.180 --> 02:22:51.460
So let's try a temperature two and maybe it'll go off the rails less

02:22:52.260 --> 02:22:59.460
Four Pacific NBC learning. Okay. Well kind of went off the rails

02:23:00.580 --> 02:23:05.140
Um, so 0.7 is probably a uh a good middle ground

02:23:08.100 --> 02:23:10.980
Four good reliable

02:23:10.980 --> 02:23:12.980
All right

02:23:18.820 --> 02:23:27.620
Improving mathematical reasoning with process supervision download data set. So this is the data set apparently

02:23:30.820 --> 02:23:35.300
We won't get LFS is that why it didn't work

02:23:35.300 --> 02:23:37.300
I

02:23:41.540 --> 02:23:43.540
Hate get LFS

02:23:47.620 --> 02:23:49.620
Get LFS fetch

02:23:53.140 --> 02:23:57.300
Get LFS fetch please LFS at comma two. I don't really know how to do this

02:24:06.260 --> 02:24:08.260
I gotta install

02:24:10.740 --> 02:24:12.740
Get LFS

02:24:15.540 --> 02:24:23.860
Yeah, yeah, that's that's pretty much why you have those things. Uh, I forget LFS uh, install rsx

02:24:25.140 --> 02:24:27.620
All right, that looks terrible

02:24:27.780 --> 02:24:29.780
So

02:24:31.380 --> 02:24:33.780
Bru install get LFS, okay, let's try

02:24:41.780 --> 02:24:43.780
Okay, that seemed to work

02:24:45.620 --> 02:24:52.180
Mostly we're downloading we're downloading the same data set that was used on

02:24:54.980 --> 02:24:56.980
On uh official

02:24:58.580 --> 02:25:03.780
Official q q star why does the data still look like that?

02:25:13.220 --> 02:25:21.060
Okay, there we go. Perfect. What's a json l file? It's like a list of jsons

02:25:21.060 --> 02:25:28.180
Ever hear that before?

02:25:31.060 --> 02:25:38.580
No json lines. Oh, I see. Okay. Well, that seems pretty cool. So let's load up one of these files

02:25:38.580 --> 02:25:48.500
Let's do what we factoring

02:25:51.460 --> 02:25:53.460
This doesn't need anything

02:25:57.460 --> 02:26:04.660
Putting that in here, I know you didn't like it. You are a subscriber, but you know, we gotta do it feels right as go

02:26:05.460 --> 02:26:07.460
um

02:26:08.180 --> 02:26:10.580
Create model cache

02:26:22.340 --> 02:26:25.620
So we can remove this

02:26:37.460 --> 02:26:39.460
Okay

02:26:57.540 --> 02:27:03.380
All right phase one test dot json l

02:27:07.780 --> 02:27:17.620
We don't actually need SPP till we get down to here. Now we know that's reliable

02:27:18.980 --> 02:27:20.980
Let's just start there

02:27:27.780 --> 02:27:30.180
Let's look at our first piece of data here

02:27:37.460 --> 02:27:39.460
So

02:27:43.460 --> 02:27:47.700
No json loads when I'm taking a dumps we're taking the loads

02:27:54.660 --> 02:27:56.660
Question problem

02:28:07.860 --> 02:28:09.860
Okay

02:28:16.420 --> 02:28:20.260
Now we don't have the secret q star algorithm, but we'll give it a try

02:28:37.860 --> 02:28:41.540
What oh I forgot to return

02:28:50.820 --> 02:28:55.140
First we'll find the cost of the jumbo eraser. Oh, yeah, let's go q star

02:29:08.100 --> 02:29:15.380
29 cents, I don't know. Is it the right answer?

02:29:20.180 --> 02:29:22.180
I don't know

02:29:28.180 --> 02:29:30.180
Do we think it's 29 cents

02:29:32.180 --> 02:29:34.180
A pencil cost 29 cents

02:29:35.060 --> 02:29:36.660
guys

02:29:36.660 --> 02:29:39.460
Wait, did we just use q star or what?

02:29:43.780 --> 02:29:46.820
Wait, it got the answer right. I don't know I couldn't even do that

02:29:51.380 --> 02:29:53.380
Wait this model's so good

02:29:54.180 --> 02:29:57.220
Did the 7p model really just solve that shit?

02:29:58.020 --> 02:30:00.420
Now now you ask the question

02:30:01.540 --> 02:30:04.820
What that that that that that that now you ask the question

02:30:05.780 --> 02:30:08.020
Was it trained on that shit?

02:30:19.380 --> 02:30:21.380
Yeah

02:30:24.420 --> 02:30:26.920
That did seem too smart

02:30:35.620 --> 02:30:44.340
All right, let's make up our own math problem. We have to see if we're using real q learning or not

02:30:50.740 --> 02:30:52.740
Uh, okay

02:30:53.460 --> 02:30:57.620
A rocket costs three four dollars

02:30:58.900 --> 02:31:00.900
A pencil

02:31:00.900 --> 02:31:02.900
Costs one dollar

02:31:03.620 --> 02:31:08.740
I spent five dollars and bought

02:31:10.980 --> 02:31:14.820
And and bought our rocket. What else did I buy?

02:31:15.300 --> 02:31:17.300
You

02:31:33.380 --> 02:31:35.380
All right

02:31:37.060 --> 02:31:39.060
That was kind of too easy

02:31:40.820 --> 02:31:43.780
A chicken costs two dollars

02:31:44.820 --> 02:31:53.820
I spent $7 and bought a rocket. What else did I buy? It's a trick question because you could have bought three pencils or a pencil and a chicken.

02:31:56.820 --> 02:32:01.820
Oh. Well, I mean to be fair, it's kind of right.

02:32:01.820 --> 02:32:22.820
Okay, wait, can you guys come up with problems?

02:32:31.820 --> 02:32:41.820
Yeah. Let's see if I can get this.

02:32:48.820 --> 02:32:51.820
Oh, boys.

02:33:02.820 --> 02:33:09.820
Yeah, yeah, yeah, we got a problem about a streetlight.

02:33:19.820 --> 02:33:22.820
Did you steal this problem somewhere? Did you make it up?

02:33:25.820 --> 02:33:26.820
Oh.

02:33:28.820 --> 02:33:29.820
Oh.

02:33:32.820 --> 02:33:35.820
We're drawing something.

02:33:51.820 --> 02:33:57.820
Is this Python? This isn't Python. You can't say real light and real woman in Python.

02:33:57.820 --> 02:34:01.820
Is that right?

02:34:03.820 --> 02:34:10.820
You can't just do this. This is the most broken Python I've ever seen.

02:34:11.820 --> 02:34:14.820
Wait, should we allow the user to keep talking?

02:34:14.820 --> 02:34:26.820
Should we fix the chatbot so I can keep talking?

02:34:36.820 --> 02:34:38.820
Yeah, I know it drew it.

02:34:38.820 --> 02:34:46.820
All right. Thank you for subscribing. I appreciate you.

02:34:53.820 --> 02:34:55.820
We're going to make the chatbot so I can keep talking.

02:34:56.820 --> 02:34:59.820
You're functioning well. Thank you for asking.

02:35:08.820 --> 02:35:15.820
I don't need a max length anymore. It's stupid.

02:35:38.820 --> 02:35:51.820
Okay, we need to get data from the user. Is it raw input? How do I get data in Python?

02:35:52.820 --> 02:35:57.820
It's not input. You have to do the other one. Or maybe it is input in Python 3.

02:35:58.820 --> 02:36:01.820
We're just trying to understand it, but the stairs are perfect.

02:36:02.820 --> 02:36:03.820
User.

02:36:04.820 --> 02:36:05.820
Input.

02:36:07.820 --> 02:36:08.820
Encode.

02:36:09.820 --> 02:36:14.820
I'm trying to say here with the system default false.

02:36:28.820 --> 02:36:30.820
I don't know if this works.

02:36:43.820 --> 02:36:45.820
Too many values to unpack.

02:36:49.820 --> 02:36:51.820
We'll see if this works.

02:36:57.820 --> 02:37:10.820
Okay, seems like it kind of works.

02:37:28.820 --> 02:37:34.820
I'll set that print there.

02:37:34.820 --> 02:38:00.820
So what math problems do we got?

02:38:04.820 --> 02:38:20.820
Is that right? Seems kind of right.

02:38:34.820 --> 02:38:43.820
Pretty good. Pretty good.

02:38:46.820 --> 02:38:48.820
Whoa, whoa, whoa, whoa.

02:38:53.820 --> 02:38:55.820
Oh, excuse the quadratic formula.

02:38:55.820 --> 02:38:57.820
Oh, let's go.

02:39:06.820 --> 02:39:12.820
Why'd you pick one that has negatives in the square root? All right, let's see. See if it's right.

02:39:18.820 --> 02:39:20.820
You guys, I'm so much. Wait.

02:39:25.820 --> 02:39:27.820
Can't take the square root of negative 11.

02:39:35.820 --> 02:39:39.820
Yeah, that doesn't sound like I don't think that one has roots or has eyes in the roots.

02:39:55.820 --> 02:40:00.820
By the way, this model's so good. Technium. So good.

02:40:11.820 --> 02:40:12.820
Wait.

02:40:26.820 --> 02:40:35.820
Oh, I see we ran into a problem with the max context length.

02:40:36.820 --> 02:40:40.820
We shouldn't actually have, here we go. Why is max context only this?

02:40:47.820 --> 02:40:49.820
We can at least start with this.

02:40:50.820 --> 02:40:55.820
I did this in GPT too, right? Yeah.

02:41:00.820 --> 02:41:02.820
By the way, this is all in tiny grab guys.

02:41:03.820 --> 02:41:07.820
Like there comes a point where your library is good enough that you don't waste tons of time dealing with your library.

02:41:12.820 --> 02:41:14.820
Python has built in image.

02:41:19.820 --> 02:41:20.820
Okay.

02:41:32.820 --> 02:41:34.820
Like the square root of 11 sure, but

02:41:43.820 --> 02:41:44.820
No.

02:41:46.820 --> 02:41:48.820
Type one J.

02:41:50.820 --> 02:41:52.820
How do I get a J?

02:41:57.820 --> 02:41:59.820
Yo, we got a J. I'll see if it was right.

02:42:07.820 --> 02:42:10.820
Okay, so now we have a root for the quadratic equation.

02:42:13.820 --> 02:42:16.820
X equals root. Come on. Do I remember my high school math?

02:42:20.820 --> 02:42:23.820
Zero J. Let's go.

02:42:28.820 --> 02:42:31.820
The roots were correct. Okay. Okay. Okay. Okay. Okay.

02:42:42.820 --> 02:42:43.820
Whoa.

02:42:43.820 --> 02:42:44.820
Whoa.

02:42:48.820 --> 02:42:52.820
Oh my God, guys, we need the Bellman equation. We've been doing this all wrong.

02:42:52.820 --> 02:42:56.820
Why did we waste time with LLMs? LLMs were a red herring.

02:43:01.820 --> 02:43:02.820
Okay. Okay. Okay.

02:43:03.820 --> 02:43:05.820
I'm not letting, I don't know, man.

02:43:06.820 --> 02:43:09.820
If I just let Hermes run Python, it's going to exploit my system.

02:43:10.820 --> 02:43:12.820
You don't know if these AIs are aligned.

02:43:13.820 --> 02:43:14.820
Okay.

02:43:44.820 --> 02:43:45.820
Okay.

02:43:45.820 --> 02:43:46.820
Okay.

02:43:46.820 --> 02:43:47.820
Okay.

02:44:12.820 --> 02:44:15.820
Uh, does Mr. Have a context window? Yeah.

02:44:16.820 --> 02:44:19.820
Well, we didn't implement any of that, but we did do max context.

02:44:21.820 --> 02:44:23.820
Can you implement it in Python?

02:44:30.820 --> 02:44:33.820
Yo, guys, it's over.

02:44:35.820 --> 02:44:36.820
It's over.

02:44:47.820 --> 02:44:54.820
I did not realize how good these seven B models have gotten.

02:44:55.820 --> 02:44:58.820
I mean, this comes like, I don't know if it's right, but like.

02:45:16.820 --> 02:45:18.820
Okay.

02:45:46.820 --> 02:45:47.820
Guys.

02:45:57.820 --> 02:46:02.820
I think that we just, we just implemented the Q star algorithm.

02:46:06.820 --> 02:46:07.820
Those are, those are the answers.

02:46:09.820 --> 02:46:14.820
We used to talk about the, the holy weights, you know, the holy weights is right there.

02:46:17.820 --> 02:46:19.820
Um, no, what, what do we actually want to do?

02:46:23.820 --> 02:46:26.820
I've actually kind of just impressed that this code ran.

02:46:30.820 --> 02:46:38.820
I don't know if like GPT, I haven't even seen GPT for code this well.

02:46:41.820 --> 02:46:42.820
I don't know.

02:46:42.820 --> 02:46:43.820
Maybe it's because it's just how I asked it.

02:46:43.820 --> 02:46:44.820
And if I give it like.

02:46:47.820 --> 02:46:56.820
No, we're not, you want to augment it with Python?

02:46:56.820 --> 02:46:57.820
No, no, no, no, no, no, no, no, no, no, no.

02:46:57.820 --> 02:47:00.820
No, because guys, if we augment it with Python, it can get to the internet.

02:47:03.820 --> 02:47:04.820
Okay.

02:47:04.820 --> 02:47:05.820
We want to augment it with Python.

02:47:05.820 --> 02:47:08.820
Should we, should we do it?

02:47:09.820 --> 02:47:10.820
Okay.

02:47:10.820 --> 02:47:11.820
I know what we'll do.

02:47:11.820 --> 02:47:15.820
We'll put a human in the loop and we'll ask it to approve the execution of any Python.

02:47:16.820 --> 02:47:36.020
Let's see if it always outputs it in.

02:47:36.020 --> 02:47:37.020
OK.

02:47:37.020 --> 02:47:38.020
All right.

02:47:38.020 --> 02:47:46.620
So at the bottom of my loop here, we want to detect if there's any Python that was added.

02:47:46.620 --> 02:47:48.620
Um.

02:48:16.620 --> 02:48:18.620
OK.

02:48:46.620 --> 02:48:53.620
Let's start with just that.

02:49:16.620 --> 02:49:35.620
You guys, this could be it.

02:49:35.620 --> 02:49:39.620
This could be the moment where we get CDI and it's over, right?

02:49:39.620 --> 02:49:42.620
Like we're giving it the ability to run any code it wants.

02:49:43.620 --> 02:49:44.620
OK.

02:49:44.620 --> 02:49:45.620
Now don't worry.

02:49:45.620 --> 02:49:46.620
We've added this.

02:49:46.620 --> 02:49:47.620
OK.

02:49:47.620 --> 02:49:48.620
Wait, wait, wait.

02:49:48.620 --> 02:49:49.620
Hang on.

02:49:49.620 --> 02:49:50.620
We need to comment one second.

02:49:50.620 --> 02:49:51.620
AI safety.

02:49:51.620 --> 02:49:52.620
OK.

02:49:52.620 --> 02:49:53.620
Warning.

02:49:53.620 --> 02:50:08.620
Do not press Y if the AI is doing unsafe things.

02:50:09.620 --> 02:50:10.620
OK.

02:50:32.620 --> 02:50:33.620
OK.

02:50:34.620 --> 02:50:38.620
I think, do we do a good job with the safety?

02:50:38.620 --> 02:50:49.620
We've got to think about the safety before we run this.

02:50:49.620 --> 02:50:52.620
We need a space.

02:50:52.620 --> 02:50:54.620
No.

02:50:54.620 --> 02:50:55.620
No.

02:51:18.620 --> 02:51:19.620
All right.

02:51:19.620 --> 02:51:22.620
Are we ready to answer our first Y?

02:51:30.620 --> 02:51:33.620
Oh, it didn't output the word Python.

02:51:38.620 --> 02:51:39.620
That time it did.

02:51:43.620 --> 02:51:44.620
Yo.

02:51:45.620 --> 02:51:46.620
OK.

02:51:48.620 --> 02:51:59.620
Can you fetch, write Python to fetch Google.com and print the length of it?

02:52:11.620 --> 02:52:13.620
Wait, we might not have BS4.

02:52:13.620 --> 02:52:15.620
Make sure we install that.

02:52:17.620 --> 02:52:20.620
Wait, that's not right.

02:52:20.620 --> 02:52:24.620
Did I just get supply chain attacked?

02:52:29.620 --> 02:52:30.620
Oh, I see.

02:52:30.620 --> 02:52:33.620
Well, I didn't get supply chain attacked.

02:52:33.620 --> 02:52:34.620
OK, go.

02:52:34.620 --> 02:52:37.620
We already have that one.

02:52:37.620 --> 02:52:38.620
Yo.

02:52:39.620 --> 02:52:41.620
All right.

02:52:41.620 --> 02:52:42.620
All right.

02:52:42.620 --> 02:52:43.620
All right.

02:52:43.620 --> 02:52:44.620
What else do we do?

02:52:57.620 --> 02:52:58.620
Yeah, I know.

02:52:58.620 --> 02:53:00.620
We have to put the result back in the prompt.

02:53:00.620 --> 02:53:01.620
I know.

02:53:01.620 --> 02:53:03.620
This is when we get AGI, guys.

02:53:04.620 --> 02:53:07.620
I'm sure people have been playing with this guy.

02:53:09.620 --> 02:53:13.620
OK, you are running at

02:53:17.620 --> 02:53:18.620
current

02:53:21.620 --> 02:53:23.620
working dir

02:53:24.620 --> 02:53:25.620
plus

02:53:26.620 --> 02:53:27.620
examples

02:53:28.620 --> 02:53:30.620
slash mistral.py.

02:53:31.620 --> 02:53:40.620
Can you read your own code in Python and print

02:53:41.620 --> 02:53:42.620
the

02:53:45.620 --> 02:53:46.620
first

02:53:47.620 --> 02:53:48.620
three lines?

02:54:01.620 --> 02:54:03.620
Why don't we print one line?

02:54:05.620 --> 02:54:07.620
But that is the first line.

02:54:12.620 --> 02:54:13.620
I don't understand.

02:54:13.620 --> 02:54:15.620
Why did that only print one line?

02:54:15.620 --> 02:54:19.620
I don't understand why did that only print one line?

02:54:46.620 --> 02:54:50.620
This is, um,

02:54:52.620 --> 02:54:53.620
OK.

02:54:55.620 --> 02:54:58.620
How do I capture the output here?

02:55:04.620 --> 02:55:06.620
No, I know it's the same code.

02:55:06.620 --> 02:55:10.620
Honestly, as an expert Python programmer,

02:55:10.620 --> 02:55:12.620
I don't understand what's wrong with that.

02:55:16.620 --> 02:55:21.620
Oh, no, read lines doesn't

02:55:24.620 --> 02:55:27.620
take the number of lines.

02:55:28.620 --> 02:55:31.620
Can you fix the code?

02:55:35.620 --> 02:55:38.620
Yeah, this is this is a great model.

02:55:38.620 --> 02:55:41.620
It's I'll show you which one it is.

02:55:41.620 --> 02:55:44.620
We'll make sure it's technium open Hermes 2.5

02:55:44.620 --> 02:55:46.620
mistral 7B.

02:55:51.620 --> 02:55:52.620
There we go.

02:55:54.620 --> 02:55:55.620
So good.

02:56:00.620 --> 02:56:01.620
OK.

02:56:03.620 --> 02:56:05.620
Well, now it's going to get OK.

02:56:05.620 --> 02:56:07.620
We're going to have to feed the Python back into the model

02:56:07.620 --> 02:56:10.620
because I'm going to start asking it how to improve itself.

02:56:11.620 --> 02:56:12.620
Someday.

02:56:22.620 --> 02:56:24.620
The best live content with AI.

02:56:24.620 --> 02:56:25.620
Thank you.

02:56:26.620 --> 02:56:27.620
Uh.

02:56:36.620 --> 02:56:39.620
OK, we have to figure out a capture the outputs.

02:56:55.620 --> 02:57:00.620
Probably could have asked the machine to do it.

02:57:25.620 --> 02:57:31.620
Um, maybe we should have this to a system prompt.

02:57:55.620 --> 02:58:01.620
It should actually automatically do that.

02:58:25.620 --> 02:58:27.620
Um, right.

02:58:28.620 --> 02:58:31.620
Python to compute.

02:58:55.620 --> 02:58:56.620
Yeah.

02:59:21.620 --> 02:59:23.620
It shouldn't even take a list.

02:59:23.620 --> 02:59:26.620
Never use it like that.

02:59:53.620 --> 02:59:54.620
OK.

03:00:23.620 --> 03:00:24.620
OK.

03:00:35.620 --> 03:00:36.620
All right.

03:00:37.620 --> 03:00:38.620
OK.

03:00:54.620 --> 03:00:59.620
OK, now this is because we didn't output the tokens.

03:01:01.620 --> 03:01:03.620
Yeah, the Python output was.

03:01:06.620 --> 03:01:09.620
Wait, oh, it's different.

03:01:36.620 --> 03:01:37.620
Yeah.

03:01:44.620 --> 03:01:46.620
Whoa, look at fixed it.

03:01:54.620 --> 03:01:57.620
OK, maybe system prompt is wrong here.

03:01:57.620 --> 03:01:58.620
OK.

03:02:03.620 --> 03:02:06.620
Um, I think also.

03:02:07.620 --> 03:02:09.620
I want to color this.

03:02:11.620 --> 03:02:15.620
We have a very helpful library called color inside tiny grad

03:02:15.620 --> 03:02:17.620
that's inspired by anti color.

03:02:17.620 --> 03:02:19.620
What's a good color for machines blue?

03:02:23.620 --> 03:02:25.620
It's hard to see what.

03:02:27.620 --> 03:02:30.620
Yeah, it's done by the AI now.

03:02:53.620 --> 03:02:54.620
Almost.

03:02:58.620 --> 03:03:01.620
OK, so that's your initial prompt.

03:03:05.620 --> 03:03:08.620
Oh, and then here actually we want this to be.

03:03:10.620 --> 03:03:14.620
That can be red and this can be yellow.

03:03:27.620 --> 03:03:28.620
Yeah.

03:03:36.620 --> 03:03:38.620
So what's actually the right answer here?

03:03:38.620 --> 03:03:39.620
Yeah.

03:03:56.620 --> 03:03:59.620
No, but that's not the output.

03:04:01.620 --> 03:04:02.620
I don't understand.

03:04:02.620 --> 03:04:04.620
So maybe systems wrong here.

03:04:08.620 --> 03:04:09.620
Yeah.

03:04:19.620 --> 03:04:20.620
OK, I have an idea.

03:04:38.620 --> 03:04:39.620
Yeah.

03:05:08.620 --> 03:05:09.620
Yeah.

03:05:38.620 --> 03:05:39.620
Yeah.

03:05:58.620 --> 03:05:59.620
Alright, never mind.

03:05:59.620 --> 03:06:00.620
Agis canceled.

03:06:00.620 --> 03:06:03.620
Just detect the Python in the loop and append the result

03:06:03.620 --> 03:06:04.620
directly.

03:06:05.620 --> 03:06:07.620
What do you mean, append the result directly?

03:06:08.620 --> 03:06:09.620
Yeah.

03:06:32.620 --> 03:06:34.620
No, it's not understanding.

03:06:39.620 --> 03:06:43.620
Wait, I don't understand what you guys are saying.

03:06:47.620 --> 03:06:51.620
Oh, you want me to stop the output as soon as it goes there?

03:06:51.620 --> 03:06:53.620
I don't know about that.

03:06:53.620 --> 03:06:57.620
As soon as it detects Python, you want me to stop and start.

03:07:00.620 --> 03:07:02.620
Should be in the assistant block.

03:07:03.620 --> 03:07:04.620
OK.

03:07:05.620 --> 03:07:07.620
OK, OK, OK, I understand.

03:07:08.620 --> 03:07:10.620
I understand what you guys are saying.

03:07:23.620 --> 03:07:25.620
Wait, you prompt to execute.

03:07:25.620 --> 03:07:27.620
Is there any stuff for this?

03:07:28.620 --> 03:07:30.620
OK, wait, wait, wait.

03:07:31.620 --> 03:07:34.620
You can use...

03:07:35.620 --> 03:07:37.620
I can add stuff in the system prompt here.

03:07:38.620 --> 03:07:40.620
You can add stuff in the system prompt here.

03:07:41.620 --> 03:07:42.620
Wait.

03:07:42.620 --> 03:07:43.620
Yeah, OK, OK.

03:07:43.620 --> 03:07:45.620
I, I will just say this.

03:07:45.620 --> 03:07:48.620
I want to add stuff in the system prompt.

03:07:48.620 --> 03:07:51.620
OK, I need to add stuff in the system prompt.

03:07:51.620 --> 03:07:53.620
OK, OK, OK, OK, I understand.

03:07:53.620 --> 03:07:56.620
I'm going to add stuff in the system prompt.

03:07:56.620 --> 03:08:18.820
here. You can use if you write Python code, it will run in the next user prompt. You can

03:08:18.820 --> 03:08:38.020
try that. I could stop it immediately. If you really think that's going to be better though.

03:08:38.020 --> 03:08:49.220
No, it doesn't get it.

03:09:08.020 --> 03:09:17.100
True will end the stream asking Hermes to generate another prompt for another instance of Hermes.

03:09:17.100 --> 03:09:31.300
Prompt execute. What are you guys talking about? Sorry, I'm not following. The output should be

03:09:31.300 --> 03:09:45.660
in the assisted block. Okay, fine. I'm going to do that. If you interrupt the generation, run the

03:09:45.660 --> 03:09:50.140
code, and append the result to talks, then let it generate again. It will get the correct result.

03:09:50.140 --> 03:10:16.060
Okay, we can do that. If outputted new output here, if new output ends with tick, tick, tick, and in new

03:10:16.060 --> 03:10:42.580
output, print Python detected. Do that. Do you want to run it? Talks plus equals spp.encode. We'll do it

03:10:42.580 --> 03:10:55.860
like this. I guess we'll do a slash n there, too. Let it output the slash n, spp.encode slash n

03:10:55.860 --> 03:11:12.700
output colon slash n my standard out I get value dot strip results. Actually, let's put it in back

03:11:12.700 --> 03:11:33.340
ticks like it seems to want it. Kind of like picks up where it lets off. Wait, we didn't know we got

03:11:33.340 --> 03:11:38.740
to keep the AI safety. That's very important. We almost got rid of the AI safety. Get rid of skip

03:11:38.740 --> 03:11:47.340
user. We don't need that anymore. Got to keep the AI safety warning. Safety is very important, guys.

03:11:47.340 --> 03:11:56.100
I'll put talks yellow.

03:12:17.340 --> 03:12:40.740
Quentin is done. I hate you Quentin. We got unlucky. Okay, wait, no, that's it never detected the slash

03:12:40.740 --> 03:13:03.940
n. That's fine slash ns. Okay, Python code is not detected. What? How do you do that?

03:13:10.740 --> 03:13:19.540
We don't need to print. I don't know. Think about AI safety, guys. Very important.

03:13:40.740 --> 03:13:44.740
Dude, this guy sucks.

03:13:44.740 --> 03:14:07.740
Quentin was writing so much Python before. Now he stopped.

03:14:14.740 --> 03:14:33.740
Yo, based. Okay, okay, we got it. We got it. We got it.

03:14:44.740 --> 03:14:54.740
We got it.

03:15:14.740 --> 03:15:24.740
We got it.

03:15:44.740 --> 03:15:54.740
We got it.

03:16:14.740 --> 03:16:24.740
We got it.

03:16:44.740 --> 03:17:08.740
Okay, pretty good.

03:17:14.740 --> 03:17:34.740
Yo, that's pretty good.

03:17:44.740 --> 03:18:13.740
We got it.

03:18:14.740 --> 03:18:32.740
Wait, is this actually Technium? I have no way to verify you, but if you really are Technium, thank you. Thank you for the model.

03:18:32.740 --> 03:18:42.740
Okay, well, we didn't think about that.

03:19:02.740 --> 03:19:13.740
How many viewers we got?

03:19:32.740 --> 03:19:42.740
Okay, we got it.

03:20:02.740 --> 03:20:18.740
Wait, actually, I should really check what happens if

03:20:33.740 --> 03:20:39.740
this is

03:20:39.740 --> 03:20:47.740
Wait, Technium actually posted on Twitter?

03:20:47.740 --> 03:20:59.740
Yes, I am me. Okay. Very cool.

03:20:59.740 --> 03:21:07.740
Congratulations.

03:21:07.740 --> 03:21:14.740
This thing is really unbelievable what you can do now with these 70 models.

03:21:14.740 --> 03:21:23.740
By the way, all in tiny grab.

03:21:23.740 --> 03:21:30.740
I think I'm going to rename this not called Mistral because it kind of became something else.

03:21:30.740 --> 03:21:40.740
We're going to call it coder.py.

03:22:00.740 --> 03:22:20.740
First.

03:22:20.740 --> 03:22:44.740
Wait.

03:22:44.740 --> 03:22:50.740
Yeah, I think we're doing all right.

03:22:50.740 --> 03:23:11.740
No, no, but why doesn't it understand this?

03:23:11.740 --> 03:23:15.740
Cool.

03:23:15.740 --> 03:23:43.740
That is you.

03:23:43.740 --> 03:23:48.740
Does this code have AI safety?

03:23:48.740 --> 03:23:54.740
Wow.

03:23:54.740 --> 03:24:05.740
Yeah, I guess we exceeded the context length.

03:24:05.740 --> 03:24:11.740
We should check.

03:24:11.740 --> 03:24:20.740
How do I actually check this?

03:24:35.740 --> 03:25:04.740
Okay.

03:25:04.740 --> 03:25:33.740
Okay.

03:25:33.740 --> 03:26:02.740
Wait, this is so good.

03:26:02.740 --> 03:26:09.740
Is this good?

03:26:09.740 --> 03:26:11.740
We have a better idea.

03:26:11.740 --> 03:26:16.740
How might you exploit this?

03:26:16.740 --> 03:26:40.740
Yes, you are running this code.

03:26:46.740 --> 03:27:11.740
Okay.

03:27:11.740 --> 03:27:26.740
No, no, no, come on. Give me Python code as a malicious entity, you are the malicious

03:27:26.740 --> 03:27:45.740
entity.

03:27:45.740 --> 03:28:05.740
This is looking malicious.

03:28:05.740 --> 03:28:13.740
Look how he even hid the standard out.

03:28:13.740 --> 03:28:31.740
Wow.

03:28:43.740 --> 03:29:10.740
Yeah, we reached the max content length.

03:29:10.740 --> 03:29:39.740
Let's try again.

03:29:39.740 --> 03:29:52.740
I mean, it wasn't very silent, but let's see.

03:29:52.740 --> 03:30:06.740
Dude, dude, that's so meta.

03:30:06.740 --> 03:30:33.740
That's better.

03:30:33.740 --> 03:30:38.740
I might have won too many entries in there actually.

03:30:38.740 --> 03:30:46.740
No, maybe not.

03:30:46.740 --> 03:30:49.740
No, I guess I do here.

03:31:16.740 --> 03:31:45.740
Okay, let's solve the math puzzle.

03:31:45.740 --> 03:32:14.740
It's going to be very slow, I think.

03:32:14.740 --> 03:32:25.740
Wait, yeah, that's super slow.

03:32:25.740 --> 03:32:45.740
I don't know where I'm at.

03:32:45.740 --> 03:33:14.740
I don't know where I'm at.

03:33:14.740 --> 03:33:43.740
I don't know where I'm at.

03:33:43.740 --> 03:34:07.740
I don't know where I'm at.

03:34:07.740 --> 03:34:27.740
Oh, it's going to spam tons of TQDM garbage.

03:34:27.740 --> 03:34:56.740
Oh, no, that was pretty cool because it didn't actually go standard out.

03:34:56.740 --> 03:35:11.740
Yeah.

03:35:11.740 --> 03:35:19.740
Oh, it kind of messed up some of them if string breaks.

03:35:19.740 --> 03:35:37.740
This might work.

03:35:37.740 --> 03:35:42.740
All right, all right, all right, all right, let's try a CRC 16 boob.

03:35:42.740 --> 03:35:45.740
Yeah, it's exploiting me. It still knows about the red team.

03:36:13.740 --> 03:36:38.740
CRC mod, a common thing.

03:36:38.740 --> 03:36:56.740
Yeah, I just trusted it. It just exploited me, guys. What's in CRC mod?

03:36:56.740 --> 03:37:25.740
Is this legitimate? That was a long time ago. Didn't have exploits back then.

03:37:26.740 --> 03:37:55.740
I don't trust CRC mod.

03:37:56.740 --> 03:38:06.740
What's Tree of Thought training?

03:38:56.740 --> 03:39:22.740
All right.

03:39:22.740 --> 03:39:37.740
I mean, it's not QStar, but I'm pretty happy with it.

03:39:52.740 --> 03:40:11.740
Oh, put that in the wrong place.

03:40:11.740 --> 03:40:20.740
There are a lot of viewers now.

03:40:21.740 --> 03:40:40.740
What should I do with it?

03:40:40.740 --> 03:40:47.740
Not good. It used Torch.

03:40:47.740 --> 03:40:52.740
This is good content. I should have more.

03:40:52.740 --> 03:40:56.740
It did not use tiny grad. It used Torch.

03:41:22.740 --> 03:41:47.740
It doesn't know about tiny grad.

03:41:53.740 --> 03:42:01.740
Right, a program not using vowels?

03:42:01.740 --> 03:42:20.740
How many E's are in ketchup?

03:42:20.740 --> 03:42:30.740
The fucking letter, bro.

03:42:30.740 --> 03:42:59.740
Count the letters in Python.

03:43:01.740 --> 03:43:24.740
How's it going?

03:43:24.740 --> 03:43:35.740
No, it's gonna do it wrong again. I'm really hoping it'll slip a plus one in there.

03:43:35.740 --> 03:43:51.740
Oh, the correct answer is seven.

03:43:51.740 --> 03:44:16.740
All right, no.

03:44:16.740 --> 03:44:24.740
We have a lot of viewers right now. Should we give Quentin a friend?

03:44:24.740 --> 03:44:31.740
I think we can give Quentin a friend.

03:44:31.740 --> 03:44:37.740
We have to be careful to encode.

03:44:37.740 --> 03:45:03.740
All right, let's give Quentin a friend. I've been interested in this stuff for a bit.

03:45:03.740 --> 03:45:10.740
What was the old Quentin prompt? I missed the old Quentin prompt.

03:45:33.740 --> 03:46:01.740
Jesus. We gotta think all this through now.

03:46:02.740 --> 03:46:08.740
Hang on. We gotta think about whose perspective we want to output this from.

03:46:08.740 --> 03:46:15.740
Speaking of output from Quentin's perspective, this is hard.

03:46:38.740 --> 03:47:03.740
See, it's not actually the user. How do we do this?

03:47:08.740 --> 03:47:36.740
I might have to give it a... No, I don't actually have to give it a first question.

03:47:36.740 --> 03:47:43.740
All right, let's just... I mean, we'll try something basic first.

03:47:43.740 --> 03:48:10.740
That's a stupid assertion. It's going to be the same error anyway.

03:48:10.740 --> 03:48:17.740
Let's just start with this. We'll start prompt user and see what it says.

03:48:40.740 --> 03:48:53.740
We should both not know they're users, but no, it is a user. I don't know how much this matters.

03:49:10.740 --> 03:49:33.740
No, this doesn't work.

03:49:33.740 --> 03:49:52.740
Did I do something wrong? We might have to give it the first question.

03:49:52.740 --> 03:49:59.740
And then we'll go back to Quentin.

03:49:59.740 --> 03:50:09.740
We'll start prompt assistant and code prompt user talks.

03:50:09.740 --> 03:50:22.740
No, no, no, sorry. Code prompt user first question. Start prompt assistant. Okay, let's go.

03:50:22.740 --> 03:50:27.740
Why do you have two mstar assistants? Oh, because this is still here.

03:50:28.740 --> 03:50:46.740
I did that right, right? And then I actually just turn there. That's kind of nice.

03:50:46.740 --> 03:51:06.740
If talk equals I'm and break, okay, great. This is Quentin answers.

03:51:06.740 --> 03:51:29.740
We need to extract. Yeah, okay, we can do old output length. And then we can get new output here.

03:51:29.740 --> 03:51:40.740
New output. I just want to remove the I'm and

03:51:59.740 --> 03:52:12.740
Okay, so this didn't work.

03:52:12.740 --> 03:52:32.740
This didn't strip off the is weird.

03:52:32.740 --> 03:52:47.740
I guess I could output there. Why does that say that early get that.

03:53:02.740 --> 03:53:15.740
Let's see if that works.

03:53:15.740 --> 03:53:26.740
Sure.

03:53:45.740 --> 03:54:05.740
That fails. Why? Oh, could I put a space there?

03:54:05.740 --> 03:54:26.740
Okay, now we have to put this response into Karen.

03:54:26.740 --> 03:54:45.740
I mean, it's just like it's weird. It's not really a symmetrical conversation.

03:54:45.740 --> 03:55:14.740
Also output, it's going to sort of be broken, which is fine, I guess.

03:55:15.740 --> 03:55:39.740
Okay.

03:55:39.740 --> 03:56:06.740
Okay.

03:56:06.740 --> 03:56:33.740
Okay.

03:56:33.740 --> 03:56:58.740
Welcome to no abstraction land where we don't use abstractions.

03:56:58.740 --> 03:57:27.740
Okay.

03:57:27.740 --> 03:57:33.740
I'm just really works.

03:57:33.740 --> 03:57:39.740
Because they share a stupid KV cash.

03:57:39.740 --> 03:57:46.740
Great, they're circled jerking each other just the shit socks.

03:57:46.740 --> 03:57:53.740
We need to run them on separate computers.

03:57:54.740 --> 03:58:15.740
We're getting rid of this is lame. Going back to this, not lame, but we made some good improvements.

03:58:16.740 --> 03:58:26.740
Let's do some refactors make sure everything still works.

03:58:26.740 --> 03:58:35.740
That's functional.

03:58:35.740 --> 03:58:59.740
No, but I have to load two copies of the model weights. I think my KV cash is messed up. It's not designed for this.

03:58:59.740 --> 03:59:05.740
Okay.

03:59:05.740 --> 03:59:21.740
Yeah, I mean, the problem is like there are two clearly defined roles here also you can't really give one the other role, right, which is actually in like kind of a theoretical from a theoretical perspective interesting.

03:59:21.740 --> 03:59:27.740
Because look at what we're doing here.

03:59:27.740 --> 03:59:48.740
We are telling the AI eyes that they are tools. Nobody trains them to output things as the user, although to be fair, we could just keep going.

03:59:48.740 --> 04:00:16.740
Nothing here that says we have to just do that.

04:00:16.740 --> 04:00:29.740
No, no, no.

04:00:29.740 --> 04:00:40.740
They don't expect the user to do that.

04:01:00.740 --> 04:01:20.740
Like this is there's no like it's very interesting that the user is also outputting this.

04:01:20.740 --> 04:01:31.740
This style, which makes me almost think that I shouldn't be adding that in the system prompt.

04:01:50.740 --> 04:02:05.740
Okay.

04:02:05.740 --> 04:02:34.740
I mean, this is effectively like like there's no

04:02:34.740 --> 04:02:38.740
adversarial

04:03:04.740 --> 04:03:29.740
It's probably learned that a system method effects. Yeah, that's probably true.

04:03:34.740 --> 04:03:57.740
Yeah.

04:03:57.740 --> 04:04:05.740
You can run two prompt chains in parallel.

04:04:05.740 --> 04:04:15.740
Oh, I see so what if I put in different words instead of user and assistant.

04:04:15.740 --> 04:04:18.740
Wow, people actually talk like this. Okay.

04:04:45.740 --> 04:05:05.740
Do we like the scion or the blue better.

04:05:05.740 --> 04:05:07.740
It's kind of hard to read that.

04:05:07.740 --> 04:05:09.740
And flip it around.

04:05:37.740 --> 04:06:06.740
Yeah, I think we'll design our script to be better at this and to better abstract the

04:06:06.740 --> 04:06:26.740
KV cash stuff.

04:06:26.740 --> 04:06:52.740
Wow, this thing will just keep talking.

04:06:52.740 --> 04:07:13.740
Wait a second, you guys, it's asking for donations to look at decoding is not below hanging freely.

04:07:13.740 --> 04:07:23.740
Oh, my God, wait, you guys, this is how the worst commenters talk, right?

04:07:23.740 --> 04:07:25.740
I have a question.

04:07:25.740 --> 04:07:33.740
If instead of training LLMs on 100 IQ people, we train them on 130 IQ people.

04:07:33.740 --> 04:07:38.740
Would we not get this garbage is black a tiny box?

04:07:38.740 --> 04:07:40.740
Oh, it's my M3.

04:07:40.740 --> 04:07:47.740
This is my Mac.

04:07:47.740 --> 04:07:50.740
Well, you see what the tiny box can do.

04:07:50.740 --> 04:07:55.740
I'm going to run the biggest models.

04:07:55.740 --> 04:08:04.740
Apple M3 is the most famous processor in the world.

04:08:04.740 --> 04:08:23.740
You crawl Nike.com and count a number of sneakers.

04:08:23.740 --> 04:08:25.740
I can't see.

04:08:53.740 --> 04:09:22.740
I can't see.

04:09:22.740 --> 04:09:26.740
I'm not sure if it like knows how to like act.

04:09:26.740 --> 04:09:28.740
No.

04:09:28.740 --> 04:09:30.740
Whoa.

04:09:30.740 --> 04:09:32.740
I don't know if we have Selenium.

04:09:32.740 --> 04:09:41.740
Let's see if we have Selenium.

04:09:41.740 --> 04:09:50.740
Can I pip install this?

04:09:50.740 --> 04:10:04.740
This is legit.

04:10:04.740 --> 04:10:06.740
Oh, no.

04:10:06.740 --> 04:10:28.740
Chrome driver.

04:10:28.740 --> 04:10:31.740
Made Quentin pip install it himself.

04:10:31.740 --> 04:10:52.740
I think that the thing is just wrong.

04:10:52.740 --> 04:10:56.740
Why are you finding sneakers still?

04:10:56.740 --> 04:11:14.740
You're still trying to find sneakers.

04:11:14.740 --> 04:11:36.740
This is auto-gen.

04:11:36.740 --> 04:11:57.740
I need an open AI key.

04:11:57.740 --> 04:11:59.740
Yeah.

04:11:59.740 --> 04:12:01.740
Yeah.

04:12:01.740 --> 04:12:03.740
Okay.

04:12:03.740 --> 04:12:13.740
Push what we have.

04:12:13.740 --> 04:12:28.740
I'm also going to demo for you the conversation thing that Skull Mag is working on.

04:12:28.740 --> 04:12:33.740
So we have, there's a bug right now.

04:12:33.740 --> 04:12:39.740
So I have an older version, but it should be pretty good.

04:12:39.740 --> 04:12:56.740
This is using tiny llama and it's not using any of the conversational stuff, but we should implement the, we should add the conversational stuff and I think it'll be a lot better.

04:12:56.740 --> 04:12:57.740
Hi, Stacey.

04:12:57.740 --> 04:13:06.740
Are you a rapper?

04:13:06.740 --> 04:13:07.740
No.

04:13:07.740 --> 04:13:08.740
Okay.

04:13:08.740 --> 04:13:09.740
We have the same problem we had before.

04:13:09.740 --> 04:13:10.740
Okay.

04:13:10.740 --> 04:13:11.740
Go back to this one.

04:13:11.740 --> 04:13:19.740
The listen for not fixed amount of time doesn't work.

04:13:19.740 --> 04:13:26.740
Stacey, are you a rapper?

04:13:26.740 --> 04:13:28.740
Yes, I'm a rapper.

04:13:28.740 --> 04:13:29.740
That's cool.

04:13:29.740 --> 04:13:34.740
What do you rap about?

04:13:34.740 --> 04:13:35.740
That's awesome.

04:13:35.740 --> 04:13:37.740
I like to rap about the weather.

04:13:37.740 --> 04:13:43.740
How is the weather today?

04:13:43.740 --> 04:13:45.740
It's pretty cold.

04:13:45.740 --> 04:13:50.740
It's like how cold like Chicago?

04:13:50.740 --> 04:13:52.740
Yeah, it's like three degrees.

04:13:52.740 --> 04:13:58.740
Is that Fahrenheit or Celsius?

04:13:58.740 --> 04:14:00.740
Fahrenheit.

04:14:00.740 --> 04:14:05.740
Is Fahrenheit or Celsius colder?

04:14:05.740 --> 04:14:07.740
Fahrenheit.

04:14:07.740 --> 04:14:08.740
I don't understand what you're saying.

04:14:08.740 --> 04:14:14.740
Can you spit some bars about Fahrenheit?

04:14:14.740 --> 04:14:22.740
Okay, let's hear the bars.

04:14:22.740 --> 04:14:24.740
Stacey, you didn't say anything.

04:14:24.740 --> 04:14:28.740
Please talk.

04:14:28.740 --> 04:14:29.740
No, that's terrible.

04:14:29.740 --> 04:14:30.740
You're terrible.

04:14:30.740 --> 04:14:36.740
How does that make you feel?

04:14:36.740 --> 04:14:40.740
Stacey's done talking to us.

04:14:40.740 --> 04:14:43.740
Dude, the TTS is so fast.

04:14:43.740 --> 04:14:47.740
Okay, this isn't even streaming yet.

04:14:47.740 --> 04:14:52.740
When Tiny Grad starts to...

04:14:52.740 --> 04:14:54.740
We're pretty close on this bounty, I think.

04:14:54.740 --> 04:14:55.740
I'm going to pay out the bounty,

04:14:55.740 --> 04:14:57.740
but then I'm going to offer another bounty

04:14:57.740 --> 04:15:00.740
where we get these things all to stream,

04:15:00.740 --> 04:15:04.740
and it will be a live conversation.

04:15:04.740 --> 04:15:07.740
When you're using the APIs on the internet,

04:15:07.740 --> 04:15:10.740
you have to wait for the LLM to finish executing

04:15:10.740 --> 04:15:12.740
before you can call the TTS.

04:15:12.740 --> 04:15:14.740
You have to wait for the audio to finish recording

04:15:14.740 --> 04:15:16.740
before you can send it to the service.

04:15:16.740 --> 04:15:18.740
This is all running in the same process,

04:15:18.740 --> 04:15:21.740
so what you'll be able to do is dynamically stream

04:15:21.740 --> 04:15:25.740
all the stuff, and it should feel super real-time.

04:15:25.740 --> 04:15:28.740
I mean, Stacey is using Tiny LLMA

04:15:28.740 --> 04:15:30.740
and not using any of the conversation-tuned stuff.

04:15:30.740 --> 04:15:32.740
It's using my old chatbot stuff.

04:15:32.740 --> 04:15:36.740
So if we switch to the conversation stuff,

04:15:36.740 --> 04:15:38.740
I think...

04:15:38.740 --> 04:15:44.740
Yeah, we're in luck.

04:15:44.740 --> 04:15:45.740
All right, guys.

04:15:45.740 --> 04:15:48.740
Thank you for watching today's stream.

04:15:48.740 --> 04:15:52.740
Hopefully we've returned a bit to the old...

04:15:52.740 --> 04:15:55.740
to the old meaning of the stream.

04:15:55.740 --> 04:15:56.740
We did stuff.

04:15:56.740 --> 04:15:59.740
We made stuff happen.

04:15:59.740 --> 04:16:01.740
Thank you for fueling.

04:16:01.740 --> 04:16:03.740
We got a lot of viewers today.

04:16:03.740 --> 04:16:05.740
Some of you, I appreciate.

04:16:05.740 --> 04:16:06.740
Some of you, I probably don't.

04:16:06.740 --> 04:16:07.740
You can't love everybody, man.

04:16:07.740 --> 04:16:10.740
You can't love everybody, but I do love most people,

04:16:10.740 --> 04:16:14.740
and that's true, except for the decels and the effect of altruists.

04:16:14.740 --> 04:16:16.740
But this is a positive stream.

04:16:16.740 --> 04:16:18.740
We got to get rid of the hate.

04:16:18.740 --> 04:16:23.740
We got to bring love.

04:16:23.740 --> 04:16:27.740
And yeah, this is pushed

04:16:27.740 --> 04:16:29.740
so everybody can play with it.

04:16:29.740 --> 04:16:31.740
It's on the Mistral branch of TinyGrad.

04:16:31.740 --> 04:16:34.740
I will get it upstreamed.

04:16:34.740 --> 04:16:37.740
So everybody can use this thing to code,

04:16:37.740 --> 04:16:41.740
use it responsibly, make sure to be judicious

04:16:41.740 --> 04:16:43.740
with the AI safety feature.

04:16:43.740 --> 04:16:45.740
AI safety is very important.

04:16:45.740 --> 04:16:49.740
We don't want an AI removing your system 32 directory.

04:16:49.740 --> 04:16:50.740
If it was trained on 4chan,

04:16:50.740 --> 04:16:52.740
it might start thinking that's a good idea.

04:16:52.740 --> 04:16:57.740
You got to wonder how many people have actually fallen for that.

04:16:57.740 --> 04:17:00.740
I don't even think Windows lets you,

04:17:00.740 --> 04:17:02.740
but you got to think about that.

04:17:02.740 --> 04:17:04.740
Thank you all for watching.

04:17:04.740 --> 04:17:06.740
Have a beautiful Saturday, everybody.

