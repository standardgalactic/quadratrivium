1
00:00:00,000 --> 00:00:04,320
So, our first speaker is Yann LeCun.

2
00:00:04,320 --> 00:00:10,280
He's currently the Silver Professor at the current Institute of Mathematical Sciences

3
00:00:10,280 --> 00:00:16,880
at NYU, where he is the founding director of the NYU Center for Data Science.

4
00:00:16,880 --> 00:00:22,800
He is also affiliated with MEDA, formerly known as Facebook, as the Vice President and Chief

5
00:00:22,800 --> 00:00:25,000
AI Scientist there.

6
00:00:25,000 --> 00:00:29,720
By the way, MEDA just released one of its large language models just a few days ago called

7
00:00:29,720 --> 00:00:34,120
LAMAS II, which I encourage you all to explore.

8
00:00:34,120 --> 00:00:39,360
And this could be a very long introduction because Yann has a very long resume, but the

9
00:00:39,360 --> 00:00:45,160
one thing I wanted to highlight is that Yann was the recipient of the 2018 Turing Award.

10
00:00:45,160 --> 00:00:48,600
For those of you who aren't familiar with it, it's kind of like the Nobel Prize in Computer

11
00:00:48,600 --> 00:00:55,480
Science, and he received that for his work on deep learning and convolutional neural networks.

12
00:00:55,480 --> 00:00:59,440
And something interesting I learned is a lot of the work he did, early work on convolutional

13
00:00:59,440 --> 00:01:03,580
neural networks, was actually when he was at Bell Labs, which is something physicists

14
00:01:03,580 --> 00:01:06,200
know quite well, I think.

15
00:01:06,200 --> 00:01:10,880
So I think you're all eager to hear from Yann, so I'm going to hand it over to him here.

16
00:01:10,880 --> 00:01:11,880
Thank you.

17
00:01:11,880 --> 00:01:15,840
Okay, I'm going to talk about, so this is going to be a somewhat technical talk, but

18
00:01:15,840 --> 00:01:22,840
not very technical, but to tell you less about the possibilities that are offered by LLM

19
00:01:22,840 --> 00:01:24,640
and more about their limitations.

20
00:01:24,640 --> 00:01:29,720
And basically tell you about what I think is coming next, though at least what I'm working

21
00:01:29,720 --> 00:01:32,400
towards coming next.

22
00:01:32,400 --> 00:01:37,160
And the first thing we should realize is that machine learning really sucks compared to what

23
00:01:37,160 --> 00:01:39,080
we observe in humans and animals.

24
00:01:39,080 --> 00:01:43,760
The capabilities of the learning systems that we have today are really terrible.

25
00:01:43,760 --> 00:01:48,200
Humans and animals can run new tasks really quickly, understand how the world works, they

26
00:01:48,200 --> 00:01:53,280
can reason, they can plan, they have some level of common sense.

27
00:01:53,280 --> 00:01:58,760
Their behavior is driven by objectives or drives, which is not the case for auto-reversed

28
00:01:58,760 --> 00:01:59,760
LLMs.

29
00:01:59,760 --> 00:02:05,920
But there is one thing that both the biological world and the recent machine learning world

30
00:02:05,920 --> 00:02:09,440
have had in common is the use of cell supervised learning.

31
00:02:09,440 --> 00:02:13,240
And really cell supervised learning has taken over the world.

32
00:02:13,240 --> 00:02:17,720
For both applications in text and natural language understanding, for images, videos,

33
00:02:17,720 --> 00:02:21,440
3D models, speech, protein folding, all that stuff.

34
00:02:21,440 --> 00:02:23,200
What is cell supervised learning really?

35
00:02:23,200 --> 00:02:29,520
It's sort of completion really, learning to fill in the blanks, right?

36
00:02:29,520 --> 00:02:36,480
So the way it's used in the context of natural language understanding or processing is you

37
00:02:36,480 --> 00:02:43,520
take a piece of text, you mask part of it by removing some other words, masking, replacing

38
00:02:43,520 --> 00:02:46,320
them by blank markers, for example.

39
00:02:46,320 --> 00:02:51,520
And then so think of it as a type of corruption, it doesn't have to be the masking but it could

40
00:02:51,520 --> 00:02:53,560
be other types of corruptions.

41
00:02:53,560 --> 00:02:59,240
And then you train some gigantic neural net to predict the words that are missing.

42
00:02:59,240 --> 00:03:05,840
And you just measure the reconstruction error basically on the parts that were missing.

43
00:03:05,840 --> 00:03:14,280
In the process of doing so, that system learns to represent text in a way that allows it

44
00:03:14,280 --> 00:03:26,480
to store or represent meaning, grammar, everything, syntax, semantics, everything there is to

45
00:03:26,480 --> 00:03:32,760
represent about language in the internal representation, which you can subsequently use for any downstream

46
00:03:32,760 --> 00:03:37,520
task like say translation or topic classification or something of that type.

47
00:03:37,520 --> 00:03:43,000
So this works amazingly well in the context of text, particularly because text is easy

48
00:03:43,080 --> 00:03:49,120
to predict with uncertainty, you can never predict exactly what word will appear at a

49
00:03:49,120 --> 00:03:54,440
particular location, but what you can do is predict some sort of probability distribution

50
00:03:54,440 --> 00:03:56,440
of all possible words in the dictionary.

51
00:03:56,440 --> 00:04:01,200
And you can do this because there's only a finite number of words in your dictionary

52
00:04:01,200 --> 00:04:02,720
or tokens.

53
00:04:02,720 --> 00:04:07,360
And so you can compute this distribution easily and handle uncertainty in the prediction pretty

54
00:04:07,360 --> 00:04:08,360
well.

55
00:04:08,920 --> 00:04:13,480
More generally, self-supervised learning is really sort of learning to capture dependencies

56
00:04:13,480 --> 00:04:17,600
between inputs.

57
00:04:17,600 --> 00:04:23,520
And so if you wanted to apply this to the problem of video prediction, for example, you would

58
00:04:23,520 --> 00:04:28,400
show a segment of video to a system and then ask it to predict what's going to happen next,

59
00:04:28,400 --> 00:04:32,120
for example, in the video and then reveal the future of the video.

60
00:04:32,120 --> 00:04:34,760
And again, I apologize for the colors.

61
00:04:35,760 --> 00:04:39,480
And then the system could adapt itself so that it does a better job at predicting what

62
00:04:39,480 --> 00:04:40,680
happened next in the video.

63
00:04:40,680 --> 00:04:44,840
Now, unfortunately, it's much harder to do for video than it is for text.

64
00:04:44,840 --> 00:04:51,080
So much harder than it might require other methods than the type of generative methods

65
00:04:51,080 --> 00:04:52,360
that work well for text.

66
00:04:52,360 --> 00:04:53,760
I'll come back to this.

67
00:04:53,760 --> 00:04:59,440
So speaking of generative methods, but generative AI and autoregressive language models is something

68
00:04:59,440 --> 00:05:03,880
that many of us have been hearing about recently.

69
00:05:05,280 --> 00:05:06,880
What is it?

70
00:05:06,880 --> 00:05:11,560
Probably most of you know already, but essentially the way you train them is very similar to

71
00:05:11,560 --> 00:05:12,760
the self-supervised learning.

72
00:05:12,760 --> 00:05:16,440
It's in fact a special case of self-supervised learning method I just mentioned.

73
00:05:16,440 --> 00:05:23,760
You take a sequence of tokens, words, whatever it is, a sequence of vectors.

74
00:05:23,760 --> 00:05:26,920
As long as you can turn things into vectors, you're OK.

75
00:05:26,920 --> 00:05:33,240
And then you only mask the last one and train a system to predict the last token in the

76
00:05:33,240 --> 00:05:34,240
sequence.

77
00:05:34,240 --> 00:05:40,440
I mean, technically, you do more than that, but that's what it comes down to in the end.

78
00:05:40,440 --> 00:05:44,200
And once you have a system that has been trained to produce the next token, you can use it

79
00:05:44,200 --> 00:05:50,480
autoregressively, recursively, basically to predict then the next next token and et cetera.

80
00:05:50,480 --> 00:05:55,080
So you predict the next token, you shift it into the input, then predict the next next

81
00:05:55,080 --> 00:05:58,360
token, shift that into the input, et cetera.

82
00:05:58,360 --> 00:06:00,200
And that's called autoregressive prediction.

83
00:06:00,200 --> 00:06:06,160
It's an old concept going back to signal processing many years ago, many decades ago, as a matter

84
00:06:06,160 --> 00:06:07,160
of fact.

85
00:06:07,160 --> 00:06:12,320
So nothing new there, but that allows the system to basically predict one token after the other

86
00:06:12,320 --> 00:06:14,440
and generate text.

87
00:06:14,440 --> 00:06:19,640
So those things work amazingly well.

88
00:06:19,640 --> 00:06:20,640
Performance is really amazing.

89
00:06:20,640 --> 00:06:24,720
The fact that, you know, they're trained only on text, even though on enormous amounts

90
00:06:24,720 --> 00:06:32,880
of text, but only on text, the amount of knowledge, if you want, that they capture from text is

91
00:06:32,880 --> 00:06:37,760
pretty amazing and surprised a lot of people.

92
00:06:37,760 --> 00:06:43,080
Those systems typically have billions or up to hundreds of billions of parameters.

93
00:06:43,080 --> 00:06:47,120
They train typically on one to two trillion tokens.

94
00:06:47,120 --> 00:06:48,120
Sometimes more.

95
00:06:48,200 --> 00:06:58,360
Their input window is anywhere between 2000 and maybe a few tens of thousands of tokens

96
00:06:58,360 --> 00:07:01,760
for their context window.

97
00:07:01,760 --> 00:07:06,800
And there's been a long history of such models that have been put out, the sort of GPT family,

98
00:07:06,800 --> 00:07:13,920
starting with GPT-123, from FAIR, there's been Blunderbot, Galactica, Llama, Version

99
00:07:14,720 --> 00:07:20,040
2 that just came out this week, Alpaca from Stanford, which is a fine-tuned version of

100
00:07:20,040 --> 00:07:26,240
Llama, Llama, Version 1, Lambda and Bard from Google, Chinche from DeepMind, you know, and

101
00:07:26,240 --> 00:07:31,160
of course, GPDT, GPT-4 from OpenAI.

102
00:07:31,160 --> 00:07:37,640
And they're really good at, as writing aids, but they have really limited knowledge of

103
00:07:37,640 --> 00:07:42,600
the underlying reality because they're purely trained from text, at least for the vast majority

104
00:07:42,600 --> 00:07:43,600
of them.

105
00:07:43,600 --> 00:07:47,520
And they really have no common sense or very limited common sense, and they have limited

106
00:07:47,520 --> 00:07:52,000
abilities to plan their answers because the answers are produced autoregressively.

107
00:07:52,000 --> 00:07:58,680
But still, it's pretty impressive how they work, so, as was mentioned, my colleagues

108
00:07:58,680 --> 00:08:02,840
just put out an open source LLM called Llama 2.

109
00:08:02,840 --> 00:08:08,960
There is three versions of this at the moment, 7 billion, 13 billion and 70 billion parameters.

110
00:08:08,960 --> 00:08:12,480
The license is fairly liberal, so it can be used commercially if you want.

111
00:08:13,480 --> 00:08:17,960
If you want to start a startup and use it as a business, you can.

112
00:08:17,960 --> 00:08:25,040
It's also available on sort of various cloud services, easy to use.

113
00:08:25,040 --> 00:08:31,520
So that very fresh just last week has been pre-trained with two trillion tokens.

114
00:08:31,520 --> 00:08:40,680
The context length is 4,096, and some versions of it have been fine-tuned for dialogue and

115
00:08:40,680 --> 00:08:41,680
things of that type.

116
00:08:41,680 --> 00:08:50,160
It compares favorably to other systems, either open or closed source on a number of benchmarks.

117
00:08:50,160 --> 00:08:54,800
But the essential characteristic of it is that it's open.

118
00:08:54,800 --> 00:09:02,440
And together with the model, we released a piece of text that a lot of people signed.

119
00:09:02,440 --> 00:09:06,560
The text says, we support an open innovation approach to AI, responsible and open innovation

120
00:09:06,560 --> 00:09:12,160
gives us all the stakes in the AI development process, bringing visibility, equity and trust

121
00:09:12,160 --> 00:09:17,760
to these technologies, opening today's LLM model will let everyone benefit from this technology.

122
00:09:17,760 --> 00:09:24,000
So what you have to understand is that at the government level, there is kind of a fork

123
00:09:24,000 --> 00:09:29,360
in the road where people are wondering whether AI, because it's powerful, should be kept

124
00:09:29,360 --> 00:09:35,120
under lock and key and controlled and heavily regulated, or whether an open source approach

125
00:09:35,120 --> 00:09:36,120
is preferable.

126
00:09:36,120 --> 00:09:42,520
Yes, they are dangerous, but historically, it's quite the case that there's a lot of

127
00:09:42,520 --> 00:09:48,160
evidence that open source software is actually more secure than the proprietary ones.

128
00:09:48,160 --> 00:09:53,840
And the benefits, the potential benefits of AI and LLM in particular are so large that

129
00:09:53,840 --> 00:09:58,320
we'll be shooting ourselves in the foot by kind of keeping this under lock and key.

130
00:09:58,320 --> 00:10:07,840
So Meta is definitely on the side of open research, has been for 10 years in AI, but

131
00:10:07,840 --> 00:10:13,640
it's still kind of an unsettled question, if you want.

132
00:10:13,640 --> 00:10:19,800
I think personally that this will open up the possibility of an entire ecosystem built

133
00:10:19,800 --> 00:10:23,120
on top of open source base LLM.

134
00:10:23,120 --> 00:10:27,760
Training base LLM is very expensive, so we don't need to have 25 different proprietary

135
00:10:27,760 --> 00:10:28,760
base LLM.

136
00:10:28,760 --> 00:10:33,960
We basically need a few that are open source so that people can build fine-tuned products

137
00:10:33,960 --> 00:10:34,960
on top of them.

138
00:10:34,960 --> 00:10:39,600
There's another reason, which is that before I go back to technical questions, which is

139
00:10:39,600 --> 00:10:46,640
that there's going to be a future in which all of our interactions with the digital world

140
00:10:46,640 --> 00:10:53,480
are going to be mediated through AI systems, virtual systems of some type, and it's going

141
00:10:53,480 --> 00:10:58,240
to become basically a repository of all human knowledge.

142
00:10:58,240 --> 00:11:06,320
So we're not going to be interrogating Google or doing a literature search directly anymore.

143
00:11:06,320 --> 00:11:12,360
We're just going to be talking to our AI assistant and asking a question and perhaps referring

144
00:11:12,360 --> 00:11:16,800
to original material and things like that.

145
00:11:16,800 --> 00:11:20,280
But basically all of our interactions with the digital world are going to be mediated

146
00:11:20,280 --> 00:11:22,440
by AI systems.

147
00:11:22,440 --> 00:11:25,000
So this is going to become the repository of all human knowledge.

148
00:11:25,000 --> 00:11:29,640
It's going to become a basic infrastructure that everybody wants to use.

149
00:11:29,640 --> 00:11:33,480
And history shows that basic infrastructure must be open source.

150
00:11:33,480 --> 00:11:38,360
If you look at the history of the internet, there was a battle between commercial providers,

151
00:11:38,360 --> 00:11:42,640
Microsoft, Sunmacrosystems, and others to provide the software infrastructure of the

152
00:11:42,640 --> 00:11:43,640
internet.

153
00:11:43,640 --> 00:11:46,720
All of those commercial providers lost.

154
00:11:46,720 --> 00:11:53,720
What runs the internet today is Linux, Apache, Chrome, Firefox, JavaScript.

155
00:11:53,720 --> 00:11:57,080
It's all open source.

156
00:11:57,080 --> 00:12:01,800
So my prediction is the same thing is going to happen in the context of AI.

157
00:12:01,800 --> 00:12:08,840
And it's necessary because a lot of countries outside the US in particular don't see with

158
00:12:08,840 --> 00:12:14,920
a favorable eye the fact that their citizens are going to get all the information from

159
00:12:14,920 --> 00:12:19,080
proprietary systems controlled by a small number of tech companies on the west coast

160
00:12:19,080 --> 00:12:20,080
of the US.

161
00:12:20,080 --> 00:12:26,080
So this is just proprietary systems are just not going to fly.

162
00:12:26,080 --> 00:12:31,160
It's just not going to be acceptable to the citizenry across the world.

163
00:12:31,160 --> 00:12:32,240
So it has to be open.

164
00:12:32,240 --> 00:12:33,240
It's inevitable.

165
00:12:33,240 --> 00:12:41,280
In fact, those systems need to be fine-tuned through what has been called RIHF, there's

166
00:12:41,280 --> 00:12:44,840
various ways to fine-tune those systems.

167
00:12:44,840 --> 00:12:49,320
Because the collection of human knowledge is so large, it includes things like physics,

168
00:12:49,320 --> 00:12:57,800
like many of you know, it's going to require contributions from millions of people in sort

169
00:12:57,800 --> 00:13:00,840
of a crowdsourcing fashion.

170
00:13:00,840 --> 00:13:05,240
Because basically those systems being the repository of all human knowledge will be sort

171
00:13:05,240 --> 00:13:06,240
of like Wikipedia.

172
00:13:06,240 --> 00:13:09,440
Wikipedia cannot be built by a proprietary company.

173
00:13:09,440 --> 00:13:14,320
It has to be, it has to gather the entire, the contribution of the entire world.

174
00:13:14,320 --> 00:13:17,560
So it's going to be the same thing with AI based systems.

175
00:13:17,560 --> 00:13:22,560
So open source AI is inevitable, in my opinion, and we're just sort of taking the first step.

176
00:13:22,560 --> 00:13:23,560
Okay.

177
00:13:23,560 --> 00:13:31,720
And so this Lama 70 billion, which is the largest of the Lama model is pretty interesting.

178
00:13:31,720 --> 00:13:33,520
Those are a few examples of what it can generate.

179
00:13:33,520 --> 00:13:42,200
These are extracted from the paper that you can read from the main website.

180
00:13:42,200 --> 00:13:47,480
The fine-tuned system actually refuses to give you kind of illegal information.

181
00:13:47,480 --> 00:13:49,600
You know, it's imperfect, but it works pretty well.

182
00:13:49,600 --> 00:13:54,880
It's got ways of detecting safety and helpfulness and toxicity and things like that.

183
00:13:54,880 --> 00:13:55,880
Okay.

184
00:13:55,880 --> 00:14:01,080
So this is all well and good, but autoregressive LLMs really suck.

185
00:14:01,080 --> 00:14:08,280
For many of us in the AI research business, LLM, the LLM revolution took place two years

186
00:14:08,280 --> 00:14:12,960
ago and it's kind of old hats already.

187
00:14:12,960 --> 00:14:16,520
Not the case for the public who's been kind of, you know, coming in contact with sensitivity

188
00:14:16,520 --> 00:14:21,600
only in the last few months, but really they're not that great.

189
00:14:21,600 --> 00:14:27,840
They don't really produce factual consistent answer.

190
00:14:27,840 --> 00:14:30,720
They hallucinate or they confibrate.

191
00:14:30,720 --> 00:14:32,720
They can't take into account recent information.

192
00:14:32,720 --> 00:14:37,440
They're trained on information that is two years old or so, or whatever snapshot of the

193
00:14:37,520 --> 00:14:41,040
crawl is used.

194
00:14:41,040 --> 00:14:46,240
They're not really, it's not really possible to make them behave properly other than through

195
00:14:46,240 --> 00:14:48,080
this RLHF, which is really perfect.

196
00:14:48,080 --> 00:14:52,200
So you can always jailbreak them by changing the prompt and sort of asking them to kind

197
00:14:52,200 --> 00:14:55,320
of act as if they were toxic.

198
00:14:55,320 --> 00:14:57,400
They don't reason.

199
00:14:57,400 --> 00:14:58,400
They don't really plan.

200
00:14:58,400 --> 00:15:03,160
They can't do math unless you almond them with tools, which you can, of course.

201
00:15:03,200 --> 00:15:06,360
And perhaps Stephen Wolfram will talk about this.

202
00:15:08,360 --> 00:15:13,240
And you need to, there's a lot of work on sort of getting them to use tools such as search

203
00:15:13,240 --> 00:15:15,760
engine calculators, database queries, et cetera.

204
00:15:15,760 --> 00:15:17,960
Right now it's a bit of a hack the way this is done.

205
00:15:19,560 --> 00:15:23,640
And the thing is we are really easily fooled by their fluency into thinking that they're

206
00:15:23,640 --> 00:15:28,040
intelligent, but their intelligence is very limited and really nothing like human intelligence.

207
00:15:29,240 --> 00:15:31,840
In particular, they really don't know how the world works.

208
00:15:31,880 --> 00:15:36,520
They have no connection with the physical reality.

209
00:15:39,320 --> 00:15:45,200
There's another reason why this, and it's basically by construction, which is that a system

210
00:15:45,200 --> 00:15:51,920
that produces one token after the other, auto-regressively, is a divergent process.

211
00:15:51,920 --> 00:15:54,400
It's a diffusion process with an exponential divergence.

212
00:15:55,120 --> 00:16:00,840
If there is a probability at any token that is produced that the token takes you out of

213
00:16:00,840 --> 00:16:04,880
the set of correct answers, those probabilities accumulate.

214
00:16:05,040 --> 00:16:11,680
And the probability that a string of tokens of length n is correct is one minus this probability

215
00:16:11,680 --> 00:16:12,840
of error to the power n.

216
00:16:12,840 --> 00:16:16,880
So the probability of correctness decreases exponentially with the length of the

217
00:16:18,560 --> 00:16:20,800
of the sequence that is produced.

218
00:16:22,000 --> 00:16:24,400
This is not fixable without major redesign.

219
00:16:25,440 --> 00:16:29,360
It's really an essential flaw of auto-regressive prediction.

220
00:16:30,840 --> 00:16:39,920
A while ago, with a colleague, Jacob Browning, we wrote a paper that essentially points out

221
00:16:39,920 --> 00:16:43,600
to the limit, it's not a technical paper, it's a philosophy paper, actually,

222
00:16:43,600 --> 00:16:45,840
the philosophy magazine called Noema.

223
00:16:46,560 --> 00:16:52,120
And it talks about the fact that most of human knowledge is non-linguistic.

224
00:16:52,160 --> 00:16:57,200
Everything that we learned before the age of one, everything that any animal learns has

225
00:16:57,200 --> 00:16:58,240
nothing to do with language.

226
00:16:58,480 --> 00:17:01,600
And it's an enormous amount of background knowledge about the world that we learned in

227
00:17:02,960 --> 00:17:06,000
in the first few months of life and that animals know.

228
00:17:06,960 --> 00:17:08,280
None of this is linguistic at all.

229
00:17:08,360 --> 00:17:12,080
And LLMs don't have access to any of this kind of knowledge.

230
00:17:13,400 --> 00:17:18,280
And so the thesis in that paper is that we're not going to be able to reach human level AI

231
00:17:18,280 --> 00:17:24,680
unless we have systems that have sort of direct sensor information in the form of vision, for example.

232
00:17:25,680 --> 00:17:28,120
You know, some way of understanding how the world works.

233
00:17:28,520 --> 00:17:33,440
Other papers that have appeared either from the cognitive science, in fact, that paper is from MIT,

234
00:17:34,000 --> 00:17:40,960
or from the classical AI kind of field, point to the fact that LLMs really cannot plan.

235
00:17:41,200 --> 00:17:48,320
They don't have the ability to think really or reason in a way that we understand this from humans.

236
00:17:48,960 --> 00:17:53,680
And very limited abilities to plan, at least compared to other systems that are specifically

237
00:17:53,680 --> 00:17:55,040
built for planning.

238
00:17:56,640 --> 00:18:01,000
So I think there is three challenges in the future for AI and machine learning research.

239
00:18:01,000 --> 00:18:06,920
And I've been showing this slide for several years now and I haven't changed it because of LLMs.

240
00:18:07,240 --> 00:18:12,520
The first one is learning representations and predictive models of the world, where the world can

241
00:18:12,520 --> 00:18:14,680
include other people that the system is talking to.

242
00:18:15,600 --> 00:18:17,320
The solution to this is self-supervised learning.

243
00:18:17,320 --> 00:18:18,720
We've known this for a number of years.

244
00:18:20,200 --> 00:18:21,160
Learning to reason.

245
00:18:22,160 --> 00:18:28,240
So autoregressive LLMs are very much like what Daniel Kahneman calls system one, which basically

246
00:18:28,240 --> 00:18:33,480
corresponds to subconscious tasks in humans, tasks that you accomplish without real planning

247
00:18:33,480 --> 00:18:42,560
or reasoning that you sort of accomplish more or less reactively without thinking too much.

248
00:18:43,560 --> 00:18:51,080
System two is the type of action that you take by deliberate reasoning using the power

249
00:18:51,280 --> 00:18:57,800
of your prefrontal cortex, using your ability to predict and then planning sequence of actions

250
00:18:57,800 --> 00:19:01,520
that will sort of satisfy a particular objective.

251
00:19:02,120 --> 00:19:04,280
And LLMs are not capable of this at the moment.

252
00:19:07,800 --> 00:19:11,320
I'm going to argue for the fact that reasoning and planning should be viewed as some sort of energy

253
00:19:11,320 --> 00:19:12,080
minimization.

254
00:19:13,080 --> 00:19:17,960
And then the last thing is learning to plan complex of actions to satisfy a number of objectives.

255
00:19:18,840 --> 00:19:23,640
And that will require learning hierarchical representations of action plans, which machine

256
00:19:23,640 --> 00:19:25,720
learning systems don't really know how to do at the moment.

257
00:19:26,520 --> 00:19:32,680
I've written this vision paper a while back called A Pass Towards Autonomous Machine Intelligence.

258
00:19:32,680 --> 00:19:34,120
I kind of changed the name of this now.

259
00:19:34,120 --> 00:19:38,600
I called it Objective Driven AI, but it really is the same concept.

260
00:19:39,880 --> 00:19:45,880
And it built around this idea of what's called cognitive architecture.

261
00:19:45,880 --> 00:19:49,400
So it's basically an architecture of different modules that interact with each other.

262
00:19:50,840 --> 00:19:55,160
A perception module that basically gives the system an estimate of the state of the world

263
00:19:55,160 --> 00:19:57,480
from perception that may be combined with memory.

264
00:19:58,120 --> 00:20:02,920
A world model and the world model essentially is there to predict what's going to happen

265
00:20:02,920 --> 00:20:06,760
in the world, perhaps as a result of actions that the agent might take,

266
00:20:07,560 --> 00:20:10,920
actions that are being imagines by another module called the actor.

267
00:20:11,480 --> 00:20:14,280
So the actor feeds actions to the world model.

268
00:20:14,280 --> 00:20:17,800
And the world model predicts the outcome of those actions.

269
00:20:19,480 --> 00:20:26,360
And then this outcome is fed to a cost module that that cost module basically

270
00:20:26,360 --> 00:20:29,080
assesses whether the outcome is good or bad.

271
00:20:29,080 --> 00:20:31,160
So it measures the quality of the outcome.

272
00:20:32,280 --> 00:20:36,840
And the entire purpose of the agent is to figure out a sequence of actions

273
00:20:36,840 --> 00:20:39,320
that minimizes that cost.

274
00:20:39,320 --> 00:20:40,760
We're not talking about learning here.

275
00:20:40,760 --> 00:20:42,440
We're talking about inference.

276
00:20:42,440 --> 00:20:48,680
So this minimization of the cost with respect to the actions imagined by the actors

277
00:20:49,800 --> 00:20:52,760
using the world model is for inference.

278
00:20:53,400 --> 00:20:56,920
Okay, so inference is not just forward propagation to a few layers of neural net.

279
00:20:56,920 --> 00:21:01,240
It's actually an optimization process, very much like what happens in business

280
00:21:01,240 --> 00:21:03,160
nets and record models and stuff like that.

281
00:21:05,480 --> 00:21:08,200
And I can describe that in more details.

282
00:21:10,280 --> 00:21:16,520
So that's kind of a very simple different representations of this kind of architecture.

283
00:21:17,400 --> 00:21:23,160
Perceives the world ready to a perception module that computes an abstract representation

284
00:21:23,160 --> 00:21:28,520
of the state of the world, perhaps combined with content of a memory that has some other

285
00:21:28,520 --> 00:21:29,720
idea with the state of the world.

286
00:21:31,000 --> 00:21:35,880
Initialize your world model with that and then feed the world model with that initial

287
00:21:35,880 --> 00:21:42,040
configuration combined with an imagined action sequence imagined by the actor.

288
00:21:43,560 --> 00:21:47,640
And then feed the results to a number of objective functions.

289
00:21:47,640 --> 00:21:51,640
A set of objectives that you can think of as guardrails that are hardwired.

290
00:21:52,840 --> 00:21:59,160
And other objectives that measure whether the task was satisfied was fulfilled.

291
00:22:00,600 --> 00:22:04,600
And the entire purpose of the system is to figure out a sequence of actions that

292
00:22:04,600 --> 00:22:06,760
will minimize those costs at inference time.

293
00:22:07,480 --> 00:22:13,080
Okay, so it cannot do anything, but I put action sequences that minimize those costs

294
00:22:13,080 --> 00:22:16,520
according to the prediction that it's making from its world model.

295
00:22:17,720 --> 00:22:19,800
So that's why I call this objective driven.

296
00:22:21,000 --> 00:22:26,040
There's no way you can job rate that system because it's hardwired to optimize those objectives.

297
00:22:26,040 --> 00:22:30,360
So unless you modify the objectives, the guardrails in particular, you're not going to be able to

298
00:22:31,080 --> 00:22:37,320
have it produce toxic content, for example, if the guardrail objective includes something

299
00:22:37,320 --> 00:22:45,560
like measuring toxicity. The world model very likely will need to be some sort of recurrent

300
00:22:45,560 --> 00:22:50,200
model that might be multiple steps to the action. So you take, for example, two actions

301
00:22:50,200 --> 00:22:53,560
and you run them through your world model twice so that you can predict in two steps

302
00:22:53,560 --> 00:22:57,160
what's going to happen. And the guardrail cost can be applied at every time step.

303
00:22:57,880 --> 00:23:01,640
Of course, the world is not deterministic. So the world model really,

304
00:23:02,520 --> 00:23:06,440
if it's a deterministic function, needs to be fed latent variables so that there might be

305
00:23:06,440 --> 00:23:09,240
multiple predictions for a single action in a single initial state.

306
00:23:10,200 --> 00:23:14,600
And when you make the latent variable vary over a set or you sample them from a distribution,

307
00:23:14,600 --> 00:23:21,080
you get multiple predictions. That complicates the planning process, of course, but the process

308
00:23:21,080 --> 00:23:24,600
by which you figure out a sequence of actions that minimize the objectives

309
00:23:24,600 --> 00:23:31,000
is a planning and reasoning procedure. Ultimately, what we really want is some

310
00:23:31,000 --> 00:23:37,640
sort of ways of doing this hierarchically. And I'm going to explain this with an example.

311
00:23:38,360 --> 00:23:44,360
So here is an example. Let's say I'm sitting in my office in New York at NYU and I want to

312
00:23:44,920 --> 00:23:51,800
fly to Paris. I want to go to Paris. So the first action I have to do is take a taxi or

313
00:23:51,800 --> 00:23:58,040
the train to the airport, either Newark or JFK. And then the second step is I need to catch a

314
00:23:58,040 --> 00:24:03,320
plane to Paris. Okay, but I have a first goal, which is to get to the airport. Now that goal

315
00:24:03,320 --> 00:24:08,280
can be decomposed into two sub-goals. The first one is I need to go down in the street

316
00:24:08,840 --> 00:24:15,880
and tell the taxi to take me to the airport. How do I go down in the street? I need to

317
00:24:15,880 --> 00:24:24,200
stand up for my chair, get out of the building and take the elevator or the stairs go down. How

318
00:24:24,200 --> 00:24:29,080
do I get out from my chair? I need to activate muscles in a particular order all the way down

319
00:24:29,080 --> 00:24:33,480
to millisecond by millisecond muscle control. We do this kind of hierarchical planning all the

320
00:24:33,480 --> 00:24:37,160
time without even thinking about it, even though it's actually a very conscious task that we're

321
00:24:37,160 --> 00:24:45,320
doing. Animals do this also. You can watch, I don't know, cats planning trajectories to kind of

322
00:24:45,320 --> 00:24:49,160
jump on a piece of furniture. They're doing this kind of planning hierarchically.

323
00:24:50,200 --> 00:24:56,440
We don't have any system, AI systems today, that can learn how to do this spontaneously.

324
00:24:56,440 --> 00:25:00,200
There are systems that do hierarchical planning, but they're hardwired. They're built by hand.

325
00:25:01,080 --> 00:25:05,080
What we need is a system that can learn the various levels of representations

326
00:25:05,080 --> 00:25:10,520
of the state of the world that will allow them to do this kind of decomposition of complex tasks

327
00:25:10,520 --> 00:25:18,360
into a hierarchy of simpler ones. Again, we don't have any system that can do this today at all.

328
00:25:18,360 --> 00:25:26,120
This is a big challenge, I think, for the future of AI research. That's the main idea of

329
00:25:26,120 --> 00:25:30,120
objective-driven AI. How can we build systems like this that can do hierarchical planning?

330
00:25:30,120 --> 00:25:34,680
They can learn models of the world that predict what's going to happen in the short term with

331
00:25:34,680 --> 00:25:40,200
high precision or in the long term with less precision in more abstract levels of representation.

332
00:25:41,000 --> 00:25:44,200
This is where I think AI research would go over the next 10 years or so,

333
00:25:45,080 --> 00:25:51,160
and this is how LLM should be built. In fact, that may be how LLM may be built in the future,

334
00:25:51,160 --> 00:25:55,880
and in fact, I have a prediction which is that the type of autoreversive LLMs that we see today

335
00:25:55,880 --> 00:26:00,920
will disappear within three to five years because they are not able to plan their answers.

336
00:26:01,800 --> 00:26:06,440
If we had a system that was able to take a query and then in some sort of abstract

337
00:26:06,440 --> 00:26:11,720
representation space was able to plan its answer, plan a representation of its answer,

338
00:26:12,520 --> 00:26:17,000
and then translate this representation of the answer into fluent text using an autoregressive

339
00:26:17,000 --> 00:26:23,000
decoder, for example, then we would have something that could actually be factual and

340
00:26:23,880 --> 00:26:32,920
simultaneously with being fluent and be non-toxic and be easily germ-broken and be steerable.

341
00:26:33,480 --> 00:26:39,960
That's my idea for where things are going. Building this and making it work is not going

342
00:26:39,960 --> 00:26:46,440
to be easy and may fail, but I think that's where we should go. If we have systems like this,

343
00:26:46,440 --> 00:26:54,440
we won't need any kind of RLHF or human feedback other than the type of systems that are required

344
00:26:54,440 --> 00:27:00,920
to train cost modules to measure things like toxicity, for example, but we won't need to

345
00:27:01,000 --> 00:27:07,480
fine-tune the system globally to be safe. We just need to put an objective so that all of the

346
00:27:07,480 --> 00:27:13,480
outputs that it produces are safe, but we don't need to retrain the entire encoders and everything

347
00:27:13,480 --> 00:27:21,320
for that. I think it would simplify training quite a bit, actually. Let me skip this.

348
00:27:24,360 --> 00:27:26,840
We come to the question of how do we build and train this word model?

349
00:27:26,840 --> 00:27:34,120
When we look at babies, babies learn in the first few months of life an enormous amount of

350
00:27:34,120 --> 00:27:38,440
background knowledge about the world, mostly by observation, a little bit by interaction,

351
00:27:38,440 --> 00:27:45,080
when they start to get old enough to actually act on the world, but mostly just by observation.

352
00:27:46,760 --> 00:27:53,640
The type of knowledge that they learn, things like intuitive physics, gravity, inertia,

353
00:27:53,640 --> 00:27:57,880
conservation of momentum, things like that, pops up only around the age of nine months.

354
00:27:58,520 --> 00:28:02,440
It takes about nine months for babies to really figure this out, that objects that are not supported

355
00:28:02,440 --> 00:28:07,400
will fall. But how do they do this? How do they learn this? Obviously, they don't do this like

356
00:28:07,400 --> 00:28:15,080
LLMs, because if LLMs were the answer to learning like humans, first of all, we would not need

357
00:28:15,720 --> 00:28:20,920
one trillion tokens to train them. Humans are not exposed to that much text information.

358
00:28:21,800 --> 00:28:30,840
Reading one and a half trillion tokens for human reading eight hours a day at normal speed would

359
00:28:30,840 --> 00:28:42,120
take about 20,000 years. That's obviously way more than any humans can do. But there are things

360
00:28:42,120 --> 00:28:47,880
that cats and dogs and young humans can do that are pretty amazing that LLMs can't even touch,

361
00:28:48,600 --> 00:28:57,400
not even remotely close. So cats and dogs can do things that robots cannot come anywhere close

362
00:28:57,400 --> 00:29:02,840
to doing today, not because we can't construct the mechanical systems for it. It's just because we

363
00:29:02,840 --> 00:29:08,360
can't build the intelligence for it. Any 10-year-old child can learn to clear up the dinner table and

364
00:29:08,360 --> 00:29:14,840
fill up the dishwasher in minutes, probably in one shot. We do not have robots that can do that.

365
00:29:14,840 --> 00:29:19,720
We don't have domestic robots. Any 17-year-old can learn to drive a car in about 20 hours of practice,

366
00:29:19,720 --> 00:29:24,600
and we still don't have a limited level of autonomous driving. So that means we're missing

367
00:29:24,600 --> 00:29:29,640
something really big in terms of learning that is very different from the way humans and animals

368
00:29:31,320 --> 00:29:36,840
learn. And this is just another example of the Moravec paradox, which is that there are things

369
00:29:36,840 --> 00:29:42,440
that seem easy for humans and turn out to be really difficult for AI and vice versa. AI systems

370
00:29:42,440 --> 00:29:49,800
are much better than humans at many tasks, narrow tasks, and we are nowhere near finding

371
00:29:52,280 --> 00:29:58,360
mechanisms by which machines can approach the sort of type of understanding of the world that a cat

372
00:29:58,360 --> 00:30:06,120
or a dog can have. Okay, so very some idea about how we can approach that problem. And again,

373
00:30:06,120 --> 00:30:11,480
it's based on self-supervised learning, learning to fill in the blanks. If we train the neural net

374
00:30:11,480 --> 00:30:17,080
to do video prediction, something we've been attempting to do for 10 years now. It doesn't

375
00:30:17,080 --> 00:30:21,560
work very well. If you look at the second column of this little animation at the bottom, the predictions

376
00:30:21,560 --> 00:30:26,920
that are produced by the system, and this is a very stylized video, are very blurry. It's because

377
00:30:26,920 --> 00:30:31,880
the system is trained to make one single prediction, and it cannot exactly predict what's going to

378
00:30:31,880 --> 00:30:36,920
happen in the video. So as a result, it predicts a kind of blurry mess, which is the average of all

379
00:30:36,920 --> 00:30:43,720
the possible features, plausible features that can happen. It's the same thing if you use a similar

380
00:30:43,720 --> 00:30:51,160
system to predict natural video, you get those blurry predictions. So my solution to this is

381
00:30:51,160 --> 00:31:00,280
something I call joint embedding predictive architecture, JEPA. And the main idea behind

382
00:31:00,280 --> 00:31:06,840
JEPA is to abandon the idea that prediction needs to be generative. Okay, so the most

383
00:31:06,840 --> 00:31:11,320
popular thing at the moment is generative AI, generative models. What I'm going to tell you

384
00:31:11,320 --> 00:31:17,080
now is to abandon it. Okay, not a very popular idea at the moment, but here is the argument.

385
00:31:18,120 --> 00:31:23,560
A generative model is one that, for which you give it an input x, let's say initial segment of a video

386
00:31:23,560 --> 00:31:29,480
or a text, run it through an encoder and a predictor, and then try to predict a variable y, which

387
00:31:29,480 --> 00:31:34,440
may be the continuation of that video or the continuation of that text, or the missing words

388
00:31:34,440 --> 00:31:42,280
in that text, and the error by which you measure the performance of the system is basically the

389
00:31:42,280 --> 00:31:46,760
some sort of divergence measure between the predicted y and the actual y. Okay, that's how

390
00:31:46,760 --> 00:31:54,200
you would train this model. It's a generative model because it predicts y. A joint embedding

391
00:31:54,200 --> 00:32:00,200
predictive architectures does not attempt to predict y. It attempts to predict a representation

392
00:32:00,200 --> 00:32:05,160
of y. Okay, so both x and y go through encoders that compute representations,

393
00:32:06,360 --> 00:32:09,080
and you perform the prediction in representation space.

394
00:32:12,520 --> 00:32:19,480
And the advantage of this is that the encoder of y may have invariant properties

395
00:32:20,200 --> 00:32:26,360
that map multiple y's to the same sy. And so if there are things that are very, very hard to predict,

396
00:32:26,360 --> 00:32:31,080
the encoder might eliminate that information that is hard to predict or impossible to predict

397
00:32:31,080 --> 00:32:36,280
from sy so that the prediction problem becomes easier. So let's say, for example, that you're

398
00:32:36,280 --> 00:32:42,280
driving along the road and the predictive model here is trying to predict what's going to happen

399
00:32:42,280 --> 00:32:46,680
on the road. So because it's a self-driving car, it wants to predict what the other cars on the road

400
00:32:46,680 --> 00:32:52,840
are going to do. But bordering the road, there might be trees and there is wind today, so the leaves

401
00:32:52,840 --> 00:32:59,320
on the trees are moving in chaotic ways. Behind the trees, there is a pond and there is ripples

402
00:32:59,320 --> 00:33:06,840
on the pond because of the wind. Those ripples and the motion of the leaves are not only very hard

403
00:33:06,840 --> 00:33:14,360
to predict, pretty much impossible to predict, but also very informative. There's a huge amount

404
00:33:14,360 --> 00:33:18,120
of information in there. And so if you use a generative model, that generative model will

405
00:33:18,120 --> 00:33:22,440
have to devote an enormous amount of resources trying to predict all of those details that

406
00:33:22,440 --> 00:33:27,720
are irrelevant to the task, really. Whereas if you have a model like the one on the right, the JEPA,

407
00:33:28,840 --> 00:33:36,920
the JEPA can choose to eliminate those details from the scene and only keep the details about why

408
00:33:36,920 --> 00:33:41,640
that are relatively easy to predict, like the motion of the other cars, for example.

409
00:33:43,000 --> 00:33:47,000
So that's my argument for the joint emitting architecture and that means abandoning generative

410
00:33:47,000 --> 00:33:50,840
models. Now, of course, you want to use generative models if you want to generate,

411
00:33:51,480 --> 00:33:56,840
but if what you want is to understand the world and then be able to plan, you don't need generative

412
00:33:56,840 --> 00:34:01,800
models. You need those joint emitting architectures. The reason I'm advocating for this is because

413
00:34:01,800 --> 00:34:07,800
experimentally, if you want to use self-supervised zoning in the context of images as opposed to text,

414
00:34:07,800 --> 00:34:11,960
the only architectures that work well are joint emitting architectures.

415
00:34:12,680 --> 00:34:16,680
There are architectures like the one on the left here, which is a joint emitting architecture

416
00:34:16,680 --> 00:34:21,320
without the predictor. This is the most successful approach to self-supervised zoning for image

417
00:34:21,320 --> 00:34:29,320
recognition. You show image X or rather image Y, then you corrupt this image Y into image X

418
00:34:29,320 --> 00:34:35,800
by distorting it, blurring it, adding noise, masking some parts of it, changing the framing,

419
00:34:35,800 --> 00:34:43,960
the size, etc. And then you run both images through the encoders and you force the system

420
00:34:43,960 --> 00:34:48,120
or you train the system to produce representations that are identical for the two images,

421
00:34:48,920 --> 00:34:54,120
so that the representation of the corrupted image is the same as the representation of the

422
00:34:54,120 --> 00:34:59,720
uncorrupted image. And that builds representations that are invariant to the corruptions, essentially.

423
00:35:01,400 --> 00:35:06,040
So those methods, there is a whole bunch of them, about a dozen of them, and they work really well,

424
00:35:06,760 --> 00:35:10,920
whereas all the methods to learn image features that are based on reconstruction,

425
00:35:10,920 --> 00:35:14,520
generative models, don't work. At least they don't work nearly as well.

426
00:35:15,880 --> 00:35:20,920
So what this slide shows is kind of different versions of this joint emitting predictive

427
00:35:20,920 --> 00:35:26,440
architecture, either with a predictor or without, with a predictor that can be stochastic,

428
00:35:26,440 --> 00:35:32,200
having latent variables or not. And the question is how you train this, because the problem is,

429
00:35:32,200 --> 00:35:38,040
if you train a system like this, without being careful, is going to collapse. If you train a

430
00:35:38,040 --> 00:35:45,000
system, you give it pairs of images, let's say x and y, or video snippets, x and y, and you tell it

431
00:35:46,040 --> 00:35:50,440
compute representations that are identical for x and y, the system will just collapse. It will

432
00:35:51,240 --> 00:35:57,320
produce sx and xy that are constant, and then just completely ignore x and y,

433
00:35:58,120 --> 00:36:03,480
so that the distance between sx and sy is minimized. So that's a collapse.

434
00:36:04,200 --> 00:36:12,520
What's a, how can you correct this? And to correct this, you have to put yourself in

435
00:36:12,520 --> 00:36:16,520
the context of something called energy-based models, and I'm sure there are a lot of physicists

436
00:36:16,520 --> 00:36:22,440
in the room, so you can understand what that means. So energy-based models are models where you don't

437
00:36:24,840 --> 00:36:30,840
explain what they do in terms of probabilistic modeling, but in terms of an energy function

438
00:36:30,840 --> 00:36:35,960
that captures the dependency between the variables. So maybe a little more explicit here. Let's say

439
00:36:35,960 --> 00:36:42,440
you have two variables, x and y, and your datasets are those greenish dots that are supposed to be

440
00:36:42,440 --> 00:36:51,960
black. The way an energy-based model captures the dependency between x and y is that it computes

441
00:36:51,960 --> 00:36:58,360
an energy function, an implicit function with a scalar output that takes x and y as an input

442
00:36:58,360 --> 00:37:05,480
and gives you an energy that needs to be low near the data points, on the data points nearby,

443
00:37:05,480 --> 00:37:12,760
and then higher outside of those data points, the region of high data density. And if you have

444
00:37:12,760 --> 00:37:16,680
such an energy landscape, you have a function that has this, that can compute this energy landscape,

445
00:37:17,640 --> 00:37:22,600
then that function will have captured the dependencies between x and y. You can infer x

446
00:37:22,600 --> 00:37:27,800
from y, you can infer y from x, you can have mapping between x and y that are not functions,

447
00:37:27,800 --> 00:37:31,480
because you can have multiple y's that are compatible with a single x, for example,

448
00:37:31,480 --> 00:37:39,880
so it captures multi-modality without having to be a probabilistic model. Of course, in physics,

449
00:37:39,880 --> 00:37:46,280
we're familiar with this, right? Very often, we write an energy function, and then we turn it

450
00:37:46,280 --> 00:37:50,120
into a probability distribution over states using a Gibbs distribution. Same idea here,

451
00:37:50,120 --> 00:37:54,200
but here we don't need the Gibbs distribution at all, we just manipulate the energy function

452
00:37:54,200 --> 00:37:59,560
directly. How do we train a system like this? There's really two categories of training methods.

453
00:38:00,200 --> 00:38:07,240
One category is contrastive methods, and those consist in generating those flashing green dots

454
00:38:07,240 --> 00:38:13,000
here that are outside the manifold data, and then changing the parameters of the energy function

455
00:38:13,000 --> 00:38:18,040
so that the energy takes low values on the data points and higher values on those contrastive

456
00:38:18,920 --> 00:38:26,040
green points. I contributed to inventing those methods back in the early 90s, but I don't like

457
00:38:26,040 --> 00:38:30,200
them anymore, because in high-dimensional spaces, the number of contrastive points you have to

458
00:38:30,200 --> 00:38:35,800
generate for the energy function to take the right shape grows exponentially, and that's not a good

459
00:38:35,800 --> 00:38:41,320
thing. I prefer another server approach, regularised methods, and there are those methods. I'm going

460
00:38:41,320 --> 00:38:46,760
to explain this with another slide. They basically consist in minimizing the volume of space that

461
00:38:46,760 --> 00:38:52,600
can take low energy through some sort of regulariser, for example, so that the system can give low

462
00:38:52,600 --> 00:38:56,760
energy to the data points by changing the parameters of the energy function so that the

463
00:38:56,760 --> 00:39:03,400
energy of the data points gets lower, but because it's regularised, it can only give low energy to

464
00:39:03,400 --> 00:39:11,000
a small volume of space. The data points get kind of shrinkwrapped if you want in the sort of region

465
00:39:11,000 --> 00:39:18,280
of low energies. So I prefer this. That seems to be more efficient, and the question is how we do

466
00:39:18,280 --> 00:39:22,920
this, but what I'm asking you to do now is abandon generative models, the most popular thing at the

467
00:39:22,920 --> 00:39:28,600
moment, abandon probabilistic models, the pillar of understanding machine learning, abandon contrastive

468
00:39:28,600 --> 00:39:33,800
methods, which also have been very popular, and also something I've been saying for a number of

469
00:39:33,800 --> 00:39:39,960
10 years, abandon reinforcement learning, because it's so damn inefficient. So those are kind of

470
00:39:39,960 --> 00:39:43,800
four of the most popular approaches to machine learning at the moment, and I'm telling people to

471
00:39:44,760 --> 00:39:50,680
move away from them. You can imagine I'm not being very popular here, but I'm used to that.

472
00:39:50,680 --> 00:39:55,720
So how do you prevent those systems from collapsing? What you can do is measure,

473
00:39:55,720 --> 00:40:00,920
have some sort of measure or information content of SX and SY across a batch, for example,

474
00:40:01,560 --> 00:40:06,200
and then try to maximise that. Now, unfortunately, it's very hard to do because we don't have lower

475
00:40:06,200 --> 00:40:12,840
bounds on information content. We only have upper bounds, but it turns out you can sort of do this.

476
00:40:12,840 --> 00:40:18,120
So one way to prevent SX from collapsing is that you can use a criterion, which is

477
00:40:19,720 --> 00:40:25,640
attempt to make sure that the variance of every component of SX over a batch is at least one.

478
00:40:25,640 --> 00:40:35,160
So that's the criterion you see in the second red box below the cover. That's a hinge loss

479
00:40:35,160 --> 00:40:41,000
that makes the standard deviation of each variable at least one. And then another term that makes sure

480
00:40:41,000 --> 00:40:47,240
the components of SX are decorrelated. That's the covariance matrix term. So you're trying to minimise

481
00:40:47,240 --> 00:40:51,960
the octagonal terms of the covariance matrix of the vectors SX over a batch.

482
00:40:53,880 --> 00:40:58,120
That's not actually sufficient. So you can also use an expander. I don't have time to explain

483
00:40:58,120 --> 00:41:02,760
why that works, but the resulting method is called Vicreg variance, invariance covariance

484
00:41:02,760 --> 00:41:08,120
regularisation. And it's a pretty general method that can be applied to a lot of situations for

485
00:41:08,120 --> 00:41:13,640
those joint embedding predictive architectures for various applications in image recognition,

486
00:41:13,640 --> 00:41:19,720
segmentation, etc. It's pretty similar to another method called MCR squared invented by EMA at its

487
00:41:19,720 --> 00:41:28,360
group at Berkeley. And it works really well. I'm not going to bore you with details, but there is a

488
00:41:28,360 --> 00:41:33,400
standard scenario in which you do use sub-supervised learning where you pre-train a convolutional net,

489
00:41:33,400 --> 00:41:39,720
let's say, using this method. And then you chop off the expander, stick a linear classifier, which

490
00:41:39,720 --> 00:41:43,880
you train supervised and you measure the performance. And you get really good performance on image net

491
00:41:43,880 --> 00:41:50,680
this way, particularly good performance for out-of-distribution transfer learning. There's

492
00:41:50,680 --> 00:41:57,320
a modification of this method called Vicreg L, which was published last year, which is more

493
00:41:57,320 --> 00:42:03,880
tuned for segmentation and things like this, but I don't have time to go into details. There's a new

494
00:42:03,880 --> 00:42:11,640
method that we rolled out at CVPR just a few weeks ago called Image JEPA that uses masking and a

495
00:42:11,640 --> 00:42:18,040
transformer architecture for learning features in images. And so the collapse prevention method

496
00:42:18,040 --> 00:42:21,720
there is different, but the advantage of this method is that it does not require any data

497
00:42:21,720 --> 00:42:26,440
augmentation other than masking. So it doesn't require to know really what type of data you're

498
00:42:26,440 --> 00:42:31,960
manipulating. And it's incredibly fast and it works really well. It gives amazing results

499
00:42:33,000 --> 00:42:38,440
for really, really good features. There's another set of method by some of our colleagues here at

500
00:42:38,440 --> 00:42:46,200
Fair Paris called Dino, the INO. It's a different way of preventing collapse, but it has some

501
00:42:46,200 --> 00:42:51,640
commonalities with IJPA. And it works really well. It gives you something like above 80% on

502
00:42:51,640 --> 00:42:56,600
ImageNet, purely supervised with no fine tuning and without any data augmentation, which is pretty

503
00:42:56,600 --> 00:43:04,520
amazing. But ultimately what we want to do is use this self-supervised learning and this

504
00:43:04,520 --> 00:43:10,360
JEPA architecture to build the systems of the type that I talked to you about earlier that are

505
00:43:10,360 --> 00:43:15,160
hierarchical. They can predict what's going to happen in the world, perhaps as a result of an

506
00:43:15,160 --> 00:43:22,600
action, with some early results on training systems from video to learn good representations of

507
00:43:22,600 --> 00:43:29,480
images and videos by being trained on successive frames from a video and distorted images.

508
00:43:29,480 --> 00:43:33,160
I don't have time to go into the details of how this works. It's called NCJEPA.

509
00:43:36,840 --> 00:43:43,320
And it is trained basically to extract good features from images for object recognition,

510
00:43:43,320 --> 00:43:47,960
but also to estimate motion in a video. And it does a pretty good job at this.

511
00:43:49,240 --> 00:43:57,640
So watch this paper on archive that you're invited to look at. So objective-driven AI is this idea

512
00:43:57,640 --> 00:44:03,160
that we're going to have objectives that are going to drive the behavior of our system and it's going

513
00:44:03,160 --> 00:44:09,800
to make it terrible and safe. And there are things that we're working on to get this to work,

514
00:44:09,800 --> 00:44:13,800
self-supervised learning from video, that recipe that really works for everything. So we're working

515
00:44:13,800 --> 00:44:19,960
with those JEPA architectures, but we don't think we have the ultimate recipe yet. We can use this

516
00:44:19,960 --> 00:44:25,000
to build LLMs that can reason and plan, that are driven by objectives, perhaps hopefully,

517
00:44:26,040 --> 00:44:32,040
learning systems that can do hierarchical planning, like animals and humans, many animals and humans.

518
00:44:32,040 --> 00:44:37,720
We have many problems to solve. JEPAs with regularized written variables to deal with uncertainty,

519
00:44:37,800 --> 00:44:41,960
planning algorithms in the presence of uncertainty, learning cost modules,

520
00:44:42,600 --> 00:44:47,000
which could be assimilated with inverse reinforcement learning, planning with

521
00:44:47,000 --> 00:44:52,680
inaccurate world models, and then exploration techniques to adjust the world model in case it's

522
00:44:52,680 --> 00:45:04,200
not completely accurate. Okay, so I'm sort of concluding. There is a computing limitation

523
00:45:04,200 --> 00:45:11,560
of autoregressive LLM, which is that they can only allocate a finite and fixed amount of

524
00:45:11,560 --> 00:45:17,560
computational resources to producing single token. You run through, you know, 48 layers

525
00:45:17,560 --> 00:45:21,560
of a transformer or something like that, you produce one token and then 48 more layers and

526
00:45:21,560 --> 00:45:30,440
produce one token. And this is not too incomplete. Whereas the method I'm suggesting, the architecture

527
00:45:30,440 --> 00:45:35,880
I'm suggesting that can produce an output by planning through energy minimization,

528
00:45:36,760 --> 00:45:41,480
that is too incomplete, because everything can be reduced to optimization, basically.

529
00:45:42,600 --> 00:45:46,680
We're still missing essential concepts to reach human ability AI, you know, this

530
00:45:49,720 --> 00:45:57,640
potential technique for planning and reasoning, you know, basic techniques that we're missing to

531
00:45:57,640 --> 00:46:05,320
learn world models from complex modalities like video. And perhaps in the future, we'll be able to

532
00:46:05,320 --> 00:46:11,480
build systems that can plan their answers to satisfy objectives and have guardrails. I don't

533
00:46:11,480 --> 00:46:15,560
believe there is such a concept as artificial general intelligence, because I think even human

534
00:46:15,560 --> 00:46:20,280
intelligence is very specialized. So let's forget about general intelligence, let's try to get to

535
00:46:20,280 --> 00:46:25,800
human level intelligence, perhaps, perhaps build machines that have the same sort of set of skills

536
00:46:25,880 --> 00:46:30,520
and ability to learn new skills and humans. But we are very specialized. In fact, we know this

537
00:46:30,520 --> 00:46:33,960
because computers are much better than us at many tasks, which means we are specialized.

538
00:46:34,920 --> 00:46:38,680
There's no question that sometimes in the future, machines will surpass human

539
00:46:38,680 --> 00:46:43,080
intelligence in all domains where humans are intelligent. You know, how long is it going to

540
00:46:43,080 --> 00:46:47,800
take? I don't know, but there's no question is going to happen. We don't need, we don't,

541
00:46:49,000 --> 00:46:54,040
we probably don't want to be threatened by that. It would be a future where every one of us would

542
00:46:54,040 --> 00:47:00,120
be assisted by a system that is more intelligent than us. And we're familiar with that concept

543
00:47:00,120 --> 00:47:04,760
with other humans. I only work with people who are smarter than me, or at least I try.

544
00:47:06,680 --> 00:47:10,360
Or if they're not smarter than me, I try to make them smarter than me, they're called students.

545
00:47:11,160 --> 00:47:16,680
And, you know, so we're familiar with that concept. We shouldn't feel threatened by machines

546
00:47:16,680 --> 00:47:21,320
that are smarter than us. We are in control of them and we still will still be in control of them.

547
00:47:21,960 --> 00:47:26,360
They won't escape our control any more than our neural cortex and escape the control of our

548
00:47:26,360 --> 00:47:33,080
visual memory, basically, in our brains. Thank you very much. I'll stop here and

549
00:47:33,720 --> 00:47:36,120
perhaps if we have time for questions, I'll take questions.

550
00:47:42,520 --> 00:47:48,280
Thank you very much. If anyone has questions, we have two mics up there. Feel free to line up.

551
00:47:49,160 --> 00:47:55,880
I guess I'll get us started with one question. So on that last slide, you mentioned the possibility,

552
00:47:55,880 --> 00:48:01,080
or not the possibility, just the prediction that machines will become more intelligent than humans,

553
00:48:01,080 --> 00:48:06,520
in all respects. And you also mentioned throughout your talk, these algorithms that can sort of

554
00:48:06,520 --> 00:48:13,480
reason and plan. Could you imagine in the near future an algorithm that, for example, could propose

555
00:48:14,040 --> 00:48:20,680
physics experiments for us to conduct, like plan an experiment to answer a question that we ask it?

556
00:48:22,040 --> 00:48:28,280
Yeah, actually, there's an entire field which precedes AI called experimental design.

557
00:48:30,040 --> 00:48:34,200
And I mean, I think to some extent that can be formulated as an optimization problem, as a

558
00:48:34,200 --> 00:48:39,640
planning problem, or as a search problem, right, trying to figure out, like, you know, how do you

559
00:48:39,640 --> 00:48:43,720
maximally get information from an experiment? Like, how do you design experiments? You get the

560
00:48:43,720 --> 00:48:49,320
maximum amount of information from it to either validate or invalidate a particular model that

561
00:48:49,320 --> 00:48:54,600
you have, or a hypothesis you have in your mind. I think that's entirely automatable.

562
00:48:56,600 --> 00:49:02,680
Now, if you want to use a generic AI system to do this, my guess is that it's not going to happen

563
00:49:02,680 --> 00:49:08,440
tomorrow. The system will probably have to be relatively, you know, experienced before they're

564
00:49:08,440 --> 00:49:14,520
better scientists than human scientists. Yeah, thank you. We have a question up there.

565
00:49:14,520 --> 00:49:19,640
Hey, can you hear me? Yeah, yeah. Great. I was wondering if you could expand a little bit on

566
00:49:19,640 --> 00:49:23,560
your assertion that you cannot build a sufficient world model from text alone.

567
00:49:24,760 --> 00:49:28,280
When we think of something like a theoretical physicist, right, this person mostly interacts

568
00:49:28,280 --> 00:49:32,760
with other people verbally and reading papers and thinking and writing. Or if you think about

569
00:49:32,760 --> 00:49:37,560
something like a blind from birth author or person, right, they're able to actually extract a lot of

570
00:49:37,560 --> 00:49:40,920
information about the structure of the world from the text. So I'm wondering if you could explain

571
00:49:40,920 --> 00:49:45,720
a little bit more about why that's insufficient for, say, achieving human level AI at least.

572
00:49:46,760 --> 00:49:52,200
Okay, when we do physics or mathematics, very much, very often, I mean, certainly physics, we

573
00:49:52,200 --> 00:49:58,040
have mental models of the world. We have some sort of internal simulator, if you want, that can

574
00:49:58,040 --> 00:50:02,280
assimilate the interesting aspect of the phenomenon that we're trying to understand.

575
00:50:03,240 --> 00:50:12,440
That allows us to arrive at answers. And we don't necessarily rely on explicit

576
00:50:12,440 --> 00:50:21,640
facts that we've learned through language. Let me take an example. So all of intuitive physics is

577
00:50:21,640 --> 00:50:29,560
learned by observation. It's not learned through language, right? You know, if that you are going

578
00:50:29,640 --> 00:50:34,600
to put a smartphone on a horizontal surface and let it go, you know, you know, it's going to,

579
00:50:34,600 --> 00:50:37,560
it's going to fall one way or the other. You may not predict in which direction,

580
00:50:37,560 --> 00:50:43,240
but you know, it's going to fall because of your notion of intuitive physics. If I tell you, I take

581
00:50:43,240 --> 00:50:50,920
an object, I throw it in the air vertically, and it's going to have a particular velocity when

582
00:50:50,920 --> 00:50:55,800
it leaves my hand, it's going to go up in the air and then fall back. What velocity will it have

583
00:50:55,800 --> 00:51:02,280
when it crosses my hand at the same location where it left my hand? And if you're any kind of intuitive

584
00:51:03,880 --> 00:51:09,560
notion of physics that go beyond, you know, normal intuitive physics, you would say, obviously,

585
00:51:09,560 --> 00:51:13,320
it's going to have the same speed because, you know, there is conservation of momentum and energy

586
00:51:13,320 --> 00:51:19,640
and stuff like that. And it's not because of energy, you know, necessarily the rule of the

587
00:51:19,640 --> 00:51:25,240
explicit language rule that you have, it's because of your sort of, you know, intuition

588
00:51:25,240 --> 00:51:30,520
that corresponds to that. And we do this all the time. We manipulate mental models. We do not

589
00:51:30,520 --> 00:51:38,200
reason with language very often. Most of our reasoning does not use language. And so most

590
00:51:38,200 --> 00:51:45,240
human knowledge is not linguistic at all. It has to do with construction of mental models,

591
00:51:45,880 --> 00:51:49,800
many of which have nothing to do with language. It's certainly true of all animals. They don't have

592
00:51:49,800 --> 00:51:57,960
language. So that's what sort of, you know, I mean, it's something that I think physicists,

593
00:51:57,960 --> 00:52:01,720
particularly physicists should really understand, right? Because we do this all the time. Good

594
00:52:01,720 --> 00:52:07,640
physicists are people who have those mental models that they can use to sort of, you know, imagine

595
00:52:07,640 --> 00:52:14,360
situations and corner cases and stuff like that, that really kind of, you know, give you some insight

596
00:52:14,360 --> 00:52:20,200
as to what the nature of reality is. And of course, you know, then after that, we do the math,

597
00:52:20,200 --> 00:52:24,520
and that gives you sort of the internal structure of language, the mathematical language,

598
00:52:24,520 --> 00:52:28,840
kind of, you know, makes you discover new properties. But a lot of it is really

599
00:52:30,440 --> 00:52:36,760
intuition with mental model is true in mathematics as well in geometry and things like that.

600
00:52:37,400 --> 00:52:44,280
So, you know, there's the Gedanke experiments of Einstein, right, for those are

601
00:52:45,240 --> 00:52:50,040
basically mental models that you manipulate to kind of discover properties. They're not linguistic.

602
00:52:54,520 --> 00:53:01,320
I have a question along similar lines, but so I agree that there's a lot of intuition involved

603
00:53:01,320 --> 00:53:07,160
in learning for humans. But is there not a fundamental problem in training such intuition?

604
00:53:07,160 --> 00:53:11,240
Because anything you train digitally would be encoded in some kind of language,

605
00:53:12,040 --> 00:53:18,840
some binary. Is there not a fundamental obstruction there to train such intuition?

606
00:53:20,200 --> 00:53:27,960
Not really, no. The input to those systems can be as, you know, continuous and kind of perceptual

607
00:53:27,960 --> 00:53:32,200
as the kind of stuff that we perceive, like, you know, like video or whatever,

608
00:53:34,120 --> 00:53:40,200
or sensory input, whatever it is, audio, you know, anything you want. And then inside the system,

609
00:53:40,200 --> 00:53:47,320
the representation of facts and knowledge inside the system is actually just a sequence of numbers.

610
00:53:47,880 --> 00:53:56,360
It's not language. It's numbers, it's vectors, you know, tensors. So I don't think that's a

611
00:53:56,360 --> 00:54:01,400
problem we need to deal with really. But the texts are also broken down into

612
00:54:01,400 --> 00:54:09,400
same numbers, right? Yeah, that's right. That's true. So text is to some extent simpler because

613
00:54:09,400 --> 00:54:16,920
it's discrete, as I explained, it makes the, you know, the management of uncertainty easier

614
00:54:16,920 --> 00:54:21,640
if you have discrete tokens. And there is a reason why language is discrete, why language

615
00:54:22,440 --> 00:54:29,800
clusters in words. The reason is because language is a communication medium, right? It's a way of

616
00:54:29,800 --> 00:54:35,400
communicating. And we need to communicate over noisy channels. And to be able to communicate over

617
00:54:35,400 --> 00:54:41,960
noisy channels, the symbols have to be discrete. Because that allows you to do error correction,

618
00:54:41,960 --> 00:54:47,480
right? To do noise, you know, to eliminate noise, right? I mean, communication engineers

619
00:54:47,560 --> 00:54:52,280
are known this for decades, you know, since China and basically, and so, or even before.

620
00:54:53,560 --> 00:55:00,680
So our language is discrete and goes into words, you know, the existence of phonemes and words

621
00:55:00,680 --> 00:55:06,200
and things like this, because we need to be able to communicate with noisy channels. But that doesn't

622
00:55:06,200 --> 00:55:11,480
mean that our thinking needs to be the same way. And in fact, our thinking is not the same way.

623
00:55:11,480 --> 00:55:20,200
Language is a pale, approximate, discretized, dumbed down representation of eternal knowledge

624
00:55:20,200 --> 00:55:26,200
representation recall thoughts. Yeah. And so my intuition would be that's when you

625
00:55:26,200 --> 00:55:31,240
encode something digitally in binary, you're doing, you're dumbing that down anyway, right?

626
00:55:31,240 --> 00:55:36,600
So even if you're processing images or videos, you're doing it in numbers and

627
00:55:36,680 --> 00:55:40,680
you're doing it in the same way, maybe it's a little more complex.

628
00:55:41,320 --> 00:55:46,520
But that's life. Okay. That's even physics does this biology does this, right? The communication

629
00:55:46,520 --> 00:55:51,960
between synapses between two neurons, there is a finite number of physicals that are released

630
00:55:51,960 --> 00:55:57,400
for the synaptic communication. And so there is granularity in this, the precision is actually

631
00:55:57,400 --> 00:56:04,040
just a few bits. And, and, you know, it's actually much less than the 32 bits that we use for or

632
00:56:04,120 --> 00:56:09,000
16 bits that we use for computation in neural nets. So I don't think the quantization here is,

633
00:56:09,000 --> 00:56:14,040
is an issue. It certainly exists in the, in the brain as well. Communication between neurons in

634
00:56:14,040 --> 00:56:21,240
the brain is binary brains, you know, the neurons actually produce spikes. For the same reason that

635
00:56:22,120 --> 00:56:25,560
language is discrete is because they need to communicate in a long distance. And for this

636
00:56:25,560 --> 00:56:31,640
to be efficient, it has to be digital basically. So, so I don't see this as an limitation that

637
00:56:31,640 --> 00:56:40,040
would discriminate between computers and human intelligence. Thanks. Okay. Unfortunately, we

638
00:56:40,040 --> 00:56:47,560
need to move on because we're running five minutes behind already. Thank you so much. This was great.

