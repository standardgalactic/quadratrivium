{"text": " So our final speaker of the day is an external speaker. So this is Stephen Wolfram from Wolfram Research. So Stephen got his PhD in Caltech, PhD in physics from Caltech at the age of 20 working on high-energy physics quantum field theory and cosmology. He's the founder and president of Wolfram Research, developers of Mathematica and Wolfram Alpha, tools that I'm sure you've all used. And recently I think something interesting that Wolfram did was release a plug-in for chat GPT, giving access to the Wolfram Alpha computational intelligence engine, which I'm sure we will hear a little bit about. But yeah, if you're ready to go, I'll give you the floor. So I guess I want to talk about two things today. I want to talk about using LLMs for physics and how physics can help study LLMs. So to start off talking about how physics can use LLMs, the first thing is what do LLMs fundamentally do? LLMs have taken four billion pages from the web and a bunch of books, and they've kind of ground that up to find the statistics of everything that's said there. And when you ask an LLM something, its mission is to try and produce reasonable text based on kind of the statistics of what it saw on the web and so on. And as we'll talk about later, the things it extracts as its statistics are surprisingly sophisticated and include having sort of found a kind of semantic grammar of language which allows it to kind of say things that make sense, at least make some kind of sense. They may be fact, they may be fiction, but they kind of fit together in a way that makes sense. But so what the LLM is fundamentally doing is it's taking stuff we humans have put on the web, and it's feeding that back to us based on things that we ask it about. It's feeding us back sort of reasonable things that we could have said on the web even though we might not actually have done so. So it's a good way to generate our, I see LLMs actually as a practical matter as kind of an important layer of a linguistic user interface. We have graphical user interfaces, now we have a linguistic user interface where you can take like five points you wanted to make, you can puff those out with an LLM into a giant report, then maybe the person you're sending that report to really wants to know only two things, so they use an LLM again, and they grind it down and get the two things they actually wanted to know from that report. And that's a very sensible transport layer using kind of the big report as the transport layer for what's going on, and maybe that will happen with academic papers as well, that they will be just the linguistic transport layer for the actual content of what's going on. I would like to think that we can do better than that, in a sense, math has provided us a notation that's better than that, computational language, the kind of thing I've worked on for the last 40 years or so, is an attempt to kind of formalize statements about the world in a way that's kind of better than just giving it as natural language. But anyway, that's kind of, you know, what the LLM is doing is it's taking what's there on the web, it's reconstituting it and feeding it back to us, it's not going to be able to discover fundamentally new things. Well, with a couple of exceptions there, I think one thing it can do is, if there are analogies that might be found between this place and that place, it's really good at finding kind of statistical facts from language. Usually we're used to doing statistics from numbers, but LLMs manage to do statistics from text as well. And so if you say, well, what was the trend in fashion in 1955, there's a good chance that the LLM will be able to take sort of the stuff that ground up from the web and answer that. And similarly, if you say, well, could there be an analogy between, you know, metamathematics and general relativity, there's a chance that it could figure that out, because it can see that the kind of the structure of what's said about those two areas has a certain similarity. Something that seems like bizarrely magic to us humans, something that some of us humans kind of pride ourselves on being able to figure out such analogies, but I think we may be about to be outdone by the AIs. But okay, so there's sort of a question of the LLM is doing this kind of taking language from the web, giving us back some sort of reconstituted version of that. But there's more to figure out things, and for example, physics, as it has emerged since basically Newton and the Newton's big idea from 1687 was, as he called it, the mathematical principles of natural philosophy. In other words, there is a more formal method for dealing with natural philosophy, i.e. physics, than just talking about it as philosophy, so to speak. So this kind of notion of formalization that started in those days and has emerged into sort of modern mathematical methods in physics. Well, this kind of idea that you can do better than just pure thought about something. You can have a formalization that allows you to make further progress, which is most of the story of physics as we know it today. So if you're a pure LLM that's just dealing with language, that's just dealing with kind of pure common sense kinds of things, then you are stuck in the pre-Newtonian kind of paradigm for how you do physics. Well, so how do we kind of connect kind of this sort of linguistic layer of thinking about things with kind of the more formal, we might now say, computational layer of thinking about things. So I've spent most of my life kind of trying to figure out how you can sort of describe the world formally using computation. And so the question is, can you connect kind of the LLM world to this computational world? And actually, back in March, we did something with the folks at OpenAI of making a plug-in to chat GBT that allows it to kind of use our computational language from within the LLM. And I'll show you, I haven't actually used this interface for a while for reasons that I'm about to show you. But let's see, if we say make a picture of an airy function or something, let's see. Maybe it will probably, maybe, I don't know, you never know what it's going to do. It's some, okay, so it says using Wolfram, that's a good sign, maybe it's going to do the right thing, maybe not, who's to know, okay. So this is, I wonder how it did that, okay. So there it made a nice little plot of an airy function. Let's see how it did it, we're going to here, okay. So what it actually did there was it wrote a piece of Wolfram language code that just says plot the airy function, very straightforward. Let's say we say something like, I don't know, how far is it from, I don't know, Chicago to Tokyo. Wait a minute, what happens to that? It disappeared. What's going on? Scrolling down. Scrolling down. Oh, okay. Thanks. I thought I was scrolling down, it didn't seem to be showing up, okay. How lovely. So what it did there, again, was, in this case, let's see what it did here, okay. So in this case, it used Wolfram Alpha and just asked distance from Chicago to Tokyo, then it got back a bunch of results from Wolfram Alpha, which it then kind of interpreted and turned that into what it was saying. So it's sort of interesting what's happening here, because actually we have this plug-in, it's used a lot every day by people, and I think, at least when I last asked, which was a few weeks ago, there's still the case that about half of the queries go to Wolfram Alpha and half the queries go to Wolfram Language. And if you read the prompt, you know, the prompt engineering is this bizarre activity where you, whether you say please or not matters, whether things are in capital letters matters, whether you repeat things at the end of the prompt after you mentioned them at the beginning, that all matters. I will say, by the way, that if you ask, you know, what's the skill you need to do prompt engineering, so far as I can tell, expository writing is the number one skill needed for good prompt engineering. Maybe one day, and we'll talk about this later, when we talk about applying physics to LLMs, maybe there will be actual kind of AI psychology theory that can be used, but as of right now, I think it's expository writing, which kind of maps on to the kind of thing that the LLM has read from the web and so on. But in any case, the prompt here is saying, you know, if you have this kind of thing, try and send it to WolfMalpha, one of the things that's really convenient about WolfMalpha is that it is a thing that takes natural language's input, which is the same stuff that the LLM is used to dealing with. So it's kind of, it's using natural language as a transport layer, and what does WolfMalpha do? Well, what it's doing is to take whatever you type in, you know, if you type, I don't know, what is the integral of, I don't know, some random thing. What it's doing there is it's converting that question written in natural language into precise computational language internally, or if I say something like, I don't know, what earthquakes happened in Japan in August 1990 or something, I wonder if it can do that, I have no idea if it can do that, but that's always living dangerously, okay, we've managed to do that. Once again, what happened here was it converted that natural language question into its underlying computational language, which is our WolfMalph language system, to be able to resolve it. I mean, just to show you how that works, you know, if I were to say here something like, if I just said, you know, New York here, that this is a WolfMalph notebook, this thing New York, if I say what's the input form of that, it's the entity, city, New York, New York, United States, but it's also a thing that I can compute with. So if I say make a, I don't know, if I say, you know, geodistance from New York to, I don't know, London or something, it'll then just use those things as entities that it can compute with. So as I say, the sort of the mission of WolfMalph is convert natural language into this precise computational language from which we can do computations based on algorithms that we've spent the last three and a half decades, you know, setting up and based on curated knowledge that we've accumulated over the last couple of decades. So, you know, you can obviously mix things like, you can say things like, I don't know, capital cities in Europe or something, and you'll get something which again, that thing got converted into precise computational language, we can evaluate it, we can say something like, you know, I don't know, we could say make a plot of those, all those standard kinds of things that you can do in WolfMalph language, or we can find shortest tours, all those sorts of things. So, from within chat GBT, you can now access all of that functionality, so I don't know, we could, let's try, let's try doing something ambitious, which probably won't work. Find a shortest tour of the capital cities of Europe. Okay, let's watch this fail, grind, grind, grind, I have no idea, I hate to even open this to find out what horrifying thing it's actually doing in there. Maybe, maybe, let's see, okay, it's trying something again, either because it didn't get the answer it wanted, or for some other reason, oh, this is a bad sign, this is not good, this looks like it's, no. But what it's going to do, what it's doing is, every time, by the way, I mean, the way LLAM's work, we'll maybe talk about this a bit more later, they're always writing one token at a time, so they never have a plan for where they're going to go, they're always just looking at what was in the past and figuring out what to say next. So that means that it's quite often, it's kind of a hack you can use, if you get it to generate an output and you say, is that correct? And it will say, no, it's not correct, and how, why did you say it, well, because it didn't know what it was going to say, it just, well, let's see, let us see, okay, wow. Wow, wow, okay, let's see what happens, now, I have no idea if this is, okay, wait a minute, how many, how many, wait a second, okay, let's see, well, let's see, let's see what happens if we say plot that, and then I'm going to find out what it actually did there. This won't work, of course. Well, okay, this is slightly promising, I wonder if this is going to work, it's a little bit confused there, but we'll see if it can recover itself, I don't know, let's look at what it, let's look, to get some idea of whether this is actually right, let's look at what it actually asked here, okay, so it asked, okay, it did something sensible here, so what it was doing, it's a little bit confusing what it did here, and we'll see how this works a bit better in a moment, but what it did here was it was actually using results that it had got earlier in this whole sequence, because it actually knew the order of the cities by now, because it must have got that in one of these previous queries here, yeah, here we go, so it knew here, find shortest tour of those capitals, it found the shortest tour, it then used that in the next step to go and try and find something, let's see what it was doing down here, let's see what it got me where, no, it's still grinding away, oh well, alright, so, but this gives some sense perhaps of how you kind of connect sort of the LLM layer to the computational layer, but we built something recently that I think you might like to see, which is what we call, well let's see, there's different versions of this, what we call a chat enabled notebook, so this is using our notebook paradigm and let me see, let me make this a little bigger, and let me just get ready to save this, the, okay, let me tell it to use GPT-4 here, alright, so let's say we say here something like, again, I'm going to look very dangerously, because this never does the same thing twice, let's say solve a harmonic oscillator, and harmonic oscillator, whatever, uh, let's see what happens, oops, so, okay, okay, I mean it'd be not the right thing for it to do, but anyway, let's see, um, huh, okay, not terrible, let's say show me the equation, so we'll talk in a minute about what the heck it was actually doing here, that's not very useful, I want to know the differential equation, if you want to visualize it, okay, let's see what it does, let's see, you never know what this thing is going to do, so it's just kind of, um, okay, that's not terrible, I would give that a, you know, maybe a pass and grade, I don't know, let's see what it actually did, so here, it, okay, so it synthesized, well from language code here, this, what these boxes look like inside, probably by next week will look a bit better than what it does right now, but, you know, it synthesized some code here, actually, if I say here, you know, show me the ODE, let me see whether it can do that, um, okay, great, okay, very good, yes, yes, all good, is that right, no, yes, yes, that's right, that's okay, um, now I say solve that, okay, not bad, so, you know, this kind of thing I view as being a pretty useful, come on, I just want to see the equation, um, what I want to see is the, is the code here, okay, let's say show me the code, because this is, this is in a sense, okay, finally we got it, okay, and then what we can do here is given this piece of code, we can just say, for example, we can just say evaluate code, oh look, wait a minute, something is happening in the background here, it apologizes for the inconvenience, um, who knows what it's doing, we'll check back for that in a few moments, but back in this notebook, we're here, I can just say use that thing to copy that code down there to the next cell and then do the evaluation and get the result, so it's kind of interesting, you know, if I go back here, maybe I can try another example, let me show you how this works, I can put in what we call a chat block, that basically breaks the context of the LLM, so the LLM, whenever I'm saying, when I say show me the code, when I ask that, it's able to see the whole, the whole conversation that it's had above it, okay, so that's um, and so now here, I broke that by saying show me another, show me another thing here and I could say, well here, for example, I can do this pull down and this um, this allows me to make all kinds of changes, so I could, for example, we have this prompt repository that contains, there we go, so it contains various um, well many, okay, we can go to the prompt repository here, this is a prompt repository that has, in this case, it will allow us to pick personas for interacting with this, so I could pick um, I don't know, let's try this, I have no idea what this is going to be like, okay, so as I install the 19th century British novel persona, I've installed that, actually I kind of think we should use Bernardo, he's fun, um, but either one, okay, so I can now pull this down, okay, we can try 19th century British novel, um, make a picture of a circle, uh, that is half red and half blue, let's say, now I think this will, uh, oh come now, oh great, well that's, that's okay, big mess, um, bad taste, okay, and this code snippet, blah blah blah, good luck, well let's try, all right, let's try, let's try doing this, actually I should just stop this yacking on like this, let's try, let's try a different persona, let's try, let's try the code assistant, but actually, you know what, I'm going to try Bernardo, Bernardo is fun, let's, let's try re-evaluating that, the, now what's it doing there, I don't know, this first creates a full red disk and it overlays a hard disk, okay, I wonder if this is actually right, dah, okay, it worked, nice, what's important about this, this maybe isn't the very best example, is you can actually read that code, unlike the, the thing that it happened to produce before, um, and the, the kind of the idea is, and this is by the way one big feature of, of kind of the whole computational language story that I've spent so long on, is that, you know, our language is intended as something that you can think in, as well as have your computer execute, so to speak, kind of like math notation would be, it's something that, where you can actually, you know, use it as your foundation for thinking about things, okay, anyway, that this is, you, you get the basic idea, I hope, of, of sort of this chat notebook notion, it's, it's pretty nice, I mean, I have to say, since the reason that I haven't used that, um, the chatGBT interface for months, is because this is really a lot nicer, you get to, not only, you know, you can also use all the standard features of notebooks, so you can say, this is a section about circles, and you can start putting in, maybe I could, well actually, let me just do this, hold on, let's say, do that, I'm not going to live dangerous, do that for a sphere, this is going to fail, of course, okay, let's see what happens, okay, interesting idea, interesting idea, I wonder whether that will work, that's definitely an interesting idea, I give that point, the spherical plot, that goes, wow, if this works, I'll be impressed, okay, let's run it, wow, that's cool, it's getting smarter, the, or how, or the fine tuning that we've done and so on is actually working, this is encouraging, um, the, because this is kind of interesting, I mean, it made a spherical plot over a certain, you know, latitude, sequence of latitude values, a different sequence of longitude values here, that's kind of interesting, you kind of learned something from that, maybe we could try, let's try one other thing, which might be fun, let's say, um, uh, show a star chart of the current position of Jupiter, now I'm probably going to have to say, use astrographics, let's see what will happen here, oh, come on, it just, well, I think it made that up, I'd be very surprised if these functions actually exist, no, they do not exist, um, well, that's a bad sign, okay, let's try saying use astro position, and what I'm expecting it's going to do, maybe, lovely, but that's also not relevant, okay, it's not doing what I thought it would do, which is to go read the documentation, we can, we could probably tell it to do that, let's say, um, blah, blah, blah, there we go, now maybe it'll get a little bit smarter, okay, this is much better, this is much, much better sign, so hopefully, if it read the documentation, it will be able to successfully do what it was, all right, I don't know whether it's blah, blah, blah, blah, blah, now probably if we now say, okay, great, it's talking about all kinds of, I don't know, it's telling us how to find the position of the large Magellanic cloud, et cetera, et cetera, et cetera, that's all fun, and we could ask it to run that, but I think use this for the picture of Jupiter, maybe this will work, maybe it won't, okay, this is much better, what, you see this is the problem, it just makes stuff up, well let's see, I wonder whether this will work, no, it made up the thing called planet marker, well we'd have to tell it not to do that, it's supposed to go back, and I'm a little bit surprised it did that here, because actually it has been told to go back and check the code it wrote to make sure that everything in it actually exists, so for some reason it didn't in this case, all right, well anyway, that's a little bit on kind of the sort of the interface between sort of LLMs, computational language, I thought another thing I would talk about, quite different subject, is using physics to think about LLMs, so let me pull up some things, so first question is what fundamentally is an LLM doing, as I said, what it's ultimately doing is it's saying, given a particular piece of text, let's see if this works, okay, so if you have something like this, you feed the prompt, the best thing about AI is its ability to, and then its mission is to give you what the next word should be, and there are some probabilities that it uses to do that, so if we kind of, we're interested in knowing, where's my mouse, come on, up, just a second, sigh, you know, maybe I should just, well, okay, let me just, that's very strange, fascinating, okay, the lost mouse, okay, the lost mouse has been found, maybe, all right, so just, I mean, let's talk a little bit about what, actually, let me show you something else here, so in our language, well, no, we can do it here, we can just say, let's use a plain chat, and let's set it so that one of the parameters is, we saw those probabilities that the LLM produced for what the next word should be, one of the things about LLMs is they have to decide, given those probabilities, which actual word should be picked, like one thing it could do is say, always pick the most probable word, another thing it could do is pick those words according to the probabilities as it generated them, there's this thing that's usually called the temperature parameter, which is an exponential distribution thing that basically is the thing that picks, zero temperature means always pick the most probable word, temperature one means pick the words in the probabilities that the LLM generated itself, as you increase the temperature, it's picking more and more bizarre words, so let's say we go here and let's say we increase the temperature to like 1.3, let's say, and we say something like, how are you today, and it will generate some, so this is now using, okay, right, great, okay, now let's try, let's change that temperature, let's go ahead here and just crank up that temperature, and let's try running this again, oh my, so I'm bonkers, oh no, okay, well at least it's stopped, often it never generates a stop token, it just keeps going forever, the, so okay, so here's an example of a physics question, is there a phase transition as a function of temperature in an LLM? The answer is almost certainly yes, probably around for something like GPT-4, it's almost certainly at a temperature around 1.3 or so, maybe there are actually two transitions that occur, actually there's a, we just had a summer school with people studying all kinds of things, and one person at our summer school studied this question, and I have to admit, I haven't read the thing they wrote about it, so, but I can show you, this is basically, as a function of temperature, this is essentially an order parameter changing, and in the LLM, and this is someplace here, this is an actual, you know, there's some innards of an LLM, and somewhere here, there should be, okay, that's some random pieces of language code, I think what was done here was to look at the extent to which it maintains kind of coherence in the structure of the sentences that it produces and so on, but anyway, the thing that I wanted to point out there is this is a very physics-like question, what, how does this work, and one of the things we don't have right now is a kind of good qualitative physics, overall physics-like model for an LLM, like you might say, oh, maybe it's like a spin glass, well, it's not really like a spin glass, maybe it's like some other statistical mechanics system, what is it really like? Well, there are a few things that we kind of know about LLM, so I can show you some pictures, let me just show you, just to get a sense of what's going on inside here, this is kind of a, like, let's say we're trying to learn this function, so we've got x and y are input parameters, and we're trying to learn that function, we're going to have a neural net, there's a neural net, and that neural net is taking those values, x and y, and at the top it has some weights, each of the connections has a certain weight, it indicated by the color of that, that connection, and then if we feed in particular values up at the top there, this neural net will have been trained, will have been set up with the correct weights, so that it will always produce a 0, 1, or minus 1 at the bottom, so for example we can, let's just, let's say, if we try and use a very, very trivial neural net, trying to learn that function, the totally trivial neural net will not succeed in producing that function, if we make the neural net more sophisticated, here are some slightly more sophisticated neural nets, as the neural net gets more sophisticated, it's going to be able to successfully learn that function, how big does a neural net have to be to learn what level of function, not really known, I mean there are theorems that say in principle you can do things with neural nets of certain sizes, but the practical question we don't know, that's another kind of thing, so now you know in terms of what, let's see, the, I mean you can do these experiments by the way, the things I've written about, I wrote some kind of whole explainer of chat GPT, which was one of the things that I've written fastest in my life, and it's the thing that seems to be read more, at least per unit of time spent on writing it, it seems to have been read more than anything else I've written, which to me is a little bit disappointing actually, but that's a different story, but anyway, so those are some things about the innards of chat GPT, and those are some, but we can start looking at kind of what's actually going on inside the system, and it's kind of complicated, and you start seeing, you know, this is a condensation of the kind of innards of the brain of actually this is GPT2, kind of a junior version of chat GPT, and this is kind of, in a sense this is taking human knowledge and human linguistics, and crushing it down to something that's represented in terms of arrays of numbers, and this is one of the pieces of what you see when that's done, I mean the full chat GPT has like 175 billion weights, this is just showing a little piece of that story. Now, okay, what can we say about what it actually does? Well, there's several different things, so one thing that's important is this concept of embeddings, we can take kind of, you know, words in a language, sentences, things like that, the big sort of idea of neural nets in some sense, and it's a very old idea, dates all the way back to when neural nets were invented in the 1940s, is don't just use digital information, use arrays of real numbers to represent things. It's not clear that you actually need to do that, you probably don't, I don't think you need to do it for physics, for example, but the way that neural nets are built, they are take everything, whether it's an image, whether it's text, whatever else, and grind it into arrays of real numbers, and then you can take those, then what you're doing is representing everything, you know, just as in standard digital computational stuff, you're representing things as bits in neural nets, you're representing everything in terms of arrays of real numbers, and so for example, any old sentence, any old piece of text is ultimately represented as an array of real numbers, and that array of real numbers we can think of as being some sort of feature vector that represents, in some sense, some digest of the meaning of the thing that we specified. So you can start asking in meaning space, in that space of embeddings, what can we see about what happens in that space, and for example, let's see, we can ask questions like, how linear is that space? You know, for example, if we do parallel transport in that space, if we look at the curvature of that space, we're looking at, you know, this is to that, as that is to that, that's kind of the analog for linguistics for sort of the structure of meaning of a question that you might ask in physics of space time or something, and you can ask about these questions about curvature in that space, I don't know all the answers to this, you can also ask things like, well, what is the trajectory that's carved out in that space? So is there, for example, a semantic law of motion? If you start in this particular way, is it the case that in this meaning space that you end up always tracing through in a particular way? And one thing that seems to be the case, the space to some experience we just did a couple weeks ago, is that the things are much more organized. So if you look at, oh, this is kind of, sorry, let me just show you, much of the time these trajectories aren't, in something like GPT2, the trajectories are quite disorganized. It seems that as you get to things like GPT4, the trajectories look a lot more organized. It's much more believable that there are semantic laws of motion, so to speak, laws of motion in meaning space in GPT4. By the way, it's worth realizing that there's sort of a quantum story to the whole thing because the whole thing is, it isn't just picking one trajectory, it's picking a whole bunch of different paths. One difference from, I mean, this is a quite different topic, but in the whole fundamental physics project that we've been doing for the last few years where it seems like we really actually do finally understand how quantum mechanics works, it becomes very important in that case that there is merging of different paths of history, as well as just branching the paths of history. In the current versions of these LLMs, there's pretty much just branching the paths of history, but you kind of get this quantum-like phenomenon going on of all these different possible things the LLM might say that aggregate up to different kinds of things. By the way, if you're, well, there's all kinds of interesting things to say about LLMs as observers in thinking about physics, but maybe one thing to talk about is just what is, this is sort of pictures of what meaning space looks like and so on, and questions like if you have a word and it has many different sort of partially, so this is the word crane, I think, and this is, in meaning space, this is where different sentences that mention cranes show up, and so I think the ones at the top are cranes as a bird and the ones at the bottom are cranes as construction equipment type thing, and you kind of see that separating, so you can kind of get, again, it's this kind of rather physics-like thing of kind of looking at this meaning space, and by the way, you can sort of ask things about the structure of that meaning space, and for instance, let me see if I can show you a picture, let me see here, maybe, okay, so in meaning space, you can ask something like, you can also do that with images, and so you can ask, for example, we can go in meaning space, we can go from a dog image to a cat image on the line in meaning space between a dog and a cat, and we could actually keep going from the cat out further, we can extend that line further out in meaning space, and we get all these kinds of weird things happening, or we can go from a plane to a cat, and we have something very strange in the middle of those two things, or we can just go out, this is what I was calling cat island, this is in the middle, so this was a generative AI, not specifically an LLM, this is an image generation AI, which uses somewhat similar, but not precisely the same technology inside, and I asked it in the middle to make a picture of a cat in a party hat, and then as you go outwards in meaning space, you see this kind of island of where you can see sort of identifiable cat things going on, and then further away, it becomes more and more bizarre, and by the way, you can ask questions like, well, what's actually out there in sort of arbitrary meaning space, and I think, well, you can look at other cat islands here, this thing is actually in 2000 dimensional space, and these are planes in 2000 dimensional space, different planes in 2000 dimensional space, and you see different cats live on different planes, but sometimes you can just, if you plop into this meaning space in some random place, you'll see things which kind of look, well, I don't know what those are, but sometimes you'll see things which kind of have a reminiscent of kind of human forms and so on, why does all this happen, same kind of reason as with LLMs, because this was trained from a five billion images, which were actual images people put on the web, and those actual images are of human relevant kinds of things, with images more so than with text, we're able to, as humans, we're able to look at things that weren't quite right, like we looked at that high temperature version of what the LLM produced, and it looked like garbage to us, it was incomprehensible, for images we do a little better at being able to not be just completely confused by what's going on, but if we kind of dive in and look, you know, what's out there in arbitrarily, let's see where do I have a picture of that, well, those are some pictures just randomly out there in kind of meaning space, and if you go in you can see, you know, there are weird things like this, these are, you know, people like pictures, or you can have, you know, pictures like these, which are kind of reminiscent of sort of landscape-like pictures, but aren't really landscape-like pictures, but this whole question about, you know, where in meaning space, where in this, I mean, there's this, if you try and imagine, where is the stuff that's meaningful, 10 to the minus 600 of all of meaning space is what we have so far explored as with sort of human language and so on, it's a very small fraction of it, with respect to images. Okay, so just to maybe finish off a bit, we could talk more about this kind of thing, but just to talk a little bit about sort of the physics of LLMs and so on, I think one of the things people, what one wants to do is, is there a kind of a narrative story of what LLMs are finding? Is there a way of saying, why do LLMs even work? It's not obvious that, you know, given that you, you know, you could say, take a sentence like, the cat sat on the, okay, based on just looking at pages on the web, you can reasonably guess the next word is going to be math, but by the time you've got a long prompt where you're asking it some physics question or something, there's no way that actual detailed text is going to be somewhere on the web, or probably not, unless it was some exercise or a book or something, but most of the time it won't be something that was on the web. So you have to have an actual model that allows you to extrapolate the model that's being used in chat GBT as a neural net. It is far from obvious. There's no fundamental reason to think it would be true that the way the neural net extrapolates will agree with the way we humans think it makes sense to extrapolate. The fact that it extrapolates to produce things that seem meaningful to us humans is a nontrivial scientific result. And, you know, I think what it's basically telling us is the way brains work is actually pretty well modeled by sort of neural net type things. And that's why the things that brains extrapolate with are pretty close to the things that these simple neural nets extrapolate with. So then the question is, well, okay, we've got this kind of extrapolation that's going on. We've got some, this thing is finding out some way to extrapolate. How is it doing that? Well, what regularities in language is it picking up to allow it to make meaningful sentences, meaningful text? Well, there's one big regularity that we know about in language, which is syntactic grammar. We know how you put parts of speech together, nouns and verbs and things like this. So in a sense, we can then construct sentences which are syntactically correct. But there are infinite number of sentences that are syntactically correct but complete nonsense. And that's, it's doing much better than just producing syntactically correct sentences. So what's it doing? Well, there's one good example of a place where we know a structure in sentences that exists that isn't sort of purely syntactic. And that's logic. And you can kind of think, you know, when Aristotle invented logic back a couple of thousand years ago, you know, what was he actually doing? Well, he was a bit like a machine learning system, because what he was effectively doing was saying, I've got all these examples of rhetoric. People make an argument that looks like this, but I can take something which instead of it being a discussion about, you know, Sparta and Athens, it can be a discussion about turtles and fishes. It doesn't matter. I can just replace those symbolically with P and Q and I can look at this sort of formal structure of these sentences. In a sense, you can lift logic out of the specifics of actual language, in his case, Greek. But in a sense, what LLMs have done is they've discovered the same thing. So people say, oh, my gosh, it's amazing, you know, LLMs have discovered logic. Well, they discovered logic, I think the same way Aristotle discovered logic, and you can find out they're basically doing so logistic logic. And if you try and feed them things which require sort of more formal, more formal kinds of things, even at the level of, you know, parenthesis matching and so on, they will fail after you get sort of too many parentheses to match. They don't do kind of the formal level of things. They don't do the computational thing. They do the kind of level of things that in a sense was the original way that logic was discovered. So that's a place where kind of one's able to lift something more semantic out of this kind of layer of pure language. But presumably, there is more that can be done along those lines. Presumably, there is kind of a semantic grammar of language, which in a sense, the LLMs have discovered something about language and common sense reasoning and so on. That is that there's this sort of thing you can lift out of language that allows you to kind of put together meaningful stuff beyond just the purely syntactic. And I think that's the thing where, well, I've been interested in this actually for a long time for different reasons, this kind of idea of sort of making a symbolic discourse language that allows you to sort of express things in a kind of, in a way that is sort of, is a symbolic way of expressing things that is not specific to the particulars of language. In a sense, the whole enterprise of making a computational language has got a certain distance with that, describing certain kinds of things in the world. But anyway, I think that there are many pieces of kind of what happens in LLMs. For example, why does few shot learning work? Why does it work to tell LLM and LLM to talk like a pirate? Why does it, how does that manage to place it somewhere in meaning space or something so that the kind of, you know, you placed it somewhere by giving that prompt, then somehow the semantic law of motion takes over and it successfully manages to produce semantically meaningful stuff. We don't know how any of this works. It's a great topic for physicists, I have to say. I think it's one of these places where it isn't particularly easy. It's something where, you know, this space of, you know, this sort of meaning space we're looking at with these images, we can sort of see things about what's out there in meaning space in a way it's a little bit easier than with text and words. But we're kind of, you know, this is sort of just the beginning of, in a sense, physicalizing using something like statistical mechanics to try and analyze what's happening inside an LLM. So I think kind of to sort of summarize, I mean, I've talked about two kinds of things. One is just the very practical aspects of using LLMs to, I think the most significant workflow there is this. You have a vague idea of what you want to do. Now I have to say to get that vague idea, you have to have an ability to sort of think computationally about things until you can express yourself in some kind of sort of with computational concepts. I mean, it's no good, you know, with some notion of how you think about the world computationally. Once you have that, you can kind of write a piece of natural language, you go sort of tell that to the LLM. The LLM will then write, you know, will write Wolfram language code or whatever, sometimes correctly, that will be an expression of what it thought you meant by the things that you said in natural language. Now sometimes when you look at that Wolfram language code, you'll say you misunderstood. It wasn't correct. You can fix that code or you can tell it to go fix the code or whatever else. But so the workflow is, you know, computationally imagine what you want to do, write it in natural language, have it kind of translated into computational language, then read the computational language. It's very important that something you can do with Wolfram language, no other, you know, that's the story of computational language, very different from programming languages which weren't intended for humans to read particularly. But so that's something where you read that computational language, you understand what it said, you fix it if you need to, then you say run that, then that becomes a sort of brick that you can use to build a whole tower of what you want on top of. And so that's, I think that's the workflow and, you know, I have to say, as we make these chat notebooks better, it's getting closer to the point where it actually makes sense, even if you know Wolfram language well, to try and use it as a way to get things started if you're not thinking very clearly, so to speak. Although as I say, to get the prompt right, you have to be kind of think expository writing because if you're totally confused, the LLM will be confused as well. But anyway, so the first thing I was talking about was this idea of how do we make use of LLMs mostly as a way to kind of get a leg up on creating kind of computational language to be able to actually do computations. I should say, by the way, I'm happy to talk about this, people interested if we have any time. But there's many use cases, like for example, we're working on a bunch of AI tutoring applications. Another use case I mentioned for physics, we've never been able to do, in Wolfram Alpha for example, we've never been able to do physics word problems. We can do that once you've turned the word problem into equations, for example, we can we can nail it. But turning going from the whole long textual description into the equations is not something we've been able to do. Now we can. Now, in fact, in practice, when you use LLMs, one of the things that's terrible, you know, you use the for example, a chat notebook or the Wolfram plugin for chat GBT, it'll sometimes, you know, correctly untangle the word problem, you know, solve the equations correctly. And then at the last minute, give the wrong answer, because it tried to inject something that it thought it knew, and it got very confused. But anyway, so lots of use cases for kind of the LLMs, their interaction with computational language. And then the second piece, really quite a disjoint piece is why did the LLMs work in the first place? This is a physics problem. And people should figure it out. And the results of figuring it out will be many important things. For example, probably most of what's inside a modern LLM doesn't need to be there. Most of what's it, you know, the actual structure you need to know enough to be able to do sort of linguistic interface, plus kind of enough common sense to support that linguistic interface is probably quite tiny compared to a current LLM. And probably you can delegate all the kind of computation and detailed computational knowledge outside of the LLM, which is an important thing in practice in terms of how much it costs to run an LLM, what kind of systems you need to run it on. But if we understand LLMs better and why they work in the first place, we have a better chance to be able to resolve those kinds of things. All right, I should stop there. Thanks. Very much. I think we have time for a couple of questions. I'd like to start with a quick one, slightly out of left field. I think you've made a good case here for physicists becoming linguists. Is there something that physicists should be learning from linguists or linguists should be transitioning to physics? Physics can give us, I don't know whether it's linguistics, I don't know whether you call it that. I don't know what you call it. But this whole idea about meaning and so on, what we're talking about, that's something that I think has now been exposed as something on which we can do experimental science on, on which we can apply physics. So I think that's the, I mean, in terms of, yeah, no, that's, I mean, it's, you know, if you look at the history of physics, right, physics has been a fantastic export field. That's, you know, populated molecular biology, it's populated, you know, quantitative finance, it's populated lots of kinds of things. It has every opportunity to populate this area and to populate and to really make some complete change to how one thinks about sort of meaning and language and so on, I believe. Well, thank you. All right, let's go to the right first. Hi, so I've seen some of your talks on the Wolfram physics project as well, and I see these n-dimensional graphs that you often use, and they often really look like neural networks. And so I wanted to ask if that was intentional or if there's some additional layers of physics going on there. They have nothing to do with neural nets. So far as I know, although there's at least one startup that believes they do, and we'll see how that works out. This is an utterly disjoint discussion about how kind of microscopic hypergraphs, you know, from them emerge space-time and quantum mechanics and so on. There is in fact a bizarre connection. Okay, this is to the deepest level of the rabbit hole immediately. There's this thing we call the rouliad, which is the entangled limit of all possible computations. Imagine you take all possible, let's say, Turing machines with all possible initial states. You run them, and they're all non-deterministic. They all have all possible rules. You run them, you get this big, messy thing. There's only one of it. It is the complete representation of all possible computations. And then that, I claim, is sort of the ultimate foundation of physics and mathematics, actually. And our physical universe ends up being, we have to exist within that. And so our physical universe ends up being our kind of, our sampling of that rouliad object. And here's the fascinating fact, at least I think it's interesting, is that if you assume that we as observers of the rouliad have two characteristics. One, we are computationally bounded. Two, we believe we are persistent in time. We believe we have a single thread of experience. Those two attributes alone are sufficient to give you, not just qualitatively, but exactly, general relativity and quantum mechanics. That's kind of exciting, because it tells you that it didn't need to be that way. The aliens who don't have those characteristics don't have to have general relativity and quantum mechanics. But it kind of gives you, I mean, this is a huge condensation of a very large amount of stuff. But that's, so okay, how does that relate to all of this? When I was showing you those weird cat pictures and things, the, this is a, one of the reasons I was studying weird cat pictures is because this is a way of understanding sort of different slices of this roulial space concept. There's much more to say about this. That's a way too, way too brief a description. Okay, let's take a question from the left now. Yeah, hi. So I've tried to use LLMs in research so far without great success. I'm a theoretical physicist. Something that would be, there's a weird echo here, I don't know. Anyway, sorry, something that would be really useful would be if I could have something where, you know, 300 page paper, I don't know, by Edward and Pierce. And I can ask it, can you give me a one page summary of that, you know, where it would be correct and where it would already kind of know from talking me to before, like what are the kind of things that I know and then I don't know. So I mean, how far are we from that? Well, you know, for example, in our company, you know, someone makes a daily digest of interesting papers about LLMs. Okay, and I got fed up trying to read the abstracts, because every abstract is written differently. They're very ponderous in many cases. I said, just get the frigging LLMs and make a two sentence summary of every paper. It works great. I mean, you can scan down this thing really quickly. The fact that the LLMs text is rather boring is actually good, because all the text is consistent, and you kind of can just see what's happening. Now, you know, do I get the essence of every paper correctly? Maybe not. But that's a statistical thing anyway, I might miss it from the abstract too. So that's a pretty good use case that I recommend, actually. In terms of the can you, if you want it to summarize for you, particularly, I think that's coming. And the, you know, kind of AI tutoring system that we're building, that's kind of one of the big ideas is know the student and be able to figure out, first of all, how is the student confused? Because one of the things that you typically can't do in sort of watching what a student does is watching the working that the student follows. But you can with an LLM, and you can kind of see, you know, how is the student confused as the student is doing their work? And then, so then the question is, will it come to the point where, you know, the LLM will know enough about me from having read, I mean, me personally, I've put 50 million words out there. So it's, I'm pretty easy to learn about. And we're trying to get an LLM to be me, so to speak, which will save time, maybe, maybe not. But anyway, the point is, I'm, you know, given that the LLM knows about you, I think there is a very good chance that the LLM will be able to say the one thing you need to know, because you're confused about this or you don't know this, is this one fact. And you say, oh, that's the thing I want to know from that paper, all the other stuff is irrelevant. I think that's pretty realistic. And I think it's reasonably short-term. But you've got to understand, like everything with machine learning, it's kind of an 80% success, 90% success story. And whenever you have a situation, like looking at these abstracts, where, you know, if I notice an abstract that looks really interesting, it's a win. If I miss one, it's not a disaster. That's a good use case. If it's a case where you're trying to do the next great, you know, precise physics calculation, it's probably a big lose to use an LLM where it might be wrong 10% of the time, you don't know which 10%. Okay, I understand that there are many, many questions, but at some point, everyone does have to go home, unfortunately. So I'm afraid we're going to have to cut off the questioning there. Let's thank our speaker again.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.96, "text": " So our final speaker of the day is an external speaker.", "tokens": [50364, 407, 527, 2572, 8145, 295, 264, 786, 307, 364, 8320, 8145, 13, 50612], "temperature": 0.0, "avg_logprob": -0.18002848105855507, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.050775282084941864}, {"id": 1, "seek": 0, "start": 4.96, "end": 9.120000000000001, "text": " So this is Stephen Wolfram from Wolfram Research.", "tokens": [50612, 407, 341, 307, 13391, 16634, 2356, 490, 16634, 2356, 10303, 13, 50820], "temperature": 0.0, "avg_logprob": -0.18002848105855507, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.050775282084941864}, {"id": 2, "seek": 0, "start": 9.120000000000001, "end": 15.24, "text": " So Stephen got his PhD in Caltech, PhD in physics from Caltech at the age of 20 working", "tokens": [50820, 407, 13391, 658, 702, 14476, 294, 3511, 25970, 11, 14476, 294, 10649, 490, 3511, 25970, 412, 264, 3205, 295, 945, 1364, 51126], "temperature": 0.0, "avg_logprob": -0.18002848105855507, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.050775282084941864}, {"id": 3, "seek": 0, "start": 15.24, "end": 18.92, "text": " on high-energy physics quantum field theory and cosmology.", "tokens": [51126, 322, 1090, 12, 49016, 10649, 13018, 2519, 5261, 293, 22207, 1793, 13, 51310], "temperature": 0.0, "avg_logprob": -0.18002848105855507, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.050775282084941864}, {"id": 4, "seek": 0, "start": 18.92, "end": 23.44, "text": " He's the founder and president of Wolfram Research, developers of Mathematica and Wolfram", "tokens": [51310, 634, 311, 264, 14917, 293, 3868, 295, 16634, 2356, 10303, 11, 8849, 295, 15776, 8615, 2262, 293, 16634, 2356, 51536], "temperature": 0.0, "avg_logprob": -0.18002848105855507, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.050775282084941864}, {"id": 5, "seek": 0, "start": 23.44, "end": 26.64, "text": " Alpha, tools that I'm sure you've all used.", "tokens": [51536, 20588, 11, 3873, 300, 286, 478, 988, 291, 600, 439, 1143, 13, 51696], "temperature": 0.0, "avg_logprob": -0.18002848105855507, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.050775282084941864}, {"id": 6, "seek": 2664, "start": 26.64, "end": 30.52, "text": " And recently I think something interesting that Wolfram did was release a plug-in for", "tokens": [50364, 400, 3938, 286, 519, 746, 1880, 300, 16634, 2356, 630, 390, 4374, 257, 5452, 12, 259, 337, 50558], "temperature": 0.0, "avg_logprob": -0.14271451808788158, "compression_ratio": 1.5703125, "no_speech_prob": 0.002386456821113825}, {"id": 7, "seek": 2664, "start": 30.52, "end": 36.04, "text": " chat GPT, giving access to the Wolfram Alpha computational intelligence engine, which I'm", "tokens": [50558, 5081, 26039, 51, 11, 2902, 2105, 281, 264, 16634, 2356, 20588, 28270, 7599, 2848, 11, 597, 286, 478, 50834], "temperature": 0.0, "avg_logprob": -0.14271451808788158, "compression_ratio": 1.5703125, "no_speech_prob": 0.002386456821113825}, {"id": 8, "seek": 2664, "start": 36.04, "end": 38.72, "text": " sure we will hear a little bit about.", "tokens": [50834, 988, 321, 486, 1568, 257, 707, 857, 466, 13, 50968], "temperature": 0.0, "avg_logprob": -0.14271451808788158, "compression_ratio": 1.5703125, "no_speech_prob": 0.002386456821113825}, {"id": 9, "seek": 2664, "start": 38.72, "end": 44.2, "text": " But yeah, if you're ready to go, I'll give you the floor.", "tokens": [50968, 583, 1338, 11, 498, 291, 434, 1919, 281, 352, 11, 286, 603, 976, 291, 264, 4123, 13, 51242], "temperature": 0.0, "avg_logprob": -0.14271451808788158, "compression_ratio": 1.5703125, "no_speech_prob": 0.002386456821113825}, {"id": 10, "seek": 2664, "start": 44.2, "end": 46.519999999999996, "text": " So I guess I want to talk about two things today.", "tokens": [51242, 407, 286, 2041, 286, 528, 281, 751, 466, 732, 721, 965, 13, 51358], "temperature": 0.0, "avg_logprob": -0.14271451808788158, "compression_ratio": 1.5703125, "no_speech_prob": 0.002386456821113825}, {"id": 11, "seek": 2664, "start": 46.519999999999996, "end": 53.58, "text": " I want to talk about using LLMs for physics and how physics can help study LLMs.", "tokens": [51358, 286, 528, 281, 751, 466, 1228, 441, 43, 26386, 337, 10649, 293, 577, 10649, 393, 854, 2979, 441, 43, 26386, 13, 51711], "temperature": 0.0, "avg_logprob": -0.14271451808788158, "compression_ratio": 1.5703125, "no_speech_prob": 0.002386456821113825}, {"id": 12, "seek": 5358, "start": 53.58, "end": 63.46, "text": " So to start off talking about how physics can use LLMs, the first thing is what do LLMs", "tokens": [50364, 407, 281, 722, 766, 1417, 466, 577, 10649, 393, 764, 441, 43, 26386, 11, 264, 700, 551, 307, 437, 360, 441, 43, 26386, 50858], "temperature": 0.0, "avg_logprob": -0.14081972203356155, "compression_ratio": 1.5589519650655022, "no_speech_prob": 0.002873364370316267}, {"id": 13, "seek": 5358, "start": 63.46, "end": 64.46, "text": " fundamentally do?", "tokens": [50858, 17879, 360, 30, 50908], "temperature": 0.0, "avg_logprob": -0.14081972203356155, "compression_ratio": 1.5589519650655022, "no_speech_prob": 0.002873364370316267}, {"id": 14, "seek": 5358, "start": 64.46, "end": 70.74, "text": " LLMs have taken four billion pages from the web and a bunch of books, and they've kind", "tokens": [50908, 441, 43, 26386, 362, 2726, 1451, 5218, 7183, 490, 264, 3670, 293, 257, 3840, 295, 3642, 11, 293, 436, 600, 733, 51222], "temperature": 0.0, "avg_logprob": -0.14081972203356155, "compression_ratio": 1.5589519650655022, "no_speech_prob": 0.002873364370316267}, {"id": 15, "seek": 5358, "start": 70.74, "end": 75.58, "text": " of ground that up to find the statistics of everything that's said there.", "tokens": [51222, 295, 2727, 300, 493, 281, 915, 264, 12523, 295, 1203, 300, 311, 848, 456, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14081972203356155, "compression_ratio": 1.5589519650655022, "no_speech_prob": 0.002873364370316267}, {"id": 16, "seek": 5358, "start": 75.58, "end": 80.82, "text": " And when you ask an LLM something, its mission is to try and produce reasonable text based", "tokens": [51464, 400, 562, 291, 1029, 364, 441, 43, 44, 746, 11, 1080, 4447, 307, 281, 853, 293, 5258, 10585, 2487, 2361, 51726], "temperature": 0.0, "avg_logprob": -0.14081972203356155, "compression_ratio": 1.5589519650655022, "no_speech_prob": 0.002873364370316267}, {"id": 17, "seek": 8082, "start": 80.82, "end": 84.69999999999999, "text": " on kind of the statistics of what it saw on the web and so on.", "tokens": [50364, 322, 733, 295, 264, 12523, 295, 437, 309, 1866, 322, 264, 3670, 293, 370, 322, 13, 50558], "temperature": 0.0, "avg_logprob": -0.09443742646945744, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.05430589243769646}, {"id": 18, "seek": 8082, "start": 84.69999999999999, "end": 89.22, "text": " And as we'll talk about later, the things it extracts as its statistics are surprisingly", "tokens": [50558, 400, 382, 321, 603, 751, 466, 1780, 11, 264, 721, 309, 8947, 82, 382, 1080, 12523, 366, 17600, 50784], "temperature": 0.0, "avg_logprob": -0.09443742646945744, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.05430589243769646}, {"id": 19, "seek": 8082, "start": 89.22, "end": 94.66, "text": " sophisticated and include having sort of found a kind of semantic grammar of language which", "tokens": [50784, 16950, 293, 4090, 1419, 1333, 295, 1352, 257, 733, 295, 47982, 22317, 295, 2856, 597, 51056], "temperature": 0.0, "avg_logprob": -0.09443742646945744, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.05430589243769646}, {"id": 20, "seek": 8082, "start": 94.66, "end": 99.5, "text": " allows it to kind of say things that make sense, at least make some kind of sense.", "tokens": [51056, 4045, 309, 281, 733, 295, 584, 721, 300, 652, 2020, 11, 412, 1935, 652, 512, 733, 295, 2020, 13, 51298], "temperature": 0.0, "avg_logprob": -0.09443742646945744, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.05430589243769646}, {"id": 21, "seek": 8082, "start": 99.5, "end": 102.5, "text": " They may be fact, they may be fiction, but they kind of fit together in a way that makes", "tokens": [51298, 814, 815, 312, 1186, 11, 436, 815, 312, 13266, 11, 457, 436, 733, 295, 3318, 1214, 294, 257, 636, 300, 1669, 51448], "temperature": 0.0, "avg_logprob": -0.09443742646945744, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.05430589243769646}, {"id": 22, "seek": 8082, "start": 102.5, "end": 103.5, "text": " sense.", "tokens": [51448, 2020, 13, 51498], "temperature": 0.0, "avg_logprob": -0.09443742646945744, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.05430589243769646}, {"id": 23, "seek": 8082, "start": 103.5, "end": 109.22, "text": " But so what the LLM is fundamentally doing is it's taking stuff we humans have put on", "tokens": [51498, 583, 370, 437, 264, 441, 43, 44, 307, 17879, 884, 307, 309, 311, 1940, 1507, 321, 6255, 362, 829, 322, 51784], "temperature": 0.0, "avg_logprob": -0.09443742646945744, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.05430589243769646}, {"id": 24, "seek": 10922, "start": 109.22, "end": 113.62, "text": " the web, and it's feeding that back to us based on things that we ask it about.", "tokens": [50364, 264, 3670, 11, 293, 309, 311, 12919, 300, 646, 281, 505, 2361, 322, 721, 300, 321, 1029, 309, 466, 13, 50584], "temperature": 0.0, "avg_logprob": -0.16470391822583746, "compression_ratio": 1.8251748251748252, "no_speech_prob": 0.04812227934598923}, {"id": 25, "seek": 10922, "start": 113.62, "end": 118.22, "text": " It's feeding us back sort of reasonable things that we could have said on the web even though", "tokens": [50584, 467, 311, 12919, 505, 646, 1333, 295, 10585, 721, 300, 321, 727, 362, 848, 322, 264, 3670, 754, 1673, 50814], "temperature": 0.0, "avg_logprob": -0.16470391822583746, "compression_ratio": 1.8251748251748252, "no_speech_prob": 0.04812227934598923}, {"id": 26, "seek": 10922, "start": 118.22, "end": 120.5, "text": " we might not actually have done so.", "tokens": [50814, 321, 1062, 406, 767, 362, 1096, 370, 13, 50928], "temperature": 0.0, "avg_logprob": -0.16470391822583746, "compression_ratio": 1.8251748251748252, "no_speech_prob": 0.04812227934598923}, {"id": 27, "seek": 10922, "start": 120.5, "end": 126.7, "text": " So it's a good way to generate our, I see LLMs actually as a practical matter as kind", "tokens": [50928, 407, 309, 311, 257, 665, 636, 281, 8460, 527, 11, 286, 536, 441, 43, 26386, 767, 382, 257, 8496, 1871, 382, 733, 51238], "temperature": 0.0, "avg_logprob": -0.16470391822583746, "compression_ratio": 1.8251748251748252, "no_speech_prob": 0.04812227934598923}, {"id": 28, "seek": 10922, "start": 126.7, "end": 129.54, "text": " of an important layer of a linguistic user interface.", "tokens": [51238, 295, 364, 1021, 4583, 295, 257, 43002, 4195, 9226, 13, 51380], "temperature": 0.0, "avg_logprob": -0.16470391822583746, "compression_ratio": 1.8251748251748252, "no_speech_prob": 0.04812227934598923}, {"id": 29, "seek": 10922, "start": 129.54, "end": 133.42, "text": " We have graphical user interfaces, now we have a linguistic user interface where you", "tokens": [51380, 492, 362, 35942, 4195, 28416, 11, 586, 321, 362, 257, 43002, 4195, 9226, 689, 291, 51574], "temperature": 0.0, "avg_logprob": -0.16470391822583746, "compression_ratio": 1.8251748251748252, "no_speech_prob": 0.04812227934598923}, {"id": 30, "seek": 10922, "start": 133.42, "end": 138.86, "text": " can take like five points you wanted to make, you can puff those out with an LLM into a", "tokens": [51574, 393, 747, 411, 1732, 2793, 291, 1415, 281, 652, 11, 291, 393, 19613, 729, 484, 365, 364, 441, 43, 44, 666, 257, 51846], "temperature": 0.0, "avg_logprob": -0.16470391822583746, "compression_ratio": 1.8251748251748252, "no_speech_prob": 0.04812227934598923}, {"id": 31, "seek": 13886, "start": 138.86, "end": 143.70000000000002, "text": " giant report, then maybe the person you're sending that report to really wants to know", "tokens": [50364, 7410, 2275, 11, 550, 1310, 264, 954, 291, 434, 7750, 300, 2275, 281, 534, 2738, 281, 458, 50606], "temperature": 0.0, "avg_logprob": -0.14245054100741858, "compression_ratio": 1.921875, "no_speech_prob": 0.061751652508974075}, {"id": 32, "seek": 13886, "start": 143.70000000000002, "end": 147.98000000000002, "text": " only two things, so they use an LLM again, and they grind it down and get the two things", "tokens": [50606, 787, 732, 721, 11, 370, 436, 764, 364, 441, 43, 44, 797, 11, 293, 436, 16700, 309, 760, 293, 483, 264, 732, 721, 50820], "temperature": 0.0, "avg_logprob": -0.14245054100741858, "compression_ratio": 1.921875, "no_speech_prob": 0.061751652508974075}, {"id": 33, "seek": 13886, "start": 147.98000000000002, "end": 151.06, "text": " they actually wanted to know from that report.", "tokens": [50820, 436, 767, 1415, 281, 458, 490, 300, 2275, 13, 50974], "temperature": 0.0, "avg_logprob": -0.14245054100741858, "compression_ratio": 1.921875, "no_speech_prob": 0.061751652508974075}, {"id": 34, "seek": 13886, "start": 151.06, "end": 156.38000000000002, "text": " And that's a very sensible transport layer using kind of the big report as the transport", "tokens": [50974, 400, 300, 311, 257, 588, 25380, 5495, 4583, 1228, 733, 295, 264, 955, 2275, 382, 264, 5495, 51240], "temperature": 0.0, "avg_logprob": -0.14245054100741858, "compression_ratio": 1.921875, "no_speech_prob": 0.061751652508974075}, {"id": 35, "seek": 13886, "start": 156.38000000000002, "end": 161.34, "text": " layer for what's going on, and maybe that will happen with academic papers as well,", "tokens": [51240, 4583, 337, 437, 311, 516, 322, 11, 293, 1310, 300, 486, 1051, 365, 7778, 10577, 382, 731, 11, 51488], "temperature": 0.0, "avg_logprob": -0.14245054100741858, "compression_ratio": 1.921875, "no_speech_prob": 0.061751652508974075}, {"id": 36, "seek": 13886, "start": 161.34, "end": 166.34, "text": " that they will be just the linguistic transport layer for the actual content of what's going", "tokens": [51488, 300, 436, 486, 312, 445, 264, 43002, 5495, 4583, 337, 264, 3539, 2701, 295, 437, 311, 516, 51738], "temperature": 0.0, "avg_logprob": -0.14245054100741858, "compression_ratio": 1.921875, "no_speech_prob": 0.061751652508974075}, {"id": 37, "seek": 13886, "start": 166.34, "end": 167.34, "text": " on.", "tokens": [51738, 322, 13, 51788], "temperature": 0.0, "avg_logprob": -0.14245054100741858, "compression_ratio": 1.921875, "no_speech_prob": 0.061751652508974075}, {"id": 38, "seek": 16734, "start": 167.34, "end": 172.5, "text": " I would like to think that we can do better than that, in a sense, math has provided us", "tokens": [50364, 286, 576, 411, 281, 519, 300, 321, 393, 360, 1101, 813, 300, 11, 294, 257, 2020, 11, 5221, 575, 5649, 505, 50622], "temperature": 0.0, "avg_logprob": -0.12681899751935685, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.005264633800834417}, {"id": 39, "seek": 16734, "start": 172.5, "end": 176.62, "text": " a notation that's better than that, computational language, the kind of thing I've worked on", "tokens": [50622, 257, 24657, 300, 311, 1101, 813, 300, 11, 28270, 2856, 11, 264, 733, 295, 551, 286, 600, 2732, 322, 50828], "temperature": 0.0, "avg_logprob": -0.12681899751935685, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.005264633800834417}, {"id": 40, "seek": 16734, "start": 176.62, "end": 180.74, "text": " for the last 40 years or so, is an attempt to kind of formalize statements about the", "tokens": [50828, 337, 264, 1036, 3356, 924, 420, 370, 11, 307, 364, 5217, 281, 733, 295, 9860, 1125, 12363, 466, 264, 51034], "temperature": 0.0, "avg_logprob": -0.12681899751935685, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.005264633800834417}, {"id": 41, "seek": 16734, "start": 180.74, "end": 186.42000000000002, "text": " world in a way that's kind of better than just giving it as natural language.", "tokens": [51034, 1002, 294, 257, 636, 300, 311, 733, 295, 1101, 813, 445, 2902, 309, 382, 3303, 2856, 13, 51318], "temperature": 0.0, "avg_logprob": -0.12681899751935685, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.005264633800834417}, {"id": 42, "seek": 16734, "start": 186.42000000000002, "end": 191.18, "text": " But anyway, that's kind of, you know, what the LLM is doing is it's taking what's there", "tokens": [51318, 583, 4033, 11, 300, 311, 733, 295, 11, 291, 458, 11, 437, 264, 441, 43, 44, 307, 884, 307, 309, 311, 1940, 437, 311, 456, 51556], "temperature": 0.0, "avg_logprob": -0.12681899751935685, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.005264633800834417}, {"id": 43, "seek": 16734, "start": 191.18, "end": 195.58, "text": " on the web, it's reconstituting it and feeding it back to us, it's not going to be able to", "tokens": [51556, 322, 264, 3670, 11, 309, 311, 16891, 270, 10861, 309, 293, 12919, 309, 646, 281, 505, 11, 309, 311, 406, 516, 281, 312, 1075, 281, 51776], "temperature": 0.0, "avg_logprob": -0.12681899751935685, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.005264633800834417}, {"id": 44, "seek": 19558, "start": 195.58, "end": 197.9, "text": " discover fundamentally new things.", "tokens": [50364, 4411, 17879, 777, 721, 13, 50480], "temperature": 0.0, "avg_logprob": -0.15680573049899751, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.028454869985580444}, {"id": 45, "seek": 19558, "start": 197.9, "end": 202.66000000000003, "text": " Well, with a couple of exceptions there, I think one thing it can do is, if there are", "tokens": [50480, 1042, 11, 365, 257, 1916, 295, 22847, 456, 11, 286, 519, 472, 551, 309, 393, 360, 307, 11, 498, 456, 366, 50718], "temperature": 0.0, "avg_logprob": -0.15680573049899751, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.028454869985580444}, {"id": 46, "seek": 19558, "start": 202.66000000000003, "end": 206.58, "text": " analogies that might be found between this place and that place, it's really good at", "tokens": [50718, 16660, 530, 300, 1062, 312, 1352, 1296, 341, 1081, 293, 300, 1081, 11, 309, 311, 534, 665, 412, 50914], "temperature": 0.0, "avg_logprob": -0.15680573049899751, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.028454869985580444}, {"id": 47, "seek": 19558, "start": 206.58, "end": 209.86, "text": " finding kind of statistical facts from language.", "tokens": [50914, 5006, 733, 295, 22820, 9130, 490, 2856, 13, 51078], "temperature": 0.0, "avg_logprob": -0.15680573049899751, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.028454869985580444}, {"id": 48, "seek": 19558, "start": 209.86, "end": 214.18, "text": " Usually we're used to doing statistics from numbers, but LLMs manage to do statistics", "tokens": [51078, 11419, 321, 434, 1143, 281, 884, 12523, 490, 3547, 11, 457, 441, 43, 26386, 3067, 281, 360, 12523, 51294], "temperature": 0.0, "avg_logprob": -0.15680573049899751, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.028454869985580444}, {"id": 49, "seek": 19558, "start": 214.18, "end": 216.42000000000002, "text": " from text as well.", "tokens": [51294, 490, 2487, 382, 731, 13, 51406], "temperature": 0.0, "avg_logprob": -0.15680573049899751, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.028454869985580444}, {"id": 50, "seek": 19558, "start": 216.42000000000002, "end": 222.62, "text": " And so if you say, well, what was the trend in fashion in 1955, there's a good chance", "tokens": [51406, 400, 370, 498, 291, 584, 11, 731, 11, 437, 390, 264, 6028, 294, 6700, 294, 46881, 11, 456, 311, 257, 665, 2931, 51716], "temperature": 0.0, "avg_logprob": -0.15680573049899751, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.028454869985580444}, {"id": 51, "seek": 22262, "start": 222.62, "end": 227.26, "text": " that the LLM will be able to take sort of the stuff that ground up from the web and", "tokens": [50364, 300, 264, 441, 43, 44, 486, 312, 1075, 281, 747, 1333, 295, 264, 1507, 300, 2727, 493, 490, 264, 3670, 293, 50596], "temperature": 0.0, "avg_logprob": -0.14175431481723128, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0033945892937481403}, {"id": 52, "seek": 22262, "start": 227.26, "end": 228.46, "text": " answer that.", "tokens": [50596, 1867, 300, 13, 50656], "temperature": 0.0, "avg_logprob": -0.14175431481723128, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0033945892937481403}, {"id": 53, "seek": 22262, "start": 228.46, "end": 233.14000000000001, "text": " And similarly, if you say, well, could there be an analogy between, you know, metamathematics", "tokens": [50656, 400, 14138, 11, 498, 291, 584, 11, 731, 11, 727, 456, 312, 364, 21663, 1296, 11, 291, 458, 11, 1131, 335, 998, 37541, 50890], "temperature": 0.0, "avg_logprob": -0.14175431481723128, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0033945892937481403}, {"id": 54, "seek": 22262, "start": 233.14000000000001, "end": 238.34, "text": " and general relativity, there's a chance that it could figure that out, because it can see", "tokens": [50890, 293, 2674, 45675, 11, 456, 311, 257, 2931, 300, 309, 727, 2573, 300, 484, 11, 570, 309, 393, 536, 51150], "temperature": 0.0, "avg_logprob": -0.14175431481723128, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0033945892937481403}, {"id": 55, "seek": 22262, "start": 238.34, "end": 243.66, "text": " that the kind of the structure of what's said about those two areas has a certain similarity.", "tokens": [51150, 300, 264, 733, 295, 264, 3877, 295, 437, 311, 848, 466, 729, 732, 3179, 575, 257, 1629, 32194, 13, 51416], "temperature": 0.0, "avg_logprob": -0.14175431481723128, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0033945892937481403}, {"id": 56, "seek": 22262, "start": 243.66, "end": 249.58, "text": " Something that seems like bizarrely magic to us humans, something that some of us humans", "tokens": [51416, 6595, 300, 2544, 411, 18265, 356, 5585, 281, 505, 6255, 11, 746, 300, 512, 295, 505, 6255, 51712], "temperature": 0.0, "avg_logprob": -0.14175431481723128, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0033945892937481403}, {"id": 57, "seek": 24958, "start": 249.62, "end": 253.54000000000002, "text": " kind of pride ourselves on being able to figure out such analogies, but I think we may be", "tokens": [50366, 733, 295, 10936, 4175, 322, 885, 1075, 281, 2573, 484, 1270, 16660, 530, 11, 457, 286, 519, 321, 815, 312, 50562], "temperature": 0.0, "avg_logprob": -0.15033082678766535, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.025088658556342125}, {"id": 58, "seek": 24958, "start": 253.54000000000002, "end": 256.78000000000003, "text": " about to be outdone by the AIs.", "tokens": [50562, 466, 281, 312, 484, 45939, 538, 264, 316, 6802, 13, 50724], "temperature": 0.0, "avg_logprob": -0.15033082678766535, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.025088658556342125}, {"id": 59, "seek": 24958, "start": 256.78000000000003, "end": 262.54, "text": " But okay, so there's sort of a question of the LLM is doing this kind of taking language", "tokens": [50724, 583, 1392, 11, 370, 456, 311, 1333, 295, 257, 1168, 295, 264, 441, 43, 44, 307, 884, 341, 733, 295, 1940, 2856, 51012], "temperature": 0.0, "avg_logprob": -0.15033082678766535, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.025088658556342125}, {"id": 60, "seek": 24958, "start": 262.54, "end": 267.3, "text": " from the web, giving us back some sort of reconstituted version of that.", "tokens": [51012, 490, 264, 3670, 11, 2902, 505, 646, 512, 1333, 295, 16891, 270, 4866, 3037, 295, 300, 13, 51250], "temperature": 0.0, "avg_logprob": -0.15033082678766535, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.025088658556342125}, {"id": 61, "seek": 24958, "start": 267.3, "end": 273.34000000000003, "text": " But there's more to figure out things, and for example, physics, as it has emerged since", "tokens": [51250, 583, 456, 311, 544, 281, 2573, 484, 721, 11, 293, 337, 1365, 11, 10649, 11, 382, 309, 575, 20178, 1670, 51552], "temperature": 0.0, "avg_logprob": -0.15033082678766535, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.025088658556342125}, {"id": 62, "seek": 27334, "start": 273.34, "end": 280.65999999999997, "text": " basically Newton and the Newton's big idea from 1687 was, as he called it, the mathematical", "tokens": [50364, 1936, 19541, 293, 264, 19541, 311, 955, 1558, 490, 3165, 23853, 390, 11, 382, 415, 1219, 309, 11, 264, 18894, 50730], "temperature": 0.0, "avg_logprob": -0.16095565289867167, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.04879544675350189}, {"id": 63, "seek": 27334, "start": 280.65999999999997, "end": 282.21999999999997, "text": " principles of natural philosophy.", "tokens": [50730, 9156, 295, 3303, 10675, 13, 50808], "temperature": 0.0, "avg_logprob": -0.16095565289867167, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.04879544675350189}, {"id": 64, "seek": 27334, "start": 282.21999999999997, "end": 288.14, "text": " In other words, there is a more formal method for dealing with natural philosophy, i.e. physics,", "tokens": [50808, 682, 661, 2283, 11, 456, 307, 257, 544, 9860, 3170, 337, 6260, 365, 3303, 10675, 11, 741, 13, 68, 13, 10649, 11, 51104], "temperature": 0.0, "avg_logprob": -0.16095565289867167, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.04879544675350189}, {"id": 65, "seek": 27334, "start": 288.14, "end": 291.97999999999996, "text": " than just talking about it as philosophy, so to speak.", "tokens": [51104, 813, 445, 1417, 466, 309, 382, 10675, 11, 370, 281, 1710, 13, 51296], "temperature": 0.0, "avg_logprob": -0.16095565289867167, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.04879544675350189}, {"id": 66, "seek": 27334, "start": 291.97999999999996, "end": 296.65999999999997, "text": " So this kind of notion of formalization that started in those days and has emerged into", "tokens": [51296, 407, 341, 733, 295, 10710, 295, 9860, 2144, 300, 1409, 294, 729, 1708, 293, 575, 20178, 666, 51530], "temperature": 0.0, "avg_logprob": -0.16095565289867167, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.04879544675350189}, {"id": 67, "seek": 27334, "start": 296.65999999999997, "end": 300.02, "text": " sort of modern mathematical methods in physics.", "tokens": [51530, 1333, 295, 4363, 18894, 7150, 294, 10649, 13, 51698], "temperature": 0.0, "avg_logprob": -0.16095565289867167, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.04879544675350189}, {"id": 68, "seek": 30002, "start": 300.02, "end": 306.5, "text": " Well, this kind of idea that you can do better than just pure thought about something.", "tokens": [50364, 1042, 11, 341, 733, 295, 1558, 300, 291, 393, 360, 1101, 813, 445, 6075, 1194, 466, 746, 13, 50688], "temperature": 0.0, "avg_logprob": -0.11289675452492454, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.002857303712517023}, {"id": 69, "seek": 30002, "start": 306.5, "end": 310.46, "text": " You can have a formalization that allows you to make further progress, which is most of", "tokens": [50688, 509, 393, 362, 257, 9860, 2144, 300, 4045, 291, 281, 652, 3052, 4205, 11, 597, 307, 881, 295, 50886], "temperature": 0.0, "avg_logprob": -0.11289675452492454, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.002857303712517023}, {"id": 70, "seek": 30002, "start": 310.46, "end": 312.97999999999996, "text": " the story of physics as we know it today.", "tokens": [50886, 264, 1657, 295, 10649, 382, 321, 458, 309, 965, 13, 51012], "temperature": 0.0, "avg_logprob": -0.11289675452492454, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.002857303712517023}, {"id": 71, "seek": 30002, "start": 312.97999999999996, "end": 318.06, "text": " So if you're a pure LLM that's just dealing with language, that's just dealing with kind", "tokens": [51012, 407, 498, 291, 434, 257, 6075, 441, 43, 44, 300, 311, 445, 6260, 365, 2856, 11, 300, 311, 445, 6260, 365, 733, 51266], "temperature": 0.0, "avg_logprob": -0.11289675452492454, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.002857303712517023}, {"id": 72, "seek": 30002, "start": 318.06, "end": 323.82, "text": " of pure common sense kinds of things, then you are stuck in the pre-Newtonian kind of", "tokens": [51266, 295, 6075, 2689, 2020, 3685, 295, 721, 11, 550, 291, 366, 5541, 294, 264, 659, 12, 18278, 1756, 952, 733, 295, 51554], "temperature": 0.0, "avg_logprob": -0.11289675452492454, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.002857303712517023}, {"id": 73, "seek": 30002, "start": 323.82, "end": 326.58, "text": " paradigm for how you do physics.", "tokens": [51554, 24709, 337, 577, 291, 360, 10649, 13, 51692], "temperature": 0.0, "avg_logprob": -0.11289675452492454, "compression_ratio": 1.6758893280632412, "no_speech_prob": 0.002857303712517023}, {"id": 74, "seek": 32658, "start": 326.58, "end": 333.3, "text": " Well, so how do we kind of connect kind of this sort of linguistic layer of thinking", "tokens": [50364, 1042, 11, 370, 577, 360, 321, 733, 295, 1745, 733, 295, 341, 1333, 295, 43002, 4583, 295, 1953, 50700], "temperature": 0.0, "avg_logprob": -0.16177909851074218, "compression_ratio": 1.8545454545454545, "no_speech_prob": 0.006350256968289614}, {"id": 75, "seek": 32658, "start": 333.3, "end": 339.46, "text": " about things with kind of the more formal, we might now say, computational layer of thinking", "tokens": [50700, 466, 721, 365, 733, 295, 264, 544, 9860, 11, 321, 1062, 586, 584, 11, 28270, 4583, 295, 1953, 51008], "temperature": 0.0, "avg_logprob": -0.16177909851074218, "compression_ratio": 1.8545454545454545, "no_speech_prob": 0.006350256968289614}, {"id": 76, "seek": 32658, "start": 339.46, "end": 340.53999999999996, "text": " about things.", "tokens": [51008, 466, 721, 13, 51062], "temperature": 0.0, "avg_logprob": -0.16177909851074218, "compression_ratio": 1.8545454545454545, "no_speech_prob": 0.006350256968289614}, {"id": 77, "seek": 32658, "start": 340.53999999999996, "end": 346.29999999999995, "text": " So I've spent most of my life kind of trying to figure out how you can sort of describe", "tokens": [51062, 407, 286, 600, 4418, 881, 295, 452, 993, 733, 295, 1382, 281, 2573, 484, 577, 291, 393, 1333, 295, 6786, 51350], "temperature": 0.0, "avg_logprob": -0.16177909851074218, "compression_ratio": 1.8545454545454545, "no_speech_prob": 0.006350256968289614}, {"id": 78, "seek": 32658, "start": 346.29999999999995, "end": 349.41999999999996, "text": " the world formally using computation.", "tokens": [51350, 264, 1002, 25983, 1228, 24903, 13, 51506], "temperature": 0.0, "avg_logprob": -0.16177909851074218, "compression_ratio": 1.8545454545454545, "no_speech_prob": 0.006350256968289614}, {"id": 79, "seek": 32658, "start": 349.41999999999996, "end": 354.82, "text": " And so the question is, can you connect kind of the LLM world to this computational world?", "tokens": [51506, 400, 370, 264, 1168, 307, 11, 393, 291, 1745, 733, 295, 264, 441, 43, 44, 1002, 281, 341, 28270, 1002, 30, 51776], "temperature": 0.0, "avg_logprob": -0.16177909851074218, "compression_ratio": 1.8545454545454545, "no_speech_prob": 0.006350256968289614}, {"id": 80, "seek": 35482, "start": 354.82, "end": 359.94, "text": " And actually, back in March, we did something with the folks at OpenAI of making a plug-in", "tokens": [50364, 400, 767, 11, 646, 294, 6129, 11, 321, 630, 746, 365, 264, 4024, 412, 7238, 48698, 295, 1455, 257, 5452, 12, 259, 50620], "temperature": 0.0, "avg_logprob": -0.15895024785455666, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.016570067033171654}, {"id": 81, "seek": 35482, "start": 359.94, "end": 368.14, "text": " to chat GBT that allows it to kind of use our computational language from within the", "tokens": [50620, 281, 5081, 26809, 51, 300, 4045, 309, 281, 733, 295, 764, 527, 28270, 2856, 490, 1951, 264, 51030], "temperature": 0.0, "avg_logprob": -0.15895024785455666, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.016570067033171654}, {"id": 82, "seek": 35482, "start": 368.14, "end": 369.14, "text": " LLM.", "tokens": [51030, 441, 43, 44, 13, 51080], "temperature": 0.0, "avg_logprob": -0.15895024785455666, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.016570067033171654}, {"id": 83, "seek": 35482, "start": 369.14, "end": 371.86, "text": " And I'll show you, I haven't actually used this interface for a while for reasons that", "tokens": [51080, 400, 286, 603, 855, 291, 11, 286, 2378, 380, 767, 1143, 341, 9226, 337, 257, 1339, 337, 4112, 300, 51216], "temperature": 0.0, "avg_logprob": -0.15895024785455666, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.016570067033171654}, {"id": 84, "seek": 35482, "start": 371.86, "end": 373.42, "text": " I'm about to show you.", "tokens": [51216, 286, 478, 466, 281, 855, 291, 13, 51294], "temperature": 0.0, "avg_logprob": -0.15895024785455666, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.016570067033171654}, {"id": 85, "seek": 35482, "start": 373.42, "end": 383.21999999999997, "text": " But let's see, if we say make a picture of an airy function or something, let's see.", "tokens": [51294, 583, 718, 311, 536, 11, 498, 321, 584, 652, 257, 3036, 295, 364, 1988, 88, 2445, 420, 746, 11, 718, 311, 536, 13, 51784], "temperature": 0.0, "avg_logprob": -0.15895024785455666, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.016570067033171654}, {"id": 86, "seek": 38322, "start": 383.22, "end": 386.74, "text": " Maybe it will probably, maybe, I don't know, you never know what it's going to do.", "tokens": [50364, 2704, 309, 486, 1391, 11, 1310, 11, 286, 500, 380, 458, 11, 291, 1128, 458, 437, 309, 311, 516, 281, 360, 13, 50540], "temperature": 0.0, "avg_logprob": -0.18821834564208983, "compression_ratio": 1.8172043010752688, "no_speech_prob": 0.24630297720432281}, {"id": 87, "seek": 38322, "start": 386.74, "end": 391.70000000000005, "text": " It's some, okay, so it says using Wolfram, that's a good sign, maybe it's going to do", "tokens": [50540, 467, 311, 512, 11, 1392, 11, 370, 309, 1619, 1228, 16634, 2356, 11, 300, 311, 257, 665, 1465, 11, 1310, 309, 311, 516, 281, 360, 50788], "temperature": 0.0, "avg_logprob": -0.18821834564208983, "compression_ratio": 1.8172043010752688, "no_speech_prob": 0.24630297720432281}, {"id": 88, "seek": 38322, "start": 391.70000000000005, "end": 396.98, "text": " the right thing, maybe not, who's to know, okay.", "tokens": [50788, 264, 558, 551, 11, 1310, 406, 11, 567, 311, 281, 458, 11, 1392, 13, 51052], "temperature": 0.0, "avg_logprob": -0.18821834564208983, "compression_ratio": 1.8172043010752688, "no_speech_prob": 0.24630297720432281}, {"id": 89, "seek": 38322, "start": 396.98, "end": 399.66, "text": " So this is, I wonder how it did that, okay.", "tokens": [51052, 407, 341, 307, 11, 286, 2441, 577, 309, 630, 300, 11, 1392, 13, 51186], "temperature": 0.0, "avg_logprob": -0.18821834564208983, "compression_ratio": 1.8172043010752688, "no_speech_prob": 0.24630297720432281}, {"id": 90, "seek": 38322, "start": 399.66, "end": 401.90000000000003, "text": " So there it made a nice little plot of an airy function.", "tokens": [51186, 407, 456, 309, 1027, 257, 1481, 707, 7542, 295, 364, 1988, 88, 2445, 13, 51298], "temperature": 0.0, "avg_logprob": -0.18821834564208983, "compression_ratio": 1.8172043010752688, "no_speech_prob": 0.24630297720432281}, {"id": 91, "seek": 38322, "start": 401.90000000000003, "end": 405.34000000000003, "text": " Let's see how it did it, we're going to here, okay.", "tokens": [51298, 961, 311, 536, 577, 309, 630, 309, 11, 321, 434, 516, 281, 510, 11, 1392, 13, 51470], "temperature": 0.0, "avg_logprob": -0.18821834564208983, "compression_ratio": 1.8172043010752688, "no_speech_prob": 0.24630297720432281}, {"id": 92, "seek": 38322, "start": 405.34000000000003, "end": 408.62, "text": " So what it actually did there was it wrote a piece of Wolfram language code that just", "tokens": [51470, 407, 437, 309, 767, 630, 456, 390, 309, 4114, 257, 2522, 295, 16634, 2356, 2856, 3089, 300, 445, 51634], "temperature": 0.0, "avg_logprob": -0.18821834564208983, "compression_ratio": 1.8172043010752688, "no_speech_prob": 0.24630297720432281}, {"id": 93, "seek": 38322, "start": 408.62, "end": 411.74, "text": " says plot the airy function, very straightforward.", "tokens": [51634, 1619, 7542, 264, 1988, 88, 2445, 11, 588, 15325, 13, 51790], "temperature": 0.0, "avg_logprob": -0.18821834564208983, "compression_ratio": 1.8172043010752688, "no_speech_prob": 0.24630297720432281}, {"id": 94, "seek": 41174, "start": 411.74, "end": 420.86, "text": " Let's say we say something like, I don't know, how far is it from, I don't know, Chicago", "tokens": [50364, 961, 311, 584, 321, 584, 746, 411, 11, 286, 500, 380, 458, 11, 577, 1400, 307, 309, 490, 11, 286, 500, 380, 458, 11, 9525, 50820], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 95, "seek": 41174, "start": 420.86, "end": 421.94, "text": " to Tokyo.", "tokens": [50820, 281, 15147, 13, 50874], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 96, "seek": 41174, "start": 421.94, "end": 429.1, "text": " Wait a minute, what happens to that?", "tokens": [50874, 3802, 257, 3456, 11, 437, 2314, 281, 300, 30, 51232], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 97, "seek": 41174, "start": 429.1, "end": 430.7, "text": " It disappeared.", "tokens": [51232, 467, 13954, 13, 51312], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 98, "seek": 41174, "start": 430.7, "end": 431.7, "text": " What's going on?", "tokens": [51312, 708, 311, 516, 322, 30, 51362], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 99, "seek": 41174, "start": 431.7, "end": 432.7, "text": " Scrolling down.", "tokens": [51362, 2747, 18688, 760, 13, 51412], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 100, "seek": 41174, "start": 432.7, "end": 433.7, "text": " Scrolling down.", "tokens": [51412, 2747, 18688, 760, 13, 51462], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 101, "seek": 41174, "start": 433.7, "end": 434.7, "text": " Oh, okay.", "tokens": [51462, 876, 11, 1392, 13, 51512], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 102, "seek": 41174, "start": 434.7, "end": 435.7, "text": " Thanks.", "tokens": [51512, 2561, 13, 51562], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 103, "seek": 41174, "start": 435.7, "end": 439.26, "text": " I thought I was scrolling down, it didn't seem to be showing up, okay.", "tokens": [51562, 286, 1194, 286, 390, 29053, 760, 11, 309, 994, 380, 1643, 281, 312, 4099, 493, 11, 1392, 13, 51740], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 104, "seek": 41174, "start": 439.26, "end": 440.26, "text": " How lovely.", "tokens": [51740, 1012, 7496, 13, 51790], "temperature": 0.0, "avg_logprob": -0.3217412508451022, "compression_ratio": 1.5279187817258884, "no_speech_prob": 0.11650977283716202}, {"id": 105, "seek": 44026, "start": 440.46, "end": 445.82, "text": " So what it did there, again, was, in this case, let's see what it did here, okay.", "tokens": [50374, 407, 437, 309, 630, 456, 11, 797, 11, 390, 11, 294, 341, 1389, 11, 718, 311, 536, 437, 309, 630, 510, 11, 1392, 13, 50642], "temperature": 0.0, "avg_logprob": -0.1880437135696411, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.10172399133443832}, {"id": 106, "seek": 44026, "start": 445.82, "end": 451.53999999999996, "text": " So in this case, it used Wolfram Alpha and just asked distance from Chicago to Tokyo,", "tokens": [50642, 407, 294, 341, 1389, 11, 309, 1143, 16634, 2356, 20588, 293, 445, 2351, 4560, 490, 9525, 281, 15147, 11, 50928], "temperature": 0.0, "avg_logprob": -0.1880437135696411, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.10172399133443832}, {"id": 107, "seek": 44026, "start": 451.53999999999996, "end": 456.21999999999997, "text": " then it got back a bunch of results from Wolfram Alpha, which it then kind of interpreted", "tokens": [50928, 550, 309, 658, 646, 257, 3840, 295, 3542, 490, 16634, 2356, 20588, 11, 597, 309, 550, 733, 295, 26749, 51162], "temperature": 0.0, "avg_logprob": -0.1880437135696411, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.10172399133443832}, {"id": 108, "seek": 44026, "start": 456.21999999999997, "end": 459.09999999999997, "text": " and turned that into what it was saying.", "tokens": [51162, 293, 3574, 300, 666, 437, 309, 390, 1566, 13, 51306], "temperature": 0.0, "avg_logprob": -0.1880437135696411, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.10172399133443832}, {"id": 109, "seek": 44026, "start": 459.09999999999997, "end": 463.09999999999997, "text": " So it's sort of interesting what's happening here, because actually we have this plug-in,", "tokens": [51306, 407, 309, 311, 1333, 295, 1880, 437, 311, 2737, 510, 11, 570, 767, 321, 362, 341, 5452, 12, 259, 11, 51506], "temperature": 0.0, "avg_logprob": -0.1880437135696411, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.10172399133443832}, {"id": 110, "seek": 44026, "start": 463.09999999999997, "end": 468.9, "text": " it's used a lot every day by people, and I think, at least when I last asked, which was", "tokens": [51506, 309, 311, 1143, 257, 688, 633, 786, 538, 561, 11, 293, 286, 519, 11, 412, 1935, 562, 286, 1036, 2351, 11, 597, 390, 51796], "temperature": 0.0, "avg_logprob": -0.1880437135696411, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.10172399133443832}, {"id": 111, "seek": 46890, "start": 468.9, "end": 473.38, "text": " a few weeks ago, there's still the case that about half of the queries go to Wolfram Alpha", "tokens": [50364, 257, 1326, 3259, 2057, 11, 456, 311, 920, 264, 1389, 300, 466, 1922, 295, 264, 24109, 352, 281, 16634, 2356, 20588, 50588], "temperature": 0.0, "avg_logprob": -0.12368148251583702, "compression_ratio": 1.9519230769230769, "no_speech_prob": 0.00855331216007471}, {"id": 112, "seek": 46890, "start": 473.38, "end": 475.58, "text": " and half the queries go to Wolfram Language.", "tokens": [50588, 293, 1922, 264, 24109, 352, 281, 16634, 2356, 24445, 13, 50698], "temperature": 0.0, "avg_logprob": -0.12368148251583702, "compression_ratio": 1.9519230769230769, "no_speech_prob": 0.00855331216007471}, {"id": 113, "seek": 46890, "start": 475.58, "end": 479.58, "text": " And if you read the prompt, you know, the prompt engineering is this bizarre activity", "tokens": [50698, 400, 498, 291, 1401, 264, 12391, 11, 291, 458, 11, 264, 12391, 7043, 307, 341, 18265, 5191, 50898], "temperature": 0.0, "avg_logprob": -0.12368148251583702, "compression_ratio": 1.9519230769230769, "no_speech_prob": 0.00855331216007471}, {"id": 114, "seek": 46890, "start": 479.58, "end": 484.38, "text": " where you, whether you say please or not matters, whether things are in capital letters matters,", "tokens": [50898, 689, 291, 11, 1968, 291, 584, 1767, 420, 406, 7001, 11, 1968, 721, 366, 294, 4238, 7825, 7001, 11, 51138], "temperature": 0.0, "avg_logprob": -0.12368148251583702, "compression_ratio": 1.9519230769230769, "no_speech_prob": 0.00855331216007471}, {"id": 115, "seek": 46890, "start": 484.38, "end": 488.29999999999995, "text": " whether you repeat things at the end of the prompt after you mentioned them at the beginning,", "tokens": [51138, 1968, 291, 7149, 721, 412, 264, 917, 295, 264, 12391, 934, 291, 2835, 552, 412, 264, 2863, 11, 51334], "temperature": 0.0, "avg_logprob": -0.12368148251583702, "compression_ratio": 1.9519230769230769, "no_speech_prob": 0.00855331216007471}, {"id": 116, "seek": 46890, "start": 488.29999999999995, "end": 489.29999999999995, "text": " that all matters.", "tokens": [51334, 300, 439, 7001, 13, 51384], "temperature": 0.0, "avg_logprob": -0.12368148251583702, "compression_ratio": 1.9519230769230769, "no_speech_prob": 0.00855331216007471}, {"id": 117, "seek": 46890, "start": 489.29999999999995, "end": 492.65999999999997, "text": " I will say, by the way, that if you ask, you know, what's the skill you need to do prompt", "tokens": [51384, 286, 486, 584, 11, 538, 264, 636, 11, 300, 498, 291, 1029, 11, 291, 458, 11, 437, 311, 264, 5389, 291, 643, 281, 360, 12391, 51552], "temperature": 0.0, "avg_logprob": -0.12368148251583702, "compression_ratio": 1.9519230769230769, "no_speech_prob": 0.00855331216007471}, {"id": 118, "seek": 46890, "start": 492.65999999999997, "end": 497.5, "text": " engineering, so far as I can tell, expository writing is the number one skill needed for", "tokens": [51552, 7043, 11, 370, 1400, 382, 286, 393, 980, 11, 1278, 9598, 827, 3579, 307, 264, 1230, 472, 5389, 2978, 337, 51794], "temperature": 0.0, "avg_logprob": -0.12368148251583702, "compression_ratio": 1.9519230769230769, "no_speech_prob": 0.00855331216007471}, {"id": 119, "seek": 49750, "start": 497.5, "end": 499.7, "text": " good prompt engineering.", "tokens": [50364, 665, 12391, 7043, 13, 50474], "temperature": 0.0, "avg_logprob": -0.16234081626957297, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.02173142321407795}, {"id": 120, "seek": 49750, "start": 499.7, "end": 505.1, "text": " Maybe one day, and we'll talk about this later, when we talk about applying physics to LLMs,", "tokens": [50474, 2704, 472, 786, 11, 293, 321, 603, 751, 466, 341, 1780, 11, 562, 321, 751, 466, 9275, 10649, 281, 441, 43, 26386, 11, 50744], "temperature": 0.0, "avg_logprob": -0.16234081626957297, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.02173142321407795}, {"id": 121, "seek": 49750, "start": 505.1, "end": 510.7, "text": " maybe there will be actual kind of AI psychology theory that can be used, but as of right now,", "tokens": [50744, 1310, 456, 486, 312, 3539, 733, 295, 7318, 15105, 5261, 300, 393, 312, 1143, 11, 457, 382, 295, 558, 586, 11, 51024], "temperature": 0.0, "avg_logprob": -0.16234081626957297, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.02173142321407795}, {"id": 122, "seek": 49750, "start": 510.7, "end": 515.58, "text": " I think it's expository writing, which kind of maps on to the kind of thing that the LLM", "tokens": [51024, 286, 519, 309, 311, 1278, 9598, 827, 3579, 11, 597, 733, 295, 11317, 322, 281, 264, 733, 295, 551, 300, 264, 441, 43, 44, 51268], "temperature": 0.0, "avg_logprob": -0.16234081626957297, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.02173142321407795}, {"id": 123, "seek": 49750, "start": 515.58, "end": 518.1, "text": " has read from the web and so on.", "tokens": [51268, 575, 1401, 490, 264, 3670, 293, 370, 322, 13, 51394], "temperature": 0.0, "avg_logprob": -0.16234081626957297, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.02173142321407795}, {"id": 124, "seek": 49750, "start": 518.1, "end": 525.18, "text": " But in any case, the prompt here is saying, you know, if you have this kind of thing,", "tokens": [51394, 583, 294, 604, 1389, 11, 264, 12391, 510, 307, 1566, 11, 291, 458, 11, 498, 291, 362, 341, 733, 295, 551, 11, 51748], "temperature": 0.0, "avg_logprob": -0.16234081626957297, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.02173142321407795}, {"id": 125, "seek": 52518, "start": 525.3399999999999, "end": 531.8599999999999, "text": " try and send it to WolfMalpha, one of the things that's really convenient about WolfMalpha", "tokens": [50372, 853, 293, 2845, 309, 281, 16634, 44, 304, 7211, 11, 472, 295, 264, 721, 300, 311, 534, 10851, 466, 16634, 44, 304, 7211, 50698], "temperature": 0.0, "avg_logprob": -0.18881946139865452, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.3858147859573364}, {"id": 126, "seek": 52518, "start": 531.8599999999999, "end": 536.9799999999999, "text": " is that it is a thing that takes natural language's input, which is the same stuff that the LLM", "tokens": [50698, 307, 300, 309, 307, 257, 551, 300, 2516, 3303, 2856, 311, 4846, 11, 597, 307, 264, 912, 1507, 300, 264, 441, 43, 44, 50954], "temperature": 0.0, "avg_logprob": -0.18881946139865452, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.3858147859573364}, {"id": 127, "seek": 52518, "start": 536.9799999999999, "end": 539.02, "text": " is used to dealing with.", "tokens": [50954, 307, 1143, 281, 6260, 365, 13, 51056], "temperature": 0.0, "avg_logprob": -0.18881946139865452, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.3858147859573364}, {"id": 128, "seek": 52518, "start": 539.02, "end": 543.42, "text": " So it's kind of, it's using natural language as a transport layer, and what does WolfMalpha", "tokens": [51056, 407, 309, 311, 733, 295, 11, 309, 311, 1228, 3303, 2856, 382, 257, 5495, 4583, 11, 293, 437, 775, 16634, 44, 304, 7211, 51276], "temperature": 0.0, "avg_logprob": -0.18881946139865452, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.3858147859573364}, {"id": 129, "seek": 52518, "start": 543.42, "end": 544.42, "text": " do?", "tokens": [51276, 360, 30, 51326], "temperature": 0.0, "avg_logprob": -0.18881946139865452, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.3858147859573364}, {"id": 130, "seek": 52518, "start": 544.42, "end": 549.38, "text": " Well, what it's doing is to take whatever you type in, you know, if you type, I don't know,", "tokens": [51326, 1042, 11, 437, 309, 311, 884, 307, 281, 747, 2035, 291, 2010, 294, 11, 291, 458, 11, 498, 291, 2010, 11, 286, 500, 380, 458, 11, 51574], "temperature": 0.0, "avg_logprob": -0.18881946139865452, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.3858147859573364}, {"id": 131, "seek": 54938, "start": 549.38, "end": 557.86, "text": " what is the integral of, I don't know, some random thing.", "tokens": [50364, 437, 307, 264, 11573, 295, 11, 286, 500, 380, 458, 11, 512, 4974, 551, 13, 50788], "temperature": 0.0, "avg_logprob": -0.16645644392286027, "compression_ratio": 1.5753424657534247, "no_speech_prob": 0.019914718344807625}, {"id": 132, "seek": 54938, "start": 557.86, "end": 564.5, "text": " What it's doing there is it's converting that question written in natural language into", "tokens": [50788, 708, 309, 311, 884, 456, 307, 309, 311, 29942, 300, 1168, 3720, 294, 3303, 2856, 666, 51120], "temperature": 0.0, "avg_logprob": -0.16645644392286027, "compression_ratio": 1.5753424657534247, "no_speech_prob": 0.019914718344807625}, {"id": 133, "seek": 54938, "start": 564.5, "end": 570.78, "text": " precise computational language internally, or if I say something like, I don't know,", "tokens": [51120, 13600, 28270, 2856, 19501, 11, 420, 498, 286, 584, 746, 411, 11, 286, 500, 380, 458, 11, 51434], "temperature": 0.0, "avg_logprob": -0.16645644392286027, "compression_ratio": 1.5753424657534247, "no_speech_prob": 0.019914718344807625}, {"id": 134, "seek": 57078, "start": 570.78, "end": 580.42, "text": " what earthquakes happened in Japan in August 1990 or something, I wonder if it can do that,", "tokens": [50364, 437, 34048, 2011, 294, 3367, 294, 6897, 13384, 420, 746, 11, 286, 2441, 498, 309, 393, 360, 300, 11, 50846], "temperature": 0.0, "avg_logprob": -0.2340517855705099, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.08197227865457535}, {"id": 135, "seek": 57078, "start": 580.42, "end": 584.9399999999999, "text": " I have no idea if it can do that, but that's always living dangerously, okay, we've managed", "tokens": [50846, 286, 362, 572, 1558, 498, 309, 393, 360, 300, 11, 457, 300, 311, 1009, 2647, 4330, 5098, 11, 1392, 11, 321, 600, 6453, 51072], "temperature": 0.0, "avg_logprob": -0.2340517855705099, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.08197227865457535}, {"id": 136, "seek": 57078, "start": 584.9399999999999, "end": 585.9399999999999, "text": " to do that.", "tokens": [51072, 281, 360, 300, 13, 51122], "temperature": 0.0, "avg_logprob": -0.2340517855705099, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.08197227865457535}, {"id": 137, "seek": 57078, "start": 585.9399999999999, "end": 594.38, "text": " Once again, what happened here was it converted that natural language question into its underlying", "tokens": [51122, 3443, 797, 11, 437, 2011, 510, 390, 309, 16424, 300, 3303, 2856, 1168, 666, 1080, 14217, 51544], "temperature": 0.0, "avg_logprob": -0.2340517855705099, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.08197227865457535}, {"id": 138, "seek": 57078, "start": 594.38, "end": 598.66, "text": " computational language, which is our WolfMalph language system, to be able to resolve it.", "tokens": [51544, 28270, 2856, 11, 597, 307, 527, 16634, 44, 304, 950, 2856, 1185, 11, 281, 312, 1075, 281, 14151, 309, 13, 51758], "temperature": 0.0, "avg_logprob": -0.2340517855705099, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.08197227865457535}, {"id": 139, "seek": 59866, "start": 598.66, "end": 602.78, "text": " I mean, just to show you how that works, you know, if I were to say here something like,", "tokens": [50364, 286, 914, 11, 445, 281, 855, 291, 577, 300, 1985, 11, 291, 458, 11, 498, 286, 645, 281, 584, 510, 746, 411, 11, 50570], "temperature": 0.0, "avg_logprob": -0.19529472765072373, "compression_ratio": 1.8325991189427313, "no_speech_prob": 0.5045446157455444}, {"id": 140, "seek": 59866, "start": 602.78, "end": 609.4599999999999, "text": " if I just said, you know, New York here, that this is a WolfMalph notebook, this thing New", "tokens": [50570, 498, 286, 445, 848, 11, 291, 458, 11, 1873, 3609, 510, 11, 300, 341, 307, 257, 16634, 44, 304, 950, 21060, 11, 341, 551, 1873, 50904], "temperature": 0.0, "avg_logprob": -0.19529472765072373, "compression_ratio": 1.8325991189427313, "no_speech_prob": 0.5045446157455444}, {"id": 141, "seek": 59866, "start": 609.4599999999999, "end": 614.26, "text": " York, if I say what's the input form of that, it's the entity, city, New York, New York,", "tokens": [50904, 3609, 11, 498, 286, 584, 437, 311, 264, 4846, 1254, 295, 300, 11, 309, 311, 264, 13977, 11, 2307, 11, 1873, 3609, 11, 1873, 3609, 11, 51144], "temperature": 0.0, "avg_logprob": -0.19529472765072373, "compression_ratio": 1.8325991189427313, "no_speech_prob": 0.5045446157455444}, {"id": 142, "seek": 59866, "start": 614.26, "end": 617.3, "text": " United States, but it's also a thing that I can compute with.", "tokens": [51144, 2824, 3040, 11, 457, 309, 311, 611, 257, 551, 300, 286, 393, 14722, 365, 13, 51296], "temperature": 0.0, "avg_logprob": -0.19529472765072373, "compression_ratio": 1.8325991189427313, "no_speech_prob": 0.5045446157455444}, {"id": 143, "seek": 59866, "start": 617.3, "end": 625.62, "text": " So if I say make a, I don't know, if I say, you know, geodistance from New York to, I", "tokens": [51296, 407, 498, 286, 584, 652, 257, 11, 286, 500, 380, 458, 11, 498, 286, 584, 11, 291, 458, 11, 1519, 378, 20829, 490, 1873, 3609, 281, 11, 286, 51712], "temperature": 0.0, "avg_logprob": -0.19529472765072373, "compression_ratio": 1.8325991189427313, "no_speech_prob": 0.5045446157455444}, {"id": 144, "seek": 62562, "start": 626.58, "end": 632.26, "text": " don't know, London or something, it'll then just use those things as entities that it", "tokens": [50412, 500, 380, 458, 11, 7042, 420, 746, 11, 309, 603, 550, 445, 764, 729, 721, 382, 16667, 300, 309, 50696], "temperature": 0.0, "avg_logprob": -0.13084807762732872, "compression_ratio": 1.7165354330708662, "no_speech_prob": 0.009583702310919762}, {"id": 145, "seek": 62562, "start": 632.26, "end": 633.66, "text": " can compute with.", "tokens": [50696, 393, 14722, 365, 13, 50766], "temperature": 0.0, "avg_logprob": -0.13084807762732872, "compression_ratio": 1.7165354330708662, "no_speech_prob": 0.009583702310919762}, {"id": 146, "seek": 62562, "start": 633.66, "end": 640.18, "text": " So as I say, the sort of the mission of WolfMalph is convert natural language into this precise", "tokens": [50766, 407, 382, 286, 584, 11, 264, 1333, 295, 264, 4447, 295, 16634, 44, 304, 950, 307, 7620, 3303, 2856, 666, 341, 13600, 51092], "temperature": 0.0, "avg_logprob": -0.13084807762732872, "compression_ratio": 1.7165354330708662, "no_speech_prob": 0.009583702310919762}, {"id": 147, "seek": 62562, "start": 640.18, "end": 645.86, "text": " computational language from which we can do computations based on algorithms that we've", "tokens": [51092, 28270, 2856, 490, 597, 321, 393, 360, 2807, 763, 2361, 322, 14642, 300, 321, 600, 51376], "temperature": 0.0, "avg_logprob": -0.13084807762732872, "compression_ratio": 1.7165354330708662, "no_speech_prob": 0.009583702310919762}, {"id": 148, "seek": 62562, "start": 645.86, "end": 651.7, "text": " spent the last three and a half decades, you know, setting up and based on curated knowledge", "tokens": [51376, 4418, 264, 1036, 1045, 293, 257, 1922, 7878, 11, 291, 458, 11, 3287, 493, 293, 2361, 322, 47851, 3601, 51668], "temperature": 0.0, "avg_logprob": -0.13084807762732872, "compression_ratio": 1.7165354330708662, "no_speech_prob": 0.009583702310919762}, {"id": 149, "seek": 62562, "start": 651.7, "end": 655.42, "text": " that we've accumulated over the last couple of decades.", "tokens": [51668, 300, 321, 600, 31346, 670, 264, 1036, 1916, 295, 7878, 13, 51854], "temperature": 0.0, "avg_logprob": -0.13084807762732872, "compression_ratio": 1.7165354330708662, "no_speech_prob": 0.009583702310919762}, {"id": 150, "seek": 65542, "start": 655.42, "end": 662.14, "text": " So, you know, you can obviously mix things like, you can say things like, I don't know,", "tokens": [50364, 407, 11, 291, 458, 11, 291, 393, 2745, 2890, 721, 411, 11, 291, 393, 584, 721, 411, 11, 286, 500, 380, 458, 11, 50700], "temperature": 0.0, "avg_logprob": -0.17936426050522747, "compression_ratio": 1.886178861788618, "no_speech_prob": 0.0010832486441358924}, {"id": 151, "seek": 65542, "start": 662.14, "end": 669.3399999999999, "text": " capital cities in Europe or something, and you'll get something which again, that thing", "tokens": [50700, 4238, 6486, 294, 3315, 420, 746, 11, 293, 291, 603, 483, 746, 597, 797, 11, 300, 551, 51060], "temperature": 0.0, "avg_logprob": -0.17936426050522747, "compression_ratio": 1.886178861788618, "no_speech_prob": 0.0010832486441358924}, {"id": 152, "seek": 65542, "start": 669.3399999999999, "end": 674.78, "text": " got converted into precise computational language, we can evaluate it, we can say something", "tokens": [51060, 658, 16424, 666, 13600, 28270, 2856, 11, 321, 393, 13059, 309, 11, 321, 393, 584, 746, 51332], "temperature": 0.0, "avg_logprob": -0.17936426050522747, "compression_ratio": 1.886178861788618, "no_speech_prob": 0.0010832486441358924}, {"id": 153, "seek": 65542, "start": 674.78, "end": 679.74, "text": " like, you know, I don't know, we could say make a plot of those, all those standard kinds", "tokens": [51332, 411, 11, 291, 458, 11, 286, 500, 380, 458, 11, 321, 727, 584, 652, 257, 7542, 295, 729, 11, 439, 729, 3832, 3685, 51580], "temperature": 0.0, "avg_logprob": -0.17936426050522747, "compression_ratio": 1.886178861788618, "no_speech_prob": 0.0010832486441358924}, {"id": 154, "seek": 65542, "start": 679.74, "end": 683.5799999999999, "text": " of things that you can do in WolfMalph language, or we can find shortest tours, all those sorts", "tokens": [51580, 295, 721, 300, 291, 393, 360, 294, 16634, 44, 304, 950, 2856, 11, 420, 321, 393, 915, 31875, 22911, 11, 439, 729, 7527, 51772], "temperature": 0.0, "avg_logprob": -0.17936426050522747, "compression_ratio": 1.886178861788618, "no_speech_prob": 0.0010832486441358924}, {"id": 155, "seek": 65542, "start": 683.5799999999999, "end": 684.5799999999999, "text": " of things.", "tokens": [51772, 295, 721, 13, 51822], "temperature": 0.0, "avg_logprob": -0.17936426050522747, "compression_ratio": 1.886178861788618, "no_speech_prob": 0.0010832486441358924}, {"id": 156, "seek": 68458, "start": 684.7800000000001, "end": 692.46, "text": " So, from within chat GBT, you can now access all of that functionality, so I don't know,", "tokens": [50374, 407, 11, 490, 1951, 5081, 26809, 51, 11, 291, 393, 586, 2105, 439, 295, 300, 14980, 11, 370, 286, 500, 380, 458, 11, 50758], "temperature": 0.0, "avg_logprob": -0.2838885350660844, "compression_ratio": 1.5120772946859904, "no_speech_prob": 0.005797162652015686}, {"id": 157, "seek": 68458, "start": 692.46, "end": 697.14, "text": " we could, let's try, let's try doing something ambitious, which probably won't work.", "tokens": [50758, 321, 727, 11, 718, 311, 853, 11, 718, 311, 853, 884, 746, 20239, 11, 597, 1391, 1582, 380, 589, 13, 50992], "temperature": 0.0, "avg_logprob": -0.2838885350660844, "compression_ratio": 1.5120772946859904, "no_speech_prob": 0.005797162652015686}, {"id": 158, "seek": 68458, "start": 697.14, "end": 702.58, "text": " Find a shortest tour of the capital cities of Europe.", "tokens": [50992, 11809, 257, 31875, 3512, 295, 264, 4238, 6486, 295, 3315, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2838885350660844, "compression_ratio": 1.5120772946859904, "no_speech_prob": 0.005797162652015686}, {"id": 159, "seek": 68458, "start": 702.58, "end": 712.58, "text": " Okay, let's watch this fail, grind, grind, grind, I have no idea, I hate to even open", "tokens": [51264, 1033, 11, 718, 311, 1159, 341, 3061, 11, 16700, 11, 16700, 11, 16700, 11, 286, 362, 572, 1558, 11, 286, 4700, 281, 754, 1269, 51764], "temperature": 0.0, "avg_logprob": -0.2838885350660844, "compression_ratio": 1.5120772946859904, "no_speech_prob": 0.005797162652015686}, {"id": 160, "seek": 71258, "start": 712.58, "end": 716.58, "text": " this to find out what horrifying thing it's actually doing in there.", "tokens": [50364, 341, 281, 915, 484, 437, 40227, 551, 309, 311, 767, 884, 294, 456, 13, 50564], "temperature": 0.0, "avg_logprob": -0.23582577338585486, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.03545140102505684}, {"id": 161, "seek": 71258, "start": 716.58, "end": 723.0600000000001, "text": " Maybe, maybe, let's see, okay, it's trying something again, either because it didn't", "tokens": [50564, 2704, 11, 1310, 11, 718, 311, 536, 11, 1392, 11, 309, 311, 1382, 746, 797, 11, 2139, 570, 309, 994, 380, 50888], "temperature": 0.0, "avg_logprob": -0.23582577338585486, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.03545140102505684}, {"id": 162, "seek": 71258, "start": 723.0600000000001, "end": 727.46, "text": " get the answer it wanted, or for some other reason, oh, this is a bad sign, this is not", "tokens": [50888, 483, 264, 1867, 309, 1415, 11, 420, 337, 512, 661, 1778, 11, 1954, 11, 341, 307, 257, 1578, 1465, 11, 341, 307, 406, 51108], "temperature": 0.0, "avg_logprob": -0.23582577338585486, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.03545140102505684}, {"id": 163, "seek": 71258, "start": 727.46, "end": 733.46, "text": " good, this looks like it's, no.", "tokens": [51108, 665, 11, 341, 1542, 411, 309, 311, 11, 572, 13, 51408], "temperature": 0.0, "avg_logprob": -0.23582577338585486, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.03545140102505684}, {"id": 164, "seek": 71258, "start": 733.46, "end": 736.7, "text": " But what it's going to do, what it's doing is, every time, by the way, I mean, the way", "tokens": [51408, 583, 437, 309, 311, 516, 281, 360, 11, 437, 309, 311, 884, 307, 11, 633, 565, 11, 538, 264, 636, 11, 286, 914, 11, 264, 636, 51570], "temperature": 0.0, "avg_logprob": -0.23582577338585486, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.03545140102505684}, {"id": 165, "seek": 71258, "start": 736.7, "end": 740.1800000000001, "text": " LLAM's work, we'll maybe talk about this a bit more later, they're always writing one", "tokens": [51570, 441, 43, 2865, 311, 589, 11, 321, 603, 1310, 751, 466, 341, 257, 857, 544, 1780, 11, 436, 434, 1009, 3579, 472, 51744], "temperature": 0.0, "avg_logprob": -0.23582577338585486, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.03545140102505684}, {"id": 166, "seek": 74018, "start": 740.2199999999999, "end": 744.9799999999999, "text": " token at a time, so they never have a plan for where they're going to go, they're always", "tokens": [50366, 14862, 412, 257, 565, 11, 370, 436, 1128, 362, 257, 1393, 337, 689, 436, 434, 516, 281, 352, 11, 436, 434, 1009, 50604], "temperature": 0.0, "avg_logprob": -0.14443585512449417, "compression_ratio": 1.8, "no_speech_prob": 0.03775671124458313}, {"id": 167, "seek": 74018, "start": 744.9799999999999, "end": 749.26, "text": " just looking at what was in the past and figuring out what to say next.", "tokens": [50604, 445, 1237, 412, 437, 390, 294, 264, 1791, 293, 15213, 484, 437, 281, 584, 958, 13, 50818], "temperature": 0.0, "avg_logprob": -0.14443585512449417, "compression_ratio": 1.8, "no_speech_prob": 0.03775671124458313}, {"id": 168, "seek": 74018, "start": 749.26, "end": 752.8199999999999, "text": " So that means that it's quite often, it's kind of a hack you can use, if you get it", "tokens": [50818, 407, 300, 1355, 300, 309, 311, 1596, 2049, 11, 309, 311, 733, 295, 257, 10339, 291, 393, 764, 11, 498, 291, 483, 309, 50996], "temperature": 0.0, "avg_logprob": -0.14443585512449417, "compression_ratio": 1.8, "no_speech_prob": 0.03775671124458313}, {"id": 169, "seek": 74018, "start": 752.8199999999999, "end": 755.9, "text": " to generate an output and you say, is that correct?", "tokens": [50996, 281, 8460, 364, 5598, 293, 291, 584, 11, 307, 300, 3006, 30, 51150], "temperature": 0.0, "avg_logprob": -0.14443585512449417, "compression_ratio": 1.8, "no_speech_prob": 0.03775671124458313}, {"id": 170, "seek": 74018, "start": 755.9, "end": 760.7399999999999, "text": " And it will say, no, it's not correct, and how, why did you say it, well, because it", "tokens": [51150, 400, 309, 486, 584, 11, 572, 11, 309, 311, 406, 3006, 11, 293, 577, 11, 983, 630, 291, 584, 309, 11, 731, 11, 570, 309, 51392], "temperature": 0.0, "avg_logprob": -0.14443585512449417, "compression_ratio": 1.8, "no_speech_prob": 0.03775671124458313}, {"id": 171, "seek": 74018, "start": 760.7399999999999, "end": 769.74, "text": " didn't know what it was going to say, it just, well, let's see, let us see, okay, wow.", "tokens": [51392, 994, 380, 458, 437, 309, 390, 516, 281, 584, 11, 309, 445, 11, 731, 11, 718, 311, 536, 11, 718, 505, 536, 11, 1392, 11, 6076, 13, 51842], "temperature": 0.0, "avg_logprob": -0.14443585512449417, "compression_ratio": 1.8, "no_speech_prob": 0.03775671124458313}, {"id": 172, "seek": 76974, "start": 770.3, "end": 779.3, "text": " Wow, wow, okay, let's see what happens, now, I have no idea if this is, okay, wait a minute,", "tokens": [50392, 3153, 11, 6076, 11, 1392, 11, 718, 311, 536, 437, 2314, 11, 586, 11, 286, 362, 572, 1558, 498, 341, 307, 11, 1392, 11, 1699, 257, 3456, 11, 50842], "temperature": 0.0, "avg_logprob": -0.24811243723673992, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00103739183396101}, {"id": 173, "seek": 76974, "start": 779.3, "end": 786.62, "text": " how many, how many, wait a second, okay, let's see, well, let's see, let's see what happens", "tokens": [50842, 577, 867, 11, 577, 867, 11, 1699, 257, 1150, 11, 1392, 11, 718, 311, 536, 11, 731, 11, 718, 311, 536, 11, 718, 311, 536, 437, 2314, 51208], "temperature": 0.0, "avg_logprob": -0.24811243723673992, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00103739183396101}, {"id": 174, "seek": 76974, "start": 786.62, "end": 794.86, "text": " if we say plot that, and then I'm going to find out what it actually did there.", "tokens": [51208, 498, 321, 584, 7542, 300, 11, 293, 550, 286, 478, 516, 281, 915, 484, 437, 309, 767, 630, 456, 13, 51620], "temperature": 0.0, "avg_logprob": -0.24811243723673992, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00103739183396101}, {"id": 175, "seek": 79486, "start": 794.94, "end": 798.94, "text": " This won't work, of course.", "tokens": [50368, 639, 1582, 380, 589, 11, 295, 1164, 13, 50568], "temperature": 0.0, "avg_logprob": -0.17285490473476026, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.02598614990711212}, {"id": 176, "seek": 79486, "start": 798.94, "end": 806.94, "text": " Well, okay, this is slightly promising, I wonder if this is going to work, it's a little", "tokens": [50568, 1042, 11, 1392, 11, 341, 307, 4748, 20257, 11, 286, 2441, 498, 341, 307, 516, 281, 589, 11, 309, 311, 257, 707, 50968], "temperature": 0.0, "avg_logprob": -0.17285490473476026, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.02598614990711212}, {"id": 177, "seek": 79486, "start": 806.94, "end": 811.86, "text": " bit confused there, but we'll see if it can recover itself, I don't know, let's look at", "tokens": [50968, 857, 9019, 456, 11, 457, 321, 603, 536, 498, 309, 393, 8114, 2564, 11, 286, 500, 380, 458, 11, 718, 311, 574, 412, 51214], "temperature": 0.0, "avg_logprob": -0.17285490473476026, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.02598614990711212}, {"id": 178, "seek": 79486, "start": 811.86, "end": 816.26, "text": " what it, let's look, to get some idea of whether this is actually right, let's look at what", "tokens": [51214, 437, 309, 11, 718, 311, 574, 11, 281, 483, 512, 1558, 295, 1968, 341, 307, 767, 558, 11, 718, 311, 574, 412, 437, 51434], "temperature": 0.0, "avg_logprob": -0.17285490473476026, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.02598614990711212}, {"id": 179, "seek": 79486, "start": 816.26, "end": 823.9, "text": " it actually asked here, okay, so it asked, okay, it did something sensible here, so what", "tokens": [51434, 309, 767, 2351, 510, 11, 1392, 11, 370, 309, 2351, 11, 1392, 11, 309, 630, 746, 25380, 510, 11, 370, 437, 51816], "temperature": 0.0, "avg_logprob": -0.17285490473476026, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.02598614990711212}, {"id": 180, "seek": 82390, "start": 823.9399999999999, "end": 828.18, "text": " it was doing, it's a little bit confusing what it did here, and we'll see how this works", "tokens": [50366, 309, 390, 884, 11, 309, 311, 257, 707, 857, 13181, 437, 309, 630, 510, 11, 293, 321, 603, 536, 577, 341, 1985, 50578], "temperature": 0.0, "avg_logprob": -0.12215225836809944, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.01940499246120453}, {"id": 181, "seek": 82390, "start": 828.18, "end": 833.6999999999999, "text": " a bit better in a moment, but what it did here was it was actually using results that", "tokens": [50578, 257, 857, 1101, 294, 257, 1623, 11, 457, 437, 309, 630, 510, 390, 309, 390, 767, 1228, 3542, 300, 50854], "temperature": 0.0, "avg_logprob": -0.12215225836809944, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.01940499246120453}, {"id": 182, "seek": 82390, "start": 833.6999999999999, "end": 838.6999999999999, "text": " it had got earlier in this whole sequence, because it actually knew the order of the cities", "tokens": [50854, 309, 632, 658, 3071, 294, 341, 1379, 8310, 11, 570, 309, 767, 2586, 264, 1668, 295, 264, 6486, 51104], "temperature": 0.0, "avg_logprob": -0.12215225836809944, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.01940499246120453}, {"id": 183, "seek": 82390, "start": 838.6999999999999, "end": 842.86, "text": " by now, because it must have got that in one of these previous queries here, yeah, here", "tokens": [51104, 538, 586, 11, 570, 309, 1633, 362, 658, 300, 294, 472, 295, 613, 3894, 24109, 510, 11, 1338, 11, 510, 51312], "temperature": 0.0, "avg_logprob": -0.12215225836809944, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.01940499246120453}, {"id": 184, "seek": 82390, "start": 842.86, "end": 848.46, "text": " we go, so it knew here, find shortest tour of those capitals, it found the shortest tour,", "tokens": [51312, 321, 352, 11, 370, 309, 2586, 510, 11, 915, 31875, 3512, 295, 729, 1410, 11118, 11, 309, 1352, 264, 31875, 3512, 11, 51592], "temperature": 0.0, "avg_logprob": -0.12215225836809944, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.01940499246120453}, {"id": 185, "seek": 82390, "start": 848.46, "end": 852.06, "text": " it then used that in the next step to go and try and find something, let's see what it", "tokens": [51592, 309, 550, 1143, 300, 294, 264, 958, 1823, 281, 352, 293, 853, 293, 915, 746, 11, 718, 311, 536, 437, 309, 51772], "temperature": 0.0, "avg_logprob": -0.12215225836809944, "compression_ratio": 1.923913043478261, "no_speech_prob": 0.01940499246120453}, {"id": 186, "seek": 85206, "start": 852.14, "end": 856.9399999999999, "text": " was doing down here, let's see what it got me where, no, it's still grinding away, oh", "tokens": [50368, 390, 884, 760, 510, 11, 718, 311, 536, 437, 309, 658, 385, 689, 11, 572, 11, 309, 311, 920, 25300, 1314, 11, 1954, 50608], "temperature": 0.0, "avg_logprob": -0.19343651290488453, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.008343759924173355}, {"id": 187, "seek": 85206, "start": 856.9399999999999, "end": 864.4599999999999, "text": " well, alright, so, but this gives some sense perhaps of how you kind of connect sort of", "tokens": [50608, 731, 11, 5845, 11, 370, 11, 457, 341, 2709, 512, 2020, 4317, 295, 577, 291, 733, 295, 1745, 1333, 295, 50984], "temperature": 0.0, "avg_logprob": -0.19343651290488453, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.008343759924173355}, {"id": 188, "seek": 85206, "start": 864.4599999999999, "end": 870.38, "text": " the LLM layer to the computational layer, but we built something recently that I think", "tokens": [50984, 264, 441, 43, 44, 4583, 281, 264, 28270, 4583, 11, 457, 321, 3094, 746, 3938, 300, 286, 519, 51280], "temperature": 0.0, "avg_logprob": -0.19343651290488453, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.008343759924173355}, {"id": 189, "seek": 85206, "start": 870.38, "end": 875.26, "text": " you might like to see, which is what we call, well let's see, there's different versions", "tokens": [51280, 291, 1062, 411, 281, 536, 11, 597, 307, 437, 321, 818, 11, 731, 718, 311, 536, 11, 456, 311, 819, 9606, 51524], "temperature": 0.0, "avg_logprob": -0.19343651290488453, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.008343759924173355}, {"id": 190, "seek": 85206, "start": 875.26, "end": 880.66, "text": " of this, what we call a chat enabled notebook, so this is using our notebook paradigm and", "tokens": [51524, 295, 341, 11, 437, 321, 818, 257, 5081, 15172, 21060, 11, 370, 341, 307, 1228, 527, 21060, 24709, 293, 51794], "temperature": 0.0, "avg_logprob": -0.19343651290488453, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.008343759924173355}, {"id": 191, "seek": 88066, "start": 881.14, "end": 887.06, "text": " let me see, let me make this a little bigger, and let me just get ready to save this,", "tokens": [50388, 718, 385, 536, 11, 718, 385, 652, 341, 257, 707, 3801, 11, 293, 718, 385, 445, 483, 1919, 281, 3155, 341, 11, 50684], "temperature": 0.0, "avg_logprob": -0.17567845608325713, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.002689773216843605}, {"id": 192, "seek": 88066, "start": 889.38, "end": 896.9, "text": " the, okay, let me tell it to use GPT-4 here, alright, so let's say we say here something like,", "tokens": [50800, 264, 11, 1392, 11, 718, 385, 980, 309, 281, 764, 26039, 51, 12, 19, 510, 11, 5845, 11, 370, 718, 311, 584, 321, 584, 510, 746, 411, 11, 51176], "temperature": 0.0, "avg_logprob": -0.17567845608325713, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.002689773216843605}, {"id": 193, "seek": 88066, "start": 898.5799999999999, "end": 901.62, "text": " again, I'm going to look very dangerously, because this never does the same thing twice,", "tokens": [51260, 797, 11, 286, 478, 516, 281, 574, 588, 4330, 5098, 11, 570, 341, 1128, 775, 264, 912, 551, 6091, 11, 51412], "temperature": 0.0, "avg_logprob": -0.17567845608325713, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.002689773216843605}, {"id": 194, "seek": 88066, "start": 902.5799999999999, "end": 907.4599999999999, "text": " let's say solve a harmonic oscillator, and harmonic oscillator, whatever,", "tokens": [51460, 718, 311, 584, 5039, 257, 32270, 43859, 11, 293, 32270, 43859, 11, 2035, 11, 51704], "temperature": 0.0, "avg_logprob": -0.17567845608325713, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.002689773216843605}, {"id": 195, "seek": 90746, "start": 908.02, "end": 922.5, "text": " uh, let's see what happens, oops, so, okay, okay, I mean it'd be not the right thing for it to do,", "tokens": [50392, 2232, 11, 718, 311, 536, 437, 2314, 11, 34166, 11, 370, 11, 1392, 11, 1392, 11, 286, 914, 309, 1116, 312, 406, 264, 558, 551, 337, 309, 281, 360, 11, 51116], "temperature": 0.0, "avg_logprob": -0.2182760723566605, "compression_ratio": 1.4796747967479675, "no_speech_prob": 0.0006258930661715567}, {"id": 196, "seek": 90746, "start": 922.5, "end": 932.58, "text": " but anyway, let's see, um, huh, okay, not terrible, let's say show me the equation,", "tokens": [51116, 457, 4033, 11, 718, 311, 536, 11, 1105, 11, 7020, 11, 1392, 11, 406, 6237, 11, 718, 311, 584, 855, 385, 264, 5367, 11, 51620], "temperature": 0.0, "avg_logprob": -0.2182760723566605, "compression_ratio": 1.4796747967479675, "no_speech_prob": 0.0006258930661715567}, {"id": 197, "seek": 93746, "start": 938.26, "end": 941.46, "text": " so we'll talk in a minute about what the heck it was actually doing here,", "tokens": [50404, 370, 321, 603, 751, 294, 257, 3456, 466, 437, 264, 12872, 309, 390, 767, 884, 510, 11, 50564], "temperature": 0.0, "avg_logprob": -0.10220868074441258, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.0005448749870993197}, {"id": 198, "seek": 93746, "start": 942.1, "end": 945.38, "text": " that's not very useful, I want to know the differential equation,", "tokens": [50596, 300, 311, 406, 588, 4420, 11, 286, 528, 281, 458, 264, 15756, 5367, 11, 50760], "temperature": 0.0, "avg_logprob": -0.10220868074441258, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.0005448749870993197}, {"id": 199, "seek": 93746, "start": 946.98, "end": 949.22, "text": " if you want to visualize it, okay, let's see what it does,", "tokens": [50840, 498, 291, 528, 281, 23273, 309, 11, 1392, 11, 718, 311, 536, 437, 309, 775, 11, 50952], "temperature": 0.0, "avg_logprob": -0.10220868074441258, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.0005448749870993197}, {"id": 200, "seek": 93746, "start": 953.46, "end": 959.0600000000001, "text": " let's see, you never know what this thing is going to do, so it's just kind of,", "tokens": [51164, 718, 311, 536, 11, 291, 1128, 458, 437, 341, 551, 307, 516, 281, 360, 11, 370, 309, 311, 445, 733, 295, 11, 51444], "temperature": 0.0, "avg_logprob": -0.10220868074441258, "compression_ratio": 1.5706214689265536, "no_speech_prob": 0.0005448749870993197}, {"id": 201, "seek": 95906, "start": 960.02, "end": 967.38, "text": " um, okay, that's not terrible, I would give that a, you know, maybe a pass and grade, I don't know,", "tokens": [50412, 1105, 11, 1392, 11, 300, 311, 406, 6237, 11, 286, 576, 976, 300, 257, 11, 291, 458, 11, 1310, 257, 1320, 293, 7204, 11, 286, 500, 380, 458, 11, 50780], "temperature": 0.0, "avg_logprob": -0.16042251813979375, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.007474755868315697}, {"id": 202, "seek": 95906, "start": 968.3399999999999, "end": 975.14, "text": " let's see what it actually did, so here, it, okay, so it synthesized, well from language code here,", "tokens": [50828, 718, 311, 536, 437, 309, 767, 630, 11, 370, 510, 11, 309, 11, 1392, 11, 370, 309, 26617, 1602, 11, 731, 490, 2856, 3089, 510, 11, 51168], "temperature": 0.0, "avg_logprob": -0.16042251813979375, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.007474755868315697}, {"id": 203, "seek": 95906, "start": 976.26, "end": 981.54, "text": " this, what these boxes look like inside, probably by next week will look a bit better than what it", "tokens": [51224, 341, 11, 437, 613, 9002, 574, 411, 1854, 11, 1391, 538, 958, 1243, 486, 574, 257, 857, 1101, 813, 437, 309, 51488], "temperature": 0.0, "avg_logprob": -0.16042251813979375, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.007474755868315697}, {"id": 204, "seek": 98154, "start": 981.54, "end": 988.18, "text": " does right now, but, you know, it synthesized some code here, actually, if I say here,", "tokens": [50364, 775, 558, 586, 11, 457, 11, 291, 458, 11, 309, 26617, 1602, 512, 3089, 510, 11, 767, 11, 498, 286, 584, 510, 11, 50696], "temperature": 0.0, "avg_logprob": -0.12121445150936351, "compression_ratio": 1.643312101910828, "no_speech_prob": 0.045624420046806335}, {"id": 205, "seek": 98154, "start": 989.4599999999999, "end": 998.66, "text": " you know, show me the ODE, let me see whether it can do that, um, okay, great,", "tokens": [50760, 291, 458, 11, 855, 385, 264, 422, 22296, 11, 718, 385, 536, 1968, 309, 393, 360, 300, 11, 1105, 11, 1392, 11, 869, 11, 51220], "temperature": 0.0, "avg_logprob": -0.12121445150936351, "compression_ratio": 1.643312101910828, "no_speech_prob": 0.045624420046806335}, {"id": 206, "seek": 98154, "start": 1001.06, "end": 1008.8199999999999, "text": " okay, very good, yes, yes, all good, is that right, no, yes, yes, that's right, that's okay,", "tokens": [51340, 1392, 11, 588, 665, 11, 2086, 11, 2086, 11, 439, 665, 11, 307, 300, 558, 11, 572, 11, 2086, 11, 2086, 11, 300, 311, 558, 11, 300, 311, 1392, 11, 51728], "temperature": 0.0, "avg_logprob": -0.12121445150936351, "compression_ratio": 1.643312101910828, "no_speech_prob": 0.045624420046806335}, {"id": 207, "seek": 100882, "start": 1009.7800000000001, "end": 1016.58, "text": " um, now I say solve that, okay, not bad,", "tokens": [50412, 1105, 11, 586, 286, 584, 5039, 300, 11, 1392, 11, 406, 1578, 11, 50752], "temperature": 0.0, "avg_logprob": -0.14083648257785372, "compression_ratio": 1.2897196261682242, "no_speech_prob": 0.0009311351459473372}, {"id": 208, "seek": 100882, "start": 1019.86, "end": 1027.7, "text": " so, you know, this kind of thing I view as being a pretty useful, come on, I just want to see the", "tokens": [50916, 370, 11, 291, 458, 11, 341, 733, 295, 551, 286, 1910, 382, 885, 257, 1238, 4420, 11, 808, 322, 11, 286, 445, 528, 281, 536, 264, 51308], "temperature": 0.0, "avg_logprob": -0.14083648257785372, "compression_ratio": 1.2897196261682242, "no_speech_prob": 0.0009311351459473372}, {"id": 209, "seek": 102770, "start": 1027.78, "end": 1034.98, "text": " equation, um, what I want to see is the, is the code here, okay, let's say show me the code,", "tokens": [50368, 5367, 11, 1105, 11, 437, 286, 528, 281, 536, 307, 264, 11, 307, 264, 3089, 510, 11, 1392, 11, 718, 311, 584, 855, 385, 264, 3089, 11, 50728], "temperature": 0.0, "avg_logprob": -0.09740478878929501, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.18743088841438293}, {"id": 210, "seek": 102770, "start": 1038.26, "end": 1045.14, "text": " because this is, this is in a sense, okay, finally we got it, okay, and then what we can do here", "tokens": [50892, 570, 341, 307, 11, 341, 307, 294, 257, 2020, 11, 1392, 11, 2721, 321, 658, 309, 11, 1392, 11, 293, 550, 437, 321, 393, 360, 510, 51236], "temperature": 0.0, "avg_logprob": -0.09740478878929501, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.18743088841438293}, {"id": 211, "seek": 102770, "start": 1045.14, "end": 1049.38, "text": " is given this piece of code, we can just say, for example, we can just say evaluate code,", "tokens": [51236, 307, 2212, 341, 2522, 295, 3089, 11, 321, 393, 445, 584, 11, 337, 1365, 11, 321, 393, 445, 584, 13059, 3089, 11, 51448], "temperature": 0.0, "avg_logprob": -0.09740478878929501, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.18743088841438293}, {"id": 212, "seek": 102770, "start": 1049.38, "end": 1054.5800000000002, "text": " oh look, wait a minute, something is happening in the background here, it apologizes for the", "tokens": [51448, 1954, 574, 11, 1699, 257, 3456, 11, 746, 307, 2737, 294, 264, 3678, 510, 11, 309, 9472, 5660, 337, 264, 51708], "temperature": 0.0, "avg_logprob": -0.09740478878929501, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.18743088841438293}, {"id": 213, "seek": 105458, "start": 1054.58, "end": 1064.26, "text": " inconvenience, um, who knows what it's doing, we'll check back for that in a few moments,", "tokens": [50364, 28752, 1182, 11, 1105, 11, 567, 3255, 437, 309, 311, 884, 11, 321, 603, 1520, 646, 337, 300, 294, 257, 1326, 6065, 11, 50848], "temperature": 0.0, "avg_logprob": -0.15658894630327616, "compression_ratio": 1.5705882352941176, "no_speech_prob": 0.004979404155164957}, {"id": 214, "seek": 105458, "start": 1065.62, "end": 1073.3, "text": " but back in this notebook, we're here, I can just say use that thing to copy that", "tokens": [50916, 457, 646, 294, 341, 21060, 11, 321, 434, 510, 11, 286, 393, 445, 584, 764, 300, 551, 281, 5055, 300, 51300], "temperature": 0.0, "avg_logprob": -0.15658894630327616, "compression_ratio": 1.5705882352941176, "no_speech_prob": 0.004979404155164957}, {"id": 215, "seek": 105458, "start": 1073.86, "end": 1080.1, "text": " code down there to the next cell and then do the evaluation and get the result, so it's kind of", "tokens": [51328, 3089, 760, 456, 281, 264, 958, 2815, 293, 550, 360, 264, 13344, 293, 483, 264, 1874, 11, 370, 309, 311, 733, 295, 51640], "temperature": 0.0, "avg_logprob": -0.15658894630327616, "compression_ratio": 1.5705882352941176, "no_speech_prob": 0.004979404155164957}, {"id": 216, "seek": 108010, "start": 1080.1, "end": 1084.5, "text": " interesting, you know, if I go back here, maybe I can try another example, let me show you how", "tokens": [50364, 1880, 11, 291, 458, 11, 498, 286, 352, 646, 510, 11, 1310, 286, 393, 853, 1071, 1365, 11, 718, 385, 855, 291, 577, 50584], "temperature": 0.0, "avg_logprob": -0.10147417637339809, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.037508003413677216}, {"id": 217, "seek": 108010, "start": 1084.5, "end": 1089.6999999999998, "text": " this works, I can put in what we call a chat block, that basically breaks the context of the LLM,", "tokens": [50584, 341, 1985, 11, 286, 393, 829, 294, 437, 321, 818, 257, 5081, 3461, 11, 300, 1936, 9857, 264, 4319, 295, 264, 441, 43, 44, 11, 50844], "temperature": 0.0, "avg_logprob": -0.10147417637339809, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.037508003413677216}, {"id": 218, "seek": 108010, "start": 1089.6999999999998, "end": 1095.78, "text": " so the LLM, whenever I'm saying, when I say show me the code, when I ask that, it's able to see the", "tokens": [50844, 370, 264, 441, 43, 44, 11, 5699, 286, 478, 1566, 11, 562, 286, 584, 855, 385, 264, 3089, 11, 562, 286, 1029, 300, 11, 309, 311, 1075, 281, 536, 264, 51148], "temperature": 0.0, "avg_logprob": -0.10147417637339809, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.037508003413677216}, {"id": 219, "seek": 108010, "start": 1095.78, "end": 1105.06, "text": " whole, the whole conversation that it's had above it, okay, so that's um, and so now here, I broke", "tokens": [51148, 1379, 11, 264, 1379, 3761, 300, 309, 311, 632, 3673, 309, 11, 1392, 11, 370, 300, 311, 1105, 11, 293, 370, 586, 510, 11, 286, 6902, 51612], "temperature": 0.0, "avg_logprob": -0.10147417637339809, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.037508003413677216}, {"id": 220, "seek": 110506, "start": 1105.06, "end": 1111.1399999999999, "text": " that by saying show me another, show me another thing here and I could say, well here, for example,", "tokens": [50364, 300, 538, 1566, 855, 385, 1071, 11, 855, 385, 1071, 551, 510, 293, 286, 727, 584, 11, 731, 510, 11, 337, 1365, 11, 50668], "temperature": 0.0, "avg_logprob": -0.1132095450221902, "compression_ratio": 1.945273631840796, "no_speech_prob": 0.18300344049930573}, {"id": 221, "seek": 110506, "start": 1111.1399999999999, "end": 1116.98, "text": " I can do this pull down and this um, this allows me to make all kinds of changes, so I could, for", "tokens": [50668, 286, 393, 360, 341, 2235, 760, 293, 341, 1105, 11, 341, 4045, 385, 281, 652, 439, 3685, 295, 2962, 11, 370, 286, 727, 11, 337, 50960], "temperature": 0.0, "avg_logprob": -0.1132095450221902, "compression_ratio": 1.945273631840796, "no_speech_prob": 0.18300344049930573}, {"id": 222, "seek": 110506, "start": 1116.98, "end": 1124.74, "text": " example, we have this prompt repository that contains, there we go, so it contains various um,", "tokens": [50960, 1365, 11, 321, 362, 341, 12391, 25841, 300, 8306, 11, 456, 321, 352, 11, 370, 309, 8306, 3683, 1105, 11, 51348], "temperature": 0.0, "avg_logprob": -0.1132095450221902, "compression_ratio": 1.945273631840796, "no_speech_prob": 0.18300344049930573}, {"id": 223, "seek": 110506, "start": 1124.74, "end": 1129.94, "text": " well many, okay, we can go to the prompt repository here, this is a prompt repository that has, in", "tokens": [51348, 731, 867, 11, 1392, 11, 321, 393, 352, 281, 264, 12391, 25841, 510, 11, 341, 307, 257, 12391, 25841, 300, 575, 11, 294, 51608], "temperature": 0.0, "avg_logprob": -0.1132095450221902, "compression_ratio": 1.945273631840796, "no_speech_prob": 0.18300344049930573}, {"id": 224, "seek": 112994, "start": 1130.02, "end": 1135.3, "text": " this case, it will allow us to pick personas for interacting with this, so I could pick um,", "tokens": [50368, 341, 1389, 11, 309, 486, 2089, 505, 281, 1888, 12019, 337, 18017, 365, 341, 11, 370, 286, 727, 1888, 1105, 11, 50632], "temperature": 0.0, "avg_logprob": -0.11069170633951823, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.034813761711120605}, {"id": 225, "seek": 112994, "start": 1135.94, "end": 1139.7, "text": " I don't know, let's try this, I have no idea what this is going to be like, okay, so as I", "tokens": [50664, 286, 500, 380, 458, 11, 718, 311, 853, 341, 11, 286, 362, 572, 1558, 437, 341, 307, 516, 281, 312, 411, 11, 1392, 11, 370, 382, 286, 50852], "temperature": 0.0, "avg_logprob": -0.11069170633951823, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.034813761711120605}, {"id": 226, "seek": 112994, "start": 1139.7, "end": 1145.54, "text": " install the 19th century British novel persona, I've installed that, actually I kind of think we", "tokens": [50852, 3625, 264, 1294, 392, 4901, 6221, 7613, 12184, 11, 286, 600, 8899, 300, 11, 767, 286, 733, 295, 519, 321, 51144], "temperature": 0.0, "avg_logprob": -0.11069170633951823, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.034813761711120605}, {"id": 227, "seek": 112994, "start": 1145.54, "end": 1152.18, "text": " should use Bernardo, he's fun, um, but either one, okay, so I can now pull this down, okay, we can try", "tokens": [51144, 820, 764, 10781, 12850, 11, 415, 311, 1019, 11, 1105, 11, 457, 2139, 472, 11, 1392, 11, 370, 286, 393, 586, 2235, 341, 760, 11, 1392, 11, 321, 393, 853, 51476], "temperature": 0.0, "avg_logprob": -0.11069170633951823, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.034813761711120605}, {"id": 228, "seek": 115218, "start": 1152.18, "end": 1168.5800000000002, "text": " 19th century British novel, um, make a picture of a circle, uh, that is half red and half blue,", "tokens": [50364, 1294, 392, 4901, 6221, 7613, 11, 1105, 11, 652, 257, 3036, 295, 257, 6329, 11, 2232, 11, 300, 307, 1922, 2182, 293, 1922, 3344, 11, 51184], "temperature": 0.0, "avg_logprob": -0.12834034294917665, "compression_ratio": 1.0919540229885059, "no_speech_prob": 0.05580950155854225}, {"id": 229, "seek": 116858, "start": 1168.58, "end": 1182.8999999999999, "text": " let's say, now I think this will, uh, oh come now, oh great, well that's, that's okay, big mess, um,", "tokens": [50364, 718, 311, 584, 11, 586, 286, 519, 341, 486, 11, 2232, 11, 1954, 808, 586, 11, 1954, 869, 11, 731, 300, 311, 11, 300, 311, 1392, 11, 955, 2082, 11, 1105, 11, 51080], "temperature": 0.0, "avg_logprob": -0.14824436604976654, "compression_ratio": 1.4923076923076923, "no_speech_prob": 0.2082616239786148}, {"id": 230, "seek": 116858, "start": 1185.1399999999999, "end": 1192.4199999999998, "text": " bad taste, okay, and this code snippet, blah blah blah, good luck, well let's try, all right,", "tokens": [51192, 1578, 3939, 11, 1392, 11, 293, 341, 3089, 35623, 302, 11, 12288, 12288, 12288, 11, 665, 3668, 11, 731, 718, 311, 853, 11, 439, 558, 11, 51556], "temperature": 0.0, "avg_logprob": -0.14824436604976654, "compression_ratio": 1.4923076923076923, "no_speech_prob": 0.2082616239786148}, {"id": 231, "seek": 119242, "start": 1192.42, "end": 1197.78, "text": " let's try, let's try doing this, actually I should just stop this yacking on like this,", "tokens": [50364, 718, 311, 853, 11, 718, 311, 853, 884, 341, 11, 767, 286, 820, 445, 1590, 341, 288, 14134, 322, 411, 341, 11, 50632], "temperature": 0.0, "avg_logprob": -0.12314166341509138, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.01789417676627636}, {"id": 232, "seek": 119242, "start": 1198.42, "end": 1203.14, "text": " let's try, let's try a different persona, let's try, let's try the code assistant, but actually,", "tokens": [50664, 718, 311, 853, 11, 718, 311, 853, 257, 819, 12184, 11, 718, 311, 853, 11, 718, 311, 853, 264, 3089, 10994, 11, 457, 767, 11, 50900], "temperature": 0.0, "avg_logprob": -0.12314166341509138, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.01789417676627636}, {"id": 233, "seek": 119242, "start": 1203.14, "end": 1209.54, "text": " you know what, I'm going to try Bernardo, Bernardo is fun, let's, let's try re-evaluating that,", "tokens": [50900, 291, 458, 437, 11, 286, 478, 516, 281, 853, 10781, 12850, 11, 10781, 12850, 307, 1019, 11, 718, 311, 11, 718, 311, 853, 319, 12, 68, 3337, 32438, 300, 11, 51220], "temperature": 0.0, "avg_logprob": -0.12314166341509138, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.01789417676627636}, {"id": 234, "seek": 119242, "start": 1213.54, "end": 1219.46, "text": " the, now what's it doing there, I don't know, this first creates a full red disk and it overlays", "tokens": [51420, 264, 11, 586, 437, 311, 309, 884, 456, 11, 286, 500, 380, 458, 11, 341, 700, 7829, 257, 1577, 2182, 12355, 293, 309, 15986, 3772, 51716], "temperature": 0.0, "avg_logprob": -0.12314166341509138, "compression_ratio": 1.821256038647343, "no_speech_prob": 0.01789417676627636}, {"id": 235, "seek": 121946, "start": 1219.46, "end": 1225.78, "text": " a hard disk, okay, I wonder if this is actually right, dah, okay, it worked, nice, what's important", "tokens": [50364, 257, 1152, 12355, 11, 1392, 11, 286, 2441, 498, 341, 307, 767, 558, 11, 16800, 11, 1392, 11, 309, 2732, 11, 1481, 11, 437, 311, 1021, 50680], "temperature": 0.0, "avg_logprob": -0.1043171344264861, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.022113362327218056}, {"id": 236, "seek": 121946, "start": 1225.78, "end": 1230.42, "text": " about this, this maybe isn't the very best example, is you can actually read that code,", "tokens": [50680, 466, 341, 11, 341, 1310, 1943, 380, 264, 588, 1151, 1365, 11, 307, 291, 393, 767, 1401, 300, 3089, 11, 50912], "temperature": 0.0, "avg_logprob": -0.1043171344264861, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.022113362327218056}, {"id": 237, "seek": 121946, "start": 1230.98, "end": 1236.58, "text": " unlike the, the thing that it happened to produce before, um, and the, the kind of the idea is,", "tokens": [50940, 8343, 264, 11, 264, 551, 300, 309, 2011, 281, 5258, 949, 11, 1105, 11, 293, 264, 11, 264, 733, 295, 264, 1558, 307, 11, 51220], "temperature": 0.0, "avg_logprob": -0.1043171344264861, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.022113362327218056}, {"id": 238, "seek": 121946, "start": 1236.58, "end": 1240.74, "text": " and this is by the way one big feature of, of kind of the whole computational language story", "tokens": [51220, 293, 341, 307, 538, 264, 636, 472, 955, 4111, 295, 11, 295, 733, 295, 264, 1379, 28270, 2856, 1657, 51428], "temperature": 0.0, "avg_logprob": -0.1043171344264861, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.022113362327218056}, {"id": 239, "seek": 121946, "start": 1240.74, "end": 1245.6200000000001, "text": " that I've spent so long on, is that, you know, our language is intended as something that you can", "tokens": [51428, 300, 286, 600, 4418, 370, 938, 322, 11, 307, 300, 11, 291, 458, 11, 527, 2856, 307, 10226, 382, 746, 300, 291, 393, 51672], "temperature": 0.0, "avg_logprob": -0.1043171344264861, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.022113362327218056}, {"id": 240, "seek": 124562, "start": 1245.62, "end": 1251.86, "text": " think in, as well as have your computer execute, so to speak, kind of like math notation would be,", "tokens": [50364, 519, 294, 11, 382, 731, 382, 362, 428, 3820, 14483, 11, 370, 281, 1710, 11, 733, 295, 411, 5221, 24657, 576, 312, 11, 50676], "temperature": 0.0, "avg_logprob": -0.10082638263702393, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.0727996975183487}, {"id": 241, "seek": 124562, "start": 1251.86, "end": 1257.62, "text": " it's something that, where you can actually, you know, use it as your foundation for thinking about", "tokens": [50676, 309, 311, 746, 300, 11, 689, 291, 393, 767, 11, 291, 458, 11, 764, 309, 382, 428, 7030, 337, 1953, 466, 50964], "temperature": 0.0, "avg_logprob": -0.10082638263702393, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.0727996975183487}, {"id": 242, "seek": 124562, "start": 1257.62, "end": 1262.34, "text": " things, okay, anyway, that this is, you, you get the basic idea, I hope, of, of sort of this chat", "tokens": [50964, 721, 11, 1392, 11, 4033, 11, 300, 341, 307, 11, 291, 11, 291, 483, 264, 3875, 1558, 11, 286, 1454, 11, 295, 11, 295, 1333, 295, 341, 5081, 51200], "temperature": 0.0, "avg_logprob": -0.10082638263702393, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.0727996975183487}, {"id": 243, "seek": 124562, "start": 1262.34, "end": 1267.62, "text": " notebook notion, it's, it's pretty nice, I mean, I have to say, since the reason that I haven't used", "tokens": [51200, 21060, 10710, 11, 309, 311, 11, 309, 311, 1238, 1481, 11, 286, 914, 11, 286, 362, 281, 584, 11, 1670, 264, 1778, 300, 286, 2378, 380, 1143, 51464], "temperature": 0.0, "avg_logprob": -0.10082638263702393, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.0727996975183487}, {"id": 244, "seek": 124562, "start": 1267.62, "end": 1274.34, "text": " that, um, the chatGBT interface for months, is because this is really a lot nicer, you get to,", "tokens": [51464, 300, 11, 1105, 11, 264, 5081, 8769, 51, 9226, 337, 2493, 11, 307, 570, 341, 307, 534, 257, 688, 22842, 11, 291, 483, 281, 11, 51800], "temperature": 0.0, "avg_logprob": -0.10082638263702393, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.0727996975183487}, {"id": 245, "seek": 127434, "start": 1274.34, "end": 1278.5, "text": " not only, you know, you can also use all the standard features of notebooks, so you can say,", "tokens": [50364, 406, 787, 11, 291, 458, 11, 291, 393, 611, 764, 439, 264, 3832, 4122, 295, 43782, 11, 370, 291, 393, 584, 11, 50572], "temperature": 0.0, "avg_logprob": -0.0965190923438882, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.0012563845375552773}, {"id": 246, "seek": 127434, "start": 1279.1399999999999, "end": 1285.3, "text": " this is a section about circles, and you can start putting in, maybe I could, well actually,", "tokens": [50604, 341, 307, 257, 3541, 466, 13040, 11, 293, 291, 393, 722, 3372, 294, 11, 1310, 286, 727, 11, 731, 767, 11, 50912], "temperature": 0.0, "avg_logprob": -0.0965190923438882, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.0012563845375552773}, {"id": 247, "seek": 127434, "start": 1285.3, "end": 1292.74, "text": " let me just do this, hold on, let's say, do that, I'm not going to live dangerous, do that for a sphere,", "tokens": [50912, 718, 385, 445, 360, 341, 11, 1797, 322, 11, 718, 311, 584, 11, 360, 300, 11, 286, 478, 406, 516, 281, 1621, 5795, 11, 360, 300, 337, 257, 16687, 11, 51284], "temperature": 0.0, "avg_logprob": -0.0965190923438882, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.0012563845375552773}, {"id": 248, "seek": 127434, "start": 1292.74, "end": 1301.3799999999999, "text": " this is going to fail, of course, okay, let's see what happens, okay, interesting idea, interesting", "tokens": [51284, 341, 307, 516, 281, 3061, 11, 295, 1164, 11, 1392, 11, 718, 311, 536, 437, 2314, 11, 1392, 11, 1880, 1558, 11, 1880, 51716], "temperature": 0.0, "avg_logprob": -0.0965190923438882, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.0012563845375552773}, {"id": 249, "seek": 130138, "start": 1301.38, "end": 1307.14, "text": " idea, I wonder whether that will work, that's definitely an interesting idea, I give that", "tokens": [50364, 1558, 11, 286, 2441, 1968, 300, 486, 589, 11, 300, 311, 2138, 364, 1880, 1558, 11, 286, 976, 300, 50652], "temperature": 0.0, "avg_logprob": -0.0939338865734282, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.002695470117032528}, {"id": 250, "seek": 130138, "start": 1307.14, "end": 1314.74, "text": " point, the spherical plot, that goes, wow, if this works, I'll be impressed, okay, let's run it,", "tokens": [50652, 935, 11, 264, 37300, 7542, 11, 300, 1709, 11, 6076, 11, 498, 341, 1985, 11, 286, 603, 312, 11679, 11, 1392, 11, 718, 311, 1190, 309, 11, 51032], "temperature": 0.0, "avg_logprob": -0.0939338865734282, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.002695470117032528}, {"id": 251, "seek": 130138, "start": 1316.0200000000002, "end": 1324.18, "text": " wow, that's cool, it's getting smarter, the, or how, or the fine tuning that we've done and so on", "tokens": [51096, 6076, 11, 300, 311, 1627, 11, 309, 311, 1242, 20294, 11, 264, 11, 420, 577, 11, 420, 264, 2489, 15164, 300, 321, 600, 1096, 293, 370, 322, 51504], "temperature": 0.0, "avg_logprob": -0.0939338865734282, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.002695470117032528}, {"id": 252, "seek": 130138, "start": 1324.18, "end": 1329.3000000000002, "text": " is actually working, this is encouraging, um, the, because this is kind of interesting, I mean,", "tokens": [51504, 307, 767, 1364, 11, 341, 307, 14580, 11, 1105, 11, 264, 11, 570, 341, 307, 733, 295, 1880, 11, 286, 914, 11, 51760], "temperature": 0.0, "avg_logprob": -0.0939338865734282, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.002695470117032528}, {"id": 253, "seek": 132930, "start": 1329.3, "end": 1335.94, "text": " it made a spherical plot over a certain, you know, latitude, sequence of latitude values,", "tokens": [50364, 309, 1027, 257, 37300, 7542, 670, 257, 1629, 11, 291, 458, 11, 45436, 11, 8310, 295, 45436, 4190, 11, 50696], "temperature": 0.0, "avg_logprob": -0.10289794626370283, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.000916812801733613}, {"id": 254, "seek": 132930, "start": 1335.94, "end": 1340.1, "text": " a different sequence of longitude values here, that's kind of interesting, you kind of learned", "tokens": [50696, 257, 819, 8310, 295, 938, 4377, 4190, 510, 11, 300, 311, 733, 295, 1880, 11, 291, 733, 295, 3264, 50904], "temperature": 0.0, "avg_logprob": -0.10289794626370283, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.000916812801733613}, {"id": 255, "seek": 132930, "start": 1340.1, "end": 1344.98, "text": " something from that, maybe we could try, let's try one other thing, which might be fun, let's say,", "tokens": [50904, 746, 490, 300, 11, 1310, 321, 727, 853, 11, 718, 311, 853, 472, 661, 551, 11, 597, 1062, 312, 1019, 11, 718, 311, 584, 11, 51148], "temperature": 0.0, "avg_logprob": -0.10289794626370283, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.000916812801733613}, {"id": 256, "seek": 134498, "start": 1345.8600000000001, "end": 1360.34, "text": " um, uh, show a star chart of the current position of Jupiter, now I'm probably going to have to say,", "tokens": [50408, 1105, 11, 2232, 11, 855, 257, 3543, 6927, 295, 264, 2190, 2535, 295, 24567, 11, 586, 286, 478, 1391, 516, 281, 362, 281, 584, 11, 51132], "temperature": 0.0, "avg_logprob": -0.23591141865171236, "compression_ratio": 1.1111111111111112, "no_speech_prob": 0.011062844656407833}, {"id": 257, "seek": 136034, "start": 1360.5, "end": 1364.98, "text": " use astrographics, let's see what will happen here,", "tokens": [50372, 764, 5357, 6675, 2662, 1167, 11, 718, 311, 536, 437, 486, 1051, 510, 11, 50596], "temperature": 0.0, "avg_logprob": -0.0982818603515625, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.01619105599820614}, {"id": 258, "seek": 136034, "start": 1368.82, "end": 1378.74, "text": " oh, come on, it just, well, I think it made that up, I'd be very surprised if these functions", "tokens": [50788, 1954, 11, 808, 322, 11, 309, 445, 11, 731, 11, 286, 519, 309, 1027, 300, 493, 11, 286, 1116, 312, 588, 6100, 498, 613, 6828, 51284], "temperature": 0.0, "avg_logprob": -0.0982818603515625, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.01619105599820614}, {"id": 259, "seek": 136034, "start": 1378.74, "end": 1388.1799999999998, "text": " actually exist, no, they do not exist, um, well, that's a bad sign, okay, let's try saying use", "tokens": [51284, 767, 2514, 11, 572, 11, 436, 360, 406, 2514, 11, 1105, 11, 731, 11, 300, 311, 257, 1578, 1465, 11, 1392, 11, 718, 311, 853, 1566, 764, 51756], "temperature": 0.0, "avg_logprob": -0.0982818603515625, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.01619105599820614}, {"id": 260, "seek": 138818, "start": 1388.26, "end": 1394.18, "text": " astro position, and what I'm expecting it's going to do, maybe,", "tokens": [50368, 5357, 340, 2535, 11, 293, 437, 286, 478, 9650, 309, 311, 516, 281, 360, 11, 1310, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08369656784893716, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.0034595211036503315}, {"id": 261, "seek": 138818, "start": 1399.3, "end": 1403.78, "text": " lovely, but that's also not relevant, okay, it's not doing what I thought it would do,", "tokens": [50920, 7496, 11, 457, 300, 311, 611, 406, 7340, 11, 1392, 11, 309, 311, 406, 884, 437, 286, 1194, 309, 576, 360, 11, 51144], "temperature": 0.0, "avg_logprob": -0.08369656784893716, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.0034595211036503315}, {"id": 262, "seek": 138818, "start": 1403.78, "end": 1409.7, "text": " which is to go read the documentation, we can, we could probably tell it to do that, let's say, um,", "tokens": [51144, 597, 307, 281, 352, 1401, 264, 14333, 11, 321, 393, 11, 321, 727, 1391, 980, 309, 281, 360, 300, 11, 718, 311, 584, 11, 1105, 11, 51440], "temperature": 0.0, "avg_logprob": -0.08369656784893716, "compression_ratio": 1.5432098765432098, "no_speech_prob": 0.0034595211036503315}, {"id": 263, "seek": 140970, "start": 1410.02, "end": 1421.38, "text": " blah, blah, blah, there we go, now maybe it'll get a little bit smarter,", "tokens": [50380, 12288, 11, 12288, 11, 12288, 11, 456, 321, 352, 11, 586, 1310, 309, 603, 483, 257, 707, 857, 20294, 11, 50948], "temperature": 0.0, "avg_logprob": -0.1534977564736018, "compression_ratio": 1.5815602836879432, "no_speech_prob": 0.0009659742936491966}, {"id": 264, "seek": 140970, "start": 1427.06, "end": 1430.5, "text": " okay, this is much better, this is much, much better sign,", "tokens": [51232, 1392, 11, 341, 307, 709, 1101, 11, 341, 307, 709, 11, 709, 1101, 1465, 11, 51404], "temperature": 0.0, "avg_logprob": -0.1534977564736018, "compression_ratio": 1.5815602836879432, "no_speech_prob": 0.0009659742936491966}, {"id": 265, "seek": 140970, "start": 1431.54, "end": 1437.06, "text": " so hopefully, if it read the documentation, it will be able to successfully do what it was,", "tokens": [51456, 370, 4696, 11, 498, 309, 1401, 264, 14333, 11, 309, 486, 312, 1075, 281, 10727, 360, 437, 309, 390, 11, 51732], "temperature": 0.0, "avg_logprob": -0.1534977564736018, "compression_ratio": 1.5815602836879432, "no_speech_prob": 0.0009659742936491966}, {"id": 266, "seek": 143706, "start": 1438.02, "end": 1443.3799999999999, "text": " all right, I don't know whether it's blah, blah, blah, blah, blah, now probably if we now say,", "tokens": [50412, 439, 558, 11, 286, 500, 380, 458, 1968, 309, 311, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 586, 1391, 498, 321, 586, 584, 11, 50680], "temperature": 0.0, "avg_logprob": -0.08877122061593191, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0028522415086627007}, {"id": 267, "seek": 143706, "start": 1443.3799999999999, "end": 1449.3, "text": " okay, great, it's talking about all kinds of, I don't know, it's telling us how to find the", "tokens": [50680, 1392, 11, 869, 11, 309, 311, 1417, 466, 439, 3685, 295, 11, 286, 500, 380, 458, 11, 309, 311, 3585, 505, 577, 281, 915, 264, 50976], "temperature": 0.0, "avg_logprob": -0.08877122061593191, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0028522415086627007}, {"id": 268, "seek": 143706, "start": 1449.3, "end": 1452.8999999999999, "text": " position of the large Magellanic cloud, et cetera, et cetera, et cetera, that's all fun,", "tokens": [50976, 2535, 295, 264, 2416, 49293, 285, 30732, 4588, 11, 1030, 11458, 11, 1030, 11458, 11, 1030, 11458, 11, 300, 311, 439, 1019, 11, 51156], "temperature": 0.0, "avg_logprob": -0.08877122061593191, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0028522415086627007}, {"id": 269, "seek": 143706, "start": 1452.8999999999999, "end": 1461.94, "text": " and we could ask it to run that, but I think use this for the picture of Jupiter,", "tokens": [51156, 293, 321, 727, 1029, 309, 281, 1190, 300, 11, 457, 286, 519, 764, 341, 337, 264, 3036, 295, 24567, 11, 51608], "temperature": 0.0, "avg_logprob": -0.08877122061593191, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0028522415086627007}, {"id": 270, "seek": 146194, "start": 1462.02, "end": 1470.3400000000001, "text": " maybe this will work, maybe it won't, okay, this is much better, what,", "tokens": [50368, 1310, 341, 486, 589, 11, 1310, 309, 1582, 380, 11, 1392, 11, 341, 307, 709, 1101, 11, 437, 11, 50784], "temperature": 0.0, "avg_logprob": -0.09592147749297474, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.01281107310205698}, {"id": 271, "seek": 146194, "start": 1472.1000000000001, "end": 1477.7, "text": " you see this is the problem, it just makes stuff up, well let's see, I wonder whether this will", "tokens": [50872, 291, 536, 341, 307, 264, 1154, 11, 309, 445, 1669, 1507, 493, 11, 731, 718, 311, 536, 11, 286, 2441, 1968, 341, 486, 51152], "temperature": 0.0, "avg_logprob": -0.09592147749297474, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.01281107310205698}, {"id": 272, "seek": 146194, "start": 1477.7, "end": 1482.66, "text": " work, no, it made up the thing called planet marker, well we'd have to tell it not to do that,", "tokens": [51152, 589, 11, 572, 11, 309, 1027, 493, 264, 551, 1219, 5054, 15247, 11, 731, 321, 1116, 362, 281, 980, 309, 406, 281, 360, 300, 11, 51400], "temperature": 0.0, "avg_logprob": -0.09592147749297474, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.01281107310205698}, {"id": 273, "seek": 146194, "start": 1483.38, "end": 1487.94, "text": " it's supposed to go back, and I'm a little bit surprised it did that here, because actually", "tokens": [51436, 309, 311, 3442, 281, 352, 646, 11, 293, 286, 478, 257, 707, 857, 6100, 309, 630, 300, 510, 11, 570, 767, 51664], "temperature": 0.0, "avg_logprob": -0.09592147749297474, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.01281107310205698}, {"id": 274, "seek": 148794, "start": 1488.02, "end": 1493.6200000000001, "text": " it has been told to go back and check the code it wrote to make sure that everything in it actually", "tokens": [50368, 309, 575, 668, 1907, 281, 352, 646, 293, 1520, 264, 3089, 309, 4114, 281, 652, 988, 300, 1203, 294, 309, 767, 50648], "temperature": 0.0, "avg_logprob": -0.09050432840983073, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00787013117223978}, {"id": 275, "seek": 148794, "start": 1493.6200000000001, "end": 1499.38, "text": " exists, so for some reason it didn't in this case, all right, well anyway, that's a little bit on", "tokens": [50648, 8198, 11, 370, 337, 512, 1778, 309, 994, 380, 294, 341, 1389, 11, 439, 558, 11, 731, 4033, 11, 300, 311, 257, 707, 857, 322, 50936], "temperature": 0.0, "avg_logprob": -0.09050432840983073, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00787013117223978}, {"id": 276, "seek": 148794, "start": 1499.38, "end": 1505.7, "text": " kind of the sort of the interface between sort of LLMs, computational language, I thought another", "tokens": [50936, 733, 295, 264, 1333, 295, 264, 9226, 1296, 1333, 295, 441, 43, 26386, 11, 28270, 2856, 11, 286, 1194, 1071, 51252], "temperature": 0.0, "avg_logprob": -0.09050432840983073, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00787013117223978}, {"id": 277, "seek": 148794, "start": 1505.7, "end": 1514.9, "text": " thing I would talk about, quite different subject, is using physics to think about LLMs, so let me", "tokens": [51252, 551, 286, 576, 751, 466, 11, 1596, 819, 3983, 11, 307, 1228, 10649, 281, 519, 466, 441, 43, 26386, 11, 370, 718, 385, 51712], "temperature": 0.0, "avg_logprob": -0.09050432840983073, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00787013117223978}, {"id": 278, "seek": 151490, "start": 1514.98, "end": 1520.26, "text": " pull up some things, so first question is what fundamentally is an LLM doing, as I said, what", "tokens": [50368, 2235, 493, 512, 721, 11, 370, 700, 1168, 307, 437, 17879, 307, 364, 441, 43, 44, 884, 11, 382, 286, 848, 11, 437, 50632], "temperature": 0.0, "avg_logprob": -0.09701528856831212, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.005597688257694244}, {"id": 279, "seek": 151490, "start": 1520.26, "end": 1526.3400000000001, "text": " it's ultimately doing is it's saying, given a particular piece of text, let's see if this works,", "tokens": [50632, 309, 311, 6284, 884, 307, 309, 311, 1566, 11, 2212, 257, 1729, 2522, 295, 2487, 11, 718, 311, 536, 498, 341, 1985, 11, 50936], "temperature": 0.0, "avg_logprob": -0.09701528856831212, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.005597688257694244}, {"id": 280, "seek": 151490, "start": 1526.3400000000001, "end": 1530.66, "text": " okay, so if you have something like this, you feed the prompt, the best thing about AI is its", "tokens": [50936, 1392, 11, 370, 498, 291, 362, 746, 411, 341, 11, 291, 3154, 264, 12391, 11, 264, 1151, 551, 466, 7318, 307, 1080, 51152], "temperature": 0.0, "avg_logprob": -0.09701528856831212, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.005597688257694244}, {"id": 281, "seek": 151490, "start": 1530.66, "end": 1535.5400000000002, "text": " ability to, and then its mission is to give you what the next word should be, and there are", "tokens": [51152, 3485, 281, 11, 293, 550, 1080, 4447, 307, 281, 976, 291, 437, 264, 958, 1349, 820, 312, 11, 293, 456, 366, 51396], "temperature": 0.0, "avg_logprob": -0.09701528856831212, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.005597688257694244}, {"id": 282, "seek": 151490, "start": 1535.5400000000002, "end": 1542.9, "text": " some probabilities that it uses to do that, so if we kind of, we're interested in knowing, where's", "tokens": [51396, 512, 33783, 300, 309, 4960, 281, 360, 300, 11, 370, 498, 321, 733, 295, 11, 321, 434, 3102, 294, 5276, 11, 689, 311, 51764], "temperature": 0.0, "avg_logprob": -0.09701528856831212, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.005597688257694244}, {"id": 283, "seek": 154290, "start": 1542.9, "end": 1557.94, "text": " my mouse, come on, up, just a second, sigh, you know, maybe I should just, well, okay, let me just,", "tokens": [50364, 452, 9719, 11, 808, 322, 11, 493, 11, 445, 257, 1150, 11, 29472, 11, 291, 458, 11, 1310, 286, 820, 445, 11, 731, 11, 1392, 11, 718, 385, 445, 11, 51116], "temperature": 0.0, "avg_logprob": -0.18589156970643161, "compression_ratio": 1.4883720930232558, "no_speech_prob": 0.0030332293827086687}, {"id": 284, "seek": 154290, "start": 1559.38, "end": 1572.18, "text": " that's very strange, fascinating, okay, the lost mouse, okay, the lost mouse has been found,", "tokens": [51188, 300, 311, 588, 5861, 11, 10343, 11, 1392, 11, 264, 2731, 9719, 11, 1392, 11, 264, 2731, 9719, 575, 668, 1352, 11, 51828], "temperature": 0.0, "avg_logprob": -0.18589156970643161, "compression_ratio": 1.4883720930232558, "no_speech_prob": 0.0030332293827086687}, {"id": 285, "seek": 157290, "start": 1572.98, "end": 1580.3400000000001, "text": " maybe, all right, so just, I mean, let's talk a little bit about what,", "tokens": [50368, 1310, 11, 439, 558, 11, 370, 445, 11, 286, 914, 11, 718, 311, 751, 257, 707, 857, 466, 437, 11, 50736], "temperature": 0.0, "avg_logprob": -0.10862250205798027, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0005871226894669235}, {"id": 286, "seek": 157290, "start": 1583.3000000000002, "end": 1590.42, "text": " actually, let me show you something else here, so in our language, well, no, we can do it here,", "tokens": [50884, 767, 11, 718, 385, 855, 291, 746, 1646, 510, 11, 370, 294, 527, 2856, 11, 731, 11, 572, 11, 321, 393, 360, 309, 510, 11, 51240], "temperature": 0.0, "avg_logprob": -0.10862250205798027, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0005871226894669235}, {"id": 287, "seek": 157290, "start": 1591.3000000000002, "end": 1600.8200000000002, "text": " we can just say, let's use a plain chat, and let's set it so that one of the parameters is,", "tokens": [51284, 321, 393, 445, 584, 11, 718, 311, 764, 257, 11121, 5081, 11, 293, 718, 311, 992, 309, 370, 300, 472, 295, 264, 9834, 307, 11, 51760], "temperature": 0.0, "avg_logprob": -0.10862250205798027, "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.0005871226894669235}, {"id": 288, "seek": 160082, "start": 1600.8999999999999, "end": 1605.06, "text": " we saw those probabilities that the LLM produced for what the next word should be,", "tokens": [50368, 321, 1866, 729, 33783, 300, 264, 441, 43, 44, 7126, 337, 437, 264, 958, 1349, 820, 312, 11, 50576], "temperature": 0.0, "avg_logprob": -0.08776771908714658, "compression_ratio": 1.9008264462809918, "no_speech_prob": 0.005187139380723238}, {"id": 289, "seek": 160082, "start": 1605.7, "end": 1610.74, "text": " one of the things about LLMs is they have to decide, given those probabilities, which actual", "tokens": [50608, 472, 295, 264, 721, 466, 441, 43, 26386, 307, 436, 362, 281, 4536, 11, 2212, 729, 33783, 11, 597, 3539, 50860], "temperature": 0.0, "avg_logprob": -0.08776771908714658, "compression_ratio": 1.9008264462809918, "no_speech_prob": 0.005187139380723238}, {"id": 290, "seek": 160082, "start": 1610.74, "end": 1616.1, "text": " word should be picked, like one thing it could do is say, always pick the most probable word,", "tokens": [50860, 1349, 820, 312, 6183, 11, 411, 472, 551, 309, 727, 360, 307, 584, 11, 1009, 1888, 264, 881, 21759, 1349, 11, 51128], "temperature": 0.0, "avg_logprob": -0.08776771908714658, "compression_ratio": 1.9008264462809918, "no_speech_prob": 0.005187139380723238}, {"id": 291, "seek": 160082, "start": 1616.1, "end": 1622.6599999999999, "text": " another thing it could do is pick those words according to the probabilities as it generated them,", "tokens": [51128, 1071, 551, 309, 727, 360, 307, 1888, 729, 2283, 4650, 281, 264, 33783, 382, 309, 10833, 552, 11, 51456], "temperature": 0.0, "avg_logprob": -0.08776771908714658, "compression_ratio": 1.9008264462809918, "no_speech_prob": 0.005187139380723238}, {"id": 292, "seek": 160082, "start": 1622.6599999999999, "end": 1626.74, "text": " there's this thing that's usually called the temperature parameter, which is an exponential", "tokens": [51456, 456, 311, 341, 551, 300, 311, 2673, 1219, 264, 4292, 13075, 11, 597, 307, 364, 21510, 51660], "temperature": 0.0, "avg_logprob": -0.08776771908714658, "compression_ratio": 1.9008264462809918, "no_speech_prob": 0.005187139380723238}, {"id": 293, "seek": 162674, "start": 1626.74, "end": 1632.1, "text": " distribution thing that basically is the thing that picks, zero temperature means always pick", "tokens": [50364, 7316, 551, 300, 1936, 307, 264, 551, 300, 16137, 11, 4018, 4292, 1355, 1009, 1888, 50632], "temperature": 0.0, "avg_logprob": -0.07294475767347548, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.060986071825027466}, {"id": 294, "seek": 162674, "start": 1632.1, "end": 1637.78, "text": " the most probable word, temperature one means pick the words in the probabilities that the LLM", "tokens": [50632, 264, 881, 21759, 1349, 11, 4292, 472, 1355, 1888, 264, 2283, 294, 264, 33783, 300, 264, 441, 43, 44, 50916], "temperature": 0.0, "avg_logprob": -0.07294475767347548, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.060986071825027466}, {"id": 295, "seek": 162674, "start": 1637.78, "end": 1644.98, "text": " generated itself, as you increase the temperature, it's picking more and more bizarre words, so let's", "tokens": [50916, 10833, 2564, 11, 382, 291, 3488, 264, 4292, 11, 309, 311, 8867, 544, 293, 544, 18265, 2283, 11, 370, 718, 311, 51276], "temperature": 0.0, "avg_logprob": -0.07294475767347548, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.060986071825027466}, {"id": 296, "seek": 162674, "start": 1644.98, "end": 1651.22, "text": " say we go here and let's say we increase the temperature to like 1.3, let's say, and we say", "tokens": [51276, 584, 321, 352, 510, 293, 718, 311, 584, 321, 3488, 264, 4292, 281, 411, 502, 13, 18, 11, 718, 311, 584, 11, 293, 321, 584, 51588], "temperature": 0.0, "avg_logprob": -0.07294475767347548, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.060986071825027466}, {"id": 297, "seek": 165122, "start": 1651.22, "end": 1660.58, "text": " something like, how are you today, and it will generate some, so this is now using, okay, right,", "tokens": [50364, 746, 411, 11, 577, 366, 291, 965, 11, 293, 309, 486, 8460, 512, 11, 370, 341, 307, 586, 1228, 11, 1392, 11, 558, 11, 50832], "temperature": 0.0, "avg_logprob": -0.12840504016516344, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.027932021766901016}, {"id": 298, "seek": 165122, "start": 1660.58, "end": 1666.34, "text": " great, okay, now let's try, let's change that temperature, let's go ahead here and just crank", "tokens": [50832, 869, 11, 1392, 11, 586, 718, 311, 853, 11, 718, 311, 1319, 300, 4292, 11, 718, 311, 352, 2286, 510, 293, 445, 21263, 51120], "temperature": 0.0, "avg_logprob": -0.12840504016516344, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.027932021766901016}, {"id": 299, "seek": 166634, "start": 1666.34, "end": 1685.3799999999999, "text": " up that temperature, and let's try running this again, oh my, so I'm bonkers, oh no,", "tokens": [50364, 493, 300, 4292, 11, 293, 718, 311, 853, 2614, 341, 797, 11, 1954, 452, 11, 370, 286, 478, 4428, 24259, 11, 1954, 572, 11, 51316], "temperature": 0.0, "avg_logprob": -0.21504864602718712, "compression_ratio": 1.3880597014925373, "no_speech_prob": 0.03996560722589493}, {"id": 300, "seek": 166634, "start": 1686.1, "end": 1690.58, "text": " okay, well at least it's stopped, often it never generates a stop token, it just keeps going forever,", "tokens": [51352, 1392, 11, 731, 412, 1935, 309, 311, 5936, 11, 2049, 309, 1128, 23815, 257, 1590, 14862, 11, 309, 445, 5965, 516, 5680, 11, 51576], "temperature": 0.0, "avg_logprob": -0.21504864602718712, "compression_ratio": 1.3880597014925373, "no_speech_prob": 0.03996560722589493}, {"id": 301, "seek": 169058, "start": 1691.3, "end": 1698.34, "text": " the, so okay, so here's an example of a physics question, is there a phase transition as a function", "tokens": [50400, 264, 11, 370, 1392, 11, 370, 510, 311, 364, 1365, 295, 257, 10649, 1168, 11, 307, 456, 257, 5574, 6034, 382, 257, 2445, 50752], "temperature": 0.0, "avg_logprob": -0.10391030191373424, "compression_ratio": 1.725, "no_speech_prob": 0.016072826460003853}, {"id": 302, "seek": 169058, "start": 1698.34, "end": 1704.5, "text": " of temperature in an LLM? The answer is almost certainly yes, probably around for something", "tokens": [50752, 295, 4292, 294, 364, 441, 43, 44, 30, 440, 1867, 307, 1920, 3297, 2086, 11, 1391, 926, 337, 746, 51060], "temperature": 0.0, "avg_logprob": -0.10391030191373424, "compression_ratio": 1.725, "no_speech_prob": 0.016072826460003853}, {"id": 303, "seek": 169058, "start": 1704.5, "end": 1709.54, "text": " like GPT-4, it's almost certainly at a temperature around 1.3 or so, maybe there are actually two", "tokens": [51060, 411, 26039, 51, 12, 19, 11, 309, 311, 1920, 3297, 412, 257, 4292, 926, 502, 13, 18, 420, 370, 11, 1310, 456, 366, 767, 732, 51312], "temperature": 0.0, "avg_logprob": -0.10391030191373424, "compression_ratio": 1.725, "no_speech_prob": 0.016072826460003853}, {"id": 304, "seek": 169058, "start": 1709.54, "end": 1715.06, "text": " transitions that occur, actually there's a, we just had a summer school with people studying", "tokens": [51312, 23767, 300, 5160, 11, 767, 456, 311, 257, 11, 321, 445, 632, 257, 4266, 1395, 365, 561, 7601, 51588], "temperature": 0.0, "avg_logprob": -0.10391030191373424, "compression_ratio": 1.725, "no_speech_prob": 0.016072826460003853}, {"id": 305, "seek": 169058, "start": 1715.06, "end": 1720.1, "text": " all kinds of things, and one person at our summer school studied this question, and I have to admit,", "tokens": [51588, 439, 3685, 295, 721, 11, 293, 472, 954, 412, 527, 4266, 1395, 9454, 341, 1168, 11, 293, 286, 362, 281, 9796, 11, 51840], "temperature": 0.0, "avg_logprob": -0.10391030191373424, "compression_ratio": 1.725, "no_speech_prob": 0.016072826460003853}, {"id": 306, "seek": 172010, "start": 1720.1, "end": 1725.06, "text": " I haven't read the thing they wrote about it, so, but I can show you, this is basically,", "tokens": [50364, 286, 2378, 380, 1401, 264, 551, 436, 4114, 466, 309, 11, 370, 11, 457, 286, 393, 855, 291, 11, 341, 307, 1936, 11, 50612], "temperature": 0.0, "avg_logprob": -0.08901397549376196, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00728064589202404}, {"id": 307, "seek": 172010, "start": 1725.06, "end": 1731.1399999999999, "text": " as a function of temperature, this is essentially an order parameter changing, and in the LLM,", "tokens": [50612, 382, 257, 2445, 295, 4292, 11, 341, 307, 4476, 364, 1668, 13075, 4473, 11, 293, 294, 264, 441, 43, 44, 11, 50916], "temperature": 0.0, "avg_logprob": -0.08901397549376196, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00728064589202404}, {"id": 308, "seek": 172010, "start": 1731.1399999999999, "end": 1735.3, "text": " and this is someplace here, this is an actual, you know, there's some innards of an LLM,", "tokens": [50916, 293, 341, 307, 37126, 510, 11, 341, 307, 364, 3539, 11, 291, 458, 11, 456, 311, 512, 7714, 2287, 295, 364, 441, 43, 44, 11, 51124], "temperature": 0.0, "avg_logprob": -0.08901397549376196, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00728064589202404}, {"id": 309, "seek": 172010, "start": 1736.34, "end": 1742.4199999999998, "text": " and somewhere here, there should be, okay, that's some random pieces of language code,", "tokens": [51176, 293, 4079, 510, 11, 456, 820, 312, 11, 1392, 11, 300, 311, 512, 4974, 3755, 295, 2856, 3089, 11, 51480], "temperature": 0.0, "avg_logprob": -0.08901397549376196, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00728064589202404}, {"id": 310, "seek": 174242, "start": 1742.5, "end": 1750.26, "text": " I think what was done here was to look at the extent to which it maintains kind of", "tokens": [50368, 286, 519, 437, 390, 1096, 510, 390, 281, 574, 412, 264, 8396, 281, 597, 309, 33385, 733, 295, 50756], "temperature": 0.0, "avg_logprob": -0.08961415552830958, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.01003000233322382}, {"id": 311, "seek": 174242, "start": 1752.02, "end": 1757.38, "text": " coherence in the structure of the sentences that it produces and so on, but anyway, the thing that", "tokens": [50844, 26528, 655, 294, 264, 3877, 295, 264, 16579, 300, 309, 14725, 293, 370, 322, 11, 457, 4033, 11, 264, 551, 300, 51112], "temperature": 0.0, "avg_logprob": -0.08961415552830958, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.01003000233322382}, {"id": 312, "seek": 174242, "start": 1757.38, "end": 1763.6200000000001, "text": " I wanted to point out there is this is a very physics-like question, what, how does this work,", "tokens": [51112, 286, 1415, 281, 935, 484, 456, 307, 341, 307, 257, 588, 10649, 12, 4092, 1168, 11, 437, 11, 577, 775, 341, 589, 11, 51424], "temperature": 0.0, "avg_logprob": -0.08961415552830958, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.01003000233322382}, {"id": 313, "seek": 174242, "start": 1763.6200000000001, "end": 1768.5800000000002, "text": " and one of the things we don't have right now is a kind of good qualitative physics,", "tokens": [51424, 293, 472, 295, 264, 721, 321, 500, 380, 362, 558, 586, 307, 257, 733, 295, 665, 31312, 10649, 11, 51672], "temperature": 0.0, "avg_logprob": -0.08961415552830958, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.01003000233322382}, {"id": 314, "seek": 176858, "start": 1768.58, "end": 1774.1, "text": " overall physics-like model for an LLM, like you might say, oh, maybe it's like a spin glass,", "tokens": [50364, 4787, 10649, 12, 4092, 2316, 337, 364, 441, 43, 44, 11, 411, 291, 1062, 584, 11, 1954, 11, 1310, 309, 311, 411, 257, 6060, 4276, 11, 50640], "temperature": 0.0, "avg_logprob": -0.07885459810495377, "compression_ratio": 1.752895752895753, "no_speech_prob": 0.005771846044808626}, {"id": 315, "seek": 176858, "start": 1774.1, "end": 1777.62, "text": " well, it's not really like a spin glass, maybe it's like some other statistical", "tokens": [50640, 731, 11, 309, 311, 406, 534, 411, 257, 6060, 4276, 11, 1310, 309, 311, 411, 512, 661, 22820, 50816], "temperature": 0.0, "avg_logprob": -0.07885459810495377, "compression_ratio": 1.752895752895753, "no_speech_prob": 0.005771846044808626}, {"id": 316, "seek": 176858, "start": 1777.62, "end": 1783.46, "text": " mechanics system, what is it really like? Well, there are a few things that we kind of know", "tokens": [50816, 12939, 1185, 11, 437, 307, 309, 534, 411, 30, 1042, 11, 456, 366, 257, 1326, 721, 300, 321, 733, 295, 458, 51108], "temperature": 0.0, "avg_logprob": -0.07885459810495377, "compression_ratio": 1.752895752895753, "no_speech_prob": 0.005771846044808626}, {"id": 317, "seek": 176858, "start": 1783.46, "end": 1789.46, "text": " about LLM, so I can show you some pictures, let me just show you, just to get a sense of what's", "tokens": [51108, 466, 441, 43, 44, 11, 370, 286, 393, 855, 291, 512, 5242, 11, 718, 385, 445, 855, 291, 11, 445, 281, 483, 257, 2020, 295, 437, 311, 51408], "temperature": 0.0, "avg_logprob": -0.07885459810495377, "compression_ratio": 1.752895752895753, "no_speech_prob": 0.005771846044808626}, {"id": 318, "seek": 176858, "start": 1789.46, "end": 1795.3, "text": " going on inside here, this is kind of a, like, let's say we're trying to learn this function,", "tokens": [51408, 516, 322, 1854, 510, 11, 341, 307, 733, 295, 257, 11, 411, 11, 718, 311, 584, 321, 434, 1382, 281, 1466, 341, 2445, 11, 51700], "temperature": 0.0, "avg_logprob": -0.07885459810495377, "compression_ratio": 1.752895752895753, "no_speech_prob": 0.005771846044808626}, {"id": 319, "seek": 179530, "start": 1795.3, "end": 1800.34, "text": " so we've got x and y are input parameters, and we're trying to learn that function,", "tokens": [50364, 370, 321, 600, 658, 2031, 293, 288, 366, 4846, 9834, 11, 293, 321, 434, 1382, 281, 1466, 300, 2445, 11, 50616], "temperature": 0.0, "avg_logprob": -0.08186269808216255, "compression_ratio": 1.970464135021097, "no_speech_prob": 0.004138686694204807}, {"id": 320, "seek": 179530, "start": 1800.34, "end": 1805.3799999999999, "text": " we're going to have a neural net, there's a neural net, and that neural net is taking those values,", "tokens": [50616, 321, 434, 516, 281, 362, 257, 18161, 2533, 11, 456, 311, 257, 18161, 2533, 11, 293, 300, 18161, 2533, 307, 1940, 729, 4190, 11, 50868], "temperature": 0.0, "avg_logprob": -0.08186269808216255, "compression_ratio": 1.970464135021097, "no_speech_prob": 0.004138686694204807}, {"id": 321, "seek": 179530, "start": 1805.3799999999999, "end": 1811.62, "text": " x and y, and at the top it has some weights, each of the connections has a certain weight,", "tokens": [50868, 2031, 293, 288, 11, 293, 412, 264, 1192, 309, 575, 512, 17443, 11, 1184, 295, 264, 9271, 575, 257, 1629, 3364, 11, 51180], "temperature": 0.0, "avg_logprob": -0.08186269808216255, "compression_ratio": 1.970464135021097, "no_speech_prob": 0.004138686694204807}, {"id": 322, "seek": 179530, "start": 1811.62, "end": 1817.3, "text": " it indicated by the color of that, that connection, and then if we feed in particular values up at", "tokens": [51180, 309, 16176, 538, 264, 2017, 295, 300, 11, 300, 4984, 11, 293, 550, 498, 321, 3154, 294, 1729, 4190, 493, 412, 51464], "temperature": 0.0, "avg_logprob": -0.08186269808216255, "compression_ratio": 1.970464135021097, "no_speech_prob": 0.004138686694204807}, {"id": 323, "seek": 179530, "start": 1817.3, "end": 1821.94, "text": " the top there, this neural net will have been trained, will have been set up with the correct", "tokens": [51464, 264, 1192, 456, 11, 341, 18161, 2533, 486, 362, 668, 8895, 11, 486, 362, 668, 992, 493, 365, 264, 3006, 51696], "temperature": 0.0, "avg_logprob": -0.08186269808216255, "compression_ratio": 1.970464135021097, "no_speech_prob": 0.004138686694204807}, {"id": 324, "seek": 182194, "start": 1821.94, "end": 1827.6200000000001, "text": " weights, so that it will always produce a 0, 1, or minus 1 at the bottom, so for example we can,", "tokens": [50364, 17443, 11, 370, 300, 309, 486, 1009, 5258, 257, 1958, 11, 502, 11, 420, 3175, 502, 412, 264, 2767, 11, 370, 337, 1365, 321, 393, 11, 50648], "temperature": 0.0, "avg_logprob": -0.13491948169210682, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.008605202659964561}, {"id": 325, "seek": 182194, "start": 1828.3400000000001, "end": 1836.02, "text": " let's just, let's say, if we try and use a very, very trivial neural net, trying to learn that", "tokens": [50684, 718, 311, 445, 11, 718, 311, 584, 11, 498, 321, 853, 293, 764, 257, 588, 11, 588, 26703, 18161, 2533, 11, 1382, 281, 1466, 300, 51068], "temperature": 0.0, "avg_logprob": -0.13491948169210682, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.008605202659964561}, {"id": 326, "seek": 182194, "start": 1836.74, "end": 1842.74, "text": " function, the totally trivial neural net will not succeed in producing that function, if we make", "tokens": [51104, 2445, 11, 264, 3879, 26703, 18161, 2533, 486, 406, 7754, 294, 10501, 300, 2445, 11, 498, 321, 652, 51404], "temperature": 0.0, "avg_logprob": -0.13491948169210682, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.008605202659964561}, {"id": 327, "seek": 182194, "start": 1842.74, "end": 1847.54, "text": " the neural net more sophisticated, here are some slightly more sophisticated neural nets, as the", "tokens": [51404, 264, 18161, 2533, 544, 16950, 11, 510, 366, 512, 4748, 544, 16950, 18161, 36170, 11, 382, 264, 51644], "temperature": 0.0, "avg_logprob": -0.13491948169210682, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.008605202659964561}, {"id": 328, "seek": 182194, "start": 1847.54, "end": 1851.46, "text": " neural net gets more sophisticated, it's going to be able to successfully learn that function,", "tokens": [51644, 18161, 2533, 2170, 544, 16950, 11, 309, 311, 516, 281, 312, 1075, 281, 10727, 1466, 300, 2445, 11, 51840], "temperature": 0.0, "avg_logprob": -0.13491948169210682, "compression_ratio": 2.0425531914893615, "no_speech_prob": 0.008605202659964561}, {"id": 329, "seek": 185194, "start": 1851.94, "end": 1856.8200000000002, "text": " how big does a neural net have to be to learn what level of function, not really known, I mean", "tokens": [50364, 577, 955, 775, 257, 18161, 2533, 362, 281, 312, 281, 1466, 437, 1496, 295, 2445, 11, 406, 534, 2570, 11, 286, 914, 50608], "temperature": 0.0, "avg_logprob": -0.11171861617795882, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.000997984199784696}, {"id": 330, "seek": 185194, "start": 1856.8200000000002, "end": 1860.74, "text": " there are theorems that say in principle you can do things with neural nets of certain sizes,", "tokens": [50608, 456, 366, 10299, 2592, 300, 584, 294, 8665, 291, 393, 360, 721, 365, 18161, 36170, 295, 1629, 11602, 11, 50804], "temperature": 0.0, "avg_logprob": -0.11171861617795882, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.000997984199784696}, {"id": 331, "seek": 185194, "start": 1860.74, "end": 1868.26, "text": " but the practical question we don't know, that's another kind of thing, so now you know in terms of", "tokens": [50804, 457, 264, 8496, 1168, 321, 500, 380, 458, 11, 300, 311, 1071, 733, 295, 551, 11, 370, 586, 291, 458, 294, 2115, 295, 51180], "temperature": 0.0, "avg_logprob": -0.11171861617795882, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.000997984199784696}, {"id": 332, "seek": 185194, "start": 1868.26, "end": 1875.8600000000001, "text": " what, let's see, the, I mean you can do these experiments by the way, the things I've written", "tokens": [51180, 437, 11, 718, 311, 536, 11, 264, 11, 286, 914, 291, 393, 360, 613, 12050, 538, 264, 636, 11, 264, 721, 286, 600, 3720, 51560], "temperature": 0.0, "avg_logprob": -0.11171861617795882, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.000997984199784696}, {"id": 333, "seek": 185194, "start": 1875.8600000000001, "end": 1880.8200000000002, "text": " about, I wrote some kind of whole explainer of chat GPT, which was one of the things that I've", "tokens": [51560, 466, 11, 286, 4114, 512, 733, 295, 1379, 2903, 260, 295, 5081, 26039, 51, 11, 597, 390, 472, 295, 264, 721, 300, 286, 600, 51808], "temperature": 0.0, "avg_logprob": -0.11171861617795882, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.000997984199784696}, {"id": 334, "seek": 188082, "start": 1881.1399999999999, "end": 1885.9399999999998, "text": " written fastest in my life, and it's the thing that seems to be read more, at least per unit of", "tokens": [50380, 3720, 14573, 294, 452, 993, 11, 293, 309, 311, 264, 551, 300, 2544, 281, 312, 1401, 544, 11, 412, 1935, 680, 4985, 295, 50620], "temperature": 0.0, "avg_logprob": -0.09022436141967774, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.008590658195316792}, {"id": 335, "seek": 188082, "start": 1885.9399999999998, "end": 1889.78, "text": " time spent on writing it, it seems to have been read more than anything else I've written, which", "tokens": [50620, 565, 4418, 322, 3579, 309, 11, 309, 2544, 281, 362, 668, 1401, 544, 813, 1340, 1646, 286, 600, 3720, 11, 597, 50812], "temperature": 0.0, "avg_logprob": -0.09022436141967774, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.008590658195316792}, {"id": 336, "seek": 188082, "start": 1889.78, "end": 1895.06, "text": " to me is a little bit disappointing actually, but that's a different story, but anyway, so", "tokens": [50812, 281, 385, 307, 257, 707, 857, 25054, 767, 11, 457, 300, 311, 257, 819, 1657, 11, 457, 4033, 11, 370, 51076], "temperature": 0.0, "avg_logprob": -0.09022436141967774, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.008590658195316792}, {"id": 337, "seek": 188082, "start": 1896.34, "end": 1903.3, "text": " those are some things about the innards of chat GPT, and those are some, but we can start looking", "tokens": [51140, 729, 366, 512, 721, 466, 264, 7714, 2287, 295, 5081, 26039, 51, 11, 293, 729, 366, 512, 11, 457, 321, 393, 722, 1237, 51488], "temperature": 0.0, "avg_logprob": -0.09022436141967774, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.008590658195316792}, {"id": 338, "seek": 188082, "start": 1903.3, "end": 1908.1799999999998, "text": " at kind of what's actually going on inside the system, and it's kind of complicated, and you", "tokens": [51488, 412, 733, 295, 437, 311, 767, 516, 322, 1854, 264, 1185, 11, 293, 309, 311, 733, 295, 6179, 11, 293, 291, 51732], "temperature": 0.0, "avg_logprob": -0.09022436141967774, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.008590658195316792}, {"id": 339, "seek": 190818, "start": 1908.18, "end": 1914.8200000000002, "text": " start seeing, you know, this is a condensation of the kind of innards of the brain of actually this", "tokens": [50364, 722, 2577, 11, 291, 458, 11, 341, 307, 257, 2224, 35292, 295, 264, 733, 295, 7714, 2287, 295, 264, 3567, 295, 767, 341, 50696], "temperature": 0.0, "avg_logprob": -0.08059607611762153, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.004221820272505283}, {"id": 340, "seek": 190818, "start": 1914.8200000000002, "end": 1922.74, "text": " is GPT2, kind of a junior version of chat GPT, and this is kind of, in a sense this is taking", "tokens": [50696, 307, 26039, 51, 17, 11, 733, 295, 257, 16195, 3037, 295, 5081, 26039, 51, 11, 293, 341, 307, 733, 295, 11, 294, 257, 2020, 341, 307, 1940, 51092], "temperature": 0.0, "avg_logprob": -0.08059607611762153, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.004221820272505283}, {"id": 341, "seek": 190818, "start": 1922.74, "end": 1928.3400000000001, "text": " human knowledge and human linguistics, and crushing it down to something that's represented", "tokens": [51092, 1952, 3601, 293, 1952, 21766, 6006, 11, 293, 31317, 309, 760, 281, 746, 300, 311, 10379, 51372], "temperature": 0.0, "avg_logprob": -0.08059607611762153, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.004221820272505283}, {"id": 342, "seek": 190818, "start": 1928.3400000000001, "end": 1932.8200000000002, "text": " in terms of arrays of numbers, and this is one of the pieces of what you see when that's done,", "tokens": [51372, 294, 2115, 295, 41011, 295, 3547, 11, 293, 341, 307, 472, 295, 264, 3755, 295, 437, 291, 536, 562, 300, 311, 1096, 11, 51596], "temperature": 0.0, "avg_logprob": -0.08059607611762153, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.004221820272505283}, {"id": 343, "seek": 193282, "start": 1932.82, "end": 1939.62, "text": " I mean the full chat GPT has like 175 billion weights, this is just showing a little piece of", "tokens": [50364, 286, 914, 264, 1577, 5081, 26039, 51, 575, 411, 41165, 5218, 17443, 11, 341, 307, 445, 4099, 257, 707, 2522, 295, 50704], "temperature": 0.0, "avg_logprob": -0.10133739312489827, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.004631559830158949}, {"id": 344, "seek": 193282, "start": 1939.62, "end": 1948.4199999999998, "text": " that story. Now, okay, what can we say about what it actually does? Well, there's several", "tokens": [50704, 300, 1657, 13, 823, 11, 1392, 11, 437, 393, 321, 584, 466, 437, 309, 767, 775, 30, 1042, 11, 456, 311, 2940, 51144], "temperature": 0.0, "avg_logprob": -0.10133739312489827, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.004631559830158949}, {"id": 345, "seek": 193282, "start": 1948.4199999999998, "end": 1953.54, "text": " different things, so one thing that's important is this concept of embeddings, we can take", "tokens": [51144, 819, 721, 11, 370, 472, 551, 300, 311, 1021, 307, 341, 3410, 295, 12240, 29432, 11, 321, 393, 747, 51400], "temperature": 0.0, "avg_logprob": -0.10133739312489827, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.004631559830158949}, {"id": 346, "seek": 193282, "start": 1954.26, "end": 1960.1, "text": " kind of, you know, words in a language, sentences, things like that, the big sort of idea of neural", "tokens": [51436, 733, 295, 11, 291, 458, 11, 2283, 294, 257, 2856, 11, 16579, 11, 721, 411, 300, 11, 264, 955, 1333, 295, 1558, 295, 18161, 51728], "temperature": 0.0, "avg_logprob": -0.10133739312489827, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.004631559830158949}, {"id": 347, "seek": 196010, "start": 1960.1, "end": 1963.86, "text": " nets in some sense, and it's a very old idea, dates all the way back to when neural nets were", "tokens": [50364, 36170, 294, 512, 2020, 11, 293, 309, 311, 257, 588, 1331, 1558, 11, 11691, 439, 264, 636, 646, 281, 562, 18161, 36170, 645, 50552], "temperature": 0.0, "avg_logprob": -0.05986551975640725, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0322163887321949}, {"id": 348, "seek": 196010, "start": 1963.86, "end": 1970.98, "text": " invented in the 1940s, is don't just use digital information, use arrays of real numbers to represent", "tokens": [50552, 14479, 294, 264, 24158, 82, 11, 307, 500, 380, 445, 764, 4562, 1589, 11, 764, 41011, 295, 957, 3547, 281, 2906, 50908], "temperature": 0.0, "avg_logprob": -0.05986551975640725, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0322163887321949}, {"id": 349, "seek": 196010, "start": 1970.98, "end": 1975.62, "text": " things. It's not clear that you actually need to do that, you probably don't, I don't think you need", "tokens": [50908, 721, 13, 467, 311, 406, 1850, 300, 291, 767, 643, 281, 360, 300, 11, 291, 1391, 500, 380, 11, 286, 500, 380, 519, 291, 643, 51140], "temperature": 0.0, "avg_logprob": -0.05986551975640725, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0322163887321949}, {"id": 350, "seek": 196010, "start": 1975.62, "end": 1981.3799999999999, "text": " to do it for physics, for example, but the way that neural nets are built, they are take everything,", "tokens": [51140, 281, 360, 309, 337, 10649, 11, 337, 1365, 11, 457, 264, 636, 300, 18161, 36170, 366, 3094, 11, 436, 366, 747, 1203, 11, 51428], "temperature": 0.0, "avg_logprob": -0.05986551975640725, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0322163887321949}, {"id": 351, "seek": 196010, "start": 1981.3799999999999, "end": 1985.86, "text": " whether it's an image, whether it's text, whatever else, and grind it into arrays of real numbers,", "tokens": [51428, 1968, 309, 311, 364, 3256, 11, 1968, 309, 311, 2487, 11, 2035, 1646, 11, 293, 16700, 309, 666, 41011, 295, 957, 3547, 11, 51652], "temperature": 0.0, "avg_logprob": -0.05986551975640725, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.0322163887321949}, {"id": 352, "seek": 198586, "start": 1985.9399999999998, "end": 1993.3, "text": " and then you can take those, then what you're doing is representing everything, you know,", "tokens": [50368, 293, 550, 291, 393, 747, 729, 11, 550, 437, 291, 434, 884, 307, 13460, 1203, 11, 291, 458, 11, 50736], "temperature": 0.0, "avg_logprob": -0.07639090220133464, "compression_ratio": 1.9915611814345993, "no_speech_prob": 0.0017391429282724857}, {"id": 353, "seek": 198586, "start": 1993.3, "end": 1998.1799999999998, "text": " just as in standard digital computational stuff, you're representing things as bits in neural", "tokens": [50736, 445, 382, 294, 3832, 4562, 28270, 1507, 11, 291, 434, 13460, 721, 382, 9239, 294, 18161, 50980], "temperature": 0.0, "avg_logprob": -0.07639090220133464, "compression_ratio": 1.9915611814345993, "no_speech_prob": 0.0017391429282724857}, {"id": 354, "seek": 198586, "start": 1998.1799999999998, "end": 2003.2199999999998, "text": " nets, you're representing everything in terms of arrays of real numbers, and so for example, any", "tokens": [50980, 36170, 11, 291, 434, 13460, 1203, 294, 2115, 295, 41011, 295, 957, 3547, 11, 293, 370, 337, 1365, 11, 604, 51232], "temperature": 0.0, "avg_logprob": -0.07639090220133464, "compression_ratio": 1.9915611814345993, "no_speech_prob": 0.0017391429282724857}, {"id": 355, "seek": 198586, "start": 2003.2199999999998, "end": 2008.5, "text": " old sentence, any old piece of text is ultimately represented as an array of real numbers, and", "tokens": [51232, 1331, 8174, 11, 604, 1331, 2522, 295, 2487, 307, 6284, 10379, 382, 364, 10225, 295, 957, 3547, 11, 293, 51496], "temperature": 0.0, "avg_logprob": -0.07639090220133464, "compression_ratio": 1.9915611814345993, "no_speech_prob": 0.0017391429282724857}, {"id": 356, "seek": 198586, "start": 2008.5, "end": 2013.86, "text": " that array of real numbers we can think of as being some sort of feature vector that represents,", "tokens": [51496, 300, 10225, 295, 957, 3547, 321, 393, 519, 295, 382, 885, 512, 1333, 295, 4111, 8062, 300, 8855, 11, 51764], "temperature": 0.0, "avg_logprob": -0.07639090220133464, "compression_ratio": 1.9915611814345993, "no_speech_prob": 0.0017391429282724857}, {"id": 357, "seek": 201386, "start": 2013.86, "end": 2020.02, "text": " in some sense, some digest of the meaning of the thing that we specified. So you can start", "tokens": [50364, 294, 512, 2020, 11, 512, 13884, 295, 264, 3620, 295, 264, 551, 300, 321, 22206, 13, 407, 291, 393, 722, 50672], "temperature": 0.0, "avg_logprob": -0.08902292835469149, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0021880208514630795}, {"id": 358, "seek": 201386, "start": 2020.02, "end": 2026.5, "text": " asking in meaning space, in that space of embeddings, what can we see about what happens in that space,", "tokens": [50672, 3365, 294, 3620, 1901, 11, 294, 300, 1901, 295, 12240, 29432, 11, 437, 393, 321, 536, 466, 437, 2314, 294, 300, 1901, 11, 50996], "temperature": 0.0, "avg_logprob": -0.08902292835469149, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0021880208514630795}, {"id": 359, "seek": 201386, "start": 2026.5, "end": 2033.2199999999998, "text": " and for example, let's see, we can ask questions like, how linear is that space? You know, for", "tokens": [50996, 293, 337, 1365, 11, 718, 311, 536, 11, 321, 393, 1029, 1651, 411, 11, 577, 8213, 307, 300, 1901, 30, 509, 458, 11, 337, 51332], "temperature": 0.0, "avg_logprob": -0.08902292835469149, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0021880208514630795}, {"id": 360, "seek": 201386, "start": 2033.2199999999998, "end": 2037.9399999999998, "text": " example, if we do parallel transport in that space, if we look at the curvature of that space,", "tokens": [51332, 1365, 11, 498, 321, 360, 8952, 5495, 294, 300, 1901, 11, 498, 321, 574, 412, 264, 37638, 295, 300, 1901, 11, 51568], "temperature": 0.0, "avg_logprob": -0.08902292835469149, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0021880208514630795}, {"id": 361, "seek": 203794, "start": 2038.02, "end": 2044.3400000000001, "text": " we're looking at, you know, this is to that, as that is to that, that's kind of the analog for", "tokens": [50368, 321, 434, 1237, 412, 11, 291, 458, 11, 341, 307, 281, 300, 11, 382, 300, 307, 281, 300, 11, 300, 311, 733, 295, 264, 16660, 337, 50684], "temperature": 0.0, "avg_logprob": -0.1158096194267273, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.018191078677773476}, {"id": 362, "seek": 203794, "start": 2044.3400000000001, "end": 2051.38, "text": " linguistics for sort of the structure of meaning of a question that you might ask in", "tokens": [50684, 21766, 6006, 337, 1333, 295, 264, 3877, 295, 3620, 295, 257, 1168, 300, 291, 1062, 1029, 294, 51036], "temperature": 0.0, "avg_logprob": -0.1158096194267273, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.018191078677773476}, {"id": 363, "seek": 203794, "start": 2052.5, "end": 2057.2200000000003, "text": " physics of space time or something, and you can ask about these questions about curvature in that", "tokens": [51092, 10649, 295, 1901, 565, 420, 746, 11, 293, 291, 393, 1029, 466, 613, 1651, 466, 37638, 294, 300, 51328], "temperature": 0.0, "avg_logprob": -0.1158096194267273, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.018191078677773476}, {"id": 364, "seek": 203794, "start": 2057.2200000000003, "end": 2064.02, "text": " space, I don't know all the answers to this, you can also ask things like, well, what is the trajectory", "tokens": [51328, 1901, 11, 286, 500, 380, 458, 439, 264, 6338, 281, 341, 11, 291, 393, 611, 1029, 721, 411, 11, 731, 11, 437, 307, 264, 21512, 51668], "temperature": 0.0, "avg_logprob": -0.1158096194267273, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.018191078677773476}, {"id": 365, "seek": 206402, "start": 2064.02, "end": 2071.22, "text": " that's carved out in that space? So is there, for example, a semantic law of motion? If you start", "tokens": [50364, 300, 311, 28613, 484, 294, 300, 1901, 30, 407, 307, 456, 11, 337, 1365, 11, 257, 47982, 2101, 295, 5394, 30, 759, 291, 722, 50724], "temperature": 0.0, "avg_logprob": -0.12492508790930923, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.01970716007053852}, {"id": 366, "seek": 206402, "start": 2071.22, "end": 2076.82, "text": " in this particular way, is it the case that in this meaning space that you end up always tracing", "tokens": [50724, 294, 341, 1729, 636, 11, 307, 309, 264, 1389, 300, 294, 341, 3620, 1901, 300, 291, 917, 493, 1009, 25262, 51004], "temperature": 0.0, "avg_logprob": -0.12492508790930923, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.01970716007053852}, {"id": 367, "seek": 206402, "start": 2076.82, "end": 2081.54, "text": " through in a particular way? And one thing that seems to be the case, the space to some experience", "tokens": [51004, 807, 294, 257, 1729, 636, 30, 400, 472, 551, 300, 2544, 281, 312, 264, 1389, 11, 264, 1901, 281, 512, 1752, 51240], "temperature": 0.0, "avg_logprob": -0.12492508790930923, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.01970716007053852}, {"id": 368, "seek": 206402, "start": 2081.54, "end": 2087.7, "text": " we just did a couple weeks ago, is that the things are much more organized. So if you look at,", "tokens": [51240, 321, 445, 630, 257, 1916, 3259, 2057, 11, 307, 300, 264, 721, 366, 709, 544, 9983, 13, 407, 498, 291, 574, 412, 11, 51548], "temperature": 0.0, "avg_logprob": -0.12492508790930923, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.01970716007053852}, {"id": 369, "seek": 208770, "start": 2088.5, "end": 2090.98, "text": " oh, this is kind of, sorry, let me just show you,", "tokens": [50404, 1954, 11, 341, 307, 733, 295, 11, 2597, 11, 718, 385, 445, 855, 291, 11, 50528], "temperature": 0.0, "avg_logprob": -0.1180472293142545, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.01832514815032482}, {"id": 370, "seek": 208770, "start": 2092.8999999999996, "end": 2099.22, "text": " much of the time these trajectories aren't, in something like GPT2, the trajectories are", "tokens": [50624, 709, 295, 264, 565, 613, 18257, 2083, 3212, 380, 11, 294, 746, 411, 26039, 51, 17, 11, 264, 18257, 2083, 366, 50940], "temperature": 0.0, "avg_logprob": -0.1180472293142545, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.01832514815032482}, {"id": 371, "seek": 208770, "start": 2099.22, "end": 2103.9399999999996, "text": " quite disorganized. It seems that as you get to things like GPT4, the trajectories look a lot", "tokens": [50940, 1596, 717, 12372, 1602, 13, 467, 2544, 300, 382, 291, 483, 281, 721, 411, 26039, 51, 19, 11, 264, 18257, 2083, 574, 257, 688, 51176], "temperature": 0.0, "avg_logprob": -0.1180472293142545, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.01832514815032482}, {"id": 372, "seek": 208770, "start": 2103.9399999999996, "end": 2109.2999999999997, "text": " more organized. It's much more believable that there are semantic laws of motion, so to speak,", "tokens": [51176, 544, 9983, 13, 467, 311, 709, 544, 1351, 17915, 300, 456, 366, 47982, 6064, 295, 5394, 11, 370, 281, 1710, 11, 51444], "temperature": 0.0, "avg_logprob": -0.1180472293142545, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.01832514815032482}, {"id": 373, "seek": 208770, "start": 2109.2999999999997, "end": 2114.98, "text": " laws of motion in meaning space in GPT4. By the way, it's worth realizing that there's sort of a", "tokens": [51444, 6064, 295, 5394, 294, 3620, 1901, 294, 26039, 51, 19, 13, 3146, 264, 636, 11, 309, 311, 3163, 16734, 300, 456, 311, 1333, 295, 257, 51728], "temperature": 0.0, "avg_logprob": -0.1180472293142545, "compression_ratio": 1.7306122448979593, "no_speech_prob": 0.01832514815032482}, {"id": 374, "seek": 211498, "start": 2115.06, "end": 2121.94, "text": " quantum story to the whole thing because the whole thing is, it isn't just picking one trajectory,", "tokens": [50368, 13018, 1657, 281, 264, 1379, 551, 570, 264, 1379, 551, 307, 11, 309, 1943, 380, 445, 8867, 472, 21512, 11, 50712], "temperature": 0.0, "avg_logprob": -0.07805125826881046, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.008289460092782974}, {"id": 375, "seek": 211498, "start": 2121.94, "end": 2127.22, "text": " it's picking a whole bunch of different paths. One difference from, I mean, this is a quite", "tokens": [50712, 309, 311, 8867, 257, 1379, 3840, 295, 819, 14518, 13, 1485, 2649, 490, 11, 286, 914, 11, 341, 307, 257, 1596, 50976], "temperature": 0.0, "avg_logprob": -0.07805125826881046, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.008289460092782974}, {"id": 376, "seek": 211498, "start": 2127.22, "end": 2131.54, "text": " different topic, but in the whole fundamental physics project that we've been doing for the", "tokens": [50976, 819, 4829, 11, 457, 294, 264, 1379, 8088, 10649, 1716, 300, 321, 600, 668, 884, 337, 264, 51192], "temperature": 0.0, "avg_logprob": -0.07805125826881046, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.008289460092782974}, {"id": 377, "seek": 211498, "start": 2131.54, "end": 2136.1, "text": " last few years where it seems like we really actually do finally understand how quantum mechanics", "tokens": [51192, 1036, 1326, 924, 689, 309, 2544, 411, 321, 534, 767, 360, 2721, 1223, 577, 13018, 12939, 51420], "temperature": 0.0, "avg_logprob": -0.07805125826881046, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.008289460092782974}, {"id": 378, "seek": 211498, "start": 2136.1, "end": 2141.86, "text": " works, it becomes very important in that case that there is merging of different paths of history,", "tokens": [51420, 1985, 11, 309, 3643, 588, 1021, 294, 300, 1389, 300, 456, 307, 44559, 295, 819, 14518, 295, 2503, 11, 51708], "temperature": 0.0, "avg_logprob": -0.07805125826881046, "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.008289460092782974}, {"id": 379, "seek": 214186, "start": 2141.86, "end": 2149.2200000000003, "text": " as well as just branching the paths of history. In the current versions of these LLMs, there's", "tokens": [50364, 382, 731, 382, 445, 9819, 278, 264, 14518, 295, 2503, 13, 682, 264, 2190, 9606, 295, 613, 441, 43, 26386, 11, 456, 311, 50732], "temperature": 0.0, "avg_logprob": -0.05883242958470395, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.002764400327578187}, {"id": 380, "seek": 214186, "start": 2149.2200000000003, "end": 2153.7000000000003, "text": " pretty much just branching the paths of history, but you kind of get this quantum-like phenomenon", "tokens": [50732, 1238, 709, 445, 9819, 278, 264, 14518, 295, 2503, 11, 457, 291, 733, 295, 483, 341, 13018, 12, 4092, 14029, 50956], "temperature": 0.0, "avg_logprob": -0.05883242958470395, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.002764400327578187}, {"id": 381, "seek": 214186, "start": 2153.7000000000003, "end": 2158.82, "text": " going on of all these different possible things the LLM might say that aggregate up to different", "tokens": [50956, 516, 322, 295, 439, 613, 819, 1944, 721, 264, 441, 43, 44, 1062, 584, 300, 26118, 493, 281, 819, 51212], "temperature": 0.0, "avg_logprob": -0.05883242958470395, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.002764400327578187}, {"id": 382, "seek": 214186, "start": 2158.82, "end": 2164.02, "text": " kinds of things. By the way, if you're, well, there's all kinds of interesting things to say", "tokens": [51212, 3685, 295, 721, 13, 3146, 264, 636, 11, 498, 291, 434, 11, 731, 11, 456, 311, 439, 3685, 295, 1880, 721, 281, 584, 51472], "temperature": 0.0, "avg_logprob": -0.05883242958470395, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.002764400327578187}, {"id": 383, "seek": 216402, "start": 2164.02, "end": 2174.02, "text": " about LLMs as observers in thinking about physics, but maybe one thing to talk about is just what is,", "tokens": [50364, 466, 441, 43, 26386, 382, 48090, 294, 1953, 466, 10649, 11, 457, 1310, 472, 551, 281, 751, 466, 307, 445, 437, 307, 11, 50864], "temperature": 0.0, "avg_logprob": -0.13153879609826494, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.07195289433002472}, {"id": 384, "seek": 216402, "start": 2174.82, "end": 2182.5, "text": " this is sort of pictures of what meaning space looks like and so on, and questions like if you", "tokens": [50904, 341, 307, 1333, 295, 5242, 295, 437, 3620, 1901, 1542, 411, 293, 370, 322, 11, 293, 1651, 411, 498, 291, 51288], "temperature": 0.0, "avg_logprob": -0.13153879609826494, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.07195289433002472}, {"id": 385, "seek": 216402, "start": 2182.5, "end": 2189.14, "text": " have a word and it has many different sort of partially, so this is the word crane, I think,", "tokens": [51288, 362, 257, 1349, 293, 309, 575, 867, 819, 1333, 295, 18886, 11, 370, 341, 307, 264, 1349, 36345, 11, 286, 519, 11, 51620], "temperature": 0.0, "avg_logprob": -0.13153879609826494, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.07195289433002472}, {"id": 386, "seek": 218914, "start": 2189.14, "end": 2195.62, "text": " and this is, in meaning space, this is where different sentences that mention cranes show up,", "tokens": [50364, 293, 341, 307, 11, 294, 3620, 1901, 11, 341, 307, 689, 819, 16579, 300, 2152, 941, 12779, 855, 493, 11, 50688], "temperature": 0.0, "avg_logprob": -0.08395007532885951, "compression_ratio": 1.9620253164556962, "no_speech_prob": 0.0067141056060791016}, {"id": 387, "seek": 218914, "start": 2195.62, "end": 2199.46, "text": " and so I think the ones at the top are cranes as a bird and the ones at the bottom", "tokens": [50688, 293, 370, 286, 519, 264, 2306, 412, 264, 1192, 366, 941, 12779, 382, 257, 5255, 293, 264, 2306, 412, 264, 2767, 50880], "temperature": 0.0, "avg_logprob": -0.08395007532885951, "compression_ratio": 1.9620253164556962, "no_speech_prob": 0.0067141056060791016}, {"id": 388, "seek": 218914, "start": 2199.46, "end": 2204.42, "text": " are cranes as construction equipment type thing, and you kind of see that separating, so you can", "tokens": [50880, 366, 941, 12779, 382, 6435, 5927, 2010, 551, 11, 293, 291, 733, 295, 536, 300, 29279, 11, 370, 291, 393, 51128], "temperature": 0.0, "avg_logprob": -0.08395007532885951, "compression_ratio": 1.9620253164556962, "no_speech_prob": 0.0067141056060791016}, {"id": 389, "seek": 218914, "start": 2204.42, "end": 2209.22, "text": " kind of get, again, it's this kind of rather physics-like thing of kind of looking at this", "tokens": [51128, 733, 295, 483, 11, 797, 11, 309, 311, 341, 733, 295, 2831, 10649, 12, 4092, 551, 295, 733, 295, 1237, 412, 341, 51368], "temperature": 0.0, "avg_logprob": -0.08395007532885951, "compression_ratio": 1.9620253164556962, "no_speech_prob": 0.0067141056060791016}, {"id": 390, "seek": 218914, "start": 2209.22, "end": 2216.2599999999998, "text": " meaning space, and by the way, you can sort of ask things about the structure of that meaning space,", "tokens": [51368, 3620, 1901, 11, 293, 538, 264, 636, 11, 291, 393, 1333, 295, 1029, 721, 466, 264, 3877, 295, 300, 3620, 1901, 11, 51720], "temperature": 0.0, "avg_logprob": -0.08395007532885951, "compression_ratio": 1.9620253164556962, "no_speech_prob": 0.0067141056060791016}, {"id": 391, "seek": 221626, "start": 2216.26, "end": 2220.9, "text": " and for instance, let me see if I can show you a picture, let me see here,", "tokens": [50364, 293, 337, 5197, 11, 718, 385, 536, 498, 286, 393, 855, 291, 257, 3036, 11, 718, 385, 536, 510, 11, 50596], "temperature": 0.0, "avg_logprob": -0.09491735635344516, "compression_ratio": 1.8763440860215055, "no_speech_prob": 0.002240573288872838}, {"id": 392, "seek": 221626, "start": 2223.6200000000003, "end": 2230.6600000000003, "text": " maybe, okay, so in meaning space, you can ask something like, you can also do that with images,", "tokens": [50732, 1310, 11, 1392, 11, 370, 294, 3620, 1901, 11, 291, 393, 1029, 746, 411, 11, 291, 393, 611, 360, 300, 365, 5267, 11, 51084], "temperature": 0.0, "avg_logprob": -0.09491735635344516, "compression_ratio": 1.8763440860215055, "no_speech_prob": 0.002240573288872838}, {"id": 393, "seek": 221626, "start": 2230.6600000000003, "end": 2235.6200000000003, "text": " and so you can ask, for example, we can go in meaning space, we can go from a dog image", "tokens": [51084, 293, 370, 291, 393, 1029, 11, 337, 1365, 11, 321, 393, 352, 294, 3620, 1901, 11, 321, 393, 352, 490, 257, 3000, 3256, 51332], "temperature": 0.0, "avg_logprob": -0.09491735635344516, "compression_ratio": 1.8763440860215055, "no_speech_prob": 0.002240573288872838}, {"id": 394, "seek": 221626, "start": 2235.6200000000003, "end": 2242.1800000000003, "text": " to a cat image on the line in meaning space between a dog and a cat, and we could actually", "tokens": [51332, 281, 257, 3857, 3256, 322, 264, 1622, 294, 3620, 1901, 1296, 257, 3000, 293, 257, 3857, 11, 293, 321, 727, 767, 51660], "temperature": 0.0, "avg_logprob": -0.09491735635344516, "compression_ratio": 1.8763440860215055, "no_speech_prob": 0.002240573288872838}, {"id": 395, "seek": 224218, "start": 2242.2599999999998, "end": 2247.7799999999997, "text": " keep going from the cat out further, we can extend that line further out in meaning space,", "tokens": [50368, 1066, 516, 490, 264, 3857, 484, 3052, 11, 321, 393, 10101, 300, 1622, 3052, 484, 294, 3620, 1901, 11, 50644], "temperature": 0.0, "avg_logprob": -0.09160497270781419, "compression_ratio": 1.814516129032258, "no_speech_prob": 0.03722529858350754}, {"id": 396, "seek": 224218, "start": 2247.7799999999997, "end": 2251.46, "text": " and we get all these kinds of weird things happening, or we can go from a plane to a cat,", "tokens": [50644, 293, 321, 483, 439, 613, 3685, 295, 3657, 721, 2737, 11, 420, 321, 393, 352, 490, 257, 5720, 281, 257, 3857, 11, 50828], "temperature": 0.0, "avg_logprob": -0.09160497270781419, "compression_ratio": 1.814516129032258, "no_speech_prob": 0.03722529858350754}, {"id": 397, "seek": 224218, "start": 2251.46, "end": 2256.02, "text": " and we have something very strange in the middle of those two things, or we can just go out,", "tokens": [50828, 293, 321, 362, 746, 588, 5861, 294, 264, 2808, 295, 729, 732, 721, 11, 420, 321, 393, 445, 352, 484, 11, 51056], "temperature": 0.0, "avg_logprob": -0.09160497270781419, "compression_ratio": 1.814516129032258, "no_speech_prob": 0.03722529858350754}, {"id": 398, "seek": 224218, "start": 2256.02, "end": 2261.8599999999997, "text": " this is what I was calling cat island, this is in the middle, so this was a generative AI,", "tokens": [51056, 341, 307, 437, 286, 390, 5141, 3857, 6077, 11, 341, 307, 294, 264, 2808, 11, 370, 341, 390, 257, 1337, 1166, 7318, 11, 51348], "temperature": 0.0, "avg_logprob": -0.09160497270781419, "compression_ratio": 1.814516129032258, "no_speech_prob": 0.03722529858350754}, {"id": 399, "seek": 224218, "start": 2262.58, "end": 2268.8199999999997, "text": " not specifically an LLM, this is an image generation AI, which uses somewhat similar,", "tokens": [51384, 406, 4682, 364, 441, 43, 44, 11, 341, 307, 364, 3256, 5125, 7318, 11, 597, 4960, 8344, 2531, 11, 51696], "temperature": 0.0, "avg_logprob": -0.09160497270781419, "compression_ratio": 1.814516129032258, "no_speech_prob": 0.03722529858350754}, {"id": 400, "seek": 226882, "start": 2268.82, "end": 2276.42, "text": " but not precisely the same technology inside, and I asked it in the middle to make a picture of a", "tokens": [50364, 457, 406, 13402, 264, 912, 2899, 1854, 11, 293, 286, 2351, 309, 294, 264, 2808, 281, 652, 257, 3036, 295, 257, 50744], "temperature": 0.0, "avg_logprob": -0.08126366933186849, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.021768001839518547}, {"id": 401, "seek": 226882, "start": 2276.42, "end": 2281.38, "text": " cat in a party hat, and then as you go outwards in meaning space, you see this kind of island", "tokens": [50744, 3857, 294, 257, 3595, 2385, 11, 293, 550, 382, 291, 352, 484, 2015, 294, 3620, 1901, 11, 291, 536, 341, 733, 295, 6077, 50992], "temperature": 0.0, "avg_logprob": -0.08126366933186849, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.021768001839518547}, {"id": 402, "seek": 226882, "start": 2281.38, "end": 2286.1000000000004, "text": " of where you can see sort of identifiable cat things going on, and then further away,", "tokens": [50992, 295, 689, 291, 393, 536, 1333, 295, 2473, 30876, 3857, 721, 516, 322, 11, 293, 550, 3052, 1314, 11, 51228], "temperature": 0.0, "avg_logprob": -0.08126366933186849, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.021768001839518547}, {"id": 403, "seek": 226882, "start": 2286.1000000000004, "end": 2290.34, "text": " it becomes more and more bizarre, and by the way, you can ask questions like, well, what's actually", "tokens": [51228, 309, 3643, 544, 293, 544, 18265, 11, 293, 538, 264, 636, 11, 291, 393, 1029, 1651, 411, 11, 731, 11, 437, 311, 767, 51440], "temperature": 0.0, "avg_logprob": -0.08126366933186849, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.021768001839518547}, {"id": 404, "seek": 226882, "start": 2290.34, "end": 2296.1800000000003, "text": " out there in sort of arbitrary meaning space, and I think, well, you can look at other cat islands", "tokens": [51440, 484, 456, 294, 1333, 295, 23211, 3620, 1901, 11, 293, 286, 519, 11, 731, 11, 291, 393, 574, 412, 661, 3857, 17402, 51732], "temperature": 0.0, "avg_logprob": -0.08126366933186849, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.021768001839518547}, {"id": 405, "seek": 229618, "start": 2296.2599999999998, "end": 2301.94, "text": " here, this thing is actually in 2000 dimensional space, and these are planes in 2000 dimensional", "tokens": [50368, 510, 11, 341, 551, 307, 767, 294, 8132, 18795, 1901, 11, 293, 613, 366, 14952, 294, 8132, 18795, 50652], "temperature": 0.0, "avg_logprob": -0.09398518437924593, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.026996908709406853}, {"id": 406, "seek": 229618, "start": 2301.94, "end": 2307.46, "text": " space, different planes in 2000 dimensional space, and you see different cats live on different planes,", "tokens": [50652, 1901, 11, 819, 14952, 294, 8132, 18795, 1901, 11, 293, 291, 536, 819, 11111, 1621, 322, 819, 14952, 11, 50928], "temperature": 0.0, "avg_logprob": -0.09398518437924593, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.026996908709406853}, {"id": 407, "seek": 229618, "start": 2308.74, "end": 2315.22, "text": " but sometimes you can just, if you plop into this meaning space in some random place, you'll see", "tokens": [50992, 457, 2171, 291, 393, 445, 11, 498, 291, 499, 404, 666, 341, 3620, 1901, 294, 512, 4974, 1081, 11, 291, 603, 536, 51316], "temperature": 0.0, "avg_logprob": -0.09398518437924593, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.026996908709406853}, {"id": 408, "seek": 229618, "start": 2315.22, "end": 2321.22, "text": " things which kind of look, well, I don't know what those are, but sometimes you'll see things which", "tokens": [51316, 721, 597, 733, 295, 574, 11, 731, 11, 286, 500, 380, 458, 437, 729, 366, 11, 457, 2171, 291, 603, 536, 721, 597, 51616], "temperature": 0.0, "avg_logprob": -0.09398518437924593, "compression_ratio": 2.005050505050505, "no_speech_prob": 0.026996908709406853}, {"id": 409, "seek": 232122, "start": 2321.2999999999997, "end": 2326.8999999999996, "text": " kind of have a reminiscent of kind of human forms and so on, why does all this happen, same kind of", "tokens": [50368, 733, 295, 362, 257, 44304, 295, 733, 295, 1952, 6422, 293, 370, 322, 11, 983, 775, 439, 341, 1051, 11, 912, 733, 295, 50648], "temperature": 0.0, "avg_logprob": -0.08287795645291687, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01637308858335018}, {"id": 410, "seek": 232122, "start": 2326.8999999999996, "end": 2332.58, "text": " reason as with LLMs, because this was trained from a five billion images, which were actual images", "tokens": [50648, 1778, 382, 365, 441, 43, 26386, 11, 570, 341, 390, 8895, 490, 257, 1732, 5218, 5267, 11, 597, 645, 3539, 5267, 50932], "temperature": 0.0, "avg_logprob": -0.08287795645291687, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01637308858335018}, {"id": 411, "seek": 232122, "start": 2332.58, "end": 2339.8599999999997, "text": " people put on the web, and those actual images are of human relevant kinds of things, with images", "tokens": [50932, 561, 829, 322, 264, 3670, 11, 293, 729, 3539, 5267, 366, 295, 1952, 7340, 3685, 295, 721, 11, 365, 5267, 51296], "temperature": 0.0, "avg_logprob": -0.08287795645291687, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01637308858335018}, {"id": 412, "seek": 232122, "start": 2339.8599999999997, "end": 2346.2599999999998, "text": " more so than with text, we're able to, as humans, we're able to look at things that weren't quite", "tokens": [51296, 544, 370, 813, 365, 2487, 11, 321, 434, 1075, 281, 11, 382, 6255, 11, 321, 434, 1075, 281, 574, 412, 721, 300, 4999, 380, 1596, 51616], "temperature": 0.0, "avg_logprob": -0.08287795645291687, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01637308858335018}, {"id": 413, "seek": 232122, "start": 2346.2599999999998, "end": 2350.2599999999998, "text": " right, like we looked at that high temperature version of what the LLM produced, and it looked", "tokens": [51616, 558, 11, 411, 321, 2956, 412, 300, 1090, 4292, 3037, 295, 437, 264, 441, 43, 44, 7126, 11, 293, 309, 2956, 51816], "temperature": 0.0, "avg_logprob": -0.08287795645291687, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.01637308858335018}, {"id": 414, "seek": 235026, "start": 2350.26, "end": 2355.94, "text": " like garbage to us, it was incomprehensible, for images we do a little better at being able to", "tokens": [50364, 411, 14150, 281, 505, 11, 309, 390, 14036, 40128, 30633, 11, 337, 5267, 321, 360, 257, 707, 1101, 412, 885, 1075, 281, 50648], "temperature": 0.0, "avg_logprob": -0.08478527977353051, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.0035979559179395437}, {"id": 415, "seek": 235026, "start": 2355.94, "end": 2360.9, "text": " not be just completely confused by what's going on, but if we kind of dive in and look, you know,", "tokens": [50648, 406, 312, 445, 2584, 9019, 538, 437, 311, 516, 322, 11, 457, 498, 321, 733, 295, 9192, 294, 293, 574, 11, 291, 458, 11, 50896], "temperature": 0.0, "avg_logprob": -0.08478527977353051, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.0035979559179395437}, {"id": 416, "seek": 235026, "start": 2360.9, "end": 2366.5800000000004, "text": " what's out there in arbitrarily, let's see where do I have a picture of that, well, those are some", "tokens": [50896, 437, 311, 484, 456, 294, 19071, 3289, 11, 718, 311, 536, 689, 360, 286, 362, 257, 3036, 295, 300, 11, 731, 11, 729, 366, 512, 51180], "temperature": 0.0, "avg_logprob": -0.08478527977353051, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.0035979559179395437}, {"id": 417, "seek": 235026, "start": 2366.5800000000004, "end": 2373.6200000000003, "text": " pictures just randomly out there in kind of meaning space, and if you go in you can see,", "tokens": [51180, 5242, 445, 16979, 484, 456, 294, 733, 295, 3620, 1901, 11, 293, 498, 291, 352, 294, 291, 393, 536, 11, 51532], "temperature": 0.0, "avg_logprob": -0.08478527977353051, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.0035979559179395437}, {"id": 418, "seek": 235026, "start": 2373.6200000000003, "end": 2377.5400000000004, "text": " you know, there are weird things like this, these are, you know, people like pictures, or you can", "tokens": [51532, 291, 458, 11, 456, 366, 3657, 721, 411, 341, 11, 613, 366, 11, 291, 458, 11, 561, 411, 5242, 11, 420, 291, 393, 51728], "temperature": 0.0, "avg_logprob": -0.08478527977353051, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.0035979559179395437}, {"id": 419, "seek": 237754, "start": 2377.54, "end": 2381.94, "text": " have, you know, pictures like these, which are kind of reminiscent of sort of landscape-like", "tokens": [50364, 362, 11, 291, 458, 11, 5242, 411, 613, 11, 597, 366, 733, 295, 44304, 295, 1333, 295, 9661, 12, 4092, 50584], "temperature": 0.0, "avg_logprob": -0.11683398586208538, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.0092593589797616}, {"id": 420, "seek": 237754, "start": 2381.94, "end": 2385.94, "text": " pictures, but aren't really landscape-like pictures, but this whole question about,", "tokens": [50584, 5242, 11, 457, 3212, 380, 534, 9661, 12, 4092, 5242, 11, 457, 341, 1379, 1168, 466, 11, 50784], "temperature": 0.0, "avg_logprob": -0.11683398586208538, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.0092593589797616}, {"id": 421, "seek": 237754, "start": 2385.94, "end": 2393.3, "text": " you know, where in meaning space, where in this, I mean, there's this, if you try and imagine,", "tokens": [50784, 291, 458, 11, 689, 294, 3620, 1901, 11, 689, 294, 341, 11, 286, 914, 11, 456, 311, 341, 11, 498, 291, 853, 293, 3811, 11, 51152], "temperature": 0.0, "avg_logprob": -0.11683398586208538, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.0092593589797616}, {"id": 422, "seek": 237754, "start": 2393.3, "end": 2398.98, "text": " where is the stuff that's meaningful, 10 to the minus 600 of all of meaning space is what we have", "tokens": [51152, 689, 307, 264, 1507, 300, 311, 10995, 11, 1266, 281, 264, 3175, 11849, 295, 439, 295, 3620, 1901, 307, 437, 321, 362, 51436], "temperature": 0.0, "avg_logprob": -0.11683398586208538, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.0092593589797616}, {"id": 423, "seek": 237754, "start": 2398.98, "end": 2404.74, "text": " so far explored as with sort of human language and so on, it's a very small fraction of it,", "tokens": [51436, 370, 1400, 24016, 382, 365, 1333, 295, 1952, 2856, 293, 370, 322, 11, 309, 311, 257, 588, 1359, 14135, 295, 309, 11, 51724], "temperature": 0.0, "avg_logprob": -0.11683398586208538, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.0092593589797616}, {"id": 424, "seek": 240474, "start": 2404.74, "end": 2410.3399999999997, "text": " with respect to images. Okay, so just to maybe finish off a bit, we could talk more about this", "tokens": [50364, 365, 3104, 281, 5267, 13, 1033, 11, 370, 445, 281, 1310, 2413, 766, 257, 857, 11, 321, 727, 751, 544, 466, 341, 50644], "temperature": 0.0, "avg_logprob": -0.07697671431082266, "compression_ratio": 1.68, "no_speech_prob": 0.006739430595189333}, {"id": 425, "seek": 240474, "start": 2410.3399999999997, "end": 2415.9399999999996, "text": " kind of thing, but just to talk a little bit about sort of the physics of LLMs and so on,", "tokens": [50644, 733, 295, 551, 11, 457, 445, 281, 751, 257, 707, 857, 466, 1333, 295, 264, 10649, 295, 441, 43, 26386, 293, 370, 322, 11, 50924], "temperature": 0.0, "avg_logprob": -0.07697671431082266, "compression_ratio": 1.68, "no_speech_prob": 0.006739430595189333}, {"id": 426, "seek": 240474, "start": 2416.58, "end": 2422.66, "text": " I think one of the things people, what one wants to do is, is there a kind of a narrative story of", "tokens": [50956, 286, 519, 472, 295, 264, 721, 561, 11, 437, 472, 2738, 281, 360, 307, 11, 307, 456, 257, 733, 295, 257, 9977, 1657, 295, 51260], "temperature": 0.0, "avg_logprob": -0.07697671431082266, "compression_ratio": 1.68, "no_speech_prob": 0.006739430595189333}, {"id": 427, "seek": 240474, "start": 2422.66, "end": 2430.4199999999996, "text": " what LLMs are finding? Is there a way of saying, why do LLMs even work? It's not obvious that,", "tokens": [51260, 437, 441, 43, 26386, 366, 5006, 30, 1119, 456, 257, 636, 295, 1566, 11, 983, 360, 441, 43, 26386, 754, 589, 30, 467, 311, 406, 6322, 300, 11, 51648], "temperature": 0.0, "avg_logprob": -0.07697671431082266, "compression_ratio": 1.68, "no_speech_prob": 0.006739430595189333}, {"id": 428, "seek": 243042, "start": 2430.42, "end": 2436.9, "text": " you know, given that you, you know, you could say, take a sentence like, the cat sat on the,", "tokens": [50364, 291, 458, 11, 2212, 300, 291, 11, 291, 458, 11, 291, 727, 584, 11, 747, 257, 8174, 411, 11, 264, 3857, 3227, 322, 264, 11, 50688], "temperature": 0.0, "avg_logprob": -0.10563584459506399, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.004976765718311071}, {"id": 429, "seek": 243042, "start": 2437.54, "end": 2443.06, "text": " okay, based on just looking at pages on the web, you can reasonably guess the next word is going", "tokens": [50720, 1392, 11, 2361, 322, 445, 1237, 412, 7183, 322, 264, 3670, 11, 291, 393, 23551, 2041, 264, 958, 1349, 307, 516, 50996], "temperature": 0.0, "avg_logprob": -0.10563584459506399, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.004976765718311071}, {"id": 430, "seek": 243042, "start": 2443.06, "end": 2447.38, "text": " to be math, but by the time you've got a long prompt where you're asking it some physics question", "tokens": [50996, 281, 312, 5221, 11, 457, 538, 264, 565, 291, 600, 658, 257, 938, 12391, 689, 291, 434, 3365, 309, 512, 10649, 1168, 51212], "temperature": 0.0, "avg_logprob": -0.10563584459506399, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.004976765718311071}, {"id": 431, "seek": 243042, "start": 2447.38, "end": 2454.5, "text": " or something, there's no way that actual detailed text is going to be somewhere on the web, or", "tokens": [51212, 420, 746, 11, 456, 311, 572, 636, 300, 3539, 9942, 2487, 307, 516, 281, 312, 4079, 322, 264, 3670, 11, 420, 51568], "temperature": 0.0, "avg_logprob": -0.10563584459506399, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.004976765718311071}, {"id": 432, "seek": 243042, "start": 2454.5, "end": 2459.38, "text": " probably not, unless it was some exercise or a book or something, but most of the time it won't", "tokens": [51568, 1391, 406, 11, 5969, 309, 390, 512, 5380, 420, 257, 1446, 420, 746, 11, 457, 881, 295, 264, 565, 309, 1582, 380, 51812], "temperature": 0.0, "avg_logprob": -0.10563584459506399, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.004976765718311071}, {"id": 433, "seek": 245938, "start": 2459.38, "end": 2464.1800000000003, "text": " be something that was on the web. So you have to have an actual model that allows you to extrapolate", "tokens": [50364, 312, 746, 300, 390, 322, 264, 3670, 13, 407, 291, 362, 281, 362, 364, 3539, 2316, 300, 4045, 291, 281, 48224, 473, 50604], "temperature": 0.0, "avg_logprob": -0.08695593747225674, "compression_ratio": 1.7963636363636364, "no_speech_prob": 0.0017859486397355795}, {"id": 434, "seek": 245938, "start": 2464.1800000000003, "end": 2469.94, "text": " the model that's being used in chat GBT as a neural net. It is far from obvious. There's no", "tokens": [50604, 264, 2316, 300, 311, 885, 1143, 294, 5081, 26809, 51, 382, 257, 18161, 2533, 13, 467, 307, 1400, 490, 6322, 13, 821, 311, 572, 50892], "temperature": 0.0, "avg_logprob": -0.08695593747225674, "compression_ratio": 1.7963636363636364, "no_speech_prob": 0.0017859486397355795}, {"id": 435, "seek": 245938, "start": 2469.94, "end": 2475.38, "text": " fundamental reason to think it would be true that the way the neural net extrapolates will agree with", "tokens": [50892, 8088, 1778, 281, 519, 309, 576, 312, 2074, 300, 264, 636, 264, 18161, 2533, 48224, 1024, 486, 3986, 365, 51164], "temperature": 0.0, "avg_logprob": -0.08695593747225674, "compression_ratio": 1.7963636363636364, "no_speech_prob": 0.0017859486397355795}, {"id": 436, "seek": 245938, "start": 2475.38, "end": 2481.86, "text": " the way we humans think it makes sense to extrapolate. The fact that it extrapolates to produce things", "tokens": [51164, 264, 636, 321, 6255, 519, 309, 1669, 2020, 281, 48224, 473, 13, 440, 1186, 300, 309, 48224, 1024, 281, 5258, 721, 51488], "temperature": 0.0, "avg_logprob": -0.08695593747225674, "compression_ratio": 1.7963636363636364, "no_speech_prob": 0.0017859486397355795}, {"id": 437, "seek": 245938, "start": 2481.86, "end": 2487.3, "text": " that seem meaningful to us humans is a nontrivial scientific result. And, you know, I think what", "tokens": [51488, 300, 1643, 10995, 281, 505, 6255, 307, 257, 297, 896, 470, 22640, 8134, 1874, 13, 400, 11, 291, 458, 11, 286, 519, 437, 51760], "temperature": 0.0, "avg_logprob": -0.08695593747225674, "compression_ratio": 1.7963636363636364, "no_speech_prob": 0.0017859486397355795}, {"id": 438, "seek": 248730, "start": 2487.3, "end": 2494.02, "text": " it's basically telling us is the way brains work is actually pretty well modeled by sort of neural", "tokens": [50364, 309, 311, 1936, 3585, 505, 307, 264, 636, 15442, 589, 307, 767, 1238, 731, 37140, 538, 1333, 295, 18161, 50700], "temperature": 0.0, "avg_logprob": -0.06813302282559669, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.0010236050002276897}, {"id": 439, "seek": 248730, "start": 2494.02, "end": 2499.78, "text": " net type things. And that's why the things that brains extrapolate with are pretty close to the", "tokens": [50700, 2533, 2010, 721, 13, 400, 300, 311, 983, 264, 721, 300, 15442, 48224, 473, 365, 366, 1238, 1998, 281, 264, 50988], "temperature": 0.0, "avg_logprob": -0.06813302282559669, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.0010236050002276897}, {"id": 440, "seek": 248730, "start": 2499.78, "end": 2505.46, "text": " things that these simple neural nets extrapolate with. So then the question is, well, okay, we've", "tokens": [50988, 721, 300, 613, 2199, 18161, 36170, 48224, 473, 365, 13, 407, 550, 264, 1168, 307, 11, 731, 11, 1392, 11, 321, 600, 51272], "temperature": 0.0, "avg_logprob": -0.06813302282559669, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.0010236050002276897}, {"id": 441, "seek": 248730, "start": 2505.46, "end": 2510.1000000000004, "text": " got this kind of extrapolation that's going on. We've got some, this thing is finding out some", "tokens": [51272, 658, 341, 733, 295, 48224, 399, 300, 311, 516, 322, 13, 492, 600, 658, 512, 11, 341, 551, 307, 5006, 484, 512, 51504], "temperature": 0.0, "avg_logprob": -0.06813302282559669, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.0010236050002276897}, {"id": 442, "seek": 248730, "start": 2510.1000000000004, "end": 2515.7000000000003, "text": " way to extrapolate. How is it doing that? Well, what regularities in language is it picking up to", "tokens": [51504, 636, 281, 48224, 473, 13, 1012, 307, 309, 884, 300, 30, 1042, 11, 437, 3890, 1088, 294, 2856, 307, 309, 8867, 493, 281, 51784], "temperature": 0.0, "avg_logprob": -0.06813302282559669, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.0010236050002276897}, {"id": 443, "seek": 251570, "start": 2515.7799999999997, "end": 2520.74, "text": " allow it to make meaningful sentences, meaningful text? Well, there's one big regularity that we", "tokens": [50368, 2089, 309, 281, 652, 10995, 16579, 11, 10995, 2487, 30, 1042, 11, 456, 311, 472, 955, 3890, 507, 300, 321, 50616], "temperature": 0.0, "avg_logprob": -0.08414066589630402, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.0040076132863759995}, {"id": 444, "seek": 251570, "start": 2520.74, "end": 2526.3399999999997, "text": " know about in language, which is syntactic grammar. We know how you put parts of speech together,", "tokens": [50616, 458, 466, 294, 2856, 11, 597, 307, 23980, 19892, 22317, 13, 492, 458, 577, 291, 829, 3166, 295, 6218, 1214, 11, 50896], "temperature": 0.0, "avg_logprob": -0.08414066589630402, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.0040076132863759995}, {"id": 445, "seek": 251570, "start": 2526.3399999999997, "end": 2532.1, "text": " nouns and verbs and things like this. So in a sense, we can then construct sentences which", "tokens": [50896, 48184, 293, 30051, 293, 721, 411, 341, 13, 407, 294, 257, 2020, 11, 321, 393, 550, 7690, 16579, 597, 51184], "temperature": 0.0, "avg_logprob": -0.08414066589630402, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.0040076132863759995}, {"id": 446, "seek": 251570, "start": 2532.1, "end": 2538.1, "text": " are syntactically correct. But there are infinite number of sentences that are syntactically correct", "tokens": [51184, 366, 23980, 578, 984, 3006, 13, 583, 456, 366, 13785, 1230, 295, 16579, 300, 366, 23980, 578, 984, 3006, 51484], "temperature": 0.0, "avg_logprob": -0.08414066589630402, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.0040076132863759995}, {"id": 447, "seek": 251570, "start": 2538.1, "end": 2544.02, "text": " but complete nonsense. And that's, it's doing much better than just producing syntactically correct", "tokens": [51484, 457, 3566, 14925, 13, 400, 300, 311, 11, 309, 311, 884, 709, 1101, 813, 445, 10501, 23980, 578, 984, 3006, 51780], "temperature": 0.0, "avg_logprob": -0.08414066589630402, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.0040076132863759995}, {"id": 448, "seek": 254402, "start": 2544.02, "end": 2550.1, "text": " sentences. So what's it doing? Well, there's one good example of a place where we know a structure", "tokens": [50364, 16579, 13, 407, 437, 311, 309, 884, 30, 1042, 11, 456, 311, 472, 665, 1365, 295, 257, 1081, 689, 321, 458, 257, 3877, 50668], "temperature": 0.0, "avg_logprob": -0.06983126624155853, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.0008500343537889421}, {"id": 449, "seek": 254402, "start": 2550.1, "end": 2557.22, "text": " in sentences that exists that isn't sort of purely syntactic. And that's logic. And you can kind of", "tokens": [50668, 294, 16579, 300, 8198, 300, 1943, 380, 1333, 295, 17491, 23980, 19892, 13, 400, 300, 311, 9952, 13, 400, 291, 393, 733, 295, 51024], "temperature": 0.0, "avg_logprob": -0.06983126624155853, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.0008500343537889421}, {"id": 450, "seek": 254402, "start": 2557.22, "end": 2561.86, "text": " think, you know, when Aristotle invented logic back a couple of thousand years ago, you know,", "tokens": [51024, 519, 11, 291, 458, 11, 562, 42368, 14479, 9952, 646, 257, 1916, 295, 4714, 924, 2057, 11, 291, 458, 11, 51256], "temperature": 0.0, "avg_logprob": -0.06983126624155853, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.0008500343537889421}, {"id": 451, "seek": 254402, "start": 2561.86, "end": 2566.1, "text": " what was he actually doing? Well, he was a bit like a machine learning system, because what he was", "tokens": [51256, 437, 390, 415, 767, 884, 30, 1042, 11, 415, 390, 257, 857, 411, 257, 3479, 2539, 1185, 11, 570, 437, 415, 390, 51468], "temperature": 0.0, "avg_logprob": -0.06983126624155853, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.0008500343537889421}, {"id": 452, "seek": 254402, "start": 2566.1, "end": 2571.14, "text": " effectively doing was saying, I've got all these examples of rhetoric. People make an argument", "tokens": [51468, 8659, 884, 390, 1566, 11, 286, 600, 658, 439, 613, 5110, 295, 29604, 13, 3432, 652, 364, 6770, 51720], "temperature": 0.0, "avg_logprob": -0.06983126624155853, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.0008500343537889421}, {"id": 453, "seek": 257114, "start": 2571.14, "end": 2576.42, "text": " that looks like this, but I can take something which instead of it being a discussion about, you", "tokens": [50364, 300, 1542, 411, 341, 11, 457, 286, 393, 747, 746, 597, 2602, 295, 309, 885, 257, 5017, 466, 11, 291, 50628], "temperature": 0.0, "avg_logprob": -0.10287789216975576, "compression_ratio": 1.6375, "no_speech_prob": 0.029295504093170166}, {"id": 454, "seek": 257114, "start": 2576.42, "end": 2582.58, "text": " know, Sparta and Athens, it can be a discussion about turtles and fishes. It doesn't matter. I can", "tokens": [50628, 458, 11, 1738, 19061, 293, 32530, 11, 309, 393, 312, 257, 5017, 466, 32422, 293, 41734, 13, 467, 1177, 380, 1871, 13, 286, 393, 50936], "temperature": 0.0, "avg_logprob": -0.10287789216975576, "compression_ratio": 1.6375, "no_speech_prob": 0.029295504093170166}, {"id": 455, "seek": 257114, "start": 2582.58, "end": 2587.94, "text": " just replace those symbolically with P and Q and I can look at this sort of formal structure of these", "tokens": [50936, 445, 7406, 729, 5986, 984, 365, 430, 293, 1249, 293, 286, 393, 574, 412, 341, 1333, 295, 9860, 3877, 295, 613, 51204], "temperature": 0.0, "avg_logprob": -0.10287789216975576, "compression_ratio": 1.6375, "no_speech_prob": 0.029295504093170166}, {"id": 456, "seek": 257114, "start": 2587.94, "end": 2595.06, "text": " sentences. In a sense, you can lift logic out of the specifics of actual language, in his case,", "tokens": [51204, 16579, 13, 682, 257, 2020, 11, 291, 393, 5533, 9952, 484, 295, 264, 28454, 295, 3539, 2856, 11, 294, 702, 1389, 11, 51560], "temperature": 0.0, "avg_logprob": -0.10287789216975576, "compression_ratio": 1.6375, "no_speech_prob": 0.029295504093170166}, {"id": 457, "seek": 259506, "start": 2595.06, "end": 2602.2599999999998, "text": " Greek. But in a sense, what LLMs have done is they've discovered the same thing. So people say,", "tokens": [50364, 10281, 13, 583, 294, 257, 2020, 11, 437, 441, 43, 26386, 362, 1096, 307, 436, 600, 6941, 264, 912, 551, 13, 407, 561, 584, 11, 50724], "temperature": 0.0, "avg_logprob": -0.11458856828751103, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.05284668505191803}, {"id": 458, "seek": 259506, "start": 2602.2599999999998, "end": 2607.14, "text": " oh, my gosh, it's amazing, you know, LLMs have discovered logic. Well, they discovered logic,", "tokens": [50724, 1954, 11, 452, 6502, 11, 309, 311, 2243, 11, 291, 458, 11, 441, 43, 26386, 362, 6941, 9952, 13, 1042, 11, 436, 6941, 9952, 11, 50968], "temperature": 0.0, "avg_logprob": -0.11458856828751103, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.05284668505191803}, {"id": 459, "seek": 259506, "start": 2607.14, "end": 2611.22, "text": " I think the same way Aristotle discovered logic, and you can find out they're basically doing", "tokens": [50968, 286, 519, 264, 912, 636, 42368, 6941, 9952, 11, 293, 291, 393, 915, 484, 436, 434, 1936, 884, 51172], "temperature": 0.0, "avg_logprob": -0.11458856828751103, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.05284668505191803}, {"id": 460, "seek": 259506, "start": 2611.22, "end": 2616.5, "text": " so logistic logic. And if you try and feed them things which require sort of more formal, more", "tokens": [51172, 370, 3565, 3142, 9952, 13, 400, 498, 291, 853, 293, 3154, 552, 721, 597, 3651, 1333, 295, 544, 9860, 11, 544, 51436], "temperature": 0.0, "avg_logprob": -0.11458856828751103, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.05284668505191803}, {"id": 461, "seek": 259506, "start": 2616.5, "end": 2621.2999999999997, "text": " formal kinds of things, even at the level of, you know, parenthesis matching and so on, they will", "tokens": [51436, 9860, 3685, 295, 721, 11, 754, 412, 264, 1496, 295, 11, 291, 458, 11, 23350, 9374, 14324, 293, 370, 322, 11, 436, 486, 51676], "temperature": 0.0, "avg_logprob": -0.11458856828751103, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.05284668505191803}, {"id": 462, "seek": 262130, "start": 2621.3, "end": 2626.34, "text": " fail after you get sort of too many parentheses to match. They don't do kind of the formal level", "tokens": [50364, 3061, 934, 291, 483, 1333, 295, 886, 867, 34153, 281, 2995, 13, 814, 500, 380, 360, 733, 295, 264, 9860, 1496, 50616], "temperature": 0.0, "avg_logprob": -0.06098852237733472, "compression_ratio": 1.8850574712643677, "no_speech_prob": 0.012744386680424213}, {"id": 463, "seek": 262130, "start": 2626.34, "end": 2632.26, "text": " of things. They don't do the computational thing. They do the kind of level of things that in a sense", "tokens": [50616, 295, 721, 13, 814, 500, 380, 360, 264, 28270, 551, 13, 814, 360, 264, 733, 295, 1496, 295, 721, 300, 294, 257, 2020, 50912], "temperature": 0.0, "avg_logprob": -0.06098852237733472, "compression_ratio": 1.8850574712643677, "no_speech_prob": 0.012744386680424213}, {"id": 464, "seek": 262130, "start": 2632.26, "end": 2636.82, "text": " was the original way that logic was discovered. So that's a place where kind of one's able to lift", "tokens": [50912, 390, 264, 3380, 636, 300, 9952, 390, 6941, 13, 407, 300, 311, 257, 1081, 689, 733, 295, 472, 311, 1075, 281, 5533, 51140], "temperature": 0.0, "avg_logprob": -0.06098852237733472, "compression_ratio": 1.8850574712643677, "no_speech_prob": 0.012744386680424213}, {"id": 465, "seek": 262130, "start": 2636.82, "end": 2643.1400000000003, "text": " something more semantic out of this kind of layer of pure language. But presumably, there is more", "tokens": [51140, 746, 544, 47982, 484, 295, 341, 733, 295, 4583, 295, 6075, 2856, 13, 583, 26742, 11, 456, 307, 544, 51456], "temperature": 0.0, "avg_logprob": -0.06098852237733472, "compression_ratio": 1.8850574712643677, "no_speech_prob": 0.012744386680424213}, {"id": 466, "seek": 262130, "start": 2643.1400000000003, "end": 2648.6600000000003, "text": " that can be done along those lines. Presumably, there is kind of a semantic grammar of language,", "tokens": [51456, 300, 393, 312, 1096, 2051, 729, 3876, 13, 2718, 449, 1188, 11, 456, 307, 733, 295, 257, 47982, 22317, 295, 2856, 11, 51732], "temperature": 0.0, "avg_logprob": -0.06098852237733472, "compression_ratio": 1.8850574712643677, "no_speech_prob": 0.012744386680424213}, {"id": 467, "seek": 264866, "start": 2648.66, "end": 2656.3399999999997, "text": " which in a sense, the LLMs have discovered something about language and common sense", "tokens": [50364, 597, 294, 257, 2020, 11, 264, 441, 43, 26386, 362, 6941, 746, 466, 2856, 293, 2689, 2020, 50748], "temperature": 0.0, "avg_logprob": -0.08626833627390307, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.006363994441926479}, {"id": 468, "seek": 264866, "start": 2656.3399999999997, "end": 2661.8599999999997, "text": " reasoning and so on. That is that there's this sort of thing you can lift out of language", "tokens": [50748, 21577, 293, 370, 322, 13, 663, 307, 300, 456, 311, 341, 1333, 295, 551, 291, 393, 5533, 484, 295, 2856, 51024], "temperature": 0.0, "avg_logprob": -0.08626833627390307, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.006363994441926479}, {"id": 469, "seek": 264866, "start": 2661.8599999999997, "end": 2668.58, "text": " that allows you to kind of put together meaningful stuff beyond just the purely syntactic. And I", "tokens": [51024, 300, 4045, 291, 281, 733, 295, 829, 1214, 10995, 1507, 4399, 445, 264, 17491, 23980, 19892, 13, 400, 286, 51360], "temperature": 0.0, "avg_logprob": -0.08626833627390307, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.006363994441926479}, {"id": 470, "seek": 264866, "start": 2668.58, "end": 2672.58, "text": " think that's the thing where, well, I've been interested in this actually for a long time", "tokens": [51360, 519, 300, 311, 264, 551, 689, 11, 731, 11, 286, 600, 668, 3102, 294, 341, 767, 337, 257, 938, 565, 51560], "temperature": 0.0, "avg_logprob": -0.08626833627390307, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.006363994441926479}, {"id": 471, "seek": 267258, "start": 2672.58, "end": 2678.9, "text": " for different reasons, this kind of idea of sort of making a symbolic discourse language that allows", "tokens": [50364, 337, 819, 4112, 11, 341, 733, 295, 1558, 295, 1333, 295, 1455, 257, 25755, 23938, 2856, 300, 4045, 50680], "temperature": 0.0, "avg_logprob": -0.08486051773757078, "compression_ratio": 1.81651376146789, "no_speech_prob": 0.1426859349012375}, {"id": 472, "seek": 267258, "start": 2678.9, "end": 2687.2999999999997, "text": " you to sort of express things in a kind of, in a way that is sort of, is a symbolic way of expressing", "tokens": [50680, 291, 281, 1333, 295, 5109, 721, 294, 257, 733, 295, 11, 294, 257, 636, 300, 307, 1333, 295, 11, 307, 257, 25755, 636, 295, 22171, 51100], "temperature": 0.0, "avg_logprob": -0.08486051773757078, "compression_ratio": 1.81651376146789, "no_speech_prob": 0.1426859349012375}, {"id": 473, "seek": 267258, "start": 2687.2999999999997, "end": 2692.66, "text": " things that is not specific to the particulars of language. In a sense, the whole enterprise of", "tokens": [51100, 721, 300, 307, 406, 2685, 281, 264, 21861, 685, 295, 2856, 13, 682, 257, 2020, 11, 264, 1379, 14132, 295, 51368], "temperature": 0.0, "avg_logprob": -0.08486051773757078, "compression_ratio": 1.81651376146789, "no_speech_prob": 0.1426859349012375}, {"id": 474, "seek": 267258, "start": 2692.66, "end": 2698.02, "text": " making a computational language has got a certain distance with that, describing certain kinds of", "tokens": [51368, 1455, 257, 28270, 2856, 575, 658, 257, 1629, 4560, 365, 300, 11, 16141, 1629, 3685, 295, 51636], "temperature": 0.0, "avg_logprob": -0.08486051773757078, "compression_ratio": 1.81651376146789, "no_speech_prob": 0.1426859349012375}, {"id": 475, "seek": 269802, "start": 2698.02, "end": 2703.14, "text": " things in the world. But anyway, I think that there are many pieces of kind of what happens in", "tokens": [50364, 721, 294, 264, 1002, 13, 583, 4033, 11, 286, 519, 300, 456, 366, 867, 3755, 295, 733, 295, 437, 2314, 294, 50620], "temperature": 0.0, "avg_logprob": -0.0786470073764607, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.04059028625488281}, {"id": 476, "seek": 269802, "start": 2703.14, "end": 2709.38, "text": " LLMs. For example, why does few shot learning work? Why does it work to tell LLM and LLM to talk", "tokens": [50620, 441, 43, 26386, 13, 1171, 1365, 11, 983, 775, 1326, 3347, 2539, 589, 30, 1545, 775, 309, 589, 281, 980, 441, 43, 44, 293, 441, 43, 44, 281, 751, 50932], "temperature": 0.0, "avg_logprob": -0.0786470073764607, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.04059028625488281}, {"id": 477, "seek": 269802, "start": 2709.38, "end": 2715.46, "text": " like a pirate? Why does it, how does that manage to place it somewhere in meaning space or something", "tokens": [50932, 411, 257, 27424, 30, 1545, 775, 309, 11, 577, 775, 300, 3067, 281, 1081, 309, 4079, 294, 3620, 1901, 420, 746, 51236], "temperature": 0.0, "avg_logprob": -0.0786470073764607, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.04059028625488281}, {"id": 478, "seek": 269802, "start": 2715.46, "end": 2720.2599999999998, "text": " so that the kind of, you know, you placed it somewhere by giving that prompt, then somehow", "tokens": [51236, 370, 300, 264, 733, 295, 11, 291, 458, 11, 291, 7074, 309, 4079, 538, 2902, 300, 12391, 11, 550, 6063, 51476], "temperature": 0.0, "avg_logprob": -0.0786470073764607, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.04059028625488281}, {"id": 479, "seek": 269802, "start": 2720.2599999999998, "end": 2725.06, "text": " the semantic law of motion takes over and it successfully manages to produce semantically", "tokens": [51476, 264, 47982, 2101, 295, 5394, 2516, 670, 293, 309, 10727, 22489, 281, 5258, 4361, 49505, 51716], "temperature": 0.0, "avg_logprob": -0.0786470073764607, "compression_ratio": 1.7518518518518518, "no_speech_prob": 0.04059028625488281}, {"id": 480, "seek": 272506, "start": 2725.06, "end": 2730.34, "text": " meaningful stuff. We don't know how any of this works. It's a great topic for physicists, I have", "tokens": [50364, 10995, 1507, 13, 492, 500, 380, 458, 577, 604, 295, 341, 1985, 13, 467, 311, 257, 869, 4829, 337, 48716, 11, 286, 362, 50628], "temperature": 0.0, "avg_logprob": -0.07011386752128601, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.012898799031972885}, {"id": 481, "seek": 272506, "start": 2730.34, "end": 2736.66, "text": " to say. I think it's one of these places where it isn't particularly easy. It's something where,", "tokens": [50628, 281, 584, 13, 286, 519, 309, 311, 472, 295, 613, 3190, 689, 309, 1943, 380, 4098, 1858, 13, 467, 311, 746, 689, 11, 50944], "temperature": 0.0, "avg_logprob": -0.07011386752128601, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.012898799031972885}, {"id": 482, "seek": 272506, "start": 2736.66, "end": 2742.1, "text": " you know, this space of, you know, this sort of meaning space we're looking at with these images,", "tokens": [50944, 291, 458, 11, 341, 1901, 295, 11, 291, 458, 11, 341, 1333, 295, 3620, 1901, 321, 434, 1237, 412, 365, 613, 5267, 11, 51216], "temperature": 0.0, "avg_logprob": -0.07011386752128601, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.012898799031972885}, {"id": 483, "seek": 272506, "start": 2742.1, "end": 2746.58, "text": " we can sort of see things about what's out there in meaning space in a way it's a little bit easier", "tokens": [51216, 321, 393, 1333, 295, 536, 721, 466, 437, 311, 484, 456, 294, 3620, 1901, 294, 257, 636, 309, 311, 257, 707, 857, 3571, 51440], "temperature": 0.0, "avg_logprob": -0.07011386752128601, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.012898799031972885}, {"id": 484, "seek": 272506, "start": 2746.58, "end": 2751.38, "text": " than with text and words. But we're kind of, you know, this is sort of just the beginning of,", "tokens": [51440, 813, 365, 2487, 293, 2283, 13, 583, 321, 434, 733, 295, 11, 291, 458, 11, 341, 307, 1333, 295, 445, 264, 2863, 295, 11, 51680], "temperature": 0.0, "avg_logprob": -0.07011386752128601, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.012898799031972885}, {"id": 485, "seek": 275138, "start": 2751.38, "end": 2755.2200000000003, "text": " in a sense, physicalizing using something like statistical mechanics", "tokens": [50364, 294, 257, 2020, 11, 4001, 3319, 1228, 746, 411, 22820, 12939, 50556], "temperature": 0.0, "avg_logprob": -0.10576703331687233, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.004900404717773199}, {"id": 486, "seek": 275138, "start": 2755.2200000000003, "end": 2760.98, "text": " to try and analyze what's happening inside an LLM. So I think kind of to sort of summarize,", "tokens": [50556, 281, 853, 293, 12477, 437, 311, 2737, 1854, 364, 441, 43, 44, 13, 407, 286, 519, 733, 295, 281, 1333, 295, 20858, 11, 50844], "temperature": 0.0, "avg_logprob": -0.10576703331687233, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.004900404717773199}, {"id": 487, "seek": 275138, "start": 2760.98, "end": 2766.42, "text": " I mean, I've talked about two kinds of things. One is just the very practical aspects of using", "tokens": [50844, 286, 914, 11, 286, 600, 2825, 466, 732, 3685, 295, 721, 13, 1485, 307, 445, 264, 588, 8496, 7270, 295, 1228, 51116], "temperature": 0.0, "avg_logprob": -0.10576703331687233, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.004900404717773199}, {"id": 488, "seek": 275138, "start": 2766.42, "end": 2774.1800000000003, "text": " LLMs to, I think the most significant workflow there is this. You have a vague idea of what you", "tokens": [51116, 441, 43, 26386, 281, 11, 286, 519, 264, 881, 4776, 20993, 456, 307, 341, 13, 509, 362, 257, 24247, 1558, 295, 437, 291, 51504], "temperature": 0.0, "avg_logprob": -0.10576703331687233, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.004900404717773199}, {"id": 489, "seek": 277418, "start": 2774.98, "end": 2781.8599999999997, "text": " want to do. Now I have to say to get that vague idea, you have to have an ability to sort of", "tokens": [50404, 528, 281, 360, 13, 823, 286, 362, 281, 584, 281, 483, 300, 24247, 1558, 11, 291, 362, 281, 362, 364, 3485, 281, 1333, 295, 50748], "temperature": 0.0, "avg_logprob": -0.10157258459862242, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.01873841881752014}, {"id": 490, "seek": 277418, "start": 2781.8599999999997, "end": 2787.7, "text": " think computationally about things until you can express yourself in some kind of sort of", "tokens": [50748, 519, 24903, 379, 466, 721, 1826, 291, 393, 5109, 1803, 294, 512, 733, 295, 1333, 295, 51040], "temperature": 0.0, "avg_logprob": -0.10157258459862242, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.01873841881752014}, {"id": 491, "seek": 277418, "start": 2787.7, "end": 2794.3399999999997, "text": " with computational concepts. I mean, it's no good, you know, with some notion of how you think about", "tokens": [51040, 365, 28270, 10392, 13, 286, 914, 11, 309, 311, 572, 665, 11, 291, 458, 11, 365, 512, 10710, 295, 577, 291, 519, 466, 51372], "temperature": 0.0, "avg_logprob": -0.10157258459862242, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.01873841881752014}, {"id": 492, "seek": 277418, "start": 2794.3399999999997, "end": 2798.98, "text": " the world computationally. Once you have that, you can kind of write a piece of natural language,", "tokens": [51372, 264, 1002, 24903, 379, 13, 3443, 291, 362, 300, 11, 291, 393, 733, 295, 2464, 257, 2522, 295, 3303, 2856, 11, 51604], "temperature": 0.0, "avg_logprob": -0.10157258459862242, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.01873841881752014}, {"id": 493, "seek": 279898, "start": 2798.98, "end": 2805.14, "text": " you go sort of tell that to the LLM. The LLM will then write, you know, will write", "tokens": [50364, 291, 352, 1333, 295, 980, 300, 281, 264, 441, 43, 44, 13, 440, 441, 43, 44, 486, 550, 2464, 11, 291, 458, 11, 486, 2464, 50672], "temperature": 0.0, "avg_logprob": -0.09801567302030675, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.024195844307541847}, {"id": 494, "seek": 279898, "start": 2805.14, "end": 2812.34, "text": " Wolfram language code or whatever, sometimes correctly, that will be an expression of what", "tokens": [50672, 16634, 2356, 2856, 3089, 420, 2035, 11, 2171, 8944, 11, 300, 486, 312, 364, 6114, 295, 437, 51032], "temperature": 0.0, "avg_logprob": -0.09801567302030675, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.024195844307541847}, {"id": 495, "seek": 279898, "start": 2812.34, "end": 2817.78, "text": " it thought you meant by the things that you said in natural language. Now sometimes when you look", "tokens": [51032, 309, 1194, 291, 4140, 538, 264, 721, 300, 291, 848, 294, 3303, 2856, 13, 823, 2171, 562, 291, 574, 51304], "temperature": 0.0, "avg_logprob": -0.09801567302030675, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.024195844307541847}, {"id": 496, "seek": 279898, "start": 2817.78, "end": 2822.58, "text": " at that Wolfram language code, you'll say you misunderstood. It wasn't correct. You can fix", "tokens": [51304, 412, 300, 16634, 2356, 2856, 3089, 11, 291, 603, 584, 291, 33870, 13, 467, 2067, 380, 3006, 13, 509, 393, 3191, 51544], "temperature": 0.0, "avg_logprob": -0.09801567302030675, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.024195844307541847}, {"id": 497, "seek": 279898, "start": 2822.58, "end": 2828.18, "text": " that code or you can tell it to go fix the code or whatever else. But so the workflow is, you know,", "tokens": [51544, 300, 3089, 420, 291, 393, 980, 309, 281, 352, 3191, 264, 3089, 420, 2035, 1646, 13, 583, 370, 264, 20993, 307, 11, 291, 458, 11, 51824], "temperature": 0.0, "avg_logprob": -0.09801567302030675, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.024195844307541847}, {"id": 498, "seek": 282818, "start": 2828.18, "end": 2835.14, "text": " computationally imagine what you want to do, write it in natural language, have it kind of", "tokens": [50364, 24903, 379, 3811, 437, 291, 528, 281, 360, 11, 2464, 309, 294, 3303, 2856, 11, 362, 309, 733, 295, 50712], "temperature": 0.0, "avg_logprob": -0.10177358954843849, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.0009191070566885173}, {"id": 499, "seek": 282818, "start": 2835.14, "end": 2839.94, "text": " translated into computational language, then read the computational language. It's very important", "tokens": [50712, 16805, 666, 28270, 2856, 11, 550, 1401, 264, 28270, 2856, 13, 467, 311, 588, 1021, 50952], "temperature": 0.0, "avg_logprob": -0.10177358954843849, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.0009191070566885173}, {"id": 500, "seek": 282818, "start": 2839.94, "end": 2844.4199999999996, "text": " that something you can do with Wolfram language, no other, you know, that's the story of computational", "tokens": [50952, 300, 746, 291, 393, 360, 365, 16634, 2356, 2856, 11, 572, 661, 11, 291, 458, 11, 300, 311, 264, 1657, 295, 28270, 51176], "temperature": 0.0, "avg_logprob": -0.10177358954843849, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.0009191070566885173}, {"id": 501, "seek": 282818, "start": 2844.4199999999996, "end": 2849.2999999999997, "text": " language, very different from programming languages which weren't intended for humans to read", "tokens": [51176, 2856, 11, 588, 819, 490, 9410, 8650, 597, 4999, 380, 10226, 337, 6255, 281, 1401, 51420], "temperature": 0.0, "avg_logprob": -0.10177358954843849, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.0009191070566885173}, {"id": 502, "seek": 282818, "start": 2849.2999999999997, "end": 2855.62, "text": " particularly. But so that's something where you read that computational language, you understand", "tokens": [51420, 4098, 13, 583, 370, 300, 311, 746, 689, 291, 1401, 300, 28270, 2856, 11, 291, 1223, 51736], "temperature": 0.0, "avg_logprob": -0.10177358954843849, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.0009191070566885173}, {"id": 503, "seek": 285562, "start": 2855.62, "end": 2861.2999999999997, "text": " what it said, you fix it if you need to, then you say run that, then that becomes a sort of brick", "tokens": [50364, 437, 309, 848, 11, 291, 3191, 309, 498, 291, 643, 281, 11, 550, 291, 584, 1190, 300, 11, 550, 300, 3643, 257, 1333, 295, 16725, 50648], "temperature": 0.0, "avg_logprob": -0.07962897217389449, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.022962655872106552}, {"id": 504, "seek": 285562, "start": 2861.2999999999997, "end": 2868.18, "text": " that you can use to build a whole tower of what you want on top of. And so that's, I think that's", "tokens": [50648, 300, 291, 393, 764, 281, 1322, 257, 1379, 10567, 295, 437, 291, 528, 322, 1192, 295, 13, 400, 370, 300, 311, 11, 286, 519, 300, 311, 50992], "temperature": 0.0, "avg_logprob": -0.07962897217389449, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.022962655872106552}, {"id": 505, "seek": 285562, "start": 2868.18, "end": 2875.38, "text": " the workflow and, you know, I have to say, as we make these chat notebooks better, it's getting", "tokens": [50992, 264, 20993, 293, 11, 291, 458, 11, 286, 362, 281, 584, 11, 382, 321, 652, 613, 5081, 43782, 1101, 11, 309, 311, 1242, 51352], "temperature": 0.0, "avg_logprob": -0.07962897217389449, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.022962655872106552}, {"id": 506, "seek": 285562, "start": 2875.38, "end": 2880.5, "text": " closer to the point where it actually makes sense, even if you know Wolfram language well,", "tokens": [51352, 4966, 281, 264, 935, 689, 309, 767, 1669, 2020, 11, 754, 498, 291, 458, 16634, 2356, 2856, 731, 11, 51608], "temperature": 0.0, "avg_logprob": -0.07962897217389449, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.022962655872106552}, {"id": 507, "seek": 288050, "start": 2880.5, "end": 2886.26, "text": " to try and use it as a way to get things started if you're not thinking very clearly, so to speak.", "tokens": [50364, 281, 853, 293, 764, 309, 382, 257, 636, 281, 483, 721, 1409, 498, 291, 434, 406, 1953, 588, 4448, 11, 370, 281, 1710, 13, 50652], "temperature": 0.0, "avg_logprob": -0.07293282173298023, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.05295424163341522}, {"id": 508, "seek": 288050, "start": 2886.26, "end": 2890.98, "text": " Although as I say, to get the prompt right, you have to be kind of think expository writing because", "tokens": [50652, 5780, 382, 286, 584, 11, 281, 483, 264, 12391, 558, 11, 291, 362, 281, 312, 733, 295, 519, 1278, 9598, 827, 3579, 570, 50888], "temperature": 0.0, "avg_logprob": -0.07293282173298023, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.05295424163341522}, {"id": 509, "seek": 288050, "start": 2890.98, "end": 2896.42, "text": " if you're totally confused, the LLM will be confused as well. But anyway, so the first thing I was", "tokens": [50888, 498, 291, 434, 3879, 9019, 11, 264, 441, 43, 44, 486, 312, 9019, 382, 731, 13, 583, 4033, 11, 370, 264, 700, 551, 286, 390, 51160], "temperature": 0.0, "avg_logprob": -0.07293282173298023, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.05295424163341522}, {"id": 510, "seek": 288050, "start": 2896.42, "end": 2904.02, "text": " talking about was this idea of how do we make use of LLMs mostly as a way to kind of get a leg up", "tokens": [51160, 1417, 466, 390, 341, 1558, 295, 577, 360, 321, 652, 764, 295, 441, 43, 26386, 5240, 382, 257, 636, 281, 733, 295, 483, 257, 1676, 493, 51540], "temperature": 0.0, "avg_logprob": -0.07293282173298023, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.05295424163341522}, {"id": 511, "seek": 290402, "start": 2904.02, "end": 2911.3, "text": " on creating kind of computational language to be able to actually do computations. I should say,", "tokens": [50364, 322, 4084, 733, 295, 28270, 2856, 281, 312, 1075, 281, 767, 360, 2807, 763, 13, 286, 820, 584, 11, 50728], "temperature": 0.0, "avg_logprob": -0.15282181898752847, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.020740699023008347}, {"id": 512, "seek": 290402, "start": 2911.3, "end": 2919.38, "text": " by the way, I'm happy to talk about this, people interested if we have any time. But there's many", "tokens": [50728, 538, 264, 636, 11, 286, 478, 2055, 281, 751, 466, 341, 11, 561, 3102, 498, 321, 362, 604, 565, 13, 583, 456, 311, 867, 51132], "temperature": 0.0, "avg_logprob": -0.15282181898752847, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.020740699023008347}, {"id": 513, "seek": 290402, "start": 2919.38, "end": 2925.7, "text": " use cases, like for example, we're working on a bunch of AI tutoring applications. Another use", "tokens": [51132, 764, 3331, 11, 411, 337, 1365, 11, 321, 434, 1364, 322, 257, 3840, 295, 7318, 44410, 5821, 13, 3996, 764, 51448], "temperature": 0.0, "avg_logprob": -0.15282181898752847, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.020740699023008347}, {"id": 514, "seek": 290402, "start": 2925.7, "end": 2930.34, "text": " case I mentioned for physics, we've never been able to do, in Wolfram Alpha for example, we've", "tokens": [51448, 1389, 286, 2835, 337, 10649, 11, 321, 600, 1128, 668, 1075, 281, 360, 11, 294, 16634, 2356, 20588, 337, 1365, 11, 321, 600, 51680], "temperature": 0.0, "avg_logprob": -0.15282181898752847, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.020740699023008347}, {"id": 515, "seek": 293034, "start": 2930.34, "end": 2936.42, "text": " never been able to do physics word problems. We can do that once you've turned the word problem", "tokens": [50364, 1128, 668, 1075, 281, 360, 10649, 1349, 2740, 13, 492, 393, 360, 300, 1564, 291, 600, 3574, 264, 1349, 1154, 50668], "temperature": 0.0, "avg_logprob": -0.11574211307600432, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.009589049965143204}, {"id": 516, "seek": 293034, "start": 2936.42, "end": 2942.6600000000003, "text": " into equations, for example, we can we can nail it. But turning going from the whole long textual", "tokens": [50668, 666, 11787, 11, 337, 1365, 11, 321, 393, 321, 393, 10173, 309, 13, 583, 6246, 516, 490, 264, 1379, 938, 2487, 901, 50980], "temperature": 0.0, "avg_logprob": -0.11574211307600432, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.009589049965143204}, {"id": 517, "seek": 293034, "start": 2942.6600000000003, "end": 2949.1400000000003, "text": " description into the equations is not something we've been able to do. Now we can. Now, in fact,", "tokens": [50980, 3855, 666, 264, 11787, 307, 406, 746, 321, 600, 668, 1075, 281, 360, 13, 823, 321, 393, 13, 823, 11, 294, 1186, 11, 51304], "temperature": 0.0, "avg_logprob": -0.11574211307600432, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.009589049965143204}, {"id": 518, "seek": 293034, "start": 2949.1400000000003, "end": 2954.1800000000003, "text": " in practice, when you use LLMs, one of the things that's terrible, you know, you use the for example,", "tokens": [51304, 294, 3124, 11, 562, 291, 764, 441, 43, 26386, 11, 472, 295, 264, 721, 300, 311, 6237, 11, 291, 458, 11, 291, 764, 264, 337, 1365, 11, 51556], "temperature": 0.0, "avg_logprob": -0.11574211307600432, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.009589049965143204}, {"id": 519, "seek": 295418, "start": 2954.2599999999998, "end": 2960.66, "text": " a chat notebook or the Wolfram plugin for chat GBT, it'll sometimes, you know, correctly untangle", "tokens": [50368, 257, 5081, 21060, 420, 264, 16634, 2356, 23407, 337, 5081, 26809, 51, 11, 309, 603, 2171, 11, 291, 458, 11, 8944, 1701, 7846, 50688], "temperature": 0.0, "avg_logprob": -0.11274954110137687, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0877397358417511}, {"id": 520, "seek": 295418, "start": 2960.66, "end": 2965.94, "text": " the word problem, you know, solve the equations correctly. And then at the last minute, give the", "tokens": [50688, 264, 1349, 1154, 11, 291, 458, 11, 5039, 264, 11787, 8944, 13, 400, 550, 412, 264, 1036, 3456, 11, 976, 264, 50952], "temperature": 0.0, "avg_logprob": -0.11274954110137687, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0877397358417511}, {"id": 521, "seek": 295418, "start": 2965.94, "end": 2971.62, "text": " wrong answer, because it tried to inject something that it thought it knew, and it got very confused.", "tokens": [50952, 2085, 1867, 11, 570, 309, 3031, 281, 10711, 746, 300, 309, 1194, 309, 2586, 11, 293, 309, 658, 588, 9019, 13, 51236], "temperature": 0.0, "avg_logprob": -0.11274954110137687, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0877397358417511}, {"id": 522, "seek": 295418, "start": 2971.62, "end": 2977.54, "text": " But anyway, so lots of use cases for kind of the LLMs, their interaction with computational", "tokens": [51236, 583, 4033, 11, 370, 3195, 295, 764, 3331, 337, 733, 295, 264, 441, 43, 26386, 11, 641, 9285, 365, 28270, 51532], "temperature": 0.0, "avg_logprob": -0.11274954110137687, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0877397358417511}, {"id": 523, "seek": 295418, "start": 2977.54, "end": 2984.02, "text": " language. And then the second piece, really quite a disjoint piece is why did the LLMs work in the", "tokens": [51532, 2856, 13, 400, 550, 264, 1150, 2522, 11, 534, 1596, 257, 717, 48613, 2522, 307, 983, 630, 264, 441, 43, 26386, 589, 294, 264, 51856], "temperature": 0.0, "avg_logprob": -0.11274954110137687, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0877397358417511}, {"id": 524, "seek": 298402, "start": 2984.02, "end": 2988.98, "text": " first place? This is a physics problem. And people should figure it out. And the results of", "tokens": [50364, 700, 1081, 30, 639, 307, 257, 10649, 1154, 13, 400, 561, 820, 2573, 309, 484, 13, 400, 264, 3542, 295, 50612], "temperature": 0.0, "avg_logprob": -0.08056290944417317, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0018840082921087742}, {"id": 525, "seek": 298402, "start": 2988.98, "end": 2994.2599999999998, "text": " figuring it out will be many important things. For example, probably most of what's inside a modern", "tokens": [50612, 15213, 309, 484, 486, 312, 867, 1021, 721, 13, 1171, 1365, 11, 1391, 881, 295, 437, 311, 1854, 257, 4363, 50876], "temperature": 0.0, "avg_logprob": -0.08056290944417317, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0018840082921087742}, {"id": 526, "seek": 298402, "start": 2994.2599999999998, "end": 2999.94, "text": " LLM doesn't need to be there. Most of what's it, you know, the actual structure you need to know", "tokens": [50876, 441, 43, 44, 1177, 380, 643, 281, 312, 456, 13, 4534, 295, 437, 311, 309, 11, 291, 458, 11, 264, 3539, 3877, 291, 643, 281, 458, 51160], "temperature": 0.0, "avg_logprob": -0.08056290944417317, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0018840082921087742}, {"id": 527, "seek": 298402, "start": 2999.94, "end": 3006.98, "text": " enough to be able to do sort of linguistic interface, plus kind of enough common sense to", "tokens": [51160, 1547, 281, 312, 1075, 281, 360, 1333, 295, 43002, 9226, 11, 1804, 733, 295, 1547, 2689, 2020, 281, 51512], "temperature": 0.0, "avg_logprob": -0.08056290944417317, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0018840082921087742}, {"id": 528, "seek": 298402, "start": 3006.98, "end": 3011.86, "text": " support that linguistic interface is probably quite tiny compared to a current LLM. And probably", "tokens": [51512, 1406, 300, 43002, 9226, 307, 1391, 1596, 5870, 5347, 281, 257, 2190, 441, 43, 44, 13, 400, 1391, 51756], "temperature": 0.0, "avg_logprob": -0.08056290944417317, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0018840082921087742}, {"id": 529, "seek": 301186, "start": 3011.94, "end": 3016.9, "text": " you can delegate all the kind of computation and detailed computational knowledge outside of the", "tokens": [50368, 291, 393, 40999, 439, 264, 733, 295, 24903, 293, 9942, 28270, 3601, 2380, 295, 264, 50616], "temperature": 0.0, "avg_logprob": -0.09223802657354446, "compression_ratio": 1.64, "no_speech_prob": 0.004208692815154791}, {"id": 530, "seek": 301186, "start": 3016.9, "end": 3022.1, "text": " LLM, which is an important thing in practice in terms of how much it costs to run an LLM,", "tokens": [50616, 441, 43, 44, 11, 597, 307, 364, 1021, 551, 294, 3124, 294, 2115, 295, 577, 709, 309, 5497, 281, 1190, 364, 441, 43, 44, 11, 50876], "temperature": 0.0, "avg_logprob": -0.09223802657354446, "compression_ratio": 1.64, "no_speech_prob": 0.004208692815154791}, {"id": 531, "seek": 301186, "start": 3022.1, "end": 3026.42, "text": " what kind of systems you need to run it on. But if we understand LLMs better and why they", "tokens": [50876, 437, 733, 295, 3652, 291, 643, 281, 1190, 309, 322, 13, 583, 498, 321, 1223, 441, 43, 26386, 1101, 293, 983, 436, 51092], "temperature": 0.0, "avg_logprob": -0.09223802657354446, "compression_ratio": 1.64, "no_speech_prob": 0.004208692815154791}, {"id": 532, "seek": 301186, "start": 3026.42, "end": 3030.42, "text": " work in the first place, we have a better chance to be able to resolve those kinds of things.", "tokens": [51092, 589, 294, 264, 700, 1081, 11, 321, 362, 257, 1101, 2931, 281, 312, 1075, 281, 14151, 729, 3685, 295, 721, 13, 51292], "temperature": 0.0, "avg_logprob": -0.09223802657354446, "compression_ratio": 1.64, "no_speech_prob": 0.004208692815154791}, {"id": 533, "seek": 301186, "start": 3030.42, "end": 3031.86, "text": " All right, I should stop there. Thanks.", "tokens": [51292, 1057, 558, 11, 286, 820, 1590, 456, 13, 2561, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09223802657354446, "compression_ratio": 1.64, "no_speech_prob": 0.004208692815154791}, {"id": 534, "seek": 303186, "start": 3032.82, "end": 3047.2200000000003, "text": " Very much. I think we have time for a couple of questions. I'd like to start with a quick one,", "tokens": [50412, 4372, 709, 13, 286, 519, 321, 362, 565, 337, 257, 1916, 295, 1651, 13, 286, 1116, 411, 281, 722, 365, 257, 1702, 472, 11, 51132], "temperature": 0.0, "avg_logprob": -0.1205294973710004, "compression_ratio": 1.5988700564971752, "no_speech_prob": 0.0042243944481015205}, {"id": 535, "seek": 303186, "start": 3047.2200000000003, "end": 3053.06, "text": " slightly out of left field. I think you've made a good case here for physicists becoming linguists.", "tokens": [51132, 4748, 484, 295, 1411, 2519, 13, 286, 519, 291, 600, 1027, 257, 665, 1389, 510, 337, 48716, 5617, 21766, 1751, 13, 51424], "temperature": 0.0, "avg_logprob": -0.1205294973710004, "compression_ratio": 1.5988700564971752, "no_speech_prob": 0.0042243944481015205}, {"id": 536, "seek": 303186, "start": 3053.6200000000003, "end": 3058.58, "text": " Is there something that physicists should be learning from linguists or linguists should", "tokens": [51452, 1119, 456, 746, 300, 48716, 820, 312, 2539, 490, 21766, 1751, 420, 21766, 1751, 820, 51700], "temperature": 0.0, "avg_logprob": -0.1205294973710004, "compression_ratio": 1.5988700564971752, "no_speech_prob": 0.0042243944481015205}, {"id": 537, "seek": 305858, "start": 3058.66, "end": 3065.38, "text": " be transitioning to physics? Physics can give us, I don't know whether it's linguistics,", "tokens": [50368, 312, 33777, 281, 10649, 30, 38355, 393, 976, 505, 11, 286, 500, 380, 458, 1968, 309, 311, 21766, 6006, 11, 50704], "temperature": 0.0, "avg_logprob": -0.09878544140887517, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.0061307684518396854}, {"id": 538, "seek": 305858, "start": 3065.38, "end": 3069.7799999999997, "text": " I don't know whether you call it that. I don't know what you call it. But this whole idea about", "tokens": [50704, 286, 500, 380, 458, 1968, 291, 818, 309, 300, 13, 286, 500, 380, 458, 437, 291, 818, 309, 13, 583, 341, 1379, 1558, 466, 50924], "temperature": 0.0, "avg_logprob": -0.09878544140887517, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.0061307684518396854}, {"id": 539, "seek": 305858, "start": 3069.7799999999997, "end": 3076.8199999999997, "text": " meaning and so on, what we're talking about, that's something that I think has now been exposed", "tokens": [50924, 3620, 293, 370, 322, 11, 437, 321, 434, 1417, 466, 11, 300, 311, 746, 300, 286, 519, 575, 586, 668, 9495, 51276], "temperature": 0.0, "avg_logprob": -0.09878544140887517, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.0061307684518396854}, {"id": 540, "seek": 305858, "start": 3076.8199999999997, "end": 3081.2999999999997, "text": " as something on which we can do experimental science on, on which we can apply physics.", "tokens": [51276, 382, 746, 322, 597, 321, 393, 360, 17069, 3497, 322, 11, 322, 597, 321, 393, 3079, 10649, 13, 51500], "temperature": 0.0, "avg_logprob": -0.09878544140887517, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.0061307684518396854}, {"id": 541, "seek": 308130, "start": 3081.3, "end": 3088.5800000000004, "text": " So I think that's the, I mean, in terms of, yeah, no, that's, I mean, it's, you know,", "tokens": [50364, 407, 286, 519, 300, 311, 264, 11, 286, 914, 11, 294, 2115, 295, 11, 1338, 11, 572, 11, 300, 311, 11, 286, 914, 11, 309, 311, 11, 291, 458, 11, 50728], "temperature": 0.0, "avg_logprob": -0.1284044879977986, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.004254327155649662}, {"id": 542, "seek": 308130, "start": 3089.78, "end": 3095.0600000000004, "text": " if you look at the history of physics, right, physics has been a fantastic export field.", "tokens": [50788, 498, 291, 574, 412, 264, 2503, 295, 10649, 11, 558, 11, 10649, 575, 668, 257, 5456, 10725, 2519, 13, 51052], "temperature": 0.0, "avg_logprob": -0.1284044879977986, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.004254327155649662}, {"id": 543, "seek": 308130, "start": 3095.0600000000004, "end": 3099.78, "text": " That's, you know, populated molecular biology, it's populated, you know, quantitative finance,", "tokens": [51052, 663, 311, 11, 291, 458, 11, 32998, 19046, 14956, 11, 309, 311, 32998, 11, 291, 458, 11, 27778, 10719, 11, 51288], "temperature": 0.0, "avg_logprob": -0.1284044879977986, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.004254327155649662}, {"id": 544, "seek": 308130, "start": 3099.78, "end": 3104.42, "text": " it's populated lots of kinds of things. It has every opportunity to populate this area", "tokens": [51288, 309, 311, 32998, 3195, 295, 3685, 295, 721, 13, 467, 575, 633, 2650, 281, 1665, 5256, 341, 1859, 51520], "temperature": 0.0, "avg_logprob": -0.1284044879977986, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.004254327155649662}, {"id": 545, "seek": 308130, "start": 3104.42, "end": 3110.6600000000003, "text": " and to populate and to really make some complete change to how one thinks about sort of meaning", "tokens": [51520, 293, 281, 1665, 5256, 293, 281, 534, 652, 512, 3566, 1319, 281, 577, 472, 7309, 466, 1333, 295, 3620, 51832], "temperature": 0.0, "avg_logprob": -0.1284044879977986, "compression_ratio": 1.8152610441767068, "no_speech_prob": 0.004254327155649662}, {"id": 546, "seek": 311066, "start": 3110.66, "end": 3115.54, "text": " and language and so on, I believe. Well, thank you. All right, let's go to the right first.", "tokens": [50364, 293, 2856, 293, 370, 322, 11, 286, 1697, 13, 1042, 11, 1309, 291, 13, 1057, 558, 11, 718, 311, 352, 281, 264, 558, 700, 13, 50608], "temperature": 0.0, "avg_logprob": -0.10804168037746263, "compression_ratio": 1.7220447284345048, "no_speech_prob": 0.0009710990125313401}, {"id": 547, "seek": 311066, "start": 3116.42, "end": 3120.18, "text": " Hi, so I've seen some of your talks on the Wolfram physics project as well,", "tokens": [50652, 2421, 11, 370, 286, 600, 1612, 512, 295, 428, 6686, 322, 264, 16634, 2356, 10649, 1716, 382, 731, 11, 50840], "temperature": 0.0, "avg_logprob": -0.10804168037746263, "compression_ratio": 1.7220447284345048, "no_speech_prob": 0.0009710990125313401}, {"id": 548, "seek": 311066, "start": 3120.18, "end": 3126.1, "text": " and I see these n-dimensional graphs that you often use, and they often really look like neural", "tokens": [50840, 293, 286, 536, 613, 297, 12, 18759, 24877, 300, 291, 2049, 764, 11, 293, 436, 2049, 534, 574, 411, 18161, 51136], "temperature": 0.0, "avg_logprob": -0.10804168037746263, "compression_ratio": 1.7220447284345048, "no_speech_prob": 0.0009710990125313401}, {"id": 549, "seek": 311066, "start": 3126.1, "end": 3130.8199999999997, "text": " networks. And so I wanted to ask if that was intentional or if there's some additional layers", "tokens": [51136, 9590, 13, 400, 370, 286, 1415, 281, 1029, 498, 300, 390, 21935, 420, 498, 456, 311, 512, 4497, 7914, 51372], "temperature": 0.0, "avg_logprob": -0.10804168037746263, "compression_ratio": 1.7220447284345048, "no_speech_prob": 0.0009710990125313401}, {"id": 550, "seek": 311066, "start": 3130.8199999999997, "end": 3136.1, "text": " of physics going on there. They have nothing to do with neural nets. So far as I know,", "tokens": [51372, 295, 10649, 516, 322, 456, 13, 814, 362, 1825, 281, 360, 365, 18161, 36170, 13, 407, 1400, 382, 286, 458, 11, 51636], "temperature": 0.0, "avg_logprob": -0.10804168037746263, "compression_ratio": 1.7220447284345048, "no_speech_prob": 0.0009710990125313401}, {"id": 551, "seek": 311066, "start": 3136.1, "end": 3139.94, "text": " although there's at least one startup that believes they do, and we'll see how that works out.", "tokens": [51636, 4878, 456, 311, 412, 1935, 472, 18578, 300, 12307, 436, 360, 11, 293, 321, 603, 536, 577, 300, 1985, 484, 13, 51828], "temperature": 0.0, "avg_logprob": -0.10804168037746263, "compression_ratio": 1.7220447284345048, "no_speech_prob": 0.0009710990125313401}, {"id": 552, "seek": 314066, "start": 3141.06, "end": 3149.14, "text": " This is an utterly disjoint discussion about how kind of microscopic", "tokens": [50384, 639, 307, 364, 30251, 717, 48613, 5017, 466, 577, 733, 295, 47897, 50788], "temperature": 0.0, "avg_logprob": -0.159517690359828, "compression_ratio": 1.518181818181818, "no_speech_prob": 0.002599966013804078}, {"id": 553, "seek": 314066, "start": 3149.14, "end": 3153.54, "text": " hypergraphs, you know, from them emerge space-time and quantum mechanics and so on.", "tokens": [50788, 9848, 34091, 82, 11, 291, 458, 11, 490, 552, 21511, 1901, 12, 3766, 293, 13018, 12939, 293, 370, 322, 13, 51008], "temperature": 0.0, "avg_logprob": -0.159517690359828, "compression_ratio": 1.518181818181818, "no_speech_prob": 0.002599966013804078}, {"id": 554, "seek": 314066, "start": 3154.8999999999996, "end": 3160.2599999999998, "text": " There is in fact a bizarre connection. Okay, this is to the deepest level of the rabbit hole", "tokens": [51076, 821, 307, 294, 1186, 257, 18265, 4984, 13, 1033, 11, 341, 307, 281, 264, 28288, 1496, 295, 264, 19509, 5458, 51344], "temperature": 0.0, "avg_logprob": -0.159517690359828, "compression_ratio": 1.518181818181818, "no_speech_prob": 0.002599966013804078}, {"id": 555, "seek": 314066, "start": 3160.2599999999998, "end": 3167.06, "text": " immediately. There's this thing we call the rouliad, which is the entangled limit of all", "tokens": [51344, 4258, 13, 821, 311, 341, 551, 321, 818, 264, 18450, 2081, 345, 11, 597, 307, 264, 948, 39101, 4948, 295, 439, 51684], "temperature": 0.0, "avg_logprob": -0.159517690359828, "compression_ratio": 1.518181818181818, "no_speech_prob": 0.002599966013804078}, {"id": 556, "seek": 316706, "start": 3167.06, "end": 3171.46, "text": " possible computations. Imagine you take all possible, let's say, Turing machines with all", "tokens": [50364, 1944, 2807, 763, 13, 11739, 291, 747, 439, 1944, 11, 718, 311, 584, 11, 314, 1345, 8379, 365, 439, 50584], "temperature": 0.0, "avg_logprob": -0.07762206097443898, "compression_ratio": 1.75, "no_speech_prob": 0.007059223484247923}, {"id": 557, "seek": 316706, "start": 3171.46, "end": 3177.94, "text": " possible initial states. You run them, and they're all non-deterministic. They all have all possible", "tokens": [50584, 1944, 5883, 4368, 13, 509, 1190, 552, 11, 293, 436, 434, 439, 2107, 12, 49136, 259, 3142, 13, 814, 439, 362, 439, 1944, 50908], "temperature": 0.0, "avg_logprob": -0.07762206097443898, "compression_ratio": 1.75, "no_speech_prob": 0.007059223484247923}, {"id": 558, "seek": 316706, "start": 3177.94, "end": 3183.86, "text": " rules. You run them, you get this big, messy thing. There's only one of it. It is the complete", "tokens": [50908, 4474, 13, 509, 1190, 552, 11, 291, 483, 341, 955, 11, 16191, 551, 13, 821, 311, 787, 472, 295, 309, 13, 467, 307, 264, 3566, 51204], "temperature": 0.0, "avg_logprob": -0.07762206097443898, "compression_ratio": 1.75, "no_speech_prob": 0.007059223484247923}, {"id": 559, "seek": 316706, "start": 3183.86, "end": 3190.74, "text": " representation of all possible computations. And then that, I claim, is sort of the ultimate", "tokens": [51204, 10290, 295, 439, 1944, 2807, 763, 13, 400, 550, 300, 11, 286, 3932, 11, 307, 1333, 295, 264, 9705, 51548], "temperature": 0.0, "avg_logprob": -0.07762206097443898, "compression_ratio": 1.75, "no_speech_prob": 0.007059223484247923}, {"id": 560, "seek": 319074, "start": 3190.74, "end": 3196.66, "text": " foundation of physics and mathematics, actually. And our physical universe ends up being,", "tokens": [50364, 7030, 295, 10649, 293, 18666, 11, 767, 13, 400, 527, 4001, 6445, 5314, 493, 885, 11, 50660], "temperature": 0.0, "avg_logprob": -0.0994655547603484, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.018991678953170776}, {"id": 561, "seek": 319074, "start": 3197.9399999999996, "end": 3204.58, "text": " we have to exist within that. And so our physical universe ends up being our kind of, our sampling", "tokens": [50724, 321, 362, 281, 2514, 1951, 300, 13, 400, 370, 527, 4001, 6445, 5314, 493, 885, 527, 733, 295, 11, 527, 21179, 51056], "temperature": 0.0, "avg_logprob": -0.0994655547603484, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.018991678953170776}, {"id": 562, "seek": 319074, "start": 3204.58, "end": 3210.5, "text": " of that rouliad object. And here's the fascinating fact, at least I think it's interesting, is that", "tokens": [51056, 295, 300, 18450, 2081, 345, 2657, 13, 400, 510, 311, 264, 10343, 1186, 11, 412, 1935, 286, 519, 309, 311, 1880, 11, 307, 300, 51352], "temperature": 0.0, "avg_logprob": -0.0994655547603484, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.018991678953170776}, {"id": 563, "seek": 319074, "start": 3210.5, "end": 3217.22, "text": " if you assume that we as observers of the rouliad have two characteristics. One, we are computationally", "tokens": [51352, 498, 291, 6552, 300, 321, 382, 48090, 295, 264, 18450, 2081, 345, 362, 732, 10891, 13, 1485, 11, 321, 366, 24903, 379, 51688], "temperature": 0.0, "avg_logprob": -0.0994655547603484, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.018991678953170776}, {"id": 564, "seek": 321722, "start": 3217.22, "end": 3222.1, "text": " bounded. Two, we believe we are persistent in time. We believe we have a single thread of", "tokens": [50364, 37498, 13, 4453, 11, 321, 1697, 321, 366, 24315, 294, 565, 13, 492, 1697, 321, 362, 257, 2167, 7207, 295, 50608], "temperature": 0.0, "avg_logprob": -0.0862214721251871, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.002335498807951808}, {"id": 565, "seek": 321722, "start": 3222.1, "end": 3227.8599999999997, "text": " experience. Those two attributes alone are sufficient to give you, not just qualitatively,", "tokens": [50608, 1752, 13, 3950, 732, 17212, 3312, 366, 11563, 281, 976, 291, 11, 406, 445, 31312, 356, 11, 50896], "temperature": 0.0, "avg_logprob": -0.0862214721251871, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.002335498807951808}, {"id": 566, "seek": 321722, "start": 3227.8599999999997, "end": 3233.7, "text": " but exactly, general relativity and quantum mechanics. That's kind of exciting, because it", "tokens": [50896, 457, 2293, 11, 2674, 45675, 293, 13018, 12939, 13, 663, 311, 733, 295, 4670, 11, 570, 309, 51188], "temperature": 0.0, "avg_logprob": -0.0862214721251871, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.002335498807951808}, {"id": 567, "seek": 321722, "start": 3233.7, "end": 3239.54, "text": " tells you that it didn't need to be that way. The aliens who don't have those characteristics", "tokens": [51188, 5112, 291, 300, 309, 994, 380, 643, 281, 312, 300, 636, 13, 440, 21594, 567, 500, 380, 362, 729, 10891, 51480], "temperature": 0.0, "avg_logprob": -0.0862214721251871, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.002335498807951808}, {"id": 568, "seek": 321722, "start": 3239.54, "end": 3242.98, "text": " don't have to have general relativity and quantum mechanics. But it kind of gives you, I mean,", "tokens": [51480, 500, 380, 362, 281, 362, 2674, 45675, 293, 13018, 12939, 13, 583, 309, 733, 295, 2709, 291, 11, 286, 914, 11, 51652], "temperature": 0.0, "avg_logprob": -0.0862214721251871, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.002335498807951808}, {"id": 569, "seek": 324298, "start": 3242.98, "end": 3248.18, "text": " this is a huge condensation of a very large amount of stuff. But that's, so okay, how does", "tokens": [50364, 341, 307, 257, 2603, 2224, 35292, 295, 257, 588, 2416, 2372, 295, 1507, 13, 583, 300, 311, 11, 370, 1392, 11, 577, 775, 50624], "temperature": 0.0, "avg_logprob": -0.10586917300184234, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.0019409960368648171}, {"id": 570, "seek": 324298, "start": 3248.18, "end": 3253.22, "text": " that relate to all of this? When I was showing you those weird cat pictures and things, the,", "tokens": [50624, 300, 10961, 281, 439, 295, 341, 30, 1133, 286, 390, 4099, 291, 729, 3657, 3857, 5242, 293, 721, 11, 264, 11, 50876], "temperature": 0.0, "avg_logprob": -0.10586917300184234, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.0019409960368648171}, {"id": 571, "seek": 324298, "start": 3253.78, "end": 3259.22, "text": " this is a, one of the reasons I was studying weird cat pictures is because this is a way of", "tokens": [50904, 341, 307, 257, 11, 472, 295, 264, 4112, 286, 390, 7601, 3657, 3857, 5242, 307, 570, 341, 307, 257, 636, 295, 51176], "temperature": 0.0, "avg_logprob": -0.10586917300184234, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.0019409960368648171}, {"id": 572, "seek": 324298, "start": 3259.22, "end": 3264.82, "text": " understanding sort of different slices of this roulial space concept. There's much more to say", "tokens": [51176, 3701, 1333, 295, 819, 19793, 295, 341, 18450, 2081, 304, 1901, 3410, 13, 821, 311, 709, 544, 281, 584, 51456], "temperature": 0.0, "avg_logprob": -0.10586917300184234, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.0019409960368648171}, {"id": 573, "seek": 324298, "start": 3264.82, "end": 3270.42, "text": " about this. That's a way too, way too brief a description. Okay, let's take a question from", "tokens": [51456, 466, 341, 13, 663, 311, 257, 636, 886, 11, 636, 886, 5353, 257, 3855, 13, 1033, 11, 718, 311, 747, 257, 1168, 490, 51736], "temperature": 0.0, "avg_logprob": -0.10586917300184234, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.0019409960368648171}, {"id": 574, "seek": 327042, "start": 3270.42, "end": 3276.42, "text": " the left now. Yeah, hi. So I've tried to use LLMs in research so far without great success.", "tokens": [50364, 264, 1411, 586, 13, 865, 11, 4879, 13, 407, 286, 600, 3031, 281, 764, 441, 43, 26386, 294, 2132, 370, 1400, 1553, 869, 2245, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11027939375056776, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.004823318216949701}, {"id": 575, "seek": 327042, "start": 3276.42, "end": 3280.1, "text": " I'm a theoretical physicist. Something that would be, there's a weird echo here, I don't know.", "tokens": [50664, 286, 478, 257, 20864, 42466, 13, 6595, 300, 576, 312, 11, 456, 311, 257, 3657, 14300, 510, 11, 286, 500, 380, 458, 13, 50848], "temperature": 0.0, "avg_logprob": -0.11027939375056776, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.004823318216949701}, {"id": 576, "seek": 327042, "start": 3280.1, "end": 3284.98, "text": " Anyway, sorry, something that would be really useful would be if I could have something where,", "tokens": [50848, 5684, 11, 2597, 11, 746, 300, 576, 312, 534, 4420, 576, 312, 498, 286, 727, 362, 746, 689, 11, 51092], "temperature": 0.0, "avg_logprob": -0.11027939375056776, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.004823318216949701}, {"id": 577, "seek": 327042, "start": 3285.78, "end": 3292.34, "text": " you know, 300 page paper, I don't know, by Edward and Pierce. And I can ask it, can you give me a", "tokens": [51132, 291, 458, 11, 6641, 3028, 3035, 11, 286, 500, 380, 458, 11, 538, 18456, 293, 45432, 13, 400, 286, 393, 1029, 309, 11, 393, 291, 976, 385, 257, 51460], "temperature": 0.0, "avg_logprob": -0.11027939375056776, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.004823318216949701}, {"id": 578, "seek": 327042, "start": 3292.34, "end": 3297.94, "text": " one page summary of that, you know, where it would be correct and where it would already kind of know", "tokens": [51460, 472, 3028, 12691, 295, 300, 11, 291, 458, 11, 689, 309, 576, 312, 3006, 293, 689, 309, 576, 1217, 733, 295, 458, 51740], "temperature": 0.0, "avg_logprob": -0.11027939375056776, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.004823318216949701}, {"id": 579, "seek": 329794, "start": 3297.94, "end": 3302.1, "text": " from talking me to before, like what are the kind of things that I know and then I don't know.", "tokens": [50364, 490, 1417, 385, 281, 949, 11, 411, 437, 366, 264, 733, 295, 721, 300, 286, 458, 293, 550, 286, 500, 380, 458, 13, 50572], "temperature": 0.0, "avg_logprob": -0.10279868249179555, "compression_ratio": 1.6507462686567165, "no_speech_prob": 0.005579906050115824}, {"id": 580, "seek": 329794, "start": 3302.1, "end": 3305.62, "text": " So I mean, how far are we from that? Well, you know, for example, in our company,", "tokens": [50572, 407, 286, 914, 11, 577, 1400, 366, 321, 490, 300, 30, 1042, 11, 291, 458, 11, 337, 1365, 11, 294, 527, 2237, 11, 50748], "temperature": 0.0, "avg_logprob": -0.10279868249179555, "compression_ratio": 1.6507462686567165, "no_speech_prob": 0.005579906050115824}, {"id": 581, "seek": 329794, "start": 3306.5, "end": 3312.34, "text": " you know, someone makes a daily digest of interesting papers about LLMs. Okay, and I got fed up", "tokens": [50792, 291, 458, 11, 1580, 1669, 257, 5212, 13884, 295, 1880, 10577, 466, 441, 43, 26386, 13, 1033, 11, 293, 286, 658, 4636, 493, 51084], "temperature": 0.0, "avg_logprob": -0.10279868249179555, "compression_ratio": 1.6507462686567165, "no_speech_prob": 0.005579906050115824}, {"id": 582, "seek": 329794, "start": 3312.34, "end": 3315.62, "text": " trying to read the abstracts, because every abstract is written differently. They're very", "tokens": [51084, 1382, 281, 1401, 264, 12649, 82, 11, 570, 633, 12649, 307, 3720, 7614, 13, 814, 434, 588, 51248], "temperature": 0.0, "avg_logprob": -0.10279868249179555, "compression_ratio": 1.6507462686567165, "no_speech_prob": 0.005579906050115824}, {"id": 583, "seek": 329794, "start": 3315.62, "end": 3320.58, "text": " ponderous in many cases. I said, just get the frigging LLMs and make a two sentence summary", "tokens": [51248, 280, 8548, 563, 294, 867, 3331, 13, 286, 848, 11, 445, 483, 264, 34697, 3249, 441, 43, 26386, 293, 652, 257, 732, 8174, 12691, 51496], "temperature": 0.0, "avg_logprob": -0.10279868249179555, "compression_ratio": 1.6507462686567165, "no_speech_prob": 0.005579906050115824}, {"id": 584, "seek": 329794, "start": 3320.58, "end": 3325.94, "text": " of every paper. It works great. I mean, you can scan down this thing really quickly. The fact that", "tokens": [51496, 295, 633, 3035, 13, 467, 1985, 869, 13, 286, 914, 11, 291, 393, 11049, 760, 341, 551, 534, 2661, 13, 440, 1186, 300, 51764], "temperature": 0.0, "avg_logprob": -0.10279868249179555, "compression_ratio": 1.6507462686567165, "no_speech_prob": 0.005579906050115824}, {"id": 585, "seek": 332594, "start": 3325.94, "end": 3331.94, "text": " the LLMs text is rather boring is actually good, because all the text is consistent, and you kind", "tokens": [50364, 264, 441, 43, 26386, 2487, 307, 2831, 9989, 307, 767, 665, 11, 570, 439, 264, 2487, 307, 8398, 11, 293, 291, 733, 50664], "temperature": 0.0, "avg_logprob": -0.0823518715652765, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0017324973596259952}, {"id": 586, "seek": 332594, "start": 3331.94, "end": 3337.14, "text": " of can just see what's happening. Now, you know, do I get the essence of every paper correctly?", "tokens": [50664, 295, 393, 445, 536, 437, 311, 2737, 13, 823, 11, 291, 458, 11, 360, 286, 483, 264, 12801, 295, 633, 3035, 8944, 30, 50924], "temperature": 0.0, "avg_logprob": -0.0823518715652765, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0017324973596259952}, {"id": 587, "seek": 332594, "start": 3337.14, "end": 3342.26, "text": " Maybe not. But that's a statistical thing anyway, I might miss it from the abstract too. So that's", "tokens": [50924, 2704, 406, 13, 583, 300, 311, 257, 22820, 551, 4033, 11, 286, 1062, 1713, 309, 490, 264, 12649, 886, 13, 407, 300, 311, 51180], "temperature": 0.0, "avg_logprob": -0.0823518715652765, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0017324973596259952}, {"id": 588, "seek": 332594, "start": 3342.26, "end": 3350.58, "text": " a pretty good use case that I recommend, actually. In terms of the can you, if you want it to summarize", "tokens": [51180, 257, 1238, 665, 764, 1389, 300, 286, 2748, 11, 767, 13, 682, 2115, 295, 264, 393, 291, 11, 498, 291, 528, 309, 281, 20858, 51596], "temperature": 0.0, "avg_logprob": -0.0823518715652765, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0017324973596259952}, {"id": 589, "seek": 335058, "start": 3350.58, "end": 3356.2599999999998, "text": " for you, particularly, I think that's coming. And the, you know, kind of AI tutoring system that", "tokens": [50364, 337, 291, 11, 4098, 11, 286, 519, 300, 311, 1348, 13, 400, 264, 11, 291, 458, 11, 733, 295, 7318, 44410, 1185, 300, 50648], "temperature": 0.0, "avg_logprob": -0.10628612642365742, "compression_ratio": 1.8853754940711462, "no_speech_prob": 0.1594020277261734}, {"id": 590, "seek": 335058, "start": 3356.2599999999998, "end": 3362.5, "text": " we're building, that's kind of one of the big ideas is know the student and be able to figure out,", "tokens": [50648, 321, 434, 2390, 11, 300, 311, 733, 295, 472, 295, 264, 955, 3487, 307, 458, 264, 3107, 293, 312, 1075, 281, 2573, 484, 11, 50960], "temperature": 0.0, "avg_logprob": -0.10628612642365742, "compression_ratio": 1.8853754940711462, "no_speech_prob": 0.1594020277261734}, {"id": 591, "seek": 335058, "start": 3363.14, "end": 3367.2999999999997, "text": " first of all, how is the student confused? Because one of the things that you typically can't do", "tokens": [50992, 700, 295, 439, 11, 577, 307, 264, 3107, 9019, 30, 1436, 472, 295, 264, 721, 300, 291, 5850, 393, 380, 360, 51200], "temperature": 0.0, "avg_logprob": -0.10628612642365742, "compression_ratio": 1.8853754940711462, "no_speech_prob": 0.1594020277261734}, {"id": 592, "seek": 335058, "start": 3367.2999999999997, "end": 3372.02, "text": " in sort of watching what a student does is watching the working that the student follows.", "tokens": [51200, 294, 1333, 295, 1976, 437, 257, 3107, 775, 307, 1976, 264, 1364, 300, 264, 3107, 10002, 13, 51436], "temperature": 0.0, "avg_logprob": -0.10628612642365742, "compression_ratio": 1.8853754940711462, "no_speech_prob": 0.1594020277261734}, {"id": 593, "seek": 335058, "start": 3372.02, "end": 3377.7, "text": " But you can with an LLM, and you can kind of see, you know, how is the student confused as the", "tokens": [51436, 583, 291, 393, 365, 364, 441, 43, 44, 11, 293, 291, 393, 733, 295, 536, 11, 291, 458, 11, 577, 307, 264, 3107, 9019, 382, 264, 51720], "temperature": 0.0, "avg_logprob": -0.10628612642365742, "compression_ratio": 1.8853754940711462, "no_speech_prob": 0.1594020277261734}, {"id": 594, "seek": 337770, "start": 3377.7, "end": 3382.98, "text": " student is doing their work? And then, so then the question is, will it come to the point where,", "tokens": [50364, 3107, 307, 884, 641, 589, 30, 400, 550, 11, 370, 550, 264, 1168, 307, 11, 486, 309, 808, 281, 264, 935, 689, 11, 50628], "temperature": 0.0, "avg_logprob": -0.12121041615804036, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.007254838943481445}, {"id": 595, "seek": 337770, "start": 3382.98, "end": 3388.8199999999997, "text": " you know, the LLM will know enough about me from having read, I mean, me personally, I've put 50", "tokens": [50628, 291, 458, 11, 264, 441, 43, 44, 486, 458, 1547, 466, 385, 490, 1419, 1401, 11, 286, 914, 11, 385, 5665, 11, 286, 600, 829, 2625, 50920], "temperature": 0.0, "avg_logprob": -0.12121041615804036, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.007254838943481445}, {"id": 596, "seek": 337770, "start": 3388.8199999999997, "end": 3396.1, "text": " million words out there. So it's, I'm pretty easy to learn about. And we're trying to get an LLM to", "tokens": [50920, 2459, 2283, 484, 456, 13, 407, 309, 311, 11, 286, 478, 1238, 1858, 281, 1466, 466, 13, 400, 321, 434, 1382, 281, 483, 364, 441, 43, 44, 281, 51284], "temperature": 0.0, "avg_logprob": -0.12121041615804036, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.007254838943481445}, {"id": 597, "seek": 337770, "start": 3396.1, "end": 3403.8599999999997, "text": " be me, so to speak, which will save time, maybe, maybe not. But anyway, the point is,", "tokens": [51284, 312, 385, 11, 370, 281, 1710, 11, 597, 486, 3155, 565, 11, 1310, 11, 1310, 406, 13, 583, 4033, 11, 264, 935, 307, 11, 51672], "temperature": 0.0, "avg_logprob": -0.12121041615804036, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.007254838943481445}, {"id": 598, "seek": 340386, "start": 3404.58, "end": 3412.5, "text": " I'm, you know, given that the LLM knows about you, I think there is a very good chance that the LLM", "tokens": [50400, 286, 478, 11, 291, 458, 11, 2212, 300, 264, 441, 43, 44, 3255, 466, 291, 11, 286, 519, 456, 307, 257, 588, 665, 2931, 300, 264, 441, 43, 44, 50796], "temperature": 0.0, "avg_logprob": -0.11092164235956528, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005160384811460972}, {"id": 599, "seek": 340386, "start": 3412.5, "end": 3417.2200000000003, "text": " will be able to say the one thing you need to know, because you're confused about this or you don't", "tokens": [50796, 486, 312, 1075, 281, 584, 264, 472, 551, 291, 643, 281, 458, 11, 570, 291, 434, 9019, 466, 341, 420, 291, 500, 380, 51032], "temperature": 0.0, "avg_logprob": -0.11092164235956528, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005160384811460972}, {"id": 600, "seek": 340386, "start": 3417.2200000000003, "end": 3422.02, "text": " know this, is this one fact. And you say, oh, that's the thing I want to know from that paper, all the", "tokens": [51032, 458, 341, 11, 307, 341, 472, 1186, 13, 400, 291, 584, 11, 1954, 11, 300, 311, 264, 551, 286, 528, 281, 458, 490, 300, 3035, 11, 439, 264, 51272], "temperature": 0.0, "avg_logprob": -0.11092164235956528, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005160384811460972}, {"id": 601, "seek": 340386, "start": 3422.02, "end": 3427.1400000000003, "text": " other stuff is irrelevant. I think that's pretty realistic. And I think it's reasonably short-term.", "tokens": [51272, 661, 1507, 307, 28682, 13, 286, 519, 300, 311, 1238, 12465, 13, 400, 286, 519, 309, 311, 23551, 2099, 12, 7039, 13, 51528], "temperature": 0.0, "avg_logprob": -0.11092164235956528, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005160384811460972}, {"id": 602, "seek": 340386, "start": 3427.1400000000003, "end": 3431.86, "text": " But you've got to understand, like everything with machine learning, it's kind of an 80% success,", "tokens": [51528, 583, 291, 600, 658, 281, 1223, 11, 411, 1203, 365, 3479, 2539, 11, 309, 311, 733, 295, 364, 4688, 4, 2245, 11, 51764], "temperature": 0.0, "avg_logprob": -0.11092164235956528, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.005160384811460972}, {"id": 603, "seek": 343186, "start": 3431.86, "end": 3437.1400000000003, "text": " 90% success story. And whenever you have a situation, like looking at these abstracts, where,", "tokens": [50364, 4289, 4, 2245, 1657, 13, 400, 5699, 291, 362, 257, 2590, 11, 411, 1237, 412, 613, 12649, 82, 11, 689, 11, 50628], "temperature": 0.0, "avg_logprob": -0.06537580490112305, "compression_ratio": 1.6350877192982456, "no_speech_prob": 0.002656778320670128}, {"id": 604, "seek": 343186, "start": 3437.1400000000003, "end": 3441.94, "text": " you know, if I notice an abstract that looks really interesting, it's a win. If I miss one,", "tokens": [50628, 291, 458, 11, 498, 286, 3449, 364, 12649, 300, 1542, 534, 1880, 11, 309, 311, 257, 1942, 13, 759, 286, 1713, 472, 11, 50868], "temperature": 0.0, "avg_logprob": -0.06537580490112305, "compression_ratio": 1.6350877192982456, "no_speech_prob": 0.002656778320670128}, {"id": 605, "seek": 343186, "start": 3441.94, "end": 3447.46, "text": " it's not a disaster. That's a good use case. If it's a case where you're trying to do the next", "tokens": [50868, 309, 311, 406, 257, 11293, 13, 663, 311, 257, 665, 764, 1389, 13, 759, 309, 311, 257, 1389, 689, 291, 434, 1382, 281, 360, 264, 958, 51144], "temperature": 0.0, "avg_logprob": -0.06537580490112305, "compression_ratio": 1.6350877192982456, "no_speech_prob": 0.002656778320670128}, {"id": 606, "seek": 343186, "start": 3447.46, "end": 3453.86, "text": " great, you know, precise physics calculation, it's probably a big lose to use an LLM where it", "tokens": [51144, 869, 11, 291, 458, 11, 13600, 10649, 17108, 11, 309, 311, 1391, 257, 955, 3624, 281, 764, 364, 441, 43, 44, 689, 309, 51464], "temperature": 0.0, "avg_logprob": -0.06537580490112305, "compression_ratio": 1.6350877192982456, "no_speech_prob": 0.002656778320670128}, {"id": 607, "seek": 343186, "start": 3453.86, "end": 3460.82, "text": " might be wrong 10% of the time, you don't know which 10%. Okay, I understand that there are", "tokens": [51464, 1062, 312, 2085, 1266, 4, 295, 264, 565, 11, 291, 500, 380, 458, 597, 1266, 6856, 1033, 11, 286, 1223, 300, 456, 366, 51812], "temperature": 0.0, "avg_logprob": -0.06537580490112305, "compression_ratio": 1.6350877192982456, "no_speech_prob": 0.002656778320670128}, {"id": 608, "seek": 346082, "start": 3460.9, "end": 3465.46, "text": " many, many questions, but at some point, everyone does have to go home, unfortunately.", "tokens": [50368, 867, 11, 867, 1651, 11, 457, 412, 512, 935, 11, 1518, 775, 362, 281, 352, 1280, 11, 7015, 13, 50596], "temperature": 0.0, "avg_logprob": -0.13720519491966734, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.0024588597007095814}, {"id": 609, "seek": 346082, "start": 3465.46, "end": 3476.7400000000002, "text": " So I'm afraid we're going to have to cut off the questioning there. Let's thank our speaker again.", "tokens": [50596, 407, 286, 478, 4638, 321, 434, 516, 281, 362, 281, 1723, 766, 264, 21257, 456, 13, 961, 311, 1309, 527, 8145, 797, 13, 51160], "temperature": 0.0, "avg_logprob": -0.13720519491966734, "compression_ratio": 1.3503649635036497, "no_speech_prob": 0.0024588597007095814}], "language": "en"}