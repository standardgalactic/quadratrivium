WEBVTT

00:00.000 --> 00:04.320
So, our first speaker is Yann LeCun.

00:04.320 --> 00:10.280
He's currently the Silver Professor at the current Institute of Mathematical Sciences

00:10.280 --> 00:16.880
at NYU, where he is the founding director of the NYU Center for Data Science.

00:16.880 --> 00:22.800
He is also affiliated with MEDA, formerly known as Facebook, as the Vice President and Chief

00:22.800 --> 00:25.000
AI Scientist there.

00:25.000 --> 00:29.720
By the way, MEDA just released one of its large language models just a few days ago called

00:29.720 --> 00:34.120
LAMAS II, which I encourage you all to explore.

00:34.120 --> 00:39.360
And this could be a very long introduction because Yann has a very long resume, but the

00:39.360 --> 00:45.160
one thing I wanted to highlight is that Yann was the recipient of the 2018 Turing Award.

00:45.160 --> 00:48.600
For those of you who aren't familiar with it, it's kind of like the Nobel Prize in Computer

00:48.600 --> 00:55.480
Science, and he received that for his work on deep learning and convolutional neural networks.

00:55.480 --> 00:59.440
And something interesting I learned is a lot of the work he did, early work on convolutional

00:59.440 --> 01:03.580
neural networks, was actually when he was at Bell Labs, which is something physicists

01:03.580 --> 01:06.200
know quite well, I think.

01:06.200 --> 01:10.880
So I think you're all eager to hear from Yann, so I'm going to hand it over to him here.

01:10.880 --> 01:11.880
Thank you.

01:11.880 --> 01:15.840
Okay, I'm going to talk about, so this is going to be a somewhat technical talk, but

01:15.840 --> 01:22.840
not very technical, but to tell you less about the possibilities that are offered by LLM

01:22.840 --> 01:24.640
and more about their limitations.

01:24.640 --> 01:29.720
And basically tell you about what I think is coming next, though at least what I'm working

01:29.720 --> 01:32.400
towards coming next.

01:32.400 --> 01:37.160
And the first thing we should realize is that machine learning really sucks compared to what

01:37.160 --> 01:39.080
we observe in humans and animals.

01:39.080 --> 01:43.760
The capabilities of the learning systems that we have today are really terrible.

01:43.760 --> 01:48.200
Humans and animals can run new tasks really quickly, understand how the world works, they

01:48.200 --> 01:53.280
can reason, they can plan, they have some level of common sense.

01:53.280 --> 01:58.760
Their behavior is driven by objectives or drives, which is not the case for auto-reversed

01:58.760 --> 01:59.760
LLMs.

01:59.760 --> 02:05.920
But there is one thing that both the biological world and the recent machine learning world

02:05.920 --> 02:09.440
have had in common is the use of cell supervised learning.

02:09.440 --> 02:13.240
And really cell supervised learning has taken over the world.

02:13.240 --> 02:17.720
For both applications in text and natural language understanding, for images, videos,

02:17.720 --> 02:21.440
3D models, speech, protein folding, all that stuff.

02:21.440 --> 02:23.200
What is cell supervised learning really?

02:23.200 --> 02:29.520
It's sort of completion really, learning to fill in the blanks, right?

02:29.520 --> 02:36.480
So the way it's used in the context of natural language understanding or processing is you

02:36.480 --> 02:43.520
take a piece of text, you mask part of it by removing some other words, masking, replacing

02:43.520 --> 02:46.320
them by blank markers, for example.

02:46.320 --> 02:51.520
And then so think of it as a type of corruption, it doesn't have to be the masking but it could

02:51.520 --> 02:53.560
be other types of corruptions.

02:53.560 --> 02:59.240
And then you train some gigantic neural net to predict the words that are missing.

02:59.240 --> 03:05.840
And you just measure the reconstruction error basically on the parts that were missing.

03:05.840 --> 03:14.280
In the process of doing so, that system learns to represent text in a way that allows it

03:14.280 --> 03:26.480
to store or represent meaning, grammar, everything, syntax, semantics, everything there is to

03:26.480 --> 03:32.760
represent about language in the internal representation, which you can subsequently use for any downstream

03:32.760 --> 03:37.520
task like say translation or topic classification or something of that type.

03:37.520 --> 03:43.000
So this works amazingly well in the context of text, particularly because text is easy

03:43.080 --> 03:49.120
to predict with uncertainty, you can never predict exactly what word will appear at a

03:49.120 --> 03:54.440
particular location, but what you can do is predict some sort of probability distribution

03:54.440 --> 03:56.440
of all possible words in the dictionary.

03:56.440 --> 04:01.200
And you can do this because there's only a finite number of words in your dictionary

04:01.200 --> 04:02.720
or tokens.

04:02.720 --> 04:07.360
And so you can compute this distribution easily and handle uncertainty in the prediction pretty

04:07.360 --> 04:08.360
well.

04:08.920 --> 04:13.480
More generally, self-supervised learning is really sort of learning to capture dependencies

04:13.480 --> 04:17.600
between inputs.

04:17.600 --> 04:23.520
And so if you wanted to apply this to the problem of video prediction, for example, you would

04:23.520 --> 04:28.400
show a segment of video to a system and then ask it to predict what's going to happen next,

04:28.400 --> 04:32.120
for example, in the video and then reveal the future of the video.

04:32.120 --> 04:34.760
And again, I apologize for the colors.

04:35.760 --> 04:39.480
And then the system could adapt itself so that it does a better job at predicting what

04:39.480 --> 04:40.680
happened next in the video.

04:40.680 --> 04:44.840
Now, unfortunately, it's much harder to do for video than it is for text.

04:44.840 --> 04:51.080
So much harder than it might require other methods than the type of generative methods

04:51.080 --> 04:52.360
that work well for text.

04:52.360 --> 04:53.760
I'll come back to this.

04:53.760 --> 04:59.440
So speaking of generative methods, but generative AI and autoregressive language models is something

04:59.440 --> 05:03.880
that many of us have been hearing about recently.

05:05.280 --> 05:06.880
What is it?

05:06.880 --> 05:11.560
Probably most of you know already, but essentially the way you train them is very similar to

05:11.560 --> 05:12.760
the self-supervised learning.

05:12.760 --> 05:16.440
It's in fact a special case of self-supervised learning method I just mentioned.

05:16.440 --> 05:23.760
You take a sequence of tokens, words, whatever it is, a sequence of vectors.

05:23.760 --> 05:26.920
As long as you can turn things into vectors, you're OK.

05:26.920 --> 05:33.240
And then you only mask the last one and train a system to predict the last token in the

05:33.240 --> 05:34.240
sequence.

05:34.240 --> 05:40.440
I mean, technically, you do more than that, but that's what it comes down to in the end.

05:40.440 --> 05:44.200
And once you have a system that has been trained to produce the next token, you can use it

05:44.200 --> 05:50.480
autoregressively, recursively, basically to predict then the next next token and et cetera.

05:50.480 --> 05:55.080
So you predict the next token, you shift it into the input, then predict the next next

05:55.080 --> 05:58.360
token, shift that into the input, et cetera.

05:58.360 --> 06:00.200
And that's called autoregressive prediction.

06:00.200 --> 06:06.160
It's an old concept going back to signal processing many years ago, many decades ago, as a matter

06:06.160 --> 06:07.160
of fact.

06:07.160 --> 06:12.320
So nothing new there, but that allows the system to basically predict one token after the other

06:12.320 --> 06:14.440
and generate text.

06:14.440 --> 06:19.640
So those things work amazingly well.

06:19.640 --> 06:20.640
Performance is really amazing.

06:20.640 --> 06:24.720
The fact that, you know, they're trained only on text, even though on enormous amounts

06:24.720 --> 06:32.880
of text, but only on text, the amount of knowledge, if you want, that they capture from text is

06:32.880 --> 06:37.760
pretty amazing and surprised a lot of people.

06:37.760 --> 06:43.080
Those systems typically have billions or up to hundreds of billions of parameters.

06:43.080 --> 06:47.120
They train typically on one to two trillion tokens.

06:47.120 --> 06:48.120
Sometimes more.

06:48.200 --> 06:58.360
Their input window is anywhere between 2000 and maybe a few tens of thousands of tokens

06:58.360 --> 07:01.760
for their context window.

07:01.760 --> 07:06.800
And there's been a long history of such models that have been put out, the sort of GPT family,

07:06.800 --> 07:13.920
starting with GPT-123, from FAIR, there's been Blunderbot, Galactica, Llama, Version

07:14.720 --> 07:20.040
2 that just came out this week, Alpaca from Stanford, which is a fine-tuned version of

07:20.040 --> 07:26.240
Llama, Llama, Version 1, Lambda and Bard from Google, Chinche from DeepMind, you know, and

07:26.240 --> 07:31.160
of course, GPDT, GPT-4 from OpenAI.

07:31.160 --> 07:37.640
And they're really good at, as writing aids, but they have really limited knowledge of

07:37.640 --> 07:42.600
the underlying reality because they're purely trained from text, at least for the vast majority

07:42.600 --> 07:43.600
of them.

07:43.600 --> 07:47.520
And they really have no common sense or very limited common sense, and they have limited

07:47.520 --> 07:52.000
abilities to plan their answers because the answers are produced autoregressively.

07:52.000 --> 07:58.680
But still, it's pretty impressive how they work, so, as was mentioned, my colleagues

07:58.680 --> 08:02.840
just put out an open source LLM called Llama 2.

08:02.840 --> 08:08.960
There is three versions of this at the moment, 7 billion, 13 billion and 70 billion parameters.

08:08.960 --> 08:12.480
The license is fairly liberal, so it can be used commercially if you want.

08:13.480 --> 08:17.960
If you want to start a startup and use it as a business, you can.

08:17.960 --> 08:25.040
It's also available on sort of various cloud services, easy to use.

08:25.040 --> 08:31.520
So that very fresh just last week has been pre-trained with two trillion tokens.

08:31.520 --> 08:40.680
The context length is 4,096, and some versions of it have been fine-tuned for dialogue and

08:40.680 --> 08:41.680
things of that type.

08:41.680 --> 08:50.160
It compares favorably to other systems, either open or closed source on a number of benchmarks.

08:50.160 --> 08:54.800
But the essential characteristic of it is that it's open.

08:54.800 --> 09:02.440
And together with the model, we released a piece of text that a lot of people signed.

09:02.440 --> 09:06.560
The text says, we support an open innovation approach to AI, responsible and open innovation

09:06.560 --> 09:12.160
gives us all the stakes in the AI development process, bringing visibility, equity and trust

09:12.160 --> 09:17.760
to these technologies, opening today's LLM model will let everyone benefit from this technology.

09:17.760 --> 09:24.000
So what you have to understand is that at the government level, there is kind of a fork

09:24.000 --> 09:29.360
in the road where people are wondering whether AI, because it's powerful, should be kept

09:29.360 --> 09:35.120
under lock and key and controlled and heavily regulated, or whether an open source approach

09:35.120 --> 09:36.120
is preferable.

09:36.120 --> 09:42.520
Yes, they are dangerous, but historically, it's quite the case that there's a lot of

09:42.520 --> 09:48.160
evidence that open source software is actually more secure than the proprietary ones.

09:48.160 --> 09:53.840
And the benefits, the potential benefits of AI and LLM in particular are so large that

09:53.840 --> 09:58.320
we'll be shooting ourselves in the foot by kind of keeping this under lock and key.

09:58.320 --> 10:07.840
So Meta is definitely on the side of open research, has been for 10 years in AI, but

10:07.840 --> 10:13.640
it's still kind of an unsettled question, if you want.

10:13.640 --> 10:19.800
I think personally that this will open up the possibility of an entire ecosystem built

10:19.800 --> 10:23.120
on top of open source base LLM.

10:23.120 --> 10:27.760
Training base LLM is very expensive, so we don't need to have 25 different proprietary

10:27.760 --> 10:28.760
base LLM.

10:28.760 --> 10:33.960
We basically need a few that are open source so that people can build fine-tuned products

10:33.960 --> 10:34.960
on top of them.

10:34.960 --> 10:39.600
There's another reason, which is that before I go back to technical questions, which is

10:39.600 --> 10:46.640
that there's going to be a future in which all of our interactions with the digital world

10:46.640 --> 10:53.480
are going to be mediated through AI systems, virtual systems of some type, and it's going

10:53.480 --> 10:58.240
to become basically a repository of all human knowledge.

10:58.240 --> 11:06.320
So we're not going to be interrogating Google or doing a literature search directly anymore.

11:06.320 --> 11:12.360
We're just going to be talking to our AI assistant and asking a question and perhaps referring

11:12.360 --> 11:16.800
to original material and things like that.

11:16.800 --> 11:20.280
But basically all of our interactions with the digital world are going to be mediated

11:20.280 --> 11:22.440
by AI systems.

11:22.440 --> 11:25.000
So this is going to become the repository of all human knowledge.

11:25.000 --> 11:29.640
It's going to become a basic infrastructure that everybody wants to use.

11:29.640 --> 11:33.480
And history shows that basic infrastructure must be open source.

11:33.480 --> 11:38.360
If you look at the history of the internet, there was a battle between commercial providers,

11:38.360 --> 11:42.640
Microsoft, Sunmacrosystems, and others to provide the software infrastructure of the

11:42.640 --> 11:43.640
internet.

11:43.640 --> 11:46.720
All of those commercial providers lost.

11:46.720 --> 11:53.720
What runs the internet today is Linux, Apache, Chrome, Firefox, JavaScript.

11:53.720 --> 11:57.080
It's all open source.

11:57.080 --> 12:01.800
So my prediction is the same thing is going to happen in the context of AI.

12:01.800 --> 12:08.840
And it's necessary because a lot of countries outside the US in particular don't see with

12:08.840 --> 12:14.920
a favorable eye the fact that their citizens are going to get all the information from

12:14.920 --> 12:19.080
proprietary systems controlled by a small number of tech companies on the west coast

12:19.080 --> 12:20.080
of the US.

12:20.080 --> 12:26.080
So this is just proprietary systems are just not going to fly.

12:26.080 --> 12:31.160
It's just not going to be acceptable to the citizenry across the world.

12:31.160 --> 12:32.240
So it has to be open.

12:32.240 --> 12:33.240
It's inevitable.

12:33.240 --> 12:41.280
In fact, those systems need to be fine-tuned through what has been called RIHF, there's

12:41.280 --> 12:44.840
various ways to fine-tune those systems.

12:44.840 --> 12:49.320
Because the collection of human knowledge is so large, it includes things like physics,

12:49.320 --> 12:57.800
like many of you know, it's going to require contributions from millions of people in sort

12:57.800 --> 13:00.840
of a crowdsourcing fashion.

13:00.840 --> 13:05.240
Because basically those systems being the repository of all human knowledge will be sort

13:05.240 --> 13:06.240
of like Wikipedia.

13:06.240 --> 13:09.440
Wikipedia cannot be built by a proprietary company.

13:09.440 --> 13:14.320
It has to be, it has to gather the entire, the contribution of the entire world.

13:14.320 --> 13:17.560
So it's going to be the same thing with AI based systems.

13:17.560 --> 13:22.560
So open source AI is inevitable, in my opinion, and we're just sort of taking the first step.

13:22.560 --> 13:23.560
Okay.

13:23.560 --> 13:31.720
And so this Lama 70 billion, which is the largest of the Lama model is pretty interesting.

13:31.720 --> 13:33.520
Those are a few examples of what it can generate.

13:33.520 --> 13:42.200
These are extracted from the paper that you can read from the main website.

13:42.200 --> 13:47.480
The fine-tuned system actually refuses to give you kind of illegal information.

13:47.480 --> 13:49.600
You know, it's imperfect, but it works pretty well.

13:49.600 --> 13:54.880
It's got ways of detecting safety and helpfulness and toxicity and things like that.

13:54.880 --> 13:55.880
Okay.

13:55.880 --> 14:01.080
So this is all well and good, but autoregressive LLMs really suck.

14:01.080 --> 14:08.280
For many of us in the AI research business, LLM, the LLM revolution took place two years

14:08.280 --> 14:12.960
ago and it's kind of old hats already.

14:12.960 --> 14:16.520
Not the case for the public who's been kind of, you know, coming in contact with sensitivity

14:16.520 --> 14:21.600
only in the last few months, but really they're not that great.

14:21.600 --> 14:27.840
They don't really produce factual consistent answer.

14:27.840 --> 14:30.720
They hallucinate or they confibrate.

14:30.720 --> 14:32.720
They can't take into account recent information.

14:32.720 --> 14:37.440
They're trained on information that is two years old or so, or whatever snapshot of the

14:37.520 --> 14:41.040
crawl is used.

14:41.040 --> 14:46.240
They're not really, it's not really possible to make them behave properly other than through

14:46.240 --> 14:48.080
this RLHF, which is really perfect.

14:48.080 --> 14:52.200
So you can always jailbreak them by changing the prompt and sort of asking them to kind

14:52.200 --> 14:55.320
of act as if they were toxic.

14:55.320 --> 14:57.400
They don't reason.

14:57.400 --> 14:58.400
They don't really plan.

14:58.400 --> 15:03.160
They can't do math unless you almond them with tools, which you can, of course.

15:03.200 --> 15:06.360
And perhaps Stephen Wolfram will talk about this.

15:08.360 --> 15:13.240
And you need to, there's a lot of work on sort of getting them to use tools such as search

15:13.240 --> 15:15.760
engine calculators, database queries, et cetera.

15:15.760 --> 15:17.960
Right now it's a bit of a hack the way this is done.

15:19.560 --> 15:23.640
And the thing is we are really easily fooled by their fluency into thinking that they're

15:23.640 --> 15:28.040
intelligent, but their intelligence is very limited and really nothing like human intelligence.

15:29.240 --> 15:31.840
In particular, they really don't know how the world works.

15:31.880 --> 15:36.520
They have no connection with the physical reality.

15:39.320 --> 15:45.200
There's another reason why this, and it's basically by construction, which is that a system

15:45.200 --> 15:51.920
that produces one token after the other, auto-regressively, is a divergent process.

15:51.920 --> 15:54.400
It's a diffusion process with an exponential divergence.

15:55.120 --> 16:00.840
If there is a probability at any token that is produced that the token takes you out of

16:00.840 --> 16:04.880
the set of correct answers, those probabilities accumulate.

16:05.040 --> 16:11.680
And the probability that a string of tokens of length n is correct is one minus this probability

16:11.680 --> 16:12.840
of error to the power n.

16:12.840 --> 16:16.880
So the probability of correctness decreases exponentially with the length of the

16:18.560 --> 16:20.800
of the sequence that is produced.

16:22.000 --> 16:24.400
This is not fixable without major redesign.

16:25.440 --> 16:29.360
It's really an essential flaw of auto-regressive prediction.

16:30.840 --> 16:39.920
A while ago, with a colleague, Jacob Browning, we wrote a paper that essentially points out

16:39.920 --> 16:43.600
to the limit, it's not a technical paper, it's a philosophy paper, actually,

16:43.600 --> 16:45.840
the philosophy magazine called Noema.

16:46.560 --> 16:52.120
And it talks about the fact that most of human knowledge is non-linguistic.

16:52.160 --> 16:57.200
Everything that we learned before the age of one, everything that any animal learns has

16:57.200 --> 16:58.240
nothing to do with language.

16:58.480 --> 17:01.600
And it's an enormous amount of background knowledge about the world that we learned in

17:02.960 --> 17:06.000
in the first few months of life and that animals know.

17:06.960 --> 17:08.280
None of this is linguistic at all.

17:08.360 --> 17:12.080
And LLMs don't have access to any of this kind of knowledge.

17:13.400 --> 17:18.280
And so the thesis in that paper is that we're not going to be able to reach human level AI

17:18.280 --> 17:24.680
unless we have systems that have sort of direct sensor information in the form of vision, for example.

17:25.680 --> 17:28.120
You know, some way of understanding how the world works.

17:28.520 --> 17:33.440
Other papers that have appeared either from the cognitive science, in fact, that paper is from MIT,

17:34.000 --> 17:40.960
or from the classical AI kind of field, point to the fact that LLMs really cannot plan.

17:41.200 --> 17:48.320
They don't have the ability to think really or reason in a way that we understand this from humans.

17:48.960 --> 17:53.680
And very limited abilities to plan, at least compared to other systems that are specifically

17:53.680 --> 17:55.040
built for planning.

17:56.640 --> 18:01.000
So I think there is three challenges in the future for AI and machine learning research.

18:01.000 --> 18:06.920
And I've been showing this slide for several years now and I haven't changed it because of LLMs.

18:07.240 --> 18:12.520
The first one is learning representations and predictive models of the world, where the world can

18:12.520 --> 18:14.680
include other people that the system is talking to.

18:15.600 --> 18:17.320
The solution to this is self-supervised learning.

18:17.320 --> 18:18.720
We've known this for a number of years.

18:20.200 --> 18:21.160
Learning to reason.

18:22.160 --> 18:28.240
So autoregressive LLMs are very much like what Daniel Kahneman calls system one, which basically

18:28.240 --> 18:33.480
corresponds to subconscious tasks in humans, tasks that you accomplish without real planning

18:33.480 --> 18:42.560
or reasoning that you sort of accomplish more or less reactively without thinking too much.

18:43.560 --> 18:51.080
System two is the type of action that you take by deliberate reasoning using the power

18:51.280 --> 18:57.800
of your prefrontal cortex, using your ability to predict and then planning sequence of actions

18:57.800 --> 19:01.520
that will sort of satisfy a particular objective.

19:02.120 --> 19:04.280
And LLMs are not capable of this at the moment.

19:07.800 --> 19:11.320
I'm going to argue for the fact that reasoning and planning should be viewed as some sort of energy

19:11.320 --> 19:12.080
minimization.

19:13.080 --> 19:17.960
And then the last thing is learning to plan complex of actions to satisfy a number of objectives.

19:18.840 --> 19:23.640
And that will require learning hierarchical representations of action plans, which machine

19:23.640 --> 19:25.720
learning systems don't really know how to do at the moment.

19:26.520 --> 19:32.680
I've written this vision paper a while back called A Pass Towards Autonomous Machine Intelligence.

19:32.680 --> 19:34.120
I kind of changed the name of this now.

19:34.120 --> 19:38.600
I called it Objective Driven AI, but it really is the same concept.

19:39.880 --> 19:45.880
And it built around this idea of what's called cognitive architecture.

19:45.880 --> 19:49.400
So it's basically an architecture of different modules that interact with each other.

19:50.840 --> 19:55.160
A perception module that basically gives the system an estimate of the state of the world

19:55.160 --> 19:57.480
from perception that may be combined with memory.

19:58.120 --> 20:02.920
A world model and the world model essentially is there to predict what's going to happen

20:02.920 --> 20:06.760
in the world, perhaps as a result of actions that the agent might take,

20:07.560 --> 20:10.920
actions that are being imagines by another module called the actor.

20:11.480 --> 20:14.280
So the actor feeds actions to the world model.

20:14.280 --> 20:17.800
And the world model predicts the outcome of those actions.

20:19.480 --> 20:26.360
And then this outcome is fed to a cost module that that cost module basically

20:26.360 --> 20:29.080
assesses whether the outcome is good or bad.

20:29.080 --> 20:31.160
So it measures the quality of the outcome.

20:32.280 --> 20:36.840
And the entire purpose of the agent is to figure out a sequence of actions

20:36.840 --> 20:39.320
that minimizes that cost.

20:39.320 --> 20:40.760
We're not talking about learning here.

20:40.760 --> 20:42.440
We're talking about inference.

20:42.440 --> 20:48.680
So this minimization of the cost with respect to the actions imagined by the actors

20:49.800 --> 20:52.760
using the world model is for inference.

20:53.400 --> 20:56.920
Okay, so inference is not just forward propagation to a few layers of neural net.

20:56.920 --> 21:01.240
It's actually an optimization process, very much like what happens in business

21:01.240 --> 21:03.160
nets and record models and stuff like that.

21:05.480 --> 21:08.200
And I can describe that in more details.

21:10.280 --> 21:16.520
So that's kind of a very simple different representations of this kind of architecture.

21:17.400 --> 21:23.160
Perceives the world ready to a perception module that computes an abstract representation

21:23.160 --> 21:28.520
of the state of the world, perhaps combined with content of a memory that has some other

21:28.520 --> 21:29.720
idea with the state of the world.

21:31.000 --> 21:35.880
Initialize your world model with that and then feed the world model with that initial

21:35.880 --> 21:42.040
configuration combined with an imagined action sequence imagined by the actor.

21:43.560 --> 21:47.640
And then feed the results to a number of objective functions.

21:47.640 --> 21:51.640
A set of objectives that you can think of as guardrails that are hardwired.

21:52.840 --> 21:59.160
And other objectives that measure whether the task was satisfied was fulfilled.

22:00.600 --> 22:04.600
And the entire purpose of the system is to figure out a sequence of actions that

22:04.600 --> 22:06.760
will minimize those costs at inference time.

22:07.480 --> 22:13.080
Okay, so it cannot do anything, but I put action sequences that minimize those costs

22:13.080 --> 22:16.520
according to the prediction that it's making from its world model.

22:17.720 --> 22:19.800
So that's why I call this objective driven.

22:21.000 --> 22:26.040
There's no way you can job rate that system because it's hardwired to optimize those objectives.

22:26.040 --> 22:30.360
So unless you modify the objectives, the guardrails in particular, you're not going to be able to

22:31.080 --> 22:37.320
have it produce toxic content, for example, if the guardrail objective includes something

22:37.320 --> 22:45.560
like measuring toxicity. The world model very likely will need to be some sort of recurrent

22:45.560 --> 22:50.200
model that might be multiple steps to the action. So you take, for example, two actions

22:50.200 --> 22:53.560
and you run them through your world model twice so that you can predict in two steps

22:53.560 --> 22:57.160
what's going to happen. And the guardrail cost can be applied at every time step.

22:57.880 --> 23:01.640
Of course, the world is not deterministic. So the world model really,

23:02.520 --> 23:06.440
if it's a deterministic function, needs to be fed latent variables so that there might be

23:06.440 --> 23:09.240
multiple predictions for a single action in a single initial state.

23:10.200 --> 23:14.600
And when you make the latent variable vary over a set or you sample them from a distribution,

23:14.600 --> 23:21.080
you get multiple predictions. That complicates the planning process, of course, but the process

23:21.080 --> 23:24.600
by which you figure out a sequence of actions that minimize the objectives

23:24.600 --> 23:31.000
is a planning and reasoning procedure. Ultimately, what we really want is some

23:31.000 --> 23:37.640
sort of ways of doing this hierarchically. And I'm going to explain this with an example.

23:38.360 --> 23:44.360
So here is an example. Let's say I'm sitting in my office in New York at NYU and I want to

23:44.920 --> 23:51.800
fly to Paris. I want to go to Paris. So the first action I have to do is take a taxi or

23:51.800 --> 23:58.040
the train to the airport, either Newark or JFK. And then the second step is I need to catch a

23:58.040 --> 24:03.320
plane to Paris. Okay, but I have a first goal, which is to get to the airport. Now that goal

24:03.320 --> 24:08.280
can be decomposed into two sub-goals. The first one is I need to go down in the street

24:08.840 --> 24:15.880
and tell the taxi to take me to the airport. How do I go down in the street? I need to

24:15.880 --> 24:24.200
stand up for my chair, get out of the building and take the elevator or the stairs go down. How

24:24.200 --> 24:29.080
do I get out from my chair? I need to activate muscles in a particular order all the way down

24:29.080 --> 24:33.480
to millisecond by millisecond muscle control. We do this kind of hierarchical planning all the

24:33.480 --> 24:37.160
time without even thinking about it, even though it's actually a very conscious task that we're

24:37.160 --> 24:45.320
doing. Animals do this also. You can watch, I don't know, cats planning trajectories to kind of

24:45.320 --> 24:49.160
jump on a piece of furniture. They're doing this kind of planning hierarchically.

24:50.200 --> 24:56.440
We don't have any system, AI systems today, that can learn how to do this spontaneously.

24:56.440 --> 25:00.200
There are systems that do hierarchical planning, but they're hardwired. They're built by hand.

25:01.080 --> 25:05.080
What we need is a system that can learn the various levels of representations

25:05.080 --> 25:10.520
of the state of the world that will allow them to do this kind of decomposition of complex tasks

25:10.520 --> 25:18.360
into a hierarchy of simpler ones. Again, we don't have any system that can do this today at all.

25:18.360 --> 25:26.120
This is a big challenge, I think, for the future of AI research. That's the main idea of

25:26.120 --> 25:30.120
objective-driven AI. How can we build systems like this that can do hierarchical planning?

25:30.120 --> 25:34.680
They can learn models of the world that predict what's going to happen in the short term with

25:34.680 --> 25:40.200
high precision or in the long term with less precision in more abstract levels of representation.

25:41.000 --> 25:44.200
This is where I think AI research would go over the next 10 years or so,

25:45.080 --> 25:51.160
and this is how LLM should be built. In fact, that may be how LLM may be built in the future,

25:51.160 --> 25:55.880
and in fact, I have a prediction which is that the type of autoreversive LLMs that we see today

25:55.880 --> 26:00.920
will disappear within three to five years because they are not able to plan their answers.

26:01.800 --> 26:06.440
If we had a system that was able to take a query and then in some sort of abstract

26:06.440 --> 26:11.720
representation space was able to plan its answer, plan a representation of its answer,

26:12.520 --> 26:17.000
and then translate this representation of the answer into fluent text using an autoregressive

26:17.000 --> 26:23.000
decoder, for example, then we would have something that could actually be factual and

26:23.880 --> 26:32.920
simultaneously with being fluent and be non-toxic and be easily germ-broken and be steerable.

26:33.480 --> 26:39.960
That's my idea for where things are going. Building this and making it work is not going

26:39.960 --> 26:46.440
to be easy and may fail, but I think that's where we should go. If we have systems like this,

26:46.440 --> 26:54.440
we won't need any kind of RLHF or human feedback other than the type of systems that are required

26:54.440 --> 27:00.920
to train cost modules to measure things like toxicity, for example, but we won't need to

27:01.000 --> 27:07.480
fine-tune the system globally to be safe. We just need to put an objective so that all of the

27:07.480 --> 27:13.480
outputs that it produces are safe, but we don't need to retrain the entire encoders and everything

27:13.480 --> 27:21.320
for that. I think it would simplify training quite a bit, actually. Let me skip this.

27:24.360 --> 27:26.840
We come to the question of how do we build and train this word model?

27:26.840 --> 27:34.120
When we look at babies, babies learn in the first few months of life an enormous amount of

27:34.120 --> 27:38.440
background knowledge about the world, mostly by observation, a little bit by interaction,

27:38.440 --> 27:45.080
when they start to get old enough to actually act on the world, but mostly just by observation.

27:46.760 --> 27:53.640
The type of knowledge that they learn, things like intuitive physics, gravity, inertia,

27:53.640 --> 27:57.880
conservation of momentum, things like that, pops up only around the age of nine months.

27:58.520 --> 28:02.440
It takes about nine months for babies to really figure this out, that objects that are not supported

28:02.440 --> 28:07.400
will fall. But how do they do this? How do they learn this? Obviously, they don't do this like

28:07.400 --> 28:15.080
LLMs, because if LLMs were the answer to learning like humans, first of all, we would not need

28:15.720 --> 28:20.920
one trillion tokens to train them. Humans are not exposed to that much text information.

28:21.800 --> 28:30.840
Reading one and a half trillion tokens for human reading eight hours a day at normal speed would

28:30.840 --> 28:42.120
take about 20,000 years. That's obviously way more than any humans can do. But there are things

28:42.120 --> 28:47.880
that cats and dogs and young humans can do that are pretty amazing that LLMs can't even touch,

28:48.600 --> 28:57.400
not even remotely close. So cats and dogs can do things that robots cannot come anywhere close

28:57.400 --> 29:02.840
to doing today, not because we can't construct the mechanical systems for it. It's just because we

29:02.840 --> 29:08.360
can't build the intelligence for it. Any 10-year-old child can learn to clear up the dinner table and

29:08.360 --> 29:14.840
fill up the dishwasher in minutes, probably in one shot. We do not have robots that can do that.

29:14.840 --> 29:19.720
We don't have domestic robots. Any 17-year-old can learn to drive a car in about 20 hours of practice,

29:19.720 --> 29:24.600
and we still don't have a limited level of autonomous driving. So that means we're missing

29:24.600 --> 29:29.640
something really big in terms of learning that is very different from the way humans and animals

29:31.320 --> 29:36.840
learn. And this is just another example of the Moravec paradox, which is that there are things

29:36.840 --> 29:42.440
that seem easy for humans and turn out to be really difficult for AI and vice versa. AI systems

29:42.440 --> 29:49.800
are much better than humans at many tasks, narrow tasks, and we are nowhere near finding

29:52.280 --> 29:58.360
mechanisms by which machines can approach the sort of type of understanding of the world that a cat

29:58.360 --> 30:06.120
or a dog can have. Okay, so very some idea about how we can approach that problem. And again,

30:06.120 --> 30:11.480
it's based on self-supervised learning, learning to fill in the blanks. If we train the neural net

30:11.480 --> 30:17.080
to do video prediction, something we've been attempting to do for 10 years now. It doesn't

30:17.080 --> 30:21.560
work very well. If you look at the second column of this little animation at the bottom, the predictions

30:21.560 --> 30:26.920
that are produced by the system, and this is a very stylized video, are very blurry. It's because

30:26.920 --> 30:31.880
the system is trained to make one single prediction, and it cannot exactly predict what's going to

30:31.880 --> 30:36.920
happen in the video. So as a result, it predicts a kind of blurry mess, which is the average of all

30:36.920 --> 30:43.720
the possible features, plausible features that can happen. It's the same thing if you use a similar

30:43.720 --> 30:51.160
system to predict natural video, you get those blurry predictions. So my solution to this is

30:51.160 --> 31:00.280
something I call joint embedding predictive architecture, JEPA. And the main idea behind

31:00.280 --> 31:06.840
JEPA is to abandon the idea that prediction needs to be generative. Okay, so the most

31:06.840 --> 31:11.320
popular thing at the moment is generative AI, generative models. What I'm going to tell you

31:11.320 --> 31:17.080
now is to abandon it. Okay, not a very popular idea at the moment, but here is the argument.

31:18.120 --> 31:23.560
A generative model is one that, for which you give it an input x, let's say initial segment of a video

31:23.560 --> 31:29.480
or a text, run it through an encoder and a predictor, and then try to predict a variable y, which

31:29.480 --> 31:34.440
may be the continuation of that video or the continuation of that text, or the missing words

31:34.440 --> 31:42.280
in that text, and the error by which you measure the performance of the system is basically the

31:42.280 --> 31:46.760
some sort of divergence measure between the predicted y and the actual y. Okay, that's how

31:46.760 --> 31:54.200
you would train this model. It's a generative model because it predicts y. A joint embedding

31:54.200 --> 32:00.200
predictive architectures does not attempt to predict y. It attempts to predict a representation

32:00.200 --> 32:05.160
of y. Okay, so both x and y go through encoders that compute representations,

32:06.360 --> 32:09.080
and you perform the prediction in representation space.

32:12.520 --> 32:19.480
And the advantage of this is that the encoder of y may have invariant properties

32:20.200 --> 32:26.360
that map multiple y's to the same sy. And so if there are things that are very, very hard to predict,

32:26.360 --> 32:31.080
the encoder might eliminate that information that is hard to predict or impossible to predict

32:31.080 --> 32:36.280
from sy so that the prediction problem becomes easier. So let's say, for example, that you're

32:36.280 --> 32:42.280
driving along the road and the predictive model here is trying to predict what's going to happen

32:42.280 --> 32:46.680
on the road. So because it's a self-driving car, it wants to predict what the other cars on the road

32:46.680 --> 32:52.840
are going to do. But bordering the road, there might be trees and there is wind today, so the leaves

32:52.840 --> 32:59.320
on the trees are moving in chaotic ways. Behind the trees, there is a pond and there is ripples

32:59.320 --> 33:06.840
on the pond because of the wind. Those ripples and the motion of the leaves are not only very hard

33:06.840 --> 33:14.360
to predict, pretty much impossible to predict, but also very informative. There's a huge amount

33:14.360 --> 33:18.120
of information in there. And so if you use a generative model, that generative model will

33:18.120 --> 33:22.440
have to devote an enormous amount of resources trying to predict all of those details that

33:22.440 --> 33:27.720
are irrelevant to the task, really. Whereas if you have a model like the one on the right, the JEPA,

33:28.840 --> 33:36.920
the JEPA can choose to eliminate those details from the scene and only keep the details about why

33:36.920 --> 33:41.640
that are relatively easy to predict, like the motion of the other cars, for example.

33:43.000 --> 33:47.000
So that's my argument for the joint emitting architecture and that means abandoning generative

33:47.000 --> 33:50.840
models. Now, of course, you want to use generative models if you want to generate,

33:51.480 --> 33:56.840
but if what you want is to understand the world and then be able to plan, you don't need generative

33:56.840 --> 34:01.800
models. You need those joint emitting architectures. The reason I'm advocating for this is because

34:01.800 --> 34:07.800
experimentally, if you want to use self-supervised zoning in the context of images as opposed to text,

34:07.800 --> 34:11.960
the only architectures that work well are joint emitting architectures.

34:12.680 --> 34:16.680
There are architectures like the one on the left here, which is a joint emitting architecture

34:16.680 --> 34:21.320
without the predictor. This is the most successful approach to self-supervised zoning for image

34:21.320 --> 34:29.320
recognition. You show image X or rather image Y, then you corrupt this image Y into image X

34:29.320 --> 34:35.800
by distorting it, blurring it, adding noise, masking some parts of it, changing the framing,

34:35.800 --> 34:43.960
the size, etc. And then you run both images through the encoders and you force the system

34:43.960 --> 34:48.120
or you train the system to produce representations that are identical for the two images,

34:48.920 --> 34:54.120
so that the representation of the corrupted image is the same as the representation of the

34:54.120 --> 34:59.720
uncorrupted image. And that builds representations that are invariant to the corruptions, essentially.

35:01.400 --> 35:06.040
So those methods, there is a whole bunch of them, about a dozen of them, and they work really well,

35:06.760 --> 35:10.920
whereas all the methods to learn image features that are based on reconstruction,

35:10.920 --> 35:14.520
generative models, don't work. At least they don't work nearly as well.

35:15.880 --> 35:20.920
So what this slide shows is kind of different versions of this joint emitting predictive

35:20.920 --> 35:26.440
architecture, either with a predictor or without, with a predictor that can be stochastic,

35:26.440 --> 35:32.200
having latent variables or not. And the question is how you train this, because the problem is,

35:32.200 --> 35:38.040
if you train a system like this, without being careful, is going to collapse. If you train a

35:38.040 --> 35:45.000
system, you give it pairs of images, let's say x and y, or video snippets, x and y, and you tell it

35:46.040 --> 35:50.440
compute representations that are identical for x and y, the system will just collapse. It will

35:51.240 --> 35:57.320
produce sx and xy that are constant, and then just completely ignore x and y,

35:58.120 --> 36:03.480
so that the distance between sx and sy is minimized. So that's a collapse.

36:04.200 --> 36:12.520
What's a, how can you correct this? And to correct this, you have to put yourself in

36:12.520 --> 36:16.520
the context of something called energy-based models, and I'm sure there are a lot of physicists

36:16.520 --> 36:22.440
in the room, so you can understand what that means. So energy-based models are models where you don't

36:24.840 --> 36:30.840
explain what they do in terms of probabilistic modeling, but in terms of an energy function

36:30.840 --> 36:35.960
that captures the dependency between the variables. So maybe a little more explicit here. Let's say

36:35.960 --> 36:42.440
you have two variables, x and y, and your datasets are those greenish dots that are supposed to be

36:42.440 --> 36:51.960
black. The way an energy-based model captures the dependency between x and y is that it computes

36:51.960 --> 36:58.360
an energy function, an implicit function with a scalar output that takes x and y as an input

36:58.360 --> 37:05.480
and gives you an energy that needs to be low near the data points, on the data points nearby,

37:05.480 --> 37:12.760
and then higher outside of those data points, the region of high data density. And if you have

37:12.760 --> 37:16.680
such an energy landscape, you have a function that has this, that can compute this energy landscape,

37:17.640 --> 37:22.600
then that function will have captured the dependencies between x and y. You can infer x

37:22.600 --> 37:27.800
from y, you can infer y from x, you can have mapping between x and y that are not functions,

37:27.800 --> 37:31.480
because you can have multiple y's that are compatible with a single x, for example,

37:31.480 --> 37:39.880
so it captures multi-modality without having to be a probabilistic model. Of course, in physics,

37:39.880 --> 37:46.280
we're familiar with this, right? Very often, we write an energy function, and then we turn it

37:46.280 --> 37:50.120
into a probability distribution over states using a Gibbs distribution. Same idea here,

37:50.120 --> 37:54.200
but here we don't need the Gibbs distribution at all, we just manipulate the energy function

37:54.200 --> 37:59.560
directly. How do we train a system like this? There's really two categories of training methods.

38:00.200 --> 38:07.240
One category is contrastive methods, and those consist in generating those flashing green dots

38:07.240 --> 38:13.000
here that are outside the manifold data, and then changing the parameters of the energy function

38:13.000 --> 38:18.040
so that the energy takes low values on the data points and higher values on those contrastive

38:18.920 --> 38:26.040
green points. I contributed to inventing those methods back in the early 90s, but I don't like

38:26.040 --> 38:30.200
them anymore, because in high-dimensional spaces, the number of contrastive points you have to

38:30.200 --> 38:35.800
generate for the energy function to take the right shape grows exponentially, and that's not a good

38:35.800 --> 38:41.320
thing. I prefer another server approach, regularised methods, and there are those methods. I'm going

38:41.320 --> 38:46.760
to explain this with another slide. They basically consist in minimizing the volume of space that

38:46.760 --> 38:52.600
can take low energy through some sort of regulariser, for example, so that the system can give low

38:52.600 --> 38:56.760
energy to the data points by changing the parameters of the energy function so that the

38:56.760 --> 39:03.400
energy of the data points gets lower, but because it's regularised, it can only give low energy to

39:03.400 --> 39:11.000
a small volume of space. The data points get kind of shrinkwrapped if you want in the sort of region

39:11.000 --> 39:18.280
of low energies. So I prefer this. That seems to be more efficient, and the question is how we do

39:18.280 --> 39:22.920
this, but what I'm asking you to do now is abandon generative models, the most popular thing at the

39:22.920 --> 39:28.600
moment, abandon probabilistic models, the pillar of understanding machine learning, abandon contrastive

39:28.600 --> 39:33.800
methods, which also have been very popular, and also something I've been saying for a number of

39:33.800 --> 39:39.960
10 years, abandon reinforcement learning, because it's so damn inefficient. So those are kind of

39:39.960 --> 39:43.800
four of the most popular approaches to machine learning at the moment, and I'm telling people to

39:44.760 --> 39:50.680
move away from them. You can imagine I'm not being very popular here, but I'm used to that.

39:50.680 --> 39:55.720
So how do you prevent those systems from collapsing? What you can do is measure,

39:55.720 --> 40:00.920
have some sort of measure or information content of SX and SY across a batch, for example,

40:01.560 --> 40:06.200
and then try to maximise that. Now, unfortunately, it's very hard to do because we don't have lower

40:06.200 --> 40:12.840
bounds on information content. We only have upper bounds, but it turns out you can sort of do this.

40:12.840 --> 40:18.120
So one way to prevent SX from collapsing is that you can use a criterion, which is

40:19.720 --> 40:25.640
attempt to make sure that the variance of every component of SX over a batch is at least one.

40:25.640 --> 40:35.160
So that's the criterion you see in the second red box below the cover. That's a hinge loss

40:35.160 --> 40:41.000
that makes the standard deviation of each variable at least one. And then another term that makes sure

40:41.000 --> 40:47.240
the components of SX are decorrelated. That's the covariance matrix term. So you're trying to minimise

40:47.240 --> 40:51.960
the octagonal terms of the covariance matrix of the vectors SX over a batch.

40:53.880 --> 40:58.120
That's not actually sufficient. So you can also use an expander. I don't have time to explain

40:58.120 --> 41:02.760
why that works, but the resulting method is called Vicreg variance, invariance covariance

41:02.760 --> 41:08.120
regularisation. And it's a pretty general method that can be applied to a lot of situations for

41:08.120 --> 41:13.640
those joint embedding predictive architectures for various applications in image recognition,

41:13.640 --> 41:19.720
segmentation, etc. It's pretty similar to another method called MCR squared invented by EMA at its

41:19.720 --> 41:28.360
group at Berkeley. And it works really well. I'm not going to bore you with details, but there is a

41:28.360 --> 41:33.400
standard scenario in which you do use sub-supervised learning where you pre-train a convolutional net,

41:33.400 --> 41:39.720
let's say, using this method. And then you chop off the expander, stick a linear classifier, which

41:39.720 --> 41:43.880
you train supervised and you measure the performance. And you get really good performance on image net

41:43.880 --> 41:50.680
this way, particularly good performance for out-of-distribution transfer learning. There's

41:50.680 --> 41:57.320
a modification of this method called Vicreg L, which was published last year, which is more

41:57.320 --> 42:03.880
tuned for segmentation and things like this, but I don't have time to go into details. There's a new

42:03.880 --> 42:11.640
method that we rolled out at CVPR just a few weeks ago called Image JEPA that uses masking and a

42:11.640 --> 42:18.040
transformer architecture for learning features in images. And so the collapse prevention method

42:18.040 --> 42:21.720
there is different, but the advantage of this method is that it does not require any data

42:21.720 --> 42:26.440
augmentation other than masking. So it doesn't require to know really what type of data you're

42:26.440 --> 42:31.960
manipulating. And it's incredibly fast and it works really well. It gives amazing results

42:33.000 --> 42:38.440
for really, really good features. There's another set of method by some of our colleagues here at

42:38.440 --> 42:46.200
Fair Paris called Dino, the INO. It's a different way of preventing collapse, but it has some

42:46.200 --> 42:51.640
commonalities with IJPA. And it works really well. It gives you something like above 80% on

42:51.640 --> 42:56.600
ImageNet, purely supervised with no fine tuning and without any data augmentation, which is pretty

42:56.600 --> 43:04.520
amazing. But ultimately what we want to do is use this self-supervised learning and this

43:04.520 --> 43:10.360
JEPA architecture to build the systems of the type that I talked to you about earlier that are

43:10.360 --> 43:15.160
hierarchical. They can predict what's going to happen in the world, perhaps as a result of an

43:15.160 --> 43:22.600
action, with some early results on training systems from video to learn good representations of

43:22.600 --> 43:29.480
images and videos by being trained on successive frames from a video and distorted images.

43:29.480 --> 43:33.160
I don't have time to go into the details of how this works. It's called NCJEPA.

43:36.840 --> 43:43.320
And it is trained basically to extract good features from images for object recognition,

43:43.320 --> 43:47.960
but also to estimate motion in a video. And it does a pretty good job at this.

43:49.240 --> 43:57.640
So watch this paper on archive that you're invited to look at. So objective-driven AI is this idea

43:57.640 --> 44:03.160
that we're going to have objectives that are going to drive the behavior of our system and it's going

44:03.160 --> 44:09.800
to make it terrible and safe. And there are things that we're working on to get this to work,

44:09.800 --> 44:13.800
self-supervised learning from video, that recipe that really works for everything. So we're working

44:13.800 --> 44:19.960
with those JEPA architectures, but we don't think we have the ultimate recipe yet. We can use this

44:19.960 --> 44:25.000
to build LLMs that can reason and plan, that are driven by objectives, perhaps hopefully,

44:26.040 --> 44:32.040
learning systems that can do hierarchical planning, like animals and humans, many animals and humans.

44:32.040 --> 44:37.720
We have many problems to solve. JEPAs with regularized written variables to deal with uncertainty,

44:37.800 --> 44:41.960
planning algorithms in the presence of uncertainty, learning cost modules,

44:42.600 --> 44:47.000
which could be assimilated with inverse reinforcement learning, planning with

44:47.000 --> 44:52.680
inaccurate world models, and then exploration techniques to adjust the world model in case it's

44:52.680 --> 45:04.200
not completely accurate. Okay, so I'm sort of concluding. There is a computing limitation

45:04.200 --> 45:11.560
of autoregressive LLM, which is that they can only allocate a finite and fixed amount of

45:11.560 --> 45:17.560
computational resources to producing single token. You run through, you know, 48 layers

45:17.560 --> 45:21.560
of a transformer or something like that, you produce one token and then 48 more layers and

45:21.560 --> 45:30.440
produce one token. And this is not too incomplete. Whereas the method I'm suggesting, the architecture

45:30.440 --> 45:35.880
I'm suggesting that can produce an output by planning through energy minimization,

45:36.760 --> 45:41.480
that is too incomplete, because everything can be reduced to optimization, basically.

45:42.600 --> 45:46.680
We're still missing essential concepts to reach human ability AI, you know, this

45:49.720 --> 45:57.640
potential technique for planning and reasoning, you know, basic techniques that we're missing to

45:57.640 --> 46:05.320
learn world models from complex modalities like video. And perhaps in the future, we'll be able to

46:05.320 --> 46:11.480
build systems that can plan their answers to satisfy objectives and have guardrails. I don't

46:11.480 --> 46:15.560
believe there is such a concept as artificial general intelligence, because I think even human

46:15.560 --> 46:20.280
intelligence is very specialized. So let's forget about general intelligence, let's try to get to

46:20.280 --> 46:25.800
human level intelligence, perhaps, perhaps build machines that have the same sort of set of skills

46:25.880 --> 46:30.520
and ability to learn new skills and humans. But we are very specialized. In fact, we know this

46:30.520 --> 46:33.960
because computers are much better than us at many tasks, which means we are specialized.

46:34.920 --> 46:38.680
There's no question that sometimes in the future, machines will surpass human

46:38.680 --> 46:43.080
intelligence in all domains where humans are intelligent. You know, how long is it going to

46:43.080 --> 46:47.800
take? I don't know, but there's no question is going to happen. We don't need, we don't,

46:49.000 --> 46:54.040
we probably don't want to be threatened by that. It would be a future where every one of us would

46:54.040 --> 47:00.120
be assisted by a system that is more intelligent than us. And we're familiar with that concept

47:00.120 --> 47:04.760
with other humans. I only work with people who are smarter than me, or at least I try.

47:06.680 --> 47:10.360
Or if they're not smarter than me, I try to make them smarter than me, they're called students.

47:11.160 --> 47:16.680
And, you know, so we're familiar with that concept. We shouldn't feel threatened by machines

47:16.680 --> 47:21.320
that are smarter than us. We are in control of them and we still will still be in control of them.

47:21.960 --> 47:26.360
They won't escape our control any more than our neural cortex and escape the control of our

47:26.360 --> 47:33.080
visual memory, basically, in our brains. Thank you very much. I'll stop here and

47:33.720 --> 47:36.120
perhaps if we have time for questions, I'll take questions.

47:42.520 --> 47:48.280
Thank you very much. If anyone has questions, we have two mics up there. Feel free to line up.

47:49.160 --> 47:55.880
I guess I'll get us started with one question. So on that last slide, you mentioned the possibility,

47:55.880 --> 48:01.080
or not the possibility, just the prediction that machines will become more intelligent than humans,

48:01.080 --> 48:06.520
in all respects. And you also mentioned throughout your talk, these algorithms that can sort of

48:06.520 --> 48:13.480
reason and plan. Could you imagine in the near future an algorithm that, for example, could propose

48:14.040 --> 48:20.680
physics experiments for us to conduct, like plan an experiment to answer a question that we ask it?

48:22.040 --> 48:28.280
Yeah, actually, there's an entire field which precedes AI called experimental design.

48:30.040 --> 48:34.200
And I mean, I think to some extent that can be formulated as an optimization problem, as a

48:34.200 --> 48:39.640
planning problem, or as a search problem, right, trying to figure out, like, you know, how do you

48:39.640 --> 48:43.720
maximally get information from an experiment? Like, how do you design experiments? You get the

48:43.720 --> 48:49.320
maximum amount of information from it to either validate or invalidate a particular model that

48:49.320 --> 48:54.600
you have, or a hypothesis you have in your mind. I think that's entirely automatable.

48:56.600 --> 49:02.680
Now, if you want to use a generic AI system to do this, my guess is that it's not going to happen

49:02.680 --> 49:08.440
tomorrow. The system will probably have to be relatively, you know, experienced before they're

49:08.440 --> 49:14.520
better scientists than human scientists. Yeah, thank you. We have a question up there.

49:14.520 --> 49:19.640
Hey, can you hear me? Yeah, yeah. Great. I was wondering if you could expand a little bit on

49:19.640 --> 49:23.560
your assertion that you cannot build a sufficient world model from text alone.

49:24.760 --> 49:28.280
When we think of something like a theoretical physicist, right, this person mostly interacts

49:28.280 --> 49:32.760
with other people verbally and reading papers and thinking and writing. Or if you think about

49:32.760 --> 49:37.560
something like a blind from birth author or person, right, they're able to actually extract a lot of

49:37.560 --> 49:40.920
information about the structure of the world from the text. So I'm wondering if you could explain

49:40.920 --> 49:45.720
a little bit more about why that's insufficient for, say, achieving human level AI at least.

49:46.760 --> 49:52.200
Okay, when we do physics or mathematics, very much, very often, I mean, certainly physics, we

49:52.200 --> 49:58.040
have mental models of the world. We have some sort of internal simulator, if you want, that can

49:58.040 --> 50:02.280
assimilate the interesting aspect of the phenomenon that we're trying to understand.

50:03.240 --> 50:12.440
That allows us to arrive at answers. And we don't necessarily rely on explicit

50:12.440 --> 50:21.640
facts that we've learned through language. Let me take an example. So all of intuitive physics is

50:21.640 --> 50:29.560
learned by observation. It's not learned through language, right? You know, if that you are going

50:29.640 --> 50:34.600
to put a smartphone on a horizontal surface and let it go, you know, you know, it's going to,

50:34.600 --> 50:37.560
it's going to fall one way or the other. You may not predict in which direction,

50:37.560 --> 50:43.240
but you know, it's going to fall because of your notion of intuitive physics. If I tell you, I take

50:43.240 --> 50:50.920
an object, I throw it in the air vertically, and it's going to have a particular velocity when

50:50.920 --> 50:55.800
it leaves my hand, it's going to go up in the air and then fall back. What velocity will it have

50:55.800 --> 51:02.280
when it crosses my hand at the same location where it left my hand? And if you're any kind of intuitive

51:03.880 --> 51:09.560
notion of physics that go beyond, you know, normal intuitive physics, you would say, obviously,

51:09.560 --> 51:13.320
it's going to have the same speed because, you know, there is conservation of momentum and energy

51:13.320 --> 51:19.640
and stuff like that. And it's not because of energy, you know, necessarily the rule of the

51:19.640 --> 51:25.240
explicit language rule that you have, it's because of your sort of, you know, intuition

51:25.240 --> 51:30.520
that corresponds to that. And we do this all the time. We manipulate mental models. We do not

51:30.520 --> 51:38.200
reason with language very often. Most of our reasoning does not use language. And so most

51:38.200 --> 51:45.240
human knowledge is not linguistic at all. It has to do with construction of mental models,

51:45.880 --> 51:49.800
many of which have nothing to do with language. It's certainly true of all animals. They don't have

51:49.800 --> 51:57.960
language. So that's what sort of, you know, I mean, it's something that I think physicists,

51:57.960 --> 52:01.720
particularly physicists should really understand, right? Because we do this all the time. Good

52:01.720 --> 52:07.640
physicists are people who have those mental models that they can use to sort of, you know, imagine

52:07.640 --> 52:14.360
situations and corner cases and stuff like that, that really kind of, you know, give you some insight

52:14.360 --> 52:20.200
as to what the nature of reality is. And of course, you know, then after that, we do the math,

52:20.200 --> 52:24.520
and that gives you sort of the internal structure of language, the mathematical language,

52:24.520 --> 52:28.840
kind of, you know, makes you discover new properties. But a lot of it is really

52:30.440 --> 52:36.760
intuition with mental model is true in mathematics as well in geometry and things like that.

52:37.400 --> 52:44.280
So, you know, there's the Gedanke experiments of Einstein, right, for those are

52:45.240 --> 52:50.040
basically mental models that you manipulate to kind of discover properties. They're not linguistic.

52:54.520 --> 53:01.320
I have a question along similar lines, but so I agree that there's a lot of intuition involved

53:01.320 --> 53:07.160
in learning for humans. But is there not a fundamental problem in training such intuition?

53:07.160 --> 53:11.240
Because anything you train digitally would be encoded in some kind of language,

53:12.040 --> 53:18.840
some binary. Is there not a fundamental obstruction there to train such intuition?

53:20.200 --> 53:27.960
Not really, no. The input to those systems can be as, you know, continuous and kind of perceptual

53:27.960 --> 53:32.200
as the kind of stuff that we perceive, like, you know, like video or whatever,

53:34.120 --> 53:40.200
or sensory input, whatever it is, audio, you know, anything you want. And then inside the system,

53:40.200 --> 53:47.320
the representation of facts and knowledge inside the system is actually just a sequence of numbers.

53:47.880 --> 53:56.360
It's not language. It's numbers, it's vectors, you know, tensors. So I don't think that's a

53:56.360 --> 54:01.400
problem we need to deal with really. But the texts are also broken down into

54:01.400 --> 54:09.400
same numbers, right? Yeah, that's right. That's true. So text is to some extent simpler because

54:09.400 --> 54:16.920
it's discrete, as I explained, it makes the, you know, the management of uncertainty easier

54:16.920 --> 54:21.640
if you have discrete tokens. And there is a reason why language is discrete, why language

54:22.440 --> 54:29.800
clusters in words. The reason is because language is a communication medium, right? It's a way of

54:29.800 --> 54:35.400
communicating. And we need to communicate over noisy channels. And to be able to communicate over

54:35.400 --> 54:41.960
noisy channels, the symbols have to be discrete. Because that allows you to do error correction,

54:41.960 --> 54:47.480
right? To do noise, you know, to eliminate noise, right? I mean, communication engineers

54:47.560 --> 54:52.280
are known this for decades, you know, since China and basically, and so, or even before.

54:53.560 --> 55:00.680
So our language is discrete and goes into words, you know, the existence of phonemes and words

55:00.680 --> 55:06.200
and things like this, because we need to be able to communicate with noisy channels. But that doesn't

55:06.200 --> 55:11.480
mean that our thinking needs to be the same way. And in fact, our thinking is not the same way.

55:11.480 --> 55:20.200
Language is a pale, approximate, discretized, dumbed down representation of eternal knowledge

55:20.200 --> 55:26.200
representation recall thoughts. Yeah. And so my intuition would be that's when you

55:26.200 --> 55:31.240
encode something digitally in binary, you're doing, you're dumbing that down anyway, right?

55:31.240 --> 55:36.600
So even if you're processing images or videos, you're doing it in numbers and

55:36.680 --> 55:40.680
you're doing it in the same way, maybe it's a little more complex.

55:41.320 --> 55:46.520
But that's life. Okay. That's even physics does this biology does this, right? The communication

55:46.520 --> 55:51.960
between synapses between two neurons, there is a finite number of physicals that are released

55:51.960 --> 55:57.400
for the synaptic communication. And so there is granularity in this, the precision is actually

55:57.400 --> 56:04.040
just a few bits. And, and, you know, it's actually much less than the 32 bits that we use for or

56:04.120 --> 56:09.000
16 bits that we use for computation in neural nets. So I don't think the quantization here is,

56:09.000 --> 56:14.040
is an issue. It certainly exists in the, in the brain as well. Communication between neurons in

56:14.040 --> 56:21.240
the brain is binary brains, you know, the neurons actually produce spikes. For the same reason that

56:22.120 --> 56:25.560
language is discrete is because they need to communicate in a long distance. And for this

56:25.560 --> 56:31.640
to be efficient, it has to be digital basically. So, so I don't see this as an limitation that

56:31.640 --> 56:40.040
would discriminate between computers and human intelligence. Thanks. Okay. Unfortunately, we

56:40.040 --> 56:47.560
need to move on because we're running five minutes behind already. Thank you so much. This was great.

