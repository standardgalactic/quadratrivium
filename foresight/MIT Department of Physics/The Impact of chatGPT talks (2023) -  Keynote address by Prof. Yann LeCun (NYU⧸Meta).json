{"text": " So, our first speaker is Yann LeCun. He's currently the Silver Professor at the current Institute of Mathematical Sciences at NYU, where he is the founding director of the NYU Center for Data Science. He is also affiliated with MEDA, formerly known as Facebook, as the Vice President and Chief AI Scientist there. By the way, MEDA just released one of its large language models just a few days ago called LAMAS II, which I encourage you all to explore. And this could be a very long introduction because Yann has a very long resume, but the one thing I wanted to highlight is that Yann was the recipient of the 2018 Turing Award. For those of you who aren't familiar with it, it's kind of like the Nobel Prize in Computer Science, and he received that for his work on deep learning and convolutional neural networks. And something interesting I learned is a lot of the work he did, early work on convolutional neural networks, was actually when he was at Bell Labs, which is something physicists know quite well, I think. So I think you're all eager to hear from Yann, so I'm going to hand it over to him here. Thank you. Okay, I'm going to talk about, so this is going to be a somewhat technical talk, but not very technical, but to tell you less about the possibilities that are offered by LLM and more about their limitations. And basically tell you about what I think is coming next, though at least what I'm working towards coming next. And the first thing we should realize is that machine learning really sucks compared to what we observe in humans and animals. The capabilities of the learning systems that we have today are really terrible. Humans and animals can run new tasks really quickly, understand how the world works, they can reason, they can plan, they have some level of common sense. Their behavior is driven by objectives or drives, which is not the case for auto-reversed LLMs. But there is one thing that both the biological world and the recent machine learning world have had in common is the use of cell supervised learning. And really cell supervised learning has taken over the world. For both applications in text and natural language understanding, for images, videos, 3D models, speech, protein folding, all that stuff. What is cell supervised learning really? It's sort of completion really, learning to fill in the blanks, right? So the way it's used in the context of natural language understanding or processing is you take a piece of text, you mask part of it by removing some other words, masking, replacing them by blank markers, for example. And then so think of it as a type of corruption, it doesn't have to be the masking but it could be other types of corruptions. And then you train some gigantic neural net to predict the words that are missing. And you just measure the reconstruction error basically on the parts that were missing. In the process of doing so, that system learns to represent text in a way that allows it to store or represent meaning, grammar, everything, syntax, semantics, everything there is to represent about language in the internal representation, which you can subsequently use for any downstream task like say translation or topic classification or something of that type. So this works amazingly well in the context of text, particularly because text is easy to predict with uncertainty, you can never predict exactly what word will appear at a particular location, but what you can do is predict some sort of probability distribution of all possible words in the dictionary. And you can do this because there's only a finite number of words in your dictionary or tokens. And so you can compute this distribution easily and handle uncertainty in the prediction pretty well. More generally, self-supervised learning is really sort of learning to capture dependencies between inputs. And so if you wanted to apply this to the problem of video prediction, for example, you would show a segment of video to a system and then ask it to predict what's going to happen next, for example, in the video and then reveal the future of the video. And again, I apologize for the colors. And then the system could adapt itself so that it does a better job at predicting what happened next in the video. Now, unfortunately, it's much harder to do for video than it is for text. So much harder than it might require other methods than the type of generative methods that work well for text. I'll come back to this. So speaking of generative methods, but generative AI and autoregressive language models is something that many of us have been hearing about recently. What is it? Probably most of you know already, but essentially the way you train them is very similar to the self-supervised learning. It's in fact a special case of self-supervised learning method I just mentioned. You take a sequence of tokens, words, whatever it is, a sequence of vectors. As long as you can turn things into vectors, you're OK. And then you only mask the last one and train a system to predict the last token in the sequence. I mean, technically, you do more than that, but that's what it comes down to in the end. And once you have a system that has been trained to produce the next token, you can use it autoregressively, recursively, basically to predict then the next next token and et cetera. So you predict the next token, you shift it into the input, then predict the next next token, shift that into the input, et cetera. And that's called autoregressive prediction. It's an old concept going back to signal processing many years ago, many decades ago, as a matter of fact. So nothing new there, but that allows the system to basically predict one token after the other and generate text. So those things work amazingly well. Performance is really amazing. The fact that, you know, they're trained only on text, even though on enormous amounts of text, but only on text, the amount of knowledge, if you want, that they capture from text is pretty amazing and surprised a lot of people. Those systems typically have billions or up to hundreds of billions of parameters. They train typically on one to two trillion tokens. Sometimes more. Their input window is anywhere between 2000 and maybe a few tens of thousands of tokens for their context window. And there's been a long history of such models that have been put out, the sort of GPT family, starting with GPT-123, from FAIR, there's been Blunderbot, Galactica, Llama, Version 2 that just came out this week, Alpaca from Stanford, which is a fine-tuned version of Llama, Llama, Version 1, Lambda and Bard from Google, Chinche from DeepMind, you know, and of course, GPDT, GPT-4 from OpenAI. And they're really good at, as writing aids, but they have really limited knowledge of the underlying reality because they're purely trained from text, at least for the vast majority of them. And they really have no common sense or very limited common sense, and they have limited abilities to plan their answers because the answers are produced autoregressively. But still, it's pretty impressive how they work, so, as was mentioned, my colleagues just put out an open source LLM called Llama 2. There is three versions of this at the moment, 7 billion, 13 billion and 70 billion parameters. The license is fairly liberal, so it can be used commercially if you want. If you want to start a startup and use it as a business, you can. It's also available on sort of various cloud services, easy to use. So that very fresh just last week has been pre-trained with two trillion tokens. The context length is 4,096, and some versions of it have been fine-tuned for dialogue and things of that type. It compares favorably to other systems, either open or closed source on a number of benchmarks. But the essential characteristic of it is that it's open. And together with the model, we released a piece of text that a lot of people signed. The text says, we support an open innovation approach to AI, responsible and open innovation gives us all the stakes in the AI development process, bringing visibility, equity and trust to these technologies, opening today's LLM model will let everyone benefit from this technology. So what you have to understand is that at the government level, there is kind of a fork in the road where people are wondering whether AI, because it's powerful, should be kept under lock and key and controlled and heavily regulated, or whether an open source approach is preferable. Yes, they are dangerous, but historically, it's quite the case that there's a lot of evidence that open source software is actually more secure than the proprietary ones. And the benefits, the potential benefits of AI and LLM in particular are so large that we'll be shooting ourselves in the foot by kind of keeping this under lock and key. So Meta is definitely on the side of open research, has been for 10 years in AI, but it's still kind of an unsettled question, if you want. I think personally that this will open up the possibility of an entire ecosystem built on top of open source base LLM. Training base LLM is very expensive, so we don't need to have 25 different proprietary base LLM. We basically need a few that are open source so that people can build fine-tuned products on top of them. There's another reason, which is that before I go back to technical questions, which is that there's going to be a future in which all of our interactions with the digital world are going to be mediated through AI systems, virtual systems of some type, and it's going to become basically a repository of all human knowledge. So we're not going to be interrogating Google or doing a literature search directly anymore. We're just going to be talking to our AI assistant and asking a question and perhaps referring to original material and things like that. But basically all of our interactions with the digital world are going to be mediated by AI systems. So this is going to become the repository of all human knowledge. It's going to become a basic infrastructure that everybody wants to use. And history shows that basic infrastructure must be open source. If you look at the history of the internet, there was a battle between commercial providers, Microsoft, Sunmacrosystems, and others to provide the software infrastructure of the internet. All of those commercial providers lost. What runs the internet today is Linux, Apache, Chrome, Firefox, JavaScript. It's all open source. So my prediction is the same thing is going to happen in the context of AI. And it's necessary because a lot of countries outside the US in particular don't see with a favorable eye the fact that their citizens are going to get all the information from proprietary systems controlled by a small number of tech companies on the west coast of the US. So this is just proprietary systems are just not going to fly. It's just not going to be acceptable to the citizenry across the world. So it has to be open. It's inevitable. In fact, those systems need to be fine-tuned through what has been called RIHF, there's various ways to fine-tune those systems. Because the collection of human knowledge is so large, it includes things like physics, like many of you know, it's going to require contributions from millions of people in sort of a crowdsourcing fashion. Because basically those systems being the repository of all human knowledge will be sort of like Wikipedia. Wikipedia cannot be built by a proprietary company. It has to be, it has to gather the entire, the contribution of the entire world. So it's going to be the same thing with AI based systems. So open source AI is inevitable, in my opinion, and we're just sort of taking the first step. Okay. And so this Lama 70 billion, which is the largest of the Lama model is pretty interesting. Those are a few examples of what it can generate. These are extracted from the paper that you can read from the main website. The fine-tuned system actually refuses to give you kind of illegal information. You know, it's imperfect, but it works pretty well. It's got ways of detecting safety and helpfulness and toxicity and things like that. Okay. So this is all well and good, but autoregressive LLMs really suck. For many of us in the AI research business, LLM, the LLM revolution took place two years ago and it's kind of old hats already. Not the case for the public who's been kind of, you know, coming in contact with sensitivity only in the last few months, but really they're not that great. They don't really produce factual consistent answer. They hallucinate or they confibrate. They can't take into account recent information. They're trained on information that is two years old or so, or whatever snapshot of the crawl is used. They're not really, it's not really possible to make them behave properly other than through this RLHF, which is really perfect. So you can always jailbreak them by changing the prompt and sort of asking them to kind of act as if they were toxic. They don't reason. They don't really plan. They can't do math unless you almond them with tools, which you can, of course. And perhaps Stephen Wolfram will talk about this. And you need to, there's a lot of work on sort of getting them to use tools such as search engine calculators, database queries, et cetera. Right now it's a bit of a hack the way this is done. And the thing is we are really easily fooled by their fluency into thinking that they're intelligent, but their intelligence is very limited and really nothing like human intelligence. In particular, they really don't know how the world works. They have no connection with the physical reality. There's another reason why this, and it's basically by construction, which is that a system that produces one token after the other, auto-regressively, is a divergent process. It's a diffusion process with an exponential divergence. If there is a probability at any token that is produced that the token takes you out of the set of correct answers, those probabilities accumulate. And the probability that a string of tokens of length n is correct is one minus this probability of error to the power n. So the probability of correctness decreases exponentially with the length of the of the sequence that is produced. This is not fixable without major redesign. It's really an essential flaw of auto-regressive prediction. A while ago, with a colleague, Jacob Browning, we wrote a paper that essentially points out to the limit, it's not a technical paper, it's a philosophy paper, actually, the philosophy magazine called Noema. And it talks about the fact that most of human knowledge is non-linguistic. Everything that we learned before the age of one, everything that any animal learns has nothing to do with language. And it's an enormous amount of background knowledge about the world that we learned in in the first few months of life and that animals know. None of this is linguistic at all. And LLMs don't have access to any of this kind of knowledge. And so the thesis in that paper is that we're not going to be able to reach human level AI unless we have systems that have sort of direct sensor information in the form of vision, for example. You know, some way of understanding how the world works. Other papers that have appeared either from the cognitive science, in fact, that paper is from MIT, or from the classical AI kind of field, point to the fact that LLMs really cannot plan. They don't have the ability to think really or reason in a way that we understand this from humans. And very limited abilities to plan, at least compared to other systems that are specifically built for planning. So I think there is three challenges in the future for AI and machine learning research. And I've been showing this slide for several years now and I haven't changed it because of LLMs. The first one is learning representations and predictive models of the world, where the world can include other people that the system is talking to. The solution to this is self-supervised learning. We've known this for a number of years. Learning to reason. So autoregressive LLMs are very much like what Daniel Kahneman calls system one, which basically corresponds to subconscious tasks in humans, tasks that you accomplish without real planning or reasoning that you sort of accomplish more or less reactively without thinking too much. System two is the type of action that you take by deliberate reasoning using the power of your prefrontal cortex, using your ability to predict and then planning sequence of actions that will sort of satisfy a particular objective. And LLMs are not capable of this at the moment. I'm going to argue for the fact that reasoning and planning should be viewed as some sort of energy minimization. And then the last thing is learning to plan complex of actions to satisfy a number of objectives. And that will require learning hierarchical representations of action plans, which machine learning systems don't really know how to do at the moment. I've written this vision paper a while back called A Pass Towards Autonomous Machine Intelligence. I kind of changed the name of this now. I called it Objective Driven AI, but it really is the same concept. And it built around this idea of what's called cognitive architecture. So it's basically an architecture of different modules that interact with each other. A perception module that basically gives the system an estimate of the state of the world from perception that may be combined with memory. A world model and the world model essentially is there to predict what's going to happen in the world, perhaps as a result of actions that the agent might take, actions that are being imagines by another module called the actor. So the actor feeds actions to the world model. And the world model predicts the outcome of those actions. And then this outcome is fed to a cost module that that cost module basically assesses whether the outcome is good or bad. So it measures the quality of the outcome. And the entire purpose of the agent is to figure out a sequence of actions that minimizes that cost. We're not talking about learning here. We're talking about inference. So this minimization of the cost with respect to the actions imagined by the actors using the world model is for inference. Okay, so inference is not just forward propagation to a few layers of neural net. It's actually an optimization process, very much like what happens in business nets and record models and stuff like that. And I can describe that in more details. So that's kind of a very simple different representations of this kind of architecture. Perceives the world ready to a perception module that computes an abstract representation of the state of the world, perhaps combined with content of a memory that has some other idea with the state of the world. Initialize your world model with that and then feed the world model with that initial configuration combined with an imagined action sequence imagined by the actor. And then feed the results to a number of objective functions. A set of objectives that you can think of as guardrails that are hardwired. And other objectives that measure whether the task was satisfied was fulfilled. And the entire purpose of the system is to figure out a sequence of actions that will minimize those costs at inference time. Okay, so it cannot do anything, but I put action sequences that minimize those costs according to the prediction that it's making from its world model. So that's why I call this objective driven. There's no way you can job rate that system because it's hardwired to optimize those objectives. So unless you modify the objectives, the guardrails in particular, you're not going to be able to have it produce toxic content, for example, if the guardrail objective includes something like measuring toxicity. The world model very likely will need to be some sort of recurrent model that might be multiple steps to the action. So you take, for example, two actions and you run them through your world model twice so that you can predict in two steps what's going to happen. And the guardrail cost can be applied at every time step. Of course, the world is not deterministic. So the world model really, if it's a deterministic function, needs to be fed latent variables so that there might be multiple predictions for a single action in a single initial state. And when you make the latent variable vary over a set or you sample them from a distribution, you get multiple predictions. That complicates the planning process, of course, but the process by which you figure out a sequence of actions that minimize the objectives is a planning and reasoning procedure. Ultimately, what we really want is some sort of ways of doing this hierarchically. And I'm going to explain this with an example. So here is an example. Let's say I'm sitting in my office in New York at NYU and I want to fly to Paris. I want to go to Paris. So the first action I have to do is take a taxi or the train to the airport, either Newark or JFK. And then the second step is I need to catch a plane to Paris. Okay, but I have a first goal, which is to get to the airport. Now that goal can be decomposed into two sub-goals. The first one is I need to go down in the street and tell the taxi to take me to the airport. How do I go down in the street? I need to stand up for my chair, get out of the building and take the elevator or the stairs go down. How do I get out from my chair? I need to activate muscles in a particular order all the way down to millisecond by millisecond muscle control. We do this kind of hierarchical planning all the time without even thinking about it, even though it's actually a very conscious task that we're doing. Animals do this also. You can watch, I don't know, cats planning trajectories to kind of jump on a piece of furniture. They're doing this kind of planning hierarchically. We don't have any system, AI systems today, that can learn how to do this spontaneously. There are systems that do hierarchical planning, but they're hardwired. They're built by hand. What we need is a system that can learn the various levels of representations of the state of the world that will allow them to do this kind of decomposition of complex tasks into a hierarchy of simpler ones. Again, we don't have any system that can do this today at all. This is a big challenge, I think, for the future of AI research. That's the main idea of objective-driven AI. How can we build systems like this that can do hierarchical planning? They can learn models of the world that predict what's going to happen in the short term with high precision or in the long term with less precision in more abstract levels of representation. This is where I think AI research would go over the next 10 years or so, and this is how LLM should be built. In fact, that may be how LLM may be built in the future, and in fact, I have a prediction which is that the type of autoreversive LLMs that we see today will disappear within three to five years because they are not able to plan their answers. If we had a system that was able to take a query and then in some sort of abstract representation space was able to plan its answer, plan a representation of its answer, and then translate this representation of the answer into fluent text using an autoregressive decoder, for example, then we would have something that could actually be factual and simultaneously with being fluent and be non-toxic and be easily germ-broken and be steerable. That's my idea for where things are going. Building this and making it work is not going to be easy and may fail, but I think that's where we should go. If we have systems like this, we won't need any kind of RLHF or human feedback other than the type of systems that are required to train cost modules to measure things like toxicity, for example, but we won't need to fine-tune the system globally to be safe. We just need to put an objective so that all of the outputs that it produces are safe, but we don't need to retrain the entire encoders and everything for that. I think it would simplify training quite a bit, actually. Let me skip this. We come to the question of how do we build and train this word model? When we look at babies, babies learn in the first few months of life an enormous amount of background knowledge about the world, mostly by observation, a little bit by interaction, when they start to get old enough to actually act on the world, but mostly just by observation. The type of knowledge that they learn, things like intuitive physics, gravity, inertia, conservation of momentum, things like that, pops up only around the age of nine months. It takes about nine months for babies to really figure this out, that objects that are not supported will fall. But how do they do this? How do they learn this? Obviously, they don't do this like LLMs, because if LLMs were the answer to learning like humans, first of all, we would not need one trillion tokens to train them. Humans are not exposed to that much text information. Reading one and a half trillion tokens for human reading eight hours a day at normal speed would take about 20,000 years. That's obviously way more than any humans can do. But there are things that cats and dogs and young humans can do that are pretty amazing that LLMs can't even touch, not even remotely close. So cats and dogs can do things that robots cannot come anywhere close to doing today, not because we can't construct the mechanical systems for it. It's just because we can't build the intelligence for it. Any 10-year-old child can learn to clear up the dinner table and fill up the dishwasher in minutes, probably in one shot. We do not have robots that can do that. We don't have domestic robots. Any 17-year-old can learn to drive a car in about 20 hours of practice, and we still don't have a limited level of autonomous driving. So that means we're missing something really big in terms of learning that is very different from the way humans and animals learn. And this is just another example of the Moravec paradox, which is that there are things that seem easy for humans and turn out to be really difficult for AI and vice versa. AI systems are much better than humans at many tasks, narrow tasks, and we are nowhere near finding mechanisms by which machines can approach the sort of type of understanding of the world that a cat or a dog can have. Okay, so very some idea about how we can approach that problem. And again, it's based on self-supervised learning, learning to fill in the blanks. If we train the neural net to do video prediction, something we've been attempting to do for 10 years now. It doesn't work very well. If you look at the second column of this little animation at the bottom, the predictions that are produced by the system, and this is a very stylized video, are very blurry. It's because the system is trained to make one single prediction, and it cannot exactly predict what's going to happen in the video. So as a result, it predicts a kind of blurry mess, which is the average of all the possible features, plausible features that can happen. It's the same thing if you use a similar system to predict natural video, you get those blurry predictions. So my solution to this is something I call joint embedding predictive architecture, JEPA. And the main idea behind JEPA is to abandon the idea that prediction needs to be generative. Okay, so the most popular thing at the moment is generative AI, generative models. What I'm going to tell you now is to abandon it. Okay, not a very popular idea at the moment, but here is the argument. A generative model is one that, for which you give it an input x, let's say initial segment of a video or a text, run it through an encoder and a predictor, and then try to predict a variable y, which may be the continuation of that video or the continuation of that text, or the missing words in that text, and the error by which you measure the performance of the system is basically the some sort of divergence measure between the predicted y and the actual y. Okay, that's how you would train this model. It's a generative model because it predicts y. A joint embedding predictive architectures does not attempt to predict y. It attempts to predict a representation of y. Okay, so both x and y go through encoders that compute representations, and you perform the prediction in representation space. And the advantage of this is that the encoder of y may have invariant properties that map multiple y's to the same sy. And so if there are things that are very, very hard to predict, the encoder might eliminate that information that is hard to predict or impossible to predict from sy so that the prediction problem becomes easier. So let's say, for example, that you're driving along the road and the predictive model here is trying to predict what's going to happen on the road. So because it's a self-driving car, it wants to predict what the other cars on the road are going to do. But bordering the road, there might be trees and there is wind today, so the leaves on the trees are moving in chaotic ways. Behind the trees, there is a pond and there is ripples on the pond because of the wind. Those ripples and the motion of the leaves are not only very hard to predict, pretty much impossible to predict, but also very informative. There's a huge amount of information in there. And so if you use a generative model, that generative model will have to devote an enormous amount of resources trying to predict all of those details that are irrelevant to the task, really. Whereas if you have a model like the one on the right, the JEPA, the JEPA can choose to eliminate those details from the scene and only keep the details about why that are relatively easy to predict, like the motion of the other cars, for example. So that's my argument for the joint emitting architecture and that means abandoning generative models. Now, of course, you want to use generative models if you want to generate, but if what you want is to understand the world and then be able to plan, you don't need generative models. You need those joint emitting architectures. The reason I'm advocating for this is because experimentally, if you want to use self-supervised zoning in the context of images as opposed to text, the only architectures that work well are joint emitting architectures. There are architectures like the one on the left here, which is a joint emitting architecture without the predictor. This is the most successful approach to self-supervised zoning for image recognition. You show image X or rather image Y, then you corrupt this image Y into image X by distorting it, blurring it, adding noise, masking some parts of it, changing the framing, the size, etc. And then you run both images through the encoders and you force the system or you train the system to produce representations that are identical for the two images, so that the representation of the corrupted image is the same as the representation of the uncorrupted image. And that builds representations that are invariant to the corruptions, essentially. So those methods, there is a whole bunch of them, about a dozen of them, and they work really well, whereas all the methods to learn image features that are based on reconstruction, generative models, don't work. At least they don't work nearly as well. So what this slide shows is kind of different versions of this joint emitting predictive architecture, either with a predictor or without, with a predictor that can be stochastic, having latent variables or not. And the question is how you train this, because the problem is, if you train a system like this, without being careful, is going to collapse. If you train a system, you give it pairs of images, let's say x and y, or video snippets, x and y, and you tell it compute representations that are identical for x and y, the system will just collapse. It will produce sx and xy that are constant, and then just completely ignore x and y, so that the distance between sx and sy is minimized. So that's a collapse. What's a, how can you correct this? And to correct this, you have to put yourself in the context of something called energy-based models, and I'm sure there are a lot of physicists in the room, so you can understand what that means. So energy-based models are models where you don't explain what they do in terms of probabilistic modeling, but in terms of an energy function that captures the dependency between the variables. So maybe a little more explicit here. Let's say you have two variables, x and y, and your datasets are those greenish dots that are supposed to be black. The way an energy-based model captures the dependency between x and y is that it computes an energy function, an implicit function with a scalar output that takes x and y as an input and gives you an energy that needs to be low near the data points, on the data points nearby, and then higher outside of those data points, the region of high data density. And if you have such an energy landscape, you have a function that has this, that can compute this energy landscape, then that function will have captured the dependencies between x and y. You can infer x from y, you can infer y from x, you can have mapping between x and y that are not functions, because you can have multiple y's that are compatible with a single x, for example, so it captures multi-modality without having to be a probabilistic model. Of course, in physics, we're familiar with this, right? Very often, we write an energy function, and then we turn it into a probability distribution over states using a Gibbs distribution. Same idea here, but here we don't need the Gibbs distribution at all, we just manipulate the energy function directly. How do we train a system like this? There's really two categories of training methods. One category is contrastive methods, and those consist in generating those flashing green dots here that are outside the manifold data, and then changing the parameters of the energy function so that the energy takes low values on the data points and higher values on those contrastive green points. I contributed to inventing those methods back in the early 90s, but I don't like them anymore, because in high-dimensional spaces, the number of contrastive points you have to generate for the energy function to take the right shape grows exponentially, and that's not a good thing. I prefer another server approach, regularised methods, and there are those methods. I'm going to explain this with another slide. They basically consist in minimizing the volume of space that can take low energy through some sort of regulariser, for example, so that the system can give low energy to the data points by changing the parameters of the energy function so that the energy of the data points gets lower, but because it's regularised, it can only give low energy to a small volume of space. The data points get kind of shrinkwrapped if you want in the sort of region of low energies. So I prefer this. That seems to be more efficient, and the question is how we do this, but what I'm asking you to do now is abandon generative models, the most popular thing at the moment, abandon probabilistic models, the pillar of understanding machine learning, abandon contrastive methods, which also have been very popular, and also something I've been saying for a number of 10 years, abandon reinforcement learning, because it's so damn inefficient. So those are kind of four of the most popular approaches to machine learning at the moment, and I'm telling people to move away from them. You can imagine I'm not being very popular here, but I'm used to that. So how do you prevent those systems from collapsing? What you can do is measure, have some sort of measure or information content of SX and SY across a batch, for example, and then try to maximise that. Now, unfortunately, it's very hard to do because we don't have lower bounds on information content. We only have upper bounds, but it turns out you can sort of do this. So one way to prevent SX from collapsing is that you can use a criterion, which is attempt to make sure that the variance of every component of SX over a batch is at least one. So that's the criterion you see in the second red box below the cover. That's a hinge loss that makes the standard deviation of each variable at least one. And then another term that makes sure the components of SX are decorrelated. That's the covariance matrix term. So you're trying to minimise the octagonal terms of the covariance matrix of the vectors SX over a batch. That's not actually sufficient. So you can also use an expander. I don't have time to explain why that works, but the resulting method is called Vicreg variance, invariance covariance regularisation. And it's a pretty general method that can be applied to a lot of situations for those joint embedding predictive architectures for various applications in image recognition, segmentation, etc. It's pretty similar to another method called MCR squared invented by EMA at its group at Berkeley. And it works really well. I'm not going to bore you with details, but there is a standard scenario in which you do use sub-supervised learning where you pre-train a convolutional net, let's say, using this method. And then you chop off the expander, stick a linear classifier, which you train supervised and you measure the performance. And you get really good performance on image net this way, particularly good performance for out-of-distribution transfer learning. There's a modification of this method called Vicreg L, which was published last year, which is more tuned for segmentation and things like this, but I don't have time to go into details. There's a new method that we rolled out at CVPR just a few weeks ago called Image JEPA that uses masking and a transformer architecture for learning features in images. And so the collapse prevention method there is different, but the advantage of this method is that it does not require any data augmentation other than masking. So it doesn't require to know really what type of data you're manipulating. And it's incredibly fast and it works really well. It gives amazing results for really, really good features. There's another set of method by some of our colleagues here at Fair Paris called Dino, the INO. It's a different way of preventing collapse, but it has some commonalities with IJPA. And it works really well. It gives you something like above 80% on ImageNet, purely supervised with no fine tuning and without any data augmentation, which is pretty amazing. But ultimately what we want to do is use this self-supervised learning and this JEPA architecture to build the systems of the type that I talked to you about earlier that are hierarchical. They can predict what's going to happen in the world, perhaps as a result of an action, with some early results on training systems from video to learn good representations of images and videos by being trained on successive frames from a video and distorted images. I don't have time to go into the details of how this works. It's called NCJEPA. And it is trained basically to extract good features from images for object recognition, but also to estimate motion in a video. And it does a pretty good job at this. So watch this paper on archive that you're invited to look at. So objective-driven AI is this idea that we're going to have objectives that are going to drive the behavior of our system and it's going to make it terrible and safe. And there are things that we're working on to get this to work, self-supervised learning from video, that recipe that really works for everything. So we're working with those JEPA architectures, but we don't think we have the ultimate recipe yet. We can use this to build LLMs that can reason and plan, that are driven by objectives, perhaps hopefully, learning systems that can do hierarchical planning, like animals and humans, many animals and humans. We have many problems to solve. JEPAs with regularized written variables to deal with uncertainty, planning algorithms in the presence of uncertainty, learning cost modules, which could be assimilated with inverse reinforcement learning, planning with inaccurate world models, and then exploration techniques to adjust the world model in case it's not completely accurate. Okay, so I'm sort of concluding. There is a computing limitation of autoregressive LLM, which is that they can only allocate a finite and fixed amount of computational resources to producing single token. You run through, you know, 48 layers of a transformer or something like that, you produce one token and then 48 more layers and produce one token. And this is not too incomplete. Whereas the method I'm suggesting, the architecture I'm suggesting that can produce an output by planning through energy minimization, that is too incomplete, because everything can be reduced to optimization, basically. We're still missing essential concepts to reach human ability AI, you know, this potential technique for planning and reasoning, you know, basic techniques that we're missing to learn world models from complex modalities like video. And perhaps in the future, we'll be able to build systems that can plan their answers to satisfy objectives and have guardrails. I don't believe there is such a concept as artificial general intelligence, because I think even human intelligence is very specialized. So let's forget about general intelligence, let's try to get to human level intelligence, perhaps, perhaps build machines that have the same sort of set of skills and ability to learn new skills and humans. But we are very specialized. In fact, we know this because computers are much better than us at many tasks, which means we are specialized. There's no question that sometimes in the future, machines will surpass human intelligence in all domains where humans are intelligent. You know, how long is it going to take? I don't know, but there's no question is going to happen. We don't need, we don't, we probably don't want to be threatened by that. It would be a future where every one of us would be assisted by a system that is more intelligent than us. And we're familiar with that concept with other humans. I only work with people who are smarter than me, or at least I try. Or if they're not smarter than me, I try to make them smarter than me, they're called students. And, you know, so we're familiar with that concept. We shouldn't feel threatened by machines that are smarter than us. We are in control of them and we still will still be in control of them. They won't escape our control any more than our neural cortex and escape the control of our visual memory, basically, in our brains. Thank you very much. I'll stop here and perhaps if we have time for questions, I'll take questions. Thank you very much. If anyone has questions, we have two mics up there. Feel free to line up. I guess I'll get us started with one question. So on that last slide, you mentioned the possibility, or not the possibility, just the prediction that machines will become more intelligent than humans, in all respects. And you also mentioned throughout your talk, these algorithms that can sort of reason and plan. Could you imagine in the near future an algorithm that, for example, could propose physics experiments for us to conduct, like plan an experiment to answer a question that we ask it? Yeah, actually, there's an entire field which precedes AI called experimental design. And I mean, I think to some extent that can be formulated as an optimization problem, as a planning problem, or as a search problem, right, trying to figure out, like, you know, how do you maximally get information from an experiment? Like, how do you design experiments? You get the maximum amount of information from it to either validate or invalidate a particular model that you have, or a hypothesis you have in your mind. I think that's entirely automatable. Now, if you want to use a generic AI system to do this, my guess is that it's not going to happen tomorrow. The system will probably have to be relatively, you know, experienced before they're better scientists than human scientists. Yeah, thank you. We have a question up there. Hey, can you hear me? Yeah, yeah. Great. I was wondering if you could expand a little bit on your assertion that you cannot build a sufficient world model from text alone. When we think of something like a theoretical physicist, right, this person mostly interacts with other people verbally and reading papers and thinking and writing. Or if you think about something like a blind from birth author or person, right, they're able to actually extract a lot of information about the structure of the world from the text. So I'm wondering if you could explain a little bit more about why that's insufficient for, say, achieving human level AI at least. Okay, when we do physics or mathematics, very much, very often, I mean, certainly physics, we have mental models of the world. We have some sort of internal simulator, if you want, that can assimilate the interesting aspect of the phenomenon that we're trying to understand. That allows us to arrive at answers. And we don't necessarily rely on explicit facts that we've learned through language. Let me take an example. So all of intuitive physics is learned by observation. It's not learned through language, right? You know, if that you are going to put a smartphone on a horizontal surface and let it go, you know, you know, it's going to, it's going to fall one way or the other. You may not predict in which direction, but you know, it's going to fall because of your notion of intuitive physics. If I tell you, I take an object, I throw it in the air vertically, and it's going to have a particular velocity when it leaves my hand, it's going to go up in the air and then fall back. What velocity will it have when it crosses my hand at the same location where it left my hand? And if you're any kind of intuitive notion of physics that go beyond, you know, normal intuitive physics, you would say, obviously, it's going to have the same speed because, you know, there is conservation of momentum and energy and stuff like that. And it's not because of energy, you know, necessarily the rule of the explicit language rule that you have, it's because of your sort of, you know, intuition that corresponds to that. And we do this all the time. We manipulate mental models. We do not reason with language very often. Most of our reasoning does not use language. And so most human knowledge is not linguistic at all. It has to do with construction of mental models, many of which have nothing to do with language. It's certainly true of all animals. They don't have language. So that's what sort of, you know, I mean, it's something that I think physicists, particularly physicists should really understand, right? Because we do this all the time. Good physicists are people who have those mental models that they can use to sort of, you know, imagine situations and corner cases and stuff like that, that really kind of, you know, give you some insight as to what the nature of reality is. And of course, you know, then after that, we do the math, and that gives you sort of the internal structure of language, the mathematical language, kind of, you know, makes you discover new properties. But a lot of it is really intuition with mental model is true in mathematics as well in geometry and things like that. So, you know, there's the Gedanke experiments of Einstein, right, for those are basically mental models that you manipulate to kind of discover properties. They're not linguistic. I have a question along similar lines, but so I agree that there's a lot of intuition involved in learning for humans. But is there not a fundamental problem in training such intuition? Because anything you train digitally would be encoded in some kind of language, some binary. Is there not a fundamental obstruction there to train such intuition? Not really, no. The input to those systems can be as, you know, continuous and kind of perceptual as the kind of stuff that we perceive, like, you know, like video or whatever, or sensory input, whatever it is, audio, you know, anything you want. And then inside the system, the representation of facts and knowledge inside the system is actually just a sequence of numbers. It's not language. It's numbers, it's vectors, you know, tensors. So I don't think that's a problem we need to deal with really. But the texts are also broken down into same numbers, right? Yeah, that's right. That's true. So text is to some extent simpler because it's discrete, as I explained, it makes the, you know, the management of uncertainty easier if you have discrete tokens. And there is a reason why language is discrete, why language clusters in words. The reason is because language is a communication medium, right? It's a way of communicating. And we need to communicate over noisy channels. And to be able to communicate over noisy channels, the symbols have to be discrete. Because that allows you to do error correction, right? To do noise, you know, to eliminate noise, right? I mean, communication engineers are known this for decades, you know, since China and basically, and so, or even before. So our language is discrete and goes into words, you know, the existence of phonemes and words and things like this, because we need to be able to communicate with noisy channels. But that doesn't mean that our thinking needs to be the same way. And in fact, our thinking is not the same way. Language is a pale, approximate, discretized, dumbed down representation of eternal knowledge representation recall thoughts. Yeah. And so my intuition would be that's when you encode something digitally in binary, you're doing, you're dumbing that down anyway, right? So even if you're processing images or videos, you're doing it in numbers and you're doing it in the same way, maybe it's a little more complex. But that's life. Okay. That's even physics does this biology does this, right? The communication between synapses between two neurons, there is a finite number of physicals that are released for the synaptic communication. And so there is granularity in this, the precision is actually just a few bits. And, and, you know, it's actually much less than the 32 bits that we use for or 16 bits that we use for computation in neural nets. So I don't think the quantization here is, is an issue. It certainly exists in the, in the brain as well. Communication between neurons in the brain is binary brains, you know, the neurons actually produce spikes. For the same reason that language is discrete is because they need to communicate in a long distance. And for this to be efficient, it has to be digital basically. So, so I don't see this as an limitation that would discriminate between computers and human intelligence. Thanks. Okay. Unfortunately, we need to move on because we're running five minutes behind already. Thank you so much. This was great.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.32, "text": " So, our first speaker is Yann LeCun.", "tokens": [50364, 407, 11, 527, 700, 8145, 307, 398, 969, 1456, 34, 409, 13, 50580], "temperature": 0.0, "avg_logprob": -0.21899907405559832, "compression_ratio": 1.518796992481203, "no_speech_prob": 0.06655050814151764}, {"id": 1, "seek": 0, "start": 4.32, "end": 10.28, "text": " He's currently the Silver Professor at the current Institute of Mathematical Sciences", "tokens": [50580, 634, 311, 4362, 264, 15861, 8419, 412, 264, 2190, 9446, 295, 15776, 8615, 804, 21108, 50878], "temperature": 0.0, "avg_logprob": -0.21899907405559832, "compression_ratio": 1.518796992481203, "no_speech_prob": 0.06655050814151764}, {"id": 2, "seek": 0, "start": 10.28, "end": 16.88, "text": " at NYU, where he is the founding director of the NYU Center for Data Science.", "tokens": [50878, 412, 42682, 11, 689, 415, 307, 264, 22223, 5391, 295, 264, 42682, 5169, 337, 11888, 8976, 13, 51208], "temperature": 0.0, "avg_logprob": -0.21899907405559832, "compression_ratio": 1.518796992481203, "no_speech_prob": 0.06655050814151764}, {"id": 3, "seek": 0, "start": 16.88, "end": 22.8, "text": " He is also affiliated with MEDA, formerly known as Facebook, as the Vice President and Chief", "tokens": [51208, 634, 307, 611, 42174, 365, 376, 4731, 32, 11, 34777, 2570, 382, 4384, 11, 382, 264, 13276, 3117, 293, 10068, 51504], "temperature": 0.0, "avg_logprob": -0.21899907405559832, "compression_ratio": 1.518796992481203, "no_speech_prob": 0.06655050814151764}, {"id": 4, "seek": 0, "start": 22.8, "end": 25.0, "text": " AI Scientist there.", "tokens": [51504, 7318, 18944, 468, 456, 13, 51614], "temperature": 0.0, "avg_logprob": -0.21899907405559832, "compression_ratio": 1.518796992481203, "no_speech_prob": 0.06655050814151764}, {"id": 5, "seek": 0, "start": 25.0, "end": 29.72, "text": " By the way, MEDA just released one of its large language models just a few days ago called", "tokens": [51614, 3146, 264, 636, 11, 376, 4731, 32, 445, 4736, 472, 295, 1080, 2416, 2856, 5245, 445, 257, 1326, 1708, 2057, 1219, 51850], "temperature": 0.0, "avg_logprob": -0.21899907405559832, "compression_ratio": 1.518796992481203, "no_speech_prob": 0.06655050814151764}, {"id": 6, "seek": 2972, "start": 29.72, "end": 34.12, "text": " LAMAS II, which I encourage you all to explore.", "tokens": [50364, 441, 2865, 3160, 6351, 11, 597, 286, 5373, 291, 439, 281, 6839, 13, 50584], "temperature": 0.0, "avg_logprob": -0.14244408761301347, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0014697080478072166}, {"id": 7, "seek": 2972, "start": 34.12, "end": 39.36, "text": " And this could be a very long introduction because Yann has a very long resume, but the", "tokens": [50584, 400, 341, 727, 312, 257, 588, 938, 9339, 570, 398, 969, 575, 257, 588, 938, 15358, 11, 457, 264, 50846], "temperature": 0.0, "avg_logprob": -0.14244408761301347, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0014697080478072166}, {"id": 8, "seek": 2972, "start": 39.36, "end": 45.16, "text": " one thing I wanted to highlight is that Yann was the recipient of the 2018 Turing Award.", "tokens": [50846, 472, 551, 286, 1415, 281, 5078, 307, 300, 398, 969, 390, 264, 26216, 295, 264, 6096, 314, 1345, 13894, 13, 51136], "temperature": 0.0, "avg_logprob": -0.14244408761301347, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0014697080478072166}, {"id": 9, "seek": 2972, "start": 45.16, "end": 48.599999999999994, "text": " For those of you who aren't familiar with it, it's kind of like the Nobel Prize in Computer", "tokens": [51136, 1171, 729, 295, 291, 567, 3212, 380, 4963, 365, 309, 11, 309, 311, 733, 295, 411, 264, 24611, 22604, 294, 22289, 51308], "temperature": 0.0, "avg_logprob": -0.14244408761301347, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0014697080478072166}, {"id": 10, "seek": 2972, "start": 48.599999999999994, "end": 55.480000000000004, "text": " Science, and he received that for his work on deep learning and convolutional neural networks.", "tokens": [51308, 8976, 11, 293, 415, 4613, 300, 337, 702, 589, 322, 2452, 2539, 293, 45216, 304, 18161, 9590, 13, 51652], "temperature": 0.0, "avg_logprob": -0.14244408761301347, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0014697080478072166}, {"id": 11, "seek": 2972, "start": 55.480000000000004, "end": 59.44, "text": " And something interesting I learned is a lot of the work he did, early work on convolutional", "tokens": [51652, 400, 746, 1880, 286, 3264, 307, 257, 688, 295, 264, 589, 415, 630, 11, 2440, 589, 322, 45216, 304, 51850], "temperature": 0.0, "avg_logprob": -0.14244408761301347, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0014697080478072166}, {"id": 12, "seek": 5944, "start": 59.44, "end": 63.58, "text": " neural networks, was actually when he was at Bell Labs, which is something physicists", "tokens": [50364, 18161, 9590, 11, 390, 767, 562, 415, 390, 412, 11485, 40047, 11, 597, 307, 746, 48716, 50571], "temperature": 0.0, "avg_logprob": -0.17755390268511476, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0409013032913208}, {"id": 13, "seek": 5944, "start": 63.58, "end": 66.2, "text": " know quite well, I think.", "tokens": [50571, 458, 1596, 731, 11, 286, 519, 13, 50702], "temperature": 0.0, "avg_logprob": -0.17755390268511476, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0409013032913208}, {"id": 14, "seek": 5944, "start": 66.2, "end": 70.88, "text": " So I think you're all eager to hear from Yann, so I'm going to hand it over to him here.", "tokens": [50702, 407, 286, 519, 291, 434, 439, 18259, 281, 1568, 490, 398, 969, 11, 370, 286, 478, 516, 281, 1011, 309, 670, 281, 796, 510, 13, 50936], "temperature": 0.0, "avg_logprob": -0.17755390268511476, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0409013032913208}, {"id": 15, "seek": 5944, "start": 70.88, "end": 71.88, "text": " Thank you.", "tokens": [50936, 1044, 291, 13, 50986], "temperature": 0.0, "avg_logprob": -0.17755390268511476, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0409013032913208}, {"id": 16, "seek": 5944, "start": 71.88, "end": 75.84, "text": " Okay, I'm going to talk about, so this is going to be a somewhat technical talk, but", "tokens": [50986, 1033, 11, 286, 478, 516, 281, 751, 466, 11, 370, 341, 307, 516, 281, 312, 257, 8344, 6191, 751, 11, 457, 51184], "temperature": 0.0, "avg_logprob": -0.17755390268511476, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0409013032913208}, {"id": 17, "seek": 5944, "start": 75.84, "end": 82.84, "text": " not very technical, but to tell you less about the possibilities that are offered by LLM", "tokens": [51184, 406, 588, 6191, 11, 457, 281, 980, 291, 1570, 466, 264, 12178, 300, 366, 8059, 538, 441, 43, 44, 51534], "temperature": 0.0, "avg_logprob": -0.17755390268511476, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0409013032913208}, {"id": 18, "seek": 5944, "start": 82.84, "end": 84.64, "text": " and more about their limitations.", "tokens": [51534, 293, 544, 466, 641, 15705, 13, 51624], "temperature": 0.0, "avg_logprob": -0.17755390268511476, "compression_ratio": 1.6496062992125984, "no_speech_prob": 0.0409013032913208}, {"id": 19, "seek": 8464, "start": 84.64, "end": 89.72, "text": " And basically tell you about what I think is coming next, though at least what I'm working", "tokens": [50364, 400, 1936, 980, 291, 466, 437, 286, 519, 307, 1348, 958, 11, 1673, 412, 1935, 437, 286, 478, 1364, 50618], "temperature": 0.0, "avg_logprob": -0.1800533753854257, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.05801676586270332}, {"id": 20, "seek": 8464, "start": 89.72, "end": 92.4, "text": " towards coming next.", "tokens": [50618, 3030, 1348, 958, 13, 50752], "temperature": 0.0, "avg_logprob": -0.1800533753854257, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.05801676586270332}, {"id": 21, "seek": 8464, "start": 92.4, "end": 97.16, "text": " And the first thing we should realize is that machine learning really sucks compared to what", "tokens": [50752, 400, 264, 700, 551, 321, 820, 4325, 307, 300, 3479, 2539, 534, 15846, 5347, 281, 437, 50990], "temperature": 0.0, "avg_logprob": -0.1800533753854257, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.05801676586270332}, {"id": 22, "seek": 8464, "start": 97.16, "end": 99.08, "text": " we observe in humans and animals.", "tokens": [50990, 321, 11441, 294, 6255, 293, 4882, 13, 51086], "temperature": 0.0, "avg_logprob": -0.1800533753854257, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.05801676586270332}, {"id": 23, "seek": 8464, "start": 99.08, "end": 103.76, "text": " The capabilities of the learning systems that we have today are really terrible.", "tokens": [51086, 440, 10862, 295, 264, 2539, 3652, 300, 321, 362, 965, 366, 534, 6237, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1800533753854257, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.05801676586270332}, {"id": 24, "seek": 8464, "start": 103.76, "end": 108.2, "text": " Humans and animals can run new tasks really quickly, understand how the world works, they", "tokens": [51320, 35809, 293, 4882, 393, 1190, 777, 9608, 534, 2661, 11, 1223, 577, 264, 1002, 1985, 11, 436, 51542], "temperature": 0.0, "avg_logprob": -0.1800533753854257, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.05801676586270332}, {"id": 25, "seek": 8464, "start": 108.2, "end": 113.28, "text": " can reason, they can plan, they have some level of common sense.", "tokens": [51542, 393, 1778, 11, 436, 393, 1393, 11, 436, 362, 512, 1496, 295, 2689, 2020, 13, 51796], "temperature": 0.0, "avg_logprob": -0.1800533753854257, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.05801676586270332}, {"id": 26, "seek": 11328, "start": 113.28, "end": 118.76, "text": " Their behavior is driven by objectives or drives, which is not the case for auto-reversed", "tokens": [50364, 6710, 5223, 307, 9555, 538, 15961, 420, 11754, 11, 597, 307, 406, 264, 1389, 337, 8399, 12, 265, 840, 292, 50638], "temperature": 0.0, "avg_logprob": -0.22981584483179554, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.01680879108607769}, {"id": 27, "seek": 11328, "start": 118.76, "end": 119.76, "text": " LLMs.", "tokens": [50638, 441, 43, 26386, 13, 50688], "temperature": 0.0, "avg_logprob": -0.22981584483179554, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.01680879108607769}, {"id": 28, "seek": 11328, "start": 119.76, "end": 125.92, "text": " But there is one thing that both the biological world and the recent machine learning world", "tokens": [50688, 583, 456, 307, 472, 551, 300, 1293, 264, 13910, 1002, 293, 264, 5162, 3479, 2539, 1002, 50996], "temperature": 0.0, "avg_logprob": -0.22981584483179554, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.01680879108607769}, {"id": 29, "seek": 11328, "start": 125.92, "end": 129.44, "text": " have had in common is the use of cell supervised learning.", "tokens": [50996, 362, 632, 294, 2689, 307, 264, 764, 295, 2815, 46533, 2539, 13, 51172], "temperature": 0.0, "avg_logprob": -0.22981584483179554, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.01680879108607769}, {"id": 30, "seek": 11328, "start": 129.44, "end": 133.24, "text": " And really cell supervised learning has taken over the world.", "tokens": [51172, 400, 534, 2815, 46533, 2539, 575, 2726, 670, 264, 1002, 13, 51362], "temperature": 0.0, "avg_logprob": -0.22981584483179554, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.01680879108607769}, {"id": 31, "seek": 11328, "start": 133.24, "end": 137.72, "text": " For both applications in text and natural language understanding, for images, videos,", "tokens": [51362, 1171, 1293, 5821, 294, 2487, 293, 3303, 2856, 3701, 11, 337, 5267, 11, 2145, 11, 51586], "temperature": 0.0, "avg_logprob": -0.22981584483179554, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.01680879108607769}, {"id": 32, "seek": 11328, "start": 137.72, "end": 141.44, "text": " 3D models, speech, protein folding, all that stuff.", "tokens": [51586, 805, 35, 5245, 11, 6218, 11, 7944, 25335, 11, 439, 300, 1507, 13, 51772], "temperature": 0.0, "avg_logprob": -0.22981584483179554, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.01680879108607769}, {"id": 33, "seek": 11328, "start": 141.44, "end": 143.2, "text": " What is cell supervised learning really?", "tokens": [51772, 708, 307, 2815, 46533, 2539, 534, 30, 51860], "temperature": 0.0, "avg_logprob": -0.22981584483179554, "compression_ratio": 1.7330960854092528, "no_speech_prob": 0.01680879108607769}, {"id": 34, "seek": 14320, "start": 143.2, "end": 149.51999999999998, "text": " It's sort of completion really, learning to fill in the blanks, right?", "tokens": [50364, 467, 311, 1333, 295, 19372, 534, 11, 2539, 281, 2836, 294, 264, 8247, 82, 11, 558, 30, 50680], "temperature": 0.0, "avg_logprob": -0.17520456215769975, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.002312329364940524}, {"id": 35, "seek": 14320, "start": 149.51999999999998, "end": 156.48, "text": " So the way it's used in the context of natural language understanding or processing is you", "tokens": [50680, 407, 264, 636, 309, 311, 1143, 294, 264, 4319, 295, 3303, 2856, 3701, 420, 9007, 307, 291, 51028], "temperature": 0.0, "avg_logprob": -0.17520456215769975, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.002312329364940524}, {"id": 36, "seek": 14320, "start": 156.48, "end": 163.51999999999998, "text": " take a piece of text, you mask part of it by removing some other words, masking, replacing", "tokens": [51028, 747, 257, 2522, 295, 2487, 11, 291, 6094, 644, 295, 309, 538, 12720, 512, 661, 2283, 11, 31226, 11, 19139, 51380], "temperature": 0.0, "avg_logprob": -0.17520456215769975, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.002312329364940524}, {"id": 37, "seek": 14320, "start": 163.51999999999998, "end": 166.32, "text": " them by blank markers, for example.", "tokens": [51380, 552, 538, 8247, 19175, 11, 337, 1365, 13, 51520], "temperature": 0.0, "avg_logprob": -0.17520456215769975, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.002312329364940524}, {"id": 38, "seek": 14320, "start": 166.32, "end": 171.51999999999998, "text": " And then so think of it as a type of corruption, it doesn't have to be the masking but it could", "tokens": [51520, 400, 550, 370, 519, 295, 309, 382, 257, 2010, 295, 17959, 11, 309, 1177, 380, 362, 281, 312, 264, 31226, 457, 309, 727, 51780], "temperature": 0.0, "avg_logprob": -0.17520456215769975, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.002312329364940524}, {"id": 39, "seek": 17152, "start": 171.52, "end": 173.56, "text": " be other types of corruptions.", "tokens": [50364, 312, 661, 3467, 295, 17366, 626, 13, 50466], "temperature": 0.0, "avg_logprob": -0.15492301200752828, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.016120968386530876}, {"id": 40, "seek": 17152, "start": 173.56, "end": 179.24, "text": " And then you train some gigantic neural net to predict the words that are missing.", "tokens": [50466, 400, 550, 291, 3847, 512, 26800, 18161, 2533, 281, 6069, 264, 2283, 300, 366, 5361, 13, 50750], "temperature": 0.0, "avg_logprob": -0.15492301200752828, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.016120968386530876}, {"id": 41, "seek": 17152, "start": 179.24, "end": 185.84, "text": " And you just measure the reconstruction error basically on the parts that were missing.", "tokens": [50750, 400, 291, 445, 3481, 264, 31565, 6713, 1936, 322, 264, 3166, 300, 645, 5361, 13, 51080], "temperature": 0.0, "avg_logprob": -0.15492301200752828, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.016120968386530876}, {"id": 42, "seek": 17152, "start": 185.84, "end": 194.28, "text": " In the process of doing so, that system learns to represent text in a way that allows it", "tokens": [51080, 682, 264, 1399, 295, 884, 370, 11, 300, 1185, 27152, 281, 2906, 2487, 294, 257, 636, 300, 4045, 309, 51502], "temperature": 0.0, "avg_logprob": -0.15492301200752828, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.016120968386530876}, {"id": 43, "seek": 19428, "start": 194.28, "end": 206.48, "text": " to store or represent meaning, grammar, everything, syntax, semantics, everything there is to", "tokens": [50364, 281, 3531, 420, 2906, 3620, 11, 22317, 11, 1203, 11, 28431, 11, 4361, 45298, 11, 1203, 456, 307, 281, 50974], "temperature": 0.0, "avg_logprob": -0.17663066451613968, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.10304295271635056}, {"id": 44, "seek": 19428, "start": 206.48, "end": 212.76, "text": " represent about language in the internal representation, which you can subsequently use for any downstream", "tokens": [50974, 2906, 466, 2856, 294, 264, 6920, 10290, 11, 597, 291, 393, 26514, 764, 337, 604, 30621, 51288], "temperature": 0.0, "avg_logprob": -0.17663066451613968, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.10304295271635056}, {"id": 45, "seek": 19428, "start": 212.76, "end": 217.52, "text": " task like say translation or topic classification or something of that type.", "tokens": [51288, 5633, 411, 584, 12853, 420, 4829, 21538, 420, 746, 295, 300, 2010, 13, 51526], "temperature": 0.0, "avg_logprob": -0.17663066451613968, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.10304295271635056}, {"id": 46, "seek": 19428, "start": 217.52, "end": 223.0, "text": " So this works amazingly well in the context of text, particularly because text is easy", "tokens": [51526, 407, 341, 1985, 31762, 731, 294, 264, 4319, 295, 2487, 11, 4098, 570, 2487, 307, 1858, 51800], "temperature": 0.0, "avg_logprob": -0.17663066451613968, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.10304295271635056}, {"id": 47, "seek": 22300, "start": 223.08, "end": 229.12, "text": " to predict with uncertainty, you can never predict exactly what word will appear at a", "tokens": [50368, 281, 6069, 365, 15697, 11, 291, 393, 1128, 6069, 2293, 437, 1349, 486, 4204, 412, 257, 50670], "temperature": 0.0, "avg_logprob": -0.20512454406074856, "compression_ratio": 1.831858407079646, "no_speech_prob": 0.08559873700141907}, {"id": 48, "seek": 22300, "start": 229.12, "end": 234.44, "text": " particular location, but what you can do is predict some sort of probability distribution", "tokens": [50670, 1729, 4914, 11, 457, 437, 291, 393, 360, 307, 6069, 512, 1333, 295, 8482, 7316, 50936], "temperature": 0.0, "avg_logprob": -0.20512454406074856, "compression_ratio": 1.831858407079646, "no_speech_prob": 0.08559873700141907}, {"id": 49, "seek": 22300, "start": 234.44, "end": 236.44, "text": " of all possible words in the dictionary.", "tokens": [50936, 295, 439, 1944, 2283, 294, 264, 25890, 13, 51036], "temperature": 0.0, "avg_logprob": -0.20512454406074856, "compression_ratio": 1.831858407079646, "no_speech_prob": 0.08559873700141907}, {"id": 50, "seek": 22300, "start": 236.44, "end": 241.2, "text": " And you can do this because there's only a finite number of words in your dictionary", "tokens": [51036, 400, 291, 393, 360, 341, 570, 456, 311, 787, 257, 19362, 1230, 295, 2283, 294, 428, 25890, 51274], "temperature": 0.0, "avg_logprob": -0.20512454406074856, "compression_ratio": 1.831858407079646, "no_speech_prob": 0.08559873700141907}, {"id": 51, "seek": 22300, "start": 241.2, "end": 242.72, "text": " or tokens.", "tokens": [51274, 420, 22667, 13, 51350], "temperature": 0.0, "avg_logprob": -0.20512454406074856, "compression_ratio": 1.831858407079646, "no_speech_prob": 0.08559873700141907}, {"id": 52, "seek": 22300, "start": 242.72, "end": 247.36, "text": " And so you can compute this distribution easily and handle uncertainty in the prediction pretty", "tokens": [51350, 400, 370, 291, 393, 14722, 341, 7316, 3612, 293, 4813, 15697, 294, 264, 17630, 1238, 51582], "temperature": 0.0, "avg_logprob": -0.20512454406074856, "compression_ratio": 1.831858407079646, "no_speech_prob": 0.08559873700141907}, {"id": 53, "seek": 22300, "start": 247.36, "end": 248.36, "text": " well.", "tokens": [51582, 731, 13, 51632], "temperature": 0.0, "avg_logprob": -0.20512454406074856, "compression_ratio": 1.831858407079646, "no_speech_prob": 0.08559873700141907}, {"id": 54, "seek": 24836, "start": 248.92000000000002, "end": 253.48000000000002, "text": " More generally, self-supervised learning is really sort of learning to capture dependencies", "tokens": [50392, 5048, 5101, 11, 2698, 12, 48172, 24420, 2539, 307, 534, 1333, 295, 2539, 281, 7983, 36606, 50620], "temperature": 0.0, "avg_logprob": -0.19134922416842715, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.005600377451628447}, {"id": 55, "seek": 24836, "start": 253.48000000000002, "end": 257.6, "text": " between inputs.", "tokens": [50620, 1296, 15743, 13, 50826], "temperature": 0.0, "avg_logprob": -0.19134922416842715, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.005600377451628447}, {"id": 56, "seek": 24836, "start": 257.6, "end": 263.52000000000004, "text": " And so if you wanted to apply this to the problem of video prediction, for example, you would", "tokens": [50826, 400, 370, 498, 291, 1415, 281, 3079, 341, 281, 264, 1154, 295, 960, 17630, 11, 337, 1365, 11, 291, 576, 51122], "temperature": 0.0, "avg_logprob": -0.19134922416842715, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.005600377451628447}, {"id": 57, "seek": 24836, "start": 263.52000000000004, "end": 268.40000000000003, "text": " show a segment of video to a system and then ask it to predict what's going to happen next,", "tokens": [51122, 855, 257, 9469, 295, 960, 281, 257, 1185, 293, 550, 1029, 309, 281, 6069, 437, 311, 516, 281, 1051, 958, 11, 51366], "temperature": 0.0, "avg_logprob": -0.19134922416842715, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.005600377451628447}, {"id": 58, "seek": 24836, "start": 268.40000000000003, "end": 272.12, "text": " for example, in the video and then reveal the future of the video.", "tokens": [51366, 337, 1365, 11, 294, 264, 960, 293, 550, 10658, 264, 2027, 295, 264, 960, 13, 51552], "temperature": 0.0, "avg_logprob": -0.19134922416842715, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.005600377451628447}, {"id": 59, "seek": 24836, "start": 272.12, "end": 274.76, "text": " And again, I apologize for the colors.", "tokens": [51552, 400, 797, 11, 286, 12328, 337, 264, 4577, 13, 51684], "temperature": 0.0, "avg_logprob": -0.19134922416842715, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.005600377451628447}, {"id": 60, "seek": 27476, "start": 275.76, "end": 279.48, "text": " And then the system could adapt itself so that it does a better job at predicting what", "tokens": [50414, 400, 550, 264, 1185, 727, 6231, 2564, 370, 300, 309, 775, 257, 1101, 1691, 412, 32884, 437, 50600], "temperature": 0.0, "avg_logprob": -0.13087438110612395, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0001712517550913617}, {"id": 61, "seek": 27476, "start": 279.48, "end": 280.68, "text": " happened next in the video.", "tokens": [50600, 2011, 958, 294, 264, 960, 13, 50660], "temperature": 0.0, "avg_logprob": -0.13087438110612395, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0001712517550913617}, {"id": 62, "seek": 27476, "start": 280.68, "end": 284.84, "text": " Now, unfortunately, it's much harder to do for video than it is for text.", "tokens": [50660, 823, 11, 7015, 11, 309, 311, 709, 6081, 281, 360, 337, 960, 813, 309, 307, 337, 2487, 13, 50868], "temperature": 0.0, "avg_logprob": -0.13087438110612395, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0001712517550913617}, {"id": 63, "seek": 27476, "start": 284.84, "end": 291.08, "text": " So much harder than it might require other methods than the type of generative methods", "tokens": [50868, 407, 709, 6081, 813, 309, 1062, 3651, 661, 7150, 813, 264, 2010, 295, 1337, 1166, 7150, 51180], "temperature": 0.0, "avg_logprob": -0.13087438110612395, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0001712517550913617}, {"id": 64, "seek": 27476, "start": 291.08, "end": 292.36, "text": " that work well for text.", "tokens": [51180, 300, 589, 731, 337, 2487, 13, 51244], "temperature": 0.0, "avg_logprob": -0.13087438110612395, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0001712517550913617}, {"id": 65, "seek": 27476, "start": 292.36, "end": 293.76, "text": " I'll come back to this.", "tokens": [51244, 286, 603, 808, 646, 281, 341, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13087438110612395, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0001712517550913617}, {"id": 66, "seek": 27476, "start": 293.76, "end": 299.44, "text": " So speaking of generative methods, but generative AI and autoregressive language models is something", "tokens": [51314, 407, 4124, 295, 1337, 1166, 7150, 11, 457, 1337, 1166, 7318, 293, 1476, 418, 3091, 488, 2856, 5245, 307, 746, 51598], "temperature": 0.0, "avg_logprob": -0.13087438110612395, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0001712517550913617}, {"id": 67, "seek": 27476, "start": 299.44, "end": 303.88, "text": " that many of us have been hearing about recently.", "tokens": [51598, 300, 867, 295, 505, 362, 668, 4763, 466, 3938, 13, 51820], "temperature": 0.0, "avg_logprob": -0.13087438110612395, "compression_ratio": 1.7527675276752768, "no_speech_prob": 0.0001712517550913617}, {"id": 68, "seek": 30476, "start": 305.28, "end": 306.88, "text": " What is it?", "tokens": [50390, 708, 307, 309, 30, 50470], "temperature": 0.0, "avg_logprob": -0.17588919924016586, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0013845821376889944}, {"id": 69, "seek": 30476, "start": 306.88, "end": 311.56, "text": " Probably most of you know already, but essentially the way you train them is very similar to", "tokens": [50470, 9210, 881, 295, 291, 458, 1217, 11, 457, 4476, 264, 636, 291, 3847, 552, 307, 588, 2531, 281, 50704], "temperature": 0.0, "avg_logprob": -0.17588919924016586, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0013845821376889944}, {"id": 70, "seek": 30476, "start": 311.56, "end": 312.76, "text": " the self-supervised learning.", "tokens": [50704, 264, 2698, 12, 48172, 24420, 2539, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17588919924016586, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0013845821376889944}, {"id": 71, "seek": 30476, "start": 312.76, "end": 316.44, "text": " It's in fact a special case of self-supervised learning method I just mentioned.", "tokens": [50764, 467, 311, 294, 1186, 257, 2121, 1389, 295, 2698, 12, 48172, 24420, 2539, 3170, 286, 445, 2835, 13, 50948], "temperature": 0.0, "avg_logprob": -0.17588919924016586, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0013845821376889944}, {"id": 72, "seek": 30476, "start": 316.44, "end": 323.76, "text": " You take a sequence of tokens, words, whatever it is, a sequence of vectors.", "tokens": [50948, 509, 747, 257, 8310, 295, 22667, 11, 2283, 11, 2035, 309, 307, 11, 257, 8310, 295, 18875, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17588919924016586, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0013845821376889944}, {"id": 73, "seek": 30476, "start": 323.76, "end": 326.92, "text": " As long as you can turn things into vectors, you're OK.", "tokens": [51314, 1018, 938, 382, 291, 393, 1261, 721, 666, 18875, 11, 291, 434, 2264, 13, 51472], "temperature": 0.0, "avg_logprob": -0.17588919924016586, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0013845821376889944}, {"id": 74, "seek": 30476, "start": 326.92, "end": 333.24, "text": " And then you only mask the last one and train a system to predict the last token in the", "tokens": [51472, 400, 550, 291, 787, 6094, 264, 1036, 472, 293, 3847, 257, 1185, 281, 6069, 264, 1036, 14862, 294, 264, 51788], "temperature": 0.0, "avg_logprob": -0.17588919924016586, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.0013845821376889944}, {"id": 75, "seek": 33324, "start": 333.24, "end": 334.24, "text": " sequence.", "tokens": [50364, 8310, 13, 50414], "temperature": 0.0, "avg_logprob": -0.15905894129729467, "compression_ratio": 2.0176211453744495, "no_speech_prob": 0.001954843057319522}, {"id": 76, "seek": 33324, "start": 334.24, "end": 340.44, "text": " I mean, technically, you do more than that, but that's what it comes down to in the end.", "tokens": [50414, 286, 914, 11, 12120, 11, 291, 360, 544, 813, 300, 11, 457, 300, 311, 437, 309, 1487, 760, 281, 294, 264, 917, 13, 50724], "temperature": 0.0, "avg_logprob": -0.15905894129729467, "compression_ratio": 2.0176211453744495, "no_speech_prob": 0.001954843057319522}, {"id": 77, "seek": 33324, "start": 340.44, "end": 344.2, "text": " And once you have a system that has been trained to produce the next token, you can use it", "tokens": [50724, 400, 1564, 291, 362, 257, 1185, 300, 575, 668, 8895, 281, 5258, 264, 958, 14862, 11, 291, 393, 764, 309, 50912], "temperature": 0.0, "avg_logprob": -0.15905894129729467, "compression_ratio": 2.0176211453744495, "no_speech_prob": 0.001954843057319522}, {"id": 78, "seek": 33324, "start": 344.2, "end": 350.48, "text": " autoregressively, recursively, basically to predict then the next next token and et cetera.", "tokens": [50912, 1476, 418, 3091, 3413, 11, 20560, 3413, 11, 1936, 281, 6069, 550, 264, 958, 958, 14862, 293, 1030, 11458, 13, 51226], "temperature": 0.0, "avg_logprob": -0.15905894129729467, "compression_ratio": 2.0176211453744495, "no_speech_prob": 0.001954843057319522}, {"id": 79, "seek": 33324, "start": 350.48, "end": 355.08, "text": " So you predict the next token, you shift it into the input, then predict the next next", "tokens": [51226, 407, 291, 6069, 264, 958, 14862, 11, 291, 5513, 309, 666, 264, 4846, 11, 550, 6069, 264, 958, 958, 51456], "temperature": 0.0, "avg_logprob": -0.15905894129729467, "compression_ratio": 2.0176211453744495, "no_speech_prob": 0.001954843057319522}, {"id": 80, "seek": 33324, "start": 355.08, "end": 358.36, "text": " token, shift that into the input, et cetera.", "tokens": [51456, 14862, 11, 5513, 300, 666, 264, 4846, 11, 1030, 11458, 13, 51620], "temperature": 0.0, "avg_logprob": -0.15905894129729467, "compression_ratio": 2.0176211453744495, "no_speech_prob": 0.001954843057319522}, {"id": 81, "seek": 33324, "start": 358.36, "end": 360.2, "text": " And that's called autoregressive prediction.", "tokens": [51620, 400, 300, 311, 1219, 1476, 418, 3091, 488, 17630, 13, 51712], "temperature": 0.0, "avg_logprob": -0.15905894129729467, "compression_ratio": 2.0176211453744495, "no_speech_prob": 0.001954843057319522}, {"id": 82, "seek": 36020, "start": 360.2, "end": 366.15999999999997, "text": " It's an old concept going back to signal processing many years ago, many decades ago, as a matter", "tokens": [50364, 467, 311, 364, 1331, 3410, 516, 646, 281, 6358, 9007, 867, 924, 2057, 11, 867, 7878, 2057, 11, 382, 257, 1871, 50662], "temperature": 0.0, "avg_logprob": -0.15113401925691994, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00496056629344821}, {"id": 83, "seek": 36020, "start": 366.15999999999997, "end": 367.15999999999997, "text": " of fact.", "tokens": [50662, 295, 1186, 13, 50712], "temperature": 0.0, "avg_logprob": -0.15113401925691994, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00496056629344821}, {"id": 84, "seek": 36020, "start": 367.15999999999997, "end": 372.32, "text": " So nothing new there, but that allows the system to basically predict one token after the other", "tokens": [50712, 407, 1825, 777, 456, 11, 457, 300, 4045, 264, 1185, 281, 1936, 6069, 472, 14862, 934, 264, 661, 50970], "temperature": 0.0, "avg_logprob": -0.15113401925691994, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00496056629344821}, {"id": 85, "seek": 36020, "start": 372.32, "end": 374.44, "text": " and generate text.", "tokens": [50970, 293, 8460, 2487, 13, 51076], "temperature": 0.0, "avg_logprob": -0.15113401925691994, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00496056629344821}, {"id": 86, "seek": 36020, "start": 374.44, "end": 379.64, "text": " So those things work amazingly well.", "tokens": [51076, 407, 729, 721, 589, 31762, 731, 13, 51336], "temperature": 0.0, "avg_logprob": -0.15113401925691994, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00496056629344821}, {"id": 87, "seek": 36020, "start": 379.64, "end": 380.64, "text": " Performance is really amazing.", "tokens": [51336, 25047, 307, 534, 2243, 13, 51386], "temperature": 0.0, "avg_logprob": -0.15113401925691994, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00496056629344821}, {"id": 88, "seek": 36020, "start": 380.64, "end": 384.71999999999997, "text": " The fact that, you know, they're trained only on text, even though on enormous amounts", "tokens": [51386, 440, 1186, 300, 11, 291, 458, 11, 436, 434, 8895, 787, 322, 2487, 11, 754, 1673, 322, 11322, 11663, 51590], "temperature": 0.0, "avg_logprob": -0.15113401925691994, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00496056629344821}, {"id": 89, "seek": 38472, "start": 384.72, "end": 392.88000000000005, "text": " of text, but only on text, the amount of knowledge, if you want, that they capture from text is", "tokens": [50364, 295, 2487, 11, 457, 787, 322, 2487, 11, 264, 2372, 295, 3601, 11, 498, 291, 528, 11, 300, 436, 7983, 490, 2487, 307, 50772], "temperature": 0.0, "avg_logprob": -0.1889513848533093, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.005807267036288977}, {"id": 90, "seek": 38472, "start": 392.88000000000005, "end": 397.76000000000005, "text": " pretty amazing and surprised a lot of people.", "tokens": [50772, 1238, 2243, 293, 6100, 257, 688, 295, 561, 13, 51016], "temperature": 0.0, "avg_logprob": -0.1889513848533093, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.005807267036288977}, {"id": 91, "seek": 38472, "start": 397.76000000000005, "end": 403.08000000000004, "text": " Those systems typically have billions or up to hundreds of billions of parameters.", "tokens": [51016, 3950, 3652, 5850, 362, 17375, 420, 493, 281, 6779, 295, 17375, 295, 9834, 13, 51282], "temperature": 0.0, "avg_logprob": -0.1889513848533093, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.005807267036288977}, {"id": 92, "seek": 38472, "start": 403.08000000000004, "end": 407.12, "text": " They train typically on one to two trillion tokens.", "tokens": [51282, 814, 3847, 5850, 322, 472, 281, 732, 18723, 22667, 13, 51484], "temperature": 0.0, "avg_logprob": -0.1889513848533093, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.005807267036288977}, {"id": 93, "seek": 38472, "start": 407.12, "end": 408.12, "text": " Sometimes more.", "tokens": [51484, 4803, 544, 13, 51534], "temperature": 0.0, "avg_logprob": -0.1889513848533093, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.005807267036288977}, {"id": 94, "seek": 40812, "start": 408.2, "end": 418.36, "text": " Their input window is anywhere between 2000 and maybe a few tens of thousands of tokens", "tokens": [50368, 6710, 4846, 4910, 307, 4992, 1296, 8132, 293, 1310, 257, 1326, 10688, 295, 5383, 295, 22667, 50876], "temperature": 0.0, "avg_logprob": -0.3461847187560282, "compression_ratio": 1.4223300970873787, "no_speech_prob": 0.2412484586238861}, {"id": 95, "seek": 40812, "start": 418.36, "end": 421.76, "text": " for their context window.", "tokens": [50876, 337, 641, 4319, 4910, 13, 51046], "temperature": 0.0, "avg_logprob": -0.3461847187560282, "compression_ratio": 1.4223300970873787, "no_speech_prob": 0.2412484586238861}, {"id": 96, "seek": 40812, "start": 421.76, "end": 426.8, "text": " And there's been a long history of such models that have been put out, the sort of GPT family,", "tokens": [51046, 400, 456, 311, 668, 257, 938, 2503, 295, 1270, 5245, 300, 362, 668, 829, 484, 11, 264, 1333, 295, 26039, 51, 1605, 11, 51298], "temperature": 0.0, "avg_logprob": -0.3461847187560282, "compression_ratio": 1.4223300970873787, "no_speech_prob": 0.2412484586238861}, {"id": 97, "seek": 40812, "start": 426.8, "end": 433.92, "text": " starting with GPT-123, from FAIR, there's been Blunderbot, Galactica, Llama, Version", "tokens": [51298, 2891, 365, 26039, 51, 12, 4762, 18, 11, 490, 19894, 7740, 11, 456, 311, 668, 2177, 6617, 18870, 11, 7336, 578, 2262, 11, 32717, 2404, 11, 35965, 51654], "temperature": 0.0, "avg_logprob": -0.3461847187560282, "compression_ratio": 1.4223300970873787, "no_speech_prob": 0.2412484586238861}, {"id": 98, "seek": 43392, "start": 434.72, "end": 440.04, "text": " 2 that just came out this week, Alpaca from Stanford, which is a fine-tuned version of", "tokens": [50404, 568, 300, 445, 1361, 484, 341, 1243, 11, 967, 79, 6628, 490, 20374, 11, 597, 307, 257, 2489, 12, 83, 43703, 3037, 295, 50670], "temperature": 0.0, "avg_logprob": -0.2716565954274145, "compression_ratio": 1.5225563909774436, "no_speech_prob": 0.019019192084670067}, {"id": 99, "seek": 43392, "start": 440.04, "end": 446.24, "text": " Llama, Llama, Version 1, Lambda and Bard from Google, Chinche from DeepMind, you know, and", "tokens": [50670, 32717, 2404, 11, 32717, 2404, 11, 35965, 502, 11, 45691, 293, 26841, 490, 3329, 11, 4430, 1876, 490, 14895, 44, 471, 11, 291, 458, 11, 293, 50980], "temperature": 0.0, "avg_logprob": -0.2716565954274145, "compression_ratio": 1.5225563909774436, "no_speech_prob": 0.019019192084670067}, {"id": 100, "seek": 43392, "start": 446.24, "end": 451.16, "text": " of course, GPDT, GPT-4 from OpenAI.", "tokens": [50980, 295, 1164, 11, 460, 17349, 51, 11, 26039, 51, 12, 19, 490, 7238, 48698, 13, 51226], "temperature": 0.0, "avg_logprob": -0.2716565954274145, "compression_ratio": 1.5225563909774436, "no_speech_prob": 0.019019192084670067}, {"id": 101, "seek": 43392, "start": 451.16, "end": 457.64, "text": " And they're really good at, as writing aids, but they have really limited knowledge of", "tokens": [51226, 400, 436, 434, 534, 665, 412, 11, 382, 3579, 28447, 11, 457, 436, 362, 534, 5567, 3601, 295, 51550], "temperature": 0.0, "avg_logprob": -0.2716565954274145, "compression_ratio": 1.5225563909774436, "no_speech_prob": 0.019019192084670067}, {"id": 102, "seek": 43392, "start": 457.64, "end": 462.6, "text": " the underlying reality because they're purely trained from text, at least for the vast majority", "tokens": [51550, 264, 14217, 4103, 570, 436, 434, 17491, 8895, 490, 2487, 11, 412, 1935, 337, 264, 8369, 6286, 51798], "temperature": 0.0, "avg_logprob": -0.2716565954274145, "compression_ratio": 1.5225563909774436, "no_speech_prob": 0.019019192084670067}, {"id": 103, "seek": 43392, "start": 462.6, "end": 463.6, "text": " of them.", "tokens": [51798, 295, 552, 13, 51848], "temperature": 0.0, "avg_logprob": -0.2716565954274145, "compression_ratio": 1.5225563909774436, "no_speech_prob": 0.019019192084670067}, {"id": 104, "seek": 46360, "start": 463.6, "end": 467.52000000000004, "text": " And they really have no common sense or very limited common sense, and they have limited", "tokens": [50364, 400, 436, 534, 362, 572, 2689, 2020, 420, 588, 5567, 2689, 2020, 11, 293, 436, 362, 5567, 50560], "temperature": 0.0, "avg_logprob": -0.23174151544985563, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0026256046257913113}, {"id": 105, "seek": 46360, "start": 467.52000000000004, "end": 472.0, "text": " abilities to plan their answers because the answers are produced autoregressively.", "tokens": [50560, 11582, 281, 1393, 641, 6338, 570, 264, 6338, 366, 7126, 1476, 418, 3091, 3413, 13, 50784], "temperature": 0.0, "avg_logprob": -0.23174151544985563, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0026256046257913113}, {"id": 106, "seek": 46360, "start": 472.0, "end": 478.68, "text": " But still, it's pretty impressive how they work, so, as was mentioned, my colleagues", "tokens": [50784, 583, 920, 11, 309, 311, 1238, 8992, 577, 436, 589, 11, 370, 11, 382, 390, 2835, 11, 452, 7734, 51118], "temperature": 0.0, "avg_logprob": -0.23174151544985563, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0026256046257913113}, {"id": 107, "seek": 46360, "start": 478.68, "end": 482.84000000000003, "text": " just put out an open source LLM called Llama 2.", "tokens": [51118, 445, 829, 484, 364, 1269, 4009, 441, 43, 44, 1219, 32717, 2404, 568, 13, 51326], "temperature": 0.0, "avg_logprob": -0.23174151544985563, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0026256046257913113}, {"id": 108, "seek": 46360, "start": 482.84000000000003, "end": 488.96000000000004, "text": " There is three versions of this at the moment, 7 billion, 13 billion and 70 billion parameters.", "tokens": [51326, 821, 307, 1045, 9606, 295, 341, 412, 264, 1623, 11, 1614, 5218, 11, 3705, 5218, 293, 5285, 5218, 9834, 13, 51632], "temperature": 0.0, "avg_logprob": -0.23174151544985563, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0026256046257913113}, {"id": 109, "seek": 46360, "start": 488.96000000000004, "end": 492.48, "text": " The license is fairly liberal, so it can be used commercially if you want.", "tokens": [51632, 440, 10476, 307, 6457, 13767, 11, 370, 309, 393, 312, 1143, 41751, 498, 291, 528, 13, 51808], "temperature": 0.0, "avg_logprob": -0.23174151544985563, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0026256046257913113}, {"id": 110, "seek": 49248, "start": 493.48, "end": 497.96000000000004, "text": " If you want to start a startup and use it as a business, you can.", "tokens": [50414, 759, 291, 528, 281, 722, 257, 18578, 293, 764, 309, 382, 257, 1606, 11, 291, 393, 13, 50638], "temperature": 0.0, "avg_logprob": -0.20294922722710504, "compression_ratio": 1.4751131221719458, "no_speech_prob": 0.01315242052078247}, {"id": 111, "seek": 49248, "start": 497.96000000000004, "end": 505.04, "text": " It's also available on sort of various cloud services, easy to use.", "tokens": [50638, 467, 311, 611, 2435, 322, 1333, 295, 3683, 4588, 3328, 11, 1858, 281, 764, 13, 50992], "temperature": 0.0, "avg_logprob": -0.20294922722710504, "compression_ratio": 1.4751131221719458, "no_speech_prob": 0.01315242052078247}, {"id": 112, "seek": 49248, "start": 505.04, "end": 511.52000000000004, "text": " So that very fresh just last week has been pre-trained with two trillion tokens.", "tokens": [50992, 407, 300, 588, 4451, 445, 1036, 1243, 575, 668, 659, 12, 17227, 2001, 365, 732, 18723, 22667, 13, 51316], "temperature": 0.0, "avg_logprob": -0.20294922722710504, "compression_ratio": 1.4751131221719458, "no_speech_prob": 0.01315242052078247}, {"id": 113, "seek": 49248, "start": 511.52000000000004, "end": 520.6800000000001, "text": " The context length is 4,096, and some versions of it have been fine-tuned for dialogue and", "tokens": [51316, 440, 4319, 4641, 307, 1017, 11, 13811, 21, 11, 293, 512, 9606, 295, 309, 362, 668, 2489, 12, 83, 43703, 337, 10221, 293, 51774], "temperature": 0.0, "avg_logprob": -0.20294922722710504, "compression_ratio": 1.4751131221719458, "no_speech_prob": 0.01315242052078247}, {"id": 114, "seek": 49248, "start": 520.6800000000001, "end": 521.6800000000001, "text": " things of that type.", "tokens": [51774, 721, 295, 300, 2010, 13, 51824], "temperature": 0.0, "avg_logprob": -0.20294922722710504, "compression_ratio": 1.4751131221719458, "no_speech_prob": 0.01315242052078247}, {"id": 115, "seek": 52168, "start": 521.68, "end": 530.16, "text": " It compares favorably to other systems, either open or closed source on a number of benchmarks.", "tokens": [50364, 467, 38334, 2294, 1188, 281, 661, 3652, 11, 2139, 1269, 420, 5395, 4009, 322, 257, 1230, 295, 43751, 13, 50788], "temperature": 0.0, "avg_logprob": -0.21659756945325181, "compression_ratio": 1.5586854460093897, "no_speech_prob": 0.0017988126492127776}, {"id": 116, "seek": 52168, "start": 530.16, "end": 534.8, "text": " But the essential characteristic of it is that it's open.", "tokens": [50788, 583, 264, 7115, 16282, 295, 309, 307, 300, 309, 311, 1269, 13, 51020], "temperature": 0.0, "avg_logprob": -0.21659756945325181, "compression_ratio": 1.5586854460093897, "no_speech_prob": 0.0017988126492127776}, {"id": 117, "seek": 52168, "start": 534.8, "end": 542.4399999999999, "text": " And together with the model, we released a piece of text that a lot of people signed.", "tokens": [51020, 400, 1214, 365, 264, 2316, 11, 321, 4736, 257, 2522, 295, 2487, 300, 257, 688, 295, 561, 8175, 13, 51402], "temperature": 0.0, "avg_logprob": -0.21659756945325181, "compression_ratio": 1.5586854460093897, "no_speech_prob": 0.0017988126492127776}, {"id": 118, "seek": 52168, "start": 542.4399999999999, "end": 546.56, "text": " The text says, we support an open innovation approach to AI, responsible and open innovation", "tokens": [51402, 440, 2487, 1619, 11, 321, 1406, 364, 1269, 8504, 3109, 281, 7318, 11, 6250, 293, 1269, 8504, 51608], "temperature": 0.0, "avg_logprob": -0.21659756945325181, "compression_ratio": 1.5586854460093897, "no_speech_prob": 0.0017988126492127776}, {"id": 119, "seek": 54656, "start": 546.56, "end": 552.16, "text": " gives us all the stakes in the AI development process, bringing visibility, equity and trust", "tokens": [50364, 2709, 505, 439, 264, 28429, 294, 264, 7318, 3250, 1399, 11, 5062, 19883, 11, 10769, 293, 3361, 50644], "temperature": 0.0, "avg_logprob": -0.1647908616774153, "compression_ratio": 1.6183745583038869, "no_speech_prob": 0.18729761242866516}, {"id": 120, "seek": 54656, "start": 552.16, "end": 557.76, "text": " to these technologies, opening today's LLM model will let everyone benefit from this technology.", "tokens": [50644, 281, 613, 7943, 11, 5193, 965, 311, 441, 43, 44, 2316, 486, 718, 1518, 5121, 490, 341, 2899, 13, 50924], "temperature": 0.0, "avg_logprob": -0.1647908616774153, "compression_ratio": 1.6183745583038869, "no_speech_prob": 0.18729761242866516}, {"id": 121, "seek": 54656, "start": 557.76, "end": 564.0, "text": " So what you have to understand is that at the government level, there is kind of a fork", "tokens": [50924, 407, 437, 291, 362, 281, 1223, 307, 300, 412, 264, 2463, 1496, 11, 456, 307, 733, 295, 257, 17716, 51236], "temperature": 0.0, "avg_logprob": -0.1647908616774153, "compression_ratio": 1.6183745583038869, "no_speech_prob": 0.18729761242866516}, {"id": 122, "seek": 54656, "start": 564.0, "end": 569.3599999999999, "text": " in the road where people are wondering whether AI, because it's powerful, should be kept", "tokens": [51236, 294, 264, 3060, 689, 561, 366, 6359, 1968, 7318, 11, 570, 309, 311, 4005, 11, 820, 312, 4305, 51504], "temperature": 0.0, "avg_logprob": -0.1647908616774153, "compression_ratio": 1.6183745583038869, "no_speech_prob": 0.18729761242866516}, {"id": 123, "seek": 54656, "start": 569.3599999999999, "end": 575.1199999999999, "text": " under lock and key and controlled and heavily regulated, or whether an open source approach", "tokens": [51504, 833, 4017, 293, 2141, 293, 10164, 293, 10950, 26243, 11, 420, 1968, 364, 1269, 4009, 3109, 51792], "temperature": 0.0, "avg_logprob": -0.1647908616774153, "compression_ratio": 1.6183745583038869, "no_speech_prob": 0.18729761242866516}, {"id": 124, "seek": 57512, "start": 575.12, "end": 576.12, "text": " is preferable.", "tokens": [50364, 307, 4382, 712, 13, 50414], "temperature": 0.0, "avg_logprob": -0.19756992383934985, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.01778279058635235}, {"id": 125, "seek": 57512, "start": 576.12, "end": 582.52, "text": " Yes, they are dangerous, but historically, it's quite the case that there's a lot of", "tokens": [50414, 1079, 11, 436, 366, 5795, 11, 457, 16180, 11, 309, 311, 1596, 264, 1389, 300, 456, 311, 257, 688, 295, 50734], "temperature": 0.0, "avg_logprob": -0.19756992383934985, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.01778279058635235}, {"id": 126, "seek": 57512, "start": 582.52, "end": 588.16, "text": " evidence that open source software is actually more secure than the proprietary ones.", "tokens": [50734, 4467, 300, 1269, 4009, 4722, 307, 767, 544, 7144, 813, 264, 38992, 2306, 13, 51016], "temperature": 0.0, "avg_logprob": -0.19756992383934985, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.01778279058635235}, {"id": 127, "seek": 57512, "start": 588.16, "end": 593.84, "text": " And the benefits, the potential benefits of AI and LLM in particular are so large that", "tokens": [51016, 400, 264, 5311, 11, 264, 3995, 5311, 295, 7318, 293, 441, 43, 44, 294, 1729, 366, 370, 2416, 300, 51300], "temperature": 0.0, "avg_logprob": -0.19756992383934985, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.01778279058635235}, {"id": 128, "seek": 57512, "start": 593.84, "end": 598.32, "text": " we'll be shooting ourselves in the foot by kind of keeping this under lock and key.", "tokens": [51300, 321, 603, 312, 5942, 4175, 294, 264, 2671, 538, 733, 295, 5145, 341, 833, 4017, 293, 2141, 13, 51524], "temperature": 0.0, "avg_logprob": -0.19756992383934985, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.01778279058635235}, {"id": 129, "seek": 59832, "start": 598.32, "end": 607.84, "text": " So Meta is definitely on the side of open research, has been for 10 years in AI, but", "tokens": [50364, 407, 6377, 64, 307, 2138, 322, 264, 1252, 295, 1269, 2132, 11, 575, 668, 337, 1266, 924, 294, 7318, 11, 457, 50840], "temperature": 0.0, "avg_logprob": -0.1745735105577406, "compression_ratio": 1.4870689655172413, "no_speech_prob": 0.007427368313074112}, {"id": 130, "seek": 59832, "start": 607.84, "end": 613.6400000000001, "text": " it's still kind of an unsettled question, if you want.", "tokens": [50840, 309, 311, 920, 733, 295, 364, 43964, 1493, 1168, 11, 498, 291, 528, 13, 51130], "temperature": 0.0, "avg_logprob": -0.1745735105577406, "compression_ratio": 1.4870689655172413, "no_speech_prob": 0.007427368313074112}, {"id": 131, "seek": 59832, "start": 613.6400000000001, "end": 619.8000000000001, "text": " I think personally that this will open up the possibility of an entire ecosystem built", "tokens": [51130, 286, 519, 5665, 300, 341, 486, 1269, 493, 264, 7959, 295, 364, 2302, 11311, 3094, 51438], "temperature": 0.0, "avg_logprob": -0.1745735105577406, "compression_ratio": 1.4870689655172413, "no_speech_prob": 0.007427368313074112}, {"id": 132, "seek": 59832, "start": 619.8000000000001, "end": 623.12, "text": " on top of open source base LLM.", "tokens": [51438, 322, 1192, 295, 1269, 4009, 3096, 441, 43, 44, 13, 51604], "temperature": 0.0, "avg_logprob": -0.1745735105577406, "compression_ratio": 1.4870689655172413, "no_speech_prob": 0.007427368313074112}, {"id": 133, "seek": 59832, "start": 623.12, "end": 627.7600000000001, "text": " Training base LLM is very expensive, so we don't need to have 25 different proprietary", "tokens": [51604, 20620, 3096, 441, 43, 44, 307, 588, 5124, 11, 370, 321, 500, 380, 643, 281, 362, 3552, 819, 38992, 51836], "temperature": 0.0, "avg_logprob": -0.1745735105577406, "compression_ratio": 1.4870689655172413, "no_speech_prob": 0.007427368313074112}, {"id": 134, "seek": 62776, "start": 627.76, "end": 628.76, "text": " base LLM.", "tokens": [50364, 3096, 441, 43, 44, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1717579388859296, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.013975750654935837}, {"id": 135, "seek": 62776, "start": 628.76, "end": 633.96, "text": " We basically need a few that are open source so that people can build fine-tuned products", "tokens": [50414, 492, 1936, 643, 257, 1326, 300, 366, 1269, 4009, 370, 300, 561, 393, 1322, 2489, 12, 83, 43703, 3383, 50674], "temperature": 0.0, "avg_logprob": -0.1717579388859296, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.013975750654935837}, {"id": 136, "seek": 62776, "start": 633.96, "end": 634.96, "text": " on top of them.", "tokens": [50674, 322, 1192, 295, 552, 13, 50724], "temperature": 0.0, "avg_logprob": -0.1717579388859296, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.013975750654935837}, {"id": 137, "seek": 62776, "start": 634.96, "end": 639.6, "text": " There's another reason, which is that before I go back to technical questions, which is", "tokens": [50724, 821, 311, 1071, 1778, 11, 597, 307, 300, 949, 286, 352, 646, 281, 6191, 1651, 11, 597, 307, 50956], "temperature": 0.0, "avg_logprob": -0.1717579388859296, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.013975750654935837}, {"id": 138, "seek": 62776, "start": 639.6, "end": 646.64, "text": " that there's going to be a future in which all of our interactions with the digital world", "tokens": [50956, 300, 456, 311, 516, 281, 312, 257, 2027, 294, 597, 439, 295, 527, 13280, 365, 264, 4562, 1002, 51308], "temperature": 0.0, "avg_logprob": -0.1717579388859296, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.013975750654935837}, {"id": 139, "seek": 62776, "start": 646.64, "end": 653.48, "text": " are going to be mediated through AI systems, virtual systems of some type, and it's going", "tokens": [51308, 366, 516, 281, 312, 17269, 770, 807, 7318, 3652, 11, 6374, 3652, 295, 512, 2010, 11, 293, 309, 311, 516, 51650], "temperature": 0.0, "avg_logprob": -0.1717579388859296, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.013975750654935837}, {"id": 140, "seek": 65348, "start": 653.48, "end": 658.24, "text": " to become basically a repository of all human knowledge.", "tokens": [50364, 281, 1813, 1936, 257, 25841, 295, 439, 1952, 3601, 13, 50602], "temperature": 0.0, "avg_logprob": -0.12770601836117831, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.1817324161529541}, {"id": 141, "seek": 65348, "start": 658.24, "end": 666.32, "text": " So we're not going to be interrogating Google or doing a literature search directly anymore.", "tokens": [50602, 407, 321, 434, 406, 516, 281, 312, 24871, 990, 3329, 420, 884, 257, 10394, 3164, 3838, 3602, 13, 51006], "temperature": 0.0, "avg_logprob": -0.12770601836117831, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.1817324161529541}, {"id": 142, "seek": 65348, "start": 666.32, "end": 672.36, "text": " We're just going to be talking to our AI assistant and asking a question and perhaps referring", "tokens": [51006, 492, 434, 445, 516, 281, 312, 1417, 281, 527, 7318, 10994, 293, 3365, 257, 1168, 293, 4317, 13761, 51308], "temperature": 0.0, "avg_logprob": -0.12770601836117831, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.1817324161529541}, {"id": 143, "seek": 65348, "start": 672.36, "end": 676.8000000000001, "text": " to original material and things like that.", "tokens": [51308, 281, 3380, 2527, 293, 721, 411, 300, 13, 51530], "temperature": 0.0, "avg_logprob": -0.12770601836117831, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.1817324161529541}, {"id": 144, "seek": 65348, "start": 676.8000000000001, "end": 680.28, "text": " But basically all of our interactions with the digital world are going to be mediated", "tokens": [51530, 583, 1936, 439, 295, 527, 13280, 365, 264, 4562, 1002, 366, 516, 281, 312, 17269, 770, 51704], "temperature": 0.0, "avg_logprob": -0.12770601836117831, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.1817324161529541}, {"id": 145, "seek": 65348, "start": 680.28, "end": 682.44, "text": " by AI systems.", "tokens": [51704, 538, 7318, 3652, 13, 51812], "temperature": 0.0, "avg_logprob": -0.12770601836117831, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.1817324161529541}, {"id": 146, "seek": 68244, "start": 682.44, "end": 685.0, "text": " So this is going to become the repository of all human knowledge.", "tokens": [50364, 407, 341, 307, 516, 281, 1813, 264, 25841, 295, 439, 1952, 3601, 13, 50492], "temperature": 0.0, "avg_logprob": -0.2014027420355349, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.002285656286403537}, {"id": 147, "seek": 68244, "start": 685.0, "end": 689.6400000000001, "text": " It's going to become a basic infrastructure that everybody wants to use.", "tokens": [50492, 467, 311, 516, 281, 1813, 257, 3875, 6896, 300, 2201, 2738, 281, 764, 13, 50724], "temperature": 0.0, "avg_logprob": -0.2014027420355349, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.002285656286403537}, {"id": 148, "seek": 68244, "start": 689.6400000000001, "end": 693.48, "text": " And history shows that basic infrastructure must be open source.", "tokens": [50724, 400, 2503, 3110, 300, 3875, 6896, 1633, 312, 1269, 4009, 13, 50916], "temperature": 0.0, "avg_logprob": -0.2014027420355349, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.002285656286403537}, {"id": 149, "seek": 68244, "start": 693.48, "end": 698.36, "text": " If you look at the history of the internet, there was a battle between commercial providers,", "tokens": [50916, 759, 291, 574, 412, 264, 2503, 295, 264, 4705, 11, 456, 390, 257, 4635, 1296, 6841, 11330, 11, 51160], "temperature": 0.0, "avg_logprob": -0.2014027420355349, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.002285656286403537}, {"id": 150, "seek": 68244, "start": 698.36, "end": 702.6400000000001, "text": " Microsoft, Sunmacrosystems, and others to provide the software infrastructure of the", "tokens": [51160, 8116, 11, 6163, 37065, 2635, 9321, 82, 11, 293, 2357, 281, 2893, 264, 4722, 6896, 295, 264, 51374], "temperature": 0.0, "avg_logprob": -0.2014027420355349, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.002285656286403537}, {"id": 151, "seek": 68244, "start": 702.6400000000001, "end": 703.6400000000001, "text": " internet.", "tokens": [51374, 4705, 13, 51424], "temperature": 0.0, "avg_logprob": -0.2014027420355349, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.002285656286403537}, {"id": 152, "seek": 68244, "start": 703.6400000000001, "end": 706.72, "text": " All of those commercial providers lost.", "tokens": [51424, 1057, 295, 729, 6841, 11330, 2731, 13, 51578], "temperature": 0.0, "avg_logprob": -0.2014027420355349, "compression_ratio": 1.849785407725322, "no_speech_prob": 0.002285656286403537}, {"id": 153, "seek": 70672, "start": 706.72, "end": 713.72, "text": " What runs the internet today is Linux, Apache, Chrome, Firefox, JavaScript.", "tokens": [50364, 708, 6676, 264, 4705, 965, 307, 18734, 11, 46597, 11, 15327, 11, 46613, 11, 15778, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1422539654900046, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.15265780687332153}, {"id": 154, "seek": 70672, "start": 713.72, "end": 717.08, "text": " It's all open source.", "tokens": [50714, 467, 311, 439, 1269, 4009, 13, 50882], "temperature": 0.0, "avg_logprob": -0.1422539654900046, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.15265780687332153}, {"id": 155, "seek": 70672, "start": 717.08, "end": 721.8000000000001, "text": " So my prediction is the same thing is going to happen in the context of AI.", "tokens": [50882, 407, 452, 17630, 307, 264, 912, 551, 307, 516, 281, 1051, 294, 264, 4319, 295, 7318, 13, 51118], "temperature": 0.0, "avg_logprob": -0.1422539654900046, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.15265780687332153}, {"id": 156, "seek": 70672, "start": 721.8000000000001, "end": 728.84, "text": " And it's necessary because a lot of countries outside the US in particular don't see with", "tokens": [51118, 400, 309, 311, 4818, 570, 257, 688, 295, 3517, 2380, 264, 2546, 294, 1729, 500, 380, 536, 365, 51470], "temperature": 0.0, "avg_logprob": -0.1422539654900046, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.15265780687332153}, {"id": 157, "seek": 70672, "start": 728.84, "end": 734.9200000000001, "text": " a favorable eye the fact that their citizens are going to get all the information from", "tokens": [51470, 257, 29557, 3313, 264, 1186, 300, 641, 7180, 366, 516, 281, 483, 439, 264, 1589, 490, 51774], "temperature": 0.0, "avg_logprob": -0.1422539654900046, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.15265780687332153}, {"id": 158, "seek": 73492, "start": 734.92, "end": 739.0799999999999, "text": " proprietary systems controlled by a small number of tech companies on the west coast", "tokens": [50364, 38992, 3652, 10164, 538, 257, 1359, 1230, 295, 7553, 3431, 322, 264, 7009, 8684, 50572], "temperature": 0.0, "avg_logprob": -0.2060270571927412, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.016351165249943733}, {"id": 159, "seek": 73492, "start": 739.0799999999999, "end": 740.0799999999999, "text": " of the US.", "tokens": [50572, 295, 264, 2546, 13, 50622], "temperature": 0.0, "avg_logprob": -0.2060270571927412, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.016351165249943733}, {"id": 160, "seek": 73492, "start": 740.0799999999999, "end": 746.0799999999999, "text": " So this is just proprietary systems are just not going to fly.", "tokens": [50622, 407, 341, 307, 445, 38992, 3652, 366, 445, 406, 516, 281, 3603, 13, 50922], "temperature": 0.0, "avg_logprob": -0.2060270571927412, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.016351165249943733}, {"id": 161, "seek": 73492, "start": 746.0799999999999, "end": 751.16, "text": " It's just not going to be acceptable to the citizenry across the world.", "tokens": [50922, 467, 311, 445, 406, 516, 281, 312, 15513, 281, 264, 13326, 627, 2108, 264, 1002, 13, 51176], "temperature": 0.0, "avg_logprob": -0.2060270571927412, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.016351165249943733}, {"id": 162, "seek": 73492, "start": 751.16, "end": 752.24, "text": " So it has to be open.", "tokens": [51176, 407, 309, 575, 281, 312, 1269, 13, 51230], "temperature": 0.0, "avg_logprob": -0.2060270571927412, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.016351165249943733}, {"id": 163, "seek": 73492, "start": 752.24, "end": 753.24, "text": " It's inevitable.", "tokens": [51230, 467, 311, 21451, 13, 51280], "temperature": 0.0, "avg_logprob": -0.2060270571927412, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.016351165249943733}, {"id": 164, "seek": 73492, "start": 753.24, "end": 761.28, "text": " In fact, those systems need to be fine-tuned through what has been called RIHF, there's", "tokens": [51280, 682, 1186, 11, 729, 3652, 643, 281, 312, 2489, 12, 83, 43703, 807, 437, 575, 668, 1219, 30474, 39, 37, 11, 456, 311, 51682], "temperature": 0.0, "avg_logprob": -0.2060270571927412, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.016351165249943733}, {"id": 165, "seek": 73492, "start": 761.28, "end": 764.8399999999999, "text": " various ways to fine-tune those systems.", "tokens": [51682, 3683, 2098, 281, 2489, 12, 83, 2613, 729, 3652, 13, 51860], "temperature": 0.0, "avg_logprob": -0.2060270571927412, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.016351165249943733}, {"id": 166, "seek": 76484, "start": 764.84, "end": 769.32, "text": " Because the collection of human knowledge is so large, it includes things like physics,", "tokens": [50364, 1436, 264, 5765, 295, 1952, 3601, 307, 370, 2416, 11, 309, 5974, 721, 411, 10649, 11, 50588], "temperature": 0.0, "avg_logprob": -0.19197901557473576, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.007209775038063526}, {"id": 167, "seek": 76484, "start": 769.32, "end": 777.8000000000001, "text": " like many of you know, it's going to require contributions from millions of people in sort", "tokens": [50588, 411, 867, 295, 291, 458, 11, 309, 311, 516, 281, 3651, 15725, 490, 6803, 295, 561, 294, 1333, 51012], "temperature": 0.0, "avg_logprob": -0.19197901557473576, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.007209775038063526}, {"id": 168, "seek": 76484, "start": 777.8000000000001, "end": 780.84, "text": " of a crowdsourcing fashion.", "tokens": [51012, 295, 257, 26070, 41849, 6700, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19197901557473576, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.007209775038063526}, {"id": 169, "seek": 76484, "start": 780.84, "end": 785.24, "text": " Because basically those systems being the repository of all human knowledge will be sort", "tokens": [51164, 1436, 1936, 729, 3652, 885, 264, 25841, 295, 439, 1952, 3601, 486, 312, 1333, 51384], "temperature": 0.0, "avg_logprob": -0.19197901557473576, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.007209775038063526}, {"id": 170, "seek": 76484, "start": 785.24, "end": 786.24, "text": " of like Wikipedia.", "tokens": [51384, 295, 411, 28999, 13, 51434], "temperature": 0.0, "avg_logprob": -0.19197901557473576, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.007209775038063526}, {"id": 171, "seek": 76484, "start": 786.24, "end": 789.44, "text": " Wikipedia cannot be built by a proprietary company.", "tokens": [51434, 28999, 2644, 312, 3094, 538, 257, 38992, 2237, 13, 51594], "temperature": 0.0, "avg_logprob": -0.19197901557473576, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.007209775038063526}, {"id": 172, "seek": 76484, "start": 789.44, "end": 794.32, "text": " It has to be, it has to gather the entire, the contribution of the entire world.", "tokens": [51594, 467, 575, 281, 312, 11, 309, 575, 281, 5448, 264, 2302, 11, 264, 13150, 295, 264, 2302, 1002, 13, 51838], "temperature": 0.0, "avg_logprob": -0.19197901557473576, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.007209775038063526}, {"id": 173, "seek": 79432, "start": 794.32, "end": 797.5600000000001, "text": " So it's going to be the same thing with AI based systems.", "tokens": [50364, 407, 309, 311, 516, 281, 312, 264, 912, 551, 365, 7318, 2361, 3652, 13, 50526], "temperature": 0.0, "avg_logprob": -0.20792546898427636, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.013129416853189468}, {"id": 174, "seek": 79432, "start": 797.5600000000001, "end": 802.5600000000001, "text": " So open source AI is inevitable, in my opinion, and we're just sort of taking the first step.", "tokens": [50526, 407, 1269, 4009, 7318, 307, 21451, 11, 294, 452, 4800, 11, 293, 321, 434, 445, 1333, 295, 1940, 264, 700, 1823, 13, 50776], "temperature": 0.0, "avg_logprob": -0.20792546898427636, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.013129416853189468}, {"id": 175, "seek": 79432, "start": 802.5600000000001, "end": 803.5600000000001, "text": " Okay.", "tokens": [50776, 1033, 13, 50826], "temperature": 0.0, "avg_logprob": -0.20792546898427636, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.013129416853189468}, {"id": 176, "seek": 79432, "start": 803.5600000000001, "end": 811.72, "text": " And so this Lama 70 billion, which is the largest of the Lama model is pretty interesting.", "tokens": [50826, 400, 370, 341, 441, 2404, 5285, 5218, 11, 597, 307, 264, 6443, 295, 264, 441, 2404, 2316, 307, 1238, 1880, 13, 51234], "temperature": 0.0, "avg_logprob": -0.20792546898427636, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.013129416853189468}, {"id": 177, "seek": 79432, "start": 811.72, "end": 813.5200000000001, "text": " Those are a few examples of what it can generate.", "tokens": [51234, 3950, 366, 257, 1326, 5110, 295, 437, 309, 393, 8460, 13, 51324], "temperature": 0.0, "avg_logprob": -0.20792546898427636, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.013129416853189468}, {"id": 178, "seek": 79432, "start": 813.5200000000001, "end": 822.2, "text": " These are extracted from the paper that you can read from the main website.", "tokens": [51324, 1981, 366, 34086, 490, 264, 3035, 300, 291, 393, 1401, 490, 264, 2135, 3144, 13, 51758], "temperature": 0.0, "avg_logprob": -0.20792546898427636, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.013129416853189468}, {"id": 179, "seek": 82220, "start": 822.2, "end": 827.48, "text": " The fine-tuned system actually refuses to give you kind of illegal information.", "tokens": [50364, 440, 2489, 12, 83, 43703, 1185, 767, 33222, 281, 976, 291, 733, 295, 11905, 1589, 13, 50628], "temperature": 0.0, "avg_logprob": -0.17548720653240496, "compression_ratio": 1.512, "no_speech_prob": 0.021494124084711075}, {"id": 180, "seek": 82220, "start": 827.48, "end": 829.6, "text": " You know, it's imperfect, but it works pretty well.", "tokens": [50628, 509, 458, 11, 309, 311, 26714, 11, 457, 309, 1985, 1238, 731, 13, 50734], "temperature": 0.0, "avg_logprob": -0.17548720653240496, "compression_ratio": 1.512, "no_speech_prob": 0.021494124084711075}, {"id": 181, "seek": 82220, "start": 829.6, "end": 834.88, "text": " It's got ways of detecting safety and helpfulness and toxicity and things like that.", "tokens": [50734, 467, 311, 658, 2098, 295, 40237, 4514, 293, 4961, 1287, 293, 45866, 293, 721, 411, 300, 13, 50998], "temperature": 0.0, "avg_logprob": -0.17548720653240496, "compression_ratio": 1.512, "no_speech_prob": 0.021494124084711075}, {"id": 182, "seek": 82220, "start": 834.88, "end": 835.88, "text": " Okay.", "tokens": [50998, 1033, 13, 51048], "temperature": 0.0, "avg_logprob": -0.17548720653240496, "compression_ratio": 1.512, "no_speech_prob": 0.021494124084711075}, {"id": 183, "seek": 82220, "start": 835.88, "end": 841.08, "text": " So this is all well and good, but autoregressive LLMs really suck.", "tokens": [51048, 407, 341, 307, 439, 731, 293, 665, 11, 457, 1476, 418, 3091, 488, 441, 43, 26386, 534, 9967, 13, 51308], "temperature": 0.0, "avg_logprob": -0.17548720653240496, "compression_ratio": 1.512, "no_speech_prob": 0.021494124084711075}, {"id": 184, "seek": 82220, "start": 841.08, "end": 848.2800000000001, "text": " For many of us in the AI research business, LLM, the LLM revolution took place two years", "tokens": [51308, 1171, 867, 295, 505, 294, 264, 7318, 2132, 1606, 11, 441, 43, 44, 11, 264, 441, 43, 44, 8894, 1890, 1081, 732, 924, 51668], "temperature": 0.0, "avg_logprob": -0.17548720653240496, "compression_ratio": 1.512, "no_speech_prob": 0.021494124084711075}, {"id": 185, "seek": 84828, "start": 848.28, "end": 852.9599999999999, "text": " ago and it's kind of old hats already.", "tokens": [50364, 2057, 293, 309, 311, 733, 295, 1331, 20549, 1217, 13, 50598], "temperature": 0.0, "avg_logprob": -0.2978573925090286, "compression_ratio": 1.66798418972332, "no_speech_prob": 0.047215741127729416}, {"id": 186, "seek": 84828, "start": 852.9599999999999, "end": 856.52, "text": " Not the case for the public who's been kind of, you know, coming in contact with sensitivity", "tokens": [50598, 1726, 264, 1389, 337, 264, 1908, 567, 311, 668, 733, 295, 11, 291, 458, 11, 1348, 294, 3385, 365, 19392, 50776], "temperature": 0.0, "avg_logprob": -0.2978573925090286, "compression_ratio": 1.66798418972332, "no_speech_prob": 0.047215741127729416}, {"id": 187, "seek": 84828, "start": 856.52, "end": 861.6, "text": " only in the last few months, but really they're not that great.", "tokens": [50776, 787, 294, 264, 1036, 1326, 2493, 11, 457, 534, 436, 434, 406, 300, 869, 13, 51030], "temperature": 0.0, "avg_logprob": -0.2978573925090286, "compression_ratio": 1.66798418972332, "no_speech_prob": 0.047215741127729416}, {"id": 188, "seek": 84828, "start": 861.6, "end": 867.8399999999999, "text": " They don't really produce factual consistent answer.", "tokens": [51030, 814, 500, 380, 534, 5258, 48029, 8398, 1867, 13, 51342], "temperature": 0.0, "avg_logprob": -0.2978573925090286, "compression_ratio": 1.66798418972332, "no_speech_prob": 0.047215741127729416}, {"id": 189, "seek": 84828, "start": 867.8399999999999, "end": 870.72, "text": " They hallucinate or they confibrate.", "tokens": [51342, 814, 35212, 13923, 420, 436, 1497, 6414, 473, 13, 51486], "temperature": 0.0, "avg_logprob": -0.2978573925090286, "compression_ratio": 1.66798418972332, "no_speech_prob": 0.047215741127729416}, {"id": 190, "seek": 84828, "start": 870.72, "end": 872.72, "text": " They can't take into account recent information.", "tokens": [51486, 814, 393, 380, 747, 666, 2696, 5162, 1589, 13, 51586], "temperature": 0.0, "avg_logprob": -0.2978573925090286, "compression_ratio": 1.66798418972332, "no_speech_prob": 0.047215741127729416}, {"id": 191, "seek": 84828, "start": 872.72, "end": 877.4399999999999, "text": " They're trained on information that is two years old or so, or whatever snapshot of the", "tokens": [51586, 814, 434, 8895, 322, 1589, 300, 307, 732, 924, 1331, 420, 370, 11, 420, 2035, 30163, 295, 264, 51822], "temperature": 0.0, "avg_logprob": -0.2978573925090286, "compression_ratio": 1.66798418972332, "no_speech_prob": 0.047215741127729416}, {"id": 192, "seek": 87744, "start": 877.5200000000001, "end": 881.0400000000001, "text": " crawl is used.", "tokens": [50368, 24767, 307, 1143, 13, 50544], "temperature": 0.0, "avg_logprob": -0.2775602429826683, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.04244710132479668}, {"id": 193, "seek": 87744, "start": 881.0400000000001, "end": 886.24, "text": " They're not really, it's not really possible to make them behave properly other than through", "tokens": [50544, 814, 434, 406, 534, 11, 309, 311, 406, 534, 1944, 281, 652, 552, 15158, 6108, 661, 813, 807, 50804], "temperature": 0.0, "avg_logprob": -0.2775602429826683, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.04244710132479668}, {"id": 194, "seek": 87744, "start": 886.24, "end": 888.08, "text": " this RLHF, which is really perfect.", "tokens": [50804, 341, 497, 43, 39, 37, 11, 597, 307, 534, 2176, 13, 50896], "temperature": 0.0, "avg_logprob": -0.2775602429826683, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.04244710132479668}, {"id": 195, "seek": 87744, "start": 888.08, "end": 892.2, "text": " So you can always jailbreak them by changing the prompt and sort of asking them to kind", "tokens": [50896, 407, 291, 393, 1009, 10511, 13225, 552, 538, 4473, 264, 12391, 293, 1333, 295, 3365, 552, 281, 733, 51102], "temperature": 0.0, "avg_logprob": -0.2775602429826683, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.04244710132479668}, {"id": 196, "seek": 87744, "start": 892.2, "end": 895.32, "text": " of act as if they were toxic.", "tokens": [51102, 295, 605, 382, 498, 436, 645, 12786, 13, 51258], "temperature": 0.0, "avg_logprob": -0.2775602429826683, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.04244710132479668}, {"id": 197, "seek": 87744, "start": 895.32, "end": 897.4000000000001, "text": " They don't reason.", "tokens": [51258, 814, 500, 380, 1778, 13, 51362], "temperature": 0.0, "avg_logprob": -0.2775602429826683, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.04244710132479668}, {"id": 198, "seek": 87744, "start": 897.4000000000001, "end": 898.4000000000001, "text": " They don't really plan.", "tokens": [51362, 814, 500, 380, 534, 1393, 13, 51412], "temperature": 0.0, "avg_logprob": -0.2775602429826683, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.04244710132479668}, {"id": 199, "seek": 87744, "start": 898.4000000000001, "end": 903.1600000000001, "text": " They can't do math unless you almond them with tools, which you can, of course.", "tokens": [51412, 814, 393, 380, 360, 5221, 5969, 291, 29506, 552, 365, 3873, 11, 597, 291, 393, 11, 295, 1164, 13, 51650], "temperature": 0.0, "avg_logprob": -0.2775602429826683, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.04244710132479668}, {"id": 200, "seek": 90316, "start": 903.1999999999999, "end": 906.36, "text": " And perhaps Stephen Wolfram will talk about this.", "tokens": [50366, 400, 4317, 13391, 16634, 2356, 486, 751, 466, 341, 13, 50524], "temperature": 0.0, "avg_logprob": -0.19611374670717896, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.000881704967468977}, {"id": 201, "seek": 90316, "start": 908.36, "end": 913.24, "text": " And you need to, there's a lot of work on sort of getting them to use tools such as search", "tokens": [50624, 400, 291, 643, 281, 11, 456, 311, 257, 688, 295, 589, 322, 1333, 295, 1242, 552, 281, 764, 3873, 1270, 382, 3164, 50868], "temperature": 0.0, "avg_logprob": -0.19611374670717896, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.000881704967468977}, {"id": 202, "seek": 90316, "start": 913.24, "end": 915.76, "text": " engine calculators, database queries, et cetera.", "tokens": [50868, 2848, 4322, 3391, 11, 8149, 24109, 11, 1030, 11458, 13, 50994], "temperature": 0.0, "avg_logprob": -0.19611374670717896, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.000881704967468977}, {"id": 203, "seek": 90316, "start": 915.76, "end": 917.9599999999999, "text": " Right now it's a bit of a hack the way this is done.", "tokens": [50994, 1779, 586, 309, 311, 257, 857, 295, 257, 10339, 264, 636, 341, 307, 1096, 13, 51104], "temperature": 0.0, "avg_logprob": -0.19611374670717896, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.000881704967468977}, {"id": 204, "seek": 90316, "start": 919.56, "end": 923.64, "text": " And the thing is we are really easily fooled by their fluency into thinking that they're", "tokens": [51184, 400, 264, 551, 307, 321, 366, 534, 3612, 33372, 538, 641, 5029, 3020, 666, 1953, 300, 436, 434, 51388], "temperature": 0.0, "avg_logprob": -0.19611374670717896, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.000881704967468977}, {"id": 205, "seek": 90316, "start": 923.64, "end": 928.04, "text": " intelligent, but their intelligence is very limited and really nothing like human intelligence.", "tokens": [51388, 13232, 11, 457, 641, 7599, 307, 588, 5567, 293, 534, 1825, 411, 1952, 7599, 13, 51608], "temperature": 0.0, "avg_logprob": -0.19611374670717896, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.000881704967468977}, {"id": 206, "seek": 90316, "start": 929.24, "end": 931.8399999999999, "text": " In particular, they really don't know how the world works.", "tokens": [51668, 682, 1729, 11, 436, 534, 500, 380, 458, 577, 264, 1002, 1985, 13, 51798], "temperature": 0.0, "avg_logprob": -0.19611374670717896, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.000881704967468977}, {"id": 207, "seek": 93184, "start": 931.88, "end": 936.52, "text": " They have no connection with the physical reality.", "tokens": [50366, 814, 362, 572, 4984, 365, 264, 4001, 4103, 13, 50598], "temperature": 0.0, "avg_logprob": -0.20215301080183548, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.000309742841636762}, {"id": 208, "seek": 93184, "start": 939.32, "end": 945.2, "text": " There's another reason why this, and it's basically by construction, which is that a system", "tokens": [50738, 821, 311, 1071, 1778, 983, 341, 11, 293, 309, 311, 1936, 538, 6435, 11, 597, 307, 300, 257, 1185, 51032], "temperature": 0.0, "avg_logprob": -0.20215301080183548, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.000309742841636762}, {"id": 209, "seek": 93184, "start": 945.2, "end": 951.9200000000001, "text": " that produces one token after the other, auto-regressively, is a divergent process.", "tokens": [51032, 300, 14725, 472, 14862, 934, 264, 661, 11, 8399, 12, 265, 3091, 3413, 11, 307, 257, 18558, 6930, 1399, 13, 51368], "temperature": 0.0, "avg_logprob": -0.20215301080183548, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.000309742841636762}, {"id": 210, "seek": 93184, "start": 951.9200000000001, "end": 954.4, "text": " It's a diffusion process with an exponential divergence.", "tokens": [51368, 467, 311, 257, 25242, 1399, 365, 364, 21510, 47387, 13, 51492], "temperature": 0.0, "avg_logprob": -0.20215301080183548, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.000309742841636762}, {"id": 211, "seek": 93184, "start": 955.12, "end": 960.84, "text": " If there is a probability at any token that is produced that the token takes you out of", "tokens": [51528, 759, 456, 307, 257, 8482, 412, 604, 14862, 300, 307, 7126, 300, 264, 14862, 2516, 291, 484, 295, 51814], "temperature": 0.0, "avg_logprob": -0.20215301080183548, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.000309742841636762}, {"id": 212, "seek": 96084, "start": 960.84, "end": 964.88, "text": " the set of correct answers, those probabilities accumulate.", "tokens": [50364, 264, 992, 295, 3006, 6338, 11, 729, 33783, 33384, 13, 50566], "temperature": 0.0, "avg_logprob": -0.20464423394972278, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.0005519940750673413}, {"id": 213, "seek": 96084, "start": 965.0400000000001, "end": 971.6800000000001, "text": " And the probability that a string of tokens of length n is correct is one minus this probability", "tokens": [50574, 400, 264, 8482, 300, 257, 6798, 295, 22667, 295, 4641, 297, 307, 3006, 307, 472, 3175, 341, 8482, 50906], "temperature": 0.0, "avg_logprob": -0.20464423394972278, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.0005519940750673413}, {"id": 214, "seek": 96084, "start": 971.6800000000001, "end": 972.84, "text": " of error to the power n.", "tokens": [50906, 295, 6713, 281, 264, 1347, 297, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20464423394972278, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.0005519940750673413}, {"id": 215, "seek": 96084, "start": 972.84, "end": 976.88, "text": " So the probability of correctness decreases exponentially with the length of the", "tokens": [50964, 407, 264, 8482, 295, 3006, 1287, 24108, 37330, 365, 264, 4641, 295, 264, 51166], "temperature": 0.0, "avg_logprob": -0.20464423394972278, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.0005519940750673413}, {"id": 216, "seek": 96084, "start": 978.5600000000001, "end": 980.8000000000001, "text": " of the sequence that is produced.", "tokens": [51250, 295, 264, 8310, 300, 307, 7126, 13, 51362], "temperature": 0.0, "avg_logprob": -0.20464423394972278, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.0005519940750673413}, {"id": 217, "seek": 96084, "start": 982.0, "end": 984.4, "text": " This is not fixable without major redesign.", "tokens": [51422, 639, 307, 406, 3191, 712, 1553, 2563, 39853, 13, 51542], "temperature": 0.0, "avg_logprob": -0.20464423394972278, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.0005519940750673413}, {"id": 218, "seek": 96084, "start": 985.44, "end": 989.36, "text": " It's really an essential flaw of auto-regressive prediction.", "tokens": [51594, 467, 311, 534, 364, 7115, 13717, 295, 8399, 12, 265, 3091, 488, 17630, 13, 51790], "temperature": 0.0, "avg_logprob": -0.20464423394972278, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.0005519940750673413}, {"id": 219, "seek": 99084, "start": 990.84, "end": 999.9200000000001, "text": " A while ago, with a colleague, Jacob Browning, we wrote a paper that essentially points out", "tokens": [50364, 316, 1339, 2057, 11, 365, 257, 13532, 11, 14117, 8030, 278, 11, 321, 4114, 257, 3035, 300, 4476, 2793, 484, 50818], "temperature": 0.0, "avg_logprob": -0.2363636589050293, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006228323327377439}, {"id": 220, "seek": 99084, "start": 999.9200000000001, "end": 1003.6, "text": " to the limit, it's not a technical paper, it's a philosophy paper, actually,", "tokens": [50818, 281, 264, 4948, 11, 309, 311, 406, 257, 6191, 3035, 11, 309, 311, 257, 10675, 3035, 11, 767, 11, 51002], "temperature": 0.0, "avg_logprob": -0.2363636589050293, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006228323327377439}, {"id": 221, "seek": 99084, "start": 1003.6, "end": 1005.84, "text": " the philosophy magazine called Noema.", "tokens": [51002, 264, 10675, 11332, 1219, 883, 5619, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2363636589050293, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006228323327377439}, {"id": 222, "seek": 99084, "start": 1006.5600000000001, "end": 1012.12, "text": " And it talks about the fact that most of human knowledge is non-linguistic.", "tokens": [51150, 400, 309, 6686, 466, 264, 1186, 300, 881, 295, 1952, 3601, 307, 2107, 12, 1688, 84, 3142, 13, 51428], "temperature": 0.0, "avg_logprob": -0.2363636589050293, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006228323327377439}, {"id": 223, "seek": 99084, "start": 1012.1600000000001, "end": 1017.2, "text": " Everything that we learned before the age of one, everything that any animal learns has", "tokens": [51430, 5471, 300, 321, 3264, 949, 264, 3205, 295, 472, 11, 1203, 300, 604, 5496, 27152, 575, 51682], "temperature": 0.0, "avg_logprob": -0.2363636589050293, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006228323327377439}, {"id": 224, "seek": 99084, "start": 1017.2, "end": 1018.24, "text": " nothing to do with language.", "tokens": [51682, 1825, 281, 360, 365, 2856, 13, 51734], "temperature": 0.0, "avg_logprob": -0.2363636589050293, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0006228323327377439}, {"id": 225, "seek": 101824, "start": 1018.48, "end": 1021.6, "text": " And it's an enormous amount of background knowledge about the world that we learned in", "tokens": [50376, 400, 309, 311, 364, 11322, 2372, 295, 3678, 3601, 466, 264, 1002, 300, 321, 3264, 294, 50532], "temperature": 0.0, "avg_logprob": -0.2095107853969681, "compression_ratio": 1.6770428015564203, "no_speech_prob": 0.0021688826382160187}, {"id": 226, "seek": 101824, "start": 1022.96, "end": 1026.0, "text": " in the first few months of life and that animals know.", "tokens": [50600, 294, 264, 700, 1326, 2493, 295, 993, 293, 300, 4882, 458, 13, 50752], "temperature": 0.0, "avg_logprob": -0.2095107853969681, "compression_ratio": 1.6770428015564203, "no_speech_prob": 0.0021688826382160187}, {"id": 227, "seek": 101824, "start": 1026.96, "end": 1028.28, "text": " None of this is linguistic at all.", "tokens": [50800, 14492, 295, 341, 307, 43002, 412, 439, 13, 50866], "temperature": 0.0, "avg_logprob": -0.2095107853969681, "compression_ratio": 1.6770428015564203, "no_speech_prob": 0.0021688826382160187}, {"id": 228, "seek": 101824, "start": 1028.36, "end": 1032.08, "text": " And LLMs don't have access to any of this kind of knowledge.", "tokens": [50870, 400, 441, 43, 26386, 500, 380, 362, 2105, 281, 604, 295, 341, 733, 295, 3601, 13, 51056], "temperature": 0.0, "avg_logprob": -0.2095107853969681, "compression_ratio": 1.6770428015564203, "no_speech_prob": 0.0021688826382160187}, {"id": 229, "seek": 101824, "start": 1033.4, "end": 1038.28, "text": " And so the thesis in that paper is that we're not going to be able to reach human level AI", "tokens": [51122, 400, 370, 264, 22288, 294, 300, 3035, 307, 300, 321, 434, 406, 516, 281, 312, 1075, 281, 2524, 1952, 1496, 7318, 51366], "temperature": 0.0, "avg_logprob": -0.2095107853969681, "compression_ratio": 1.6770428015564203, "no_speech_prob": 0.0021688826382160187}, {"id": 230, "seek": 101824, "start": 1038.28, "end": 1044.68, "text": " unless we have systems that have sort of direct sensor information in the form of vision, for example.", "tokens": [51366, 5969, 321, 362, 3652, 300, 362, 1333, 295, 2047, 10200, 1589, 294, 264, 1254, 295, 5201, 11, 337, 1365, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2095107853969681, "compression_ratio": 1.6770428015564203, "no_speech_prob": 0.0021688826382160187}, {"id": 231, "seek": 104468, "start": 1045.68, "end": 1048.1200000000001, "text": " You know, some way of understanding how the world works.", "tokens": [50414, 509, 458, 11, 512, 636, 295, 3701, 577, 264, 1002, 1985, 13, 50536], "temperature": 0.0, "avg_logprob": -0.19165252236758962, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0016419879393652081}, {"id": 232, "seek": 104468, "start": 1048.52, "end": 1053.44, "text": " Other papers that have appeared either from the cognitive science, in fact, that paper is from MIT,", "tokens": [50556, 5358, 10577, 300, 362, 8516, 2139, 490, 264, 15605, 3497, 11, 294, 1186, 11, 300, 3035, 307, 490, 13100, 11, 50802], "temperature": 0.0, "avg_logprob": -0.19165252236758962, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0016419879393652081}, {"id": 233, "seek": 104468, "start": 1054.0, "end": 1060.96, "text": " or from the classical AI kind of field, point to the fact that LLMs really cannot plan.", "tokens": [50830, 420, 490, 264, 13735, 7318, 733, 295, 2519, 11, 935, 281, 264, 1186, 300, 441, 43, 26386, 534, 2644, 1393, 13, 51178], "temperature": 0.0, "avg_logprob": -0.19165252236758962, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0016419879393652081}, {"id": 234, "seek": 104468, "start": 1061.2, "end": 1068.3200000000002, "text": " They don't have the ability to think really or reason in a way that we understand this from humans.", "tokens": [51190, 814, 500, 380, 362, 264, 3485, 281, 519, 534, 420, 1778, 294, 257, 636, 300, 321, 1223, 341, 490, 6255, 13, 51546], "temperature": 0.0, "avg_logprob": -0.19165252236758962, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0016419879393652081}, {"id": 235, "seek": 104468, "start": 1068.96, "end": 1073.68, "text": " And very limited abilities to plan, at least compared to other systems that are specifically", "tokens": [51578, 400, 588, 5567, 11582, 281, 1393, 11, 412, 1935, 5347, 281, 661, 3652, 300, 366, 4682, 51814], "temperature": 0.0, "avg_logprob": -0.19165252236758962, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.0016419879393652081}, {"id": 236, "seek": 107368, "start": 1073.68, "end": 1075.04, "text": " built for planning.", "tokens": [50364, 3094, 337, 5038, 13, 50432], "temperature": 0.0, "avg_logprob": -0.16518169536925198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007518738857470453}, {"id": 237, "seek": 107368, "start": 1076.64, "end": 1081.0, "text": " So I think there is three challenges in the future for AI and machine learning research.", "tokens": [50512, 407, 286, 519, 456, 307, 1045, 4759, 294, 264, 2027, 337, 7318, 293, 3479, 2539, 2132, 13, 50730], "temperature": 0.0, "avg_logprob": -0.16518169536925198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007518738857470453}, {"id": 238, "seek": 107368, "start": 1081.0, "end": 1086.92, "text": " And I've been showing this slide for several years now and I haven't changed it because of LLMs.", "tokens": [50730, 400, 286, 600, 668, 4099, 341, 4137, 337, 2940, 924, 586, 293, 286, 2378, 380, 3105, 309, 570, 295, 441, 43, 26386, 13, 51026], "temperature": 0.0, "avg_logprob": -0.16518169536925198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007518738857470453}, {"id": 239, "seek": 107368, "start": 1087.24, "end": 1092.52, "text": " The first one is learning representations and predictive models of the world, where the world can", "tokens": [51042, 440, 700, 472, 307, 2539, 33358, 293, 35521, 5245, 295, 264, 1002, 11, 689, 264, 1002, 393, 51306], "temperature": 0.0, "avg_logprob": -0.16518169536925198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007518738857470453}, {"id": 240, "seek": 107368, "start": 1092.52, "end": 1094.68, "text": " include other people that the system is talking to.", "tokens": [51306, 4090, 661, 561, 300, 264, 1185, 307, 1417, 281, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16518169536925198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007518738857470453}, {"id": 241, "seek": 107368, "start": 1095.6000000000001, "end": 1097.3200000000002, "text": " The solution to this is self-supervised learning.", "tokens": [51460, 440, 3827, 281, 341, 307, 2698, 12, 48172, 24420, 2539, 13, 51546], "temperature": 0.0, "avg_logprob": -0.16518169536925198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007518738857470453}, {"id": 242, "seek": 107368, "start": 1097.3200000000002, "end": 1098.72, "text": " We've known this for a number of years.", "tokens": [51546, 492, 600, 2570, 341, 337, 257, 1230, 295, 924, 13, 51616], "temperature": 0.0, "avg_logprob": -0.16518169536925198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007518738857470453}, {"id": 243, "seek": 107368, "start": 1100.2, "end": 1101.16, "text": " Learning to reason.", "tokens": [51690, 15205, 281, 1778, 13, 51738], "temperature": 0.0, "avg_logprob": -0.16518169536925198, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007518738857470453}, {"id": 244, "seek": 110116, "start": 1102.16, "end": 1108.24, "text": " So autoregressive LLMs are very much like what Daniel Kahneman calls system one, which basically", "tokens": [50414, 407, 1476, 418, 3091, 488, 441, 43, 26386, 366, 588, 709, 411, 437, 8033, 591, 12140, 15023, 5498, 1185, 472, 11, 597, 1936, 50718], "temperature": 0.0, "avg_logprob": -0.2609956979751587, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.000807517790235579}, {"id": 245, "seek": 110116, "start": 1108.24, "end": 1113.48, "text": " corresponds to subconscious tasks in humans, tasks that you accomplish without real planning", "tokens": [50718, 23249, 281, 27389, 9608, 294, 6255, 11, 9608, 300, 291, 9021, 1553, 957, 5038, 50980], "temperature": 0.0, "avg_logprob": -0.2609956979751587, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.000807517790235579}, {"id": 246, "seek": 110116, "start": 1113.48, "end": 1122.5600000000002, "text": " or reasoning that you sort of accomplish more or less reactively without thinking too much.", "tokens": [50980, 420, 21577, 300, 291, 1333, 295, 9021, 544, 420, 1570, 4515, 3413, 1553, 1953, 886, 709, 13, 51434], "temperature": 0.0, "avg_logprob": -0.2609956979751587, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.000807517790235579}, {"id": 247, "seek": 110116, "start": 1123.5600000000002, "end": 1131.0800000000002, "text": " System two is the type of action that you take by deliberate reasoning using the power", "tokens": [51484, 8910, 732, 307, 264, 2010, 295, 3069, 300, 291, 747, 538, 30515, 21577, 1228, 264, 1347, 51860], "temperature": 0.0, "avg_logprob": -0.2609956979751587, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.000807517790235579}, {"id": 248, "seek": 113108, "start": 1131.28, "end": 1137.8, "text": " of your prefrontal cortex, using your ability to predict and then planning sequence of actions", "tokens": [50374, 295, 428, 659, 11496, 304, 33312, 11, 1228, 428, 3485, 281, 6069, 293, 550, 5038, 8310, 295, 5909, 50700], "temperature": 0.0, "avg_logprob": -0.17859150966008505, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0008784690871834755}, {"id": 249, "seek": 113108, "start": 1137.8, "end": 1141.52, "text": " that will sort of satisfy a particular objective.", "tokens": [50700, 300, 486, 1333, 295, 19319, 257, 1729, 10024, 13, 50886], "temperature": 0.0, "avg_logprob": -0.17859150966008505, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0008784690871834755}, {"id": 250, "seek": 113108, "start": 1142.12, "end": 1144.28, "text": " And LLMs are not capable of this at the moment.", "tokens": [50916, 400, 441, 43, 26386, 366, 406, 8189, 295, 341, 412, 264, 1623, 13, 51024], "temperature": 0.0, "avg_logprob": -0.17859150966008505, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0008784690871834755}, {"id": 251, "seek": 113108, "start": 1147.8, "end": 1151.32, "text": " I'm going to argue for the fact that reasoning and planning should be viewed as some sort of energy", "tokens": [51200, 286, 478, 516, 281, 9695, 337, 264, 1186, 300, 21577, 293, 5038, 820, 312, 19174, 382, 512, 1333, 295, 2281, 51376], "temperature": 0.0, "avg_logprob": -0.17859150966008505, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0008784690871834755}, {"id": 252, "seek": 113108, "start": 1151.32, "end": 1152.08, "text": " minimization.", "tokens": [51376, 4464, 2144, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17859150966008505, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0008784690871834755}, {"id": 253, "seek": 113108, "start": 1153.08, "end": 1157.96, "text": " And then the last thing is learning to plan complex of actions to satisfy a number of objectives.", "tokens": [51464, 400, 550, 264, 1036, 551, 307, 2539, 281, 1393, 3997, 295, 5909, 281, 19319, 257, 1230, 295, 15961, 13, 51708], "temperature": 0.0, "avg_logprob": -0.17859150966008505, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.0008784690871834755}, {"id": 254, "seek": 115796, "start": 1158.8400000000001, "end": 1163.64, "text": " And that will require learning hierarchical representations of action plans, which machine", "tokens": [50408, 400, 300, 486, 3651, 2539, 35250, 804, 33358, 295, 3069, 5482, 11, 597, 3479, 50648], "temperature": 0.0, "avg_logprob": -0.22583581924438476, "compression_ratio": 1.5851851851851853, "no_speech_prob": 0.0009241813677363098}, {"id": 255, "seek": 115796, "start": 1163.64, "end": 1165.72, "text": " learning systems don't really know how to do at the moment.", "tokens": [50648, 2539, 3652, 500, 380, 534, 458, 577, 281, 360, 412, 264, 1623, 13, 50752], "temperature": 0.0, "avg_logprob": -0.22583581924438476, "compression_ratio": 1.5851851851851853, "no_speech_prob": 0.0009241813677363098}, {"id": 256, "seek": 115796, "start": 1166.52, "end": 1172.68, "text": " I've written this vision paper a while back called A Pass Towards Autonomous Machine Intelligence.", "tokens": [50792, 286, 600, 3720, 341, 5201, 3035, 257, 1339, 646, 1219, 316, 10319, 48938, 6049, 12481, 563, 22155, 27274, 13, 51100], "temperature": 0.0, "avg_logprob": -0.22583581924438476, "compression_ratio": 1.5851851851851853, "no_speech_prob": 0.0009241813677363098}, {"id": 257, "seek": 115796, "start": 1172.68, "end": 1174.1200000000001, "text": " I kind of changed the name of this now.", "tokens": [51100, 286, 733, 295, 3105, 264, 1315, 295, 341, 586, 13, 51172], "temperature": 0.0, "avg_logprob": -0.22583581924438476, "compression_ratio": 1.5851851851851853, "no_speech_prob": 0.0009241813677363098}, {"id": 258, "seek": 115796, "start": 1174.1200000000001, "end": 1178.6000000000001, "text": " I called it Objective Driven AI, but it really is the same concept.", "tokens": [51172, 286, 1219, 309, 24753, 488, 19150, 553, 7318, 11, 457, 309, 534, 307, 264, 912, 3410, 13, 51396], "temperature": 0.0, "avg_logprob": -0.22583581924438476, "compression_ratio": 1.5851851851851853, "no_speech_prob": 0.0009241813677363098}, {"id": 259, "seek": 115796, "start": 1179.88, "end": 1185.88, "text": " And it built around this idea of what's called cognitive architecture.", "tokens": [51460, 400, 309, 3094, 926, 341, 1558, 295, 437, 311, 1219, 15605, 9482, 13, 51760], "temperature": 0.0, "avg_logprob": -0.22583581924438476, "compression_ratio": 1.5851851851851853, "no_speech_prob": 0.0009241813677363098}, {"id": 260, "seek": 118588, "start": 1185.88, "end": 1189.4, "text": " So it's basically an architecture of different modules that interact with each other.", "tokens": [50364, 407, 309, 311, 1936, 364, 9482, 295, 819, 16679, 300, 4648, 365, 1184, 661, 13, 50540], "temperature": 0.0, "avg_logprob": -0.1194483470916748, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0008960498962551355}, {"id": 261, "seek": 118588, "start": 1190.8400000000001, "end": 1195.16, "text": " A perception module that basically gives the system an estimate of the state of the world", "tokens": [50612, 316, 12860, 10088, 300, 1936, 2709, 264, 1185, 364, 12539, 295, 264, 1785, 295, 264, 1002, 50828], "temperature": 0.0, "avg_logprob": -0.1194483470916748, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0008960498962551355}, {"id": 262, "seek": 118588, "start": 1195.16, "end": 1197.48, "text": " from perception that may be combined with memory.", "tokens": [50828, 490, 12860, 300, 815, 312, 9354, 365, 4675, 13, 50944], "temperature": 0.0, "avg_logprob": -0.1194483470916748, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0008960498962551355}, {"id": 263, "seek": 118588, "start": 1198.1200000000001, "end": 1202.92, "text": " A world model and the world model essentially is there to predict what's going to happen", "tokens": [50976, 316, 1002, 2316, 293, 264, 1002, 2316, 4476, 307, 456, 281, 6069, 437, 311, 516, 281, 1051, 51216], "temperature": 0.0, "avg_logprob": -0.1194483470916748, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0008960498962551355}, {"id": 264, "seek": 118588, "start": 1202.92, "end": 1206.7600000000002, "text": " in the world, perhaps as a result of actions that the agent might take,", "tokens": [51216, 294, 264, 1002, 11, 4317, 382, 257, 1874, 295, 5909, 300, 264, 9461, 1062, 747, 11, 51408], "temperature": 0.0, "avg_logprob": -0.1194483470916748, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0008960498962551355}, {"id": 265, "seek": 118588, "start": 1207.5600000000002, "end": 1210.92, "text": " actions that are being imagines by another module called the actor.", "tokens": [51448, 5909, 300, 366, 885, 2576, 1652, 538, 1071, 10088, 1219, 264, 8747, 13, 51616], "temperature": 0.0, "avg_logprob": -0.1194483470916748, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.0008960498962551355}, {"id": 266, "seek": 121092, "start": 1211.48, "end": 1214.28, "text": " So the actor feeds actions to the world model.", "tokens": [50392, 407, 264, 8747, 23712, 5909, 281, 264, 1002, 2316, 13, 50532], "temperature": 0.0, "avg_logprob": -0.0897419939758957, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.0005526303430087864}, {"id": 267, "seek": 121092, "start": 1214.28, "end": 1217.8000000000002, "text": " And the world model predicts the outcome of those actions.", "tokens": [50532, 400, 264, 1002, 2316, 6069, 82, 264, 9700, 295, 729, 5909, 13, 50708], "temperature": 0.0, "avg_logprob": -0.0897419939758957, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.0005526303430087864}, {"id": 268, "seek": 121092, "start": 1219.48, "end": 1226.3600000000001, "text": " And then this outcome is fed to a cost module that that cost module basically", "tokens": [50792, 400, 550, 341, 9700, 307, 4636, 281, 257, 2063, 10088, 300, 300, 2063, 10088, 1936, 51136], "temperature": 0.0, "avg_logprob": -0.0897419939758957, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.0005526303430087864}, {"id": 269, "seek": 121092, "start": 1226.3600000000001, "end": 1229.0800000000002, "text": " assesses whether the outcome is good or bad.", "tokens": [51136, 5877, 279, 1968, 264, 9700, 307, 665, 420, 1578, 13, 51272], "temperature": 0.0, "avg_logprob": -0.0897419939758957, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.0005526303430087864}, {"id": 270, "seek": 121092, "start": 1229.0800000000002, "end": 1231.16, "text": " So it measures the quality of the outcome.", "tokens": [51272, 407, 309, 8000, 264, 3125, 295, 264, 9700, 13, 51376], "temperature": 0.0, "avg_logprob": -0.0897419939758957, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.0005526303430087864}, {"id": 271, "seek": 121092, "start": 1232.28, "end": 1236.8400000000001, "text": " And the entire purpose of the agent is to figure out a sequence of actions", "tokens": [51432, 400, 264, 2302, 4334, 295, 264, 9461, 307, 281, 2573, 484, 257, 8310, 295, 5909, 51660], "temperature": 0.0, "avg_logprob": -0.0897419939758957, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.0005526303430087864}, {"id": 272, "seek": 121092, "start": 1236.8400000000001, "end": 1239.3200000000002, "text": " that minimizes that cost.", "tokens": [51660, 300, 4464, 5660, 300, 2063, 13, 51784], "temperature": 0.0, "avg_logprob": -0.0897419939758957, "compression_ratio": 1.8883248730964468, "no_speech_prob": 0.0005526303430087864}, {"id": 273, "seek": 123932, "start": 1239.32, "end": 1240.76, "text": " We're not talking about learning here.", "tokens": [50364, 492, 434, 406, 1417, 466, 2539, 510, 13, 50436], "temperature": 0.0, "avg_logprob": -0.12634609295771673, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0005698869354091585}, {"id": 274, "seek": 123932, "start": 1240.76, "end": 1242.4399999999998, "text": " We're talking about inference.", "tokens": [50436, 492, 434, 1417, 466, 38253, 13, 50520], "temperature": 0.0, "avg_logprob": -0.12634609295771673, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0005698869354091585}, {"id": 275, "seek": 123932, "start": 1242.4399999999998, "end": 1248.6799999999998, "text": " So this minimization of the cost with respect to the actions imagined by the actors", "tokens": [50520, 407, 341, 4464, 2144, 295, 264, 2063, 365, 3104, 281, 264, 5909, 16590, 538, 264, 10037, 50832], "temperature": 0.0, "avg_logprob": -0.12634609295771673, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0005698869354091585}, {"id": 276, "seek": 123932, "start": 1249.8, "end": 1252.76, "text": " using the world model is for inference.", "tokens": [50888, 1228, 264, 1002, 2316, 307, 337, 38253, 13, 51036], "temperature": 0.0, "avg_logprob": -0.12634609295771673, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0005698869354091585}, {"id": 277, "seek": 123932, "start": 1253.3999999999999, "end": 1256.9199999999998, "text": " Okay, so inference is not just forward propagation to a few layers of neural net.", "tokens": [51068, 1033, 11, 370, 38253, 307, 406, 445, 2128, 38377, 281, 257, 1326, 7914, 295, 18161, 2533, 13, 51244], "temperature": 0.0, "avg_logprob": -0.12634609295771673, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0005698869354091585}, {"id": 278, "seek": 123932, "start": 1256.9199999999998, "end": 1261.24, "text": " It's actually an optimization process, very much like what happens in business", "tokens": [51244, 467, 311, 767, 364, 19618, 1399, 11, 588, 709, 411, 437, 2314, 294, 1606, 51460], "temperature": 0.0, "avg_logprob": -0.12634609295771673, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0005698869354091585}, {"id": 279, "seek": 123932, "start": 1261.24, "end": 1263.1599999999999, "text": " nets and record models and stuff like that.", "tokens": [51460, 36170, 293, 2136, 5245, 293, 1507, 411, 300, 13, 51556], "temperature": 0.0, "avg_logprob": -0.12634609295771673, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0005698869354091585}, {"id": 280, "seek": 123932, "start": 1265.48, "end": 1268.2, "text": " And I can describe that in more details.", "tokens": [51672, 400, 286, 393, 6786, 300, 294, 544, 4365, 13, 51808], "temperature": 0.0, "avg_logprob": -0.12634609295771673, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0005698869354091585}, {"id": 281, "seek": 126932, "start": 1270.28, "end": 1276.52, "text": " So that's kind of a very simple different representations of this kind of architecture.", "tokens": [50412, 407, 300, 311, 733, 295, 257, 588, 2199, 819, 33358, 295, 341, 733, 295, 9482, 13, 50724], "temperature": 0.0, "avg_logprob": -0.15315799055428342, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.00019398514996282756}, {"id": 282, "seek": 126932, "start": 1277.3999999999999, "end": 1283.1599999999999, "text": " Perceives the world ready to a perception module that computes an abstract representation", "tokens": [50768, 3026, 384, 1539, 264, 1002, 1919, 281, 257, 12860, 10088, 300, 715, 1819, 364, 12649, 10290, 51056], "temperature": 0.0, "avg_logprob": -0.15315799055428342, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.00019398514996282756}, {"id": 283, "seek": 126932, "start": 1283.1599999999999, "end": 1288.52, "text": " of the state of the world, perhaps combined with content of a memory that has some other", "tokens": [51056, 295, 264, 1785, 295, 264, 1002, 11, 4317, 9354, 365, 2701, 295, 257, 4675, 300, 575, 512, 661, 51324], "temperature": 0.0, "avg_logprob": -0.15315799055428342, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.00019398514996282756}, {"id": 284, "seek": 126932, "start": 1288.52, "end": 1289.72, "text": " idea with the state of the world.", "tokens": [51324, 1558, 365, 264, 1785, 295, 264, 1002, 13, 51384], "temperature": 0.0, "avg_logprob": -0.15315799055428342, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.00019398514996282756}, {"id": 285, "seek": 126932, "start": 1291.0, "end": 1295.8799999999999, "text": " Initialize your world model with that and then feed the world model with that initial", "tokens": [51448, 22937, 831, 1125, 428, 1002, 2316, 365, 300, 293, 550, 3154, 264, 1002, 2316, 365, 300, 5883, 51692], "temperature": 0.0, "avg_logprob": -0.15315799055428342, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.00019398514996282756}, {"id": 286, "seek": 129588, "start": 1295.88, "end": 1302.0400000000002, "text": " configuration combined with an imagined action sequence imagined by the actor.", "tokens": [50364, 11694, 9354, 365, 364, 16590, 3069, 8310, 16590, 538, 264, 8747, 13, 50672], "temperature": 0.0, "avg_logprob": -0.12597040903000606, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0005190653027966619}, {"id": 287, "seek": 129588, "start": 1303.5600000000002, "end": 1307.64, "text": " And then feed the results to a number of objective functions.", "tokens": [50748, 400, 550, 3154, 264, 3542, 281, 257, 1230, 295, 10024, 6828, 13, 50952], "temperature": 0.0, "avg_logprob": -0.12597040903000606, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0005190653027966619}, {"id": 288, "seek": 129588, "start": 1307.64, "end": 1311.64, "text": " A set of objectives that you can think of as guardrails that are hardwired.", "tokens": [50952, 316, 992, 295, 15961, 300, 291, 393, 519, 295, 382, 6290, 424, 4174, 300, 366, 1152, 86, 1824, 13, 51152], "temperature": 0.0, "avg_logprob": -0.12597040903000606, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0005190653027966619}, {"id": 289, "seek": 129588, "start": 1312.8400000000001, "end": 1319.16, "text": " And other objectives that measure whether the task was satisfied was fulfilled.", "tokens": [51212, 400, 661, 15961, 300, 3481, 1968, 264, 5633, 390, 11239, 390, 21380, 13, 51528], "temperature": 0.0, "avg_logprob": -0.12597040903000606, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0005190653027966619}, {"id": 290, "seek": 129588, "start": 1320.6000000000001, "end": 1324.6000000000001, "text": " And the entire purpose of the system is to figure out a sequence of actions that", "tokens": [51600, 400, 264, 2302, 4334, 295, 264, 1185, 307, 281, 2573, 484, 257, 8310, 295, 5909, 300, 51800], "temperature": 0.0, "avg_logprob": -0.12597040903000606, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0005190653027966619}, {"id": 291, "seek": 132460, "start": 1324.6, "end": 1326.76, "text": " will minimize those costs at inference time.", "tokens": [50364, 486, 17522, 729, 5497, 412, 38253, 565, 13, 50472], "temperature": 0.0, "avg_logprob": -0.11674682072230748, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.00029584395815618336}, {"id": 292, "seek": 132460, "start": 1327.48, "end": 1333.08, "text": " Okay, so it cannot do anything, but I put action sequences that minimize those costs", "tokens": [50508, 1033, 11, 370, 309, 2644, 360, 1340, 11, 457, 286, 829, 3069, 22978, 300, 17522, 729, 5497, 50788], "temperature": 0.0, "avg_logprob": -0.11674682072230748, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.00029584395815618336}, {"id": 293, "seek": 132460, "start": 1333.08, "end": 1336.52, "text": " according to the prediction that it's making from its world model.", "tokens": [50788, 4650, 281, 264, 17630, 300, 309, 311, 1455, 490, 1080, 1002, 2316, 13, 50960], "temperature": 0.0, "avg_logprob": -0.11674682072230748, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.00029584395815618336}, {"id": 294, "seek": 132460, "start": 1337.7199999999998, "end": 1339.8, "text": " So that's why I call this objective driven.", "tokens": [51020, 407, 300, 311, 983, 286, 818, 341, 10024, 9555, 13, 51124], "temperature": 0.0, "avg_logprob": -0.11674682072230748, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.00029584395815618336}, {"id": 295, "seek": 132460, "start": 1341.0, "end": 1346.04, "text": " There's no way you can job rate that system because it's hardwired to optimize those objectives.", "tokens": [51184, 821, 311, 572, 636, 291, 393, 1691, 3314, 300, 1185, 570, 309, 311, 1152, 86, 1824, 281, 19719, 729, 15961, 13, 51436], "temperature": 0.0, "avg_logprob": -0.11674682072230748, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.00029584395815618336}, {"id": 296, "seek": 132460, "start": 1346.04, "end": 1350.36, "text": " So unless you modify the objectives, the guardrails in particular, you're not going to be able to", "tokens": [51436, 407, 5969, 291, 16927, 264, 15961, 11, 264, 6290, 424, 4174, 294, 1729, 11, 291, 434, 406, 516, 281, 312, 1075, 281, 51652], "temperature": 0.0, "avg_logprob": -0.11674682072230748, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.00029584395815618336}, {"id": 297, "seek": 135036, "start": 1351.08, "end": 1357.32, "text": " have it produce toxic content, for example, if the guardrail objective includes something", "tokens": [50400, 362, 309, 5258, 12786, 2701, 11, 337, 1365, 11, 498, 264, 6290, 44765, 10024, 5974, 746, 50712], "temperature": 0.0, "avg_logprob": -0.11535606761970142, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0008161062141880393}, {"id": 298, "seek": 135036, "start": 1357.32, "end": 1365.56, "text": " like measuring toxicity. The world model very likely will need to be some sort of recurrent", "tokens": [50712, 411, 13389, 45866, 13, 440, 1002, 2316, 588, 3700, 486, 643, 281, 312, 512, 1333, 295, 18680, 1753, 51124], "temperature": 0.0, "avg_logprob": -0.11535606761970142, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0008161062141880393}, {"id": 299, "seek": 135036, "start": 1365.56, "end": 1370.1999999999998, "text": " model that might be multiple steps to the action. So you take, for example, two actions", "tokens": [51124, 2316, 300, 1062, 312, 3866, 4439, 281, 264, 3069, 13, 407, 291, 747, 11, 337, 1365, 11, 732, 5909, 51356], "temperature": 0.0, "avg_logprob": -0.11535606761970142, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0008161062141880393}, {"id": 300, "seek": 135036, "start": 1370.1999999999998, "end": 1373.56, "text": " and you run them through your world model twice so that you can predict in two steps", "tokens": [51356, 293, 291, 1190, 552, 807, 428, 1002, 2316, 6091, 370, 300, 291, 393, 6069, 294, 732, 4439, 51524], "temperature": 0.0, "avg_logprob": -0.11535606761970142, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0008161062141880393}, {"id": 301, "seek": 135036, "start": 1373.56, "end": 1377.1599999999999, "text": " what's going to happen. And the guardrail cost can be applied at every time step.", "tokens": [51524, 437, 311, 516, 281, 1051, 13, 400, 264, 6290, 44765, 2063, 393, 312, 6456, 412, 633, 565, 1823, 13, 51704], "temperature": 0.0, "avg_logprob": -0.11535606761970142, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0008161062141880393}, {"id": 302, "seek": 137716, "start": 1377.88, "end": 1381.64, "text": " Of course, the world is not deterministic. So the world model really,", "tokens": [50400, 2720, 1164, 11, 264, 1002, 307, 406, 15957, 3142, 13, 407, 264, 1002, 2316, 534, 11, 50588], "temperature": 0.0, "avg_logprob": -0.11636095648413305, "compression_ratio": 1.8021978021978022, "no_speech_prob": 0.00022660546528641135}, {"id": 303, "seek": 137716, "start": 1382.52, "end": 1386.44, "text": " if it's a deterministic function, needs to be fed latent variables so that there might be", "tokens": [50632, 498, 309, 311, 257, 15957, 3142, 2445, 11, 2203, 281, 312, 4636, 48994, 9102, 370, 300, 456, 1062, 312, 50828], "temperature": 0.0, "avg_logprob": -0.11636095648413305, "compression_ratio": 1.8021978021978022, "no_speech_prob": 0.00022660546528641135}, {"id": 304, "seek": 137716, "start": 1386.44, "end": 1389.24, "text": " multiple predictions for a single action in a single initial state.", "tokens": [50828, 3866, 21264, 337, 257, 2167, 3069, 294, 257, 2167, 5883, 1785, 13, 50968], "temperature": 0.0, "avg_logprob": -0.11636095648413305, "compression_ratio": 1.8021978021978022, "no_speech_prob": 0.00022660546528641135}, {"id": 305, "seek": 137716, "start": 1390.2, "end": 1394.6000000000001, "text": " And when you make the latent variable vary over a set or you sample them from a distribution,", "tokens": [51016, 400, 562, 291, 652, 264, 48994, 7006, 10559, 670, 257, 992, 420, 291, 6889, 552, 490, 257, 7316, 11, 51236], "temperature": 0.0, "avg_logprob": -0.11636095648413305, "compression_ratio": 1.8021978021978022, "no_speech_prob": 0.00022660546528641135}, {"id": 306, "seek": 137716, "start": 1394.6000000000001, "end": 1401.0800000000002, "text": " you get multiple predictions. That complicates the planning process, of course, but the process", "tokens": [51236, 291, 483, 3866, 21264, 13, 663, 16060, 1024, 264, 5038, 1399, 11, 295, 1164, 11, 457, 264, 1399, 51560], "temperature": 0.0, "avg_logprob": -0.11636095648413305, "compression_ratio": 1.8021978021978022, "no_speech_prob": 0.00022660546528641135}, {"id": 307, "seek": 137716, "start": 1401.0800000000002, "end": 1404.6000000000001, "text": " by which you figure out a sequence of actions that minimize the objectives", "tokens": [51560, 538, 597, 291, 2573, 484, 257, 8310, 295, 5909, 300, 17522, 264, 15961, 51736], "temperature": 0.0, "avg_logprob": -0.11636095648413305, "compression_ratio": 1.8021978021978022, "no_speech_prob": 0.00022660546528641135}, {"id": 308, "seek": 140460, "start": 1404.6, "end": 1411.0, "text": " is a planning and reasoning procedure. Ultimately, what we really want is some", "tokens": [50364, 307, 257, 5038, 293, 21577, 10747, 13, 23921, 11, 437, 321, 534, 528, 307, 512, 50684], "temperature": 0.0, "avg_logprob": -0.09422123816705519, "compression_ratio": 1.5844748858447488, "no_speech_prob": 0.002312366385012865}, {"id": 309, "seek": 140460, "start": 1411.0, "end": 1417.6399999999999, "text": " sort of ways of doing this hierarchically. And I'm going to explain this with an example.", "tokens": [50684, 1333, 295, 2098, 295, 884, 341, 35250, 984, 13, 400, 286, 478, 516, 281, 2903, 341, 365, 364, 1365, 13, 51016], "temperature": 0.0, "avg_logprob": -0.09422123816705519, "compression_ratio": 1.5844748858447488, "no_speech_prob": 0.002312366385012865}, {"id": 310, "seek": 140460, "start": 1418.36, "end": 1424.36, "text": " So here is an example. Let's say I'm sitting in my office in New York at NYU and I want to", "tokens": [51052, 407, 510, 307, 364, 1365, 13, 961, 311, 584, 286, 478, 3798, 294, 452, 3398, 294, 1873, 3609, 412, 42682, 293, 286, 528, 281, 51352], "temperature": 0.0, "avg_logprob": -0.09422123816705519, "compression_ratio": 1.5844748858447488, "no_speech_prob": 0.002312366385012865}, {"id": 311, "seek": 140460, "start": 1424.9199999999998, "end": 1431.8, "text": " fly to Paris. I want to go to Paris. So the first action I have to do is take a taxi or", "tokens": [51380, 3603, 281, 8380, 13, 286, 528, 281, 352, 281, 8380, 13, 407, 264, 700, 3069, 286, 362, 281, 360, 307, 747, 257, 18984, 420, 51724], "temperature": 0.0, "avg_logprob": -0.09422123816705519, "compression_ratio": 1.5844748858447488, "no_speech_prob": 0.002312366385012865}, {"id": 312, "seek": 143180, "start": 1431.8, "end": 1438.04, "text": " the train to the airport, either Newark or JFK. And then the second step is I need to catch a", "tokens": [50364, 264, 3847, 281, 264, 10155, 11, 2139, 1873, 809, 420, 40951, 42, 13, 400, 550, 264, 1150, 1823, 307, 286, 643, 281, 3745, 257, 50676], "temperature": 0.0, "avg_logprob": -0.1362977302991427, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.010302687995135784}, {"id": 313, "seek": 143180, "start": 1438.04, "end": 1443.32, "text": " plane to Paris. Okay, but I have a first goal, which is to get to the airport. Now that goal", "tokens": [50676, 5720, 281, 8380, 13, 1033, 11, 457, 286, 362, 257, 700, 3387, 11, 597, 307, 281, 483, 281, 264, 10155, 13, 823, 300, 3387, 50940], "temperature": 0.0, "avg_logprob": -0.1362977302991427, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.010302687995135784}, {"id": 314, "seek": 143180, "start": 1443.32, "end": 1448.28, "text": " can be decomposed into two sub-goals. The first one is I need to go down in the street", "tokens": [50940, 393, 312, 22867, 1744, 666, 732, 1422, 12, 1571, 1124, 13, 440, 700, 472, 307, 286, 643, 281, 352, 760, 294, 264, 4838, 51188], "temperature": 0.0, "avg_logprob": -0.1362977302991427, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.010302687995135784}, {"id": 315, "seek": 143180, "start": 1448.84, "end": 1455.8799999999999, "text": " and tell the taxi to take me to the airport. How do I go down in the street? I need to", "tokens": [51216, 293, 980, 264, 18984, 281, 747, 385, 281, 264, 10155, 13, 1012, 360, 286, 352, 760, 294, 264, 4838, 30, 286, 643, 281, 51568], "temperature": 0.0, "avg_logprob": -0.1362977302991427, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.010302687995135784}, {"id": 316, "seek": 145588, "start": 1455.88, "end": 1464.2, "text": " stand up for my chair, get out of the building and take the elevator or the stairs go down. How", "tokens": [50364, 1463, 493, 337, 452, 6090, 11, 483, 484, 295, 264, 2390, 293, 747, 264, 18782, 420, 264, 13471, 352, 760, 13, 1012, 50780], "temperature": 0.0, "avg_logprob": -0.09188715342817635, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.028786607086658478}, {"id": 317, "seek": 145588, "start": 1464.2, "end": 1469.0800000000002, "text": " do I get out from my chair? I need to activate muscles in a particular order all the way down", "tokens": [50780, 360, 286, 483, 484, 490, 452, 6090, 30, 286, 643, 281, 13615, 9530, 294, 257, 1729, 1668, 439, 264, 636, 760, 51024], "temperature": 0.0, "avg_logprob": -0.09188715342817635, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.028786607086658478}, {"id": 318, "seek": 145588, "start": 1469.0800000000002, "end": 1473.48, "text": " to millisecond by millisecond muscle control. We do this kind of hierarchical planning all the", "tokens": [51024, 281, 27940, 18882, 538, 27940, 18882, 8679, 1969, 13, 492, 360, 341, 733, 295, 35250, 804, 5038, 439, 264, 51244], "temperature": 0.0, "avg_logprob": -0.09188715342817635, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.028786607086658478}, {"id": 319, "seek": 145588, "start": 1473.48, "end": 1477.16, "text": " time without even thinking about it, even though it's actually a very conscious task that we're", "tokens": [51244, 565, 1553, 754, 1953, 466, 309, 11, 754, 1673, 309, 311, 767, 257, 588, 6648, 5633, 300, 321, 434, 51428], "temperature": 0.0, "avg_logprob": -0.09188715342817635, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.028786607086658478}, {"id": 320, "seek": 145588, "start": 1477.16, "end": 1485.3200000000002, "text": " doing. Animals do this also. You can watch, I don't know, cats planning trajectories to kind of", "tokens": [51428, 884, 13, 47164, 360, 341, 611, 13, 509, 393, 1159, 11, 286, 500, 380, 458, 11, 11111, 5038, 18257, 2083, 281, 733, 295, 51836], "temperature": 0.0, "avg_logprob": -0.09188715342817635, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.028786607086658478}, {"id": 321, "seek": 148532, "start": 1485.32, "end": 1489.1599999999999, "text": " jump on a piece of furniture. They're doing this kind of planning hierarchically.", "tokens": [50364, 3012, 322, 257, 2522, 295, 15671, 13, 814, 434, 884, 341, 733, 295, 5038, 35250, 984, 13, 50556], "temperature": 0.0, "avg_logprob": -0.07955770309154804, "compression_ratio": 1.76, "no_speech_prob": 0.0013628201559185982}, {"id": 322, "seek": 148532, "start": 1490.2, "end": 1496.4399999999998, "text": " We don't have any system, AI systems today, that can learn how to do this spontaneously.", "tokens": [50608, 492, 500, 380, 362, 604, 1185, 11, 7318, 3652, 965, 11, 300, 393, 1466, 577, 281, 360, 341, 47632, 13, 50920], "temperature": 0.0, "avg_logprob": -0.07955770309154804, "compression_ratio": 1.76, "no_speech_prob": 0.0013628201559185982}, {"id": 323, "seek": 148532, "start": 1496.4399999999998, "end": 1500.2, "text": " There are systems that do hierarchical planning, but they're hardwired. They're built by hand.", "tokens": [50920, 821, 366, 3652, 300, 360, 35250, 804, 5038, 11, 457, 436, 434, 1152, 86, 1824, 13, 814, 434, 3094, 538, 1011, 13, 51108], "temperature": 0.0, "avg_logprob": -0.07955770309154804, "compression_ratio": 1.76, "no_speech_prob": 0.0013628201559185982}, {"id": 324, "seek": 148532, "start": 1501.08, "end": 1505.08, "text": " What we need is a system that can learn the various levels of representations", "tokens": [51152, 708, 321, 643, 307, 257, 1185, 300, 393, 1466, 264, 3683, 4358, 295, 33358, 51352], "temperature": 0.0, "avg_logprob": -0.07955770309154804, "compression_ratio": 1.76, "no_speech_prob": 0.0013628201559185982}, {"id": 325, "seek": 148532, "start": 1505.08, "end": 1510.52, "text": " of the state of the world that will allow them to do this kind of decomposition of complex tasks", "tokens": [51352, 295, 264, 1785, 295, 264, 1002, 300, 486, 2089, 552, 281, 360, 341, 733, 295, 48356, 295, 3997, 9608, 51624], "temperature": 0.0, "avg_logprob": -0.07955770309154804, "compression_ratio": 1.76, "no_speech_prob": 0.0013628201559185982}, {"id": 326, "seek": 151052, "start": 1510.52, "end": 1518.36, "text": " into a hierarchy of simpler ones. Again, we don't have any system that can do this today at all.", "tokens": [50364, 666, 257, 22333, 295, 18587, 2306, 13, 3764, 11, 321, 500, 380, 362, 604, 1185, 300, 393, 360, 341, 965, 412, 439, 13, 50756], "temperature": 0.0, "avg_logprob": -0.10033369493914081, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0008026704308576882}, {"id": 327, "seek": 151052, "start": 1518.36, "end": 1526.12, "text": " This is a big challenge, I think, for the future of AI research. That's the main idea of", "tokens": [50756, 639, 307, 257, 955, 3430, 11, 286, 519, 11, 337, 264, 2027, 295, 7318, 2132, 13, 663, 311, 264, 2135, 1558, 295, 51144], "temperature": 0.0, "avg_logprob": -0.10033369493914081, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0008026704308576882}, {"id": 328, "seek": 151052, "start": 1526.12, "end": 1530.12, "text": " objective-driven AI. How can we build systems like this that can do hierarchical planning?", "tokens": [51144, 10024, 12, 25456, 7318, 13, 1012, 393, 321, 1322, 3652, 411, 341, 300, 393, 360, 35250, 804, 5038, 30, 51344], "temperature": 0.0, "avg_logprob": -0.10033369493914081, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0008026704308576882}, {"id": 329, "seek": 151052, "start": 1530.12, "end": 1534.68, "text": " They can learn models of the world that predict what's going to happen in the short term with", "tokens": [51344, 814, 393, 1466, 5245, 295, 264, 1002, 300, 6069, 437, 311, 516, 281, 1051, 294, 264, 2099, 1433, 365, 51572], "temperature": 0.0, "avg_logprob": -0.10033369493914081, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0008026704308576882}, {"id": 330, "seek": 151052, "start": 1534.68, "end": 1540.2, "text": " high precision or in the long term with less precision in more abstract levels of representation.", "tokens": [51572, 1090, 18356, 420, 294, 264, 938, 1433, 365, 1570, 18356, 294, 544, 12649, 4358, 295, 10290, 13, 51848], "temperature": 0.0, "avg_logprob": -0.10033369493914081, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0008026704308576882}, {"id": 331, "seek": 154052, "start": 1541.0, "end": 1544.2, "text": " This is where I think AI research would go over the next 10 years or so,", "tokens": [50388, 639, 307, 689, 286, 519, 7318, 2132, 576, 352, 670, 264, 958, 1266, 924, 420, 370, 11, 50548], "temperature": 0.0, "avg_logprob": -0.11286410638841532, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.0003295005299150944}, {"id": 332, "seek": 154052, "start": 1545.08, "end": 1551.16, "text": " and this is how LLM should be built. In fact, that may be how LLM may be built in the future,", "tokens": [50592, 293, 341, 307, 577, 441, 43, 44, 820, 312, 3094, 13, 682, 1186, 11, 300, 815, 312, 577, 441, 43, 44, 815, 312, 3094, 294, 264, 2027, 11, 50896], "temperature": 0.0, "avg_logprob": -0.11286410638841532, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.0003295005299150944}, {"id": 333, "seek": 154052, "start": 1551.16, "end": 1555.8799999999999, "text": " and in fact, I have a prediction which is that the type of autoreversive LLMs that we see today", "tokens": [50896, 293, 294, 1186, 11, 286, 362, 257, 17630, 597, 307, 300, 264, 2010, 295, 1476, 418, 840, 488, 441, 43, 26386, 300, 321, 536, 965, 51132], "temperature": 0.0, "avg_logprob": -0.11286410638841532, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.0003295005299150944}, {"id": 334, "seek": 154052, "start": 1555.8799999999999, "end": 1560.92, "text": " will disappear within three to five years because they are not able to plan their answers.", "tokens": [51132, 486, 11596, 1951, 1045, 281, 1732, 924, 570, 436, 366, 406, 1075, 281, 1393, 641, 6338, 13, 51384], "temperature": 0.0, "avg_logprob": -0.11286410638841532, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.0003295005299150944}, {"id": 335, "seek": 154052, "start": 1561.8, "end": 1566.44, "text": " If we had a system that was able to take a query and then in some sort of abstract", "tokens": [51428, 759, 321, 632, 257, 1185, 300, 390, 1075, 281, 747, 257, 14581, 293, 550, 294, 512, 1333, 295, 12649, 51660], "temperature": 0.0, "avg_logprob": -0.11286410638841532, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.0003295005299150944}, {"id": 336, "seek": 156644, "start": 1566.44, "end": 1571.72, "text": " representation space was able to plan its answer, plan a representation of its answer,", "tokens": [50364, 10290, 1901, 390, 1075, 281, 1393, 1080, 1867, 11, 1393, 257, 10290, 295, 1080, 1867, 11, 50628], "temperature": 0.0, "avg_logprob": -0.22210253020863474, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.0029740585014224052}, {"id": 337, "seek": 156644, "start": 1572.52, "end": 1577.0, "text": " and then translate this representation of the answer into fluent text using an autoregressive", "tokens": [50668, 293, 550, 13799, 341, 10290, 295, 264, 1867, 666, 40799, 2487, 1228, 364, 1476, 418, 3091, 488, 50892], "temperature": 0.0, "avg_logprob": -0.22210253020863474, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.0029740585014224052}, {"id": 338, "seek": 156644, "start": 1577.0, "end": 1583.0, "text": " decoder, for example, then we would have something that could actually be factual and", "tokens": [50892, 979, 19866, 11, 337, 1365, 11, 550, 321, 576, 362, 746, 300, 727, 767, 312, 48029, 293, 51192], "temperature": 0.0, "avg_logprob": -0.22210253020863474, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.0029740585014224052}, {"id": 339, "seek": 156644, "start": 1583.88, "end": 1592.92, "text": " simultaneously with being fluent and be non-toxic and be easily germ-broken and be steerable.", "tokens": [51236, 16561, 365, 885, 40799, 293, 312, 2107, 12, 1353, 47228, 293, 312, 3612, 19858, 12, 37947, 293, 312, 30814, 712, 13, 51688], "temperature": 0.0, "avg_logprob": -0.22210253020863474, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.0029740585014224052}, {"id": 340, "seek": 159292, "start": 1593.48, "end": 1599.96, "text": " That's my idea for where things are going. Building this and making it work is not going", "tokens": [50392, 663, 311, 452, 1558, 337, 689, 721, 366, 516, 13, 18974, 341, 293, 1455, 309, 589, 307, 406, 516, 50716], "temperature": 0.0, "avg_logprob": -0.1380134381745991, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0014034295454621315}, {"id": 341, "seek": 159292, "start": 1599.96, "end": 1606.44, "text": " to be easy and may fail, but I think that's where we should go. If we have systems like this,", "tokens": [50716, 281, 312, 1858, 293, 815, 3061, 11, 457, 286, 519, 300, 311, 689, 321, 820, 352, 13, 759, 321, 362, 3652, 411, 341, 11, 51040], "temperature": 0.0, "avg_logprob": -0.1380134381745991, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0014034295454621315}, {"id": 342, "seek": 159292, "start": 1606.44, "end": 1614.44, "text": " we won't need any kind of RLHF or human feedback other than the type of systems that are required", "tokens": [51040, 321, 1582, 380, 643, 604, 733, 295, 497, 43, 39, 37, 420, 1952, 5824, 661, 813, 264, 2010, 295, 3652, 300, 366, 4739, 51440], "temperature": 0.0, "avg_logprob": -0.1380134381745991, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0014034295454621315}, {"id": 343, "seek": 159292, "start": 1614.44, "end": 1620.92, "text": " to train cost modules to measure things like toxicity, for example, but we won't need to", "tokens": [51440, 281, 3847, 2063, 16679, 281, 3481, 721, 411, 45866, 11, 337, 1365, 11, 457, 321, 1582, 380, 643, 281, 51764], "temperature": 0.0, "avg_logprob": -0.1380134381745991, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0014034295454621315}, {"id": 344, "seek": 162092, "start": 1621.0, "end": 1627.48, "text": " fine-tune the system globally to be safe. We just need to put an objective so that all of the", "tokens": [50368, 2489, 12, 83, 2613, 264, 1185, 18958, 281, 312, 3273, 13, 492, 445, 643, 281, 829, 364, 10024, 370, 300, 439, 295, 264, 50692], "temperature": 0.0, "avg_logprob": -0.12752005818125967, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0008159004501067102}, {"id": 345, "seek": 162092, "start": 1627.48, "end": 1633.48, "text": " outputs that it produces are safe, but we don't need to retrain the entire encoders and everything", "tokens": [50692, 23930, 300, 309, 14725, 366, 3273, 11, 457, 321, 500, 380, 643, 281, 1533, 7146, 264, 2302, 2058, 378, 433, 293, 1203, 50992], "temperature": 0.0, "avg_logprob": -0.12752005818125967, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0008159004501067102}, {"id": 346, "seek": 162092, "start": 1633.48, "end": 1641.3200000000002, "text": " for that. I think it would simplify training quite a bit, actually. Let me skip this.", "tokens": [50992, 337, 300, 13, 286, 519, 309, 576, 20460, 3097, 1596, 257, 857, 11, 767, 13, 961, 385, 10023, 341, 13, 51384], "temperature": 0.0, "avg_logprob": -0.12752005818125967, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0008159004501067102}, {"id": 347, "seek": 162092, "start": 1644.3600000000001, "end": 1646.8400000000001, "text": " We come to the question of how do we build and train this word model?", "tokens": [51536, 492, 808, 281, 264, 1168, 295, 577, 360, 321, 1322, 293, 3847, 341, 1349, 2316, 30, 51660], "temperature": 0.0, "avg_logprob": -0.12752005818125967, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0008159004501067102}, {"id": 348, "seek": 164684, "start": 1646.84, "end": 1654.12, "text": " When we look at babies, babies learn in the first few months of life an enormous amount of", "tokens": [50364, 1133, 321, 574, 412, 10917, 11, 10917, 1466, 294, 264, 700, 1326, 2493, 295, 993, 364, 11322, 2372, 295, 50728], "temperature": 0.0, "avg_logprob": -0.12754526370909156, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.0016679485561326146}, {"id": 349, "seek": 164684, "start": 1654.12, "end": 1658.4399999999998, "text": " background knowledge about the world, mostly by observation, a little bit by interaction,", "tokens": [50728, 3678, 3601, 466, 264, 1002, 11, 5240, 538, 14816, 11, 257, 707, 857, 538, 9285, 11, 50944], "temperature": 0.0, "avg_logprob": -0.12754526370909156, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.0016679485561326146}, {"id": 350, "seek": 164684, "start": 1658.4399999999998, "end": 1665.08, "text": " when they start to get old enough to actually act on the world, but mostly just by observation.", "tokens": [50944, 562, 436, 722, 281, 483, 1331, 1547, 281, 767, 605, 322, 264, 1002, 11, 457, 5240, 445, 538, 14816, 13, 51276], "temperature": 0.0, "avg_logprob": -0.12754526370909156, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.0016679485561326146}, {"id": 351, "seek": 164684, "start": 1666.76, "end": 1673.6399999999999, "text": " The type of knowledge that they learn, things like intuitive physics, gravity, inertia,", "tokens": [51360, 440, 2010, 295, 3601, 300, 436, 1466, 11, 721, 411, 21769, 10649, 11, 12110, 11, 37234, 11, 51704], "temperature": 0.0, "avg_logprob": -0.12754526370909156, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.0016679485561326146}, {"id": 352, "seek": 167364, "start": 1673.64, "end": 1677.88, "text": " conservation of momentum, things like that, pops up only around the age of nine months.", "tokens": [50364, 16185, 295, 11244, 11, 721, 411, 300, 11, 16795, 493, 787, 926, 264, 3205, 295, 4949, 2493, 13, 50576], "temperature": 0.0, "avg_logprob": -0.08959963425346043, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.0024283286184072495}, {"id": 353, "seek": 167364, "start": 1678.5200000000002, "end": 1682.44, "text": " It takes about nine months for babies to really figure this out, that objects that are not supported", "tokens": [50608, 467, 2516, 466, 4949, 2493, 337, 10917, 281, 534, 2573, 341, 484, 11, 300, 6565, 300, 366, 406, 8104, 50804], "temperature": 0.0, "avg_logprob": -0.08959963425346043, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.0024283286184072495}, {"id": 354, "seek": 167364, "start": 1682.44, "end": 1687.4, "text": " will fall. But how do they do this? How do they learn this? Obviously, they don't do this like", "tokens": [50804, 486, 2100, 13, 583, 577, 360, 436, 360, 341, 30, 1012, 360, 436, 1466, 341, 30, 7580, 11, 436, 500, 380, 360, 341, 411, 51052], "temperature": 0.0, "avg_logprob": -0.08959963425346043, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.0024283286184072495}, {"id": 355, "seek": 167364, "start": 1687.4, "end": 1695.0800000000002, "text": " LLMs, because if LLMs were the answer to learning like humans, first of all, we would not need", "tokens": [51052, 441, 43, 26386, 11, 570, 498, 441, 43, 26386, 645, 264, 1867, 281, 2539, 411, 6255, 11, 700, 295, 439, 11, 321, 576, 406, 643, 51436], "temperature": 0.0, "avg_logprob": -0.08959963425346043, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.0024283286184072495}, {"id": 356, "seek": 167364, "start": 1695.72, "end": 1700.92, "text": " one trillion tokens to train them. Humans are not exposed to that much text information.", "tokens": [51468, 472, 18723, 22667, 281, 3847, 552, 13, 35809, 366, 406, 9495, 281, 300, 709, 2487, 1589, 13, 51728], "temperature": 0.0, "avg_logprob": -0.08959963425346043, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.0024283286184072495}, {"id": 357, "seek": 170092, "start": 1701.8000000000002, "end": 1710.8400000000001, "text": " Reading one and a half trillion tokens for human reading eight hours a day at normal speed would", "tokens": [50408, 29766, 472, 293, 257, 1922, 18723, 22667, 337, 1952, 3760, 3180, 2496, 257, 786, 412, 2710, 3073, 576, 50860], "temperature": 0.0, "avg_logprob": -0.14562211717878068, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.0013215686194598675}, {"id": 358, "seek": 170092, "start": 1710.8400000000001, "end": 1722.1200000000001, "text": " take about 20,000 years. That's obviously way more than any humans can do. But there are things", "tokens": [50860, 747, 466, 945, 11, 1360, 924, 13, 663, 311, 2745, 636, 544, 813, 604, 6255, 393, 360, 13, 583, 456, 366, 721, 51424], "temperature": 0.0, "avg_logprob": -0.14562211717878068, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.0013215686194598675}, {"id": 359, "seek": 170092, "start": 1722.1200000000001, "end": 1727.88, "text": " that cats and dogs and young humans can do that are pretty amazing that LLMs can't even touch,", "tokens": [51424, 300, 11111, 293, 7197, 293, 2037, 6255, 393, 360, 300, 366, 1238, 2243, 300, 441, 43, 26386, 393, 380, 754, 2557, 11, 51712], "temperature": 0.0, "avg_logprob": -0.14562211717878068, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.0013215686194598675}, {"id": 360, "seek": 172788, "start": 1728.6000000000001, "end": 1737.4, "text": " not even remotely close. So cats and dogs can do things that robots cannot come anywhere close", "tokens": [50400, 406, 754, 20824, 1998, 13, 407, 11111, 293, 7197, 393, 360, 721, 300, 14733, 2644, 808, 4992, 1998, 50840], "temperature": 0.0, "avg_logprob": -0.09722118377685547, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.004450996406376362}, {"id": 361, "seek": 172788, "start": 1737.4, "end": 1742.8400000000001, "text": " to doing today, not because we can't construct the mechanical systems for it. It's just because we", "tokens": [50840, 281, 884, 965, 11, 406, 570, 321, 393, 380, 7690, 264, 12070, 3652, 337, 309, 13, 467, 311, 445, 570, 321, 51112], "temperature": 0.0, "avg_logprob": -0.09722118377685547, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.004450996406376362}, {"id": 362, "seek": 172788, "start": 1742.8400000000001, "end": 1748.3600000000001, "text": " can't build the intelligence for it. Any 10-year-old child can learn to clear up the dinner table and", "tokens": [51112, 393, 380, 1322, 264, 7599, 337, 309, 13, 2639, 1266, 12, 5294, 12, 2641, 1440, 393, 1466, 281, 1850, 493, 264, 6148, 3199, 293, 51388], "temperature": 0.0, "avg_logprob": -0.09722118377685547, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.004450996406376362}, {"id": 363, "seek": 172788, "start": 1748.3600000000001, "end": 1754.8400000000001, "text": " fill up the dishwasher in minutes, probably in one shot. We do not have robots that can do that.", "tokens": [51388, 2836, 493, 264, 38009, 294, 2077, 11, 1391, 294, 472, 3347, 13, 492, 360, 406, 362, 14733, 300, 393, 360, 300, 13, 51712], "temperature": 0.0, "avg_logprob": -0.09722118377685547, "compression_ratio": 1.6680851063829787, "no_speech_prob": 0.004450996406376362}, {"id": 364, "seek": 175484, "start": 1754.84, "end": 1759.72, "text": " We don't have domestic robots. Any 17-year-old can learn to drive a car in about 20 hours of practice,", "tokens": [50364, 492, 500, 380, 362, 10939, 14733, 13, 2639, 3282, 12, 5294, 12, 2641, 393, 1466, 281, 3332, 257, 1032, 294, 466, 945, 2496, 295, 3124, 11, 50608], "temperature": 0.0, "avg_logprob": -0.12061999777088994, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0021422789432108402}, {"id": 365, "seek": 175484, "start": 1759.72, "end": 1764.6, "text": " and we still don't have a limited level of autonomous driving. So that means we're missing", "tokens": [50608, 293, 321, 920, 500, 380, 362, 257, 5567, 1496, 295, 23797, 4840, 13, 407, 300, 1355, 321, 434, 5361, 50852], "temperature": 0.0, "avg_logprob": -0.12061999777088994, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0021422789432108402}, {"id": 366, "seek": 175484, "start": 1764.6, "end": 1769.6399999999999, "text": " something really big in terms of learning that is very different from the way humans and animals", "tokens": [50852, 746, 534, 955, 294, 2115, 295, 2539, 300, 307, 588, 819, 490, 264, 636, 6255, 293, 4882, 51104], "temperature": 0.0, "avg_logprob": -0.12061999777088994, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0021422789432108402}, {"id": 367, "seek": 175484, "start": 1771.32, "end": 1776.84, "text": " learn. And this is just another example of the Moravec paradox, which is that there are things", "tokens": [51188, 1466, 13, 400, 341, 307, 445, 1071, 1365, 295, 264, 5146, 946, 66, 26221, 11, 597, 307, 300, 456, 366, 721, 51464], "temperature": 0.0, "avg_logprob": -0.12061999777088994, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0021422789432108402}, {"id": 368, "seek": 175484, "start": 1776.84, "end": 1782.4399999999998, "text": " that seem easy for humans and turn out to be really difficult for AI and vice versa. AI systems", "tokens": [51464, 300, 1643, 1858, 337, 6255, 293, 1261, 484, 281, 312, 534, 2252, 337, 7318, 293, 11964, 25650, 13, 7318, 3652, 51744], "temperature": 0.0, "avg_logprob": -0.12061999777088994, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.0021422789432108402}, {"id": 369, "seek": 178244, "start": 1782.44, "end": 1789.8, "text": " are much better than humans at many tasks, narrow tasks, and we are nowhere near finding", "tokens": [50364, 366, 709, 1101, 813, 6255, 412, 867, 9608, 11, 9432, 9608, 11, 293, 321, 366, 11159, 2651, 5006, 50732], "temperature": 0.0, "avg_logprob": -0.13271507140128844, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0021735781338065863}, {"id": 370, "seek": 178244, "start": 1792.28, "end": 1798.3600000000001, "text": " mechanisms by which machines can approach the sort of type of understanding of the world that a cat", "tokens": [50856, 15902, 538, 597, 8379, 393, 3109, 264, 1333, 295, 2010, 295, 3701, 295, 264, 1002, 300, 257, 3857, 51160], "temperature": 0.0, "avg_logprob": -0.13271507140128844, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0021735781338065863}, {"id": 371, "seek": 178244, "start": 1798.3600000000001, "end": 1806.1200000000001, "text": " or a dog can have. Okay, so very some idea about how we can approach that problem. And again,", "tokens": [51160, 420, 257, 3000, 393, 362, 13, 1033, 11, 370, 588, 512, 1558, 466, 577, 321, 393, 3109, 300, 1154, 13, 400, 797, 11, 51548], "temperature": 0.0, "avg_logprob": -0.13271507140128844, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0021735781338065863}, {"id": 372, "seek": 178244, "start": 1806.1200000000001, "end": 1811.48, "text": " it's based on self-supervised learning, learning to fill in the blanks. If we train the neural net", "tokens": [51548, 309, 311, 2361, 322, 2698, 12, 48172, 24420, 2539, 11, 2539, 281, 2836, 294, 264, 8247, 82, 13, 759, 321, 3847, 264, 18161, 2533, 51816], "temperature": 0.0, "avg_logprob": -0.13271507140128844, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0021735781338065863}, {"id": 373, "seek": 181148, "start": 1811.48, "end": 1817.08, "text": " to do video prediction, something we've been attempting to do for 10 years now. It doesn't", "tokens": [50364, 281, 360, 960, 17630, 11, 746, 321, 600, 668, 22001, 281, 360, 337, 1266, 924, 586, 13, 467, 1177, 380, 50644], "temperature": 0.0, "avg_logprob": -0.06236131191253662, "compression_ratio": 1.7263157894736842, "no_speech_prob": 0.0035757720470428467}, {"id": 374, "seek": 181148, "start": 1817.08, "end": 1821.56, "text": " work very well. If you look at the second column of this little animation at the bottom, the predictions", "tokens": [50644, 589, 588, 731, 13, 759, 291, 574, 412, 264, 1150, 7738, 295, 341, 707, 9603, 412, 264, 2767, 11, 264, 21264, 50868], "temperature": 0.0, "avg_logprob": -0.06236131191253662, "compression_ratio": 1.7263157894736842, "no_speech_prob": 0.0035757720470428467}, {"id": 375, "seek": 181148, "start": 1821.56, "end": 1826.92, "text": " that are produced by the system, and this is a very stylized video, are very blurry. It's because", "tokens": [50868, 300, 366, 7126, 538, 264, 1185, 11, 293, 341, 307, 257, 588, 23736, 1602, 960, 11, 366, 588, 37644, 13, 467, 311, 570, 51136], "temperature": 0.0, "avg_logprob": -0.06236131191253662, "compression_ratio": 1.7263157894736842, "no_speech_prob": 0.0035757720470428467}, {"id": 376, "seek": 181148, "start": 1826.92, "end": 1831.88, "text": " the system is trained to make one single prediction, and it cannot exactly predict what's going to", "tokens": [51136, 264, 1185, 307, 8895, 281, 652, 472, 2167, 17630, 11, 293, 309, 2644, 2293, 6069, 437, 311, 516, 281, 51384], "temperature": 0.0, "avg_logprob": -0.06236131191253662, "compression_ratio": 1.7263157894736842, "no_speech_prob": 0.0035757720470428467}, {"id": 377, "seek": 181148, "start": 1831.88, "end": 1836.92, "text": " happen in the video. So as a result, it predicts a kind of blurry mess, which is the average of all", "tokens": [51384, 1051, 294, 264, 960, 13, 407, 382, 257, 1874, 11, 309, 6069, 82, 257, 733, 295, 37644, 2082, 11, 597, 307, 264, 4274, 295, 439, 51636], "temperature": 0.0, "avg_logprob": -0.06236131191253662, "compression_ratio": 1.7263157894736842, "no_speech_prob": 0.0035757720470428467}, {"id": 378, "seek": 183692, "start": 1836.92, "end": 1843.72, "text": " the possible features, plausible features that can happen. It's the same thing if you use a similar", "tokens": [50364, 264, 1944, 4122, 11, 39925, 4122, 300, 393, 1051, 13, 467, 311, 264, 912, 551, 498, 291, 764, 257, 2531, 50704], "temperature": 0.0, "avg_logprob": -0.10141888980207772, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.0015667418483644724}, {"id": 379, "seek": 183692, "start": 1843.72, "end": 1851.16, "text": " system to predict natural video, you get those blurry predictions. So my solution to this is", "tokens": [50704, 1185, 281, 6069, 3303, 960, 11, 291, 483, 729, 37644, 21264, 13, 407, 452, 3827, 281, 341, 307, 51076], "temperature": 0.0, "avg_logprob": -0.10141888980207772, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.0015667418483644724}, {"id": 380, "seek": 183692, "start": 1851.16, "end": 1860.28, "text": " something I call joint embedding predictive architecture, JEPA. And the main idea behind", "tokens": [51076, 746, 286, 818, 7225, 12240, 3584, 35521, 9482, 11, 508, 8929, 32, 13, 400, 264, 2135, 1558, 2261, 51532], "temperature": 0.0, "avg_logprob": -0.10141888980207772, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.0015667418483644724}, {"id": 381, "seek": 183692, "start": 1860.28, "end": 1866.8400000000001, "text": " JEPA is to abandon the idea that prediction needs to be generative. Okay, so the most", "tokens": [51532, 508, 8929, 32, 307, 281, 9072, 264, 1558, 300, 17630, 2203, 281, 312, 1337, 1166, 13, 1033, 11, 370, 264, 881, 51860], "temperature": 0.0, "avg_logprob": -0.10141888980207772, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.0015667418483644724}, {"id": 382, "seek": 186684, "start": 1866.84, "end": 1871.32, "text": " popular thing at the moment is generative AI, generative models. What I'm going to tell you", "tokens": [50364, 3743, 551, 412, 264, 1623, 307, 1337, 1166, 7318, 11, 1337, 1166, 5245, 13, 708, 286, 478, 516, 281, 980, 291, 50588], "temperature": 0.0, "avg_logprob": -0.11113549816992975, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.0004943421809002757}, {"id": 383, "seek": 186684, "start": 1871.32, "end": 1877.08, "text": " now is to abandon it. Okay, not a very popular idea at the moment, but here is the argument.", "tokens": [50588, 586, 307, 281, 9072, 309, 13, 1033, 11, 406, 257, 588, 3743, 1558, 412, 264, 1623, 11, 457, 510, 307, 264, 6770, 13, 50876], "temperature": 0.0, "avg_logprob": -0.11113549816992975, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.0004943421809002757}, {"id": 384, "seek": 186684, "start": 1878.12, "end": 1883.56, "text": " A generative model is one that, for which you give it an input x, let's say initial segment of a video", "tokens": [50928, 316, 1337, 1166, 2316, 307, 472, 300, 11, 337, 597, 291, 976, 309, 364, 4846, 2031, 11, 718, 311, 584, 5883, 9469, 295, 257, 960, 51200], "temperature": 0.0, "avg_logprob": -0.11113549816992975, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.0004943421809002757}, {"id": 385, "seek": 186684, "start": 1883.56, "end": 1889.48, "text": " or a text, run it through an encoder and a predictor, and then try to predict a variable y, which", "tokens": [51200, 420, 257, 2487, 11, 1190, 309, 807, 364, 2058, 19866, 293, 257, 6069, 284, 11, 293, 550, 853, 281, 6069, 257, 7006, 288, 11, 597, 51496], "temperature": 0.0, "avg_logprob": -0.11113549816992975, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.0004943421809002757}, {"id": 386, "seek": 186684, "start": 1889.48, "end": 1894.4399999999998, "text": " may be the continuation of that video or the continuation of that text, or the missing words", "tokens": [51496, 815, 312, 264, 29357, 295, 300, 960, 420, 264, 29357, 295, 300, 2487, 11, 420, 264, 5361, 2283, 51744], "temperature": 0.0, "avg_logprob": -0.11113549816992975, "compression_ratio": 1.7969924812030076, "no_speech_prob": 0.0004943421809002757}, {"id": 387, "seek": 189444, "start": 1894.44, "end": 1902.28, "text": " in that text, and the error by which you measure the performance of the system is basically the", "tokens": [50364, 294, 300, 2487, 11, 293, 264, 6713, 538, 597, 291, 3481, 264, 3389, 295, 264, 1185, 307, 1936, 264, 50756], "temperature": 0.0, "avg_logprob": -0.08879855621692746, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0006560832262039185}, {"id": 388, "seek": 189444, "start": 1902.28, "end": 1906.76, "text": " some sort of divergence measure between the predicted y and the actual y. Okay, that's how", "tokens": [50756, 512, 1333, 295, 47387, 3481, 1296, 264, 19147, 288, 293, 264, 3539, 288, 13, 1033, 11, 300, 311, 577, 50980], "temperature": 0.0, "avg_logprob": -0.08879855621692746, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0006560832262039185}, {"id": 389, "seek": 189444, "start": 1906.76, "end": 1914.2, "text": " you would train this model. It's a generative model because it predicts y. A joint embedding", "tokens": [50980, 291, 576, 3847, 341, 2316, 13, 467, 311, 257, 1337, 1166, 2316, 570, 309, 6069, 82, 288, 13, 316, 7225, 12240, 3584, 51352], "temperature": 0.0, "avg_logprob": -0.08879855621692746, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0006560832262039185}, {"id": 390, "seek": 189444, "start": 1914.2, "end": 1920.2, "text": " predictive architectures does not attempt to predict y. It attempts to predict a representation", "tokens": [51352, 35521, 6331, 1303, 775, 406, 5217, 281, 6069, 288, 13, 467, 15257, 281, 6069, 257, 10290, 51652], "temperature": 0.0, "avg_logprob": -0.08879855621692746, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0006560832262039185}, {"id": 391, "seek": 192020, "start": 1920.2, "end": 1925.16, "text": " of y. Okay, so both x and y go through encoders that compute representations,", "tokens": [50364, 295, 288, 13, 1033, 11, 370, 1293, 2031, 293, 288, 352, 807, 2058, 378, 433, 300, 14722, 33358, 11, 50612], "temperature": 0.0, "avg_logprob": -0.09372943639755249, "compression_ratio": 1.6373056994818653, "no_speech_prob": 0.0008417710196226835}, {"id": 392, "seek": 192020, "start": 1926.3600000000001, "end": 1929.0800000000002, "text": " and you perform the prediction in representation space.", "tokens": [50672, 293, 291, 2042, 264, 17630, 294, 10290, 1901, 13, 50808], "temperature": 0.0, "avg_logprob": -0.09372943639755249, "compression_ratio": 1.6373056994818653, "no_speech_prob": 0.0008417710196226835}, {"id": 393, "seek": 192020, "start": 1932.52, "end": 1939.48, "text": " And the advantage of this is that the encoder of y may have invariant properties", "tokens": [50980, 400, 264, 5002, 295, 341, 307, 300, 264, 2058, 19866, 295, 288, 815, 362, 33270, 394, 7221, 51328], "temperature": 0.0, "avg_logprob": -0.09372943639755249, "compression_ratio": 1.6373056994818653, "no_speech_prob": 0.0008417710196226835}, {"id": 394, "seek": 192020, "start": 1940.2, "end": 1946.3600000000001, "text": " that map multiple y's to the same sy. And so if there are things that are very, very hard to predict,", "tokens": [51364, 300, 4471, 3866, 288, 311, 281, 264, 912, 943, 13, 400, 370, 498, 456, 366, 721, 300, 366, 588, 11, 588, 1152, 281, 6069, 11, 51672], "temperature": 0.0, "avg_logprob": -0.09372943639755249, "compression_ratio": 1.6373056994818653, "no_speech_prob": 0.0008417710196226835}, {"id": 395, "seek": 194636, "start": 1946.36, "end": 1951.08, "text": " the encoder might eliminate that information that is hard to predict or impossible to predict", "tokens": [50364, 264, 2058, 19866, 1062, 13819, 300, 1589, 300, 307, 1152, 281, 6069, 420, 6243, 281, 6069, 50600], "temperature": 0.0, "avg_logprob": -0.11220995311079354, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.0011326473904773593}, {"id": 396, "seek": 194636, "start": 1951.08, "end": 1956.28, "text": " from sy so that the prediction problem becomes easier. So let's say, for example, that you're", "tokens": [50600, 490, 943, 370, 300, 264, 17630, 1154, 3643, 3571, 13, 407, 718, 311, 584, 11, 337, 1365, 11, 300, 291, 434, 50860], "temperature": 0.0, "avg_logprob": -0.11220995311079354, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.0011326473904773593}, {"id": 397, "seek": 194636, "start": 1956.28, "end": 1962.28, "text": " driving along the road and the predictive model here is trying to predict what's going to happen", "tokens": [50860, 4840, 2051, 264, 3060, 293, 264, 35521, 2316, 510, 307, 1382, 281, 6069, 437, 311, 516, 281, 1051, 51160], "temperature": 0.0, "avg_logprob": -0.11220995311079354, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.0011326473904773593}, {"id": 398, "seek": 194636, "start": 1962.28, "end": 1966.6799999999998, "text": " on the road. So because it's a self-driving car, it wants to predict what the other cars on the road", "tokens": [51160, 322, 264, 3060, 13, 407, 570, 309, 311, 257, 2698, 12, 47094, 1032, 11, 309, 2738, 281, 6069, 437, 264, 661, 5163, 322, 264, 3060, 51380], "temperature": 0.0, "avg_logprob": -0.11220995311079354, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.0011326473904773593}, {"id": 399, "seek": 194636, "start": 1966.6799999999998, "end": 1972.84, "text": " are going to do. But bordering the road, there might be trees and there is wind today, so the leaves", "tokens": [51380, 366, 516, 281, 360, 13, 583, 25872, 1794, 264, 3060, 11, 456, 1062, 312, 5852, 293, 456, 307, 2468, 965, 11, 370, 264, 5510, 51688], "temperature": 0.0, "avg_logprob": -0.11220995311079354, "compression_ratio": 1.8837209302325582, "no_speech_prob": 0.0011326473904773593}, {"id": 400, "seek": 197284, "start": 1972.84, "end": 1979.32, "text": " on the trees are moving in chaotic ways. Behind the trees, there is a pond and there is ripples", "tokens": [50364, 322, 264, 5852, 366, 2684, 294, 27013, 2098, 13, 20475, 264, 5852, 11, 456, 307, 257, 17384, 293, 456, 307, 367, 37674, 50688], "temperature": 0.0, "avg_logprob": -0.10524052113026112, "compression_ratio": 1.83984375, "no_speech_prob": 0.0008280204492621124}, {"id": 401, "seek": 197284, "start": 1979.32, "end": 1986.84, "text": " on the pond because of the wind. Those ripples and the motion of the leaves are not only very hard", "tokens": [50688, 322, 264, 17384, 570, 295, 264, 2468, 13, 3950, 367, 37674, 293, 264, 5394, 295, 264, 5510, 366, 406, 787, 588, 1152, 51064], "temperature": 0.0, "avg_logprob": -0.10524052113026112, "compression_ratio": 1.83984375, "no_speech_prob": 0.0008280204492621124}, {"id": 402, "seek": 197284, "start": 1986.84, "end": 1994.36, "text": " to predict, pretty much impossible to predict, but also very informative. There's a huge amount", "tokens": [51064, 281, 6069, 11, 1238, 709, 6243, 281, 6069, 11, 457, 611, 588, 27759, 13, 821, 311, 257, 2603, 2372, 51440], "temperature": 0.0, "avg_logprob": -0.10524052113026112, "compression_ratio": 1.83984375, "no_speech_prob": 0.0008280204492621124}, {"id": 403, "seek": 197284, "start": 1994.36, "end": 1998.12, "text": " of information in there. And so if you use a generative model, that generative model will", "tokens": [51440, 295, 1589, 294, 456, 13, 400, 370, 498, 291, 764, 257, 1337, 1166, 2316, 11, 300, 1337, 1166, 2316, 486, 51628], "temperature": 0.0, "avg_logprob": -0.10524052113026112, "compression_ratio": 1.83984375, "no_speech_prob": 0.0008280204492621124}, {"id": 404, "seek": 197284, "start": 1998.12, "end": 2002.4399999999998, "text": " have to devote an enormous amount of resources trying to predict all of those details that", "tokens": [51628, 362, 281, 23184, 364, 11322, 2372, 295, 3593, 1382, 281, 6069, 439, 295, 729, 4365, 300, 51844], "temperature": 0.0, "avg_logprob": -0.10524052113026112, "compression_ratio": 1.83984375, "no_speech_prob": 0.0008280204492621124}, {"id": 405, "seek": 200244, "start": 2002.44, "end": 2007.72, "text": " are irrelevant to the task, really. Whereas if you have a model like the one on the right, the JEPA,", "tokens": [50364, 366, 28682, 281, 264, 5633, 11, 534, 13, 13813, 498, 291, 362, 257, 2316, 411, 264, 472, 322, 264, 558, 11, 264, 508, 8929, 32, 11, 50628], "temperature": 0.0, "avg_logprob": -0.12375444278382418, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0004156929790042341}, {"id": 406, "seek": 200244, "start": 2008.8400000000001, "end": 2016.92, "text": " the JEPA can choose to eliminate those details from the scene and only keep the details about why", "tokens": [50684, 264, 508, 8929, 32, 393, 2826, 281, 13819, 729, 4365, 490, 264, 4145, 293, 787, 1066, 264, 4365, 466, 983, 51088], "temperature": 0.0, "avg_logprob": -0.12375444278382418, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0004156929790042341}, {"id": 407, "seek": 200244, "start": 2016.92, "end": 2021.64, "text": " that are relatively easy to predict, like the motion of the other cars, for example.", "tokens": [51088, 300, 366, 7226, 1858, 281, 6069, 11, 411, 264, 5394, 295, 264, 661, 5163, 11, 337, 1365, 13, 51324], "temperature": 0.0, "avg_logprob": -0.12375444278382418, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0004156929790042341}, {"id": 408, "seek": 200244, "start": 2023.0, "end": 2027.0, "text": " So that's my argument for the joint emitting architecture and that means abandoning generative", "tokens": [51392, 407, 300, 311, 452, 6770, 337, 264, 7225, 846, 2414, 9482, 293, 300, 1355, 9072, 278, 1337, 1166, 51592], "temperature": 0.0, "avg_logprob": -0.12375444278382418, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0004156929790042341}, {"id": 409, "seek": 200244, "start": 2027.0, "end": 2030.8400000000001, "text": " models. Now, of course, you want to use generative models if you want to generate,", "tokens": [51592, 5245, 13, 823, 11, 295, 1164, 11, 291, 528, 281, 764, 1337, 1166, 5245, 498, 291, 528, 281, 8460, 11, 51784], "temperature": 0.0, "avg_logprob": -0.12375444278382418, "compression_ratio": 1.7137546468401488, "no_speech_prob": 0.0004156929790042341}, {"id": 410, "seek": 203084, "start": 2031.48, "end": 2036.84, "text": " but if what you want is to understand the world and then be able to plan, you don't need generative", "tokens": [50396, 457, 498, 437, 291, 528, 307, 281, 1223, 264, 1002, 293, 550, 312, 1075, 281, 1393, 11, 291, 500, 380, 643, 1337, 1166, 50664], "temperature": 0.0, "avg_logprob": -0.08563001306207331, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.0007711330545134842}, {"id": 411, "seek": 203084, "start": 2036.84, "end": 2041.8, "text": " models. You need those joint emitting architectures. The reason I'm advocating for this is because", "tokens": [50664, 5245, 13, 509, 643, 729, 7225, 846, 2414, 6331, 1303, 13, 440, 1778, 286, 478, 32050, 337, 341, 307, 570, 50912], "temperature": 0.0, "avg_logprob": -0.08563001306207331, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.0007711330545134842}, {"id": 412, "seek": 203084, "start": 2041.8, "end": 2047.8, "text": " experimentally, if you want to use self-supervised zoning in the context of images as opposed to text,", "tokens": [50912, 5120, 379, 11, 498, 291, 528, 281, 764, 2698, 12, 48172, 24420, 37184, 294, 264, 4319, 295, 5267, 382, 8851, 281, 2487, 11, 51212], "temperature": 0.0, "avg_logprob": -0.08563001306207331, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.0007711330545134842}, {"id": 413, "seek": 203084, "start": 2047.8, "end": 2051.96, "text": " the only architectures that work well are joint emitting architectures.", "tokens": [51212, 264, 787, 6331, 1303, 300, 589, 731, 366, 7225, 846, 2414, 6331, 1303, 13, 51420], "temperature": 0.0, "avg_logprob": -0.08563001306207331, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.0007711330545134842}, {"id": 414, "seek": 203084, "start": 2052.68, "end": 2056.68, "text": " There are architectures like the one on the left here, which is a joint emitting architecture", "tokens": [51456, 821, 366, 6331, 1303, 411, 264, 472, 322, 264, 1411, 510, 11, 597, 307, 257, 7225, 846, 2414, 9482, 51656], "temperature": 0.0, "avg_logprob": -0.08563001306207331, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.0007711330545134842}, {"id": 415, "seek": 205668, "start": 2056.68, "end": 2061.3199999999997, "text": " without the predictor. This is the most successful approach to self-supervised zoning for image", "tokens": [50364, 1553, 264, 6069, 284, 13, 639, 307, 264, 881, 4406, 3109, 281, 2698, 12, 48172, 24420, 37184, 337, 3256, 50596], "temperature": 0.0, "avg_logprob": -0.09335759411687436, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.007782267406582832}, {"id": 416, "seek": 205668, "start": 2061.3199999999997, "end": 2069.3199999999997, "text": " recognition. You show image X or rather image Y, then you corrupt this image Y into image X", "tokens": [50596, 11150, 13, 509, 855, 3256, 1783, 420, 2831, 3256, 398, 11, 550, 291, 17366, 341, 3256, 398, 666, 3256, 1783, 50996], "temperature": 0.0, "avg_logprob": -0.09335759411687436, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.007782267406582832}, {"id": 417, "seek": 205668, "start": 2069.3199999999997, "end": 2075.7999999999997, "text": " by distorting it, blurring it, adding noise, masking some parts of it, changing the framing,", "tokens": [50996, 538, 37555, 278, 309, 11, 14257, 2937, 309, 11, 5127, 5658, 11, 31226, 512, 3166, 295, 309, 11, 4473, 264, 28971, 11, 51320], "temperature": 0.0, "avg_logprob": -0.09335759411687436, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.007782267406582832}, {"id": 418, "seek": 205668, "start": 2075.7999999999997, "end": 2083.96, "text": " the size, etc. And then you run both images through the encoders and you force the system", "tokens": [51320, 264, 2744, 11, 5183, 13, 400, 550, 291, 1190, 1293, 5267, 807, 264, 2058, 378, 433, 293, 291, 3464, 264, 1185, 51728], "temperature": 0.0, "avg_logprob": -0.09335759411687436, "compression_ratio": 1.6228070175438596, "no_speech_prob": 0.007782267406582832}, {"id": 419, "seek": 208396, "start": 2083.96, "end": 2088.12, "text": " or you train the system to produce representations that are identical for the two images,", "tokens": [50364, 420, 291, 3847, 264, 1185, 281, 5258, 33358, 300, 366, 14800, 337, 264, 732, 5267, 11, 50572], "temperature": 0.0, "avg_logprob": -0.116836121938761, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.0014999491395428777}, {"id": 420, "seek": 208396, "start": 2088.92, "end": 2094.12, "text": " so that the representation of the corrupted image is the same as the representation of the", "tokens": [50612, 370, 300, 264, 10290, 295, 264, 39480, 3256, 307, 264, 912, 382, 264, 10290, 295, 264, 50872], "temperature": 0.0, "avg_logprob": -0.116836121938761, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.0014999491395428777}, {"id": 421, "seek": 208396, "start": 2094.12, "end": 2099.7200000000003, "text": " uncorrupted image. And that builds representations that are invariant to the corruptions, essentially.", "tokens": [50872, 6219, 284, 5428, 292, 3256, 13, 400, 300, 15182, 33358, 300, 366, 33270, 394, 281, 264, 17366, 626, 11, 4476, 13, 51152], "temperature": 0.0, "avg_logprob": -0.116836121938761, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.0014999491395428777}, {"id": 422, "seek": 208396, "start": 2101.4, "end": 2106.04, "text": " So those methods, there is a whole bunch of them, about a dozen of them, and they work really well,", "tokens": [51236, 407, 729, 7150, 11, 456, 307, 257, 1379, 3840, 295, 552, 11, 466, 257, 16654, 295, 552, 11, 293, 436, 589, 534, 731, 11, 51468], "temperature": 0.0, "avg_logprob": -0.116836121938761, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.0014999491395428777}, {"id": 423, "seek": 208396, "start": 2106.76, "end": 2110.92, "text": " whereas all the methods to learn image features that are based on reconstruction,", "tokens": [51504, 9735, 439, 264, 7150, 281, 1466, 3256, 4122, 300, 366, 2361, 322, 31565, 11, 51712], "temperature": 0.0, "avg_logprob": -0.116836121938761, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.0014999491395428777}, {"id": 424, "seek": 211092, "start": 2110.92, "end": 2114.52, "text": " generative models, don't work. At least they don't work nearly as well.", "tokens": [50364, 1337, 1166, 5245, 11, 500, 380, 589, 13, 1711, 1935, 436, 500, 380, 589, 6217, 382, 731, 13, 50544], "temperature": 0.0, "avg_logprob": -0.1351632630383527, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0009089498780667782}, {"id": 425, "seek": 211092, "start": 2115.88, "end": 2120.92, "text": " So what this slide shows is kind of different versions of this joint emitting predictive", "tokens": [50612, 407, 437, 341, 4137, 3110, 307, 733, 295, 819, 9606, 295, 341, 7225, 846, 2414, 35521, 50864], "temperature": 0.0, "avg_logprob": -0.1351632630383527, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0009089498780667782}, {"id": 426, "seek": 211092, "start": 2120.92, "end": 2126.44, "text": " architecture, either with a predictor or without, with a predictor that can be stochastic,", "tokens": [50864, 9482, 11, 2139, 365, 257, 6069, 284, 420, 1553, 11, 365, 257, 6069, 284, 300, 393, 312, 342, 8997, 2750, 11, 51140], "temperature": 0.0, "avg_logprob": -0.1351632630383527, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0009089498780667782}, {"id": 427, "seek": 211092, "start": 2126.44, "end": 2132.2000000000003, "text": " having latent variables or not. And the question is how you train this, because the problem is,", "tokens": [51140, 1419, 48994, 9102, 420, 406, 13, 400, 264, 1168, 307, 577, 291, 3847, 341, 11, 570, 264, 1154, 307, 11, 51428], "temperature": 0.0, "avg_logprob": -0.1351632630383527, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0009089498780667782}, {"id": 428, "seek": 211092, "start": 2132.2000000000003, "end": 2138.04, "text": " if you train a system like this, without being careful, is going to collapse. If you train a", "tokens": [51428, 498, 291, 3847, 257, 1185, 411, 341, 11, 1553, 885, 5026, 11, 307, 516, 281, 15584, 13, 759, 291, 3847, 257, 51720], "temperature": 0.0, "avg_logprob": -0.1351632630383527, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0009089498780667782}, {"id": 429, "seek": 213804, "start": 2138.04, "end": 2145.0, "text": " system, you give it pairs of images, let's say x and y, or video snippets, x and y, and you tell it", "tokens": [50364, 1185, 11, 291, 976, 309, 15494, 295, 5267, 11, 718, 311, 584, 2031, 293, 288, 11, 420, 960, 35623, 1385, 11, 2031, 293, 288, 11, 293, 291, 980, 309, 50712], "temperature": 0.0, "avg_logprob": -0.1378111487940738, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.00046498875599354506}, {"id": 430, "seek": 213804, "start": 2146.04, "end": 2150.44, "text": " compute representations that are identical for x and y, the system will just collapse. It will", "tokens": [50764, 14722, 33358, 300, 366, 14800, 337, 2031, 293, 288, 11, 264, 1185, 486, 445, 15584, 13, 467, 486, 50984], "temperature": 0.0, "avg_logprob": -0.1378111487940738, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.00046498875599354506}, {"id": 431, "seek": 213804, "start": 2151.24, "end": 2157.32, "text": " produce sx and xy that are constant, and then just completely ignore x and y,", "tokens": [51024, 5258, 262, 87, 293, 2031, 88, 300, 366, 5754, 11, 293, 550, 445, 2584, 11200, 2031, 293, 288, 11, 51328], "temperature": 0.0, "avg_logprob": -0.1378111487940738, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.00046498875599354506}, {"id": 432, "seek": 213804, "start": 2158.12, "end": 2163.48, "text": " so that the distance between sx and sy is minimized. So that's a collapse.", "tokens": [51368, 370, 300, 264, 4560, 1296, 262, 87, 293, 943, 307, 4464, 1602, 13, 407, 300, 311, 257, 15584, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1378111487940738, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.00046498875599354506}, {"id": 433, "seek": 216348, "start": 2164.2, "end": 2172.52, "text": " What's a, how can you correct this? And to correct this, you have to put yourself in", "tokens": [50400, 708, 311, 257, 11, 577, 393, 291, 3006, 341, 30, 400, 281, 3006, 341, 11, 291, 362, 281, 829, 1803, 294, 50816], "temperature": 0.0, "avg_logprob": -0.11992265867150348, "compression_ratio": 1.7, "no_speech_prob": 0.0008949923212639987}, {"id": 434, "seek": 216348, "start": 2172.52, "end": 2176.52, "text": " the context of something called energy-based models, and I'm sure there are a lot of physicists", "tokens": [50816, 264, 4319, 295, 746, 1219, 2281, 12, 6032, 5245, 11, 293, 286, 478, 988, 456, 366, 257, 688, 295, 48716, 51016], "temperature": 0.0, "avg_logprob": -0.11992265867150348, "compression_ratio": 1.7, "no_speech_prob": 0.0008949923212639987}, {"id": 435, "seek": 216348, "start": 2176.52, "end": 2182.44, "text": " in the room, so you can understand what that means. So energy-based models are models where you don't", "tokens": [51016, 294, 264, 1808, 11, 370, 291, 393, 1223, 437, 300, 1355, 13, 407, 2281, 12, 6032, 5245, 366, 5245, 689, 291, 500, 380, 51312], "temperature": 0.0, "avg_logprob": -0.11992265867150348, "compression_ratio": 1.7, "no_speech_prob": 0.0008949923212639987}, {"id": 436, "seek": 216348, "start": 2184.84, "end": 2190.84, "text": " explain what they do in terms of probabilistic modeling, but in terms of an energy function", "tokens": [51432, 2903, 437, 436, 360, 294, 2115, 295, 31959, 3142, 15983, 11, 457, 294, 2115, 295, 364, 2281, 2445, 51732], "temperature": 0.0, "avg_logprob": -0.11992265867150348, "compression_ratio": 1.7, "no_speech_prob": 0.0008949923212639987}, {"id": 437, "seek": 219084, "start": 2190.84, "end": 2195.96, "text": " that captures the dependency between the variables. So maybe a little more explicit here. Let's say", "tokens": [50364, 300, 27986, 264, 33621, 1296, 264, 9102, 13, 407, 1310, 257, 707, 544, 13691, 510, 13, 961, 311, 584, 50620], "temperature": 0.0, "avg_logprob": -0.10832325442806705, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.0012597853783518076}, {"id": 438, "seek": 219084, "start": 2195.96, "end": 2202.44, "text": " you have two variables, x and y, and your datasets are those greenish dots that are supposed to be", "tokens": [50620, 291, 362, 732, 9102, 11, 2031, 293, 288, 11, 293, 428, 42856, 366, 729, 3092, 742, 15026, 300, 366, 3442, 281, 312, 50944], "temperature": 0.0, "avg_logprob": -0.10832325442806705, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.0012597853783518076}, {"id": 439, "seek": 219084, "start": 2202.44, "end": 2211.96, "text": " black. The way an energy-based model captures the dependency between x and y is that it computes", "tokens": [50944, 2211, 13, 440, 636, 364, 2281, 12, 6032, 2316, 27986, 264, 33621, 1296, 2031, 293, 288, 307, 300, 309, 715, 1819, 51420], "temperature": 0.0, "avg_logprob": -0.10832325442806705, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.0012597853783518076}, {"id": 440, "seek": 219084, "start": 2211.96, "end": 2218.36, "text": " an energy function, an implicit function with a scalar output that takes x and y as an input", "tokens": [51420, 364, 2281, 2445, 11, 364, 26947, 2445, 365, 257, 39684, 5598, 300, 2516, 2031, 293, 288, 382, 364, 4846, 51740], "temperature": 0.0, "avg_logprob": -0.10832325442806705, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.0012597853783518076}, {"id": 441, "seek": 221836, "start": 2218.36, "end": 2225.48, "text": " and gives you an energy that needs to be low near the data points, on the data points nearby,", "tokens": [50364, 293, 2709, 291, 364, 2281, 300, 2203, 281, 312, 2295, 2651, 264, 1412, 2793, 11, 322, 264, 1412, 2793, 11184, 11, 50720], "temperature": 0.0, "avg_logprob": -0.07914590413591503, "compression_ratio": 2.0, "no_speech_prob": 0.0008285296498797834}, {"id": 442, "seek": 221836, "start": 2225.48, "end": 2232.76, "text": " and then higher outside of those data points, the region of high data density. And if you have", "tokens": [50720, 293, 550, 2946, 2380, 295, 729, 1412, 2793, 11, 264, 4458, 295, 1090, 1412, 10305, 13, 400, 498, 291, 362, 51084], "temperature": 0.0, "avg_logprob": -0.07914590413591503, "compression_ratio": 2.0, "no_speech_prob": 0.0008285296498797834}, {"id": 443, "seek": 221836, "start": 2232.76, "end": 2236.6800000000003, "text": " such an energy landscape, you have a function that has this, that can compute this energy landscape,", "tokens": [51084, 1270, 364, 2281, 9661, 11, 291, 362, 257, 2445, 300, 575, 341, 11, 300, 393, 14722, 341, 2281, 9661, 11, 51280], "temperature": 0.0, "avg_logprob": -0.07914590413591503, "compression_ratio": 2.0, "no_speech_prob": 0.0008285296498797834}, {"id": 444, "seek": 221836, "start": 2237.6400000000003, "end": 2242.6, "text": " then that function will have captured the dependencies between x and y. You can infer x", "tokens": [51328, 550, 300, 2445, 486, 362, 11828, 264, 36606, 1296, 2031, 293, 288, 13, 509, 393, 13596, 2031, 51576], "temperature": 0.0, "avg_logprob": -0.07914590413591503, "compression_ratio": 2.0, "no_speech_prob": 0.0008285296498797834}, {"id": 445, "seek": 221836, "start": 2242.6, "end": 2247.8, "text": " from y, you can infer y from x, you can have mapping between x and y that are not functions,", "tokens": [51576, 490, 288, 11, 291, 393, 13596, 288, 490, 2031, 11, 291, 393, 362, 18350, 1296, 2031, 293, 288, 300, 366, 406, 6828, 11, 51836], "temperature": 0.0, "avg_logprob": -0.07914590413591503, "compression_ratio": 2.0, "no_speech_prob": 0.0008285296498797834}, {"id": 446, "seek": 224780, "start": 2247.8, "end": 2251.48, "text": " because you can have multiple y's that are compatible with a single x, for example,", "tokens": [50364, 570, 291, 393, 362, 3866, 288, 311, 300, 366, 18218, 365, 257, 2167, 2031, 11, 337, 1365, 11, 50548], "temperature": 0.0, "avg_logprob": -0.10634545299494377, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.0005254134302958846}, {"id": 447, "seek": 224780, "start": 2251.48, "end": 2259.88, "text": " so it captures multi-modality without having to be a probabilistic model. Of course, in physics,", "tokens": [50548, 370, 309, 27986, 4825, 12, 8014, 1860, 1553, 1419, 281, 312, 257, 31959, 3142, 2316, 13, 2720, 1164, 11, 294, 10649, 11, 50968], "temperature": 0.0, "avg_logprob": -0.10634545299494377, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.0005254134302958846}, {"id": 448, "seek": 224780, "start": 2259.88, "end": 2266.28, "text": " we're familiar with this, right? Very often, we write an energy function, and then we turn it", "tokens": [50968, 321, 434, 4963, 365, 341, 11, 558, 30, 4372, 2049, 11, 321, 2464, 364, 2281, 2445, 11, 293, 550, 321, 1261, 309, 51288], "temperature": 0.0, "avg_logprob": -0.10634545299494377, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.0005254134302958846}, {"id": 449, "seek": 224780, "start": 2266.28, "end": 2270.1200000000003, "text": " into a probability distribution over states using a Gibbs distribution. Same idea here,", "tokens": [51288, 666, 257, 8482, 7316, 670, 4368, 1228, 257, 30199, 7316, 13, 10635, 1558, 510, 11, 51480], "temperature": 0.0, "avg_logprob": -0.10634545299494377, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.0005254134302958846}, {"id": 450, "seek": 224780, "start": 2270.1200000000003, "end": 2274.2000000000003, "text": " but here we don't need the Gibbs distribution at all, we just manipulate the energy function", "tokens": [51480, 457, 510, 321, 500, 380, 643, 264, 30199, 7316, 412, 439, 11, 321, 445, 20459, 264, 2281, 2445, 51684], "temperature": 0.0, "avg_logprob": -0.10634545299494377, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.0005254134302958846}, {"id": 451, "seek": 227420, "start": 2274.2, "end": 2279.56, "text": " directly. How do we train a system like this? There's really two categories of training methods.", "tokens": [50364, 3838, 13, 1012, 360, 321, 3847, 257, 1185, 411, 341, 30, 821, 311, 534, 732, 10479, 295, 3097, 7150, 13, 50632], "temperature": 0.0, "avg_logprob": -0.07097917132907444, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0007605342543683946}, {"id": 452, "seek": 227420, "start": 2280.2, "end": 2287.24, "text": " One category is contrastive methods, and those consist in generating those flashing green dots", "tokens": [50664, 1485, 7719, 307, 8712, 488, 7150, 11, 293, 729, 4603, 294, 17746, 729, 31049, 3092, 15026, 51016], "temperature": 0.0, "avg_logprob": -0.07097917132907444, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0007605342543683946}, {"id": 453, "seek": 227420, "start": 2287.24, "end": 2293.0, "text": " here that are outside the manifold data, and then changing the parameters of the energy function", "tokens": [51016, 510, 300, 366, 2380, 264, 47138, 1412, 11, 293, 550, 4473, 264, 9834, 295, 264, 2281, 2445, 51304], "temperature": 0.0, "avg_logprob": -0.07097917132907444, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0007605342543683946}, {"id": 454, "seek": 227420, "start": 2293.0, "end": 2298.04, "text": " so that the energy takes low values on the data points and higher values on those contrastive", "tokens": [51304, 370, 300, 264, 2281, 2516, 2295, 4190, 322, 264, 1412, 2793, 293, 2946, 4190, 322, 729, 8712, 488, 51556], "temperature": 0.0, "avg_logprob": -0.07097917132907444, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.0007605342543683946}, {"id": 455, "seek": 229804, "start": 2298.92, "end": 2306.04, "text": " green points. I contributed to inventing those methods back in the early 90s, but I don't like", "tokens": [50408, 3092, 2793, 13, 286, 18434, 281, 7962, 278, 729, 7150, 646, 294, 264, 2440, 4289, 82, 11, 457, 286, 500, 380, 411, 50764], "temperature": 0.0, "avg_logprob": -0.11743905327536842, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.003875025548040867}, {"id": 456, "seek": 229804, "start": 2306.04, "end": 2310.2, "text": " them anymore, because in high-dimensional spaces, the number of contrastive points you have to", "tokens": [50764, 552, 3602, 11, 570, 294, 1090, 12, 18759, 7673, 11, 264, 1230, 295, 8712, 488, 2793, 291, 362, 281, 50972], "temperature": 0.0, "avg_logprob": -0.11743905327536842, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.003875025548040867}, {"id": 457, "seek": 229804, "start": 2310.2, "end": 2315.8, "text": " generate for the energy function to take the right shape grows exponentially, and that's not a good", "tokens": [50972, 8460, 337, 264, 2281, 2445, 281, 747, 264, 558, 3909, 13156, 37330, 11, 293, 300, 311, 406, 257, 665, 51252], "temperature": 0.0, "avg_logprob": -0.11743905327536842, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.003875025548040867}, {"id": 458, "seek": 229804, "start": 2315.8, "end": 2321.32, "text": " thing. I prefer another server approach, regularised methods, and there are those methods. I'm going", "tokens": [51252, 551, 13, 286, 4382, 1071, 7154, 3109, 11, 3890, 2640, 7150, 11, 293, 456, 366, 729, 7150, 13, 286, 478, 516, 51528], "temperature": 0.0, "avg_logprob": -0.11743905327536842, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.003875025548040867}, {"id": 459, "seek": 229804, "start": 2321.32, "end": 2326.7599999999998, "text": " to explain this with another slide. They basically consist in minimizing the volume of space that", "tokens": [51528, 281, 2903, 341, 365, 1071, 4137, 13, 814, 1936, 4603, 294, 46608, 264, 5523, 295, 1901, 300, 51800], "temperature": 0.0, "avg_logprob": -0.11743905327536842, "compression_ratio": 1.6655290102389078, "no_speech_prob": 0.003875025548040867}, {"id": 460, "seek": 232676, "start": 2326.76, "end": 2332.6000000000004, "text": " can take low energy through some sort of regulariser, for example, so that the system can give low", "tokens": [50364, 393, 747, 2295, 2281, 807, 512, 1333, 295, 3890, 6694, 11, 337, 1365, 11, 370, 300, 264, 1185, 393, 976, 2295, 50656], "temperature": 0.0, "avg_logprob": -0.09413974515853389, "compression_ratio": 1.8557692307692308, "no_speech_prob": 0.0004518362111411989}, {"id": 461, "seek": 232676, "start": 2332.6000000000004, "end": 2336.76, "text": " energy to the data points by changing the parameters of the energy function so that the", "tokens": [50656, 2281, 281, 264, 1412, 2793, 538, 4473, 264, 9834, 295, 264, 2281, 2445, 370, 300, 264, 50864], "temperature": 0.0, "avg_logprob": -0.09413974515853389, "compression_ratio": 1.8557692307692308, "no_speech_prob": 0.0004518362111411989}, {"id": 462, "seek": 232676, "start": 2336.76, "end": 2343.4, "text": " energy of the data points gets lower, but because it's regularised, it can only give low energy to", "tokens": [50864, 2281, 295, 264, 1412, 2793, 2170, 3126, 11, 457, 570, 309, 311, 3890, 2640, 11, 309, 393, 787, 976, 2295, 2281, 281, 51196], "temperature": 0.0, "avg_logprob": -0.09413974515853389, "compression_ratio": 1.8557692307692308, "no_speech_prob": 0.0004518362111411989}, {"id": 463, "seek": 232676, "start": 2343.4, "end": 2351.0, "text": " a small volume of space. The data points get kind of shrinkwrapped if you want in the sort of region", "tokens": [51196, 257, 1359, 5523, 295, 1901, 13, 440, 1412, 2793, 483, 733, 295, 23060, 86, 424, 3320, 498, 291, 528, 294, 264, 1333, 295, 4458, 51576], "temperature": 0.0, "avg_logprob": -0.09413974515853389, "compression_ratio": 1.8557692307692308, "no_speech_prob": 0.0004518362111411989}, {"id": 464, "seek": 235100, "start": 2351.0, "end": 2358.28, "text": " of low energies. So I prefer this. That seems to be more efficient, and the question is how we do", "tokens": [50364, 295, 2295, 25737, 13, 407, 286, 4382, 341, 13, 663, 2544, 281, 312, 544, 7148, 11, 293, 264, 1168, 307, 577, 321, 360, 50728], "temperature": 0.0, "avg_logprob": -0.12898906071980795, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.0064489999786019325}, {"id": 465, "seek": 235100, "start": 2358.28, "end": 2362.92, "text": " this, but what I'm asking you to do now is abandon generative models, the most popular thing at the", "tokens": [50728, 341, 11, 457, 437, 286, 478, 3365, 291, 281, 360, 586, 307, 9072, 1337, 1166, 5245, 11, 264, 881, 3743, 551, 412, 264, 50960], "temperature": 0.0, "avg_logprob": -0.12898906071980795, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.0064489999786019325}, {"id": 466, "seek": 235100, "start": 2362.92, "end": 2368.6, "text": " moment, abandon probabilistic models, the pillar of understanding machine learning, abandon contrastive", "tokens": [50960, 1623, 11, 9072, 31959, 3142, 5245, 11, 264, 27592, 295, 3701, 3479, 2539, 11, 9072, 8712, 488, 51244], "temperature": 0.0, "avg_logprob": -0.12898906071980795, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.0064489999786019325}, {"id": 467, "seek": 235100, "start": 2368.6, "end": 2373.8, "text": " methods, which also have been very popular, and also something I've been saying for a number of", "tokens": [51244, 7150, 11, 597, 611, 362, 668, 588, 3743, 11, 293, 611, 746, 286, 600, 668, 1566, 337, 257, 1230, 295, 51504], "temperature": 0.0, "avg_logprob": -0.12898906071980795, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.0064489999786019325}, {"id": 468, "seek": 235100, "start": 2373.8, "end": 2379.96, "text": " 10 years, abandon reinforcement learning, because it's so damn inefficient. So those are kind of", "tokens": [51504, 1266, 924, 11, 9072, 29280, 2539, 11, 570, 309, 311, 370, 8151, 43495, 13, 407, 729, 366, 733, 295, 51812], "temperature": 0.0, "avg_logprob": -0.12898906071980795, "compression_ratio": 1.7580071174377223, "no_speech_prob": 0.0064489999786019325}, {"id": 469, "seek": 237996, "start": 2379.96, "end": 2383.8, "text": " four of the most popular approaches to machine learning at the moment, and I'm telling people to", "tokens": [50364, 1451, 295, 264, 881, 3743, 11587, 281, 3479, 2539, 412, 264, 1623, 11, 293, 286, 478, 3585, 561, 281, 50556], "temperature": 0.0, "avg_logprob": -0.08297476561173149, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0013848254457116127}, {"id": 470, "seek": 237996, "start": 2384.76, "end": 2390.68, "text": " move away from them. You can imagine I'm not being very popular here, but I'm used to that.", "tokens": [50604, 1286, 1314, 490, 552, 13, 509, 393, 3811, 286, 478, 406, 885, 588, 3743, 510, 11, 457, 286, 478, 1143, 281, 300, 13, 50900], "temperature": 0.0, "avg_logprob": -0.08297476561173149, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0013848254457116127}, {"id": 471, "seek": 237996, "start": 2390.68, "end": 2395.7200000000003, "text": " So how do you prevent those systems from collapsing? What you can do is measure,", "tokens": [50900, 407, 577, 360, 291, 4871, 729, 3652, 490, 45339, 30, 708, 291, 393, 360, 307, 3481, 11, 51152], "temperature": 0.0, "avg_logprob": -0.08297476561173149, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0013848254457116127}, {"id": 472, "seek": 237996, "start": 2395.7200000000003, "end": 2400.92, "text": " have some sort of measure or information content of SX and SY across a batch, for example,", "tokens": [51152, 362, 512, 1333, 295, 3481, 420, 1589, 2701, 295, 318, 55, 293, 32624, 2108, 257, 15245, 11, 337, 1365, 11, 51412], "temperature": 0.0, "avg_logprob": -0.08297476561173149, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0013848254457116127}, {"id": 473, "seek": 237996, "start": 2401.56, "end": 2406.2, "text": " and then try to maximise that. Now, unfortunately, it's very hard to do because we don't have lower", "tokens": [51444, 293, 550, 853, 281, 5138, 908, 300, 13, 823, 11, 7015, 11, 309, 311, 588, 1152, 281, 360, 570, 321, 500, 380, 362, 3126, 51676], "temperature": 0.0, "avg_logprob": -0.08297476561173149, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0013848254457116127}, {"id": 474, "seek": 240620, "start": 2406.2, "end": 2412.8399999999997, "text": " bounds on information content. We only have upper bounds, but it turns out you can sort of do this.", "tokens": [50364, 29905, 322, 1589, 2701, 13, 492, 787, 362, 6597, 29905, 11, 457, 309, 4523, 484, 291, 393, 1333, 295, 360, 341, 13, 50696], "temperature": 0.0, "avg_logprob": -0.10602923362485824, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0025171188171952963}, {"id": 475, "seek": 240620, "start": 2412.8399999999997, "end": 2418.12, "text": " So one way to prevent SX from collapsing is that you can use a criterion, which is", "tokens": [50696, 407, 472, 636, 281, 4871, 318, 55, 490, 45339, 307, 300, 291, 393, 764, 257, 46691, 11, 597, 307, 50960], "temperature": 0.0, "avg_logprob": -0.10602923362485824, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0025171188171952963}, {"id": 476, "seek": 240620, "start": 2419.72, "end": 2425.64, "text": " attempt to make sure that the variance of every component of SX over a batch is at least one.", "tokens": [51040, 5217, 281, 652, 988, 300, 264, 21977, 295, 633, 6542, 295, 318, 55, 670, 257, 15245, 307, 412, 1935, 472, 13, 51336], "temperature": 0.0, "avg_logprob": -0.10602923362485824, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0025171188171952963}, {"id": 477, "seek": 240620, "start": 2425.64, "end": 2435.16, "text": " So that's the criterion you see in the second red box below the cover. That's a hinge loss", "tokens": [51336, 407, 300, 311, 264, 46691, 291, 536, 294, 264, 1150, 2182, 2424, 2507, 264, 2060, 13, 663, 311, 257, 28822, 4470, 51812], "temperature": 0.0, "avg_logprob": -0.10602923362485824, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0025171188171952963}, {"id": 478, "seek": 243516, "start": 2435.16, "end": 2441.0, "text": " that makes the standard deviation of each variable at least one. And then another term that makes sure", "tokens": [50364, 300, 1669, 264, 3832, 25163, 295, 1184, 7006, 412, 1935, 472, 13, 400, 550, 1071, 1433, 300, 1669, 988, 50656], "temperature": 0.0, "avg_logprob": -0.13014978325885276, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0007077425834722817}, {"id": 479, "seek": 243516, "start": 2441.0, "end": 2447.24, "text": " the components of SX are decorrelated. That's the covariance matrix term. So you're trying to minimise", "tokens": [50656, 264, 6677, 295, 318, 55, 366, 979, 284, 12004, 13, 663, 311, 264, 49851, 719, 8141, 1433, 13, 407, 291, 434, 1382, 281, 4464, 908, 50968], "temperature": 0.0, "avg_logprob": -0.13014978325885276, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0007077425834722817}, {"id": 480, "seek": 243516, "start": 2447.24, "end": 2451.96, "text": " the octagonal terms of the covariance matrix of the vectors SX over a batch.", "tokens": [50968, 264, 13350, 6709, 304, 2115, 295, 264, 49851, 719, 8141, 295, 264, 18875, 318, 55, 670, 257, 15245, 13, 51204], "temperature": 0.0, "avg_logprob": -0.13014978325885276, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0007077425834722817}, {"id": 481, "seek": 243516, "start": 2453.8799999999997, "end": 2458.12, "text": " That's not actually sufficient. So you can also use an expander. I don't have time to explain", "tokens": [51300, 663, 311, 406, 767, 11563, 13, 407, 291, 393, 611, 764, 364, 1278, 4483, 13, 286, 500, 380, 362, 565, 281, 2903, 51512], "temperature": 0.0, "avg_logprob": -0.13014978325885276, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0007077425834722817}, {"id": 482, "seek": 243516, "start": 2458.12, "end": 2462.7599999999998, "text": " why that works, but the resulting method is called Vicreg variance, invariance covariance", "tokens": [51512, 983, 300, 1985, 11, 457, 264, 16505, 3170, 307, 1219, 33316, 3375, 21977, 11, 33270, 719, 49851, 719, 51744], "temperature": 0.0, "avg_logprob": -0.13014978325885276, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0007077425834722817}, {"id": 483, "seek": 246276, "start": 2462.76, "end": 2468.1200000000003, "text": " regularisation. And it's a pretty general method that can be applied to a lot of situations for", "tokens": [50364, 3890, 7623, 13, 400, 309, 311, 257, 1238, 2674, 3170, 300, 393, 312, 6456, 281, 257, 688, 295, 6851, 337, 50632], "temperature": 0.0, "avg_logprob": -0.1349148114522298, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.0011067923624068499}, {"id": 484, "seek": 246276, "start": 2468.1200000000003, "end": 2473.6400000000003, "text": " those joint embedding predictive architectures for various applications in image recognition,", "tokens": [50632, 729, 7225, 12240, 3584, 35521, 6331, 1303, 337, 3683, 5821, 294, 3256, 11150, 11, 50908], "temperature": 0.0, "avg_logprob": -0.1349148114522298, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.0011067923624068499}, {"id": 485, "seek": 246276, "start": 2473.6400000000003, "end": 2479.7200000000003, "text": " segmentation, etc. It's pretty similar to another method called MCR squared invented by EMA at its", "tokens": [50908, 9469, 399, 11, 5183, 13, 467, 311, 1238, 2531, 281, 1071, 3170, 1219, 8797, 49, 8889, 14479, 538, 462, 9998, 412, 1080, 51212], "temperature": 0.0, "avg_logprob": -0.1349148114522298, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.0011067923624068499}, {"id": 486, "seek": 246276, "start": 2479.7200000000003, "end": 2488.36, "text": " group at Berkeley. And it works really well. I'm not going to bore you with details, but there is a", "tokens": [51212, 1594, 412, 23684, 13, 400, 309, 1985, 534, 731, 13, 286, 478, 406, 516, 281, 26002, 291, 365, 4365, 11, 457, 456, 307, 257, 51644], "temperature": 0.0, "avg_logprob": -0.1349148114522298, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.0011067923624068499}, {"id": 487, "seek": 248836, "start": 2488.36, "end": 2493.4, "text": " standard scenario in which you do use sub-supervised learning where you pre-train a convolutional net,", "tokens": [50364, 3832, 9005, 294, 597, 291, 360, 764, 1422, 12, 48172, 24420, 2539, 689, 291, 659, 12, 83, 7146, 257, 45216, 304, 2533, 11, 50616], "temperature": 0.0, "avg_logprob": -0.1922041240491365, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.007880683057010174}, {"id": 488, "seek": 248836, "start": 2493.4, "end": 2499.7200000000003, "text": " let's say, using this method. And then you chop off the expander, stick a linear classifier, which", "tokens": [50616, 718, 311, 584, 11, 1228, 341, 3170, 13, 400, 550, 291, 7931, 766, 264, 1278, 4483, 11, 2897, 257, 8213, 1508, 9902, 11, 597, 50932], "temperature": 0.0, "avg_logprob": -0.1922041240491365, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.007880683057010174}, {"id": 489, "seek": 248836, "start": 2499.7200000000003, "end": 2503.88, "text": " you train supervised and you measure the performance. And you get really good performance on image net", "tokens": [50932, 291, 3847, 46533, 293, 291, 3481, 264, 3389, 13, 400, 291, 483, 534, 665, 3389, 322, 3256, 2533, 51140], "temperature": 0.0, "avg_logprob": -0.1922041240491365, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.007880683057010174}, {"id": 490, "seek": 248836, "start": 2503.88, "end": 2510.6800000000003, "text": " this way, particularly good performance for out-of-distribution transfer learning. There's", "tokens": [51140, 341, 636, 11, 4098, 665, 3389, 337, 484, 12, 2670, 12, 42649, 30783, 5003, 2539, 13, 821, 311, 51480], "temperature": 0.0, "avg_logprob": -0.1922041240491365, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.007880683057010174}, {"id": 491, "seek": 248836, "start": 2510.6800000000003, "end": 2517.32, "text": " a modification of this method called Vicreg L, which was published last year, which is more", "tokens": [51480, 257, 26747, 295, 341, 3170, 1219, 33316, 3375, 441, 11, 597, 390, 6572, 1036, 1064, 11, 597, 307, 544, 51812], "temperature": 0.0, "avg_logprob": -0.1922041240491365, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.007880683057010174}, {"id": 492, "seek": 251732, "start": 2517.32, "end": 2523.88, "text": " tuned for segmentation and things like this, but I don't have time to go into details. There's a new", "tokens": [50364, 10870, 337, 9469, 399, 293, 721, 411, 341, 11, 457, 286, 500, 380, 362, 565, 281, 352, 666, 4365, 13, 821, 311, 257, 777, 50692], "temperature": 0.0, "avg_logprob": -0.11754177712105415, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.0008646086207590997}, {"id": 493, "seek": 251732, "start": 2523.88, "end": 2531.6400000000003, "text": " method that we rolled out at CVPR just a few weeks ago called Image JEPA that uses masking and a", "tokens": [50692, 3170, 300, 321, 14306, 484, 412, 22995, 15958, 445, 257, 1326, 3259, 2057, 1219, 29903, 508, 8929, 32, 300, 4960, 31226, 293, 257, 51080], "temperature": 0.0, "avg_logprob": -0.11754177712105415, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.0008646086207590997}, {"id": 494, "seek": 251732, "start": 2531.6400000000003, "end": 2538.04, "text": " transformer architecture for learning features in images. And so the collapse prevention method", "tokens": [51080, 31782, 9482, 337, 2539, 4122, 294, 5267, 13, 400, 370, 264, 15584, 14630, 3170, 51400], "temperature": 0.0, "avg_logprob": -0.11754177712105415, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.0008646086207590997}, {"id": 495, "seek": 251732, "start": 2538.04, "end": 2541.7200000000003, "text": " there is different, but the advantage of this method is that it does not require any data", "tokens": [51400, 456, 307, 819, 11, 457, 264, 5002, 295, 341, 3170, 307, 300, 309, 775, 406, 3651, 604, 1412, 51584], "temperature": 0.0, "avg_logprob": -0.11754177712105415, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.0008646086207590997}, {"id": 496, "seek": 251732, "start": 2541.7200000000003, "end": 2546.44, "text": " augmentation other than masking. So it doesn't require to know really what type of data you're", "tokens": [51584, 14501, 19631, 661, 813, 31226, 13, 407, 309, 1177, 380, 3651, 281, 458, 534, 437, 2010, 295, 1412, 291, 434, 51820], "temperature": 0.0, "avg_logprob": -0.11754177712105415, "compression_ratio": 1.6597222222222223, "no_speech_prob": 0.0008646086207590997}, {"id": 497, "seek": 254644, "start": 2546.44, "end": 2551.96, "text": " manipulating. And it's incredibly fast and it works really well. It gives amazing results", "tokens": [50364, 40805, 13, 400, 309, 311, 6252, 2370, 293, 309, 1985, 534, 731, 13, 467, 2709, 2243, 3542, 50640], "temperature": 0.0, "avg_logprob": -0.1745269609534222, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0015668412670493126}, {"id": 498, "seek": 254644, "start": 2553.0, "end": 2558.44, "text": " for really, really good features. There's another set of method by some of our colleagues here at", "tokens": [50692, 337, 534, 11, 534, 665, 4122, 13, 821, 311, 1071, 992, 295, 3170, 538, 512, 295, 527, 7734, 510, 412, 50964], "temperature": 0.0, "avg_logprob": -0.1745269609534222, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0015668412670493126}, {"id": 499, "seek": 254644, "start": 2558.44, "end": 2566.2000000000003, "text": " Fair Paris called Dino, the INO. It's a different way of preventing collapse, but it has some", "tokens": [50964, 12157, 8380, 1219, 413, 2982, 11, 264, 6892, 46, 13, 467, 311, 257, 819, 636, 295, 19965, 15584, 11, 457, 309, 575, 512, 51352], "temperature": 0.0, "avg_logprob": -0.1745269609534222, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0015668412670493126}, {"id": 500, "seek": 254644, "start": 2566.2000000000003, "end": 2571.64, "text": " commonalities with IJPA. And it works really well. It gives you something like above 80% on", "tokens": [51352, 2689, 16110, 365, 286, 41, 10297, 13, 400, 309, 1985, 534, 731, 13, 467, 2709, 291, 746, 411, 3673, 4688, 4, 322, 51624], "temperature": 0.0, "avg_logprob": -0.1745269609534222, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0015668412670493126}, {"id": 501, "seek": 257164, "start": 2571.64, "end": 2576.6, "text": " ImageNet, purely supervised with no fine tuning and without any data augmentation, which is pretty", "tokens": [50364, 29903, 31890, 11, 17491, 46533, 365, 572, 2489, 15164, 293, 1553, 604, 1412, 14501, 19631, 11, 597, 307, 1238, 50612], "temperature": 0.0, "avg_logprob": -0.17006492614746094, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.0012538385344669223}, {"id": 502, "seek": 257164, "start": 2576.6, "end": 2584.52, "text": " amazing. But ultimately what we want to do is use this self-supervised learning and this", "tokens": [50612, 2243, 13, 583, 6284, 437, 321, 528, 281, 360, 307, 764, 341, 2698, 12, 48172, 24420, 2539, 293, 341, 51008], "temperature": 0.0, "avg_logprob": -0.17006492614746094, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.0012538385344669223}, {"id": 503, "seek": 257164, "start": 2584.52, "end": 2590.3599999999997, "text": " JEPA architecture to build the systems of the type that I talked to you about earlier that are", "tokens": [51008, 508, 8929, 32, 9482, 281, 1322, 264, 3652, 295, 264, 2010, 300, 286, 2825, 281, 291, 466, 3071, 300, 366, 51300], "temperature": 0.0, "avg_logprob": -0.17006492614746094, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.0012538385344669223}, {"id": 504, "seek": 257164, "start": 2590.3599999999997, "end": 2595.16, "text": " hierarchical. They can predict what's going to happen in the world, perhaps as a result of an", "tokens": [51300, 35250, 804, 13, 814, 393, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 11, 4317, 382, 257, 1874, 295, 364, 51540], "temperature": 0.0, "avg_logprob": -0.17006492614746094, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.0012538385344669223}, {"id": 505, "seek": 259516, "start": 2595.16, "end": 2602.6, "text": " action, with some early results on training systems from video to learn good representations of", "tokens": [50364, 3069, 11, 365, 512, 2440, 3542, 322, 3097, 3652, 490, 960, 281, 1466, 665, 33358, 295, 50736], "temperature": 0.0, "avg_logprob": -0.17986232042312622, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.015922829508781433}, {"id": 506, "seek": 259516, "start": 2602.6, "end": 2609.48, "text": " images and videos by being trained on successive frames from a video and distorted images.", "tokens": [50736, 5267, 293, 2145, 538, 885, 8895, 322, 48043, 12083, 490, 257, 960, 293, 33431, 5267, 13, 51080], "temperature": 0.0, "avg_logprob": -0.17986232042312622, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.015922829508781433}, {"id": 507, "seek": 259516, "start": 2609.48, "end": 2613.16, "text": " I don't have time to go into the details of how this works. It's called NCJEPA.", "tokens": [51080, 286, 500, 380, 362, 565, 281, 352, 666, 264, 4365, 295, 577, 341, 1985, 13, 467, 311, 1219, 20786, 41, 8929, 32, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17986232042312622, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.015922829508781433}, {"id": 508, "seek": 259516, "start": 2616.8399999999997, "end": 2623.3199999999997, "text": " And it is trained basically to extract good features from images for object recognition,", "tokens": [51448, 400, 309, 307, 8895, 1936, 281, 8947, 665, 4122, 490, 5267, 337, 2657, 11150, 11, 51772], "temperature": 0.0, "avg_logprob": -0.17986232042312622, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.015922829508781433}, {"id": 509, "seek": 262332, "start": 2623.32, "end": 2627.96, "text": " but also to estimate motion in a video. And it does a pretty good job at this.", "tokens": [50364, 457, 611, 281, 12539, 5394, 294, 257, 960, 13, 400, 309, 775, 257, 1238, 665, 1691, 412, 341, 13, 50596], "temperature": 0.0, "avg_logprob": -0.12114036844131794, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0020381116773933172}, {"id": 510, "seek": 262332, "start": 2629.2400000000002, "end": 2637.6400000000003, "text": " So watch this paper on archive that you're invited to look at. So objective-driven AI is this idea", "tokens": [50660, 407, 1159, 341, 3035, 322, 23507, 300, 291, 434, 9185, 281, 574, 412, 13, 407, 10024, 12, 25456, 7318, 307, 341, 1558, 51080], "temperature": 0.0, "avg_logprob": -0.12114036844131794, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0020381116773933172}, {"id": 511, "seek": 262332, "start": 2637.6400000000003, "end": 2643.1600000000003, "text": " that we're going to have objectives that are going to drive the behavior of our system and it's going", "tokens": [51080, 300, 321, 434, 516, 281, 362, 15961, 300, 366, 516, 281, 3332, 264, 5223, 295, 527, 1185, 293, 309, 311, 516, 51356], "temperature": 0.0, "avg_logprob": -0.12114036844131794, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0020381116773933172}, {"id": 512, "seek": 262332, "start": 2643.1600000000003, "end": 2649.8, "text": " to make it terrible and safe. And there are things that we're working on to get this to work,", "tokens": [51356, 281, 652, 309, 6237, 293, 3273, 13, 400, 456, 366, 721, 300, 321, 434, 1364, 322, 281, 483, 341, 281, 589, 11, 51688], "temperature": 0.0, "avg_logprob": -0.12114036844131794, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.0020381116773933172}, {"id": 513, "seek": 264980, "start": 2649.8, "end": 2653.8, "text": " self-supervised learning from video, that recipe that really works for everything. So we're working", "tokens": [50364, 2698, 12, 48172, 24420, 2539, 490, 960, 11, 300, 6782, 300, 534, 1985, 337, 1203, 13, 407, 321, 434, 1364, 50564], "temperature": 0.0, "avg_logprob": -0.1192112217778745, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.0071558235213160515}, {"id": 514, "seek": 264980, "start": 2653.8, "end": 2659.96, "text": " with those JEPA architectures, but we don't think we have the ultimate recipe yet. We can use this", "tokens": [50564, 365, 729, 508, 8929, 32, 6331, 1303, 11, 457, 321, 500, 380, 519, 321, 362, 264, 9705, 6782, 1939, 13, 492, 393, 764, 341, 50872], "temperature": 0.0, "avg_logprob": -0.1192112217778745, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.0071558235213160515}, {"id": 515, "seek": 264980, "start": 2659.96, "end": 2665.0, "text": " to build LLMs that can reason and plan, that are driven by objectives, perhaps hopefully,", "tokens": [50872, 281, 1322, 441, 43, 26386, 300, 393, 1778, 293, 1393, 11, 300, 366, 9555, 538, 15961, 11, 4317, 4696, 11, 51124], "temperature": 0.0, "avg_logprob": -0.1192112217778745, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.0071558235213160515}, {"id": 516, "seek": 264980, "start": 2666.04, "end": 2672.04, "text": " learning systems that can do hierarchical planning, like animals and humans, many animals and humans.", "tokens": [51176, 2539, 3652, 300, 393, 360, 35250, 804, 5038, 11, 411, 4882, 293, 6255, 11, 867, 4882, 293, 6255, 13, 51476], "temperature": 0.0, "avg_logprob": -0.1192112217778745, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.0071558235213160515}, {"id": 517, "seek": 264980, "start": 2672.04, "end": 2677.7200000000003, "text": " We have many problems to solve. JEPAs with regularized written variables to deal with uncertainty,", "tokens": [51476, 492, 362, 867, 2740, 281, 5039, 13, 508, 8929, 10884, 365, 3890, 1602, 3720, 9102, 281, 2028, 365, 15697, 11, 51760], "temperature": 0.0, "avg_logprob": -0.1192112217778745, "compression_ratio": 1.6804123711340206, "no_speech_prob": 0.0071558235213160515}, {"id": 518, "seek": 267772, "start": 2677.7999999999997, "end": 2681.9599999999996, "text": " planning algorithms in the presence of uncertainty, learning cost modules,", "tokens": [50368, 5038, 14642, 294, 264, 6814, 295, 15697, 11, 2539, 2063, 16679, 11, 50576], "temperature": 0.0, "avg_logprob": -0.14529517037527903, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.0006944441702216864}, {"id": 519, "seek": 267772, "start": 2682.6, "end": 2687.0, "text": " which could be assimilated with inverse reinforcement learning, planning with", "tokens": [50608, 597, 727, 312, 8249, 45678, 365, 17340, 29280, 2539, 11, 5038, 365, 50828], "temperature": 0.0, "avg_logprob": -0.14529517037527903, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.0006944441702216864}, {"id": 520, "seek": 267772, "start": 2687.0, "end": 2692.68, "text": " inaccurate world models, and then exploration techniques to adjust the world model in case it's", "tokens": [50828, 46443, 1002, 5245, 11, 293, 550, 16197, 7512, 281, 4369, 264, 1002, 2316, 294, 1389, 309, 311, 51112], "temperature": 0.0, "avg_logprob": -0.14529517037527903, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.0006944441702216864}, {"id": 521, "seek": 267772, "start": 2692.68, "end": 2704.2, "text": " not completely accurate. Okay, so I'm sort of concluding. There is a computing limitation", "tokens": [51112, 406, 2584, 8559, 13, 1033, 11, 370, 286, 478, 1333, 295, 9312, 278, 13, 821, 307, 257, 15866, 27432, 51688], "temperature": 0.0, "avg_logprob": -0.14529517037527903, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.0006944441702216864}, {"id": 522, "seek": 270420, "start": 2704.2, "end": 2711.56, "text": " of autoregressive LLM, which is that they can only allocate a finite and fixed amount of", "tokens": [50364, 295, 1476, 418, 3091, 488, 441, 43, 44, 11, 597, 307, 300, 436, 393, 787, 35713, 257, 19362, 293, 6806, 2372, 295, 50732], "temperature": 0.0, "avg_logprob": -0.14764824578928393, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.002034144476056099}, {"id": 523, "seek": 270420, "start": 2711.56, "end": 2717.56, "text": " computational resources to producing single token. You run through, you know, 48 layers", "tokens": [50732, 28270, 3593, 281, 10501, 2167, 14862, 13, 509, 1190, 807, 11, 291, 458, 11, 11174, 7914, 51032], "temperature": 0.0, "avg_logprob": -0.14764824578928393, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.002034144476056099}, {"id": 524, "seek": 270420, "start": 2717.56, "end": 2721.56, "text": " of a transformer or something like that, you produce one token and then 48 more layers and", "tokens": [51032, 295, 257, 31782, 420, 746, 411, 300, 11, 291, 5258, 472, 14862, 293, 550, 11174, 544, 7914, 293, 51232], "temperature": 0.0, "avg_logprob": -0.14764824578928393, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.002034144476056099}, {"id": 525, "seek": 270420, "start": 2721.56, "end": 2730.4399999999996, "text": " produce one token. And this is not too incomplete. Whereas the method I'm suggesting, the architecture", "tokens": [51232, 5258, 472, 14862, 13, 400, 341, 307, 406, 886, 31709, 13, 13813, 264, 3170, 286, 478, 18094, 11, 264, 9482, 51676], "temperature": 0.0, "avg_logprob": -0.14764824578928393, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.002034144476056099}, {"id": 526, "seek": 273044, "start": 2730.44, "end": 2735.88, "text": " I'm suggesting that can produce an output by planning through energy minimization,", "tokens": [50364, 286, 478, 18094, 300, 393, 5258, 364, 5598, 538, 5038, 807, 2281, 4464, 2144, 11, 50636], "temperature": 0.0, "avg_logprob": -0.18585708043346666, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0010292634833604097}, {"id": 527, "seek": 273044, "start": 2736.76, "end": 2741.48, "text": " that is too incomplete, because everything can be reduced to optimization, basically.", "tokens": [50680, 300, 307, 886, 31709, 11, 570, 1203, 393, 312, 9212, 281, 19618, 11, 1936, 13, 50916], "temperature": 0.0, "avg_logprob": -0.18585708043346666, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0010292634833604097}, {"id": 528, "seek": 273044, "start": 2742.6, "end": 2746.68, "text": " We're still missing essential concepts to reach human ability AI, you know, this", "tokens": [50972, 492, 434, 920, 5361, 7115, 10392, 281, 2524, 1952, 3485, 7318, 11, 291, 458, 11, 341, 51176], "temperature": 0.0, "avg_logprob": -0.18585708043346666, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0010292634833604097}, {"id": 529, "seek": 273044, "start": 2749.7200000000003, "end": 2757.64, "text": " potential technique for planning and reasoning, you know, basic techniques that we're missing to", "tokens": [51328, 3995, 6532, 337, 5038, 293, 21577, 11, 291, 458, 11, 3875, 7512, 300, 321, 434, 5361, 281, 51724], "temperature": 0.0, "avg_logprob": -0.18585708043346666, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0010292634833604097}, {"id": 530, "seek": 275764, "start": 2757.64, "end": 2765.3199999999997, "text": " learn world models from complex modalities like video. And perhaps in the future, we'll be able to", "tokens": [50364, 1466, 1002, 5245, 490, 3997, 1072, 16110, 411, 960, 13, 400, 4317, 294, 264, 2027, 11, 321, 603, 312, 1075, 281, 50748], "temperature": 0.0, "avg_logprob": -0.11817126501174201, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.002068900503218174}, {"id": 531, "seek": 275764, "start": 2765.3199999999997, "end": 2771.48, "text": " build systems that can plan their answers to satisfy objectives and have guardrails. I don't", "tokens": [50748, 1322, 3652, 300, 393, 1393, 641, 6338, 281, 19319, 15961, 293, 362, 6290, 424, 4174, 13, 286, 500, 380, 51056], "temperature": 0.0, "avg_logprob": -0.11817126501174201, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.002068900503218174}, {"id": 532, "seek": 275764, "start": 2771.48, "end": 2775.56, "text": " believe there is such a concept as artificial general intelligence, because I think even human", "tokens": [51056, 1697, 456, 307, 1270, 257, 3410, 382, 11677, 2674, 7599, 11, 570, 286, 519, 754, 1952, 51260], "temperature": 0.0, "avg_logprob": -0.11817126501174201, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.002068900503218174}, {"id": 533, "seek": 275764, "start": 2775.56, "end": 2780.2799999999997, "text": " intelligence is very specialized. So let's forget about general intelligence, let's try to get to", "tokens": [51260, 7599, 307, 588, 19813, 13, 407, 718, 311, 2870, 466, 2674, 7599, 11, 718, 311, 853, 281, 483, 281, 51496], "temperature": 0.0, "avg_logprob": -0.11817126501174201, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.002068900503218174}, {"id": 534, "seek": 275764, "start": 2780.2799999999997, "end": 2785.7999999999997, "text": " human level intelligence, perhaps, perhaps build machines that have the same sort of set of skills", "tokens": [51496, 1952, 1496, 7599, 11, 4317, 11, 4317, 1322, 8379, 300, 362, 264, 912, 1333, 295, 992, 295, 3942, 51772], "temperature": 0.0, "avg_logprob": -0.11817126501174201, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.002068900503218174}, {"id": 535, "seek": 278580, "start": 2785.88, "end": 2790.52, "text": " and ability to learn new skills and humans. But we are very specialized. In fact, we know this", "tokens": [50368, 293, 3485, 281, 1466, 777, 3942, 293, 6255, 13, 583, 321, 366, 588, 19813, 13, 682, 1186, 11, 321, 458, 341, 50600], "temperature": 0.0, "avg_logprob": -0.09927604415199974, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.002166681457310915}, {"id": 536, "seek": 278580, "start": 2790.52, "end": 2793.96, "text": " because computers are much better than us at many tasks, which means we are specialized.", "tokens": [50600, 570, 10807, 366, 709, 1101, 813, 505, 412, 867, 9608, 11, 597, 1355, 321, 366, 19813, 13, 50772], "temperature": 0.0, "avg_logprob": -0.09927604415199974, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.002166681457310915}, {"id": 537, "seek": 278580, "start": 2794.92, "end": 2798.6800000000003, "text": " There's no question that sometimes in the future, machines will surpass human", "tokens": [50820, 821, 311, 572, 1168, 300, 2171, 294, 264, 2027, 11, 8379, 486, 27650, 1952, 51008], "temperature": 0.0, "avg_logprob": -0.09927604415199974, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.002166681457310915}, {"id": 538, "seek": 278580, "start": 2798.6800000000003, "end": 2803.0800000000004, "text": " intelligence in all domains where humans are intelligent. You know, how long is it going to", "tokens": [51008, 7599, 294, 439, 25514, 689, 6255, 366, 13232, 13, 509, 458, 11, 577, 938, 307, 309, 516, 281, 51228], "temperature": 0.0, "avg_logprob": -0.09927604415199974, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.002166681457310915}, {"id": 539, "seek": 278580, "start": 2803.0800000000004, "end": 2807.8, "text": " take? I don't know, but there's no question is going to happen. We don't need, we don't,", "tokens": [51228, 747, 30, 286, 500, 380, 458, 11, 457, 456, 311, 572, 1168, 307, 516, 281, 1051, 13, 492, 500, 380, 643, 11, 321, 500, 380, 11, 51464], "temperature": 0.0, "avg_logprob": -0.09927604415199974, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.002166681457310915}, {"id": 540, "seek": 278580, "start": 2809.0, "end": 2814.04, "text": " we probably don't want to be threatened by that. It would be a future where every one of us would", "tokens": [51524, 321, 1391, 500, 380, 528, 281, 312, 18268, 538, 300, 13, 467, 576, 312, 257, 2027, 689, 633, 472, 295, 505, 576, 51776], "temperature": 0.0, "avg_logprob": -0.09927604415199974, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.002166681457310915}, {"id": 541, "seek": 281404, "start": 2814.04, "end": 2820.12, "text": " be assisted by a system that is more intelligent than us. And we're familiar with that concept", "tokens": [50364, 312, 30291, 538, 257, 1185, 300, 307, 544, 13232, 813, 505, 13, 400, 321, 434, 4963, 365, 300, 3410, 50668], "temperature": 0.0, "avg_logprob": -0.10933517197431145, "compression_ratio": 1.9705882352941178, "no_speech_prob": 0.0013755463296547532}, {"id": 542, "seek": 281404, "start": 2820.12, "end": 2824.7599999999998, "text": " with other humans. I only work with people who are smarter than me, or at least I try.", "tokens": [50668, 365, 661, 6255, 13, 286, 787, 589, 365, 561, 567, 366, 20294, 813, 385, 11, 420, 412, 1935, 286, 853, 13, 50900], "temperature": 0.0, "avg_logprob": -0.10933517197431145, "compression_ratio": 1.9705882352941178, "no_speech_prob": 0.0013755463296547532}, {"id": 543, "seek": 281404, "start": 2826.68, "end": 2830.36, "text": " Or if they're not smarter than me, I try to make them smarter than me, they're called students.", "tokens": [50996, 1610, 498, 436, 434, 406, 20294, 813, 385, 11, 286, 853, 281, 652, 552, 20294, 813, 385, 11, 436, 434, 1219, 1731, 13, 51180], "temperature": 0.0, "avg_logprob": -0.10933517197431145, "compression_ratio": 1.9705882352941178, "no_speech_prob": 0.0013755463296547532}, {"id": 544, "seek": 281404, "start": 2831.16, "end": 2836.68, "text": " And, you know, so we're familiar with that concept. We shouldn't feel threatened by machines", "tokens": [51220, 400, 11, 291, 458, 11, 370, 321, 434, 4963, 365, 300, 3410, 13, 492, 4659, 380, 841, 18268, 538, 8379, 51496], "temperature": 0.0, "avg_logprob": -0.10933517197431145, "compression_ratio": 1.9705882352941178, "no_speech_prob": 0.0013755463296547532}, {"id": 545, "seek": 281404, "start": 2836.68, "end": 2841.32, "text": " that are smarter than us. We are in control of them and we still will still be in control of them.", "tokens": [51496, 300, 366, 20294, 813, 505, 13, 492, 366, 294, 1969, 295, 552, 293, 321, 920, 486, 920, 312, 294, 1969, 295, 552, 13, 51728], "temperature": 0.0, "avg_logprob": -0.10933517197431145, "compression_ratio": 1.9705882352941178, "no_speech_prob": 0.0013755463296547532}, {"id": 546, "seek": 284132, "start": 2841.96, "end": 2846.36, "text": " They won't escape our control any more than our neural cortex and escape the control of our", "tokens": [50396, 814, 1582, 380, 7615, 527, 1969, 604, 544, 813, 527, 18161, 33312, 293, 7615, 264, 1969, 295, 527, 50616], "temperature": 0.0, "avg_logprob": -0.21353156308093704, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.001473360345698893}, {"id": 547, "seek": 284132, "start": 2846.36, "end": 2853.0800000000004, "text": " visual memory, basically, in our brains. Thank you very much. I'll stop here and", "tokens": [50616, 5056, 4675, 11, 1936, 11, 294, 527, 15442, 13, 1044, 291, 588, 709, 13, 286, 603, 1590, 510, 293, 50952], "temperature": 0.0, "avg_logprob": -0.21353156308093704, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.001473360345698893}, {"id": 548, "seek": 284132, "start": 2853.7200000000003, "end": 2856.1200000000003, "text": " perhaps if we have time for questions, I'll take questions.", "tokens": [50984, 4317, 498, 321, 362, 565, 337, 1651, 11, 286, 603, 747, 1651, 13, 51104], "temperature": 0.0, "avg_logprob": -0.21353156308093704, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.001473360345698893}, {"id": 549, "seek": 284132, "start": 2862.52, "end": 2868.28, "text": " Thank you very much. If anyone has questions, we have two mics up there. Feel free to line up.", "tokens": [51424, 1044, 291, 588, 709, 13, 759, 2878, 575, 1651, 11, 321, 362, 732, 45481, 493, 456, 13, 14113, 1737, 281, 1622, 493, 13, 51712], "temperature": 0.0, "avg_logprob": -0.21353156308093704, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.001473360345698893}, {"id": 550, "seek": 286828, "start": 2869.1600000000003, "end": 2875.88, "text": " I guess I'll get us started with one question. So on that last slide, you mentioned the possibility,", "tokens": [50408, 286, 2041, 286, 603, 483, 505, 1409, 365, 472, 1168, 13, 407, 322, 300, 1036, 4137, 11, 291, 2835, 264, 7959, 11, 50744], "temperature": 0.0, "avg_logprob": -0.19181692189183727, "compression_ratio": 1.6569037656903767, "no_speech_prob": 0.0011479479726403952}, {"id": 551, "seek": 286828, "start": 2875.88, "end": 2881.0800000000004, "text": " or not the possibility, just the prediction that machines will become more intelligent than humans,", "tokens": [50744, 420, 406, 264, 7959, 11, 445, 264, 17630, 300, 8379, 486, 1813, 544, 13232, 813, 6255, 11, 51004], "temperature": 0.0, "avg_logprob": -0.19181692189183727, "compression_ratio": 1.6569037656903767, "no_speech_prob": 0.0011479479726403952}, {"id": 552, "seek": 286828, "start": 2881.0800000000004, "end": 2886.52, "text": " in all respects. And you also mentioned throughout your talk, these algorithms that can sort of", "tokens": [51004, 294, 439, 24126, 13, 400, 291, 611, 2835, 3710, 428, 751, 11, 613, 14642, 300, 393, 1333, 295, 51276], "temperature": 0.0, "avg_logprob": -0.19181692189183727, "compression_ratio": 1.6569037656903767, "no_speech_prob": 0.0011479479726403952}, {"id": 553, "seek": 286828, "start": 2886.52, "end": 2893.48, "text": " reason and plan. Could you imagine in the near future an algorithm that, for example, could propose", "tokens": [51276, 1778, 293, 1393, 13, 7497, 291, 3811, 294, 264, 2651, 2027, 364, 9284, 300, 11, 337, 1365, 11, 727, 17421, 51624], "temperature": 0.0, "avg_logprob": -0.19181692189183727, "compression_ratio": 1.6569037656903767, "no_speech_prob": 0.0011479479726403952}, {"id": 554, "seek": 289348, "start": 2894.04, "end": 2900.68, "text": " physics experiments for us to conduct, like plan an experiment to answer a question that we ask it?", "tokens": [50392, 10649, 12050, 337, 505, 281, 6018, 11, 411, 1393, 364, 5120, 281, 1867, 257, 1168, 300, 321, 1029, 309, 30, 50724], "temperature": 0.0, "avg_logprob": -0.16652053791088062, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0029730156529694796}, {"id": 555, "seek": 289348, "start": 2902.04, "end": 2908.28, "text": " Yeah, actually, there's an entire field which precedes AI called experimental design.", "tokens": [50792, 865, 11, 767, 11, 456, 311, 364, 2302, 2519, 597, 16969, 279, 7318, 1219, 17069, 1715, 13, 51104], "temperature": 0.0, "avg_logprob": -0.16652053791088062, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0029730156529694796}, {"id": 556, "seek": 289348, "start": 2910.04, "end": 2914.2, "text": " And I mean, I think to some extent that can be formulated as an optimization problem, as a", "tokens": [51192, 400, 286, 914, 11, 286, 519, 281, 512, 8396, 300, 393, 312, 48936, 382, 364, 19618, 1154, 11, 382, 257, 51400], "temperature": 0.0, "avg_logprob": -0.16652053791088062, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0029730156529694796}, {"id": 557, "seek": 289348, "start": 2914.2, "end": 2919.64, "text": " planning problem, or as a search problem, right, trying to figure out, like, you know, how do you", "tokens": [51400, 5038, 1154, 11, 420, 382, 257, 3164, 1154, 11, 558, 11, 1382, 281, 2573, 484, 11, 411, 11, 291, 458, 11, 577, 360, 291, 51672], "temperature": 0.0, "avg_logprob": -0.16652053791088062, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0029730156529694796}, {"id": 558, "seek": 291964, "start": 2919.64, "end": 2923.72, "text": " maximally get information from an experiment? Like, how do you design experiments? You get the", "tokens": [50364, 5138, 379, 483, 1589, 490, 364, 5120, 30, 1743, 11, 577, 360, 291, 1715, 12050, 30, 509, 483, 264, 50568], "temperature": 0.0, "avg_logprob": -0.10558906901966442, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0022478506434708834}, {"id": 559, "seek": 291964, "start": 2923.72, "end": 2929.3199999999997, "text": " maximum amount of information from it to either validate or invalidate a particular model that", "tokens": [50568, 6674, 2372, 295, 1589, 490, 309, 281, 2139, 29562, 420, 34702, 473, 257, 1729, 2316, 300, 50848], "temperature": 0.0, "avg_logprob": -0.10558906901966442, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0022478506434708834}, {"id": 560, "seek": 291964, "start": 2929.3199999999997, "end": 2934.6, "text": " you have, or a hypothesis you have in your mind. I think that's entirely automatable.", "tokens": [50848, 291, 362, 11, 420, 257, 17291, 291, 362, 294, 428, 1575, 13, 286, 519, 300, 311, 7696, 28034, 712, 13, 51112], "temperature": 0.0, "avg_logprob": -0.10558906901966442, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0022478506434708834}, {"id": 561, "seek": 291964, "start": 2936.6, "end": 2942.68, "text": " Now, if you want to use a generic AI system to do this, my guess is that it's not going to happen", "tokens": [51212, 823, 11, 498, 291, 528, 281, 764, 257, 19577, 7318, 1185, 281, 360, 341, 11, 452, 2041, 307, 300, 309, 311, 406, 516, 281, 1051, 51516], "temperature": 0.0, "avg_logprob": -0.10558906901966442, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0022478506434708834}, {"id": 562, "seek": 291964, "start": 2942.68, "end": 2948.44, "text": " tomorrow. The system will probably have to be relatively, you know, experienced before they're", "tokens": [51516, 4153, 13, 440, 1185, 486, 1391, 362, 281, 312, 7226, 11, 291, 458, 11, 6751, 949, 436, 434, 51804], "temperature": 0.0, "avg_logprob": -0.10558906901966442, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.0022478506434708834}, {"id": 563, "seek": 294844, "start": 2948.44, "end": 2954.52, "text": " better scientists than human scientists. Yeah, thank you. We have a question up there.", "tokens": [50364, 1101, 7708, 813, 1952, 7708, 13, 865, 11, 1309, 291, 13, 492, 362, 257, 1168, 493, 456, 13, 50668], "temperature": 0.0, "avg_logprob": -0.11004319763183594, "compression_ratio": 1.70625, "no_speech_prob": 0.0014266676735132933}, {"id": 564, "seek": 294844, "start": 2954.52, "end": 2959.64, "text": " Hey, can you hear me? Yeah, yeah. Great. I was wondering if you could expand a little bit on", "tokens": [50668, 1911, 11, 393, 291, 1568, 385, 30, 865, 11, 1338, 13, 3769, 13, 286, 390, 6359, 498, 291, 727, 5268, 257, 707, 857, 322, 50924], "temperature": 0.0, "avg_logprob": -0.11004319763183594, "compression_ratio": 1.70625, "no_speech_prob": 0.0014266676735132933}, {"id": 565, "seek": 294844, "start": 2959.64, "end": 2963.56, "text": " your assertion that you cannot build a sufficient world model from text alone.", "tokens": [50924, 428, 19810, 313, 300, 291, 2644, 1322, 257, 11563, 1002, 2316, 490, 2487, 3312, 13, 51120], "temperature": 0.0, "avg_logprob": -0.11004319763183594, "compression_ratio": 1.70625, "no_speech_prob": 0.0014266676735132933}, {"id": 566, "seek": 294844, "start": 2964.76, "end": 2968.28, "text": " When we think of something like a theoretical physicist, right, this person mostly interacts", "tokens": [51180, 1133, 321, 519, 295, 746, 411, 257, 20864, 42466, 11, 558, 11, 341, 954, 5240, 43582, 51356], "temperature": 0.0, "avg_logprob": -0.11004319763183594, "compression_ratio": 1.70625, "no_speech_prob": 0.0014266676735132933}, {"id": 567, "seek": 294844, "start": 2968.28, "end": 2972.76, "text": " with other people verbally and reading papers and thinking and writing. Or if you think about", "tokens": [51356, 365, 661, 561, 48162, 293, 3760, 10577, 293, 1953, 293, 3579, 13, 1610, 498, 291, 519, 466, 51580], "temperature": 0.0, "avg_logprob": -0.11004319763183594, "compression_ratio": 1.70625, "no_speech_prob": 0.0014266676735132933}, {"id": 568, "seek": 294844, "start": 2972.76, "end": 2977.56, "text": " something like a blind from birth author or person, right, they're able to actually extract a lot of", "tokens": [51580, 746, 411, 257, 6865, 490, 3965, 3793, 420, 954, 11, 558, 11, 436, 434, 1075, 281, 767, 8947, 257, 688, 295, 51820], "temperature": 0.0, "avg_logprob": -0.11004319763183594, "compression_ratio": 1.70625, "no_speech_prob": 0.0014266676735132933}, {"id": 569, "seek": 297756, "start": 2977.56, "end": 2980.92, "text": " information about the structure of the world from the text. So I'm wondering if you could explain", "tokens": [50364, 1589, 466, 264, 3877, 295, 264, 1002, 490, 264, 2487, 13, 407, 286, 478, 6359, 498, 291, 727, 2903, 50532], "temperature": 0.0, "avg_logprob": -0.11067870778774996, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.0004951134906150401}, {"id": 570, "seek": 297756, "start": 2980.92, "end": 2985.72, "text": " a little bit more about why that's insufficient for, say, achieving human level AI at least.", "tokens": [50532, 257, 707, 857, 544, 466, 983, 300, 311, 41709, 337, 11, 584, 11, 19626, 1952, 1496, 7318, 412, 1935, 13, 50772], "temperature": 0.0, "avg_logprob": -0.11067870778774996, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.0004951134906150401}, {"id": 571, "seek": 297756, "start": 2986.7599999999998, "end": 2992.2, "text": " Okay, when we do physics or mathematics, very much, very often, I mean, certainly physics, we", "tokens": [50824, 1033, 11, 562, 321, 360, 10649, 420, 18666, 11, 588, 709, 11, 588, 2049, 11, 286, 914, 11, 3297, 10649, 11, 321, 51096], "temperature": 0.0, "avg_logprob": -0.11067870778774996, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.0004951134906150401}, {"id": 572, "seek": 297756, "start": 2992.2, "end": 2998.04, "text": " have mental models of the world. We have some sort of internal simulator, if you want, that can", "tokens": [51096, 362, 4973, 5245, 295, 264, 1002, 13, 492, 362, 512, 1333, 295, 6920, 32974, 11, 498, 291, 528, 11, 300, 393, 51388], "temperature": 0.0, "avg_logprob": -0.11067870778774996, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.0004951134906150401}, {"id": 573, "seek": 297756, "start": 2998.04, "end": 3002.2799999999997, "text": " assimilate the interesting aspect of the phenomenon that we're trying to understand.", "tokens": [51388, 8249, 48104, 264, 1880, 4171, 295, 264, 14029, 300, 321, 434, 1382, 281, 1223, 13, 51600], "temperature": 0.0, "avg_logprob": -0.11067870778774996, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.0004951134906150401}, {"id": 574, "seek": 300228, "start": 3003.2400000000002, "end": 3012.44, "text": " That allows us to arrive at answers. And we don't necessarily rely on explicit", "tokens": [50412, 663, 4045, 505, 281, 8881, 412, 6338, 13, 400, 321, 500, 380, 4725, 10687, 322, 13691, 50872], "temperature": 0.0, "avg_logprob": -0.13692528651310848, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.0057032108306884766}, {"id": 575, "seek": 300228, "start": 3012.44, "end": 3021.6400000000003, "text": " facts that we've learned through language. Let me take an example. So all of intuitive physics is", "tokens": [50872, 9130, 300, 321, 600, 3264, 807, 2856, 13, 961, 385, 747, 364, 1365, 13, 407, 439, 295, 21769, 10649, 307, 51332], "temperature": 0.0, "avg_logprob": -0.13692528651310848, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.0057032108306884766}, {"id": 576, "seek": 300228, "start": 3021.6400000000003, "end": 3029.5600000000004, "text": " learned by observation. It's not learned through language, right? You know, if that you are going", "tokens": [51332, 3264, 538, 14816, 13, 467, 311, 406, 3264, 807, 2856, 11, 558, 30, 509, 458, 11, 498, 300, 291, 366, 516, 51728], "temperature": 0.0, "avg_logprob": -0.13692528651310848, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.0057032108306884766}, {"id": 577, "seek": 302956, "start": 3029.64, "end": 3034.6, "text": " to put a smartphone on a horizontal surface and let it go, you know, you know, it's going to,", "tokens": [50368, 281, 829, 257, 13307, 322, 257, 12750, 3753, 293, 718, 309, 352, 11, 291, 458, 11, 291, 458, 11, 309, 311, 516, 281, 11, 50616], "temperature": 0.0, "avg_logprob": -0.12687079368099088, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.005117020569741726}, {"id": 578, "seek": 302956, "start": 3034.6, "end": 3037.56, "text": " it's going to fall one way or the other. You may not predict in which direction,", "tokens": [50616, 309, 311, 516, 281, 2100, 472, 636, 420, 264, 661, 13, 509, 815, 406, 6069, 294, 597, 3513, 11, 50764], "temperature": 0.0, "avg_logprob": -0.12687079368099088, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.005117020569741726}, {"id": 579, "seek": 302956, "start": 3037.56, "end": 3043.24, "text": " but you know, it's going to fall because of your notion of intuitive physics. If I tell you, I take", "tokens": [50764, 457, 291, 458, 11, 309, 311, 516, 281, 2100, 570, 295, 428, 10710, 295, 21769, 10649, 13, 759, 286, 980, 291, 11, 286, 747, 51048], "temperature": 0.0, "avg_logprob": -0.12687079368099088, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.005117020569741726}, {"id": 580, "seek": 302956, "start": 3043.24, "end": 3050.92, "text": " an object, I throw it in the air vertically, and it's going to have a particular velocity when", "tokens": [51048, 364, 2657, 11, 286, 3507, 309, 294, 264, 1988, 28450, 11, 293, 309, 311, 516, 281, 362, 257, 1729, 9269, 562, 51432], "temperature": 0.0, "avg_logprob": -0.12687079368099088, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.005117020569741726}, {"id": 581, "seek": 302956, "start": 3050.92, "end": 3055.7999999999997, "text": " it leaves my hand, it's going to go up in the air and then fall back. What velocity will it have", "tokens": [51432, 309, 5510, 452, 1011, 11, 309, 311, 516, 281, 352, 493, 294, 264, 1988, 293, 550, 2100, 646, 13, 708, 9269, 486, 309, 362, 51676], "temperature": 0.0, "avg_logprob": -0.12687079368099088, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.005117020569741726}, {"id": 582, "seek": 305580, "start": 3055.8, "end": 3062.28, "text": " when it crosses my hand at the same location where it left my hand? And if you're any kind of intuitive", "tokens": [50364, 562, 309, 28467, 452, 1011, 412, 264, 912, 4914, 689, 309, 1411, 452, 1011, 30, 400, 498, 291, 434, 604, 733, 295, 21769, 50688], "temperature": 0.0, "avg_logprob": -0.113458716351053, "compression_ratio": 1.8964143426294822, "no_speech_prob": 0.001320387702435255}, {"id": 583, "seek": 305580, "start": 3063.88, "end": 3069.5600000000004, "text": " notion of physics that go beyond, you know, normal intuitive physics, you would say, obviously,", "tokens": [50768, 10710, 295, 10649, 300, 352, 4399, 11, 291, 458, 11, 2710, 21769, 10649, 11, 291, 576, 584, 11, 2745, 11, 51052], "temperature": 0.0, "avg_logprob": -0.113458716351053, "compression_ratio": 1.8964143426294822, "no_speech_prob": 0.001320387702435255}, {"id": 584, "seek": 305580, "start": 3069.5600000000004, "end": 3073.32, "text": " it's going to have the same speed because, you know, there is conservation of momentum and energy", "tokens": [51052, 309, 311, 516, 281, 362, 264, 912, 3073, 570, 11, 291, 458, 11, 456, 307, 16185, 295, 11244, 293, 2281, 51240], "temperature": 0.0, "avg_logprob": -0.113458716351053, "compression_ratio": 1.8964143426294822, "no_speech_prob": 0.001320387702435255}, {"id": 585, "seek": 305580, "start": 3073.32, "end": 3079.6400000000003, "text": " and stuff like that. And it's not because of energy, you know, necessarily the rule of the", "tokens": [51240, 293, 1507, 411, 300, 13, 400, 309, 311, 406, 570, 295, 2281, 11, 291, 458, 11, 4725, 264, 4978, 295, 264, 51556], "temperature": 0.0, "avg_logprob": -0.113458716351053, "compression_ratio": 1.8964143426294822, "no_speech_prob": 0.001320387702435255}, {"id": 586, "seek": 305580, "start": 3079.6400000000003, "end": 3085.2400000000002, "text": " explicit language rule that you have, it's because of your sort of, you know, intuition", "tokens": [51556, 13691, 2856, 4978, 300, 291, 362, 11, 309, 311, 570, 295, 428, 1333, 295, 11, 291, 458, 11, 24002, 51836], "temperature": 0.0, "avg_logprob": -0.113458716351053, "compression_ratio": 1.8964143426294822, "no_speech_prob": 0.001320387702435255}, {"id": 587, "seek": 308524, "start": 3085.24, "end": 3090.52, "text": " that corresponds to that. And we do this all the time. We manipulate mental models. We do not", "tokens": [50364, 300, 23249, 281, 300, 13, 400, 321, 360, 341, 439, 264, 565, 13, 492, 20459, 4973, 5245, 13, 492, 360, 406, 50628], "temperature": 0.0, "avg_logprob": -0.08072946848494283, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0009984710486605763}, {"id": 588, "seek": 308524, "start": 3090.52, "end": 3098.2, "text": " reason with language very often. Most of our reasoning does not use language. And so most", "tokens": [50628, 1778, 365, 2856, 588, 2049, 13, 4534, 295, 527, 21577, 775, 406, 764, 2856, 13, 400, 370, 881, 51012], "temperature": 0.0, "avg_logprob": -0.08072946848494283, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0009984710486605763}, {"id": 589, "seek": 308524, "start": 3098.2, "end": 3105.24, "text": " human knowledge is not linguistic at all. It has to do with construction of mental models,", "tokens": [51012, 1952, 3601, 307, 406, 43002, 412, 439, 13, 467, 575, 281, 360, 365, 6435, 295, 4973, 5245, 11, 51364], "temperature": 0.0, "avg_logprob": -0.08072946848494283, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0009984710486605763}, {"id": 590, "seek": 308524, "start": 3105.8799999999997, "end": 3109.7999999999997, "text": " many of which have nothing to do with language. It's certainly true of all animals. They don't have", "tokens": [51396, 867, 295, 597, 362, 1825, 281, 360, 365, 2856, 13, 467, 311, 3297, 2074, 295, 439, 4882, 13, 814, 500, 380, 362, 51592], "temperature": 0.0, "avg_logprob": -0.08072946848494283, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0009984710486605763}, {"id": 591, "seek": 310980, "start": 3109.8, "end": 3117.96, "text": " language. So that's what sort of, you know, I mean, it's something that I think physicists,", "tokens": [50364, 2856, 13, 407, 300, 311, 437, 1333, 295, 11, 291, 458, 11, 286, 914, 11, 309, 311, 746, 300, 286, 519, 48716, 11, 50772], "temperature": 0.0, "avg_logprob": -0.10059779030936104, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0028315423987805843}, {"id": 592, "seek": 310980, "start": 3117.96, "end": 3121.7200000000003, "text": " particularly physicists should really understand, right? Because we do this all the time. Good", "tokens": [50772, 4098, 48716, 820, 534, 1223, 11, 558, 30, 1436, 321, 360, 341, 439, 264, 565, 13, 2205, 50960], "temperature": 0.0, "avg_logprob": -0.10059779030936104, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0028315423987805843}, {"id": 593, "seek": 310980, "start": 3121.7200000000003, "end": 3127.6400000000003, "text": " physicists are people who have those mental models that they can use to sort of, you know, imagine", "tokens": [50960, 48716, 366, 561, 567, 362, 729, 4973, 5245, 300, 436, 393, 764, 281, 1333, 295, 11, 291, 458, 11, 3811, 51256], "temperature": 0.0, "avg_logprob": -0.10059779030936104, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0028315423987805843}, {"id": 594, "seek": 310980, "start": 3127.6400000000003, "end": 3134.36, "text": " situations and corner cases and stuff like that, that really kind of, you know, give you some insight", "tokens": [51256, 6851, 293, 4538, 3331, 293, 1507, 411, 300, 11, 300, 534, 733, 295, 11, 291, 458, 11, 976, 291, 512, 11269, 51592], "temperature": 0.0, "avg_logprob": -0.10059779030936104, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.0028315423987805843}, {"id": 595, "seek": 313436, "start": 3134.36, "end": 3140.2000000000003, "text": " as to what the nature of reality is. And of course, you know, then after that, we do the math,", "tokens": [50364, 382, 281, 437, 264, 3687, 295, 4103, 307, 13, 400, 295, 1164, 11, 291, 458, 11, 550, 934, 300, 11, 321, 360, 264, 5221, 11, 50656], "temperature": 0.0, "avg_logprob": -0.12085213058296292, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0016442050691694021}, {"id": 596, "seek": 313436, "start": 3140.2000000000003, "end": 3144.52, "text": " and that gives you sort of the internal structure of language, the mathematical language,", "tokens": [50656, 293, 300, 2709, 291, 1333, 295, 264, 6920, 3877, 295, 2856, 11, 264, 18894, 2856, 11, 50872], "temperature": 0.0, "avg_logprob": -0.12085213058296292, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0016442050691694021}, {"id": 597, "seek": 313436, "start": 3144.52, "end": 3148.84, "text": " kind of, you know, makes you discover new properties. But a lot of it is really", "tokens": [50872, 733, 295, 11, 291, 458, 11, 1669, 291, 4411, 777, 7221, 13, 583, 257, 688, 295, 309, 307, 534, 51088], "temperature": 0.0, "avg_logprob": -0.12085213058296292, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0016442050691694021}, {"id": 598, "seek": 313436, "start": 3150.44, "end": 3156.76, "text": " intuition with mental model is true in mathematics as well in geometry and things like that.", "tokens": [51168, 24002, 365, 4973, 2316, 307, 2074, 294, 18666, 382, 731, 294, 18426, 293, 721, 411, 300, 13, 51484], "temperature": 0.0, "avg_logprob": -0.12085213058296292, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0016442050691694021}, {"id": 599, "seek": 315676, "start": 3157.4, "end": 3164.28, "text": " So, you know, there's the Gedanke experiments of Einstein, right, for those are", "tokens": [50396, 407, 11, 291, 458, 11, 456, 311, 264, 28166, 282, 330, 12050, 295, 23486, 11, 558, 11, 337, 729, 366, 50740], "temperature": 0.0, "avg_logprob": -0.22372366098257213, "compression_ratio": 1.4345549738219896, "no_speech_prob": 0.0021954812109470367}, {"id": 600, "seek": 315676, "start": 3165.2400000000002, "end": 3170.0400000000004, "text": " basically mental models that you manipulate to kind of discover properties. They're not linguistic.", "tokens": [50788, 1936, 4973, 5245, 300, 291, 20459, 281, 733, 295, 4411, 7221, 13, 814, 434, 406, 43002, 13, 51028], "temperature": 0.0, "avg_logprob": -0.22372366098257213, "compression_ratio": 1.4345549738219896, "no_speech_prob": 0.0021954812109470367}, {"id": 601, "seek": 315676, "start": 3174.5200000000004, "end": 3181.32, "text": " I have a question along similar lines, but so I agree that there's a lot of intuition involved", "tokens": [51252, 286, 362, 257, 1168, 2051, 2531, 3876, 11, 457, 370, 286, 3986, 300, 456, 311, 257, 688, 295, 24002, 3288, 51592], "temperature": 0.0, "avg_logprob": -0.22372366098257213, "compression_ratio": 1.4345549738219896, "no_speech_prob": 0.0021954812109470367}, {"id": 602, "seek": 318132, "start": 3181.32, "end": 3187.1600000000003, "text": " in learning for humans. But is there not a fundamental problem in training such intuition?", "tokens": [50364, 294, 2539, 337, 6255, 13, 583, 307, 456, 406, 257, 8088, 1154, 294, 3097, 1270, 24002, 30, 50656], "temperature": 0.0, "avg_logprob": -0.13498852252960206, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.007850535213947296}, {"id": 603, "seek": 318132, "start": 3187.1600000000003, "end": 3191.2400000000002, "text": " Because anything you train digitally would be encoded in some kind of language,", "tokens": [50656, 1436, 1340, 291, 3847, 36938, 576, 312, 2058, 12340, 294, 512, 733, 295, 2856, 11, 50860], "temperature": 0.0, "avg_logprob": -0.13498852252960206, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.007850535213947296}, {"id": 604, "seek": 318132, "start": 3192.04, "end": 3198.84, "text": " some binary. Is there not a fundamental obstruction there to train such intuition?", "tokens": [50900, 512, 17434, 13, 1119, 456, 406, 257, 8088, 49711, 456, 281, 3847, 1270, 24002, 30, 51240], "temperature": 0.0, "avg_logprob": -0.13498852252960206, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.007850535213947296}, {"id": 605, "seek": 318132, "start": 3200.2000000000003, "end": 3207.96, "text": " Not really, no. The input to those systems can be as, you know, continuous and kind of perceptual", "tokens": [51308, 1726, 534, 11, 572, 13, 440, 4846, 281, 729, 3652, 393, 312, 382, 11, 291, 458, 11, 10957, 293, 733, 295, 43276, 901, 51696], "temperature": 0.0, "avg_logprob": -0.13498852252960206, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.007850535213947296}, {"id": 606, "seek": 320796, "start": 3207.96, "end": 3212.2, "text": " as the kind of stuff that we perceive, like, you know, like video or whatever,", "tokens": [50364, 382, 264, 733, 295, 1507, 300, 321, 20281, 11, 411, 11, 291, 458, 11, 411, 960, 420, 2035, 11, 50576], "temperature": 0.0, "avg_logprob": -0.10678321123123169, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0011131606297567487}, {"id": 607, "seek": 320796, "start": 3214.12, "end": 3220.2, "text": " or sensory input, whatever it is, audio, you know, anything you want. And then inside the system,", "tokens": [50672, 420, 27233, 4846, 11, 2035, 309, 307, 11, 6278, 11, 291, 458, 11, 1340, 291, 528, 13, 400, 550, 1854, 264, 1185, 11, 50976], "temperature": 0.0, "avg_logprob": -0.10678321123123169, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0011131606297567487}, {"id": 608, "seek": 320796, "start": 3220.2, "end": 3227.32, "text": " the representation of facts and knowledge inside the system is actually just a sequence of numbers.", "tokens": [50976, 264, 10290, 295, 9130, 293, 3601, 1854, 264, 1185, 307, 767, 445, 257, 8310, 295, 3547, 13, 51332], "temperature": 0.0, "avg_logprob": -0.10678321123123169, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0011131606297567487}, {"id": 609, "seek": 320796, "start": 3227.88, "end": 3236.36, "text": " It's not language. It's numbers, it's vectors, you know, tensors. So I don't think that's a", "tokens": [51360, 467, 311, 406, 2856, 13, 467, 311, 3547, 11, 309, 311, 18875, 11, 291, 458, 11, 10688, 830, 13, 407, 286, 500, 380, 519, 300, 311, 257, 51784], "temperature": 0.0, "avg_logprob": -0.10678321123123169, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0011131606297567487}, {"id": 610, "seek": 323636, "start": 3236.36, "end": 3241.4, "text": " problem we need to deal with really. But the texts are also broken down into", "tokens": [50364, 1154, 321, 643, 281, 2028, 365, 534, 13, 583, 264, 15765, 366, 611, 5463, 760, 666, 50616], "temperature": 0.0, "avg_logprob": -0.11511968196123495, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.0012789430329576135}, {"id": 611, "seek": 323636, "start": 3241.4, "end": 3249.4, "text": " same numbers, right? Yeah, that's right. That's true. So text is to some extent simpler because", "tokens": [50616, 912, 3547, 11, 558, 30, 865, 11, 300, 311, 558, 13, 663, 311, 2074, 13, 407, 2487, 307, 281, 512, 8396, 18587, 570, 51016], "temperature": 0.0, "avg_logprob": -0.11511968196123495, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.0012789430329576135}, {"id": 612, "seek": 323636, "start": 3249.4, "end": 3256.92, "text": " it's discrete, as I explained, it makes the, you know, the management of uncertainty easier", "tokens": [51016, 309, 311, 27706, 11, 382, 286, 8825, 11, 309, 1669, 264, 11, 291, 458, 11, 264, 4592, 295, 15697, 3571, 51392], "temperature": 0.0, "avg_logprob": -0.11511968196123495, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.0012789430329576135}, {"id": 613, "seek": 323636, "start": 3256.92, "end": 3261.6400000000003, "text": " if you have discrete tokens. And there is a reason why language is discrete, why language", "tokens": [51392, 498, 291, 362, 27706, 22667, 13, 400, 456, 307, 257, 1778, 983, 2856, 307, 27706, 11, 983, 2856, 51628], "temperature": 0.0, "avg_logprob": -0.11511968196123495, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.0012789430329576135}, {"id": 614, "seek": 326164, "start": 3262.44, "end": 3269.7999999999997, "text": " clusters in words. The reason is because language is a communication medium, right? It's a way of", "tokens": [50404, 23313, 294, 2283, 13, 440, 1778, 307, 570, 2856, 307, 257, 6101, 6399, 11, 558, 30, 467, 311, 257, 636, 295, 50772], "temperature": 0.0, "avg_logprob": -0.1042354865507646, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.009074016474187374}, {"id": 615, "seek": 326164, "start": 3269.7999999999997, "end": 3275.4, "text": " communicating. And we need to communicate over noisy channels. And to be able to communicate over", "tokens": [50772, 17559, 13, 400, 321, 643, 281, 7890, 670, 24518, 9235, 13, 400, 281, 312, 1075, 281, 7890, 670, 51052], "temperature": 0.0, "avg_logprob": -0.1042354865507646, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.009074016474187374}, {"id": 616, "seek": 326164, "start": 3275.4, "end": 3281.96, "text": " noisy channels, the symbols have to be discrete. Because that allows you to do error correction,", "tokens": [51052, 24518, 9235, 11, 264, 16944, 362, 281, 312, 27706, 13, 1436, 300, 4045, 291, 281, 360, 6713, 19984, 11, 51380], "temperature": 0.0, "avg_logprob": -0.1042354865507646, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.009074016474187374}, {"id": 617, "seek": 326164, "start": 3281.96, "end": 3287.48, "text": " right? To do noise, you know, to eliminate noise, right? I mean, communication engineers", "tokens": [51380, 558, 30, 1407, 360, 5658, 11, 291, 458, 11, 281, 13819, 5658, 11, 558, 30, 286, 914, 11, 6101, 11955, 51656], "temperature": 0.0, "avg_logprob": -0.1042354865507646, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.009074016474187374}, {"id": 618, "seek": 328748, "start": 3287.56, "end": 3292.28, "text": " are known this for decades, you know, since China and basically, and so, or even before.", "tokens": [50368, 366, 2570, 341, 337, 7878, 11, 291, 458, 11, 1670, 3533, 293, 1936, 11, 293, 370, 11, 420, 754, 949, 13, 50604], "temperature": 0.0, "avg_logprob": -0.16728083292643228, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.00731629366055131}, {"id": 619, "seek": 328748, "start": 3293.56, "end": 3300.68, "text": " So our language is discrete and goes into words, you know, the existence of phonemes and words", "tokens": [50668, 407, 527, 2856, 307, 27706, 293, 1709, 666, 2283, 11, 291, 458, 11, 264, 9123, 295, 30754, 443, 279, 293, 2283, 51024], "temperature": 0.0, "avg_logprob": -0.16728083292643228, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.00731629366055131}, {"id": 620, "seek": 328748, "start": 3300.68, "end": 3306.2, "text": " and things like this, because we need to be able to communicate with noisy channels. But that doesn't", "tokens": [51024, 293, 721, 411, 341, 11, 570, 321, 643, 281, 312, 1075, 281, 7890, 365, 24518, 9235, 13, 583, 300, 1177, 380, 51300], "temperature": 0.0, "avg_logprob": -0.16728083292643228, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.00731629366055131}, {"id": 621, "seek": 328748, "start": 3306.2, "end": 3311.48, "text": " mean that our thinking needs to be the same way. And in fact, our thinking is not the same way.", "tokens": [51300, 914, 300, 527, 1953, 2203, 281, 312, 264, 912, 636, 13, 400, 294, 1186, 11, 527, 1953, 307, 406, 264, 912, 636, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16728083292643228, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.00731629366055131}, {"id": 622, "seek": 331148, "start": 3311.48, "end": 3320.2, "text": " Language is a pale, approximate, discretized, dumbed down representation of eternal knowledge", "tokens": [50364, 24445, 307, 257, 19546, 11, 30874, 11, 25656, 1602, 11, 10316, 292, 760, 10290, 295, 14503, 3601, 50800], "temperature": 0.0, "avg_logprob": -0.14917060475290558, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.011325548402965069}, {"id": 623, "seek": 331148, "start": 3320.2, "end": 3326.2, "text": " representation recall thoughts. Yeah. And so my intuition would be that's when you", "tokens": [50800, 10290, 9901, 4598, 13, 865, 13, 400, 370, 452, 24002, 576, 312, 300, 311, 562, 291, 51100], "temperature": 0.0, "avg_logprob": -0.14917060475290558, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.011325548402965069}, {"id": 624, "seek": 331148, "start": 3326.2, "end": 3331.2400000000002, "text": " encode something digitally in binary, you're doing, you're dumbing that down anyway, right?", "tokens": [51100, 2058, 1429, 746, 36938, 294, 17434, 11, 291, 434, 884, 11, 291, 434, 10316, 278, 300, 760, 4033, 11, 558, 30, 51352], "temperature": 0.0, "avg_logprob": -0.14917060475290558, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.011325548402965069}, {"id": 625, "seek": 331148, "start": 3331.2400000000002, "end": 3336.6, "text": " So even if you're processing images or videos, you're doing it in numbers and", "tokens": [51352, 407, 754, 498, 291, 434, 9007, 5267, 420, 2145, 11, 291, 434, 884, 309, 294, 3547, 293, 51620], "temperature": 0.0, "avg_logprob": -0.14917060475290558, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.011325548402965069}, {"id": 626, "seek": 333660, "start": 3336.68, "end": 3340.68, "text": " you're doing it in the same way, maybe it's a little more complex.", "tokens": [50368, 291, 434, 884, 309, 294, 264, 912, 636, 11, 1310, 309, 311, 257, 707, 544, 3997, 13, 50568], "temperature": 0.0, "avg_logprob": -0.1481541509213655, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.003305623074993491}, {"id": 627, "seek": 333660, "start": 3341.3199999999997, "end": 3346.52, "text": " But that's life. Okay. That's even physics does this biology does this, right? The communication", "tokens": [50600, 583, 300, 311, 993, 13, 1033, 13, 663, 311, 754, 10649, 775, 341, 14956, 775, 341, 11, 558, 30, 440, 6101, 50860], "temperature": 0.0, "avg_logprob": -0.1481541509213655, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.003305623074993491}, {"id": 628, "seek": 333660, "start": 3346.52, "end": 3351.96, "text": " between synapses between two neurons, there is a finite number of physicals that are released", "tokens": [50860, 1296, 5451, 2382, 279, 1296, 732, 22027, 11, 456, 307, 257, 19362, 1230, 295, 4001, 82, 300, 366, 4736, 51132], "temperature": 0.0, "avg_logprob": -0.1481541509213655, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.003305623074993491}, {"id": 629, "seek": 333660, "start": 3351.96, "end": 3357.4, "text": " for the synaptic communication. And so there is granularity in this, the precision is actually", "tokens": [51132, 337, 264, 5451, 2796, 299, 6101, 13, 400, 370, 456, 307, 39962, 507, 294, 341, 11, 264, 18356, 307, 767, 51404], "temperature": 0.0, "avg_logprob": -0.1481541509213655, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.003305623074993491}, {"id": 630, "seek": 333660, "start": 3357.4, "end": 3364.04, "text": " just a few bits. And, and, you know, it's actually much less than the 32 bits that we use for or", "tokens": [51404, 445, 257, 1326, 9239, 13, 400, 11, 293, 11, 291, 458, 11, 309, 311, 767, 709, 1570, 813, 264, 8858, 9239, 300, 321, 764, 337, 420, 51736], "temperature": 0.0, "avg_logprob": -0.1481541509213655, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.003305623074993491}, {"id": 631, "seek": 336404, "start": 3364.12, "end": 3369.0, "text": " 16 bits that we use for computation in neural nets. So I don't think the quantization here is,", "tokens": [50368, 3165, 9239, 300, 321, 764, 337, 24903, 294, 18161, 36170, 13, 407, 286, 500, 380, 519, 264, 4426, 2144, 510, 307, 11, 50612], "temperature": 0.0, "avg_logprob": -0.147862852665416, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0022775016259402037}, {"id": 632, "seek": 336404, "start": 3369.0, "end": 3374.04, "text": " is an issue. It certainly exists in the, in the brain as well. Communication between neurons in", "tokens": [50612, 307, 364, 2734, 13, 467, 3297, 8198, 294, 264, 11, 294, 264, 3567, 382, 731, 13, 34930, 1296, 22027, 294, 50864], "temperature": 0.0, "avg_logprob": -0.147862852665416, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0022775016259402037}, {"id": 633, "seek": 336404, "start": 3374.04, "end": 3381.24, "text": " the brain is binary brains, you know, the neurons actually produce spikes. For the same reason that", "tokens": [50864, 264, 3567, 307, 17434, 15442, 11, 291, 458, 11, 264, 22027, 767, 5258, 28997, 13, 1171, 264, 912, 1778, 300, 51224], "temperature": 0.0, "avg_logprob": -0.147862852665416, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0022775016259402037}, {"id": 634, "seek": 336404, "start": 3382.12, "end": 3385.56, "text": " language is discrete is because they need to communicate in a long distance. And for this", "tokens": [51268, 2856, 307, 27706, 307, 570, 436, 643, 281, 7890, 294, 257, 938, 4560, 13, 400, 337, 341, 51440], "temperature": 0.0, "avg_logprob": -0.147862852665416, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0022775016259402037}, {"id": 635, "seek": 336404, "start": 3385.56, "end": 3391.64, "text": " to be efficient, it has to be digital basically. So, so I don't see this as an limitation that", "tokens": [51440, 281, 312, 7148, 11, 309, 575, 281, 312, 4562, 1936, 13, 407, 11, 370, 286, 500, 380, 536, 341, 382, 364, 27432, 300, 51744], "temperature": 0.0, "avg_logprob": -0.147862852665416, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0022775016259402037}, {"id": 636, "seek": 339164, "start": 3391.64, "end": 3400.04, "text": " would discriminate between computers and human intelligence. Thanks. Okay. Unfortunately, we", "tokens": [50364, 576, 47833, 1296, 10807, 293, 1952, 7599, 13, 2561, 13, 1033, 13, 8590, 11, 321, 50784], "temperature": 0.0, "avg_logprob": -0.14990165120079404, "compression_ratio": 1.3472222222222223, "no_speech_prob": 0.0023504039272665977}, {"id": 637, "seek": 339164, "start": 3400.04, "end": 3407.56, "text": " need to move on because we're running five minutes behind already. Thank you so much. This was great.", "tokens": [50784, 643, 281, 1286, 322, 570, 321, 434, 2614, 1732, 2077, 2261, 1217, 13, 1044, 291, 370, 709, 13, 639, 390, 869, 13, 51160], "temperature": 0.0, "avg_logprob": -0.14990165120079404, "compression_ratio": 1.3472222222222223, "no_speech_prob": 0.0023504039272665977}], "language": "en"}