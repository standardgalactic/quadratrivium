start	end	text
0	4320	So, our first speaker is Yann LeCun.
4320	10280	He's currently the Silver Professor at the current Institute of Mathematical Sciences
10280	16880	at NYU, where he is the founding director of the NYU Center for Data Science.
16880	22800	He is also affiliated with MEDA, formerly known as Facebook, as the Vice President and Chief
22800	25000	AI Scientist there.
25000	29720	By the way, MEDA just released one of its large language models just a few days ago called
29720	34120	LAMAS II, which I encourage you all to explore.
34120	39360	And this could be a very long introduction because Yann has a very long resume, but the
39360	45160	one thing I wanted to highlight is that Yann was the recipient of the 2018 Turing Award.
45160	48600	For those of you who aren't familiar with it, it's kind of like the Nobel Prize in Computer
48600	55480	Science, and he received that for his work on deep learning and convolutional neural networks.
55480	59440	And something interesting I learned is a lot of the work he did, early work on convolutional
59440	63580	neural networks, was actually when he was at Bell Labs, which is something physicists
63580	66200	know quite well, I think.
66200	70880	So I think you're all eager to hear from Yann, so I'm going to hand it over to him here.
70880	71880	Thank you.
71880	75840	Okay, I'm going to talk about, so this is going to be a somewhat technical talk, but
75840	82840	not very technical, but to tell you less about the possibilities that are offered by LLM
82840	84640	and more about their limitations.
84640	89720	And basically tell you about what I think is coming next, though at least what I'm working
89720	92400	towards coming next.
92400	97160	And the first thing we should realize is that machine learning really sucks compared to what
97160	99080	we observe in humans and animals.
99080	103760	The capabilities of the learning systems that we have today are really terrible.
103760	108200	Humans and animals can run new tasks really quickly, understand how the world works, they
108200	113280	can reason, they can plan, they have some level of common sense.
113280	118760	Their behavior is driven by objectives or drives, which is not the case for auto-reversed
118760	119760	LLMs.
119760	125920	But there is one thing that both the biological world and the recent machine learning world
125920	129440	have had in common is the use of cell supervised learning.
129440	133240	And really cell supervised learning has taken over the world.
133240	137720	For both applications in text and natural language understanding, for images, videos,
137720	141440	3D models, speech, protein folding, all that stuff.
141440	143200	What is cell supervised learning really?
143200	149520	It's sort of completion really, learning to fill in the blanks, right?
149520	156480	So the way it's used in the context of natural language understanding or processing is you
156480	163520	take a piece of text, you mask part of it by removing some other words, masking, replacing
163520	166320	them by blank markers, for example.
166320	171520	And then so think of it as a type of corruption, it doesn't have to be the masking but it could
171520	173560	be other types of corruptions.
173560	179240	And then you train some gigantic neural net to predict the words that are missing.
179240	185840	And you just measure the reconstruction error basically on the parts that were missing.
185840	194280	In the process of doing so, that system learns to represent text in a way that allows it
194280	206480	to store or represent meaning, grammar, everything, syntax, semantics, everything there is to
206480	212760	represent about language in the internal representation, which you can subsequently use for any downstream
212760	217520	task like say translation or topic classification or something of that type.
217520	223000	So this works amazingly well in the context of text, particularly because text is easy
223080	229120	to predict with uncertainty, you can never predict exactly what word will appear at a
229120	234440	particular location, but what you can do is predict some sort of probability distribution
234440	236440	of all possible words in the dictionary.
236440	241200	And you can do this because there's only a finite number of words in your dictionary
241200	242720	or tokens.
242720	247360	And so you can compute this distribution easily and handle uncertainty in the prediction pretty
247360	248360	well.
248920	253480	More generally, self-supervised learning is really sort of learning to capture dependencies
253480	257600	between inputs.
257600	263520	And so if you wanted to apply this to the problem of video prediction, for example, you would
263520	268400	show a segment of video to a system and then ask it to predict what's going to happen next,
268400	272120	for example, in the video and then reveal the future of the video.
272120	274760	And again, I apologize for the colors.
275760	279480	And then the system could adapt itself so that it does a better job at predicting what
279480	280680	happened next in the video.
280680	284840	Now, unfortunately, it's much harder to do for video than it is for text.
284840	291080	So much harder than it might require other methods than the type of generative methods
291080	292360	that work well for text.
292360	293760	I'll come back to this.
293760	299440	So speaking of generative methods, but generative AI and autoregressive language models is something
299440	303880	that many of us have been hearing about recently.
305280	306880	What is it?
306880	311560	Probably most of you know already, but essentially the way you train them is very similar to
311560	312760	the self-supervised learning.
312760	316440	It's in fact a special case of self-supervised learning method I just mentioned.
316440	323760	You take a sequence of tokens, words, whatever it is, a sequence of vectors.
323760	326920	As long as you can turn things into vectors, you're OK.
326920	333240	And then you only mask the last one and train a system to predict the last token in the
333240	334240	sequence.
334240	340440	I mean, technically, you do more than that, but that's what it comes down to in the end.
340440	344200	And once you have a system that has been trained to produce the next token, you can use it
344200	350480	autoregressively, recursively, basically to predict then the next next token and et cetera.
350480	355080	So you predict the next token, you shift it into the input, then predict the next next
355080	358360	token, shift that into the input, et cetera.
358360	360200	And that's called autoregressive prediction.
360200	366160	It's an old concept going back to signal processing many years ago, many decades ago, as a matter
366160	367160	of fact.
367160	372320	So nothing new there, but that allows the system to basically predict one token after the other
372320	374440	and generate text.
374440	379640	So those things work amazingly well.
379640	380640	Performance is really amazing.
380640	384720	The fact that, you know, they're trained only on text, even though on enormous amounts
384720	392880	of text, but only on text, the amount of knowledge, if you want, that they capture from text is
392880	397760	pretty amazing and surprised a lot of people.
397760	403080	Those systems typically have billions or up to hundreds of billions of parameters.
403080	407120	They train typically on one to two trillion tokens.
407120	408120	Sometimes more.
408200	418360	Their input window is anywhere between 2000 and maybe a few tens of thousands of tokens
418360	421760	for their context window.
421760	426800	And there's been a long history of such models that have been put out, the sort of GPT family,
426800	433920	starting with GPT-123, from FAIR, there's been Blunderbot, Galactica, Llama, Version
434720	440040	2 that just came out this week, Alpaca from Stanford, which is a fine-tuned version of
440040	446240	Llama, Llama, Version 1, Lambda and Bard from Google, Chinche from DeepMind, you know, and
446240	451160	of course, GPDT, GPT-4 from OpenAI.
451160	457640	And they're really good at, as writing aids, but they have really limited knowledge of
457640	462600	the underlying reality because they're purely trained from text, at least for the vast majority
462600	463600	of them.
463600	467520	And they really have no common sense or very limited common sense, and they have limited
467520	472000	abilities to plan their answers because the answers are produced autoregressively.
472000	478680	But still, it's pretty impressive how they work, so, as was mentioned, my colleagues
478680	482840	just put out an open source LLM called Llama 2.
482840	488960	There is three versions of this at the moment, 7 billion, 13 billion and 70 billion parameters.
488960	492480	The license is fairly liberal, so it can be used commercially if you want.
493480	497960	If you want to start a startup and use it as a business, you can.
497960	505040	It's also available on sort of various cloud services, easy to use.
505040	511520	So that very fresh just last week has been pre-trained with two trillion tokens.
511520	520680	The context length is 4,096, and some versions of it have been fine-tuned for dialogue and
520680	521680	things of that type.
521680	530160	It compares favorably to other systems, either open or closed source on a number of benchmarks.
530160	534800	But the essential characteristic of it is that it's open.
534800	542440	And together with the model, we released a piece of text that a lot of people signed.
542440	546560	The text says, we support an open innovation approach to AI, responsible and open innovation
546560	552160	gives us all the stakes in the AI development process, bringing visibility, equity and trust
552160	557760	to these technologies, opening today's LLM model will let everyone benefit from this technology.
557760	564000	So what you have to understand is that at the government level, there is kind of a fork
564000	569360	in the road where people are wondering whether AI, because it's powerful, should be kept
569360	575120	under lock and key and controlled and heavily regulated, or whether an open source approach
575120	576120	is preferable.
576120	582520	Yes, they are dangerous, but historically, it's quite the case that there's a lot of
582520	588160	evidence that open source software is actually more secure than the proprietary ones.
588160	593840	And the benefits, the potential benefits of AI and LLM in particular are so large that
593840	598320	we'll be shooting ourselves in the foot by kind of keeping this under lock and key.
598320	607840	So Meta is definitely on the side of open research, has been for 10 years in AI, but
607840	613640	it's still kind of an unsettled question, if you want.
613640	619800	I think personally that this will open up the possibility of an entire ecosystem built
619800	623120	on top of open source base LLM.
623120	627760	Training base LLM is very expensive, so we don't need to have 25 different proprietary
627760	628760	base LLM.
628760	633960	We basically need a few that are open source so that people can build fine-tuned products
633960	634960	on top of them.
634960	639600	There's another reason, which is that before I go back to technical questions, which is
639600	646640	that there's going to be a future in which all of our interactions with the digital world
646640	653480	are going to be mediated through AI systems, virtual systems of some type, and it's going
653480	658240	to become basically a repository of all human knowledge.
658240	666320	So we're not going to be interrogating Google or doing a literature search directly anymore.
666320	672360	We're just going to be talking to our AI assistant and asking a question and perhaps referring
672360	676800	to original material and things like that.
676800	680280	But basically all of our interactions with the digital world are going to be mediated
680280	682440	by AI systems.
682440	685000	So this is going to become the repository of all human knowledge.
685000	689640	It's going to become a basic infrastructure that everybody wants to use.
689640	693480	And history shows that basic infrastructure must be open source.
693480	698360	If you look at the history of the internet, there was a battle between commercial providers,
698360	702640	Microsoft, Sunmacrosystems, and others to provide the software infrastructure of the
702640	703640	internet.
703640	706720	All of those commercial providers lost.
706720	713720	What runs the internet today is Linux, Apache, Chrome, Firefox, JavaScript.
713720	717080	It's all open source.
717080	721800	So my prediction is the same thing is going to happen in the context of AI.
721800	728840	And it's necessary because a lot of countries outside the US in particular don't see with
728840	734920	a favorable eye the fact that their citizens are going to get all the information from
734920	739080	proprietary systems controlled by a small number of tech companies on the west coast
739080	740080	of the US.
740080	746080	So this is just proprietary systems are just not going to fly.
746080	751160	It's just not going to be acceptable to the citizenry across the world.
751160	752240	So it has to be open.
752240	753240	It's inevitable.
753240	761280	In fact, those systems need to be fine-tuned through what has been called RIHF, there's
761280	764840	various ways to fine-tune those systems.
764840	769320	Because the collection of human knowledge is so large, it includes things like physics,
769320	777800	like many of you know, it's going to require contributions from millions of people in sort
777800	780840	of a crowdsourcing fashion.
780840	785240	Because basically those systems being the repository of all human knowledge will be sort
785240	786240	of like Wikipedia.
786240	789440	Wikipedia cannot be built by a proprietary company.
789440	794320	It has to be, it has to gather the entire, the contribution of the entire world.
794320	797560	So it's going to be the same thing with AI based systems.
797560	802560	So open source AI is inevitable, in my opinion, and we're just sort of taking the first step.
802560	803560	Okay.
803560	811720	And so this Lama 70 billion, which is the largest of the Lama model is pretty interesting.
811720	813520	Those are a few examples of what it can generate.
813520	822200	These are extracted from the paper that you can read from the main website.
822200	827480	The fine-tuned system actually refuses to give you kind of illegal information.
827480	829600	You know, it's imperfect, but it works pretty well.
829600	834880	It's got ways of detecting safety and helpfulness and toxicity and things like that.
834880	835880	Okay.
835880	841080	So this is all well and good, but autoregressive LLMs really suck.
841080	848280	For many of us in the AI research business, LLM, the LLM revolution took place two years
848280	852960	ago and it's kind of old hats already.
852960	856520	Not the case for the public who's been kind of, you know, coming in contact with sensitivity
856520	861600	only in the last few months, but really they're not that great.
861600	867840	They don't really produce factual consistent answer.
867840	870720	They hallucinate or they confibrate.
870720	872720	They can't take into account recent information.
872720	877440	They're trained on information that is two years old or so, or whatever snapshot of the
877520	881040	crawl is used.
881040	886240	They're not really, it's not really possible to make them behave properly other than through
886240	888080	this RLHF, which is really perfect.
888080	892200	So you can always jailbreak them by changing the prompt and sort of asking them to kind
892200	895320	of act as if they were toxic.
895320	897400	They don't reason.
897400	898400	They don't really plan.
898400	903160	They can't do math unless you almond them with tools, which you can, of course.
903200	906360	And perhaps Stephen Wolfram will talk about this.
908360	913240	And you need to, there's a lot of work on sort of getting them to use tools such as search
913240	915760	engine calculators, database queries, et cetera.
915760	917960	Right now it's a bit of a hack the way this is done.
919560	923640	And the thing is we are really easily fooled by their fluency into thinking that they're
923640	928040	intelligent, but their intelligence is very limited and really nothing like human intelligence.
929240	931840	In particular, they really don't know how the world works.
931880	936520	They have no connection with the physical reality.
939320	945200	There's another reason why this, and it's basically by construction, which is that a system
945200	951920	that produces one token after the other, auto-regressively, is a divergent process.
951920	954400	It's a diffusion process with an exponential divergence.
955120	960840	If there is a probability at any token that is produced that the token takes you out of
960840	964880	the set of correct answers, those probabilities accumulate.
965040	971680	And the probability that a string of tokens of length n is correct is one minus this probability
971680	972840	of error to the power n.
972840	976880	So the probability of correctness decreases exponentially with the length of the
978560	980800	of the sequence that is produced.
982000	984400	This is not fixable without major redesign.
985440	989360	It's really an essential flaw of auto-regressive prediction.
990840	999920	A while ago, with a colleague, Jacob Browning, we wrote a paper that essentially points out
999920	1003600	to the limit, it's not a technical paper, it's a philosophy paper, actually,
1003600	1005840	the philosophy magazine called Noema.
1006560	1012120	And it talks about the fact that most of human knowledge is non-linguistic.
1012160	1017200	Everything that we learned before the age of one, everything that any animal learns has
1017200	1018240	nothing to do with language.
1018480	1021600	And it's an enormous amount of background knowledge about the world that we learned in
1022960	1026000	in the first few months of life and that animals know.
1026960	1028280	None of this is linguistic at all.
1028360	1032080	And LLMs don't have access to any of this kind of knowledge.
1033400	1038280	And so the thesis in that paper is that we're not going to be able to reach human level AI
1038280	1044680	unless we have systems that have sort of direct sensor information in the form of vision, for example.
1045680	1048120	You know, some way of understanding how the world works.
1048520	1053440	Other papers that have appeared either from the cognitive science, in fact, that paper is from MIT,
1054000	1060960	or from the classical AI kind of field, point to the fact that LLMs really cannot plan.
1061200	1068320	They don't have the ability to think really or reason in a way that we understand this from humans.
1068960	1073680	And very limited abilities to plan, at least compared to other systems that are specifically
1073680	1075040	built for planning.
1076640	1081000	So I think there is three challenges in the future for AI and machine learning research.
1081000	1086920	And I've been showing this slide for several years now and I haven't changed it because of LLMs.
1087240	1092520	The first one is learning representations and predictive models of the world, where the world can
1092520	1094680	include other people that the system is talking to.
1095600	1097320	The solution to this is self-supervised learning.
1097320	1098720	We've known this for a number of years.
1100200	1101160	Learning to reason.
1102160	1108240	So autoregressive LLMs are very much like what Daniel Kahneman calls system one, which basically
1108240	1113480	corresponds to subconscious tasks in humans, tasks that you accomplish without real planning
1113480	1122560	or reasoning that you sort of accomplish more or less reactively without thinking too much.
1123560	1131080	System two is the type of action that you take by deliberate reasoning using the power
1131280	1137800	of your prefrontal cortex, using your ability to predict and then planning sequence of actions
1137800	1141520	that will sort of satisfy a particular objective.
1142120	1144280	And LLMs are not capable of this at the moment.
1147800	1151320	I'm going to argue for the fact that reasoning and planning should be viewed as some sort of energy
1151320	1152080	minimization.
1153080	1157960	And then the last thing is learning to plan complex of actions to satisfy a number of objectives.
1158840	1163640	And that will require learning hierarchical representations of action plans, which machine
1163640	1165720	learning systems don't really know how to do at the moment.
1166520	1172680	I've written this vision paper a while back called A Pass Towards Autonomous Machine Intelligence.
1172680	1174120	I kind of changed the name of this now.
1174120	1178600	I called it Objective Driven AI, but it really is the same concept.
1179880	1185880	And it built around this idea of what's called cognitive architecture.
1185880	1189400	So it's basically an architecture of different modules that interact with each other.
1190840	1195160	A perception module that basically gives the system an estimate of the state of the world
1195160	1197480	from perception that may be combined with memory.
1198120	1202920	A world model and the world model essentially is there to predict what's going to happen
1202920	1206760	in the world, perhaps as a result of actions that the agent might take,
1207560	1210920	actions that are being imagines by another module called the actor.
1211480	1214280	So the actor feeds actions to the world model.
1214280	1217800	And the world model predicts the outcome of those actions.
1219480	1226360	And then this outcome is fed to a cost module that that cost module basically
1226360	1229080	assesses whether the outcome is good or bad.
1229080	1231160	So it measures the quality of the outcome.
1232280	1236840	And the entire purpose of the agent is to figure out a sequence of actions
1236840	1239320	that minimizes that cost.
1239320	1240760	We're not talking about learning here.
1240760	1242440	We're talking about inference.
1242440	1248680	So this minimization of the cost with respect to the actions imagined by the actors
1249800	1252760	using the world model is for inference.
1253400	1256920	Okay, so inference is not just forward propagation to a few layers of neural net.
1256920	1261240	It's actually an optimization process, very much like what happens in business
1261240	1263160	nets and record models and stuff like that.
1265480	1268200	And I can describe that in more details.
1270280	1276520	So that's kind of a very simple different representations of this kind of architecture.
1277400	1283160	Perceives the world ready to a perception module that computes an abstract representation
1283160	1288520	of the state of the world, perhaps combined with content of a memory that has some other
1288520	1289720	idea with the state of the world.
1291000	1295880	Initialize your world model with that and then feed the world model with that initial
1295880	1302040	configuration combined with an imagined action sequence imagined by the actor.
1303560	1307640	And then feed the results to a number of objective functions.
1307640	1311640	A set of objectives that you can think of as guardrails that are hardwired.
1312840	1319160	And other objectives that measure whether the task was satisfied was fulfilled.
1320600	1324600	And the entire purpose of the system is to figure out a sequence of actions that
1324600	1326760	will minimize those costs at inference time.
1327480	1333080	Okay, so it cannot do anything, but I put action sequences that minimize those costs
1333080	1336520	according to the prediction that it's making from its world model.
1337720	1339800	So that's why I call this objective driven.
1341000	1346040	There's no way you can job rate that system because it's hardwired to optimize those objectives.
1346040	1350360	So unless you modify the objectives, the guardrails in particular, you're not going to be able to
1351080	1357320	have it produce toxic content, for example, if the guardrail objective includes something
1357320	1365560	like measuring toxicity. The world model very likely will need to be some sort of recurrent
1365560	1370200	model that might be multiple steps to the action. So you take, for example, two actions
1370200	1373560	and you run them through your world model twice so that you can predict in two steps
1373560	1377160	what's going to happen. And the guardrail cost can be applied at every time step.
1377880	1381640	Of course, the world is not deterministic. So the world model really,
1382520	1386440	if it's a deterministic function, needs to be fed latent variables so that there might be
1386440	1389240	multiple predictions for a single action in a single initial state.
1390200	1394600	And when you make the latent variable vary over a set or you sample them from a distribution,
1394600	1401080	you get multiple predictions. That complicates the planning process, of course, but the process
1401080	1404600	by which you figure out a sequence of actions that minimize the objectives
1404600	1411000	is a planning and reasoning procedure. Ultimately, what we really want is some
1411000	1417640	sort of ways of doing this hierarchically. And I'm going to explain this with an example.
1418360	1424360	So here is an example. Let's say I'm sitting in my office in New York at NYU and I want to
1424920	1431800	fly to Paris. I want to go to Paris. So the first action I have to do is take a taxi or
1431800	1438040	the train to the airport, either Newark or JFK. And then the second step is I need to catch a
1438040	1443320	plane to Paris. Okay, but I have a first goal, which is to get to the airport. Now that goal
1443320	1448280	can be decomposed into two sub-goals. The first one is I need to go down in the street
1448840	1455880	and tell the taxi to take me to the airport. How do I go down in the street? I need to
1455880	1464200	stand up for my chair, get out of the building and take the elevator or the stairs go down. How
1464200	1469080	do I get out from my chair? I need to activate muscles in a particular order all the way down
1469080	1473480	to millisecond by millisecond muscle control. We do this kind of hierarchical planning all the
1473480	1477160	time without even thinking about it, even though it's actually a very conscious task that we're
1477160	1485320	doing. Animals do this also. You can watch, I don't know, cats planning trajectories to kind of
1485320	1489160	jump on a piece of furniture. They're doing this kind of planning hierarchically.
1490200	1496440	We don't have any system, AI systems today, that can learn how to do this spontaneously.
1496440	1500200	There are systems that do hierarchical planning, but they're hardwired. They're built by hand.
1501080	1505080	What we need is a system that can learn the various levels of representations
1505080	1510520	of the state of the world that will allow them to do this kind of decomposition of complex tasks
1510520	1518360	into a hierarchy of simpler ones. Again, we don't have any system that can do this today at all.
1518360	1526120	This is a big challenge, I think, for the future of AI research. That's the main idea of
1526120	1530120	objective-driven AI. How can we build systems like this that can do hierarchical planning?
1530120	1534680	They can learn models of the world that predict what's going to happen in the short term with
1534680	1540200	high precision or in the long term with less precision in more abstract levels of representation.
1541000	1544200	This is where I think AI research would go over the next 10 years or so,
1545080	1551160	and this is how LLM should be built. In fact, that may be how LLM may be built in the future,
1551160	1555880	and in fact, I have a prediction which is that the type of autoreversive LLMs that we see today
1555880	1560920	will disappear within three to five years because they are not able to plan their answers.
1561800	1566440	If we had a system that was able to take a query and then in some sort of abstract
1566440	1571720	representation space was able to plan its answer, plan a representation of its answer,
1572520	1577000	and then translate this representation of the answer into fluent text using an autoregressive
1577000	1583000	decoder, for example, then we would have something that could actually be factual and
1583880	1592920	simultaneously with being fluent and be non-toxic and be easily germ-broken and be steerable.
1593480	1599960	That's my idea for where things are going. Building this and making it work is not going
1599960	1606440	to be easy and may fail, but I think that's where we should go. If we have systems like this,
1606440	1614440	we won't need any kind of RLHF or human feedback other than the type of systems that are required
1614440	1620920	to train cost modules to measure things like toxicity, for example, but we won't need to
1621000	1627480	fine-tune the system globally to be safe. We just need to put an objective so that all of the
1627480	1633480	outputs that it produces are safe, but we don't need to retrain the entire encoders and everything
1633480	1641320	for that. I think it would simplify training quite a bit, actually. Let me skip this.
1644360	1646840	We come to the question of how do we build and train this word model?
1646840	1654120	When we look at babies, babies learn in the first few months of life an enormous amount of
1654120	1658440	background knowledge about the world, mostly by observation, a little bit by interaction,
1658440	1665080	when they start to get old enough to actually act on the world, but mostly just by observation.
1666760	1673640	The type of knowledge that they learn, things like intuitive physics, gravity, inertia,
1673640	1677880	conservation of momentum, things like that, pops up only around the age of nine months.
1678520	1682440	It takes about nine months for babies to really figure this out, that objects that are not supported
1682440	1687400	will fall. But how do they do this? How do they learn this? Obviously, they don't do this like
1687400	1695080	LLMs, because if LLMs were the answer to learning like humans, first of all, we would not need
1695720	1700920	one trillion tokens to train them. Humans are not exposed to that much text information.
1701800	1710840	Reading one and a half trillion tokens for human reading eight hours a day at normal speed would
1710840	1722120	take about 20,000 years. That's obviously way more than any humans can do. But there are things
1722120	1727880	that cats and dogs and young humans can do that are pretty amazing that LLMs can't even touch,
1728600	1737400	not even remotely close. So cats and dogs can do things that robots cannot come anywhere close
1737400	1742840	to doing today, not because we can't construct the mechanical systems for it. It's just because we
1742840	1748360	can't build the intelligence for it. Any 10-year-old child can learn to clear up the dinner table and
1748360	1754840	fill up the dishwasher in minutes, probably in one shot. We do not have robots that can do that.
1754840	1759720	We don't have domestic robots. Any 17-year-old can learn to drive a car in about 20 hours of practice,
1759720	1764600	and we still don't have a limited level of autonomous driving. So that means we're missing
1764600	1769640	something really big in terms of learning that is very different from the way humans and animals
1771320	1776840	learn. And this is just another example of the Moravec paradox, which is that there are things
1776840	1782440	that seem easy for humans and turn out to be really difficult for AI and vice versa. AI systems
1782440	1789800	are much better than humans at many tasks, narrow tasks, and we are nowhere near finding
1792280	1798360	mechanisms by which machines can approach the sort of type of understanding of the world that a cat
1798360	1806120	or a dog can have. Okay, so very some idea about how we can approach that problem. And again,
1806120	1811480	it's based on self-supervised learning, learning to fill in the blanks. If we train the neural net
1811480	1817080	to do video prediction, something we've been attempting to do for 10 years now. It doesn't
1817080	1821560	work very well. If you look at the second column of this little animation at the bottom, the predictions
1821560	1826920	that are produced by the system, and this is a very stylized video, are very blurry. It's because
1826920	1831880	the system is trained to make one single prediction, and it cannot exactly predict what's going to
1831880	1836920	happen in the video. So as a result, it predicts a kind of blurry mess, which is the average of all
1836920	1843720	the possible features, plausible features that can happen. It's the same thing if you use a similar
1843720	1851160	system to predict natural video, you get those blurry predictions. So my solution to this is
1851160	1860280	something I call joint embedding predictive architecture, JEPA. And the main idea behind
1860280	1866840	JEPA is to abandon the idea that prediction needs to be generative. Okay, so the most
1866840	1871320	popular thing at the moment is generative AI, generative models. What I'm going to tell you
1871320	1877080	now is to abandon it. Okay, not a very popular idea at the moment, but here is the argument.
1878120	1883560	A generative model is one that, for which you give it an input x, let's say initial segment of a video
1883560	1889480	or a text, run it through an encoder and a predictor, and then try to predict a variable y, which
1889480	1894440	may be the continuation of that video or the continuation of that text, or the missing words
1894440	1902280	in that text, and the error by which you measure the performance of the system is basically the
1902280	1906760	some sort of divergence measure between the predicted y and the actual y. Okay, that's how
1906760	1914200	you would train this model. It's a generative model because it predicts y. A joint embedding
1914200	1920200	predictive architectures does not attempt to predict y. It attempts to predict a representation
1920200	1925160	of y. Okay, so both x and y go through encoders that compute representations,
1926360	1929080	and you perform the prediction in representation space.
1932520	1939480	And the advantage of this is that the encoder of y may have invariant properties
1940200	1946360	that map multiple y's to the same sy. And so if there are things that are very, very hard to predict,
1946360	1951080	the encoder might eliminate that information that is hard to predict or impossible to predict
1951080	1956280	from sy so that the prediction problem becomes easier. So let's say, for example, that you're
1956280	1962280	driving along the road and the predictive model here is trying to predict what's going to happen
1962280	1966680	on the road. So because it's a self-driving car, it wants to predict what the other cars on the road
1966680	1972840	are going to do. But bordering the road, there might be trees and there is wind today, so the leaves
1972840	1979320	on the trees are moving in chaotic ways. Behind the trees, there is a pond and there is ripples
1979320	1986840	on the pond because of the wind. Those ripples and the motion of the leaves are not only very hard
1986840	1994360	to predict, pretty much impossible to predict, but also very informative. There's a huge amount
1994360	1998120	of information in there. And so if you use a generative model, that generative model will
1998120	2002440	have to devote an enormous amount of resources trying to predict all of those details that
2002440	2007720	are irrelevant to the task, really. Whereas if you have a model like the one on the right, the JEPA,
2008840	2016920	the JEPA can choose to eliminate those details from the scene and only keep the details about why
2016920	2021640	that are relatively easy to predict, like the motion of the other cars, for example.
2023000	2027000	So that's my argument for the joint emitting architecture and that means abandoning generative
2027000	2030840	models. Now, of course, you want to use generative models if you want to generate,
2031480	2036840	but if what you want is to understand the world and then be able to plan, you don't need generative
2036840	2041800	models. You need those joint emitting architectures. The reason I'm advocating for this is because
2041800	2047800	experimentally, if you want to use self-supervised zoning in the context of images as opposed to text,
2047800	2051960	the only architectures that work well are joint emitting architectures.
2052680	2056680	There are architectures like the one on the left here, which is a joint emitting architecture
2056680	2061320	without the predictor. This is the most successful approach to self-supervised zoning for image
2061320	2069320	recognition. You show image X or rather image Y, then you corrupt this image Y into image X
2069320	2075800	by distorting it, blurring it, adding noise, masking some parts of it, changing the framing,
2075800	2083960	the size, etc. And then you run both images through the encoders and you force the system
2083960	2088120	or you train the system to produce representations that are identical for the two images,
2088920	2094120	so that the representation of the corrupted image is the same as the representation of the
2094120	2099720	uncorrupted image. And that builds representations that are invariant to the corruptions, essentially.
2101400	2106040	So those methods, there is a whole bunch of them, about a dozen of them, and they work really well,
2106760	2110920	whereas all the methods to learn image features that are based on reconstruction,
2110920	2114520	generative models, don't work. At least they don't work nearly as well.
2115880	2120920	So what this slide shows is kind of different versions of this joint emitting predictive
2120920	2126440	architecture, either with a predictor or without, with a predictor that can be stochastic,
2126440	2132200	having latent variables or not. And the question is how you train this, because the problem is,
2132200	2138040	if you train a system like this, without being careful, is going to collapse. If you train a
2138040	2145000	system, you give it pairs of images, let's say x and y, or video snippets, x and y, and you tell it
2146040	2150440	compute representations that are identical for x and y, the system will just collapse. It will
2151240	2157320	produce sx and xy that are constant, and then just completely ignore x and y,
2158120	2163480	so that the distance between sx and sy is minimized. So that's a collapse.
2164200	2172520	What's a, how can you correct this? And to correct this, you have to put yourself in
2172520	2176520	the context of something called energy-based models, and I'm sure there are a lot of physicists
2176520	2182440	in the room, so you can understand what that means. So energy-based models are models where you don't
2184840	2190840	explain what they do in terms of probabilistic modeling, but in terms of an energy function
2190840	2195960	that captures the dependency between the variables. So maybe a little more explicit here. Let's say
2195960	2202440	you have two variables, x and y, and your datasets are those greenish dots that are supposed to be
2202440	2211960	black. The way an energy-based model captures the dependency between x and y is that it computes
2211960	2218360	an energy function, an implicit function with a scalar output that takes x and y as an input
2218360	2225480	and gives you an energy that needs to be low near the data points, on the data points nearby,
2225480	2232760	and then higher outside of those data points, the region of high data density. And if you have
2232760	2236680	such an energy landscape, you have a function that has this, that can compute this energy landscape,
2237640	2242600	then that function will have captured the dependencies between x and y. You can infer x
2242600	2247800	from y, you can infer y from x, you can have mapping between x and y that are not functions,
2247800	2251480	because you can have multiple y's that are compatible with a single x, for example,
2251480	2259880	so it captures multi-modality without having to be a probabilistic model. Of course, in physics,
2259880	2266280	we're familiar with this, right? Very often, we write an energy function, and then we turn it
2266280	2270120	into a probability distribution over states using a Gibbs distribution. Same idea here,
2270120	2274200	but here we don't need the Gibbs distribution at all, we just manipulate the energy function
2274200	2279560	directly. How do we train a system like this? There's really two categories of training methods.
2280200	2287240	One category is contrastive methods, and those consist in generating those flashing green dots
2287240	2293000	here that are outside the manifold data, and then changing the parameters of the energy function
2293000	2298040	so that the energy takes low values on the data points and higher values on those contrastive
2298920	2306040	green points. I contributed to inventing those methods back in the early 90s, but I don't like
2306040	2310200	them anymore, because in high-dimensional spaces, the number of contrastive points you have to
2310200	2315800	generate for the energy function to take the right shape grows exponentially, and that's not a good
2315800	2321320	thing. I prefer another server approach, regularised methods, and there are those methods. I'm going
2321320	2326760	to explain this with another slide. They basically consist in minimizing the volume of space that
2326760	2332600	can take low energy through some sort of regulariser, for example, so that the system can give low
2332600	2336760	energy to the data points by changing the parameters of the energy function so that the
2336760	2343400	energy of the data points gets lower, but because it's regularised, it can only give low energy to
2343400	2351000	a small volume of space. The data points get kind of shrinkwrapped if you want in the sort of region
2351000	2358280	of low energies. So I prefer this. That seems to be more efficient, and the question is how we do
2358280	2362920	this, but what I'm asking you to do now is abandon generative models, the most popular thing at the
2362920	2368600	moment, abandon probabilistic models, the pillar of understanding machine learning, abandon contrastive
2368600	2373800	methods, which also have been very popular, and also something I've been saying for a number of
2373800	2379960	10 years, abandon reinforcement learning, because it's so damn inefficient. So those are kind of
2379960	2383800	four of the most popular approaches to machine learning at the moment, and I'm telling people to
2384760	2390680	move away from them. You can imagine I'm not being very popular here, but I'm used to that.
2390680	2395720	So how do you prevent those systems from collapsing? What you can do is measure,
2395720	2400920	have some sort of measure or information content of SX and SY across a batch, for example,
2401560	2406200	and then try to maximise that. Now, unfortunately, it's very hard to do because we don't have lower
2406200	2412840	bounds on information content. We only have upper bounds, but it turns out you can sort of do this.
2412840	2418120	So one way to prevent SX from collapsing is that you can use a criterion, which is
2419720	2425640	attempt to make sure that the variance of every component of SX over a batch is at least one.
2425640	2435160	So that's the criterion you see in the second red box below the cover. That's a hinge loss
2435160	2441000	that makes the standard deviation of each variable at least one. And then another term that makes sure
2441000	2447240	the components of SX are decorrelated. That's the covariance matrix term. So you're trying to minimise
2447240	2451960	the octagonal terms of the covariance matrix of the vectors SX over a batch.
2453880	2458120	That's not actually sufficient. So you can also use an expander. I don't have time to explain
2458120	2462760	why that works, but the resulting method is called Vicreg variance, invariance covariance
2462760	2468120	regularisation. And it's a pretty general method that can be applied to a lot of situations for
2468120	2473640	those joint embedding predictive architectures for various applications in image recognition,
2473640	2479720	segmentation, etc. It's pretty similar to another method called MCR squared invented by EMA at its
2479720	2488360	group at Berkeley. And it works really well. I'm not going to bore you with details, but there is a
2488360	2493400	standard scenario in which you do use sub-supervised learning where you pre-train a convolutional net,
2493400	2499720	let's say, using this method. And then you chop off the expander, stick a linear classifier, which
2499720	2503880	you train supervised and you measure the performance. And you get really good performance on image net
2503880	2510680	this way, particularly good performance for out-of-distribution transfer learning. There's
2510680	2517320	a modification of this method called Vicreg L, which was published last year, which is more
2517320	2523880	tuned for segmentation and things like this, but I don't have time to go into details. There's a new
2523880	2531640	method that we rolled out at CVPR just a few weeks ago called Image JEPA that uses masking and a
2531640	2538040	transformer architecture for learning features in images. And so the collapse prevention method
2538040	2541720	there is different, but the advantage of this method is that it does not require any data
2541720	2546440	augmentation other than masking. So it doesn't require to know really what type of data you're
2546440	2551960	manipulating. And it's incredibly fast and it works really well. It gives amazing results
2553000	2558440	for really, really good features. There's another set of method by some of our colleagues here at
2558440	2566200	Fair Paris called Dino, the INO. It's a different way of preventing collapse, but it has some
2566200	2571640	commonalities with IJPA. And it works really well. It gives you something like above 80% on
2571640	2576600	ImageNet, purely supervised with no fine tuning and without any data augmentation, which is pretty
2576600	2584520	amazing. But ultimately what we want to do is use this self-supervised learning and this
2584520	2590360	JEPA architecture to build the systems of the type that I talked to you about earlier that are
2590360	2595160	hierarchical. They can predict what's going to happen in the world, perhaps as a result of an
2595160	2602600	action, with some early results on training systems from video to learn good representations of
2602600	2609480	images and videos by being trained on successive frames from a video and distorted images.
2609480	2613160	I don't have time to go into the details of how this works. It's called NCJEPA.
2616840	2623320	And it is trained basically to extract good features from images for object recognition,
2623320	2627960	but also to estimate motion in a video. And it does a pretty good job at this.
2629240	2637640	So watch this paper on archive that you're invited to look at. So objective-driven AI is this idea
2637640	2643160	that we're going to have objectives that are going to drive the behavior of our system and it's going
2643160	2649800	to make it terrible and safe. And there are things that we're working on to get this to work,
2649800	2653800	self-supervised learning from video, that recipe that really works for everything. So we're working
2653800	2659960	with those JEPA architectures, but we don't think we have the ultimate recipe yet. We can use this
2659960	2665000	to build LLMs that can reason and plan, that are driven by objectives, perhaps hopefully,
2666040	2672040	learning systems that can do hierarchical planning, like animals and humans, many animals and humans.
2672040	2677720	We have many problems to solve. JEPAs with regularized written variables to deal with uncertainty,
2677800	2681960	planning algorithms in the presence of uncertainty, learning cost modules,
2682600	2687000	which could be assimilated with inverse reinforcement learning, planning with
2687000	2692680	inaccurate world models, and then exploration techniques to adjust the world model in case it's
2692680	2704200	not completely accurate. Okay, so I'm sort of concluding. There is a computing limitation
2704200	2711560	of autoregressive LLM, which is that they can only allocate a finite and fixed amount of
2711560	2717560	computational resources to producing single token. You run through, you know, 48 layers
2717560	2721560	of a transformer or something like that, you produce one token and then 48 more layers and
2721560	2730440	produce one token. And this is not too incomplete. Whereas the method I'm suggesting, the architecture
2730440	2735880	I'm suggesting that can produce an output by planning through energy minimization,
2736760	2741480	that is too incomplete, because everything can be reduced to optimization, basically.
2742600	2746680	We're still missing essential concepts to reach human ability AI, you know, this
2749720	2757640	potential technique for planning and reasoning, you know, basic techniques that we're missing to
2757640	2765320	learn world models from complex modalities like video. And perhaps in the future, we'll be able to
2765320	2771480	build systems that can plan their answers to satisfy objectives and have guardrails. I don't
2771480	2775560	believe there is such a concept as artificial general intelligence, because I think even human
2775560	2780280	intelligence is very specialized. So let's forget about general intelligence, let's try to get to
2780280	2785800	human level intelligence, perhaps, perhaps build machines that have the same sort of set of skills
2785880	2790520	and ability to learn new skills and humans. But we are very specialized. In fact, we know this
2790520	2793960	because computers are much better than us at many tasks, which means we are specialized.
2794920	2798680	There's no question that sometimes in the future, machines will surpass human
2798680	2803080	intelligence in all domains where humans are intelligent. You know, how long is it going to
2803080	2807800	take? I don't know, but there's no question is going to happen. We don't need, we don't,
2809000	2814040	we probably don't want to be threatened by that. It would be a future where every one of us would
2814040	2820120	be assisted by a system that is more intelligent than us. And we're familiar with that concept
2820120	2824760	with other humans. I only work with people who are smarter than me, or at least I try.
2826680	2830360	Or if they're not smarter than me, I try to make them smarter than me, they're called students.
2831160	2836680	And, you know, so we're familiar with that concept. We shouldn't feel threatened by machines
2836680	2841320	that are smarter than us. We are in control of them and we still will still be in control of them.
2841960	2846360	They won't escape our control any more than our neural cortex and escape the control of our
2846360	2853080	visual memory, basically, in our brains. Thank you very much. I'll stop here and
2853720	2856120	perhaps if we have time for questions, I'll take questions.
2862520	2868280	Thank you very much. If anyone has questions, we have two mics up there. Feel free to line up.
2869160	2875880	I guess I'll get us started with one question. So on that last slide, you mentioned the possibility,
2875880	2881080	or not the possibility, just the prediction that machines will become more intelligent than humans,
2881080	2886520	in all respects. And you also mentioned throughout your talk, these algorithms that can sort of
2886520	2893480	reason and plan. Could you imagine in the near future an algorithm that, for example, could propose
2894040	2900680	physics experiments for us to conduct, like plan an experiment to answer a question that we ask it?
2902040	2908280	Yeah, actually, there's an entire field which precedes AI called experimental design.
2910040	2914200	And I mean, I think to some extent that can be formulated as an optimization problem, as a
2914200	2919640	planning problem, or as a search problem, right, trying to figure out, like, you know, how do you
2919640	2923720	maximally get information from an experiment? Like, how do you design experiments? You get the
2923720	2929320	maximum amount of information from it to either validate or invalidate a particular model that
2929320	2934600	you have, or a hypothesis you have in your mind. I think that's entirely automatable.
2936600	2942680	Now, if you want to use a generic AI system to do this, my guess is that it's not going to happen
2942680	2948440	tomorrow. The system will probably have to be relatively, you know, experienced before they're
2948440	2954520	better scientists than human scientists. Yeah, thank you. We have a question up there.
2954520	2959640	Hey, can you hear me? Yeah, yeah. Great. I was wondering if you could expand a little bit on
2959640	2963560	your assertion that you cannot build a sufficient world model from text alone.
2964760	2968280	When we think of something like a theoretical physicist, right, this person mostly interacts
2968280	2972760	with other people verbally and reading papers and thinking and writing. Or if you think about
2972760	2977560	something like a blind from birth author or person, right, they're able to actually extract a lot of
2977560	2980920	information about the structure of the world from the text. So I'm wondering if you could explain
2980920	2985720	a little bit more about why that's insufficient for, say, achieving human level AI at least.
2986760	2992200	Okay, when we do physics or mathematics, very much, very often, I mean, certainly physics, we
2992200	2998040	have mental models of the world. We have some sort of internal simulator, if you want, that can
2998040	3002280	assimilate the interesting aspect of the phenomenon that we're trying to understand.
3003240	3012440	That allows us to arrive at answers. And we don't necessarily rely on explicit
3012440	3021640	facts that we've learned through language. Let me take an example. So all of intuitive physics is
3021640	3029560	learned by observation. It's not learned through language, right? You know, if that you are going
3029640	3034600	to put a smartphone on a horizontal surface and let it go, you know, you know, it's going to,
3034600	3037560	it's going to fall one way or the other. You may not predict in which direction,
3037560	3043240	but you know, it's going to fall because of your notion of intuitive physics. If I tell you, I take
3043240	3050920	an object, I throw it in the air vertically, and it's going to have a particular velocity when
3050920	3055800	it leaves my hand, it's going to go up in the air and then fall back. What velocity will it have
3055800	3062280	when it crosses my hand at the same location where it left my hand? And if you're any kind of intuitive
3063880	3069560	notion of physics that go beyond, you know, normal intuitive physics, you would say, obviously,
3069560	3073320	it's going to have the same speed because, you know, there is conservation of momentum and energy
3073320	3079640	and stuff like that. And it's not because of energy, you know, necessarily the rule of the
3079640	3085240	explicit language rule that you have, it's because of your sort of, you know, intuition
3085240	3090520	that corresponds to that. And we do this all the time. We manipulate mental models. We do not
3090520	3098200	reason with language very often. Most of our reasoning does not use language. And so most
3098200	3105240	human knowledge is not linguistic at all. It has to do with construction of mental models,
3105880	3109800	many of which have nothing to do with language. It's certainly true of all animals. They don't have
3109800	3117960	language. So that's what sort of, you know, I mean, it's something that I think physicists,
3117960	3121720	particularly physicists should really understand, right? Because we do this all the time. Good
3121720	3127640	physicists are people who have those mental models that they can use to sort of, you know, imagine
3127640	3134360	situations and corner cases and stuff like that, that really kind of, you know, give you some insight
3134360	3140200	as to what the nature of reality is. And of course, you know, then after that, we do the math,
3140200	3144520	and that gives you sort of the internal structure of language, the mathematical language,
3144520	3148840	kind of, you know, makes you discover new properties. But a lot of it is really
3150440	3156760	intuition with mental model is true in mathematics as well in geometry and things like that.
3157400	3164280	So, you know, there's the Gedanke experiments of Einstein, right, for those are
3165240	3170040	basically mental models that you manipulate to kind of discover properties. They're not linguistic.
3174520	3181320	I have a question along similar lines, but so I agree that there's a lot of intuition involved
3181320	3187160	in learning for humans. But is there not a fundamental problem in training such intuition?
3187160	3191240	Because anything you train digitally would be encoded in some kind of language,
3192040	3198840	some binary. Is there not a fundamental obstruction there to train such intuition?
3200200	3207960	Not really, no. The input to those systems can be as, you know, continuous and kind of perceptual
3207960	3212200	as the kind of stuff that we perceive, like, you know, like video or whatever,
3214120	3220200	or sensory input, whatever it is, audio, you know, anything you want. And then inside the system,
3220200	3227320	the representation of facts and knowledge inside the system is actually just a sequence of numbers.
3227880	3236360	It's not language. It's numbers, it's vectors, you know, tensors. So I don't think that's a
3236360	3241400	problem we need to deal with really. But the texts are also broken down into
3241400	3249400	same numbers, right? Yeah, that's right. That's true. So text is to some extent simpler because
3249400	3256920	it's discrete, as I explained, it makes the, you know, the management of uncertainty easier
3256920	3261640	if you have discrete tokens. And there is a reason why language is discrete, why language
3262440	3269800	clusters in words. The reason is because language is a communication medium, right? It's a way of
3269800	3275400	communicating. And we need to communicate over noisy channels. And to be able to communicate over
3275400	3281960	noisy channels, the symbols have to be discrete. Because that allows you to do error correction,
3281960	3287480	right? To do noise, you know, to eliminate noise, right? I mean, communication engineers
3287560	3292280	are known this for decades, you know, since China and basically, and so, or even before.
3293560	3300680	So our language is discrete and goes into words, you know, the existence of phonemes and words
3300680	3306200	and things like this, because we need to be able to communicate with noisy channels. But that doesn't
3306200	3311480	mean that our thinking needs to be the same way. And in fact, our thinking is not the same way.
3311480	3320200	Language is a pale, approximate, discretized, dumbed down representation of eternal knowledge
3320200	3326200	representation recall thoughts. Yeah. And so my intuition would be that's when you
3326200	3331240	encode something digitally in binary, you're doing, you're dumbing that down anyway, right?
3331240	3336600	So even if you're processing images or videos, you're doing it in numbers and
3336680	3340680	you're doing it in the same way, maybe it's a little more complex.
3341320	3346520	But that's life. Okay. That's even physics does this biology does this, right? The communication
3346520	3351960	between synapses between two neurons, there is a finite number of physicals that are released
3351960	3357400	for the synaptic communication. And so there is granularity in this, the precision is actually
3357400	3364040	just a few bits. And, and, you know, it's actually much less than the 32 bits that we use for or
3364120	3369000	16 bits that we use for computation in neural nets. So I don't think the quantization here is,
3369000	3374040	is an issue. It certainly exists in the, in the brain as well. Communication between neurons in
3374040	3381240	the brain is binary brains, you know, the neurons actually produce spikes. For the same reason that
3382120	3385560	language is discrete is because they need to communicate in a long distance. And for this
3385560	3391640	to be efficient, it has to be digital basically. So, so I don't see this as an limitation that
3391640	3400040	would discriminate between computers and human intelligence. Thanks. Okay. Unfortunately, we
3400040	3407560	need to move on because we're running five minutes behind already. Thank you so much. This was great.
