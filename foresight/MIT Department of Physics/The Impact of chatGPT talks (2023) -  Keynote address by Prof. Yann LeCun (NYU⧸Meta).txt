So, our first speaker is Yann LeCun.
He's currently the Silver Professor at the current Institute of Mathematical Sciences
at NYU, where he is the founding director of the NYU Center for Data Science.
He is also affiliated with MEDA, formerly known as Facebook, as the Vice President and Chief
AI Scientist there.
By the way, MEDA just released one of its large language models just a few days ago called
LAMAS II, which I encourage you all to explore.
And this could be a very long introduction because Yann has a very long resume, but the
one thing I wanted to highlight is that Yann was the recipient of the 2018 Turing Award.
For those of you who aren't familiar with it, it's kind of like the Nobel Prize in Computer
Science, and he received that for his work on deep learning and convolutional neural networks.
And something interesting I learned is a lot of the work he did, early work on convolutional
neural networks, was actually when he was at Bell Labs, which is something physicists
know quite well, I think.
So I think you're all eager to hear from Yann, so I'm going to hand it over to him here.
Thank you.
Okay, I'm going to talk about, so this is going to be a somewhat technical talk, but
not very technical, but to tell you less about the possibilities that are offered by LLM
and more about their limitations.
And basically tell you about what I think is coming next, though at least what I'm working
towards coming next.
And the first thing we should realize is that machine learning really sucks compared to what
we observe in humans and animals.
The capabilities of the learning systems that we have today are really terrible.
Humans and animals can run new tasks really quickly, understand how the world works, they
can reason, they can plan, they have some level of common sense.
Their behavior is driven by objectives or drives, which is not the case for auto-reversed
LLMs.
But there is one thing that both the biological world and the recent machine learning world
have had in common is the use of cell supervised learning.
And really cell supervised learning has taken over the world.
For both applications in text and natural language understanding, for images, videos,
3D models, speech, protein folding, all that stuff.
What is cell supervised learning really?
It's sort of completion really, learning to fill in the blanks, right?
So the way it's used in the context of natural language understanding or processing is you
take a piece of text, you mask part of it by removing some other words, masking, replacing
them by blank markers, for example.
And then so think of it as a type of corruption, it doesn't have to be the masking but it could
be other types of corruptions.
And then you train some gigantic neural net to predict the words that are missing.
And you just measure the reconstruction error basically on the parts that were missing.
In the process of doing so, that system learns to represent text in a way that allows it
to store or represent meaning, grammar, everything, syntax, semantics, everything there is to
represent about language in the internal representation, which you can subsequently use for any downstream
task like say translation or topic classification or something of that type.
So this works amazingly well in the context of text, particularly because text is easy
to predict with uncertainty, you can never predict exactly what word will appear at a
particular location, but what you can do is predict some sort of probability distribution
of all possible words in the dictionary.
And you can do this because there's only a finite number of words in your dictionary
or tokens.
And so you can compute this distribution easily and handle uncertainty in the prediction pretty
well.
More generally, self-supervised learning is really sort of learning to capture dependencies
between inputs.
And so if you wanted to apply this to the problem of video prediction, for example, you would
show a segment of video to a system and then ask it to predict what's going to happen next,
for example, in the video and then reveal the future of the video.
And again, I apologize for the colors.
And then the system could adapt itself so that it does a better job at predicting what
happened next in the video.
Now, unfortunately, it's much harder to do for video than it is for text.
So much harder than it might require other methods than the type of generative methods
that work well for text.
I'll come back to this.
So speaking of generative methods, but generative AI and autoregressive language models is something
that many of us have been hearing about recently.
What is it?
Probably most of you know already, but essentially the way you train them is very similar to
the self-supervised learning.
It's in fact a special case of self-supervised learning method I just mentioned.
You take a sequence of tokens, words, whatever it is, a sequence of vectors.
As long as you can turn things into vectors, you're OK.
And then you only mask the last one and train a system to predict the last token in the
sequence.
I mean, technically, you do more than that, but that's what it comes down to in the end.
And once you have a system that has been trained to produce the next token, you can use it
autoregressively, recursively, basically to predict then the next next token and et cetera.
So you predict the next token, you shift it into the input, then predict the next next
token, shift that into the input, et cetera.
And that's called autoregressive prediction.
It's an old concept going back to signal processing many years ago, many decades ago, as a matter
of fact.
So nothing new there, but that allows the system to basically predict one token after the other
and generate text.
So those things work amazingly well.
Performance is really amazing.
The fact that, you know, they're trained only on text, even though on enormous amounts
of text, but only on text, the amount of knowledge, if you want, that they capture from text is
pretty amazing and surprised a lot of people.
Those systems typically have billions or up to hundreds of billions of parameters.
They train typically on one to two trillion tokens.
Sometimes more.
Their input window is anywhere between 2000 and maybe a few tens of thousands of tokens
for their context window.
And there's been a long history of such models that have been put out, the sort of GPT family,
starting with GPT-123, from FAIR, there's been Blunderbot, Galactica, Llama, Version
2 that just came out this week, Alpaca from Stanford, which is a fine-tuned version of
Llama, Llama, Version 1, Lambda and Bard from Google, Chinche from DeepMind, you know, and
of course, GPDT, GPT-4 from OpenAI.
And they're really good at, as writing aids, but they have really limited knowledge of
the underlying reality because they're purely trained from text, at least for the vast majority
of them.
And they really have no common sense or very limited common sense, and they have limited
abilities to plan their answers because the answers are produced autoregressively.
But still, it's pretty impressive how they work, so, as was mentioned, my colleagues
just put out an open source LLM called Llama 2.
There is three versions of this at the moment, 7 billion, 13 billion and 70 billion parameters.
The license is fairly liberal, so it can be used commercially if you want.
If you want to start a startup and use it as a business, you can.
It's also available on sort of various cloud services, easy to use.
So that very fresh just last week has been pre-trained with two trillion tokens.
The context length is 4,096, and some versions of it have been fine-tuned for dialogue and
things of that type.
It compares favorably to other systems, either open or closed source on a number of benchmarks.
But the essential characteristic of it is that it's open.
And together with the model, we released a piece of text that a lot of people signed.
The text says, we support an open innovation approach to AI, responsible and open innovation
gives us all the stakes in the AI development process, bringing visibility, equity and trust
to these technologies, opening today's LLM model will let everyone benefit from this technology.
So what you have to understand is that at the government level, there is kind of a fork
in the road where people are wondering whether AI, because it's powerful, should be kept
under lock and key and controlled and heavily regulated, or whether an open source approach
is preferable.
Yes, they are dangerous, but historically, it's quite the case that there's a lot of
evidence that open source software is actually more secure than the proprietary ones.
And the benefits, the potential benefits of AI and LLM in particular are so large that
we'll be shooting ourselves in the foot by kind of keeping this under lock and key.
So Meta is definitely on the side of open research, has been for 10 years in AI, but
it's still kind of an unsettled question, if you want.
I think personally that this will open up the possibility of an entire ecosystem built
on top of open source base LLM.
Training base LLM is very expensive, so we don't need to have 25 different proprietary
base LLM.
We basically need a few that are open source so that people can build fine-tuned products
on top of them.
There's another reason, which is that before I go back to technical questions, which is
that there's going to be a future in which all of our interactions with the digital world
are going to be mediated through AI systems, virtual systems of some type, and it's going
to become basically a repository of all human knowledge.
So we're not going to be interrogating Google or doing a literature search directly anymore.
We're just going to be talking to our AI assistant and asking a question and perhaps referring
to original material and things like that.
But basically all of our interactions with the digital world are going to be mediated
by AI systems.
So this is going to become the repository of all human knowledge.
It's going to become a basic infrastructure that everybody wants to use.
And history shows that basic infrastructure must be open source.
If you look at the history of the internet, there was a battle between commercial providers,
Microsoft, Sunmacrosystems, and others to provide the software infrastructure of the
internet.
All of those commercial providers lost.
What runs the internet today is Linux, Apache, Chrome, Firefox, JavaScript.
It's all open source.
So my prediction is the same thing is going to happen in the context of AI.
And it's necessary because a lot of countries outside the US in particular don't see with
a favorable eye the fact that their citizens are going to get all the information from
proprietary systems controlled by a small number of tech companies on the west coast
of the US.
So this is just proprietary systems are just not going to fly.
It's just not going to be acceptable to the citizenry across the world.
So it has to be open.
It's inevitable.
In fact, those systems need to be fine-tuned through what has been called RIHF, there's
various ways to fine-tune those systems.
Because the collection of human knowledge is so large, it includes things like physics,
like many of you know, it's going to require contributions from millions of people in sort
of a crowdsourcing fashion.
Because basically those systems being the repository of all human knowledge will be sort
of like Wikipedia.
Wikipedia cannot be built by a proprietary company.
It has to be, it has to gather the entire, the contribution of the entire world.
So it's going to be the same thing with AI based systems.
So open source AI is inevitable, in my opinion, and we're just sort of taking the first step.
Okay.
And so this Lama 70 billion, which is the largest of the Lama model is pretty interesting.
Those are a few examples of what it can generate.
These are extracted from the paper that you can read from the main website.
The fine-tuned system actually refuses to give you kind of illegal information.
You know, it's imperfect, but it works pretty well.
It's got ways of detecting safety and helpfulness and toxicity and things like that.
Okay.
So this is all well and good, but autoregressive LLMs really suck.
For many of us in the AI research business, LLM, the LLM revolution took place two years
ago and it's kind of old hats already.
Not the case for the public who's been kind of, you know, coming in contact with sensitivity
only in the last few months, but really they're not that great.
They don't really produce factual consistent answer.
They hallucinate or they confibrate.
They can't take into account recent information.
They're trained on information that is two years old or so, or whatever snapshot of the
crawl is used.
They're not really, it's not really possible to make them behave properly other than through
this RLHF, which is really perfect.
So you can always jailbreak them by changing the prompt and sort of asking them to kind
of act as if they were toxic.
They don't reason.
They don't really plan.
They can't do math unless you almond them with tools, which you can, of course.
And perhaps Stephen Wolfram will talk about this.
And you need to, there's a lot of work on sort of getting them to use tools such as search
engine calculators, database queries, et cetera.
Right now it's a bit of a hack the way this is done.
And the thing is we are really easily fooled by their fluency into thinking that they're
intelligent, but their intelligence is very limited and really nothing like human intelligence.
In particular, they really don't know how the world works.
They have no connection with the physical reality.
There's another reason why this, and it's basically by construction, which is that a system
that produces one token after the other, auto-regressively, is a divergent process.
It's a diffusion process with an exponential divergence.
If there is a probability at any token that is produced that the token takes you out of
the set of correct answers, those probabilities accumulate.
And the probability that a string of tokens of length n is correct is one minus this probability
of error to the power n.
So the probability of correctness decreases exponentially with the length of the
of the sequence that is produced.
This is not fixable without major redesign.
It's really an essential flaw of auto-regressive prediction.
A while ago, with a colleague, Jacob Browning, we wrote a paper that essentially points out
to the limit, it's not a technical paper, it's a philosophy paper, actually,
the philosophy magazine called Noema.
And it talks about the fact that most of human knowledge is non-linguistic.
Everything that we learned before the age of one, everything that any animal learns has
nothing to do with language.
And it's an enormous amount of background knowledge about the world that we learned in
in the first few months of life and that animals know.
None of this is linguistic at all.
And LLMs don't have access to any of this kind of knowledge.
And so the thesis in that paper is that we're not going to be able to reach human level AI
unless we have systems that have sort of direct sensor information in the form of vision, for example.
You know, some way of understanding how the world works.
Other papers that have appeared either from the cognitive science, in fact, that paper is from MIT,
or from the classical AI kind of field, point to the fact that LLMs really cannot plan.
They don't have the ability to think really or reason in a way that we understand this from humans.
And very limited abilities to plan, at least compared to other systems that are specifically
built for planning.
So I think there is three challenges in the future for AI and machine learning research.
And I've been showing this slide for several years now and I haven't changed it because of LLMs.
The first one is learning representations and predictive models of the world, where the world can
include other people that the system is talking to.
The solution to this is self-supervised learning.
We've known this for a number of years.
Learning to reason.
So autoregressive LLMs are very much like what Daniel Kahneman calls system one, which basically
corresponds to subconscious tasks in humans, tasks that you accomplish without real planning
or reasoning that you sort of accomplish more or less reactively without thinking too much.
System two is the type of action that you take by deliberate reasoning using the power
of your prefrontal cortex, using your ability to predict and then planning sequence of actions
that will sort of satisfy a particular objective.
And LLMs are not capable of this at the moment.
I'm going to argue for the fact that reasoning and planning should be viewed as some sort of energy
minimization.
And then the last thing is learning to plan complex of actions to satisfy a number of objectives.
And that will require learning hierarchical representations of action plans, which machine
learning systems don't really know how to do at the moment.
I've written this vision paper a while back called A Pass Towards Autonomous Machine Intelligence.
I kind of changed the name of this now.
I called it Objective Driven AI, but it really is the same concept.
And it built around this idea of what's called cognitive architecture.
So it's basically an architecture of different modules that interact with each other.
A perception module that basically gives the system an estimate of the state of the world
from perception that may be combined with memory.
A world model and the world model essentially is there to predict what's going to happen
in the world, perhaps as a result of actions that the agent might take,
actions that are being imagines by another module called the actor.
So the actor feeds actions to the world model.
And the world model predicts the outcome of those actions.
And then this outcome is fed to a cost module that that cost module basically
assesses whether the outcome is good or bad.
So it measures the quality of the outcome.
And the entire purpose of the agent is to figure out a sequence of actions
that minimizes that cost.
We're not talking about learning here.
We're talking about inference.
So this minimization of the cost with respect to the actions imagined by the actors
using the world model is for inference.
Okay, so inference is not just forward propagation to a few layers of neural net.
It's actually an optimization process, very much like what happens in business
nets and record models and stuff like that.
And I can describe that in more details.
So that's kind of a very simple different representations of this kind of architecture.
Perceives the world ready to a perception module that computes an abstract representation
of the state of the world, perhaps combined with content of a memory that has some other
idea with the state of the world.
Initialize your world model with that and then feed the world model with that initial
configuration combined with an imagined action sequence imagined by the actor.
And then feed the results to a number of objective functions.
A set of objectives that you can think of as guardrails that are hardwired.
And other objectives that measure whether the task was satisfied was fulfilled.
And the entire purpose of the system is to figure out a sequence of actions that
will minimize those costs at inference time.
Okay, so it cannot do anything, but I put action sequences that minimize those costs
according to the prediction that it's making from its world model.
So that's why I call this objective driven.
There's no way you can job rate that system because it's hardwired to optimize those objectives.
So unless you modify the objectives, the guardrails in particular, you're not going to be able to
have it produce toxic content, for example, if the guardrail objective includes something
like measuring toxicity. The world model very likely will need to be some sort of recurrent
model that might be multiple steps to the action. So you take, for example, two actions
and you run them through your world model twice so that you can predict in two steps
what's going to happen. And the guardrail cost can be applied at every time step.
Of course, the world is not deterministic. So the world model really,
if it's a deterministic function, needs to be fed latent variables so that there might be
multiple predictions for a single action in a single initial state.
And when you make the latent variable vary over a set or you sample them from a distribution,
you get multiple predictions. That complicates the planning process, of course, but the process
by which you figure out a sequence of actions that minimize the objectives
is a planning and reasoning procedure. Ultimately, what we really want is some
sort of ways of doing this hierarchically. And I'm going to explain this with an example.
So here is an example. Let's say I'm sitting in my office in New York at NYU and I want to
fly to Paris. I want to go to Paris. So the first action I have to do is take a taxi or
the train to the airport, either Newark or JFK. And then the second step is I need to catch a
plane to Paris. Okay, but I have a first goal, which is to get to the airport. Now that goal
can be decomposed into two sub-goals. The first one is I need to go down in the street
and tell the taxi to take me to the airport. How do I go down in the street? I need to
stand up for my chair, get out of the building and take the elevator or the stairs go down. How
do I get out from my chair? I need to activate muscles in a particular order all the way down
to millisecond by millisecond muscle control. We do this kind of hierarchical planning all the
time without even thinking about it, even though it's actually a very conscious task that we're
doing. Animals do this also. You can watch, I don't know, cats planning trajectories to kind of
jump on a piece of furniture. They're doing this kind of planning hierarchically.
We don't have any system, AI systems today, that can learn how to do this spontaneously.
There are systems that do hierarchical planning, but they're hardwired. They're built by hand.
What we need is a system that can learn the various levels of representations
of the state of the world that will allow them to do this kind of decomposition of complex tasks
into a hierarchy of simpler ones. Again, we don't have any system that can do this today at all.
This is a big challenge, I think, for the future of AI research. That's the main idea of
objective-driven AI. How can we build systems like this that can do hierarchical planning?
They can learn models of the world that predict what's going to happen in the short term with
high precision or in the long term with less precision in more abstract levels of representation.
This is where I think AI research would go over the next 10 years or so,
and this is how LLM should be built. In fact, that may be how LLM may be built in the future,
and in fact, I have a prediction which is that the type of autoreversive LLMs that we see today
will disappear within three to five years because they are not able to plan their answers.
If we had a system that was able to take a query and then in some sort of abstract
representation space was able to plan its answer, plan a representation of its answer,
and then translate this representation of the answer into fluent text using an autoregressive
decoder, for example, then we would have something that could actually be factual and
simultaneously with being fluent and be non-toxic and be easily germ-broken and be steerable.
That's my idea for where things are going. Building this and making it work is not going
to be easy and may fail, but I think that's where we should go. If we have systems like this,
we won't need any kind of RLHF or human feedback other than the type of systems that are required
to train cost modules to measure things like toxicity, for example, but we won't need to
fine-tune the system globally to be safe. We just need to put an objective so that all of the
outputs that it produces are safe, but we don't need to retrain the entire encoders and everything
for that. I think it would simplify training quite a bit, actually. Let me skip this.
We come to the question of how do we build and train this word model?
When we look at babies, babies learn in the first few months of life an enormous amount of
background knowledge about the world, mostly by observation, a little bit by interaction,
when they start to get old enough to actually act on the world, but mostly just by observation.
The type of knowledge that they learn, things like intuitive physics, gravity, inertia,
conservation of momentum, things like that, pops up only around the age of nine months.
It takes about nine months for babies to really figure this out, that objects that are not supported
will fall. But how do they do this? How do they learn this? Obviously, they don't do this like
LLMs, because if LLMs were the answer to learning like humans, first of all, we would not need
one trillion tokens to train them. Humans are not exposed to that much text information.
Reading one and a half trillion tokens for human reading eight hours a day at normal speed would
take about 20,000 years. That's obviously way more than any humans can do. But there are things
that cats and dogs and young humans can do that are pretty amazing that LLMs can't even touch,
not even remotely close. So cats and dogs can do things that robots cannot come anywhere close
to doing today, not because we can't construct the mechanical systems for it. It's just because we
can't build the intelligence for it. Any 10-year-old child can learn to clear up the dinner table and
fill up the dishwasher in minutes, probably in one shot. We do not have robots that can do that.
We don't have domestic robots. Any 17-year-old can learn to drive a car in about 20 hours of practice,
and we still don't have a limited level of autonomous driving. So that means we're missing
something really big in terms of learning that is very different from the way humans and animals
learn. And this is just another example of the Moravec paradox, which is that there are things
that seem easy for humans and turn out to be really difficult for AI and vice versa. AI systems
are much better than humans at many tasks, narrow tasks, and we are nowhere near finding
mechanisms by which machines can approach the sort of type of understanding of the world that a cat
or a dog can have. Okay, so very some idea about how we can approach that problem. And again,
it's based on self-supervised learning, learning to fill in the blanks. If we train the neural net
to do video prediction, something we've been attempting to do for 10 years now. It doesn't
work very well. If you look at the second column of this little animation at the bottom, the predictions
that are produced by the system, and this is a very stylized video, are very blurry. It's because
the system is trained to make one single prediction, and it cannot exactly predict what's going to
happen in the video. So as a result, it predicts a kind of blurry mess, which is the average of all
the possible features, plausible features that can happen. It's the same thing if you use a similar
system to predict natural video, you get those blurry predictions. So my solution to this is
something I call joint embedding predictive architecture, JEPA. And the main idea behind
JEPA is to abandon the idea that prediction needs to be generative. Okay, so the most
popular thing at the moment is generative AI, generative models. What I'm going to tell you
now is to abandon it. Okay, not a very popular idea at the moment, but here is the argument.
A generative model is one that, for which you give it an input x, let's say initial segment of a video
or a text, run it through an encoder and a predictor, and then try to predict a variable y, which
may be the continuation of that video or the continuation of that text, or the missing words
in that text, and the error by which you measure the performance of the system is basically the
some sort of divergence measure between the predicted y and the actual y. Okay, that's how
you would train this model. It's a generative model because it predicts y. A joint embedding
predictive architectures does not attempt to predict y. It attempts to predict a representation
of y. Okay, so both x and y go through encoders that compute representations,
and you perform the prediction in representation space.
And the advantage of this is that the encoder of y may have invariant properties
that map multiple y's to the same sy. And so if there are things that are very, very hard to predict,
the encoder might eliminate that information that is hard to predict or impossible to predict
from sy so that the prediction problem becomes easier. So let's say, for example, that you're
driving along the road and the predictive model here is trying to predict what's going to happen
on the road. So because it's a self-driving car, it wants to predict what the other cars on the road
are going to do. But bordering the road, there might be trees and there is wind today, so the leaves
on the trees are moving in chaotic ways. Behind the trees, there is a pond and there is ripples
on the pond because of the wind. Those ripples and the motion of the leaves are not only very hard
to predict, pretty much impossible to predict, but also very informative. There's a huge amount
of information in there. And so if you use a generative model, that generative model will
have to devote an enormous amount of resources trying to predict all of those details that
are irrelevant to the task, really. Whereas if you have a model like the one on the right, the JEPA,
the JEPA can choose to eliminate those details from the scene and only keep the details about why
that are relatively easy to predict, like the motion of the other cars, for example.
So that's my argument for the joint emitting architecture and that means abandoning generative
models. Now, of course, you want to use generative models if you want to generate,
but if what you want is to understand the world and then be able to plan, you don't need generative
models. You need those joint emitting architectures. The reason I'm advocating for this is because
experimentally, if you want to use self-supervised zoning in the context of images as opposed to text,
the only architectures that work well are joint emitting architectures.
There are architectures like the one on the left here, which is a joint emitting architecture
without the predictor. This is the most successful approach to self-supervised zoning for image
recognition. You show image X or rather image Y, then you corrupt this image Y into image X
by distorting it, blurring it, adding noise, masking some parts of it, changing the framing,
the size, etc. And then you run both images through the encoders and you force the system
or you train the system to produce representations that are identical for the two images,
so that the representation of the corrupted image is the same as the representation of the
uncorrupted image. And that builds representations that are invariant to the corruptions, essentially.
So those methods, there is a whole bunch of them, about a dozen of them, and they work really well,
whereas all the methods to learn image features that are based on reconstruction,
generative models, don't work. At least they don't work nearly as well.
So what this slide shows is kind of different versions of this joint emitting predictive
architecture, either with a predictor or without, with a predictor that can be stochastic,
having latent variables or not. And the question is how you train this, because the problem is,
if you train a system like this, without being careful, is going to collapse. If you train a
system, you give it pairs of images, let's say x and y, or video snippets, x and y, and you tell it
compute representations that are identical for x and y, the system will just collapse. It will
produce sx and xy that are constant, and then just completely ignore x and y,
so that the distance between sx and sy is minimized. So that's a collapse.
What's a, how can you correct this? And to correct this, you have to put yourself in
the context of something called energy-based models, and I'm sure there are a lot of physicists
in the room, so you can understand what that means. So energy-based models are models where you don't
explain what they do in terms of probabilistic modeling, but in terms of an energy function
that captures the dependency between the variables. So maybe a little more explicit here. Let's say
you have two variables, x and y, and your datasets are those greenish dots that are supposed to be
black. The way an energy-based model captures the dependency between x and y is that it computes
an energy function, an implicit function with a scalar output that takes x and y as an input
and gives you an energy that needs to be low near the data points, on the data points nearby,
and then higher outside of those data points, the region of high data density. And if you have
such an energy landscape, you have a function that has this, that can compute this energy landscape,
then that function will have captured the dependencies between x and y. You can infer x
from y, you can infer y from x, you can have mapping between x and y that are not functions,
because you can have multiple y's that are compatible with a single x, for example,
so it captures multi-modality without having to be a probabilistic model. Of course, in physics,
we're familiar with this, right? Very often, we write an energy function, and then we turn it
into a probability distribution over states using a Gibbs distribution. Same idea here,
but here we don't need the Gibbs distribution at all, we just manipulate the energy function
directly. How do we train a system like this? There's really two categories of training methods.
One category is contrastive methods, and those consist in generating those flashing green dots
here that are outside the manifold data, and then changing the parameters of the energy function
so that the energy takes low values on the data points and higher values on those contrastive
green points. I contributed to inventing those methods back in the early 90s, but I don't like
them anymore, because in high-dimensional spaces, the number of contrastive points you have to
generate for the energy function to take the right shape grows exponentially, and that's not a good
thing. I prefer another server approach, regularised methods, and there are those methods. I'm going
to explain this with another slide. They basically consist in minimizing the volume of space that
can take low energy through some sort of regulariser, for example, so that the system can give low
energy to the data points by changing the parameters of the energy function so that the
energy of the data points gets lower, but because it's regularised, it can only give low energy to
a small volume of space. The data points get kind of shrinkwrapped if you want in the sort of region
of low energies. So I prefer this. That seems to be more efficient, and the question is how we do
this, but what I'm asking you to do now is abandon generative models, the most popular thing at the
moment, abandon probabilistic models, the pillar of understanding machine learning, abandon contrastive
methods, which also have been very popular, and also something I've been saying for a number of
10 years, abandon reinforcement learning, because it's so damn inefficient. So those are kind of
four of the most popular approaches to machine learning at the moment, and I'm telling people to
move away from them. You can imagine I'm not being very popular here, but I'm used to that.
So how do you prevent those systems from collapsing? What you can do is measure,
have some sort of measure or information content of SX and SY across a batch, for example,
and then try to maximise that. Now, unfortunately, it's very hard to do because we don't have lower
bounds on information content. We only have upper bounds, but it turns out you can sort of do this.
So one way to prevent SX from collapsing is that you can use a criterion, which is
attempt to make sure that the variance of every component of SX over a batch is at least one.
So that's the criterion you see in the second red box below the cover. That's a hinge loss
that makes the standard deviation of each variable at least one. And then another term that makes sure
the components of SX are decorrelated. That's the covariance matrix term. So you're trying to minimise
the octagonal terms of the covariance matrix of the vectors SX over a batch.
That's not actually sufficient. So you can also use an expander. I don't have time to explain
why that works, but the resulting method is called Vicreg variance, invariance covariance
regularisation. And it's a pretty general method that can be applied to a lot of situations for
those joint embedding predictive architectures for various applications in image recognition,
segmentation, etc. It's pretty similar to another method called MCR squared invented by EMA at its
group at Berkeley. And it works really well. I'm not going to bore you with details, but there is a
standard scenario in which you do use sub-supervised learning where you pre-train a convolutional net,
let's say, using this method. And then you chop off the expander, stick a linear classifier, which
you train supervised and you measure the performance. And you get really good performance on image net
this way, particularly good performance for out-of-distribution transfer learning. There's
a modification of this method called Vicreg L, which was published last year, which is more
tuned for segmentation and things like this, but I don't have time to go into details. There's a new
method that we rolled out at CVPR just a few weeks ago called Image JEPA that uses masking and a
transformer architecture for learning features in images. And so the collapse prevention method
there is different, but the advantage of this method is that it does not require any data
augmentation other than masking. So it doesn't require to know really what type of data you're
manipulating. And it's incredibly fast and it works really well. It gives amazing results
for really, really good features. There's another set of method by some of our colleagues here at
Fair Paris called Dino, the INO. It's a different way of preventing collapse, but it has some
commonalities with IJPA. And it works really well. It gives you something like above 80% on
ImageNet, purely supervised with no fine tuning and without any data augmentation, which is pretty
amazing. But ultimately what we want to do is use this self-supervised learning and this
JEPA architecture to build the systems of the type that I talked to you about earlier that are
hierarchical. They can predict what's going to happen in the world, perhaps as a result of an
action, with some early results on training systems from video to learn good representations of
images and videos by being trained on successive frames from a video and distorted images.
I don't have time to go into the details of how this works. It's called NCJEPA.
And it is trained basically to extract good features from images for object recognition,
but also to estimate motion in a video. And it does a pretty good job at this.
So watch this paper on archive that you're invited to look at. So objective-driven AI is this idea
that we're going to have objectives that are going to drive the behavior of our system and it's going
to make it terrible and safe. And there are things that we're working on to get this to work,
self-supervised learning from video, that recipe that really works for everything. So we're working
with those JEPA architectures, but we don't think we have the ultimate recipe yet. We can use this
to build LLMs that can reason and plan, that are driven by objectives, perhaps hopefully,
learning systems that can do hierarchical planning, like animals and humans, many animals and humans.
We have many problems to solve. JEPAs with regularized written variables to deal with uncertainty,
planning algorithms in the presence of uncertainty, learning cost modules,
which could be assimilated with inverse reinforcement learning, planning with
inaccurate world models, and then exploration techniques to adjust the world model in case it's
not completely accurate. Okay, so I'm sort of concluding. There is a computing limitation
of autoregressive LLM, which is that they can only allocate a finite and fixed amount of
computational resources to producing single token. You run through, you know, 48 layers
of a transformer or something like that, you produce one token and then 48 more layers and
produce one token. And this is not too incomplete. Whereas the method I'm suggesting, the architecture
I'm suggesting that can produce an output by planning through energy minimization,
that is too incomplete, because everything can be reduced to optimization, basically.
We're still missing essential concepts to reach human ability AI, you know, this
potential technique for planning and reasoning, you know, basic techniques that we're missing to
learn world models from complex modalities like video. And perhaps in the future, we'll be able to
build systems that can plan their answers to satisfy objectives and have guardrails. I don't
believe there is such a concept as artificial general intelligence, because I think even human
intelligence is very specialized. So let's forget about general intelligence, let's try to get to
human level intelligence, perhaps, perhaps build machines that have the same sort of set of skills
and ability to learn new skills and humans. But we are very specialized. In fact, we know this
because computers are much better than us at many tasks, which means we are specialized.
There's no question that sometimes in the future, machines will surpass human
intelligence in all domains where humans are intelligent. You know, how long is it going to
take? I don't know, but there's no question is going to happen. We don't need, we don't,
we probably don't want to be threatened by that. It would be a future where every one of us would
be assisted by a system that is more intelligent than us. And we're familiar with that concept
with other humans. I only work with people who are smarter than me, or at least I try.
Or if they're not smarter than me, I try to make them smarter than me, they're called students.
And, you know, so we're familiar with that concept. We shouldn't feel threatened by machines
that are smarter than us. We are in control of them and we still will still be in control of them.
They won't escape our control any more than our neural cortex and escape the control of our
visual memory, basically, in our brains. Thank you very much. I'll stop here and
perhaps if we have time for questions, I'll take questions.
Thank you very much. If anyone has questions, we have two mics up there. Feel free to line up.
I guess I'll get us started with one question. So on that last slide, you mentioned the possibility,
or not the possibility, just the prediction that machines will become more intelligent than humans,
in all respects. And you also mentioned throughout your talk, these algorithms that can sort of
reason and plan. Could you imagine in the near future an algorithm that, for example, could propose
physics experiments for us to conduct, like plan an experiment to answer a question that we ask it?
Yeah, actually, there's an entire field which precedes AI called experimental design.
And I mean, I think to some extent that can be formulated as an optimization problem, as a
planning problem, or as a search problem, right, trying to figure out, like, you know, how do you
maximally get information from an experiment? Like, how do you design experiments? You get the
maximum amount of information from it to either validate or invalidate a particular model that
you have, or a hypothesis you have in your mind. I think that's entirely automatable.
Now, if you want to use a generic AI system to do this, my guess is that it's not going to happen
tomorrow. The system will probably have to be relatively, you know, experienced before they're
better scientists than human scientists. Yeah, thank you. We have a question up there.
Hey, can you hear me? Yeah, yeah. Great. I was wondering if you could expand a little bit on
your assertion that you cannot build a sufficient world model from text alone.
When we think of something like a theoretical physicist, right, this person mostly interacts
with other people verbally and reading papers and thinking and writing. Or if you think about
something like a blind from birth author or person, right, they're able to actually extract a lot of
information about the structure of the world from the text. So I'm wondering if you could explain
a little bit more about why that's insufficient for, say, achieving human level AI at least.
Okay, when we do physics or mathematics, very much, very often, I mean, certainly physics, we
have mental models of the world. We have some sort of internal simulator, if you want, that can
assimilate the interesting aspect of the phenomenon that we're trying to understand.
That allows us to arrive at answers. And we don't necessarily rely on explicit
facts that we've learned through language. Let me take an example. So all of intuitive physics is
learned by observation. It's not learned through language, right? You know, if that you are going
to put a smartphone on a horizontal surface and let it go, you know, you know, it's going to,
it's going to fall one way or the other. You may not predict in which direction,
but you know, it's going to fall because of your notion of intuitive physics. If I tell you, I take
an object, I throw it in the air vertically, and it's going to have a particular velocity when
it leaves my hand, it's going to go up in the air and then fall back. What velocity will it have
when it crosses my hand at the same location where it left my hand? And if you're any kind of intuitive
notion of physics that go beyond, you know, normal intuitive physics, you would say, obviously,
it's going to have the same speed because, you know, there is conservation of momentum and energy
and stuff like that. And it's not because of energy, you know, necessarily the rule of the
explicit language rule that you have, it's because of your sort of, you know, intuition
that corresponds to that. And we do this all the time. We manipulate mental models. We do not
reason with language very often. Most of our reasoning does not use language. And so most
human knowledge is not linguistic at all. It has to do with construction of mental models,
many of which have nothing to do with language. It's certainly true of all animals. They don't have
language. So that's what sort of, you know, I mean, it's something that I think physicists,
particularly physicists should really understand, right? Because we do this all the time. Good
physicists are people who have those mental models that they can use to sort of, you know, imagine
situations and corner cases and stuff like that, that really kind of, you know, give you some insight
as to what the nature of reality is. And of course, you know, then after that, we do the math,
and that gives you sort of the internal structure of language, the mathematical language,
kind of, you know, makes you discover new properties. But a lot of it is really
intuition with mental model is true in mathematics as well in geometry and things like that.
So, you know, there's the Gedanke experiments of Einstein, right, for those are
basically mental models that you manipulate to kind of discover properties. They're not linguistic.
I have a question along similar lines, but so I agree that there's a lot of intuition involved
in learning for humans. But is there not a fundamental problem in training such intuition?
Because anything you train digitally would be encoded in some kind of language,
some binary. Is there not a fundamental obstruction there to train such intuition?
Not really, no. The input to those systems can be as, you know, continuous and kind of perceptual
as the kind of stuff that we perceive, like, you know, like video or whatever,
or sensory input, whatever it is, audio, you know, anything you want. And then inside the system,
the representation of facts and knowledge inside the system is actually just a sequence of numbers.
It's not language. It's numbers, it's vectors, you know, tensors. So I don't think that's a
problem we need to deal with really. But the texts are also broken down into
same numbers, right? Yeah, that's right. That's true. So text is to some extent simpler because
it's discrete, as I explained, it makes the, you know, the management of uncertainty easier
if you have discrete tokens. And there is a reason why language is discrete, why language
clusters in words. The reason is because language is a communication medium, right? It's a way of
communicating. And we need to communicate over noisy channels. And to be able to communicate over
noisy channels, the symbols have to be discrete. Because that allows you to do error correction,
right? To do noise, you know, to eliminate noise, right? I mean, communication engineers
are known this for decades, you know, since China and basically, and so, or even before.
So our language is discrete and goes into words, you know, the existence of phonemes and words
and things like this, because we need to be able to communicate with noisy channels. But that doesn't
mean that our thinking needs to be the same way. And in fact, our thinking is not the same way.
Language is a pale, approximate, discretized, dumbed down representation of eternal knowledge
representation recall thoughts. Yeah. And so my intuition would be that's when you
encode something digitally in binary, you're doing, you're dumbing that down anyway, right?
So even if you're processing images or videos, you're doing it in numbers and
you're doing it in the same way, maybe it's a little more complex.
But that's life. Okay. That's even physics does this biology does this, right? The communication
between synapses between two neurons, there is a finite number of physicals that are released
for the synaptic communication. And so there is granularity in this, the precision is actually
just a few bits. And, and, you know, it's actually much less than the 32 bits that we use for or
16 bits that we use for computation in neural nets. So I don't think the quantization here is,
is an issue. It certainly exists in the, in the brain as well. Communication between neurons in
the brain is binary brains, you know, the neurons actually produce spikes. For the same reason that
language is discrete is because they need to communicate in a long distance. And for this
to be efficient, it has to be digital basically. So, so I don't see this as an limitation that
would discriminate between computers and human intelligence. Thanks. Okay. Unfortunately, we
need to move on because we're running five minutes behind already. Thank you so much. This was great.
