So our final speaker of the day is an external speaker.
So this is Stephen Wolfram from Wolfram Research.
So Stephen got his PhD in Caltech, PhD in physics from Caltech at the age of 20 working
on high-energy physics quantum field theory and cosmology.
He's the founder and president of Wolfram Research, developers of Mathematica and Wolfram
Alpha, tools that I'm sure you've all used.
And recently I think something interesting that Wolfram did was release a plug-in for
chat GPT, giving access to the Wolfram Alpha computational intelligence engine, which I'm
sure we will hear a little bit about.
But yeah, if you're ready to go, I'll give you the floor.
So I guess I want to talk about two things today.
I want to talk about using LLMs for physics and how physics can help study LLMs.
So to start off talking about how physics can use LLMs, the first thing is what do LLMs
fundamentally do?
LLMs have taken four billion pages from the web and a bunch of books, and they've kind
of ground that up to find the statistics of everything that's said there.
And when you ask an LLM something, its mission is to try and produce reasonable text based
on kind of the statistics of what it saw on the web and so on.
And as we'll talk about later, the things it extracts as its statistics are surprisingly
sophisticated and include having sort of found a kind of semantic grammar of language which
allows it to kind of say things that make sense, at least make some kind of sense.
They may be fact, they may be fiction, but they kind of fit together in a way that makes
sense.
But so what the LLM is fundamentally doing is it's taking stuff we humans have put on
the web, and it's feeding that back to us based on things that we ask it about.
It's feeding us back sort of reasonable things that we could have said on the web even though
we might not actually have done so.
So it's a good way to generate our, I see LLMs actually as a practical matter as kind
of an important layer of a linguistic user interface.
We have graphical user interfaces, now we have a linguistic user interface where you
can take like five points you wanted to make, you can puff those out with an LLM into a
giant report, then maybe the person you're sending that report to really wants to know
only two things, so they use an LLM again, and they grind it down and get the two things
they actually wanted to know from that report.
And that's a very sensible transport layer using kind of the big report as the transport
layer for what's going on, and maybe that will happen with academic papers as well,
that they will be just the linguistic transport layer for the actual content of what's going
on.
I would like to think that we can do better than that, in a sense, math has provided us
a notation that's better than that, computational language, the kind of thing I've worked on
for the last 40 years or so, is an attempt to kind of formalize statements about the
world in a way that's kind of better than just giving it as natural language.
But anyway, that's kind of, you know, what the LLM is doing is it's taking what's there
on the web, it's reconstituting it and feeding it back to us, it's not going to be able to
discover fundamentally new things.
Well, with a couple of exceptions there, I think one thing it can do is, if there are
analogies that might be found between this place and that place, it's really good at
finding kind of statistical facts from language.
Usually we're used to doing statistics from numbers, but LLMs manage to do statistics
from text as well.
And so if you say, well, what was the trend in fashion in 1955, there's a good chance
that the LLM will be able to take sort of the stuff that ground up from the web and
answer that.
And similarly, if you say, well, could there be an analogy between, you know, metamathematics
and general relativity, there's a chance that it could figure that out, because it can see
that the kind of the structure of what's said about those two areas has a certain similarity.
Something that seems like bizarrely magic to us humans, something that some of us humans
kind of pride ourselves on being able to figure out such analogies, but I think we may be
about to be outdone by the AIs.
But okay, so there's sort of a question of the LLM is doing this kind of taking language
from the web, giving us back some sort of reconstituted version of that.
But there's more to figure out things, and for example, physics, as it has emerged since
basically Newton and the Newton's big idea from 1687 was, as he called it, the mathematical
principles of natural philosophy.
In other words, there is a more formal method for dealing with natural philosophy, i.e. physics,
than just talking about it as philosophy, so to speak.
So this kind of notion of formalization that started in those days and has emerged into
sort of modern mathematical methods in physics.
Well, this kind of idea that you can do better than just pure thought about something.
You can have a formalization that allows you to make further progress, which is most of
the story of physics as we know it today.
So if you're a pure LLM that's just dealing with language, that's just dealing with kind
of pure common sense kinds of things, then you are stuck in the pre-Newtonian kind of
paradigm for how you do physics.
Well, so how do we kind of connect kind of this sort of linguistic layer of thinking
about things with kind of the more formal, we might now say, computational layer of thinking
about things.
So I've spent most of my life kind of trying to figure out how you can sort of describe
the world formally using computation.
And so the question is, can you connect kind of the LLM world to this computational world?
And actually, back in March, we did something with the folks at OpenAI of making a plug-in
to chat GBT that allows it to kind of use our computational language from within the
LLM.
And I'll show you, I haven't actually used this interface for a while for reasons that
I'm about to show you.
But let's see, if we say make a picture of an airy function or something, let's see.
Maybe it will probably, maybe, I don't know, you never know what it's going to do.
It's some, okay, so it says using Wolfram, that's a good sign, maybe it's going to do
the right thing, maybe not, who's to know, okay.
So this is, I wonder how it did that, okay.
So there it made a nice little plot of an airy function.
Let's see how it did it, we're going to here, okay.
So what it actually did there was it wrote a piece of Wolfram language code that just
says plot the airy function, very straightforward.
Let's say we say something like, I don't know, how far is it from, I don't know, Chicago
to Tokyo.
Wait a minute, what happens to that?
It disappeared.
What's going on?
Scrolling down.
Scrolling down.
Oh, okay.
Thanks.
I thought I was scrolling down, it didn't seem to be showing up, okay.
How lovely.
So what it did there, again, was, in this case, let's see what it did here, okay.
So in this case, it used Wolfram Alpha and just asked distance from Chicago to Tokyo,
then it got back a bunch of results from Wolfram Alpha, which it then kind of interpreted
and turned that into what it was saying.
So it's sort of interesting what's happening here, because actually we have this plug-in,
it's used a lot every day by people, and I think, at least when I last asked, which was
a few weeks ago, there's still the case that about half of the queries go to Wolfram Alpha
and half the queries go to Wolfram Language.
And if you read the prompt, you know, the prompt engineering is this bizarre activity
where you, whether you say please or not matters, whether things are in capital letters matters,
whether you repeat things at the end of the prompt after you mentioned them at the beginning,
that all matters.
I will say, by the way, that if you ask, you know, what's the skill you need to do prompt
engineering, so far as I can tell, expository writing is the number one skill needed for
good prompt engineering.
Maybe one day, and we'll talk about this later, when we talk about applying physics to LLMs,
maybe there will be actual kind of AI psychology theory that can be used, but as of right now,
I think it's expository writing, which kind of maps on to the kind of thing that the LLM
has read from the web and so on.
But in any case, the prompt here is saying, you know, if you have this kind of thing,
try and send it to WolfMalpha, one of the things that's really convenient about WolfMalpha
is that it is a thing that takes natural language's input, which is the same stuff that the LLM
is used to dealing with.
So it's kind of, it's using natural language as a transport layer, and what does WolfMalpha
do?
Well, what it's doing is to take whatever you type in, you know, if you type, I don't know,
what is the integral of, I don't know, some random thing.
What it's doing there is it's converting that question written in natural language into
precise computational language internally, or if I say something like, I don't know,
what earthquakes happened in Japan in August 1990 or something, I wonder if it can do that,
I have no idea if it can do that, but that's always living dangerously, okay, we've managed
to do that.
Once again, what happened here was it converted that natural language question into its underlying
computational language, which is our WolfMalph language system, to be able to resolve it.
I mean, just to show you how that works, you know, if I were to say here something like,
if I just said, you know, New York here, that this is a WolfMalph notebook, this thing New
York, if I say what's the input form of that, it's the entity, city, New York, New York,
United States, but it's also a thing that I can compute with.
So if I say make a, I don't know, if I say, you know, geodistance from New York to, I
don't know, London or something, it'll then just use those things as entities that it
can compute with.
So as I say, the sort of the mission of WolfMalph is convert natural language into this precise
computational language from which we can do computations based on algorithms that we've
spent the last three and a half decades, you know, setting up and based on curated knowledge
that we've accumulated over the last couple of decades.
So, you know, you can obviously mix things like, you can say things like, I don't know,
capital cities in Europe or something, and you'll get something which again, that thing
got converted into precise computational language, we can evaluate it, we can say something
like, you know, I don't know, we could say make a plot of those, all those standard kinds
of things that you can do in WolfMalph language, or we can find shortest tours, all those sorts
of things.
So, from within chat GBT, you can now access all of that functionality, so I don't know,
we could, let's try, let's try doing something ambitious, which probably won't work.
Find a shortest tour of the capital cities of Europe.
Okay, let's watch this fail, grind, grind, grind, I have no idea, I hate to even open
this to find out what horrifying thing it's actually doing in there.
Maybe, maybe, let's see, okay, it's trying something again, either because it didn't
get the answer it wanted, or for some other reason, oh, this is a bad sign, this is not
good, this looks like it's, no.
But what it's going to do, what it's doing is, every time, by the way, I mean, the way
LLAM's work, we'll maybe talk about this a bit more later, they're always writing one
token at a time, so they never have a plan for where they're going to go, they're always
just looking at what was in the past and figuring out what to say next.
So that means that it's quite often, it's kind of a hack you can use, if you get it
to generate an output and you say, is that correct?
And it will say, no, it's not correct, and how, why did you say it, well, because it
didn't know what it was going to say, it just, well, let's see, let us see, okay, wow.
Wow, wow, okay, let's see what happens, now, I have no idea if this is, okay, wait a minute,
how many, how many, wait a second, okay, let's see, well, let's see, let's see what happens
if we say plot that, and then I'm going to find out what it actually did there.
This won't work, of course.
Well, okay, this is slightly promising, I wonder if this is going to work, it's a little
bit confused there, but we'll see if it can recover itself, I don't know, let's look at
what it, let's look, to get some idea of whether this is actually right, let's look at what
it actually asked here, okay, so it asked, okay, it did something sensible here, so what
it was doing, it's a little bit confusing what it did here, and we'll see how this works
a bit better in a moment, but what it did here was it was actually using results that
it had got earlier in this whole sequence, because it actually knew the order of the cities
by now, because it must have got that in one of these previous queries here, yeah, here
we go, so it knew here, find shortest tour of those capitals, it found the shortest tour,
it then used that in the next step to go and try and find something, let's see what it
was doing down here, let's see what it got me where, no, it's still grinding away, oh
well, alright, so, but this gives some sense perhaps of how you kind of connect sort of
the LLM layer to the computational layer, but we built something recently that I think
you might like to see, which is what we call, well let's see, there's different versions
of this, what we call a chat enabled notebook, so this is using our notebook paradigm and
let me see, let me make this a little bigger, and let me just get ready to save this,
the, okay, let me tell it to use GPT-4 here, alright, so let's say we say here something like,
again, I'm going to look very dangerously, because this never does the same thing twice,
let's say solve a harmonic oscillator, and harmonic oscillator, whatever,
uh, let's see what happens, oops, so, okay, okay, I mean it'd be not the right thing for it to do,
but anyway, let's see, um, huh, okay, not terrible, let's say show me the equation,
so we'll talk in a minute about what the heck it was actually doing here,
that's not very useful, I want to know the differential equation,
if you want to visualize it, okay, let's see what it does,
let's see, you never know what this thing is going to do, so it's just kind of,
um, okay, that's not terrible, I would give that a, you know, maybe a pass and grade, I don't know,
let's see what it actually did, so here, it, okay, so it synthesized, well from language code here,
this, what these boxes look like inside, probably by next week will look a bit better than what it
does right now, but, you know, it synthesized some code here, actually, if I say here,
you know, show me the ODE, let me see whether it can do that, um, okay, great,
okay, very good, yes, yes, all good, is that right, no, yes, yes, that's right, that's okay,
um, now I say solve that, okay, not bad,
so, you know, this kind of thing I view as being a pretty useful, come on, I just want to see the
equation, um, what I want to see is the, is the code here, okay, let's say show me the code,
because this is, this is in a sense, okay, finally we got it, okay, and then what we can do here
is given this piece of code, we can just say, for example, we can just say evaluate code,
oh look, wait a minute, something is happening in the background here, it apologizes for the
inconvenience, um, who knows what it's doing, we'll check back for that in a few moments,
but back in this notebook, we're here, I can just say use that thing to copy that
code down there to the next cell and then do the evaluation and get the result, so it's kind of
interesting, you know, if I go back here, maybe I can try another example, let me show you how
this works, I can put in what we call a chat block, that basically breaks the context of the LLM,
so the LLM, whenever I'm saying, when I say show me the code, when I ask that, it's able to see the
whole, the whole conversation that it's had above it, okay, so that's um, and so now here, I broke
that by saying show me another, show me another thing here and I could say, well here, for example,
I can do this pull down and this um, this allows me to make all kinds of changes, so I could, for
example, we have this prompt repository that contains, there we go, so it contains various um,
well many, okay, we can go to the prompt repository here, this is a prompt repository that has, in
this case, it will allow us to pick personas for interacting with this, so I could pick um,
I don't know, let's try this, I have no idea what this is going to be like, okay, so as I
install the 19th century British novel persona, I've installed that, actually I kind of think we
should use Bernardo, he's fun, um, but either one, okay, so I can now pull this down, okay, we can try
19th century British novel, um, make a picture of a circle, uh, that is half red and half blue,
let's say, now I think this will, uh, oh come now, oh great, well that's, that's okay, big mess, um,
bad taste, okay, and this code snippet, blah blah blah, good luck, well let's try, all right,
let's try, let's try doing this, actually I should just stop this yacking on like this,
let's try, let's try a different persona, let's try, let's try the code assistant, but actually,
you know what, I'm going to try Bernardo, Bernardo is fun, let's, let's try re-evaluating that,
the, now what's it doing there, I don't know, this first creates a full red disk and it overlays
a hard disk, okay, I wonder if this is actually right, dah, okay, it worked, nice, what's important
about this, this maybe isn't the very best example, is you can actually read that code,
unlike the, the thing that it happened to produce before, um, and the, the kind of the idea is,
and this is by the way one big feature of, of kind of the whole computational language story
that I've spent so long on, is that, you know, our language is intended as something that you can
think in, as well as have your computer execute, so to speak, kind of like math notation would be,
it's something that, where you can actually, you know, use it as your foundation for thinking about
things, okay, anyway, that this is, you, you get the basic idea, I hope, of, of sort of this chat
notebook notion, it's, it's pretty nice, I mean, I have to say, since the reason that I haven't used
that, um, the chatGBT interface for months, is because this is really a lot nicer, you get to,
not only, you know, you can also use all the standard features of notebooks, so you can say,
this is a section about circles, and you can start putting in, maybe I could, well actually,
let me just do this, hold on, let's say, do that, I'm not going to live dangerous, do that for a sphere,
this is going to fail, of course, okay, let's see what happens, okay, interesting idea, interesting
idea, I wonder whether that will work, that's definitely an interesting idea, I give that
point, the spherical plot, that goes, wow, if this works, I'll be impressed, okay, let's run it,
wow, that's cool, it's getting smarter, the, or how, or the fine tuning that we've done and so on
is actually working, this is encouraging, um, the, because this is kind of interesting, I mean,
it made a spherical plot over a certain, you know, latitude, sequence of latitude values,
a different sequence of longitude values here, that's kind of interesting, you kind of learned
something from that, maybe we could try, let's try one other thing, which might be fun, let's say,
um, uh, show a star chart of the current position of Jupiter, now I'm probably going to have to say,
use astrographics, let's see what will happen here,
oh, come on, it just, well, I think it made that up, I'd be very surprised if these functions
actually exist, no, they do not exist, um, well, that's a bad sign, okay, let's try saying use
astro position, and what I'm expecting it's going to do, maybe,
lovely, but that's also not relevant, okay, it's not doing what I thought it would do,
which is to go read the documentation, we can, we could probably tell it to do that, let's say, um,
blah, blah, blah, there we go, now maybe it'll get a little bit smarter,
okay, this is much better, this is much, much better sign,
so hopefully, if it read the documentation, it will be able to successfully do what it was,
all right, I don't know whether it's blah, blah, blah, blah, blah, now probably if we now say,
okay, great, it's talking about all kinds of, I don't know, it's telling us how to find the
position of the large Magellanic cloud, et cetera, et cetera, et cetera, that's all fun,
and we could ask it to run that, but I think use this for the picture of Jupiter,
maybe this will work, maybe it won't, okay, this is much better, what,
you see this is the problem, it just makes stuff up, well let's see, I wonder whether this will
work, no, it made up the thing called planet marker, well we'd have to tell it not to do that,
it's supposed to go back, and I'm a little bit surprised it did that here, because actually
it has been told to go back and check the code it wrote to make sure that everything in it actually
exists, so for some reason it didn't in this case, all right, well anyway, that's a little bit on
kind of the sort of the interface between sort of LLMs, computational language, I thought another
thing I would talk about, quite different subject, is using physics to think about LLMs, so let me
pull up some things, so first question is what fundamentally is an LLM doing, as I said, what
it's ultimately doing is it's saying, given a particular piece of text, let's see if this works,
okay, so if you have something like this, you feed the prompt, the best thing about AI is its
ability to, and then its mission is to give you what the next word should be, and there are
some probabilities that it uses to do that, so if we kind of, we're interested in knowing, where's
my mouse, come on, up, just a second, sigh, you know, maybe I should just, well, okay, let me just,
that's very strange, fascinating, okay, the lost mouse, okay, the lost mouse has been found,
maybe, all right, so just, I mean, let's talk a little bit about what,
actually, let me show you something else here, so in our language, well, no, we can do it here,
we can just say, let's use a plain chat, and let's set it so that one of the parameters is,
we saw those probabilities that the LLM produced for what the next word should be,
one of the things about LLMs is they have to decide, given those probabilities, which actual
word should be picked, like one thing it could do is say, always pick the most probable word,
another thing it could do is pick those words according to the probabilities as it generated them,
there's this thing that's usually called the temperature parameter, which is an exponential
distribution thing that basically is the thing that picks, zero temperature means always pick
the most probable word, temperature one means pick the words in the probabilities that the LLM
generated itself, as you increase the temperature, it's picking more and more bizarre words, so let's
say we go here and let's say we increase the temperature to like 1.3, let's say, and we say
something like, how are you today, and it will generate some, so this is now using, okay, right,
great, okay, now let's try, let's change that temperature, let's go ahead here and just crank
up that temperature, and let's try running this again, oh my, so I'm bonkers, oh no,
okay, well at least it's stopped, often it never generates a stop token, it just keeps going forever,
the, so okay, so here's an example of a physics question, is there a phase transition as a function
of temperature in an LLM? The answer is almost certainly yes, probably around for something
like GPT-4, it's almost certainly at a temperature around 1.3 or so, maybe there are actually two
transitions that occur, actually there's a, we just had a summer school with people studying
all kinds of things, and one person at our summer school studied this question, and I have to admit,
I haven't read the thing they wrote about it, so, but I can show you, this is basically,
as a function of temperature, this is essentially an order parameter changing, and in the LLM,
and this is someplace here, this is an actual, you know, there's some innards of an LLM,
and somewhere here, there should be, okay, that's some random pieces of language code,
I think what was done here was to look at the extent to which it maintains kind of
coherence in the structure of the sentences that it produces and so on, but anyway, the thing that
I wanted to point out there is this is a very physics-like question, what, how does this work,
and one of the things we don't have right now is a kind of good qualitative physics,
overall physics-like model for an LLM, like you might say, oh, maybe it's like a spin glass,
well, it's not really like a spin glass, maybe it's like some other statistical
mechanics system, what is it really like? Well, there are a few things that we kind of know
about LLM, so I can show you some pictures, let me just show you, just to get a sense of what's
going on inside here, this is kind of a, like, let's say we're trying to learn this function,
so we've got x and y are input parameters, and we're trying to learn that function,
we're going to have a neural net, there's a neural net, and that neural net is taking those values,
x and y, and at the top it has some weights, each of the connections has a certain weight,
it indicated by the color of that, that connection, and then if we feed in particular values up at
the top there, this neural net will have been trained, will have been set up with the correct
weights, so that it will always produce a 0, 1, or minus 1 at the bottom, so for example we can,
let's just, let's say, if we try and use a very, very trivial neural net, trying to learn that
function, the totally trivial neural net will not succeed in producing that function, if we make
the neural net more sophisticated, here are some slightly more sophisticated neural nets, as the
neural net gets more sophisticated, it's going to be able to successfully learn that function,
how big does a neural net have to be to learn what level of function, not really known, I mean
there are theorems that say in principle you can do things with neural nets of certain sizes,
but the practical question we don't know, that's another kind of thing, so now you know in terms of
what, let's see, the, I mean you can do these experiments by the way, the things I've written
about, I wrote some kind of whole explainer of chat GPT, which was one of the things that I've
written fastest in my life, and it's the thing that seems to be read more, at least per unit of
time spent on writing it, it seems to have been read more than anything else I've written, which
to me is a little bit disappointing actually, but that's a different story, but anyway, so
those are some things about the innards of chat GPT, and those are some, but we can start looking
at kind of what's actually going on inside the system, and it's kind of complicated, and you
start seeing, you know, this is a condensation of the kind of innards of the brain of actually this
is GPT2, kind of a junior version of chat GPT, and this is kind of, in a sense this is taking
human knowledge and human linguistics, and crushing it down to something that's represented
in terms of arrays of numbers, and this is one of the pieces of what you see when that's done,
I mean the full chat GPT has like 175 billion weights, this is just showing a little piece of
that story. Now, okay, what can we say about what it actually does? Well, there's several
different things, so one thing that's important is this concept of embeddings, we can take
kind of, you know, words in a language, sentences, things like that, the big sort of idea of neural
nets in some sense, and it's a very old idea, dates all the way back to when neural nets were
invented in the 1940s, is don't just use digital information, use arrays of real numbers to represent
things. It's not clear that you actually need to do that, you probably don't, I don't think you need
to do it for physics, for example, but the way that neural nets are built, they are take everything,
whether it's an image, whether it's text, whatever else, and grind it into arrays of real numbers,
and then you can take those, then what you're doing is representing everything, you know,
just as in standard digital computational stuff, you're representing things as bits in neural
nets, you're representing everything in terms of arrays of real numbers, and so for example, any
old sentence, any old piece of text is ultimately represented as an array of real numbers, and
that array of real numbers we can think of as being some sort of feature vector that represents,
in some sense, some digest of the meaning of the thing that we specified. So you can start
asking in meaning space, in that space of embeddings, what can we see about what happens in that space,
and for example, let's see, we can ask questions like, how linear is that space? You know, for
example, if we do parallel transport in that space, if we look at the curvature of that space,
we're looking at, you know, this is to that, as that is to that, that's kind of the analog for
linguistics for sort of the structure of meaning of a question that you might ask in
physics of space time or something, and you can ask about these questions about curvature in that
space, I don't know all the answers to this, you can also ask things like, well, what is the trajectory
that's carved out in that space? So is there, for example, a semantic law of motion? If you start
in this particular way, is it the case that in this meaning space that you end up always tracing
through in a particular way? And one thing that seems to be the case, the space to some experience
we just did a couple weeks ago, is that the things are much more organized. So if you look at,
oh, this is kind of, sorry, let me just show you,
much of the time these trajectories aren't, in something like GPT2, the trajectories are
quite disorganized. It seems that as you get to things like GPT4, the trajectories look a lot
more organized. It's much more believable that there are semantic laws of motion, so to speak,
laws of motion in meaning space in GPT4. By the way, it's worth realizing that there's sort of a
quantum story to the whole thing because the whole thing is, it isn't just picking one trajectory,
it's picking a whole bunch of different paths. One difference from, I mean, this is a quite
different topic, but in the whole fundamental physics project that we've been doing for the
last few years where it seems like we really actually do finally understand how quantum mechanics
works, it becomes very important in that case that there is merging of different paths of history,
as well as just branching the paths of history. In the current versions of these LLMs, there's
pretty much just branching the paths of history, but you kind of get this quantum-like phenomenon
going on of all these different possible things the LLM might say that aggregate up to different
kinds of things. By the way, if you're, well, there's all kinds of interesting things to say
about LLMs as observers in thinking about physics, but maybe one thing to talk about is just what is,
this is sort of pictures of what meaning space looks like and so on, and questions like if you
have a word and it has many different sort of partially, so this is the word crane, I think,
and this is, in meaning space, this is where different sentences that mention cranes show up,
and so I think the ones at the top are cranes as a bird and the ones at the bottom
are cranes as construction equipment type thing, and you kind of see that separating, so you can
kind of get, again, it's this kind of rather physics-like thing of kind of looking at this
meaning space, and by the way, you can sort of ask things about the structure of that meaning space,
and for instance, let me see if I can show you a picture, let me see here,
maybe, okay, so in meaning space, you can ask something like, you can also do that with images,
and so you can ask, for example, we can go in meaning space, we can go from a dog image
to a cat image on the line in meaning space between a dog and a cat, and we could actually
keep going from the cat out further, we can extend that line further out in meaning space,
and we get all these kinds of weird things happening, or we can go from a plane to a cat,
and we have something very strange in the middle of those two things, or we can just go out,
this is what I was calling cat island, this is in the middle, so this was a generative AI,
not specifically an LLM, this is an image generation AI, which uses somewhat similar,
but not precisely the same technology inside, and I asked it in the middle to make a picture of a
cat in a party hat, and then as you go outwards in meaning space, you see this kind of island
of where you can see sort of identifiable cat things going on, and then further away,
it becomes more and more bizarre, and by the way, you can ask questions like, well, what's actually
out there in sort of arbitrary meaning space, and I think, well, you can look at other cat islands
here, this thing is actually in 2000 dimensional space, and these are planes in 2000 dimensional
space, different planes in 2000 dimensional space, and you see different cats live on different planes,
but sometimes you can just, if you plop into this meaning space in some random place, you'll see
things which kind of look, well, I don't know what those are, but sometimes you'll see things which
kind of have a reminiscent of kind of human forms and so on, why does all this happen, same kind of
reason as with LLMs, because this was trained from a five billion images, which were actual images
people put on the web, and those actual images are of human relevant kinds of things, with images
more so than with text, we're able to, as humans, we're able to look at things that weren't quite
right, like we looked at that high temperature version of what the LLM produced, and it looked
like garbage to us, it was incomprehensible, for images we do a little better at being able to
not be just completely confused by what's going on, but if we kind of dive in and look, you know,
what's out there in arbitrarily, let's see where do I have a picture of that, well, those are some
pictures just randomly out there in kind of meaning space, and if you go in you can see,
you know, there are weird things like this, these are, you know, people like pictures, or you can
have, you know, pictures like these, which are kind of reminiscent of sort of landscape-like
pictures, but aren't really landscape-like pictures, but this whole question about,
you know, where in meaning space, where in this, I mean, there's this, if you try and imagine,
where is the stuff that's meaningful, 10 to the minus 600 of all of meaning space is what we have
so far explored as with sort of human language and so on, it's a very small fraction of it,
with respect to images. Okay, so just to maybe finish off a bit, we could talk more about this
kind of thing, but just to talk a little bit about sort of the physics of LLMs and so on,
I think one of the things people, what one wants to do is, is there a kind of a narrative story of
what LLMs are finding? Is there a way of saying, why do LLMs even work? It's not obvious that,
you know, given that you, you know, you could say, take a sentence like, the cat sat on the,
okay, based on just looking at pages on the web, you can reasonably guess the next word is going
to be math, but by the time you've got a long prompt where you're asking it some physics question
or something, there's no way that actual detailed text is going to be somewhere on the web, or
probably not, unless it was some exercise or a book or something, but most of the time it won't
be something that was on the web. So you have to have an actual model that allows you to extrapolate
the model that's being used in chat GBT as a neural net. It is far from obvious. There's no
fundamental reason to think it would be true that the way the neural net extrapolates will agree with
the way we humans think it makes sense to extrapolate. The fact that it extrapolates to produce things
that seem meaningful to us humans is a nontrivial scientific result. And, you know, I think what
it's basically telling us is the way brains work is actually pretty well modeled by sort of neural
net type things. And that's why the things that brains extrapolate with are pretty close to the
things that these simple neural nets extrapolate with. So then the question is, well, okay, we've
got this kind of extrapolation that's going on. We've got some, this thing is finding out some
way to extrapolate. How is it doing that? Well, what regularities in language is it picking up to
allow it to make meaningful sentences, meaningful text? Well, there's one big regularity that we
know about in language, which is syntactic grammar. We know how you put parts of speech together,
nouns and verbs and things like this. So in a sense, we can then construct sentences which
are syntactically correct. But there are infinite number of sentences that are syntactically correct
but complete nonsense. And that's, it's doing much better than just producing syntactically correct
sentences. So what's it doing? Well, there's one good example of a place where we know a structure
in sentences that exists that isn't sort of purely syntactic. And that's logic. And you can kind of
think, you know, when Aristotle invented logic back a couple of thousand years ago, you know,
what was he actually doing? Well, he was a bit like a machine learning system, because what he was
effectively doing was saying, I've got all these examples of rhetoric. People make an argument
that looks like this, but I can take something which instead of it being a discussion about, you
know, Sparta and Athens, it can be a discussion about turtles and fishes. It doesn't matter. I can
just replace those symbolically with P and Q and I can look at this sort of formal structure of these
sentences. In a sense, you can lift logic out of the specifics of actual language, in his case,
Greek. But in a sense, what LLMs have done is they've discovered the same thing. So people say,
oh, my gosh, it's amazing, you know, LLMs have discovered logic. Well, they discovered logic,
I think the same way Aristotle discovered logic, and you can find out they're basically doing
so logistic logic. And if you try and feed them things which require sort of more formal, more
formal kinds of things, even at the level of, you know, parenthesis matching and so on, they will
fail after you get sort of too many parentheses to match. They don't do kind of the formal level
of things. They don't do the computational thing. They do the kind of level of things that in a sense
was the original way that logic was discovered. So that's a place where kind of one's able to lift
something more semantic out of this kind of layer of pure language. But presumably, there is more
that can be done along those lines. Presumably, there is kind of a semantic grammar of language,
which in a sense, the LLMs have discovered something about language and common sense
reasoning and so on. That is that there's this sort of thing you can lift out of language
that allows you to kind of put together meaningful stuff beyond just the purely syntactic. And I
think that's the thing where, well, I've been interested in this actually for a long time
for different reasons, this kind of idea of sort of making a symbolic discourse language that allows
you to sort of express things in a kind of, in a way that is sort of, is a symbolic way of expressing
things that is not specific to the particulars of language. In a sense, the whole enterprise of
making a computational language has got a certain distance with that, describing certain kinds of
things in the world. But anyway, I think that there are many pieces of kind of what happens in
LLMs. For example, why does few shot learning work? Why does it work to tell LLM and LLM to talk
like a pirate? Why does it, how does that manage to place it somewhere in meaning space or something
so that the kind of, you know, you placed it somewhere by giving that prompt, then somehow
the semantic law of motion takes over and it successfully manages to produce semantically
meaningful stuff. We don't know how any of this works. It's a great topic for physicists, I have
to say. I think it's one of these places where it isn't particularly easy. It's something where,
you know, this space of, you know, this sort of meaning space we're looking at with these images,
we can sort of see things about what's out there in meaning space in a way it's a little bit easier
than with text and words. But we're kind of, you know, this is sort of just the beginning of,
in a sense, physicalizing using something like statistical mechanics
to try and analyze what's happening inside an LLM. So I think kind of to sort of summarize,
I mean, I've talked about two kinds of things. One is just the very practical aspects of using
LLMs to, I think the most significant workflow there is this. You have a vague idea of what you
want to do. Now I have to say to get that vague idea, you have to have an ability to sort of
think computationally about things until you can express yourself in some kind of sort of
with computational concepts. I mean, it's no good, you know, with some notion of how you think about
the world computationally. Once you have that, you can kind of write a piece of natural language,
you go sort of tell that to the LLM. The LLM will then write, you know, will write
Wolfram language code or whatever, sometimes correctly, that will be an expression of what
it thought you meant by the things that you said in natural language. Now sometimes when you look
at that Wolfram language code, you'll say you misunderstood. It wasn't correct. You can fix
that code or you can tell it to go fix the code or whatever else. But so the workflow is, you know,
computationally imagine what you want to do, write it in natural language, have it kind of
translated into computational language, then read the computational language. It's very important
that something you can do with Wolfram language, no other, you know, that's the story of computational
language, very different from programming languages which weren't intended for humans to read
particularly. But so that's something where you read that computational language, you understand
what it said, you fix it if you need to, then you say run that, then that becomes a sort of brick
that you can use to build a whole tower of what you want on top of. And so that's, I think that's
the workflow and, you know, I have to say, as we make these chat notebooks better, it's getting
closer to the point where it actually makes sense, even if you know Wolfram language well,
to try and use it as a way to get things started if you're not thinking very clearly, so to speak.
Although as I say, to get the prompt right, you have to be kind of think expository writing because
if you're totally confused, the LLM will be confused as well. But anyway, so the first thing I was
talking about was this idea of how do we make use of LLMs mostly as a way to kind of get a leg up
on creating kind of computational language to be able to actually do computations. I should say,
by the way, I'm happy to talk about this, people interested if we have any time. But there's many
use cases, like for example, we're working on a bunch of AI tutoring applications. Another use
case I mentioned for physics, we've never been able to do, in Wolfram Alpha for example, we've
never been able to do physics word problems. We can do that once you've turned the word problem
into equations, for example, we can we can nail it. But turning going from the whole long textual
description into the equations is not something we've been able to do. Now we can. Now, in fact,
in practice, when you use LLMs, one of the things that's terrible, you know, you use the for example,
a chat notebook or the Wolfram plugin for chat GBT, it'll sometimes, you know, correctly untangle
the word problem, you know, solve the equations correctly. And then at the last minute, give the
wrong answer, because it tried to inject something that it thought it knew, and it got very confused.
But anyway, so lots of use cases for kind of the LLMs, their interaction with computational
language. And then the second piece, really quite a disjoint piece is why did the LLMs work in the
first place? This is a physics problem. And people should figure it out. And the results of
figuring it out will be many important things. For example, probably most of what's inside a modern
LLM doesn't need to be there. Most of what's it, you know, the actual structure you need to know
enough to be able to do sort of linguistic interface, plus kind of enough common sense to
support that linguistic interface is probably quite tiny compared to a current LLM. And probably
you can delegate all the kind of computation and detailed computational knowledge outside of the
LLM, which is an important thing in practice in terms of how much it costs to run an LLM,
what kind of systems you need to run it on. But if we understand LLMs better and why they
work in the first place, we have a better chance to be able to resolve those kinds of things.
All right, I should stop there. Thanks.
Very much. I think we have time for a couple of questions. I'd like to start with a quick one,
slightly out of left field. I think you've made a good case here for physicists becoming linguists.
Is there something that physicists should be learning from linguists or linguists should
be transitioning to physics? Physics can give us, I don't know whether it's linguistics,
I don't know whether you call it that. I don't know what you call it. But this whole idea about
meaning and so on, what we're talking about, that's something that I think has now been exposed
as something on which we can do experimental science on, on which we can apply physics.
So I think that's the, I mean, in terms of, yeah, no, that's, I mean, it's, you know,
if you look at the history of physics, right, physics has been a fantastic export field.
That's, you know, populated molecular biology, it's populated, you know, quantitative finance,
it's populated lots of kinds of things. It has every opportunity to populate this area
and to populate and to really make some complete change to how one thinks about sort of meaning
and language and so on, I believe. Well, thank you. All right, let's go to the right first.
Hi, so I've seen some of your talks on the Wolfram physics project as well,
and I see these n-dimensional graphs that you often use, and they often really look like neural
networks. And so I wanted to ask if that was intentional or if there's some additional layers
of physics going on there. They have nothing to do with neural nets. So far as I know,
although there's at least one startup that believes they do, and we'll see how that works out.
This is an utterly disjoint discussion about how kind of microscopic
hypergraphs, you know, from them emerge space-time and quantum mechanics and so on.
There is in fact a bizarre connection. Okay, this is to the deepest level of the rabbit hole
immediately. There's this thing we call the rouliad, which is the entangled limit of all
possible computations. Imagine you take all possible, let's say, Turing machines with all
possible initial states. You run them, and they're all non-deterministic. They all have all possible
rules. You run them, you get this big, messy thing. There's only one of it. It is the complete
representation of all possible computations. And then that, I claim, is sort of the ultimate
foundation of physics and mathematics, actually. And our physical universe ends up being,
we have to exist within that. And so our physical universe ends up being our kind of, our sampling
of that rouliad object. And here's the fascinating fact, at least I think it's interesting, is that
if you assume that we as observers of the rouliad have two characteristics. One, we are computationally
bounded. Two, we believe we are persistent in time. We believe we have a single thread of
experience. Those two attributes alone are sufficient to give you, not just qualitatively,
but exactly, general relativity and quantum mechanics. That's kind of exciting, because it
tells you that it didn't need to be that way. The aliens who don't have those characteristics
don't have to have general relativity and quantum mechanics. But it kind of gives you, I mean,
this is a huge condensation of a very large amount of stuff. But that's, so okay, how does
that relate to all of this? When I was showing you those weird cat pictures and things, the,
this is a, one of the reasons I was studying weird cat pictures is because this is a way of
understanding sort of different slices of this roulial space concept. There's much more to say
about this. That's a way too, way too brief a description. Okay, let's take a question from
the left now. Yeah, hi. So I've tried to use LLMs in research so far without great success.
I'm a theoretical physicist. Something that would be, there's a weird echo here, I don't know.
Anyway, sorry, something that would be really useful would be if I could have something where,
you know, 300 page paper, I don't know, by Edward and Pierce. And I can ask it, can you give me a
one page summary of that, you know, where it would be correct and where it would already kind of know
from talking me to before, like what are the kind of things that I know and then I don't know.
So I mean, how far are we from that? Well, you know, for example, in our company,
you know, someone makes a daily digest of interesting papers about LLMs. Okay, and I got fed up
trying to read the abstracts, because every abstract is written differently. They're very
ponderous in many cases. I said, just get the frigging LLMs and make a two sentence summary
of every paper. It works great. I mean, you can scan down this thing really quickly. The fact that
the LLMs text is rather boring is actually good, because all the text is consistent, and you kind
of can just see what's happening. Now, you know, do I get the essence of every paper correctly?
Maybe not. But that's a statistical thing anyway, I might miss it from the abstract too. So that's
a pretty good use case that I recommend, actually. In terms of the can you, if you want it to summarize
for you, particularly, I think that's coming. And the, you know, kind of AI tutoring system that
we're building, that's kind of one of the big ideas is know the student and be able to figure out,
first of all, how is the student confused? Because one of the things that you typically can't do
in sort of watching what a student does is watching the working that the student follows.
But you can with an LLM, and you can kind of see, you know, how is the student confused as the
student is doing their work? And then, so then the question is, will it come to the point where,
you know, the LLM will know enough about me from having read, I mean, me personally, I've put 50
million words out there. So it's, I'm pretty easy to learn about. And we're trying to get an LLM to
be me, so to speak, which will save time, maybe, maybe not. But anyway, the point is,
I'm, you know, given that the LLM knows about you, I think there is a very good chance that the LLM
will be able to say the one thing you need to know, because you're confused about this or you don't
know this, is this one fact. And you say, oh, that's the thing I want to know from that paper, all the
other stuff is irrelevant. I think that's pretty realistic. And I think it's reasonably short-term.
But you've got to understand, like everything with machine learning, it's kind of an 80% success,
90% success story. And whenever you have a situation, like looking at these abstracts, where,
you know, if I notice an abstract that looks really interesting, it's a win. If I miss one,
it's not a disaster. That's a good use case. If it's a case where you're trying to do the next
great, you know, precise physics calculation, it's probably a big lose to use an LLM where it
might be wrong 10% of the time, you don't know which 10%. Okay, I understand that there are
many, many questions, but at some point, everyone does have to go home, unfortunately.
So I'm afraid we're going to have to cut off the questioning there. Let's thank our speaker again.
