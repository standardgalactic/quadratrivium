start	end	text
0	4960	So our final speaker of the day is an external speaker.
4960	9120	So this is Stephen Wolfram from Wolfram Research.
9120	15240	So Stephen got his PhD in Caltech, PhD in physics from Caltech at the age of 20 working
15240	18920	on high-energy physics quantum field theory and cosmology.
18920	23440	He's the founder and president of Wolfram Research, developers of Mathematica and Wolfram
23440	26640	Alpha, tools that I'm sure you've all used.
26640	30520	And recently I think something interesting that Wolfram did was release a plug-in for
30520	36040	chat GPT, giving access to the Wolfram Alpha computational intelligence engine, which I'm
36040	38720	sure we will hear a little bit about.
38720	44200	But yeah, if you're ready to go, I'll give you the floor.
44200	46520	So I guess I want to talk about two things today.
46520	53580	I want to talk about using LLMs for physics and how physics can help study LLMs.
53580	63460	So to start off talking about how physics can use LLMs, the first thing is what do LLMs
63460	64460	fundamentally do?
64460	70740	LLMs have taken four billion pages from the web and a bunch of books, and they've kind
70740	75580	of ground that up to find the statistics of everything that's said there.
75580	80820	And when you ask an LLM something, its mission is to try and produce reasonable text based
80820	84700	on kind of the statistics of what it saw on the web and so on.
84700	89220	And as we'll talk about later, the things it extracts as its statistics are surprisingly
89220	94660	sophisticated and include having sort of found a kind of semantic grammar of language which
94660	99500	allows it to kind of say things that make sense, at least make some kind of sense.
99500	102500	They may be fact, they may be fiction, but they kind of fit together in a way that makes
102500	103500	sense.
103500	109220	But so what the LLM is fundamentally doing is it's taking stuff we humans have put on
109220	113620	the web, and it's feeding that back to us based on things that we ask it about.
113620	118220	It's feeding us back sort of reasonable things that we could have said on the web even though
118220	120500	we might not actually have done so.
120500	126700	So it's a good way to generate our, I see LLMs actually as a practical matter as kind
126700	129540	of an important layer of a linguistic user interface.
129540	133420	We have graphical user interfaces, now we have a linguistic user interface where you
133420	138860	can take like five points you wanted to make, you can puff those out with an LLM into a
138860	143700	giant report, then maybe the person you're sending that report to really wants to know
143700	147980	only two things, so they use an LLM again, and they grind it down and get the two things
147980	151060	they actually wanted to know from that report.
151060	156380	And that's a very sensible transport layer using kind of the big report as the transport
156380	161340	layer for what's going on, and maybe that will happen with academic papers as well,
161340	166340	that they will be just the linguistic transport layer for the actual content of what's going
166340	167340	on.
167340	172500	I would like to think that we can do better than that, in a sense, math has provided us
172500	176620	a notation that's better than that, computational language, the kind of thing I've worked on
176620	180740	for the last 40 years or so, is an attempt to kind of formalize statements about the
180740	186420	world in a way that's kind of better than just giving it as natural language.
186420	191180	But anyway, that's kind of, you know, what the LLM is doing is it's taking what's there
191180	195580	on the web, it's reconstituting it and feeding it back to us, it's not going to be able to
195580	197900	discover fundamentally new things.
197900	202660	Well, with a couple of exceptions there, I think one thing it can do is, if there are
202660	206580	analogies that might be found between this place and that place, it's really good at
206580	209860	finding kind of statistical facts from language.
209860	214180	Usually we're used to doing statistics from numbers, but LLMs manage to do statistics
214180	216420	from text as well.
216420	222620	And so if you say, well, what was the trend in fashion in 1955, there's a good chance
222620	227260	that the LLM will be able to take sort of the stuff that ground up from the web and
227260	228460	answer that.
228460	233140	And similarly, if you say, well, could there be an analogy between, you know, metamathematics
233140	238340	and general relativity, there's a chance that it could figure that out, because it can see
238340	243660	that the kind of the structure of what's said about those two areas has a certain similarity.
243660	249580	Something that seems like bizarrely magic to us humans, something that some of us humans
249620	253540	kind of pride ourselves on being able to figure out such analogies, but I think we may be
253540	256780	about to be outdone by the AIs.
256780	262540	But okay, so there's sort of a question of the LLM is doing this kind of taking language
262540	267300	from the web, giving us back some sort of reconstituted version of that.
267300	273340	But there's more to figure out things, and for example, physics, as it has emerged since
273340	280660	basically Newton and the Newton's big idea from 1687 was, as he called it, the mathematical
280660	282220	principles of natural philosophy.
282220	288140	In other words, there is a more formal method for dealing with natural philosophy, i.e. physics,
288140	291980	than just talking about it as philosophy, so to speak.
291980	296660	So this kind of notion of formalization that started in those days and has emerged into
296660	300020	sort of modern mathematical methods in physics.
300020	306500	Well, this kind of idea that you can do better than just pure thought about something.
306500	310460	You can have a formalization that allows you to make further progress, which is most of
310460	312980	the story of physics as we know it today.
312980	318060	So if you're a pure LLM that's just dealing with language, that's just dealing with kind
318060	323820	of pure common sense kinds of things, then you are stuck in the pre-Newtonian kind of
323820	326580	paradigm for how you do physics.
326580	333300	Well, so how do we kind of connect kind of this sort of linguistic layer of thinking
333300	339460	about things with kind of the more formal, we might now say, computational layer of thinking
339460	340540	about things.
340540	346300	So I've spent most of my life kind of trying to figure out how you can sort of describe
346300	349420	the world formally using computation.
349420	354820	And so the question is, can you connect kind of the LLM world to this computational world?
354820	359940	And actually, back in March, we did something with the folks at OpenAI of making a plug-in
359940	368140	to chat GBT that allows it to kind of use our computational language from within the
368140	369140	LLM.
369140	371860	And I'll show you, I haven't actually used this interface for a while for reasons that
371860	373420	I'm about to show you.
373420	383220	But let's see, if we say make a picture of an airy function or something, let's see.
383220	386740	Maybe it will probably, maybe, I don't know, you never know what it's going to do.
386740	391700	It's some, okay, so it says using Wolfram, that's a good sign, maybe it's going to do
391700	396980	the right thing, maybe not, who's to know, okay.
396980	399660	So this is, I wonder how it did that, okay.
399660	401900	So there it made a nice little plot of an airy function.
401900	405340	Let's see how it did it, we're going to here, okay.
405340	408620	So what it actually did there was it wrote a piece of Wolfram language code that just
408620	411740	says plot the airy function, very straightforward.
411740	420860	Let's say we say something like, I don't know, how far is it from, I don't know, Chicago
420860	421940	to Tokyo.
421940	429100	Wait a minute, what happens to that?
429100	430700	It disappeared.
430700	431700	What's going on?
431700	432700	Scrolling down.
432700	433700	Scrolling down.
433700	434700	Oh, okay.
434700	435700	Thanks.
435700	439260	I thought I was scrolling down, it didn't seem to be showing up, okay.
439260	440260	How lovely.
440460	445820	So what it did there, again, was, in this case, let's see what it did here, okay.
445820	451540	So in this case, it used Wolfram Alpha and just asked distance from Chicago to Tokyo,
451540	456220	then it got back a bunch of results from Wolfram Alpha, which it then kind of interpreted
456220	459100	and turned that into what it was saying.
459100	463100	So it's sort of interesting what's happening here, because actually we have this plug-in,
463100	468900	it's used a lot every day by people, and I think, at least when I last asked, which was
468900	473380	a few weeks ago, there's still the case that about half of the queries go to Wolfram Alpha
473380	475580	and half the queries go to Wolfram Language.
475580	479580	And if you read the prompt, you know, the prompt engineering is this bizarre activity
479580	484380	where you, whether you say please or not matters, whether things are in capital letters matters,
484380	488300	whether you repeat things at the end of the prompt after you mentioned them at the beginning,
488300	489300	that all matters.
489300	492660	I will say, by the way, that if you ask, you know, what's the skill you need to do prompt
492660	497500	engineering, so far as I can tell, expository writing is the number one skill needed for
497500	499700	good prompt engineering.
499700	505100	Maybe one day, and we'll talk about this later, when we talk about applying physics to LLMs,
505100	510700	maybe there will be actual kind of AI psychology theory that can be used, but as of right now,
510700	515580	I think it's expository writing, which kind of maps on to the kind of thing that the LLM
515580	518100	has read from the web and so on.
518100	525180	But in any case, the prompt here is saying, you know, if you have this kind of thing,
525340	531860	try and send it to WolfMalpha, one of the things that's really convenient about WolfMalpha
531860	536980	is that it is a thing that takes natural language's input, which is the same stuff that the LLM
536980	539020	is used to dealing with.
539020	543420	So it's kind of, it's using natural language as a transport layer, and what does WolfMalpha
543420	544420	do?
544420	549380	Well, what it's doing is to take whatever you type in, you know, if you type, I don't know,
549380	557860	what is the integral of, I don't know, some random thing.
557860	564500	What it's doing there is it's converting that question written in natural language into
564500	570780	precise computational language internally, or if I say something like, I don't know,
570780	580420	what earthquakes happened in Japan in August 1990 or something, I wonder if it can do that,
580420	584940	I have no idea if it can do that, but that's always living dangerously, okay, we've managed
584940	585940	to do that.
585940	594380	Once again, what happened here was it converted that natural language question into its underlying
594380	598660	computational language, which is our WolfMalph language system, to be able to resolve it.
598660	602780	I mean, just to show you how that works, you know, if I were to say here something like,
602780	609460	if I just said, you know, New York here, that this is a WolfMalph notebook, this thing New
609460	614260	York, if I say what's the input form of that, it's the entity, city, New York, New York,
614260	617300	United States, but it's also a thing that I can compute with.
617300	625620	So if I say make a, I don't know, if I say, you know, geodistance from New York to, I
626580	632260	don't know, London or something, it'll then just use those things as entities that it
632260	633660	can compute with.
633660	640180	So as I say, the sort of the mission of WolfMalph is convert natural language into this precise
640180	645860	computational language from which we can do computations based on algorithms that we've
645860	651700	spent the last three and a half decades, you know, setting up and based on curated knowledge
651700	655420	that we've accumulated over the last couple of decades.
655420	662140	So, you know, you can obviously mix things like, you can say things like, I don't know,
662140	669340	capital cities in Europe or something, and you'll get something which again, that thing
669340	674780	got converted into precise computational language, we can evaluate it, we can say something
674780	679740	like, you know, I don't know, we could say make a plot of those, all those standard kinds
679740	683580	of things that you can do in WolfMalph language, or we can find shortest tours, all those sorts
683580	684580	of things.
684780	692460	So, from within chat GBT, you can now access all of that functionality, so I don't know,
692460	697140	we could, let's try, let's try doing something ambitious, which probably won't work.
697140	702580	Find a shortest tour of the capital cities of Europe.
702580	712580	Okay, let's watch this fail, grind, grind, grind, I have no idea, I hate to even open
712580	716580	this to find out what horrifying thing it's actually doing in there.
716580	723060	Maybe, maybe, let's see, okay, it's trying something again, either because it didn't
723060	727460	get the answer it wanted, or for some other reason, oh, this is a bad sign, this is not
727460	733460	good, this looks like it's, no.
733460	736700	But what it's going to do, what it's doing is, every time, by the way, I mean, the way
736700	740180	LLAM's work, we'll maybe talk about this a bit more later, they're always writing one
740220	744980	token at a time, so they never have a plan for where they're going to go, they're always
744980	749260	just looking at what was in the past and figuring out what to say next.
749260	752820	So that means that it's quite often, it's kind of a hack you can use, if you get it
752820	755900	to generate an output and you say, is that correct?
755900	760740	And it will say, no, it's not correct, and how, why did you say it, well, because it
760740	769740	didn't know what it was going to say, it just, well, let's see, let us see, okay, wow.
770300	779300	Wow, wow, okay, let's see what happens, now, I have no idea if this is, okay, wait a minute,
779300	786620	how many, how many, wait a second, okay, let's see, well, let's see, let's see what happens
786620	794860	if we say plot that, and then I'm going to find out what it actually did there.
794940	798940	This won't work, of course.
798940	806940	Well, okay, this is slightly promising, I wonder if this is going to work, it's a little
806940	811860	bit confused there, but we'll see if it can recover itself, I don't know, let's look at
811860	816260	what it, let's look, to get some idea of whether this is actually right, let's look at what
816260	823900	it actually asked here, okay, so it asked, okay, it did something sensible here, so what
823940	828180	it was doing, it's a little bit confusing what it did here, and we'll see how this works
828180	833700	a bit better in a moment, but what it did here was it was actually using results that
833700	838700	it had got earlier in this whole sequence, because it actually knew the order of the cities
838700	842860	by now, because it must have got that in one of these previous queries here, yeah, here
842860	848460	we go, so it knew here, find shortest tour of those capitals, it found the shortest tour,
848460	852060	it then used that in the next step to go and try and find something, let's see what it
852140	856940	was doing down here, let's see what it got me where, no, it's still grinding away, oh
856940	864460	well, alright, so, but this gives some sense perhaps of how you kind of connect sort of
864460	870380	the LLM layer to the computational layer, but we built something recently that I think
870380	875260	you might like to see, which is what we call, well let's see, there's different versions
875260	880660	of this, what we call a chat enabled notebook, so this is using our notebook paradigm and
881140	887060	let me see, let me make this a little bigger, and let me just get ready to save this,
889380	896900	the, okay, let me tell it to use GPT-4 here, alright, so let's say we say here something like,
898580	901620	again, I'm going to look very dangerously, because this never does the same thing twice,
902580	907460	let's say solve a harmonic oscillator, and harmonic oscillator, whatever,
908020	922500	uh, let's see what happens, oops, so, okay, okay, I mean it'd be not the right thing for it to do,
922500	932580	but anyway, let's see, um, huh, okay, not terrible, let's say show me the equation,
938260	941460	so we'll talk in a minute about what the heck it was actually doing here,
942100	945380	that's not very useful, I want to know the differential equation,
946980	949220	if you want to visualize it, okay, let's see what it does,
953460	959060	let's see, you never know what this thing is going to do, so it's just kind of,
960020	967380	um, okay, that's not terrible, I would give that a, you know, maybe a pass and grade, I don't know,
968340	975140	let's see what it actually did, so here, it, okay, so it synthesized, well from language code here,
976260	981540	this, what these boxes look like inside, probably by next week will look a bit better than what it
981540	988180	does right now, but, you know, it synthesized some code here, actually, if I say here,
989460	998660	you know, show me the ODE, let me see whether it can do that, um, okay, great,
1001060	1008820	okay, very good, yes, yes, all good, is that right, no, yes, yes, that's right, that's okay,
1009780	1016580	um, now I say solve that, okay, not bad,
1019860	1027700	so, you know, this kind of thing I view as being a pretty useful, come on, I just want to see the
1027780	1034980	equation, um, what I want to see is the, is the code here, okay, let's say show me the code,
1038260	1045140	because this is, this is in a sense, okay, finally we got it, okay, and then what we can do here
1045140	1049380	is given this piece of code, we can just say, for example, we can just say evaluate code,
1049380	1054580	oh look, wait a minute, something is happening in the background here, it apologizes for the
1054580	1064260	inconvenience, um, who knows what it's doing, we'll check back for that in a few moments,
1065620	1073300	but back in this notebook, we're here, I can just say use that thing to copy that
1073860	1080100	code down there to the next cell and then do the evaluation and get the result, so it's kind of
1080100	1084500	interesting, you know, if I go back here, maybe I can try another example, let me show you how
1084500	1089700	this works, I can put in what we call a chat block, that basically breaks the context of the LLM,
1089700	1095780	so the LLM, whenever I'm saying, when I say show me the code, when I ask that, it's able to see the
1095780	1105060	whole, the whole conversation that it's had above it, okay, so that's um, and so now here, I broke
1105060	1111140	that by saying show me another, show me another thing here and I could say, well here, for example,
1111140	1116980	I can do this pull down and this um, this allows me to make all kinds of changes, so I could, for
1116980	1124740	example, we have this prompt repository that contains, there we go, so it contains various um,
1124740	1129940	well many, okay, we can go to the prompt repository here, this is a prompt repository that has, in
1130020	1135300	this case, it will allow us to pick personas for interacting with this, so I could pick um,
1135940	1139700	I don't know, let's try this, I have no idea what this is going to be like, okay, so as I
1139700	1145540	install the 19th century British novel persona, I've installed that, actually I kind of think we
1145540	1152180	should use Bernardo, he's fun, um, but either one, okay, so I can now pull this down, okay, we can try
1152180	1168580	19th century British novel, um, make a picture of a circle, uh, that is half red and half blue,
1168580	1182900	let's say, now I think this will, uh, oh come now, oh great, well that's, that's okay, big mess, um,
1185140	1192420	bad taste, okay, and this code snippet, blah blah blah, good luck, well let's try, all right,
1192420	1197780	let's try, let's try doing this, actually I should just stop this yacking on like this,
1198420	1203140	let's try, let's try a different persona, let's try, let's try the code assistant, but actually,
1203140	1209540	you know what, I'm going to try Bernardo, Bernardo is fun, let's, let's try re-evaluating that,
1213540	1219460	the, now what's it doing there, I don't know, this first creates a full red disk and it overlays
1219460	1225780	a hard disk, okay, I wonder if this is actually right, dah, okay, it worked, nice, what's important
1225780	1230420	about this, this maybe isn't the very best example, is you can actually read that code,
1230980	1236580	unlike the, the thing that it happened to produce before, um, and the, the kind of the idea is,
1236580	1240740	and this is by the way one big feature of, of kind of the whole computational language story
1240740	1245620	that I've spent so long on, is that, you know, our language is intended as something that you can
1245620	1251860	think in, as well as have your computer execute, so to speak, kind of like math notation would be,
1251860	1257620	it's something that, where you can actually, you know, use it as your foundation for thinking about
1257620	1262340	things, okay, anyway, that this is, you, you get the basic idea, I hope, of, of sort of this chat
1262340	1267620	notebook notion, it's, it's pretty nice, I mean, I have to say, since the reason that I haven't used
1267620	1274340	that, um, the chatGBT interface for months, is because this is really a lot nicer, you get to,
1274340	1278500	not only, you know, you can also use all the standard features of notebooks, so you can say,
1279140	1285300	this is a section about circles, and you can start putting in, maybe I could, well actually,
1285300	1292740	let me just do this, hold on, let's say, do that, I'm not going to live dangerous, do that for a sphere,
1292740	1301380	this is going to fail, of course, okay, let's see what happens, okay, interesting idea, interesting
1301380	1307140	idea, I wonder whether that will work, that's definitely an interesting idea, I give that
1307140	1314740	point, the spherical plot, that goes, wow, if this works, I'll be impressed, okay, let's run it,
1316020	1324180	wow, that's cool, it's getting smarter, the, or how, or the fine tuning that we've done and so on
1324180	1329300	is actually working, this is encouraging, um, the, because this is kind of interesting, I mean,
1329300	1335940	it made a spherical plot over a certain, you know, latitude, sequence of latitude values,
1335940	1340100	a different sequence of longitude values here, that's kind of interesting, you kind of learned
1340100	1344980	something from that, maybe we could try, let's try one other thing, which might be fun, let's say,
1345860	1360340	um, uh, show a star chart of the current position of Jupiter, now I'm probably going to have to say,
1360500	1364980	use astrographics, let's see what will happen here,
1368820	1378740	oh, come on, it just, well, I think it made that up, I'd be very surprised if these functions
1378740	1388180	actually exist, no, they do not exist, um, well, that's a bad sign, okay, let's try saying use
1388260	1394180	astro position, and what I'm expecting it's going to do, maybe,
1399300	1403780	lovely, but that's also not relevant, okay, it's not doing what I thought it would do,
1403780	1409700	which is to go read the documentation, we can, we could probably tell it to do that, let's say, um,
1410020	1421380	blah, blah, blah, there we go, now maybe it'll get a little bit smarter,
1427060	1430500	okay, this is much better, this is much, much better sign,
1431540	1437060	so hopefully, if it read the documentation, it will be able to successfully do what it was,
1438020	1443380	all right, I don't know whether it's blah, blah, blah, blah, blah, now probably if we now say,
1443380	1449300	okay, great, it's talking about all kinds of, I don't know, it's telling us how to find the
1449300	1452900	position of the large Magellanic cloud, et cetera, et cetera, et cetera, that's all fun,
1452900	1461940	and we could ask it to run that, but I think use this for the picture of Jupiter,
1462020	1470340	maybe this will work, maybe it won't, okay, this is much better, what,
1472100	1477700	you see this is the problem, it just makes stuff up, well let's see, I wonder whether this will
1477700	1482660	work, no, it made up the thing called planet marker, well we'd have to tell it not to do that,
1483380	1487940	it's supposed to go back, and I'm a little bit surprised it did that here, because actually
1488020	1493620	it has been told to go back and check the code it wrote to make sure that everything in it actually
1493620	1499380	exists, so for some reason it didn't in this case, all right, well anyway, that's a little bit on
1499380	1505700	kind of the sort of the interface between sort of LLMs, computational language, I thought another
1505700	1514900	thing I would talk about, quite different subject, is using physics to think about LLMs, so let me
1514980	1520260	pull up some things, so first question is what fundamentally is an LLM doing, as I said, what
1520260	1526340	it's ultimately doing is it's saying, given a particular piece of text, let's see if this works,
1526340	1530660	okay, so if you have something like this, you feed the prompt, the best thing about AI is its
1530660	1535540	ability to, and then its mission is to give you what the next word should be, and there are
1535540	1542900	some probabilities that it uses to do that, so if we kind of, we're interested in knowing, where's
1542900	1557940	my mouse, come on, up, just a second, sigh, you know, maybe I should just, well, okay, let me just,
1559380	1572180	that's very strange, fascinating, okay, the lost mouse, okay, the lost mouse has been found,
1572980	1580340	maybe, all right, so just, I mean, let's talk a little bit about what,
1583300	1590420	actually, let me show you something else here, so in our language, well, no, we can do it here,
1591300	1600820	we can just say, let's use a plain chat, and let's set it so that one of the parameters is,
1600900	1605060	we saw those probabilities that the LLM produced for what the next word should be,
1605700	1610740	one of the things about LLMs is they have to decide, given those probabilities, which actual
1610740	1616100	word should be picked, like one thing it could do is say, always pick the most probable word,
1616100	1622660	another thing it could do is pick those words according to the probabilities as it generated them,
1622660	1626740	there's this thing that's usually called the temperature parameter, which is an exponential
1626740	1632100	distribution thing that basically is the thing that picks, zero temperature means always pick
1632100	1637780	the most probable word, temperature one means pick the words in the probabilities that the LLM
1637780	1644980	generated itself, as you increase the temperature, it's picking more and more bizarre words, so let's
1644980	1651220	say we go here and let's say we increase the temperature to like 1.3, let's say, and we say
1651220	1660580	something like, how are you today, and it will generate some, so this is now using, okay, right,
1660580	1666340	great, okay, now let's try, let's change that temperature, let's go ahead here and just crank
1666340	1685380	up that temperature, and let's try running this again, oh my, so I'm bonkers, oh no,
1686100	1690580	okay, well at least it's stopped, often it never generates a stop token, it just keeps going forever,
1691300	1698340	the, so okay, so here's an example of a physics question, is there a phase transition as a function
1698340	1704500	of temperature in an LLM? The answer is almost certainly yes, probably around for something
1704500	1709540	like GPT-4, it's almost certainly at a temperature around 1.3 or so, maybe there are actually two
1709540	1715060	transitions that occur, actually there's a, we just had a summer school with people studying
1715060	1720100	all kinds of things, and one person at our summer school studied this question, and I have to admit,
1720100	1725060	I haven't read the thing they wrote about it, so, but I can show you, this is basically,
1725060	1731140	as a function of temperature, this is essentially an order parameter changing, and in the LLM,
1731140	1735300	and this is someplace here, this is an actual, you know, there's some innards of an LLM,
1736340	1742420	and somewhere here, there should be, okay, that's some random pieces of language code,
1742500	1750260	I think what was done here was to look at the extent to which it maintains kind of
1752020	1757380	coherence in the structure of the sentences that it produces and so on, but anyway, the thing that
1757380	1763620	I wanted to point out there is this is a very physics-like question, what, how does this work,
1763620	1768580	and one of the things we don't have right now is a kind of good qualitative physics,
1768580	1774100	overall physics-like model for an LLM, like you might say, oh, maybe it's like a spin glass,
1774100	1777620	well, it's not really like a spin glass, maybe it's like some other statistical
1777620	1783460	mechanics system, what is it really like? Well, there are a few things that we kind of know
1783460	1789460	about LLM, so I can show you some pictures, let me just show you, just to get a sense of what's
1789460	1795300	going on inside here, this is kind of a, like, let's say we're trying to learn this function,
1795300	1800340	so we've got x and y are input parameters, and we're trying to learn that function,
1800340	1805380	we're going to have a neural net, there's a neural net, and that neural net is taking those values,
1805380	1811620	x and y, and at the top it has some weights, each of the connections has a certain weight,
1811620	1817300	it indicated by the color of that, that connection, and then if we feed in particular values up at
1817300	1821940	the top there, this neural net will have been trained, will have been set up with the correct
1821940	1827620	weights, so that it will always produce a 0, 1, or minus 1 at the bottom, so for example we can,
1828340	1836020	let's just, let's say, if we try and use a very, very trivial neural net, trying to learn that
1836740	1842740	function, the totally trivial neural net will not succeed in producing that function, if we make
1842740	1847540	the neural net more sophisticated, here are some slightly more sophisticated neural nets, as the
1847540	1851460	neural net gets more sophisticated, it's going to be able to successfully learn that function,
1851940	1856820	how big does a neural net have to be to learn what level of function, not really known, I mean
1856820	1860740	there are theorems that say in principle you can do things with neural nets of certain sizes,
1860740	1868260	but the practical question we don't know, that's another kind of thing, so now you know in terms of
1868260	1875860	what, let's see, the, I mean you can do these experiments by the way, the things I've written
1875860	1880820	about, I wrote some kind of whole explainer of chat GPT, which was one of the things that I've
1881140	1885940	written fastest in my life, and it's the thing that seems to be read more, at least per unit of
1885940	1889780	time spent on writing it, it seems to have been read more than anything else I've written, which
1889780	1895060	to me is a little bit disappointing actually, but that's a different story, but anyway, so
1896340	1903300	those are some things about the innards of chat GPT, and those are some, but we can start looking
1903300	1908180	at kind of what's actually going on inside the system, and it's kind of complicated, and you
1908180	1914820	start seeing, you know, this is a condensation of the kind of innards of the brain of actually this
1914820	1922740	is GPT2, kind of a junior version of chat GPT, and this is kind of, in a sense this is taking
1922740	1928340	human knowledge and human linguistics, and crushing it down to something that's represented
1928340	1932820	in terms of arrays of numbers, and this is one of the pieces of what you see when that's done,
1932820	1939620	I mean the full chat GPT has like 175 billion weights, this is just showing a little piece of
1939620	1948420	that story. Now, okay, what can we say about what it actually does? Well, there's several
1948420	1953540	different things, so one thing that's important is this concept of embeddings, we can take
1954260	1960100	kind of, you know, words in a language, sentences, things like that, the big sort of idea of neural
1960100	1963860	nets in some sense, and it's a very old idea, dates all the way back to when neural nets were
1963860	1970980	invented in the 1940s, is don't just use digital information, use arrays of real numbers to represent
1970980	1975620	things. It's not clear that you actually need to do that, you probably don't, I don't think you need
1975620	1981380	to do it for physics, for example, but the way that neural nets are built, they are take everything,
1981380	1985860	whether it's an image, whether it's text, whatever else, and grind it into arrays of real numbers,
1985940	1993300	and then you can take those, then what you're doing is representing everything, you know,
1993300	1998180	just as in standard digital computational stuff, you're representing things as bits in neural
1998180	2003220	nets, you're representing everything in terms of arrays of real numbers, and so for example, any
2003220	2008500	old sentence, any old piece of text is ultimately represented as an array of real numbers, and
2008500	2013860	that array of real numbers we can think of as being some sort of feature vector that represents,
2013860	2020020	in some sense, some digest of the meaning of the thing that we specified. So you can start
2020020	2026500	asking in meaning space, in that space of embeddings, what can we see about what happens in that space,
2026500	2033220	and for example, let's see, we can ask questions like, how linear is that space? You know, for
2033220	2037940	example, if we do parallel transport in that space, if we look at the curvature of that space,
2038020	2044340	we're looking at, you know, this is to that, as that is to that, that's kind of the analog for
2044340	2051380	linguistics for sort of the structure of meaning of a question that you might ask in
2052500	2057220	physics of space time or something, and you can ask about these questions about curvature in that
2057220	2064020	space, I don't know all the answers to this, you can also ask things like, well, what is the trajectory
2064020	2071220	that's carved out in that space? So is there, for example, a semantic law of motion? If you start
2071220	2076820	in this particular way, is it the case that in this meaning space that you end up always tracing
2076820	2081540	through in a particular way? And one thing that seems to be the case, the space to some experience
2081540	2087700	we just did a couple weeks ago, is that the things are much more organized. So if you look at,
2088500	2090980	oh, this is kind of, sorry, let me just show you,
2092900	2099220	much of the time these trajectories aren't, in something like GPT2, the trajectories are
2099220	2103940	quite disorganized. It seems that as you get to things like GPT4, the trajectories look a lot
2103940	2109300	more organized. It's much more believable that there are semantic laws of motion, so to speak,
2109300	2114980	laws of motion in meaning space in GPT4. By the way, it's worth realizing that there's sort of a
2115060	2121940	quantum story to the whole thing because the whole thing is, it isn't just picking one trajectory,
2121940	2127220	it's picking a whole bunch of different paths. One difference from, I mean, this is a quite
2127220	2131540	different topic, but in the whole fundamental physics project that we've been doing for the
2131540	2136100	last few years where it seems like we really actually do finally understand how quantum mechanics
2136100	2141860	works, it becomes very important in that case that there is merging of different paths of history,
2141860	2149220	as well as just branching the paths of history. In the current versions of these LLMs, there's
2149220	2153700	pretty much just branching the paths of history, but you kind of get this quantum-like phenomenon
2153700	2158820	going on of all these different possible things the LLM might say that aggregate up to different
2158820	2164020	kinds of things. By the way, if you're, well, there's all kinds of interesting things to say
2164020	2174020	about LLMs as observers in thinking about physics, but maybe one thing to talk about is just what is,
2174820	2182500	this is sort of pictures of what meaning space looks like and so on, and questions like if you
2182500	2189140	have a word and it has many different sort of partially, so this is the word crane, I think,
2189140	2195620	and this is, in meaning space, this is where different sentences that mention cranes show up,
2195620	2199460	and so I think the ones at the top are cranes as a bird and the ones at the bottom
2199460	2204420	are cranes as construction equipment type thing, and you kind of see that separating, so you can
2204420	2209220	kind of get, again, it's this kind of rather physics-like thing of kind of looking at this
2209220	2216260	meaning space, and by the way, you can sort of ask things about the structure of that meaning space,
2216260	2220900	and for instance, let me see if I can show you a picture, let me see here,
2223620	2230660	maybe, okay, so in meaning space, you can ask something like, you can also do that with images,
2230660	2235620	and so you can ask, for example, we can go in meaning space, we can go from a dog image
2235620	2242180	to a cat image on the line in meaning space between a dog and a cat, and we could actually
2242260	2247780	keep going from the cat out further, we can extend that line further out in meaning space,
2247780	2251460	and we get all these kinds of weird things happening, or we can go from a plane to a cat,
2251460	2256020	and we have something very strange in the middle of those two things, or we can just go out,
2256020	2261860	this is what I was calling cat island, this is in the middle, so this was a generative AI,
2262580	2268820	not specifically an LLM, this is an image generation AI, which uses somewhat similar,
2268820	2276420	but not precisely the same technology inside, and I asked it in the middle to make a picture of a
2276420	2281380	cat in a party hat, and then as you go outwards in meaning space, you see this kind of island
2281380	2286100	of where you can see sort of identifiable cat things going on, and then further away,
2286100	2290340	it becomes more and more bizarre, and by the way, you can ask questions like, well, what's actually
2290340	2296180	out there in sort of arbitrary meaning space, and I think, well, you can look at other cat islands
2296260	2301940	here, this thing is actually in 2000 dimensional space, and these are planes in 2000 dimensional
2301940	2307460	space, different planes in 2000 dimensional space, and you see different cats live on different planes,
2308740	2315220	but sometimes you can just, if you plop into this meaning space in some random place, you'll see
2315220	2321220	things which kind of look, well, I don't know what those are, but sometimes you'll see things which
2321300	2326900	kind of have a reminiscent of kind of human forms and so on, why does all this happen, same kind of
2326900	2332580	reason as with LLMs, because this was trained from a five billion images, which were actual images
2332580	2339860	people put on the web, and those actual images are of human relevant kinds of things, with images
2339860	2346260	more so than with text, we're able to, as humans, we're able to look at things that weren't quite
2346260	2350260	right, like we looked at that high temperature version of what the LLM produced, and it looked
2350260	2355940	like garbage to us, it was incomprehensible, for images we do a little better at being able to
2355940	2360900	not be just completely confused by what's going on, but if we kind of dive in and look, you know,
2360900	2366580	what's out there in arbitrarily, let's see where do I have a picture of that, well, those are some
2366580	2373620	pictures just randomly out there in kind of meaning space, and if you go in you can see,
2373620	2377540	you know, there are weird things like this, these are, you know, people like pictures, or you can
2377540	2381940	have, you know, pictures like these, which are kind of reminiscent of sort of landscape-like
2381940	2385940	pictures, but aren't really landscape-like pictures, but this whole question about,
2385940	2393300	you know, where in meaning space, where in this, I mean, there's this, if you try and imagine,
2393300	2398980	where is the stuff that's meaningful, 10 to the minus 600 of all of meaning space is what we have
2398980	2404740	so far explored as with sort of human language and so on, it's a very small fraction of it,
2404740	2410340	with respect to images. Okay, so just to maybe finish off a bit, we could talk more about this
2410340	2415940	kind of thing, but just to talk a little bit about sort of the physics of LLMs and so on,
2416580	2422660	I think one of the things people, what one wants to do is, is there a kind of a narrative story of
2422660	2430420	what LLMs are finding? Is there a way of saying, why do LLMs even work? It's not obvious that,
2430420	2436900	you know, given that you, you know, you could say, take a sentence like, the cat sat on the,
2437540	2443060	okay, based on just looking at pages on the web, you can reasonably guess the next word is going
2443060	2447380	to be math, but by the time you've got a long prompt where you're asking it some physics question
2447380	2454500	or something, there's no way that actual detailed text is going to be somewhere on the web, or
2454500	2459380	probably not, unless it was some exercise or a book or something, but most of the time it won't
2459380	2464180	be something that was on the web. So you have to have an actual model that allows you to extrapolate
2464180	2469940	the model that's being used in chat GBT as a neural net. It is far from obvious. There's no
2469940	2475380	fundamental reason to think it would be true that the way the neural net extrapolates will agree with
2475380	2481860	the way we humans think it makes sense to extrapolate. The fact that it extrapolates to produce things
2481860	2487300	that seem meaningful to us humans is a nontrivial scientific result. And, you know, I think what
2487300	2494020	it's basically telling us is the way brains work is actually pretty well modeled by sort of neural
2494020	2499780	net type things. And that's why the things that brains extrapolate with are pretty close to the
2499780	2505460	things that these simple neural nets extrapolate with. So then the question is, well, okay, we've
2505460	2510100	got this kind of extrapolation that's going on. We've got some, this thing is finding out some
2510100	2515700	way to extrapolate. How is it doing that? Well, what regularities in language is it picking up to
2515780	2520740	allow it to make meaningful sentences, meaningful text? Well, there's one big regularity that we
2520740	2526340	know about in language, which is syntactic grammar. We know how you put parts of speech together,
2526340	2532100	nouns and verbs and things like this. So in a sense, we can then construct sentences which
2532100	2538100	are syntactically correct. But there are infinite number of sentences that are syntactically correct
2538100	2544020	but complete nonsense. And that's, it's doing much better than just producing syntactically correct
2544020	2550100	sentences. So what's it doing? Well, there's one good example of a place where we know a structure
2550100	2557220	in sentences that exists that isn't sort of purely syntactic. And that's logic. And you can kind of
2557220	2561860	think, you know, when Aristotle invented logic back a couple of thousand years ago, you know,
2561860	2566100	what was he actually doing? Well, he was a bit like a machine learning system, because what he was
2566100	2571140	effectively doing was saying, I've got all these examples of rhetoric. People make an argument
2571140	2576420	that looks like this, but I can take something which instead of it being a discussion about, you
2576420	2582580	know, Sparta and Athens, it can be a discussion about turtles and fishes. It doesn't matter. I can
2582580	2587940	just replace those symbolically with P and Q and I can look at this sort of formal structure of these
2587940	2595060	sentences. In a sense, you can lift logic out of the specifics of actual language, in his case,
2595060	2602260	Greek. But in a sense, what LLMs have done is they've discovered the same thing. So people say,
2602260	2607140	oh, my gosh, it's amazing, you know, LLMs have discovered logic. Well, they discovered logic,
2607140	2611220	I think the same way Aristotle discovered logic, and you can find out they're basically doing
2611220	2616500	so logistic logic. And if you try and feed them things which require sort of more formal, more
2616500	2621300	formal kinds of things, even at the level of, you know, parenthesis matching and so on, they will
2621300	2626340	fail after you get sort of too many parentheses to match. They don't do kind of the formal level
2626340	2632260	of things. They don't do the computational thing. They do the kind of level of things that in a sense
2632260	2636820	was the original way that logic was discovered. So that's a place where kind of one's able to lift
2636820	2643140	something more semantic out of this kind of layer of pure language. But presumably, there is more
2643140	2648660	that can be done along those lines. Presumably, there is kind of a semantic grammar of language,
2648660	2656340	which in a sense, the LLMs have discovered something about language and common sense
2656340	2661860	reasoning and so on. That is that there's this sort of thing you can lift out of language
2661860	2668580	that allows you to kind of put together meaningful stuff beyond just the purely syntactic. And I
2668580	2672580	think that's the thing where, well, I've been interested in this actually for a long time
2672580	2678900	for different reasons, this kind of idea of sort of making a symbolic discourse language that allows
2678900	2687300	you to sort of express things in a kind of, in a way that is sort of, is a symbolic way of expressing
2687300	2692660	things that is not specific to the particulars of language. In a sense, the whole enterprise of
2692660	2698020	making a computational language has got a certain distance with that, describing certain kinds of
2698020	2703140	things in the world. But anyway, I think that there are many pieces of kind of what happens in
2703140	2709380	LLMs. For example, why does few shot learning work? Why does it work to tell LLM and LLM to talk
2709380	2715460	like a pirate? Why does it, how does that manage to place it somewhere in meaning space or something
2715460	2720260	so that the kind of, you know, you placed it somewhere by giving that prompt, then somehow
2720260	2725060	the semantic law of motion takes over and it successfully manages to produce semantically
2725060	2730340	meaningful stuff. We don't know how any of this works. It's a great topic for physicists, I have
2730340	2736660	to say. I think it's one of these places where it isn't particularly easy. It's something where,
2736660	2742100	you know, this space of, you know, this sort of meaning space we're looking at with these images,
2742100	2746580	we can sort of see things about what's out there in meaning space in a way it's a little bit easier
2746580	2751380	than with text and words. But we're kind of, you know, this is sort of just the beginning of,
2751380	2755220	in a sense, physicalizing using something like statistical mechanics
2755220	2760980	to try and analyze what's happening inside an LLM. So I think kind of to sort of summarize,
2760980	2766420	I mean, I've talked about two kinds of things. One is just the very practical aspects of using
2766420	2774180	LLMs to, I think the most significant workflow there is this. You have a vague idea of what you
2774980	2781860	want to do. Now I have to say to get that vague idea, you have to have an ability to sort of
2781860	2787700	think computationally about things until you can express yourself in some kind of sort of
2787700	2794340	with computational concepts. I mean, it's no good, you know, with some notion of how you think about
2794340	2798980	the world computationally. Once you have that, you can kind of write a piece of natural language,
2798980	2805140	you go sort of tell that to the LLM. The LLM will then write, you know, will write
2805140	2812340	Wolfram language code or whatever, sometimes correctly, that will be an expression of what
2812340	2817780	it thought you meant by the things that you said in natural language. Now sometimes when you look
2817780	2822580	at that Wolfram language code, you'll say you misunderstood. It wasn't correct. You can fix
2822580	2828180	that code or you can tell it to go fix the code or whatever else. But so the workflow is, you know,
2828180	2835140	computationally imagine what you want to do, write it in natural language, have it kind of
2835140	2839940	translated into computational language, then read the computational language. It's very important
2839940	2844420	that something you can do with Wolfram language, no other, you know, that's the story of computational
2844420	2849300	language, very different from programming languages which weren't intended for humans to read
2849300	2855620	particularly. But so that's something where you read that computational language, you understand
2855620	2861300	what it said, you fix it if you need to, then you say run that, then that becomes a sort of brick
2861300	2868180	that you can use to build a whole tower of what you want on top of. And so that's, I think that's
2868180	2875380	the workflow and, you know, I have to say, as we make these chat notebooks better, it's getting
2875380	2880500	closer to the point where it actually makes sense, even if you know Wolfram language well,
2880500	2886260	to try and use it as a way to get things started if you're not thinking very clearly, so to speak.
2886260	2890980	Although as I say, to get the prompt right, you have to be kind of think expository writing because
2890980	2896420	if you're totally confused, the LLM will be confused as well. But anyway, so the first thing I was
2896420	2904020	talking about was this idea of how do we make use of LLMs mostly as a way to kind of get a leg up
2904020	2911300	on creating kind of computational language to be able to actually do computations. I should say,
2911300	2919380	by the way, I'm happy to talk about this, people interested if we have any time. But there's many
2919380	2925700	use cases, like for example, we're working on a bunch of AI tutoring applications. Another use
2925700	2930340	case I mentioned for physics, we've never been able to do, in Wolfram Alpha for example, we've
2930340	2936420	never been able to do physics word problems. We can do that once you've turned the word problem
2936420	2942660	into equations, for example, we can we can nail it. But turning going from the whole long textual
2942660	2949140	description into the equations is not something we've been able to do. Now we can. Now, in fact,
2949140	2954180	in practice, when you use LLMs, one of the things that's terrible, you know, you use the for example,
2954260	2960660	a chat notebook or the Wolfram plugin for chat GBT, it'll sometimes, you know, correctly untangle
2960660	2965940	the word problem, you know, solve the equations correctly. And then at the last minute, give the
2965940	2971620	wrong answer, because it tried to inject something that it thought it knew, and it got very confused.
2971620	2977540	But anyway, so lots of use cases for kind of the LLMs, their interaction with computational
2977540	2984020	language. And then the second piece, really quite a disjoint piece is why did the LLMs work in the
2984020	2988980	first place? This is a physics problem. And people should figure it out. And the results of
2988980	2994260	figuring it out will be many important things. For example, probably most of what's inside a modern
2994260	2999940	LLM doesn't need to be there. Most of what's it, you know, the actual structure you need to know
2999940	3006980	enough to be able to do sort of linguistic interface, plus kind of enough common sense to
3006980	3011860	support that linguistic interface is probably quite tiny compared to a current LLM. And probably
3011940	3016900	you can delegate all the kind of computation and detailed computational knowledge outside of the
3016900	3022100	LLM, which is an important thing in practice in terms of how much it costs to run an LLM,
3022100	3026420	what kind of systems you need to run it on. But if we understand LLMs better and why they
3026420	3030420	work in the first place, we have a better chance to be able to resolve those kinds of things.
3030420	3031860	All right, I should stop there. Thanks.
3032820	3047220	Very much. I think we have time for a couple of questions. I'd like to start with a quick one,
3047220	3053060	slightly out of left field. I think you've made a good case here for physicists becoming linguists.
3053620	3058580	Is there something that physicists should be learning from linguists or linguists should
3058660	3065380	be transitioning to physics? Physics can give us, I don't know whether it's linguistics,
3065380	3069780	I don't know whether you call it that. I don't know what you call it. But this whole idea about
3069780	3076820	meaning and so on, what we're talking about, that's something that I think has now been exposed
3076820	3081300	as something on which we can do experimental science on, on which we can apply physics.
3081300	3088580	So I think that's the, I mean, in terms of, yeah, no, that's, I mean, it's, you know,
3089780	3095060	if you look at the history of physics, right, physics has been a fantastic export field.
3095060	3099780	That's, you know, populated molecular biology, it's populated, you know, quantitative finance,
3099780	3104420	it's populated lots of kinds of things. It has every opportunity to populate this area
3104420	3110660	and to populate and to really make some complete change to how one thinks about sort of meaning
3110660	3115540	and language and so on, I believe. Well, thank you. All right, let's go to the right first.
3116420	3120180	Hi, so I've seen some of your talks on the Wolfram physics project as well,
3120180	3126100	and I see these n-dimensional graphs that you often use, and they often really look like neural
3126100	3130820	networks. And so I wanted to ask if that was intentional or if there's some additional layers
3130820	3136100	of physics going on there. They have nothing to do with neural nets. So far as I know,
3136100	3139940	although there's at least one startup that believes they do, and we'll see how that works out.
3141060	3149140	This is an utterly disjoint discussion about how kind of microscopic
3149140	3153540	hypergraphs, you know, from them emerge space-time and quantum mechanics and so on.
3154900	3160260	There is in fact a bizarre connection. Okay, this is to the deepest level of the rabbit hole
3160260	3167060	immediately. There's this thing we call the rouliad, which is the entangled limit of all
3167060	3171460	possible computations. Imagine you take all possible, let's say, Turing machines with all
3171460	3177940	possible initial states. You run them, and they're all non-deterministic. They all have all possible
3177940	3183860	rules. You run them, you get this big, messy thing. There's only one of it. It is the complete
3183860	3190740	representation of all possible computations. And then that, I claim, is sort of the ultimate
3190740	3196660	foundation of physics and mathematics, actually. And our physical universe ends up being,
3197940	3204580	we have to exist within that. And so our physical universe ends up being our kind of, our sampling
3204580	3210500	of that rouliad object. And here's the fascinating fact, at least I think it's interesting, is that
3210500	3217220	if you assume that we as observers of the rouliad have two characteristics. One, we are computationally
3217220	3222100	bounded. Two, we believe we are persistent in time. We believe we have a single thread of
3222100	3227860	experience. Those two attributes alone are sufficient to give you, not just qualitatively,
3227860	3233700	but exactly, general relativity and quantum mechanics. That's kind of exciting, because it
3233700	3239540	tells you that it didn't need to be that way. The aliens who don't have those characteristics
3239540	3242980	don't have to have general relativity and quantum mechanics. But it kind of gives you, I mean,
3242980	3248180	this is a huge condensation of a very large amount of stuff. But that's, so okay, how does
3248180	3253220	that relate to all of this? When I was showing you those weird cat pictures and things, the,
3253780	3259220	this is a, one of the reasons I was studying weird cat pictures is because this is a way of
3259220	3264820	understanding sort of different slices of this roulial space concept. There's much more to say
3264820	3270420	about this. That's a way too, way too brief a description. Okay, let's take a question from
3270420	3276420	the left now. Yeah, hi. So I've tried to use LLMs in research so far without great success.
3276420	3280100	I'm a theoretical physicist. Something that would be, there's a weird echo here, I don't know.
3280100	3284980	Anyway, sorry, something that would be really useful would be if I could have something where,
3285780	3292340	you know, 300 page paper, I don't know, by Edward and Pierce. And I can ask it, can you give me a
3292340	3297940	one page summary of that, you know, where it would be correct and where it would already kind of know
3297940	3302100	from talking me to before, like what are the kind of things that I know and then I don't know.
3302100	3305620	So I mean, how far are we from that? Well, you know, for example, in our company,
3306500	3312340	you know, someone makes a daily digest of interesting papers about LLMs. Okay, and I got fed up
3312340	3315620	trying to read the abstracts, because every abstract is written differently. They're very
3315620	3320580	ponderous in many cases. I said, just get the frigging LLMs and make a two sentence summary
3320580	3325940	of every paper. It works great. I mean, you can scan down this thing really quickly. The fact that
3325940	3331940	the LLMs text is rather boring is actually good, because all the text is consistent, and you kind
3331940	3337140	of can just see what's happening. Now, you know, do I get the essence of every paper correctly?
3337140	3342260	Maybe not. But that's a statistical thing anyway, I might miss it from the abstract too. So that's
3342260	3350580	a pretty good use case that I recommend, actually. In terms of the can you, if you want it to summarize
3350580	3356260	for you, particularly, I think that's coming. And the, you know, kind of AI tutoring system that
3356260	3362500	we're building, that's kind of one of the big ideas is know the student and be able to figure out,
3363140	3367300	first of all, how is the student confused? Because one of the things that you typically can't do
3367300	3372020	in sort of watching what a student does is watching the working that the student follows.
3372020	3377700	But you can with an LLM, and you can kind of see, you know, how is the student confused as the
3377700	3382980	student is doing their work? And then, so then the question is, will it come to the point where,
3382980	3388820	you know, the LLM will know enough about me from having read, I mean, me personally, I've put 50
3388820	3396100	million words out there. So it's, I'm pretty easy to learn about. And we're trying to get an LLM to
3396100	3403860	be me, so to speak, which will save time, maybe, maybe not. But anyway, the point is,
3404580	3412500	I'm, you know, given that the LLM knows about you, I think there is a very good chance that the LLM
3412500	3417220	will be able to say the one thing you need to know, because you're confused about this or you don't
3417220	3422020	know this, is this one fact. And you say, oh, that's the thing I want to know from that paper, all the
3422020	3427140	other stuff is irrelevant. I think that's pretty realistic. And I think it's reasonably short-term.
3427140	3431860	But you've got to understand, like everything with machine learning, it's kind of an 80% success,
3431860	3437140	90% success story. And whenever you have a situation, like looking at these abstracts, where,
3437140	3441940	you know, if I notice an abstract that looks really interesting, it's a win. If I miss one,
3441940	3447460	it's not a disaster. That's a good use case. If it's a case where you're trying to do the next
3447460	3453860	great, you know, precise physics calculation, it's probably a big lose to use an LLM where it
3453860	3460820	might be wrong 10% of the time, you don't know which 10%. Okay, I understand that there are
3460900	3465460	many, many questions, but at some point, everyone does have to go home, unfortunately.
3465460	3476740	So I'm afraid we're going to have to cut off the questioning there. Let's thank our speaker again.
