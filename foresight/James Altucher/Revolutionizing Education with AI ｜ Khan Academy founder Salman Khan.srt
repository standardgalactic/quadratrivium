1
00:00:00,000 --> 00:00:04,960
From pit lane to podium, the Las Vegas Grand Prix is providing fans a race day experience at the

2
00:00:04,960 --> 00:00:10,160
speed they deserve. With the help of T-Mobile for Business, our 5G Advanced Network solutions

3
00:00:10,160 --> 00:00:14,960
are powering race day operations with event-wide connectivity. From streamlined gate entry to

4
00:00:14,960 --> 00:00:20,480
an immersive app, giving fans blazing fast access to the sport they love. This is accelerating

5
00:00:20,480 --> 00:00:25,760
innovation. This is the Las Vegas Grand Prix with T-Mobile for Business. Take your business further

6
00:00:25,840 --> 00:00:36,640
at T-Mobile.com. Okay, let's do some quick math. The less your business spends on operations,

7
00:00:36,640 --> 00:00:41,440
on multiple systems, on delivering your product or service, the more margin you have and the

8
00:00:41,440 --> 00:00:48,320
more money you keep. That's obvious. But with higher expenses on materials, employees, distribution

9
00:00:48,320 --> 00:00:55,920
and borrowing, everything costs more. To reduce costs and headaches, smart businesses are graduating

10
00:00:55,920 --> 00:01:01,760
to NetSuite by Oracle. Here's the thing. Information is power. Information is money.

11
00:01:02,400 --> 00:01:09,040
Literally, the currency of today's world of entrepreneurship is information. And if you

12
00:01:09,040 --> 00:01:15,600
could bring all of the information about your business into one dashboard, this is incredibly

13
00:01:15,600 --> 00:01:21,440
valuable. NetSuite is the number one cloud financial system, bringing accounting, financial

14
00:01:21,440 --> 00:01:29,920
management, inventory, HR into one platform and one source of the truth about your business.

15
00:01:29,920 --> 00:01:36,240
With NetSuite, you reduce IT costs because NetSuite lives in the cloud with no hardware required,

16
00:01:36,240 --> 00:01:40,800
access from anywhere. You cut the cost of maintaining multiple systems because you've

17
00:01:40,800 --> 00:01:46,240
got one unified business management suite and you're improving efficiency by bringing all of

18
00:01:46,240 --> 00:01:52,880
your major business processes into one platform, slashing manual tasks and errors. This is so

19
00:01:52,880 --> 00:01:56,960
valuable. You just hit a button and you can see all the information about your business instead of

20
00:01:56,960 --> 00:02:01,680
having to like call five different departments and get all these emails and put it all together and

21
00:02:01,680 --> 00:02:08,160
make sense of it. Over 37,000 companies have already made the move. So do the math, see how

22
00:02:08,160 --> 00:02:14,400
you're profit with NetSuite. Back by popular demand, NetSuite has extended its one-of-a-kind

23
00:02:14,400 --> 00:02:21,760
flexible financing program for a few more weeks. Head to netsuite.com slash James net suite.com

24
00:02:21,760 --> 00:02:33,360
slash James net suite.com slash James.

25
00:02:41,920 --> 00:02:48,960
Oh my gosh, I've been wanting to have this guy on my podcast for literally 10 years ever since I

26
00:02:48,960 --> 00:02:57,280
started this podcast. I am so impressed with him and he speaks about a subject near and dear to my

27
00:02:57,280 --> 00:03:05,520
heart. So Salman Khan, Sal Khan is the creator of the Khan Academy, which was really the first big

28
00:03:06,160 --> 00:03:14,000
online academy. It was to learn math, coding, all these things. And there was generating people who

29
00:03:14,000 --> 00:03:20,080
really learned the topics as opposed to people who go all through 10 years of school and never

30
00:03:20,080 --> 00:03:25,520
learned these topics at all or the schools failed to teach them appropriately. The Khan Academy

31
00:03:25,520 --> 00:03:30,640
really seemed to understand education and it became this huge thing. 150 million students have gone

32
00:03:30,640 --> 00:03:37,520
through the Khan Academy or use it every month or that number of registered users. Salman Khan,

33
00:03:37,600 --> 00:03:45,040
Sal Khan, and he just wrote the book about how AI will revolutionize education. The book is called

34
00:03:45,040 --> 00:03:52,880
Brave New Words. How AI will revolutionize education and why that's a good thing. And he addresses

35
00:03:52,880 --> 00:03:58,880
how to use AI with education, whether you're a student, teacher, employer. It was so valuable in

36
00:03:58,880 --> 00:04:04,880
terms of understanding not only education, but AI and how we can use AI on our lives. And it

37
00:04:04,880 --> 00:04:11,200
addresses all the fears people are having right now about AI and its role in creativity and learning

38
00:04:11,200 --> 00:04:16,720
and will it replace jobs or will it make it easier to get a job. So Brave New Words,

39
00:04:16,720 --> 00:04:22,000
how AI will revolutionize education and why that's a good thing. I finally got to interview

40
00:04:22,000 --> 00:04:28,080
Sal Khan about the Khan Academy and AI and I learned so much and I hope you will as well.

41
00:04:35,280 --> 00:04:54,320
Sal, can I start off by asking you a question that is peripherally related to the book, but you do

42
00:04:54,320 --> 00:05:00,240
mention this towards the end of the book, which is there was one line that I was curious about

43
00:05:00,240 --> 00:05:05,760
and it was like a personal thing, but you brought it up in the book. Sure. So you mentioned that you

44
00:05:05,760 --> 00:05:10,560
thought your dad, who you didn't really know, you met him. He left early, your parents separated

45
00:05:10,560 --> 00:05:18,080
when you were very young and you only met him once. He passed away when you were around 14 years old,

46
00:05:18,080 --> 00:05:23,120
but you said he probably suffered from depression and I was curious why you thought that.

47
00:05:23,840 --> 00:05:33,200
You know, I don't have, no one told me that, so I don't have any strong, well, the evidence is

48
00:05:34,160 --> 00:05:41,840
he kind of disappeared and when, the one time I met him when he was, when I was 13,

49
00:05:43,200 --> 00:05:52,720
we briefly went to his place and let's just say it looked like he was down and then it seems to fit

50
00:05:52,800 --> 00:05:59,040
with a lot of the narrative of what we've observed from my end.

51
00:05:59,600 --> 00:06:02,160
What were the circumstances of the visit? Like why did you visit him?

52
00:06:02,880 --> 00:06:12,240
You know, I was 13 years old. I don't remember. If I remember correctly, he had, I guess, a cousin

53
00:06:12,240 --> 00:06:17,440
of mine who I don't know, my father's side of the family that well, a nephew of his, who really

54
00:06:17,440 --> 00:06:24,880
wanted to put him in touch with us again and I forgot all the context. I think we were up in,

55
00:06:24,880 --> 00:06:31,280
he was living in Philadelphia at the time, so we were up in that area anyway and so this cousin,

56
00:06:31,280 --> 00:06:36,720
older cousin than me, you know, probably 20 years older than me, facilitated this, you know, us to

57
00:06:36,720 --> 00:06:40,640
get together and to me and my sister and then we, you know, we spent an evening together.

58
00:06:41,520 --> 00:06:45,760
Well, I was just curious about that particularly since you brought it up in the book, but

59
00:06:46,640 --> 00:06:52,480
look, I've been incredibly impressed with everything you've done. The Khan Academy,

60
00:06:53,440 --> 00:06:59,280
you know, 155 million users, you know, so many people have learned from

61
00:07:00,320 --> 00:07:07,040
the videos on the Khan Academy, the educational style of the Khan Academy and I know people know

62
00:07:07,040 --> 00:07:12,400
the story and it's not central to the book also, but maybe, you know, if you could spend a few

63
00:07:12,400 --> 00:07:17,120
minutes describing how you started this, like it started, apparently, you were, you were tutoring

64
00:07:18,000 --> 00:07:22,800
one of your cousins, Nadia, about math and then other cousins knew, hey,

65
00:07:22,800 --> 00:07:29,520
Sol's available for some free tutoring. We'd, you know, we want to, we want to help and so you

66
00:07:29,520 --> 00:07:36,560
started creating tools and technology and videos and this grew into the Khan Academy.

67
00:07:37,200 --> 00:07:43,120
Yeah, that's generally right. It was back in 2004. I, my original background is technology.

68
00:07:43,120 --> 00:07:47,760
By 2004, I'd gone to business school. I was a year out. I was now working as an analyst at a hedge

69
00:07:47,760 --> 00:07:54,480
fund. I had just gotten married in New Jersey and I was living in Boston and my family from New

70
00:07:54,480 --> 00:07:58,640
Orleans, which is where I was born and raised, were up visiting in the Northeast and they came to,

71
00:07:59,280 --> 00:08:04,320
some of them came to Boston for the July 4th weekend. This was 2004 and just came out of

72
00:08:04,320 --> 00:08:08,800
conversation that my cousin Nadia was having trouble in math. Her mom told me, my aunt,

73
00:08:08,800 --> 00:08:13,680
and that's when I offered to, when I learned more about what was going on with Nadia,

74
00:08:13,680 --> 00:08:20,480
I offered to tutor her and Nadia agreed. And as you mentioned, well, first of all, that tutoring

75
00:08:20,480 --> 00:08:24,960
seemed to work with her. She was struggling with unit conversion, got her caught up even ahead

76
00:08:24,960 --> 00:08:29,120
of her class. And, you know, I joke sometimes I became a tiger cousin at that point. I call up

77
00:08:29,120 --> 00:08:33,840
her school, they let her retake placement exams. It really helped her. Then I started

78
00:08:33,840 --> 00:08:37,760
tutoring her brothers, words, friends, and my family on the free tutoring, as you mentioned. And

79
00:08:37,760 --> 00:08:44,240
before I know it, 10, 15 cousins, family, friends. And I've always been interested in education. So

80
00:08:44,240 --> 00:08:51,280
this wasn't a complete fluke that I was doing this. I'd done tutoring in other times in my life.

81
00:08:51,280 --> 00:08:57,840
And I have always, I had always dreamt of one day starting a school of some kind. So I was always

82
00:08:57,840 --> 00:09:03,120
interested by the problem and I thought I could help my cousins. And I was always interested in

83
00:09:03,120 --> 00:09:07,840
the intersection of, well, if you can solve a problem, can you use technology to help you scale

84
00:09:07,840 --> 00:09:13,280
any solutions that you had? So that was always in the back of my mind a bit. But in 2005,

85
00:09:14,080 --> 00:09:18,800
with that in mind, I did start to make some practice software for these cousins. That was

86
00:09:18,800 --> 00:09:24,720
the first Khan Academy had no videos or anything like that. But it was just to help scale a solution

87
00:09:24,720 --> 00:09:28,400
to a pattern I was seeing. A lot of my cousins just had gaps in their knowledge. They needed more

88
00:09:28,400 --> 00:09:33,760
practice. I as their tutor wanted to make sure that they were getting that practice and I wanted

89
00:09:33,760 --> 00:09:37,920
to monitor it. So that's, that's why I wrote that software. And once again, just as a hobby.

90
00:09:37,920 --> 00:09:43,760
And then in 2006, a friend suggested that I help scale my lessons with videos.

91
00:09:43,760 --> 00:09:49,600
I thought it was, I legitimately thought it was a silly thing to do. I thought it was very low tech.

92
00:09:49,600 --> 00:09:54,240
I thought YouTube was kind of a place for entertainment, not a place for learning.

93
00:09:55,120 --> 00:10:00,480
But I gave that a shot and my cousins famously told me they liked me better on YouTube than in

94
00:10:00,480 --> 00:10:06,240
person. And what they were saying was they liked the pause, the repeat, always accessible, no

95
00:10:06,240 --> 00:10:10,880
embarrassment if they had to review something from before. And so I kept going. And that was

96
00:10:10,880 --> 00:10:14,720
obviously very discoverable by a lot of people as well. So I just kept working on that software

97
00:10:14,720 --> 00:10:21,280
and kept working on those, those, those videos by 2008, 2009. That's where my brain was focusing

98
00:10:21,280 --> 00:10:26,560
most of the time. And so my wife and I looked at our finances. We had some money saved up for a

99
00:10:26,560 --> 00:10:30,240
down payment on a house. At this point, we had moved out to Northern California. But

100
00:10:31,360 --> 00:10:35,440
you know, we felt like this, there's something happening here. There was about 50 to 100,000

101
00:10:35,440 --> 00:10:42,640
folks using the nascent Khan Academy on a monthly basis. And there were some philanthropists that

102
00:10:42,640 --> 00:10:47,520
were, there were actually both venture capitalists and philanthropists who are already signaling

103
00:10:47,520 --> 00:10:52,000
that they were interested. In fact, the venture capitalists were more interested back then.

104
00:10:52,640 --> 00:10:56,720
But it felt important that this would be a nonprofit organization, set it up mission

105
00:10:56,720 --> 00:11:03,280
free world-class education for anyone anywhere. And yeah, 2009, I took the plunge to try to get that

106
00:11:03,280 --> 00:11:09,840
philanthropic support to become a real organization. It was a tough year to be very clear, probably the

107
00:11:09,840 --> 00:11:14,080
most stressful year of my life. Our first child had been born and I'd given up a good career.

108
00:11:14,080 --> 00:11:19,120
But by 2010, we had our first real support. And you know, everything we've been doing since then

109
00:11:19,120 --> 00:11:24,320
has really been that same pattern. How do we scale up that same personalization, that same

110
00:11:24,320 --> 00:11:29,360
student-centered learning that you could do when it was just myself working with Nadia. And, you

111
00:11:29,360 --> 00:11:34,160
know, most of our history has been improving on the software, creating more and more content,

112
00:11:34,880 --> 00:11:40,320
exercises, videos, articles, teacher tools, working with school districts. And obviously,

113
00:11:40,320 --> 00:11:43,440
more recently, it's been leveraging artificial intelligence on top of that.

114
00:11:44,320 --> 00:11:50,320
Right. So your new book, Brave New Words, how AI will revolutionize education and why that's a

115
00:11:50,320 --> 00:11:57,520
good thing. It's a really interesting book because on the one hand, your book is what the title says,

116
00:11:57,520 --> 00:12:03,760
how AI will revolutionize education and you talk about developments in AI, developments that you've

117
00:12:03,760 --> 00:12:09,840
contributed to AI. You were an early user tester and so on of all the versions of

118
00:12:09,840 --> 00:12:19,840
ChatGPT. But the book is also, it almost could be titled, What Everyone is Afraid of in AI

119
00:12:19,840 --> 00:12:27,280
and How We Can Solve Each Fear. And I noticed that is the flip side of AI. I was at a dinner

120
00:12:27,280 --> 00:12:32,640
recently where there was a bunch of academics and journalists and scientists. It was all in the

121
00:12:32,640 --> 00:12:39,200
science domain. And every single one of these scientists or writers of science were really

122
00:12:39,200 --> 00:12:46,720
afraid of AI. They thought AI was dangerous. There was no positive thing said about AI except

123
00:12:46,720 --> 00:12:53,280
when I pointed this out to all of them. Why do you think so many people are afraid of AI? And then

124
00:12:53,280 --> 00:12:55,760
I want to hit how you address some of these things in the book.

125
00:12:58,400 --> 00:13:06,160
I think AI is, it's interesting because it's especially, it's especially threatening to folks

126
00:13:06,240 --> 00:13:13,120
who have always defined their identity by their ability to write and speak and think original

127
00:13:13,120 --> 00:13:19,440
thoughts. So it's not a coincidence that the intelligentsia, so to speak, are the ones that

128
00:13:19,440 --> 00:13:26,240
are probably most concerned about this. And it's not to mean that these aren't legitimate concerns

129
00:13:26,240 --> 00:13:32,720
or a lot of them aren't legitimate concerns. But I think it is affecting folks with education,

130
00:13:32,720 --> 00:13:41,920
folks who write. It gets awfully close to home in terms of identity. And so I think it immediately

131
00:13:42,560 --> 00:13:46,800
puts people into a little bit of a defensive posture and a little bit of a fear-based posture.

132
00:13:47,440 --> 00:13:52,240
And once again, I can't completely predict the future. We even saw that when we first got access

133
00:13:52,240 --> 00:13:57,600
to the technology before the rest of the world did. And I would say half of our team was really

134
00:13:57,600 --> 00:14:02,960
excited about it and want to go all in. And the other half had a lot of this trepidation. And

135
00:14:04,000 --> 00:14:07,680
to your point, maybe your title of the book would have been a better one because you're

136
00:14:07,680 --> 00:14:14,480
right. It really is this pattern that I probably have always tried to lean in this way, but it's

137
00:14:14,480 --> 00:14:19,840
definitely accelerated with AI, which is, all right, we should write down fears. One should not

138
00:14:19,840 --> 00:14:25,280
ignore fears and risks. But fears and risks are not reasons to not try to move forward and make

139
00:14:25,280 --> 00:14:33,920
positive use of it. There are things to address and to deal with so that you can have a positive

140
00:14:33,920 --> 00:14:40,400
outcome. Yeah, I mean, obviously the very first thing that comes up when you talk about AI and

141
00:14:40,400 --> 00:14:46,480
education, and I have all my kids are basically college age or graduate school age. The first

142
00:14:46,480 --> 00:14:54,880
thing that comes up is cheating. And I'll give a just transparent experience

143
00:14:55,360 --> 00:15:02,480
one daughter was in the final stages of her finals on the year. And she had like one day

144
00:15:02,480 --> 00:15:07,920
to write an essay. And she hadn't even read the text she was supposed to be writing the essay

145
00:15:07,920 --> 00:15:16,080
about. And I said, look, there's this thing called chat GPT, just cheat and get it done and graduate.

146
00:15:16,080 --> 00:15:21,120
Just do it. And I don't know if she followed my advice or not, but that was my advice because I

147
00:15:21,520 --> 00:15:25,680
for me it was more important for her to graduate than for her to learn this one thing.

148
00:15:26,240 --> 00:15:33,120
But that is a big concern of teachers and parents and everything because a lot of people don't

149
00:15:33,120 --> 00:15:37,040
care about their education or they don't think that their college or high school or whatever is

150
00:15:37,040 --> 00:15:43,840
giving them a good education. So they're willing to use it to cheat. Yeah, it's a real thing. And

151
00:15:44,400 --> 00:15:48,320
you know, what I write about in the book is I always like to start from first principles,

152
00:15:48,320 --> 00:15:52,720
like, okay, you know, what was the state of things even before chat GPT came on to the scene?

153
00:15:52,720 --> 00:15:58,000
And it didn't take long to realize that cheating was already pretty rampant. And if anything,

154
00:15:58,000 --> 00:16:03,200
there does seem to have been a cultural shift even since you or I were in college in terms of

155
00:16:03,200 --> 00:16:09,040
probably more acceptance of cheating than ever before. And once again, before AI existed,

156
00:16:09,040 --> 00:16:15,760
probably the internet software tools, their services that are happy to write essays for you

157
00:16:15,760 --> 00:16:20,720
or do other types of problem sets for you, their quasi legitimate publicly traded companies that

158
00:16:20,720 --> 00:16:25,600
will help you do your homework and help is I'm saying it very euphemistically, they'll essentially

159
00:16:25,600 --> 00:16:34,400
do your homework for you. So this existed well before. And so I think the question is, in this

160
00:16:34,400 --> 00:16:42,240
world, what are we hoping to get out of students when they do these types of tasks? And if part

161
00:16:42,320 --> 00:16:46,160
of it is to make sure that it is their work. And in the book, I talk about, look, there's

162
00:16:46,160 --> 00:16:50,960
certain cases where you probably want students to use the tools. In fact, these tools are going

163
00:16:50,960 --> 00:16:54,960
to be part of their future. But there are certain cases where you definitely want to make sure that

164
00:16:54,960 --> 00:17:00,960
it's the students own work, how you can, how you can address it. And you know, the AI actually

165
00:17:00,960 --> 00:17:06,240
isn't, it can be part of the problem, obviously, with things like chat GPT. But in a lot of ways,

166
00:17:06,240 --> 00:17:11,120
I'm optimistic that it can be, it can be a very big part of the solution that addresses not just AI

167
00:17:11,120 --> 00:17:16,640
cheating, but cheating in general, where students won't have the work done by the AI, but the AI

168
00:17:16,640 --> 00:17:22,800
can be their coach, an ethical tutor, ethical writing coach, and then it can make transparent

169
00:17:22,800 --> 00:17:28,560
the process to the teacher. So the teacher doesn't just get the essay, they get the whole transcript

170
00:17:28,560 --> 00:17:32,000
of how the student worked on it and the AI's analysis that it's similar to the student's

171
00:17:32,000 --> 00:17:36,400
other writing or that they worked on it for four hours or they had trouble with the thesis statement

172
00:17:36,400 --> 00:17:41,920
initially, but eventually got there and be able to give the teacher insights across the whole

173
00:17:41,920 --> 00:17:45,920
classroom that, look, 15 of your kids are having trouble with thesis statements. Here's a lesson

174
00:17:45,920 --> 00:17:50,560
plan that you might want to go on, go and do. So I'm optimistic on that front.

175
00:17:55,440 --> 00:18:00,080
Take a quick break. If you like this episode, I really, really appreciate it. It means so much

176
00:18:00,080 --> 00:18:06,480
to me. Please share it with your friends and subscribe to the podcast. Email me at AltaturaGmail.com

177
00:18:06,480 --> 00:18:22,400
and tell me why you subscribed. Thanks. I have to say Airbnb has changed my life. I just love

178
00:18:22,960 --> 00:18:28,240
staying in Airbnb's. Like in about a month, I'm going to Cocoa Beach, which is right next to Cape

179
00:18:28,240 --> 00:18:32,880
Canaveral. I'm going to watch some rocket launches. I'm going to, of course, be staying in a very nice

180
00:18:32,880 --> 00:18:39,600
Airbnb on the beach. And it's just such a great experience. Like the whole world is available

181
00:18:39,600 --> 00:18:49,040
to us now because of Airbnb. But whenever I'm at an Airbnb, I always realize, you know, the home

182
00:18:49,040 --> 00:18:56,160
that I left to come to this Airbnb, I could be making money on that right now by hosting and

183
00:18:56,160 --> 00:19:02,640
being an Airbnb myself. So, and I've known people, I had a friend who basically,

184
00:19:03,440 --> 00:19:09,680
you know, made a living from turning his home into an Airbnb. So if you have a home, but you're not

185
00:19:09,680 --> 00:19:15,440
always at home, you do have an Airbnb there. And it can easily fit into your lifestyle,

186
00:19:15,440 --> 00:19:21,920
and it's a great way to earn some money. Your home might be worth more than you think.

187
00:19:21,920 --> 00:19:26,800
Find out how much at Airbnb.com slash host.

188
00:19:33,280 --> 00:19:39,040
The famous Abraham Lincoln quote says, good things come to those who wait. I wonder,

189
00:19:39,040 --> 00:19:41,520
did he really say it? Jay, did he really say that? Can you look that up?

190
00:19:42,240 --> 00:19:45,840
Regardless of who said it, that's only part of the quote. The full quote is,

191
00:19:45,840 --> 00:19:50,480
good things come to those who wait, but only the things left by those who hustle.

192
00:19:51,120 --> 00:19:56,560
Well, if you're a business owner and want the best people on your team, the same applies.

193
00:19:57,520 --> 00:20:03,760
And listen, I've interviewed 1500 people now and a lot of entrepreneurs. I can safely say

194
00:20:04,400 --> 00:20:11,840
the one thing consistent among all entrepreneurs and CEOs, the successful ones, is that it's all

195
00:20:11,840 --> 00:20:17,760
about the people you surround yourself. If you hire well, you're going to have a great business.

196
00:20:17,760 --> 00:20:24,080
And you know, thankfully zip recruiter puts the hustle in your hiring. So you find qualified

197
00:20:24,080 --> 00:20:29,360
candidates fast. This is so important. And I want you to try it. You could try it as a potential

198
00:20:29,360 --> 00:20:34,640
employer or employee. You could try it for free at ziprecruiter.com slash James,

199
00:20:34,640 --> 00:20:39,040
zip recruiters smart technology finds top talent for your roles right away.

200
00:20:39,040 --> 00:20:43,120
Immediately after you post your job, if you're hiring, zip recruiters matching technology

201
00:20:43,120 --> 00:20:48,640
starts showing you qualified people for it. And I will tell you that I signed up on zip recruiter

202
00:20:48,640 --> 00:20:52,400
as a potential employee. You know, I just wanted to see how it works. And right away,

203
00:20:52,400 --> 00:20:57,280
it started matching me with really amazing potential employers. So give it a try at zip

204
00:20:57,280 --> 00:21:02,960
recruiter.com slash James, let's zip recruiter give you the hiring hustle you need. See why four

205
00:21:02,960 --> 00:21:07,280
out of five employers who post in zip recruiter, get a quality candidate within the first day.

206
00:21:07,280 --> 00:21:13,280
Just go to zip recruit.com slash James to try it for free. Again, that's zip recruit.com slash

207
00:21:13,280 --> 00:21:25,840
James zip recruiter, the smartest way to hire. Right. So, and this isn't in general,

208
00:21:25,840 --> 00:21:31,920
like a theme that runs throughout is that viewing AI or, or, or chat GPT like programs

209
00:21:31,920 --> 00:21:40,160
as a collaborator or a partner rather than as a secret weapon is kind of the basic solution to

210
00:21:40,160 --> 00:21:47,280
all of these fears. So, so for instance, teaching doesn't go away. But hey, if just like with the

211
00:21:47,280 --> 00:21:51,680
kind of kind of people were able to watch the video. So now the T and learn basic math lessons.

212
00:21:51,680 --> 00:21:56,960
So now the teachers can focus on more more discussions and interesting topics in the

213
00:21:56,960 --> 00:22:03,280
classroom itself. So, so for instance, if you, if the teacher knows the student is using chat GPT

214
00:22:03,920 --> 00:22:09,600
to answer questions about the great Gatsby, that might actually be a benefit to the educational

215
00:22:09,600 --> 00:22:14,960
experience. Yes and no. I think the key is that everyone is going into it with eyes wide open

216
00:22:14,960 --> 00:22:18,240
that the teachers and the educators know why they're asking students to do things.

217
00:22:18,880 --> 00:22:24,480
They're very clear about certain tasks where it will be pedagogically valuable to use

218
00:22:24,560 --> 00:22:29,200
the chat GPTs of the world and other things where you don't want to use them. But I want to be

219
00:22:29,200 --> 00:22:36,400
very clear about use. There is the use of say a general AI platform like chat GPT that would be,

220
00:22:36,400 --> 00:22:42,480
you know, that it could be used to do very productive things like do research, help understand a

221
00:22:42,480 --> 00:22:48,240
concept. But those also could be used to write your essay for you or do things that a lot of

222
00:22:48,240 --> 00:22:52,240
educators would find, hey, that's probably undermining what I was trying to get out of this.

223
00:22:52,240 --> 00:22:57,040
There's other use cases of generative AI and this is where ConMigo, which right now is built

224
00:22:57,040 --> 00:23:03,680
primarily on GPT-4, but it's a different application than chat GPT. But that's built explicitly to

225
00:23:03,680 --> 00:23:09,280
support students and teachers in a, in a education framework. So it won't cheat, but it will

226
00:23:09,280 --> 00:23:14,480
socratically work with the students. The bottom line is, I think we're entering this phase where

227
00:23:15,360 --> 00:23:19,600
educators have to be more explicit about why they're giving certain tasks and

228
00:23:20,400 --> 00:23:25,280
also explicit on the types of tools that are acceptable and aren't, probably with some

229
00:23:26,320 --> 00:23:31,040
light oversight, potentially AI empowered oversight of what the students are doing.

230
00:23:31,040 --> 00:23:36,080
And it's going to be more and more on the students also to be very clear of like, hey,

231
00:23:36,080 --> 00:23:38,960
this is not something that I should do and this is something I should do, but also know

232
00:23:38,960 --> 00:23:43,200
that they're going to be held accountable by it, that there are, that ConMigo, what we're doing

233
00:23:43,280 --> 00:23:49,840
at Khan Academy, there are tools now to monitor what the student is doing, make sure it is their

234
00:23:49,840 --> 00:23:55,520
own work, when the teacher wants to make sure that it's the students own work. Although it seems

235
00:23:55,520 --> 00:24:03,680
like every time there's an attempt to, oh, this is definitely created by AI, humans are smarter

236
00:24:03,680 --> 00:24:08,160
in this sense. They're always going to develop ways to override whatever tools are being used to

237
00:24:08,160 --> 00:24:14,560
detect AI usage and homework. Yeah. Anyone who tells you that they have some type of an algorithm

238
00:24:14,560 --> 00:24:20,640
or watermarking technology or statistical algorithm, whatever it might be that can detect AI generated

239
00:24:20,640 --> 00:24:31,040
work is either significantly exaggerating or lying to you. There have been plagiarism detectors,

240
00:24:31,040 --> 00:24:38,400
but for the most part, what AI is generating is novel, is novel text. So it's very hard to detect.

241
00:24:38,400 --> 00:24:43,600
Now, the way that some teachers have detected it, just with their spider senses, they have seen

242
00:24:44,320 --> 00:24:49,440
work that just seems different than what the student has turned in before, or it just seems

243
00:24:49,440 --> 00:24:55,920
surprisingly cogent, or maybe a little bit bland compared to what a typical people would write.

244
00:24:55,920 --> 00:25:00,320
Now, even those cases is usually because the student didn't use the AI artfully enough.

245
00:25:00,880 --> 00:25:05,440
That if they really tweaked it a little bit, tweaked their prompts, maybe adapted some of

246
00:25:05,440 --> 00:25:08,800
what the AI was writing to make it a little bit more personal, it would have been very hard to

247
00:25:08,800 --> 00:25:16,000
detect. So our strategy here isn't to try to magically just read a paper and say this was

248
00:25:16,000 --> 00:25:22,720
written by an AI or human or 5050 or some other percentage in between. It's to make the process

249
00:25:22,720 --> 00:25:31,120
transparent to the educator. So with Conmigo, we aren't saying, oh, we ran some algorithm and

250
00:25:31,120 --> 00:25:36,720
there's an 80% chance this is AI written. Instead, we tell the teacher, here's the transcript of the

251
00:25:36,720 --> 00:25:43,600
student working with our AI, working with Conmigo on their essay. It took them four hours. Here's

252
00:25:43,600 --> 00:25:47,520
the conversation. Here's a synopsis of the conversation. They had trouble with the thesis

253
00:25:47,520 --> 00:25:53,600
statement. We worked on outlining in this way, etc. Maybe a comparison to previous writing that

254
00:25:53,600 --> 00:25:58,480
it seems similar. So it's not trying to just detect based only on the output. It's more of giving a

255
00:25:58,480 --> 00:26:04,640
very strong indication based on making the entire process transparent. If the student goes to chat

256
00:26:04,640 --> 00:26:11,440
GPT or gets their sister to write the paper and it just shows up inside of Conmigo, Conmigo is

257
00:26:11,440 --> 00:26:15,520
going to tell the teacher, hey, this just showed up. We didn't work on this. You should look into

258
00:26:15,520 --> 00:26:19,120
it. And by the way, it's very different than what the student has written before. I think that's the

259
00:26:19,120 --> 00:26:26,720
kind of lens that we need to really not only police cheating, but I don't want to make this just about

260
00:26:26,720 --> 00:26:31,680
policing and making it punitive. I also think this is how we're going to better support students and

261
00:26:31,680 --> 00:26:36,000
teachers because the students going to be much more supported as they write and the teacher is

262
00:26:36,000 --> 00:26:40,960
going to get many more insights about where the students strengths and weaknesses are in their

263
00:26:40,960 --> 00:26:50,400
writing. Yeah. So in that sense, the transparency that, hey, I'm using an AI tool and here's the

264
00:26:50,400 --> 00:26:55,280
final output. Part of the final output is the essay I came up with. Part of the final output is

265
00:26:55,280 --> 00:27:01,440
Conmigo's like your AI tool, Conmigo's analysis of our sessions together. That's right.

266
00:27:02,720 --> 00:27:08,240
I think that I think that's very valuable. I think that's I think this idea of AI as collaborator

267
00:27:08,240 --> 00:27:16,160
is an important reframing of the role of AI in our lives where just like we have a calculator now

268
00:27:16,160 --> 00:27:22,800
to do math equations. Oh, I have kind of an intelligence assistant to help me with problems

269
00:27:22,800 --> 00:27:26,960
so I could really focus on the bigger problems. That's right. You know, what we've been doing

270
00:27:26,960 --> 00:27:33,120
when we do professional development with teachers is we tell them, look, imagine if

271
00:27:33,920 --> 00:27:39,040
all of a sudden your school district discovered a few billion dollars and they want to use that

272
00:27:39,040 --> 00:27:46,000
money to hire three or four super hardworking teaching assistants for you that will help you

273
00:27:46,000 --> 00:27:51,600
do lesson plans, help you grade papers, help you write progress reports. They'll also analyze what

274
00:27:51,600 --> 00:27:55,760
your students are doing on a regular basis to let you know. And by the way, they're also going to

275
00:27:55,760 --> 00:28:00,400
be able to tutor the students. They're available 24 seven. These are amazing teaching assistants

276
00:28:00,400 --> 00:28:05,200
and they'll be able to connect what you're doing in class to where the students need help and vice

277
00:28:05,200 --> 00:28:11,520
versa. Every teacher on the planet would say, yes, sign me up for that. And that I truly believe

278
00:28:11,520 --> 00:28:17,200
that's where it's going. This isn't about replacing teachers. Now, one thing we've always said at Khan

279
00:28:17,200 --> 00:28:22,480
Academy, our ideal is let's raise the ceiling when a student does have access to a reasonably good

280
00:28:22,480 --> 00:28:28,560
classroom and educators. But there are cases where students don't have access to a classroom.

281
00:28:29,360 --> 00:28:33,840
It could be a developing nation. There's no school at all. It could be even in the U.S.

282
00:28:33,840 --> 00:28:38,080
you're a rural part of the world or a country where there's not a calculus class within an hour

283
00:28:38,640 --> 00:28:44,960
hour's drive. Then we want Khan Academy and Khan Mego by extension to help raise the floor

284
00:28:45,520 --> 00:28:48,400
so that you can get more supports when there aren't any others.

285
00:28:49,680 --> 00:28:53,920
You know, you talk about teachers being worried about they're going to be replaced by Ann. I

286
00:28:53,920 --> 00:28:58,240
think this is happening in a lot of professions right now. And that's part of this. There's this

287
00:28:58,240 --> 00:29:03,680
existential fear that we're all, you know, horse and buggy drivers and we're not going to need

288
00:29:03,680 --> 00:29:10,960
it anymore once the cars are around. And so whether it's, you know, writers, teachers, other

289
00:29:10,960 --> 00:29:19,920
professionals and so on. But maybe we don't need teachers. Maybe AI and Khan Academy with Khan Mego

290
00:29:19,920 --> 00:29:24,640
can do the job. What is the role of teaching right now?

291
00:29:25,280 --> 00:29:29,360
I'm not saying what I'm about to say just because it's the right thing to say. It's

292
00:29:29,360 --> 00:29:34,160
what I genuinely believe. I wouldn't be saying it otherwise. I would just stay quiet if it wasn't

293
00:29:34,160 --> 00:29:43,760
what I genuinely believe. I think any role that is about the human connection, about the human

294
00:29:43,760 --> 00:29:51,520
motivation is going to be not only in very good shape in an AI world, but those are going to

295
00:29:51,520 --> 00:29:58,400
be the roles, the jobs, the careers that are most enhanced in an AI world. So even though

296
00:29:58,400 --> 00:30:04,800
historically, many people associated teaching with, oh, I'm going to write a lesson plan,

297
00:30:04,800 --> 00:30:08,400
then I'm going to deliver this lecture, then we're going to have a quiz every two weeks,

298
00:30:08,400 --> 00:30:12,800
and then I'm going to grade those quizzes, and then we're going to rinse and repeat for two weeks.

299
00:30:14,240 --> 00:30:20,480
Any great teacher will tell you that's actually not their job. What's actually their job is on a

300
00:30:20,480 --> 00:30:27,440
very human level to connect with their students, either individually in small groups as a whole

301
00:30:27,440 --> 00:30:32,960
class, to motivate them to see the wonder in the world, to make sure that these students feel seen

302
00:30:34,240 --> 00:30:40,240
and supported. And then on top of that, the teachers have to grade papers, write lesson plans,

303
00:30:40,240 --> 00:30:47,920
progress reports, IEPs, prepare them for standardized tests. And because we're asking so

304
00:30:47,920 --> 00:30:53,760
much of teachers and some of that latter half of stuff is more tangible and more measurable,

305
00:30:53,760 --> 00:30:59,840
it's actually squeezed out a lot of teachers' opportunities to do those very human things.

306
00:30:59,840 --> 00:31:04,000
Anyone who goes into the teaching profession, they dream of these moments where they're able

307
00:31:04,000 --> 00:31:09,200
to have some time in a small group with some students or give that one student who was really

308
00:31:09,200 --> 00:31:12,880
down and out of pep talk, and it changes their life forever. And it will change their life forever.

309
00:31:12,880 --> 00:31:20,240
I think in my 12 years, 13 years in the K-12 system, I remember probably every moment

310
00:31:20,240 --> 00:31:25,120
when a teacher did single me out out of the 30 kids and say, hey, Sal, can I talk to you about

311
00:31:25,120 --> 00:31:30,080
something? I really liked what you did, or I really didn't like what you did. It affected me,

312
00:31:30,080 --> 00:31:36,480
it changed who I am. And I think that's what, or a teacher ran a Socratic conversation with 10 of

313
00:31:36,480 --> 00:31:42,000
us as opposed to all 30 at the same time, or we had a field trip where we ran a simulation. And

314
00:31:42,000 --> 00:31:46,080
those are the things that I don't think the AI is going to be able to drive,

315
00:31:46,080 --> 00:31:51,600
but I think the AI will be able to support it. But once again, freeing up the teacher to do more

316
00:31:51,600 --> 00:31:58,880
of that. The jobs that I think will be under threat with AI, or there will be fewer of them,

317
00:31:59,600 --> 00:32:05,360
are the ones that you are, if great writers should not feel threatened, but if you're the

318
00:32:05,360 --> 00:32:11,600
person who writes the fairly generic text every day on the news sites about why the stock market

319
00:32:11,600 --> 00:32:19,120
went up or down. I'm surprised if those aren't already written by AIs. I think a lot of them

320
00:32:19,120 --> 00:32:22,320
actually are. They feel like it. They feel like they've been written by AIs for 10 years.

321
00:32:24,160 --> 00:32:31,440
I think if you are, if I'm dealing with the education system, the registrar's office,

322
00:32:31,440 --> 00:32:36,320
I think is going to be, you're going to see a lot of automation there. But once again,

323
00:32:37,280 --> 00:32:44,320
as just a citizen of the world, I think the more resources that we can save on the non-student

324
00:32:44,320 --> 00:32:49,840
facing thing and put it onto the student facing thing, that's a good thing. If you look at school

325
00:32:49,840 --> 00:32:57,760
districts like New York City, they spend an average of $40,000 per student per year. There are 25

326
00:32:57,760 --> 00:33:02,800
students in an average classroom in the New York City Department of Education. So that means if you

327
00:33:02,880 --> 00:33:08,080
just took $40,000 and multiplied it by those 25 students, that means that there's a million dollars.

328
00:33:08,720 --> 00:33:14,960
I guarantee you that no matter how good the teacher's benefits, pensions, et cetera,

329
00:33:14,960 --> 00:33:20,320
they're getting a small fraction of that million dollars. Maybe a very senior teacher with a great

330
00:33:20,320 --> 00:33:24,800
pension, et cetera, maybe with great health insurance, maybe it's $150,000. If you put all

331
00:33:24,800 --> 00:33:31,040
the benefits in there, approaching $200,000, that's very, I think I'm already being overly,

332
00:33:32,000 --> 00:33:37,680
no one's really making that much. So where are all the other resources going? It's actually

333
00:33:37,680 --> 00:33:42,720
not going through the real estate in most cases. Where is it going? And it's going to all of this,

334
00:33:42,720 --> 00:33:48,240
let's call it back office, administrative stuff that really isn't moving the dial with students.

335
00:33:48,240 --> 00:33:53,200
And actually, I don't write too much about that aspect of it. That's probably less interesting

336
00:33:53,200 --> 00:34:00,640
for a lot of folks. But yeah, register's office, scheduling, just a lot of the administrative

337
00:34:00,640 --> 00:34:03,200
things I think will hopefully get much more streamlined.

338
00:34:05,200 --> 00:34:10,720
You know, what about the current education system is an antique at this point? I mean,

339
00:34:10,720 --> 00:34:17,760
I feel like the current, roughly the current education system's been around for 200 or so years

340
00:34:17,760 --> 00:34:23,920
that you go to a school, you go to a location where there's a bunch of subjects taught throughout

341
00:34:23,920 --> 00:34:28,960
the day. So every day it's like six different subjects. You go from class to class to class

342
00:34:29,040 --> 00:34:34,960
and you get homework. At the end you get tested. So that's roughly the structure of the modern

343
00:34:34,960 --> 00:34:38,560
education system. And what part of that you think is outdated now?

344
00:34:39,440 --> 00:34:43,040
Yeah. And you know, my first book that I had written back in 2012, One World Schoolhouse,

345
00:34:43,040 --> 00:34:49,200
it kind of goes into how did we end up with the system? And it really does, it's not a coincidence

346
00:34:49,200 --> 00:34:56,240
when you say it's roughly 200, 250 years old. It came out of the Industrial Revolution. And for

347
00:34:56,240 --> 00:35:03,440
the most part, it was a very utopian idea. For most of human history, we learned through apprenticeship,

348
00:35:03,440 --> 00:35:09,360
we learned through following around, learning from our cousins or our parents how to hunt or how

349
00:35:09,360 --> 00:35:16,320
to cook. Once society became more advanced and more specialized, we would hang out with the

350
00:35:16,320 --> 00:35:22,720
blacksmith or we would hang out with the, I don't know, the carriage repair guy, whatever it would

351
00:35:22,720 --> 00:35:30,480
be to learn or even things like law and medicine was an apprenticeship until about 200, 250 years

352
00:35:30,480 --> 00:35:36,160
ago. But the issue with a lot of these is that they didn't scale. If you go back 300 years ago,

353
00:35:36,160 --> 00:35:41,600
very, even in more literate parts of the world, you only had a 30 or 40% literacy rates. Most

354
00:35:41,600 --> 00:35:47,760
of the world, you only had about a 10 or 15% literacy rate. And so when we had the Industrial

355
00:35:47,760 --> 00:35:54,160
Revolution, we had a more abundant society because of technology. And a lot of these societies, and

356
00:35:54,160 --> 00:35:58,400
it's not a coincidence that these were some of the first societies to develop a middle class,

357
00:35:59,120 --> 00:36:05,840
the UK, what would eventually become Germany, Japan, the United States, they were the first

358
00:36:05,840 --> 00:36:11,680
to say, hey, let's have a mass public education system. Very utopian idea. But they said the only

359
00:36:11,680 --> 00:36:16,240
way we can afford to do that, the only way we can scale it is by leveraging the techniques of the

360
00:36:16,240 --> 00:36:21,200
Industrial Revolution. We're going to bat students together, usually by age, move them together at

361
00:36:21,200 --> 00:36:26,480
a set process, literally a bell will ring every hour. I mean, that directly comes from a factory.

362
00:36:26,480 --> 00:36:31,280
We're going to have standards. We now take standards for granted in education, but this was

363
00:36:31,280 --> 00:36:40,240
a thing that came out of mass production in a factory. We will assess periodically. But we're

364
00:36:40,240 --> 00:36:45,440
not going to slow down that assembly line. Instead, we're going to assess and some of the quote

365
00:36:45,520 --> 00:36:50,480
product is going to be said, okay, this is the product that's going to be the

366
00:36:50,480 --> 00:36:55,040
doctors, lawyers, engineers, and some of it, well, the information or the knowledge doesn't seem to

367
00:36:55,040 --> 00:37:01,840
be sticking. Well, these will be the less skilled laborers. So it kind of worked. We could talk about,

368
00:37:02,560 --> 00:37:08,480
was it fair for folks? You could have been, and people have been talking about the inequities

369
00:37:08,480 --> 00:37:13,760
of tracking for decades. When you're 12, something's not sticking and all of a sudden you're tracked

370
00:37:13,760 --> 00:37:18,480
into a slower track and now all of a sudden no one expects you to go to college and people expect

371
00:37:18,480 --> 00:37:24,320
you to be a lower skilled laborer versus becoming a doctoral lawyer. Is that fair? That's a very

372
00:37:24,320 --> 00:37:31,520
big question. I don't think it is, but it kind of worked for society in that we didn't need a huge

373
00:37:31,520 --> 00:37:36,560
number of people in the knowledge economy. And we did need a lot of people who are in either the

374
00:37:36,560 --> 00:37:42,640
middle income or lower income jobs, a lot of the mid-skilled or lower skilled labor. We did need

375
00:37:43,200 --> 00:37:48,960
it. But if you imagine the world that we are going into, we know what's going to happen to the

376
00:37:48,960 --> 00:37:53,760
less skilled jobs, you know, or even the mid-skilled jobs. Obviously, we're talking about artificial

377
00:37:53,760 --> 00:38:00,000
intelligence, which is all about kind of that mid-skill processing of information. Everyone I

378
00:38:00,000 --> 00:38:04,320
talk to robotics is not far behind. I think a lot of folks are saying in the next five or ten years,

379
00:38:04,320 --> 00:38:10,640
we're going to have a chat GPT type moment with robotics as well. So that also means that some

380
00:38:10,640 --> 00:38:16,080
of the lower or mid-skilled labor is also going to be automated, probably in the next 10 or 20

381
00:38:16,080 --> 00:38:20,720
years. And so where does that leave us? Well, it leaves the high-skill labor, the knowledge economy,

382
00:38:20,720 --> 00:38:28,160
being a researcher, being an engineer, being an entrepreneur is where all of the productivity,

383
00:38:28,160 --> 00:38:33,040
all of the wealth will accrue there. And so as a society, we have to decide, are we okay with

384
00:38:33,040 --> 00:38:37,200
only right now 10% of people participate there? And then that's not a stable society, or we're

385
00:38:37,200 --> 00:38:41,120
going to have to do redistribution, which to me is still pretty dystopian because people want to

386
00:38:41,120 --> 00:38:46,720
have a sense of purpose. Or do we leverage the technologies that we have to get as many people

387
00:38:46,720 --> 00:38:51,760
as possible into the top of that pyramid to be in the knowledge economy? Instead of having a labor

388
00:38:51,760 --> 00:38:56,320
pyramid, you can have an inverted pyramid where most people are able to be in that knowledge

389
00:38:56,320 --> 00:39:03,200
economy. And so I think that's where the imperative is. So how do you start making those changes in

390
00:39:03,200 --> 00:39:08,560
the educational system? And also, I think there's some question as to whether people effectively

391
00:39:08,560 --> 00:39:14,640
learn by taking six different subjects in a day. Like multi-tasking learning has been shown to me,

392
00:39:14,640 --> 00:39:18,560
not the most effective way to learn. And you even mentioned this in the book, that the more one-on-one

393
00:39:18,560 --> 00:39:25,920
more immersion is probably better for education. Yeah. The core ideas, and this is something I've

394
00:39:25,920 --> 00:39:31,200
been preaching well before artificial intelligence came on the scene, is instead of a traditional

395
00:39:31,200 --> 00:39:36,800
model, what we hold fixed is how long you get to work on something because that assembly line keeps

396
00:39:36,800 --> 00:39:41,360
moving. And what's variable is how well you learn it. And we get that in the form of grades. You

397
00:39:41,360 --> 00:39:46,880
and I take a class together, we're in algebra for this term, you got an A minus, I got a C plus.

398
00:39:46,880 --> 00:39:51,360
All right, it goes in my permanent transcript and we move on. Somehow expecting me that I got to,

399
00:39:51,360 --> 00:39:55,280
you know, expecting someone who got the C plus to now understand algebra two or understand

400
00:39:55,920 --> 00:40:00,800
something more advanced. And so what I've all, what I've been advocating for the last 10 years

401
00:40:01,520 --> 00:40:06,960
or more is let's do it the other way around. What should be, what we should, what should be variable

402
00:40:06,960 --> 00:40:11,520
is when and how long you work on something and what's fixed is how well you learn it. And that's

403
00:40:11,520 --> 00:40:16,880
what a personal tutor, when Aristotle was Alexander the Great's tutor, I'm sure that's what he did.

404
00:40:16,880 --> 00:40:22,640
If young Alexander hadn't mastered a concept yet, he would have slowed down a bit or, you know,

405
00:40:22,640 --> 00:40:26,800
given him a second chance to take that test to make sure that he really mastered whatever,

406
00:40:26,800 --> 00:40:32,000
military strategy or, you know, governance, whatever, whatever was the, whatever was the

407
00:40:32,000 --> 00:40:38,800
topic for the day. Now, if I said this, say 30 years ago, people would have rolled their eyes

408
00:40:38,800 --> 00:40:43,840
or say, well, okay, it's easy for you to say, but unless you have a lot of resources to provide

409
00:40:43,840 --> 00:40:48,240
that level of personalization, there's no way you can, you can have 30 different students learning

410
00:40:48,240 --> 00:40:54,640
at 30 different paces. And if one student is falling a little bit behind for, for them to

411
00:40:54,640 --> 00:41:00,640
have a second chance to take that test, that's just logistically impossible to do. Now, what's

412
00:41:00,640 --> 00:41:05,040
changed over the last 30 years, even before artificial intelligence is you have tools like

413
00:41:05,040 --> 00:41:10,080
Khan Academy, you have software that can let every student practice at their own time and pace,

414
00:41:10,080 --> 00:41:14,800
give teachers real time information. If a student needs to refresh their knowledge on something,

415
00:41:14,800 --> 00:41:19,360
they could watch an on-demand video, they could read an article, now they can have a conversation

416
00:41:19,360 --> 00:41:24,480
with a artificially intelligent tutor. And once again, that tutor is also in contact with the

417
00:41:24,480 --> 00:41:30,560
teacher. So constantly guiding the teacher on how they can be a conductor of this class of 30,

418
00:41:30,560 --> 00:41:34,880
where they don't have to make every person play the same thing at the same time, regardless of how

419
00:41:34,880 --> 00:41:41,200
bad the quality or how, how, how, how not ready for it they are. Now you can, you can, you can

420
00:41:41,200 --> 00:41:47,280
personalize a lot more. And so, and a corollary to that is, you know, I talk about mastery learning,

421
00:41:47,280 --> 00:41:52,160
which is, if you haven't learned it well yet, keep trying. And once again, this is how we've,

422
00:41:52,160 --> 00:41:56,000
we've always instilled do, if you want to be an Olympic athlete, if you want to be a

423
00:41:56,000 --> 00:42:00,720
musician, you are doing mastery learning. We just don't do it in our school system,

424
00:42:00,720 --> 00:42:05,200
and you see the outcome differences between how good people can get us if they have a personal

425
00:42:05,200 --> 00:42:09,920
coach. But, but a corollary to mastery learning is what I would call competency-based learning.

426
00:42:09,920 --> 00:42:14,480
Right now, our entire credentialing system is based on seat time, for the most part.

427
00:42:14,480 --> 00:42:19,760
Did you sit in a chair for 12 years, and kind of do what you were told, even though you don't

428
00:42:19,760 --> 00:42:23,680
really understand a lot of it? Okay, we'll give you something called a high school diploma.

429
00:42:23,680 --> 00:42:30,720
Did you sit in a chair for 12 hours this week for a term? Okay, we'll give you the 12 credit

430
00:42:30,720 --> 00:42:34,880
hours or however they want to account for it for that course, and you kind of learned it.

431
00:42:34,880 --> 00:42:39,360
We know in reality, very few people actually retain most of what they quote are exposed to

432
00:42:39,360 --> 00:42:45,520
in school. 60, 70 percent of kids going to community colleges have to get remediation

433
00:42:46,160 --> 00:42:49,920
not even at a high school level, essentially at a seventh grade level. So, they sat in these

434
00:42:49,920 --> 00:42:54,480
classes called algebra one, algebra two, trigonometry, some of them even take calculus,

435
00:42:54,480 --> 00:42:57,440
and the colleges are saying, you're not even ready to learn algebra yet,

436
00:42:57,440 --> 00:43:02,000
go back, we're going to work on your pre-algebra. So, a corollary to personalization and mastery

437
00:43:02,000 --> 00:43:06,560
learning is moving to a competency-based credentialing system. If you know it,

438
00:43:06,560 --> 00:43:10,160
take some type of a rich assessment and we'll give you credit, even if it only took you two days

439
00:43:10,160 --> 00:43:14,480
to learn it, but if you haven't learned it yet, that's cool too. Here are some resources to

440
00:43:14,480 --> 00:43:18,000
keep learning. Come back in a month, come back in two months, come back in a year,

441
00:43:18,000 --> 00:43:31,680
and I'll give you a second chance to actually learn the material.

442
00:43:32,320 --> 00:43:41,920
There are over 75 million monthly 2B viewers. That's more people than there are influencers

443
00:43:41,920 --> 00:43:48,720
on the internet, which means 2B is more popular than sponsored posts for digestive enzymes and

444
00:43:48,720 --> 00:43:55,600
high coverage foundation, more popular than soft launching your boyfriend, more popular

445
00:43:55,600 --> 00:44:00,480
than making boomers explode with rage when you tell them how much you make on a single post.

446
00:44:00,480 --> 00:44:05,600
2B, it's more popular than influencers. See you in there.

447
00:44:30,480 --> 00:44:47,360
So, now let's bring in AI. How would you use AI to who would decide, would it be the student or

448
00:44:47,360 --> 00:44:54,000
the teacher or both or the AI, who would decide like, oh, James needs a little more work on his

449
00:44:54,000 --> 00:44:59,280
algebra too, as opposed to Sal who got an A plus on every test. Who would decide how I,

450
00:44:59,920 --> 00:45:04,880
you know, how much more time do I need and what would I be assigned to do? Like would the AI give

451
00:45:04,880 --> 00:45:09,920
me tests or teach me or like what would happen? It's a little bit of all of the above where

452
00:45:09,920 --> 00:45:14,880
even before we had artificial intelligence, we have these classrooms where students are working

453
00:45:14,880 --> 00:45:20,240
on Khan Academy and we were able to give signals to teachers as to which students are moving ahead

454
00:45:20,240 --> 00:45:23,440
and we want to encourage them to do so, where students are on track and where students are

455
00:45:23,440 --> 00:45:28,320
maybe falling behind. But we did leave it all, you know, we did professional development for

456
00:45:28,320 --> 00:45:33,040
teachers on how they might want to navigate that. Hey, for the kids who are ready to move ahead,

457
00:45:33,040 --> 00:45:37,440
let them move ahead for the kids who are behind, why don't you do a focused intervention with those

458
00:45:37,440 --> 00:45:42,880
six or seven kids. And a lot of teachers were doing that type of thing, but you can imagine it can get

459
00:45:42,880 --> 00:45:47,040
pretty complex and pretty hard, even if you have some of these software tools.

460
00:45:47,040 --> 00:45:50,960
What we're able to do now with the artificial intelligence is it really is acting like a data

461
00:45:50,960 --> 00:45:55,520
analyst and or teaching assistant for these teachers. So, instead of a teacher every night

462
00:45:55,520 --> 00:46:00,640
having to look at a spreadsheet like dashboard and say, okay, these are the kids who are struggling,

463
00:46:00,640 --> 00:46:04,240
let me break them out or let me do another lesson plan, which is a lot of work for the teacher,

464
00:46:04,960 --> 00:46:09,440
they can just ask the AI what's going on. And the AI says, look, here's what's going on,

465
00:46:09,440 --> 00:46:14,160
these are the kids on track here, you know, if we do a differentiated lesson plan tomorrow,

466
00:46:14,160 --> 00:46:19,600
I recommend these kids work on this task while you spend the first 20 minutes with these students.

467
00:46:19,600 --> 00:46:24,560
And, and the teachers in charge, this thing is in support of the teacher, the teacher is going

468
00:46:24,560 --> 00:46:27,760
to be able to say, no, I don't really like that activity. Can you, and we already have this where

469
00:46:27,760 --> 00:46:31,280
they can highlight parts of it and they can say, let's come up with something that's a little bit

470
00:46:31,280 --> 00:46:35,920
more fun or a little bit more engaging or gets the kids out of their chair. And the AI will say,

471
00:46:35,920 --> 00:46:42,080
yes, sir, yes, ma'am, and do that. But as you can imagine, it dramatically lowers the amount of

472
00:46:43,280 --> 00:46:49,200
analysis and prep time that a teacher has to spend in order to create these really engaging

473
00:46:49,200 --> 00:46:56,080
and differentiated personalized education experiences. It's also going to all this,

474
00:46:56,080 --> 00:47:01,120
all these other trappings of school that as students, we never saw as parents, we oftentimes

475
00:47:01,120 --> 00:47:06,960
don't see writing progress reports, grading papers, you know, IEPs, which are these

476
00:47:06,960 --> 00:47:11,440
plans you have to write for, for students who are, who need special supports, which are

477
00:47:12,000 --> 00:47:16,320
increasingly a larger and larger percentage of the student population takes a lot of the teacher's

478
00:47:16,320 --> 00:47:22,560
time. Teachers have to prep their own knowledge. If the AI can dramatically improve that time,

479
00:47:22,560 --> 00:47:26,800
we've been getting signals from school districts that Kanmigo is helping the teachers with their,

480
00:47:26,800 --> 00:47:33,600
with this non-student facing work, it's saving them five, 10 hours a week. And so you're going to

481
00:47:33,600 --> 00:47:39,040
see it there. You're going to see, you know, there's this phenomenon called learning management

482
00:47:39,040 --> 00:47:43,680
systems, which are increasingly the ways that teachers and students communicate with each other

483
00:47:43,680 --> 00:47:48,400
and, you know, make assignments and submit assignments. And right now these are web-based

484
00:47:48,400 --> 00:47:54,240
things. I see more and more the AI acts as that intermediary, where it's working with the teacher

485
00:47:54,240 --> 00:47:59,360
on the planning, on the creation, it provisions it to the student, it works with the students on

486
00:47:59,360 --> 00:48:04,480
those things, and then it's able to report back to the teacher. So, you know, we can, and I can

487
00:48:04,480 --> 00:48:08,800
daydream more and more. You can imagine a world where an AI can start to even know what's going

488
00:48:08,800 --> 00:48:13,760
on in a classroom and support even more. But even before we go into those more sci-fi use cases,

489
00:48:14,720 --> 00:48:18,400
it, I think it's just going to be a, it's just going to be in the fabric of everything that the

490
00:48:18,400 --> 00:48:22,080
teacher and the students do, and hopefully in a way that supports the students and the teachers

491
00:48:22,080 --> 00:48:27,760
better and streamlines things. Do you think it will happen? Like, let's say, you know, AI is

492
00:48:27,760 --> 00:48:32,240
moving pretty fast, and already there are tools like Kanmigo just, you know, what is it like a

493
00:48:32,240 --> 00:48:38,560
year and a half after the first chat CPT was released? What, where do you see education five

494
00:48:38,560 --> 00:48:42,720
years from now? Like in K through 12, and then I'll ask about other types of education.

495
00:48:42,720 --> 00:48:46,000
Yeah, you know, I think there are certain trends that are going to happen that are not fully

496
00:48:46,000 --> 00:48:49,600
technologically related, and there's going to be things that are going to be AI related.

497
00:48:49,600 --> 00:48:53,920
The non-technologically related, I hope we do move, as I mentioned earlier, to a competency-based

498
00:48:53,920 --> 00:48:59,600
system, less seat time, that we can go more to a mastery-based system. And that starts to involve

499
00:48:59,600 --> 00:49:04,080
a little bit more technology, because if you're asking a teacher to say, hey, if a student got

500
00:49:04,080 --> 00:49:07,840
to see the first time, they should have another chance at that assessment. Well, who's going to

501
00:49:07,840 --> 00:49:12,960
write that assessment? That's work. Who's going to grade that assessment? That's work. And so,

502
00:49:12,960 --> 00:49:19,040
I think that's where the technology plays into it. In terms of how the technology is going to

503
00:49:19,040 --> 00:49:27,920
manifest itself, for sure, all of that support work that we used to bury teachers with, I think

504
00:49:27,920 --> 00:49:32,880
is going to be dramatically streamlined. Once again, lesson planning, writing exit tickets,

505
00:49:32,880 --> 00:49:38,080
grading papers. Teacher's always in charge, but I mean, imagine being a seventh grade English

506
00:49:38,080 --> 00:49:44,960
teacher, and every two weeks, you literally have to sit there and read 100, 150 papers about

507
00:49:45,840 --> 00:49:51,680
great expectations from seventh graders. I could imagine by the third or fourth paper,

508
00:49:51,680 --> 00:49:57,200
it can get a little bit tedious. Instead, you had a reliable AI that can give analysis,

509
00:49:57,200 --> 00:50:02,160
you can spot check it. You're the final arbiter, but it could take that thing that was ruining

510
00:50:02,160 --> 00:50:09,680
your weekend to a task that might take an hour instead of 10. So, you're going to see that.

511
00:50:09,680 --> 00:50:13,920
You're going to see the AIs get better and better at supporting the students, not just conceptually,

512
00:50:13,920 --> 00:50:19,520
not just academically, but I think from an executive functioning, from a metacognitive. So,

513
00:50:20,240 --> 00:50:24,480
one project we're working on, we call it Proactive Conmigo. It doesn't just act as a

514
00:50:24,480 --> 00:50:29,200
Socratic tutor. It's going to start messaging students. Where are you? The things I used to do

515
00:50:29,200 --> 00:50:35,920
at Nadia back in the day. Hey, you said you were going to hit these goals by Friday. It's Saturday

516
00:50:35,920 --> 00:50:41,520
now, and you're only halfway. Come on, what's going on? It's going to be able to facilitate a lot

517
00:50:41,520 --> 00:50:47,120
more communication between the teacher, the student, and the parent. When it finds that,

518
00:50:47,120 --> 00:50:51,120
hey, there's something off parent, you should know about this. And hey, teacher, by the way,

519
00:50:51,120 --> 00:50:55,520
I already told the parent that this is happening. I think you're going to see more of that.

520
00:50:55,520 --> 00:51:00,400
I think the whole area of assessment is going to be really, really interesting. A lot of the

521
00:51:00,400 --> 00:51:06,160
criticism of education over the last 20 or 30 years is we've tried to measure more things,

522
00:51:06,160 --> 00:51:10,240
which for the most part is a good thing. And we've been trying to measure it in standardized

523
00:51:10,240 --> 00:51:16,400
ways, which for the most part is a good thing. But because so many people started indexing

524
00:51:16,400 --> 00:51:21,120
on these things that you can measure in a standardized way, they maybe have lost sight on

525
00:51:21,120 --> 00:51:28,400
other things. I was talking to a senior administrator at Harvard, and he was telling me that even at

526
00:51:28,400 --> 00:51:34,720
Harvard, they're seeing almost an epidemic of kids who can't write. And if Harvard's seeing that,

527
00:51:34,720 --> 00:51:39,360
I can guarantee you it's 10 times worse pretty much everywhere else. And it's probably,

528
00:51:40,240 --> 00:51:44,320
no one knows exactly why. Well, think about what standardized testing has been for the last

529
00:51:45,120 --> 00:51:50,320
20 years. There's no writing in it because writing has fundamentally been hard to assess,

530
00:51:50,320 --> 00:51:55,040
or to be able to give someone standardized clear feedback on. And so it's kind of maybe

531
00:51:55,040 --> 00:52:00,880
started to disappear in many cases. We see in the other things in math, when people focus on a

532
00:52:00,880 --> 00:52:06,080
multiple choice assessment only, multiple choice can have value. There's nothing inherently wrong

533
00:52:06,080 --> 00:52:11,200
with multiple choice, but it might squeeze out other things if we put too much emphasis on it.

534
00:52:11,200 --> 00:52:17,520
And so I think AI with its ability to make sense of more open-ended responses,

535
00:52:18,320 --> 00:52:22,800
and even visual responses, or even video responses, I think it's going to broaden the

536
00:52:22,800 --> 00:52:27,680
aperture of how we assess and how we even assess in a standardized way. And that's going to be good

537
00:52:27,680 --> 00:52:33,680
because it's going to allow school to going back to, let's not just focus on what can be assessed,

538
00:52:34,560 --> 00:52:38,960
or narrowly assessed, but what's important. And now we can hopefully assess that in a broader way.

539
00:52:39,920 --> 00:52:45,200
So do you think, is this hopeful, or do you think this actually will happen in the educational

540
00:52:45,200 --> 00:52:56,800
system? We're working on it. I'm both. There's definitely a possibility that a lot of folks,

541
00:52:56,800 --> 00:53:02,000
well-intentioned folks, have good ideas, and either they don't get traction in a system that

542
00:53:02,000 --> 00:53:07,600
historically has sometimes been slow to move, or it manifests itself in unproductive ways.

543
00:53:08,560 --> 00:53:13,440
That's one of my fears. That's why I tell our team at Khan Academy why our role is important.

544
00:53:13,440 --> 00:53:17,760
We're not for profit. We are focused on how does this actually drive impact?

545
00:53:17,760 --> 00:53:23,440
There are other players. There may be more sales focused, and they're just happy to make the sale

546
00:53:23,440 --> 00:53:30,320
however it gets used. And that might not lead to the best possible outcomes for teachers and students.

547
00:53:30,320 --> 00:53:35,200
But generally speaking, I'm, obviously, I plan on devoting my life to this,

548
00:53:35,200 --> 00:53:40,640
so I wouldn't be doing it if I didn't think that this was going to happen.

549
00:53:42,160 --> 00:53:47,440
And so you mentioned earlier that a lot of times students don't retain what they learn.

550
00:53:47,440 --> 00:53:52,240
I think this is really true. It's what you learn in history class in ninth grade.

551
00:53:52,240 --> 00:53:57,040
I would say there's probably a 1% chance you remember it now, unless it's something that

552
00:53:57,040 --> 00:54:04,400
particularly fascinates you. What do you think will change in the system that will increase

553
00:54:04,400 --> 00:54:10,320
retention? I think if we are more explicit about moving to a competency-based system,

554
00:54:10,320 --> 00:54:15,600
where we say, well, one, competency-based system makes you have to get more clear

555
00:54:15,600 --> 00:54:21,520
about what you care about. And look, some of the things that we learned in ninth grade that we

556
00:54:21,520 --> 00:54:29,600
forgot, that might be okay because it was less about learning the text of, I don't even remember

557
00:54:29,600 --> 00:54:33,840
even the eighth amendment, the text of the eighth amendment, but it was more about learning

558
00:54:34,560 --> 00:54:40,400
to critically analyze things, et cetera. And that might hopefully be a skill that we have retained.

559
00:54:40,400 --> 00:54:44,240
But there probably is content knowledge that we do want. In fact, there for sure is content

560
00:54:44,240 --> 00:54:50,000
knowledge that we want people to retain. But we need to get just more explicit about that.

561
00:54:51,040 --> 00:54:56,880
And then if we get more explicit about, look, everyone who graduates from high school should,

562
00:54:56,880 --> 00:55:03,040
for the rest of their life, be able to solve a simple equation. They should for sure know

563
00:55:03,680 --> 00:55:13,440
who Alexander the Great was. They should for sure know how to make sense of a informational text

564
00:55:13,440 --> 00:55:19,520
written at at least maybe a seventh or eighth grade level, which is what most texts in our life

565
00:55:19,520 --> 00:55:24,560
are actually written at. And then if we can focus on those things, because competency-based learning

566
00:55:24,560 --> 00:55:29,920
also focuses you, because right now there's a lot of just hoop jumping and seat time and this

567
00:55:29,920 --> 00:55:35,280
or that that could be very idiosyncratic to a particular classroom. It allows you to focus

568
00:55:35,280 --> 00:55:39,760
on those things and make sure those things happen. And then hopefully streamlines from some of the

569
00:55:39,760 --> 00:55:45,520
other busy work that doesn't have to happen as much. But do you ever see a point where, okay,

570
00:55:46,080 --> 00:55:51,600
this student's really good at math? Oh, he spent one week learning algebra one. That was all he

571
00:55:51,600 --> 00:55:55,840
was focused on. So now he's able to move on to algebra two, and he goes at his pace. And another

572
00:55:55,840 --> 00:56:02,240
student actually goes at a slower pace. And so you do have, for a classroom of 30 students, you do

573
00:56:02,240 --> 00:56:07,760
have people going at 30 different paces, and they're immersed. So that increases retention. There's

574
00:56:07,760 --> 00:56:14,560
less multitasking from going from biology to history to math or whatever. So some of that's

575
00:56:14,560 --> 00:56:20,320
already happening. After I wrote my last book, and my oldest at the time was entering kindergarten,

576
00:56:20,320 --> 00:56:25,040
we started a school con lab school that implements a lot of these. And I won't claim that we figured

577
00:56:25,040 --> 00:56:31,840
it all out. In fact, I'm always pushing the school to be thinking a little bit more about

578
00:56:31,840 --> 00:56:35,360
questioning a lot of the assumptions. But, you know, at that school, yeah, you do have

579
00:56:36,320 --> 00:56:40,000
a good number of students. And math is probably where we're seeing it the most,

580
00:56:40,640 --> 00:56:45,760
where they could easily be three, four, five, six grade levels ahead. There's a lot of students who

581
00:56:45,840 --> 00:56:52,080
are probably, you know, at or one or two grade levels ahead. And there's a few who, you know,

582
00:56:52,080 --> 00:56:57,040
the faculty is working extra hard to make sure that they're going at least at a reasonable pace,

583
00:56:57,040 --> 00:57:00,320
that they're going to get at least get to calculus before they leave high school,

584
00:57:00,320 --> 00:57:05,200
which in the broader world is already a win, because most kids don't even get there. So we're

585
00:57:05,200 --> 00:57:13,520
already, we're already seeing that type of reality. But I think we need to see more experiments. I

586
00:57:13,600 --> 00:57:17,360
would, you just mentioned of like being able to focus on certain things at a time. I'm intrigued

587
00:57:17,360 --> 00:57:22,320
with that idea. There's universities like Colorado College, or colleges, I should say, that do do

588
00:57:22,320 --> 00:57:28,480
that. You take classes, instead of taking four or five classes, you take one at a time, but you

589
00:57:28,480 --> 00:57:32,320
do it for two weeks. And that's all you do. And then you go and switch and you do another one.

590
00:57:32,320 --> 00:57:36,960
I think that's interesting. And I don't think it's just going to be the con lab schools and the

591
00:57:36,960 --> 00:57:42,080
Colorado colleges of the world. We have partnerships with very mainstream school districts. Newark,

592
00:57:42,080 --> 00:57:46,400
New Jersey, we're seeing some really good things. And they're doing a bit of a hybrid where the

593
00:57:46,400 --> 00:57:50,640
teachers are using Khan Academy to assign the daily practice. And by the way, the kids have

594
00:57:50,640 --> 00:57:56,240
support from Khan Mego. But the district says, Hey, look, if you finish your assignment and you

595
00:57:56,240 --> 00:58:01,520
did it well and you got, you showed your proficiency, you can keep going. But if you did the assignment,

596
00:58:01,520 --> 00:58:06,960
but you only got 30% of it right, you should have another go at it. And now it's going to be

597
00:58:06,960 --> 00:58:11,840
different items because it's sampling from a very deep item bank, or maybe you're not ready for

598
00:58:11,840 --> 00:58:16,720
that assignment because you're missing some prerequisite skills. How do we get you to get

599
00:58:16,720 --> 00:58:21,040
some more practice on that without falling too far behind? This is happening in Newark, New Jersey,

600
00:58:21,040 --> 00:58:29,440
as we speak. It's interesting. In 1982, I was part of this program where these seventh graders would

601
00:58:29,440 --> 00:58:35,200
take the SATs. And if you did well, you were invited to participate in this program at Duke

602
00:58:35,200 --> 00:58:41,680
University called TIP, where it had this immersion experiment. And I always remember, finally,

603
00:58:41,680 --> 00:58:47,280
like in three weeks, I was able to go through like a couple of years of math. And then when I got back

604
00:58:47,280 --> 00:58:51,600
to regular school, so I was in a summer program. And when I got to back to regular school, I was

605
00:58:51,600 --> 00:58:57,040
able to advance more quickly. And I always thought that was an incredible experiment that they were

606
00:58:57,040 --> 00:59:01,520
doing. And then at the time, there was only 22 students in the program. Now there's like tens of

607
00:59:01,520 --> 00:59:07,600
thousands across various campuses. And I just wonder when kind of the mainstream educational

608
00:59:07,600 --> 00:59:11,440
system is going to start adapting this, but it sounds like it is to some extent.

609
00:59:11,440 --> 00:59:17,520
It is. And if you look at communities where I live, I live in the middle of Silicon Valley,

610
00:59:17,520 --> 00:59:24,000
what you're describing is more the norm than the exception. But yeah, and I think the work is how

611
00:59:24,000 --> 00:59:28,800
do we make that accessible and the norm more broadly. And that's why most of our work at

612
00:59:28,800 --> 00:59:34,320
Khan Academy, most of the places where we are deploying Kanmigo are large-scale public school

613
00:59:34,320 --> 00:59:40,400
districts. But we're seeing more traction than I would have guessed a couple of years ago.

614
00:59:41,760 --> 00:59:47,360
Now in terms of the existential threat for various industries, one area that has been

615
00:59:47,360 --> 00:59:54,640
a big concern, and you mentioned this quite a bit, is the ability to write. Like this has been a big

616
00:59:55,520 --> 00:59:59,200
debate in education. Like you mentioned at Harvard, some students don't even have the

617
00:59:59,200 --> 01:00:06,400
ability to write. And then in the creative industry, like screenwriting, I think there is a real

618
01:00:07,760 --> 01:00:20,160
question. Will AI, will the GPT-7 be able to write an Oscar-winning movie? And I think the

619
01:00:20,160 --> 01:00:24,320
conclusion in general, and your conclusion is that it won't be, but it'll be a useful tool.

620
01:00:24,400 --> 01:00:30,000
Again, for the creative to be even more creative. But I think people are very worried about this.

621
01:00:30,000 --> 01:00:35,440
And I will tell you, just from conversations I've had with heads of major movie studios,

622
01:00:35,440 --> 01:00:39,040
they are looking for solutions other than screenwriters.

623
01:00:39,760 --> 01:00:49,040
Yeah. Well, I think there's a bunch in this. So on the first level, most writing essentially has

624
01:00:49,040 --> 01:00:57,280
two pieces to it. There is the putting words on paper in a structured, hopefully engaging way,

625
01:00:58,080 --> 01:01:03,840
grammatical way. And then there's the, how do you come up with what you're going to write?

626
01:01:03,840 --> 01:01:09,600
So if you're a journalist, I would think most of your work should be going out there talking

627
01:01:09,600 --> 01:01:14,800
to people, looking through public documents, attending public hearings, being on the same

628
01:01:14,800 --> 01:01:19,760
scene of the crime to report what's going on. So that's all the work that I don't think AI is

629
01:01:19,760 --> 01:01:26,000
going to be able to do anytime soon. And then the journalist takes all of that, and then they write

630
01:01:26,000 --> 01:01:32,560
the article, you know, and all of that. And so I think AI there very clearly has a role of, well,

631
01:01:32,560 --> 01:01:38,160
if it can help the journalists, let's say take all of their notes that they just got from multiple

632
01:01:38,160 --> 01:01:43,520
resources, sources, maybe different recordings, maybe even videos, and then help them get to a

633
01:01:43,520 --> 01:01:47,840
first draft pretty fast, maybe with a little bit of prompting. I think that's a win for that

634
01:01:47,840 --> 01:01:52,240
journalist. They're going to be able to spend more time on the information gathering and less

635
01:01:52,240 --> 01:01:56,400
time on the wordsmithing. But once again, I don't think it's that you're just going to take all that

636
01:01:56,400 --> 01:02:00,800
input and let the AI just pop something out that, okay, let's put it in the New York Times. No, the

637
01:02:00,800 --> 01:02:04,960
journalist is then going to tweak it and say, no, that they're going to act more like an editor.

638
01:02:04,960 --> 01:02:09,360
And they're going to say, no, that let we're burying the lead, let's put this quote up front,

639
01:02:09,360 --> 01:02:13,120
et cetera, et cetera, let's tighten it, let's make it a little bit more punchy. So there's still

640
01:02:13,120 --> 01:02:18,240
going to be work and the craft of writing and the ability to recognize good writing is still

641
01:02:18,240 --> 01:02:26,160
going to matter. I think if we go to the movie industry, I'm not sure exactly how this is going

642
01:02:26,160 --> 01:02:33,120
to play out, but, and I write about this in the book, I think what is going to happen is AI is

643
01:02:33,120 --> 01:02:40,720
going to lower, like I found it ironic that it was the screenwriters who are afraid of AI and

644
01:02:40,800 --> 01:02:47,120
that the production companies really want it. I actually think the AI is going to bring the

645
01:02:47,120 --> 01:02:52,400
balance of power to the individual, to the screenwriters and take it away from the gatekeepers

646
01:02:52,400 --> 01:02:57,120
who are the production houses. We already saw that before AI with things like YouTube and social

647
01:02:57,120 --> 01:03:02,560
media. I mean, you're even seeing this with mainstream news now. It's essentially been

648
01:03:02,560 --> 01:03:10,400
disrupted already by YouTube and social media. The ability for someone to publish and be discovered

649
01:03:10,400 --> 01:03:17,200
without having to go through gatekeepers now is completely transformed. Justin Bieber is a

650
01:03:17,200 --> 01:03:22,960
YouTube artifact. I'm a YouTube artifact. People can publish podcasts now. You don't have to make

651
01:03:22,960 --> 01:03:28,160
a pitch to some executive at wherever to do that anymore. That's the way the world was 30 years

652
01:03:28,160 --> 01:03:35,120
ago, 40 years ago. With AI, I think you're going to see a similar, you're lowering the cost of

653
01:03:35,120 --> 01:03:42,480
getting into the game. Today, if I have a great idea for a science fiction movie, and I have a few,

654
01:03:43,280 --> 01:03:48,960
I dream about one day, not only would I have to write a screenplay, I would have to get in

655
01:03:48,960 --> 01:03:53,200
in front of people who take me seriously. Even if I wrote a great screenplay, I might not even

656
01:03:53,200 --> 01:03:58,240
be noticed unless I'm connected in the right ways, unless I get the right people to read my screenplay.

657
01:03:59,680 --> 01:04:03,600
Someone will say, it was a pretty good screenplay. I'll pay a couple hundred grand for it if it's

658
01:04:03,600 --> 01:04:08,560
really good. In most cases, I'll pay a couple of tens of grand for it. Go write another one

659
01:04:08,560 --> 01:04:14,560
while you're living off of ramen. Then that movie studio might throw some of them away,

660
01:04:14,560 --> 01:04:17,920
but eventually say, okay, we're going to put $100 million behind this one,

661
01:04:17,920 --> 01:04:22,480
hire actors, director, blah, blah, blah, blah, blah. Five, 10 years later, the movie comes out.

662
01:04:23,040 --> 01:04:28,160
If it's a blockbuster, the movie studio is going to make hundreds of millions of dollars that the

663
01:04:28,160 --> 01:04:33,680
screenplay writer probably made less than that, a lot less, a lot less. If they're very savvy,

664
01:04:33,680 --> 01:04:36,880
they might have gotten a little bit of a cut of that movie, but in most cases, they're getting

665
01:04:36,880 --> 01:04:43,200
next to nothing. In the world we're going into, if I have a great sense of story, if I know what a

666
01:04:43,200 --> 01:04:50,000
great movie should look like, I don't have to stop at the screenplay. I will be able to produce the

667
01:04:50,000 --> 01:04:57,760
entire movie, probably for tens of thousands of dollars or less, including editing, sound.

668
01:04:58,800 --> 01:05:03,840
I'm not going to have to pay actors. I would worry if I'm an actor, actually, in this world,

669
01:05:04,400 --> 01:05:08,720
but the creative who's at the core of the idea is going to be able to go much, much further,

670
01:05:08,720 --> 01:05:13,120
and then they're going to be able to self-publish it on YouTube and monetize it on YouTube, or

671
01:05:13,120 --> 01:05:17,360
maybe there'll be paths, maybe there'll be a YouTube slash Netflix-like thing where

672
01:05:17,440 --> 01:05:23,200
slightly vetted people can surface and put their content out. I would be much more worried if I was

673
01:05:24,720 --> 01:05:28,800
production houses. The production houses probably are going to invest in it. They're going to figure

674
01:05:28,800 --> 01:05:34,640
out ways to do editing much cheaper. They are going to be able to write screenplays,

675
01:05:36,880 --> 01:05:42,960
but a commodity screenplay is very different than a great screenplay. I think for society,

676
01:05:42,960 --> 01:05:46,320
it's going to be good because think about how many hundred million dollar duds there are,

677
01:05:46,320 --> 01:05:51,920
how many bad movies, frankly, most hundred million dollar movies are bad. What a waste of

678
01:05:51,920 --> 01:05:59,280
society's resources. Now, we're going to have a bunch more of movies that probably cost 10 or

679
01:05:59,280 --> 01:06:04,960
$100,000 to make, and they're going to have a higher percentage of bad ones because it's going

680
01:06:04,960 --> 01:06:08,400
to be like YouTube, but we're also going to have a higher total quality of good ones.

681
01:06:09,120 --> 01:06:15,520
It's interesting because Tyler Perry, for instance, he shut down plans to

682
01:06:16,160 --> 01:06:22,480
build a hundred million dollar studio space because he says AI is going to do this. He's a big

683
01:06:22,480 --> 01:06:28,560
movie creator. He has the same view as you. I think I agree, but it's interesting to see how

684
01:06:29,280 --> 01:06:34,080
nervous the writers are. Maybe it's just like an insecurity of writers because they haven't been

685
01:06:34,080 --> 01:06:38,160
historically making that money. They've been kept down in the system, so they just assumed

686
01:06:38,160 --> 01:06:42,880
they're going to be kept down again. But you're right, it is going to give them power. But it's

687
01:06:42,880 --> 01:06:50,480
interesting from your book, there's various groups of people that have these fears. There's

688
01:06:50,480 --> 01:06:54,720
creatives have these fear. There are teachers that have this existential fear. There's professors who

689
01:06:54,720 --> 01:06:59,120
worried the students are cheating. An interesting chapter is the one on parenting. Do you see

690
01:06:59,120 --> 01:07:05,200
parents being afraid AI could take their place? I don't think any parents, well, hopefully parents

691
01:07:05,200 --> 01:07:15,200
aren't afraid of AI taking their job as a parent. I think most of the fears of a parent are we see

692
01:07:15,200 --> 01:07:21,360
directly our kids getting addicted to devices. We either see directly in our own families or schools

693
01:07:21,360 --> 01:07:27,520
or we read about how things like social media and cell phones are affecting mental health of

694
01:07:27,520 --> 01:07:34,000
especially young people, teenagers, probably disproportionately young girls. That's scary

695
01:07:34,400 --> 01:07:38,160
to parents. And so when we see a new technology that's as powerful as AI,

696
01:07:38,160 --> 01:07:42,560
okay, is this going to make everything worse? Not really having a clear idea of how it will

697
01:07:42,560 --> 01:07:50,080
make it worse. My hope once again is I actually think used well, I'm sure there's going to be

698
01:07:50,080 --> 01:07:58,320
use cases of AI that are not great, that can almost amplify a lot of the bad things that we

699
01:07:58,320 --> 01:08:03,440
already see on the internet, whether it's marketing to you, whether it's forms of social media that

700
01:08:03,440 --> 01:08:09,040
make you feel insecure or you have permanent fear of missing out or whatever's going on

701
01:08:09,040 --> 01:08:13,680
or hurt your body image, whatever. But I think there's going to be use cases, these are the ones

702
01:08:13,680 --> 01:08:18,960
we're going to focus on, hopefully others focus on it as well, where the AI beyond just helping you

703
01:08:18,960 --> 01:08:24,880
academically, it can help you even handle the world that you're dealing with. I write in the book

704
01:08:24,880 --> 01:08:31,120
about the AI acting as a guardian angel. Right now, when you surf the internet and our children

705
01:08:31,120 --> 01:08:37,680
surf the internet, they don't realize it, but they're already doing battle and they don't even

706
01:08:37,680 --> 01:08:42,480
know they're in a battle with AI's where these AI's are figuring out the next thing to show on

707
01:08:42,480 --> 01:08:48,160
their social media feed or the search results or the next ad. And their objective function is what's

708
01:08:48,160 --> 01:08:52,880
going to get you to click on that ad or what's going to make you watch longer. And unfortunately,

709
01:08:52,880 --> 01:08:58,000
it seems like triggering content, polarizing content, content that makes you feel bad about

710
01:08:58,000 --> 01:09:04,080
yourself is the stuff that makes you sit on it longer. And any of us who have fallen into it,

711
01:09:04,080 --> 01:09:08,640
we've all spent some time on the internet and clicked on an ad or spent more time on social

712
01:09:08,640 --> 01:09:13,920
media and we never feel good about it when we're done. We all feel like we kind of wait.

713
01:09:13,920 --> 01:09:19,360
Is that true? I feel like that's a little bit of a cliche. Sometimes I surf the internet and I

714
01:09:20,240 --> 01:09:30,240
enjoy the experience. For me, it's true. There's definitely times where my friend sends me a fun

715
01:09:30,240 --> 01:09:36,800
video or a meme or I fall into it a little bit on YouTube or on TikTok and I get a good giggle and

716
01:09:36,800 --> 01:09:44,240
that was exactly what I needed that day. But more often than not, we have fully developed front

717
01:09:44,320 --> 01:09:50,000
to lobes and we can regulate ourselves after 10 or 15 minutes of that. But also even TikTok,

718
01:09:50,000 --> 01:09:55,120
which I've shut down my account and I had a reason to be on it. We had a social media

719
01:09:55,120 --> 01:10:00,640
following. I just took it off because I found even myself, I just went to TikTok to announce a new

720
01:10:00,640 --> 01:10:06,000
feature on Khan Academy and then I end up just swiping and swiping and then 10, 15 minutes

721
01:10:06,000 --> 01:10:10,400
go by. I'm like, I don't feel good about what I'm doing with my day and I stop. I was able to stop

722
01:10:10,400 --> 01:10:14,320
at 15 minutes. There's a lot of young people who aren't stopping at all and they're going two,

723
01:10:14,320 --> 01:10:19,040
three, four hours. And look, it's great if someone has the self-regulation to be able to stop after

724
01:10:19,040 --> 01:10:24,240
15, 20 minutes. That's probably healthy fun, as long as it's not making them feel bad about

725
01:10:24,240 --> 01:10:29,680
themselves. But I am imagining a world now and we're working on this where an AI can

726
01:10:30,720 --> 01:10:36,800
act on your behalf or act on your parents or your teacher's behalf where, yeah, okay, I'll let my

727
01:10:36,800 --> 01:10:42,320
daughter use a cell phone, but I want to see a world where there is an AI agent on that phone

728
01:10:42,320 --> 01:10:47,040
that I am in conversation with. And I would tell them, like, look, I'm cool with her spending

729
01:10:47,040 --> 01:10:53,680
some time on it, but let's be careful about her seeing things that hurt her body image or things

730
01:10:53,680 --> 01:11:00,480
that make her feel like she's missing out on things or just put her into this polarizing haze

731
01:11:00,480 --> 01:11:05,600
or whatever it might be. Then the AI can make sense of what she's surfing and looking at and say,

732
01:11:05,600 --> 01:11:11,520
hey, we've just spent the last 10 minutes looking at pictures of that friend's birthday party

733
01:11:12,320 --> 01:11:17,600
and when you were out of town, maybe we want to go do something else. Or if it can see patterns in

734
01:11:17,600 --> 01:11:22,560
what the daughter is doing, tell the parents, hey, did you know you're child spending an awful

735
01:11:22,560 --> 01:11:27,680
lot of time doing X, Y, or Z? One, that might just be something to police a little bit more,

736
01:11:27,680 --> 01:11:31,600
or it might be an opportunity to have a connection. Like, did you know that they're really interested

737
01:11:31,600 --> 01:11:36,240
in crocheting? Maybe you should take it up. Maybe you should have a conversation with them. This

738
01:11:36,240 --> 01:11:41,520
is something to talk about at the dinner table. So I'm hoping that the AI can act much more as a

739
01:11:43,040 --> 01:11:48,320
used well, a little bit of a guardian angel, and it can help parents, it can support parents,

740
01:11:48,320 --> 01:11:52,640
engage with their children more. I hope that, you know, right now we have devices like Siri

741
01:11:52,640 --> 01:11:57,040
and Alexa and Google Home. And right now they're kind of like, you know, what time is it? Put a

742
01:11:57,040 --> 01:12:02,480
timer on, you know, what's the weather today? You know, how many people live in Ukraine? That's

743
01:12:02,480 --> 01:12:08,640
the type of things we use it for. I would love a time, which I think is going to happen in the

744
01:12:08,640 --> 01:12:15,200
next year or two, where I'm, you know, sometimes we're having dinner together. I want to, and

745
01:12:16,320 --> 01:12:21,360
we're having a conversation, but I know that there's more that my kids have going on in their life

746
01:12:21,360 --> 01:12:26,400
that I'm not getting out of them. When I just ask them, how is your day? How was school? What's,

747
01:12:26,400 --> 01:12:31,280
you know, how's history class going? And I could imagine a world where an AI says,

748
01:12:31,280 --> 01:12:36,320
I could say, hey, Alexa, can you moderate a conversation, a fun conversation between us as

749
01:12:36,320 --> 01:12:40,240
kind of an icebreaker? And it does. And it says, all right, we're going to go around and everyone's

750
01:12:40,240 --> 01:12:44,240
going to say the best part of their day. And then when my nine year old says, oh, my best part of

751
01:12:44,240 --> 01:12:49,840
the day was getting to hang out with my, you know, my friend who was out sick, then the AI could say,

752
01:12:49,840 --> 01:12:54,720
well, why was that? And, and, but once again, it's not squeezing out the parent. It's creating a

753
01:12:54,720 --> 01:13:00,400
context to, as a parent, I create more context where I can engage with my children in a more

754
01:13:00,400 --> 01:13:05,440
meaningful way, as opposed to just being transactional, put that iPad away, go to bed, eat your food.

755
01:13:06,080 --> 01:13:10,320
Hey, it's time to get up. Hey, we're late for the bus. I don't, we all have to do that as parents,

756
01:13:10,320 --> 01:13:14,640
but it squeezes out the like, what do you think is the meaning of life? Or what does it mean to be

757
01:13:14,640 --> 01:13:21,200
a great friend? Or how can we all be better family members? If we could have an expert facilitator

758
01:13:21,200 --> 01:13:26,240
all the time, that'd be pretty incredible. Do you think, do you think, are you going to get a

759
01:13:26,240 --> 01:13:33,840
wearable like AI, like this humane AI pin or, or rabbit or, or maybe make Conmingo a wearable?

760
01:13:33,840 --> 01:13:39,680
Yeah, yeah, it's, it's, I have no imminent plans to do, to do that. Because I'm also someone who

761
01:13:39,680 --> 01:13:46,720
really likes going off the grid, so to speak, and being completely unplugged in certain cases.

762
01:13:47,360 --> 01:13:55,680
But if, if there are devices that come out that could, that I, that I can legitimately believe

763
01:13:55,680 --> 01:14:00,720
will enhance my life in some way, I'd be open to it. And as long as they have the data privacy and

764
01:14:01,280 --> 01:14:07,280
all of the right safeguards in place. Now, I'm wondering what, one big question out there is,

765
01:14:07,280 --> 01:14:14,640
does AI somehow plateau? Like, is there only so much it could learn and simulate, you know,

766
01:14:14,640 --> 01:14:19,360
human conversation? So you mentioned how like GPT-5 is going to have a trillion parameters

767
01:14:19,360 --> 01:14:26,480
as opposed to GPT-4, which just had 175 billion as opposed to GPT-1, which had, I don't know,

768
01:14:26,480 --> 01:14:31,840
a million or whatever it was at a trillion parameters or 10 trillion parameters.

769
01:14:32,480 --> 01:14:35,440
What can AI do that prior versions couldn't do?

770
01:14:36,000 --> 01:14:41,280
None of us know for sure. And I think it is a philosophical debate about if an AI is trained

771
01:14:41,280 --> 01:14:48,640
on human created content for the most part, can it transcend human intelligence in certain ways?

772
01:14:48,640 --> 01:14:53,120
I think the answer is in certain ways it will be able to, because as these, as the parameters

773
01:14:53,120 --> 01:15:00,720
increase, it'll be able to find patterns that are not maybe explicitly obvious to us and maybe

774
01:15:00,720 --> 01:15:06,480
leverage those patterns to create things. But will it be like, you know, a truly transcendent

775
01:15:06,480 --> 01:15:15,280
intelligence? I don't know. I think one of the interesting things that are going to happen is

776
01:15:16,880 --> 01:15:22,240
they're going to be collecting more and more inputs. Right now, it's pretty much human created,

777
01:15:22,240 --> 01:15:29,760
written text for the most part, and now images, videos, sound, files. I think when the AIs are,

778
01:15:31,120 --> 01:15:35,520
get more sensory perception, you know, once you have wearables, once they have cameras,

779
01:15:35,520 --> 01:15:39,520
I know this is a little bit dystopian. So, you know, we have to think about how this gets provisioned.

780
01:15:39,520 --> 01:15:44,640
It's going to get more training data. I also think robotics is interesting because

781
01:15:45,440 --> 01:15:50,800
when we learn, we don't just observe our environment, we play with the environment. I mean,

782
01:15:50,800 --> 01:15:55,600
we literally, that's what that play is literally learning. A kid pokes something, throws it, you

783
01:15:55,600 --> 01:16:02,240
know, shakes it and figures out what, how they're investigating the world. And I can imagine

784
01:16:02,800 --> 01:16:09,040
once you pair AI with robotics and then it can actively investigate the world,

785
01:16:10,080 --> 01:16:16,880
it will get even more input on, and maybe even, and look, it's going to be able to have

786
01:16:16,880 --> 01:16:22,320
sensory input that we don't have. It will be able to see the entire electromagnetic spectrum,

787
01:16:22,320 --> 01:16:27,840
not just that, you know, it's going to be able to hear every, every frequency of sound. It's going

788
01:16:27,840 --> 01:16:33,840
to be able to, you know, detect molecules that we can't at least consciously detect.

789
01:16:34,960 --> 01:16:41,520
So, it's an interesting philosophical debate, but you know, once you start talking about orders

790
01:16:41,520 --> 01:16:48,400
of magnitude, more parameters than we have synapses in the human brain, and it's tireless,

791
01:16:48,400 --> 01:16:53,440
and it has access to all of this information, and it can process it faster than we can.

792
01:16:54,080 --> 01:16:58,000
It is interesting, and I know everything I, even me talking out loud has, I've kind of

793
01:16:58,000 --> 01:17:04,080
scared myself a bit, but, but, but you know, once again, it's all about intent and how we use it,

794
01:17:04,080 --> 01:17:09,440
and, and, and can we use it for, for good purposes, because there, there can be a lot of good purposes

795
01:17:09,440 --> 01:17:17,040
here. I mean, it's just something I said, let's say a good purpose is education. I do, and there's,

796
01:17:17,040 --> 01:17:26,000
there's evidence that I think with a general AI that basically exceeds our current educational

797
01:17:26,000 --> 01:17:32,480
efforts, kids are going to get smarter, meaning they're just going to learn faster, they're going

798
01:17:32,480 --> 01:17:39,280
to retain more, they're going to get that feedback loop much more quickly, and, and so they're going

799
01:17:39,280 --> 01:17:44,480
to be able to, to advance more quickly than our generation was or the generations in between.

800
01:17:44,480 --> 01:17:49,680
I mean, an example that you can see in a specific domain, it's not a general domain,

801
01:17:49,680 --> 01:17:57,920
but look, computers have been better than humans at chess since 1997, and, and computers have

802
01:17:57,920 --> 01:18:03,280
evolved incredibly since then. So it's been, you know, over 20, you know, 25 years or 20,

803
01:18:03,280 --> 01:18:09,520
whatever, 27 years of computers improving since they were already better than humans,

804
01:18:09,520 --> 01:18:17,840
and, and kids use computers to learn chess with some coaching, but 99% is they're playing

805
01:18:17,840 --> 01:18:22,080
the computer and getting immediate feedback. And then there might be some coaching and kids are

806
01:18:22,080 --> 01:18:30,080
so much better now than they were back in 1997. And their rate of improvement is so much faster.

807
01:18:30,080 --> 01:18:35,200
I mean, a 17 year old, just the other day became the challenger to the world championship

808
01:18:35,280 --> 01:18:42,480
that never would have happened 25 years ago. So it's, it's incredible how in, in that specific

809
01:18:42,480 --> 01:18:48,000
domain and other domains like that, kids have advanced and learned and retained so much faster

810
01:18:48,000 --> 01:18:52,160
than kids back then. And I wonder if the same thing's going to happen in, in general education.

811
01:18:52,160 --> 01:18:59,680
I obviously hope so. And it seems like it definitely will. Like our kids will be smarter.

812
01:18:59,680 --> 01:19:04,480
And I, you know, I, I, I typically traffic in optimistic scenarios, but I, and so I hope what

813
01:19:04,480 --> 01:19:08,960
you're saying is right. There's a, there is a, a more dystopian, I don't know if it's dystopian,

814
01:19:08,960 --> 01:19:13,920
but a less optimistic scenario where you're going to have a segment of students who,

815
01:19:14,880 --> 01:19:20,400
who take these tools and run with it and will do exactly what you said. They're going to,

816
01:19:20,400 --> 01:19:25,440
to, to get to heights that we never thought was possible by, at very young ages. And look,

817
01:19:25,440 --> 01:19:29,760
that's, that by itself is not in any way a negative thing. That is a positive thing.

818
01:19:29,760 --> 01:19:35,200
These are going to be the young people who cure diseases for us, who start the next, you know,

819
01:19:35,200 --> 01:19:40,320
great companies who write the next great novel, who produce the next great movies,

820
01:19:41,360 --> 01:19:46,880
all of the above, push AI forward or make sure that AI is used in a safe way. But I do,

821
01:19:48,000 --> 01:19:53,200
I think it's how do we make sure that the other, whatever it is, 60, 70% of students

822
01:19:53,760 --> 01:20:02,720
stay engaged and engaged enough to not be, not be thrown behind by, by all of this.

823
01:20:02,720 --> 01:20:07,680
Because if they are, it's not, it's not, it's not good for them as human beings and it's not good

824
01:20:07,680 --> 01:20:11,440
for society. And so that's where a lot of our, well, we're already seeing with Conmigo in the

825
01:20:11,440 --> 01:20:16,800
classroom is there is a segment of student who just runs with it and they are off to the races.

826
01:20:16,800 --> 01:20:19,600
And once again, no one should hold them back. I mean, sometimes the school system,

827
01:20:19,600 --> 01:20:24,880
temptation is to hold those students back. That is not a good idea. We want these students to,

828
01:20:24,880 --> 01:20:32,000
to move it as fast as they can. But how do we make sure that everyone has a, has a decent chance of,

829
01:20:32,000 --> 01:20:35,920
of being able to participate without holding people back is where a lot of our work is.

830
01:20:37,680 --> 01:20:42,080
Well, and you know, you've been involved in, in obviously been involved in education for

831
01:20:42,080 --> 01:20:47,680
almost 20 years. Con Academy has been around almost that long. Why do we have the old system?

832
01:20:47,840 --> 01:20:53,360
Why do we have credentialing at all where, oh, in order to get a job in a high profile place,

833
01:20:53,360 --> 01:20:58,960
I need a degree from Harvard. Why can't I just go to Con Academy for a year or two,

834
01:20:58,960 --> 01:21:04,960
learn what I need to learn and get a job at Goldman Sachs or Silicon Valley or Hollywood or

835
01:21:04,960 --> 01:21:10,320
whatever. Like, like what is, what is the real role now of, of higher education? Why can't I just

836
01:21:10,320 --> 01:21:17,360
use online education for that? I agree with you 100%. And you're starting to see aspects of that

837
01:21:17,360 --> 01:21:20,960
exist. You're definitely seeing that in, in fields like software engineering, where

838
01:21:22,160 --> 01:21:25,520
you know, the same employers that 20 or 30 years ago might have said, okay, we're only going to

839
01:21:25,520 --> 01:21:33,200
hire from MIT and Caltech and Stanford. They're now saying, hey, anyone can take this boot camp

840
01:21:33,200 --> 01:21:37,280
or take this assessment. And if you pass it, we're going to interview you the same as we would

841
01:21:37,280 --> 01:21:43,200
interview a, you know, 4.0 grad from Stanford. So that's already happening in, in, in certain fields.

842
01:21:43,760 --> 01:21:51,360
I think what you're going to see, and this is a passion of mine, is I do want to create

843
01:21:51,360 --> 01:21:58,080
competency based credentials that have the same or greater prestige as going to Harvard or going

844
01:21:58,080 --> 01:22:03,120
to Oxford. So much so that even if you go to Oxford or Harvard, you'll still want to get these

845
01:22:03,120 --> 01:22:08,720
things to show that you actually learned some, some very useful, very useful skills. So,

846
01:22:09,680 --> 01:22:14,960
you know, systems change is harder than technological change for a whole series of reasons.

847
01:22:14,960 --> 01:22:19,040
But this is something that I, you know, and hopefully I have many decades left on this

848
01:22:19,040 --> 01:22:23,840
planet. But I'm hoping in the next 10 or 20 years, hopefully closer to five or 10 years,

849
01:22:23,840 --> 01:22:28,480
you're going to hear some things even from us about ways that you can get high school,

850
01:22:28,480 --> 01:22:35,200
college credit, potentially even job opportunities via a pretty streamlined route. Once again,

851
01:22:35,200 --> 01:22:38,560
that doesn't mean that it's neither or you might want to do both. But if you don't have

852
01:22:38,560 --> 01:22:43,840
access to Harvard, which very, you know, I was telling some very senior people at Harvard that

853
01:22:43,840 --> 01:22:48,880
right now there's this false tension where they feel, you know, everyone writes about admissions

854
01:22:48,880 --> 01:22:55,520
and it feels like a false tension between equity and merit where there's like, oh, affirmative

855
01:22:55,520 --> 01:23:00,720
action that gets ruled down by the Supreme Court. So how are we going to make sure we have equity

856
01:23:00,720 --> 01:23:06,160
and that we have right, you know, good representation if we have to go more based purely on test scores

857
01:23:06,160 --> 01:23:10,480
or whatever else. And what I've told them is like, you know, it's a false tension because

858
01:23:10,480 --> 01:23:15,040
what you've hold, what you've held constant is capacity. Like you're, you're, you only,

859
01:23:15,040 --> 01:23:21,360
you only admit whatever 2000 students every year, even though there's probably 100,000

860
01:23:21,360 --> 01:23:26,960
qualified students every year. If you could, if you could serve all of the 100,000 qualified

861
01:23:26,960 --> 01:23:32,400
students, you wouldn't have to have these weird, bizarre discussions about admissions and equity

862
01:23:32,400 --> 01:23:35,840
and all of this that people are having, you would just be able to, if you're, if you can handle the

863
01:23:35,840 --> 01:23:40,560
work, we are admitting you and you will get a credential that can get, that can open up the

864
01:23:40,560 --> 01:23:45,200
world to you. So yes, I hope that we can, we can create some pathways like that.

865
01:23:46,400 --> 01:23:50,240
I think the part of that tension is, is the scarcity principle, which is that

866
01:23:50,800 --> 01:23:55,120
Harvard prices their scarcity. And so they make money from their scarcity. You and

867
01:23:55,760 --> 01:24:02,000
Khan Academy or let's say a commercial equivalent makes money from scalability as opposed to scarcity.

868
01:24:02,480 --> 01:24:07,360
And Harvard doesn't want to have 100,000 students. They want to accept only 2000

869
01:24:07,360 --> 01:24:12,160
because that loud, they get charged 300,000 a semester and they'd still fill up. And

870
01:24:14,800 --> 01:24:16,640
you know, I think, I think that's the tension really.

871
01:24:16,640 --> 01:24:21,120
Yeah. And, you know, and obviously our motivation is, you know, our costs go up as we scale. So

872
01:24:21,120 --> 01:24:25,120
our motivation is more of just like, well, how do we maximize impact? But you're right about,

873
01:24:25,120 --> 01:24:30,000
there is, there's always going to be a notion of scarcity and the reality of the Harvard's of the

874
01:24:30,000 --> 01:24:35,360
world. And so much of the education debate focuses on, let's call it about 30 universities,

875
01:24:35,360 --> 01:24:42,080
even though it's less than 1% of students go to those 30 universities. But those 30 universities

876
01:24:42,080 --> 01:24:47,920
are relevant because they, they do over influence society. And so I think there are,

877
01:24:49,040 --> 01:24:53,280
it's not going to go away anytime soon that there's a certain, you know, the US doesn't

878
01:24:53,280 --> 01:24:59,040
have a caste system in the traditional sense, but our, our elite institutions are the best proxy

879
01:24:59,040 --> 01:25:03,760
for them where, oh, oh, you're Harvard class of whatever, wherever, were you there? Oh,

880
01:25:03,760 --> 01:25:08,800
did you have this professor? Oh, wow. Like we're part of the same, part of the same club.

881
01:25:10,080 --> 01:25:16,000
I don't think that's going to go away. What I, and I think there's other positives of not just

882
01:25:16,000 --> 01:25:20,960
a Harvard or a Stanford, but college in general of being there in person, forming bonds. I met

883
01:25:20,960 --> 01:25:25,120
my wife and many of my best friends in college and we have our best memories there. So I think

884
01:25:25,120 --> 01:25:30,000
we can always optimize for some of those in person experiences, but I want to create also

885
01:25:30,000 --> 01:25:34,880
alternative pathways. So that's not the only way that you can get into an upper middle class or,

886
01:25:34,880 --> 01:25:40,960
you know, promising a career. On, on that optimistic note, and I, and I really hope you're

887
01:25:40,960 --> 01:25:46,560
right. I've been writing about this also for a long time. And, and I'm really looking forward to

888
01:25:46,560 --> 01:25:53,120
the utopian world that you have been outlining and creating and explaining particularly in your

889
01:25:53,120 --> 01:25:59,680
book. I'm going to say the title again, because I always forget things really quickly, brave new

890
01:25:59,680 --> 01:26:05,440
words, how AI will revolutionize education and why that's a good thing. And I think what you've

891
01:26:05,440 --> 01:26:10,560
explained today has solved a lot of the, also the fears that many people have about AI, but Salman

892
01:26:10,560 --> 01:26:16,320
Kahn, creator of the Kahn Academy, thanks once again for coming on the show. Thanks for hugging me, James.

893
01:26:40,560 --> 01:26:51,760
From a flat tire in the city to a dead battery on a distant drive, Triple A is partnering with

894
01:26:51,760 --> 01:26:56,880
T-Mobile for business to accelerate response times and get more drivers back on the road fast.

895
01:26:56,880 --> 01:27:01,120
Our nationwide connectivity powers location telematics so Triple A's fleet can find

896
01:27:01,120 --> 01:27:04,880
stranded drivers quickly while being fully equipped with the in-vehicle tools to have

897
01:27:04,880 --> 01:27:10,000
answers when they get there. This is elevating the member experience. This is Triple A with

898
01:27:10,000 --> 01:27:17,680
T-Mobile for business. Take your business further at T-Mobile.com slash now.

