start	end	text
0	5880	Welcome back everybody. We're really glad to see you all here today and to those people online as well
6760	8760	I'm still Jillian Hadfield
9560	14600	And still director and chair nobody's taken me out yet of the Schwartz-Riesman Institute
15840	18560	Looking really forward to these sessions today
19680	21680	new questions and challenges and
23240	26480	Lots of lots of wonderful lots of wonderful sessions
26720	31680	Let's see is there anything else you want to make sure I say before we get started
34160	40120	No, okay, I think we're gonna get we're just gonna move into our first session which is which I think
41000	48960	Yeah, I'll stay up here to do the intro. So this first session is on the reward hypothesis which we got some mention of
49640	51640	yesterday and
51640	53640	And
53960	58600	Will we're interested in this I think rich will give us who is the
59360	61840	the source of the word hypothesis
62520	64520	posited 20 years ago that
65600	72600	Maybe rich you're gonna say this but let me for just summarize it here for that all we mean by goals and purposes can be well thought of as
73440	76880	maximization of the expected value of the cumulative sum of a scalar
77360	79360	Receive scalar signal or reward
80040	85480	So the question that we're gonna be discussing is then is this a good model of
86280	90840	Reinforcement learning is a good model for understanding human behavior and values. How far can it go?
91280	95840	Can it guide normative decision-making for individuals and groups?
96640	97880	and
97880	101760	Totally delighted to have with us on this on this panel
103280	107280	Rich Sutton who I neglected to actually
108080	114320	Introduce yesterday when we started our session with blaze. So my apologies for that because he is
115200	116560	world-class
116560	117440	research
117440	125680	Researcher and reinforcement learning as well as chief scientific advisor fellow in Canada CIFAR AI chair at Amy the Alberta machine intelligence Institute
126040	132160	He's a professor of computing science at the University of Alberta and a distinguished research scientist at DeepMind
132760	140320	He's been named a fellow of the Royal Society of Canada the Association for the Advancement of Artificial Intelligence and the Canadian Artificial Intelligence
141160	143160	Association where he received a lifetime
143440	147520	Achievement award in 2018 and as I mentioned yesterday, we're also very delighted
147520	152640	He's on our advisory board at Schwartz-Reisman and joining rich today is Julia Haas
153200	157240	Senior research scientist in the ethics research team at DeepMind
157720	164840	She was previously an assistant professor in philosophy and neuroscience at Rhodes College and an affiliated researcher with
165400	170880	and use humanizing machine intelligence grand challenge her research is in the philosophy of
171160	176840	Cognitive science and neuroscience. She works on the nature evaluation and its roles and theories of the mind
177120	182400	Your current work includes investigating the possibility of meaningful moral artificial
182840	189440	Intelligence and I will also mention that Julia is this is a return trip to absolutely interdisciplinary for Julia
189440	193560	Which we're also very grateful. It's wonderful to have people saying yes to those invitations
194320	198520	But our previous one of course was online, so it's wonderful to have you here in person Julia
198520	201680	And so rich, I think you're gonna get us started
202480	204480	Thanks very much
213400	215400	Thank you, Jillian
217440	221760	Good to see y'all again. Let's try to have some more fun today
224280	226280	My topic is
226360	230720	Roughly the reward hypothesis, but the reward and other related
231480	234880	Reductionist hypotheses because the reward hypothesis is pretty reductionists
235600	239840	And I do have a slide just stating all so you can all see it
240680	244560	All of what we mean by goals and purposes can be well thought of as the maximization
245080	251120	With expected value of the cumulative sum of a received a scalar signal called reward
252200	253400	so
253400	255400	That's kind of a long
255840	257320	sentence
257320	259320	Sounds like it's got lots of little bits that
259880	261280	that are
261280	263280	intricate
263320	264880	but
264880	268160	But really it just says that maybe the goals of
268920	273720	Whenever you want to talk about goals, maybe you can just talk about maximizing a single number and
276720	279400	That is pretty reductionist and
280880	284200	I'll be talking about that a little bit, but I don't want to start there
284200	288880	I want to start by going back a bit. So this is my outline. I want to talk about intelligence and
289520	293200	To what degree it's intrinsically tied up with the notion of a goal
293720	296480	Which of course is the reward hypothesis is about
297080	300040	And then we'll step into the reward hypothesis fully
300920	302800	And then I also want to talk about you know
302800	309720	What goes on inside because the reward hypothesis is not about what goes on inside the reward hypothesis like how you how?
309720	311720	What's an appropriate framing of the problem?
312240	315600	What's the appropriate framing of of a goal or a purpose?
316320	319560	And it's not about how that purpose is achieved
320280	326040	Okay, but that's what the agent the agent is the intelligent agent like we are the agents and
326480	331880	the robots are the agents and what are the essential for
333320	336000	What are they literally essential components, you know, that's
336840	343920	That's something I want to talk about that. So and then and then some other hypotheses the value function hypothesis is really
344480	351640	important to understand the implications of the reward hypothesis and then I'll have just really just want to slide we're
352400	356880	Prepare us to start to think about how this might have implications for thinking about ethics
358480	360840	Okay, so let's start with intelligence and
362040	363880	Here's some
363880	365440	quotes
365440	368440	William James was the like the original psychologist
369000	373240	His textbook was in 1890 and he spoke about
375080	377520	Well, he didn't spoke about intelligence he spoke about mind
378120	383400	So the hallmark of mind is attaining consistent ends by variable means
384080	391000	What do you think about that? That's really talking about a goal right consistent ends from variable means something is varying its means in order to
391240	396080	Achieve those consistent ends. That's really saying the hallmark of mind is
396680	398680	goal-seeking
399480	402280	And then there's the field of artificial intelligence which is
402760	407440	Famous for not defining what intelligence is or what artificial intelligence is
409800	415040	Yeah, for so long it just refused to do it and you know, I think that's not okay
415320	422400	You have to everyone have a well-defined field. You have to have be clear about what your objectives are and what your subject is
424600	426600	John McCarthy is the fellow who
428040	431960	Invented the term artificial intelligence. He of course is one of the founding fathers
433120	435120	But many years later
436120	438120	He wrote down a specific
438640	444800	Definition that's the one here that I that I like as the computational part of the ability to achieve goals
445560	452640	The computational part the ability to achieve goals. I think that's a really interesting definition
455840	457840	It's not the only definition
458160	462120	Often intelligence is taken to be like mimicking people as in
463120	465120	AI
465160	469440	Seeks to reproduce behavior that we would call intelligent if it was done by people
470040	473360	The classic Turing test is focusing on behaving like a person
474040	476240	supervised learning the task is often to label
476680	483760	Pictures say the same as a person would label those pictures and then the large language models, of course the large language models are all about
484800	486800	mimicking
486920	488920	Text generation by people
489080	492320	Okay, so this is this is a major thing
493240	497800	You notice aren't really any goals involved in mimicking people except, you know
497800	499240	You could say mimicking people as a goal
499240	506520	But really that's not what what I think goals are about goals are about affecting some change in the world
507240	512840	Observing something and then you're being satisfied that you're mimicking it is not about a change in the world
513560	515760	But you know think of all those systems
516840	522000	That are mimicking there. They're not seeking to change their input at all. They're just seeking to mimic it
522800	524800	Okay, anyway, so there's two definitions
524920	528840	one is that intelligence is has to do with mimicking people and
529280	532080	The other one is achieving goals and we might ask
532760	534760	which is better and
535920	541960	I want you to think about that and so I don't want to think about it with a little bit of sophistication
542360	547160	Maybe this is kind of like yesterday, you know, I don't want us to jump to an answer because I want to think about what it means
547160	549160	for there to be an answer and
550000	552840	You don't really have any slides on this, but I do have this slide
553800	559680	That just reminds us that you know a word like intelligence when you look in the dictionary you will find multiple
560440	562200	definitions
562200	566880	The second one being a military intelligence like spies and stuff
567800	571760	So every word is like this and that's the way language is and it's it's good
571920	575880	it's good that there are multiple definitions and multiple meanings for words and
576760	579680	There so there isn't a sense that one is right or wrong. There's
580760	585360	There's a sense in which they're useful for or not useful for particular purposes
586000	591120	So it's our we have there's a free choice when you define a term. It's a totally free choice
591120	594960	There's no right and there's no wrong in the in the definition of a word
595520	597520	but there is
598160	600800	Consequences right because you're you're choosing how to use it
600800	604760	And so then you have to use it that way and you can decide whether for your purpose
605240	608680	That's a useful way to define the term
609200	614680	So the to the extent there is a right or wrong is just it's not it's it's a sense whether they're useful
615480	617480	suited to purpose
617800	619480	so
619480	621480	Now let's go on and try to
622840	624840	Assess the different meanings
625880	627880	and
629000	633680	I want to do that by reference to this quote from Ray Kurzweil
635640	640800	He says that intelligence is the most powerful phenomenon in the universe
644760	653640	So that's not a definition obviously it's it's like a property of what intelligence means something and then and then it's claiming that it's powerful
655480	657320	But
657320	659320	Just take it on his face for a moment
659920	667820	Could intelligence be the most powerful phenomenon in the universe? I mean, what about you know black holes and supernova supernova very powerful
671920	679800	But I think Ray means this literally and I think it's not crazy to mean this literally like you know
680480	682360	supernova
682400	684880	Are big but isn't it it's
685600	687600	Isn't it also plausible that
687640	693280	Well supernova have been around for billions of years. They've had billions of years to develop intelligence has been around for
694120	696120	Few hundreds of thousands of years
696560	700240	If you give give intelligence, you know a billion years
701040	704240	Is it is it doesn't seem almost likely that?
705000	710840	Intelligence if it if it still exists would would be doing things like moving the stars around moving the planets
711280	716040	It would be a powerful phenomenon in the universe at that scale. I
717360	719360	think it's
719600	723760	It sort of expresses the ambition of what we want intelligence to be
725480	727480	Okay, so
728560	732360	So when when I suggested to you that it might
733440	738160	Become a big deal and in a universal galaxy level
739040	744120	What what I was thinking was could the fact that there are
745320	747320	Agents in the world that have goals
748000	750600	And it's best to think of them that way
751360	756880	Could it be that powerful phenomenon eventually and that's what I want to say yes to
760560	766360	For the other meaning could the ability to mimic people be such a powerful phenomenon
768960	772080	I'm I think I think the answer is just no
773360	779440	People are the powerful thing and so mimicking the thing that mimic will gain some power from the people
780360	784320	But the the ability to mimic people is not powerful in this sense
785800	787200	so
787200	794560	That's my first conclusion that the powerful part of intelligence is not the ability to mimic people but the ability to achieve goals and
795360	796800	so
796800	803120	That's a point in favor of using the word in this way the ability to achieve goals
805320	807080	Okay, now
807080	810760	Let's look a little bit deeper into McCarthy's definition
810880	815000	You define as the computational part of the ability to achieve goals
815600	822320	So that computational part is meant to rule out like you I can achieve goals because I'm stronger as I'm faster
822440	824440	And I have better sensors
824680	830500	These would make you better able to achieve many goals, but it would not be because of your computations
831000	834280	So we don't consider that to be intelligence
835000	837000	There are at least that definition doesn't
838120	841360	So moving that up a little bit to give myself give myself a little more room
842560	844560	I think similarly
844800	849240	You could achieve goals better if you're given knowledge about
849960	855760	The world or the domain you were working in you know that would enable you to perform better to achieve goals better
856600	859480	But it's not again. It's not because of your computations
859480	865480	It's not your intelligence is because of the computations of where we gave you that knowledge gave you that help
867000	868760	so I
868760	872360	guess I have a conclusion out of that that I
873000	878920	Want to say the intelligence is the computational and domain independent part of the ability to achieve goals
878920	880920	maybe that's a refinement or a
881440	885600	narrowing of McCarthy's definition, but I think it's in the spirit of
887480	892040	Having a powerful phenomenon of Intel for intelligence being a powerful phenomenon
900040	902040	Okay, so just to summarize that
903040	906400	Mimicry and domain knowledge are not the powerful part of intelligence
907400	913560	Mimicry is getting goal directed behavior without the goals or the processes that compute the behavior from the goals
915800	921440	Injecting domain knowledge is a way of getting gold directed behavior without the processes for obtaining the domain knowledge
922200	924640	So this is sort of the way I understand
925240	929280	The limitations of large language models. They are the abilities
930280	934480	But they were given to them and they don't have the ability to
935520	938360	To develop that knowledge and that behavior themselves
939400	942560	So they're they're both incomplete can't stand on their own
943400	946640	These shortcuts don't have the power of intelligence. They can be very useful
947520	949520	but but
949520	951520	That shouldn't make them intelligence
952240	958040	Using in the word in that way would weaken the search for an understanding of intelligence. That's powerful in Kurzweil sense
958480	961680	Let me just say another another thing on that which is that
962680	968760	intelligence the word intelligence AI AI in particular in today's world has cash a
970240	972160	like
972160	974560	Actually, it's not even just today's world. It's always been this way
975200	976360	but
976360	982960	What I mean is like for example, I went to the grocery store the drugstore the other day and I saw the aisle full of
983840	985840	electric toothbrushes and
986000	991200	You go to the electric toothbrush section today and they will say this toothbrush has AI in it. I
991840	993840	Mean literally they will say that
994000	999560	It has cash a to have AI to have intelligence is as viewed as a very positive thing
999560	1001840	And so everyone wants to be that way
1001840	1008480	I hear that LG has a has a clothes washer that's has AI in it
1009480	1011920	In the old days they used to make
1012680	1014440	computer terminals and
1014440	1020800	They would call the terminals intelligent terminals and all they would do they were only intelligent because they could use multiple fonts
1022160	1025120	You know, so this has always been the case
1026800	1031280	And so there's a tendency for everything to become AI
1031600	1037880	More's law or the generalized Moore's law of increasing
1038760	1041560	Computation per dollar that's gone on for a hundred years or more
1042760	1048880	That is all about increasing computation. It's not about increasing not not it's not necessarily about increasing AI
1049640	1055320	So there's a big there's a tendency to conflate the two trends that AI is becoming more important and
1056320	1059680	Computation is becoming more plentiful. So all everywhere there's computation
1060160	1062820	And that's that's what we're seeing with this
1063520	1065200	Stretching of the term
1065200	1067200	intelligence to cover everything
1067640	1072960	That just uses computation. I don't think that's a useful direction to stretch the term
1075960	1082080	Okay, I think now I'm ready for a joke and so I'm gonna I think I'm ready now to present my present my my joke
1082080	1085680	Or my cartoon. This is my cartoon. You may have seen it
1085680	1092040	To think that this all began with letting autocomplete finish our sentences
1093040	1096480	So this I found this in the New York. I hope I'm not violating
1097440	1099440	important copyright things anyway
1101200	1104360	I'd like I really like this cartoon because it's
1104960	1108360	It's exaggerated. It's making both fun making fun of
1109800	1113600	Both the positive and the negative hype that's around AI
1114520	1122080	The the positive the positive hype is that autocomplete finishing our senses will lead to you know, the robots
1122920	1124920	being super powerful and taking over and
1129920	1132760	Guess the negative hype is that you know if if
1133800	1141120	Computers become powerful if we should robots become powerful they will subject us all to slavery as in as in this picture
1142080	1147400	Okay, so humor is the best way to purify or
1149280	1151280	our thoughts and and
1151800	1153800	Think about controversial things
1154080	1158280	Okay, so now I'm ready to talk more specifically about the reward hypothesis, but I hope you
1158880	1166520	You've gotten the point the point, you know the word hypothesis about how we talk about purposes and goals and
1166960	1171520	Intelligence is is really centrally about goals and purposes
1172240	1178920	so if this is true if it's true that all goals and purposes can be thought of as as
1181800	1189400	Maximizing a single number in this way, then it means that all of intelligence can be thought of as maximizing a simple number
1189920	1199280	So this is one the multiple ways to develop this hypothesis one is to do it mathematically and formally and and
1201200	1205320	Like you know, what's this? What does it mean to have a have a goal?
1205840	1214080	It's an ordering on possible outcomes and what are they all possible different ways of doing an ordering and you're looking at this
1214080	1216080	Carefully as a mathematical formal statement
1216440	1221560	Like one might in do in economics or the theory of decision-making
1222880	1226960	So that's that's the work this idea has been developed in that direction
1226960	1229720	I'm and I'm referring to the work in the corner
1230120	1233480	It's more recent work by Michael bowling and John Martin David
1234000	1238800	Abel and Will Dabney where they formally assess it
1239440	1241440	to the extent that it is
1242320	1248280	Complete and includes anything else you might propose. So what are the other things that are ruled out? You may be thinking, you know
1248960	1252880	What's ruled out would be things like considerations of risk in a special way
1254320	1256320	consideration of multiple objectives
1259080	1263600	You see it's all about the expected value rather than the distribution of possibilities
1265000	1268640	Okay, now another another hypothesis that's been around
1271440	1273440	The reward is enough hypothesis
1274000	1280280	That there's intelligence and all of its associated abilities can be understood as subserving the maximization of reward
1280800	1288200	So this is very similar. It's almost like the combination of the reward hypothesis and the McCarthy's definition of intelligence
1290400	1295760	But notice all these things are about the goals are about the the problem the problem
1296480	1300080	They're not at all talking about solution methods. They're just saying
1301040	1308000	Reward might be enough to motivate and drive the achieving the achievement of the associated abilities, whatever they may be
1309280	1314200	You can understand them as as being driven or subserving the maximization of reward
1315520	1318000	Okay, so now I want to
1319000	1325320	Is reward enough is a single number enough? It doesn't it's it's it's it doesn't seem like enough. I just wanted to
1326320	1330360	Bring this on the table. I'm sure you felt that it seems too small a
1331040	1332360	single number
1332360	1334280	coming from outside
1334280	1336200	the agent
1336200	1341920	You know people seem to choose their own goals. I mean, that's one of the biggest things we see for ourselves
1342280	1345200	We define ourselves by the goals we set out to achieve
1346920	1353440	Reward just seems too small too reductive. It's definitely reductionist hypothesis. It's and as reduct being reductive
1353440	1355240	It's kind of demeaning
1355240	1360720	Surely our goals are grander than maximizing, you know pleasure and pain or some number
1361160	1365720	You know, we have things like raising a family saving the planet making the world a better place
1366200	1368120	contributing to human knowledge
1368120	1370120	Not just these small things so
1370360	1378360	So that's really the tension. I think around the reward hypothesis that it seems like it seems too small and yet it seems
1378760	1384280	We keep being driven back to it as we try to get formal as we try to be clear about goals
1384840	1386080	we're
1386080	1388080	We're driven back to it because it's clear
1388720	1390520	because it's
1390520	1392520	experiential
1394280	1399040	Kind of grounds things it gives us a well-defined place to proceed
1399880	1401880	so now a few slides
1402320	1404320	exemplifying how
1404920	1413080	Even though we're uneasy with this idea we keep coming back to it we what is we I mean, it's an interdisciplinary idea
1414440	1416440	This is the modern view about
1417320	1421920	About how our brains work is that there is there is a measure of
1422560	1424560	Pleasure and pain and then
1424560	1427120	There are calculations. I'll get into a little bit later
1427120	1433520	But it's totally consistent with this and not just we it's like all animals have like a dopamine center
1433520	1435240	and
1435240	1441480	This is a good it has been true that a good way to think about them has been in terms of a scalar outcome
1442320	1444240	Not entirely
1444240	1447160	Okay, so let me go through some facts
1449120	1451760	This is some of these slides a little more detailed than we need
1452720	1457440	But I just want to refer that within AI which is also uneasy with the idea of reward
1458200	1461480	It's it's become more comfortable with it over time
1461880	1465840	So the very earliest AI systems were all formulated their goals as
1466360	1468360	Attain the state of the world
1468680	1470800	Okay, which is which is very different from
1471720	1474740	Maximizes number coming into your into your mind
1475920	1482240	It's as if we can access directly the states of the world which which we cannot of course we have to infer them from our
1482640	1484640	from our sensations
1485120	1487120	but
1487440	1491960	And and I'm saying it's even true the latest version of the standard AI textbook
1492440	1500080	Russell Norvig it still talks about goals primarily in terms of states of the world and not in terms of experience not in terms of reward
1500840	1504720	But it also has chapters on reinforcement learning and those those all use reward and
1505640	1513200	With the rise of machine learning within AI the reward formulation has becoming more and more standard as in planning and mark-up decision processes
1513960	1515680	and
1515680	1519920	We can look at Yanlacoon Yanlacoon who was who's sort of an anti
1520680	1526000	Reinforcement learning person he now admits that if the if the mind is a cake
1526640	1528640	This is his metaphor
1529640	1534840	Is that if the mind is a cake then reinforcement learning is a tiny part of it's like the cherry on top
1535360	1540600	but he thinks the reinforcement learning acknowledges that reinforced learning is necessary because goals are necessary and
1541360	1548520	Either the substance of the of the cake is maybe doing prediction or unsupervised learning and so it those parts are not
1548880	1552160	Goal oriented. They're just trying to predict and understand the world
1552160	1557920	And I shouldn't say just they're trying to predict and they're trying to understand the world and model the world those don't
1559320	1562280	Don't have goals in them. They're just
1563000	1567280	I said it again just gotta watch out for that little word just they are
1568280	1572600	Understanding the the truth of the world as separate from
1573600	1575600	Trying to direct that in any particular way
1576080	1581720	Okay, but anyway, he's got the cherry on the top and to me. It's a cherry on the top of the cake. So it's pretty important
1582640	1584600	and
1584600	1590000	Here's another one from classic AI artificial intelligence
1591000	1595120	There's this these cognitive architectures since the 80s
1595200	1603840	They're very symbolic and using production rules and since like 2008 and they've included reward as part of it as as a as a basis for
1605080	1607600	The the goals of the system
1608120	1613000	So single numbers have becoming more prominent in AI as a formulation of the goal
1613720	1615120	now this
1615120	1618600	now let's talk more in a more interdisciplinary sense and
1619600	1623120	I recently wrote a little paper where I just sort of said, oh, you know
1623520	1626840	There's a lot of commonality between many many fields thinking about
1627600	1636120	Agenthood and goals and purposes in mind, you know in psychology control theory AI economics neuroscience operations research at least these six
1636400	1639360	You can find basically the elements
1639960	1646200	In these two pictures the first picture is the agent interacting with the world or the environment and
1646880	1652960	So this is the basic picture of a of a decision maker. He
1653920	1660640	Sees information from the world his observations and he picks actions and then he gets back a
1661240	1668440	Scalar measure of performance that he with the objective is to maximize it. That's sort of accepting the reward framework
1669000	1671000	You will find this
1671960	1677120	Of course these ideas very basic, but even when you look inside the agent now
1677120	1683480	So I'm gonna look inside the agent how the problem is solved you can find some of the common elements and
1684640	1686640	So there are four of them
1687160	1690400	But let's talk first about the core two
1690840	1696640	So I'm gonna block some of these out. Yeah, the core two is the perception and
1697280	1700760	The policy the reactive policy so perception
1701520	1705560	Takes in observation so observations are like, you know, your sensory input and
1706800	1711520	From that you construct a sense of where you are. That's that's the state representation
1711520	1715600	And then you choose what to do based upon the state representation
1715920	1719960	The difference between the two boxes is that the policy is
1720440	1726160	Memoryless right you can't use old states to decide what you're gonna do now the whole point of a stage as it says
1726160	1729880	This is where I am now this I should decide what to do based on where I am now
1730440	1731760	it's
1731760	1735920	And it's the job of perception to construct that sense of where you are now
1737000	1744880	So perception is recurrent it involves the last thing you did the last observation and the the previous state
1745680	1747680	Now
1749720	1751720	This is a complete behaving system
1751920	1756440	Observation comes in flows through perception and the policy produced in action
1756440	1761880	That's all you need to be a complete behaving system and you notice reward is kind of hanging out there
1761880	1764760	It's not really doing anything and that's because
1766000	1771520	This is a complete behaving system, but not a system that's made up for changing what that system does
1771920	1776040	So let me dig do one more dig at large language models. They are this part, you know
1776040	1778040	they are a way of
1778240	1781240	transforming the sequence of words into a
1782480	1787020	Memory of where you are and then deciding which word to output
1788080	1795120	But it's not a way of changing that the there isn't a once your large language model is in place is running in the world
1795360	1798640	It can't change. It's it's just these two parts now
1798640	1802440	You can change in the sense that perception can accumulate your a
1803120	1809800	More refined state as you get more observations, but you can't get a change in the policy for example
1810520	1813200	or or a change in the
1814080	1816080	Function that is implemented as perception
1820200	1823400	Maybe I'll dwell on this a little bit more and then on the
1825920	1827920	interdisciplinary aspect
1827920	1834960	So we see just remember that all these different fields have used different names like if you're in psychology you talk about
1835720	1838760	so the action you talk about the response and
1840160	1843000	in control theory you would talk about the
1843760	1847280	the control instead of the policy they talk about the
1847960	1849600	control law
1849600	1851400	instead of instead of
1851400	1855360	Perception in in control theory they talk about a state estimation
1856000	1857280	box
1857280	1863400	In psychology the observation would be a stimulus and the reward might be reward
1865280	1867800	But actually the subtleties of the
1869200	1871800	Connotations of the word are somewhat different in psychology
1872560	1874560	So
1877560	1883120	I think it's it's kind of interesting to try to find a relatively neutral language that can
1883840	1886280	Apply to all these all these different fields
1888240	1890000	Okay, so
1890000	1895040	I'm unblocked the transition model the transition model is supposed to be the model of the world
1895680	1899920	It's gonna have your domain major place where domain knowledge is
1900760	1902760	Okay, I'm not gonna talk about that today
1903160	1908280	So let's get rid of that. Anyway, but it would do planning. That's how you do planning
1908760	1910760	We're not gonna talk about that
1912840	1918280	But I do want to talk today as part of talking about the reward hypothesis about the
1918840	1923640	The other box the value function box is the value function is the major source of
1924840	1926840	Learning input of changes in the policy
1927840	1932760	And this notion of a value function is also common to all these fields
1937760	1940440	Hey, I through reinforcement learning has value functions
1941680	1943680	psychology has has
1945240	1950160	Reinforcers and secondary reinforcers and and it's built up in the same way
1951120	1953120	and
1953800	1958440	There are issues of proxy objective functions and control theory that are all about
1959160	1963340	Constructing value functions. So what is this value function thing? Let's talk about that
1964520	1969760	Oh, but I couldn't I couldn't resist I want I got in this x1 extra slide where I'm just
1970200	1975040	It does remind me to say to say one important thing that even though there's only one objective
1975480	1979760	the reward to maximize reward the agent can well as as as
1980480	1982040	Means as
1982040	1988720	Solution strategies it might have it might pose some problems for itself other problems for itself
1988720	1994840	Like you might learn how to how to walk or how to drink a glass or how to navigate to the monk school
1995520	1999000	You learn all those things that they're not your main task
1999440	2005120	But it might if you learn how to do those things it may be useful for solving the main task. So you you
2006120	2010960	My theory of of mind and the next step of that theory would be that we
2011360	2017280	We pose some problems for ourselves and solve those and then use those skills that we develop working on the sub problems
2017280	2020680	In order to solve the real problem, which is to maximize reward
2021920	2026080	Okay, so value functions. We're getting almost done
2026840	2028840	value functions
2029440	2034800	So reward and value reward defines what's good. So we seek a policy that maximizes reward
2034800	2037160	That's done defining the problem
2038080	2043880	But reward is often delayed and that makes it hard to learn a policy using reward
2043880	2050040	And so instead of working directly with reward value functions map states to predictions of the future reward
2050120	2052840	if you have prediction of the future that includes the
2053240	2059800	Delay and if you can bring that into the into the present if you can predict now what the future rewards will be that
2060320	2062080	enables you to
2062080	2065920	Eliminate the delay and makes finding a good policy much easier
2066680	2070200	Maybe that's intuitive you think about I always think about playing chess
2070320	2073040	You know the reward is checkmating your opponent
2073040	2078200	And so then you get maybe a plus one for checkmating a minus one for losing is zero for drawings
2078200	2079480	so that is
2079480	2080840	and of course
2080840	2087000	Formally you get a reward all during the game, but they're all zero and you only get an interesting reward at the end and
2087360	2091080	So there's a delay between any good move or poor move
2091080	2098060	You might make early in the game and and whether you won or you lost and so of course what we would all do is we would
2098440	2102880	Learn to predict. Am I going to be checkmated in this game?
2103320	2109080	Okay, so that's a prediction of the future and it's often called an evaluation function or a value function
2109080	2113760	It's a prediction of the reward. So I think it's it's
2114760	2118160	Well, this is the the value function hypothesis is that
2119400	2122920	Forming such value. I guess that's that's that's right on my slide here
2123360	2128320	The value function hypothesis is that all efficient methods for solving
2129440	2133960	Sequential decision problems, which is what just means decision problems over and over again through time
2135600	2137600	They learn or compute
2138240	2141000	Value functions as an intermediate solution step
2142000	2148520	So when you're gonna play chess you learn the sense of am I when you're losing and then you know when you make a bad move and now
2148520	2152040	You thought you were winning and now you're losing you see then no
2152040	2153960	That's the critical part
2153960	2160720	Where you made the mistake and you can use that to assign credit to your policy and change your policy in a much better way
2162200	2165000	Okay, so that's all sounds awfully technical
2166440	2169920	Let's go back to philosophy go back to Plato, you know
2170720	2177620	You can find in Plato talking about good and evil and pleasure and pain and he will say things that are a lot like this
2178680	2186120	These are some quotes even enjoying yourself. You call evil whenever it leads to the loss of a pleasure greater than its own
2186600	2191120	Or it lays up pains lays up pains that outweigh its pleasures
2191960	2196160	Isn't it the same when we turn back to pain to suffer pain you call good
2196880	2203000	When it either rids us of greater pains than its own or leads to pleasures that outweigh them
2203200	2207040	So what does he say he's saying good and evil are about the sum of upcoming reward?
2208320	2210960	Which is what we try to predict with a value function
2211800	2218360	So basically it's all hedonism. It's all hedonism, but value functions make it hedonism with foresight
2219320	2225680	Okay, that's the story the main story of reward and value how we use them in decision-making to change our policies
2227040	2229440	But I want to try to do one more thing
2231160	2235640	I get you understand the way it works in a little bit more
2236560	2242560	I don't know if this is gonna work, but let me try I call it the mystery of expectation and reinforcement
2242760	2247280	I call it the mystery of expectation and reinforcement now
2247840	2253040	Superficially those are new topics. I haven't talked about expectation and haven't talked about reinforcement
2253120	2260440	But by expectation I mean like the prediction of reward that the value function is the value function learns predictions i.e
2261400	2263920	expectations of upcoming reward and
2265640	2267640	Reinforcement reinforcement
2268640	2273200	Is not reward reinforcement is the
2275160	2279680	Moment-by-moment signal that reinforces the behavior at that time
2279840	2286680	So it's really your sense of did I do something good or did I do something bad is should I stamp what in should I stamp in what?
2286680	2290920	I did or not so reinforcement is not reward because there are many cases
2290920	2296080	And we just alluded to some of them where you get say a positive reward, but you
2296880	2298880	View it negatively
2299520	2304760	And so expectations play a role there so expectations
2305720	2309680	Contribute negatively to reinforcement in the sense like you know I went to the movie
2309680	2314320	I thought it'd be a really good movie and I went there. I was just kind of so so so actually I feel bad about it
2315280	2317480	so so expectations are
2318640	2323080	Negatively to the reinforcement movie was just so so but because you expected it to be good
2323560	2328520	That gave you a bad feeling about it a bad negative reinforcement
2329720	2335160	But expectations also contribute positively to reinforcement like when you
2337360	2346600	Expect to get reward that that is a reinforcing event if you if you I don't know are given a million dollars
2347600	2355680	That's a good thing, okay, but it's actually not really reward reward becomes because you expect to have a million dollars and be able to use it
2355680	2357680	you know to buy
2357680	2363400	Good food or anything you might want so that would be the reward and the expectations are also positive
2363400	2370800	And so that's the mystery. How can expectations contribute both positively and negatively it is in just a common-sense way and
2371840	2373840	the answer is over time and
2374440	2381600	So just consider this simple case of evolving through three states states one two and three and
2383000	2386120	This is like your life some life moment by moment
2386840	2395320	so three moments two transitions and I'll just take it that your expectations your predictions your values of the three states are
2396240	2400080	zero ten and zero and then
2401080	2407160	The rewards on the when you experience these three states are zero and eight
2407160	2410920	Okay, so now I want to ask you the question or ask you to ask this question of yourself
2411560	2413560	How do you feel about the first transition?
2414160	2417920	So what's the reinforcing effect is it positive?
2417920	2422840	It's negative like is it good is it bad as you went from state one to state two and
2423480	2426640	It's I want you to give me a number because it's all coming down to numbers
2427640	2434280	So what number would you give to that transition from state one to state two
2435840	2443200	Expectation was zero now. It's ten the reward was zero. Do you feel good? You feel bad?
2446200	2448200	Anybody
2449880	2452720	John's giving me a thumbs up she was giving me a thumbs up
2453240	2456720	Yeah, I think we're gonna say this is like a plus ten experience because
2457560	2460440	You weren't expecting anything and you didn't get anything right away
2460440	2465080	But you now you now you expect to get ten in the future. That's like the getting the million dollars
2465600	2467600	And now the second transition
2468400	2470120	What do you expect?
2470120	2475520	Expecting ten you got eight and now you don't expect anything in the future
2476720	2478720	That's gonna be like minus two
2478840	2483520	Okay, so that all makes sense. It's very intuitive. Okay, so if you just
2484120	2489080	Make a formula out of what you you've just understood is as obviously what should happen
2489200	2492720	You're gonna end up with temporal difference learning the fundamental algorithm of of
2493600	2500000	Reinforcement learning. Okay, so the the thing you just formed in your head was that the reinforcement is sure it's the reward
2500560	2505600	But beyond that it's the change in your expectation from one time step to the next
2506360	2509320	As has the expectation gone up as it gone down
2509320	2514480	And so it's the sum of the reward and the change in the expectation because it's a change
2514480	2519120	It's a temporal difference temporal difference just means time difference, which is just change
2519440	2524560	It's the change in your expectation contributes to the reinforcement. And so this is the
2525280	2527280	temporal difference error and
2527560	2529560	also called the reward prediction error and
2530320	2534360	I need to finish up, but I did want to mention since it's such an interdisciplinary
2535360	2539720	Conference that this theory of brain systems as
2540720	2542720	Following TD learning is one of the most
2543480	2549200	Important interactions ever between engineering science as a neuroscience is a big big thing around the turn of the century
2550400	2554960	And finally my last slide is on ethics
2555600	2561320	So what we've talked about rewards are a good way to think about the ultimate goal and value functions
2561440	2567960	Which are predictions of reward are a good way to think about how that goal is achieved. All this is neat and complete
2567960	2571480	It's a good theory of a single agents decision making
2571840	2578760	But it's it doesn't have what we would expect what we might hope to get from a theory of ethics is that there would be some
2579040	2581440	universality to it there would be a
2582320	2589600	Reason to for different agents to have similar values, you know, that's what that's that's what I understand ethics to be about
2589600	2591600	It's when we reach for some
2591800	2595900	universality rather than just an individual and what we have so far doesn't have any
2596480	2599480	element of that and I'm gonna now admit that I have no
2600040	2602040	no sense how
2603400	2609680	Any universality could occur and so I want to propose that maybe there is no universality there's a need to be universal
2610080	2613200	Sality for there to be ethics. Maybe we just
2614200	2620200	Many of us have there is an aspect of what we do of commonality between many people and
2620800	2625800	That that's what ethics is about. It's so this is a reductionist hypothesis again
2625800	2631440	that maybe ethics is just values that are happened to be held in common by many agents and
2632160	2634160	so even though
2634600	2638840	Even things that are held in common by many agents won't be held in common by all agents
2639840	2643120	The lion and the gazelle will have different values
2645680	2651880	So that's really all I wanted to say and I thought it would be a good basis for starting a discussion of ethics
2653880	2655880	Thank you very much
2659080	2661080	You're up
2668840	2670840	Jinx
2672040	2676960	Thanks, Rich, I'm really glad we got a chance to do this and thanks to Jillian for the same reason
2678280	2684000	So I'm gonna start with just a little bit of housekeeping disciplinary housekeeping just to make sure we're all on the same page
2685400	2687400	So like any paradigm
2688480	2693280	Reinforcement learning is going to have certain technical and foundational commitments
2694080	2695720	and then
2695720	2702680	Loss of different versions of those frameworks are going to add certain assumptions and relax others
2703040	2704640	And so just before I get started
2704640	2709080	I want to make clear two assumptions that I'm committing to because they will inform kind of what I say next
2709120	2715440	I'm gonna call this RL star or sometimes they can also be called RLDM like reinforcement learning and decision-making I
2716880	2721120	Think it also make clear maybe where where Rich and I agree and sometimes might disagree
2721760	2723760	So the first assumption
2723760	2725760	In my kind of particular
2725960	2731280	Version of reinforcement learning is that there is in fact something really special
2731520	2738000	About reinforcement learning when it comes to understanding the mind, right? And so I think this is an assumption we share we both subscribe to this assumption
2739520	2743960	Peter Diane who just got mentioned and put up as like a little plastic bobble head and yell these
2744800	2750680	Put forward something like the following claim so reinforcement learning algorithms such as the temporal defense learning
2750800	2757800	Rule apparent to be directly instantiated in neural mechanisms such as the phasic activity of dopamine neurons
2758000	2761920	That reinforcement learning appears to be so transparently embedded has made it possible
2762440	2766520	To use it in a much more immediate way to make hypotheses about and
2766880	2774040	Retradictive and predictive interpretations of a wealth of behavioral and neural data collected in a huge range of paradigms and systems, right?
2774040	2776040	So there's this
2776040	2778040	Is the sound coming out already?
2778320	2780400	Yeah, I've been wondering that too. So thanks, Rich
2781000	2787080	There's a star beside instantiated for the philosophers of science in the room. Just relax about that
2787080	2793160	It doesn't matter we can just say look reinforcement learning is shockingly surprisingly good about capturing something of the mind
2793160	2796400	We don't have to commit to instantiate. It's one of those weasel words we talked about yesterday
2797160	2800440	It's a quote from them, but that's just not what we don't want to talk about here
2800440	2802440	There's something about reinforcement learning
2803080	2808480	That is extremely powerful when it comes to understanding the mind whether it's instantiated or not
2808480	2814120	So that's the first commitment that I'm making here the second commitment where we may disagree although
2814600	2815680	sudden
2815680	2820680	June 21st 2023 suggest maybe that's not the case is that
2823040	2830400	Mines like ours so personally assigned subjective rewards somehow so there's a big debate about this question of where do rewards come from?
2830440	2835920	right and the kind of classic view is that reward comes externally and
2836480	2839360	That is an important tenet of some frameworks of reinforcement learning
2839880	2846000	But I'm gonna go ahead and say and this is kind of an agnostic claim that the mind somehow
2847640	2852120	Internalizes reward there are good competing theories for how that takes place
2852440	2856760	Such as by Chris and refilled it doesn't matter. I just want to say that the mind somehow
2857400	2859400	Internalizes reward in ways that are important
2859920	2864280	So that's gonna be a commitment where we perhaps depart from one another
2864840	2867780	But it's gonna inform kind of how we want to go forward about this
2868520	2870520	Okay, so that's a bit of ground-clearing
2870960	2872960	What I want to use my time
2873040	2876520	For today and kind of as a basis or a backdrop for the discussion
2876520	2881880	Let's make three claims about kind of this version of reinforcement learning and what it means
2882440	2886800	For understanding the mind and so the first question that we have right is you know
2886800	2890560	Is the reward hypothesis a good model for understanding the mind?
2891240	2892440	And
2892440	2897680	As rich has pointed out, you know, it's been kind of transformative in the decision sciences and all those schools that we talked about
2898680	2904760	Cognitive neuroscience computational neuroscience psychology economics increasingly. I think in philosophy
2904760	2906520	It's like making inroads in philosophy
2906520	2911320	But I would actually say the first kind of claim that I want to make is that it's still underappreciated
2912040	2919000	In how much it should and can transform my understanding of what the mind is fundamentally in the business of doing
2921040	2924840	So this is kind of the cartoon dialectic slide, right?
2924840	2929280	I think when we think of the mind we have historically but even in
2929680	2934880	Recent kind of cognitive scientific history. So we've seen this in Joel's slide in Blaise's slide
2935560	2940080	Yesterday is you know, when we think of what the mind is fundamentally in the business of doing
2940600	2947120	As how Glenn put it, you know, it's about thinking. It's what intellect. It's it's fundamentally kind of an epistemic machine
2947360	2949360	It's what the mind is a computer
2949480	2955840	And one of the ways that we can put that is that the mind is in the business of performing computations
2956400	2964240	over representations of descriptive matter of fact, which is just to say the mind is in the business of making better and worse kind of
2964920	2969960	Assessments of what's going on out there in the world. Is it raining? You know, what day of the week is it?
2969960	2973520	It's about knowing the world so that we can move around in it
2974360	2979440	In various ways and so more or less factive representations of the world out there
2981120	2987400	And I hope Blaise's ears are ringing right now because he basically went up yesterday and said, you know
2987560	2992280	Maximum entropy first in all these people, you know, a new way of putting these kind of very epistemic
2993080	2995400	Conceptions of mind is that it's all about prediction
2995920	3001840	You know, the mind is a prediction machine and all we do is make predictions about what's out there in the world
3001840	3006920	Then we update them. So but again, it's it's essentially about knowing in some lowercase case
3007400	3009640	Sense of what the mind is all about
3010320	3011880	And I really disagree with that
3011880	3018120	I think the thing that we learn from the reward hypothesis and kind of the reinforcement learning framework that follows from it is
3019120	3023000	That the mind not only performs computations over
3023480	3027720	Representations of descriptive matters of fact, but the mind also
3028720	3033520	Fundamentally performs computations over those representations as better or worse
3034280	3036280	So we are continually
3037120	3041160	Representing the world as we move around in it, you know what this room looks like
3041880	3044000	What day of the week it is whether it's raining or not
3044160	3051360	But as part and parcel of that process, we are also continually representing all of these states as
3051920	3054600	Better or worse with respect to our goals
3054800	3060360	So what we're doing is we're laying over these fabrics kind of of the states and assigning
3061120	3067360	Value in the way that rich just described and so we're not just seeing whether it's raining, but oh, it's raining
3067360	3070800	Here's what that means for me or oh, it's raining. I live in London, of course, it's raining, right?
3070800	3075320	These things can matter more or less, but that we're not just experiencing the world. We're
3076280	3082040	Continually evaluating and so I think one of the things that the report hypothesis and
3082560	3088080	Tells us is that the mind is not just a thinking machine. It's a valuation machine. It's continually
3088880	3090400	evaluating
3090400	3095840	Now one of the examples that I like to use for this is kind of the thin end of the wedge. It's very narrow case
3096440	3098960	But it's the phenomenon of vernacular robbery. So
3099720	3104440	vernacular rivalry occurs when you place some kind of division between the left eye and the right eye
3105200	3111880	And you show one stimulus to one eye and another stimulus to the other eye. So in this case borrowed from
3113360	3117040	Feldman Barrett's lab is one eye sees one eye is presented
3117040	3119760	I should say with a house and one side is
3120240	3122720	presented with a face and what you might expect
3123280	3125720	The participants to see is you know house face
3126040	3131480	But the reason vernacular rivalry is interesting is because what they actually perceive what they actually experience is an alternation
3131760	3135480	They see a house they see a face they see a house and they see a face and
3136240	3139880	obviously for you know, the view that the mind is
3141200	3144800	Engaging with the world that the mind is computer that we're making prediction machines
3144800	3148440	This is a very interesting case because we know for a fact that we are not perceiving
3149320	3152120	What we are essentially seeing right the experience
3152640	3154640	Strictly departs from
3155440	3157440	what we know is being presented and
3157920	3163480	This was a very important test case for predictive processing and first in like folks because they gave a very elegant
3164360	3165600	Explanation of what's going on?
3165600	3173440	So they basically gave a Bayesian interpretation of predictive of vernacular rivalry and said that roughly what happens is that we have very low
3173480	3176520	Priors for seeing a house face in the natural world
3176520	3180160	And so what the mind does is it says well it can't be a house face
3180240	3182560	So it must be a ha it must be a house
3183000	3186200	But then the prediction error comes back up and says normal
3186200	3189080	You're only accounting for half of what you're experiencing here
3189080	3193560	So it must be the other stimulus and it switches and so it's saying okay now it's a face
3194560	3196760	Again a strong prediction error and an ultrace
3197760	3200960	And what you have here is you know perceptual dominance
3200960	3206380	So you might see the face first or you might see the face for longer or it might alternate more
3206720	3210440	Okay, the mind is a thinking machine. It's epistemic great
3210560	3215760	We have a very you know epistemically normatively informed explanation of what's going on there
3216240	3221400	The reason I like to use vernacular rivalry as a case for the evaluative mind
3221400	3225320	Is that when you subscribe to something like the reward hypothesis as I did?
3226080	3232280	Let's say in 2017 when I was presented with this is that I thought you know, I bet you reward will modulate
3232880	3239560	This phenomenon so it allows you to make predictions even from the armchair and I bet you reward will
3240720	3245040	Modulate the experience vernacular rivalry and that is in fact what you find so if you
3245560	3252320	Reward a certain stimulus like the face so you say hey, I'm gonna give you a penny every time the face appears
3252800	3254800	We have perceptual dominance
3255320	3261560	Of the face but it gets better than that you can also reward just the percept so people don't cheat basically and
3261840	3265120	Every time they report seeing a face you can do like a pitching
3265960	3271240	And again the face will be more dominant. You'll have perceptual dominance
3272840	3274840	First in folks when you email them
3276040	3280920	They're like yeah, but it could just be information right the reward is an added piece of information
3280920	3285520	So you can put that into kind of your Bayesian function and that can still explain
3286080	3291440	Why this perceptual dominance is occurring, but what you can do is you can have a
3291880	3295760	Punished percept so every time they see a house you can make a little sound that says
3296680	3299000	You know like you're sad
3299000	3304160	And if it's just information you should have perceptual dominance of the house
3304680	3308080	That's not what you find you actually still have perceptual dominance
3308400	3315320	Of the non-punished percept of the face so really what this tells you and this might glamorize the finding a bit
3316840	3318520	You want me to like
3318520	3322000	Animate less. Yeah, no, I know I know I know it's okay
3322880	3324280	I'll try
3324280	3326280	Yes, sorry
3326400	3328400	I will hold on to the podium
3329440	3337800	This is a small finding but it tells you basically that to some very limited non-exciting degree you see you perceive
3339080	3346060	What it is in your interest with respect to your goals to perceive right again, this is a very thin case is very narrow case
3346760	3348760	But it tells you that you don't just
3349560	3352680	Perceive the world we perceive the world conditional on our goals
3352680	3355880	Which in this case is to make a little bit of funny in these studies, right?
3356360	3360080	Binocular rivalry is a tiny case, but if you look at all the research
3360760	3364080	That kind of riches alluded to over the course of the last several decades
3364520	3369440	We find that every stage of mental processing from sensation
3370680	3377680	And computation to action at every level of description and mental processing from the sub personal to the personal
3379040	3382720	It's conditional on these attributions of reward and value
3382720	3386960	So we sense we perceive and we attend to the features of our environment
3387200	3392900	conditional on reward and value we remember and remember to remember remember
3393080	3395080	prospectively conditional on
3395420	3397420	the attribution of
3397580	3402260	Reward and value our cognitive control our ability to decide choose
3402660	3408900	Plan our future actions in each case you will find a body of evidence that's just it's just like in binocular rivalry
3409340	3412620	It is conditional on the attribution of reward so
3414260	3418900	We have this kind of classic sorry picture of the mind as a thinking machine
3419340	3423180	I think that's part of the story, but the mind is also
3423860	3429460	continually in the business of evaluating all of these parts of the so-called kind of cognitive sandwich
3430140	3435740	So I want to make some kind of caveats here. First of all, it's an additive thesis
3436460	3438460	What I want to say is that the mind
3438900	3446500	computes over these representations of matter-of-fact and it continually evaluates them now for some people that might seem kind of weak sauce
3446500	3450340	Particularly, I think philosophers sometimes want to come out swinging and say, you know, this is everything
3450340	3455580	I call that the stronger thesis the stronger thesis something like what the mind is fundamentally in the business of doing is
3455740	3461660	Evaluating things is better or worse, right? That's that's mainly what the mind is about. I don't want to go that far
3461660	3467460	We can talk about that in the discussion of why not I want to say that the mind is fundamentally in the business of doing both
3468140	3471700	So I'm making an additive claim. I don't subscribe to the stronger thesis
3471700	3475740	Although I do think it's kind of fun and it it does kind of really drive a lot of nice research
3476740	3480500	And I also don't think this is exactly the same thing as saying reward is enough
3481620	3485580	My claim is about the nature of the mind and what the mind is in the business of doing
3485580	3488580	I take reward is enough on one interpretation to be saying
3489140	3495380	What's needed in order to produce intelligent behavior? Those are obviously cousins as positions
3496140	3498140	But I think reward is enough again
3498460	3504460	Make some stronger claims than than than what I would subscribe to but that's definitely something that we can talk to you
3504540	3510420	But I think we need to start thinking of the mind as evaluative in these ways not just because that's what we're doing
3510620	3516460	But also if we want to design artificial intelligence, that is a very different picture of what we're trying to build
3517340	3521740	And we'll also go towards addressing some of the kinds of challenges that we heard about yesterday
3521900	3526860	So the first question was does the reward hypothesis provide a good model for understanding the mind?
3526900	3529700	Yeah, I really think it does and the answer to that is yes
3529700	3534580	And I think it changes our understanding of what the mind is kind of in the business of doing
3535780	3542260	The second question is you know, how far does the reward hypothesis go as you might imagine I'm gonna say something like pretty far
3542660	3545980	So I gave you a list of kind of all the the kinds of
3546620	3553700	Processes that reward and value are are implicated in and you can kind of see that a lot of these are sort of you know
3553700	3555700	So called low-level cognitive capacities
3556220	3562780	But all the way kind of extending into sort of high level cognitive capacities that people might be be interested in and so
3562780	3567660	I don't want to go into that too much, but I want to focus on what you might think as one of the kind of
3569100	3573820	Crown jewels of the human mind, right, which is our ability to have moral experiences
3574340	3578700	If there's anything that we might want to think is special, but how the mind works
3579260	3581020	One would be language
3581020	3583020	I can talk about that after yesterday
3583820	3588460	But another one might be our ability to have moral experiences, so when I say moral cognition
3588460	3590500	I mean something quite straight forward
3590500	3596900	It's the capacity to create and respond to situations of moral significance. So let me give you an example. It's nothing fancy
3597780	3599780	You know my sister Barbara
3600140	3605140	Goes to buy a coffee at a coffee shop and she can make the following decision
3605140	3609500	She can buy the more expensive fair-trade coffee or she can buy the cheaper
3610260	3614420	Commercially sourced coffee, right and she can stand in line. She can make that decision and she
3615020	3618660	Realistically probably past the cheaper commercially sourced coffee. Sorry Barbara
3619540	3624420	But that is an example of moral cognition. She's not making a decision about a trolley problem or anything like that
3624420	3631420	These are the everyday routine moral experiences that we have that we make like this and that's the kind of thing that I have in mind
3631420	3633420	When I'm talking about moral cognition
3633820	3636700	So again, they're kind of quite
3639700	3641060	well
3641060	3645660	established received views on kind of how our moral cognition
3647300	3649980	Works what the mechanisms behind that are and so
3650660	3656100	Again, this this isn't actually even too much of a cartoon. I think we have kind of these rationalist inferentialist
3656780	3659140	Views where the idea is that Barbara
3659460	3666180	Does or doesn't buy the fair-trade coffee based on the belief or set of beliefs that something is the right thing to do
3666180	3669420	And then it will kind of follow from you know her knowledge
3669540	3671540	about a particular situation
3671660	3673660	On the other end of that dichotomy
3673660	3676540	There's this idea that Barbara might do or not do the right thing
3677100	3678980	Because of her moral emotions
3678980	3682260	She might feel empathy towards the workers or not
3682420	3687140	So an agent has the fitting moral emotion or emotions that something is the right thing to do
3687380	3692020	And then despite the kind of distribution of the literature actually I think a lot of mainstream views
3692340	3698220	Are hybrid views some combination of the views that you know certain things that you have certain kinds of beliefs
3698220	3702580	You have the relevant beliefs. You also have the accompanying emotions that might motivate that behavior
3703260	3709140	And we should definitely talk about that Plato slide, but without going all the way back to you Plato
3709140	3711980	I think these have been kind of the main
3712900	3717140	Alternatives in understanding moral psychology and understanding our moral cognition. I
3718300	3723060	Think the reward hypothesis suggests that there's a kind of a new player on the scene
3723740	3730380	And that moral cognition is in fact constituted by the sub personal attribution of goal and context
3730580	3735580	Dependent subjective reward value. So again that familiar phrase that we see
3736420	3745740	What we need to see in binocular rivalry is actually one important ingredient that mechanism is also driving our experiences of saying something like
3745900	3751760	That's just wrong. That's morally disgusting. I really ought to be doing this thing
3752040	3758600	Again in the descriptive cases, but what I am suggesting is that we actually recruit the same reward mechanisms
3759000	3765980	In our moral cognitive experiences now I'll flush that out again because that's a bit strong and a bit strong stuff
3766120	3768440	But what I want to suggest is that
3769320	3773080	There is quite a lot of flattening going on here, right? So in fact
3773960	3775960	When you are experiencing
3776280	3784160	Something as right or wrong you are attributing reward and value or laying that fabric in just the same way in a moral
3784280	3790600	Context with moral determinants like something like fairness something like honesty as you are when you are just choosing between
3791240	3793240	You know left and right to your coffee
3793640	3798920	So that's to say that if I'm choosing between to your coffee or coffee and fair-trade coffee
3798920	3802960	I'm actually recruiting some of the same mechanisms now again
3802960	3805560	There will be caveats that come to kind of flesh out this picture
3805800	3812880	But this is an important flattening of our understanding of what's going on in our moral psychology in our moral cognitive experiences
3813920	3818200	And you might think okay, but I really feel strongly about certain
3818720	3821600	You know moral propositions, and I don't feel particularly strongly
3822400	3825360	about to your coffee or something like that and
3825560	3832840	Sort of the second thesis of this view is that reward and value the strength of the reward and value are actually the source of our
3832960	3837440	Moral motivational force so through our evolutionary history and through our upbringing
3838320	3840000	certain things are
3840000	3841240	reinforced
3841240	3845920	To be extremely important to be absolute no-knows or absolute musts
3846360	3852680	And that these are the sources of why we sometimes feel that something is right or wrong and why we experience things
3853640	3857520	Something we really ought to do again. This is not a normative on but it's the feeling of I oh
3857560	3860400	I really should call my grandmother or something like that
3860400	3866280	It's because the driving mechanism here is again reward and the value not my emotions not my
3866760	3869760	Knowledge though again, I'll build a kind of more complete puzzle there
3869760	3876560	And so this is kind of one of the things that fundamentally differentiates between something like our social and our moral values
3876560	3879080	Right, we know we shouldn't wear white after Labor Day
3879600	3881800	Or we did know that once when I was like five
3881840	3887600	That has some force, but it has much different force from you know
3887600	3893200	It's wrong to lie and that is I think a function of the strength of the value that has been
3893720	3897240	Attributed to that over the course of our lifetime experiences our
3897680	3900600	Education's our philosophical conversations and things like that
3900600	3904120	So it's a source of the strength of the moral motivational force
3905360	3909480	Now this is pretty quick and again, I'm gonna flush out the picture in a second, but I think
3910160	3916400	This kind of you starts to account for features of our moral psychology that the kind of rationalist
3916560	3923120	Inferentialist views and the sentimentalist views don't account for so it doesn't it it accounts for something like the multidimensional nature
3923680	3928760	It's hard to explain the Barbara fair-trade coffee case with just the emotions, right?
3928760	3933880	There's lots of features that need to be weighed off there that a kind of complex
3934520	3940920	Waiting of our values allows us to explain explains why sometimes she buys fair-trade coffee and sometimes she doesn't
3941200	3945280	Why we have cross cultural values, so we have themes in our values
3945280	3951980	So all communities, you know value honesty, but what it means to be honest or what it means to murder or what counts as
3952480	3956240	incest is going to vary in us because we share some of these
3957040	3963200	Mechanisms, but how we tune them up is going to really depend on the community and the culture same with
3963440	3965440	moral learning
3965440	3967440	It's going to
3967800	3975080	Allow us to understand kind of what counts as morally exceptional, right where people stand up against all kinds of
3975680	3977880	opposition and costs and
3978400	3983480	But we're going to kind of place a very high value on the certain principle and stand by that principle
3984000	3986240	Through that strong moral motivational force
3987080	3989580	and I think kind of one of the
3989900	3994900	The more complex but also more compelling kind of pieces of evidence for a view like this is
3995300	3999420	What happens when we have failures in our moral cognition make errors?
3999420	4003420	We have dysfunctions and sometimes we have pathologies in our moral cognition
4003420	4006860	And one of the interesting things is that these seem to rise and fall
4007300	4014860	With dysfunctions in our reward systems in our reward mechanisms in ways that we wouldn't necessarily expect to see otherwise, so I
4016460	4018460	think
4019580	4023580	I do want to specify that I think it's a part of the puzzle again
4023580	4027660	It's going to be an inclusive view in the same way that the evaluative mind is an inclusive view
4027660	4032020	That's not to say that we don't have any place for the emotions
4032660	4038140	In our moral cognitive experiences. It doesn't mean that we don't reason and read philosophy and things like this
4038420	4042340	But when you're standing in the coffee shop and you're trying to decide whether you're making
4043220	4045460	Whether you're going to buy the fair-trade coffee or not
4046460	4051620	What you're recruiting there are your valuation systems and your life-long experiences
4051900	4056540	Of what you value and what we attribute our values over now
4056540	4063060	I think one key open question here is what do we attribute those rewards and values over is it to states?
4063060	4066500	Is it to state action pairs? Is it to abstract ideas?
4066500	4073140	Is it possible that philosophers for example have learned to attribute a high amount of value just to the concept of
4073420	4078920	Justice and the existence of justice in the world, right? I think these are important questions of how exactly it plays out
4080500	4084020	But I think and here I will take a dig actually at that Plato slide
4085020	4087500	It's not about pleasure and pain actually
4087500	4091860	I think pleasure so I follow Kent barrage here and distinguishing between pleasure and pain and
4092260	4100220	Reward and punishment or reward and and disappointment and I think our moral cognitive theories have been based on what we can introspect
4100220	4107100	Oh, I think this oh, I feel this and we don't necessarily have introspective access to our reward mechanisms
4107340	4109460	And so I think actually in a way
4109460	4115620	It's a disservice to try and tie it back to hedonism because it's rewarded something different here that hasn't been on the scene
4116020	4122140	But I think is a driving factor in this much more complex architecture of our of our moral cognition
4123020	4125500	So how far does it go? It goes pretty far
4125500	4131700	It goes to certain things that I think we thought, you know, we're a little bit untouchable in our human experience
4132260	4137420	And again, I'm just touch on this and we can talk about it in the discussion, but I think it really informs
4138980	4140820	How we understand
4140820	4146220	Moral artificial intelligence because this is actually quite tractable. This is actually quite quantifiable
4146620	4151700	We can actually go much further in understanding what works and what doesn't work in our moral cognition
4152420	4154420	Because we have this nice
4154860	4161380	Quantifiable computational theory and we can go much farther and understanding our moral cognitive experiences
4161740	4167660	And in designing those right so morality on a sore architecture is gonna suck
4168500	4172020	But it doesn't need to suck because that's actually not how it works in us either
4172460	4179220	And so I think having a handle on this opens some pretty interesting avenues even in the realm of artificial intelligence
4180180	4182700	So lastly, and this will be the fastest part
4183020	4188940	But the third question should it should this understanding then guide our normative decision-making and here
4189340	4194660	It'll definitely depart have from from rich. I think my answer straightforward Lee
4195300	4198060	That got formatted out of existence. Sorry
4198780	4202140	Is no so I take guide to be pretty strong
4202780	4206660	Guide means I should then use this to govern my normative decision-making
4207180	4213900	And the analogy that I often draw here is that moral cognition should be understood in analogy to folk physics
4214620	4221300	So, you know the Wiley Coyote case where Wiley runs off the cliff and hangs out there for a second and then falls right or
4221300	4226100	Other cases might be Muller liar illusions or putting a straw in a cup and things like that
4227100	4235460	We have understandings of physics that are produced by our sensations and our perceptions that we know to be incorrect and
4236780	4240940	If we rely on them, we will not land on the moon and we will
4241300	4249340	You know not make other scientific achievements, and I think we have to understand moral cognition as the equivalent of folk physics
4250180	4255260	It's full of errors now does that mean that the reward hypothesis is not useful
4255460	4258660	No, because I actually think it's really important to understand
4259300	4265060	Where our moral cognition reliably and in fact systemically falls down, right?
4265900	4269980	So this is have basically reduced to a bumper sticker. I used to walk past
4270580	4276500	Basically every day when I was doing my PhD, which says don't believe everything you think, right? It's you've all seen it
4276500	4284620	It's right, but actually that's basically exactly what this is is we have very strong moral experiences hatred judgment
4285300	4287300	empathy
4288020	4294980	But we shouldn't believe all of those right but what the reward hypothesis and what reinforcement learning along with many other
4295140	4301420	Contributions allow us to do is understand the mechanisms that generate those things that we experience is right or wrong
4301420	4305420	And I do think that's powerful. I wouldn't use it to guide normative decision-making
4305460	4307900	I wouldn't say okay. Well that I guess that's what we're doing
4308180	4311540	But I think we can use it to inform it in the following way
4311540	4315900	So one of the things that I have suggested is that we should have a kind of fault line approach
4315900	4319980	So when I say fault line, I mean something like tectonic plate movement, right?
4319980	4321900	If you understand the movement of tectonic plates
4321900	4327900	You can make nice predictions about where earthquakes and volcanoes are going to occur where things are gonna kind of continually erupt
4327900	4332540	And I think if you can understand the contours of our moral cognitive decision-making
4333460	4336260	You can also start to see sort of the fault lines
4336260	4339900	We're not just gonna make token errors in our moral cognitive judgments
4339900	4343180	We're gonna make type errors in our moral cognitive judgments
4343180	4346380	and I think we can use that to inform then
4347180	4350340	The kinds of things because I think in in sort of everyday life
4350420	4356180	We really do take our moral cognitive experiences at face value with one another and I think that's a mistake
4356260	4361500	Whereas if we have a kind of causal understanding of what generally sees and where they might depart
4362100	4365100	Then that can be used as a kind of type remediation
4365580	4369860	Of what's going on not necessarily in a policy sense or anything like that
4369860	4374820	But I certainly think you know, it's useful knowledge to have it can inform us for the better
4374820	4378780	I think it's good to know kind of the contours of the mistakes that you're making
4379380	4383660	But I wouldn't say guide because of the way these things are generated
4383660	4386540	So I think that might be a place where we depart
4388500	4390500	So yeah, I think it's pretty productive
4392020	4402180	Thanks, Julia and and rich that's just terrific. I'm so glad I got you both on to this conversation
4403220	4406980	And and I I have a couple of questions to get us started, but also
4407580	4411420	Ready to sort of follow with different directions. You'd like to head
4411420	4416340	I heard two things perhaps that it might be worth exploring one was pleasure and pain
4417340	4420380	And rich I don't know if you want to respond to that one and the second one
4420380	4426100	Oh, which I'll come back to is is the guiding normative decision-making, but let's take on pleasure and pain
4426100	4433500	Is that an important part of how you're thinking about this to me the pleasure and pain are just examples of what reward might be
4433500	4439380	Well, I think it was the play-doh slide. Yeah. Yeah. Yeah. Yeah. Yeah, he was clearly talking about pleasure and pain
4440820	4444060	Reward could be pleasure payment. It's undoubtedly more complicated than that
4447340	4449340	And
4450180	4456100	I agree with you that we don't know what our word signal is we we don't have
4457340	4458260	introspection
4458260	4460260	introspective access to it
4462260	4464260	And and more than that
4465780	4469220	What we do have an introspective access to it is the
4469380	4471380	I
4472980	4477340	Want to call it TD error remember it's the thing I was talking about is reinforcement
4477980	4484140	It's which is which is this measure of how do we feel about how well things are going this as you're saying the
4484740	4488700	Constant sense of our things getting better or worse. We evaluating all the time
4489820	4494220	So we have access to our evaluations and we we have very
4495220	4500900	Preeminent access or prominent access to our things getting better or worse
4502060	4504380	But things getting better and worse
4505100	4509300	if you remember it's the reward plus the change in your evaluation function and
4509820	4515700	So these two things the reward and the change in your evaluation these two are mixed together
4515700	4521740	And so when you feel good or feel bad, you don't know if it's you don't really know if it's because the reward was high
4521740	4523740	Or the change in evaluation was high
4524460	4525740	and so
4525740	4529780	So that is a confusion that we all have we don't know
4531180	4537220	Once so once an evaluation a value function becomes very well established then, you know, it feels
4537820	4545980	Bad when our values are violated just as if we had you know a form of pain or a bad thing directly. So
4547860	4551100	This is this is a major phenomenon, you know in terms of our
4551620	4554220	subjective experience that it's hard to tell
4555540	4557540	Difficulties in a reward and a change of value
4563860	4565860	Doing anything
4567820	4569820	Yeah, I would say that and again this is
4570340	4576860	Following Kent Barrage. I think pleasure and pain are sick, you know are evolved biological signals of
4577740	4579740	reward and value and
4580260	4585060	As signals, they're pretty good, which means they go together a lot of the time
4585060	4590500	I think one of the cases that the barrage talks about is the case of for example a digger
4590500	4595260	This might be David Reddish talking about cases of addiction to heroin which where the signals
4595940	4598700	and the reward and disappointment come apart so
4600820	4604460	You very clearly start to have not pleasurable experiences
4605380	4608780	When you will become a heroin addict, but the reward is so strong
4609740	4612500	That you continue to do it right heroin is basically ruining your life
4613020	4617980	But because it's hijacked the reward mechanisms and you continue. So these are kind of edge cases
4618900	4621740	but I mean, I think one of the powers of
4622580	4628500	Like the reward hypothesis, but also the scientific process and that discovery that you're talking about with Peter Diane and others
4628500	4631000	Is that it really gets at an important part?
4631620	4636180	Of of our kind of mechanistic composition that we don't have direct
4637060	4638340	introspective access to you
4638340	4644940	But where there are departures between what a hedonistic or pleasure and pain-based view would predict about us?
4645260	4652020	And what the reward view would predict about us and and clearly the reward view makes better predictions is better supported by the evidence
4652540	4653860	and so
4653860	4655420	It's kind of funny saying this to you
4655420	4660860	But I would say like there's no reason to lean into the pleasure and pain because like reward actually provides a
4661180	4665420	Better model in some cases, even though they you know, they often walk together
4669340	4672980	So you're thinking it's important to keep the distinction
4681300	4683300	I mean the reward
4683860	4685860	way of talking is more abstract and
4686540	4688540	And
4688860	4691620	If it was equated with pain
4692940	4696180	Pain and pleasure then it becomes more specific and
4697580	4701100	And that they're there for falsifiable
4703220	4705220	Which is good
4706420	4712580	Yeah, so I'm totally on board of course with it's a general thing it doesn't have to be pain and pleasure and there may be cases I
4713220	4714660	mean, I think I
4714660	4719060	Guess that totally makes sense to me like there may be pain and then maybe you could
4723460	4730880	Change so that the pain is less important to you or the pleasure is less important. Yeah, it's just more general
4732140	4733580	The reward is more general
4733580	4738860	I think another example might be the emotions where again we have introspective access to our emotions
4739180	4744180	And in that very hastily shown chart, right? I think there's incredibly close coupling
4744660	4750860	Between our affective responses fear anger and so on and the valuation component
4751380	4758020	But you might ask yourself, you know, how do you know I lost that between the the emotions and and the
4758940	4764420	Valuations, yeah, and so if you are you have a component theory of affect or emotion
4764420	4767300	You say that there's an appraisal component of the emotion, right?
4767620	4773260	What triggers certain emotions? Well on my view would be something like the valuation system the reward system
4774020	4780060	But I think you lose a lot if you only look at the introspective component. You say it's all emotion
4780860	4782380	Emotion is important
4782380	4789900	But actually the emotions are kicked off by the valuation or mechanism by the valuation assessment that then triggers these
4790660	4797020	ballistic responses like empathy or fear or anger or so on so I think again, they're they're very close
4797020	4800020	I don't know if bad fellows is the right word like these things really run together
4800340	4807500	But I do think that you come up with cases that reward accounts for and the emotions do not and that there are cases where
4807940	4812860	reward explains the phenomenon and pleasure and pain do not and so I
4814780	4820400	Do you think it's you know, they're obviously very closely related but something is lost by blurring between those two
4823420	4825100	Maybe I
4825100	4827300	Of course economists of which I am one
4828300	4834700	Have thought of and debated about these questions a lot because we represent a utility function
4834700	4840300	Most of the modeling is done with the idea that you can take all decision-making and put it into a
4841060	4848140	Continuous function, but I want to press a little bit because I hear in Julia's framework
4849620	4855700	Again with this concept this broader concept of valuation to say like your brain may actually have many sources
4855980	4860100	many valuations schemes that aren't necessarily
4861380	4863580	Representable as a continuous
4864140	4870820	Scaler reward and so I wouldn't want to push on the scalar part because I think that's a difference between
4871500	4873500	What you've presented
4873700	4875100	and
4875100	4878100	That's the economics agrees with the scalar
4878940	4882780	Yeah, you do. Yeah, I do. All right. I'll be the disagreeer. Yeah
4883220	4885420	So there's a nice paper by
4890260	4891860	I'll come up with the name in a second
4891860	4895700	Which just basically suggests that you can have these vectors that reduced to the scalar
4896020	4898820	and so that there are kind of mathematical ways of
4899540	4906340	Taking that kind of consideration and still having it reduced to scalar and you can say like whoof the theory holds that it's scalar
4906860	4911020	And so I think I'm probably less invested in this part of of it
4911020	4917500	But that would be the way that I would appeal to it is that you can accommodate that kind of intuition and still preserve the scalar feature of
4917500	4920140	The theory, but you might want to defend it like more
4921660	4928940	The original work arguing that you can reduce the vector to the scalar is in second is in economics
4929940	4931940	I
4933620	4935620	Morgan Stern and
4935620	4943620	Somebody rather you know, well, yes, it's hope and see there's a debate in economics about and and but by far the dominant
4943820	4950540	Neoclassical economics is that you can you can do that. Yeah, but there's also a different project in economics, which is
4951260	4955140	We're trying to build models that will predict the way
4956260	4958140	humans and
4958140	4965260	Entities composed of humans like markets will behave and so then when you say it's enough
4965260	4968580	You're saying oh, it's good enough to be predictive
4968580	4974060	And so one of the things I think about is how much are we taking that and now saying well, this is the way
4975580	4977580	Valuation works in
4977820	4984440	Human societies in the human and in human societies and I guess I want to put a wedge in there to say
4984760	4993400	You know, it could be that I could predict pretty well even if values are incommensurate and can't be reduced to a trade-off
4994440	5000160	Nonetheless at the end of the day. I'm just predicting are you gonna choose this or this so I could have a representation
5000600	5003000	That worked for that predictive exercise
5004200	5005960	so
5005960	5008320	All bets are off when you have groups
5009080	5013760	you know the the the basic theory is the theory of an individual and and
5015080	5018960	Yet and yet we want to move to say something interesting about
5020000	5023360	About groups or to ask if it's possible to say anything
5024440	5025880	normative
5025880	5027880	or
5029320	5033920	Yeah, can we say something normative normative, so what does this mean this mean I
5036920	5040520	See I might any definition I'll give will be reductive again, so
5045400	5052440	So how can we walk up to this so one way to walk up to it is the way Julia did which saying well, maybe these
5054000	5056000	The moral decisions
5056920	5062360	Are are not that different from the ordinary decision-making we do it every day
5063160	5065000	And to that to me that makes sense
5065000	5071720	And so I mean I want to push and say that for the individual we can think of it all as one system
5072120	5076640	Now to make it a little bit more acceptable to you that it might be that way
5076760	5081400	You want sure you remember that the the value judgments as well as the policy
5081400	5090560	But let's the value judgments are compiled. They are they are they may they may be due to learning or reasoning
5090840	5095120	but in the end they are they are they are like
5096000	5097480	they're
5097520	5103400	Hard-coded and they're automatic and they're intuitive. They're not the reasons for them are no longer present
5103480	5109040	that's so I see the reasons you might have a very firm feeling that that
5112480	5115400	Children should have a loving parent and
5117160	5118920	You might not have
5118920	5125440	Any more have the reasons for that belief you just believe it really strongly and and maybe maybe there was a reason for it
5125440	5131520	Maybe you had experience or you've seen people or you've done some reasoning process and you came up with a reason
5131520	5137880	But but now you just know you have this belief and so this is why we tend to think of values as core things
5138720	5140720	And and more emotional
5141560	5145480	Because we we don't have the reasons now it doesn't mean we lost the reasons
5145480	5149440	You know, maybe the reasons who are built built into us by evolution to me
5149440	5157480	It doesn't really matter. You have ended up with a judgment about the the the goodness or badness of certain things and and
5162080	5167840	Nevertheless a good way to think about them maybe as a prediction of the likely rewards
5168520	5170680	For being in that state or acting in that way
5171960	5174440	And to me that that's a really exciting
5174440	5181400	Apothos that you might be able to think about of all the judgments that people may including their moral ones in terms of a single framework
5183720	5189960	So I want to separate that is that impressive and I feel you from your talk that you were like
5190320	5192600	trying to lead lead us that way and
5193600	5199440	So I want to separate that there are two big things to talk about is that makes sense. Okay, and then
5200440	5202200	If that makes sense
5202200	5210200	What about when you have multiple agents and and then and we asked the question of universality and normative judgments
5210640	5212640	So are we at the?
5213840	5218920	Yeah, are we ready to have we have we done accepted the first thing and are we ready to do the second thing?
5219480	5224240	Are we still unsure and I'm I'm gonna ask you, you know, everyone, you know
5225160	5227160	Anyway, these these these two steps
5227880	5229560	Are we ready?
5229560	5234640	So can I just jump in on that real quick because I think there's a step between is that okay? Yeah, no, that's great
5234640	5239080	And then I'll go to question. Um, so I like this idea of reasons are no longer present
5239080	5244120	So on the slide and have time to talk about it, but I had this case of moral dumbfounding. So moral dumbfounding is when
5245000	5247000	You give people scenarios
5247200	5249000	like, you know
5249000	5254040	You know Jane and and John are brother and sister and and they would like to have sex
5254520	5260640	Just the one time with protection. So, you know, there's no possibility of any of the genetic consequences
5261360	5263080	of, you know
5263080	5267720	incest and so on and so forth and you ask people like, you know, is this morally acceptable and you know
5269120	5270720	and
5270720	5275800	You ask them and they'll give you kinds of reasons, but they they can't really so it's this dumbfounding case where you're like
5275800	5279640	Yeah, okay, there should be everything fine with this. The other cases are like having sex with the chicken
5279640	5283200	Anyway, there's some stuff going on in the lab there that came up these examples
5283200	5288960	But there's strong cases we have these the responses and I really like this way of putting the reasons are no longer present
5289600	5291600	Right, the reasons are good reasons
5292400	5299360	But they're no longer present in this case because if if John and Jane or whatever I said use contraception then, you know
5299360	5305640	This this evolved reason for why we're opposed morally opposed to incest is no longer there
5305840	5307840	But that's actually a really important
5308400	5312320	Junction, right? That suggests that those intuitions
5313200	5316880	Maybe over the course of evolution are a good
5317760	5322480	signal of reward or not, but that there are departures from that
5323200	5325200	And that's where the normative
5326320	5334720	Story comes in so we have these descriptive experiences like no, but the reasoning and the moral philosophy or the ethics or
5335280	5340680	Policymaking or all these normative disciplines law economics come in because we tell ourselves, okay
5340680	5347560	But shunning the outgroup may no longer be doing us a service because we live in a global cosmopolitan society where
5347840	5349760	We all want to get along
5349760	5352920	And who's getting along there is all of a sudden a much bigger group
5353640	5359480	And so I think you can have a part of that first story which is radical in and of itself
5359600	5362920	But not necessarily follow it all the way to step two
5363720	5365960	Because sometimes we believe things that are just
5367480	5369480	Silly like you shouldn't have sex with a chicken
5375080	5381160	So I I'm just I'm gonna I'm gonna just throw out there and then I'll come to the audience
5381160	5385360	and I may want to put Joel and will on the spot on this one as well, but
5386120	5394320	You know the the the idea of thinking about the values or as or the as
5396400	5398400	Intuitions as
5398480	5402160	Some kind of thing that is just there
5402160	5409120	We don't know where it came from it accumulated from arts as opposed to which is what I really like about your framework Julia is
5410080	5416880	It's a very active process and actually so I make the claim in the work that I'm doing thinking about normative systems
5416880	5419760	And I have a very very reductive definition of normativity
5420240	5424120	It's what the group labels and we just have a labeling scheme
5424120	5432560	This group says this is okay, and this is not okay, and we've you know coordinated a cognitive structures and institutions
5432560	5435720	And so on to to produce that but it's adaptive it changes
5436120	5438400	We get angry about rule violations
5438400	5445120	But you can go to a different environment where the rule is different and you used to get mad when people came into the building with short
5445440	5447760	shorts on and now you don't
5448240	5451640	Right, and so I think it's it's it's very much a
5452560	5455720	Processing of the information from the group
5456360	5458360	So it's a group thing
5458400	5461480	It's a pick a particular group all groups don't have to agree
5461960	5465000	Even a single group may change its mind over time
5465000	5470040	Yes, and that that's actually a complex system to understand how that how that functions
5470760	5476320	And the silly rules about chickens for example or something that thought a fair bit about
5476800	5484800	But I want to make sure we get because I know that there's lots of provocations for our audience here to to contribute on so let me
5485080	5488080	Let me open it up here. I'm gonna go to Jennifer first
5488080	5490080	Yes
5490320	5492320	Yes, and do wait for the mic
5494040	5497640	So so this is sort of following up on Julia's claim that minds
5498920	5502640	Subpersonally a sign reward, and I don't disagree with that. I think absolutely we do that
5503040	5509440	I'm curious about whether we also personally do this and whether it's not an expression of our
5510040	5513640	Autonomy as rational agents capable of setting goals that we do this
5513640	5517560	This is also my reservation about the sort of McCarthy definition of intelligence as
5518560	5525840	You know being about the computational part of achieving goals surely we can apply our rational intelligence to setting
5526160	5533080	Goals, I mean we don't have that much flexibility with respect to the sort of homeostatic needs that are
5534040	5536160	Encoded in us as biological creatures
5536160	5541720	But but beyond that we're on a long leash from evolution and we can surely in
5542360	5546680	Rational conversations with each other work towards finding
5549040	5553540	Goals that stand the test of
5554800	5556800	You know
5556800	5558160	rational
5558160	5560160	scrutiny and
5560160	5564240	And and and so that kind of that kind of expression of our freedom
5565520	5569800	Intellectually seems to me to be a very very important application of human intelligence
5569800	5575480	We're not just slaves of our reward functions, you know just looking for ways to maximize
5575680	5578240	You know, whatever this damn thing is in us
5578240	5584800	We get to say what we're pursuing and that seems a way in which you know our intelligence is
5585480	5587480	Caught into play
5587600	5594200	Isn't there a famous philosopher who said who said that our reason is a slave to the passions
5594720	5596640	David Hume
5596640	5599560	Reason is and ought only to be a slave to the passions
5599960	5601960	Yeah, but that's you. Yeah
5602840	5608440	And he's obviously wrong about that so so I'm very much on this. I'm very much on the side of the manual count here that we
5609400	5613440	That that what is what is right is what is universalizable?
5614040	5621160	there are so so so you know the the the maximum underpinning your conduct should be something that
5621960	5626200	That is universalizable that could be could be a law for all
5627200	5632480	There's there's an important contribution of the inclinations in here, right?
5632480	5639320	So we have certain natural inclinations that we have to satisfy to continue our existence as the kinds of biological creatures
5639320	5644440	That we are and we may have to discover things about people's inclinations empirically through interactions with each other
5644720	5645880	but
5645880	5648400	But the question of what we're inclined to do is
5648880	5653080	Sabrable from the question of what we ought to do we can criticize our inclinations
5653080	5659360	I could find myself inclined to do stuff that that I think is wrong actually even just kind of at the level of biological fitness, right?
5659360	5661120	there's you know various
5661120	5666520	appetites that I have towards you know sweet and salty foods and so on and I can I can appreciate
5667120	5670800	rationally the reasons for which I have those inclinations and
5671320	5677520	Actually reject them as you know in the current environment not contributing to my fitness. I can overrule them
5678120	5680440	And that's not something that you know reckons can do
5681440	5684000	That's why we're more intelligent than they are
5688480	5692280	My sister literally had four raccoons in her garage this morning, so
5695440	5700320	Okay, so obviously like a big question, so let me kind of try and step through that
5701440	5706360	So we were talking about this a little bit yesterday, so I would say that I'm a motivational involuntary
5706960	5711560	So I'll give the example of coffee, but cigarettes might be a good example for other people
5712600	5716120	Over the course of my experience. I have acquired a very strong
5716920	5718920	liking or a cup of coffee and
5720200	5725120	That guides what I do, but again coffee is kind of a trivial case. Oh and Jennifer
5725120	5728880	I definitely put an asterisk there just for you because I was like
5731880	5733880	So in my defense
5734360	5738840	Now I can't just reach in and say I'm not going to need that cup of coffee tomorrow
5738840	5745400	I'm not going to need that cigarette and I I can't say either when you scale it up to some of my moral responses
5745920	5752040	Maybe less savory moral responses. I'm just not going to judge that way or I'm going to do that thing
5752040	5755360	So I don't think we can overrule them in the that that sense
5755760	5763180	And I think that's basically I'm an early modern nationalist pre-con that I think that they're they're thick causal mechanisms
5763780	5765780	Of course, we're not raccoons
5765980	5768940	And we do have reasons we can make
5769340	5772340	Outs as groups and and to the the group
5772860	5778860	You know what what is normative is for group? I think one way to kind of start picking at that is that some groups are just better
5779780	5783580	They have better principles and better norms that work better for the group than others
5783580	5785580	So that should be a kind of clue that
5785580	5790180	It's not you know that there there might be a kind of normative dimensions like okay
5790180	5794220	These institutions that in fact work better towards the goals of us getting along together
5794980	5799700	Close-eyed bar. So what do we do when we have this involuntarism?
5799700	5803300	but we also come equipped with the raccoon plus of
5804700	5810740	Knowing that there is a better way. I think we have relatively indirect paths
5812020	5817060	To let's say diminishing my reward for coffee. So what I can do is I can
5818060	5822500	You know drink a glass of salty water every day that I have a cup of coffee and
5823780	5825780	Indirectly ratchet down
5825780	5830780	The value that I place on coffee or the value that I place on the smell of a cup of coffee and so on and so forth
5831460	5835300	So it's kind of an intermediate view where it's like I think we can leverage our reason
5835300	5838180	But it's by no means as easy as some of us would want
5838580	5842860	To hope that it is and I think it's important to emphasize this end of the debate
5842860	5848300	I mean again, it's always a question of emphasis, but it's important to emphasize the difficulty because I think if we don't
5848460	5852260	We think we could just pull the lever and I think that leads to all kinds of
5853420	5855420	disasters basically
5856220	5857980	So I want to
5857980	5859660	React as well
5859660	5865060	I'm really glad you asked this question because I think it's really the heart of the whole thing
5865060	5871820	It's you know, is it demeaning to think that there we have there they have something that's a signal coming in
5873660	5875660	I'm gonna call it pleasure and pain because it
5876660	5884060	It goes with it with the demeaning aspect to it. We are a slave to our pleasure and pain and it makes us seem small
5885060	5890780	And like we don't have choices and these are the most important things in our lives
5890900	5895140	What are what we're trying to do with our lives and that we the idea we don't have
5896020	5898020	Control of them is really annoying
5903860	5909060	But I think it absolutely is true and it's true in kind of a definitional sense because
5909460	5915300	When you say you have a reason for something you mean I do this because I'm trying to do this and
5916540	5923980	Obviously throughout our lives. We have reasons like that. Why I'm going to work to earn money. I'm trying to earn money. So
5925620	5932020	Raise a family. I'm trying to raise a family because I enjoy certain activities and I want to see things happening
5932020	5935540	We all we have many many reasons and we're used to that
5935540	5941100	But we also should acknowledge that eventually it has to stop, you know, you have reasons for things
5941100	5947180	But eventually there's some final thing that doesn't have any reason for it or at least that's
5948260	5954260	Maybe the standard way to think about things in science. I don't know, you know
5954260	5957500	It's the old thing about the odds and the is is you can't derive
5958020	5962340	By learning about the world you cannot really derive what you should want
5963300	5965300	there's there is
5965540	5968740	There is one standard philosophical view. I can't do it justice
5968740	5974420	But but it's this the view that that you can't derive an ought from it is and that so you have to have some ultimate
5974700	5982260	Oughts that are given and those ultimate odds are the things that we can't choose. So there must be something that we can't choose
5983260	5988860	And that's what the reward is meant to be. It's the one thing that we can't choose comes from
5989700	5991060	outside
5991060	5994100	And it's an outside just means we can't choose it
5994740	5999780	And so since that has to be true, I think even though it feels
6001100	6002180	demeaning
6002180	6005620	That we should get used to it and stop feeling that it makes it demeaning
6005620	6012220	We have something that we ultimately want and and so we're ultimately and it's individual in some sense
6012300	6014300	selfish
6017420	6019420	And I think
6019460	6024620	That if you embrace that it's kind of liberating, you know different people are different. They want different things
6024820	6027060	It's okay for them to pursue different things
6028660	6030660	Doesn't make us bad people
6031340	6035620	I'm going to jump to another question because I want to make sure we get a few more people in here Sheila
6036020	6038020	I
6040340	6044500	Thank you so much to all three of you for for such an engaging
6045460	6047460	discussion, so I was I
6047940	6049940	Guess I wanted to start by just
6050060	6054820	Harking back to a conversation that that rich and I had yesterday. I think I and in defense of John McCarthy
6054820	6063620	I think that McCarthy in 1955 defined a notion of artificial intelligence and and that that intelligence is is a vague term
6063820	6067900	Just like our conversation yesterday about about baldness, you know
6067900	6072820	If I start pulling hairs out of my head at what point do I recognize that it that I'm bald
6072820	6076980	Whereas with when I put that hair back in I'm when you call me this way. Yeah
6078580	6083180	Anyways, I think there are lots of terms that are vague and I think that people define them purposefully
6083180	6085180	And I think that intelligence is one of those so
6085660	6089460	But what I but I wanted to I was really struck by by Julia's
6090100	6096060	Discussion of binocular rivalry because I feel like I'm having binocular rivalry, you know between the two of you
6096060	6101420	And and that we're all just trying to make sense of this this complicated idea
6101420	6105180	And and we and again to a point that Julia made you know
6105220	6112020	What we see the way that we interpret the way that we make sense of the world is indeed informed by by some sort of
6112380	6116980	Bias we have or it's sorry. That's not the word that you use but some sort of expectation of what we want to achieve
6117140	6119140	So so what I wanted to just to
6120580	6122740	To to make sense of myself is
6123340	6130900	Where in your conceptualizations of things you you absolutely disagree or where you actually where we're some of the
6131260	6135820	Disagreements we see and even to Gillian's question about the reward hypothesis is just about what we actually
6136620	6140860	Decide to highlight in our formalization or our understanding of things like so
6141380	6146620	Rich even when I was looking at your your decision-making box on the right, which is one that and one of your slides
6146620	6153380	Which is one that I'm very familiar with, you know, I thought where's memory? Where's long-term memory everything's been compiled
6153380	6155380	You know, we're from classical
6156500	6165460	Decision-making where's the notion of a you know of a of a that predictive mechanism is actually being not sort of compiled into the value
6165460	6168260	function or everything being compiled into
6169140	6176380	Scalar reward to Julia's response to to to Gillian and and so I guess I want to push back on on saying
6176380	6178900	What are the elements that are important? Where's long-term memory?
6179180	6186780	Where is where is this notion some of the the elements that actually contribute to a reward eventually being compiled into a scalar reward?
6186780	6189660	And and is that diagram for you on the right?
6190140	6193700	really representing everything that that it needs and then I guess
6193860	6199780	Point and and and again, I thought about things again that we think about in the context of multi-agent systems about about
6200620	6203740	Goal-seeking, you know the ability to create goals about
6204340	6211460	Commitment about intention all of those elements that we often use to make sense of the world that that are not part of your diagrams
6211460	6214700	Probably we can they're compiled in somewhere in there
6214700	6216860	But I think there's utility in including them
6216860	6223260	So what what do we have are there things in your formalisms that are just compiled away that are important and then?
6223260	6228860	I guess the other question was where do we fundamentally or where do you in your binocular rivalry?
6229380	6230620	fundamentally
6230620	6234220	Or my my sense of you're the binocular of rivalry that I'm having
6234740	6237780	fundamentally disagree and and and I was and maybe this is a
6239580	6241580	Conversation to take offline and for afterwards
6241580	6248220	But one of the things I was intrigued about is this you know what what I perceive is our perhaps our ability to be able to reflect on our
6248900	6254940	Art to have an awareness of our reward to be able to reflect on our reward as as as we do
6255140	6262860	Representation other representations and whether that's an element that that that that is of utility and explaining some of what we see is
6263100	6266260	Understands the mind. So that's it. Well, those are a lot of things
6266820	6268820	Julie is writing down her thoughts
6270180	6272620	I'm not writing so I have to go first because
6273540	6275540	I'll forget
6275900	6280860	There are many things I will I will forget some remind me if you get them wrong
6284700	6286900	First memory long-term memory
6288820	6294780	There were four boxes all four boxes learn and so maybe they're four boxes are for
6295420	6298780	Maybe they're for neural networks. You that'd be a fine way to think about it
6298780	6301960	And then they all have weights and those weights are all adjusted through
6302780	6308700	Learning and planning both so that they would all have long-term memory. Memory is throughout
6313460	6315460	So I want us
6315660	6318840	We can get a sense of freedom. We get a sense of intention
6319380	6323980	We can set a sense of choosing goals because I do think it's a really major
6324780	6332220	Psychological thing that we choose goals. We choose except they're not they're not they're not reward goals
6332220	6335900	So maybe they shouldn't be called goals. They are sub-goals. They are
6337180	6338620	strategies
6338620	6339980	that
6339980	6345900	That you choose as part of your solution of the problem the problem is fixed the problem is reward
6346380	6353620	The problem is is it's the reward is not a compilation of something. It is the it is the thing that is unchanged
6354380	6356380	It is not a compilation of something
6357180	6359380	other things are compiled like by
6360380	6367060	Learning what gets rewarding and what's not rewarding you decide what to do by learning what's rewarding. What's not rewarding you?
6367060	6372180	You compile you compile into the policy you compile into the value function
6372860	6377700	You even compile into the model and you compile into the perceptual process where you're figuring out
6377700	6383100	Oh, this is a good way to think about the state that I'm in or this is a bad way to think about the state that I'm in
6383380	6387100	all those are learned and in that sense compiled and
6388060	6394020	If you want to look at reasoning reasoning is in the model of the world we learned that you know this causes that and
6394700	6396100	that that that
6396100	6403220	Reasoning that that knowledge of the world is in multiple levels low levels low level physics and high level decisions like like oh
6403220	6405780	I'll go to the conference or I'll take this job
6408180	6415260	But there's no compiling in the reward signal or you know if there's any compiling it would be evolution compiled its goals into our
6415380	6417380	Reward, but for us
6417900	6421660	Reward is just a primitive thing and it's never never changed
6423340	6428940	And yet we absolutely it's appropriate to have the sense that we choose our sub-goals
6428940	6434540	And that's the most important thing we make I'm gonna I'm going to choose to study
6436540	6438540	Economics or
6439260	6441260	Or our philosophy
6441940	6448420	Or I'm going to choose to devote my life to understanding how the mind works. Okay, those are and I'm thinking
6448860	6451360	I'm not thinking about this, but
6452420	6459460	Maybe if I devote my life to figuring out how the mind works, I will enjoy myself better. I think that's what the way it goes
6460980	6462980	I will forget more reward
6463820	6471320	So I think yeah, there's also a multi-part and I feel like Joel basically asked you Sheila to ask this question
6473540	6477540	So in terms of the question of you know emphasis and binocular rivalry, you know
6477540	6481360	I think that's just the nature of scientific and philosophical theorizing, right?
6481380	6485180	It's a question of emphasis in a lot of cases
6485340	6488980	So depending on how we configure our audience
6488980	6495660	We would either be like aggressive opponents or allies depending on who we're engaging with and the point that you're trying to get across
6496060	6498180	There's just a sociological feature of that
6499020	6503060	But I think where things really start to come apart is what predictions
6503540	6508540	Do your commitments generate and what implications follow from your view?
6508900	6512220	And that's where you also start to see some departures between us
6512940	6517380	So just examples from the kind of conversation that we've had so far that I'm thinking about is okay
6517380	6520540	I think we actually really disagree on this pleasure and pain thing like we're like oh
6520900	6524900	No, we really disagree about that and it will generate different predictions
6524900	6531660	And it has very different implications for how we understand certain things about the nature of human beings and and how we kind of follow from that
6531660	6533660	I think we disagree on
6535900	6541140	This normative question of what then we should take away from these experiences of values that we have
6541460	6544860	It's funny because I wrote down liberating to but I mean it in a very different way
6545260	6549860	It's liberating in the sense that if you recognize your limitations, that's very liberating
6549860	6552700	So you accept the flaws in your system
6552700	6559660	That's actually very empowering to the way that I was mentioning with the coffee because then I can use I can leverage that to become better
6560500	6562660	and
6562660	6566060	again, so you'll see different predictions there about what works in terms of
6567100	6573740	Self-regulation and there will just be departures and in the kinds of commitments they have and then I would say in terms of
6574380	6581740	What we're striving for with artificial intelligence and how we should recognize them for example as people that would be another place where
6583060	6587940	You would actually see our commitments that look very similar in some places
6589140	6591140	separate because of
6591820	6594260	the implications that follow from that so I think
6595780	6601540	But it but it really is like there's partly a sociological phenomenon of who we're engaging with and who we're trying to persuade
6602420	6606420	Where in some cases we're a united front because I think the reward hypothesis
6607580	6612340	Is in a minority or can be in a minority despite how powerful it is
6612900	6617180	But then you know between one another we would see see big differences or something like that
6619820	6624340	Thank you, and this may be our last question we'll see yeah
6625620	6628620	This was a really fascinating talk I
6629340	6634780	Don't have a specific question in particular, but I do want to ask in the
6635580	6640140	Interests of internalizing reward and self-regulation. I think those things you brought up
6641420	6646140	If it's not beyond the scope of your research, I wanted to ask about reward hacking and
6647180	6652420	The idea of good-hearted law which is when the measure becomes a target it seems to be a good measure
6652940	6655620	I'm really interested in hearing if you have any thoughts on that
6659620	6661620	I don't know if I heard everything
6662500	6667700	But reward hacking definitely happens to people. Maybe that's what drug abuse is about
6683900	6686300	I'm not sure I understood everything you're asking I
6687300	6689300	Can repeat if
6691260	6697620	Yeah, so I was asking about reward hacking which is something you answered but also good-hearted law which is
6698780	6702060	When the measure becomes a target it ceases to be a good measure
6703140	6709260	If you're familiar with that law, I wanted to ask in the interest of self-regulation as something Julia mentioned
6709980	6711980	If there are thoughts on that
6717300	6722700	I guess good-hearted law doesn't apply because that's sort of a community thing
6723060	6727520	You know it gets the measure gets manipulated by actors
6728180	6730180	But if it's your own measure inside your head
6733020	6736900	It doesn't get get lost in its importance in that way
6740380	6742380	Self-regulation
6742380	6744380	I
6744780	6746780	Mean it's super important to see
6748140	6753420	Binocular rivalry the two different views. It's what that's the most important thing that's going on here that you know like oh
6753900	6760180	It's it's ugly to think that we're just doing one number of pain and pleasure or whatever it is rather than
6760740	6762740	The grand reviews we have ourselves
6764100	6766100	But they're both true
6766540	6771580	We have to self-regulate we have to decide what what is our defining goal
6772020	6777820	And it's it's we we have to decide but I've just said we don't have to decide because it's given and
6778300	6783700	It's in it and it's in us, but we don't know what it is and besides
6783940	6789220	It's it's there's many senses in which the values are more important than the rewards
6789220	6791220	I mean the rewards are the ultimate determination
6791860	6798420	But the values are what the consequences of that that come about in interaction with the world
6798660	6806620	In interaction with the world, I figure out that the thing I need to do in order to get the most reward I
6808580	6810580	Have to figure that out
6810580	6812820	The fact that the rewards are fixed doesn't help me
6813340	6818820	Doesn't inform me about what what I should do in the world to decide what to do in the world and what I should
6819380	6822060	Say to myself and say to others is my goal
6823500	6826940	That will be all about value and values come by
6828420	6833700	Through the complex interaction with the world and what is what is actually going to make me happier in the long run?
6833740	6837860	You know the rewards can't tell you that the words just tell you when you get there
6837860	6839700	I'll tell you if you're happy or not
6839700	6843860	They can't tell you what you should do and they can't tell you so you have to decide
6843860	6849900	You know which which career to have and and which person to be involved in and all those
6850260	6855580	Subtle hard decisions. It doesn't the fact that it's fixed doesn't help me make those decisions. You still have to do those
6855580	6857580	I
6858260	6862300	Think I probably also will and I'm looking at the clock will also only address part of your question
6862300	6868100	but um, I do think reward hacking occurs and people I think addiction is kind of the edge case of that
6868540	6871860	But I'm wondering whether you would also find there's a philosophical literature
6872220	6874220	Maybe just one philosophical author
6874500	6880420	On this idea of value capture and so I think one of the examples given in the phenomenon value capture is
6880740	6882740	people wear like a Fitbit
6882860	6889420	And the Fitbit tracks the number of steps that you take and obviously you need to take steps in order to maintain your health or like
6889420	6891740	whatever the man tries to tell you and
6892420	6894420	I'm just kidding. It's probably important and
6895540	6897540	But people become obsessed
6898220	6902620	With getting their steps. They're like they if they go for a walk and they don't have their Fitbit
6902620	6906220	It didn't count and that's an example of value capture. That's obviously like a very
6907060	6912220	Sort of relatively cute example of value capture, but I think this is something that happens like
6913500	6919620	Professionally, right? It happens like if we have certain goals like, you know, the pursuit of knowledge or contributing to something, you know
6919620	6925580	But we value capture in in publications or whatever, you know, lots of cases across not just an academic phenomenon
6925580	6927580	This is something that happens
6927980	6932060	Across different aspects of human life. I don't know if that's exactly what you're getting at but
6932860	6940300	In terms of self-regulation, I think understanding again the understanding is extremely powerful because if you recognize what's happening
6940860	6946380	You can regulate around that. So I think just based on our conversation a little bit yesterday based on your interest
6946380	6950820	I think I wonder I was wondering whether value capture might be something that you're also interested in looking at
6952420	6958300	Which is just a kind of nuanced version of how that happens in everyday life, I think
6959900	6965780	Thank you. Well, this is we are we are at two hours folks. So we're gonna we're gonna call him here
6965780	6967780	But we have a 30 minute a
6967940	6970020	30-minute coffee break now
6970140	6977780	So I'll invite you to to join us and I'm actually gonna ask that you let our speakers move to the coffee room before
6978300	6985020	Asking them questions so they don't get trapped here, which I've started to notice and do come back with us at 11 30
6985020	6992700	We have Abby Goldfarb Daniel Daniel Rock and Frank British on in machine learning in the workplace, which will be as as riveting as this one
6992700	6994700	Thanks so much also
6997780	7000780	You
