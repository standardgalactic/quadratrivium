1
00:00:00,000 --> 00:00:05,880
Welcome back everybody. We're really glad to see you all here today and to those people online as well

2
00:00:06,760 --> 00:00:08,760
I'm still Jillian Hadfield

3
00:00:09,560 --> 00:00:14,600
And still director and chair nobody's taken me out yet of the Schwartz-Riesman Institute

4
00:00:15,840 --> 00:00:18,560
Looking really forward to these sessions today

5
00:00:19,680 --> 00:00:21,680
new questions and challenges and

6
00:00:23,240 --> 00:00:26,480
Lots of lots of wonderful lots of wonderful sessions

7
00:00:26,720 --> 00:00:31,680
Let's see is there anything else you want to make sure I say before we get started

8
00:00:34,160 --> 00:00:40,120
No, okay, I think we're gonna get we're just gonna move into our first session which is which I think

9
00:00:41,000 --> 00:00:48,960
Yeah, I'll stay up here to do the intro. So this first session is on the reward hypothesis which we got some mention of

10
00:00:49,640 --> 00:00:51,640
yesterday and

11
00:00:51,640 --> 00:00:53,640
And

12
00:00:53,960 --> 00:00:58,600
Will we're interested in this I think rich will give us who is the

13
00:00:59,360 --> 00:01:01,840
the source of the word hypothesis

14
00:01:02,520 --> 00:01:04,520
posited 20 years ago that

15
00:01:05,600 --> 00:01:12,600
Maybe rich you're gonna say this but let me for just summarize it here for that all we mean by goals and purposes can be well thought of as

16
00:01:13,440 --> 00:01:16,880
maximization of the expected value of the cumulative sum of a scalar

17
00:01:17,360 --> 00:01:19,360
Receive scalar signal or reward

18
00:01:20,040 --> 00:01:25,480
So the question that we're gonna be discussing is then is this a good model of

19
00:01:26,280 --> 00:01:30,840
Reinforcement learning is a good model for understanding human behavior and values. How far can it go?

20
00:01:31,280 --> 00:01:35,840
Can it guide normative decision-making for individuals and groups?

21
00:01:36,640 --> 00:01:37,880
and

22
00:01:37,880 --> 00:01:41,760
Totally delighted to have with us on this on this panel

23
00:01:43,280 --> 00:01:47,280
Rich Sutton who I neglected to actually

24
00:01:48,080 --> 00:01:54,320
Introduce yesterday when we started our session with blaze. So my apologies for that because he is

25
00:01:55,200 --> 00:01:56,560
world-class

26
00:01:56,560 --> 00:01:57,440
research

27
00:01:57,440 --> 00:02:05,680
Researcher and reinforcement learning as well as chief scientific advisor fellow in Canada CIFAR AI chair at Amy the Alberta machine intelligence Institute

28
00:02:06,040 --> 00:02:12,160
He's a professor of computing science at the University of Alberta and a distinguished research scientist at DeepMind

29
00:02:12,760 --> 00:02:20,320
He's been named a fellow of the Royal Society of Canada the Association for the Advancement of Artificial Intelligence and the Canadian Artificial Intelligence

30
00:02:21,160 --> 00:02:23,160
Association where he received a lifetime

31
00:02:23,440 --> 00:02:27,520
Achievement award in 2018 and as I mentioned yesterday, we're also very delighted

32
00:02:27,520 --> 00:02:32,640
He's on our advisory board at Schwartz-Reisman and joining rich today is Julia Haas

33
00:02:33,200 --> 00:02:37,240
Senior research scientist in the ethics research team at DeepMind

34
00:02:37,720 --> 00:02:44,840
She was previously an assistant professor in philosophy and neuroscience at Rhodes College and an affiliated researcher with

35
00:02:45,400 --> 00:02:50,880
and use humanizing machine intelligence grand challenge her research is in the philosophy of

36
00:02:51,160 --> 00:02:56,840
Cognitive science and neuroscience. She works on the nature evaluation and its roles and theories of the mind

37
00:02:57,120 --> 00:03:02,400
Your current work includes investigating the possibility of meaningful moral artificial

38
00:03:02,840 --> 00:03:09,440
Intelligence and I will also mention that Julia is this is a return trip to absolutely interdisciplinary for Julia

39
00:03:09,440 --> 00:03:13,560
Which we're also very grateful. It's wonderful to have people saying yes to those invitations

40
00:03:14,320 --> 00:03:18,520
But our previous one of course was online, so it's wonderful to have you here in person Julia

41
00:03:18,520 --> 00:03:21,680
And so rich, I think you're gonna get us started

42
00:03:22,480 --> 00:03:24,480
Thanks very much

43
00:03:33,400 --> 00:03:35,400
Thank you, Jillian

44
00:03:37,440 --> 00:03:41,760
Good to see y'all again. Let's try to have some more fun today

45
00:03:44,280 --> 00:03:46,280
My topic is

46
00:03:46,360 --> 00:03:50,720
Roughly the reward hypothesis, but the reward and other related

47
00:03:51,480 --> 00:03:54,880
Reductionist hypotheses because the reward hypothesis is pretty reductionists

48
00:03:55,600 --> 00:03:59,840
And I do have a slide just stating all so you can all see it

49
00:04:00,680 --> 00:04:04,560
All of what we mean by goals and purposes can be well thought of as the maximization

50
00:04:05,080 --> 00:04:11,120
With expected value of the cumulative sum of a received a scalar signal called reward

51
00:04:12,200 --> 00:04:13,400
so

52
00:04:13,400 --> 00:04:15,400
That's kind of a long

53
00:04:15,840 --> 00:04:17,320
sentence

54
00:04:17,320 --> 00:04:19,320
Sounds like it's got lots of little bits that

55
00:04:19,880 --> 00:04:21,280
that are

56
00:04:21,280 --> 00:04:23,280
intricate

57
00:04:23,320 --> 00:04:24,880
but

58
00:04:24,880 --> 00:04:28,160
But really it just says that maybe the goals of

59
00:04:28,920 --> 00:04:33,720
Whenever you want to talk about goals, maybe you can just talk about maximizing a single number and

60
00:04:36,720 --> 00:04:39,400
That is pretty reductionist and

61
00:04:40,880 --> 00:04:44,200
I'll be talking about that a little bit, but I don't want to start there

62
00:04:44,200 --> 00:04:48,880
I want to start by going back a bit. So this is my outline. I want to talk about intelligence and

63
00:04:49,520 --> 00:04:53,200
To what degree it's intrinsically tied up with the notion of a goal

64
00:04:53,720 --> 00:04:56,480
Which of course is the reward hypothesis is about

65
00:04:57,080 --> 00:05:00,040
And then we'll step into the reward hypothesis fully

66
00:05:00,920 --> 00:05:02,800
And then I also want to talk about you know

67
00:05:02,800 --> 00:05:09,720
What goes on inside because the reward hypothesis is not about what goes on inside the reward hypothesis like how you how?

68
00:05:09,720 --> 00:05:11,720
What's an appropriate framing of the problem?

69
00:05:12,240 --> 00:05:15,600
What's the appropriate framing of of a goal or a purpose?

70
00:05:16,320 --> 00:05:19,560
And it's not about how that purpose is achieved

71
00:05:20,280 --> 00:05:26,040
Okay, but that's what the agent the agent is the intelligent agent like we are the agents and

72
00:05:26,480 --> 00:05:31,880
the robots are the agents and what are the essential for

73
00:05:33,320 --> 00:05:36,000
What are they literally essential components, you know, that's

74
00:05:36,840 --> 00:05:43,920
That's something I want to talk about that. So and then and then some other hypotheses the value function hypothesis is really

75
00:05:44,480 --> 00:05:51,640
important to understand the implications of the reward hypothesis and then I'll have just really just want to slide we're

76
00:05:52,400 --> 00:05:56,880
Prepare us to start to think about how this might have implications for thinking about ethics

77
00:05:58,480 --> 00:06:00,840
Okay, so let's start with intelligence and

78
00:06:02,040 --> 00:06:03,880
Here's some

79
00:06:03,880 --> 00:06:05,440
quotes

80
00:06:05,440 --> 00:06:08,440
William James was the like the original psychologist

81
00:06:09,000 --> 00:06:13,240
His textbook was in 1890 and he spoke about

82
00:06:15,080 --> 00:06:17,520
Well, he didn't spoke about intelligence he spoke about mind

83
00:06:18,120 --> 00:06:23,400
So the hallmark of mind is attaining consistent ends by variable means

84
00:06:24,080 --> 00:06:31,000
What do you think about that? That's really talking about a goal right consistent ends from variable means something is varying its means in order to

85
00:06:31,240 --> 00:06:36,080
Achieve those consistent ends. That's really saying the hallmark of mind is

86
00:06:36,680 --> 00:06:38,680
goal-seeking

87
00:06:39,480 --> 00:06:42,280
And then there's the field of artificial intelligence which is

88
00:06:42,760 --> 00:06:47,440
Famous for not defining what intelligence is or what artificial intelligence is

89
00:06:49,800 --> 00:06:55,040
Yeah, for so long it just refused to do it and you know, I think that's not okay

90
00:06:55,320 --> 00:07:02,400
You have to everyone have a well-defined field. You have to have be clear about what your objectives are and what your subject is

91
00:07:04,600 --> 00:07:06,600
John McCarthy is the fellow who

92
00:07:08,040 --> 00:07:11,960
Invented the term artificial intelligence. He of course is one of the founding fathers

93
00:07:13,120 --> 00:07:15,120
But many years later

94
00:07:16,120 --> 00:07:18,120
He wrote down a specific

95
00:07:18,640 --> 00:07:24,800
Definition that's the one here that I that I like as the computational part of the ability to achieve goals

96
00:07:25,560 --> 00:07:32,640
The computational part the ability to achieve goals. I think that's a really interesting definition

97
00:07:35,840 --> 00:07:37,840
It's not the only definition

98
00:07:38,160 --> 00:07:42,120
Often intelligence is taken to be like mimicking people as in

99
00:07:43,120 --> 00:07:45,120
AI

100
00:07:45,160 --> 00:07:49,440
Seeks to reproduce behavior that we would call intelligent if it was done by people

101
00:07:50,040 --> 00:07:53,360
The classic Turing test is focusing on behaving like a person

102
00:07:54,040 --> 00:07:56,240
supervised learning the task is often to label

103
00:07:56,680 --> 00:08:03,760
Pictures say the same as a person would label those pictures and then the large language models, of course the large language models are all about

104
00:08:04,800 --> 00:08:06,800
mimicking

105
00:08:06,920 --> 00:08:08,920
Text generation by people

106
00:08:09,080 --> 00:08:12,320
Okay, so this is this is a major thing

107
00:08:13,240 --> 00:08:17,800
You notice aren't really any goals involved in mimicking people except, you know

108
00:08:17,800 --> 00:08:19,240
You could say mimicking people as a goal

109
00:08:19,240 --> 00:08:26,520
But really that's not what what I think goals are about goals are about affecting some change in the world

110
00:08:27,240 --> 00:08:32,840
Observing something and then you're being satisfied that you're mimicking it is not about a change in the world

111
00:08:33,560 --> 00:08:35,760
But you know think of all those systems

112
00:08:36,840 --> 00:08:42,000
That are mimicking there. They're not seeking to change their input at all. They're just seeking to mimic it

113
00:08:42,800 --> 00:08:44,800
Okay, anyway, so there's two definitions

114
00:08:44,920 --> 00:08:48,840
one is that intelligence is has to do with mimicking people and

115
00:08:49,280 --> 00:08:52,080
The other one is achieving goals and we might ask

116
00:08:52,760 --> 00:08:54,760
which is better and

117
00:08:55,920 --> 00:09:01,960
I want you to think about that and so I don't want to think about it with a little bit of sophistication

118
00:09:02,360 --> 00:09:07,160
Maybe this is kind of like yesterday, you know, I don't want us to jump to an answer because I want to think about what it means

119
00:09:07,160 --> 00:09:09,160
for there to be an answer and

120
00:09:10,000 --> 00:09:12,840
You don't really have any slides on this, but I do have this slide

121
00:09:13,800 --> 00:09:19,680
That just reminds us that you know a word like intelligence when you look in the dictionary you will find multiple

122
00:09:20,440 --> 00:09:22,200
definitions

123
00:09:22,200 --> 00:09:26,880
The second one being a military intelligence like spies and stuff

124
00:09:27,800 --> 00:09:31,760
So every word is like this and that's the way language is and it's it's good

125
00:09:31,920 --> 00:09:35,880
it's good that there are multiple definitions and multiple meanings for words and

126
00:09:36,760 --> 00:09:39,680
There so there isn't a sense that one is right or wrong. There's

127
00:09:40,760 --> 00:09:45,360
There's a sense in which they're useful for or not useful for particular purposes

128
00:09:46,000 --> 00:09:51,120
So it's our we have there's a free choice when you define a term. It's a totally free choice

129
00:09:51,120 --> 00:09:54,960
There's no right and there's no wrong in the in the definition of a word

130
00:09:55,520 --> 00:09:57,520
but there is

131
00:09:58,160 --> 00:10:00,800
Consequences right because you're you're choosing how to use it

132
00:10:00,800 --> 00:10:04,760
And so then you have to use it that way and you can decide whether for your purpose

133
00:10:05,240 --> 00:10:08,680
That's a useful way to define the term

134
00:10:09,200 --> 00:10:14,680
So the to the extent there is a right or wrong is just it's not it's it's a sense whether they're useful

135
00:10:15,480 --> 00:10:17,480
suited to purpose

136
00:10:17,800 --> 00:10:19,480
so

137
00:10:19,480 --> 00:10:21,480
Now let's go on and try to

138
00:10:22,840 --> 00:10:24,840
Assess the different meanings

139
00:10:25,880 --> 00:10:27,880
and

140
00:10:29,000 --> 00:10:33,680
I want to do that by reference to this quote from Ray Kurzweil

141
00:10:35,640 --> 00:10:40,800
He says that intelligence is the most powerful phenomenon in the universe

142
00:10:44,760 --> 00:10:53,640
So that's not a definition obviously it's it's like a property of what intelligence means something and then and then it's claiming that it's powerful

143
00:10:55,480 --> 00:10:57,320
But

144
00:10:57,320 --> 00:10:59,320
Just take it on his face for a moment

145
00:10:59,920 --> 00:11:07,820
Could intelligence be the most powerful phenomenon in the universe? I mean, what about you know black holes and supernova supernova very powerful

146
00:11:11,920 --> 00:11:19,800
But I think Ray means this literally and I think it's not crazy to mean this literally like you know

147
00:11:20,480 --> 00:11:22,360
supernova

148
00:11:22,400 --> 00:11:24,880
Are big but isn't it it's

149
00:11:25,600 --> 00:11:27,600
Isn't it also plausible that

150
00:11:27,640 --> 00:11:33,280
Well supernova have been around for billions of years. They've had billions of years to develop intelligence has been around for

151
00:11:34,120 --> 00:11:36,120
Few hundreds of thousands of years

152
00:11:36,560 --> 00:11:40,240
If you give give intelligence, you know a billion years

153
00:11:41,040 --> 00:11:44,240
Is it is it doesn't seem almost likely that?

154
00:11:45,000 --> 00:11:50,840
Intelligence if it if it still exists would would be doing things like moving the stars around moving the planets

155
00:11:51,280 --> 00:11:56,040
It would be a powerful phenomenon in the universe at that scale. I

156
00:11:57,360 --> 00:11:59,360
think it's

157
00:11:59,600 --> 00:12:03,760
It sort of expresses the ambition of what we want intelligence to be

158
00:12:05,480 --> 00:12:07,480
Okay, so

159
00:12:08,560 --> 00:12:12,360
So when when I suggested to you that it might

160
00:12:13,440 --> 00:12:18,160
Become a big deal and in a universal galaxy level

161
00:12:19,040 --> 00:12:24,120
What what I was thinking was could the fact that there are

162
00:12:25,320 --> 00:12:27,320
Agents in the world that have goals

163
00:12:28,000 --> 00:12:30,600
And it's best to think of them that way

164
00:12:31,360 --> 00:12:36,880
Could it be that powerful phenomenon eventually and that's what I want to say yes to

165
00:12:40,560 --> 00:12:46,360
For the other meaning could the ability to mimic people be such a powerful phenomenon

166
00:12:48,960 --> 00:12:52,080
I'm I think I think the answer is just no

167
00:12:53,360 --> 00:12:59,440
People are the powerful thing and so mimicking the thing that mimic will gain some power from the people

168
00:13:00,360 --> 00:13:04,320
But the the ability to mimic people is not powerful in this sense

169
00:13:05,800 --> 00:13:07,200
so

170
00:13:07,200 --> 00:13:14,560
That's my first conclusion that the powerful part of intelligence is not the ability to mimic people but the ability to achieve goals and

171
00:13:15,360 --> 00:13:16,800
so

172
00:13:16,800 --> 00:13:23,120
That's a point in favor of using the word in this way the ability to achieve goals

173
00:13:25,320 --> 00:13:27,080
Okay, now

174
00:13:27,080 --> 00:13:30,760
Let's look a little bit deeper into McCarthy's definition

175
00:13:30,880 --> 00:13:35,000
You define as the computational part of the ability to achieve goals

176
00:13:35,600 --> 00:13:42,320
So that computational part is meant to rule out like you I can achieve goals because I'm stronger as I'm faster

177
00:13:42,440 --> 00:13:44,440
And I have better sensors

178
00:13:44,680 --> 00:13:50,500
These would make you better able to achieve many goals, but it would not be because of your computations

179
00:13:51,000 --> 00:13:54,280
So we don't consider that to be intelligence

180
00:13:55,000 --> 00:13:57,000
There are at least that definition doesn't

181
00:13:58,120 --> 00:14:01,360
So moving that up a little bit to give myself give myself a little more room

182
00:14:02,560 --> 00:14:04,560
I think similarly

183
00:14:04,800 --> 00:14:09,240
You could achieve goals better if you're given knowledge about

184
00:14:09,960 --> 00:14:15,760
The world or the domain you were working in you know that would enable you to perform better to achieve goals better

185
00:14:16,600 --> 00:14:19,480
But it's not again. It's not because of your computations

186
00:14:19,480 --> 00:14:25,480
It's not your intelligence is because of the computations of where we gave you that knowledge gave you that help

187
00:14:27,000 --> 00:14:28,760
so I

188
00:14:28,760 --> 00:14:32,360
guess I have a conclusion out of that that I

189
00:14:33,000 --> 00:14:38,920
Want to say the intelligence is the computational and domain independent part of the ability to achieve goals

190
00:14:38,920 --> 00:14:40,920
maybe that's a refinement or a

191
00:14:41,440 --> 00:14:45,600
narrowing of McCarthy's definition, but I think it's in the spirit of

192
00:14:47,480 --> 00:14:52,040
Having a powerful phenomenon of Intel for intelligence being a powerful phenomenon

193
00:15:00,040 --> 00:15:02,040
Okay, so just to summarize that

194
00:15:03,040 --> 00:15:06,400
Mimicry and domain knowledge are not the powerful part of intelligence

195
00:15:07,400 --> 00:15:13,560
Mimicry is getting goal directed behavior without the goals or the processes that compute the behavior from the goals

196
00:15:15,800 --> 00:15:21,440
Injecting domain knowledge is a way of getting gold directed behavior without the processes for obtaining the domain knowledge

197
00:15:22,200 --> 00:15:24,640
So this is sort of the way I understand

198
00:15:25,240 --> 00:15:29,280
The limitations of large language models. They are the abilities

199
00:15:30,280 --> 00:15:34,480
But they were given to them and they don't have the ability to

200
00:15:35,520 --> 00:15:38,360
To develop that knowledge and that behavior themselves

201
00:15:39,400 --> 00:15:42,560
So they're they're both incomplete can't stand on their own

202
00:15:43,400 --> 00:15:46,640
These shortcuts don't have the power of intelligence. They can be very useful

203
00:15:47,520 --> 00:15:49,520
but but

204
00:15:49,520 --> 00:15:51,520
That shouldn't make them intelligence

205
00:15:52,240 --> 00:15:58,040
Using in the word in that way would weaken the search for an understanding of intelligence. That's powerful in Kurzweil sense

206
00:15:58,480 --> 00:16:01,680
Let me just say another another thing on that which is that

207
00:16:02,680 --> 00:16:08,760
intelligence the word intelligence AI AI in particular in today's world has cash a

208
00:16:10,240 --> 00:16:12,160
like

209
00:16:12,160 --> 00:16:14,560
Actually, it's not even just today's world. It's always been this way

210
00:16:15,200 --> 00:16:16,360
but

211
00:16:16,360 --> 00:16:22,960
What I mean is like for example, I went to the grocery store the drugstore the other day and I saw the aisle full of

212
00:16:23,840 --> 00:16:25,840
electric toothbrushes and

213
00:16:26,000 --> 00:16:31,200
You go to the electric toothbrush section today and they will say this toothbrush has AI in it. I

214
00:16:31,840 --> 00:16:33,840
Mean literally they will say that

215
00:16:34,000 --> 00:16:39,560
It has cash a to have AI to have intelligence is as viewed as a very positive thing

216
00:16:39,560 --> 00:16:41,840
And so everyone wants to be that way

217
00:16:41,840 --> 00:16:48,480
I hear that LG has a has a clothes washer that's has AI in it

218
00:16:49,480 --> 00:16:51,920
In the old days they used to make

219
00:16:52,680 --> 00:16:54,440
computer terminals and

220
00:16:54,440 --> 00:17:00,800
They would call the terminals intelligent terminals and all they would do they were only intelligent because they could use multiple fonts

221
00:17:02,160 --> 00:17:05,120
You know, so this has always been the case

222
00:17:06,800 --> 00:17:11,280
And so there's a tendency for everything to become AI

223
00:17:11,600 --> 00:17:17,880
More's law or the generalized Moore's law of increasing

224
00:17:18,760 --> 00:17:21,560
Computation per dollar that's gone on for a hundred years or more

225
00:17:22,760 --> 00:17:28,880
That is all about increasing computation. It's not about increasing not not it's not necessarily about increasing AI

226
00:17:29,640 --> 00:17:35,320
So there's a big there's a tendency to conflate the two trends that AI is becoming more important and

227
00:17:36,320 --> 00:17:39,680
Computation is becoming more plentiful. So all everywhere there's computation

228
00:17:40,160 --> 00:17:42,820
And that's that's what we're seeing with this

229
00:17:43,520 --> 00:17:45,200
Stretching of the term

230
00:17:45,200 --> 00:17:47,200
intelligence to cover everything

231
00:17:47,640 --> 00:17:52,960
That just uses computation. I don't think that's a useful direction to stretch the term

232
00:17:55,960 --> 00:18:02,080
Okay, I think now I'm ready for a joke and so I'm gonna I think I'm ready now to present my present my my joke

233
00:18:02,080 --> 00:18:05,680
Or my cartoon. This is my cartoon. You may have seen it

234
00:18:05,680 --> 00:18:12,040
To think that this all began with letting autocomplete finish our sentences

235
00:18:13,040 --> 00:18:16,480
So this I found this in the New York. I hope I'm not violating

236
00:18:17,440 --> 00:18:19,440
important copyright things anyway

237
00:18:21,200 --> 00:18:24,360
I'd like I really like this cartoon because it's

238
00:18:24,960 --> 00:18:28,360
It's exaggerated. It's making both fun making fun of

239
00:18:29,800 --> 00:18:33,600
Both the positive and the negative hype that's around AI

240
00:18:34,520 --> 00:18:42,080
The the positive the positive hype is that autocomplete finishing our senses will lead to you know, the robots

241
00:18:42,920 --> 00:18:44,920
being super powerful and taking over and

242
00:18:49,920 --> 00:18:52,760
Guess the negative hype is that you know if if

243
00:18:53,800 --> 00:19:01,120
Computers become powerful if we should robots become powerful they will subject us all to slavery as in as in this picture

244
00:19:02,080 --> 00:19:07,400
Okay, so humor is the best way to purify or

245
00:19:09,280 --> 00:19:11,280
our thoughts and and

246
00:19:11,800 --> 00:19:13,800
Think about controversial things

247
00:19:14,080 --> 00:19:18,280
Okay, so now I'm ready to talk more specifically about the reward hypothesis, but I hope you

248
00:19:18,880 --> 00:19:26,520
You've gotten the point the point, you know the word hypothesis about how we talk about purposes and goals and

249
00:19:26,960 --> 00:19:31,520
Intelligence is is really centrally about goals and purposes

250
00:19:32,240 --> 00:19:38,920
so if this is true if it's true that all goals and purposes can be thought of as as

251
00:19:41,800 --> 00:19:49,400
Maximizing a single number in this way, then it means that all of intelligence can be thought of as maximizing a simple number

252
00:19:49,920 --> 00:19:59,280
So this is one the multiple ways to develop this hypothesis one is to do it mathematically and formally and and

253
00:20:01,200 --> 00:20:05,320
Like you know, what's this? What does it mean to have a have a goal?

254
00:20:05,840 --> 00:20:14,080
It's an ordering on possible outcomes and what are they all possible different ways of doing an ordering and you're looking at this

255
00:20:14,080 --> 00:20:16,080
Carefully as a mathematical formal statement

256
00:20:16,440 --> 00:20:21,560
Like one might in do in economics or the theory of decision-making

257
00:20:22,880 --> 00:20:26,960
So that's that's the work this idea has been developed in that direction

258
00:20:26,960 --> 00:20:29,720
I'm and I'm referring to the work in the corner

259
00:20:30,120 --> 00:20:33,480
It's more recent work by Michael bowling and John Martin David

260
00:20:34,000 --> 00:20:38,800
Abel and Will Dabney where they formally assess it

261
00:20:39,440 --> 00:20:41,440
to the extent that it is

262
00:20:42,320 --> 00:20:48,280
Complete and includes anything else you might propose. So what are the other things that are ruled out? You may be thinking, you know

263
00:20:48,960 --> 00:20:52,880
What's ruled out would be things like considerations of risk in a special way

264
00:20:54,320 --> 00:20:56,320
consideration of multiple objectives

265
00:20:59,080 --> 00:21:03,600
You see it's all about the expected value rather than the distribution of possibilities

266
00:21:05,000 --> 00:21:08,640
Okay, now another another hypothesis that's been around

267
00:21:11,440 --> 00:21:13,440
The reward is enough hypothesis

268
00:21:14,000 --> 00:21:20,280
That there's intelligence and all of its associated abilities can be understood as subserving the maximization of reward

269
00:21:20,800 --> 00:21:28,200
So this is very similar. It's almost like the combination of the reward hypothesis and the McCarthy's definition of intelligence

270
00:21:30,400 --> 00:21:35,760
But notice all these things are about the goals are about the the problem the problem

271
00:21:36,480 --> 00:21:40,080
They're not at all talking about solution methods. They're just saying

272
00:21:41,040 --> 00:21:48,000
Reward might be enough to motivate and drive the achieving the achievement of the associated abilities, whatever they may be

273
00:21:49,280 --> 00:21:54,200
You can understand them as as being driven or subserving the maximization of reward

274
00:21:55,520 --> 00:21:58,000
Okay, so now I want to

275
00:21:59,000 --> 00:22:05,320
Is reward enough is a single number enough? It doesn't it's it's it's it doesn't seem like enough. I just wanted to

276
00:22:06,320 --> 00:22:10,360
Bring this on the table. I'm sure you felt that it seems too small a

277
00:22:11,040 --> 00:22:12,360
single number

278
00:22:12,360 --> 00:22:14,280
coming from outside

279
00:22:14,280 --> 00:22:16,200
the agent

280
00:22:16,200 --> 00:22:21,920
You know people seem to choose their own goals. I mean, that's one of the biggest things we see for ourselves

281
00:22:22,280 --> 00:22:25,200
We define ourselves by the goals we set out to achieve

282
00:22:26,920 --> 00:22:33,440
Reward just seems too small too reductive. It's definitely reductionist hypothesis. It's and as reduct being reductive

283
00:22:33,440 --> 00:22:35,240
It's kind of demeaning

284
00:22:35,240 --> 00:22:40,720
Surely our goals are grander than maximizing, you know pleasure and pain or some number

285
00:22:41,160 --> 00:22:45,720
You know, we have things like raising a family saving the planet making the world a better place

286
00:22:46,200 --> 00:22:48,120
contributing to human knowledge

287
00:22:48,120 --> 00:22:50,120
Not just these small things so

288
00:22:50,360 --> 00:22:58,360
So that's really the tension. I think around the reward hypothesis that it seems like it seems too small and yet it seems

289
00:22:58,760 --> 00:23:04,280
We keep being driven back to it as we try to get formal as we try to be clear about goals

290
00:23:04,840 --> 00:23:06,080
we're

291
00:23:06,080 --> 00:23:08,080
We're driven back to it because it's clear

292
00:23:08,720 --> 00:23:10,520
because it's

293
00:23:10,520 --> 00:23:12,520
experiential

294
00:23:14,280 --> 00:23:19,040
Kind of grounds things it gives us a well-defined place to proceed

295
00:23:19,880 --> 00:23:21,880
so now a few slides

296
00:23:22,320 --> 00:23:24,320
exemplifying how

297
00:23:24,920 --> 00:23:33,080
Even though we're uneasy with this idea we keep coming back to it we what is we I mean, it's an interdisciplinary idea

298
00:23:34,440 --> 00:23:36,440
This is the modern view about

299
00:23:37,320 --> 00:23:41,920
About how our brains work is that there is there is a measure of

300
00:23:42,560 --> 00:23:44,560
Pleasure and pain and then

301
00:23:44,560 --> 00:23:47,120
There are calculations. I'll get into a little bit later

302
00:23:47,120 --> 00:23:53,520
But it's totally consistent with this and not just we it's like all animals have like a dopamine center

303
00:23:53,520 --> 00:23:55,240
and

304
00:23:55,240 --> 00:24:01,480
This is a good it has been true that a good way to think about them has been in terms of a scalar outcome

305
00:24:02,320 --> 00:24:04,240
Not entirely

306
00:24:04,240 --> 00:24:07,160
Okay, so let me go through some facts

307
00:24:09,120 --> 00:24:11,760
This is some of these slides a little more detailed than we need

308
00:24:12,720 --> 00:24:17,440
But I just want to refer that within AI which is also uneasy with the idea of reward

309
00:24:18,200 --> 00:24:21,480
It's it's become more comfortable with it over time

310
00:24:21,880 --> 00:24:25,840
So the very earliest AI systems were all formulated their goals as

311
00:24:26,360 --> 00:24:28,360
Attain the state of the world

312
00:24:28,680 --> 00:24:30,800
Okay, which is which is very different from

313
00:24:31,720 --> 00:24:34,740
Maximizes number coming into your into your mind

314
00:24:35,920 --> 00:24:42,240
It's as if we can access directly the states of the world which which we cannot of course we have to infer them from our

315
00:24:42,640 --> 00:24:44,640
from our sensations

316
00:24:45,120 --> 00:24:47,120
but

317
00:24:47,440 --> 00:24:51,960
And and I'm saying it's even true the latest version of the standard AI textbook

318
00:24:52,440 --> 00:25:00,080
Russell Norvig it still talks about goals primarily in terms of states of the world and not in terms of experience not in terms of reward

319
00:25:00,840 --> 00:25:04,720
But it also has chapters on reinforcement learning and those those all use reward and

320
00:25:05,640 --> 00:25:13,200
With the rise of machine learning within AI the reward formulation has becoming more and more standard as in planning and mark-up decision processes

321
00:25:13,960 --> 00:25:15,680
and

322
00:25:15,680 --> 00:25:19,920
We can look at Yanlacoon Yanlacoon who was who's sort of an anti

323
00:25:20,680 --> 00:25:26,000
Reinforcement learning person he now admits that if the if the mind is a cake

324
00:25:26,640 --> 00:25:28,640
This is his metaphor

325
00:25:29,640 --> 00:25:34,840
Is that if the mind is a cake then reinforcement learning is a tiny part of it's like the cherry on top

326
00:25:35,360 --> 00:25:40,600
but he thinks the reinforcement learning acknowledges that reinforced learning is necessary because goals are necessary and

327
00:25:41,360 --> 00:25:48,520
Either the substance of the of the cake is maybe doing prediction or unsupervised learning and so it those parts are not

328
00:25:48,880 --> 00:25:52,160
Goal oriented. They're just trying to predict and understand the world

329
00:25:52,160 --> 00:25:57,920
And I shouldn't say just they're trying to predict and they're trying to understand the world and model the world those don't

330
00:25:59,320 --> 00:26:02,280
Don't have goals in them. They're just

331
00:26:03,000 --> 00:26:07,280
I said it again just gotta watch out for that little word just they are

332
00:26:08,280 --> 00:26:12,600
Understanding the the truth of the world as separate from

333
00:26:13,600 --> 00:26:15,600
Trying to direct that in any particular way

334
00:26:16,080 --> 00:26:21,720
Okay, but anyway, he's got the cherry on the top and to me. It's a cherry on the top of the cake. So it's pretty important

335
00:26:22,640 --> 00:26:24,600
and

336
00:26:24,600 --> 00:26:30,000
Here's another one from classic AI artificial intelligence

337
00:26:31,000 --> 00:26:35,120
There's this these cognitive architectures since the 80s

338
00:26:35,200 --> 00:26:43,840
They're very symbolic and using production rules and since like 2008 and they've included reward as part of it as as a as a basis for

339
00:26:45,080 --> 00:26:47,600
The the goals of the system

340
00:26:48,120 --> 00:26:53,000
So single numbers have becoming more prominent in AI as a formulation of the goal

341
00:26:53,720 --> 00:26:55,120
now this

342
00:26:55,120 --> 00:26:58,600
now let's talk more in a more interdisciplinary sense and

343
00:26:59,600 --> 00:27:03,120
I recently wrote a little paper where I just sort of said, oh, you know

344
00:27:03,520 --> 00:27:06,840
There's a lot of commonality between many many fields thinking about

345
00:27:07,600 --> 00:27:16,120
Agenthood and goals and purposes in mind, you know in psychology control theory AI economics neuroscience operations research at least these six

346
00:27:16,400 --> 00:27:19,360
You can find basically the elements

347
00:27:19,960 --> 00:27:26,200
In these two pictures the first picture is the agent interacting with the world or the environment and

348
00:27:26,880 --> 00:27:32,960
So this is the basic picture of a of a decision maker. He

349
00:27:33,920 --> 00:27:40,640
Sees information from the world his observations and he picks actions and then he gets back a

350
00:27:41,240 --> 00:27:48,440
Scalar measure of performance that he with the objective is to maximize it. That's sort of accepting the reward framework

351
00:27:49,000 --> 00:27:51,000
You will find this

352
00:27:51,960 --> 00:27:57,120
Of course these ideas very basic, but even when you look inside the agent now

353
00:27:57,120 --> 00:28:03,480
So I'm gonna look inside the agent how the problem is solved you can find some of the common elements and

354
00:28:04,640 --> 00:28:06,640
So there are four of them

355
00:28:07,160 --> 00:28:10,400
But let's talk first about the core two

356
00:28:10,840 --> 00:28:16,640
So I'm gonna block some of these out. Yeah, the core two is the perception and

357
00:28:17,280 --> 00:28:20,760
The policy the reactive policy so perception

358
00:28:21,520 --> 00:28:25,560
Takes in observation so observations are like, you know, your sensory input and

359
00:28:26,800 --> 00:28:31,520
From that you construct a sense of where you are. That's that's the state representation

360
00:28:31,520 --> 00:28:35,600
And then you choose what to do based upon the state representation

361
00:28:35,920 --> 00:28:39,960
The difference between the two boxes is that the policy is

362
00:28:40,440 --> 00:28:46,160
Memoryless right you can't use old states to decide what you're gonna do now the whole point of a stage as it says

363
00:28:46,160 --> 00:28:49,880
This is where I am now this I should decide what to do based on where I am now

364
00:28:50,440 --> 00:28:51,760
it's

365
00:28:51,760 --> 00:28:55,920
And it's the job of perception to construct that sense of where you are now

366
00:28:57,000 --> 00:29:04,880
So perception is recurrent it involves the last thing you did the last observation and the the previous state

367
00:29:05,680 --> 00:29:07,680
Now

368
00:29:09,720 --> 00:29:11,720
This is a complete behaving system

369
00:29:11,920 --> 00:29:16,440
Observation comes in flows through perception and the policy produced in action

370
00:29:16,440 --> 00:29:21,880
That's all you need to be a complete behaving system and you notice reward is kind of hanging out there

371
00:29:21,880 --> 00:29:24,760
It's not really doing anything and that's because

372
00:29:26,000 --> 00:29:31,520
This is a complete behaving system, but not a system that's made up for changing what that system does

373
00:29:31,920 --> 00:29:36,040
So let me dig do one more dig at large language models. They are this part, you know

374
00:29:36,040 --> 00:29:38,040
they are a way of

375
00:29:38,240 --> 00:29:41,240
transforming the sequence of words into a

376
00:29:42,480 --> 00:29:47,020
Memory of where you are and then deciding which word to output

377
00:29:48,080 --> 00:29:55,120
But it's not a way of changing that the there isn't a once your large language model is in place is running in the world

378
00:29:55,360 --> 00:29:58,640
It can't change. It's it's just these two parts now

379
00:29:58,640 --> 00:30:02,440
You can change in the sense that perception can accumulate your a

380
00:30:03,120 --> 00:30:09,800
More refined state as you get more observations, but you can't get a change in the policy for example

381
00:30:10,520 --> 00:30:13,200
or or a change in the

382
00:30:14,080 --> 00:30:16,080
Function that is implemented as perception

383
00:30:20,200 --> 00:30:23,400
Maybe I'll dwell on this a little bit more and then on the

384
00:30:25,920 --> 00:30:27,920
interdisciplinary aspect

385
00:30:27,920 --> 00:30:34,960
So we see just remember that all these different fields have used different names like if you're in psychology you talk about

386
00:30:35,720 --> 00:30:38,760
so the action you talk about the response and

387
00:30:40,160 --> 00:30:43,000
in control theory you would talk about the

388
00:30:43,760 --> 00:30:47,280
the control instead of the policy they talk about the

389
00:30:47,960 --> 00:30:49,600
control law

390
00:30:49,600 --> 00:30:51,400
instead of instead of

391
00:30:51,400 --> 00:30:55,360
Perception in in control theory they talk about a state estimation

392
00:30:56,000 --> 00:30:57,280
box

393
00:30:57,280 --> 00:31:03,400
In psychology the observation would be a stimulus and the reward might be reward

394
00:31:05,280 --> 00:31:07,800
But actually the subtleties of the

395
00:31:09,200 --> 00:31:11,800
Connotations of the word are somewhat different in psychology

396
00:31:12,560 --> 00:31:14,560
So

397
00:31:17,560 --> 00:31:23,120
I think it's it's kind of interesting to try to find a relatively neutral language that can

398
00:31:23,840 --> 00:31:26,280
Apply to all these all these different fields

399
00:31:28,240 --> 00:31:30,000
Okay, so

400
00:31:30,000 --> 00:31:35,040
I'm unblocked the transition model the transition model is supposed to be the model of the world

401
00:31:35,680 --> 00:31:39,920
It's gonna have your domain major place where domain knowledge is

402
00:31:40,760 --> 00:31:42,760
Okay, I'm not gonna talk about that today

403
00:31:43,160 --> 00:31:48,280
So let's get rid of that. Anyway, but it would do planning. That's how you do planning

404
00:31:48,760 --> 00:31:50,760
We're not gonna talk about that

405
00:31:52,840 --> 00:31:58,280
But I do want to talk today as part of talking about the reward hypothesis about the

406
00:31:58,840 --> 00:32:03,640
The other box the value function box is the value function is the major source of

407
00:32:04,840 --> 00:32:06,840
Learning input of changes in the policy

408
00:32:07,840 --> 00:32:12,760
And this notion of a value function is also common to all these fields

409
00:32:17,760 --> 00:32:20,440
Hey, I through reinforcement learning has value functions

410
00:32:21,680 --> 00:32:23,680
psychology has has

411
00:32:25,240 --> 00:32:30,160
Reinforcers and secondary reinforcers and and it's built up in the same way

412
00:32:31,120 --> 00:32:33,120
and

413
00:32:33,800 --> 00:32:38,440
There are issues of proxy objective functions and control theory that are all about

414
00:32:39,160 --> 00:32:43,340
Constructing value functions. So what is this value function thing? Let's talk about that

415
00:32:44,520 --> 00:32:49,760
Oh, but I couldn't I couldn't resist I want I got in this x1 extra slide where I'm just

416
00:32:50,200 --> 00:32:55,040
It does remind me to say to say one important thing that even though there's only one objective

417
00:32:55,480 --> 00:32:59,760
the reward to maximize reward the agent can well as as as

418
00:33:00,480 --> 00:33:02,040
Means as

419
00:33:02,040 --> 00:33:08,720
Solution strategies it might have it might pose some problems for itself other problems for itself

420
00:33:08,720 --> 00:33:14,840
Like you might learn how to how to walk or how to drink a glass or how to navigate to the monk school

421
00:33:15,520 --> 00:33:19,000
You learn all those things that they're not your main task

422
00:33:19,440 --> 00:33:25,120
But it might if you learn how to do those things it may be useful for solving the main task. So you you

423
00:33:26,120 --> 00:33:30,960
My theory of of mind and the next step of that theory would be that we

424
00:33:31,360 --> 00:33:37,280
We pose some problems for ourselves and solve those and then use those skills that we develop working on the sub problems

425
00:33:37,280 --> 00:33:40,680
In order to solve the real problem, which is to maximize reward

426
00:33:41,920 --> 00:33:46,080
Okay, so value functions. We're getting almost done

427
00:33:46,840 --> 00:33:48,840
value functions

428
00:33:49,440 --> 00:33:54,800
So reward and value reward defines what's good. So we seek a policy that maximizes reward

429
00:33:54,800 --> 00:33:57,160
That's done defining the problem

430
00:33:58,080 --> 00:34:03,880
But reward is often delayed and that makes it hard to learn a policy using reward

431
00:34:03,880 --> 00:34:10,040
And so instead of working directly with reward value functions map states to predictions of the future reward

432
00:34:10,120 --> 00:34:12,840
if you have prediction of the future that includes the

433
00:34:13,240 --> 00:34:19,800
Delay and if you can bring that into the into the present if you can predict now what the future rewards will be that

434
00:34:20,320 --> 00:34:22,080
enables you to

435
00:34:22,080 --> 00:34:25,920
Eliminate the delay and makes finding a good policy much easier

436
00:34:26,680 --> 00:34:30,200
Maybe that's intuitive you think about I always think about playing chess

437
00:34:30,320 --> 00:34:33,040
You know the reward is checkmating your opponent

438
00:34:33,040 --> 00:34:38,200
And so then you get maybe a plus one for checkmating a minus one for losing is zero for drawings

439
00:34:38,200 --> 00:34:39,480
so that is

440
00:34:39,480 --> 00:34:40,840
and of course

441
00:34:40,840 --> 00:34:47,000
Formally you get a reward all during the game, but they're all zero and you only get an interesting reward at the end and

442
00:34:47,360 --> 00:34:51,080
So there's a delay between any good move or poor move

443
00:34:51,080 --> 00:34:58,060
You might make early in the game and and whether you won or you lost and so of course what we would all do is we would

444
00:34:58,440 --> 00:35:02,880
Learn to predict. Am I going to be checkmated in this game?

445
00:35:03,320 --> 00:35:09,080
Okay, so that's a prediction of the future and it's often called an evaluation function or a value function

446
00:35:09,080 --> 00:35:13,760
It's a prediction of the reward. So I think it's it's

447
00:35:14,760 --> 00:35:18,160
Well, this is the the value function hypothesis is that

448
00:35:19,400 --> 00:35:22,920
Forming such value. I guess that's that's that's right on my slide here

449
00:35:23,360 --> 00:35:28,320
The value function hypothesis is that all efficient methods for solving

450
00:35:29,440 --> 00:35:33,960
Sequential decision problems, which is what just means decision problems over and over again through time

451
00:35:35,600 --> 00:35:37,600
They learn or compute

452
00:35:38,240 --> 00:35:41,000
Value functions as an intermediate solution step

453
00:35:42,000 --> 00:35:48,520
So when you're gonna play chess you learn the sense of am I when you're losing and then you know when you make a bad move and now

454
00:35:48,520 --> 00:35:52,040
You thought you were winning and now you're losing you see then no

455
00:35:52,040 --> 00:35:53,960
That's the critical part

456
00:35:53,960 --> 00:36:00,720
Where you made the mistake and you can use that to assign credit to your policy and change your policy in a much better way

457
00:36:02,200 --> 00:36:05,000
Okay, so that's all sounds awfully technical

458
00:36:06,440 --> 00:36:09,920
Let's go back to philosophy go back to Plato, you know

459
00:36:10,720 --> 00:36:17,620
You can find in Plato talking about good and evil and pleasure and pain and he will say things that are a lot like this

460
00:36:18,680 --> 00:36:26,120
These are some quotes even enjoying yourself. You call evil whenever it leads to the loss of a pleasure greater than its own

461
00:36:26,600 --> 00:36:31,120
Or it lays up pains lays up pains that outweigh its pleasures

462
00:36:31,960 --> 00:36:36,160
Isn't it the same when we turn back to pain to suffer pain you call good

463
00:36:36,880 --> 00:36:43,000
When it either rids us of greater pains than its own or leads to pleasures that outweigh them

464
00:36:43,200 --> 00:36:47,040
So what does he say he's saying good and evil are about the sum of upcoming reward?

465
00:36:48,320 --> 00:36:50,960
Which is what we try to predict with a value function

466
00:36:51,800 --> 00:36:58,360
So basically it's all hedonism. It's all hedonism, but value functions make it hedonism with foresight

467
00:36:59,320 --> 00:37:05,680
Okay, that's the story the main story of reward and value how we use them in decision-making to change our policies

468
00:37:07,040 --> 00:37:09,440
But I want to try to do one more thing

469
00:37:11,160 --> 00:37:15,640
I get you understand the way it works in a little bit more

470
00:37:16,560 --> 00:37:22,560
I don't know if this is gonna work, but let me try I call it the mystery of expectation and reinforcement

471
00:37:22,760 --> 00:37:27,280
I call it the mystery of expectation and reinforcement now

472
00:37:27,840 --> 00:37:33,040
Superficially those are new topics. I haven't talked about expectation and haven't talked about reinforcement

473
00:37:33,120 --> 00:37:40,440
But by expectation I mean like the prediction of reward that the value function is the value function learns predictions i.e

474
00:37:41,400 --> 00:37:43,920
expectations of upcoming reward and

475
00:37:45,640 --> 00:37:47,640
Reinforcement reinforcement

476
00:37:48,640 --> 00:37:53,200
Is not reward reinforcement is the

477
00:37:55,160 --> 00:37:59,680
Moment-by-moment signal that reinforces the behavior at that time

478
00:37:59,840 --> 00:38:06,680
So it's really your sense of did I do something good or did I do something bad is should I stamp what in should I stamp in what?

479
00:38:06,680 --> 00:38:10,920
I did or not so reinforcement is not reward because there are many cases

480
00:38:10,920 --> 00:38:16,080
And we just alluded to some of them where you get say a positive reward, but you

481
00:38:16,880 --> 00:38:18,880
View it negatively

482
00:38:19,520 --> 00:38:24,760
And so expectations play a role there so expectations

483
00:38:25,720 --> 00:38:29,680
Contribute negatively to reinforcement in the sense like you know I went to the movie

484
00:38:29,680 --> 00:38:34,320
I thought it'd be a really good movie and I went there. I was just kind of so so so actually I feel bad about it

485
00:38:35,280 --> 00:38:37,480
so so expectations are

486
00:38:38,640 --> 00:38:43,080
Negatively to the reinforcement movie was just so so but because you expected it to be good

487
00:38:43,560 --> 00:38:48,520
That gave you a bad feeling about it a bad negative reinforcement

488
00:38:49,720 --> 00:38:55,160
But expectations also contribute positively to reinforcement like when you

489
00:38:57,360 --> 00:39:06,600
Expect to get reward that that is a reinforcing event if you if you I don't know are given a million dollars

490
00:39:07,600 --> 00:39:15,680
That's a good thing, okay, but it's actually not really reward reward becomes because you expect to have a million dollars and be able to use it

491
00:39:15,680 --> 00:39:17,680
you know to buy

492
00:39:17,680 --> 00:39:23,400
Good food or anything you might want so that would be the reward and the expectations are also positive

493
00:39:23,400 --> 00:39:30,800
And so that's the mystery. How can expectations contribute both positively and negatively it is in just a common-sense way and

494
00:39:31,840 --> 00:39:33,840
the answer is over time and

495
00:39:34,440 --> 00:39:41,600
So just consider this simple case of evolving through three states states one two and three and

496
00:39:43,000 --> 00:39:46,120
This is like your life some life moment by moment

497
00:39:46,840 --> 00:39:55,320
so three moments two transitions and I'll just take it that your expectations your predictions your values of the three states are

498
00:39:56,240 --> 00:40:00,080
zero ten and zero and then

499
00:40:01,080 --> 00:40:07,160
The rewards on the when you experience these three states are zero and eight

500
00:40:07,160 --> 00:40:10,920
Okay, so now I want to ask you the question or ask you to ask this question of yourself

501
00:40:11,560 --> 00:40:13,560
How do you feel about the first transition?

502
00:40:14,160 --> 00:40:17,920
So what's the reinforcing effect is it positive?

503
00:40:17,920 --> 00:40:22,840
It's negative like is it good is it bad as you went from state one to state two and

504
00:40:23,480 --> 00:40:26,640
It's I want you to give me a number because it's all coming down to numbers

505
00:40:27,640 --> 00:40:34,280
So what number would you give to that transition from state one to state two

506
00:40:35,840 --> 00:40:43,200
Expectation was zero now. It's ten the reward was zero. Do you feel good? You feel bad?

507
00:40:46,200 --> 00:40:48,200
Anybody

508
00:40:49,880 --> 00:40:52,720
John's giving me a thumbs up she was giving me a thumbs up

509
00:40:53,240 --> 00:40:56,720
Yeah, I think we're gonna say this is like a plus ten experience because

510
00:40:57,560 --> 00:41:00,440
You weren't expecting anything and you didn't get anything right away

511
00:41:00,440 --> 00:41:05,080
But you now you now you expect to get ten in the future. That's like the getting the million dollars

512
00:41:05,600 --> 00:41:07,600
And now the second transition

513
00:41:08,400 --> 00:41:10,120
What do you expect?

514
00:41:10,120 --> 00:41:15,520
Expecting ten you got eight and now you don't expect anything in the future

515
00:41:16,720 --> 00:41:18,720
That's gonna be like minus two

516
00:41:18,840 --> 00:41:23,520
Okay, so that all makes sense. It's very intuitive. Okay, so if you just

517
00:41:24,120 --> 00:41:29,080
Make a formula out of what you you've just understood is as obviously what should happen

518
00:41:29,200 --> 00:41:32,720
You're gonna end up with temporal difference learning the fundamental algorithm of of

519
00:41:33,600 --> 00:41:40,000
Reinforcement learning. Okay, so the the thing you just formed in your head was that the reinforcement is sure it's the reward

520
00:41:40,560 --> 00:41:45,600
But beyond that it's the change in your expectation from one time step to the next

521
00:41:46,360 --> 00:41:49,320
As has the expectation gone up as it gone down

522
00:41:49,320 --> 00:41:54,480
And so it's the sum of the reward and the change in the expectation because it's a change

523
00:41:54,480 --> 00:41:59,120
It's a temporal difference temporal difference just means time difference, which is just change

524
00:41:59,440 --> 00:42:04,560
It's the change in your expectation contributes to the reinforcement. And so this is the

525
00:42:05,280 --> 00:42:07,280
temporal difference error and

526
00:42:07,560 --> 00:42:09,560
also called the reward prediction error and

527
00:42:10,320 --> 00:42:14,360
I need to finish up, but I did want to mention since it's such an interdisciplinary

528
00:42:15,360 --> 00:42:19,720
Conference that this theory of brain systems as

529
00:42:20,720 --> 00:42:22,720
Following TD learning is one of the most

530
00:42:23,480 --> 00:42:29,200
Important interactions ever between engineering science as a neuroscience is a big big thing around the turn of the century

531
00:42:30,400 --> 00:42:34,960
And finally my last slide is on ethics

532
00:42:35,600 --> 00:42:41,320
So what we've talked about rewards are a good way to think about the ultimate goal and value functions

533
00:42:41,440 --> 00:42:47,960
Which are predictions of reward are a good way to think about how that goal is achieved. All this is neat and complete

534
00:42:47,960 --> 00:42:51,480
It's a good theory of a single agents decision making

535
00:42:51,840 --> 00:42:58,760
But it's it doesn't have what we would expect what we might hope to get from a theory of ethics is that there would be some

536
00:42:59,040 --> 00:43:01,440
universality to it there would be a

537
00:43:02,320 --> 00:43:09,600
Reason to for different agents to have similar values, you know, that's what that's that's what I understand ethics to be about

538
00:43:09,600 --> 00:43:11,600
It's when we reach for some

539
00:43:11,800 --> 00:43:15,900
universality rather than just an individual and what we have so far doesn't have any

540
00:43:16,480 --> 00:43:19,480
element of that and I'm gonna now admit that I have no

541
00:43:20,040 --> 00:43:22,040
no sense how

542
00:43:23,400 --> 00:43:29,680
Any universality could occur and so I want to propose that maybe there is no universality there's a need to be universal

543
00:43:30,080 --> 00:43:33,200
Sality for there to be ethics. Maybe we just

544
00:43:34,200 --> 00:43:40,200
Many of us have there is an aspect of what we do of commonality between many people and

545
00:43:40,800 --> 00:43:45,800
That that's what ethics is about. It's so this is a reductionist hypothesis again

546
00:43:45,800 --> 00:43:51,440
that maybe ethics is just values that are happened to be held in common by many agents and

547
00:43:52,160 --> 00:43:54,160
so even though

548
00:43:54,600 --> 00:43:58,840
Even things that are held in common by many agents won't be held in common by all agents

549
00:43:59,840 --> 00:44:03,120
The lion and the gazelle will have different values

550
00:44:05,680 --> 00:44:11,880
So that's really all I wanted to say and I thought it would be a good basis for starting a discussion of ethics

551
00:44:13,880 --> 00:44:15,880
Thank you very much

552
00:44:19,080 --> 00:44:21,080
You're up

553
00:44:28,840 --> 00:44:30,840
Jinx

554
00:44:32,040 --> 00:44:36,960
Thanks, Rich, I'm really glad we got a chance to do this and thanks to Jillian for the same reason

555
00:44:38,280 --> 00:44:44,000
So I'm gonna start with just a little bit of housekeeping disciplinary housekeeping just to make sure we're all on the same page

556
00:44:45,400 --> 00:44:47,400
So like any paradigm

557
00:44:48,480 --> 00:44:53,280
Reinforcement learning is going to have certain technical and foundational commitments

558
00:44:54,080 --> 00:44:55,720
and then

559
00:44:55,720 --> 00:45:02,680
Loss of different versions of those frameworks are going to add certain assumptions and relax others

560
00:45:03,040 --> 00:45:04,640
And so just before I get started

561
00:45:04,640 --> 00:45:09,080
I want to make clear two assumptions that I'm committing to because they will inform kind of what I say next

562
00:45:09,120 --> 00:45:15,440
I'm gonna call this RL star or sometimes they can also be called RLDM like reinforcement learning and decision-making I

563
00:45:16,880 --> 00:45:21,120
Think it also make clear maybe where where Rich and I agree and sometimes might disagree

564
00:45:21,760 --> 00:45:23,760
So the first assumption

565
00:45:23,760 --> 00:45:25,760
In my kind of particular

566
00:45:25,960 --> 00:45:31,280
Version of reinforcement learning is that there is in fact something really special

567
00:45:31,520 --> 00:45:38,000
About reinforcement learning when it comes to understanding the mind, right? And so I think this is an assumption we share we both subscribe to this assumption

568
00:45:39,520 --> 00:45:43,960
Peter Diane who just got mentioned and put up as like a little plastic bobble head and yell these

569
00:45:44,800 --> 00:45:50,680
Put forward something like the following claim so reinforcement learning algorithms such as the temporal defense learning

570
00:45:50,800 --> 00:45:57,800
Rule apparent to be directly instantiated in neural mechanisms such as the phasic activity of dopamine neurons

571
00:45:58,000 --> 00:46:01,920
That reinforcement learning appears to be so transparently embedded has made it possible

572
00:46:02,440 --> 00:46:06,520
To use it in a much more immediate way to make hypotheses about and

573
00:46:06,880 --> 00:46:14,040
Retradictive and predictive interpretations of a wealth of behavioral and neural data collected in a huge range of paradigms and systems, right?

574
00:46:14,040 --> 00:46:16,040
So there's this

575
00:46:16,040 --> 00:46:18,040
Is the sound coming out already?

576
00:46:18,320 --> 00:46:20,400
Yeah, I've been wondering that too. So thanks, Rich

577
00:46:21,000 --> 00:46:27,080
There's a star beside instantiated for the philosophers of science in the room. Just relax about that

578
00:46:27,080 --> 00:46:33,160
It doesn't matter we can just say look reinforcement learning is shockingly surprisingly good about capturing something of the mind

579
00:46:33,160 --> 00:46:36,400
We don't have to commit to instantiate. It's one of those weasel words we talked about yesterday

580
00:46:37,160 --> 00:46:40,440
It's a quote from them, but that's just not what we don't want to talk about here

581
00:46:40,440 --> 00:46:42,440
There's something about reinforcement learning

582
00:46:43,080 --> 00:46:48,480
That is extremely powerful when it comes to understanding the mind whether it's instantiated or not

583
00:46:48,480 --> 00:46:54,120
So that's the first commitment that I'm making here the second commitment where we may disagree although

584
00:46:54,600 --> 00:46:55,680
sudden

585
00:46:55,680 --> 00:47:00,680
June 21st 2023 suggest maybe that's not the case is that

586
00:47:03,040 --> 00:47:10,400
Mines like ours so personally assigned subjective rewards somehow so there's a big debate about this question of where do rewards come from?

587
00:47:10,440 --> 00:47:15,920
right and the kind of classic view is that reward comes externally and

588
00:47:16,480 --> 00:47:19,360
That is an important tenet of some frameworks of reinforcement learning

589
00:47:19,880 --> 00:47:26,000
But I'm gonna go ahead and say and this is kind of an agnostic claim that the mind somehow

590
00:47:27,640 --> 00:47:32,120
Internalizes reward there are good competing theories for how that takes place

591
00:47:32,440 --> 00:47:36,760
Such as by Chris and refilled it doesn't matter. I just want to say that the mind somehow

592
00:47:37,400 --> 00:47:39,400
Internalizes reward in ways that are important

593
00:47:39,920 --> 00:47:44,280
So that's gonna be a commitment where we perhaps depart from one another

594
00:47:44,840 --> 00:47:47,780
But it's gonna inform kind of how we want to go forward about this

595
00:47:48,520 --> 00:47:50,520
Okay, so that's a bit of ground-clearing

596
00:47:50,960 --> 00:47:52,960
What I want to use my time

597
00:47:53,040 --> 00:47:56,520
For today and kind of as a basis or a backdrop for the discussion

598
00:47:56,520 --> 00:48:01,880
Let's make three claims about kind of this version of reinforcement learning and what it means

599
00:48:02,440 --> 00:48:06,800
For understanding the mind and so the first question that we have right is you know

600
00:48:06,800 --> 00:48:10,560
Is the reward hypothesis a good model for understanding the mind?

601
00:48:11,240 --> 00:48:12,440
And

602
00:48:12,440 --> 00:48:17,680
As rich has pointed out, you know, it's been kind of transformative in the decision sciences and all those schools that we talked about

603
00:48:18,680 --> 00:48:24,760
Cognitive neuroscience computational neuroscience psychology economics increasingly. I think in philosophy

604
00:48:24,760 --> 00:48:26,520
It's like making inroads in philosophy

605
00:48:26,520 --> 00:48:31,320
But I would actually say the first kind of claim that I want to make is that it's still underappreciated

606
00:48:32,040 --> 00:48:39,000
In how much it should and can transform my understanding of what the mind is fundamentally in the business of doing

607
00:48:41,040 --> 00:48:44,840
So this is kind of the cartoon dialectic slide, right?

608
00:48:44,840 --> 00:48:49,280
I think when we think of the mind we have historically but even in

609
00:48:49,680 --> 00:48:54,880
Recent kind of cognitive scientific history. So we've seen this in Joel's slide in Blaise's slide

610
00:48:55,560 --> 00:49:00,080
Yesterday is you know, when we think of what the mind is fundamentally in the business of doing

611
00:49:00,600 --> 00:49:07,120
As how Glenn put it, you know, it's about thinking. It's what intellect. It's it's fundamentally kind of an epistemic machine

612
00:49:07,360 --> 00:49:09,360
It's what the mind is a computer

613
00:49:09,480 --> 00:49:15,840
And one of the ways that we can put that is that the mind is in the business of performing computations

614
00:49:16,400 --> 00:49:24,240
over representations of descriptive matter of fact, which is just to say the mind is in the business of making better and worse kind of

615
00:49:24,920 --> 00:49:29,960
Assessments of what's going on out there in the world. Is it raining? You know, what day of the week is it?

616
00:49:29,960 --> 00:49:33,520
It's about knowing the world so that we can move around in it

617
00:49:34,360 --> 00:49:39,440
In various ways and so more or less factive representations of the world out there

618
00:49:41,120 --> 00:49:47,400
And I hope Blaise's ears are ringing right now because he basically went up yesterday and said, you know

619
00:49:47,560 --> 00:49:52,280
Maximum entropy first in all these people, you know, a new way of putting these kind of very epistemic

620
00:49:53,080 --> 00:49:55,400
Conceptions of mind is that it's all about prediction

621
00:49:55,920 --> 00:50:01,840
You know, the mind is a prediction machine and all we do is make predictions about what's out there in the world

622
00:50:01,840 --> 00:50:06,920
Then we update them. So but again, it's it's essentially about knowing in some lowercase case

623
00:50:07,400 --> 00:50:09,640
Sense of what the mind is all about

624
00:50:10,320 --> 00:50:11,880
And I really disagree with that

625
00:50:11,880 --> 00:50:18,120
I think the thing that we learn from the reward hypothesis and kind of the reinforcement learning framework that follows from it is

626
00:50:19,120 --> 00:50:23,000
That the mind not only performs computations over

627
00:50:23,480 --> 00:50:27,720
Representations of descriptive matters of fact, but the mind also

628
00:50:28,720 --> 00:50:33,520
Fundamentally performs computations over those representations as better or worse

629
00:50:34,280 --> 00:50:36,280
So we are continually

630
00:50:37,120 --> 00:50:41,160
Representing the world as we move around in it, you know what this room looks like

631
00:50:41,880 --> 00:50:44,000
What day of the week it is whether it's raining or not

632
00:50:44,160 --> 00:50:51,360
But as part and parcel of that process, we are also continually representing all of these states as

633
00:50:51,920 --> 00:50:54,600
Better or worse with respect to our goals

634
00:50:54,800 --> 00:51:00,360
So what we're doing is we're laying over these fabrics kind of of the states and assigning

635
00:51:01,120 --> 00:51:07,360
Value in the way that rich just described and so we're not just seeing whether it's raining, but oh, it's raining

636
00:51:07,360 --> 00:51:10,800
Here's what that means for me or oh, it's raining. I live in London, of course, it's raining, right?

637
00:51:10,800 --> 00:51:15,320
These things can matter more or less, but that we're not just experiencing the world. We're

638
00:51:16,280 --> 00:51:22,040
Continually evaluating and so I think one of the things that the report hypothesis and

639
00:51:22,560 --> 00:51:28,080
Tells us is that the mind is not just a thinking machine. It's a valuation machine. It's continually

640
00:51:28,880 --> 00:51:30,400
evaluating

641
00:51:30,400 --> 00:51:35,840
Now one of the examples that I like to use for this is kind of the thin end of the wedge. It's very narrow case

642
00:51:36,440 --> 00:51:38,960
But it's the phenomenon of vernacular robbery. So

643
00:51:39,720 --> 00:51:44,440
vernacular rivalry occurs when you place some kind of division between the left eye and the right eye

644
00:51:45,200 --> 00:51:51,880
And you show one stimulus to one eye and another stimulus to the other eye. So in this case borrowed from

645
00:51:53,360 --> 00:51:57,040
Feldman Barrett's lab is one eye sees one eye is presented

646
00:51:57,040 --> 00:51:59,760
I should say with a house and one side is

647
00:52:00,240 --> 00:52:02,720
presented with a face and what you might expect

648
00:52:03,280 --> 00:52:05,720
The participants to see is you know house face

649
00:52:06,040 --> 00:52:11,480
But the reason vernacular rivalry is interesting is because what they actually perceive what they actually experience is an alternation

650
00:52:11,760 --> 00:52:15,480
They see a house they see a face they see a house and they see a face and

651
00:52:16,240 --> 00:52:19,880
obviously for you know, the view that the mind is

652
00:52:21,200 --> 00:52:24,800
Engaging with the world that the mind is computer that we're making prediction machines

653
00:52:24,800 --> 00:52:28,440
This is a very interesting case because we know for a fact that we are not perceiving

654
00:52:29,320 --> 00:52:32,120
What we are essentially seeing right the experience

655
00:52:32,640 --> 00:52:34,640
Strictly departs from

656
00:52:35,440 --> 00:52:37,440
what we know is being presented and

657
00:52:37,920 --> 00:52:43,480
This was a very important test case for predictive processing and first in like folks because they gave a very elegant

658
00:52:44,360 --> 00:52:45,600
Explanation of what's going on?

659
00:52:45,600 --> 00:52:53,440
So they basically gave a Bayesian interpretation of predictive of vernacular rivalry and said that roughly what happens is that we have very low

660
00:52:53,480 --> 00:52:56,520
Priors for seeing a house face in the natural world

661
00:52:56,520 --> 00:53:00,160
And so what the mind does is it says well it can't be a house face

662
00:53:00,240 --> 00:53:02,560
So it must be a ha it must be a house

663
00:53:03,000 --> 00:53:06,200
But then the prediction error comes back up and says normal

664
00:53:06,200 --> 00:53:09,080
You're only accounting for half of what you're experiencing here

665
00:53:09,080 --> 00:53:13,560
So it must be the other stimulus and it switches and so it's saying okay now it's a face

666
00:53:14,560 --> 00:53:16,760
Again a strong prediction error and an ultrace

667
00:53:17,760 --> 00:53:20,960
And what you have here is you know perceptual dominance

668
00:53:20,960 --> 00:53:26,380
So you might see the face first or you might see the face for longer or it might alternate more

669
00:53:26,720 --> 00:53:30,440
Okay, the mind is a thinking machine. It's epistemic great

670
00:53:30,560 --> 00:53:35,760
We have a very you know epistemically normatively informed explanation of what's going on there

671
00:53:36,240 --> 00:53:41,400
The reason I like to use vernacular rivalry as a case for the evaluative mind

672
00:53:41,400 --> 00:53:45,320
Is that when you subscribe to something like the reward hypothesis as I did?

673
00:53:46,080 --> 00:53:52,280
Let's say in 2017 when I was presented with this is that I thought you know, I bet you reward will modulate

674
00:53:52,880 --> 00:53:59,560
This phenomenon so it allows you to make predictions even from the armchair and I bet you reward will

675
00:54:00,720 --> 00:54:05,040
Modulate the experience vernacular rivalry and that is in fact what you find so if you

676
00:54:05,560 --> 00:54:12,320
Reward a certain stimulus like the face so you say hey, I'm gonna give you a penny every time the face appears

677
00:54:12,800 --> 00:54:14,800
We have perceptual dominance

678
00:54:15,320 --> 00:54:21,560
Of the face but it gets better than that you can also reward just the percept so people don't cheat basically and

679
00:54:21,840 --> 00:54:25,120
Every time they report seeing a face you can do like a pitching

680
00:54:25,960 --> 00:54:31,240
And again the face will be more dominant. You'll have perceptual dominance

681
00:54:32,840 --> 00:54:34,840
First in folks when you email them

682
00:54:36,040 --> 00:54:40,920
They're like yeah, but it could just be information right the reward is an added piece of information

683
00:54:40,920 --> 00:54:45,520
So you can put that into kind of your Bayesian function and that can still explain

684
00:54:46,080 --> 00:54:51,440
Why this perceptual dominance is occurring, but what you can do is you can have a

685
00:54:51,880 --> 00:54:55,760
Punished percept so every time they see a house you can make a little sound that says

686
00:54:56,680 --> 00:54:59,000
You know like you're sad

687
00:54:59,000 --> 00:55:04,160
And if it's just information you should have perceptual dominance of the house

688
00:55:04,680 --> 00:55:08,080
That's not what you find you actually still have perceptual dominance

689
00:55:08,400 --> 00:55:15,320
Of the non-punished percept of the face so really what this tells you and this might glamorize the finding a bit

690
00:55:16,840 --> 00:55:18,520
You want me to like

691
00:55:18,520 --> 00:55:22,000
Animate less. Yeah, no, I know I know I know it's okay

692
00:55:22,880 --> 00:55:24,280
I'll try

693
00:55:24,280 --> 00:55:26,280
Yes, sorry

694
00:55:26,400 --> 00:55:28,400
I will hold on to the podium

695
00:55:29,440 --> 00:55:37,800
This is a small finding but it tells you basically that to some very limited non-exciting degree you see you perceive

696
00:55:39,080 --> 00:55:46,060
What it is in your interest with respect to your goals to perceive right again, this is a very thin case is very narrow case

697
00:55:46,760 --> 00:55:48,760
But it tells you that you don't just

698
00:55:49,560 --> 00:55:52,680
Perceive the world we perceive the world conditional on our goals

699
00:55:52,680 --> 00:55:55,880
Which in this case is to make a little bit of funny in these studies, right?

700
00:55:56,360 --> 00:56:00,080
Binocular rivalry is a tiny case, but if you look at all the research

701
00:56:00,760 --> 00:56:04,080
That kind of riches alluded to over the course of the last several decades

702
00:56:04,520 --> 00:56:09,440
We find that every stage of mental processing from sensation

703
00:56:10,680 --> 00:56:17,680
And computation to action at every level of description and mental processing from the sub personal to the personal

704
00:56:19,040 --> 00:56:22,720
It's conditional on these attributions of reward and value

705
00:56:22,720 --> 00:56:26,960
So we sense we perceive and we attend to the features of our environment

706
00:56:27,200 --> 00:56:32,900
conditional on reward and value we remember and remember to remember remember

707
00:56:33,080 --> 00:56:35,080
prospectively conditional on

708
00:56:35,420 --> 00:56:37,420
the attribution of

709
00:56:37,580 --> 00:56:42,260
Reward and value our cognitive control our ability to decide choose

710
00:56:42,660 --> 00:56:48,900
Plan our future actions in each case you will find a body of evidence that's just it's just like in binocular rivalry

711
00:56:49,340 --> 00:56:52,620
It is conditional on the attribution of reward so

712
00:56:54,260 --> 00:56:58,900
We have this kind of classic sorry picture of the mind as a thinking machine

713
00:56:59,340 --> 00:57:03,180
I think that's part of the story, but the mind is also

714
00:57:03,860 --> 00:57:09,460
continually in the business of evaluating all of these parts of the so-called kind of cognitive sandwich

715
00:57:10,140 --> 00:57:15,740
So I want to make some kind of caveats here. First of all, it's an additive thesis

716
00:57:16,460 --> 00:57:18,460
What I want to say is that the mind

717
00:57:18,900 --> 00:57:26,500
computes over these representations of matter-of-fact and it continually evaluates them now for some people that might seem kind of weak sauce

718
00:57:26,500 --> 00:57:30,340
Particularly, I think philosophers sometimes want to come out swinging and say, you know, this is everything

719
00:57:30,340 --> 00:57:35,580
I call that the stronger thesis the stronger thesis something like what the mind is fundamentally in the business of doing is

720
00:57:35,740 --> 00:57:41,660
Evaluating things is better or worse, right? That's that's mainly what the mind is about. I don't want to go that far

721
00:57:41,660 --> 00:57:47,460
We can talk about that in the discussion of why not I want to say that the mind is fundamentally in the business of doing both

722
00:57:48,140 --> 00:57:51,700
So I'm making an additive claim. I don't subscribe to the stronger thesis

723
00:57:51,700 --> 00:57:55,740
Although I do think it's kind of fun and it it does kind of really drive a lot of nice research

724
00:57:56,740 --> 00:58:00,500
And I also don't think this is exactly the same thing as saying reward is enough

725
00:58:01,620 --> 00:58:05,580
My claim is about the nature of the mind and what the mind is in the business of doing

726
00:58:05,580 --> 00:58:08,580
I take reward is enough on one interpretation to be saying

727
00:58:09,140 --> 00:58:15,380
What's needed in order to produce intelligent behavior? Those are obviously cousins as positions

728
00:58:16,140 --> 00:58:18,140
But I think reward is enough again

729
00:58:18,460 --> 00:58:24,460
Make some stronger claims than than than what I would subscribe to but that's definitely something that we can talk to you

730
00:58:24,540 --> 00:58:30,420
But I think we need to start thinking of the mind as evaluative in these ways not just because that's what we're doing

731
00:58:30,620 --> 00:58:36,460
But also if we want to design artificial intelligence, that is a very different picture of what we're trying to build

732
00:58:37,340 --> 00:58:41,740
And we'll also go towards addressing some of the kinds of challenges that we heard about yesterday

733
00:58:41,900 --> 00:58:46,860
So the first question was does the reward hypothesis provide a good model for understanding the mind?

734
00:58:46,900 --> 00:58:49,700
Yeah, I really think it does and the answer to that is yes

735
00:58:49,700 --> 00:58:54,580
And I think it changes our understanding of what the mind is kind of in the business of doing

736
00:58:55,780 --> 00:59:02,260
The second question is you know, how far does the reward hypothesis go as you might imagine I'm gonna say something like pretty far

737
00:59:02,660 --> 00:59:05,980
So I gave you a list of kind of all the the kinds of

738
00:59:06,620 --> 00:59:13,700
Processes that reward and value are are implicated in and you can kind of see that a lot of these are sort of you know

739
00:59:13,700 --> 00:59:15,700
So called low-level cognitive capacities

740
00:59:16,220 --> 00:59:22,780
But all the way kind of extending into sort of high level cognitive capacities that people might be be interested in and so

741
00:59:22,780 --> 00:59:27,660
I don't want to go into that too much, but I want to focus on what you might think as one of the kind of

742
00:59:29,100 --> 00:59:33,820
Crown jewels of the human mind, right, which is our ability to have moral experiences

743
00:59:34,340 --> 00:59:38,700
If there's anything that we might want to think is special, but how the mind works

744
00:59:39,260 --> 00:59:41,020
One would be language

745
00:59:41,020 --> 00:59:43,020
I can talk about that after yesterday

746
00:59:43,820 --> 00:59:48,460
But another one might be our ability to have moral experiences, so when I say moral cognition

747
00:59:48,460 --> 00:59:50,500
I mean something quite straight forward

748
00:59:50,500 --> 00:59:56,900
It's the capacity to create and respond to situations of moral significance. So let me give you an example. It's nothing fancy

749
00:59:57,780 --> 00:59:59,780
You know my sister Barbara

750
01:00:00,140 --> 01:00:05,140
Goes to buy a coffee at a coffee shop and she can make the following decision

751
01:00:05,140 --> 01:00:09,500
She can buy the more expensive fair-trade coffee or she can buy the cheaper

752
01:00:10,260 --> 01:00:14,420
Commercially sourced coffee, right and she can stand in line. She can make that decision and she

753
01:00:15,020 --> 01:00:18,660
Realistically probably past the cheaper commercially sourced coffee. Sorry Barbara

754
01:00:19,540 --> 01:00:24,420
But that is an example of moral cognition. She's not making a decision about a trolley problem or anything like that

755
01:00:24,420 --> 01:00:31,420
These are the everyday routine moral experiences that we have that we make like this and that's the kind of thing that I have in mind

756
01:00:31,420 --> 01:00:33,420
When I'm talking about moral cognition

757
01:00:33,820 --> 01:00:36,700
So again, they're kind of quite

758
01:00:39,700 --> 01:00:41,060
well

759
01:00:41,060 --> 01:00:45,660
established received views on kind of how our moral cognition

760
01:00:47,300 --> 01:00:49,980
Works what the mechanisms behind that are and so

761
01:00:50,660 --> 01:00:56,100
Again, this this isn't actually even too much of a cartoon. I think we have kind of these rationalist inferentialist

762
01:00:56,780 --> 01:00:59,140
Views where the idea is that Barbara

763
01:00:59,460 --> 01:01:06,180
Does or doesn't buy the fair-trade coffee based on the belief or set of beliefs that something is the right thing to do

764
01:01:06,180 --> 01:01:09,420
And then it will kind of follow from you know her knowledge

765
01:01:09,540 --> 01:01:11,540
about a particular situation

766
01:01:11,660 --> 01:01:13,660
On the other end of that dichotomy

767
01:01:13,660 --> 01:01:16,540
There's this idea that Barbara might do or not do the right thing

768
01:01:17,100 --> 01:01:18,980
Because of her moral emotions

769
01:01:18,980 --> 01:01:22,260
She might feel empathy towards the workers or not

770
01:01:22,420 --> 01:01:27,140
So an agent has the fitting moral emotion or emotions that something is the right thing to do

771
01:01:27,380 --> 01:01:32,020
And then despite the kind of distribution of the literature actually I think a lot of mainstream views

772
01:01:32,340 --> 01:01:38,220
Are hybrid views some combination of the views that you know certain things that you have certain kinds of beliefs

773
01:01:38,220 --> 01:01:42,580
You have the relevant beliefs. You also have the accompanying emotions that might motivate that behavior

774
01:01:43,260 --> 01:01:49,140
And we should definitely talk about that Plato slide, but without going all the way back to you Plato

775
01:01:49,140 --> 01:01:51,980
I think these have been kind of the main

776
01:01:52,900 --> 01:01:57,140
Alternatives in understanding moral psychology and understanding our moral cognition. I

777
01:01:58,300 --> 01:02:03,060
Think the reward hypothesis suggests that there's a kind of a new player on the scene

778
01:02:03,740 --> 01:02:10,380
And that moral cognition is in fact constituted by the sub personal attribution of goal and context

779
01:02:10,580 --> 01:02:15,580
Dependent subjective reward value. So again that familiar phrase that we see

780
01:02:16,420 --> 01:02:25,740
What we need to see in binocular rivalry is actually one important ingredient that mechanism is also driving our experiences of saying something like

781
01:02:25,900 --> 01:02:31,760
That's just wrong. That's morally disgusting. I really ought to be doing this thing

782
01:02:32,040 --> 01:02:38,600
Again in the descriptive cases, but what I am suggesting is that we actually recruit the same reward mechanisms

783
01:02:39,000 --> 01:02:45,980
In our moral cognitive experiences now I'll flush that out again because that's a bit strong and a bit strong stuff

784
01:02:46,120 --> 01:02:48,440
But what I want to suggest is that

785
01:02:49,320 --> 01:02:53,080
There is quite a lot of flattening going on here, right? So in fact

786
01:02:53,960 --> 01:02:55,960
When you are experiencing

787
01:02:56,280 --> 01:03:04,160
Something as right or wrong you are attributing reward and value or laying that fabric in just the same way in a moral

788
01:03:04,280 --> 01:03:10,600
Context with moral determinants like something like fairness something like honesty as you are when you are just choosing between

789
01:03:11,240 --> 01:03:13,240
You know left and right to your coffee

790
01:03:13,640 --> 01:03:18,920
So that's to say that if I'm choosing between to your coffee or coffee and fair-trade coffee

791
01:03:18,920 --> 01:03:22,960
I'm actually recruiting some of the same mechanisms now again

792
01:03:22,960 --> 01:03:25,560
There will be caveats that come to kind of flesh out this picture

793
01:03:25,800 --> 01:03:32,880
But this is an important flattening of our understanding of what's going on in our moral psychology in our moral cognitive experiences

794
01:03:33,920 --> 01:03:38,200
And you might think okay, but I really feel strongly about certain

795
01:03:38,720 --> 01:03:41,600
You know moral propositions, and I don't feel particularly strongly

796
01:03:42,400 --> 01:03:45,360
about to your coffee or something like that and

797
01:03:45,560 --> 01:03:52,840
Sort of the second thesis of this view is that reward and value the strength of the reward and value are actually the source of our

798
01:03:52,960 --> 01:03:57,440
Moral motivational force so through our evolutionary history and through our upbringing

799
01:03:58,320 --> 01:04:00,000
certain things are

800
01:04:00,000 --> 01:04:01,240
reinforced

801
01:04:01,240 --> 01:04:05,920
To be extremely important to be absolute no-knows or absolute musts

802
01:04:06,360 --> 01:04:12,680
And that these are the sources of why we sometimes feel that something is right or wrong and why we experience things

803
01:04:13,640 --> 01:04:17,520
Something we really ought to do again. This is not a normative on but it's the feeling of I oh

804
01:04:17,560 --> 01:04:20,400
I really should call my grandmother or something like that

805
01:04:20,400 --> 01:04:26,280
It's because the driving mechanism here is again reward and the value not my emotions not my

806
01:04:26,760 --> 01:04:29,760
Knowledge though again, I'll build a kind of more complete puzzle there

807
01:04:29,760 --> 01:04:36,560
And so this is kind of one of the things that fundamentally differentiates between something like our social and our moral values

808
01:04:36,560 --> 01:04:39,080
Right, we know we shouldn't wear white after Labor Day

809
01:04:39,600 --> 01:04:41,800
Or we did know that once when I was like five

810
01:04:41,840 --> 01:04:47,600
That has some force, but it has much different force from you know

811
01:04:47,600 --> 01:04:53,200
It's wrong to lie and that is I think a function of the strength of the value that has been

812
01:04:53,720 --> 01:04:57,240
Attributed to that over the course of our lifetime experiences our

813
01:04:57,680 --> 01:05:00,600
Education's our philosophical conversations and things like that

814
01:05:00,600 --> 01:05:04,120
So it's a source of the strength of the moral motivational force

815
01:05:05,360 --> 01:05:09,480
Now this is pretty quick and again, I'm gonna flush out the picture in a second, but I think

816
01:05:10,160 --> 01:05:16,400
This kind of you starts to account for features of our moral psychology that the kind of rationalist

817
01:05:16,560 --> 01:05:23,120
Inferentialist views and the sentimentalist views don't account for so it doesn't it it accounts for something like the multidimensional nature

818
01:05:23,680 --> 01:05:28,760
It's hard to explain the Barbara fair-trade coffee case with just the emotions, right?

819
01:05:28,760 --> 01:05:33,880
There's lots of features that need to be weighed off there that a kind of complex

820
01:05:34,520 --> 01:05:40,920
Waiting of our values allows us to explain explains why sometimes she buys fair-trade coffee and sometimes she doesn't

821
01:05:41,200 --> 01:05:45,280
Why we have cross cultural values, so we have themes in our values

822
01:05:45,280 --> 01:05:51,980
So all communities, you know value honesty, but what it means to be honest or what it means to murder or what counts as

823
01:05:52,480 --> 01:05:56,240
incest is going to vary in us because we share some of these

824
01:05:57,040 --> 01:06:03,200
Mechanisms, but how we tune them up is going to really depend on the community and the culture same with

825
01:06:03,440 --> 01:06:05,440
moral learning

826
01:06:05,440 --> 01:06:07,440
It's going to

827
01:06:07,800 --> 01:06:15,080
Allow us to understand kind of what counts as morally exceptional, right where people stand up against all kinds of

828
01:06:15,680 --> 01:06:17,880
opposition and costs and

829
01:06:18,400 --> 01:06:23,480
But we're going to kind of place a very high value on the certain principle and stand by that principle

830
01:06:24,000 --> 01:06:26,240
Through that strong moral motivational force

831
01:06:27,080 --> 01:06:29,580
and I think kind of one of the

832
01:06:29,900 --> 01:06:34,900
The more complex but also more compelling kind of pieces of evidence for a view like this is

833
01:06:35,300 --> 01:06:39,420
What happens when we have failures in our moral cognition make errors?

834
01:06:39,420 --> 01:06:43,420
We have dysfunctions and sometimes we have pathologies in our moral cognition

835
01:06:43,420 --> 01:06:46,860
And one of the interesting things is that these seem to rise and fall

836
01:06:47,300 --> 01:06:54,860
With dysfunctions in our reward systems in our reward mechanisms in ways that we wouldn't necessarily expect to see otherwise, so I

837
01:06:56,460 --> 01:06:58,460
think

838
01:06:59,580 --> 01:07:03,580
I do want to specify that I think it's a part of the puzzle again

839
01:07:03,580 --> 01:07:07,660
It's going to be an inclusive view in the same way that the evaluative mind is an inclusive view

840
01:07:07,660 --> 01:07:12,020
That's not to say that we don't have any place for the emotions

841
01:07:12,660 --> 01:07:18,140
In our moral cognitive experiences. It doesn't mean that we don't reason and read philosophy and things like this

842
01:07:18,420 --> 01:07:22,340
But when you're standing in the coffee shop and you're trying to decide whether you're making

843
01:07:23,220 --> 01:07:25,460
Whether you're going to buy the fair-trade coffee or not

844
01:07:26,460 --> 01:07:31,620
What you're recruiting there are your valuation systems and your life-long experiences

845
01:07:31,900 --> 01:07:36,540
Of what you value and what we attribute our values over now

846
01:07:36,540 --> 01:07:43,060
I think one key open question here is what do we attribute those rewards and values over is it to states?

847
01:07:43,060 --> 01:07:46,500
Is it to state action pairs? Is it to abstract ideas?

848
01:07:46,500 --> 01:07:53,140
Is it possible that philosophers for example have learned to attribute a high amount of value just to the concept of

849
01:07:53,420 --> 01:07:58,920
Justice and the existence of justice in the world, right? I think these are important questions of how exactly it plays out

850
01:08:00,500 --> 01:08:04,020
But I think and here I will take a dig actually at that Plato slide

851
01:08:05,020 --> 01:08:07,500
It's not about pleasure and pain actually

852
01:08:07,500 --> 01:08:11,860
I think pleasure so I follow Kent barrage here and distinguishing between pleasure and pain and

853
01:08:12,260 --> 01:08:20,220
Reward and punishment or reward and and disappointment and I think our moral cognitive theories have been based on what we can introspect

854
01:08:20,220 --> 01:08:27,100
Oh, I think this oh, I feel this and we don't necessarily have introspective access to our reward mechanisms

855
01:08:27,340 --> 01:08:29,460
And so I think actually in a way

856
01:08:29,460 --> 01:08:35,620
It's a disservice to try and tie it back to hedonism because it's rewarded something different here that hasn't been on the scene

857
01:08:36,020 --> 01:08:42,140
But I think is a driving factor in this much more complex architecture of our of our moral cognition

858
01:08:43,020 --> 01:08:45,500
So how far does it go? It goes pretty far

859
01:08:45,500 --> 01:08:51,700
It goes to certain things that I think we thought, you know, we're a little bit untouchable in our human experience

860
01:08:52,260 --> 01:08:57,420
And again, I'm just touch on this and we can talk about it in the discussion, but I think it really informs

861
01:08:58,980 --> 01:09:00,820
How we understand

862
01:09:00,820 --> 01:09:06,220
Moral artificial intelligence because this is actually quite tractable. This is actually quite quantifiable

863
01:09:06,620 --> 01:09:11,700
We can actually go much further in understanding what works and what doesn't work in our moral cognition

864
01:09:12,420 --> 01:09:14,420
Because we have this nice

865
01:09:14,860 --> 01:09:21,380
Quantifiable computational theory and we can go much farther and understanding our moral cognitive experiences

866
01:09:21,740 --> 01:09:27,660
And in designing those right so morality on a sore architecture is gonna suck

867
01:09:28,500 --> 01:09:32,020
But it doesn't need to suck because that's actually not how it works in us either

868
01:09:32,460 --> 01:09:39,220
And so I think having a handle on this opens some pretty interesting avenues even in the realm of artificial intelligence

869
01:09:40,180 --> 01:09:42,700
So lastly, and this will be the fastest part

870
01:09:43,020 --> 01:09:48,940
But the third question should it should this understanding then guide our normative decision-making and here

871
01:09:49,340 --> 01:09:54,660
It'll definitely depart have from from rich. I think my answer straightforward Lee

872
01:09:55,300 --> 01:09:58,060
That got formatted out of existence. Sorry

873
01:09:58,780 --> 01:10:02,140
Is no so I take guide to be pretty strong

874
01:10:02,780 --> 01:10:06,660
Guide means I should then use this to govern my normative decision-making

875
01:10:07,180 --> 01:10:13,900
And the analogy that I often draw here is that moral cognition should be understood in analogy to folk physics

876
01:10:14,620 --> 01:10:21,300
So, you know the Wiley Coyote case where Wiley runs off the cliff and hangs out there for a second and then falls right or

877
01:10:21,300 --> 01:10:26,100
Other cases might be Muller liar illusions or putting a straw in a cup and things like that

878
01:10:27,100 --> 01:10:35,460
We have understandings of physics that are produced by our sensations and our perceptions that we know to be incorrect and

879
01:10:36,780 --> 01:10:40,940
If we rely on them, we will not land on the moon and we will

880
01:10:41,300 --> 01:10:49,340
You know not make other scientific achievements, and I think we have to understand moral cognition as the equivalent of folk physics

881
01:10:50,180 --> 01:10:55,260
It's full of errors now does that mean that the reward hypothesis is not useful

882
01:10:55,460 --> 01:10:58,660
No, because I actually think it's really important to understand

883
01:10:59,300 --> 01:11:05,060
Where our moral cognition reliably and in fact systemically falls down, right?

884
01:11:05,900 --> 01:11:09,980
So this is have basically reduced to a bumper sticker. I used to walk past

885
01:11:10,580 --> 01:11:16,500
Basically every day when I was doing my PhD, which says don't believe everything you think, right? It's you've all seen it

886
01:11:16,500 --> 01:11:24,620
It's right, but actually that's basically exactly what this is is we have very strong moral experiences hatred judgment

887
01:11:25,300 --> 01:11:27,300
empathy

888
01:11:28,020 --> 01:11:34,980
But we shouldn't believe all of those right but what the reward hypothesis and what reinforcement learning along with many other

889
01:11:35,140 --> 01:11:41,420
Contributions allow us to do is understand the mechanisms that generate those things that we experience is right or wrong

890
01:11:41,420 --> 01:11:45,420
And I do think that's powerful. I wouldn't use it to guide normative decision-making

891
01:11:45,460 --> 01:11:47,900
I wouldn't say okay. Well that I guess that's what we're doing

892
01:11:48,180 --> 01:11:51,540
But I think we can use it to inform it in the following way

893
01:11:51,540 --> 01:11:55,900
So one of the things that I have suggested is that we should have a kind of fault line approach

894
01:11:55,900 --> 01:11:59,980
So when I say fault line, I mean something like tectonic plate movement, right?

895
01:11:59,980 --> 01:12:01,900
If you understand the movement of tectonic plates

896
01:12:01,900 --> 01:12:07,900
You can make nice predictions about where earthquakes and volcanoes are going to occur where things are gonna kind of continually erupt

897
01:12:07,900 --> 01:12:12,540
And I think if you can understand the contours of our moral cognitive decision-making

898
01:12:13,460 --> 01:12:16,260
You can also start to see sort of the fault lines

899
01:12:16,260 --> 01:12:19,900
We're not just gonna make token errors in our moral cognitive judgments

900
01:12:19,900 --> 01:12:23,180
We're gonna make type errors in our moral cognitive judgments

901
01:12:23,180 --> 01:12:26,380
and I think we can use that to inform then

902
01:12:27,180 --> 01:12:30,340
The kinds of things because I think in in sort of everyday life

903
01:12:30,420 --> 01:12:36,180
We really do take our moral cognitive experiences at face value with one another and I think that's a mistake

904
01:12:36,260 --> 01:12:41,500
Whereas if we have a kind of causal understanding of what generally sees and where they might depart

905
01:12:42,100 --> 01:12:45,100
Then that can be used as a kind of type remediation

906
01:12:45,580 --> 01:12:49,860
Of what's going on not necessarily in a policy sense or anything like that

907
01:12:49,860 --> 01:12:54,820
But I certainly think you know, it's useful knowledge to have it can inform us for the better

908
01:12:54,820 --> 01:12:58,780
I think it's good to know kind of the contours of the mistakes that you're making

909
01:12:59,380 --> 01:13:03,660
But I wouldn't say guide because of the way these things are generated

910
01:13:03,660 --> 01:13:06,540
So I think that might be a place where we depart

911
01:13:08,500 --> 01:13:10,500
So yeah, I think it's pretty productive

912
01:13:12,020 --> 01:13:22,180
Thanks, Julia and and rich that's just terrific. I'm so glad I got you both on to this conversation

913
01:13:23,220 --> 01:13:26,980
And and I I have a couple of questions to get us started, but also

914
01:13:27,580 --> 01:13:31,420
Ready to sort of follow with different directions. You'd like to head

915
01:13:31,420 --> 01:13:36,340
I heard two things perhaps that it might be worth exploring one was pleasure and pain

916
01:13:37,340 --> 01:13:40,380
And rich I don't know if you want to respond to that one and the second one

917
01:13:40,380 --> 01:13:46,100
Oh, which I'll come back to is is the guiding normative decision-making, but let's take on pleasure and pain

918
01:13:46,100 --> 01:13:53,500
Is that an important part of how you're thinking about this to me the pleasure and pain are just examples of what reward might be

919
01:13:53,500 --> 01:13:59,380
Well, I think it was the play-doh slide. Yeah. Yeah. Yeah. Yeah. Yeah, he was clearly talking about pleasure and pain

920
01:14:00,820 --> 01:14:04,060
Reward could be pleasure payment. It's undoubtedly more complicated than that

921
01:14:07,340 --> 01:14:09,340
And

922
01:14:10,180 --> 01:14:16,100
I agree with you that we don't know what our word signal is we we don't have

923
01:14:17,340 --> 01:14:18,260
introspection

924
01:14:18,260 --> 01:14:20,260
introspective access to it

925
01:14:22,260 --> 01:14:24,260
And and more than that

926
01:14:25,780 --> 01:14:29,220
What we do have an introspective access to it is the

927
01:14:29,380 --> 01:14:31,380
I

928
01:14:32,980 --> 01:14:37,340
Want to call it TD error remember it's the thing I was talking about is reinforcement

929
01:14:37,980 --> 01:14:44,140
It's which is which is this measure of how do we feel about how well things are going this as you're saying the

930
01:14:44,740 --> 01:14:48,700
Constant sense of our things getting better or worse. We evaluating all the time

931
01:14:49,820 --> 01:14:54,220
So we have access to our evaluations and we we have very

932
01:14:55,220 --> 01:15:00,900
Preeminent access or prominent access to our things getting better or worse

933
01:15:02,060 --> 01:15:04,380
But things getting better and worse

934
01:15:05,100 --> 01:15:09,300
if you remember it's the reward plus the change in your evaluation function and

935
01:15:09,820 --> 01:15:15,700
So these two things the reward and the change in your evaluation these two are mixed together

936
01:15:15,700 --> 01:15:21,740
And so when you feel good or feel bad, you don't know if it's you don't really know if it's because the reward was high

937
01:15:21,740 --> 01:15:23,740
Or the change in evaluation was high

938
01:15:24,460 --> 01:15:25,740
and so

939
01:15:25,740 --> 01:15:29,780
So that is a confusion that we all have we don't know

940
01:15:31,180 --> 01:15:37,220
Once so once an evaluation a value function becomes very well established then, you know, it feels

941
01:15:37,820 --> 01:15:45,980
Bad when our values are violated just as if we had you know a form of pain or a bad thing directly. So

942
01:15:47,860 --> 01:15:51,100
This is this is a major phenomenon, you know in terms of our

943
01:15:51,620 --> 01:15:54,220
subjective experience that it's hard to tell

944
01:15:55,540 --> 01:15:57,540
Difficulties in a reward and a change of value

945
01:16:03,860 --> 01:16:05,860
Doing anything

946
01:16:07,820 --> 01:16:09,820
Yeah, I would say that and again this is

947
01:16:10,340 --> 01:16:16,860
Following Kent Barrage. I think pleasure and pain are sick, you know are evolved biological signals of

948
01:16:17,740 --> 01:16:19,740
reward and value and

949
01:16:20,260 --> 01:16:25,060
As signals, they're pretty good, which means they go together a lot of the time

950
01:16:25,060 --> 01:16:30,500
I think one of the cases that the barrage talks about is the case of for example a digger

951
01:16:30,500 --> 01:16:35,260
This might be David Reddish talking about cases of addiction to heroin which where the signals

952
01:16:35,940 --> 01:16:38,700
and the reward and disappointment come apart so

953
01:16:40,820 --> 01:16:44,460
You very clearly start to have not pleasurable experiences

954
01:16:45,380 --> 01:16:48,780
When you will become a heroin addict, but the reward is so strong

955
01:16:49,740 --> 01:16:52,500
That you continue to do it right heroin is basically ruining your life

956
01:16:53,020 --> 01:16:57,980
But because it's hijacked the reward mechanisms and you continue. So these are kind of edge cases

957
01:16:58,900 --> 01:17:01,740
but I mean, I think one of the powers of

958
01:17:02,580 --> 01:17:08,500
Like the reward hypothesis, but also the scientific process and that discovery that you're talking about with Peter Diane and others

959
01:17:08,500 --> 01:17:11,000
Is that it really gets at an important part?

960
01:17:11,620 --> 01:17:16,180
Of of our kind of mechanistic composition that we don't have direct

961
01:17:17,060 --> 01:17:18,340
introspective access to you

962
01:17:18,340 --> 01:17:24,940
But where there are departures between what a hedonistic or pleasure and pain-based view would predict about us?

963
01:17:25,260 --> 01:17:32,020
And what the reward view would predict about us and and clearly the reward view makes better predictions is better supported by the evidence

964
01:17:32,540 --> 01:17:33,860
and so

965
01:17:33,860 --> 01:17:35,420
It's kind of funny saying this to you

966
01:17:35,420 --> 01:17:40,860
But I would say like there's no reason to lean into the pleasure and pain because like reward actually provides a

967
01:17:41,180 --> 01:17:45,420
Better model in some cases, even though they you know, they often walk together

968
01:17:49,340 --> 01:17:52,980
So you're thinking it's important to keep the distinction

969
01:18:01,300 --> 01:18:03,300
I mean the reward

970
01:18:03,860 --> 01:18:05,860
way of talking is more abstract and

971
01:18:06,540 --> 01:18:08,540
And

972
01:18:08,860 --> 01:18:11,620
If it was equated with pain

973
01:18:12,940 --> 01:18:16,180
Pain and pleasure then it becomes more specific and

974
01:18:17,580 --> 01:18:21,100
And that they're there for falsifiable

975
01:18:23,220 --> 01:18:25,220
Which is good

976
01:18:26,420 --> 01:18:32,580
Yeah, so I'm totally on board of course with it's a general thing it doesn't have to be pain and pleasure and there may be cases I

977
01:18:33,220 --> 01:18:34,660
mean, I think I

978
01:18:34,660 --> 01:18:39,060
Guess that totally makes sense to me like there may be pain and then maybe you could

979
01:18:43,460 --> 01:18:50,880
Change so that the pain is less important to you or the pleasure is less important. Yeah, it's just more general

980
01:18:52,140 --> 01:18:53,580
The reward is more general

981
01:18:53,580 --> 01:18:58,860
I think another example might be the emotions where again we have introspective access to our emotions

982
01:18:59,180 --> 01:19:04,180
And in that very hastily shown chart, right? I think there's incredibly close coupling

983
01:19:04,660 --> 01:19:10,860
Between our affective responses fear anger and so on and the valuation component

984
01:19:11,380 --> 01:19:18,020
But you might ask yourself, you know, how do you know I lost that between the the emotions and and the

985
01:19:18,940 --> 01:19:24,420
Valuations, yeah, and so if you are you have a component theory of affect or emotion

986
01:19:24,420 --> 01:19:27,300
You say that there's an appraisal component of the emotion, right?

987
01:19:27,620 --> 01:19:33,260
What triggers certain emotions? Well on my view would be something like the valuation system the reward system

988
01:19:34,020 --> 01:19:40,060
But I think you lose a lot if you only look at the introspective component. You say it's all emotion

989
01:19:40,860 --> 01:19:42,380
Emotion is important

990
01:19:42,380 --> 01:19:49,900
But actually the emotions are kicked off by the valuation or mechanism by the valuation assessment that then triggers these

991
01:19:50,660 --> 01:19:57,020
ballistic responses like empathy or fear or anger or so on so I think again, they're they're very close

992
01:19:57,020 --> 01:20:00,020
I don't know if bad fellows is the right word like these things really run together

993
01:20:00,340 --> 01:20:07,500
But I do think that you come up with cases that reward accounts for and the emotions do not and that there are cases where

994
01:20:07,940 --> 01:20:12,860
reward explains the phenomenon and pleasure and pain do not and so I

995
01:20:14,780 --> 01:20:20,400
Do you think it's you know, they're obviously very closely related but something is lost by blurring between those two

996
01:20:23,420 --> 01:20:25,100
Maybe I

997
01:20:25,100 --> 01:20:27,300
Of course economists of which I am one

998
01:20:28,300 --> 01:20:34,700
Have thought of and debated about these questions a lot because we represent a utility function

999
01:20:34,700 --> 01:20:40,300
Most of the modeling is done with the idea that you can take all decision-making and put it into a

1000
01:20:41,060 --> 01:20:48,140
Continuous function, but I want to press a little bit because I hear in Julia's framework

1001
01:20:49,620 --> 01:20:55,700
Again with this concept this broader concept of valuation to say like your brain may actually have many sources

1002
01:20:55,980 --> 01:21:00,100
many valuations schemes that aren't necessarily

1003
01:21:01,380 --> 01:21:03,580
Representable as a continuous

1004
01:21:04,140 --> 01:21:10,820
Scaler reward and so I wouldn't want to push on the scalar part because I think that's a difference between

1005
01:21:11,500 --> 01:21:13,500
What you've presented

1006
01:21:13,700 --> 01:21:15,100
and

1007
01:21:15,100 --> 01:21:18,100
That's the economics agrees with the scalar

1008
01:21:18,940 --> 01:21:22,780
Yeah, you do. Yeah, I do. All right. I'll be the disagreeer. Yeah

1009
01:21:23,220 --> 01:21:25,420
So there's a nice paper by

1010
01:21:30,260 --> 01:21:31,860
I'll come up with the name in a second

1011
01:21:31,860 --> 01:21:35,700
Which just basically suggests that you can have these vectors that reduced to the scalar

1012
01:21:36,020 --> 01:21:38,820
and so that there are kind of mathematical ways of

1013
01:21:39,540 --> 01:21:46,340
Taking that kind of consideration and still having it reduced to scalar and you can say like whoof the theory holds that it's scalar

1014
01:21:46,860 --> 01:21:51,020
And so I think I'm probably less invested in this part of of it

1015
01:21:51,020 --> 01:21:57,500
But that would be the way that I would appeal to it is that you can accommodate that kind of intuition and still preserve the scalar feature of

1016
01:21:57,500 --> 01:22:00,140
The theory, but you might want to defend it like more

1017
01:22:01,660 --> 01:22:08,940
The original work arguing that you can reduce the vector to the scalar is in second is in economics

1018
01:22:09,940 --> 01:22:11,940
I

1019
01:22:13,620 --> 01:22:15,620
Morgan Stern and

1020
01:22:15,620 --> 01:22:23,620
Somebody rather you know, well, yes, it's hope and see there's a debate in economics about and and but by far the dominant

1021
01:22:23,820 --> 01:22:30,540
Neoclassical economics is that you can you can do that. Yeah, but there's also a different project in economics, which is

1022
01:22:31,260 --> 01:22:35,140
We're trying to build models that will predict the way

1023
01:22:36,260 --> 01:22:38,140
humans and

1024
01:22:38,140 --> 01:22:45,260
Entities composed of humans like markets will behave and so then when you say it's enough

1025
01:22:45,260 --> 01:22:48,580
You're saying oh, it's good enough to be predictive

1026
01:22:48,580 --> 01:22:54,060
And so one of the things I think about is how much are we taking that and now saying well, this is the way

1027
01:22:55,580 --> 01:22:57,580
Valuation works in

1028
01:22:57,820 --> 01:23:04,440
Human societies in the human and in human societies and I guess I want to put a wedge in there to say

1029
01:23:04,760 --> 01:23:13,400
You know, it could be that I could predict pretty well even if values are incommensurate and can't be reduced to a trade-off

1030
01:23:14,440 --> 01:23:20,160
Nonetheless at the end of the day. I'm just predicting are you gonna choose this or this so I could have a representation

1031
01:23:20,600 --> 01:23:23,000
That worked for that predictive exercise

1032
01:23:24,200 --> 01:23:25,960
so

1033
01:23:25,960 --> 01:23:28,320
All bets are off when you have groups

1034
01:23:29,080 --> 01:23:33,760
you know the the the basic theory is the theory of an individual and and

1035
01:23:35,080 --> 01:23:38,960
Yet and yet we want to move to say something interesting about

1036
01:23:40,000 --> 01:23:43,360
About groups or to ask if it's possible to say anything

1037
01:23:44,440 --> 01:23:45,880
normative

1038
01:23:45,880 --> 01:23:47,880
or

1039
01:23:49,320 --> 01:23:53,920
Yeah, can we say something normative normative, so what does this mean this mean I

1040
01:23:56,920 --> 01:24:00,520
See I might any definition I'll give will be reductive again, so

1041
01:24:05,400 --> 01:24:12,440
So how can we walk up to this so one way to walk up to it is the way Julia did which saying well, maybe these

1042
01:24:14,000 --> 01:24:16,000
The moral decisions

1043
01:24:16,920 --> 01:24:22,360
Are are not that different from the ordinary decision-making we do it every day

1044
01:24:23,160 --> 01:24:25,000
And to that to me that makes sense

1045
01:24:25,000 --> 01:24:31,720
And so I mean I want to push and say that for the individual we can think of it all as one system

1046
01:24:32,120 --> 01:24:36,640
Now to make it a little bit more acceptable to you that it might be that way

1047
01:24:36,760 --> 01:24:41,400
You want sure you remember that the the value judgments as well as the policy

1048
01:24:41,400 --> 01:24:50,560
But let's the value judgments are compiled. They are they are they may they may be due to learning or reasoning

1049
01:24:50,840 --> 01:24:55,120
but in the end they are they are they are like

1050
01:24:56,000 --> 01:24:57,480
they're

1051
01:24:57,520 --> 01:25:03,400
Hard-coded and they're automatic and they're intuitive. They're not the reasons for them are no longer present

1052
01:25:03,480 --> 01:25:09,040
that's so I see the reasons you might have a very firm feeling that that

1053
01:25:12,480 --> 01:25:15,400
Children should have a loving parent and

1054
01:25:17,160 --> 01:25:18,920
You might not have

1055
01:25:18,920 --> 01:25:25,440
Any more have the reasons for that belief you just believe it really strongly and and maybe maybe there was a reason for it

1056
01:25:25,440 --> 01:25:31,520
Maybe you had experience or you've seen people or you've done some reasoning process and you came up with a reason

1057
01:25:31,520 --> 01:25:37,880
But but now you just know you have this belief and so this is why we tend to think of values as core things

1058
01:25:38,720 --> 01:25:40,720
And and more emotional

1059
01:25:41,560 --> 01:25:45,480
Because we we don't have the reasons now it doesn't mean we lost the reasons

1060
01:25:45,480 --> 01:25:49,440
You know, maybe the reasons who are built built into us by evolution to me

1061
01:25:49,440 --> 01:25:57,480
It doesn't really matter. You have ended up with a judgment about the the the goodness or badness of certain things and and

1062
01:26:02,080 --> 01:26:07,840
Nevertheless a good way to think about them maybe as a prediction of the likely rewards

1063
01:26:08,520 --> 01:26:10,680
For being in that state or acting in that way

1064
01:26:11,960 --> 01:26:14,440
And to me that that's a really exciting

1065
01:26:14,440 --> 01:26:21,400
Apothos that you might be able to think about of all the judgments that people may including their moral ones in terms of a single framework

1066
01:26:23,720 --> 01:26:29,960
So I want to separate that is that impressive and I feel you from your talk that you were like

1067
01:26:30,320 --> 01:26:32,600
trying to lead lead us that way and

1068
01:26:33,600 --> 01:26:39,440
So I want to separate that there are two big things to talk about is that makes sense. Okay, and then

1069
01:26:40,440 --> 01:26:42,200
If that makes sense

1070
01:26:42,200 --> 01:26:50,200
What about when you have multiple agents and and then and we asked the question of universality and normative judgments

1071
01:26:50,640 --> 01:26:52,640
So are we at the?

1072
01:26:53,840 --> 01:26:58,920
Yeah, are we ready to have we have we done accepted the first thing and are we ready to do the second thing?

1073
01:26:59,480 --> 01:27:04,240
Are we still unsure and I'm I'm gonna ask you, you know, everyone, you know

1074
01:27:05,160 --> 01:27:07,160
Anyway, these these these two steps

1075
01:27:07,880 --> 01:27:09,560
Are we ready?

1076
01:27:09,560 --> 01:27:14,640
So can I just jump in on that real quick because I think there's a step between is that okay? Yeah, no, that's great

1077
01:27:14,640 --> 01:27:19,080
And then I'll go to question. Um, so I like this idea of reasons are no longer present

1078
01:27:19,080 --> 01:27:24,120
So on the slide and have time to talk about it, but I had this case of moral dumbfounding. So moral dumbfounding is when

1079
01:27:25,000 --> 01:27:27,000
You give people scenarios

1080
01:27:27,200 --> 01:27:29,000
like, you know

1081
01:27:29,000 --> 01:27:34,040
You know Jane and and John are brother and sister and and they would like to have sex

1082
01:27:34,520 --> 01:27:40,640
Just the one time with protection. So, you know, there's no possibility of any of the genetic consequences

1083
01:27:41,360 --> 01:27:43,080
of, you know

1084
01:27:43,080 --> 01:27:47,720
incest and so on and so forth and you ask people like, you know, is this morally acceptable and you know

1085
01:27:49,120 --> 01:27:50,720
and

1086
01:27:50,720 --> 01:27:55,800
You ask them and they'll give you kinds of reasons, but they they can't really so it's this dumbfounding case where you're like

1087
01:27:55,800 --> 01:27:59,640
Yeah, okay, there should be everything fine with this. The other cases are like having sex with the chicken

1088
01:27:59,640 --> 01:28:03,200
Anyway, there's some stuff going on in the lab there that came up these examples

1089
01:28:03,200 --> 01:28:08,960
But there's strong cases we have these the responses and I really like this way of putting the reasons are no longer present

1090
01:28:09,600 --> 01:28:11,600
Right, the reasons are good reasons

1091
01:28:12,400 --> 01:28:19,360
But they're no longer present in this case because if if John and Jane or whatever I said use contraception then, you know

1092
01:28:19,360 --> 01:28:25,640
This this evolved reason for why we're opposed morally opposed to incest is no longer there

1093
01:28:25,840 --> 01:28:27,840
But that's actually a really important

1094
01:28:28,400 --> 01:28:32,320
Junction, right? That suggests that those intuitions

1095
01:28:33,200 --> 01:28:36,880
Maybe over the course of evolution are a good

1096
01:28:37,760 --> 01:28:42,480
signal of reward or not, but that there are departures from that

1097
01:28:43,200 --> 01:28:45,200
And that's where the normative

1098
01:28:46,320 --> 01:28:54,720
Story comes in so we have these descriptive experiences like no, but the reasoning and the moral philosophy or the ethics or

1099
01:28:55,280 --> 01:29:00,680
Policymaking or all these normative disciplines law economics come in because we tell ourselves, okay

1100
01:29:00,680 --> 01:29:07,560
But shunning the outgroup may no longer be doing us a service because we live in a global cosmopolitan society where

1101
01:29:07,840 --> 01:29:09,760
We all want to get along

1102
01:29:09,760 --> 01:29:12,920
And who's getting along there is all of a sudden a much bigger group

1103
01:29:13,640 --> 01:29:19,480
And so I think you can have a part of that first story which is radical in and of itself

1104
01:29:19,600 --> 01:29:22,920
But not necessarily follow it all the way to step two

1105
01:29:23,720 --> 01:29:25,960
Because sometimes we believe things that are just

1106
01:29:27,480 --> 01:29:29,480
Silly like you shouldn't have sex with a chicken

1107
01:29:35,080 --> 01:29:41,160
So I I'm just I'm gonna I'm gonna just throw out there and then I'll come to the audience

1108
01:29:41,160 --> 01:29:45,360
and I may want to put Joel and will on the spot on this one as well, but

1109
01:29:46,120 --> 01:29:54,320
You know the the the idea of thinking about the values or as or the as

1110
01:29:56,400 --> 01:29:58,400
Intuitions as

1111
01:29:58,480 --> 01:30:02,160
Some kind of thing that is just there

1112
01:30:02,160 --> 01:30:09,120
We don't know where it came from it accumulated from arts as opposed to which is what I really like about your framework Julia is

1113
01:30:10,080 --> 01:30:16,880
It's a very active process and actually so I make the claim in the work that I'm doing thinking about normative systems

1114
01:30:16,880 --> 01:30:19,760
And I have a very very reductive definition of normativity

1115
01:30:20,240 --> 01:30:24,120
It's what the group labels and we just have a labeling scheme

1116
01:30:24,120 --> 01:30:32,560
This group says this is okay, and this is not okay, and we've you know coordinated a cognitive structures and institutions

1117
01:30:32,560 --> 01:30:35,720
And so on to to produce that but it's adaptive it changes

1118
01:30:36,120 --> 01:30:38,400
We get angry about rule violations

1119
01:30:38,400 --> 01:30:45,120
But you can go to a different environment where the rule is different and you used to get mad when people came into the building with short

1120
01:30:45,440 --> 01:30:47,760
shorts on and now you don't

1121
01:30:48,240 --> 01:30:51,640
Right, and so I think it's it's it's very much a

1122
01:30:52,560 --> 01:30:55,720
Processing of the information from the group

1123
01:30:56,360 --> 01:30:58,360
So it's a group thing

1124
01:30:58,400 --> 01:31:01,480
It's a pick a particular group all groups don't have to agree

1125
01:31:01,960 --> 01:31:05,000
Even a single group may change its mind over time

1126
01:31:05,000 --> 01:31:10,040
Yes, and that that's actually a complex system to understand how that how that functions

1127
01:31:10,760 --> 01:31:16,320
And the silly rules about chickens for example or something that thought a fair bit about

1128
01:31:16,800 --> 01:31:24,800
But I want to make sure we get because I know that there's lots of provocations for our audience here to to contribute on so let me

1129
01:31:25,080 --> 01:31:28,080
Let me open it up here. I'm gonna go to Jennifer first

1130
01:31:28,080 --> 01:31:30,080
Yes

1131
01:31:30,320 --> 01:31:32,320
Yes, and do wait for the mic

1132
01:31:34,040 --> 01:31:37,640
So so this is sort of following up on Julia's claim that minds

1133
01:31:38,920 --> 01:31:42,640
Subpersonally a sign reward, and I don't disagree with that. I think absolutely we do that

1134
01:31:43,040 --> 01:31:49,440
I'm curious about whether we also personally do this and whether it's not an expression of our

1135
01:31:50,040 --> 01:31:53,640
Autonomy as rational agents capable of setting goals that we do this

1136
01:31:53,640 --> 01:31:57,560
This is also my reservation about the sort of McCarthy definition of intelligence as

1137
01:31:58,560 --> 01:32:05,840
You know being about the computational part of achieving goals surely we can apply our rational intelligence to setting

1138
01:32:06,160 --> 01:32:13,080
Goals, I mean we don't have that much flexibility with respect to the sort of homeostatic needs that are

1139
01:32:14,040 --> 01:32:16,160
Encoded in us as biological creatures

1140
01:32:16,160 --> 01:32:21,720
But but beyond that we're on a long leash from evolution and we can surely in

1141
01:32:22,360 --> 01:32:26,680
Rational conversations with each other work towards finding

1142
01:32:29,040 --> 01:32:33,540
Goals that stand the test of

1143
01:32:34,800 --> 01:32:36,800
You know

1144
01:32:36,800 --> 01:32:38,160
rational

1145
01:32:38,160 --> 01:32:40,160
scrutiny and

1146
01:32:40,160 --> 01:32:44,240
And and and so that kind of that kind of expression of our freedom

1147
01:32:45,520 --> 01:32:49,800
Intellectually seems to me to be a very very important application of human intelligence

1148
01:32:49,800 --> 01:32:55,480
We're not just slaves of our reward functions, you know just looking for ways to maximize

1149
01:32:55,680 --> 01:32:58,240
You know, whatever this damn thing is in us

1150
01:32:58,240 --> 01:33:04,800
We get to say what we're pursuing and that seems a way in which you know our intelligence is

1151
01:33:05,480 --> 01:33:07,480
Caught into play

1152
01:33:07,600 --> 01:33:14,200
Isn't there a famous philosopher who said who said that our reason is a slave to the passions

1153
01:33:14,720 --> 01:33:16,640
David Hume

1154
01:33:16,640 --> 01:33:19,560
Reason is and ought only to be a slave to the passions

1155
01:33:19,960 --> 01:33:21,960
Yeah, but that's you. Yeah

1156
01:33:22,840 --> 01:33:28,440
And he's obviously wrong about that so so I'm very much on this. I'm very much on the side of the manual count here that we

1157
01:33:29,400 --> 01:33:33,440
That that what is what is right is what is universalizable?

1158
01:33:34,040 --> 01:33:41,160
there are so so so you know the the the maximum underpinning your conduct should be something that

1159
01:33:41,960 --> 01:33:46,200
That is universalizable that could be could be a law for all

1160
01:33:47,200 --> 01:33:52,480
There's there's an important contribution of the inclinations in here, right?

1161
01:33:52,480 --> 01:33:59,320
So we have certain natural inclinations that we have to satisfy to continue our existence as the kinds of biological creatures

1162
01:33:59,320 --> 01:34:04,440
That we are and we may have to discover things about people's inclinations empirically through interactions with each other

1163
01:34:04,720 --> 01:34:05,880
but

1164
01:34:05,880 --> 01:34:08,400
But the question of what we're inclined to do is

1165
01:34:08,880 --> 01:34:13,080
Sabrable from the question of what we ought to do we can criticize our inclinations

1166
01:34:13,080 --> 01:34:19,360
I could find myself inclined to do stuff that that I think is wrong actually even just kind of at the level of biological fitness, right?

1167
01:34:19,360 --> 01:34:21,120
there's you know various

1168
01:34:21,120 --> 01:34:26,520
appetites that I have towards you know sweet and salty foods and so on and I can I can appreciate

1169
01:34:27,120 --> 01:34:30,800
rationally the reasons for which I have those inclinations and

1170
01:34:31,320 --> 01:34:37,520
Actually reject them as you know in the current environment not contributing to my fitness. I can overrule them

1171
01:34:38,120 --> 01:34:40,440
And that's not something that you know reckons can do

1172
01:34:41,440 --> 01:34:44,000
That's why we're more intelligent than they are

1173
01:34:48,480 --> 01:34:52,280
My sister literally had four raccoons in her garage this morning, so

1174
01:34:55,440 --> 01:35:00,320
Okay, so obviously like a big question, so let me kind of try and step through that

1175
01:35:01,440 --> 01:35:06,360
So we were talking about this a little bit yesterday, so I would say that I'm a motivational involuntary

1176
01:35:06,960 --> 01:35:11,560
So I'll give the example of coffee, but cigarettes might be a good example for other people

1177
01:35:12,600 --> 01:35:16,120
Over the course of my experience. I have acquired a very strong

1178
01:35:16,920 --> 01:35:18,920
liking or a cup of coffee and

1179
01:35:20,200 --> 01:35:25,120
That guides what I do, but again coffee is kind of a trivial case. Oh and Jennifer

1180
01:35:25,120 --> 01:35:28,880
I definitely put an asterisk there just for you because I was like

1181
01:35:31,880 --> 01:35:33,880
So in my defense

1182
01:35:34,360 --> 01:35:38,840
Now I can't just reach in and say I'm not going to need that cup of coffee tomorrow

1183
01:35:38,840 --> 01:35:45,400
I'm not going to need that cigarette and I I can't say either when you scale it up to some of my moral responses

1184
01:35:45,920 --> 01:35:52,040
Maybe less savory moral responses. I'm just not going to judge that way or I'm going to do that thing

1185
01:35:52,040 --> 01:35:55,360
So I don't think we can overrule them in the that that sense

1186
01:35:55,760 --> 01:36:03,180
And I think that's basically I'm an early modern nationalist pre-con that I think that they're they're thick causal mechanisms

1187
01:36:03,780 --> 01:36:05,780
Of course, we're not raccoons

1188
01:36:05,980 --> 01:36:08,940
And we do have reasons we can make

1189
01:36:09,340 --> 01:36:12,340
Outs as groups and and to the the group

1190
01:36:12,860 --> 01:36:18,860
You know what what is normative is for group? I think one way to kind of start picking at that is that some groups are just better

1191
01:36:19,780 --> 01:36:23,580
They have better principles and better norms that work better for the group than others

1192
01:36:23,580 --> 01:36:25,580
So that should be a kind of clue that

1193
01:36:25,580 --> 01:36:30,180
It's not you know that there there might be a kind of normative dimensions like okay

1194
01:36:30,180 --> 01:36:34,220
These institutions that in fact work better towards the goals of us getting along together

1195
01:36:34,980 --> 01:36:39,700
Close-eyed bar. So what do we do when we have this involuntarism?

1196
01:36:39,700 --> 01:36:43,300
but we also come equipped with the raccoon plus of

1197
01:36:44,700 --> 01:36:50,740
Knowing that there is a better way. I think we have relatively indirect paths

1198
01:36:52,020 --> 01:36:57,060
To let's say diminishing my reward for coffee. So what I can do is I can

1199
01:36:58,060 --> 01:37:02,500
You know drink a glass of salty water every day that I have a cup of coffee and

1200
01:37:03,780 --> 01:37:05,780
Indirectly ratchet down

1201
01:37:05,780 --> 01:37:10,780
The value that I place on coffee or the value that I place on the smell of a cup of coffee and so on and so forth

1202
01:37:11,460 --> 01:37:15,300
So it's kind of an intermediate view where it's like I think we can leverage our reason

1203
01:37:15,300 --> 01:37:18,180
But it's by no means as easy as some of us would want

1204
01:37:18,580 --> 01:37:22,860
To hope that it is and I think it's important to emphasize this end of the debate

1205
01:37:22,860 --> 01:37:28,300
I mean again, it's always a question of emphasis, but it's important to emphasize the difficulty because I think if we don't

1206
01:37:28,460 --> 01:37:32,260
We think we could just pull the lever and I think that leads to all kinds of

1207
01:37:33,420 --> 01:37:35,420
disasters basically

1208
01:37:36,220 --> 01:37:37,980
So I want to

1209
01:37:37,980 --> 01:37:39,660
React as well

1210
01:37:39,660 --> 01:37:45,060
I'm really glad you asked this question because I think it's really the heart of the whole thing

1211
01:37:45,060 --> 01:37:51,820
It's you know, is it demeaning to think that there we have there they have something that's a signal coming in

1212
01:37:53,660 --> 01:37:55,660
I'm gonna call it pleasure and pain because it

1213
01:37:56,660 --> 01:38:04,060
It goes with it with the demeaning aspect to it. We are a slave to our pleasure and pain and it makes us seem small

1214
01:38:05,060 --> 01:38:10,780
And like we don't have choices and these are the most important things in our lives

1215
01:38:10,900 --> 01:38:15,140
What are what we're trying to do with our lives and that we the idea we don't have

1216
01:38:16,020 --> 01:38:18,020
Control of them is really annoying

1217
01:38:23,860 --> 01:38:29,060
But I think it absolutely is true and it's true in kind of a definitional sense because

1218
01:38:29,460 --> 01:38:35,300
When you say you have a reason for something you mean I do this because I'm trying to do this and

1219
01:38:36,540 --> 01:38:43,980
Obviously throughout our lives. We have reasons like that. Why I'm going to work to earn money. I'm trying to earn money. So

1220
01:38:45,620 --> 01:38:52,020
Raise a family. I'm trying to raise a family because I enjoy certain activities and I want to see things happening

1221
01:38:52,020 --> 01:38:55,540
We all we have many many reasons and we're used to that

1222
01:38:55,540 --> 01:39:01,100
But we also should acknowledge that eventually it has to stop, you know, you have reasons for things

1223
01:39:01,100 --> 01:39:07,180
But eventually there's some final thing that doesn't have any reason for it or at least that's

1224
01:39:08,260 --> 01:39:14,260
Maybe the standard way to think about things in science. I don't know, you know

1225
01:39:14,260 --> 01:39:17,500
It's the old thing about the odds and the is is you can't derive

1226
01:39:18,020 --> 01:39:22,340
By learning about the world you cannot really derive what you should want

1227
01:39:23,300 --> 01:39:25,300
there's there is

1228
01:39:25,540 --> 01:39:28,740
There is one standard philosophical view. I can't do it justice

1229
01:39:28,740 --> 01:39:34,420
But but it's this the view that that you can't derive an ought from it is and that so you have to have some ultimate

1230
01:39:34,700 --> 01:39:42,260
Oughts that are given and those ultimate odds are the things that we can't choose. So there must be something that we can't choose

1231
01:39:43,260 --> 01:39:48,860
And that's what the reward is meant to be. It's the one thing that we can't choose comes from

1232
01:39:49,700 --> 01:39:51,060
outside

1233
01:39:51,060 --> 01:39:54,100
And it's an outside just means we can't choose it

1234
01:39:54,740 --> 01:39:59,780
And so since that has to be true, I think even though it feels

1235
01:40:01,100 --> 01:40:02,180
demeaning

1236
01:40:02,180 --> 01:40:05,620
That we should get used to it and stop feeling that it makes it demeaning

1237
01:40:05,620 --> 01:40:12,220
We have something that we ultimately want and and so we're ultimately and it's individual in some sense

1238
01:40:12,300 --> 01:40:14,300
selfish

1239
01:40:17,420 --> 01:40:19,420
And I think

1240
01:40:19,460 --> 01:40:24,620
That if you embrace that it's kind of liberating, you know different people are different. They want different things

1241
01:40:24,820 --> 01:40:27,060
It's okay for them to pursue different things

1242
01:40:28,660 --> 01:40:30,660
Doesn't make us bad people

1243
01:40:31,340 --> 01:40:35,620
I'm going to jump to another question because I want to make sure we get a few more people in here Sheila

1244
01:40:36,020 --> 01:40:38,020
I

1245
01:40:40,340 --> 01:40:44,500
Thank you so much to all three of you for for such an engaging

1246
01:40:45,460 --> 01:40:47,460
discussion, so I was I

1247
01:40:47,940 --> 01:40:49,940
Guess I wanted to start by just

1248
01:40:50,060 --> 01:40:54,820
Harking back to a conversation that that rich and I had yesterday. I think I and in defense of John McCarthy

1249
01:40:54,820 --> 01:41:03,620
I think that McCarthy in 1955 defined a notion of artificial intelligence and and that that intelligence is is a vague term

1250
01:41:03,820 --> 01:41:07,900
Just like our conversation yesterday about about baldness, you know

1251
01:41:07,900 --> 01:41:12,820
If I start pulling hairs out of my head at what point do I recognize that it that I'm bald

1252
01:41:12,820 --> 01:41:16,980
Whereas with when I put that hair back in I'm when you call me this way. Yeah

1253
01:41:18,580 --> 01:41:23,180
Anyways, I think there are lots of terms that are vague and I think that people define them purposefully

1254
01:41:23,180 --> 01:41:25,180
And I think that intelligence is one of those so

1255
01:41:25,660 --> 01:41:29,460
But what I but I wanted to I was really struck by by Julia's

1256
01:41:30,100 --> 01:41:36,060
Discussion of binocular rivalry because I feel like I'm having binocular rivalry, you know between the two of you

1257
01:41:36,060 --> 01:41:41,420
And and that we're all just trying to make sense of this this complicated idea

1258
01:41:41,420 --> 01:41:45,180
And and we and again to a point that Julia made you know

1259
01:41:45,220 --> 01:41:52,020
What we see the way that we interpret the way that we make sense of the world is indeed informed by by some sort of

1260
01:41:52,380 --> 01:41:56,980
Bias we have or it's sorry. That's not the word that you use but some sort of expectation of what we want to achieve

1261
01:41:57,140 --> 01:41:59,140
So so what I wanted to just to

1262
01:42:00,580 --> 01:42:02,740
To to make sense of myself is

1263
01:42:03,340 --> 01:42:10,900
Where in your conceptualizations of things you you absolutely disagree or where you actually where we're some of the

1264
01:42:11,260 --> 01:42:15,820
Disagreements we see and even to Gillian's question about the reward hypothesis is just about what we actually

1265
01:42:16,620 --> 01:42:20,860
Decide to highlight in our formalization or our understanding of things like so

1266
01:42:21,380 --> 01:42:26,620
Rich even when I was looking at your your decision-making box on the right, which is one that and one of your slides

1267
01:42:26,620 --> 01:42:33,380
Which is one that I'm very familiar with, you know, I thought where's memory? Where's long-term memory everything's been compiled

1268
01:42:33,380 --> 01:42:35,380
You know, we're from classical

1269
01:42:36,500 --> 01:42:45,460
Decision-making where's the notion of a you know of a of a that predictive mechanism is actually being not sort of compiled into the value

1270
01:42:45,460 --> 01:42:48,260
function or everything being compiled into

1271
01:42:49,140 --> 01:42:56,380
Scalar reward to Julia's response to to to Gillian and and so I guess I want to push back on on saying

1272
01:42:56,380 --> 01:42:58,900
What are the elements that are important? Where's long-term memory?

1273
01:42:59,180 --> 01:43:06,780
Where is where is this notion some of the the elements that actually contribute to a reward eventually being compiled into a scalar reward?

1274
01:43:06,780 --> 01:43:09,660
And and is that diagram for you on the right?

1275
01:43:10,140 --> 01:43:13,700
really representing everything that that it needs and then I guess

1276
01:43:13,860 --> 01:43:19,780
Point and and and again, I thought about things again that we think about in the context of multi-agent systems about about

1277
01:43:20,620 --> 01:43:23,740
Goal-seeking, you know the ability to create goals about

1278
01:43:24,340 --> 01:43:31,460
Commitment about intention all of those elements that we often use to make sense of the world that that are not part of your diagrams

1279
01:43:31,460 --> 01:43:34,700
Probably we can they're compiled in somewhere in there

1280
01:43:34,700 --> 01:43:36,860
But I think there's utility in including them

1281
01:43:36,860 --> 01:43:43,260
So what what do we have are there things in your formalisms that are just compiled away that are important and then?

1282
01:43:43,260 --> 01:43:48,860
I guess the other question was where do we fundamentally or where do you in your binocular rivalry?

1283
01:43:49,380 --> 01:43:50,620
fundamentally

1284
01:43:50,620 --> 01:43:54,220
Or my my sense of you're the binocular of rivalry that I'm having

1285
01:43:54,740 --> 01:43:57,780
fundamentally disagree and and and I was and maybe this is a

1286
01:43:59,580 --> 01:44:01,580
Conversation to take offline and for afterwards

1287
01:44:01,580 --> 01:44:08,220
But one of the things I was intrigued about is this you know what what I perceive is our perhaps our ability to be able to reflect on our

1288
01:44:08,900 --> 01:44:14,940
Art to have an awareness of our reward to be able to reflect on our reward as as as we do

1289
01:44:15,140 --> 01:44:22,860
Representation other representations and whether that's an element that that that that is of utility and explaining some of what we see is

1290
01:44:23,100 --> 01:44:26,260
Understands the mind. So that's it. Well, those are a lot of things

1291
01:44:26,820 --> 01:44:28,820
Julie is writing down her thoughts

1292
01:44:30,180 --> 01:44:32,620
I'm not writing so I have to go first because

1293
01:44:33,540 --> 01:44:35,540
I'll forget

1294
01:44:35,900 --> 01:44:40,860
There are many things I will I will forget some remind me if you get them wrong

1295
01:44:44,700 --> 01:44:46,900
First memory long-term memory

1296
01:44:48,820 --> 01:44:54,780
There were four boxes all four boxes learn and so maybe they're four boxes are for

1297
01:44:55,420 --> 01:44:58,780
Maybe they're for neural networks. You that'd be a fine way to think about it

1298
01:44:58,780 --> 01:45:01,960
And then they all have weights and those weights are all adjusted through

1299
01:45:02,780 --> 01:45:08,700
Learning and planning both so that they would all have long-term memory. Memory is throughout

1300
01:45:13,460 --> 01:45:15,460
So I want us

1301
01:45:15,660 --> 01:45:18,840
We can get a sense of freedom. We get a sense of intention

1302
01:45:19,380 --> 01:45:23,980
We can set a sense of choosing goals because I do think it's a really major

1303
01:45:24,780 --> 01:45:32,220
Psychological thing that we choose goals. We choose except they're not they're not they're not reward goals

1304
01:45:32,220 --> 01:45:35,900
So maybe they shouldn't be called goals. They are sub-goals. They are

1305
01:45:37,180 --> 01:45:38,620
strategies

1306
01:45:38,620 --> 01:45:39,980
that

1307
01:45:39,980 --> 01:45:45,900
That you choose as part of your solution of the problem the problem is fixed the problem is reward

1308
01:45:46,380 --> 01:45:53,620
The problem is is it's the reward is not a compilation of something. It is the it is the thing that is unchanged

1309
01:45:54,380 --> 01:45:56,380
It is not a compilation of something

1310
01:45:57,180 --> 01:45:59,380
other things are compiled like by

1311
01:46:00,380 --> 01:46:07,060
Learning what gets rewarding and what's not rewarding you decide what to do by learning what's rewarding. What's not rewarding you?

1312
01:46:07,060 --> 01:46:12,180
You compile you compile into the policy you compile into the value function

1313
01:46:12,860 --> 01:46:17,700
You even compile into the model and you compile into the perceptual process where you're figuring out

1314
01:46:17,700 --> 01:46:23,100
Oh, this is a good way to think about the state that I'm in or this is a bad way to think about the state that I'm in

1315
01:46:23,380 --> 01:46:27,100
all those are learned and in that sense compiled and

1316
01:46:28,060 --> 01:46:34,020
If you want to look at reasoning reasoning is in the model of the world we learned that you know this causes that and

1317
01:46:34,700 --> 01:46:36,100
that that that

1318
01:46:36,100 --> 01:46:43,220
Reasoning that that knowledge of the world is in multiple levels low levels low level physics and high level decisions like like oh

1319
01:46:43,220 --> 01:46:45,780
I'll go to the conference or I'll take this job

1320
01:46:48,180 --> 01:46:55,260
But there's no compiling in the reward signal or you know if there's any compiling it would be evolution compiled its goals into our

1321
01:46:55,380 --> 01:46:57,380
Reward, but for us

1322
01:46:57,900 --> 01:47:01,660
Reward is just a primitive thing and it's never never changed

1323
01:47:03,340 --> 01:47:08,940
And yet we absolutely it's appropriate to have the sense that we choose our sub-goals

1324
01:47:08,940 --> 01:47:14,540
And that's the most important thing we make I'm gonna I'm going to choose to study

1325
01:47:16,540 --> 01:47:18,540
Economics or

1326
01:47:19,260 --> 01:47:21,260
Or our philosophy

1327
01:47:21,940 --> 01:47:28,420
Or I'm going to choose to devote my life to understanding how the mind works. Okay, those are and I'm thinking

1328
01:47:28,860 --> 01:47:31,360
I'm not thinking about this, but

1329
01:47:32,420 --> 01:47:39,460
Maybe if I devote my life to figuring out how the mind works, I will enjoy myself better. I think that's what the way it goes

1330
01:47:40,980 --> 01:47:42,980
I will forget more reward

1331
01:47:43,820 --> 01:47:51,320
So I think yeah, there's also a multi-part and I feel like Joel basically asked you Sheila to ask this question

1332
01:47:53,540 --> 01:47:57,540
So in terms of the question of you know emphasis and binocular rivalry, you know

1333
01:47:57,540 --> 01:48:01,360
I think that's just the nature of scientific and philosophical theorizing, right?

1334
01:48:01,380 --> 01:48:05,180
It's a question of emphasis in a lot of cases

1335
01:48:05,340 --> 01:48:08,980
So depending on how we configure our audience

1336
01:48:08,980 --> 01:48:15,660
We would either be like aggressive opponents or allies depending on who we're engaging with and the point that you're trying to get across

1337
01:48:16,060 --> 01:48:18,180
There's just a sociological feature of that

1338
01:48:19,020 --> 01:48:23,060
But I think where things really start to come apart is what predictions

1339
01:48:23,540 --> 01:48:28,540
Do your commitments generate and what implications follow from your view?

1340
01:48:28,900 --> 01:48:32,220
And that's where you also start to see some departures between us

1341
01:48:32,940 --> 01:48:37,380
So just examples from the kind of conversation that we've had so far that I'm thinking about is okay

1342
01:48:37,380 --> 01:48:40,540
I think we actually really disagree on this pleasure and pain thing like we're like oh

1343
01:48:40,900 --> 01:48:44,900
No, we really disagree about that and it will generate different predictions

1344
01:48:44,900 --> 01:48:51,660
And it has very different implications for how we understand certain things about the nature of human beings and and how we kind of follow from that

1345
01:48:51,660 --> 01:48:53,660
I think we disagree on

1346
01:48:55,900 --> 01:49:01,140
This normative question of what then we should take away from these experiences of values that we have

1347
01:49:01,460 --> 01:49:04,860
It's funny because I wrote down liberating to but I mean it in a very different way

1348
01:49:05,260 --> 01:49:09,860
It's liberating in the sense that if you recognize your limitations, that's very liberating

1349
01:49:09,860 --> 01:49:12,700
So you accept the flaws in your system

1350
01:49:12,700 --> 01:49:19,660
That's actually very empowering to the way that I was mentioning with the coffee because then I can use I can leverage that to become better

1351
01:49:20,500 --> 01:49:22,660
and

1352
01:49:22,660 --> 01:49:26,060
again, so you'll see different predictions there about what works in terms of

1353
01:49:27,100 --> 01:49:33,740
Self-regulation and there will just be departures and in the kinds of commitments they have and then I would say in terms of

1354
01:49:34,380 --> 01:49:41,740
What we're striving for with artificial intelligence and how we should recognize them for example as people that would be another place where

1355
01:49:43,060 --> 01:49:47,940
You would actually see our commitments that look very similar in some places

1356
01:49:49,140 --> 01:49:51,140
separate because of

1357
01:49:51,820 --> 01:49:54,260
the implications that follow from that so I think

1358
01:49:55,780 --> 01:50:01,540
But it but it really is like there's partly a sociological phenomenon of who we're engaging with and who we're trying to persuade

1359
01:50:02,420 --> 01:50:06,420
Where in some cases we're a united front because I think the reward hypothesis

1360
01:50:07,580 --> 01:50:12,340
Is in a minority or can be in a minority despite how powerful it is

1361
01:50:12,900 --> 01:50:17,180
But then you know between one another we would see see big differences or something like that

1362
01:50:19,820 --> 01:50:24,340
Thank you, and this may be our last question we'll see yeah

1363
01:50:25,620 --> 01:50:28,620
This was a really fascinating talk I

1364
01:50:29,340 --> 01:50:34,780
Don't have a specific question in particular, but I do want to ask in the

1365
01:50:35,580 --> 01:50:40,140
Interests of internalizing reward and self-regulation. I think those things you brought up

1366
01:50:41,420 --> 01:50:46,140
If it's not beyond the scope of your research, I wanted to ask about reward hacking and

1367
01:50:47,180 --> 01:50:52,420
The idea of good-hearted law which is when the measure becomes a target it seems to be a good measure

1368
01:50:52,940 --> 01:50:55,620
I'm really interested in hearing if you have any thoughts on that

1369
01:50:59,620 --> 01:51:01,620
I don't know if I heard everything

1370
01:51:02,500 --> 01:51:07,700
But reward hacking definitely happens to people. Maybe that's what drug abuse is about

1371
01:51:23,900 --> 01:51:26,300
I'm not sure I understood everything you're asking I

1372
01:51:27,300 --> 01:51:29,300
Can repeat if

1373
01:51:31,260 --> 01:51:37,620
Yeah, so I was asking about reward hacking which is something you answered but also good-hearted law which is

1374
01:51:38,780 --> 01:51:42,060
When the measure becomes a target it ceases to be a good measure

1375
01:51:43,140 --> 01:51:49,260
If you're familiar with that law, I wanted to ask in the interest of self-regulation as something Julia mentioned

1376
01:51:49,980 --> 01:51:51,980
If there are thoughts on that

1377
01:51:57,300 --> 01:52:02,700
I guess good-hearted law doesn't apply because that's sort of a community thing

1378
01:52:03,060 --> 01:52:07,520
You know it gets the measure gets manipulated by actors

1379
01:52:08,180 --> 01:52:10,180
But if it's your own measure inside your head

1380
01:52:13,020 --> 01:52:16,900
It doesn't get get lost in its importance in that way

1381
01:52:20,380 --> 01:52:22,380
Self-regulation

1382
01:52:22,380 --> 01:52:24,380
I

1383
01:52:24,780 --> 01:52:26,780
Mean it's super important to see

1384
01:52:28,140 --> 01:52:33,420
Binocular rivalry the two different views. It's what that's the most important thing that's going on here that you know like oh

1385
01:52:33,900 --> 01:52:40,180
It's it's ugly to think that we're just doing one number of pain and pleasure or whatever it is rather than

1386
01:52:40,740 --> 01:52:42,740
The grand reviews we have ourselves

1387
01:52:44,100 --> 01:52:46,100
But they're both true

1388
01:52:46,540 --> 01:52:51,580
We have to self-regulate we have to decide what what is our defining goal

1389
01:52:52,020 --> 01:52:57,820
And it's it's we we have to decide but I've just said we don't have to decide because it's given and

1390
01:52:58,300 --> 01:53:03,700
It's in it and it's in us, but we don't know what it is and besides

1391
01:53:03,940 --> 01:53:09,220
It's it's there's many senses in which the values are more important than the rewards

1392
01:53:09,220 --> 01:53:11,220
I mean the rewards are the ultimate determination

1393
01:53:11,860 --> 01:53:18,420
But the values are what the consequences of that that come about in interaction with the world

1394
01:53:18,660 --> 01:53:26,620
In interaction with the world, I figure out that the thing I need to do in order to get the most reward I

1395
01:53:28,580 --> 01:53:30,580
Have to figure that out

1396
01:53:30,580 --> 01:53:32,820
The fact that the rewards are fixed doesn't help me

1397
01:53:33,340 --> 01:53:38,820
Doesn't inform me about what what I should do in the world to decide what to do in the world and what I should

1398
01:53:39,380 --> 01:53:42,060
Say to myself and say to others is my goal

1399
01:53:43,500 --> 01:53:46,940
That will be all about value and values come by

1400
01:53:48,420 --> 01:53:53,700
Through the complex interaction with the world and what is what is actually going to make me happier in the long run?

1401
01:53:53,740 --> 01:53:57,860
You know the rewards can't tell you that the words just tell you when you get there

1402
01:53:57,860 --> 01:53:59,700
I'll tell you if you're happy or not

1403
01:53:59,700 --> 01:54:03,860
They can't tell you what you should do and they can't tell you so you have to decide

1404
01:54:03,860 --> 01:54:09,900
You know which which career to have and and which person to be involved in and all those

1405
01:54:10,260 --> 01:54:15,580
Subtle hard decisions. It doesn't the fact that it's fixed doesn't help me make those decisions. You still have to do those

1406
01:54:15,580 --> 01:54:17,580
I

1407
01:54:18,260 --> 01:54:22,300
Think I probably also will and I'm looking at the clock will also only address part of your question

1408
01:54:22,300 --> 01:54:28,100
but um, I do think reward hacking occurs and people I think addiction is kind of the edge case of that

1409
01:54:28,540 --> 01:54:31,860
But I'm wondering whether you would also find there's a philosophical literature

1410
01:54:32,220 --> 01:54:34,220
Maybe just one philosophical author

1411
01:54:34,500 --> 01:54:40,420
On this idea of value capture and so I think one of the examples given in the phenomenon value capture is

1412
01:54:40,740 --> 01:54:42,740
people wear like a Fitbit

1413
01:54:42,860 --> 01:54:49,420
And the Fitbit tracks the number of steps that you take and obviously you need to take steps in order to maintain your health or like

1414
01:54:49,420 --> 01:54:51,740
whatever the man tries to tell you and

1415
01:54:52,420 --> 01:54:54,420
I'm just kidding. It's probably important and

1416
01:54:55,540 --> 01:54:57,540
But people become obsessed

1417
01:54:58,220 --> 01:55:02,620
With getting their steps. They're like they if they go for a walk and they don't have their Fitbit

1418
01:55:02,620 --> 01:55:06,220
It didn't count and that's an example of value capture. That's obviously like a very

1419
01:55:07,060 --> 01:55:12,220
Sort of relatively cute example of value capture, but I think this is something that happens like

1420
01:55:13,500 --> 01:55:19,620
Professionally, right? It happens like if we have certain goals like, you know, the pursuit of knowledge or contributing to something, you know

1421
01:55:19,620 --> 01:55:25,580
But we value capture in in publications or whatever, you know, lots of cases across not just an academic phenomenon

1422
01:55:25,580 --> 01:55:27,580
This is something that happens

1423
01:55:27,980 --> 01:55:32,060
Across different aspects of human life. I don't know if that's exactly what you're getting at but

1424
01:55:32,860 --> 01:55:40,300
In terms of self-regulation, I think understanding again the understanding is extremely powerful because if you recognize what's happening

1425
01:55:40,860 --> 01:55:46,380
You can regulate around that. So I think just based on our conversation a little bit yesterday based on your interest

1426
01:55:46,380 --> 01:55:50,820
I think I wonder I was wondering whether value capture might be something that you're also interested in looking at

1427
01:55:52,420 --> 01:55:58,300
Which is just a kind of nuanced version of how that happens in everyday life, I think

1428
01:55:59,900 --> 01:56:05,780
Thank you. Well, this is we are we are at two hours folks. So we're gonna we're gonna call him here

1429
01:56:05,780 --> 01:56:07,780
But we have a 30 minute a

1430
01:56:07,940 --> 01:56:10,020
30-minute coffee break now

1431
01:56:10,140 --> 01:56:17,780
So I'll invite you to to join us and I'm actually gonna ask that you let our speakers move to the coffee room before

1432
01:56:18,300 --> 01:56:25,020
Asking them questions so they don't get trapped here, which I've started to notice and do come back with us at 11 30

1433
01:56:25,020 --> 01:56:32,700
We have Abby Goldfarb Daniel Daniel Rock and Frank British on in machine learning in the workplace, which will be as as riveting as this one

1434
01:56:32,700 --> 01:56:34,700
Thanks so much also

1435
01:56:37,780 --> 01:56:40,780
You

