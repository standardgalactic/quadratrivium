WEBVTT

00:00.000 --> 00:05.880
Welcome back everybody. We're really glad to see you all here today and to those people online as well

00:06.760 --> 00:08.760
I'm still Jillian Hadfield

00:09.560 --> 00:14.600
And still director and chair nobody's taken me out yet of the Schwartz-Riesman Institute

00:15.840 --> 00:18.560
Looking really forward to these sessions today

00:19.680 --> 00:21.680
new questions and challenges and

00:23.240 --> 00:26.480
Lots of lots of wonderful lots of wonderful sessions

00:26.720 --> 00:31.680
Let's see is there anything else you want to make sure I say before we get started

00:34.160 --> 00:40.120
No, okay, I think we're gonna get we're just gonna move into our first session which is which I think

00:41.000 --> 00:48.960
Yeah, I'll stay up here to do the intro. So this first session is on the reward hypothesis which we got some mention of

00:49.640 --> 00:51.640
yesterday and

00:51.640 --> 00:53.640
And

00:53.960 --> 00:58.600
Will we're interested in this I think rich will give us who is the

00:59.360 --> 01:01.840
the source of the word hypothesis

01:02.520 --> 01:04.520
posited 20 years ago that

01:05.600 --> 01:12.600
Maybe rich you're gonna say this but let me for just summarize it here for that all we mean by goals and purposes can be well thought of as

01:13.440 --> 01:16.880
maximization of the expected value of the cumulative sum of a scalar

01:17.360 --> 01:19.360
Receive scalar signal or reward

01:20.040 --> 01:25.480
So the question that we're gonna be discussing is then is this a good model of

01:26.280 --> 01:30.840
Reinforcement learning is a good model for understanding human behavior and values. How far can it go?

01:31.280 --> 01:35.840
Can it guide normative decision-making for individuals and groups?

01:36.640 --> 01:37.880
and

01:37.880 --> 01:41.760
Totally delighted to have with us on this on this panel

01:43.280 --> 01:47.280
Rich Sutton who I neglected to actually

01:48.080 --> 01:54.320
Introduce yesterday when we started our session with blaze. So my apologies for that because he is

01:55.200 --> 01:56.560
world-class

01:56.560 --> 01:57.440
research

01:57.440 --> 02:05.680
Researcher and reinforcement learning as well as chief scientific advisor fellow in Canada CIFAR AI chair at Amy the Alberta machine intelligence Institute

02:06.040 --> 02:12.160
He's a professor of computing science at the University of Alberta and a distinguished research scientist at DeepMind

02:12.760 --> 02:20.320
He's been named a fellow of the Royal Society of Canada the Association for the Advancement of Artificial Intelligence and the Canadian Artificial Intelligence

02:21.160 --> 02:23.160
Association where he received a lifetime

02:23.440 --> 02:27.520
Achievement award in 2018 and as I mentioned yesterday, we're also very delighted

02:27.520 --> 02:32.640
He's on our advisory board at Schwartz-Reisman and joining rich today is Julia Haas

02:33.200 --> 02:37.240
Senior research scientist in the ethics research team at DeepMind

02:37.720 --> 02:44.840
She was previously an assistant professor in philosophy and neuroscience at Rhodes College and an affiliated researcher with

02:45.400 --> 02:50.880
and use humanizing machine intelligence grand challenge her research is in the philosophy of

02:51.160 --> 02:56.840
Cognitive science and neuroscience. She works on the nature evaluation and its roles and theories of the mind

02:57.120 --> 03:02.400
Your current work includes investigating the possibility of meaningful moral artificial

03:02.840 --> 03:09.440
Intelligence and I will also mention that Julia is this is a return trip to absolutely interdisciplinary for Julia

03:09.440 --> 03:13.560
Which we're also very grateful. It's wonderful to have people saying yes to those invitations

03:14.320 --> 03:18.520
But our previous one of course was online, so it's wonderful to have you here in person Julia

03:18.520 --> 03:21.680
And so rich, I think you're gonna get us started

03:22.480 --> 03:24.480
Thanks very much

03:33.400 --> 03:35.400
Thank you, Jillian

03:37.440 --> 03:41.760
Good to see y'all again. Let's try to have some more fun today

03:44.280 --> 03:46.280
My topic is

03:46.360 --> 03:50.720
Roughly the reward hypothesis, but the reward and other related

03:51.480 --> 03:54.880
Reductionist hypotheses because the reward hypothesis is pretty reductionists

03:55.600 --> 03:59.840
And I do have a slide just stating all so you can all see it

04:00.680 --> 04:04.560
All of what we mean by goals and purposes can be well thought of as the maximization

04:05.080 --> 04:11.120
With expected value of the cumulative sum of a received a scalar signal called reward

04:12.200 --> 04:13.400
so

04:13.400 --> 04:15.400
That's kind of a long

04:15.840 --> 04:17.320
sentence

04:17.320 --> 04:19.320
Sounds like it's got lots of little bits that

04:19.880 --> 04:21.280
that are

04:21.280 --> 04:23.280
intricate

04:23.320 --> 04:24.880
but

04:24.880 --> 04:28.160
But really it just says that maybe the goals of

04:28.920 --> 04:33.720
Whenever you want to talk about goals, maybe you can just talk about maximizing a single number and

04:36.720 --> 04:39.400
That is pretty reductionist and

04:40.880 --> 04:44.200
I'll be talking about that a little bit, but I don't want to start there

04:44.200 --> 04:48.880
I want to start by going back a bit. So this is my outline. I want to talk about intelligence and

04:49.520 --> 04:53.200
To what degree it's intrinsically tied up with the notion of a goal

04:53.720 --> 04:56.480
Which of course is the reward hypothesis is about

04:57.080 --> 05:00.040
And then we'll step into the reward hypothesis fully

05:00.920 --> 05:02.800
And then I also want to talk about you know

05:02.800 --> 05:09.720
What goes on inside because the reward hypothesis is not about what goes on inside the reward hypothesis like how you how?

05:09.720 --> 05:11.720
What's an appropriate framing of the problem?

05:12.240 --> 05:15.600
What's the appropriate framing of of a goal or a purpose?

05:16.320 --> 05:19.560
And it's not about how that purpose is achieved

05:20.280 --> 05:26.040
Okay, but that's what the agent the agent is the intelligent agent like we are the agents and

05:26.480 --> 05:31.880
the robots are the agents and what are the essential for

05:33.320 --> 05:36.000
What are they literally essential components, you know, that's

05:36.840 --> 05:43.920
That's something I want to talk about that. So and then and then some other hypotheses the value function hypothesis is really

05:44.480 --> 05:51.640
important to understand the implications of the reward hypothesis and then I'll have just really just want to slide we're

05:52.400 --> 05:56.880
Prepare us to start to think about how this might have implications for thinking about ethics

05:58.480 --> 06:00.840
Okay, so let's start with intelligence and

06:02.040 --> 06:03.880
Here's some

06:03.880 --> 06:05.440
quotes

06:05.440 --> 06:08.440
William James was the like the original psychologist

06:09.000 --> 06:13.240
His textbook was in 1890 and he spoke about

06:15.080 --> 06:17.520
Well, he didn't spoke about intelligence he spoke about mind

06:18.120 --> 06:23.400
So the hallmark of mind is attaining consistent ends by variable means

06:24.080 --> 06:31.000
What do you think about that? That's really talking about a goal right consistent ends from variable means something is varying its means in order to

06:31.240 --> 06:36.080
Achieve those consistent ends. That's really saying the hallmark of mind is

06:36.680 --> 06:38.680
goal-seeking

06:39.480 --> 06:42.280
And then there's the field of artificial intelligence which is

06:42.760 --> 06:47.440
Famous for not defining what intelligence is or what artificial intelligence is

06:49.800 --> 06:55.040
Yeah, for so long it just refused to do it and you know, I think that's not okay

06:55.320 --> 07:02.400
You have to everyone have a well-defined field. You have to have be clear about what your objectives are and what your subject is

07:04.600 --> 07:06.600
John McCarthy is the fellow who

07:08.040 --> 07:11.960
Invented the term artificial intelligence. He of course is one of the founding fathers

07:13.120 --> 07:15.120
But many years later

07:16.120 --> 07:18.120
He wrote down a specific

07:18.640 --> 07:24.800
Definition that's the one here that I that I like as the computational part of the ability to achieve goals

07:25.560 --> 07:32.640
The computational part the ability to achieve goals. I think that's a really interesting definition

07:35.840 --> 07:37.840
It's not the only definition

07:38.160 --> 07:42.120
Often intelligence is taken to be like mimicking people as in

07:43.120 --> 07:45.120
AI

07:45.160 --> 07:49.440
Seeks to reproduce behavior that we would call intelligent if it was done by people

07:50.040 --> 07:53.360
The classic Turing test is focusing on behaving like a person

07:54.040 --> 07:56.240
supervised learning the task is often to label

07:56.680 --> 08:03.760
Pictures say the same as a person would label those pictures and then the large language models, of course the large language models are all about

08:04.800 --> 08:06.800
mimicking

08:06.920 --> 08:08.920
Text generation by people

08:09.080 --> 08:12.320
Okay, so this is this is a major thing

08:13.240 --> 08:17.800
You notice aren't really any goals involved in mimicking people except, you know

08:17.800 --> 08:19.240
You could say mimicking people as a goal

08:19.240 --> 08:26.520
But really that's not what what I think goals are about goals are about affecting some change in the world

08:27.240 --> 08:32.840
Observing something and then you're being satisfied that you're mimicking it is not about a change in the world

08:33.560 --> 08:35.760
But you know think of all those systems

08:36.840 --> 08:42.000
That are mimicking there. They're not seeking to change their input at all. They're just seeking to mimic it

08:42.800 --> 08:44.800
Okay, anyway, so there's two definitions

08:44.920 --> 08:48.840
one is that intelligence is has to do with mimicking people and

08:49.280 --> 08:52.080
The other one is achieving goals and we might ask

08:52.760 --> 08:54.760
which is better and

08:55.920 --> 09:01.960
I want you to think about that and so I don't want to think about it with a little bit of sophistication

09:02.360 --> 09:07.160
Maybe this is kind of like yesterday, you know, I don't want us to jump to an answer because I want to think about what it means

09:07.160 --> 09:09.160
for there to be an answer and

09:10.000 --> 09:12.840
You don't really have any slides on this, but I do have this slide

09:13.800 --> 09:19.680
That just reminds us that you know a word like intelligence when you look in the dictionary you will find multiple

09:20.440 --> 09:22.200
definitions

09:22.200 --> 09:26.880
The second one being a military intelligence like spies and stuff

09:27.800 --> 09:31.760
So every word is like this and that's the way language is and it's it's good

09:31.920 --> 09:35.880
it's good that there are multiple definitions and multiple meanings for words and

09:36.760 --> 09:39.680
There so there isn't a sense that one is right or wrong. There's

09:40.760 --> 09:45.360
There's a sense in which they're useful for or not useful for particular purposes

09:46.000 --> 09:51.120
So it's our we have there's a free choice when you define a term. It's a totally free choice

09:51.120 --> 09:54.960
There's no right and there's no wrong in the in the definition of a word

09:55.520 --> 09:57.520
but there is

09:58.160 --> 10:00.800
Consequences right because you're you're choosing how to use it

10:00.800 --> 10:04.760
And so then you have to use it that way and you can decide whether for your purpose

10:05.240 --> 10:08.680
That's a useful way to define the term

10:09.200 --> 10:14.680
So the to the extent there is a right or wrong is just it's not it's it's a sense whether they're useful

10:15.480 --> 10:17.480
suited to purpose

10:17.800 --> 10:19.480
so

10:19.480 --> 10:21.480
Now let's go on and try to

10:22.840 --> 10:24.840
Assess the different meanings

10:25.880 --> 10:27.880
and

10:29.000 --> 10:33.680
I want to do that by reference to this quote from Ray Kurzweil

10:35.640 --> 10:40.800
He says that intelligence is the most powerful phenomenon in the universe

10:44.760 --> 10:53.640
So that's not a definition obviously it's it's like a property of what intelligence means something and then and then it's claiming that it's powerful

10:55.480 --> 10:57.320
But

10:57.320 --> 10:59.320
Just take it on his face for a moment

10:59.920 --> 11:07.820
Could intelligence be the most powerful phenomenon in the universe? I mean, what about you know black holes and supernova supernova very powerful

11:11.920 --> 11:19.800
But I think Ray means this literally and I think it's not crazy to mean this literally like you know

11:20.480 --> 11:22.360
supernova

11:22.400 --> 11:24.880
Are big but isn't it it's

11:25.600 --> 11:27.600
Isn't it also plausible that

11:27.640 --> 11:33.280
Well supernova have been around for billions of years. They've had billions of years to develop intelligence has been around for

11:34.120 --> 11:36.120
Few hundreds of thousands of years

11:36.560 --> 11:40.240
If you give give intelligence, you know a billion years

11:41.040 --> 11:44.240
Is it is it doesn't seem almost likely that?

11:45.000 --> 11:50.840
Intelligence if it if it still exists would would be doing things like moving the stars around moving the planets

11:51.280 --> 11:56.040
It would be a powerful phenomenon in the universe at that scale. I

11:57.360 --> 11:59.360
think it's

11:59.600 --> 12:03.760
It sort of expresses the ambition of what we want intelligence to be

12:05.480 --> 12:07.480
Okay, so

12:08.560 --> 12:12.360
So when when I suggested to you that it might

12:13.440 --> 12:18.160
Become a big deal and in a universal galaxy level

12:19.040 --> 12:24.120
What what I was thinking was could the fact that there are

12:25.320 --> 12:27.320
Agents in the world that have goals

12:28.000 --> 12:30.600
And it's best to think of them that way

12:31.360 --> 12:36.880
Could it be that powerful phenomenon eventually and that's what I want to say yes to

12:40.560 --> 12:46.360
For the other meaning could the ability to mimic people be such a powerful phenomenon

12:48.960 --> 12:52.080
I'm I think I think the answer is just no

12:53.360 --> 12:59.440
People are the powerful thing and so mimicking the thing that mimic will gain some power from the people

13:00.360 --> 13:04.320
But the the ability to mimic people is not powerful in this sense

13:05.800 --> 13:07.200
so

13:07.200 --> 13:14.560
That's my first conclusion that the powerful part of intelligence is not the ability to mimic people but the ability to achieve goals and

13:15.360 --> 13:16.800
so

13:16.800 --> 13:23.120
That's a point in favor of using the word in this way the ability to achieve goals

13:25.320 --> 13:27.080
Okay, now

13:27.080 --> 13:30.760
Let's look a little bit deeper into McCarthy's definition

13:30.880 --> 13:35.000
You define as the computational part of the ability to achieve goals

13:35.600 --> 13:42.320
So that computational part is meant to rule out like you I can achieve goals because I'm stronger as I'm faster

13:42.440 --> 13:44.440
And I have better sensors

13:44.680 --> 13:50.500
These would make you better able to achieve many goals, but it would not be because of your computations

13:51.000 --> 13:54.280
So we don't consider that to be intelligence

13:55.000 --> 13:57.000
There are at least that definition doesn't

13:58.120 --> 14:01.360
So moving that up a little bit to give myself give myself a little more room

14:02.560 --> 14:04.560
I think similarly

14:04.800 --> 14:09.240
You could achieve goals better if you're given knowledge about

14:09.960 --> 14:15.760
The world or the domain you were working in you know that would enable you to perform better to achieve goals better

14:16.600 --> 14:19.480
But it's not again. It's not because of your computations

14:19.480 --> 14:25.480
It's not your intelligence is because of the computations of where we gave you that knowledge gave you that help

14:27.000 --> 14:28.760
so I

14:28.760 --> 14:32.360
guess I have a conclusion out of that that I

14:33.000 --> 14:38.920
Want to say the intelligence is the computational and domain independent part of the ability to achieve goals

14:38.920 --> 14:40.920
maybe that's a refinement or a

14:41.440 --> 14:45.600
narrowing of McCarthy's definition, but I think it's in the spirit of

14:47.480 --> 14:52.040
Having a powerful phenomenon of Intel for intelligence being a powerful phenomenon

15:00.040 --> 15:02.040
Okay, so just to summarize that

15:03.040 --> 15:06.400
Mimicry and domain knowledge are not the powerful part of intelligence

15:07.400 --> 15:13.560
Mimicry is getting goal directed behavior without the goals or the processes that compute the behavior from the goals

15:15.800 --> 15:21.440
Injecting domain knowledge is a way of getting gold directed behavior without the processes for obtaining the domain knowledge

15:22.200 --> 15:24.640
So this is sort of the way I understand

15:25.240 --> 15:29.280
The limitations of large language models. They are the abilities

15:30.280 --> 15:34.480
But they were given to them and they don't have the ability to

15:35.520 --> 15:38.360
To develop that knowledge and that behavior themselves

15:39.400 --> 15:42.560
So they're they're both incomplete can't stand on their own

15:43.400 --> 15:46.640
These shortcuts don't have the power of intelligence. They can be very useful

15:47.520 --> 15:49.520
but but

15:49.520 --> 15:51.520
That shouldn't make them intelligence

15:52.240 --> 15:58.040
Using in the word in that way would weaken the search for an understanding of intelligence. That's powerful in Kurzweil sense

15:58.480 --> 16:01.680
Let me just say another another thing on that which is that

16:02.680 --> 16:08.760
intelligence the word intelligence AI AI in particular in today's world has cash a

16:10.240 --> 16:12.160
like

16:12.160 --> 16:14.560
Actually, it's not even just today's world. It's always been this way

16:15.200 --> 16:16.360
but

16:16.360 --> 16:22.960
What I mean is like for example, I went to the grocery store the drugstore the other day and I saw the aisle full of

16:23.840 --> 16:25.840
electric toothbrushes and

16:26.000 --> 16:31.200
You go to the electric toothbrush section today and they will say this toothbrush has AI in it. I

16:31.840 --> 16:33.840
Mean literally they will say that

16:34.000 --> 16:39.560
It has cash a to have AI to have intelligence is as viewed as a very positive thing

16:39.560 --> 16:41.840
And so everyone wants to be that way

16:41.840 --> 16:48.480
I hear that LG has a has a clothes washer that's has AI in it

16:49.480 --> 16:51.920
In the old days they used to make

16:52.680 --> 16:54.440
computer terminals and

16:54.440 --> 17:00.800
They would call the terminals intelligent terminals and all they would do they were only intelligent because they could use multiple fonts

17:02.160 --> 17:05.120
You know, so this has always been the case

17:06.800 --> 17:11.280
And so there's a tendency for everything to become AI

17:11.600 --> 17:17.880
More's law or the generalized Moore's law of increasing

17:18.760 --> 17:21.560
Computation per dollar that's gone on for a hundred years or more

17:22.760 --> 17:28.880
That is all about increasing computation. It's not about increasing not not it's not necessarily about increasing AI

17:29.640 --> 17:35.320
So there's a big there's a tendency to conflate the two trends that AI is becoming more important and

17:36.320 --> 17:39.680
Computation is becoming more plentiful. So all everywhere there's computation

17:40.160 --> 17:42.820
And that's that's what we're seeing with this

17:43.520 --> 17:45.200
Stretching of the term

17:45.200 --> 17:47.200
intelligence to cover everything

17:47.640 --> 17:52.960
That just uses computation. I don't think that's a useful direction to stretch the term

17:55.960 --> 18:02.080
Okay, I think now I'm ready for a joke and so I'm gonna I think I'm ready now to present my present my my joke

18:02.080 --> 18:05.680
Or my cartoon. This is my cartoon. You may have seen it

18:05.680 --> 18:12.040
To think that this all began with letting autocomplete finish our sentences

18:13.040 --> 18:16.480
So this I found this in the New York. I hope I'm not violating

18:17.440 --> 18:19.440
important copyright things anyway

18:21.200 --> 18:24.360
I'd like I really like this cartoon because it's

18:24.960 --> 18:28.360
It's exaggerated. It's making both fun making fun of

18:29.800 --> 18:33.600
Both the positive and the negative hype that's around AI

18:34.520 --> 18:42.080
The the positive the positive hype is that autocomplete finishing our senses will lead to you know, the robots

18:42.920 --> 18:44.920
being super powerful and taking over and

18:49.920 --> 18:52.760
Guess the negative hype is that you know if if

18:53.800 --> 19:01.120
Computers become powerful if we should robots become powerful they will subject us all to slavery as in as in this picture

19:02.080 --> 19:07.400
Okay, so humor is the best way to purify or

19:09.280 --> 19:11.280
our thoughts and and

19:11.800 --> 19:13.800
Think about controversial things

19:14.080 --> 19:18.280
Okay, so now I'm ready to talk more specifically about the reward hypothesis, but I hope you

19:18.880 --> 19:26.520
You've gotten the point the point, you know the word hypothesis about how we talk about purposes and goals and

19:26.960 --> 19:31.520
Intelligence is is really centrally about goals and purposes

19:32.240 --> 19:38.920
so if this is true if it's true that all goals and purposes can be thought of as as

19:41.800 --> 19:49.400
Maximizing a single number in this way, then it means that all of intelligence can be thought of as maximizing a simple number

19:49.920 --> 19:59.280
So this is one the multiple ways to develop this hypothesis one is to do it mathematically and formally and and

20:01.200 --> 20:05.320
Like you know, what's this? What does it mean to have a have a goal?

20:05.840 --> 20:14.080
It's an ordering on possible outcomes and what are they all possible different ways of doing an ordering and you're looking at this

20:14.080 --> 20:16.080
Carefully as a mathematical formal statement

20:16.440 --> 20:21.560
Like one might in do in economics or the theory of decision-making

20:22.880 --> 20:26.960
So that's that's the work this idea has been developed in that direction

20:26.960 --> 20:29.720
I'm and I'm referring to the work in the corner

20:30.120 --> 20:33.480
It's more recent work by Michael bowling and John Martin David

20:34.000 --> 20:38.800
Abel and Will Dabney where they formally assess it

20:39.440 --> 20:41.440
to the extent that it is

20:42.320 --> 20:48.280
Complete and includes anything else you might propose. So what are the other things that are ruled out? You may be thinking, you know

20:48.960 --> 20:52.880
What's ruled out would be things like considerations of risk in a special way

20:54.320 --> 20:56.320
consideration of multiple objectives

20:59.080 --> 21:03.600
You see it's all about the expected value rather than the distribution of possibilities

21:05.000 --> 21:08.640
Okay, now another another hypothesis that's been around

21:11.440 --> 21:13.440
The reward is enough hypothesis

21:14.000 --> 21:20.280
That there's intelligence and all of its associated abilities can be understood as subserving the maximization of reward

21:20.800 --> 21:28.200
So this is very similar. It's almost like the combination of the reward hypothesis and the McCarthy's definition of intelligence

21:30.400 --> 21:35.760
But notice all these things are about the goals are about the the problem the problem

21:36.480 --> 21:40.080
They're not at all talking about solution methods. They're just saying

21:41.040 --> 21:48.000
Reward might be enough to motivate and drive the achieving the achievement of the associated abilities, whatever they may be

21:49.280 --> 21:54.200
You can understand them as as being driven or subserving the maximization of reward

21:55.520 --> 21:58.000
Okay, so now I want to

21:59.000 --> 22:05.320
Is reward enough is a single number enough? It doesn't it's it's it's it doesn't seem like enough. I just wanted to

22:06.320 --> 22:10.360
Bring this on the table. I'm sure you felt that it seems too small a

22:11.040 --> 22:12.360
single number

22:12.360 --> 22:14.280
coming from outside

22:14.280 --> 22:16.200
the agent

22:16.200 --> 22:21.920
You know people seem to choose their own goals. I mean, that's one of the biggest things we see for ourselves

22:22.280 --> 22:25.200
We define ourselves by the goals we set out to achieve

22:26.920 --> 22:33.440
Reward just seems too small too reductive. It's definitely reductionist hypothesis. It's and as reduct being reductive

22:33.440 --> 22:35.240
It's kind of demeaning

22:35.240 --> 22:40.720
Surely our goals are grander than maximizing, you know pleasure and pain or some number

22:41.160 --> 22:45.720
You know, we have things like raising a family saving the planet making the world a better place

22:46.200 --> 22:48.120
contributing to human knowledge

22:48.120 --> 22:50.120
Not just these small things so

22:50.360 --> 22:58.360
So that's really the tension. I think around the reward hypothesis that it seems like it seems too small and yet it seems

22:58.760 --> 23:04.280
We keep being driven back to it as we try to get formal as we try to be clear about goals

23:04.840 --> 23:06.080
we're

23:06.080 --> 23:08.080
We're driven back to it because it's clear

23:08.720 --> 23:10.520
because it's

23:10.520 --> 23:12.520
experiential

23:14.280 --> 23:19.040
Kind of grounds things it gives us a well-defined place to proceed

23:19.880 --> 23:21.880
so now a few slides

23:22.320 --> 23:24.320
exemplifying how

23:24.920 --> 23:33.080
Even though we're uneasy with this idea we keep coming back to it we what is we I mean, it's an interdisciplinary idea

23:34.440 --> 23:36.440
This is the modern view about

23:37.320 --> 23:41.920
About how our brains work is that there is there is a measure of

23:42.560 --> 23:44.560
Pleasure and pain and then

23:44.560 --> 23:47.120
There are calculations. I'll get into a little bit later

23:47.120 --> 23:53.520
But it's totally consistent with this and not just we it's like all animals have like a dopamine center

23:53.520 --> 23:55.240
and

23:55.240 --> 24:01.480
This is a good it has been true that a good way to think about them has been in terms of a scalar outcome

24:02.320 --> 24:04.240
Not entirely

24:04.240 --> 24:07.160
Okay, so let me go through some facts

24:09.120 --> 24:11.760
This is some of these slides a little more detailed than we need

24:12.720 --> 24:17.440
But I just want to refer that within AI which is also uneasy with the idea of reward

24:18.200 --> 24:21.480
It's it's become more comfortable with it over time

24:21.880 --> 24:25.840
So the very earliest AI systems were all formulated their goals as

24:26.360 --> 24:28.360
Attain the state of the world

24:28.680 --> 24:30.800
Okay, which is which is very different from

24:31.720 --> 24:34.740
Maximizes number coming into your into your mind

24:35.920 --> 24:42.240
It's as if we can access directly the states of the world which which we cannot of course we have to infer them from our

24:42.640 --> 24:44.640
from our sensations

24:45.120 --> 24:47.120
but

24:47.440 --> 24:51.960
And and I'm saying it's even true the latest version of the standard AI textbook

24:52.440 --> 25:00.080
Russell Norvig it still talks about goals primarily in terms of states of the world and not in terms of experience not in terms of reward

25:00.840 --> 25:04.720
But it also has chapters on reinforcement learning and those those all use reward and

25:05.640 --> 25:13.200
With the rise of machine learning within AI the reward formulation has becoming more and more standard as in planning and mark-up decision processes

25:13.960 --> 25:15.680
and

25:15.680 --> 25:19.920
We can look at Yanlacoon Yanlacoon who was who's sort of an anti

25:20.680 --> 25:26.000
Reinforcement learning person he now admits that if the if the mind is a cake

25:26.640 --> 25:28.640
This is his metaphor

25:29.640 --> 25:34.840
Is that if the mind is a cake then reinforcement learning is a tiny part of it's like the cherry on top

25:35.360 --> 25:40.600
but he thinks the reinforcement learning acknowledges that reinforced learning is necessary because goals are necessary and

25:41.360 --> 25:48.520
Either the substance of the of the cake is maybe doing prediction or unsupervised learning and so it those parts are not

25:48.880 --> 25:52.160
Goal oriented. They're just trying to predict and understand the world

25:52.160 --> 25:57.920
And I shouldn't say just they're trying to predict and they're trying to understand the world and model the world those don't

25:59.320 --> 26:02.280
Don't have goals in them. They're just

26:03.000 --> 26:07.280
I said it again just gotta watch out for that little word just they are

26:08.280 --> 26:12.600
Understanding the the truth of the world as separate from

26:13.600 --> 26:15.600
Trying to direct that in any particular way

26:16.080 --> 26:21.720
Okay, but anyway, he's got the cherry on the top and to me. It's a cherry on the top of the cake. So it's pretty important

26:22.640 --> 26:24.600
and

26:24.600 --> 26:30.000
Here's another one from classic AI artificial intelligence

26:31.000 --> 26:35.120
There's this these cognitive architectures since the 80s

26:35.200 --> 26:43.840
They're very symbolic and using production rules and since like 2008 and they've included reward as part of it as as a as a basis for

26:45.080 --> 26:47.600
The the goals of the system

26:48.120 --> 26:53.000
So single numbers have becoming more prominent in AI as a formulation of the goal

26:53.720 --> 26:55.120
now this

26:55.120 --> 26:58.600
now let's talk more in a more interdisciplinary sense and

26:59.600 --> 27:03.120
I recently wrote a little paper where I just sort of said, oh, you know

27:03.520 --> 27:06.840
There's a lot of commonality between many many fields thinking about

27:07.600 --> 27:16.120
Agenthood and goals and purposes in mind, you know in psychology control theory AI economics neuroscience operations research at least these six

27:16.400 --> 27:19.360
You can find basically the elements

27:19.960 --> 27:26.200
In these two pictures the first picture is the agent interacting with the world or the environment and

27:26.880 --> 27:32.960
So this is the basic picture of a of a decision maker. He

27:33.920 --> 27:40.640
Sees information from the world his observations and he picks actions and then he gets back a

27:41.240 --> 27:48.440
Scalar measure of performance that he with the objective is to maximize it. That's sort of accepting the reward framework

27:49.000 --> 27:51.000
You will find this

27:51.960 --> 27:57.120
Of course these ideas very basic, but even when you look inside the agent now

27:57.120 --> 28:03.480
So I'm gonna look inside the agent how the problem is solved you can find some of the common elements and

28:04.640 --> 28:06.640
So there are four of them

28:07.160 --> 28:10.400
But let's talk first about the core two

28:10.840 --> 28:16.640
So I'm gonna block some of these out. Yeah, the core two is the perception and

28:17.280 --> 28:20.760
The policy the reactive policy so perception

28:21.520 --> 28:25.560
Takes in observation so observations are like, you know, your sensory input and

28:26.800 --> 28:31.520
From that you construct a sense of where you are. That's that's the state representation

28:31.520 --> 28:35.600
And then you choose what to do based upon the state representation

28:35.920 --> 28:39.960
The difference between the two boxes is that the policy is

28:40.440 --> 28:46.160
Memoryless right you can't use old states to decide what you're gonna do now the whole point of a stage as it says

28:46.160 --> 28:49.880
This is where I am now this I should decide what to do based on where I am now

28:50.440 --> 28:51.760
it's

28:51.760 --> 28:55.920
And it's the job of perception to construct that sense of where you are now

28:57.000 --> 29:04.880
So perception is recurrent it involves the last thing you did the last observation and the the previous state

29:05.680 --> 29:07.680
Now

29:09.720 --> 29:11.720
This is a complete behaving system

29:11.920 --> 29:16.440
Observation comes in flows through perception and the policy produced in action

29:16.440 --> 29:21.880
That's all you need to be a complete behaving system and you notice reward is kind of hanging out there

29:21.880 --> 29:24.760
It's not really doing anything and that's because

29:26.000 --> 29:31.520
This is a complete behaving system, but not a system that's made up for changing what that system does

29:31.920 --> 29:36.040
So let me dig do one more dig at large language models. They are this part, you know

29:36.040 --> 29:38.040
they are a way of

29:38.240 --> 29:41.240
transforming the sequence of words into a

29:42.480 --> 29:47.020
Memory of where you are and then deciding which word to output

29:48.080 --> 29:55.120
But it's not a way of changing that the there isn't a once your large language model is in place is running in the world

29:55.360 --> 29:58.640
It can't change. It's it's just these two parts now

29:58.640 --> 30:02.440
You can change in the sense that perception can accumulate your a

30:03.120 --> 30:09.800
More refined state as you get more observations, but you can't get a change in the policy for example

30:10.520 --> 30:13.200
or or a change in the

30:14.080 --> 30:16.080
Function that is implemented as perception

30:20.200 --> 30:23.400
Maybe I'll dwell on this a little bit more and then on the

30:25.920 --> 30:27.920
interdisciplinary aspect

30:27.920 --> 30:34.960
So we see just remember that all these different fields have used different names like if you're in psychology you talk about

30:35.720 --> 30:38.760
so the action you talk about the response and

30:40.160 --> 30:43.000
in control theory you would talk about the

30:43.760 --> 30:47.280
the control instead of the policy they talk about the

30:47.960 --> 30:49.600
control law

30:49.600 --> 30:51.400
instead of instead of

30:51.400 --> 30:55.360
Perception in in control theory they talk about a state estimation

30:56.000 --> 30:57.280
box

30:57.280 --> 31:03.400
In psychology the observation would be a stimulus and the reward might be reward

31:05.280 --> 31:07.800
But actually the subtleties of the

31:09.200 --> 31:11.800
Connotations of the word are somewhat different in psychology

31:12.560 --> 31:14.560
So

31:17.560 --> 31:23.120
I think it's it's kind of interesting to try to find a relatively neutral language that can

31:23.840 --> 31:26.280
Apply to all these all these different fields

31:28.240 --> 31:30.000
Okay, so

31:30.000 --> 31:35.040
I'm unblocked the transition model the transition model is supposed to be the model of the world

31:35.680 --> 31:39.920
It's gonna have your domain major place where domain knowledge is

31:40.760 --> 31:42.760
Okay, I'm not gonna talk about that today

31:43.160 --> 31:48.280
So let's get rid of that. Anyway, but it would do planning. That's how you do planning

31:48.760 --> 31:50.760
We're not gonna talk about that

31:52.840 --> 31:58.280
But I do want to talk today as part of talking about the reward hypothesis about the

31:58.840 --> 32:03.640
The other box the value function box is the value function is the major source of

32:04.840 --> 32:06.840
Learning input of changes in the policy

32:07.840 --> 32:12.760
And this notion of a value function is also common to all these fields

32:17.760 --> 32:20.440
Hey, I through reinforcement learning has value functions

32:21.680 --> 32:23.680
psychology has has

32:25.240 --> 32:30.160
Reinforcers and secondary reinforcers and and it's built up in the same way

32:31.120 --> 32:33.120
and

32:33.800 --> 32:38.440
There are issues of proxy objective functions and control theory that are all about

32:39.160 --> 32:43.340
Constructing value functions. So what is this value function thing? Let's talk about that

32:44.520 --> 32:49.760
Oh, but I couldn't I couldn't resist I want I got in this x1 extra slide where I'm just

32:50.200 --> 32:55.040
It does remind me to say to say one important thing that even though there's only one objective

32:55.480 --> 32:59.760
the reward to maximize reward the agent can well as as as

33:00.480 --> 33:02.040
Means as

33:02.040 --> 33:08.720
Solution strategies it might have it might pose some problems for itself other problems for itself

33:08.720 --> 33:14.840
Like you might learn how to how to walk or how to drink a glass or how to navigate to the monk school

33:15.520 --> 33:19.000
You learn all those things that they're not your main task

33:19.440 --> 33:25.120
But it might if you learn how to do those things it may be useful for solving the main task. So you you

33:26.120 --> 33:30.960
My theory of of mind and the next step of that theory would be that we

33:31.360 --> 33:37.280
We pose some problems for ourselves and solve those and then use those skills that we develop working on the sub problems

33:37.280 --> 33:40.680
In order to solve the real problem, which is to maximize reward

33:41.920 --> 33:46.080
Okay, so value functions. We're getting almost done

33:46.840 --> 33:48.840
value functions

33:49.440 --> 33:54.800
So reward and value reward defines what's good. So we seek a policy that maximizes reward

33:54.800 --> 33:57.160
That's done defining the problem

33:58.080 --> 34:03.880
But reward is often delayed and that makes it hard to learn a policy using reward

34:03.880 --> 34:10.040
And so instead of working directly with reward value functions map states to predictions of the future reward

34:10.120 --> 34:12.840
if you have prediction of the future that includes the

34:13.240 --> 34:19.800
Delay and if you can bring that into the into the present if you can predict now what the future rewards will be that

34:20.320 --> 34:22.080
enables you to

34:22.080 --> 34:25.920
Eliminate the delay and makes finding a good policy much easier

34:26.680 --> 34:30.200
Maybe that's intuitive you think about I always think about playing chess

34:30.320 --> 34:33.040
You know the reward is checkmating your opponent

34:33.040 --> 34:38.200
And so then you get maybe a plus one for checkmating a minus one for losing is zero for drawings

34:38.200 --> 34:39.480
so that is

34:39.480 --> 34:40.840
and of course

34:40.840 --> 34:47.000
Formally you get a reward all during the game, but they're all zero and you only get an interesting reward at the end and

34:47.360 --> 34:51.080
So there's a delay between any good move or poor move

34:51.080 --> 34:58.060
You might make early in the game and and whether you won or you lost and so of course what we would all do is we would

34:58.440 --> 35:02.880
Learn to predict. Am I going to be checkmated in this game?

35:03.320 --> 35:09.080
Okay, so that's a prediction of the future and it's often called an evaluation function or a value function

35:09.080 --> 35:13.760
It's a prediction of the reward. So I think it's it's

35:14.760 --> 35:18.160
Well, this is the the value function hypothesis is that

35:19.400 --> 35:22.920
Forming such value. I guess that's that's that's right on my slide here

35:23.360 --> 35:28.320
The value function hypothesis is that all efficient methods for solving

35:29.440 --> 35:33.960
Sequential decision problems, which is what just means decision problems over and over again through time

35:35.600 --> 35:37.600
They learn or compute

35:38.240 --> 35:41.000
Value functions as an intermediate solution step

35:42.000 --> 35:48.520
So when you're gonna play chess you learn the sense of am I when you're losing and then you know when you make a bad move and now

35:48.520 --> 35:52.040
You thought you were winning and now you're losing you see then no

35:52.040 --> 35:53.960
That's the critical part

35:53.960 --> 36:00.720
Where you made the mistake and you can use that to assign credit to your policy and change your policy in a much better way

36:02.200 --> 36:05.000
Okay, so that's all sounds awfully technical

36:06.440 --> 36:09.920
Let's go back to philosophy go back to Plato, you know

36:10.720 --> 36:17.620
You can find in Plato talking about good and evil and pleasure and pain and he will say things that are a lot like this

36:18.680 --> 36:26.120
These are some quotes even enjoying yourself. You call evil whenever it leads to the loss of a pleasure greater than its own

36:26.600 --> 36:31.120
Or it lays up pains lays up pains that outweigh its pleasures

36:31.960 --> 36:36.160
Isn't it the same when we turn back to pain to suffer pain you call good

36:36.880 --> 36:43.000
When it either rids us of greater pains than its own or leads to pleasures that outweigh them

36:43.200 --> 36:47.040
So what does he say he's saying good and evil are about the sum of upcoming reward?

36:48.320 --> 36:50.960
Which is what we try to predict with a value function

36:51.800 --> 36:58.360
So basically it's all hedonism. It's all hedonism, but value functions make it hedonism with foresight

36:59.320 --> 37:05.680
Okay, that's the story the main story of reward and value how we use them in decision-making to change our policies

37:07.040 --> 37:09.440
But I want to try to do one more thing

37:11.160 --> 37:15.640
I get you understand the way it works in a little bit more

37:16.560 --> 37:22.560
I don't know if this is gonna work, but let me try I call it the mystery of expectation and reinforcement

37:22.760 --> 37:27.280
I call it the mystery of expectation and reinforcement now

37:27.840 --> 37:33.040
Superficially those are new topics. I haven't talked about expectation and haven't talked about reinforcement

37:33.120 --> 37:40.440
But by expectation I mean like the prediction of reward that the value function is the value function learns predictions i.e

37:41.400 --> 37:43.920
expectations of upcoming reward and

37:45.640 --> 37:47.640
Reinforcement reinforcement

37:48.640 --> 37:53.200
Is not reward reinforcement is the

37:55.160 --> 37:59.680
Moment-by-moment signal that reinforces the behavior at that time

37:59.840 --> 38:06.680
So it's really your sense of did I do something good or did I do something bad is should I stamp what in should I stamp in what?

38:06.680 --> 38:10.920
I did or not so reinforcement is not reward because there are many cases

38:10.920 --> 38:16.080
And we just alluded to some of them where you get say a positive reward, but you

38:16.880 --> 38:18.880
View it negatively

38:19.520 --> 38:24.760
And so expectations play a role there so expectations

38:25.720 --> 38:29.680
Contribute negatively to reinforcement in the sense like you know I went to the movie

38:29.680 --> 38:34.320
I thought it'd be a really good movie and I went there. I was just kind of so so so actually I feel bad about it

38:35.280 --> 38:37.480
so so expectations are

38:38.640 --> 38:43.080
Negatively to the reinforcement movie was just so so but because you expected it to be good

38:43.560 --> 38:48.520
That gave you a bad feeling about it a bad negative reinforcement

38:49.720 --> 38:55.160
But expectations also contribute positively to reinforcement like when you

38:57.360 --> 39:06.600
Expect to get reward that that is a reinforcing event if you if you I don't know are given a million dollars

39:07.600 --> 39:15.680
That's a good thing, okay, but it's actually not really reward reward becomes because you expect to have a million dollars and be able to use it

39:15.680 --> 39:17.680
you know to buy

39:17.680 --> 39:23.400
Good food or anything you might want so that would be the reward and the expectations are also positive

39:23.400 --> 39:30.800
And so that's the mystery. How can expectations contribute both positively and negatively it is in just a common-sense way and

39:31.840 --> 39:33.840
the answer is over time and

39:34.440 --> 39:41.600
So just consider this simple case of evolving through three states states one two and three and

39:43.000 --> 39:46.120
This is like your life some life moment by moment

39:46.840 --> 39:55.320
so three moments two transitions and I'll just take it that your expectations your predictions your values of the three states are

39:56.240 --> 40:00.080
zero ten and zero and then

40:01.080 --> 40:07.160
The rewards on the when you experience these three states are zero and eight

40:07.160 --> 40:10.920
Okay, so now I want to ask you the question or ask you to ask this question of yourself

40:11.560 --> 40:13.560
How do you feel about the first transition?

40:14.160 --> 40:17.920
So what's the reinforcing effect is it positive?

40:17.920 --> 40:22.840
It's negative like is it good is it bad as you went from state one to state two and

40:23.480 --> 40:26.640
It's I want you to give me a number because it's all coming down to numbers

40:27.640 --> 40:34.280
So what number would you give to that transition from state one to state two

40:35.840 --> 40:43.200
Expectation was zero now. It's ten the reward was zero. Do you feel good? You feel bad?

40:46.200 --> 40:48.200
Anybody

40:49.880 --> 40:52.720
John's giving me a thumbs up she was giving me a thumbs up

40:53.240 --> 40:56.720
Yeah, I think we're gonna say this is like a plus ten experience because

40:57.560 --> 41:00.440
You weren't expecting anything and you didn't get anything right away

41:00.440 --> 41:05.080
But you now you now you expect to get ten in the future. That's like the getting the million dollars

41:05.600 --> 41:07.600
And now the second transition

41:08.400 --> 41:10.120
What do you expect?

41:10.120 --> 41:15.520
Expecting ten you got eight and now you don't expect anything in the future

41:16.720 --> 41:18.720
That's gonna be like minus two

41:18.840 --> 41:23.520
Okay, so that all makes sense. It's very intuitive. Okay, so if you just

41:24.120 --> 41:29.080
Make a formula out of what you you've just understood is as obviously what should happen

41:29.200 --> 41:32.720
You're gonna end up with temporal difference learning the fundamental algorithm of of

41:33.600 --> 41:40.000
Reinforcement learning. Okay, so the the thing you just formed in your head was that the reinforcement is sure it's the reward

41:40.560 --> 41:45.600
But beyond that it's the change in your expectation from one time step to the next

41:46.360 --> 41:49.320
As has the expectation gone up as it gone down

41:49.320 --> 41:54.480
And so it's the sum of the reward and the change in the expectation because it's a change

41:54.480 --> 41:59.120
It's a temporal difference temporal difference just means time difference, which is just change

41:59.440 --> 42:04.560
It's the change in your expectation contributes to the reinforcement. And so this is the

42:05.280 --> 42:07.280
temporal difference error and

42:07.560 --> 42:09.560
also called the reward prediction error and

42:10.320 --> 42:14.360
I need to finish up, but I did want to mention since it's such an interdisciplinary

42:15.360 --> 42:19.720
Conference that this theory of brain systems as

42:20.720 --> 42:22.720
Following TD learning is one of the most

42:23.480 --> 42:29.200
Important interactions ever between engineering science as a neuroscience is a big big thing around the turn of the century

42:30.400 --> 42:34.960
And finally my last slide is on ethics

42:35.600 --> 42:41.320
So what we've talked about rewards are a good way to think about the ultimate goal and value functions

42:41.440 --> 42:47.960
Which are predictions of reward are a good way to think about how that goal is achieved. All this is neat and complete

42:47.960 --> 42:51.480
It's a good theory of a single agents decision making

42:51.840 --> 42:58.760
But it's it doesn't have what we would expect what we might hope to get from a theory of ethics is that there would be some

42:59.040 --> 43:01.440
universality to it there would be a

43:02.320 --> 43:09.600
Reason to for different agents to have similar values, you know, that's what that's that's what I understand ethics to be about

43:09.600 --> 43:11.600
It's when we reach for some

43:11.800 --> 43:15.900
universality rather than just an individual and what we have so far doesn't have any

43:16.480 --> 43:19.480
element of that and I'm gonna now admit that I have no

43:20.040 --> 43:22.040
no sense how

43:23.400 --> 43:29.680
Any universality could occur and so I want to propose that maybe there is no universality there's a need to be universal

43:30.080 --> 43:33.200
Sality for there to be ethics. Maybe we just

43:34.200 --> 43:40.200
Many of us have there is an aspect of what we do of commonality between many people and

43:40.800 --> 43:45.800
That that's what ethics is about. It's so this is a reductionist hypothesis again

43:45.800 --> 43:51.440
that maybe ethics is just values that are happened to be held in common by many agents and

43:52.160 --> 43:54.160
so even though

43:54.600 --> 43:58.840
Even things that are held in common by many agents won't be held in common by all agents

43:59.840 --> 44:03.120
The lion and the gazelle will have different values

44:05.680 --> 44:11.880
So that's really all I wanted to say and I thought it would be a good basis for starting a discussion of ethics

44:13.880 --> 44:15.880
Thank you very much

44:19.080 --> 44:21.080
You're up

44:28.840 --> 44:30.840
Jinx

44:32.040 --> 44:36.960
Thanks, Rich, I'm really glad we got a chance to do this and thanks to Jillian for the same reason

44:38.280 --> 44:44.000
So I'm gonna start with just a little bit of housekeeping disciplinary housekeeping just to make sure we're all on the same page

44:45.400 --> 44:47.400
So like any paradigm

44:48.480 --> 44:53.280
Reinforcement learning is going to have certain technical and foundational commitments

44:54.080 --> 44:55.720
and then

44:55.720 --> 45:02.680
Loss of different versions of those frameworks are going to add certain assumptions and relax others

45:03.040 --> 45:04.640
And so just before I get started

45:04.640 --> 45:09.080
I want to make clear two assumptions that I'm committing to because they will inform kind of what I say next

45:09.120 --> 45:15.440
I'm gonna call this RL star or sometimes they can also be called RLDM like reinforcement learning and decision-making I

45:16.880 --> 45:21.120
Think it also make clear maybe where where Rich and I agree and sometimes might disagree

45:21.760 --> 45:23.760
So the first assumption

45:23.760 --> 45:25.760
In my kind of particular

45:25.960 --> 45:31.280
Version of reinforcement learning is that there is in fact something really special

45:31.520 --> 45:38.000
About reinforcement learning when it comes to understanding the mind, right? And so I think this is an assumption we share we both subscribe to this assumption

45:39.520 --> 45:43.960
Peter Diane who just got mentioned and put up as like a little plastic bobble head and yell these

45:44.800 --> 45:50.680
Put forward something like the following claim so reinforcement learning algorithms such as the temporal defense learning

45:50.800 --> 45:57.800
Rule apparent to be directly instantiated in neural mechanisms such as the phasic activity of dopamine neurons

45:58.000 --> 46:01.920
That reinforcement learning appears to be so transparently embedded has made it possible

46:02.440 --> 46:06.520
To use it in a much more immediate way to make hypotheses about and

46:06.880 --> 46:14.040
Retradictive and predictive interpretations of a wealth of behavioral and neural data collected in a huge range of paradigms and systems, right?

46:14.040 --> 46:16.040
So there's this

46:16.040 --> 46:18.040
Is the sound coming out already?

46:18.320 --> 46:20.400
Yeah, I've been wondering that too. So thanks, Rich

46:21.000 --> 46:27.080
There's a star beside instantiated for the philosophers of science in the room. Just relax about that

46:27.080 --> 46:33.160
It doesn't matter we can just say look reinforcement learning is shockingly surprisingly good about capturing something of the mind

46:33.160 --> 46:36.400
We don't have to commit to instantiate. It's one of those weasel words we talked about yesterday

46:37.160 --> 46:40.440
It's a quote from them, but that's just not what we don't want to talk about here

46:40.440 --> 46:42.440
There's something about reinforcement learning

46:43.080 --> 46:48.480
That is extremely powerful when it comes to understanding the mind whether it's instantiated or not

46:48.480 --> 46:54.120
So that's the first commitment that I'm making here the second commitment where we may disagree although

46:54.600 --> 46:55.680
sudden

46:55.680 --> 47:00.680
June 21st 2023 suggest maybe that's not the case is that

47:03.040 --> 47:10.400
Mines like ours so personally assigned subjective rewards somehow so there's a big debate about this question of where do rewards come from?

47:10.440 --> 47:15.920
right and the kind of classic view is that reward comes externally and

47:16.480 --> 47:19.360
That is an important tenet of some frameworks of reinforcement learning

47:19.880 --> 47:26.000
But I'm gonna go ahead and say and this is kind of an agnostic claim that the mind somehow

47:27.640 --> 47:32.120
Internalizes reward there are good competing theories for how that takes place

47:32.440 --> 47:36.760
Such as by Chris and refilled it doesn't matter. I just want to say that the mind somehow

47:37.400 --> 47:39.400
Internalizes reward in ways that are important

47:39.920 --> 47:44.280
So that's gonna be a commitment where we perhaps depart from one another

47:44.840 --> 47:47.780
But it's gonna inform kind of how we want to go forward about this

47:48.520 --> 47:50.520
Okay, so that's a bit of ground-clearing

47:50.960 --> 47:52.960
What I want to use my time

47:53.040 --> 47:56.520
For today and kind of as a basis or a backdrop for the discussion

47:56.520 --> 48:01.880
Let's make three claims about kind of this version of reinforcement learning and what it means

48:02.440 --> 48:06.800
For understanding the mind and so the first question that we have right is you know

48:06.800 --> 48:10.560
Is the reward hypothesis a good model for understanding the mind?

48:11.240 --> 48:12.440
And

48:12.440 --> 48:17.680
As rich has pointed out, you know, it's been kind of transformative in the decision sciences and all those schools that we talked about

48:18.680 --> 48:24.760
Cognitive neuroscience computational neuroscience psychology economics increasingly. I think in philosophy

48:24.760 --> 48:26.520
It's like making inroads in philosophy

48:26.520 --> 48:31.320
But I would actually say the first kind of claim that I want to make is that it's still underappreciated

48:32.040 --> 48:39.000
In how much it should and can transform my understanding of what the mind is fundamentally in the business of doing

48:41.040 --> 48:44.840
So this is kind of the cartoon dialectic slide, right?

48:44.840 --> 48:49.280
I think when we think of the mind we have historically but even in

48:49.680 --> 48:54.880
Recent kind of cognitive scientific history. So we've seen this in Joel's slide in Blaise's slide

48:55.560 --> 49:00.080
Yesterday is you know, when we think of what the mind is fundamentally in the business of doing

49:00.600 --> 49:07.120
As how Glenn put it, you know, it's about thinking. It's what intellect. It's it's fundamentally kind of an epistemic machine

49:07.360 --> 49:09.360
It's what the mind is a computer

49:09.480 --> 49:15.840
And one of the ways that we can put that is that the mind is in the business of performing computations

49:16.400 --> 49:24.240
over representations of descriptive matter of fact, which is just to say the mind is in the business of making better and worse kind of

49:24.920 --> 49:29.960
Assessments of what's going on out there in the world. Is it raining? You know, what day of the week is it?

49:29.960 --> 49:33.520
It's about knowing the world so that we can move around in it

49:34.360 --> 49:39.440
In various ways and so more or less factive representations of the world out there

49:41.120 --> 49:47.400
And I hope Blaise's ears are ringing right now because he basically went up yesterday and said, you know

49:47.560 --> 49:52.280
Maximum entropy first in all these people, you know, a new way of putting these kind of very epistemic

49:53.080 --> 49:55.400
Conceptions of mind is that it's all about prediction

49:55.920 --> 50:01.840
You know, the mind is a prediction machine and all we do is make predictions about what's out there in the world

50:01.840 --> 50:06.920
Then we update them. So but again, it's it's essentially about knowing in some lowercase case

50:07.400 --> 50:09.640
Sense of what the mind is all about

50:10.320 --> 50:11.880
And I really disagree with that

50:11.880 --> 50:18.120
I think the thing that we learn from the reward hypothesis and kind of the reinforcement learning framework that follows from it is

50:19.120 --> 50:23.000
That the mind not only performs computations over

50:23.480 --> 50:27.720
Representations of descriptive matters of fact, but the mind also

50:28.720 --> 50:33.520
Fundamentally performs computations over those representations as better or worse

50:34.280 --> 50:36.280
So we are continually

50:37.120 --> 50:41.160
Representing the world as we move around in it, you know what this room looks like

50:41.880 --> 50:44.000
What day of the week it is whether it's raining or not

50:44.160 --> 50:51.360
But as part and parcel of that process, we are also continually representing all of these states as

50:51.920 --> 50:54.600
Better or worse with respect to our goals

50:54.800 --> 51:00.360
So what we're doing is we're laying over these fabrics kind of of the states and assigning

51:01.120 --> 51:07.360
Value in the way that rich just described and so we're not just seeing whether it's raining, but oh, it's raining

51:07.360 --> 51:10.800
Here's what that means for me or oh, it's raining. I live in London, of course, it's raining, right?

51:10.800 --> 51:15.320
These things can matter more or less, but that we're not just experiencing the world. We're

51:16.280 --> 51:22.040
Continually evaluating and so I think one of the things that the report hypothesis and

51:22.560 --> 51:28.080
Tells us is that the mind is not just a thinking machine. It's a valuation machine. It's continually

51:28.880 --> 51:30.400
evaluating

51:30.400 --> 51:35.840
Now one of the examples that I like to use for this is kind of the thin end of the wedge. It's very narrow case

51:36.440 --> 51:38.960
But it's the phenomenon of vernacular robbery. So

51:39.720 --> 51:44.440
vernacular rivalry occurs when you place some kind of division between the left eye and the right eye

51:45.200 --> 51:51.880
And you show one stimulus to one eye and another stimulus to the other eye. So in this case borrowed from

51:53.360 --> 51:57.040
Feldman Barrett's lab is one eye sees one eye is presented

51:57.040 --> 51:59.760
I should say with a house and one side is

52:00.240 --> 52:02.720
presented with a face and what you might expect

52:03.280 --> 52:05.720
The participants to see is you know house face

52:06.040 --> 52:11.480
But the reason vernacular rivalry is interesting is because what they actually perceive what they actually experience is an alternation

52:11.760 --> 52:15.480
They see a house they see a face they see a house and they see a face and

52:16.240 --> 52:19.880
obviously for you know, the view that the mind is

52:21.200 --> 52:24.800
Engaging with the world that the mind is computer that we're making prediction machines

52:24.800 --> 52:28.440
This is a very interesting case because we know for a fact that we are not perceiving

52:29.320 --> 52:32.120
What we are essentially seeing right the experience

52:32.640 --> 52:34.640
Strictly departs from

52:35.440 --> 52:37.440
what we know is being presented and

52:37.920 --> 52:43.480
This was a very important test case for predictive processing and first in like folks because they gave a very elegant

52:44.360 --> 52:45.600
Explanation of what's going on?

52:45.600 --> 52:53.440
So they basically gave a Bayesian interpretation of predictive of vernacular rivalry and said that roughly what happens is that we have very low

52:53.480 --> 52:56.520
Priors for seeing a house face in the natural world

52:56.520 --> 53:00.160
And so what the mind does is it says well it can't be a house face

53:00.240 --> 53:02.560
So it must be a ha it must be a house

53:03.000 --> 53:06.200
But then the prediction error comes back up and says normal

53:06.200 --> 53:09.080
You're only accounting for half of what you're experiencing here

53:09.080 --> 53:13.560
So it must be the other stimulus and it switches and so it's saying okay now it's a face

53:14.560 --> 53:16.760
Again a strong prediction error and an ultrace

53:17.760 --> 53:20.960
And what you have here is you know perceptual dominance

53:20.960 --> 53:26.380
So you might see the face first or you might see the face for longer or it might alternate more

53:26.720 --> 53:30.440
Okay, the mind is a thinking machine. It's epistemic great

53:30.560 --> 53:35.760
We have a very you know epistemically normatively informed explanation of what's going on there

53:36.240 --> 53:41.400
The reason I like to use vernacular rivalry as a case for the evaluative mind

53:41.400 --> 53:45.320
Is that when you subscribe to something like the reward hypothesis as I did?

53:46.080 --> 53:52.280
Let's say in 2017 when I was presented with this is that I thought you know, I bet you reward will modulate

53:52.880 --> 53:59.560
This phenomenon so it allows you to make predictions even from the armchair and I bet you reward will

54:00.720 --> 54:05.040
Modulate the experience vernacular rivalry and that is in fact what you find so if you

54:05.560 --> 54:12.320
Reward a certain stimulus like the face so you say hey, I'm gonna give you a penny every time the face appears

54:12.800 --> 54:14.800
We have perceptual dominance

54:15.320 --> 54:21.560
Of the face but it gets better than that you can also reward just the percept so people don't cheat basically and

54:21.840 --> 54:25.120
Every time they report seeing a face you can do like a pitching

54:25.960 --> 54:31.240
And again the face will be more dominant. You'll have perceptual dominance

54:32.840 --> 54:34.840
First in folks when you email them

54:36.040 --> 54:40.920
They're like yeah, but it could just be information right the reward is an added piece of information

54:40.920 --> 54:45.520
So you can put that into kind of your Bayesian function and that can still explain

54:46.080 --> 54:51.440
Why this perceptual dominance is occurring, but what you can do is you can have a

54:51.880 --> 54:55.760
Punished percept so every time they see a house you can make a little sound that says

54:56.680 --> 54:59.000
You know like you're sad

54:59.000 --> 55:04.160
And if it's just information you should have perceptual dominance of the house

55:04.680 --> 55:08.080
That's not what you find you actually still have perceptual dominance

55:08.400 --> 55:15.320
Of the non-punished percept of the face so really what this tells you and this might glamorize the finding a bit

55:16.840 --> 55:18.520
You want me to like

55:18.520 --> 55:22.000
Animate less. Yeah, no, I know I know I know it's okay

55:22.880 --> 55:24.280
I'll try

55:24.280 --> 55:26.280
Yes, sorry

55:26.400 --> 55:28.400
I will hold on to the podium

55:29.440 --> 55:37.800
This is a small finding but it tells you basically that to some very limited non-exciting degree you see you perceive

55:39.080 --> 55:46.060
What it is in your interest with respect to your goals to perceive right again, this is a very thin case is very narrow case

55:46.760 --> 55:48.760
But it tells you that you don't just

55:49.560 --> 55:52.680
Perceive the world we perceive the world conditional on our goals

55:52.680 --> 55:55.880
Which in this case is to make a little bit of funny in these studies, right?

55:56.360 --> 56:00.080
Binocular rivalry is a tiny case, but if you look at all the research

56:00.760 --> 56:04.080
That kind of riches alluded to over the course of the last several decades

56:04.520 --> 56:09.440
We find that every stage of mental processing from sensation

56:10.680 --> 56:17.680
And computation to action at every level of description and mental processing from the sub personal to the personal

56:19.040 --> 56:22.720
It's conditional on these attributions of reward and value

56:22.720 --> 56:26.960
So we sense we perceive and we attend to the features of our environment

56:27.200 --> 56:32.900
conditional on reward and value we remember and remember to remember remember

56:33.080 --> 56:35.080
prospectively conditional on

56:35.420 --> 56:37.420
the attribution of

56:37.580 --> 56:42.260
Reward and value our cognitive control our ability to decide choose

56:42.660 --> 56:48.900
Plan our future actions in each case you will find a body of evidence that's just it's just like in binocular rivalry

56:49.340 --> 56:52.620
It is conditional on the attribution of reward so

56:54.260 --> 56:58.900
We have this kind of classic sorry picture of the mind as a thinking machine

56:59.340 --> 57:03.180
I think that's part of the story, but the mind is also

57:03.860 --> 57:09.460
continually in the business of evaluating all of these parts of the so-called kind of cognitive sandwich

57:10.140 --> 57:15.740
So I want to make some kind of caveats here. First of all, it's an additive thesis

57:16.460 --> 57:18.460
What I want to say is that the mind

57:18.900 --> 57:26.500
computes over these representations of matter-of-fact and it continually evaluates them now for some people that might seem kind of weak sauce

57:26.500 --> 57:30.340
Particularly, I think philosophers sometimes want to come out swinging and say, you know, this is everything

57:30.340 --> 57:35.580
I call that the stronger thesis the stronger thesis something like what the mind is fundamentally in the business of doing is

57:35.740 --> 57:41.660
Evaluating things is better or worse, right? That's that's mainly what the mind is about. I don't want to go that far

57:41.660 --> 57:47.460
We can talk about that in the discussion of why not I want to say that the mind is fundamentally in the business of doing both

57:48.140 --> 57:51.700
So I'm making an additive claim. I don't subscribe to the stronger thesis

57:51.700 --> 57:55.740
Although I do think it's kind of fun and it it does kind of really drive a lot of nice research

57:56.740 --> 58:00.500
And I also don't think this is exactly the same thing as saying reward is enough

58:01.620 --> 58:05.580
My claim is about the nature of the mind and what the mind is in the business of doing

58:05.580 --> 58:08.580
I take reward is enough on one interpretation to be saying

58:09.140 --> 58:15.380
What's needed in order to produce intelligent behavior? Those are obviously cousins as positions

58:16.140 --> 58:18.140
But I think reward is enough again

58:18.460 --> 58:24.460
Make some stronger claims than than than what I would subscribe to but that's definitely something that we can talk to you

58:24.540 --> 58:30.420
But I think we need to start thinking of the mind as evaluative in these ways not just because that's what we're doing

58:30.620 --> 58:36.460
But also if we want to design artificial intelligence, that is a very different picture of what we're trying to build

58:37.340 --> 58:41.740
And we'll also go towards addressing some of the kinds of challenges that we heard about yesterday

58:41.900 --> 58:46.860
So the first question was does the reward hypothesis provide a good model for understanding the mind?

58:46.900 --> 58:49.700
Yeah, I really think it does and the answer to that is yes

58:49.700 --> 58:54.580
And I think it changes our understanding of what the mind is kind of in the business of doing

58:55.780 --> 59:02.260
The second question is you know, how far does the reward hypothesis go as you might imagine I'm gonna say something like pretty far

59:02.660 --> 59:05.980
So I gave you a list of kind of all the the kinds of

59:06.620 --> 59:13.700
Processes that reward and value are are implicated in and you can kind of see that a lot of these are sort of you know

59:13.700 --> 59:15.700
So called low-level cognitive capacities

59:16.220 --> 59:22.780
But all the way kind of extending into sort of high level cognitive capacities that people might be be interested in and so

59:22.780 --> 59:27.660
I don't want to go into that too much, but I want to focus on what you might think as one of the kind of

59:29.100 --> 59:33.820
Crown jewels of the human mind, right, which is our ability to have moral experiences

59:34.340 --> 59:38.700
If there's anything that we might want to think is special, but how the mind works

59:39.260 --> 59:41.020
One would be language

59:41.020 --> 59:43.020
I can talk about that after yesterday

59:43.820 --> 59:48.460
But another one might be our ability to have moral experiences, so when I say moral cognition

59:48.460 --> 59:50.500
I mean something quite straight forward

59:50.500 --> 59:56.900
It's the capacity to create and respond to situations of moral significance. So let me give you an example. It's nothing fancy

59:57.780 --> 59:59.780
You know my sister Barbara

01:00:00.140 --> 01:00:05.140
Goes to buy a coffee at a coffee shop and she can make the following decision

01:00:05.140 --> 01:00:09.500
She can buy the more expensive fair-trade coffee or she can buy the cheaper

01:00:10.260 --> 01:00:14.420
Commercially sourced coffee, right and she can stand in line. She can make that decision and she

01:00:15.020 --> 01:00:18.660
Realistically probably past the cheaper commercially sourced coffee. Sorry Barbara

01:00:19.540 --> 01:00:24.420
But that is an example of moral cognition. She's not making a decision about a trolley problem or anything like that

01:00:24.420 --> 01:00:31.420
These are the everyday routine moral experiences that we have that we make like this and that's the kind of thing that I have in mind

01:00:31.420 --> 01:00:33.420
When I'm talking about moral cognition

01:00:33.820 --> 01:00:36.700
So again, they're kind of quite

01:00:39.700 --> 01:00:41.060
well

01:00:41.060 --> 01:00:45.660
established received views on kind of how our moral cognition

01:00:47.300 --> 01:00:49.980
Works what the mechanisms behind that are and so

01:00:50.660 --> 01:00:56.100
Again, this this isn't actually even too much of a cartoon. I think we have kind of these rationalist inferentialist

01:00:56.780 --> 01:00:59.140
Views where the idea is that Barbara

01:00:59.460 --> 01:01:06.180
Does or doesn't buy the fair-trade coffee based on the belief or set of beliefs that something is the right thing to do

01:01:06.180 --> 01:01:09.420
And then it will kind of follow from you know her knowledge

01:01:09.540 --> 01:01:11.540
about a particular situation

01:01:11.660 --> 01:01:13.660
On the other end of that dichotomy

01:01:13.660 --> 01:01:16.540
There's this idea that Barbara might do or not do the right thing

01:01:17.100 --> 01:01:18.980
Because of her moral emotions

01:01:18.980 --> 01:01:22.260
She might feel empathy towards the workers or not

01:01:22.420 --> 01:01:27.140
So an agent has the fitting moral emotion or emotions that something is the right thing to do

01:01:27.380 --> 01:01:32.020
And then despite the kind of distribution of the literature actually I think a lot of mainstream views

01:01:32.340 --> 01:01:38.220
Are hybrid views some combination of the views that you know certain things that you have certain kinds of beliefs

01:01:38.220 --> 01:01:42.580
You have the relevant beliefs. You also have the accompanying emotions that might motivate that behavior

01:01:43.260 --> 01:01:49.140
And we should definitely talk about that Plato slide, but without going all the way back to you Plato

01:01:49.140 --> 01:01:51.980
I think these have been kind of the main

01:01:52.900 --> 01:01:57.140
Alternatives in understanding moral psychology and understanding our moral cognition. I

01:01:58.300 --> 01:02:03.060
Think the reward hypothesis suggests that there's a kind of a new player on the scene

01:02:03.740 --> 01:02:10.380
And that moral cognition is in fact constituted by the sub personal attribution of goal and context

01:02:10.580 --> 01:02:15.580
Dependent subjective reward value. So again that familiar phrase that we see

01:02:16.420 --> 01:02:25.740
What we need to see in binocular rivalry is actually one important ingredient that mechanism is also driving our experiences of saying something like

01:02:25.900 --> 01:02:31.760
That's just wrong. That's morally disgusting. I really ought to be doing this thing

01:02:32.040 --> 01:02:38.600
Again in the descriptive cases, but what I am suggesting is that we actually recruit the same reward mechanisms

01:02:39.000 --> 01:02:45.980
In our moral cognitive experiences now I'll flush that out again because that's a bit strong and a bit strong stuff

01:02:46.120 --> 01:02:48.440
But what I want to suggest is that

01:02:49.320 --> 01:02:53.080
There is quite a lot of flattening going on here, right? So in fact

01:02:53.960 --> 01:02:55.960
When you are experiencing

01:02:56.280 --> 01:03:04.160
Something as right or wrong you are attributing reward and value or laying that fabric in just the same way in a moral

01:03:04.280 --> 01:03:10.600
Context with moral determinants like something like fairness something like honesty as you are when you are just choosing between

01:03:11.240 --> 01:03:13.240
You know left and right to your coffee

01:03:13.640 --> 01:03:18.920
So that's to say that if I'm choosing between to your coffee or coffee and fair-trade coffee

01:03:18.920 --> 01:03:22.960
I'm actually recruiting some of the same mechanisms now again

01:03:22.960 --> 01:03:25.560
There will be caveats that come to kind of flesh out this picture

01:03:25.800 --> 01:03:32.880
But this is an important flattening of our understanding of what's going on in our moral psychology in our moral cognitive experiences

01:03:33.920 --> 01:03:38.200
And you might think okay, but I really feel strongly about certain

01:03:38.720 --> 01:03:41.600
You know moral propositions, and I don't feel particularly strongly

01:03:42.400 --> 01:03:45.360
about to your coffee or something like that and

01:03:45.560 --> 01:03:52.840
Sort of the second thesis of this view is that reward and value the strength of the reward and value are actually the source of our

01:03:52.960 --> 01:03:57.440
Moral motivational force so through our evolutionary history and through our upbringing

01:03:58.320 --> 01:04:00.000
certain things are

01:04:00.000 --> 01:04:01.240
reinforced

01:04:01.240 --> 01:04:05.920
To be extremely important to be absolute no-knows or absolute musts

01:04:06.360 --> 01:04:12.680
And that these are the sources of why we sometimes feel that something is right or wrong and why we experience things

01:04:13.640 --> 01:04:17.520
Something we really ought to do again. This is not a normative on but it's the feeling of I oh

01:04:17.560 --> 01:04:20.400
I really should call my grandmother or something like that

01:04:20.400 --> 01:04:26.280
It's because the driving mechanism here is again reward and the value not my emotions not my

01:04:26.760 --> 01:04:29.760
Knowledge though again, I'll build a kind of more complete puzzle there

01:04:29.760 --> 01:04:36.560
And so this is kind of one of the things that fundamentally differentiates between something like our social and our moral values

01:04:36.560 --> 01:04:39.080
Right, we know we shouldn't wear white after Labor Day

01:04:39.600 --> 01:04:41.800
Or we did know that once when I was like five

01:04:41.840 --> 01:04:47.600
That has some force, but it has much different force from you know

01:04:47.600 --> 01:04:53.200
It's wrong to lie and that is I think a function of the strength of the value that has been

01:04:53.720 --> 01:04:57.240
Attributed to that over the course of our lifetime experiences our

01:04:57.680 --> 01:05:00.600
Education's our philosophical conversations and things like that

01:05:00.600 --> 01:05:04.120
So it's a source of the strength of the moral motivational force

01:05:05.360 --> 01:05:09.480
Now this is pretty quick and again, I'm gonna flush out the picture in a second, but I think

01:05:10.160 --> 01:05:16.400
This kind of you starts to account for features of our moral psychology that the kind of rationalist

01:05:16.560 --> 01:05:23.120
Inferentialist views and the sentimentalist views don't account for so it doesn't it it accounts for something like the multidimensional nature

01:05:23.680 --> 01:05:28.760
It's hard to explain the Barbara fair-trade coffee case with just the emotions, right?

01:05:28.760 --> 01:05:33.880
There's lots of features that need to be weighed off there that a kind of complex

01:05:34.520 --> 01:05:40.920
Waiting of our values allows us to explain explains why sometimes she buys fair-trade coffee and sometimes she doesn't

01:05:41.200 --> 01:05:45.280
Why we have cross cultural values, so we have themes in our values

01:05:45.280 --> 01:05:51.980
So all communities, you know value honesty, but what it means to be honest or what it means to murder or what counts as

01:05:52.480 --> 01:05:56.240
incest is going to vary in us because we share some of these

01:05:57.040 --> 01:06:03.200
Mechanisms, but how we tune them up is going to really depend on the community and the culture same with

01:06:03.440 --> 01:06:05.440
moral learning

01:06:05.440 --> 01:06:07.440
It's going to

01:06:07.800 --> 01:06:15.080
Allow us to understand kind of what counts as morally exceptional, right where people stand up against all kinds of

01:06:15.680 --> 01:06:17.880
opposition and costs and

01:06:18.400 --> 01:06:23.480
But we're going to kind of place a very high value on the certain principle and stand by that principle

01:06:24.000 --> 01:06:26.240
Through that strong moral motivational force

01:06:27.080 --> 01:06:29.580
and I think kind of one of the

01:06:29.900 --> 01:06:34.900
The more complex but also more compelling kind of pieces of evidence for a view like this is

01:06:35.300 --> 01:06:39.420
What happens when we have failures in our moral cognition make errors?

01:06:39.420 --> 01:06:43.420
We have dysfunctions and sometimes we have pathologies in our moral cognition

01:06:43.420 --> 01:06:46.860
And one of the interesting things is that these seem to rise and fall

01:06:47.300 --> 01:06:54.860
With dysfunctions in our reward systems in our reward mechanisms in ways that we wouldn't necessarily expect to see otherwise, so I

01:06:56.460 --> 01:06:58.460
think

01:06:59.580 --> 01:07:03.580
I do want to specify that I think it's a part of the puzzle again

01:07:03.580 --> 01:07:07.660
It's going to be an inclusive view in the same way that the evaluative mind is an inclusive view

01:07:07.660 --> 01:07:12.020
That's not to say that we don't have any place for the emotions

01:07:12.660 --> 01:07:18.140
In our moral cognitive experiences. It doesn't mean that we don't reason and read philosophy and things like this

01:07:18.420 --> 01:07:22.340
But when you're standing in the coffee shop and you're trying to decide whether you're making

01:07:23.220 --> 01:07:25.460
Whether you're going to buy the fair-trade coffee or not

01:07:26.460 --> 01:07:31.620
What you're recruiting there are your valuation systems and your life-long experiences

01:07:31.900 --> 01:07:36.540
Of what you value and what we attribute our values over now

01:07:36.540 --> 01:07:43.060
I think one key open question here is what do we attribute those rewards and values over is it to states?

01:07:43.060 --> 01:07:46.500
Is it to state action pairs? Is it to abstract ideas?

01:07:46.500 --> 01:07:53.140
Is it possible that philosophers for example have learned to attribute a high amount of value just to the concept of

01:07:53.420 --> 01:07:58.920
Justice and the existence of justice in the world, right? I think these are important questions of how exactly it plays out

01:08:00.500 --> 01:08:04.020
But I think and here I will take a dig actually at that Plato slide

01:08:05.020 --> 01:08:07.500
It's not about pleasure and pain actually

01:08:07.500 --> 01:08:11.860
I think pleasure so I follow Kent barrage here and distinguishing between pleasure and pain and

01:08:12.260 --> 01:08:20.220
Reward and punishment or reward and and disappointment and I think our moral cognitive theories have been based on what we can introspect

01:08:20.220 --> 01:08:27.100
Oh, I think this oh, I feel this and we don't necessarily have introspective access to our reward mechanisms

01:08:27.340 --> 01:08:29.460
And so I think actually in a way

01:08:29.460 --> 01:08:35.620
It's a disservice to try and tie it back to hedonism because it's rewarded something different here that hasn't been on the scene

01:08:36.020 --> 01:08:42.140
But I think is a driving factor in this much more complex architecture of our of our moral cognition

01:08:43.020 --> 01:08:45.500
So how far does it go? It goes pretty far

01:08:45.500 --> 01:08:51.700
It goes to certain things that I think we thought, you know, we're a little bit untouchable in our human experience

01:08:52.260 --> 01:08:57.420
And again, I'm just touch on this and we can talk about it in the discussion, but I think it really informs

01:08:58.980 --> 01:09:00.820
How we understand

01:09:00.820 --> 01:09:06.220
Moral artificial intelligence because this is actually quite tractable. This is actually quite quantifiable

01:09:06.620 --> 01:09:11.700
We can actually go much further in understanding what works and what doesn't work in our moral cognition

01:09:12.420 --> 01:09:14.420
Because we have this nice

01:09:14.860 --> 01:09:21.380
Quantifiable computational theory and we can go much farther and understanding our moral cognitive experiences

01:09:21.740 --> 01:09:27.660
And in designing those right so morality on a sore architecture is gonna suck

01:09:28.500 --> 01:09:32.020
But it doesn't need to suck because that's actually not how it works in us either

01:09:32.460 --> 01:09:39.220
And so I think having a handle on this opens some pretty interesting avenues even in the realm of artificial intelligence

01:09:40.180 --> 01:09:42.700
So lastly, and this will be the fastest part

01:09:43.020 --> 01:09:48.940
But the third question should it should this understanding then guide our normative decision-making and here

01:09:49.340 --> 01:09:54.660
It'll definitely depart have from from rich. I think my answer straightforward Lee

01:09:55.300 --> 01:09:58.060
That got formatted out of existence. Sorry

01:09:58.780 --> 01:10:02.140
Is no so I take guide to be pretty strong

01:10:02.780 --> 01:10:06.660
Guide means I should then use this to govern my normative decision-making

01:10:07.180 --> 01:10:13.900
And the analogy that I often draw here is that moral cognition should be understood in analogy to folk physics

01:10:14.620 --> 01:10:21.300
So, you know the Wiley Coyote case where Wiley runs off the cliff and hangs out there for a second and then falls right or

01:10:21.300 --> 01:10:26.100
Other cases might be Muller liar illusions or putting a straw in a cup and things like that

01:10:27.100 --> 01:10:35.460
We have understandings of physics that are produced by our sensations and our perceptions that we know to be incorrect and

01:10:36.780 --> 01:10:40.940
If we rely on them, we will not land on the moon and we will

01:10:41.300 --> 01:10:49.340
You know not make other scientific achievements, and I think we have to understand moral cognition as the equivalent of folk physics

01:10:50.180 --> 01:10:55.260
It's full of errors now does that mean that the reward hypothesis is not useful

01:10:55.460 --> 01:10:58.660
No, because I actually think it's really important to understand

01:10:59.300 --> 01:11:05.060
Where our moral cognition reliably and in fact systemically falls down, right?

01:11:05.900 --> 01:11:09.980
So this is have basically reduced to a bumper sticker. I used to walk past

01:11:10.580 --> 01:11:16.500
Basically every day when I was doing my PhD, which says don't believe everything you think, right? It's you've all seen it

01:11:16.500 --> 01:11:24.620
It's right, but actually that's basically exactly what this is is we have very strong moral experiences hatred judgment

01:11:25.300 --> 01:11:27.300
empathy

01:11:28.020 --> 01:11:34.980
But we shouldn't believe all of those right but what the reward hypothesis and what reinforcement learning along with many other

01:11:35.140 --> 01:11:41.420
Contributions allow us to do is understand the mechanisms that generate those things that we experience is right or wrong

01:11:41.420 --> 01:11:45.420
And I do think that's powerful. I wouldn't use it to guide normative decision-making

01:11:45.460 --> 01:11:47.900
I wouldn't say okay. Well that I guess that's what we're doing

01:11:48.180 --> 01:11:51.540
But I think we can use it to inform it in the following way

01:11:51.540 --> 01:11:55.900
So one of the things that I have suggested is that we should have a kind of fault line approach

01:11:55.900 --> 01:11:59.980
So when I say fault line, I mean something like tectonic plate movement, right?

01:11:59.980 --> 01:12:01.900
If you understand the movement of tectonic plates

01:12:01.900 --> 01:12:07.900
You can make nice predictions about where earthquakes and volcanoes are going to occur where things are gonna kind of continually erupt

01:12:07.900 --> 01:12:12.540
And I think if you can understand the contours of our moral cognitive decision-making

01:12:13.460 --> 01:12:16.260
You can also start to see sort of the fault lines

01:12:16.260 --> 01:12:19.900
We're not just gonna make token errors in our moral cognitive judgments

01:12:19.900 --> 01:12:23.180
We're gonna make type errors in our moral cognitive judgments

01:12:23.180 --> 01:12:26.380
and I think we can use that to inform then

01:12:27.180 --> 01:12:30.340
The kinds of things because I think in in sort of everyday life

01:12:30.420 --> 01:12:36.180
We really do take our moral cognitive experiences at face value with one another and I think that's a mistake

01:12:36.260 --> 01:12:41.500
Whereas if we have a kind of causal understanding of what generally sees and where they might depart

01:12:42.100 --> 01:12:45.100
Then that can be used as a kind of type remediation

01:12:45.580 --> 01:12:49.860
Of what's going on not necessarily in a policy sense or anything like that

01:12:49.860 --> 01:12:54.820
But I certainly think you know, it's useful knowledge to have it can inform us for the better

01:12:54.820 --> 01:12:58.780
I think it's good to know kind of the contours of the mistakes that you're making

01:12:59.380 --> 01:13:03.660
But I wouldn't say guide because of the way these things are generated

01:13:03.660 --> 01:13:06.540
So I think that might be a place where we depart

01:13:08.500 --> 01:13:10.500
So yeah, I think it's pretty productive

01:13:12.020 --> 01:13:22.180
Thanks, Julia and and rich that's just terrific. I'm so glad I got you both on to this conversation

01:13:23.220 --> 01:13:26.980
And and I I have a couple of questions to get us started, but also

01:13:27.580 --> 01:13:31.420
Ready to sort of follow with different directions. You'd like to head

01:13:31.420 --> 01:13:36.340
I heard two things perhaps that it might be worth exploring one was pleasure and pain

01:13:37.340 --> 01:13:40.380
And rich I don't know if you want to respond to that one and the second one

01:13:40.380 --> 01:13:46.100
Oh, which I'll come back to is is the guiding normative decision-making, but let's take on pleasure and pain

01:13:46.100 --> 01:13:53.500
Is that an important part of how you're thinking about this to me the pleasure and pain are just examples of what reward might be

01:13:53.500 --> 01:13:59.380
Well, I think it was the play-doh slide. Yeah. Yeah. Yeah. Yeah. Yeah, he was clearly talking about pleasure and pain

01:14:00.820 --> 01:14:04.060
Reward could be pleasure payment. It's undoubtedly more complicated than that

01:14:07.340 --> 01:14:09.340
And

01:14:10.180 --> 01:14:16.100
I agree with you that we don't know what our word signal is we we don't have

01:14:17.340 --> 01:14:18.260
introspection

01:14:18.260 --> 01:14:20.260
introspective access to it

01:14:22.260 --> 01:14:24.260
And and more than that

01:14:25.780 --> 01:14:29.220
What we do have an introspective access to it is the

01:14:29.380 --> 01:14:31.380
I

01:14:32.980 --> 01:14:37.340
Want to call it TD error remember it's the thing I was talking about is reinforcement

01:14:37.980 --> 01:14:44.140
It's which is which is this measure of how do we feel about how well things are going this as you're saying the

01:14:44.740 --> 01:14:48.700
Constant sense of our things getting better or worse. We evaluating all the time

01:14:49.820 --> 01:14:54.220
So we have access to our evaluations and we we have very

01:14:55.220 --> 01:15:00.900
Preeminent access or prominent access to our things getting better or worse

01:15:02.060 --> 01:15:04.380
But things getting better and worse

01:15:05.100 --> 01:15:09.300
if you remember it's the reward plus the change in your evaluation function and

01:15:09.820 --> 01:15:15.700
So these two things the reward and the change in your evaluation these two are mixed together

01:15:15.700 --> 01:15:21.740
And so when you feel good or feel bad, you don't know if it's you don't really know if it's because the reward was high

01:15:21.740 --> 01:15:23.740
Or the change in evaluation was high

01:15:24.460 --> 01:15:25.740
and so

01:15:25.740 --> 01:15:29.780
So that is a confusion that we all have we don't know

01:15:31.180 --> 01:15:37.220
Once so once an evaluation a value function becomes very well established then, you know, it feels

01:15:37.820 --> 01:15:45.980
Bad when our values are violated just as if we had you know a form of pain or a bad thing directly. So

01:15:47.860 --> 01:15:51.100
This is this is a major phenomenon, you know in terms of our

01:15:51.620 --> 01:15:54.220
subjective experience that it's hard to tell

01:15:55.540 --> 01:15:57.540
Difficulties in a reward and a change of value

01:16:03.860 --> 01:16:05.860
Doing anything

01:16:07.820 --> 01:16:09.820
Yeah, I would say that and again this is

01:16:10.340 --> 01:16:16.860
Following Kent Barrage. I think pleasure and pain are sick, you know are evolved biological signals of

01:16:17.740 --> 01:16:19.740
reward and value and

01:16:20.260 --> 01:16:25.060
As signals, they're pretty good, which means they go together a lot of the time

01:16:25.060 --> 01:16:30.500
I think one of the cases that the barrage talks about is the case of for example a digger

01:16:30.500 --> 01:16:35.260
This might be David Reddish talking about cases of addiction to heroin which where the signals

01:16:35.940 --> 01:16:38.700
and the reward and disappointment come apart so

01:16:40.820 --> 01:16:44.460
You very clearly start to have not pleasurable experiences

01:16:45.380 --> 01:16:48.780
When you will become a heroin addict, but the reward is so strong

01:16:49.740 --> 01:16:52.500
That you continue to do it right heroin is basically ruining your life

01:16:53.020 --> 01:16:57.980
But because it's hijacked the reward mechanisms and you continue. So these are kind of edge cases

01:16:58.900 --> 01:17:01.740
but I mean, I think one of the powers of

01:17:02.580 --> 01:17:08.500
Like the reward hypothesis, but also the scientific process and that discovery that you're talking about with Peter Diane and others

01:17:08.500 --> 01:17:11.000
Is that it really gets at an important part?

01:17:11.620 --> 01:17:16.180
Of of our kind of mechanistic composition that we don't have direct

01:17:17.060 --> 01:17:18.340
introspective access to you

01:17:18.340 --> 01:17:24.940
But where there are departures between what a hedonistic or pleasure and pain-based view would predict about us?

01:17:25.260 --> 01:17:32.020
And what the reward view would predict about us and and clearly the reward view makes better predictions is better supported by the evidence

01:17:32.540 --> 01:17:33.860
and so

01:17:33.860 --> 01:17:35.420
It's kind of funny saying this to you

01:17:35.420 --> 01:17:40.860
But I would say like there's no reason to lean into the pleasure and pain because like reward actually provides a

01:17:41.180 --> 01:17:45.420
Better model in some cases, even though they you know, they often walk together

01:17:49.340 --> 01:17:52.980
So you're thinking it's important to keep the distinction

01:18:01.300 --> 01:18:03.300
I mean the reward

01:18:03.860 --> 01:18:05.860
way of talking is more abstract and

01:18:06.540 --> 01:18:08.540
And

01:18:08.860 --> 01:18:11.620
If it was equated with pain

01:18:12.940 --> 01:18:16.180
Pain and pleasure then it becomes more specific and

01:18:17.580 --> 01:18:21.100
And that they're there for falsifiable

01:18:23.220 --> 01:18:25.220
Which is good

01:18:26.420 --> 01:18:32.580
Yeah, so I'm totally on board of course with it's a general thing it doesn't have to be pain and pleasure and there may be cases I

01:18:33.220 --> 01:18:34.660
mean, I think I

01:18:34.660 --> 01:18:39.060
Guess that totally makes sense to me like there may be pain and then maybe you could

01:18:43.460 --> 01:18:50.880
Change so that the pain is less important to you or the pleasure is less important. Yeah, it's just more general

01:18:52.140 --> 01:18:53.580
The reward is more general

01:18:53.580 --> 01:18:58.860
I think another example might be the emotions where again we have introspective access to our emotions

01:18:59.180 --> 01:19:04.180
And in that very hastily shown chart, right? I think there's incredibly close coupling

01:19:04.660 --> 01:19:10.860
Between our affective responses fear anger and so on and the valuation component

01:19:11.380 --> 01:19:18.020
But you might ask yourself, you know, how do you know I lost that between the the emotions and and the

01:19:18.940 --> 01:19:24.420
Valuations, yeah, and so if you are you have a component theory of affect or emotion

01:19:24.420 --> 01:19:27.300
You say that there's an appraisal component of the emotion, right?

01:19:27.620 --> 01:19:33.260
What triggers certain emotions? Well on my view would be something like the valuation system the reward system

01:19:34.020 --> 01:19:40.060
But I think you lose a lot if you only look at the introspective component. You say it's all emotion

01:19:40.860 --> 01:19:42.380
Emotion is important

01:19:42.380 --> 01:19:49.900
But actually the emotions are kicked off by the valuation or mechanism by the valuation assessment that then triggers these

01:19:50.660 --> 01:19:57.020
ballistic responses like empathy or fear or anger or so on so I think again, they're they're very close

01:19:57.020 --> 01:20:00.020
I don't know if bad fellows is the right word like these things really run together

01:20:00.340 --> 01:20:07.500
But I do think that you come up with cases that reward accounts for and the emotions do not and that there are cases where

01:20:07.940 --> 01:20:12.860
reward explains the phenomenon and pleasure and pain do not and so I

01:20:14.780 --> 01:20:20.400
Do you think it's you know, they're obviously very closely related but something is lost by blurring between those two

01:20:23.420 --> 01:20:25.100
Maybe I

01:20:25.100 --> 01:20:27.300
Of course economists of which I am one

01:20:28.300 --> 01:20:34.700
Have thought of and debated about these questions a lot because we represent a utility function

01:20:34.700 --> 01:20:40.300
Most of the modeling is done with the idea that you can take all decision-making and put it into a

01:20:41.060 --> 01:20:48.140
Continuous function, but I want to press a little bit because I hear in Julia's framework

01:20:49.620 --> 01:20:55.700
Again with this concept this broader concept of valuation to say like your brain may actually have many sources

01:20:55.980 --> 01:21:00.100
many valuations schemes that aren't necessarily

01:21:01.380 --> 01:21:03.580
Representable as a continuous

01:21:04.140 --> 01:21:10.820
Scaler reward and so I wouldn't want to push on the scalar part because I think that's a difference between

01:21:11.500 --> 01:21:13.500
What you've presented

01:21:13.700 --> 01:21:15.100
and

01:21:15.100 --> 01:21:18.100
That's the economics agrees with the scalar

01:21:18.940 --> 01:21:22.780
Yeah, you do. Yeah, I do. All right. I'll be the disagreeer. Yeah

01:21:23.220 --> 01:21:25.420
So there's a nice paper by

01:21:30.260 --> 01:21:31.860
I'll come up with the name in a second

01:21:31.860 --> 01:21:35.700
Which just basically suggests that you can have these vectors that reduced to the scalar

01:21:36.020 --> 01:21:38.820
and so that there are kind of mathematical ways of

01:21:39.540 --> 01:21:46.340
Taking that kind of consideration and still having it reduced to scalar and you can say like whoof the theory holds that it's scalar

01:21:46.860 --> 01:21:51.020
And so I think I'm probably less invested in this part of of it

01:21:51.020 --> 01:21:57.500
But that would be the way that I would appeal to it is that you can accommodate that kind of intuition and still preserve the scalar feature of

01:21:57.500 --> 01:22:00.140
The theory, but you might want to defend it like more

01:22:01.660 --> 01:22:08.940
The original work arguing that you can reduce the vector to the scalar is in second is in economics

01:22:09.940 --> 01:22:11.940
I

01:22:13.620 --> 01:22:15.620
Morgan Stern and

01:22:15.620 --> 01:22:23.620
Somebody rather you know, well, yes, it's hope and see there's a debate in economics about and and but by far the dominant

01:22:23.820 --> 01:22:30.540
Neoclassical economics is that you can you can do that. Yeah, but there's also a different project in economics, which is

01:22:31.260 --> 01:22:35.140
We're trying to build models that will predict the way

01:22:36.260 --> 01:22:38.140
humans and

01:22:38.140 --> 01:22:45.260
Entities composed of humans like markets will behave and so then when you say it's enough

01:22:45.260 --> 01:22:48.580
You're saying oh, it's good enough to be predictive

01:22:48.580 --> 01:22:54.060
And so one of the things I think about is how much are we taking that and now saying well, this is the way

01:22:55.580 --> 01:22:57.580
Valuation works in

01:22:57.820 --> 01:23:04.440
Human societies in the human and in human societies and I guess I want to put a wedge in there to say

01:23:04.760 --> 01:23:13.400
You know, it could be that I could predict pretty well even if values are incommensurate and can't be reduced to a trade-off

01:23:14.440 --> 01:23:20.160
Nonetheless at the end of the day. I'm just predicting are you gonna choose this or this so I could have a representation

01:23:20.600 --> 01:23:23.000
That worked for that predictive exercise

01:23:24.200 --> 01:23:25.960
so

01:23:25.960 --> 01:23:28.320
All bets are off when you have groups

01:23:29.080 --> 01:23:33.760
you know the the the basic theory is the theory of an individual and and

01:23:35.080 --> 01:23:38.960
Yet and yet we want to move to say something interesting about

01:23:40.000 --> 01:23:43.360
About groups or to ask if it's possible to say anything

01:23:44.440 --> 01:23:45.880
normative

01:23:45.880 --> 01:23:47.880
or

01:23:49.320 --> 01:23:53.920
Yeah, can we say something normative normative, so what does this mean this mean I

01:23:56.920 --> 01:24:00.520
See I might any definition I'll give will be reductive again, so

01:24:05.400 --> 01:24:12.440
So how can we walk up to this so one way to walk up to it is the way Julia did which saying well, maybe these

01:24:14.000 --> 01:24:16.000
The moral decisions

01:24:16.920 --> 01:24:22.360
Are are not that different from the ordinary decision-making we do it every day

01:24:23.160 --> 01:24:25.000
And to that to me that makes sense

01:24:25.000 --> 01:24:31.720
And so I mean I want to push and say that for the individual we can think of it all as one system

01:24:32.120 --> 01:24:36.640
Now to make it a little bit more acceptable to you that it might be that way

01:24:36.760 --> 01:24:41.400
You want sure you remember that the the value judgments as well as the policy

01:24:41.400 --> 01:24:50.560
But let's the value judgments are compiled. They are they are they may they may be due to learning or reasoning

01:24:50.840 --> 01:24:55.120
but in the end they are they are they are like

01:24:56.000 --> 01:24:57.480
they're

01:24:57.520 --> 01:25:03.400
Hard-coded and they're automatic and they're intuitive. They're not the reasons for them are no longer present

01:25:03.480 --> 01:25:09.040
that's so I see the reasons you might have a very firm feeling that that

01:25:12.480 --> 01:25:15.400
Children should have a loving parent and

01:25:17.160 --> 01:25:18.920
You might not have

01:25:18.920 --> 01:25:25.440
Any more have the reasons for that belief you just believe it really strongly and and maybe maybe there was a reason for it

01:25:25.440 --> 01:25:31.520
Maybe you had experience or you've seen people or you've done some reasoning process and you came up with a reason

01:25:31.520 --> 01:25:37.880
But but now you just know you have this belief and so this is why we tend to think of values as core things

01:25:38.720 --> 01:25:40.720
And and more emotional

01:25:41.560 --> 01:25:45.480
Because we we don't have the reasons now it doesn't mean we lost the reasons

01:25:45.480 --> 01:25:49.440
You know, maybe the reasons who are built built into us by evolution to me

01:25:49.440 --> 01:25:57.480
It doesn't really matter. You have ended up with a judgment about the the the goodness or badness of certain things and and

01:26:02.080 --> 01:26:07.840
Nevertheless a good way to think about them maybe as a prediction of the likely rewards

01:26:08.520 --> 01:26:10.680
For being in that state or acting in that way

01:26:11.960 --> 01:26:14.440
And to me that that's a really exciting

01:26:14.440 --> 01:26:21.400
Apothos that you might be able to think about of all the judgments that people may including their moral ones in terms of a single framework

01:26:23.720 --> 01:26:29.960
So I want to separate that is that impressive and I feel you from your talk that you were like

01:26:30.320 --> 01:26:32.600
trying to lead lead us that way and

01:26:33.600 --> 01:26:39.440
So I want to separate that there are two big things to talk about is that makes sense. Okay, and then

01:26:40.440 --> 01:26:42.200
If that makes sense

01:26:42.200 --> 01:26:50.200
What about when you have multiple agents and and then and we asked the question of universality and normative judgments

01:26:50.640 --> 01:26:52.640
So are we at the?

01:26:53.840 --> 01:26:58.920
Yeah, are we ready to have we have we done accepted the first thing and are we ready to do the second thing?

01:26:59.480 --> 01:27:04.240
Are we still unsure and I'm I'm gonna ask you, you know, everyone, you know

01:27:05.160 --> 01:27:07.160
Anyway, these these these two steps

01:27:07.880 --> 01:27:09.560
Are we ready?

01:27:09.560 --> 01:27:14.640
So can I just jump in on that real quick because I think there's a step between is that okay? Yeah, no, that's great

01:27:14.640 --> 01:27:19.080
And then I'll go to question. Um, so I like this idea of reasons are no longer present

01:27:19.080 --> 01:27:24.120
So on the slide and have time to talk about it, but I had this case of moral dumbfounding. So moral dumbfounding is when

01:27:25.000 --> 01:27:27.000
You give people scenarios

01:27:27.200 --> 01:27:29.000
like, you know

01:27:29.000 --> 01:27:34.040
You know Jane and and John are brother and sister and and they would like to have sex

01:27:34.520 --> 01:27:40.640
Just the one time with protection. So, you know, there's no possibility of any of the genetic consequences

01:27:41.360 --> 01:27:43.080
of, you know

01:27:43.080 --> 01:27:47.720
incest and so on and so forth and you ask people like, you know, is this morally acceptable and you know

01:27:49.120 --> 01:27:50.720
and

01:27:50.720 --> 01:27:55.800
You ask them and they'll give you kinds of reasons, but they they can't really so it's this dumbfounding case where you're like

01:27:55.800 --> 01:27:59.640
Yeah, okay, there should be everything fine with this. The other cases are like having sex with the chicken

01:27:59.640 --> 01:28:03.200
Anyway, there's some stuff going on in the lab there that came up these examples

01:28:03.200 --> 01:28:08.960
But there's strong cases we have these the responses and I really like this way of putting the reasons are no longer present

01:28:09.600 --> 01:28:11.600
Right, the reasons are good reasons

01:28:12.400 --> 01:28:19.360
But they're no longer present in this case because if if John and Jane or whatever I said use contraception then, you know

01:28:19.360 --> 01:28:25.640
This this evolved reason for why we're opposed morally opposed to incest is no longer there

01:28:25.840 --> 01:28:27.840
But that's actually a really important

01:28:28.400 --> 01:28:32.320
Junction, right? That suggests that those intuitions

01:28:33.200 --> 01:28:36.880
Maybe over the course of evolution are a good

01:28:37.760 --> 01:28:42.480
signal of reward or not, but that there are departures from that

01:28:43.200 --> 01:28:45.200
And that's where the normative

01:28:46.320 --> 01:28:54.720
Story comes in so we have these descriptive experiences like no, but the reasoning and the moral philosophy or the ethics or

01:28:55.280 --> 01:29:00.680
Policymaking or all these normative disciplines law economics come in because we tell ourselves, okay

01:29:00.680 --> 01:29:07.560
But shunning the outgroup may no longer be doing us a service because we live in a global cosmopolitan society where

01:29:07.840 --> 01:29:09.760
We all want to get along

01:29:09.760 --> 01:29:12.920
And who's getting along there is all of a sudden a much bigger group

01:29:13.640 --> 01:29:19.480
And so I think you can have a part of that first story which is radical in and of itself

01:29:19.600 --> 01:29:22.920
But not necessarily follow it all the way to step two

01:29:23.720 --> 01:29:25.960
Because sometimes we believe things that are just

01:29:27.480 --> 01:29:29.480
Silly like you shouldn't have sex with a chicken

01:29:35.080 --> 01:29:41.160
So I I'm just I'm gonna I'm gonna just throw out there and then I'll come to the audience

01:29:41.160 --> 01:29:45.360
and I may want to put Joel and will on the spot on this one as well, but

01:29:46.120 --> 01:29:54.320
You know the the the idea of thinking about the values or as or the as

01:29:56.400 --> 01:29:58.400
Intuitions as

01:29:58.480 --> 01:30:02.160
Some kind of thing that is just there

01:30:02.160 --> 01:30:09.120
We don't know where it came from it accumulated from arts as opposed to which is what I really like about your framework Julia is

01:30:10.080 --> 01:30:16.880
It's a very active process and actually so I make the claim in the work that I'm doing thinking about normative systems

01:30:16.880 --> 01:30:19.760
And I have a very very reductive definition of normativity

01:30:20.240 --> 01:30:24.120
It's what the group labels and we just have a labeling scheme

01:30:24.120 --> 01:30:32.560
This group says this is okay, and this is not okay, and we've you know coordinated a cognitive structures and institutions

01:30:32.560 --> 01:30:35.720
And so on to to produce that but it's adaptive it changes

01:30:36.120 --> 01:30:38.400
We get angry about rule violations

01:30:38.400 --> 01:30:45.120
But you can go to a different environment where the rule is different and you used to get mad when people came into the building with short

01:30:45.440 --> 01:30:47.760
shorts on and now you don't

01:30:48.240 --> 01:30:51.640
Right, and so I think it's it's it's very much a

01:30:52.560 --> 01:30:55.720
Processing of the information from the group

01:30:56.360 --> 01:30:58.360
So it's a group thing

01:30:58.400 --> 01:31:01.480
It's a pick a particular group all groups don't have to agree

01:31:01.960 --> 01:31:05.000
Even a single group may change its mind over time

01:31:05.000 --> 01:31:10.040
Yes, and that that's actually a complex system to understand how that how that functions

01:31:10.760 --> 01:31:16.320
And the silly rules about chickens for example or something that thought a fair bit about

01:31:16.800 --> 01:31:24.800
But I want to make sure we get because I know that there's lots of provocations for our audience here to to contribute on so let me

01:31:25.080 --> 01:31:28.080
Let me open it up here. I'm gonna go to Jennifer first

01:31:28.080 --> 01:31:30.080
Yes

01:31:30.320 --> 01:31:32.320
Yes, and do wait for the mic

01:31:34.040 --> 01:31:37.640
So so this is sort of following up on Julia's claim that minds

01:31:38.920 --> 01:31:42.640
Subpersonally a sign reward, and I don't disagree with that. I think absolutely we do that

01:31:43.040 --> 01:31:49.440
I'm curious about whether we also personally do this and whether it's not an expression of our

01:31:50.040 --> 01:31:53.640
Autonomy as rational agents capable of setting goals that we do this

01:31:53.640 --> 01:31:57.560
This is also my reservation about the sort of McCarthy definition of intelligence as

01:31:58.560 --> 01:32:05.840
You know being about the computational part of achieving goals surely we can apply our rational intelligence to setting

01:32:06.160 --> 01:32:13.080
Goals, I mean we don't have that much flexibility with respect to the sort of homeostatic needs that are

01:32:14.040 --> 01:32:16.160
Encoded in us as biological creatures

01:32:16.160 --> 01:32:21.720
But but beyond that we're on a long leash from evolution and we can surely in

01:32:22.360 --> 01:32:26.680
Rational conversations with each other work towards finding

01:32:29.040 --> 01:32:33.540
Goals that stand the test of

01:32:34.800 --> 01:32:36.800
You know

01:32:36.800 --> 01:32:38.160
rational

01:32:38.160 --> 01:32:40.160
scrutiny and

01:32:40.160 --> 01:32:44.240
And and and so that kind of that kind of expression of our freedom

01:32:45.520 --> 01:32:49.800
Intellectually seems to me to be a very very important application of human intelligence

01:32:49.800 --> 01:32:55.480
We're not just slaves of our reward functions, you know just looking for ways to maximize

01:32:55.680 --> 01:32:58.240
You know, whatever this damn thing is in us

01:32:58.240 --> 01:33:04.800
We get to say what we're pursuing and that seems a way in which you know our intelligence is

01:33:05.480 --> 01:33:07.480
Caught into play

01:33:07.600 --> 01:33:14.200
Isn't there a famous philosopher who said who said that our reason is a slave to the passions

01:33:14.720 --> 01:33:16.640
David Hume

01:33:16.640 --> 01:33:19.560
Reason is and ought only to be a slave to the passions

01:33:19.960 --> 01:33:21.960
Yeah, but that's you. Yeah

01:33:22.840 --> 01:33:28.440
And he's obviously wrong about that so so I'm very much on this. I'm very much on the side of the manual count here that we

01:33:29.400 --> 01:33:33.440
That that what is what is right is what is universalizable?

01:33:34.040 --> 01:33:41.160
there are so so so you know the the the maximum underpinning your conduct should be something that

01:33:41.960 --> 01:33:46.200
That is universalizable that could be could be a law for all

01:33:47.200 --> 01:33:52.480
There's there's an important contribution of the inclinations in here, right?

01:33:52.480 --> 01:33:59.320
So we have certain natural inclinations that we have to satisfy to continue our existence as the kinds of biological creatures

01:33:59.320 --> 01:34:04.440
That we are and we may have to discover things about people's inclinations empirically through interactions with each other

01:34:04.720 --> 01:34:05.880
but

01:34:05.880 --> 01:34:08.400
But the question of what we're inclined to do is

01:34:08.880 --> 01:34:13.080
Sabrable from the question of what we ought to do we can criticize our inclinations

01:34:13.080 --> 01:34:19.360
I could find myself inclined to do stuff that that I think is wrong actually even just kind of at the level of biological fitness, right?

01:34:19.360 --> 01:34:21.120
there's you know various

01:34:21.120 --> 01:34:26.520
appetites that I have towards you know sweet and salty foods and so on and I can I can appreciate

01:34:27.120 --> 01:34:30.800
rationally the reasons for which I have those inclinations and

01:34:31.320 --> 01:34:37.520
Actually reject them as you know in the current environment not contributing to my fitness. I can overrule them

01:34:38.120 --> 01:34:40.440
And that's not something that you know reckons can do

01:34:41.440 --> 01:34:44.000
That's why we're more intelligent than they are

01:34:48.480 --> 01:34:52.280
My sister literally had four raccoons in her garage this morning, so

01:34:55.440 --> 01:35:00.320
Okay, so obviously like a big question, so let me kind of try and step through that

01:35:01.440 --> 01:35:06.360
So we were talking about this a little bit yesterday, so I would say that I'm a motivational involuntary

01:35:06.960 --> 01:35:11.560
So I'll give the example of coffee, but cigarettes might be a good example for other people

01:35:12.600 --> 01:35:16.120
Over the course of my experience. I have acquired a very strong

01:35:16.920 --> 01:35:18.920
liking or a cup of coffee and

01:35:20.200 --> 01:35:25.120
That guides what I do, but again coffee is kind of a trivial case. Oh and Jennifer

01:35:25.120 --> 01:35:28.880
I definitely put an asterisk there just for you because I was like

01:35:31.880 --> 01:35:33.880
So in my defense

01:35:34.360 --> 01:35:38.840
Now I can't just reach in and say I'm not going to need that cup of coffee tomorrow

01:35:38.840 --> 01:35:45.400
I'm not going to need that cigarette and I I can't say either when you scale it up to some of my moral responses

01:35:45.920 --> 01:35:52.040
Maybe less savory moral responses. I'm just not going to judge that way or I'm going to do that thing

01:35:52.040 --> 01:35:55.360
So I don't think we can overrule them in the that that sense

01:35:55.760 --> 01:36:03.180
And I think that's basically I'm an early modern nationalist pre-con that I think that they're they're thick causal mechanisms

01:36:03.780 --> 01:36:05.780
Of course, we're not raccoons

01:36:05.980 --> 01:36:08.940
And we do have reasons we can make

01:36:09.340 --> 01:36:12.340
Outs as groups and and to the the group

01:36:12.860 --> 01:36:18.860
You know what what is normative is for group? I think one way to kind of start picking at that is that some groups are just better

01:36:19.780 --> 01:36:23.580
They have better principles and better norms that work better for the group than others

01:36:23.580 --> 01:36:25.580
So that should be a kind of clue that

01:36:25.580 --> 01:36:30.180
It's not you know that there there might be a kind of normative dimensions like okay

01:36:30.180 --> 01:36:34.220
These institutions that in fact work better towards the goals of us getting along together

01:36:34.980 --> 01:36:39.700
Close-eyed bar. So what do we do when we have this involuntarism?

01:36:39.700 --> 01:36:43.300
but we also come equipped with the raccoon plus of

01:36:44.700 --> 01:36:50.740
Knowing that there is a better way. I think we have relatively indirect paths

01:36:52.020 --> 01:36:57.060
To let's say diminishing my reward for coffee. So what I can do is I can

01:36:58.060 --> 01:37:02.500
You know drink a glass of salty water every day that I have a cup of coffee and

01:37:03.780 --> 01:37:05.780
Indirectly ratchet down

01:37:05.780 --> 01:37:10.780
The value that I place on coffee or the value that I place on the smell of a cup of coffee and so on and so forth

01:37:11.460 --> 01:37:15.300
So it's kind of an intermediate view where it's like I think we can leverage our reason

01:37:15.300 --> 01:37:18.180
But it's by no means as easy as some of us would want

01:37:18.580 --> 01:37:22.860
To hope that it is and I think it's important to emphasize this end of the debate

01:37:22.860 --> 01:37:28.300
I mean again, it's always a question of emphasis, but it's important to emphasize the difficulty because I think if we don't

01:37:28.460 --> 01:37:32.260
We think we could just pull the lever and I think that leads to all kinds of

01:37:33.420 --> 01:37:35.420
disasters basically

01:37:36.220 --> 01:37:37.980
So I want to

01:37:37.980 --> 01:37:39.660
React as well

01:37:39.660 --> 01:37:45.060
I'm really glad you asked this question because I think it's really the heart of the whole thing

01:37:45.060 --> 01:37:51.820
It's you know, is it demeaning to think that there we have there they have something that's a signal coming in

01:37:53.660 --> 01:37:55.660
I'm gonna call it pleasure and pain because it

01:37:56.660 --> 01:38:04.060
It goes with it with the demeaning aspect to it. We are a slave to our pleasure and pain and it makes us seem small

01:38:05.060 --> 01:38:10.780
And like we don't have choices and these are the most important things in our lives

01:38:10.900 --> 01:38:15.140
What are what we're trying to do with our lives and that we the idea we don't have

01:38:16.020 --> 01:38:18.020
Control of them is really annoying

01:38:23.860 --> 01:38:29.060
But I think it absolutely is true and it's true in kind of a definitional sense because

01:38:29.460 --> 01:38:35.300
When you say you have a reason for something you mean I do this because I'm trying to do this and

01:38:36.540 --> 01:38:43.980
Obviously throughout our lives. We have reasons like that. Why I'm going to work to earn money. I'm trying to earn money. So

01:38:45.620 --> 01:38:52.020
Raise a family. I'm trying to raise a family because I enjoy certain activities and I want to see things happening

01:38:52.020 --> 01:38:55.540
We all we have many many reasons and we're used to that

01:38:55.540 --> 01:39:01.100
But we also should acknowledge that eventually it has to stop, you know, you have reasons for things

01:39:01.100 --> 01:39:07.180
But eventually there's some final thing that doesn't have any reason for it or at least that's

01:39:08.260 --> 01:39:14.260
Maybe the standard way to think about things in science. I don't know, you know

01:39:14.260 --> 01:39:17.500
It's the old thing about the odds and the is is you can't derive

01:39:18.020 --> 01:39:22.340
By learning about the world you cannot really derive what you should want

01:39:23.300 --> 01:39:25.300
there's there is

01:39:25.540 --> 01:39:28.740
There is one standard philosophical view. I can't do it justice

01:39:28.740 --> 01:39:34.420
But but it's this the view that that you can't derive an ought from it is and that so you have to have some ultimate

01:39:34.700 --> 01:39:42.260
Oughts that are given and those ultimate odds are the things that we can't choose. So there must be something that we can't choose

01:39:43.260 --> 01:39:48.860
And that's what the reward is meant to be. It's the one thing that we can't choose comes from

01:39:49.700 --> 01:39:51.060
outside

01:39:51.060 --> 01:39:54.100
And it's an outside just means we can't choose it

01:39:54.740 --> 01:39:59.780
And so since that has to be true, I think even though it feels

01:40:01.100 --> 01:40:02.180
demeaning

01:40:02.180 --> 01:40:05.620
That we should get used to it and stop feeling that it makes it demeaning

01:40:05.620 --> 01:40:12.220
We have something that we ultimately want and and so we're ultimately and it's individual in some sense

01:40:12.300 --> 01:40:14.300
selfish

01:40:17.420 --> 01:40:19.420
And I think

01:40:19.460 --> 01:40:24.620
That if you embrace that it's kind of liberating, you know different people are different. They want different things

01:40:24.820 --> 01:40:27.060
It's okay for them to pursue different things

01:40:28.660 --> 01:40:30.660
Doesn't make us bad people

01:40:31.340 --> 01:40:35.620
I'm going to jump to another question because I want to make sure we get a few more people in here Sheila

01:40:36.020 --> 01:40:38.020
I

01:40:40.340 --> 01:40:44.500
Thank you so much to all three of you for for such an engaging

01:40:45.460 --> 01:40:47.460
discussion, so I was I

01:40:47.940 --> 01:40:49.940
Guess I wanted to start by just

01:40:50.060 --> 01:40:54.820
Harking back to a conversation that that rich and I had yesterday. I think I and in defense of John McCarthy

01:40:54.820 --> 01:41:03.620
I think that McCarthy in 1955 defined a notion of artificial intelligence and and that that intelligence is is a vague term

01:41:03.820 --> 01:41:07.900
Just like our conversation yesterday about about baldness, you know

01:41:07.900 --> 01:41:12.820
If I start pulling hairs out of my head at what point do I recognize that it that I'm bald

01:41:12.820 --> 01:41:16.980
Whereas with when I put that hair back in I'm when you call me this way. Yeah

01:41:18.580 --> 01:41:23.180
Anyways, I think there are lots of terms that are vague and I think that people define them purposefully

01:41:23.180 --> 01:41:25.180
And I think that intelligence is one of those so

01:41:25.660 --> 01:41:29.460
But what I but I wanted to I was really struck by by Julia's

01:41:30.100 --> 01:41:36.060
Discussion of binocular rivalry because I feel like I'm having binocular rivalry, you know between the two of you

01:41:36.060 --> 01:41:41.420
And and that we're all just trying to make sense of this this complicated idea

01:41:41.420 --> 01:41:45.180
And and we and again to a point that Julia made you know

01:41:45.220 --> 01:41:52.020
What we see the way that we interpret the way that we make sense of the world is indeed informed by by some sort of

01:41:52.380 --> 01:41:56.980
Bias we have or it's sorry. That's not the word that you use but some sort of expectation of what we want to achieve

01:41:57.140 --> 01:41:59.140
So so what I wanted to just to

01:42:00.580 --> 01:42:02.740
To to make sense of myself is

01:42:03.340 --> 01:42:10.900
Where in your conceptualizations of things you you absolutely disagree or where you actually where we're some of the

01:42:11.260 --> 01:42:15.820
Disagreements we see and even to Gillian's question about the reward hypothesis is just about what we actually

01:42:16.620 --> 01:42:20.860
Decide to highlight in our formalization or our understanding of things like so

01:42:21.380 --> 01:42:26.620
Rich even when I was looking at your your decision-making box on the right, which is one that and one of your slides

01:42:26.620 --> 01:42:33.380
Which is one that I'm very familiar with, you know, I thought where's memory? Where's long-term memory everything's been compiled

01:42:33.380 --> 01:42:35.380
You know, we're from classical

01:42:36.500 --> 01:42:45.460
Decision-making where's the notion of a you know of a of a that predictive mechanism is actually being not sort of compiled into the value

01:42:45.460 --> 01:42:48.260
function or everything being compiled into

01:42:49.140 --> 01:42:56.380
Scalar reward to Julia's response to to to Gillian and and so I guess I want to push back on on saying

01:42:56.380 --> 01:42:58.900
What are the elements that are important? Where's long-term memory?

01:42:59.180 --> 01:43:06.780
Where is where is this notion some of the the elements that actually contribute to a reward eventually being compiled into a scalar reward?

01:43:06.780 --> 01:43:09.660
And and is that diagram for you on the right?

01:43:10.140 --> 01:43:13.700
really representing everything that that it needs and then I guess

01:43:13.860 --> 01:43:19.780
Point and and and again, I thought about things again that we think about in the context of multi-agent systems about about

01:43:20.620 --> 01:43:23.740
Goal-seeking, you know the ability to create goals about

01:43:24.340 --> 01:43:31.460
Commitment about intention all of those elements that we often use to make sense of the world that that are not part of your diagrams

01:43:31.460 --> 01:43:34.700
Probably we can they're compiled in somewhere in there

01:43:34.700 --> 01:43:36.860
But I think there's utility in including them

01:43:36.860 --> 01:43:43.260
So what what do we have are there things in your formalisms that are just compiled away that are important and then?

01:43:43.260 --> 01:43:48.860
I guess the other question was where do we fundamentally or where do you in your binocular rivalry?

01:43:49.380 --> 01:43:50.620
fundamentally

01:43:50.620 --> 01:43:54.220
Or my my sense of you're the binocular of rivalry that I'm having

01:43:54.740 --> 01:43:57.780
fundamentally disagree and and and I was and maybe this is a

01:43:59.580 --> 01:44:01.580
Conversation to take offline and for afterwards

01:44:01.580 --> 01:44:08.220
But one of the things I was intrigued about is this you know what what I perceive is our perhaps our ability to be able to reflect on our

01:44:08.900 --> 01:44:14.940
Art to have an awareness of our reward to be able to reflect on our reward as as as we do

01:44:15.140 --> 01:44:22.860
Representation other representations and whether that's an element that that that that is of utility and explaining some of what we see is

01:44:23.100 --> 01:44:26.260
Understands the mind. So that's it. Well, those are a lot of things

01:44:26.820 --> 01:44:28.820
Julie is writing down her thoughts

01:44:30.180 --> 01:44:32.620
I'm not writing so I have to go first because

01:44:33.540 --> 01:44:35.540
I'll forget

01:44:35.900 --> 01:44:40.860
There are many things I will I will forget some remind me if you get them wrong

01:44:44.700 --> 01:44:46.900
First memory long-term memory

01:44:48.820 --> 01:44:54.780
There were four boxes all four boxes learn and so maybe they're four boxes are for

01:44:55.420 --> 01:44:58.780
Maybe they're for neural networks. You that'd be a fine way to think about it

01:44:58.780 --> 01:45:01.960
And then they all have weights and those weights are all adjusted through

01:45:02.780 --> 01:45:08.700
Learning and planning both so that they would all have long-term memory. Memory is throughout

01:45:13.460 --> 01:45:15.460
So I want us

01:45:15.660 --> 01:45:18.840
We can get a sense of freedom. We get a sense of intention

01:45:19.380 --> 01:45:23.980
We can set a sense of choosing goals because I do think it's a really major

01:45:24.780 --> 01:45:32.220
Psychological thing that we choose goals. We choose except they're not they're not they're not reward goals

01:45:32.220 --> 01:45:35.900
So maybe they shouldn't be called goals. They are sub-goals. They are

01:45:37.180 --> 01:45:38.620
strategies

01:45:38.620 --> 01:45:39.980
that

01:45:39.980 --> 01:45:45.900
That you choose as part of your solution of the problem the problem is fixed the problem is reward

01:45:46.380 --> 01:45:53.620
The problem is is it's the reward is not a compilation of something. It is the it is the thing that is unchanged

01:45:54.380 --> 01:45:56.380
It is not a compilation of something

01:45:57.180 --> 01:45:59.380
other things are compiled like by

01:46:00.380 --> 01:46:07.060
Learning what gets rewarding and what's not rewarding you decide what to do by learning what's rewarding. What's not rewarding you?

01:46:07.060 --> 01:46:12.180
You compile you compile into the policy you compile into the value function

01:46:12.860 --> 01:46:17.700
You even compile into the model and you compile into the perceptual process where you're figuring out

01:46:17.700 --> 01:46:23.100
Oh, this is a good way to think about the state that I'm in or this is a bad way to think about the state that I'm in

01:46:23.380 --> 01:46:27.100
all those are learned and in that sense compiled and

01:46:28.060 --> 01:46:34.020
If you want to look at reasoning reasoning is in the model of the world we learned that you know this causes that and

01:46:34.700 --> 01:46:36.100
that that that

01:46:36.100 --> 01:46:43.220
Reasoning that that knowledge of the world is in multiple levels low levels low level physics and high level decisions like like oh

01:46:43.220 --> 01:46:45.780
I'll go to the conference or I'll take this job

01:46:48.180 --> 01:46:55.260
But there's no compiling in the reward signal or you know if there's any compiling it would be evolution compiled its goals into our

01:46:55.380 --> 01:46:57.380
Reward, but for us

01:46:57.900 --> 01:47:01.660
Reward is just a primitive thing and it's never never changed

01:47:03.340 --> 01:47:08.940
And yet we absolutely it's appropriate to have the sense that we choose our sub-goals

01:47:08.940 --> 01:47:14.540
And that's the most important thing we make I'm gonna I'm going to choose to study

01:47:16.540 --> 01:47:18.540
Economics or

01:47:19.260 --> 01:47:21.260
Or our philosophy

01:47:21.940 --> 01:47:28.420
Or I'm going to choose to devote my life to understanding how the mind works. Okay, those are and I'm thinking

01:47:28.860 --> 01:47:31.360
I'm not thinking about this, but

01:47:32.420 --> 01:47:39.460
Maybe if I devote my life to figuring out how the mind works, I will enjoy myself better. I think that's what the way it goes

01:47:40.980 --> 01:47:42.980
I will forget more reward

01:47:43.820 --> 01:47:51.320
So I think yeah, there's also a multi-part and I feel like Joel basically asked you Sheila to ask this question

01:47:53.540 --> 01:47:57.540
So in terms of the question of you know emphasis and binocular rivalry, you know

01:47:57.540 --> 01:48:01.360
I think that's just the nature of scientific and philosophical theorizing, right?

01:48:01.380 --> 01:48:05.180
It's a question of emphasis in a lot of cases

01:48:05.340 --> 01:48:08.980
So depending on how we configure our audience

01:48:08.980 --> 01:48:15.660
We would either be like aggressive opponents or allies depending on who we're engaging with and the point that you're trying to get across

01:48:16.060 --> 01:48:18.180
There's just a sociological feature of that

01:48:19.020 --> 01:48:23.060
But I think where things really start to come apart is what predictions

01:48:23.540 --> 01:48:28.540
Do your commitments generate and what implications follow from your view?

01:48:28.900 --> 01:48:32.220
And that's where you also start to see some departures between us

01:48:32.940 --> 01:48:37.380
So just examples from the kind of conversation that we've had so far that I'm thinking about is okay

01:48:37.380 --> 01:48:40.540
I think we actually really disagree on this pleasure and pain thing like we're like oh

01:48:40.900 --> 01:48:44.900
No, we really disagree about that and it will generate different predictions

01:48:44.900 --> 01:48:51.660
And it has very different implications for how we understand certain things about the nature of human beings and and how we kind of follow from that

01:48:51.660 --> 01:48:53.660
I think we disagree on

01:48:55.900 --> 01:49:01.140
This normative question of what then we should take away from these experiences of values that we have

01:49:01.460 --> 01:49:04.860
It's funny because I wrote down liberating to but I mean it in a very different way

01:49:05.260 --> 01:49:09.860
It's liberating in the sense that if you recognize your limitations, that's very liberating

01:49:09.860 --> 01:49:12.700
So you accept the flaws in your system

01:49:12.700 --> 01:49:19.660
That's actually very empowering to the way that I was mentioning with the coffee because then I can use I can leverage that to become better

01:49:20.500 --> 01:49:22.660
and

01:49:22.660 --> 01:49:26.060
again, so you'll see different predictions there about what works in terms of

01:49:27.100 --> 01:49:33.740
Self-regulation and there will just be departures and in the kinds of commitments they have and then I would say in terms of

01:49:34.380 --> 01:49:41.740
What we're striving for with artificial intelligence and how we should recognize them for example as people that would be another place where

01:49:43.060 --> 01:49:47.940
You would actually see our commitments that look very similar in some places

01:49:49.140 --> 01:49:51.140
separate because of

01:49:51.820 --> 01:49:54.260
the implications that follow from that so I think

01:49:55.780 --> 01:50:01.540
But it but it really is like there's partly a sociological phenomenon of who we're engaging with and who we're trying to persuade

01:50:02.420 --> 01:50:06.420
Where in some cases we're a united front because I think the reward hypothesis

01:50:07.580 --> 01:50:12.340
Is in a minority or can be in a minority despite how powerful it is

01:50:12.900 --> 01:50:17.180
But then you know between one another we would see see big differences or something like that

01:50:19.820 --> 01:50:24.340
Thank you, and this may be our last question we'll see yeah

01:50:25.620 --> 01:50:28.620
This was a really fascinating talk I

01:50:29.340 --> 01:50:34.780
Don't have a specific question in particular, but I do want to ask in the

01:50:35.580 --> 01:50:40.140
Interests of internalizing reward and self-regulation. I think those things you brought up

01:50:41.420 --> 01:50:46.140
If it's not beyond the scope of your research, I wanted to ask about reward hacking and

01:50:47.180 --> 01:50:52.420
The idea of good-hearted law which is when the measure becomes a target it seems to be a good measure

01:50:52.940 --> 01:50:55.620
I'm really interested in hearing if you have any thoughts on that

01:50:59.620 --> 01:51:01.620
I don't know if I heard everything

01:51:02.500 --> 01:51:07.700
But reward hacking definitely happens to people. Maybe that's what drug abuse is about

01:51:23.900 --> 01:51:26.300
I'm not sure I understood everything you're asking I

01:51:27.300 --> 01:51:29.300
Can repeat if

01:51:31.260 --> 01:51:37.620
Yeah, so I was asking about reward hacking which is something you answered but also good-hearted law which is

01:51:38.780 --> 01:51:42.060
When the measure becomes a target it ceases to be a good measure

01:51:43.140 --> 01:51:49.260
If you're familiar with that law, I wanted to ask in the interest of self-regulation as something Julia mentioned

01:51:49.980 --> 01:51:51.980
If there are thoughts on that

01:51:57.300 --> 01:52:02.700
I guess good-hearted law doesn't apply because that's sort of a community thing

01:52:03.060 --> 01:52:07.520
You know it gets the measure gets manipulated by actors

01:52:08.180 --> 01:52:10.180
But if it's your own measure inside your head

01:52:13.020 --> 01:52:16.900
It doesn't get get lost in its importance in that way

01:52:20.380 --> 01:52:22.380
Self-regulation

01:52:22.380 --> 01:52:24.380
I

01:52:24.780 --> 01:52:26.780
Mean it's super important to see

01:52:28.140 --> 01:52:33.420
Binocular rivalry the two different views. It's what that's the most important thing that's going on here that you know like oh

01:52:33.900 --> 01:52:40.180
It's it's ugly to think that we're just doing one number of pain and pleasure or whatever it is rather than

01:52:40.740 --> 01:52:42.740
The grand reviews we have ourselves

01:52:44.100 --> 01:52:46.100
But they're both true

01:52:46.540 --> 01:52:51.580
We have to self-regulate we have to decide what what is our defining goal

01:52:52.020 --> 01:52:57.820
And it's it's we we have to decide but I've just said we don't have to decide because it's given and

01:52:58.300 --> 01:53:03.700
It's in it and it's in us, but we don't know what it is and besides

01:53:03.940 --> 01:53:09.220
It's it's there's many senses in which the values are more important than the rewards

01:53:09.220 --> 01:53:11.220
I mean the rewards are the ultimate determination

01:53:11.860 --> 01:53:18.420
But the values are what the consequences of that that come about in interaction with the world

01:53:18.660 --> 01:53:26.620
In interaction with the world, I figure out that the thing I need to do in order to get the most reward I

01:53:28.580 --> 01:53:30.580
Have to figure that out

01:53:30.580 --> 01:53:32.820
The fact that the rewards are fixed doesn't help me

01:53:33.340 --> 01:53:38.820
Doesn't inform me about what what I should do in the world to decide what to do in the world and what I should

01:53:39.380 --> 01:53:42.060
Say to myself and say to others is my goal

01:53:43.500 --> 01:53:46.940
That will be all about value and values come by

01:53:48.420 --> 01:53:53.700
Through the complex interaction with the world and what is what is actually going to make me happier in the long run?

01:53:53.740 --> 01:53:57.860
You know the rewards can't tell you that the words just tell you when you get there

01:53:57.860 --> 01:53:59.700
I'll tell you if you're happy or not

01:53:59.700 --> 01:54:03.860
They can't tell you what you should do and they can't tell you so you have to decide

01:54:03.860 --> 01:54:09.900
You know which which career to have and and which person to be involved in and all those

01:54:10.260 --> 01:54:15.580
Subtle hard decisions. It doesn't the fact that it's fixed doesn't help me make those decisions. You still have to do those

01:54:15.580 --> 01:54:17.580
I

01:54:18.260 --> 01:54:22.300
Think I probably also will and I'm looking at the clock will also only address part of your question

01:54:22.300 --> 01:54:28.100
but um, I do think reward hacking occurs and people I think addiction is kind of the edge case of that

01:54:28.540 --> 01:54:31.860
But I'm wondering whether you would also find there's a philosophical literature

01:54:32.220 --> 01:54:34.220
Maybe just one philosophical author

01:54:34.500 --> 01:54:40.420
On this idea of value capture and so I think one of the examples given in the phenomenon value capture is

01:54:40.740 --> 01:54:42.740
people wear like a Fitbit

01:54:42.860 --> 01:54:49.420
And the Fitbit tracks the number of steps that you take and obviously you need to take steps in order to maintain your health or like

01:54:49.420 --> 01:54:51.740
whatever the man tries to tell you and

01:54:52.420 --> 01:54:54.420
I'm just kidding. It's probably important and

01:54:55.540 --> 01:54:57.540
But people become obsessed

01:54:58.220 --> 01:55:02.620
With getting their steps. They're like they if they go for a walk and they don't have their Fitbit

01:55:02.620 --> 01:55:06.220
It didn't count and that's an example of value capture. That's obviously like a very

01:55:07.060 --> 01:55:12.220
Sort of relatively cute example of value capture, but I think this is something that happens like

01:55:13.500 --> 01:55:19.620
Professionally, right? It happens like if we have certain goals like, you know, the pursuit of knowledge or contributing to something, you know

01:55:19.620 --> 01:55:25.580
But we value capture in in publications or whatever, you know, lots of cases across not just an academic phenomenon

01:55:25.580 --> 01:55:27.580
This is something that happens

01:55:27.980 --> 01:55:32.060
Across different aspects of human life. I don't know if that's exactly what you're getting at but

01:55:32.860 --> 01:55:40.300
In terms of self-regulation, I think understanding again the understanding is extremely powerful because if you recognize what's happening

01:55:40.860 --> 01:55:46.380
You can regulate around that. So I think just based on our conversation a little bit yesterday based on your interest

01:55:46.380 --> 01:55:50.820
I think I wonder I was wondering whether value capture might be something that you're also interested in looking at

01:55:52.420 --> 01:55:58.300
Which is just a kind of nuanced version of how that happens in everyday life, I think

01:55:59.900 --> 01:56:05.780
Thank you. Well, this is we are we are at two hours folks. So we're gonna we're gonna call him here

01:56:05.780 --> 01:56:07.780
But we have a 30 minute a

01:56:07.940 --> 01:56:10.020
30-minute coffee break now

01:56:10.140 --> 01:56:17.780
So I'll invite you to to join us and I'm actually gonna ask that you let our speakers move to the coffee room before

01:56:18.300 --> 01:56:25.020
Asking them questions so they don't get trapped here, which I've started to notice and do come back with us at 11 30

01:56:25.020 --> 01:56:32.700
We have Abby Goldfarb Daniel Daniel Rock and Frank British on in machine learning in the workplace, which will be as as riveting as this one

01:56:32.700 --> 01:56:34.700
Thanks so much also

01:56:37.780 --> 01:56:40.780
You

