1
00:00:00,000 --> 00:00:04,640
Awesome, so we're so excited to have Hila Chepur, am I pronouncing it right?

2
00:00:04,640 --> 00:00:06,400
Yeah, it's actually in Hebrew, it's Chepur.

3
00:00:06,400 --> 00:00:08,800
Oh, it's Chepur.

4
00:00:08,800 --> 00:00:10,080
Yeah.

5
00:00:10,080 --> 00:00:10,880
All right.

6
00:00:10,880 --> 00:00:13,840
So Hila is a PhD candidate at Tel Aviv University,

7
00:00:13,840 --> 00:00:15,680
advised by Professor Leon Wolfe.

8
00:00:16,400 --> 00:00:19,920
Her research focuses on developing reliable XAI algorithms

9
00:00:19,920 --> 00:00:22,880
and leveraging them to promote model accuracy and fairness.

10
00:00:23,760 --> 00:00:26,640
Today she's going to talk to us about transform or explainability,

11
00:00:26,640 --> 00:00:28,560
and we're so excited to hear from you.

12
00:00:28,640 --> 00:00:30,400
Oh, thank you for that great introduction,

13
00:00:30,400 --> 00:00:32,320
and I actually have some slides here.

14
00:00:32,960 --> 00:00:33,760
I think you can just...

15
00:00:34,480 --> 00:00:35,440
Oh, yeah.

16
00:00:35,440 --> 00:00:38,880
Slides here introducing myself, but I think you did that perfectly,

17
00:00:38,880 --> 00:00:39,760
so I'll just skip that.

18
00:00:41,200 --> 00:00:44,560
So maybe the first thing we want to talk about is motivation.

19
00:00:44,560 --> 00:00:46,560
We're here to talk about transform or explainability,

20
00:00:46,560 --> 00:00:47,920
but why should you care?

21
00:00:48,640 --> 00:00:50,080
And let's just have a disclaimer,

22
00:00:50,080 --> 00:00:52,640
because we all know that explainability is really important

23
00:00:52,640 --> 00:00:56,240
for aspects like accountability, reliability, and so on.

24
00:00:56,240 --> 00:00:57,680
But when we write research papers,

25
00:00:57,680 --> 00:01:00,080
we usually focus on other measuring sticks,

26
00:01:00,080 --> 00:01:03,040
such as accuracy and robustness, right?

27
00:01:03,680 --> 00:01:06,800
So I'm here to convince you that the explainability queue

28
00:01:06,800 --> 00:01:10,240
is actually really useful for those measuring sticks as well,

29
00:01:10,240 --> 00:01:13,360
and that you should consider using explainability

30
00:01:13,360 --> 00:01:15,200
in your research, even if it's unrelated

31
00:01:15,200 --> 00:01:17,200
to accountability, reliability, and so on.

32
00:01:18,640 --> 00:01:21,120
So to do that, I have a few examples showing

33
00:01:21,120 --> 00:01:24,080
how explainability can be used to improve model accuracy

34
00:01:24,080 --> 00:01:25,680
and robustness.

35
00:01:25,760 --> 00:01:27,520
The first example is text to live.

36
00:01:27,520 --> 00:01:28,240
Maybe you know it.

37
00:01:28,240 --> 00:01:30,560
It got accepted to ECCV 2022.

38
00:01:31,120 --> 00:01:35,200
And the objective of the model is to take a target edit text,

39
00:01:35,200 --> 00:01:39,440
for example, here around hat, and apply the edit to the image.

40
00:01:39,440 --> 00:01:42,320
So what they try to do is unlike other works,

41
00:01:42,320 --> 00:01:45,600
they try to prevent the part where the user actually has

42
00:01:45,600 --> 00:01:48,960
to insert a manual segmentation mask

43
00:01:48,960 --> 00:01:51,120
to indicate where the hat is.

44
00:01:51,680 --> 00:01:54,800
So they wanted some automatic way of getting

45
00:01:54,800 --> 00:01:57,040
to the region of interest, which is the hat.

46
00:01:57,040 --> 00:01:59,360
So they use relevancy maps of the clip model,

47
00:01:59,360 --> 00:02:00,880
which I guess you're familiar with.

48
00:02:02,560 --> 00:02:06,000
So the clip model with the relevancy map actually indicated

49
00:02:06,000 --> 00:02:12,480
to their downstream path, where pretty much the hat is in the image.

50
00:02:13,040 --> 00:02:16,080
And then their model refined those relevancy maps

51
00:02:16,080 --> 00:02:20,480
and applied the edit according to the location indicated

52
00:02:20,480 --> 00:02:21,840
by the relevancy map.

53
00:02:21,840 --> 00:02:24,000
So here you can see what happens without this.

54
00:02:24,000 --> 00:02:26,880
They call it bootstrapping without using the relevancy map.

55
00:02:26,880 --> 00:02:29,120
So you can see that there are additional artifacts,

56
00:02:29,120 --> 00:02:31,920
such as the faces turning red and not just the hat.

57
00:02:31,920 --> 00:02:34,640
And when you use bootstrapping, when you use relevancy maps,

58
00:02:34,640 --> 00:02:38,240
then the edit is quite localized to the hat.

59
00:02:38,240 --> 00:02:40,720
And here you can see other edits that are quite nice.

60
00:02:40,720 --> 00:02:44,160
They take a sponge cake and they turn it into an ice cake

61
00:02:44,160 --> 00:02:45,280
or a spinach mousse cake.

62
00:02:45,280 --> 00:02:47,760
So I think it's a nice example to show.

63
00:02:48,800 --> 00:02:51,360
Another example is a paper called Kripaso.

64
00:02:51,360 --> 00:02:54,240
It's that best paper awarded Seagraph 2022.

65
00:02:54,800 --> 00:02:56,640
It also uses relevancy maps.

66
00:02:56,640 --> 00:02:59,120
The goal of the model is to take an input image

67
00:02:59,120 --> 00:03:02,480
and create sketches with different levels of abstraction.

68
00:03:02,480 --> 00:03:04,720
So you can choose which level of abstraction you want.

69
00:03:04,720 --> 00:03:06,240
So for example, the flamingo here,

70
00:03:06,960 --> 00:03:11,680
very abstract painting of the flamingo or very detailed painting.

71
00:03:11,680 --> 00:03:13,840
And what they did is they actually used the relevancy maps

72
00:03:13,840 --> 00:03:18,880
as an initializer for the model to understand where the object is

73
00:03:18,880 --> 00:03:20,960
and how to create the stroke's product.

74
00:03:22,160 --> 00:03:23,760
Another example is Sameer's work,

75
00:03:23,760 --> 00:03:25,680
which you're probably familiar with.

76
00:03:27,760 --> 00:03:31,040
What you did here, really, is you used the relevancy maps

77
00:03:31,040 --> 00:03:34,160
in order to locate objects which aren't necessarily

78
00:03:34,880 --> 00:03:38,480
objects in the wild, objects that appear in living rooms

79
00:03:38,480 --> 00:03:42,160
and that do not necessarily appear in the training set

80
00:03:42,160 --> 00:03:46,160
of some models of desegmentation or localization.

81
00:03:46,160 --> 00:03:49,120
So you can use Kripaso in order to identify objects

82
00:03:49,120 --> 00:03:51,920
that are really not really objects

83
00:03:51,920 --> 00:03:53,680
that are so common in training sets.

84
00:03:55,760 --> 00:03:59,280
So if you consider all the examples that we've seen before,

85
00:03:59,280 --> 00:04:01,040
they have one thing in common.

86
00:04:01,040 --> 00:04:04,160
They use the relevancy maps as a fixed signal.

87
00:04:04,160 --> 00:04:06,640
They didn't train on the relevancy map

88
00:04:06,640 --> 00:04:08,480
or create a loss with the relevancy map.

89
00:04:08,480 --> 00:04:11,840
They use it as an initialization for the answering task.

90
00:04:12,400 --> 00:04:14,880
But what we did in our last work is actually just,

91
00:04:14,880 --> 00:04:18,560
we showed that its reliability can be used as a loss term

92
00:04:18,560 --> 00:04:20,080
in order to improve models.

93
00:04:20,640 --> 00:04:22,560
So if you think about what it means

94
00:04:22,560 --> 00:04:25,440
to create a loss term based on explainability maps,

95
00:04:25,440 --> 00:04:29,440
it's really meant to teach the model how to do something

96
00:04:29,440 --> 00:04:32,800
or why it does something, not just how to do something.

97
00:04:32,800 --> 00:04:34,800
And we talk about classification models.

98
00:04:34,800 --> 00:04:37,760
They tend to learn spurious cues

99
00:04:37,760 --> 00:04:40,960
to help them make shortcuts to make a prediction.

100
00:04:40,960 --> 00:04:43,360
So for example, a model can learn a spurious cue

101
00:04:43,360 --> 00:04:47,360
that if you have a round object with the background of a grass,

102
00:04:47,360 --> 00:04:48,800
then it's a golf ball.

103
00:04:48,800 --> 00:04:51,040
And here you can see that the model classified this lemon

104
00:04:51,040 --> 00:04:53,520
as a golf ball because of the background of the grass.

105
00:04:54,160 --> 00:04:56,560
When you force the model to actually focus

106
00:04:56,560 --> 00:04:58,240
on the foreground of the image

107
00:04:58,240 --> 00:04:59,760
and not just the background of the image,

108
00:05:00,560 --> 00:05:04,080
via a loss applied directly to the explainability maps,

109
00:05:04,080 --> 00:05:06,880
you can correct wrong predictions based on spurious cues.

110
00:05:07,440 --> 00:05:10,080
So what we're doing usually is we're teaching the model

111
00:05:10,080 --> 00:05:11,440
to predict something, right?

112
00:05:11,440 --> 00:05:15,360
Predict golf ball, car, glasses, etc.

113
00:05:15,360 --> 00:05:17,600
But we're not really teaching it why.

114
00:05:17,600 --> 00:05:20,240
Why this is the object in the image.

115
00:05:20,240 --> 00:05:24,800
So what we're showing here is that by fine-tuning directly

116
00:05:24,800 --> 00:05:27,440
the relevant slots or the explainability maps,

117
00:05:27,440 --> 00:05:30,160
we can correct wrong predictions based on spurious conditions.

118
00:05:30,960 --> 00:05:37,200
But we'll get to it in depth later on.

119
00:05:38,400 --> 00:05:40,000
So this was the motivation part,

120
00:05:40,000 --> 00:05:42,480
and hopefully you got fully motivated

121
00:05:42,480 --> 00:05:44,720
as to why Transformer experiment is interesting.

122
00:05:45,360 --> 00:05:48,240
Our talk is going to be a construed of two parts.

123
00:05:48,240 --> 00:05:49,840
The first part is going to be,

124
00:05:49,840 --> 00:05:52,720
we're going to talk about how we do Transformer explainability.

125
00:05:52,720 --> 00:05:54,640
We're going to see the attention mechanism

126
00:05:54,640 --> 00:05:56,160
which I'm sure you're all familiar with,

127
00:05:56,160 --> 00:05:59,280
but we're going to have emphasis on specific parts

128
00:05:59,280 --> 00:06:01,280
of the attention mechanism that are going to be useful.

129
00:06:01,840 --> 00:06:03,200
Then we're going to ask ourselves,

130
00:06:03,200 --> 00:06:04,720
is attention an explanation?

131
00:06:04,720 --> 00:06:07,360
Which is really the most prominent question

132
00:06:07,360 --> 00:06:09,440
when doing Transformer explainability.

133
00:06:09,440 --> 00:06:12,400
We're going to talk about three explainability algorithms.

134
00:06:12,400 --> 00:06:14,240
The first one is attention roll-up, not by me.

135
00:06:14,960 --> 00:06:17,840
But it is the groundbreaking first algorithm

136
00:06:17,840 --> 00:06:19,200
in the big Transformer explainability.

137
00:06:19,200 --> 00:06:21,360
Then I'm going to present two of my works

138
00:06:21,360 --> 00:06:23,280
that have to do with Transformer explainability.

139
00:06:24,160 --> 00:06:25,040
And in the last part,

140
00:06:25,040 --> 00:06:27,360
we're going to talk about the work that I just presented

141
00:06:27,360 --> 00:06:29,280
and that Shiran had a question on.

142
00:06:30,240 --> 00:06:32,640
And probably hopefully we'll answer the questions

143
00:06:32,640 --> 00:06:35,200
and see how Transformer explainability can be used to

144
00:06:35,840 --> 00:06:40,320
devise models or maybe incorrect spray-excused

145
00:06:40,320 --> 00:06:41,040
at the models where.

146
00:06:41,040 --> 00:06:44,800
So just to set us up,

147
00:06:44,800 --> 00:06:46,320
this is a Transformer architecture

148
00:06:46,320 --> 00:06:48,000
as presented in attention is all you need.

149
00:06:48,000 --> 00:06:49,520
I'm sure you're all familiar with it.

150
00:06:50,080 --> 00:06:52,320
We will be focusing on the encoder

151
00:06:52,320 --> 00:06:54,240
since the attention mechanism here

152
00:06:54,240 --> 00:06:55,840
is a self-attention mechanism.

153
00:06:55,840 --> 00:06:57,040
And it's quite more intuitive

154
00:06:57,040 --> 00:06:58,960
and easy to grasp and understand.

155
00:06:58,960 --> 00:07:01,200
And the concepts that apply here to the encoder

156
00:07:01,200 --> 00:07:03,680
are pretty easily generalized to the encoder.

157
00:07:03,680 --> 00:07:06,320
So we're going to focus on the encoder for simplicity

158
00:07:06,320 --> 00:07:08,640
and for intuitive explanations,

159
00:07:08,640 --> 00:07:11,840
but the principle is quite easily generalized

160
00:07:11,840 --> 00:07:13,040
to the encoder as well.

161
00:07:13,040 --> 00:07:14,160
By the way, if you have questions,

162
00:07:14,160 --> 00:07:15,680
just feel free to stop me.

163
00:07:17,680 --> 00:07:20,960
Okay, so let's talk about the intuition behind self-attention.

164
00:07:21,520 --> 00:07:23,760
What self-attention does is actually creates

165
00:07:24,480 --> 00:07:26,160
contextualized representations.

166
00:07:26,160 --> 00:07:27,680
So I'm sorry for the people on Zoom,

167
00:07:27,680 --> 00:07:29,520
but I'm going to write here on the right for it.

168
00:07:29,520 --> 00:07:31,040
So we have an example.

169
00:07:31,040 --> 00:07:32,560
We're running an example to work with.

170
00:07:33,120 --> 00:07:35,840
Say we have the sentence, the count,

171
00:07:36,800 --> 00:07:40,240
set on the max.

172
00:07:42,560 --> 00:07:44,800
And let's consider the first day in the sentence.

173
00:07:45,760 --> 00:07:48,560
We want to create now an embedding

174
00:07:48,560 --> 00:07:50,240
for each one of the words in the sentence,

175
00:07:50,240 --> 00:07:52,320
for each one of the token incidents.

176
00:07:52,320 --> 00:07:54,160
But the token does quite meaningless

177
00:07:54,160 --> 00:07:55,440
with our context, right?

178
00:07:55,440 --> 00:07:56,800
It could refer to anywhere.

179
00:07:57,360 --> 00:07:58,960
So what the attention mechanism does

180
00:07:58,960 --> 00:08:01,520
is it actually creates contextualized representation.

181
00:08:02,160 --> 00:08:05,920
It should take information from the other token

182
00:08:05,920 --> 00:08:09,920
and insert it to the current token that we're interested in.

183
00:08:10,480 --> 00:08:13,440
So for example, intuitively, maybe we would expect

184
00:08:13,440 --> 00:08:17,360
that since the word the refers to the word cat,

185
00:08:18,480 --> 00:08:22,240
information from the word cat will be moved into the word the,

186
00:08:22,240 --> 00:08:25,680
such that the embedding of the word that is contextualized

187
00:08:25,680 --> 00:08:29,280
is enriched by context from the word cat.

188
00:08:30,240 --> 00:08:32,240
So what the attention mechanism does

189
00:08:32,240 --> 00:08:36,160
is it actually uses query, key, and value matrices.

190
00:08:36,160 --> 00:08:40,880
And we can think about it maybe as a database theory information.

191
00:08:41,440 --> 00:08:44,960
So when we talk about databases, we have queries,

192
00:08:46,880 --> 00:08:49,680
which are questions that we run on our database.

193
00:08:50,480 --> 00:08:51,680
We have keys.

194
00:08:51,680 --> 00:08:54,160
The keys represent the entries in our database.

195
00:08:54,800 --> 00:08:57,840
And we have values that corresponds to the keys, right?

196
00:08:57,840 --> 00:09:01,840
So what we're doing here is we're actually running a query

197
00:09:01,840 --> 00:09:12,000
asking which tokens are relevant to the...

198
00:09:14,160 --> 00:09:16,560
We can think about it intuitively as running a query

199
00:09:16,560 --> 00:09:18,560
on all of these tokens that we have.

200
00:09:19,120 --> 00:09:21,520
And then the keys represent all the other tokens.

201
00:09:22,080 --> 00:09:23,840
What we do with the attention mechanism

202
00:09:24,160 --> 00:09:26,960
is we calculate an attention matrix

203
00:09:26,960 --> 00:09:29,840
that is going to be the star of every transformer experience

204
00:09:29,840 --> 00:09:30,960
ability algorithm.

205
00:09:31,840 --> 00:09:35,840
It's going to be a soft mass of the multiplication

206
00:09:35,840 --> 00:09:40,960
between queries and keys normalized by the embedding dimension.

207
00:09:41,840 --> 00:09:45,840
This similarity scores are actually telling us

208
00:09:45,840 --> 00:09:49,840
how much each word is relevant to our word of interest.

209
00:09:50,800 --> 00:09:52,640
So the multiplication between queries and keys,

210
00:09:52,640 --> 00:09:55,840
we can think about it kind of like as relevant scores.

211
00:09:55,840 --> 00:09:59,040
How much is each token relevant to the token that...

212
00:09:59,040 --> 00:10:00,240
to the word that...

213
00:10:03,040 --> 00:10:06,240
And after we calculate those similarity scores,

214
00:10:06,240 --> 00:10:08,480
we create the enriched representation

215
00:10:09,200 --> 00:10:11,840
by multiplying the scores by the values.

216
00:10:13,040 --> 00:10:16,640
Such that each word gets information from the other words

217
00:10:17,360 --> 00:10:18,640
by these relevance values.

218
00:10:18,640 --> 00:10:21,440
So these relevance values determine how much each word

219
00:10:21,440 --> 00:10:24,640
is going to influence the word of after the attention.

220
00:10:26,640 --> 00:10:29,440
So this is going to be the key intuition to everything

221
00:10:29,440 --> 00:10:32,240
that we do later on to explain a transformers.

222
00:10:34,640 --> 00:10:36,640
The most important thing to remember about explaining

223
00:10:36,640 --> 00:10:40,640
transformers is we don't have just a single attention matrix.

224
00:10:40,640 --> 00:10:42,640
This mechanism happens H times,

225
00:10:42,640 --> 00:10:44,640
where H is the number of attention heads

226
00:10:44,640 --> 00:10:45,640
that we have.

227
00:10:45,640 --> 00:10:47,640
And intuitively, we can think about it as,

228
00:10:47,640 --> 00:10:49,640
you know, in CNNs, you have kernels.

229
00:10:49,640 --> 00:10:51,640
Each kernel has its own purpose.

230
00:10:51,640 --> 00:10:54,640
Some refer to the background, some refer to the edges,

231
00:10:54,640 --> 00:10:56,640
the shapes, and so on.

232
00:10:56,640 --> 00:10:58,640
Transformers have the same thing with attention heads.

233
00:10:58,640 --> 00:11:00,640
So each attention head can have a different purpose.

234
00:11:00,640 --> 00:11:02,640
And actually, researchers have shown

235
00:11:02,640 --> 00:11:06,640
that you can probably prune most of the attention head

236
00:11:06,640 --> 00:11:09,640
and achieve the same accuracy,

237
00:11:09,640 --> 00:11:12,640
which means that most attention heads are really not important

238
00:11:13,640 --> 00:11:16,640
to the prediction, to a specific prediction of the model.

239
00:11:16,640 --> 00:11:19,640
So it's really important when we think about transformers

240
00:11:19,640 --> 00:11:22,640
and to understand that the different heads have different needs.

241
00:11:25,640 --> 00:11:28,640
The final thing that we need to remember about transformer,

242
00:11:28,640 --> 00:11:31,640
you know, predictions is that transformers use

243
00:11:31,640 --> 00:11:34,640
a classification token for the prediction.

244
00:11:34,640 --> 00:11:36,640
So once the entire attention mechanism is done

245
00:11:36,640 --> 00:11:38,640
and all the tokens are contextualized,

246
00:11:38,640 --> 00:11:41,640
the classification token is the only token.

247
00:11:42,640 --> 00:11:44,640
That is used to make the prediction.

248
00:11:44,640 --> 00:11:46,640
There's a linear layer on top of the classification token

249
00:11:46,640 --> 00:11:48,640
and then the prediction is made.

250
00:11:48,640 --> 00:11:50,640
So basically what the classification token does

251
00:11:50,640 --> 00:11:54,640
is kind of like creates an aggregated representation

252
00:11:54,640 --> 00:11:56,640
of the entire input.

253
00:11:56,640 --> 00:11:58,640
You can think about it as a global representation

254
00:11:58,640 --> 00:12:00,640
of all the tokens in the input.

255
00:12:00,640 --> 00:12:02,640
You have questions so far,

256
00:12:02,640 --> 00:12:05,640
because we're going to move on to the interesting stuff.

257
00:12:05,640 --> 00:12:07,640
Yeah.

258
00:12:07,640 --> 00:12:09,640
So moving on to transformer explainability,

259
00:12:09,640 --> 00:12:12,640
it's really important to set up our goals.

260
00:12:12,640 --> 00:12:15,640
My goal is to facilitate explanations that help you guys,

261
00:12:15,640 --> 00:12:18,640
the researchers that actually use the models.

262
00:12:18,640 --> 00:12:21,640
And the way that we do that is by creating hitmaps.

263
00:12:21,640 --> 00:12:24,640
So the hitmaps should correspond to the pixels,

264
00:12:24,640 --> 00:12:27,640
if we're speaking of images or if we're speaking of text,

265
00:12:27,640 --> 00:12:30,640
and hitmaps should correspond to the tokens.

266
00:12:30,640 --> 00:12:32,640
The hitmaps should correspond to the pixels

267
00:12:32,640 --> 00:12:34,640
that influence the prediction by the model.

268
00:12:34,640 --> 00:12:36,640
So for example, here we see the verb

269
00:12:36,640 --> 00:12:39,640
and the hitmaps actually highlights the pixels

270
00:12:39,640 --> 00:12:41,640
relating to the verb.

271
00:12:41,640 --> 00:12:45,640
And the toothbrush or the ship or the bikes and so on.

272
00:12:45,640 --> 00:12:48,640
So the hitmaps should tell us which pixels in the input

273
00:12:48,640 --> 00:12:53,640
make the prediction as it is.

274
00:12:53,640 --> 00:12:55,640
Okay, we got to the interesting part.

275
00:12:55,640 --> 00:12:57,640
Yeah.

276
00:12:57,640 --> 00:12:59,640
When you talk about transformer explainability,

277
00:12:59,640 --> 00:13:02,640
researchers have looked at this attention matrix

278
00:13:02,640 --> 00:13:04,640
and asked the question,

279
00:13:04,640 --> 00:13:07,640
is this attention matrix an explanation?

280
00:13:07,640 --> 00:13:09,640
How can it be an explanation?

281
00:13:09,640 --> 00:13:12,640
Because we said that the attention values are actually

282
00:13:12,640 --> 00:13:14,640
kind of like relevance values, right?

283
00:13:14,640 --> 00:13:17,640
There are values that reflect how much each token

284
00:13:17,640 --> 00:13:19,640
influences each other's token.

285
00:13:19,640 --> 00:13:22,640
And we also said that the classification token

286
00:13:22,640 --> 00:13:25,640
is the only token that is used for the prediction, right?

287
00:13:25,640 --> 00:13:28,640
So if we look at the row in the attention matrix,

288
00:13:28,640 --> 00:13:30,640
that corresponds to the classification token,

289
00:13:30,640 --> 00:13:32,640
and look at these relevance values,

290
00:13:32,640 --> 00:13:34,640
these should be the relevance values that determine

291
00:13:34,640 --> 00:13:37,640
how much each token influences the classification token,

292
00:13:37,640 --> 00:13:40,640
which is basically how much each token influences

293
00:13:40,640 --> 00:13:42,640
the classification, right?

294
00:13:42,640 --> 00:13:45,640
So maybe these values are just the relevance values.

295
00:13:45,640 --> 00:13:47,640
Each token represents a patch in the image.

296
00:13:47,640 --> 00:13:50,640
Maybe these are just the values that we need.

297
00:13:50,640 --> 00:13:54,640
And we're all done just like decision trees are self-explanable

298
00:13:54,640 --> 00:13:57,640
or linear regression is self-explanable.

299
00:13:57,640 --> 00:13:59,640
What do you think? Are we done?

300
00:13:59,640 --> 00:14:00,640
We're done.

301
00:14:00,640 --> 00:14:02,640
Yeah, we're done.

302
00:14:02,640 --> 00:14:03,640
We're done.

303
00:14:03,640 --> 00:14:04,640
Yeah, we're done.

304
00:14:04,640 --> 00:14:05,640
We're done.

305
00:14:05,640 --> 00:14:06,640
We're done.

306
00:14:06,640 --> 00:14:07,640
We're done.

307
00:14:07,640 --> 00:14:08,640
We're done.

308
00:14:08,640 --> 00:14:09,640
Yeah.

309
00:14:09,640 --> 00:14:10,640
Yeah.

310
00:14:10,640 --> 00:14:12,640
The attention matrix is used to multiply the value representation.

311
00:14:12,640 --> 00:14:13,640
Yeah.

312
00:14:13,640 --> 00:14:15,640
The representation should be positive, negative,

313
00:14:15,640 --> 00:14:16,640
large, small.

314
00:14:16,640 --> 00:14:19,640
It doesn't actually tell us how much it is actually contributing

315
00:14:19,640 --> 00:14:20,640
to the final classification.

316
00:14:20,640 --> 00:14:21,640
Yeah.

317
00:14:21,640 --> 00:14:22,640
I mean,

318
00:14:22,640 --> 00:14:24,640
the two problems that we,

319
00:14:24,640 --> 00:14:26,640
we point out to.

320
00:14:27,640 --> 00:14:30,640
The values can't be negative,

321
00:14:30,640 --> 00:14:33,640
but I don't think really when you say, okay,

322
00:14:33,640 --> 00:14:35,640
let's refer to.

323
00:14:35,640 --> 00:14:39,640
These values are actually directly determining how much

324
00:14:39,640 --> 00:14:41,640
information from each token you're going to take.

325
00:14:41,640 --> 00:14:43,640
And think there's a softmax operation here.

326
00:14:43,640 --> 00:14:46,640
All the values are non-negative, right?

327
00:14:46,640 --> 00:14:49,640
So there is a distribution that's defined on all these tokens

328
00:14:49,640 --> 00:14:51,640
of how much each token is.

329
00:14:52,640 --> 00:14:56,640
So intuitively, these are really relevant values,

330
00:14:56,640 --> 00:15:00,640
but we do have two other issues that we should refer to.

331
00:15:00,640 --> 00:15:03,640
The first one is we said we have a few attention heads, right?

332
00:15:03,640 --> 00:15:06,640
Each tension head has a different meaning.

333
00:15:06,640 --> 00:15:09,640
Some attention heads are really not relevant to the prediction.

334
00:15:09,640 --> 00:15:13,640
How do we aggregate across these attention heads in a way that takes

335
00:15:13,640 --> 00:15:15,640
into account the meaning of each head?

336
00:15:15,640 --> 00:15:18,640
We wouldn't want to take into account heads that do not affect

337
00:15:18,640 --> 00:15:20,640
the final prediction of the model.

338
00:15:20,640 --> 00:15:23,640
And there are such heads since there's research that show that

339
00:15:23,640 --> 00:15:26,640
you can prune most of the heads without impacting the prediction

340
00:15:26,640 --> 00:15:27,640
of the model.

341
00:15:27,640 --> 00:15:29,640
So you have a few attention heads and it isn't,

342
00:15:29,640 --> 00:15:32,640
isn't clear how you aggregate across these attention heads in a

343
00:15:32,640 --> 00:15:35,640
way that takes into account the importance of each head.

344
00:15:35,640 --> 00:15:38,640
And the second question that we have is we refer to the

345
00:15:38,640 --> 00:15:42,640
single attention layer, but we have a few attention layers.

346
00:15:42,640 --> 00:15:45,640
So the first attention layer may incorporate information into

347
00:15:45,640 --> 00:15:47,640
token one from token three.

348
00:15:47,640 --> 00:15:49,640
And then in the second layer,

349
00:15:49,640 --> 00:15:52,640
token one isn't simply the patch that it represented in the

350
00:15:52,640 --> 00:15:53,640
beginning.

351
00:15:53,640 --> 00:15:56,640
It is this patch with information from this patch.

352
00:15:56,640 --> 00:15:57,640
In the second layer,

353
00:15:57,640 --> 00:15:59,640
it's this patch with information from this patch,

354
00:15:59,640 --> 00:16:00,640
and maybe this patch,

355
00:16:00,640 --> 00:16:01,640
and maybe this patch.

356
00:16:01,640 --> 00:16:04,640
And by the end of the attention mechanism,

357
00:16:04,640 --> 00:16:08,640
how do we know which token refers to which input patch, right?

358
00:16:08,640 --> 00:16:09,640
They're all mixed up.

359
00:16:09,640 --> 00:16:12,640
That's the entire idea of the attention.

360
00:16:12,640 --> 00:16:14,640
So we have two issues here.

361
00:16:14,640 --> 00:16:16,640
How do we aggregate across attention heads?

362
00:16:16,640 --> 00:16:19,640
Since we know that they have different means and how do we

363
00:16:19,640 --> 00:16:21,640
aggregate across attention layers?

364
00:16:21,640 --> 00:16:22,640
Yeah.

365
00:16:22,640 --> 00:16:23,640
Just so that I understand.

366
00:16:23,640 --> 00:16:25,640
So if there's only one attention head,

367
00:16:25,640 --> 00:16:27,640
and also there's only one attention layer,

368
00:16:27,640 --> 00:16:29,640
then the relevance board is the attention.

369
00:16:29,640 --> 00:16:30,640
Yeah.

370
00:16:30,640 --> 00:16:31,640
Yeah.

371
00:16:31,640 --> 00:16:32,640
By this hypothesis, yes.

372
00:16:32,640 --> 00:16:33,640
Okay.

373
00:16:33,640 --> 00:16:34,640
Yes.

374
00:16:34,640 --> 00:16:40,640
And then I think there are some models that use this for visual

375
00:16:40,640 --> 00:16:43,640
question answering and actually did that visualization.

376
00:16:43,640 --> 00:16:44,640
And it works.

377
00:16:44,640 --> 00:16:45,640
Pretty well.

378
00:16:45,640 --> 00:16:48,640
So assuming you have one attention head and one attention layer,

379
00:16:48,640 --> 00:16:49,640
it should be.

380
00:16:49,640 --> 00:16:50,640
Fingers crossed.

381
00:16:50,640 --> 00:16:52,640
It should be their elements.

382
00:16:52,640 --> 00:16:54,640
I haven't tried that, but, but yeah.

383
00:16:54,640 --> 00:16:56,640
By this intuition.

384
00:16:56,640 --> 00:17:01,640
So the attention all of mechanism is actually the first method to

385
00:17:01,640 --> 00:17:03,640
explain transfer was that came out.

386
00:17:03,640 --> 00:17:04,640
In 2020.

387
00:17:04,640 --> 00:17:05,640
And they add,

388
00:17:05,640 --> 00:17:08,640
they propose that the two simplest solutions,

389
00:17:08,640 --> 00:17:11,640
maybe that we can think of to solve those three issues.

390
00:17:11,640 --> 00:17:13,640
Head aggregation by averaging.

391
00:17:13,640 --> 00:17:14,640
And again,

392
00:17:14,640 --> 00:17:16,640
remember we said different meanings to different heads.

393
00:17:16,640 --> 00:17:18,640
So that's maybe over simplistic.

394
00:17:18,640 --> 00:17:21,640
And they're aggregation by matrix multiplication.

395
00:17:21,640 --> 00:17:22,640
And if you think about it,

396
00:17:22,640 --> 00:17:25,640
matrix multiplication from the end to the beginning,

397
00:17:25,640 --> 00:17:27,640
kind of unravels things.

398
00:17:27,640 --> 00:17:31,640
The connections between the two that were made by the attention

399
00:17:31,640 --> 00:17:32,640
mechanism.

400
00:17:32,640 --> 00:17:38,640
They also propose another method called attention flow,

401
00:17:38,640 --> 00:17:43,640
which evaluates the flow values in the attention graph,

402
00:17:43,640 --> 00:17:46,640
like a classic flow problem from algorithms,

403
00:17:46,640 --> 00:17:48,640
but it's too computationally expensive for images.

404
00:17:48,640 --> 00:17:50,640
So we're not really going to get into it.

405
00:17:51,640 --> 00:17:54,640
So getting into the first method we propose,

406
00:17:54,640 --> 00:17:57,640
what we were saying is that the assumptions made by the attention

407
00:17:57,640 --> 00:17:59,640
role of mechanism were solid,

408
00:17:59,640 --> 00:18:00,640
but maybe over simplistic.

409
00:18:00,640 --> 00:18:01,640
Yeah.

410
00:18:01,640 --> 00:18:02,640
Question.

411
00:18:02,640 --> 00:18:04,640
You may have some questions.

412
00:18:04,640 --> 00:18:08,640
Oh, yeah.

413
00:18:08,640 --> 00:18:09,640
We say in Hebrew.

414
00:18:09,640 --> 00:18:10,640
Oh,

415
00:18:10,640 --> 00:18:21,640
right.

416
00:18:21,640 --> 00:18:26,640
I may take those at the end of the talk just because otherwise

417
00:18:26,640 --> 00:18:28,640
we won't be able to finish with time.

418
00:18:28,640 --> 00:18:31,640
Yeah.

419
00:18:31,640 --> 00:18:32,640
Yeah.

420
00:18:32,640 --> 00:18:35,640
So getting back to the first time we're going to propose.

421
00:18:35,640 --> 00:18:38,640
We were saying that the assumptions made by attention roll off were

422
00:18:38,640 --> 00:18:40,640
nice and worked in some cases,

423
00:18:40,640 --> 00:18:42,640
but are maybe a bit simplistic.

424
00:18:42,640 --> 00:18:45,640
We want to be able to average across the heads in a way that

425
00:18:45,640 --> 00:18:47,640
actually takes into account the meaning of each other.

426
00:18:47,640 --> 00:18:50,640
So what we're going to do is we're going to use a signal that is

427
00:18:50,640 --> 00:18:52,640
very useful in explainability in general,

428
00:18:52,640 --> 00:18:54,640
which is radiance, right?

429
00:18:54,640 --> 00:18:58,640
Because radiance intuitively mean if I change this a bit,

430
00:18:58,640 --> 00:19:00,640
how does this change a bit, right?

431
00:19:00,640 --> 00:19:04,640
So if we take the gradients with regards to the output of the model,

432
00:19:04,640 --> 00:19:07,640
which is over here, the gradients of the attention matter.

433
00:19:08,640 --> 00:19:12,640
We can use the gradients as weights for the attention maps.

434
00:19:12,640 --> 00:19:15,640
So instead of just averaging across the maps,

435
00:19:15,640 --> 00:19:19,640
we take the gradient, the gradient gives us the weights element.

436
00:19:19,640 --> 00:19:23,640
And we multiply the gradients by the attention and then each head

437
00:19:23,640 --> 00:19:25,640
gets a weight from the gradient.

438
00:19:25,640 --> 00:19:29,640
And then each attention head is not just the simple attention head

439
00:19:29,640 --> 00:19:30,640
that it was in the beginning.

440
00:19:30,640 --> 00:19:33,640
It is the attention weighted by the gradient.

441
00:19:33,640 --> 00:19:36,640
And then we can average across the heads in a way that takes into

442
00:19:36,640 --> 00:19:38,640
account the meaning of each head.

443
00:19:38,640 --> 00:19:40,640
So this is why the gradients are here.

444
00:19:40,640 --> 00:19:44,640
But we have another component that I won't get too deeply into

445
00:19:44,640 --> 00:19:48,640
because it was removed for our second method.

446
00:19:48,640 --> 00:19:52,640
It is the LRP component, layer-wise relevance propagation.

447
00:19:52,640 --> 00:19:56,640
The second thing we thought of was that we actually reduce the

448
00:19:56,640 --> 00:19:59,640
entire transformer architecture to just a multiplication

449
00:19:59,640 --> 00:20:00,640
of queries and keys.

450
00:20:00,640 --> 00:20:03,640
So it's not even the entire attention mechanism because we also

451
00:20:03,640 --> 00:20:05,640
had the values there, remember?

452
00:20:05,640 --> 00:20:08,640
So we narrowed down this entire, not so complex,

453
00:20:08,640 --> 00:20:10,640
but architecture, right?

454
00:20:10,640 --> 00:20:11,640
It has activations.

455
00:20:11,640 --> 00:20:13,640
It has linear projections.

456
00:20:13,640 --> 00:20:16,640
We narrowed all down to the multiplication between queries

457
00:20:16,640 --> 00:20:17,640
and keys.

458
00:20:17,640 --> 00:20:20,640
So we do want to take into account the other layers of the

459
00:20:20,640 --> 00:20:24,640
transformer and how they impact the calculations.

460
00:20:24,640 --> 00:20:28,640
So instead of just taking, let's get back to the whiteboard here,

461
00:20:28,640 --> 00:20:31,640
instead of just taking the attention map,

462
00:20:32,640 --> 00:20:35,640
that is quite, it's simplistic, right?

463
00:20:35,640 --> 00:20:38,640
It's not that, but the multiplication between queries and keys,

464
00:20:38,640 --> 00:20:41,640
we want to take into account a different attention map,

465
00:20:41,640 --> 00:20:46,640
we'll call it RA, which takes into account the other layers

466
00:20:46,640 --> 00:20:48,640
of the transformer architecture.

467
00:20:48,640 --> 00:20:51,640
So instead of taking just these raw relevance values,

468
00:20:51,640 --> 00:20:55,640
we take relevance values calculated by LRP.

469
00:20:55,640 --> 00:20:59,640
And LRP is a mechanism that does back propagation with

470
00:20:59,640 --> 00:21:02,640
gradients from the end of the network all the way to the beginning.

471
00:21:02,640 --> 00:21:06,640
And it can give us relevant values for specifically this attention

472
00:21:06,640 --> 00:21:07,640
matrix.

473
00:21:07,640 --> 00:21:10,640
So instead of taking into account the attention values,

474
00:21:10,640 --> 00:21:13,640
the raw attention values, we take into account the relevance values

475
00:21:13,640 --> 00:21:16,640
of the attention matrix.

476
00:21:16,640 --> 00:21:19,640
And as I said, I won't get too deep into it because we actually

477
00:21:19,640 --> 00:21:21,640
removed it in our second method,

478
00:21:21,640 --> 00:21:25,640
which is the one that I want to get into in more details.

479
00:21:25,640 --> 00:21:31,640
So we have the attention gradients to average across the heads.

480
00:21:31,640 --> 00:21:34,640
And we have the relevance in order to account for all the other layers

481
00:21:34,640 --> 00:21:36,640
in the transformer.

482
00:21:36,640 --> 00:21:38,640
So this is how we average across the heads.

483
00:21:38,640 --> 00:21:41,640
And the way that we average across the layers is by matrix multiplication.

484
00:21:41,640 --> 00:21:46,640
Here we adopted the interpretation from attention robot.

485
00:21:47,640 --> 00:21:56,640
Oh, no, not element wise, actual matrix multiplication.

486
00:21:56,640 --> 00:21:58,640
The matrices are squared matrices.

487
00:21:58,640 --> 00:22:02,640
Yeah, because they are self attention matrices so you can actually

488
00:22:02,640 --> 00:22:03,640
multiply them.

489
00:22:03,640 --> 00:22:06,640
And if you think about it, you can unravel it when multiplying two

490
00:22:06,640 --> 00:22:07,640
attention matrices.

491
00:22:07,640 --> 00:22:11,640
It actually says, if the previous layer gave token one information

492
00:22:11,640 --> 00:22:14,640
from token three, and this layer gives token one information from

493
00:22:14,640 --> 00:22:19,640
token four, then it unravels both operations to ensure that you

494
00:22:19,640 --> 00:22:25,640
actually take into account all the context.

495
00:22:25,640 --> 00:22:29,640
Yeah, so this is just a rewind of what we saw in the previous slide.

496
00:22:29,640 --> 00:22:31,640
How do we average across heads?

497
00:22:31,640 --> 00:22:33,640
We take the gradients as weights.

498
00:22:33,640 --> 00:22:35,640
We take the relevance instead of the pure attention weights.

499
00:22:35,640 --> 00:22:37,640
And then we do averaging.

500
00:22:37,640 --> 00:22:39,640
But here the average is not just the raw average.

501
00:22:39,640 --> 00:22:44,640
If we had before it is weighted by the gradients.

502
00:22:44,640 --> 00:22:50,640
And here you can see a few examples of how our method works.

503
00:22:50,640 --> 00:22:53,640
So by the way, this is a slide that was added, but we don't have the

504
00:22:53,640 --> 00:22:54,640
updated slide.

505
00:22:54,640 --> 00:22:58,640
So let's just see what we get in the end of this calculation.

506
00:22:58,640 --> 00:23:04,640
So

507
00:23:04,640 --> 00:23:09,640
at the end of this calculation, we had an attention matrix, which

508
00:23:09,640 --> 00:23:12,640
is the attention matrix after by the averaging and everything.

509
00:23:12,640 --> 00:23:17,640
We have attention matrices for all the layers.

510
00:23:17,640 --> 00:23:25,640
And then we multiply these.

511
00:23:25,640 --> 00:23:31,640
So really, we have one attention matrix that takes into account all

512
00:23:31,640 --> 00:23:34,640
the layers and all the heads.

513
00:23:34,640 --> 00:23:39,640
And now we can get back to, can we go back in the slides?

514
00:23:39,640 --> 00:23:42,640
Oh, no, it's only going forward.

515
00:23:42,640 --> 00:23:43,640
Yeah.

516
00:23:43,640 --> 00:23:46,640
Maybe there's an arrow at the bottom left corner.

517
00:23:46,640 --> 00:23:48,640
If you move your mouse.

518
00:23:48,640 --> 00:23:51,640
Oh, the back arrow.

519
00:23:51,640 --> 00:23:55,640
Oh, it's the other way around.

520
00:23:55,640 --> 00:23:58,640
Yeah.

521
00:23:58,640 --> 00:24:01,640
And now we're actually getting to the point that Sharon made that right

522
00:24:01,640 --> 00:24:06,640
now we only after all the aggregations that we made, we have one aggregated

523
00:24:06,640 --> 00:24:10,640
attention matrix for the entire network because we aggregate across heads

524
00:24:10,640 --> 00:24:12,640
and then we aggregate across layers.

525
00:24:12,640 --> 00:24:16,640
And once we have that one attention matrix for the entire, for the entire

526
00:24:16,640 --> 00:24:21,640
network, then we can use that intuition that we had that the row corresponding

527
00:24:21,640 --> 00:24:24,640
to the classification token is actually the explanation.

528
00:24:24,640 --> 00:24:26,640
So this is how we extract the final explanation.

529
00:24:26,640 --> 00:24:30,640
And then we're actually the relevance values that we use.

530
00:24:30,640 --> 00:24:35,640
And the one the other way around, right.

531
00:24:35,640 --> 00:24:38,640
Okay.

532
00:24:38,640 --> 00:24:42,640
So as you can see here, we have comparisons between our method and other methods

533
00:24:42,640 --> 00:24:47,640
that are either adapted from CNNs, or methods that were constructed for

534
00:24:47,640 --> 00:24:49,640
transformers such as rollout.

535
00:24:49,640 --> 00:24:53,640
So as you can see here, rollout tends to have a lot of noise in the background

536
00:24:53,640 --> 00:24:57,640
and we think about it intuitively as resulting from the fact that they just

537
00:24:57,640 --> 00:25:01,640
average across the heads and not take into account the meaning of each head.

538
00:25:01,640 --> 00:25:06,640
And some methods such as partial LRP fail on some cases, but in these

539
00:25:06,640 --> 00:25:09,640
specific cases, they actually do pretty well.

540
00:25:09,640 --> 00:25:13,640
But I do want to point out that they do not distinct between classes.

541
00:25:13,640 --> 00:25:18,640
So for example, if we have an elephant and a zebra in an image, our method is able

542
00:25:18,640 --> 00:25:22,640
to produce explanations specifically for the elephant or specifically for the zebra.

543
00:25:22,640 --> 00:25:27,640
And when we don't do that, and we want to use this method say partial LRP to

544
00:25:27,640 --> 00:25:32,640
explain predictions by the model, it will be hard to do that because if you want

545
00:25:32,640 --> 00:25:37,640
to explain the elephant prediction, we may have results coming in from other classes.

546
00:25:37,640 --> 00:25:41,640
So we're not really sure if the things that we're seeing highlighted are highlighted

547
00:25:41,640 --> 00:25:46,640
because of the elephant or because of other classes making their way into the

548
00:25:46,640 --> 00:25:47,640
explanation.

549
00:25:47,640 --> 00:25:51,640
So I think personally, class specific explanations are really important to ensure

550
00:25:51,640 --> 00:25:54,640
that we're really explaining the specific prediction of the model.

551
00:25:54,640 --> 00:25:55,640
We're good.

552
00:26:04,640 --> 00:26:05,640
That's a fantastic question.

553
00:26:05,640 --> 00:26:07,640
That's a fantastic question.

554
00:26:07,640 --> 00:26:11,640
Usually people from explainability evaluate explanations differently than what you

555
00:26:11,640 --> 00:26:13,640
as end users may have to be.

556
00:26:13,640 --> 00:26:17,640
So what we do is we use erasure based methods.

557
00:26:17,640 --> 00:26:21,640
So what we do is we take the pixels that are said to be important to buy with it.

558
00:26:21,640 --> 00:26:25,640
We take them out and we see if the model changes its prediction or not.

559
00:26:25,640 --> 00:26:27,640
And similarly, we do the other way around.

560
00:26:27,640 --> 00:26:31,640
We take the pixels that are unimportant by benefit and take them out and see that

561
00:26:31,640 --> 00:26:33,640
the model still predicts the same.

562
00:26:33,640 --> 00:26:37,640
You have to take into account when you can see that the model still predicts the

563
00:26:37,640 --> 00:26:38,640
same.

564
00:26:38,640 --> 00:26:42,640
You have to take into account when you do that, that you create images that are

565
00:26:42,640 --> 00:26:45,640
out of the distribution of the model was trained on.

566
00:26:45,640 --> 00:26:48,640
So this method is not really, you know, airtight.

567
00:26:48,640 --> 00:26:51,640
And there's a lot of research around how do we value it.

568
00:26:51,640 --> 00:26:55,640
And how do we know if the explanation is really good or not.

569
00:26:55,640 --> 00:26:57,640
Any other questions.

570
00:26:57,640 --> 00:26:58,640
Yeah.

571
00:26:59,640 --> 00:27:10,640
Yeah.

572
00:27:10,640 --> 00:27:11,640
Yeah.

573
00:27:11,640 --> 00:27:12,640
Yeah.

574
00:27:12,640 --> 00:27:13,640
Yeah.

575
00:27:13,640 --> 00:27:14,640
Yeah.

576
00:27:14,640 --> 00:27:15,640
Yeah.

577
00:27:15,640 --> 00:27:16,640
Yeah.

578
00:27:16,640 --> 00:27:20,640
Um, just because we want to have a measuring stick that actually measures the

579
00:27:20,640 --> 00:27:24,640
explainability without relation to the algorithm itself.

580
00:27:24,640 --> 00:27:28,640
So the measure should be unrelated to whether it's a transformer or CNN.

581
00:27:28,640 --> 00:27:32,640
It should be unified throughout all the different architectures, right?

582
00:27:32,640 --> 00:27:36,640
Just as you use accuracy to measure CNNs or transformers or whatever

583
00:27:36,640 --> 00:27:37,640
architecture you use.

584
00:27:37,640 --> 00:27:41,640
You want to have a measuring stick that really measures the method and not

585
00:27:41,640 --> 00:27:44,640
something that has something to do with specifically.

586
00:27:45,640 --> 00:27:48,640
If you have an explanation for CNN, it also has, you know,

587
00:27:48,640 --> 00:27:50,640
values for each pixel.

588
00:27:50,640 --> 00:27:51,640
Yeah.

589
00:27:51,640 --> 00:27:52,640
Yeah.

590
00:27:52,640 --> 00:27:53,640
Just zero it out.

591
00:27:53,640 --> 00:27:54,640
You just zero it out.

592
00:27:54,640 --> 00:27:56,640
And it works on the input itself.

593
00:27:56,640 --> 00:27:57,640
So it really,

594
00:27:57,640 --> 00:27:59,640
it is undependent even of.

595
00:27:59,640 --> 00:28:00,640
You know,

596
00:28:00,640 --> 00:28:01,640
you know,

597
00:28:01,640 --> 00:28:02,640
you know,

598
00:28:02,640 --> 00:28:03,640
you know,

599
00:28:03,640 --> 00:28:04,640
you know,

600
00:28:04,640 --> 00:28:05,640
you know,

601
00:28:05,640 --> 00:28:06,640
you know,

602
00:28:06,640 --> 00:28:07,640
you know,

603
00:28:07,640 --> 00:28:08,640
you know,

604
00:28:08,640 --> 00:28:09,640
you know,

605
00:28:09,640 --> 00:28:10,640
you know,

606
00:28:10,640 --> 00:28:11,640
you know,

607
00:28:11,640 --> 00:28:12,640
you know,

608
00:28:12,640 --> 00:28:15,640
it really it is independent even of the method you use or the model.

609
00:28:15,640 --> 00:28:37,640
It is a measuring stick that has nothing to do with which method you use

610
00:28:37,640 --> 00:28:40,640
for expansion or expansion in which version.

611
00:28:40,640 --> 00:28:49,840
the sun. I'm not sure I got your question exactly, but I would say that there are methods evaluating

612
00:28:49,840 --> 00:28:55,440
explanations by adding sparse correlation, making sure that the model reaches 100 percent accuracy

613
00:28:55,440 --> 00:29:00,400
due to the sparse correlations, and then making sure that the explanation outputs these sparse

614
00:29:00,400 --> 00:29:06,240
correlations versus the odd correlation. So there are methods that do that, but yeah. But we usually,

615
00:29:06,240 --> 00:29:11,280
I usually use erasers as methods to evaluate explanations, but this is a really active

616
00:29:11,280 --> 00:29:16,640
goal of research, right? So it's not really, you know, obvious how we evaluate explanations

617
00:29:16,640 --> 00:29:24,880
and what's the right way to do that. I think I'm maybe moving backwards instead of forward.

618
00:29:26,000 --> 00:29:32,160
Some technical issues. Yeah, okay. I may just skip this because we do have the motivation

619
00:29:32,160 --> 00:29:37,600
that we did in the beginning and we're a bit behind on time. So our second method

620
00:29:38,560 --> 00:29:44,080
said, you know what? We really believe that multimodal models are going to be a big thing.

621
00:29:44,960 --> 00:29:49,520
And we only explained self-attention before, as you saw. We didn't go into

622
00:29:49,520 --> 00:29:56,320
cross-attention or encoded encoded attention. And assuming that most transformers don't just do

623
00:29:56,320 --> 00:29:59,520
right self-attention, we need a mechanism that can explain

624
00:30:00,160 --> 00:30:04,080
cross-attention and encoded encoded attention as well, not just self-attention.

625
00:30:04,080 --> 00:30:09,600
So the second paper actually expands the first paper, but for other types of attention.

626
00:30:12,080 --> 00:30:16,560
So the first thing we do is get rid of the LLP, and that's why I don't, you know, get into a lot

627
00:30:16,560 --> 00:30:21,360
of detail with regards to the LLP. The reason that we did that is because if you think about it,

628
00:30:21,360 --> 00:30:25,920
we use LLP in order to account for all the layers, but really gradients account for all the layers

629
00:30:25,920 --> 00:30:30,080
because backpropagation is backpropagated from the output all the way back together.

630
00:30:30,800 --> 00:30:35,360
So we said, what happens if we remove LLP, which makes it easier for you guys to implement

631
00:30:35,360 --> 00:30:41,200
the algorithm, and it makes it faster and more convenient, and it actually works pretty well.

632
00:30:41,200 --> 00:30:46,640
So we remove the LLP component. I will say that if you want really accurate explanations,

633
00:30:46,640 --> 00:30:53,200
usually I would go for the LLP version, right? Because LLP adds this added component that doesn't

634
00:30:53,200 --> 00:30:57,680
exist without LLP. It does account for all the layers quite systematically.

635
00:30:59,120 --> 00:31:04,480
So when we talk about cross-model interactions, we have four types of interactions in such models.

636
00:31:04,480 --> 00:31:09,120
We have the self-attention interactions between the text tokens, how the text tokens influence

637
00:31:09,120 --> 00:31:16,160
themselves, the self-attention interactions between the image tokens, and then two types of cross-attention

638
00:31:16,160 --> 00:31:20,160
interactions, how text influences image and how image influences.

639
00:31:23,920 --> 00:31:30,160
And then what we thought we would do is really track the self-attention layers.

640
00:31:30,160 --> 00:31:35,760
So each self-attention layer mixes tokens. Okay, we'll mix the tokens in the relevance matrices.

641
00:31:36,400 --> 00:31:42,000
So we start with an initialization of the relevance matrices at the beginning of the

642
00:31:42,000 --> 00:31:47,360
modalities or self-contained. So images only affect images and text only affects text,

643
00:31:47,360 --> 00:31:52,320
and each image token only influences itself. So the initialization for the self-attention

644
00:31:52,320 --> 00:31:58,240
relations are just the identity matrices. And for the cross-model relations, it's a zero matrix,

645
00:31:58,240 --> 00:32:03,440
because there are no cross-model interactions before we do any attention. And what the method

646
00:32:03,440 --> 00:32:09,040
really does is it just goes on a forward pass through the attention layers, and as the attention

647
00:32:09,040 --> 00:32:15,840
layers mix the tokens, the relevance values are mixed as well, just tracking the attention as it goes.

648
00:32:17,600 --> 00:32:22,880
So I won't get into all the rules, all the rules that we have for all these specific attention

649
00:32:22,880 --> 00:32:26,640
layers. I'm just giving you a motivation of how it works. And really, believe me, it's really

650
00:32:26,640 --> 00:32:33,040
simple, even though the equations look complicated. So let's go over just the self-attention rule.

651
00:32:33,760 --> 00:32:40,720
A self-attention layer has, again, multiple heads. We average across the heads using gradients just as

652
00:32:40,720 --> 00:32:48,320
before. So we have now a single attention matrix marked here as a bar. And what we do again is just

653
00:32:48,320 --> 00:32:55,360
matrix multiplication between the current attention mixture and the old attention mixture that existed

654
00:32:55,360 --> 00:33:02,320
in the relevance matrix. So matrix multiplication and update the relevance matrix. This is all we

655
00:33:02,320 --> 00:33:06,960
do. We just track the attention as it goes. As it mixes between tokens, we mix between the

656
00:33:06,960 --> 00:33:12,160
relevance values. That's what we do. That's the entire algorithm. And head aggregation is done

657
00:33:12,800 --> 00:33:20,480
via gradients as before. So taking a look at some examples that we have to demonstrate how this works.

658
00:33:21,040 --> 00:33:26,960
For example, for CLIP, you can see that we've entered different texts with the same input image

659
00:33:26,960 --> 00:33:32,880
and propagated gradients. And by the way, for CLIP, gradients are propagated. Let's take them back

660
00:33:32,880 --> 00:33:39,440
to the whiteboard. For CLIP, because I know this is specifically interesting to you,

661
00:33:40,480 --> 00:33:46,960
let's talk about how we propagate relevance for CLIP. For CLIP, you have an imaging quarter

662
00:33:48,640 --> 00:33:53,120
and then a texting quarter. Both of them, by the way, use pure self-attention. So there's no cross

663
00:33:53,200 --> 00:34:01,440
connection. This and this output representation and vector, which is by the way from the

664
00:34:01,440 --> 00:34:06,720
classification. So this is the vector for the text and this is the vector for the image.

665
00:34:07,600 --> 00:34:16,640
And the stimuli score is just a dot product. Both scores. So what we do is we propagate

666
00:34:16,640 --> 00:34:23,360
gradients from this dot product back to the texting quarter

667
00:34:25,280 --> 00:34:29,360
and back to the imaging quarter. And those gradients are going to be used to average

668
00:34:29,360 --> 00:34:33,520
across the attention pens as we saw before. And then the attention heads are going to be

669
00:34:33,520 --> 00:34:39,280
aggregated across different letters by matrix multiplication. So here we don't have an output

670
00:34:39,280 --> 00:34:45,760
logic as we have for the classification, but we use this dot product between their presentations

671
00:34:45,760 --> 00:34:51,520
to calculate the score that we propagate the gradients with regards to. So all that we do

672
00:34:51,520 --> 00:34:56,560
here is really simple. Calculate the dot product between their presentations, propagate gradients

673
00:34:56,560 --> 00:35:01,920
with regards to the dot product. Those gradients are going to be used as weights for the attention

674
00:35:01,920 --> 00:35:10,080
matrices to average across them. And as you can see, the results are text specific since we

675
00:35:10,080 --> 00:35:14,480
propagated the gradients with regards to the specific multiplication between the specific text

676
00:35:14,480 --> 00:35:18,880
and the specific image. So actually for an elephant, you can see that the hidden map

677
00:35:18,880 --> 00:35:22,240
corresponds to the elephant. For a zebra, the hidden map corresponds to the zebra. And for a

678
00:35:22,240 --> 00:35:25,680
leg, the hidden map corresponds to the leg, showing us that the model really knows how to

679
00:35:25,680 --> 00:35:30,240
distinct between different parts or different objects in the image according to the text input

680
00:35:30,240 --> 00:35:38,240
that we give it. This is an example that we saw before. And visual question answering in case

681
00:35:38,240 --> 00:35:43,360
any one of you is interested is actually an interesting use case. Because for visual question

682
00:35:43,360 --> 00:35:47,440
answering, the model is given an image and a question, and it's supposed to answer the question

683
00:35:47,440 --> 00:35:52,400
based on the image. And researchers have shown that when you actually lack out the entire image

684
00:35:52,400 --> 00:35:59,440
and just give the model the question, it answers the question about 30% of design correctly.

685
00:36:00,080 --> 00:36:04,720
So the question here is assuming that the model answers the question without seeing the image.

686
00:36:06,400 --> 00:36:11,520
How do we measure the accuracy of such models? So you can use explainability to ensure that

687
00:36:11,520 --> 00:36:16,080
the model actually used the image and the correct parts of the image to make the prediction.

688
00:36:16,080 --> 00:36:21,520
For example here, the question is, did he catch a ball? We see that the player actually caught the

689
00:36:21,520 --> 00:36:27,280
ball. And the answer is yes, but we also see that the model focused on the right parts of the image.

690
00:36:27,280 --> 00:36:31,040
So it can really tell that the model made the prediction based on the image and not just the

691
00:36:31,040 --> 00:36:41,280
question. I'm going to skip this part too. Yay. So we're switching gears. We're going to talk about

692
00:36:41,600 --> 00:36:45,920
our method to improve model robustness using explainability. So if you have any questions

693
00:36:45,920 --> 00:36:51,120
about the previous part on explaining transformers, this is the time to ask them.

694
00:36:52,960 --> 00:36:56,960
No, no questions. Oh, I had a couple of questions in the chat. Yeah.

695
00:36:58,800 --> 00:37:03,600
I'm sorry about that. There is no one really, you know, maintaining the chat.

696
00:37:04,400 --> 00:37:08,640
Yeah, let's make it brief and then try to answer questions. Yeah.

697
00:37:09,520 --> 00:37:13,280
Oh, okay. I was just wondering, why does it make sense to only look at the

698
00:37:15,760 --> 00:37:21,120
attention maps outputted by the softmax? Because don't we have, don't we multiply by an output

699
00:37:21,120 --> 00:37:29,120
matrix then that is able to shuffle across tokens afterwards? Do you mean the values matrix? No,

700
00:37:29,120 --> 00:37:37,520
the output matrix. I guess that the intuition is just that the self attention mechanism,

701
00:37:37,520 --> 00:37:43,600
its purpose is to contextualize in the way that the contextualization is made by the attention

702
00:37:43,600 --> 00:37:49,840
values. So the attention value is actually, you know, determine how much each token is going

703
00:37:49,840 --> 00:37:57,040
to be incorporated into the other tokens. We do have an additional output matrix and you mean

704
00:37:57,040 --> 00:38:02,480
after the attention mechanism, right? Yes, yes. Yeah, okay. So some researchers have actually

705
00:38:02,480 --> 00:38:08,080
used that output, if I'm not mistaken, it was that output, the norm of the output matrix in order

706
00:38:08,080 --> 00:38:16,000
to average across the different heads to account for each head's meaning in the attention matrix,

707
00:38:16,560 --> 00:38:23,440
in the attention mechanism. But, you know, just, you know, very naively thinking the attention

708
00:38:24,640 --> 00:38:29,360
really mixes the tokens using the values determined by the attention matrix. So it's

709
00:38:29,360 --> 00:38:35,760
really a naive intuitive outlook on the attention mechanism. And the output matrix that you're

710
00:38:35,760 --> 00:38:43,360
referring to is I view it as a weights matrix, which will weight each layer since not all layers

711
00:38:43,360 --> 00:38:47,680
influence the prediction the same, right? We know that usually the last attention layer

712
00:38:47,680 --> 00:38:51,040
is the most influential or the previous attention layers are not that impactful.

713
00:38:51,920 --> 00:38:58,720
So I view it as the output matrix kind of reweighting the result from the attention mechanism.

714
00:38:59,360 --> 00:39:04,640
But all that we're saying right now are just intuitions, right? We've seen empirically that the

715
00:39:04,640 --> 00:39:09,840
attention matrix is quite indicative of what the model learns to do, how it learns to contextualize

716
00:39:10,720 --> 00:39:15,120
parts of the input. It's not necessarily the best thing to do, the smartest thing to do or the

717
00:39:15,120 --> 00:39:20,880
most correct thing to do. It's just what empirically worked well. And it has an intuition basis as

718
00:39:20,880 --> 00:39:27,120
explained before. I hope that answers your question. It does. I had one other question if there's time.

719
00:39:29,920 --> 00:39:35,360
We're really tight on time. I mean, we have 13 minutes. So maybe we'll take that offline.

720
00:39:36,160 --> 00:39:39,440
Sure. Thank you. Sorry for that. I really apologize.

721
00:39:42,240 --> 00:39:49,360
Okay. So when we talk about VIT models, the image is split into patches. The patches go

722
00:39:49,360 --> 00:39:54,240
through linear projections. And then a transformer encoder and vanilla transformer encoder is used

723
00:39:54,240 --> 00:39:58,960
to make the prediction again with the classification code. So really a simple and clean architecture.

724
00:40:00,000 --> 00:40:07,600
And usually those models are trained using ImageNet. ImageNet is a classification data set.

725
00:40:08,320 --> 00:40:11,680
And what those classification data sets do is actually they train the model

726
00:40:11,680 --> 00:40:16,560
to make a prediction. So they train the model to see an image and make the prediction that this

727
00:40:16,560 --> 00:40:21,680
is a car. But it doesn't do anything beyond that, right? The model should predict that it's a car,

728
00:40:21,680 --> 00:40:26,880
but it doesn't have to have an understanding of what a car constructs and how a car looks.

729
00:40:27,600 --> 00:40:33,520
It should just see this image of a car and output car. We don't enforce anything too

730
00:40:33,520 --> 00:40:39,680
smart that the model should learn. So what researchers have noticed a long time ago

731
00:40:39,680 --> 00:40:44,480
is that ImageNet contains sparse predictions. What it means is that, for example,

732
00:40:44,480 --> 00:40:51,600
cows usually appear on the background of green grass. So a reasonable inference that the model

733
00:40:51,600 --> 00:40:56,240
can make. Really reasonable, right? Because this is the statistics of the data in the data set that

734
00:40:56,240 --> 00:41:02,080
it gets. It's to learn that green grass is actually a cow. And now we learn to predict

735
00:41:02,080 --> 00:41:07,760
that this image is an image of a cow based on the green grass, not really the object in the image.

736
00:41:10,000 --> 00:41:23,360
What it causes is, oh, can you mute this? Thank you. So what it causes is cases where

737
00:41:23,360 --> 00:41:28,480
the distribution is likely shifted from ImageNet. And in cases where we would actually expect the

738
00:41:28,480 --> 00:41:34,080
model to really work well on. The model really doesn't. And the accuracy plunge, we're talking

739
00:41:34,080 --> 00:41:41,040
about 90% to 30% sometimes and even less. So really cases where we would expect the model to still

740
00:41:41,040 --> 00:41:48,560
learn to make smart and great prediction, but it really does. It predicts based on the sparse

741
00:41:48,560 --> 00:41:52,720
predictions that it learned from ImageNet and they don't apply to other predictions. So for example,

742
00:41:52,720 --> 00:41:57,200
we have the golf ball in the lemon here and we have another orange that is classified as a maze

743
00:41:57,200 --> 00:42:02,560
due to the carpet in the bathroom, right? Because it kind of looks like a maze. And a school bus

744
00:42:02,560 --> 00:42:07,680
here that is classified as a snowplow because of the presence of snow. So we can imagine that the

745
00:42:07,680 --> 00:42:12,720
model learns some kind of sparse correlation here, such as vehicle plus snow equals snowplow.

746
00:42:13,600 --> 00:42:23,120
Okay. So we want to solve these issues, but without training the models with, you know,

747
00:42:23,120 --> 00:42:28,320
a stronger queue, it is really hard to do that because we just teach the model based on some

748
00:42:28,320 --> 00:42:33,600
data set that we have, which is ImageNet. It is, you know, the most used data set to predict,

749
00:42:34,160 --> 00:42:42,480
to train object detection, object recognition. And we have no way of really controlling

750
00:42:42,480 --> 00:42:48,400
what the model learns. And intuitively, training the explainability signal is really teaching

751
00:42:48,400 --> 00:42:53,280
the model not just what is in the image, but why this is the object in the image. So we would want

752
00:42:53,280 --> 00:42:59,680
to apply a last term directly to the explanations of the model to teach it why this prediction is

753
00:42:59,680 --> 00:43:07,520
correct. So here you see some sparse correlations that the model uses. So for example, here the

754
00:43:07,520 --> 00:43:13,760
model classifies the images of chestnut with a confidence of 100% based on just the background

755
00:43:13,760 --> 00:43:19,760
pixels, not even one photo. And here a very sparse consideration of the zebra gives us a

756
00:43:19,760 --> 00:43:25,040
confidence of 99.9% that this is a zebra. So really behavior that we would really want to discourage.

757
00:43:28,080 --> 00:43:33,760
Since the second method that we saw is based on pure gradients, everything there is derivable.

758
00:43:33,760 --> 00:43:37,280
The gradients can be derived again, and the last term can be applied directly to the

759
00:43:37,280 --> 00:43:43,200
explainability. And we can force the model to make the prediction based on the program instead of

760
00:43:43,200 --> 00:43:50,960
the background image pixels. The issue that we had after that is, you know, we're researchers at

761
00:43:50,960 --> 00:43:58,640
the university, right? We don't have the resources to train VIT large or huge from scratch. So we

762
00:43:58,640 --> 00:44:04,960
need to come up with a method that is efficient in time and space and not too complicated. So what

763
00:44:04,960 --> 00:44:10,880
we opted to do is fine tune an existing model. So we would fine tune the model. It works pretty

764
00:44:10,880 --> 00:44:15,760
well on ImageNet, right? We don't want to change the prediction that it gives on ImageNet. We just

765
00:44:15,760 --> 00:44:21,920
want to change the reasoning that it gives to the prediction. So we fine tune the models with only

766
00:44:21,920 --> 00:44:27,920
three examples per class, really not that many examples for just 500 classes. So just half the

767
00:44:27,920 --> 00:44:33,200
classes in ImageNet to change the relevance maps to focus on the foreground instead of the background.

768
00:44:35,600 --> 00:44:40,960
So we identified two science issues with VIT models. The first one is an over interpretation

769
00:44:40,960 --> 00:44:45,200
of the background, which we saw on your clients. And the second one is a sparse consideration of

770
00:44:45,200 --> 00:44:53,040
the program. The first idea was to fine tune the explanation maps to just be segmentation maps,

771
00:44:53,120 --> 00:45:00,080
like this. This is actually an example of me fine tuning a VIT based model to make the relevance

772
00:45:00,080 --> 00:45:06,720
maps resemble or be identical to segmentation maps. So as you can see before the explanations weren't

773
00:45:06,720 --> 00:45:12,960
really segmentation maps and after they're quite well segmented in the image. So can anyone guess

774
00:45:12,960 --> 00:45:18,480
why that's not an optimal solution to the problem that we have just creating segmentation maps?

775
00:45:19,360 --> 00:45:26,640
People from Zoom can guess too. Why wouldn't we want the model to output relevance maps that are

776
00:45:26,640 --> 00:45:34,560
actually segmentation maps? Let's have a thought experiment today. I'm going to draw with my

777
00:45:34,560 --> 00:45:41,120
magnificent drawing skills and objects and you're going to try to identify which animal this is,

778
00:45:41,120 --> 00:45:49,280
right? Again, I'm not the best drawing. Which animal is this? Which snake?

779
00:45:51,200 --> 00:46:00,480
Nail. Oh, no, this snake. Which snake is this? Cobra. Yeah, why cobra? Because of the head, right?

780
00:46:01,200 --> 00:46:06,880
And humans, we don't classify cobra as a cobra because of its tail, right? We look at the head

781
00:46:06,880 --> 00:46:14,400
pixels or the head featured and determine that this is a cobra. So we don't, as humans, give

782
00:46:14,400 --> 00:46:19,840
identical relevance to all the pixels in the image. What we do here when we fine tune the

783
00:46:19,840 --> 00:46:24,560
experiment maps to be segmentation maps, we force the model to look equally at all the pixels of

784
00:46:24,560 --> 00:46:30,480
the cobra. We do want to give the model the opportunity to give some relevance to pixels

785
00:46:30,480 --> 00:46:39,360
that is higher than other pixels. So this is too harsh. And we need to have a refined version of it.

786
00:46:41,200 --> 00:46:47,200
This is why we split the loss into two different losses. One is a background loss and one is

787
00:46:47,200 --> 00:46:53,200
foreground loss. The background loss is a mean squared error loss, encouraging the relevance

788
00:46:53,200 --> 00:46:57,360
on the background to be close to zero. And we're using segmentation maps here,

789
00:46:57,360 --> 00:47:03,120
S is the segmentation map of the image. And the foreground loss encourages the foreground

790
00:47:03,120 --> 00:47:10,240
of the image to be closer to one. By splitting into two loss terms, we can give different

791
00:47:10,240 --> 00:47:15,520
values or different coefficients to each of the loss terms. So the background loss is going to get

792
00:47:15,520 --> 00:47:20,320
a relatively high coefficient too, because we don't want a lot of relevance on the background.

793
00:47:20,320 --> 00:47:25,360
By the way, we're not striving to completely eliminate the background, the relevance on the

794
00:47:25,360 --> 00:47:29,440
foreground. Just make sure that the relevance of the background is lower than the relevance of the

795
00:47:29,440 --> 00:47:35,200
foreground. And the foreground loss is going to get a relatively low coefficient. We would want to

796
00:47:35,200 --> 00:47:40,720
encourage the model to look more at more pixels of the foreground, but we wouldn't want to make

797
00:47:40,720 --> 00:47:43,520
the model look at all the pixels in the foreground equally.

798
00:47:46,240 --> 00:47:52,080
We do also have a classification loss, which ensures that the new prediction by the model

799
00:47:52,160 --> 00:47:56,960
or the new distribution is similar to the all distribution by the model. Just to make sure

800
00:47:56,960 --> 00:48:02,240
that the model doesn't forget how to classify images. And again, the model does a pretty good

801
00:48:02,240 --> 00:48:06,080
job on ImageNet. So we don't want to change the prediction by the model. We just want to change

802
00:48:06,080 --> 00:48:16,720
the reasoning. So the giant tables of results here are comparisons between the accuracy of the

803
00:48:16,720 --> 00:48:22,880
model before and after a cartooning process. And as you can see here, it's quite tiny, but I hope

804
00:48:22,880 --> 00:48:28,640
you can see it still. For the ImageNet validation set, we're experiencing a bit of a decrease in

805
00:48:28,640 --> 00:48:32,720
performance. This is because the model relied on spurious cues, and now we're taking them away

806
00:48:32,720 --> 00:48:38,080
from it. And so the spurious cues that previously helped the model reach very, very high accuracy

807
00:48:38,080 --> 00:48:44,800
and overfit are now taking away. But the decrease in accuracy on average across seven models is

808
00:48:44,800 --> 00:48:50,720
not that big. I mean, it's less than 1%. And when you take into account other shifted distributions,

809
00:48:50,720 --> 00:49:00,000
such as ImageNet A, ImageNet R, Sketch, ImageNet ObjectNet, and SIScores, you can see that there

810
00:49:00,000 --> 00:49:05,840
is a pretty big or significant increase in accuracy. For ImageNet A, for example, plus

811
00:49:06,640 --> 00:49:14,320
5.8% in top one accuracy plus 7.8% in top five accuracy. So really a slight decrease

812
00:49:14,320 --> 00:49:18,320
in the accuracy on the data set that the model was originally trained on and

813
00:49:18,320 --> 00:49:23,040
a significant increase in accuracy for distribution shifts, as we would expect.

814
00:49:23,680 --> 00:49:26,640
So to train it, you have to know the program. What is the program?

815
00:49:27,440 --> 00:49:33,520
Yeah, you have to know that. You have segmentation maps. You have segmentation maps. And we do

816
00:49:33,520 --> 00:49:39,680
experiment with two types of segmentation maps. One is manually human, manually tagged by humans.

817
00:49:39,680 --> 00:49:45,680
And the second one is by token cut, which is a version that uses dyno. This is in case you're

818
00:49:45,680 --> 00:49:53,040
training with non-ImageNet data sets and you don't want to manually tag. Even if you do manually

819
00:49:53,040 --> 00:49:59,280
tag, I mean, we use three examples for half the classes. So it's not that many examples to tell,

820
00:49:59,280 --> 00:50:04,160
but we do provide for an option for ad supervised segmentation. Yeah.

821
00:50:04,320 --> 00:50:10,320
So I think that's really cool. The one thing about this, why not just do segmentation?

822
00:50:11,200 --> 00:50:14,640
You can just train a segmentation system. Is that kind of naturally explainable?

823
00:50:16,640 --> 00:50:21,040
That's an excellent question. Do models that were trained on segmentation

824
00:50:22,160 --> 00:50:27,200
have that, you know, brief pass on Spurs correlation? Do they get that in her?

825
00:50:28,160 --> 00:50:34,000
What we thought about or I thought about in that context is you can think about a model

826
00:50:34,000 --> 00:50:39,920
that learns to classify using Spurs correlation and then identify the object using edge detection.

827
00:50:40,880 --> 00:50:46,960
So just because you learn to identify an object does not mean or learn to segment an object.

828
00:50:46,960 --> 00:50:52,320
Does not mean that you learn to recognize the object by the segmentation. And also we can think

829
00:50:52,320 --> 00:50:57,200
about when you want to train really big models, you need a lot of data to do that. And segmentation

830
00:50:57,200 --> 00:51:02,640
data is quite expensive. You usually don't have that amount of data as you do for classification,

831
00:51:02,640 --> 00:51:08,560
which is an easier task. You have a lot of data just lying around there. So classification is

832
00:51:08,560 --> 00:51:20,160
usually the go to task. Yeah, but only just a few. Okay. Yeah. Just 1500 segmentation maps,

833
00:51:20,160 --> 00:51:25,200
either supervised or unsupervised. Yeah. A very few amount of segmentation maps.

834
00:51:40,000 --> 00:51:46,960
We did experiment with using more segmentation maps and it showed that the accuracy kind of

835
00:51:46,960 --> 00:51:51,600
fluctuates at some point. I mean, there's some point where it doesn't improve more if you add

836
00:51:51,600 --> 00:51:57,200
more segmentation maps, but you do have to take into account two things. One, we did find two,

837
00:51:57,200 --> 00:52:02,000
and we didn't trade for scratch. Two, we didn't have the resources to hyper parameter search for

838
00:52:02,000 --> 00:52:09,200
each selection of the number of. So it's possible that if you use more data, you would need to

839
00:52:09,200 --> 00:52:15,360
retune your hyper parameters and then get better accuracy improvement, but we didn't have the

840
00:52:15,360 --> 00:52:20,640
resources to do that. So it could be the case, but I don't really know. Yeah, I don't really have

841
00:52:20,640 --> 00:52:29,440
any finance for that. One thing we did to ensure that the model actually learns to predict better

842
00:52:29,440 --> 00:52:35,520
or to have better explanations is we looked at the accuracy increase for the non-training classes

843
00:52:35,520 --> 00:52:41,040
as well, because we said that we only use half the initial classes. It is really interesting to see

844
00:52:41,040 --> 00:52:48,080
if the model really improves on the non-training data as well. Does it learn to generalize the

845
00:52:48,080 --> 00:52:53,840
positive influence or the positive logic? And as you can see here, this is the ImageNet

846
00:52:55,120 --> 00:53:00,240
validation set. So yeah, there's a decrease as we saw before, but for the non-ImageNet

847
00:53:00,240 --> 00:53:05,120
distributions, for the shift of distribution, you can see that the improvement for the non-training

848
00:53:05,120 --> 00:53:10,240
classes is actually quite similar and sometimes even better than that of the training classes.

849
00:53:10,240 --> 00:53:16,880
So the model really from this experiment learns to generalize that healthy say-and-behavior to

850
00:53:16,880 --> 00:53:24,720
classes that were not in the training set. And here are some visual examples. These are

851
00:53:24,720 --> 00:53:30,320
examples from the ImageNet data set. So examples from the original data set of the model straight

852
00:53:30,320 --> 00:53:35,760
map. And here you can see that the same prediction is made for two different reasons. Here, the

853
00:53:35,760 --> 00:53:41,200
background, here actually the foreground, the snowplow. And here you can see corrective predictions

854
00:53:41,200 --> 00:53:47,200
where the model originally predicted that this is a can opener based on the eye of the puppet.

855
00:53:47,760 --> 00:53:53,760
And once we find you the model to look at the entire object or to look for, you know, less

856
00:53:53,760 --> 00:53:58,560
sparsely as the object, it actually talks about the teddy bear. And here you can see that even

857
00:53:58,560 --> 00:54:05,360
if the model is now wrong and was previously correct, you can usually quite easily explain

858
00:54:05,360 --> 00:54:10,800
why the model was wrong. So here's an example where the ground truth classification is tripod

859
00:54:10,800 --> 00:54:14,880
and the model predicted actually fine tuning a strawberry, but you can actually see that there

860
00:54:14,880 --> 00:54:19,360
exists a strawberry in the image. So it kind of makes sense that the model made that mistake.

861
00:54:21,760 --> 00:54:26,560
These are examples for shifted distributions. So as you can see before, for this example,

862
00:54:26,560 --> 00:54:32,800
the model predicted a garbage truck. Well, this is the forklift because the forklift is in a garbage

863
00:54:32,800 --> 00:54:39,280
area. So we correct the prediction to be a forklift based on foreground rather than the background.

864
00:54:39,280 --> 00:54:43,520
Here you can see a teddy bear that was classified as a ping-pong ball due to the sparse consideration

865
00:54:43,520 --> 00:54:49,520
of just its spot. And after the fine tuning, it is correctly classified. And the third example is a

866
00:54:49,520 --> 00:54:54,720
porcupine that was classified as a sea lion due to the background of the ocean. So once the model

867
00:54:54,720 --> 00:55:01,120
really learns to look at the correct pixels, it does make the correct prediction.

868
00:55:02,880 --> 00:55:08,080
These are additional examples, but really, we don't have time. And another interesting thing

869
00:55:08,080 --> 00:55:12,880
that we've noticed that I think is quite cool, even when you take examples that are completely

870
00:55:12,880 --> 00:55:19,280
out of distribution, I mean, this is an image generated by Dalit. And the models not know

871
00:55:19,280 --> 00:55:25,600
the class robot or oil painting and so on. Originally, it made a ludicrous prediction

872
00:55:25,600 --> 00:55:32,640
that this is a guillotine based on, I don't know what, you can't really understand. But after a

873
00:55:32,640 --> 00:55:37,120
fine tuning process, you can see that the model does not make maybe the best prediction that you

874
00:55:37,120 --> 00:55:41,920
can think of, which is the robot because it doesn't know that class. But it does predict the grand piano

875
00:55:41,920 --> 00:55:47,040
and it kind of makes sense because there is a piano in the image. So while the prediction,

876
00:55:47,760 --> 00:55:53,680
again, still does not make the most sense. At least it is based on some of the objects inside

877
00:55:53,680 --> 00:55:58,880
the image and not just something that you cannot make sense of due to sparse correlation.

878
00:56:00,560 --> 00:56:07,120
So this was the entire talk. Yeah, we got through it in time. Thank you very much. And the table

879
00:56:07,120 --> 00:56:12,000
of content is here in case you want to ask a question about specific parts of the lecture. Thank you.

880
00:56:17,040 --> 00:56:24,640
Yeah, one or two we can do. Yeah.

881
00:56:32,240 --> 00:56:34,960
Yeah, it's visible. Thanks for rotating it.

882
00:56:41,680 --> 00:56:44,800
Okay, the questions here are really lacking context because they were probably

883
00:56:45,200 --> 00:56:50,000
asked during that. So if anyone wants to ask a question again. Yeah.

884
00:56:52,720 --> 00:56:57,840
Yeah, I guess one question I had. Have you thought about including layer norm at all into your

885
00:57:01,360 --> 00:57:07,120
explanations? Because it seems that that does scale tokens in some way and could that be relevant

886
00:57:07,120 --> 00:57:13,040
for your output? Include what? Sorry, can you repeat it? Layer norm? Layer norm? Oh,

887
00:57:14,800 --> 00:57:21,760
no, but as I said, there is a method that I don't quite remember the name of the method that

888
00:57:21,760 --> 00:57:28,800
did take into account the norms of the output matrix, I think, in order to average across the

889
00:57:28,800 --> 00:57:35,760
different attention heads. But we haven't considered that. Yeah. We do consider that the gradients

890
00:57:35,760 --> 00:57:41,520
should be able to scale the different attention heads according to their influence on the prediction.

891
00:57:46,880 --> 00:57:48,560
Any other questions? Any other questions?

892
00:57:51,440 --> 00:57:54,800
I had a question if no one's going. Oh, yeah, go ahead.

893
00:57:57,360 --> 00:58:01,760
Wait, me or someone in the room? Sorry. Yeah. Okay, thank you.

894
00:58:02,720 --> 00:58:09,600
Okay. So I was wondering with regards to the stuff you said at the end, where some of them

895
00:58:10,160 --> 00:58:14,960
you see it and then you're like, okay, that was wrong, but like, makes sense. That's

896
00:58:18,000 --> 00:58:26,400
is there a way to quantify that and were related things or is it more like a you know it when

897
00:58:26,400 --> 00:58:32,640
you see it? Oh, yeah, that's a great question. There is a work done by Google, I think, that actually

898
00:58:34,000 --> 00:58:41,600
relaxes the task of classifying objects using ImageNet. They actually re-tagged ImageNet,

899
00:58:41,600 --> 00:58:46,880
where, you know, a strawberry in that case wouldn't be a mistake, but maybe it would be

900
00:58:46,880 --> 00:58:53,600
half a mistake or something like that. Yeah, so there's such a work that re-tags the entire ImageNet

901
00:58:53,600 --> 00:58:59,040
dataset to account for mistakes that aren't really mistakes, but actually makes sense.

902
00:59:00,160 --> 00:59:07,440
But other than that, I would say there isn't an automatic way to know that. I mean, I can't think

903
00:59:07,440 --> 00:59:11,920
off the top of my head of an automatic way to know when the model is mistaken, but it's okay.

904
00:59:13,360 --> 00:59:19,440
Cool. How did you guys check? Like, was it mainly the accuracy increase on the distribution

905
00:59:19,440 --> 00:59:27,120
shifted versions? Yeah, yeah, it was mainly, yeah, it was mainly the accuracy on the distribution

906
00:59:27,120 --> 00:59:35,680
shifts. And, yeah, yeah, and also looking at a lot of examples, right? Because I started out

907
00:59:35,680 --> 00:59:44,800
with many. Yeah, yeah, yeah, and a lot of manual work on actualizing examples that got me to the

908
00:59:44,800 --> 00:59:48,720
intuition that I'm presenting now, because I actually thought in the beginning that having

909
00:59:48,720 --> 00:59:55,600
the relevance be a segmentation map is quite logical. Yeah, so it takes some time to get

910
00:59:55,600 --> 01:00:02,000
through all the conclusions. Yeah, yeah. I was just having one idea coming out. Is that possible?

911
01:00:02,000 --> 01:00:09,760
So you are going to be this key time. So that in a strawberry case, you can crop out that region

912
01:00:09,760 --> 01:00:15,200
and then maybe run through another like Oracle network to tell you whether it is a strawberry or

913
01:00:15,280 --> 01:00:20,320
not. And that gives you some kind of a semi-automatic way. Yeah, yeah, definitely. That's an

914
01:00:20,320 --> 01:00:25,600
interesting take. It's interesting, particularly because I saw that different models tend to

915
01:00:25,600 --> 01:00:30,800
learn different spurious correlations. So it actually makes sense to check models using other

916
01:00:30,800 --> 01:00:35,840
models. Yeah, they're consistently making the same prediction with these vets. Yeah, yeah, perhaps.

917
01:00:35,840 --> 01:00:41,760
Yeah, yeah, that's an interesting idea. Your current relevancy extractor approach

918
01:00:42,720 --> 01:00:50,480
is limited by the VITs tile resolution. It outputs the attention map that is the size of the

919
01:00:50,480 --> 01:00:58,480
tiles and then you can bi-level it. Upscaling, yeah. I was wondering whether there's a way to bypass

920
01:00:58,480 --> 01:01:05,520
this tile resolution just by considering that we also have pixels coming into the tile.

921
01:01:06,080 --> 01:01:13,440
Yeah, yeah, we have tried to propagate relevance all the way back to the actual input and not on

922
01:01:13,440 --> 01:01:19,920
the level of each patch. It didn't come out just quite as we hoped. I think that the issue there

923
01:01:19,920 --> 01:01:25,040
is probably the positional encoding in the way. Somehow that layer of positional encoding

924
01:01:26,320 --> 01:01:32,800
ruins or destroys the relevance values once you propagate back from it. I couldn't figure out how

925
01:01:32,800 --> 01:01:42,080
to get past that layer that actually kind of added noise to the output relevance map.

926
01:01:42,560 --> 01:01:59,840
That's an interesting point, but yeah. Yeah, I haven't come across any such architectures

927
01:02:00,400 --> 01:02:07,040
if you do let me know and I can give you the try. No, there was a question here, right?

928
01:02:07,360 --> 01:02:12,480
I was just wondering, when you're about to work for, what if I wanted to explain about a color

929
01:02:12,480 --> 01:02:16,000
or something, or non-message, I'd like to just go to something about discussion, right? Maybe

930
01:02:16,640 --> 01:02:21,200
then perhaps lemon versus orange, you can color, kind of the main thing. That's just curious.

931
01:02:24,720 --> 01:02:29,760
This specific method would not be able to do that, but I know that there are explainability

932
01:02:29,760 --> 01:02:36,960
methods that kind of create a decision tree from the model. So you pay the price,

933
01:02:37,120 --> 01:02:42,480
that the accuracy decreases to some extent, and then you create a decision tree based on the

934
01:02:42,480 --> 01:02:47,600
decisions of the model. You kind of model the model using a decision tree, and then you may

935
01:02:47,600 --> 01:02:55,120
have a split that it has to do with, you pass a lot of images through a lot of images of oranges

936
01:02:55,120 --> 01:03:00,800
and lemons, and you see that one of the splits is by the color. Yeah, and then you know that.

937
01:03:01,200 --> 01:03:08,960
And probably you can do some trivial things to test specific theories, like turn the image

938
01:03:08,960 --> 01:03:14,160
into black and white and see what happens, to consider if the model takes into account,

939
01:03:14,800 --> 01:03:17,360
but this method will not be able to do that.

940
01:03:20,000 --> 01:03:21,520
All right, let's hand this speaker.

