start	end	text
0	4640	Awesome, so we're so excited to have Hila Chepur, am I pronouncing it right?
4640	6400	Yeah, it's actually in Hebrew, it's Chepur.
6400	8800	Oh, it's Chepur.
8800	10080	Yeah.
10080	10880	All right.
10880	13840	So Hila is a PhD candidate at Tel Aviv University,
13840	15680	advised by Professor Leon Wolfe.
16400	19920	Her research focuses on developing reliable XAI algorithms
19920	22880	and leveraging them to promote model accuracy and fairness.
23760	26640	Today she's going to talk to us about transform or explainability,
26640	28560	and we're so excited to hear from you.
28640	30400	Oh, thank you for that great introduction,
30400	32320	and I actually have some slides here.
32960	33760	I think you can just...
34480	35440	Oh, yeah.
35440	38880	Slides here introducing myself, but I think you did that perfectly,
38880	39760	so I'll just skip that.
41200	44560	So maybe the first thing we want to talk about is motivation.
44560	46560	We're here to talk about transform or explainability,
46560	47920	but why should you care?
48640	50080	And let's just have a disclaimer,
50080	52640	because we all know that explainability is really important
52640	56240	for aspects like accountability, reliability, and so on.
56240	57680	But when we write research papers,
57680	60080	we usually focus on other measuring sticks,
60080	63040	such as accuracy and robustness, right?
63680	66800	So I'm here to convince you that the explainability queue
66800	70240	is actually really useful for those measuring sticks as well,
70240	73360	and that you should consider using explainability
73360	75200	in your research, even if it's unrelated
75200	77200	to accountability, reliability, and so on.
78640	81120	So to do that, I have a few examples showing
81120	84080	how explainability can be used to improve model accuracy
84080	85680	and robustness.
85760	87520	The first example is text to live.
87520	88240	Maybe you know it.
88240	90560	It got accepted to ECCV 2022.
91120	95200	And the objective of the model is to take a target edit text,
95200	99440	for example, here around hat, and apply the edit to the image.
99440	102320	So what they try to do is unlike other works,
102320	105600	they try to prevent the part where the user actually has
105600	108960	to insert a manual segmentation mask
108960	111120	to indicate where the hat is.
111680	114800	So they wanted some automatic way of getting
114800	117040	to the region of interest, which is the hat.
117040	119360	So they use relevancy maps of the clip model,
119360	120880	which I guess you're familiar with.
122560	126000	So the clip model with the relevancy map actually indicated
126000	132480	to their downstream path, where pretty much the hat is in the image.
133040	136080	And then their model refined those relevancy maps
136080	140480	and applied the edit according to the location indicated
140480	141840	by the relevancy map.
141840	144000	So here you can see what happens without this.
144000	146880	They call it bootstrapping without using the relevancy map.
146880	149120	So you can see that there are additional artifacts,
149120	151920	such as the faces turning red and not just the hat.
151920	154640	And when you use bootstrapping, when you use relevancy maps,
154640	158240	then the edit is quite localized to the hat.
158240	160720	And here you can see other edits that are quite nice.
160720	164160	They take a sponge cake and they turn it into an ice cake
164160	165280	or a spinach mousse cake.
165280	167760	So I think it's a nice example to show.
168800	171360	Another example is a paper called Kripaso.
171360	174240	It's that best paper awarded Seagraph 2022.
174800	176640	It also uses relevancy maps.
176640	179120	The goal of the model is to take an input image
179120	182480	and create sketches with different levels of abstraction.
182480	184720	So you can choose which level of abstraction you want.
184720	186240	So for example, the flamingo here,
186960	191680	very abstract painting of the flamingo or very detailed painting.
191680	193840	And what they did is they actually used the relevancy maps
193840	198880	as an initializer for the model to understand where the object is
198880	200960	and how to create the stroke's product.
202160	203760	Another example is Sameer's work,
203760	205680	which you're probably familiar with.
207760	211040	What you did here, really, is you used the relevancy maps
211040	214160	in order to locate objects which aren't necessarily
214880	218480	objects in the wild, objects that appear in living rooms
218480	222160	and that do not necessarily appear in the training set
222160	226160	of some models of desegmentation or localization.
226160	229120	So you can use Kripaso in order to identify objects
229120	231920	that are really not really objects
231920	233680	that are so common in training sets.
235760	239280	So if you consider all the examples that we've seen before,
239280	241040	they have one thing in common.
241040	244160	They use the relevancy maps as a fixed signal.
244160	246640	They didn't train on the relevancy map
246640	248480	or create a loss with the relevancy map.
248480	251840	They use it as an initialization for the answering task.
252400	254880	But what we did in our last work is actually just,
254880	258560	we showed that its reliability can be used as a loss term
258560	260080	in order to improve models.
260640	262560	So if you think about what it means
262560	265440	to create a loss term based on explainability maps,
265440	269440	it's really meant to teach the model how to do something
269440	272800	or why it does something, not just how to do something.
272800	274800	And we talk about classification models.
274800	277760	They tend to learn spurious cues
277760	280960	to help them make shortcuts to make a prediction.
280960	283360	So for example, a model can learn a spurious cue
283360	287360	that if you have a round object with the background of a grass,
287360	288800	then it's a golf ball.
288800	291040	And here you can see that the model classified this lemon
291040	293520	as a golf ball because of the background of the grass.
294160	296560	When you force the model to actually focus
296560	298240	on the foreground of the image
298240	299760	and not just the background of the image,
300560	304080	via a loss applied directly to the explainability maps,
304080	306880	you can correct wrong predictions based on spurious cues.
307440	310080	So what we're doing usually is we're teaching the model
310080	311440	to predict something, right?
311440	315360	Predict golf ball, car, glasses, etc.
315360	317600	But we're not really teaching it why.
317600	320240	Why this is the object in the image.
320240	324800	So what we're showing here is that by fine-tuning directly
324800	327440	the relevant slots or the explainability maps,
327440	330160	we can correct wrong predictions based on spurious conditions.
330960	337200	But we'll get to it in depth later on.
338400	340000	So this was the motivation part,
340000	342480	and hopefully you got fully motivated
342480	344720	as to why Transformer experiment is interesting.
345360	348240	Our talk is going to be a construed of two parts.
348240	349840	The first part is going to be,
349840	352720	we're going to talk about how we do Transformer explainability.
352720	354640	We're going to see the attention mechanism
354640	356160	which I'm sure you're all familiar with,
356160	359280	but we're going to have emphasis on specific parts
359280	361280	of the attention mechanism that are going to be useful.
361840	363200	Then we're going to ask ourselves,
363200	364720	is attention an explanation?
364720	367360	Which is really the most prominent question
367360	369440	when doing Transformer explainability.
369440	372400	We're going to talk about three explainability algorithms.
372400	374240	The first one is attention roll-up, not by me.
374960	377840	But it is the groundbreaking first algorithm
377840	379200	in the big Transformer explainability.
379200	381360	Then I'm going to present two of my works
381360	383280	that have to do with Transformer explainability.
384160	385040	And in the last part,
385040	387360	we're going to talk about the work that I just presented
387360	389280	and that Shiran had a question on.
390240	392640	And probably hopefully we'll answer the questions
392640	395200	and see how Transformer explainability can be used to
395840	400320	devise models or maybe incorrect spray-excused
400320	401040	at the models where.
401040	404800	So just to set us up,
404800	406320	this is a Transformer architecture
406320	408000	as presented in attention is all you need.
408000	409520	I'm sure you're all familiar with it.
410080	412320	We will be focusing on the encoder
412320	414240	since the attention mechanism here
414240	415840	is a self-attention mechanism.
415840	417040	And it's quite more intuitive
417040	418960	and easy to grasp and understand.
418960	421200	And the concepts that apply here to the encoder
421200	423680	are pretty easily generalized to the encoder.
423680	426320	So we're going to focus on the encoder for simplicity
426320	428640	and for intuitive explanations,
428640	431840	but the principle is quite easily generalized
431840	433040	to the encoder as well.
433040	434160	By the way, if you have questions,
434160	435680	just feel free to stop me.
437680	440960	Okay, so let's talk about the intuition behind self-attention.
441520	443760	What self-attention does is actually creates
444480	446160	contextualized representations.
446160	447680	So I'm sorry for the people on Zoom,
447680	449520	but I'm going to write here on the right for it.
449520	451040	So we have an example.
451040	452560	We're running an example to work with.
453120	455840	Say we have the sentence, the count,
456800	460240	set on the max.
462560	464800	And let's consider the first day in the sentence.
465760	468560	We want to create now an embedding
468560	470240	for each one of the words in the sentence,
470240	472320	for each one of the token incidents.
472320	474160	But the token does quite meaningless
474160	475440	with our context, right?
475440	476800	It could refer to anywhere.
477360	478960	So what the attention mechanism does
478960	481520	is it actually creates contextualized representation.
482160	485920	It should take information from the other token
485920	489920	and insert it to the current token that we're interested in.
490480	493440	So for example, intuitively, maybe we would expect
493440	497360	that since the word the refers to the word cat,
498480	502240	information from the word cat will be moved into the word the,
502240	505680	such that the embedding of the word that is contextualized
505680	509280	is enriched by context from the word cat.
510240	512240	So what the attention mechanism does
512240	516160	is it actually uses query, key, and value matrices.
516160	520880	And we can think about it maybe as a database theory information.
521440	524960	So when we talk about databases, we have queries,
526880	529680	which are questions that we run on our database.
530480	531680	We have keys.
531680	534160	The keys represent the entries in our database.
534800	537840	And we have values that corresponds to the keys, right?
537840	541840	So what we're doing here is we're actually running a query
541840	552000	asking which tokens are relevant to the...
554160	556560	We can think about it intuitively as running a query
556560	558560	on all of these tokens that we have.
559120	561520	And then the keys represent all the other tokens.
562080	563840	What we do with the attention mechanism
564160	566960	is we calculate an attention matrix
566960	569840	that is going to be the star of every transformer experience
569840	570960	ability algorithm.
571840	575840	It's going to be a soft mass of the multiplication
575840	580960	between queries and keys normalized by the embedding dimension.
581840	585840	This similarity scores are actually telling us
585840	589840	how much each word is relevant to our word of interest.
590800	592640	So the multiplication between queries and keys,
592640	595840	we can think about it kind of like as relevant scores.
595840	599040	How much is each token relevant to the token that...
599040	600240	to the word that...
603040	606240	And after we calculate those similarity scores,
606240	608480	we create the enriched representation
609200	611840	by multiplying the scores by the values.
613040	616640	Such that each word gets information from the other words
617360	618640	by these relevance values.
618640	621440	So these relevance values determine how much each word
621440	624640	is going to influence the word of after the attention.
626640	629440	So this is going to be the key intuition to everything
629440	632240	that we do later on to explain a transformers.
634640	636640	The most important thing to remember about explaining
636640	640640	transformers is we don't have just a single attention matrix.
640640	642640	This mechanism happens H times,
642640	644640	where H is the number of attention heads
644640	645640	that we have.
645640	647640	And intuitively, we can think about it as,
647640	649640	you know, in CNNs, you have kernels.
649640	651640	Each kernel has its own purpose.
651640	654640	Some refer to the background, some refer to the edges,
654640	656640	the shapes, and so on.
656640	658640	Transformers have the same thing with attention heads.
658640	660640	So each attention head can have a different purpose.
660640	662640	And actually, researchers have shown
662640	666640	that you can probably prune most of the attention head
666640	669640	and achieve the same accuracy,
669640	672640	which means that most attention heads are really not important
673640	676640	to the prediction, to a specific prediction of the model.
676640	679640	So it's really important when we think about transformers
679640	682640	and to understand that the different heads have different needs.
685640	688640	The final thing that we need to remember about transformer,
688640	691640	you know, predictions is that transformers use
691640	694640	a classification token for the prediction.
694640	696640	So once the entire attention mechanism is done
696640	698640	and all the tokens are contextualized,
698640	701640	the classification token is the only token.
702640	704640	That is used to make the prediction.
704640	706640	There's a linear layer on top of the classification token
706640	708640	and then the prediction is made.
708640	710640	So basically what the classification token does
710640	714640	is kind of like creates an aggregated representation
714640	716640	of the entire input.
716640	718640	You can think about it as a global representation
718640	720640	of all the tokens in the input.
720640	722640	You have questions so far,
722640	725640	because we're going to move on to the interesting stuff.
725640	727640	Yeah.
727640	729640	So moving on to transformer explainability,
729640	732640	it's really important to set up our goals.
732640	735640	My goal is to facilitate explanations that help you guys,
735640	738640	the researchers that actually use the models.
738640	741640	And the way that we do that is by creating hitmaps.
741640	744640	So the hitmaps should correspond to the pixels,
744640	747640	if we're speaking of images or if we're speaking of text,
747640	750640	and hitmaps should correspond to the tokens.
750640	752640	The hitmaps should correspond to the pixels
752640	754640	that influence the prediction by the model.
754640	756640	So for example, here we see the verb
756640	759640	and the hitmaps actually highlights the pixels
759640	761640	relating to the verb.
761640	765640	And the toothbrush or the ship or the bikes and so on.
765640	768640	So the hitmaps should tell us which pixels in the input
768640	773640	make the prediction as it is.
773640	775640	Okay, we got to the interesting part.
775640	777640	Yeah.
777640	779640	When you talk about transformer explainability,
779640	782640	researchers have looked at this attention matrix
782640	784640	and asked the question,
784640	787640	is this attention matrix an explanation?
787640	789640	How can it be an explanation?
789640	792640	Because we said that the attention values are actually
792640	794640	kind of like relevance values, right?
794640	797640	There are values that reflect how much each token
797640	799640	influences each other's token.
799640	802640	And we also said that the classification token
802640	805640	is the only token that is used for the prediction, right?
805640	808640	So if we look at the row in the attention matrix,
808640	810640	that corresponds to the classification token,
810640	812640	and look at these relevance values,
812640	814640	these should be the relevance values that determine
814640	817640	how much each token influences the classification token,
817640	820640	which is basically how much each token influences
820640	822640	the classification, right?
822640	825640	So maybe these values are just the relevance values.
825640	827640	Each token represents a patch in the image.
827640	830640	Maybe these are just the values that we need.
830640	834640	And we're all done just like decision trees are self-explanable
834640	837640	or linear regression is self-explanable.
837640	839640	What do you think? Are we done?
839640	840640	We're done.
840640	842640	Yeah, we're done.
842640	843640	We're done.
843640	844640	Yeah, we're done.
844640	845640	We're done.
845640	846640	We're done.
846640	847640	We're done.
847640	848640	We're done.
848640	849640	Yeah.
849640	850640	Yeah.
850640	852640	The attention matrix is used to multiply the value representation.
852640	853640	Yeah.
853640	855640	The representation should be positive, negative,
855640	856640	large, small.
856640	859640	It doesn't actually tell us how much it is actually contributing
859640	860640	to the final classification.
860640	861640	Yeah.
861640	862640	I mean,
862640	864640	the two problems that we,
864640	866640	we point out to.
867640	870640	The values can't be negative,
870640	873640	but I don't think really when you say, okay,
873640	875640	let's refer to.
875640	879640	These values are actually directly determining how much
879640	881640	information from each token you're going to take.
881640	883640	And think there's a softmax operation here.
883640	886640	All the values are non-negative, right?
886640	889640	So there is a distribution that's defined on all these tokens
889640	891640	of how much each token is.
892640	896640	So intuitively, these are really relevant values,
896640	900640	but we do have two other issues that we should refer to.
900640	903640	The first one is we said we have a few attention heads, right?
903640	906640	Each tension head has a different meaning.
906640	909640	Some attention heads are really not relevant to the prediction.
909640	913640	How do we aggregate across these attention heads in a way that takes
913640	915640	into account the meaning of each head?
915640	918640	We wouldn't want to take into account heads that do not affect
918640	920640	the final prediction of the model.
920640	923640	And there are such heads since there's research that show that
923640	926640	you can prune most of the heads without impacting the prediction
926640	927640	of the model.
927640	929640	So you have a few attention heads and it isn't,
929640	932640	isn't clear how you aggregate across these attention heads in a
932640	935640	way that takes into account the importance of each head.
935640	938640	And the second question that we have is we refer to the
938640	942640	single attention layer, but we have a few attention layers.
942640	945640	So the first attention layer may incorporate information into
945640	947640	token one from token three.
947640	949640	And then in the second layer,
949640	952640	token one isn't simply the patch that it represented in the
952640	953640	beginning.
953640	956640	It is this patch with information from this patch.
956640	957640	In the second layer,
957640	959640	it's this patch with information from this patch,
959640	960640	and maybe this patch,
960640	961640	and maybe this patch.
961640	964640	And by the end of the attention mechanism,
964640	968640	how do we know which token refers to which input patch, right?
968640	969640	They're all mixed up.
969640	972640	That's the entire idea of the attention.
972640	974640	So we have two issues here.
974640	976640	How do we aggregate across attention heads?
976640	979640	Since we know that they have different means and how do we
979640	981640	aggregate across attention layers?
981640	982640	Yeah.
982640	983640	Just so that I understand.
983640	985640	So if there's only one attention head,
985640	987640	and also there's only one attention layer,
987640	989640	then the relevance board is the attention.
989640	990640	Yeah.
990640	991640	Yeah.
991640	992640	By this hypothesis, yes.
992640	993640	Okay.
993640	994640	Yes.
994640	1000640	And then I think there are some models that use this for visual
1000640	1003640	question answering and actually did that visualization.
1003640	1004640	And it works.
1004640	1005640	Pretty well.
1005640	1008640	So assuming you have one attention head and one attention layer,
1008640	1009640	it should be.
1009640	1010640	Fingers crossed.
1010640	1012640	It should be their elements.
1012640	1014640	I haven't tried that, but, but yeah.
1014640	1016640	By this intuition.
1016640	1021640	So the attention all of mechanism is actually the first method to
1021640	1023640	explain transfer was that came out.
1023640	1024640	In 2020.
1024640	1025640	And they add,
1025640	1028640	they propose that the two simplest solutions,
1028640	1031640	maybe that we can think of to solve those three issues.
1031640	1033640	Head aggregation by averaging.
1033640	1034640	And again,
1034640	1036640	remember we said different meanings to different heads.
1036640	1038640	So that's maybe over simplistic.
1038640	1041640	And they're aggregation by matrix multiplication.
1041640	1042640	And if you think about it,
1042640	1045640	matrix multiplication from the end to the beginning,
1045640	1047640	kind of unravels things.
1047640	1051640	The connections between the two that were made by the attention
1051640	1052640	mechanism.
1052640	1058640	They also propose another method called attention flow,
1058640	1063640	which evaluates the flow values in the attention graph,
1063640	1066640	like a classic flow problem from algorithms,
1066640	1068640	but it's too computationally expensive for images.
1068640	1070640	So we're not really going to get into it.
1071640	1074640	So getting into the first method we propose,
1074640	1077640	what we were saying is that the assumptions made by the attention
1077640	1079640	role of mechanism were solid,
1079640	1080640	but maybe over simplistic.
1080640	1081640	Yeah.
1081640	1082640	Question.
1082640	1084640	You may have some questions.
1084640	1088640	Oh, yeah.
1088640	1089640	We say in Hebrew.
1089640	1090640	Oh,
1090640	1101640	right.
1101640	1106640	I may take those at the end of the talk just because otherwise
1106640	1108640	we won't be able to finish with time.
1108640	1111640	Yeah.
1111640	1112640	Yeah.
1112640	1115640	So getting back to the first time we're going to propose.
1115640	1118640	We were saying that the assumptions made by attention roll off were
1118640	1120640	nice and worked in some cases,
1120640	1122640	but are maybe a bit simplistic.
1122640	1125640	We want to be able to average across the heads in a way that
1125640	1127640	actually takes into account the meaning of each other.
1127640	1130640	So what we're going to do is we're going to use a signal that is
1130640	1132640	very useful in explainability in general,
1132640	1134640	which is radiance, right?
1134640	1138640	Because radiance intuitively mean if I change this a bit,
1138640	1140640	how does this change a bit, right?
1140640	1144640	So if we take the gradients with regards to the output of the model,
1144640	1147640	which is over here, the gradients of the attention matter.
1148640	1152640	We can use the gradients as weights for the attention maps.
1152640	1155640	So instead of just averaging across the maps,
1155640	1159640	we take the gradient, the gradient gives us the weights element.
1159640	1163640	And we multiply the gradients by the attention and then each head
1163640	1165640	gets a weight from the gradient.
1165640	1169640	And then each attention head is not just the simple attention head
1169640	1170640	that it was in the beginning.
1170640	1173640	It is the attention weighted by the gradient.
1173640	1176640	And then we can average across the heads in a way that takes into
1176640	1178640	account the meaning of each head.
1178640	1180640	So this is why the gradients are here.
1180640	1184640	But we have another component that I won't get too deeply into
1184640	1188640	because it was removed for our second method.
1188640	1192640	It is the LRP component, layer-wise relevance propagation.
1192640	1196640	The second thing we thought of was that we actually reduce the
1196640	1199640	entire transformer architecture to just a multiplication
1199640	1200640	of queries and keys.
1200640	1203640	So it's not even the entire attention mechanism because we also
1203640	1205640	had the values there, remember?
1205640	1208640	So we narrowed down this entire, not so complex,
1208640	1210640	but architecture, right?
1210640	1211640	It has activations.
1211640	1213640	It has linear projections.
1213640	1216640	We narrowed all down to the multiplication between queries
1216640	1217640	and keys.
1217640	1220640	So we do want to take into account the other layers of the
1220640	1224640	transformer and how they impact the calculations.
1224640	1228640	So instead of just taking, let's get back to the whiteboard here,
1228640	1231640	instead of just taking the attention map,
1232640	1235640	that is quite, it's simplistic, right?
1235640	1238640	It's not that, but the multiplication between queries and keys,
1238640	1241640	we want to take into account a different attention map,
1241640	1246640	we'll call it RA, which takes into account the other layers
1246640	1248640	of the transformer architecture.
1248640	1251640	So instead of taking just these raw relevance values,
1251640	1255640	we take relevance values calculated by LRP.
1255640	1259640	And LRP is a mechanism that does back propagation with
1259640	1262640	gradients from the end of the network all the way to the beginning.
1262640	1266640	And it can give us relevant values for specifically this attention
1266640	1267640	matrix.
1267640	1270640	So instead of taking into account the attention values,
1270640	1273640	the raw attention values, we take into account the relevance values
1273640	1276640	of the attention matrix.
1276640	1279640	And as I said, I won't get too deep into it because we actually
1279640	1281640	removed it in our second method,
1281640	1285640	which is the one that I want to get into in more details.
1285640	1291640	So we have the attention gradients to average across the heads.
1291640	1294640	And we have the relevance in order to account for all the other layers
1294640	1296640	in the transformer.
1296640	1298640	So this is how we average across the heads.
1298640	1301640	And the way that we average across the layers is by matrix multiplication.
1301640	1306640	Here we adopted the interpretation from attention robot.
1307640	1316640	Oh, no, not element wise, actual matrix multiplication.
1316640	1318640	The matrices are squared matrices.
1318640	1322640	Yeah, because they are self attention matrices so you can actually
1322640	1323640	multiply them.
1323640	1326640	And if you think about it, you can unravel it when multiplying two
1326640	1327640	attention matrices.
1327640	1331640	It actually says, if the previous layer gave token one information
1331640	1334640	from token three, and this layer gives token one information from
1334640	1339640	token four, then it unravels both operations to ensure that you
1339640	1345640	actually take into account all the context.
1345640	1349640	Yeah, so this is just a rewind of what we saw in the previous slide.
1349640	1351640	How do we average across heads?
1351640	1353640	We take the gradients as weights.
1353640	1355640	We take the relevance instead of the pure attention weights.
1355640	1357640	And then we do averaging.
1357640	1359640	But here the average is not just the raw average.
1359640	1364640	If we had before it is weighted by the gradients.
1364640	1370640	And here you can see a few examples of how our method works.
1370640	1373640	So by the way, this is a slide that was added, but we don't have the
1373640	1374640	updated slide.
1374640	1378640	So let's just see what we get in the end of this calculation.
1378640	1384640	So
1384640	1389640	at the end of this calculation, we had an attention matrix, which
1389640	1392640	is the attention matrix after by the averaging and everything.
1392640	1397640	We have attention matrices for all the layers.
1397640	1405640	And then we multiply these.
1405640	1411640	So really, we have one attention matrix that takes into account all
1411640	1414640	the layers and all the heads.
1414640	1419640	And now we can get back to, can we go back in the slides?
1419640	1422640	Oh, no, it's only going forward.
1422640	1423640	Yeah.
1423640	1426640	Maybe there's an arrow at the bottom left corner.
1426640	1428640	If you move your mouse.
1428640	1431640	Oh, the back arrow.
1431640	1435640	Oh, it's the other way around.
1435640	1438640	Yeah.
1438640	1441640	And now we're actually getting to the point that Sharon made that right
1441640	1446640	now we only after all the aggregations that we made, we have one aggregated
1446640	1450640	attention matrix for the entire network because we aggregate across heads
1450640	1452640	and then we aggregate across layers.
1452640	1456640	And once we have that one attention matrix for the entire, for the entire
1456640	1461640	network, then we can use that intuition that we had that the row corresponding
1461640	1464640	to the classification token is actually the explanation.
1464640	1466640	So this is how we extract the final explanation.
1466640	1470640	And then we're actually the relevance values that we use.
1470640	1475640	And the one the other way around, right.
1475640	1478640	Okay.
1478640	1482640	So as you can see here, we have comparisons between our method and other methods
1482640	1487640	that are either adapted from CNNs, or methods that were constructed for
1487640	1489640	transformers such as rollout.
1489640	1493640	So as you can see here, rollout tends to have a lot of noise in the background
1493640	1497640	and we think about it intuitively as resulting from the fact that they just
1497640	1501640	average across the heads and not take into account the meaning of each head.
1501640	1506640	And some methods such as partial LRP fail on some cases, but in these
1506640	1509640	specific cases, they actually do pretty well.
1509640	1513640	But I do want to point out that they do not distinct between classes.
1513640	1518640	So for example, if we have an elephant and a zebra in an image, our method is able
1518640	1522640	to produce explanations specifically for the elephant or specifically for the zebra.
1522640	1527640	And when we don't do that, and we want to use this method say partial LRP to
1527640	1532640	explain predictions by the model, it will be hard to do that because if you want
1532640	1537640	to explain the elephant prediction, we may have results coming in from other classes.
1537640	1541640	So we're not really sure if the things that we're seeing highlighted are highlighted
1541640	1546640	because of the elephant or because of other classes making their way into the
1546640	1547640	explanation.
1547640	1551640	So I think personally, class specific explanations are really important to ensure
1551640	1554640	that we're really explaining the specific prediction of the model.
1554640	1555640	We're good.
1564640	1565640	That's a fantastic question.
1565640	1567640	That's a fantastic question.
1567640	1571640	Usually people from explainability evaluate explanations differently than what you
1571640	1573640	as end users may have to be.
1573640	1577640	So what we do is we use erasure based methods.
1577640	1581640	So what we do is we take the pixels that are said to be important to buy with it.
1581640	1585640	We take them out and we see if the model changes its prediction or not.
1585640	1587640	And similarly, we do the other way around.
1587640	1591640	We take the pixels that are unimportant by benefit and take them out and see that
1591640	1593640	the model still predicts the same.
1593640	1597640	You have to take into account when you can see that the model still predicts the
1597640	1598640	same.
1598640	1602640	You have to take into account when you do that, that you create images that are
1602640	1605640	out of the distribution of the model was trained on.
1605640	1608640	So this method is not really, you know, airtight.
1608640	1611640	And there's a lot of research around how do we value it.
1611640	1615640	And how do we know if the explanation is really good or not.
1615640	1617640	Any other questions.
1617640	1618640	Yeah.
1619640	1630640	Yeah.
1630640	1631640	Yeah.
1631640	1632640	Yeah.
1632640	1633640	Yeah.
1633640	1634640	Yeah.
1634640	1635640	Yeah.
1635640	1636640	Yeah.
1636640	1640640	Um, just because we want to have a measuring stick that actually measures the
1640640	1644640	explainability without relation to the algorithm itself.
1644640	1648640	So the measure should be unrelated to whether it's a transformer or CNN.
1648640	1652640	It should be unified throughout all the different architectures, right?
1652640	1656640	Just as you use accuracy to measure CNNs or transformers or whatever
1656640	1657640	architecture you use.
1657640	1661640	You want to have a measuring stick that really measures the method and not
1661640	1664640	something that has something to do with specifically.
1665640	1668640	If you have an explanation for CNN, it also has, you know,
1668640	1670640	values for each pixel.
1670640	1671640	Yeah.
1671640	1672640	Yeah.
1672640	1673640	Just zero it out.
1673640	1674640	You just zero it out.
1674640	1676640	And it works on the input itself.
1676640	1677640	So it really,
1677640	1679640	it is undependent even of.
1679640	1680640	You know,
1680640	1681640	you know,
1681640	1682640	you know,
1682640	1683640	you know,
1683640	1684640	you know,
1684640	1685640	you know,
1685640	1686640	you know,
1686640	1687640	you know,
1687640	1688640	you know,
1688640	1689640	you know,
1689640	1690640	you know,
1690640	1691640	you know,
1691640	1692640	you know,
1692640	1695640	it really it is independent even of the method you use or the model.
1695640	1717640	It is a measuring stick that has nothing to do with which method you use
1717640	1720640	for expansion or expansion in which version.
1720640	1729840	the sun. I'm not sure I got your question exactly, but I would say that there are methods evaluating
1729840	1735440	explanations by adding sparse correlation, making sure that the model reaches 100 percent accuracy
1735440	1740400	due to the sparse correlations, and then making sure that the explanation outputs these sparse
1740400	1746240	correlations versus the odd correlation. So there are methods that do that, but yeah. But we usually,
1746240	1751280	I usually use erasers as methods to evaluate explanations, but this is a really active
1751280	1756640	goal of research, right? So it's not really, you know, obvious how we evaluate explanations
1756640	1764880	and what's the right way to do that. I think I'm maybe moving backwards instead of forward.
1766000	1772160	Some technical issues. Yeah, okay. I may just skip this because we do have the motivation
1772160	1777600	that we did in the beginning and we're a bit behind on time. So our second method
1778560	1784080	said, you know what? We really believe that multimodal models are going to be a big thing.
1784960	1789520	And we only explained self-attention before, as you saw. We didn't go into
1789520	1796320	cross-attention or encoded encoded attention. And assuming that most transformers don't just do
1796320	1799520	right self-attention, we need a mechanism that can explain
1800160	1804080	cross-attention and encoded encoded attention as well, not just self-attention.
1804080	1809600	So the second paper actually expands the first paper, but for other types of attention.
1812080	1816560	So the first thing we do is get rid of the LLP, and that's why I don't, you know, get into a lot
1816560	1821360	of detail with regards to the LLP. The reason that we did that is because if you think about it,
1821360	1825920	we use LLP in order to account for all the layers, but really gradients account for all the layers
1825920	1830080	because backpropagation is backpropagated from the output all the way back together.
1830800	1835360	So we said, what happens if we remove LLP, which makes it easier for you guys to implement
1835360	1841200	the algorithm, and it makes it faster and more convenient, and it actually works pretty well.
1841200	1846640	So we remove the LLP component. I will say that if you want really accurate explanations,
1846640	1853200	usually I would go for the LLP version, right? Because LLP adds this added component that doesn't
1853200	1857680	exist without LLP. It does account for all the layers quite systematically.
1859120	1864480	So when we talk about cross-model interactions, we have four types of interactions in such models.
1864480	1869120	We have the self-attention interactions between the text tokens, how the text tokens influence
1869120	1876160	themselves, the self-attention interactions between the image tokens, and then two types of cross-attention
1876160	1880160	interactions, how text influences image and how image influences.
1883920	1890160	And then what we thought we would do is really track the self-attention layers.
1890160	1895760	So each self-attention layer mixes tokens. Okay, we'll mix the tokens in the relevance matrices.
1896400	1902000	So we start with an initialization of the relevance matrices at the beginning of the
1902000	1907360	modalities or self-contained. So images only affect images and text only affects text,
1907360	1912320	and each image token only influences itself. So the initialization for the self-attention
1912320	1918240	relations are just the identity matrices. And for the cross-model relations, it's a zero matrix,
1918240	1923440	because there are no cross-model interactions before we do any attention. And what the method
1923440	1929040	really does is it just goes on a forward pass through the attention layers, and as the attention
1929040	1935840	layers mix the tokens, the relevance values are mixed as well, just tracking the attention as it goes.
1937600	1942880	So I won't get into all the rules, all the rules that we have for all these specific attention
1942880	1946640	layers. I'm just giving you a motivation of how it works. And really, believe me, it's really
1946640	1953040	simple, even though the equations look complicated. So let's go over just the self-attention rule.
1953760	1960720	A self-attention layer has, again, multiple heads. We average across the heads using gradients just as
1960720	1968320	before. So we have now a single attention matrix marked here as a bar. And what we do again is just
1968320	1975360	matrix multiplication between the current attention mixture and the old attention mixture that existed
1975360	1982320	in the relevance matrix. So matrix multiplication and update the relevance matrix. This is all we
1982320	1986960	do. We just track the attention as it goes. As it mixes between tokens, we mix between the
1986960	1992160	relevance values. That's what we do. That's the entire algorithm. And head aggregation is done
1992800	2000480	via gradients as before. So taking a look at some examples that we have to demonstrate how this works.
2001040	2006960	For example, for CLIP, you can see that we've entered different texts with the same input image
2006960	2012880	and propagated gradients. And by the way, for CLIP, gradients are propagated. Let's take them back
2012880	2019440	to the whiteboard. For CLIP, because I know this is specifically interesting to you,
2020480	2026960	let's talk about how we propagate relevance for CLIP. For CLIP, you have an imaging quarter
2028640	2033120	and then a texting quarter. Both of them, by the way, use pure self-attention. So there's no cross
2033200	2041440	connection. This and this output representation and vector, which is by the way from the
2041440	2046720	classification. So this is the vector for the text and this is the vector for the image.
2047600	2056640	And the stimuli score is just a dot product. Both scores. So what we do is we propagate
2056640	2063360	gradients from this dot product back to the texting quarter
2065280	2069360	and back to the imaging quarter. And those gradients are going to be used to average
2069360	2073520	across the attention pens as we saw before. And then the attention heads are going to be
2073520	2079280	aggregated across different letters by matrix multiplication. So here we don't have an output
2079280	2085760	logic as we have for the classification, but we use this dot product between their presentations
2085760	2091520	to calculate the score that we propagate the gradients with regards to. So all that we do
2091520	2096560	here is really simple. Calculate the dot product between their presentations, propagate gradients
2096560	2101920	with regards to the dot product. Those gradients are going to be used as weights for the attention
2101920	2110080	matrices to average across them. And as you can see, the results are text specific since we
2110080	2114480	propagated the gradients with regards to the specific multiplication between the specific text
2114480	2118880	and the specific image. So actually for an elephant, you can see that the hidden map
2118880	2122240	corresponds to the elephant. For a zebra, the hidden map corresponds to the zebra. And for a
2122240	2125680	leg, the hidden map corresponds to the leg, showing us that the model really knows how to
2125680	2130240	distinct between different parts or different objects in the image according to the text input
2130240	2138240	that we give it. This is an example that we saw before. And visual question answering in case
2138240	2143360	any one of you is interested is actually an interesting use case. Because for visual question
2143360	2147440	answering, the model is given an image and a question, and it's supposed to answer the question
2147440	2152400	based on the image. And researchers have shown that when you actually lack out the entire image
2152400	2159440	and just give the model the question, it answers the question about 30% of design correctly.
2160080	2164720	So the question here is assuming that the model answers the question without seeing the image.
2166400	2171520	How do we measure the accuracy of such models? So you can use explainability to ensure that
2171520	2176080	the model actually used the image and the correct parts of the image to make the prediction.
2176080	2181520	For example here, the question is, did he catch a ball? We see that the player actually caught the
2181520	2187280	ball. And the answer is yes, but we also see that the model focused on the right parts of the image.
2187280	2191040	So it can really tell that the model made the prediction based on the image and not just the
2191040	2201280	question. I'm going to skip this part too. Yay. So we're switching gears. We're going to talk about
2201600	2205920	our method to improve model robustness using explainability. So if you have any questions
2205920	2211120	about the previous part on explaining transformers, this is the time to ask them.
2212960	2216960	No, no questions. Oh, I had a couple of questions in the chat. Yeah.
2218800	2223600	I'm sorry about that. There is no one really, you know, maintaining the chat.
2224400	2228640	Yeah, let's make it brief and then try to answer questions. Yeah.
2229520	2233280	Oh, okay. I was just wondering, why does it make sense to only look at the
2235760	2241120	attention maps outputted by the softmax? Because don't we have, don't we multiply by an output
2241120	2249120	matrix then that is able to shuffle across tokens afterwards? Do you mean the values matrix? No,
2249120	2257520	the output matrix. I guess that the intuition is just that the self attention mechanism,
2257520	2263600	its purpose is to contextualize in the way that the contextualization is made by the attention
2263600	2269840	values. So the attention value is actually, you know, determine how much each token is going
2269840	2277040	to be incorporated into the other tokens. We do have an additional output matrix and you mean
2277040	2282480	after the attention mechanism, right? Yes, yes. Yeah, okay. So some researchers have actually
2282480	2288080	used that output, if I'm not mistaken, it was that output, the norm of the output matrix in order
2288080	2296000	to average across the different heads to account for each head's meaning in the attention matrix,
2296560	2303440	in the attention mechanism. But, you know, just, you know, very naively thinking the attention
2304640	2309360	really mixes the tokens using the values determined by the attention matrix. So it's
2309360	2315760	really a naive intuitive outlook on the attention mechanism. And the output matrix that you're
2315760	2323360	referring to is I view it as a weights matrix, which will weight each layer since not all layers
2323360	2327680	influence the prediction the same, right? We know that usually the last attention layer
2327680	2331040	is the most influential or the previous attention layers are not that impactful.
2331920	2338720	So I view it as the output matrix kind of reweighting the result from the attention mechanism.
2339360	2344640	But all that we're saying right now are just intuitions, right? We've seen empirically that the
2344640	2349840	attention matrix is quite indicative of what the model learns to do, how it learns to contextualize
2350720	2355120	parts of the input. It's not necessarily the best thing to do, the smartest thing to do or the
2355120	2360880	most correct thing to do. It's just what empirically worked well. And it has an intuition basis as
2360880	2367120	explained before. I hope that answers your question. It does. I had one other question if there's time.
2369920	2375360	We're really tight on time. I mean, we have 13 minutes. So maybe we'll take that offline.
2376160	2379440	Sure. Thank you. Sorry for that. I really apologize.
2382240	2389360	Okay. So when we talk about VIT models, the image is split into patches. The patches go
2389360	2394240	through linear projections. And then a transformer encoder and vanilla transformer encoder is used
2394240	2398960	to make the prediction again with the classification code. So really a simple and clean architecture.
2400000	2407600	And usually those models are trained using ImageNet. ImageNet is a classification data set.
2408320	2411680	And what those classification data sets do is actually they train the model
2411680	2416560	to make a prediction. So they train the model to see an image and make the prediction that this
2416560	2421680	is a car. But it doesn't do anything beyond that, right? The model should predict that it's a car,
2421680	2426880	but it doesn't have to have an understanding of what a car constructs and how a car looks.
2427600	2433520	It should just see this image of a car and output car. We don't enforce anything too
2433520	2439680	smart that the model should learn. So what researchers have noticed a long time ago
2439680	2444480	is that ImageNet contains sparse predictions. What it means is that, for example,
2444480	2451600	cows usually appear on the background of green grass. So a reasonable inference that the model
2451600	2456240	can make. Really reasonable, right? Because this is the statistics of the data in the data set that
2456240	2462080	it gets. It's to learn that green grass is actually a cow. And now we learn to predict
2462080	2467760	that this image is an image of a cow based on the green grass, not really the object in the image.
2470000	2483360	What it causes is, oh, can you mute this? Thank you. So what it causes is cases where
2483360	2488480	the distribution is likely shifted from ImageNet. And in cases where we would actually expect the
2488480	2494080	model to really work well on. The model really doesn't. And the accuracy plunge, we're talking
2494080	2501040	about 90% to 30% sometimes and even less. So really cases where we would expect the model to still
2501040	2508560	learn to make smart and great prediction, but it really does. It predicts based on the sparse
2508560	2512720	predictions that it learned from ImageNet and they don't apply to other predictions. So for example,
2512720	2517200	we have the golf ball in the lemon here and we have another orange that is classified as a maze
2517200	2522560	due to the carpet in the bathroom, right? Because it kind of looks like a maze. And a school bus
2522560	2527680	here that is classified as a snowplow because of the presence of snow. So we can imagine that the
2527680	2532720	model learns some kind of sparse correlation here, such as vehicle plus snow equals snowplow.
2533600	2543120	Okay. So we want to solve these issues, but without training the models with, you know,
2543120	2548320	a stronger queue, it is really hard to do that because we just teach the model based on some
2548320	2553600	data set that we have, which is ImageNet. It is, you know, the most used data set to predict,
2554160	2562480	to train object detection, object recognition. And we have no way of really controlling
2562480	2568400	what the model learns. And intuitively, training the explainability signal is really teaching
2568400	2573280	the model not just what is in the image, but why this is the object in the image. So we would want
2573280	2579680	to apply a last term directly to the explanations of the model to teach it why this prediction is
2579680	2587520	correct. So here you see some sparse correlations that the model uses. So for example, here the
2587520	2593760	model classifies the images of chestnut with a confidence of 100% based on just the background
2593760	2599760	pixels, not even one photo. And here a very sparse consideration of the zebra gives us a
2599760	2605040	confidence of 99.9% that this is a zebra. So really behavior that we would really want to discourage.
2608080	2613760	Since the second method that we saw is based on pure gradients, everything there is derivable.
2613760	2617280	The gradients can be derived again, and the last term can be applied directly to the
2617280	2623200	explainability. And we can force the model to make the prediction based on the program instead of
2623200	2630960	the background image pixels. The issue that we had after that is, you know, we're researchers at
2630960	2638640	the university, right? We don't have the resources to train VIT large or huge from scratch. So we
2638640	2644960	need to come up with a method that is efficient in time and space and not too complicated. So what
2644960	2650880	we opted to do is fine tune an existing model. So we would fine tune the model. It works pretty
2650880	2655760	well on ImageNet, right? We don't want to change the prediction that it gives on ImageNet. We just
2655760	2661920	want to change the reasoning that it gives to the prediction. So we fine tune the models with only
2661920	2667920	three examples per class, really not that many examples for just 500 classes. So just half the
2667920	2673200	classes in ImageNet to change the relevance maps to focus on the foreground instead of the background.
2675600	2680960	So we identified two science issues with VIT models. The first one is an over interpretation
2680960	2685200	of the background, which we saw on your clients. And the second one is a sparse consideration of
2685200	2693040	the program. The first idea was to fine tune the explanation maps to just be segmentation maps,
2693120	2700080	like this. This is actually an example of me fine tuning a VIT based model to make the relevance
2700080	2706720	maps resemble or be identical to segmentation maps. So as you can see before the explanations weren't
2706720	2712960	really segmentation maps and after they're quite well segmented in the image. So can anyone guess
2712960	2718480	why that's not an optimal solution to the problem that we have just creating segmentation maps?
2719360	2726640	People from Zoom can guess too. Why wouldn't we want the model to output relevance maps that are
2726640	2734560	actually segmentation maps? Let's have a thought experiment today. I'm going to draw with my
2734560	2741120	magnificent drawing skills and objects and you're going to try to identify which animal this is,
2741120	2749280	right? Again, I'm not the best drawing. Which animal is this? Which snake?
2751200	2760480	Nail. Oh, no, this snake. Which snake is this? Cobra. Yeah, why cobra? Because of the head, right?
2761200	2766880	And humans, we don't classify cobra as a cobra because of its tail, right? We look at the head
2766880	2774400	pixels or the head featured and determine that this is a cobra. So we don't, as humans, give
2774400	2779840	identical relevance to all the pixels in the image. What we do here when we fine tune the
2779840	2784560	experiment maps to be segmentation maps, we force the model to look equally at all the pixels of
2784560	2790480	the cobra. We do want to give the model the opportunity to give some relevance to pixels
2790480	2799360	that is higher than other pixels. So this is too harsh. And we need to have a refined version of it.
2801200	2807200	This is why we split the loss into two different losses. One is a background loss and one is
2807200	2813200	foreground loss. The background loss is a mean squared error loss, encouraging the relevance
2813200	2817360	on the background to be close to zero. And we're using segmentation maps here,
2817360	2823120	S is the segmentation map of the image. And the foreground loss encourages the foreground
2823120	2830240	of the image to be closer to one. By splitting into two loss terms, we can give different
2830240	2835520	values or different coefficients to each of the loss terms. So the background loss is going to get
2835520	2840320	a relatively high coefficient too, because we don't want a lot of relevance on the background.
2840320	2845360	By the way, we're not striving to completely eliminate the background, the relevance on the
2845360	2849440	foreground. Just make sure that the relevance of the background is lower than the relevance of the
2849440	2855200	foreground. And the foreground loss is going to get a relatively low coefficient. We would want to
2855200	2860720	encourage the model to look more at more pixels of the foreground, but we wouldn't want to make
2860720	2863520	the model look at all the pixels in the foreground equally.
2866240	2872080	We do also have a classification loss, which ensures that the new prediction by the model
2872160	2876960	or the new distribution is similar to the all distribution by the model. Just to make sure
2876960	2882240	that the model doesn't forget how to classify images. And again, the model does a pretty good
2882240	2886080	job on ImageNet. So we don't want to change the prediction by the model. We just want to change
2886080	2896720	the reasoning. So the giant tables of results here are comparisons between the accuracy of the
2896720	2902880	model before and after a cartooning process. And as you can see here, it's quite tiny, but I hope
2902880	2908640	you can see it still. For the ImageNet validation set, we're experiencing a bit of a decrease in
2908640	2912720	performance. This is because the model relied on spurious cues, and now we're taking them away
2912720	2918080	from it. And so the spurious cues that previously helped the model reach very, very high accuracy
2918080	2924800	and overfit are now taking away. But the decrease in accuracy on average across seven models is
2924800	2930720	not that big. I mean, it's less than 1%. And when you take into account other shifted distributions,
2930720	2940000	such as ImageNet A, ImageNet R, Sketch, ImageNet ObjectNet, and SIScores, you can see that there
2940000	2945840	is a pretty big or significant increase in accuracy. For ImageNet A, for example, plus
2946640	2954320	5.8% in top one accuracy plus 7.8% in top five accuracy. So really a slight decrease
2954320	2958320	in the accuracy on the data set that the model was originally trained on and
2958320	2963040	a significant increase in accuracy for distribution shifts, as we would expect.
2963680	2966640	So to train it, you have to know the program. What is the program?
2967440	2973520	Yeah, you have to know that. You have segmentation maps. You have segmentation maps. And we do
2973520	2979680	experiment with two types of segmentation maps. One is manually human, manually tagged by humans.
2979680	2985680	And the second one is by token cut, which is a version that uses dyno. This is in case you're
2985680	2993040	training with non-ImageNet data sets and you don't want to manually tag. Even if you do manually
2993040	2999280	tag, I mean, we use three examples for half the classes. So it's not that many examples to tell,
2999280	3004160	but we do provide for an option for ad supervised segmentation. Yeah.
3004320	3010320	So I think that's really cool. The one thing about this, why not just do segmentation?
3011200	3014640	You can just train a segmentation system. Is that kind of naturally explainable?
3016640	3021040	That's an excellent question. Do models that were trained on segmentation
3022160	3027200	have that, you know, brief pass on Spurs correlation? Do they get that in her?
3028160	3034000	What we thought about or I thought about in that context is you can think about a model
3034000	3039920	that learns to classify using Spurs correlation and then identify the object using edge detection.
3040880	3046960	So just because you learn to identify an object does not mean or learn to segment an object.
3046960	3052320	Does not mean that you learn to recognize the object by the segmentation. And also we can think
3052320	3057200	about when you want to train really big models, you need a lot of data to do that. And segmentation
3057200	3062640	data is quite expensive. You usually don't have that amount of data as you do for classification,
3062640	3068560	which is an easier task. You have a lot of data just lying around there. So classification is
3068560	3080160	usually the go to task. Yeah, but only just a few. Okay. Yeah. Just 1500 segmentation maps,
3080160	3085200	either supervised or unsupervised. Yeah. A very few amount of segmentation maps.
3100000	3106960	We did experiment with using more segmentation maps and it showed that the accuracy kind of
3106960	3111600	fluctuates at some point. I mean, there's some point where it doesn't improve more if you add
3111600	3117200	more segmentation maps, but you do have to take into account two things. One, we did find two,
3117200	3122000	and we didn't trade for scratch. Two, we didn't have the resources to hyper parameter search for
3122000	3129200	each selection of the number of. So it's possible that if you use more data, you would need to
3129200	3135360	retune your hyper parameters and then get better accuracy improvement, but we didn't have the
3135360	3140640	resources to do that. So it could be the case, but I don't really know. Yeah, I don't really have
3140640	3149440	any finance for that. One thing we did to ensure that the model actually learns to predict better
3149440	3155520	or to have better explanations is we looked at the accuracy increase for the non-training classes
3155520	3161040	as well, because we said that we only use half the initial classes. It is really interesting to see
3161040	3168080	if the model really improves on the non-training data as well. Does it learn to generalize the
3168080	3173840	positive influence or the positive logic? And as you can see here, this is the ImageNet
3175120	3180240	validation set. So yeah, there's a decrease as we saw before, but for the non-ImageNet
3180240	3185120	distributions, for the shift of distribution, you can see that the improvement for the non-training
3185120	3190240	classes is actually quite similar and sometimes even better than that of the training classes.
3190240	3196880	So the model really from this experiment learns to generalize that healthy say-and-behavior to
3196880	3204720	classes that were not in the training set. And here are some visual examples. These are
3204720	3210320	examples from the ImageNet data set. So examples from the original data set of the model straight
3210320	3215760	map. And here you can see that the same prediction is made for two different reasons. Here, the
3215760	3221200	background, here actually the foreground, the snowplow. And here you can see corrective predictions
3221200	3227200	where the model originally predicted that this is a can opener based on the eye of the puppet.
3227760	3233760	And once we find you the model to look at the entire object or to look for, you know, less
3233760	3238560	sparsely as the object, it actually talks about the teddy bear. And here you can see that even
3238560	3245360	if the model is now wrong and was previously correct, you can usually quite easily explain
3245360	3250800	why the model was wrong. So here's an example where the ground truth classification is tripod
3250800	3254880	and the model predicted actually fine tuning a strawberry, but you can actually see that there
3254880	3259360	exists a strawberry in the image. So it kind of makes sense that the model made that mistake.
3261760	3266560	These are examples for shifted distributions. So as you can see before, for this example,
3266560	3272800	the model predicted a garbage truck. Well, this is the forklift because the forklift is in a garbage
3272800	3279280	area. So we correct the prediction to be a forklift based on foreground rather than the background.
3279280	3283520	Here you can see a teddy bear that was classified as a ping-pong ball due to the sparse consideration
3283520	3289520	of just its spot. And after the fine tuning, it is correctly classified. And the third example is a
3289520	3294720	porcupine that was classified as a sea lion due to the background of the ocean. So once the model
3294720	3301120	really learns to look at the correct pixels, it does make the correct prediction.
3302880	3308080	These are additional examples, but really, we don't have time. And another interesting thing
3308080	3312880	that we've noticed that I think is quite cool, even when you take examples that are completely
3312880	3319280	out of distribution, I mean, this is an image generated by Dalit. And the models not know
3319280	3325600	the class robot or oil painting and so on. Originally, it made a ludicrous prediction
3325600	3332640	that this is a guillotine based on, I don't know what, you can't really understand. But after a
3332640	3337120	fine tuning process, you can see that the model does not make maybe the best prediction that you
3337120	3341920	can think of, which is the robot because it doesn't know that class. But it does predict the grand piano
3341920	3347040	and it kind of makes sense because there is a piano in the image. So while the prediction,
3347760	3353680	again, still does not make the most sense. At least it is based on some of the objects inside
3353680	3358880	the image and not just something that you cannot make sense of due to sparse correlation.
3360560	3367120	So this was the entire talk. Yeah, we got through it in time. Thank you very much. And the table
3367120	3372000	of content is here in case you want to ask a question about specific parts of the lecture. Thank you.
3377040	3384640	Yeah, one or two we can do. Yeah.
3392240	3394960	Yeah, it's visible. Thanks for rotating it.
3401680	3404800	Okay, the questions here are really lacking context because they were probably
3405200	3410000	asked during that. So if anyone wants to ask a question again. Yeah.
3412720	3417840	Yeah, I guess one question I had. Have you thought about including layer norm at all into your
3421360	3427120	explanations? Because it seems that that does scale tokens in some way and could that be relevant
3427120	3433040	for your output? Include what? Sorry, can you repeat it? Layer norm? Layer norm? Oh,
3434800	3441760	no, but as I said, there is a method that I don't quite remember the name of the method that
3441760	3448800	did take into account the norms of the output matrix, I think, in order to average across the
3448800	3455760	different attention heads. But we haven't considered that. Yeah. We do consider that the gradients
3455760	3461520	should be able to scale the different attention heads according to their influence on the prediction.
3466880	3468560	Any other questions? Any other questions?
3471440	3474800	I had a question if no one's going. Oh, yeah, go ahead.
3477360	3481760	Wait, me or someone in the room? Sorry. Yeah. Okay, thank you.
3482720	3489600	Okay. So I was wondering with regards to the stuff you said at the end, where some of them
3490160	3494960	you see it and then you're like, okay, that was wrong, but like, makes sense. That's
3498000	3506400	is there a way to quantify that and were related things or is it more like a you know it when
3506400	3512640	you see it? Oh, yeah, that's a great question. There is a work done by Google, I think, that actually
3514000	3521600	relaxes the task of classifying objects using ImageNet. They actually re-tagged ImageNet,
3521600	3526880	where, you know, a strawberry in that case wouldn't be a mistake, but maybe it would be
3526880	3533600	half a mistake or something like that. Yeah, so there's such a work that re-tags the entire ImageNet
3533600	3539040	dataset to account for mistakes that aren't really mistakes, but actually makes sense.
3540160	3547440	But other than that, I would say there isn't an automatic way to know that. I mean, I can't think
3547440	3551920	off the top of my head of an automatic way to know when the model is mistaken, but it's okay.
3553360	3559440	Cool. How did you guys check? Like, was it mainly the accuracy increase on the distribution
3559440	3567120	shifted versions? Yeah, yeah, it was mainly, yeah, it was mainly the accuracy on the distribution
3567120	3575680	shifts. And, yeah, yeah, and also looking at a lot of examples, right? Because I started out
3575680	3584800	with many. Yeah, yeah, yeah, and a lot of manual work on actualizing examples that got me to the
3584800	3588720	intuition that I'm presenting now, because I actually thought in the beginning that having
3588720	3595600	the relevance be a segmentation map is quite logical. Yeah, so it takes some time to get
3595600	3602000	through all the conclusions. Yeah, yeah. I was just having one idea coming out. Is that possible?
3602000	3609760	So you are going to be this key time. So that in a strawberry case, you can crop out that region
3609760	3615200	and then maybe run through another like Oracle network to tell you whether it is a strawberry or
3615280	3620320	not. And that gives you some kind of a semi-automatic way. Yeah, yeah, definitely. That's an
3620320	3625600	interesting take. It's interesting, particularly because I saw that different models tend to
3625600	3630800	learn different spurious correlations. So it actually makes sense to check models using other
3630800	3635840	models. Yeah, they're consistently making the same prediction with these vets. Yeah, yeah, perhaps.
3635840	3641760	Yeah, yeah, that's an interesting idea. Your current relevancy extractor approach
3642720	3650480	is limited by the VITs tile resolution. It outputs the attention map that is the size of the
3650480	3658480	tiles and then you can bi-level it. Upscaling, yeah. I was wondering whether there's a way to bypass
3658480	3665520	this tile resolution just by considering that we also have pixels coming into the tile.
3666080	3673440	Yeah, yeah, we have tried to propagate relevance all the way back to the actual input and not on
3673440	3679920	the level of each patch. It didn't come out just quite as we hoped. I think that the issue there
3679920	3685040	is probably the positional encoding in the way. Somehow that layer of positional encoding
3686320	3692800	ruins or destroys the relevance values once you propagate back from it. I couldn't figure out how
3692800	3702080	to get past that layer that actually kind of added noise to the output relevance map.
3702560	3719840	That's an interesting point, but yeah. Yeah, I haven't come across any such architectures
3720400	3727040	if you do let me know and I can give you the try. No, there was a question here, right?
3727360	3732480	I was just wondering, when you're about to work for, what if I wanted to explain about a color
3732480	3736000	or something, or non-message, I'd like to just go to something about discussion, right? Maybe
3736640	3741200	then perhaps lemon versus orange, you can color, kind of the main thing. That's just curious.
3744720	3749760	This specific method would not be able to do that, but I know that there are explainability
3749760	3756960	methods that kind of create a decision tree from the model. So you pay the price,
3757120	3762480	that the accuracy decreases to some extent, and then you create a decision tree based on the
3762480	3767600	decisions of the model. You kind of model the model using a decision tree, and then you may
3767600	3775120	have a split that it has to do with, you pass a lot of images through a lot of images of oranges
3775120	3780800	and lemons, and you see that one of the splits is by the color. Yeah, and then you know that.
3781200	3788960	And probably you can do some trivial things to test specific theories, like turn the image
3788960	3794160	into black and white and see what happens, to consider if the model takes into account,
3794800	3797360	but this method will not be able to do that.
3800000	3801520	All right, let's hand this speaker.
