{"text": " Awesome, so we're so excited to have Hila Chepur, am I pronouncing it right? Yeah, it's actually in Hebrew, it's Chepur. Oh, it's Chepur. Yeah. All right. So Hila is a PhD candidate at Tel Aviv University, advised by Professor Leon Wolfe. Her research focuses on developing reliable XAI algorithms and leveraging them to promote model accuracy and fairness. Today she's going to talk to us about transform or explainability, and we're so excited to hear from you. Oh, thank you for that great introduction, and I actually have some slides here. I think you can just... Oh, yeah. Slides here introducing myself, but I think you did that perfectly, so I'll just skip that. So maybe the first thing we want to talk about is motivation. We're here to talk about transform or explainability, but why should you care? And let's just have a disclaimer, because we all know that explainability is really important for aspects like accountability, reliability, and so on. But when we write research papers, we usually focus on other measuring sticks, such as accuracy and robustness, right? So I'm here to convince you that the explainability queue is actually really useful for those measuring sticks as well, and that you should consider using explainability in your research, even if it's unrelated to accountability, reliability, and so on. So to do that, I have a few examples showing how explainability can be used to improve model accuracy and robustness. The first example is text to live. Maybe you know it. It got accepted to ECCV 2022. And the objective of the model is to take a target edit text, for example, here around hat, and apply the edit to the image. So what they try to do is unlike other works, they try to prevent the part where the user actually has to insert a manual segmentation mask to indicate where the hat is. So they wanted some automatic way of getting to the region of interest, which is the hat. So they use relevancy maps of the clip model, which I guess you're familiar with. So the clip model with the relevancy map actually indicated to their downstream path, where pretty much the hat is in the image. And then their model refined those relevancy maps and applied the edit according to the location indicated by the relevancy map. So here you can see what happens without this. They call it bootstrapping without using the relevancy map. So you can see that there are additional artifacts, such as the faces turning red and not just the hat. And when you use bootstrapping, when you use relevancy maps, then the edit is quite localized to the hat. And here you can see other edits that are quite nice. They take a sponge cake and they turn it into an ice cake or a spinach mousse cake. So I think it's a nice example to show. Another example is a paper called Kripaso. It's that best paper awarded Seagraph 2022. It also uses relevancy maps. The goal of the model is to take an input image and create sketches with different levels of abstraction. So you can choose which level of abstraction you want. So for example, the flamingo here, very abstract painting of the flamingo or very detailed painting. And what they did is they actually used the relevancy maps as an initializer for the model to understand where the object is and how to create the stroke's product. Another example is Sameer's work, which you're probably familiar with. What you did here, really, is you used the relevancy maps in order to locate objects which aren't necessarily objects in the wild, objects that appear in living rooms and that do not necessarily appear in the training set of some models of desegmentation or localization. So you can use Kripaso in order to identify objects that are really not really objects that are so common in training sets. So if you consider all the examples that we've seen before, they have one thing in common. They use the relevancy maps as a fixed signal. They didn't train on the relevancy map or create a loss with the relevancy map. They use it as an initialization for the answering task. But what we did in our last work is actually just, we showed that its reliability can be used as a loss term in order to improve models. So if you think about what it means to create a loss term based on explainability maps, it's really meant to teach the model how to do something or why it does something, not just how to do something. And we talk about classification models. They tend to learn spurious cues to help them make shortcuts to make a prediction. So for example, a model can learn a spurious cue that if you have a round object with the background of a grass, then it's a golf ball. And here you can see that the model classified this lemon as a golf ball because of the background of the grass. When you force the model to actually focus on the foreground of the image and not just the background of the image, via a loss applied directly to the explainability maps, you can correct wrong predictions based on spurious cues. So what we're doing usually is we're teaching the model to predict something, right? Predict golf ball, car, glasses, etc. But we're not really teaching it why. Why this is the object in the image. So what we're showing here is that by fine-tuning directly the relevant slots or the explainability maps, we can correct wrong predictions based on spurious conditions. But we'll get to it in depth later on. So this was the motivation part, and hopefully you got fully motivated as to why Transformer experiment is interesting. Our talk is going to be a construed of two parts. The first part is going to be, we're going to talk about how we do Transformer explainability. We're going to see the attention mechanism which I'm sure you're all familiar with, but we're going to have emphasis on specific parts of the attention mechanism that are going to be useful. Then we're going to ask ourselves, is attention an explanation? Which is really the most prominent question when doing Transformer explainability. We're going to talk about three explainability algorithms. The first one is attention roll-up, not by me. But it is the groundbreaking first algorithm in the big Transformer explainability. Then I'm going to present two of my works that have to do with Transformer explainability. And in the last part, we're going to talk about the work that I just presented and that Shiran had a question on. And probably hopefully we'll answer the questions and see how Transformer explainability can be used to devise models or maybe incorrect spray-excused at the models where. So just to set us up, this is a Transformer architecture as presented in attention is all you need. I'm sure you're all familiar with it. We will be focusing on the encoder since the attention mechanism here is a self-attention mechanism. And it's quite more intuitive and easy to grasp and understand. And the concepts that apply here to the encoder are pretty easily generalized to the encoder. So we're going to focus on the encoder for simplicity and for intuitive explanations, but the principle is quite easily generalized to the encoder as well. By the way, if you have questions, just feel free to stop me. Okay, so let's talk about the intuition behind self-attention. What self-attention does is actually creates contextualized representations. So I'm sorry for the people on Zoom, but I'm going to write here on the right for it. So we have an example. We're running an example to work with. Say we have the sentence, the count, set on the max. And let's consider the first day in the sentence. We want to create now an embedding for each one of the words in the sentence, for each one of the token incidents. But the token does quite meaningless with our context, right? It could refer to anywhere. So what the attention mechanism does is it actually creates contextualized representation. It should take information from the other token and insert it to the current token that we're interested in. So for example, intuitively, maybe we would expect that since the word the refers to the word cat, information from the word cat will be moved into the word the, such that the embedding of the word that is contextualized is enriched by context from the word cat. So what the attention mechanism does is it actually uses query, key, and value matrices. And we can think about it maybe as a database theory information. So when we talk about databases, we have queries, which are questions that we run on our database. We have keys. The keys represent the entries in our database. And we have values that corresponds to the keys, right? So what we're doing here is we're actually running a query asking which tokens are relevant to the... We can think about it intuitively as running a query on all of these tokens that we have. And then the keys represent all the other tokens. What we do with the attention mechanism is we calculate an attention matrix that is going to be the star of every transformer experience ability algorithm. It's going to be a soft mass of the multiplication between queries and keys normalized by the embedding dimension. This similarity scores are actually telling us how much each word is relevant to our word of interest. So the multiplication between queries and keys, we can think about it kind of like as relevant scores. How much is each token relevant to the token that... to the word that... And after we calculate those similarity scores, we create the enriched representation by multiplying the scores by the values. Such that each word gets information from the other words by these relevance values. So these relevance values determine how much each word is going to influence the word of after the attention. So this is going to be the key intuition to everything that we do later on to explain a transformers. The most important thing to remember about explaining transformers is we don't have just a single attention matrix. This mechanism happens H times, where H is the number of attention heads that we have. And intuitively, we can think about it as, you know, in CNNs, you have kernels. Each kernel has its own purpose. Some refer to the background, some refer to the edges, the shapes, and so on. Transformers have the same thing with attention heads. So each attention head can have a different purpose. And actually, researchers have shown that you can probably prune most of the attention head and achieve the same accuracy, which means that most attention heads are really not important to the prediction, to a specific prediction of the model. So it's really important when we think about transformers and to understand that the different heads have different needs. The final thing that we need to remember about transformer, you know, predictions is that transformers use a classification token for the prediction. So once the entire attention mechanism is done and all the tokens are contextualized, the classification token is the only token. That is used to make the prediction. There's a linear layer on top of the classification token and then the prediction is made. So basically what the classification token does is kind of like creates an aggregated representation of the entire input. You can think about it as a global representation of all the tokens in the input. You have questions so far, because we're going to move on to the interesting stuff. Yeah. So moving on to transformer explainability, it's really important to set up our goals. My goal is to facilitate explanations that help you guys, the researchers that actually use the models. And the way that we do that is by creating hitmaps. So the hitmaps should correspond to the pixels, if we're speaking of images or if we're speaking of text, and hitmaps should correspond to the tokens. The hitmaps should correspond to the pixels that influence the prediction by the model. So for example, here we see the verb and the hitmaps actually highlights the pixels relating to the verb. And the toothbrush or the ship or the bikes and so on. So the hitmaps should tell us which pixels in the input make the prediction as it is. Okay, we got to the interesting part. Yeah. When you talk about transformer explainability, researchers have looked at this attention matrix and asked the question, is this attention matrix an explanation? How can it be an explanation? Because we said that the attention values are actually kind of like relevance values, right? There are values that reflect how much each token influences each other's token. And we also said that the classification token is the only token that is used for the prediction, right? So if we look at the row in the attention matrix, that corresponds to the classification token, and look at these relevance values, these should be the relevance values that determine how much each token influences the classification token, which is basically how much each token influences the classification, right? So maybe these values are just the relevance values. Each token represents a patch in the image. Maybe these are just the values that we need. And we're all done just like decision trees are self-explanable or linear regression is self-explanable. What do you think? Are we done? We're done. Yeah, we're done. We're done. Yeah, we're done. We're done. We're done. We're done. We're done. Yeah. Yeah. The attention matrix is used to multiply the value representation. Yeah. The representation should be positive, negative, large, small. It doesn't actually tell us how much it is actually contributing to the final classification. Yeah. I mean, the two problems that we, we point out to. The values can't be negative, but I don't think really when you say, okay, let's refer to. These values are actually directly determining how much information from each token you're going to take. And think there's a softmax operation here. All the values are non-negative, right? So there is a distribution that's defined on all these tokens of how much each token is. So intuitively, these are really relevant values, but we do have two other issues that we should refer to. The first one is we said we have a few attention heads, right? Each tension head has a different meaning. Some attention heads are really not relevant to the prediction. How do we aggregate across these attention heads in a way that takes into account the meaning of each head? We wouldn't want to take into account heads that do not affect the final prediction of the model. And there are such heads since there's research that show that you can prune most of the heads without impacting the prediction of the model. So you have a few attention heads and it isn't, isn't clear how you aggregate across these attention heads in a way that takes into account the importance of each head. And the second question that we have is we refer to the single attention layer, but we have a few attention layers. So the first attention layer may incorporate information into token one from token three. And then in the second layer, token one isn't simply the patch that it represented in the beginning. It is this patch with information from this patch. In the second layer, it's this patch with information from this patch, and maybe this patch, and maybe this patch. And by the end of the attention mechanism, how do we know which token refers to which input patch, right? They're all mixed up. That's the entire idea of the attention. So we have two issues here. How do we aggregate across attention heads? Since we know that they have different means and how do we aggregate across attention layers? Yeah. Just so that I understand. So if there's only one attention head, and also there's only one attention layer, then the relevance board is the attention. Yeah. Yeah. By this hypothesis, yes. Okay. Yes. And then I think there are some models that use this for visual question answering and actually did that visualization. And it works. Pretty well. So assuming you have one attention head and one attention layer, it should be. Fingers crossed. It should be their elements. I haven't tried that, but, but yeah. By this intuition. So the attention all of mechanism is actually the first method to explain transfer was that came out. In 2020. And they add, they propose that the two simplest solutions, maybe that we can think of to solve those three issues. Head aggregation by averaging. And again, remember we said different meanings to different heads. So that's maybe over simplistic. And they're aggregation by matrix multiplication. And if you think about it, matrix multiplication from the end to the beginning, kind of unravels things. The connections between the two that were made by the attention mechanism. They also propose another method called attention flow, which evaluates the flow values in the attention graph, like a classic flow problem from algorithms, but it's too computationally expensive for images. So we're not really going to get into it. So getting into the first method we propose, what we were saying is that the assumptions made by the attention role of mechanism were solid, but maybe over simplistic. Yeah. Question. You may have some questions. Oh, yeah. We say in Hebrew. Oh, right. I may take those at the end of the talk just because otherwise we won't be able to finish with time. Yeah. Yeah. So getting back to the first time we're going to propose. We were saying that the assumptions made by attention roll off were nice and worked in some cases, but are maybe a bit simplistic. We want to be able to average across the heads in a way that actually takes into account the meaning of each other. So what we're going to do is we're going to use a signal that is very useful in explainability in general, which is radiance, right? Because radiance intuitively mean if I change this a bit, how does this change a bit, right? So if we take the gradients with regards to the output of the model, which is over here, the gradients of the attention matter. We can use the gradients as weights for the attention maps. So instead of just averaging across the maps, we take the gradient, the gradient gives us the weights element. And we multiply the gradients by the attention and then each head gets a weight from the gradient. And then each attention head is not just the simple attention head that it was in the beginning. It is the attention weighted by the gradient. And then we can average across the heads in a way that takes into account the meaning of each head. So this is why the gradients are here. But we have another component that I won't get too deeply into because it was removed for our second method. It is the LRP component, layer-wise relevance propagation. The second thing we thought of was that we actually reduce the entire transformer architecture to just a multiplication of queries and keys. So it's not even the entire attention mechanism because we also had the values there, remember? So we narrowed down this entire, not so complex, but architecture, right? It has activations. It has linear projections. We narrowed all down to the multiplication between queries and keys. So we do want to take into account the other layers of the transformer and how they impact the calculations. So instead of just taking, let's get back to the whiteboard here, instead of just taking the attention map, that is quite, it's simplistic, right? It's not that, but the multiplication between queries and keys, we want to take into account a different attention map, we'll call it RA, which takes into account the other layers of the transformer architecture. So instead of taking just these raw relevance values, we take relevance values calculated by LRP. And LRP is a mechanism that does back propagation with gradients from the end of the network all the way to the beginning. And it can give us relevant values for specifically this attention matrix. So instead of taking into account the attention values, the raw attention values, we take into account the relevance values of the attention matrix. And as I said, I won't get too deep into it because we actually removed it in our second method, which is the one that I want to get into in more details. So we have the attention gradients to average across the heads. And we have the relevance in order to account for all the other layers in the transformer. So this is how we average across the heads. And the way that we average across the layers is by matrix multiplication. Here we adopted the interpretation from attention robot. Oh, no, not element wise, actual matrix multiplication. The matrices are squared matrices. Yeah, because they are self attention matrices so you can actually multiply them. And if you think about it, you can unravel it when multiplying two attention matrices. It actually says, if the previous layer gave token one information from token three, and this layer gives token one information from token four, then it unravels both operations to ensure that you actually take into account all the context. Yeah, so this is just a rewind of what we saw in the previous slide. How do we average across heads? We take the gradients as weights. We take the relevance instead of the pure attention weights. And then we do averaging. But here the average is not just the raw average. If we had before it is weighted by the gradients. And here you can see a few examples of how our method works. So by the way, this is a slide that was added, but we don't have the updated slide. So let's just see what we get in the end of this calculation. So at the end of this calculation, we had an attention matrix, which is the attention matrix after by the averaging and everything. We have attention matrices for all the layers. And then we multiply these. So really, we have one attention matrix that takes into account all the layers and all the heads. And now we can get back to, can we go back in the slides? Oh, no, it's only going forward. Yeah. Maybe there's an arrow at the bottom left corner. If you move your mouse. Oh, the back arrow. Oh, it's the other way around. Yeah. And now we're actually getting to the point that Sharon made that right now we only after all the aggregations that we made, we have one aggregated attention matrix for the entire network because we aggregate across heads and then we aggregate across layers. And once we have that one attention matrix for the entire, for the entire network, then we can use that intuition that we had that the row corresponding to the classification token is actually the explanation. So this is how we extract the final explanation. And then we're actually the relevance values that we use. And the one the other way around, right. Okay. So as you can see here, we have comparisons between our method and other methods that are either adapted from CNNs, or methods that were constructed for transformers such as rollout. So as you can see here, rollout tends to have a lot of noise in the background and we think about it intuitively as resulting from the fact that they just average across the heads and not take into account the meaning of each head. And some methods such as partial LRP fail on some cases, but in these specific cases, they actually do pretty well. But I do want to point out that they do not distinct between classes. So for example, if we have an elephant and a zebra in an image, our method is able to produce explanations specifically for the elephant or specifically for the zebra. And when we don't do that, and we want to use this method say partial LRP to explain predictions by the model, it will be hard to do that because if you want to explain the elephant prediction, we may have results coming in from other classes. So we're not really sure if the things that we're seeing highlighted are highlighted because of the elephant or because of other classes making their way into the explanation. So I think personally, class specific explanations are really important to ensure that we're really explaining the specific prediction of the model. We're good. That's a fantastic question. That's a fantastic question. Usually people from explainability evaluate explanations differently than what you as end users may have to be. So what we do is we use erasure based methods. So what we do is we take the pixels that are said to be important to buy with it. We take them out and we see if the model changes its prediction or not. And similarly, we do the other way around. We take the pixels that are unimportant by benefit and take them out and see that the model still predicts the same. You have to take into account when you can see that the model still predicts the same. You have to take into account when you do that, that you create images that are out of the distribution of the model was trained on. So this method is not really, you know, airtight. And there's a lot of research around how do we value it. And how do we know if the explanation is really good or not. Any other questions. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Um, just because we want to have a measuring stick that actually measures the explainability without relation to the algorithm itself. So the measure should be unrelated to whether it's a transformer or CNN. It should be unified throughout all the different architectures, right? Just as you use accuracy to measure CNNs or transformers or whatever architecture you use. You want to have a measuring stick that really measures the method and not something that has something to do with specifically. If you have an explanation for CNN, it also has, you know, values for each pixel. Yeah. Yeah. Just zero it out. You just zero it out. And it works on the input itself. So it really, it is undependent even of. You know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, it really it is independent even of the method you use or the model. It is a measuring stick that has nothing to do with which method you use for expansion or expansion in which version. the sun. I'm not sure I got your question exactly, but I would say that there are methods evaluating explanations by adding sparse correlation, making sure that the model reaches 100 percent accuracy due to the sparse correlations, and then making sure that the explanation outputs these sparse correlations versus the odd correlation. So there are methods that do that, but yeah. But we usually, I usually use erasers as methods to evaluate explanations, but this is a really active goal of research, right? So it's not really, you know, obvious how we evaluate explanations and what's the right way to do that. I think I'm maybe moving backwards instead of forward. Some technical issues. Yeah, okay. I may just skip this because we do have the motivation that we did in the beginning and we're a bit behind on time. So our second method said, you know what? We really believe that multimodal models are going to be a big thing. And we only explained self-attention before, as you saw. We didn't go into cross-attention or encoded encoded attention. And assuming that most transformers don't just do right self-attention, we need a mechanism that can explain cross-attention and encoded encoded attention as well, not just self-attention. So the second paper actually expands the first paper, but for other types of attention. So the first thing we do is get rid of the LLP, and that's why I don't, you know, get into a lot of detail with regards to the LLP. The reason that we did that is because if you think about it, we use LLP in order to account for all the layers, but really gradients account for all the layers because backpropagation is backpropagated from the output all the way back together. So we said, what happens if we remove LLP, which makes it easier for you guys to implement the algorithm, and it makes it faster and more convenient, and it actually works pretty well. So we remove the LLP component. I will say that if you want really accurate explanations, usually I would go for the LLP version, right? Because LLP adds this added component that doesn't exist without LLP. It does account for all the layers quite systematically. So when we talk about cross-model interactions, we have four types of interactions in such models. We have the self-attention interactions between the text tokens, how the text tokens influence themselves, the self-attention interactions between the image tokens, and then two types of cross-attention interactions, how text influences image and how image influences. And then what we thought we would do is really track the self-attention layers. So each self-attention layer mixes tokens. Okay, we'll mix the tokens in the relevance matrices. So we start with an initialization of the relevance matrices at the beginning of the modalities or self-contained. So images only affect images and text only affects text, and each image token only influences itself. So the initialization for the self-attention relations are just the identity matrices. And for the cross-model relations, it's a zero matrix, because there are no cross-model interactions before we do any attention. And what the method really does is it just goes on a forward pass through the attention layers, and as the attention layers mix the tokens, the relevance values are mixed as well, just tracking the attention as it goes. So I won't get into all the rules, all the rules that we have for all these specific attention layers. I'm just giving you a motivation of how it works. And really, believe me, it's really simple, even though the equations look complicated. So let's go over just the self-attention rule. A self-attention layer has, again, multiple heads. We average across the heads using gradients just as before. So we have now a single attention matrix marked here as a bar. And what we do again is just matrix multiplication between the current attention mixture and the old attention mixture that existed in the relevance matrix. So matrix multiplication and update the relevance matrix. This is all we do. We just track the attention as it goes. As it mixes between tokens, we mix between the relevance values. That's what we do. That's the entire algorithm. And head aggregation is done via gradients as before. So taking a look at some examples that we have to demonstrate how this works. For example, for CLIP, you can see that we've entered different texts with the same input image and propagated gradients. And by the way, for CLIP, gradients are propagated. Let's take them back to the whiteboard. For CLIP, because I know this is specifically interesting to you, let's talk about how we propagate relevance for CLIP. For CLIP, you have an imaging quarter and then a texting quarter. Both of them, by the way, use pure self-attention. So there's no cross connection. This and this output representation and vector, which is by the way from the classification. So this is the vector for the text and this is the vector for the image. And the stimuli score is just a dot product. Both scores. So what we do is we propagate gradients from this dot product back to the texting quarter and back to the imaging quarter. And those gradients are going to be used to average across the attention pens as we saw before. And then the attention heads are going to be aggregated across different letters by matrix multiplication. So here we don't have an output logic as we have for the classification, but we use this dot product between their presentations to calculate the score that we propagate the gradients with regards to. So all that we do here is really simple. Calculate the dot product between their presentations, propagate gradients with regards to the dot product. Those gradients are going to be used as weights for the attention matrices to average across them. And as you can see, the results are text specific since we propagated the gradients with regards to the specific multiplication between the specific text and the specific image. So actually for an elephant, you can see that the hidden map corresponds to the elephant. For a zebra, the hidden map corresponds to the zebra. And for a leg, the hidden map corresponds to the leg, showing us that the model really knows how to distinct between different parts or different objects in the image according to the text input that we give it. This is an example that we saw before. And visual question answering in case any one of you is interested is actually an interesting use case. Because for visual question answering, the model is given an image and a question, and it's supposed to answer the question based on the image. And researchers have shown that when you actually lack out the entire image and just give the model the question, it answers the question about 30% of design correctly. So the question here is assuming that the model answers the question without seeing the image. How do we measure the accuracy of such models? So you can use explainability to ensure that the model actually used the image and the correct parts of the image to make the prediction. For example here, the question is, did he catch a ball? We see that the player actually caught the ball. And the answer is yes, but we also see that the model focused on the right parts of the image. So it can really tell that the model made the prediction based on the image and not just the question. I'm going to skip this part too. Yay. So we're switching gears. We're going to talk about our method to improve model robustness using explainability. So if you have any questions about the previous part on explaining transformers, this is the time to ask them. No, no questions. Oh, I had a couple of questions in the chat. Yeah. I'm sorry about that. There is no one really, you know, maintaining the chat. Yeah, let's make it brief and then try to answer questions. Yeah. Oh, okay. I was just wondering, why does it make sense to only look at the attention maps outputted by the softmax? Because don't we have, don't we multiply by an output matrix then that is able to shuffle across tokens afterwards? Do you mean the values matrix? No, the output matrix. I guess that the intuition is just that the self attention mechanism, its purpose is to contextualize in the way that the contextualization is made by the attention values. So the attention value is actually, you know, determine how much each token is going to be incorporated into the other tokens. We do have an additional output matrix and you mean after the attention mechanism, right? Yes, yes. Yeah, okay. So some researchers have actually used that output, if I'm not mistaken, it was that output, the norm of the output matrix in order to average across the different heads to account for each head's meaning in the attention matrix, in the attention mechanism. But, you know, just, you know, very naively thinking the attention really mixes the tokens using the values determined by the attention matrix. So it's really a naive intuitive outlook on the attention mechanism. And the output matrix that you're referring to is I view it as a weights matrix, which will weight each layer since not all layers influence the prediction the same, right? We know that usually the last attention layer is the most influential or the previous attention layers are not that impactful. So I view it as the output matrix kind of reweighting the result from the attention mechanism. But all that we're saying right now are just intuitions, right? We've seen empirically that the attention matrix is quite indicative of what the model learns to do, how it learns to contextualize parts of the input. It's not necessarily the best thing to do, the smartest thing to do or the most correct thing to do. It's just what empirically worked well. And it has an intuition basis as explained before. I hope that answers your question. It does. I had one other question if there's time. We're really tight on time. I mean, we have 13 minutes. So maybe we'll take that offline. Sure. Thank you. Sorry for that. I really apologize. Okay. So when we talk about VIT models, the image is split into patches. The patches go through linear projections. And then a transformer encoder and vanilla transformer encoder is used to make the prediction again with the classification code. So really a simple and clean architecture. And usually those models are trained using ImageNet. ImageNet is a classification data set. And what those classification data sets do is actually they train the model to make a prediction. So they train the model to see an image and make the prediction that this is a car. But it doesn't do anything beyond that, right? The model should predict that it's a car, but it doesn't have to have an understanding of what a car constructs and how a car looks. It should just see this image of a car and output car. We don't enforce anything too smart that the model should learn. So what researchers have noticed a long time ago is that ImageNet contains sparse predictions. What it means is that, for example, cows usually appear on the background of green grass. So a reasonable inference that the model can make. Really reasonable, right? Because this is the statistics of the data in the data set that it gets. It's to learn that green grass is actually a cow. And now we learn to predict that this image is an image of a cow based on the green grass, not really the object in the image. What it causes is, oh, can you mute this? Thank you. So what it causes is cases where the distribution is likely shifted from ImageNet. And in cases where we would actually expect the model to really work well on. The model really doesn't. And the accuracy plunge, we're talking about 90% to 30% sometimes and even less. So really cases where we would expect the model to still learn to make smart and great prediction, but it really does. It predicts based on the sparse predictions that it learned from ImageNet and they don't apply to other predictions. So for example, we have the golf ball in the lemon here and we have another orange that is classified as a maze due to the carpet in the bathroom, right? Because it kind of looks like a maze. And a school bus here that is classified as a snowplow because of the presence of snow. So we can imagine that the model learns some kind of sparse correlation here, such as vehicle plus snow equals snowplow. Okay. So we want to solve these issues, but without training the models with, you know, a stronger queue, it is really hard to do that because we just teach the model based on some data set that we have, which is ImageNet. It is, you know, the most used data set to predict, to train object detection, object recognition. And we have no way of really controlling what the model learns. And intuitively, training the explainability signal is really teaching the model not just what is in the image, but why this is the object in the image. So we would want to apply a last term directly to the explanations of the model to teach it why this prediction is correct. So here you see some sparse correlations that the model uses. So for example, here the model classifies the images of chestnut with a confidence of 100% based on just the background pixels, not even one photo. And here a very sparse consideration of the zebra gives us a confidence of 99.9% that this is a zebra. So really behavior that we would really want to discourage. Since the second method that we saw is based on pure gradients, everything there is derivable. The gradients can be derived again, and the last term can be applied directly to the explainability. And we can force the model to make the prediction based on the program instead of the background image pixels. The issue that we had after that is, you know, we're researchers at the university, right? We don't have the resources to train VIT large or huge from scratch. So we need to come up with a method that is efficient in time and space and not too complicated. So what we opted to do is fine tune an existing model. So we would fine tune the model. It works pretty well on ImageNet, right? We don't want to change the prediction that it gives on ImageNet. We just want to change the reasoning that it gives to the prediction. So we fine tune the models with only three examples per class, really not that many examples for just 500 classes. So just half the classes in ImageNet to change the relevance maps to focus on the foreground instead of the background. So we identified two science issues with VIT models. The first one is an over interpretation of the background, which we saw on your clients. And the second one is a sparse consideration of the program. The first idea was to fine tune the explanation maps to just be segmentation maps, like this. This is actually an example of me fine tuning a VIT based model to make the relevance maps resemble or be identical to segmentation maps. So as you can see before the explanations weren't really segmentation maps and after they're quite well segmented in the image. So can anyone guess why that's not an optimal solution to the problem that we have just creating segmentation maps? People from Zoom can guess too. Why wouldn't we want the model to output relevance maps that are actually segmentation maps? Let's have a thought experiment today. I'm going to draw with my magnificent drawing skills and objects and you're going to try to identify which animal this is, right? Again, I'm not the best drawing. Which animal is this? Which snake? Nail. Oh, no, this snake. Which snake is this? Cobra. Yeah, why cobra? Because of the head, right? And humans, we don't classify cobra as a cobra because of its tail, right? We look at the head pixels or the head featured and determine that this is a cobra. So we don't, as humans, give identical relevance to all the pixels in the image. What we do here when we fine tune the experiment maps to be segmentation maps, we force the model to look equally at all the pixels of the cobra. We do want to give the model the opportunity to give some relevance to pixels that is higher than other pixels. So this is too harsh. And we need to have a refined version of it. This is why we split the loss into two different losses. One is a background loss and one is foreground loss. The background loss is a mean squared error loss, encouraging the relevance on the background to be close to zero. And we're using segmentation maps here, S is the segmentation map of the image. And the foreground loss encourages the foreground of the image to be closer to one. By splitting into two loss terms, we can give different values or different coefficients to each of the loss terms. So the background loss is going to get a relatively high coefficient too, because we don't want a lot of relevance on the background. By the way, we're not striving to completely eliminate the background, the relevance on the foreground. Just make sure that the relevance of the background is lower than the relevance of the foreground. And the foreground loss is going to get a relatively low coefficient. We would want to encourage the model to look more at more pixels of the foreground, but we wouldn't want to make the model look at all the pixels in the foreground equally. We do also have a classification loss, which ensures that the new prediction by the model or the new distribution is similar to the all distribution by the model. Just to make sure that the model doesn't forget how to classify images. And again, the model does a pretty good job on ImageNet. So we don't want to change the prediction by the model. We just want to change the reasoning. So the giant tables of results here are comparisons between the accuracy of the model before and after a cartooning process. And as you can see here, it's quite tiny, but I hope you can see it still. For the ImageNet validation set, we're experiencing a bit of a decrease in performance. This is because the model relied on spurious cues, and now we're taking them away from it. And so the spurious cues that previously helped the model reach very, very high accuracy and overfit are now taking away. But the decrease in accuracy on average across seven models is not that big. I mean, it's less than 1%. And when you take into account other shifted distributions, such as ImageNet A, ImageNet R, Sketch, ImageNet ObjectNet, and SIScores, you can see that there is a pretty big or significant increase in accuracy. For ImageNet A, for example, plus 5.8% in top one accuracy plus 7.8% in top five accuracy. So really a slight decrease in the accuracy on the data set that the model was originally trained on and a significant increase in accuracy for distribution shifts, as we would expect. So to train it, you have to know the program. What is the program? Yeah, you have to know that. You have segmentation maps. You have segmentation maps. And we do experiment with two types of segmentation maps. One is manually human, manually tagged by humans. And the second one is by token cut, which is a version that uses dyno. This is in case you're training with non-ImageNet data sets and you don't want to manually tag. Even if you do manually tag, I mean, we use three examples for half the classes. So it's not that many examples to tell, but we do provide for an option for ad supervised segmentation. Yeah. So I think that's really cool. The one thing about this, why not just do segmentation? You can just train a segmentation system. Is that kind of naturally explainable? That's an excellent question. Do models that were trained on segmentation have that, you know, brief pass on Spurs correlation? Do they get that in her? What we thought about or I thought about in that context is you can think about a model that learns to classify using Spurs correlation and then identify the object using edge detection. So just because you learn to identify an object does not mean or learn to segment an object. Does not mean that you learn to recognize the object by the segmentation. And also we can think about when you want to train really big models, you need a lot of data to do that. And segmentation data is quite expensive. You usually don't have that amount of data as you do for classification, which is an easier task. You have a lot of data just lying around there. So classification is usually the go to task. Yeah, but only just a few. Okay. Yeah. Just 1500 segmentation maps, either supervised or unsupervised. Yeah. A very few amount of segmentation maps. We did experiment with using more segmentation maps and it showed that the accuracy kind of fluctuates at some point. I mean, there's some point where it doesn't improve more if you add more segmentation maps, but you do have to take into account two things. One, we did find two, and we didn't trade for scratch. Two, we didn't have the resources to hyper parameter search for each selection of the number of. So it's possible that if you use more data, you would need to retune your hyper parameters and then get better accuracy improvement, but we didn't have the resources to do that. So it could be the case, but I don't really know. Yeah, I don't really have any finance for that. One thing we did to ensure that the model actually learns to predict better or to have better explanations is we looked at the accuracy increase for the non-training classes as well, because we said that we only use half the initial classes. It is really interesting to see if the model really improves on the non-training data as well. Does it learn to generalize the positive influence or the positive logic? And as you can see here, this is the ImageNet validation set. So yeah, there's a decrease as we saw before, but for the non-ImageNet distributions, for the shift of distribution, you can see that the improvement for the non-training classes is actually quite similar and sometimes even better than that of the training classes. So the model really from this experiment learns to generalize that healthy say-and-behavior to classes that were not in the training set. And here are some visual examples. These are examples from the ImageNet data set. So examples from the original data set of the model straight map. And here you can see that the same prediction is made for two different reasons. Here, the background, here actually the foreground, the snowplow. And here you can see corrective predictions where the model originally predicted that this is a can opener based on the eye of the puppet. And once we find you the model to look at the entire object or to look for, you know, less sparsely as the object, it actually talks about the teddy bear. And here you can see that even if the model is now wrong and was previously correct, you can usually quite easily explain why the model was wrong. So here's an example where the ground truth classification is tripod and the model predicted actually fine tuning a strawberry, but you can actually see that there exists a strawberry in the image. So it kind of makes sense that the model made that mistake. These are examples for shifted distributions. So as you can see before, for this example, the model predicted a garbage truck. Well, this is the forklift because the forklift is in a garbage area. So we correct the prediction to be a forklift based on foreground rather than the background. Here you can see a teddy bear that was classified as a ping-pong ball due to the sparse consideration of just its spot. And after the fine tuning, it is correctly classified. And the third example is a porcupine that was classified as a sea lion due to the background of the ocean. So once the model really learns to look at the correct pixels, it does make the correct prediction. These are additional examples, but really, we don't have time. And another interesting thing that we've noticed that I think is quite cool, even when you take examples that are completely out of distribution, I mean, this is an image generated by Dalit. And the models not know the class robot or oil painting and so on. Originally, it made a ludicrous prediction that this is a guillotine based on, I don't know what, you can't really understand. But after a fine tuning process, you can see that the model does not make maybe the best prediction that you can think of, which is the robot because it doesn't know that class. But it does predict the grand piano and it kind of makes sense because there is a piano in the image. So while the prediction, again, still does not make the most sense. At least it is based on some of the objects inside the image and not just something that you cannot make sense of due to sparse correlation. So this was the entire talk. Yeah, we got through it in time. Thank you very much. And the table of content is here in case you want to ask a question about specific parts of the lecture. Thank you. Yeah, one or two we can do. Yeah. Yeah, it's visible. Thanks for rotating it. Okay, the questions here are really lacking context because they were probably asked during that. So if anyone wants to ask a question again. Yeah. Yeah, I guess one question I had. Have you thought about including layer norm at all into your explanations? Because it seems that that does scale tokens in some way and could that be relevant for your output? Include what? Sorry, can you repeat it? Layer norm? Layer norm? Oh, no, but as I said, there is a method that I don't quite remember the name of the method that did take into account the norms of the output matrix, I think, in order to average across the different attention heads. But we haven't considered that. Yeah. We do consider that the gradients should be able to scale the different attention heads according to their influence on the prediction. Any other questions? Any other questions? I had a question if no one's going. Oh, yeah, go ahead. Wait, me or someone in the room? Sorry. Yeah. Okay, thank you. Okay. So I was wondering with regards to the stuff you said at the end, where some of them you see it and then you're like, okay, that was wrong, but like, makes sense. That's is there a way to quantify that and were related things or is it more like a you know it when you see it? Oh, yeah, that's a great question. There is a work done by Google, I think, that actually relaxes the task of classifying objects using ImageNet. They actually re-tagged ImageNet, where, you know, a strawberry in that case wouldn't be a mistake, but maybe it would be half a mistake or something like that. Yeah, so there's such a work that re-tags the entire ImageNet dataset to account for mistakes that aren't really mistakes, but actually makes sense. But other than that, I would say there isn't an automatic way to know that. I mean, I can't think off the top of my head of an automatic way to know when the model is mistaken, but it's okay. Cool. How did you guys check? Like, was it mainly the accuracy increase on the distribution shifted versions? Yeah, yeah, it was mainly, yeah, it was mainly the accuracy on the distribution shifts. And, yeah, yeah, and also looking at a lot of examples, right? Because I started out with many. Yeah, yeah, yeah, and a lot of manual work on actualizing examples that got me to the intuition that I'm presenting now, because I actually thought in the beginning that having the relevance be a segmentation map is quite logical. Yeah, so it takes some time to get through all the conclusions. Yeah, yeah. I was just having one idea coming out. Is that possible? So you are going to be this key time. So that in a strawberry case, you can crop out that region and then maybe run through another like Oracle network to tell you whether it is a strawberry or not. And that gives you some kind of a semi-automatic way. Yeah, yeah, definitely. That's an interesting take. It's interesting, particularly because I saw that different models tend to learn different spurious correlations. So it actually makes sense to check models using other models. Yeah, they're consistently making the same prediction with these vets. Yeah, yeah, perhaps. Yeah, yeah, that's an interesting idea. Your current relevancy extractor approach is limited by the VITs tile resolution. It outputs the attention map that is the size of the tiles and then you can bi-level it. Upscaling, yeah. I was wondering whether there's a way to bypass this tile resolution just by considering that we also have pixels coming into the tile. Yeah, yeah, we have tried to propagate relevance all the way back to the actual input and not on the level of each patch. It didn't come out just quite as we hoped. I think that the issue there is probably the positional encoding in the way. Somehow that layer of positional encoding ruins or destroys the relevance values once you propagate back from it. I couldn't figure out how to get past that layer that actually kind of added noise to the output relevance map. That's an interesting point, but yeah. Yeah, I haven't come across any such architectures if you do let me know and I can give you the try. No, there was a question here, right? I was just wondering, when you're about to work for, what if I wanted to explain about a color or something, or non-message, I'd like to just go to something about discussion, right? Maybe then perhaps lemon versus orange, you can color, kind of the main thing. That's just curious. This specific method would not be able to do that, but I know that there are explainability methods that kind of create a decision tree from the model. So you pay the price, that the accuracy decreases to some extent, and then you create a decision tree based on the decisions of the model. You kind of model the model using a decision tree, and then you may have a split that it has to do with, you pass a lot of images through a lot of images of oranges and lemons, and you see that one of the splits is by the color. Yeah, and then you know that. And probably you can do some trivial things to test specific theories, like turn the image into black and white and see what happens, to consider if the model takes into account, but this method will not be able to do that. All right, let's hand this speaker.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.64, "text": " Awesome, so we're so excited to have Hila Chepur, am I pronouncing it right?", "tokens": [50364, 10391, 11, 370, 321, 434, 370, 2919, 281, 362, 389, 7371, 761, 595, 374, 11, 669, 286, 14144, 2175, 309, 558, 30, 50596], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 1, "seek": 0, "start": 4.64, "end": 6.4, "text": " Yeah, it's actually in Hebrew, it's Chepur.", "tokens": [50596, 865, 11, 309, 311, 767, 294, 17895, 11, 309, 311, 761, 595, 374, 13, 50684], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 2, "seek": 0, "start": 6.4, "end": 8.8, "text": " Oh, it's Chepur.", "tokens": [50684, 876, 11, 309, 311, 761, 595, 374, 13, 50804], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 3, "seek": 0, "start": 8.8, "end": 10.08, "text": " Yeah.", "tokens": [50804, 865, 13, 50868], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 4, "seek": 0, "start": 10.08, "end": 10.88, "text": " All right.", "tokens": [50868, 1057, 558, 13, 50908], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 5, "seek": 0, "start": 10.88, "end": 13.84, "text": " So Hila is a PhD candidate at Tel Aviv University,", "tokens": [50908, 407, 389, 7371, 307, 257, 14476, 11532, 412, 27729, 11667, 592, 3535, 11, 51056], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 6, "seek": 0, "start": 13.84, "end": 15.68, "text": " advised by Professor Leon Wolfe.", "tokens": [51056, 26269, 538, 8419, 13244, 19925, 2106, 13, 51148], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 7, "seek": 0, "start": 16.4, "end": 19.92, "text": " Her research focuses on developing reliable XAI algorithms", "tokens": [51184, 3204, 2132, 16109, 322, 6416, 12924, 1783, 48698, 14642, 51360], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 8, "seek": 0, "start": 19.92, "end": 22.88, "text": " and leveraging them to promote model accuracy and fairness.", "tokens": [51360, 293, 32666, 552, 281, 9773, 2316, 14170, 293, 29765, 13, 51508], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 9, "seek": 0, "start": 23.76, "end": 26.64, "text": " Today she's going to talk to us about transform or explainability,", "tokens": [51552, 2692, 750, 311, 516, 281, 751, 281, 505, 466, 4088, 420, 2903, 2310, 11, 51696], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 10, "seek": 0, "start": 26.64, "end": 28.560000000000002, "text": " and we're so excited to hear from you.", "tokens": [51696, 293, 321, 434, 370, 2919, 281, 1568, 490, 291, 13, 51792], "temperature": 0.0, "avg_logprob": -0.27335677427404065, "compression_ratio": 1.5589225589225588, "no_speech_prob": 0.028302092105150223}, {"id": 11, "seek": 2856, "start": 28.639999999999997, "end": 30.4, "text": " Oh, thank you for that great introduction,", "tokens": [50368, 876, 11, 1309, 291, 337, 300, 869, 9339, 11, 50456], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 12, "seek": 2856, "start": 30.4, "end": 32.32, "text": " and I actually have some slides here.", "tokens": [50456, 293, 286, 767, 362, 512, 9788, 510, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 13, "seek": 2856, "start": 32.96, "end": 33.76, "text": " I think you can just...", "tokens": [50584, 286, 519, 291, 393, 445, 485, 50624], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 14, "seek": 2856, "start": 34.48, "end": 35.44, "text": " Oh, yeah.", "tokens": [50660, 876, 11, 1338, 13, 50708], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 15, "seek": 2856, "start": 35.44, "end": 38.879999999999995, "text": " Slides here introducing myself, but I think you did that perfectly,", "tokens": [50708, 6187, 1875, 510, 15424, 2059, 11, 457, 286, 519, 291, 630, 300, 6239, 11, 50880], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 16, "seek": 2856, "start": 38.879999999999995, "end": 39.76, "text": " so I'll just skip that.", "tokens": [50880, 370, 286, 603, 445, 10023, 300, 13, 50924], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 17, "seek": 2856, "start": 41.2, "end": 44.56, "text": " So maybe the first thing we want to talk about is motivation.", "tokens": [50996, 407, 1310, 264, 700, 551, 321, 528, 281, 751, 466, 307, 12335, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 18, "seek": 2856, "start": 44.56, "end": 46.56, "text": " We're here to talk about transform or explainability,", "tokens": [51164, 492, 434, 510, 281, 751, 466, 4088, 420, 2903, 2310, 11, 51264], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 19, "seek": 2856, "start": 46.56, "end": 47.92, "text": " but why should you care?", "tokens": [51264, 457, 983, 820, 291, 1127, 30, 51332], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 20, "seek": 2856, "start": 48.64, "end": 50.08, "text": " And let's just have a disclaimer,", "tokens": [51368, 400, 718, 311, 445, 362, 257, 40896, 11, 51440], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 21, "seek": 2856, "start": 50.08, "end": 52.64, "text": " because we all know that explainability is really important", "tokens": [51440, 570, 321, 439, 458, 300, 2903, 2310, 307, 534, 1021, 51568], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 22, "seek": 2856, "start": 52.64, "end": 56.239999999999995, "text": " for aspects like accountability, reliability, and so on.", "tokens": [51568, 337, 7270, 411, 19380, 11, 24550, 11, 293, 370, 322, 13, 51748], "temperature": 0.0, "avg_logprob": -0.11426926376228046, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0016708857147023082}, {"id": 23, "seek": 5624, "start": 56.24, "end": 57.68, "text": " But when we write research papers,", "tokens": [50364, 583, 562, 321, 2464, 2132, 10577, 11, 50436], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 24, "seek": 5624, "start": 57.68, "end": 60.08, "text": " we usually focus on other measuring sticks,", "tokens": [50436, 321, 2673, 1879, 322, 661, 13389, 12518, 11, 50556], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 25, "seek": 5624, "start": 60.08, "end": 63.04, "text": " such as accuracy and robustness, right?", "tokens": [50556, 1270, 382, 14170, 293, 13956, 1287, 11, 558, 30, 50704], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 26, "seek": 5624, "start": 63.68, "end": 66.8, "text": " So I'm here to convince you that the explainability queue", "tokens": [50736, 407, 286, 478, 510, 281, 13447, 291, 300, 264, 2903, 2310, 18639, 50892], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 27, "seek": 5624, "start": 66.8, "end": 70.24000000000001, "text": " is actually really useful for those measuring sticks as well,", "tokens": [50892, 307, 767, 534, 4420, 337, 729, 13389, 12518, 382, 731, 11, 51064], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 28, "seek": 5624, "start": 70.24000000000001, "end": 73.36, "text": " and that you should consider using explainability", "tokens": [51064, 293, 300, 291, 820, 1949, 1228, 2903, 2310, 51220], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 29, "seek": 5624, "start": 73.36, "end": 75.2, "text": " in your research, even if it's unrelated", "tokens": [51220, 294, 428, 2132, 11, 754, 498, 309, 311, 38967, 51312], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 30, "seek": 5624, "start": 75.2, "end": 77.2, "text": " to accountability, reliability, and so on.", "tokens": [51312, 281, 19380, 11, 24550, 11, 293, 370, 322, 13, 51412], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 31, "seek": 5624, "start": 78.64, "end": 81.12, "text": " So to do that, I have a few examples showing", "tokens": [51484, 407, 281, 360, 300, 11, 286, 362, 257, 1326, 5110, 4099, 51608], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 32, "seek": 5624, "start": 81.12, "end": 84.08, "text": " how explainability can be used to improve model accuracy", "tokens": [51608, 577, 2903, 2310, 393, 312, 1143, 281, 3470, 2316, 14170, 51756], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 33, "seek": 5624, "start": 84.08, "end": 85.68, "text": " and robustness.", "tokens": [51756, 293, 13956, 1287, 13, 51836], "temperature": 0.0, "avg_logprob": -0.05295429073396276, "compression_ratio": 1.7883211678832116, "no_speech_prob": 0.0002779275819193572}, {"id": 34, "seek": 8568, "start": 85.76, "end": 87.52000000000001, "text": " The first example is text to live.", "tokens": [50368, 440, 700, 1365, 307, 2487, 281, 1621, 13, 50456], "temperature": 0.0, "avg_logprob": -0.14754590417584804, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003567686944734305}, {"id": 35, "seek": 8568, "start": 87.52000000000001, "end": 88.24000000000001, "text": " Maybe you know it.", "tokens": [50456, 2704, 291, 458, 309, 13, 50492], "temperature": 0.0, "avg_logprob": -0.14754590417584804, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003567686944734305}, {"id": 36, "seek": 8568, "start": 88.24000000000001, "end": 90.56, "text": " It got accepted to ECCV 2022.", "tokens": [50492, 467, 658, 9035, 281, 462, 11717, 53, 20229, 13, 50608], "temperature": 0.0, "avg_logprob": -0.14754590417584804, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003567686944734305}, {"id": 37, "seek": 8568, "start": 91.12, "end": 95.2, "text": " And the objective of the model is to take a target edit text,", "tokens": [50636, 400, 264, 10024, 295, 264, 2316, 307, 281, 747, 257, 3779, 8129, 2487, 11, 50840], "temperature": 0.0, "avg_logprob": -0.14754590417584804, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003567686944734305}, {"id": 38, "seek": 8568, "start": 95.2, "end": 99.44000000000001, "text": " for example, here around hat, and apply the edit to the image.", "tokens": [50840, 337, 1365, 11, 510, 926, 2385, 11, 293, 3079, 264, 8129, 281, 264, 3256, 13, 51052], "temperature": 0.0, "avg_logprob": -0.14754590417584804, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003567686944734305}, {"id": 39, "seek": 8568, "start": 99.44000000000001, "end": 102.32000000000001, "text": " So what they try to do is unlike other works,", "tokens": [51052, 407, 437, 436, 853, 281, 360, 307, 8343, 661, 1985, 11, 51196], "temperature": 0.0, "avg_logprob": -0.14754590417584804, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003567686944734305}, {"id": 40, "seek": 8568, "start": 102.32000000000001, "end": 105.60000000000001, "text": " they try to prevent the part where the user actually has", "tokens": [51196, 436, 853, 281, 4871, 264, 644, 689, 264, 4195, 767, 575, 51360], "temperature": 0.0, "avg_logprob": -0.14754590417584804, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003567686944734305}, {"id": 41, "seek": 8568, "start": 105.60000000000001, "end": 108.96000000000001, "text": " to insert a manual segmentation mask", "tokens": [51360, 281, 8969, 257, 9688, 9469, 399, 6094, 51528], "temperature": 0.0, "avg_logprob": -0.14754590417584804, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003567686944734305}, {"id": 42, "seek": 8568, "start": 108.96000000000001, "end": 111.12, "text": " to indicate where the hat is.", "tokens": [51528, 281, 13330, 689, 264, 2385, 307, 13, 51636], "temperature": 0.0, "avg_logprob": -0.14754590417584804, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003567686944734305}, {"id": 43, "seek": 8568, "start": 111.68, "end": 114.80000000000001, "text": " So they wanted some automatic way of getting", "tokens": [51664, 407, 436, 1415, 512, 12509, 636, 295, 1242, 51820], "temperature": 0.0, "avg_logprob": -0.14754590417584804, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003567686944734305}, {"id": 44, "seek": 11480, "start": 114.8, "end": 117.03999999999999, "text": " to the region of interest, which is the hat.", "tokens": [50364, 281, 264, 4458, 295, 1179, 11, 597, 307, 264, 2385, 13, 50476], "temperature": 0.0, "avg_logprob": -0.10470935648137873, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0004170785832684487}, {"id": 45, "seek": 11480, "start": 117.03999999999999, "end": 119.36, "text": " So they use relevancy maps of the clip model,", "tokens": [50476, 407, 436, 764, 25916, 6717, 11317, 295, 264, 7353, 2316, 11, 50592], "temperature": 0.0, "avg_logprob": -0.10470935648137873, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0004170785832684487}, {"id": 46, "seek": 11480, "start": 119.36, "end": 120.88, "text": " which I guess you're familiar with.", "tokens": [50592, 597, 286, 2041, 291, 434, 4963, 365, 13, 50668], "temperature": 0.0, "avg_logprob": -0.10470935648137873, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0004170785832684487}, {"id": 47, "seek": 11480, "start": 122.56, "end": 126.0, "text": " So the clip model with the relevancy map actually indicated", "tokens": [50752, 407, 264, 7353, 2316, 365, 264, 25916, 6717, 4471, 767, 16176, 50924], "temperature": 0.0, "avg_logprob": -0.10470935648137873, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0004170785832684487}, {"id": 48, "seek": 11480, "start": 126.0, "end": 132.48, "text": " to their downstream path, where pretty much the hat is in the image.", "tokens": [50924, 281, 641, 30621, 3100, 11, 689, 1238, 709, 264, 2385, 307, 294, 264, 3256, 13, 51248], "temperature": 0.0, "avg_logprob": -0.10470935648137873, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0004170785832684487}, {"id": 49, "seek": 11480, "start": 133.04, "end": 136.07999999999998, "text": " And then their model refined those relevancy maps", "tokens": [51276, 400, 550, 641, 2316, 26201, 729, 25916, 6717, 11317, 51428], "temperature": 0.0, "avg_logprob": -0.10470935648137873, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0004170785832684487}, {"id": 50, "seek": 11480, "start": 136.07999999999998, "end": 140.48, "text": " and applied the edit according to the location indicated", "tokens": [51428, 293, 6456, 264, 8129, 4650, 281, 264, 4914, 16176, 51648], "temperature": 0.0, "avg_logprob": -0.10470935648137873, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0004170785832684487}, {"id": 51, "seek": 11480, "start": 140.48, "end": 141.84, "text": " by the relevancy map.", "tokens": [51648, 538, 264, 25916, 6717, 4471, 13, 51716], "temperature": 0.0, "avg_logprob": -0.10470935648137873, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0004170785832684487}, {"id": 52, "seek": 11480, "start": 141.84, "end": 144.0, "text": " So here you can see what happens without this.", "tokens": [51716, 407, 510, 291, 393, 536, 437, 2314, 1553, 341, 13, 51824], "temperature": 0.0, "avg_logprob": -0.10470935648137873, "compression_ratio": 1.8658008658008658, "no_speech_prob": 0.0004170785832684487}, {"id": 53, "seek": 14400, "start": 144.0, "end": 146.88, "text": " They call it bootstrapping without using the relevancy map.", "tokens": [50364, 814, 818, 309, 11450, 19639, 3759, 1553, 1228, 264, 25916, 6717, 4471, 13, 50508], "temperature": 0.0, "avg_logprob": -0.10379685295952691, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.00035676802508533}, {"id": 54, "seek": 14400, "start": 146.88, "end": 149.12, "text": " So you can see that there are additional artifacts,", "tokens": [50508, 407, 291, 393, 536, 300, 456, 366, 4497, 24617, 11, 50620], "temperature": 0.0, "avg_logprob": -0.10379685295952691, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.00035676802508533}, {"id": 55, "seek": 14400, "start": 149.12, "end": 151.92, "text": " such as the faces turning red and not just the hat.", "tokens": [50620, 1270, 382, 264, 8475, 6246, 2182, 293, 406, 445, 264, 2385, 13, 50760], "temperature": 0.0, "avg_logprob": -0.10379685295952691, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.00035676802508533}, {"id": 56, "seek": 14400, "start": 151.92, "end": 154.64, "text": " And when you use bootstrapping, when you use relevancy maps,", "tokens": [50760, 400, 562, 291, 764, 11450, 19639, 3759, 11, 562, 291, 764, 25916, 6717, 11317, 11, 50896], "temperature": 0.0, "avg_logprob": -0.10379685295952691, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.00035676802508533}, {"id": 57, "seek": 14400, "start": 154.64, "end": 158.24, "text": " then the edit is quite localized to the hat.", "tokens": [50896, 550, 264, 8129, 307, 1596, 44574, 281, 264, 2385, 13, 51076], "temperature": 0.0, "avg_logprob": -0.10379685295952691, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.00035676802508533}, {"id": 58, "seek": 14400, "start": 158.24, "end": 160.72, "text": " And here you can see other edits that are quite nice.", "tokens": [51076, 400, 510, 291, 393, 536, 661, 41752, 300, 366, 1596, 1481, 13, 51200], "temperature": 0.0, "avg_logprob": -0.10379685295952691, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.00035676802508533}, {"id": 59, "seek": 14400, "start": 160.72, "end": 164.16, "text": " They take a sponge cake and they turn it into an ice cake", "tokens": [51200, 814, 747, 257, 23134, 5908, 293, 436, 1261, 309, 666, 364, 4435, 5908, 51372], "temperature": 0.0, "avg_logprob": -0.10379685295952691, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.00035676802508533}, {"id": 60, "seek": 14400, "start": 164.16, "end": 165.28, "text": " or a spinach mousse cake.", "tokens": [51372, 420, 257, 27784, 275, 28521, 5908, 13, 51428], "temperature": 0.0, "avg_logprob": -0.10379685295952691, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.00035676802508533}, {"id": 61, "seek": 14400, "start": 165.28, "end": 167.76, "text": " So I think it's a nice example to show.", "tokens": [51428, 407, 286, 519, 309, 311, 257, 1481, 1365, 281, 855, 13, 51552], "temperature": 0.0, "avg_logprob": -0.10379685295952691, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.00035676802508533}, {"id": 62, "seek": 14400, "start": 168.8, "end": 171.36, "text": " Another example is a paper called Kripaso.", "tokens": [51604, 3996, 1365, 307, 257, 3035, 1219, 591, 8400, 35281, 13, 51732], "temperature": 0.0, "avg_logprob": -0.10379685295952691, "compression_ratio": 1.849056603773585, "no_speech_prob": 0.00035676802508533}, {"id": 63, "seek": 17136, "start": 171.36, "end": 174.24, "text": " It's that best paper awarded Seagraph 2022.", "tokens": [50364, 467, 311, 300, 1151, 3035, 19100, 1100, 559, 2662, 20229, 13, 50508], "temperature": 0.0, "avg_logprob": -0.10419310760498048, "compression_ratio": 1.7147766323024054, "no_speech_prob": 0.00021309529256541282}, {"id": 64, "seek": 17136, "start": 174.8, "end": 176.64000000000001, "text": " It also uses relevancy maps.", "tokens": [50536, 467, 611, 4960, 25916, 6717, 11317, 13, 50628], "temperature": 0.0, "avg_logprob": -0.10419310760498048, "compression_ratio": 1.7147766323024054, "no_speech_prob": 0.00021309529256541282}, {"id": 65, "seek": 17136, "start": 176.64000000000001, "end": 179.12, "text": " The goal of the model is to take an input image", "tokens": [50628, 440, 3387, 295, 264, 2316, 307, 281, 747, 364, 4846, 3256, 50752], "temperature": 0.0, "avg_logprob": -0.10419310760498048, "compression_ratio": 1.7147766323024054, "no_speech_prob": 0.00021309529256541282}, {"id": 66, "seek": 17136, "start": 179.12, "end": 182.48000000000002, "text": " and create sketches with different levels of abstraction.", "tokens": [50752, 293, 1884, 34547, 365, 819, 4358, 295, 37765, 13, 50920], "temperature": 0.0, "avg_logprob": -0.10419310760498048, "compression_ratio": 1.7147766323024054, "no_speech_prob": 0.00021309529256541282}, {"id": 67, "seek": 17136, "start": 182.48000000000002, "end": 184.72000000000003, "text": " So you can choose which level of abstraction you want.", "tokens": [50920, 407, 291, 393, 2826, 597, 1496, 295, 37765, 291, 528, 13, 51032], "temperature": 0.0, "avg_logprob": -0.10419310760498048, "compression_ratio": 1.7147766323024054, "no_speech_prob": 0.00021309529256541282}, {"id": 68, "seek": 17136, "start": 184.72000000000003, "end": 186.24, "text": " So for example, the flamingo here,", "tokens": [51032, 407, 337, 1365, 11, 264, 45718, 78, 510, 11, 51108], "temperature": 0.0, "avg_logprob": -0.10419310760498048, "compression_ratio": 1.7147766323024054, "no_speech_prob": 0.00021309529256541282}, {"id": 69, "seek": 17136, "start": 186.96, "end": 191.68, "text": " very abstract painting of the flamingo or very detailed painting.", "tokens": [51144, 588, 12649, 5370, 295, 264, 45718, 78, 420, 588, 9942, 5370, 13, 51380], "temperature": 0.0, "avg_logprob": -0.10419310760498048, "compression_ratio": 1.7147766323024054, "no_speech_prob": 0.00021309529256541282}, {"id": 70, "seek": 17136, "start": 191.68, "end": 193.84, "text": " And what they did is they actually used the relevancy maps", "tokens": [51380, 400, 437, 436, 630, 307, 436, 767, 1143, 264, 25916, 6717, 11317, 51488], "temperature": 0.0, "avg_logprob": -0.10419310760498048, "compression_ratio": 1.7147766323024054, "no_speech_prob": 0.00021309529256541282}, {"id": 71, "seek": 17136, "start": 193.84, "end": 198.88000000000002, "text": " as an initializer for the model to understand where the object is", "tokens": [51488, 382, 364, 5883, 6545, 337, 264, 2316, 281, 1223, 689, 264, 2657, 307, 51740], "temperature": 0.0, "avg_logprob": -0.10419310760498048, "compression_ratio": 1.7147766323024054, "no_speech_prob": 0.00021309529256541282}, {"id": 72, "seek": 17136, "start": 198.88000000000002, "end": 200.96, "text": " and how to create the stroke's product.", "tokens": [51740, 293, 577, 281, 1884, 264, 12403, 311, 1674, 13, 51844], "temperature": 0.0, "avg_logprob": -0.10419310760498048, "compression_ratio": 1.7147766323024054, "no_speech_prob": 0.00021309529256541282}, {"id": 73, "seek": 20136, "start": 202.16000000000003, "end": 203.76000000000002, "text": " Another example is Sameer's work,", "tokens": [50404, 3996, 1365, 307, 10635, 260, 311, 589, 11, 50484], "temperature": 0.0, "avg_logprob": -0.13245540618896484, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.247800385812297e-05}, {"id": 74, "seek": 20136, "start": 203.76000000000002, "end": 205.68, "text": " which you're probably familiar with.", "tokens": [50484, 597, 291, 434, 1391, 4963, 365, 13, 50580], "temperature": 0.0, "avg_logprob": -0.13245540618896484, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.247800385812297e-05}, {"id": 75, "seek": 20136, "start": 207.76000000000002, "end": 211.04000000000002, "text": " What you did here, really, is you used the relevancy maps", "tokens": [50684, 708, 291, 630, 510, 11, 534, 11, 307, 291, 1143, 264, 25916, 6717, 11317, 50848], "temperature": 0.0, "avg_logprob": -0.13245540618896484, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.247800385812297e-05}, {"id": 76, "seek": 20136, "start": 211.04000000000002, "end": 214.16000000000003, "text": " in order to locate objects which aren't necessarily", "tokens": [50848, 294, 1668, 281, 22370, 6565, 597, 3212, 380, 4725, 51004], "temperature": 0.0, "avg_logprob": -0.13245540618896484, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.247800385812297e-05}, {"id": 77, "seek": 20136, "start": 214.88000000000002, "end": 218.48000000000002, "text": " objects in the wild, objects that appear in living rooms", "tokens": [51040, 6565, 294, 264, 4868, 11, 6565, 300, 4204, 294, 2647, 9396, 51220], "temperature": 0.0, "avg_logprob": -0.13245540618896484, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.247800385812297e-05}, {"id": 78, "seek": 20136, "start": 218.48000000000002, "end": 222.16000000000003, "text": " and that do not necessarily appear in the training set", "tokens": [51220, 293, 300, 360, 406, 4725, 4204, 294, 264, 3097, 992, 51404], "temperature": 0.0, "avg_logprob": -0.13245540618896484, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.247800385812297e-05}, {"id": 79, "seek": 20136, "start": 222.16000000000003, "end": 226.16000000000003, "text": " of some models of desegmentation or localization.", "tokens": [51404, 295, 512, 5245, 295, 730, 1146, 19631, 420, 2654, 2144, 13, 51604], "temperature": 0.0, "avg_logprob": -0.13245540618896484, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.247800385812297e-05}, {"id": 80, "seek": 20136, "start": 226.16000000000003, "end": 229.12, "text": " So you can use Kripaso in order to identify objects", "tokens": [51604, 407, 291, 393, 764, 591, 8400, 35281, 294, 1668, 281, 5876, 6565, 51752], "temperature": 0.0, "avg_logprob": -0.13245540618896484, "compression_ratio": 1.676595744680851, "no_speech_prob": 7.247800385812297e-05}, {"id": 81, "seek": 22912, "start": 229.12, "end": 231.92000000000002, "text": " that are really not really objects", "tokens": [50364, 300, 366, 534, 406, 534, 6565, 50504], "temperature": 0.0, "avg_logprob": -0.10760534786787189, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.00020978512475267053}, {"id": 82, "seek": 22912, "start": 231.92000000000002, "end": 233.68, "text": " that are so common in training sets.", "tokens": [50504, 300, 366, 370, 2689, 294, 3097, 6352, 13, 50592], "temperature": 0.0, "avg_logprob": -0.10760534786787189, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.00020978512475267053}, {"id": 83, "seek": 22912, "start": 235.76, "end": 239.28, "text": " So if you consider all the examples that we've seen before,", "tokens": [50696, 407, 498, 291, 1949, 439, 264, 5110, 300, 321, 600, 1612, 949, 11, 50872], "temperature": 0.0, "avg_logprob": -0.10760534786787189, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.00020978512475267053}, {"id": 84, "seek": 22912, "start": 239.28, "end": 241.04, "text": " they have one thing in common.", "tokens": [50872, 436, 362, 472, 551, 294, 2689, 13, 50960], "temperature": 0.0, "avg_logprob": -0.10760534786787189, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.00020978512475267053}, {"id": 85, "seek": 22912, "start": 241.04, "end": 244.16, "text": " They use the relevancy maps as a fixed signal.", "tokens": [50960, 814, 764, 264, 25916, 6717, 11317, 382, 257, 6806, 6358, 13, 51116], "temperature": 0.0, "avg_logprob": -0.10760534786787189, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.00020978512475267053}, {"id": 86, "seek": 22912, "start": 244.16, "end": 246.64000000000001, "text": " They didn't train on the relevancy map", "tokens": [51116, 814, 994, 380, 3847, 322, 264, 25916, 6717, 4471, 51240], "temperature": 0.0, "avg_logprob": -0.10760534786787189, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.00020978512475267053}, {"id": 87, "seek": 22912, "start": 246.64000000000001, "end": 248.48000000000002, "text": " or create a loss with the relevancy map.", "tokens": [51240, 420, 1884, 257, 4470, 365, 264, 25916, 6717, 4471, 13, 51332], "temperature": 0.0, "avg_logprob": -0.10760534786787189, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.00020978512475267053}, {"id": 88, "seek": 22912, "start": 248.48000000000002, "end": 251.84, "text": " They use it as an initialization for the answering task.", "tokens": [51332, 814, 764, 309, 382, 364, 5883, 2144, 337, 264, 13430, 5633, 13, 51500], "temperature": 0.0, "avg_logprob": -0.10760534786787189, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.00020978512475267053}, {"id": 89, "seek": 22912, "start": 252.4, "end": 254.88, "text": " But what we did in our last work is actually just,", "tokens": [51528, 583, 437, 321, 630, 294, 527, 1036, 589, 307, 767, 445, 11, 51652], "temperature": 0.0, "avg_logprob": -0.10760534786787189, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.00020978512475267053}, {"id": 90, "seek": 22912, "start": 254.88, "end": 258.56, "text": " we showed that its reliability can be used as a loss term", "tokens": [51652, 321, 4712, 300, 1080, 24550, 393, 312, 1143, 382, 257, 4470, 1433, 51836], "temperature": 0.0, "avg_logprob": -0.10760534786787189, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.00020978512475267053}, {"id": 91, "seek": 25856, "start": 258.56, "end": 260.08, "text": " in order to improve models.", "tokens": [50364, 294, 1668, 281, 3470, 5245, 13, 50440], "temperature": 0.0, "avg_logprob": -0.07747546180349882, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00013546913396567106}, {"id": 92, "seek": 25856, "start": 260.64, "end": 262.56, "text": " So if you think about what it means", "tokens": [50468, 407, 498, 291, 519, 466, 437, 309, 1355, 50564], "temperature": 0.0, "avg_logprob": -0.07747546180349882, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00013546913396567106}, {"id": 93, "seek": 25856, "start": 262.56, "end": 265.44, "text": " to create a loss term based on explainability maps,", "tokens": [50564, 281, 1884, 257, 4470, 1433, 2361, 322, 2903, 2310, 11317, 11, 50708], "temperature": 0.0, "avg_logprob": -0.07747546180349882, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00013546913396567106}, {"id": 94, "seek": 25856, "start": 265.44, "end": 269.44, "text": " it's really meant to teach the model how to do something", "tokens": [50708, 309, 311, 534, 4140, 281, 2924, 264, 2316, 577, 281, 360, 746, 50908], "temperature": 0.0, "avg_logprob": -0.07747546180349882, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00013546913396567106}, {"id": 95, "seek": 25856, "start": 269.44, "end": 272.8, "text": " or why it does something, not just how to do something.", "tokens": [50908, 420, 983, 309, 775, 746, 11, 406, 445, 577, 281, 360, 746, 13, 51076], "temperature": 0.0, "avg_logprob": -0.07747546180349882, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00013546913396567106}, {"id": 96, "seek": 25856, "start": 272.8, "end": 274.8, "text": " And we talk about classification models.", "tokens": [51076, 400, 321, 751, 466, 21538, 5245, 13, 51176], "temperature": 0.0, "avg_logprob": -0.07747546180349882, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00013546913396567106}, {"id": 97, "seek": 25856, "start": 274.8, "end": 277.76, "text": " They tend to learn spurious cues", "tokens": [51176, 814, 3928, 281, 1466, 637, 24274, 32192, 51324], "temperature": 0.0, "avg_logprob": -0.07747546180349882, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00013546913396567106}, {"id": 98, "seek": 25856, "start": 277.76, "end": 280.96, "text": " to help them make shortcuts to make a prediction.", "tokens": [51324, 281, 854, 552, 652, 34620, 281, 652, 257, 17630, 13, 51484], "temperature": 0.0, "avg_logprob": -0.07747546180349882, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00013546913396567106}, {"id": 99, "seek": 25856, "start": 280.96, "end": 283.36, "text": " So for example, a model can learn a spurious cue", "tokens": [51484, 407, 337, 1365, 11, 257, 2316, 393, 1466, 257, 637, 24274, 22656, 51604], "temperature": 0.0, "avg_logprob": -0.07747546180349882, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00013546913396567106}, {"id": 100, "seek": 25856, "start": 283.36, "end": 287.36, "text": " that if you have a round object with the background of a grass,", "tokens": [51604, 300, 498, 291, 362, 257, 3098, 2657, 365, 264, 3678, 295, 257, 8054, 11, 51804], "temperature": 0.0, "avg_logprob": -0.07747546180349882, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.00013546913396567106}, {"id": 101, "seek": 28736, "start": 287.36, "end": 288.8, "text": " then it's a golf ball.", "tokens": [50364, 550, 309, 311, 257, 12880, 2594, 13, 50436], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 102, "seek": 28736, "start": 288.8, "end": 291.04, "text": " And here you can see that the model classified this lemon", "tokens": [50436, 400, 510, 291, 393, 536, 300, 264, 2316, 20627, 341, 11356, 50548], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 103, "seek": 28736, "start": 291.04, "end": 293.52000000000004, "text": " as a golf ball because of the background of the grass.", "tokens": [50548, 382, 257, 12880, 2594, 570, 295, 264, 3678, 295, 264, 8054, 13, 50672], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 104, "seek": 28736, "start": 294.16, "end": 296.56, "text": " When you force the model to actually focus", "tokens": [50704, 1133, 291, 3464, 264, 2316, 281, 767, 1879, 50824], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 105, "seek": 28736, "start": 296.56, "end": 298.24, "text": " on the foreground of the image", "tokens": [50824, 322, 264, 32058, 295, 264, 3256, 50908], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 106, "seek": 28736, "start": 298.24, "end": 299.76, "text": " and not just the background of the image,", "tokens": [50908, 293, 406, 445, 264, 3678, 295, 264, 3256, 11, 50984], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 107, "seek": 28736, "start": 300.56, "end": 304.08000000000004, "text": " via a loss applied directly to the explainability maps,", "tokens": [51024, 5766, 257, 4470, 6456, 3838, 281, 264, 2903, 2310, 11317, 11, 51200], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 108, "seek": 28736, "start": 304.08000000000004, "end": 306.88, "text": " you can correct wrong predictions based on spurious cues.", "tokens": [51200, 291, 393, 3006, 2085, 21264, 2361, 322, 637, 24274, 32192, 13, 51340], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 109, "seek": 28736, "start": 307.44, "end": 310.08000000000004, "text": " So what we're doing usually is we're teaching the model", "tokens": [51368, 407, 437, 321, 434, 884, 2673, 307, 321, 434, 4571, 264, 2316, 51500], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 110, "seek": 28736, "start": 310.08000000000004, "end": 311.44, "text": " to predict something, right?", "tokens": [51500, 281, 6069, 746, 11, 558, 30, 51568], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 111, "seek": 28736, "start": 311.44, "end": 315.36, "text": " Predict golf ball, car, glasses, etc.", "tokens": [51568, 430, 24945, 12880, 2594, 11, 1032, 11, 10812, 11, 5183, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09385262429714203, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.0003798475372605026}, {"id": 112, "seek": 31536, "start": 315.36, "end": 317.6, "text": " But we're not really teaching it why.", "tokens": [50364, 583, 321, 434, 406, 534, 4571, 309, 983, 13, 50476], "temperature": 0.0, "avg_logprob": -0.16123027067918044, "compression_ratio": 1.608, "no_speech_prob": 0.0005094277439638972}, {"id": 113, "seek": 31536, "start": 317.6, "end": 320.24, "text": " Why this is the object in the image.", "tokens": [50476, 1545, 341, 307, 264, 2657, 294, 264, 3256, 13, 50608], "temperature": 0.0, "avg_logprob": -0.16123027067918044, "compression_ratio": 1.608, "no_speech_prob": 0.0005094277439638972}, {"id": 114, "seek": 31536, "start": 320.24, "end": 324.8, "text": " So what we're showing here is that by fine-tuning directly", "tokens": [50608, 407, 437, 321, 434, 4099, 510, 307, 300, 538, 2489, 12, 83, 37726, 3838, 50836], "temperature": 0.0, "avg_logprob": -0.16123027067918044, "compression_ratio": 1.608, "no_speech_prob": 0.0005094277439638972}, {"id": 115, "seek": 31536, "start": 324.8, "end": 327.44, "text": " the relevant slots or the explainability maps,", "tokens": [50836, 264, 7340, 24266, 420, 264, 2903, 2310, 11317, 11, 50968], "temperature": 0.0, "avg_logprob": -0.16123027067918044, "compression_ratio": 1.608, "no_speech_prob": 0.0005094277439638972}, {"id": 116, "seek": 31536, "start": 327.44, "end": 330.16, "text": " we can correct wrong predictions based on spurious conditions.", "tokens": [50968, 321, 393, 3006, 2085, 21264, 2361, 322, 637, 24274, 4487, 13, 51104], "temperature": 0.0, "avg_logprob": -0.16123027067918044, "compression_ratio": 1.608, "no_speech_prob": 0.0005094277439638972}, {"id": 117, "seek": 31536, "start": 330.96000000000004, "end": 337.2, "text": " But we'll get to it in depth later on.", "tokens": [51144, 583, 321, 603, 483, 281, 309, 294, 7161, 1780, 322, 13, 51456], "temperature": 0.0, "avg_logprob": -0.16123027067918044, "compression_ratio": 1.608, "no_speech_prob": 0.0005094277439638972}, {"id": 118, "seek": 31536, "start": 338.40000000000003, "end": 340.0, "text": " So this was the motivation part,", "tokens": [51516, 407, 341, 390, 264, 12335, 644, 11, 51596], "temperature": 0.0, "avg_logprob": -0.16123027067918044, "compression_ratio": 1.608, "no_speech_prob": 0.0005094277439638972}, {"id": 119, "seek": 31536, "start": 340.0, "end": 342.48, "text": " and hopefully you got fully motivated", "tokens": [51596, 293, 4696, 291, 658, 4498, 14515, 51720], "temperature": 0.0, "avg_logprob": -0.16123027067918044, "compression_ratio": 1.608, "no_speech_prob": 0.0005094277439638972}, {"id": 120, "seek": 31536, "start": 342.48, "end": 344.72, "text": " as to why Transformer experiment is interesting.", "tokens": [51720, 382, 281, 983, 27938, 260, 5120, 307, 1880, 13, 51832], "temperature": 0.0, "avg_logprob": -0.16123027067918044, "compression_ratio": 1.608, "no_speech_prob": 0.0005094277439638972}, {"id": 121, "seek": 34536, "start": 345.36, "end": 348.24, "text": " Our talk is going to be a construed of two parts.", "tokens": [50364, 2621, 751, 307, 516, 281, 312, 257, 12946, 292, 295, 732, 3166, 13, 50508], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 122, "seek": 34536, "start": 348.24, "end": 349.84000000000003, "text": " The first part is going to be,", "tokens": [50508, 440, 700, 644, 307, 516, 281, 312, 11, 50588], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 123, "seek": 34536, "start": 349.84000000000003, "end": 352.72, "text": " we're going to talk about how we do Transformer explainability.", "tokens": [50588, 321, 434, 516, 281, 751, 466, 577, 321, 360, 27938, 260, 2903, 2310, 13, 50732], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 124, "seek": 34536, "start": 352.72, "end": 354.64, "text": " We're going to see the attention mechanism", "tokens": [50732, 492, 434, 516, 281, 536, 264, 3202, 7513, 50828], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 125, "seek": 34536, "start": 354.64, "end": 356.16, "text": " which I'm sure you're all familiar with,", "tokens": [50828, 597, 286, 478, 988, 291, 434, 439, 4963, 365, 11, 50904], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 126, "seek": 34536, "start": 356.16, "end": 359.28000000000003, "text": " but we're going to have emphasis on specific parts", "tokens": [50904, 457, 321, 434, 516, 281, 362, 16271, 322, 2685, 3166, 51060], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 127, "seek": 34536, "start": 359.28000000000003, "end": 361.28000000000003, "text": " of the attention mechanism that are going to be useful.", "tokens": [51060, 295, 264, 3202, 7513, 300, 366, 516, 281, 312, 4420, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 128, "seek": 34536, "start": 361.84000000000003, "end": 363.2, "text": " Then we're going to ask ourselves,", "tokens": [51188, 1396, 321, 434, 516, 281, 1029, 4175, 11, 51256], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 129, "seek": 34536, "start": 363.2, "end": 364.72, "text": " is attention an explanation?", "tokens": [51256, 307, 3202, 364, 10835, 30, 51332], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 130, "seek": 34536, "start": 364.72, "end": 367.36, "text": " Which is really the most prominent question", "tokens": [51332, 3013, 307, 534, 264, 881, 17034, 1168, 51464], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 131, "seek": 34536, "start": 367.36, "end": 369.44, "text": " when doing Transformer explainability.", "tokens": [51464, 562, 884, 27938, 260, 2903, 2310, 13, 51568], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 132, "seek": 34536, "start": 369.44, "end": 372.40000000000003, "text": " We're going to talk about three explainability algorithms.", "tokens": [51568, 492, 434, 516, 281, 751, 466, 1045, 2903, 2310, 14642, 13, 51716], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 133, "seek": 34536, "start": 372.40000000000003, "end": 374.24, "text": " The first one is attention roll-up, not by me.", "tokens": [51716, 440, 700, 472, 307, 3202, 3373, 12, 1010, 11, 406, 538, 385, 13, 51808], "temperature": 0.0, "avg_logprob": -0.1108451793396395, "compression_ratio": 2.0704225352112675, "no_speech_prob": 0.0004718461714219302}, {"id": 134, "seek": 37424, "start": 374.96000000000004, "end": 377.84000000000003, "text": " But it is the groundbreaking first algorithm", "tokens": [50400, 583, 309, 307, 264, 42491, 700, 9284, 50544], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 135, "seek": 37424, "start": 377.84000000000003, "end": 379.2, "text": " in the big Transformer explainability.", "tokens": [50544, 294, 264, 955, 27938, 260, 2903, 2310, 13, 50612], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 136, "seek": 37424, "start": 379.2, "end": 381.36, "text": " Then I'm going to present two of my works", "tokens": [50612, 1396, 286, 478, 516, 281, 1974, 732, 295, 452, 1985, 50720], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 137, "seek": 37424, "start": 381.36, "end": 383.28000000000003, "text": " that have to do with Transformer explainability.", "tokens": [50720, 300, 362, 281, 360, 365, 27938, 260, 2903, 2310, 13, 50816], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 138, "seek": 37424, "start": 384.16, "end": 385.04, "text": " And in the last part,", "tokens": [50860, 400, 294, 264, 1036, 644, 11, 50904], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 139, "seek": 37424, "start": 385.04, "end": 387.36, "text": " we're going to talk about the work that I just presented", "tokens": [50904, 321, 434, 516, 281, 751, 466, 264, 589, 300, 286, 445, 8212, 51020], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 140, "seek": 37424, "start": 387.36, "end": 389.28000000000003, "text": " and that Shiran had a question on.", "tokens": [51020, 293, 300, 27239, 282, 632, 257, 1168, 322, 13, 51116], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 141, "seek": 37424, "start": 390.24, "end": 392.64, "text": " And probably hopefully we'll answer the questions", "tokens": [51164, 400, 1391, 4696, 321, 603, 1867, 264, 1651, 51284], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 142, "seek": 37424, "start": 392.64, "end": 395.2, "text": " and see how Transformer explainability can be used to", "tokens": [51284, 293, 536, 577, 27938, 260, 2903, 2310, 393, 312, 1143, 281, 51412], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 143, "seek": 37424, "start": 395.84000000000003, "end": 400.32, "text": " devise models or maybe incorrect spray-excused", "tokens": [51444, 1905, 908, 5245, 420, 1310, 18424, 8519, 12, 3121, 1149, 292, 51668], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 144, "seek": 37424, "start": 400.32, "end": 401.04, "text": " at the models where.", "tokens": [51668, 412, 264, 5245, 689, 13, 51704], "temperature": 0.0, "avg_logprob": -0.2309846956867817, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.00048716741730459034}, {"id": 145, "seek": 40104, "start": 401.04, "end": 404.8, "text": " So just to set us up,", "tokens": [50364, 407, 445, 281, 992, 505, 493, 11, 50552], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 146, "seek": 40104, "start": 404.8, "end": 406.32, "text": " this is a Transformer architecture", "tokens": [50552, 341, 307, 257, 27938, 260, 9482, 50628], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 147, "seek": 40104, "start": 406.32, "end": 408.0, "text": " as presented in attention is all you need.", "tokens": [50628, 382, 8212, 294, 3202, 307, 439, 291, 643, 13, 50712], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 148, "seek": 40104, "start": 408.0, "end": 409.52000000000004, "text": " I'm sure you're all familiar with it.", "tokens": [50712, 286, 478, 988, 291, 434, 439, 4963, 365, 309, 13, 50788], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 149, "seek": 40104, "start": 410.08000000000004, "end": 412.32, "text": " We will be focusing on the encoder", "tokens": [50816, 492, 486, 312, 8416, 322, 264, 2058, 19866, 50928], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 150, "seek": 40104, "start": 412.32, "end": 414.24, "text": " since the attention mechanism here", "tokens": [50928, 1670, 264, 3202, 7513, 510, 51024], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 151, "seek": 40104, "start": 414.24, "end": 415.84000000000003, "text": " is a self-attention mechanism.", "tokens": [51024, 307, 257, 2698, 12, 1591, 1251, 7513, 13, 51104], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 152, "seek": 40104, "start": 415.84000000000003, "end": 417.04, "text": " And it's quite more intuitive", "tokens": [51104, 400, 309, 311, 1596, 544, 21769, 51164], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 153, "seek": 40104, "start": 417.04, "end": 418.96000000000004, "text": " and easy to grasp and understand.", "tokens": [51164, 293, 1858, 281, 21743, 293, 1223, 13, 51260], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 154, "seek": 40104, "start": 418.96000000000004, "end": 421.20000000000005, "text": " And the concepts that apply here to the encoder", "tokens": [51260, 400, 264, 10392, 300, 3079, 510, 281, 264, 2058, 19866, 51372], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 155, "seek": 40104, "start": 421.20000000000005, "end": 423.68, "text": " are pretty easily generalized to the encoder.", "tokens": [51372, 366, 1238, 3612, 44498, 281, 264, 2058, 19866, 13, 51496], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 156, "seek": 40104, "start": 423.68, "end": 426.32000000000005, "text": " So we're going to focus on the encoder for simplicity", "tokens": [51496, 407, 321, 434, 516, 281, 1879, 322, 264, 2058, 19866, 337, 25632, 51628], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 157, "seek": 40104, "start": 426.32000000000005, "end": 428.64000000000004, "text": " and for intuitive explanations,", "tokens": [51628, 293, 337, 21769, 28708, 11, 51744], "temperature": 0.0, "avg_logprob": -0.1026440033545861, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0002911947958637029}, {"id": 158, "seek": 42864, "start": 428.64, "end": 431.84, "text": " but the principle is quite easily generalized", "tokens": [50364, 457, 264, 8665, 307, 1596, 3612, 44498, 50524], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 159, "seek": 42864, "start": 431.84, "end": 433.03999999999996, "text": " to the encoder as well.", "tokens": [50524, 281, 264, 2058, 19866, 382, 731, 13, 50584], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 160, "seek": 42864, "start": 433.03999999999996, "end": 434.15999999999997, "text": " By the way, if you have questions,", "tokens": [50584, 3146, 264, 636, 11, 498, 291, 362, 1651, 11, 50640], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 161, "seek": 42864, "start": 434.15999999999997, "end": 435.68, "text": " just feel free to stop me.", "tokens": [50640, 445, 841, 1737, 281, 1590, 385, 13, 50716], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 162, "seek": 42864, "start": 437.68, "end": 440.96, "text": " Okay, so let's talk about the intuition behind self-attention.", "tokens": [50816, 1033, 11, 370, 718, 311, 751, 466, 264, 24002, 2261, 2698, 12, 1591, 1251, 13, 50980], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 163, "seek": 42864, "start": 441.52, "end": 443.76, "text": " What self-attention does is actually creates", "tokens": [51008, 708, 2698, 12, 1591, 1251, 775, 307, 767, 7829, 51120], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 164, "seek": 42864, "start": 444.47999999999996, "end": 446.15999999999997, "text": " contextualized representations.", "tokens": [51156, 35526, 1602, 33358, 13, 51240], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 165, "seek": 42864, "start": 446.15999999999997, "end": 447.68, "text": " So I'm sorry for the people on Zoom,", "tokens": [51240, 407, 286, 478, 2597, 337, 264, 561, 322, 13453, 11, 51316], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 166, "seek": 42864, "start": 447.68, "end": 449.52, "text": " but I'm going to write here on the right for it.", "tokens": [51316, 457, 286, 478, 516, 281, 2464, 510, 322, 264, 558, 337, 309, 13, 51408], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 167, "seek": 42864, "start": 449.52, "end": 451.03999999999996, "text": " So we have an example.", "tokens": [51408, 407, 321, 362, 364, 1365, 13, 51484], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 168, "seek": 42864, "start": 451.03999999999996, "end": 452.56, "text": " We're running an example to work with.", "tokens": [51484, 492, 434, 2614, 364, 1365, 281, 589, 365, 13, 51560], "temperature": 0.0, "avg_logprob": -0.14332602818806967, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.00015588890528306365}, {"id": 169, "seek": 45256, "start": 453.12, "end": 455.84, "text": " Say we have the sentence, the count,", "tokens": [50392, 6463, 321, 362, 264, 8174, 11, 264, 1207, 11, 50528], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 170, "seek": 45256, "start": 456.8, "end": 460.24, "text": " set on the max.", "tokens": [50576, 992, 322, 264, 11469, 13, 50748], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 171, "seek": 45256, "start": 462.56, "end": 464.8, "text": " And let's consider the first day in the sentence.", "tokens": [50864, 400, 718, 311, 1949, 264, 700, 786, 294, 264, 8174, 13, 50976], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 172, "seek": 45256, "start": 465.76, "end": 468.56, "text": " We want to create now an embedding", "tokens": [51024, 492, 528, 281, 1884, 586, 364, 12240, 3584, 51164], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 173, "seek": 45256, "start": 468.56, "end": 470.24, "text": " for each one of the words in the sentence,", "tokens": [51164, 337, 1184, 472, 295, 264, 2283, 294, 264, 8174, 11, 51248], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 174, "seek": 45256, "start": 470.24, "end": 472.32, "text": " for each one of the token incidents.", "tokens": [51248, 337, 1184, 472, 295, 264, 14862, 21139, 13, 51352], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 175, "seek": 45256, "start": 472.32, "end": 474.16, "text": " But the token does quite meaningless", "tokens": [51352, 583, 264, 14862, 775, 1596, 33232, 51444], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 176, "seek": 45256, "start": 474.16, "end": 475.44, "text": " with our context, right?", "tokens": [51444, 365, 527, 4319, 11, 558, 30, 51508], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 177, "seek": 45256, "start": 475.44, "end": 476.8, "text": " It could refer to anywhere.", "tokens": [51508, 467, 727, 2864, 281, 4992, 13, 51576], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 178, "seek": 45256, "start": 477.36, "end": 478.96, "text": " So what the attention mechanism does", "tokens": [51604, 407, 437, 264, 3202, 7513, 775, 51684], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 179, "seek": 45256, "start": 478.96, "end": 481.52, "text": " is it actually creates contextualized representation.", "tokens": [51684, 307, 309, 767, 7829, 35526, 1602, 10290, 13, 51812], "temperature": 0.0, "avg_logprob": -0.19346199748672058, "compression_ratio": 1.7008547008547008, "no_speech_prob": 0.0015475229592993855}, {"id": 180, "seek": 48152, "start": 482.15999999999997, "end": 485.91999999999996, "text": " It should take information from the other token", "tokens": [50396, 467, 820, 747, 1589, 490, 264, 661, 14862, 50584], "temperature": 0.0, "avg_logprob": -0.1694876940354057, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.0007202510605566204}, {"id": 181, "seek": 48152, "start": 485.91999999999996, "end": 489.91999999999996, "text": " and insert it to the current token that we're interested in.", "tokens": [50584, 293, 8969, 309, 281, 264, 2190, 14862, 300, 321, 434, 3102, 294, 13, 50784], "temperature": 0.0, "avg_logprob": -0.1694876940354057, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.0007202510605566204}, {"id": 182, "seek": 48152, "start": 490.47999999999996, "end": 493.44, "text": " So for example, intuitively, maybe we would expect", "tokens": [50812, 407, 337, 1365, 11, 46506, 11, 1310, 321, 576, 2066, 50960], "temperature": 0.0, "avg_logprob": -0.1694876940354057, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.0007202510605566204}, {"id": 183, "seek": 48152, "start": 493.44, "end": 497.35999999999996, "text": " that since the word the refers to the word cat,", "tokens": [50960, 300, 1670, 264, 1349, 264, 14942, 281, 264, 1349, 3857, 11, 51156], "temperature": 0.0, "avg_logprob": -0.1694876940354057, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.0007202510605566204}, {"id": 184, "seek": 48152, "start": 498.47999999999996, "end": 502.24, "text": " information from the word cat will be moved into the word the,", "tokens": [51212, 1589, 490, 264, 1349, 3857, 486, 312, 4259, 666, 264, 1349, 264, 11, 51400], "temperature": 0.0, "avg_logprob": -0.1694876940354057, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.0007202510605566204}, {"id": 185, "seek": 48152, "start": 502.24, "end": 505.68, "text": " such that the embedding of the word that is contextualized", "tokens": [51400, 1270, 300, 264, 12240, 3584, 295, 264, 1349, 300, 307, 35526, 1602, 51572], "temperature": 0.0, "avg_logprob": -0.1694876940354057, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.0007202510605566204}, {"id": 186, "seek": 48152, "start": 505.68, "end": 509.28, "text": " is enriched by context from the word cat.", "tokens": [51572, 307, 48624, 538, 4319, 490, 264, 1349, 3857, 13, 51752], "temperature": 0.0, "avg_logprob": -0.1694876940354057, "compression_ratio": 1.8186274509803921, "no_speech_prob": 0.0007202510605566204}, {"id": 187, "seek": 50928, "start": 510.23999999999995, "end": 512.24, "text": " So what the attention mechanism does", "tokens": [50412, 407, 437, 264, 3202, 7513, 775, 50512], "temperature": 0.0, "avg_logprob": -0.15607681274414062, "compression_ratio": 1.6863636363636363, "no_speech_prob": 9.168983524432406e-05}, {"id": 188, "seek": 50928, "start": 512.24, "end": 516.16, "text": " is it actually uses query, key, and value matrices.", "tokens": [50512, 307, 309, 767, 4960, 14581, 11, 2141, 11, 293, 2158, 32284, 13, 50708], "temperature": 0.0, "avg_logprob": -0.15607681274414062, "compression_ratio": 1.6863636363636363, "no_speech_prob": 9.168983524432406e-05}, {"id": 189, "seek": 50928, "start": 516.16, "end": 520.88, "text": " And we can think about it maybe as a database theory information.", "tokens": [50708, 400, 321, 393, 519, 466, 309, 1310, 382, 257, 8149, 5261, 1589, 13, 50944], "temperature": 0.0, "avg_logprob": -0.15607681274414062, "compression_ratio": 1.6863636363636363, "no_speech_prob": 9.168983524432406e-05}, {"id": 190, "seek": 50928, "start": 521.4399999999999, "end": 524.9599999999999, "text": " So when we talk about databases, we have queries,", "tokens": [50972, 407, 562, 321, 751, 466, 22380, 11, 321, 362, 24109, 11, 51148], "temperature": 0.0, "avg_logprob": -0.15607681274414062, "compression_ratio": 1.6863636363636363, "no_speech_prob": 9.168983524432406e-05}, {"id": 191, "seek": 50928, "start": 526.88, "end": 529.68, "text": " which are questions that we run on our database.", "tokens": [51244, 597, 366, 1651, 300, 321, 1190, 322, 527, 8149, 13, 51384], "temperature": 0.0, "avg_logprob": -0.15607681274414062, "compression_ratio": 1.6863636363636363, "no_speech_prob": 9.168983524432406e-05}, {"id": 192, "seek": 50928, "start": 530.48, "end": 531.68, "text": " We have keys.", "tokens": [51424, 492, 362, 9317, 13, 51484], "temperature": 0.0, "avg_logprob": -0.15607681274414062, "compression_ratio": 1.6863636363636363, "no_speech_prob": 9.168983524432406e-05}, {"id": 193, "seek": 50928, "start": 531.68, "end": 534.16, "text": " The keys represent the entries in our database.", "tokens": [51484, 440, 9317, 2906, 264, 23041, 294, 527, 8149, 13, 51608], "temperature": 0.0, "avg_logprob": -0.15607681274414062, "compression_ratio": 1.6863636363636363, "no_speech_prob": 9.168983524432406e-05}, {"id": 194, "seek": 50928, "start": 534.8, "end": 537.8399999999999, "text": " And we have values that corresponds to the keys, right?", "tokens": [51640, 400, 321, 362, 4190, 300, 23249, 281, 264, 9317, 11, 558, 30, 51792], "temperature": 0.0, "avg_logprob": -0.15607681274414062, "compression_ratio": 1.6863636363636363, "no_speech_prob": 9.168983524432406e-05}, {"id": 195, "seek": 53784, "start": 537.84, "end": 541.84, "text": " So what we're doing here is we're actually running a query", "tokens": [50364, 407, 437, 321, 434, 884, 510, 307, 321, 434, 767, 2614, 257, 14581, 50564], "temperature": 0.0, "avg_logprob": -0.1815394549302652, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.00014421217201743275}, {"id": 196, "seek": 53784, "start": 541.84, "end": 552.0, "text": " asking which tokens are relevant to the...", "tokens": [50564, 3365, 597, 22667, 366, 7340, 281, 264, 485, 51072], "temperature": 0.0, "avg_logprob": -0.1815394549302652, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.00014421217201743275}, {"id": 197, "seek": 53784, "start": 554.1600000000001, "end": 556.5600000000001, "text": " We can think about it intuitively as running a query", "tokens": [51180, 492, 393, 519, 466, 309, 46506, 382, 2614, 257, 14581, 51300], "temperature": 0.0, "avg_logprob": -0.1815394549302652, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.00014421217201743275}, {"id": 198, "seek": 53784, "start": 556.5600000000001, "end": 558.5600000000001, "text": " on all of these tokens that we have.", "tokens": [51300, 322, 439, 295, 613, 22667, 300, 321, 362, 13, 51400], "temperature": 0.0, "avg_logprob": -0.1815394549302652, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.00014421217201743275}, {"id": 199, "seek": 53784, "start": 559.12, "end": 561.52, "text": " And then the keys represent all the other tokens.", "tokens": [51428, 400, 550, 264, 9317, 2906, 439, 264, 661, 22667, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1815394549302652, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.00014421217201743275}, {"id": 200, "seek": 53784, "start": 562.08, "end": 563.84, "text": " What we do with the attention mechanism", "tokens": [51576, 708, 321, 360, 365, 264, 3202, 7513, 51664], "temperature": 0.0, "avg_logprob": -0.1815394549302652, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.00014421217201743275}, {"id": 201, "seek": 56384, "start": 564.1600000000001, "end": 566.96, "text": " is we calculate an attention matrix", "tokens": [50380, 307, 321, 8873, 364, 3202, 8141, 50520], "temperature": 0.0, "avg_logprob": -0.34136093436897574, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0005355168832466006}, {"id": 202, "seek": 56384, "start": 566.96, "end": 569.84, "text": " that is going to be the star of every transformer experience", "tokens": [50520, 300, 307, 516, 281, 312, 264, 3543, 295, 633, 31782, 1752, 50664], "temperature": 0.0, "avg_logprob": -0.34136093436897574, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0005355168832466006}, {"id": 203, "seek": 56384, "start": 569.84, "end": 570.96, "text": " ability algorithm.", "tokens": [50664, 3485, 9284, 13, 50720], "temperature": 0.0, "avg_logprob": -0.34136093436897574, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0005355168832466006}, {"id": 204, "seek": 56384, "start": 571.84, "end": 575.84, "text": " It's going to be a soft mass of the multiplication", "tokens": [50764, 467, 311, 516, 281, 312, 257, 2787, 2758, 295, 264, 27290, 50964], "temperature": 0.0, "avg_logprob": -0.34136093436897574, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0005355168832466006}, {"id": 205, "seek": 56384, "start": 575.84, "end": 580.96, "text": " between queries and keys normalized by the embedding dimension.", "tokens": [50964, 1296, 24109, 293, 9317, 48704, 538, 264, 12240, 3584, 10139, 13, 51220], "temperature": 0.0, "avg_logprob": -0.34136093436897574, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0005355168832466006}, {"id": 206, "seek": 56384, "start": 581.84, "end": 585.84, "text": " This similarity scores are actually telling us", "tokens": [51264, 639, 32194, 13444, 366, 767, 3585, 505, 51464], "temperature": 0.0, "avg_logprob": -0.34136093436897574, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0005355168832466006}, {"id": 207, "seek": 56384, "start": 585.84, "end": 589.84, "text": " how much each word is relevant to our word of interest.", "tokens": [51464, 577, 709, 1184, 1349, 307, 7340, 281, 527, 1349, 295, 1179, 13, 51664], "temperature": 0.0, "avg_logprob": -0.34136093436897574, "compression_ratio": 1.5560747663551402, "no_speech_prob": 0.0005355168832466006}, {"id": 208, "seek": 58984, "start": 590.8000000000001, "end": 592.64, "text": " So the multiplication between queries and keys,", "tokens": [50412, 407, 264, 27290, 1296, 24109, 293, 9317, 11, 50504], "temperature": 0.0, "avg_logprob": -0.36449546813964845, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0003199188213329762}, {"id": 209, "seek": 58984, "start": 592.64, "end": 595.84, "text": " we can think about it kind of like as relevant scores.", "tokens": [50504, 321, 393, 519, 466, 309, 733, 295, 411, 382, 7340, 13444, 13, 50664], "temperature": 0.0, "avg_logprob": -0.36449546813964845, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0003199188213329762}, {"id": 210, "seek": 58984, "start": 595.84, "end": 599.0400000000001, "text": " How much is each token relevant to the token that...", "tokens": [50664, 1012, 709, 307, 1184, 14862, 7340, 281, 264, 14862, 300, 485, 50824], "temperature": 0.0, "avg_logprob": -0.36449546813964845, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0003199188213329762}, {"id": 211, "seek": 58984, "start": 599.0400000000001, "end": 600.24, "text": " to the word that...", "tokens": [50824, 281, 264, 1349, 300, 485, 50884], "temperature": 0.0, "avg_logprob": -0.36449546813964845, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0003199188213329762}, {"id": 212, "seek": 58984, "start": 603.0400000000001, "end": 606.24, "text": " And after we calculate those similarity scores,", "tokens": [51024, 400, 934, 321, 8873, 729, 32194, 13444, 11, 51184], "temperature": 0.0, "avg_logprob": -0.36449546813964845, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0003199188213329762}, {"id": 213, "seek": 58984, "start": 606.24, "end": 608.48, "text": " we create the enriched representation", "tokens": [51184, 321, 1884, 264, 48624, 10290, 51296], "temperature": 0.0, "avg_logprob": -0.36449546813964845, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0003199188213329762}, {"id": 214, "seek": 58984, "start": 609.2, "end": 611.84, "text": " by multiplying the scores by the values.", "tokens": [51332, 538, 30955, 264, 13444, 538, 264, 4190, 13, 51464], "temperature": 0.0, "avg_logprob": -0.36449546813964845, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0003199188213329762}, {"id": 215, "seek": 58984, "start": 613.0400000000001, "end": 616.64, "text": " Such that each word gets information from the other words", "tokens": [51524, 9653, 300, 1184, 1349, 2170, 1589, 490, 264, 661, 2283, 51704], "temperature": 0.0, "avg_logprob": -0.36449546813964845, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.0003199188213329762}, {"id": 216, "seek": 61664, "start": 617.36, "end": 618.64, "text": " by these relevance values.", "tokens": [50400, 538, 613, 32684, 4190, 13, 50464], "temperature": 0.0, "avg_logprob": -0.27889403174905214, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0002570580109022558}, {"id": 217, "seek": 61664, "start": 618.64, "end": 621.4399999999999, "text": " So these relevance values determine how much each word", "tokens": [50464, 407, 613, 32684, 4190, 6997, 577, 709, 1184, 1349, 50604], "temperature": 0.0, "avg_logprob": -0.27889403174905214, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0002570580109022558}, {"id": 218, "seek": 61664, "start": 621.4399999999999, "end": 624.64, "text": " is going to influence the word of after the attention.", "tokens": [50604, 307, 516, 281, 6503, 264, 1349, 295, 934, 264, 3202, 13, 50764], "temperature": 0.0, "avg_logprob": -0.27889403174905214, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0002570580109022558}, {"id": 219, "seek": 61664, "start": 626.64, "end": 629.4399999999999, "text": " So this is going to be the key intuition to everything", "tokens": [50864, 407, 341, 307, 516, 281, 312, 264, 2141, 24002, 281, 1203, 51004], "temperature": 0.0, "avg_logprob": -0.27889403174905214, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0002570580109022558}, {"id": 220, "seek": 61664, "start": 629.4399999999999, "end": 632.24, "text": " that we do later on to explain a transformers.", "tokens": [51004, 300, 321, 360, 1780, 322, 281, 2903, 257, 4088, 433, 13, 51144], "temperature": 0.0, "avg_logprob": -0.27889403174905214, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0002570580109022558}, {"id": 221, "seek": 61664, "start": 634.64, "end": 636.64, "text": " The most important thing to remember about explaining", "tokens": [51264, 440, 881, 1021, 551, 281, 1604, 466, 13468, 51364], "temperature": 0.0, "avg_logprob": -0.27889403174905214, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0002570580109022558}, {"id": 222, "seek": 61664, "start": 636.64, "end": 640.64, "text": " transformers is we don't have just a single attention matrix.", "tokens": [51364, 4088, 433, 307, 321, 500, 380, 362, 445, 257, 2167, 3202, 8141, 13, 51564], "temperature": 0.0, "avg_logprob": -0.27889403174905214, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0002570580109022558}, {"id": 223, "seek": 61664, "start": 640.64, "end": 642.64, "text": " This mechanism happens H times,", "tokens": [51564, 639, 7513, 2314, 389, 1413, 11, 51664], "temperature": 0.0, "avg_logprob": -0.27889403174905214, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0002570580109022558}, {"id": 224, "seek": 61664, "start": 642.64, "end": 644.64, "text": " where H is the number of attention heads", "tokens": [51664, 689, 389, 307, 264, 1230, 295, 3202, 8050, 51764], "temperature": 0.0, "avg_logprob": -0.27889403174905214, "compression_ratio": 1.8093220338983051, "no_speech_prob": 0.0002570580109022558}, {"id": 225, "seek": 64464, "start": 644.64, "end": 645.64, "text": " that we have.", "tokens": [50364, 300, 321, 362, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 226, "seek": 64464, "start": 645.64, "end": 647.64, "text": " And intuitively, we can think about it as,", "tokens": [50414, 400, 46506, 11, 321, 393, 519, 466, 309, 382, 11, 50514], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 227, "seek": 64464, "start": 647.64, "end": 649.64, "text": " you know, in CNNs, you have kernels.", "tokens": [50514, 291, 458, 11, 294, 24859, 82, 11, 291, 362, 23434, 1625, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 228, "seek": 64464, "start": 649.64, "end": 651.64, "text": " Each kernel has its own purpose.", "tokens": [50614, 6947, 28256, 575, 1080, 1065, 4334, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 229, "seek": 64464, "start": 651.64, "end": 654.64, "text": " Some refer to the background, some refer to the edges,", "tokens": [50714, 2188, 2864, 281, 264, 3678, 11, 512, 2864, 281, 264, 8819, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 230, "seek": 64464, "start": 654.64, "end": 656.64, "text": " the shapes, and so on.", "tokens": [50864, 264, 10854, 11, 293, 370, 322, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 231, "seek": 64464, "start": 656.64, "end": 658.64, "text": " Transformers have the same thing with attention heads.", "tokens": [50964, 27938, 433, 362, 264, 912, 551, 365, 3202, 8050, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 232, "seek": 64464, "start": 658.64, "end": 660.64, "text": " So each attention head can have a different purpose.", "tokens": [51064, 407, 1184, 3202, 1378, 393, 362, 257, 819, 4334, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 233, "seek": 64464, "start": 660.64, "end": 662.64, "text": " And actually, researchers have shown", "tokens": [51164, 400, 767, 11, 10309, 362, 4898, 51264], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 234, "seek": 64464, "start": 662.64, "end": 666.64, "text": " that you can probably prune most of the attention head", "tokens": [51264, 300, 291, 393, 1391, 582, 2613, 881, 295, 264, 3202, 1378, 51464], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 235, "seek": 64464, "start": 666.64, "end": 669.64, "text": " and achieve the same accuracy,", "tokens": [51464, 293, 4584, 264, 912, 14170, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 236, "seek": 64464, "start": 669.64, "end": 672.64, "text": " which means that most attention heads are really not important", "tokens": [51614, 597, 1355, 300, 881, 3202, 8050, 366, 534, 406, 1021, 51764], "temperature": 0.0, "avg_logprob": -0.1400525187718049, "compression_ratio": 1.8444444444444446, "no_speech_prob": 0.00012531191168818623}, {"id": 237, "seek": 67264, "start": 673.64, "end": 676.64, "text": " to the prediction, to a specific prediction of the model.", "tokens": [50414, 281, 264, 17630, 11, 281, 257, 2685, 17630, 295, 264, 2316, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1422100067138672, "compression_ratio": 1.9087136929460582, "no_speech_prob": 0.0001419640757376328}, {"id": 238, "seek": 67264, "start": 676.64, "end": 679.64, "text": " So it's really important when we think about transformers", "tokens": [50564, 407, 309, 311, 534, 1021, 562, 321, 519, 466, 4088, 433, 50714], "temperature": 0.0, "avg_logprob": -0.1422100067138672, "compression_ratio": 1.9087136929460582, "no_speech_prob": 0.0001419640757376328}, {"id": 239, "seek": 67264, "start": 679.64, "end": 682.64, "text": " and to understand that the different heads have different needs.", "tokens": [50714, 293, 281, 1223, 300, 264, 819, 8050, 362, 819, 2203, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1422100067138672, "compression_ratio": 1.9087136929460582, "no_speech_prob": 0.0001419640757376328}, {"id": 240, "seek": 67264, "start": 685.64, "end": 688.64, "text": " The final thing that we need to remember about transformer,", "tokens": [51014, 440, 2572, 551, 300, 321, 643, 281, 1604, 466, 31782, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1422100067138672, "compression_ratio": 1.9087136929460582, "no_speech_prob": 0.0001419640757376328}, {"id": 241, "seek": 67264, "start": 688.64, "end": 691.64, "text": " you know, predictions is that transformers use", "tokens": [51164, 291, 458, 11, 21264, 307, 300, 4088, 433, 764, 51314], "temperature": 0.0, "avg_logprob": -0.1422100067138672, "compression_ratio": 1.9087136929460582, "no_speech_prob": 0.0001419640757376328}, {"id": 242, "seek": 67264, "start": 691.64, "end": 694.64, "text": " a classification token for the prediction.", "tokens": [51314, 257, 21538, 14862, 337, 264, 17630, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1422100067138672, "compression_ratio": 1.9087136929460582, "no_speech_prob": 0.0001419640757376328}, {"id": 243, "seek": 67264, "start": 694.64, "end": 696.64, "text": " So once the entire attention mechanism is done", "tokens": [51464, 407, 1564, 264, 2302, 3202, 7513, 307, 1096, 51564], "temperature": 0.0, "avg_logprob": -0.1422100067138672, "compression_ratio": 1.9087136929460582, "no_speech_prob": 0.0001419640757376328}, {"id": 244, "seek": 67264, "start": 696.64, "end": 698.64, "text": " and all the tokens are contextualized,", "tokens": [51564, 293, 439, 264, 22667, 366, 35526, 1602, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1422100067138672, "compression_ratio": 1.9087136929460582, "no_speech_prob": 0.0001419640757376328}, {"id": 245, "seek": 67264, "start": 698.64, "end": 701.64, "text": " the classification token is the only token.", "tokens": [51664, 264, 21538, 14862, 307, 264, 787, 14862, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1422100067138672, "compression_ratio": 1.9087136929460582, "no_speech_prob": 0.0001419640757376328}, {"id": 246, "seek": 70164, "start": 702.64, "end": 704.64, "text": " That is used to make the prediction.", "tokens": [50414, 663, 307, 1143, 281, 652, 264, 17630, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 247, "seek": 70164, "start": 704.64, "end": 706.64, "text": " There's a linear layer on top of the classification token", "tokens": [50514, 821, 311, 257, 8213, 4583, 322, 1192, 295, 264, 21538, 14862, 50614], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 248, "seek": 70164, "start": 706.64, "end": 708.64, "text": " and then the prediction is made.", "tokens": [50614, 293, 550, 264, 17630, 307, 1027, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 249, "seek": 70164, "start": 708.64, "end": 710.64, "text": " So basically what the classification token does", "tokens": [50714, 407, 1936, 437, 264, 21538, 14862, 775, 50814], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 250, "seek": 70164, "start": 710.64, "end": 714.64, "text": " is kind of like creates an aggregated representation", "tokens": [50814, 307, 733, 295, 411, 7829, 364, 16743, 770, 10290, 51014], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 251, "seek": 70164, "start": 714.64, "end": 716.64, "text": " of the entire input.", "tokens": [51014, 295, 264, 2302, 4846, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 252, "seek": 70164, "start": 716.64, "end": 718.64, "text": " You can think about it as a global representation", "tokens": [51114, 509, 393, 519, 466, 309, 382, 257, 4338, 10290, 51214], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 253, "seek": 70164, "start": 718.64, "end": 720.64, "text": " of all the tokens in the input.", "tokens": [51214, 295, 439, 264, 22667, 294, 264, 4846, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 254, "seek": 70164, "start": 720.64, "end": 722.64, "text": " You have questions so far,", "tokens": [51314, 509, 362, 1651, 370, 1400, 11, 51414], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 255, "seek": 70164, "start": 722.64, "end": 725.64, "text": " because we're going to move on to the interesting stuff.", "tokens": [51414, 570, 321, 434, 516, 281, 1286, 322, 281, 264, 1880, 1507, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 256, "seek": 70164, "start": 725.64, "end": 727.64, "text": " Yeah.", "tokens": [51564, 865, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 257, "seek": 70164, "start": 727.64, "end": 729.64, "text": " So moving on to transformer explainability,", "tokens": [51664, 407, 2684, 322, 281, 31782, 2903, 2310, 11, 51764], "temperature": 0.0, "avg_logprob": -0.13331735740273687, "compression_ratio": 1.802325581395349, "no_speech_prob": 0.0005270636174827814}, {"id": 258, "seek": 72964, "start": 729.64, "end": 732.64, "text": " it's really important to set up our goals.", "tokens": [50364, 309, 311, 534, 1021, 281, 992, 493, 527, 5493, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09677493383014013, "compression_ratio": 1.9586776859504131, "no_speech_prob": 0.0006874740938656032}, {"id": 259, "seek": 72964, "start": 732.64, "end": 735.64, "text": " My goal is to facilitate explanations that help you guys,", "tokens": [50514, 1222, 3387, 307, 281, 20207, 28708, 300, 854, 291, 1074, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09677493383014013, "compression_ratio": 1.9586776859504131, "no_speech_prob": 0.0006874740938656032}, {"id": 260, "seek": 72964, "start": 735.64, "end": 738.64, "text": " the researchers that actually use the models.", "tokens": [50664, 264, 10309, 300, 767, 764, 264, 5245, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09677493383014013, "compression_ratio": 1.9586776859504131, "no_speech_prob": 0.0006874740938656032}, {"id": 261, "seek": 72964, "start": 738.64, "end": 741.64, "text": " And the way that we do that is by creating hitmaps.", "tokens": [50814, 400, 264, 636, 300, 321, 360, 300, 307, 538, 4084, 2045, 76, 2382, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09677493383014013, "compression_ratio": 1.9586776859504131, "no_speech_prob": 0.0006874740938656032}, {"id": 262, "seek": 72964, "start": 741.64, "end": 744.64, "text": " So the hitmaps should correspond to the pixels,", "tokens": [50964, 407, 264, 2045, 76, 2382, 820, 6805, 281, 264, 18668, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09677493383014013, "compression_ratio": 1.9586776859504131, "no_speech_prob": 0.0006874740938656032}, {"id": 263, "seek": 72964, "start": 744.64, "end": 747.64, "text": " if we're speaking of images or if we're speaking of text,", "tokens": [51114, 498, 321, 434, 4124, 295, 5267, 420, 498, 321, 434, 4124, 295, 2487, 11, 51264], "temperature": 0.0, "avg_logprob": -0.09677493383014013, "compression_ratio": 1.9586776859504131, "no_speech_prob": 0.0006874740938656032}, {"id": 264, "seek": 72964, "start": 747.64, "end": 750.64, "text": " and hitmaps should correspond to the tokens.", "tokens": [51264, 293, 2045, 76, 2382, 820, 6805, 281, 264, 22667, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09677493383014013, "compression_ratio": 1.9586776859504131, "no_speech_prob": 0.0006874740938656032}, {"id": 265, "seek": 72964, "start": 750.64, "end": 752.64, "text": " The hitmaps should correspond to the pixels", "tokens": [51414, 440, 2045, 76, 2382, 820, 6805, 281, 264, 18668, 51514], "temperature": 0.0, "avg_logprob": -0.09677493383014013, "compression_ratio": 1.9586776859504131, "no_speech_prob": 0.0006874740938656032}, {"id": 266, "seek": 72964, "start": 752.64, "end": 754.64, "text": " that influence the prediction by the model.", "tokens": [51514, 300, 6503, 264, 17630, 538, 264, 2316, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09677493383014013, "compression_ratio": 1.9586776859504131, "no_speech_prob": 0.0006874740938656032}, {"id": 267, "seek": 72964, "start": 754.64, "end": 756.64, "text": " So for example, here we see the verb", "tokens": [51614, 407, 337, 1365, 11, 510, 321, 536, 264, 9595, 51714], "temperature": 0.0, "avg_logprob": -0.09677493383014013, "compression_ratio": 1.9586776859504131, "no_speech_prob": 0.0006874740938656032}, {"id": 268, "seek": 75664, "start": 756.64, "end": 759.64, "text": " and the hitmaps actually highlights the pixels", "tokens": [50364, 293, 264, 2045, 76, 2382, 767, 14254, 264, 18668, 50514], "temperature": 0.0, "avg_logprob": -0.11386478424072266, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006254923646338284}, {"id": 269, "seek": 75664, "start": 759.64, "end": 761.64, "text": " relating to the verb.", "tokens": [50514, 23968, 281, 264, 9595, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11386478424072266, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006254923646338284}, {"id": 270, "seek": 75664, "start": 761.64, "end": 765.64, "text": " And the toothbrush or the ship or the bikes and so on.", "tokens": [50614, 400, 264, 37568, 420, 264, 5374, 420, 264, 16035, 293, 370, 322, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11386478424072266, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006254923646338284}, {"id": 271, "seek": 75664, "start": 765.64, "end": 768.64, "text": " So the hitmaps should tell us which pixels in the input", "tokens": [50814, 407, 264, 2045, 76, 2382, 820, 980, 505, 597, 18668, 294, 264, 4846, 50964], "temperature": 0.0, "avg_logprob": -0.11386478424072266, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006254923646338284}, {"id": 272, "seek": 75664, "start": 768.64, "end": 773.64, "text": " make the prediction as it is.", "tokens": [50964, 652, 264, 17630, 382, 309, 307, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11386478424072266, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006254923646338284}, {"id": 273, "seek": 75664, "start": 773.64, "end": 775.64, "text": " Okay, we got to the interesting part.", "tokens": [51214, 1033, 11, 321, 658, 281, 264, 1880, 644, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11386478424072266, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006254923646338284}, {"id": 274, "seek": 75664, "start": 775.64, "end": 777.64, "text": " Yeah.", "tokens": [51314, 865, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11386478424072266, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006254923646338284}, {"id": 275, "seek": 75664, "start": 777.64, "end": 779.64, "text": " When you talk about transformer explainability,", "tokens": [51414, 1133, 291, 751, 466, 31782, 2903, 2310, 11, 51514], "temperature": 0.0, "avg_logprob": -0.11386478424072266, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006254923646338284}, {"id": 276, "seek": 75664, "start": 779.64, "end": 782.64, "text": " researchers have looked at this attention matrix", "tokens": [51514, 10309, 362, 2956, 412, 341, 3202, 8141, 51664], "temperature": 0.0, "avg_logprob": -0.11386478424072266, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006254923646338284}, {"id": 277, "seek": 75664, "start": 782.64, "end": 784.64, "text": " and asked the question,", "tokens": [51664, 293, 2351, 264, 1168, 11, 51764], "temperature": 0.0, "avg_logprob": -0.11386478424072266, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006254923646338284}, {"id": 278, "seek": 78464, "start": 784.64, "end": 787.64, "text": " is this attention matrix an explanation?", "tokens": [50364, 307, 341, 3202, 8141, 364, 10835, 30, 50514], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 279, "seek": 78464, "start": 787.64, "end": 789.64, "text": " How can it be an explanation?", "tokens": [50514, 1012, 393, 309, 312, 364, 10835, 30, 50614], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 280, "seek": 78464, "start": 789.64, "end": 792.64, "text": " Because we said that the attention values are actually", "tokens": [50614, 1436, 321, 848, 300, 264, 3202, 4190, 366, 767, 50764], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 281, "seek": 78464, "start": 792.64, "end": 794.64, "text": " kind of like relevance values, right?", "tokens": [50764, 733, 295, 411, 32684, 4190, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 282, "seek": 78464, "start": 794.64, "end": 797.64, "text": " There are values that reflect how much each token", "tokens": [50864, 821, 366, 4190, 300, 5031, 577, 709, 1184, 14862, 51014], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 283, "seek": 78464, "start": 797.64, "end": 799.64, "text": " influences each other's token.", "tokens": [51014, 21222, 1184, 661, 311, 14862, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 284, "seek": 78464, "start": 799.64, "end": 802.64, "text": " And we also said that the classification token", "tokens": [51114, 400, 321, 611, 848, 300, 264, 21538, 14862, 51264], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 285, "seek": 78464, "start": 802.64, "end": 805.64, "text": " is the only token that is used for the prediction, right?", "tokens": [51264, 307, 264, 787, 14862, 300, 307, 1143, 337, 264, 17630, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 286, "seek": 78464, "start": 805.64, "end": 808.64, "text": " So if we look at the row in the attention matrix,", "tokens": [51414, 407, 498, 321, 574, 412, 264, 5386, 294, 264, 3202, 8141, 11, 51564], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 287, "seek": 78464, "start": 808.64, "end": 810.64, "text": " that corresponds to the classification token,", "tokens": [51564, 300, 23249, 281, 264, 21538, 14862, 11, 51664], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 288, "seek": 78464, "start": 810.64, "end": 812.64, "text": " and look at these relevance values,", "tokens": [51664, 293, 574, 412, 613, 32684, 4190, 11, 51764], "temperature": 0.0, "avg_logprob": -0.09852007515410073, "compression_ratio": 1.9794238683127572, "no_speech_prob": 0.0009690510341897607}, {"id": 289, "seek": 81264, "start": 812.64, "end": 814.64, "text": " these should be the relevance values that determine", "tokens": [50364, 613, 820, 312, 264, 32684, 4190, 300, 6997, 50464], "temperature": 0.0, "avg_logprob": -0.0913400069527004, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0013240904081612825}, {"id": 290, "seek": 81264, "start": 814.64, "end": 817.64, "text": " how much each token influences the classification token,", "tokens": [50464, 577, 709, 1184, 14862, 21222, 264, 21538, 14862, 11, 50614], "temperature": 0.0, "avg_logprob": -0.0913400069527004, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0013240904081612825}, {"id": 291, "seek": 81264, "start": 817.64, "end": 820.64, "text": " which is basically how much each token influences", "tokens": [50614, 597, 307, 1936, 577, 709, 1184, 14862, 21222, 50764], "temperature": 0.0, "avg_logprob": -0.0913400069527004, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0013240904081612825}, {"id": 292, "seek": 81264, "start": 820.64, "end": 822.64, "text": " the classification, right?", "tokens": [50764, 264, 21538, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.0913400069527004, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0013240904081612825}, {"id": 293, "seek": 81264, "start": 822.64, "end": 825.64, "text": " So maybe these values are just the relevance values.", "tokens": [50864, 407, 1310, 613, 4190, 366, 445, 264, 32684, 4190, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0913400069527004, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0013240904081612825}, {"id": 294, "seek": 81264, "start": 825.64, "end": 827.64, "text": " Each token represents a patch in the image.", "tokens": [51014, 6947, 14862, 8855, 257, 9972, 294, 264, 3256, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0913400069527004, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0013240904081612825}, {"id": 295, "seek": 81264, "start": 827.64, "end": 830.64, "text": " Maybe these are just the values that we need.", "tokens": [51114, 2704, 613, 366, 445, 264, 4190, 300, 321, 643, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0913400069527004, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0013240904081612825}, {"id": 296, "seek": 81264, "start": 830.64, "end": 834.64, "text": " And we're all done just like decision trees are self-explanable", "tokens": [51264, 400, 321, 434, 439, 1096, 445, 411, 3537, 5852, 366, 2698, 12, 3121, 16554, 712, 51464], "temperature": 0.0, "avg_logprob": -0.0913400069527004, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0013240904081612825}, {"id": 297, "seek": 81264, "start": 834.64, "end": 837.64, "text": " or linear regression is self-explanable.", "tokens": [51464, 420, 8213, 24590, 307, 2698, 12, 3121, 16554, 712, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0913400069527004, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0013240904081612825}, {"id": 298, "seek": 81264, "start": 837.64, "end": 839.64, "text": " What do you think? Are we done?", "tokens": [51614, 708, 360, 291, 519, 30, 2014, 321, 1096, 30, 51714], "temperature": 0.0, "avg_logprob": -0.0913400069527004, "compression_ratio": 1.921487603305785, "no_speech_prob": 0.0013240904081612825}, {"id": 299, "seek": 83964, "start": 839.64, "end": 840.64, "text": " We're done.", "tokens": [50364, 492, 434, 1096, 13, 50414], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 300, "seek": 83964, "start": 840.64, "end": 842.64, "text": " Yeah, we're done.", "tokens": [50414, 865, 11, 321, 434, 1096, 13, 50514], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 301, "seek": 83964, "start": 842.64, "end": 843.64, "text": " We're done.", "tokens": [50514, 492, 434, 1096, 13, 50564], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 302, "seek": 83964, "start": 843.64, "end": 844.64, "text": " Yeah, we're done.", "tokens": [50564, 865, 11, 321, 434, 1096, 13, 50614], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 303, "seek": 83964, "start": 844.64, "end": 845.64, "text": " We're done.", "tokens": [50614, 492, 434, 1096, 13, 50664], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 304, "seek": 83964, "start": 845.64, "end": 846.64, "text": " We're done.", "tokens": [50664, 492, 434, 1096, 13, 50714], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 305, "seek": 83964, "start": 846.64, "end": 847.64, "text": " We're done.", "tokens": [50714, 492, 434, 1096, 13, 50764], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 306, "seek": 83964, "start": 847.64, "end": 848.64, "text": " We're done.", "tokens": [50764, 492, 434, 1096, 13, 50814], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 307, "seek": 83964, "start": 848.64, "end": 849.64, "text": " Yeah.", "tokens": [50814, 865, 13, 50864], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 308, "seek": 83964, "start": 849.64, "end": 850.64, "text": " Yeah.", "tokens": [50864, 865, 13, 50914], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 309, "seek": 83964, "start": 850.64, "end": 852.64, "text": " The attention matrix is used to multiply the value representation.", "tokens": [50914, 440, 3202, 8141, 307, 1143, 281, 12972, 264, 2158, 10290, 13, 51014], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 310, "seek": 83964, "start": 852.64, "end": 853.64, "text": " Yeah.", "tokens": [51014, 865, 13, 51064], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 311, "seek": 83964, "start": 853.64, "end": 855.64, "text": " The representation should be positive, negative,", "tokens": [51064, 440, 10290, 820, 312, 3353, 11, 3671, 11, 51164], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 312, "seek": 83964, "start": 855.64, "end": 856.64, "text": " large, small.", "tokens": [51164, 2416, 11, 1359, 13, 51214], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 313, "seek": 83964, "start": 856.64, "end": 859.64, "text": " It doesn't actually tell us how much it is actually contributing", "tokens": [51214, 467, 1177, 380, 767, 980, 505, 577, 709, 309, 307, 767, 19270, 51364], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 314, "seek": 83964, "start": 859.64, "end": 860.64, "text": " to the final classification.", "tokens": [51364, 281, 264, 2572, 21538, 13, 51414], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 315, "seek": 83964, "start": 860.64, "end": 861.64, "text": " Yeah.", "tokens": [51414, 865, 13, 51464], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 316, "seek": 83964, "start": 861.64, "end": 862.64, "text": " I mean,", "tokens": [51464, 286, 914, 11, 51514], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 317, "seek": 83964, "start": 862.64, "end": 864.64, "text": " the two problems that we,", "tokens": [51514, 264, 732, 2740, 300, 321, 11, 51614], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 318, "seek": 83964, "start": 864.64, "end": 866.64, "text": " we point out to.", "tokens": [51614, 321, 935, 484, 281, 13, 51714], "temperature": 0.4, "avg_logprob": -0.4221705572945731, "compression_ratio": 1.970873786407767, "no_speech_prob": 0.0069951568730175495}, {"id": 319, "seek": 86664, "start": 867.64, "end": 870.64, "text": " The values can't be negative,", "tokens": [50414, 440, 4190, 393, 380, 312, 3671, 11, 50564], "temperature": 0.0, "avg_logprob": -0.32842617408902036, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0018951863748952746}, {"id": 320, "seek": 86664, "start": 870.64, "end": 873.64, "text": " but I don't think really when you say, okay,", "tokens": [50564, 457, 286, 500, 380, 519, 534, 562, 291, 584, 11, 1392, 11, 50714], "temperature": 0.0, "avg_logprob": -0.32842617408902036, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0018951863748952746}, {"id": 321, "seek": 86664, "start": 873.64, "end": 875.64, "text": " let's refer to.", "tokens": [50714, 718, 311, 2864, 281, 13, 50814], "temperature": 0.0, "avg_logprob": -0.32842617408902036, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0018951863748952746}, {"id": 322, "seek": 86664, "start": 875.64, "end": 879.64, "text": " These values are actually directly determining how much", "tokens": [50814, 1981, 4190, 366, 767, 3838, 23751, 577, 709, 51014], "temperature": 0.0, "avg_logprob": -0.32842617408902036, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0018951863748952746}, {"id": 323, "seek": 86664, "start": 879.64, "end": 881.64, "text": " information from each token you're going to take.", "tokens": [51014, 1589, 490, 1184, 14862, 291, 434, 516, 281, 747, 13, 51114], "temperature": 0.0, "avg_logprob": -0.32842617408902036, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0018951863748952746}, {"id": 324, "seek": 86664, "start": 881.64, "end": 883.64, "text": " And think there's a softmax operation here.", "tokens": [51114, 400, 519, 456, 311, 257, 2787, 41167, 6916, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.32842617408902036, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0018951863748952746}, {"id": 325, "seek": 86664, "start": 883.64, "end": 886.64, "text": " All the values are non-negative, right?", "tokens": [51214, 1057, 264, 4190, 366, 2107, 12, 28561, 1166, 11, 558, 30, 51364], "temperature": 0.0, "avg_logprob": -0.32842617408902036, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0018951863748952746}, {"id": 326, "seek": 86664, "start": 886.64, "end": 889.64, "text": " So there is a distribution that's defined on all these tokens", "tokens": [51364, 407, 456, 307, 257, 7316, 300, 311, 7642, 322, 439, 613, 22667, 51514], "temperature": 0.0, "avg_logprob": -0.32842617408902036, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0018951863748952746}, {"id": 327, "seek": 86664, "start": 889.64, "end": 891.64, "text": " of how much each token is.", "tokens": [51514, 295, 577, 709, 1184, 14862, 307, 13, 51614], "temperature": 0.0, "avg_logprob": -0.32842617408902036, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0018951863748952746}, {"id": 328, "seek": 89164, "start": 892.64, "end": 896.64, "text": " So intuitively, these are really relevant values,", "tokens": [50414, 407, 46506, 11, 613, 366, 534, 7340, 4190, 11, 50614], "temperature": 0.0, "avg_logprob": -0.2057494091731246, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.00228310190141201}, {"id": 329, "seek": 89164, "start": 896.64, "end": 900.64, "text": " but we do have two other issues that we should refer to.", "tokens": [50614, 457, 321, 360, 362, 732, 661, 2663, 300, 321, 820, 2864, 281, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2057494091731246, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.00228310190141201}, {"id": 330, "seek": 89164, "start": 900.64, "end": 903.64, "text": " The first one is we said we have a few attention heads, right?", "tokens": [50814, 440, 700, 472, 307, 321, 848, 321, 362, 257, 1326, 3202, 8050, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.2057494091731246, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.00228310190141201}, {"id": 331, "seek": 89164, "start": 903.64, "end": 906.64, "text": " Each tension head has a different meaning.", "tokens": [50964, 6947, 8980, 1378, 575, 257, 819, 3620, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2057494091731246, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.00228310190141201}, {"id": 332, "seek": 89164, "start": 906.64, "end": 909.64, "text": " Some attention heads are really not relevant to the prediction.", "tokens": [51114, 2188, 3202, 8050, 366, 534, 406, 7340, 281, 264, 17630, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2057494091731246, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.00228310190141201}, {"id": 333, "seek": 89164, "start": 909.64, "end": 913.64, "text": " How do we aggregate across these attention heads in a way that takes", "tokens": [51264, 1012, 360, 321, 26118, 2108, 613, 3202, 8050, 294, 257, 636, 300, 2516, 51464], "temperature": 0.0, "avg_logprob": -0.2057494091731246, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.00228310190141201}, {"id": 334, "seek": 89164, "start": 913.64, "end": 915.64, "text": " into account the meaning of each head?", "tokens": [51464, 666, 2696, 264, 3620, 295, 1184, 1378, 30, 51564], "temperature": 0.0, "avg_logprob": -0.2057494091731246, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.00228310190141201}, {"id": 335, "seek": 91564, "start": 915.64, "end": 918.64, "text": " We wouldn't want to take into account heads that do not affect", "tokens": [50364, 492, 2759, 380, 528, 281, 747, 666, 2696, 8050, 300, 360, 406, 3345, 50514], "temperature": 0.0, "avg_logprob": -0.17482551906419838, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0013038975885137916}, {"id": 336, "seek": 91564, "start": 918.64, "end": 920.64, "text": " the final prediction of the model.", "tokens": [50514, 264, 2572, 17630, 295, 264, 2316, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17482551906419838, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0013038975885137916}, {"id": 337, "seek": 91564, "start": 920.64, "end": 923.64, "text": " And there are such heads since there's research that show that", "tokens": [50614, 400, 456, 366, 1270, 8050, 1670, 456, 311, 2132, 300, 855, 300, 50764], "temperature": 0.0, "avg_logprob": -0.17482551906419838, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0013038975885137916}, {"id": 338, "seek": 91564, "start": 923.64, "end": 926.64, "text": " you can prune most of the heads without impacting the prediction", "tokens": [50764, 291, 393, 582, 2613, 881, 295, 264, 8050, 1553, 29963, 264, 17630, 50914], "temperature": 0.0, "avg_logprob": -0.17482551906419838, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0013038975885137916}, {"id": 339, "seek": 91564, "start": 926.64, "end": 927.64, "text": " of the model.", "tokens": [50914, 295, 264, 2316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17482551906419838, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0013038975885137916}, {"id": 340, "seek": 91564, "start": 927.64, "end": 929.64, "text": " So you have a few attention heads and it isn't,", "tokens": [50964, 407, 291, 362, 257, 1326, 3202, 8050, 293, 309, 1943, 380, 11, 51064], "temperature": 0.0, "avg_logprob": -0.17482551906419838, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0013038975885137916}, {"id": 341, "seek": 91564, "start": 929.64, "end": 932.64, "text": " isn't clear how you aggregate across these attention heads in a", "tokens": [51064, 1943, 380, 1850, 577, 291, 26118, 2108, 613, 3202, 8050, 294, 257, 51214], "temperature": 0.0, "avg_logprob": -0.17482551906419838, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0013038975885137916}, {"id": 342, "seek": 91564, "start": 932.64, "end": 935.64, "text": " way that takes into account the importance of each head.", "tokens": [51214, 636, 300, 2516, 666, 2696, 264, 7379, 295, 1184, 1378, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17482551906419838, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0013038975885137916}, {"id": 343, "seek": 91564, "start": 935.64, "end": 938.64, "text": " And the second question that we have is we refer to the", "tokens": [51364, 400, 264, 1150, 1168, 300, 321, 362, 307, 321, 2864, 281, 264, 51514], "temperature": 0.0, "avg_logprob": -0.17482551906419838, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0013038975885137916}, {"id": 344, "seek": 93864, "start": 938.64, "end": 942.64, "text": " single attention layer, but we have a few attention layers.", "tokens": [50364, 2167, 3202, 4583, 11, 457, 321, 362, 257, 1326, 3202, 7914, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 345, "seek": 93864, "start": 942.64, "end": 945.64, "text": " So the first attention layer may incorporate information into", "tokens": [50564, 407, 264, 700, 3202, 4583, 815, 16091, 1589, 666, 50714], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 346, "seek": 93864, "start": 945.64, "end": 947.64, "text": " token one from token three.", "tokens": [50714, 14862, 472, 490, 14862, 1045, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 347, "seek": 93864, "start": 947.64, "end": 949.64, "text": " And then in the second layer,", "tokens": [50814, 400, 550, 294, 264, 1150, 4583, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 348, "seek": 93864, "start": 949.64, "end": 952.64, "text": " token one isn't simply the patch that it represented in the", "tokens": [50914, 14862, 472, 1943, 380, 2935, 264, 9972, 300, 309, 10379, 294, 264, 51064], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 349, "seek": 93864, "start": 952.64, "end": 953.64, "text": " beginning.", "tokens": [51064, 2863, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 350, "seek": 93864, "start": 953.64, "end": 956.64, "text": " It is this patch with information from this patch.", "tokens": [51114, 467, 307, 341, 9972, 365, 1589, 490, 341, 9972, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 351, "seek": 93864, "start": 956.64, "end": 957.64, "text": " In the second layer,", "tokens": [51264, 682, 264, 1150, 4583, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 352, "seek": 93864, "start": 957.64, "end": 959.64, "text": " it's this patch with information from this patch,", "tokens": [51314, 309, 311, 341, 9972, 365, 1589, 490, 341, 9972, 11, 51414], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 353, "seek": 93864, "start": 959.64, "end": 960.64, "text": " and maybe this patch,", "tokens": [51414, 293, 1310, 341, 9972, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 354, "seek": 93864, "start": 960.64, "end": 961.64, "text": " and maybe this patch.", "tokens": [51464, 293, 1310, 341, 9972, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1406693235736027, "compression_ratio": 2.1443298969072164, "no_speech_prob": 0.017417699098587036}, {"id": 355, "seek": 96164, "start": 961.64, "end": 964.64, "text": " And by the end of the attention mechanism,", "tokens": [50364, 400, 538, 264, 917, 295, 264, 3202, 7513, 11, 50514], "temperature": 0.0, "avg_logprob": -0.18805255060610565, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.004196168389171362}, {"id": 356, "seek": 96164, "start": 964.64, "end": 968.64, "text": " how do we know which token refers to which input patch, right?", "tokens": [50514, 577, 360, 321, 458, 597, 14862, 14942, 281, 597, 4846, 9972, 11, 558, 30, 50714], "temperature": 0.0, "avg_logprob": -0.18805255060610565, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.004196168389171362}, {"id": 357, "seek": 96164, "start": 968.64, "end": 969.64, "text": " They're all mixed up.", "tokens": [50714, 814, 434, 439, 7467, 493, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18805255060610565, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.004196168389171362}, {"id": 358, "seek": 96164, "start": 969.64, "end": 972.64, "text": " That's the entire idea of the attention.", "tokens": [50764, 663, 311, 264, 2302, 1558, 295, 264, 3202, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18805255060610565, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.004196168389171362}, {"id": 359, "seek": 96164, "start": 972.64, "end": 974.64, "text": " So we have two issues here.", "tokens": [50914, 407, 321, 362, 732, 2663, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18805255060610565, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.004196168389171362}, {"id": 360, "seek": 96164, "start": 974.64, "end": 976.64, "text": " How do we aggregate across attention heads?", "tokens": [51014, 1012, 360, 321, 26118, 2108, 3202, 8050, 30, 51114], "temperature": 0.0, "avg_logprob": -0.18805255060610565, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.004196168389171362}, {"id": 361, "seek": 96164, "start": 976.64, "end": 979.64, "text": " Since we know that they have different means and how do we", "tokens": [51114, 4162, 321, 458, 300, 436, 362, 819, 1355, 293, 577, 360, 321, 51264], "temperature": 0.0, "avg_logprob": -0.18805255060610565, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.004196168389171362}, {"id": 362, "seek": 96164, "start": 979.64, "end": 981.64, "text": " aggregate across attention layers?", "tokens": [51264, 26118, 2108, 3202, 7914, 30, 51364], "temperature": 0.0, "avg_logprob": -0.18805255060610565, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.004196168389171362}, {"id": 363, "seek": 96164, "start": 981.64, "end": 982.64, "text": " Yeah.", "tokens": [51364, 865, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18805255060610565, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.004196168389171362}, {"id": 364, "seek": 98264, "start": 982.64, "end": 983.64, "text": " Just so that I understand.", "tokens": [50364, 1449, 370, 300, 286, 1223, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 365, "seek": 98264, "start": 983.64, "end": 985.64, "text": " So if there's only one attention head,", "tokens": [50414, 407, 498, 456, 311, 787, 472, 3202, 1378, 11, 50514], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 366, "seek": 98264, "start": 985.64, "end": 987.64, "text": " and also there's only one attention layer,", "tokens": [50514, 293, 611, 456, 311, 787, 472, 3202, 4583, 11, 50614], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 367, "seek": 98264, "start": 987.64, "end": 989.64, "text": " then the relevance board is the attention.", "tokens": [50614, 550, 264, 32684, 3150, 307, 264, 3202, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 368, "seek": 98264, "start": 989.64, "end": 990.64, "text": " Yeah.", "tokens": [50714, 865, 13, 50764], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 369, "seek": 98264, "start": 990.64, "end": 991.64, "text": " Yeah.", "tokens": [50764, 865, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 370, "seek": 98264, "start": 991.64, "end": 992.64, "text": " By this hypothesis, yes.", "tokens": [50814, 3146, 341, 17291, 11, 2086, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 371, "seek": 98264, "start": 992.64, "end": 993.64, "text": " Okay.", "tokens": [50864, 1033, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 372, "seek": 98264, "start": 993.64, "end": 994.64, "text": " Yes.", "tokens": [50914, 1079, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 373, "seek": 98264, "start": 994.64, "end": 1000.64, "text": " And then I think there are some models that use this for visual", "tokens": [50964, 400, 550, 286, 519, 456, 366, 512, 5245, 300, 764, 341, 337, 5056, 51264], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 374, "seek": 98264, "start": 1000.64, "end": 1003.64, "text": " question answering and actually did that visualization.", "tokens": [51264, 1168, 13430, 293, 767, 630, 300, 25801, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 375, "seek": 98264, "start": 1003.64, "end": 1004.64, "text": " And it works.", "tokens": [51414, 400, 309, 1985, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3029978054085958, "compression_ratio": 1.665, "no_speech_prob": 0.012401719577610493}, {"id": 376, "seek": 100464, "start": 1004.64, "end": 1005.64, "text": " Pretty well.", "tokens": [50364, 10693, 731, 13, 50414], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 377, "seek": 100464, "start": 1005.64, "end": 1008.64, "text": " So assuming you have one attention head and one attention layer,", "tokens": [50414, 407, 11926, 291, 362, 472, 3202, 1378, 293, 472, 3202, 4583, 11, 50564], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 378, "seek": 100464, "start": 1008.64, "end": 1009.64, "text": " it should be.", "tokens": [50564, 309, 820, 312, 13, 50614], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 379, "seek": 100464, "start": 1009.64, "end": 1010.64, "text": " Fingers crossed.", "tokens": [50614, 479, 40180, 14622, 13, 50664], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 380, "seek": 100464, "start": 1010.64, "end": 1012.64, "text": " It should be their elements.", "tokens": [50664, 467, 820, 312, 641, 4959, 13, 50764], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 381, "seek": 100464, "start": 1012.64, "end": 1014.64, "text": " I haven't tried that, but, but yeah.", "tokens": [50764, 286, 2378, 380, 3031, 300, 11, 457, 11, 457, 1338, 13, 50864], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 382, "seek": 100464, "start": 1014.64, "end": 1016.64, "text": " By this intuition.", "tokens": [50864, 3146, 341, 24002, 13, 50964], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 383, "seek": 100464, "start": 1016.64, "end": 1021.64, "text": " So the attention all of mechanism is actually the first method to", "tokens": [50964, 407, 264, 3202, 439, 295, 7513, 307, 767, 264, 700, 3170, 281, 51214], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 384, "seek": 100464, "start": 1021.64, "end": 1023.64, "text": " explain transfer was that came out.", "tokens": [51214, 2903, 5003, 390, 300, 1361, 484, 13, 51314], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 385, "seek": 100464, "start": 1023.64, "end": 1024.6399999999999, "text": " In 2020.", "tokens": [51314, 682, 4808, 13, 51364], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 386, "seek": 100464, "start": 1024.6399999999999, "end": 1025.6399999999999, "text": " And they add,", "tokens": [51364, 400, 436, 909, 11, 51414], "temperature": 0.0, "avg_logprob": -0.29879166217560466, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.0020494912751019}, {"id": 387, "seek": 102564, "start": 1025.64, "end": 1028.64, "text": " they propose that the two simplest solutions,", "tokens": [50364, 436, 17421, 300, 264, 732, 22811, 6547, 11, 50514], "temperature": 0.0, "avg_logprob": -0.26812849239427217, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.004003666806966066}, {"id": 388, "seek": 102564, "start": 1028.64, "end": 1031.64, "text": " maybe that we can think of to solve those three issues.", "tokens": [50514, 1310, 300, 321, 393, 519, 295, 281, 5039, 729, 1045, 2663, 13, 50664], "temperature": 0.0, "avg_logprob": -0.26812849239427217, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.004003666806966066}, {"id": 389, "seek": 102564, "start": 1031.64, "end": 1033.64, "text": " Head aggregation by averaging.", "tokens": [50664, 11398, 16743, 399, 538, 47308, 13, 50764], "temperature": 0.0, "avg_logprob": -0.26812849239427217, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.004003666806966066}, {"id": 390, "seek": 102564, "start": 1033.64, "end": 1034.64, "text": " And again,", "tokens": [50764, 400, 797, 11, 50814], "temperature": 0.0, "avg_logprob": -0.26812849239427217, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.004003666806966066}, {"id": 391, "seek": 102564, "start": 1034.64, "end": 1036.64, "text": " remember we said different meanings to different heads.", "tokens": [50814, 1604, 321, 848, 819, 28138, 281, 819, 8050, 13, 50914], "temperature": 0.0, "avg_logprob": -0.26812849239427217, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.004003666806966066}, {"id": 392, "seek": 102564, "start": 1036.64, "end": 1038.64, "text": " So that's maybe over simplistic.", "tokens": [50914, 407, 300, 311, 1310, 670, 44199, 13, 51014], "temperature": 0.0, "avg_logprob": -0.26812849239427217, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.004003666806966066}, {"id": 393, "seek": 102564, "start": 1038.64, "end": 1041.64, "text": " And they're aggregation by matrix multiplication.", "tokens": [51014, 400, 436, 434, 16743, 399, 538, 8141, 27290, 13, 51164], "temperature": 0.0, "avg_logprob": -0.26812849239427217, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.004003666806966066}, {"id": 394, "seek": 102564, "start": 1041.64, "end": 1042.64, "text": " And if you think about it,", "tokens": [51164, 400, 498, 291, 519, 466, 309, 11, 51214], "temperature": 0.0, "avg_logprob": -0.26812849239427217, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.004003666806966066}, {"id": 395, "seek": 102564, "start": 1042.64, "end": 1045.64, "text": " matrix multiplication from the end to the beginning,", "tokens": [51214, 8141, 27290, 490, 264, 917, 281, 264, 2863, 11, 51364], "temperature": 0.0, "avg_logprob": -0.26812849239427217, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.004003666806966066}, {"id": 396, "seek": 102564, "start": 1045.64, "end": 1047.64, "text": " kind of unravels things.", "tokens": [51364, 733, 295, 40507, 82, 721, 13, 51464], "temperature": 0.0, "avg_logprob": -0.26812849239427217, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.004003666806966066}, {"id": 397, "seek": 104764, "start": 1047.64, "end": 1051.64, "text": " The connections between the two that were made by the attention", "tokens": [50364, 440, 9271, 1296, 264, 732, 300, 645, 1027, 538, 264, 3202, 50564], "temperature": 0.2, "avg_logprob": -0.30377857501690203, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0030736627522855997}, {"id": 398, "seek": 104764, "start": 1051.64, "end": 1052.64, "text": " mechanism.", "tokens": [50564, 7513, 13, 50614], "temperature": 0.2, "avg_logprob": -0.30377857501690203, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0030736627522855997}, {"id": 399, "seek": 104764, "start": 1052.64, "end": 1058.64, "text": " They also propose another method called attention flow,", "tokens": [50614, 814, 611, 17421, 1071, 3170, 1219, 3202, 3095, 11, 50914], "temperature": 0.2, "avg_logprob": -0.30377857501690203, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0030736627522855997}, {"id": 400, "seek": 104764, "start": 1058.64, "end": 1063.64, "text": " which evaluates the flow values in the attention graph,", "tokens": [50914, 597, 6133, 1024, 264, 3095, 4190, 294, 264, 3202, 4295, 11, 51164], "temperature": 0.2, "avg_logprob": -0.30377857501690203, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0030736627522855997}, {"id": 401, "seek": 104764, "start": 1063.64, "end": 1066.64, "text": " like a classic flow problem from algorithms,", "tokens": [51164, 411, 257, 7230, 3095, 1154, 490, 14642, 11, 51314], "temperature": 0.2, "avg_logprob": -0.30377857501690203, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0030736627522855997}, {"id": 402, "seek": 104764, "start": 1066.64, "end": 1068.64, "text": " but it's too computationally expensive for images.", "tokens": [51314, 457, 309, 311, 886, 24903, 379, 5124, 337, 5267, 13, 51414], "temperature": 0.2, "avg_logprob": -0.30377857501690203, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0030736627522855997}, {"id": 403, "seek": 104764, "start": 1068.64, "end": 1070.64, "text": " So we're not really going to get into it.", "tokens": [51414, 407, 321, 434, 406, 534, 516, 281, 483, 666, 309, 13, 51514], "temperature": 0.2, "avg_logprob": -0.30377857501690203, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.0030736627522855997}, {"id": 404, "seek": 107064, "start": 1071.64, "end": 1074.64, "text": " So getting into the first method we propose,", "tokens": [50414, 407, 1242, 666, 264, 700, 3170, 321, 17421, 11, 50564], "temperature": 0.0, "avg_logprob": -0.3730481719970703, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.007210591807961464}, {"id": 405, "seek": 107064, "start": 1074.64, "end": 1077.64, "text": " what we were saying is that the assumptions made by the attention", "tokens": [50564, 437, 321, 645, 1566, 307, 300, 264, 17695, 1027, 538, 264, 3202, 50714], "temperature": 0.0, "avg_logprob": -0.3730481719970703, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.007210591807961464}, {"id": 406, "seek": 107064, "start": 1077.64, "end": 1079.64, "text": " role of mechanism were solid,", "tokens": [50714, 3090, 295, 7513, 645, 5100, 11, 50814], "temperature": 0.0, "avg_logprob": -0.3730481719970703, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.007210591807961464}, {"id": 407, "seek": 107064, "start": 1079.64, "end": 1080.64, "text": " but maybe over simplistic.", "tokens": [50814, 457, 1310, 670, 44199, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3730481719970703, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.007210591807961464}, {"id": 408, "seek": 107064, "start": 1080.64, "end": 1081.64, "text": " Yeah.", "tokens": [50864, 865, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3730481719970703, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.007210591807961464}, {"id": 409, "seek": 107064, "start": 1081.64, "end": 1082.64, "text": " Question.", "tokens": [50914, 14464, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3730481719970703, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.007210591807961464}, {"id": 410, "seek": 107064, "start": 1082.64, "end": 1084.64, "text": " You may have some questions.", "tokens": [50964, 509, 815, 362, 512, 1651, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3730481719970703, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.007210591807961464}, {"id": 411, "seek": 107064, "start": 1084.64, "end": 1088.64, "text": " Oh, yeah.", "tokens": [51064, 876, 11, 1338, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3730481719970703, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.007210591807961464}, {"id": 412, "seek": 107064, "start": 1088.64, "end": 1089.64, "text": " We say in Hebrew.", "tokens": [51264, 492, 584, 294, 17895, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3730481719970703, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.007210591807961464}, {"id": 413, "seek": 107064, "start": 1089.64, "end": 1090.64, "text": " Oh,", "tokens": [51314, 876, 11, 51364], "temperature": 0.0, "avg_logprob": -0.3730481719970703, "compression_ratio": 1.4610778443113772, "no_speech_prob": 0.007210591807961464}, {"id": 414, "seek": 109064, "start": 1090.64, "end": 1101.64, "text": " right.", "tokens": [50364, 558, 13, 50914], "temperature": 0.0, "avg_logprob": -0.37211905888148716, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.015288841910660267}, {"id": 415, "seek": 109064, "start": 1101.64, "end": 1106.64, "text": " I may take those at the end of the talk just because otherwise", "tokens": [50914, 286, 815, 747, 729, 412, 264, 917, 295, 264, 751, 445, 570, 5911, 51164], "temperature": 0.0, "avg_logprob": -0.37211905888148716, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.015288841910660267}, {"id": 416, "seek": 109064, "start": 1106.64, "end": 1108.64, "text": " we won't be able to finish with time.", "tokens": [51164, 321, 1582, 380, 312, 1075, 281, 2413, 365, 565, 13, 51264], "temperature": 0.0, "avg_logprob": -0.37211905888148716, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.015288841910660267}, {"id": 417, "seek": 109064, "start": 1108.64, "end": 1111.64, "text": " Yeah.", "tokens": [51264, 865, 13, 51414], "temperature": 0.0, "avg_logprob": -0.37211905888148716, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.015288841910660267}, {"id": 418, "seek": 109064, "start": 1111.64, "end": 1112.64, "text": " Yeah.", "tokens": [51414, 865, 13, 51464], "temperature": 0.0, "avg_logprob": -0.37211905888148716, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.015288841910660267}, {"id": 419, "seek": 109064, "start": 1112.64, "end": 1115.64, "text": " So getting back to the first time we're going to propose.", "tokens": [51464, 407, 1242, 646, 281, 264, 700, 565, 321, 434, 516, 281, 17421, 13, 51614], "temperature": 0.0, "avg_logprob": -0.37211905888148716, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.015288841910660267}, {"id": 420, "seek": 109064, "start": 1115.64, "end": 1118.64, "text": " We were saying that the assumptions made by attention roll off were", "tokens": [51614, 492, 645, 1566, 300, 264, 17695, 1027, 538, 3202, 3373, 766, 645, 51764], "temperature": 0.0, "avg_logprob": -0.37211905888148716, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.015288841910660267}, {"id": 421, "seek": 111864, "start": 1118.64, "end": 1120.64, "text": " nice and worked in some cases,", "tokens": [50364, 1481, 293, 2732, 294, 512, 3331, 11, 50464], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 422, "seek": 111864, "start": 1120.64, "end": 1122.64, "text": " but are maybe a bit simplistic.", "tokens": [50464, 457, 366, 1310, 257, 857, 44199, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 423, "seek": 111864, "start": 1122.64, "end": 1125.64, "text": " We want to be able to average across the heads in a way that", "tokens": [50564, 492, 528, 281, 312, 1075, 281, 4274, 2108, 264, 8050, 294, 257, 636, 300, 50714], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 424, "seek": 111864, "start": 1125.64, "end": 1127.64, "text": " actually takes into account the meaning of each other.", "tokens": [50714, 767, 2516, 666, 2696, 264, 3620, 295, 1184, 661, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 425, "seek": 111864, "start": 1127.64, "end": 1130.64, "text": " So what we're going to do is we're going to use a signal that is", "tokens": [50814, 407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 764, 257, 6358, 300, 307, 50964], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 426, "seek": 111864, "start": 1130.64, "end": 1132.64, "text": " very useful in explainability in general,", "tokens": [50964, 588, 4420, 294, 2903, 2310, 294, 2674, 11, 51064], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 427, "seek": 111864, "start": 1132.64, "end": 1134.64, "text": " which is radiance, right?", "tokens": [51064, 597, 307, 2843, 6276, 11, 558, 30, 51164], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 428, "seek": 111864, "start": 1134.64, "end": 1138.64, "text": " Because radiance intuitively mean if I change this a bit,", "tokens": [51164, 1436, 2843, 6276, 46506, 914, 498, 286, 1319, 341, 257, 857, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 429, "seek": 111864, "start": 1138.64, "end": 1140.64, "text": " how does this change a bit, right?", "tokens": [51364, 577, 775, 341, 1319, 257, 857, 11, 558, 30, 51464], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 430, "seek": 111864, "start": 1140.64, "end": 1144.64, "text": " So if we take the gradients with regards to the output of the model,", "tokens": [51464, 407, 498, 321, 747, 264, 2771, 2448, 365, 14258, 281, 264, 5598, 295, 264, 2316, 11, 51664], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 431, "seek": 111864, "start": 1144.64, "end": 1147.64, "text": " which is over here, the gradients of the attention matter.", "tokens": [51664, 597, 307, 670, 510, 11, 264, 2771, 2448, 295, 264, 3202, 1871, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12697197331322563, "compression_ratio": 1.7615894039735098, "no_speech_prob": 0.0054601095616817474}, {"id": 432, "seek": 114764, "start": 1148.64, "end": 1152.64, "text": " We can use the gradients as weights for the attention maps.", "tokens": [50414, 492, 393, 764, 264, 2771, 2448, 382, 17443, 337, 264, 3202, 11317, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1253424831944653, "compression_ratio": 2.1150442477876106, "no_speech_prob": 0.005993476137518883}, {"id": 433, "seek": 114764, "start": 1152.64, "end": 1155.64, "text": " So instead of just averaging across the maps,", "tokens": [50614, 407, 2602, 295, 445, 47308, 2108, 264, 11317, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1253424831944653, "compression_ratio": 2.1150442477876106, "no_speech_prob": 0.005993476137518883}, {"id": 434, "seek": 114764, "start": 1155.64, "end": 1159.64, "text": " we take the gradient, the gradient gives us the weights element.", "tokens": [50764, 321, 747, 264, 16235, 11, 264, 16235, 2709, 505, 264, 17443, 4478, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1253424831944653, "compression_ratio": 2.1150442477876106, "no_speech_prob": 0.005993476137518883}, {"id": 435, "seek": 114764, "start": 1159.64, "end": 1163.64, "text": " And we multiply the gradients by the attention and then each head", "tokens": [50964, 400, 321, 12972, 264, 2771, 2448, 538, 264, 3202, 293, 550, 1184, 1378, 51164], "temperature": 0.0, "avg_logprob": -0.1253424831944653, "compression_ratio": 2.1150442477876106, "no_speech_prob": 0.005993476137518883}, {"id": 436, "seek": 114764, "start": 1163.64, "end": 1165.64, "text": " gets a weight from the gradient.", "tokens": [51164, 2170, 257, 3364, 490, 264, 16235, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1253424831944653, "compression_ratio": 2.1150442477876106, "no_speech_prob": 0.005993476137518883}, {"id": 437, "seek": 114764, "start": 1165.64, "end": 1169.64, "text": " And then each attention head is not just the simple attention head", "tokens": [51264, 400, 550, 1184, 3202, 1378, 307, 406, 445, 264, 2199, 3202, 1378, 51464], "temperature": 0.0, "avg_logprob": -0.1253424831944653, "compression_ratio": 2.1150442477876106, "no_speech_prob": 0.005993476137518883}, {"id": 438, "seek": 114764, "start": 1169.64, "end": 1170.64, "text": " that it was in the beginning.", "tokens": [51464, 300, 309, 390, 294, 264, 2863, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1253424831944653, "compression_ratio": 2.1150442477876106, "no_speech_prob": 0.005993476137518883}, {"id": 439, "seek": 114764, "start": 1170.64, "end": 1173.64, "text": " It is the attention weighted by the gradient.", "tokens": [51514, 467, 307, 264, 3202, 32807, 538, 264, 16235, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1253424831944653, "compression_ratio": 2.1150442477876106, "no_speech_prob": 0.005993476137518883}, {"id": 440, "seek": 114764, "start": 1173.64, "end": 1176.64, "text": " And then we can average across the heads in a way that takes into", "tokens": [51664, 400, 550, 321, 393, 4274, 2108, 264, 8050, 294, 257, 636, 300, 2516, 666, 51814], "temperature": 0.0, "avg_logprob": -0.1253424831944653, "compression_ratio": 2.1150442477876106, "no_speech_prob": 0.005993476137518883}, {"id": 441, "seek": 117664, "start": 1176.64, "end": 1178.64, "text": " account the meaning of each head.", "tokens": [50364, 2696, 264, 3620, 295, 1184, 1378, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08307735736553486, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.0008422230021096766}, {"id": 442, "seek": 117664, "start": 1178.64, "end": 1180.64, "text": " So this is why the gradients are here.", "tokens": [50464, 407, 341, 307, 983, 264, 2771, 2448, 366, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08307735736553486, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.0008422230021096766}, {"id": 443, "seek": 117664, "start": 1180.64, "end": 1184.64, "text": " But we have another component that I won't get too deeply into", "tokens": [50564, 583, 321, 362, 1071, 6542, 300, 286, 1582, 380, 483, 886, 8760, 666, 50764], "temperature": 0.0, "avg_logprob": -0.08307735736553486, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.0008422230021096766}, {"id": 444, "seek": 117664, "start": 1184.64, "end": 1188.64, "text": " because it was removed for our second method.", "tokens": [50764, 570, 309, 390, 7261, 337, 527, 1150, 3170, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08307735736553486, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.0008422230021096766}, {"id": 445, "seek": 117664, "start": 1188.64, "end": 1192.64, "text": " It is the LRP component, layer-wise relevance propagation.", "tokens": [50964, 467, 307, 264, 441, 28516, 6542, 11, 4583, 12, 3711, 32684, 38377, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08307735736553486, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.0008422230021096766}, {"id": 446, "seek": 117664, "start": 1192.64, "end": 1196.64, "text": " The second thing we thought of was that we actually reduce the", "tokens": [51164, 440, 1150, 551, 321, 1194, 295, 390, 300, 321, 767, 5407, 264, 51364], "temperature": 0.0, "avg_logprob": -0.08307735736553486, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.0008422230021096766}, {"id": 447, "seek": 117664, "start": 1196.64, "end": 1199.64, "text": " entire transformer architecture to just a multiplication", "tokens": [51364, 2302, 31782, 9482, 281, 445, 257, 27290, 51514], "temperature": 0.0, "avg_logprob": -0.08307735736553486, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.0008422230021096766}, {"id": 448, "seek": 117664, "start": 1199.64, "end": 1200.64, "text": " of queries and keys.", "tokens": [51514, 295, 24109, 293, 9317, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08307735736553486, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.0008422230021096766}, {"id": 449, "seek": 117664, "start": 1200.64, "end": 1203.64, "text": " So it's not even the entire attention mechanism because we also", "tokens": [51564, 407, 309, 311, 406, 754, 264, 2302, 3202, 7513, 570, 321, 611, 51714], "temperature": 0.0, "avg_logprob": -0.08307735736553486, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.0008422230021096766}, {"id": 450, "seek": 117664, "start": 1203.64, "end": 1205.64, "text": " had the values there, remember?", "tokens": [51714, 632, 264, 4190, 456, 11, 1604, 30, 51814], "temperature": 0.0, "avg_logprob": -0.08307735736553486, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.0008422230021096766}, {"id": 451, "seek": 120564, "start": 1205.64, "end": 1208.64, "text": " So we narrowed down this entire, not so complex,", "tokens": [50364, 407, 321, 9432, 292, 760, 341, 2302, 11, 406, 370, 3997, 11, 50514], "temperature": 0.0, "avg_logprob": -0.1142278386053638, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.0002822744136210531}, {"id": 452, "seek": 120564, "start": 1208.64, "end": 1210.64, "text": " but architecture, right?", "tokens": [50514, 457, 9482, 11, 558, 30, 50614], "temperature": 0.0, "avg_logprob": -0.1142278386053638, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.0002822744136210531}, {"id": 453, "seek": 120564, "start": 1210.64, "end": 1211.64, "text": " It has activations.", "tokens": [50614, 467, 575, 2430, 763, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1142278386053638, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.0002822744136210531}, {"id": 454, "seek": 120564, "start": 1211.64, "end": 1213.64, "text": " It has linear projections.", "tokens": [50664, 467, 575, 8213, 32371, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1142278386053638, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.0002822744136210531}, {"id": 455, "seek": 120564, "start": 1213.64, "end": 1216.64, "text": " We narrowed all down to the multiplication between queries", "tokens": [50764, 492, 9432, 292, 439, 760, 281, 264, 27290, 1296, 24109, 50914], "temperature": 0.0, "avg_logprob": -0.1142278386053638, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.0002822744136210531}, {"id": 456, "seek": 120564, "start": 1216.64, "end": 1217.64, "text": " and keys.", "tokens": [50914, 293, 9317, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1142278386053638, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.0002822744136210531}, {"id": 457, "seek": 120564, "start": 1217.64, "end": 1220.64, "text": " So we do want to take into account the other layers of the", "tokens": [50964, 407, 321, 360, 528, 281, 747, 666, 2696, 264, 661, 7914, 295, 264, 51114], "temperature": 0.0, "avg_logprob": -0.1142278386053638, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.0002822744136210531}, {"id": 458, "seek": 120564, "start": 1220.64, "end": 1224.64, "text": " transformer and how they impact the calculations.", "tokens": [51114, 31782, 293, 577, 436, 2712, 264, 20448, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1142278386053638, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.0002822744136210531}, {"id": 459, "seek": 120564, "start": 1224.64, "end": 1228.64, "text": " So instead of just taking, let's get back to the whiteboard here,", "tokens": [51314, 407, 2602, 295, 445, 1940, 11, 718, 311, 483, 646, 281, 264, 2418, 3787, 510, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1142278386053638, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.0002822744136210531}, {"id": 460, "seek": 120564, "start": 1228.64, "end": 1231.64, "text": " instead of just taking the attention map,", "tokens": [51514, 2602, 295, 445, 1940, 264, 3202, 4471, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1142278386053638, "compression_ratio": 1.6846473029045643, "no_speech_prob": 0.0002822744136210531}, {"id": 461, "seek": 123164, "start": 1232.64, "end": 1235.64, "text": " that is quite, it's simplistic, right?", "tokens": [50414, 300, 307, 1596, 11, 309, 311, 44199, 11, 558, 30, 50564], "temperature": 0.0, "avg_logprob": -0.16394468307495116, "compression_ratio": 1.616, "no_speech_prob": 0.0006260725203901529}, {"id": 462, "seek": 123164, "start": 1235.64, "end": 1238.64, "text": " It's not that, but the multiplication between queries and keys,", "tokens": [50564, 467, 311, 406, 300, 11, 457, 264, 27290, 1296, 24109, 293, 9317, 11, 50714], "temperature": 0.0, "avg_logprob": -0.16394468307495116, "compression_ratio": 1.616, "no_speech_prob": 0.0006260725203901529}, {"id": 463, "seek": 123164, "start": 1238.64, "end": 1241.64, "text": " we want to take into account a different attention map,", "tokens": [50714, 321, 528, 281, 747, 666, 2696, 257, 819, 3202, 4471, 11, 50864], "temperature": 0.0, "avg_logprob": -0.16394468307495116, "compression_ratio": 1.616, "no_speech_prob": 0.0006260725203901529}, {"id": 464, "seek": 123164, "start": 1241.64, "end": 1246.64, "text": " we'll call it RA, which takes into account the other layers", "tokens": [50864, 321, 603, 818, 309, 14626, 11, 597, 2516, 666, 2696, 264, 661, 7914, 51114], "temperature": 0.0, "avg_logprob": -0.16394468307495116, "compression_ratio": 1.616, "no_speech_prob": 0.0006260725203901529}, {"id": 465, "seek": 123164, "start": 1246.64, "end": 1248.64, "text": " of the transformer architecture.", "tokens": [51114, 295, 264, 31782, 9482, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16394468307495116, "compression_ratio": 1.616, "no_speech_prob": 0.0006260725203901529}, {"id": 466, "seek": 123164, "start": 1248.64, "end": 1251.64, "text": " So instead of taking just these raw relevance values,", "tokens": [51214, 407, 2602, 295, 1940, 445, 613, 8936, 32684, 4190, 11, 51364], "temperature": 0.0, "avg_logprob": -0.16394468307495116, "compression_ratio": 1.616, "no_speech_prob": 0.0006260725203901529}, {"id": 467, "seek": 123164, "start": 1251.64, "end": 1255.64, "text": " we take relevance values calculated by LRP.", "tokens": [51364, 321, 747, 32684, 4190, 15598, 538, 441, 28516, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16394468307495116, "compression_ratio": 1.616, "no_speech_prob": 0.0006260725203901529}, {"id": 468, "seek": 123164, "start": 1255.64, "end": 1259.64, "text": " And LRP is a mechanism that does back propagation with", "tokens": [51564, 400, 441, 28516, 307, 257, 7513, 300, 775, 646, 38377, 365, 51764], "temperature": 0.0, "avg_logprob": -0.16394468307495116, "compression_ratio": 1.616, "no_speech_prob": 0.0006260725203901529}, {"id": 469, "seek": 125964, "start": 1259.64, "end": 1262.64, "text": " gradients from the end of the network all the way to the beginning.", "tokens": [50364, 2771, 2448, 490, 264, 917, 295, 264, 3209, 439, 264, 636, 281, 264, 2863, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09046384266444615, "compression_ratio": 1.8278688524590163, "no_speech_prob": 0.00020017445785924792}, {"id": 470, "seek": 125964, "start": 1262.64, "end": 1266.64, "text": " And it can give us relevant values for specifically this attention", "tokens": [50514, 400, 309, 393, 976, 505, 7340, 4190, 337, 4682, 341, 3202, 50714], "temperature": 0.0, "avg_logprob": -0.09046384266444615, "compression_ratio": 1.8278688524590163, "no_speech_prob": 0.00020017445785924792}, {"id": 471, "seek": 125964, "start": 1266.64, "end": 1267.64, "text": " matrix.", "tokens": [50714, 8141, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09046384266444615, "compression_ratio": 1.8278688524590163, "no_speech_prob": 0.00020017445785924792}, {"id": 472, "seek": 125964, "start": 1267.64, "end": 1270.64, "text": " So instead of taking into account the attention values,", "tokens": [50764, 407, 2602, 295, 1940, 666, 2696, 264, 3202, 4190, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09046384266444615, "compression_ratio": 1.8278688524590163, "no_speech_prob": 0.00020017445785924792}, {"id": 473, "seek": 125964, "start": 1270.64, "end": 1273.64, "text": " the raw attention values, we take into account the relevance values", "tokens": [50914, 264, 8936, 3202, 4190, 11, 321, 747, 666, 2696, 264, 32684, 4190, 51064], "temperature": 0.0, "avg_logprob": -0.09046384266444615, "compression_ratio": 1.8278688524590163, "no_speech_prob": 0.00020017445785924792}, {"id": 474, "seek": 125964, "start": 1273.64, "end": 1276.64, "text": " of the attention matrix.", "tokens": [51064, 295, 264, 3202, 8141, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09046384266444615, "compression_ratio": 1.8278688524590163, "no_speech_prob": 0.00020017445785924792}, {"id": 475, "seek": 125964, "start": 1276.64, "end": 1279.64, "text": " And as I said, I won't get too deep into it because we actually", "tokens": [51214, 400, 382, 286, 848, 11, 286, 1582, 380, 483, 886, 2452, 666, 309, 570, 321, 767, 51364], "temperature": 0.0, "avg_logprob": -0.09046384266444615, "compression_ratio": 1.8278688524590163, "no_speech_prob": 0.00020017445785924792}, {"id": 476, "seek": 125964, "start": 1279.64, "end": 1281.64, "text": " removed it in our second method,", "tokens": [51364, 7261, 309, 294, 527, 1150, 3170, 11, 51464], "temperature": 0.0, "avg_logprob": -0.09046384266444615, "compression_ratio": 1.8278688524590163, "no_speech_prob": 0.00020017445785924792}, {"id": 477, "seek": 125964, "start": 1281.64, "end": 1285.64, "text": " which is the one that I want to get into in more details.", "tokens": [51464, 597, 307, 264, 472, 300, 286, 528, 281, 483, 666, 294, 544, 4365, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09046384266444615, "compression_ratio": 1.8278688524590163, "no_speech_prob": 0.00020017445785924792}, {"id": 478, "seek": 128564, "start": 1285.64, "end": 1291.64, "text": " So we have the attention gradients to average across the heads.", "tokens": [50364, 407, 321, 362, 264, 3202, 2771, 2448, 281, 4274, 2108, 264, 8050, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11070015491583408, "compression_ratio": 1.8232044198895028, "no_speech_prob": 0.0009991904953494668}, {"id": 479, "seek": 128564, "start": 1291.64, "end": 1294.64, "text": " And we have the relevance in order to account for all the other layers", "tokens": [50664, 400, 321, 362, 264, 32684, 294, 1668, 281, 2696, 337, 439, 264, 661, 7914, 50814], "temperature": 0.0, "avg_logprob": -0.11070015491583408, "compression_ratio": 1.8232044198895028, "no_speech_prob": 0.0009991904953494668}, {"id": 480, "seek": 128564, "start": 1294.64, "end": 1296.64, "text": " in the transformer.", "tokens": [50814, 294, 264, 31782, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11070015491583408, "compression_ratio": 1.8232044198895028, "no_speech_prob": 0.0009991904953494668}, {"id": 481, "seek": 128564, "start": 1296.64, "end": 1298.64, "text": " So this is how we average across the heads.", "tokens": [50914, 407, 341, 307, 577, 321, 4274, 2108, 264, 8050, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11070015491583408, "compression_ratio": 1.8232044198895028, "no_speech_prob": 0.0009991904953494668}, {"id": 482, "seek": 128564, "start": 1298.64, "end": 1301.64, "text": " And the way that we average across the layers is by matrix multiplication.", "tokens": [51014, 400, 264, 636, 300, 321, 4274, 2108, 264, 7914, 307, 538, 8141, 27290, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11070015491583408, "compression_ratio": 1.8232044198895028, "no_speech_prob": 0.0009991904953494668}, {"id": 483, "seek": 128564, "start": 1301.64, "end": 1306.64, "text": " Here we adopted the interpretation from attention robot.", "tokens": [51164, 1692, 321, 12175, 264, 14174, 490, 3202, 7881, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11070015491583408, "compression_ratio": 1.8232044198895028, "no_speech_prob": 0.0009991904953494668}, {"id": 484, "seek": 130664, "start": 1307.64, "end": 1316.64, "text": " Oh, no, not element wise, actual matrix multiplication.", "tokens": [50414, 876, 11, 572, 11, 406, 4478, 10829, 11, 3539, 8141, 27290, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20613925353340481, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.015402068383991718}, {"id": 485, "seek": 130664, "start": 1316.64, "end": 1318.64, "text": " The matrices are squared matrices.", "tokens": [50864, 440, 32284, 366, 8889, 32284, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20613925353340481, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.015402068383991718}, {"id": 486, "seek": 130664, "start": 1318.64, "end": 1322.64, "text": " Yeah, because they are self attention matrices so you can actually", "tokens": [50964, 865, 11, 570, 436, 366, 2698, 3202, 32284, 370, 291, 393, 767, 51164], "temperature": 0.0, "avg_logprob": -0.20613925353340481, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.015402068383991718}, {"id": 487, "seek": 130664, "start": 1322.64, "end": 1323.64, "text": " multiply them.", "tokens": [51164, 12972, 552, 13, 51214], "temperature": 0.0, "avg_logprob": -0.20613925353340481, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.015402068383991718}, {"id": 488, "seek": 130664, "start": 1323.64, "end": 1326.64, "text": " And if you think about it, you can unravel it when multiplying two", "tokens": [51214, 400, 498, 291, 519, 466, 309, 11, 291, 393, 40507, 309, 562, 30955, 732, 51364], "temperature": 0.0, "avg_logprob": -0.20613925353340481, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.015402068383991718}, {"id": 489, "seek": 130664, "start": 1326.64, "end": 1327.64, "text": " attention matrices.", "tokens": [51364, 3202, 32284, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20613925353340481, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.015402068383991718}, {"id": 490, "seek": 130664, "start": 1327.64, "end": 1331.64, "text": " It actually says, if the previous layer gave token one information", "tokens": [51414, 467, 767, 1619, 11, 498, 264, 3894, 4583, 2729, 14862, 472, 1589, 51614], "temperature": 0.0, "avg_logprob": -0.20613925353340481, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.015402068383991718}, {"id": 491, "seek": 130664, "start": 1331.64, "end": 1334.64, "text": " from token three, and this layer gives token one information from", "tokens": [51614, 490, 14862, 1045, 11, 293, 341, 4583, 2709, 14862, 472, 1589, 490, 51764], "temperature": 0.0, "avg_logprob": -0.20613925353340481, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.015402068383991718}, {"id": 492, "seek": 133464, "start": 1334.64, "end": 1339.64, "text": " token four, then it unravels both operations to ensure that you", "tokens": [50364, 14862, 1451, 11, 550, 309, 40507, 82, 1293, 7705, 281, 5586, 300, 291, 50614], "temperature": 0.0, "avg_logprob": -0.12984288340867167, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004106847336515784}, {"id": 493, "seek": 133464, "start": 1339.64, "end": 1345.64, "text": " actually take into account all the context.", "tokens": [50614, 767, 747, 666, 2696, 439, 264, 4319, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12984288340867167, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004106847336515784}, {"id": 494, "seek": 133464, "start": 1345.64, "end": 1349.64, "text": " Yeah, so this is just a rewind of what we saw in the previous slide.", "tokens": [50914, 865, 11, 370, 341, 307, 445, 257, 41458, 295, 437, 321, 1866, 294, 264, 3894, 4137, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12984288340867167, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004106847336515784}, {"id": 495, "seek": 133464, "start": 1349.64, "end": 1351.64, "text": " How do we average across heads?", "tokens": [51114, 1012, 360, 321, 4274, 2108, 8050, 30, 51214], "temperature": 0.0, "avg_logprob": -0.12984288340867167, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004106847336515784}, {"id": 496, "seek": 133464, "start": 1351.64, "end": 1353.64, "text": " We take the gradients as weights.", "tokens": [51214, 492, 747, 264, 2771, 2448, 382, 17443, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12984288340867167, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004106847336515784}, {"id": 497, "seek": 133464, "start": 1353.64, "end": 1355.64, "text": " We take the relevance instead of the pure attention weights.", "tokens": [51314, 492, 747, 264, 32684, 2602, 295, 264, 6075, 3202, 17443, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12984288340867167, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004106847336515784}, {"id": 498, "seek": 133464, "start": 1355.64, "end": 1357.64, "text": " And then we do averaging.", "tokens": [51414, 400, 550, 321, 360, 47308, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12984288340867167, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004106847336515784}, {"id": 499, "seek": 133464, "start": 1357.64, "end": 1359.64, "text": " But here the average is not just the raw average.", "tokens": [51514, 583, 510, 264, 4274, 307, 406, 445, 264, 8936, 4274, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12984288340867167, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0004106847336515784}, {"id": 500, "seek": 135964, "start": 1359.64, "end": 1364.64, "text": " If we had before it is weighted by the gradients.", "tokens": [50364, 759, 321, 632, 949, 309, 307, 32807, 538, 264, 2771, 2448, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17344207030076247, "compression_ratio": 1.4715909090909092, "no_speech_prob": 0.002321843756362796}, {"id": 501, "seek": 135964, "start": 1364.64, "end": 1370.64, "text": " And here you can see a few examples of how our method works.", "tokens": [50614, 400, 510, 291, 393, 536, 257, 1326, 5110, 295, 577, 527, 3170, 1985, 13, 50914], "temperature": 0.0, "avg_logprob": -0.17344207030076247, "compression_ratio": 1.4715909090909092, "no_speech_prob": 0.002321843756362796}, {"id": 502, "seek": 135964, "start": 1370.64, "end": 1373.64, "text": " So by the way, this is a slide that was added, but we don't have the", "tokens": [50914, 407, 538, 264, 636, 11, 341, 307, 257, 4137, 300, 390, 3869, 11, 457, 321, 500, 380, 362, 264, 51064], "temperature": 0.0, "avg_logprob": -0.17344207030076247, "compression_ratio": 1.4715909090909092, "no_speech_prob": 0.002321843756362796}, {"id": 503, "seek": 135964, "start": 1373.64, "end": 1374.64, "text": " updated slide.", "tokens": [51064, 10588, 4137, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17344207030076247, "compression_ratio": 1.4715909090909092, "no_speech_prob": 0.002321843756362796}, {"id": 504, "seek": 135964, "start": 1374.64, "end": 1378.64, "text": " So let's just see what we get in the end of this calculation.", "tokens": [51114, 407, 718, 311, 445, 536, 437, 321, 483, 294, 264, 917, 295, 341, 17108, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17344207030076247, "compression_ratio": 1.4715909090909092, "no_speech_prob": 0.002321843756362796}, {"id": 505, "seek": 135964, "start": 1378.64, "end": 1384.64, "text": " So", "tokens": [51314, 407, 51614], "temperature": 0.0, "avg_logprob": -0.17344207030076247, "compression_ratio": 1.4715909090909092, "no_speech_prob": 0.002321843756362796}, {"id": 506, "seek": 138464, "start": 1384.64, "end": 1389.64, "text": " at the end of this calculation, we had an attention matrix, which", "tokens": [50364, 412, 264, 917, 295, 341, 17108, 11, 321, 632, 364, 3202, 8141, 11, 597, 50614], "temperature": 0.0, "avg_logprob": -0.16118981288029596, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.0011327979154884815}, {"id": 507, "seek": 138464, "start": 1389.64, "end": 1392.64, "text": " is the attention matrix after by the averaging and everything.", "tokens": [50614, 307, 264, 3202, 8141, 934, 538, 264, 47308, 293, 1203, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16118981288029596, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.0011327979154884815}, {"id": 508, "seek": 138464, "start": 1392.64, "end": 1397.64, "text": " We have attention matrices for all the layers.", "tokens": [50764, 492, 362, 3202, 32284, 337, 439, 264, 7914, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16118981288029596, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.0011327979154884815}, {"id": 509, "seek": 138464, "start": 1397.64, "end": 1405.64, "text": " And then we multiply these.", "tokens": [51014, 400, 550, 321, 12972, 613, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16118981288029596, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.0011327979154884815}, {"id": 510, "seek": 138464, "start": 1405.64, "end": 1411.64, "text": " So really, we have one attention matrix that takes into account all", "tokens": [51414, 407, 534, 11, 321, 362, 472, 3202, 8141, 300, 2516, 666, 2696, 439, 51714], "temperature": 0.0, "avg_logprob": -0.16118981288029596, "compression_ratio": 1.6728395061728396, "no_speech_prob": 0.0011327979154884815}, {"id": 511, "seek": 141164, "start": 1411.64, "end": 1414.64, "text": " the layers and all the heads.", "tokens": [50364, 264, 7914, 293, 439, 264, 8050, 13, 50514], "temperature": 0.0, "avg_logprob": -0.22741476337561448, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0142381452023983}, {"id": 512, "seek": 141164, "start": 1414.64, "end": 1419.64, "text": " And now we can get back to, can we go back in the slides?", "tokens": [50514, 400, 586, 321, 393, 483, 646, 281, 11, 393, 321, 352, 646, 294, 264, 9788, 30, 50764], "temperature": 0.0, "avg_logprob": -0.22741476337561448, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0142381452023983}, {"id": 513, "seek": 141164, "start": 1419.64, "end": 1422.64, "text": " Oh, no, it's only going forward.", "tokens": [50764, 876, 11, 572, 11, 309, 311, 787, 516, 2128, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22741476337561448, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0142381452023983}, {"id": 514, "seek": 141164, "start": 1422.64, "end": 1423.64, "text": " Yeah.", "tokens": [50914, 865, 13, 50964], "temperature": 0.0, "avg_logprob": -0.22741476337561448, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0142381452023983}, {"id": 515, "seek": 141164, "start": 1423.64, "end": 1426.64, "text": " Maybe there's an arrow at the bottom left corner.", "tokens": [50964, 2704, 456, 311, 364, 11610, 412, 264, 2767, 1411, 4538, 13, 51114], "temperature": 0.0, "avg_logprob": -0.22741476337561448, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0142381452023983}, {"id": 516, "seek": 141164, "start": 1426.64, "end": 1428.64, "text": " If you move your mouse.", "tokens": [51114, 759, 291, 1286, 428, 9719, 13, 51214], "temperature": 0.0, "avg_logprob": -0.22741476337561448, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0142381452023983}, {"id": 517, "seek": 141164, "start": 1428.64, "end": 1431.64, "text": " Oh, the back arrow.", "tokens": [51214, 876, 11, 264, 646, 11610, 13, 51364], "temperature": 0.0, "avg_logprob": -0.22741476337561448, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0142381452023983}, {"id": 518, "seek": 141164, "start": 1431.64, "end": 1435.64, "text": " Oh, it's the other way around.", "tokens": [51364, 876, 11, 309, 311, 264, 661, 636, 926, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22741476337561448, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0142381452023983}, {"id": 519, "seek": 141164, "start": 1435.64, "end": 1438.64, "text": " Yeah.", "tokens": [51564, 865, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22741476337561448, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.0142381452023983}, {"id": 520, "seek": 143864, "start": 1438.64, "end": 1441.64, "text": " And now we're actually getting to the point that Sharon made that right", "tokens": [50364, 400, 586, 321, 434, 767, 1242, 281, 264, 935, 300, 28573, 1027, 300, 558, 50514], "temperature": 0.0, "avg_logprob": -0.17965994210078798, "compression_ratio": 2.068, "no_speech_prob": 0.0020172952208667994}, {"id": 521, "seek": 143864, "start": 1441.64, "end": 1446.64, "text": " now we only after all the aggregations that we made, we have one aggregated", "tokens": [50514, 586, 321, 787, 934, 439, 264, 16743, 763, 300, 321, 1027, 11, 321, 362, 472, 16743, 770, 50764], "temperature": 0.0, "avg_logprob": -0.17965994210078798, "compression_ratio": 2.068, "no_speech_prob": 0.0020172952208667994}, {"id": 522, "seek": 143864, "start": 1446.64, "end": 1450.64, "text": " attention matrix for the entire network because we aggregate across heads", "tokens": [50764, 3202, 8141, 337, 264, 2302, 3209, 570, 321, 26118, 2108, 8050, 50964], "temperature": 0.0, "avg_logprob": -0.17965994210078798, "compression_ratio": 2.068, "no_speech_prob": 0.0020172952208667994}, {"id": 523, "seek": 143864, "start": 1450.64, "end": 1452.64, "text": " and then we aggregate across layers.", "tokens": [50964, 293, 550, 321, 26118, 2108, 7914, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17965994210078798, "compression_ratio": 2.068, "no_speech_prob": 0.0020172952208667994}, {"id": 524, "seek": 143864, "start": 1452.64, "end": 1456.64, "text": " And once we have that one attention matrix for the entire, for the entire", "tokens": [51064, 400, 1564, 321, 362, 300, 472, 3202, 8141, 337, 264, 2302, 11, 337, 264, 2302, 51264], "temperature": 0.0, "avg_logprob": -0.17965994210078798, "compression_ratio": 2.068, "no_speech_prob": 0.0020172952208667994}, {"id": 525, "seek": 143864, "start": 1456.64, "end": 1461.64, "text": " network, then we can use that intuition that we had that the row corresponding", "tokens": [51264, 3209, 11, 550, 321, 393, 764, 300, 24002, 300, 321, 632, 300, 264, 5386, 11760, 51514], "temperature": 0.0, "avg_logprob": -0.17965994210078798, "compression_ratio": 2.068, "no_speech_prob": 0.0020172952208667994}, {"id": 526, "seek": 143864, "start": 1461.64, "end": 1464.64, "text": " to the classification token is actually the explanation.", "tokens": [51514, 281, 264, 21538, 14862, 307, 767, 264, 10835, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17965994210078798, "compression_ratio": 2.068, "no_speech_prob": 0.0020172952208667994}, {"id": 527, "seek": 143864, "start": 1464.64, "end": 1466.64, "text": " So this is how we extract the final explanation.", "tokens": [51664, 407, 341, 307, 577, 321, 8947, 264, 2572, 10835, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17965994210078798, "compression_ratio": 2.068, "no_speech_prob": 0.0020172952208667994}, {"id": 528, "seek": 146664, "start": 1466.64, "end": 1470.64, "text": " And then we're actually the relevance values that we use.", "tokens": [50364, 400, 550, 321, 434, 767, 264, 32684, 4190, 300, 321, 764, 13, 50564], "temperature": 0.0, "avg_logprob": -0.25910065571467084, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0007671561324968934}, {"id": 529, "seek": 146664, "start": 1470.64, "end": 1475.64, "text": " And the one the other way around, right.", "tokens": [50564, 400, 264, 472, 264, 661, 636, 926, 11, 558, 13, 50814], "temperature": 0.0, "avg_logprob": -0.25910065571467084, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0007671561324968934}, {"id": 530, "seek": 146664, "start": 1475.64, "end": 1478.64, "text": " Okay.", "tokens": [50814, 1033, 13, 50964], "temperature": 0.0, "avg_logprob": -0.25910065571467084, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0007671561324968934}, {"id": 531, "seek": 146664, "start": 1478.64, "end": 1482.64, "text": " So as you can see here, we have comparisons between our method and other methods", "tokens": [50964, 407, 382, 291, 393, 536, 510, 11, 321, 362, 33157, 1296, 527, 3170, 293, 661, 7150, 51164], "temperature": 0.0, "avg_logprob": -0.25910065571467084, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0007671561324968934}, {"id": 532, "seek": 146664, "start": 1482.64, "end": 1487.64, "text": " that are either adapted from CNNs, or methods that were constructed for", "tokens": [51164, 300, 366, 2139, 20871, 490, 24859, 82, 11, 420, 7150, 300, 645, 17083, 337, 51414], "temperature": 0.0, "avg_logprob": -0.25910065571467084, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0007671561324968934}, {"id": 533, "seek": 146664, "start": 1487.64, "end": 1489.64, "text": " transformers such as rollout.", "tokens": [51414, 4088, 433, 1270, 382, 3373, 346, 13, 51514], "temperature": 0.0, "avg_logprob": -0.25910065571467084, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0007671561324968934}, {"id": 534, "seek": 146664, "start": 1489.64, "end": 1493.64, "text": " So as you can see here, rollout tends to have a lot of noise in the background", "tokens": [51514, 407, 382, 291, 393, 536, 510, 11, 3373, 346, 12258, 281, 362, 257, 688, 295, 5658, 294, 264, 3678, 51714], "temperature": 0.0, "avg_logprob": -0.25910065571467084, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0007671561324968934}, {"id": 535, "seek": 149364, "start": 1493.64, "end": 1497.64, "text": " and we think about it intuitively as resulting from the fact that they just", "tokens": [50364, 293, 321, 519, 466, 309, 46506, 382, 16505, 490, 264, 1186, 300, 436, 445, 50564], "temperature": 0.0, "avg_logprob": -0.1580174332958157, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.004751149564981461}, {"id": 536, "seek": 149364, "start": 1497.64, "end": 1501.64, "text": " average across the heads and not take into account the meaning of each head.", "tokens": [50564, 4274, 2108, 264, 8050, 293, 406, 747, 666, 2696, 264, 3620, 295, 1184, 1378, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1580174332958157, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.004751149564981461}, {"id": 537, "seek": 149364, "start": 1501.64, "end": 1506.64, "text": " And some methods such as partial LRP fail on some cases, but in these", "tokens": [50764, 400, 512, 7150, 1270, 382, 14641, 441, 28516, 3061, 322, 512, 3331, 11, 457, 294, 613, 51014], "temperature": 0.0, "avg_logprob": -0.1580174332958157, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.004751149564981461}, {"id": 538, "seek": 149364, "start": 1506.64, "end": 1509.64, "text": " specific cases, they actually do pretty well.", "tokens": [51014, 2685, 3331, 11, 436, 767, 360, 1238, 731, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1580174332958157, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.004751149564981461}, {"id": 539, "seek": 149364, "start": 1509.64, "end": 1513.64, "text": " But I do want to point out that they do not distinct between classes.", "tokens": [51164, 583, 286, 360, 528, 281, 935, 484, 300, 436, 360, 406, 10644, 1296, 5359, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1580174332958157, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.004751149564981461}, {"id": 540, "seek": 149364, "start": 1513.64, "end": 1518.64, "text": " So for example, if we have an elephant and a zebra in an image, our method is able", "tokens": [51364, 407, 337, 1365, 11, 498, 321, 362, 364, 19791, 293, 257, 47060, 294, 364, 3256, 11, 527, 3170, 307, 1075, 51614], "temperature": 0.0, "avg_logprob": -0.1580174332958157, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.004751149564981461}, {"id": 541, "seek": 149364, "start": 1518.64, "end": 1522.64, "text": " to produce explanations specifically for the elephant or specifically for the zebra.", "tokens": [51614, 281, 5258, 28708, 4682, 337, 264, 19791, 420, 4682, 337, 264, 47060, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1580174332958157, "compression_ratio": 1.726962457337884, "no_speech_prob": 0.004751149564981461}, {"id": 542, "seek": 152264, "start": 1522.64, "end": 1527.64, "text": " And when we don't do that, and we want to use this method say partial LRP to", "tokens": [50364, 400, 562, 321, 500, 380, 360, 300, 11, 293, 321, 528, 281, 764, 341, 3170, 584, 14641, 441, 28516, 281, 50614], "temperature": 0.0, "avg_logprob": -0.13010716679120304, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0033744939137250185}, {"id": 543, "seek": 152264, "start": 1527.64, "end": 1532.64, "text": " explain predictions by the model, it will be hard to do that because if you want", "tokens": [50614, 2903, 21264, 538, 264, 2316, 11, 309, 486, 312, 1152, 281, 360, 300, 570, 498, 291, 528, 50864], "temperature": 0.0, "avg_logprob": -0.13010716679120304, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0033744939137250185}, {"id": 544, "seek": 152264, "start": 1532.64, "end": 1537.64, "text": " to explain the elephant prediction, we may have results coming in from other classes.", "tokens": [50864, 281, 2903, 264, 19791, 17630, 11, 321, 815, 362, 3542, 1348, 294, 490, 661, 5359, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13010716679120304, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0033744939137250185}, {"id": 545, "seek": 152264, "start": 1537.64, "end": 1541.64, "text": " So we're not really sure if the things that we're seeing highlighted are highlighted", "tokens": [51114, 407, 321, 434, 406, 534, 988, 498, 264, 721, 300, 321, 434, 2577, 17173, 366, 17173, 51314], "temperature": 0.0, "avg_logprob": -0.13010716679120304, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0033744939137250185}, {"id": 546, "seek": 152264, "start": 1541.64, "end": 1546.64, "text": " because of the elephant or because of other classes making their way into the", "tokens": [51314, 570, 295, 264, 19791, 420, 570, 295, 661, 5359, 1455, 641, 636, 666, 264, 51564], "temperature": 0.0, "avg_logprob": -0.13010716679120304, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0033744939137250185}, {"id": 547, "seek": 152264, "start": 1546.64, "end": 1547.64, "text": " explanation.", "tokens": [51564, 10835, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13010716679120304, "compression_ratio": 1.7754237288135593, "no_speech_prob": 0.0033744939137250185}, {"id": 548, "seek": 154764, "start": 1547.64, "end": 1551.64, "text": " So I think personally, class specific explanations are really important to ensure", "tokens": [50364, 407, 286, 519, 5665, 11, 1508, 2685, 28708, 366, 534, 1021, 281, 5586, 50564], "temperature": 0.0, "avg_logprob": -0.24705874292474045, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0012224909150972962}, {"id": 549, "seek": 154764, "start": 1551.64, "end": 1554.64, "text": " that we're really explaining the specific prediction of the model.", "tokens": [50564, 300, 321, 434, 534, 13468, 264, 2685, 17630, 295, 264, 2316, 13, 50714], "temperature": 0.0, "avg_logprob": -0.24705874292474045, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0012224909150972962}, {"id": 550, "seek": 154764, "start": 1554.64, "end": 1555.64, "text": " We're good.", "tokens": [50714, 492, 434, 665, 13, 50764], "temperature": 0.0, "avg_logprob": -0.24705874292474045, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0012224909150972962}, {"id": 551, "seek": 154764, "start": 1564.64, "end": 1565.64, "text": " That's a fantastic question.", "tokens": [51214, 663, 311, 257, 5456, 1168, 13, 51264], "temperature": 0.0, "avg_logprob": -0.24705874292474045, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0012224909150972962}, {"id": 552, "seek": 154764, "start": 1565.64, "end": 1567.64, "text": " That's a fantastic question.", "tokens": [51264, 663, 311, 257, 5456, 1168, 13, 51364], "temperature": 0.0, "avg_logprob": -0.24705874292474045, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0012224909150972962}, {"id": 553, "seek": 154764, "start": 1567.64, "end": 1571.64, "text": " Usually people from explainability evaluate explanations differently than what you", "tokens": [51364, 11419, 561, 490, 2903, 2310, 13059, 28708, 7614, 813, 437, 291, 51564], "temperature": 0.0, "avg_logprob": -0.24705874292474045, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0012224909150972962}, {"id": 554, "seek": 154764, "start": 1571.64, "end": 1573.64, "text": " as end users may have to be.", "tokens": [51564, 382, 917, 5022, 815, 362, 281, 312, 13, 51664], "temperature": 0.0, "avg_logprob": -0.24705874292474045, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.0012224909150972962}, {"id": 555, "seek": 157364, "start": 1573.64, "end": 1577.64, "text": " So what we do is we use erasure based methods.", "tokens": [50364, 407, 437, 321, 360, 307, 321, 764, 1189, 2508, 2361, 7150, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3278950055440267, "compression_ratio": 2.02262443438914, "no_speech_prob": 0.012410841882228851}, {"id": 556, "seek": 157364, "start": 1577.64, "end": 1581.64, "text": " So what we do is we take the pixels that are said to be important to buy with it.", "tokens": [50564, 407, 437, 321, 360, 307, 321, 747, 264, 18668, 300, 366, 848, 281, 312, 1021, 281, 2256, 365, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.3278950055440267, "compression_ratio": 2.02262443438914, "no_speech_prob": 0.012410841882228851}, {"id": 557, "seek": 157364, "start": 1581.64, "end": 1585.64, "text": " We take them out and we see if the model changes its prediction or not.", "tokens": [50764, 492, 747, 552, 484, 293, 321, 536, 498, 264, 2316, 2962, 1080, 17630, 420, 406, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3278950055440267, "compression_ratio": 2.02262443438914, "no_speech_prob": 0.012410841882228851}, {"id": 558, "seek": 157364, "start": 1585.64, "end": 1587.64, "text": " And similarly, we do the other way around.", "tokens": [50964, 400, 14138, 11, 321, 360, 264, 661, 636, 926, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3278950055440267, "compression_ratio": 2.02262443438914, "no_speech_prob": 0.012410841882228851}, {"id": 559, "seek": 157364, "start": 1587.64, "end": 1591.64, "text": " We take the pixels that are unimportant by benefit and take them out and see that", "tokens": [51064, 492, 747, 264, 18668, 300, 366, 517, 41654, 538, 5121, 293, 747, 552, 484, 293, 536, 300, 51264], "temperature": 0.0, "avg_logprob": -0.3278950055440267, "compression_ratio": 2.02262443438914, "no_speech_prob": 0.012410841882228851}, {"id": 560, "seek": 157364, "start": 1591.64, "end": 1593.64, "text": " the model still predicts the same.", "tokens": [51264, 264, 2316, 920, 6069, 82, 264, 912, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3278950055440267, "compression_ratio": 2.02262443438914, "no_speech_prob": 0.012410841882228851}, {"id": 561, "seek": 157364, "start": 1593.64, "end": 1597.64, "text": " You have to take into account when you can see that the model still predicts the", "tokens": [51364, 509, 362, 281, 747, 666, 2696, 562, 291, 393, 536, 300, 264, 2316, 920, 6069, 82, 264, 51564], "temperature": 0.0, "avg_logprob": -0.3278950055440267, "compression_ratio": 2.02262443438914, "no_speech_prob": 0.012410841882228851}, {"id": 562, "seek": 157364, "start": 1597.64, "end": 1598.64, "text": " same.", "tokens": [51564, 912, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3278950055440267, "compression_ratio": 2.02262443438914, "no_speech_prob": 0.012410841882228851}, {"id": 563, "seek": 159864, "start": 1598.64, "end": 1602.64, "text": " You have to take into account when you do that, that you create images that are", "tokens": [50364, 509, 362, 281, 747, 666, 2696, 562, 291, 360, 300, 11, 300, 291, 1884, 5267, 300, 366, 50564], "temperature": 0.0, "avg_logprob": -0.23268936492584563, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0024692676961421967}, {"id": 564, "seek": 159864, "start": 1602.64, "end": 1605.64, "text": " out of the distribution of the model was trained on.", "tokens": [50564, 484, 295, 264, 7316, 295, 264, 2316, 390, 8895, 322, 13, 50714], "temperature": 0.0, "avg_logprob": -0.23268936492584563, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0024692676961421967}, {"id": 565, "seek": 159864, "start": 1605.64, "end": 1608.64, "text": " So this method is not really, you know, airtight.", "tokens": [50714, 407, 341, 3170, 307, 406, 534, 11, 291, 458, 11, 1988, 41738, 13, 50864], "temperature": 0.0, "avg_logprob": -0.23268936492584563, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0024692676961421967}, {"id": 566, "seek": 159864, "start": 1608.64, "end": 1611.64, "text": " And there's a lot of research around how do we value it.", "tokens": [50864, 400, 456, 311, 257, 688, 295, 2132, 926, 577, 360, 321, 2158, 309, 13, 51014], "temperature": 0.0, "avg_logprob": -0.23268936492584563, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0024692676961421967}, {"id": 567, "seek": 159864, "start": 1611.64, "end": 1615.64, "text": " And how do we know if the explanation is really good or not.", "tokens": [51014, 400, 577, 360, 321, 458, 498, 264, 10835, 307, 534, 665, 420, 406, 13, 51214], "temperature": 0.0, "avg_logprob": -0.23268936492584563, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0024692676961421967}, {"id": 568, "seek": 159864, "start": 1615.64, "end": 1617.64, "text": " Any other questions.", "tokens": [51214, 2639, 661, 1651, 13, 51314], "temperature": 0.0, "avg_logprob": -0.23268936492584563, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0024692676961421967}, {"id": 569, "seek": 159864, "start": 1617.64, "end": 1618.64, "text": " Yeah.", "tokens": [51314, 865, 13, 51364], "temperature": 0.0, "avg_logprob": -0.23268936492584563, "compression_ratio": 1.6029411764705883, "no_speech_prob": 0.0024692676961421967}, {"id": 570, "seek": 161864, "start": 1619.64, "end": 1630.64, "text": " Yeah.", "tokens": [50414, 865, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2905124907797955, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.021809939295053482}, {"id": 571, "seek": 161864, "start": 1630.64, "end": 1631.64, "text": " Yeah.", "tokens": [50964, 865, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2905124907797955, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.021809939295053482}, {"id": 572, "seek": 161864, "start": 1631.64, "end": 1632.64, "text": " Yeah.", "tokens": [51014, 865, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2905124907797955, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.021809939295053482}, {"id": 573, "seek": 161864, "start": 1632.64, "end": 1633.64, "text": " Yeah.", "tokens": [51064, 865, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2905124907797955, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.021809939295053482}, {"id": 574, "seek": 161864, "start": 1633.64, "end": 1634.64, "text": " Yeah.", "tokens": [51114, 865, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2905124907797955, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.021809939295053482}, {"id": 575, "seek": 161864, "start": 1634.64, "end": 1635.64, "text": " Yeah.", "tokens": [51164, 865, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2905124907797955, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.021809939295053482}, {"id": 576, "seek": 161864, "start": 1635.64, "end": 1636.64, "text": " Yeah.", "tokens": [51214, 865, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2905124907797955, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.021809939295053482}, {"id": 577, "seek": 161864, "start": 1636.64, "end": 1640.64, "text": " Um, just because we want to have a measuring stick that actually measures the", "tokens": [51264, 3301, 11, 445, 570, 321, 528, 281, 362, 257, 13389, 2897, 300, 767, 8000, 264, 51464], "temperature": 0.0, "avg_logprob": -0.2905124907797955, "compression_ratio": 1.451219512195122, "no_speech_prob": 0.021809939295053482}, {"id": 578, "seek": 164064, "start": 1640.64, "end": 1644.64, "text": " explainability without relation to the algorithm itself.", "tokens": [50364, 2903, 2310, 1553, 9721, 281, 264, 9284, 2564, 13, 50564], "temperature": 0.0, "avg_logprob": -0.23506221365421376, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.004326456226408482}, {"id": 579, "seek": 164064, "start": 1644.64, "end": 1648.64, "text": " So the measure should be unrelated to whether it's a transformer or CNN.", "tokens": [50564, 407, 264, 3481, 820, 312, 38967, 281, 1968, 309, 311, 257, 31782, 420, 24859, 13, 50764], "temperature": 0.0, "avg_logprob": -0.23506221365421376, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.004326456226408482}, {"id": 580, "seek": 164064, "start": 1648.64, "end": 1652.64, "text": " It should be unified throughout all the different architectures, right?", "tokens": [50764, 467, 820, 312, 26787, 3710, 439, 264, 819, 6331, 1303, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.23506221365421376, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.004326456226408482}, {"id": 581, "seek": 164064, "start": 1652.64, "end": 1656.64, "text": " Just as you use accuracy to measure CNNs or transformers or whatever", "tokens": [50964, 1449, 382, 291, 764, 14170, 281, 3481, 24859, 82, 420, 4088, 433, 420, 2035, 51164], "temperature": 0.0, "avg_logprob": -0.23506221365421376, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.004326456226408482}, {"id": 582, "seek": 164064, "start": 1656.64, "end": 1657.64, "text": " architecture you use.", "tokens": [51164, 9482, 291, 764, 13, 51214], "temperature": 0.0, "avg_logprob": -0.23506221365421376, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.004326456226408482}, {"id": 583, "seek": 164064, "start": 1657.64, "end": 1661.64, "text": " You want to have a measuring stick that really measures the method and not", "tokens": [51214, 509, 528, 281, 362, 257, 13389, 2897, 300, 534, 8000, 264, 3170, 293, 406, 51414], "temperature": 0.0, "avg_logprob": -0.23506221365421376, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.004326456226408482}, {"id": 584, "seek": 164064, "start": 1661.64, "end": 1664.64, "text": " something that has something to do with specifically.", "tokens": [51414, 746, 300, 575, 746, 281, 360, 365, 4682, 13, 51564], "temperature": 0.0, "avg_logprob": -0.23506221365421376, "compression_ratio": 1.7044534412955465, "no_speech_prob": 0.004326456226408482}, {"id": 585, "seek": 166464, "start": 1665.64, "end": 1668.64, "text": " If you have an explanation for CNN, it also has, you know,", "tokens": [50414, 759, 291, 362, 364, 10835, 337, 24859, 11, 309, 611, 575, 11, 291, 458, 11, 50564], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 586, "seek": 166464, "start": 1668.64, "end": 1670.64, "text": " values for each pixel.", "tokens": [50564, 4190, 337, 1184, 19261, 13, 50664], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 587, "seek": 166464, "start": 1670.64, "end": 1671.64, "text": " Yeah.", "tokens": [50664, 865, 13, 50714], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 588, "seek": 166464, "start": 1671.64, "end": 1672.64, "text": " Yeah.", "tokens": [50714, 865, 13, 50764], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 589, "seek": 166464, "start": 1672.64, "end": 1673.64, "text": " Just zero it out.", "tokens": [50764, 1449, 4018, 309, 484, 13, 50814], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 590, "seek": 166464, "start": 1673.64, "end": 1674.64, "text": " You just zero it out.", "tokens": [50814, 509, 445, 4018, 309, 484, 13, 50864], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 591, "seek": 166464, "start": 1674.64, "end": 1676.64, "text": " And it works on the input itself.", "tokens": [50864, 400, 309, 1985, 322, 264, 4846, 2564, 13, 50964], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 592, "seek": 166464, "start": 1676.64, "end": 1677.64, "text": " So it really,", "tokens": [50964, 407, 309, 534, 11, 51014], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 593, "seek": 166464, "start": 1677.64, "end": 1679.64, "text": " it is undependent even of.", "tokens": [51014, 309, 307, 517, 36763, 317, 754, 295, 13, 51114], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 594, "seek": 166464, "start": 1679.64, "end": 1680.64, "text": " You know,", "tokens": [51114, 509, 458, 11, 51164], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 595, "seek": 166464, "start": 1680.64, "end": 1681.64, "text": " you know,", "tokens": [51164, 291, 458, 11, 51214], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 596, "seek": 166464, "start": 1681.64, "end": 1682.64, "text": " you know,", "tokens": [51214, 291, 458, 11, 51264], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 597, "seek": 166464, "start": 1682.64, "end": 1683.64, "text": " you know,", "tokens": [51264, 291, 458, 11, 51314], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 598, "seek": 166464, "start": 1683.64, "end": 1684.64, "text": " you know,", "tokens": [51314, 291, 458, 11, 51364], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 599, "seek": 166464, "start": 1684.64, "end": 1685.64, "text": " you know,", "tokens": [51364, 291, 458, 11, 51414], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 600, "seek": 166464, "start": 1685.64, "end": 1686.64, "text": " you know,", "tokens": [51414, 291, 458, 11, 51464], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 601, "seek": 166464, "start": 1686.64, "end": 1687.64, "text": " you know,", "tokens": [51464, 291, 458, 11, 51514], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 602, "seek": 166464, "start": 1687.64, "end": 1688.64, "text": " you know,", "tokens": [51514, 291, 458, 11, 51564], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 603, "seek": 166464, "start": 1688.64, "end": 1689.64, "text": " you know,", "tokens": [51564, 291, 458, 11, 51614], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 604, "seek": 166464, "start": 1689.64, "end": 1690.64, "text": " you know,", "tokens": [51614, 291, 458, 11, 51664], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 605, "seek": 166464, "start": 1690.64, "end": 1691.64, "text": " you know,", "tokens": [51664, 291, 458, 11, 51714], "temperature": 0.0, "avg_logprob": -0.37240049574110246, "compression_ratio": 2.1161290322580646, "no_speech_prob": 0.01362222246825695}, {"id": 606, "seek": 169164, "start": 1691.64, "end": 1692.64, "text": " you know,", "tokens": [50364, 291, 458, 11, 50414], "temperature": 1.0, "avg_logprob": -0.8847550036860448, "compression_ratio": 1.53125, "no_speech_prob": 0.009541049599647522}, {"id": 607, "seek": 169164, "start": 1692.64, "end": 1695.64, "text": " it really it is independent even of the method you use or the model.", "tokens": [50414, 309, 534, 309, 307, 6695, 754, 295, 264, 3170, 291, 764, 420, 264, 2316, 13, 50564], "temperature": 1.0, "avg_logprob": -0.8847550036860448, "compression_ratio": 1.53125, "no_speech_prob": 0.009541049599647522}, {"id": 608, "seek": 169164, "start": 1695.64, "end": 1717.64, "text": " It is a measuring stick that has nothing to do with which method you use", "tokens": [50564, 467, 307, 257, 13389, 2897, 300, 575, 1825, 281, 360, 365, 597, 3170, 291, 764, 51664], "temperature": 1.0, "avg_logprob": -0.8847550036860448, "compression_ratio": 1.53125, "no_speech_prob": 0.009541049599647522}, {"id": 609, "seek": 169164, "start": 1717.64, "end": 1720.64, "text": " for expansion or expansion in which version.", "tokens": [51664, 337, 11260, 420, 11260, 294, 597, 3037, 13, 51814], "temperature": 1.0, "avg_logprob": -0.8847550036860448, "compression_ratio": 1.53125, "no_speech_prob": 0.009541049599647522}, {"id": 610, "seek": 172064, "start": 1720.64, "end": 1729.8400000000001, "text": " the sun. I'm not sure I got your question exactly, but I would say that there are methods evaluating", "tokens": [50364, 264, 3295, 13, 286, 478, 406, 988, 286, 658, 428, 1168, 2293, 11, 457, 286, 576, 584, 300, 456, 366, 7150, 27479, 50824], "temperature": 0.0, "avg_logprob": -0.25482167018933244, "compression_ratio": 1.8767772511848342, "no_speech_prob": 0.050234418362379074}, {"id": 611, "seek": 172064, "start": 1729.8400000000001, "end": 1735.44, "text": " explanations by adding sparse correlation, making sure that the model reaches 100 percent accuracy", "tokens": [50824, 28708, 538, 5127, 637, 11668, 20009, 11, 1455, 988, 300, 264, 2316, 14235, 2319, 3043, 14170, 51104], "temperature": 0.0, "avg_logprob": -0.25482167018933244, "compression_ratio": 1.8767772511848342, "no_speech_prob": 0.050234418362379074}, {"id": 612, "seek": 172064, "start": 1735.44, "end": 1740.4, "text": " due to the sparse correlations, and then making sure that the explanation outputs these sparse", "tokens": [51104, 3462, 281, 264, 637, 11668, 13983, 763, 11, 293, 550, 1455, 988, 300, 264, 10835, 23930, 613, 637, 11668, 51352], "temperature": 0.0, "avg_logprob": -0.25482167018933244, "compression_ratio": 1.8767772511848342, "no_speech_prob": 0.050234418362379074}, {"id": 613, "seek": 172064, "start": 1740.4, "end": 1746.24, "text": " correlations versus the odd correlation. So there are methods that do that, but yeah. But we usually,", "tokens": [51352, 13983, 763, 5717, 264, 7401, 20009, 13, 407, 456, 366, 7150, 300, 360, 300, 11, 457, 1338, 13, 583, 321, 2673, 11, 51644], "temperature": 0.0, "avg_logprob": -0.25482167018933244, "compression_ratio": 1.8767772511848342, "no_speech_prob": 0.050234418362379074}, {"id": 614, "seek": 174624, "start": 1746.24, "end": 1751.28, "text": " I usually use erasers as methods to evaluate explanations, but this is a really active", "tokens": [50364, 286, 2673, 764, 1189, 296, 433, 382, 7150, 281, 13059, 28708, 11, 457, 341, 307, 257, 534, 4967, 50616], "temperature": 0.0, "avg_logprob": -0.2452815662730824, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0021812370978295803}, {"id": 615, "seek": 174624, "start": 1751.28, "end": 1756.64, "text": " goal of research, right? So it's not really, you know, obvious how we evaluate explanations", "tokens": [50616, 3387, 295, 2132, 11, 558, 30, 407, 309, 311, 406, 534, 11, 291, 458, 11, 6322, 577, 321, 13059, 28708, 50884], "temperature": 0.0, "avg_logprob": -0.2452815662730824, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0021812370978295803}, {"id": 616, "seek": 174624, "start": 1756.64, "end": 1764.88, "text": " and what's the right way to do that. I think I'm maybe moving backwards instead of forward.", "tokens": [50884, 293, 437, 311, 264, 558, 636, 281, 360, 300, 13, 286, 519, 286, 478, 1310, 2684, 12204, 2602, 295, 2128, 13, 51296], "temperature": 0.0, "avg_logprob": -0.2452815662730824, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0021812370978295803}, {"id": 617, "seek": 174624, "start": 1766.0, "end": 1772.16, "text": " Some technical issues. Yeah, okay. I may just skip this because we do have the motivation", "tokens": [51352, 2188, 6191, 2663, 13, 865, 11, 1392, 13, 286, 815, 445, 10023, 341, 570, 321, 360, 362, 264, 12335, 51660], "temperature": 0.0, "avg_logprob": -0.2452815662730824, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0021812370978295803}, {"id": 618, "seek": 177216, "start": 1772.16, "end": 1777.6000000000001, "text": " that we did in the beginning and we're a bit behind on time. So our second method", "tokens": [50364, 300, 321, 630, 294, 264, 2863, 293, 321, 434, 257, 857, 2261, 322, 565, 13, 407, 527, 1150, 3170, 50636], "temperature": 0.0, "avg_logprob": -0.15656526796110384, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0017000490333884954}, {"id": 619, "seek": 177216, "start": 1778.5600000000002, "end": 1784.0800000000002, "text": " said, you know what? We really believe that multimodal models are going to be a big thing.", "tokens": [50684, 848, 11, 291, 458, 437, 30, 492, 534, 1697, 300, 32972, 378, 304, 5245, 366, 516, 281, 312, 257, 955, 551, 13, 50960], "temperature": 0.0, "avg_logprob": -0.15656526796110384, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0017000490333884954}, {"id": 620, "seek": 177216, "start": 1784.96, "end": 1789.52, "text": " And we only explained self-attention before, as you saw. We didn't go into", "tokens": [51004, 400, 321, 787, 8825, 2698, 12, 1591, 1251, 949, 11, 382, 291, 1866, 13, 492, 994, 380, 352, 666, 51232], "temperature": 0.0, "avg_logprob": -0.15656526796110384, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0017000490333884954}, {"id": 621, "seek": 177216, "start": 1789.52, "end": 1796.3200000000002, "text": " cross-attention or encoded encoded attention. And assuming that most transformers don't just do", "tokens": [51232, 3278, 12, 1591, 1251, 420, 2058, 12340, 2058, 12340, 3202, 13, 400, 11926, 300, 881, 4088, 433, 500, 380, 445, 360, 51572], "temperature": 0.0, "avg_logprob": -0.15656526796110384, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0017000490333884954}, {"id": 622, "seek": 179632, "start": 1796.32, "end": 1799.52, "text": " right self-attention, we need a mechanism that can explain", "tokens": [50364, 558, 2698, 12, 1591, 1251, 11, 321, 643, 257, 7513, 300, 393, 2903, 50524], "temperature": 0.0, "avg_logprob": -0.10583069704580998, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.0003625286335591227}, {"id": 623, "seek": 179632, "start": 1800.1599999999999, "end": 1804.08, "text": " cross-attention and encoded encoded attention as well, not just self-attention.", "tokens": [50556, 3278, 12, 1591, 1251, 293, 2058, 12340, 2058, 12340, 3202, 382, 731, 11, 406, 445, 2698, 12, 1591, 1251, 13, 50752], "temperature": 0.0, "avg_logprob": -0.10583069704580998, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.0003625286335591227}, {"id": 624, "seek": 179632, "start": 1804.08, "end": 1809.6, "text": " So the second paper actually expands the first paper, but for other types of attention.", "tokens": [50752, 407, 264, 1150, 3035, 767, 33706, 264, 700, 3035, 11, 457, 337, 661, 3467, 295, 3202, 13, 51028], "temperature": 0.0, "avg_logprob": -0.10583069704580998, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.0003625286335591227}, {"id": 625, "seek": 179632, "start": 1812.08, "end": 1816.56, "text": " So the first thing we do is get rid of the LLP, and that's why I don't, you know, get into a lot", "tokens": [51152, 407, 264, 700, 551, 321, 360, 307, 483, 3973, 295, 264, 441, 45196, 11, 293, 300, 311, 983, 286, 500, 380, 11, 291, 458, 11, 483, 666, 257, 688, 51376], "temperature": 0.0, "avg_logprob": -0.10583069704580998, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.0003625286335591227}, {"id": 626, "seek": 179632, "start": 1816.56, "end": 1821.36, "text": " of detail with regards to the LLP. The reason that we did that is because if you think about it,", "tokens": [51376, 295, 2607, 365, 14258, 281, 264, 441, 45196, 13, 440, 1778, 300, 321, 630, 300, 307, 570, 498, 291, 519, 466, 309, 11, 51616], "temperature": 0.0, "avg_logprob": -0.10583069704580998, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.0003625286335591227}, {"id": 627, "seek": 179632, "start": 1821.36, "end": 1825.9199999999998, "text": " we use LLP in order to account for all the layers, but really gradients account for all the layers", "tokens": [51616, 321, 764, 441, 45196, 294, 1668, 281, 2696, 337, 439, 264, 7914, 11, 457, 534, 2771, 2448, 2696, 337, 439, 264, 7914, 51844], "temperature": 0.0, "avg_logprob": -0.10583069704580998, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.0003625286335591227}, {"id": 628, "seek": 182592, "start": 1825.92, "end": 1830.0800000000002, "text": " because backpropagation is backpropagated from the output all the way back together.", "tokens": [50364, 570, 646, 79, 1513, 559, 399, 307, 646, 79, 1513, 559, 770, 490, 264, 5598, 439, 264, 636, 646, 1214, 13, 50572], "temperature": 0.0, "avg_logprob": -0.1353584172432883, "compression_ratio": 1.673992673992674, "no_speech_prob": 0.00039192085387185216}, {"id": 629, "seek": 182592, "start": 1830.8000000000002, "end": 1835.3600000000001, "text": " So we said, what happens if we remove LLP, which makes it easier for you guys to implement", "tokens": [50608, 407, 321, 848, 11, 437, 2314, 498, 321, 4159, 441, 45196, 11, 597, 1669, 309, 3571, 337, 291, 1074, 281, 4445, 50836], "temperature": 0.0, "avg_logprob": -0.1353584172432883, "compression_ratio": 1.673992673992674, "no_speech_prob": 0.00039192085387185216}, {"id": 630, "seek": 182592, "start": 1835.3600000000001, "end": 1841.2, "text": " the algorithm, and it makes it faster and more convenient, and it actually works pretty well.", "tokens": [50836, 264, 9284, 11, 293, 309, 1669, 309, 4663, 293, 544, 10851, 11, 293, 309, 767, 1985, 1238, 731, 13, 51128], "temperature": 0.0, "avg_logprob": -0.1353584172432883, "compression_ratio": 1.673992673992674, "no_speech_prob": 0.00039192085387185216}, {"id": 631, "seek": 182592, "start": 1841.2, "end": 1846.64, "text": " So we remove the LLP component. I will say that if you want really accurate explanations,", "tokens": [51128, 407, 321, 4159, 264, 441, 45196, 6542, 13, 286, 486, 584, 300, 498, 291, 528, 534, 8559, 28708, 11, 51400], "temperature": 0.0, "avg_logprob": -0.1353584172432883, "compression_ratio": 1.673992673992674, "no_speech_prob": 0.00039192085387185216}, {"id": 632, "seek": 182592, "start": 1846.64, "end": 1853.2, "text": " usually I would go for the LLP version, right? Because LLP adds this added component that doesn't", "tokens": [51400, 2673, 286, 576, 352, 337, 264, 441, 45196, 3037, 11, 558, 30, 1436, 441, 45196, 10860, 341, 3869, 6542, 300, 1177, 380, 51728], "temperature": 0.0, "avg_logprob": -0.1353584172432883, "compression_ratio": 1.673992673992674, "no_speech_prob": 0.00039192085387185216}, {"id": 633, "seek": 185320, "start": 1853.2, "end": 1857.68, "text": " exist without LLP. It does account for all the layers quite systematically.", "tokens": [50364, 2514, 1553, 441, 45196, 13, 467, 775, 2696, 337, 439, 264, 7914, 1596, 39531, 13, 50588], "temperature": 0.0, "avg_logprob": -0.09360641656919967, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.00015112824621610343}, {"id": 634, "seek": 185320, "start": 1859.1200000000001, "end": 1864.48, "text": " So when we talk about cross-model interactions, we have four types of interactions in such models.", "tokens": [50660, 407, 562, 321, 751, 466, 3278, 12, 8014, 338, 13280, 11, 321, 362, 1451, 3467, 295, 13280, 294, 1270, 5245, 13, 50928], "temperature": 0.0, "avg_logprob": -0.09360641656919967, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.00015112824621610343}, {"id": 635, "seek": 185320, "start": 1864.48, "end": 1869.1200000000001, "text": " We have the self-attention interactions between the text tokens, how the text tokens influence", "tokens": [50928, 492, 362, 264, 2698, 12, 1591, 1251, 13280, 1296, 264, 2487, 22667, 11, 577, 264, 2487, 22667, 6503, 51160], "temperature": 0.0, "avg_logprob": -0.09360641656919967, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.00015112824621610343}, {"id": 636, "seek": 185320, "start": 1869.1200000000001, "end": 1876.16, "text": " themselves, the self-attention interactions between the image tokens, and then two types of cross-attention", "tokens": [51160, 2969, 11, 264, 2698, 12, 1591, 1251, 13280, 1296, 264, 3256, 22667, 11, 293, 550, 732, 3467, 295, 3278, 12, 1591, 1251, 51512], "temperature": 0.0, "avg_logprob": -0.09360641656919967, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.00015112824621610343}, {"id": 637, "seek": 187616, "start": 1876.16, "end": 1880.16, "text": " interactions, how text influences image and how image influences.", "tokens": [50364, 13280, 11, 577, 2487, 21222, 3256, 293, 577, 3256, 21222, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1407969957822329, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.0009694827604107559}, {"id": 638, "seek": 187616, "start": 1883.92, "end": 1890.16, "text": " And then what we thought we would do is really track the self-attention layers.", "tokens": [50752, 400, 550, 437, 321, 1194, 321, 576, 360, 307, 534, 2837, 264, 2698, 12, 1591, 1251, 7914, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1407969957822329, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.0009694827604107559}, {"id": 639, "seek": 187616, "start": 1890.16, "end": 1895.76, "text": " So each self-attention layer mixes tokens. Okay, we'll mix the tokens in the relevance matrices.", "tokens": [51064, 407, 1184, 2698, 12, 1591, 1251, 4583, 37121, 22667, 13, 1033, 11, 321, 603, 2890, 264, 22667, 294, 264, 32684, 32284, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1407969957822329, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.0009694827604107559}, {"id": 640, "seek": 187616, "start": 1896.4, "end": 1902.0, "text": " So we start with an initialization of the relevance matrices at the beginning of the", "tokens": [51376, 407, 321, 722, 365, 364, 5883, 2144, 295, 264, 32684, 32284, 412, 264, 2863, 295, 264, 51656], "temperature": 0.0, "avg_logprob": -0.1407969957822329, "compression_ratio": 1.712041884816754, "no_speech_prob": 0.0009694827604107559}, {"id": 641, "seek": 190200, "start": 1902.0, "end": 1907.36, "text": " modalities or self-contained. So images only affect images and text only affects text,", "tokens": [50364, 1072, 16110, 420, 2698, 12, 9000, 3563, 13, 407, 5267, 787, 3345, 5267, 293, 2487, 787, 11807, 2487, 11, 50632], "temperature": 0.0, "avg_logprob": -0.08624506434169385, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0002452910121064633}, {"id": 642, "seek": 190200, "start": 1907.36, "end": 1912.32, "text": " and each image token only influences itself. So the initialization for the self-attention", "tokens": [50632, 293, 1184, 3256, 14862, 787, 21222, 2564, 13, 407, 264, 5883, 2144, 337, 264, 2698, 12, 1591, 1251, 50880], "temperature": 0.0, "avg_logprob": -0.08624506434169385, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0002452910121064633}, {"id": 643, "seek": 190200, "start": 1912.32, "end": 1918.24, "text": " relations are just the identity matrices. And for the cross-model relations, it's a zero matrix,", "tokens": [50880, 2299, 366, 445, 264, 6575, 32284, 13, 400, 337, 264, 3278, 12, 8014, 338, 2299, 11, 309, 311, 257, 4018, 8141, 11, 51176], "temperature": 0.0, "avg_logprob": -0.08624506434169385, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0002452910121064633}, {"id": 644, "seek": 190200, "start": 1918.24, "end": 1923.44, "text": " because there are no cross-model interactions before we do any attention. And what the method", "tokens": [51176, 570, 456, 366, 572, 3278, 12, 8014, 338, 13280, 949, 321, 360, 604, 3202, 13, 400, 437, 264, 3170, 51436], "temperature": 0.0, "avg_logprob": -0.08624506434169385, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0002452910121064633}, {"id": 645, "seek": 190200, "start": 1923.44, "end": 1929.04, "text": " really does is it just goes on a forward pass through the attention layers, and as the attention", "tokens": [51436, 534, 775, 307, 309, 445, 1709, 322, 257, 2128, 1320, 807, 264, 3202, 7914, 11, 293, 382, 264, 3202, 51716], "temperature": 0.0, "avg_logprob": -0.08624506434169385, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.0002452910121064633}, {"id": 646, "seek": 192904, "start": 1929.04, "end": 1935.84, "text": " layers mix the tokens, the relevance values are mixed as well, just tracking the attention as it goes.", "tokens": [50364, 7914, 2890, 264, 22667, 11, 264, 32684, 4190, 366, 7467, 382, 731, 11, 445, 11603, 264, 3202, 382, 309, 1709, 13, 50704], "temperature": 0.0, "avg_logprob": -0.0939573405944195, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.00030513631645590067}, {"id": 647, "seek": 192904, "start": 1937.6, "end": 1942.8799999999999, "text": " So I won't get into all the rules, all the rules that we have for all these specific attention", "tokens": [50792, 407, 286, 1582, 380, 483, 666, 439, 264, 4474, 11, 439, 264, 4474, 300, 321, 362, 337, 439, 613, 2685, 3202, 51056], "temperature": 0.0, "avg_logprob": -0.0939573405944195, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.00030513631645590067}, {"id": 648, "seek": 192904, "start": 1942.8799999999999, "end": 1946.6399999999999, "text": " layers. I'm just giving you a motivation of how it works. And really, believe me, it's really", "tokens": [51056, 7914, 13, 286, 478, 445, 2902, 291, 257, 12335, 295, 577, 309, 1985, 13, 400, 534, 11, 1697, 385, 11, 309, 311, 534, 51244], "temperature": 0.0, "avg_logprob": -0.0939573405944195, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.00030513631645590067}, {"id": 649, "seek": 192904, "start": 1946.6399999999999, "end": 1953.04, "text": " simple, even though the equations look complicated. So let's go over just the self-attention rule.", "tokens": [51244, 2199, 11, 754, 1673, 264, 11787, 574, 6179, 13, 407, 718, 311, 352, 670, 445, 264, 2698, 12, 1591, 1251, 4978, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0939573405944195, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.00030513631645590067}, {"id": 650, "seek": 195304, "start": 1953.76, "end": 1960.72, "text": " A self-attention layer has, again, multiple heads. We average across the heads using gradients just as", "tokens": [50400, 316, 2698, 12, 1591, 1251, 4583, 575, 11, 797, 11, 3866, 8050, 13, 492, 4274, 2108, 264, 8050, 1228, 2771, 2448, 445, 382, 50748], "temperature": 0.0, "avg_logprob": -0.10511001673611728, "compression_ratio": 1.8657407407407407, "no_speech_prob": 0.0007432131096720695}, {"id": 651, "seek": 195304, "start": 1960.72, "end": 1968.32, "text": " before. So we have now a single attention matrix marked here as a bar. And what we do again is just", "tokens": [50748, 949, 13, 407, 321, 362, 586, 257, 2167, 3202, 8141, 12658, 510, 382, 257, 2159, 13, 400, 437, 321, 360, 797, 307, 445, 51128], "temperature": 0.0, "avg_logprob": -0.10511001673611728, "compression_ratio": 1.8657407407407407, "no_speech_prob": 0.0007432131096720695}, {"id": 652, "seek": 195304, "start": 1968.32, "end": 1975.36, "text": " matrix multiplication between the current attention mixture and the old attention mixture that existed", "tokens": [51128, 8141, 27290, 1296, 264, 2190, 3202, 9925, 293, 264, 1331, 3202, 9925, 300, 13135, 51480], "temperature": 0.0, "avg_logprob": -0.10511001673611728, "compression_ratio": 1.8657407407407407, "no_speech_prob": 0.0007432131096720695}, {"id": 653, "seek": 195304, "start": 1975.36, "end": 1982.32, "text": " in the relevance matrix. So matrix multiplication and update the relevance matrix. This is all we", "tokens": [51480, 294, 264, 32684, 8141, 13, 407, 8141, 27290, 293, 5623, 264, 32684, 8141, 13, 639, 307, 439, 321, 51828], "temperature": 0.0, "avg_logprob": -0.10511001673611728, "compression_ratio": 1.8657407407407407, "no_speech_prob": 0.0007432131096720695}, {"id": 654, "seek": 198232, "start": 1982.32, "end": 1986.96, "text": " do. We just track the attention as it goes. As it mixes between tokens, we mix between the", "tokens": [50364, 360, 13, 492, 445, 2837, 264, 3202, 382, 309, 1709, 13, 1018, 309, 37121, 1296, 22667, 11, 321, 2890, 1296, 264, 50596], "temperature": 0.0, "avg_logprob": -0.08430082421553763, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.00021989128435961902}, {"id": 655, "seek": 198232, "start": 1986.96, "end": 1992.1599999999999, "text": " relevance values. That's what we do. That's the entire algorithm. And head aggregation is done", "tokens": [50596, 32684, 4190, 13, 663, 311, 437, 321, 360, 13, 663, 311, 264, 2302, 9284, 13, 400, 1378, 16743, 399, 307, 1096, 50856], "temperature": 0.0, "avg_logprob": -0.08430082421553763, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.00021989128435961902}, {"id": 656, "seek": 198232, "start": 1992.8, "end": 2000.48, "text": " via gradients as before. So taking a look at some examples that we have to demonstrate how this works.", "tokens": [50888, 5766, 2771, 2448, 382, 949, 13, 407, 1940, 257, 574, 412, 512, 5110, 300, 321, 362, 281, 11698, 577, 341, 1985, 13, 51272], "temperature": 0.0, "avg_logprob": -0.08430082421553763, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.00021989128435961902}, {"id": 657, "seek": 198232, "start": 2001.04, "end": 2006.96, "text": " For example, for CLIP, you can see that we've entered different texts with the same input image", "tokens": [51300, 1171, 1365, 11, 337, 12855, 9139, 11, 291, 393, 536, 300, 321, 600, 9065, 819, 15765, 365, 264, 912, 4846, 3256, 51596], "temperature": 0.0, "avg_logprob": -0.08430082421553763, "compression_ratio": 1.5483870967741935, "no_speech_prob": 0.00021989128435961902}, {"id": 658, "seek": 200696, "start": 2006.96, "end": 2012.88, "text": " and propagated gradients. And by the way, for CLIP, gradients are propagated. Let's take them back", "tokens": [50364, 293, 12425, 770, 2771, 2448, 13, 400, 538, 264, 636, 11, 337, 12855, 9139, 11, 2771, 2448, 366, 12425, 770, 13, 961, 311, 747, 552, 646, 50660], "temperature": 0.0, "avg_logprob": -0.11325359344482422, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.0007435731822624803}, {"id": 659, "seek": 200696, "start": 2012.88, "end": 2019.44, "text": " to the whiteboard. For CLIP, because I know this is specifically interesting to you,", "tokens": [50660, 281, 264, 2418, 3787, 13, 1171, 12855, 9139, 11, 570, 286, 458, 341, 307, 4682, 1880, 281, 291, 11, 50988], "temperature": 0.0, "avg_logprob": -0.11325359344482422, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.0007435731822624803}, {"id": 660, "seek": 200696, "start": 2020.48, "end": 2026.96, "text": " let's talk about how we propagate relevance for CLIP. For CLIP, you have an imaging quarter", "tokens": [51040, 718, 311, 751, 466, 577, 321, 48256, 32684, 337, 12855, 9139, 13, 1171, 12855, 9139, 11, 291, 362, 364, 25036, 6555, 51364], "temperature": 0.0, "avg_logprob": -0.11325359344482422, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.0007435731822624803}, {"id": 661, "seek": 200696, "start": 2028.64, "end": 2033.1200000000001, "text": " and then a texting quarter. Both of them, by the way, use pure self-attention. So there's no cross", "tokens": [51448, 293, 550, 257, 29897, 6555, 13, 6767, 295, 552, 11, 538, 264, 636, 11, 764, 6075, 2698, 12, 1591, 1251, 13, 407, 456, 311, 572, 3278, 51672], "temperature": 0.0, "avg_logprob": -0.11325359344482422, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.0007435731822624803}, {"id": 662, "seek": 203312, "start": 2033.1999999999998, "end": 2041.4399999999998, "text": " connection. This and this output representation and vector, which is by the way from the", "tokens": [50368, 4984, 13, 639, 293, 341, 5598, 10290, 293, 8062, 11, 597, 307, 538, 264, 636, 490, 264, 50780], "temperature": 0.0, "avg_logprob": -0.27011096477508545, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.0004581404209602624}, {"id": 663, "seek": 203312, "start": 2041.4399999999998, "end": 2046.7199999999998, "text": " classification. So this is the vector for the text and this is the vector for the image.", "tokens": [50780, 21538, 13, 407, 341, 307, 264, 8062, 337, 264, 2487, 293, 341, 307, 264, 8062, 337, 264, 3256, 13, 51044], "temperature": 0.0, "avg_logprob": -0.27011096477508545, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.0004581404209602624}, {"id": 664, "seek": 203312, "start": 2047.6, "end": 2056.64, "text": " And the stimuli score is just a dot product. Both scores. So what we do is we propagate", "tokens": [51088, 400, 264, 47752, 6175, 307, 445, 257, 5893, 1674, 13, 6767, 13444, 13, 407, 437, 321, 360, 307, 321, 48256, 51540], "temperature": 0.0, "avg_logprob": -0.27011096477508545, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.0004581404209602624}, {"id": 665, "seek": 205664, "start": 2056.64, "end": 2063.3599999999997, "text": " gradients from this dot product back to the texting quarter", "tokens": [50364, 2771, 2448, 490, 341, 5893, 1674, 646, 281, 264, 29897, 6555, 50700], "temperature": 0.0, "avg_logprob": -0.13769780435869772, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.001016093883663416}, {"id": 666, "seek": 205664, "start": 2065.2799999999997, "end": 2069.3599999999997, "text": " and back to the imaging quarter. And those gradients are going to be used to average", "tokens": [50796, 293, 646, 281, 264, 25036, 6555, 13, 400, 729, 2771, 2448, 366, 516, 281, 312, 1143, 281, 4274, 51000], "temperature": 0.0, "avg_logprob": -0.13769780435869772, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.001016093883663416}, {"id": 667, "seek": 205664, "start": 2069.3599999999997, "end": 2073.52, "text": " across the attention pens as we saw before. And then the attention heads are going to be", "tokens": [51000, 2108, 264, 3202, 6099, 382, 321, 1866, 949, 13, 400, 550, 264, 3202, 8050, 366, 516, 281, 312, 51208], "temperature": 0.0, "avg_logprob": -0.13769780435869772, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.001016093883663416}, {"id": 668, "seek": 205664, "start": 2073.52, "end": 2079.2799999999997, "text": " aggregated across different letters by matrix multiplication. So here we don't have an output", "tokens": [51208, 16743, 770, 2108, 819, 7825, 538, 8141, 27290, 13, 407, 510, 321, 500, 380, 362, 364, 5598, 51496], "temperature": 0.0, "avg_logprob": -0.13769780435869772, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.001016093883663416}, {"id": 669, "seek": 205664, "start": 2079.2799999999997, "end": 2085.7599999999998, "text": " logic as we have for the classification, but we use this dot product between their presentations", "tokens": [51496, 9952, 382, 321, 362, 337, 264, 21538, 11, 457, 321, 764, 341, 5893, 1674, 1296, 641, 18964, 51820], "temperature": 0.0, "avg_logprob": -0.13769780435869772, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.001016093883663416}, {"id": 670, "seek": 208576, "start": 2085.76, "end": 2091.5200000000004, "text": " to calculate the score that we propagate the gradients with regards to. So all that we do", "tokens": [50364, 281, 8873, 264, 6175, 300, 321, 48256, 264, 2771, 2448, 365, 14258, 281, 13, 407, 439, 300, 321, 360, 50652], "temperature": 0.0, "avg_logprob": -0.09739655429877124, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.00014650590310338885}, {"id": 671, "seek": 208576, "start": 2091.5200000000004, "end": 2096.5600000000004, "text": " here is really simple. Calculate the dot product between their presentations, propagate gradients", "tokens": [50652, 510, 307, 534, 2199, 13, 3511, 2444, 473, 264, 5893, 1674, 1296, 641, 18964, 11, 48256, 2771, 2448, 50904], "temperature": 0.0, "avg_logprob": -0.09739655429877124, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.00014650590310338885}, {"id": 672, "seek": 208576, "start": 2096.5600000000004, "end": 2101.92, "text": " with regards to the dot product. Those gradients are going to be used as weights for the attention", "tokens": [50904, 365, 14258, 281, 264, 5893, 1674, 13, 3950, 2771, 2448, 366, 516, 281, 312, 1143, 382, 17443, 337, 264, 3202, 51172], "temperature": 0.0, "avg_logprob": -0.09739655429877124, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.00014650590310338885}, {"id": 673, "seek": 208576, "start": 2101.92, "end": 2110.0800000000004, "text": " matrices to average across them. And as you can see, the results are text specific since we", "tokens": [51172, 32284, 281, 4274, 2108, 552, 13, 400, 382, 291, 393, 536, 11, 264, 3542, 366, 2487, 2685, 1670, 321, 51580], "temperature": 0.0, "avg_logprob": -0.09739655429877124, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.00014650590310338885}, {"id": 674, "seek": 208576, "start": 2110.0800000000004, "end": 2114.48, "text": " propagated the gradients with regards to the specific multiplication between the specific text", "tokens": [51580, 12425, 770, 264, 2771, 2448, 365, 14258, 281, 264, 2685, 27290, 1296, 264, 2685, 2487, 51800], "temperature": 0.0, "avg_logprob": -0.09739655429877124, "compression_ratio": 2.0300429184549356, "no_speech_prob": 0.00014650590310338885}, {"id": 675, "seek": 211448, "start": 2114.48, "end": 2118.88, "text": " and the specific image. So actually for an elephant, you can see that the hidden map", "tokens": [50364, 293, 264, 2685, 3256, 13, 407, 767, 337, 364, 19791, 11, 291, 393, 536, 300, 264, 7633, 4471, 50584], "temperature": 0.0, "avg_logprob": -0.12224355558069741, "compression_ratio": 1.9855595667870036, "no_speech_prob": 0.00011408701539039612}, {"id": 676, "seek": 211448, "start": 2118.88, "end": 2122.2400000000002, "text": " corresponds to the elephant. For a zebra, the hidden map corresponds to the zebra. And for a", "tokens": [50584, 23249, 281, 264, 19791, 13, 1171, 257, 47060, 11, 264, 7633, 4471, 23249, 281, 264, 47060, 13, 400, 337, 257, 50752], "temperature": 0.0, "avg_logprob": -0.12224355558069741, "compression_ratio": 1.9855595667870036, "no_speech_prob": 0.00011408701539039612}, {"id": 677, "seek": 211448, "start": 2122.2400000000002, "end": 2125.68, "text": " leg, the hidden map corresponds to the leg, showing us that the model really knows how to", "tokens": [50752, 1676, 11, 264, 7633, 4471, 23249, 281, 264, 1676, 11, 4099, 505, 300, 264, 2316, 534, 3255, 577, 281, 50924], "temperature": 0.0, "avg_logprob": -0.12224355558069741, "compression_ratio": 1.9855595667870036, "no_speech_prob": 0.00011408701539039612}, {"id": 678, "seek": 211448, "start": 2125.68, "end": 2130.2400000000002, "text": " distinct between different parts or different objects in the image according to the text input", "tokens": [50924, 10644, 1296, 819, 3166, 420, 819, 6565, 294, 264, 3256, 4650, 281, 264, 2487, 4846, 51152], "temperature": 0.0, "avg_logprob": -0.12224355558069741, "compression_ratio": 1.9855595667870036, "no_speech_prob": 0.00011408701539039612}, {"id": 679, "seek": 211448, "start": 2130.2400000000002, "end": 2138.2400000000002, "text": " that we give it. This is an example that we saw before. And visual question answering in case", "tokens": [51152, 300, 321, 976, 309, 13, 639, 307, 364, 1365, 300, 321, 1866, 949, 13, 400, 5056, 1168, 13430, 294, 1389, 51552], "temperature": 0.0, "avg_logprob": -0.12224355558069741, "compression_ratio": 1.9855595667870036, "no_speech_prob": 0.00011408701539039612}, {"id": 680, "seek": 211448, "start": 2138.2400000000002, "end": 2143.36, "text": " any one of you is interested is actually an interesting use case. Because for visual question", "tokens": [51552, 604, 472, 295, 291, 307, 3102, 307, 767, 364, 1880, 764, 1389, 13, 1436, 337, 5056, 1168, 51808], "temperature": 0.0, "avg_logprob": -0.12224355558069741, "compression_ratio": 1.9855595667870036, "no_speech_prob": 0.00011408701539039612}, {"id": 681, "seek": 214336, "start": 2143.36, "end": 2147.44, "text": " answering, the model is given an image and a question, and it's supposed to answer the question", "tokens": [50364, 13430, 11, 264, 2316, 307, 2212, 364, 3256, 293, 257, 1168, 11, 293, 309, 311, 3442, 281, 1867, 264, 1168, 50568], "temperature": 0.0, "avg_logprob": -0.11753661746070498, "compression_ratio": 1.83984375, "no_speech_prob": 0.00045814746408723295}, {"id": 682, "seek": 214336, "start": 2147.44, "end": 2152.4, "text": " based on the image. And researchers have shown that when you actually lack out the entire image", "tokens": [50568, 2361, 322, 264, 3256, 13, 400, 10309, 362, 4898, 300, 562, 291, 767, 5011, 484, 264, 2302, 3256, 50816], "temperature": 0.0, "avg_logprob": -0.11753661746070498, "compression_ratio": 1.83984375, "no_speech_prob": 0.00045814746408723295}, {"id": 683, "seek": 214336, "start": 2152.4, "end": 2159.44, "text": " and just give the model the question, it answers the question about 30% of design correctly.", "tokens": [50816, 293, 445, 976, 264, 2316, 264, 1168, 11, 309, 6338, 264, 1168, 466, 2217, 4, 295, 1715, 8944, 13, 51168], "temperature": 0.0, "avg_logprob": -0.11753661746070498, "compression_ratio": 1.83984375, "no_speech_prob": 0.00045814746408723295}, {"id": 684, "seek": 214336, "start": 2160.08, "end": 2164.7200000000003, "text": " So the question here is assuming that the model answers the question without seeing the image.", "tokens": [51200, 407, 264, 1168, 510, 307, 11926, 300, 264, 2316, 6338, 264, 1168, 1553, 2577, 264, 3256, 13, 51432], "temperature": 0.0, "avg_logprob": -0.11753661746070498, "compression_ratio": 1.83984375, "no_speech_prob": 0.00045814746408723295}, {"id": 685, "seek": 214336, "start": 2166.4, "end": 2171.52, "text": " How do we measure the accuracy of such models? So you can use explainability to ensure that", "tokens": [51516, 1012, 360, 321, 3481, 264, 14170, 295, 1270, 5245, 30, 407, 291, 393, 764, 2903, 2310, 281, 5586, 300, 51772], "temperature": 0.0, "avg_logprob": -0.11753661746070498, "compression_ratio": 1.83984375, "no_speech_prob": 0.00045814746408723295}, {"id": 686, "seek": 217152, "start": 2171.52, "end": 2176.08, "text": " the model actually used the image and the correct parts of the image to make the prediction.", "tokens": [50364, 264, 2316, 767, 1143, 264, 3256, 293, 264, 3006, 3166, 295, 264, 3256, 281, 652, 264, 17630, 13, 50592], "temperature": 0.0, "avg_logprob": -0.1327382336143686, "compression_ratio": 1.89453125, "no_speech_prob": 0.000340488477377221}, {"id": 687, "seek": 217152, "start": 2176.08, "end": 2181.52, "text": " For example here, the question is, did he catch a ball? We see that the player actually caught the", "tokens": [50592, 1171, 1365, 510, 11, 264, 1168, 307, 11, 630, 415, 3745, 257, 2594, 30, 492, 536, 300, 264, 4256, 767, 5415, 264, 50864], "temperature": 0.0, "avg_logprob": -0.1327382336143686, "compression_ratio": 1.89453125, "no_speech_prob": 0.000340488477377221}, {"id": 688, "seek": 217152, "start": 2181.52, "end": 2187.28, "text": " ball. And the answer is yes, but we also see that the model focused on the right parts of the image.", "tokens": [50864, 2594, 13, 400, 264, 1867, 307, 2086, 11, 457, 321, 611, 536, 300, 264, 2316, 5178, 322, 264, 558, 3166, 295, 264, 3256, 13, 51152], "temperature": 0.0, "avg_logprob": -0.1327382336143686, "compression_ratio": 1.89453125, "no_speech_prob": 0.000340488477377221}, {"id": 689, "seek": 217152, "start": 2187.28, "end": 2191.04, "text": " So it can really tell that the model made the prediction based on the image and not just the", "tokens": [51152, 407, 309, 393, 534, 980, 300, 264, 2316, 1027, 264, 17630, 2361, 322, 264, 3256, 293, 406, 445, 264, 51340], "temperature": 0.0, "avg_logprob": -0.1327382336143686, "compression_ratio": 1.89453125, "no_speech_prob": 0.000340488477377221}, {"id": 690, "seek": 217152, "start": 2191.04, "end": 2201.28, "text": " question. I'm going to skip this part too. Yay. So we're switching gears. We're going to talk about", "tokens": [51340, 1168, 13, 286, 478, 516, 281, 10023, 341, 644, 886, 13, 13268, 13, 407, 321, 434, 16493, 20915, 13, 492, 434, 516, 281, 751, 466, 51852], "temperature": 0.0, "avg_logprob": -0.1327382336143686, "compression_ratio": 1.89453125, "no_speech_prob": 0.000340488477377221}, {"id": 691, "seek": 220152, "start": 2201.6, "end": 2205.92, "text": " our method to improve model robustness using explainability. So if you have any questions", "tokens": [50368, 527, 3170, 281, 3470, 2316, 13956, 1287, 1228, 2903, 2310, 13, 407, 498, 291, 362, 604, 1651, 50584], "temperature": 0.0, "avg_logprob": -0.1402562999725342, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0021113730035722256}, {"id": 692, "seek": 220152, "start": 2205.92, "end": 2211.12, "text": " about the previous part on explaining transformers, this is the time to ask them.", "tokens": [50584, 466, 264, 3894, 644, 322, 13468, 4088, 433, 11, 341, 307, 264, 565, 281, 1029, 552, 13, 50844], "temperature": 0.0, "avg_logprob": -0.1402562999725342, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0021113730035722256}, {"id": 693, "seek": 220152, "start": 2212.96, "end": 2216.96, "text": " No, no questions. Oh, I had a couple of questions in the chat. Yeah.", "tokens": [50936, 883, 11, 572, 1651, 13, 876, 11, 286, 632, 257, 1916, 295, 1651, 294, 264, 5081, 13, 865, 13, 51136], "temperature": 0.0, "avg_logprob": -0.1402562999725342, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0021113730035722256}, {"id": 694, "seek": 220152, "start": 2218.8, "end": 2223.6, "text": " I'm sorry about that. There is no one really, you know, maintaining the chat.", "tokens": [51228, 286, 478, 2597, 466, 300, 13, 821, 307, 572, 472, 534, 11, 291, 458, 11, 14916, 264, 5081, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1402562999725342, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0021113730035722256}, {"id": 695, "seek": 220152, "start": 2224.4, "end": 2228.64, "text": " Yeah, let's make it brief and then try to answer questions. Yeah.", "tokens": [51508, 865, 11, 718, 311, 652, 309, 5353, 293, 550, 853, 281, 1867, 1651, 13, 865, 13, 51720], "temperature": 0.0, "avg_logprob": -0.1402562999725342, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0021113730035722256}, {"id": 696, "seek": 222864, "start": 2229.52, "end": 2233.2799999999997, "text": " Oh, okay. I was just wondering, why does it make sense to only look at the", "tokens": [50408, 876, 11, 1392, 13, 286, 390, 445, 6359, 11, 983, 775, 309, 652, 2020, 281, 787, 574, 412, 264, 50596], "temperature": 0.0, "avg_logprob": -0.15193005041642624, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.000869228970259428}, {"id": 697, "seek": 222864, "start": 2235.7599999999998, "end": 2241.12, "text": " attention maps outputted by the softmax? Because don't we have, don't we multiply by an output", "tokens": [50720, 3202, 11317, 5598, 14727, 538, 264, 2787, 41167, 30, 1436, 500, 380, 321, 362, 11, 500, 380, 321, 12972, 538, 364, 5598, 50988], "temperature": 0.0, "avg_logprob": -0.15193005041642624, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.000869228970259428}, {"id": 698, "seek": 222864, "start": 2241.12, "end": 2249.12, "text": " matrix then that is able to shuffle across tokens afterwards? Do you mean the values matrix? No,", "tokens": [50988, 8141, 550, 300, 307, 1075, 281, 39426, 2108, 22667, 10543, 30, 1144, 291, 914, 264, 4190, 8141, 30, 883, 11, 51388], "temperature": 0.0, "avg_logprob": -0.15193005041642624, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.000869228970259428}, {"id": 699, "seek": 222864, "start": 2249.12, "end": 2257.52, "text": " the output matrix. I guess that the intuition is just that the self attention mechanism,", "tokens": [51388, 264, 5598, 8141, 13, 286, 2041, 300, 264, 24002, 307, 445, 300, 264, 2698, 3202, 7513, 11, 51808], "temperature": 0.0, "avg_logprob": -0.15193005041642624, "compression_ratio": 1.6063348416289593, "no_speech_prob": 0.000869228970259428}, {"id": 700, "seek": 225752, "start": 2257.52, "end": 2263.6, "text": " its purpose is to contextualize in the way that the contextualization is made by the attention", "tokens": [50364, 1080, 4334, 307, 281, 35526, 1125, 294, 264, 636, 300, 264, 35526, 2144, 307, 1027, 538, 264, 3202, 50668], "temperature": 0.0, "avg_logprob": -0.11566973293528837, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009539257152937353}, {"id": 701, "seek": 225752, "start": 2263.6, "end": 2269.84, "text": " values. So the attention value is actually, you know, determine how much each token is going", "tokens": [50668, 4190, 13, 407, 264, 3202, 2158, 307, 767, 11, 291, 458, 11, 6997, 577, 709, 1184, 14862, 307, 516, 50980], "temperature": 0.0, "avg_logprob": -0.11566973293528837, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009539257152937353}, {"id": 702, "seek": 225752, "start": 2269.84, "end": 2277.04, "text": " to be incorporated into the other tokens. We do have an additional output matrix and you mean", "tokens": [50980, 281, 312, 21654, 666, 264, 661, 22667, 13, 492, 360, 362, 364, 4497, 5598, 8141, 293, 291, 914, 51340], "temperature": 0.0, "avg_logprob": -0.11566973293528837, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009539257152937353}, {"id": 703, "seek": 225752, "start": 2277.04, "end": 2282.48, "text": " after the attention mechanism, right? Yes, yes. Yeah, okay. So some researchers have actually", "tokens": [51340, 934, 264, 3202, 7513, 11, 558, 30, 1079, 11, 2086, 13, 865, 11, 1392, 13, 407, 512, 10309, 362, 767, 51612], "temperature": 0.0, "avg_logprob": -0.11566973293528837, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0009539257152937353}, {"id": 704, "seek": 228248, "start": 2282.48, "end": 2288.08, "text": " used that output, if I'm not mistaken, it was that output, the norm of the output matrix in order", "tokens": [50364, 1143, 300, 5598, 11, 498, 286, 478, 406, 21333, 11, 309, 390, 300, 5598, 11, 264, 2026, 295, 264, 5598, 8141, 294, 1668, 50644], "temperature": 0.0, "avg_logprob": -0.10279669544913551, "compression_ratio": 1.8028846153846154, "no_speech_prob": 0.0017267958028241992}, {"id": 705, "seek": 228248, "start": 2288.08, "end": 2296.0, "text": " to average across the different heads to account for each head's meaning in the attention matrix,", "tokens": [50644, 281, 4274, 2108, 264, 819, 8050, 281, 2696, 337, 1184, 1378, 311, 3620, 294, 264, 3202, 8141, 11, 51040], "temperature": 0.0, "avg_logprob": -0.10279669544913551, "compression_ratio": 1.8028846153846154, "no_speech_prob": 0.0017267958028241992}, {"id": 706, "seek": 228248, "start": 2296.56, "end": 2303.44, "text": " in the attention mechanism. But, you know, just, you know, very naively thinking the attention", "tokens": [51068, 294, 264, 3202, 7513, 13, 583, 11, 291, 458, 11, 445, 11, 291, 458, 11, 588, 1667, 3413, 1953, 264, 3202, 51412], "temperature": 0.0, "avg_logprob": -0.10279669544913551, "compression_ratio": 1.8028846153846154, "no_speech_prob": 0.0017267958028241992}, {"id": 707, "seek": 228248, "start": 2304.64, "end": 2309.36, "text": " really mixes the tokens using the values determined by the attention matrix. So it's", "tokens": [51472, 534, 37121, 264, 22667, 1228, 264, 4190, 9540, 538, 264, 3202, 8141, 13, 407, 309, 311, 51708], "temperature": 0.0, "avg_logprob": -0.10279669544913551, "compression_ratio": 1.8028846153846154, "no_speech_prob": 0.0017267958028241992}, {"id": 708, "seek": 230936, "start": 2309.36, "end": 2315.76, "text": " really a naive intuitive outlook on the attention mechanism. And the output matrix that you're", "tokens": [50364, 534, 257, 29052, 21769, 26650, 322, 264, 3202, 7513, 13, 400, 264, 5598, 8141, 300, 291, 434, 50684], "temperature": 0.0, "avg_logprob": -0.10971872174009985, "compression_ratio": 1.8724279835390947, "no_speech_prob": 6.107359513407573e-05}, {"id": 709, "seek": 230936, "start": 2315.76, "end": 2323.36, "text": " referring to is I view it as a weights matrix, which will weight each layer since not all layers", "tokens": [50684, 13761, 281, 307, 286, 1910, 309, 382, 257, 17443, 8141, 11, 597, 486, 3364, 1184, 4583, 1670, 406, 439, 7914, 51064], "temperature": 0.0, "avg_logprob": -0.10971872174009985, "compression_ratio": 1.8724279835390947, "no_speech_prob": 6.107359513407573e-05}, {"id": 710, "seek": 230936, "start": 2323.36, "end": 2327.6800000000003, "text": " influence the prediction the same, right? We know that usually the last attention layer", "tokens": [51064, 6503, 264, 17630, 264, 912, 11, 558, 30, 492, 458, 300, 2673, 264, 1036, 3202, 4583, 51280], "temperature": 0.0, "avg_logprob": -0.10971872174009985, "compression_ratio": 1.8724279835390947, "no_speech_prob": 6.107359513407573e-05}, {"id": 711, "seek": 230936, "start": 2327.6800000000003, "end": 2331.04, "text": " is the most influential or the previous attention layers are not that impactful.", "tokens": [51280, 307, 264, 881, 22215, 420, 264, 3894, 3202, 7914, 366, 406, 300, 30842, 13, 51448], "temperature": 0.0, "avg_logprob": -0.10971872174009985, "compression_ratio": 1.8724279835390947, "no_speech_prob": 6.107359513407573e-05}, {"id": 712, "seek": 230936, "start": 2331.92, "end": 2338.7200000000003, "text": " So I view it as the output matrix kind of reweighting the result from the attention mechanism.", "tokens": [51492, 407, 286, 1910, 309, 382, 264, 5598, 8141, 733, 295, 319, 12329, 278, 264, 1874, 490, 264, 3202, 7513, 13, 51832], "temperature": 0.0, "avg_logprob": -0.10971872174009985, "compression_ratio": 1.8724279835390947, "no_speech_prob": 6.107359513407573e-05}, {"id": 713, "seek": 233936, "start": 2339.36, "end": 2344.6400000000003, "text": " But all that we're saying right now are just intuitions, right? We've seen empirically that the", "tokens": [50364, 583, 439, 300, 321, 434, 1566, 558, 586, 366, 445, 16224, 626, 11, 558, 30, 492, 600, 1612, 25790, 984, 300, 264, 50628], "temperature": 0.0, "avg_logprob": -0.10580760090291, "compression_ratio": 1.7992700729927007, "no_speech_prob": 6.201293581398204e-05}, {"id": 714, "seek": 233936, "start": 2344.6400000000003, "end": 2349.84, "text": " attention matrix is quite indicative of what the model learns to do, how it learns to contextualize", "tokens": [50628, 3202, 8141, 307, 1596, 47513, 295, 437, 264, 2316, 27152, 281, 360, 11, 577, 309, 27152, 281, 35526, 1125, 50888], "temperature": 0.0, "avg_logprob": -0.10580760090291, "compression_ratio": 1.7992700729927007, "no_speech_prob": 6.201293581398204e-05}, {"id": 715, "seek": 233936, "start": 2350.7200000000003, "end": 2355.1200000000003, "text": " parts of the input. It's not necessarily the best thing to do, the smartest thing to do or the", "tokens": [50932, 3166, 295, 264, 4846, 13, 467, 311, 406, 4725, 264, 1151, 551, 281, 360, 11, 264, 41491, 551, 281, 360, 420, 264, 51152], "temperature": 0.0, "avg_logprob": -0.10580760090291, "compression_ratio": 1.7992700729927007, "no_speech_prob": 6.201293581398204e-05}, {"id": 716, "seek": 233936, "start": 2355.1200000000003, "end": 2360.88, "text": " most correct thing to do. It's just what empirically worked well. And it has an intuition basis as", "tokens": [51152, 881, 3006, 551, 281, 360, 13, 467, 311, 445, 437, 25790, 984, 2732, 731, 13, 400, 309, 575, 364, 24002, 5143, 382, 51440], "temperature": 0.0, "avg_logprob": -0.10580760090291, "compression_ratio": 1.7992700729927007, "no_speech_prob": 6.201293581398204e-05}, {"id": 717, "seek": 233936, "start": 2360.88, "end": 2367.1200000000003, "text": " explained before. I hope that answers your question. It does. I had one other question if there's time.", "tokens": [51440, 8825, 949, 13, 286, 1454, 300, 6338, 428, 1168, 13, 467, 775, 13, 286, 632, 472, 661, 1168, 498, 456, 311, 565, 13, 51752], "temperature": 0.0, "avg_logprob": -0.10580760090291, "compression_ratio": 1.7992700729927007, "no_speech_prob": 6.201293581398204e-05}, {"id": 718, "seek": 236936, "start": 2369.92, "end": 2375.36, "text": " We're really tight on time. I mean, we have 13 minutes. So maybe we'll take that offline.", "tokens": [50392, 492, 434, 534, 4524, 322, 565, 13, 286, 914, 11, 321, 362, 3705, 2077, 13, 407, 1310, 321, 603, 747, 300, 21857, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15865674065154733, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0006978745223022997}, {"id": 719, "seek": 236936, "start": 2376.1600000000003, "end": 2379.44, "text": " Sure. Thank you. Sorry for that. I really apologize.", "tokens": [50704, 4894, 13, 1044, 291, 13, 4919, 337, 300, 13, 286, 534, 12328, 13, 50868], "temperature": 0.0, "avg_logprob": -0.15865674065154733, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0006978745223022997}, {"id": 720, "seek": 236936, "start": 2382.2400000000002, "end": 2389.36, "text": " Okay. So when we talk about VIT models, the image is split into patches. The patches go", "tokens": [51008, 1033, 13, 407, 562, 321, 751, 466, 691, 3927, 5245, 11, 264, 3256, 307, 7472, 666, 26531, 13, 440, 26531, 352, 51364], "temperature": 0.0, "avg_logprob": -0.15865674065154733, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0006978745223022997}, {"id": 721, "seek": 236936, "start": 2389.36, "end": 2394.2400000000002, "text": " through linear projections. And then a transformer encoder and vanilla transformer encoder is used", "tokens": [51364, 807, 8213, 32371, 13, 400, 550, 257, 31782, 2058, 19866, 293, 17528, 31782, 2058, 19866, 307, 1143, 51608], "temperature": 0.0, "avg_logprob": -0.15865674065154733, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0006978745223022997}, {"id": 722, "seek": 236936, "start": 2394.2400000000002, "end": 2398.96, "text": " to make the prediction again with the classification code. So really a simple and clean architecture.", "tokens": [51608, 281, 652, 264, 17630, 797, 365, 264, 21538, 3089, 13, 407, 534, 257, 2199, 293, 2541, 9482, 13, 51844], "temperature": 0.0, "avg_logprob": -0.15865674065154733, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0006978745223022997}, {"id": 723, "seek": 239936, "start": 2400.0, "end": 2407.6, "text": " And usually those models are trained using ImageNet. ImageNet is a classification data set.", "tokens": [50396, 400, 2673, 729, 5245, 366, 8895, 1228, 29903, 31890, 13, 29903, 31890, 307, 257, 21538, 1412, 992, 13, 50776], "temperature": 0.0, "avg_logprob": -0.09512736819206027, "compression_ratio": 1.9113924050632911, "no_speech_prob": 6.013580787112005e-05}, {"id": 724, "seek": 239936, "start": 2408.32, "end": 2411.6800000000003, "text": " And what those classification data sets do is actually they train the model", "tokens": [50812, 400, 437, 729, 21538, 1412, 6352, 360, 307, 767, 436, 3847, 264, 2316, 50980], "temperature": 0.0, "avg_logprob": -0.09512736819206027, "compression_ratio": 1.9113924050632911, "no_speech_prob": 6.013580787112005e-05}, {"id": 725, "seek": 239936, "start": 2411.6800000000003, "end": 2416.56, "text": " to make a prediction. So they train the model to see an image and make the prediction that this", "tokens": [50980, 281, 652, 257, 17630, 13, 407, 436, 3847, 264, 2316, 281, 536, 364, 3256, 293, 652, 264, 17630, 300, 341, 51224], "temperature": 0.0, "avg_logprob": -0.09512736819206027, "compression_ratio": 1.9113924050632911, "no_speech_prob": 6.013580787112005e-05}, {"id": 726, "seek": 239936, "start": 2416.56, "end": 2421.6800000000003, "text": " is a car. But it doesn't do anything beyond that, right? The model should predict that it's a car,", "tokens": [51224, 307, 257, 1032, 13, 583, 309, 1177, 380, 360, 1340, 4399, 300, 11, 558, 30, 440, 2316, 820, 6069, 300, 309, 311, 257, 1032, 11, 51480], "temperature": 0.0, "avg_logprob": -0.09512736819206027, "compression_ratio": 1.9113924050632911, "no_speech_prob": 6.013580787112005e-05}, {"id": 727, "seek": 239936, "start": 2421.6800000000003, "end": 2426.88, "text": " but it doesn't have to have an understanding of what a car constructs and how a car looks.", "tokens": [51480, 457, 309, 1177, 380, 362, 281, 362, 364, 3701, 295, 437, 257, 1032, 7690, 82, 293, 577, 257, 1032, 1542, 13, 51740], "temperature": 0.0, "avg_logprob": -0.09512736819206027, "compression_ratio": 1.9113924050632911, "no_speech_prob": 6.013580787112005e-05}, {"id": 728, "seek": 242688, "start": 2427.6, "end": 2433.52, "text": " It should just see this image of a car and output car. We don't enforce anything too", "tokens": [50400, 467, 820, 445, 536, 341, 3256, 295, 257, 1032, 293, 5598, 1032, 13, 492, 500, 380, 24825, 1340, 886, 50696], "temperature": 0.0, "avg_logprob": -0.10506378099756333, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0001376290019834414}, {"id": 729, "seek": 242688, "start": 2433.52, "end": 2439.6800000000003, "text": " smart that the model should learn. So what researchers have noticed a long time ago", "tokens": [50696, 4069, 300, 264, 2316, 820, 1466, 13, 407, 437, 10309, 362, 5694, 257, 938, 565, 2057, 51004], "temperature": 0.0, "avg_logprob": -0.10506378099756333, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0001376290019834414}, {"id": 730, "seek": 242688, "start": 2439.6800000000003, "end": 2444.48, "text": " is that ImageNet contains sparse predictions. What it means is that, for example,", "tokens": [51004, 307, 300, 29903, 31890, 8306, 637, 11668, 21264, 13, 708, 309, 1355, 307, 300, 11, 337, 1365, 11, 51244], "temperature": 0.0, "avg_logprob": -0.10506378099756333, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0001376290019834414}, {"id": 731, "seek": 242688, "start": 2444.48, "end": 2451.6, "text": " cows usually appear on the background of green grass. So a reasonable inference that the model", "tokens": [51244, 19148, 2673, 4204, 322, 264, 3678, 295, 3092, 8054, 13, 407, 257, 10585, 38253, 300, 264, 2316, 51600], "temperature": 0.0, "avg_logprob": -0.10506378099756333, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0001376290019834414}, {"id": 732, "seek": 242688, "start": 2451.6, "end": 2456.2400000000002, "text": " can make. Really reasonable, right? Because this is the statistics of the data in the data set that", "tokens": [51600, 393, 652, 13, 4083, 10585, 11, 558, 30, 1436, 341, 307, 264, 12523, 295, 264, 1412, 294, 264, 1412, 992, 300, 51832], "temperature": 0.0, "avg_logprob": -0.10506378099756333, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0001376290019834414}, {"id": 733, "seek": 245624, "start": 2456.24, "end": 2462.08, "text": " it gets. It's to learn that green grass is actually a cow. And now we learn to predict", "tokens": [50364, 309, 2170, 13, 467, 311, 281, 1466, 300, 3092, 8054, 307, 767, 257, 8408, 13, 400, 586, 321, 1466, 281, 6069, 50656], "temperature": 0.0, "avg_logprob": -0.13288733765885635, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.0002735777816269547}, {"id": 734, "seek": 245624, "start": 2462.08, "end": 2467.7599999999998, "text": " that this image is an image of a cow based on the green grass, not really the object in the image.", "tokens": [50656, 300, 341, 3256, 307, 364, 3256, 295, 257, 8408, 2361, 322, 264, 3092, 8054, 11, 406, 534, 264, 2657, 294, 264, 3256, 13, 50940], "temperature": 0.0, "avg_logprob": -0.13288733765885635, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.0002735777816269547}, {"id": 735, "seek": 245624, "start": 2470.0, "end": 2483.3599999999997, "text": " What it causes is, oh, can you mute this? Thank you. So what it causes is cases where", "tokens": [51052, 708, 309, 7700, 307, 11, 1954, 11, 393, 291, 24523, 341, 30, 1044, 291, 13, 407, 437, 309, 7700, 307, 3331, 689, 51720], "temperature": 0.0, "avg_logprob": -0.13288733765885635, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.0002735777816269547}, {"id": 736, "seek": 248336, "start": 2483.36, "end": 2488.48, "text": " the distribution is likely shifted from ImageNet. And in cases where we would actually expect the", "tokens": [50364, 264, 7316, 307, 3700, 18892, 490, 29903, 31890, 13, 400, 294, 3331, 689, 321, 576, 767, 2066, 264, 50620], "temperature": 0.0, "avg_logprob": -0.1540689263292538, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002113916678354144}, {"id": 737, "seek": 248336, "start": 2488.48, "end": 2494.08, "text": " model to really work well on. The model really doesn't. And the accuracy plunge, we're talking", "tokens": [50620, 2316, 281, 534, 589, 731, 322, 13, 440, 2316, 534, 1177, 380, 13, 400, 264, 14170, 499, 27588, 11, 321, 434, 1417, 50900], "temperature": 0.0, "avg_logprob": -0.1540689263292538, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002113916678354144}, {"id": 738, "seek": 248336, "start": 2494.08, "end": 2501.04, "text": " about 90% to 30% sometimes and even less. So really cases where we would expect the model to still", "tokens": [50900, 466, 4289, 4, 281, 2217, 4, 2171, 293, 754, 1570, 13, 407, 534, 3331, 689, 321, 576, 2066, 264, 2316, 281, 920, 51248], "temperature": 0.0, "avg_logprob": -0.1540689263292538, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002113916678354144}, {"id": 739, "seek": 248336, "start": 2501.04, "end": 2508.56, "text": " learn to make smart and great prediction, but it really does. It predicts based on the sparse", "tokens": [51248, 1466, 281, 652, 4069, 293, 869, 17630, 11, 457, 309, 534, 775, 13, 467, 6069, 82, 2361, 322, 264, 637, 11668, 51624], "temperature": 0.0, "avg_logprob": -0.1540689263292538, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002113916678354144}, {"id": 740, "seek": 250856, "start": 2508.56, "end": 2512.72, "text": " predictions that it learned from ImageNet and they don't apply to other predictions. So for example,", "tokens": [50364, 21264, 300, 309, 3264, 490, 29903, 31890, 293, 436, 500, 380, 3079, 281, 661, 21264, 13, 407, 337, 1365, 11, 50572], "temperature": 0.0, "avg_logprob": -0.19560328100481603, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.002433324232697487}, {"id": 741, "seek": 250856, "start": 2512.72, "end": 2517.2, "text": " we have the golf ball in the lemon here and we have another orange that is classified as a maze", "tokens": [50572, 321, 362, 264, 12880, 2594, 294, 264, 11356, 510, 293, 321, 362, 1071, 7671, 300, 307, 20627, 382, 257, 33032, 50796], "temperature": 0.0, "avg_logprob": -0.19560328100481603, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.002433324232697487}, {"id": 742, "seek": 250856, "start": 2517.2, "end": 2522.56, "text": " due to the carpet in the bathroom, right? Because it kind of looks like a maze. And a school bus", "tokens": [50796, 3462, 281, 264, 18119, 294, 264, 8687, 11, 558, 30, 1436, 309, 733, 295, 1542, 411, 257, 33032, 13, 400, 257, 1395, 1255, 51064], "temperature": 0.0, "avg_logprob": -0.19560328100481603, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.002433324232697487}, {"id": 743, "seek": 250856, "start": 2522.56, "end": 2527.68, "text": " here that is classified as a snowplow because of the presence of snow. So we can imagine that the", "tokens": [51064, 510, 300, 307, 20627, 382, 257, 5756, 564, 305, 570, 295, 264, 6814, 295, 5756, 13, 407, 321, 393, 3811, 300, 264, 51320], "temperature": 0.0, "avg_logprob": -0.19560328100481603, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.002433324232697487}, {"id": 744, "seek": 250856, "start": 2527.68, "end": 2532.72, "text": " model learns some kind of sparse correlation here, such as vehicle plus snow equals snowplow.", "tokens": [51320, 2316, 27152, 512, 733, 295, 637, 11668, 20009, 510, 11, 1270, 382, 5864, 1804, 5756, 6915, 5756, 564, 305, 13, 51572], "temperature": 0.0, "avg_logprob": -0.19560328100481603, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.002433324232697487}, {"id": 745, "seek": 253272, "start": 2533.6, "end": 2543.12, "text": " Okay. So we want to solve these issues, but without training the models with, you know,", "tokens": [50408, 1033, 13, 407, 321, 528, 281, 5039, 613, 2663, 11, 457, 1553, 3097, 264, 5245, 365, 11, 291, 458, 11, 50884], "temperature": 0.0, "avg_logprob": -0.1836832277067415, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.0003352712083142251}, {"id": 746, "seek": 253272, "start": 2543.12, "end": 2548.3199999999997, "text": " a stronger queue, it is really hard to do that because we just teach the model based on some", "tokens": [50884, 257, 7249, 18639, 11, 309, 307, 534, 1152, 281, 360, 300, 570, 321, 445, 2924, 264, 2316, 2361, 322, 512, 51144], "temperature": 0.0, "avg_logprob": -0.1836832277067415, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.0003352712083142251}, {"id": 747, "seek": 253272, "start": 2548.3199999999997, "end": 2553.6, "text": " data set that we have, which is ImageNet. It is, you know, the most used data set to predict,", "tokens": [51144, 1412, 992, 300, 321, 362, 11, 597, 307, 29903, 31890, 13, 467, 307, 11, 291, 458, 11, 264, 881, 1143, 1412, 992, 281, 6069, 11, 51408], "temperature": 0.0, "avg_logprob": -0.1836832277067415, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.0003352712083142251}, {"id": 748, "seek": 253272, "start": 2554.16, "end": 2562.48, "text": " to train object detection, object recognition. And we have no way of really controlling", "tokens": [51436, 281, 3847, 2657, 17784, 11, 2657, 11150, 13, 400, 321, 362, 572, 636, 295, 534, 14905, 51852], "temperature": 0.0, "avg_logprob": -0.1836832277067415, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.0003352712083142251}, {"id": 749, "seek": 256248, "start": 2562.48, "end": 2568.4, "text": " what the model learns. And intuitively, training the explainability signal is really teaching", "tokens": [50364, 437, 264, 2316, 27152, 13, 400, 46506, 11, 3097, 264, 2903, 2310, 6358, 307, 534, 4571, 50660], "temperature": 0.0, "avg_logprob": -0.0738350585266784, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0001376336585963145}, {"id": 750, "seek": 256248, "start": 2568.4, "end": 2573.28, "text": " the model not just what is in the image, but why this is the object in the image. So we would want", "tokens": [50660, 264, 2316, 406, 445, 437, 307, 294, 264, 3256, 11, 457, 983, 341, 307, 264, 2657, 294, 264, 3256, 13, 407, 321, 576, 528, 50904], "temperature": 0.0, "avg_logprob": -0.0738350585266784, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0001376336585963145}, {"id": 751, "seek": 256248, "start": 2573.28, "end": 2579.68, "text": " to apply a last term directly to the explanations of the model to teach it why this prediction is", "tokens": [50904, 281, 3079, 257, 1036, 1433, 3838, 281, 264, 28708, 295, 264, 2316, 281, 2924, 309, 983, 341, 17630, 307, 51224], "temperature": 0.0, "avg_logprob": -0.0738350585266784, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0001376336585963145}, {"id": 752, "seek": 256248, "start": 2579.68, "end": 2587.52, "text": " correct. So here you see some sparse correlations that the model uses. So for example, here the", "tokens": [51224, 3006, 13, 407, 510, 291, 536, 512, 637, 11668, 13983, 763, 300, 264, 2316, 4960, 13, 407, 337, 1365, 11, 510, 264, 51616], "temperature": 0.0, "avg_logprob": -0.0738350585266784, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0001376336585963145}, {"id": 753, "seek": 258752, "start": 2587.52, "end": 2593.7599999999998, "text": " model classifies the images of chestnut with a confidence of 100% based on just the background", "tokens": [50364, 2316, 1508, 11221, 264, 5267, 295, 7443, 18316, 365, 257, 6687, 295, 2319, 4, 2361, 322, 445, 264, 3678, 50676], "temperature": 0.0, "avg_logprob": -0.16867864260109522, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0003738704544957727}, {"id": 754, "seek": 258752, "start": 2593.7599999999998, "end": 2599.7599999999998, "text": " pixels, not even one photo. And here a very sparse consideration of the zebra gives us a", "tokens": [50676, 18668, 11, 406, 754, 472, 5052, 13, 400, 510, 257, 588, 637, 11668, 12381, 295, 264, 47060, 2709, 505, 257, 50976], "temperature": 0.0, "avg_logprob": -0.16867864260109522, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0003738704544957727}, {"id": 755, "seek": 258752, "start": 2599.7599999999998, "end": 2605.04, "text": " confidence of 99.9% that this is a zebra. So really behavior that we would really want to discourage.", "tokens": [50976, 6687, 295, 11803, 13, 24, 4, 300, 341, 307, 257, 47060, 13, 407, 534, 5223, 300, 321, 576, 534, 528, 281, 21497, 609, 13, 51240], "temperature": 0.0, "avg_logprob": -0.16867864260109522, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0003738704544957727}, {"id": 756, "seek": 258752, "start": 2608.08, "end": 2613.7599999999998, "text": " Since the second method that we saw is based on pure gradients, everything there is derivable.", "tokens": [51392, 4162, 264, 1150, 3170, 300, 321, 1866, 307, 2361, 322, 6075, 2771, 2448, 11, 1203, 456, 307, 10151, 712, 13, 51676], "temperature": 0.0, "avg_logprob": -0.16867864260109522, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0003738704544957727}, {"id": 757, "seek": 261376, "start": 2613.76, "end": 2617.28, "text": " The gradients can be derived again, and the last term can be applied directly to the", "tokens": [50364, 440, 2771, 2448, 393, 312, 18949, 797, 11, 293, 264, 1036, 1433, 393, 312, 6456, 3838, 281, 264, 50540], "temperature": 0.0, "avg_logprob": -0.11014366149902344, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0002233322593383491}, {"id": 758, "seek": 261376, "start": 2617.28, "end": 2623.2000000000003, "text": " explainability. And we can force the model to make the prediction based on the program instead of", "tokens": [50540, 2903, 2310, 13, 400, 321, 393, 3464, 264, 2316, 281, 652, 264, 17630, 2361, 322, 264, 1461, 2602, 295, 50836], "temperature": 0.0, "avg_logprob": -0.11014366149902344, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0002233322593383491}, {"id": 759, "seek": 261376, "start": 2623.2000000000003, "end": 2630.96, "text": " the background image pixels. The issue that we had after that is, you know, we're researchers at", "tokens": [50836, 264, 3678, 3256, 18668, 13, 440, 2734, 300, 321, 632, 934, 300, 307, 11, 291, 458, 11, 321, 434, 10309, 412, 51224], "temperature": 0.0, "avg_logprob": -0.11014366149902344, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0002233322593383491}, {"id": 760, "seek": 261376, "start": 2630.96, "end": 2638.6400000000003, "text": " the university, right? We don't have the resources to train VIT large or huge from scratch. So we", "tokens": [51224, 264, 5454, 11, 558, 30, 492, 500, 380, 362, 264, 3593, 281, 3847, 691, 3927, 2416, 420, 2603, 490, 8459, 13, 407, 321, 51608], "temperature": 0.0, "avg_logprob": -0.11014366149902344, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0002233322593383491}, {"id": 761, "seek": 263864, "start": 2638.64, "end": 2644.96, "text": " need to come up with a method that is efficient in time and space and not too complicated. So what", "tokens": [50364, 643, 281, 808, 493, 365, 257, 3170, 300, 307, 7148, 294, 565, 293, 1901, 293, 406, 886, 6179, 13, 407, 437, 50680], "temperature": 0.0, "avg_logprob": -0.09046202190851761, "compression_ratio": 1.90234375, "no_speech_prob": 0.0001767118665156886}, {"id": 762, "seek": 263864, "start": 2644.96, "end": 2650.8799999999997, "text": " we opted to do is fine tune an existing model. So we would fine tune the model. It works pretty", "tokens": [50680, 321, 40768, 281, 360, 307, 2489, 10864, 364, 6741, 2316, 13, 407, 321, 576, 2489, 10864, 264, 2316, 13, 467, 1985, 1238, 50976], "temperature": 0.0, "avg_logprob": -0.09046202190851761, "compression_ratio": 1.90234375, "no_speech_prob": 0.0001767118665156886}, {"id": 763, "seek": 263864, "start": 2650.8799999999997, "end": 2655.7599999999998, "text": " well on ImageNet, right? We don't want to change the prediction that it gives on ImageNet. We just", "tokens": [50976, 731, 322, 29903, 31890, 11, 558, 30, 492, 500, 380, 528, 281, 1319, 264, 17630, 300, 309, 2709, 322, 29903, 31890, 13, 492, 445, 51220], "temperature": 0.0, "avg_logprob": -0.09046202190851761, "compression_ratio": 1.90234375, "no_speech_prob": 0.0001767118665156886}, {"id": 764, "seek": 263864, "start": 2655.7599999999998, "end": 2661.92, "text": " want to change the reasoning that it gives to the prediction. So we fine tune the models with only", "tokens": [51220, 528, 281, 1319, 264, 21577, 300, 309, 2709, 281, 264, 17630, 13, 407, 321, 2489, 10864, 264, 5245, 365, 787, 51528], "temperature": 0.0, "avg_logprob": -0.09046202190851761, "compression_ratio": 1.90234375, "no_speech_prob": 0.0001767118665156886}, {"id": 765, "seek": 263864, "start": 2661.92, "end": 2667.92, "text": " three examples per class, really not that many examples for just 500 classes. So just half the", "tokens": [51528, 1045, 5110, 680, 1508, 11, 534, 406, 300, 867, 5110, 337, 445, 5923, 5359, 13, 407, 445, 1922, 264, 51828], "temperature": 0.0, "avg_logprob": -0.09046202190851761, "compression_ratio": 1.90234375, "no_speech_prob": 0.0001767118665156886}, {"id": 766, "seek": 266792, "start": 2667.92, "end": 2673.2000000000003, "text": " classes in ImageNet to change the relevance maps to focus on the foreground instead of the background.", "tokens": [50364, 5359, 294, 29903, 31890, 281, 1319, 264, 32684, 11317, 281, 1879, 322, 264, 32058, 2602, 295, 264, 3678, 13, 50628], "temperature": 0.0, "avg_logprob": -0.17083255811171097, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.0001464939268771559}, {"id": 767, "seek": 266792, "start": 2675.6, "end": 2680.96, "text": " So we identified two science issues with VIT models. The first one is an over interpretation", "tokens": [50748, 407, 321, 9234, 732, 3497, 2663, 365, 691, 3927, 5245, 13, 440, 700, 472, 307, 364, 670, 14174, 51016], "temperature": 0.0, "avg_logprob": -0.17083255811171097, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.0001464939268771559}, {"id": 768, "seek": 266792, "start": 2680.96, "end": 2685.2000000000003, "text": " of the background, which we saw on your clients. And the second one is a sparse consideration of", "tokens": [51016, 295, 264, 3678, 11, 597, 321, 1866, 322, 428, 6982, 13, 400, 264, 1150, 472, 307, 257, 637, 11668, 12381, 295, 51228], "temperature": 0.0, "avg_logprob": -0.17083255811171097, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.0001464939268771559}, {"id": 769, "seek": 266792, "start": 2685.2000000000003, "end": 2693.04, "text": " the program. The first idea was to fine tune the explanation maps to just be segmentation maps,", "tokens": [51228, 264, 1461, 13, 440, 700, 1558, 390, 281, 2489, 10864, 264, 10835, 11317, 281, 445, 312, 9469, 399, 11317, 11, 51620], "temperature": 0.0, "avg_logprob": -0.17083255811171097, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.0001464939268771559}, {"id": 770, "seek": 269304, "start": 2693.12, "end": 2700.08, "text": " like this. This is actually an example of me fine tuning a VIT based model to make the relevance", "tokens": [50368, 411, 341, 13, 639, 307, 767, 364, 1365, 295, 385, 2489, 15164, 257, 691, 3927, 2361, 2316, 281, 652, 264, 32684, 50716], "temperature": 0.0, "avg_logprob": -0.0864015367296007, "compression_ratio": 1.6752136752136753, "no_speech_prob": 0.0011874090414494276}, {"id": 771, "seek": 269304, "start": 2700.08, "end": 2706.72, "text": " maps resemble or be identical to segmentation maps. So as you can see before the explanations weren't", "tokens": [50716, 11317, 36870, 420, 312, 14800, 281, 9469, 399, 11317, 13, 407, 382, 291, 393, 536, 949, 264, 28708, 4999, 380, 51048], "temperature": 0.0, "avg_logprob": -0.0864015367296007, "compression_ratio": 1.6752136752136753, "no_speech_prob": 0.0011874090414494276}, {"id": 772, "seek": 269304, "start": 2706.72, "end": 2712.96, "text": " really segmentation maps and after they're quite well segmented in the image. So can anyone guess", "tokens": [51048, 534, 9469, 399, 11317, 293, 934, 436, 434, 1596, 731, 9469, 292, 294, 264, 3256, 13, 407, 393, 2878, 2041, 51360], "temperature": 0.0, "avg_logprob": -0.0864015367296007, "compression_ratio": 1.6752136752136753, "no_speech_prob": 0.0011874090414494276}, {"id": 773, "seek": 269304, "start": 2712.96, "end": 2718.48, "text": " why that's not an optimal solution to the problem that we have just creating segmentation maps?", "tokens": [51360, 983, 300, 311, 406, 364, 16252, 3827, 281, 264, 1154, 300, 321, 362, 445, 4084, 9469, 399, 11317, 30, 51636], "temperature": 0.0, "avg_logprob": -0.0864015367296007, "compression_ratio": 1.6752136752136753, "no_speech_prob": 0.0011874090414494276}, {"id": 774, "seek": 271848, "start": 2719.36, "end": 2726.64, "text": " People from Zoom can guess too. Why wouldn't we want the model to output relevance maps that are", "tokens": [50408, 3432, 490, 13453, 393, 2041, 886, 13, 1545, 2759, 380, 321, 528, 264, 2316, 281, 5598, 32684, 11317, 300, 366, 50772], "temperature": 0.0, "avg_logprob": -0.10877154090187767, "compression_ratio": 1.43, "no_speech_prob": 0.0003250140871386975}, {"id": 775, "seek": 271848, "start": 2726.64, "end": 2734.56, "text": " actually segmentation maps? Let's have a thought experiment today. I'm going to draw with my", "tokens": [50772, 767, 9469, 399, 11317, 30, 961, 311, 362, 257, 1194, 5120, 965, 13, 286, 478, 516, 281, 2642, 365, 452, 51168], "temperature": 0.0, "avg_logprob": -0.10877154090187767, "compression_ratio": 1.43, "no_speech_prob": 0.0003250140871386975}, {"id": 776, "seek": 271848, "start": 2734.56, "end": 2741.12, "text": " magnificent drawing skills and objects and you're going to try to identify which animal this is,", "tokens": [51168, 23690, 6316, 3942, 293, 6565, 293, 291, 434, 516, 281, 853, 281, 5876, 597, 5496, 341, 307, 11, 51496], "temperature": 0.0, "avg_logprob": -0.10877154090187767, "compression_ratio": 1.43, "no_speech_prob": 0.0003250140871386975}, {"id": 777, "seek": 274112, "start": 2741.12, "end": 2749.2799999999997, "text": " right? Again, I'm not the best drawing. Which animal is this? Which snake?", "tokens": [50364, 558, 30, 3764, 11, 286, 478, 406, 264, 1151, 6316, 13, 3013, 5496, 307, 341, 30, 3013, 12650, 30, 50772], "temperature": 0.0, "avg_logprob": -0.2504673719406128, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.016113631427288055}, {"id": 778, "seek": 274112, "start": 2751.2, "end": 2760.48, "text": " Nail. Oh, no, this snake. Which snake is this? Cobra. Yeah, why cobra? Because of the head, right?", "tokens": [50868, 426, 864, 13, 876, 11, 572, 11, 341, 12650, 13, 3013, 12650, 307, 341, 30, 383, 24393, 13, 865, 11, 983, 48790, 30, 1436, 295, 264, 1378, 11, 558, 30, 51332], "temperature": 0.0, "avg_logprob": -0.2504673719406128, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.016113631427288055}, {"id": 779, "seek": 274112, "start": 2761.2, "end": 2766.88, "text": " And humans, we don't classify cobra as a cobra because of its tail, right? We look at the head", "tokens": [51368, 400, 6255, 11, 321, 500, 380, 33872, 48790, 382, 257, 48790, 570, 295, 1080, 6838, 11, 558, 30, 492, 574, 412, 264, 1378, 51652], "temperature": 0.0, "avg_logprob": -0.2504673719406128, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.016113631427288055}, {"id": 780, "seek": 276688, "start": 2766.88, "end": 2774.4, "text": " pixels or the head featured and determine that this is a cobra. So we don't, as humans, give", "tokens": [50364, 18668, 420, 264, 1378, 13822, 293, 6997, 300, 341, 307, 257, 48790, 13, 407, 321, 500, 380, 11, 382, 6255, 11, 976, 50740], "temperature": 0.0, "avg_logprob": -0.10222500361753313, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0009246983099728823}, {"id": 781, "seek": 276688, "start": 2774.4, "end": 2779.84, "text": " identical relevance to all the pixels in the image. What we do here when we fine tune the", "tokens": [50740, 14800, 32684, 281, 439, 264, 18668, 294, 264, 3256, 13, 708, 321, 360, 510, 562, 321, 2489, 10864, 264, 51012], "temperature": 0.0, "avg_logprob": -0.10222500361753313, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0009246983099728823}, {"id": 782, "seek": 276688, "start": 2779.84, "end": 2784.56, "text": " experiment maps to be segmentation maps, we force the model to look equally at all the pixels of", "tokens": [51012, 5120, 11317, 281, 312, 9469, 399, 11317, 11, 321, 3464, 264, 2316, 281, 574, 12309, 412, 439, 264, 18668, 295, 51248], "temperature": 0.0, "avg_logprob": -0.10222500361753313, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0009246983099728823}, {"id": 783, "seek": 276688, "start": 2784.56, "end": 2790.48, "text": " the cobra. We do want to give the model the opportunity to give some relevance to pixels", "tokens": [51248, 264, 48790, 13, 492, 360, 528, 281, 976, 264, 2316, 264, 2650, 281, 976, 512, 32684, 281, 18668, 51544], "temperature": 0.0, "avg_logprob": -0.10222500361753313, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.0009246983099728823}, {"id": 784, "seek": 279048, "start": 2790.48, "end": 2799.36, "text": " that is higher than other pixels. So this is too harsh. And we need to have a refined version of it.", "tokens": [50364, 300, 307, 2946, 813, 661, 18668, 13, 407, 341, 307, 886, 14897, 13, 400, 321, 643, 281, 362, 257, 26201, 3037, 295, 309, 13, 50808], "temperature": 0.0, "avg_logprob": -0.09391603686592796, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0015724565600976348}, {"id": 785, "seek": 279048, "start": 2801.2, "end": 2807.2, "text": " This is why we split the loss into two different losses. One is a background loss and one is", "tokens": [50900, 639, 307, 983, 321, 7472, 264, 4470, 666, 732, 819, 15352, 13, 1485, 307, 257, 3678, 4470, 293, 472, 307, 51200], "temperature": 0.0, "avg_logprob": -0.09391603686592796, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0015724565600976348}, {"id": 786, "seek": 279048, "start": 2807.2, "end": 2813.2, "text": " foreground loss. The background loss is a mean squared error loss, encouraging the relevance", "tokens": [51200, 32058, 4470, 13, 440, 3678, 4470, 307, 257, 914, 8889, 6713, 4470, 11, 14580, 264, 32684, 51500], "temperature": 0.0, "avg_logprob": -0.09391603686592796, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0015724565600976348}, {"id": 787, "seek": 279048, "start": 2813.2, "end": 2817.36, "text": " on the background to be close to zero. And we're using segmentation maps here,", "tokens": [51500, 322, 264, 3678, 281, 312, 1998, 281, 4018, 13, 400, 321, 434, 1228, 9469, 399, 11317, 510, 11, 51708], "temperature": 0.0, "avg_logprob": -0.09391603686592796, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0015724565600976348}, {"id": 788, "seek": 281736, "start": 2817.36, "end": 2823.1200000000003, "text": " S is the segmentation map of the image. And the foreground loss encourages the foreground", "tokens": [50364, 318, 307, 264, 9469, 399, 4471, 295, 264, 3256, 13, 400, 264, 32058, 4470, 28071, 264, 32058, 50652], "temperature": 0.0, "avg_logprob": -0.11489974430629185, "compression_ratio": 1.929460580912863, "no_speech_prob": 0.0003004214959219098}, {"id": 789, "seek": 281736, "start": 2823.1200000000003, "end": 2830.2400000000002, "text": " of the image to be closer to one. By splitting into two loss terms, we can give different", "tokens": [50652, 295, 264, 3256, 281, 312, 4966, 281, 472, 13, 3146, 30348, 666, 732, 4470, 2115, 11, 321, 393, 976, 819, 51008], "temperature": 0.0, "avg_logprob": -0.11489974430629185, "compression_ratio": 1.929460580912863, "no_speech_prob": 0.0003004214959219098}, {"id": 790, "seek": 281736, "start": 2830.2400000000002, "end": 2835.52, "text": " values or different coefficients to each of the loss terms. So the background loss is going to get", "tokens": [51008, 4190, 420, 819, 31994, 281, 1184, 295, 264, 4470, 2115, 13, 407, 264, 3678, 4470, 307, 516, 281, 483, 51272], "temperature": 0.0, "avg_logprob": -0.11489974430629185, "compression_ratio": 1.929460580912863, "no_speech_prob": 0.0003004214959219098}, {"id": 791, "seek": 281736, "start": 2835.52, "end": 2840.32, "text": " a relatively high coefficient too, because we don't want a lot of relevance on the background.", "tokens": [51272, 257, 7226, 1090, 17619, 886, 11, 570, 321, 500, 380, 528, 257, 688, 295, 32684, 322, 264, 3678, 13, 51512], "temperature": 0.0, "avg_logprob": -0.11489974430629185, "compression_ratio": 1.929460580912863, "no_speech_prob": 0.0003004214959219098}, {"id": 792, "seek": 281736, "start": 2840.32, "end": 2845.36, "text": " By the way, we're not striving to completely eliminate the background, the relevance on the", "tokens": [51512, 3146, 264, 636, 11, 321, 434, 406, 36582, 281, 2584, 13819, 264, 3678, 11, 264, 32684, 322, 264, 51764], "temperature": 0.0, "avg_logprob": -0.11489974430629185, "compression_ratio": 1.929460580912863, "no_speech_prob": 0.0003004214959219098}, {"id": 793, "seek": 284536, "start": 2845.36, "end": 2849.44, "text": " foreground. Just make sure that the relevance of the background is lower than the relevance of the", "tokens": [50364, 32058, 13, 1449, 652, 988, 300, 264, 32684, 295, 264, 3678, 307, 3126, 813, 264, 32684, 295, 264, 50568], "temperature": 0.0, "avg_logprob": -0.08804672591540279, "compression_ratio": 1.9177489177489178, "no_speech_prob": 0.00018517460557632148}, {"id": 794, "seek": 284536, "start": 2849.44, "end": 2855.2000000000003, "text": " foreground. And the foreground loss is going to get a relatively low coefficient. We would want to", "tokens": [50568, 32058, 13, 400, 264, 32058, 4470, 307, 516, 281, 483, 257, 7226, 2295, 17619, 13, 492, 576, 528, 281, 50856], "temperature": 0.0, "avg_logprob": -0.08804672591540279, "compression_ratio": 1.9177489177489178, "no_speech_prob": 0.00018517460557632148}, {"id": 795, "seek": 284536, "start": 2855.2000000000003, "end": 2860.7200000000003, "text": " encourage the model to look more at more pixels of the foreground, but we wouldn't want to make", "tokens": [50856, 5373, 264, 2316, 281, 574, 544, 412, 544, 18668, 295, 264, 32058, 11, 457, 321, 2759, 380, 528, 281, 652, 51132], "temperature": 0.0, "avg_logprob": -0.08804672591540279, "compression_ratio": 1.9177489177489178, "no_speech_prob": 0.00018517460557632148}, {"id": 796, "seek": 284536, "start": 2860.7200000000003, "end": 2863.52, "text": " the model look at all the pixels in the foreground equally.", "tokens": [51132, 264, 2316, 574, 412, 439, 264, 18668, 294, 264, 32058, 12309, 13, 51272], "temperature": 0.0, "avg_logprob": -0.08804672591540279, "compression_ratio": 1.9177489177489178, "no_speech_prob": 0.00018517460557632148}, {"id": 797, "seek": 284536, "start": 2866.2400000000002, "end": 2872.08, "text": " We do also have a classification loss, which ensures that the new prediction by the model", "tokens": [51408, 492, 360, 611, 362, 257, 21538, 4470, 11, 597, 28111, 300, 264, 777, 17630, 538, 264, 2316, 51700], "temperature": 0.0, "avg_logprob": -0.08804672591540279, "compression_ratio": 1.9177489177489178, "no_speech_prob": 0.00018517460557632148}, {"id": 798, "seek": 287208, "start": 2872.16, "end": 2876.96, "text": " or the new distribution is similar to the all distribution by the model. Just to make sure", "tokens": [50368, 420, 264, 777, 7316, 307, 2531, 281, 264, 439, 7316, 538, 264, 2316, 13, 1449, 281, 652, 988, 50608], "temperature": 0.0, "avg_logprob": -0.08065085519443858, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.00023771364067215472}, {"id": 799, "seek": 287208, "start": 2876.96, "end": 2882.24, "text": " that the model doesn't forget how to classify images. And again, the model does a pretty good", "tokens": [50608, 300, 264, 2316, 1177, 380, 2870, 577, 281, 33872, 5267, 13, 400, 797, 11, 264, 2316, 775, 257, 1238, 665, 50872], "temperature": 0.0, "avg_logprob": -0.08065085519443858, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.00023771364067215472}, {"id": 800, "seek": 287208, "start": 2882.24, "end": 2886.08, "text": " job on ImageNet. So we don't want to change the prediction by the model. We just want to change", "tokens": [50872, 1691, 322, 29903, 31890, 13, 407, 321, 500, 380, 528, 281, 1319, 264, 17630, 538, 264, 2316, 13, 492, 445, 528, 281, 1319, 51064], "temperature": 0.0, "avg_logprob": -0.08065085519443858, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.00023771364067215472}, {"id": 801, "seek": 287208, "start": 2886.08, "end": 2896.72, "text": " the reasoning. So the giant tables of results here are comparisons between the accuracy of the", "tokens": [51064, 264, 21577, 13, 407, 264, 7410, 8020, 295, 3542, 510, 366, 33157, 1296, 264, 14170, 295, 264, 51596], "temperature": 0.0, "avg_logprob": -0.08065085519443858, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.00023771364067215472}, {"id": 802, "seek": 289672, "start": 2896.72, "end": 2902.8799999999997, "text": " model before and after a cartooning process. And as you can see here, it's quite tiny, but I hope", "tokens": [50364, 2316, 949, 293, 934, 257, 18569, 278, 1399, 13, 400, 382, 291, 393, 536, 510, 11, 309, 311, 1596, 5870, 11, 457, 286, 1454, 50672], "temperature": 0.0, "avg_logprob": -0.12938986069116837, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.0014096541563048959}, {"id": 803, "seek": 289672, "start": 2902.8799999999997, "end": 2908.64, "text": " you can see it still. For the ImageNet validation set, we're experiencing a bit of a decrease in", "tokens": [50672, 291, 393, 536, 309, 920, 13, 1171, 264, 29903, 31890, 24071, 992, 11, 321, 434, 11139, 257, 857, 295, 257, 11514, 294, 50960], "temperature": 0.0, "avg_logprob": -0.12938986069116837, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.0014096541563048959}, {"id": 804, "seek": 289672, "start": 2908.64, "end": 2912.72, "text": " performance. This is because the model relied on spurious cues, and now we're taking them away", "tokens": [50960, 3389, 13, 639, 307, 570, 264, 2316, 35463, 322, 637, 24274, 32192, 11, 293, 586, 321, 434, 1940, 552, 1314, 51164], "temperature": 0.0, "avg_logprob": -0.12938986069116837, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.0014096541563048959}, {"id": 805, "seek": 289672, "start": 2912.72, "end": 2918.08, "text": " from it. And so the spurious cues that previously helped the model reach very, very high accuracy", "tokens": [51164, 490, 309, 13, 400, 370, 264, 637, 24274, 32192, 300, 8046, 4254, 264, 2316, 2524, 588, 11, 588, 1090, 14170, 51432], "temperature": 0.0, "avg_logprob": -0.12938986069116837, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.0014096541563048959}, {"id": 806, "seek": 289672, "start": 2918.08, "end": 2924.7999999999997, "text": " and overfit are now taking away. But the decrease in accuracy on average across seven models is", "tokens": [51432, 293, 670, 6845, 366, 586, 1940, 1314, 13, 583, 264, 11514, 294, 14170, 322, 4274, 2108, 3407, 5245, 307, 51768], "temperature": 0.0, "avg_logprob": -0.12938986069116837, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.0014096541563048959}, {"id": 807, "seek": 292480, "start": 2924.8, "end": 2930.7200000000003, "text": " not that big. I mean, it's less than 1%. And when you take into account other shifted distributions,", "tokens": [50364, 406, 300, 955, 13, 286, 914, 11, 309, 311, 1570, 813, 502, 6856, 400, 562, 291, 747, 666, 2696, 661, 18892, 37870, 11, 50660], "temperature": 0.0, "avg_logprob": -0.11726443281451475, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.0006766853621229529}, {"id": 808, "seek": 292480, "start": 2930.7200000000003, "end": 2940.0, "text": " such as ImageNet A, ImageNet R, Sketch, ImageNet ObjectNet, and SIScores, you can see that there", "tokens": [50660, 1270, 382, 29903, 31890, 316, 11, 29903, 31890, 497, 11, 49245, 11, 29903, 31890, 24753, 31890, 11, 293, 318, 2343, 66, 2706, 11, 291, 393, 536, 300, 456, 51124], "temperature": 0.0, "avg_logprob": -0.11726443281451475, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.0006766853621229529}, {"id": 809, "seek": 292480, "start": 2940.0, "end": 2945.84, "text": " is a pretty big or significant increase in accuracy. For ImageNet A, for example, plus", "tokens": [51124, 307, 257, 1238, 955, 420, 4776, 3488, 294, 14170, 13, 1171, 29903, 31890, 316, 11, 337, 1365, 11, 1804, 51416], "temperature": 0.0, "avg_logprob": -0.11726443281451475, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.0006766853621229529}, {"id": 810, "seek": 292480, "start": 2946.6400000000003, "end": 2954.32, "text": " 5.8% in top one accuracy plus 7.8% in top five accuracy. So really a slight decrease", "tokens": [51456, 1025, 13, 23, 4, 294, 1192, 472, 14170, 1804, 1614, 13, 23, 4, 294, 1192, 1732, 14170, 13, 407, 534, 257, 4036, 11514, 51840], "temperature": 0.0, "avg_logprob": -0.11726443281451475, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.0006766853621229529}, {"id": 811, "seek": 295432, "start": 2954.32, "end": 2958.32, "text": " in the accuracy on the data set that the model was originally trained on and", "tokens": [50364, 294, 264, 14170, 322, 264, 1412, 992, 300, 264, 2316, 390, 7993, 8895, 322, 293, 50564], "temperature": 0.0, "avg_logprob": -0.14971468925476075, "compression_ratio": 1.8488888888888888, "no_speech_prob": 0.00016598911315668374}, {"id": 812, "seek": 295432, "start": 2958.32, "end": 2963.04, "text": " a significant increase in accuracy for distribution shifts, as we would expect.", "tokens": [50564, 257, 4776, 3488, 294, 14170, 337, 7316, 19201, 11, 382, 321, 576, 2066, 13, 50800], "temperature": 0.0, "avg_logprob": -0.14971468925476075, "compression_ratio": 1.8488888888888888, "no_speech_prob": 0.00016598911315668374}, {"id": 813, "seek": 295432, "start": 2963.6800000000003, "end": 2966.6400000000003, "text": " So to train it, you have to know the program. What is the program?", "tokens": [50832, 407, 281, 3847, 309, 11, 291, 362, 281, 458, 264, 1461, 13, 708, 307, 264, 1461, 30, 50980], "temperature": 0.0, "avg_logprob": -0.14971468925476075, "compression_ratio": 1.8488888888888888, "no_speech_prob": 0.00016598911315668374}, {"id": 814, "seek": 295432, "start": 2967.44, "end": 2973.52, "text": " Yeah, you have to know that. You have segmentation maps. You have segmentation maps. And we do", "tokens": [51020, 865, 11, 291, 362, 281, 458, 300, 13, 509, 362, 9469, 399, 11317, 13, 509, 362, 9469, 399, 11317, 13, 400, 321, 360, 51324], "temperature": 0.0, "avg_logprob": -0.14971468925476075, "compression_ratio": 1.8488888888888888, "no_speech_prob": 0.00016598911315668374}, {"id": 815, "seek": 295432, "start": 2973.52, "end": 2979.6800000000003, "text": " experiment with two types of segmentation maps. One is manually human, manually tagged by humans.", "tokens": [51324, 5120, 365, 732, 3467, 295, 9469, 399, 11317, 13, 1485, 307, 16945, 1952, 11, 16945, 40239, 538, 6255, 13, 51632], "temperature": 0.0, "avg_logprob": -0.14971468925476075, "compression_ratio": 1.8488888888888888, "no_speech_prob": 0.00016598911315668374}, {"id": 816, "seek": 297968, "start": 2979.68, "end": 2985.68, "text": " And the second one is by token cut, which is a version that uses dyno. This is in case you're", "tokens": [50364, 400, 264, 1150, 472, 307, 538, 14862, 1723, 11, 597, 307, 257, 3037, 300, 4960, 14584, 1771, 13, 639, 307, 294, 1389, 291, 434, 50664], "temperature": 0.0, "avg_logprob": -0.16806999842325845, "compression_ratio": 1.5321888412017168, "no_speech_prob": 0.0014094564830884337}, {"id": 817, "seek": 297968, "start": 2985.68, "end": 2993.04, "text": " training with non-ImageNet data sets and you don't want to manually tag. Even if you do manually", "tokens": [50664, 3097, 365, 2107, 12, 31128, 609, 31890, 1412, 6352, 293, 291, 500, 380, 528, 281, 16945, 6162, 13, 2754, 498, 291, 360, 16945, 51032], "temperature": 0.0, "avg_logprob": -0.16806999842325845, "compression_ratio": 1.5321888412017168, "no_speech_prob": 0.0014094564830884337}, {"id": 818, "seek": 297968, "start": 2993.04, "end": 2999.2799999999997, "text": " tag, I mean, we use three examples for half the classes. So it's not that many examples to tell,", "tokens": [51032, 6162, 11, 286, 914, 11, 321, 764, 1045, 5110, 337, 1922, 264, 5359, 13, 407, 309, 311, 406, 300, 867, 5110, 281, 980, 11, 51344], "temperature": 0.0, "avg_logprob": -0.16806999842325845, "compression_ratio": 1.5321888412017168, "no_speech_prob": 0.0014094564830884337}, {"id": 819, "seek": 297968, "start": 2999.2799999999997, "end": 3004.16, "text": " but we do provide for an option for ad supervised segmentation. Yeah.", "tokens": [51344, 457, 321, 360, 2893, 337, 364, 3614, 337, 614, 46533, 9469, 399, 13, 865, 13, 51588], "temperature": 0.0, "avg_logprob": -0.16806999842325845, "compression_ratio": 1.5321888412017168, "no_speech_prob": 0.0014094564830884337}, {"id": 820, "seek": 300416, "start": 3004.3199999999997, "end": 3010.3199999999997, "text": " So I think that's really cool. The one thing about this, why not just do segmentation?", "tokens": [50372, 407, 286, 519, 300, 311, 534, 1627, 13, 440, 472, 551, 466, 341, 11, 983, 406, 445, 360, 9469, 399, 30, 50672], "temperature": 0.0, "avg_logprob": -0.32367729559177305, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.003122860100120306}, {"id": 821, "seek": 300416, "start": 3011.2, "end": 3014.64, "text": " You can just train a segmentation system. Is that kind of naturally explainable?", "tokens": [50716, 509, 393, 445, 3847, 257, 9469, 399, 1185, 13, 1119, 300, 733, 295, 8195, 2903, 712, 30, 50888], "temperature": 0.0, "avg_logprob": -0.32367729559177305, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.003122860100120306}, {"id": 822, "seek": 300416, "start": 3016.64, "end": 3021.04, "text": " That's an excellent question. Do models that were trained on segmentation", "tokens": [50988, 663, 311, 364, 7103, 1168, 13, 1144, 5245, 300, 645, 8895, 322, 9469, 399, 51208], "temperature": 0.0, "avg_logprob": -0.32367729559177305, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.003122860100120306}, {"id": 823, "seek": 300416, "start": 3022.16, "end": 3027.2, "text": " have that, you know, brief pass on Spurs correlation? Do they get that in her?", "tokens": [51264, 362, 300, 11, 291, 458, 11, 5353, 1320, 322, 1738, 2156, 20009, 30, 1144, 436, 483, 300, 294, 720, 30, 51516], "temperature": 0.0, "avg_logprob": -0.32367729559177305, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.003122860100120306}, {"id": 824, "seek": 302720, "start": 3028.16, "end": 3034.0, "text": " What we thought about or I thought about in that context is you can think about a model", "tokens": [50412, 708, 321, 1194, 466, 420, 286, 1194, 466, 294, 300, 4319, 307, 291, 393, 519, 466, 257, 2316, 50704], "temperature": 0.0, "avg_logprob": -0.13369159471421016, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.0023592812940478325}, {"id": 825, "seek": 302720, "start": 3034.0, "end": 3039.9199999999996, "text": " that learns to classify using Spurs correlation and then identify the object using edge detection.", "tokens": [50704, 300, 27152, 281, 33872, 1228, 1738, 2156, 20009, 293, 550, 5876, 264, 2657, 1228, 4691, 17784, 13, 51000], "temperature": 0.0, "avg_logprob": -0.13369159471421016, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.0023592812940478325}, {"id": 826, "seek": 302720, "start": 3040.8799999999997, "end": 3046.96, "text": " So just because you learn to identify an object does not mean or learn to segment an object.", "tokens": [51048, 407, 445, 570, 291, 1466, 281, 5876, 364, 2657, 775, 406, 914, 420, 1466, 281, 9469, 364, 2657, 13, 51352], "temperature": 0.0, "avg_logprob": -0.13369159471421016, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.0023592812940478325}, {"id": 827, "seek": 302720, "start": 3046.96, "end": 3052.3199999999997, "text": " Does not mean that you learn to recognize the object by the segmentation. And also we can think", "tokens": [51352, 4402, 406, 914, 300, 291, 1466, 281, 5521, 264, 2657, 538, 264, 9469, 399, 13, 400, 611, 321, 393, 519, 51620], "temperature": 0.0, "avg_logprob": -0.13369159471421016, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.0023592812940478325}, {"id": 828, "seek": 305232, "start": 3052.32, "end": 3057.2000000000003, "text": " about when you want to train really big models, you need a lot of data to do that. And segmentation", "tokens": [50364, 466, 562, 291, 528, 281, 3847, 534, 955, 5245, 11, 291, 643, 257, 688, 295, 1412, 281, 360, 300, 13, 400, 9469, 399, 50608], "temperature": 0.0, "avg_logprob": -0.1478181032790351, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0015244805254042149}, {"id": 829, "seek": 305232, "start": 3057.2000000000003, "end": 3062.6400000000003, "text": " data is quite expensive. You usually don't have that amount of data as you do for classification,", "tokens": [50608, 1412, 307, 1596, 5124, 13, 509, 2673, 500, 380, 362, 300, 2372, 295, 1412, 382, 291, 360, 337, 21538, 11, 50880], "temperature": 0.0, "avg_logprob": -0.1478181032790351, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0015244805254042149}, {"id": 830, "seek": 305232, "start": 3062.6400000000003, "end": 3068.56, "text": " which is an easier task. You have a lot of data just lying around there. So classification is", "tokens": [50880, 597, 307, 364, 3571, 5633, 13, 509, 362, 257, 688, 295, 1412, 445, 8493, 926, 456, 13, 407, 21538, 307, 51176], "temperature": 0.0, "avg_logprob": -0.1478181032790351, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0015244805254042149}, {"id": 831, "seek": 305232, "start": 3068.56, "end": 3080.1600000000003, "text": " usually the go to task. Yeah, but only just a few. Okay. Yeah. Just 1500 segmentation maps,", "tokens": [51176, 2673, 264, 352, 281, 5633, 13, 865, 11, 457, 787, 445, 257, 1326, 13, 1033, 13, 865, 13, 1449, 22671, 9469, 399, 11317, 11, 51756], "temperature": 0.0, "avg_logprob": -0.1478181032790351, "compression_ratio": 1.6798245614035088, "no_speech_prob": 0.0015244805254042149}, {"id": 832, "seek": 308016, "start": 3080.16, "end": 3085.2, "text": " either supervised or unsupervised. Yeah. A very few amount of segmentation maps.", "tokens": [50364, 2139, 46533, 420, 2693, 12879, 24420, 13, 865, 13, 316, 588, 1326, 2372, 295, 9469, 399, 11317, 13, 50616], "temperature": 0.0, "avg_logprob": -0.24603645975996807, "compression_ratio": 1.421487603305785, "no_speech_prob": 0.0008160925353877246}, {"id": 833, "seek": 308016, "start": 3100.0, "end": 3106.96, "text": " We did experiment with using more segmentation maps and it showed that the accuracy kind of", "tokens": [51356, 492, 630, 5120, 365, 1228, 544, 9469, 399, 11317, 293, 309, 4712, 300, 264, 14170, 733, 295, 51704], "temperature": 0.0, "avg_logprob": -0.24603645975996807, "compression_ratio": 1.421487603305785, "no_speech_prob": 0.0008160925353877246}, {"id": 834, "seek": 310696, "start": 3106.96, "end": 3111.6, "text": " fluctuates at some point. I mean, there's some point where it doesn't improve more if you add", "tokens": [50364, 23448, 27710, 412, 512, 935, 13, 286, 914, 11, 456, 311, 512, 935, 689, 309, 1177, 380, 3470, 544, 498, 291, 909, 50596], "temperature": 0.0, "avg_logprob": -0.18454742431640625, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.002630736446008086}, {"id": 835, "seek": 310696, "start": 3111.6, "end": 3117.2, "text": " more segmentation maps, but you do have to take into account two things. One, we did find two,", "tokens": [50596, 544, 9469, 399, 11317, 11, 457, 291, 360, 362, 281, 747, 666, 2696, 732, 721, 13, 1485, 11, 321, 630, 915, 732, 11, 50876], "temperature": 0.0, "avg_logprob": -0.18454742431640625, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.002630736446008086}, {"id": 836, "seek": 310696, "start": 3117.2, "end": 3122.0, "text": " and we didn't trade for scratch. Two, we didn't have the resources to hyper parameter search for", "tokens": [50876, 293, 321, 994, 380, 4923, 337, 8459, 13, 4453, 11, 321, 994, 380, 362, 264, 3593, 281, 9848, 13075, 3164, 337, 51116], "temperature": 0.0, "avg_logprob": -0.18454742431640625, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.002630736446008086}, {"id": 837, "seek": 310696, "start": 3122.0, "end": 3129.2, "text": " each selection of the number of. So it's possible that if you use more data, you would need to", "tokens": [51116, 1184, 9450, 295, 264, 1230, 295, 13, 407, 309, 311, 1944, 300, 498, 291, 764, 544, 1412, 11, 291, 576, 643, 281, 51476], "temperature": 0.0, "avg_logprob": -0.18454742431640625, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.002630736446008086}, {"id": 838, "seek": 310696, "start": 3129.2, "end": 3135.36, "text": " retune your hyper parameters and then get better accuracy improvement, but we didn't have the", "tokens": [51476, 1533, 2613, 428, 9848, 9834, 293, 550, 483, 1101, 14170, 10444, 11, 457, 321, 994, 380, 362, 264, 51784], "temperature": 0.0, "avg_logprob": -0.18454742431640625, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.002630736446008086}, {"id": 839, "seek": 313536, "start": 3135.36, "end": 3140.6400000000003, "text": " resources to do that. So it could be the case, but I don't really know. Yeah, I don't really have", "tokens": [50364, 3593, 281, 360, 300, 13, 407, 309, 727, 312, 264, 1389, 11, 457, 286, 500, 380, 534, 458, 13, 865, 11, 286, 500, 380, 534, 362, 50628], "temperature": 0.0, "avg_logprob": -0.129638671875, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0003249106521252543}, {"id": 840, "seek": 313536, "start": 3140.6400000000003, "end": 3149.44, "text": " any finance for that. One thing we did to ensure that the model actually learns to predict better", "tokens": [50628, 604, 10719, 337, 300, 13, 1485, 551, 321, 630, 281, 5586, 300, 264, 2316, 767, 27152, 281, 6069, 1101, 51068], "temperature": 0.0, "avg_logprob": -0.129638671875, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0003249106521252543}, {"id": 841, "seek": 313536, "start": 3149.44, "end": 3155.52, "text": " or to have better explanations is we looked at the accuracy increase for the non-training classes", "tokens": [51068, 420, 281, 362, 1101, 28708, 307, 321, 2956, 412, 264, 14170, 3488, 337, 264, 2107, 12, 17227, 1760, 5359, 51372], "temperature": 0.0, "avg_logprob": -0.129638671875, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0003249106521252543}, {"id": 842, "seek": 313536, "start": 3155.52, "end": 3161.04, "text": " as well, because we said that we only use half the initial classes. It is really interesting to see", "tokens": [51372, 382, 731, 11, 570, 321, 848, 300, 321, 787, 764, 1922, 264, 5883, 5359, 13, 467, 307, 534, 1880, 281, 536, 51648], "temperature": 0.0, "avg_logprob": -0.129638671875, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0003249106521252543}, {"id": 843, "seek": 316104, "start": 3161.04, "end": 3168.08, "text": " if the model really improves on the non-training data as well. Does it learn to generalize the", "tokens": [50364, 498, 264, 2316, 534, 24771, 322, 264, 2107, 12, 17227, 1760, 1412, 382, 731, 13, 4402, 309, 1466, 281, 2674, 1125, 264, 50716], "temperature": 0.0, "avg_logprob": -0.08243033715656825, "compression_ratio": 1.8196078431372549, "no_speech_prob": 0.0002867856528609991}, {"id": 844, "seek": 316104, "start": 3168.08, "end": 3173.84, "text": " positive influence or the positive logic? And as you can see here, this is the ImageNet", "tokens": [50716, 3353, 6503, 420, 264, 3353, 9952, 30, 400, 382, 291, 393, 536, 510, 11, 341, 307, 264, 29903, 31890, 51004], "temperature": 0.0, "avg_logprob": -0.08243033715656825, "compression_ratio": 1.8196078431372549, "no_speech_prob": 0.0002867856528609991}, {"id": 845, "seek": 316104, "start": 3175.12, "end": 3180.24, "text": " validation set. So yeah, there's a decrease as we saw before, but for the non-ImageNet", "tokens": [51068, 24071, 992, 13, 407, 1338, 11, 456, 311, 257, 11514, 382, 321, 1866, 949, 11, 457, 337, 264, 2107, 12, 31128, 609, 31890, 51324], "temperature": 0.0, "avg_logprob": -0.08243033715656825, "compression_ratio": 1.8196078431372549, "no_speech_prob": 0.0002867856528609991}, {"id": 846, "seek": 316104, "start": 3180.24, "end": 3185.12, "text": " distributions, for the shift of distribution, you can see that the improvement for the non-training", "tokens": [51324, 37870, 11, 337, 264, 5513, 295, 7316, 11, 291, 393, 536, 300, 264, 10444, 337, 264, 2107, 12, 17227, 1760, 51568], "temperature": 0.0, "avg_logprob": -0.08243033715656825, "compression_ratio": 1.8196078431372549, "no_speech_prob": 0.0002867856528609991}, {"id": 847, "seek": 316104, "start": 3185.12, "end": 3190.24, "text": " classes is actually quite similar and sometimes even better than that of the training classes.", "tokens": [51568, 5359, 307, 767, 1596, 2531, 293, 2171, 754, 1101, 813, 300, 295, 264, 3097, 5359, 13, 51824], "temperature": 0.0, "avg_logprob": -0.08243033715656825, "compression_ratio": 1.8196078431372549, "no_speech_prob": 0.0002867856528609991}, {"id": 848, "seek": 319024, "start": 3190.24, "end": 3196.8799999999997, "text": " So the model really from this experiment learns to generalize that healthy say-and-behavior to", "tokens": [50364, 407, 264, 2316, 534, 490, 341, 5120, 27152, 281, 2674, 1125, 300, 4627, 584, 12, 474, 12, 29437, 38387, 281, 50696], "temperature": 0.0, "avg_logprob": -0.14880756898359818, "compression_ratio": 1.663716814159292, "no_speech_prob": 6.707473221467808e-05}, {"id": 849, "seek": 319024, "start": 3196.8799999999997, "end": 3204.72, "text": " classes that were not in the training set. And here are some visual examples. These are", "tokens": [50696, 5359, 300, 645, 406, 294, 264, 3097, 992, 13, 400, 510, 366, 512, 5056, 5110, 13, 1981, 366, 51088], "temperature": 0.0, "avg_logprob": -0.14880756898359818, "compression_ratio": 1.663716814159292, "no_speech_prob": 6.707473221467808e-05}, {"id": 850, "seek": 319024, "start": 3204.72, "end": 3210.3199999999997, "text": " examples from the ImageNet data set. So examples from the original data set of the model straight", "tokens": [51088, 5110, 490, 264, 29903, 31890, 1412, 992, 13, 407, 5110, 490, 264, 3380, 1412, 992, 295, 264, 2316, 2997, 51368], "temperature": 0.0, "avg_logprob": -0.14880756898359818, "compression_ratio": 1.663716814159292, "no_speech_prob": 6.707473221467808e-05}, {"id": 851, "seek": 319024, "start": 3210.3199999999997, "end": 3215.7599999999998, "text": " map. And here you can see that the same prediction is made for two different reasons. Here, the", "tokens": [51368, 4471, 13, 400, 510, 291, 393, 536, 300, 264, 912, 17630, 307, 1027, 337, 732, 819, 4112, 13, 1692, 11, 264, 51640], "temperature": 0.0, "avg_logprob": -0.14880756898359818, "compression_ratio": 1.663716814159292, "no_speech_prob": 6.707473221467808e-05}, {"id": 852, "seek": 321576, "start": 3215.76, "end": 3221.2000000000003, "text": " background, here actually the foreground, the snowplow. And here you can see corrective predictions", "tokens": [50364, 3678, 11, 510, 767, 264, 32058, 11, 264, 5756, 564, 305, 13, 400, 510, 291, 393, 536, 3006, 488, 21264, 50636], "temperature": 0.0, "avg_logprob": -0.16903039387294225, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.0004171761975158006}, {"id": 853, "seek": 321576, "start": 3221.2000000000003, "end": 3227.2000000000003, "text": " where the model originally predicted that this is a can opener based on the eye of the puppet.", "tokens": [50636, 689, 264, 2316, 7993, 19147, 300, 341, 307, 257, 393, 43850, 2361, 322, 264, 3313, 295, 264, 32107, 13, 50936], "temperature": 0.0, "avg_logprob": -0.16903039387294225, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.0004171761975158006}, {"id": 854, "seek": 321576, "start": 3227.76, "end": 3233.76, "text": " And once we find you the model to look at the entire object or to look for, you know, less", "tokens": [50964, 400, 1564, 321, 915, 291, 264, 2316, 281, 574, 412, 264, 2302, 2657, 420, 281, 574, 337, 11, 291, 458, 11, 1570, 51264], "temperature": 0.0, "avg_logprob": -0.16903039387294225, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.0004171761975158006}, {"id": 855, "seek": 321576, "start": 3233.76, "end": 3238.5600000000004, "text": " sparsely as the object, it actually talks about the teddy bear. And here you can see that even", "tokens": [51264, 637, 685, 736, 382, 264, 2657, 11, 309, 767, 6686, 466, 264, 45116, 6155, 13, 400, 510, 291, 393, 536, 300, 754, 51504], "temperature": 0.0, "avg_logprob": -0.16903039387294225, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.0004171761975158006}, {"id": 856, "seek": 321576, "start": 3238.5600000000004, "end": 3245.36, "text": " if the model is now wrong and was previously correct, you can usually quite easily explain", "tokens": [51504, 498, 264, 2316, 307, 586, 2085, 293, 390, 8046, 3006, 11, 291, 393, 2673, 1596, 3612, 2903, 51844], "temperature": 0.0, "avg_logprob": -0.16903039387294225, "compression_ratio": 1.8543307086614174, "no_speech_prob": 0.0004171761975158006}, {"id": 857, "seek": 324536, "start": 3245.36, "end": 3250.8, "text": " why the model was wrong. So here's an example where the ground truth classification is tripod", "tokens": [50364, 983, 264, 2316, 390, 2085, 13, 407, 510, 311, 364, 1365, 689, 264, 2727, 3494, 21538, 307, 28020, 50636], "temperature": 0.0, "avg_logprob": -0.10746136936572713, "compression_ratio": 1.8549019607843138, "no_speech_prob": 0.0001253222580999136}, {"id": 858, "seek": 324536, "start": 3250.8, "end": 3254.88, "text": " and the model predicted actually fine tuning a strawberry, but you can actually see that there", "tokens": [50636, 293, 264, 2316, 19147, 767, 2489, 15164, 257, 20440, 11, 457, 291, 393, 767, 536, 300, 456, 50840], "temperature": 0.0, "avg_logprob": -0.10746136936572713, "compression_ratio": 1.8549019607843138, "no_speech_prob": 0.0001253222580999136}, {"id": 859, "seek": 324536, "start": 3254.88, "end": 3259.36, "text": " exists a strawberry in the image. So it kind of makes sense that the model made that mistake.", "tokens": [50840, 8198, 257, 20440, 294, 264, 3256, 13, 407, 309, 733, 295, 1669, 2020, 300, 264, 2316, 1027, 300, 6146, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10746136936572713, "compression_ratio": 1.8549019607843138, "no_speech_prob": 0.0001253222580999136}, {"id": 860, "seek": 324536, "start": 3261.76, "end": 3266.56, "text": " These are examples for shifted distributions. So as you can see before, for this example,", "tokens": [51184, 1981, 366, 5110, 337, 18892, 37870, 13, 407, 382, 291, 393, 536, 949, 11, 337, 341, 1365, 11, 51424], "temperature": 0.0, "avg_logprob": -0.10746136936572713, "compression_ratio": 1.8549019607843138, "no_speech_prob": 0.0001253222580999136}, {"id": 861, "seek": 324536, "start": 3266.56, "end": 3272.8, "text": " the model predicted a garbage truck. Well, this is the forklift because the forklift is in a garbage", "tokens": [51424, 264, 2316, 19147, 257, 14150, 5898, 13, 1042, 11, 341, 307, 264, 337, 7837, 2008, 570, 264, 337, 7837, 2008, 307, 294, 257, 14150, 51736], "temperature": 0.0, "avg_logprob": -0.10746136936572713, "compression_ratio": 1.8549019607843138, "no_speech_prob": 0.0001253222580999136}, {"id": 862, "seek": 327280, "start": 3272.8, "end": 3279.28, "text": " area. So we correct the prediction to be a forklift based on foreground rather than the background.", "tokens": [50364, 1859, 13, 407, 321, 3006, 264, 17630, 281, 312, 257, 337, 7837, 2008, 2361, 322, 32058, 2831, 813, 264, 3678, 13, 50688], "temperature": 0.0, "avg_logprob": -0.14574973437250877, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.00021981839381624013}, {"id": 863, "seek": 327280, "start": 3279.28, "end": 3283.52, "text": " Here you can see a teddy bear that was classified as a ping-pong ball due to the sparse consideration", "tokens": [50688, 1692, 291, 393, 536, 257, 45116, 6155, 300, 390, 20627, 382, 257, 26151, 12, 79, 556, 2594, 3462, 281, 264, 637, 11668, 12381, 50900], "temperature": 0.0, "avg_logprob": -0.14574973437250877, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.00021981839381624013}, {"id": 864, "seek": 327280, "start": 3283.52, "end": 3289.52, "text": " of just its spot. And after the fine tuning, it is correctly classified. And the third example is a", "tokens": [50900, 295, 445, 1080, 4008, 13, 400, 934, 264, 2489, 15164, 11, 309, 307, 8944, 20627, 13, 400, 264, 2636, 1365, 307, 257, 51200], "temperature": 0.0, "avg_logprob": -0.14574973437250877, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.00021981839381624013}, {"id": 865, "seek": 327280, "start": 3289.52, "end": 3294.7200000000003, "text": " porcupine that was classified as a sea lion due to the background of the ocean. So once the model", "tokens": [51200, 1515, 16794, 533, 300, 390, 20627, 382, 257, 4158, 17226, 3462, 281, 264, 3678, 295, 264, 7810, 13, 407, 1564, 264, 2316, 51460], "temperature": 0.0, "avg_logprob": -0.14574973437250877, "compression_ratio": 1.7577092511013215, "no_speech_prob": 0.00021981839381624013}, {"id": 866, "seek": 329472, "start": 3294.72, "end": 3301.12, "text": " really learns to look at the correct pixels, it does make the correct prediction.", "tokens": [50364, 534, 27152, 281, 574, 412, 264, 3006, 18668, 11, 309, 775, 652, 264, 3006, 17630, 13, 50684], "temperature": 0.0, "avg_logprob": -0.09789352189926874, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.002590369898825884}, {"id": 867, "seek": 329472, "start": 3302.8799999999997, "end": 3308.08, "text": " These are additional examples, but really, we don't have time. And another interesting thing", "tokens": [50772, 1981, 366, 4497, 5110, 11, 457, 534, 11, 321, 500, 380, 362, 565, 13, 400, 1071, 1880, 551, 51032], "temperature": 0.0, "avg_logprob": -0.09789352189926874, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.002590369898825884}, {"id": 868, "seek": 329472, "start": 3308.08, "end": 3312.8799999999997, "text": " that we've noticed that I think is quite cool, even when you take examples that are completely", "tokens": [51032, 300, 321, 600, 5694, 300, 286, 519, 307, 1596, 1627, 11, 754, 562, 291, 747, 5110, 300, 366, 2584, 51272], "temperature": 0.0, "avg_logprob": -0.09789352189926874, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.002590369898825884}, {"id": 869, "seek": 329472, "start": 3312.8799999999997, "end": 3319.2799999999997, "text": " out of distribution, I mean, this is an image generated by Dalit. And the models not know", "tokens": [51272, 484, 295, 7316, 11, 286, 914, 11, 341, 307, 364, 3256, 10833, 538, 17357, 270, 13, 400, 264, 5245, 406, 458, 51592], "temperature": 0.0, "avg_logprob": -0.09789352189926874, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.002590369898825884}, {"id": 870, "seek": 331928, "start": 3319.28, "end": 3325.6000000000004, "text": " the class robot or oil painting and so on. Originally, it made a ludicrous prediction", "tokens": [50364, 264, 1508, 7881, 420, 3184, 5370, 293, 370, 322, 13, 28696, 11, 309, 1027, 257, 15946, 299, 21189, 17630, 50680], "temperature": 0.0, "avg_logprob": -0.10694368815017959, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.005216725170612335}, {"id": 871, "seek": 331928, "start": 3325.6000000000004, "end": 3332.6400000000003, "text": " that this is a guillotine based on, I don't know what, you can't really understand. But after a", "tokens": [50680, 300, 341, 307, 257, 695, 373, 39658, 2361, 322, 11, 286, 500, 380, 458, 437, 11, 291, 393, 380, 534, 1223, 13, 583, 934, 257, 51032], "temperature": 0.0, "avg_logprob": -0.10694368815017959, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.005216725170612335}, {"id": 872, "seek": 331928, "start": 3332.6400000000003, "end": 3337.1200000000003, "text": " fine tuning process, you can see that the model does not make maybe the best prediction that you", "tokens": [51032, 2489, 15164, 1399, 11, 291, 393, 536, 300, 264, 2316, 775, 406, 652, 1310, 264, 1151, 17630, 300, 291, 51256], "temperature": 0.0, "avg_logprob": -0.10694368815017959, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.005216725170612335}, {"id": 873, "seek": 331928, "start": 3337.1200000000003, "end": 3341.92, "text": " can think of, which is the robot because it doesn't know that class. But it does predict the grand piano", "tokens": [51256, 393, 519, 295, 11, 597, 307, 264, 7881, 570, 309, 1177, 380, 458, 300, 1508, 13, 583, 309, 775, 6069, 264, 2697, 9211, 51496], "temperature": 0.0, "avg_logprob": -0.10694368815017959, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.005216725170612335}, {"id": 874, "seek": 331928, "start": 3341.92, "end": 3347.0400000000004, "text": " and it kind of makes sense because there is a piano in the image. So while the prediction,", "tokens": [51496, 293, 309, 733, 295, 1669, 2020, 570, 456, 307, 257, 9211, 294, 264, 3256, 13, 407, 1339, 264, 17630, 11, 51752], "temperature": 0.0, "avg_logprob": -0.10694368815017959, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.005216725170612335}, {"id": 875, "seek": 334704, "start": 3347.7599999999998, "end": 3353.68, "text": " again, still does not make the most sense. At least it is based on some of the objects inside", "tokens": [50400, 797, 11, 920, 775, 406, 652, 264, 881, 2020, 13, 1711, 1935, 309, 307, 2361, 322, 512, 295, 264, 6565, 1854, 50696], "temperature": 0.0, "avg_logprob": -0.15723373534831595, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0006663267267867923}, {"id": 876, "seek": 334704, "start": 3353.68, "end": 3358.88, "text": " the image and not just something that you cannot make sense of due to sparse correlation.", "tokens": [50696, 264, 3256, 293, 406, 445, 746, 300, 291, 2644, 652, 2020, 295, 3462, 281, 637, 11668, 20009, 13, 50956], "temperature": 0.0, "avg_logprob": -0.15723373534831595, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0006663267267867923}, {"id": 877, "seek": 334704, "start": 3360.56, "end": 3367.12, "text": " So this was the entire talk. Yeah, we got through it in time. Thank you very much. And the table", "tokens": [51040, 407, 341, 390, 264, 2302, 751, 13, 865, 11, 321, 658, 807, 309, 294, 565, 13, 1044, 291, 588, 709, 13, 400, 264, 3199, 51368], "temperature": 0.0, "avg_logprob": -0.15723373534831595, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0006663267267867923}, {"id": 878, "seek": 334704, "start": 3367.12, "end": 3372.0, "text": " of content is here in case you want to ask a question about specific parts of the lecture. Thank you.", "tokens": [51368, 295, 2701, 307, 510, 294, 1389, 291, 528, 281, 1029, 257, 1168, 466, 2685, 3166, 295, 264, 7991, 13, 1044, 291, 13, 51612], "temperature": 0.0, "avg_logprob": -0.15723373534831595, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0006663267267867923}, {"id": 879, "seek": 337704, "start": 3377.04, "end": 3384.64, "text": " Yeah, one or two we can do. Yeah.", "tokens": [50364, 865, 11, 472, 420, 732, 321, 393, 360, 13, 865, 13, 50744], "temperature": 0.0, "avg_logprob": -0.3986641196317451, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.0057668727822601795}, {"id": 880, "seek": 337704, "start": 3392.24, "end": 3394.96, "text": " Yeah, it's visible. Thanks for rotating it.", "tokens": [51124, 865, 11, 309, 311, 8974, 13, 2561, 337, 19627, 309, 13, 51260], "temperature": 0.0, "avg_logprob": -0.3986641196317451, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.0057668727822601795}, {"id": 881, "seek": 337704, "start": 3401.68, "end": 3404.8, "text": " Okay, the questions here are really lacking context because they were probably", "tokens": [51596, 1033, 11, 264, 1651, 510, 366, 534, 20889, 4319, 570, 436, 645, 1391, 51752], "temperature": 0.0, "avg_logprob": -0.3986641196317451, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.0057668727822601795}, {"id": 882, "seek": 340480, "start": 3405.2000000000003, "end": 3410.0, "text": " asked during that. So if anyone wants to ask a question again. Yeah.", "tokens": [50384, 2351, 1830, 300, 13, 407, 498, 2878, 2738, 281, 1029, 257, 1168, 797, 13, 865, 13, 50624], "temperature": 0.0, "avg_logprob": -0.23746975006595736, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.0012438853736966848}, {"id": 883, "seek": 340480, "start": 3412.7200000000003, "end": 3417.84, "text": " Yeah, I guess one question I had. Have you thought about including layer norm at all into your", "tokens": [50760, 865, 11, 286, 2041, 472, 1168, 286, 632, 13, 3560, 291, 1194, 466, 3009, 4583, 2026, 412, 439, 666, 428, 51016], "temperature": 0.0, "avg_logprob": -0.23746975006595736, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.0012438853736966848}, {"id": 884, "seek": 340480, "start": 3421.36, "end": 3427.1200000000003, "text": " explanations? Because it seems that that does scale tokens in some way and could that be relevant", "tokens": [51192, 28708, 30, 1436, 309, 2544, 300, 300, 775, 4373, 22667, 294, 512, 636, 293, 727, 300, 312, 7340, 51480], "temperature": 0.0, "avg_logprob": -0.23746975006595736, "compression_ratio": 1.4662921348314606, "no_speech_prob": 0.0012438853736966848}, {"id": 885, "seek": 342712, "start": 3427.12, "end": 3433.04, "text": " for your output? Include what? Sorry, can you repeat it? Layer norm? Layer norm? Oh,", "tokens": [50364, 337, 428, 5598, 30, 7779, 32334, 437, 30, 4919, 11, 393, 291, 7149, 309, 30, 35166, 2026, 30, 35166, 2026, 30, 876, 11, 50660], "temperature": 0.0, "avg_logprob": -0.16630558172861734, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.004977406468242407}, {"id": 886, "seek": 342712, "start": 3434.7999999999997, "end": 3441.7599999999998, "text": " no, but as I said, there is a method that I don't quite remember the name of the method that", "tokens": [50748, 572, 11, 457, 382, 286, 848, 11, 456, 307, 257, 3170, 300, 286, 500, 380, 1596, 1604, 264, 1315, 295, 264, 3170, 300, 51096], "temperature": 0.0, "avg_logprob": -0.16630558172861734, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.004977406468242407}, {"id": 887, "seek": 342712, "start": 3441.7599999999998, "end": 3448.7999999999997, "text": " did take into account the norms of the output matrix, I think, in order to average across the", "tokens": [51096, 630, 747, 666, 2696, 264, 24357, 295, 264, 5598, 8141, 11, 286, 519, 11, 294, 1668, 281, 4274, 2108, 264, 51448], "temperature": 0.0, "avg_logprob": -0.16630558172861734, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.004977406468242407}, {"id": 888, "seek": 342712, "start": 3448.7999999999997, "end": 3455.7599999999998, "text": " different attention heads. But we haven't considered that. Yeah. We do consider that the gradients", "tokens": [51448, 819, 3202, 8050, 13, 583, 321, 2378, 380, 4888, 300, 13, 865, 13, 492, 360, 1949, 300, 264, 2771, 2448, 51796], "temperature": 0.0, "avg_logprob": -0.16630558172861734, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.004977406468242407}, {"id": 889, "seek": 345576, "start": 3455.76, "end": 3461.5200000000004, "text": " should be able to scale the different attention heads according to their influence on the prediction.", "tokens": [50364, 820, 312, 1075, 281, 4373, 264, 819, 3202, 8050, 4650, 281, 641, 6503, 322, 264, 17630, 13, 50652], "temperature": 0.0, "avg_logprob": -0.18311398369925364, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.0006161497440189123}, {"id": 890, "seek": 345576, "start": 3466.88, "end": 3468.5600000000004, "text": " Any other questions? Any other questions?", "tokens": [50920, 2639, 661, 1651, 30, 2639, 661, 1651, 30, 51004], "temperature": 0.0, "avg_logprob": -0.18311398369925364, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.0006161497440189123}, {"id": 891, "seek": 345576, "start": 3471.44, "end": 3474.8, "text": " I had a question if no one's going. Oh, yeah, go ahead.", "tokens": [51148, 286, 632, 257, 1168, 498, 572, 472, 311, 516, 13, 876, 11, 1338, 11, 352, 2286, 13, 51316], "temperature": 0.0, "avg_logprob": -0.18311398369925364, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.0006161497440189123}, {"id": 892, "seek": 345576, "start": 3477.36, "end": 3481.76, "text": " Wait, me or someone in the room? Sorry. Yeah. Okay, thank you.", "tokens": [51444, 3802, 11, 385, 420, 1580, 294, 264, 1808, 30, 4919, 13, 865, 13, 1033, 11, 1309, 291, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18311398369925364, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.0006161497440189123}, {"id": 893, "seek": 348176, "start": 3482.7200000000003, "end": 3489.6000000000004, "text": " Okay. So I was wondering with regards to the stuff you said at the end, where some of them", "tokens": [50412, 1033, 13, 407, 286, 390, 6359, 365, 14258, 281, 264, 1507, 291, 848, 412, 264, 917, 11, 689, 512, 295, 552, 50756], "temperature": 0.0, "avg_logprob": -0.20607470159661279, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.0029788799583911896}, {"id": 894, "seek": 348176, "start": 3490.1600000000003, "end": 3494.96, "text": " you see it and then you're like, okay, that was wrong, but like, makes sense. That's", "tokens": [50784, 291, 536, 309, 293, 550, 291, 434, 411, 11, 1392, 11, 300, 390, 2085, 11, 457, 411, 11, 1669, 2020, 13, 663, 311, 51024], "temperature": 0.0, "avg_logprob": -0.20607470159661279, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.0029788799583911896}, {"id": 895, "seek": 348176, "start": 3498.0, "end": 3506.4, "text": " is there a way to quantify that and were related things or is it more like a you know it when", "tokens": [51176, 307, 456, 257, 636, 281, 40421, 300, 293, 645, 4077, 721, 420, 307, 309, 544, 411, 257, 291, 458, 309, 562, 51596], "temperature": 0.0, "avg_logprob": -0.20607470159661279, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.0029788799583911896}, {"id": 896, "seek": 350640, "start": 3506.4, "end": 3512.64, "text": " you see it? Oh, yeah, that's a great question. There is a work done by Google, I think, that actually", "tokens": [50364, 291, 536, 309, 30, 876, 11, 1338, 11, 300, 311, 257, 869, 1168, 13, 821, 307, 257, 589, 1096, 538, 3329, 11, 286, 519, 11, 300, 767, 50676], "temperature": 0.0, "avg_logprob": -0.10717721168811505, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0016220820834860206}, {"id": 897, "seek": 350640, "start": 3514.0, "end": 3521.6, "text": " relaxes the task of classifying objects using ImageNet. They actually re-tagged ImageNet,", "tokens": [50744, 5789, 279, 264, 5633, 295, 1508, 5489, 6565, 1228, 29903, 31890, 13, 814, 767, 319, 12, 25030, 3004, 29903, 31890, 11, 51124], "temperature": 0.0, "avg_logprob": -0.10717721168811505, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0016220820834860206}, {"id": 898, "seek": 350640, "start": 3521.6, "end": 3526.88, "text": " where, you know, a strawberry in that case wouldn't be a mistake, but maybe it would be", "tokens": [51124, 689, 11, 291, 458, 11, 257, 20440, 294, 300, 1389, 2759, 380, 312, 257, 6146, 11, 457, 1310, 309, 576, 312, 51388], "temperature": 0.0, "avg_logprob": -0.10717721168811505, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0016220820834860206}, {"id": 899, "seek": 350640, "start": 3526.88, "end": 3533.6, "text": " half a mistake or something like that. Yeah, so there's such a work that re-tags the entire ImageNet", "tokens": [51388, 1922, 257, 6146, 420, 746, 411, 300, 13, 865, 11, 370, 456, 311, 1270, 257, 589, 300, 319, 12, 83, 12109, 264, 2302, 29903, 31890, 51724], "temperature": 0.0, "avg_logprob": -0.10717721168811505, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0016220820834860206}, {"id": 900, "seek": 353360, "start": 3533.6, "end": 3539.04, "text": " dataset to account for mistakes that aren't really mistakes, but actually makes sense.", "tokens": [50364, 28872, 281, 2696, 337, 8038, 300, 3212, 380, 534, 8038, 11, 457, 767, 1669, 2020, 13, 50636], "temperature": 0.0, "avg_logprob": -0.10890159812024845, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007207064772956073}, {"id": 901, "seek": 353360, "start": 3540.16, "end": 3547.44, "text": " But other than that, I would say there isn't an automatic way to know that. I mean, I can't think", "tokens": [50692, 583, 661, 813, 300, 11, 286, 576, 584, 456, 1943, 380, 364, 12509, 636, 281, 458, 300, 13, 286, 914, 11, 286, 393, 380, 519, 51056], "temperature": 0.0, "avg_logprob": -0.10890159812024845, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007207064772956073}, {"id": 902, "seek": 353360, "start": 3547.44, "end": 3551.92, "text": " off the top of my head of an automatic way to know when the model is mistaken, but it's okay.", "tokens": [51056, 766, 264, 1192, 295, 452, 1378, 295, 364, 12509, 636, 281, 458, 562, 264, 2316, 307, 21333, 11, 457, 309, 311, 1392, 13, 51280], "temperature": 0.0, "avg_logprob": -0.10890159812024845, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007207064772956073}, {"id": 903, "seek": 353360, "start": 3553.36, "end": 3559.44, "text": " Cool. How did you guys check? Like, was it mainly the accuracy increase on the distribution", "tokens": [51352, 8561, 13, 1012, 630, 291, 1074, 1520, 30, 1743, 11, 390, 309, 8704, 264, 14170, 3488, 322, 264, 7316, 51656], "temperature": 0.0, "avg_logprob": -0.10890159812024845, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0007207064772956073}, {"id": 904, "seek": 355944, "start": 3559.44, "end": 3567.12, "text": " shifted versions? Yeah, yeah, it was mainly, yeah, it was mainly the accuracy on the distribution", "tokens": [50364, 18892, 9606, 30, 865, 11, 1338, 11, 309, 390, 8704, 11, 1338, 11, 309, 390, 8704, 264, 14170, 322, 264, 7316, 50748], "temperature": 0.0, "avg_logprob": -0.2323143127116751, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0030256116297096014}, {"id": 905, "seek": 355944, "start": 3567.12, "end": 3575.68, "text": " shifts. And, yeah, yeah, and also looking at a lot of examples, right? Because I started out", "tokens": [50748, 19201, 13, 400, 11, 1338, 11, 1338, 11, 293, 611, 1237, 412, 257, 688, 295, 5110, 11, 558, 30, 1436, 286, 1409, 484, 51176], "temperature": 0.0, "avg_logprob": -0.2323143127116751, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0030256116297096014}, {"id": 906, "seek": 355944, "start": 3575.68, "end": 3584.8, "text": " with many. Yeah, yeah, yeah, and a lot of manual work on actualizing examples that got me to the", "tokens": [51176, 365, 867, 13, 865, 11, 1338, 11, 1338, 11, 293, 257, 688, 295, 9688, 589, 322, 3539, 3319, 5110, 300, 658, 385, 281, 264, 51632], "temperature": 0.0, "avg_logprob": -0.2323143127116751, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0030256116297096014}, {"id": 907, "seek": 355944, "start": 3584.8, "end": 3588.7200000000003, "text": " intuition that I'm presenting now, because I actually thought in the beginning that having", "tokens": [51632, 24002, 300, 286, 478, 15578, 586, 11, 570, 286, 767, 1194, 294, 264, 2863, 300, 1419, 51828], "temperature": 0.0, "avg_logprob": -0.2323143127116751, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0030256116297096014}, {"id": 908, "seek": 358872, "start": 3588.72, "end": 3595.6, "text": " the relevance be a segmentation map is quite logical. Yeah, so it takes some time to get", "tokens": [50364, 264, 32684, 312, 257, 9469, 399, 4471, 307, 1596, 14978, 13, 865, 11, 370, 309, 2516, 512, 565, 281, 483, 50708], "temperature": 0.0, "avg_logprob": -0.3082411161033056, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0012633494334295392}, {"id": 909, "seek": 358872, "start": 3595.6, "end": 3602.0, "text": " through all the conclusions. Yeah, yeah. I was just having one idea coming out. Is that possible?", "tokens": [50708, 807, 439, 264, 22865, 13, 865, 11, 1338, 13, 286, 390, 445, 1419, 472, 1558, 1348, 484, 13, 1119, 300, 1944, 30, 51028], "temperature": 0.0, "avg_logprob": -0.3082411161033056, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0012633494334295392}, {"id": 910, "seek": 358872, "start": 3602.0, "end": 3609.7599999999998, "text": " So you are going to be this key time. So that in a strawberry case, you can crop out that region", "tokens": [51028, 407, 291, 366, 516, 281, 312, 341, 2141, 565, 13, 407, 300, 294, 257, 20440, 1389, 11, 291, 393, 9086, 484, 300, 4458, 51416], "temperature": 0.0, "avg_logprob": -0.3082411161033056, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0012633494334295392}, {"id": 911, "seek": 358872, "start": 3609.7599999999998, "end": 3615.2, "text": " and then maybe run through another like Oracle network to tell you whether it is a strawberry or", "tokens": [51416, 293, 550, 1310, 1190, 807, 1071, 411, 25654, 3209, 281, 980, 291, 1968, 309, 307, 257, 20440, 420, 51688], "temperature": 0.0, "avg_logprob": -0.3082411161033056, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.0012633494334295392}, {"id": 912, "seek": 361520, "start": 3615.2799999999997, "end": 3620.3199999999997, "text": " not. And that gives you some kind of a semi-automatic way. Yeah, yeah, definitely. That's an", "tokens": [50368, 406, 13, 400, 300, 2709, 291, 512, 733, 295, 257, 12909, 12, 1375, 13143, 636, 13, 865, 11, 1338, 11, 2138, 13, 663, 311, 364, 50620], "temperature": 0.0, "avg_logprob": -0.20212985397478855, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.002980017801746726}, {"id": 913, "seek": 361520, "start": 3620.3199999999997, "end": 3625.6, "text": " interesting take. It's interesting, particularly because I saw that different models tend to", "tokens": [50620, 1880, 747, 13, 467, 311, 1880, 11, 4098, 570, 286, 1866, 300, 819, 5245, 3928, 281, 50884], "temperature": 0.0, "avg_logprob": -0.20212985397478855, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.002980017801746726}, {"id": 914, "seek": 361520, "start": 3625.6, "end": 3630.7999999999997, "text": " learn different spurious correlations. So it actually makes sense to check models using other", "tokens": [50884, 1466, 819, 637, 24274, 13983, 763, 13, 407, 309, 767, 1669, 2020, 281, 1520, 5245, 1228, 661, 51144], "temperature": 0.0, "avg_logprob": -0.20212985397478855, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.002980017801746726}, {"id": 915, "seek": 361520, "start": 3630.7999999999997, "end": 3635.8399999999997, "text": " models. Yeah, they're consistently making the same prediction with these vets. Yeah, yeah, perhaps.", "tokens": [51144, 5245, 13, 865, 11, 436, 434, 14961, 1455, 264, 912, 17630, 365, 613, 371, 1385, 13, 865, 11, 1338, 11, 4317, 13, 51396], "temperature": 0.0, "avg_logprob": -0.20212985397478855, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.002980017801746726}, {"id": 916, "seek": 361520, "start": 3635.8399999999997, "end": 3641.7599999999998, "text": " Yeah, yeah, that's an interesting idea. Your current relevancy extractor approach", "tokens": [51396, 865, 11, 1338, 11, 300, 311, 364, 1880, 1558, 13, 2260, 2190, 25916, 6717, 8947, 284, 3109, 51692], "temperature": 0.0, "avg_logprob": -0.20212985397478855, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.002980017801746726}, {"id": 917, "seek": 364176, "start": 3642.7200000000003, "end": 3650.48, "text": " is limited by the VITs tile resolution. It outputs the attention map that is the size of the", "tokens": [50412, 307, 5567, 538, 264, 691, 3927, 82, 20590, 8669, 13, 467, 23930, 264, 3202, 4471, 300, 307, 264, 2744, 295, 264, 50800], "temperature": 0.0, "avg_logprob": -0.3303030309542804, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0014077964005991817}, {"id": 918, "seek": 364176, "start": 3650.48, "end": 3658.48, "text": " tiles and then you can bi-level it. Upscaling, yeah. I was wondering whether there's a way to bypass", "tokens": [50800, 21982, 293, 550, 291, 393, 3228, 12, 12418, 309, 13, 5858, 4417, 4270, 11, 1338, 13, 286, 390, 6359, 1968, 456, 311, 257, 636, 281, 24996, 51200], "temperature": 0.0, "avg_logprob": -0.3303030309542804, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0014077964005991817}, {"id": 919, "seek": 364176, "start": 3658.48, "end": 3665.5200000000004, "text": " this tile resolution just by considering that we also have pixels coming into the tile.", "tokens": [51200, 341, 20590, 8669, 445, 538, 8079, 300, 321, 611, 362, 18668, 1348, 666, 264, 20590, 13, 51552], "temperature": 0.0, "avg_logprob": -0.3303030309542804, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0014077964005991817}, {"id": 920, "seek": 366552, "start": 3666.08, "end": 3673.44, "text": " Yeah, yeah, we have tried to propagate relevance all the way back to the actual input and not on", "tokens": [50392, 865, 11, 1338, 11, 321, 362, 3031, 281, 48256, 32684, 439, 264, 636, 646, 281, 264, 3539, 4846, 293, 406, 322, 50760], "temperature": 0.0, "avg_logprob": -0.09726669523451063, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0021820724941790104}, {"id": 921, "seek": 366552, "start": 3673.44, "end": 3679.92, "text": " the level of each patch. It didn't come out just quite as we hoped. I think that the issue there", "tokens": [50760, 264, 1496, 295, 1184, 9972, 13, 467, 994, 380, 808, 484, 445, 1596, 382, 321, 19737, 13, 286, 519, 300, 264, 2734, 456, 51084], "temperature": 0.0, "avg_logprob": -0.09726669523451063, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0021820724941790104}, {"id": 922, "seek": 366552, "start": 3679.92, "end": 3685.04, "text": " is probably the positional encoding in the way. Somehow that layer of positional encoding", "tokens": [51084, 307, 1391, 264, 2535, 304, 43430, 294, 264, 636, 13, 28357, 300, 4583, 295, 2535, 304, 43430, 51340], "temperature": 0.0, "avg_logprob": -0.09726669523451063, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0021820724941790104}, {"id": 923, "seek": 366552, "start": 3686.32, "end": 3692.8, "text": " ruins or destroys the relevance values once you propagate back from it. I couldn't figure out how", "tokens": [51404, 24747, 420, 36714, 264, 32684, 4190, 1564, 291, 48256, 646, 490, 309, 13, 286, 2809, 380, 2573, 484, 577, 51728], "temperature": 0.0, "avg_logprob": -0.09726669523451063, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0021820724941790104}, {"id": 924, "seek": 369280, "start": 3692.8, "end": 3702.0800000000004, "text": " to get past that layer that actually kind of added noise to the output relevance map.", "tokens": [50364, 281, 483, 1791, 300, 4583, 300, 767, 733, 295, 3869, 5658, 281, 264, 5598, 32684, 4471, 13, 50828], "temperature": 0.0, "avg_logprob": -0.23780634236890216, "compression_ratio": 1.3157894736842106, "no_speech_prob": 0.002547502052038908}, {"id": 925, "seek": 369280, "start": 3702.5600000000004, "end": 3719.84, "text": " That's an interesting point, but yeah. Yeah, I haven't come across any such architectures", "tokens": [50852, 663, 311, 364, 1880, 935, 11, 457, 1338, 13, 865, 11, 286, 2378, 380, 808, 2108, 604, 1270, 6331, 1303, 51716], "temperature": 0.0, "avg_logprob": -0.23780634236890216, "compression_ratio": 1.3157894736842106, "no_speech_prob": 0.002547502052038908}, {"id": 926, "seek": 371984, "start": 3720.4, "end": 3727.04, "text": " if you do let me know and I can give you the try. No, there was a question here, right?", "tokens": [50392, 498, 291, 360, 718, 385, 458, 293, 286, 393, 976, 291, 264, 853, 13, 883, 11, 456, 390, 257, 1168, 510, 11, 558, 30, 50724], "temperature": 0.0, "avg_logprob": -0.2560816832951137, "compression_ratio": 1.0609756097560976, "no_speech_prob": 0.009261613711714745}, {"id": 927, "seek": 372704, "start": 3727.36, "end": 3732.48, "text": " I was just wondering, when you're about to work for, what if I wanted to explain about a color", "tokens": [50380, 286, 390, 445, 6359, 11, 562, 291, 434, 466, 281, 589, 337, 11, 437, 498, 286, 1415, 281, 2903, 466, 257, 2017, 50636], "temperature": 0.0, "avg_logprob": -0.44197991412618887, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.02331053465604782}, {"id": 928, "seek": 372704, "start": 3732.48, "end": 3736.0, "text": " or something, or non-message, I'd like to just go to something about discussion, right? Maybe", "tokens": [50636, 420, 746, 11, 420, 2107, 12, 76, 442, 609, 11, 286, 1116, 411, 281, 445, 352, 281, 746, 466, 5017, 11, 558, 30, 2704, 50812], "temperature": 0.0, "avg_logprob": -0.44197991412618887, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.02331053465604782}, {"id": 929, "seek": 372704, "start": 3736.64, "end": 3741.2, "text": " then perhaps lemon versus orange, you can color, kind of the main thing. That's just curious.", "tokens": [50844, 550, 4317, 11356, 5717, 7671, 11, 291, 393, 2017, 11, 733, 295, 264, 2135, 551, 13, 663, 311, 445, 6369, 13, 51072], "temperature": 0.0, "avg_logprob": -0.44197991412618887, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.02331053465604782}, {"id": 930, "seek": 372704, "start": 3744.72, "end": 3749.7599999999998, "text": " This specific method would not be able to do that, but I know that there are explainability", "tokens": [51248, 639, 2685, 3170, 576, 406, 312, 1075, 281, 360, 300, 11, 457, 286, 458, 300, 456, 366, 2903, 2310, 51500], "temperature": 0.0, "avg_logprob": -0.44197991412618887, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.02331053465604782}, {"id": 931, "seek": 372704, "start": 3749.7599999999998, "end": 3756.96, "text": " methods that kind of create a decision tree from the model. So you pay the price,", "tokens": [51500, 7150, 300, 733, 295, 1884, 257, 3537, 4230, 490, 264, 2316, 13, 407, 291, 1689, 264, 3218, 11, 51860], "temperature": 0.0, "avg_logprob": -0.44197991412618887, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.02331053465604782}, {"id": 932, "seek": 375704, "start": 3757.12, "end": 3762.48, "text": " that the accuracy decreases to some extent, and then you create a decision tree based on the", "tokens": [50368, 300, 264, 14170, 24108, 281, 512, 8396, 11, 293, 550, 291, 1884, 257, 3537, 4230, 2361, 322, 264, 50636], "temperature": 0.0, "avg_logprob": -0.11968814448306435, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.0008825682452879846}, {"id": 933, "seek": 375704, "start": 3762.48, "end": 3767.6, "text": " decisions of the model. You kind of model the model using a decision tree, and then you may", "tokens": [50636, 5327, 295, 264, 2316, 13, 509, 733, 295, 2316, 264, 2316, 1228, 257, 3537, 4230, 11, 293, 550, 291, 815, 50892], "temperature": 0.0, "avg_logprob": -0.11968814448306435, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.0008825682452879846}, {"id": 934, "seek": 375704, "start": 3767.6, "end": 3775.12, "text": " have a split that it has to do with, you pass a lot of images through a lot of images of oranges", "tokens": [50892, 362, 257, 7472, 300, 309, 575, 281, 360, 365, 11, 291, 1320, 257, 688, 295, 5267, 807, 257, 688, 295, 5267, 295, 35474, 51268], "temperature": 0.0, "avg_logprob": -0.11968814448306435, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.0008825682452879846}, {"id": 935, "seek": 375704, "start": 3775.12, "end": 3780.8, "text": " and lemons, and you see that one of the splits is by the color. Yeah, and then you know that.", "tokens": [51268, 293, 47098, 11, 293, 291, 536, 300, 472, 295, 264, 37741, 307, 538, 264, 2017, 13, 865, 11, 293, 550, 291, 458, 300, 13, 51552], "temperature": 0.0, "avg_logprob": -0.11968814448306435, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.0008825682452879846}, {"id": 936, "seek": 378080, "start": 3781.2000000000003, "end": 3788.96, "text": " And probably you can do some trivial things to test specific theories, like turn the image", "tokens": [50384, 400, 1391, 291, 393, 360, 512, 26703, 721, 281, 1500, 2685, 13667, 11, 411, 1261, 264, 3256, 50772], "temperature": 0.0, "avg_logprob": -0.2566487491130829, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.00164670473895967}, {"id": 937, "seek": 378080, "start": 3788.96, "end": 3794.1600000000003, "text": " into black and white and see what happens, to consider if the model takes into account,", "tokens": [50772, 666, 2211, 293, 2418, 293, 536, 437, 2314, 11, 281, 1949, 498, 264, 2316, 2516, 666, 2696, 11, 51032], "temperature": 0.0, "avg_logprob": -0.2566487491130829, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.00164670473895967}, {"id": 938, "seek": 378080, "start": 3794.8, "end": 3797.36, "text": " but this method will not be able to do that.", "tokens": [51064, 457, 341, 3170, 486, 406, 312, 1075, 281, 360, 300, 13, 51192], "temperature": 0.0, "avg_logprob": -0.2566487491130829, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.00164670473895967}, {"id": 939, "seek": 378080, "start": 3800.0, "end": 3801.52, "text": " All right, let's hand this speaker.", "tokens": [51324, 1057, 558, 11, 718, 311, 1011, 341, 8145, 13, 51400], "temperature": 0.0, "avg_logprob": -0.2566487491130829, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.00164670473895967}], "language": "en"}