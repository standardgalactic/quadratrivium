WEBVTT

00:00.000 --> 00:04.640
Awesome, so we're so excited to have Hila Chepur, am I pronouncing it right?

00:04.640 --> 00:06.400
Yeah, it's actually in Hebrew, it's Chepur.

00:06.400 --> 00:08.800
Oh, it's Chepur.

00:08.800 --> 00:10.080
Yeah.

00:10.080 --> 00:10.880
All right.

00:10.880 --> 00:13.840
So Hila is a PhD candidate at Tel Aviv University,

00:13.840 --> 00:15.680
advised by Professor Leon Wolfe.

00:16.400 --> 00:19.920
Her research focuses on developing reliable XAI algorithms

00:19.920 --> 00:22.880
and leveraging them to promote model accuracy and fairness.

00:23.760 --> 00:26.640
Today she's going to talk to us about transform or explainability,

00:26.640 --> 00:28.560
and we're so excited to hear from you.

00:28.640 --> 00:30.400
Oh, thank you for that great introduction,

00:30.400 --> 00:32.320
and I actually have some slides here.

00:32.960 --> 00:33.760
I think you can just...

00:34.480 --> 00:35.440
Oh, yeah.

00:35.440 --> 00:38.880
Slides here introducing myself, but I think you did that perfectly,

00:38.880 --> 00:39.760
so I'll just skip that.

00:41.200 --> 00:44.560
So maybe the first thing we want to talk about is motivation.

00:44.560 --> 00:46.560
We're here to talk about transform or explainability,

00:46.560 --> 00:47.920
but why should you care?

00:48.640 --> 00:50.080
And let's just have a disclaimer,

00:50.080 --> 00:52.640
because we all know that explainability is really important

00:52.640 --> 00:56.240
for aspects like accountability, reliability, and so on.

00:56.240 --> 00:57.680
But when we write research papers,

00:57.680 --> 01:00.080
we usually focus on other measuring sticks,

01:00.080 --> 01:03.040
such as accuracy and robustness, right?

01:03.680 --> 01:06.800
So I'm here to convince you that the explainability queue

01:06.800 --> 01:10.240
is actually really useful for those measuring sticks as well,

01:10.240 --> 01:13.360
and that you should consider using explainability

01:13.360 --> 01:15.200
in your research, even if it's unrelated

01:15.200 --> 01:17.200
to accountability, reliability, and so on.

01:18.640 --> 01:21.120
So to do that, I have a few examples showing

01:21.120 --> 01:24.080
how explainability can be used to improve model accuracy

01:24.080 --> 01:25.680
and robustness.

01:25.760 --> 01:27.520
The first example is text to live.

01:27.520 --> 01:28.240
Maybe you know it.

01:28.240 --> 01:30.560
It got accepted to ECCV 2022.

01:31.120 --> 01:35.200
And the objective of the model is to take a target edit text,

01:35.200 --> 01:39.440
for example, here around hat, and apply the edit to the image.

01:39.440 --> 01:42.320
So what they try to do is unlike other works,

01:42.320 --> 01:45.600
they try to prevent the part where the user actually has

01:45.600 --> 01:48.960
to insert a manual segmentation mask

01:48.960 --> 01:51.120
to indicate where the hat is.

01:51.680 --> 01:54.800
So they wanted some automatic way of getting

01:54.800 --> 01:57.040
to the region of interest, which is the hat.

01:57.040 --> 01:59.360
So they use relevancy maps of the clip model,

01:59.360 --> 02:00.880
which I guess you're familiar with.

02:02.560 --> 02:06.000
So the clip model with the relevancy map actually indicated

02:06.000 --> 02:12.480
to their downstream path, where pretty much the hat is in the image.

02:13.040 --> 02:16.080
And then their model refined those relevancy maps

02:16.080 --> 02:20.480
and applied the edit according to the location indicated

02:20.480 --> 02:21.840
by the relevancy map.

02:21.840 --> 02:24.000
So here you can see what happens without this.

02:24.000 --> 02:26.880
They call it bootstrapping without using the relevancy map.

02:26.880 --> 02:29.120
So you can see that there are additional artifacts,

02:29.120 --> 02:31.920
such as the faces turning red and not just the hat.

02:31.920 --> 02:34.640
And when you use bootstrapping, when you use relevancy maps,

02:34.640 --> 02:38.240
then the edit is quite localized to the hat.

02:38.240 --> 02:40.720
And here you can see other edits that are quite nice.

02:40.720 --> 02:44.160
They take a sponge cake and they turn it into an ice cake

02:44.160 --> 02:45.280
or a spinach mousse cake.

02:45.280 --> 02:47.760
So I think it's a nice example to show.

02:48.800 --> 02:51.360
Another example is a paper called Kripaso.

02:51.360 --> 02:54.240
It's that best paper awarded Seagraph 2022.

02:54.800 --> 02:56.640
It also uses relevancy maps.

02:56.640 --> 02:59.120
The goal of the model is to take an input image

02:59.120 --> 03:02.480
and create sketches with different levels of abstraction.

03:02.480 --> 03:04.720
So you can choose which level of abstraction you want.

03:04.720 --> 03:06.240
So for example, the flamingo here,

03:06.960 --> 03:11.680
very abstract painting of the flamingo or very detailed painting.

03:11.680 --> 03:13.840
And what they did is they actually used the relevancy maps

03:13.840 --> 03:18.880
as an initializer for the model to understand where the object is

03:18.880 --> 03:20.960
and how to create the stroke's product.

03:22.160 --> 03:23.760
Another example is Sameer's work,

03:23.760 --> 03:25.680
which you're probably familiar with.

03:27.760 --> 03:31.040
What you did here, really, is you used the relevancy maps

03:31.040 --> 03:34.160
in order to locate objects which aren't necessarily

03:34.880 --> 03:38.480
objects in the wild, objects that appear in living rooms

03:38.480 --> 03:42.160
and that do not necessarily appear in the training set

03:42.160 --> 03:46.160
of some models of desegmentation or localization.

03:46.160 --> 03:49.120
So you can use Kripaso in order to identify objects

03:49.120 --> 03:51.920
that are really not really objects

03:51.920 --> 03:53.680
that are so common in training sets.

03:55.760 --> 03:59.280
So if you consider all the examples that we've seen before,

03:59.280 --> 04:01.040
they have one thing in common.

04:01.040 --> 04:04.160
They use the relevancy maps as a fixed signal.

04:04.160 --> 04:06.640
They didn't train on the relevancy map

04:06.640 --> 04:08.480
or create a loss with the relevancy map.

04:08.480 --> 04:11.840
They use it as an initialization for the answering task.

04:12.400 --> 04:14.880
But what we did in our last work is actually just,

04:14.880 --> 04:18.560
we showed that its reliability can be used as a loss term

04:18.560 --> 04:20.080
in order to improve models.

04:20.640 --> 04:22.560
So if you think about what it means

04:22.560 --> 04:25.440
to create a loss term based on explainability maps,

04:25.440 --> 04:29.440
it's really meant to teach the model how to do something

04:29.440 --> 04:32.800
or why it does something, not just how to do something.

04:32.800 --> 04:34.800
And we talk about classification models.

04:34.800 --> 04:37.760
They tend to learn spurious cues

04:37.760 --> 04:40.960
to help them make shortcuts to make a prediction.

04:40.960 --> 04:43.360
So for example, a model can learn a spurious cue

04:43.360 --> 04:47.360
that if you have a round object with the background of a grass,

04:47.360 --> 04:48.800
then it's a golf ball.

04:48.800 --> 04:51.040
And here you can see that the model classified this lemon

04:51.040 --> 04:53.520
as a golf ball because of the background of the grass.

04:54.160 --> 04:56.560
When you force the model to actually focus

04:56.560 --> 04:58.240
on the foreground of the image

04:58.240 --> 04:59.760
and not just the background of the image,

05:00.560 --> 05:04.080
via a loss applied directly to the explainability maps,

05:04.080 --> 05:06.880
you can correct wrong predictions based on spurious cues.

05:07.440 --> 05:10.080
So what we're doing usually is we're teaching the model

05:10.080 --> 05:11.440
to predict something, right?

05:11.440 --> 05:15.360
Predict golf ball, car, glasses, etc.

05:15.360 --> 05:17.600
But we're not really teaching it why.

05:17.600 --> 05:20.240
Why this is the object in the image.

05:20.240 --> 05:24.800
So what we're showing here is that by fine-tuning directly

05:24.800 --> 05:27.440
the relevant slots or the explainability maps,

05:27.440 --> 05:30.160
we can correct wrong predictions based on spurious conditions.

05:30.960 --> 05:37.200
But we'll get to it in depth later on.

05:38.400 --> 05:40.000
So this was the motivation part,

05:40.000 --> 05:42.480
and hopefully you got fully motivated

05:42.480 --> 05:44.720
as to why Transformer experiment is interesting.

05:45.360 --> 05:48.240
Our talk is going to be a construed of two parts.

05:48.240 --> 05:49.840
The first part is going to be,

05:49.840 --> 05:52.720
we're going to talk about how we do Transformer explainability.

05:52.720 --> 05:54.640
We're going to see the attention mechanism

05:54.640 --> 05:56.160
which I'm sure you're all familiar with,

05:56.160 --> 05:59.280
but we're going to have emphasis on specific parts

05:59.280 --> 06:01.280
of the attention mechanism that are going to be useful.

06:01.840 --> 06:03.200
Then we're going to ask ourselves,

06:03.200 --> 06:04.720
is attention an explanation?

06:04.720 --> 06:07.360
Which is really the most prominent question

06:07.360 --> 06:09.440
when doing Transformer explainability.

06:09.440 --> 06:12.400
We're going to talk about three explainability algorithms.

06:12.400 --> 06:14.240
The first one is attention roll-up, not by me.

06:14.960 --> 06:17.840
But it is the groundbreaking first algorithm

06:17.840 --> 06:19.200
in the big Transformer explainability.

06:19.200 --> 06:21.360
Then I'm going to present two of my works

06:21.360 --> 06:23.280
that have to do with Transformer explainability.

06:24.160 --> 06:25.040
And in the last part,

06:25.040 --> 06:27.360
we're going to talk about the work that I just presented

06:27.360 --> 06:29.280
and that Shiran had a question on.

06:30.240 --> 06:32.640
And probably hopefully we'll answer the questions

06:32.640 --> 06:35.200
and see how Transformer explainability can be used to

06:35.840 --> 06:40.320
devise models or maybe incorrect spray-excused

06:40.320 --> 06:41.040
at the models where.

06:41.040 --> 06:44.800
So just to set us up,

06:44.800 --> 06:46.320
this is a Transformer architecture

06:46.320 --> 06:48.000
as presented in attention is all you need.

06:48.000 --> 06:49.520
I'm sure you're all familiar with it.

06:50.080 --> 06:52.320
We will be focusing on the encoder

06:52.320 --> 06:54.240
since the attention mechanism here

06:54.240 --> 06:55.840
is a self-attention mechanism.

06:55.840 --> 06:57.040
And it's quite more intuitive

06:57.040 --> 06:58.960
and easy to grasp and understand.

06:58.960 --> 07:01.200
And the concepts that apply here to the encoder

07:01.200 --> 07:03.680
are pretty easily generalized to the encoder.

07:03.680 --> 07:06.320
So we're going to focus on the encoder for simplicity

07:06.320 --> 07:08.640
and for intuitive explanations,

07:08.640 --> 07:11.840
but the principle is quite easily generalized

07:11.840 --> 07:13.040
to the encoder as well.

07:13.040 --> 07:14.160
By the way, if you have questions,

07:14.160 --> 07:15.680
just feel free to stop me.

07:17.680 --> 07:20.960
Okay, so let's talk about the intuition behind self-attention.

07:21.520 --> 07:23.760
What self-attention does is actually creates

07:24.480 --> 07:26.160
contextualized representations.

07:26.160 --> 07:27.680
So I'm sorry for the people on Zoom,

07:27.680 --> 07:29.520
but I'm going to write here on the right for it.

07:29.520 --> 07:31.040
So we have an example.

07:31.040 --> 07:32.560
We're running an example to work with.

07:33.120 --> 07:35.840
Say we have the sentence, the count,

07:36.800 --> 07:40.240
set on the max.

07:42.560 --> 07:44.800
And let's consider the first day in the sentence.

07:45.760 --> 07:48.560
We want to create now an embedding

07:48.560 --> 07:50.240
for each one of the words in the sentence,

07:50.240 --> 07:52.320
for each one of the token incidents.

07:52.320 --> 07:54.160
But the token does quite meaningless

07:54.160 --> 07:55.440
with our context, right?

07:55.440 --> 07:56.800
It could refer to anywhere.

07:57.360 --> 07:58.960
So what the attention mechanism does

07:58.960 --> 08:01.520
is it actually creates contextualized representation.

08:02.160 --> 08:05.920
It should take information from the other token

08:05.920 --> 08:09.920
and insert it to the current token that we're interested in.

08:10.480 --> 08:13.440
So for example, intuitively, maybe we would expect

08:13.440 --> 08:17.360
that since the word the refers to the word cat,

08:18.480 --> 08:22.240
information from the word cat will be moved into the word the,

08:22.240 --> 08:25.680
such that the embedding of the word that is contextualized

08:25.680 --> 08:29.280
is enriched by context from the word cat.

08:30.240 --> 08:32.240
So what the attention mechanism does

08:32.240 --> 08:36.160
is it actually uses query, key, and value matrices.

08:36.160 --> 08:40.880
And we can think about it maybe as a database theory information.

08:41.440 --> 08:44.960
So when we talk about databases, we have queries,

08:46.880 --> 08:49.680
which are questions that we run on our database.

08:50.480 --> 08:51.680
We have keys.

08:51.680 --> 08:54.160
The keys represent the entries in our database.

08:54.800 --> 08:57.840
And we have values that corresponds to the keys, right?

08:57.840 --> 09:01.840
So what we're doing here is we're actually running a query

09:01.840 --> 09:12.000
asking which tokens are relevant to the...

09:14.160 --> 09:16.560
We can think about it intuitively as running a query

09:16.560 --> 09:18.560
on all of these tokens that we have.

09:19.120 --> 09:21.520
And then the keys represent all the other tokens.

09:22.080 --> 09:23.840
What we do with the attention mechanism

09:24.160 --> 09:26.960
is we calculate an attention matrix

09:26.960 --> 09:29.840
that is going to be the star of every transformer experience

09:29.840 --> 09:30.960
ability algorithm.

09:31.840 --> 09:35.840
It's going to be a soft mass of the multiplication

09:35.840 --> 09:40.960
between queries and keys normalized by the embedding dimension.

09:41.840 --> 09:45.840
This similarity scores are actually telling us

09:45.840 --> 09:49.840
how much each word is relevant to our word of interest.

09:50.800 --> 09:52.640
So the multiplication between queries and keys,

09:52.640 --> 09:55.840
we can think about it kind of like as relevant scores.

09:55.840 --> 09:59.040
How much is each token relevant to the token that...

09:59.040 --> 10:00.240
to the word that...

10:03.040 --> 10:06.240
And after we calculate those similarity scores,

10:06.240 --> 10:08.480
we create the enriched representation

10:09.200 --> 10:11.840
by multiplying the scores by the values.

10:13.040 --> 10:16.640
Such that each word gets information from the other words

10:17.360 --> 10:18.640
by these relevance values.

10:18.640 --> 10:21.440
So these relevance values determine how much each word

10:21.440 --> 10:24.640
is going to influence the word of after the attention.

10:26.640 --> 10:29.440
So this is going to be the key intuition to everything

10:29.440 --> 10:32.240
that we do later on to explain a transformers.

10:34.640 --> 10:36.640
The most important thing to remember about explaining

10:36.640 --> 10:40.640
transformers is we don't have just a single attention matrix.

10:40.640 --> 10:42.640
This mechanism happens H times,

10:42.640 --> 10:44.640
where H is the number of attention heads

10:44.640 --> 10:45.640
that we have.

10:45.640 --> 10:47.640
And intuitively, we can think about it as,

10:47.640 --> 10:49.640
you know, in CNNs, you have kernels.

10:49.640 --> 10:51.640
Each kernel has its own purpose.

10:51.640 --> 10:54.640
Some refer to the background, some refer to the edges,

10:54.640 --> 10:56.640
the shapes, and so on.

10:56.640 --> 10:58.640
Transformers have the same thing with attention heads.

10:58.640 --> 11:00.640
So each attention head can have a different purpose.

11:00.640 --> 11:02.640
And actually, researchers have shown

11:02.640 --> 11:06.640
that you can probably prune most of the attention head

11:06.640 --> 11:09.640
and achieve the same accuracy,

11:09.640 --> 11:12.640
which means that most attention heads are really not important

11:13.640 --> 11:16.640
to the prediction, to a specific prediction of the model.

11:16.640 --> 11:19.640
So it's really important when we think about transformers

11:19.640 --> 11:22.640
and to understand that the different heads have different needs.

11:25.640 --> 11:28.640
The final thing that we need to remember about transformer,

11:28.640 --> 11:31.640
you know, predictions is that transformers use

11:31.640 --> 11:34.640
a classification token for the prediction.

11:34.640 --> 11:36.640
So once the entire attention mechanism is done

11:36.640 --> 11:38.640
and all the tokens are contextualized,

11:38.640 --> 11:41.640
the classification token is the only token.

11:42.640 --> 11:44.640
That is used to make the prediction.

11:44.640 --> 11:46.640
There's a linear layer on top of the classification token

11:46.640 --> 11:48.640
and then the prediction is made.

11:48.640 --> 11:50.640
So basically what the classification token does

11:50.640 --> 11:54.640
is kind of like creates an aggregated representation

11:54.640 --> 11:56.640
of the entire input.

11:56.640 --> 11:58.640
You can think about it as a global representation

11:58.640 --> 12:00.640
of all the tokens in the input.

12:00.640 --> 12:02.640
You have questions so far,

12:02.640 --> 12:05.640
because we're going to move on to the interesting stuff.

12:05.640 --> 12:07.640
Yeah.

12:07.640 --> 12:09.640
So moving on to transformer explainability,

12:09.640 --> 12:12.640
it's really important to set up our goals.

12:12.640 --> 12:15.640
My goal is to facilitate explanations that help you guys,

12:15.640 --> 12:18.640
the researchers that actually use the models.

12:18.640 --> 12:21.640
And the way that we do that is by creating hitmaps.

12:21.640 --> 12:24.640
So the hitmaps should correspond to the pixels,

12:24.640 --> 12:27.640
if we're speaking of images or if we're speaking of text,

12:27.640 --> 12:30.640
and hitmaps should correspond to the tokens.

12:30.640 --> 12:32.640
The hitmaps should correspond to the pixels

12:32.640 --> 12:34.640
that influence the prediction by the model.

12:34.640 --> 12:36.640
So for example, here we see the verb

12:36.640 --> 12:39.640
and the hitmaps actually highlights the pixels

12:39.640 --> 12:41.640
relating to the verb.

12:41.640 --> 12:45.640
And the toothbrush or the ship or the bikes and so on.

12:45.640 --> 12:48.640
So the hitmaps should tell us which pixels in the input

12:48.640 --> 12:53.640
make the prediction as it is.

12:53.640 --> 12:55.640
Okay, we got to the interesting part.

12:55.640 --> 12:57.640
Yeah.

12:57.640 --> 12:59.640
When you talk about transformer explainability,

12:59.640 --> 13:02.640
researchers have looked at this attention matrix

13:02.640 --> 13:04.640
and asked the question,

13:04.640 --> 13:07.640
is this attention matrix an explanation?

13:07.640 --> 13:09.640
How can it be an explanation?

13:09.640 --> 13:12.640
Because we said that the attention values are actually

13:12.640 --> 13:14.640
kind of like relevance values, right?

13:14.640 --> 13:17.640
There are values that reflect how much each token

13:17.640 --> 13:19.640
influences each other's token.

13:19.640 --> 13:22.640
And we also said that the classification token

13:22.640 --> 13:25.640
is the only token that is used for the prediction, right?

13:25.640 --> 13:28.640
So if we look at the row in the attention matrix,

13:28.640 --> 13:30.640
that corresponds to the classification token,

13:30.640 --> 13:32.640
and look at these relevance values,

13:32.640 --> 13:34.640
these should be the relevance values that determine

13:34.640 --> 13:37.640
how much each token influences the classification token,

13:37.640 --> 13:40.640
which is basically how much each token influences

13:40.640 --> 13:42.640
the classification, right?

13:42.640 --> 13:45.640
So maybe these values are just the relevance values.

13:45.640 --> 13:47.640
Each token represents a patch in the image.

13:47.640 --> 13:50.640
Maybe these are just the values that we need.

13:50.640 --> 13:54.640
And we're all done just like decision trees are self-explanable

13:54.640 --> 13:57.640
or linear regression is self-explanable.

13:57.640 --> 13:59.640
What do you think? Are we done?

13:59.640 --> 14:00.640
We're done.

14:00.640 --> 14:02.640
Yeah, we're done.

14:02.640 --> 14:03.640
We're done.

14:03.640 --> 14:04.640
Yeah, we're done.

14:04.640 --> 14:05.640
We're done.

14:05.640 --> 14:06.640
We're done.

14:06.640 --> 14:07.640
We're done.

14:07.640 --> 14:08.640
We're done.

14:08.640 --> 14:09.640
Yeah.

14:09.640 --> 14:10.640
Yeah.

14:10.640 --> 14:12.640
The attention matrix is used to multiply the value representation.

14:12.640 --> 14:13.640
Yeah.

14:13.640 --> 14:15.640
The representation should be positive, negative,

14:15.640 --> 14:16.640
large, small.

14:16.640 --> 14:19.640
It doesn't actually tell us how much it is actually contributing

14:19.640 --> 14:20.640
to the final classification.

14:20.640 --> 14:21.640
Yeah.

14:21.640 --> 14:22.640
I mean,

14:22.640 --> 14:24.640
the two problems that we,

14:24.640 --> 14:26.640
we point out to.

14:27.640 --> 14:30.640
The values can't be negative,

14:30.640 --> 14:33.640
but I don't think really when you say, okay,

14:33.640 --> 14:35.640
let's refer to.

14:35.640 --> 14:39.640
These values are actually directly determining how much

14:39.640 --> 14:41.640
information from each token you're going to take.

14:41.640 --> 14:43.640
And think there's a softmax operation here.

14:43.640 --> 14:46.640
All the values are non-negative, right?

14:46.640 --> 14:49.640
So there is a distribution that's defined on all these tokens

14:49.640 --> 14:51.640
of how much each token is.

14:52.640 --> 14:56.640
So intuitively, these are really relevant values,

14:56.640 --> 15:00.640
but we do have two other issues that we should refer to.

15:00.640 --> 15:03.640
The first one is we said we have a few attention heads, right?

15:03.640 --> 15:06.640
Each tension head has a different meaning.

15:06.640 --> 15:09.640
Some attention heads are really not relevant to the prediction.

15:09.640 --> 15:13.640
How do we aggregate across these attention heads in a way that takes

15:13.640 --> 15:15.640
into account the meaning of each head?

15:15.640 --> 15:18.640
We wouldn't want to take into account heads that do not affect

15:18.640 --> 15:20.640
the final prediction of the model.

15:20.640 --> 15:23.640
And there are such heads since there's research that show that

15:23.640 --> 15:26.640
you can prune most of the heads without impacting the prediction

15:26.640 --> 15:27.640
of the model.

15:27.640 --> 15:29.640
So you have a few attention heads and it isn't,

15:29.640 --> 15:32.640
isn't clear how you aggregate across these attention heads in a

15:32.640 --> 15:35.640
way that takes into account the importance of each head.

15:35.640 --> 15:38.640
And the second question that we have is we refer to the

15:38.640 --> 15:42.640
single attention layer, but we have a few attention layers.

15:42.640 --> 15:45.640
So the first attention layer may incorporate information into

15:45.640 --> 15:47.640
token one from token three.

15:47.640 --> 15:49.640
And then in the second layer,

15:49.640 --> 15:52.640
token one isn't simply the patch that it represented in the

15:52.640 --> 15:53.640
beginning.

15:53.640 --> 15:56.640
It is this patch with information from this patch.

15:56.640 --> 15:57.640
In the second layer,

15:57.640 --> 15:59.640
it's this patch with information from this patch,

15:59.640 --> 16:00.640
and maybe this patch,

16:00.640 --> 16:01.640
and maybe this patch.

16:01.640 --> 16:04.640
And by the end of the attention mechanism,

16:04.640 --> 16:08.640
how do we know which token refers to which input patch, right?

16:08.640 --> 16:09.640
They're all mixed up.

16:09.640 --> 16:12.640
That's the entire idea of the attention.

16:12.640 --> 16:14.640
So we have two issues here.

16:14.640 --> 16:16.640
How do we aggregate across attention heads?

16:16.640 --> 16:19.640
Since we know that they have different means and how do we

16:19.640 --> 16:21.640
aggregate across attention layers?

16:21.640 --> 16:22.640
Yeah.

16:22.640 --> 16:23.640
Just so that I understand.

16:23.640 --> 16:25.640
So if there's only one attention head,

16:25.640 --> 16:27.640
and also there's only one attention layer,

16:27.640 --> 16:29.640
then the relevance board is the attention.

16:29.640 --> 16:30.640
Yeah.

16:30.640 --> 16:31.640
Yeah.

16:31.640 --> 16:32.640
By this hypothesis, yes.

16:32.640 --> 16:33.640
Okay.

16:33.640 --> 16:34.640
Yes.

16:34.640 --> 16:40.640
And then I think there are some models that use this for visual

16:40.640 --> 16:43.640
question answering and actually did that visualization.

16:43.640 --> 16:44.640
And it works.

16:44.640 --> 16:45.640
Pretty well.

16:45.640 --> 16:48.640
So assuming you have one attention head and one attention layer,

16:48.640 --> 16:49.640
it should be.

16:49.640 --> 16:50.640
Fingers crossed.

16:50.640 --> 16:52.640
It should be their elements.

16:52.640 --> 16:54.640
I haven't tried that, but, but yeah.

16:54.640 --> 16:56.640
By this intuition.

16:56.640 --> 17:01.640
So the attention all of mechanism is actually the first method to

17:01.640 --> 17:03.640
explain transfer was that came out.

17:03.640 --> 17:04.640
In 2020.

17:04.640 --> 17:05.640
And they add,

17:05.640 --> 17:08.640
they propose that the two simplest solutions,

17:08.640 --> 17:11.640
maybe that we can think of to solve those three issues.

17:11.640 --> 17:13.640
Head aggregation by averaging.

17:13.640 --> 17:14.640
And again,

17:14.640 --> 17:16.640
remember we said different meanings to different heads.

17:16.640 --> 17:18.640
So that's maybe over simplistic.

17:18.640 --> 17:21.640
And they're aggregation by matrix multiplication.

17:21.640 --> 17:22.640
And if you think about it,

17:22.640 --> 17:25.640
matrix multiplication from the end to the beginning,

17:25.640 --> 17:27.640
kind of unravels things.

17:27.640 --> 17:31.640
The connections between the two that were made by the attention

17:31.640 --> 17:32.640
mechanism.

17:32.640 --> 17:38.640
They also propose another method called attention flow,

17:38.640 --> 17:43.640
which evaluates the flow values in the attention graph,

17:43.640 --> 17:46.640
like a classic flow problem from algorithms,

17:46.640 --> 17:48.640
but it's too computationally expensive for images.

17:48.640 --> 17:50.640
So we're not really going to get into it.

17:51.640 --> 17:54.640
So getting into the first method we propose,

17:54.640 --> 17:57.640
what we were saying is that the assumptions made by the attention

17:57.640 --> 17:59.640
role of mechanism were solid,

17:59.640 --> 18:00.640
but maybe over simplistic.

18:00.640 --> 18:01.640
Yeah.

18:01.640 --> 18:02.640
Question.

18:02.640 --> 18:04.640
You may have some questions.

18:04.640 --> 18:08.640
Oh, yeah.

18:08.640 --> 18:09.640
We say in Hebrew.

18:09.640 --> 18:10.640
Oh,

18:10.640 --> 18:21.640
right.

18:21.640 --> 18:26.640
I may take those at the end of the talk just because otherwise

18:26.640 --> 18:28.640
we won't be able to finish with time.

18:28.640 --> 18:31.640
Yeah.

18:31.640 --> 18:32.640
Yeah.

18:32.640 --> 18:35.640
So getting back to the first time we're going to propose.

18:35.640 --> 18:38.640
We were saying that the assumptions made by attention roll off were

18:38.640 --> 18:40.640
nice and worked in some cases,

18:40.640 --> 18:42.640
but are maybe a bit simplistic.

18:42.640 --> 18:45.640
We want to be able to average across the heads in a way that

18:45.640 --> 18:47.640
actually takes into account the meaning of each other.

18:47.640 --> 18:50.640
So what we're going to do is we're going to use a signal that is

18:50.640 --> 18:52.640
very useful in explainability in general,

18:52.640 --> 18:54.640
which is radiance, right?

18:54.640 --> 18:58.640
Because radiance intuitively mean if I change this a bit,

18:58.640 --> 19:00.640
how does this change a bit, right?

19:00.640 --> 19:04.640
So if we take the gradients with regards to the output of the model,

19:04.640 --> 19:07.640
which is over here, the gradients of the attention matter.

19:08.640 --> 19:12.640
We can use the gradients as weights for the attention maps.

19:12.640 --> 19:15.640
So instead of just averaging across the maps,

19:15.640 --> 19:19.640
we take the gradient, the gradient gives us the weights element.

19:19.640 --> 19:23.640
And we multiply the gradients by the attention and then each head

19:23.640 --> 19:25.640
gets a weight from the gradient.

19:25.640 --> 19:29.640
And then each attention head is not just the simple attention head

19:29.640 --> 19:30.640
that it was in the beginning.

19:30.640 --> 19:33.640
It is the attention weighted by the gradient.

19:33.640 --> 19:36.640
And then we can average across the heads in a way that takes into

19:36.640 --> 19:38.640
account the meaning of each head.

19:38.640 --> 19:40.640
So this is why the gradients are here.

19:40.640 --> 19:44.640
But we have another component that I won't get too deeply into

19:44.640 --> 19:48.640
because it was removed for our second method.

19:48.640 --> 19:52.640
It is the LRP component, layer-wise relevance propagation.

19:52.640 --> 19:56.640
The second thing we thought of was that we actually reduce the

19:56.640 --> 19:59.640
entire transformer architecture to just a multiplication

19:59.640 --> 20:00.640
of queries and keys.

20:00.640 --> 20:03.640
So it's not even the entire attention mechanism because we also

20:03.640 --> 20:05.640
had the values there, remember?

20:05.640 --> 20:08.640
So we narrowed down this entire, not so complex,

20:08.640 --> 20:10.640
but architecture, right?

20:10.640 --> 20:11.640
It has activations.

20:11.640 --> 20:13.640
It has linear projections.

20:13.640 --> 20:16.640
We narrowed all down to the multiplication between queries

20:16.640 --> 20:17.640
and keys.

20:17.640 --> 20:20.640
So we do want to take into account the other layers of the

20:20.640 --> 20:24.640
transformer and how they impact the calculations.

20:24.640 --> 20:28.640
So instead of just taking, let's get back to the whiteboard here,

20:28.640 --> 20:31.640
instead of just taking the attention map,

20:32.640 --> 20:35.640
that is quite, it's simplistic, right?

20:35.640 --> 20:38.640
It's not that, but the multiplication between queries and keys,

20:38.640 --> 20:41.640
we want to take into account a different attention map,

20:41.640 --> 20:46.640
we'll call it RA, which takes into account the other layers

20:46.640 --> 20:48.640
of the transformer architecture.

20:48.640 --> 20:51.640
So instead of taking just these raw relevance values,

20:51.640 --> 20:55.640
we take relevance values calculated by LRP.

20:55.640 --> 20:59.640
And LRP is a mechanism that does back propagation with

20:59.640 --> 21:02.640
gradients from the end of the network all the way to the beginning.

21:02.640 --> 21:06.640
And it can give us relevant values for specifically this attention

21:06.640 --> 21:07.640
matrix.

21:07.640 --> 21:10.640
So instead of taking into account the attention values,

21:10.640 --> 21:13.640
the raw attention values, we take into account the relevance values

21:13.640 --> 21:16.640
of the attention matrix.

21:16.640 --> 21:19.640
And as I said, I won't get too deep into it because we actually

21:19.640 --> 21:21.640
removed it in our second method,

21:21.640 --> 21:25.640
which is the one that I want to get into in more details.

21:25.640 --> 21:31.640
So we have the attention gradients to average across the heads.

21:31.640 --> 21:34.640
And we have the relevance in order to account for all the other layers

21:34.640 --> 21:36.640
in the transformer.

21:36.640 --> 21:38.640
So this is how we average across the heads.

21:38.640 --> 21:41.640
And the way that we average across the layers is by matrix multiplication.

21:41.640 --> 21:46.640
Here we adopted the interpretation from attention robot.

21:47.640 --> 21:56.640
Oh, no, not element wise, actual matrix multiplication.

21:56.640 --> 21:58.640
The matrices are squared matrices.

21:58.640 --> 22:02.640
Yeah, because they are self attention matrices so you can actually

22:02.640 --> 22:03.640
multiply them.

22:03.640 --> 22:06.640
And if you think about it, you can unravel it when multiplying two

22:06.640 --> 22:07.640
attention matrices.

22:07.640 --> 22:11.640
It actually says, if the previous layer gave token one information

22:11.640 --> 22:14.640
from token three, and this layer gives token one information from

22:14.640 --> 22:19.640
token four, then it unravels both operations to ensure that you

22:19.640 --> 22:25.640
actually take into account all the context.

22:25.640 --> 22:29.640
Yeah, so this is just a rewind of what we saw in the previous slide.

22:29.640 --> 22:31.640
How do we average across heads?

22:31.640 --> 22:33.640
We take the gradients as weights.

22:33.640 --> 22:35.640
We take the relevance instead of the pure attention weights.

22:35.640 --> 22:37.640
And then we do averaging.

22:37.640 --> 22:39.640
But here the average is not just the raw average.

22:39.640 --> 22:44.640
If we had before it is weighted by the gradients.

22:44.640 --> 22:50.640
And here you can see a few examples of how our method works.

22:50.640 --> 22:53.640
So by the way, this is a slide that was added, but we don't have the

22:53.640 --> 22:54.640
updated slide.

22:54.640 --> 22:58.640
So let's just see what we get in the end of this calculation.

22:58.640 --> 23:04.640
So

23:04.640 --> 23:09.640
at the end of this calculation, we had an attention matrix, which

23:09.640 --> 23:12.640
is the attention matrix after by the averaging and everything.

23:12.640 --> 23:17.640
We have attention matrices for all the layers.

23:17.640 --> 23:25.640
And then we multiply these.

23:25.640 --> 23:31.640
So really, we have one attention matrix that takes into account all

23:31.640 --> 23:34.640
the layers and all the heads.

23:34.640 --> 23:39.640
And now we can get back to, can we go back in the slides?

23:39.640 --> 23:42.640
Oh, no, it's only going forward.

23:42.640 --> 23:43.640
Yeah.

23:43.640 --> 23:46.640
Maybe there's an arrow at the bottom left corner.

23:46.640 --> 23:48.640
If you move your mouse.

23:48.640 --> 23:51.640
Oh, the back arrow.

23:51.640 --> 23:55.640
Oh, it's the other way around.

23:55.640 --> 23:58.640
Yeah.

23:58.640 --> 24:01.640
And now we're actually getting to the point that Sharon made that right

24:01.640 --> 24:06.640
now we only after all the aggregations that we made, we have one aggregated

24:06.640 --> 24:10.640
attention matrix for the entire network because we aggregate across heads

24:10.640 --> 24:12.640
and then we aggregate across layers.

24:12.640 --> 24:16.640
And once we have that one attention matrix for the entire, for the entire

24:16.640 --> 24:21.640
network, then we can use that intuition that we had that the row corresponding

24:21.640 --> 24:24.640
to the classification token is actually the explanation.

24:24.640 --> 24:26.640
So this is how we extract the final explanation.

24:26.640 --> 24:30.640
And then we're actually the relevance values that we use.

24:30.640 --> 24:35.640
And the one the other way around, right.

24:35.640 --> 24:38.640
Okay.

24:38.640 --> 24:42.640
So as you can see here, we have comparisons between our method and other methods

24:42.640 --> 24:47.640
that are either adapted from CNNs, or methods that were constructed for

24:47.640 --> 24:49.640
transformers such as rollout.

24:49.640 --> 24:53.640
So as you can see here, rollout tends to have a lot of noise in the background

24:53.640 --> 24:57.640
and we think about it intuitively as resulting from the fact that they just

24:57.640 --> 25:01.640
average across the heads and not take into account the meaning of each head.

25:01.640 --> 25:06.640
And some methods such as partial LRP fail on some cases, but in these

25:06.640 --> 25:09.640
specific cases, they actually do pretty well.

25:09.640 --> 25:13.640
But I do want to point out that they do not distinct between classes.

25:13.640 --> 25:18.640
So for example, if we have an elephant and a zebra in an image, our method is able

25:18.640 --> 25:22.640
to produce explanations specifically for the elephant or specifically for the zebra.

25:22.640 --> 25:27.640
And when we don't do that, and we want to use this method say partial LRP to

25:27.640 --> 25:32.640
explain predictions by the model, it will be hard to do that because if you want

25:32.640 --> 25:37.640
to explain the elephant prediction, we may have results coming in from other classes.

25:37.640 --> 25:41.640
So we're not really sure if the things that we're seeing highlighted are highlighted

25:41.640 --> 25:46.640
because of the elephant or because of other classes making their way into the

25:46.640 --> 25:47.640
explanation.

25:47.640 --> 25:51.640
So I think personally, class specific explanations are really important to ensure

25:51.640 --> 25:54.640
that we're really explaining the specific prediction of the model.

25:54.640 --> 25:55.640
We're good.

26:04.640 --> 26:05.640
That's a fantastic question.

26:05.640 --> 26:07.640
That's a fantastic question.

26:07.640 --> 26:11.640
Usually people from explainability evaluate explanations differently than what you

26:11.640 --> 26:13.640
as end users may have to be.

26:13.640 --> 26:17.640
So what we do is we use erasure based methods.

26:17.640 --> 26:21.640
So what we do is we take the pixels that are said to be important to buy with it.

26:21.640 --> 26:25.640
We take them out and we see if the model changes its prediction or not.

26:25.640 --> 26:27.640
And similarly, we do the other way around.

26:27.640 --> 26:31.640
We take the pixels that are unimportant by benefit and take them out and see that

26:31.640 --> 26:33.640
the model still predicts the same.

26:33.640 --> 26:37.640
You have to take into account when you can see that the model still predicts the

26:37.640 --> 26:38.640
same.

26:38.640 --> 26:42.640
You have to take into account when you do that, that you create images that are

26:42.640 --> 26:45.640
out of the distribution of the model was trained on.

26:45.640 --> 26:48.640
So this method is not really, you know, airtight.

26:48.640 --> 26:51.640
And there's a lot of research around how do we value it.

26:51.640 --> 26:55.640
And how do we know if the explanation is really good or not.

26:55.640 --> 26:57.640
Any other questions.

26:57.640 --> 26:58.640
Yeah.

26:59.640 --> 27:10.640
Yeah.

27:10.640 --> 27:11.640
Yeah.

27:11.640 --> 27:12.640
Yeah.

27:12.640 --> 27:13.640
Yeah.

27:13.640 --> 27:14.640
Yeah.

27:14.640 --> 27:15.640
Yeah.

27:15.640 --> 27:16.640
Yeah.

27:16.640 --> 27:20.640
Um, just because we want to have a measuring stick that actually measures the

27:20.640 --> 27:24.640
explainability without relation to the algorithm itself.

27:24.640 --> 27:28.640
So the measure should be unrelated to whether it's a transformer or CNN.

27:28.640 --> 27:32.640
It should be unified throughout all the different architectures, right?

27:32.640 --> 27:36.640
Just as you use accuracy to measure CNNs or transformers or whatever

27:36.640 --> 27:37.640
architecture you use.

27:37.640 --> 27:41.640
You want to have a measuring stick that really measures the method and not

27:41.640 --> 27:44.640
something that has something to do with specifically.

27:45.640 --> 27:48.640
If you have an explanation for CNN, it also has, you know,

27:48.640 --> 27:50.640
values for each pixel.

27:50.640 --> 27:51.640
Yeah.

27:51.640 --> 27:52.640
Yeah.

27:52.640 --> 27:53.640
Just zero it out.

27:53.640 --> 27:54.640
You just zero it out.

27:54.640 --> 27:56.640
And it works on the input itself.

27:56.640 --> 27:57.640
So it really,

27:57.640 --> 27:59.640
it is undependent even of.

27:59.640 --> 28:00.640
You know,

28:00.640 --> 28:01.640
you know,

28:01.640 --> 28:02.640
you know,

28:02.640 --> 28:03.640
you know,

28:03.640 --> 28:04.640
you know,

28:04.640 --> 28:05.640
you know,

28:05.640 --> 28:06.640
you know,

28:06.640 --> 28:07.640
you know,

28:07.640 --> 28:08.640
you know,

28:08.640 --> 28:09.640
you know,

28:09.640 --> 28:10.640
you know,

28:10.640 --> 28:11.640
you know,

28:11.640 --> 28:12.640
you know,

28:12.640 --> 28:15.640
it really it is independent even of the method you use or the model.

28:15.640 --> 28:37.640
It is a measuring stick that has nothing to do with which method you use

28:37.640 --> 28:40.640
for expansion or expansion in which version.

28:40.640 --> 28:49.840
the sun. I'm not sure I got your question exactly, but I would say that there are methods evaluating

28:49.840 --> 28:55.440
explanations by adding sparse correlation, making sure that the model reaches 100 percent accuracy

28:55.440 --> 29:00.400
due to the sparse correlations, and then making sure that the explanation outputs these sparse

29:00.400 --> 29:06.240
correlations versus the odd correlation. So there are methods that do that, but yeah. But we usually,

29:06.240 --> 29:11.280
I usually use erasers as methods to evaluate explanations, but this is a really active

29:11.280 --> 29:16.640
goal of research, right? So it's not really, you know, obvious how we evaluate explanations

29:16.640 --> 29:24.880
and what's the right way to do that. I think I'm maybe moving backwards instead of forward.

29:26.000 --> 29:32.160
Some technical issues. Yeah, okay. I may just skip this because we do have the motivation

29:32.160 --> 29:37.600
that we did in the beginning and we're a bit behind on time. So our second method

29:38.560 --> 29:44.080
said, you know what? We really believe that multimodal models are going to be a big thing.

29:44.960 --> 29:49.520
And we only explained self-attention before, as you saw. We didn't go into

29:49.520 --> 29:56.320
cross-attention or encoded encoded attention. And assuming that most transformers don't just do

29:56.320 --> 29:59.520
right self-attention, we need a mechanism that can explain

30:00.160 --> 30:04.080
cross-attention and encoded encoded attention as well, not just self-attention.

30:04.080 --> 30:09.600
So the second paper actually expands the first paper, but for other types of attention.

30:12.080 --> 30:16.560
So the first thing we do is get rid of the LLP, and that's why I don't, you know, get into a lot

30:16.560 --> 30:21.360
of detail with regards to the LLP. The reason that we did that is because if you think about it,

30:21.360 --> 30:25.920
we use LLP in order to account for all the layers, but really gradients account for all the layers

30:25.920 --> 30:30.080
because backpropagation is backpropagated from the output all the way back together.

30:30.800 --> 30:35.360
So we said, what happens if we remove LLP, which makes it easier for you guys to implement

30:35.360 --> 30:41.200
the algorithm, and it makes it faster and more convenient, and it actually works pretty well.

30:41.200 --> 30:46.640
So we remove the LLP component. I will say that if you want really accurate explanations,

30:46.640 --> 30:53.200
usually I would go for the LLP version, right? Because LLP adds this added component that doesn't

30:53.200 --> 30:57.680
exist without LLP. It does account for all the layers quite systematically.

30:59.120 --> 31:04.480
So when we talk about cross-model interactions, we have four types of interactions in such models.

31:04.480 --> 31:09.120
We have the self-attention interactions between the text tokens, how the text tokens influence

31:09.120 --> 31:16.160
themselves, the self-attention interactions between the image tokens, and then two types of cross-attention

31:16.160 --> 31:20.160
interactions, how text influences image and how image influences.

31:23.920 --> 31:30.160
And then what we thought we would do is really track the self-attention layers.

31:30.160 --> 31:35.760
So each self-attention layer mixes tokens. Okay, we'll mix the tokens in the relevance matrices.

31:36.400 --> 31:42.000
So we start with an initialization of the relevance matrices at the beginning of the

31:42.000 --> 31:47.360
modalities or self-contained. So images only affect images and text only affects text,

31:47.360 --> 31:52.320
and each image token only influences itself. So the initialization for the self-attention

31:52.320 --> 31:58.240
relations are just the identity matrices. And for the cross-model relations, it's a zero matrix,

31:58.240 --> 32:03.440
because there are no cross-model interactions before we do any attention. And what the method

32:03.440 --> 32:09.040
really does is it just goes on a forward pass through the attention layers, and as the attention

32:09.040 --> 32:15.840
layers mix the tokens, the relevance values are mixed as well, just tracking the attention as it goes.

32:17.600 --> 32:22.880
So I won't get into all the rules, all the rules that we have for all these specific attention

32:22.880 --> 32:26.640
layers. I'm just giving you a motivation of how it works. And really, believe me, it's really

32:26.640 --> 32:33.040
simple, even though the equations look complicated. So let's go over just the self-attention rule.

32:33.760 --> 32:40.720
A self-attention layer has, again, multiple heads. We average across the heads using gradients just as

32:40.720 --> 32:48.320
before. So we have now a single attention matrix marked here as a bar. And what we do again is just

32:48.320 --> 32:55.360
matrix multiplication between the current attention mixture and the old attention mixture that existed

32:55.360 --> 33:02.320
in the relevance matrix. So matrix multiplication and update the relevance matrix. This is all we

33:02.320 --> 33:06.960
do. We just track the attention as it goes. As it mixes between tokens, we mix between the

33:06.960 --> 33:12.160
relevance values. That's what we do. That's the entire algorithm. And head aggregation is done

33:12.800 --> 33:20.480
via gradients as before. So taking a look at some examples that we have to demonstrate how this works.

33:21.040 --> 33:26.960
For example, for CLIP, you can see that we've entered different texts with the same input image

33:26.960 --> 33:32.880
and propagated gradients. And by the way, for CLIP, gradients are propagated. Let's take them back

33:32.880 --> 33:39.440
to the whiteboard. For CLIP, because I know this is specifically interesting to you,

33:40.480 --> 33:46.960
let's talk about how we propagate relevance for CLIP. For CLIP, you have an imaging quarter

33:48.640 --> 33:53.120
and then a texting quarter. Both of them, by the way, use pure self-attention. So there's no cross

33:53.200 --> 34:01.440
connection. This and this output representation and vector, which is by the way from the

34:01.440 --> 34:06.720
classification. So this is the vector for the text and this is the vector for the image.

34:07.600 --> 34:16.640
And the stimuli score is just a dot product. Both scores. So what we do is we propagate

34:16.640 --> 34:23.360
gradients from this dot product back to the texting quarter

34:25.280 --> 34:29.360
and back to the imaging quarter. And those gradients are going to be used to average

34:29.360 --> 34:33.520
across the attention pens as we saw before. And then the attention heads are going to be

34:33.520 --> 34:39.280
aggregated across different letters by matrix multiplication. So here we don't have an output

34:39.280 --> 34:45.760
logic as we have for the classification, but we use this dot product between their presentations

34:45.760 --> 34:51.520
to calculate the score that we propagate the gradients with regards to. So all that we do

34:51.520 --> 34:56.560
here is really simple. Calculate the dot product between their presentations, propagate gradients

34:56.560 --> 35:01.920
with regards to the dot product. Those gradients are going to be used as weights for the attention

35:01.920 --> 35:10.080
matrices to average across them. And as you can see, the results are text specific since we

35:10.080 --> 35:14.480
propagated the gradients with regards to the specific multiplication between the specific text

35:14.480 --> 35:18.880
and the specific image. So actually for an elephant, you can see that the hidden map

35:18.880 --> 35:22.240
corresponds to the elephant. For a zebra, the hidden map corresponds to the zebra. And for a

35:22.240 --> 35:25.680
leg, the hidden map corresponds to the leg, showing us that the model really knows how to

35:25.680 --> 35:30.240
distinct between different parts or different objects in the image according to the text input

35:30.240 --> 35:38.240
that we give it. This is an example that we saw before. And visual question answering in case

35:38.240 --> 35:43.360
any one of you is interested is actually an interesting use case. Because for visual question

35:43.360 --> 35:47.440
answering, the model is given an image and a question, and it's supposed to answer the question

35:47.440 --> 35:52.400
based on the image. And researchers have shown that when you actually lack out the entire image

35:52.400 --> 35:59.440
and just give the model the question, it answers the question about 30% of design correctly.

36:00.080 --> 36:04.720
So the question here is assuming that the model answers the question without seeing the image.

36:06.400 --> 36:11.520
How do we measure the accuracy of such models? So you can use explainability to ensure that

36:11.520 --> 36:16.080
the model actually used the image and the correct parts of the image to make the prediction.

36:16.080 --> 36:21.520
For example here, the question is, did he catch a ball? We see that the player actually caught the

36:21.520 --> 36:27.280
ball. And the answer is yes, but we also see that the model focused on the right parts of the image.

36:27.280 --> 36:31.040
So it can really tell that the model made the prediction based on the image and not just the

36:31.040 --> 36:41.280
question. I'm going to skip this part too. Yay. So we're switching gears. We're going to talk about

36:41.600 --> 36:45.920
our method to improve model robustness using explainability. So if you have any questions

36:45.920 --> 36:51.120
about the previous part on explaining transformers, this is the time to ask them.

36:52.960 --> 36:56.960
No, no questions. Oh, I had a couple of questions in the chat. Yeah.

36:58.800 --> 37:03.600
I'm sorry about that. There is no one really, you know, maintaining the chat.

37:04.400 --> 37:08.640
Yeah, let's make it brief and then try to answer questions. Yeah.

37:09.520 --> 37:13.280
Oh, okay. I was just wondering, why does it make sense to only look at the

37:15.760 --> 37:21.120
attention maps outputted by the softmax? Because don't we have, don't we multiply by an output

37:21.120 --> 37:29.120
matrix then that is able to shuffle across tokens afterwards? Do you mean the values matrix? No,

37:29.120 --> 37:37.520
the output matrix. I guess that the intuition is just that the self attention mechanism,

37:37.520 --> 37:43.600
its purpose is to contextualize in the way that the contextualization is made by the attention

37:43.600 --> 37:49.840
values. So the attention value is actually, you know, determine how much each token is going

37:49.840 --> 37:57.040
to be incorporated into the other tokens. We do have an additional output matrix and you mean

37:57.040 --> 38:02.480
after the attention mechanism, right? Yes, yes. Yeah, okay. So some researchers have actually

38:02.480 --> 38:08.080
used that output, if I'm not mistaken, it was that output, the norm of the output matrix in order

38:08.080 --> 38:16.000
to average across the different heads to account for each head's meaning in the attention matrix,

38:16.560 --> 38:23.440
in the attention mechanism. But, you know, just, you know, very naively thinking the attention

38:24.640 --> 38:29.360
really mixes the tokens using the values determined by the attention matrix. So it's

38:29.360 --> 38:35.760
really a naive intuitive outlook on the attention mechanism. And the output matrix that you're

38:35.760 --> 38:43.360
referring to is I view it as a weights matrix, which will weight each layer since not all layers

38:43.360 --> 38:47.680
influence the prediction the same, right? We know that usually the last attention layer

38:47.680 --> 38:51.040
is the most influential or the previous attention layers are not that impactful.

38:51.920 --> 38:58.720
So I view it as the output matrix kind of reweighting the result from the attention mechanism.

38:59.360 --> 39:04.640
But all that we're saying right now are just intuitions, right? We've seen empirically that the

39:04.640 --> 39:09.840
attention matrix is quite indicative of what the model learns to do, how it learns to contextualize

39:10.720 --> 39:15.120
parts of the input. It's not necessarily the best thing to do, the smartest thing to do or the

39:15.120 --> 39:20.880
most correct thing to do. It's just what empirically worked well. And it has an intuition basis as

39:20.880 --> 39:27.120
explained before. I hope that answers your question. It does. I had one other question if there's time.

39:29.920 --> 39:35.360
We're really tight on time. I mean, we have 13 minutes. So maybe we'll take that offline.

39:36.160 --> 39:39.440
Sure. Thank you. Sorry for that. I really apologize.

39:42.240 --> 39:49.360
Okay. So when we talk about VIT models, the image is split into patches. The patches go

39:49.360 --> 39:54.240
through linear projections. And then a transformer encoder and vanilla transformer encoder is used

39:54.240 --> 39:58.960
to make the prediction again with the classification code. So really a simple and clean architecture.

40:00.000 --> 40:07.600
And usually those models are trained using ImageNet. ImageNet is a classification data set.

40:08.320 --> 40:11.680
And what those classification data sets do is actually they train the model

40:11.680 --> 40:16.560
to make a prediction. So they train the model to see an image and make the prediction that this

40:16.560 --> 40:21.680
is a car. But it doesn't do anything beyond that, right? The model should predict that it's a car,

40:21.680 --> 40:26.880
but it doesn't have to have an understanding of what a car constructs and how a car looks.

40:27.600 --> 40:33.520
It should just see this image of a car and output car. We don't enforce anything too

40:33.520 --> 40:39.680
smart that the model should learn. So what researchers have noticed a long time ago

40:39.680 --> 40:44.480
is that ImageNet contains sparse predictions. What it means is that, for example,

40:44.480 --> 40:51.600
cows usually appear on the background of green grass. So a reasonable inference that the model

40:51.600 --> 40:56.240
can make. Really reasonable, right? Because this is the statistics of the data in the data set that

40:56.240 --> 41:02.080
it gets. It's to learn that green grass is actually a cow. And now we learn to predict

41:02.080 --> 41:07.760
that this image is an image of a cow based on the green grass, not really the object in the image.

41:10.000 --> 41:23.360
What it causes is, oh, can you mute this? Thank you. So what it causes is cases where

41:23.360 --> 41:28.480
the distribution is likely shifted from ImageNet. And in cases where we would actually expect the

41:28.480 --> 41:34.080
model to really work well on. The model really doesn't. And the accuracy plunge, we're talking

41:34.080 --> 41:41.040
about 90% to 30% sometimes and even less. So really cases where we would expect the model to still

41:41.040 --> 41:48.560
learn to make smart and great prediction, but it really does. It predicts based on the sparse

41:48.560 --> 41:52.720
predictions that it learned from ImageNet and they don't apply to other predictions. So for example,

41:52.720 --> 41:57.200
we have the golf ball in the lemon here and we have another orange that is classified as a maze

41:57.200 --> 42:02.560
due to the carpet in the bathroom, right? Because it kind of looks like a maze. And a school bus

42:02.560 --> 42:07.680
here that is classified as a snowplow because of the presence of snow. So we can imagine that the

42:07.680 --> 42:12.720
model learns some kind of sparse correlation here, such as vehicle plus snow equals snowplow.

42:13.600 --> 42:23.120
Okay. So we want to solve these issues, but without training the models with, you know,

42:23.120 --> 42:28.320
a stronger queue, it is really hard to do that because we just teach the model based on some

42:28.320 --> 42:33.600
data set that we have, which is ImageNet. It is, you know, the most used data set to predict,

42:34.160 --> 42:42.480
to train object detection, object recognition. And we have no way of really controlling

42:42.480 --> 42:48.400
what the model learns. And intuitively, training the explainability signal is really teaching

42:48.400 --> 42:53.280
the model not just what is in the image, but why this is the object in the image. So we would want

42:53.280 --> 42:59.680
to apply a last term directly to the explanations of the model to teach it why this prediction is

42:59.680 --> 43:07.520
correct. So here you see some sparse correlations that the model uses. So for example, here the

43:07.520 --> 43:13.760
model classifies the images of chestnut with a confidence of 100% based on just the background

43:13.760 --> 43:19.760
pixels, not even one photo. And here a very sparse consideration of the zebra gives us a

43:19.760 --> 43:25.040
confidence of 99.9% that this is a zebra. So really behavior that we would really want to discourage.

43:28.080 --> 43:33.760
Since the second method that we saw is based on pure gradients, everything there is derivable.

43:33.760 --> 43:37.280
The gradients can be derived again, and the last term can be applied directly to the

43:37.280 --> 43:43.200
explainability. And we can force the model to make the prediction based on the program instead of

43:43.200 --> 43:50.960
the background image pixels. The issue that we had after that is, you know, we're researchers at

43:50.960 --> 43:58.640
the university, right? We don't have the resources to train VIT large or huge from scratch. So we

43:58.640 --> 44:04.960
need to come up with a method that is efficient in time and space and not too complicated. So what

44:04.960 --> 44:10.880
we opted to do is fine tune an existing model. So we would fine tune the model. It works pretty

44:10.880 --> 44:15.760
well on ImageNet, right? We don't want to change the prediction that it gives on ImageNet. We just

44:15.760 --> 44:21.920
want to change the reasoning that it gives to the prediction. So we fine tune the models with only

44:21.920 --> 44:27.920
three examples per class, really not that many examples for just 500 classes. So just half the

44:27.920 --> 44:33.200
classes in ImageNet to change the relevance maps to focus on the foreground instead of the background.

44:35.600 --> 44:40.960
So we identified two science issues with VIT models. The first one is an over interpretation

44:40.960 --> 44:45.200
of the background, which we saw on your clients. And the second one is a sparse consideration of

44:45.200 --> 44:53.040
the program. The first idea was to fine tune the explanation maps to just be segmentation maps,

44:53.120 --> 45:00.080
like this. This is actually an example of me fine tuning a VIT based model to make the relevance

45:00.080 --> 45:06.720
maps resemble or be identical to segmentation maps. So as you can see before the explanations weren't

45:06.720 --> 45:12.960
really segmentation maps and after they're quite well segmented in the image. So can anyone guess

45:12.960 --> 45:18.480
why that's not an optimal solution to the problem that we have just creating segmentation maps?

45:19.360 --> 45:26.640
People from Zoom can guess too. Why wouldn't we want the model to output relevance maps that are

45:26.640 --> 45:34.560
actually segmentation maps? Let's have a thought experiment today. I'm going to draw with my

45:34.560 --> 45:41.120
magnificent drawing skills and objects and you're going to try to identify which animal this is,

45:41.120 --> 45:49.280
right? Again, I'm not the best drawing. Which animal is this? Which snake?

45:51.200 --> 46:00.480
Nail. Oh, no, this snake. Which snake is this? Cobra. Yeah, why cobra? Because of the head, right?

46:01.200 --> 46:06.880
And humans, we don't classify cobra as a cobra because of its tail, right? We look at the head

46:06.880 --> 46:14.400
pixels or the head featured and determine that this is a cobra. So we don't, as humans, give

46:14.400 --> 46:19.840
identical relevance to all the pixels in the image. What we do here when we fine tune the

46:19.840 --> 46:24.560
experiment maps to be segmentation maps, we force the model to look equally at all the pixels of

46:24.560 --> 46:30.480
the cobra. We do want to give the model the opportunity to give some relevance to pixels

46:30.480 --> 46:39.360
that is higher than other pixels. So this is too harsh. And we need to have a refined version of it.

46:41.200 --> 46:47.200
This is why we split the loss into two different losses. One is a background loss and one is

46:47.200 --> 46:53.200
foreground loss. The background loss is a mean squared error loss, encouraging the relevance

46:53.200 --> 46:57.360
on the background to be close to zero. And we're using segmentation maps here,

46:57.360 --> 47:03.120
S is the segmentation map of the image. And the foreground loss encourages the foreground

47:03.120 --> 47:10.240
of the image to be closer to one. By splitting into two loss terms, we can give different

47:10.240 --> 47:15.520
values or different coefficients to each of the loss terms. So the background loss is going to get

47:15.520 --> 47:20.320
a relatively high coefficient too, because we don't want a lot of relevance on the background.

47:20.320 --> 47:25.360
By the way, we're not striving to completely eliminate the background, the relevance on the

47:25.360 --> 47:29.440
foreground. Just make sure that the relevance of the background is lower than the relevance of the

47:29.440 --> 47:35.200
foreground. And the foreground loss is going to get a relatively low coefficient. We would want to

47:35.200 --> 47:40.720
encourage the model to look more at more pixels of the foreground, but we wouldn't want to make

47:40.720 --> 47:43.520
the model look at all the pixels in the foreground equally.

47:46.240 --> 47:52.080
We do also have a classification loss, which ensures that the new prediction by the model

47:52.160 --> 47:56.960
or the new distribution is similar to the all distribution by the model. Just to make sure

47:56.960 --> 48:02.240
that the model doesn't forget how to classify images. And again, the model does a pretty good

48:02.240 --> 48:06.080
job on ImageNet. So we don't want to change the prediction by the model. We just want to change

48:06.080 --> 48:16.720
the reasoning. So the giant tables of results here are comparisons between the accuracy of the

48:16.720 --> 48:22.880
model before and after a cartooning process. And as you can see here, it's quite tiny, but I hope

48:22.880 --> 48:28.640
you can see it still. For the ImageNet validation set, we're experiencing a bit of a decrease in

48:28.640 --> 48:32.720
performance. This is because the model relied on spurious cues, and now we're taking them away

48:32.720 --> 48:38.080
from it. And so the spurious cues that previously helped the model reach very, very high accuracy

48:38.080 --> 48:44.800
and overfit are now taking away. But the decrease in accuracy on average across seven models is

48:44.800 --> 48:50.720
not that big. I mean, it's less than 1%. And when you take into account other shifted distributions,

48:50.720 --> 49:00.000
such as ImageNet A, ImageNet R, Sketch, ImageNet ObjectNet, and SIScores, you can see that there

49:00.000 --> 49:05.840
is a pretty big or significant increase in accuracy. For ImageNet A, for example, plus

49:06.640 --> 49:14.320
5.8% in top one accuracy plus 7.8% in top five accuracy. So really a slight decrease

49:14.320 --> 49:18.320
in the accuracy on the data set that the model was originally trained on and

49:18.320 --> 49:23.040
a significant increase in accuracy for distribution shifts, as we would expect.

49:23.680 --> 49:26.640
So to train it, you have to know the program. What is the program?

49:27.440 --> 49:33.520
Yeah, you have to know that. You have segmentation maps. You have segmentation maps. And we do

49:33.520 --> 49:39.680
experiment with two types of segmentation maps. One is manually human, manually tagged by humans.

49:39.680 --> 49:45.680
And the second one is by token cut, which is a version that uses dyno. This is in case you're

49:45.680 --> 49:53.040
training with non-ImageNet data sets and you don't want to manually tag. Even if you do manually

49:53.040 --> 49:59.280
tag, I mean, we use three examples for half the classes. So it's not that many examples to tell,

49:59.280 --> 50:04.160
but we do provide for an option for ad supervised segmentation. Yeah.

50:04.320 --> 50:10.320
So I think that's really cool. The one thing about this, why not just do segmentation?

50:11.200 --> 50:14.640
You can just train a segmentation system. Is that kind of naturally explainable?

50:16.640 --> 50:21.040
That's an excellent question. Do models that were trained on segmentation

50:22.160 --> 50:27.200
have that, you know, brief pass on Spurs correlation? Do they get that in her?

50:28.160 --> 50:34.000
What we thought about or I thought about in that context is you can think about a model

50:34.000 --> 50:39.920
that learns to classify using Spurs correlation and then identify the object using edge detection.

50:40.880 --> 50:46.960
So just because you learn to identify an object does not mean or learn to segment an object.

50:46.960 --> 50:52.320
Does not mean that you learn to recognize the object by the segmentation. And also we can think

50:52.320 --> 50:57.200
about when you want to train really big models, you need a lot of data to do that. And segmentation

50:57.200 --> 51:02.640
data is quite expensive. You usually don't have that amount of data as you do for classification,

51:02.640 --> 51:08.560
which is an easier task. You have a lot of data just lying around there. So classification is

51:08.560 --> 51:20.160
usually the go to task. Yeah, but only just a few. Okay. Yeah. Just 1500 segmentation maps,

51:20.160 --> 51:25.200
either supervised or unsupervised. Yeah. A very few amount of segmentation maps.

51:40.000 --> 51:46.960
We did experiment with using more segmentation maps and it showed that the accuracy kind of

51:46.960 --> 51:51.600
fluctuates at some point. I mean, there's some point where it doesn't improve more if you add

51:51.600 --> 51:57.200
more segmentation maps, but you do have to take into account two things. One, we did find two,

51:57.200 --> 52:02.000
and we didn't trade for scratch. Two, we didn't have the resources to hyper parameter search for

52:02.000 --> 52:09.200
each selection of the number of. So it's possible that if you use more data, you would need to

52:09.200 --> 52:15.360
retune your hyper parameters and then get better accuracy improvement, but we didn't have the

52:15.360 --> 52:20.640
resources to do that. So it could be the case, but I don't really know. Yeah, I don't really have

52:20.640 --> 52:29.440
any finance for that. One thing we did to ensure that the model actually learns to predict better

52:29.440 --> 52:35.520
or to have better explanations is we looked at the accuracy increase for the non-training classes

52:35.520 --> 52:41.040
as well, because we said that we only use half the initial classes. It is really interesting to see

52:41.040 --> 52:48.080
if the model really improves on the non-training data as well. Does it learn to generalize the

52:48.080 --> 52:53.840
positive influence or the positive logic? And as you can see here, this is the ImageNet

52:55.120 --> 53:00.240
validation set. So yeah, there's a decrease as we saw before, but for the non-ImageNet

53:00.240 --> 53:05.120
distributions, for the shift of distribution, you can see that the improvement for the non-training

53:05.120 --> 53:10.240
classes is actually quite similar and sometimes even better than that of the training classes.

53:10.240 --> 53:16.880
So the model really from this experiment learns to generalize that healthy say-and-behavior to

53:16.880 --> 53:24.720
classes that were not in the training set. And here are some visual examples. These are

53:24.720 --> 53:30.320
examples from the ImageNet data set. So examples from the original data set of the model straight

53:30.320 --> 53:35.760
map. And here you can see that the same prediction is made for two different reasons. Here, the

53:35.760 --> 53:41.200
background, here actually the foreground, the snowplow. And here you can see corrective predictions

53:41.200 --> 53:47.200
where the model originally predicted that this is a can opener based on the eye of the puppet.

53:47.760 --> 53:53.760
And once we find you the model to look at the entire object or to look for, you know, less

53:53.760 --> 53:58.560
sparsely as the object, it actually talks about the teddy bear. And here you can see that even

53:58.560 --> 54:05.360
if the model is now wrong and was previously correct, you can usually quite easily explain

54:05.360 --> 54:10.800
why the model was wrong. So here's an example where the ground truth classification is tripod

54:10.800 --> 54:14.880
and the model predicted actually fine tuning a strawberry, but you can actually see that there

54:14.880 --> 54:19.360
exists a strawberry in the image. So it kind of makes sense that the model made that mistake.

54:21.760 --> 54:26.560
These are examples for shifted distributions. So as you can see before, for this example,

54:26.560 --> 54:32.800
the model predicted a garbage truck. Well, this is the forklift because the forklift is in a garbage

54:32.800 --> 54:39.280
area. So we correct the prediction to be a forklift based on foreground rather than the background.

54:39.280 --> 54:43.520
Here you can see a teddy bear that was classified as a ping-pong ball due to the sparse consideration

54:43.520 --> 54:49.520
of just its spot. And after the fine tuning, it is correctly classified. And the third example is a

54:49.520 --> 54:54.720
porcupine that was classified as a sea lion due to the background of the ocean. So once the model

54:54.720 --> 55:01.120
really learns to look at the correct pixels, it does make the correct prediction.

55:02.880 --> 55:08.080
These are additional examples, but really, we don't have time. And another interesting thing

55:08.080 --> 55:12.880
that we've noticed that I think is quite cool, even when you take examples that are completely

55:12.880 --> 55:19.280
out of distribution, I mean, this is an image generated by Dalit. And the models not know

55:19.280 --> 55:25.600
the class robot or oil painting and so on. Originally, it made a ludicrous prediction

55:25.600 --> 55:32.640
that this is a guillotine based on, I don't know what, you can't really understand. But after a

55:32.640 --> 55:37.120
fine tuning process, you can see that the model does not make maybe the best prediction that you

55:37.120 --> 55:41.920
can think of, which is the robot because it doesn't know that class. But it does predict the grand piano

55:41.920 --> 55:47.040
and it kind of makes sense because there is a piano in the image. So while the prediction,

55:47.760 --> 55:53.680
again, still does not make the most sense. At least it is based on some of the objects inside

55:53.680 --> 55:58.880
the image and not just something that you cannot make sense of due to sparse correlation.

56:00.560 --> 56:07.120
So this was the entire talk. Yeah, we got through it in time. Thank you very much. And the table

56:07.120 --> 56:12.000
of content is here in case you want to ask a question about specific parts of the lecture. Thank you.

56:17.040 --> 56:24.640
Yeah, one or two we can do. Yeah.

56:32.240 --> 56:34.960
Yeah, it's visible. Thanks for rotating it.

56:41.680 --> 56:44.800
Okay, the questions here are really lacking context because they were probably

56:45.200 --> 56:50.000
asked during that. So if anyone wants to ask a question again. Yeah.

56:52.720 --> 56:57.840
Yeah, I guess one question I had. Have you thought about including layer norm at all into your

57:01.360 --> 57:07.120
explanations? Because it seems that that does scale tokens in some way and could that be relevant

57:07.120 --> 57:13.040
for your output? Include what? Sorry, can you repeat it? Layer norm? Layer norm? Oh,

57:14.800 --> 57:21.760
no, but as I said, there is a method that I don't quite remember the name of the method that

57:21.760 --> 57:28.800
did take into account the norms of the output matrix, I think, in order to average across the

57:28.800 --> 57:35.760
different attention heads. But we haven't considered that. Yeah. We do consider that the gradients

57:35.760 --> 57:41.520
should be able to scale the different attention heads according to their influence on the prediction.

57:46.880 --> 57:48.560
Any other questions? Any other questions?

57:51.440 --> 57:54.800
I had a question if no one's going. Oh, yeah, go ahead.

57:57.360 --> 58:01.760
Wait, me or someone in the room? Sorry. Yeah. Okay, thank you.

58:02.720 --> 58:09.600
Okay. So I was wondering with regards to the stuff you said at the end, where some of them

58:10.160 --> 58:14.960
you see it and then you're like, okay, that was wrong, but like, makes sense. That's

58:18.000 --> 58:26.400
is there a way to quantify that and were related things or is it more like a you know it when

58:26.400 --> 58:32.640
you see it? Oh, yeah, that's a great question. There is a work done by Google, I think, that actually

58:34.000 --> 58:41.600
relaxes the task of classifying objects using ImageNet. They actually re-tagged ImageNet,

58:41.600 --> 58:46.880
where, you know, a strawberry in that case wouldn't be a mistake, but maybe it would be

58:46.880 --> 58:53.600
half a mistake or something like that. Yeah, so there's such a work that re-tags the entire ImageNet

58:53.600 --> 58:59.040
dataset to account for mistakes that aren't really mistakes, but actually makes sense.

59:00.160 --> 59:07.440
But other than that, I would say there isn't an automatic way to know that. I mean, I can't think

59:07.440 --> 59:11.920
off the top of my head of an automatic way to know when the model is mistaken, but it's okay.

59:13.360 --> 59:19.440
Cool. How did you guys check? Like, was it mainly the accuracy increase on the distribution

59:19.440 --> 59:27.120
shifted versions? Yeah, yeah, it was mainly, yeah, it was mainly the accuracy on the distribution

59:27.120 --> 59:35.680
shifts. And, yeah, yeah, and also looking at a lot of examples, right? Because I started out

59:35.680 --> 59:44.800
with many. Yeah, yeah, yeah, and a lot of manual work on actualizing examples that got me to the

59:44.800 --> 59:48.720
intuition that I'm presenting now, because I actually thought in the beginning that having

59:48.720 --> 59:55.600
the relevance be a segmentation map is quite logical. Yeah, so it takes some time to get

59:55.600 --> 01:00:02.000
through all the conclusions. Yeah, yeah. I was just having one idea coming out. Is that possible?

01:00:02.000 --> 01:00:09.760
So you are going to be this key time. So that in a strawberry case, you can crop out that region

01:00:09.760 --> 01:00:15.200
and then maybe run through another like Oracle network to tell you whether it is a strawberry or

01:00:15.280 --> 01:00:20.320
not. And that gives you some kind of a semi-automatic way. Yeah, yeah, definitely. That's an

01:00:20.320 --> 01:00:25.600
interesting take. It's interesting, particularly because I saw that different models tend to

01:00:25.600 --> 01:00:30.800
learn different spurious correlations. So it actually makes sense to check models using other

01:00:30.800 --> 01:00:35.840
models. Yeah, they're consistently making the same prediction with these vets. Yeah, yeah, perhaps.

01:00:35.840 --> 01:00:41.760
Yeah, yeah, that's an interesting idea. Your current relevancy extractor approach

01:00:42.720 --> 01:00:50.480
is limited by the VITs tile resolution. It outputs the attention map that is the size of the

01:00:50.480 --> 01:00:58.480
tiles and then you can bi-level it. Upscaling, yeah. I was wondering whether there's a way to bypass

01:00:58.480 --> 01:01:05.520
this tile resolution just by considering that we also have pixels coming into the tile.

01:01:06.080 --> 01:01:13.440
Yeah, yeah, we have tried to propagate relevance all the way back to the actual input and not on

01:01:13.440 --> 01:01:19.920
the level of each patch. It didn't come out just quite as we hoped. I think that the issue there

01:01:19.920 --> 01:01:25.040
is probably the positional encoding in the way. Somehow that layer of positional encoding

01:01:26.320 --> 01:01:32.800
ruins or destroys the relevance values once you propagate back from it. I couldn't figure out how

01:01:32.800 --> 01:01:42.080
to get past that layer that actually kind of added noise to the output relevance map.

01:01:42.560 --> 01:01:59.840
That's an interesting point, but yeah. Yeah, I haven't come across any such architectures

01:02:00.400 --> 01:02:07.040
if you do let me know and I can give you the try. No, there was a question here, right?

01:02:07.360 --> 01:02:12.480
I was just wondering, when you're about to work for, what if I wanted to explain about a color

01:02:12.480 --> 01:02:16.000
or something, or non-message, I'd like to just go to something about discussion, right? Maybe

01:02:16.640 --> 01:02:21.200
then perhaps lemon versus orange, you can color, kind of the main thing. That's just curious.

01:02:24.720 --> 01:02:29.760
This specific method would not be able to do that, but I know that there are explainability

01:02:29.760 --> 01:02:36.960
methods that kind of create a decision tree from the model. So you pay the price,

01:02:37.120 --> 01:02:42.480
that the accuracy decreases to some extent, and then you create a decision tree based on the

01:02:42.480 --> 01:02:47.600
decisions of the model. You kind of model the model using a decision tree, and then you may

01:02:47.600 --> 01:02:55.120
have a split that it has to do with, you pass a lot of images through a lot of images of oranges

01:02:55.120 --> 01:03:00.800
and lemons, and you see that one of the splits is by the color. Yeah, and then you know that.

01:03:01.200 --> 01:03:08.960
And probably you can do some trivial things to test specific theories, like turn the image

01:03:08.960 --> 01:03:14.160
into black and white and see what happens, to consider if the model takes into account,

01:03:14.800 --> 01:03:17.360
but this method will not be able to do that.

01:03:20.000 --> 01:03:21.520
All right, let's hand this speaker.

