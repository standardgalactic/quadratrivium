Thanks to Xanadu and all the organizers of Q-Hack for having me.
It's always a pleasure, you know, really love this event every year.
So yeah, so this year I'll be talking about quantum probabilistic information geometry
quite a bit with a lot of memes and hopefully a lot of shapes in the space of rotations
or unitaries.
Okay.
So, you know, this is my mandatory slide kind of reviewing the difference between quantum
theory and probability theory, right?
In probability theory we have positive numbers describing probability over outcomes and in
quantum theory we have wave functions that are complex value, right?
And these two theories are quite similar actually.
A lot of people like to say quantum theory happens in exponentially large space but
so does probability theory, right?
So you could consider probabilistic bits, let me just get a laser pointer here, yeah.
So probabilistic bits here and then you have qubits, right?
So that's one probabilistic bit, they're probability of zero and one, wave function
of zero and one, you know this by now.
So where does, you know, you hear about entanglement, superposition, all sorts of voodoo sounding
stuff, where does quantum computational power come from?
Is it from, you know, an exponentially large space?
Well probability theory, probabilistic systems have also an exponentially large space, so
it's not that simple, right?
We have superpositions instead of mixtures of outcomes, right?
What was shown experimentally many times now is that shallow depth quantum neural networks
have become classically unsimulatable, right?
So what's happening?
Why is the case, right?
So what I'm referring to is quantum supremacy, quantum supremacy was, you know, kind of like
achieving orbit or a split-nick moment for quantum computing.
Let's dig into this orbit analogy a little bit more.
So imagine quantum complexity, right?
Like zero quantum complexity or computational complexity is like the center of the earth
and then classical computers are kind of like planes and they can get to a certain altitude
but then they can't go deep into the Hilbert space.
In this case, space, Hilbert space, same thing, right?
Quantum computers are like Hilbert space ships and classical computers are like plain old
planes, right?
So there's a lot of caveats to this, but let's just say quantum complexity kind of scales
up, scales, you know, more or less linearly with the quantum circuit depth, right?
Why is that?
Well, to compute the wave function amplitudes, right, of all the outcomes that are put, there
is kind of doubly exponentially with the volume of the circuit, many paths to add contributions
from in order to compute the likelihood of measurement with a certain outcome and that
blows up really quickly and that's very hard to assimilate.
So really it's about interference in a very large space, not just having a large space,
right?
So the NISC era, we're kind of bounded in the maximum altitude that we could go not
because, you know, there's a hard wall, it's more kind of a soft wall that, you know, our
computers are not, you know, they have very high fault rate, right?
So statistically speaking, your rocket blows up before you get too far from Earth, right?
But we all have a space, a certain altitude to operate and that's why we do quantum neural
networks.
We stay within that space and we search within that subspace of low depth circuits, right?
So what's the solution to go, you know, further and with higher performance, it's to hybridize,
right?
Here we see a plane launching a rocket, right?
So that's kind of the philosophy of what I'll be presenting.
It's kind of using classical and probabilistic ML as far as we can go and then adding quantum
on top.
So you know, the solution is to create hybrid models and software.
I'll talk a bit about both, mostly about models, but, you know, hybrid models, you've
heard about hybrid quantum classical neural networks, you know, we built TensorFlow Quantum
to handle them.
You're familiar with Penny Lane as well.
They're kind of the TensorFlow and PyTorch of quantum classical ML.
So you're familiar with these, you've heard about them all week.
So I'll focus on the models and, you know, check out tensorflow.org slash quantum if you're
interested, trying it out.
So we want to hybridize quantum and classical deep learning neural networks and quantum circuits.
What are the ways we can do that?
Well, in ML, as we know, there's, you know, two of the big pillars, there's others, but
the big ones are generative modeling and discriminative modeling, but really they're both kind of
modeling distributions, right?
Like generative modeling is trying to learn a parametric model of a distribution from
which the samples, which are the data came from, and then generative modeling is conditional
distributions, really, that we're trying to learn.
So what I'll be focusing on is kind of hybridizing for generative models, not so much discriminative
models.
There's a lot of examples in TFQ, just in feedforward hybrid networks.
So okay, so what happens at the intersection of quantum theory and probability theory instead
of trying to replace, you know, probabilistic computing with a quantum?
Well, we know that a lot of systems actually are described by probabilistic mixtures of
quantum states.
So you know, those are called mixed states, right?
And mixed states have an eigen decomposition.
So how can we use this to model mixed states, right?
So how do we represent mixed states, right, states that are in the intersection of these
two theories, while we can compose literally models that are made for probabilistic ML
and models for quantum ML, right?
So the hybrid approach is to sprinkle in some quantum on top of our probabilistic ML stake,
right?
Thanks, Salpe, for demonstrating.
So you know, you've heard about VQE, QOA, QNNs, and so on.
That's all pure states, right?
So you have an initial state, you have parametrized, unitary, and then you train it.
So what we're focused on is quantum probabilistic models.
So you have a probabilistic model that parametrizes probably distributions, right, over bit strings,
and then you, to prepare the, to kind of transduce that sample distributions to the quantum computer,
you flip the bits according to each sample from this generative model that is classical.
And then you have a diagonal density operator, and then you apply a unitary quantum neural
network.
And on average, the resulting, you know, onsots for mixed state is one that has classical
parameters data, and quantum, and then parameters five, and you could train both basically.
And that's really what we're focusing on all day today.
So how do we cleverly parametrize quantum probabilistic models?
E, just E, you know, let's use E, we like E.
So exponential families, right?
E is nice because of calculus reasons, and it's nice because of physics, there's Boltzmann distributions,
but E is going to turn out to be very, very useful.
So instead of parametrizing the density matrix directly, we parametrized the argument inside an exponential, right?
So we parametrized it as a quantum thermal state of a parametrized Hamiltonian operator, right?
And so how can we parametrize this Hamiltonian operator?
Well, you know, any operator that's Hermitian is going to have an eigen decomposition.
So we could parametrize the eigen values with a classical neural network.
So it could be, you know, a deep neural network, be a Boltzmann machine, you know, a second order binary
polynomial, whatever you'd like.
And, you know, for the diagonalizing unitary, it could be whatever your favorite Q&N is, right?
And together, you get this onsots, right?
So these are called quantum Hamiltonian-based models, QHBMs.
And that's what we're going to be talking about all day today as well.
OK, so how do we train these and what do we use them for, right?
So, you know, quantum simulation, learning quantum data, it's all the same task of minimizing quantum relative entropy,
and QHBMs are really strong at that.
So it's kind of the, you know, I'll use words such as VQT for quantum simulation, QMHL for quantum
data learning, and QHBMs all the time.
But, you know, it's all about, you know, because we made this choice of using E, we can minimize quantum
relative entropies really nicely.
So quantum relative entropy for those who, you know, aren't familiar, is this expression?
We're taking logs of density matrices, which is, you know, usually really hard to evaluate if you didn't pick your
parameterization cleverly.
But because we picked it with the diagonal form, there's all sorts of simplifications that happen.
I won't give you all the math.
I'm going to flash screen later.
But the TODR is that you can simulate thermal states, right?
Which is, you know, makes sense because we parameterized our models as parametric thermal states.
So if you're giving a temperature or inverse temperature of beta, cold coldness, in a Hamiltonian, you can
pretty easily generate, well, depending on your onsots, you could generate the thermal state, right?
By minimizing the quantum relative entropy this way, where it's relative entropy of your model
versus the target thermal state, right?
And it's kind of a generalization of VQE called VQT because it's nonzero temperature.
As we'll see, I'll flash the math, but for now you have to trust me, there's really clean parameter shifts.
Just like the parameter shifts you've seen for Q&Ns, there's parameter shifts for the classical model as well.
And later on, we'll see hybrid parameter shifts for the metric.
And you don't need to estimate the partition function if you use gradients, right?
So you always want to use gradients for these models.
The dual task is given data, I want to represent it as a thermal state of a parametric Hamiltonian.
So I want to figure out that what is called modular Hamiltonian of my model's density matrix, right?
It's like a quantum version of log likelihood.
So let's say I have an unknown data state, it could be a mixed state from a certain distribution.
Well, I want to be able to generatively model the state, create new copies.
So what I do, I just minimize relative entropy the other way around, right?
Now I'm going to flash math for those that are watching the replay, because I don't have time to cover it now.
But this is the math of VQT, this is the gradients, and this is how you could evaluate the loss function going forwards.
It's kind of like VQE, but with some side classical computing, right?
And the dual task, when you have the data, is to ingest the data and run it backwards through the Q&N.
And then evaluate expectation values to evaluate the loss.
You can compute the gradients of the classical and the quantum parameters pretty cleanly.
It's very nice.
And as we'll see, we're actually going to release software to do this for you, so you'll have to understand all this math.
So we've had a paper for a while on this.
It's covered in the TFQ paper and the original paper.
I won't go through these results, but if you have a good ansatz, you could model fermionic.
Pozonic, spinchains, et cetera.
But this talk is about going further than before.
So we can model single systems with these algorithms, or if we're given many copies of one state, we can generally model it.
We've shown that.
But what if we want to have a lot of tasks that are very similar to one another?
Like modeling spaces of different material parameters or different temperatures,
or modeling how quantum states propagate through time, which is why we wanted to build quantum computers in the first place?
Well, if we just train everything over again from scratch, we're kind of throwing out our work, and it's not very clever.
And even when we're gradient ascending, because of the quantum chaos of random Q&Ns when you're using quantum hardware-efficient ansatz,
change the parameter a bit, you injected a bunch of chaos, and you kind of threw away your work.
You're in a completely different part at the output of the space of states.
How do you control this chaos?
Well, you need geometry, right?
So if you don't try to control the chaos and you try to model a bunch of states at different temperatures and you work with depth,
you see baron plateaus happening here, you see there's dips, there's sectors of parameter space here, in this case temperature,
where it's harder to generatively model.
So could we use, could we copy ourselves, kind of self-plagiarize our optima we found from different temperatures,
different regions of parameter space in order to instruct where to look?
The problem is, quantum hardware-efficient ansatz with no information about where to look in parameter space,
it's too big, it's too expressive, right?
We want to reduce where we're looking to a small ball in the space of states.
So how do we best traverse the quantum probabilistic landscape?
This is where the devs memes really kicks in.
Devs is a cool show about quantum computing.
And check it out if you understand these references.
So quantum probabilistic information geometry, the main meat of the talk.
So you've heard of natural gradient descent.
It's technically, that's technically natural gradient descent for pure states,
which is a different metric called Fubini study.
But here we're talking about a different type of metric called the Bogoli above Kubumori metric.
Anyways, that's for the connoisseurs.
But at a high level, we have the space of density operators.
We have the, you know, some manifold spanned by our ansatz for all the values of the parameters.
And let's say we have a fixed value of the parameters, we have our density matrix ansatz.
Let's say we wanted to take a step in the space of density matrix,
the density matrices that was of a constant size in the relative entropy, relative to itself, right?
It's relative as entropic distance is constant, right?
Well, you could do all sorts of, you know, calculus Lagrange methods to find, you know, this,
you know, what nudge in parameter space from a certain point, you know, can obey this equation, right?
Let's say you're doing gradient descent or some other types of moves as we'll see later.
Well, what you could show is natural gradient descent is optimal.
And in our case, the natural gradient descent is the Hessian of the self-relative entropy.
So again, it's like the relative entropy with respect to the perturbation away from a point, right?
And you're familiar with this update, right?
The inverse metric times the gradient.
They'll have all sorts of pictures explaining how we come up with this update in a second.
But, you know, this is more for the advanced folks.
What's cool about this is that, let's say this is our parameter space, you know, on our classical
computer before we inject it on the quantum computer, a circle in state space gets pulled
back to an ellipse in parameter space, right?
There's a local squishing, right?
And the major axes of this ellipse are actually the eigenvectors times the eigenvalues of the inverse metric, right?
So, you know, if we want a notion of a circle or we want to have a notion of distance in parameter space,
we could pull back that circle or pull back that notion of distance and we get a metric.
And that metric we can use to regulate our step sizes.
As we'll see in a second, it's actually kind of like adding friction where the friction is computed
with a notion of velocity that is the velocity in the distance defined by the information geometry by this metric.
Okay.
So, again, for the advanced folks, we're doing the Bogolibov-Kubumar metric.
We can compute it.
We have parameter shift rules for all the matrix elements if you really want to compute it and have a lot of fun.
There are a lot.
But we have ways to not compute the metric and still get all the benefits and actually even more performance.
As I'll present in a second.
So, okay, that's complicated math.
That was like manifold math.
Let's just show me some shapes.
I heard it was shapes and rotations.
You talked about rotations, unitaries.
Where are the shapes, right?
So, here's a shape.
Here's a lost landscape.
We have our point.
We could take the tangent plane, right?
So, first order to the expansion at point in parameter space.
If we add a self-relative entropy regularizer, right, to that potential or that lost landscape,
maybe it looks something like this, like this green thing,
we could also tailor expand to second order this regularizer.
So, what is NGD in this case?
So, NGD adds the plane and the bowl, the plane from the loss and the bowl from the regularizer.
And, you know, if you add, you've completed squares in high school, right?
If you add a second order polynomial to a linear function,
then the new, if you add them together, the new minimum has shifted, right?
The, where the derivative is zero, right?
You could find it with gradient descent or you could find it analytically
by inverting using the pseudo inverse.
So, the thing is here, you need to get the second order to tailor expansion.
So, you have a lot of terms to compute,
which doesn't scale because it's order N squared for N parameters, right?
So, how do we avoid doing this?
I'll get to that, actually, sorry.
I want to, I want to explain the, you know, what this looks like
if there's more than one point, right, once you do many updates.
So, if you have a lost landscape, you could compute, oh, that's weird.
It looks really weird on photo, but yeah, you could compute a gradient vector field, right?
Just the gradients for this potential, and similarly, you know,
just like I showed you, you can do a second order tailor expansion
of the regularizer anchored at each point in this space.
And then, you know, if you, if you slice a bowl, it becomes an ellipse.
The ellipse has two major axes.
That's what we're plotting here, right?
For the advanced folks, I'm plotting the inverse metric as a, as a tensor field, right?
And then the point is you can multiply this inverse metric times the gradient
and get a new gradient vector field, and that's the thing we're following.
We're just flowing according to that, right?
And that's basically Riemannian gradient flow, or our quantum
probabilistic Riemannian gradient flow.
So, the thing that's happening here, though, is I have a restorative potential
anchored at each point that kind of like curd step size, right?
So if I consider gradient descent as like integrating or dynamics according
to certain steps or finite amounts of time, how much step,
how long of a step I do per unit time is basically velocity.
And here I basically have a force that is negatively proportional to the velocity
as measured in the inverse metric, right?
So, we're basically just adding friction to tell our gradient descent optimizer
to slow down, but it's a notion of friction that is, you know,
from the information geometry length, again, which is from the relative entropy at distance.
Okay, so how do we avoid doing, how do we get the benefits of NGD
but avoid doing the second order expansion?
Going back to the full picture, well, instead of doing second order expansion,
we could just use the self relative entropy directly, right?
And we have the first order expansion and then we have the self relative entropy
and we could just find the new equilibrium point with an inner loop of gradient descent.
So that's the first order method because we're just using gradients.
We could evaluate the gradients of anything you want to do with relative entribes and QHBMs.
So that's what we use and basically turns out to perform extremely well.
And we call this quantum probabilistic mirror descent.
It's an alternative to quantum probabilistic natural gradient descent
and the two are equivalent, you know, in the limit of small step sizes.
So, you know, MGD versus MD, it's a graphical comparison.
Pause.
And yeah, so this is mathematically what it looks like.
We find per step, you know, the argument is an error between the parameter
and the vector of the gradient.
Yeah, so it's specific to the Boguli above Kubo-Mori metric.
It's much more tractable and we don't need to do block approximations or anything like that.
It's great.
All right, showing some results.
So what happens when we supercharge VQT with geometric optimizers?
So vanilla is just SGD.
This is fidelity.
So fidelity is our test loss.
We don't train our fidelity.
We train on relative entropy, right?
So vanilla is SGD.
Adam, definitely struggling.
And then natural gradient descent is kind of noisy and not as consistent
because again, there's all sorts of difficulties estimating the metric.
But then mirror descent always ends up winning.
And that's been our experience so far.
And you could just keep going closer and closer to perfect fidelity.
And this is like a six qubit system.
So that's great.
Mirror descent is really cool.
We called it the chat optimizer internally.
But yeah, so can we go much further?
Now that we got this cool optimizer, can we be ambitious?
Can we take inspiration from the show devs and try to replicate some of the algorithms
they have there and make them happen now?
So yeah, so we're going to present now a way to variationally do real
and imaginary time evolution using these geometric optimizers
and actually a few more applications with those techniques.
So I'm going to introduce Quartz and the spoiler alert.
The V is Roman.
Sorry for spoilers.
But yeah, so Quartz V is Roman.
And so you could consider if I have a geometry of my model parameter space,
but there's also any sort of tasks that are related to one another
also live in the geometry of states.
And I could kind of pull back paths in state space to my parameter space.
That's the advanced version.
But practically, if you're used to trotterization or open system time
evolution, trotterization equivalent, you have tiny time steps
and your state evolves in state space.
It moves a bit.
And each time step, if you take it in decimal, the state should move just a little bit.
And there's kind of speed limits to how much your state should move per unit time,
depending on the strength of the dynamics.
So can we leverage the fact that these tasks are generally modeling
the state at different points in time that are related to one another one
and they're close to one another too?
And the answer is yes.
So if you take really tiny time steps, they're all within one circle of one another.
And then you could use geometric optimizers to restrict your search
to search locally in the space of parameters
so that the optimization of re-optimizing your state to propagate
forwards in time is convex and it's really easy.
This is what the algorithm looks like.
You generate the QHBM for the previous time step,
you apply some trotter steps, and then you relearn it.
We're kind of like, instead of using the quantum computer
to propagate states through time on the quantum computer in the memory,
we're kind of saving or in representing data as coordinates in the geometry.
And we're just updating the coordinates.
So this is again from Devs.
They can extrapolate on the future or the past,
and it was inspired by that algorithm.
So this is our analogous movie here.
So on the left you have the ground truth density matrix,
again for an interesting system.
In the center you have our geometric optimizer quarts.
And on the right is just if you try to do smart initialization of atom.
So it's much better to use quarts in geometry.
So what does it look like as we do time evolutions, the fidelity?
So it does decay exponentially, but as you can see,
after 40 time steps we're still at above 0.9 fidelity
of the time evolution with chain mirror.
Chain to atom is doing something,
and then if you're just trying to do it independently
and not using the locality, it decays very quickly, right?
Why is that?
Well, chain mirror descent in geometry or where doesn't kind of kick itself out of like,
it was close to the good value of the parameters
because time evolution is small time steps, right?
Whereas like atom has no notion of the geometry,
has like an approximate notion of the geometry,
but it's not very good.
And so it kicks itself out of, you know, the region of the optimum, right?
So, yeah, I mean, that's why we build quantum computers to do time evolution.
Now we have a QML native way to do time evolution.
There's something else in physics called imaginary time evolution, which is really interesting.
Imaginary time evolution is like cooling, progressively colder and colder,
simulations, right?
And sometimes that helps because, you know, just like you get a needle,
a metal, you could find better optima by cooling slowly, right?
Can we find better optima for VQE or VQT by starting at high temperature and cooling, right?
In physics, this is called WIC rotated or imaginary time evolution, right?
Because this is Twitch, you know, can I get some wickets in the chat
because we're about to get wicked in here with a WIC rotation.
Okay, so taking inspiration from our time evolution plot,
you could have what are called coldness, so inverse temperature.
You could step in coldness, it's called Euclidean time evolution.
And similarly, we can kind of reuse our work as we, you know, step in coldness
and we could restrict our optimization to be in a local neighborhood, right?
And as we'll see in a second, actually works for not just stepping in coldness,
but any parameter of the Hamiltonian you'd like to simulate as long as you step slowly.
Okay, so how does it work?
So this is us, you know, cranking up beta inverse temperature.
Again, you know, chain mirror descent is amazing.
Adam doesn't work so well, everything else crashes.
So if you just try to do a bunch of VQTs that are, you know, as we saw,
if you do independent VQTs, there's a dip in the fidelity, it's pretty significant.
But we vanquish that now.
And again, we see it from the lot, these are the loss curves of each.
Adam kind of, you know, just like the meme I showed before,
it kind of screws up its work before having to fix it.
Whereas mirror descent, yeah, it doesn't flinch as much.
So, you know, why does this work so well?
Well, you know, remember with NGD or equivalent mirror descent,
we have this regularizer that you add to the landscape to make sure that the,
you know, and you take kind of the average of the two potentials
that you take the gradient of both to equilibrate.
And that's how you do your updates.
So basically, if the landscape slowly shifted, slowly shifts,
and you track at each point your regularizer,
you're always and effectively a bull, right?
And gradient descent really likes bulls.
And so, yeah, so it converges really well.
And basically any sort of manifold of tasks or paths of tasks,
you could use this geometric optimization strategy.
Okay, so that was some crazy math.
You know, this is a hackathon, not a mathathon.
So, you know, it seems like some of you could use some coding tools
to tackle this, right?
And so, this is a QHAC exclusive.
We're releasing, as of today, a QHBM library.
It's an open source library for quantum probabilistic generative modeling.
So there's QR code here.
So QHBM lib is kind of the first our knowledge
like quantum probabilistic ML focused software package.
Yeah, it's open source.
It's built on TensorFlow quantum and TensorFlow probability, naturally,
doubling up on TF.
It's much, much, much higher level than trying to code all of this from scratch.
You know, this is like doing a whole QMHL,
all the crazy math I was showing you, a few lines of code, right?
So it has QHBMs as first class objects.
It's a much higher level API for manipulating QNNs or taking expectations.
It has VQT, QMHL, and, you know, pre-programmed.
There's all sorts of tools for sampling.
Like, there's weird MCMC algorithms you need for, like, neural networks
to find on discrete spaces that we put in there.
And we're going to add our research baselines very soon.
But it's available, as of today, at github.com slash Google slash QHBM library.
Check it out.
But, you know, it's all about the devs, right?
And the team's been working on this very, very hard.
I just want to give a huge shout out to the team.
Antonio's been leading the library work for a while now.
Very diligently working.
Ferris has been leading this geometry project.
You know, watch this guy.
Ferris is a Padawan, soon to be Jedi of QML.
And shout out to, you know, Dimitri and Geoffrey for help with the theory,
and as well as Jay and Sahil for help with the engineering.
Check out their Twitters.
Wrapping things up.
So, quantum Hamiltonian based models.
They're great for evaluating relative entropy.
They parameterize density matrices in a way that makes sense,
where we use classical analysis as far as we can go and just sprinkle in some quantum.
And MGD, mirror descent, is really good.
Check out the library for VQT and QMHL,
and we'll have some papers soon on the quartz and meta VQT.
All right, so those are the references.
Just go flash them a little bit.
And that's it for me.
I guess we'll take questions now.
Awesome. Thank you so much, Guillaume.
Great talk, as usual.
Great memes.
I love the closing with the alfalfa sprouts there.
Nice call.
Spoiler alert for those who haven't seen all of Devs.
I'm sure some of that stuff is spoiler territory.
So just be mindful.
Cool. Guillaume, why don't we start with,
and we're talking about QML, QML libraries.
You just announced a new library.
That's amazing.
I know you and I first met at a conference maybe five years ago in New York,
an IBM conference.
It's actually quite a cool conference because there's lots of people there
who were just starting out.
And now it's amazing to see where people have ended up years later.
In those days, QML wasn't really much out there yet.
And I know that we at Sanity were keen on it.
I know for sure that was an area that you were super keen about.
How have you seen the field of QML growing in the past five years
compared to when we first met?
I mean, it's grown exponentially, right?
I remember when it was...
Yeah, I think basically everybody who was working on QML
could fit in that room in the IBM lobby realistically, almost.
And yeah, I mean, it's been wonderful to see, I think,
companies like Xanadu and certainly some of the efforts we've been doing at Google and X.
We've been trying to fuel the growth of this space.
And I think we could give ourselves a little pat on the back
for helping it propel itself forward and grow so much.
And I always look forward to the QHAC event every year
because it kind of gives a measure of how much the space has grown.
And I see all the engagement on Twitter and so on.
And it gets me really excited because ultimately, that's what it's all about.
I've been passionate about this field since 2017.
And so have you and Xanadu.
And yeah, it's just been wonderful to see it grow so much.
Is there anything you could put your finger on as the biggest catalyst
to the explosion of the field?
Yeah, I think developing software and open-sourcing software
and kind of transducing from the culture of just writing papers
with really large proofs for future quantum computers and asymptotics
to like, here, I turned this into engineering.
Here it is. It becomes kind of more of empirical science.
And engineers and software engineers can participate
and help propel things forward so it's even more inclusive.
And there's all sorts of talents that are needed to propel QML research
from software engineering to, in this case,
geometry, mathematics to engineering infrastructure,
making cool diagrams.
Yeah, no, it's been wonderful.
Where do you think we're going next?
What's our biggest thing we have to overcome?
What's our destination next?
Well, I'm pretty bullish on these directions.
I want to keep exploring whatever this area is that we've been exploring.
I'm really interested in the probabilistic ML theory
and taking inspiration from some stuff there.
But yeah, I think it's really getting variational algorithms
to work on hardware, really advanced error mitigation ideally
that's kind of native to QML
and demonstrating that maybe we don't need error correction.
I think that would be a very bold direction
and that's what I'm aiming for personally, I guess,
which is bold.
I think error correction is like the sacred cow in quantum computing.
It's kind of an assumption that we'll need it.
But yeah, maybe there are certain devices that have enough fidelity
and with clever error mitigation, we can make low depth circuits useful.
We saw today we could do propagation of states over time
but it's low depth circuits throughout the whole thing.
But it's going to depend on how good the devices are
because low depth really depends on the quality of the device.
But yeah, to me, that's where I'm placing my bets for the direction
because even though QML is going to be useful for fault tolerant error,
fault tolerance has a lot of engineering challenges
and it's going to take a while to get there, obviously.
Cool, so you still think that there's some intermediate territory before then
where we can do something interesting?
Yeah, I mean, I hope so, but it's not just hope.
I'm actively working on it as hard as I can, I guess,
and trying to import as much leverage from classical ML
as possible for every aspect, hybridizing,
and there's optimal control with ML that can help in better device fidelity.
There's error mitigation with ML, just throw ML at everything
and see if we can make this thing work.
Let's see how far we can get.
Cool, man. I want to kind of turn to a different direction here.
Since we last met you at QHAC, you've become a bit of an artist.
Is that true?
Yeah, I may have sold a few NFTs.
So for those of us who are naive, don't really follow the scene.
What's an NFT? What does it have to do with quantum? What are you doing there?
Right, so this journey started last April, on April 1st.
I wanted to, you know, NFTs and crypto tokens were taking off
and I wanted to make a nice April Fool's Day joke that wasn't a joke
or I wasn't sure it was a joke.
And that morning I had an idea for, you know, creating a piece of art
that basically NFTs are, well, right now they're mostly images,
but they could be any sort of file.
They're a token that is tracked on the blockchain.
The blockchain is kind of a decentralized computer database
where, you know, Ethereum is actually a universal computer that's decentralized.
But the point is that there's kind of a global state.
You know, there's a global memory for kind of the earth
and so you could keep track of who owns what, for example.
And NFTs do that.
So let's say you have a digital piece of art,
because you could copy classical information, not quantum information.
You could copy classical information.
You know, it's hard to keep track of who owns the digital copy,
but with NFTs you can have kind of a record that's indelible of who owns what and what time.
And so it really opened up the space of digital art.
And of course, if you're going to use a quantum computer to make art,
it's probably going to be digital.
And so I had an idea to do the first quantum AI-based NFT,
and so I did a little VQE of some bosons,
you know, simulating something from my Masters in Quantum Cosmology,
simulating the early universe.
And I put it up for sale, and it sold for, I think, 5 Ethereum.
And then that was, you know, that was quite the moment.
I was like, oh my god, there is a demand for this.
This is serious.
This was a joke at first, but actually maybe I should take this seriously.
And then I learned more about the space,
and I've made many more and been pretty successful,
if not addictive to some extent for me to produce these.
And, you know, it's helped me test out several platforms for quantum computing
to generate these pieces of art for fun on weekends.
And you've even minted a piece in commemoration of QA, is that right?
Yeah, that's right.
So, yeah, if you check out my Twitter, and there's a link to OpenSea,
and there's a peace sign because, you know, it's the 70s theme.
So I minted one for QHAC.
So if anybody's really into NFTs, it's the same algorithm I've been using.
It's a simulation of bosonic quantum field in the early universe.
And then I do a coherent state that is using a displacement at various frequencies
so that it makes an image, basically.
So it's a peace sign, so check it out.
Cool. Yeah, maybe we can get the chat to run up the bidding price for that.
That'd be great. I mean, it certainly, you know, I mean, I've been using that capital
to make, get a better PC, get a GPU, to train more QNNs and so on.
You know, GPUs these days are basically like used cars.
It's hard to come back. Yeah.
Cool. I just want to quickly ask you one more question about the show Dev.
So in that show, you know, one of the things they do is they have this quantum computer
that can really look very far forward and backward in time.
Again, spoiler alert.
Where do you think, do you think we'll ever get to a stage where we could do like large scale
simulation of anything beyond like a single molecule at the scale that they're doing on that show?
Like temporal extrapolation?
Like, you know, all of the molecules in a room to be able to like rewind or fast forward the simulation of that.
Yeah, so that's the part, you know, that is less realistic, I guess.
It's like, you know, you have order Avogadro's number plus or minus few orders of magnitude to simulate a whole room
was worth of particles and we need, you know, similar order within a few orders of magnitude of kind of qubits to do a full scale simulation.
But, you know, maybe if we if we don't mind, you know, filling in some blanks with some neural nets, maybe we can we can have a simulation that's partly quantum mechanical of the whole room and it's still accurate.
But if your question is, do we live in a simulation?
The answer is, I don't know.
I don't I don't believe that was the question.
No.
But that's the theme of the show.
Yeah.
Has been answered.
Cool.
Why don't we take some questions from the chat?
There's a couple here I can get to.
So here's one.
I'm not going to even bother repeating that username because I think they just mashed the keyboard there.
Sorry if you mentioned this, but why train on relative entropy rather than fidelity?
Yeah.
So so fidelity, first of all, is you can evaluate for pure state with the swaps swap test.
It's kind of very sparse and difficult of a of a loss function, right?
It's kind of zero or close to zero for most most states except one, right?
And you got to get really close to that state.
It's actually a pretty bad, bad loss function to evaluate.
And there's there's proofs by Patrick Coles and the LANL group on, you know, bearing plateaus induced by the cost function.
So that's one reason.
But mainly we're using density matrices and evaluating the fidelity.
If you look up fidelity from mixed states, it's a bit more complicated.
And in ML, if you know, you know, if you in ML, it's cross entropy and basically kale divergence everywhere.
And that's what we use.
We use quantum cross entropy and quantum free energy.
It's kind of the king of loss functions in ML.
And so we King and Queen, I guess, and we we use that but a quantum version of it because we can't evaluate it with our models.
Why is it really good?
It's kind of because you're taking this log, right?
So if there's just state overlap, if you have low probability for things, you don't get a lot of signal.
But the log, it kind of just you get just better loss signals when using log.
That's why people use log likelihood in classical ML.
And we're seeing that with with quantum ML as well.
So all this to say basically the loss is way less sparse and and and and yeah, we're taking inspiration from classical ML.
Nice.
My favorite source for inspiration.
All right.
Maybe one more question here.
Question from Abusef 19.
Are GANs good generative models for quantum?
Yeah, so I mean, you know, GANs have been around for a while.
Now they're not as much in vogue.
It's all about, you know, energy based models, transformers, something called, I think, stochastic differential equations and and flows.
And so, you know, our approach is kind of like similar to flows, but also similar to energy based models.
So I'm obviously biased, but I think GANs have kind of fallen out of fashion because they're hard to train.
Finding the equilibrium is very tricky.
And just in terms of performance, other types of generative modeling has taken over.
GANs are really cool because they're they're very accessible to understand the basic principle, right?
The adversarial examples and the class fires, right?
Cat dog and so on.
But, you know, kind of when you progress in ML and you can learn more advanced concepts like relative entropies and score functions and so on, you can, you can, you know, tackle some fancy methods, maybe with a bigger learning curve, but that perform better.
And so I'm biased, but I would say, like, let's explore other types of generative models other than quantum GANs for really good performance and generative modeling with quantum computers.
Yeah, it's interesting to see, as you mentioned, that energy based models are kind of back in favor.
They're not they're not a new invention, though, right?
They were originally kind of inspired by physics.
So, yeah, exactly.
Well, I won't get to the slide, but ultimate machines were like second order polynomials defined over binary values.
And those were the original kind of energy based models.
But now you have deep BBMs, right?
So you have a deep neural network parameterizing the energy function.
And then you use Markov chain Monte Carlo that uses gradient information, something like Hamiltonian Monte Carlo to sample that landscape.
And so you could kind of take the expressive power of deep neural nets and all this differentiable computing power and then bring it to, you know, the the all the cool techniques and results that, you know, were first derived for EBMs, right?
And they were all working with, like, both machines and EBMs, and they had all sorts of interesting ideas.
And now we could take the, you know, the expressive power of all these deep neural nets and differentiable computing and tackle some of these ideas with all that compute power now.
So it's, it's really interesting.
And that's definitely what motivated originally this work into kind of transducing the philosophy of deep EBMs into QHBMs, right?
Like the difference between this and kind of a Boltzmann machine is that we have a deep function, a deep QNN and then a deep neural network to evaluate the energy.
So very much inspired by that.
Great. Yeah, thanks a lot for your amazing insights.
Gilm, it's been great to have you on here.
