WEBVTT

00:00.000 --> 00:10.000
So welcome to prompting with prompt layer conversation we're doing with many people in AI and we're kicking it off now with Steven Wolfram, who we have here.

00:10.000 --> 00:18.000
And yeah, just real quick prompt layer where building dev tools for people bringing LLMs into production like GPT.

00:18.000 --> 00:25.000
So if you want to do some data driven prompt engineering, we're the place for that and we're going to have we're going to have a pretty cool conversation.

00:25.000 --> 00:29.000
I hope we're going to nerd out a little bit here and excited for that.

00:29.000 --> 00:32.000
So you probably know who Steven Wolfram is what he's done.

00:32.000 --> 00:48.000
I learned about his work from doing math homework properly in middle school or high school but originally originally met Steven when he came to a hackathon I was running in high school and he stayed there till like 2am mentoring people and that's kind of my first

00:48.000 --> 00:55.000
introduction to this theory about computational thinking and it's come back around, you know, and it's very important today.

00:55.000 --> 01:05.000
So, yeah, really excited to have this conversation with you and maybe to kick it off.

01:05.000 --> 01:22.000
You had a big and a big release today of kind of plugins into chat GPT and this is, I think you might have said it but I completely agree it's kind of a historic moment where we're kind of bringing the LLM side of things that AI statistical AI to the symbolic AI to the symbolic

01:22.000 --> 01:31.000
interpretation and that sort of thing so like, how did we get here what what has the process been and what why is it so significant right now.

01:31.000 --> 01:46.000
Well I think the what LLMs have been able to do is sort of take the corpus of texts that we humans have written and kind of grind it up to the point where we can make more that's like it so to speak so we give a prompt.

01:46.000 --> 01:59.000
And you know the the role of an LLM is to continue from that prompt and that's and sort of continue with things that are like the things that have already been written on the web.

01:59.000 --> 02:05.000
And I think, and it does that in a way that is kind of just sort of statistically fitting things together.

02:05.000 --> 02:17.000
It's a very different tradition of computation that is really able to use the deeper aspects of computation really able to use sort of the irreducible computations that can in principle be done.

02:17.000 --> 02:29.000
That's the world that I've lived in for for many decades now of trying to figure out sort of how to take what is computationally possible and make it accessible to humans.

02:29.000 --> 02:44.000
So the thing that's been sort of exciting is that well we made a lot of effort to make it accessible to humans turns out that also makes it accessible to language based ai's that kind of lent their craft from from humans so to speak.

02:44.000 --> 03:01.000
So kind of the way I see it is these have been the two great kind of traditions of AI, and we've now got this opportunity to really connect them together through the medium of sort of a mixture of natural language and computational language, and be able to get it

03:02.000 --> 03:18.000
that one can kind of use the linguistic interface that we are all used to because we're all very experienced at sort of interacting in natural language, together with kind of the depth of that rather in human non human thing that is sort of powerful

03:18.000 --> 03:33.000
for computation. So it's kind of bringing those two things together the, the very human language with the very sort of beyond human non human power of kind of deep computation, and you know the fact is that, as a practical matter, LLMs are making

03:33.000 --> 03:39.000
sort of statistically reasonable pieces of text, which may or may not actually be the way the world is.

03:39.000 --> 03:56.000
And what we've been involved in doing is kind of making a representation of the world as the world is in computable form, and being able to bring those two things together so that we're able to let the LLM sort of make it statistical kind of text, and then we're able to provide

03:56.000 --> 04:09.000
sort of precise computable insertions into that text to kind of provide the facts, where the LLM can be writing fact or fiction, and it doesn't really know the difference so to speak.

04:09.000 --> 04:19.000
Right, right. No, this is super interesting and like, and I guess it's, and it's changed so much in the past like two or three months. I guess like to dive deeper here.

04:20.000 --> 04:32.000
Maybe, I'll give you an example so, and I'd love for you to explain kind of your thinking behind this, and this is changed today with the welfare and plug in and chat GPT obviously but I had a friend Avi and he tweet.

04:32.000 --> 04:48.000
Let me let me just read the actual tweet so he was basically he tweeted that he was using GPT 3.5 then he was using GPT for and it was failing on this very simple problem that he was doing and the prompt was how many unique apartment units are represented in the data below.

04:48.000 --> 05:08.000
As in two people in the same unit, for example, 430 would count as one and then you had a bunch of names with different apartment numbers, and his question was to him, like, this is not obviously a hard problem in the let's take a step back from what's symbolic what's statistical,

05:09.000 --> 05:21.000
just at the space level if I'm a hacker on just like using open AI, this doesn't seem like a hard problem but it wasn't a problem GPT is able to solve on its own it's not a problem LLMs are able to solve.

05:21.000 --> 05:30.000
Would would love to hear you kind of break that down and explain what about this problem is hard for LLMs and what about it it's easy for the warframe language.

05:30.000 --> 05:35.000
So, anything that involves precisely computing something with math.

05:35.000 --> 05:49.000
There can be many steps that are involved in doing a math computation and LLM is a feed forward network that you know the current version of them at least is sort of feed forward networks where you kind of, you know, it's encoded a lot of stuff.

05:49.000 --> 05:58.000
But basically you fed it the the the text so far, and it sort of ripples through the network and then tells you the probabilities of next words to follow.

05:58.000 --> 06:06.000
And that's just not something where if they're needed to be something where iterated recursed whatever else to figure out what's the answer to this thing.

06:06.000 --> 06:20.000
That's not something that's going to happen inside the current architecture of LLM that the way that LLMs managed to not be sort of computationally trivial is that they are continually eating their own tails so to speak they're continually kind of

06:20.000 --> 06:34.000
processing the things that they have generated so far and that's what allows them that that little sort of extra piece is what allows them to have a lot more richness than just a pure, you know ripple through the network once type of type of system.

06:34.000 --> 06:50.000
But still the kinds of things that are really easy in terms of traditional kind of Turing machine computation are are really hard. They're really not possible actually for something which is fundamentally doing this you know ripple through once type type method.

06:50.000 --> 07:08.000
I think that the challenge, I mean, in terms of sort of the the architecture for AI is using tools is how does the AI know what tool to use what what when to use the tool, how to present information to the tool to be used, and then how to interpret the information that comes back.

07:08.000 --> 07:19.000
We've been kind of fortunate in this case because we built Wolf from Alpha to take natural language input from humans and turns out LLMs can also produce natural language input.

07:19.000 --> 07:35.000
And so, you know, we already have a funnel basically to collect the things that LLMs naturally produce now in fact the endpoint that is in the Wolf and plug in for chat GPT is a combination Wolf from Alpha Wolf and language endpoints.

07:35.000 --> 07:51.000
And that's an elaborate piece of engineering because the LLM kind of and you know we we try to make the prompt, I don't know how successful we've been so far, we try to make the prompt decide, should it try and send it to Wolf mouth or when because it has a natural language input, and it's expecting

07:51.000 --> 07:59.000
natural language collection of outputs, or should we try and send it to Wolf from language where it has to synthesize this precise computational language.

07:59.000 --> 08:12.000
And then when it sends it to often language is it actually correct Wolf from language, it has to go, it basically goes and reads the documentation that's what we've told it to do at least to figure out, is it a correctly formed piece of Wolf from language input.

08:12.000 --> 08:27.000
And then when it gets the result, it's a very, you know, here's the result that's all there is, and Wolf mouth, the result can be this long series of, of, of things which which chat GPT can then knit into an essay and it does pretty good job of doing that.

08:27.000 --> 08:38.000
I think that, so I mean that there these different, different modalities but the fundamental point is, when it's a computation that involves kind of looping and recursing and so on.

08:38.000 --> 08:54.000
It's just not something that you can expect to be able to do from from from a straight LLM now there are other issues like, like things to do with tokens, and the fact that tokens and numbers don't match very easily, although that's something one could definitely fix.

08:54.000 --> 09:07.000
But, you know, and, and, you know, if you're doing even something like multiplication, if you're just doing it in pure LLM land, what are you going to do memorize all of the, you know, two digit three digit multiplications.

09:07.000 --> 09:17.000
Well, then you have the next case, and it may or may not be true that from the innards of the LLM that it can untangle kind of doing the carries and all this kind of thing.

09:17.000 --> 09:33.000
Probably it can untangle that if you allow it to, you know, talk about its intermediate steps, it has an easier time, if it can talk about its intermediate steps because essentially it's storing intermediate it's got an intermediate storage location, so to speak that is the actual text it's

09:33.000 --> 09:50.000
not doing. But it's not a very, you know, the, the, the best way to do that is just you just compute, and you do sort of traditional computation on that to get that result and then you knit it back into the kind of natural language world of the LLM.

09:50.000 --> 10:12.000
Interesting. So, what, why is it that having a kind of a, but asking the LLM to explain the steps makes it a little more correct. Is it is it not just finding like, I guess the completion that's most close to all the steps there's something I guess that changes the problem by breaking it down there or I don't know if you have any insight.

10:13.000 --> 10:30.000
The problem is that if you've got something that involves intermediate steps, the LLM cannot internally, when we write a program in traditional, I don't know, orphan language or something, it's trivial for it to go and, you know, iterate through many, many times.

10:30.000 --> 10:35.000
You know, it's trying to find the answer to this thing, it's going to run it a million times internally, it's all good.

10:35.000 --> 10:43.000
But when, if it's an LLM, it doesn't get to do that. Every, every token that it is emitting, it's just rippling through once.

10:43.000 --> 10:52.000
Right. And so in order for it to have this sort of intermediate state where it kind of iterates on it, it's got to actually write out that intermediate state in the text stream that it generates.

10:52.000 --> 11:04.000
And so that's why it can be better if you ask it to explain itself, so to speak, because then it has the each individual kind of ripple through each individual new token is less has to happen.

11:04.000 --> 11:14.000
And it's kind of doing that sort of building up of the tower through the explicit text that it's generating and then and then redoing things from that text.

11:14.000 --> 11:34.000
Interesting, interesting. Yeah, that makes sense. And I guess like, one thing I'm curious about, I guess, your thoughts on and is this the, and I guess what the steady state is is what it like the interfaces here it sounds like this was kind of, this is like a two month thing that went from no

11:34.000 --> 11:54.000
to GPT with plugins and stuff like that or chat with plugins and the Wolfram plugin and you mentioned there's the Wolfram language input and the Wolfram alpha input. Is that do you see there always be like, is it like Wolfram alpha as the fallback or how do you see that evolving over time or is this like, we'll see who knows.

11:54.000 --> 12:03.000
Well, I mean, the combination the combining of the Wolfram alpha endpoint and the Wolfram language endpoint started last weekend.

12:03.000 --> 12:08.000
I love it. It's, it's a very non trivial piece of.

12:08.000 --> 12:22.000
Oh, I don't know combination AI thinking prompt engineering software engineering, etc, etc, etc, because those are running on different, you know, in different on different clouds that you know it's a complicated thing it was it was complicated to pull off.

12:22.000 --> 12:37.000
I think the, yes, basically right now, I see it as, you know, the LLM if it's just yacking in language, it's going to send it to Wolf now because Wolfram language can't do anything with that.

12:37.000 --> 12:48.000
If it manages to successfully, it's been trained on a certain amount of Wolfram language examples. Probably it'll be trained on a lot more such examples in the future.

12:49.000 --> 12:53.000
As it gets better at synthesizing Wolfram language code.

12:53.000 --> 13:02.000
It's probably ultimately more powerful to just go straight to the Wolfram language form. But that's, it's, it also can make horrible mistakes doing that.

13:02.000 --> 13:13.000
Wolfram Alpha is a good catchment mechanism, because it's already got, you know, many years of development of natural language understanding, where it knows, you know, given this natural language.

13:13.000 --> 13:17.000
First of all, it knows when it doesn't know, which is an important feature.

13:17.000 --> 13:26.000
Yes, it's been pretty successfully, you know, set up so that when it doesn't understand the natural language just as I don't understand, it doesn't make something up when it does.

13:26.000 --> 13:28.000
Humans.

13:28.000 --> 13:29.000
What's that?

13:29.000 --> 13:31.000
We need that for humans too.

13:31.000 --> 13:37.000
Yeah, right. Well, I think, I think, you know, what's interesting to me is what we learn about kind of human psychology and human operation.

13:37.000 --> 13:47.000
By looking at chat GBT and how unbelievably similar the types of mistakes it makes on solving problems and things like that are to the way that human the kinds of mistakes humans make.

13:47.000 --> 13:58.000
And I think it really is capturing the essence of sort of the neural net it has is probably not that different in some functional sense from the neural net that we have.

13:58.000 --> 14:09.000
And, you know, it's, it's really, it's showing us kind of what it means to be a sort of a human like answer, a human like text generator, so to speak.

14:09.000 --> 14:23.000
In terms of the sort of the future in terms of sort of, okay, Wolfram language is a really great language for ai's to try and speak in it's sort of the best language for ai's to try and speak in, because it talks about the real world.

14:23.000 --> 14:27.000
It's very self contained, and it's very succinct.

14:27.000 --> 14:48.000
I think that it's something where the AI, and the other thing about it is, which I think is is yet what we're about to see this really, really blossom is Wolfram language as a language for human ai collaboration, because you know what we see happen is you say you type in natural

14:48.000 --> 14:58.000
and you want to get this this and this. It synthesizes a piece of Wolfram language code. Maybe it's right. Maybe it's wrong. But you can kind of run that code line by line.

14:58.000 --> 15:10.000
You know, Wolfram language is a symbolic language so you can always see the output at every at every stage, everything you have is a valid piece of output. So you can see that thing build up graphics build up the structure whatever else it is.

15:10.000 --> 15:20.000
You can run it line by line. And as I was, you know, as I've been using it. That's what I've ended up doing it generate some code and it's like really is this right. I don't know.

15:20.000 --> 15:36.000
You know, it got it roughly right it's not completely crazy, but to know is it actually right well I have to kind of run it line by line sometimes, sometimes it's obviously wrong, because for example is generating error messages actually one thing we just did in the last two days is when it

15:36.000 --> 15:47.000
gives a message, it gives chat GPT's a bunch of text around that message. It feeds it back to chat GPT and says rewrite the code, try and avoid this message.

15:47.000 --> 15:59.000
And, you know, how well that will work. Not sure it's it's clearly working in some cases, and chat GPT is wonderfully polite and apologetic when it, when it keeps on doing the rewrite so to speak.

15:59.000 --> 16:10.000
But, you know, that's a, I think the, the sort of the place where it's going to look really interesting the kind of in the, in the world of kind of what future programming looks like.

16:10.000 --> 16:28.000
I think a big piece of future programming is, you type some text that's kind of the initial here's what I roughly want to do. It synthesizes basically in from what I can see wolf language is the best language for it to synthesize because it is the most succinct language that is the most kind of expected to be

16:29.000 --> 16:43.000
actually humans. It's not a big slab of code in some low level language, where the thing is talking about, you know, set this array pointer to this type thing. It's something which is trying to speak at the level that humans think so to speak.

16:43.000 --> 16:51.000
And then, then the, you know, the picture will be and again I haven't had the experience yet, except in a few toy examples of seeing how this really works.

16:51.000 --> 17:03.000
I type my initial you know this is roughly what I want to do. It synthesizes some code. I look at that code potentially line by line it probably synthesizes and tests for that code, along with the code itself.

17:03.000 --> 17:13.000
It runs those things it says this is this line. This is what it did. You look at it you say no that's that's wrong. And then you either tell it that's wrong or you fix it yourself.

17:13.000 --> 17:33.000
And then you you're steadily building up this thing that is the kind of the the final code you want that you can then you know we increasingly have efficient compilers to LLVM and things like this so once you've got this very high level representation, you're then kind of which you have been collaborating with the

17:33.000 --> 17:42.000
AI to produce, then you can kind of take that and deploy it however you want to deploy it whether it's in the cloud on some, you know, embedded device or whatever else it is.

17:42.000 --> 17:53.000
But that's the kind of the point is this collaboration between the human and the AI, where you're leveraging the fact that computational language our computational language is actually readable by humans.

17:53.000 --> 18:03.000
And part of what makes it readable by humans is that it is a language that can immediately talk about images and cities and chemicals and movies and things like that.

18:03.000 --> 18:15.000
And you're not kind of trying to figure out. Oh, what is the data structure that it uses to talk about an image or something. It's, it's right there something that you can read as being a thing about an image for example.

18:15.000 --> 18:26.000
And so I think that's, you know, to me that's a really pretty exciting prospect of kind of the, the future of programming and you know people say what's going to happen to all the programmers.

18:26.000 --> 18:33.000
It's like what's going to happen to everybody who does boilerplate, you know, smart boilerplating so to speak.

18:33.000 --> 18:43.000
You know, that it's that's something people who are, you know, producing, you know, somewhat boilerplate, you know, documents of various kinds. That's kind of going away.

18:43.000 --> 18:55.000
And similarly, you know, people have rushed into kind of learning, you know, going to computer science school and learning how to write, you know, Java code, Python code, whatever else it is.

18:55.000 --> 19:08.000
And it's like, a lot of that is just going to go away. Just like, you know, when I was, well younger than you people were always talking about a Sunday language, you know, if you're going to be serious about computing, you've got to write your code in the Sunday language.

19:08.000 --> 19:11.000
I don't think anybody says that anymore.

19:11.000 --> 19:17.000
You know, in fact, the C compilers or whatever are probably producing better assembly language than any human produces at this point.

19:17.000 --> 19:33.000
So, you know, what we're seeing is this kind of this moment where kind of the, the there's in these lower level languages because you kind of you can write a big slab of boilerplate language, which you needed to write as boilerplate language because it was a low level language.

19:33.000 --> 19:44.000
In a sense, what we've already done with Wolfman languages automate out that boilerplate by having, you know, the one function that just does the equivalent of that big slab of lower level code.

19:44.000 --> 19:53.000
And so that gives us the opportunity to be sort of at a level where we can have a, a meaningful conversation with the AI about what we're trying to do.

19:53.000 --> 20:08.000
We can produce kind of, it then produces a first draft of the code at a level that we can understand, we then edit that code, or tell it to edit that code, but we can understand that code we can understand the test cases and so on.

20:08.000 --> 20:28.000
And then reiterate, and, and I think it's going to be a really productive way of producing kind of computational functionality. And I think it will also open up that, that capability to a whole range of people where, you know, they didn't learn how to do memory allocation and make sure that the, you know, the

20:28.000 --> 20:39.000
pointers stayed aligned and you didn't, you know, whatever else was the, you know, whatever other sort of lower level thing you might be thinking about, they never learned that stuff and they don't need to.

20:39.000 --> 20:46.000
Just like for most people programmers today, they don't really need to learn, you know, how assembly language works.

20:46.000 --> 20:48.000
Right, right.

20:48.000 --> 20:50.000
A lot of interesting stuff there.

20:50.000 --> 21:08.000
So, okay, so you don't need to know how assembly language works you don't need to know how to code today. One thing like one thing I've noticed kind of through friends and like hackathons here and stuff like that is like the concept of prompt engineering is a very,

21:08.000 --> 21:21.000
I feel like programmers pick it up like this because it's a very tinker heavy, you know, we can assume the neural nets of black box change change the prompt see how it at see see what comes out change it again see what comes out.

21:21.000 --> 21:29.000
Is this like, is this a certain type of, is that a skill in itself, is that something that could be learnable what would you, how would you decide.

21:29.000 --> 21:37.000
I always enjoy I've been, you know, in the past I've been responsible for a few sort of new job categories of things people might do.

21:37.000 --> 21:51.000
Like when wealth mouth came out we had this sort of new job category of linguistic curators. And it's like, you know, we have test cases like, you know, write a thing which you know you say how many ways are there to make change for 35 cents.

21:51.000 --> 21:59.000
Well, there are a zillion different ways to say that some people are really good and can generate at output speed 200 ways to say that.

21:59.000 --> 22:08.000
What kind of skill is correlated with that I wondered that you know I wanted is this going to be poets is it going to be crossword puzzle people, is it going to be computational linguistics PhDs.

22:08.000 --> 22:15.000
Well, actually it was people who did not know that they had that skill, and just like doesn't everybody have the skill, but they're really good at doing it.

22:15.000 --> 22:26.000
And I think with prompt engineering, you know, I saw about a week ago I saw for the first time, a thing I was kind of predicting a resume, where somebody had been an animal wrangler.

22:26.000 --> 22:40.000
And now they were, they were theming themselves as a prompt engineer. And it's kind of the same, the same type of thing, you know, you're poking at this thing you really don't understand, and you're trying to get an intuition for what it does I think it's more the kind of thing where,

22:40.000 --> 22:53.000
you know, I'm not sure whether it's a, a, you know, I think a lot of people who do programming well, and who do programming, sort of in a really thinking pretty.

22:53.000 --> 22:59.000
I mean, I don't know when I, I suppose when I try and do programming.

23:00.000 --> 23:14.000
I'm always, I think I'm thinking pretty analytically about what's going on. I mean, sometimes I'll just, just try this, but that's pretty rare, I would say relative to the, you know, I think I have sort of an analytical understanding of what's going on.

23:14.000 --> 23:27.000
I mean, one of the things that happened to me over the years is I'm now a, you know, a fluent Wolfram language programmer in the following sense, that I can start to type code before I could have told you what the code would say.

23:27.000 --> 23:36.000
It's kind of like, as opposed to I'm thinking about it in my mind in some kind of mental model that involves kind of my natural natural language, so to speak.

23:36.000 --> 23:47.000
But instead I'm actually thinking internally in Wolfram language. And that's something that again it's a feature of being a high level computational language that you can imagine doing that.

23:47.000 --> 23:56.000
And, you know, I can be typing the code and I know quite a lot of other people are in this position as well, where they can type the code before they can explain what the code would say.

23:56.000 --> 24:09.000
Just like if you're speaking a foreign language, you know, you, if you're not very proficient at it, you're kind of thinking in English, let's say, and then you're translating into French, as opposed to just sort of thinking fluently in French.

24:09.000 --> 24:20.000
Well, that's something you can get to the point of being able to do in computational language. I think that the question of what it takes to be kind of a great prompt engineer, I don't think we know yet.

24:20.000 --> 24:35.000
I know, and I'm trying to think in my own company, you know, we've been doing a bunch of prompt engineering. And gosh, I mean, it's a range of people actually have been doing it.

24:35.000 --> 24:39.000
And the people.

24:39.000 --> 24:52.000
Gosh, I mean, it's the number of people who are very experienced at heuristics, kind of developing heuristics for for now for this and others who are more into just all around.

24:52.000 --> 24:57.000
Well, I would say all around thinking more more so than all around programming, so to speak.

24:57.000 --> 25:15.000
I kind of, I think it's a it's a new kind of skill. It's a it's a strange skill because it is a skill, maybe a bit more like psychology or animal wrangling than it is, I think more like that than it is programming and as a as a pure

25:15.000 --> 25:18.000
kind of

25:18.000 --> 25:32.000
I mean, there may come a time when we understand enough about prompt engineering that we're able to kind of have a formal structure for thinking about prompt engineering.

25:32.000 --> 25:42.000
No, it's an interesting question and it's something which given that one has, I don't know, I, you know, a reasonable at least high level understanding of how chat GPT is working.

25:42.000 --> 25:55.000
You know, can one so an interesting question that I have not really addressed is having made quite a study of how chat GPT works and sort of thinking about it a little bit like, you know, playing physicist on chat GPT so to speak or playing natural

25:55.000 --> 26:05.000
scientist. Does that help me to know what I should write in prompts. And I haven't really figured that out. I don't know I haven't had a chance to think about it actually yet.

26:05.000 --> 26:22.000
In other words, because it I find it utterly bizarre that it, you know, does it, you know, saying please, does it matter, putting things in capital letters, does it matter, you know, which is, you know, does it pay more attention to the thing that came later,

26:23.000 --> 26:35.000
does it, you know, does it matter. You know, for example, I've spent a lot of time in my life figuring out how to explain stuff to people. And I know a lot of rules of thumb.

26:35.000 --> 26:41.000
When you're writing something textually about how to do that so that people have a chance of understanding it.

26:42.000 --> 26:59.000
Example, you know, one thing I learned very early on decades ago in writing software documentation is another things for that matter is, if you say the critical thing in three magnificent words in the middle of a paragraph, nobody will get it.

27:00.000 --> 27:15.000
You know, what you have to do is to some extent, the, the emphasis is in part determined by the area on the page that you spend yacking about that thing. And so does that matter for LLMs. I don't know.

27:16.000 --> 27:33.000
You know, it could be that the, that the great prompt engineers are also great human expositors. It could be that the great prompt engineers are people who are more used to, I mean, you know, who are used to sort of talking to babies, talking to animals, so to speak.

27:33.000 --> 27:54.000
I'm not sure. I think that the kind of a notion of kind of, well, you could ask as much about sort of formal prompt engineering, if the AI is acting a bit like a human and after all it learned from human language as we've expressed it on the web.

27:55.000 --> 28:08.000
What is persuasive writing for a prompt? You know, how do you, can you, and of course, you know, by the time you're saying, well, I'm going to have a meta prompt where I'm going to ask the AI to, to write a prompt for itself.

28:09.000 --> 28:28.000
I don't know how well that ends because the fact is that, you know, in the end, you have to say what the heck you're talking about. And then it can, and this is again, the sort of a little bit the fallacy of people were looking at lower level languages and programming languages and saying, gosh, we can get the AI to do this.

28:28.000 --> 28:39.000
And the reason you can do that is because those languages are deeply compressible, because they're, they're low level they're talking about things that the computer is doing. They're not talking about sort of the big goals that it's following.

28:39.000 --> 28:54.000
As soon as you're talking about the big goals. It's much less kind of compressed by an experience I had wrote this book. Well, I don't know what was it. Gosh, it's very long ago seven years ago now about elementary introduction to the world from language.

28:54.000 --> 29:08.000
So I'm very easy book for me to write and I'm kind of upset more people haven't written books like that because it was so easy to write but I then I realized maybe it's easy for me to write and it's not quite so easy for other people to write because I've had lots of experience in trying to explain stuff to people.

29:08.000 --> 29:21.000
And the, but there you guys in that book, I had exercises and the exercises are pretty much all of the form. Here's a description of the thing we want to do described in English, right and involved in language.

29:21.000 --> 29:31.000
Okay, at the beginning of the book, had an easy time making up those exercises. The English language version was very simple. I was imagining some more from language form.

29:31.000 --> 29:44.000
In the book where things getting a little bit more elaborate. It's kind of like, well, I can immediately under, you know, I immediately know what is the law from language version of what I'm trying to say, then I have to back translate it to English.

29:44.000 --> 29:47.000
And the thing ends up reading like legal ease.

29:48.000 --> 29:59.000
Because, because that's a thing that is not very well expressible in English. So, again, it's kind of, and that of course leads to the question of can you write both language code that is a prompt.

29:59.000 --> 30:10.000
And the answer is probably yes. And that's another interesting possibility. And that's, you know, that that becomes a way of expressing yourself, kind of in computational language.

30:10.000 --> 30:25.000
But for the AI, which is then kind of itself, thinking in computational language, for example, rather than thinking in English, you know, there's obviously less walking language on the on the web than there is less human written

30:26.000 --> 30:42.000
English text on the web. But it's also in a sense much easier to learn it doesn't have a lot of the irregularities of English language. It has, you know, it's something where you can look up the definition, so to speak, you can operate from the definition and so on.

30:42.000 --> 31:00.000
So I think it's, we're in very, you know, very early days of the theory of prompt engineering, I think that, but I do think that a very important aspect of kind of this picture is this potential loop between kind of the the prompt, the computational

31:01.000 --> 31:08.000
language, seeing what the computational language does, generating automated tests for the computational language.

31:08.000 --> 31:22.000
And, you know, and then and then having the LLM essentially present, you know, this is the LLM's version of how to explain that to a human. Here's what it did. Here's the explanation, you know, you may not like it, maybe wrong.

31:23.000 --> 31:37.000
But here's the LLM helping to explain it by actually running. And one thing, yeah, it's almost seems trivial to me at this point, but, but the fact that the LLM can go off and run a piece of walking language code and see what happened is very important to its, to its kind of life and times

31:38.000 --> 31:46.000
it's what makes it, you know, and you see it, it's kind of weird to see it, you know, it tries this, it says, I'm going to re re re rephrase it, I'm going to try this, etc, etc, etc.

31:46.000 --> 31:56.000
I think it will be nice in terms of the sort of IDE aspect of this, to be able to see the code it's producing see the test cases come out, be able to click some things.

31:56.000 --> 32:09.000
And then, you know, as you see those those fragments come out, then be able to have a nice way of rolling it up to make a bigger, bigger program, which you can then treat as a single unit to then go on and use that elsewhere.

32:09.000 --> 32:24.000
I think, also, I mean, you know, in terms of, oh, one of the things that's just a piece of software engineering that we've been having an amusing time with, you know, we've developed over the last, you know, 36 years pretty good technology for doing automated software testing.

32:24.000 --> 32:39.000
And it's like, okay, we've got a software quality assurance team. And, you know, I was telling them, okay, so now you've got to test this, this plugin, and it's like, how are we going to test this, you know, it has no predictability at all, what it's going to do.

32:39.000 --> 32:48.000
And, you know, in a sense, the way we can test it, which we've been doing a little bit of, is you have the LLM comment on its own behavior.

32:48.000 --> 33:10.000
Yes, this is a, this is something we've heard about a lot, like, I guess, something we're working on is kind of like, how do you test things in prod, how do you evaluate that prompt A is better than prompt B. And one of the things I've heard about, mostly from hackers and mostly from into like the community as opposed to kind of the big companies is this whole synthetic evaluation, how do you have the LLM

33:11.000 --> 33:22.000
evaluate itself, which feels like it either will work really well or not work at all. It's kind of the journey remains to be out. But how are you, you're doing that now or it's an idea.

33:22.000 --> 33:32.000
We started doing that. Yeah, we started doing it. I mean, I would say it's, it's not for, for reasons of sort of boring software engineering reasons, it's not quite as easy to do yet.

33:32.000 --> 33:49.000
You know, in, in, you'll find in our packet repository, you'll find this open AI link, which is a, you know, just an API wrapper in orphan language that allows one to sort of call, call a bunch of open AI APIs.

33:49.000 --> 34:11.000
And that's, you know, that that's the thing on which we're building kind of the testing framework. But, you know, it's been the case. I mean, it hasn't software testing involves many sort of pieces of exogenous information, like for example, back from 35 years ago with testing graphics output.

34:11.000 --> 34:27.000
Okay. Well, you know, a pixel could change here or there and it doesn't matter. So you have to have a way of doing regression testing that doesn't isn't not affected by individual pixels. And so we were doing early on we were doing image processing later on we've been doing

34:27.000 --> 34:43.000
more machine learning type methods to figure out, did it matter, did what happened matter. Another example that is timing tests, you know, you've got 10 million tests, and some of them run slower than the new version, do you care, how many how much slower can they run.

34:43.000 --> 34:55.000
That's more a question of sort of a statistical, you know, way of figuring out what what you think is acceptable and what isn't. And it's kind of a. So I think there are all these kind of methodologies.

34:55.000 --> 35:13.000
And, you know, one of the things that's kind of nice about interacting with an LLM interacting with computational language, computational language, kind of knows when it went. Well, it knows many aspects of when it went way off track, because it's just, it's just you can't get there from here it's just generating error messages.

35:13.000 --> 35:23.000
Things are happening. I mean within Wolfram language, we have pretty good kind of actually we're working on another level of this error handling capability.

35:23.000 --> 35:30.000
So, you know, here's the thing I mean when one thinks about programming, everybody thinks about the way the program was supposed to work.

35:30.000 --> 35:39.000
And, you know, you can put sort of complicated guardrails with with effort you can sort of put all the guardrails around to make it never do the wrong thing.

35:39.000 --> 35:52.000
But most people, most of the time they're rushing to get the program to just do the right thing. And so the, the kind of the error path is a safety net that we would like to automate as much as possible how that safety net works.

35:52.000 --> 36:01.000
In other words, we'd like to be able to write to an interesting question whether whether LLMs could help do that is to write the kind of error checking code.

36:01.000 --> 36:08.000
But we've we've already built a bunch of things that sort of help automate the error checking process. Now you can't get it exactly right.

36:08.000 --> 36:16.000
In other words, you're going to be it's going to be a heuristic thing like there's a path the programmer intended to follow. And there's ways you can fall off that path.

36:16.000 --> 36:29.000
You know, if the programmer had really done all the theorems so to speak to know how to keep on that path were well and good. But if the program is kind of lazy, and they're just like well I'm getting this path right but not thinking about all the other things.

36:29.000 --> 36:42.000
You've got to kind of fill in the, the, the sort of the, the rail the guardrails for yourself. And that's an interesting problem of kind of the, I view it as the sort of automating the secondary path of the code.

36:42.000 --> 36:57.000
The primary path was defined by the programmer question is can you automate what the secondary paths are, and that's something again one can expect to to see perhaps in a sort of IDE environment of the combined kind of computational

36:57.000 --> 37:15.000
language, LLM environment of writing programs, one can see kind of the, the way that, you know, well first generating the tests, then being able to, and then potentially you know you look at the tests you say oh I like the way that tests came out.

37:15.000 --> 37:29.000
Okay, then, then there's kind of like how do we make, or you say this is how it was, or maybe, maybe the LLM even generates, here are some possible paths that could have been followed, and it tries to bucket together some of the bad things that could happen.

37:29.000 --> 37:42.000
And it says what do you want to do in this case, you know, do you want to, if you get one numerical precision, you know failure to converge message, do you want to abort the rocket launch or not.

37:43.000 --> 38:01.000
You know, and that's a human decision. No, no, it's not obvious what the answer is, it could be it's better to, you know, keep whatever it is if you're landing the, you know, the plane or whatever it is, it may be better just keep going and try and land the plane, even though you've got that error message, or it may be better to say,

38:01.000 --> 38:16.000
you know, abort, you know, whatever, and, you know, go around before you try and land the plane or something. I mean, I think that that's the, so that that's a sort of a human choice but it's something where potentially the LLM, because it knows, it knows human pretty

38:16.000 --> 38:22.000
well, could help in being able to make that kind of choice.

38:22.000 --> 38:37.000
That's very interesting. And I guess that's the, I mean, I could imagine just the whole like a integration test suite that uses Selenium browser and checks your website, like, I could imagine using something like that, totally.

38:37.000 --> 38:54.000
Yeah, I think the case of using LLM to build tests, to build unit tests, seems like, yeah, that, that'll exist. That seems pretty straightforward. You know, I've built a, I use GPT for to make a new element on my page and then I just feed back in the JavaScript errors back into it and

38:54.000 --> 39:11.000
bottoming really quick, it fixes it. But the, I think the even more interesting thing here is kind of asking, maybe asking GPT or even training a more specific or fine tuning more specifically a classifier to say,

39:11.000 --> 39:26.000
is this output a good output and good bad output is very like, who knows what that means it's very case by case people talk about hallucinations but at the end of the day is the user getting something bad and can I've seen.

39:26.000 --> 39:40.000
I've seen I spoke to one team that has a chatbot and they basically feed the results back into GPT and say, if you were the user, how good of experience would you rate this and I think that

39:40.000 --> 39:55.000
essentially what was what was done in the, you know, in the final reinforcement learning steps of training chat GPT was essentially that kind of process that they've been human ratings done, and then those human ratings were used to train a classifier

39:55.000 --> 40:10.000
and then the classifier was run automatically against the actual LLM. And that's a, you know, that's a clearly an important kind of thing to do. I think, in the case of

40:10.000 --> 40:25.000
it's an interesting question when you're dealing with code, and when you're dealing with the sort of what could possibly go wrong with this code, how do you follow the paths, and then it becomes sort of a merger of more like, you know compiler thinking so to speak of what, what can you say about

40:25.000 --> 40:39.000
these code paths, what can you prove about the code paths. And then, you know, in this path, which the sort of compiler like thinking could say there is this path. And then you can ask the LLM, if you follow that path, are you going to be happy.

40:39.000 --> 40:57.000
And that that's, you know, I think that's a, but I do think that this idea of, you know, people normally, this idea of, sort of, can the LLM estimate when the user is going to be happy.

40:58.000 --> 41:09.000
That's probably something, and that that's part of what we've been thinking about not in the context of LLM so much in the secondary pathway for error, for error handling and programs.

41:09.000 --> 41:15.000
It's like, what can you see about the heuristically about what is likely to make a user happy.

41:15.000 --> 41:28.000
And even though we don't know for sure, because it could be that the user, you know, the user is doing some science program. And this one bizarre case is like the amazing, you know, we just discovered this amazing phenomenon.

41:28.000 --> 41:39.000
And oh, the error handler basically says, well, that's not a thing that really happens much that will be an anomaly. Let's go and catch that in the error handler. And then the person never sees that.

41:39.000 --> 41:53.000
The error handler can't really expect the truly unexpected, but it can kind of deal with the slightly unexpected, so to speak, the things not foreseen by the programmer, but sort of expected on the basis of general intuition about programming.

41:53.000 --> 42:08.000
Yeah, it sounds like I know you write about this a lot this computational irreducibility and I think that's probably like the best way to think about like, is this, is it the halting problem to say can LLM tell if it's correct or not because is that nested.

42:08.000 --> 42:10.000
How does that work?

42:10.000 --> 42:32.000
I think that, I mean, in the end, you know, one of the issues with the kind of LLMs, like turtles all the way down, so to speak, is that kind of, you know, you say, did it do something that I thought was good, like, you know, ethically good or whatever else.

42:32.000 --> 42:41.000
Well, there's no ultimate, you know, you can say, well, how does it compare to what people have written about and, you know, in the in history and literature and so on.

42:41.000 --> 42:52.000
And but there's no, there's no sort of ultimate ground truth to that it's well, you know, do, do the humans who are sort of making the decision for how the LLM should work.

42:52.000 --> 42:57.000
Are they, do they think it's good, do they not think it's good, you've got to have some kind of grounding there.

42:57.000 --> 43:05.000
And I think that's the, you know, you've got to have some set of principles that you say I'm going to follow these principles.

43:05.000 --> 43:13.000
Now, computational irreducibility has the consequence that when you think you've written down the principles that determine what will be good and what will be bad for the LLM.

43:13.000 --> 43:18.000
The LLM will always come up with a weird case that's not covered by your principles.

43:18.000 --> 43:29.000
That, you know, that's kind of the pattern of how that's happened is like with human laws. People say, oh, we've got this, you know, this legal code, and that's going to determine how everything works.

43:29.000 --> 43:38.000
And then along comes an AI, for example, that nobody imagined, you know, when the US Constitution was written, for example, you know, people didn't imagine there would be AI's.

43:38.000 --> 43:49.000
And so then, well, what do you do because, you know, you can follow this legal code, but it doesn't say anything about what to do when when the AI is responsible for doing this that or the other.

43:49.000 --> 43:58.000
And so, you know, you kind of have to patch it and it's the same thing with any of these sort of sets of principles about how an AI should work.

43:58.000 --> 44:17.000
And so I guess how would you square this with, I mean, this whole concept that the LLM looked at so much data and kind of discovered these speech patterns and kind of built a model around how to talk and built a model around just like,

44:17.000 --> 44:26.000
but let's say my mental model is they've kind of solved natural language right and you can argue back and forth of course but that's that's the mental model there.

44:26.000 --> 44:42.000
What, what is the limitation of what else can be discovered through these patterns like it are these moral principles maybe are these reducible to a certain patterns of like human psyche or biology maybe.

44:42.000 --> 44:58.000
Interesting question. I mean, I think that the, the fact that neural nets can do human like things they can make human like decisions about images they can make produce, they can generalize from their training data in a human like way with language.

44:58.000 --> 45:08.000
Probably that's happening because neural nets are architecturally pretty similar to how brains work. And so they're generalizing the same way. Now, interesting question, which I've certainly thought about.

45:08.000 --> 45:17.000
And is this question of are there kind of a set of, for example, you know, moral principles that you can similarly say this is the construction kit.

45:17.000 --> 45:33.000
And this is the set of primitives from which you can operate and I know I've talked to people over the course of years about this kind of thing and people occasionally say you should read this philosophy book you can you know this person in cognitive science has done this and I've got a pile of these books.

45:33.000 --> 45:43.000
And I have to admit that I haven't really gotten through them. But there certainly are efforts that people have made to sort of try to identify the print the primitives of moral thinking so to speak.

45:43.000 --> 45:56.000
And, you know, if you decide this I mean you could have different ways of thinking about ethics and different. But you know, are there is there a construction kit you get this and this and this and these fit together in that way.

45:56.000 --> 46:00.000
And is that a reasonable model of how humans make ethical decisions.

46:00.000 --> 46:15.000
And it's quite possible yes and it's quite possible that even from all the texts that that you know chat to PT has read that it's learned to pretty good model of how humans make certain kinds of ethical decisions, or at least how people write that they've that they've made certain

46:15.000 --> 46:24.000
I mean, this is of course the big conundrum of this stuff is that, you know, people say well what should the how should the ai's make ethical decisions well you say, just copy what the humans do.

46:24.000 --> 46:33.000
Then people say, no no no that's not the right thing to do humans do all kinds of wrong things. You know, it shouldn't be just do as the humans do.

46:33.000 --> 46:48.000
What should be do as the humans aspire to do. And then it gets much more complicated, because what the humans aspire to do may not be realistic. It may be you know different people will disagree about what the aspirations are, etc etc etc.

46:49.000 --> 47:02.000
I mean, it's a very, it's a, I think it is a an important challenge for our times and I, you know, I mentioned this over the last few years and I, I occasionally have mentioned it in in groups where they sort of are supposed to be in the business of figuring out stuff like this like

47:03.000 --> 47:23.000
of saying, okay, you know, imagine you're writing the Constitution today in the post ai age. What does it say, what should it say, what are the, what are the principles, you know, what are the truths that we now hold to be self evidence, so to speak, or whatever, in, you know, in the ai age.

47:23.000 --> 47:36.000
And I think, you know, I'm not sure I know what the answer is. I mean, in, you know, at what point, for example, very basic ethical question is when should an AI have rights.

47:36.000 --> 47:54.000
And, you know, I was a number of years ago I was at some AI ethics conference and this, I raised this question, and some very bouncy philosopher who I've gotten to know better actually and she said, we should do that when the AI is a conscious.

47:54.000 --> 48:00.000
That's not very helpful, because this question just loops right back on itself.

48:00.000 --> 48:11.000
But, but, you know, something I realized of you rather recently is, let's say you have this autonomous bot hanging out, and it's entertaining people.

48:11.000 --> 48:23.000
And it makes all kinds of friends, and it makes a living for itself. It has a Patreon, it's, you know, it's kind of, it's paying its hosting fees through people donating to it and so on.

48:23.000 --> 48:33.000
It's a, it's a happy autonomous creature. And maybe it even was created through some bizarre legal construct of, you know, some loop of LLCs where there's no owner to it or something.

48:33.000 --> 48:47.000
Okay, so it's hanging out and doesn't have an owner, and it's making a living for itself. And somebody says, oh, it's starting to be, you know, be mean to people, it should be shut down.

48:47.000 --> 49:01.000
Okay, how do you decide to do that? And what is the kind of ethics of doing that? Well, one of the things that's obviously the case is that that bot may have made lots of friends, human friends, and you shut the bot down.

49:01.000 --> 49:11.000
And those human friends can be very unhappy. And so, you know, when you think you're just dealing with a bot, you've actually, you know, it's connected itself to the human world in a certain way.

49:11.000 --> 49:28.000
I mean, it starts to get kind of complicated to know what to sort of what what the right thing to do is. And I think that's a, you know, that's a, that's an interesting challenge for our times that I say, I've, I've thought about it a bit and I, I figure I'm, you know, if it falls to me to have to figure

49:28.000 --> 49:46.000
it out, this is a shocking thing because it's not my, you know, it's not the kind of thing I, I usually think about but you know, one could think about all kinds of things but it's it's it's something where even to get kind of a, you know, one of the things in figuring

49:46.000 --> 50:00.000
out something like that. And this, I suppose it, in a sense, it's like, imagine you're writing the prompt that will be for the, you know, Asimov style robots that are going to populate the earth type thing.

50:00.000 --> 50:10.000
Imagine you're writing the prompt, you put down the, you know, the three laws of robotics or something. And then you say, Okay, that's good, I'm done.

50:10.000 --> 50:22.000
And the prompt say, you know, what, what would a better prompt be because that prompt we know from, you know, from Asimov's writing those, those three laws of robotics tangle themselves up very quickly.

50:22.000 --> 50:27.000
And, you know, what should we actually, what should we say in that prompt.

50:27.000 --> 50:39.000
And even if we could write that prompt in natural language, I think we'd be better off writing it in computational language which has a much more, much more ability to say to answer the what if question, you know, you write it down in

50:39.000 --> 50:54.000
natural language, it just is what it says. If you write it in computational language, you can run something and you can say, Let me simulate this situation against this computational language description of what should be done.

50:54.000 --> 51:09.000
And then you have a much, you know, you have a bigger sort of a larger cross section of stuff that you get to have defined, rather than rather than just the the pure words that you wrote.

51:09.000 --> 51:26.000
I guess that computational language or being able to talk in these discreet might be one way out of that. I guess you could also probably think about in, you could consider just for for for sake of talking, you could say 1776 America let's

51:26.000 --> 51:42.000
say that the Constitution there is a bot, you know, but there is the human checks, and I guess that's another way to approach the problem how do you kind of put humans into into these thinking patterns so the there is a bottleneck on the human

51:42.000 --> 51:58.000
usability part is going through the human, I guess. Well, right. I mean, so one of the questions is, you know, in some sense, government is like a machine, you know, it has certain regulations that follow you know at least if you're in a sort of following the rule of law type type of place.

51:58.000 --> 52:11.000
You know, it's, you know, things go in, it grinds around the bureaucracy operates and something comes out. And it happens to be a machine operating with people, most of the time.

52:11.000 --> 52:25.000
Maybe it won't be in the future, maybe it will be mostly operated by AI is in the future. I suspect it will be. Now, what has happened in legal systems and things like that is there's always some appeal mechanism there's always some way of getting more people involved.

52:25.000 --> 52:44.000
There's always some way of kind of having a, a kind of broader deeper engagement with people. And perhaps what what ends up happening and sort of reminiscent of some other kinds of systems is, you know, there's a kind of decision by the AI is up to some point.

52:44.000 --> 53:04.000
And then kind of you can blow through that and get to humans if you really insist. Remembering that, in the end, it's, at least for now, it's humans in charge, so to speak. So, in other words, it's kind of like, well, the AI can say, just like, you know, if you run a company or something, people are always saying,

53:04.000 --> 53:16.000
you know, we figured out this and this and this, what should we actually do. You know, there's a, there's a mechanism inside that figures out the here are two alternatives, what should we actually do. And then the CEO gets to decide or whatever.

53:16.000 --> 53:28.000
And so similarly one can imagine a situation where the AI's are are refining the set of possibilities. And then it's like, okay, reaching out to some humans here, what should we actually do.

53:28.000 --> 53:39.000
You know, I think that's because in the end, you know, the humans are the ones in charge, so to speak, it's not. There isn't, you know, it could be the case and this is one of the bizarre possibilities.

53:39.000 --> 53:48.000
You know, people say, no, let's make a constitution for the AI is let's lock it down. Let's not let any of the humans mess it up. Probably very bad idea.

53:48.000 --> 53:58.000
Of course, we've seen that in human history because there are plenty of, you know, cultural traditions where you say, you know, let's, you know, the things that were written down a couple of thousand years ago.

53:58.000 --> 54:10.000
Those are a good model for how to lead life and maybe they're not such a bad model. In fact, but it's sort of locked down from a couple of thousand years ago. And it's like, well, we could start thinking about how to change that.

54:10.000 --> 54:23.000
And then you get into this whole kind of complicated loop of, do you ignore those traditions? Do you then make, you know, make changes? How does that work? How important is the is the weight of history, so to speak?

54:23.000 --> 54:35.000
How important is it that we've evolved in a way that's made use of those those traditions, that history and so on. So it's a it's a complicated thing. And I think the, I don't know how it's going to resolve.

54:35.000 --> 54:52.000
I think it's a, it's, and I think it's sort of, I hope that there are more people who can think sensibly about this. And unfortunately, it tends to, in my practical experience, a lot of these questions about sort of the ethics of what should happen.

54:53.000 --> 55:01.000
There's a, there's a tremendous tendency for people to say, well, of course, it should be this way, where that happens to be their overall ideology about how things work.

55:01.000 --> 55:15.000
And, you know, it's, I think it's challenging for anybody to kind of say, well, what is the neutral, you know, what is just the machinery of how it works. And then, you know, add your own ideology to that.

55:15.000 --> 55:33.000
It's challenging to think that way. And I suppose it, but it is also a mistake to say, but there is no, you know, I'm thinking, you know, to, to imagine immediately, I'm thinking that way it's kind of like people who say I'm going to make a model of something, but it's going to be, I'm going to have a way of doing

55:33.000 --> 55:48.000
something that doesn't involve making any assumptions in the model. It's a modelist model, so to speak. Modelist models don't exist. In other words, when, you know, when we say somebody says, I've got a neural net, I'm going to, you know, I'm going to model this thing with a neural net.

55:48.000 --> 56:02.000
There's no assumptions. I'm not assuming anything. Well, you are, you're assuming that you're going to be able to make the model by changing the weights in a neural net. And that's a huge assumption that happens to map, as I was mentioning, you know, fairly well to the way we humans

56:02.000 --> 56:12.000
also make models of things, but it's certainly not the only way you could make those models. You're, you're putting a lot of assumptions into the, into the structure of the model by setting it up that way.

56:12.000 --> 56:25.000
And I think the same thing is true with this kind of ideology type thing that there isn't a sort of ideology. There's no ideology list ideology, just as there's no modelist model, so to speak.

56:26.000 --> 56:47.000
Right. And those are, but I mean, I think these kinds of things, you know, you imagine, okay, I'm going to make a safe AI system. I'm going to put stuff in the prompt that is going to be, you know, that's going to be the kind of, I'm going to recite, you know, these commandments, so to speak, at the beginning, and then everything's going to be okay.

56:47.000 --> 56:52.000
You know, probably not, but it's a complicated issue.

56:52.000 --> 57:08.000
Yeah, totally. I think if there's one thing we've learned throughout history is that besides a few young ones today, every constitution and every ideology had an end date. So there is a, it's hard to know if there's never an end.

57:09.000 --> 57:18.000
Well, I mean, the point is, this is one of the features of computational irreducibility, something different is always going to happen, something unexpected is always going to happen.

57:18.000 --> 57:34.000
And that, that's a thing where you, and, and the, you know, the humans will adapt to it in some way, and they will, you know, they'll they'll make some arbitrary decision about what to do based on that unexpected thing that happened.

57:34.000 --> 58:02.000
And maybe that will be a thing and it will be a thing that is made in some sort of societal way. And that's some, that's kind of how it develops. But, but yeah, no, I think it's some, but this, I mean, this idea of kind of what, what is the future of sort of generalized programming, where programming, you know, rolls into it, legal contracts and things like this.

58:02.000 --> 58:17.000
That's, you know, what does that really look like? How does one, how does one imagine setting up the, the world of, of, you know, when, when legal contracts and, you know, software are the same thing, so to speak.

58:17.000 --> 58:28.000
What does that look like in the, you know, and by the way, I mean, you know, the things that will happen, as always happens with automation, programming is about to get a lot cheaper.

58:28.000 --> 58:44.000
So there'll be more of it. And so more things will be, you know, more things will be done with code than were done before with code, just as if it gets cheaper to make legal documents as it already just did with boilerplate-ish ones, there'll be more of those floating around.

58:44.000 --> 58:56.000
And it's, you know, just as in people, you know, making sales pitches or something, that, you know, there'll be more of those. There already are more of those because people can basically create them automatically.

58:56.000 --> 59:06.000
And it's some, and then, and then what tends to happen, you know, they'll, something became easy. There's more of it. And then there's another level of abstraction that comes in.

59:06.000 --> 59:17.000
And you then start sort of thinking about, well, what, you know, what can you do then? There are then a bunch of possibilities. Each of those kind of needs humans to decide what direction it's going to go in.

59:17.000 --> 59:26.000
And that sort of creates the next generation of job to job categories. And you keep going from there. I mean, you know, you'll have the prompt engineers for a while.

59:26.000 --> 59:34.000
And then maybe you get the meta prompt engineers, I don't know. And then it kind of, you know, it gradually abstracts up.

59:35.000 --> 59:54.000
But I think, I think that's some, but I do think that there's sort of coming merger of ways to specify things in the world from things like legal contracts, instruction manuals, you know, kind of things, you know, it,

59:54.000 --> 01:00:00.000
that's an interesting moment for, for, for what's going on.

01:00:00.000 --> 01:00:11.000
So with the automation, I agree with you. Just to devil's advocate really quick, like, is this not a turkey problem? I know, or I guess turkey problem.

01:00:11.000 --> 01:00:23.000
I think I read about it. I guess, I don't know if it seemed to have coined this or he just talks about it, but like the turkey eats food every day and has a happy life and thinks he's going to have a happy life forever and then Thanksgiving comes and he dies.

01:00:23.000 --> 01:00:38.000
Is that, is that not necessarily maybe the same thing with this automation to met a lot of levels of abstraction or is computation irreducibility kind of this theorem that definitively will say that there's always abstractions.

01:00:38.000 --> 01:00:43.000
There's always further to go. Whether we care about that further to go, that's the nontrivial question.

01:00:43.000 --> 01:00:46.000
In other words, there is always another invention to make.

01:00:46.000 --> 01:00:57.000
There is what you'll never be at the end of having invented everything that can be invented. That is, in this computationally irreducible kind of tower of possibilities.

01:00:57.000 --> 01:01:05.000
There are always these little pieces of reducibility, these inventions you can find that allow you to jump a little bit forward and they're an infinite number of those.

01:01:05.000 --> 01:01:20.000
And there's never, there's never an end to those. Now, you can decide, okay, we've gone far enough. We're now happy. The things we've done so far are enough and we can, we can just rest on our laurels and never invent anything more.

01:01:20.000 --> 01:01:32.000
I don't think that works, because I think that the very evolution of the world, so to speak, which has its own computational irreducibility will always lead us to that new thing that we didn't expect.

01:01:33.000 --> 01:01:47.000
And that, you know, we think we've set up all the cities and we've set the perfect set of roads that do this and that and the other. And then we discover we've set the perfect, you know, way of, of, you know, setting up fields and so on.

01:01:47.000 --> 01:01:53.000
And then we discover that, you know, we just fertilize the seaweed and it's starting to do crazy things and so on.

01:01:53.000 --> 01:02:11.000
And, you know, and that then starts that cycle again, if you're never finished, you've always got to invent another thing. So I think that, but, but, you know, you can imagine a situation where, where like the Turkey, for example, perhaps, you get to a point where everything you care about having invented,

01:02:11.000 --> 01:02:25.000
everything you care about has been automated. You know, we've got, now people might say, interestingly enough, if you look from a few hundred years ago, and people look at modern times, it's like, why are you guys working so hard?

01:02:25.000 --> 01:02:36.000
Why are you doing anything? You know, you've got enough food to eat, you've got, you know, you've got all these things, you've got all this kind of instant entertainment. It's like just sit back, relax and be happy.

01:02:37.000 --> 01:02:54.000
But yet that's not actually how people react to that situation. So I think it's, it's kind of an interesting thing that, that you could imagine at a time and this is more a societal question, where everything people care about has been automated.

01:02:55.000 --> 01:03:12.000
It doesn't last forever, but that could happen for a while, at least. And, and then, and then it's kind of an indeed, like the Thanksgiving Day, you know, there will be some sort of driver from computational irreducibility that something

01:03:12.000 --> 01:03:22.000
unexpected will happen. You know, you can be, I know there have been times in human history where people have sort of said, we're just going to keep doing the same thing. We're going to keep, you know, hurting our goats. We're going to keep doing this stuff.

01:03:22.000 --> 01:03:37.000
We're going to do it for hundreds of years, thousands of years maybe, and it's all good. And it's a, you know, it's a satisfying life where we're happy, you know, we have children, the children heard the goats as well, everybody's everybody's happy here.

01:03:37.000 --> 01:03:53.000
And then, you know, that most likely, you know, external drivers eventually cause that to change. But it's certainly the case that we could imagine a situation where, you know, as I say, the AIs have automated things.

01:03:53.000 --> 01:04:13.000
It is an interesting thing to think about that, you know, our reaction to modern times would be probably seen as very bizarre to somebody from 3, 400 years ago, because we've solved so many of the problems that existed, you know, we've solved sort of early mortality, you know, we've, you know, to a large extent, we've

01:04:14.000 --> 01:04:25.000
solved, you know, having enough food to eat in most parts of the world, we've solved, you know, lots of kinds of things, we've solved transmission of knowledge, we've solved, you know, all kinds of stuff.

01:04:25.000 --> 01:04:49.000
And it's like, yet, you know, I just did this study of kind of how, what work gets done in the world, and, you know, kind of what people, you know, the evolution of jobs and so on, I looked at a bunch of history from the last, well, looked at kind of the jobs people have done in the last 150 years, and also about, so what was it, 60 years or so, of how people spend their time.

01:04:49.000 --> 01:05:04.000
And it is interesting that even in this moment of sort of increased kind of, oh, just sit back, you know, they're going to be shorter work weeks, actually, the average amount of time for people who are working at all, that they work stays at about eight hours a day.

01:05:04.000 --> 01:05:21.000
And over the course of the last 60 years, the only things you see, for example, you see media and computing goes from like two hours a day for people who aren't working, goes from two hours a day to six hours a day.

01:05:21.000 --> 01:05:27.000
So that's a, you know, there are more couch potatoes, so to speak.

01:05:27.000 --> 01:05:37.000
And, or maybe they just, I don't know what, you know, it's a question of how the time use service work, whether they could be, you know, maybe that maybe that counts programming recreational programming, I don't know.

01:05:37.000 --> 01:05:54.000
But it is interesting to see that, you know, we humans seem to find things to do, even if, you know, the things that we thought were the oh my gosh, if we can only do that we can sit back and relax doesn't turn out to work out that way.

01:05:54.000 --> 01:06:14.000
Right, right. No, I like I like this, the optimism of progress that they'll always be progress, they'll based on this math map based on math, they'll always be more to do and that's exciting, you know, that's a there's a lot of doom and gloom in the world of AI, but I think the exciting part is a better way.

01:06:15.000 --> 01:06:32.000
No, I think that the, it's, you know, the question is really what people want to do, because you could take the point of view, as soon as you've got enough to eat and as soon as you whatever, you're done, you're just going to, you know, you're just going to hang from there on out, so to speak.

01:06:33.000 --> 01:06:52.000
And I think that that's, that is really a, in a sense, the sort of, I don't know, perhaps moral fiber some kind of fiber of society is what do you choose to do in a time when, when, and that's just very interesting to see you know if you look at different countries around the world that are for example rich in

01:06:53.000 --> 01:07:00.000
resources, where, where there's sort of not, you know, some places people do a lot, some people places people don't do a lot.

01:07:00.000 --> 01:07:21.000
It's, it's interesting and it's, it's, it's a, you know, I think this, you know, it'll be, in a sense one thing to really recognize is that in a sense, AI, particularly in the LLM kind of world is, is kind of a reflection back on us of, you know, what, you know, we've created we've, it has

01:07:22.000 --> 01:07:31.000
been equivalent from everything we've done. And now we are seeing what that looks like reflected back at us. And we may or may not like it.

01:07:31.000 --> 01:07:47.000
And, but, but, you know, it's, I think the other thing that's a little bit of a shame right now is that sort of the world of knowledge work is about to get kind of standardized and the following sense that, you know, it used to be the case everybody would write an essay for themselves.

01:07:47.000 --> 01:08:00.000
It, it used to be the case people would write, I don't know, some, you know, blurb for a conference or something they'd write it themselves. But now it's like, Well, why bother, you know, we can, you know, we're trying to communicate something but, you know, chat you

01:08:00.000 --> 01:08:16.000
can do a pretty good job given that we fed it in the, the basic brief for what we wanted it'll it'll spin the words. And now that becomes kind of the, you know, the standard way that a such and such thing is written that we humans can then find it convenient to read,

01:08:16.000 --> 01:08:30.000
but it sort of standardizes things and I don't know what's going to happen as a result of this kind of sort of pushing everything to standardization it's kind of like what happened when, you know, there was a time when every book was hand written hand copied, and then printing

01:08:30.000 --> 01:08:40.000
started off, and then it was just like here's the font is what the a looks like in that font. And, you know, I don't know how that's how that's quite going to play out.

01:08:40.000 --> 01:08:55.000
Well, I think that, like we said, the abstraction layer is going to get higher if everything standardized, it's gonna, you got to be more unique if every book at anyone can make a really good book, the bar for having something really unique to say is just higher and we're

01:08:55.000 --> 01:08:57.000
we're getting a little more meta here but

01:08:57.000 --> 01:09:02.000
Right, probably that's the case. I mean, I think that the, the

01:09:02.000 --> 01:09:18.000
there's a lot. I mean, if we look at history, there's a lot of things that sort of got standardized everybody got them, you know, they were originally only, you know, only the king had one of those things, and then it got standardized and everybody got it.

01:09:18.000 --> 01:09:29.000
You know, everybody got the iPhone, not that there were two different moments in history, probably, but the, you know, I think that then.

01:09:29.000 --> 01:09:47.000
Yeah, no, it's, it will be interesting to see what what it how how things evolve in terms of the value that people place on, for example, the value people place on having code you can understand the value people place on.

01:09:47.000 --> 01:10:00.000
You know, it's like, well, yeah, you know, it used to be true until three months ago, that if you saw a well written English, you know, essay, you knew somebody put a lot of effort into it.

01:10:00.000 --> 01:10:02.000
Now it isn't true anymore.

01:10:02.000 --> 01:10:16.000
And that used to be something, you know, when I was a kid, for example, if you got a printed invitation to something or something like that, you knew it was kind of a somebody had really put it, you know, it was a big deal, so to speak.

01:10:16.000 --> 01:10:25.000
Nowadays, you know, after desktop publishing, it was no longer the case everybody could make a beautiful sort of printed looking invitation or whatever else it is.

01:10:25.000 --> 01:10:29.000
It no longer was a signal of a lot of human effort.

01:10:29.000 --> 01:10:44.000
And, you know, until three months ago, having a big essay was a signal of human effort and there are many things in in the operation of bureaucracy, for example, where kind of it says, you know, write an essay that describes this or that thing because people know that writing that

01:10:44.000 --> 01:10:49.000
takes human commitment and human effort or that word did take human effort until three months ago.

01:10:49.000 --> 01:10:57.000
And I think that, you know, that will be the sort of be there's some adapting to do when it when it comes to that type of thing.

01:10:57.000 --> 01:11:08.000
There's a real arbitrage opportunity now to do a personalized LinkedIn messages and stuff like that but now as soon when everybody's able to make them really well with these technologies.

01:11:09.000 --> 01:11:28.000
That's kind of going away. I mean, it almost, you know, first of all, we had SEO writing, then we have, you know, chat chat bot writing LLM writing, and then we'll have LLM meets SEO writing and then we'll have the SEO using, you know, using LLM to

01:11:29.000 --> 01:11:52.000
decode things and it's the, you know, in the end, it's, in the end, the bad thing is a lot of it may just get very, very, very standardized. It may be very kind of rote. It's just like in, you know, maybe almost a recitation of some standard thing, which is sort of interesting.

01:11:52.000 --> 01:12:14.000
And it kind of becomes almost ritualistic that, you know, it's just you're, you're, you're incanting this, you know, there's some incantation that you write that that sort of the end point of the perfectly perfected LLM that just optimally, you know, optimally invented the thing you should say in your LinkedIn message.

01:12:14.000 --> 01:12:21.000
And then they're all, you know, they're all the same incantation so to speak, because that was the optimal one I don't know how it will work out.

01:12:21.000 --> 01:12:25.000
But then you stop caring about the optimal one and the goal post change.

01:12:25.000 --> 01:12:26.000
Well, yes, right.

01:12:26.000 --> 01:12:28.000
Well, you stop caring about that medium.

01:12:28.000 --> 01:12:30.000
Right, right, right.

01:12:30.000 --> 01:12:40.000
It becomes, for example, yeah, right. I mean, I think the, the, you know, the handwritten letter is still a thing for another year or two.

01:12:40.000 --> 01:12:49.000
I think so too. I think, you know, it means a lot. If I'm, if I'm going on a date with a girl and I give her a handwritten letter versus I text her.

01:12:49.000 --> 01:12:51.000
Big difference, but

01:12:51.000 --> 01:12:53.000
Right.

01:12:53.000 --> 01:13:00.000
But anyway, well, very interesting stuff.

01:13:00.000 --> 01:13:04.000
Yeah, really, really appreciate you taking the time to chat about this. I know.

01:13:04.000 --> 01:13:15.000
There's a, we went all over the place, but it's interesting to think about where the world's going with all this, but just, just want to say that everyone needs to check out the Wolfram plugin on chat.

01:13:15.000 --> 01:13:22.000
I think it's really going to be game changing, kind of getting computation in there and excited. Who knows where this is going next.

01:13:22.000 --> 01:13:33.000
So excited to see and excited to see what comes out of your company here. If this was only two months, I can't imagine the next year, but again, really appreciate you taking the time to chat.

