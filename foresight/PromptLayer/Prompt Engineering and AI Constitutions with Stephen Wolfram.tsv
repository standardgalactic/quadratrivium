start	end	text
0	10000	So welcome to prompting with prompt layer conversation we're doing with many people in AI and we're kicking it off now with Steven Wolfram, who we have here.
10000	18000	And yeah, just real quick prompt layer where building dev tools for people bringing LLMs into production like GPT.
18000	25000	So if you want to do some data driven prompt engineering, we're the place for that and we're going to have we're going to have a pretty cool conversation.
25000	29000	I hope we're going to nerd out a little bit here and excited for that.
29000	32000	So you probably know who Steven Wolfram is what he's done.
32000	48000	I learned about his work from doing math homework properly in middle school or high school but originally originally met Steven when he came to a hackathon I was running in high school and he stayed there till like 2am mentoring people and that's kind of my first
48000	55000	introduction to this theory about computational thinking and it's come back around, you know, and it's very important today.
55000	65000	So, yeah, really excited to have this conversation with you and maybe to kick it off.
65000	82000	You had a big and a big release today of kind of plugins into chat GPT and this is, I think you might have said it but I completely agree it's kind of a historic moment where we're kind of bringing the LLM side of things that AI statistical AI to the symbolic AI to the symbolic
82000	91000	interpretation and that sort of thing so like, how did we get here what what has the process been and what why is it so significant right now.
91000	106000	Well I think the what LLMs have been able to do is sort of take the corpus of texts that we humans have written and kind of grind it up to the point where we can make more that's like it so to speak so we give a prompt.
106000	119000	And you know the the role of an LLM is to continue from that prompt and that's and sort of continue with things that are like the things that have already been written on the web.
119000	125000	And I think, and it does that in a way that is kind of just sort of statistically fitting things together.
125000	137000	It's a very different tradition of computation that is really able to use the deeper aspects of computation really able to use sort of the irreducible computations that can in principle be done.
137000	149000	That's the world that I've lived in for for many decades now of trying to figure out sort of how to take what is computationally possible and make it accessible to humans.
149000	164000	So the thing that's been sort of exciting is that well we made a lot of effort to make it accessible to humans turns out that also makes it accessible to language based ai's that kind of lent their craft from from humans so to speak.
164000	181000	So kind of the way I see it is these have been the two great kind of traditions of AI, and we've now got this opportunity to really connect them together through the medium of sort of a mixture of natural language and computational language, and be able to get it
182000	198000	that one can kind of use the linguistic interface that we are all used to because we're all very experienced at sort of interacting in natural language, together with kind of the depth of that rather in human non human thing that is sort of powerful
198000	213000	for computation. So it's kind of bringing those two things together the, the very human language with the very sort of beyond human non human power of kind of deep computation, and you know the fact is that, as a practical matter, LLMs are making
213000	219000	sort of statistically reasonable pieces of text, which may or may not actually be the way the world is.
219000	236000	And what we've been involved in doing is kind of making a representation of the world as the world is in computable form, and being able to bring those two things together so that we're able to let the LLM sort of make it statistical kind of text, and then we're able to provide
236000	249000	sort of precise computable insertions into that text to kind of provide the facts, where the LLM can be writing fact or fiction, and it doesn't really know the difference so to speak.
249000	259000	Right, right. No, this is super interesting and like, and I guess it's, and it's changed so much in the past like two or three months. I guess like to dive deeper here.
260000	272000	Maybe, I'll give you an example so, and I'd love for you to explain kind of your thinking behind this, and this is changed today with the welfare and plug in and chat GPT obviously but I had a friend Avi and he tweet.
272000	288000	Let me let me just read the actual tweet so he was basically he tweeted that he was using GPT 3.5 then he was using GPT for and it was failing on this very simple problem that he was doing and the prompt was how many unique apartment units are represented in the data below.
288000	308000	As in two people in the same unit, for example, 430 would count as one and then you had a bunch of names with different apartment numbers, and his question was to him, like, this is not obviously a hard problem in the let's take a step back from what's symbolic what's statistical,
309000	321000	just at the space level if I'm a hacker on just like using open AI, this doesn't seem like a hard problem but it wasn't a problem GPT is able to solve on its own it's not a problem LLMs are able to solve.
321000	330000	Would would love to hear you kind of break that down and explain what about this problem is hard for LLMs and what about it it's easy for the warframe language.
330000	335000	So, anything that involves precisely computing something with math.
335000	349000	There can be many steps that are involved in doing a math computation and LLM is a feed forward network that you know the current version of them at least is sort of feed forward networks where you kind of, you know, it's encoded a lot of stuff.
349000	358000	But basically you fed it the the the text so far, and it sort of ripples through the network and then tells you the probabilities of next words to follow.
358000	366000	And that's just not something where if they're needed to be something where iterated recursed whatever else to figure out what's the answer to this thing.
366000	380000	That's not something that's going to happen inside the current architecture of LLM that the way that LLMs managed to not be sort of computationally trivial is that they are continually eating their own tails so to speak they're continually kind of
380000	394000	processing the things that they have generated so far and that's what allows them that that little sort of extra piece is what allows them to have a lot more richness than just a pure, you know ripple through the network once type of type of system.
394000	410000	But still the kinds of things that are really easy in terms of traditional kind of Turing machine computation are are really hard. They're really not possible actually for something which is fundamentally doing this you know ripple through once type type method.
410000	428000	I think that the challenge, I mean, in terms of sort of the the architecture for AI is using tools is how does the AI know what tool to use what what when to use the tool, how to present information to the tool to be used, and then how to interpret the information that comes back.
428000	439000	We've been kind of fortunate in this case because we built Wolf from Alpha to take natural language input from humans and turns out LLMs can also produce natural language input.
439000	455000	And so, you know, we already have a funnel basically to collect the things that LLMs naturally produce now in fact the endpoint that is in the Wolf and plug in for chat GPT is a combination Wolf from Alpha Wolf and language endpoints.
455000	471000	And that's an elaborate piece of engineering because the LLM kind of and you know we we try to make the prompt, I don't know how successful we've been so far, we try to make the prompt decide, should it try and send it to Wolf mouth or when because it has a natural language input, and it's expecting
471000	479000	natural language collection of outputs, or should we try and send it to Wolf from language where it has to synthesize this precise computational language.
479000	492000	And then when it sends it to often language is it actually correct Wolf from language, it has to go, it basically goes and reads the documentation that's what we've told it to do at least to figure out, is it a correctly formed piece of Wolf from language input.
492000	507000	And then when it gets the result, it's a very, you know, here's the result that's all there is, and Wolf mouth, the result can be this long series of, of, of things which which chat GPT can then knit into an essay and it does pretty good job of doing that.
507000	518000	I think that, so I mean that there these different, different modalities but the fundamental point is, when it's a computation that involves kind of looping and recursing and so on.
518000	534000	It's just not something that you can expect to be able to do from from from a straight LLM now there are other issues like, like things to do with tokens, and the fact that tokens and numbers don't match very easily, although that's something one could definitely fix.
534000	547000	But, you know, and, and, you know, if you're doing even something like multiplication, if you're just doing it in pure LLM land, what are you going to do memorize all of the, you know, two digit three digit multiplications.
547000	557000	Well, then you have the next case, and it may or may not be true that from the innards of the LLM that it can untangle kind of doing the carries and all this kind of thing.
557000	573000	Probably it can untangle that if you allow it to, you know, talk about its intermediate steps, it has an easier time, if it can talk about its intermediate steps because essentially it's storing intermediate it's got an intermediate storage location, so to speak that is the actual text it's
573000	590000	not doing. But it's not a very, you know, the, the, the best way to do that is just you just compute, and you do sort of traditional computation on that to get that result and then you knit it back into the kind of natural language world of the LLM.
590000	612000	Interesting. So, what, why is it that having a kind of a, but asking the LLM to explain the steps makes it a little more correct. Is it is it not just finding like, I guess the completion that's most close to all the steps there's something I guess that changes the problem by breaking it down there or I don't know if you have any insight.
613000	630000	The problem is that if you've got something that involves intermediate steps, the LLM cannot internally, when we write a program in traditional, I don't know, orphan language or something, it's trivial for it to go and, you know, iterate through many, many times.
630000	635000	You know, it's trying to find the answer to this thing, it's going to run it a million times internally, it's all good.
635000	643000	But when, if it's an LLM, it doesn't get to do that. Every, every token that it is emitting, it's just rippling through once.
643000	652000	Right. And so in order for it to have this sort of intermediate state where it kind of iterates on it, it's got to actually write out that intermediate state in the text stream that it generates.
652000	664000	And so that's why it can be better if you ask it to explain itself, so to speak, because then it has the each individual kind of ripple through each individual new token is less has to happen.
664000	674000	And it's kind of doing that sort of building up of the tower through the explicit text that it's generating and then and then redoing things from that text.
674000	694000	Interesting, interesting. Yeah, that makes sense. And I guess like, one thing I'm curious about, I guess, your thoughts on and is this the, and I guess what the steady state is is what it like the interfaces here it sounds like this was kind of, this is like a two month thing that went from no
694000	714000	to GPT with plugins and stuff like that or chat with plugins and the Wolfram plugin and you mentioned there's the Wolfram language input and the Wolfram alpha input. Is that do you see there always be like, is it like Wolfram alpha as the fallback or how do you see that evolving over time or is this like, we'll see who knows.
714000	723000	Well, I mean, the combination the combining of the Wolfram alpha endpoint and the Wolfram language endpoint started last weekend.
723000	728000	I love it. It's, it's a very non trivial piece of.
728000	742000	Oh, I don't know combination AI thinking prompt engineering software engineering, etc, etc, etc, because those are running on different, you know, in different on different clouds that you know it's a complicated thing it was it was complicated to pull off.
742000	757000	I think the, yes, basically right now, I see it as, you know, the LLM if it's just yacking in language, it's going to send it to Wolf now because Wolfram language can't do anything with that.
757000	768000	If it manages to successfully, it's been trained on a certain amount of Wolfram language examples. Probably it'll be trained on a lot more such examples in the future.
769000	773000	As it gets better at synthesizing Wolfram language code.
773000	782000	It's probably ultimately more powerful to just go straight to the Wolfram language form. But that's, it's, it also can make horrible mistakes doing that.
782000	793000	Wolfram Alpha is a good catchment mechanism, because it's already got, you know, many years of development of natural language understanding, where it knows, you know, given this natural language.
793000	797000	First of all, it knows when it doesn't know, which is an important feature.
797000	806000	Yes, it's been pretty successfully, you know, set up so that when it doesn't understand the natural language just as I don't understand, it doesn't make something up when it does.
806000	808000	Humans.
808000	809000	What's that?
809000	811000	We need that for humans too.
811000	817000	Yeah, right. Well, I think, I think, you know, what's interesting to me is what we learn about kind of human psychology and human operation.
817000	827000	By looking at chat GBT and how unbelievably similar the types of mistakes it makes on solving problems and things like that are to the way that human the kinds of mistakes humans make.
827000	838000	And I think it really is capturing the essence of sort of the neural net it has is probably not that different in some functional sense from the neural net that we have.
838000	849000	And, you know, it's, it's really, it's showing us kind of what it means to be a sort of a human like answer, a human like text generator, so to speak.
849000	863000	In terms of the sort of the future in terms of sort of, okay, Wolfram language is a really great language for ai's to try and speak in it's sort of the best language for ai's to try and speak in, because it talks about the real world.
863000	867000	It's very self contained, and it's very succinct.
867000	888000	I think that it's something where the AI, and the other thing about it is, which I think is is yet what we're about to see this really, really blossom is Wolfram language as a language for human ai collaboration, because you know what we see happen is you say you type in natural
888000	898000	and you want to get this this and this. It synthesizes a piece of Wolfram language code. Maybe it's right. Maybe it's wrong. But you can kind of run that code line by line.
898000	910000	You know, Wolfram language is a symbolic language so you can always see the output at every at every stage, everything you have is a valid piece of output. So you can see that thing build up graphics build up the structure whatever else it is.
910000	920000	You can run it line by line. And as I was, you know, as I've been using it. That's what I've ended up doing it generate some code and it's like really is this right. I don't know.
920000	936000	You know, it got it roughly right it's not completely crazy, but to know is it actually right well I have to kind of run it line by line sometimes, sometimes it's obviously wrong, because for example is generating error messages actually one thing we just did in the last two days is when it
936000	947000	gives a message, it gives chat GPT's a bunch of text around that message. It feeds it back to chat GPT and says rewrite the code, try and avoid this message.
947000	959000	And, you know, how well that will work. Not sure it's it's clearly working in some cases, and chat GPT is wonderfully polite and apologetic when it, when it keeps on doing the rewrite so to speak.
959000	970000	But, you know, that's a, I think the, the sort of the place where it's going to look really interesting the kind of in the, in the world of kind of what future programming looks like.
970000	988000	I think a big piece of future programming is, you type some text that's kind of the initial here's what I roughly want to do. It synthesizes basically in from what I can see wolf language is the best language for it to synthesize because it is the most succinct language that is the most kind of expected to be
989000	1003000	actually humans. It's not a big slab of code in some low level language, where the thing is talking about, you know, set this array pointer to this type thing. It's something which is trying to speak at the level that humans think so to speak.
1003000	1011000	And then, then the, you know, the picture will be and again I haven't had the experience yet, except in a few toy examples of seeing how this really works.
1011000	1023000	I type my initial you know this is roughly what I want to do. It synthesizes some code. I look at that code potentially line by line it probably synthesizes and tests for that code, along with the code itself.
1023000	1033000	It runs those things it says this is this line. This is what it did. You look at it you say no that's that's wrong. And then you either tell it that's wrong or you fix it yourself.
1033000	1053000	And then you you're steadily building up this thing that is the kind of the the final code you want that you can then you know we increasingly have efficient compilers to LLVM and things like this so once you've got this very high level representation, you're then kind of which you have been collaborating with the
1053000	1062000	AI to produce, then you can kind of take that and deploy it however you want to deploy it whether it's in the cloud on some, you know, embedded device or whatever else it is.
1062000	1073000	But that's the kind of the point is this collaboration between the human and the AI, where you're leveraging the fact that computational language our computational language is actually readable by humans.
1073000	1083000	And part of what makes it readable by humans is that it is a language that can immediately talk about images and cities and chemicals and movies and things like that.
1083000	1095000	And you're not kind of trying to figure out. Oh, what is the data structure that it uses to talk about an image or something. It's, it's right there something that you can read as being a thing about an image for example.
1095000	1106000	And so I think that's, you know, to me that's a really pretty exciting prospect of kind of the, the future of programming and you know people say what's going to happen to all the programmers.
1106000	1113000	It's like what's going to happen to everybody who does boilerplate, you know, smart boilerplating so to speak.
1113000	1123000	You know, that it's that's something people who are, you know, producing, you know, somewhat boilerplate, you know, documents of various kinds. That's kind of going away.
1123000	1135000	And similarly, you know, people have rushed into kind of learning, you know, going to computer science school and learning how to write, you know, Java code, Python code, whatever else it is.
1135000	1148000	And it's like, a lot of that is just going to go away. Just like, you know, when I was, well younger than you people were always talking about a Sunday language, you know, if you're going to be serious about computing, you've got to write your code in the Sunday language.
1148000	1151000	I don't think anybody says that anymore.
1151000	1157000	You know, in fact, the C compilers or whatever are probably producing better assembly language than any human produces at this point.
1157000	1173000	So, you know, what we're seeing is this kind of this moment where kind of the, the there's in these lower level languages because you kind of you can write a big slab of boilerplate language, which you needed to write as boilerplate language because it was a low level language.
1173000	1184000	In a sense, what we've already done with Wolfman languages automate out that boilerplate by having, you know, the one function that just does the equivalent of that big slab of lower level code.
1184000	1193000	And so that gives us the opportunity to be sort of at a level where we can have a, a meaningful conversation with the AI about what we're trying to do.
1193000	1208000	We can produce kind of, it then produces a first draft of the code at a level that we can understand, we then edit that code, or tell it to edit that code, but we can understand that code we can understand the test cases and so on.
1208000	1228000	And then reiterate, and, and I think it's going to be a really productive way of producing kind of computational functionality. And I think it will also open up that, that capability to a whole range of people where, you know, they didn't learn how to do memory allocation and make sure that the, you know, the
1228000	1239000	pointers stayed aligned and you didn't, you know, whatever else was the, you know, whatever other sort of lower level thing you might be thinking about, they never learned that stuff and they don't need to.
1239000	1246000	Just like for most people programmers today, they don't really need to learn, you know, how assembly language works.
1246000	1248000	Right, right.
1248000	1250000	A lot of interesting stuff there.
1250000	1268000	So, okay, so you don't need to know how assembly language works you don't need to know how to code today. One thing like one thing I've noticed kind of through friends and like hackathons here and stuff like that is like the concept of prompt engineering is a very,
1268000	1281000	I feel like programmers pick it up like this because it's a very tinker heavy, you know, we can assume the neural nets of black box change change the prompt see how it at see see what comes out change it again see what comes out.
1281000	1289000	Is this like, is this a certain type of, is that a skill in itself, is that something that could be learnable what would you, how would you decide.
1289000	1297000	I always enjoy I've been, you know, in the past I've been responsible for a few sort of new job categories of things people might do.
1297000	1311000	Like when wealth mouth came out we had this sort of new job category of linguistic curators. And it's like, you know, we have test cases like, you know, write a thing which you know you say how many ways are there to make change for 35 cents.
1311000	1319000	Well, there are a zillion different ways to say that some people are really good and can generate at output speed 200 ways to say that.
1319000	1328000	What kind of skill is correlated with that I wondered that you know I wanted is this going to be poets is it going to be crossword puzzle people, is it going to be computational linguistics PhDs.
1328000	1335000	Well, actually it was people who did not know that they had that skill, and just like doesn't everybody have the skill, but they're really good at doing it.
1335000	1346000	And I think with prompt engineering, you know, I saw about a week ago I saw for the first time, a thing I was kind of predicting a resume, where somebody had been an animal wrangler.
1346000	1360000	And now they were, they were theming themselves as a prompt engineer. And it's kind of the same, the same type of thing, you know, you're poking at this thing you really don't understand, and you're trying to get an intuition for what it does I think it's more the kind of thing where,
1360000	1373000	you know, I'm not sure whether it's a, a, you know, I think a lot of people who do programming well, and who do programming, sort of in a really thinking pretty.
1373000	1379000	I mean, I don't know when I, I suppose when I try and do programming.
1380000	1394000	I'm always, I think I'm thinking pretty analytically about what's going on. I mean, sometimes I'll just, just try this, but that's pretty rare, I would say relative to the, you know, I think I have sort of an analytical understanding of what's going on.
1394000	1407000	I mean, one of the things that happened to me over the years is I'm now a, you know, a fluent Wolfram language programmer in the following sense, that I can start to type code before I could have told you what the code would say.
1407000	1416000	It's kind of like, as opposed to I'm thinking about it in my mind in some kind of mental model that involves kind of my natural natural language, so to speak.
1416000	1427000	But instead I'm actually thinking internally in Wolfram language. And that's something that again it's a feature of being a high level computational language that you can imagine doing that.
1427000	1436000	And, you know, I can be typing the code and I know quite a lot of other people are in this position as well, where they can type the code before they can explain what the code would say.
1436000	1449000	Just like if you're speaking a foreign language, you know, you, if you're not very proficient at it, you're kind of thinking in English, let's say, and then you're translating into French, as opposed to just sort of thinking fluently in French.
1449000	1460000	Well, that's something you can get to the point of being able to do in computational language. I think that the question of what it takes to be kind of a great prompt engineer, I don't think we know yet.
1460000	1475000	I know, and I'm trying to think in my own company, you know, we've been doing a bunch of prompt engineering. And gosh, I mean, it's a range of people actually have been doing it.
1475000	1479000	And the people.
1479000	1492000	Gosh, I mean, it's the number of people who are very experienced at heuristics, kind of developing heuristics for for now for this and others who are more into just all around.
1492000	1497000	Well, I would say all around thinking more more so than all around programming, so to speak.
1497000	1515000	I kind of, I think it's a it's a new kind of skill. It's a it's a strange skill because it is a skill, maybe a bit more like psychology or animal wrangling than it is, I think more like that than it is programming and as a as a pure
1515000	1518000	kind of
1518000	1532000	I mean, there may come a time when we understand enough about prompt engineering that we're able to kind of have a formal structure for thinking about prompt engineering.
1532000	1542000	No, it's an interesting question and it's something which given that one has, I don't know, I, you know, a reasonable at least high level understanding of how chat GPT is working.
1542000	1555000	You know, can one so an interesting question that I have not really addressed is having made quite a study of how chat GPT works and sort of thinking about it a little bit like, you know, playing physicist on chat GPT so to speak or playing natural
1555000	1565000	scientist. Does that help me to know what I should write in prompts. And I haven't really figured that out. I don't know I haven't had a chance to think about it actually yet.
1565000	1582000	In other words, because it I find it utterly bizarre that it, you know, does it, you know, saying please, does it matter, putting things in capital letters, does it matter, you know, which is, you know, does it pay more attention to the thing that came later,
1583000	1595000	does it, you know, does it matter. You know, for example, I've spent a lot of time in my life figuring out how to explain stuff to people. And I know a lot of rules of thumb.
1595000	1601000	When you're writing something textually about how to do that so that people have a chance of understanding it.
1602000	1619000	Example, you know, one thing I learned very early on decades ago in writing software documentation is another things for that matter is, if you say the critical thing in three magnificent words in the middle of a paragraph, nobody will get it.
1620000	1635000	You know, what you have to do is to some extent, the, the emphasis is in part determined by the area on the page that you spend yacking about that thing. And so does that matter for LLMs. I don't know.
1636000	1653000	You know, it could be that the, that the great prompt engineers are also great human expositors. It could be that the great prompt engineers are people who are more used to, I mean, you know, who are used to sort of talking to babies, talking to animals, so to speak.
1653000	1674000	I'm not sure. I think that the kind of a notion of kind of, well, you could ask as much about sort of formal prompt engineering, if the AI is acting a bit like a human and after all it learned from human language as we've expressed it on the web.
1675000	1688000	What is persuasive writing for a prompt? You know, how do you, can you, and of course, you know, by the time you're saying, well, I'm going to have a meta prompt where I'm going to ask the AI to, to write a prompt for itself.
1689000	1708000	I don't know how well that ends because the fact is that, you know, in the end, you have to say what the heck you're talking about. And then it can, and this is again, the sort of a little bit the fallacy of people were looking at lower level languages and programming languages and saying, gosh, we can get the AI to do this.
1708000	1719000	And the reason you can do that is because those languages are deeply compressible, because they're, they're low level they're talking about things that the computer is doing. They're not talking about sort of the big goals that it's following.
1719000	1734000	As soon as you're talking about the big goals. It's much less kind of compressed by an experience I had wrote this book. Well, I don't know what was it. Gosh, it's very long ago seven years ago now about elementary introduction to the world from language.
1734000	1748000	So I'm very easy book for me to write and I'm kind of upset more people haven't written books like that because it was so easy to write but I then I realized maybe it's easy for me to write and it's not quite so easy for other people to write because I've had lots of experience in trying to explain stuff to people.
1748000	1761000	And the, but there you guys in that book, I had exercises and the exercises are pretty much all of the form. Here's a description of the thing we want to do described in English, right and involved in language.
1761000	1771000	Okay, at the beginning of the book, had an easy time making up those exercises. The English language version was very simple. I was imagining some more from language form.
1771000	1784000	In the book where things getting a little bit more elaborate. It's kind of like, well, I can immediately under, you know, I immediately know what is the law from language version of what I'm trying to say, then I have to back translate it to English.
1784000	1787000	And the thing ends up reading like legal ease.
1788000	1799000	Because, because that's a thing that is not very well expressible in English. So, again, it's kind of, and that of course leads to the question of can you write both language code that is a prompt.
1799000	1810000	And the answer is probably yes. And that's another interesting possibility. And that's, you know, that that becomes a way of expressing yourself, kind of in computational language.
1810000	1825000	But for the AI, which is then kind of itself, thinking in computational language, for example, rather than thinking in English, you know, there's obviously less walking language on the on the web than there is less human written
1826000	1842000	English text on the web. But it's also in a sense much easier to learn it doesn't have a lot of the irregularities of English language. It has, you know, it's something where you can look up the definition, so to speak, you can operate from the definition and so on.
1842000	1860000	So I think it's, we're in very, you know, very early days of the theory of prompt engineering, I think that, but I do think that a very important aspect of kind of this picture is this potential loop between kind of the the prompt, the computational
1861000	1868000	language, seeing what the computational language does, generating automated tests for the computational language.
1868000	1882000	And, you know, and then and then having the LLM essentially present, you know, this is the LLM's version of how to explain that to a human. Here's what it did. Here's the explanation, you know, you may not like it, maybe wrong.
1883000	1897000	But here's the LLM helping to explain it by actually running. And one thing, yeah, it's almost seems trivial to me at this point, but, but the fact that the LLM can go off and run a piece of walking language code and see what happened is very important to its, to its kind of life and times
1898000	1906000	it's what makes it, you know, and you see it, it's kind of weird to see it, you know, it tries this, it says, I'm going to re re re rephrase it, I'm going to try this, etc, etc, etc.
1906000	1916000	I think it will be nice in terms of the sort of IDE aspect of this, to be able to see the code it's producing see the test cases come out, be able to click some things.
1916000	1929000	And then, you know, as you see those those fragments come out, then be able to have a nice way of rolling it up to make a bigger, bigger program, which you can then treat as a single unit to then go on and use that elsewhere.
1929000	1944000	I think, also, I mean, you know, in terms of, oh, one of the things that's just a piece of software engineering that we've been having an amusing time with, you know, we've developed over the last, you know, 36 years pretty good technology for doing automated software testing.
1944000	1959000	And it's like, okay, we've got a software quality assurance team. And, you know, I was telling them, okay, so now you've got to test this, this plugin, and it's like, how are we going to test this, you know, it has no predictability at all, what it's going to do.
1959000	1968000	And, you know, in a sense, the way we can test it, which we've been doing a little bit of, is you have the LLM comment on its own behavior.
1968000	1990000	Yes, this is a, this is something we've heard about a lot, like, I guess, something we're working on is kind of like, how do you test things in prod, how do you evaluate that prompt A is better than prompt B. And one of the things I've heard about, mostly from hackers and mostly from into like the community as opposed to kind of the big companies is this whole synthetic evaluation, how do you have the LLM
1991000	2002000	evaluate itself, which feels like it either will work really well or not work at all. It's kind of the journey remains to be out. But how are you, you're doing that now or it's an idea.
2002000	2012000	We started doing that. Yeah, we started doing it. I mean, I would say it's, it's not for, for reasons of sort of boring software engineering reasons, it's not quite as easy to do yet.
2012000	2029000	You know, in, in, you'll find in our packet repository, you'll find this open AI link, which is a, you know, just an API wrapper in orphan language that allows one to sort of call, call a bunch of open AI APIs.
2029000	2051000	And that's, you know, that that's the thing on which we're building kind of the testing framework. But, you know, it's been the case. I mean, it hasn't software testing involves many sort of pieces of exogenous information, like for example, back from 35 years ago with testing graphics output.
2051000	2067000	Okay. Well, you know, a pixel could change here or there and it doesn't matter. So you have to have a way of doing regression testing that doesn't isn't not affected by individual pixels. And so we were doing early on we were doing image processing later on we've been doing
2067000	2083000	more machine learning type methods to figure out, did it matter, did what happened matter. Another example that is timing tests, you know, you've got 10 million tests, and some of them run slower than the new version, do you care, how many how much slower can they run.
2083000	2095000	That's more a question of sort of a statistical, you know, way of figuring out what what you think is acceptable and what isn't. And it's kind of a. So I think there are all these kind of methodologies.
2095000	2113000	And, you know, one of the things that's kind of nice about interacting with an LLM interacting with computational language, computational language, kind of knows when it went. Well, it knows many aspects of when it went way off track, because it's just, it's just you can't get there from here it's just generating error messages.
2113000	2123000	Things are happening. I mean within Wolfram language, we have pretty good kind of actually we're working on another level of this error handling capability.
2123000	2130000	So, you know, here's the thing I mean when one thinks about programming, everybody thinks about the way the program was supposed to work.
2130000	2139000	And, you know, you can put sort of complicated guardrails with with effort you can sort of put all the guardrails around to make it never do the wrong thing.
2139000	2152000	But most people, most of the time they're rushing to get the program to just do the right thing. And so the, the kind of the error path is a safety net that we would like to automate as much as possible how that safety net works.
2152000	2161000	In other words, we'd like to be able to write to an interesting question whether whether LLMs could help do that is to write the kind of error checking code.
2161000	2168000	But we've we've already built a bunch of things that sort of help automate the error checking process. Now you can't get it exactly right.
2168000	2176000	In other words, you're going to be it's going to be a heuristic thing like there's a path the programmer intended to follow. And there's ways you can fall off that path.
2176000	2189000	You know, if the programmer had really done all the theorems so to speak to know how to keep on that path were well and good. But if the program is kind of lazy, and they're just like well I'm getting this path right but not thinking about all the other things.
2189000	2202000	You've got to kind of fill in the, the, the sort of the, the rail the guardrails for yourself. And that's an interesting problem of kind of the, I view it as the sort of automating the secondary path of the code.
2202000	2217000	The primary path was defined by the programmer question is can you automate what the secondary paths are, and that's something again one can expect to to see perhaps in a sort of IDE environment of the combined kind of computational
2217000	2235000	language, LLM environment of writing programs, one can see kind of the, the way that, you know, well first generating the tests, then being able to, and then potentially you know you look at the tests you say oh I like the way that tests came out.
2235000	2249000	Okay, then, then there's kind of like how do we make, or you say this is how it was, or maybe, maybe the LLM even generates, here are some possible paths that could have been followed, and it tries to bucket together some of the bad things that could happen.
2249000	2262000	And it says what do you want to do in this case, you know, do you want to, if you get one numerical precision, you know failure to converge message, do you want to abort the rocket launch or not.
2263000	2281000	You know, and that's a human decision. No, no, it's not obvious what the answer is, it could be it's better to, you know, keep whatever it is if you're landing the, you know, the plane or whatever it is, it may be better just keep going and try and land the plane, even though you've got that error message, or it may be better to say,
2281000	2296000	you know, abort, you know, whatever, and, you know, go around before you try and land the plane or something. I mean, I think that that's the, so that that's a sort of a human choice but it's something where potentially the LLM, because it knows, it knows human pretty
2296000	2302000	well, could help in being able to make that kind of choice.
2302000	2317000	That's very interesting. And I guess that's the, I mean, I could imagine just the whole like a integration test suite that uses Selenium browser and checks your website, like, I could imagine using something like that, totally.
2317000	2334000	Yeah, I think the case of using LLM to build tests, to build unit tests, seems like, yeah, that, that'll exist. That seems pretty straightforward. You know, I've built a, I use GPT for to make a new element on my page and then I just feed back in the JavaScript errors back into it and
2334000	2351000	bottoming really quick, it fixes it. But the, I think the even more interesting thing here is kind of asking, maybe asking GPT or even training a more specific or fine tuning more specifically a classifier to say,
2351000	2366000	is this output a good output and good bad output is very like, who knows what that means it's very case by case people talk about hallucinations but at the end of the day is the user getting something bad and can I've seen.
2366000	2380000	I've seen I spoke to one team that has a chatbot and they basically feed the results back into GPT and say, if you were the user, how good of experience would you rate this and I think that
2380000	2395000	essentially what was what was done in the, you know, in the final reinforcement learning steps of training chat GPT was essentially that kind of process that they've been human ratings done, and then those human ratings were used to train a classifier
2395000	2410000	and then the classifier was run automatically against the actual LLM. And that's a, you know, that's a clearly an important kind of thing to do. I think, in the case of
2410000	2425000	it's an interesting question when you're dealing with code, and when you're dealing with the sort of what could possibly go wrong with this code, how do you follow the paths, and then it becomes sort of a merger of more like, you know compiler thinking so to speak of what, what can you say about
2425000	2439000	these code paths, what can you prove about the code paths. And then, you know, in this path, which the sort of compiler like thinking could say there is this path. And then you can ask the LLM, if you follow that path, are you going to be happy.
2439000	2457000	And that that's, you know, I think that's a, but I do think that this idea of, you know, people normally, this idea of, sort of, can the LLM estimate when the user is going to be happy.
2458000	2469000	That's probably something, and that that's part of what we've been thinking about not in the context of LLM so much in the secondary pathway for error, for error handling and programs.
2469000	2475000	It's like, what can you see about the heuristically about what is likely to make a user happy.
2475000	2488000	And even though we don't know for sure, because it could be that the user, you know, the user is doing some science program. And this one bizarre case is like the amazing, you know, we just discovered this amazing phenomenon.
2488000	2499000	And oh, the error handler basically says, well, that's not a thing that really happens much that will be an anomaly. Let's go and catch that in the error handler. And then the person never sees that.
2499000	2513000	The error handler can't really expect the truly unexpected, but it can kind of deal with the slightly unexpected, so to speak, the things not foreseen by the programmer, but sort of expected on the basis of general intuition about programming.
2513000	2528000	Yeah, it sounds like I know you write about this a lot this computational irreducibility and I think that's probably like the best way to think about like, is this, is it the halting problem to say can LLM tell if it's correct or not because is that nested.
2528000	2530000	How does that work?
2530000	2552000	I think that, I mean, in the end, you know, one of the issues with the kind of LLMs, like turtles all the way down, so to speak, is that kind of, you know, you say, did it do something that I thought was good, like, you know, ethically good or whatever else.
2552000	2561000	Well, there's no ultimate, you know, you can say, well, how does it compare to what people have written about and, you know, in the in history and literature and so on.
2561000	2572000	And but there's no, there's no sort of ultimate ground truth to that it's well, you know, do, do the humans who are sort of making the decision for how the LLM should work.
2572000	2577000	Are they, do they think it's good, do they not think it's good, you've got to have some kind of grounding there.
2577000	2585000	And I think that's the, you know, you've got to have some set of principles that you say I'm going to follow these principles.
2585000	2593000	Now, computational irreducibility has the consequence that when you think you've written down the principles that determine what will be good and what will be bad for the LLM.
2593000	2598000	The LLM will always come up with a weird case that's not covered by your principles.
2598000	2609000	That, you know, that's kind of the pattern of how that's happened is like with human laws. People say, oh, we've got this, you know, this legal code, and that's going to determine how everything works.
2609000	2618000	And then along comes an AI, for example, that nobody imagined, you know, when the US Constitution was written, for example, you know, people didn't imagine there would be AI's.
2618000	2629000	And so then, well, what do you do because, you know, you can follow this legal code, but it doesn't say anything about what to do when when the AI is responsible for doing this that or the other.
2629000	2638000	And so, you know, you kind of have to patch it and it's the same thing with any of these sort of sets of principles about how an AI should work.
2638000	2657000	And so I guess how would you square this with, I mean, this whole concept that the LLM looked at so much data and kind of discovered these speech patterns and kind of built a model around how to talk and built a model around just like,
2657000	2666000	but let's say my mental model is they've kind of solved natural language right and you can argue back and forth of course but that's that's the mental model there.
2666000	2682000	What, what is the limitation of what else can be discovered through these patterns like it are these moral principles maybe are these reducible to a certain patterns of like human psyche or biology maybe.
2682000	2698000	Interesting question. I mean, I think that the, the fact that neural nets can do human like things they can make human like decisions about images they can make produce, they can generalize from their training data in a human like way with language.
2698000	2708000	Probably that's happening because neural nets are architecturally pretty similar to how brains work. And so they're generalizing the same way. Now, interesting question, which I've certainly thought about.
2708000	2717000	And is this question of are there kind of a set of, for example, you know, moral principles that you can similarly say this is the construction kit.
2717000	2733000	And this is the set of primitives from which you can operate and I know I've talked to people over the course of years about this kind of thing and people occasionally say you should read this philosophy book you can you know this person in cognitive science has done this and I've got a pile of these books.
2733000	2743000	And I have to admit that I haven't really gotten through them. But there certainly are efforts that people have made to sort of try to identify the print the primitives of moral thinking so to speak.
2743000	2756000	And, you know, if you decide this I mean you could have different ways of thinking about ethics and different. But you know, are there is there a construction kit you get this and this and this and these fit together in that way.
2756000	2760000	And is that a reasonable model of how humans make ethical decisions.
2760000	2775000	And it's quite possible yes and it's quite possible that even from all the texts that that you know chat to PT has read that it's learned to pretty good model of how humans make certain kinds of ethical decisions, or at least how people write that they've that they've made certain
2775000	2784000	I mean, this is of course the big conundrum of this stuff is that, you know, people say well what should the how should the ai's make ethical decisions well you say, just copy what the humans do.
2784000	2793000	Then people say, no no no that's not the right thing to do humans do all kinds of wrong things. You know, it shouldn't be just do as the humans do.
2793000	2808000	What should be do as the humans aspire to do. And then it gets much more complicated, because what the humans aspire to do may not be realistic. It may be you know different people will disagree about what the aspirations are, etc etc etc.
2809000	2822000	I mean, it's a very, it's a, I think it is a an important challenge for our times and I, you know, I mentioned this over the last few years and I, I occasionally have mentioned it in in groups where they sort of are supposed to be in the business of figuring out stuff like this like
2823000	2843000	of saying, okay, you know, imagine you're writing the Constitution today in the post ai age. What does it say, what should it say, what are the, what are the principles, you know, what are the truths that we now hold to be self evidence, so to speak, or whatever, in, you know, in the ai age.
2843000	2856000	And I think, you know, I'm not sure I know what the answer is. I mean, in, you know, at what point, for example, very basic ethical question is when should an AI have rights.
2856000	2874000	And, you know, I was a number of years ago I was at some AI ethics conference and this, I raised this question, and some very bouncy philosopher who I've gotten to know better actually and she said, we should do that when the AI is a conscious.
2874000	2880000	That's not very helpful, because this question just loops right back on itself.
2880000	2891000	But, but, you know, something I realized of you rather recently is, let's say you have this autonomous bot hanging out, and it's entertaining people.
2891000	2903000	And it makes all kinds of friends, and it makes a living for itself. It has a Patreon, it's, you know, it's kind of, it's paying its hosting fees through people donating to it and so on.
2903000	2913000	It's a, it's a happy autonomous creature. And maybe it even was created through some bizarre legal construct of, you know, some loop of LLCs where there's no owner to it or something.
2913000	2927000	Okay, so it's hanging out and doesn't have an owner, and it's making a living for itself. And somebody says, oh, it's starting to be, you know, be mean to people, it should be shut down.
2927000	2941000	Okay, how do you decide to do that? And what is the kind of ethics of doing that? Well, one of the things that's obviously the case is that that bot may have made lots of friends, human friends, and you shut the bot down.
2941000	2951000	And those human friends can be very unhappy. And so, you know, when you think you're just dealing with a bot, you've actually, you know, it's connected itself to the human world in a certain way.
2951000	2968000	I mean, it starts to get kind of complicated to know what to sort of what what the right thing to do is. And I think that's a, you know, that's a, that's an interesting challenge for our times that I say, I've, I've thought about it a bit and I, I figure I'm, you know, if it falls to me to have to figure
2968000	2986000	it out, this is a shocking thing because it's not my, you know, it's not the kind of thing I, I usually think about but you know, one could think about all kinds of things but it's it's it's something where even to get kind of a, you know, one of the things in figuring
2986000	3000000	out something like that. And this, I suppose it, in a sense, it's like, imagine you're writing the prompt that will be for the, you know, Asimov style robots that are going to populate the earth type thing.
3000000	3010000	Imagine you're writing the prompt, you put down the, you know, the three laws of robotics or something. And then you say, Okay, that's good, I'm done.
3010000	3022000	And the prompt say, you know, what, what would a better prompt be because that prompt we know from, you know, from Asimov's writing those, those three laws of robotics tangle themselves up very quickly.
3022000	3027000	And, you know, what should we actually, what should we say in that prompt.
3027000	3039000	And even if we could write that prompt in natural language, I think we'd be better off writing it in computational language which has a much more, much more ability to say to answer the what if question, you know, you write it down in
3039000	3054000	natural language, it just is what it says. If you write it in computational language, you can run something and you can say, Let me simulate this situation against this computational language description of what should be done.
3054000	3069000	And then you have a much, you know, you have a bigger sort of a larger cross section of stuff that you get to have defined, rather than rather than just the the pure words that you wrote.
3069000	3086000	I guess that computational language or being able to talk in these discreet might be one way out of that. I guess you could also probably think about in, you could consider just for for for sake of talking, you could say 1776 America let's
3086000	3102000	say that the Constitution there is a bot, you know, but there is the human checks, and I guess that's another way to approach the problem how do you kind of put humans into into these thinking patterns so the there is a bottleneck on the human
3102000	3118000	usability part is going through the human, I guess. Well, right. I mean, so one of the questions is, you know, in some sense, government is like a machine, you know, it has certain regulations that follow you know at least if you're in a sort of following the rule of law type type of place.
3118000	3131000	You know, it's, you know, things go in, it grinds around the bureaucracy operates and something comes out. And it happens to be a machine operating with people, most of the time.
3131000	3145000	Maybe it won't be in the future, maybe it will be mostly operated by AI is in the future. I suspect it will be. Now, what has happened in legal systems and things like that is there's always some appeal mechanism there's always some way of getting more people involved.
3145000	3164000	There's always some way of kind of having a, a kind of broader deeper engagement with people. And perhaps what what ends up happening and sort of reminiscent of some other kinds of systems is, you know, there's a kind of decision by the AI is up to some point.
3164000	3184000	And then kind of you can blow through that and get to humans if you really insist. Remembering that, in the end, it's, at least for now, it's humans in charge, so to speak. So, in other words, it's kind of like, well, the AI can say, just like, you know, if you run a company or something, people are always saying,
3184000	3196000	you know, we figured out this and this and this, what should we actually do. You know, there's a, there's a mechanism inside that figures out the here are two alternatives, what should we actually do. And then the CEO gets to decide or whatever.
3196000	3208000	And so similarly one can imagine a situation where the AI's are are refining the set of possibilities. And then it's like, okay, reaching out to some humans here, what should we actually do.
3208000	3219000	You know, I think that's because in the end, you know, the humans are the ones in charge, so to speak, it's not. There isn't, you know, it could be the case and this is one of the bizarre possibilities.
3219000	3228000	You know, people say, no, let's make a constitution for the AI is let's lock it down. Let's not let any of the humans mess it up. Probably very bad idea.
3228000	3238000	Of course, we've seen that in human history because there are plenty of, you know, cultural traditions where you say, you know, let's, you know, the things that were written down a couple of thousand years ago.
3238000	3250000	Those are a good model for how to lead life and maybe they're not such a bad model. In fact, but it's sort of locked down from a couple of thousand years ago. And it's like, well, we could start thinking about how to change that.
3250000	3263000	And then you get into this whole kind of complicated loop of, do you ignore those traditions? Do you then make, you know, make changes? How does that work? How important is the is the weight of history, so to speak?
3263000	3275000	How important is it that we've evolved in a way that's made use of those those traditions, that history and so on. So it's a it's a complicated thing. And I think the, I don't know how it's going to resolve.
3275000	3292000	I think it's a, it's, and I think it's sort of, I hope that there are more people who can think sensibly about this. And unfortunately, it tends to, in my practical experience, a lot of these questions about sort of the ethics of what should happen.
3293000	3301000	There's a, there's a tremendous tendency for people to say, well, of course, it should be this way, where that happens to be their overall ideology about how things work.
3301000	3315000	And, you know, it's, I think it's challenging for anybody to kind of say, well, what is the neutral, you know, what is just the machinery of how it works. And then, you know, add your own ideology to that.
3315000	3333000	It's challenging to think that way. And I suppose it, but it is also a mistake to say, but there is no, you know, I'm thinking, you know, to, to imagine immediately, I'm thinking that way it's kind of like people who say I'm going to make a model of something, but it's going to be, I'm going to have a way of doing
3333000	3348000	something that doesn't involve making any assumptions in the model. It's a modelist model, so to speak. Modelist models don't exist. In other words, when, you know, when we say somebody says, I've got a neural net, I'm going to, you know, I'm going to model this thing with a neural net.
3348000	3362000	There's no assumptions. I'm not assuming anything. Well, you are, you're assuming that you're going to be able to make the model by changing the weights in a neural net. And that's a huge assumption that happens to map, as I was mentioning, you know, fairly well to the way we humans
3362000	3372000	also make models of things, but it's certainly not the only way you could make those models. You're, you're putting a lot of assumptions into the, into the structure of the model by setting it up that way.
3372000	3385000	And I think the same thing is true with this kind of ideology type thing that there isn't a sort of ideology. There's no ideology list ideology, just as there's no modelist model, so to speak.
3386000	3407000	Right. And those are, but I mean, I think these kinds of things, you know, you imagine, okay, I'm going to make a safe AI system. I'm going to put stuff in the prompt that is going to be, you know, that's going to be the kind of, I'm going to recite, you know, these commandments, so to speak, at the beginning, and then everything's going to be okay.
3407000	3412000	You know, probably not, but it's a complicated issue.
3412000	3428000	Yeah, totally. I think if there's one thing we've learned throughout history is that besides a few young ones today, every constitution and every ideology had an end date. So there is a, it's hard to know if there's never an end.
3429000	3438000	Well, I mean, the point is, this is one of the features of computational irreducibility, something different is always going to happen, something unexpected is always going to happen.
3438000	3454000	And that, that's a thing where you, and, and the, you know, the humans will adapt to it in some way, and they will, you know, they'll they'll make some arbitrary decision about what to do based on that unexpected thing that happened.
3454000	3482000	And maybe that will be a thing and it will be a thing that is made in some sort of societal way. And that's some, that's kind of how it develops. But, but yeah, no, I think it's some, but this, I mean, this idea of kind of what, what is the future of sort of generalized programming, where programming, you know, rolls into it, legal contracts and things like this.
3482000	3497000	That's, you know, what does that really look like? How does one, how does one imagine setting up the, the world of, of, you know, when, when legal contracts and, you know, software are the same thing, so to speak.
3497000	3508000	What does that look like in the, you know, and by the way, I mean, you know, the things that will happen, as always happens with automation, programming is about to get a lot cheaper.
3508000	3524000	So there'll be more of it. And so more things will be, you know, more things will be done with code than were done before with code, just as if it gets cheaper to make legal documents as it already just did with boilerplate-ish ones, there'll be more of those floating around.
3524000	3536000	And it's, you know, just as in people, you know, making sales pitches or something, that, you know, there'll be more of those. There already are more of those because people can basically create them automatically.
3536000	3546000	And it's some, and then, and then what tends to happen, you know, they'll, something became easy. There's more of it. And then there's another level of abstraction that comes in.
3546000	3557000	And you then start sort of thinking about, well, what, you know, what can you do then? There are then a bunch of possibilities. Each of those kind of needs humans to decide what direction it's going to go in.
3557000	3566000	And that sort of creates the next generation of job to job categories. And you keep going from there. I mean, you know, you'll have the prompt engineers for a while.
3566000	3574000	And then maybe you get the meta prompt engineers, I don't know. And then it kind of, you know, it gradually abstracts up.
3575000	3594000	But I think, I think that's some, but I do think that there's sort of coming merger of ways to specify things in the world from things like legal contracts, instruction manuals, you know, kind of things, you know, it,
3594000	3600000	that's an interesting moment for, for, for what's going on.
3600000	3611000	So with the automation, I agree with you. Just to devil's advocate really quick, like, is this not a turkey problem? I know, or I guess turkey problem.
3611000	3623000	I think I read about it. I guess, I don't know if it seemed to have coined this or he just talks about it, but like the turkey eats food every day and has a happy life and thinks he's going to have a happy life forever and then Thanksgiving comes and he dies.
3623000	3638000	Is that, is that not necessarily maybe the same thing with this automation to met a lot of levels of abstraction or is computation irreducibility kind of this theorem that definitively will say that there's always abstractions.
3638000	3643000	There's always further to go. Whether we care about that further to go, that's the nontrivial question.
3643000	3646000	In other words, there is always another invention to make.
3646000	3657000	There is what you'll never be at the end of having invented everything that can be invented. That is, in this computationally irreducible kind of tower of possibilities.
3657000	3665000	There are always these little pieces of reducibility, these inventions you can find that allow you to jump a little bit forward and they're an infinite number of those.
3665000	3680000	And there's never, there's never an end to those. Now, you can decide, okay, we've gone far enough. We're now happy. The things we've done so far are enough and we can, we can just rest on our laurels and never invent anything more.
3680000	3692000	I don't think that works, because I think that the very evolution of the world, so to speak, which has its own computational irreducibility will always lead us to that new thing that we didn't expect.
3693000	3707000	And that, you know, we think we've set up all the cities and we've set the perfect set of roads that do this and that and the other. And then we discover we've set the perfect, you know, way of, of, you know, setting up fields and so on.
3707000	3713000	And then we discover that, you know, we just fertilize the seaweed and it's starting to do crazy things and so on.
3713000	3731000	And, you know, and that then starts that cycle again, if you're never finished, you've always got to invent another thing. So I think that, but, but, you know, you can imagine a situation where, where like the Turkey, for example, perhaps, you get to a point where everything you care about having invented,
3731000	3745000	everything you care about has been automated. You know, we've got, now people might say, interestingly enough, if you look from a few hundred years ago, and people look at modern times, it's like, why are you guys working so hard?
3745000	3756000	Why are you doing anything? You know, you've got enough food to eat, you've got, you know, you've got all these things, you've got all this kind of instant entertainment. It's like just sit back, relax and be happy.
3757000	3774000	But yet that's not actually how people react to that situation. So I think it's, it's kind of an interesting thing that, that you could imagine at a time and this is more a societal question, where everything people care about has been automated.
3775000	3792000	It doesn't last forever, but that could happen for a while, at least. And, and then, and then it's kind of an indeed, like the Thanksgiving Day, you know, there will be some sort of driver from computational irreducibility that something
3792000	3802000	unexpected will happen. You know, you can be, I know there have been times in human history where people have sort of said, we're just going to keep doing the same thing. We're going to keep, you know, hurting our goats. We're going to keep doing this stuff.
3802000	3817000	We're going to do it for hundreds of years, thousands of years maybe, and it's all good. And it's a, you know, it's a satisfying life where we're happy, you know, we have children, the children heard the goats as well, everybody's everybody's happy here.
3817000	3833000	And then, you know, that most likely, you know, external drivers eventually cause that to change. But it's certainly the case that we could imagine a situation where, you know, as I say, the AIs have automated things.
3833000	3853000	It is an interesting thing to think about that, you know, our reaction to modern times would be probably seen as very bizarre to somebody from 3, 400 years ago, because we've solved so many of the problems that existed, you know, we've solved sort of early mortality, you know, we've, you know, to a large extent, we've
3854000	3865000	solved, you know, having enough food to eat in most parts of the world, we've solved, you know, lots of kinds of things, we've solved transmission of knowledge, we've solved, you know, all kinds of stuff.
3865000	3889000	And it's like, yet, you know, I just did this study of kind of how, what work gets done in the world, and, you know, kind of what people, you know, the evolution of jobs and so on, I looked at a bunch of history from the last, well, looked at kind of the jobs people have done in the last 150 years, and also about, so what was it, 60 years or so, of how people spend their time.
3889000	3904000	And it is interesting that even in this moment of sort of increased kind of, oh, just sit back, you know, they're going to be shorter work weeks, actually, the average amount of time for people who are working at all, that they work stays at about eight hours a day.
3904000	3921000	And over the course of the last 60 years, the only things you see, for example, you see media and computing goes from like two hours a day for people who aren't working, goes from two hours a day to six hours a day.
3921000	3927000	So that's a, you know, there are more couch potatoes, so to speak.
3927000	3937000	And, or maybe they just, I don't know what, you know, it's a question of how the time use service work, whether they could be, you know, maybe that maybe that counts programming recreational programming, I don't know.
3937000	3954000	But it is interesting to see that, you know, we humans seem to find things to do, even if, you know, the things that we thought were the oh my gosh, if we can only do that we can sit back and relax doesn't turn out to work out that way.
3954000	3974000	Right, right. No, I like I like this, the optimism of progress that they'll always be progress, they'll based on this math map based on math, they'll always be more to do and that's exciting, you know, that's a there's a lot of doom and gloom in the world of AI, but I think the exciting part is a better way.
3975000	3992000	No, I think that the, it's, you know, the question is really what people want to do, because you could take the point of view, as soon as you've got enough to eat and as soon as you whatever, you're done, you're just going to, you know, you're just going to hang from there on out, so to speak.
3993000	4012000	And I think that that's, that is really a, in a sense, the sort of, I don't know, perhaps moral fiber some kind of fiber of society is what do you choose to do in a time when, when, and that's just very interesting to see you know if you look at different countries around the world that are for example rich in
4013000	4020000	resources, where, where there's sort of not, you know, some places people do a lot, some people places people don't do a lot.
4020000	4041000	It's, it's interesting and it's, it's, it's a, you know, I think this, you know, it'll be, in a sense one thing to really recognize is that in a sense, AI, particularly in the LLM kind of world is, is kind of a reflection back on us of, you know, what, you know, we've created we've, it has
4042000	4051000	been equivalent from everything we've done. And now we are seeing what that looks like reflected back at us. And we may or may not like it.
4051000	4067000	And, but, but, you know, it's, I think the other thing that's a little bit of a shame right now is that sort of the world of knowledge work is about to get kind of standardized and the following sense that, you know, it used to be the case everybody would write an essay for themselves.
4067000	4080000	It, it used to be the case people would write, I don't know, some, you know, blurb for a conference or something they'd write it themselves. But now it's like, Well, why bother, you know, we can, you know, we're trying to communicate something but, you know, chat you
4080000	4096000	can do a pretty good job given that we fed it in the, the basic brief for what we wanted it'll it'll spin the words. And now that becomes kind of the, you know, the standard way that a such and such thing is written that we humans can then find it convenient to read,
4096000	4110000	but it sort of standardizes things and I don't know what's going to happen as a result of this kind of sort of pushing everything to standardization it's kind of like what happened when, you know, there was a time when every book was hand written hand copied, and then printing
4110000	4120000	started off, and then it was just like here's the font is what the a looks like in that font. And, you know, I don't know how that's how that's quite going to play out.
4120000	4135000	Well, I think that, like we said, the abstraction layer is going to get higher if everything standardized, it's gonna, you got to be more unique if every book at anyone can make a really good book, the bar for having something really unique to say is just higher and we're
4135000	4137000	we're getting a little more meta here but
4137000	4142000	Right, probably that's the case. I mean, I think that the, the
4142000	4158000	there's a lot. I mean, if we look at history, there's a lot of things that sort of got standardized everybody got them, you know, they were originally only, you know, only the king had one of those things, and then it got standardized and everybody got it.
4158000	4169000	You know, everybody got the iPhone, not that there were two different moments in history, probably, but the, you know, I think that then.
4169000	4187000	Yeah, no, it's, it will be interesting to see what what it how how things evolve in terms of the value that people place on, for example, the value people place on having code you can understand the value people place on.
4187000	4200000	You know, it's like, well, yeah, you know, it used to be true until three months ago, that if you saw a well written English, you know, essay, you knew somebody put a lot of effort into it.
4200000	4202000	Now it isn't true anymore.
4202000	4216000	And that used to be something, you know, when I was a kid, for example, if you got a printed invitation to something or something like that, you knew it was kind of a somebody had really put it, you know, it was a big deal, so to speak.
4216000	4225000	Nowadays, you know, after desktop publishing, it was no longer the case everybody could make a beautiful sort of printed looking invitation or whatever else it is.
4225000	4229000	It no longer was a signal of a lot of human effort.
4229000	4244000	And, you know, until three months ago, having a big essay was a signal of human effort and there are many things in in the operation of bureaucracy, for example, where kind of it says, you know, write an essay that describes this or that thing because people know that writing that
4244000	4249000	takes human commitment and human effort or that word did take human effort until three months ago.
4249000	4257000	And I think that, you know, that will be the sort of be there's some adapting to do when it when it comes to that type of thing.
4257000	4268000	There's a real arbitrage opportunity now to do a personalized LinkedIn messages and stuff like that but now as soon when everybody's able to make them really well with these technologies.
4269000	4288000	That's kind of going away. I mean, it almost, you know, first of all, we had SEO writing, then we have, you know, chat chat bot writing LLM writing, and then we'll have LLM meets SEO writing and then we'll have the SEO using, you know, using LLM to
4289000	4312000	decode things and it's the, you know, in the end, it's, in the end, the bad thing is a lot of it may just get very, very, very standardized. It may be very kind of rote. It's just like in, you know, maybe almost a recitation of some standard thing, which is sort of interesting.
4312000	4334000	And it kind of becomes almost ritualistic that, you know, it's just you're, you're, you're incanting this, you know, there's some incantation that you write that that sort of the end point of the perfectly perfected LLM that just optimally, you know, optimally invented the thing you should say in your LinkedIn message.
4334000	4341000	And then they're all, you know, they're all the same incantation so to speak, because that was the optimal one I don't know how it will work out.
4341000	4345000	But then you stop caring about the optimal one and the goal post change.
4345000	4346000	Well, yes, right.
4346000	4348000	Well, you stop caring about that medium.
4348000	4350000	Right, right, right.
4350000	4360000	It becomes, for example, yeah, right. I mean, I think the, the, you know, the handwritten letter is still a thing for another year or two.
4360000	4369000	I think so too. I think, you know, it means a lot. If I'm, if I'm going on a date with a girl and I give her a handwritten letter versus I text her.
4369000	4371000	Big difference, but
4371000	4373000	Right.
4373000	4380000	But anyway, well, very interesting stuff.
4380000	4384000	Yeah, really, really appreciate you taking the time to chat about this. I know.
4384000	4395000	There's a, we went all over the place, but it's interesting to think about where the world's going with all this, but just, just want to say that everyone needs to check out the Wolfram plugin on chat.
4395000	4402000	I think it's really going to be game changing, kind of getting computation in there and excited. Who knows where this is going next.
4402000	4413000	So excited to see and excited to see what comes out of your company here. If this was only two months, I can't imagine the next year, but again, really appreciate you taking the time to chat.
