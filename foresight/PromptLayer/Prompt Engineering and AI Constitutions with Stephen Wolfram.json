{"text": " So welcome to prompting with prompt layer conversation we're doing with many people in AI and we're kicking it off now with Steven Wolfram, who we have here. And yeah, just real quick prompt layer where building dev tools for people bringing LLMs into production like GPT. So if you want to do some data driven prompt engineering, we're the place for that and we're going to have we're going to have a pretty cool conversation. I hope we're going to nerd out a little bit here and excited for that. So you probably know who Steven Wolfram is what he's done. I learned about his work from doing math homework properly in middle school or high school but originally originally met Steven when he came to a hackathon I was running in high school and he stayed there till like 2am mentoring people and that's kind of my first introduction to this theory about computational thinking and it's come back around, you know, and it's very important today. So, yeah, really excited to have this conversation with you and maybe to kick it off. You had a big and a big release today of kind of plugins into chat GPT and this is, I think you might have said it but I completely agree it's kind of a historic moment where we're kind of bringing the LLM side of things that AI statistical AI to the symbolic AI to the symbolic interpretation and that sort of thing so like, how did we get here what what has the process been and what why is it so significant right now. Well I think the what LLMs have been able to do is sort of take the corpus of texts that we humans have written and kind of grind it up to the point where we can make more that's like it so to speak so we give a prompt. And you know the the role of an LLM is to continue from that prompt and that's and sort of continue with things that are like the things that have already been written on the web. And I think, and it does that in a way that is kind of just sort of statistically fitting things together. It's a very different tradition of computation that is really able to use the deeper aspects of computation really able to use sort of the irreducible computations that can in principle be done. That's the world that I've lived in for for many decades now of trying to figure out sort of how to take what is computationally possible and make it accessible to humans. So the thing that's been sort of exciting is that well we made a lot of effort to make it accessible to humans turns out that also makes it accessible to language based ai's that kind of lent their craft from from humans so to speak. So kind of the way I see it is these have been the two great kind of traditions of AI, and we've now got this opportunity to really connect them together through the medium of sort of a mixture of natural language and computational language, and be able to get it that one can kind of use the linguistic interface that we are all used to because we're all very experienced at sort of interacting in natural language, together with kind of the depth of that rather in human non human thing that is sort of powerful for computation. So it's kind of bringing those two things together the, the very human language with the very sort of beyond human non human power of kind of deep computation, and you know the fact is that, as a practical matter, LLMs are making sort of statistically reasonable pieces of text, which may or may not actually be the way the world is. And what we've been involved in doing is kind of making a representation of the world as the world is in computable form, and being able to bring those two things together so that we're able to let the LLM sort of make it statistical kind of text, and then we're able to provide sort of precise computable insertions into that text to kind of provide the facts, where the LLM can be writing fact or fiction, and it doesn't really know the difference so to speak. Right, right. No, this is super interesting and like, and I guess it's, and it's changed so much in the past like two or three months. I guess like to dive deeper here. Maybe, I'll give you an example so, and I'd love for you to explain kind of your thinking behind this, and this is changed today with the welfare and plug in and chat GPT obviously but I had a friend Avi and he tweet. Let me let me just read the actual tweet so he was basically he tweeted that he was using GPT 3.5 then he was using GPT for and it was failing on this very simple problem that he was doing and the prompt was how many unique apartment units are represented in the data below. As in two people in the same unit, for example, 430 would count as one and then you had a bunch of names with different apartment numbers, and his question was to him, like, this is not obviously a hard problem in the let's take a step back from what's symbolic what's statistical, just at the space level if I'm a hacker on just like using open AI, this doesn't seem like a hard problem but it wasn't a problem GPT is able to solve on its own it's not a problem LLMs are able to solve. Would would love to hear you kind of break that down and explain what about this problem is hard for LLMs and what about it it's easy for the warframe language. So, anything that involves precisely computing something with math. There can be many steps that are involved in doing a math computation and LLM is a feed forward network that you know the current version of them at least is sort of feed forward networks where you kind of, you know, it's encoded a lot of stuff. But basically you fed it the the the text so far, and it sort of ripples through the network and then tells you the probabilities of next words to follow. And that's just not something where if they're needed to be something where iterated recursed whatever else to figure out what's the answer to this thing. That's not something that's going to happen inside the current architecture of LLM that the way that LLMs managed to not be sort of computationally trivial is that they are continually eating their own tails so to speak they're continually kind of processing the things that they have generated so far and that's what allows them that that little sort of extra piece is what allows them to have a lot more richness than just a pure, you know ripple through the network once type of type of system. But still the kinds of things that are really easy in terms of traditional kind of Turing machine computation are are really hard. They're really not possible actually for something which is fundamentally doing this you know ripple through once type type method. I think that the challenge, I mean, in terms of sort of the the architecture for AI is using tools is how does the AI know what tool to use what what when to use the tool, how to present information to the tool to be used, and then how to interpret the information that comes back. We've been kind of fortunate in this case because we built Wolf from Alpha to take natural language input from humans and turns out LLMs can also produce natural language input. And so, you know, we already have a funnel basically to collect the things that LLMs naturally produce now in fact the endpoint that is in the Wolf and plug in for chat GPT is a combination Wolf from Alpha Wolf and language endpoints. And that's an elaborate piece of engineering because the LLM kind of and you know we we try to make the prompt, I don't know how successful we've been so far, we try to make the prompt decide, should it try and send it to Wolf mouth or when because it has a natural language input, and it's expecting natural language collection of outputs, or should we try and send it to Wolf from language where it has to synthesize this precise computational language. And then when it sends it to often language is it actually correct Wolf from language, it has to go, it basically goes and reads the documentation that's what we've told it to do at least to figure out, is it a correctly formed piece of Wolf from language input. And then when it gets the result, it's a very, you know, here's the result that's all there is, and Wolf mouth, the result can be this long series of, of, of things which which chat GPT can then knit into an essay and it does pretty good job of doing that. I think that, so I mean that there these different, different modalities but the fundamental point is, when it's a computation that involves kind of looping and recursing and so on. It's just not something that you can expect to be able to do from from from a straight LLM now there are other issues like, like things to do with tokens, and the fact that tokens and numbers don't match very easily, although that's something one could definitely fix. But, you know, and, and, you know, if you're doing even something like multiplication, if you're just doing it in pure LLM land, what are you going to do memorize all of the, you know, two digit three digit multiplications. Well, then you have the next case, and it may or may not be true that from the innards of the LLM that it can untangle kind of doing the carries and all this kind of thing. Probably it can untangle that if you allow it to, you know, talk about its intermediate steps, it has an easier time, if it can talk about its intermediate steps because essentially it's storing intermediate it's got an intermediate storage location, so to speak that is the actual text it's not doing. But it's not a very, you know, the, the, the best way to do that is just you just compute, and you do sort of traditional computation on that to get that result and then you knit it back into the kind of natural language world of the LLM. Interesting. So, what, why is it that having a kind of a, but asking the LLM to explain the steps makes it a little more correct. Is it is it not just finding like, I guess the completion that's most close to all the steps there's something I guess that changes the problem by breaking it down there or I don't know if you have any insight. The problem is that if you've got something that involves intermediate steps, the LLM cannot internally, when we write a program in traditional, I don't know, orphan language or something, it's trivial for it to go and, you know, iterate through many, many times. You know, it's trying to find the answer to this thing, it's going to run it a million times internally, it's all good. But when, if it's an LLM, it doesn't get to do that. Every, every token that it is emitting, it's just rippling through once. Right. And so in order for it to have this sort of intermediate state where it kind of iterates on it, it's got to actually write out that intermediate state in the text stream that it generates. And so that's why it can be better if you ask it to explain itself, so to speak, because then it has the each individual kind of ripple through each individual new token is less has to happen. And it's kind of doing that sort of building up of the tower through the explicit text that it's generating and then and then redoing things from that text. Interesting, interesting. Yeah, that makes sense. And I guess like, one thing I'm curious about, I guess, your thoughts on and is this the, and I guess what the steady state is is what it like the interfaces here it sounds like this was kind of, this is like a two month thing that went from no to GPT with plugins and stuff like that or chat with plugins and the Wolfram plugin and you mentioned there's the Wolfram language input and the Wolfram alpha input. Is that do you see there always be like, is it like Wolfram alpha as the fallback or how do you see that evolving over time or is this like, we'll see who knows. Well, I mean, the combination the combining of the Wolfram alpha endpoint and the Wolfram language endpoint started last weekend. I love it. It's, it's a very non trivial piece of. Oh, I don't know combination AI thinking prompt engineering software engineering, etc, etc, etc, because those are running on different, you know, in different on different clouds that you know it's a complicated thing it was it was complicated to pull off. I think the, yes, basically right now, I see it as, you know, the LLM if it's just yacking in language, it's going to send it to Wolf now because Wolfram language can't do anything with that. If it manages to successfully, it's been trained on a certain amount of Wolfram language examples. Probably it'll be trained on a lot more such examples in the future. As it gets better at synthesizing Wolfram language code. It's probably ultimately more powerful to just go straight to the Wolfram language form. But that's, it's, it also can make horrible mistakes doing that. Wolfram Alpha is a good catchment mechanism, because it's already got, you know, many years of development of natural language understanding, where it knows, you know, given this natural language. First of all, it knows when it doesn't know, which is an important feature. Yes, it's been pretty successfully, you know, set up so that when it doesn't understand the natural language just as I don't understand, it doesn't make something up when it does. Humans. What's that? We need that for humans too. Yeah, right. Well, I think, I think, you know, what's interesting to me is what we learn about kind of human psychology and human operation. By looking at chat GBT and how unbelievably similar the types of mistakes it makes on solving problems and things like that are to the way that human the kinds of mistakes humans make. And I think it really is capturing the essence of sort of the neural net it has is probably not that different in some functional sense from the neural net that we have. And, you know, it's, it's really, it's showing us kind of what it means to be a sort of a human like answer, a human like text generator, so to speak. In terms of the sort of the future in terms of sort of, okay, Wolfram language is a really great language for ai's to try and speak in it's sort of the best language for ai's to try and speak in, because it talks about the real world. It's very self contained, and it's very succinct. I think that it's something where the AI, and the other thing about it is, which I think is is yet what we're about to see this really, really blossom is Wolfram language as a language for human ai collaboration, because you know what we see happen is you say you type in natural and you want to get this this and this. It synthesizes a piece of Wolfram language code. Maybe it's right. Maybe it's wrong. But you can kind of run that code line by line. You know, Wolfram language is a symbolic language so you can always see the output at every at every stage, everything you have is a valid piece of output. So you can see that thing build up graphics build up the structure whatever else it is. You can run it line by line. And as I was, you know, as I've been using it. That's what I've ended up doing it generate some code and it's like really is this right. I don't know. You know, it got it roughly right it's not completely crazy, but to know is it actually right well I have to kind of run it line by line sometimes, sometimes it's obviously wrong, because for example is generating error messages actually one thing we just did in the last two days is when it gives a message, it gives chat GPT's a bunch of text around that message. It feeds it back to chat GPT and says rewrite the code, try and avoid this message. And, you know, how well that will work. Not sure it's it's clearly working in some cases, and chat GPT is wonderfully polite and apologetic when it, when it keeps on doing the rewrite so to speak. But, you know, that's a, I think the, the sort of the place where it's going to look really interesting the kind of in the, in the world of kind of what future programming looks like. I think a big piece of future programming is, you type some text that's kind of the initial here's what I roughly want to do. It synthesizes basically in from what I can see wolf language is the best language for it to synthesize because it is the most succinct language that is the most kind of expected to be actually humans. It's not a big slab of code in some low level language, where the thing is talking about, you know, set this array pointer to this type thing. It's something which is trying to speak at the level that humans think so to speak. And then, then the, you know, the picture will be and again I haven't had the experience yet, except in a few toy examples of seeing how this really works. I type my initial you know this is roughly what I want to do. It synthesizes some code. I look at that code potentially line by line it probably synthesizes and tests for that code, along with the code itself. It runs those things it says this is this line. This is what it did. You look at it you say no that's that's wrong. And then you either tell it that's wrong or you fix it yourself. And then you you're steadily building up this thing that is the kind of the the final code you want that you can then you know we increasingly have efficient compilers to LLVM and things like this so once you've got this very high level representation, you're then kind of which you have been collaborating with the AI to produce, then you can kind of take that and deploy it however you want to deploy it whether it's in the cloud on some, you know, embedded device or whatever else it is. But that's the kind of the point is this collaboration between the human and the AI, where you're leveraging the fact that computational language our computational language is actually readable by humans. And part of what makes it readable by humans is that it is a language that can immediately talk about images and cities and chemicals and movies and things like that. And you're not kind of trying to figure out. Oh, what is the data structure that it uses to talk about an image or something. It's, it's right there something that you can read as being a thing about an image for example. And so I think that's, you know, to me that's a really pretty exciting prospect of kind of the, the future of programming and you know people say what's going to happen to all the programmers. It's like what's going to happen to everybody who does boilerplate, you know, smart boilerplating so to speak. You know, that it's that's something people who are, you know, producing, you know, somewhat boilerplate, you know, documents of various kinds. That's kind of going away. And similarly, you know, people have rushed into kind of learning, you know, going to computer science school and learning how to write, you know, Java code, Python code, whatever else it is. And it's like, a lot of that is just going to go away. Just like, you know, when I was, well younger than you people were always talking about a Sunday language, you know, if you're going to be serious about computing, you've got to write your code in the Sunday language. I don't think anybody says that anymore. You know, in fact, the C compilers or whatever are probably producing better assembly language than any human produces at this point. So, you know, what we're seeing is this kind of this moment where kind of the, the there's in these lower level languages because you kind of you can write a big slab of boilerplate language, which you needed to write as boilerplate language because it was a low level language. In a sense, what we've already done with Wolfman languages automate out that boilerplate by having, you know, the one function that just does the equivalent of that big slab of lower level code. And so that gives us the opportunity to be sort of at a level where we can have a, a meaningful conversation with the AI about what we're trying to do. We can produce kind of, it then produces a first draft of the code at a level that we can understand, we then edit that code, or tell it to edit that code, but we can understand that code we can understand the test cases and so on. And then reiterate, and, and I think it's going to be a really productive way of producing kind of computational functionality. And I think it will also open up that, that capability to a whole range of people where, you know, they didn't learn how to do memory allocation and make sure that the, you know, the pointers stayed aligned and you didn't, you know, whatever else was the, you know, whatever other sort of lower level thing you might be thinking about, they never learned that stuff and they don't need to. Just like for most people programmers today, they don't really need to learn, you know, how assembly language works. Right, right. A lot of interesting stuff there. So, okay, so you don't need to know how assembly language works you don't need to know how to code today. One thing like one thing I've noticed kind of through friends and like hackathons here and stuff like that is like the concept of prompt engineering is a very, I feel like programmers pick it up like this because it's a very tinker heavy, you know, we can assume the neural nets of black box change change the prompt see how it at see see what comes out change it again see what comes out. Is this like, is this a certain type of, is that a skill in itself, is that something that could be learnable what would you, how would you decide. I always enjoy I've been, you know, in the past I've been responsible for a few sort of new job categories of things people might do. Like when wealth mouth came out we had this sort of new job category of linguistic curators. And it's like, you know, we have test cases like, you know, write a thing which you know you say how many ways are there to make change for 35 cents. Well, there are a zillion different ways to say that some people are really good and can generate at output speed 200 ways to say that. What kind of skill is correlated with that I wondered that you know I wanted is this going to be poets is it going to be crossword puzzle people, is it going to be computational linguistics PhDs. Well, actually it was people who did not know that they had that skill, and just like doesn't everybody have the skill, but they're really good at doing it. And I think with prompt engineering, you know, I saw about a week ago I saw for the first time, a thing I was kind of predicting a resume, where somebody had been an animal wrangler. And now they were, they were theming themselves as a prompt engineer. And it's kind of the same, the same type of thing, you know, you're poking at this thing you really don't understand, and you're trying to get an intuition for what it does I think it's more the kind of thing where, you know, I'm not sure whether it's a, a, you know, I think a lot of people who do programming well, and who do programming, sort of in a really thinking pretty. I mean, I don't know when I, I suppose when I try and do programming. I'm always, I think I'm thinking pretty analytically about what's going on. I mean, sometimes I'll just, just try this, but that's pretty rare, I would say relative to the, you know, I think I have sort of an analytical understanding of what's going on. I mean, one of the things that happened to me over the years is I'm now a, you know, a fluent Wolfram language programmer in the following sense, that I can start to type code before I could have told you what the code would say. It's kind of like, as opposed to I'm thinking about it in my mind in some kind of mental model that involves kind of my natural natural language, so to speak. But instead I'm actually thinking internally in Wolfram language. And that's something that again it's a feature of being a high level computational language that you can imagine doing that. And, you know, I can be typing the code and I know quite a lot of other people are in this position as well, where they can type the code before they can explain what the code would say. Just like if you're speaking a foreign language, you know, you, if you're not very proficient at it, you're kind of thinking in English, let's say, and then you're translating into French, as opposed to just sort of thinking fluently in French. Well, that's something you can get to the point of being able to do in computational language. I think that the question of what it takes to be kind of a great prompt engineer, I don't think we know yet. I know, and I'm trying to think in my own company, you know, we've been doing a bunch of prompt engineering. And gosh, I mean, it's a range of people actually have been doing it. And the people. Gosh, I mean, it's the number of people who are very experienced at heuristics, kind of developing heuristics for for now for this and others who are more into just all around. Well, I would say all around thinking more more so than all around programming, so to speak. I kind of, I think it's a it's a new kind of skill. It's a it's a strange skill because it is a skill, maybe a bit more like psychology or animal wrangling than it is, I think more like that than it is programming and as a as a pure kind of I mean, there may come a time when we understand enough about prompt engineering that we're able to kind of have a formal structure for thinking about prompt engineering. No, it's an interesting question and it's something which given that one has, I don't know, I, you know, a reasonable at least high level understanding of how chat GPT is working. You know, can one so an interesting question that I have not really addressed is having made quite a study of how chat GPT works and sort of thinking about it a little bit like, you know, playing physicist on chat GPT so to speak or playing natural scientist. Does that help me to know what I should write in prompts. And I haven't really figured that out. I don't know I haven't had a chance to think about it actually yet. In other words, because it I find it utterly bizarre that it, you know, does it, you know, saying please, does it matter, putting things in capital letters, does it matter, you know, which is, you know, does it pay more attention to the thing that came later, does it, you know, does it matter. You know, for example, I've spent a lot of time in my life figuring out how to explain stuff to people. And I know a lot of rules of thumb. When you're writing something textually about how to do that so that people have a chance of understanding it. Example, you know, one thing I learned very early on decades ago in writing software documentation is another things for that matter is, if you say the critical thing in three magnificent words in the middle of a paragraph, nobody will get it. You know, what you have to do is to some extent, the, the emphasis is in part determined by the area on the page that you spend yacking about that thing. And so does that matter for LLMs. I don't know. You know, it could be that the, that the great prompt engineers are also great human expositors. It could be that the great prompt engineers are people who are more used to, I mean, you know, who are used to sort of talking to babies, talking to animals, so to speak. I'm not sure. I think that the kind of a notion of kind of, well, you could ask as much about sort of formal prompt engineering, if the AI is acting a bit like a human and after all it learned from human language as we've expressed it on the web. What is persuasive writing for a prompt? You know, how do you, can you, and of course, you know, by the time you're saying, well, I'm going to have a meta prompt where I'm going to ask the AI to, to write a prompt for itself. I don't know how well that ends because the fact is that, you know, in the end, you have to say what the heck you're talking about. And then it can, and this is again, the sort of a little bit the fallacy of people were looking at lower level languages and programming languages and saying, gosh, we can get the AI to do this. And the reason you can do that is because those languages are deeply compressible, because they're, they're low level they're talking about things that the computer is doing. They're not talking about sort of the big goals that it's following. As soon as you're talking about the big goals. It's much less kind of compressed by an experience I had wrote this book. Well, I don't know what was it. Gosh, it's very long ago seven years ago now about elementary introduction to the world from language. So I'm very easy book for me to write and I'm kind of upset more people haven't written books like that because it was so easy to write but I then I realized maybe it's easy for me to write and it's not quite so easy for other people to write because I've had lots of experience in trying to explain stuff to people. And the, but there you guys in that book, I had exercises and the exercises are pretty much all of the form. Here's a description of the thing we want to do described in English, right and involved in language. Okay, at the beginning of the book, had an easy time making up those exercises. The English language version was very simple. I was imagining some more from language form. In the book where things getting a little bit more elaborate. It's kind of like, well, I can immediately under, you know, I immediately know what is the law from language version of what I'm trying to say, then I have to back translate it to English. And the thing ends up reading like legal ease. Because, because that's a thing that is not very well expressible in English. So, again, it's kind of, and that of course leads to the question of can you write both language code that is a prompt. And the answer is probably yes. And that's another interesting possibility. And that's, you know, that that becomes a way of expressing yourself, kind of in computational language. But for the AI, which is then kind of itself, thinking in computational language, for example, rather than thinking in English, you know, there's obviously less walking language on the on the web than there is less human written English text on the web. But it's also in a sense much easier to learn it doesn't have a lot of the irregularities of English language. It has, you know, it's something where you can look up the definition, so to speak, you can operate from the definition and so on. So I think it's, we're in very, you know, very early days of the theory of prompt engineering, I think that, but I do think that a very important aspect of kind of this picture is this potential loop between kind of the the prompt, the computational language, seeing what the computational language does, generating automated tests for the computational language. And, you know, and then and then having the LLM essentially present, you know, this is the LLM's version of how to explain that to a human. Here's what it did. Here's the explanation, you know, you may not like it, maybe wrong. But here's the LLM helping to explain it by actually running. And one thing, yeah, it's almost seems trivial to me at this point, but, but the fact that the LLM can go off and run a piece of walking language code and see what happened is very important to its, to its kind of life and times it's what makes it, you know, and you see it, it's kind of weird to see it, you know, it tries this, it says, I'm going to re re re rephrase it, I'm going to try this, etc, etc, etc. I think it will be nice in terms of the sort of IDE aspect of this, to be able to see the code it's producing see the test cases come out, be able to click some things. And then, you know, as you see those those fragments come out, then be able to have a nice way of rolling it up to make a bigger, bigger program, which you can then treat as a single unit to then go on and use that elsewhere. I think, also, I mean, you know, in terms of, oh, one of the things that's just a piece of software engineering that we've been having an amusing time with, you know, we've developed over the last, you know, 36 years pretty good technology for doing automated software testing. And it's like, okay, we've got a software quality assurance team. And, you know, I was telling them, okay, so now you've got to test this, this plugin, and it's like, how are we going to test this, you know, it has no predictability at all, what it's going to do. And, you know, in a sense, the way we can test it, which we've been doing a little bit of, is you have the LLM comment on its own behavior. Yes, this is a, this is something we've heard about a lot, like, I guess, something we're working on is kind of like, how do you test things in prod, how do you evaluate that prompt A is better than prompt B. And one of the things I've heard about, mostly from hackers and mostly from into like the community as opposed to kind of the big companies is this whole synthetic evaluation, how do you have the LLM evaluate itself, which feels like it either will work really well or not work at all. It's kind of the journey remains to be out. But how are you, you're doing that now or it's an idea. We started doing that. Yeah, we started doing it. I mean, I would say it's, it's not for, for reasons of sort of boring software engineering reasons, it's not quite as easy to do yet. You know, in, in, you'll find in our packet repository, you'll find this open AI link, which is a, you know, just an API wrapper in orphan language that allows one to sort of call, call a bunch of open AI APIs. And that's, you know, that that's the thing on which we're building kind of the testing framework. But, you know, it's been the case. I mean, it hasn't software testing involves many sort of pieces of exogenous information, like for example, back from 35 years ago with testing graphics output. Okay. Well, you know, a pixel could change here or there and it doesn't matter. So you have to have a way of doing regression testing that doesn't isn't not affected by individual pixels. And so we were doing early on we were doing image processing later on we've been doing more machine learning type methods to figure out, did it matter, did what happened matter. Another example that is timing tests, you know, you've got 10 million tests, and some of them run slower than the new version, do you care, how many how much slower can they run. That's more a question of sort of a statistical, you know, way of figuring out what what you think is acceptable and what isn't. And it's kind of a. So I think there are all these kind of methodologies. And, you know, one of the things that's kind of nice about interacting with an LLM interacting with computational language, computational language, kind of knows when it went. Well, it knows many aspects of when it went way off track, because it's just, it's just you can't get there from here it's just generating error messages. Things are happening. I mean within Wolfram language, we have pretty good kind of actually we're working on another level of this error handling capability. So, you know, here's the thing I mean when one thinks about programming, everybody thinks about the way the program was supposed to work. And, you know, you can put sort of complicated guardrails with with effort you can sort of put all the guardrails around to make it never do the wrong thing. But most people, most of the time they're rushing to get the program to just do the right thing. And so the, the kind of the error path is a safety net that we would like to automate as much as possible how that safety net works. In other words, we'd like to be able to write to an interesting question whether whether LLMs could help do that is to write the kind of error checking code. But we've we've already built a bunch of things that sort of help automate the error checking process. Now you can't get it exactly right. In other words, you're going to be it's going to be a heuristic thing like there's a path the programmer intended to follow. And there's ways you can fall off that path. You know, if the programmer had really done all the theorems so to speak to know how to keep on that path were well and good. But if the program is kind of lazy, and they're just like well I'm getting this path right but not thinking about all the other things. You've got to kind of fill in the, the, the sort of the, the rail the guardrails for yourself. And that's an interesting problem of kind of the, I view it as the sort of automating the secondary path of the code. The primary path was defined by the programmer question is can you automate what the secondary paths are, and that's something again one can expect to to see perhaps in a sort of IDE environment of the combined kind of computational language, LLM environment of writing programs, one can see kind of the, the way that, you know, well first generating the tests, then being able to, and then potentially you know you look at the tests you say oh I like the way that tests came out. Okay, then, then there's kind of like how do we make, or you say this is how it was, or maybe, maybe the LLM even generates, here are some possible paths that could have been followed, and it tries to bucket together some of the bad things that could happen. And it says what do you want to do in this case, you know, do you want to, if you get one numerical precision, you know failure to converge message, do you want to abort the rocket launch or not. You know, and that's a human decision. No, no, it's not obvious what the answer is, it could be it's better to, you know, keep whatever it is if you're landing the, you know, the plane or whatever it is, it may be better just keep going and try and land the plane, even though you've got that error message, or it may be better to say, you know, abort, you know, whatever, and, you know, go around before you try and land the plane or something. I mean, I think that that's the, so that that's a sort of a human choice but it's something where potentially the LLM, because it knows, it knows human pretty well, could help in being able to make that kind of choice. That's very interesting. And I guess that's the, I mean, I could imagine just the whole like a integration test suite that uses Selenium browser and checks your website, like, I could imagine using something like that, totally. Yeah, I think the case of using LLM to build tests, to build unit tests, seems like, yeah, that, that'll exist. That seems pretty straightforward. You know, I've built a, I use GPT for to make a new element on my page and then I just feed back in the JavaScript errors back into it and bottoming really quick, it fixes it. But the, I think the even more interesting thing here is kind of asking, maybe asking GPT or even training a more specific or fine tuning more specifically a classifier to say, is this output a good output and good bad output is very like, who knows what that means it's very case by case people talk about hallucinations but at the end of the day is the user getting something bad and can I've seen. I've seen I spoke to one team that has a chatbot and they basically feed the results back into GPT and say, if you were the user, how good of experience would you rate this and I think that essentially what was what was done in the, you know, in the final reinforcement learning steps of training chat GPT was essentially that kind of process that they've been human ratings done, and then those human ratings were used to train a classifier and then the classifier was run automatically against the actual LLM. And that's a, you know, that's a clearly an important kind of thing to do. I think, in the case of it's an interesting question when you're dealing with code, and when you're dealing with the sort of what could possibly go wrong with this code, how do you follow the paths, and then it becomes sort of a merger of more like, you know compiler thinking so to speak of what, what can you say about these code paths, what can you prove about the code paths. And then, you know, in this path, which the sort of compiler like thinking could say there is this path. And then you can ask the LLM, if you follow that path, are you going to be happy. And that that's, you know, I think that's a, but I do think that this idea of, you know, people normally, this idea of, sort of, can the LLM estimate when the user is going to be happy. That's probably something, and that that's part of what we've been thinking about not in the context of LLM so much in the secondary pathway for error, for error handling and programs. It's like, what can you see about the heuristically about what is likely to make a user happy. And even though we don't know for sure, because it could be that the user, you know, the user is doing some science program. And this one bizarre case is like the amazing, you know, we just discovered this amazing phenomenon. And oh, the error handler basically says, well, that's not a thing that really happens much that will be an anomaly. Let's go and catch that in the error handler. And then the person never sees that. The error handler can't really expect the truly unexpected, but it can kind of deal with the slightly unexpected, so to speak, the things not foreseen by the programmer, but sort of expected on the basis of general intuition about programming. Yeah, it sounds like I know you write about this a lot this computational irreducibility and I think that's probably like the best way to think about like, is this, is it the halting problem to say can LLM tell if it's correct or not because is that nested. How does that work? I think that, I mean, in the end, you know, one of the issues with the kind of LLMs, like turtles all the way down, so to speak, is that kind of, you know, you say, did it do something that I thought was good, like, you know, ethically good or whatever else. Well, there's no ultimate, you know, you can say, well, how does it compare to what people have written about and, you know, in the in history and literature and so on. And but there's no, there's no sort of ultimate ground truth to that it's well, you know, do, do the humans who are sort of making the decision for how the LLM should work. Are they, do they think it's good, do they not think it's good, you've got to have some kind of grounding there. And I think that's the, you know, you've got to have some set of principles that you say I'm going to follow these principles. Now, computational irreducibility has the consequence that when you think you've written down the principles that determine what will be good and what will be bad for the LLM. The LLM will always come up with a weird case that's not covered by your principles. That, you know, that's kind of the pattern of how that's happened is like with human laws. People say, oh, we've got this, you know, this legal code, and that's going to determine how everything works. And then along comes an AI, for example, that nobody imagined, you know, when the US Constitution was written, for example, you know, people didn't imagine there would be AI's. And so then, well, what do you do because, you know, you can follow this legal code, but it doesn't say anything about what to do when when the AI is responsible for doing this that or the other. And so, you know, you kind of have to patch it and it's the same thing with any of these sort of sets of principles about how an AI should work. And so I guess how would you square this with, I mean, this whole concept that the LLM looked at so much data and kind of discovered these speech patterns and kind of built a model around how to talk and built a model around just like, but let's say my mental model is they've kind of solved natural language right and you can argue back and forth of course but that's that's the mental model there. What, what is the limitation of what else can be discovered through these patterns like it are these moral principles maybe are these reducible to a certain patterns of like human psyche or biology maybe. Interesting question. I mean, I think that the, the fact that neural nets can do human like things they can make human like decisions about images they can make produce, they can generalize from their training data in a human like way with language. Probably that's happening because neural nets are architecturally pretty similar to how brains work. And so they're generalizing the same way. Now, interesting question, which I've certainly thought about. And is this question of are there kind of a set of, for example, you know, moral principles that you can similarly say this is the construction kit. And this is the set of primitives from which you can operate and I know I've talked to people over the course of years about this kind of thing and people occasionally say you should read this philosophy book you can you know this person in cognitive science has done this and I've got a pile of these books. And I have to admit that I haven't really gotten through them. But there certainly are efforts that people have made to sort of try to identify the print the primitives of moral thinking so to speak. And, you know, if you decide this I mean you could have different ways of thinking about ethics and different. But you know, are there is there a construction kit you get this and this and this and these fit together in that way. And is that a reasonable model of how humans make ethical decisions. And it's quite possible yes and it's quite possible that even from all the texts that that you know chat to PT has read that it's learned to pretty good model of how humans make certain kinds of ethical decisions, or at least how people write that they've that they've made certain I mean, this is of course the big conundrum of this stuff is that, you know, people say well what should the how should the ai's make ethical decisions well you say, just copy what the humans do. Then people say, no no no that's not the right thing to do humans do all kinds of wrong things. You know, it shouldn't be just do as the humans do. What should be do as the humans aspire to do. And then it gets much more complicated, because what the humans aspire to do may not be realistic. It may be you know different people will disagree about what the aspirations are, etc etc etc. I mean, it's a very, it's a, I think it is a an important challenge for our times and I, you know, I mentioned this over the last few years and I, I occasionally have mentioned it in in groups where they sort of are supposed to be in the business of figuring out stuff like this like of saying, okay, you know, imagine you're writing the Constitution today in the post ai age. What does it say, what should it say, what are the, what are the principles, you know, what are the truths that we now hold to be self evidence, so to speak, or whatever, in, you know, in the ai age. And I think, you know, I'm not sure I know what the answer is. I mean, in, you know, at what point, for example, very basic ethical question is when should an AI have rights. And, you know, I was a number of years ago I was at some AI ethics conference and this, I raised this question, and some very bouncy philosopher who I've gotten to know better actually and she said, we should do that when the AI is a conscious. That's not very helpful, because this question just loops right back on itself. But, but, you know, something I realized of you rather recently is, let's say you have this autonomous bot hanging out, and it's entertaining people. And it makes all kinds of friends, and it makes a living for itself. It has a Patreon, it's, you know, it's kind of, it's paying its hosting fees through people donating to it and so on. It's a, it's a happy autonomous creature. And maybe it even was created through some bizarre legal construct of, you know, some loop of LLCs where there's no owner to it or something. Okay, so it's hanging out and doesn't have an owner, and it's making a living for itself. And somebody says, oh, it's starting to be, you know, be mean to people, it should be shut down. Okay, how do you decide to do that? And what is the kind of ethics of doing that? Well, one of the things that's obviously the case is that that bot may have made lots of friends, human friends, and you shut the bot down. And those human friends can be very unhappy. And so, you know, when you think you're just dealing with a bot, you've actually, you know, it's connected itself to the human world in a certain way. I mean, it starts to get kind of complicated to know what to sort of what what the right thing to do is. And I think that's a, you know, that's a, that's an interesting challenge for our times that I say, I've, I've thought about it a bit and I, I figure I'm, you know, if it falls to me to have to figure it out, this is a shocking thing because it's not my, you know, it's not the kind of thing I, I usually think about but you know, one could think about all kinds of things but it's it's it's something where even to get kind of a, you know, one of the things in figuring out something like that. And this, I suppose it, in a sense, it's like, imagine you're writing the prompt that will be for the, you know, Asimov style robots that are going to populate the earth type thing. Imagine you're writing the prompt, you put down the, you know, the three laws of robotics or something. And then you say, Okay, that's good, I'm done. And the prompt say, you know, what, what would a better prompt be because that prompt we know from, you know, from Asimov's writing those, those three laws of robotics tangle themselves up very quickly. And, you know, what should we actually, what should we say in that prompt. And even if we could write that prompt in natural language, I think we'd be better off writing it in computational language which has a much more, much more ability to say to answer the what if question, you know, you write it down in natural language, it just is what it says. If you write it in computational language, you can run something and you can say, Let me simulate this situation against this computational language description of what should be done. And then you have a much, you know, you have a bigger sort of a larger cross section of stuff that you get to have defined, rather than rather than just the the pure words that you wrote. I guess that computational language or being able to talk in these discreet might be one way out of that. I guess you could also probably think about in, you could consider just for for for sake of talking, you could say 1776 America let's say that the Constitution there is a bot, you know, but there is the human checks, and I guess that's another way to approach the problem how do you kind of put humans into into these thinking patterns so the there is a bottleneck on the human usability part is going through the human, I guess. Well, right. I mean, so one of the questions is, you know, in some sense, government is like a machine, you know, it has certain regulations that follow you know at least if you're in a sort of following the rule of law type type of place. You know, it's, you know, things go in, it grinds around the bureaucracy operates and something comes out. And it happens to be a machine operating with people, most of the time. Maybe it won't be in the future, maybe it will be mostly operated by AI is in the future. I suspect it will be. Now, what has happened in legal systems and things like that is there's always some appeal mechanism there's always some way of getting more people involved. There's always some way of kind of having a, a kind of broader deeper engagement with people. And perhaps what what ends up happening and sort of reminiscent of some other kinds of systems is, you know, there's a kind of decision by the AI is up to some point. And then kind of you can blow through that and get to humans if you really insist. Remembering that, in the end, it's, at least for now, it's humans in charge, so to speak. So, in other words, it's kind of like, well, the AI can say, just like, you know, if you run a company or something, people are always saying, you know, we figured out this and this and this, what should we actually do. You know, there's a, there's a mechanism inside that figures out the here are two alternatives, what should we actually do. And then the CEO gets to decide or whatever. And so similarly one can imagine a situation where the AI's are are refining the set of possibilities. And then it's like, okay, reaching out to some humans here, what should we actually do. You know, I think that's because in the end, you know, the humans are the ones in charge, so to speak, it's not. There isn't, you know, it could be the case and this is one of the bizarre possibilities. You know, people say, no, let's make a constitution for the AI is let's lock it down. Let's not let any of the humans mess it up. Probably very bad idea. Of course, we've seen that in human history because there are plenty of, you know, cultural traditions where you say, you know, let's, you know, the things that were written down a couple of thousand years ago. Those are a good model for how to lead life and maybe they're not such a bad model. In fact, but it's sort of locked down from a couple of thousand years ago. And it's like, well, we could start thinking about how to change that. And then you get into this whole kind of complicated loop of, do you ignore those traditions? Do you then make, you know, make changes? How does that work? How important is the is the weight of history, so to speak? How important is it that we've evolved in a way that's made use of those those traditions, that history and so on. So it's a it's a complicated thing. And I think the, I don't know how it's going to resolve. I think it's a, it's, and I think it's sort of, I hope that there are more people who can think sensibly about this. And unfortunately, it tends to, in my practical experience, a lot of these questions about sort of the ethics of what should happen. There's a, there's a tremendous tendency for people to say, well, of course, it should be this way, where that happens to be their overall ideology about how things work. And, you know, it's, I think it's challenging for anybody to kind of say, well, what is the neutral, you know, what is just the machinery of how it works. And then, you know, add your own ideology to that. It's challenging to think that way. And I suppose it, but it is also a mistake to say, but there is no, you know, I'm thinking, you know, to, to imagine immediately, I'm thinking that way it's kind of like people who say I'm going to make a model of something, but it's going to be, I'm going to have a way of doing something that doesn't involve making any assumptions in the model. It's a modelist model, so to speak. Modelist models don't exist. In other words, when, you know, when we say somebody says, I've got a neural net, I'm going to, you know, I'm going to model this thing with a neural net. There's no assumptions. I'm not assuming anything. Well, you are, you're assuming that you're going to be able to make the model by changing the weights in a neural net. And that's a huge assumption that happens to map, as I was mentioning, you know, fairly well to the way we humans also make models of things, but it's certainly not the only way you could make those models. You're, you're putting a lot of assumptions into the, into the structure of the model by setting it up that way. And I think the same thing is true with this kind of ideology type thing that there isn't a sort of ideology. There's no ideology list ideology, just as there's no modelist model, so to speak. Right. And those are, but I mean, I think these kinds of things, you know, you imagine, okay, I'm going to make a safe AI system. I'm going to put stuff in the prompt that is going to be, you know, that's going to be the kind of, I'm going to recite, you know, these commandments, so to speak, at the beginning, and then everything's going to be okay. You know, probably not, but it's a complicated issue. Yeah, totally. I think if there's one thing we've learned throughout history is that besides a few young ones today, every constitution and every ideology had an end date. So there is a, it's hard to know if there's never an end. Well, I mean, the point is, this is one of the features of computational irreducibility, something different is always going to happen, something unexpected is always going to happen. And that, that's a thing where you, and, and the, you know, the humans will adapt to it in some way, and they will, you know, they'll they'll make some arbitrary decision about what to do based on that unexpected thing that happened. And maybe that will be a thing and it will be a thing that is made in some sort of societal way. And that's some, that's kind of how it develops. But, but yeah, no, I think it's some, but this, I mean, this idea of kind of what, what is the future of sort of generalized programming, where programming, you know, rolls into it, legal contracts and things like this. That's, you know, what does that really look like? How does one, how does one imagine setting up the, the world of, of, you know, when, when legal contracts and, you know, software are the same thing, so to speak. What does that look like in the, you know, and by the way, I mean, you know, the things that will happen, as always happens with automation, programming is about to get a lot cheaper. So there'll be more of it. And so more things will be, you know, more things will be done with code than were done before with code, just as if it gets cheaper to make legal documents as it already just did with boilerplate-ish ones, there'll be more of those floating around. And it's, you know, just as in people, you know, making sales pitches or something, that, you know, there'll be more of those. There already are more of those because people can basically create them automatically. And it's some, and then, and then what tends to happen, you know, they'll, something became easy. There's more of it. And then there's another level of abstraction that comes in. And you then start sort of thinking about, well, what, you know, what can you do then? There are then a bunch of possibilities. Each of those kind of needs humans to decide what direction it's going to go in. And that sort of creates the next generation of job to job categories. And you keep going from there. I mean, you know, you'll have the prompt engineers for a while. And then maybe you get the meta prompt engineers, I don't know. And then it kind of, you know, it gradually abstracts up. But I think, I think that's some, but I do think that there's sort of coming merger of ways to specify things in the world from things like legal contracts, instruction manuals, you know, kind of things, you know, it, that's an interesting moment for, for, for what's going on. So with the automation, I agree with you. Just to devil's advocate really quick, like, is this not a turkey problem? I know, or I guess turkey problem. I think I read about it. I guess, I don't know if it seemed to have coined this or he just talks about it, but like the turkey eats food every day and has a happy life and thinks he's going to have a happy life forever and then Thanksgiving comes and he dies. Is that, is that not necessarily maybe the same thing with this automation to met a lot of levels of abstraction or is computation irreducibility kind of this theorem that definitively will say that there's always abstractions. There's always further to go. Whether we care about that further to go, that's the nontrivial question. In other words, there is always another invention to make. There is what you'll never be at the end of having invented everything that can be invented. That is, in this computationally irreducible kind of tower of possibilities. There are always these little pieces of reducibility, these inventions you can find that allow you to jump a little bit forward and they're an infinite number of those. And there's never, there's never an end to those. Now, you can decide, okay, we've gone far enough. We're now happy. The things we've done so far are enough and we can, we can just rest on our laurels and never invent anything more. I don't think that works, because I think that the very evolution of the world, so to speak, which has its own computational irreducibility will always lead us to that new thing that we didn't expect. And that, you know, we think we've set up all the cities and we've set the perfect set of roads that do this and that and the other. And then we discover we've set the perfect, you know, way of, of, you know, setting up fields and so on. And then we discover that, you know, we just fertilize the seaweed and it's starting to do crazy things and so on. And, you know, and that then starts that cycle again, if you're never finished, you've always got to invent another thing. So I think that, but, but, you know, you can imagine a situation where, where like the Turkey, for example, perhaps, you get to a point where everything you care about having invented, everything you care about has been automated. You know, we've got, now people might say, interestingly enough, if you look from a few hundred years ago, and people look at modern times, it's like, why are you guys working so hard? Why are you doing anything? You know, you've got enough food to eat, you've got, you know, you've got all these things, you've got all this kind of instant entertainment. It's like just sit back, relax and be happy. But yet that's not actually how people react to that situation. So I think it's, it's kind of an interesting thing that, that you could imagine at a time and this is more a societal question, where everything people care about has been automated. It doesn't last forever, but that could happen for a while, at least. And, and then, and then it's kind of an indeed, like the Thanksgiving Day, you know, there will be some sort of driver from computational irreducibility that something unexpected will happen. You know, you can be, I know there have been times in human history where people have sort of said, we're just going to keep doing the same thing. We're going to keep, you know, hurting our goats. We're going to keep doing this stuff. We're going to do it for hundreds of years, thousands of years maybe, and it's all good. And it's a, you know, it's a satisfying life where we're happy, you know, we have children, the children heard the goats as well, everybody's everybody's happy here. And then, you know, that most likely, you know, external drivers eventually cause that to change. But it's certainly the case that we could imagine a situation where, you know, as I say, the AIs have automated things. It is an interesting thing to think about that, you know, our reaction to modern times would be probably seen as very bizarre to somebody from 3, 400 years ago, because we've solved so many of the problems that existed, you know, we've solved sort of early mortality, you know, we've, you know, to a large extent, we've solved, you know, having enough food to eat in most parts of the world, we've solved, you know, lots of kinds of things, we've solved transmission of knowledge, we've solved, you know, all kinds of stuff. And it's like, yet, you know, I just did this study of kind of how, what work gets done in the world, and, you know, kind of what people, you know, the evolution of jobs and so on, I looked at a bunch of history from the last, well, looked at kind of the jobs people have done in the last 150 years, and also about, so what was it, 60 years or so, of how people spend their time. And it is interesting that even in this moment of sort of increased kind of, oh, just sit back, you know, they're going to be shorter work weeks, actually, the average amount of time for people who are working at all, that they work stays at about eight hours a day. And over the course of the last 60 years, the only things you see, for example, you see media and computing goes from like two hours a day for people who aren't working, goes from two hours a day to six hours a day. So that's a, you know, there are more couch potatoes, so to speak. And, or maybe they just, I don't know what, you know, it's a question of how the time use service work, whether they could be, you know, maybe that maybe that counts programming recreational programming, I don't know. But it is interesting to see that, you know, we humans seem to find things to do, even if, you know, the things that we thought were the oh my gosh, if we can only do that we can sit back and relax doesn't turn out to work out that way. Right, right. No, I like I like this, the optimism of progress that they'll always be progress, they'll based on this math map based on math, they'll always be more to do and that's exciting, you know, that's a there's a lot of doom and gloom in the world of AI, but I think the exciting part is a better way. No, I think that the, it's, you know, the question is really what people want to do, because you could take the point of view, as soon as you've got enough to eat and as soon as you whatever, you're done, you're just going to, you know, you're just going to hang from there on out, so to speak. And I think that that's, that is really a, in a sense, the sort of, I don't know, perhaps moral fiber some kind of fiber of society is what do you choose to do in a time when, when, and that's just very interesting to see you know if you look at different countries around the world that are for example rich in resources, where, where there's sort of not, you know, some places people do a lot, some people places people don't do a lot. It's, it's interesting and it's, it's, it's a, you know, I think this, you know, it'll be, in a sense one thing to really recognize is that in a sense, AI, particularly in the LLM kind of world is, is kind of a reflection back on us of, you know, what, you know, we've created we've, it has been equivalent from everything we've done. And now we are seeing what that looks like reflected back at us. And we may or may not like it. And, but, but, you know, it's, I think the other thing that's a little bit of a shame right now is that sort of the world of knowledge work is about to get kind of standardized and the following sense that, you know, it used to be the case everybody would write an essay for themselves. It, it used to be the case people would write, I don't know, some, you know, blurb for a conference or something they'd write it themselves. But now it's like, Well, why bother, you know, we can, you know, we're trying to communicate something but, you know, chat you can do a pretty good job given that we fed it in the, the basic brief for what we wanted it'll it'll spin the words. And now that becomes kind of the, you know, the standard way that a such and such thing is written that we humans can then find it convenient to read, but it sort of standardizes things and I don't know what's going to happen as a result of this kind of sort of pushing everything to standardization it's kind of like what happened when, you know, there was a time when every book was hand written hand copied, and then printing started off, and then it was just like here's the font is what the a looks like in that font. And, you know, I don't know how that's how that's quite going to play out. Well, I think that, like we said, the abstraction layer is going to get higher if everything standardized, it's gonna, you got to be more unique if every book at anyone can make a really good book, the bar for having something really unique to say is just higher and we're we're getting a little more meta here but Right, probably that's the case. I mean, I think that the, the there's a lot. I mean, if we look at history, there's a lot of things that sort of got standardized everybody got them, you know, they were originally only, you know, only the king had one of those things, and then it got standardized and everybody got it. You know, everybody got the iPhone, not that there were two different moments in history, probably, but the, you know, I think that then. Yeah, no, it's, it will be interesting to see what what it how how things evolve in terms of the value that people place on, for example, the value people place on having code you can understand the value people place on. You know, it's like, well, yeah, you know, it used to be true until three months ago, that if you saw a well written English, you know, essay, you knew somebody put a lot of effort into it. Now it isn't true anymore. And that used to be something, you know, when I was a kid, for example, if you got a printed invitation to something or something like that, you knew it was kind of a somebody had really put it, you know, it was a big deal, so to speak. Nowadays, you know, after desktop publishing, it was no longer the case everybody could make a beautiful sort of printed looking invitation or whatever else it is. It no longer was a signal of a lot of human effort. And, you know, until three months ago, having a big essay was a signal of human effort and there are many things in in the operation of bureaucracy, for example, where kind of it says, you know, write an essay that describes this or that thing because people know that writing that takes human commitment and human effort or that word did take human effort until three months ago. And I think that, you know, that will be the sort of be there's some adapting to do when it when it comes to that type of thing. There's a real arbitrage opportunity now to do a personalized LinkedIn messages and stuff like that but now as soon when everybody's able to make them really well with these technologies. That's kind of going away. I mean, it almost, you know, first of all, we had SEO writing, then we have, you know, chat chat bot writing LLM writing, and then we'll have LLM meets SEO writing and then we'll have the SEO using, you know, using LLM to decode things and it's the, you know, in the end, it's, in the end, the bad thing is a lot of it may just get very, very, very standardized. It may be very kind of rote. It's just like in, you know, maybe almost a recitation of some standard thing, which is sort of interesting. And it kind of becomes almost ritualistic that, you know, it's just you're, you're, you're incanting this, you know, there's some incantation that you write that that sort of the end point of the perfectly perfected LLM that just optimally, you know, optimally invented the thing you should say in your LinkedIn message. And then they're all, you know, they're all the same incantation so to speak, because that was the optimal one I don't know how it will work out. But then you stop caring about the optimal one and the goal post change. Well, yes, right. Well, you stop caring about that medium. Right, right, right. It becomes, for example, yeah, right. I mean, I think the, the, you know, the handwritten letter is still a thing for another year or two. I think so too. I think, you know, it means a lot. If I'm, if I'm going on a date with a girl and I give her a handwritten letter versus I text her. Big difference, but Right. But anyway, well, very interesting stuff. Yeah, really, really appreciate you taking the time to chat about this. I know. There's a, we went all over the place, but it's interesting to think about where the world's going with all this, but just, just want to say that everyone needs to check out the Wolfram plugin on chat. I think it's really going to be game changing, kind of getting computation in there and excited. Who knows where this is going next. So excited to see and excited to see what comes out of your company here. If this was only two months, I can't imagine the next year, but again, really appreciate you taking the time to chat.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " So welcome to prompting with prompt layer conversation we're doing with many people in AI and we're kicking it off now with Steven Wolfram, who we have here.", "tokens": [50364, 407, 2928, 281, 12391, 278, 365, 12391, 4583, 3761, 321, 434, 884, 365, 867, 561, 294, 7318, 293, 321, 434, 19137, 309, 766, 586, 365, 12754, 16634, 2356, 11, 567, 321, 362, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.169709726541984, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.026714302599430084}, {"id": 1, "seek": 0, "start": 10.0, "end": 18.0, "text": " And yeah, just real quick prompt layer where building dev tools for people bringing LLMs into production like GPT.", "tokens": [50864, 400, 1338, 11, 445, 957, 1702, 12391, 4583, 689, 2390, 1905, 3873, 337, 561, 5062, 441, 43, 26386, 666, 4265, 411, 26039, 51, 13, 51264], "temperature": 0.0, "avg_logprob": -0.169709726541984, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.026714302599430084}, {"id": 2, "seek": 0, "start": 18.0, "end": 25.0, "text": " So if you want to do some data driven prompt engineering, we're the place for that and we're going to have we're going to have a pretty cool conversation.", "tokens": [51264, 407, 498, 291, 528, 281, 360, 512, 1412, 9555, 12391, 7043, 11, 321, 434, 264, 1081, 337, 300, 293, 321, 434, 516, 281, 362, 321, 434, 516, 281, 362, 257, 1238, 1627, 3761, 13, 51614], "temperature": 0.0, "avg_logprob": -0.169709726541984, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.026714302599430084}, {"id": 3, "seek": 0, "start": 25.0, "end": 29.0, "text": " I hope we're going to nerd out a little bit here and excited for that.", "tokens": [51614, 286, 1454, 321, 434, 516, 281, 23229, 484, 257, 707, 857, 510, 293, 2919, 337, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.169709726541984, "compression_ratio": 1.79136690647482, "no_speech_prob": 0.026714302599430084}, {"id": 4, "seek": 2900, "start": 29.0, "end": 32.0, "text": " So you probably know who Steven Wolfram is what he's done.", "tokens": [50364, 407, 291, 1391, 458, 567, 12754, 16634, 2356, 307, 437, 415, 311, 1096, 13, 50514], "temperature": 0.0, "avg_logprob": -0.14891718834945836, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.013410530053079128}, {"id": 5, "seek": 2900, "start": 32.0, "end": 48.0, "text": " I learned about his work from doing math homework properly in middle school or high school but originally originally met Steven when he came to a hackathon I was running in high school and he stayed there till like 2am mentoring people and that's kind of my first", "tokens": [50514, 286, 3264, 466, 702, 589, 490, 884, 5221, 14578, 6108, 294, 2808, 1395, 420, 1090, 1395, 457, 7993, 7993, 1131, 12754, 562, 415, 1361, 281, 257, 10339, 18660, 286, 390, 2614, 294, 1090, 1395, 293, 415, 9181, 456, 4288, 411, 568, 335, 30257, 561, 293, 300, 311, 733, 295, 452, 700, 51314], "temperature": 0.0, "avg_logprob": -0.14891718834945836, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.013410530053079128}, {"id": 6, "seek": 2900, "start": 48.0, "end": 55.0, "text": " introduction to this theory about computational thinking and it's come back around, you know, and it's very important today.", "tokens": [51314, 9339, 281, 341, 5261, 466, 28270, 1953, 293, 309, 311, 808, 646, 926, 11, 291, 458, 11, 293, 309, 311, 588, 1021, 965, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14891718834945836, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.013410530053079128}, {"id": 7, "seek": 5500, "start": 55.0, "end": 65.0, "text": " So, yeah, really excited to have this conversation with you and maybe to kick it off.", "tokens": [50364, 407, 11, 1338, 11, 534, 2919, 281, 362, 341, 3761, 365, 291, 293, 1310, 281, 4437, 309, 766, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15241406180641867, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.1327991932630539}, {"id": 8, "seek": 5500, "start": 65.0, "end": 82.0, "text": " You had a big and a big release today of kind of plugins into chat GPT and this is, I think you might have said it but I completely agree it's kind of a historic moment where we're kind of bringing the LLM side of things that AI statistical AI to the symbolic AI to the symbolic", "tokens": [50864, 509, 632, 257, 955, 293, 257, 955, 4374, 965, 295, 733, 295, 33759, 666, 5081, 26039, 51, 293, 341, 307, 11, 286, 519, 291, 1062, 362, 848, 309, 457, 286, 2584, 3986, 309, 311, 733, 295, 257, 13236, 1623, 689, 321, 434, 733, 295, 5062, 264, 441, 43, 44, 1252, 295, 721, 300, 7318, 22820, 7318, 281, 264, 25755, 7318, 281, 264, 25755, 51714], "temperature": 0.0, "avg_logprob": -0.15241406180641867, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.1327991932630539}, {"id": 9, "seek": 8200, "start": 82.0, "end": 91.0, "text": " interpretation and that sort of thing so like, how did we get here what what has the process been and what why is it so significant right now.", "tokens": [50364, 14174, 293, 300, 1333, 295, 551, 370, 411, 11, 577, 630, 321, 483, 510, 437, 437, 575, 264, 1399, 668, 293, 437, 983, 307, 309, 370, 4776, 558, 586, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12468763987223307, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.23508015275001526}, {"id": 10, "seek": 8200, "start": 91.0, "end": 106.0, "text": " Well I think the what LLMs have been able to do is sort of take the corpus of texts that we humans have written and kind of grind it up to the point where we can make more that's like it so to speak so we give a prompt.", "tokens": [50814, 1042, 286, 519, 264, 437, 441, 43, 26386, 362, 668, 1075, 281, 360, 307, 1333, 295, 747, 264, 1181, 31624, 295, 15765, 300, 321, 6255, 362, 3720, 293, 733, 295, 16700, 309, 493, 281, 264, 935, 689, 321, 393, 652, 544, 300, 311, 411, 309, 370, 281, 1710, 370, 321, 976, 257, 12391, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12468763987223307, "compression_ratio": 1.6837209302325582, "no_speech_prob": 0.23508015275001526}, {"id": 11, "seek": 10600, "start": 106.0, "end": 119.0, "text": " And you know the the role of an LLM is to continue from that prompt and that's and sort of continue with things that are like the things that have already been written on the web.", "tokens": [50364, 400, 291, 458, 264, 264, 3090, 295, 364, 441, 43, 44, 307, 281, 2354, 490, 300, 12391, 293, 300, 311, 293, 1333, 295, 2354, 365, 721, 300, 366, 411, 264, 721, 300, 362, 1217, 668, 3720, 322, 264, 3670, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1124806196793266, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.6703752875328064}, {"id": 12, "seek": 10600, "start": 119.0, "end": 125.0, "text": " And I think, and it does that in a way that is kind of just sort of statistically fitting things together.", "tokens": [51014, 400, 286, 519, 11, 293, 309, 775, 300, 294, 257, 636, 300, 307, 733, 295, 445, 1333, 295, 36478, 15669, 721, 1214, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1124806196793266, "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.6703752875328064}, {"id": 13, "seek": 12500, "start": 125.0, "end": 137.0, "text": " It's a very different tradition of computation that is really able to use the deeper aspects of computation really able to use sort of the irreducible computations that can in principle be done.", "tokens": [50364, 467, 311, 257, 588, 819, 6994, 295, 24903, 300, 307, 534, 1075, 281, 764, 264, 7731, 7270, 295, 24903, 534, 1075, 281, 764, 1333, 295, 264, 16014, 769, 32128, 2807, 763, 300, 393, 294, 8665, 312, 1096, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09891705513000489, "compression_ratio": 1.7766990291262137, "no_speech_prob": 0.9397467374801636}, {"id": 14, "seek": 12500, "start": 137.0, "end": 149.0, "text": " That's the world that I've lived in for for many decades now of trying to figure out sort of how to take what is computationally possible and make it accessible to humans.", "tokens": [50964, 663, 311, 264, 1002, 300, 286, 600, 5152, 294, 337, 337, 867, 7878, 586, 295, 1382, 281, 2573, 484, 1333, 295, 577, 281, 747, 437, 307, 24903, 379, 1944, 293, 652, 309, 9515, 281, 6255, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09891705513000489, "compression_ratio": 1.7766990291262137, "no_speech_prob": 0.9397467374801636}, {"id": 15, "seek": 14900, "start": 149.0, "end": 164.0, "text": " So the thing that's been sort of exciting is that well we made a lot of effort to make it accessible to humans turns out that also makes it accessible to language based ai's that kind of lent their craft from from humans so to speak.", "tokens": [50364, 407, 264, 551, 300, 311, 668, 1333, 295, 4670, 307, 300, 731, 321, 1027, 257, 688, 295, 4630, 281, 652, 309, 9515, 281, 6255, 4523, 484, 300, 611, 1669, 309, 9515, 281, 2856, 2361, 9783, 311, 300, 733, 295, 23556, 641, 8448, 490, 490, 6255, 370, 281, 1710, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1512695888303361, "compression_ratio": 1.6293706293706294, "no_speech_prob": 0.2596241533756256}, {"id": 16, "seek": 16400, "start": 164.0, "end": 181.0, "text": " So kind of the way I see it is these have been the two great kind of traditions of AI, and we've now got this opportunity to really connect them together through the medium of sort of a mixture of natural language and computational language, and be able to get it", "tokens": [50364, 407, 733, 295, 264, 636, 286, 536, 309, 307, 613, 362, 668, 264, 732, 869, 733, 295, 15643, 295, 7318, 11, 293, 321, 600, 586, 658, 341, 2650, 281, 534, 1745, 552, 1214, 807, 264, 6399, 295, 1333, 295, 257, 9925, 295, 3303, 2856, 293, 28270, 2856, 11, 293, 312, 1075, 281, 483, 309, 51214], "temperature": 0.0, "avg_logprob": -0.128364661644245, "compression_ratio": 1.5562130177514792, "no_speech_prob": 0.7519895434379578}, {"id": 17, "seek": 18100, "start": 182.0, "end": 198.0, "text": " that one can kind of use the linguistic interface that we are all used to because we're all very experienced at sort of interacting in natural language, together with kind of the depth of that rather in human non human thing that is sort of powerful", "tokens": [50414, 300, 472, 393, 733, 295, 764, 264, 43002, 9226, 300, 321, 366, 439, 1143, 281, 570, 321, 434, 439, 588, 6751, 412, 1333, 295, 18017, 294, 3303, 2856, 11, 1214, 365, 733, 295, 264, 7161, 295, 300, 2831, 294, 1952, 2107, 1952, 551, 300, 307, 1333, 295, 4005, 51214], "temperature": 0.0, "avg_logprob": -0.15091561354123628, "compression_ratio": 1.6168831168831168, "no_speech_prob": 0.586210310459137}, {"id": 18, "seek": 19800, "start": 198.0, "end": 213.0, "text": " for computation. So it's kind of bringing those two things together the, the very human language with the very sort of beyond human non human power of kind of deep computation, and you know the fact is that, as a practical matter, LLMs are making", "tokens": [50364, 337, 24903, 13, 407, 309, 311, 733, 295, 5062, 729, 732, 721, 1214, 264, 11, 264, 588, 1952, 2856, 365, 264, 588, 1333, 295, 4399, 1952, 2107, 1952, 1347, 295, 733, 295, 2452, 24903, 11, 293, 291, 458, 264, 1186, 307, 300, 11, 382, 257, 8496, 1871, 11, 441, 43, 26386, 366, 1455, 51114], "temperature": 0.0, "avg_logprob": -0.2065220832824707, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.702519416809082}, {"id": 19, "seek": 19800, "start": 213.0, "end": 219.0, "text": " sort of statistically reasonable pieces of text, which may or may not actually be the way the world is.", "tokens": [51114, 1333, 295, 36478, 10585, 3755, 295, 2487, 11, 597, 815, 420, 815, 406, 767, 312, 264, 636, 264, 1002, 307, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2065220832824707, "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.702519416809082}, {"id": 20, "seek": 21900, "start": 219.0, "end": 236.0, "text": " And what we've been involved in doing is kind of making a representation of the world as the world is in computable form, and being able to bring those two things together so that we're able to let the LLM sort of make it statistical kind of text, and then we're able to provide", "tokens": [50364, 400, 437, 321, 600, 668, 3288, 294, 884, 307, 733, 295, 1455, 257, 10290, 295, 264, 1002, 382, 264, 1002, 307, 294, 2807, 712, 1254, 11, 293, 885, 1075, 281, 1565, 729, 732, 721, 1214, 370, 300, 321, 434, 1075, 281, 718, 264, 441, 43, 44, 1333, 295, 652, 309, 22820, 733, 295, 2487, 11, 293, 550, 321, 434, 1075, 281, 2893, 51214], "temperature": 0.0, "avg_logprob": -0.08195626374446985, "compression_ratio": 1.6547619047619047, "no_speech_prob": 0.6452837586402893}, {"id": 21, "seek": 23600, "start": 236.0, "end": 249.0, "text": " sort of precise computable insertions into that text to kind of provide the facts, where the LLM can be writing fact or fiction, and it doesn't really know the difference so to speak.", "tokens": [50364, 1333, 295, 13600, 2807, 712, 8969, 626, 666, 300, 2487, 281, 733, 295, 2893, 264, 9130, 11, 689, 264, 441, 43, 44, 393, 312, 3579, 1186, 420, 13266, 11, 293, 309, 1177, 380, 534, 458, 264, 2649, 370, 281, 1710, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12005825257033445, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.31139305233955383}, {"id": 22, "seek": 23600, "start": 249.0, "end": 259.0, "text": " Right, right. No, this is super interesting and like, and I guess it's, and it's changed so much in the past like two or three months. I guess like to dive deeper here.", "tokens": [51014, 1779, 11, 558, 13, 883, 11, 341, 307, 1687, 1880, 293, 411, 11, 293, 286, 2041, 309, 311, 11, 293, 309, 311, 3105, 370, 709, 294, 264, 1791, 411, 732, 420, 1045, 2493, 13, 286, 2041, 411, 281, 9192, 7731, 510, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12005825257033445, "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.31139305233955383}, {"id": 23, "seek": 25900, "start": 260.0, "end": 272.0, "text": " Maybe, I'll give you an example so, and I'd love for you to explain kind of your thinking behind this, and this is changed today with the welfare and plug in and chat GPT obviously but I had a friend Avi and he tweet.", "tokens": [50414, 2704, 11, 286, 603, 976, 291, 364, 1365, 370, 11, 293, 286, 1116, 959, 337, 291, 281, 2903, 733, 295, 428, 1953, 2261, 341, 11, 293, 341, 307, 3105, 965, 365, 264, 17788, 293, 5452, 294, 293, 5081, 26039, 51, 2745, 457, 286, 632, 257, 1277, 40712, 293, 415, 15258, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17575950296516094, "compression_ratio": 1.7024221453287198, "no_speech_prob": 0.33396294713020325}, {"id": 24, "seek": 25900, "start": 272.0, "end": 288.0, "text": " Let me let me just read the actual tweet so he was basically he tweeted that he was using GPT 3.5 then he was using GPT for and it was failing on this very simple problem that he was doing and the prompt was how many unique apartment units are represented in the data below.", "tokens": [51014, 961, 385, 718, 385, 445, 1401, 264, 3539, 15258, 370, 415, 390, 1936, 415, 25646, 300, 415, 390, 1228, 26039, 51, 805, 13, 20, 550, 415, 390, 1228, 26039, 51, 337, 293, 309, 390, 18223, 322, 341, 588, 2199, 1154, 300, 415, 390, 884, 293, 264, 12391, 390, 577, 867, 3845, 9587, 6815, 366, 10379, 294, 264, 1412, 2507, 13, 51814], "temperature": 0.0, "avg_logprob": -0.17575950296516094, "compression_ratio": 1.7024221453287198, "no_speech_prob": 0.33396294713020325}, {"id": 25, "seek": 28800, "start": 288.0, "end": 308.0, "text": " As in two people in the same unit, for example, 430 would count as one and then you had a bunch of names with different apartment numbers, and his question was to him, like, this is not obviously a hard problem in the let's take a step back from what's symbolic what's statistical,", "tokens": [50364, 1018, 294, 732, 561, 294, 264, 912, 4985, 11, 337, 1365, 11, 1017, 3446, 576, 1207, 382, 472, 293, 550, 291, 632, 257, 3840, 295, 5288, 365, 819, 9587, 3547, 11, 293, 702, 1168, 390, 281, 796, 11, 411, 11, 341, 307, 406, 2745, 257, 1152, 1154, 294, 264, 718, 311, 747, 257, 1823, 646, 490, 437, 311, 25755, 437, 311, 22820, 11, 51364], "temperature": 0.0, "avg_logprob": -0.19382445491961578, "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.01168208010494709}, {"id": 26, "seek": 30800, "start": 309.0, "end": 321.0, "text": " just at the space level if I'm a hacker on just like using open AI, this doesn't seem like a hard problem but it wasn't a problem GPT is able to solve on its own it's not a problem LLMs are able to solve.", "tokens": [50414, 445, 412, 264, 1901, 1496, 498, 286, 478, 257, 38155, 322, 445, 411, 1228, 1269, 7318, 11, 341, 1177, 380, 1643, 411, 257, 1152, 1154, 457, 309, 2067, 380, 257, 1154, 26039, 51, 307, 1075, 281, 5039, 322, 1080, 1065, 309, 311, 406, 257, 1154, 441, 43, 26386, 366, 1075, 281, 5039, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1861227135909231, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.29522112011909485}, {"id": 27, "seek": 30800, "start": 321.0, "end": 330.0, "text": " Would would love to hear you kind of break that down and explain what about this problem is hard for LLMs and what about it it's easy for the warframe language.", "tokens": [51014, 6068, 576, 959, 281, 1568, 291, 733, 295, 1821, 300, 760, 293, 2903, 437, 466, 341, 1154, 307, 1152, 337, 441, 43, 26386, 293, 437, 466, 309, 309, 311, 1858, 337, 264, 1516, 17265, 2856, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1861227135909231, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.29522112011909485}, {"id": 28, "seek": 33000, "start": 330.0, "end": 335.0, "text": " So, anything that involves precisely computing something with math.", "tokens": [50364, 407, 11, 1340, 300, 11626, 13402, 15866, 746, 365, 5221, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14627628682929777, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.7523165345191956}, {"id": 29, "seek": 33000, "start": 335.0, "end": 349.0, "text": " There can be many steps that are involved in doing a math computation and LLM is a feed forward network that you know the current version of them at least is sort of feed forward networks where you kind of, you know, it's encoded a lot of stuff.", "tokens": [50614, 821, 393, 312, 867, 4439, 300, 366, 3288, 294, 884, 257, 5221, 24903, 293, 441, 43, 44, 307, 257, 3154, 2128, 3209, 300, 291, 458, 264, 2190, 3037, 295, 552, 412, 1935, 307, 1333, 295, 3154, 2128, 9590, 689, 291, 733, 295, 11, 291, 458, 11, 309, 311, 2058, 12340, 257, 688, 295, 1507, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14627628682929777, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.7523165345191956}, {"id": 30, "seek": 33000, "start": 349.0, "end": 358.0, "text": " But basically you fed it the the the text so far, and it sort of ripples through the network and then tells you the probabilities of next words to follow.", "tokens": [51314, 583, 1936, 291, 4636, 309, 264, 264, 264, 2487, 370, 1400, 11, 293, 309, 1333, 295, 367, 37674, 807, 264, 3209, 293, 550, 5112, 291, 264, 33783, 295, 958, 2283, 281, 1524, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14627628682929777, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.7523165345191956}, {"id": 31, "seek": 35800, "start": 358.0, "end": 366.0, "text": " And that's just not something where if they're needed to be something where iterated recursed whatever else to figure out what's the answer to this thing.", "tokens": [50364, 400, 300, 311, 445, 406, 746, 689, 498, 436, 434, 2978, 281, 312, 746, 689, 17138, 770, 20560, 292, 2035, 1646, 281, 2573, 484, 437, 311, 264, 1867, 281, 341, 551, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10591936111450195, "compression_ratio": 1.7946428571428572, "no_speech_prob": 0.07571914047002792}, {"id": 32, "seek": 35800, "start": 366.0, "end": 380.0, "text": " That's not something that's going to happen inside the current architecture of LLM that the way that LLMs managed to not be sort of computationally trivial is that they are continually eating their own tails so to speak they're continually kind of", "tokens": [50764, 663, 311, 406, 746, 300, 311, 516, 281, 1051, 1854, 264, 2190, 9482, 295, 441, 43, 44, 300, 264, 636, 300, 441, 43, 26386, 6453, 281, 406, 312, 1333, 295, 24903, 379, 26703, 307, 300, 436, 366, 22277, 3936, 641, 1065, 28537, 370, 281, 1710, 436, 434, 22277, 733, 295, 51464], "temperature": 0.0, "avg_logprob": -0.10591936111450195, "compression_ratio": 1.7946428571428572, "no_speech_prob": 0.07571914047002792}, {"id": 33, "seek": 38000, "start": 380.0, "end": 394.0, "text": " processing the things that they have generated so far and that's what allows them that that little sort of extra piece is what allows them to have a lot more richness than just a pure, you know ripple through the network once type of type of system.", "tokens": [50364, 9007, 264, 721, 300, 436, 362, 10833, 370, 1400, 293, 300, 311, 437, 4045, 552, 300, 300, 707, 1333, 295, 2857, 2522, 307, 437, 4045, 552, 281, 362, 257, 688, 544, 44506, 813, 445, 257, 6075, 11, 291, 458, 40688, 807, 264, 3209, 1564, 2010, 295, 2010, 295, 1185, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11472826533847386, "compression_ratio": 1.6064516129032258, "no_speech_prob": 0.7674190998077393}, {"id": 34, "seek": 39400, "start": 394.0, "end": 410.0, "text": " But still the kinds of things that are really easy in terms of traditional kind of Turing machine computation are are really hard. They're really not possible actually for something which is fundamentally doing this you know ripple through once type type method.", "tokens": [50364, 583, 920, 264, 3685, 295, 721, 300, 366, 534, 1858, 294, 2115, 295, 5164, 733, 295, 314, 1345, 3479, 24903, 366, 366, 534, 1152, 13, 814, 434, 534, 406, 1944, 767, 337, 746, 597, 307, 17879, 884, 341, 291, 458, 40688, 807, 1564, 2010, 2010, 3170, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13727415309232824, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.7672789096832275}, {"id": 35, "seek": 41000, "start": 410.0, "end": 428.0, "text": " I think that the challenge, I mean, in terms of sort of the the architecture for AI is using tools is how does the AI know what tool to use what what when to use the tool, how to present information to the tool to be used, and then how to interpret the information that comes back.", "tokens": [50364, 286, 519, 300, 264, 3430, 11, 286, 914, 11, 294, 2115, 295, 1333, 295, 264, 264, 9482, 337, 7318, 307, 1228, 3873, 307, 577, 775, 264, 7318, 458, 437, 2290, 281, 764, 437, 437, 562, 281, 764, 264, 2290, 11, 577, 281, 1974, 1589, 281, 264, 2290, 281, 312, 1143, 11, 293, 550, 577, 281, 7302, 264, 1589, 300, 1487, 646, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1275642423918753, "compression_ratio": 1.7134146341463414, "no_speech_prob": 0.43179386854171753}, {"id": 36, "seek": 42800, "start": 428.0, "end": 439.0, "text": " We've been kind of fortunate in this case because we built Wolf from Alpha to take natural language input from humans and turns out LLMs can also produce natural language input.", "tokens": [50364, 492, 600, 668, 733, 295, 14096, 294, 341, 1389, 570, 321, 3094, 16634, 490, 20588, 281, 747, 3303, 2856, 4846, 490, 6255, 293, 4523, 484, 441, 43, 26386, 393, 611, 5258, 3303, 2856, 4846, 13, 50914], "temperature": 0.0, "avg_logprob": -0.154083985548753, "compression_ratio": 1.4390243902439024, "no_speech_prob": 0.5845161080360413}, {"id": 37, "seek": 43900, "start": 439.0, "end": 455.0, "text": " And so, you know, we already have a funnel basically to collect the things that LLMs naturally produce now in fact the endpoint that is in the Wolf and plug in for chat GPT is a combination Wolf from Alpha Wolf and language endpoints.", "tokens": [50364, 400, 370, 11, 291, 458, 11, 321, 1217, 362, 257, 24515, 1936, 281, 2500, 264, 721, 300, 441, 43, 26386, 8195, 5258, 586, 294, 1186, 264, 35795, 300, 307, 294, 264, 16634, 293, 5452, 294, 337, 5081, 26039, 51, 307, 257, 6562, 16634, 490, 20588, 16634, 293, 2856, 917, 20552, 13, 51164], "temperature": 0.0, "avg_logprob": -0.212654616615989, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.4534023404121399}, {"id": 38, "seek": 45500, "start": 455.0, "end": 471.0, "text": " And that's an elaborate piece of engineering because the LLM kind of and you know we we try to make the prompt, I don't know how successful we've been so far, we try to make the prompt decide, should it try and send it to Wolf mouth or when because it has a natural language input, and it's expecting", "tokens": [50364, 400, 300, 311, 364, 20945, 2522, 295, 7043, 570, 264, 441, 43, 44, 733, 295, 293, 291, 458, 321, 321, 853, 281, 652, 264, 12391, 11, 286, 500, 380, 458, 577, 4406, 321, 600, 668, 370, 1400, 11, 321, 853, 281, 652, 264, 12391, 4536, 11, 820, 309, 853, 293, 2845, 309, 281, 16634, 4525, 420, 562, 570, 309, 575, 257, 3303, 2856, 4846, 11, 293, 309, 311, 9650, 51164], "temperature": 0.0, "avg_logprob": -0.1649248044784755, "compression_ratio": 1.5625, "no_speech_prob": 0.5710528492927551}, {"id": 39, "seek": 47100, "start": 471.0, "end": 479.0, "text": " natural language collection of outputs, or should we try and send it to Wolf from language where it has to synthesize this precise computational language.", "tokens": [50364, 3303, 2856, 5765, 295, 23930, 11, 420, 820, 321, 853, 293, 2845, 309, 281, 16634, 490, 2856, 689, 309, 575, 281, 26617, 1125, 341, 13600, 28270, 2856, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1642491658528646, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.7185889482498169}, {"id": 40, "seek": 47100, "start": 479.0, "end": 492.0, "text": " And then when it sends it to often language is it actually correct Wolf from language, it has to go, it basically goes and reads the documentation that's what we've told it to do at least to figure out, is it a correctly formed piece of Wolf from language input.", "tokens": [50764, 400, 550, 562, 309, 14790, 309, 281, 2049, 2856, 307, 309, 767, 3006, 16634, 490, 2856, 11, 309, 575, 281, 352, 11, 309, 1936, 1709, 293, 15700, 264, 14333, 300, 311, 437, 321, 600, 1907, 309, 281, 360, 412, 1935, 281, 2573, 484, 11, 307, 309, 257, 8944, 8693, 2522, 295, 16634, 490, 2856, 4846, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1642491658528646, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.7185889482498169}, {"id": 41, "seek": 49200, "start": 492.0, "end": 507.0, "text": " And then when it gets the result, it's a very, you know, here's the result that's all there is, and Wolf mouth, the result can be this long series of, of, of things which which chat GPT can then knit into an essay and it does pretty good job of doing that.", "tokens": [50364, 400, 550, 562, 309, 2170, 264, 1874, 11, 309, 311, 257, 588, 11, 291, 458, 11, 510, 311, 264, 1874, 300, 311, 439, 456, 307, 11, 293, 16634, 4525, 11, 264, 1874, 393, 312, 341, 938, 2638, 295, 11, 295, 11, 295, 721, 597, 597, 5081, 26039, 51, 393, 550, 15594, 666, 364, 16238, 293, 309, 775, 1238, 665, 1691, 295, 884, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1844330675461713, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.5490564107894897}, {"id": 42, "seek": 50700, "start": 507.0, "end": 518.0, "text": " I think that, so I mean that there these different, different modalities but the fundamental point is, when it's a computation that involves kind of looping and recursing and so on.", "tokens": [50364, 286, 519, 300, 11, 370, 286, 914, 300, 456, 613, 819, 11, 819, 1072, 16110, 457, 264, 8088, 935, 307, 11, 562, 309, 311, 257, 24903, 300, 11626, 733, 295, 6367, 278, 293, 20560, 278, 293, 370, 322, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11824032866839067, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.63811856508255}, {"id": 43, "seek": 50700, "start": 518.0, "end": 534.0, "text": " It's just not something that you can expect to be able to do from from from a straight LLM now there are other issues like, like things to do with tokens, and the fact that tokens and numbers don't match very easily, although that's something one could definitely fix.", "tokens": [50914, 467, 311, 445, 406, 746, 300, 291, 393, 2066, 281, 312, 1075, 281, 360, 490, 490, 490, 257, 2997, 441, 43, 44, 586, 456, 366, 661, 2663, 411, 11, 411, 721, 281, 360, 365, 22667, 11, 293, 264, 1186, 300, 22667, 293, 3547, 500, 380, 2995, 588, 3612, 11, 4878, 300, 311, 746, 472, 727, 2138, 3191, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11824032866839067, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.63811856508255}, {"id": 44, "seek": 53400, "start": 534.0, "end": 547.0, "text": " But, you know, and, and, you know, if you're doing even something like multiplication, if you're just doing it in pure LLM land, what are you going to do memorize all of the, you know, two digit three digit multiplications.", "tokens": [50364, 583, 11, 291, 458, 11, 293, 11, 293, 11, 291, 458, 11, 498, 291, 434, 884, 754, 746, 411, 27290, 11, 498, 291, 434, 445, 884, 309, 294, 6075, 441, 43, 44, 2117, 11, 437, 366, 291, 516, 281, 360, 27478, 439, 295, 264, 11, 291, 458, 11, 732, 14293, 1045, 14293, 17596, 763, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10160754975818452, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.15688034892082214}, {"id": 45, "seek": 53400, "start": 547.0, "end": 557.0, "text": " Well, then you have the next case, and it may or may not be true that from the innards of the LLM that it can untangle kind of doing the carries and all this kind of thing.", "tokens": [51014, 1042, 11, 550, 291, 362, 264, 958, 1389, 11, 293, 309, 815, 420, 815, 406, 312, 2074, 300, 490, 264, 7714, 2287, 295, 264, 441, 43, 44, 300, 309, 393, 1701, 7846, 733, 295, 884, 264, 16402, 293, 439, 341, 733, 295, 551, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10160754975818452, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.15688034892082214}, {"id": 46, "seek": 55700, "start": 557.0, "end": 573.0, "text": " Probably it can untangle that if you allow it to, you know, talk about its intermediate steps, it has an easier time, if it can talk about its intermediate steps because essentially it's storing intermediate it's got an intermediate storage location, so to speak that is the actual text it's", "tokens": [50364, 9210, 309, 393, 1701, 7846, 300, 498, 291, 2089, 309, 281, 11, 291, 458, 11, 751, 466, 1080, 19376, 4439, 11, 309, 575, 364, 3571, 565, 11, 498, 309, 393, 751, 466, 1080, 19376, 4439, 570, 4476, 309, 311, 26085, 19376, 309, 311, 658, 364, 19376, 6725, 4914, 11, 370, 281, 1710, 300, 307, 264, 3539, 2487, 309, 311, 51164], "temperature": 0.0, "avg_logprob": -0.14683084639291916, "compression_ratio": 1.774390243902439, "no_speech_prob": 0.40616896748542786}, {"id": 47, "seek": 57300, "start": 573.0, "end": 590.0, "text": " not doing. But it's not a very, you know, the, the, the best way to do that is just you just compute, and you do sort of traditional computation on that to get that result and then you knit it back into the kind of natural language world of the LLM.", "tokens": [50364, 406, 884, 13, 583, 309, 311, 406, 257, 588, 11, 291, 458, 11, 264, 11, 264, 11, 264, 1151, 636, 281, 360, 300, 307, 445, 291, 445, 14722, 11, 293, 291, 360, 1333, 295, 5164, 24903, 322, 300, 281, 483, 300, 1874, 293, 550, 291, 15594, 309, 646, 666, 264, 733, 295, 3303, 2856, 1002, 295, 264, 441, 43, 44, 13, 51214], "temperature": 0.0, "avg_logprob": -0.163687984759991, "compression_ratio": 1.546583850931677, "no_speech_prob": 0.4242700934410095}, {"id": 48, "seek": 59000, "start": 590.0, "end": 612.0, "text": " Interesting. So, what, why is it that having a kind of a, but asking the LLM to explain the steps makes it a little more correct. Is it is it not just finding like, I guess the completion that's most close to all the steps there's something I guess that changes the problem by breaking it down there or I don't know if you have any insight.", "tokens": [50364, 14711, 13, 407, 11, 437, 11, 983, 307, 309, 300, 1419, 257, 733, 295, 257, 11, 457, 3365, 264, 441, 43, 44, 281, 2903, 264, 4439, 1669, 309, 257, 707, 544, 3006, 13, 1119, 309, 307, 309, 406, 445, 5006, 411, 11, 286, 2041, 264, 19372, 300, 311, 881, 1998, 281, 439, 264, 4439, 456, 311, 746, 286, 2041, 300, 2962, 264, 1154, 538, 7697, 309, 760, 456, 420, 286, 500, 380, 458, 498, 291, 362, 604, 11269, 13, 51464], "temperature": 0.0, "avg_logprob": -0.20077436516083866, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.40473222732543945}, {"id": 49, "seek": 61200, "start": 613.0, "end": 630.0, "text": " The problem is that if you've got something that involves intermediate steps, the LLM cannot internally, when we write a program in traditional, I don't know, orphan language or something, it's trivial for it to go and, you know, iterate through many, many times.", "tokens": [50414, 440, 1154, 307, 300, 498, 291, 600, 658, 746, 300, 11626, 19376, 4439, 11, 264, 441, 43, 44, 2644, 19501, 11, 562, 321, 2464, 257, 1461, 294, 5164, 11, 286, 500, 380, 458, 11, 28711, 2856, 420, 746, 11, 309, 311, 26703, 337, 309, 281, 352, 293, 11, 291, 458, 11, 44497, 807, 867, 11, 867, 1413, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16220052191551695, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.5038177371025085}, {"id": 50, "seek": 61200, "start": 630.0, "end": 635.0, "text": " You know, it's trying to find the answer to this thing, it's going to run it a million times internally, it's all good.", "tokens": [51264, 509, 458, 11, 309, 311, 1382, 281, 915, 264, 1867, 281, 341, 551, 11, 309, 311, 516, 281, 1190, 309, 257, 2459, 1413, 19501, 11, 309, 311, 439, 665, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16220052191551695, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.5038177371025085}, {"id": 51, "seek": 63500, "start": 635.0, "end": 643.0, "text": " But when, if it's an LLM, it doesn't get to do that. Every, every token that it is emitting, it's just rippling through once.", "tokens": [50364, 583, 562, 11, 498, 309, 311, 364, 441, 43, 44, 11, 309, 1177, 380, 483, 281, 360, 300, 13, 2048, 11, 633, 14862, 300, 309, 307, 846, 2414, 11, 309, 311, 445, 367, 2488, 1688, 807, 1564, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09338864614797193, "compression_ratio": 1.5812807881773399, "no_speech_prob": 0.4060225188732147}, {"id": 52, "seek": 63500, "start": 643.0, "end": 652.0, "text": " Right. And so in order for it to have this sort of intermediate state where it kind of iterates on it, it's got to actually write out that intermediate state in the text stream that it generates.", "tokens": [50764, 1779, 13, 400, 370, 294, 1668, 337, 309, 281, 362, 341, 1333, 295, 19376, 1785, 689, 309, 733, 295, 17138, 1024, 322, 309, 11, 309, 311, 658, 281, 767, 2464, 484, 300, 19376, 1785, 294, 264, 2487, 4309, 300, 309, 23815, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09338864614797193, "compression_ratio": 1.5812807881773399, "no_speech_prob": 0.4060225188732147}, {"id": 53, "seek": 65200, "start": 652.0, "end": 664.0, "text": " And so that's why it can be better if you ask it to explain itself, so to speak, because then it has the each individual kind of ripple through each individual new token is less has to happen.", "tokens": [50364, 400, 370, 300, 311, 983, 309, 393, 312, 1101, 498, 291, 1029, 309, 281, 2903, 2564, 11, 370, 281, 1710, 11, 570, 550, 309, 575, 264, 1184, 2609, 733, 295, 40688, 807, 1184, 2609, 777, 14862, 307, 1570, 575, 281, 1051, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07682358188393676, "compression_ratio": 1.7277227722772277, "no_speech_prob": 0.2001858651638031}, {"id": 54, "seek": 65200, "start": 664.0, "end": 674.0, "text": " And it's kind of doing that sort of building up of the tower through the explicit text that it's generating and then and then redoing things from that text.", "tokens": [50964, 400, 309, 311, 733, 295, 884, 300, 1333, 295, 2390, 493, 295, 264, 10567, 807, 264, 13691, 2487, 300, 309, 311, 17746, 293, 550, 293, 550, 29956, 278, 721, 490, 300, 2487, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07682358188393676, "compression_ratio": 1.7277227722772277, "no_speech_prob": 0.2001858651638031}, {"id": 55, "seek": 67400, "start": 674.0, "end": 694.0, "text": " Interesting, interesting. Yeah, that makes sense. And I guess like, one thing I'm curious about, I guess, your thoughts on and is this the, and I guess what the steady state is is what it like the interfaces here it sounds like this was kind of, this is like a two month thing that went from no", "tokens": [50364, 14711, 11, 1880, 13, 865, 11, 300, 1669, 2020, 13, 400, 286, 2041, 411, 11, 472, 551, 286, 478, 6369, 466, 11, 286, 2041, 11, 428, 4598, 322, 293, 307, 341, 264, 11, 293, 286, 2041, 437, 264, 13211, 1785, 307, 307, 437, 309, 411, 264, 28416, 510, 309, 3263, 411, 341, 390, 733, 295, 11, 341, 307, 411, 257, 732, 1618, 551, 300, 1437, 490, 572, 51364], "temperature": 0.0, "avg_logprob": -0.21582208552830656, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.2533170282840729}, {"id": 56, "seek": 69400, "start": 694.0, "end": 714.0, "text": " to GPT with plugins and stuff like that or chat with plugins and the Wolfram plugin and you mentioned there's the Wolfram language input and the Wolfram alpha input. Is that do you see there always be like, is it like Wolfram alpha as the fallback or how do you see that evolving over time or is this like, we'll see who knows.", "tokens": [50364, 281, 26039, 51, 365, 33759, 293, 1507, 411, 300, 420, 5081, 365, 33759, 293, 264, 16634, 2356, 23407, 293, 291, 2835, 456, 311, 264, 16634, 2356, 2856, 4846, 293, 264, 16634, 2356, 8961, 4846, 13, 1119, 300, 360, 291, 536, 456, 1009, 312, 411, 11, 307, 309, 411, 16634, 2356, 8961, 382, 264, 2100, 3207, 420, 577, 360, 291, 536, 300, 21085, 670, 565, 420, 307, 341, 411, 11, 321, 603, 536, 567, 3255, 13, 51364], "temperature": 0.0, "avg_logprob": -0.20720104001603037, "compression_ratio": 1.9869565217391305, "no_speech_prob": 0.4663882553577423}, {"id": 57, "seek": 69400, "start": 714.0, "end": 723.0, "text": " Well, I mean, the combination the combining of the Wolfram alpha endpoint and the Wolfram language endpoint started last weekend.", "tokens": [51364, 1042, 11, 286, 914, 11, 264, 6562, 264, 21928, 295, 264, 16634, 2356, 8961, 35795, 293, 264, 16634, 2356, 2856, 35795, 1409, 1036, 6711, 13, 51814], "temperature": 0.0, "avg_logprob": -0.20720104001603037, "compression_ratio": 1.9869565217391305, "no_speech_prob": 0.4663882553577423}, {"id": 58, "seek": 72300, "start": 723.0, "end": 728.0, "text": " I love it. It's, it's a very non trivial piece of.", "tokens": [50364, 286, 959, 309, 13, 467, 311, 11, 309, 311, 257, 588, 2107, 26703, 2522, 295, 13, 50614], "temperature": 0.0, "avg_logprob": -0.16156085761817726, "compression_ratio": 1.701657458563536, "no_speech_prob": 0.11666081845760345}, {"id": 59, "seek": 72300, "start": 728.0, "end": 742.0, "text": " Oh, I don't know combination AI thinking prompt engineering software engineering, etc, etc, etc, because those are running on different, you know, in different on different clouds that you know it's a complicated thing it was it was complicated to pull off.", "tokens": [50614, 876, 11, 286, 500, 380, 458, 6562, 7318, 1953, 12391, 7043, 4722, 7043, 11, 5183, 11, 5183, 11, 5183, 11, 570, 729, 366, 2614, 322, 819, 11, 291, 458, 11, 294, 819, 322, 819, 12193, 300, 291, 458, 309, 311, 257, 6179, 551, 309, 390, 309, 390, 6179, 281, 2235, 766, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16156085761817726, "compression_ratio": 1.701657458563536, "no_speech_prob": 0.11666081845760345}, {"id": 60, "seek": 74200, "start": 742.0, "end": 757.0, "text": " I think the, yes, basically right now, I see it as, you know, the LLM if it's just yacking in language, it's going to send it to Wolf now because Wolfram language can't do anything with that.", "tokens": [50364, 286, 519, 264, 11, 2086, 11, 1936, 558, 586, 11, 286, 536, 309, 382, 11, 291, 458, 11, 264, 441, 43, 44, 498, 309, 311, 445, 288, 14134, 294, 2856, 11, 309, 311, 516, 281, 2845, 309, 281, 16634, 586, 570, 16634, 2356, 2856, 393, 380, 360, 1340, 365, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16714942973593008, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.852546215057373}, {"id": 61, "seek": 74200, "start": 757.0, "end": 768.0, "text": " If it manages to successfully, it's been trained on a certain amount of Wolfram language examples. Probably it'll be trained on a lot more such examples in the future.", "tokens": [51114, 759, 309, 22489, 281, 10727, 11, 309, 311, 668, 8895, 322, 257, 1629, 2372, 295, 16634, 2356, 2856, 5110, 13, 9210, 309, 603, 312, 8895, 322, 257, 688, 544, 1270, 5110, 294, 264, 2027, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16714942973593008, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.852546215057373}, {"id": 62, "seek": 76800, "start": 769.0, "end": 773.0, "text": " As it gets better at synthesizing Wolfram language code.", "tokens": [50414, 1018, 309, 2170, 1101, 412, 26617, 3319, 16634, 2356, 2856, 3089, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12766452864104627, "compression_ratio": 1.4, "no_speech_prob": 0.08228915184736252}, {"id": 63, "seek": 76800, "start": 773.0, "end": 782.0, "text": " It's probably ultimately more powerful to just go straight to the Wolfram language form. But that's, it's, it also can make horrible mistakes doing that.", "tokens": [50614, 467, 311, 1391, 6284, 544, 4005, 281, 445, 352, 2997, 281, 264, 16634, 2356, 2856, 1254, 13, 583, 300, 311, 11, 309, 311, 11, 309, 611, 393, 652, 9263, 8038, 884, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12766452864104627, "compression_ratio": 1.4, "no_speech_prob": 0.08228915184736252}, {"id": 64, "seek": 78200, "start": 782.0, "end": 793.0, "text": " Wolfram Alpha is a good catchment mechanism, because it's already got, you know, many years of development of natural language understanding, where it knows, you know, given this natural language.", "tokens": [50364, 16634, 2356, 20588, 307, 257, 665, 3745, 518, 7513, 11, 570, 309, 311, 1217, 658, 11, 291, 458, 11, 867, 924, 295, 3250, 295, 3303, 2856, 3701, 11, 689, 309, 3255, 11, 291, 458, 11, 2212, 341, 3303, 2856, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15895396145907315, "compression_ratio": 1.8623481781376519, "no_speech_prob": 0.25403279066085815}, {"id": 65, "seek": 78200, "start": 793.0, "end": 797.0, "text": " First of all, it knows when it doesn't know, which is an important feature.", "tokens": [50914, 2386, 295, 439, 11, 309, 3255, 562, 309, 1177, 380, 458, 11, 597, 307, 364, 1021, 4111, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15895396145907315, "compression_ratio": 1.8623481781376519, "no_speech_prob": 0.25403279066085815}, {"id": 66, "seek": 78200, "start": 797.0, "end": 806.0, "text": " Yes, it's been pretty successfully, you know, set up so that when it doesn't understand the natural language just as I don't understand, it doesn't make something up when it does.", "tokens": [51114, 1079, 11, 309, 311, 668, 1238, 10727, 11, 291, 458, 11, 992, 493, 370, 300, 562, 309, 1177, 380, 1223, 264, 3303, 2856, 445, 382, 286, 500, 380, 1223, 11, 309, 1177, 380, 652, 746, 493, 562, 309, 775, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15895396145907315, "compression_ratio": 1.8623481781376519, "no_speech_prob": 0.25403279066085815}, {"id": 67, "seek": 78200, "start": 806.0, "end": 808.0, "text": " Humans.", "tokens": [51564, 35809, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15895396145907315, "compression_ratio": 1.8623481781376519, "no_speech_prob": 0.25403279066085815}, {"id": 68, "seek": 80800, "start": 808.0, "end": 809.0, "text": " What's that?", "tokens": [50364, 708, 311, 300, 30, 50414], "temperature": 0.0, "avg_logprob": -0.13648152881198458, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.012533402070403099}, {"id": 69, "seek": 80800, "start": 809.0, "end": 811.0, "text": " We need that for humans too.", "tokens": [50414, 492, 643, 300, 337, 6255, 886, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13648152881198458, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.012533402070403099}, {"id": 70, "seek": 80800, "start": 811.0, "end": 817.0, "text": " Yeah, right. Well, I think, I think, you know, what's interesting to me is what we learn about kind of human psychology and human operation.", "tokens": [50514, 865, 11, 558, 13, 1042, 11, 286, 519, 11, 286, 519, 11, 291, 458, 11, 437, 311, 1880, 281, 385, 307, 437, 321, 1466, 466, 733, 295, 1952, 15105, 293, 1952, 6916, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13648152881198458, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.012533402070403099}, {"id": 71, "seek": 80800, "start": 817.0, "end": 827.0, "text": " By looking at chat GBT and how unbelievably similar the types of mistakes it makes on solving problems and things like that are to the way that human the kinds of mistakes humans make.", "tokens": [50814, 3146, 1237, 412, 5081, 26809, 51, 293, 577, 43593, 2531, 264, 3467, 295, 8038, 309, 1669, 322, 12606, 2740, 293, 721, 411, 300, 366, 281, 264, 636, 300, 1952, 264, 3685, 295, 8038, 6255, 652, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13648152881198458, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.012533402070403099}, {"id": 72, "seek": 82700, "start": 827.0, "end": 838.0, "text": " And I think it really is capturing the essence of sort of the neural net it has is probably not that different in some functional sense from the neural net that we have.", "tokens": [50364, 400, 286, 519, 309, 534, 307, 23384, 264, 12801, 295, 1333, 295, 264, 18161, 2533, 309, 575, 307, 1391, 406, 300, 819, 294, 512, 11745, 2020, 490, 264, 18161, 2533, 300, 321, 362, 13, 50914], "temperature": 0.0, "avg_logprob": -0.090660489635703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.6090091466903687}, {"id": 73, "seek": 82700, "start": 838.0, "end": 849.0, "text": " And, you know, it's, it's really, it's showing us kind of what it means to be a sort of a human like answer, a human like text generator, so to speak.", "tokens": [50914, 400, 11, 291, 458, 11, 309, 311, 11, 309, 311, 534, 11, 309, 311, 4099, 505, 733, 295, 437, 309, 1355, 281, 312, 257, 1333, 295, 257, 1952, 411, 1867, 11, 257, 1952, 411, 2487, 19265, 11, 370, 281, 1710, 13, 51464], "temperature": 0.0, "avg_logprob": -0.090660489635703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.6090091466903687}, {"id": 74, "seek": 84900, "start": 849.0, "end": 863.0, "text": " In terms of the sort of the future in terms of sort of, okay, Wolfram language is a really great language for ai's to try and speak in it's sort of the best language for ai's to try and speak in, because it talks about the real world.", "tokens": [50364, 682, 2115, 295, 264, 1333, 295, 264, 2027, 294, 2115, 295, 1333, 295, 11, 1392, 11, 16634, 2356, 2856, 307, 257, 534, 869, 2856, 337, 9783, 311, 281, 853, 293, 1710, 294, 309, 311, 1333, 295, 264, 1151, 2856, 337, 9783, 311, 281, 853, 293, 1710, 294, 11, 570, 309, 6686, 466, 264, 957, 1002, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15893798828125, "compression_ratio": 1.8089171974522293, "no_speech_prob": 0.4691632390022278}, {"id": 75, "seek": 84900, "start": 863.0, "end": 867.0, "text": " It's very self contained, and it's very succinct.", "tokens": [51064, 467, 311, 588, 2698, 16212, 11, 293, 309, 311, 588, 21578, 5460, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15893798828125, "compression_ratio": 1.8089171974522293, "no_speech_prob": 0.4691632390022278}, {"id": 76, "seek": 86700, "start": 867.0, "end": 888.0, "text": " I think that it's something where the AI, and the other thing about it is, which I think is is yet what we're about to see this really, really blossom is Wolfram language as a language for human ai collaboration, because you know what we see happen is you say you type in natural", "tokens": [50364, 286, 519, 300, 309, 311, 746, 689, 264, 7318, 11, 293, 264, 661, 551, 466, 309, 307, 11, 597, 286, 519, 307, 307, 1939, 437, 321, 434, 466, 281, 536, 341, 534, 11, 534, 38524, 307, 16634, 2356, 2856, 382, 257, 2856, 337, 1952, 9783, 9363, 11, 570, 291, 458, 437, 321, 536, 1051, 307, 291, 584, 291, 2010, 294, 3303, 51414], "temperature": 0.0, "avg_logprob": -0.21864610818716196, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.7440086007118225}, {"id": 77, "seek": 88800, "start": 888.0, "end": 898.0, "text": " and you want to get this this and this. It synthesizes a piece of Wolfram language code. Maybe it's right. Maybe it's wrong. But you can kind of run that code line by line.", "tokens": [50364, 293, 291, 528, 281, 483, 341, 341, 293, 341, 13, 467, 26617, 5660, 257, 2522, 295, 16634, 2356, 2856, 3089, 13, 2704, 309, 311, 558, 13, 2704, 309, 311, 2085, 13, 583, 291, 393, 733, 295, 1190, 300, 3089, 1622, 538, 1622, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16307043075561523, "compression_ratio": 1.7702127659574467, "no_speech_prob": 0.7594977021217346}, {"id": 78, "seek": 88800, "start": 898.0, "end": 910.0, "text": " You know, Wolfram language is a symbolic language so you can always see the output at every at every stage, everything you have is a valid piece of output. So you can see that thing build up graphics build up the structure whatever else it is.", "tokens": [50864, 509, 458, 11, 16634, 2356, 2856, 307, 257, 25755, 2856, 370, 291, 393, 1009, 536, 264, 5598, 412, 633, 412, 633, 3233, 11, 1203, 291, 362, 307, 257, 7363, 2522, 295, 5598, 13, 407, 291, 393, 536, 300, 551, 1322, 493, 11837, 1322, 493, 264, 3877, 2035, 1646, 309, 307, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16307043075561523, "compression_ratio": 1.7702127659574467, "no_speech_prob": 0.7594977021217346}, {"id": 79, "seek": 91000, "start": 910.0, "end": 920.0, "text": " You can run it line by line. And as I was, you know, as I've been using it. That's what I've ended up doing it generate some code and it's like really is this right. I don't know.", "tokens": [50364, 509, 393, 1190, 309, 1622, 538, 1622, 13, 400, 382, 286, 390, 11, 291, 458, 11, 382, 286, 600, 668, 1228, 309, 13, 663, 311, 437, 286, 600, 4590, 493, 884, 309, 8460, 512, 3089, 293, 309, 311, 411, 534, 307, 341, 558, 13, 286, 500, 380, 458, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12294254631831728, "compression_ratio": 1.7509293680297398, "no_speech_prob": 0.7814719676971436}, {"id": 80, "seek": 91000, "start": 920.0, "end": 936.0, "text": " You know, it got it roughly right it's not completely crazy, but to know is it actually right well I have to kind of run it line by line sometimes, sometimes it's obviously wrong, because for example is generating error messages actually one thing we just did in the last two days is when it", "tokens": [50864, 509, 458, 11, 309, 658, 309, 9810, 558, 309, 311, 406, 2584, 3219, 11, 457, 281, 458, 307, 309, 767, 558, 731, 286, 362, 281, 733, 295, 1190, 309, 1622, 538, 1622, 2171, 11, 2171, 309, 311, 2745, 2085, 11, 570, 337, 1365, 307, 17746, 6713, 7897, 767, 472, 551, 321, 445, 630, 294, 264, 1036, 732, 1708, 307, 562, 309, 51664], "temperature": 0.0, "avg_logprob": -0.12294254631831728, "compression_ratio": 1.7509293680297398, "no_speech_prob": 0.7814719676971436}, {"id": 81, "seek": 93600, "start": 936.0, "end": 947.0, "text": " gives a message, it gives chat GPT's a bunch of text around that message. It feeds it back to chat GPT and says rewrite the code, try and avoid this message.", "tokens": [50364, 2709, 257, 3636, 11, 309, 2709, 5081, 26039, 51, 311, 257, 3840, 295, 2487, 926, 300, 3636, 13, 467, 23712, 309, 646, 281, 5081, 26039, 51, 293, 1619, 28132, 264, 3089, 11, 853, 293, 5042, 341, 3636, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20436871706784426, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.20615321397781372}, {"id": 82, "seek": 93600, "start": 947.0, "end": 959.0, "text": " And, you know, how well that will work. Not sure it's it's clearly working in some cases, and chat GPT is wonderfully polite and apologetic when it, when it keeps on doing the rewrite so to speak.", "tokens": [50914, 400, 11, 291, 458, 11, 577, 731, 300, 486, 589, 13, 1726, 988, 309, 311, 309, 311, 4448, 1364, 294, 512, 3331, 11, 293, 5081, 26039, 51, 307, 38917, 25171, 293, 9472, 3532, 562, 309, 11, 562, 309, 5965, 322, 884, 264, 28132, 370, 281, 1710, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20436871706784426, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.20615321397781372}, {"id": 83, "seek": 95900, "start": 959.0, "end": 970.0, "text": " But, you know, that's a, I think the, the sort of the place where it's going to look really interesting the kind of in the, in the world of kind of what future programming looks like.", "tokens": [50364, 583, 11, 291, 458, 11, 300, 311, 257, 11, 286, 519, 264, 11, 264, 1333, 295, 264, 1081, 689, 309, 311, 516, 281, 574, 534, 1880, 264, 733, 295, 294, 264, 11, 294, 264, 1002, 295, 733, 295, 437, 2027, 9410, 1542, 411, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09589832027753194, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.27060583233833313}, {"id": 84, "seek": 97000, "start": 970.0, "end": 988.0, "text": " I think a big piece of future programming is, you type some text that's kind of the initial here's what I roughly want to do. It synthesizes basically in from what I can see wolf language is the best language for it to synthesize because it is the most succinct language that is the most kind of expected to be", "tokens": [50364, 286, 519, 257, 955, 2522, 295, 2027, 9410, 307, 11, 291, 2010, 512, 2487, 300, 311, 733, 295, 264, 5883, 510, 311, 437, 286, 9810, 528, 281, 360, 13, 467, 26617, 5660, 1936, 294, 490, 437, 286, 393, 536, 19216, 2856, 307, 264, 1151, 2856, 337, 309, 281, 26617, 1125, 570, 309, 307, 264, 881, 21578, 5460, 2856, 300, 307, 264, 881, 733, 295, 5176, 281, 312, 51264], "temperature": 0.0, "avg_logprob": -0.1774550693135866, "compression_ratio": 1.6577540106951871, "no_speech_prob": 0.7087183594703674}, {"id": 85, "seek": 98800, "start": 989.0, "end": 1003.0, "text": " actually humans. It's not a big slab of code in some low level language, where the thing is talking about, you know, set this array pointer to this type thing. It's something which is trying to speak at the level that humans think so to speak.", "tokens": [50414, 767, 6255, 13, 467, 311, 406, 257, 955, 38616, 295, 3089, 294, 512, 2295, 1496, 2856, 11, 689, 264, 551, 307, 1417, 466, 11, 291, 458, 11, 992, 341, 10225, 23918, 281, 341, 2010, 551, 13, 467, 311, 746, 597, 307, 1382, 281, 1710, 412, 264, 1496, 300, 6255, 519, 370, 281, 1710, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10052249828974406, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.8526893854141235}, {"id": 86, "seek": 98800, "start": 1003.0, "end": 1011.0, "text": " And then, then the, you know, the picture will be and again I haven't had the experience yet, except in a few toy examples of seeing how this really works.", "tokens": [51114, 400, 550, 11, 550, 264, 11, 291, 458, 11, 264, 3036, 486, 312, 293, 797, 286, 2378, 380, 632, 264, 1752, 1939, 11, 3993, 294, 257, 1326, 12058, 5110, 295, 2577, 577, 341, 534, 1985, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10052249828974406, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.8526893854141235}, {"id": 87, "seek": 101100, "start": 1011.0, "end": 1023.0, "text": " I type my initial you know this is roughly what I want to do. It synthesizes some code. I look at that code potentially line by line it probably synthesizes and tests for that code, along with the code itself.", "tokens": [50364, 286, 2010, 452, 5883, 291, 458, 341, 307, 9810, 437, 286, 528, 281, 360, 13, 467, 26617, 5660, 512, 3089, 13, 286, 574, 412, 300, 3089, 7263, 1622, 538, 1622, 309, 1391, 26617, 5660, 293, 6921, 337, 300, 3089, 11, 2051, 365, 264, 3089, 2564, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12006125499292747, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.7603762149810791}, {"id": 88, "seek": 101100, "start": 1023.0, "end": 1033.0, "text": " It runs those things it says this is this line. This is what it did. You look at it you say no that's that's wrong. And then you either tell it that's wrong or you fix it yourself.", "tokens": [50964, 467, 6676, 729, 721, 309, 1619, 341, 307, 341, 1622, 13, 639, 307, 437, 309, 630, 13, 509, 574, 412, 309, 291, 584, 572, 300, 311, 300, 311, 2085, 13, 400, 550, 291, 2139, 980, 309, 300, 311, 2085, 420, 291, 3191, 309, 1803, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12006125499292747, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.7603762149810791}, {"id": 89, "seek": 103300, "start": 1033.0, "end": 1053.0, "text": " And then you you're steadily building up this thing that is the kind of the the final code you want that you can then you know we increasingly have efficient compilers to LLVM and things like this so once you've got this very high level representation, you're then kind of which you have been collaborating with the", "tokens": [50364, 400, 550, 291, 291, 434, 36129, 2390, 493, 341, 551, 300, 307, 264, 733, 295, 264, 264, 2572, 3089, 291, 528, 300, 291, 393, 550, 291, 458, 321, 12980, 362, 7148, 715, 388, 433, 281, 441, 43, 53, 44, 293, 721, 411, 341, 370, 1564, 291, 600, 658, 341, 588, 1090, 1496, 10290, 11, 291, 434, 550, 733, 295, 597, 291, 362, 668, 30188, 365, 264, 51364], "temperature": 0.0, "avg_logprob": -0.13091579164777484, "compression_ratio": 1.6237113402061856, "no_speech_prob": 0.30402281880378723}, {"id": 90, "seek": 105300, "start": 1053.0, "end": 1062.0, "text": " AI to produce, then you can kind of take that and deploy it however you want to deploy it whether it's in the cloud on some, you know, embedded device or whatever else it is.", "tokens": [50364, 7318, 281, 5258, 11, 550, 291, 393, 733, 295, 747, 300, 293, 7274, 309, 4461, 291, 528, 281, 7274, 309, 1968, 309, 311, 294, 264, 4588, 322, 512, 11, 291, 458, 11, 16741, 4302, 420, 2035, 1646, 309, 307, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0662654318460604, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.8218419551849365}, {"id": 91, "seek": 105300, "start": 1062.0, "end": 1073.0, "text": " But that's the kind of the point is this collaboration between the human and the AI, where you're leveraging the fact that computational language our computational language is actually readable by humans.", "tokens": [50814, 583, 300, 311, 264, 733, 295, 264, 935, 307, 341, 9363, 1296, 264, 1952, 293, 264, 7318, 11, 689, 291, 434, 32666, 264, 1186, 300, 28270, 2856, 527, 28270, 2856, 307, 767, 49857, 538, 6255, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0662654318460604, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.8218419551849365}, {"id": 92, "seek": 107300, "start": 1073.0, "end": 1083.0, "text": " And part of what makes it readable by humans is that it is a language that can immediately talk about images and cities and chemicals and movies and things like that.", "tokens": [50364, 400, 644, 295, 437, 1669, 309, 49857, 538, 6255, 307, 300, 309, 307, 257, 2856, 300, 393, 4258, 751, 466, 5267, 293, 6486, 293, 16152, 293, 6233, 293, 721, 411, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07434180613314167, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.6448878049850464}, {"id": 93, "seek": 107300, "start": 1083.0, "end": 1095.0, "text": " And you're not kind of trying to figure out. Oh, what is the data structure that it uses to talk about an image or something. It's, it's right there something that you can read as being a thing about an image for example.", "tokens": [50864, 400, 291, 434, 406, 733, 295, 1382, 281, 2573, 484, 13, 876, 11, 437, 307, 264, 1412, 3877, 300, 309, 4960, 281, 751, 466, 364, 3256, 420, 746, 13, 467, 311, 11, 309, 311, 558, 456, 746, 300, 291, 393, 1401, 382, 885, 257, 551, 466, 364, 3256, 337, 1365, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07434180613314167, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.6448878049850464}, {"id": 94, "seek": 109500, "start": 1095.0, "end": 1106.0, "text": " And so I think that's, you know, to me that's a really pretty exciting prospect of kind of the, the future of programming and you know people say what's going to happen to all the programmers.", "tokens": [50364, 400, 370, 286, 519, 300, 311, 11, 291, 458, 11, 281, 385, 300, 311, 257, 534, 1238, 4670, 15005, 295, 733, 295, 264, 11, 264, 2027, 295, 9410, 293, 291, 458, 561, 584, 437, 311, 516, 281, 1051, 281, 439, 264, 41504, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09302995681762695, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.470628559589386}, {"id": 95, "seek": 109500, "start": 1106.0, "end": 1113.0, "text": " It's like what's going to happen to everybody who does boilerplate, you know, smart boilerplating so to speak.", "tokens": [50914, 467, 311, 411, 437, 311, 516, 281, 1051, 281, 2201, 567, 775, 39228, 37008, 11, 291, 458, 11, 4069, 39228, 564, 990, 370, 281, 1710, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09302995681762695, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.470628559589386}, {"id": 96, "seek": 111300, "start": 1113.0, "end": 1123.0, "text": " You know, that it's that's something people who are, you know, producing, you know, somewhat boilerplate, you know, documents of various kinds. That's kind of going away.", "tokens": [50364, 509, 458, 11, 300, 309, 311, 300, 311, 746, 561, 567, 366, 11, 291, 458, 11, 10501, 11, 291, 458, 11, 8344, 39228, 37008, 11, 291, 458, 11, 8512, 295, 3683, 3685, 13, 663, 311, 733, 295, 516, 1314, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10341005975549872, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.7006203532218933}, {"id": 97, "seek": 111300, "start": 1123.0, "end": 1135.0, "text": " And similarly, you know, people have rushed into kind of learning, you know, going to computer science school and learning how to write, you know, Java code, Python code, whatever else it is.", "tokens": [50864, 400, 14138, 11, 291, 458, 11, 561, 362, 24421, 666, 733, 295, 2539, 11, 291, 458, 11, 516, 281, 3820, 3497, 1395, 293, 2539, 577, 281, 2464, 11, 291, 458, 11, 10745, 3089, 11, 15329, 3089, 11, 2035, 1646, 309, 307, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10341005975549872, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.7006203532218933}, {"id": 98, "seek": 113500, "start": 1135.0, "end": 1148.0, "text": " And it's like, a lot of that is just going to go away. Just like, you know, when I was, well younger than you people were always talking about a Sunday language, you know, if you're going to be serious about computing, you've got to write your code in the Sunday language.", "tokens": [50364, 400, 309, 311, 411, 11, 257, 688, 295, 300, 307, 445, 516, 281, 352, 1314, 13, 1449, 411, 11, 291, 458, 11, 562, 286, 390, 11, 731, 7037, 813, 291, 561, 645, 1009, 1417, 466, 257, 7776, 2856, 11, 291, 458, 11, 498, 291, 434, 516, 281, 312, 3156, 466, 15866, 11, 291, 600, 658, 281, 2464, 428, 3089, 294, 264, 7776, 2856, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1226327147664903, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.5858314037322998}, {"id": 99, "seek": 113500, "start": 1148.0, "end": 1151.0, "text": " I don't think anybody says that anymore.", "tokens": [51014, 286, 500, 380, 519, 4472, 1619, 300, 3602, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1226327147664903, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.5858314037322998}, {"id": 100, "seek": 115100, "start": 1151.0, "end": 1157.0, "text": " You know, in fact, the C compilers or whatever are probably producing better assembly language than any human produces at this point.", "tokens": [50364, 509, 458, 11, 294, 1186, 11, 264, 383, 715, 388, 433, 420, 2035, 366, 1391, 10501, 1101, 12103, 2856, 813, 604, 1952, 14725, 412, 341, 935, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1364338167252079, "compression_ratio": 1.823008849557522, "no_speech_prob": 0.5195038318634033}, {"id": 101, "seek": 115100, "start": 1157.0, "end": 1173.0, "text": " So, you know, what we're seeing is this kind of this moment where kind of the, the there's in these lower level languages because you kind of you can write a big slab of boilerplate language, which you needed to write as boilerplate language because it was a low level language.", "tokens": [50664, 407, 11, 291, 458, 11, 437, 321, 434, 2577, 307, 341, 733, 295, 341, 1623, 689, 733, 295, 264, 11, 264, 456, 311, 294, 613, 3126, 1496, 8650, 570, 291, 733, 295, 291, 393, 2464, 257, 955, 38616, 295, 39228, 37008, 2856, 11, 597, 291, 2978, 281, 2464, 382, 39228, 37008, 2856, 570, 309, 390, 257, 2295, 1496, 2856, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1364338167252079, "compression_ratio": 1.823008849557522, "no_speech_prob": 0.5195038318634033}, {"id": 102, "seek": 117300, "start": 1173.0, "end": 1184.0, "text": " In a sense, what we've already done with Wolfman languages automate out that boilerplate by having, you know, the one function that just does the equivalent of that big slab of lower level code.", "tokens": [50364, 682, 257, 2020, 11, 437, 321, 600, 1217, 1096, 365, 16634, 1601, 8650, 31605, 484, 300, 39228, 37008, 538, 1419, 11, 291, 458, 11, 264, 472, 2445, 300, 445, 775, 264, 10344, 295, 300, 955, 38616, 295, 3126, 1496, 3089, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10043845941990982, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.6533969044685364}, {"id": 103, "seek": 117300, "start": 1184.0, "end": 1193.0, "text": " And so that gives us the opportunity to be sort of at a level where we can have a, a meaningful conversation with the AI about what we're trying to do.", "tokens": [50914, 400, 370, 300, 2709, 505, 264, 2650, 281, 312, 1333, 295, 412, 257, 1496, 689, 321, 393, 362, 257, 11, 257, 10995, 3761, 365, 264, 7318, 466, 437, 321, 434, 1382, 281, 360, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10043845941990982, "compression_ratio": 1.5727272727272728, "no_speech_prob": 0.6533969044685364}, {"id": 104, "seek": 119300, "start": 1193.0, "end": 1208.0, "text": " We can produce kind of, it then produces a first draft of the code at a level that we can understand, we then edit that code, or tell it to edit that code, but we can understand that code we can understand the test cases and so on.", "tokens": [50364, 492, 393, 5258, 733, 295, 11, 309, 550, 14725, 257, 700, 11206, 295, 264, 3089, 412, 257, 1496, 300, 321, 393, 1223, 11, 321, 550, 8129, 300, 3089, 11, 420, 980, 309, 281, 8129, 300, 3089, 11, 457, 321, 393, 1223, 300, 3089, 321, 393, 1223, 264, 1500, 3331, 293, 370, 322, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15927682843124658, "compression_ratio": 1.8629032258064515, "no_speech_prob": 0.9104024767875671}, {"id": 105, "seek": 120800, "start": 1208.0, "end": 1228.0, "text": " And then reiterate, and, and I think it's going to be a really productive way of producing kind of computational functionality. And I think it will also open up that, that capability to a whole range of people where, you know, they didn't learn how to do memory allocation and make sure that the, you know, the", "tokens": [50364, 400, 550, 33528, 11, 293, 11, 293, 286, 519, 309, 311, 516, 281, 312, 257, 534, 13304, 636, 295, 10501, 733, 295, 28270, 14980, 13, 400, 286, 519, 309, 486, 611, 1269, 493, 300, 11, 300, 13759, 281, 257, 1379, 3613, 295, 561, 689, 11, 291, 458, 11, 436, 994, 380, 1466, 577, 281, 360, 4675, 27599, 293, 652, 988, 300, 264, 11, 291, 458, 11, 264, 51364], "temperature": 0.0, "avg_logprob": -0.11717778864041181, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.7697880864143372}, {"id": 106, "seek": 122800, "start": 1228.0, "end": 1239.0, "text": " pointers stayed aligned and you didn't, you know, whatever else was the, you know, whatever other sort of lower level thing you might be thinking about, they never learned that stuff and they don't need to.", "tokens": [50364, 44548, 9181, 17962, 293, 291, 994, 380, 11, 291, 458, 11, 2035, 1646, 390, 264, 11, 291, 458, 11, 2035, 661, 1333, 295, 3126, 1496, 551, 291, 1062, 312, 1953, 466, 11, 436, 1128, 3264, 300, 1507, 293, 436, 500, 380, 643, 281, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1498974146467916, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.23018544912338257}, {"id": 107, "seek": 122800, "start": 1239.0, "end": 1246.0, "text": " Just like for most people programmers today, they don't really need to learn, you know, how assembly language works.", "tokens": [50914, 1449, 411, 337, 881, 561, 41504, 965, 11, 436, 500, 380, 534, 643, 281, 1466, 11, 291, 458, 11, 577, 12103, 2856, 1985, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1498974146467916, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.23018544912338257}, {"id": 108, "seek": 122800, "start": 1246.0, "end": 1248.0, "text": " Right, right.", "tokens": [51264, 1779, 11, 558, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1498974146467916, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.23018544912338257}, {"id": 109, "seek": 122800, "start": 1248.0, "end": 1250.0, "text": " A lot of interesting stuff there.", "tokens": [51364, 316, 688, 295, 1880, 1507, 456, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1498974146467916, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.23018544912338257}, {"id": 110, "seek": 125000, "start": 1250.0, "end": 1268.0, "text": " So, okay, so you don't need to know how assembly language works you don't need to know how to code today. One thing like one thing I've noticed kind of through friends and like hackathons here and stuff like that is like the concept of prompt engineering is a very,", "tokens": [50364, 407, 11, 1392, 11, 370, 291, 500, 380, 643, 281, 458, 577, 12103, 2856, 1985, 291, 500, 380, 643, 281, 458, 577, 281, 3089, 965, 13, 1485, 551, 411, 472, 551, 286, 600, 5694, 733, 295, 807, 1855, 293, 411, 10339, 998, 892, 510, 293, 1507, 411, 300, 307, 411, 264, 3410, 295, 12391, 7043, 307, 257, 588, 11, 51264], "temperature": 0.0, "avg_logprob": -0.1166505359468006, "compression_ratio": 1.5868263473053892, "no_speech_prob": 0.043326400220394135}, {"id": 111, "seek": 126800, "start": 1268.0, "end": 1281.0, "text": " I feel like programmers pick it up like this because it's a very tinker heavy, you know, we can assume the neural nets of black box change change the prompt see how it at see see what comes out change it again see what comes out.", "tokens": [50364, 286, 841, 411, 41504, 1888, 309, 493, 411, 341, 570, 309, 311, 257, 588, 256, 40467, 4676, 11, 291, 458, 11, 321, 393, 6552, 264, 18161, 36170, 295, 2211, 2424, 1319, 1319, 264, 12391, 536, 577, 309, 412, 536, 536, 437, 1487, 484, 1319, 309, 797, 536, 437, 1487, 484, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14717342542565387, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.5767107009887695}, {"id": 112, "seek": 126800, "start": 1281.0, "end": 1289.0, "text": " Is this like, is this a certain type of, is that a skill in itself, is that something that could be learnable what would you, how would you decide.", "tokens": [51014, 1119, 341, 411, 11, 307, 341, 257, 1629, 2010, 295, 11, 307, 300, 257, 5389, 294, 2564, 11, 307, 300, 746, 300, 727, 312, 1466, 712, 437, 576, 291, 11, 577, 576, 291, 4536, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14717342542565387, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.5767107009887695}, {"id": 113, "seek": 128900, "start": 1289.0, "end": 1297.0, "text": " I always enjoy I've been, you know, in the past I've been responsible for a few sort of new job categories of things people might do.", "tokens": [50364, 286, 1009, 2103, 286, 600, 668, 11, 291, 458, 11, 294, 264, 1791, 286, 600, 668, 6250, 337, 257, 1326, 1333, 295, 777, 1691, 10479, 295, 721, 561, 1062, 360, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16498316487958353, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.4985034167766571}, {"id": 114, "seek": 128900, "start": 1297.0, "end": 1311.0, "text": " Like when wealth mouth came out we had this sort of new job category of linguistic curators. And it's like, you know, we have test cases like, you know, write a thing which you know you say how many ways are there to make change for 35 cents.", "tokens": [50764, 1743, 562, 7203, 4525, 1361, 484, 321, 632, 341, 1333, 295, 777, 1691, 7719, 295, 43002, 1262, 3391, 13, 400, 309, 311, 411, 11, 291, 458, 11, 321, 362, 1500, 3331, 411, 11, 291, 458, 11, 2464, 257, 551, 597, 291, 458, 291, 584, 577, 867, 2098, 366, 456, 281, 652, 1319, 337, 6976, 14941, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16498316487958353, "compression_ratio": 1.7013574660633484, "no_speech_prob": 0.4985034167766571}, {"id": 115, "seek": 131100, "start": 1311.0, "end": 1319.0, "text": " Well, there are a zillion different ways to say that some people are really good and can generate at output speed 200 ways to say that.", "tokens": [50364, 1042, 11, 456, 366, 257, 710, 11836, 819, 2098, 281, 584, 300, 512, 561, 366, 534, 665, 293, 393, 8460, 412, 5598, 3073, 2331, 2098, 281, 584, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13442505451670864, "compression_ratio": 1.781021897810219, "no_speech_prob": 0.6390706896781921}, {"id": 116, "seek": 131100, "start": 1319.0, "end": 1328.0, "text": " What kind of skill is correlated with that I wondered that you know I wanted is this going to be poets is it going to be crossword puzzle people, is it going to be computational linguistics PhDs.", "tokens": [50764, 708, 733, 295, 5389, 307, 38574, 365, 300, 286, 17055, 300, 291, 458, 286, 1415, 307, 341, 516, 281, 312, 38364, 307, 309, 516, 281, 312, 3278, 7462, 12805, 561, 11, 307, 309, 516, 281, 312, 28270, 21766, 6006, 14476, 82, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13442505451670864, "compression_ratio": 1.781021897810219, "no_speech_prob": 0.6390706896781921}, {"id": 117, "seek": 131100, "start": 1328.0, "end": 1335.0, "text": " Well, actually it was people who did not know that they had that skill, and just like doesn't everybody have the skill, but they're really good at doing it.", "tokens": [51214, 1042, 11, 767, 309, 390, 561, 567, 630, 406, 458, 300, 436, 632, 300, 5389, 11, 293, 445, 411, 1177, 380, 2201, 362, 264, 5389, 11, 457, 436, 434, 534, 665, 412, 884, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13442505451670864, "compression_ratio": 1.781021897810219, "no_speech_prob": 0.6390706896781921}, {"id": 118, "seek": 133500, "start": 1335.0, "end": 1346.0, "text": " And I think with prompt engineering, you know, I saw about a week ago I saw for the first time, a thing I was kind of predicting a resume, where somebody had been an animal wrangler.", "tokens": [50364, 400, 286, 519, 365, 12391, 7043, 11, 291, 458, 11, 286, 1866, 466, 257, 1243, 2057, 286, 1866, 337, 264, 700, 565, 11, 257, 551, 286, 390, 733, 295, 32884, 257, 15358, 11, 689, 2618, 632, 668, 364, 5496, 928, 656, 1918, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11238428642009866, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.7516369223594666}, {"id": 119, "seek": 133500, "start": 1346.0, "end": 1360.0, "text": " And now they were, they were theming themselves as a prompt engineer. And it's kind of the same, the same type of thing, you know, you're poking at this thing you really don't understand, and you're trying to get an intuition for what it does I think it's more the kind of thing where,", "tokens": [50914, 400, 586, 436, 645, 11, 436, 645, 552, 278, 2969, 382, 257, 12391, 11403, 13, 400, 309, 311, 733, 295, 264, 912, 11, 264, 912, 2010, 295, 551, 11, 291, 458, 11, 291, 434, 42684, 412, 341, 551, 291, 534, 500, 380, 1223, 11, 293, 291, 434, 1382, 281, 483, 364, 24002, 337, 437, 309, 775, 286, 519, 309, 311, 544, 264, 733, 295, 551, 689, 11, 51614], "temperature": 0.0, "avg_logprob": -0.11238428642009866, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.7516369223594666}, {"id": 120, "seek": 136000, "start": 1360.0, "end": 1373.0, "text": " you know, I'm not sure whether it's a, a, you know, I think a lot of people who do programming well, and who do programming, sort of in a really thinking pretty.", "tokens": [50364, 291, 458, 11, 286, 478, 406, 988, 1968, 309, 311, 257, 11, 257, 11, 291, 458, 11, 286, 519, 257, 688, 295, 561, 567, 360, 9410, 731, 11, 293, 567, 360, 9410, 11, 1333, 295, 294, 257, 534, 1953, 1238, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12460324258515329, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.26795151829719543}, {"id": 121, "seek": 136000, "start": 1373.0, "end": 1379.0, "text": " I mean, I don't know when I, I suppose when I try and do programming.", "tokens": [51014, 286, 914, 11, 286, 500, 380, 458, 562, 286, 11, 286, 7297, 562, 286, 853, 293, 360, 9410, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12460324258515329, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.26795151829719543}, {"id": 122, "seek": 137900, "start": 1380.0, "end": 1394.0, "text": " I'm always, I think I'm thinking pretty analytically about what's going on. I mean, sometimes I'll just, just try this, but that's pretty rare, I would say relative to the, you know, I think I have sort of an analytical understanding of what's going on.", "tokens": [50414, 286, 478, 1009, 11, 286, 519, 286, 478, 1953, 1238, 10783, 984, 466, 437, 311, 516, 322, 13, 286, 914, 11, 2171, 286, 603, 445, 11, 445, 853, 341, 11, 457, 300, 311, 1238, 5892, 11, 286, 576, 584, 4972, 281, 264, 11, 291, 458, 11, 286, 519, 286, 362, 1333, 295, 364, 29579, 3701, 295, 437, 311, 516, 322, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12011067867279053, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.578227162361145}, {"id": 123, "seek": 137900, "start": 1394.0, "end": 1407.0, "text": " I mean, one of the things that happened to me over the years is I'm now a, you know, a fluent Wolfram language programmer in the following sense, that I can start to type code before I could have told you what the code would say.", "tokens": [51114, 286, 914, 11, 472, 295, 264, 721, 300, 2011, 281, 385, 670, 264, 924, 307, 286, 478, 586, 257, 11, 291, 458, 11, 257, 40799, 16634, 2356, 2856, 32116, 294, 264, 3480, 2020, 11, 300, 286, 393, 722, 281, 2010, 3089, 949, 286, 727, 362, 1907, 291, 437, 264, 3089, 576, 584, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12011067867279053, "compression_ratio": 1.8157894736842106, "no_speech_prob": 0.578227162361145}, {"id": 124, "seek": 140700, "start": 1407.0, "end": 1416.0, "text": " It's kind of like, as opposed to I'm thinking about it in my mind in some kind of mental model that involves kind of my natural natural language, so to speak.", "tokens": [50364, 467, 311, 733, 295, 411, 11, 382, 8851, 281, 286, 478, 1953, 466, 309, 294, 452, 1575, 294, 512, 733, 295, 4973, 2316, 300, 11626, 733, 295, 452, 3303, 3303, 2856, 11, 370, 281, 1710, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10179365598238431, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.23318149149417877}, {"id": 125, "seek": 140700, "start": 1416.0, "end": 1427.0, "text": " But instead I'm actually thinking internally in Wolfram language. And that's something that again it's a feature of being a high level computational language that you can imagine doing that.", "tokens": [50814, 583, 2602, 286, 478, 767, 1953, 19501, 294, 16634, 2356, 2856, 13, 400, 300, 311, 746, 300, 797, 309, 311, 257, 4111, 295, 885, 257, 1090, 1496, 28270, 2856, 300, 291, 393, 3811, 884, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10179365598238431, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.23318149149417877}, {"id": 126, "seek": 142700, "start": 1427.0, "end": 1436.0, "text": " And, you know, I can be typing the code and I know quite a lot of other people are in this position as well, where they can type the code before they can explain what the code would say.", "tokens": [50364, 400, 11, 291, 458, 11, 286, 393, 312, 18444, 264, 3089, 293, 286, 458, 1596, 257, 688, 295, 661, 561, 366, 294, 341, 2535, 382, 731, 11, 689, 436, 393, 2010, 264, 3089, 949, 436, 393, 2903, 437, 264, 3089, 576, 584, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05986475944519043, "compression_ratio": 1.7520325203252032, "no_speech_prob": 0.6276982426643372}, {"id": 127, "seek": 142700, "start": 1436.0, "end": 1449.0, "text": " Just like if you're speaking a foreign language, you know, you, if you're not very proficient at it, you're kind of thinking in English, let's say, and then you're translating into French, as opposed to just sort of thinking fluently in French.", "tokens": [50814, 1449, 411, 498, 291, 434, 4124, 257, 5329, 2856, 11, 291, 458, 11, 291, 11, 498, 291, 434, 406, 588, 1740, 24549, 412, 309, 11, 291, 434, 733, 295, 1953, 294, 3669, 11, 718, 311, 584, 11, 293, 550, 291, 434, 35030, 666, 5522, 11, 382, 8851, 281, 445, 1333, 295, 1953, 5029, 2276, 294, 5522, 13, 51464], "temperature": 0.0, "avg_logprob": -0.05986475944519043, "compression_ratio": 1.7520325203252032, "no_speech_prob": 0.6276982426643372}, {"id": 128, "seek": 144900, "start": 1449.0, "end": 1460.0, "text": " Well, that's something you can get to the point of being able to do in computational language. I think that the question of what it takes to be kind of a great prompt engineer, I don't think we know yet.", "tokens": [50364, 1042, 11, 300, 311, 746, 291, 393, 483, 281, 264, 935, 295, 885, 1075, 281, 360, 294, 28270, 2856, 13, 286, 519, 300, 264, 1168, 295, 437, 309, 2516, 281, 312, 733, 295, 257, 869, 12391, 11403, 11, 286, 500, 380, 519, 321, 458, 1939, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0808579454716948, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.760766327381134}, {"id": 129, "seek": 144900, "start": 1460.0, "end": 1475.0, "text": " I know, and I'm trying to think in my own company, you know, we've been doing a bunch of prompt engineering. And gosh, I mean, it's a range of people actually have been doing it.", "tokens": [50914, 286, 458, 11, 293, 286, 478, 1382, 281, 519, 294, 452, 1065, 2237, 11, 291, 458, 11, 321, 600, 668, 884, 257, 3840, 295, 12391, 7043, 13, 400, 6502, 11, 286, 914, 11, 309, 311, 257, 3613, 295, 561, 767, 362, 668, 884, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0808579454716948, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.760766327381134}, {"id": 130, "seek": 147500, "start": 1475.0, "end": 1479.0, "text": " And the people.", "tokens": [50364, 400, 264, 561, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17097922547222816, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.13138724863529205}, {"id": 131, "seek": 147500, "start": 1479.0, "end": 1492.0, "text": " Gosh, I mean, it's the number of people who are very experienced at heuristics, kind of developing heuristics for for now for this and others who are more into just all around.", "tokens": [50564, 19185, 11, 286, 914, 11, 309, 311, 264, 1230, 295, 561, 567, 366, 588, 6751, 412, 415, 374, 6006, 11, 733, 295, 6416, 415, 374, 6006, 337, 337, 586, 337, 341, 293, 2357, 567, 366, 544, 666, 445, 439, 926, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17097922547222816, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.13138724863529205}, {"id": 132, "seek": 147500, "start": 1492.0, "end": 1497.0, "text": " Well, I would say all around thinking more more so than all around programming, so to speak.", "tokens": [51214, 1042, 11, 286, 576, 584, 439, 926, 1953, 544, 544, 370, 813, 439, 926, 9410, 11, 370, 281, 1710, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17097922547222816, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.13138724863529205}, {"id": 133, "seek": 149700, "start": 1497.0, "end": 1515.0, "text": " I kind of, I think it's a it's a new kind of skill. It's a it's a strange skill because it is a skill, maybe a bit more like psychology or animal wrangling than it is, I think more like that than it is programming and as a as a pure", "tokens": [50364, 286, 733, 295, 11, 286, 519, 309, 311, 257, 309, 311, 257, 777, 733, 295, 5389, 13, 467, 311, 257, 309, 311, 257, 5861, 5389, 570, 309, 307, 257, 5389, 11, 1310, 257, 857, 544, 411, 15105, 420, 5496, 928, 656, 1688, 813, 309, 307, 11, 286, 519, 544, 411, 300, 813, 309, 307, 9410, 293, 382, 257, 382, 257, 6075, 51264], "temperature": 0.0, "avg_logprob": -0.14495136554424579, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.5457723736763}, {"id": 134, "seek": 151500, "start": 1515.0, "end": 1518.0, "text": " kind of", "tokens": [50364, 733, 295, 50514], "temperature": 0.0, "avg_logprob": -0.133760666847229, "compression_ratio": 1.4471544715447155, "no_speech_prob": 0.6992074251174927}, {"id": 135, "seek": 151500, "start": 1518.0, "end": 1532.0, "text": " I mean, there may come a time when we understand enough about prompt engineering that we're able to kind of have a formal structure for thinking about prompt engineering.", "tokens": [50514, 286, 914, 11, 456, 815, 808, 257, 565, 562, 321, 1223, 1547, 466, 12391, 7043, 300, 321, 434, 1075, 281, 733, 295, 362, 257, 9860, 3877, 337, 1953, 466, 12391, 7043, 13, 51214], "temperature": 0.0, "avg_logprob": -0.133760666847229, "compression_ratio": 1.4471544715447155, "no_speech_prob": 0.6992074251174927}, {"id": 136, "seek": 153200, "start": 1532.0, "end": 1542.0, "text": " No, it's an interesting question and it's something which given that one has, I don't know, I, you know, a reasonable at least high level understanding of how chat GPT is working.", "tokens": [50364, 883, 11, 309, 311, 364, 1880, 1168, 293, 309, 311, 746, 597, 2212, 300, 472, 575, 11, 286, 500, 380, 458, 11, 286, 11, 291, 458, 11, 257, 10585, 412, 1935, 1090, 1496, 3701, 295, 577, 5081, 26039, 51, 307, 1364, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11198081592522045, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.7335416078567505}, {"id": 137, "seek": 153200, "start": 1542.0, "end": 1555.0, "text": " You know, can one so an interesting question that I have not really addressed is having made quite a study of how chat GPT works and sort of thinking about it a little bit like, you know, playing physicist on chat GPT so to speak or playing natural", "tokens": [50864, 509, 458, 11, 393, 472, 370, 364, 1880, 1168, 300, 286, 362, 406, 534, 13847, 307, 1419, 1027, 1596, 257, 2979, 295, 577, 5081, 26039, 51, 1985, 293, 1333, 295, 1953, 466, 309, 257, 707, 857, 411, 11, 291, 458, 11, 2433, 42466, 322, 5081, 26039, 51, 370, 281, 1710, 420, 2433, 3303, 51514], "temperature": 0.0, "avg_logprob": -0.11198081592522045, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.7335416078567505}, {"id": 138, "seek": 155500, "start": 1555.0, "end": 1565.0, "text": " scientist. Does that help me to know what I should write in prompts. And I haven't really figured that out. I don't know I haven't had a chance to think about it actually yet.", "tokens": [50364, 12662, 13, 4402, 300, 854, 385, 281, 458, 437, 286, 820, 2464, 294, 41095, 13, 400, 286, 2378, 380, 534, 8932, 300, 484, 13, 286, 500, 380, 458, 286, 2378, 380, 632, 257, 2931, 281, 519, 466, 309, 767, 1939, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14148049884372288, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.7069078683853149}, {"id": 139, "seek": 155500, "start": 1565.0, "end": 1582.0, "text": " In other words, because it I find it utterly bizarre that it, you know, does it, you know, saying please, does it matter, putting things in capital letters, does it matter, you know, which is, you know, does it pay more attention to the thing that came later,", "tokens": [50864, 682, 661, 2283, 11, 570, 309, 286, 915, 309, 30251, 18265, 300, 309, 11, 291, 458, 11, 775, 309, 11, 291, 458, 11, 1566, 1767, 11, 775, 309, 1871, 11, 3372, 721, 294, 4238, 7825, 11, 775, 309, 1871, 11, 291, 458, 11, 597, 307, 11, 291, 458, 11, 775, 309, 1689, 544, 3202, 281, 264, 551, 300, 1361, 1780, 11, 51714], "temperature": 0.0, "avg_logprob": -0.14148049884372288, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.7069078683853149}, {"id": 140, "seek": 158200, "start": 1583.0, "end": 1595.0, "text": " does it, you know, does it matter. You know, for example, I've spent a lot of time in my life figuring out how to explain stuff to people. And I know a lot of rules of thumb.", "tokens": [50414, 775, 309, 11, 291, 458, 11, 775, 309, 1871, 13, 509, 458, 11, 337, 1365, 11, 286, 600, 4418, 257, 688, 295, 565, 294, 452, 993, 15213, 484, 577, 281, 2903, 1507, 281, 561, 13, 400, 286, 458, 257, 688, 295, 4474, 295, 9298, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09623762026225051, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.2542385160923004}, {"id": 141, "seek": 158200, "start": 1595.0, "end": 1601.0, "text": " When you're writing something textually about how to do that so that people have a chance of understanding it.", "tokens": [51014, 1133, 291, 434, 3579, 746, 2487, 671, 466, 577, 281, 360, 300, 370, 300, 561, 362, 257, 2931, 295, 3701, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09623762026225051, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.2542385160923004}, {"id": 142, "seek": 160100, "start": 1602.0, "end": 1619.0, "text": " Example, you know, one thing I learned very early on decades ago in writing software documentation is another things for that matter is, if you say the critical thing in three magnificent words in the middle of a paragraph, nobody will get it.", "tokens": [50414, 24755, 781, 11, 291, 458, 11, 472, 551, 286, 3264, 588, 2440, 322, 7878, 2057, 294, 3579, 4722, 14333, 307, 1071, 721, 337, 300, 1871, 307, 11, 498, 291, 584, 264, 4924, 551, 294, 1045, 23690, 2283, 294, 264, 2808, 295, 257, 18865, 11, 5079, 486, 483, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13421204405010873, "compression_ratio": 1.4550898203592815, "no_speech_prob": 0.35872387886047363}, {"id": 143, "seek": 161900, "start": 1620.0, "end": 1635.0, "text": " You know, what you have to do is to some extent, the, the emphasis is in part determined by the area on the page that you spend yacking about that thing. And so does that matter for LLMs. I don't know.", "tokens": [50414, 509, 458, 11, 437, 291, 362, 281, 360, 307, 281, 512, 8396, 11, 264, 11, 264, 16271, 307, 294, 644, 9540, 538, 264, 1859, 322, 264, 3028, 300, 291, 3496, 288, 14134, 466, 300, 551, 13, 400, 370, 775, 300, 1871, 337, 441, 43, 26386, 13, 286, 500, 380, 458, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16473981683904473, "compression_ratio": 1.4055944055944056, "no_speech_prob": 0.3833758533000946}, {"id": 144, "seek": 163500, "start": 1636.0, "end": 1653.0, "text": " You know, it could be that the, that the great prompt engineers are also great human expositors. It could be that the great prompt engineers are people who are more used to, I mean, you know, who are used to sort of talking to babies, talking to animals, so to speak.", "tokens": [50414, 509, 458, 11, 309, 727, 312, 300, 264, 11, 300, 264, 869, 12391, 11955, 366, 611, 869, 1952, 1278, 9598, 830, 13, 467, 727, 312, 300, 264, 869, 12391, 11955, 366, 561, 567, 366, 544, 1143, 281, 11, 286, 914, 11, 291, 458, 11, 567, 366, 1143, 281, 1333, 295, 1417, 281, 10917, 11, 1417, 281, 4882, 11, 370, 281, 1710, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10337053645740855, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.37045520544052124}, {"id": 145, "seek": 165300, "start": 1653.0, "end": 1674.0, "text": " I'm not sure. I think that the kind of a notion of kind of, well, you could ask as much about sort of formal prompt engineering, if the AI is acting a bit like a human and after all it learned from human language as we've expressed it on the web.", "tokens": [50364, 286, 478, 406, 988, 13, 286, 519, 300, 264, 733, 295, 257, 10710, 295, 733, 295, 11, 731, 11, 291, 727, 1029, 382, 709, 466, 1333, 295, 9860, 12391, 7043, 11, 498, 264, 7318, 307, 6577, 257, 857, 411, 257, 1952, 293, 934, 439, 309, 3264, 490, 1952, 2856, 382, 321, 600, 12675, 309, 322, 264, 3670, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15081345650457567, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.562360942363739}, {"id": 146, "seek": 167400, "start": 1675.0, "end": 1688.0, "text": " What is persuasive writing for a prompt? You know, how do you, can you, and of course, you know, by the time you're saying, well, I'm going to have a meta prompt where I'm going to ask the AI to, to write a prompt for itself.", "tokens": [50414, 708, 307, 16336, 23686, 3579, 337, 257, 12391, 30, 509, 458, 11, 577, 360, 291, 11, 393, 291, 11, 293, 295, 1164, 11, 291, 458, 11, 538, 264, 565, 291, 434, 1566, 11, 731, 11, 286, 478, 516, 281, 362, 257, 19616, 12391, 689, 286, 478, 516, 281, 1029, 264, 7318, 281, 11, 281, 2464, 257, 12391, 337, 2564, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11849933117628098, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.4931604564189911}, {"id": 147, "seek": 168800, "start": 1689.0, "end": 1708.0, "text": " I don't know how well that ends because the fact is that, you know, in the end, you have to say what the heck you're talking about. And then it can, and this is again, the sort of a little bit the fallacy of people were looking at lower level languages and programming languages and saying, gosh, we can get the AI to do this.", "tokens": [50414, 286, 500, 380, 458, 577, 731, 300, 5314, 570, 264, 1186, 307, 300, 11, 291, 458, 11, 294, 264, 917, 11, 291, 362, 281, 584, 437, 264, 12872, 291, 434, 1417, 466, 13, 400, 550, 309, 393, 11, 293, 341, 307, 797, 11, 264, 1333, 295, 257, 707, 857, 264, 2100, 2551, 295, 561, 645, 1237, 412, 3126, 1496, 8650, 293, 9410, 8650, 293, 1566, 11, 6502, 11, 321, 393, 483, 264, 7318, 281, 360, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13492743174235025, "compression_ratio": 1.6464646464646464, "no_speech_prob": 0.662834882736206}, {"id": 148, "seek": 170800, "start": 1708.0, "end": 1719.0, "text": " And the reason you can do that is because those languages are deeply compressible, because they're, they're low level they're talking about things that the computer is doing. They're not talking about sort of the big goals that it's following.", "tokens": [50364, 400, 264, 1778, 291, 393, 360, 300, 307, 570, 729, 8650, 366, 8760, 14778, 964, 11, 570, 436, 434, 11, 436, 434, 2295, 1496, 436, 434, 1417, 466, 721, 300, 264, 3820, 307, 884, 13, 814, 434, 406, 1417, 466, 1333, 295, 264, 955, 5493, 300, 309, 311, 3480, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1228567815459935, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.39115384221076965}, {"id": 149, "seek": 170800, "start": 1719.0, "end": 1734.0, "text": " As soon as you're talking about the big goals. It's much less kind of compressed by an experience I had wrote this book. Well, I don't know what was it. Gosh, it's very long ago seven years ago now about elementary introduction to the world from language.", "tokens": [50914, 1018, 2321, 382, 291, 434, 1417, 466, 264, 955, 5493, 13, 467, 311, 709, 1570, 733, 295, 30353, 538, 364, 1752, 286, 632, 4114, 341, 1446, 13, 1042, 11, 286, 500, 380, 458, 437, 390, 309, 13, 19185, 11, 309, 311, 588, 938, 2057, 3407, 924, 2057, 586, 466, 16429, 9339, 281, 264, 1002, 490, 2856, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1228567815459935, "compression_ratio": 1.7758007117437722, "no_speech_prob": 0.39115384221076965}, {"id": 150, "seek": 173400, "start": 1734.0, "end": 1748.0, "text": " So I'm very easy book for me to write and I'm kind of upset more people haven't written books like that because it was so easy to write but I then I realized maybe it's easy for me to write and it's not quite so easy for other people to write because I've had lots of experience in trying to explain stuff to people.", "tokens": [50364, 407, 286, 478, 588, 1858, 1446, 337, 385, 281, 2464, 293, 286, 478, 733, 295, 8340, 544, 561, 2378, 380, 3720, 3642, 411, 300, 570, 309, 390, 370, 1858, 281, 2464, 457, 286, 550, 286, 5334, 1310, 309, 311, 1858, 337, 385, 281, 2464, 293, 309, 311, 406, 1596, 370, 1858, 337, 661, 561, 281, 2464, 570, 286, 600, 632, 3195, 295, 1752, 294, 1382, 281, 2903, 1507, 281, 561, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15856854756673178, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.28255394101142883}, {"id": 151, "seek": 174800, "start": 1748.0, "end": 1761.0, "text": " And the, but there you guys in that book, I had exercises and the exercises are pretty much all of the form. Here's a description of the thing we want to do described in English, right and involved in language.", "tokens": [50364, 400, 264, 11, 457, 456, 291, 1074, 294, 300, 1446, 11, 286, 632, 11900, 293, 264, 11900, 366, 1238, 709, 439, 295, 264, 1254, 13, 1692, 311, 257, 3855, 295, 264, 551, 321, 528, 281, 360, 7619, 294, 3669, 11, 558, 293, 3288, 294, 2856, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2554968472184806, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.6931121349334717}, {"id": 152, "seek": 174800, "start": 1761.0, "end": 1771.0, "text": " Okay, at the beginning of the book, had an easy time making up those exercises. The English language version was very simple. I was imagining some more from language form.", "tokens": [51014, 1033, 11, 412, 264, 2863, 295, 264, 1446, 11, 632, 364, 1858, 565, 1455, 493, 729, 11900, 13, 440, 3669, 2856, 3037, 390, 588, 2199, 13, 286, 390, 27798, 512, 544, 490, 2856, 1254, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2554968472184806, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.6931121349334717}, {"id": 153, "seek": 177100, "start": 1771.0, "end": 1784.0, "text": " In the book where things getting a little bit more elaborate. It's kind of like, well, I can immediately under, you know, I immediately know what is the law from language version of what I'm trying to say, then I have to back translate it to English.", "tokens": [50364, 682, 264, 1446, 689, 721, 1242, 257, 707, 857, 544, 20945, 13, 467, 311, 733, 295, 411, 11, 731, 11, 286, 393, 4258, 833, 11, 291, 458, 11, 286, 4258, 458, 437, 307, 264, 2101, 490, 2856, 3037, 295, 437, 286, 478, 1382, 281, 584, 11, 550, 286, 362, 281, 646, 13799, 309, 281, 3669, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18769889407687718, "compression_ratio": 1.523076923076923, "no_speech_prob": 0.7145441174507141}, {"id": 154, "seek": 177100, "start": 1784.0, "end": 1787.0, "text": " And the thing ends up reading like legal ease.", "tokens": [51014, 400, 264, 551, 5314, 493, 3760, 411, 5089, 12708, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18769889407687718, "compression_ratio": 1.523076923076923, "no_speech_prob": 0.7145441174507141}, {"id": 155, "seek": 178700, "start": 1788.0, "end": 1799.0, "text": " Because, because that's a thing that is not very well expressible in English. So, again, it's kind of, and that of course leads to the question of can you write both language code that is a prompt.", "tokens": [50414, 1436, 11, 570, 300, 311, 257, 551, 300, 307, 406, 588, 731, 5109, 964, 294, 3669, 13, 407, 11, 797, 11, 309, 311, 733, 295, 11, 293, 300, 295, 1164, 6689, 281, 264, 1168, 295, 393, 291, 2464, 1293, 2856, 3089, 300, 307, 257, 12391, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12940669059753418, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.77122563123703}, {"id": 156, "seek": 178700, "start": 1799.0, "end": 1810.0, "text": " And the answer is probably yes. And that's another interesting possibility. And that's, you know, that that becomes a way of expressing yourself, kind of in computational language.", "tokens": [50964, 400, 264, 1867, 307, 1391, 2086, 13, 400, 300, 311, 1071, 1880, 7959, 13, 400, 300, 311, 11, 291, 458, 11, 300, 300, 3643, 257, 636, 295, 22171, 1803, 11, 733, 295, 294, 28270, 2856, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12940669059753418, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.77122563123703}, {"id": 157, "seek": 181000, "start": 1810.0, "end": 1825.0, "text": " But for the AI, which is then kind of itself, thinking in computational language, for example, rather than thinking in English, you know, there's obviously less walking language on the on the web than there is less human written", "tokens": [50364, 583, 337, 264, 7318, 11, 597, 307, 550, 733, 295, 2564, 11, 1953, 294, 28270, 2856, 11, 337, 1365, 11, 2831, 813, 1953, 294, 3669, 11, 291, 458, 11, 456, 311, 2745, 1570, 4494, 2856, 322, 264, 322, 264, 3670, 813, 456, 307, 1570, 1952, 3720, 51114], "temperature": 0.0, "avg_logprob": -0.21978916168212892, "compression_ratio": 1.5, "no_speech_prob": 0.7195740342140198}, {"id": 158, "seek": 182500, "start": 1826.0, "end": 1842.0, "text": " English text on the web. But it's also in a sense much easier to learn it doesn't have a lot of the irregularities of English language. It has, you know, it's something where you can look up the definition, so to speak, you can operate from the definition and so on.", "tokens": [50414, 3669, 2487, 322, 264, 3670, 13, 583, 309, 311, 611, 294, 257, 2020, 709, 3571, 281, 1466, 309, 1177, 380, 362, 257, 688, 295, 264, 29349, 1088, 295, 3669, 2856, 13, 467, 575, 11, 291, 458, 11, 309, 311, 746, 689, 291, 393, 574, 493, 264, 7123, 11, 370, 281, 1710, 11, 291, 393, 9651, 490, 264, 7123, 293, 370, 322, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08780630429585774, "compression_ratio": 1.52, "no_speech_prob": 0.6890965104103088}, {"id": 159, "seek": 184200, "start": 1842.0, "end": 1860.0, "text": " So I think it's, we're in very, you know, very early days of the theory of prompt engineering, I think that, but I do think that a very important aspect of kind of this picture is this potential loop between kind of the the prompt, the computational", "tokens": [50364, 407, 286, 519, 309, 311, 11, 321, 434, 294, 588, 11, 291, 458, 11, 588, 2440, 1708, 295, 264, 5261, 295, 12391, 7043, 11, 286, 519, 300, 11, 457, 286, 360, 519, 300, 257, 588, 1021, 4171, 295, 733, 295, 341, 3036, 307, 341, 3995, 6367, 1296, 733, 295, 264, 264, 12391, 11, 264, 28270, 51264], "temperature": 0.0, "avg_logprob": -0.11434029724638341, "compression_ratio": 1.6064516129032258, "no_speech_prob": 0.1538926362991333}, {"id": 160, "seek": 186000, "start": 1861.0, "end": 1868.0, "text": " language, seeing what the computational language does, generating automated tests for the computational language.", "tokens": [50414, 2856, 11, 2577, 437, 264, 28270, 2856, 775, 11, 17746, 18473, 6921, 337, 264, 28270, 2856, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13586775078830948, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.6291828751564026}, {"id": 161, "seek": 186000, "start": 1868.0, "end": 1882.0, "text": " And, you know, and then and then having the LLM essentially present, you know, this is the LLM's version of how to explain that to a human. Here's what it did. Here's the explanation, you know, you may not like it, maybe wrong.", "tokens": [50764, 400, 11, 291, 458, 11, 293, 550, 293, 550, 1419, 264, 441, 43, 44, 4476, 1974, 11, 291, 458, 11, 341, 307, 264, 441, 43, 44, 311, 3037, 295, 577, 281, 2903, 300, 281, 257, 1952, 13, 1692, 311, 437, 309, 630, 13, 1692, 311, 264, 10835, 11, 291, 458, 11, 291, 815, 406, 411, 309, 11, 1310, 2085, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13586775078830948, "compression_ratio": 1.7309644670050761, "no_speech_prob": 0.6291828751564026}, {"id": 162, "seek": 188200, "start": 1883.0, "end": 1897.0, "text": " But here's the LLM helping to explain it by actually running. And one thing, yeah, it's almost seems trivial to me at this point, but, but the fact that the LLM can go off and run a piece of walking language code and see what happened is very important to its, to its kind of life and times", "tokens": [50414, 583, 510, 311, 264, 441, 43, 44, 4315, 281, 2903, 309, 538, 767, 2614, 13, 400, 472, 551, 11, 1338, 11, 309, 311, 1920, 2544, 26703, 281, 385, 412, 341, 935, 11, 457, 11, 457, 264, 1186, 300, 264, 441, 43, 44, 393, 352, 766, 293, 1190, 257, 2522, 295, 4494, 2856, 3089, 293, 536, 437, 2011, 307, 588, 1021, 281, 1080, 11, 281, 1080, 733, 295, 993, 293, 1413, 51114], "temperature": 0.0, "avg_logprob": -0.18558924906962626, "compression_ratio": 1.5183246073298429, "no_speech_prob": 0.47335436940193176}, {"id": 163, "seek": 189700, "start": 1898.0, "end": 1906.0, "text": " it's what makes it, you know, and you see it, it's kind of weird to see it, you know, it tries this, it says, I'm going to re re re rephrase it, I'm going to try this, etc, etc, etc.", "tokens": [50414, 309, 311, 437, 1669, 309, 11, 291, 458, 11, 293, 291, 536, 309, 11, 309, 311, 733, 295, 3657, 281, 536, 309, 11, 291, 458, 11, 309, 9898, 341, 11, 309, 1619, 11, 286, 478, 516, 281, 319, 319, 319, 319, 44598, 651, 309, 11, 286, 478, 516, 281, 853, 341, 11, 5183, 11, 5183, 11, 5183, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1303926467895508, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.8409212231636047}, {"id": 164, "seek": 189700, "start": 1906.0, "end": 1916.0, "text": " I think it will be nice in terms of the sort of IDE aspect of this, to be able to see the code it's producing see the test cases come out, be able to click some things.", "tokens": [50814, 286, 519, 309, 486, 312, 1481, 294, 2115, 295, 264, 1333, 295, 40930, 4171, 295, 341, 11, 281, 312, 1075, 281, 536, 264, 3089, 309, 311, 10501, 536, 264, 1500, 3331, 808, 484, 11, 312, 1075, 281, 2052, 512, 721, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1303926467895508, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.8409212231636047}, {"id": 165, "seek": 191600, "start": 1916.0, "end": 1929.0, "text": " And then, you know, as you see those those fragments come out, then be able to have a nice way of rolling it up to make a bigger, bigger program, which you can then treat as a single unit to then go on and use that elsewhere.", "tokens": [50364, 400, 550, 11, 291, 458, 11, 382, 291, 536, 729, 729, 29197, 808, 484, 11, 550, 312, 1075, 281, 362, 257, 1481, 636, 295, 9439, 309, 493, 281, 652, 257, 3801, 11, 3801, 1461, 11, 597, 291, 393, 550, 2387, 382, 257, 2167, 4985, 281, 550, 352, 322, 293, 764, 300, 14517, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06786972577454614, "compression_ratio": 1.7285223367697595, "no_speech_prob": 0.3172004520893097}, {"id": 166, "seek": 191600, "start": 1929.0, "end": 1944.0, "text": " I think, also, I mean, you know, in terms of, oh, one of the things that's just a piece of software engineering that we've been having an amusing time with, you know, we've developed over the last, you know, 36 years pretty good technology for doing automated software testing.", "tokens": [51014, 286, 519, 11, 611, 11, 286, 914, 11, 291, 458, 11, 294, 2115, 295, 11, 1954, 11, 472, 295, 264, 721, 300, 311, 445, 257, 2522, 295, 4722, 7043, 300, 321, 600, 668, 1419, 364, 47809, 565, 365, 11, 291, 458, 11, 321, 600, 4743, 670, 264, 1036, 11, 291, 458, 11, 8652, 924, 1238, 665, 2899, 337, 884, 18473, 4722, 4997, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06786972577454614, "compression_ratio": 1.7285223367697595, "no_speech_prob": 0.3172004520893097}, {"id": 167, "seek": 194400, "start": 1944.0, "end": 1959.0, "text": " And it's like, okay, we've got a software quality assurance team. And, you know, I was telling them, okay, so now you've got to test this, this plugin, and it's like, how are we going to test this, you know, it has no predictability at all, what it's going to do.", "tokens": [50364, 400, 309, 311, 411, 11, 1392, 11, 321, 600, 658, 257, 4722, 3125, 32189, 1469, 13, 400, 11, 291, 458, 11, 286, 390, 3585, 552, 11, 1392, 11, 370, 586, 291, 600, 658, 281, 1500, 341, 11, 341, 23407, 11, 293, 309, 311, 411, 11, 577, 366, 321, 516, 281, 1500, 341, 11, 291, 458, 11, 309, 575, 572, 6069, 2310, 412, 439, 11, 437, 309, 311, 516, 281, 360, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08012999337295006, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.06412570178508759}, {"id": 168, "seek": 194400, "start": 1959.0, "end": 1968.0, "text": " And, you know, in a sense, the way we can test it, which we've been doing a little bit of, is you have the LLM comment on its own behavior.", "tokens": [51114, 400, 11, 291, 458, 11, 294, 257, 2020, 11, 264, 636, 321, 393, 1500, 309, 11, 597, 321, 600, 668, 884, 257, 707, 857, 295, 11, 307, 291, 362, 264, 441, 43, 44, 2871, 322, 1080, 1065, 5223, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08012999337295006, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.06412570178508759}, {"id": 169, "seek": 196800, "start": 1968.0, "end": 1990.0, "text": " Yes, this is a, this is something we've heard about a lot, like, I guess, something we're working on is kind of like, how do you test things in prod, how do you evaluate that prompt A is better than prompt B. And one of the things I've heard about, mostly from hackers and mostly from into like the community as opposed to kind of the big companies is this whole synthetic evaluation, how do you have the LLM", "tokens": [50364, 1079, 11, 341, 307, 257, 11, 341, 307, 746, 321, 600, 2198, 466, 257, 688, 11, 411, 11, 286, 2041, 11, 746, 321, 434, 1364, 322, 307, 733, 295, 411, 11, 577, 360, 291, 1500, 721, 294, 15792, 11, 577, 360, 291, 13059, 300, 12391, 316, 307, 1101, 813, 12391, 363, 13, 400, 472, 295, 264, 721, 286, 600, 2198, 466, 11, 5240, 490, 39766, 293, 5240, 490, 666, 411, 264, 1768, 382, 8851, 281, 733, 295, 264, 955, 3431, 307, 341, 1379, 23420, 13344, 11, 577, 360, 291, 362, 264, 441, 43, 44, 51464], "temperature": 0.0, "avg_logprob": -0.15092945098876953, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.3992158770561218}, {"id": 170, "seek": 199000, "start": 1991.0, "end": 2002.0, "text": " evaluate itself, which feels like it either will work really well or not work at all. It's kind of the journey remains to be out. But how are you, you're doing that now or it's an idea.", "tokens": [50414, 13059, 2564, 11, 597, 3417, 411, 309, 2139, 486, 589, 534, 731, 420, 406, 589, 412, 439, 13, 467, 311, 733, 295, 264, 4671, 7023, 281, 312, 484, 13, 583, 577, 366, 291, 11, 291, 434, 884, 300, 586, 420, 309, 311, 364, 1558, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14572693146381183, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.6741346716880798}, {"id": 171, "seek": 199000, "start": 2002.0, "end": 2012.0, "text": " We started doing that. Yeah, we started doing it. I mean, I would say it's, it's not for, for reasons of sort of boring software engineering reasons, it's not quite as easy to do yet.", "tokens": [50964, 492, 1409, 884, 300, 13, 865, 11, 321, 1409, 884, 309, 13, 286, 914, 11, 286, 576, 584, 309, 311, 11, 309, 311, 406, 337, 11, 337, 4112, 295, 1333, 295, 9989, 4722, 7043, 4112, 11, 309, 311, 406, 1596, 382, 1858, 281, 360, 1939, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14572693146381183, "compression_ratio": 1.654708520179372, "no_speech_prob": 0.6741346716880798}, {"id": 172, "seek": 201200, "start": 2012.0, "end": 2029.0, "text": " You know, in, in, you'll find in our packet repository, you'll find this open AI link, which is a, you know, just an API wrapper in orphan language that allows one to sort of call, call a bunch of open AI APIs.", "tokens": [50364, 509, 458, 11, 294, 11, 294, 11, 291, 603, 915, 294, 527, 20300, 25841, 11, 291, 603, 915, 341, 1269, 7318, 2113, 11, 597, 307, 257, 11, 291, 458, 11, 445, 364, 9362, 46906, 294, 28711, 2856, 300, 4045, 472, 281, 1333, 295, 818, 11, 818, 257, 3840, 295, 1269, 7318, 21445, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21342148697167113, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.7023725509643555}, {"id": 173, "seek": 202900, "start": 2029.0, "end": 2051.0, "text": " And that's, you know, that that's the thing on which we're building kind of the testing framework. But, you know, it's been the case. I mean, it hasn't software testing involves many sort of pieces of exogenous information, like for example, back from 35 years ago with testing graphics output.", "tokens": [50364, 400, 300, 311, 11, 291, 458, 11, 300, 300, 311, 264, 551, 322, 597, 321, 434, 2390, 733, 295, 264, 4997, 8388, 13, 583, 11, 291, 458, 11, 309, 311, 668, 264, 1389, 13, 286, 914, 11, 309, 6132, 380, 4722, 4997, 11626, 867, 1333, 295, 3755, 295, 454, 45519, 1589, 11, 411, 337, 1365, 11, 646, 490, 6976, 924, 2057, 365, 4997, 11837, 5598, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12710163933890206, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.35536760091781616}, {"id": 174, "seek": 205100, "start": 2051.0, "end": 2067.0, "text": " Okay. Well, you know, a pixel could change here or there and it doesn't matter. So you have to have a way of doing regression testing that doesn't isn't not affected by individual pixels. And so we were doing early on we were doing image processing later on we've been doing", "tokens": [50364, 1033, 13, 1042, 11, 291, 458, 11, 257, 19261, 727, 1319, 510, 420, 456, 293, 309, 1177, 380, 1871, 13, 407, 291, 362, 281, 362, 257, 636, 295, 884, 24590, 4997, 300, 1177, 380, 1943, 380, 406, 8028, 538, 2609, 18668, 13, 400, 370, 321, 645, 884, 2440, 322, 321, 645, 884, 3256, 9007, 1780, 322, 321, 600, 668, 884, 51164], "temperature": 0.0, "avg_logprob": -0.11264412105083466, "compression_ratio": 1.5480225988700564, "no_speech_prob": 0.5960118174552917}, {"id": 175, "seek": 206700, "start": 2067.0, "end": 2083.0, "text": " more machine learning type methods to figure out, did it matter, did what happened matter. Another example that is timing tests, you know, you've got 10 million tests, and some of them run slower than the new version, do you care, how many how much slower can they run.", "tokens": [50364, 544, 3479, 2539, 2010, 7150, 281, 2573, 484, 11, 630, 309, 1871, 11, 630, 437, 2011, 1871, 13, 3996, 1365, 300, 307, 10822, 6921, 11, 291, 458, 11, 291, 600, 658, 1266, 2459, 6921, 11, 293, 512, 295, 552, 1190, 14009, 813, 264, 777, 3037, 11, 360, 291, 1127, 11, 577, 867, 577, 709, 14009, 393, 436, 1190, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1300163437834883, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.5902523398399353}, {"id": 176, "seek": 206700, "start": 2083.0, "end": 2095.0, "text": " That's more a question of sort of a statistical, you know, way of figuring out what what you think is acceptable and what isn't. And it's kind of a. So I think there are all these kind of methodologies.", "tokens": [51164, 663, 311, 544, 257, 1168, 295, 1333, 295, 257, 22820, 11, 291, 458, 11, 636, 295, 15213, 484, 437, 437, 291, 519, 307, 15513, 293, 437, 1943, 380, 13, 400, 309, 311, 733, 295, 257, 13, 407, 286, 519, 456, 366, 439, 613, 733, 295, 3170, 6204, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1300163437834883, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.5902523398399353}, {"id": 177, "seek": 209500, "start": 2095.0, "end": 2113.0, "text": " And, you know, one of the things that's kind of nice about interacting with an LLM interacting with computational language, computational language, kind of knows when it went. Well, it knows many aspects of when it went way off track, because it's just, it's just you can't get there from here it's just generating error messages.", "tokens": [50364, 400, 11, 291, 458, 11, 472, 295, 264, 721, 300, 311, 733, 295, 1481, 466, 18017, 365, 364, 441, 43, 44, 18017, 365, 28270, 2856, 11, 28270, 2856, 11, 733, 295, 3255, 562, 309, 1437, 13, 1042, 11, 309, 3255, 867, 7270, 295, 562, 309, 1437, 636, 766, 2837, 11, 570, 309, 311, 445, 11, 309, 311, 445, 291, 393, 380, 483, 456, 490, 510, 309, 311, 445, 17746, 6713, 7897, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12482605157075105, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.22263404726982117}, {"id": 178, "seek": 209500, "start": 2113.0, "end": 2123.0, "text": " Things are happening. I mean within Wolfram language, we have pretty good kind of actually we're working on another level of this error handling capability.", "tokens": [51264, 9514, 366, 2737, 13, 286, 914, 1951, 16634, 2356, 2856, 11, 321, 362, 1238, 665, 733, 295, 767, 321, 434, 1364, 322, 1071, 1496, 295, 341, 6713, 13175, 13759, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12482605157075105, "compression_ratio": 1.7773722627737227, "no_speech_prob": 0.22263404726982117}, {"id": 179, "seek": 212300, "start": 2123.0, "end": 2130.0, "text": " So, you know, here's the thing I mean when one thinks about programming, everybody thinks about the way the program was supposed to work.", "tokens": [50364, 407, 11, 291, 458, 11, 510, 311, 264, 551, 286, 914, 562, 472, 7309, 466, 9410, 11, 2201, 7309, 466, 264, 636, 264, 1461, 390, 3442, 281, 589, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07299781980968657, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.07290050387382507}, {"id": 180, "seek": 212300, "start": 2130.0, "end": 2139.0, "text": " And, you know, you can put sort of complicated guardrails with with effort you can sort of put all the guardrails around to make it never do the wrong thing.", "tokens": [50714, 400, 11, 291, 458, 11, 291, 393, 829, 1333, 295, 6179, 6290, 424, 4174, 365, 365, 4630, 291, 393, 1333, 295, 829, 439, 264, 6290, 424, 4174, 926, 281, 652, 309, 1128, 360, 264, 2085, 551, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07299781980968657, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.07290050387382507}, {"id": 181, "seek": 212300, "start": 2139.0, "end": 2152.0, "text": " But most people, most of the time they're rushing to get the program to just do the right thing. And so the, the kind of the error path is a safety net that we would like to automate as much as possible how that safety net works.", "tokens": [51164, 583, 881, 561, 11, 881, 295, 264, 565, 436, 434, 25876, 281, 483, 264, 1461, 281, 445, 360, 264, 558, 551, 13, 400, 370, 264, 11, 264, 733, 295, 264, 6713, 3100, 307, 257, 4514, 2533, 300, 321, 576, 411, 281, 31605, 382, 709, 382, 1944, 577, 300, 4514, 2533, 1985, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07299781980968657, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.07290050387382507}, {"id": 182, "seek": 215200, "start": 2152.0, "end": 2161.0, "text": " In other words, we'd like to be able to write to an interesting question whether whether LLMs could help do that is to write the kind of error checking code.", "tokens": [50364, 682, 661, 2283, 11, 321, 1116, 411, 281, 312, 1075, 281, 2464, 281, 364, 1880, 1168, 1968, 1968, 441, 43, 26386, 727, 854, 360, 300, 307, 281, 2464, 264, 733, 295, 6713, 8568, 3089, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10463552307664302, "compression_ratio": 1.7718631178707225, "no_speech_prob": 0.03436633199453354}, {"id": 183, "seek": 215200, "start": 2161.0, "end": 2168.0, "text": " But we've we've already built a bunch of things that sort of help automate the error checking process. Now you can't get it exactly right.", "tokens": [50814, 583, 321, 600, 321, 600, 1217, 3094, 257, 3840, 295, 721, 300, 1333, 295, 854, 31605, 264, 6713, 8568, 1399, 13, 823, 291, 393, 380, 483, 309, 2293, 558, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10463552307664302, "compression_ratio": 1.7718631178707225, "no_speech_prob": 0.03436633199453354}, {"id": 184, "seek": 215200, "start": 2168.0, "end": 2176.0, "text": " In other words, you're going to be it's going to be a heuristic thing like there's a path the programmer intended to follow. And there's ways you can fall off that path.", "tokens": [51164, 682, 661, 2283, 11, 291, 434, 516, 281, 312, 309, 311, 516, 281, 312, 257, 415, 374, 3142, 551, 411, 456, 311, 257, 3100, 264, 32116, 10226, 281, 1524, 13, 400, 456, 311, 2098, 291, 393, 2100, 766, 300, 3100, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10463552307664302, "compression_ratio": 1.7718631178707225, "no_speech_prob": 0.03436633199453354}, {"id": 185, "seek": 217600, "start": 2176.0, "end": 2189.0, "text": " You know, if the programmer had really done all the theorems so to speak to know how to keep on that path were well and good. But if the program is kind of lazy, and they're just like well I'm getting this path right but not thinking about all the other things.", "tokens": [50364, 509, 458, 11, 498, 264, 32116, 632, 534, 1096, 439, 264, 10299, 2592, 370, 281, 1710, 281, 458, 577, 281, 1066, 322, 300, 3100, 645, 731, 293, 665, 13, 583, 498, 264, 1461, 307, 733, 295, 14847, 11, 293, 436, 434, 445, 411, 731, 286, 478, 1242, 341, 3100, 558, 457, 406, 1953, 466, 439, 264, 661, 721, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12457327115333687, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.12351866811513901}, {"id": 186, "seek": 217600, "start": 2189.0, "end": 2202.0, "text": " You've got to kind of fill in the, the, the sort of the, the rail the guardrails for yourself. And that's an interesting problem of kind of the, I view it as the sort of automating the secondary path of the code.", "tokens": [51014, 509, 600, 658, 281, 733, 295, 2836, 294, 264, 11, 264, 11, 264, 1333, 295, 264, 11, 264, 8765, 264, 6290, 424, 4174, 337, 1803, 13, 400, 300, 311, 364, 1880, 1154, 295, 733, 295, 264, 11, 286, 1910, 309, 382, 264, 1333, 295, 3553, 990, 264, 11396, 3100, 295, 264, 3089, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12457327115333687, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.12351866811513901}, {"id": 187, "seek": 220200, "start": 2202.0, "end": 2217.0, "text": " The primary path was defined by the programmer question is can you automate what the secondary paths are, and that's something again one can expect to to see perhaps in a sort of IDE environment of the combined kind of computational", "tokens": [50364, 440, 6194, 3100, 390, 7642, 538, 264, 32116, 1168, 307, 393, 291, 31605, 437, 264, 11396, 14518, 366, 11, 293, 300, 311, 746, 797, 472, 393, 2066, 281, 281, 536, 4317, 294, 257, 1333, 295, 40930, 2823, 295, 264, 9354, 733, 295, 28270, 51114], "temperature": 0.0, "avg_logprob": -0.1358688029837101, "compression_ratio": 1.45, "no_speech_prob": 0.14413808286190033}, {"id": 188, "seek": 221700, "start": 2217.0, "end": 2235.0, "text": " language, LLM environment of writing programs, one can see kind of the, the way that, you know, well first generating the tests, then being able to, and then potentially you know you look at the tests you say oh I like the way that tests came out.", "tokens": [50364, 2856, 11, 441, 43, 44, 2823, 295, 3579, 4268, 11, 472, 393, 536, 733, 295, 264, 11, 264, 636, 300, 11, 291, 458, 11, 731, 700, 17746, 264, 6921, 11, 550, 885, 1075, 281, 11, 293, 550, 7263, 291, 458, 291, 574, 412, 264, 6921, 291, 584, 1954, 286, 411, 264, 636, 300, 6921, 1361, 484, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15721485263011495, "compression_ratio": 1.5732484076433122, "no_speech_prob": 0.689691424369812}, {"id": 189, "seek": 223500, "start": 2235.0, "end": 2249.0, "text": " Okay, then, then there's kind of like how do we make, or you say this is how it was, or maybe, maybe the LLM even generates, here are some possible paths that could have been followed, and it tries to bucket together some of the bad things that could happen.", "tokens": [50364, 1033, 11, 550, 11, 550, 456, 311, 733, 295, 411, 577, 360, 321, 652, 11, 420, 291, 584, 341, 307, 577, 309, 390, 11, 420, 1310, 11, 1310, 264, 441, 43, 44, 754, 23815, 11, 510, 366, 512, 1944, 14518, 300, 727, 362, 668, 6263, 11, 293, 309, 9898, 281, 13058, 1214, 512, 295, 264, 1578, 721, 300, 727, 1051, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10159157440725682, "compression_ratio": 1.752895752895753, "no_speech_prob": 0.6682931780815125}, {"id": 190, "seek": 223500, "start": 2249.0, "end": 2262.0, "text": " And it says what do you want to do in this case, you know, do you want to, if you get one numerical precision, you know failure to converge message, do you want to abort the rocket launch or not.", "tokens": [51064, 400, 309, 1619, 437, 360, 291, 528, 281, 360, 294, 341, 1389, 11, 291, 458, 11, 360, 291, 528, 281, 11, 498, 291, 483, 472, 29054, 18356, 11, 291, 458, 7763, 281, 41881, 3636, 11, 360, 291, 528, 281, 38117, 264, 13012, 4025, 420, 406, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10159157440725682, "compression_ratio": 1.752895752895753, "no_speech_prob": 0.6682931780815125}, {"id": 191, "seek": 226200, "start": 2263.0, "end": 2281.0, "text": " You know, and that's a human decision. No, no, it's not obvious what the answer is, it could be it's better to, you know, keep whatever it is if you're landing the, you know, the plane or whatever it is, it may be better just keep going and try and land the plane, even though you've got that error message, or it may be better to say,", "tokens": [50414, 509, 458, 11, 293, 300, 311, 257, 1952, 3537, 13, 883, 11, 572, 11, 309, 311, 406, 6322, 437, 264, 1867, 307, 11, 309, 727, 312, 309, 311, 1101, 281, 11, 291, 458, 11, 1066, 2035, 309, 307, 498, 291, 434, 11202, 264, 11, 291, 458, 11, 264, 5720, 420, 2035, 309, 307, 11, 309, 815, 312, 1101, 445, 1066, 516, 293, 853, 293, 2117, 264, 5720, 11, 754, 1673, 291, 600, 658, 300, 6713, 3636, 11, 420, 309, 815, 312, 1101, 281, 584, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1764181222808495, "compression_ratio": 1.7819148936170213, "no_speech_prob": 0.10665956139564514}, {"id": 192, "seek": 228100, "start": 2281.0, "end": 2296.0, "text": " you know, abort, you know, whatever, and, you know, go around before you try and land the plane or something. I mean, I think that that's the, so that that's a sort of a human choice but it's something where potentially the LLM, because it knows, it knows human pretty", "tokens": [50364, 291, 458, 11, 38117, 11, 291, 458, 11, 2035, 11, 293, 11, 291, 458, 11, 352, 926, 949, 291, 853, 293, 2117, 264, 5720, 420, 746, 13, 286, 914, 11, 286, 519, 300, 300, 311, 264, 11, 370, 300, 300, 311, 257, 1333, 295, 257, 1952, 3922, 457, 309, 311, 746, 689, 7263, 264, 441, 43, 44, 11, 570, 309, 3255, 11, 309, 3255, 1952, 1238, 51114], "temperature": 0.0, "avg_logprob": -0.1404193168462709, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.18624410033226013}, {"id": 193, "seek": 228100, "start": 2296.0, "end": 2302.0, "text": " well, could help in being able to make that kind of choice.", "tokens": [51114, 731, 11, 727, 854, 294, 885, 1075, 281, 652, 300, 733, 295, 3922, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1404193168462709, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.18624410033226013}, {"id": 194, "seek": 230200, "start": 2302.0, "end": 2317.0, "text": " That's very interesting. And I guess that's the, I mean, I could imagine just the whole like a integration test suite that uses Selenium browser and checks your website, like, I could imagine using something like that, totally.", "tokens": [50364, 663, 311, 588, 1880, 13, 400, 286, 2041, 300, 311, 264, 11, 286, 914, 11, 286, 727, 3811, 445, 264, 1379, 411, 257, 10980, 1500, 14205, 300, 4960, 10736, 268, 2197, 11185, 293, 13834, 428, 3144, 11, 411, 11, 286, 727, 3811, 1228, 746, 411, 300, 11, 3879, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12642155053480617, "compression_ratio": 1.474025974025974, "no_speech_prob": 0.3481503427028656}, {"id": 195, "seek": 231700, "start": 2317.0, "end": 2334.0, "text": " Yeah, I think the case of using LLM to build tests, to build unit tests, seems like, yeah, that, that'll exist. That seems pretty straightforward. You know, I've built a, I use GPT for to make a new element on my page and then I just feed back in the JavaScript errors back into it and", "tokens": [50364, 865, 11, 286, 519, 264, 1389, 295, 1228, 441, 43, 44, 281, 1322, 6921, 11, 281, 1322, 4985, 6921, 11, 2544, 411, 11, 1338, 11, 300, 11, 300, 603, 2514, 13, 663, 2544, 1238, 15325, 13, 509, 458, 11, 286, 600, 3094, 257, 11, 286, 764, 26039, 51, 337, 281, 652, 257, 777, 4478, 322, 452, 3028, 293, 550, 286, 445, 3154, 646, 294, 264, 15778, 13603, 646, 666, 309, 293, 51214], "temperature": 0.0, "avg_logprob": -0.1948132578531901, "compression_ratio": 1.4540816326530612, "no_speech_prob": 0.29357537627220154}, {"id": 196, "seek": 233400, "start": 2334.0, "end": 2351.0, "text": " bottoming really quick, it fixes it. But the, I think the even more interesting thing here is kind of asking, maybe asking GPT or even training a more specific or fine tuning more specifically a classifier to say,", "tokens": [50364, 2767, 278, 534, 1702, 11, 309, 32539, 309, 13, 583, 264, 11, 286, 519, 264, 754, 544, 1880, 551, 510, 307, 733, 295, 3365, 11, 1310, 3365, 26039, 51, 420, 754, 3097, 257, 544, 2685, 420, 2489, 15164, 544, 4682, 257, 1508, 9902, 281, 584, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1719687080383301, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.5878799557685852}, {"id": 197, "seek": 235100, "start": 2351.0, "end": 2366.0, "text": " is this output a good output and good bad output is very like, who knows what that means it's very case by case people talk about hallucinations but at the end of the day is the user getting something bad and can I've seen.", "tokens": [50364, 307, 341, 5598, 257, 665, 5598, 293, 665, 1578, 5598, 307, 588, 411, 11, 567, 3255, 437, 300, 1355, 309, 311, 588, 1389, 538, 1389, 561, 751, 466, 35212, 10325, 457, 412, 264, 917, 295, 264, 786, 307, 264, 4195, 1242, 746, 1578, 293, 393, 286, 600, 1612, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1135356594817807, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.7437477111816406}, {"id": 198, "seek": 235100, "start": 2366.0, "end": 2380.0, "text": " I've seen I spoke to one team that has a chatbot and they basically feed the results back into GPT and say, if you were the user, how good of experience would you rate this and I think that", "tokens": [51114, 286, 600, 1612, 286, 7179, 281, 472, 1469, 300, 575, 257, 5081, 18870, 293, 436, 1936, 3154, 264, 3542, 646, 666, 26039, 51, 293, 584, 11, 498, 291, 645, 264, 4195, 11, 577, 665, 295, 1752, 576, 291, 3314, 341, 293, 286, 519, 300, 51814], "temperature": 0.0, "avg_logprob": -0.1135356594817807, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.7437477111816406}, {"id": 199, "seek": 238000, "start": 2380.0, "end": 2395.0, "text": " essentially what was what was done in the, you know, in the final reinforcement learning steps of training chat GPT was essentially that kind of process that they've been human ratings done, and then those human ratings were used to train a classifier", "tokens": [50364, 4476, 437, 390, 437, 390, 1096, 294, 264, 11, 291, 458, 11, 294, 264, 2572, 29280, 2539, 4439, 295, 3097, 5081, 26039, 51, 390, 4476, 300, 733, 295, 1399, 300, 436, 600, 668, 1952, 24603, 1096, 11, 293, 550, 729, 1952, 24603, 645, 1143, 281, 3847, 257, 1508, 9902, 51114], "temperature": 0.0, "avg_logprob": -0.16245032256504274, "compression_ratio": 1.5987261146496816, "no_speech_prob": 0.10368999093770981}, {"id": 200, "seek": 239500, "start": 2395.0, "end": 2410.0, "text": " and then the classifier was run automatically against the actual LLM. And that's a, you know, that's a clearly an important kind of thing to do. I think, in the case of", "tokens": [50364, 293, 550, 264, 1508, 9902, 390, 1190, 6772, 1970, 264, 3539, 441, 43, 44, 13, 400, 300, 311, 257, 11, 291, 458, 11, 300, 311, 257, 4448, 364, 1021, 733, 295, 551, 281, 360, 13, 286, 519, 11, 294, 264, 1389, 295, 51114], "temperature": 0.0, "avg_logprob": -0.10604062287703804, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.8587128520011902}, {"id": 201, "seek": 241000, "start": 2410.0, "end": 2425.0, "text": " it's an interesting question when you're dealing with code, and when you're dealing with the sort of what could possibly go wrong with this code, how do you follow the paths, and then it becomes sort of a merger of more like, you know compiler thinking so to speak of what, what can you say about", "tokens": [50364, 309, 311, 364, 1880, 1168, 562, 291, 434, 6260, 365, 3089, 11, 293, 562, 291, 434, 6260, 365, 264, 1333, 295, 437, 727, 6264, 352, 2085, 365, 341, 3089, 11, 577, 360, 291, 1524, 264, 14518, 11, 293, 550, 309, 3643, 1333, 295, 257, 48002, 295, 544, 411, 11, 291, 458, 31958, 1953, 370, 281, 1710, 295, 437, 11, 437, 393, 291, 584, 466, 51114], "temperature": 0.0, "avg_logprob": -0.13635699889239142, "compression_ratio": 1.7411764705882353, "no_speech_prob": 0.5724078416824341}, {"id": 202, "seek": 242500, "start": 2425.0, "end": 2439.0, "text": " these code paths, what can you prove about the code paths. And then, you know, in this path, which the sort of compiler like thinking could say there is this path. And then you can ask the LLM, if you follow that path, are you going to be happy.", "tokens": [50364, 613, 3089, 14518, 11, 437, 393, 291, 7081, 466, 264, 3089, 14518, 13, 400, 550, 11, 291, 458, 11, 294, 341, 3100, 11, 597, 264, 1333, 295, 31958, 411, 1953, 727, 584, 456, 307, 341, 3100, 13, 400, 550, 291, 393, 1029, 264, 441, 43, 44, 11, 498, 291, 1524, 300, 3100, 11, 366, 291, 516, 281, 312, 2055, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10221721231937408, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.756687343120575}, {"id": 203, "seek": 243900, "start": 2439.0, "end": 2457.0, "text": " And that that's, you know, I think that's a, but I do think that this idea of, you know, people normally, this idea of, sort of, can the LLM estimate when the user is going to be happy.", "tokens": [50364, 400, 300, 300, 311, 11, 291, 458, 11, 286, 519, 300, 311, 257, 11, 457, 286, 360, 519, 300, 341, 1558, 295, 11, 291, 458, 11, 561, 5646, 11, 341, 1558, 295, 11, 1333, 295, 11, 393, 264, 441, 43, 44, 12539, 562, 264, 4195, 307, 516, 281, 312, 2055, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10750042308460582, "compression_ratio": 1.4919354838709677, "no_speech_prob": 0.4112183153629303}, {"id": 204, "seek": 245700, "start": 2458.0, "end": 2469.0, "text": " That's probably something, and that that's part of what we've been thinking about not in the context of LLM so much in the secondary pathway for error, for error handling and programs.", "tokens": [50414, 663, 311, 1391, 746, 11, 293, 300, 300, 311, 644, 295, 437, 321, 600, 668, 1953, 466, 406, 294, 264, 4319, 295, 441, 43, 44, 370, 709, 294, 264, 11396, 18590, 337, 6713, 11, 337, 6713, 13175, 293, 4268, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09885429990464363, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.48490995168685913}, {"id": 205, "seek": 245700, "start": 2469.0, "end": 2475.0, "text": " It's like, what can you see about the heuristically about what is likely to make a user happy.", "tokens": [50964, 467, 311, 411, 11, 437, 393, 291, 536, 466, 264, 415, 374, 20458, 466, 437, 307, 3700, 281, 652, 257, 4195, 2055, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09885429990464363, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.48490995168685913}, {"id": 206, "seek": 247500, "start": 2475.0, "end": 2488.0, "text": " And even though we don't know for sure, because it could be that the user, you know, the user is doing some science program. And this one bizarre case is like the amazing, you know, we just discovered this amazing phenomenon.", "tokens": [50364, 400, 754, 1673, 321, 500, 380, 458, 337, 988, 11, 570, 309, 727, 312, 300, 264, 4195, 11, 291, 458, 11, 264, 4195, 307, 884, 512, 3497, 1461, 13, 400, 341, 472, 18265, 1389, 307, 411, 264, 2243, 11, 291, 458, 11, 321, 445, 6941, 341, 2243, 14029, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11697073936462403, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.746169924736023}, {"id": 207, "seek": 247500, "start": 2488.0, "end": 2499.0, "text": " And oh, the error handler basically says, well, that's not a thing that really happens much that will be an anomaly. Let's go and catch that in the error handler. And then the person never sees that.", "tokens": [51014, 400, 1954, 11, 264, 6713, 41967, 1936, 1619, 11, 731, 11, 300, 311, 406, 257, 551, 300, 534, 2314, 709, 300, 486, 312, 364, 42737, 13, 961, 311, 352, 293, 3745, 300, 294, 264, 6713, 41967, 13, 400, 550, 264, 954, 1128, 8194, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11697073936462403, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.746169924736023}, {"id": 208, "seek": 249900, "start": 2499.0, "end": 2513.0, "text": " The error handler can't really expect the truly unexpected, but it can kind of deal with the slightly unexpected, so to speak, the things not foreseen by the programmer, but sort of expected on the basis of general intuition about programming.", "tokens": [50364, 440, 6713, 41967, 393, 380, 534, 2066, 264, 4908, 13106, 11, 457, 309, 393, 733, 295, 2028, 365, 264, 4748, 13106, 11, 370, 281, 1710, 11, 264, 721, 406, 2091, 22008, 538, 264, 32116, 11, 457, 1333, 295, 5176, 322, 264, 5143, 295, 2674, 24002, 466, 9410, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11057399881297145, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.04840504005551338}, {"id": 209, "seek": 249900, "start": 2513.0, "end": 2528.0, "text": " Yeah, it sounds like I know you write about this a lot this computational irreducibility and I think that's probably like the best way to think about like, is this, is it the halting problem to say can LLM tell if it's correct or not because is that nested.", "tokens": [51064, 865, 11, 309, 3263, 411, 286, 458, 291, 2464, 466, 341, 257, 688, 341, 28270, 16014, 769, 537, 39802, 293, 286, 519, 300, 311, 1391, 411, 264, 1151, 636, 281, 519, 466, 411, 11, 307, 341, 11, 307, 309, 264, 7523, 783, 1154, 281, 584, 393, 441, 43, 44, 980, 498, 309, 311, 3006, 420, 406, 570, 307, 300, 15646, 292, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11057399881297145, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.04840504005551338}, {"id": 210, "seek": 252800, "start": 2528.0, "end": 2530.0, "text": " How does that work?", "tokens": [50364, 1012, 775, 300, 589, 30, 50464], "temperature": 0.0, "avg_logprob": -0.13355146808388793, "compression_ratio": 1.6352941176470588, "no_speech_prob": 0.054792169481515884}, {"id": 211, "seek": 252800, "start": 2530.0, "end": 2552.0, "text": " I think that, I mean, in the end, you know, one of the issues with the kind of LLMs, like turtles all the way down, so to speak, is that kind of, you know, you say, did it do something that I thought was good, like, you know, ethically good or whatever else.", "tokens": [50464, 286, 519, 300, 11, 286, 914, 11, 294, 264, 917, 11, 291, 458, 11, 472, 295, 264, 2663, 365, 264, 733, 295, 441, 43, 26386, 11, 411, 32422, 439, 264, 636, 760, 11, 370, 281, 1710, 11, 307, 300, 733, 295, 11, 291, 458, 11, 291, 584, 11, 630, 309, 360, 746, 300, 286, 1194, 390, 665, 11, 411, 11, 291, 458, 11, 6468, 984, 665, 420, 2035, 1646, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13355146808388793, "compression_ratio": 1.6352941176470588, "no_speech_prob": 0.054792169481515884}, {"id": 212, "seek": 255200, "start": 2552.0, "end": 2561.0, "text": " Well, there's no ultimate, you know, you can say, well, how does it compare to what people have written about and, you know, in the in history and literature and so on.", "tokens": [50364, 1042, 11, 456, 311, 572, 9705, 11, 291, 458, 11, 291, 393, 584, 11, 731, 11, 577, 775, 309, 6794, 281, 437, 561, 362, 3720, 466, 293, 11, 291, 458, 11, 294, 264, 294, 2503, 293, 10394, 293, 370, 322, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10743260774456087, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.33877184987068176}, {"id": 213, "seek": 255200, "start": 2561.0, "end": 2572.0, "text": " And but there's no, there's no sort of ultimate ground truth to that it's well, you know, do, do the humans who are sort of making the decision for how the LLM should work.", "tokens": [50814, 400, 457, 456, 311, 572, 11, 456, 311, 572, 1333, 295, 9705, 2727, 3494, 281, 300, 309, 311, 731, 11, 291, 458, 11, 360, 11, 360, 264, 6255, 567, 366, 1333, 295, 1455, 264, 3537, 337, 577, 264, 441, 43, 44, 820, 589, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10743260774456087, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.33877184987068176}, {"id": 214, "seek": 255200, "start": 2572.0, "end": 2577.0, "text": " Are they, do they think it's good, do they not think it's good, you've got to have some kind of grounding there.", "tokens": [51364, 2014, 436, 11, 360, 436, 519, 309, 311, 665, 11, 360, 436, 406, 519, 309, 311, 665, 11, 291, 600, 658, 281, 362, 512, 733, 295, 46727, 456, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10743260774456087, "compression_ratio": 1.8455284552845528, "no_speech_prob": 0.33877184987068176}, {"id": 215, "seek": 257700, "start": 2577.0, "end": 2585.0, "text": " And I think that's the, you know, you've got to have some set of principles that you say I'm going to follow these principles.", "tokens": [50364, 400, 286, 519, 300, 311, 264, 11, 291, 458, 11, 291, 600, 658, 281, 362, 512, 992, 295, 9156, 300, 291, 584, 286, 478, 516, 281, 1524, 613, 9156, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07483650006745991, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.2667824923992157}, {"id": 216, "seek": 257700, "start": 2585.0, "end": 2593.0, "text": " Now, computational irreducibility has the consequence that when you think you've written down the principles that determine what will be good and what will be bad for the LLM.", "tokens": [50764, 823, 11, 28270, 16014, 769, 537, 39802, 575, 264, 18326, 300, 562, 291, 519, 291, 600, 3720, 760, 264, 9156, 300, 6997, 437, 486, 312, 665, 293, 437, 486, 312, 1578, 337, 264, 441, 43, 44, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07483650006745991, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.2667824923992157}, {"id": 217, "seek": 257700, "start": 2593.0, "end": 2598.0, "text": " The LLM will always come up with a weird case that's not covered by your principles.", "tokens": [51164, 440, 441, 43, 44, 486, 1009, 808, 493, 365, 257, 3657, 1389, 300, 311, 406, 5343, 538, 428, 9156, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07483650006745991, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.2667824923992157}, {"id": 218, "seek": 259800, "start": 2598.0, "end": 2609.0, "text": " That, you know, that's kind of the pattern of how that's happened is like with human laws. People say, oh, we've got this, you know, this legal code, and that's going to determine how everything works.", "tokens": [50364, 663, 11, 291, 458, 11, 300, 311, 733, 295, 264, 5102, 295, 577, 300, 311, 2011, 307, 411, 365, 1952, 6064, 13, 3432, 584, 11, 1954, 11, 321, 600, 658, 341, 11, 291, 458, 11, 341, 5089, 3089, 11, 293, 300, 311, 516, 281, 6997, 577, 1203, 1985, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12091969941791735, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.3806573748588562}, {"id": 219, "seek": 259800, "start": 2609.0, "end": 2618.0, "text": " And then along comes an AI, for example, that nobody imagined, you know, when the US Constitution was written, for example, you know, people didn't imagine there would be AI's.", "tokens": [50914, 400, 550, 2051, 1487, 364, 7318, 11, 337, 1365, 11, 300, 5079, 16590, 11, 291, 458, 11, 562, 264, 2546, 14505, 390, 3720, 11, 337, 1365, 11, 291, 458, 11, 561, 994, 380, 3811, 456, 576, 312, 7318, 311, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12091969941791735, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.3806573748588562}, {"id": 220, "seek": 261800, "start": 2618.0, "end": 2629.0, "text": " And so then, well, what do you do because, you know, you can follow this legal code, but it doesn't say anything about what to do when when the AI is responsible for doing this that or the other.", "tokens": [50364, 400, 370, 550, 11, 731, 11, 437, 360, 291, 360, 570, 11, 291, 458, 11, 291, 393, 1524, 341, 5089, 3089, 11, 457, 309, 1177, 380, 584, 1340, 466, 437, 281, 360, 562, 562, 264, 7318, 307, 6250, 337, 884, 341, 300, 420, 264, 661, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06381250249928441, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.32308751344680786}, {"id": 221, "seek": 261800, "start": 2629.0, "end": 2638.0, "text": " And so, you know, you kind of have to patch it and it's the same thing with any of these sort of sets of principles about how an AI should work.", "tokens": [50914, 400, 370, 11, 291, 458, 11, 291, 733, 295, 362, 281, 9972, 309, 293, 309, 311, 264, 912, 551, 365, 604, 295, 613, 1333, 295, 6352, 295, 9156, 466, 577, 364, 7318, 820, 589, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06381250249928441, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.32308751344680786}, {"id": 222, "seek": 263800, "start": 2638.0, "end": 2657.0, "text": " And so I guess how would you square this with, I mean, this whole concept that the LLM looked at so much data and kind of discovered these speech patterns and kind of built a model around how to talk and built a model around just like,", "tokens": [50364, 400, 370, 286, 2041, 577, 576, 291, 3732, 341, 365, 11, 286, 914, 11, 341, 1379, 3410, 300, 264, 441, 43, 44, 2956, 412, 370, 709, 1412, 293, 733, 295, 6941, 613, 6218, 8294, 293, 733, 295, 3094, 257, 2316, 926, 577, 281, 751, 293, 3094, 257, 2316, 926, 445, 411, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1260484286717006, "compression_ratio": 1.5460526315789473, "no_speech_prob": 0.1730739027261734}, {"id": 223, "seek": 265700, "start": 2657.0, "end": 2666.0, "text": " but let's say my mental model is they've kind of solved natural language right and you can argue back and forth of course but that's that's the mental model there.", "tokens": [50364, 457, 718, 311, 584, 452, 4973, 2316, 307, 436, 600, 733, 295, 13041, 3303, 2856, 558, 293, 291, 393, 9695, 646, 293, 5220, 295, 1164, 457, 300, 311, 300, 311, 264, 4973, 2316, 456, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11730669721772399, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.3021286427974701}, {"id": 224, "seek": 265700, "start": 2666.0, "end": 2682.0, "text": " What, what is the limitation of what else can be discovered through these patterns like it are these moral principles maybe are these reducible to a certain patterns of like human psyche or biology maybe.", "tokens": [50814, 708, 11, 437, 307, 264, 27432, 295, 437, 1646, 393, 312, 6941, 807, 613, 8294, 411, 309, 366, 613, 9723, 9156, 1310, 366, 613, 2783, 32128, 281, 257, 1629, 8294, 295, 411, 1952, 50223, 420, 14956, 1310, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11730669721772399, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.3021286427974701}, {"id": 225, "seek": 268200, "start": 2682.0, "end": 2698.0, "text": " Interesting question. I mean, I think that the, the fact that neural nets can do human like things they can make human like decisions about images they can make produce, they can generalize from their training data in a human like way with language.", "tokens": [50364, 14711, 1168, 13, 286, 914, 11, 286, 519, 300, 264, 11, 264, 1186, 300, 18161, 36170, 393, 360, 1952, 411, 721, 436, 393, 652, 1952, 411, 5327, 466, 5267, 436, 393, 652, 5258, 11, 436, 393, 2674, 1125, 490, 641, 3097, 1412, 294, 257, 1952, 411, 636, 365, 2856, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12862763581452547, "compression_ratio": 1.6490066225165563, "no_speech_prob": 0.6417616605758667}, {"id": 226, "seek": 269800, "start": 2698.0, "end": 2708.0, "text": " Probably that's happening because neural nets are architecturally pretty similar to how brains work. And so they're generalizing the same way. Now, interesting question, which I've certainly thought about.", "tokens": [50364, 9210, 300, 311, 2737, 570, 18161, 36170, 366, 6331, 6512, 1238, 2531, 281, 577, 15442, 589, 13, 400, 370, 436, 434, 2674, 3319, 264, 912, 636, 13, 823, 11, 1880, 1168, 11, 597, 286, 600, 3297, 1194, 466, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10484110844599737, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.3319898247718811}, {"id": 227, "seek": 269800, "start": 2708.0, "end": 2717.0, "text": " And is this question of are there kind of a set of, for example, you know, moral principles that you can similarly say this is the construction kit.", "tokens": [50864, 400, 307, 341, 1168, 295, 366, 456, 733, 295, 257, 992, 295, 11, 337, 1365, 11, 291, 458, 11, 9723, 9156, 300, 291, 393, 14138, 584, 341, 307, 264, 6435, 8260, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10484110844599737, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.3319898247718811}, {"id": 228, "seek": 271700, "start": 2717.0, "end": 2733.0, "text": " And this is the set of primitives from which you can operate and I know I've talked to people over the course of years about this kind of thing and people occasionally say you should read this philosophy book you can you know this person in cognitive science has done this and I've got a pile of these books.", "tokens": [50364, 400, 341, 307, 264, 992, 295, 2886, 38970, 490, 597, 291, 393, 9651, 293, 286, 458, 286, 600, 2825, 281, 561, 670, 264, 1164, 295, 924, 466, 341, 733, 295, 551, 293, 561, 16895, 584, 291, 820, 1401, 341, 10675, 1446, 291, 393, 291, 458, 341, 954, 294, 15605, 3497, 575, 1096, 341, 293, 286, 600, 658, 257, 14375, 295, 613, 3642, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09254935004494406, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.6385853886604309}, {"id": 229, "seek": 271700, "start": 2733.0, "end": 2743.0, "text": " And I have to admit that I haven't really gotten through them. But there certainly are efforts that people have made to sort of try to identify the print the primitives of moral thinking so to speak.", "tokens": [51164, 400, 286, 362, 281, 9796, 300, 286, 2378, 380, 534, 5768, 807, 552, 13, 583, 456, 3297, 366, 6484, 300, 561, 362, 1027, 281, 1333, 295, 853, 281, 5876, 264, 4482, 264, 2886, 38970, 295, 9723, 1953, 370, 281, 1710, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09254935004494406, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.6385853886604309}, {"id": 230, "seek": 274300, "start": 2743.0, "end": 2756.0, "text": " And, you know, if you decide this I mean you could have different ways of thinking about ethics and different. But you know, are there is there a construction kit you get this and this and this and these fit together in that way.", "tokens": [50364, 400, 11, 291, 458, 11, 498, 291, 4536, 341, 286, 914, 291, 727, 362, 819, 2098, 295, 1953, 466, 19769, 293, 819, 13, 583, 291, 458, 11, 366, 456, 307, 456, 257, 6435, 8260, 291, 483, 341, 293, 341, 293, 341, 293, 613, 3318, 1214, 294, 300, 636, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09775905749377083, "compression_ratio": 1.6836158192090396, "no_speech_prob": 0.024742964655160904}, {"id": 231, "seek": 274300, "start": 2756.0, "end": 2760.0, "text": " And is that a reasonable model of how humans make ethical decisions.", "tokens": [51014, 400, 307, 300, 257, 10585, 2316, 295, 577, 6255, 652, 18890, 5327, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09775905749377083, "compression_ratio": 1.6836158192090396, "no_speech_prob": 0.024742964655160904}, {"id": 232, "seek": 276000, "start": 2760.0, "end": 2775.0, "text": " And it's quite possible yes and it's quite possible that even from all the texts that that you know chat to PT has read that it's learned to pretty good model of how humans make certain kinds of ethical decisions, or at least how people write that they've that they've made certain", "tokens": [50364, 400, 309, 311, 1596, 1944, 2086, 293, 309, 311, 1596, 1944, 300, 754, 490, 439, 264, 15765, 300, 300, 291, 458, 5081, 281, 35460, 575, 1401, 300, 309, 311, 3264, 281, 1238, 665, 2316, 295, 577, 6255, 652, 1629, 3685, 295, 18890, 5327, 11, 420, 412, 1935, 577, 561, 2464, 300, 436, 600, 300, 436, 600, 1027, 1629, 51114], "temperature": 0.0, "avg_logprob": -0.23809259168563351, "compression_ratio": 1.6432748538011697, "no_speech_prob": 0.8273789286613464}, {"id": 233, "seek": 277500, "start": 2775.0, "end": 2784.0, "text": " I mean, this is of course the big conundrum of this stuff is that, you know, people say well what should the how should the ai's make ethical decisions well you say, just copy what the humans do.", "tokens": [50364, 286, 914, 11, 341, 307, 295, 1164, 264, 955, 416, 997, 6247, 295, 341, 1507, 307, 300, 11, 291, 458, 11, 561, 584, 731, 437, 820, 264, 577, 820, 264, 9783, 311, 652, 18890, 5327, 731, 291, 584, 11, 445, 5055, 437, 264, 6255, 360, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14478784196832206, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.6667648553848267}, {"id": 234, "seek": 277500, "start": 2784.0, "end": 2793.0, "text": " Then people say, no no no that's not the right thing to do humans do all kinds of wrong things. You know, it shouldn't be just do as the humans do.", "tokens": [50814, 1396, 561, 584, 11, 572, 572, 572, 300, 311, 406, 264, 558, 551, 281, 360, 6255, 360, 439, 3685, 295, 2085, 721, 13, 509, 458, 11, 309, 4659, 380, 312, 445, 360, 382, 264, 6255, 360, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14478784196832206, "compression_ratio": 1.7323232323232323, "no_speech_prob": 0.6667648553848267}, {"id": 235, "seek": 279300, "start": 2793.0, "end": 2808.0, "text": " What should be do as the humans aspire to do. And then it gets much more complicated, because what the humans aspire to do may not be realistic. It may be you know different people will disagree about what the aspirations are, etc etc etc.", "tokens": [50364, 708, 820, 312, 360, 382, 264, 6255, 41224, 281, 360, 13, 400, 550, 309, 2170, 709, 544, 6179, 11, 570, 437, 264, 6255, 41224, 281, 360, 815, 406, 312, 12465, 13, 467, 815, 312, 291, 458, 819, 561, 486, 14091, 466, 437, 264, 32458, 366, 11, 5183, 5183, 5183, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1319802513828984, "compression_ratio": 1.551948051948052, "no_speech_prob": 0.8713882565498352}, {"id": 236, "seek": 280800, "start": 2809.0, "end": 2822.0, "text": " I mean, it's a very, it's a, I think it is a an important challenge for our times and I, you know, I mentioned this over the last few years and I, I occasionally have mentioned it in in groups where they sort of are supposed to be in the business of figuring out stuff like this like", "tokens": [50414, 286, 914, 11, 309, 311, 257, 588, 11, 309, 311, 257, 11, 286, 519, 309, 307, 257, 364, 1021, 3430, 337, 527, 1413, 293, 286, 11, 291, 458, 11, 286, 2835, 341, 670, 264, 1036, 1326, 924, 293, 286, 11, 286, 16895, 362, 2835, 309, 294, 294, 3935, 689, 436, 1333, 295, 366, 3442, 281, 312, 294, 264, 1606, 295, 15213, 484, 1507, 411, 341, 411, 51064], "temperature": 0.0, "avg_logprob": -0.14174961362566266, "compression_ratio": 1.5898876404494382, "no_speech_prob": 0.7799019813537598}, {"id": 237, "seek": 282200, "start": 2823.0, "end": 2843.0, "text": " of saying, okay, you know, imagine you're writing the Constitution today in the post ai age. What does it say, what should it say, what are the, what are the principles, you know, what are the truths that we now hold to be self evidence, so to speak, or whatever, in, you know, in the ai age.", "tokens": [50414, 295, 1566, 11, 1392, 11, 291, 458, 11, 3811, 291, 434, 3579, 264, 14505, 965, 294, 264, 2183, 9783, 3205, 13, 708, 775, 309, 584, 11, 437, 820, 309, 584, 11, 437, 366, 264, 11, 437, 366, 264, 9156, 11, 291, 458, 11, 437, 366, 264, 30079, 300, 321, 586, 1797, 281, 312, 2698, 4467, 11, 370, 281, 1710, 11, 420, 2035, 11, 294, 11, 291, 458, 11, 294, 264, 9783, 3205, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20127199841784194, "compression_ratio": 1.727810650887574, "no_speech_prob": 0.6989879608154297}, {"id": 238, "seek": 284300, "start": 2843.0, "end": 2856.0, "text": " And I think, you know, I'm not sure I know what the answer is. I mean, in, you know, at what point, for example, very basic ethical question is when should an AI have rights.", "tokens": [50364, 400, 286, 519, 11, 291, 458, 11, 286, 478, 406, 988, 286, 458, 437, 264, 1867, 307, 13, 286, 914, 11, 294, 11, 291, 458, 11, 412, 437, 935, 11, 337, 1365, 11, 588, 3875, 18890, 1168, 307, 562, 820, 364, 7318, 362, 4601, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09096977662067024, "compression_ratio": 1.3384615384615384, "no_speech_prob": 0.33650511503219604}, {"id": 239, "seek": 285600, "start": 2856.0, "end": 2874.0, "text": " And, you know, I was a number of years ago I was at some AI ethics conference and this, I raised this question, and some very bouncy philosopher who I've gotten to know better actually and she said, we should do that when the AI is a conscious.", "tokens": [50364, 400, 11, 291, 458, 11, 286, 390, 257, 1230, 295, 924, 2057, 286, 390, 412, 512, 7318, 19769, 7586, 293, 341, 11, 286, 6005, 341, 1168, 11, 293, 512, 588, 49704, 29805, 567, 286, 600, 5768, 281, 458, 1101, 767, 293, 750, 848, 11, 321, 820, 360, 300, 562, 264, 7318, 307, 257, 6648, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18507456375380693, "compression_ratio": 1.4787878787878788, "no_speech_prob": 0.4408981502056122}, {"id": 240, "seek": 287400, "start": 2874.0, "end": 2880.0, "text": " That's not very helpful, because this question just loops right back on itself.", "tokens": [50364, 663, 311, 406, 588, 4961, 11, 570, 341, 1168, 445, 16121, 558, 646, 322, 2564, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16741730083118786, "compression_ratio": 1.396341463414634, "no_speech_prob": 0.5993809700012207}, {"id": 241, "seek": 287400, "start": 2880.0, "end": 2891.0, "text": " But, but, you know, something I realized of you rather recently is, let's say you have this autonomous bot hanging out, and it's entertaining people.", "tokens": [50664, 583, 11, 457, 11, 291, 458, 11, 746, 286, 5334, 295, 291, 2831, 3938, 307, 11, 718, 311, 584, 291, 362, 341, 23797, 10592, 8345, 484, 11, 293, 309, 311, 20402, 561, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16741730083118786, "compression_ratio": 1.396341463414634, "no_speech_prob": 0.5993809700012207}, {"id": 242, "seek": 289100, "start": 2891.0, "end": 2903.0, "text": " And it makes all kinds of friends, and it makes a living for itself. It has a Patreon, it's, you know, it's kind of, it's paying its hosting fees through people donating to it and so on.", "tokens": [50364, 400, 309, 1669, 439, 3685, 295, 1855, 11, 293, 309, 1669, 257, 2647, 337, 2564, 13, 467, 575, 257, 15692, 11, 309, 311, 11, 291, 458, 11, 309, 311, 733, 295, 11, 309, 311, 6229, 1080, 16058, 13370, 807, 561, 36686, 281, 309, 293, 370, 322, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13285019523219058, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.7878856062889099}, {"id": 243, "seek": 289100, "start": 2903.0, "end": 2913.0, "text": " It's a, it's a happy autonomous creature. And maybe it even was created through some bizarre legal construct of, you know, some loop of LLCs where there's no owner to it or something.", "tokens": [50964, 467, 311, 257, 11, 309, 311, 257, 2055, 23797, 12797, 13, 400, 1310, 309, 754, 390, 2942, 807, 512, 18265, 5089, 7690, 295, 11, 291, 458, 11, 512, 6367, 295, 33698, 82, 689, 456, 311, 572, 7289, 281, 309, 420, 746, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13285019523219058, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.7878856062889099}, {"id": 244, "seek": 291300, "start": 2913.0, "end": 2927.0, "text": " Okay, so it's hanging out and doesn't have an owner, and it's making a living for itself. And somebody says, oh, it's starting to be, you know, be mean to people, it should be shut down.", "tokens": [50364, 1033, 11, 370, 309, 311, 8345, 484, 293, 1177, 380, 362, 364, 7289, 11, 293, 309, 311, 1455, 257, 2647, 337, 2564, 13, 400, 2618, 1619, 11, 1954, 11, 309, 311, 2891, 281, 312, 11, 291, 458, 11, 312, 914, 281, 561, 11, 309, 820, 312, 5309, 760, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08161616325378418, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.3811054229736328}, {"id": 245, "seek": 291300, "start": 2927.0, "end": 2941.0, "text": " Okay, how do you decide to do that? And what is the kind of ethics of doing that? Well, one of the things that's obviously the case is that that bot may have made lots of friends, human friends, and you shut the bot down.", "tokens": [51064, 1033, 11, 577, 360, 291, 4536, 281, 360, 300, 30, 400, 437, 307, 264, 733, 295, 19769, 295, 884, 300, 30, 1042, 11, 472, 295, 264, 721, 300, 311, 2745, 264, 1389, 307, 300, 300, 10592, 815, 362, 1027, 3195, 295, 1855, 11, 1952, 1855, 11, 293, 291, 5309, 264, 10592, 760, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08161616325378418, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.3811054229736328}, {"id": 246, "seek": 294100, "start": 2941.0, "end": 2951.0, "text": " And those human friends can be very unhappy. And so, you know, when you think you're just dealing with a bot, you've actually, you know, it's connected itself to the human world in a certain way.", "tokens": [50364, 400, 729, 1952, 1855, 393, 312, 588, 22172, 13, 400, 370, 11, 291, 458, 11, 562, 291, 519, 291, 434, 445, 6260, 365, 257, 10592, 11, 291, 600, 767, 11, 291, 458, 11, 309, 311, 4582, 2564, 281, 264, 1952, 1002, 294, 257, 1629, 636, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07171829223632813, "compression_ratio": 1.3928571428571428, "no_speech_prob": 0.17040188610553741}, {"id": 247, "seek": 295100, "start": 2951.0, "end": 2968.0, "text": " I mean, it starts to get kind of complicated to know what to sort of what what the right thing to do is. And I think that's a, you know, that's a, that's an interesting challenge for our times that I say, I've, I've thought about it a bit and I, I figure I'm, you know, if it falls to me to have to figure", "tokens": [50364, 286, 914, 11, 309, 3719, 281, 483, 733, 295, 6179, 281, 458, 437, 281, 1333, 295, 437, 437, 264, 558, 551, 281, 360, 307, 13, 400, 286, 519, 300, 311, 257, 11, 291, 458, 11, 300, 311, 257, 11, 300, 311, 364, 1880, 3430, 337, 527, 1413, 300, 286, 584, 11, 286, 600, 11, 286, 600, 1194, 466, 309, 257, 857, 293, 286, 11, 286, 2573, 286, 478, 11, 291, 458, 11, 498, 309, 8804, 281, 385, 281, 362, 281, 2573, 51214], "temperature": 0.0, "avg_logprob": -0.14974928463206572, "compression_ratio": 1.6758241758241759, "no_speech_prob": 0.9032055139541626}, {"id": 248, "seek": 296800, "start": 2968.0, "end": 2986.0, "text": " it out, this is a shocking thing because it's not my, you know, it's not the kind of thing I, I usually think about but you know, one could think about all kinds of things but it's it's it's something where even to get kind of a, you know, one of the things in figuring", "tokens": [50364, 309, 484, 11, 341, 307, 257, 18776, 551, 570, 309, 311, 406, 452, 11, 291, 458, 11, 309, 311, 406, 264, 733, 295, 551, 286, 11, 286, 2673, 519, 466, 457, 291, 458, 11, 472, 727, 519, 466, 439, 3685, 295, 721, 457, 309, 311, 309, 311, 309, 311, 746, 689, 754, 281, 483, 733, 295, 257, 11, 291, 458, 11, 472, 295, 264, 721, 294, 15213, 51264], "temperature": 0.0, "avg_logprob": -0.13212614999690525, "compression_ratio": 1.8299319727891157, "no_speech_prob": 0.9114688634872437}, {"id": 249, "seek": 298600, "start": 2986.0, "end": 3000.0, "text": " out something like that. And this, I suppose it, in a sense, it's like, imagine you're writing the prompt that will be for the, you know, Asimov style robots that are going to populate the earth type thing.", "tokens": [50364, 484, 746, 411, 300, 13, 400, 341, 11, 286, 7297, 309, 11, 294, 257, 2020, 11, 309, 311, 411, 11, 3811, 291, 434, 3579, 264, 12391, 300, 486, 312, 337, 264, 11, 291, 458, 11, 1018, 332, 5179, 3758, 14733, 300, 366, 516, 281, 1665, 5256, 264, 4120, 2010, 551, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11756195269132916, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.8293792605400085}, {"id": 250, "seek": 298600, "start": 3000.0, "end": 3010.0, "text": " Imagine you're writing the prompt, you put down the, you know, the three laws of robotics or something. And then you say, Okay, that's good, I'm done.", "tokens": [51064, 11739, 291, 434, 3579, 264, 12391, 11, 291, 829, 760, 264, 11, 291, 458, 11, 264, 1045, 6064, 295, 34145, 420, 746, 13, 400, 550, 291, 584, 11, 1033, 11, 300, 311, 665, 11, 286, 478, 1096, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11756195269132916, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.8293792605400085}, {"id": 251, "seek": 301000, "start": 3010.0, "end": 3022.0, "text": " And the prompt say, you know, what, what would a better prompt be because that prompt we know from, you know, from Asimov's writing those, those three laws of robotics tangle themselves up very quickly.", "tokens": [50364, 400, 264, 12391, 584, 11, 291, 458, 11, 437, 11, 437, 576, 257, 1101, 12391, 312, 570, 300, 12391, 321, 458, 490, 11, 291, 458, 11, 490, 1018, 332, 5179, 311, 3579, 729, 11, 729, 1045, 6064, 295, 34145, 256, 7846, 2969, 493, 588, 2661, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13424277696453157, "compression_ratio": 1.8962962962962964, "no_speech_prob": 0.502707839012146}, {"id": 252, "seek": 301000, "start": 3022.0, "end": 3027.0, "text": " And, you know, what should we actually, what should we say in that prompt.", "tokens": [50964, 400, 11, 291, 458, 11, 437, 820, 321, 767, 11, 437, 820, 321, 584, 294, 300, 12391, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13424277696453157, "compression_ratio": 1.8962962962962964, "no_speech_prob": 0.502707839012146}, {"id": 253, "seek": 301000, "start": 3027.0, "end": 3039.0, "text": " And even if we could write that prompt in natural language, I think we'd be better off writing it in computational language which has a much more, much more ability to say to answer the what if question, you know, you write it down in", "tokens": [51214, 400, 754, 498, 321, 727, 2464, 300, 12391, 294, 3303, 2856, 11, 286, 519, 321, 1116, 312, 1101, 766, 3579, 309, 294, 28270, 2856, 597, 575, 257, 709, 544, 11, 709, 544, 3485, 281, 584, 281, 1867, 264, 437, 498, 1168, 11, 291, 458, 11, 291, 2464, 309, 760, 294, 51814], "temperature": 0.0, "avg_logprob": -0.13424277696453157, "compression_ratio": 1.8962962962962964, "no_speech_prob": 0.502707839012146}, {"id": 254, "seek": 303900, "start": 3039.0, "end": 3054.0, "text": " natural language, it just is what it says. If you write it in computational language, you can run something and you can say, Let me simulate this situation against this computational language description of what should be done.", "tokens": [50364, 3303, 2856, 11, 309, 445, 307, 437, 309, 1619, 13, 759, 291, 2464, 309, 294, 28270, 2856, 11, 291, 393, 1190, 746, 293, 291, 393, 584, 11, 961, 385, 27817, 341, 2590, 1970, 341, 28270, 2856, 3855, 295, 437, 820, 312, 1096, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07008815318980116, "compression_ratio": 1.6099290780141844, "no_speech_prob": 0.10427550971508026}, {"id": 255, "seek": 305400, "start": 3054.0, "end": 3069.0, "text": " And then you have a much, you know, you have a bigger sort of a larger cross section of stuff that you get to have defined, rather than rather than just the the pure words that you wrote.", "tokens": [50364, 400, 550, 291, 362, 257, 709, 11, 291, 458, 11, 291, 362, 257, 3801, 1333, 295, 257, 4833, 3278, 3541, 295, 1507, 300, 291, 483, 281, 362, 7642, 11, 2831, 813, 2831, 813, 445, 264, 264, 6075, 2283, 300, 291, 4114, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08605648123699686, "compression_ratio": 1.5327868852459017, "no_speech_prob": 0.169120654463768}, {"id": 256, "seek": 306900, "start": 3069.0, "end": 3086.0, "text": " I guess that computational language or being able to talk in these discreet might be one way out of that. I guess you could also probably think about in, you could consider just for for for sake of talking, you could say 1776 America let's", "tokens": [50364, 286, 2041, 300, 28270, 2856, 420, 885, 1075, 281, 751, 294, 613, 2983, 4751, 1062, 312, 472, 636, 484, 295, 300, 13, 286, 2041, 291, 727, 611, 1391, 519, 466, 294, 11, 291, 727, 1949, 445, 337, 337, 337, 9717, 295, 1417, 11, 291, 727, 584, 3282, 25026, 3374, 718, 311, 51214], "temperature": 0.0, "avg_logprob": -0.2273662046952681, "compression_ratio": 1.49375, "no_speech_prob": 0.653779923915863}, {"id": 257, "seek": 308600, "start": 3086.0, "end": 3102.0, "text": " say that the Constitution there is a bot, you know, but there is the human checks, and I guess that's another way to approach the problem how do you kind of put humans into into these thinking patterns so the there is a bottleneck on the human", "tokens": [50364, 584, 300, 264, 14505, 456, 307, 257, 10592, 11, 291, 458, 11, 457, 456, 307, 264, 1952, 13834, 11, 293, 286, 2041, 300, 311, 1071, 636, 281, 3109, 264, 1154, 577, 360, 291, 733, 295, 829, 6255, 666, 666, 613, 1953, 8294, 370, 264, 456, 307, 257, 44641, 547, 322, 264, 1952, 51164], "temperature": 0.0, "avg_logprob": -0.16842707565852574, "compression_ratio": 1.5986842105263157, "no_speech_prob": 0.5742026567459106}, {"id": 258, "seek": 310200, "start": 3102.0, "end": 3118.0, "text": " usability part is going through the human, I guess. Well, right. I mean, so one of the questions is, you know, in some sense, government is like a machine, you know, it has certain regulations that follow you know at least if you're in a sort of following the rule of law type type of place.", "tokens": [50364, 46878, 644, 307, 516, 807, 264, 1952, 11, 286, 2041, 13, 1042, 11, 558, 13, 286, 914, 11, 370, 472, 295, 264, 1651, 307, 11, 291, 458, 11, 294, 512, 2020, 11, 2463, 307, 411, 257, 3479, 11, 291, 458, 11, 309, 575, 1629, 12563, 300, 1524, 291, 458, 412, 1935, 498, 291, 434, 294, 257, 1333, 295, 3480, 264, 4978, 295, 2101, 2010, 2010, 295, 1081, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16237933105892605, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.6207693815231323}, {"id": 259, "seek": 311800, "start": 3118.0, "end": 3131.0, "text": " You know, it's, you know, things go in, it grinds around the bureaucracy operates and something comes out. And it happens to be a machine operating with people, most of the time.", "tokens": [50364, 509, 458, 11, 309, 311, 11, 291, 458, 11, 721, 352, 294, 11, 309, 16700, 82, 926, 264, 44671, 22577, 293, 746, 1487, 484, 13, 400, 309, 2314, 281, 312, 257, 3479, 7447, 365, 561, 11, 881, 295, 264, 565, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11350641704740978, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.509441614151001}, {"id": 260, "seek": 311800, "start": 3131.0, "end": 3145.0, "text": " Maybe it won't be in the future, maybe it will be mostly operated by AI is in the future. I suspect it will be. Now, what has happened in legal systems and things like that is there's always some appeal mechanism there's always some way of getting more people involved.", "tokens": [51014, 2704, 309, 1582, 380, 312, 294, 264, 2027, 11, 1310, 309, 486, 312, 5240, 20826, 538, 7318, 307, 294, 264, 2027, 13, 286, 9091, 309, 486, 312, 13, 823, 11, 437, 575, 2011, 294, 5089, 3652, 293, 721, 411, 300, 307, 456, 311, 1009, 512, 13668, 7513, 456, 311, 1009, 512, 636, 295, 1242, 544, 561, 3288, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11350641704740978, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.509441614151001}, {"id": 261, "seek": 314500, "start": 3145.0, "end": 3164.0, "text": " There's always some way of kind of having a, a kind of broader deeper engagement with people. And perhaps what what ends up happening and sort of reminiscent of some other kinds of systems is, you know, there's a kind of decision by the AI is up to some point.", "tokens": [50364, 821, 311, 1009, 512, 636, 295, 733, 295, 1419, 257, 11, 257, 733, 295, 13227, 7731, 8742, 365, 561, 13, 400, 4317, 437, 437, 5314, 493, 2737, 293, 1333, 295, 44304, 295, 512, 661, 3685, 295, 3652, 307, 11, 291, 458, 11, 456, 311, 257, 733, 295, 3537, 538, 264, 7318, 307, 493, 281, 512, 935, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09533567897609023, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.04718039929866791}, {"id": 262, "seek": 316400, "start": 3164.0, "end": 3184.0, "text": " And then kind of you can blow through that and get to humans if you really insist. Remembering that, in the end, it's, at least for now, it's humans in charge, so to speak. So, in other words, it's kind of like, well, the AI can say, just like, you know, if you run a company or something, people are always saying,", "tokens": [50364, 400, 550, 733, 295, 291, 393, 6327, 807, 300, 293, 483, 281, 6255, 498, 291, 534, 13466, 13, 5459, 278, 300, 11, 294, 264, 917, 11, 309, 311, 11, 412, 1935, 337, 586, 11, 309, 311, 6255, 294, 4602, 11, 370, 281, 1710, 13, 407, 11, 294, 661, 2283, 11, 309, 311, 733, 295, 411, 11, 731, 11, 264, 7318, 393, 584, 11, 445, 411, 11, 291, 458, 11, 498, 291, 1190, 257, 2237, 420, 746, 11, 561, 366, 1009, 1566, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11645951381949492, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.7091794013977051}, {"id": 263, "seek": 318400, "start": 3184.0, "end": 3196.0, "text": " you know, we figured out this and this and this, what should we actually do. You know, there's a, there's a mechanism inside that figures out the here are two alternatives, what should we actually do. And then the CEO gets to decide or whatever.", "tokens": [50364, 291, 458, 11, 321, 8932, 484, 341, 293, 341, 293, 341, 11, 437, 820, 321, 767, 360, 13, 509, 458, 11, 456, 311, 257, 11, 456, 311, 257, 7513, 1854, 300, 9624, 484, 264, 510, 366, 732, 20478, 11, 437, 820, 321, 767, 360, 13, 400, 550, 264, 9282, 2170, 281, 4536, 420, 2035, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1221711056903728, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.7918334007263184}, {"id": 264, "seek": 318400, "start": 3196.0, "end": 3208.0, "text": " And so similarly one can imagine a situation where the AI's are are refining the set of possibilities. And then it's like, okay, reaching out to some humans here, what should we actually do.", "tokens": [50964, 400, 370, 14138, 472, 393, 3811, 257, 2590, 689, 264, 7318, 311, 366, 366, 1895, 1760, 264, 992, 295, 12178, 13, 400, 550, 309, 311, 411, 11, 1392, 11, 9906, 484, 281, 512, 6255, 510, 11, 437, 820, 321, 767, 360, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1221711056903728, "compression_ratio": 1.8874458874458875, "no_speech_prob": 0.7918334007263184}, {"id": 265, "seek": 320800, "start": 3208.0, "end": 3219.0, "text": " You know, I think that's because in the end, you know, the humans are the ones in charge, so to speak, it's not. There isn't, you know, it could be the case and this is one of the bizarre possibilities.", "tokens": [50364, 509, 458, 11, 286, 519, 300, 311, 570, 294, 264, 917, 11, 291, 458, 11, 264, 6255, 366, 264, 2306, 294, 4602, 11, 370, 281, 1710, 11, 309, 311, 406, 13, 821, 1943, 380, 11, 291, 458, 11, 309, 727, 312, 264, 1389, 293, 341, 307, 472, 295, 264, 18265, 12178, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0979779593798579, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.665113627910614}, {"id": 266, "seek": 320800, "start": 3219.0, "end": 3228.0, "text": " You know, people say, no, let's make a constitution for the AI is let's lock it down. Let's not let any of the humans mess it up. Probably very bad idea.", "tokens": [50914, 509, 458, 11, 561, 584, 11, 572, 11, 718, 311, 652, 257, 11937, 337, 264, 7318, 307, 718, 311, 4017, 309, 760, 13, 961, 311, 406, 718, 604, 295, 264, 6255, 2082, 309, 493, 13, 9210, 588, 1578, 1558, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0979779593798579, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.665113627910614}, {"id": 267, "seek": 322800, "start": 3228.0, "end": 3238.0, "text": " Of course, we've seen that in human history because there are plenty of, you know, cultural traditions where you say, you know, let's, you know, the things that were written down a couple of thousand years ago.", "tokens": [50364, 2720, 1164, 11, 321, 600, 1612, 300, 294, 1952, 2503, 570, 456, 366, 7140, 295, 11, 291, 458, 11, 6988, 15643, 689, 291, 584, 11, 291, 458, 11, 718, 311, 11, 291, 458, 11, 264, 721, 300, 645, 3720, 760, 257, 1916, 295, 4714, 924, 2057, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08625803170380769, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.36751294136047363}, {"id": 268, "seek": 322800, "start": 3238.0, "end": 3250.0, "text": " Those are a good model for how to lead life and maybe they're not such a bad model. In fact, but it's sort of locked down from a couple of thousand years ago. And it's like, well, we could start thinking about how to change that.", "tokens": [50864, 3950, 366, 257, 665, 2316, 337, 577, 281, 1477, 993, 293, 1310, 436, 434, 406, 1270, 257, 1578, 2316, 13, 682, 1186, 11, 457, 309, 311, 1333, 295, 9376, 760, 490, 257, 1916, 295, 4714, 924, 2057, 13, 400, 309, 311, 411, 11, 731, 11, 321, 727, 722, 1953, 466, 577, 281, 1319, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08625803170380769, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.36751294136047363}, {"id": 269, "seek": 325000, "start": 3250.0, "end": 3263.0, "text": " And then you get into this whole kind of complicated loop of, do you ignore those traditions? Do you then make, you know, make changes? How does that work? How important is the is the weight of history, so to speak?", "tokens": [50364, 400, 550, 291, 483, 666, 341, 1379, 733, 295, 6179, 6367, 295, 11, 360, 291, 11200, 729, 15643, 30, 1144, 291, 550, 652, 11, 291, 458, 11, 652, 2962, 30, 1012, 775, 300, 589, 30, 1012, 1021, 307, 264, 307, 264, 3364, 295, 2503, 11, 370, 281, 1710, 30, 51014], "temperature": 0.0, "avg_logprob": -0.10864509726470371, "compression_ratio": 1.4726027397260273, "no_speech_prob": 0.6437371373176575}, {"id": 270, "seek": 326300, "start": 3263.0, "end": 3275.0, "text": " How important is it that we've evolved in a way that's made use of those those traditions, that history and so on. So it's a it's a complicated thing. And I think the, I don't know how it's going to resolve.", "tokens": [50364, 1012, 1021, 307, 309, 300, 321, 600, 14178, 294, 257, 636, 300, 311, 1027, 764, 295, 729, 729, 15643, 11, 300, 2503, 293, 370, 322, 13, 407, 309, 311, 257, 309, 311, 257, 6179, 551, 13, 400, 286, 519, 264, 11, 286, 500, 380, 458, 577, 309, 311, 516, 281, 14151, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10079047211215027, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.7413375377655029}, {"id": 271, "seek": 326300, "start": 3275.0, "end": 3292.0, "text": " I think it's a, it's, and I think it's sort of, I hope that there are more people who can think sensibly about this. And unfortunately, it tends to, in my practical experience, a lot of these questions about sort of the ethics of what should happen.", "tokens": [50964, 286, 519, 309, 311, 257, 11, 309, 311, 11, 293, 286, 519, 309, 311, 1333, 295, 11, 286, 1454, 300, 456, 366, 544, 561, 567, 393, 519, 2923, 3545, 466, 341, 13, 400, 7015, 11, 309, 12258, 281, 11, 294, 452, 8496, 1752, 11, 257, 688, 295, 613, 1651, 466, 1333, 295, 264, 19769, 295, 437, 820, 1051, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10079047211215027, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.7413375377655029}, {"id": 272, "seek": 329200, "start": 3293.0, "end": 3301.0, "text": " There's a, there's a tremendous tendency for people to say, well, of course, it should be this way, where that happens to be their overall ideology about how things work.", "tokens": [50414, 821, 311, 257, 11, 456, 311, 257, 10048, 18187, 337, 561, 281, 584, 11, 731, 11, 295, 1164, 11, 309, 820, 312, 341, 636, 11, 689, 300, 2314, 281, 312, 641, 4787, 23101, 466, 577, 721, 589, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09390729727204312, "compression_ratio": 1.7168949771689497, "no_speech_prob": 0.04580218344926834}, {"id": 273, "seek": 329200, "start": 3301.0, "end": 3315.0, "text": " And, you know, it's, I think it's challenging for anybody to kind of say, well, what is the neutral, you know, what is just the machinery of how it works. And then, you know, add your own ideology to that.", "tokens": [50814, 400, 11, 291, 458, 11, 309, 311, 11, 286, 519, 309, 311, 7595, 337, 4472, 281, 733, 295, 584, 11, 731, 11, 437, 307, 264, 10598, 11, 291, 458, 11, 437, 307, 445, 264, 27302, 295, 577, 309, 1985, 13, 400, 550, 11, 291, 458, 11, 909, 428, 1065, 23101, 281, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09390729727204312, "compression_ratio": 1.7168949771689497, "no_speech_prob": 0.04580218344926834}, {"id": 274, "seek": 331500, "start": 3315.0, "end": 3333.0, "text": " It's challenging to think that way. And I suppose it, but it is also a mistake to say, but there is no, you know, I'm thinking, you know, to, to imagine immediately, I'm thinking that way it's kind of like people who say I'm going to make a model of something, but it's going to be, I'm going to have a way of doing", "tokens": [50364, 467, 311, 7595, 281, 519, 300, 636, 13, 400, 286, 7297, 309, 11, 457, 309, 307, 611, 257, 6146, 281, 584, 11, 457, 456, 307, 572, 11, 291, 458, 11, 286, 478, 1953, 11, 291, 458, 11, 281, 11, 281, 3811, 4258, 11, 286, 478, 1953, 300, 636, 309, 311, 733, 295, 411, 561, 567, 584, 286, 478, 516, 281, 652, 257, 2316, 295, 746, 11, 457, 309, 311, 516, 281, 312, 11, 286, 478, 516, 281, 362, 257, 636, 295, 884, 51264], "temperature": 0.0, "avg_logprob": -0.12683734228444654, "compression_ratio": 1.7403314917127073, "no_speech_prob": 0.5697783827781677}, {"id": 275, "seek": 333300, "start": 3333.0, "end": 3348.0, "text": " something that doesn't involve making any assumptions in the model. It's a modelist model, so to speak. Modelist models don't exist. In other words, when, you know, when we say somebody says, I've got a neural net, I'm going to, you know, I'm going to model this thing with a neural net.", "tokens": [50364, 746, 300, 1177, 380, 9494, 1455, 604, 17695, 294, 264, 2316, 13, 467, 311, 257, 2316, 468, 2316, 11, 370, 281, 1710, 13, 17105, 468, 5245, 500, 380, 2514, 13, 682, 661, 2283, 11, 562, 11, 291, 458, 11, 562, 321, 584, 2618, 1619, 11, 286, 600, 658, 257, 18161, 2533, 11, 286, 478, 516, 281, 11, 291, 458, 11, 286, 478, 516, 281, 2316, 341, 551, 365, 257, 18161, 2533, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11912779374556108, "compression_ratio": 1.9826388888888888, "no_speech_prob": 0.6935979127883911}, {"id": 276, "seek": 333300, "start": 3348.0, "end": 3362.0, "text": " There's no assumptions. I'm not assuming anything. Well, you are, you're assuming that you're going to be able to make the model by changing the weights in a neural net. And that's a huge assumption that happens to map, as I was mentioning, you know, fairly well to the way we humans", "tokens": [51114, 821, 311, 572, 17695, 13, 286, 478, 406, 11926, 1340, 13, 1042, 11, 291, 366, 11, 291, 434, 11926, 300, 291, 434, 516, 281, 312, 1075, 281, 652, 264, 2316, 538, 4473, 264, 17443, 294, 257, 18161, 2533, 13, 400, 300, 311, 257, 2603, 15302, 300, 2314, 281, 4471, 11, 382, 286, 390, 18315, 11, 291, 458, 11, 6457, 731, 281, 264, 636, 321, 6255, 51814], "temperature": 0.0, "avg_logprob": -0.11912779374556108, "compression_ratio": 1.9826388888888888, "no_speech_prob": 0.6935979127883911}, {"id": 277, "seek": 336200, "start": 3362.0, "end": 3372.0, "text": " also make models of things, but it's certainly not the only way you could make those models. You're, you're putting a lot of assumptions into the, into the structure of the model by setting it up that way.", "tokens": [50364, 611, 652, 5245, 295, 721, 11, 457, 309, 311, 3297, 406, 264, 787, 636, 291, 727, 652, 729, 5245, 13, 509, 434, 11, 291, 434, 3372, 257, 688, 295, 17695, 666, 264, 11, 666, 264, 3877, 295, 264, 2316, 538, 3287, 309, 493, 300, 636, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08917610843976338, "compression_ratio": 1.768888888888889, "no_speech_prob": 0.03426819294691086}, {"id": 278, "seek": 336200, "start": 3372.0, "end": 3385.0, "text": " And I think the same thing is true with this kind of ideology type thing that there isn't a sort of ideology. There's no ideology list ideology, just as there's no modelist model, so to speak.", "tokens": [50864, 400, 286, 519, 264, 912, 551, 307, 2074, 365, 341, 733, 295, 23101, 2010, 551, 300, 456, 1943, 380, 257, 1333, 295, 23101, 13, 821, 311, 572, 23101, 1329, 23101, 11, 445, 382, 456, 311, 572, 2316, 468, 2316, 11, 370, 281, 1710, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08917610843976338, "compression_ratio": 1.768888888888889, "no_speech_prob": 0.03426819294691086}, {"id": 279, "seek": 338500, "start": 3386.0, "end": 3407.0, "text": " Right. And those are, but I mean, I think these kinds of things, you know, you imagine, okay, I'm going to make a safe AI system. I'm going to put stuff in the prompt that is going to be, you know, that's going to be the kind of, I'm going to recite, you know, these commandments, so to speak, at the beginning, and then everything's going to be okay.", "tokens": [50414, 1779, 13, 400, 729, 366, 11, 457, 286, 914, 11, 286, 519, 613, 3685, 295, 721, 11, 291, 458, 11, 291, 3811, 11, 1392, 11, 286, 478, 516, 281, 652, 257, 3273, 7318, 1185, 13, 286, 478, 516, 281, 829, 1507, 294, 264, 12391, 300, 307, 516, 281, 312, 11, 291, 458, 11, 300, 311, 516, 281, 312, 264, 733, 295, 11, 286, 478, 516, 281, 39434, 11, 291, 458, 11, 613, 40289, 11, 370, 281, 1710, 11, 412, 264, 2863, 11, 293, 550, 1203, 311, 516, 281, 312, 1392, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11159660165960139, "compression_ratio": 1.832579185520362, "no_speech_prob": 0.4750032126903534}, {"id": 280, "seek": 338500, "start": 3407.0, "end": 3412.0, "text": " You know, probably not, but it's a complicated issue.", "tokens": [51464, 509, 458, 11, 1391, 406, 11, 457, 309, 311, 257, 6179, 2734, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11159660165960139, "compression_ratio": 1.832579185520362, "no_speech_prob": 0.4750032126903534}, {"id": 281, "seek": 341200, "start": 3412.0, "end": 3428.0, "text": " Yeah, totally. I think if there's one thing we've learned throughout history is that besides a few young ones today, every constitution and every ideology had an end date. So there is a, it's hard to know if there's never an end.", "tokens": [50364, 865, 11, 3879, 13, 286, 519, 498, 456, 311, 472, 551, 321, 600, 3264, 3710, 2503, 307, 300, 11868, 257, 1326, 2037, 2306, 965, 11, 633, 11937, 293, 633, 23101, 632, 364, 917, 4002, 13, 407, 456, 307, 257, 11, 309, 311, 1152, 281, 458, 498, 456, 311, 1128, 364, 917, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13938591309956141, "compression_ratio": 1.4870129870129871, "no_speech_prob": 0.015299893915653229}, {"id": 282, "seek": 342800, "start": 3429.0, "end": 3438.0, "text": " Well, I mean, the point is, this is one of the features of computational irreducibility, something different is always going to happen, something unexpected is always going to happen.", "tokens": [50414, 1042, 11, 286, 914, 11, 264, 935, 307, 11, 341, 307, 472, 295, 264, 4122, 295, 28270, 16014, 769, 537, 39802, 11, 746, 819, 307, 1009, 516, 281, 1051, 11, 746, 13106, 307, 1009, 516, 281, 1051, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0916717028377032, "compression_ratio": 1.8209606986899562, "no_speech_prob": 0.1753900796175003}, {"id": 283, "seek": 342800, "start": 3438.0, "end": 3454.0, "text": " And that, that's a thing where you, and, and the, you know, the humans will adapt to it in some way, and they will, you know, they'll they'll make some arbitrary decision about what to do based on that unexpected thing that happened.", "tokens": [50864, 400, 300, 11, 300, 311, 257, 551, 689, 291, 11, 293, 11, 293, 264, 11, 291, 458, 11, 264, 6255, 486, 6231, 281, 309, 294, 512, 636, 11, 293, 436, 486, 11, 291, 458, 11, 436, 603, 436, 603, 652, 512, 23211, 3537, 466, 437, 281, 360, 2361, 322, 300, 13106, 551, 300, 2011, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0916717028377032, "compression_ratio": 1.8209606986899562, "no_speech_prob": 0.1753900796175003}, {"id": 284, "seek": 345400, "start": 3454.0, "end": 3482.0, "text": " And maybe that will be a thing and it will be a thing that is made in some sort of societal way. And that's some, that's kind of how it develops. But, but yeah, no, I think it's some, but this, I mean, this idea of kind of what, what is the future of sort of generalized programming, where programming, you know, rolls into it, legal contracts and things like this.", "tokens": [50364, 400, 1310, 300, 486, 312, 257, 551, 293, 309, 486, 312, 257, 551, 300, 307, 1027, 294, 512, 1333, 295, 33472, 636, 13, 400, 300, 311, 512, 11, 300, 311, 733, 295, 577, 309, 25453, 13, 583, 11, 457, 1338, 11, 572, 11, 286, 519, 309, 311, 512, 11, 457, 341, 11, 286, 914, 11, 341, 1558, 295, 733, 295, 437, 11, 437, 307, 264, 2027, 295, 1333, 295, 44498, 9410, 11, 689, 9410, 11, 291, 458, 11, 15767, 666, 309, 11, 5089, 13952, 293, 721, 411, 341, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15883611863659275, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.07917127758264542}, {"id": 285, "seek": 348200, "start": 3482.0, "end": 3497.0, "text": " That's, you know, what does that really look like? How does one, how does one imagine setting up the, the world of, of, you know, when, when legal contracts and, you know, software are the same thing, so to speak.", "tokens": [50364, 663, 311, 11, 291, 458, 11, 437, 775, 300, 534, 574, 411, 30, 1012, 775, 472, 11, 577, 775, 472, 3811, 3287, 493, 264, 11, 264, 1002, 295, 11, 295, 11, 291, 458, 11, 562, 11, 562, 5089, 13952, 293, 11, 291, 458, 11, 4722, 366, 264, 912, 551, 11, 370, 281, 1710, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07145822858347477, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.08225658535957336}, {"id": 286, "seek": 348200, "start": 3497.0, "end": 3508.0, "text": " What does that look like in the, you know, and by the way, I mean, you know, the things that will happen, as always happens with automation, programming is about to get a lot cheaper.", "tokens": [51114, 708, 775, 300, 574, 411, 294, 264, 11, 291, 458, 11, 293, 538, 264, 636, 11, 286, 914, 11, 291, 458, 11, 264, 721, 300, 486, 1051, 11, 382, 1009, 2314, 365, 17769, 11, 9410, 307, 466, 281, 483, 257, 688, 12284, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07145822858347477, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.08225658535957336}, {"id": 287, "seek": 350800, "start": 3508.0, "end": 3524.0, "text": " So there'll be more of it. And so more things will be, you know, more things will be done with code than were done before with code, just as if it gets cheaper to make legal documents as it already just did with boilerplate-ish ones, there'll be more of those floating around.", "tokens": [50364, 407, 456, 603, 312, 544, 295, 309, 13, 400, 370, 544, 721, 486, 312, 11, 291, 458, 11, 544, 721, 486, 312, 1096, 365, 3089, 813, 645, 1096, 949, 365, 3089, 11, 445, 382, 498, 309, 2170, 12284, 281, 652, 5089, 8512, 382, 309, 1217, 445, 630, 365, 39228, 37008, 12, 742, 2306, 11, 456, 603, 312, 544, 295, 729, 12607, 926, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09097492161081798, "compression_ratio": 1.6626506024096386, "no_speech_prob": 0.3084385097026825}, {"id": 288, "seek": 352400, "start": 3524.0, "end": 3536.0, "text": " And it's, you know, just as in people, you know, making sales pitches or something, that, you know, there'll be more of those. There already are more of those because people can basically create them automatically.", "tokens": [50364, 400, 309, 311, 11, 291, 458, 11, 445, 382, 294, 561, 11, 291, 458, 11, 1455, 5763, 43110, 420, 746, 11, 300, 11, 291, 458, 11, 456, 603, 312, 544, 295, 729, 13, 821, 1217, 366, 544, 295, 729, 570, 561, 393, 1936, 1884, 552, 6772, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09009783466657002, "compression_ratio": 1.827906976744186, "no_speech_prob": 0.6110639572143555}, {"id": 289, "seek": 352400, "start": 3536.0, "end": 3546.0, "text": " And it's some, and then, and then what tends to happen, you know, they'll, something became easy. There's more of it. And then there's another level of abstraction that comes in.", "tokens": [50964, 400, 309, 311, 512, 11, 293, 550, 11, 293, 550, 437, 12258, 281, 1051, 11, 291, 458, 11, 436, 603, 11, 746, 3062, 1858, 13, 821, 311, 544, 295, 309, 13, 400, 550, 456, 311, 1071, 1496, 295, 37765, 300, 1487, 294, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09009783466657002, "compression_ratio": 1.827906976744186, "no_speech_prob": 0.6110639572143555}, {"id": 290, "seek": 354600, "start": 3546.0, "end": 3557.0, "text": " And you then start sort of thinking about, well, what, you know, what can you do then? There are then a bunch of possibilities. Each of those kind of needs humans to decide what direction it's going to go in.", "tokens": [50364, 400, 291, 550, 722, 1333, 295, 1953, 466, 11, 731, 11, 437, 11, 291, 458, 11, 437, 393, 291, 360, 550, 30, 821, 366, 550, 257, 3840, 295, 12178, 13, 6947, 295, 729, 733, 295, 2203, 6255, 281, 4536, 437, 3513, 309, 311, 516, 281, 352, 294, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0727336852531123, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.6701639294624329}, {"id": 291, "seek": 354600, "start": 3557.0, "end": 3566.0, "text": " And that sort of creates the next generation of job to job categories. And you keep going from there. I mean, you know, you'll have the prompt engineers for a while.", "tokens": [50914, 400, 300, 1333, 295, 7829, 264, 958, 5125, 295, 1691, 281, 1691, 10479, 13, 400, 291, 1066, 516, 490, 456, 13, 286, 914, 11, 291, 458, 11, 291, 603, 362, 264, 12391, 11955, 337, 257, 1339, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0727336852531123, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.6701639294624329}, {"id": 292, "seek": 354600, "start": 3566.0, "end": 3574.0, "text": " And then maybe you get the meta prompt engineers, I don't know. And then it kind of, you know, it gradually abstracts up.", "tokens": [51364, 400, 550, 1310, 291, 483, 264, 19616, 12391, 11955, 11, 286, 500, 380, 458, 13, 400, 550, 309, 733, 295, 11, 291, 458, 11, 309, 13145, 12649, 82, 493, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0727336852531123, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.6701639294624329}, {"id": 293, "seek": 357400, "start": 3575.0, "end": 3594.0, "text": " But I think, I think that's some, but I do think that there's sort of coming merger of ways to specify things in the world from things like legal contracts, instruction manuals, you know, kind of things, you know, it,", "tokens": [50414, 583, 286, 519, 11, 286, 519, 300, 311, 512, 11, 457, 286, 360, 519, 300, 456, 311, 1333, 295, 1348, 48002, 295, 2098, 281, 16500, 721, 294, 264, 1002, 490, 721, 411, 5089, 13952, 11, 10951, 9688, 82, 11, 291, 458, 11, 733, 295, 721, 11, 291, 458, 11, 309, 11, 51364], "temperature": 0.0, "avg_logprob": -0.15475539366404215, "compression_ratio": 1.658682634730539, "no_speech_prob": 0.023793460801243782}, {"id": 294, "seek": 357400, "start": 3594.0, "end": 3600.0, "text": " that's an interesting moment for, for, for what's going on.", "tokens": [51364, 300, 311, 364, 1880, 1623, 337, 11, 337, 11, 337, 437, 311, 516, 322, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15475539366404215, "compression_ratio": 1.658682634730539, "no_speech_prob": 0.023793460801243782}, {"id": 295, "seek": 360000, "start": 3600.0, "end": 3611.0, "text": " So with the automation, I agree with you. Just to devil's advocate really quick, like, is this not a turkey problem? I know, or I guess turkey problem.", "tokens": [50364, 407, 365, 264, 17769, 11, 286, 3986, 365, 291, 13, 1449, 281, 13297, 311, 14608, 534, 1702, 11, 411, 11, 307, 341, 406, 257, 21551, 1154, 30, 286, 458, 11, 420, 286, 2041, 21551, 1154, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14958531248803233, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.025929266586899757}, {"id": 296, "seek": 360000, "start": 3611.0, "end": 3623.0, "text": " I think I read about it. I guess, I don't know if it seemed to have coined this or he just talks about it, but like the turkey eats food every day and has a happy life and thinks he's going to have a happy life forever and then Thanksgiving comes and he dies.", "tokens": [50914, 286, 519, 286, 1401, 466, 309, 13, 286, 2041, 11, 286, 500, 380, 458, 498, 309, 6576, 281, 362, 45222, 341, 420, 415, 445, 6686, 466, 309, 11, 457, 411, 264, 21551, 18109, 1755, 633, 786, 293, 575, 257, 2055, 993, 293, 7309, 415, 311, 516, 281, 362, 257, 2055, 993, 5680, 293, 550, 21230, 1487, 293, 415, 2714, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14958531248803233, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.025929266586899757}, {"id": 297, "seek": 362300, "start": 3623.0, "end": 3638.0, "text": " Is that, is that not necessarily maybe the same thing with this automation to met a lot of levels of abstraction or is computation irreducibility kind of this theorem that definitively will say that there's always abstractions.", "tokens": [50364, 1119, 300, 11, 307, 300, 406, 4725, 1310, 264, 912, 551, 365, 341, 17769, 281, 1131, 257, 688, 295, 4358, 295, 37765, 420, 307, 24903, 16014, 769, 537, 39802, 733, 295, 341, 20904, 300, 28152, 356, 486, 584, 300, 456, 311, 1009, 12649, 626, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1661926375495063, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.4118460714817047}, {"id": 298, "seek": 362300, "start": 3638.0, "end": 3643.0, "text": " There's always further to go. Whether we care about that further to go, that's the nontrivial question.", "tokens": [51114, 821, 311, 1009, 3052, 281, 352, 13, 8503, 321, 1127, 466, 300, 3052, 281, 352, 11, 300, 311, 264, 297, 896, 470, 22640, 1168, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1661926375495063, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.4118460714817047}, {"id": 299, "seek": 362300, "start": 3643.0, "end": 3646.0, "text": " In other words, there is always another invention to make.", "tokens": [51364, 682, 661, 2283, 11, 456, 307, 1009, 1071, 22265, 281, 652, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1661926375495063, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.4118460714817047}, {"id": 300, "seek": 364600, "start": 3646.0, "end": 3657.0, "text": " There is what you'll never be at the end of having invented everything that can be invented. That is, in this computationally irreducible kind of tower of possibilities.", "tokens": [50364, 821, 307, 437, 291, 603, 1128, 312, 412, 264, 917, 295, 1419, 14479, 1203, 300, 393, 312, 14479, 13, 663, 307, 11, 294, 341, 24903, 379, 16014, 769, 32128, 733, 295, 10567, 295, 12178, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10598110834757488, "compression_ratio": 1.6568627450980393, "no_speech_prob": 0.3006276786327362}, {"id": 301, "seek": 364600, "start": 3657.0, "end": 3665.0, "text": " There are always these little pieces of reducibility, these inventions you can find that allow you to jump a little bit forward and they're an infinite number of those.", "tokens": [50914, 821, 366, 1009, 613, 707, 3755, 295, 2783, 537, 39802, 11, 613, 43748, 291, 393, 915, 300, 2089, 291, 281, 3012, 257, 707, 857, 2128, 293, 436, 434, 364, 13785, 1230, 295, 729, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10598110834757488, "compression_ratio": 1.6568627450980393, "no_speech_prob": 0.3006276786327362}, {"id": 302, "seek": 366500, "start": 3665.0, "end": 3680.0, "text": " And there's never, there's never an end to those. Now, you can decide, okay, we've gone far enough. We're now happy. The things we've done so far are enough and we can, we can just rest on our laurels and never invent anything more.", "tokens": [50364, 400, 456, 311, 1128, 11, 456, 311, 1128, 364, 917, 281, 729, 13, 823, 11, 291, 393, 4536, 11, 1392, 11, 321, 600, 2780, 1400, 1547, 13, 492, 434, 586, 2055, 13, 440, 721, 321, 600, 1096, 370, 1400, 366, 1547, 293, 321, 393, 11, 321, 393, 445, 1472, 322, 527, 49469, 11784, 293, 1128, 7962, 1340, 544, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08386604135686701, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.7554962038993835}, {"id": 303, "seek": 366500, "start": 3680.0, "end": 3692.0, "text": " I don't think that works, because I think that the very evolution of the world, so to speak, which has its own computational irreducibility will always lead us to that new thing that we didn't expect.", "tokens": [51114, 286, 500, 380, 519, 300, 1985, 11, 570, 286, 519, 300, 264, 588, 9303, 295, 264, 1002, 11, 370, 281, 1710, 11, 597, 575, 1080, 1065, 28270, 16014, 769, 537, 39802, 486, 1009, 1477, 505, 281, 300, 777, 551, 300, 321, 994, 380, 2066, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08386604135686701, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.7554962038993835}, {"id": 304, "seek": 369200, "start": 3693.0, "end": 3707.0, "text": " And that, you know, we think we've set up all the cities and we've set the perfect set of roads that do this and that and the other. And then we discover we've set the perfect, you know, way of, of, you know, setting up fields and so on.", "tokens": [50414, 400, 300, 11, 291, 458, 11, 321, 519, 321, 600, 992, 493, 439, 264, 6486, 293, 321, 600, 992, 264, 2176, 992, 295, 11344, 300, 360, 341, 293, 300, 293, 264, 661, 13, 400, 550, 321, 4411, 321, 600, 992, 264, 2176, 11, 291, 458, 11, 636, 295, 11, 295, 11, 291, 458, 11, 3287, 493, 7909, 293, 370, 322, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09616500773328415, "compression_ratio": 2.0114285714285716, "no_speech_prob": 0.1300419718027115}, {"id": 305, "seek": 369200, "start": 3707.0, "end": 3713.0, "text": " And then we discover that, you know, we just fertilize the seaweed and it's starting to do crazy things and so on.", "tokens": [51114, 400, 550, 321, 4411, 300, 11, 291, 458, 11, 321, 445, 18512, 1125, 264, 29449, 293, 309, 311, 2891, 281, 360, 3219, 721, 293, 370, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09616500773328415, "compression_ratio": 2.0114285714285716, "no_speech_prob": 0.1300419718027115}, {"id": 306, "seek": 371300, "start": 3713.0, "end": 3731.0, "text": " And, you know, and that then starts that cycle again, if you're never finished, you've always got to invent another thing. So I think that, but, but, you know, you can imagine a situation where, where like the Turkey, for example, perhaps, you get to a point where everything you care about having invented,", "tokens": [50364, 400, 11, 291, 458, 11, 293, 300, 550, 3719, 300, 6586, 797, 11, 498, 291, 434, 1128, 4335, 11, 291, 600, 1009, 658, 281, 7962, 1071, 551, 13, 407, 286, 519, 300, 11, 457, 11, 457, 11, 291, 458, 11, 291, 393, 3811, 257, 2590, 689, 11, 689, 411, 264, 12647, 11, 337, 1365, 11, 4317, 11, 291, 483, 281, 257, 935, 689, 1203, 291, 1127, 466, 1419, 14479, 11, 51264], "temperature": 0.0, "avg_logprob": -0.1563288843309557, "compression_ratio": 1.5824742268041236, "no_speech_prob": 0.5180957317352295}, {"id": 307, "seek": 373100, "start": 3731.0, "end": 3745.0, "text": " everything you care about has been automated. You know, we've got, now people might say, interestingly enough, if you look from a few hundred years ago, and people look at modern times, it's like, why are you guys working so hard?", "tokens": [50364, 1203, 291, 1127, 466, 575, 668, 18473, 13, 509, 458, 11, 321, 600, 658, 11, 586, 561, 1062, 584, 11, 25873, 1547, 11, 498, 291, 574, 490, 257, 1326, 3262, 924, 2057, 11, 293, 561, 574, 412, 4363, 1413, 11, 309, 311, 411, 11, 983, 366, 291, 1074, 1364, 370, 1152, 30, 51064], "temperature": 0.0, "avg_logprob": -0.08874409477989953, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.6955305933952332}, {"id": 308, "seek": 373100, "start": 3745.0, "end": 3756.0, "text": " Why are you doing anything? You know, you've got enough food to eat, you've got, you know, you've got all these things, you've got all this kind of instant entertainment. It's like just sit back, relax and be happy.", "tokens": [51064, 1545, 366, 291, 884, 1340, 30, 509, 458, 11, 291, 600, 658, 1547, 1755, 281, 1862, 11, 291, 600, 658, 11, 291, 458, 11, 291, 600, 658, 439, 613, 721, 11, 291, 600, 658, 439, 341, 733, 295, 9836, 12393, 13, 467, 311, 411, 445, 1394, 646, 11, 5789, 293, 312, 2055, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08874409477989953, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.6955305933952332}, {"id": 309, "seek": 375600, "start": 3757.0, "end": 3774.0, "text": " But yet that's not actually how people react to that situation. So I think it's, it's kind of an interesting thing that, that you could imagine at a time and this is more a societal question, where everything people care about has been automated.", "tokens": [50414, 583, 1939, 300, 311, 406, 767, 577, 561, 4515, 281, 300, 2590, 13, 407, 286, 519, 309, 311, 11, 309, 311, 733, 295, 364, 1880, 551, 300, 11, 300, 291, 727, 3811, 412, 257, 565, 293, 341, 307, 544, 257, 33472, 1168, 11, 689, 1203, 561, 1127, 466, 575, 668, 18473, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09608909913471766, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.5138071179389954}, {"id": 310, "seek": 377400, "start": 3775.0, "end": 3792.0, "text": " It doesn't last forever, but that could happen for a while, at least. And, and then, and then it's kind of an indeed, like the Thanksgiving Day, you know, there will be some sort of driver from computational irreducibility that something", "tokens": [50414, 467, 1177, 380, 1036, 5680, 11, 457, 300, 727, 1051, 337, 257, 1339, 11, 412, 1935, 13, 400, 11, 293, 550, 11, 293, 550, 309, 311, 733, 295, 364, 6451, 11, 411, 264, 21230, 5226, 11, 291, 458, 11, 456, 486, 312, 512, 1333, 295, 6787, 490, 28270, 16014, 769, 537, 39802, 300, 746, 51264], "temperature": 0.0, "avg_logprob": -0.10460983473679115, "compression_ratio": 1.4191616766467066, "no_speech_prob": 0.44489723443984985}, {"id": 311, "seek": 379200, "start": 3792.0, "end": 3802.0, "text": " unexpected will happen. You know, you can be, I know there have been times in human history where people have sort of said, we're just going to keep doing the same thing. We're going to keep, you know, hurting our goats. We're going to keep doing this stuff.", "tokens": [50364, 13106, 486, 1051, 13, 509, 458, 11, 291, 393, 312, 11, 286, 458, 456, 362, 668, 1413, 294, 1952, 2503, 689, 561, 362, 1333, 295, 848, 11, 321, 434, 445, 516, 281, 1066, 884, 264, 912, 551, 13, 492, 434, 516, 281, 1066, 11, 291, 458, 11, 17744, 527, 34219, 13, 492, 434, 516, 281, 1066, 884, 341, 1507, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11947707086801529, "compression_ratio": 1.9431818181818181, "no_speech_prob": 0.8738782405853271}, {"id": 312, "seek": 379200, "start": 3802.0, "end": 3817.0, "text": " We're going to do it for hundreds of years, thousands of years maybe, and it's all good. And it's a, you know, it's a satisfying life where we're happy, you know, we have children, the children heard the goats as well, everybody's everybody's happy here.", "tokens": [50864, 492, 434, 516, 281, 360, 309, 337, 6779, 295, 924, 11, 5383, 295, 924, 1310, 11, 293, 309, 311, 439, 665, 13, 400, 309, 311, 257, 11, 291, 458, 11, 309, 311, 257, 18348, 993, 689, 321, 434, 2055, 11, 291, 458, 11, 321, 362, 2227, 11, 264, 2227, 2198, 264, 34219, 382, 731, 11, 2201, 311, 2201, 311, 2055, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11947707086801529, "compression_ratio": 1.9431818181818181, "no_speech_prob": 0.8738782405853271}, {"id": 313, "seek": 381700, "start": 3817.0, "end": 3833.0, "text": " And then, you know, that most likely, you know, external drivers eventually cause that to change. But it's certainly the case that we could imagine a situation where, you know, as I say, the AIs have automated things.", "tokens": [50364, 400, 550, 11, 291, 458, 11, 300, 881, 3700, 11, 291, 458, 11, 8320, 11590, 4728, 3082, 300, 281, 1319, 13, 583, 309, 311, 3297, 264, 1389, 300, 321, 727, 3811, 257, 2590, 689, 11, 291, 458, 11, 382, 286, 584, 11, 264, 316, 6802, 362, 18473, 721, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10587382766435731, "compression_ratio": 1.4662162162162162, "no_speech_prob": 0.058949775993824005}, {"id": 314, "seek": 383300, "start": 3833.0, "end": 3853.0, "text": " It is an interesting thing to think about that, you know, our reaction to modern times would be probably seen as very bizarre to somebody from 3, 400 years ago, because we've solved so many of the problems that existed, you know, we've solved sort of early mortality, you know, we've, you know, to a large extent, we've", "tokens": [50364, 467, 307, 364, 1880, 551, 281, 519, 466, 300, 11, 291, 458, 11, 527, 5480, 281, 4363, 1413, 576, 312, 1391, 1612, 382, 588, 18265, 281, 2618, 490, 805, 11, 8423, 924, 2057, 11, 570, 321, 600, 13041, 370, 867, 295, 264, 2740, 300, 13135, 11, 291, 458, 11, 321, 600, 13041, 1333, 295, 2440, 23330, 11, 291, 458, 11, 321, 600, 11, 291, 458, 11, 281, 257, 2416, 8396, 11, 321, 600, 51364], "temperature": 0.0, "avg_logprob": -0.11853760558289367, "compression_ratio": 1.6614583333333333, "no_speech_prob": 0.7801287770271301}, {"id": 315, "seek": 385300, "start": 3854.0, "end": 3865.0, "text": " solved, you know, having enough food to eat in most parts of the world, we've solved, you know, lots of kinds of things, we've solved transmission of knowledge, we've solved, you know, all kinds of stuff.", "tokens": [50414, 13041, 11, 291, 458, 11, 1419, 1547, 1755, 281, 1862, 294, 881, 3166, 295, 264, 1002, 11, 321, 600, 13041, 11, 291, 458, 11, 3195, 295, 3685, 295, 721, 11, 321, 600, 13041, 11574, 295, 3601, 11, 321, 600, 13041, 11, 291, 458, 11, 439, 3685, 295, 1507, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08929580112673202, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.7859598398208618}, {"id": 316, "seek": 386500, "start": 3865.0, "end": 3889.0, "text": " And it's like, yet, you know, I just did this study of kind of how, what work gets done in the world, and, you know, kind of what people, you know, the evolution of jobs and so on, I looked at a bunch of history from the last, well, looked at kind of the jobs people have done in the last 150 years, and also about, so what was it, 60 years or so, of how people spend their time.", "tokens": [50364, 400, 309, 311, 411, 11, 1939, 11, 291, 458, 11, 286, 445, 630, 341, 2979, 295, 733, 295, 577, 11, 437, 589, 2170, 1096, 294, 264, 1002, 11, 293, 11, 291, 458, 11, 733, 295, 437, 561, 11, 291, 458, 11, 264, 9303, 295, 4782, 293, 370, 322, 11, 286, 2956, 412, 257, 3840, 295, 2503, 490, 264, 1036, 11, 731, 11, 2956, 412, 733, 295, 264, 4782, 561, 362, 1096, 294, 264, 1036, 8451, 924, 11, 293, 611, 466, 11, 370, 437, 390, 309, 11, 4060, 924, 420, 370, 11, 295, 577, 561, 3496, 641, 565, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12929649914012237, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.5329814553260803}, {"id": 317, "seek": 388900, "start": 3889.0, "end": 3904.0, "text": " And it is interesting that even in this moment of sort of increased kind of, oh, just sit back, you know, they're going to be shorter work weeks, actually, the average amount of time for people who are working at all, that they work stays at about eight hours a day.", "tokens": [50364, 400, 309, 307, 1880, 300, 754, 294, 341, 1623, 295, 1333, 295, 6505, 733, 295, 11, 1954, 11, 445, 1394, 646, 11, 291, 458, 11, 436, 434, 516, 281, 312, 11639, 589, 3259, 11, 767, 11, 264, 4274, 2372, 295, 565, 337, 561, 567, 366, 1364, 412, 439, 11, 300, 436, 589, 10834, 412, 466, 3180, 2496, 257, 786, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09351479262113571, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.127240389585495}, {"id": 318, "seek": 390400, "start": 3904.0, "end": 3921.0, "text": " And over the course of the last 60 years, the only things you see, for example, you see media and computing goes from like two hours a day for people who aren't working, goes from two hours a day to six hours a day.", "tokens": [50364, 400, 670, 264, 1164, 295, 264, 1036, 4060, 924, 11, 264, 787, 721, 291, 536, 11, 337, 1365, 11, 291, 536, 3021, 293, 15866, 1709, 490, 411, 732, 2496, 257, 786, 337, 561, 567, 3212, 380, 1364, 11, 1709, 490, 732, 2496, 257, 786, 281, 2309, 2496, 257, 786, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09032613259774668, "compression_ratio": 1.5579710144927537, "no_speech_prob": 0.5739575624465942}, {"id": 319, "seek": 392100, "start": 3921.0, "end": 3927.0, "text": " So that's a, you know, there are more couch potatoes, so to speak.", "tokens": [50364, 407, 300, 311, 257, 11, 291, 458, 11, 456, 366, 544, 16511, 11811, 11, 370, 281, 1710, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11621102771243534, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.5712448358535767}, {"id": 320, "seek": 392100, "start": 3927.0, "end": 3937.0, "text": " And, or maybe they just, I don't know what, you know, it's a question of how the time use service work, whether they could be, you know, maybe that maybe that counts programming recreational programming, I don't know.", "tokens": [50664, 400, 11, 420, 1310, 436, 445, 11, 286, 500, 380, 458, 437, 11, 291, 458, 11, 309, 311, 257, 1168, 295, 577, 264, 565, 764, 2643, 589, 11, 1968, 436, 727, 312, 11, 291, 458, 11, 1310, 300, 1310, 300, 14893, 9410, 37554, 9410, 11, 286, 500, 380, 458, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11621102771243534, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.5712448358535767}, {"id": 321, "seek": 393700, "start": 3937.0, "end": 3954.0, "text": " But it is interesting to see that, you know, we humans seem to find things to do, even if, you know, the things that we thought were the oh my gosh, if we can only do that we can sit back and relax doesn't turn out to work out that way.", "tokens": [50364, 583, 309, 307, 1880, 281, 536, 300, 11, 291, 458, 11, 321, 6255, 1643, 281, 915, 721, 281, 360, 11, 754, 498, 11, 291, 458, 11, 264, 721, 300, 321, 1194, 645, 264, 1954, 452, 6502, 11, 498, 321, 393, 787, 360, 300, 321, 393, 1394, 646, 293, 5789, 1177, 380, 1261, 484, 281, 589, 484, 300, 636, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1115362909105089, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.6458957195281982}, {"id": 322, "seek": 395400, "start": 3954.0, "end": 3974.0, "text": " Right, right. No, I like I like this, the optimism of progress that they'll always be progress, they'll based on this math map based on math, they'll always be more to do and that's exciting, you know, that's a there's a lot of doom and gloom in the world of AI, but I think the exciting part is a better way.", "tokens": [50364, 1779, 11, 558, 13, 883, 11, 286, 411, 286, 411, 341, 11, 264, 31074, 295, 4205, 300, 436, 603, 1009, 312, 4205, 11, 436, 603, 2361, 322, 341, 5221, 4471, 2361, 322, 5221, 11, 436, 603, 1009, 312, 544, 281, 360, 293, 300, 311, 4670, 11, 291, 458, 11, 300, 311, 257, 456, 311, 257, 688, 295, 37131, 293, 3114, 298, 294, 264, 1002, 295, 7318, 11, 457, 286, 519, 264, 4670, 644, 307, 257, 1101, 636, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19573774570372046, "compression_ratio": 1.6978021978021978, "no_speech_prob": 0.5747733116149902}, {"id": 323, "seek": 397400, "start": 3975.0, "end": 3992.0, "text": " No, I think that the, it's, you know, the question is really what people want to do, because you could take the point of view, as soon as you've got enough to eat and as soon as you whatever, you're done, you're just going to, you know, you're just going to hang from there on out, so to speak.", "tokens": [50414, 883, 11, 286, 519, 300, 264, 11, 309, 311, 11, 291, 458, 11, 264, 1168, 307, 534, 437, 561, 528, 281, 360, 11, 570, 291, 727, 747, 264, 935, 295, 1910, 11, 382, 2321, 382, 291, 600, 658, 1547, 281, 1862, 293, 382, 2321, 382, 291, 2035, 11, 291, 434, 1096, 11, 291, 434, 445, 516, 281, 11, 291, 458, 11, 291, 434, 445, 516, 281, 3967, 490, 456, 322, 484, 11, 370, 281, 1710, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11593410968780518, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.7345010638237}, {"id": 324, "seek": 399200, "start": 3993.0, "end": 4012.0, "text": " And I think that that's, that is really a, in a sense, the sort of, I don't know, perhaps moral fiber some kind of fiber of society is what do you choose to do in a time when, when, and that's just very interesting to see you know if you look at different countries around the world that are for example rich in", "tokens": [50414, 400, 286, 519, 300, 300, 311, 11, 300, 307, 534, 257, 11, 294, 257, 2020, 11, 264, 1333, 295, 11, 286, 500, 380, 458, 11, 4317, 9723, 12874, 512, 733, 295, 12874, 295, 4086, 307, 437, 360, 291, 2826, 281, 360, 294, 257, 565, 562, 11, 562, 11, 293, 300, 311, 445, 588, 1880, 281, 536, 291, 458, 498, 291, 574, 412, 819, 3517, 926, 264, 1002, 300, 366, 337, 1365, 4593, 294, 51364], "temperature": 0.0, "avg_logprob": -0.149074480131075, "compression_ratio": 1.61139896373057, "no_speech_prob": 0.7090911865234375}, {"id": 325, "seek": 401200, "start": 4013.0, "end": 4020.0, "text": " resources, where, where there's sort of not, you know, some places people do a lot, some people places people don't do a lot.", "tokens": [50414, 3593, 11, 689, 11, 689, 456, 311, 1333, 295, 406, 11, 291, 458, 11, 512, 3190, 561, 360, 257, 688, 11, 512, 561, 3190, 561, 500, 380, 360, 257, 688, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17840466499328614, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.7952556014060974}, {"id": 326, "seek": 401200, "start": 4020.0, "end": 4041.0, "text": " It's, it's interesting and it's, it's, it's a, you know, I think this, you know, it'll be, in a sense one thing to really recognize is that in a sense, AI, particularly in the LLM kind of world is, is kind of a reflection back on us of, you know, what, you know, we've created we've, it has", "tokens": [50764, 467, 311, 11, 309, 311, 1880, 293, 309, 311, 11, 309, 311, 11, 309, 311, 257, 11, 291, 458, 11, 286, 519, 341, 11, 291, 458, 11, 309, 603, 312, 11, 294, 257, 2020, 472, 551, 281, 534, 5521, 307, 300, 294, 257, 2020, 11, 7318, 11, 4098, 294, 264, 441, 43, 44, 733, 295, 1002, 307, 11, 307, 733, 295, 257, 12914, 646, 322, 505, 295, 11, 291, 458, 11, 437, 11, 291, 458, 11, 321, 600, 2942, 321, 600, 11, 309, 575, 51814], "temperature": 0.0, "avg_logprob": -0.17840466499328614, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.7952556014060974}, {"id": 327, "seek": 404200, "start": 4042.0, "end": 4051.0, "text": " been equivalent from everything we've done. And now we are seeing what that looks like reflected back at us. And we may or may not like it.", "tokens": [50364, 668, 10344, 490, 1203, 321, 600, 1096, 13, 400, 586, 321, 366, 2577, 437, 300, 1542, 411, 15502, 646, 412, 505, 13, 400, 321, 815, 420, 815, 406, 411, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1201694914438192, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.052098631858825684}, {"id": 328, "seek": 404200, "start": 4051.0, "end": 4067.0, "text": " And, but, but, you know, it's, I think the other thing that's a little bit of a shame right now is that sort of the world of knowledge work is about to get kind of standardized and the following sense that, you know, it used to be the case everybody would write an essay for themselves.", "tokens": [50814, 400, 11, 457, 11, 457, 11, 291, 458, 11, 309, 311, 11, 286, 519, 264, 661, 551, 300, 311, 257, 707, 857, 295, 257, 10069, 558, 586, 307, 300, 1333, 295, 264, 1002, 295, 3601, 589, 307, 466, 281, 483, 733, 295, 31677, 293, 264, 3480, 2020, 300, 11, 291, 458, 11, 309, 1143, 281, 312, 264, 1389, 2201, 576, 2464, 364, 16238, 337, 2969, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1201694914438192, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.052098631858825684}, {"id": 329, "seek": 406700, "start": 4067.0, "end": 4080.0, "text": " It, it used to be the case people would write, I don't know, some, you know, blurb for a conference or something they'd write it themselves. But now it's like, Well, why bother, you know, we can, you know, we're trying to communicate something but, you know, chat you", "tokens": [50364, 467, 11, 309, 1143, 281, 312, 264, 1389, 561, 576, 2464, 11, 286, 500, 380, 458, 11, 512, 11, 291, 458, 11, 14257, 65, 337, 257, 7586, 420, 746, 436, 1116, 2464, 309, 2969, 13, 583, 586, 309, 311, 411, 11, 1042, 11, 983, 8677, 11, 291, 458, 11, 321, 393, 11, 291, 458, 11, 321, 434, 1382, 281, 7890, 746, 457, 11, 291, 458, 11, 5081, 291, 51014], "temperature": 0.0, "avg_logprob": -0.18025596936543783, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.10668108612298965}, {"id": 330, "seek": 408000, "start": 4080.0, "end": 4096.0, "text": " can do a pretty good job given that we fed it in the, the basic brief for what we wanted it'll it'll spin the words. And now that becomes kind of the, you know, the standard way that a such and such thing is written that we humans can then find it convenient to read,", "tokens": [50364, 393, 360, 257, 1238, 665, 1691, 2212, 300, 321, 4636, 309, 294, 264, 11, 264, 3875, 5353, 337, 437, 321, 1415, 309, 603, 309, 603, 6060, 264, 2283, 13, 400, 586, 300, 3643, 733, 295, 264, 11, 291, 458, 11, 264, 3832, 636, 300, 257, 1270, 293, 1270, 551, 307, 3720, 300, 321, 6255, 393, 550, 915, 309, 10851, 281, 1401, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1730447971459591, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.8814145922660828}, {"id": 331, "seek": 409600, "start": 4096.0, "end": 4110.0, "text": " but it sort of standardizes things and I don't know what's going to happen as a result of this kind of sort of pushing everything to standardization it's kind of like what happened when, you know, there was a time when every book was hand written hand copied, and then printing", "tokens": [50364, 457, 309, 1333, 295, 3832, 5660, 721, 293, 286, 500, 380, 458, 437, 311, 516, 281, 1051, 382, 257, 1874, 295, 341, 733, 295, 1333, 295, 7380, 1203, 281, 3832, 2144, 309, 311, 733, 295, 411, 437, 2011, 562, 11, 291, 458, 11, 456, 390, 257, 565, 562, 633, 1446, 390, 1011, 3720, 1011, 25365, 11, 293, 550, 14699, 51064], "temperature": 0.0, "avg_logprob": -0.10577774922782128, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.5318600535392761}, {"id": 332, "seek": 409600, "start": 4110.0, "end": 4120.0, "text": " started off, and then it was just like here's the font is what the a looks like in that font. And, you know, I don't know how that's how that's quite going to play out.", "tokens": [51064, 1409, 766, 11, 293, 550, 309, 390, 445, 411, 510, 311, 264, 10703, 307, 437, 264, 257, 1542, 411, 294, 300, 10703, 13, 400, 11, 291, 458, 11, 286, 500, 380, 458, 577, 300, 311, 577, 300, 311, 1596, 516, 281, 862, 484, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10577774922782128, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.5318600535392761}, {"id": 333, "seek": 412000, "start": 4120.0, "end": 4135.0, "text": " Well, I think that, like we said, the abstraction layer is going to get higher if everything standardized, it's gonna, you got to be more unique if every book at anyone can make a really good book, the bar for having something really unique to say is just higher and we're", "tokens": [50364, 1042, 11, 286, 519, 300, 11, 411, 321, 848, 11, 264, 37765, 4583, 307, 516, 281, 483, 2946, 498, 1203, 31677, 11, 309, 311, 799, 11, 291, 658, 281, 312, 544, 3845, 498, 633, 1446, 412, 2878, 393, 652, 257, 534, 665, 1446, 11, 264, 2159, 337, 1419, 746, 534, 3845, 281, 584, 307, 445, 2946, 293, 321, 434, 51114], "temperature": 0.0, "avg_logprob": -0.14124291651957743, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.20810560882091522}, {"id": 334, "seek": 412000, "start": 4135.0, "end": 4137.0, "text": " we're getting a little more meta here but", "tokens": [51114, 321, 434, 1242, 257, 707, 544, 19616, 510, 457, 51214], "temperature": 0.0, "avg_logprob": -0.14124291651957743, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.20810560882091522}, {"id": 335, "seek": 413700, "start": 4137.0, "end": 4142.0, "text": " Right, probably that's the case. I mean, I think that the, the", "tokens": [50364, 1779, 11, 1391, 300, 311, 264, 1389, 13, 286, 914, 11, 286, 519, 300, 264, 11, 264, 50614], "temperature": 0.0, "avg_logprob": -0.14924311056369688, "compression_ratio": 1.782122905027933, "no_speech_prob": 0.6278480887413025}, {"id": 336, "seek": 413700, "start": 4142.0, "end": 4158.0, "text": " there's a lot. I mean, if we look at history, there's a lot of things that sort of got standardized everybody got them, you know, they were originally only, you know, only the king had one of those things, and then it got standardized and everybody got it.", "tokens": [50614, 456, 311, 257, 688, 13, 286, 914, 11, 498, 321, 574, 412, 2503, 11, 456, 311, 257, 688, 295, 721, 300, 1333, 295, 658, 31677, 2201, 658, 552, 11, 291, 458, 11, 436, 645, 7993, 787, 11, 291, 458, 11, 787, 264, 4867, 632, 472, 295, 729, 721, 11, 293, 550, 309, 658, 31677, 293, 2201, 658, 309, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14924311056369688, "compression_ratio": 1.782122905027933, "no_speech_prob": 0.6278480887413025}, {"id": 337, "seek": 415800, "start": 4158.0, "end": 4169.0, "text": " You know, everybody got the iPhone, not that there were two different moments in history, probably, but the, you know, I think that then.", "tokens": [50364, 509, 458, 11, 2201, 658, 264, 7252, 11, 406, 300, 456, 645, 732, 819, 6065, 294, 2503, 11, 1391, 11, 457, 264, 11, 291, 458, 11, 286, 519, 300, 550, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14490219604137333, "compression_ratio": 1.7427184466019416, "no_speech_prob": 0.5731568336486816}, {"id": 338, "seek": 415800, "start": 4169.0, "end": 4187.0, "text": " Yeah, no, it's, it will be interesting to see what what it how how things evolve in terms of the value that people place on, for example, the value people place on having code you can understand the value people place on.", "tokens": [50914, 865, 11, 572, 11, 309, 311, 11, 309, 486, 312, 1880, 281, 536, 437, 437, 309, 577, 577, 721, 16693, 294, 2115, 295, 264, 2158, 300, 561, 1081, 322, 11, 337, 1365, 11, 264, 2158, 561, 1081, 322, 1419, 3089, 291, 393, 1223, 264, 2158, 561, 1081, 322, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14490219604137333, "compression_ratio": 1.7427184466019416, "no_speech_prob": 0.5731568336486816}, {"id": 339, "seek": 418700, "start": 4187.0, "end": 4200.0, "text": " You know, it's like, well, yeah, you know, it used to be true until three months ago, that if you saw a well written English, you know, essay, you knew somebody put a lot of effort into it.", "tokens": [50364, 509, 458, 11, 309, 311, 411, 11, 731, 11, 1338, 11, 291, 458, 11, 309, 1143, 281, 312, 2074, 1826, 1045, 2493, 2057, 11, 300, 498, 291, 1866, 257, 731, 3720, 3669, 11, 291, 458, 11, 16238, 11, 291, 2586, 2618, 829, 257, 688, 295, 4630, 666, 309, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08762816132092086, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.053564343601465225}, {"id": 340, "seek": 418700, "start": 4200.0, "end": 4202.0, "text": " Now it isn't true anymore.", "tokens": [51014, 823, 309, 1943, 380, 2074, 3602, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08762816132092086, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.053564343601465225}, {"id": 341, "seek": 418700, "start": 4202.0, "end": 4216.0, "text": " And that used to be something, you know, when I was a kid, for example, if you got a printed invitation to something or something like that, you knew it was kind of a somebody had really put it, you know, it was a big deal, so to speak.", "tokens": [51114, 400, 300, 1143, 281, 312, 746, 11, 291, 458, 11, 562, 286, 390, 257, 1636, 11, 337, 1365, 11, 498, 291, 658, 257, 13567, 17890, 281, 746, 420, 746, 411, 300, 11, 291, 2586, 309, 390, 733, 295, 257, 2618, 632, 534, 829, 309, 11, 291, 458, 11, 309, 390, 257, 955, 2028, 11, 370, 281, 1710, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08762816132092086, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.053564343601465225}, {"id": 342, "seek": 421600, "start": 4216.0, "end": 4225.0, "text": " Nowadays, you know, after desktop publishing, it was no longer the case everybody could make a beautiful sort of printed looking invitation or whatever else it is.", "tokens": [50364, 28908, 11, 291, 458, 11, 934, 14502, 17832, 11, 309, 390, 572, 2854, 264, 1389, 2201, 727, 652, 257, 2238, 1333, 295, 13567, 1237, 17890, 420, 2035, 1646, 309, 307, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09371760084822371, "compression_ratio": 1.7624113475177305, "no_speech_prob": 0.15282279253005981}, {"id": 343, "seek": 421600, "start": 4225.0, "end": 4229.0, "text": " It no longer was a signal of a lot of human effort.", "tokens": [50814, 467, 572, 2854, 390, 257, 6358, 295, 257, 688, 295, 1952, 4630, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09371760084822371, "compression_ratio": 1.7624113475177305, "no_speech_prob": 0.15282279253005981}, {"id": 344, "seek": 421600, "start": 4229.0, "end": 4244.0, "text": " And, you know, until three months ago, having a big essay was a signal of human effort and there are many things in in the operation of bureaucracy, for example, where kind of it says, you know, write an essay that describes this or that thing because people know that writing that", "tokens": [51014, 400, 11, 291, 458, 11, 1826, 1045, 2493, 2057, 11, 1419, 257, 955, 16238, 390, 257, 6358, 295, 1952, 4630, 293, 456, 366, 867, 721, 294, 294, 264, 6916, 295, 44671, 11, 337, 1365, 11, 689, 733, 295, 309, 1619, 11, 291, 458, 11, 2464, 364, 16238, 300, 15626, 341, 420, 300, 551, 570, 561, 458, 300, 3579, 300, 51764], "temperature": 0.0, "avg_logprob": -0.09371760084822371, "compression_ratio": 1.7624113475177305, "no_speech_prob": 0.15282279253005981}, {"id": 345, "seek": 424400, "start": 4244.0, "end": 4249.0, "text": " takes human commitment and human effort or that word did take human effort until three months ago.", "tokens": [50364, 2516, 1952, 8371, 293, 1952, 4630, 420, 300, 1349, 630, 747, 1952, 4630, 1826, 1045, 2493, 2057, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12692037192724084, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.16516141593456268}, {"id": 346, "seek": 424400, "start": 4249.0, "end": 4257.0, "text": " And I think that, you know, that will be the sort of be there's some adapting to do when it when it comes to that type of thing.", "tokens": [50614, 400, 286, 519, 300, 11, 291, 458, 11, 300, 486, 312, 264, 1333, 295, 312, 456, 311, 512, 34942, 281, 360, 562, 309, 562, 309, 1487, 281, 300, 2010, 295, 551, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12692037192724084, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.16516141593456268}, {"id": 347, "seek": 424400, "start": 4257.0, "end": 4268.0, "text": " There's a real arbitrage opportunity now to do a personalized LinkedIn messages and stuff like that but now as soon when everybody's able to make them really well with these technologies.", "tokens": [51014, 821, 311, 257, 957, 14931, 16223, 2650, 586, 281, 360, 257, 28415, 20657, 7897, 293, 1507, 411, 300, 457, 586, 382, 2321, 562, 2201, 311, 1075, 281, 652, 552, 534, 731, 365, 613, 7943, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12692037192724084, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.16516141593456268}, {"id": 348, "seek": 426800, "start": 4269.0, "end": 4288.0, "text": " That's kind of going away. I mean, it almost, you know, first of all, we had SEO writing, then we have, you know, chat chat bot writing LLM writing, and then we'll have LLM meets SEO writing and then we'll have the SEO using, you know, using LLM to", "tokens": [50414, 663, 311, 733, 295, 516, 1314, 13, 286, 914, 11, 309, 1920, 11, 291, 458, 11, 700, 295, 439, 11, 321, 632, 22964, 3579, 11, 550, 321, 362, 11, 291, 458, 11, 5081, 5081, 10592, 3579, 441, 43, 44, 3579, 11, 293, 550, 321, 603, 362, 441, 43, 44, 13961, 22964, 3579, 293, 550, 321, 603, 362, 264, 22964, 1228, 11, 291, 458, 11, 1228, 441, 43, 44, 281, 51364], "temperature": 0.0, "avg_logprob": -0.17630993830014582, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.6797451972961426}, {"id": 349, "seek": 428800, "start": 4289.0, "end": 4312.0, "text": " decode things and it's the, you know, in the end, it's, in the end, the bad thing is a lot of it may just get very, very, very standardized. It may be very kind of rote. It's just like in, you know, maybe almost a recitation of some standard thing, which is sort of interesting.", "tokens": [50414, 979, 1429, 721, 293, 309, 311, 264, 11, 291, 458, 11, 294, 264, 917, 11, 309, 311, 11, 294, 264, 917, 11, 264, 1578, 551, 307, 257, 688, 295, 309, 815, 445, 483, 588, 11, 588, 11, 588, 31677, 13, 467, 815, 312, 588, 733, 295, 367, 1370, 13, 467, 311, 445, 411, 294, 11, 291, 458, 11, 1310, 1920, 257, 850, 4614, 295, 512, 3832, 551, 11, 597, 307, 1333, 295, 1880, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16017930935590696, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.5765354633331299}, {"id": 350, "seek": 431200, "start": 4312.0, "end": 4334.0, "text": " And it kind of becomes almost ritualistic that, you know, it's just you're, you're, you're incanting this, you know, there's some incantation that you write that that sort of the end point of the perfectly perfected LLM that just optimally, you know, optimally invented the thing you should say in your LinkedIn message.", "tokens": [50364, 400, 309, 733, 295, 3643, 1920, 13792, 3142, 300, 11, 291, 458, 11, 309, 311, 445, 291, 434, 11, 291, 434, 11, 291, 434, 834, 18571, 341, 11, 291, 458, 11, 456, 311, 512, 834, 394, 399, 300, 291, 2464, 300, 300, 1333, 295, 264, 917, 935, 295, 264, 6239, 2176, 292, 441, 43, 44, 300, 445, 5028, 379, 11, 291, 458, 11, 5028, 379, 14479, 264, 551, 291, 820, 584, 294, 428, 20657, 3636, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16085847485967042, "compression_ratio": 1.8866396761133604, "no_speech_prob": 0.3589441478252411}, {"id": 351, "seek": 431200, "start": 4334.0, "end": 4341.0, "text": " And then they're all, you know, they're all the same incantation so to speak, because that was the optimal one I don't know how it will work out.", "tokens": [51464, 400, 550, 436, 434, 439, 11, 291, 458, 11, 436, 434, 439, 264, 912, 834, 394, 399, 370, 281, 1710, 11, 570, 300, 390, 264, 16252, 472, 286, 500, 380, 458, 577, 309, 486, 589, 484, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16085847485967042, "compression_ratio": 1.8866396761133604, "no_speech_prob": 0.3589441478252411}, {"id": 352, "seek": 434100, "start": 4341.0, "end": 4345.0, "text": " But then you stop caring about the optimal one and the goal post change.", "tokens": [50364, 583, 550, 291, 1590, 15365, 466, 264, 16252, 472, 293, 264, 3387, 2183, 1319, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2052912257966541, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.021139254793524742}, {"id": 353, "seek": 434100, "start": 4345.0, "end": 4346.0, "text": " Well, yes, right.", "tokens": [50564, 1042, 11, 2086, 11, 558, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2052912257966541, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.021139254793524742}, {"id": 354, "seek": 434100, "start": 4346.0, "end": 4348.0, "text": " Well, you stop caring about that medium.", "tokens": [50614, 1042, 11, 291, 1590, 15365, 466, 300, 6399, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2052912257966541, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.021139254793524742}, {"id": 355, "seek": 434100, "start": 4348.0, "end": 4350.0, "text": " Right, right, right.", "tokens": [50714, 1779, 11, 558, 11, 558, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2052912257966541, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.021139254793524742}, {"id": 356, "seek": 434100, "start": 4350.0, "end": 4360.0, "text": " It becomes, for example, yeah, right. I mean, I think the, the, you know, the handwritten letter is still a thing for another year or two.", "tokens": [50814, 467, 3643, 11, 337, 1365, 11, 1338, 11, 558, 13, 286, 914, 11, 286, 519, 264, 11, 264, 11, 291, 458, 11, 264, 1011, 26859, 5063, 307, 920, 257, 551, 337, 1071, 1064, 420, 732, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2052912257966541, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.021139254793524742}, {"id": 357, "seek": 436000, "start": 4360.0, "end": 4369.0, "text": " I think so too. I think, you know, it means a lot. If I'm, if I'm going on a date with a girl and I give her a handwritten letter versus I text her.", "tokens": [50364, 286, 519, 370, 886, 13, 286, 519, 11, 291, 458, 11, 309, 1355, 257, 688, 13, 759, 286, 478, 11, 498, 286, 478, 516, 322, 257, 4002, 365, 257, 2013, 293, 286, 976, 720, 257, 1011, 26859, 5063, 5717, 286, 2487, 720, 13, 50814], "temperature": 0.0, "avg_logprob": -0.25352209264581854, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.43845677375793457}, {"id": 358, "seek": 436000, "start": 4369.0, "end": 4371.0, "text": " Big difference, but", "tokens": [50814, 5429, 2649, 11, 457, 50914], "temperature": 0.0, "avg_logprob": -0.25352209264581854, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.43845677375793457}, {"id": 359, "seek": 436000, "start": 4371.0, "end": 4373.0, "text": " Right.", "tokens": [50914, 1779, 13, 51014], "temperature": 0.0, "avg_logprob": -0.25352209264581854, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.43845677375793457}, {"id": 360, "seek": 436000, "start": 4373.0, "end": 4380.0, "text": " But anyway, well, very interesting stuff.", "tokens": [51014, 583, 4033, 11, 731, 11, 588, 1880, 1507, 13, 51364], "temperature": 0.0, "avg_logprob": -0.25352209264581854, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.43845677375793457}, {"id": 361, "seek": 436000, "start": 4380.0, "end": 4384.0, "text": " Yeah, really, really appreciate you taking the time to chat about this. I know.", "tokens": [51364, 865, 11, 534, 11, 534, 4449, 291, 1940, 264, 565, 281, 5081, 466, 341, 13, 286, 458, 13, 51564], "temperature": 0.0, "avg_logprob": -0.25352209264581854, "compression_ratio": 1.4776119402985075, "no_speech_prob": 0.43845677375793457}, {"id": 362, "seek": 438400, "start": 4384.0, "end": 4395.0, "text": " There's a, we went all over the place, but it's interesting to think about where the world's going with all this, but just, just want to say that everyone needs to check out the Wolfram plugin on chat.", "tokens": [50364, 821, 311, 257, 11, 321, 1437, 439, 670, 264, 1081, 11, 457, 309, 311, 1880, 281, 519, 466, 689, 264, 1002, 311, 516, 365, 439, 341, 11, 457, 445, 11, 445, 528, 281, 584, 300, 1518, 2203, 281, 1520, 484, 264, 16634, 2356, 23407, 322, 5081, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13823407793801928, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.6120542883872986}, {"id": 363, "seek": 438400, "start": 4395.0, "end": 4402.0, "text": " I think it's really going to be game changing, kind of getting computation in there and excited. Who knows where this is going next.", "tokens": [50914, 286, 519, 309, 311, 534, 516, 281, 312, 1216, 4473, 11, 733, 295, 1242, 24903, 294, 456, 293, 2919, 13, 2102, 3255, 689, 341, 307, 516, 958, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13823407793801928, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.6120542883872986}, {"id": 364, "seek": 438400, "start": 4402.0, "end": 4413.0, "text": " So excited to see and excited to see what comes out of your company here. If this was only two months, I can't imagine the next year, but again, really appreciate you taking the time to chat.", "tokens": [51264, 407, 2919, 281, 536, 293, 2919, 281, 536, 437, 1487, 484, 295, 428, 2237, 510, 13, 759, 341, 390, 787, 732, 2493, 11, 286, 393, 380, 3811, 264, 958, 1064, 11, 457, 797, 11, 534, 4449, 291, 1940, 264, 565, 281, 5081, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13823407793801928, "compression_ratio": 1.771043771043771, "no_speech_prob": 0.6120542883872986}], "language": "en"}