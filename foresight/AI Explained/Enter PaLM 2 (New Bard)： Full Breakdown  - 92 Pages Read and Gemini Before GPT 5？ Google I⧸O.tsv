start	end	text
0	6960	Less than 24 hours ago, Google released the Palm 2 technical report. I have read all 92 pages,
6960	12560	watched the Palm 2 presentation, read the release notes and have already tested the model in a dozen
12560	19200	ways. But before getting into it all, my four main takeaways are these. First, Palm 2 is competitive
19200	25520	with GPT-4 and while it is probably less smart overall, it's better in certain ways and that
26400	31360	Second, Google is saying very little about the data it used to train the model or about
31360	36720	parameters or about compute, although we can make educated guesses on each.
36720	43280	Third, Gemini was announced to be in training and will likely rival GPT-5 while arriving earlier
43280	49600	than GPT-5. As you probably know, Sam Orman said that GPT-5 isn't in training and won't be for a
49600	56720	long time. Fourth, while dedicating 20 pages to bias, toxicity and misgendering, there wasn't a
56720	63120	single page on AI impacts more broadly. Google boasted of giving Gemini planning abilities in a
63120	69600	move that, surprise as I am to say it, makes open AI look like paragons of responsibility.
69600	76080	So a lot to get to, but let's look at the first reason that Palm 2 is different from GPT-4. On
76080	81680	page 3, they say we designed a more multilingual and diverse pre-training mixture, extending across
81680	86960	hundreds of languages and domains like programming, mathematics, etc. So because the text that they
86960	92800	train Palm 2 on is different to the text that open AI trained GPT-4 on, it means that those
92800	99520	models have different abilities and I would say Palm 2 is better at translation and linguistics
99520	104080	and in certain other areas which I'll get to shortly. If that's data, what about parameter
104080	110160	count? Well, Google never actually say they only use words like it's significantly smaller than
110160	116320	the largest Palm model, which was 540 billion parameters. Sometimes they say significantly,
116320	122240	other times dramatically. Despite this, it significantly outperforms Palm on a variety
122240	127440	of tasks. To all the references you may have seen to imminent 100 trillion parameter models
127440	134240	were bogus. Skipping ahead to page 91 out of 92, in the model summary, they say further details of
134240	139520	model size and architecture are withheld from external publication. But earlier on, they did
139520	145040	seem to want to give hints about the parameter count inside Palm 2, which OpenAI never did. Here
145040	150880	they present the optimal number of parameters given a certain amount of compute flops. Scaling
150880	156560	this up to the estimated number of flops used to train Palm 2, that would give an optimal parameter
156560	163840	count of between 100 and 200 billion. That is a comparable parameter count to GPT-3 while getting
163840	170400	competitive performance with GPT-4. Bard is apparently now powered by Palm 2 and the inference
170400	176560	speed is about 10 times faster than GPT-4 for the exact same prompt. And I know there are other
176560	182160	factors that influence inference speed, but that would broadly fit with an order of magnitude
182160	187520	fewer parameters. This has other implications, of course, and they say that Palm 2 is dramatically
187520	193600	smaller, cheaper, and faster to serve. Not only that, Palm 2 itself comes in different sizes,
193600	199760	as Sundar Pichai said. Palm 2 models deliver excellent foundational capabilities across a wide
199760	209120	range of sizes. We have affectionately named them Gecko, Otter, Bison, and Unicon. Gecko is so
209120	215360	lightweight that it can work on mobile devices fast enough for great interactive applications
215360	221840	on-device, even when offline. I would expect Gecko to soon be inside the Google Pixel phones.
221840	227760	Going back to data, Google cryptically said that their pre-training corpus is composed of a diverse
227760	234480	set of sources, documents, books, code, mathematics, and conversational data. I've done a whole video
234480	239600	on the data issues that these companies face, but suffice to say they're not saying anything about
239600	244800	where the data comes from. Next, they don't go into detail, but they do say that Palm 2 was trained
244800	249600	to increase the context length of a model significantly beyond that of Palm. As of today,
249600	254720	you can input around 10,000 characters into BARD, but they end this paragraph with something a bit
254720	259920	more interesting. They say, without demonstrating, our results show that it is possible to increase
260000	264960	the context length of the model without hurting its performance on generic benchmarks.
264960	270000	The bit about not hurting performance is interesting because in this experiment published a few weeks ago
270000	275840	about extending the input size in tokens up to around 2 million tokens, the performance did drop
275840	281840	off. If Google had found a way to increase the input size in tokens and not affect performance,
281840	287200	that would be a breakthrough. On multilingual benchmarks, notice how the performance of Palm
287200	293440	2 in English is not dramatically better than in other languages. In fact, in many other languages,
293440	298800	it does better than in English. This is very different to GPT-4, which was noticeably better
298800	304320	in English than in all other languages. As Google hinted earlier, this is likely due to the
304320	310720	multilingual text data that Google trained Palm 2 with. In fact, on page 17, Google admit that the
310720	316800	performance of Palm 2 exceeds Google Translate for certain languages, and they show on page 4 that
316880	323040	it can pass the mastery exams across a range of languages like Chinese, Japanese, Italian,
323040	328800	French, Spanish, German, etc. Look at the difference between Palm 2 and Palm in red.
328800	334000	Now, before you rush off and try BARD in all of those languages, I tried that and apparently
334000	339600	you can only use BARD at the moment in the following languages, English, US English, what a pity,
339600	347440	and Japanese and Korean. But I was able to test BARD in Korean on a question translated via Google
347440	353840	Translate from the MMLU dataset. It got the question right in each of its drafts. In contrast,
353840	360480	GPT-4 not only got the question wrong in Korean, when I originally tested it for my smart GPT video,
360480	365680	it got the question wrong in English. In case any of my regular viewers are wondering, I am working
365680	371520	very hard on smart GPT to understand what it's capable of and getting it benchmarked officially,
371520	376720	and thank you so much for all the kind offers of help in that regard. I must admit it was very
376720	384160	interesting to see on page 14 a direct comparison between Palm 2 and GPT-4, and Google do admit
384160	390000	for the Palm 2 results they use chain of thought prompting and self-consistency. Reading the
390080	395520	self-consistency paper did remind me quite a lot actually of smart GPT, where it picks the
395520	401760	most consistent answer of multiple outputs. So I do wonder if this comparison is totally fair
401760	406800	if Palm 2 used this method and GPT-4 didn't. I'll have to talk about these benchmarks more
406800	412160	in another video, otherwise this one would be too long, but a quick hint is that Wynogrand is
412160	418320	about identifying what the pronoun in a sentence refers to. Google also weighed into the emerging
418320	424160	abilities debate, saying that Palm 2 does indeed demonstrate new emerging abilities. They say it
424160	430160	does so in things like multi-step arithmetic problems, temporal sequences, and hierarchical
430160	434720	reasoning. Of course I'm going to test all of those and have begun to do so already, and in my
434720	440000	early experiments I'm getting quite an interesting result. Palm 2 gets a lot of questions wrong that
440000	446080	GPT-4 gets right, but it can also get questions right that GPT-4 gets wrong, and I must admit
446080	451120	it's really weird to see Palm 2 getting really advanced college-level math questions right,
451120	456960	that GPT-4 gets wrong. And yet also when I ask it a basic question about prime numbers, it gets
456960	461920	it kind of hilariously wrong. Honestly I'm not certain what's going on there, but I do have my
461920	467760	suspicions. Remember though that recent papers have claimed that emergent abilities are a mirage,
467760	474320	so Google begs to differ. When Google put Palm 2 up against GPT-4 in high school mathematics problems,
474320	481360	it did outperform GPT-4, but again it was using an advanced prompting strategy, not 100% different
481360	486880	from smart GPT, so I wonder if the comparison is quite fair. What about coding? Well again it's
486880	492880	really hard to find a direct comparison that's fair between the two models. Overall I would guess
492880	500000	that the specialized coding model of Palm, what they call Palm 2S, is worse than GPT-4. It says
500000	507840	it's pass at one accuracy, as in past first time, is 37.6%. Remember the Sparks of AGI paper? Well
507840	515120	that gave GPT-4 as having an 82% zero shot pass at one accuracy level. However as I talked about in
515120	522160	the Sparks of AGI video, the paper admits that it could be that GPT-4 has seen and memorized some
522160	527840	or all of human eval. There is one thing I will give Google credit on, which is that their code
527840	533280	now sometimes references where it came from. Here is a brief extract from the Google keynote
533280	539760	presentation. How would I use Python to generate the Scholar's Mate move in chess? Okay here Bard
539760	545600	created a script to recreate this chess move in Python, and notice how it also formatted the code
545600	551200	nicely, making it easy to read. We've also heard great feedback from developers about how Bard
551200	557280	provides code citations, and starting next week you'll notice something right here. We're making
557280	563600	code citations even more precise. If Bard brings in a block of code, just click this annotation,
563600	568960	and Bard will underline the block and link to the source. As always, it seems the appendix contained
568960	574080	more interesting information sometimes than the main body of the technical report. For example,
574080	581920	we get a direct and fair comparison between GPT-4 and palm 2, or I should say flan palm 2. That is
581920	587440	the instruction fine tuned version of palm 2. Essentially that's the version where it's been
587440	592560	fine tuned to get better at following a question and answer format. But anyway, the original palm
592560	602480	2 scored 78.3%, and flan palm 2 scored 81.2%. That's below the 86.4% of GPT-4. And that's why my
602480	608960	broad conclusion is that GPT-4 is a bit smarter than palm 2. But as I'll be showing over the
608960	615840	coming days and weeks, there are genuinely quite a few areas in which palm 2 is better than GPT-4.
615840	620400	What about the big bench, which was designed to be particularly tough for language models? I talked
620400	626560	a lot about this in my earliest videos. Well, the graph is going to look pretty weird because palm 2
626560	633200	has improved upon palm while reducing the number of parameters. So the graph kind of doubles back
633280	639520	on itself back up here up to around 69% according to the technical report. I would say this is
639520	645440	quite a major moment in human history. There is now virtually no language task that the average
645440	651840	human can do better than palm 2. Of course, expert humans can do better in individual domains, but
651840	657840	the average human is now worse in virtually every domain of language. Here you can see that confirmation
657920	664640	of the big bench hard results for flan palm 2, 69.1%. Interestingly, in the original chart,
664640	671440	palm 2 is even claimed to have higher performance than that at 78.1%. If you remember, the reason we
671440	677440	can't compare that to GPT-4 is that in the technical report for GPT-4, they admit that during their
677440	683040	contamination check, we discovered that portions of big bench were inadvertently mixed into the
683040	688560	training set and we excluded it from our reported results. Before we get to Gemini, Google show off
688560	694160	in the latter half of the technical report with examples of linguistic ability, like writing
694160	700080	paragraphs in Tejiki and then translating them into Persian. They go on to show examples in
700080	705840	Tamil and they are really making a big point of showing off its multilingual capabilities. At
705840	711200	this point, and I'm going to admit this is my personal opinion, Google then strays into dozens
711200	718000	of pages on bias, toxicity and gender. Interestingly, some of the people paid to assess these risks
718000	723120	were paid only 1.5 cents per judgment. These things do need to be addressed, of course,
723120	728480	but it was somewhat shocking to me to see 20 pages of that and not a single page on the
728480	734000	broader AI impacts. As many of you may know, I have criticized open AI plenty of times on this
734000	739440	channel, but compare their technical report, which goes into far more detail about what we need to
739440	745360	monitor. The closest Google got was showing how their universal translator could be used for deep fakes.
745360	751600	Universal Translate is an experimental AI video dubbing service that helps experts
751600	757120	translate a speaker's voice while also matching their lip movements. Let me show you how it works
757120	762560	with an online college course created in partnership with Arizona State University.
762560	767440	What many college students don't realize is that knowing when to ask for help and then following
767440	771280	through and using helpful resources is actually a hallmark of becoming a productive adult.
781040	785440	It just seems a massive black hole when one of their recent former employees,
785440	788640	Jeffrey Hinton, had this to say this week on CNN.
788640	795200	You've spoken out saying that AI could manipulate or possibly figure out a way to kill humans?
795440	796640	How could it kill humans?
796640	800320	If it gets to be much smarter than us, it'll be very good at manipulation because it will
800320	805200	have learned that from us. And a very few examples of a more intelligent thing being
805200	809760	controlled by a less intelligent thing. And it knows how to program, so it'll figure out ways of
809760	814800	getting around restrictions we put on it. It'll figure out ways of manipulating people to do
814800	820720	what it wants. It's not clear to me that we can solve this problem. I believe we should put a big
820720	824720	effort into thinking about ways to solve the problem. I don't have a solution at present.
824720	829280	I just want people to be aware that this is a really serious problem and we need to be thinking
829280	833680	about it very hard. This all seems particularly relevant when Google made this announcement
833680	840560	about Gemini, their rival to GPT-5. All this helps set the stage for the inflection point we are at
840560	846640	today. We recently brought these two teams together into a single unit, Google DeepMind.
847360	853040	Using the computational resources of Google, they are focused on building more capable systems
853760	860560	safely and responsibly. This includes our next generation foundation model, Gemini,
860560	866400	which is still in training. Gemini was created from the ground up to be multi-modal,
867280	873600	highly efficient at tool and API integrations, and built to enable future innovations
873600	879280	like memory and planning. That ability to plan may ring a bell from the GPT-4
880080	884400	which said this, novel capabilities often emerge in more powerful models.
884400	890160	Some that are particularly concerning are the ability to create and act on long-term plans.
890160	895760	Remember, Google didn't identify planning as a risk but as a selling point for Gemini.
895760	900880	Next, Google talked about accelerating their progress, which was again directly mentioned
900880	905920	in the GPT-4 technical report. It said, one concern of particular importance to open AI
905920	910080	is the risk of racing dynamics leading to a decline in safety standards,
910080	915840	the diffusion of bad norms and accelerated AI timelines, each of which heightens societal
915840	921920	risks associated with AI. We refer to these here as acceleration risk and make no mistake,
921920	928880	Gemini will be very accelerated from Palm II. It looks set to use the TPU V5 chip,
928880	934800	which was announced back in January of last year. And on page 91 of the Palm II technical report,
934800	940960	they say that that model used TPU V4. Now, it should be said that Palm II is leading to some
940960	946240	impressive medical applications. As I actually first reported on seven weeks ago without quite
946240	951440	realizing it, here's MedPalm II. We believe large language models have the potential to revolutionize
951440	956240	healthcare and benefit society. MedPalm is a large language model that we've taken and tuned for the
956240	962960	medical domain. Medical question answering has been a research grand challenge for several decades
962960	967280	but till date the progress has been kind of slow. But then over the course of the last
968000	973280	three to four months, first with MedPalm and MedPalm II, we have kind of like broken through that
973280	979200	barrier. Unlike previous versions, MedPalm II was able to score 85% on the USMLA medical licensing
979200	984000	exam. Yeah, this is immensely exciting because people have been working on medical question
984000	988800	answering for over three decades. And finally, we are at a stage where we can say with confidence
988800	994400	that AI systems can now at least answer USMLA questions as good as experts. As many of you
994400	1000400	may know, the CEO of Google as well as the CEO of Microsoft and Sam Altman and the CEO of Anthropic
1000400	1005840	all went to the White House to discuss AI risk and opportunity. But given that the main outcome
1005840	1012000	from that seems to be 140 million to establish seven new AI research institutes, that feels a
1012000	1016800	little slow given all the acceleration that's occurring. Because as Google somewhat soberly
1016800	1021680	conclude their report, we believe that further scaling of both model parameters and data set
1021680	1026720	size and quality as well as improvements in the architecture and objective will continue to yield
1026720	1032800	gains in language understanding and generation. They are not slowing down and the world hasn't
1032800	1046640	yet caught up. Thank you so much for watching to the end and have a wonderful day.
