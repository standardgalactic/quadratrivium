WEBVTT

00:00.000 --> 00:06.080
Like buses, AI news can sometimes be slow and sometimes arrive all at once.

00:06.080 --> 00:12.640
In the last few days we have had dramatic new leaked insights into the sheer breadth of Google's

00:12.640 --> 00:18.960
Gemini. Just today we've had the release of Meta's Code Lama and earlier their impressive

00:18.960 --> 00:26.800
multilingual seamless M4T model. And last but definitely not least, this 88-page AI

00:26.880 --> 00:32.640
consciousness report. And yes, I read it all, it's juicy so I'm saving that for the end.

00:32.640 --> 00:38.720
But let's start with two major paywall articles, one from the information and one from the New York

00:38.720 --> 00:45.040
Times about Google's Gemini model. From both of them I counted a total of nine new revelations,

00:45.040 --> 00:48.480
so let's get straight to it. To give you a sense of timeline, by the way,

00:48.480 --> 00:55.440
Google's newly merged AI SWOT team, they call it, is preparing for a big fall or autumn launch.

00:55.440 --> 01:01.200
The takeaway for me from both articles is that Gemini is going to be the everything model. Did

01:01.200 --> 01:07.920
you know it's going to be the rival to mid-journey and stable diffusion? Mid-journey only has 11

01:07.920 --> 01:14.000
full-time staff, so it is more than plausible that Google's Gemini could outperform mid-journey

01:14.000 --> 01:21.120
version 5. Next we may be able to create graphics with just text descriptions and control software

01:21.120 --> 01:26.640
using only text or voice commands. These next two are speculations, so I'm not even counting them

01:26.640 --> 01:32.160
in the list of leaks. I've already covered in a previous video that Gemini has been trained on

01:32.160 --> 01:38.800
YouTube video transcripts, and the speculation is that by integrating video and audio into Gemini,

01:38.800 --> 01:44.160
it could perhaps help a mechanic diagnose a problem with a car repair based on a video,

01:44.160 --> 01:50.720
or be a rival to Runway ML by generating advanced text to video based on descriptions of what a user

01:50.720 --> 01:55.440
wants to see. You can start to see why I'm beginning to think of it as the everything model.

01:55.440 --> 02:01.120
Another leak is that one of the co-founders of Google, Sergey Brin, is working on the front lines

02:01.120 --> 02:06.320
of Google Gemini. And lastly from this article, I found it really interesting that Google's

02:06.320 --> 02:12.560
lawyers have been closely evaluating the training, and they made researchers remove training data

02:12.560 --> 02:18.160
that had come from textbooks, even though those textbooks helped the model answer questions about

02:18.160 --> 02:24.080
subjects like astronomy or biology. And I do wonder if they privately benchmarked Gemini before

02:24.080 --> 02:30.720
removing that crucial textbook data. But if that's not enough, prepare to also receive life advice.

02:30.720 --> 02:36.000
My theory here is that Google wants to compete directly for market share with inflection's

02:36.000 --> 02:41.280
pie. What if you want scientific, creative, or professional writing? Yep, they're working on

02:41.280 --> 02:46.480
that too. In fact, we already know that Google has software named Genesis that they're pitching

02:46.480 --> 02:51.920
to the New York Times, which can generate news articles, rewrite them, suggest headlines, etc.

02:51.920 --> 02:56.720
But some people will be more interested in this feature that Google DeepMind is working on,

02:56.720 --> 03:02.480
the ability to draft critiques of an argument and generate quizzes, word, and number puzzles.

03:02.480 --> 03:08.320
It's almost easier at this point to ask what might Google Gemini not be able to do. And yes,

03:08.320 --> 03:15.440
this is not Gemini, but Google DeepMind is also using AI to design the next generation of semiconductors.

03:15.440 --> 03:21.120
But if the fall seems far away, how about today when we got CodeLama from Meta?

03:21.120 --> 03:27.200
I spent much of the last two hours reading most of the 47-page paper, and you can see CodeLama

03:27.200 --> 03:33.440
in action on screen. Some highlights include that the CodeLama models provide stable generations with

03:33.440 --> 03:39.360
up to 100,000 tokens of context. Obviously, that could be used for generating longer programs or

03:39.360 --> 03:44.240
providing the model with more context from your code base to make the generations more relevant.

03:44.240 --> 03:49.200
It comes in three versions, CodeLama, CodeLama Instruct, which can better understand natural

03:49.200 --> 03:54.080
language instructions, and CodeLama Python, better, of course, at Python. It's available

03:54.080 --> 04:00.800
for commercial use, and as you can see, some of the versions rival GPT 3.5 on human eval.

04:00.800 --> 04:07.840
That top score of 53.7% on Passat 1 puts it in the same ballpark as Phi 1. I've actually done

04:07.840 --> 04:15.120
a full video on Phi 1, so do check that out, but that got 50.6%. But it is about 25 times smaller

04:15.120 --> 04:20.480
at 1.3 billion parameters. Interestingly, the CodeLama paper, which also came out about two

04:20.480 --> 04:26.400
hours ago, mentions Phi 1 directly, saying that it follows in a similar spirit, but the difference

04:26.400 --> 04:32.240
is that Phi 1 is closed source. Anyway, a couple more interesting things before we move on from

04:32.320 --> 04:38.080
CodeLama, and the first one is the self-instruct method that they used. Let me know if you also

04:38.080 --> 04:44.240
find this fascinating, because step one was to generate 62,000 interview style programming

04:44.240 --> 04:49.760
questions by prompting Lama 2, the 70 billion parameter model. Then they removed duplicates in

04:49.760 --> 04:53.920
step two. But here's where it gets interesting. For each of those questions, they first generated

04:53.920 --> 05:00.560
a unit test by prompting CodeLama 7 billion parameters. Then they generated 10 Python solutions

05:00.560 --> 05:06.080
by prompting CodeLama. Finally, they ran unit tests on those 10 solutions, and they added the

05:06.080 --> 05:10.880
first solution that passes those tests, along with the corresponding question and test, to the

05:10.880 --> 05:15.280
self-instruct data set. If that sounded a bit complicated, let me try to distill it a bit.

05:15.280 --> 05:22.080
They asked the Big Brother Lama 2 model to generate questions, then got the Little Brother CodeLama

05:22.080 --> 05:28.400
to generate tests for those questions, then got the model to generate solutions to its own tests,

05:28.400 --> 05:33.280
found the good solutions that don't forget it produced, and then used those to further train

05:33.280 --> 05:39.520
the model. To be honest, synthetic data and self-instruct seem to be the future of feedback.

05:39.520 --> 05:45.200
One final interesting quote from the paper on safety, and that was an argument advanced by one

05:45.200 --> 05:51.360
of their red teamers. They made the point that various scripts and code is readily available on

05:51.360 --> 05:56.640
mainstream public websites, hacking forums, or the dark web. And the advanced malware development

05:56.640 --> 06:02.560
is beyond the current capabilities of available LLMs. And even an advanced LLM paired with an

06:02.560 --> 06:08.240
expert malware developer is not particularly useful at the moment, as the barrier is not typically

06:08.240 --> 06:14.560
writing the malware code itself. Let me know what you think in the comments. But we must move on to

06:14.560 --> 06:22.080
seamless M4T released a couple of days ago from Meta, which frankly seems amazing for multi-lingual

06:22.080 --> 06:28.160
translation. That's speech to text, speech to speech, text to text, and more. It has speech

06:28.160 --> 06:34.800
recognition for nearly 100 languages and can output in 36 languages. But there's one feature I find

06:34.800 --> 06:42.800
particularly cool. Now, let's talk about code switching. Code switching happens when a multilingual

06:42.800 --> 06:48.800
speaker switches between languages while they are speaking. Our model seamless M4T automatically

06:48.800 --> 06:54.640
recognizes and translates more than one language when mixed in the same sentence. As a multilingual

06:54.640 --> 07:00.000
speaker, this is a very exciting capability for me. I often switch from Hindi to Telugu

07:00.000 --> 07:04.960
when I speak with my dad. Notice in the following example when I change languages.

07:19.040 --> 07:27.280
I can speak Hindi, Telugu, and English. Sometimes I use all three languages in one conversation.

07:27.840 --> 07:34.800
Speaking of cool though, we had this epic story out yesterday. AI gave a paralyzed woman her voice

07:34.800 --> 07:41.360
back. In a moment, you're going to see her being plugged in to the model. There we go. And the short

07:41.360 --> 07:47.760
version is that this woman suffered a stroke that left her unable to speak. But now for the first

07:47.760 --> 07:54.160
time, her speech and facial expressions can be synthesized from her brain signals, decoding

07:54.160 --> 08:00.080
these signals into text at nearly 80 words per minute up from 14 words per minute. But let's

08:00.080 --> 08:06.400
now end on this, an 88 page report on consciousness in artificial intelligence, which counts as one

08:06.400 --> 08:12.240
of its co-authors, Yoshua Benjio, the Turing Award winner. It was dense and quite technical,

08:12.240 --> 08:18.640
but well worth the read. Look at this sentence in just the abstract. Our analysis suggests that no

08:18.640 --> 08:24.240
current AI systems are conscious, but also suggests that there are no obvious technical barriers to

08:24.240 --> 08:31.600
building AI systems which satisfy these indicators. These are the indicators and each one gets a few

08:31.600 --> 08:37.760
pages in the report. And the reason that they're split up is because each one rests on a certain

08:37.760 --> 08:44.240
theory of consciousness. Obviously, the key problem is that we don't have a consensus theory on what

08:44.240 --> 08:50.000
consciousness is or how it comes about. So in a way, to hedge their bets, they group in different

08:50.000 --> 08:55.200
theories and look at the kind of indicators that would satisfy each one. You might say that list

08:55.200 --> 09:00.960
seems so theoretical. Why not just test the model or even ask the model? For more on that approach,

09:01.040 --> 09:05.760
see my theory of mind video. But the problem is, as they say on page four,

09:05.760 --> 09:12.320
the main alternative to a theory heavy approach is to use behavioral tests for consciousness.

09:12.320 --> 09:17.280
But as I talked about in the other video, that method is unreliable because AI systems can be

09:17.280 --> 09:22.800
trained, of course they are, to mimic human behaviors, are working actually in very different

09:22.800 --> 09:28.640
ways. Essentially, LLMs have broken the traditional tests for consciousness, including of course the

09:28.640 --> 09:33.600
Turing test. The paper also rests on the assumption of computational functionalism,

09:33.600 --> 09:38.960
essentially that computations are essential for consciousness. As in, it's not what you're made

09:38.960 --> 09:44.960
of, it's what you do. If this is wrong, and the substrate in fact is key, say biological cells,

09:44.960 --> 09:49.680
then it stands to reason that AI would never be conscious. But one of their early conclusions

09:49.680 --> 09:55.440
is that if computational functionalism is true, and it is widely believed, conscious AI systems

09:55.440 --> 10:00.640
could realistically be built in the near term. Having digested the entire paper, they're strongly

10:00.640 --> 10:06.240
suggesting that we're not there yet. But if this theory is true, we could be there, especially if

10:06.240 --> 10:12.160
researchers deliberately designed systems to meet these criteria. In fact, here is a key quote from

10:12.160 --> 10:17.760
one of the authors in Science that came along with the piece. It would be trivial to design all of

10:17.760 --> 10:23.600
these features into an AI. The reason no one has done so is it is not clear that they would be useful

10:23.920 --> 10:29.600
for tasks. Now, to be honest, it is way beyond my pay grade to try to explain every aspect of the

10:29.600 --> 10:35.440
paper. But I'm going to try my best to convey the key bits. First, what is the definition of

10:35.440 --> 10:40.560
consciousness that they are working with? Well, skipping the jargon, they essentially say, if you

10:40.560 --> 10:46.160
are reading this report on a screen, you are having a conscious visual experience of the screen. That

10:46.160 --> 10:52.000
is separated from sentence, which is also sometimes used to mean being capable of pleasure or pain.

10:52.000 --> 10:56.640
And they say that it's possible for a system to be sentient without being conscious by sensing

10:56.640 --> 11:02.080
its body or environment. And it's possible for a system to be conscious without sensing its body

11:02.080 --> 11:07.920
or environment. It also might be possible to be slightly conscious or conscious to a greater

11:07.920 --> 11:14.560
degree than humans. Ilya Sutskova famously said, it may be that today's large neural networks are

11:14.560 --> 11:20.240
slightly conscious. And the Karl Schulman and Nick Bostrom wrote an entire chapter of a book

11:20.240 --> 11:25.120
on the possibility that models become more conscious than human beings. They say such

11:25.120 --> 11:29.920
beings could contribute immense value to the world and failing to respect their interests could

11:29.920 --> 11:35.760
produce a moral catastrophe. One of the theories of consciousness discussed is recurrent processing

11:35.760 --> 11:41.680
theory. And here is the key part of that theory. One initial feed forward sweep of activity through

11:41.680 --> 11:47.120
the hierarchy of visual areas is sufficient for some visual operations like extracting features

11:47.120 --> 11:53.040
from a scene, but not sufficient for conscious experience. However, when the stimulus is sufficiently

11:53.040 --> 11:58.320
strong or salient, we get this looped recurrent processing in which signals are sent back from

11:58.320 --> 12:05.280
higher areas to lower ones. It's only then that you get a conscious representation of an organized

12:05.280 --> 12:11.200
scene. The paper then draws indicators based on each theory. For example, if recurrent processing

12:11.200 --> 12:17.840
theory is accurate, then here are two indicators that something would be conscious. They then draw

12:17.840 --> 12:24.320
analogies for each theory to AI systems. For example, on recurrence, specifically algorithmic

12:24.320 --> 12:29.600
recurrence, they say that's a weak condition that many AI systems already meet. But don't forget

12:29.600 --> 12:35.200
when they say that it's an analogy. Not only does it require the theory to be correct, it requires

12:35.200 --> 12:41.280
the analogy to hold true. i.e. is the recurrence that we see in AI a good analogy for the recurrence

12:41.280 --> 12:47.360
of this theory. Or what about the next one, global workspace theory? If that theory is correct, here

12:47.360 --> 12:52.720
are four indicators of something being conscious according to that theory. To be honest, if you

12:52.720 --> 12:58.880
are at all interested in consciousness, the pages on each one of these taught me a lot about tests

12:58.880 --> 13:04.240
for consciousness and just theories of consciousness. But again, let's just say that theory is correct.

13:04.240 --> 13:10.080
Do AI systems demonstrate these indicators? Do they have modules that can work in parallel

13:10.080 --> 13:16.640
and a global workspace at the center? Is that workspace bandwidth limited, requiring the compression

13:16.640 --> 13:23.200
and selection of information from the modules? Well, here again, we can only rely on analogies,

13:23.200 --> 13:28.640
in this case, to the transformer architecture. They say, in a sense, they do have modules,

13:28.640 --> 13:33.520
they do have a limited capacity workspace introducing a bottleneck, but then the authors

13:33.520 --> 13:39.280
introduce plenty of points about how the analogy is not perfect, even here. Of course, you can pause

13:39.280 --> 13:44.720
and read the details if you like, or indeed read the entire paper. So that's the tone of the paper.

13:44.720 --> 13:51.200
If silicon can be a replacement to carbon, and if these analogies hold, then there is a strong

13:51.200 --> 13:56.240
case that most or all of the conditions for consciousness, suggested by current computational

13:56.240 --> 14:01.600
theories, can be met using existing techniques in AI. This is not to say that current AI systems

14:01.600 --> 14:05.920
are likely to be conscious. There is also the issue of whether they combine existing techniques

14:05.920 --> 14:10.800
in the right ways. But it does suggest that conscious AI is not merely a remote possibility in the

14:10.800 --> 14:16.640
distant future. And here is the key bit. If it is at all possible to build conscious AI systems

14:16.640 --> 14:22.240
without radically new hardware, it may be possible now. Of course, even if all of those conditions

14:22.240 --> 14:28.160
and analogies hold, it may not be the same type of consciousness as our consciousness. It seems

14:28.160 --> 14:33.840
possible, they say, to imagine a conscious being that had only a succession of brief static

14:33.840 --> 14:38.640
discrete experiences, perhaps just during pre-training. Or they might have experiences

14:38.640 --> 14:44.320
without feeling that they are a persisting subject. But my own summary is this. We clearly

14:44.320 --> 14:49.760
don't fully understand consciousness or what is required for consciousness. We don't know if

14:49.760 --> 14:55.520
consciousness in AI systems is theoretically impossible or imminent. The authors actually

14:55.520 --> 15:00.800
quote this open letter from the Association for Mathematical Consciousness Science. And in it,

15:00.800 --> 15:06.240
at the end, the letter says, we emphasize that the rapid development of AI is exposing the urgent

15:06.240 --> 15:11.200
need to accelerate research in the field of consciousness science, even if we develop a

15:11.200 --> 15:16.160
system that ticks all of these indicators. And trust me, someone is probably working on that

15:16.160 --> 15:21.440
right now. We still won't know for sure, and many people will deny forever that that system

15:21.440 --> 15:25.760
is conscious. I'm not claiming I have the answer, by the way. I have absolutely no idea

15:25.760 --> 15:30.080
if these systems are imminently conscious, already conscious, or will never be conscious.

15:30.080 --> 15:35.120
All I can say is that it's a bit less sci-fi than many people believe. And the authors also point

15:35.120 --> 15:40.560
out two risks, under attributing consciousness to AI, playing down the possibility. But they also

15:40.560 --> 15:46.000
point out the risk of over attributing consciousness to AI. On under attributing consciousness,

15:46.000 --> 15:51.280
they say this, given the uncertainties about consciousness mentioned above, we may create

15:51.280 --> 15:56.720
conscious AI systems long before we recognize that we have done so. And I see this sentence as a

15:56.720 --> 16:02.480
fateful prediction. This tendency is further amplified when AI systems exhibit human-like

16:02.480 --> 16:08.000
characteristics, such as natural language processing, which they already do, but also facial expressions

16:08.000 --> 16:12.800
or adaptive learning capabilities. So imagine what people are going to think when photo-realistic

16:12.800 --> 16:19.360
AI avatars are everywhere. And finally, there is the risk of experimentation itself. On balance,

16:19.360 --> 16:24.640
we believe that research to better understand the mechanisms which might underlie consciousness in AI

16:24.640 --> 16:29.840
is beneficial. However, of course, research on this topic runs the risk of building or enabling

16:29.840 --> 16:34.880
others to build a conscious AI system, which should not be done lightly, and that mitigating

16:34.880 --> 16:40.080
this kind of risk should be carefully weighed against the value of better understanding consciousness

16:40.080 --> 16:45.600
in AI. And to tie this back to the start of the video, Google's AI safety experts have added

16:45.680 --> 16:50.160
that some users who grew too dependent on this technology could think it was sentient.

16:50.160 --> 16:56.160
And I do wonder if that's an eventuality, Google's newly merged AI SWOT team is preparing for.

16:56.160 --> 17:00.400
As always, thank you so much for watching to the end and have a wonderful day.

17:00.400 --> 17:06.080
Oh, and just quickly before I end, I now have a discord, AI Explained Community. More info in the

17:06.080 --> 17:06.720
description.

