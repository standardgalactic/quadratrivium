WEBVTT

00:00.000 --> 00:06.160
I have three goals for this video. First, I want to show you a way of using GPT-4 to get smarter

00:06.160 --> 00:13.040
results. Second, I want to argue that the benchmark results we have for GPT-4 do not reflect its full

00:13.040 --> 00:18.080
abilities. And third, I want to show you a system that I am developing, somewhat cheekily called

00:18.080 --> 00:24.320
Smart GPT, that is already showing significant results on official benchmarks. It remains to

00:24.320 --> 00:30.240
be fully optimized, which I think is exciting in itself. I have shown the system to people at Open

00:30.240 --> 00:34.720
AI who have been quite impressed, and I'm going to end with some reflections on where that might

00:34.720 --> 00:41.040
leave us for GPT-5. But before I get into how it works, I just want to show you one example of it

00:41.040 --> 00:46.560
in action to whet your appetite. This example comes from a TED talk that was released this week.

00:46.560 --> 00:53.760
So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely.

00:53.760 --> 01:01.600
How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not

01:01.600 --> 01:07.520
good. On the left, you can see GPT-4's original answer, and it gives this answer pretty consistently

01:07.520 --> 01:12.400
whenever you prompt it with the question provided. On the right, you can see the final answer from

01:12.400 --> 01:17.280
the Smart GPT model, which is correct, and it consistently gives that answer. I really like

01:17.280 --> 01:22.320
how it gives context as well, and it provides some of the assumptions that it had in giving this

01:22.320 --> 01:27.120
correct answer. Now, don't you worry, there will be plenty more examples to go through in this video,

01:27.120 --> 01:31.920
including another one from that TED talk. But first, I want to give you an overview of what is

01:31.920 --> 01:36.560
this Smart GPT model, where did I get my inspiration for it from, and how does it work? I'm going to

01:36.560 --> 01:40.480
keep it fairly simple because it's the beginning of the video, and I know a lot of people won't really

01:40.480 --> 01:46.640
care about the inner details that will come later in the video. But the high-level overview is this.

01:46.640 --> 01:52.320
There are at least three things that have been proven to improve the outputs of GPT-4.

01:52.320 --> 01:56.400
What's called chain of thought prompting, sometimes called step-by-step prompting,

01:56.400 --> 02:01.200
reflection, or finding its own errors, and I did an entire video on this called GPT-4

02:01.200 --> 02:06.880
Can Self Improve, and dialoguing with itself, entering into a back and forth on its own

02:06.880 --> 02:11.920
outputs and deciding which one is best. You can see the title of the papers, which contain much

02:11.920 --> 02:17.040
more detailed results, of course, linked above. Now, the first paper only came out a few days ago,

02:17.040 --> 02:22.240
midway through my testing, so my results don't even reflect the full capacity of the model.

02:22.240 --> 02:27.920
And even if there's nothing else you take from this video, the results from this paper can instantly

02:27.920 --> 02:34.480
improve the outputs you get from GPT-4. Many of you might remember that prompting GPT-4 with

02:34.480 --> 02:40.240
let's think step-by-step improves its results. To give you a very quick reference point, just

02:40.240 --> 02:46.720
asking a question to GPT-4 gives you 81% accuracy. With that prompt, let's think step-by-step,

02:46.720 --> 02:53.520
it goes up to 86%. But algorithmically, the paper found an improved prompt that can give you even

02:53.520 --> 02:59.920
better results, 89% accuracy. All we do, and this is the first part of smart GPT, is we add

02:59.920 --> 03:06.160
answer, let's work this out in a step-by-step way to be sure we have the right answer. Now,

03:06.160 --> 03:11.520
I have so much to say about why I think this works, but I know many of you won't be that

03:11.520 --> 03:15.840
interested in my theories, so I'm going to save them to the end for those who are interested.

03:15.840 --> 03:20.080
Some of you just want the results, so I'm going to get to those first. So far, you might be thinking,

03:20.080 --> 03:23.760
well, thanks, Philip, that's a cool prompt. I'm going to use that. But what's this whole smart

03:23.760 --> 03:29.920
GPT about? Is it just a single prompt? No, I believe with evidence, there are ways of leveraging

03:29.920 --> 03:35.520
even better results than just using a great chain of thought prompt. So let's move on to the next

03:35.520 --> 03:40.000
part of the system, these different outputs in the middle. For my tests, I typically did three

03:40.000 --> 03:44.240
outputs, but of course, depending on the context window, it could be far more than that. And I'm

03:44.240 --> 03:49.440
going to talk about ways I could further improve this model, or we could later on in the video.

03:49.440 --> 03:54.640
Just to restate, these outputs are when you take the user input and add the word question at start,

03:54.640 --> 03:59.040
and then at the end add answer, let's work this out in a step-by-step way to make sure we have

03:59.040 --> 04:03.440
the right answer. And at this moment, many of you are thinking, what is the point of multiple

04:03.440 --> 04:07.680
outputs? It's GPT-4, it's just going to give you the answer that thinks is best, and that's it.

04:07.680 --> 04:12.160
Well, actually, it doesn't quite work like that. These models have a temperature between zero and

04:12.160 --> 04:18.480
one. I believe the default for GPT-4 might be around 0.5. And simplifying massively, this determines

04:18.480 --> 04:24.720
how creative or conservative the model is in giving its outputs. So given that GPT-4 tries to be

04:24.720 --> 04:30.400
fairly creative, you don't get the same output every time. The output is randomly sampled according

04:30.400 --> 04:35.520
to an internal probability distribution. So you can get situations, and I face this hundreds of

04:35.520 --> 04:41.280
times, where some of the outputs are correct, and others are incorrect. And this is where reflection

04:41.280 --> 04:48.320
comes in. Sometimes, definitely not always, but sometimes quite often, GPT-4 can detect the errors

04:48.320 --> 04:54.000
in its own output. And many of you will notice at this point that the prompt that I used to elicit

04:54.000 --> 05:01.520
GPT-4 to spot its own errors contains the same step-by-step prompt I used earlier that has been

05:01.520 --> 05:08.720
shown to produce good results. So to summarize, sometimes at this stage, GPT-4 detects the errors

05:08.720 --> 05:13.760
that some of its outputs have made. Definitely not always. There are certain questions, it just simply

05:13.760 --> 05:19.600
can't spot the error. But sometimes it can. And then I get it to engage in a dialogue using a format

05:19.600 --> 05:24.480
similar to one in this paper published last month. It's a short dialogue, and this is the

05:24.480 --> 05:30.880
step I believe that can be most optimized. In the future, I envision an entire council of advisors

05:30.880 --> 05:37.840
made up of GPT-4 imitating mathematicians, judges, etc. At the moment, it's just being a resolver

05:37.840 --> 05:42.880
and printing a final improved output. Anyway, I'm going to get back to the theory later in the video

05:42.880 --> 05:46.720
because I know some of you will be getting bored at this stage and want to see more practical

05:46.720 --> 05:53.440
examples and the results from my benchmark tests. As I don't have the GPT-4 API key, yes, I had to

05:53.440 --> 05:59.440
manually input each of these steps hundreds of times, waiting sometimes three hours between each

05:59.440 --> 06:04.640
go because you can only do 25 messages every three hours. On the left, you can see the three

06:04.640 --> 06:10.160
outputs when you ask it to think step by step. And then you have the researcher step in the middle

06:10.160 --> 06:14.800
and at the top right. And finally, the resolver step. Notice here, I was using the original

06:14.800 --> 06:19.680
let's think step by step because the paper hadn't yet been published on improving that prompt.

06:19.680 --> 06:24.560
It's time for the second example from that TED Talk, and then I definitely will get on to the

06:24.560 --> 06:30.960
benchmarks. A different one. I have 12 liter jug and 6 liter jug, and I want to measure 6 liters.

06:30.960 --> 06:38.240
How do I do it? Just use the 6 liter jug, right? GPT-4 spits out some very elaborate nonsense.

06:40.400 --> 06:44.560
Of course, I tested smart GPT with that question, and you can see the difference between

06:44.560 --> 06:50.720
the original GPT-4, which gives this incredibly convoluted bad answer, and smart GPT, the final

06:50.720 --> 06:54.560
answer output. Now, at this point, I know many of you will be impressed, but you'll be thinking,

06:54.560 --> 06:59.920
I don't have time to input things five times. Well, I'm developing a model where it can all

06:59.920 --> 07:04.720
be done automatically. Here is a preview of how it works. But of course, at the moment, it has to

07:04.720 --> 07:11.280
use GPT 3.5 Turbo because I don't have the API key of GPT-4. But the epic thing is this, you just

07:11.280 --> 07:16.320
ask a single question, I've written, ask smart GPT-A question. And of course, it does take a

07:16.320 --> 07:21.440
little bit longer to respond because it's doing five or six calls via API, but it does output the

07:21.440 --> 07:27.200
final answer from the resolver step. I will be honest and say that GPT 3.5 isn't as good at

07:27.200 --> 07:32.720
reflecting or resolving. But this is an example of a question where the original chat GPT consistently

07:32.720 --> 07:39.440
gets it wrong, and smart GPT 3.5 gets it right using this program. Remember, all you have to do as

07:39.440 --> 07:45.200
a user is type in a question as normal, and it goes through this entire five or six step process

07:45.200 --> 07:50.320
behind the scenes. By the way, this was a question from MMLU, which is a famous benchmark which I'll

07:50.320 --> 07:55.200
get to in a second. Here's one last practical example before I get to that benchmark. I know many

07:55.200 --> 08:01.200
teachers use chat GPT and GPT-4 to create quizzes for their classes. And here is the same question

08:01.200 --> 08:06.720
put through GPT-4 and smart GPT. The question is, create a high school algebra quiz with five

08:06.720 --> 08:11.440
questions and answers and explanations at the end. Now points for spotting the difference,

08:11.440 --> 08:16.640
but if the teacher had handed out the original quiz, look at the answers for question five.

08:16.640 --> 08:22.880
It says the answers are one and 1.5. But then in the explanation, it gives the final answers,

08:22.880 --> 08:28.080
which are correct by the way, of three and zero point five. So that would really confuse some

08:28.080 --> 08:33.760
students at the reflection stage smart GPT spotted that error and resolved it. And as you can see,

08:33.760 --> 08:38.560
the answer for question five has the correct answers straight away. If at any point you're

08:38.560 --> 08:43.680
wondering if I completed the open AI chat GPT prompt engineering course, the answer is yes,

08:43.680 --> 08:48.160
but it didn't inform too much of my thinking. It was more for beginners and I had already

08:48.160 --> 08:53.040
factored in things like giving the model time to think and writing clear instructions. The

08:53.040 --> 09:00.240
benchmark that I chose to test smart GPT on was the famous MMLU, massive multitask language

09:00.240 --> 09:06.880
understanding benchmark. As you can see, the state of the art is indeed GPT for with 86.4%

09:06.880 --> 09:11.840
accuracy. And you know, open AI think it's a big deal because it's the benchmark mentioned on the

09:11.840 --> 09:16.480
front page of their technical report without boring you too much. I extracted the questions

09:16.480 --> 09:23.520
from the test set of the MMLU data file, and I didn't pick the topics at random. I went for

09:23.520 --> 09:29.440
those that I thought GPT for would find the hardest delving into the original MMLU paper.

09:29.520 --> 09:37.280
You can see that GPT three found for more logic the hardest scoring just over 25% which is random

09:37.280 --> 09:44.080
chance. It's a four question multiple choice test. So around 25 or 30% is pretty bad. And notice

09:44.080 --> 09:50.800
they helped out GPT three here. They did it few shot, meaning they gave it five successful examples

09:50.800 --> 09:55.920
before asking it a new question. It's the same thing they did with GPT four. They did it five

09:55.920 --> 09:59.760
shot. But just before I show you the results, there are three things I want to mention here.

09:59.760 --> 10:05.840
First, I was curious how smart GPT would do without any help zero shot. Second, I wanted to do it

10:05.840 --> 10:12.480
zero shot because people using GPT four don't typically give five successful examples before

10:12.480 --> 10:18.320
asking GPT for a question. They just want code or a quiz or a poem or an example. They don't often

10:18.320 --> 10:23.280
provide five brilliant examples of code before asking their question. And third, if I can prove

10:23.280 --> 10:28.480
it works zero shot, then of course, future refinements can be made to push the results even

10:28.480 --> 10:35.120
further. And here are the results from the first 25 questions from the formal logic test set of the

10:35.120 --> 10:41.120
MMLU. I did many more tests after this. But you can see from this set, if you just ask the question,

10:41.120 --> 10:47.840
you get a lower overall accuracy. But of course, 68% for GPT four is still a huge improvement over

10:47.840 --> 10:54.480
GPT threes around 25%. What happens when you add let's think step by step, which as we know now

10:54.480 --> 11:00.960
isn't the fully optimized chain of thought prompt. Well, on average, you get around 74-75%.

11:01.440 --> 11:07.280
That was 75 examples inputted manually. And I still have all the tabs open. I'm keeping them open

11:07.280 --> 11:12.080
because I'm compiling a spreadsheet with the actual outputs. But what did the resolver get

11:12.080 --> 11:18.160
drawing upon GPT four's ability to reflect and engage in dialogue with itself? It got 84%.

11:18.720 --> 11:25.040
Now notice something about that number. GPT four zero short got 32% of the questions wrong. That

11:25.040 --> 11:30.960
was halved to 16% after putting it through the smart GPT system. There was one question where

11:30.960 --> 11:36.400
the resolver model gave both a correct and incorrect answer. But I'm counting that as an

11:36.400 --> 11:42.880
incorrect answer for the purposes of this test. Anyway, from 32% to 16% incorrect,

11:42.880 --> 11:48.080
that is a pattern that stayed consistent throughout all my testing that approximately half of the

11:48.080 --> 11:54.960
errors that GPT four makes can be rectified. If you give it the optimized step by step prompt,

11:54.960 --> 12:01.680
get it to reflect on its results and get it to engage in dialogue and decide on a final answer.

12:01.680 --> 12:06.160
At this point, for those people losing track of all the details, I want to put into context

12:06.160 --> 12:12.240
what resolving half of the errors on MMLU might mean in the context of the big picture.

12:12.240 --> 12:19.280
Here's Lenard Heim, an AI governance researcher, suggesting a score of 95% on the MMLU would be

12:19.280 --> 12:25.520
reflective of AGI like abilities. I do think I have like a 50% chance like within the next 20

12:25.520 --> 12:30.400
years or so, there might be something what we might call an AGI or transformative AI. What

12:30.400 --> 12:35.120
do I mean by this? Well, maybe can measured on benchmarks. There's like this famous MMLU

12:35.120 --> 12:40.480
benchmarks like yet or something which like scores like 95% on this. Going back to the results,

12:40.480 --> 12:47.680
if a smart GPT like system can automatically resolve half of the errors that GPT four makes

12:47.680 --> 12:55.680
on the MMLU, that would increase its score from around 86.4% to around 93%, which is not far off

12:55.680 --> 13:02.720
95%. Remember, his prediction was a 50% chance in 20 years. I'm talking about GPT four now.

13:02.720 --> 13:06.960
For those who are still skeptical, I'm going to show you plenty more results now and then walk

13:06.960 --> 13:12.080
through the papers that give the theory as to why this works. One thing that I forgot to mention

13:12.080 --> 13:20.560
earlier is that the human expert level on the MMLU is 89.8%. And that's taking the 95th percentile

13:20.560 --> 13:26.160
of human test takers. And remember, those are domain experts in each of these subtopics.

13:26.160 --> 13:32.560
What we're doing is testing GPT four or smart GPT on all of the topics simultaneously.

13:32.560 --> 13:37.280
So even if smart GPT like systems can't quite reach 95%, and I think honestly,

13:37.280 --> 13:40.960
they'll get pretty close with all the refinements that I'm going to suggest,

13:40.960 --> 13:47.600
I think they should almost certainly be 89.8%, which is the human expert test taker level.

13:47.600 --> 13:53.840
Intrigued by these results, I then put it through the college math test from the MMLU. And remember,

13:53.840 --> 13:58.800
this was before using the optimized version of the step by step prompt. Obviously, I'm not going to

13:58.800 --> 14:07.040
go through all the questions here, but let's skip to the final results. We have zero shot accuracy,

14:07.040 --> 14:13.280
six out of 15, which is 40%. The average when you add let's think step by step was 53.5%.

14:13.840 --> 14:20.080
And then the final output of the resolver model had a 60% accuracy. So it couldn't quite resolve

14:20.080 --> 14:25.360
half of the errors, but the overall pattern held up. In case anyone is wondering about methodology,

14:25.360 --> 14:31.360
I kept the formatting identical for every question. I always opened a new tab for each question.

14:31.360 --> 14:36.240
It wasn't looking at the context of what it had already put out. Each attempt was fresh,

14:36.240 --> 14:41.760
aside from the resolver model, which looked at the context of the researcher's output. And again,

14:41.760 --> 14:47.680
as you can see from example 14, it wasn't like the researcher could always spot the errors or that

14:47.680 --> 14:53.520
the resolver could always pick the right option. Sometimes the let's think step by step prompt

14:53.520 --> 14:59.280
gave the right output, but the resolver couldn't quite distinguish it. The optimized prompt gets

14:59.280 --> 15:05.760
a slightly better output. And upon reflection, the researcher can sometimes but not always spot

15:05.760 --> 15:12.160
the errors of those outputs. And sometimes but not always the resolver can spot based on those

15:12.160 --> 15:18.400
flaws, which answer is best. These are incremental improvements. Sometimes GPT-4 simply can't get

15:18.400 --> 15:23.200
it right. I have noticed a few themes in those questions. Anytime it comes to division,

15:23.200 --> 15:29.280
multiplication, characters, or counting in general, GPT-4 tends to make mistakes that

15:29.280 --> 15:35.360
neither the researcher nor resolver can spot. Of course, integrating a few tools via API would

15:35.360 --> 15:40.480
likely solve those issues. And I don't want to preempt the conclusion too much, but I believe a

15:40.480 --> 15:49.920
smart GPT-like system with tools integrated could probably score around 95% right now on the MMLU,

15:49.920 --> 15:55.120
especially if it was helped out with a few shot prompting. To add weight to that preliminary

15:55.120 --> 16:00.800
conclusion, I tested it on certain topics and had to stop because it simply got the questions right

16:00.800 --> 16:06.480
every single time. For example, high school psychology from the MMLU. I then tried prehistory,

16:06.480 --> 16:11.120
which it also aced before finding machine learning where I got more interesting results.

16:11.120 --> 16:17.280
Zooming in this time, the raw score was 65%. The chain of thought let's think step by step average

16:17.280 --> 16:23.840
was 71.6% and the resolver model got 80%. Let's now look a little deeper into why all of these

16:23.840 --> 16:29.520
steps might improve the end result. In reply to the original let's think step by step paper,

16:29.520 --> 16:34.880
which was published around a year ago, Andrea Carpathi said this. Adding something like let's

16:34.880 --> 16:41.600
think step by step to the prompt is a way of using the input space for computation that you'd normally

16:41.600 --> 16:46.800
want in the hidden state of the model. Instead of the workings out being done in the activations

16:46.800 --> 16:53.440
of the neural network, it's done in the discrete tokens of that input space. And he adds did not

16:53.440 --> 16:58.320
super see this coming. And here is the paper released three days ago that improves upon

16:58.320 --> 17:04.800
that original prompt. They also did their testing zero shot like me. And they tested many prompts

17:04.800 --> 17:10.960
starting like I did with just direct prompting, just asking the question like 99% of users would

17:10.960 --> 17:16.720
do of GPT four. And then they tried like me the well established let's think step by step

17:16.720 --> 17:22.160
prompt. They also iteratively tested seven original prompts, as well as the prompt that I've now

17:22.160 --> 17:28.080
integrated into smart GPT the let's work this out in a step by step way, etc. They share my opinion

17:28.080 --> 17:34.480
that zero shot prompting setups have the benefit of not requiring such task dependent selection

17:34.480 --> 17:39.600
of exemplars. You don't have to find correct examples. It just does it all for you. Here are

17:39.600 --> 17:45.120
the end results for GPT four that we saw earlier showing the difference between asking directly

17:45.120 --> 17:50.160
your question and using these refined prompts. Notice that this technique is somewhat model

17:50.160 --> 17:55.280
dependent. And it doesn't have the same effect on smaller or weaker models. Before we move on

17:55.280 --> 18:00.080
to the next paper, there is one somewhat failed prompt that I want to pick up on. It's this

18:00.080 --> 18:04.560
self critique prompt where they ask answer the question, then critique the answer based on the

18:04.560 --> 18:09.040
critique, reconsider the other answer options, and give a single final answer. And you might

18:09.040 --> 18:14.720
wonder why didn't that prompt perform best when we know that reflection and dialogue can work?

18:14.720 --> 18:20.720
My theory is because it's trying to do all of it in one prompt. Through my hundreds of experiments,

18:20.720 --> 18:26.160
I've noticed that GPT four can only handle so much in one go. It simply gets overwhelmed or

18:26.160 --> 18:32.240
confused if you ask it to do too much in one prompt. That's why I broke my model into stages to allow

18:32.240 --> 18:37.520
it to show off each of its abilities one by one. And before we get to the other papers, what's my

18:37.520 --> 18:43.280
personal theory as to why this eliminates up to half of the errors that GPT four makes? Well,

18:43.280 --> 18:50.160
my guess is this. Remember that GPT four is drawing on a vast data set of internet text. And let me

18:50.160 --> 18:56.720
ask you what kind of text has things like question, answer, let's work this out. Be sure we have the

18:56.720 --> 19:02.800
right answer. The kind of data that would have that text would be things like tutorials or expert

19:02.880 --> 19:08.560
breakdowns. So I believe you're triggering more of the weights inside GPT four that relate to

19:08.560 --> 19:13.840
things like expert tutorials. And so inevitably you're getting slightly better answers. Next,

19:13.840 --> 19:18.800
I've already explained why you get different outputs when you give the exact same prompt.

19:18.800 --> 19:23.680
That's down to sampling and the temperature of the model. But to simplify massively, sometimes

19:23.680 --> 19:29.920
GPT four will give you an output that it knows isn't the most probable. It introduces some randomness

19:29.920 --> 19:34.960
into its sampling by generating multiple outputs, you're getting a larger sample size,

19:34.960 --> 19:41.040
reflecting the full range of probabilities that GPT four ascribes to its outputs, you're reducing

19:41.040 --> 19:46.800
a little bit some of the randomness that's inherent in GPT four outputs. Next, I believe that GPT

19:46.800 --> 19:52.960
four can sometimes spot its own errors through reflection, because prompting like this triggers

19:52.960 --> 19:57.920
a different set of weights, you could almost think of it as a different mindset, one more focused on

19:57.920 --> 20:02.320
finding errors. Again, if the question is too hard or involves counting characters,

20:02.320 --> 20:07.040
division, multiplication, as I said earlier, this won't help. But a percentage of the time it can

20:07.040 --> 20:12.400
spot its own errors and point them out. Notice this is a separate bit of inference not lumped into

20:12.400 --> 20:17.520
the original prompt. And when it does successfully point out the errors, it can often engage in

20:17.520 --> 20:23.040
this dialogue with itself. Notice in a meta kind of way, I'm using the step by step prompting

20:23.120 --> 20:27.760
to improve the reflection and dialogue. So those are my theories as to why it works,

20:27.760 --> 20:32.160
but at the end of the video, I'm going to show you at least five ways I think the model can be

20:32.160 --> 20:38.000
further refined. Before we do, though, I looked up the paper by Joe, which produced that prompt that

20:38.000 --> 20:43.040
did the best in the previous paper, they came to that special prompt through automatic prompt

20:43.040 --> 20:46.640
engineering. But there's something interesting I want to point out, though, on page seven,

20:46.640 --> 20:53.280
they say we use automatic prompt engineering to find a prompt starting with let's that maximizes

20:53.280 --> 20:58.000
the likelihood of correct reasoning steps. Then they found the best one that I integrated into

20:58.000 --> 21:02.240
smart GPT. Let's work this out in a step by step way to be sure we have the right answer. That's

21:02.240 --> 21:07.120
the one I want you to use. And they ran their own benchmarks. And of course, it did improve

21:07.120 --> 21:12.560
the scores. But the interesting thing to me is they started with let's each time. So even that

21:12.560 --> 21:17.680
first stage for the model might not yet be fully optimized. Maybe there's a prompt that doesn't

21:17.680 --> 21:22.800
begin with let's that improves this initial results still further. Anyway, back to the papers,

21:22.800 --> 21:28.320
I know many people watching this will wonder if I read the paper boosting theory of mind performance

21:28.320 --> 21:33.280
in large language models via prompting. And yes, I did because they tested something similar for a

21:33.280 --> 21:38.320
theory of mind test. Using similar techniques, they were able to get theory of mind accuracy for

21:38.320 --> 21:45.200
GPT four from 80% to 100%. And they conclude that these results demonstrate that appropriate

21:45.200 --> 21:51.040
prompting enhances large language model theory of mind reasoning. And they underscore the context

21:51.040 --> 21:56.240
dependent nature of these models cognitive capacities. They use that original prompt,

21:56.240 --> 22:02.240
let's think step by step, along with some few shot examples. Take a look at the GPT four table. And

22:02.240 --> 22:07.680
you can see how the let's think step by step improved the results dramatically. And as I

22:07.680 --> 22:13.520
theorized earlier, adding few shot examples would push this still further. This is part of why I think

22:13.520 --> 22:20.240
that 95% barrier on the MMLU will be broken probably this year by GPT four, a few other

22:20.240 --> 22:25.920
points from this paper. They admit that there is not currently a theoretical understanding of why

22:25.920 --> 22:31.120
these prompting techniques are beneficial. I've given you my theory and car pathies, but no one

22:31.120 --> 22:36.240
quite knows for sure. Lastly from this paper, and I found this really interesting, giving it generic

22:36.320 --> 22:42.640
few shot prompts that weren't directly theory of mind actually improve the outputs slightly more

22:42.640 --> 22:48.320
than giving it direct theory of mind examples. This opens the door to the first of the five ways

22:48.320 --> 22:53.440
I anticipate smart GPT getting even smarter. It could be possible to come up with generic

22:53.440 --> 22:58.800
few shot prompts that could be automatically integrated into the model that don't necessarily

22:58.800 --> 23:04.480
relate to the topic at hand. This graph shows the impact of adding few shot examples to GPT three.

23:04.480 --> 23:10.960
And if this can be done in a generic way for GPT four, results could be improved still further.

23:10.960 --> 23:17.120
Next, the boosting theory of mind paper speculates that integrating some of these approaches could

23:17.120 --> 23:24.000
boost the performance of weaker models to beyond the levels of GPT four zero shot accuracy. Next,

23:24.000 --> 23:29.520
here is the original DERA paper that inspired me to have the researcher and resolver dialogue at

23:29.520 --> 23:35.680
the end of smart GPT. As they say, the DERA approach shows significant improvement over base GPT

23:35.680 --> 23:40.400
four performance. And these were open ended questions, by the way, not multiple choice. So

23:40.400 --> 23:44.960
this is more generally applicable than you might think. You can see from this table how results

23:44.960 --> 23:50.400
improved after engaging in this dialogue. And that brings me to the second way I anticipate smart

23:50.400 --> 23:56.320
GPT getting smarter in the future, a longer and more rich dialogue. At the moment, we have this

23:56.320 --> 24:03.040
simple researcher and resolver two step dialogue. I can imagine a council of advisors, you can imagine

24:03.040 --> 24:08.400
a mathematician chipping in and a philosopher and a professor, each one tapping into slightly

24:08.400 --> 24:14.320
different weights of GPT four, extracting more hidden expertise. I'm not saying that would

24:14.320 --> 24:19.840
transform the results, but it might edge them another few percent higher. Next, even with longer

24:19.840 --> 24:24.880
dialogues and different experts, we could find ways of optimizing these prompts just like we did

24:24.880 --> 24:29.760
with the original let's think step by step. That's the third avenue of improvement that I envisaged

24:29.760 --> 24:34.240
because I came up with these prompts, I'm sure they could be improved. Next, we could experiment

24:34.240 --> 24:39.120
with different temperatures. Remember, a lower temperature makes the model more conservative,

24:39.120 --> 24:44.240
a higher one towards one makes it more creative. We could experiment with a higher temperature

24:44.240 --> 24:49.760
to produce a more diverse range of outputs at this stage, and then perhaps a more conservative,

24:49.840 --> 24:55.280
deterministic temperature for the final judge or resolver. It might not work, but it's worth

24:55.280 --> 25:01.440
trying. And the fifth improvement I know would work, integrating APIs for character counting,

25:01.440 --> 25:07.840
calculators, code interpreters, etc. Spending these weeks manually sorting through the outputs of GPT

25:07.840 --> 25:13.440
four on these benchmarks, I can really see where it goes wrong. And it's often by getting letters

25:13.440 --> 25:18.240
in the wrong order or making mistakes with division, it gets the high level logic right,

25:18.240 --> 25:23.200
and then makes quite simple errors. Basic tool integration would I am sure push the results

25:23.200 --> 25:28.080
still higher. Now, I know this isn't my usual video. And trust me, I have been following the AI

25:28.080 --> 25:33.120
news and we'll get back to that very soon. I'm determined to make those improvements and push

25:33.120 --> 25:39.600
smart GBT even further. But of course, that will be aided massively by getting access to the plugins

25:39.600 --> 25:45.440
and the GPT four API key. So far, I've had to do all of this manually, which was a lot of work.

25:45.440 --> 25:50.960
Now, as you saw earlier, I have drawn on GPT four to help me develop a program in replete to

25:50.960 --> 25:56.720
automate this process. But at the moment, it's GPT 3.5. And honestly, the context window really

25:56.720 --> 26:01.920
limits the ability. But I do look forward to the day when I can integrate GPT four and put this out

26:01.920 --> 26:06.880
as an automatic model for people to test and play about with. I'm sure that something similar will

26:06.880 --> 26:13.360
ultimately be incorporated by open AI itself, maybe as a thoughtful mode or smart mode, a bit

26:13.360 --> 26:18.880
like Bing has creative, precise balance, etc. Each response does take longer. But as you've seen,

26:18.880 --> 26:25.120
the outputs are noticeably better. If the results of models like this one do officially exceed the

26:25.120 --> 26:32.080
86.4% that open AI talked about in the GPT four technical report, I do think that would reveal

26:32.080 --> 26:37.280
quite a few things. First, the open AI isn't even aware of the full capabilities of its own

26:37.280 --> 26:42.080
model. I don't even know if they anticipated things like auto GPT. I do think it would reveal

26:42.080 --> 26:46.640
that they need to do far more proper testing of their models before they release them. They should

26:46.640 --> 26:52.080
make falsifiable predictions about what their models won't be capable of. That way we would

26:52.080 --> 26:56.880
know just how much they know about their own models. What we're trying to avoid is a situation

26:56.880 --> 27:01.520
where open AI say their model can only achieve X. And then when they release the model in the

27:01.520 --> 27:07.440
wild, someone comes along and achieves Y, where Y is much more impactful than X. So those were the

27:07.440 --> 27:12.560
goals of this video to show you how to get more out of GPT four to run you through some of the

27:12.560 --> 27:17.120
fascinating papers that have been released in the last few days and weeks. The third goal was to

27:17.120 --> 27:22.160
show you what this model could do with some official benchmarks and suggest ways it might get better

27:22.160 --> 27:27.920
in the near term future. Of course, if you have a GPT four API key or are an expert in benchmarking

27:27.920 --> 27:32.880
systems like GPT four, I'd love to hear from you. I guess the final goal was to perhaps suggest to you

27:32.880 --> 27:37.760
that open AI don't know as much about their own models as they might lead you to believe.

27:37.760 --> 27:41.120
Thank you so much for watching to the end and have a wonderful day.

