{"text": " Less than 24 hours ago, Google released the Palm 2 technical report. I have read all 92 pages, watched the Palm 2 presentation, read the release notes and have already tested the model in a dozen ways. But before getting into it all, my four main takeaways are these. First, Palm 2 is competitive with GPT-4 and while it is probably less smart overall, it's better in certain ways and that Second, Google is saying very little about the data it used to train the model or about parameters or about compute, although we can make educated guesses on each. Third, Gemini was announced to be in training and will likely rival GPT-5 while arriving earlier than GPT-5. As you probably know, Sam Orman said that GPT-5 isn't in training and won't be for a long time. Fourth, while dedicating 20 pages to bias, toxicity and misgendering, there wasn't a single page on AI impacts more broadly. Google boasted of giving Gemini planning abilities in a move that, surprise as I am to say it, makes open AI look like paragons of responsibility. So a lot to get to, but let's look at the first reason that Palm 2 is different from GPT-4. On page 3, they say we designed a more multilingual and diverse pre-training mixture, extending across hundreds of languages and domains like programming, mathematics, etc. So because the text that they train Palm 2 on is different to the text that open AI trained GPT-4 on, it means that those models have different abilities and I would say Palm 2 is better at translation and linguistics and in certain other areas which I'll get to shortly. If that's data, what about parameter count? Well, Google never actually say they only use words like it's significantly smaller than the largest Palm model, which was 540 billion parameters. Sometimes they say significantly, other times dramatically. Despite this, it significantly outperforms Palm on a variety of tasks. To all the references you may have seen to imminent 100 trillion parameter models were bogus. Skipping ahead to page 91 out of 92, in the model summary, they say further details of model size and architecture are withheld from external publication. But earlier on, they did seem to want to give hints about the parameter count inside Palm 2, which OpenAI never did. Here they present the optimal number of parameters given a certain amount of compute flops. Scaling this up to the estimated number of flops used to train Palm 2, that would give an optimal parameter count of between 100 and 200 billion. That is a comparable parameter count to GPT-3 while getting competitive performance with GPT-4. Bard is apparently now powered by Palm 2 and the inference speed is about 10 times faster than GPT-4 for the exact same prompt. And I know there are other factors that influence inference speed, but that would broadly fit with an order of magnitude fewer parameters. This has other implications, of course, and they say that Palm 2 is dramatically smaller, cheaper, and faster to serve. Not only that, Palm 2 itself comes in different sizes, as Sundar Pichai said. Palm 2 models deliver excellent foundational capabilities across a wide range of sizes. We have affectionately named them Gecko, Otter, Bison, and Unicon. Gecko is so lightweight that it can work on mobile devices fast enough for great interactive applications on-device, even when offline. I would expect Gecko to soon be inside the Google Pixel phones. Going back to data, Google cryptically said that their pre-training corpus is composed of a diverse set of sources, documents, books, code, mathematics, and conversational data. I've done a whole video on the data issues that these companies face, but suffice to say they're not saying anything about where the data comes from. Next, they don't go into detail, but they do say that Palm 2 was trained to increase the context length of a model significantly beyond that of Palm. As of today, you can input around 10,000 characters into BARD, but they end this paragraph with something a bit more interesting. They say, without demonstrating, our results show that it is possible to increase the context length of the model without hurting its performance on generic benchmarks. The bit about not hurting performance is interesting because in this experiment published a few weeks ago about extending the input size in tokens up to around 2 million tokens, the performance did drop off. If Google had found a way to increase the input size in tokens and not affect performance, that would be a breakthrough. On multilingual benchmarks, notice how the performance of Palm 2 in English is not dramatically better than in other languages. In fact, in many other languages, it does better than in English. This is very different to GPT-4, which was noticeably better in English than in all other languages. As Google hinted earlier, this is likely due to the multilingual text data that Google trained Palm 2 with. In fact, on page 17, Google admit that the performance of Palm 2 exceeds Google Translate for certain languages, and they show on page 4 that it can pass the mastery exams across a range of languages like Chinese, Japanese, Italian, French, Spanish, German, etc. Look at the difference between Palm 2 and Palm in red. Now, before you rush off and try BARD in all of those languages, I tried that and apparently you can only use BARD at the moment in the following languages, English, US English, what a pity, and Japanese and Korean. But I was able to test BARD in Korean on a question translated via Google Translate from the MMLU dataset. It got the question right in each of its drafts. In contrast, GPT-4 not only got the question wrong in Korean, when I originally tested it for my smart GPT video, it got the question wrong in English. In case any of my regular viewers are wondering, I am working very hard on smart GPT to understand what it's capable of and getting it benchmarked officially, and thank you so much for all the kind offers of help in that regard. I must admit it was very interesting to see on page 14 a direct comparison between Palm 2 and GPT-4, and Google do admit for the Palm 2 results they use chain of thought prompting and self-consistency. Reading the self-consistency paper did remind me quite a lot actually of smart GPT, where it picks the most consistent answer of multiple outputs. So I do wonder if this comparison is totally fair if Palm 2 used this method and GPT-4 didn't. I'll have to talk about these benchmarks more in another video, otherwise this one would be too long, but a quick hint is that Wynogrand is about identifying what the pronoun in a sentence refers to. Google also weighed into the emerging abilities debate, saying that Palm 2 does indeed demonstrate new emerging abilities. They say it does so in things like multi-step arithmetic problems, temporal sequences, and hierarchical reasoning. Of course I'm going to test all of those and have begun to do so already, and in my early experiments I'm getting quite an interesting result. Palm 2 gets a lot of questions wrong that GPT-4 gets right, but it can also get questions right that GPT-4 gets wrong, and I must admit it's really weird to see Palm 2 getting really advanced college-level math questions right, that GPT-4 gets wrong. And yet also when I ask it a basic question about prime numbers, it gets it kind of hilariously wrong. Honestly I'm not certain what's going on there, but I do have my suspicions. Remember though that recent papers have claimed that emergent abilities are a mirage, so Google begs to differ. When Google put Palm 2 up against GPT-4 in high school mathematics problems, it did outperform GPT-4, but again it was using an advanced prompting strategy, not 100% different from smart GPT, so I wonder if the comparison is quite fair. What about coding? Well again it's really hard to find a direct comparison that's fair between the two models. Overall I would guess that the specialized coding model of Palm, what they call Palm 2S, is worse than GPT-4. It says it's pass at one accuracy, as in past first time, is 37.6%. Remember the Sparks of AGI paper? Well that gave GPT-4 as having an 82% zero shot pass at one accuracy level. However as I talked about in the Sparks of AGI video, the paper admits that it could be that GPT-4 has seen and memorized some or all of human eval. There is one thing I will give Google credit on, which is that their code now sometimes references where it came from. Here is a brief extract from the Google keynote presentation. How would I use Python to generate the Scholar's Mate move in chess? Okay here Bard created a script to recreate this chess move in Python, and notice how it also formatted the code nicely, making it easy to read. We've also heard great feedback from developers about how Bard provides code citations, and starting next week you'll notice something right here. We're making code citations even more precise. If Bard brings in a block of code, just click this annotation, and Bard will underline the block and link to the source. As always, it seems the appendix contained more interesting information sometimes than the main body of the technical report. For example, we get a direct and fair comparison between GPT-4 and palm 2, or I should say flan palm 2. That is the instruction fine tuned version of palm 2. Essentially that's the version where it's been fine tuned to get better at following a question and answer format. But anyway, the original palm 2 scored 78.3%, and flan palm 2 scored 81.2%. That's below the 86.4% of GPT-4. And that's why my broad conclusion is that GPT-4 is a bit smarter than palm 2. But as I'll be showing over the coming days and weeks, there are genuinely quite a few areas in which palm 2 is better than GPT-4. What about the big bench, which was designed to be particularly tough for language models? I talked a lot about this in my earliest videos. Well, the graph is going to look pretty weird because palm 2 has improved upon palm while reducing the number of parameters. So the graph kind of doubles back on itself back up here up to around 69% according to the technical report. I would say this is quite a major moment in human history. There is now virtually no language task that the average human can do better than palm 2. Of course, expert humans can do better in individual domains, but the average human is now worse in virtually every domain of language. Here you can see that confirmation of the big bench hard results for flan palm 2, 69.1%. Interestingly, in the original chart, palm 2 is even claimed to have higher performance than that at 78.1%. If you remember, the reason we can't compare that to GPT-4 is that in the technical report for GPT-4, they admit that during their contamination check, we discovered that portions of big bench were inadvertently mixed into the training set and we excluded it from our reported results. Before we get to Gemini, Google show off in the latter half of the technical report with examples of linguistic ability, like writing paragraphs in Tejiki and then translating them into Persian. They go on to show examples in Tamil and they are really making a big point of showing off its multilingual capabilities. At this point, and I'm going to admit this is my personal opinion, Google then strays into dozens of pages on bias, toxicity and gender. Interestingly, some of the people paid to assess these risks were paid only 1.5 cents per judgment. These things do need to be addressed, of course, but it was somewhat shocking to me to see 20 pages of that and not a single page on the broader AI impacts. As many of you may know, I have criticized open AI plenty of times on this channel, but compare their technical report, which goes into far more detail about what we need to monitor. The closest Google got was showing how their universal translator could be used for deep fakes. Universal Translate is an experimental AI video dubbing service that helps experts translate a speaker's voice while also matching their lip movements. Let me show you how it works with an online college course created in partnership with Arizona State University. What many college students don't realize is that knowing when to ask for help and then following through and using helpful resources is actually a hallmark of becoming a productive adult. It just seems a massive black hole when one of their recent former employees, Jeffrey Hinton, had this to say this week on CNN. You've spoken out saying that AI could manipulate or possibly figure out a way to kill humans? How could it kill humans? If it gets to be much smarter than us, it'll be very good at manipulation because it will have learned that from us. And a very few examples of a more intelligent thing being controlled by a less intelligent thing. And it knows how to program, so it'll figure out ways of getting around restrictions we put on it. It'll figure out ways of manipulating people to do what it wants. It's not clear to me that we can solve this problem. I believe we should put a big effort into thinking about ways to solve the problem. I don't have a solution at present. I just want people to be aware that this is a really serious problem and we need to be thinking about it very hard. This all seems particularly relevant when Google made this announcement about Gemini, their rival to GPT-5. All this helps set the stage for the inflection point we are at today. We recently brought these two teams together into a single unit, Google DeepMind. Using the computational resources of Google, they are focused on building more capable systems safely and responsibly. This includes our next generation foundation model, Gemini, which is still in training. Gemini was created from the ground up to be multi-modal, highly efficient at tool and API integrations, and built to enable future innovations like memory and planning. That ability to plan may ring a bell from the GPT-4 which said this, novel capabilities often emerge in more powerful models. Some that are particularly concerning are the ability to create and act on long-term plans. Remember, Google didn't identify planning as a risk but as a selling point for Gemini. Next, Google talked about accelerating their progress, which was again directly mentioned in the GPT-4 technical report. It said, one concern of particular importance to open AI is the risk of racing dynamics leading to a decline in safety standards, the diffusion of bad norms and accelerated AI timelines, each of which heightens societal risks associated with AI. We refer to these here as acceleration risk and make no mistake, Gemini will be very accelerated from Palm II. It looks set to use the TPU V5 chip, which was announced back in January of last year. And on page 91 of the Palm II technical report, they say that that model used TPU V4. Now, it should be said that Palm II is leading to some impressive medical applications. As I actually first reported on seven weeks ago without quite realizing it, here's MedPalm II. We believe large language models have the potential to revolutionize healthcare and benefit society. MedPalm is a large language model that we've taken and tuned for the medical domain. Medical question answering has been a research grand challenge for several decades but till date the progress has been kind of slow. But then over the course of the last three to four months, first with MedPalm and MedPalm II, we have kind of like broken through that barrier. Unlike previous versions, MedPalm II was able to score 85% on the USMLA medical licensing exam. Yeah, this is immensely exciting because people have been working on medical question answering for over three decades. And finally, we are at a stage where we can say with confidence that AI systems can now at least answer USMLA questions as good as experts. As many of you may know, the CEO of Google as well as the CEO of Microsoft and Sam Altman and the CEO of Anthropic all went to the White House to discuss AI risk and opportunity. But given that the main outcome from that seems to be 140 million to establish seven new AI research institutes, that feels a little slow given all the acceleration that's occurring. Because as Google somewhat soberly conclude their report, we believe that further scaling of both model parameters and data set size and quality as well as improvements in the architecture and objective will continue to yield gains in language understanding and generation. They are not slowing down and the world hasn't yet caught up. Thank you so much for watching to the end and have a wonderful day.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.96, "text": " Less than 24 hours ago, Google released the Palm 2 technical report. I have read all 92 pages,", "tokens": [50364, 18649, 813, 4022, 2496, 2057, 11, 3329, 4736, 264, 32668, 568, 6191, 2275, 13, 286, 362, 1401, 439, 28225, 7183, 11, 50712], "temperature": 0.0, "avg_logprob": -0.09973406284413439, "compression_ratio": 1.556, "no_speech_prob": 0.014717641286551952}, {"id": 1, "seek": 0, "start": 6.96, "end": 12.56, "text": " watched the Palm 2 presentation, read the release notes and have already tested the model in a dozen", "tokens": [50712, 6337, 264, 32668, 568, 5860, 11, 1401, 264, 4374, 5570, 293, 362, 1217, 8246, 264, 2316, 294, 257, 16654, 50992], "temperature": 0.0, "avg_logprob": -0.09973406284413439, "compression_ratio": 1.556, "no_speech_prob": 0.014717641286551952}, {"id": 2, "seek": 0, "start": 12.56, "end": 19.2, "text": " ways. But before getting into it all, my four main takeaways are these. First, Palm 2 is competitive", "tokens": [50992, 2098, 13, 583, 949, 1242, 666, 309, 439, 11, 452, 1451, 2135, 45584, 366, 613, 13, 2386, 11, 32668, 568, 307, 10043, 51324], "temperature": 0.0, "avg_logprob": -0.09973406284413439, "compression_ratio": 1.556, "no_speech_prob": 0.014717641286551952}, {"id": 3, "seek": 0, "start": 19.2, "end": 25.52, "text": " with GPT-4 and while it is probably less smart overall, it's better in certain ways and that", "tokens": [51324, 365, 26039, 51, 12, 19, 293, 1339, 309, 307, 1391, 1570, 4069, 4787, 11, 309, 311, 1101, 294, 1629, 2098, 293, 300, 51640], "temperature": 0.0, "avg_logprob": -0.09973406284413439, "compression_ratio": 1.556, "no_speech_prob": 0.014717641286551952}, {"id": 4, "seek": 2552, "start": 26.4, "end": 31.36, "text": " Second, Google is saying very little about the data it used to train the model or about", "tokens": [50408, 5736, 11, 3329, 307, 1566, 588, 707, 466, 264, 1412, 309, 1143, 281, 3847, 264, 2316, 420, 466, 50656], "temperature": 0.0, "avg_logprob": -0.11318639529648647, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.26260265707969666}, {"id": 5, "seek": 2552, "start": 31.36, "end": 36.72, "text": " parameters or about compute, although we can make educated guesses on each.", "tokens": [50656, 9834, 420, 466, 14722, 11, 4878, 321, 393, 652, 15872, 42703, 322, 1184, 13, 50924], "temperature": 0.0, "avg_logprob": -0.11318639529648647, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.26260265707969666}, {"id": 6, "seek": 2552, "start": 36.72, "end": 43.28, "text": " Third, Gemini was announced to be in training and will likely rival GPT-5 while arriving earlier", "tokens": [50924, 12548, 11, 22894, 3812, 390, 7548, 281, 312, 294, 3097, 293, 486, 3700, 16286, 26039, 51, 12, 20, 1339, 22436, 3071, 51252], "temperature": 0.0, "avg_logprob": -0.11318639529648647, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.26260265707969666}, {"id": 7, "seek": 2552, "start": 43.28, "end": 49.6, "text": " than GPT-5. As you probably know, Sam Orman said that GPT-5 isn't in training and won't be for a", "tokens": [51252, 813, 26039, 51, 12, 20, 13, 1018, 291, 1391, 458, 11, 4832, 1610, 1601, 848, 300, 26039, 51, 12, 20, 1943, 380, 294, 3097, 293, 1582, 380, 312, 337, 257, 51568], "temperature": 0.0, "avg_logprob": -0.11318639529648647, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.26260265707969666}, {"id": 8, "seek": 4960, "start": 49.6, "end": 56.72, "text": " long time. Fourth, while dedicating 20 pages to bias, toxicity and misgendering, there wasn't a", "tokens": [50364, 938, 565, 13, 23773, 11, 1339, 4172, 30541, 945, 7183, 281, 12577, 11, 45866, 293, 3346, 9395, 1794, 11, 456, 2067, 380, 257, 50720], "temperature": 0.0, "avg_logprob": -0.10138808250427246, "compression_ratio": 1.4901185770750989, "no_speech_prob": 0.1275637149810791}, {"id": 9, "seek": 4960, "start": 56.72, "end": 63.120000000000005, "text": " single page on AI impacts more broadly. Google boasted of giving Gemini planning abilities in a", "tokens": [50720, 2167, 3028, 322, 7318, 11606, 544, 19511, 13, 3329, 748, 34440, 295, 2902, 22894, 3812, 5038, 11582, 294, 257, 51040], "temperature": 0.0, "avg_logprob": -0.10138808250427246, "compression_ratio": 1.4901185770750989, "no_speech_prob": 0.1275637149810791}, {"id": 10, "seek": 4960, "start": 63.120000000000005, "end": 69.6, "text": " move that, surprise as I am to say it, makes open AI look like paragons of responsibility.", "tokens": [51040, 1286, 300, 11, 6365, 382, 286, 669, 281, 584, 309, 11, 1669, 1269, 7318, 574, 411, 17372, 892, 295, 6357, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10138808250427246, "compression_ratio": 1.4901185770750989, "no_speech_prob": 0.1275637149810791}, {"id": 11, "seek": 4960, "start": 69.6, "end": 76.08, "text": " So a lot to get to, but let's look at the first reason that Palm 2 is different from GPT-4. On", "tokens": [51364, 407, 257, 688, 281, 483, 281, 11, 457, 718, 311, 574, 412, 264, 700, 1778, 300, 32668, 568, 307, 819, 490, 26039, 51, 12, 19, 13, 1282, 51688], "temperature": 0.0, "avg_logprob": -0.10138808250427246, "compression_ratio": 1.4901185770750989, "no_speech_prob": 0.1275637149810791}, {"id": 12, "seek": 7608, "start": 76.08, "end": 81.67999999999999, "text": " page 3, they say we designed a more multilingual and diverse pre-training mixture, extending across", "tokens": [50364, 3028, 805, 11, 436, 584, 321, 4761, 257, 544, 2120, 38219, 293, 9521, 659, 12, 17227, 1760, 9925, 11, 24360, 2108, 50644], "temperature": 0.0, "avg_logprob": -0.11125844663327879, "compression_ratio": 1.6313993174061434, "no_speech_prob": 0.004069000482559204}, {"id": 13, "seek": 7608, "start": 81.67999999999999, "end": 86.96, "text": " hundreds of languages and domains like programming, mathematics, etc. So because the text that they", "tokens": [50644, 6779, 295, 8650, 293, 25514, 411, 9410, 11, 18666, 11, 5183, 13, 407, 570, 264, 2487, 300, 436, 50908], "temperature": 0.0, "avg_logprob": -0.11125844663327879, "compression_ratio": 1.6313993174061434, "no_speech_prob": 0.004069000482559204}, {"id": 14, "seek": 7608, "start": 86.96, "end": 92.8, "text": " train Palm 2 on is different to the text that open AI trained GPT-4 on, it means that those", "tokens": [50908, 3847, 32668, 568, 322, 307, 819, 281, 264, 2487, 300, 1269, 7318, 8895, 26039, 51, 12, 19, 322, 11, 309, 1355, 300, 729, 51200], "temperature": 0.0, "avg_logprob": -0.11125844663327879, "compression_ratio": 1.6313993174061434, "no_speech_prob": 0.004069000482559204}, {"id": 15, "seek": 7608, "start": 92.8, "end": 99.52, "text": " models have different abilities and I would say Palm 2 is better at translation and linguistics", "tokens": [51200, 5245, 362, 819, 11582, 293, 286, 576, 584, 32668, 568, 307, 1101, 412, 12853, 293, 21766, 6006, 51536], "temperature": 0.0, "avg_logprob": -0.11125844663327879, "compression_ratio": 1.6313993174061434, "no_speech_prob": 0.004069000482559204}, {"id": 16, "seek": 7608, "start": 99.52, "end": 104.08, "text": " and in certain other areas which I'll get to shortly. If that's data, what about parameter", "tokens": [51536, 293, 294, 1629, 661, 3179, 597, 286, 603, 483, 281, 13392, 13, 759, 300, 311, 1412, 11, 437, 466, 13075, 51764], "temperature": 0.0, "avg_logprob": -0.11125844663327879, "compression_ratio": 1.6313993174061434, "no_speech_prob": 0.004069000482559204}, {"id": 17, "seek": 10408, "start": 104.08, "end": 110.16, "text": " count? Well, Google never actually say they only use words like it's significantly smaller than", "tokens": [50364, 1207, 30, 1042, 11, 3329, 1128, 767, 584, 436, 787, 764, 2283, 411, 309, 311, 10591, 4356, 813, 50668], "temperature": 0.0, "avg_logprob": -0.12133005337837414, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.03620873764157295}, {"id": 18, "seek": 10408, "start": 110.16, "end": 116.32, "text": " the largest Palm model, which was 540 billion parameters. Sometimes they say significantly,", "tokens": [50668, 264, 6443, 32668, 2316, 11, 597, 390, 1025, 5254, 5218, 9834, 13, 4803, 436, 584, 10591, 11, 50976], "temperature": 0.0, "avg_logprob": -0.12133005337837414, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.03620873764157295}, {"id": 19, "seek": 10408, "start": 116.32, "end": 122.24, "text": " other times dramatically. Despite this, it significantly outperforms Palm on a variety", "tokens": [50976, 661, 1413, 17548, 13, 11334, 341, 11, 309, 10591, 484, 26765, 82, 32668, 322, 257, 5673, 51272], "temperature": 0.0, "avg_logprob": -0.12133005337837414, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.03620873764157295}, {"id": 20, "seek": 10408, "start": 122.24, "end": 127.44, "text": " of tasks. To all the references you may have seen to imminent 100 trillion parameter models", "tokens": [51272, 295, 9608, 13, 1407, 439, 264, 15400, 291, 815, 362, 1612, 281, 44339, 2319, 18723, 13075, 5245, 51532], "temperature": 0.0, "avg_logprob": -0.12133005337837414, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.03620873764157295}, {"id": 21, "seek": 12744, "start": 127.44, "end": 134.24, "text": " were bogus. Skipping ahead to page 91 out of 92, in the model summary, they say further details of", "tokens": [50364, 645, 26132, 301, 13, 7324, 6297, 2286, 281, 3028, 31064, 484, 295, 28225, 11, 294, 264, 2316, 12691, 11, 436, 584, 3052, 4365, 295, 50704], "temperature": 0.0, "avg_logprob": -0.0970054558948078, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.21197007596492767}, {"id": 22, "seek": 12744, "start": 134.24, "end": 139.52, "text": " model size and architecture are withheld from external publication. But earlier on, they did", "tokens": [50704, 2316, 2744, 293, 9482, 366, 365, 23504, 490, 8320, 19953, 13, 583, 3071, 322, 11, 436, 630, 50968], "temperature": 0.0, "avg_logprob": -0.0970054558948078, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.21197007596492767}, {"id": 23, "seek": 12744, "start": 139.52, "end": 145.04, "text": " seem to want to give hints about the parameter count inside Palm 2, which OpenAI never did. Here", "tokens": [50968, 1643, 281, 528, 281, 976, 27271, 466, 264, 13075, 1207, 1854, 32668, 568, 11, 597, 7238, 48698, 1128, 630, 13, 1692, 51244], "temperature": 0.0, "avg_logprob": -0.0970054558948078, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.21197007596492767}, {"id": 24, "seek": 12744, "start": 145.04, "end": 150.88, "text": " they present the optimal number of parameters given a certain amount of compute flops. Scaling", "tokens": [51244, 436, 1974, 264, 16252, 1230, 295, 9834, 2212, 257, 1629, 2372, 295, 14722, 932, 3370, 13, 2747, 4270, 51536], "temperature": 0.0, "avg_logprob": -0.0970054558948078, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.21197007596492767}, {"id": 25, "seek": 12744, "start": 150.88, "end": 156.56, "text": " this up to the estimated number of flops used to train Palm 2, that would give an optimal parameter", "tokens": [51536, 341, 493, 281, 264, 14109, 1230, 295, 932, 3370, 1143, 281, 3847, 32668, 568, 11, 300, 576, 976, 364, 16252, 13075, 51820], "temperature": 0.0, "avg_logprob": -0.0970054558948078, "compression_ratio": 1.654109589041096, "no_speech_prob": 0.21197007596492767}, {"id": 26, "seek": 15656, "start": 156.56, "end": 163.84, "text": " count of between 100 and 200 billion. That is a comparable parameter count to GPT-3 while getting", "tokens": [50364, 1207, 295, 1296, 2319, 293, 2331, 5218, 13, 663, 307, 257, 25323, 13075, 1207, 281, 26039, 51, 12, 18, 1339, 1242, 50728], "temperature": 0.0, "avg_logprob": -0.08012711064199383, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.006902242545038462}, {"id": 27, "seek": 15656, "start": 163.84, "end": 170.4, "text": " competitive performance with GPT-4. Bard is apparently now powered by Palm 2 and the inference", "tokens": [50728, 10043, 3389, 365, 26039, 51, 12, 19, 13, 26841, 307, 7970, 586, 17786, 538, 32668, 568, 293, 264, 38253, 51056], "temperature": 0.0, "avg_logprob": -0.08012711064199383, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.006902242545038462}, {"id": 28, "seek": 15656, "start": 170.4, "end": 176.56, "text": " speed is about 10 times faster than GPT-4 for the exact same prompt. And I know there are other", "tokens": [51056, 3073, 307, 466, 1266, 1413, 4663, 813, 26039, 51, 12, 19, 337, 264, 1900, 912, 12391, 13, 400, 286, 458, 456, 366, 661, 51364], "temperature": 0.0, "avg_logprob": -0.08012711064199383, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.006902242545038462}, {"id": 29, "seek": 15656, "start": 176.56, "end": 182.16, "text": " factors that influence inference speed, but that would broadly fit with an order of magnitude", "tokens": [51364, 6771, 300, 6503, 38253, 3073, 11, 457, 300, 576, 19511, 3318, 365, 364, 1668, 295, 15668, 51644], "temperature": 0.0, "avg_logprob": -0.08012711064199383, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.006902242545038462}, {"id": 30, "seek": 18216, "start": 182.16, "end": 187.52, "text": " fewer parameters. This has other implications, of course, and they say that Palm 2 is dramatically", "tokens": [50364, 13366, 9834, 13, 639, 575, 661, 16602, 11, 295, 1164, 11, 293, 436, 584, 300, 32668, 568, 307, 17548, 50632], "temperature": 0.0, "avg_logprob": -0.1136558030780993, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0038240670692175627}, {"id": 31, "seek": 18216, "start": 187.52, "end": 193.6, "text": " smaller, cheaper, and faster to serve. Not only that, Palm 2 itself comes in different sizes,", "tokens": [50632, 4356, 11, 12284, 11, 293, 4663, 281, 4596, 13, 1726, 787, 300, 11, 32668, 568, 2564, 1487, 294, 819, 11602, 11, 50936], "temperature": 0.0, "avg_logprob": -0.1136558030780993, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0038240670692175627}, {"id": 32, "seek": 18216, "start": 193.6, "end": 199.76, "text": " as Sundar Pichai said. Palm 2 models deliver excellent foundational capabilities across a wide", "tokens": [50936, 382, 6942, 289, 430, 480, 1301, 848, 13, 32668, 568, 5245, 4239, 7103, 32195, 10862, 2108, 257, 4874, 51244], "temperature": 0.0, "avg_logprob": -0.1136558030780993, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0038240670692175627}, {"id": 33, "seek": 18216, "start": 199.76, "end": 209.12, "text": " range of sizes. We have affectionately named them Gecko, Otter, Bison, and Unicon. Gecko is so", "tokens": [51244, 3613, 295, 11602, 13, 492, 362, 20080, 1592, 4926, 552, 2876, 41416, 11, 12936, 391, 11, 363, 2770, 11, 293, 1156, 11911, 13, 2876, 41416, 307, 370, 51712], "temperature": 0.0, "avg_logprob": -0.1136558030780993, "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.0038240670692175627}, {"id": 34, "seek": 20912, "start": 209.12, "end": 215.36, "text": " lightweight that it can work on mobile devices fast enough for great interactive applications", "tokens": [50364, 22052, 300, 309, 393, 589, 322, 6013, 5759, 2370, 1547, 337, 869, 15141, 5821, 50676], "temperature": 0.0, "avg_logprob": -0.0828833061715831, "compression_ratio": 1.556, "no_speech_prob": 0.001225461601279676}, {"id": 35, "seek": 20912, "start": 215.36, "end": 221.84, "text": " on-device, even when offline. I would expect Gecko to soon be inside the Google Pixel phones.", "tokens": [50676, 322, 12, 40343, 573, 11, 754, 562, 21857, 13, 286, 576, 2066, 2876, 41416, 281, 2321, 312, 1854, 264, 3329, 28323, 10216, 13, 51000], "temperature": 0.0, "avg_logprob": -0.0828833061715831, "compression_ratio": 1.556, "no_speech_prob": 0.001225461601279676}, {"id": 36, "seek": 20912, "start": 221.84, "end": 227.76, "text": " Going back to data, Google cryptically said that their pre-training corpus is composed of a diverse", "tokens": [51000, 10963, 646, 281, 1412, 11, 3329, 9844, 984, 848, 300, 641, 659, 12, 17227, 1760, 1181, 31624, 307, 18204, 295, 257, 9521, 51296], "temperature": 0.0, "avg_logprob": -0.0828833061715831, "compression_ratio": 1.556, "no_speech_prob": 0.001225461601279676}, {"id": 37, "seek": 20912, "start": 227.76, "end": 234.48000000000002, "text": " set of sources, documents, books, code, mathematics, and conversational data. I've done a whole video", "tokens": [51296, 992, 295, 7139, 11, 8512, 11, 3642, 11, 3089, 11, 18666, 11, 293, 2615, 1478, 1412, 13, 286, 600, 1096, 257, 1379, 960, 51632], "temperature": 0.0, "avg_logprob": -0.0828833061715831, "compression_ratio": 1.556, "no_speech_prob": 0.001225461601279676}, {"id": 38, "seek": 23448, "start": 234.48, "end": 239.6, "text": " on the data issues that these companies face, but suffice to say they're not saying anything about", "tokens": [50364, 322, 264, 1412, 2663, 300, 613, 3431, 1851, 11, 457, 3889, 573, 281, 584, 436, 434, 406, 1566, 1340, 466, 50620], "temperature": 0.0, "avg_logprob": -0.07655786834986864, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.10083506256341934}, {"id": 39, "seek": 23448, "start": 239.6, "end": 244.79999999999998, "text": " where the data comes from. Next, they don't go into detail, but they do say that Palm 2 was trained", "tokens": [50620, 689, 264, 1412, 1487, 490, 13, 3087, 11, 436, 500, 380, 352, 666, 2607, 11, 457, 436, 360, 584, 300, 32668, 568, 390, 8895, 50880], "temperature": 0.0, "avg_logprob": -0.07655786834986864, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.10083506256341934}, {"id": 40, "seek": 23448, "start": 244.79999999999998, "end": 249.6, "text": " to increase the context length of a model significantly beyond that of Palm. As of today,", "tokens": [50880, 281, 3488, 264, 4319, 4641, 295, 257, 2316, 10591, 4399, 300, 295, 32668, 13, 1018, 295, 965, 11, 51120], "temperature": 0.0, "avg_logprob": -0.07655786834986864, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.10083506256341934}, {"id": 41, "seek": 23448, "start": 249.6, "end": 254.72, "text": " you can input around 10,000 characters into BARD, but they end this paragraph with something a bit", "tokens": [51120, 291, 393, 4846, 926, 1266, 11, 1360, 4342, 666, 363, 13259, 11, 457, 436, 917, 341, 18865, 365, 746, 257, 857, 51376], "temperature": 0.0, "avg_logprob": -0.07655786834986864, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.10083506256341934}, {"id": 42, "seek": 23448, "start": 254.72, "end": 259.92, "text": " more interesting. They say, without demonstrating, our results show that it is possible to increase", "tokens": [51376, 544, 1880, 13, 814, 584, 11, 1553, 29889, 11, 527, 3542, 855, 300, 309, 307, 1944, 281, 3488, 51636], "temperature": 0.0, "avg_logprob": -0.07655786834986864, "compression_ratio": 1.6793103448275861, "no_speech_prob": 0.10083506256341934}, {"id": 43, "seek": 25992, "start": 260.0, "end": 264.96000000000004, "text": " the context length of the model without hurting its performance on generic benchmarks.", "tokens": [50368, 264, 4319, 4641, 295, 264, 2316, 1553, 17744, 1080, 3389, 322, 19577, 43751, 13, 50616], "temperature": 0.0, "avg_logprob": -0.060251454917752016, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.048830244690179825}, {"id": 44, "seek": 25992, "start": 264.96000000000004, "end": 270.0, "text": " The bit about not hurting performance is interesting because in this experiment published a few weeks ago", "tokens": [50616, 440, 857, 466, 406, 17744, 3389, 307, 1880, 570, 294, 341, 5120, 6572, 257, 1326, 3259, 2057, 50868], "temperature": 0.0, "avg_logprob": -0.060251454917752016, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.048830244690179825}, {"id": 45, "seek": 25992, "start": 270.0, "end": 275.84000000000003, "text": " about extending the input size in tokens up to around 2 million tokens, the performance did drop", "tokens": [50868, 466, 24360, 264, 4846, 2744, 294, 22667, 493, 281, 926, 568, 2459, 22667, 11, 264, 3389, 630, 3270, 51160], "temperature": 0.0, "avg_logprob": -0.060251454917752016, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.048830244690179825}, {"id": 46, "seek": 25992, "start": 275.84000000000003, "end": 281.84000000000003, "text": " off. If Google had found a way to increase the input size in tokens and not affect performance,", "tokens": [51160, 766, 13, 759, 3329, 632, 1352, 257, 636, 281, 3488, 264, 4846, 2744, 294, 22667, 293, 406, 3345, 3389, 11, 51460], "temperature": 0.0, "avg_logprob": -0.060251454917752016, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.048830244690179825}, {"id": 47, "seek": 25992, "start": 281.84000000000003, "end": 287.20000000000005, "text": " that would be a breakthrough. On multilingual benchmarks, notice how the performance of Palm", "tokens": [51460, 300, 576, 312, 257, 22397, 13, 1282, 2120, 38219, 43751, 11, 3449, 577, 264, 3389, 295, 32668, 51728], "temperature": 0.0, "avg_logprob": -0.060251454917752016, "compression_ratio": 1.7902621722846441, "no_speech_prob": 0.048830244690179825}, {"id": 48, "seek": 28720, "start": 287.2, "end": 293.44, "text": " 2 in English is not dramatically better than in other languages. In fact, in many other languages,", "tokens": [50364, 568, 294, 3669, 307, 406, 17548, 1101, 813, 294, 661, 8650, 13, 682, 1186, 11, 294, 867, 661, 8650, 11, 50676], "temperature": 0.0, "avg_logprob": -0.07710778609566067, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.028414027765393257}, {"id": 49, "seek": 28720, "start": 293.44, "end": 298.8, "text": " it does better than in English. This is very different to GPT-4, which was noticeably better", "tokens": [50676, 309, 775, 1101, 813, 294, 3669, 13, 639, 307, 588, 819, 281, 26039, 51, 12, 19, 11, 597, 390, 3449, 1188, 1101, 50944], "temperature": 0.0, "avg_logprob": -0.07710778609566067, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.028414027765393257}, {"id": 50, "seek": 28720, "start": 298.8, "end": 304.32, "text": " in English than in all other languages. As Google hinted earlier, this is likely due to the", "tokens": [50944, 294, 3669, 813, 294, 439, 661, 8650, 13, 1018, 3329, 12075, 292, 3071, 11, 341, 307, 3700, 3462, 281, 264, 51220], "temperature": 0.0, "avg_logprob": -0.07710778609566067, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.028414027765393257}, {"id": 51, "seek": 28720, "start": 304.32, "end": 310.71999999999997, "text": " multilingual text data that Google trained Palm 2 with. In fact, on page 17, Google admit that the", "tokens": [51220, 2120, 38219, 2487, 1412, 300, 3329, 8895, 32668, 568, 365, 13, 682, 1186, 11, 322, 3028, 3282, 11, 3329, 9796, 300, 264, 51540], "temperature": 0.0, "avg_logprob": -0.07710778609566067, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.028414027765393257}, {"id": 52, "seek": 28720, "start": 310.71999999999997, "end": 316.8, "text": " performance of Palm 2 exceeds Google Translate for certain languages, and they show on page 4 that", "tokens": [51540, 3389, 295, 32668, 568, 43305, 3329, 6531, 17593, 337, 1629, 8650, 11, 293, 436, 855, 322, 3028, 1017, 300, 51844], "temperature": 0.0, "avg_logprob": -0.07710778609566067, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.028414027765393257}, {"id": 53, "seek": 31680, "start": 316.88, "end": 323.04, "text": " it can pass the mastery exams across a range of languages like Chinese, Japanese, Italian,", "tokens": [50368, 309, 393, 1320, 264, 37951, 20514, 2108, 257, 3613, 295, 8650, 411, 4649, 11, 5433, 11, 10003, 11, 50676], "temperature": 0.0, "avg_logprob": -0.09210126296333644, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.00856612715870142}, {"id": 54, "seek": 31680, "start": 323.04, "end": 328.8, "text": " French, Spanish, German, etc. Look at the difference between Palm 2 and Palm in red.", "tokens": [50676, 5522, 11, 8058, 11, 6521, 11, 5183, 13, 2053, 412, 264, 2649, 1296, 32668, 568, 293, 32668, 294, 2182, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09210126296333644, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.00856612715870142}, {"id": 55, "seek": 31680, "start": 328.8, "end": 334.0, "text": " Now, before you rush off and try BARD in all of those languages, I tried that and apparently", "tokens": [50964, 823, 11, 949, 291, 9300, 766, 293, 853, 363, 13259, 294, 439, 295, 729, 8650, 11, 286, 3031, 300, 293, 7970, 51224], "temperature": 0.0, "avg_logprob": -0.09210126296333644, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.00856612715870142}, {"id": 56, "seek": 31680, "start": 334.0, "end": 339.6, "text": " you can only use BARD at the moment in the following languages, English, US English, what a pity,", "tokens": [51224, 291, 393, 787, 764, 363, 13259, 412, 264, 1623, 294, 264, 3480, 8650, 11, 3669, 11, 2546, 3669, 11, 437, 257, 21103, 11, 51504], "temperature": 0.0, "avg_logprob": -0.09210126296333644, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.00856612715870142}, {"id": 57, "seek": 33960, "start": 339.6, "end": 347.44, "text": " and Japanese and Korean. But I was able to test BARD in Korean on a question translated via Google", "tokens": [50364, 293, 5433, 293, 6933, 13, 583, 286, 390, 1075, 281, 1500, 363, 13259, 294, 6933, 322, 257, 1168, 16805, 5766, 3329, 50756], "temperature": 0.0, "avg_logprob": -0.07540694321736251, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.25060683488845825}, {"id": 58, "seek": 33960, "start": 347.44, "end": 353.84000000000003, "text": " Translate from the MMLU dataset. It got the question right in each of its drafts. In contrast,", "tokens": [50756, 6531, 17593, 490, 264, 376, 12683, 52, 28872, 13, 467, 658, 264, 1168, 558, 294, 1184, 295, 1080, 11206, 82, 13, 682, 8712, 11, 51076], "temperature": 0.0, "avg_logprob": -0.07540694321736251, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.25060683488845825}, {"id": 59, "seek": 33960, "start": 353.84000000000003, "end": 360.48, "text": " GPT-4 not only got the question wrong in Korean, when I originally tested it for my smart GPT video,", "tokens": [51076, 26039, 51, 12, 19, 406, 787, 658, 264, 1168, 2085, 294, 6933, 11, 562, 286, 7993, 8246, 309, 337, 452, 4069, 26039, 51, 960, 11, 51408], "temperature": 0.0, "avg_logprob": -0.07540694321736251, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.25060683488845825}, {"id": 60, "seek": 33960, "start": 360.48, "end": 365.68, "text": " it got the question wrong in English. In case any of my regular viewers are wondering, I am working", "tokens": [51408, 309, 658, 264, 1168, 2085, 294, 3669, 13, 682, 1389, 604, 295, 452, 3890, 8499, 366, 6359, 11, 286, 669, 1364, 51668], "temperature": 0.0, "avg_logprob": -0.07540694321736251, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.25060683488845825}, {"id": 61, "seek": 36568, "start": 365.68, "end": 371.52, "text": " very hard on smart GPT to understand what it's capable of and getting it benchmarked officially,", "tokens": [50364, 588, 1152, 322, 4069, 26039, 51, 281, 1223, 437, 309, 311, 8189, 295, 293, 1242, 309, 18927, 292, 12053, 11, 50656], "temperature": 0.0, "avg_logprob": -0.10211836162366365, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.15386757254600525}, {"id": 62, "seek": 36568, "start": 371.52, "end": 376.72, "text": " and thank you so much for all the kind offers of help in that regard. I must admit it was very", "tokens": [50656, 293, 1309, 291, 370, 709, 337, 439, 264, 733, 7736, 295, 854, 294, 300, 3843, 13, 286, 1633, 9796, 309, 390, 588, 50916], "temperature": 0.0, "avg_logprob": -0.10211836162366365, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.15386757254600525}, {"id": 63, "seek": 36568, "start": 376.72, "end": 384.16, "text": " interesting to see on page 14 a direct comparison between Palm 2 and GPT-4, and Google do admit", "tokens": [50916, 1880, 281, 536, 322, 3028, 3499, 257, 2047, 9660, 1296, 32668, 568, 293, 26039, 51, 12, 19, 11, 293, 3329, 360, 9796, 51288], "temperature": 0.0, "avg_logprob": -0.10211836162366365, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.15386757254600525}, {"id": 64, "seek": 36568, "start": 384.16, "end": 390.0, "text": " for the Palm 2 results they use chain of thought prompting and self-consistency. Reading the", "tokens": [51288, 337, 264, 32668, 568, 3542, 436, 764, 5021, 295, 1194, 12391, 278, 293, 2698, 12, 21190, 468, 3020, 13, 29766, 264, 51580], "temperature": 0.0, "avg_logprob": -0.10211836162366365, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.15386757254600525}, {"id": 65, "seek": 39000, "start": 390.08, "end": 395.52, "text": " self-consistency paper did remind me quite a lot actually of smart GPT, where it picks the", "tokens": [50368, 2698, 12, 21190, 468, 3020, 3035, 630, 4160, 385, 1596, 257, 688, 767, 295, 4069, 26039, 51, 11, 689, 309, 16137, 264, 50640], "temperature": 0.0, "avg_logprob": -0.12171322002745512, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.1329054832458496}, {"id": 66, "seek": 39000, "start": 395.52, "end": 401.76, "text": " most consistent answer of multiple outputs. So I do wonder if this comparison is totally fair", "tokens": [50640, 881, 8398, 1867, 295, 3866, 23930, 13, 407, 286, 360, 2441, 498, 341, 9660, 307, 3879, 3143, 50952], "temperature": 0.0, "avg_logprob": -0.12171322002745512, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.1329054832458496}, {"id": 67, "seek": 39000, "start": 401.76, "end": 406.8, "text": " if Palm 2 used this method and GPT-4 didn't. I'll have to talk about these benchmarks more", "tokens": [50952, 498, 32668, 568, 1143, 341, 3170, 293, 26039, 51, 12, 19, 994, 380, 13, 286, 603, 362, 281, 751, 466, 613, 43751, 544, 51204], "temperature": 0.0, "avg_logprob": -0.12171322002745512, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.1329054832458496}, {"id": 68, "seek": 39000, "start": 406.8, "end": 412.16, "text": " in another video, otherwise this one would be too long, but a quick hint is that Wynogrand is", "tokens": [51204, 294, 1071, 960, 11, 5911, 341, 472, 576, 312, 886, 938, 11, 457, 257, 1702, 12075, 307, 300, 343, 2534, 664, 3699, 307, 51472], "temperature": 0.0, "avg_logprob": -0.12171322002745512, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.1329054832458496}, {"id": 69, "seek": 39000, "start": 412.16, "end": 418.32, "text": " about identifying what the pronoun in a sentence refers to. Google also weighed into the emerging", "tokens": [51472, 466, 16696, 437, 264, 14144, 294, 257, 8174, 14942, 281, 13, 3329, 611, 32844, 666, 264, 14989, 51780], "temperature": 0.0, "avg_logprob": -0.12171322002745512, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.1329054832458496}, {"id": 70, "seek": 41832, "start": 418.32, "end": 424.15999999999997, "text": " abilities debate, saying that Palm 2 does indeed demonstrate new emerging abilities. They say it", "tokens": [50364, 11582, 7958, 11, 1566, 300, 32668, 568, 775, 6451, 11698, 777, 14989, 11582, 13, 814, 584, 309, 50656], "temperature": 0.0, "avg_logprob": -0.0789967419808371, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.044661298394203186}, {"id": 71, "seek": 41832, "start": 424.15999999999997, "end": 430.15999999999997, "text": " does so in things like multi-step arithmetic problems, temporal sequences, and hierarchical", "tokens": [50656, 775, 370, 294, 721, 411, 4825, 12, 16792, 42973, 2740, 11, 30881, 22978, 11, 293, 35250, 804, 50956], "temperature": 0.0, "avg_logprob": -0.0789967419808371, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.044661298394203186}, {"id": 72, "seek": 41832, "start": 430.15999999999997, "end": 434.71999999999997, "text": " reasoning. Of course I'm going to test all of those and have begun to do so already, and in my", "tokens": [50956, 21577, 13, 2720, 1164, 286, 478, 516, 281, 1500, 439, 295, 729, 293, 362, 16009, 281, 360, 370, 1217, 11, 293, 294, 452, 51184], "temperature": 0.0, "avg_logprob": -0.0789967419808371, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.044661298394203186}, {"id": 73, "seek": 41832, "start": 434.71999999999997, "end": 440.0, "text": " early experiments I'm getting quite an interesting result. Palm 2 gets a lot of questions wrong that", "tokens": [51184, 2440, 12050, 286, 478, 1242, 1596, 364, 1880, 1874, 13, 32668, 568, 2170, 257, 688, 295, 1651, 2085, 300, 51448], "temperature": 0.0, "avg_logprob": -0.0789967419808371, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.044661298394203186}, {"id": 74, "seek": 41832, "start": 440.0, "end": 446.08, "text": " GPT-4 gets right, but it can also get questions right that GPT-4 gets wrong, and I must admit", "tokens": [51448, 26039, 51, 12, 19, 2170, 558, 11, 457, 309, 393, 611, 483, 1651, 558, 300, 26039, 51, 12, 19, 2170, 2085, 11, 293, 286, 1633, 9796, 51752], "temperature": 0.0, "avg_logprob": -0.0789967419808371, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.044661298394203186}, {"id": 75, "seek": 44608, "start": 446.08, "end": 451.12, "text": " it's really weird to see Palm 2 getting really advanced college-level math questions right,", "tokens": [50364, 309, 311, 534, 3657, 281, 536, 32668, 568, 1242, 534, 7339, 3859, 12, 12418, 5221, 1651, 558, 11, 50616], "temperature": 0.0, "avg_logprob": -0.07929101266151617, "compression_ratio": 1.5888157894736843, "no_speech_prob": 0.026748143136501312}, {"id": 76, "seek": 44608, "start": 451.12, "end": 456.96, "text": " that GPT-4 gets wrong. And yet also when I ask it a basic question about prime numbers, it gets", "tokens": [50616, 300, 26039, 51, 12, 19, 2170, 2085, 13, 400, 1939, 611, 562, 286, 1029, 309, 257, 3875, 1168, 466, 5835, 3547, 11, 309, 2170, 50908], "temperature": 0.0, "avg_logprob": -0.07929101266151617, "compression_ratio": 1.5888157894736843, "no_speech_prob": 0.026748143136501312}, {"id": 77, "seek": 44608, "start": 456.96, "end": 461.91999999999996, "text": " it kind of hilariously wrong. Honestly I'm not certain what's going on there, but I do have my", "tokens": [50908, 309, 733, 295, 18661, 8994, 2085, 13, 12348, 286, 478, 406, 1629, 437, 311, 516, 322, 456, 11, 457, 286, 360, 362, 452, 51156], "temperature": 0.0, "avg_logprob": -0.07929101266151617, "compression_ratio": 1.5888157894736843, "no_speech_prob": 0.026748143136501312}, {"id": 78, "seek": 44608, "start": 461.91999999999996, "end": 467.76, "text": " suspicions. Remember though that recent papers have claimed that emergent abilities are a mirage,", "tokens": [51156, 6535, 299, 626, 13, 5459, 1673, 300, 5162, 10577, 362, 12941, 300, 4345, 6930, 11582, 366, 257, 3149, 609, 11, 51448], "temperature": 0.0, "avg_logprob": -0.07929101266151617, "compression_ratio": 1.5888157894736843, "no_speech_prob": 0.026748143136501312}, {"id": 79, "seek": 44608, "start": 467.76, "end": 474.32, "text": " so Google begs to differ. When Google put Palm 2 up against GPT-4 in high school mathematics problems,", "tokens": [51448, 370, 3329, 4612, 82, 281, 743, 13, 1133, 3329, 829, 32668, 568, 493, 1970, 26039, 51, 12, 19, 294, 1090, 1395, 18666, 2740, 11, 51776], "temperature": 0.0, "avg_logprob": -0.07929101266151617, "compression_ratio": 1.5888157894736843, "no_speech_prob": 0.026748143136501312}, {"id": 80, "seek": 47432, "start": 474.32, "end": 481.36, "text": " it did outperform GPT-4, but again it was using an advanced prompting strategy, not 100% different", "tokens": [50364, 309, 630, 484, 26765, 26039, 51, 12, 19, 11, 457, 797, 309, 390, 1228, 364, 7339, 12391, 278, 5206, 11, 406, 2319, 4, 819, 50716], "temperature": 0.0, "avg_logprob": -0.09053099981629022, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.0037066179793328047}, {"id": 81, "seek": 47432, "start": 481.36, "end": 486.88, "text": " from smart GPT, so I wonder if the comparison is quite fair. What about coding? Well again it's", "tokens": [50716, 490, 4069, 26039, 51, 11, 370, 286, 2441, 498, 264, 9660, 307, 1596, 3143, 13, 708, 466, 17720, 30, 1042, 797, 309, 311, 50992], "temperature": 0.0, "avg_logprob": -0.09053099981629022, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.0037066179793328047}, {"id": 82, "seek": 47432, "start": 486.88, "end": 492.88, "text": " really hard to find a direct comparison that's fair between the two models. Overall I would guess", "tokens": [50992, 534, 1152, 281, 915, 257, 2047, 9660, 300, 311, 3143, 1296, 264, 732, 5245, 13, 18420, 286, 576, 2041, 51292], "temperature": 0.0, "avg_logprob": -0.09053099981629022, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.0037066179793328047}, {"id": 83, "seek": 47432, "start": 492.88, "end": 500.0, "text": " that the specialized coding model of Palm, what they call Palm 2S, is worse than GPT-4. It says", "tokens": [51292, 300, 264, 19813, 17720, 2316, 295, 32668, 11, 437, 436, 818, 32668, 568, 50, 11, 307, 5324, 813, 26039, 51, 12, 19, 13, 467, 1619, 51648], "temperature": 0.0, "avg_logprob": -0.09053099981629022, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.0037066179793328047}, {"id": 84, "seek": 50000, "start": 500.0, "end": 507.84, "text": " it's pass at one accuracy, as in past first time, is 37.6%. Remember the Sparks of AGI paper? Well", "tokens": [50364, 309, 311, 1320, 412, 472, 14170, 11, 382, 294, 1791, 700, 565, 11, 307, 13435, 13, 21, 6856, 5459, 264, 1738, 20851, 295, 316, 26252, 3035, 30, 1042, 50756], "temperature": 0.0, "avg_logprob": -0.0941234622989689, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.037318065762519836}, {"id": 85, "seek": 50000, "start": 507.84, "end": 515.12, "text": " that gave GPT-4 as having an 82% zero shot pass at one accuracy level. However as I talked about in", "tokens": [50756, 300, 2729, 26039, 51, 12, 19, 382, 1419, 364, 29097, 4, 4018, 3347, 1320, 412, 472, 14170, 1496, 13, 2908, 382, 286, 2825, 466, 294, 51120], "temperature": 0.0, "avg_logprob": -0.0941234622989689, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.037318065762519836}, {"id": 86, "seek": 50000, "start": 515.12, "end": 522.16, "text": " the Sparks of AGI video, the paper admits that it could be that GPT-4 has seen and memorized some", "tokens": [51120, 264, 1738, 20851, 295, 316, 26252, 960, 11, 264, 3035, 46682, 300, 309, 727, 312, 300, 26039, 51, 12, 19, 575, 1612, 293, 46677, 512, 51472], "temperature": 0.0, "avg_logprob": -0.0941234622989689, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.037318065762519836}, {"id": 87, "seek": 50000, "start": 522.16, "end": 527.84, "text": " or all of human eval. There is one thing I will give Google credit on, which is that their code", "tokens": [51472, 420, 439, 295, 1952, 1073, 304, 13, 821, 307, 472, 551, 286, 486, 976, 3329, 5397, 322, 11, 597, 307, 300, 641, 3089, 51756], "temperature": 0.0, "avg_logprob": -0.0941234622989689, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.037318065762519836}, {"id": 88, "seek": 52784, "start": 527.84, "end": 533.2800000000001, "text": " now sometimes references where it came from. Here is a brief extract from the Google keynote", "tokens": [50364, 586, 2171, 15400, 689, 309, 1361, 490, 13, 1692, 307, 257, 5353, 8947, 490, 264, 3329, 33896, 50636], "temperature": 0.0, "avg_logprob": -0.09361840177465368, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.007815087214112282}, {"id": 89, "seek": 52784, "start": 533.2800000000001, "end": 539.76, "text": " presentation. How would I use Python to generate the Scholar's Mate move in chess? Okay here Bard", "tokens": [50636, 5860, 13, 1012, 576, 286, 764, 15329, 281, 8460, 264, 2065, 15276, 311, 27594, 1286, 294, 24122, 30, 1033, 510, 26841, 50960], "temperature": 0.0, "avg_logprob": -0.09361840177465368, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.007815087214112282}, {"id": 90, "seek": 52784, "start": 539.76, "end": 545.6, "text": " created a script to recreate this chess move in Python, and notice how it also formatted the code", "tokens": [50960, 2942, 257, 5755, 281, 25833, 341, 24122, 1286, 294, 15329, 11, 293, 3449, 577, 309, 611, 1254, 32509, 264, 3089, 51252], "temperature": 0.0, "avg_logprob": -0.09361840177465368, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.007815087214112282}, {"id": 91, "seek": 52784, "start": 545.6, "end": 551.2, "text": " nicely, making it easy to read. We've also heard great feedback from developers about how Bard", "tokens": [51252, 9594, 11, 1455, 309, 1858, 281, 1401, 13, 492, 600, 611, 2198, 869, 5824, 490, 8849, 466, 577, 26841, 51532], "temperature": 0.0, "avg_logprob": -0.09361840177465368, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.007815087214112282}, {"id": 92, "seek": 52784, "start": 551.2, "end": 557.2800000000001, "text": " provides code citations, and starting next week you'll notice something right here. We're making", "tokens": [51532, 6417, 3089, 4814, 763, 11, 293, 2891, 958, 1243, 291, 603, 3449, 746, 558, 510, 13, 492, 434, 1455, 51836], "temperature": 0.0, "avg_logprob": -0.09361840177465368, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.007815087214112282}, {"id": 93, "seek": 55728, "start": 557.28, "end": 563.6, "text": " code citations even more precise. If Bard brings in a block of code, just click this annotation,", "tokens": [50364, 3089, 4814, 763, 754, 544, 13600, 13, 759, 26841, 5607, 294, 257, 3461, 295, 3089, 11, 445, 2052, 341, 48654, 11, 50680], "temperature": 0.0, "avg_logprob": -0.09704810380935669, "compression_ratio": 1.5433070866141732, "no_speech_prob": 0.0015977449947968125}, {"id": 94, "seek": 55728, "start": 563.6, "end": 568.9599999999999, "text": " and Bard will underline the block and link to the source. As always, it seems the appendix contained", "tokens": [50680, 293, 26841, 486, 833, 1889, 264, 3461, 293, 2113, 281, 264, 4009, 13, 1018, 1009, 11, 309, 2544, 264, 34116, 970, 16212, 50948], "temperature": 0.0, "avg_logprob": -0.09704810380935669, "compression_ratio": 1.5433070866141732, "no_speech_prob": 0.0015977449947968125}, {"id": 95, "seek": 55728, "start": 568.9599999999999, "end": 574.0799999999999, "text": " more interesting information sometimes than the main body of the technical report. For example,", "tokens": [50948, 544, 1880, 1589, 2171, 813, 264, 2135, 1772, 295, 264, 6191, 2275, 13, 1171, 1365, 11, 51204], "temperature": 0.0, "avg_logprob": -0.09704810380935669, "compression_ratio": 1.5433070866141732, "no_speech_prob": 0.0015977449947968125}, {"id": 96, "seek": 55728, "start": 574.0799999999999, "end": 581.92, "text": " we get a direct and fair comparison between GPT-4 and palm 2, or I should say flan palm 2. That is", "tokens": [51204, 321, 483, 257, 2047, 293, 3143, 9660, 1296, 26039, 51, 12, 19, 293, 17018, 568, 11, 420, 286, 820, 584, 932, 282, 17018, 568, 13, 663, 307, 51596], "temperature": 0.0, "avg_logprob": -0.09704810380935669, "compression_ratio": 1.5433070866141732, "no_speech_prob": 0.0015977449947968125}, {"id": 97, "seek": 58192, "start": 581.92, "end": 587.4399999999999, "text": " the instruction fine tuned version of palm 2. Essentially that's the version where it's been", "tokens": [50364, 264, 10951, 2489, 10870, 3037, 295, 17018, 568, 13, 23596, 300, 311, 264, 3037, 689, 309, 311, 668, 50640], "temperature": 0.0, "avg_logprob": -0.08003042329032466, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.13288287818431854}, {"id": 98, "seek": 58192, "start": 587.4399999999999, "end": 592.56, "text": " fine tuned to get better at following a question and answer format. But anyway, the original palm", "tokens": [50640, 2489, 10870, 281, 483, 1101, 412, 3480, 257, 1168, 293, 1867, 7877, 13, 583, 4033, 11, 264, 3380, 17018, 50896], "temperature": 0.0, "avg_logprob": -0.08003042329032466, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.13288287818431854}, {"id": 99, "seek": 58192, "start": 592.56, "end": 602.4799999999999, "text": " 2 scored 78.3%, and flan palm 2 scored 81.2%. That's below the 86.4% of GPT-4. And that's why my", "tokens": [50896, 568, 18139, 26369, 13, 18, 8923, 293, 932, 282, 17018, 568, 18139, 30827, 13, 17, 6856, 663, 311, 2507, 264, 26687, 13, 19, 4, 295, 26039, 51, 12, 19, 13, 400, 300, 311, 983, 452, 51392], "temperature": 0.0, "avg_logprob": -0.08003042329032466, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.13288287818431854}, {"id": 100, "seek": 58192, "start": 602.4799999999999, "end": 608.9599999999999, "text": " broad conclusion is that GPT-4 is a bit smarter than palm 2. But as I'll be showing over the", "tokens": [51392, 4152, 10063, 307, 300, 26039, 51, 12, 19, 307, 257, 857, 20294, 813, 17018, 568, 13, 583, 382, 286, 603, 312, 4099, 670, 264, 51716], "temperature": 0.0, "avg_logprob": -0.08003042329032466, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.13288287818431854}, {"id": 101, "seek": 60896, "start": 608.96, "end": 615.84, "text": " coming days and weeks, there are genuinely quite a few areas in which palm 2 is better than GPT-4.", "tokens": [50364, 1348, 1708, 293, 3259, 11, 456, 366, 17839, 1596, 257, 1326, 3179, 294, 597, 17018, 568, 307, 1101, 813, 26039, 51, 12, 19, 13, 50708], "temperature": 0.0, "avg_logprob": -0.0734683826405515, "compression_ratio": 1.5568627450980392, "no_speech_prob": 0.007814411073923111}, {"id": 102, "seek": 60896, "start": 615.84, "end": 620.4000000000001, "text": " What about the big bench, which was designed to be particularly tough for language models? I talked", "tokens": [50708, 708, 466, 264, 955, 10638, 11, 597, 390, 4761, 281, 312, 4098, 4930, 337, 2856, 5245, 30, 286, 2825, 50936], "temperature": 0.0, "avg_logprob": -0.0734683826405515, "compression_ratio": 1.5568627450980392, "no_speech_prob": 0.007814411073923111}, {"id": 103, "seek": 60896, "start": 620.4000000000001, "end": 626.5600000000001, "text": " a lot about this in my earliest videos. Well, the graph is going to look pretty weird because palm 2", "tokens": [50936, 257, 688, 466, 341, 294, 452, 20573, 2145, 13, 1042, 11, 264, 4295, 307, 516, 281, 574, 1238, 3657, 570, 17018, 568, 51244], "temperature": 0.0, "avg_logprob": -0.0734683826405515, "compression_ratio": 1.5568627450980392, "no_speech_prob": 0.007814411073923111}, {"id": 104, "seek": 60896, "start": 626.5600000000001, "end": 633.2, "text": " has improved upon palm while reducing the number of parameters. So the graph kind of doubles back", "tokens": [51244, 575, 9689, 3564, 17018, 1339, 12245, 264, 1230, 295, 9834, 13, 407, 264, 4295, 733, 295, 31634, 646, 51576], "temperature": 0.0, "avg_logprob": -0.0734683826405515, "compression_ratio": 1.5568627450980392, "no_speech_prob": 0.007814411073923111}, {"id": 105, "seek": 63320, "start": 633.2800000000001, "end": 639.5200000000001, "text": " on itself back up here up to around 69% according to the technical report. I would say this is", "tokens": [50368, 322, 2564, 646, 493, 510, 493, 281, 926, 28267, 4, 4650, 281, 264, 6191, 2275, 13, 286, 576, 584, 341, 307, 50680], "temperature": 0.0, "avg_logprob": -0.055765634172418144, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.20671531558036804}, {"id": 106, "seek": 63320, "start": 639.5200000000001, "end": 645.44, "text": " quite a major moment in human history. There is now virtually no language task that the average", "tokens": [50680, 1596, 257, 2563, 1623, 294, 1952, 2503, 13, 821, 307, 586, 14103, 572, 2856, 5633, 300, 264, 4274, 50976], "temperature": 0.0, "avg_logprob": -0.055765634172418144, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.20671531558036804}, {"id": 107, "seek": 63320, "start": 645.44, "end": 651.84, "text": " human can do better than palm 2. Of course, expert humans can do better in individual domains, but", "tokens": [50976, 1952, 393, 360, 1101, 813, 17018, 568, 13, 2720, 1164, 11, 5844, 6255, 393, 360, 1101, 294, 2609, 25514, 11, 457, 51296], "temperature": 0.0, "avg_logprob": -0.055765634172418144, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.20671531558036804}, {"id": 108, "seek": 63320, "start": 651.84, "end": 657.84, "text": " the average human is now worse in virtually every domain of language. Here you can see that confirmation", "tokens": [51296, 264, 4274, 1952, 307, 586, 5324, 294, 14103, 633, 9274, 295, 2856, 13, 1692, 291, 393, 536, 300, 21871, 51596], "temperature": 0.0, "avg_logprob": -0.055765634172418144, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.20671531558036804}, {"id": 109, "seek": 65784, "start": 657.9200000000001, "end": 664.64, "text": " of the big bench hard results for flan palm 2, 69.1%. Interestingly, in the original chart,", "tokens": [50368, 295, 264, 955, 10638, 1152, 3542, 337, 932, 282, 17018, 568, 11, 28267, 13, 16, 6856, 30564, 11, 294, 264, 3380, 6927, 11, 50704], "temperature": 0.0, "avg_logprob": -0.0882936497123874, "compression_ratio": 1.552, "no_speech_prob": 0.07581508904695511}, {"id": 110, "seek": 65784, "start": 664.64, "end": 671.44, "text": " palm 2 is even claimed to have higher performance than that at 78.1%. If you remember, the reason we", "tokens": [50704, 17018, 568, 307, 754, 12941, 281, 362, 2946, 3389, 813, 300, 412, 26369, 13, 16, 6856, 759, 291, 1604, 11, 264, 1778, 321, 51044], "temperature": 0.0, "avg_logprob": -0.0882936497123874, "compression_ratio": 1.552, "no_speech_prob": 0.07581508904695511}, {"id": 111, "seek": 65784, "start": 671.44, "end": 677.44, "text": " can't compare that to GPT-4 is that in the technical report for GPT-4, they admit that during their", "tokens": [51044, 393, 380, 6794, 300, 281, 26039, 51, 12, 19, 307, 300, 294, 264, 6191, 2275, 337, 26039, 51, 12, 19, 11, 436, 9796, 300, 1830, 641, 51344], "temperature": 0.0, "avg_logprob": -0.0882936497123874, "compression_ratio": 1.552, "no_speech_prob": 0.07581508904695511}, {"id": 112, "seek": 65784, "start": 677.44, "end": 683.0400000000001, "text": " contamination check, we discovered that portions of big bench were inadvertently mixed into the", "tokens": [51344, 33012, 1520, 11, 321, 6941, 300, 25070, 295, 955, 10638, 645, 49152, 2276, 7467, 666, 264, 51624], "temperature": 0.0, "avg_logprob": -0.0882936497123874, "compression_ratio": 1.552, "no_speech_prob": 0.07581508904695511}, {"id": 113, "seek": 68304, "start": 683.04, "end": 688.56, "text": " training set and we excluded it from our reported results. Before we get to Gemini, Google show off", "tokens": [50364, 3097, 992, 293, 321, 29486, 309, 490, 527, 7055, 3542, 13, 4546, 321, 483, 281, 22894, 3812, 11, 3329, 855, 766, 50640], "temperature": 0.0, "avg_logprob": -0.07713034442652052, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.022968145087361336}, {"id": 114, "seek": 68304, "start": 688.56, "end": 694.16, "text": " in the latter half of the technical report with examples of linguistic ability, like writing", "tokens": [50640, 294, 264, 18481, 1922, 295, 264, 6191, 2275, 365, 5110, 295, 43002, 3485, 11, 411, 3579, 50920], "temperature": 0.0, "avg_logprob": -0.07713034442652052, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.022968145087361336}, {"id": 115, "seek": 68304, "start": 694.16, "end": 700.0799999999999, "text": " paragraphs in Tejiki and then translating them into Persian. They go on to show examples in", "tokens": [50920, 48910, 294, 1989, 73, 9850, 293, 550, 35030, 552, 666, 30699, 13, 814, 352, 322, 281, 855, 5110, 294, 51216], "temperature": 0.0, "avg_logprob": -0.07713034442652052, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.022968145087361336}, {"id": 116, "seek": 68304, "start": 700.0799999999999, "end": 705.8399999999999, "text": " Tamil and they are really making a big point of showing off its multilingual capabilities. At", "tokens": [51216, 39938, 293, 436, 366, 534, 1455, 257, 955, 935, 295, 4099, 766, 1080, 2120, 38219, 10862, 13, 1711, 51504], "temperature": 0.0, "avg_logprob": -0.07713034442652052, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.022968145087361336}, {"id": 117, "seek": 68304, "start": 705.8399999999999, "end": 711.1999999999999, "text": " this point, and I'm going to admit this is my personal opinion, Google then strays into dozens", "tokens": [51504, 341, 935, 11, 293, 286, 478, 516, 281, 9796, 341, 307, 452, 2973, 4800, 11, 3329, 550, 1056, 3772, 666, 18431, 51772], "temperature": 0.0, "avg_logprob": -0.07713034442652052, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.022968145087361336}, {"id": 118, "seek": 71120, "start": 711.2, "end": 718.0, "text": " of pages on bias, toxicity and gender. Interestingly, some of the people paid to assess these risks", "tokens": [50364, 295, 7183, 322, 12577, 11, 45866, 293, 7898, 13, 30564, 11, 512, 295, 264, 561, 4835, 281, 5877, 613, 10888, 50704], "temperature": 0.0, "avg_logprob": -0.06720010791204672, "compression_ratio": 1.611683848797251, "no_speech_prob": 0.039612602442502975}, {"id": 119, "seek": 71120, "start": 718.0, "end": 723.12, "text": " were paid only 1.5 cents per judgment. These things do need to be addressed, of course,", "tokens": [50704, 645, 4835, 787, 502, 13, 20, 14941, 680, 12216, 13, 1981, 721, 360, 643, 281, 312, 13847, 11, 295, 1164, 11, 50960], "temperature": 0.0, "avg_logprob": -0.06720010791204672, "compression_ratio": 1.611683848797251, "no_speech_prob": 0.039612602442502975}, {"id": 120, "seek": 71120, "start": 723.12, "end": 728.48, "text": " but it was somewhat shocking to me to see 20 pages of that and not a single page on the", "tokens": [50960, 457, 309, 390, 8344, 18776, 281, 385, 281, 536, 945, 7183, 295, 300, 293, 406, 257, 2167, 3028, 322, 264, 51228], "temperature": 0.0, "avg_logprob": -0.06720010791204672, "compression_ratio": 1.611683848797251, "no_speech_prob": 0.039612602442502975}, {"id": 121, "seek": 71120, "start": 728.48, "end": 734.0, "text": " broader AI impacts. As many of you may know, I have criticized open AI plenty of times on this", "tokens": [51228, 13227, 7318, 11606, 13, 1018, 867, 295, 291, 815, 458, 11, 286, 362, 28011, 1269, 7318, 7140, 295, 1413, 322, 341, 51504], "temperature": 0.0, "avg_logprob": -0.06720010791204672, "compression_ratio": 1.611683848797251, "no_speech_prob": 0.039612602442502975}, {"id": 122, "seek": 71120, "start": 734.0, "end": 739.44, "text": " channel, but compare their technical report, which goes into far more detail about what we need to", "tokens": [51504, 2269, 11, 457, 6794, 641, 6191, 2275, 11, 597, 1709, 666, 1400, 544, 2607, 466, 437, 321, 643, 281, 51776], "temperature": 0.0, "avg_logprob": -0.06720010791204672, "compression_ratio": 1.611683848797251, "no_speech_prob": 0.039612602442502975}, {"id": 123, "seek": 73944, "start": 739.44, "end": 745.36, "text": " monitor. The closest Google got was showing how their universal translator could be used for deep fakes.", "tokens": [50364, 6002, 13, 440, 13699, 3329, 658, 390, 4099, 577, 641, 11455, 35223, 727, 312, 1143, 337, 2452, 283, 3419, 13, 50660], "temperature": 0.0, "avg_logprob": -0.08490171035130818, "compression_ratio": 1.6524822695035462, "no_speech_prob": 0.0433444045484066}, {"id": 124, "seek": 73944, "start": 745.36, "end": 751.6, "text": " Universal Translate is an experimental AI video dubbing service that helps experts", "tokens": [50660, 22617, 6531, 17593, 307, 364, 17069, 7318, 960, 18540, 4324, 2643, 300, 3665, 8572, 50972], "temperature": 0.0, "avg_logprob": -0.08490171035130818, "compression_ratio": 1.6524822695035462, "no_speech_prob": 0.0433444045484066}, {"id": 125, "seek": 73944, "start": 751.6, "end": 757.12, "text": " translate a speaker's voice while also matching their lip movements. Let me show you how it works", "tokens": [50972, 13799, 257, 8145, 311, 3177, 1339, 611, 14324, 641, 8280, 9981, 13, 961, 385, 855, 291, 577, 309, 1985, 51248], "temperature": 0.0, "avg_logprob": -0.08490171035130818, "compression_ratio": 1.6524822695035462, "no_speech_prob": 0.0433444045484066}, {"id": 126, "seek": 73944, "start": 757.12, "end": 762.5600000000001, "text": " with an online college course created in partnership with Arizona State University.", "tokens": [51248, 365, 364, 2950, 3859, 1164, 2942, 294, 9982, 365, 14723, 4533, 3535, 13, 51520], "temperature": 0.0, "avg_logprob": -0.08490171035130818, "compression_ratio": 1.6524822695035462, "no_speech_prob": 0.0433444045484066}, {"id": 127, "seek": 73944, "start": 762.5600000000001, "end": 767.44, "text": " What many college students don't realize is that knowing when to ask for help and then following", "tokens": [51520, 708, 867, 3859, 1731, 500, 380, 4325, 307, 300, 5276, 562, 281, 1029, 337, 854, 293, 550, 3480, 51764], "temperature": 0.0, "avg_logprob": -0.08490171035130818, "compression_ratio": 1.6524822695035462, "no_speech_prob": 0.0433444045484066}, {"id": 128, "seek": 76744, "start": 767.44, "end": 771.2800000000001, "text": " through and using helpful resources is actually a hallmark of becoming a productive adult.", "tokens": [50364, 807, 293, 1228, 4961, 3593, 307, 767, 257, 6500, 5638, 295, 5617, 257, 13304, 5075, 13, 50556], "temperature": 0.0, "avg_logprob": -0.12750764089087918, "compression_ratio": 1.4357798165137614, "no_speech_prob": 0.0028378311544656754}, {"id": 129, "seek": 76744, "start": 781.0400000000001, "end": 785.44, "text": " It just seems a massive black hole when one of their recent former employees,", "tokens": [51044, 467, 445, 2544, 257, 5994, 2211, 5458, 562, 472, 295, 641, 5162, 5819, 6619, 11, 51264], "temperature": 0.0, "avg_logprob": -0.12750764089087918, "compression_ratio": 1.4357798165137614, "no_speech_prob": 0.0028378311544656754}, {"id": 130, "seek": 76744, "start": 785.44, "end": 788.6400000000001, "text": " Jeffrey Hinton, had this to say this week on CNN.", "tokens": [51264, 28721, 389, 12442, 11, 632, 341, 281, 584, 341, 1243, 322, 24859, 13, 51424], "temperature": 0.0, "avg_logprob": -0.12750764089087918, "compression_ratio": 1.4357798165137614, "no_speech_prob": 0.0028378311544656754}, {"id": 131, "seek": 76744, "start": 788.6400000000001, "end": 795.2, "text": " You've spoken out saying that AI could manipulate or possibly figure out a way to kill humans?", "tokens": [51424, 509, 600, 10759, 484, 1566, 300, 7318, 727, 20459, 420, 6264, 2573, 484, 257, 636, 281, 1961, 6255, 30, 51752], "temperature": 0.0, "avg_logprob": -0.12750764089087918, "compression_ratio": 1.4357798165137614, "no_speech_prob": 0.0028378311544656754}, {"id": 132, "seek": 79520, "start": 795.44, "end": 796.6400000000001, "text": " How could it kill humans?", "tokens": [50376, 1012, 727, 309, 1961, 6255, 30, 50436], "temperature": 0.0, "avg_logprob": -0.10557886894713057, "compression_ratio": 1.8176100628930818, "no_speech_prob": 0.08855742961168289}, {"id": 133, "seek": 79520, "start": 796.6400000000001, "end": 800.32, "text": " If it gets to be much smarter than us, it'll be very good at manipulation because it will", "tokens": [50436, 759, 309, 2170, 281, 312, 709, 20294, 813, 505, 11, 309, 603, 312, 588, 665, 412, 26475, 570, 309, 486, 50620], "temperature": 0.0, "avg_logprob": -0.10557886894713057, "compression_ratio": 1.8176100628930818, "no_speech_prob": 0.08855742961168289}, {"id": 134, "seek": 79520, "start": 800.32, "end": 805.2, "text": " have learned that from us. And a very few examples of a more intelligent thing being", "tokens": [50620, 362, 3264, 300, 490, 505, 13, 400, 257, 588, 1326, 5110, 295, 257, 544, 13232, 551, 885, 50864], "temperature": 0.0, "avg_logprob": -0.10557886894713057, "compression_ratio": 1.8176100628930818, "no_speech_prob": 0.08855742961168289}, {"id": 135, "seek": 79520, "start": 805.2, "end": 809.76, "text": " controlled by a less intelligent thing. And it knows how to program, so it'll figure out ways of", "tokens": [50864, 10164, 538, 257, 1570, 13232, 551, 13, 400, 309, 3255, 577, 281, 1461, 11, 370, 309, 603, 2573, 484, 2098, 295, 51092], "temperature": 0.0, "avg_logprob": -0.10557886894713057, "compression_ratio": 1.8176100628930818, "no_speech_prob": 0.08855742961168289}, {"id": 136, "seek": 79520, "start": 809.76, "end": 814.8000000000001, "text": " getting around restrictions we put on it. It'll figure out ways of manipulating people to do", "tokens": [51092, 1242, 926, 14191, 321, 829, 322, 309, 13, 467, 603, 2573, 484, 2098, 295, 40805, 561, 281, 360, 51344], "temperature": 0.0, "avg_logprob": -0.10557886894713057, "compression_ratio": 1.8176100628930818, "no_speech_prob": 0.08855742961168289}, {"id": 137, "seek": 79520, "start": 814.8000000000001, "end": 820.72, "text": " what it wants. It's not clear to me that we can solve this problem. I believe we should put a big", "tokens": [51344, 437, 309, 2738, 13, 467, 311, 406, 1850, 281, 385, 300, 321, 393, 5039, 341, 1154, 13, 286, 1697, 321, 820, 829, 257, 955, 51640], "temperature": 0.0, "avg_logprob": -0.10557886894713057, "compression_ratio": 1.8176100628930818, "no_speech_prob": 0.08855742961168289}, {"id": 138, "seek": 79520, "start": 820.72, "end": 824.72, "text": " effort into thinking about ways to solve the problem. I don't have a solution at present.", "tokens": [51640, 4630, 666, 1953, 466, 2098, 281, 5039, 264, 1154, 13, 286, 500, 380, 362, 257, 3827, 412, 1974, 13, 51840], "temperature": 0.0, "avg_logprob": -0.10557886894713057, "compression_ratio": 1.8176100628930818, "no_speech_prob": 0.08855742961168289}, {"id": 139, "seek": 82472, "start": 824.72, "end": 829.28, "text": " I just want people to be aware that this is a really serious problem and we need to be thinking", "tokens": [50364, 286, 445, 528, 561, 281, 312, 3650, 300, 341, 307, 257, 534, 3156, 1154, 293, 321, 643, 281, 312, 1953, 50592], "temperature": 0.0, "avg_logprob": -0.0766703436307818, "compression_ratio": 1.57, "no_speech_prob": 0.0004952818271704018}, {"id": 140, "seek": 82472, "start": 829.28, "end": 833.6800000000001, "text": " about it very hard. This all seems particularly relevant when Google made this announcement", "tokens": [50592, 466, 309, 588, 1152, 13, 639, 439, 2544, 4098, 7340, 562, 3329, 1027, 341, 12847, 50812], "temperature": 0.0, "avg_logprob": -0.0766703436307818, "compression_ratio": 1.57, "no_speech_prob": 0.0004952818271704018}, {"id": 141, "seek": 82472, "start": 833.6800000000001, "end": 840.5600000000001, "text": " about Gemini, their rival to GPT-5. All this helps set the stage for the inflection point we are at", "tokens": [50812, 466, 22894, 3812, 11, 641, 16286, 281, 26039, 51, 12, 20, 13, 1057, 341, 3665, 992, 264, 3233, 337, 264, 1536, 5450, 935, 321, 366, 412, 51156], "temperature": 0.0, "avg_logprob": -0.0766703436307818, "compression_ratio": 1.57, "no_speech_prob": 0.0004952818271704018}, {"id": 142, "seek": 82472, "start": 840.5600000000001, "end": 846.64, "text": " today. We recently brought these two teams together into a single unit, Google DeepMind.", "tokens": [51156, 965, 13, 492, 3938, 3038, 613, 732, 5491, 1214, 666, 257, 2167, 4985, 11, 3329, 14895, 44, 471, 13, 51460], "temperature": 0.0, "avg_logprob": -0.0766703436307818, "compression_ratio": 1.57, "no_speech_prob": 0.0004952818271704018}, {"id": 143, "seek": 82472, "start": 847.36, "end": 853.0400000000001, "text": " Using the computational resources of Google, they are focused on building more capable systems", "tokens": [51496, 11142, 264, 28270, 3593, 295, 3329, 11, 436, 366, 5178, 322, 2390, 544, 8189, 3652, 51780], "temperature": 0.0, "avg_logprob": -0.0766703436307818, "compression_ratio": 1.57, "no_speech_prob": 0.0004952818271704018}, {"id": 144, "seek": 85304, "start": 853.76, "end": 860.56, "text": " safely and responsibly. This includes our next generation foundation model, Gemini,", "tokens": [50400, 11750, 293, 2914, 3545, 13, 639, 5974, 527, 958, 5125, 7030, 2316, 11, 22894, 3812, 11, 50740], "temperature": 0.0, "avg_logprob": -0.07939849076447664, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.0016477933386340737}, {"id": 145, "seek": 85304, "start": 860.56, "end": 866.4, "text": " which is still in training. Gemini was created from the ground up to be multi-modal,", "tokens": [50740, 597, 307, 920, 294, 3097, 13, 22894, 3812, 390, 2942, 490, 264, 2727, 493, 281, 312, 4825, 12, 8014, 304, 11, 51032], "temperature": 0.0, "avg_logprob": -0.07939849076447664, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.0016477933386340737}, {"id": 146, "seek": 85304, "start": 867.28, "end": 873.5999999999999, "text": " highly efficient at tool and API integrations, and built to enable future innovations", "tokens": [51076, 5405, 7148, 412, 2290, 293, 9362, 3572, 763, 11, 293, 3094, 281, 9528, 2027, 24283, 51392], "temperature": 0.0, "avg_logprob": -0.07939849076447664, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.0016477933386340737}, {"id": 147, "seek": 85304, "start": 873.5999999999999, "end": 879.28, "text": " like memory and planning. That ability to plan may ring a bell from the GPT-4", "tokens": [51392, 411, 4675, 293, 5038, 13, 663, 3485, 281, 1393, 815, 4875, 257, 4549, 490, 264, 26039, 51, 12, 19, 51676], "temperature": 0.0, "avg_logprob": -0.07939849076447664, "compression_ratio": 1.509090909090909, "no_speech_prob": 0.0016477933386340737}, {"id": 148, "seek": 87928, "start": 880.0799999999999, "end": 884.4, "text": " which said this, novel capabilities often emerge in more powerful models.", "tokens": [50404, 597, 848, 341, 11, 7613, 10862, 2049, 21511, 294, 544, 4005, 5245, 13, 50620], "temperature": 0.0, "avg_logprob": -0.11450276772181193, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.06368277966976166}, {"id": 149, "seek": 87928, "start": 884.4, "end": 890.16, "text": " Some that are particularly concerning are the ability to create and act on long-term plans.", "tokens": [50620, 2188, 300, 366, 4098, 18087, 366, 264, 3485, 281, 1884, 293, 605, 322, 938, 12, 7039, 5482, 13, 50908], "temperature": 0.0, "avg_logprob": -0.11450276772181193, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.06368277966976166}, {"id": 150, "seek": 87928, "start": 890.16, "end": 895.76, "text": " Remember, Google didn't identify planning as a risk but as a selling point for Gemini.", "tokens": [50908, 5459, 11, 3329, 994, 380, 5876, 5038, 382, 257, 3148, 457, 382, 257, 6511, 935, 337, 22894, 3812, 13, 51188], "temperature": 0.0, "avg_logprob": -0.11450276772181193, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.06368277966976166}, {"id": 151, "seek": 87928, "start": 895.76, "end": 900.88, "text": " Next, Google talked about accelerating their progress, which was again directly mentioned", "tokens": [51188, 3087, 11, 3329, 2825, 466, 34391, 641, 4205, 11, 597, 390, 797, 3838, 2835, 51444], "temperature": 0.0, "avg_logprob": -0.11450276772181193, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.06368277966976166}, {"id": 152, "seek": 87928, "start": 900.88, "end": 905.92, "text": " in the GPT-4 technical report. It said, one concern of particular importance to open AI", "tokens": [51444, 294, 264, 26039, 51, 12, 19, 6191, 2275, 13, 467, 848, 11, 472, 3136, 295, 1729, 7379, 281, 1269, 7318, 51696], "temperature": 0.0, "avg_logprob": -0.11450276772181193, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.06368277966976166}, {"id": 153, "seek": 90592, "start": 905.92, "end": 910.0799999999999, "text": " is the risk of racing dynamics leading to a decline in safety standards,", "tokens": [50364, 307, 264, 3148, 295, 12553, 15679, 5775, 281, 257, 15635, 294, 4514, 7787, 11, 50572], "temperature": 0.0, "avg_logprob": -0.11590708575202423, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.10369841754436493}, {"id": 154, "seek": 90592, "start": 910.0799999999999, "end": 915.8399999999999, "text": " the diffusion of bad norms and accelerated AI timelines, each of which heightens societal", "tokens": [50572, 264, 25242, 295, 1578, 24357, 293, 29763, 7318, 45886, 11, 1184, 295, 597, 6681, 694, 33472, 50860], "temperature": 0.0, "avg_logprob": -0.11590708575202423, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.10369841754436493}, {"id": 155, "seek": 90592, "start": 915.8399999999999, "end": 921.92, "text": " risks associated with AI. We refer to these here as acceleration risk and make no mistake,", "tokens": [50860, 10888, 6615, 365, 7318, 13, 492, 2864, 281, 613, 510, 382, 17162, 3148, 293, 652, 572, 6146, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11590708575202423, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.10369841754436493}, {"id": 156, "seek": 90592, "start": 921.92, "end": 928.88, "text": " Gemini will be very accelerated from Palm II. It looks set to use the TPU V5 chip,", "tokens": [51164, 22894, 3812, 486, 312, 588, 29763, 490, 32668, 6351, 13, 467, 1542, 992, 281, 764, 264, 314, 8115, 691, 20, 11409, 11, 51512], "temperature": 0.0, "avg_logprob": -0.11590708575202423, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.10369841754436493}, {"id": 157, "seek": 90592, "start": 928.88, "end": 934.8, "text": " which was announced back in January of last year. And on page 91 of the Palm II technical report,", "tokens": [51512, 597, 390, 7548, 646, 294, 7061, 295, 1036, 1064, 13, 400, 322, 3028, 31064, 295, 264, 32668, 6351, 6191, 2275, 11, 51808], "temperature": 0.0, "avg_logprob": -0.11590708575202423, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.10369841754436493}, {"id": 158, "seek": 93480, "start": 934.8, "end": 940.9599999999999, "text": " they say that that model used TPU V4. Now, it should be said that Palm II is leading to some", "tokens": [50364, 436, 584, 300, 300, 2316, 1143, 314, 8115, 691, 19, 13, 823, 11, 309, 820, 312, 848, 300, 32668, 6351, 307, 5775, 281, 512, 50672], "temperature": 0.0, "avg_logprob": -0.12308570190712258, "compression_ratio": 1.63, "no_speech_prob": 0.0011512956116348505}, {"id": 159, "seek": 93480, "start": 940.9599999999999, "end": 946.24, "text": " impressive medical applications. As I actually first reported on seven weeks ago without quite", "tokens": [50672, 8992, 4625, 5821, 13, 1018, 286, 767, 700, 7055, 322, 3407, 3259, 2057, 1553, 1596, 50936], "temperature": 0.0, "avg_logprob": -0.12308570190712258, "compression_ratio": 1.63, "no_speech_prob": 0.0011512956116348505}, {"id": 160, "seek": 93480, "start": 946.24, "end": 951.4399999999999, "text": " realizing it, here's MedPalm II. We believe large language models have the potential to revolutionize", "tokens": [50936, 16734, 309, 11, 510, 311, 3982, 29141, 76, 6351, 13, 492, 1697, 2416, 2856, 5245, 362, 264, 3995, 281, 8894, 1125, 51196], "temperature": 0.0, "avg_logprob": -0.12308570190712258, "compression_ratio": 1.63, "no_speech_prob": 0.0011512956116348505}, {"id": 161, "seek": 93480, "start": 951.4399999999999, "end": 956.24, "text": " healthcare and benefit society. MedPalm is a large language model that we've taken and tuned for the", "tokens": [51196, 8884, 293, 5121, 4086, 13, 3982, 29141, 76, 307, 257, 2416, 2856, 2316, 300, 321, 600, 2726, 293, 10870, 337, 264, 51436], "temperature": 0.0, "avg_logprob": -0.12308570190712258, "compression_ratio": 1.63, "no_speech_prob": 0.0011512956116348505}, {"id": 162, "seek": 93480, "start": 956.24, "end": 962.9599999999999, "text": " medical domain. Medical question answering has been a research grand challenge for several decades", "tokens": [51436, 4625, 9274, 13, 15896, 1168, 13430, 575, 668, 257, 2132, 2697, 3430, 337, 2940, 7878, 51772], "temperature": 0.0, "avg_logprob": -0.12308570190712258, "compression_ratio": 1.63, "no_speech_prob": 0.0011512956116348505}, {"id": 163, "seek": 96296, "start": 962.96, "end": 967.2800000000001, "text": " but till date the progress has been kind of slow. But then over the course of the last", "tokens": [50364, 457, 4288, 4002, 264, 4205, 575, 668, 733, 295, 2964, 13, 583, 550, 670, 264, 1164, 295, 264, 1036, 50580], "temperature": 0.0, "avg_logprob": -0.09320427242078279, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0009847725741565228}, {"id": 164, "seek": 96296, "start": 968.0, "end": 973.2800000000001, "text": " three to four months, first with MedPalm and MedPalm II, we have kind of like broken through that", "tokens": [50616, 1045, 281, 1451, 2493, 11, 700, 365, 3982, 29141, 76, 293, 3982, 29141, 76, 6351, 11, 321, 362, 733, 295, 411, 5463, 807, 300, 50880], "temperature": 0.0, "avg_logprob": -0.09320427242078279, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0009847725741565228}, {"id": 165, "seek": 96296, "start": 973.2800000000001, "end": 979.2, "text": " barrier. Unlike previous versions, MedPalm II was able to score 85% on the USMLA medical licensing", "tokens": [50880, 13357, 13, 17657, 3894, 9606, 11, 3982, 29141, 76, 6351, 390, 1075, 281, 6175, 14695, 4, 322, 264, 2546, 44, 11435, 4625, 29759, 51176], "temperature": 0.0, "avg_logprob": -0.09320427242078279, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0009847725741565228}, {"id": 166, "seek": 96296, "start": 979.2, "end": 984.0, "text": " exam. Yeah, this is immensely exciting because people have been working on medical question", "tokens": [51176, 1139, 13, 865, 11, 341, 307, 38674, 4670, 570, 561, 362, 668, 1364, 322, 4625, 1168, 51416], "temperature": 0.0, "avg_logprob": -0.09320427242078279, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0009847725741565228}, {"id": 167, "seek": 96296, "start": 984.0, "end": 988.8000000000001, "text": " answering for over three decades. And finally, we are at a stage where we can say with confidence", "tokens": [51416, 13430, 337, 670, 1045, 7878, 13, 400, 2721, 11, 321, 366, 412, 257, 3233, 689, 321, 393, 584, 365, 6687, 51656], "temperature": 0.0, "avg_logprob": -0.09320427242078279, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.0009847725741565228}, {"id": 168, "seek": 98880, "start": 988.8, "end": 994.4, "text": " that AI systems can now at least answer USMLA questions as good as experts. As many of you", "tokens": [50364, 300, 7318, 3652, 393, 586, 412, 1935, 1867, 2546, 44, 11435, 1651, 382, 665, 382, 8572, 13, 1018, 867, 295, 291, 50644], "temperature": 0.0, "avg_logprob": -0.07337519499632689, "compression_ratio": 1.5785953177257526, "no_speech_prob": 0.009123318828642368}, {"id": 169, "seek": 98880, "start": 994.4, "end": 1000.4, "text": " may know, the CEO of Google as well as the CEO of Microsoft and Sam Altman and the CEO of Anthropic", "tokens": [50644, 815, 458, 11, 264, 9282, 295, 3329, 382, 731, 382, 264, 9282, 295, 8116, 293, 4832, 15992, 1601, 293, 264, 9282, 295, 12727, 39173, 50944], "temperature": 0.0, "avg_logprob": -0.07337519499632689, "compression_ratio": 1.5785953177257526, "no_speech_prob": 0.009123318828642368}, {"id": 170, "seek": 98880, "start": 1000.4, "end": 1005.8399999999999, "text": " all went to the White House to discuss AI risk and opportunity. But given that the main outcome", "tokens": [50944, 439, 1437, 281, 264, 5552, 4928, 281, 2248, 7318, 3148, 293, 2650, 13, 583, 2212, 300, 264, 2135, 9700, 51216], "temperature": 0.0, "avg_logprob": -0.07337519499632689, "compression_ratio": 1.5785953177257526, "no_speech_prob": 0.009123318828642368}, {"id": 171, "seek": 98880, "start": 1005.8399999999999, "end": 1012.0, "text": " from that seems to be 140 million to establish seven new AI research institutes, that feels a", "tokens": [51216, 490, 300, 2544, 281, 312, 21548, 2459, 281, 8327, 3407, 777, 7318, 2132, 4348, 1819, 11, 300, 3417, 257, 51524], "temperature": 0.0, "avg_logprob": -0.07337519499632689, "compression_ratio": 1.5785953177257526, "no_speech_prob": 0.009123318828642368}, {"id": 172, "seek": 98880, "start": 1012.0, "end": 1016.8, "text": " little slow given all the acceleration that's occurring. Because as Google somewhat soberly", "tokens": [51524, 707, 2964, 2212, 439, 264, 17162, 300, 311, 18386, 13, 1436, 382, 3329, 8344, 26212, 356, 51764], "temperature": 0.0, "avg_logprob": -0.07337519499632689, "compression_ratio": 1.5785953177257526, "no_speech_prob": 0.009123318828642368}, {"id": 173, "seek": 101680, "start": 1016.8, "end": 1021.68, "text": " conclude their report, we believe that further scaling of both model parameters and data set", "tokens": [50364, 16886, 641, 2275, 11, 321, 1697, 300, 3052, 21589, 295, 1293, 2316, 9834, 293, 1412, 992, 50608], "temperature": 0.0, "avg_logprob": -0.10694680895124163, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.02675316482782364}, {"id": 174, "seek": 101680, "start": 1021.68, "end": 1026.72, "text": " size and quality as well as improvements in the architecture and objective will continue to yield", "tokens": [50608, 2744, 293, 3125, 382, 731, 382, 13797, 294, 264, 9482, 293, 10024, 486, 2354, 281, 11257, 50860], "temperature": 0.0, "avg_logprob": -0.10694680895124163, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.02675316482782364}, {"id": 175, "seek": 101680, "start": 1026.72, "end": 1032.8, "text": " gains in language understanding and generation. They are not slowing down and the world hasn't", "tokens": [50860, 16823, 294, 2856, 3701, 293, 5125, 13, 814, 366, 406, 26958, 760, 293, 264, 1002, 6132, 380, 51164], "temperature": 0.0, "avg_logprob": -0.10694680895124163, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.02675316482782364}, {"id": 176, "seek": 101680, "start": 1032.8, "end": 1046.6399999999999, "text": " yet caught up. Thank you so much for watching to the end and have a wonderful day.", "tokens": [51164, 1939, 5415, 493, 13, 1044, 291, 370, 709, 337, 1976, 281, 264, 917, 293, 362, 257, 3715, 786, 13, 51856], "temperature": 0.0, "avg_logprob": -0.10694680895124163, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.02675316482782364}], "language": "en"}