1
00:00:00,000 --> 00:00:06,080
Like buses, AI news can sometimes be slow and sometimes arrive all at once.

2
00:00:06,080 --> 00:00:12,640
In the last few days we have had dramatic new leaked insights into the sheer breadth of Google's

3
00:00:12,640 --> 00:00:18,960
Gemini. Just today we've had the release of Meta's Code Lama and earlier their impressive

4
00:00:18,960 --> 00:00:26,800
multilingual seamless M4T model. And last but definitely not least, this 88-page AI

5
00:00:26,880 --> 00:00:32,640
consciousness report. And yes, I read it all, it's juicy so I'm saving that for the end.

6
00:00:32,640 --> 00:00:38,720
But let's start with two major paywall articles, one from the information and one from the New York

7
00:00:38,720 --> 00:00:45,040
Times about Google's Gemini model. From both of them I counted a total of nine new revelations,

8
00:00:45,040 --> 00:00:48,480
so let's get straight to it. To give you a sense of timeline, by the way,

9
00:00:48,480 --> 00:00:55,440
Google's newly merged AI SWOT team, they call it, is preparing for a big fall or autumn launch.

10
00:00:55,440 --> 00:01:01,200
The takeaway for me from both articles is that Gemini is going to be the everything model. Did

11
00:01:01,200 --> 00:01:07,920
you know it's going to be the rival to mid-journey and stable diffusion? Mid-journey only has 11

12
00:01:07,920 --> 00:01:14,000
full-time staff, so it is more than plausible that Google's Gemini could outperform mid-journey

13
00:01:14,000 --> 00:01:21,120
version 5. Next we may be able to create graphics with just text descriptions and control software

14
00:01:21,120 --> 00:01:26,640
using only text or voice commands. These next two are speculations, so I'm not even counting them

15
00:01:26,640 --> 00:01:32,160
in the list of leaks. I've already covered in a previous video that Gemini has been trained on

16
00:01:32,160 --> 00:01:38,800
YouTube video transcripts, and the speculation is that by integrating video and audio into Gemini,

17
00:01:38,800 --> 00:01:44,160
it could perhaps help a mechanic diagnose a problem with a car repair based on a video,

18
00:01:44,160 --> 00:01:50,720
or be a rival to Runway ML by generating advanced text to video based on descriptions of what a user

19
00:01:50,720 --> 00:01:55,440
wants to see. You can start to see why I'm beginning to think of it as the everything model.

20
00:01:55,440 --> 00:02:01,120
Another leak is that one of the co-founders of Google, Sergey Brin, is working on the front lines

21
00:02:01,120 --> 00:02:06,320
of Google Gemini. And lastly from this article, I found it really interesting that Google's

22
00:02:06,320 --> 00:02:12,560
lawyers have been closely evaluating the training, and they made researchers remove training data

23
00:02:12,560 --> 00:02:18,160
that had come from textbooks, even though those textbooks helped the model answer questions about

24
00:02:18,160 --> 00:02:24,080
subjects like astronomy or biology. And I do wonder if they privately benchmarked Gemini before

25
00:02:24,080 --> 00:02:30,720
removing that crucial textbook data. But if that's not enough, prepare to also receive life advice.

26
00:02:30,720 --> 00:02:36,000
My theory here is that Google wants to compete directly for market share with inflection's

27
00:02:36,000 --> 00:02:41,280
pie. What if you want scientific, creative, or professional writing? Yep, they're working on

28
00:02:41,280 --> 00:02:46,480
that too. In fact, we already know that Google has software named Genesis that they're pitching

29
00:02:46,480 --> 00:02:51,920
to the New York Times, which can generate news articles, rewrite them, suggest headlines, etc.

30
00:02:51,920 --> 00:02:56,720
But some people will be more interested in this feature that Google DeepMind is working on,

31
00:02:56,720 --> 00:03:02,480
the ability to draft critiques of an argument and generate quizzes, word, and number puzzles.

32
00:03:02,480 --> 00:03:08,320
It's almost easier at this point to ask what might Google Gemini not be able to do. And yes,

33
00:03:08,320 --> 00:03:15,440
this is not Gemini, but Google DeepMind is also using AI to design the next generation of semiconductors.

34
00:03:15,440 --> 00:03:21,120
But if the fall seems far away, how about today when we got CodeLama from Meta?

35
00:03:21,120 --> 00:03:27,200
I spent much of the last two hours reading most of the 47-page paper, and you can see CodeLama

36
00:03:27,200 --> 00:03:33,440
in action on screen. Some highlights include that the CodeLama models provide stable generations with

37
00:03:33,440 --> 00:03:39,360
up to 100,000 tokens of context. Obviously, that could be used for generating longer programs or

38
00:03:39,360 --> 00:03:44,240
providing the model with more context from your code base to make the generations more relevant.

39
00:03:44,240 --> 00:03:49,200
It comes in three versions, CodeLama, CodeLama Instruct, which can better understand natural

40
00:03:49,200 --> 00:03:54,080
language instructions, and CodeLama Python, better, of course, at Python. It's available

41
00:03:54,080 --> 00:04:00,800
for commercial use, and as you can see, some of the versions rival GPT 3.5 on human eval.

42
00:04:00,800 --> 00:04:07,840
That top score of 53.7% on Passat 1 puts it in the same ballpark as Phi 1. I've actually done

43
00:04:07,840 --> 00:04:15,120
a full video on Phi 1, so do check that out, but that got 50.6%. But it is about 25 times smaller

44
00:04:15,120 --> 00:04:20,480
at 1.3 billion parameters. Interestingly, the CodeLama paper, which also came out about two

45
00:04:20,480 --> 00:04:26,400
hours ago, mentions Phi 1 directly, saying that it follows in a similar spirit, but the difference

46
00:04:26,400 --> 00:04:32,240
is that Phi 1 is closed source. Anyway, a couple more interesting things before we move on from

47
00:04:32,320 --> 00:04:38,080
CodeLama, and the first one is the self-instruct method that they used. Let me know if you also

48
00:04:38,080 --> 00:04:44,240
find this fascinating, because step one was to generate 62,000 interview style programming

49
00:04:44,240 --> 00:04:49,760
questions by prompting Lama 2, the 70 billion parameter model. Then they removed duplicates in

50
00:04:49,760 --> 00:04:53,920
step two. But here's where it gets interesting. For each of those questions, they first generated

51
00:04:53,920 --> 00:05:00,560
a unit test by prompting CodeLama 7 billion parameters. Then they generated 10 Python solutions

52
00:05:00,560 --> 00:05:06,080
by prompting CodeLama. Finally, they ran unit tests on those 10 solutions, and they added the

53
00:05:06,080 --> 00:05:10,880
first solution that passes those tests, along with the corresponding question and test, to the

54
00:05:10,880 --> 00:05:15,280
self-instruct data set. If that sounded a bit complicated, let me try to distill it a bit.

55
00:05:15,280 --> 00:05:22,080
They asked the Big Brother Lama 2 model to generate questions, then got the Little Brother CodeLama

56
00:05:22,080 --> 00:05:28,400
to generate tests for those questions, then got the model to generate solutions to its own tests,

57
00:05:28,400 --> 00:05:33,280
found the good solutions that don't forget it produced, and then used those to further train

58
00:05:33,280 --> 00:05:39,520
the model. To be honest, synthetic data and self-instruct seem to be the future of feedback.

59
00:05:39,520 --> 00:05:45,200
One final interesting quote from the paper on safety, and that was an argument advanced by one

60
00:05:45,200 --> 00:05:51,360
of their red teamers. They made the point that various scripts and code is readily available on

61
00:05:51,360 --> 00:05:56,640
mainstream public websites, hacking forums, or the dark web. And the advanced malware development

62
00:05:56,640 --> 00:06:02,560
is beyond the current capabilities of available LLMs. And even an advanced LLM paired with an

63
00:06:02,560 --> 00:06:08,240
expert malware developer is not particularly useful at the moment, as the barrier is not typically

64
00:06:08,240 --> 00:06:14,560
writing the malware code itself. Let me know what you think in the comments. But we must move on to

65
00:06:14,560 --> 00:06:22,080
seamless M4T released a couple of days ago from Meta, which frankly seems amazing for multi-lingual

66
00:06:22,080 --> 00:06:28,160
translation. That's speech to text, speech to speech, text to text, and more. It has speech

67
00:06:28,160 --> 00:06:34,800
recognition for nearly 100 languages and can output in 36 languages. But there's one feature I find

68
00:06:34,800 --> 00:06:42,800
particularly cool. Now, let's talk about code switching. Code switching happens when a multilingual

69
00:06:42,800 --> 00:06:48,800
speaker switches between languages while they are speaking. Our model seamless M4T automatically

70
00:06:48,800 --> 00:06:54,640
recognizes and translates more than one language when mixed in the same sentence. As a multilingual

71
00:06:54,640 --> 00:07:00,000
speaker, this is a very exciting capability for me. I often switch from Hindi to Telugu

72
00:07:00,000 --> 00:07:04,960
when I speak with my dad. Notice in the following example when I change languages.

73
00:07:19,040 --> 00:07:27,280
I can speak Hindi, Telugu, and English. Sometimes I use all three languages in one conversation.

74
00:07:27,840 --> 00:07:34,800
Speaking of cool though, we had this epic story out yesterday. AI gave a paralyzed woman her voice

75
00:07:34,800 --> 00:07:41,360
back. In a moment, you're going to see her being plugged in to the model. There we go. And the short

76
00:07:41,360 --> 00:07:47,760
version is that this woman suffered a stroke that left her unable to speak. But now for the first

77
00:07:47,760 --> 00:07:54,160
time, her speech and facial expressions can be synthesized from her brain signals, decoding

78
00:07:54,160 --> 00:08:00,080
these signals into text at nearly 80 words per minute up from 14 words per minute. But let's

79
00:08:00,080 --> 00:08:06,400
now end on this, an 88 page report on consciousness in artificial intelligence, which counts as one

80
00:08:06,400 --> 00:08:12,240
of its co-authors, Yoshua Benjio, the Turing Award winner. It was dense and quite technical,

81
00:08:12,240 --> 00:08:18,640
but well worth the read. Look at this sentence in just the abstract. Our analysis suggests that no

82
00:08:18,640 --> 00:08:24,240
current AI systems are conscious, but also suggests that there are no obvious technical barriers to

83
00:08:24,240 --> 00:08:31,600
building AI systems which satisfy these indicators. These are the indicators and each one gets a few

84
00:08:31,600 --> 00:08:37,760
pages in the report. And the reason that they're split up is because each one rests on a certain

85
00:08:37,760 --> 00:08:44,240
theory of consciousness. Obviously, the key problem is that we don't have a consensus theory on what

86
00:08:44,240 --> 00:08:50,000
consciousness is or how it comes about. So in a way, to hedge their bets, they group in different

87
00:08:50,000 --> 00:08:55,200
theories and look at the kind of indicators that would satisfy each one. You might say that list

88
00:08:55,200 --> 00:09:00,960
seems so theoretical. Why not just test the model or even ask the model? For more on that approach,

89
00:09:01,040 --> 00:09:05,760
see my theory of mind video. But the problem is, as they say on page four,

90
00:09:05,760 --> 00:09:12,320
the main alternative to a theory heavy approach is to use behavioral tests for consciousness.

91
00:09:12,320 --> 00:09:17,280
But as I talked about in the other video, that method is unreliable because AI systems can be

92
00:09:17,280 --> 00:09:22,800
trained, of course they are, to mimic human behaviors, are working actually in very different

93
00:09:22,800 --> 00:09:28,640
ways. Essentially, LLMs have broken the traditional tests for consciousness, including of course the

94
00:09:28,640 --> 00:09:33,600
Turing test. The paper also rests on the assumption of computational functionalism,

95
00:09:33,600 --> 00:09:38,960
essentially that computations are essential for consciousness. As in, it's not what you're made

96
00:09:38,960 --> 00:09:44,960
of, it's what you do. If this is wrong, and the substrate in fact is key, say biological cells,

97
00:09:44,960 --> 00:09:49,680
then it stands to reason that AI would never be conscious. But one of their early conclusions

98
00:09:49,680 --> 00:09:55,440
is that if computational functionalism is true, and it is widely believed, conscious AI systems

99
00:09:55,440 --> 00:10:00,640
could realistically be built in the near term. Having digested the entire paper, they're strongly

100
00:10:00,640 --> 00:10:06,240
suggesting that we're not there yet. But if this theory is true, we could be there, especially if

101
00:10:06,240 --> 00:10:12,160
researchers deliberately designed systems to meet these criteria. In fact, here is a key quote from

102
00:10:12,160 --> 00:10:17,760
one of the authors in Science that came along with the piece. It would be trivial to design all of

103
00:10:17,760 --> 00:10:23,600
these features into an AI. The reason no one has done so is it is not clear that they would be useful

104
00:10:23,920 --> 00:10:29,600
for tasks. Now, to be honest, it is way beyond my pay grade to try to explain every aspect of the

105
00:10:29,600 --> 00:10:35,440
paper. But I'm going to try my best to convey the key bits. First, what is the definition of

106
00:10:35,440 --> 00:10:40,560
consciousness that they are working with? Well, skipping the jargon, they essentially say, if you

107
00:10:40,560 --> 00:10:46,160
are reading this report on a screen, you are having a conscious visual experience of the screen. That

108
00:10:46,160 --> 00:10:52,000
is separated from sentence, which is also sometimes used to mean being capable of pleasure or pain.

109
00:10:52,000 --> 00:10:56,640
And they say that it's possible for a system to be sentient without being conscious by sensing

110
00:10:56,640 --> 00:11:02,080
its body or environment. And it's possible for a system to be conscious without sensing its body

111
00:11:02,080 --> 00:11:07,920
or environment. It also might be possible to be slightly conscious or conscious to a greater

112
00:11:07,920 --> 00:11:14,560
degree than humans. Ilya Sutskova famously said, it may be that today's large neural networks are

113
00:11:14,560 --> 00:11:20,240
slightly conscious. And the Karl Schulman and Nick Bostrom wrote an entire chapter of a book

114
00:11:20,240 --> 00:11:25,120
on the possibility that models become more conscious than human beings. They say such

115
00:11:25,120 --> 00:11:29,920
beings could contribute immense value to the world and failing to respect their interests could

116
00:11:29,920 --> 00:11:35,760
produce a moral catastrophe. One of the theories of consciousness discussed is recurrent processing

117
00:11:35,760 --> 00:11:41,680
theory. And here is the key part of that theory. One initial feed forward sweep of activity through

118
00:11:41,680 --> 00:11:47,120
the hierarchy of visual areas is sufficient for some visual operations like extracting features

119
00:11:47,120 --> 00:11:53,040
from a scene, but not sufficient for conscious experience. However, when the stimulus is sufficiently

120
00:11:53,040 --> 00:11:58,320
strong or salient, we get this looped recurrent processing in which signals are sent back from

121
00:11:58,320 --> 00:12:05,280
higher areas to lower ones. It's only then that you get a conscious representation of an organized

122
00:12:05,280 --> 00:12:11,200
scene. The paper then draws indicators based on each theory. For example, if recurrent processing

123
00:12:11,200 --> 00:12:17,840
theory is accurate, then here are two indicators that something would be conscious. They then draw

124
00:12:17,840 --> 00:12:24,320
analogies for each theory to AI systems. For example, on recurrence, specifically algorithmic

125
00:12:24,320 --> 00:12:29,600
recurrence, they say that's a weak condition that many AI systems already meet. But don't forget

126
00:12:29,600 --> 00:12:35,200
when they say that it's an analogy. Not only does it require the theory to be correct, it requires

127
00:12:35,200 --> 00:12:41,280
the analogy to hold true. i.e. is the recurrence that we see in AI a good analogy for the recurrence

128
00:12:41,280 --> 00:12:47,360
of this theory. Or what about the next one, global workspace theory? If that theory is correct, here

129
00:12:47,360 --> 00:12:52,720
are four indicators of something being conscious according to that theory. To be honest, if you

130
00:12:52,720 --> 00:12:58,880
are at all interested in consciousness, the pages on each one of these taught me a lot about tests

131
00:12:58,880 --> 00:13:04,240
for consciousness and just theories of consciousness. But again, let's just say that theory is correct.

132
00:13:04,240 --> 00:13:10,080
Do AI systems demonstrate these indicators? Do they have modules that can work in parallel

133
00:13:10,080 --> 00:13:16,640
and a global workspace at the center? Is that workspace bandwidth limited, requiring the compression

134
00:13:16,640 --> 00:13:23,200
and selection of information from the modules? Well, here again, we can only rely on analogies,

135
00:13:23,200 --> 00:13:28,640
in this case, to the transformer architecture. They say, in a sense, they do have modules,

136
00:13:28,640 --> 00:13:33,520
they do have a limited capacity workspace introducing a bottleneck, but then the authors

137
00:13:33,520 --> 00:13:39,280
introduce plenty of points about how the analogy is not perfect, even here. Of course, you can pause

138
00:13:39,280 --> 00:13:44,720
and read the details if you like, or indeed read the entire paper. So that's the tone of the paper.

139
00:13:44,720 --> 00:13:51,200
If silicon can be a replacement to carbon, and if these analogies hold, then there is a strong

140
00:13:51,200 --> 00:13:56,240
case that most or all of the conditions for consciousness, suggested by current computational

141
00:13:56,240 --> 00:14:01,600
theories, can be met using existing techniques in AI. This is not to say that current AI systems

142
00:14:01,600 --> 00:14:05,920
are likely to be conscious. There is also the issue of whether they combine existing techniques

143
00:14:05,920 --> 00:14:10,800
in the right ways. But it does suggest that conscious AI is not merely a remote possibility in the

144
00:14:10,800 --> 00:14:16,640
distant future. And here is the key bit. If it is at all possible to build conscious AI systems

145
00:14:16,640 --> 00:14:22,240
without radically new hardware, it may be possible now. Of course, even if all of those conditions

146
00:14:22,240 --> 00:14:28,160
and analogies hold, it may not be the same type of consciousness as our consciousness. It seems

147
00:14:28,160 --> 00:14:33,840
possible, they say, to imagine a conscious being that had only a succession of brief static

148
00:14:33,840 --> 00:14:38,640
discrete experiences, perhaps just during pre-training. Or they might have experiences

149
00:14:38,640 --> 00:14:44,320
without feeling that they are a persisting subject. But my own summary is this. We clearly

150
00:14:44,320 --> 00:14:49,760
don't fully understand consciousness or what is required for consciousness. We don't know if

151
00:14:49,760 --> 00:14:55,520
consciousness in AI systems is theoretically impossible or imminent. The authors actually

152
00:14:55,520 --> 00:15:00,800
quote this open letter from the Association for Mathematical Consciousness Science. And in it,

153
00:15:00,800 --> 00:15:06,240
at the end, the letter says, we emphasize that the rapid development of AI is exposing the urgent

154
00:15:06,240 --> 00:15:11,200
need to accelerate research in the field of consciousness science, even if we develop a

155
00:15:11,200 --> 00:15:16,160
system that ticks all of these indicators. And trust me, someone is probably working on that

156
00:15:16,160 --> 00:15:21,440
right now. We still won't know for sure, and many people will deny forever that that system

157
00:15:21,440 --> 00:15:25,760
is conscious. I'm not claiming I have the answer, by the way. I have absolutely no idea

158
00:15:25,760 --> 00:15:30,080
if these systems are imminently conscious, already conscious, or will never be conscious.

159
00:15:30,080 --> 00:15:35,120
All I can say is that it's a bit less sci-fi than many people believe. And the authors also point

160
00:15:35,120 --> 00:15:40,560
out two risks, under attributing consciousness to AI, playing down the possibility. But they also

161
00:15:40,560 --> 00:15:46,000
point out the risk of over attributing consciousness to AI. On under attributing consciousness,

162
00:15:46,000 --> 00:15:51,280
they say this, given the uncertainties about consciousness mentioned above, we may create

163
00:15:51,280 --> 00:15:56,720
conscious AI systems long before we recognize that we have done so. And I see this sentence as a

164
00:15:56,720 --> 00:16:02,480
fateful prediction. This tendency is further amplified when AI systems exhibit human-like

165
00:16:02,480 --> 00:16:08,000
characteristics, such as natural language processing, which they already do, but also facial expressions

166
00:16:08,000 --> 00:16:12,800
or adaptive learning capabilities. So imagine what people are going to think when photo-realistic

167
00:16:12,800 --> 00:16:19,360
AI avatars are everywhere. And finally, there is the risk of experimentation itself. On balance,

168
00:16:19,360 --> 00:16:24,640
we believe that research to better understand the mechanisms which might underlie consciousness in AI

169
00:16:24,640 --> 00:16:29,840
is beneficial. However, of course, research on this topic runs the risk of building or enabling

170
00:16:29,840 --> 00:16:34,880
others to build a conscious AI system, which should not be done lightly, and that mitigating

171
00:16:34,880 --> 00:16:40,080
this kind of risk should be carefully weighed against the value of better understanding consciousness

172
00:16:40,080 --> 00:16:45,600
in AI. And to tie this back to the start of the video, Google's AI safety experts have added

173
00:16:45,680 --> 00:16:50,160
that some users who grew too dependent on this technology could think it was sentient.

174
00:16:50,160 --> 00:16:56,160
And I do wonder if that's an eventuality, Google's newly merged AI SWOT team is preparing for.

175
00:16:56,160 --> 00:17:00,400
As always, thank you so much for watching to the end and have a wonderful day.

176
00:17:00,400 --> 00:17:06,080
Oh, and just quickly before I end, I now have a discord, AI Explained Community. More info in the

177
00:17:06,080 --> 00:17:06,720
description.

