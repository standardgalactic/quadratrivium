{"text": " Do you remember this paper, less than two weeks old? It made waves by concluding that open source models can mimic the style, but not the factuality of chat GPT. Overall, we can conclude they say that model imitation is a false promise. Well, 48 hours ago, we have this, a 51 page report on Orca, based on a small 13 billion parameter model. I don't often comment on open source models because they're simply not competitive with open AI's models. But Orca is not just competitive with GPT 3.5. It beats it in quite a few well-established benchmarks and even matches GPT-4 in a couple of tests of reasoning. As always, I've read both papers in full and can also bring in just-released comments from Sam Altman and Ilya Sutskova on competition from open source models. But let's start with Orca, named presumably because Orca's or killer whales are frequent visitors to South American coastlines. And South America is, of course, the land of llamas and vicunas. But all the research was done by Microsoft, which I find interesting and I'll come back to that at the end. But why did they make Orca and why does it perform better than models like llama, alpaca, and vicuna? Well, they say here in the abstract that those other models lack rigorous evaluation, resulting in overestimating the small model's capability as they tend to learn to imitate the style but not the reasoning of LFMs, large foundation models. To address these challenges, we developed Orca, a 13 billion parameter model, that learns to imitate the reasoning process of the larger models. Orca learned by looking at GPT-4's step-by-step thought processes and is guided by teacher assistants from chat GPT, which is GPT 3.5. And to give you a taste of what's to come, Orca surpasses conventional state-of-the-art models, such as vicuna, by more than 100% in complex zero-shot reasoning benchmarks, like the big bench hard, which I'll talk about, and by 42% on AGI eval. It goes on, Orca reaches parity with chat GPT on the big bench hard and shows competitive performance in professional and academic examinations by the SAT, LSAT, GRE, and GMAT. And I know many of you will be interested in this footnote. We are working with our legal team to publicly release a diff of the model weights in accordance with Lama's release policy. So if this is anything like Lama, it's going to be leaked across the internet imminently. I'm going to show you so many tests and benchmarks in a moment, but just to give you a sample, here is Orca outperforming chat GPT in the vicuna evaluation set and matching text DaVinci 3 in the SAT, LSAT, GRE, and GMAT. And as I'll touch on later, this was zero shot without chain of thought or any advanced methods. You can watch pretty much any of my other videos to see how advanced prompt engineering would probably boost those results still further. For those who didn't know, 13 billion parameters is about 7% the size of GPT-3, which is 175 billion parameters, and possibly around one or 2% of GPT-4's size. That gives you an indication of the difference in size between Orca and these models that it's competing with. And if that doesn't make any sense, a smaller size means it can be run on much smaller devices, like a desktop or even possibly a laptop. The authors start off by giving a little slap to the other paper. You know that one that said model imitation is a false promise. And they continue that contrary to this assertion, it is possible to reduce the gap with proprietary LLMs on multiple zero shot benchmarks that require sophisticated reasoning. As we'll see, models like Vakuna claim to have 90% of chat GPT's quality, but when it came to reasoning tasks or more technical tasks, it basically flopped. Here's a chart I'll come back to outlining some of the more technical challenges you can give a language model. We should remember that Vakuna is a fine-tuned version of the Llama model, and it's competitive or even better than Palm II. But give it some of the harder challenges for a language model, and it really struggles, as you can see in this column. Take logical deduction, where it only scored 1.2%. Well, this Orca model was 2,900% better than that, scoring 36% competitive with chat GPT. I'm going to come back to the big benchmark, but look for a second at causal judgment, where Orca, a 13 billion parameter model matches GPT4, which is about 100 times the size. But back to how they actually did it. Models like Alpaca and Vakuna were given lots of query and responses from chat GPT or GPT4. But what they did is they leveraged system instructions, asking models like GPT4 and chat GPT to think step by step. This gave Orca access to detailed responses from the model that explained the reasoning process of the teacher as it generates the response. It allowed these parent models of GPT3.5 and GPT4 to be much better tutors for this young Orca. Also, they let the teachers of chat GPT, which is 3.5, and GPT4 give far more examples to their student. 5 million and 1 million examples, respectively. That compares to the other models you may have heard of, like Alpaca, Wizard, Vakuna, etc., which had tens of thousands or the low hundreds of thousands of examples. But again, the key difference is the explanations, the step by step thinking that the smaller Orca could then imitate. They give a quick demo here of how the other open source models learn from their GPT parents, with a simplistic question and answer format. In contrast, the authors leveraged system messages to get chat GPT and GPT4 to think step by step, leading to much richer explanations, as you can see in this diagram. It wasn't just let's think step by step, by the way, also things like explain like I'm 5. They also wanted the task to be as complex and diverse as possible, so they used the Flan collection. This was released by Google in February, and focused on balancing the kind of prompts and tasks that you fine tune the language models on. You can see here the 16 system messages that they give to chat GPT and GPT4, and you can see here the kind of difference that that makes. Imagine a language model trying to learn from this human. The human is asked, pick which sentence is not logical. Sentence A, people in the desert often look forward to flood, or sentence B, people in the desert often look forward to rain. The human responds, there is no reason to look forward to a flood, because floods cause damage, the answer is sentence A. Now yes, a language model can learn from that, but by leveraging those system assistant messages, look at the kind of response that GPT4 gives. Now Orca can learn a lot more from that explanation, and that's one of the main reasons it's better than all the other open source models. Because remember, Vikuna is the best of the open source models. In this leaderboard, it has an elo of 1054, better even than Palm II Bison. All the models higher than it are proprietary. But there is another reason why Orca performs so much better. You might have wondered, why didn't they just use only GPT4? Well yes, there were cost and time considerations, but there was another factor that they found. They were able to use chat GPT or GPT3.5 as an intermediate teacher. That teacher, chat GPT, was able to reduce the gap in capabilities. So Orca got smarter and better able to learn. A bit like progressive learning, where you first learn from easier examples, then followed by harder ones. After that, they gave it outputs from GPT4. Notice by the way, what happens if you skip the chat GPT teaching assistant and only train on those one million examples from GPT4. What happens is a bit like a student struggling in a class that's too advanced for them. Orca actually performs worse in those circumstances, averaging 37%, but with that intermediate teacher beforehand, it gets 41.7%. Speaking of time, it only took about 200 hours to train Orca on 20 A100 GPUs. They did take a few weeks to collect the data from chat GPT and GPT4, but presumably if they're planning to open source this, which they say they are, then that step could be skipped by the wider community. Let's now look at some more of the results. First, for open-ended generation, not multiple choice. Orca is 95% of chat GPT quality and 85% of GPT4's quality as assessed by GPT4, but they wanted to quickly move on to some more definitive tasks because there is a problem of using GPT4 as an assessor. For example, they observed that there is a positive bias in GPT4 evaluation toward the response of the first model in the comparison set. This reminded me of the unfaithful reasoning paper that I talked about in one of my recent videos. You can't always trust GPT4 to give its true reasoning, but here it is in more objective multiple choice questions. And notice how much harder many of these tests are for even these advanced language models. I am fortunate and proud to have attained a perfect score in some of the tests in this chart, like the GRE and GMAT. They were part of the Aquarat tests that they gave the models, so I can say that they really are quite challenging, hence why GPT4 only gets a 40%. You can see that throughout, Orca outperforms Vecuna by quite a margin and is very competitive with Textavinci 3. Of course, overall, it does lag behind GPT4, but this is all zero shot. A bit later on, I'll come back to the range of methods that we could use to further improve on Orca. The percentages, by the way, are the improvements on Vecuna, again the second best open source model. So far, we've looked at human centric benchmarks like the GMAT and GRE. These are grouped with the lovely name AGI EVAL, and as we've seen, even the top models lag behind the top human performers. But what about a benchmark specifically for language models? It's called Big Bench Hard. The original Big Bench had 207 tasks, but language models got so good that they had to narrow down the benchmark to just the 23 challenging tasks where human raters still did better than language models. Now, it turns out when you add Chain of Thought prompting to the models, they do even better and there are even fewer tasks that humans are better at. But anyway, all you have to remember is that these are 23 of the hardest tasks for language models. And I'll just let you compare the results for yourself. But the trend is really quite clear. Orca massively outperforming the previous best open source model, Vecuna, beating even chat GPT on average, but still, of course, lagging behind GPT4, except for a few tasks. Look at Web of Lies, where Orca outperforms GPT4. That would be a question like this. Alexis says Shonda tells the truth. Jim Lies? Antoine says Jim tells the truth. Shonda says Antoine Lies. Does Alexis tell the truth? Or what about temporal sequences, where Orca absolutely crushes Vecuna and doubles chat GPT's performance? That would be a situation like this. Now, I'm not going to read it all out, but essentially you have to figure out when the timings match up. Basically keeping track of time and Orca does really well and chat GPT flops getting it wrong. Interestingly, they also tested all four models on that common sense reasoning question that I demonstrated for smart GPT, about hanging the clothes to dry. As you might remember, you can use prompt engineering to nudge the models to almost always get it right, which is partly why I view these results more as a baseline rather than a cap. And the authors admit this too. Orca has been trained on data that simulates zero shot setting with standard prompts. The model's performance in other contexts, such as multi-turn conversations, like the DERA paper I talked about on the channel, in context learning and few shot learning, or advanced prompting techniques, that smart GPT or Tree of Thoughts, for example, and they say like chain of thought prompting, remains untested. These results are a baseline, not a cap. They mention other ways that Orca could be improved, for example, through tool augmentation. And that's not just calculators, calendars, Bing, or auto GPT. I was going to do a separate video on this paper, but I'll just mention it here. This paper from last week demonstrated that larger models can create tools that smaller models can then use more efficiently. Once the best language model, say GPT-4, has created a generic Python function, which is the tool, and then written some unit tests, it can then wrap and hand over those tools to smaller models like GPT-3.5, or in this case, Orca, and check out the toolmaking row to see the improvement for chat GPT, or in our case, Orca, when they're given these tools created by GPT-4 or better language models. Their performance across a range of tasks goes dramatically up, and we haven't even talked about using a process-based reward model, like in the Let's Verify step-by-step paper. That, of course, could further improve Orca's performance. Of course, when this model becomes publicly available, I will test all of this out, but it hasn't been open-sourced yet, and they do say this model is solely designed for research settings. That does seem a little bit naive to me. I mean, that's what Metta said when they released Lama, but then everyone and their grandma just use the language model for whatever. I do wonder what it means when they say we are working with our legal team. And it is particularly interesting to me that this was all done by Microsoft. I'm gonna go into a little bit of speculation here about why I think they conducted this research. You might remember that leaked memo from Google. We have no motes, and they even mentioned Vakuna, and talked about how it circumvented restrictions on the OpenAI API by using shared GPT. And my theory is that the Microsoft researchers were testing this point from the memo. The point was that training giant models from scratch not only throws away the pre-training, but also any iterative open-source improvements that have been made on top. It doesn't take long for those improvements to dominate, making the full retrain extremely costly. Maybe Microsoft is hesitating about future investments in GPT-5 or GPT-6. And they really wanna test out if it's easy to imitate those large models on the cheap. If it is, then why would Microsoft invest billions in a new giant model? That's my own theory as to why Microsoft is working on this, but let me know in the comments what your theory is. In the conclusion, the authors state that Orca suggests that learning from step-by-step explanations could significantly improve the quality of models regardless of their size, and that they hope these insights will inform the design of more robust evaluation methods, compared to those used for a vacuna, for example, and the advancement of alignment and post-training techniques, and the more effective use of powerful models like GPT-4 as teachers. And maybe they should have said, and also with chat GPT as an intermediate teacher. I'm gonna end with the thoughts of the leaders of OpenAI, Ilya Sutskova, and Sam Oltman on open source models. And I think there is a bit of a contrast between the two answers. Ilya Sutskova thinks that the gap is growing ever wider. To the open source versus non-open source models question, you don't wanna think about it in binary black and white terms where like, there is a secret source that will never be rediscovered. What I will say, or whether GPT-4 will ever be reproduced by open source models, perhaps one day it will be. But when it will be, there will be a much more powerful model in the companies. So there will always be a gap between the open source models and the private models. And this gap may even be increasing this time. The amount of effort and engineering and research that it takes to produce one such neural net keeps increasing. And so even if there are open source models, they will never be, they will be less and less produced by small groups of dedicated researchers and engineers. And it will only be the providence of a company, a big company. While Sam Oltman seems to say that even if open source models do catch up, OpenAI will always have a different kind of moat. What are your thoughts about the, we have no moat document that was released lately? The leak document. The thing that is special about OpenAI, and I think the thing that is so misunderstood by that document, aside from the fact that we have a gigantic number of users and people that have formed some sort of relationship with us and our products, is what OpenAI is special about is figuring out what comes next. It is the ability, it is easy to copy something once you know it can be done. And in that sense, sure. It is very hard to go figure out what to do next. And the ideas, the big ideas, the medium size ideas, the small ideas, and the careful execution on them that it takes to get from here to superintelligence, that's what our moat is. Anyway, this video could have been at least three times longer. There was so much I had to edit out for brevity. If you're interested in me talking more about open source models, do let me know in the comments. I've got much more to say. As always, thank you so much for watching to the end and have a wonderful day.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.84, "text": " Do you remember this paper, less than two weeks old?", "tokens": [50364, 1144, 291, 1604, 341, 3035, 11, 1570, 813, 732, 3259, 1331, 30, 50556], "temperature": 0.0, "avg_logprob": -0.15064701044334555, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.0011691133258864284}, {"id": 1, "seek": 0, "start": 3.84, "end": 7.2, "text": " It made waves by concluding that open source models", "tokens": [50556, 467, 1027, 9417, 538, 9312, 278, 300, 1269, 4009, 5245, 50724], "temperature": 0.0, "avg_logprob": -0.15064701044334555, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.0011691133258864284}, {"id": 2, "seek": 0, "start": 7.2, "end": 11.92, "text": " can mimic the style, but not the factuality of chat GPT.", "tokens": [50724, 393, 31075, 264, 3758, 11, 457, 406, 264, 48029, 507, 295, 5081, 26039, 51, 13, 50960], "temperature": 0.0, "avg_logprob": -0.15064701044334555, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.0011691133258864284}, {"id": 3, "seek": 0, "start": 11.92, "end": 14.24, "text": " Overall, we can conclude they say", "tokens": [50960, 18420, 11, 321, 393, 16886, 436, 584, 51076], "temperature": 0.0, "avg_logprob": -0.15064701044334555, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.0011691133258864284}, {"id": 4, "seek": 0, "start": 14.24, "end": 17.52, "text": " that model imitation is a false promise.", "tokens": [51076, 300, 2316, 47624, 307, 257, 7908, 6228, 13, 51240], "temperature": 0.0, "avg_logprob": -0.15064701044334555, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.0011691133258864284}, {"id": 5, "seek": 0, "start": 17.52, "end": 23.84, "text": " Well, 48 hours ago, we have this, a 51 page report on Orca,", "tokens": [51240, 1042, 11, 11174, 2496, 2057, 11, 321, 362, 341, 11, 257, 18485, 3028, 2275, 322, 1610, 496, 11, 51556], "temperature": 0.0, "avg_logprob": -0.15064701044334555, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.0011691133258864284}, {"id": 6, "seek": 0, "start": 23.84, "end": 27.2, "text": " based on a small 13 billion parameter model.", "tokens": [51556, 2361, 322, 257, 1359, 3705, 5218, 13075, 2316, 13, 51724], "temperature": 0.0, "avg_logprob": -0.15064701044334555, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.0011691133258864284}, {"id": 7, "seek": 0, "start": 27.2, "end": 29.52, "text": " I don't often comment on open source models", "tokens": [51724, 286, 500, 380, 2049, 2871, 322, 1269, 4009, 5245, 51840], "temperature": 0.0, "avg_logprob": -0.15064701044334555, "compression_ratio": 1.5338645418326693, "no_speech_prob": 0.0011691133258864284}, {"id": 8, "seek": 2952, "start": 29.52, "end": 32.8, "text": " because they're simply not competitive with open AI's models.", "tokens": [50364, 570, 436, 434, 2935, 406, 10043, 365, 1269, 7318, 311, 5245, 13, 50528], "temperature": 0.0, "avg_logprob": -0.13564551822722903, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.0015723285032436252}, {"id": 9, "seek": 2952, "start": 32.8, "end": 36.64, "text": " But Orca is not just competitive with GPT 3.5.", "tokens": [50528, 583, 1610, 496, 307, 406, 445, 10043, 365, 26039, 51, 805, 13, 20, 13, 50720], "temperature": 0.0, "avg_logprob": -0.13564551822722903, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.0015723285032436252}, {"id": 10, "seek": 2952, "start": 36.64, "end": 40.32, "text": " It beats it in quite a few well-established benchmarks", "tokens": [50720, 467, 16447, 309, 294, 1596, 257, 1326, 731, 12, 33542, 4173, 43751, 50904], "temperature": 0.0, "avg_logprob": -0.13564551822722903, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.0015723285032436252}, {"id": 11, "seek": 2952, "start": 40.32, "end": 44.32, "text": " and even matches GPT-4 in a couple of tests of reasoning.", "tokens": [50904, 293, 754, 10676, 26039, 51, 12, 19, 294, 257, 1916, 295, 6921, 295, 21577, 13, 51104], "temperature": 0.0, "avg_logprob": -0.13564551822722903, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.0015723285032436252}, {"id": 12, "seek": 2952, "start": 44.32, "end": 46.4, "text": " As always, I've read both papers in full", "tokens": [51104, 1018, 1009, 11, 286, 600, 1401, 1293, 10577, 294, 1577, 51208], "temperature": 0.0, "avg_logprob": -0.13564551822722903, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.0015723285032436252}, {"id": 13, "seek": 2952, "start": 46.4, "end": 48.8, "text": " and can also bring in just-released comments", "tokens": [51208, 293, 393, 611, 1565, 294, 445, 12, 265, 41087, 3053, 51328], "temperature": 0.0, "avg_logprob": -0.13564551822722903, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.0015723285032436252}, {"id": 14, "seek": 2952, "start": 48.8, "end": 53.92, "text": " from Sam Altman and Ilya Sutskova on competition from open source models.", "tokens": [51328, 490, 4832, 15992, 1601, 293, 286, 45106, 318, 3648, 4093, 2757, 322, 6211, 490, 1269, 4009, 5245, 13, 51584], "temperature": 0.0, "avg_logprob": -0.13564551822722903, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.0015723285032436252}, {"id": 15, "seek": 2952, "start": 53.92, "end": 59.04, "text": " But let's start with Orca, named presumably because Orca's or killer whales", "tokens": [51584, 583, 718, 311, 722, 365, 1610, 496, 11, 4926, 26742, 570, 1610, 496, 311, 420, 13364, 32403, 51840], "temperature": 0.0, "avg_logprob": -0.13564551822722903, "compression_ratio": 1.5868055555555556, "no_speech_prob": 0.0015723285032436252}, {"id": 16, "seek": 5904, "start": 59.04, "end": 62.0, "text": " are frequent visitors to South American coastlines.", "tokens": [50364, 366, 18004, 14315, 281, 4242, 2665, 8684, 11045, 13, 50512], "temperature": 0.0, "avg_logprob": -0.10688832600911459, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0018098977161571383}, {"id": 17, "seek": 5904, "start": 62.0, "end": 65.68, "text": " And South America is, of course, the land of llamas and vicunas.", "tokens": [50512, 400, 4242, 3374, 307, 11, 295, 1164, 11, 264, 2117, 295, 16848, 296, 293, 26031, 409, 296, 13, 50696], "temperature": 0.0, "avg_logprob": -0.10688832600911459, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0018098977161571383}, {"id": 18, "seek": 5904, "start": 65.68, "end": 68.72, "text": " But all the research was done by Microsoft,", "tokens": [50696, 583, 439, 264, 2132, 390, 1096, 538, 8116, 11, 50848], "temperature": 0.0, "avg_logprob": -0.10688832600911459, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0018098977161571383}, {"id": 19, "seek": 5904, "start": 68.72, "end": 72.0, "text": " which I find interesting and I'll come back to that at the end.", "tokens": [50848, 597, 286, 915, 1880, 293, 286, 603, 808, 646, 281, 300, 412, 264, 917, 13, 51012], "temperature": 0.0, "avg_logprob": -0.10688832600911459, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0018098977161571383}, {"id": 20, "seek": 5904, "start": 72.0, "end": 74.88, "text": " But why did they make Orca and why does it perform better", "tokens": [51012, 583, 983, 630, 436, 652, 1610, 496, 293, 983, 775, 309, 2042, 1101, 51156], "temperature": 0.0, "avg_logprob": -0.10688832600911459, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0018098977161571383}, {"id": 21, "seek": 5904, "start": 74.88, "end": 77.44, "text": " than models like llama, alpaca, and vicuna?", "tokens": [51156, 813, 5245, 411, 23272, 11, 419, 79, 6628, 11, 293, 26031, 5051, 30, 51284], "temperature": 0.0, "avg_logprob": -0.10688832600911459, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0018098977161571383}, {"id": 22, "seek": 5904, "start": 77.44, "end": 79.2, "text": " Well, they say here in the abstract", "tokens": [51284, 1042, 11, 436, 584, 510, 294, 264, 12649, 51372], "temperature": 0.0, "avg_logprob": -0.10688832600911459, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0018098977161571383}, {"id": 23, "seek": 5904, "start": 79.2, "end": 82.4, "text": " that those other models lack rigorous evaluation,", "tokens": [51372, 300, 729, 661, 5245, 5011, 29882, 13344, 11, 51532], "temperature": 0.0, "avg_logprob": -0.10688832600911459, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0018098977161571383}, {"id": 24, "seek": 5904, "start": 82.4, "end": 85.92, "text": " resulting in overestimating the small model's capability", "tokens": [51532, 16505, 294, 670, 377, 332, 990, 264, 1359, 2316, 311, 13759, 51708], "temperature": 0.0, "avg_logprob": -0.10688832600911459, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0018098977161571383}, {"id": 25, "seek": 5904, "start": 85.92, "end": 88.64, "text": " as they tend to learn to imitate the style", "tokens": [51708, 382, 436, 3928, 281, 1466, 281, 35556, 264, 3758, 51844], "temperature": 0.0, "avg_logprob": -0.10688832600911459, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0018098977161571383}, {"id": 26, "seek": 8864, "start": 88.72, "end": 92.56, "text": " but not the reasoning of LFMs, large foundation models.", "tokens": [50368, 457, 406, 264, 21577, 295, 441, 37, 26386, 11, 2416, 7030, 5245, 13, 50560], "temperature": 0.0, "avg_logprob": -0.09380643875872502, "compression_ratio": 1.5904059040590406, "no_speech_prob": 0.0029799400363117456}, {"id": 27, "seek": 8864, "start": 92.56, "end": 94.88, "text": " To address these challenges, we developed Orca,", "tokens": [50560, 1407, 2985, 613, 4759, 11, 321, 4743, 1610, 496, 11, 50676], "temperature": 0.0, "avg_logprob": -0.09380643875872502, "compression_ratio": 1.5904059040590406, "no_speech_prob": 0.0029799400363117456}, {"id": 28, "seek": 8864, "start": 94.88, "end": 98.0, "text": " a 13 billion parameter model, that learns to imitate", "tokens": [50676, 257, 3705, 5218, 13075, 2316, 11, 300, 27152, 281, 35556, 50832], "temperature": 0.0, "avg_logprob": -0.09380643875872502, "compression_ratio": 1.5904059040590406, "no_speech_prob": 0.0029799400363117456}, {"id": 29, "seek": 8864, "start": 98.0, "end": 100.88, "text": " the reasoning process of the larger models.", "tokens": [50832, 264, 21577, 1399, 295, 264, 4833, 5245, 13, 50976], "temperature": 0.0, "avg_logprob": -0.09380643875872502, "compression_ratio": 1.5904059040590406, "no_speech_prob": 0.0029799400363117456}, {"id": 30, "seek": 8864, "start": 100.88, "end": 105.36, "text": " Orca learned by looking at GPT-4's step-by-step thought processes", "tokens": [50976, 1610, 496, 3264, 538, 1237, 412, 26039, 51, 12, 19, 311, 1823, 12, 2322, 12, 16792, 1194, 7555, 51200], "temperature": 0.0, "avg_logprob": -0.09380643875872502, "compression_ratio": 1.5904059040590406, "no_speech_prob": 0.0029799400363117456}, {"id": 31, "seek": 8864, "start": 105.36, "end": 110.64, "text": " and is guided by teacher assistants from chat GPT, which is GPT 3.5.", "tokens": [51200, 293, 307, 19663, 538, 5027, 34949, 490, 5081, 26039, 51, 11, 597, 307, 26039, 51, 805, 13, 20, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09380643875872502, "compression_ratio": 1.5904059040590406, "no_speech_prob": 0.0029799400363117456}, {"id": 32, "seek": 8864, "start": 110.64, "end": 112.56, "text": " And to give you a taste of what's to come,", "tokens": [51464, 400, 281, 976, 291, 257, 3939, 295, 437, 311, 281, 808, 11, 51560], "temperature": 0.0, "avg_logprob": -0.09380643875872502, "compression_ratio": 1.5904059040590406, "no_speech_prob": 0.0029799400363117456}, {"id": 33, "seek": 8864, "start": 112.56, "end": 115.76, "text": " Orca surpasses conventional state-of-the-art models,", "tokens": [51560, 1610, 496, 27650, 279, 16011, 1785, 12, 2670, 12, 3322, 12, 446, 5245, 11, 51720], "temperature": 0.0, "avg_logprob": -0.09380643875872502, "compression_ratio": 1.5904059040590406, "no_speech_prob": 0.0029799400363117456}, {"id": 34, "seek": 11576, "start": 115.76, "end": 118.96000000000001, "text": " such as vicuna, by more than 100%", "tokens": [50364, 1270, 382, 26031, 5051, 11, 538, 544, 813, 2319, 4, 50524], "temperature": 0.0, "avg_logprob": -0.096538970344945, "compression_ratio": 1.4694656488549618, "no_speech_prob": 0.030191175639629364}, {"id": 35, "seek": 11576, "start": 118.96000000000001, "end": 122.4, "text": " in complex zero-shot reasoning benchmarks,", "tokens": [50524, 294, 3997, 4018, 12, 18402, 21577, 43751, 11, 50696], "temperature": 0.0, "avg_logprob": -0.096538970344945, "compression_ratio": 1.4694656488549618, "no_speech_prob": 0.030191175639629364}, {"id": 36, "seek": 11576, "start": 122.4, "end": 124.96000000000001, "text": " like the big bench hard, which I'll talk about,", "tokens": [50696, 411, 264, 955, 10638, 1152, 11, 597, 286, 603, 751, 466, 11, 50824], "temperature": 0.0, "avg_logprob": -0.096538970344945, "compression_ratio": 1.4694656488549618, "no_speech_prob": 0.030191175639629364}, {"id": 37, "seek": 11576, "start": 124.96000000000001, "end": 128.4, "text": " and by 42% on AGI eval.", "tokens": [50824, 293, 538, 14034, 4, 322, 316, 26252, 1073, 304, 13, 50996], "temperature": 0.0, "avg_logprob": -0.096538970344945, "compression_ratio": 1.4694656488549618, "no_speech_prob": 0.030191175639629364}, {"id": 38, "seek": 11576, "start": 128.4, "end": 132.48000000000002, "text": " It goes on, Orca reaches parity with chat GPT", "tokens": [50996, 467, 1709, 322, 11, 1610, 496, 14235, 44747, 365, 5081, 26039, 51, 51200], "temperature": 0.0, "avg_logprob": -0.096538970344945, "compression_ratio": 1.4694656488549618, "no_speech_prob": 0.030191175639629364}, {"id": 39, "seek": 11576, "start": 132.48000000000002, "end": 136.4, "text": " on the big bench hard and shows competitive performance", "tokens": [51200, 322, 264, 955, 10638, 1152, 293, 3110, 10043, 3389, 51396], "temperature": 0.0, "avg_logprob": -0.096538970344945, "compression_ratio": 1.4694656488549618, "no_speech_prob": 0.030191175639629364}, {"id": 40, "seek": 11576, "start": 136.4, "end": 138.8, "text": " in professional and academic examinations", "tokens": [51396, 294, 4843, 293, 7778, 1139, 10325, 51516], "temperature": 0.0, "avg_logprob": -0.096538970344945, "compression_ratio": 1.4694656488549618, "no_speech_prob": 0.030191175639629364}, {"id": 41, "seek": 11576, "start": 138.8, "end": 141.52, "text": " by the SAT, LSAT, GRE, and GMAT.", "tokens": [51516, 538, 264, 31536, 11, 36657, 2218, 11, 20830, 11, 293, 16609, 2218, 13, 51652], "temperature": 0.0, "avg_logprob": -0.096538970344945, "compression_ratio": 1.4694656488549618, "no_speech_prob": 0.030191175639629364}, {"id": 42, "seek": 11576, "start": 141.52, "end": 144.32, "text": " And I know many of you will be interested in this footnote.", "tokens": [51652, 400, 286, 458, 867, 295, 291, 486, 312, 3102, 294, 341, 2671, 22178, 13, 51792], "temperature": 0.0, "avg_logprob": -0.096538970344945, "compression_ratio": 1.4694656488549618, "no_speech_prob": 0.030191175639629364}, {"id": 43, "seek": 14432, "start": 144.32, "end": 148.16, "text": " We are working with our legal team to publicly release", "tokens": [50364, 492, 366, 1364, 365, 527, 5089, 1469, 281, 14843, 4374, 50556], "temperature": 0.0, "avg_logprob": -0.09892644500732421, "compression_ratio": 1.5316901408450705, "no_speech_prob": 0.003592934226617217}, {"id": 44, "seek": 14432, "start": 148.16, "end": 150.48, "text": " a diff of the model weights in accordance", "tokens": [50556, 257, 7593, 295, 264, 2316, 17443, 294, 31110, 50672], "temperature": 0.0, "avg_logprob": -0.09892644500732421, "compression_ratio": 1.5316901408450705, "no_speech_prob": 0.003592934226617217}, {"id": 45, "seek": 14432, "start": 150.48, "end": 152.23999999999998, "text": " with Lama's release policy.", "tokens": [50672, 365, 441, 2404, 311, 4374, 3897, 13, 50760], "temperature": 0.0, "avg_logprob": -0.09892644500732421, "compression_ratio": 1.5316901408450705, "no_speech_prob": 0.003592934226617217}, {"id": 46, "seek": 14432, "start": 152.23999999999998, "end": 154.0, "text": " So if this is anything like Lama,", "tokens": [50760, 407, 498, 341, 307, 1340, 411, 441, 2404, 11, 50848], "temperature": 0.0, "avg_logprob": -0.09892644500732421, "compression_ratio": 1.5316901408450705, "no_speech_prob": 0.003592934226617217}, {"id": 47, "seek": 14432, "start": 154.0, "end": 156.32, "text": " it's going to be leaked across the internet imminently.", "tokens": [50848, 309, 311, 516, 281, 312, 31779, 2108, 264, 4705, 40728, 2276, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09892644500732421, "compression_ratio": 1.5316901408450705, "no_speech_prob": 0.003592934226617217}, {"id": 48, "seek": 14432, "start": 156.32, "end": 159.76, "text": " I'm going to show you so many tests and benchmarks in a moment,", "tokens": [50964, 286, 478, 516, 281, 855, 291, 370, 867, 6921, 293, 43751, 294, 257, 1623, 11, 51136], "temperature": 0.0, "avg_logprob": -0.09892644500732421, "compression_ratio": 1.5316901408450705, "no_speech_prob": 0.003592934226617217}, {"id": 49, "seek": 14432, "start": 159.76, "end": 161.44, "text": " but just to give you a sample,", "tokens": [51136, 457, 445, 281, 976, 291, 257, 6889, 11, 51220], "temperature": 0.0, "avg_logprob": -0.09892644500732421, "compression_ratio": 1.5316901408450705, "no_speech_prob": 0.003592934226617217}, {"id": 50, "seek": 14432, "start": 161.44, "end": 166.56, "text": " here is Orca outperforming chat GPT in the vicuna evaluation set", "tokens": [51220, 510, 307, 1610, 496, 484, 26765, 278, 5081, 26039, 51, 294, 264, 26031, 5051, 13344, 992, 51476], "temperature": 0.0, "avg_logprob": -0.09892644500732421, "compression_ratio": 1.5316901408450705, "no_speech_prob": 0.003592934226617217}, {"id": 51, "seek": 14432, "start": 166.56, "end": 171.6, "text": " and matching text DaVinci 3 in the SAT, LSAT, GRE, and GMAT.", "tokens": [51476, 293, 14324, 2487, 3933, 53, 21961, 805, 294, 264, 31536, 11, 36657, 2218, 11, 20830, 11, 293, 16609, 2218, 13, 51728], "temperature": 0.0, "avg_logprob": -0.09892644500732421, "compression_ratio": 1.5316901408450705, "no_speech_prob": 0.003592934226617217}, {"id": 52, "seek": 17160, "start": 171.6, "end": 172.96, "text": " And as I'll touch on later,", "tokens": [50364, 400, 382, 286, 603, 2557, 322, 1780, 11, 50432], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 53, "seek": 17160, "start": 172.96, "end": 175.84, "text": " this was zero shot without chain of thought", "tokens": [50432, 341, 390, 4018, 3347, 1553, 5021, 295, 1194, 50576], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 54, "seek": 17160, "start": 175.84, "end": 177.44, "text": " or any advanced methods.", "tokens": [50576, 420, 604, 7339, 7150, 13, 50656], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 55, "seek": 17160, "start": 177.44, "end": 179.51999999999998, "text": " You can watch pretty much any of my other videos", "tokens": [50656, 509, 393, 1159, 1238, 709, 604, 295, 452, 661, 2145, 50760], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 56, "seek": 17160, "start": 179.51999999999998, "end": 181.76, "text": " to see how advanced prompt engineering", "tokens": [50760, 281, 536, 577, 7339, 12391, 7043, 50872], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 57, "seek": 17160, "start": 181.76, "end": 184.4, "text": " would probably boost those results still further.", "tokens": [50872, 576, 1391, 9194, 729, 3542, 920, 3052, 13, 51004], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 58, "seek": 17160, "start": 184.4, "end": 185.51999999999998, "text": " For those who didn't know,", "tokens": [51004, 1171, 729, 567, 994, 380, 458, 11, 51060], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 59, "seek": 17160, "start": 185.51999999999998, "end": 190.48, "text": " 13 billion parameters is about 7% the size of GPT-3,", "tokens": [51060, 3705, 5218, 9834, 307, 466, 1614, 4, 264, 2744, 295, 26039, 51, 12, 18, 11, 51308], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 60, "seek": 17160, "start": 190.48, "end": 192.88, "text": " which is 175 billion parameters,", "tokens": [51308, 597, 307, 41165, 5218, 9834, 11, 51428], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 61, "seek": 17160, "start": 192.88, "end": 197.68, "text": " and possibly around one or 2% of GPT-4's size.", "tokens": [51428, 293, 6264, 926, 472, 420, 568, 4, 295, 26039, 51, 12, 19, 311, 2744, 13, 51668], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 62, "seek": 17160, "start": 197.68, "end": 200.0, "text": " That gives you an indication of the difference", "tokens": [51668, 663, 2709, 291, 364, 18877, 295, 264, 2649, 51784], "temperature": 0.0, "avg_logprob": -0.07732843557993571, "compression_ratio": 1.5419580419580419, "no_speech_prob": 0.020327413454651833}, {"id": 63, "seek": 20000, "start": 200.08, "end": 203.36, "text": " in size between Orca and these models that it's competing with.", "tokens": [50368, 294, 2744, 1296, 1610, 496, 293, 613, 5245, 300, 309, 311, 15439, 365, 13, 50532], "temperature": 0.0, "avg_logprob": -0.07263031313496252, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.008844790980219841}, {"id": 64, "seek": 20000, "start": 203.36, "end": 204.88, "text": " And if that doesn't make any sense,", "tokens": [50532, 400, 498, 300, 1177, 380, 652, 604, 2020, 11, 50608], "temperature": 0.0, "avg_logprob": -0.07263031313496252, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.008844790980219841}, {"id": 65, "seek": 20000, "start": 204.88, "end": 208.8, "text": " a smaller size means it can be run on much smaller devices,", "tokens": [50608, 257, 4356, 2744, 1355, 309, 393, 312, 1190, 322, 709, 4356, 5759, 11, 50804], "temperature": 0.0, "avg_logprob": -0.07263031313496252, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.008844790980219841}, {"id": 66, "seek": 20000, "start": 208.8, "end": 211.68, "text": " like a desktop or even possibly a laptop.", "tokens": [50804, 411, 257, 14502, 420, 754, 6264, 257, 10732, 13, 50948], "temperature": 0.0, "avg_logprob": -0.07263031313496252, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.008844790980219841}, {"id": 67, "seek": 20000, "start": 211.68, "end": 214.72, "text": " The authors start off by giving a little slap to the other paper.", "tokens": [50948, 440, 16552, 722, 766, 538, 2902, 257, 707, 21075, 281, 264, 661, 3035, 13, 51100], "temperature": 0.0, "avg_logprob": -0.07263031313496252, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.008844790980219841}, {"id": 68, "seek": 20000, "start": 214.72, "end": 217.92000000000002, "text": " You know that one that said model imitation is a false promise.", "tokens": [51100, 509, 458, 300, 472, 300, 848, 2316, 47624, 307, 257, 7908, 6228, 13, 51260], "temperature": 0.0, "avg_logprob": -0.07263031313496252, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.008844790980219841}, {"id": 69, "seek": 20000, "start": 217.92000000000002, "end": 220.48, "text": " And they continue that contrary to this assertion,", "tokens": [51260, 400, 436, 2354, 300, 19506, 281, 341, 19810, 313, 11, 51388], "temperature": 0.0, "avg_logprob": -0.07263031313496252, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.008844790980219841}, {"id": 70, "seek": 20000, "start": 220.48, "end": 225.04, "text": " it is possible to reduce the gap with proprietary LLMs", "tokens": [51388, 309, 307, 1944, 281, 5407, 264, 7417, 365, 38992, 441, 43, 26386, 51616], "temperature": 0.0, "avg_logprob": -0.07263031313496252, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.008844790980219841}, {"id": 71, "seek": 20000, "start": 225.04, "end": 229.36, "text": " on multiple zero shot benchmarks that require sophisticated reasoning.", "tokens": [51616, 322, 3866, 4018, 3347, 43751, 300, 3651, 16950, 21577, 13, 51832], "temperature": 0.0, "avg_logprob": -0.07263031313496252, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.008844790980219841}, {"id": 72, "seek": 22936, "start": 229.36, "end": 234.16000000000003, "text": " As we'll see, models like Vakuna claim to have 90% of chat GPT's quality,", "tokens": [50364, 1018, 321, 603, 536, 11, 5245, 411, 691, 514, 5051, 3932, 281, 362, 4289, 4, 295, 5081, 26039, 51, 311, 3125, 11, 50604], "temperature": 0.0, "avg_logprob": -0.10301216555313325, "compression_ratio": 1.693069306930693, "no_speech_prob": 0.002251414814963937}, {"id": 73, "seek": 22936, "start": 234.16000000000003, "end": 237.60000000000002, "text": " but when it came to reasoning tasks or more technical tasks,", "tokens": [50604, 457, 562, 309, 1361, 281, 21577, 9608, 420, 544, 6191, 9608, 11, 50776], "temperature": 0.0, "avg_logprob": -0.10301216555313325, "compression_ratio": 1.693069306930693, "no_speech_prob": 0.002251414814963937}, {"id": 74, "seek": 22936, "start": 237.60000000000002, "end": 238.72000000000003, "text": " it basically flopped.", "tokens": [50776, 309, 1936, 25343, 3452, 13, 50832], "temperature": 0.0, "avg_logprob": -0.10301216555313325, "compression_ratio": 1.693069306930693, "no_speech_prob": 0.002251414814963937}, {"id": 75, "seek": 22936, "start": 238.72000000000003, "end": 240.8, "text": " Here's a chart I'll come back to outlining", "tokens": [50832, 1692, 311, 257, 6927, 286, 603, 808, 646, 281, 484, 31079, 50936], "temperature": 0.0, "avg_logprob": -0.10301216555313325, "compression_ratio": 1.693069306930693, "no_speech_prob": 0.002251414814963937}, {"id": 76, "seek": 22936, "start": 240.8, "end": 244.24, "text": " some of the more technical challenges you can give a language model.", "tokens": [50936, 512, 295, 264, 544, 6191, 4759, 291, 393, 976, 257, 2856, 2316, 13, 51108], "temperature": 0.0, "avg_logprob": -0.10301216555313325, "compression_ratio": 1.693069306930693, "no_speech_prob": 0.002251414814963937}, {"id": 77, "seek": 22936, "start": 244.24, "end": 247.44000000000003, "text": " We should remember that Vakuna is a fine-tuned version", "tokens": [51108, 492, 820, 1604, 300, 691, 514, 5051, 307, 257, 2489, 12, 83, 43703, 3037, 51268], "temperature": 0.0, "avg_logprob": -0.10301216555313325, "compression_ratio": 1.693069306930693, "no_speech_prob": 0.002251414814963937}, {"id": 78, "seek": 22936, "start": 247.44000000000003, "end": 248.88000000000002, "text": " of the Llama model,", "tokens": [51268, 295, 264, 32717, 2404, 2316, 11, 51340], "temperature": 0.0, "avg_logprob": -0.10301216555313325, "compression_ratio": 1.693069306930693, "no_speech_prob": 0.002251414814963937}, {"id": 79, "seek": 22936, "start": 248.88000000000002, "end": 252.32000000000002, "text": " and it's competitive or even better than Palm II.", "tokens": [51340, 293, 309, 311, 10043, 420, 754, 1101, 813, 32668, 6351, 13, 51512], "temperature": 0.0, "avg_logprob": -0.10301216555313325, "compression_ratio": 1.693069306930693, "no_speech_prob": 0.002251414814963937}, {"id": 80, "seek": 22936, "start": 252.32000000000002, "end": 255.84, "text": " But give it some of the harder challenges for a language model,", "tokens": [51512, 583, 976, 309, 512, 295, 264, 6081, 4759, 337, 257, 2856, 2316, 11, 51688], "temperature": 0.0, "avg_logprob": -0.10301216555313325, "compression_ratio": 1.693069306930693, "no_speech_prob": 0.002251414814963937}, {"id": 81, "seek": 22936, "start": 255.84, "end": 258.72, "text": " and it really struggles, as you can see in this column.", "tokens": [51688, 293, 309, 534, 17592, 11, 382, 291, 393, 536, 294, 341, 7738, 13, 51832], "temperature": 0.0, "avg_logprob": -0.10301216555313325, "compression_ratio": 1.693069306930693, "no_speech_prob": 0.002251414814963937}, {"id": 82, "seek": 25872, "start": 258.72, "end": 262.24, "text": " Take logical deduction, where it only scored 1.2%.", "tokens": [50364, 3664, 14978, 46385, 11, 689, 309, 787, 18139, 502, 13, 17, 6856, 50540], "temperature": 0.0, "avg_logprob": -0.10055911925531202, "compression_ratio": 1.4570446735395188, "no_speech_prob": 0.0009398278198204935}, {"id": 83, "seek": 25872, "start": 262.24, "end": 266.32000000000005, "text": " Well, this Orca model was 2,900% better than that,", "tokens": [50540, 1042, 11, 341, 1610, 496, 2316, 390, 568, 11, 23943, 4, 1101, 813, 300, 11, 50744], "temperature": 0.0, "avg_logprob": -0.10055911925531202, "compression_ratio": 1.4570446735395188, "no_speech_prob": 0.0009398278198204935}, {"id": 84, "seek": 25872, "start": 266.32000000000005, "end": 269.52000000000004, "text": " scoring 36% competitive with chat GPT.", "tokens": [50744, 22358, 8652, 4, 10043, 365, 5081, 26039, 51, 13, 50904], "temperature": 0.0, "avg_logprob": -0.10055911925531202, "compression_ratio": 1.4570446735395188, "no_speech_prob": 0.0009398278198204935}, {"id": 85, "seek": 25872, "start": 269.52000000000004, "end": 271.84000000000003, "text": " I'm going to come back to the big benchmark,", "tokens": [50904, 286, 478, 516, 281, 808, 646, 281, 264, 955, 18927, 11, 51020], "temperature": 0.0, "avg_logprob": -0.10055911925531202, "compression_ratio": 1.4570446735395188, "no_speech_prob": 0.0009398278198204935}, {"id": 86, "seek": 25872, "start": 271.84000000000003, "end": 274.32000000000005, "text": " but look for a second at causal judgment,", "tokens": [51020, 457, 574, 337, 257, 1150, 412, 38755, 12216, 11, 51144], "temperature": 0.0, "avg_logprob": -0.10055911925531202, "compression_ratio": 1.4570446735395188, "no_speech_prob": 0.0009398278198204935}, {"id": 87, "seek": 25872, "start": 274.32000000000005, "end": 279.44000000000005, "text": " where Orca, a 13 billion parameter model matches GPT4,", "tokens": [51144, 689, 1610, 496, 11, 257, 3705, 5218, 13075, 2316, 10676, 26039, 51, 19, 11, 51400], "temperature": 0.0, "avg_logprob": -0.10055911925531202, "compression_ratio": 1.4570446735395188, "no_speech_prob": 0.0009398278198204935}, {"id": 88, "seek": 25872, "start": 279.44000000000005, "end": 282.16, "text": " which is about 100 times the size.", "tokens": [51400, 597, 307, 466, 2319, 1413, 264, 2744, 13, 51536], "temperature": 0.0, "avg_logprob": -0.10055911925531202, "compression_ratio": 1.4570446735395188, "no_speech_prob": 0.0009398278198204935}, {"id": 89, "seek": 25872, "start": 282.16, "end": 284.08000000000004, "text": " But back to how they actually did it.", "tokens": [51536, 583, 646, 281, 577, 436, 767, 630, 309, 13, 51632], "temperature": 0.0, "avg_logprob": -0.10055911925531202, "compression_ratio": 1.4570446735395188, "no_speech_prob": 0.0009398278198204935}, {"id": 90, "seek": 25872, "start": 284.08000000000004, "end": 288.64000000000004, "text": " Models like Alpaca and Vakuna were given lots of query and responses", "tokens": [51632, 6583, 1625, 411, 967, 79, 6628, 293, 691, 514, 5051, 645, 2212, 3195, 295, 14581, 293, 13019, 51860], "temperature": 0.0, "avg_logprob": -0.10055911925531202, "compression_ratio": 1.4570446735395188, "no_speech_prob": 0.0009398278198204935}, {"id": 91, "seek": 28864, "start": 288.64, "end": 290.88, "text": " from chat GPT or GPT4.", "tokens": [50364, 490, 5081, 26039, 51, 420, 26039, 51, 19, 13, 50476], "temperature": 0.0, "avg_logprob": -0.05817940100184027, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008039294625632465}, {"id": 92, "seek": 28864, "start": 290.88, "end": 294.08, "text": " But what they did is they leveraged system instructions,", "tokens": [50476, 583, 437, 436, 630, 307, 436, 12451, 2980, 1185, 9415, 11, 50636], "temperature": 0.0, "avg_logprob": -0.05817940100184027, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008039294625632465}, {"id": 93, "seek": 28864, "start": 294.08, "end": 298.0, "text": " asking models like GPT4 and chat GPT to think step by step.", "tokens": [50636, 3365, 5245, 411, 26039, 51, 19, 293, 5081, 26039, 51, 281, 519, 1823, 538, 1823, 13, 50832], "temperature": 0.0, "avg_logprob": -0.05817940100184027, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008039294625632465}, {"id": 94, "seek": 28864, "start": 298.0, "end": 302.08, "text": " This gave Orca access to detailed responses from the model", "tokens": [50832, 639, 2729, 1610, 496, 2105, 281, 9942, 13019, 490, 264, 2316, 51036], "temperature": 0.0, "avg_logprob": -0.05817940100184027, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008039294625632465}, {"id": 95, "seek": 28864, "start": 302.08, "end": 305.2, "text": " that explained the reasoning process of the teacher", "tokens": [51036, 300, 8825, 264, 21577, 1399, 295, 264, 5027, 51192], "temperature": 0.0, "avg_logprob": -0.05817940100184027, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008039294625632465}, {"id": 96, "seek": 28864, "start": 305.2, "end": 306.8, "text": " as it generates the response.", "tokens": [51192, 382, 309, 23815, 264, 4134, 13, 51272], "temperature": 0.0, "avg_logprob": -0.05817940100184027, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008039294625632465}, {"id": 97, "seek": 28864, "start": 306.8, "end": 310.71999999999997, "text": " It allowed these parent models of GPT3.5 and GPT4", "tokens": [51272, 467, 4350, 613, 2596, 5245, 295, 26039, 51, 18, 13, 20, 293, 26039, 51, 19, 51468], "temperature": 0.0, "avg_logprob": -0.05817940100184027, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008039294625632465}, {"id": 98, "seek": 28864, "start": 310.71999999999997, "end": 314.15999999999997, "text": " to be much better tutors for this young Orca.", "tokens": [51468, 281, 312, 709, 1101, 3672, 830, 337, 341, 2037, 1610, 496, 13, 51640], "temperature": 0.0, "avg_logprob": -0.05817940100184027, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.0008039294625632465}, {"id": 99, "seek": 31416, "start": 314.24, "end": 317.68, "text": " Also, they let the teachers of chat GPT, which is 3.5,", "tokens": [50368, 2743, 11, 436, 718, 264, 6023, 295, 5081, 26039, 51, 11, 597, 307, 805, 13, 20, 11, 50540], "temperature": 0.0, "avg_logprob": -0.08642299851374839, "compression_ratio": 1.6480263157894737, "no_speech_prob": 0.007118755951523781}, {"id": 100, "seek": 31416, "start": 317.68, "end": 321.04, "text": " and GPT4 give far more examples to their student.", "tokens": [50540, 293, 26039, 51, 19, 976, 1400, 544, 5110, 281, 641, 3107, 13, 50708], "temperature": 0.0, "avg_logprob": -0.08642299851374839, "compression_ratio": 1.6480263157894737, "no_speech_prob": 0.007118755951523781}, {"id": 101, "seek": 31416, "start": 321.04, "end": 324.08000000000004, "text": " 5 million and 1 million examples, respectively.", "tokens": [50708, 1025, 2459, 293, 502, 2459, 5110, 11, 25009, 13, 50860], "temperature": 0.0, "avg_logprob": -0.08642299851374839, "compression_ratio": 1.6480263157894737, "no_speech_prob": 0.007118755951523781}, {"id": 102, "seek": 31416, "start": 324.08000000000004, "end": 326.24, "text": " That compares to the other models you may have heard of,", "tokens": [50860, 663, 38334, 281, 264, 661, 5245, 291, 815, 362, 2198, 295, 11, 50968], "temperature": 0.0, "avg_logprob": -0.08642299851374839, "compression_ratio": 1.6480263157894737, "no_speech_prob": 0.007118755951523781}, {"id": 103, "seek": 31416, "start": 326.24, "end": 328.8, "text": " like Alpaca, Wizard, Vakuna, etc.,", "tokens": [50968, 411, 967, 79, 6628, 11, 37449, 11, 691, 514, 5051, 11, 5183, 7933, 51096], "temperature": 0.0, "avg_logprob": -0.08642299851374839, "compression_ratio": 1.6480263157894737, "no_speech_prob": 0.007118755951523781}, {"id": 104, "seek": 31416, "start": 328.8, "end": 333.04, "text": " which had tens of thousands or the low hundreds of thousands of examples.", "tokens": [51096, 597, 632, 10688, 295, 5383, 420, 264, 2295, 6779, 295, 5383, 295, 5110, 13, 51308], "temperature": 0.0, "avg_logprob": -0.08642299851374839, "compression_ratio": 1.6480263157894737, "no_speech_prob": 0.007118755951523781}, {"id": 105, "seek": 31416, "start": 333.04, "end": 336.08000000000004, "text": " But again, the key difference is the explanations,", "tokens": [51308, 583, 797, 11, 264, 2141, 2649, 307, 264, 28708, 11, 51460], "temperature": 0.0, "avg_logprob": -0.08642299851374839, "compression_ratio": 1.6480263157894737, "no_speech_prob": 0.007118755951523781}, {"id": 106, "seek": 31416, "start": 336.08000000000004, "end": 340.0, "text": " the step by step thinking that the smaller Orca could then imitate.", "tokens": [51460, 264, 1823, 538, 1823, 1953, 300, 264, 4356, 1610, 496, 727, 550, 35556, 13, 51656], "temperature": 0.0, "avg_logprob": -0.08642299851374839, "compression_ratio": 1.6480263157894737, "no_speech_prob": 0.007118755951523781}, {"id": 107, "seek": 31416, "start": 340.0, "end": 343.04, "text": " They give a quick demo here of how the other open source models", "tokens": [51656, 814, 976, 257, 1702, 10723, 510, 295, 577, 264, 661, 1269, 4009, 5245, 51808], "temperature": 0.0, "avg_logprob": -0.08642299851374839, "compression_ratio": 1.6480263157894737, "no_speech_prob": 0.007118755951523781}, {"id": 108, "seek": 34304, "start": 343.04, "end": 344.96000000000004, "text": " learn from their GPT parents,", "tokens": [50364, 1466, 490, 641, 26039, 51, 3152, 11, 50460], "temperature": 0.0, "avg_logprob": -0.10583899201465254, "compression_ratio": 1.6171003717472119, "no_speech_prob": 0.008573818951845169}, {"id": 109, "seek": 34304, "start": 344.96000000000004, "end": 348.32, "text": " with a simplistic question and answer format.", "tokens": [50460, 365, 257, 44199, 1168, 293, 1867, 7877, 13, 50628], "temperature": 0.0, "avg_logprob": -0.10583899201465254, "compression_ratio": 1.6171003717472119, "no_speech_prob": 0.008573818951845169}, {"id": 110, "seek": 34304, "start": 348.32, "end": 351.52000000000004, "text": " In contrast, the authors leveraged system messages", "tokens": [50628, 682, 8712, 11, 264, 16552, 12451, 2980, 1185, 7897, 50788], "temperature": 0.0, "avg_logprob": -0.10583899201465254, "compression_ratio": 1.6171003717472119, "no_speech_prob": 0.008573818951845169}, {"id": 111, "seek": 34304, "start": 351.52000000000004, "end": 355.6, "text": " to get chat GPT and GPT4 to think step by step,", "tokens": [50788, 281, 483, 5081, 26039, 51, 293, 26039, 51, 19, 281, 519, 1823, 538, 1823, 11, 50992], "temperature": 0.0, "avg_logprob": -0.10583899201465254, "compression_ratio": 1.6171003717472119, "no_speech_prob": 0.008573818951845169}, {"id": 112, "seek": 34304, "start": 355.6, "end": 360.08000000000004, "text": " leading to much richer explanations, as you can see in this diagram.", "tokens": [50992, 5775, 281, 709, 29021, 28708, 11, 382, 291, 393, 536, 294, 341, 10686, 13, 51216], "temperature": 0.0, "avg_logprob": -0.10583899201465254, "compression_ratio": 1.6171003717472119, "no_speech_prob": 0.008573818951845169}, {"id": 113, "seek": 34304, "start": 360.08000000000004, "end": 362.0, "text": " It wasn't just let's think step by step,", "tokens": [51216, 467, 2067, 380, 445, 718, 311, 519, 1823, 538, 1823, 11, 51312], "temperature": 0.0, "avg_logprob": -0.10583899201465254, "compression_ratio": 1.6171003717472119, "no_speech_prob": 0.008573818951845169}, {"id": 114, "seek": 34304, "start": 362.0, "end": 365.12, "text": " by the way, also things like explain like I'm 5.", "tokens": [51312, 538, 264, 636, 11, 611, 721, 411, 2903, 411, 286, 478, 1025, 13, 51468], "temperature": 0.0, "avg_logprob": -0.10583899201465254, "compression_ratio": 1.6171003717472119, "no_speech_prob": 0.008573818951845169}, {"id": 115, "seek": 34304, "start": 365.12, "end": 369.6, "text": " They also wanted the task to be as complex and diverse as possible,", "tokens": [51468, 814, 611, 1415, 264, 5633, 281, 312, 382, 3997, 293, 9521, 382, 1944, 11, 51692], "temperature": 0.0, "avg_logprob": -0.10583899201465254, "compression_ratio": 1.6171003717472119, "no_speech_prob": 0.008573818951845169}, {"id": 116, "seek": 34304, "start": 369.6, "end": 371.92, "text": " so they used the Flan collection.", "tokens": [51692, 370, 436, 1143, 264, 3235, 282, 5765, 13, 51808], "temperature": 0.0, "avg_logprob": -0.10583899201465254, "compression_ratio": 1.6171003717472119, "no_speech_prob": 0.008573818951845169}, {"id": 117, "seek": 37192, "start": 371.92, "end": 374.0, "text": " This was released by Google in February,", "tokens": [50364, 639, 390, 4736, 538, 3329, 294, 8711, 11, 50468], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 118, "seek": 37192, "start": 374.0, "end": 377.6, "text": " and focused on balancing the kind of prompts and tasks", "tokens": [50468, 293, 5178, 322, 22495, 264, 733, 295, 41095, 293, 9608, 50648], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 119, "seek": 37192, "start": 377.6, "end": 379.92, "text": " that you fine tune the language models on.", "tokens": [50648, 300, 291, 2489, 10864, 264, 2856, 5245, 322, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 120, "seek": 37192, "start": 379.92, "end": 382.48, "text": " You can see here the 16 system messages", "tokens": [50764, 509, 393, 536, 510, 264, 3165, 1185, 7897, 50892], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 121, "seek": 37192, "start": 382.48, "end": 385.36, "text": " that they give to chat GPT and GPT4,", "tokens": [50892, 300, 436, 976, 281, 5081, 26039, 51, 293, 26039, 51, 19, 11, 51036], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 122, "seek": 37192, "start": 385.36, "end": 388.08000000000004, "text": " and you can see here the kind of difference that that makes.", "tokens": [51036, 293, 291, 393, 536, 510, 264, 733, 295, 2649, 300, 300, 1669, 13, 51172], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 123, "seek": 37192, "start": 388.08000000000004, "end": 391.36, "text": " Imagine a language model trying to learn from this human.", "tokens": [51172, 11739, 257, 2856, 2316, 1382, 281, 1466, 490, 341, 1952, 13, 51336], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 124, "seek": 37192, "start": 391.36, "end": 392.48, "text": " The human is asked,", "tokens": [51336, 440, 1952, 307, 2351, 11, 51392], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 125, "seek": 37192, "start": 392.48, "end": 394.64, "text": " pick which sentence is not logical.", "tokens": [51392, 1888, 597, 8174, 307, 406, 14978, 13, 51500], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 126, "seek": 37192, "start": 394.64, "end": 397.76, "text": " Sentence A, people in the desert often look forward to flood,", "tokens": [51500, 23652, 655, 316, 11, 561, 294, 264, 11029, 2049, 574, 2128, 281, 10481, 11, 51656], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 127, "seek": 37192, "start": 397.76, "end": 401.04, "text": " or sentence B, people in the desert often look forward to rain.", "tokens": [51656, 420, 8174, 363, 11, 561, 294, 264, 11029, 2049, 574, 2128, 281, 4830, 13, 51820], "temperature": 0.0, "avg_logprob": -0.07694101686830875, "compression_ratio": 1.8169014084507042, "no_speech_prob": 0.00970389787107706}, {"id": 128, "seek": 40104, "start": 401.04, "end": 402.24, "text": " The human responds,", "tokens": [50364, 440, 1952, 27331, 11, 50424], "temperature": 0.0, "avg_logprob": -0.09738124570538921, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00026945682475343347}, {"id": 129, "seek": 40104, "start": 402.24, "end": 404.48, "text": " there is no reason to look forward to a flood,", "tokens": [50424, 456, 307, 572, 1778, 281, 574, 2128, 281, 257, 10481, 11, 50536], "temperature": 0.0, "avg_logprob": -0.09738124570538921, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00026945682475343347}, {"id": 130, "seek": 40104, "start": 404.48, "end": 407.52000000000004, "text": " because floods cause damage, the answer is sentence A.", "tokens": [50536, 570, 35536, 3082, 4344, 11, 264, 1867, 307, 8174, 316, 13, 50688], "temperature": 0.0, "avg_logprob": -0.09738124570538921, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00026945682475343347}, {"id": 131, "seek": 40104, "start": 407.52000000000004, "end": 410.08000000000004, "text": " Now yes, a language model can learn from that,", "tokens": [50688, 823, 2086, 11, 257, 2856, 2316, 393, 1466, 490, 300, 11, 50816], "temperature": 0.0, "avg_logprob": -0.09738124570538921, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00026945682475343347}, {"id": 132, "seek": 40104, "start": 410.08000000000004, "end": 413.20000000000005, "text": " but by leveraging those system assistant messages,", "tokens": [50816, 457, 538, 32666, 729, 1185, 10994, 7897, 11, 50972], "temperature": 0.0, "avg_logprob": -0.09738124570538921, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00026945682475343347}, {"id": 133, "seek": 40104, "start": 413.20000000000005, "end": 415.68, "text": " look at the kind of response that GPT4 gives.", "tokens": [50972, 574, 412, 264, 733, 295, 4134, 300, 26039, 51, 19, 2709, 13, 51096], "temperature": 0.0, "avg_logprob": -0.09738124570538921, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00026945682475343347}, {"id": 134, "seek": 40104, "start": 415.68, "end": 419.12, "text": " Now Orca can learn a lot more from that explanation,", "tokens": [51096, 823, 1610, 496, 393, 1466, 257, 688, 544, 490, 300, 10835, 11, 51268], "temperature": 0.0, "avg_logprob": -0.09738124570538921, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00026945682475343347}, {"id": 135, "seek": 40104, "start": 419.12, "end": 421.68, "text": " and that's one of the main reasons it's better", "tokens": [51268, 293, 300, 311, 472, 295, 264, 2135, 4112, 309, 311, 1101, 51396], "temperature": 0.0, "avg_logprob": -0.09738124570538921, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00026945682475343347}, {"id": 136, "seek": 40104, "start": 421.68, "end": 423.92, "text": " than all the other open source models.", "tokens": [51396, 813, 439, 264, 661, 1269, 4009, 5245, 13, 51508], "temperature": 0.0, "avg_logprob": -0.09738124570538921, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00026945682475343347}, {"id": 137, "seek": 40104, "start": 423.92, "end": 427.76, "text": " Because remember, Vikuna is the best of the open source models.", "tokens": [51508, 1436, 1604, 11, 29465, 5051, 307, 264, 1151, 295, 264, 1269, 4009, 5245, 13, 51700], "temperature": 0.0, "avg_logprob": -0.09738124570538921, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00026945682475343347}, {"id": 138, "seek": 42776, "start": 427.76, "end": 430.88, "text": " In this leaderboard, it has an elo of 1054,", "tokens": [50364, 682, 341, 5263, 3787, 11, 309, 575, 364, 38682, 295, 1266, 19563, 11, 50520], "temperature": 0.0, "avg_logprob": -0.09778888137252242, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.014951294288039207}, {"id": 139, "seek": 42776, "start": 430.88, "end": 433.12, "text": " better even than Palm II Bison.", "tokens": [50520, 1101, 754, 813, 32668, 6351, 363, 2770, 13, 50632], "temperature": 0.0, "avg_logprob": -0.09778888137252242, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.014951294288039207}, {"id": 140, "seek": 42776, "start": 433.12, "end": 435.52, "text": " All the models higher than it are proprietary.", "tokens": [50632, 1057, 264, 5245, 2946, 813, 309, 366, 38992, 13, 50752], "temperature": 0.0, "avg_logprob": -0.09778888137252242, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.014951294288039207}, {"id": 141, "seek": 42776, "start": 435.52, "end": 438.96, "text": " But there is another reason why Orca performs so much better.", "tokens": [50752, 583, 456, 307, 1071, 1778, 983, 1610, 496, 26213, 370, 709, 1101, 13, 50924], "temperature": 0.0, "avg_logprob": -0.09778888137252242, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.014951294288039207}, {"id": 142, "seek": 42776, "start": 438.96, "end": 439.84, "text": " You might have wondered,", "tokens": [50924, 509, 1062, 362, 17055, 11, 50968], "temperature": 0.0, "avg_logprob": -0.09778888137252242, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.014951294288039207}, {"id": 143, "seek": 42776, "start": 439.84, "end": 442.4, "text": " why didn't they just use only GPT4?", "tokens": [50968, 983, 994, 380, 436, 445, 764, 787, 26039, 51, 19, 30, 51096], "temperature": 0.0, "avg_logprob": -0.09778888137252242, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.014951294288039207}, {"id": 144, "seek": 42776, "start": 442.4, "end": 445.2, "text": " Well yes, there were cost and time considerations,", "tokens": [51096, 1042, 2086, 11, 456, 645, 2063, 293, 565, 24070, 11, 51236], "temperature": 0.0, "avg_logprob": -0.09778888137252242, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.014951294288039207}, {"id": 145, "seek": 42776, "start": 445.2, "end": 447.12, "text": " but there was another factor that they found.", "tokens": [51236, 457, 456, 390, 1071, 5952, 300, 436, 1352, 13, 51332], "temperature": 0.0, "avg_logprob": -0.09778888137252242, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.014951294288039207}, {"id": 146, "seek": 42776, "start": 447.12, "end": 452.4, "text": " They were able to use chat GPT or GPT3.5 as an intermediate teacher.", "tokens": [51332, 814, 645, 1075, 281, 764, 5081, 26039, 51, 420, 26039, 51, 18, 13, 20, 382, 364, 19376, 5027, 13, 51596], "temperature": 0.0, "avg_logprob": -0.09778888137252242, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.014951294288039207}, {"id": 147, "seek": 42776, "start": 452.4, "end": 456.8, "text": " That teacher, chat GPT, was able to reduce the gap in capabilities.", "tokens": [51596, 663, 5027, 11, 5081, 26039, 51, 11, 390, 1075, 281, 5407, 264, 7417, 294, 10862, 13, 51816], "temperature": 0.0, "avg_logprob": -0.09778888137252242, "compression_ratio": 1.629251700680272, "no_speech_prob": 0.014951294288039207}, {"id": 148, "seek": 45680, "start": 456.8, "end": 459.6, "text": " So Orca got smarter and better able to learn.", "tokens": [50364, 407, 1610, 496, 658, 20294, 293, 1101, 1075, 281, 1466, 13, 50504], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 149, "seek": 45680, "start": 459.6, "end": 461.2, "text": " A bit like progressive learning,", "tokens": [50504, 316, 857, 411, 16131, 2539, 11, 50584], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 150, "seek": 45680, "start": 461.2, "end": 463.68, "text": " where you first learn from easier examples,", "tokens": [50584, 689, 291, 700, 1466, 490, 3571, 5110, 11, 50708], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 151, "seek": 45680, "start": 463.68, "end": 465.2, "text": " then followed by harder ones.", "tokens": [50708, 550, 6263, 538, 6081, 2306, 13, 50784], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 152, "seek": 45680, "start": 465.2, "end": 468.24, "text": " After that, they gave it outputs from GPT4.", "tokens": [50784, 2381, 300, 11, 436, 2729, 309, 23930, 490, 26039, 51, 19, 13, 50936], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 153, "seek": 45680, "start": 468.24, "end": 469.04, "text": " Notice by the way,", "tokens": [50936, 13428, 538, 264, 636, 11, 50976], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 154, "seek": 45680, "start": 469.04, "end": 472.8, "text": " what happens if you skip the chat GPT teaching assistant", "tokens": [50976, 437, 2314, 498, 291, 10023, 264, 5081, 26039, 51, 4571, 10994, 51164], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 155, "seek": 45680, "start": 472.8, "end": 477.04, "text": " and only train on those one million examples from GPT4.", "tokens": [51164, 293, 787, 3847, 322, 729, 472, 2459, 5110, 490, 26039, 51, 19, 13, 51376], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 156, "seek": 45680, "start": 477.04, "end": 480.24, "text": " What happens is a bit like a student struggling in a class", "tokens": [51376, 708, 2314, 307, 257, 857, 411, 257, 3107, 9314, 294, 257, 1508, 51536], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 157, "seek": 45680, "start": 480.24, "end": 481.6, "text": " that's too advanced for them.", "tokens": [51536, 300, 311, 886, 7339, 337, 552, 13, 51604], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 158, "seek": 45680, "start": 481.6, "end": 486.32, "text": " Orca actually performs worse in those circumstances, averaging 37%,", "tokens": [51604, 1610, 496, 767, 26213, 5324, 294, 729, 9121, 11, 47308, 13435, 8923, 51840], "temperature": 0.0, "avg_logprob": -0.08102483073557455, "compression_ratio": 1.6496598639455782, "no_speech_prob": 0.0013668256578966975}, {"id": 159, "seek": 48632, "start": 486.32, "end": 490.8, "text": " but with that intermediate teacher beforehand, it gets 41.7%.", "tokens": [50364, 457, 365, 300, 19376, 5027, 22893, 11, 309, 2170, 18173, 13, 22, 6856, 50588], "temperature": 0.0, "avg_logprob": -0.08077958322340442, "compression_ratio": 1.4983164983164983, "no_speech_prob": 0.0011331194546073675}, {"id": 160, "seek": 48632, "start": 490.8, "end": 491.84, "text": " Speaking of time,", "tokens": [50588, 13069, 295, 565, 11, 50640], "temperature": 0.0, "avg_logprob": -0.08077958322340442, "compression_ratio": 1.4983164983164983, "no_speech_prob": 0.0011331194546073675}, {"id": 161, "seek": 48632, "start": 491.84, "end": 497.84, "text": " it only took about 200 hours to train Orca on 20 A100 GPUs.", "tokens": [50640, 309, 787, 1890, 466, 2331, 2496, 281, 3847, 1610, 496, 322, 945, 316, 6879, 18407, 82, 13, 50940], "temperature": 0.0, "avg_logprob": -0.08077958322340442, "compression_ratio": 1.4983164983164983, "no_speech_prob": 0.0011331194546073675}, {"id": 162, "seek": 48632, "start": 497.84, "end": 502.4, "text": " They did take a few weeks to collect the data from chat GPT and GPT4,", "tokens": [50940, 814, 630, 747, 257, 1326, 3259, 281, 2500, 264, 1412, 490, 5081, 26039, 51, 293, 26039, 51, 19, 11, 51168], "temperature": 0.0, "avg_logprob": -0.08077958322340442, "compression_ratio": 1.4983164983164983, "no_speech_prob": 0.0011331194546073675}, {"id": 163, "seek": 48632, "start": 502.4, "end": 504.96, "text": " but presumably if they're planning to open source this,", "tokens": [51168, 457, 26742, 498, 436, 434, 5038, 281, 1269, 4009, 341, 11, 51296], "temperature": 0.0, "avg_logprob": -0.08077958322340442, "compression_ratio": 1.4983164983164983, "no_speech_prob": 0.0011331194546073675}, {"id": 164, "seek": 48632, "start": 504.96, "end": 506.15999999999997, "text": " which they say they are,", "tokens": [51296, 597, 436, 584, 436, 366, 11, 51356], "temperature": 0.0, "avg_logprob": -0.08077958322340442, "compression_ratio": 1.4983164983164983, "no_speech_prob": 0.0011331194546073675}, {"id": 165, "seek": 48632, "start": 506.15999999999997, "end": 509.12, "text": " then that step could be skipped by the wider community.", "tokens": [51356, 550, 300, 1823, 727, 312, 30193, 538, 264, 11842, 1768, 13, 51504], "temperature": 0.0, "avg_logprob": -0.08077958322340442, "compression_ratio": 1.4983164983164983, "no_speech_prob": 0.0011331194546073675}, {"id": 166, "seek": 48632, "start": 509.12, "end": 511.28, "text": " Let's now look at some more of the results.", "tokens": [51504, 961, 311, 586, 574, 412, 512, 544, 295, 264, 3542, 13, 51612], "temperature": 0.0, "avg_logprob": -0.08077958322340442, "compression_ratio": 1.4983164983164983, "no_speech_prob": 0.0011331194546073675}, {"id": 167, "seek": 48632, "start": 511.28, "end": 514.64, "text": " First, for open-ended generation, not multiple choice.", "tokens": [51612, 2386, 11, 337, 1269, 12, 3502, 5125, 11, 406, 3866, 3922, 13, 51780], "temperature": 0.0, "avg_logprob": -0.08077958322340442, "compression_ratio": 1.4983164983164983, "no_speech_prob": 0.0011331194546073675}, {"id": 168, "seek": 51464, "start": 514.64, "end": 518.56, "text": " Orca is 95% of chat GPT quality", "tokens": [50364, 1610, 496, 307, 13420, 4, 295, 5081, 26039, 51, 3125, 50560], "temperature": 0.0, "avg_logprob": -0.043323945999145505, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0014548591570928693}, {"id": 169, "seek": 51464, "start": 518.56, "end": 523.04, "text": " and 85% of GPT4's quality as assessed by GPT4,", "tokens": [50560, 293, 14695, 4, 295, 26039, 51, 19, 311, 3125, 382, 36051, 538, 26039, 51, 19, 11, 50784], "temperature": 0.0, "avg_logprob": -0.043323945999145505, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0014548591570928693}, {"id": 170, "seek": 51464, "start": 523.04, "end": 526.48, "text": " but they wanted to quickly move on to some more definitive tasks", "tokens": [50784, 457, 436, 1415, 281, 2661, 1286, 322, 281, 512, 544, 28152, 9608, 50956], "temperature": 0.0, "avg_logprob": -0.043323945999145505, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0014548591570928693}, {"id": 171, "seek": 51464, "start": 526.48, "end": 529.92, "text": " because there is a problem of using GPT4 as an assessor.", "tokens": [50956, 570, 456, 307, 257, 1154, 295, 1228, 26039, 51, 19, 382, 364, 5877, 284, 13, 51128], "temperature": 0.0, "avg_logprob": -0.043323945999145505, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0014548591570928693}, {"id": 172, "seek": 51464, "start": 529.92, "end": 530.64, "text": " For example,", "tokens": [51128, 1171, 1365, 11, 51164], "temperature": 0.0, "avg_logprob": -0.043323945999145505, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0014548591570928693}, {"id": 173, "seek": 51464, "start": 530.64, "end": 534.72, "text": " they observed that there is a positive bias in GPT4 evaluation", "tokens": [51164, 436, 13095, 300, 456, 307, 257, 3353, 12577, 294, 26039, 51, 19, 13344, 51368], "temperature": 0.0, "avg_logprob": -0.043323945999145505, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0014548591570928693}, {"id": 174, "seek": 51464, "start": 534.72, "end": 538.88, "text": " toward the response of the first model in the comparison set.", "tokens": [51368, 7361, 264, 4134, 295, 264, 700, 2316, 294, 264, 9660, 992, 13, 51576], "temperature": 0.0, "avg_logprob": -0.043323945999145505, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0014548591570928693}, {"id": 175, "seek": 51464, "start": 538.88, "end": 542.0, "text": " This reminded me of the unfaithful reasoning paper", "tokens": [51576, 639, 15920, 385, 295, 264, 3971, 64, 355, 906, 21577, 3035, 51732], "temperature": 0.0, "avg_logprob": -0.043323945999145505, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.0014548591570928693}, {"id": 176, "seek": 54200, "start": 542.0, "end": 544.64, "text": " that I talked about in one of my recent videos.", "tokens": [50364, 300, 286, 2825, 466, 294, 472, 295, 452, 5162, 2145, 13, 50496], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 177, "seek": 54200, "start": 544.64, "end": 548.0, "text": " You can't always trust GPT4 to give its true reasoning,", "tokens": [50496, 509, 393, 380, 1009, 3361, 26039, 51, 19, 281, 976, 1080, 2074, 21577, 11, 50664], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 178, "seek": 54200, "start": 548.0, "end": 551.36, "text": " but here it is in more objective multiple choice questions.", "tokens": [50664, 457, 510, 309, 307, 294, 544, 10024, 3866, 3922, 1651, 13, 50832], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 179, "seek": 54200, "start": 551.36, "end": 553.76, "text": " And notice how much harder many of these tests are", "tokens": [50832, 400, 3449, 577, 709, 6081, 867, 295, 613, 6921, 366, 50952], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 180, "seek": 54200, "start": 553.76, "end": 556.0, "text": " for even these advanced language models.", "tokens": [50952, 337, 754, 613, 7339, 2856, 5245, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 181, "seek": 54200, "start": 556.0, "end": 559.2, "text": " I am fortunate and proud to have attained a perfect score", "tokens": [51064, 286, 669, 14096, 293, 4570, 281, 362, 46633, 257, 2176, 6175, 51224], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 182, "seek": 54200, "start": 559.2, "end": 560.88, "text": " in some of the tests in this chart,", "tokens": [51224, 294, 512, 295, 264, 6921, 294, 341, 6927, 11, 51308], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 183, "seek": 54200, "start": 560.88, "end": 562.4, "text": " like the GRE and GMAT.", "tokens": [51308, 411, 264, 20830, 293, 16609, 2218, 13, 51384], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 184, "seek": 54200, "start": 562.4, "end": 565.76, "text": " They were part of the Aquarat tests that they gave the models,", "tokens": [51384, 814, 645, 644, 295, 264, 8728, 18452, 6921, 300, 436, 2729, 264, 5245, 11, 51552], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 185, "seek": 54200, "start": 565.76, "end": 568.16, "text": " so I can say that they really are quite challenging,", "tokens": [51552, 370, 286, 393, 584, 300, 436, 534, 366, 1596, 7595, 11, 51672], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 186, "seek": 54200, "start": 568.16, "end": 570.8, "text": " hence why GPT4 only gets a 40%.", "tokens": [51672, 16678, 983, 26039, 51, 19, 787, 2170, 257, 3356, 6856, 51804], "temperature": 0.0, "avg_logprob": -0.0856329965934479, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.11578769236803055}, {"id": 187, "seek": 57080, "start": 570.88, "end": 572.24, "text": " You can see that throughout,", "tokens": [50368, 509, 393, 536, 300, 3710, 11, 50436], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 188, "seek": 57080, "start": 572.24, "end": 575.4399999999999, "text": " Orca outperforms Vecuna by quite a margin", "tokens": [50436, 1610, 496, 484, 26765, 82, 691, 3045, 5051, 538, 1596, 257, 10270, 50596], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 189, "seek": 57080, "start": 575.4399999999999, "end": 578.64, "text": " and is very competitive with Textavinci 3.", "tokens": [50596, 293, 307, 588, 10043, 365, 18643, 706, 21961, 805, 13, 50756], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 190, "seek": 57080, "start": 578.64, "end": 581.3599999999999, "text": " Of course, overall, it does lag behind GPT4,", "tokens": [50756, 2720, 1164, 11, 4787, 11, 309, 775, 8953, 2261, 26039, 51, 19, 11, 50892], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 191, "seek": 57080, "start": 581.3599999999999, "end": 583.1999999999999, "text": " but this is all zero shot.", "tokens": [50892, 457, 341, 307, 439, 4018, 3347, 13, 50984], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 192, "seek": 57080, "start": 583.1999999999999, "end": 583.92, "text": " A bit later on,", "tokens": [50984, 316, 857, 1780, 322, 11, 51020], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 193, "seek": 57080, "start": 583.92, "end": 586.8, "text": " I'll come back to the range of methods that we could use", "tokens": [51020, 286, 603, 808, 646, 281, 264, 3613, 295, 7150, 300, 321, 727, 764, 51164], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 194, "seek": 57080, "start": 586.8, "end": 588.88, "text": " to further improve on Orca.", "tokens": [51164, 281, 3052, 3470, 322, 1610, 496, 13, 51268], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 195, "seek": 57080, "start": 588.88, "end": 590.56, "text": " The percentages, by the way,", "tokens": [51268, 440, 42270, 11, 538, 264, 636, 11, 51352], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 196, "seek": 57080, "start": 590.56, "end": 592.4799999999999, "text": " are the improvements on Vecuna,", "tokens": [51352, 366, 264, 13797, 322, 691, 3045, 5051, 11, 51448], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 197, "seek": 57080, "start": 592.4799999999999, "end": 595.04, "text": " again the second best open source model.", "tokens": [51448, 797, 264, 1150, 1151, 1269, 4009, 2316, 13, 51576], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 198, "seek": 57080, "start": 595.04, "end": 597.8399999999999, "text": " So far, we've looked at human centric benchmarks", "tokens": [51576, 407, 1400, 11, 321, 600, 2956, 412, 1952, 1489, 1341, 43751, 51716], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 199, "seek": 57080, "start": 597.8399999999999, "end": 599.68, "text": " like the GMAT and GRE.", "tokens": [51716, 411, 264, 16609, 2218, 293, 20830, 13, 51808], "temperature": 0.0, "avg_logprob": -0.12568602763431172, "compression_ratio": 1.498371335504886, "no_speech_prob": 0.010647907853126526}, {"id": 200, "seek": 59968, "start": 599.68, "end": 602.8, "text": " These are grouped with the lovely name AGI EVAL,", "tokens": [50364, 1981, 366, 41877, 365, 264, 7496, 1315, 316, 26252, 15733, 3427, 11, 50520], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 201, "seek": 59968, "start": 602.8, "end": 603.5999999999999, "text": " and as we've seen,", "tokens": [50520, 293, 382, 321, 600, 1612, 11, 50560], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 202, "seek": 59968, "start": 603.5999999999999, "end": 607.28, "text": " even the top models lag behind the top human performers.", "tokens": [50560, 754, 264, 1192, 5245, 8953, 2261, 264, 1192, 1952, 30768, 13, 50744], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 203, "seek": 59968, "start": 607.28, "end": 610.88, "text": " But what about a benchmark specifically for language models?", "tokens": [50744, 583, 437, 466, 257, 18927, 4682, 337, 2856, 5245, 30, 50924], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 204, "seek": 59968, "start": 610.88, "end": 612.7199999999999, "text": " It's called Big Bench Hard.", "tokens": [50924, 467, 311, 1219, 5429, 3964, 339, 11817, 13, 51016], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 205, "seek": 59968, "start": 612.7199999999999, "end": 615.68, "text": " The original Big Bench had 207 tasks,", "tokens": [51016, 440, 3380, 5429, 3964, 339, 632, 945, 22, 9608, 11, 51164], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 206, "seek": 59968, "start": 615.68, "end": 617.52, "text": " but language models got so good", "tokens": [51164, 457, 2856, 5245, 658, 370, 665, 51256], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 207, "seek": 59968, "start": 617.52, "end": 619.76, "text": " that they had to narrow down the benchmark", "tokens": [51256, 300, 436, 632, 281, 9432, 760, 264, 18927, 51368], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 208, "seek": 59968, "start": 619.76, "end": 622.4799999999999, "text": " to just the 23 challenging tasks", "tokens": [51368, 281, 445, 264, 6673, 7595, 9608, 51504], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 209, "seek": 59968, "start": 622.4799999999999, "end": 625.4399999999999, "text": " where human raters still did better than language models.", "tokens": [51504, 689, 1952, 5937, 433, 920, 630, 1101, 813, 2856, 5245, 13, 51652], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 210, "seek": 59968, "start": 625.4399999999999, "end": 627.5999999999999, "text": " Now, it turns out when you add Chain of Thought prompting", "tokens": [51652, 823, 11, 309, 4523, 484, 562, 291, 909, 33252, 295, 23058, 12391, 278, 51760], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 211, "seek": 59968, "start": 627.5999999999999, "end": 629.28, "text": " to the models, they do even better", "tokens": [51760, 281, 264, 5245, 11, 436, 360, 754, 1101, 51844], "temperature": 0.0, "avg_logprob": -0.08428790481002242, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.001000268617644906}, {"id": 212, "seek": 62928, "start": 629.28, "end": 631.92, "text": " and there are even fewer tasks that humans are better at.", "tokens": [50364, 293, 456, 366, 754, 13366, 9608, 300, 6255, 366, 1101, 412, 13, 50496], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 213, "seek": 62928, "start": 631.92, "end": 633.4399999999999, "text": " But anyway, all you have to remember", "tokens": [50496, 583, 4033, 11, 439, 291, 362, 281, 1604, 50572], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 214, "seek": 62928, "start": 633.4399999999999, "end": 637.76, "text": " is that these are 23 of the hardest tasks for language models.", "tokens": [50572, 307, 300, 613, 366, 6673, 295, 264, 13158, 9608, 337, 2856, 5245, 13, 50788], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 215, "seek": 62928, "start": 637.76, "end": 640.3199999999999, "text": " And I'll just let you compare the results for yourself.", "tokens": [50788, 400, 286, 603, 445, 718, 291, 6794, 264, 3542, 337, 1803, 13, 50916], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 216, "seek": 62928, "start": 640.3199999999999, "end": 642.88, "text": " But the trend is really quite clear.", "tokens": [50916, 583, 264, 6028, 307, 534, 1596, 1850, 13, 51044], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 217, "seek": 62928, "start": 642.88, "end": 645.4399999999999, "text": " Orca massively outperforming", "tokens": [51044, 1610, 496, 29379, 484, 26765, 278, 51172], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 218, "seek": 62928, "start": 645.4399999999999, "end": 648.24, "text": " the previous best open source model, Vecuna,", "tokens": [51172, 264, 3894, 1151, 1269, 4009, 2316, 11, 691, 3045, 5051, 11, 51312], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 219, "seek": 62928, "start": 648.24, "end": 650.72, "text": " beating even chat GPT on average,", "tokens": [51312, 13497, 754, 5081, 26039, 51, 322, 4274, 11, 51436], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 220, "seek": 62928, "start": 650.72, "end": 653.36, "text": " but still, of course, lagging behind GPT4,", "tokens": [51436, 457, 920, 11, 295, 1164, 11, 8953, 3249, 2261, 26039, 51, 19, 11, 51568], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 221, "seek": 62928, "start": 653.36, "end": 655.36, "text": " except for a few tasks.", "tokens": [51568, 3993, 337, 257, 1326, 9608, 13, 51668], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 222, "seek": 62928, "start": 655.36, "end": 656.72, "text": " Look at Web of Lies,", "tokens": [51668, 2053, 412, 9573, 295, 441, 530, 11, 51736], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 223, "seek": 62928, "start": 656.72, "end": 658.9599999999999, "text": " where Orca outperforms GPT4.", "tokens": [51736, 689, 1610, 496, 484, 26765, 82, 26039, 51, 19, 13, 51848], "temperature": 0.0, "avg_logprob": -0.07239095824105399, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0009695216431282461}, {"id": 224, "seek": 65896, "start": 658.96, "end": 660.8000000000001, "text": " That would be a question like this.", "tokens": [50364, 663, 576, 312, 257, 1168, 411, 341, 13, 50456], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 225, "seek": 65896, "start": 660.8000000000001, "end": 663.2, "text": " Alexis says Shonda tells the truth.", "tokens": [50456, 39826, 1619, 1160, 12233, 5112, 264, 3494, 13, 50576], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 226, "seek": 65896, "start": 663.2, "end": 664.08, "text": " Jim Lies?", "tokens": [50576, 6637, 441, 530, 30, 50620], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 227, "seek": 65896, "start": 664.08, "end": 666.64, "text": " Antoine says Jim tells the truth.", "tokens": [50620, 5130, 44454, 1619, 6637, 5112, 264, 3494, 13, 50748], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 228, "seek": 65896, "start": 666.64, "end": 668.8000000000001, "text": " Shonda says Antoine Lies.", "tokens": [50748, 1160, 12233, 1619, 5130, 44454, 441, 530, 13, 50856], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 229, "seek": 65896, "start": 668.8000000000001, "end": 670.48, "text": " Does Alexis tell the truth?", "tokens": [50856, 4402, 39826, 980, 264, 3494, 30, 50940], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 230, "seek": 65896, "start": 670.48, "end": 672.8000000000001, "text": " Or what about temporal sequences,", "tokens": [50940, 1610, 437, 466, 30881, 22978, 11, 51056], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 231, "seek": 65896, "start": 672.8000000000001, "end": 675.6800000000001, "text": " where Orca absolutely crushes Vecuna", "tokens": [51056, 689, 1610, 496, 3122, 10321, 279, 691, 3045, 5051, 51200], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 232, "seek": 65896, "start": 675.6800000000001, "end": 678.32, "text": " and doubles chat GPT's performance?", "tokens": [51200, 293, 31634, 5081, 26039, 51, 311, 3389, 30, 51332], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 233, "seek": 65896, "start": 678.32, "end": 680.1600000000001, "text": " That would be a situation like this.", "tokens": [51332, 663, 576, 312, 257, 2590, 411, 341, 13, 51424], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 234, "seek": 65896, "start": 680.1600000000001, "end": 681.52, "text": " Now, I'm not going to read it all out,", "tokens": [51424, 823, 11, 286, 478, 406, 516, 281, 1401, 309, 439, 484, 11, 51492], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 235, "seek": 65896, "start": 681.52, "end": 683.2800000000001, "text": " but essentially you have to figure out", "tokens": [51492, 457, 4476, 291, 362, 281, 2573, 484, 51580], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 236, "seek": 65896, "start": 683.2800000000001, "end": 684.72, "text": " when the timings match up.", "tokens": [51580, 562, 264, 524, 1109, 2995, 493, 13, 51652], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 237, "seek": 65896, "start": 684.72, "end": 686.48, "text": " Basically keeping track of time", "tokens": [51652, 8537, 5145, 2837, 295, 565, 51740], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 238, "seek": 65896, "start": 686.48, "end": 688.32, "text": " and Orca does really well", "tokens": [51740, 293, 1610, 496, 775, 534, 731, 51832], "temperature": 0.0, "avg_logprob": -0.10206463639165314, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.008058030158281326}, {"id": 239, "seek": 68832, "start": 688.4000000000001, "end": 690.72, "text": " and chat GPT flops getting it wrong.", "tokens": [50368, 293, 5081, 26039, 51, 932, 3370, 1242, 309, 2085, 13, 50484], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 240, "seek": 68832, "start": 690.72, "end": 693.5200000000001, "text": " Interestingly, they also tested all four models", "tokens": [50484, 30564, 11, 436, 611, 8246, 439, 1451, 5245, 50624], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 241, "seek": 68832, "start": 693.5200000000001, "end": 695.6, "text": " on that common sense reasoning question", "tokens": [50624, 322, 300, 2689, 2020, 21577, 1168, 50728], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 242, "seek": 68832, "start": 695.6, "end": 697.9200000000001, "text": " that I demonstrated for smart GPT,", "tokens": [50728, 300, 286, 18772, 337, 4069, 26039, 51, 11, 50844], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 243, "seek": 68832, "start": 697.9200000000001, "end": 699.7600000000001, "text": " about hanging the clothes to dry.", "tokens": [50844, 466, 8345, 264, 5534, 281, 4016, 13, 50936], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 244, "seek": 68832, "start": 699.7600000000001, "end": 700.6400000000001, "text": " As you might remember,", "tokens": [50936, 1018, 291, 1062, 1604, 11, 50980], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 245, "seek": 68832, "start": 700.6400000000001, "end": 702.1600000000001, "text": " you can use prompt engineering", "tokens": [50980, 291, 393, 764, 12391, 7043, 51056], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 246, "seek": 68832, "start": 702.1600000000001, "end": 704.72, "text": " to nudge the models to almost always get it right,", "tokens": [51056, 281, 297, 16032, 264, 5245, 281, 1920, 1009, 483, 309, 558, 11, 51184], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 247, "seek": 68832, "start": 704.72, "end": 707.0400000000001, "text": " which is partly why I view these results", "tokens": [51184, 597, 307, 17031, 983, 286, 1910, 613, 3542, 51300], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 248, "seek": 68832, "start": 707.0400000000001, "end": 709.44, "text": " more as a baseline rather than a cap.", "tokens": [51300, 544, 382, 257, 20518, 2831, 813, 257, 1410, 13, 51420], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 249, "seek": 68832, "start": 709.44, "end": 711.0400000000001, "text": " And the authors admit this too.", "tokens": [51420, 400, 264, 16552, 9796, 341, 886, 13, 51500], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 250, "seek": 68832, "start": 711.0400000000001, "end": 713.0400000000001, "text": " Orca has been trained on data", "tokens": [51500, 1610, 496, 575, 668, 8895, 322, 1412, 51600], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 251, "seek": 68832, "start": 713.0400000000001, "end": 715.2800000000001, "text": " that simulates zero shot setting", "tokens": [51600, 300, 1034, 26192, 4018, 3347, 3287, 51712], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 252, "seek": 68832, "start": 715.2800000000001, "end": 716.48, "text": " with standard prompts.", "tokens": [51712, 365, 3832, 41095, 13, 51772], "temperature": 0.0, "avg_logprob": -0.06958828550396544, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0027144229970872402}, {"id": 253, "seek": 71648, "start": 716.48, "end": 718.64, "text": " The model's performance in other contexts,", "tokens": [50364, 440, 2316, 311, 3389, 294, 661, 30628, 11, 50472], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 254, "seek": 71648, "start": 718.64, "end": 720.72, "text": " such as multi-turn conversations,", "tokens": [50472, 1270, 382, 4825, 12, 33886, 7315, 11, 50576], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 255, "seek": 71648, "start": 720.72, "end": 722.88, "text": " like the DERA paper I talked about on the channel,", "tokens": [50576, 411, 264, 413, 1598, 32, 3035, 286, 2825, 466, 322, 264, 2269, 11, 50684], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 256, "seek": 71648, "start": 722.88, "end": 725.52, "text": " in context learning and few shot learning,", "tokens": [50684, 294, 4319, 2539, 293, 1326, 3347, 2539, 11, 50816], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 257, "seek": 71648, "start": 725.52, "end": 727.6, "text": " or advanced prompting techniques,", "tokens": [50816, 420, 7339, 12391, 278, 7512, 11, 50920], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 258, "seek": 71648, "start": 727.6, "end": 730.4, "text": " that smart GPT or Tree of Thoughts, for example,", "tokens": [50920, 300, 4069, 26039, 51, 420, 22291, 295, 23058, 82, 11, 337, 1365, 11, 51060], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 259, "seek": 71648, "start": 730.4, "end": 732.88, "text": " and they say like chain of thought prompting,", "tokens": [51060, 293, 436, 584, 411, 5021, 295, 1194, 12391, 278, 11, 51184], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 260, "seek": 71648, "start": 732.88, "end": 734.32, "text": " remains untested.", "tokens": [51184, 7023, 1701, 21885, 13, 51256], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 261, "seek": 71648, "start": 734.32, "end": 736.88, "text": " These results are a baseline, not a cap.", "tokens": [51256, 1981, 3542, 366, 257, 20518, 11, 406, 257, 1410, 13, 51384], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 262, "seek": 71648, "start": 736.88, "end": 739.6800000000001, "text": " They mention other ways that Orca could be improved,", "tokens": [51384, 814, 2152, 661, 2098, 300, 1610, 496, 727, 312, 9689, 11, 51524], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 263, "seek": 71648, "start": 739.6800000000001, "end": 742.4, "text": " for example, through tool augmentation.", "tokens": [51524, 337, 1365, 11, 807, 2290, 14501, 19631, 13, 51660], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 264, "seek": 71648, "start": 742.4, "end": 744.0, "text": " And that's not just calculators,", "tokens": [51660, 400, 300, 311, 406, 445, 4322, 3391, 11, 51740], "temperature": 0.0, "avg_logprob": -0.12099525422760934, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.0053834267891943455}, {"id": 265, "seek": 74400, "start": 744.08, "end": 746.88, "text": " calendars, Bing, or auto GPT.", "tokens": [50368, 37022, 685, 11, 30755, 11, 420, 8399, 26039, 51, 13, 50508], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 266, "seek": 74400, "start": 746.88, "end": 749.04, "text": " I was going to do a separate video on this paper,", "tokens": [50508, 286, 390, 516, 281, 360, 257, 4994, 960, 322, 341, 3035, 11, 50616], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 267, "seek": 74400, "start": 749.04, "end": 750.4, "text": " but I'll just mention it here.", "tokens": [50616, 457, 286, 603, 445, 2152, 309, 510, 13, 50684], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 268, "seek": 74400, "start": 750.4, "end": 752.4, "text": " This paper from last week demonstrated", "tokens": [50684, 639, 3035, 490, 1036, 1243, 18772, 50784], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 269, "seek": 74400, "start": 752.4, "end": 754.96, "text": " that larger models can create tools", "tokens": [50784, 300, 4833, 5245, 393, 1884, 3873, 50912], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 270, "seek": 74400, "start": 754.96, "end": 757.68, "text": " that smaller models can then use more efficiently.", "tokens": [50912, 300, 4356, 5245, 393, 550, 764, 544, 19621, 13, 51048], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 271, "seek": 74400, "start": 757.68, "end": 760.4, "text": " Once the best language model, say GPT-4,", "tokens": [51048, 3443, 264, 1151, 2856, 2316, 11, 584, 26039, 51, 12, 19, 11, 51184], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 272, "seek": 74400, "start": 760.4, "end": 762.88, "text": " has created a generic Python function,", "tokens": [51184, 575, 2942, 257, 19577, 15329, 2445, 11, 51308], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 273, "seek": 74400, "start": 762.88, "end": 763.84, "text": " which is the tool,", "tokens": [51308, 597, 307, 264, 2290, 11, 51356], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 274, "seek": 74400, "start": 763.84, "end": 765.76, "text": " and then written some unit tests,", "tokens": [51356, 293, 550, 3720, 512, 4985, 6921, 11, 51452], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 275, "seek": 74400, "start": 765.76, "end": 768.24, "text": " it can then wrap and hand over those tools", "tokens": [51452, 309, 393, 550, 7019, 293, 1011, 670, 729, 3873, 51576], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 276, "seek": 74400, "start": 768.24, "end": 770.8, "text": " to smaller models like GPT-3.5,", "tokens": [51576, 281, 4356, 5245, 411, 26039, 51, 12, 18, 13, 20, 11, 51704], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 277, "seek": 74400, "start": 770.8, "end": 772.08, "text": " or in this case, Orca,", "tokens": [51704, 420, 294, 341, 1389, 11, 1610, 496, 11, 51768], "temperature": 0.0, "avg_logprob": -0.0727992196013962, "compression_ratio": 1.6560283687943262, "no_speech_prob": 0.0021153483539819717}, {"id": 278, "seek": 77208, "start": 772.08, "end": 774.1600000000001, "text": " and check out the toolmaking row", "tokens": [50364, 293, 1520, 484, 264, 2290, 12402, 5386, 50468], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 279, "seek": 77208, "start": 774.1600000000001, "end": 776.72, "text": " to see the improvement for chat GPT,", "tokens": [50468, 281, 536, 264, 10444, 337, 5081, 26039, 51, 11, 50596], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 280, "seek": 77208, "start": 776.72, "end": 778.1600000000001, "text": " or in our case, Orca,", "tokens": [50596, 420, 294, 527, 1389, 11, 1610, 496, 11, 50668], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 281, "seek": 77208, "start": 778.1600000000001, "end": 781.2800000000001, "text": " when they're given these tools created by GPT-4", "tokens": [50668, 562, 436, 434, 2212, 613, 3873, 2942, 538, 26039, 51, 12, 19, 50824], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 282, "seek": 77208, "start": 781.2800000000001, "end": 782.72, "text": " or better language models.", "tokens": [50824, 420, 1101, 2856, 5245, 13, 50896], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 283, "seek": 77208, "start": 782.72, "end": 785.2, "text": " Their performance across a range of tasks", "tokens": [50896, 6710, 3389, 2108, 257, 3613, 295, 9608, 51020], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 284, "seek": 77208, "start": 785.2, "end": 786.64, "text": " goes dramatically up,", "tokens": [51020, 1709, 17548, 493, 11, 51092], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 285, "seek": 77208, "start": 786.64, "end": 788.0, "text": " and we haven't even talked about", "tokens": [51092, 293, 321, 2378, 380, 754, 2825, 466, 51160], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 286, "seek": 77208, "start": 788.0, "end": 790.4000000000001, "text": " using a process-based reward model,", "tokens": [51160, 1228, 257, 1399, 12, 6032, 7782, 2316, 11, 51280], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 287, "seek": 77208, "start": 790.4000000000001, "end": 793.2, "text": " like in the Let's Verify step-by-step paper.", "tokens": [51280, 411, 294, 264, 961, 311, 4281, 2505, 1823, 12, 2322, 12, 16792, 3035, 13, 51420], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 288, "seek": 77208, "start": 793.2, "end": 796.48, "text": " That, of course, could further improve Orca's performance.", "tokens": [51420, 663, 11, 295, 1164, 11, 727, 3052, 3470, 1610, 496, 311, 3389, 13, 51584], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 289, "seek": 77208, "start": 796.48, "end": 798.96, "text": " Of course, when this model becomes publicly available,", "tokens": [51584, 2720, 1164, 11, 562, 341, 2316, 3643, 14843, 2435, 11, 51708], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 290, "seek": 77208, "start": 798.96, "end": 800.72, "text": " I will test all of this out,", "tokens": [51708, 286, 486, 1500, 439, 295, 341, 484, 11, 51796], "temperature": 0.0, "avg_logprob": -0.07990243094308036, "compression_ratio": 1.6233333333333333, "no_speech_prob": 0.0006665827822871506}, {"id": 291, "seek": 80072, "start": 800.72, "end": 802.8000000000001, "text": " but it hasn't been open-sourced yet,", "tokens": [50364, 457, 309, 6132, 380, 668, 1269, 12, 82, 396, 1232, 1939, 11, 50468], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 292, "seek": 80072, "start": 802.8000000000001, "end": 804.5600000000001, "text": " and they do say this model", "tokens": [50468, 293, 436, 360, 584, 341, 2316, 50556], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 293, "seek": 80072, "start": 804.5600000000001, "end": 807.36, "text": " is solely designed for research settings.", "tokens": [50556, 307, 23309, 4761, 337, 2132, 6257, 13, 50696], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 294, "seek": 80072, "start": 807.36, "end": 809.6800000000001, "text": " That does seem a little bit naive to me.", "tokens": [50696, 663, 775, 1643, 257, 707, 857, 29052, 281, 385, 13, 50812], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 295, "seek": 80072, "start": 809.6800000000001, "end": 811.0400000000001, "text": " I mean, that's what Metta said", "tokens": [50812, 286, 914, 11, 300, 311, 437, 6377, 1328, 848, 50880], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 296, "seek": 80072, "start": 811.0400000000001, "end": 812.1600000000001, "text": " when they released Lama,", "tokens": [50880, 562, 436, 4736, 441, 2404, 11, 50936], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 297, "seek": 80072, "start": 812.1600000000001, "end": 814.08, "text": " but then everyone and their grandma", "tokens": [50936, 457, 550, 1518, 293, 641, 15766, 51032], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 298, "seek": 80072, "start": 814.08, "end": 816.08, "text": " just use the language model for whatever.", "tokens": [51032, 445, 764, 264, 2856, 2316, 337, 2035, 13, 51132], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 299, "seek": 80072, "start": 816.08, "end": 817.6800000000001, "text": " I do wonder what it means when they say", "tokens": [51132, 286, 360, 2441, 437, 309, 1355, 562, 436, 584, 51212], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 300, "seek": 80072, "start": 817.6800000000001, "end": 819.76, "text": " we are working with our legal team.", "tokens": [51212, 321, 366, 1364, 365, 527, 5089, 1469, 13, 51316], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 301, "seek": 80072, "start": 819.76, "end": 821.9200000000001, "text": " And it is particularly interesting to me", "tokens": [51316, 400, 309, 307, 4098, 1880, 281, 385, 51424], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 302, "seek": 80072, "start": 821.9200000000001, "end": 824.4, "text": " that this was all done by Microsoft.", "tokens": [51424, 300, 341, 390, 439, 1096, 538, 8116, 13, 51548], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 303, "seek": 80072, "start": 824.4, "end": 826.48, "text": " I'm gonna go into a little bit of speculation here", "tokens": [51548, 286, 478, 799, 352, 666, 257, 707, 857, 295, 27696, 510, 51652], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 304, "seek": 80072, "start": 826.48, "end": 829.2, "text": " about why I think they conducted this research.", "tokens": [51652, 466, 983, 286, 519, 436, 13809, 341, 2132, 13, 51788], "temperature": 0.0, "avg_logprob": -0.07625205549475265, "compression_ratio": 1.6867088607594938, "no_speech_prob": 0.004196987021714449}, {"id": 305, "seek": 82920, "start": 829.2, "end": 831.6, "text": " You might remember that leaked memo from Google.", "tokens": [50364, 509, 1062, 1604, 300, 31779, 35900, 490, 3329, 13, 50484], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 306, "seek": 82920, "start": 831.6, "end": 834.48, "text": " We have no motes, and they even mentioned Vakuna,", "tokens": [50484, 492, 362, 572, 2184, 279, 11, 293, 436, 754, 2835, 691, 514, 5051, 11, 50628], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 307, "seek": 82920, "start": 834.48, "end": 837.2800000000001, "text": " and talked about how it circumvented restrictions", "tokens": [50628, 293, 2825, 466, 577, 309, 7125, 2475, 292, 14191, 50768], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 308, "seek": 82920, "start": 837.2800000000001, "end": 840.8000000000001, "text": " on the OpenAI API by using shared GPT.", "tokens": [50768, 322, 264, 7238, 48698, 9362, 538, 1228, 5507, 26039, 51, 13, 50944], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 309, "seek": 82920, "start": 840.8000000000001, "end": 843.36, "text": " And my theory is that the Microsoft researchers", "tokens": [50944, 400, 452, 5261, 307, 300, 264, 8116, 10309, 51072], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 310, "seek": 82920, "start": 843.36, "end": 845.5200000000001, "text": " were testing this point from the memo.", "tokens": [51072, 645, 4997, 341, 935, 490, 264, 35900, 13, 51180], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 311, "seek": 82920, "start": 845.5200000000001, "end": 848.32, "text": " The point was that training giant models from scratch", "tokens": [51180, 440, 935, 390, 300, 3097, 7410, 5245, 490, 8459, 51320], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 312, "seek": 82920, "start": 848.32, "end": 850.32, "text": " not only throws away the pre-training,", "tokens": [51320, 406, 787, 19251, 1314, 264, 659, 12, 17227, 1760, 11, 51420], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 313, "seek": 82920, "start": 850.32, "end": 852.96, "text": " but also any iterative open-source improvements", "tokens": [51420, 457, 611, 604, 17138, 1166, 1269, 12, 41676, 13797, 51552], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 314, "seek": 82920, "start": 852.96, "end": 854.1600000000001, "text": " that have been made on top.", "tokens": [51552, 300, 362, 668, 1027, 322, 1192, 13, 51612], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 315, "seek": 82920, "start": 854.1600000000001, "end": 856.5600000000001, "text": " It doesn't take long for those improvements to dominate,", "tokens": [51612, 467, 1177, 380, 747, 938, 337, 729, 13797, 281, 28246, 11, 51732], "temperature": 0.0, "avg_logprob": -0.09380424854367278, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.008843684569001198}, {"id": 316, "seek": 85656, "start": 856.64, "end": 859.3599999999999, "text": " making the full retrain extremely costly.", "tokens": [50368, 1455, 264, 1577, 1533, 7146, 4664, 28328, 13, 50504], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 317, "seek": 85656, "start": 859.3599999999999, "end": 862.4799999999999, "text": " Maybe Microsoft is hesitating about future investments", "tokens": [50504, 2704, 8116, 307, 10453, 16350, 466, 2027, 13784, 50660], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 318, "seek": 85656, "start": 862.4799999999999, "end": 864.88, "text": " in GPT-5 or GPT-6.", "tokens": [50660, 294, 26039, 51, 12, 20, 420, 26039, 51, 12, 21, 13, 50780], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 319, "seek": 85656, "start": 864.88, "end": 866.4799999999999, "text": " And they really wanna test out", "tokens": [50780, 400, 436, 534, 1948, 1500, 484, 50860], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 320, "seek": 85656, "start": 866.4799999999999, "end": 869.8399999999999, "text": " if it's easy to imitate those large models on the cheap.", "tokens": [50860, 498, 309, 311, 1858, 281, 35556, 729, 2416, 5245, 322, 264, 7084, 13, 51028], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 321, "seek": 85656, "start": 869.8399999999999, "end": 872.64, "text": " If it is, then why would Microsoft invest billions", "tokens": [51028, 759, 309, 307, 11, 550, 983, 576, 8116, 1963, 17375, 51168], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 322, "seek": 85656, "start": 872.64, "end": 874.3199999999999, "text": " in a new giant model?", "tokens": [51168, 294, 257, 777, 7410, 2316, 30, 51252], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 323, "seek": 85656, "start": 874.3199999999999, "end": 877.28, "text": " That's my own theory as to why Microsoft is working on this,", "tokens": [51252, 663, 311, 452, 1065, 5261, 382, 281, 983, 8116, 307, 1364, 322, 341, 11, 51400], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 324, "seek": 85656, "start": 877.28, "end": 879.52, "text": " but let me know in the comments what your theory is.", "tokens": [51400, 457, 718, 385, 458, 294, 264, 3053, 437, 428, 5261, 307, 13, 51512], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 325, "seek": 85656, "start": 879.52, "end": 881.52, "text": " In the conclusion, the authors state that", "tokens": [51512, 682, 264, 10063, 11, 264, 16552, 1785, 300, 51612], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 326, "seek": 85656, "start": 881.52, "end": 885.1199999999999, "text": " Orca suggests that learning from step-by-step explanations", "tokens": [51612, 1610, 496, 13409, 300, 2539, 490, 1823, 12, 2322, 12, 16792, 28708, 51792], "temperature": 0.0, "avg_logprob": -0.07315450726133405, "compression_ratio": 1.6587837837837838, "no_speech_prob": 0.009122678078711033}, {"id": 327, "seek": 88512, "start": 885.12, "end": 887.76, "text": " could significantly improve the quality of models", "tokens": [50364, 727, 10591, 3470, 264, 3125, 295, 5245, 50496], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 328, "seek": 88512, "start": 887.76, "end": 889.36, "text": " regardless of their size,", "tokens": [50496, 10060, 295, 641, 2744, 11, 50576], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 329, "seek": 88512, "start": 889.36, "end": 892.16, "text": " and that they hope these insights will inform the design", "tokens": [50576, 293, 300, 436, 1454, 613, 14310, 486, 1356, 264, 1715, 50716], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 330, "seek": 88512, "start": 892.16, "end": 894.72, "text": " of more robust evaluation methods,", "tokens": [50716, 295, 544, 13956, 13344, 7150, 11, 50844], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 331, "seek": 88512, "start": 894.72, "end": 896.96, "text": " compared to those used for a vacuna, for example,", "tokens": [50844, 5347, 281, 729, 1143, 337, 257, 2842, 5051, 11, 337, 1365, 11, 50956], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 332, "seek": 88512, "start": 896.96, "end": 900.4, "text": " and the advancement of alignment and post-training techniques,", "tokens": [50956, 293, 264, 35764, 295, 18515, 293, 2183, 12, 17227, 1760, 7512, 11, 51128], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 333, "seek": 88512, "start": 900.4, "end": 903.76, "text": " and the more effective use of powerful models", "tokens": [51128, 293, 264, 544, 4942, 764, 295, 4005, 5245, 51296], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 334, "seek": 88512, "start": 903.76, "end": 905.84, "text": " like GPT-4 as teachers.", "tokens": [51296, 411, 26039, 51, 12, 19, 382, 6023, 13, 51400], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 335, "seek": 88512, "start": 905.84, "end": 906.88, "text": " And maybe they should have said,", "tokens": [51400, 400, 1310, 436, 820, 362, 848, 11, 51452], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 336, "seek": 88512, "start": 906.88, "end": 910.24, "text": " and also with chat GPT as an intermediate teacher.", "tokens": [51452, 293, 611, 365, 5081, 26039, 51, 382, 364, 19376, 5027, 13, 51620], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 337, "seek": 88512, "start": 910.24, "end": 913.2, "text": " I'm gonna end with the thoughts of the leaders of OpenAI,", "tokens": [51620, 286, 478, 799, 917, 365, 264, 4598, 295, 264, 3523, 295, 7238, 48698, 11, 51768], "temperature": 0.0, "avg_logprob": -0.09996389573620211, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.018535342067480087}, {"id": 338, "seek": 91320, "start": 913.2800000000001, "end": 916.4000000000001, "text": " Ilya Sutskova, and Sam Oltman on open source models.", "tokens": [50368, 286, 45106, 318, 3648, 4093, 2757, 11, 293, 4832, 422, 2282, 1601, 322, 1269, 4009, 5245, 13, 50524], "temperature": 0.0, "avg_logprob": -0.1728859870664535, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.004325868561863899}, {"id": 339, "seek": 91320, "start": 916.4000000000001, "end": 918.24, "text": " And I think there is a bit of a contrast", "tokens": [50524, 400, 286, 519, 456, 307, 257, 857, 295, 257, 8712, 50616], "temperature": 0.0, "avg_logprob": -0.1728859870664535, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.004325868561863899}, {"id": 340, "seek": 91320, "start": 918.24, "end": 919.5200000000001, "text": " between the two answers.", "tokens": [50616, 1296, 264, 732, 6338, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1728859870664535, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.004325868561863899}, {"id": 341, "seek": 91320, "start": 919.5200000000001, "end": 922.6400000000001, "text": " Ilya Sutskova thinks that the gap is growing ever wider.", "tokens": [50680, 286, 45106, 318, 3648, 4093, 2757, 7309, 300, 264, 7417, 307, 4194, 1562, 11842, 13, 50836], "temperature": 0.0, "avg_logprob": -0.1728859870664535, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.004325868561863899}, {"id": 342, "seek": 91320, "start": 923.2, "end": 926.72, "text": " To the open source versus non-open source models question,", "tokens": [50864, 1407, 264, 1269, 4009, 5717, 2107, 12, 15752, 4009, 5245, 1168, 11, 51040], "temperature": 0.0, "avg_logprob": -0.1728859870664535, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.004325868561863899}, {"id": 343, "seek": 91320, "start": 927.6800000000001, "end": 931.12, "text": " you don't wanna think about it in binary black and white terms", "tokens": [51088, 291, 500, 380, 1948, 519, 466, 309, 294, 17434, 2211, 293, 2418, 2115, 51260], "temperature": 0.0, "avg_logprob": -0.1728859870664535, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.004325868561863899}, {"id": 344, "seek": 91320, "start": 931.12, "end": 934.5600000000001, "text": " where like, there is a secret source", "tokens": [51260, 689, 411, 11, 456, 307, 257, 4054, 4009, 51432], "temperature": 0.0, "avg_logprob": -0.1728859870664535, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.004325868561863899}, {"id": 345, "seek": 91320, "start": 934.5600000000001, "end": 936.96, "text": " that will never be rediscovered.", "tokens": [51432, 300, 486, 1128, 312, 2182, 40080, 292, 13, 51552], "temperature": 0.0, "avg_logprob": -0.1728859870664535, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.004325868561863899}, {"id": 346, "seek": 91320, "start": 937.84, "end": 941.5200000000001, "text": " What I will say, or whether GPT-4 will ever be reproduced", "tokens": [51596, 708, 286, 486, 584, 11, 420, 1968, 26039, 51, 12, 19, 486, 1562, 312, 11408, 1232, 51780], "temperature": 0.0, "avg_logprob": -0.1728859870664535, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.004325868561863899}, {"id": 347, "seek": 94152, "start": 941.6, "end": 945.04, "text": " by open source models, perhaps one day it will be.", "tokens": [50368, 538, 1269, 4009, 5245, 11, 4317, 472, 786, 309, 486, 312, 13, 50540], "temperature": 0.0, "avg_logprob": -0.0951366524947317, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.003075311193242669}, {"id": 348, "seek": 94152, "start": 945.76, "end": 947.04, "text": " But when it will be,", "tokens": [50576, 583, 562, 309, 486, 312, 11, 50640], "temperature": 0.0, "avg_logprob": -0.0951366524947317, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.003075311193242669}, {"id": 349, "seek": 94152, "start": 947.04, "end": 949.6, "text": " there will be a much more powerful model in the companies.", "tokens": [50640, 456, 486, 312, 257, 709, 544, 4005, 2316, 294, 264, 3431, 13, 50768], "temperature": 0.0, "avg_logprob": -0.0951366524947317, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.003075311193242669}, {"id": 350, "seek": 94152, "start": 950.64, "end": 952.24, "text": " So there will always be a gap", "tokens": [50820, 407, 456, 486, 1009, 312, 257, 7417, 50900], "temperature": 0.0, "avg_logprob": -0.0951366524947317, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.003075311193242669}, {"id": 351, "seek": 94152, "start": 952.24, "end": 956.48, "text": " between the open source models and the private models.", "tokens": [50900, 1296, 264, 1269, 4009, 5245, 293, 264, 4551, 5245, 13, 51112], "temperature": 0.0, "avg_logprob": -0.0951366524947317, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.003075311193242669}, {"id": 352, "seek": 94152, "start": 957.4399999999999, "end": 961.04, "text": " And this gap may even be increasing this time.", "tokens": [51160, 400, 341, 7417, 815, 754, 312, 5662, 341, 565, 13, 51340], "temperature": 0.0, "avg_logprob": -0.0951366524947317, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.003075311193242669}, {"id": 353, "seek": 94152, "start": 961.92, "end": 965.92, "text": " The amount of effort and engineering and research", "tokens": [51384, 440, 2372, 295, 4630, 293, 7043, 293, 2132, 51584], "temperature": 0.0, "avg_logprob": -0.0951366524947317, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.003075311193242669}, {"id": 354, "seek": 94152, "start": 965.92, "end": 969.84, "text": " that it takes to produce one such neural net keeps increasing.", "tokens": [51584, 300, 309, 2516, 281, 5258, 472, 1270, 18161, 2533, 5965, 5662, 13, 51780], "temperature": 0.0, "avg_logprob": -0.0951366524947317, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.003075311193242669}, {"id": 355, "seek": 96984, "start": 969.9200000000001, "end": 973.0400000000001, "text": " And so even if there are open source models,", "tokens": [50368, 400, 370, 754, 498, 456, 366, 1269, 4009, 5245, 11, 50524], "temperature": 0.0, "avg_logprob": -0.11000943183898926, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0006850478821434081}, {"id": 356, "seek": 96984, "start": 973.0400000000001, "end": 974.08, "text": " they will never be,", "tokens": [50524, 436, 486, 1128, 312, 11, 50576], "temperature": 0.0, "avg_logprob": -0.11000943183898926, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0006850478821434081}, {"id": 357, "seek": 96984, "start": 974.08, "end": 977.2800000000001, "text": " they will be less and less produced by small groups", "tokens": [50576, 436, 486, 312, 1570, 293, 1570, 7126, 538, 1359, 3935, 50736], "temperature": 0.0, "avg_logprob": -0.11000943183898926, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0006850478821434081}, {"id": 358, "seek": 96984, "start": 977.2800000000001, "end": 981.0400000000001, "text": " of dedicated researchers and engineers.", "tokens": [50736, 295, 8374, 10309, 293, 11955, 13, 50924], "temperature": 0.0, "avg_logprob": -0.11000943183898926, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0006850478821434081}, {"id": 359, "seek": 96984, "start": 981.0400000000001, "end": 985.2, "text": " And it will only be the providence of a company, a big company.", "tokens": [50924, 400, 309, 486, 787, 312, 264, 1439, 2778, 295, 257, 2237, 11, 257, 955, 2237, 13, 51132], "temperature": 0.0, "avg_logprob": -0.11000943183898926, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0006850478821434081}, {"id": 360, "seek": 96984, "start": 985.76, "end": 987.2, "text": " While Sam Oltman seems to say", "tokens": [51160, 3987, 4832, 422, 2282, 1601, 2544, 281, 584, 51232], "temperature": 0.0, "avg_logprob": -0.11000943183898926, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0006850478821434081}, {"id": 361, "seek": 96984, "start": 987.2, "end": 989.84, "text": " that even if open source models do catch up,", "tokens": [51232, 300, 754, 498, 1269, 4009, 5245, 360, 3745, 493, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11000943183898926, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0006850478821434081}, {"id": 362, "seek": 96984, "start": 989.84, "end": 992.72, "text": " OpenAI will always have a different kind of moat.", "tokens": [51364, 7238, 48698, 486, 1009, 362, 257, 819, 733, 295, 705, 267, 13, 51508], "temperature": 0.0, "avg_logprob": -0.11000943183898926, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0006850478821434081}, {"id": 363, "seek": 96984, "start": 992.72, "end": 994.32, "text": " What are your thoughts about the,", "tokens": [51508, 708, 366, 428, 4598, 466, 264, 11, 51588], "temperature": 0.0, "avg_logprob": -0.11000943183898926, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0006850478821434081}, {"id": 364, "seek": 96984, "start": 994.32, "end": 998.32, "text": " we have no moat document that was released lately?", "tokens": [51588, 321, 362, 572, 705, 267, 4166, 300, 390, 4736, 12881, 30, 51788], "temperature": 0.0, "avg_logprob": -0.11000943183898926, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0006850478821434081}, {"id": 365, "seek": 99984, "start": 1000.48, "end": 1001.36, "text": " The leak document.", "tokens": [50396, 440, 17143, 4166, 13, 50440], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 366, "seek": 99984, "start": 1004.24, "end": 1006.72, "text": " The thing that is special about OpenAI,", "tokens": [50584, 440, 551, 300, 307, 2121, 466, 7238, 48698, 11, 50708], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 367, "seek": 99984, "start": 1006.72, "end": 1009.9200000000001, "text": " and I think the thing that is so misunderstood by that document,", "tokens": [50708, 293, 286, 519, 264, 551, 300, 307, 370, 33870, 538, 300, 4166, 11, 50868], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 368, "seek": 99984, "start": 1009.9200000000001, "end": 1013.2, "text": " aside from the fact that we have a gigantic number of users", "tokens": [50868, 7359, 490, 264, 1186, 300, 321, 362, 257, 26800, 1230, 295, 5022, 51032], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 369, "seek": 99984, "start": 1013.2, "end": 1015.84, "text": " and people that have formed some sort of relationship", "tokens": [51032, 293, 561, 300, 362, 8693, 512, 1333, 295, 2480, 51164], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 370, "seek": 99984, "start": 1015.84, "end": 1016.8000000000001, "text": " with us and our products,", "tokens": [51164, 365, 505, 293, 527, 3383, 11, 51212], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 371, "seek": 99984, "start": 1017.52, "end": 1020.4, "text": " is what OpenAI is special about", "tokens": [51248, 307, 437, 7238, 48698, 307, 2121, 466, 51392], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 372, "seek": 99984, "start": 1020.96, "end": 1022.88, "text": " is figuring out what comes next.", "tokens": [51420, 307, 15213, 484, 437, 1487, 958, 13, 51516], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 373, "seek": 99984, "start": 1023.52, "end": 1024.56, "text": " It is the ability,", "tokens": [51548, 467, 307, 264, 3485, 11, 51600], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 374, "seek": 99984, "start": 1024.56, "end": 1027.28, "text": " it is easy to copy something once you know it can be done.", "tokens": [51600, 309, 307, 1858, 281, 5055, 746, 1564, 291, 458, 309, 393, 312, 1096, 13, 51736], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 375, "seek": 99984, "start": 1027.28, "end": 1028.72, "text": " And in that sense, sure.", "tokens": [51736, 400, 294, 300, 2020, 11, 988, 13, 51808], "temperature": 0.0, "avg_logprob": -0.13072616218501687, "compression_ratio": 1.724, "no_speech_prob": 0.0008146106265485287}, {"id": 376, "seek": 102872, "start": 1029.68, "end": 1032.08, "text": " It is very hard to go figure out what to do next.", "tokens": [50412, 467, 307, 588, 1152, 281, 352, 2573, 484, 437, 281, 360, 958, 13, 50532], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 377, "seek": 102872, "start": 1032.72, "end": 1034.72, "text": " And the ideas, the big ideas,", "tokens": [50564, 400, 264, 3487, 11, 264, 955, 3487, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 378, "seek": 102872, "start": 1034.72, "end": 1036.96, "text": " the medium size ideas, the small ideas,", "tokens": [50664, 264, 6399, 2744, 3487, 11, 264, 1359, 3487, 11, 50776], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 379, "seek": 102872, "start": 1036.96, "end": 1038.96, "text": " and the careful execution on them", "tokens": [50776, 293, 264, 5026, 15058, 322, 552, 50876], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 380, "seek": 102872, "start": 1038.96, "end": 1041.52, "text": " that it takes to get from here to superintelligence,", "tokens": [50876, 300, 309, 2516, 281, 483, 490, 510, 281, 1687, 20761, 17644, 11, 51004], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 381, "seek": 102872, "start": 1041.52, "end": 1042.72, "text": " that's what our moat is.", "tokens": [51004, 300, 311, 437, 527, 705, 267, 307, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 382, "seek": 102872, "start": 1042.72, "end": 1045.52, "text": " Anyway, this video could have been at least three times longer.", "tokens": [51064, 5684, 11, 341, 960, 727, 362, 668, 412, 1935, 1045, 1413, 2854, 13, 51204], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 383, "seek": 102872, "start": 1045.52, "end": 1048.4, "text": " There was so much I had to edit out for brevity.", "tokens": [51204, 821, 390, 370, 709, 286, 632, 281, 8129, 484, 337, 1403, 23110, 13, 51348], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 384, "seek": 102872, "start": 1048.4, "end": 1051.3600000000001, "text": " If you're interested in me talking more about open source models,", "tokens": [51348, 759, 291, 434, 3102, 294, 385, 1417, 544, 466, 1269, 4009, 5245, 11, 51496], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 385, "seek": 102872, "start": 1051.3600000000001, "end": 1052.96, "text": " do let me know in the comments.", "tokens": [51496, 360, 718, 385, 458, 294, 264, 3053, 13, 51576], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 386, "seek": 102872, "start": 1052.96, "end": 1054.56, "text": " I've got much more to say.", "tokens": [51576, 286, 600, 658, 709, 544, 281, 584, 13, 51656], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 387, "seek": 102872, "start": 1054.56, "end": 1057.04, "text": " As always, thank you so much for watching to the end", "tokens": [51656, 1018, 1009, 11, 1309, 291, 370, 709, 337, 1976, 281, 264, 917, 51780], "temperature": 0.0, "avg_logprob": -0.08849600977545617, "compression_ratio": 1.6948051948051948, "no_speech_prob": 0.0004727774066850543}, {"id": 388, "seek": 105704, "start": 1057.04, "end": 1059.04, "text": " and have a wonderful day.", "tokens": [50364, 293, 362, 257, 3715, 786, 13, 50464], "temperature": 0.0, "avg_logprob": -0.35286712646484375, "compression_ratio": 0.7575757575757576, "no_speech_prob": 0.3060861825942993}], "language": "en"}