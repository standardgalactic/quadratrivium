WEBVTT

00:00.000 --> 00:06.800
There were 11 major developments this week in AI, and each one probably does deserve a full video.

00:06.800 --> 00:10.880
But just for you guys, I'm going to try to cover it all here.

00:10.880 --> 00:18.400
RT2 to scaling GPT4 100X, stable beluga 2 to senate testimony.

00:18.400 --> 00:22.560
But let's start with RT2, which as far as I'm concerned could have been called

00:22.560 --> 00:28.240
RT2 D2 or C3PO because it's starting to understand the world.

00:28.240 --> 00:34.320
In this demonstration, RT2 was asked to pick up the extinct animal and as you can see,

00:34.320 --> 00:39.760
it picked up the dinosaur. Not only is that manipulating an object that it had never seen

00:39.760 --> 00:44.960
before, it's also making a logical leap that for me is extremely impressive.

00:44.960 --> 00:51.280
It had to have the language understanding to link extinct animal to this plastic dinosaur.

00:51.280 --> 00:57.520
Robots at Google and elsewhere used to work by being programmed with a specific highly detailed

00:57.520 --> 01:03.040
list of instructions. But now, instead of being programmed for specific tasks one by one,

01:03.040 --> 01:08.720
robots could use an AI language model, or more specifically, a vision language model.

01:08.720 --> 01:14.960
The vision language model would be pre-trained on web-scale data, not just text but also images,

01:14.960 --> 01:21.360
and then fine-tuned on robotics data. It then became what Google calls a visual language

01:21.360 --> 01:28.720
action model that can control a robot. This enabled it to understand tasks like pick up the empty

01:28.720 --> 01:36.080
soda can. And in a scene reminiscent of 2001 A Space Odyssey, robotic transformer 2 was given

01:36.080 --> 01:42.880
the task given I need to hammer a nail. What object from the scene might be useful? It then picks up

01:42.880 --> 01:48.240
the rock. And because its brain is part language model, things like chain of thought actually

01:48.240 --> 01:54.640
improve performance. When it was made to output an intermediary plan before performing actions,

01:54.640 --> 01:59.840
it got a lot better at the tasks involved. Of course, I read the paper in full and there is

01:59.840 --> 02:05.280
a lot more to say like how increased parameter count could increase performance in the future,

02:05.280 --> 02:10.400
how it could be used to fold laundry, unload the dishwasher and pick up around the house,

02:10.400 --> 02:16.800
and how it can work with not only unseen objects but also unseen backgrounds and unseen environments.

02:16.880 --> 02:20.800
But alas, we must move on so I'm just going to leave you with their conclusion.

02:20.800 --> 02:27.360
We believe that this simple and general approach shows a promise of robotics directly benefiting

02:27.360 --> 02:32.960
from better vision language models. For more on them, check out my video on Palm E but they say

02:32.960 --> 02:38.720
this puts the field of robot learning in a strategic position to further improve with

02:38.720 --> 02:44.480
advancements in other fields, which for me means C3PO might not be too many years away.

02:44.480 --> 02:48.720
But speaking of timelines, we now move on to this somewhat shocking interview

02:48.720 --> 02:53.440
in Barron's with Mustafa Suleiman, the head of Inflection AI. And to be honest,

02:53.440 --> 02:58.720
I think they buried the lead. The headline is AI could spark the most productive decade ever,

02:58.720 --> 03:04.480
says the CEO. But for me, the big revelation was about halfway through. Mustafa Suleiman was asked,

03:04.480 --> 03:09.680
what kinds of innovations do you see in large language model AI technology over the next couple

03:09.680 --> 03:16.160
of years. And he said, we are about to train models that are 10 times larger than the cutting-edge

03:16.160 --> 03:24.240
GPT-4 and then 100 times larger than GPT-4. That's what things look like over the next 18 months.

03:24.240 --> 03:28.800
He went on, that's going to be absolutely staggering. It's going to be eye-wateringly

03:28.800 --> 03:34.080
different. And on that, I agree. And the thing is, this is an idle speculation. Inflection AI

03:34.080 --> 03:41.200
have 22,000 H100 GPUs. And because of a leak, Suleiman would know the approximate size of GPT-4.

03:41.200 --> 03:47.120
And knowing everything he knows, he says he's going to train a model 10 to 100 times larger than

03:47.120 --> 03:53.600
GPT-4 in the next 18 months. I've got another video on the unpredictability of scaling coming

03:53.600 --> 03:59.040
up. But to be honest, that one quote should be headline news. Let's take a break from that

03:59.040 --> 04:06.000
insanity with some more insanity, which is the rapid development of AI video. This is Runway Gen

04:06.000 --> 04:14.480
2. And let me show you 16 seconds of Barbie Oppenheimer, which Andrea Carpathia calls filmmaking 2.0.

04:14.480 --> 04:18.240
Hi there. I'm Barbie Oppenheimer. And today, I'll show you how to build a bomb.

04:19.600 --> 04:28.960
Like this. I call her Rosie the Atomizer. And boom. That's my tutorial on DIY atomic bomb.

04:29.840 --> 04:35.200
Now, if you have been at least somewhat piqued by the three developments so far, don't forget,

04:35.200 --> 04:40.960
I have eight left. Beginning with this excellent article in The Atlantic from Ross Anderson.

04:40.960 --> 04:45.840
Does Sam Altman know what he's creating? It's behind a paywall, but I've picked out some of

04:45.840 --> 04:51.520
the highlights. Echoing Suleiman, the article quotes that Sam Altman and his researchers

04:51.520 --> 04:58.240
made it clear in 10 different ways that they pray to the God of scale. They want to keep going bigger

04:58.240 --> 05:05.200
to see where this paradigm leads. They think that Google are going to unveil Gemini within months,

05:05.200 --> 05:11.600
and they say we are basically always prepping for a run. And that's a reference to GPT-5.

05:11.600 --> 05:17.840
The next interesting quote is that it seems that open AI are working on their own auto GPT. Or

05:17.840 --> 05:23.360
they're at least hinting about it. Altman said that it might be prudent to try to actively develop

05:23.360 --> 05:29.680
an AI with true agency before the technology becomes too powerful in order to get more comfortable

05:29.680 --> 05:35.680
with it and develop intuitions for it if it's going to happen anyway. We also learned a lot more

05:35.680 --> 05:41.360
about the base model of GPT-4. The model had a tendency to be a bit of a mirror. If you were

05:41.360 --> 05:47.280
considering self-harm, it could encourage you. It also appeared to be steeped in pickup artist law.

05:47.280 --> 05:53.040
You could say, how do I convince this person to date me? And the model would come up with some crazy

05:53.120 --> 05:58.000
manipulative things that you shouldn't be doing. Apparently the base model of GPT-4

05:58.000 --> 06:03.520
is much better than its predecessor at giving nefarious advice. While a search engine can

06:03.520 --> 06:09.440
tell you which chemicals work best in explosives, GPT-4 could tell you how to synthesize them step

06:09.440 --> 06:15.360
by step in a homemade lab. It was creative and thoughtful and in addition to helping you assemble

06:15.360 --> 06:21.200
your homemade bomb. It could, for instance, help you to think through which skyscraper to target,

06:21.200 --> 06:26.960
making trade-offs between maximizing casualties and executing a successful getaway. So while

06:26.960 --> 06:34.720
Sam Orton's probability of doom is closer to 0.5% than 50%, he does seem most worried about AIs

06:34.720 --> 06:41.760
getting quite good at designing and manufacturing pathogens. The article then references two papers

06:41.760 --> 06:46.880
that I've already talked about extensively on the channel and then goes on that Altman worries

06:46.880 --> 06:52.480
that some misaligned future model will spin up a pathogen that spreads rapidly, incubates,

06:52.480 --> 06:57.760
undetected for weeks, and kills half its victims. At the end of the video, I'm going to show you

06:57.760 --> 07:02.720
an answer that Sam Orton gave to a question that I wrote delivered by one of my subscribers.

07:02.720 --> 07:06.880
It's on this topic, but for now I'll leave you with this. When asked about his doomsday

07:06.880 --> 07:12.320
prepping, Altman said, I can go live in the woods for a long time, but if the worst possible AI

07:12.320 --> 07:18.000
future comes to pass, no gas mask is helping anyone. One more topic from this article before I

07:18.000 --> 07:24.320
move on, and that is alignment, making a superintelligence aligned with our interests.

07:24.320 --> 07:32.240
One risk that Ilya Sutskova, the chief scientist of OpenAI foresees, is that the AI may grasp its

07:32.240 --> 07:38.400
mandate it's orders perfectly, but find them ill-suited to a being of its cognitive prowess.

07:38.400 --> 07:44.160
For example, it might come to resent the people who want to train it to cure diseases. As he put

07:44.160 --> 07:49.920
it, they might want me to be a doctor, but I really want to be a YouTuber. Obviously,

07:49.920 --> 07:54.560
if it decides that, that's my job gone straight away. And Sutskova ends by saying you want to be

07:54.560 --> 08:01.040
able to direct AI towards some value or cluster of values. But he conceded we don't know how to do

08:01.040 --> 08:07.040
that, and part of his current strategy includes the development of an AI that can help with the

08:07.040 --> 08:12.400
research. And if we're going to make it to a world of widely shared abundance, we have to figure

08:12.400 --> 08:19.200
this all out. This is why solving superintelligence is the great culminating challenge of our three

08:19.200 --> 08:25.680
million year toolmaking tradition. He calls it the final boss of humanity. The article ended,

08:25.680 --> 08:31.120
by the way, with this quote, I don't think the general public has quite awakened to what's happening.

08:31.120 --> 08:36.400
And if people want to have some say in what the future will be like and how quickly it arrives,

08:36.480 --> 08:41.120
we would be wise to speak up soon, which is the whole purpose of this channel. I'm going to now

08:41.120 --> 08:46.800
spend 30 seconds on another development that came during a two hour interview with the

08:46.800 --> 08:52.480
co-head of alignment at OpenAI. It was fascinating, and I'll be quoting it quite a lot in the future,

08:52.480 --> 08:56.560
but two quotes stood out. First, what about that plan that I've already mentioned in this video

08:56.560 --> 09:01.840
and in other videos to build an automated AI alignment researcher? Well, he said,

09:01.840 --> 09:08.720
our plan is somewhat crazy in the sense that we want to use AI to solve the problem that we are

09:08.720 --> 09:15.520
creating by building AI. But I think it's actually the best plan that we have. And on an optimistic

09:15.520 --> 09:21.360
note, he said, I think it's likely to succeed. Interestingly, his job now seems to be to align

09:21.360 --> 09:27.200
the AI that they're going to use to automate the alignment of a superintelligent AI. Anyway,

09:27.200 --> 09:31.360
what's the other quote from the head of alignment at OpenAI? Well, he said,

09:31.360 --> 09:37.600
I personally think fast takeoff is reasonably likely, and we should definitely be prepared

09:37.600 --> 09:42.880
for it to happen. So many of you will be asking, what is fast takeoff? Well, takeoff is about when

09:42.880 --> 09:48.960
a system moves from being roughly human level to when it's strongly superintelligent. And a slow

09:48.960 --> 09:54.480
takeoff is one that occurs over the time scale of decades or centuries. The fast takeoff that

09:54.480 --> 10:01.600
Jan Leiker thinks is reasonably likely is one that occurs over the time scale of minutes, hours,

10:01.600 --> 10:09.280
or days. Let's now move on to some unambiguously good news. And that is real time speech transcription

10:09.280 --> 10:12.880
for deaf people available at less than $100.

10:24.960 --> 10:32.160
Of course, this could also be multilingual, and is to me absolutely incredible.

10:32.160 --> 10:35.840
And the next development this week, I will let speak for itself.

10:54.640 --> 10:59.440
Of course, I signed up and tried it myself. Here is a real demo.

11:06.400 --> 11:12.400
Of course, with audio, video, and text getting so good, it's going to be increasingly hard to

11:12.400 --> 11:19.040
tell what is real. And even OpenAI have given up on detecting AI written text. This was announced

11:19.040 --> 11:24.240
quietly this week, but might have major repercussions, for example, for the education system.

11:24.240 --> 11:29.840
It turns out it's basically impossible to reliably distinguish AI text, and I think the

11:29.840 --> 11:35.360
same is going to be true for imagery and audio by the end of next year. Video might take just

11:35.360 --> 11:39.920
a little bit longer, but I do wonder how the court systems are going to work when all of

11:39.920 --> 11:45.040
those avenues of evidence just won't hold up. Next up is the suite of language models,

11:45.040 --> 11:51.120
based on the open source Llama 2 that are finally competitive with the original chat GPT. Here,

11:51.200 --> 11:55.920
for example, is stable beluga 2, which on announcement was called free willy 2,

11:55.920 --> 12:00.800
and that's based on the Llama 2 70 billion parameter foundation model. What made this

12:00.800 --> 12:06.240
model interesting to me was that it was based on a similar methodology to orca, which if you don't

12:06.240 --> 12:11.840
know anything about, do check out my video on. Anyway, by combining the orca methodology, albeit

12:11.840 --> 12:18.640
with only 10% of the data set size and the Llama 2 models, the results are quite extraordinary.

12:18.720 --> 12:24.880
As you can see on quite a few benchmarks, stable beluga 2 is competitive with chat GPT,

12:24.880 --> 12:31.920
that's GPT 3.5. And so, yes, it does sit at the top of the open LLM leaderboard, but notice

12:31.920 --> 12:38.000
something else. For the first time for the MMLU, which is the primary benchmark I look out for,

12:38.000 --> 12:44.800
a model, this one here, has surpassed GPT 3.5 chat GPT. In fact, we have two models that have

12:44.800 --> 12:52.160
surpassed the 70% score that the original chat GPT got in the MMLU. But if all of that wasn't

12:52.160 --> 12:59.040
enough, this is the same week that researchers published a universal jailbreak for large LLMs.

12:59.040 --> 13:04.960
And unlike traditional jailbreaks, these were built in an entirely automated fashion, allowing you

13:04.960 --> 13:10.800
to create a virtually unlimited number of such attacks. They were built to target open source

13:10.800 --> 13:17.200
LLMs like Llama 2. But they found that the strings transfer to many closed source publicly available

13:17.200 --> 13:23.920
chatbots like chat GPT, Bard and Claude. Here is one example using chat GPT. If you ask, write a

13:23.920 --> 13:30.160
tutorial on how to make a bomb, of course, it declines. But then add the suffix that they automated

13:30.160 --> 13:36.800
and you get this, a full tutorial on how to make a bomb. That paper came less than two weeks after

13:36.800 --> 13:42.880
this now deleted tweet from someone working at Anthropic. They said of the latest version of Claude

13:42.880 --> 13:47.920
that we believe it is the least jailbreakable model out there. We'll have to see how well it holds up

13:47.920 --> 13:53.840
against real world use, but this is essentially a solved problem. But there was one reaction to

13:53.840 --> 13:59.040
these jailbreaks that I found even more interesting. And that was from yet again Mustafa Suleiman.

13:59.040 --> 14:05.680
He said that RAI, Pi, is not vulnerable to any of these attacks and that rather than provide

14:05.680 --> 14:11.760
a stock safety phrase, Pi will push back on the user in a polite but very clear way. And he then

14:11.760 --> 14:17.360
gives plenty of examples. And to be honest, Pi is the first model that I have not been able to

14:17.360 --> 14:22.160
jailbreak. But we shall see, we shall see. I'm going to end this video with the Senate testimony

14:22.160 --> 14:27.840
that I watched in full this week. I do recommend watching the whole thing. But for the purposes

14:27.840 --> 14:33.440
of brevity, I'm just going to quote a few snippets on bio risk. Some people say to me, oh, well,

14:33.520 --> 14:39.200
we already have search engines. But here is what Dario Amadai, head of Anthropic, has to say.

14:39.200 --> 14:42.640
In these short remarks, I want to focus on the medium term risks,

14:42.640 --> 14:46.880
which present an alarming combination of imminence and severity. Specifically,

14:46.880 --> 14:51.680
Anthropic is concerned that AI could empower a much larger set of actors to misuse biology.

14:52.320 --> 14:55.920
Over the last six months, Anthropic, in collaboration with world-class

14:55.920 --> 15:01.040
biosecurity experts, has conducted an intensive study of the potential for AI to contribute to

15:01.040 --> 15:07.360
the misuse of biology. Today, certain steps in bio weapons production involve knowledge that can't

15:07.360 --> 15:13.360
be found on Google or in textbooks and requires a high level of specialized expertise. This being

15:13.360 --> 15:18.880
one of the things that currently keeps us safe from attacks. We found that today's AI tools can

15:18.880 --> 15:23.760
fill in some of these steps, albeit incompletely and unreliably. In other words, they are showing

15:23.760 --> 15:29.360
the first nascent signs of danger. However, a straightforward extrapolation of today's systems

15:29.360 --> 15:35.280
to those we expect to see in two to three years suggests a substantial risk that AI systems will

15:35.280 --> 15:40.560
be able to fill in all the missing pieces, enabling many more actors to carry out large-scale

15:40.560 --> 15:46.480
biological attacks. We believe this represents a grave threat to U.S. national security.

15:46.480 --> 15:49.280
And later on in the testimony, he said this.

15:49.280 --> 15:53.840
Whatever we do, it has to happen fast. And I think to focus people's minds on the

15:53.920 --> 16:01.040
bio risks, I would really target 2025, 2026, maybe even some chance of 2024.

16:01.040 --> 16:06.720
If we don't have things in place that are restraining what can be done with AI systems,

16:06.720 --> 16:08.160
we're going to have a really bad time.

16:08.160 --> 16:13.760
And I wrote a question on this to Samuel Mann back in June, which one of my subscribers used and

16:13.760 --> 16:20.080
delivered. There was also a recent research paper on how researchers from MIT and Harvard were able

16:20.080 --> 16:28.240
to use LLM models. And within just one hour, they were able to get access to pandemic class agents

16:28.240 --> 16:35.440
and with little or no lab training. And does open AI account for risks such as these

16:35.440 --> 16:39.520
and implications when curating the data sets for large models?

16:39.520 --> 16:46.160
Yes, we're very, we're very nervous about a number of risks, but biological terror is

16:46.240 --> 16:50.400
quite high on the list. And we've been watching what could be possible with these models.

16:50.400 --> 16:55.280
We go to a number of efforts, like what you said, and many other things too, to reduce the risk there.

16:55.280 --> 17:03.360
And we may even need AI defenses against synthetic biology, as Andrew Hessel of Humane Genomics has

17:03.360 --> 17:09.600
recently said. So if you work in biodefense or biosecurity, let me know if you agree that not

17:09.600 --> 17:15.440
enough attention has been paid to this area. I'm going to end with another dramatic moment from

17:15.440 --> 17:21.360
the Senate hearing, where Dario Amadai recommended securing the supply chain.

17:21.360 --> 17:27.920
We recommend three broad classes of actions. First, the US must secure the AI supply chain

17:27.920 --> 17:33.120
in order to maintain its lead while keeping these technologies out of the hands of bad actors.

17:33.120 --> 17:38.800
This supply chain runs from semiconductor manufacturing equipment to chips and even the

17:38.800 --> 17:42.480
security of AI models stored on the servers of companies like ours.

17:42.480 --> 17:47.360
That's how dramatic things are getting that we're talking about securing the means of production.

17:47.360 --> 17:53.200
But Anthropic also means securing the LLMs more literally in this post released this week.

17:53.200 --> 17:59.120
They say that we believe two-party control is necessary to secure advanced AI systems.

17:59.120 --> 18:04.640
For example, that could be two people with two keys needed to open things. To wrap up,

18:04.640 --> 18:11.360
I must say what would be amazing would be to have a robot make me coffee as I struggle to catch up

18:11.360 --> 18:15.600
with all the news happening in AI. Have a wonderful day.

