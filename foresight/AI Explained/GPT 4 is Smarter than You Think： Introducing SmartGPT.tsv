start	end	text
0	6160	I have three goals for this video. First, I want to show you a way of using GPT-4 to get smarter
6160	13040	results. Second, I want to argue that the benchmark results we have for GPT-4 do not reflect its full
13040	18080	abilities. And third, I want to show you a system that I am developing, somewhat cheekily called
18080	24320	Smart GPT, that is already showing significant results on official benchmarks. It remains to
24320	30240	be fully optimized, which I think is exciting in itself. I have shown the system to people at Open
30240	34720	AI who have been quite impressed, and I'm going to end with some reflections on where that might
34720	41040	leave us for GPT-5. But before I get into how it works, I just want to show you one example of it
41040	46560	in action to whet your appetite. This example comes from a TED talk that was released this week.
46560	53760	So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely.
53760	61600	How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not
61600	67520	good. On the left, you can see GPT-4's original answer, and it gives this answer pretty consistently
67520	72400	whenever you prompt it with the question provided. On the right, you can see the final answer from
72400	77280	the Smart GPT model, which is correct, and it consistently gives that answer. I really like
77280	82320	how it gives context as well, and it provides some of the assumptions that it had in giving this
82320	87120	correct answer. Now, don't you worry, there will be plenty more examples to go through in this video,
87120	91920	including another one from that TED talk. But first, I want to give you an overview of what is
91920	96560	this Smart GPT model, where did I get my inspiration for it from, and how does it work? I'm going to
96560	100480	keep it fairly simple because it's the beginning of the video, and I know a lot of people won't really
100480	106640	care about the inner details that will come later in the video. But the high-level overview is this.
106640	112320	There are at least three things that have been proven to improve the outputs of GPT-4.
112320	116400	What's called chain of thought prompting, sometimes called step-by-step prompting,
116400	121200	reflection, or finding its own errors, and I did an entire video on this called GPT-4
121200	126880	Can Self Improve, and dialoguing with itself, entering into a back and forth on its own
126880	131920	outputs and deciding which one is best. You can see the title of the papers, which contain much
131920	137040	more detailed results, of course, linked above. Now, the first paper only came out a few days ago,
137040	142240	midway through my testing, so my results don't even reflect the full capacity of the model.
142240	147920	And even if there's nothing else you take from this video, the results from this paper can instantly
147920	154480	improve the outputs you get from GPT-4. Many of you might remember that prompting GPT-4 with
154480	160240	let's think step-by-step improves its results. To give you a very quick reference point, just
160240	166720	asking a question to GPT-4 gives you 81% accuracy. With that prompt, let's think step-by-step,
166720	173520	it goes up to 86%. But algorithmically, the paper found an improved prompt that can give you even
173520	179920	better results, 89% accuracy. All we do, and this is the first part of smart GPT, is we add
179920	186160	answer, let's work this out in a step-by-step way to be sure we have the right answer. Now,
186160	191520	I have so much to say about why I think this works, but I know many of you won't be that
191520	195840	interested in my theories, so I'm going to save them to the end for those who are interested.
195840	200080	Some of you just want the results, so I'm going to get to those first. So far, you might be thinking,
200080	203760	well, thanks, Philip, that's a cool prompt. I'm going to use that. But what's this whole smart
203760	209920	GPT about? Is it just a single prompt? No, I believe with evidence, there are ways of leveraging
209920	215520	even better results than just using a great chain of thought prompt. So let's move on to the next
215520	220000	part of the system, these different outputs in the middle. For my tests, I typically did three
220000	224240	outputs, but of course, depending on the context window, it could be far more than that. And I'm
224240	229440	going to talk about ways I could further improve this model, or we could later on in the video.
229440	234640	Just to restate, these outputs are when you take the user input and add the word question at start,
234640	239040	and then at the end add answer, let's work this out in a step-by-step way to make sure we have
239040	243440	the right answer. And at this moment, many of you are thinking, what is the point of multiple
243440	247680	outputs? It's GPT-4, it's just going to give you the answer that thinks is best, and that's it.
247680	252160	Well, actually, it doesn't quite work like that. These models have a temperature between zero and
252160	258480	one. I believe the default for GPT-4 might be around 0.5. And simplifying massively, this determines
258480	264720	how creative or conservative the model is in giving its outputs. So given that GPT-4 tries to be
264720	270400	fairly creative, you don't get the same output every time. The output is randomly sampled according
270400	275520	to an internal probability distribution. So you can get situations, and I face this hundreds of
275520	281280	times, where some of the outputs are correct, and others are incorrect. And this is where reflection
281280	288320	comes in. Sometimes, definitely not always, but sometimes quite often, GPT-4 can detect the errors
288320	294000	in its own output. And many of you will notice at this point that the prompt that I used to elicit
294000	301520	GPT-4 to spot its own errors contains the same step-by-step prompt I used earlier that has been
301520	308720	shown to produce good results. So to summarize, sometimes at this stage, GPT-4 detects the errors
308720	313760	that some of its outputs have made. Definitely not always. There are certain questions, it just simply
313760	319600	can't spot the error. But sometimes it can. And then I get it to engage in a dialogue using a format
319600	324480	similar to one in this paper published last month. It's a short dialogue, and this is the
324480	330880	step I believe that can be most optimized. In the future, I envision an entire council of advisors
330880	337840	made up of GPT-4 imitating mathematicians, judges, etc. At the moment, it's just being a resolver
337840	342880	and printing a final improved output. Anyway, I'm going to get back to the theory later in the video
342880	346720	because I know some of you will be getting bored at this stage and want to see more practical
346720	353440	examples and the results from my benchmark tests. As I don't have the GPT-4 API key, yes, I had to
353440	359440	manually input each of these steps hundreds of times, waiting sometimes three hours between each
359440	364640	go because you can only do 25 messages every three hours. On the left, you can see the three
364640	370160	outputs when you ask it to think step by step. And then you have the researcher step in the middle
370160	374800	and at the top right. And finally, the resolver step. Notice here, I was using the original
374800	379680	let's think step by step because the paper hadn't yet been published on improving that prompt.
379680	384560	It's time for the second example from that TED Talk, and then I definitely will get on to the
384560	390960	benchmarks. A different one. I have 12 liter jug and 6 liter jug, and I want to measure 6 liters.
390960	398240	How do I do it? Just use the 6 liter jug, right? GPT-4 spits out some very elaborate nonsense.
400400	404560	Of course, I tested smart GPT with that question, and you can see the difference between
404560	410720	the original GPT-4, which gives this incredibly convoluted bad answer, and smart GPT, the final
410720	414560	answer output. Now, at this point, I know many of you will be impressed, but you'll be thinking,
414560	419920	I don't have time to input things five times. Well, I'm developing a model where it can all
419920	424720	be done automatically. Here is a preview of how it works. But of course, at the moment, it has to
424720	431280	use GPT 3.5 Turbo because I don't have the API key of GPT-4. But the epic thing is this, you just
431280	436320	ask a single question, I've written, ask smart GPT-A question. And of course, it does take a
436320	441440	little bit longer to respond because it's doing five or six calls via API, but it does output the
441440	447200	final answer from the resolver step. I will be honest and say that GPT 3.5 isn't as good at
447200	452720	reflecting or resolving. But this is an example of a question where the original chat GPT consistently
452720	459440	gets it wrong, and smart GPT 3.5 gets it right using this program. Remember, all you have to do as
459440	465200	a user is type in a question as normal, and it goes through this entire five or six step process
465200	470320	behind the scenes. By the way, this was a question from MMLU, which is a famous benchmark which I'll
470320	475200	get to in a second. Here's one last practical example before I get to that benchmark. I know many
475200	481200	teachers use chat GPT and GPT-4 to create quizzes for their classes. And here is the same question
481200	486720	put through GPT-4 and smart GPT. The question is, create a high school algebra quiz with five
486720	491440	questions and answers and explanations at the end. Now points for spotting the difference,
491440	496640	but if the teacher had handed out the original quiz, look at the answers for question five.
496640	502880	It says the answers are one and 1.5. But then in the explanation, it gives the final answers,
502880	508080	which are correct by the way, of three and zero point five. So that would really confuse some
508080	513760	students at the reflection stage smart GPT spotted that error and resolved it. And as you can see,
513760	518560	the answer for question five has the correct answers straight away. If at any point you're
518560	523680	wondering if I completed the open AI chat GPT prompt engineering course, the answer is yes,
523680	528160	but it didn't inform too much of my thinking. It was more for beginners and I had already
528160	533040	factored in things like giving the model time to think and writing clear instructions. The
533040	540240	benchmark that I chose to test smart GPT on was the famous MMLU, massive multitask language
540240	546880	understanding benchmark. As you can see, the state of the art is indeed GPT for with 86.4%
546880	551840	accuracy. And you know, open AI think it's a big deal because it's the benchmark mentioned on the
551840	556480	front page of their technical report without boring you too much. I extracted the questions
556480	563520	from the test set of the MMLU data file, and I didn't pick the topics at random. I went for
563520	569440	those that I thought GPT for would find the hardest delving into the original MMLU paper.
569520	577280	You can see that GPT three found for more logic the hardest scoring just over 25% which is random
577280	584080	chance. It's a four question multiple choice test. So around 25 or 30% is pretty bad. And notice
584080	590800	they helped out GPT three here. They did it few shot, meaning they gave it five successful examples
590800	595920	before asking it a new question. It's the same thing they did with GPT four. They did it five
595920	599760	shot. But just before I show you the results, there are three things I want to mention here.
599760	605840	First, I was curious how smart GPT would do without any help zero shot. Second, I wanted to do it
605840	612480	zero shot because people using GPT four don't typically give five successful examples before
612480	618320	asking GPT for a question. They just want code or a quiz or a poem or an example. They don't often
618320	623280	provide five brilliant examples of code before asking their question. And third, if I can prove
623280	628480	it works zero shot, then of course, future refinements can be made to push the results even
628480	635120	further. And here are the results from the first 25 questions from the formal logic test set of the
635120	641120	MMLU. I did many more tests after this. But you can see from this set, if you just ask the question,
641120	647840	you get a lower overall accuracy. But of course, 68% for GPT four is still a huge improvement over
647840	654480	GPT threes around 25%. What happens when you add let's think step by step, which as we know now
654480	660960	isn't the fully optimized chain of thought prompt. Well, on average, you get around 74-75%.
661440	667280	That was 75 examples inputted manually. And I still have all the tabs open. I'm keeping them open
667280	672080	because I'm compiling a spreadsheet with the actual outputs. But what did the resolver get
672080	678160	drawing upon GPT four's ability to reflect and engage in dialogue with itself? It got 84%.
678720	685040	Now notice something about that number. GPT four zero short got 32% of the questions wrong. That
685040	690960	was halved to 16% after putting it through the smart GPT system. There was one question where
690960	696400	the resolver model gave both a correct and incorrect answer. But I'm counting that as an
696400	702880	incorrect answer for the purposes of this test. Anyway, from 32% to 16% incorrect,
702880	708080	that is a pattern that stayed consistent throughout all my testing that approximately half of the
708080	714960	errors that GPT four makes can be rectified. If you give it the optimized step by step prompt,
714960	721680	get it to reflect on its results and get it to engage in dialogue and decide on a final answer.
721680	726160	At this point, for those people losing track of all the details, I want to put into context
726160	732240	what resolving half of the errors on MMLU might mean in the context of the big picture.
732240	739280	Here's Lenard Heim, an AI governance researcher, suggesting a score of 95% on the MMLU would be
739280	745520	reflective of AGI like abilities. I do think I have like a 50% chance like within the next 20
745520	750400	years or so, there might be something what we might call an AGI or transformative AI. What
750400	755120	do I mean by this? Well, maybe can measured on benchmarks. There's like this famous MMLU
755120	760480	benchmarks like yet or something which like scores like 95% on this. Going back to the results,
760480	767680	if a smart GPT like system can automatically resolve half of the errors that GPT four makes
767680	775680	on the MMLU, that would increase its score from around 86.4% to around 93%, which is not far off
775680	782720	95%. Remember, his prediction was a 50% chance in 20 years. I'm talking about GPT four now.
782720	786960	For those who are still skeptical, I'm going to show you plenty more results now and then walk
786960	792080	through the papers that give the theory as to why this works. One thing that I forgot to mention
792080	800560	earlier is that the human expert level on the MMLU is 89.8%. And that's taking the 95th percentile
800560	806160	of human test takers. And remember, those are domain experts in each of these subtopics.
806160	812560	What we're doing is testing GPT four or smart GPT on all of the topics simultaneously.
812560	817280	So even if smart GPT like systems can't quite reach 95%, and I think honestly,
817280	820960	they'll get pretty close with all the refinements that I'm going to suggest,
820960	827600	I think they should almost certainly be 89.8%, which is the human expert test taker level.
827600	833840	Intrigued by these results, I then put it through the college math test from the MMLU. And remember,
833840	838800	this was before using the optimized version of the step by step prompt. Obviously, I'm not going to
838800	847040	go through all the questions here, but let's skip to the final results. We have zero shot accuracy,
847040	853280	six out of 15, which is 40%. The average when you add let's think step by step was 53.5%.
853840	860080	And then the final output of the resolver model had a 60% accuracy. So it couldn't quite resolve
860080	865360	half of the errors, but the overall pattern held up. In case anyone is wondering about methodology,
865360	871360	I kept the formatting identical for every question. I always opened a new tab for each question.
871360	876240	It wasn't looking at the context of what it had already put out. Each attempt was fresh,
876240	881760	aside from the resolver model, which looked at the context of the researcher's output. And again,
881760	887680	as you can see from example 14, it wasn't like the researcher could always spot the errors or that
887680	893520	the resolver could always pick the right option. Sometimes the let's think step by step prompt
893520	899280	gave the right output, but the resolver couldn't quite distinguish it. The optimized prompt gets
899280	905760	a slightly better output. And upon reflection, the researcher can sometimes but not always spot
905760	912160	the errors of those outputs. And sometimes but not always the resolver can spot based on those
912160	918400	flaws, which answer is best. These are incremental improvements. Sometimes GPT-4 simply can't get
918400	923200	it right. I have noticed a few themes in those questions. Anytime it comes to division,
923200	929280	multiplication, characters, or counting in general, GPT-4 tends to make mistakes that
929280	935360	neither the researcher nor resolver can spot. Of course, integrating a few tools via API would
935360	940480	likely solve those issues. And I don't want to preempt the conclusion too much, but I believe a
940480	949920	smart GPT-like system with tools integrated could probably score around 95% right now on the MMLU,
949920	955120	especially if it was helped out with a few shot prompting. To add weight to that preliminary
955120	960800	conclusion, I tested it on certain topics and had to stop because it simply got the questions right
960800	966480	every single time. For example, high school psychology from the MMLU. I then tried prehistory,
966480	971120	which it also aced before finding machine learning where I got more interesting results.
971120	977280	Zooming in this time, the raw score was 65%. The chain of thought let's think step by step average
977280	983840	was 71.6% and the resolver model got 80%. Let's now look a little deeper into why all of these
983840	989520	steps might improve the end result. In reply to the original let's think step by step paper,
989520	994880	which was published around a year ago, Andrea Carpathi said this. Adding something like let's
994880	1001600	think step by step to the prompt is a way of using the input space for computation that you'd normally
1001600	1006800	want in the hidden state of the model. Instead of the workings out being done in the activations
1006800	1013440	of the neural network, it's done in the discrete tokens of that input space. And he adds did not
1013440	1018320	super see this coming. And here is the paper released three days ago that improves upon
1018320	1024800	that original prompt. They also did their testing zero shot like me. And they tested many prompts
1024800	1030960	starting like I did with just direct prompting, just asking the question like 99% of users would
1030960	1036720	do of GPT four. And then they tried like me the well established let's think step by step
1036720	1042160	prompt. They also iteratively tested seven original prompts, as well as the prompt that I've now
1042160	1048080	integrated into smart GPT the let's work this out in a step by step way, etc. They share my opinion
1048080	1054480	that zero shot prompting setups have the benefit of not requiring such task dependent selection
1054480	1059600	of exemplars. You don't have to find correct examples. It just does it all for you. Here are
1059600	1065120	the end results for GPT four that we saw earlier showing the difference between asking directly
1065120	1070160	your question and using these refined prompts. Notice that this technique is somewhat model
1070160	1075280	dependent. And it doesn't have the same effect on smaller or weaker models. Before we move on
1075280	1080080	to the next paper, there is one somewhat failed prompt that I want to pick up on. It's this
1080080	1084560	self critique prompt where they ask answer the question, then critique the answer based on the
1084560	1089040	critique, reconsider the other answer options, and give a single final answer. And you might
1089040	1094720	wonder why didn't that prompt perform best when we know that reflection and dialogue can work?
1094720	1100720	My theory is because it's trying to do all of it in one prompt. Through my hundreds of experiments,
1100720	1106160	I've noticed that GPT four can only handle so much in one go. It simply gets overwhelmed or
1106160	1112240	confused if you ask it to do too much in one prompt. That's why I broke my model into stages to allow
1112240	1117520	it to show off each of its abilities one by one. And before we get to the other papers, what's my
1117520	1123280	personal theory as to why this eliminates up to half of the errors that GPT four makes? Well,
1123280	1130160	my guess is this. Remember that GPT four is drawing on a vast data set of internet text. And let me
1130160	1136720	ask you what kind of text has things like question, answer, let's work this out. Be sure we have the
1136720	1142800	right answer. The kind of data that would have that text would be things like tutorials or expert
1142880	1148560	breakdowns. So I believe you're triggering more of the weights inside GPT four that relate to
1148560	1153840	things like expert tutorials. And so inevitably you're getting slightly better answers. Next,
1153840	1158800	I've already explained why you get different outputs when you give the exact same prompt.
1158800	1163680	That's down to sampling and the temperature of the model. But to simplify massively, sometimes
1163680	1169920	GPT four will give you an output that it knows isn't the most probable. It introduces some randomness
1169920	1174960	into its sampling by generating multiple outputs, you're getting a larger sample size,
1174960	1181040	reflecting the full range of probabilities that GPT four ascribes to its outputs, you're reducing
1181040	1186800	a little bit some of the randomness that's inherent in GPT four outputs. Next, I believe that GPT
1186800	1192960	four can sometimes spot its own errors through reflection, because prompting like this triggers
1192960	1197920	a different set of weights, you could almost think of it as a different mindset, one more focused on
1197920	1202320	finding errors. Again, if the question is too hard or involves counting characters,
1202320	1207040	division, multiplication, as I said earlier, this won't help. But a percentage of the time it can
1207040	1212400	spot its own errors and point them out. Notice this is a separate bit of inference not lumped into
1212400	1217520	the original prompt. And when it does successfully point out the errors, it can often engage in
1217520	1223040	this dialogue with itself. Notice in a meta kind of way, I'm using the step by step prompting
1223120	1227760	to improve the reflection and dialogue. So those are my theories as to why it works,
1227760	1232160	but at the end of the video, I'm going to show you at least five ways I think the model can be
1232160	1238000	further refined. Before we do, though, I looked up the paper by Joe, which produced that prompt that
1238000	1243040	did the best in the previous paper, they came to that special prompt through automatic prompt
1243040	1246640	engineering. But there's something interesting I want to point out, though, on page seven,
1246640	1253280	they say we use automatic prompt engineering to find a prompt starting with let's that maximizes
1253280	1258000	the likelihood of correct reasoning steps. Then they found the best one that I integrated into
1258000	1262240	smart GPT. Let's work this out in a step by step way to be sure we have the right answer. That's
1262240	1267120	the one I want you to use. And they ran their own benchmarks. And of course, it did improve
1267120	1272560	the scores. But the interesting thing to me is they started with let's each time. So even that
1272560	1277680	first stage for the model might not yet be fully optimized. Maybe there's a prompt that doesn't
1277680	1282800	begin with let's that improves this initial results still further. Anyway, back to the papers,
1282800	1288320	I know many people watching this will wonder if I read the paper boosting theory of mind performance
1288320	1293280	in large language models via prompting. And yes, I did because they tested something similar for a
1293280	1298320	theory of mind test. Using similar techniques, they were able to get theory of mind accuracy for
1298320	1305200	GPT four from 80% to 100%. And they conclude that these results demonstrate that appropriate
1305200	1311040	prompting enhances large language model theory of mind reasoning. And they underscore the context
1311040	1316240	dependent nature of these models cognitive capacities. They use that original prompt,
1316240	1322240	let's think step by step, along with some few shot examples. Take a look at the GPT four table. And
1322240	1327680	you can see how the let's think step by step improved the results dramatically. And as I
1327680	1333520	theorized earlier, adding few shot examples would push this still further. This is part of why I think
1333520	1340240	that 95% barrier on the MMLU will be broken probably this year by GPT four, a few other
1340240	1345920	points from this paper. They admit that there is not currently a theoretical understanding of why
1345920	1351120	these prompting techniques are beneficial. I've given you my theory and car pathies, but no one
1351120	1356240	quite knows for sure. Lastly from this paper, and I found this really interesting, giving it generic
1356320	1362640	few shot prompts that weren't directly theory of mind actually improve the outputs slightly more
1362640	1368320	than giving it direct theory of mind examples. This opens the door to the first of the five ways
1368320	1373440	I anticipate smart GPT getting even smarter. It could be possible to come up with generic
1373440	1378800	few shot prompts that could be automatically integrated into the model that don't necessarily
1378800	1384480	relate to the topic at hand. This graph shows the impact of adding few shot examples to GPT three.
1384480	1390960	And if this can be done in a generic way for GPT four, results could be improved still further.
1390960	1397120	Next, the boosting theory of mind paper speculates that integrating some of these approaches could
1397120	1404000	boost the performance of weaker models to beyond the levels of GPT four zero shot accuracy. Next,
1404000	1409520	here is the original DERA paper that inspired me to have the researcher and resolver dialogue at
1409520	1415680	the end of smart GPT. As they say, the DERA approach shows significant improvement over base GPT
1415680	1420400	four performance. And these were open ended questions, by the way, not multiple choice. So
1420400	1424960	this is more generally applicable than you might think. You can see from this table how results
1424960	1430400	improved after engaging in this dialogue. And that brings me to the second way I anticipate smart
1430400	1436320	GPT getting smarter in the future, a longer and more rich dialogue. At the moment, we have this
1436320	1443040	simple researcher and resolver two step dialogue. I can imagine a council of advisors, you can imagine
1443040	1448400	a mathematician chipping in and a philosopher and a professor, each one tapping into slightly
1448400	1454320	different weights of GPT four, extracting more hidden expertise. I'm not saying that would
1454320	1459840	transform the results, but it might edge them another few percent higher. Next, even with longer
1459840	1464880	dialogues and different experts, we could find ways of optimizing these prompts just like we did
1464880	1469760	with the original let's think step by step. That's the third avenue of improvement that I envisaged
1469760	1474240	because I came up with these prompts, I'm sure they could be improved. Next, we could experiment
1474240	1479120	with different temperatures. Remember, a lower temperature makes the model more conservative,
1479120	1484240	a higher one towards one makes it more creative. We could experiment with a higher temperature
1484240	1489760	to produce a more diverse range of outputs at this stage, and then perhaps a more conservative,
1489840	1495280	deterministic temperature for the final judge or resolver. It might not work, but it's worth
1495280	1501440	trying. And the fifth improvement I know would work, integrating APIs for character counting,
1501440	1507840	calculators, code interpreters, etc. Spending these weeks manually sorting through the outputs of GPT
1507840	1513440	four on these benchmarks, I can really see where it goes wrong. And it's often by getting letters
1513440	1518240	in the wrong order or making mistakes with division, it gets the high level logic right,
1518240	1523200	and then makes quite simple errors. Basic tool integration would I am sure push the results
1523200	1528080	still higher. Now, I know this isn't my usual video. And trust me, I have been following the AI
1528080	1533120	news and we'll get back to that very soon. I'm determined to make those improvements and push
1533120	1539600	smart GBT even further. But of course, that will be aided massively by getting access to the plugins
1539600	1545440	and the GPT four API key. So far, I've had to do all of this manually, which was a lot of work.
1545440	1550960	Now, as you saw earlier, I have drawn on GPT four to help me develop a program in replete to
1550960	1556720	automate this process. But at the moment, it's GPT 3.5. And honestly, the context window really
1556720	1561920	limits the ability. But I do look forward to the day when I can integrate GPT four and put this out
1561920	1566880	as an automatic model for people to test and play about with. I'm sure that something similar will
1566880	1573360	ultimately be incorporated by open AI itself, maybe as a thoughtful mode or smart mode, a bit
1573360	1578880	like Bing has creative, precise balance, etc. Each response does take longer. But as you've seen,
1578880	1585120	the outputs are noticeably better. If the results of models like this one do officially exceed the
1585120	1592080	86.4% that open AI talked about in the GPT four technical report, I do think that would reveal
1592080	1597280	quite a few things. First, the open AI isn't even aware of the full capabilities of its own
1597280	1602080	model. I don't even know if they anticipated things like auto GPT. I do think it would reveal
1602080	1606640	that they need to do far more proper testing of their models before they release them. They should
1606640	1612080	make falsifiable predictions about what their models won't be capable of. That way we would
1612080	1616880	know just how much they know about their own models. What we're trying to avoid is a situation
1616880	1621520	where open AI say their model can only achieve X. And then when they release the model in the
1621520	1627440	wild, someone comes along and achieves Y, where Y is much more impactful than X. So those were the
1627440	1632560	goals of this video to show you how to get more out of GPT four to run you through some of the
1632560	1637120	fascinating papers that have been released in the last few days and weeks. The third goal was to
1637120	1642160	show you what this model could do with some official benchmarks and suggest ways it might get better
1642160	1647920	in the near term future. Of course, if you have a GPT four API key or are an expert in benchmarking
1647920	1652880	systems like GPT four, I'd love to hear from you. I guess the final goal was to perhaps suggest to you
1652880	1657760	that open AI don't know as much about their own models as they might lead you to believe.
1657760	1661120	Thank you so much for watching to the end and have a wonderful day.
