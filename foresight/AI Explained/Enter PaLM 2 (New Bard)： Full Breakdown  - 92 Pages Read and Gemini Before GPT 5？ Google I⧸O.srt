1
00:00:00,000 --> 00:00:06,960
Less than 24 hours ago, Google released the Palm 2 technical report. I have read all 92 pages,

2
00:00:06,960 --> 00:00:12,560
watched the Palm 2 presentation, read the release notes and have already tested the model in a dozen

3
00:00:12,560 --> 00:00:19,200
ways. But before getting into it all, my four main takeaways are these. First, Palm 2 is competitive

4
00:00:19,200 --> 00:00:25,520
with GPT-4 and while it is probably less smart overall, it's better in certain ways and that

5
00:00:26,400 --> 00:00:31,360
Second, Google is saying very little about the data it used to train the model or about

6
00:00:31,360 --> 00:00:36,720
parameters or about compute, although we can make educated guesses on each.

7
00:00:36,720 --> 00:00:43,280
Third, Gemini was announced to be in training and will likely rival GPT-5 while arriving earlier

8
00:00:43,280 --> 00:00:49,600
than GPT-5. As you probably know, Sam Orman said that GPT-5 isn't in training and won't be for a

9
00:00:49,600 --> 00:00:56,720
long time. Fourth, while dedicating 20 pages to bias, toxicity and misgendering, there wasn't a

10
00:00:56,720 --> 00:01:03,120
single page on AI impacts more broadly. Google boasted of giving Gemini planning abilities in a

11
00:01:03,120 --> 00:01:09,600
move that, surprise as I am to say it, makes open AI look like paragons of responsibility.

12
00:01:09,600 --> 00:01:16,080
So a lot to get to, but let's look at the first reason that Palm 2 is different from GPT-4. On

13
00:01:16,080 --> 00:01:21,680
page 3, they say we designed a more multilingual and diverse pre-training mixture, extending across

14
00:01:21,680 --> 00:01:26,960
hundreds of languages and domains like programming, mathematics, etc. So because the text that they

15
00:01:26,960 --> 00:01:32,800
train Palm 2 on is different to the text that open AI trained GPT-4 on, it means that those

16
00:01:32,800 --> 00:01:39,520
models have different abilities and I would say Palm 2 is better at translation and linguistics

17
00:01:39,520 --> 00:01:44,080
and in certain other areas which I'll get to shortly. If that's data, what about parameter

18
00:01:44,080 --> 00:01:50,160
count? Well, Google never actually say they only use words like it's significantly smaller than

19
00:01:50,160 --> 00:01:56,320
the largest Palm model, which was 540 billion parameters. Sometimes they say significantly,

20
00:01:56,320 --> 00:02:02,240
other times dramatically. Despite this, it significantly outperforms Palm on a variety

21
00:02:02,240 --> 00:02:07,440
of tasks. To all the references you may have seen to imminent 100 trillion parameter models

22
00:02:07,440 --> 00:02:14,240
were bogus. Skipping ahead to page 91 out of 92, in the model summary, they say further details of

23
00:02:14,240 --> 00:02:19,520
model size and architecture are withheld from external publication. But earlier on, they did

24
00:02:19,520 --> 00:02:25,040
seem to want to give hints about the parameter count inside Palm 2, which OpenAI never did. Here

25
00:02:25,040 --> 00:02:30,880
they present the optimal number of parameters given a certain amount of compute flops. Scaling

26
00:02:30,880 --> 00:02:36,560
this up to the estimated number of flops used to train Palm 2, that would give an optimal parameter

27
00:02:36,560 --> 00:02:43,840
count of between 100 and 200 billion. That is a comparable parameter count to GPT-3 while getting

28
00:02:43,840 --> 00:02:50,400
competitive performance with GPT-4. Bard is apparently now powered by Palm 2 and the inference

29
00:02:50,400 --> 00:02:56,560
speed is about 10 times faster than GPT-4 for the exact same prompt. And I know there are other

30
00:02:56,560 --> 00:03:02,160
factors that influence inference speed, but that would broadly fit with an order of magnitude

31
00:03:02,160 --> 00:03:07,520
fewer parameters. This has other implications, of course, and they say that Palm 2 is dramatically

32
00:03:07,520 --> 00:03:13,600
smaller, cheaper, and faster to serve. Not only that, Palm 2 itself comes in different sizes,

33
00:03:13,600 --> 00:03:19,760
as Sundar Pichai said. Palm 2 models deliver excellent foundational capabilities across a wide

34
00:03:19,760 --> 00:03:29,120
range of sizes. We have affectionately named them Gecko, Otter, Bison, and Unicon. Gecko is so

35
00:03:29,120 --> 00:03:35,360
lightweight that it can work on mobile devices fast enough for great interactive applications

36
00:03:35,360 --> 00:03:41,840
on-device, even when offline. I would expect Gecko to soon be inside the Google Pixel phones.

37
00:03:41,840 --> 00:03:47,760
Going back to data, Google cryptically said that their pre-training corpus is composed of a diverse

38
00:03:47,760 --> 00:03:54,480
set of sources, documents, books, code, mathematics, and conversational data. I've done a whole video

39
00:03:54,480 --> 00:03:59,600
on the data issues that these companies face, but suffice to say they're not saying anything about

40
00:03:59,600 --> 00:04:04,800
where the data comes from. Next, they don't go into detail, but they do say that Palm 2 was trained

41
00:04:04,800 --> 00:04:09,600
to increase the context length of a model significantly beyond that of Palm. As of today,

42
00:04:09,600 --> 00:04:14,720
you can input around 10,000 characters into BARD, but they end this paragraph with something a bit

43
00:04:14,720 --> 00:04:19,920
more interesting. They say, without demonstrating, our results show that it is possible to increase

44
00:04:20,000 --> 00:04:24,960
the context length of the model without hurting its performance on generic benchmarks.

45
00:04:24,960 --> 00:04:30,000
The bit about not hurting performance is interesting because in this experiment published a few weeks ago

46
00:04:30,000 --> 00:04:35,840
about extending the input size in tokens up to around 2 million tokens, the performance did drop

47
00:04:35,840 --> 00:04:41,840
off. If Google had found a way to increase the input size in tokens and not affect performance,

48
00:04:41,840 --> 00:04:47,200
that would be a breakthrough. On multilingual benchmarks, notice how the performance of Palm

49
00:04:47,200 --> 00:04:53,440
2 in English is not dramatically better than in other languages. In fact, in many other languages,

50
00:04:53,440 --> 00:04:58,800
it does better than in English. This is very different to GPT-4, which was noticeably better

51
00:04:58,800 --> 00:05:04,320
in English than in all other languages. As Google hinted earlier, this is likely due to the

52
00:05:04,320 --> 00:05:10,720
multilingual text data that Google trained Palm 2 with. In fact, on page 17, Google admit that the

53
00:05:10,720 --> 00:05:16,800
performance of Palm 2 exceeds Google Translate for certain languages, and they show on page 4 that

54
00:05:16,880 --> 00:05:23,040
it can pass the mastery exams across a range of languages like Chinese, Japanese, Italian,

55
00:05:23,040 --> 00:05:28,800
French, Spanish, German, etc. Look at the difference between Palm 2 and Palm in red.

56
00:05:28,800 --> 00:05:34,000
Now, before you rush off and try BARD in all of those languages, I tried that and apparently

57
00:05:34,000 --> 00:05:39,600
you can only use BARD at the moment in the following languages, English, US English, what a pity,

58
00:05:39,600 --> 00:05:47,440
and Japanese and Korean. But I was able to test BARD in Korean on a question translated via Google

59
00:05:47,440 --> 00:05:53,840
Translate from the MMLU dataset. It got the question right in each of its drafts. In contrast,

60
00:05:53,840 --> 00:06:00,480
GPT-4 not only got the question wrong in Korean, when I originally tested it for my smart GPT video,

61
00:06:00,480 --> 00:06:05,680
it got the question wrong in English. In case any of my regular viewers are wondering, I am working

62
00:06:05,680 --> 00:06:11,520
very hard on smart GPT to understand what it's capable of and getting it benchmarked officially,

63
00:06:11,520 --> 00:06:16,720
and thank you so much for all the kind offers of help in that regard. I must admit it was very

64
00:06:16,720 --> 00:06:24,160
interesting to see on page 14 a direct comparison between Palm 2 and GPT-4, and Google do admit

65
00:06:24,160 --> 00:06:30,000
for the Palm 2 results they use chain of thought prompting and self-consistency. Reading the

66
00:06:30,080 --> 00:06:35,520
self-consistency paper did remind me quite a lot actually of smart GPT, where it picks the

67
00:06:35,520 --> 00:06:41,760
most consistent answer of multiple outputs. So I do wonder if this comparison is totally fair

68
00:06:41,760 --> 00:06:46,800
if Palm 2 used this method and GPT-4 didn't. I'll have to talk about these benchmarks more

69
00:06:46,800 --> 00:06:52,160
in another video, otherwise this one would be too long, but a quick hint is that Wynogrand is

70
00:06:52,160 --> 00:06:58,320
about identifying what the pronoun in a sentence refers to. Google also weighed into the emerging

71
00:06:58,320 --> 00:07:04,160
abilities debate, saying that Palm 2 does indeed demonstrate new emerging abilities. They say it

72
00:07:04,160 --> 00:07:10,160
does so in things like multi-step arithmetic problems, temporal sequences, and hierarchical

73
00:07:10,160 --> 00:07:14,720
reasoning. Of course I'm going to test all of those and have begun to do so already, and in my

74
00:07:14,720 --> 00:07:20,000
early experiments I'm getting quite an interesting result. Palm 2 gets a lot of questions wrong that

75
00:07:20,000 --> 00:07:26,080
GPT-4 gets right, but it can also get questions right that GPT-4 gets wrong, and I must admit

76
00:07:26,080 --> 00:07:31,120
it's really weird to see Palm 2 getting really advanced college-level math questions right,

77
00:07:31,120 --> 00:07:36,960
that GPT-4 gets wrong. And yet also when I ask it a basic question about prime numbers, it gets

78
00:07:36,960 --> 00:07:41,920
it kind of hilariously wrong. Honestly I'm not certain what's going on there, but I do have my

79
00:07:41,920 --> 00:07:47,760
suspicions. Remember though that recent papers have claimed that emergent abilities are a mirage,

80
00:07:47,760 --> 00:07:54,320
so Google begs to differ. When Google put Palm 2 up against GPT-4 in high school mathematics problems,

81
00:07:54,320 --> 00:08:01,360
it did outperform GPT-4, but again it was using an advanced prompting strategy, not 100% different

82
00:08:01,360 --> 00:08:06,880
from smart GPT, so I wonder if the comparison is quite fair. What about coding? Well again it's

83
00:08:06,880 --> 00:08:12,880
really hard to find a direct comparison that's fair between the two models. Overall I would guess

84
00:08:12,880 --> 00:08:20,000
that the specialized coding model of Palm, what they call Palm 2S, is worse than GPT-4. It says

85
00:08:20,000 --> 00:08:27,840
it's pass at one accuracy, as in past first time, is 37.6%. Remember the Sparks of AGI paper? Well

86
00:08:27,840 --> 00:08:35,120
that gave GPT-4 as having an 82% zero shot pass at one accuracy level. However as I talked about in

87
00:08:35,120 --> 00:08:42,160
the Sparks of AGI video, the paper admits that it could be that GPT-4 has seen and memorized some

88
00:08:42,160 --> 00:08:47,840
or all of human eval. There is one thing I will give Google credit on, which is that their code

89
00:08:47,840 --> 00:08:53,280
now sometimes references where it came from. Here is a brief extract from the Google keynote

90
00:08:53,280 --> 00:08:59,760
presentation. How would I use Python to generate the Scholar's Mate move in chess? Okay here Bard

91
00:08:59,760 --> 00:09:05,600
created a script to recreate this chess move in Python, and notice how it also formatted the code

92
00:09:05,600 --> 00:09:11,200
nicely, making it easy to read. We've also heard great feedback from developers about how Bard

93
00:09:11,200 --> 00:09:17,280
provides code citations, and starting next week you'll notice something right here. We're making

94
00:09:17,280 --> 00:09:23,600
code citations even more precise. If Bard brings in a block of code, just click this annotation,

95
00:09:23,600 --> 00:09:28,960
and Bard will underline the block and link to the source. As always, it seems the appendix contained

96
00:09:28,960 --> 00:09:34,080
more interesting information sometimes than the main body of the technical report. For example,

97
00:09:34,080 --> 00:09:41,920
we get a direct and fair comparison between GPT-4 and palm 2, or I should say flan palm 2. That is

98
00:09:41,920 --> 00:09:47,440
the instruction fine tuned version of palm 2. Essentially that's the version where it's been

99
00:09:47,440 --> 00:09:52,560
fine tuned to get better at following a question and answer format. But anyway, the original palm

100
00:09:52,560 --> 00:10:02,480
2 scored 78.3%, and flan palm 2 scored 81.2%. That's below the 86.4% of GPT-4. And that's why my

101
00:10:02,480 --> 00:10:08,960
broad conclusion is that GPT-4 is a bit smarter than palm 2. But as I'll be showing over the

102
00:10:08,960 --> 00:10:15,840
coming days and weeks, there are genuinely quite a few areas in which palm 2 is better than GPT-4.

103
00:10:15,840 --> 00:10:20,400
What about the big bench, which was designed to be particularly tough for language models? I talked

104
00:10:20,400 --> 00:10:26,560
a lot about this in my earliest videos. Well, the graph is going to look pretty weird because palm 2

105
00:10:26,560 --> 00:10:33,200
has improved upon palm while reducing the number of parameters. So the graph kind of doubles back

106
00:10:33,280 --> 00:10:39,520
on itself back up here up to around 69% according to the technical report. I would say this is

107
00:10:39,520 --> 00:10:45,440
quite a major moment in human history. There is now virtually no language task that the average

108
00:10:45,440 --> 00:10:51,840
human can do better than palm 2. Of course, expert humans can do better in individual domains, but

109
00:10:51,840 --> 00:10:57,840
the average human is now worse in virtually every domain of language. Here you can see that confirmation

110
00:10:57,920 --> 00:11:04,640
of the big bench hard results for flan palm 2, 69.1%. Interestingly, in the original chart,

111
00:11:04,640 --> 00:11:11,440
palm 2 is even claimed to have higher performance than that at 78.1%. If you remember, the reason we

112
00:11:11,440 --> 00:11:17,440
can't compare that to GPT-4 is that in the technical report for GPT-4, they admit that during their

113
00:11:17,440 --> 00:11:23,040
contamination check, we discovered that portions of big bench were inadvertently mixed into the

114
00:11:23,040 --> 00:11:28,560
training set and we excluded it from our reported results. Before we get to Gemini, Google show off

115
00:11:28,560 --> 00:11:34,160
in the latter half of the technical report with examples of linguistic ability, like writing

116
00:11:34,160 --> 00:11:40,080
paragraphs in Tejiki and then translating them into Persian. They go on to show examples in

117
00:11:40,080 --> 00:11:45,840
Tamil and they are really making a big point of showing off its multilingual capabilities. At

118
00:11:45,840 --> 00:11:51,200
this point, and I'm going to admit this is my personal opinion, Google then strays into dozens

119
00:11:51,200 --> 00:11:58,000
of pages on bias, toxicity and gender. Interestingly, some of the people paid to assess these risks

120
00:11:58,000 --> 00:12:03,120
were paid only 1.5 cents per judgment. These things do need to be addressed, of course,

121
00:12:03,120 --> 00:12:08,480
but it was somewhat shocking to me to see 20 pages of that and not a single page on the

122
00:12:08,480 --> 00:12:14,000
broader AI impacts. As many of you may know, I have criticized open AI plenty of times on this

123
00:12:14,000 --> 00:12:19,440
channel, but compare their technical report, which goes into far more detail about what we need to

124
00:12:19,440 --> 00:12:25,360
monitor. The closest Google got was showing how their universal translator could be used for deep fakes.

125
00:12:25,360 --> 00:12:31,600
Universal Translate is an experimental AI video dubbing service that helps experts

126
00:12:31,600 --> 00:12:37,120
translate a speaker's voice while also matching their lip movements. Let me show you how it works

127
00:12:37,120 --> 00:12:42,560
with an online college course created in partnership with Arizona State University.

128
00:12:42,560 --> 00:12:47,440
What many college students don't realize is that knowing when to ask for help and then following

129
00:12:47,440 --> 00:12:51,280
through and using helpful resources is actually a hallmark of becoming a productive adult.

130
00:13:01,040 --> 00:13:05,440
It just seems a massive black hole when one of their recent former employees,

131
00:13:05,440 --> 00:13:08,640
Jeffrey Hinton, had this to say this week on CNN.

132
00:13:08,640 --> 00:13:15,200
You've spoken out saying that AI could manipulate or possibly figure out a way to kill humans?

133
00:13:15,440 --> 00:13:16,640
How could it kill humans?

134
00:13:16,640 --> 00:13:20,320
If it gets to be much smarter than us, it'll be very good at manipulation because it will

135
00:13:20,320 --> 00:13:25,200
have learned that from us. And a very few examples of a more intelligent thing being

136
00:13:25,200 --> 00:13:29,760
controlled by a less intelligent thing. And it knows how to program, so it'll figure out ways of

137
00:13:29,760 --> 00:13:34,800
getting around restrictions we put on it. It'll figure out ways of manipulating people to do

138
00:13:34,800 --> 00:13:40,720
what it wants. It's not clear to me that we can solve this problem. I believe we should put a big

139
00:13:40,720 --> 00:13:44,720
effort into thinking about ways to solve the problem. I don't have a solution at present.

140
00:13:44,720 --> 00:13:49,280
I just want people to be aware that this is a really serious problem and we need to be thinking

141
00:13:49,280 --> 00:13:53,680
about it very hard. This all seems particularly relevant when Google made this announcement

142
00:13:53,680 --> 00:14:00,560
about Gemini, their rival to GPT-5. All this helps set the stage for the inflection point we are at

143
00:14:00,560 --> 00:14:06,640
today. We recently brought these two teams together into a single unit, Google DeepMind.

144
00:14:07,360 --> 00:14:13,040
Using the computational resources of Google, they are focused on building more capable systems

145
00:14:13,760 --> 00:14:20,560
safely and responsibly. This includes our next generation foundation model, Gemini,

146
00:14:20,560 --> 00:14:26,400
which is still in training. Gemini was created from the ground up to be multi-modal,

147
00:14:27,280 --> 00:14:33,600
highly efficient at tool and API integrations, and built to enable future innovations

148
00:14:33,600 --> 00:14:39,280
like memory and planning. That ability to plan may ring a bell from the GPT-4

149
00:14:40,080 --> 00:14:44,400
which said this, novel capabilities often emerge in more powerful models.

150
00:14:44,400 --> 00:14:50,160
Some that are particularly concerning are the ability to create and act on long-term plans.

151
00:14:50,160 --> 00:14:55,760
Remember, Google didn't identify planning as a risk but as a selling point for Gemini.

152
00:14:55,760 --> 00:15:00,880
Next, Google talked about accelerating their progress, which was again directly mentioned

153
00:15:00,880 --> 00:15:05,920
in the GPT-4 technical report. It said, one concern of particular importance to open AI

154
00:15:05,920 --> 00:15:10,080
is the risk of racing dynamics leading to a decline in safety standards,

155
00:15:10,080 --> 00:15:15,840
the diffusion of bad norms and accelerated AI timelines, each of which heightens societal

156
00:15:15,840 --> 00:15:21,920
risks associated with AI. We refer to these here as acceleration risk and make no mistake,

157
00:15:21,920 --> 00:15:28,880
Gemini will be very accelerated from Palm II. It looks set to use the TPU V5 chip,

158
00:15:28,880 --> 00:15:34,800
which was announced back in January of last year. And on page 91 of the Palm II technical report,

159
00:15:34,800 --> 00:15:40,960
they say that that model used TPU V4. Now, it should be said that Palm II is leading to some

160
00:15:40,960 --> 00:15:46,240
impressive medical applications. As I actually first reported on seven weeks ago without quite

161
00:15:46,240 --> 00:15:51,440
realizing it, here's MedPalm II. We believe large language models have the potential to revolutionize

162
00:15:51,440 --> 00:15:56,240
healthcare and benefit society. MedPalm is a large language model that we've taken and tuned for the

163
00:15:56,240 --> 00:16:02,960
medical domain. Medical question answering has been a research grand challenge for several decades

164
00:16:02,960 --> 00:16:07,280
but till date the progress has been kind of slow. But then over the course of the last

165
00:16:08,000 --> 00:16:13,280
three to four months, first with MedPalm and MedPalm II, we have kind of like broken through that

166
00:16:13,280 --> 00:16:19,200
barrier. Unlike previous versions, MedPalm II was able to score 85% on the USMLA medical licensing

167
00:16:19,200 --> 00:16:24,000
exam. Yeah, this is immensely exciting because people have been working on medical question

168
00:16:24,000 --> 00:16:28,800
answering for over three decades. And finally, we are at a stage where we can say with confidence

169
00:16:28,800 --> 00:16:34,400
that AI systems can now at least answer USMLA questions as good as experts. As many of you

170
00:16:34,400 --> 00:16:40,400
may know, the CEO of Google as well as the CEO of Microsoft and Sam Altman and the CEO of Anthropic

171
00:16:40,400 --> 00:16:45,840
all went to the White House to discuss AI risk and opportunity. But given that the main outcome

172
00:16:45,840 --> 00:16:52,000
from that seems to be 140 million to establish seven new AI research institutes, that feels a

173
00:16:52,000 --> 00:16:56,800
little slow given all the acceleration that's occurring. Because as Google somewhat soberly

174
00:16:56,800 --> 00:17:01,680
conclude their report, we believe that further scaling of both model parameters and data set

175
00:17:01,680 --> 00:17:06,720
size and quality as well as improvements in the architecture and objective will continue to yield

176
00:17:06,720 --> 00:17:12,800
gains in language understanding and generation. They are not slowing down and the world hasn't

177
00:17:12,800 --> 00:17:26,640
yet caught up. Thank you so much for watching to the end and have a wonderful day.

