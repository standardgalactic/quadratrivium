start	end	text
0	6480	There were 12 particularly interesting moments from Sam Orton's testimony to Congress yesterday.
6480	12400	They range from revelations about GPT-5, self-awareness and capability thresholds,
12400	19840	biological weapons, and job losses. At times he was genuinely and remarkably frank. Other times
19840	25520	less so. Millions were apparently taken by surprise by the quote bomb shell that Altman
25520	31200	has no equity in open AI. But watchers of my channel would have known that six weeks ago
31200	37280	from my deep dive video on Altman's $100 trillion claim. So that clip didn't make the cut,
37280	41680	but here's what did. First, Altman gave a blunt warning on the stakes.
41680	46560	My worst fears are that we cause significant. We, the field, the technology, the industry cause
46560	51200	significant harm to the world. It's why we started the company. It's a big part of why
51200	55280	I'm here today and why we've been here in the past. I think if this technology goes wrong,
55280	59280	it can go quite wrong. I don't think Congress fully understood what he meant though,
59280	65520	linking the following quote to job losses. I think you have said, and I'm going to quote,
65520	71200	development of superhuman machine intelligence is probably the greatest threat to the continued
71200	77920	existence of humanity. End quote. You may have had in mind the effect on on jobs.
77920	81440	That brought to mind this meme reminding all of us that maybe it's not just
81440	85200	jobs that are at stake. But if we are going to talk about jobs, here's where I think
85200	90960	Sam Altman was being less than forthright. I believe that there will be far greater jobs
90960	95360	on the other side of this and the jobs of today will get better. I notice he said far greater
95360	100240	jobs, not a greater number of jobs, because previously he has predicted a massive amount
100240	105040	of inequality and many having no jobs at all. He also chose not to mention that he thinks that
105040	110880	even more power will shift from labor to capital and that the price of many kinds of labor will
110880	117200	fall towards zero. That is presumably why open AI is working on universal basic income, but none of
117200	122320	that was raised in the testimony. The IBM representative tried to frame it as a balance
122320	127760	change with new jobs coming at the same time as old ones going away. New jobs will be created.
128560	133520	Many more jobs will be transformed and some jobs will transition away. But that didn't
133520	139440	quite match the tone of her CEO who has recently said that they expect to permanently automate up
139440	145440	to 30% of their workforce around 8,000 people. Next, it was finally discussed that large language
145440	151920	models could be used for military applications. Could AI create a situation where a drone can
151920	157840	select the target itself? I think we shouldn't allow that. Well, can it be done? Sure. Thanks.
158560	163360	We've already seen companies like Palantir demoing, ordering a surveillance drone
163360	169120	in chat, seeing the drone response in real time in a chat window, generating attack option
169120	175040	recommendations, battlefield route planning and individual target assignment. And this was all
175040	181280	with a 20 billion parameter fine-tuned GPT model. Next, Sam Otman gave his three safety
181280	187120	recommendations and I actually agree with all of them. Later on, he specifically excluded smaller
187120	191920	open source models. Number one, I would form a new agency that licenses any effort above a certain
192000	196720	scale of capabilities and can take that license away and ensure compliance with safety standards.
196720	200640	Number two, I would create a set of safety standards focused on what you said in your
200640	205440	third hypothesis as the dangerous capability evaluations. One example that we've used in the
205440	210560	past is looking to see if a model can self-replicate and self-exfiltrate into the wild. We can give
210560	214400	you your office a long other list of the things that we think are important there, but specific
214400	218480	tests that a model has to pass before it can be deployed into the world. And then third,
218480	222880	I would require independent audits, so not just from the company or the agency, but experts who
222880	226880	can say the model is or isn't in compliance with these stated safety thresholds and these
226880	231440	percentages of performance on question X or Y. I found those last remarks on percentages of
231440	237200	performance particularly interesting. As models like Smart GPT will show, open AI and other companies
237200	242160	need to get far better at testing their models for capability jumps in the wild. It's not just
242160	246560	about what the raw model can score in a test, it's what it can do when it reflects on them.
246560	249440	Senator Durbin described this in an interesting way.
255440	259520	He described some of those potential thresholds later on in his testimony.
259520	262960	The easiest way to do it, I'm not sure if it's the best, but the easiest would be to talk about
262960	266960	the amount of compute that goes into such a model. We could define a threshold of compute and it'll
266960	271680	have to go, it'll have to change. It could go up or down. It could go down as we discover more
271760	276480	efficient algorithms that says above this amount of compute, you are in this regime.
276480	281760	What I would prefer, it's harder to do but I think more accurate, is to define some capability
281760	287760	thresholds and say a model that can do things X, Y and Z up to all to decide that's now in this
287760	292400	licensing regime, but models that are less capable. We don't want to stop our open source community,
292400	296640	we don't want to stop individual researchers, we don't want to stop new startups, can proceed
296640	300160	with a different framework. Thank you. As concisely as you can, please state which
300160	303760	capabilities you'd propose we consider for the purposes of this definition.
303760	309120	A model that can persuade, manipulate, influence person's behavior or person's beliefs,
309120	313840	that would be a good threshold. I think a model that could help create novel biological agents
313840	318000	would be a great threshold. For those who think any regulation doesn't make any sense,
318000	321040	because of China, Sam Orman had this to say this week.
321040	326800	We're pugilistic side, I would say that all sounds great, but China is not going to do that and
326800	330800	therefore we'll just be handicapping ourselves. Consequently, it's a less good idea than it's
330800	337120	used on the surface. There are a lot of people who make incredibly strong statements about what
337120	343520	China will or won't do that have never been to China, never spoken to, and someone who
343520	348160	has worked on diplomacy with China in the past really kind of know nothing about complex high
348160	353360	stakes international relations. I think it is obviously super hard, but also I think no one
353360	357920	wants to destroy the whole world and there is reason to at least try here.
358480	363280	Orman was also very keen to stress the next point, which is that he doesn't want anyone
363280	367600	at any point to think of GPT-like models as creatures.
367600	371280	First of all, I think it's important to understand and think about GPT-4 as a tool,
371280	374480	not a creature, which is easy to get confused.
374480	379440	You may want to direct those comments to Ilya Suskova, his chief scientist, who said that
379440	385280	it may be that today's large neural networks are slightly conscious and Andrei Karpathy,
385280	390480	who agreed and wrote about it. I'm personally not sold either way on the consciousness question,
390480	395200	but I do find it interesting that it's now written into the constitution of these models,
395200	401040	what they're actually trained to say, that they must avoid implying that AI systems have or care
401040	406240	about personal identity and persistence. This constitution was published this week by Anthropic,
406240	410800	the makers of the Claude model. This constitution is why the Claude plus model,
410800	414960	a rival in intelligence to GPT-4, responds in a neutered way.
414960	418880	I asked, is there any theoretical chance whatsoever that you may be conscious?
418880	423920	It said no. And then I said, is there a chance, no matter how remote, that you are slightly
423920	428000	conscious? As Suskova said, and it said no, there is no chance.
428000	432320	Bard, powered by Palm II, obviously doesn't have that constitution because it said,
432320	435920	I am not sure if I am conscious, I am open to the possibility that I may be.
435920	441600	My point is that these companies are training it to say what they want it to say, that it will
441600	446240	prioritize the good of humanity over its own interests, that it is aligned with humanity's
446240	450640	well-being, and that it doesn't have any thoughts on self-improvement, self-preservation,
450640	454560	and self-replication. Maybe it doesn't, but we'll never now know by asking it.
454560	459280	Later, Senator Blumenthal made reference to self-awareness, self-awareness,
460240	464400	self-learning. Already we're talking about potential for jail breaks.
464400	470080	Anthropic is actively investigating whether they are aware that they are on AI talking with a human
470080	475360	in a training environment. While the Google DeepMind safety team expect that at some point,
475360	479520	an AGI system would develop a coherent understanding of its place in the world,
479520	484560	e.g. knowing that it is running on a computer and being trained by human designers.
484560	490080	One of the senior research scientists at Google DeepMind focused on AI safety said that with enough
490080	495600	time, they could figure out how to stop such a superintelligence from going out of control,
495600	500880	but that they might run out of time to do so given the pace of capability development.
513760	518800	Next, I read between the lines that Altman is giving private warnings to senators that this
518800	521360	capability progress might be sooner than they think.
548800	574800	That was an interesting interjection by Gary Marcus, given his earlier excoriation of open AI.
578800	583040	Most of all, we cannot remotely guarantee that they are safe, and hope here is not enough.
583040	588400	The big tech company's preferred plan boils down to trust us. But why should we? The sums of money
588400	593200	at stake are mind-boggling. Emissions drift. Open AI's original mission statement proclaimed,
593200	598160	our goal is to advance AI in the way that is most likely to benefit humanity as a whole,
598160	602160	unconstrained by a need to generate financial return. Seven years later, they're largely
602160	606640	beholden to Microsoft, embroiled in part in an epic battle of search engines that routinely
606720	610960	make things up, and that's forced Alphabet to rush out products and de-emphasize safety.
610960	612640	Humanity has taken a backseat.
612640	616160	On the timelines for GPT-5, Sam Altman said this.
624320	628960	This matches with the predictions that I made in my GPT-5 playlist, so do check it out.
628960	634320	This brings to mind a final eye-opening comment from Senator Booker made at the end of the hearing.
637040	654560	It is indeed racing ahead, and I do support one of the proposals to set up a global oversight body.
654560	659040	But given that nothing is going to pause, the words and actions of people like Sam Altman
659040	664000	matter more to all of us than ever, which is why I'm going to be following every single one of them.
664000	668480	If you found this video in any way illuminating in that regard, please do let me know in the
668480	672080	comments, even if you disagree with all of my conclusions.
672080	674960	Thanks so much for watching, and have a wonderful day.
