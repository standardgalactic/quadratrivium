WEBVTT

00:00.000 --> 00:06.480
There were 12 particularly interesting moments from Sam Orton's testimony to Congress yesterday.

00:06.480 --> 00:12.400
They range from revelations about GPT-5, self-awareness and capability thresholds,

00:12.400 --> 00:19.840
biological weapons, and job losses. At times he was genuinely and remarkably frank. Other times

00:19.840 --> 00:25.520
less so. Millions were apparently taken by surprise by the quote bomb shell that Altman

00:25.520 --> 00:31.200
has no equity in open AI. But watchers of my channel would have known that six weeks ago

00:31.200 --> 00:37.280
from my deep dive video on Altman's $100 trillion claim. So that clip didn't make the cut,

00:37.280 --> 00:41.680
but here's what did. First, Altman gave a blunt warning on the stakes.

00:41.680 --> 00:46.560
My worst fears are that we cause significant. We, the field, the technology, the industry cause

00:46.560 --> 00:51.200
significant harm to the world. It's why we started the company. It's a big part of why

00:51.200 --> 00:55.280
I'm here today and why we've been here in the past. I think if this technology goes wrong,

00:55.280 --> 00:59.280
it can go quite wrong. I don't think Congress fully understood what he meant though,

00:59.280 --> 01:05.520
linking the following quote to job losses. I think you have said, and I'm going to quote,

01:05.520 --> 01:11.200
development of superhuman machine intelligence is probably the greatest threat to the continued

01:11.200 --> 01:17.920
existence of humanity. End quote. You may have had in mind the effect on on jobs.

01:17.920 --> 01:21.440
That brought to mind this meme reminding all of us that maybe it's not just

01:21.440 --> 01:25.200
jobs that are at stake. But if we are going to talk about jobs, here's where I think

01:25.200 --> 01:30.960
Sam Altman was being less than forthright. I believe that there will be far greater jobs

01:30.960 --> 01:35.360
on the other side of this and the jobs of today will get better. I notice he said far greater

01:35.360 --> 01:40.240
jobs, not a greater number of jobs, because previously he has predicted a massive amount

01:40.240 --> 01:45.040
of inequality and many having no jobs at all. He also chose not to mention that he thinks that

01:45.040 --> 01:50.880
even more power will shift from labor to capital and that the price of many kinds of labor will

01:50.880 --> 01:57.200
fall towards zero. That is presumably why open AI is working on universal basic income, but none of

01:57.200 --> 02:02.320
that was raised in the testimony. The IBM representative tried to frame it as a balance

02:02.320 --> 02:07.760
change with new jobs coming at the same time as old ones going away. New jobs will be created.

02:08.560 --> 02:13.520
Many more jobs will be transformed and some jobs will transition away. But that didn't

02:13.520 --> 02:19.440
quite match the tone of her CEO who has recently said that they expect to permanently automate up

02:19.440 --> 02:25.440
to 30% of their workforce around 8,000 people. Next, it was finally discussed that large language

02:25.440 --> 02:31.920
models could be used for military applications. Could AI create a situation where a drone can

02:31.920 --> 02:37.840
select the target itself? I think we shouldn't allow that. Well, can it be done? Sure. Thanks.

02:38.560 --> 02:43.360
We've already seen companies like Palantir demoing, ordering a surveillance drone

02:43.360 --> 02:49.120
in chat, seeing the drone response in real time in a chat window, generating attack option

02:49.120 --> 02:55.040
recommendations, battlefield route planning and individual target assignment. And this was all

02:55.040 --> 03:01.280
with a 20 billion parameter fine-tuned GPT model. Next, Sam Otman gave his three safety

03:01.280 --> 03:07.120
recommendations and I actually agree with all of them. Later on, he specifically excluded smaller

03:07.120 --> 03:11.920
open source models. Number one, I would form a new agency that licenses any effort above a certain

03:12.000 --> 03:16.720
scale of capabilities and can take that license away and ensure compliance with safety standards.

03:16.720 --> 03:20.640
Number two, I would create a set of safety standards focused on what you said in your

03:20.640 --> 03:25.440
third hypothesis as the dangerous capability evaluations. One example that we've used in the

03:25.440 --> 03:30.560
past is looking to see if a model can self-replicate and self-exfiltrate into the wild. We can give

03:30.560 --> 03:34.400
you your office a long other list of the things that we think are important there, but specific

03:34.400 --> 03:38.480
tests that a model has to pass before it can be deployed into the world. And then third,

03:38.480 --> 03:42.880
I would require independent audits, so not just from the company or the agency, but experts who

03:42.880 --> 03:46.880
can say the model is or isn't in compliance with these stated safety thresholds and these

03:46.880 --> 03:51.440
percentages of performance on question X or Y. I found those last remarks on percentages of

03:51.440 --> 03:57.200
performance particularly interesting. As models like Smart GPT will show, open AI and other companies

03:57.200 --> 04:02.160
need to get far better at testing their models for capability jumps in the wild. It's not just

04:02.160 --> 04:06.560
about what the raw model can score in a test, it's what it can do when it reflects on them.

04:06.560 --> 04:09.440
Senator Durbin described this in an interesting way.

04:15.440 --> 04:19.520
He described some of those potential thresholds later on in his testimony.

04:19.520 --> 04:22.960
The easiest way to do it, I'm not sure if it's the best, but the easiest would be to talk about

04:22.960 --> 04:26.960
the amount of compute that goes into such a model. We could define a threshold of compute and it'll

04:26.960 --> 04:31.680
have to go, it'll have to change. It could go up or down. It could go down as we discover more

04:31.760 --> 04:36.480
efficient algorithms that says above this amount of compute, you are in this regime.

04:36.480 --> 04:41.760
What I would prefer, it's harder to do but I think more accurate, is to define some capability

04:41.760 --> 04:47.760
thresholds and say a model that can do things X, Y and Z up to all to decide that's now in this

04:47.760 --> 04:52.400
licensing regime, but models that are less capable. We don't want to stop our open source community,

04:52.400 --> 04:56.640
we don't want to stop individual researchers, we don't want to stop new startups, can proceed

04:56.640 --> 05:00.160
with a different framework. Thank you. As concisely as you can, please state which

05:00.160 --> 05:03.760
capabilities you'd propose we consider for the purposes of this definition.

05:03.760 --> 05:09.120
A model that can persuade, manipulate, influence person's behavior or person's beliefs,

05:09.120 --> 05:13.840
that would be a good threshold. I think a model that could help create novel biological agents

05:13.840 --> 05:18.000
would be a great threshold. For those who think any regulation doesn't make any sense,

05:18.000 --> 05:21.040
because of China, Sam Orman had this to say this week.

05:21.040 --> 05:26.800
We're pugilistic side, I would say that all sounds great, but China is not going to do that and

05:26.800 --> 05:30.800
therefore we'll just be handicapping ourselves. Consequently, it's a less good idea than it's

05:30.800 --> 05:37.120
used on the surface. There are a lot of people who make incredibly strong statements about what

05:37.120 --> 05:43.520
China will or won't do that have never been to China, never spoken to, and someone who

05:43.520 --> 05:48.160
has worked on diplomacy with China in the past really kind of know nothing about complex high

05:48.160 --> 05:53.360
stakes international relations. I think it is obviously super hard, but also I think no one

05:53.360 --> 05:57.920
wants to destroy the whole world and there is reason to at least try here.

05:58.480 --> 06:03.280
Orman was also very keen to stress the next point, which is that he doesn't want anyone

06:03.280 --> 06:07.600
at any point to think of GPT-like models as creatures.

06:07.600 --> 06:11.280
First of all, I think it's important to understand and think about GPT-4 as a tool,

06:11.280 --> 06:14.480
not a creature, which is easy to get confused.

06:14.480 --> 06:19.440
You may want to direct those comments to Ilya Suskova, his chief scientist, who said that

06:19.440 --> 06:25.280
it may be that today's large neural networks are slightly conscious and Andrei Karpathy,

06:25.280 --> 06:30.480
who agreed and wrote about it. I'm personally not sold either way on the consciousness question,

06:30.480 --> 06:35.200
but I do find it interesting that it's now written into the constitution of these models,

06:35.200 --> 06:41.040
what they're actually trained to say, that they must avoid implying that AI systems have or care

06:41.040 --> 06:46.240
about personal identity and persistence. This constitution was published this week by Anthropic,

06:46.240 --> 06:50.800
the makers of the Claude model. This constitution is why the Claude plus model,

06:50.800 --> 06:54.960
a rival in intelligence to GPT-4, responds in a neutered way.

06:54.960 --> 06:58.880
I asked, is there any theoretical chance whatsoever that you may be conscious?

06:58.880 --> 07:03.920
It said no. And then I said, is there a chance, no matter how remote, that you are slightly

07:03.920 --> 07:08.000
conscious? As Suskova said, and it said no, there is no chance.

07:08.000 --> 07:12.320
Bard, powered by Palm II, obviously doesn't have that constitution because it said,

07:12.320 --> 07:15.920
I am not sure if I am conscious, I am open to the possibility that I may be.

07:15.920 --> 07:21.600
My point is that these companies are training it to say what they want it to say, that it will

07:21.600 --> 07:26.240
prioritize the good of humanity over its own interests, that it is aligned with humanity's

07:26.240 --> 07:30.640
well-being, and that it doesn't have any thoughts on self-improvement, self-preservation,

07:30.640 --> 07:34.560
and self-replication. Maybe it doesn't, but we'll never now know by asking it.

07:34.560 --> 07:39.280
Later, Senator Blumenthal made reference to self-awareness, self-awareness,

07:40.240 --> 07:44.400
self-learning. Already we're talking about potential for jail breaks.

07:44.400 --> 07:50.080
Anthropic is actively investigating whether they are aware that they are on AI talking with a human

07:50.080 --> 07:55.360
in a training environment. While the Google DeepMind safety team expect that at some point,

07:55.360 --> 07:59.520
an AGI system would develop a coherent understanding of its place in the world,

07:59.520 --> 08:04.560
e.g. knowing that it is running on a computer and being trained by human designers.

08:04.560 --> 08:10.080
One of the senior research scientists at Google DeepMind focused on AI safety said that with enough

08:10.080 --> 08:15.600
time, they could figure out how to stop such a superintelligence from going out of control,

08:15.600 --> 08:20.880
but that they might run out of time to do so given the pace of capability development.

08:33.760 --> 08:38.800
Next, I read between the lines that Altman is giving private warnings to senators that this

08:38.800 --> 08:41.360
capability progress might be sooner than they think.

09:08.800 --> 09:34.800
That was an interesting interjection by Gary Marcus, given his earlier excoriation of open AI.

09:38.800 --> 09:43.040
Most of all, we cannot remotely guarantee that they are safe, and hope here is not enough.

09:43.040 --> 09:48.400
The big tech company's preferred plan boils down to trust us. But why should we? The sums of money

09:48.400 --> 09:53.200
at stake are mind-boggling. Emissions drift. Open AI's original mission statement proclaimed,

09:53.200 --> 09:58.160
our goal is to advance AI in the way that is most likely to benefit humanity as a whole,

09:58.160 --> 10:02.160
unconstrained by a need to generate financial return. Seven years later, they're largely

10:02.160 --> 10:06.640
beholden to Microsoft, embroiled in part in an epic battle of search engines that routinely

10:06.720 --> 10:10.960
make things up, and that's forced Alphabet to rush out products and de-emphasize safety.

10:10.960 --> 10:12.640
Humanity has taken a backseat.

10:12.640 --> 10:16.160
On the timelines for GPT-5, Sam Altman said this.

10:24.320 --> 10:28.960
This matches with the predictions that I made in my GPT-5 playlist, so do check it out.

10:28.960 --> 10:34.320
This brings to mind a final eye-opening comment from Senator Booker made at the end of the hearing.

10:37.040 --> 10:54.560
It is indeed racing ahead, and I do support one of the proposals to set up a global oversight body.

10:54.560 --> 10:59.040
But given that nothing is going to pause, the words and actions of people like Sam Altman

10:59.040 --> 11:04.000
matter more to all of us than ever, which is why I'm going to be following every single one of them.

11:04.000 --> 11:08.480
If you found this video in any way illuminating in that regard, please do let me know in the

11:08.480 --> 11:12.080
comments, even if you disagree with all of my conclusions.

11:12.080 --> 11:14.960
Thanks so much for watching, and have a wonderful day.

