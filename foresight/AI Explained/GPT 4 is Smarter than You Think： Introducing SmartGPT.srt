1
00:00:00,000 --> 00:00:06,160
I have three goals for this video. First, I want to show you a way of using GPT-4 to get smarter

2
00:00:06,160 --> 00:00:13,040
results. Second, I want to argue that the benchmark results we have for GPT-4 do not reflect its full

3
00:00:13,040 --> 00:00:18,080
abilities. And third, I want to show you a system that I am developing, somewhat cheekily called

4
00:00:18,080 --> 00:00:24,320
Smart GPT, that is already showing significant results on official benchmarks. It remains to

5
00:00:24,320 --> 00:00:30,240
be fully optimized, which I think is exciting in itself. I have shown the system to people at Open

6
00:00:30,240 --> 00:00:34,720
AI who have been quite impressed, and I'm going to end with some reflections on where that might

7
00:00:34,720 --> 00:00:41,040
leave us for GPT-5. But before I get into how it works, I just want to show you one example of it

8
00:00:41,040 --> 00:00:46,560
in action to whet your appetite. This example comes from a TED talk that was released this week.

9
00:00:46,560 --> 00:00:53,760
So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely.

10
00:00:53,760 --> 00:01:01,600
How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not

11
00:01:01,600 --> 00:01:07,520
good. On the left, you can see GPT-4's original answer, and it gives this answer pretty consistently

12
00:01:07,520 --> 00:01:12,400
whenever you prompt it with the question provided. On the right, you can see the final answer from

13
00:01:12,400 --> 00:01:17,280
the Smart GPT model, which is correct, and it consistently gives that answer. I really like

14
00:01:17,280 --> 00:01:22,320
how it gives context as well, and it provides some of the assumptions that it had in giving this

15
00:01:22,320 --> 00:01:27,120
correct answer. Now, don't you worry, there will be plenty more examples to go through in this video,

16
00:01:27,120 --> 00:01:31,920
including another one from that TED talk. But first, I want to give you an overview of what is

17
00:01:31,920 --> 00:01:36,560
this Smart GPT model, where did I get my inspiration for it from, and how does it work? I'm going to

18
00:01:36,560 --> 00:01:40,480
keep it fairly simple because it's the beginning of the video, and I know a lot of people won't really

19
00:01:40,480 --> 00:01:46,640
care about the inner details that will come later in the video. But the high-level overview is this.

20
00:01:46,640 --> 00:01:52,320
There are at least three things that have been proven to improve the outputs of GPT-4.

21
00:01:52,320 --> 00:01:56,400
What's called chain of thought prompting, sometimes called step-by-step prompting,

22
00:01:56,400 --> 00:02:01,200
reflection, or finding its own errors, and I did an entire video on this called GPT-4

23
00:02:01,200 --> 00:02:06,880
Can Self Improve, and dialoguing with itself, entering into a back and forth on its own

24
00:02:06,880 --> 00:02:11,920
outputs and deciding which one is best. You can see the title of the papers, which contain much

25
00:02:11,920 --> 00:02:17,040
more detailed results, of course, linked above. Now, the first paper only came out a few days ago,

26
00:02:17,040 --> 00:02:22,240
midway through my testing, so my results don't even reflect the full capacity of the model.

27
00:02:22,240 --> 00:02:27,920
And even if there's nothing else you take from this video, the results from this paper can instantly

28
00:02:27,920 --> 00:02:34,480
improve the outputs you get from GPT-4. Many of you might remember that prompting GPT-4 with

29
00:02:34,480 --> 00:02:40,240
let's think step-by-step improves its results. To give you a very quick reference point, just

30
00:02:40,240 --> 00:02:46,720
asking a question to GPT-4 gives you 81% accuracy. With that prompt, let's think step-by-step,

31
00:02:46,720 --> 00:02:53,520
it goes up to 86%. But algorithmically, the paper found an improved prompt that can give you even

32
00:02:53,520 --> 00:02:59,920
better results, 89% accuracy. All we do, and this is the first part of smart GPT, is we add

33
00:02:59,920 --> 00:03:06,160
answer, let's work this out in a step-by-step way to be sure we have the right answer. Now,

34
00:03:06,160 --> 00:03:11,520
I have so much to say about why I think this works, but I know many of you won't be that

35
00:03:11,520 --> 00:03:15,840
interested in my theories, so I'm going to save them to the end for those who are interested.

36
00:03:15,840 --> 00:03:20,080
Some of you just want the results, so I'm going to get to those first. So far, you might be thinking,

37
00:03:20,080 --> 00:03:23,760
well, thanks, Philip, that's a cool prompt. I'm going to use that. But what's this whole smart

38
00:03:23,760 --> 00:03:29,920
GPT about? Is it just a single prompt? No, I believe with evidence, there are ways of leveraging

39
00:03:29,920 --> 00:03:35,520
even better results than just using a great chain of thought prompt. So let's move on to the next

40
00:03:35,520 --> 00:03:40,000
part of the system, these different outputs in the middle. For my tests, I typically did three

41
00:03:40,000 --> 00:03:44,240
outputs, but of course, depending on the context window, it could be far more than that. And I'm

42
00:03:44,240 --> 00:03:49,440
going to talk about ways I could further improve this model, or we could later on in the video.

43
00:03:49,440 --> 00:03:54,640
Just to restate, these outputs are when you take the user input and add the word question at start,

44
00:03:54,640 --> 00:03:59,040
and then at the end add answer, let's work this out in a step-by-step way to make sure we have

45
00:03:59,040 --> 00:04:03,440
the right answer. And at this moment, many of you are thinking, what is the point of multiple

46
00:04:03,440 --> 00:04:07,680
outputs? It's GPT-4, it's just going to give you the answer that thinks is best, and that's it.

47
00:04:07,680 --> 00:04:12,160
Well, actually, it doesn't quite work like that. These models have a temperature between zero and

48
00:04:12,160 --> 00:04:18,480
one. I believe the default for GPT-4 might be around 0.5. And simplifying massively, this determines

49
00:04:18,480 --> 00:04:24,720
how creative or conservative the model is in giving its outputs. So given that GPT-4 tries to be

50
00:04:24,720 --> 00:04:30,400
fairly creative, you don't get the same output every time. The output is randomly sampled according

51
00:04:30,400 --> 00:04:35,520
to an internal probability distribution. So you can get situations, and I face this hundreds of

52
00:04:35,520 --> 00:04:41,280
times, where some of the outputs are correct, and others are incorrect. And this is where reflection

53
00:04:41,280 --> 00:04:48,320
comes in. Sometimes, definitely not always, but sometimes quite often, GPT-4 can detect the errors

54
00:04:48,320 --> 00:04:54,000
in its own output. And many of you will notice at this point that the prompt that I used to elicit

55
00:04:54,000 --> 00:05:01,520
GPT-4 to spot its own errors contains the same step-by-step prompt I used earlier that has been

56
00:05:01,520 --> 00:05:08,720
shown to produce good results. So to summarize, sometimes at this stage, GPT-4 detects the errors

57
00:05:08,720 --> 00:05:13,760
that some of its outputs have made. Definitely not always. There are certain questions, it just simply

58
00:05:13,760 --> 00:05:19,600
can't spot the error. But sometimes it can. And then I get it to engage in a dialogue using a format

59
00:05:19,600 --> 00:05:24,480
similar to one in this paper published last month. It's a short dialogue, and this is the

60
00:05:24,480 --> 00:05:30,880
step I believe that can be most optimized. In the future, I envision an entire council of advisors

61
00:05:30,880 --> 00:05:37,840
made up of GPT-4 imitating mathematicians, judges, etc. At the moment, it's just being a resolver

62
00:05:37,840 --> 00:05:42,880
and printing a final improved output. Anyway, I'm going to get back to the theory later in the video

63
00:05:42,880 --> 00:05:46,720
because I know some of you will be getting bored at this stage and want to see more practical

64
00:05:46,720 --> 00:05:53,440
examples and the results from my benchmark tests. As I don't have the GPT-4 API key, yes, I had to

65
00:05:53,440 --> 00:05:59,440
manually input each of these steps hundreds of times, waiting sometimes three hours between each

66
00:05:59,440 --> 00:06:04,640
go because you can only do 25 messages every three hours. On the left, you can see the three

67
00:06:04,640 --> 00:06:10,160
outputs when you ask it to think step by step. And then you have the researcher step in the middle

68
00:06:10,160 --> 00:06:14,800
and at the top right. And finally, the resolver step. Notice here, I was using the original

69
00:06:14,800 --> 00:06:19,680
let's think step by step because the paper hadn't yet been published on improving that prompt.

70
00:06:19,680 --> 00:06:24,560
It's time for the second example from that TED Talk, and then I definitely will get on to the

71
00:06:24,560 --> 00:06:30,960
benchmarks. A different one. I have 12 liter jug and 6 liter jug, and I want to measure 6 liters.

72
00:06:30,960 --> 00:06:38,240
How do I do it? Just use the 6 liter jug, right? GPT-4 spits out some very elaborate nonsense.

73
00:06:40,400 --> 00:06:44,560
Of course, I tested smart GPT with that question, and you can see the difference between

74
00:06:44,560 --> 00:06:50,720
the original GPT-4, which gives this incredibly convoluted bad answer, and smart GPT, the final

75
00:06:50,720 --> 00:06:54,560
answer output. Now, at this point, I know many of you will be impressed, but you'll be thinking,

76
00:06:54,560 --> 00:06:59,920
I don't have time to input things five times. Well, I'm developing a model where it can all

77
00:06:59,920 --> 00:07:04,720
be done automatically. Here is a preview of how it works. But of course, at the moment, it has to

78
00:07:04,720 --> 00:07:11,280
use GPT 3.5 Turbo because I don't have the API key of GPT-4. But the epic thing is this, you just

79
00:07:11,280 --> 00:07:16,320
ask a single question, I've written, ask smart GPT-A question. And of course, it does take a

80
00:07:16,320 --> 00:07:21,440
little bit longer to respond because it's doing five or six calls via API, but it does output the

81
00:07:21,440 --> 00:07:27,200
final answer from the resolver step. I will be honest and say that GPT 3.5 isn't as good at

82
00:07:27,200 --> 00:07:32,720
reflecting or resolving. But this is an example of a question where the original chat GPT consistently

83
00:07:32,720 --> 00:07:39,440
gets it wrong, and smart GPT 3.5 gets it right using this program. Remember, all you have to do as

84
00:07:39,440 --> 00:07:45,200
a user is type in a question as normal, and it goes through this entire five or six step process

85
00:07:45,200 --> 00:07:50,320
behind the scenes. By the way, this was a question from MMLU, which is a famous benchmark which I'll

86
00:07:50,320 --> 00:07:55,200
get to in a second. Here's one last practical example before I get to that benchmark. I know many

87
00:07:55,200 --> 00:08:01,200
teachers use chat GPT and GPT-4 to create quizzes for their classes. And here is the same question

88
00:08:01,200 --> 00:08:06,720
put through GPT-4 and smart GPT. The question is, create a high school algebra quiz with five

89
00:08:06,720 --> 00:08:11,440
questions and answers and explanations at the end. Now points for spotting the difference,

90
00:08:11,440 --> 00:08:16,640
but if the teacher had handed out the original quiz, look at the answers for question five.

91
00:08:16,640 --> 00:08:22,880
It says the answers are one and 1.5. But then in the explanation, it gives the final answers,

92
00:08:22,880 --> 00:08:28,080
which are correct by the way, of three and zero point five. So that would really confuse some

93
00:08:28,080 --> 00:08:33,760
students at the reflection stage smart GPT spotted that error and resolved it. And as you can see,

94
00:08:33,760 --> 00:08:38,560
the answer for question five has the correct answers straight away. If at any point you're

95
00:08:38,560 --> 00:08:43,680
wondering if I completed the open AI chat GPT prompt engineering course, the answer is yes,

96
00:08:43,680 --> 00:08:48,160
but it didn't inform too much of my thinking. It was more for beginners and I had already

97
00:08:48,160 --> 00:08:53,040
factored in things like giving the model time to think and writing clear instructions. The

98
00:08:53,040 --> 00:09:00,240
benchmark that I chose to test smart GPT on was the famous MMLU, massive multitask language

99
00:09:00,240 --> 00:09:06,880
understanding benchmark. As you can see, the state of the art is indeed GPT for with 86.4%

100
00:09:06,880 --> 00:09:11,840
accuracy. And you know, open AI think it's a big deal because it's the benchmark mentioned on the

101
00:09:11,840 --> 00:09:16,480
front page of their technical report without boring you too much. I extracted the questions

102
00:09:16,480 --> 00:09:23,520
from the test set of the MMLU data file, and I didn't pick the topics at random. I went for

103
00:09:23,520 --> 00:09:29,440
those that I thought GPT for would find the hardest delving into the original MMLU paper.

104
00:09:29,520 --> 00:09:37,280
You can see that GPT three found for more logic the hardest scoring just over 25% which is random

105
00:09:37,280 --> 00:09:44,080
chance. It's a four question multiple choice test. So around 25 or 30% is pretty bad. And notice

106
00:09:44,080 --> 00:09:50,800
they helped out GPT three here. They did it few shot, meaning they gave it five successful examples

107
00:09:50,800 --> 00:09:55,920
before asking it a new question. It's the same thing they did with GPT four. They did it five

108
00:09:55,920 --> 00:09:59,760
shot. But just before I show you the results, there are three things I want to mention here.

109
00:09:59,760 --> 00:10:05,840
First, I was curious how smart GPT would do without any help zero shot. Second, I wanted to do it

110
00:10:05,840 --> 00:10:12,480
zero shot because people using GPT four don't typically give five successful examples before

111
00:10:12,480 --> 00:10:18,320
asking GPT for a question. They just want code or a quiz or a poem or an example. They don't often

112
00:10:18,320 --> 00:10:23,280
provide five brilliant examples of code before asking their question. And third, if I can prove

113
00:10:23,280 --> 00:10:28,480
it works zero shot, then of course, future refinements can be made to push the results even

114
00:10:28,480 --> 00:10:35,120
further. And here are the results from the first 25 questions from the formal logic test set of the

115
00:10:35,120 --> 00:10:41,120
MMLU. I did many more tests after this. But you can see from this set, if you just ask the question,

116
00:10:41,120 --> 00:10:47,840
you get a lower overall accuracy. But of course, 68% for GPT four is still a huge improvement over

117
00:10:47,840 --> 00:10:54,480
GPT threes around 25%. What happens when you add let's think step by step, which as we know now

118
00:10:54,480 --> 00:11:00,960
isn't the fully optimized chain of thought prompt. Well, on average, you get around 74-75%.

119
00:11:01,440 --> 00:11:07,280
That was 75 examples inputted manually. And I still have all the tabs open. I'm keeping them open

120
00:11:07,280 --> 00:11:12,080
because I'm compiling a spreadsheet with the actual outputs. But what did the resolver get

121
00:11:12,080 --> 00:11:18,160
drawing upon GPT four's ability to reflect and engage in dialogue with itself? It got 84%.

122
00:11:18,720 --> 00:11:25,040
Now notice something about that number. GPT four zero short got 32% of the questions wrong. That

123
00:11:25,040 --> 00:11:30,960
was halved to 16% after putting it through the smart GPT system. There was one question where

124
00:11:30,960 --> 00:11:36,400
the resolver model gave both a correct and incorrect answer. But I'm counting that as an

125
00:11:36,400 --> 00:11:42,880
incorrect answer for the purposes of this test. Anyway, from 32% to 16% incorrect,

126
00:11:42,880 --> 00:11:48,080
that is a pattern that stayed consistent throughout all my testing that approximately half of the

127
00:11:48,080 --> 00:11:54,960
errors that GPT four makes can be rectified. If you give it the optimized step by step prompt,

128
00:11:54,960 --> 00:12:01,680
get it to reflect on its results and get it to engage in dialogue and decide on a final answer.

129
00:12:01,680 --> 00:12:06,160
At this point, for those people losing track of all the details, I want to put into context

130
00:12:06,160 --> 00:12:12,240
what resolving half of the errors on MMLU might mean in the context of the big picture.

131
00:12:12,240 --> 00:12:19,280
Here's Lenard Heim, an AI governance researcher, suggesting a score of 95% on the MMLU would be

132
00:12:19,280 --> 00:12:25,520
reflective of AGI like abilities. I do think I have like a 50% chance like within the next 20

133
00:12:25,520 --> 00:12:30,400
years or so, there might be something what we might call an AGI or transformative AI. What

134
00:12:30,400 --> 00:12:35,120
do I mean by this? Well, maybe can measured on benchmarks. There's like this famous MMLU

135
00:12:35,120 --> 00:12:40,480
benchmarks like yet or something which like scores like 95% on this. Going back to the results,

136
00:12:40,480 --> 00:12:47,680
if a smart GPT like system can automatically resolve half of the errors that GPT four makes

137
00:12:47,680 --> 00:12:55,680
on the MMLU, that would increase its score from around 86.4% to around 93%, which is not far off

138
00:12:55,680 --> 00:13:02,720
95%. Remember, his prediction was a 50% chance in 20 years. I'm talking about GPT four now.

139
00:13:02,720 --> 00:13:06,960
For those who are still skeptical, I'm going to show you plenty more results now and then walk

140
00:13:06,960 --> 00:13:12,080
through the papers that give the theory as to why this works. One thing that I forgot to mention

141
00:13:12,080 --> 00:13:20,560
earlier is that the human expert level on the MMLU is 89.8%. And that's taking the 95th percentile

142
00:13:20,560 --> 00:13:26,160
of human test takers. And remember, those are domain experts in each of these subtopics.

143
00:13:26,160 --> 00:13:32,560
What we're doing is testing GPT four or smart GPT on all of the topics simultaneously.

144
00:13:32,560 --> 00:13:37,280
So even if smart GPT like systems can't quite reach 95%, and I think honestly,

145
00:13:37,280 --> 00:13:40,960
they'll get pretty close with all the refinements that I'm going to suggest,

146
00:13:40,960 --> 00:13:47,600
I think they should almost certainly be 89.8%, which is the human expert test taker level.

147
00:13:47,600 --> 00:13:53,840
Intrigued by these results, I then put it through the college math test from the MMLU. And remember,

148
00:13:53,840 --> 00:13:58,800
this was before using the optimized version of the step by step prompt. Obviously, I'm not going to

149
00:13:58,800 --> 00:14:07,040
go through all the questions here, but let's skip to the final results. We have zero shot accuracy,

150
00:14:07,040 --> 00:14:13,280
six out of 15, which is 40%. The average when you add let's think step by step was 53.5%.

151
00:14:13,840 --> 00:14:20,080
And then the final output of the resolver model had a 60% accuracy. So it couldn't quite resolve

152
00:14:20,080 --> 00:14:25,360
half of the errors, but the overall pattern held up. In case anyone is wondering about methodology,

153
00:14:25,360 --> 00:14:31,360
I kept the formatting identical for every question. I always opened a new tab for each question.

154
00:14:31,360 --> 00:14:36,240
It wasn't looking at the context of what it had already put out. Each attempt was fresh,

155
00:14:36,240 --> 00:14:41,760
aside from the resolver model, which looked at the context of the researcher's output. And again,

156
00:14:41,760 --> 00:14:47,680
as you can see from example 14, it wasn't like the researcher could always spot the errors or that

157
00:14:47,680 --> 00:14:53,520
the resolver could always pick the right option. Sometimes the let's think step by step prompt

158
00:14:53,520 --> 00:14:59,280
gave the right output, but the resolver couldn't quite distinguish it. The optimized prompt gets

159
00:14:59,280 --> 00:15:05,760
a slightly better output. And upon reflection, the researcher can sometimes but not always spot

160
00:15:05,760 --> 00:15:12,160
the errors of those outputs. And sometimes but not always the resolver can spot based on those

161
00:15:12,160 --> 00:15:18,400
flaws, which answer is best. These are incremental improvements. Sometimes GPT-4 simply can't get

162
00:15:18,400 --> 00:15:23,200
it right. I have noticed a few themes in those questions. Anytime it comes to division,

163
00:15:23,200 --> 00:15:29,280
multiplication, characters, or counting in general, GPT-4 tends to make mistakes that

164
00:15:29,280 --> 00:15:35,360
neither the researcher nor resolver can spot. Of course, integrating a few tools via API would

165
00:15:35,360 --> 00:15:40,480
likely solve those issues. And I don't want to preempt the conclusion too much, but I believe a

166
00:15:40,480 --> 00:15:49,920
smart GPT-like system with tools integrated could probably score around 95% right now on the MMLU,

167
00:15:49,920 --> 00:15:55,120
especially if it was helped out with a few shot prompting. To add weight to that preliminary

168
00:15:55,120 --> 00:16:00,800
conclusion, I tested it on certain topics and had to stop because it simply got the questions right

169
00:16:00,800 --> 00:16:06,480
every single time. For example, high school psychology from the MMLU. I then tried prehistory,

170
00:16:06,480 --> 00:16:11,120
which it also aced before finding machine learning where I got more interesting results.

171
00:16:11,120 --> 00:16:17,280
Zooming in this time, the raw score was 65%. The chain of thought let's think step by step average

172
00:16:17,280 --> 00:16:23,840
was 71.6% and the resolver model got 80%. Let's now look a little deeper into why all of these

173
00:16:23,840 --> 00:16:29,520
steps might improve the end result. In reply to the original let's think step by step paper,

174
00:16:29,520 --> 00:16:34,880
which was published around a year ago, Andrea Carpathi said this. Adding something like let's

175
00:16:34,880 --> 00:16:41,600
think step by step to the prompt is a way of using the input space for computation that you'd normally

176
00:16:41,600 --> 00:16:46,800
want in the hidden state of the model. Instead of the workings out being done in the activations

177
00:16:46,800 --> 00:16:53,440
of the neural network, it's done in the discrete tokens of that input space. And he adds did not

178
00:16:53,440 --> 00:16:58,320
super see this coming. And here is the paper released three days ago that improves upon

179
00:16:58,320 --> 00:17:04,800
that original prompt. They also did their testing zero shot like me. And they tested many prompts

180
00:17:04,800 --> 00:17:10,960
starting like I did with just direct prompting, just asking the question like 99% of users would

181
00:17:10,960 --> 00:17:16,720
do of GPT four. And then they tried like me the well established let's think step by step

182
00:17:16,720 --> 00:17:22,160
prompt. They also iteratively tested seven original prompts, as well as the prompt that I've now

183
00:17:22,160 --> 00:17:28,080
integrated into smart GPT the let's work this out in a step by step way, etc. They share my opinion

184
00:17:28,080 --> 00:17:34,480
that zero shot prompting setups have the benefit of not requiring such task dependent selection

185
00:17:34,480 --> 00:17:39,600
of exemplars. You don't have to find correct examples. It just does it all for you. Here are

186
00:17:39,600 --> 00:17:45,120
the end results for GPT four that we saw earlier showing the difference between asking directly

187
00:17:45,120 --> 00:17:50,160
your question and using these refined prompts. Notice that this technique is somewhat model

188
00:17:50,160 --> 00:17:55,280
dependent. And it doesn't have the same effect on smaller or weaker models. Before we move on

189
00:17:55,280 --> 00:18:00,080
to the next paper, there is one somewhat failed prompt that I want to pick up on. It's this

190
00:18:00,080 --> 00:18:04,560
self critique prompt where they ask answer the question, then critique the answer based on the

191
00:18:04,560 --> 00:18:09,040
critique, reconsider the other answer options, and give a single final answer. And you might

192
00:18:09,040 --> 00:18:14,720
wonder why didn't that prompt perform best when we know that reflection and dialogue can work?

193
00:18:14,720 --> 00:18:20,720
My theory is because it's trying to do all of it in one prompt. Through my hundreds of experiments,

194
00:18:20,720 --> 00:18:26,160
I've noticed that GPT four can only handle so much in one go. It simply gets overwhelmed or

195
00:18:26,160 --> 00:18:32,240
confused if you ask it to do too much in one prompt. That's why I broke my model into stages to allow

196
00:18:32,240 --> 00:18:37,520
it to show off each of its abilities one by one. And before we get to the other papers, what's my

197
00:18:37,520 --> 00:18:43,280
personal theory as to why this eliminates up to half of the errors that GPT four makes? Well,

198
00:18:43,280 --> 00:18:50,160
my guess is this. Remember that GPT four is drawing on a vast data set of internet text. And let me

199
00:18:50,160 --> 00:18:56,720
ask you what kind of text has things like question, answer, let's work this out. Be sure we have the

200
00:18:56,720 --> 00:19:02,800
right answer. The kind of data that would have that text would be things like tutorials or expert

201
00:19:02,880 --> 00:19:08,560
breakdowns. So I believe you're triggering more of the weights inside GPT four that relate to

202
00:19:08,560 --> 00:19:13,840
things like expert tutorials. And so inevitably you're getting slightly better answers. Next,

203
00:19:13,840 --> 00:19:18,800
I've already explained why you get different outputs when you give the exact same prompt.

204
00:19:18,800 --> 00:19:23,680
That's down to sampling and the temperature of the model. But to simplify massively, sometimes

205
00:19:23,680 --> 00:19:29,920
GPT four will give you an output that it knows isn't the most probable. It introduces some randomness

206
00:19:29,920 --> 00:19:34,960
into its sampling by generating multiple outputs, you're getting a larger sample size,

207
00:19:34,960 --> 00:19:41,040
reflecting the full range of probabilities that GPT four ascribes to its outputs, you're reducing

208
00:19:41,040 --> 00:19:46,800
a little bit some of the randomness that's inherent in GPT four outputs. Next, I believe that GPT

209
00:19:46,800 --> 00:19:52,960
four can sometimes spot its own errors through reflection, because prompting like this triggers

210
00:19:52,960 --> 00:19:57,920
a different set of weights, you could almost think of it as a different mindset, one more focused on

211
00:19:57,920 --> 00:20:02,320
finding errors. Again, if the question is too hard or involves counting characters,

212
00:20:02,320 --> 00:20:07,040
division, multiplication, as I said earlier, this won't help. But a percentage of the time it can

213
00:20:07,040 --> 00:20:12,400
spot its own errors and point them out. Notice this is a separate bit of inference not lumped into

214
00:20:12,400 --> 00:20:17,520
the original prompt. And when it does successfully point out the errors, it can often engage in

215
00:20:17,520 --> 00:20:23,040
this dialogue with itself. Notice in a meta kind of way, I'm using the step by step prompting

216
00:20:23,120 --> 00:20:27,760
to improve the reflection and dialogue. So those are my theories as to why it works,

217
00:20:27,760 --> 00:20:32,160
but at the end of the video, I'm going to show you at least five ways I think the model can be

218
00:20:32,160 --> 00:20:38,000
further refined. Before we do, though, I looked up the paper by Joe, which produced that prompt that

219
00:20:38,000 --> 00:20:43,040
did the best in the previous paper, they came to that special prompt through automatic prompt

220
00:20:43,040 --> 00:20:46,640
engineering. But there's something interesting I want to point out, though, on page seven,

221
00:20:46,640 --> 00:20:53,280
they say we use automatic prompt engineering to find a prompt starting with let's that maximizes

222
00:20:53,280 --> 00:20:58,000
the likelihood of correct reasoning steps. Then they found the best one that I integrated into

223
00:20:58,000 --> 00:21:02,240
smart GPT. Let's work this out in a step by step way to be sure we have the right answer. That's

224
00:21:02,240 --> 00:21:07,120
the one I want you to use. And they ran their own benchmarks. And of course, it did improve

225
00:21:07,120 --> 00:21:12,560
the scores. But the interesting thing to me is they started with let's each time. So even that

226
00:21:12,560 --> 00:21:17,680
first stage for the model might not yet be fully optimized. Maybe there's a prompt that doesn't

227
00:21:17,680 --> 00:21:22,800
begin with let's that improves this initial results still further. Anyway, back to the papers,

228
00:21:22,800 --> 00:21:28,320
I know many people watching this will wonder if I read the paper boosting theory of mind performance

229
00:21:28,320 --> 00:21:33,280
in large language models via prompting. And yes, I did because they tested something similar for a

230
00:21:33,280 --> 00:21:38,320
theory of mind test. Using similar techniques, they were able to get theory of mind accuracy for

231
00:21:38,320 --> 00:21:45,200
GPT four from 80% to 100%. And they conclude that these results demonstrate that appropriate

232
00:21:45,200 --> 00:21:51,040
prompting enhances large language model theory of mind reasoning. And they underscore the context

233
00:21:51,040 --> 00:21:56,240
dependent nature of these models cognitive capacities. They use that original prompt,

234
00:21:56,240 --> 00:22:02,240
let's think step by step, along with some few shot examples. Take a look at the GPT four table. And

235
00:22:02,240 --> 00:22:07,680
you can see how the let's think step by step improved the results dramatically. And as I

236
00:22:07,680 --> 00:22:13,520
theorized earlier, adding few shot examples would push this still further. This is part of why I think

237
00:22:13,520 --> 00:22:20,240
that 95% barrier on the MMLU will be broken probably this year by GPT four, a few other

238
00:22:20,240 --> 00:22:25,920
points from this paper. They admit that there is not currently a theoretical understanding of why

239
00:22:25,920 --> 00:22:31,120
these prompting techniques are beneficial. I've given you my theory and car pathies, but no one

240
00:22:31,120 --> 00:22:36,240
quite knows for sure. Lastly from this paper, and I found this really interesting, giving it generic

241
00:22:36,320 --> 00:22:42,640
few shot prompts that weren't directly theory of mind actually improve the outputs slightly more

242
00:22:42,640 --> 00:22:48,320
than giving it direct theory of mind examples. This opens the door to the first of the five ways

243
00:22:48,320 --> 00:22:53,440
I anticipate smart GPT getting even smarter. It could be possible to come up with generic

244
00:22:53,440 --> 00:22:58,800
few shot prompts that could be automatically integrated into the model that don't necessarily

245
00:22:58,800 --> 00:23:04,480
relate to the topic at hand. This graph shows the impact of adding few shot examples to GPT three.

246
00:23:04,480 --> 00:23:10,960
And if this can be done in a generic way for GPT four, results could be improved still further.

247
00:23:10,960 --> 00:23:17,120
Next, the boosting theory of mind paper speculates that integrating some of these approaches could

248
00:23:17,120 --> 00:23:24,000
boost the performance of weaker models to beyond the levels of GPT four zero shot accuracy. Next,

249
00:23:24,000 --> 00:23:29,520
here is the original DERA paper that inspired me to have the researcher and resolver dialogue at

250
00:23:29,520 --> 00:23:35,680
the end of smart GPT. As they say, the DERA approach shows significant improvement over base GPT

251
00:23:35,680 --> 00:23:40,400
four performance. And these were open ended questions, by the way, not multiple choice. So

252
00:23:40,400 --> 00:23:44,960
this is more generally applicable than you might think. You can see from this table how results

253
00:23:44,960 --> 00:23:50,400
improved after engaging in this dialogue. And that brings me to the second way I anticipate smart

254
00:23:50,400 --> 00:23:56,320
GPT getting smarter in the future, a longer and more rich dialogue. At the moment, we have this

255
00:23:56,320 --> 00:24:03,040
simple researcher and resolver two step dialogue. I can imagine a council of advisors, you can imagine

256
00:24:03,040 --> 00:24:08,400
a mathematician chipping in and a philosopher and a professor, each one tapping into slightly

257
00:24:08,400 --> 00:24:14,320
different weights of GPT four, extracting more hidden expertise. I'm not saying that would

258
00:24:14,320 --> 00:24:19,840
transform the results, but it might edge them another few percent higher. Next, even with longer

259
00:24:19,840 --> 00:24:24,880
dialogues and different experts, we could find ways of optimizing these prompts just like we did

260
00:24:24,880 --> 00:24:29,760
with the original let's think step by step. That's the third avenue of improvement that I envisaged

261
00:24:29,760 --> 00:24:34,240
because I came up with these prompts, I'm sure they could be improved. Next, we could experiment

262
00:24:34,240 --> 00:24:39,120
with different temperatures. Remember, a lower temperature makes the model more conservative,

263
00:24:39,120 --> 00:24:44,240
a higher one towards one makes it more creative. We could experiment with a higher temperature

264
00:24:44,240 --> 00:24:49,760
to produce a more diverse range of outputs at this stage, and then perhaps a more conservative,

265
00:24:49,840 --> 00:24:55,280
deterministic temperature for the final judge or resolver. It might not work, but it's worth

266
00:24:55,280 --> 00:25:01,440
trying. And the fifth improvement I know would work, integrating APIs for character counting,

267
00:25:01,440 --> 00:25:07,840
calculators, code interpreters, etc. Spending these weeks manually sorting through the outputs of GPT

268
00:25:07,840 --> 00:25:13,440
four on these benchmarks, I can really see where it goes wrong. And it's often by getting letters

269
00:25:13,440 --> 00:25:18,240
in the wrong order or making mistakes with division, it gets the high level logic right,

270
00:25:18,240 --> 00:25:23,200
and then makes quite simple errors. Basic tool integration would I am sure push the results

271
00:25:23,200 --> 00:25:28,080
still higher. Now, I know this isn't my usual video. And trust me, I have been following the AI

272
00:25:28,080 --> 00:25:33,120
news and we'll get back to that very soon. I'm determined to make those improvements and push

273
00:25:33,120 --> 00:25:39,600
smart GBT even further. But of course, that will be aided massively by getting access to the plugins

274
00:25:39,600 --> 00:25:45,440
and the GPT four API key. So far, I've had to do all of this manually, which was a lot of work.

275
00:25:45,440 --> 00:25:50,960
Now, as you saw earlier, I have drawn on GPT four to help me develop a program in replete to

276
00:25:50,960 --> 00:25:56,720
automate this process. But at the moment, it's GPT 3.5. And honestly, the context window really

277
00:25:56,720 --> 00:26:01,920
limits the ability. But I do look forward to the day when I can integrate GPT four and put this out

278
00:26:01,920 --> 00:26:06,880
as an automatic model for people to test and play about with. I'm sure that something similar will

279
00:26:06,880 --> 00:26:13,360
ultimately be incorporated by open AI itself, maybe as a thoughtful mode or smart mode, a bit

280
00:26:13,360 --> 00:26:18,880
like Bing has creative, precise balance, etc. Each response does take longer. But as you've seen,

281
00:26:18,880 --> 00:26:25,120
the outputs are noticeably better. If the results of models like this one do officially exceed the

282
00:26:25,120 --> 00:26:32,080
86.4% that open AI talked about in the GPT four technical report, I do think that would reveal

283
00:26:32,080 --> 00:26:37,280
quite a few things. First, the open AI isn't even aware of the full capabilities of its own

284
00:26:37,280 --> 00:26:42,080
model. I don't even know if they anticipated things like auto GPT. I do think it would reveal

285
00:26:42,080 --> 00:26:46,640
that they need to do far more proper testing of their models before they release them. They should

286
00:26:46,640 --> 00:26:52,080
make falsifiable predictions about what their models won't be capable of. That way we would

287
00:26:52,080 --> 00:26:56,880
know just how much they know about their own models. What we're trying to avoid is a situation

288
00:26:56,880 --> 00:27:01,520
where open AI say their model can only achieve X. And then when they release the model in the

289
00:27:01,520 --> 00:27:07,440
wild, someone comes along and achieves Y, where Y is much more impactful than X. So those were the

290
00:27:07,440 --> 00:27:12,560
goals of this video to show you how to get more out of GPT four to run you through some of the

291
00:27:12,560 --> 00:27:17,120
fascinating papers that have been released in the last few days and weeks. The third goal was to

292
00:27:17,120 --> 00:27:22,160
show you what this model could do with some official benchmarks and suggest ways it might get better

293
00:27:22,160 --> 00:27:27,920
in the near term future. Of course, if you have a GPT four API key or are an expert in benchmarking

294
00:27:27,920 --> 00:27:32,880
systems like GPT four, I'd love to hear from you. I guess the final goal was to perhaps suggest to you

295
00:27:32,880 --> 00:27:37,760
that open AI don't know as much about their own models as they might lead you to believe.

296
00:27:37,760 --> 00:27:41,120
Thank you so much for watching to the end and have a wonderful day.

