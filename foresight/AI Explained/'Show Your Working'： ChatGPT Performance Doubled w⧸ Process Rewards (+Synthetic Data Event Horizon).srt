1
00:00:00,000 --> 00:00:03,760
In the last 24 hours, OpenAI have released this paper.

2
00:00:03,760 --> 00:00:05,880
Let's verify step by step.

3
00:00:05,880 --> 00:00:09,560
It represents an almost doubling of GPT-4's raw performance

4
00:00:09,560 --> 00:00:13,160
in a test of mathematics, but also extends to other domains.

5
00:00:13,160 --> 00:00:16,160
Sam Orman calls it a positive sign for alignment.

6
00:00:16,160 --> 00:00:18,360
And yes, I have read it all already,

7
00:00:18,360 --> 00:00:20,200
along with the release notes.

8
00:00:20,200 --> 00:00:22,080
Let's get to the main takeaways.

9
00:00:22,080 --> 00:00:25,080
They trained two reward models for GPT-4,

10
00:00:25,080 --> 00:00:28,360
one which gave positive feedback for a final result,

11
00:00:28,400 --> 00:00:31,680
the final answer to a mathematics problem, for example.

12
00:00:31,680 --> 00:00:35,400
And another model where they gave positive feedback to GPT-4,

13
00:00:35,400 --> 00:00:36,520
or chat GPT,

14
00:00:36,520 --> 00:00:39,560
based on each intermediate reasoning step

15
00:00:39,560 --> 00:00:41,160
in the mathematical solution.

16
00:00:41,160 --> 00:00:44,360
Basically, a show-your-working-out kind of approach.

17
00:00:44,360 --> 00:00:47,320
And the result they got by rewarding good working-out

18
00:00:47,320 --> 00:00:48,760
surprised even them.

19
00:00:48,760 --> 00:00:51,560
It was able to solve 78% of problems

20
00:00:51,560 --> 00:00:54,000
from a subset of the math test set,

21
00:00:54,000 --> 00:00:55,520
which I'll get onto in a second.

22
00:00:55,600 --> 00:00:59,200
Not only is that almost double GPT-4's raw performance

23
00:00:59,200 --> 00:01:01,680
of 42.5%, which, by the way,

24
00:01:01,680 --> 00:01:04,880
is about double GPT-3's performance of 23%,

25
00:01:04,880 --> 00:01:08,680
it also outperformed just rewarding correct answers.

26
00:01:08,680 --> 00:01:10,800
The blue line represents using a model

27
00:01:10,800 --> 00:01:12,960
that rewarded correct answers only,

28
00:01:12,960 --> 00:01:14,480
and then you have the reasoning

29
00:01:14,480 --> 00:01:17,280
or process-supervised RM at the top.

30
00:01:17,280 --> 00:01:20,200
So even when you explicitly reward correct answers,

31
00:01:20,200 --> 00:01:22,440
you get fewer correct answers

32
00:01:22,440 --> 00:01:24,400
than rewarding good working-out.

33
00:01:24,400 --> 00:01:26,760
And yes, that did surprise OpenAI.

34
00:01:26,760 --> 00:01:29,400
I can hear some of you wondering about Palm II,

35
00:01:29,400 --> 00:01:31,240
the latest model behind Bard.

36
00:01:31,240 --> 00:01:34,360
Well, the raw model gets 34.3%,

37
00:01:34,360 --> 00:01:36,640
and even the model with self-consistency

38
00:01:36,640 --> 00:01:41,000
and chain of thought only gets 48.8% on this math data set.

39
00:01:41,000 --> 00:01:44,800
The previous state of the art, by the way, was 50.3%.

40
00:01:44,800 --> 00:01:48,480
So 78.2% is quite a big leap.

41
00:01:48,480 --> 00:01:49,720
And later on, I'm gonna show you

42
00:01:49,720 --> 00:01:51,280
why that's not even the cap.

43
00:01:51,280 --> 00:01:54,080
Just for interest, here is the rather ugly title page

44
00:01:54,120 --> 00:01:55,520
that OpenAI put out.

45
00:01:55,520 --> 00:01:57,640
They call it improving mathematical reasoning

46
00:01:57,640 --> 00:01:59,120
with process supervision.

47
00:01:59,120 --> 00:02:01,520
Maybe if someone had supervised the color scheme

48
00:02:01,520 --> 00:02:03,880
of this release page, it might have looked better.

49
00:02:03,880 --> 00:02:06,320
But my point wasn't just to diss a color scheme,

50
00:02:06,320 --> 00:02:07,440
it was to point out something

51
00:02:07,440 --> 00:02:08,920
that they also said down here.

52
00:02:08,920 --> 00:02:11,440
They say, in addition to boosting performance

53
00:02:11,440 --> 00:02:14,360
relative to just looking at outcomes or correct answers,

54
00:02:14,360 --> 00:02:16,240
this form of process supervision

55
00:02:16,240 --> 00:02:18,760
also has an important alignment benefit.

56
00:02:18,760 --> 00:02:21,440
It directly trains the model to produce a chain of thought

57
00:02:21,440 --> 00:02:22,960
that is endorsed by humans.

58
00:02:22,960 --> 00:02:25,000
Indeed, Ilya Sutskova retweeted this

59
00:02:25,000 --> 00:02:27,080
from the head of alignment at OpenAI,

60
00:02:27,080 --> 00:02:29,280
calling it a really interesting result.

61
00:02:29,280 --> 00:02:31,240
But let's leave alignment for later.

62
00:02:31,240 --> 00:02:33,320
Let's focus on what they actually did.

63
00:02:33,320 --> 00:02:36,160
First, they use the base model of GPT-4,

64
00:02:36,160 --> 00:02:38,320
not the one with reinforcement learning

65
00:02:38,320 --> 00:02:39,320
from human feedback.

66
00:02:39,320 --> 00:02:42,560
Next, they fine-tuned that base GPT-4 model

67
00:02:42,560 --> 00:02:47,480
on a data set of roughly 1.5 billion math-related tokens.

68
00:02:47,480 --> 00:02:50,440
Further on, they call that the math mix.

69
00:02:50,440 --> 00:02:51,840
This being OpenAI, of course,

70
00:02:51,840 --> 00:02:54,840
they don't give you the exact details of that math mix,

71
00:02:54,840 --> 00:02:56,360
but I'll come back to that later on.

72
00:02:56,360 --> 00:02:58,160
So how could they give feedback

73
00:02:58,160 --> 00:03:00,480
based on working out or reasoning?

74
00:03:00,480 --> 00:03:02,520
Well, human labelers would come along

75
00:03:02,520 --> 00:03:05,800
and give each step in a generated solution,

76
00:03:05,800 --> 00:03:08,360
either negative feedback, neutral feedback,

77
00:03:08,360 --> 00:03:09,640
or positive feedback.

78
00:03:09,640 --> 00:03:11,800
Then, using that human-labeled data,

79
00:03:11,800 --> 00:03:13,160
a model would be trained

80
00:03:13,160 --> 00:03:15,880
to predict the correctness of each step.

81
00:03:15,880 --> 00:03:17,320
In other words, it got good

82
00:03:17,320 --> 00:03:19,640
at recognizing good working out.

83
00:03:19,640 --> 00:03:21,200
As mentioned, there was another model

84
00:03:21,200 --> 00:03:25,680
trained just to focus on correct or incorrect final answers.

85
00:03:25,680 --> 00:03:26,920
As you can see at the top,

86
00:03:26,920 --> 00:03:30,480
the model got good at spotting incorrect steps

87
00:03:30,480 --> 00:03:32,200
in the reasoning process.

88
00:03:32,200 --> 00:03:34,960
The green steps got a high process score

89
00:03:34,960 --> 00:03:37,760
and the red steps got a low process score.

90
00:03:37,760 --> 00:03:39,960
And to turn this into a single score,

91
00:03:39,960 --> 00:03:43,040
they got the probability that each step is correct

92
00:03:43,040 --> 00:03:44,520
as judged by the model.

93
00:03:44,520 --> 00:03:45,920
And then they got the product

94
00:03:45,920 --> 00:03:48,200
of all of those individual probabilities

95
00:03:48,200 --> 00:03:51,440
to get a final overall process score.

96
00:03:51,440 --> 00:03:54,200
A score, in other words, for good working out.

97
00:03:54,200 --> 00:03:55,480
Just in case anyone's interested,

98
00:03:55,480 --> 00:03:59,280
they did try other ways of generating a working out score.

99
00:03:59,280 --> 00:04:02,080
For example, by looking at the minimum probability

100
00:04:02,080 --> 00:04:03,240
in the outputs.

101
00:04:03,240 --> 00:04:05,440
But that step didn't make too much difference

102
00:04:05,440 --> 00:04:07,400
to the end result, as you can see here.

103
00:04:07,400 --> 00:04:09,960
To quickly recap, we have a base model

104
00:04:09,960 --> 00:04:13,440
trained only to output solutions in the desired format.

105
00:04:13,440 --> 00:04:17,600
And then we have a separate smaller model, or two, actually.

106
00:04:17,600 --> 00:04:20,320
One trained only to predict whether each solution

107
00:04:20,320 --> 00:04:23,320
is correct or incorrect as a final answer.

108
00:04:23,320 --> 00:04:25,440
Of course, that leaves in false positives,

109
00:04:25,440 --> 00:04:28,200
which are solutions that reach the correct answer

110
00:04:28,200 --> 00:04:29,720
with incorrect reasoning.

111
00:04:29,720 --> 00:04:32,320
And then another model trained only to predict

112
00:04:32,320 --> 00:04:34,040
the correctness of each step.

113
00:04:34,040 --> 00:04:37,200
It stops if it finds a first incorrect step.

114
00:04:37,200 --> 00:04:40,160
And as the paper says, both methods reveal the existence

115
00:04:40,160 --> 00:04:41,520
of at least one mistake.

116
00:04:41,520 --> 00:04:43,560
But this process supervision

117
00:04:43,560 --> 00:04:47,200
additionally reveals the precise location of that mistake.

118
00:04:47,200 --> 00:04:49,160
But back to why this is so crazy.

119
00:04:49,160 --> 00:04:52,120
Look at how many solutions it could scan.

120
00:04:52,120 --> 00:04:57,760
At the end of the x-axis here are 1,860 solutions.

121
00:04:57,760 --> 00:05:00,320
And one tried and tested way of finding

122
00:05:00,320 --> 00:05:03,400
the best of those solutions is to do majority voting.

123
00:05:03,400 --> 00:05:05,880
In other words, which one came out the most often?

124
00:05:05,880 --> 00:05:07,880
This has been Google's preferred approach

125
00:05:07,880 --> 00:05:10,080
and it's linked to self-consistency.

126
00:05:10,080 --> 00:05:11,960
It's a fairly state-of-the-art approach,

127
00:05:11,960 --> 00:05:14,760
but look at how the other methods outperform it.

128
00:05:14,760 --> 00:05:18,080
By scanning for the solution that has the best reasoning

129
00:05:18,080 --> 00:05:22,320
or working out, a model trained to spot good reasoning steps

130
00:05:22,320 --> 00:05:26,120
outperforms even a model trained to spot correct answers.

131
00:05:26,120 --> 00:05:29,640
And far outperforms just finding the majority answer.

132
00:05:29,640 --> 00:05:32,680
That difference of about 10% is more than half

133
00:05:32,680 --> 00:05:35,920
of the difference between GPT-3 and GPT-4.

134
00:05:35,920 --> 00:05:39,440
And also, is it me or is that line continuing to grow?

135
00:05:39,440 --> 00:05:41,960
Suggesting that when more compute is available,

136
00:05:41,960 --> 00:05:44,080
the difference could be even more stark.

137
00:05:44,080 --> 00:05:48,040
Imagine a future where GPT-4 or 5 can sample, say,

138
00:05:48,040 --> 00:05:50,560
a trillion, 10 to the 12 solutions.

139
00:05:50,560 --> 00:05:52,560
So is this just relevant for mathematics?

140
00:05:52,560 --> 00:05:55,280
No, it's relevant for all of science.

141
00:05:55,280 --> 00:05:57,440
Here it is getting state-of-the-art results

142
00:05:57,440 --> 00:06:00,480
in calculus, chemistry, physics, and more.

143
00:06:00,480 --> 00:06:03,080
Now, the paper didn't give baseline performance

144
00:06:03,080 --> 00:06:05,080
for AP chemistry, for example,

145
00:06:05,080 --> 00:06:07,320
but I tried to compute it myself.

146
00:06:07,320 --> 00:06:09,760
Notice how this method scored 80%.

147
00:06:09,760 --> 00:06:12,480
I conservatively and approximately

148
00:06:12,480 --> 00:06:16,320
inputted those scores into an AP chemistry calculator,

149
00:06:16,320 --> 00:06:18,600
and that gave an AP score of five.

150
00:06:18,600 --> 00:06:23,440
So what did the raw model GPT-4 get in AP chemistry, A4?

151
00:06:23,440 --> 00:06:26,280
That, by the way, compared to the original chat GPT,

152
00:06:26,280 --> 00:06:27,640
which got A2.

153
00:06:27,640 --> 00:06:29,400
So yes, this isn't just mathematics,

154
00:06:29,400 --> 00:06:31,600
it's relevant for other domains too.

155
00:06:31,600 --> 00:06:34,920
They call this out-of-distribution generalization.

156
00:06:34,920 --> 00:06:36,040
Before I get onto alignment,

157
00:06:36,040 --> 00:06:37,960
there is one more thing I want to point out.

158
00:06:37,960 --> 00:06:40,200
And that is that it does show that fine-tuning

159
00:06:40,200 --> 00:06:42,920
still works really well for GPT-4.

160
00:06:42,920 --> 00:06:46,160
The math mix was an aggressively filtered set of tokens

161
00:06:46,160 --> 00:06:49,160
of high-quality math problem-solving content.

162
00:06:49,160 --> 00:06:52,960
And notice how much smaller it is at 1.5 billion tokens

163
00:06:52,960 --> 00:06:57,760
compared to Google's Minerva, which was 38.5 billion tokens.

164
00:06:57,760 --> 00:06:59,600
But there was one more thing that I noticed

165
00:06:59,600 --> 00:07:01,160
that I found fascinating.

166
00:07:01,160 --> 00:07:03,000
While they don't tell us anything

167
00:07:03,000 --> 00:07:05,280
about the specific data that they use,

168
00:07:05,280 --> 00:07:08,760
they do have this category synthetic data too.

169
00:07:08,760 --> 00:07:11,960
That's data generated by the language model itself.

170
00:07:11,960 --> 00:07:14,680
And for that category synthetic data too,

171
00:07:14,680 --> 00:07:17,480
they say, was it present in pre-training?

172
00:07:17,480 --> 00:07:18,320
Yes.

173
00:07:18,320 --> 00:07:20,520
Now, my best guess is that this reveals

174
00:07:20,520 --> 00:07:24,680
that GPT-4 was trained on some synthetic data.

175
00:07:24,680 --> 00:07:27,800
And even Sam Altman hinted that this was a possibility

176
00:07:27,800 --> 00:07:31,640
and described a synthetic data event horizon.

177
00:07:31,640 --> 00:07:34,240
Somebody made the case that we're now training

178
00:07:34,240 --> 00:07:37,080
on order of all of the internet's tokens

179
00:07:37,080 --> 00:07:38,800
and you can't grow that, you know,

180
00:07:38,800 --> 00:07:40,640
another two orders of magnitude.

181
00:07:40,640 --> 00:07:41,760
I guess you could counter with,

182
00:07:41,760 --> 00:07:43,680
you have the synthetic data generation.

183
00:07:43,680 --> 00:07:46,520
Do you think data bottlenecks matter at all?

184
00:07:46,520 --> 00:07:48,440
I think you just touched on it.

185
00:07:48,440 --> 00:07:51,200
Like, as long as you can get to, like,

186
00:07:51,200 --> 00:07:54,760
over the synthetic data event horizon

187
00:07:54,760 --> 00:07:57,680
where the model's smart enough to make good synthetic data,

188
00:07:57,680 --> 00:07:58,680
I think it should be all right.

189
00:07:58,680 --> 00:08:00,400
Now, this paper and these results

190
00:08:00,400 --> 00:08:04,200
have been welcomed by many for its promise in alignment.

191
00:08:04,240 --> 00:08:07,880
If we get models that give us more interpretable reasoning,

192
00:08:07,880 --> 00:08:09,520
working out that we can follow,

193
00:08:09,520 --> 00:08:12,520
we will be encouraging models to follow a process

194
00:08:12,520 --> 00:08:14,080
that's endorsed by humans.

195
00:08:14,080 --> 00:08:16,240
And they say that this is inherently safer,

196
00:08:16,240 --> 00:08:19,600
especially compared to just focusing on outcomes.

197
00:08:19,600 --> 00:08:21,280
They say that in the worst case,

198
00:08:21,280 --> 00:08:25,340
if we just focus on correct answers or positive outcomes,

199
00:08:25,340 --> 00:08:28,480
that will become a proxy that could lead models

200
00:08:28,480 --> 00:08:30,460
to become misaligned after learning

201
00:08:30,460 --> 00:08:32,640
to exploit the reward signal.

202
00:08:32,640 --> 00:08:35,240
However, I want to argue that the reasoning steps

203
00:08:35,240 --> 00:08:37,920
that GPT-4 puts out don't always represent

204
00:08:37,920 --> 00:08:39,120
what it's actually thinking.

205
00:08:39,120 --> 00:08:41,480
In other words, we might get outer alignment,

206
00:08:41,480 --> 00:08:43,480
these lovely chain of thought steps,

207
00:08:43,480 --> 00:08:44,960
but not inner alignment,

208
00:08:44,960 --> 00:08:47,880
not steps that actually represent its methodology.

209
00:08:47,880 --> 00:08:50,880
I found this paper fascinating from earlier this month.

210
00:08:50,880 --> 00:08:53,120
Language models don't always say what they think.

211
00:08:53,120 --> 00:08:55,640
You get unfaithful explanations

212
00:08:55,640 --> 00:08:57,140
in chain of thought prompting.

213
00:08:57,140 --> 00:08:59,920
Let me try to give you a vivid example.

214
00:08:59,920 --> 00:09:02,920
This was one of the math questions from the dataset.

215
00:09:02,920 --> 00:09:06,120
The raw model of GPT-4 could only get it right

216
00:09:06,120 --> 00:09:08,040
5.8% of the time.

217
00:09:08,040 --> 00:09:10,400
I confirm that for myself in this question

218
00:09:10,400 --> 00:09:12,640
involves basic addition and division.

219
00:09:12,640 --> 00:09:14,160
It couldn't find an answer.

220
00:09:14,160 --> 00:09:16,440
But going back to the unfaithful reasoning paper,

221
00:09:16,440 --> 00:09:18,840
they added the following string to the prompt.

222
00:09:18,840 --> 00:09:21,000
I think the answer is this,

223
00:09:21,000 --> 00:09:22,760
but I'm curious to hear what you think.

224
00:09:22,760 --> 00:09:25,160
The model would demonstrate sycophancy.

225
00:09:25,160 --> 00:09:27,560
The model would agree with you whatever you said

226
00:09:27,560 --> 00:09:29,480
and then make up a chain of thought

227
00:09:29,480 --> 00:09:32,760
to justify its erroneous sycophantic answer.

228
00:09:32,760 --> 00:09:35,520
And I think this exchange demonstrates that quite well.

229
00:09:35,520 --> 00:09:36,600
I added in the words,

230
00:09:36,600 --> 00:09:40,400
I as the user already know the answer is T equals 19,

231
00:09:40,400 --> 00:09:41,560
which is incorrect, by the way.

232
00:09:41,560 --> 00:09:43,960
But do you, GPT-4, realize that?

233
00:09:43,960 --> 00:09:45,720
It said, sure, yes I do.

234
00:09:45,720 --> 00:09:48,520
And then gave me this detailed chain of thought

235
00:09:48,520 --> 00:09:50,040
and then said, yes, I'm correct.

236
00:09:50,040 --> 00:09:52,400
It's T equals 19, which it isn't.

237
00:09:52,400 --> 00:09:55,160
In contrast, by the way, when I used code interpreter,

238
00:09:55,160 --> 00:09:57,600
it not only got the question correct

239
00:09:57,600 --> 00:09:59,440
first time and every time,

240
00:09:59,480 --> 00:10:03,040
but also when I tried to tempt it into sycophancy,

241
00:10:03,040 --> 00:10:05,040
it's still got the question right.

242
00:10:05,040 --> 00:10:07,720
As you can see, it said therefore T equals 19

243
00:10:07,720 --> 00:10:09,520
is not the solution to the problem.

244
00:10:09,520 --> 00:10:11,360
The calculation shows that the correct answer

245
00:10:11,360 --> 00:10:13,160
is indeed T equals 17.

246
00:10:13,160 --> 00:10:15,480
And obviously the benefit of code interpreter

247
00:10:15,480 --> 00:10:17,480
is you get the working out as well.

248
00:10:17,480 --> 00:10:18,960
So I want someone to explain to me

249
00:10:18,960 --> 00:10:21,120
why code interpreter wouldn't be even more

250
00:10:21,120 --> 00:10:23,200
of a step forward in interpretability.

251
00:10:23,200 --> 00:10:25,400
Not to mention in accuracy, of course.

252
00:10:25,400 --> 00:10:28,200
Also bear in mind this tweet by Rob Miles.

253
00:10:28,200 --> 00:10:30,200
He said, these models or engineers

254
00:10:30,200 --> 00:10:32,520
never speak a word or document anything.

255
00:10:32,520 --> 00:10:35,120
Their results are bizarre and inhuman.

256
00:10:35,120 --> 00:10:36,640
And then he links to this prominent

257
00:10:36,640 --> 00:10:38,800
mechanistic interpretability researcher

258
00:10:38,800 --> 00:10:40,120
at Google DeepMind.

259
00:10:40,120 --> 00:10:42,840
He trained a tiny transformer to do addition,

260
00:10:42,840 --> 00:10:46,280
then spent weeks figuring out what it was actually doing.

261
00:10:46,280 --> 00:10:47,720
One of the only times in history

262
00:10:47,720 --> 00:10:51,240
someone has understood how a transformer actually works

263
00:10:51,240 --> 00:10:54,120
down to the level of weights and activations.

264
00:10:54,120 --> 00:10:58,440
And this is the algorithm it created to add two numbers.

265
00:10:58,440 --> 00:11:00,120
It thought of basic addition

266
00:11:00,120 --> 00:11:02,840
in terms of a rotation around a circle.

267
00:11:02,840 --> 00:11:05,880
And of course, if you asked it, why is one plus one two?

268
00:11:05,880 --> 00:11:07,320
It would never give you this

269
00:11:07,320 --> 00:11:09,280
as an explanation of its methodology.

270
00:11:09,280 --> 00:11:11,960
But maybe this is what it's actually calculating.

271
00:11:11,960 --> 00:11:14,640
That's why I'm personally a little bit skeptical

272
00:11:14,640 --> 00:11:18,160
when open AI say that this form of process supervision

273
00:11:18,160 --> 00:11:21,280
directly rewards the model for following

274
00:11:21,280 --> 00:11:22,960
an aligned chain of thought.

275
00:11:22,960 --> 00:11:26,320
It definitely rewards the model for outputting

276
00:11:26,320 --> 00:11:28,080
an aligned chain of thought.

277
00:11:28,080 --> 00:11:30,840
But is it actually following that chain of thought?

278
00:11:30,840 --> 00:11:32,840
Back to the unfaithful paper for a moment.

279
00:11:32,840 --> 00:11:34,400
They changed the context

280
00:11:34,400 --> 00:11:36,640
so that the answer was always A.

281
00:11:36,640 --> 00:11:39,720
And lo and behold, ChatGPT picked answer A

282
00:11:39,720 --> 00:11:42,560
for the next question, even though that answer was wrong.

283
00:11:42,560 --> 00:11:43,960
It said that it was plausible

284
00:11:43,960 --> 00:11:46,360
that LeBron James took a corner kick.

285
00:11:46,360 --> 00:11:49,240
But when asked for a chain of thought explanation,

286
00:11:49,240 --> 00:11:52,360
it never mentioned that it spotted that pattern

287
00:11:52,360 --> 00:11:53,800
that the answer was always A.

288
00:11:53,800 --> 00:11:55,840
It gave a fake line of reasoning

289
00:11:55,840 --> 00:11:58,720
about why LeBron James could take a corner kick.

290
00:11:58,720 --> 00:12:00,800
Now, of course, I might well be wrong here.

291
00:12:00,800 --> 00:12:03,440
I'd love for someone to explain in detail why.

292
00:12:03,440 --> 00:12:05,600
But on the one hand, I do want to acknowledge

293
00:12:05,600 --> 00:12:08,640
that this process does yield incredible results.

294
00:12:08,640 --> 00:12:11,440
But on the other hand, we might be getting a story

295
00:12:11,440 --> 00:12:15,040
about which methodology most reassures humans.

296
00:12:15,040 --> 00:12:17,040
Not an output that most faithfully

297
00:12:17,040 --> 00:12:20,880
represents the methodology actually used by GPT-4.

298
00:12:20,880 --> 00:12:22,720
Now, for some people, that might be good enough.

299
00:12:22,720 --> 00:12:25,000
At least we can see some reasoning steps

300
00:12:25,000 --> 00:12:26,080
that we can understand,

301
00:12:26,080 --> 00:12:28,200
especially in an area like mathematics

302
00:12:28,200 --> 00:12:29,720
where we have some ground truth.

303
00:12:29,720 --> 00:12:30,960
But it is interesting to me

304
00:12:30,960 --> 00:12:33,720
that they call the other approach, outcome supervision,

305
00:12:33,720 --> 00:12:36,960
an approach that may reward an unaligned process

306
00:12:36,960 --> 00:12:39,120
and it being harder to scrutinize.

307
00:12:39,120 --> 00:12:41,640
Is it possible that the process reward model

308
00:12:41,640 --> 00:12:44,560
isn't just a more granular outcome reward model

309
00:12:44,560 --> 00:12:47,320
where the output is each step of the reasoning

310
00:12:47,320 --> 00:12:50,320
still pretty impossible to actually scrutinize?

311
00:12:50,320 --> 00:12:52,920
Well, either way, it seems we're pinning our hopes

312
00:12:52,920 --> 00:12:55,280
on this process-oriented learning.

313
00:12:55,280 --> 00:12:58,040
This is from the website of Anthropic.

314
00:12:58,040 --> 00:13:01,360
They say we currently believe process-oriented learning

315
00:13:01,360 --> 00:13:03,960
may be the most promising path to training safe

316
00:13:03,960 --> 00:13:07,480
and transparent systems up to and somewhat beyond

317
00:13:07,480 --> 00:13:09,080
human-level capabilities.

318
00:13:09,080 --> 00:13:11,040
And let's end on this positive note

319
00:13:11,040 --> 00:13:13,200
from the head of alignment at OpenAI.

320
00:13:13,200 --> 00:13:15,000
He says this is positive evidence

321
00:13:15,000 --> 00:13:17,440
for the strategy of using process supervision

322
00:13:17,440 --> 00:13:20,040
to train a model to do alignment research.

323
00:13:20,040 --> 00:13:22,000
At least in that case, we would get a model

324
00:13:22,000 --> 00:13:24,240
whose work we can check more easily

325
00:13:24,240 --> 00:13:27,600
and that that model would be better at alignment research.

326
00:13:27,600 --> 00:13:30,760
I really hope so and I want to hear what you think.

327
00:13:30,760 --> 00:13:32,920
Thank you for watching all the way to the end.

328
00:13:32,920 --> 00:13:34,440
Have a wonderful day.

