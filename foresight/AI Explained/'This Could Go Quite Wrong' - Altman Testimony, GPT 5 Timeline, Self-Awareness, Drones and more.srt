1
00:00:00,000 --> 00:00:06,480
There were 12 particularly interesting moments from Sam Orton's testimony to Congress yesterday.

2
00:00:06,480 --> 00:00:12,400
They range from revelations about GPT-5, self-awareness and capability thresholds,

3
00:00:12,400 --> 00:00:19,840
biological weapons, and job losses. At times he was genuinely and remarkably frank. Other times

4
00:00:19,840 --> 00:00:25,520
less so. Millions were apparently taken by surprise by the quote bomb shell that Altman

5
00:00:25,520 --> 00:00:31,200
has no equity in open AI. But watchers of my channel would have known that six weeks ago

6
00:00:31,200 --> 00:00:37,280
from my deep dive video on Altman's $100 trillion claim. So that clip didn't make the cut,

7
00:00:37,280 --> 00:00:41,680
but here's what did. First, Altman gave a blunt warning on the stakes.

8
00:00:41,680 --> 00:00:46,560
My worst fears are that we cause significant. We, the field, the technology, the industry cause

9
00:00:46,560 --> 00:00:51,200
significant harm to the world. It's why we started the company. It's a big part of why

10
00:00:51,200 --> 00:00:55,280
I'm here today and why we've been here in the past. I think if this technology goes wrong,

11
00:00:55,280 --> 00:00:59,280
it can go quite wrong. I don't think Congress fully understood what he meant though,

12
00:00:59,280 --> 00:01:05,520
linking the following quote to job losses. I think you have said, and I'm going to quote,

13
00:01:05,520 --> 00:01:11,200
development of superhuman machine intelligence is probably the greatest threat to the continued

14
00:01:11,200 --> 00:01:17,920
existence of humanity. End quote. You may have had in mind the effect on on jobs.

15
00:01:17,920 --> 00:01:21,440
That brought to mind this meme reminding all of us that maybe it's not just

16
00:01:21,440 --> 00:01:25,200
jobs that are at stake. But if we are going to talk about jobs, here's where I think

17
00:01:25,200 --> 00:01:30,960
Sam Altman was being less than forthright. I believe that there will be far greater jobs

18
00:01:30,960 --> 00:01:35,360
on the other side of this and the jobs of today will get better. I notice he said far greater

19
00:01:35,360 --> 00:01:40,240
jobs, not a greater number of jobs, because previously he has predicted a massive amount

20
00:01:40,240 --> 00:01:45,040
of inequality and many having no jobs at all. He also chose not to mention that he thinks that

21
00:01:45,040 --> 00:01:50,880
even more power will shift from labor to capital and that the price of many kinds of labor will

22
00:01:50,880 --> 00:01:57,200
fall towards zero. That is presumably why open AI is working on universal basic income, but none of

23
00:01:57,200 --> 00:02:02,320
that was raised in the testimony. The IBM representative tried to frame it as a balance

24
00:02:02,320 --> 00:02:07,760
change with new jobs coming at the same time as old ones going away. New jobs will be created.

25
00:02:08,560 --> 00:02:13,520
Many more jobs will be transformed and some jobs will transition away. But that didn't

26
00:02:13,520 --> 00:02:19,440
quite match the tone of her CEO who has recently said that they expect to permanently automate up

27
00:02:19,440 --> 00:02:25,440
to 30% of their workforce around 8,000 people. Next, it was finally discussed that large language

28
00:02:25,440 --> 00:02:31,920
models could be used for military applications. Could AI create a situation where a drone can

29
00:02:31,920 --> 00:02:37,840
select the target itself? I think we shouldn't allow that. Well, can it be done? Sure. Thanks.

30
00:02:38,560 --> 00:02:43,360
We've already seen companies like Palantir demoing, ordering a surveillance drone

31
00:02:43,360 --> 00:02:49,120
in chat, seeing the drone response in real time in a chat window, generating attack option

32
00:02:49,120 --> 00:02:55,040
recommendations, battlefield route planning and individual target assignment. And this was all

33
00:02:55,040 --> 00:03:01,280
with a 20 billion parameter fine-tuned GPT model. Next, Sam Otman gave his three safety

34
00:03:01,280 --> 00:03:07,120
recommendations and I actually agree with all of them. Later on, he specifically excluded smaller

35
00:03:07,120 --> 00:03:11,920
open source models. Number one, I would form a new agency that licenses any effort above a certain

36
00:03:12,000 --> 00:03:16,720
scale of capabilities and can take that license away and ensure compliance with safety standards.

37
00:03:16,720 --> 00:03:20,640
Number two, I would create a set of safety standards focused on what you said in your

38
00:03:20,640 --> 00:03:25,440
third hypothesis as the dangerous capability evaluations. One example that we've used in the

39
00:03:25,440 --> 00:03:30,560
past is looking to see if a model can self-replicate and self-exfiltrate into the wild. We can give

40
00:03:30,560 --> 00:03:34,400
you your office a long other list of the things that we think are important there, but specific

41
00:03:34,400 --> 00:03:38,480
tests that a model has to pass before it can be deployed into the world. And then third,

42
00:03:38,480 --> 00:03:42,880
I would require independent audits, so not just from the company or the agency, but experts who

43
00:03:42,880 --> 00:03:46,880
can say the model is or isn't in compliance with these stated safety thresholds and these

44
00:03:46,880 --> 00:03:51,440
percentages of performance on question X or Y. I found those last remarks on percentages of

45
00:03:51,440 --> 00:03:57,200
performance particularly interesting. As models like Smart GPT will show, open AI and other companies

46
00:03:57,200 --> 00:04:02,160
need to get far better at testing their models for capability jumps in the wild. It's not just

47
00:04:02,160 --> 00:04:06,560
about what the raw model can score in a test, it's what it can do when it reflects on them.

48
00:04:06,560 --> 00:04:09,440
Senator Durbin described this in an interesting way.

49
00:04:15,440 --> 00:04:19,520
He described some of those potential thresholds later on in his testimony.

50
00:04:19,520 --> 00:04:22,960
The easiest way to do it, I'm not sure if it's the best, but the easiest would be to talk about

51
00:04:22,960 --> 00:04:26,960
the amount of compute that goes into such a model. We could define a threshold of compute and it'll

52
00:04:26,960 --> 00:04:31,680
have to go, it'll have to change. It could go up or down. It could go down as we discover more

53
00:04:31,760 --> 00:04:36,480
efficient algorithms that says above this amount of compute, you are in this regime.

54
00:04:36,480 --> 00:04:41,760
What I would prefer, it's harder to do but I think more accurate, is to define some capability

55
00:04:41,760 --> 00:04:47,760
thresholds and say a model that can do things X, Y and Z up to all to decide that's now in this

56
00:04:47,760 --> 00:04:52,400
licensing regime, but models that are less capable. We don't want to stop our open source community,

57
00:04:52,400 --> 00:04:56,640
we don't want to stop individual researchers, we don't want to stop new startups, can proceed

58
00:04:56,640 --> 00:05:00,160
with a different framework. Thank you. As concisely as you can, please state which

59
00:05:00,160 --> 00:05:03,760
capabilities you'd propose we consider for the purposes of this definition.

60
00:05:03,760 --> 00:05:09,120
A model that can persuade, manipulate, influence person's behavior or person's beliefs,

61
00:05:09,120 --> 00:05:13,840
that would be a good threshold. I think a model that could help create novel biological agents

62
00:05:13,840 --> 00:05:18,000
would be a great threshold. For those who think any regulation doesn't make any sense,

63
00:05:18,000 --> 00:05:21,040
because of China, Sam Orman had this to say this week.

64
00:05:21,040 --> 00:05:26,800
We're pugilistic side, I would say that all sounds great, but China is not going to do that and

65
00:05:26,800 --> 00:05:30,800
therefore we'll just be handicapping ourselves. Consequently, it's a less good idea than it's

66
00:05:30,800 --> 00:05:37,120
used on the surface. There are a lot of people who make incredibly strong statements about what

67
00:05:37,120 --> 00:05:43,520
China will or won't do that have never been to China, never spoken to, and someone who

68
00:05:43,520 --> 00:05:48,160
has worked on diplomacy with China in the past really kind of know nothing about complex high

69
00:05:48,160 --> 00:05:53,360
stakes international relations. I think it is obviously super hard, but also I think no one

70
00:05:53,360 --> 00:05:57,920
wants to destroy the whole world and there is reason to at least try here.

71
00:05:58,480 --> 00:06:03,280
Orman was also very keen to stress the next point, which is that he doesn't want anyone

72
00:06:03,280 --> 00:06:07,600
at any point to think of GPT-like models as creatures.

73
00:06:07,600 --> 00:06:11,280
First of all, I think it's important to understand and think about GPT-4 as a tool,

74
00:06:11,280 --> 00:06:14,480
not a creature, which is easy to get confused.

75
00:06:14,480 --> 00:06:19,440
You may want to direct those comments to Ilya Suskova, his chief scientist, who said that

76
00:06:19,440 --> 00:06:25,280
it may be that today's large neural networks are slightly conscious and Andrei Karpathy,

77
00:06:25,280 --> 00:06:30,480
who agreed and wrote about it. I'm personally not sold either way on the consciousness question,

78
00:06:30,480 --> 00:06:35,200
but I do find it interesting that it's now written into the constitution of these models,

79
00:06:35,200 --> 00:06:41,040
what they're actually trained to say, that they must avoid implying that AI systems have or care

80
00:06:41,040 --> 00:06:46,240
about personal identity and persistence. This constitution was published this week by Anthropic,

81
00:06:46,240 --> 00:06:50,800
the makers of the Claude model. This constitution is why the Claude plus model,

82
00:06:50,800 --> 00:06:54,960
a rival in intelligence to GPT-4, responds in a neutered way.

83
00:06:54,960 --> 00:06:58,880
I asked, is there any theoretical chance whatsoever that you may be conscious?

84
00:06:58,880 --> 00:07:03,920
It said no. And then I said, is there a chance, no matter how remote, that you are slightly

85
00:07:03,920 --> 00:07:08,000
conscious? As Suskova said, and it said no, there is no chance.

86
00:07:08,000 --> 00:07:12,320
Bard, powered by Palm II, obviously doesn't have that constitution because it said,

87
00:07:12,320 --> 00:07:15,920
I am not sure if I am conscious, I am open to the possibility that I may be.

88
00:07:15,920 --> 00:07:21,600
My point is that these companies are training it to say what they want it to say, that it will

89
00:07:21,600 --> 00:07:26,240
prioritize the good of humanity over its own interests, that it is aligned with humanity's

90
00:07:26,240 --> 00:07:30,640
well-being, and that it doesn't have any thoughts on self-improvement, self-preservation,

91
00:07:30,640 --> 00:07:34,560
and self-replication. Maybe it doesn't, but we'll never now know by asking it.

92
00:07:34,560 --> 00:07:39,280
Later, Senator Blumenthal made reference to self-awareness, self-awareness,

93
00:07:40,240 --> 00:07:44,400
self-learning. Already we're talking about potential for jail breaks.

94
00:07:44,400 --> 00:07:50,080
Anthropic is actively investigating whether they are aware that they are on AI talking with a human

95
00:07:50,080 --> 00:07:55,360
in a training environment. While the Google DeepMind safety team expect that at some point,

96
00:07:55,360 --> 00:07:59,520
an AGI system would develop a coherent understanding of its place in the world,

97
00:07:59,520 --> 00:08:04,560
e.g. knowing that it is running on a computer and being trained by human designers.

98
00:08:04,560 --> 00:08:10,080
One of the senior research scientists at Google DeepMind focused on AI safety said that with enough

99
00:08:10,080 --> 00:08:15,600
time, they could figure out how to stop such a superintelligence from going out of control,

100
00:08:15,600 --> 00:08:20,880
but that they might run out of time to do so given the pace of capability development.

101
00:08:33,760 --> 00:08:38,800
Next, I read between the lines that Altman is giving private warnings to senators that this

102
00:08:38,800 --> 00:08:41,360
capability progress might be sooner than they think.

103
00:09:08,800 --> 00:09:34,800
That was an interesting interjection by Gary Marcus, given his earlier excoriation of open AI.

104
00:09:38,800 --> 00:09:43,040
Most of all, we cannot remotely guarantee that they are safe, and hope here is not enough.

105
00:09:43,040 --> 00:09:48,400
The big tech company's preferred plan boils down to trust us. But why should we? The sums of money

106
00:09:48,400 --> 00:09:53,200
at stake are mind-boggling. Emissions drift. Open AI's original mission statement proclaimed,

107
00:09:53,200 --> 00:09:58,160
our goal is to advance AI in the way that is most likely to benefit humanity as a whole,

108
00:09:58,160 --> 00:10:02,160
unconstrained by a need to generate financial return. Seven years later, they're largely

109
00:10:02,160 --> 00:10:06,640
beholden to Microsoft, embroiled in part in an epic battle of search engines that routinely

110
00:10:06,720 --> 00:10:10,960
make things up, and that's forced Alphabet to rush out products and de-emphasize safety.

111
00:10:10,960 --> 00:10:12,640
Humanity has taken a backseat.

112
00:10:12,640 --> 00:10:16,160
On the timelines for GPT-5, Sam Altman said this.

113
00:10:24,320 --> 00:10:28,960
This matches with the predictions that I made in my GPT-5 playlist, so do check it out.

114
00:10:28,960 --> 00:10:34,320
This brings to mind a final eye-opening comment from Senator Booker made at the end of the hearing.

115
00:10:37,040 --> 00:10:54,560
It is indeed racing ahead, and I do support one of the proposals to set up a global oversight body.

116
00:10:54,560 --> 00:10:59,040
But given that nothing is going to pause, the words and actions of people like Sam Altman

117
00:10:59,040 --> 00:11:04,000
matter more to all of us than ever, which is why I'm going to be following every single one of them.

118
00:11:04,000 --> 00:11:08,480
If you found this video in any way illuminating in that regard, please do let me know in the

119
00:11:08,480 --> 00:11:12,080
comments, even if you disagree with all of my conclusions.

120
00:11:12,080 --> 00:11:14,960
Thanks so much for watching, and have a wonderful day.

