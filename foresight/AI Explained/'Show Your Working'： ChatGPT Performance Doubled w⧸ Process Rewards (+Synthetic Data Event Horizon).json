{"text": " In the last 24 hours, OpenAI have released this paper. Let's verify step by step. It represents an almost doubling of GPT-4's raw performance in a test of mathematics, but also extends to other domains. Sam Orman calls it a positive sign for alignment. And yes, I have read it all already, along with the release notes. Let's get to the main takeaways. They trained two reward models for GPT-4, one which gave positive feedback for a final result, the final answer to a mathematics problem, for example. And another model where they gave positive feedback to GPT-4, or chat GPT, based on each intermediate reasoning step in the mathematical solution. Basically, a show-your-working-out kind of approach. And the result they got by rewarding good working-out surprised even them. It was able to solve 78% of problems from a subset of the math test set, which I'll get onto in a second. Not only is that almost double GPT-4's raw performance of 42.5%, which, by the way, is about double GPT-3's performance of 23%, it also outperformed just rewarding correct answers. The blue line represents using a model that rewarded correct answers only, and then you have the reasoning or process-supervised RM at the top. So even when you explicitly reward correct answers, you get fewer correct answers than rewarding good working-out. And yes, that did surprise OpenAI. I can hear some of you wondering about Palm II, the latest model behind Bard. Well, the raw model gets 34.3%, and even the model with self-consistency and chain of thought only gets 48.8% on this math data set. The previous state of the art, by the way, was 50.3%. So 78.2% is quite a big leap. And later on, I'm gonna show you why that's not even the cap. Just for interest, here is the rather ugly title page that OpenAI put out. They call it improving mathematical reasoning with process supervision. Maybe if someone had supervised the color scheme of this release page, it might have looked better. But my point wasn't just to diss a color scheme, it was to point out something that they also said down here. They say, in addition to boosting performance relative to just looking at outcomes or correct answers, this form of process supervision also has an important alignment benefit. It directly trains the model to produce a chain of thought that is endorsed by humans. Indeed, Ilya Sutskova retweeted this from the head of alignment at OpenAI, calling it a really interesting result. But let's leave alignment for later. Let's focus on what they actually did. First, they use the base model of GPT-4, not the one with reinforcement learning from human feedback. Next, they fine-tuned that base GPT-4 model on a data set of roughly 1.5 billion math-related tokens. Further on, they call that the math mix. This being OpenAI, of course, they don't give you the exact details of that math mix, but I'll come back to that later on. So how could they give feedback based on working out or reasoning? Well, human labelers would come along and give each step in a generated solution, either negative feedback, neutral feedback, or positive feedback. Then, using that human-labeled data, a model would be trained to predict the correctness of each step. In other words, it got good at recognizing good working out. As mentioned, there was another model trained just to focus on correct or incorrect final answers. As you can see at the top, the model got good at spotting incorrect steps in the reasoning process. The green steps got a high process score and the red steps got a low process score. And to turn this into a single score, they got the probability that each step is correct as judged by the model. And then they got the product of all of those individual probabilities to get a final overall process score. A score, in other words, for good working out. Just in case anyone's interested, they did try other ways of generating a working out score. For example, by looking at the minimum probability in the outputs. But that step didn't make too much difference to the end result, as you can see here. To quickly recap, we have a base model trained only to output solutions in the desired format. And then we have a separate smaller model, or two, actually. One trained only to predict whether each solution is correct or incorrect as a final answer. Of course, that leaves in false positives, which are solutions that reach the correct answer with incorrect reasoning. And then another model trained only to predict the correctness of each step. It stops if it finds a first incorrect step. And as the paper says, both methods reveal the existence of at least one mistake. But this process supervision additionally reveals the precise location of that mistake. But back to why this is so crazy. Look at how many solutions it could scan. At the end of the x-axis here are 1,860 solutions. And one tried and tested way of finding the best of those solutions is to do majority voting. In other words, which one came out the most often? This has been Google's preferred approach and it's linked to self-consistency. It's a fairly state-of-the-art approach, but look at how the other methods outperform it. By scanning for the solution that has the best reasoning or working out, a model trained to spot good reasoning steps outperforms even a model trained to spot correct answers. And far outperforms just finding the majority answer. That difference of about 10% is more than half of the difference between GPT-3 and GPT-4. And also, is it me or is that line continuing to grow? Suggesting that when more compute is available, the difference could be even more stark. Imagine a future where GPT-4 or 5 can sample, say, a trillion, 10 to the 12 solutions. So is this just relevant for mathematics? No, it's relevant for all of science. Here it is getting state-of-the-art results in calculus, chemistry, physics, and more. Now, the paper didn't give baseline performance for AP chemistry, for example, but I tried to compute it myself. Notice how this method scored 80%. I conservatively and approximately inputted those scores into an AP chemistry calculator, and that gave an AP score of five. So what did the raw model GPT-4 get in AP chemistry, A4? That, by the way, compared to the original chat GPT, which got A2. So yes, this isn't just mathematics, it's relevant for other domains too. They call this out-of-distribution generalization. Before I get onto alignment, there is one more thing I want to point out. And that is that it does show that fine-tuning still works really well for GPT-4. The math mix was an aggressively filtered set of tokens of high-quality math problem-solving content. And notice how much smaller it is at 1.5 billion tokens compared to Google's Minerva, which was 38.5 billion tokens. But there was one more thing that I noticed that I found fascinating. While they don't tell us anything about the specific data that they use, they do have this category synthetic data too. That's data generated by the language model itself. And for that category synthetic data too, they say, was it present in pre-training? Yes. Now, my best guess is that this reveals that GPT-4 was trained on some synthetic data. And even Sam Altman hinted that this was a possibility and described a synthetic data event horizon. Somebody made the case that we're now training on order of all of the internet's tokens and you can't grow that, you know, another two orders of magnitude. I guess you could counter with, you have the synthetic data generation. Do you think data bottlenecks matter at all? I think you just touched on it. Like, as long as you can get to, like, over the synthetic data event horizon where the model's smart enough to make good synthetic data, I think it should be all right. Now, this paper and these results have been welcomed by many for its promise in alignment. If we get models that give us more interpretable reasoning, working out that we can follow, we will be encouraging models to follow a process that's endorsed by humans. And they say that this is inherently safer, especially compared to just focusing on outcomes. They say that in the worst case, if we just focus on correct answers or positive outcomes, that will become a proxy that could lead models to become misaligned after learning to exploit the reward signal. However, I want to argue that the reasoning steps that GPT-4 puts out don't always represent what it's actually thinking. In other words, we might get outer alignment, these lovely chain of thought steps, but not inner alignment, not steps that actually represent its methodology. I found this paper fascinating from earlier this month. Language models don't always say what they think. You get unfaithful explanations in chain of thought prompting. Let me try to give you a vivid example. This was one of the math questions from the dataset. The raw model of GPT-4 could only get it right 5.8% of the time. I confirm that for myself in this question involves basic addition and division. It couldn't find an answer. But going back to the unfaithful reasoning paper, they added the following string to the prompt. I think the answer is this, but I'm curious to hear what you think. The model would demonstrate sycophancy. The model would agree with you whatever you said and then make up a chain of thought to justify its erroneous sycophantic answer. And I think this exchange demonstrates that quite well. I added in the words, I as the user already know the answer is T equals 19, which is incorrect, by the way. But do you, GPT-4, realize that? It said, sure, yes I do. And then gave me this detailed chain of thought and then said, yes, I'm correct. It's T equals 19, which it isn't. In contrast, by the way, when I used code interpreter, it not only got the question correct first time and every time, but also when I tried to tempt it into sycophancy, it's still got the question right. As you can see, it said therefore T equals 19 is not the solution to the problem. The calculation shows that the correct answer is indeed T equals 17. And obviously the benefit of code interpreter is you get the working out as well. So I want someone to explain to me why code interpreter wouldn't be even more of a step forward in interpretability. Not to mention in accuracy, of course. Also bear in mind this tweet by Rob Miles. He said, these models or engineers never speak a word or document anything. Their results are bizarre and inhuman. And then he links to this prominent mechanistic interpretability researcher at Google DeepMind. He trained a tiny transformer to do addition, then spent weeks figuring out what it was actually doing. One of the only times in history someone has understood how a transformer actually works down to the level of weights and activations. And this is the algorithm it created to add two numbers. It thought of basic addition in terms of a rotation around a circle. And of course, if you asked it, why is one plus one two? It would never give you this as an explanation of its methodology. But maybe this is what it's actually calculating. That's why I'm personally a little bit skeptical when open AI say that this form of process supervision directly rewards the model for following an aligned chain of thought. It definitely rewards the model for outputting an aligned chain of thought. But is it actually following that chain of thought? Back to the unfaithful paper for a moment. They changed the context so that the answer was always A. And lo and behold, ChatGPT picked answer A for the next question, even though that answer was wrong. It said that it was plausible that LeBron James took a corner kick. But when asked for a chain of thought explanation, it never mentioned that it spotted that pattern that the answer was always A. It gave a fake line of reasoning about why LeBron James could take a corner kick. Now, of course, I might well be wrong here. I'd love for someone to explain in detail why. But on the one hand, I do want to acknowledge that this process does yield incredible results. But on the other hand, we might be getting a story about which methodology most reassures humans. Not an output that most faithfully represents the methodology actually used by GPT-4. Now, for some people, that might be good enough. At least we can see some reasoning steps that we can understand, especially in an area like mathematics where we have some ground truth. But it is interesting to me that they call the other approach, outcome supervision, an approach that may reward an unaligned process and it being harder to scrutinize. Is it possible that the process reward model isn't just a more granular outcome reward model where the output is each step of the reasoning still pretty impossible to actually scrutinize? Well, either way, it seems we're pinning our hopes on this process-oriented learning. This is from the website of Anthropic. They say we currently believe process-oriented learning may be the most promising path to training safe and transparent systems up to and somewhat beyond human-level capabilities. And let's end on this positive note from the head of alignment at OpenAI. He says this is positive evidence for the strategy of using process supervision to train a model to do alignment research. At least in that case, we would get a model whose work we can check more easily and that that model would be better at alignment research. I really hope so and I want to hear what you think. Thank you for watching all the way to the end. Have a wonderful day.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.7600000000000002, "text": " In the last 24 hours, OpenAI have released this paper.", "tokens": [50364, 682, 264, 1036, 4022, 2496, 11, 7238, 48698, 362, 4736, 341, 3035, 13, 50552], "temperature": 0.0, "avg_logprob": -0.13668844776768838, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0009545062202960253}, {"id": 1, "seek": 0, "start": 3.7600000000000002, "end": 5.88, "text": " Let's verify step by step.", "tokens": [50552, 961, 311, 16888, 1823, 538, 1823, 13, 50658], "temperature": 0.0, "avg_logprob": -0.13668844776768838, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0009545062202960253}, {"id": 2, "seek": 0, "start": 5.88, "end": 9.56, "text": " It represents an almost doubling of GPT-4's raw performance", "tokens": [50658, 467, 8855, 364, 1920, 33651, 295, 26039, 51, 12, 19, 311, 8936, 3389, 50842], "temperature": 0.0, "avg_logprob": -0.13668844776768838, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0009545062202960253}, {"id": 3, "seek": 0, "start": 9.56, "end": 13.16, "text": " in a test of mathematics, but also extends to other domains.", "tokens": [50842, 294, 257, 1500, 295, 18666, 11, 457, 611, 26448, 281, 661, 25514, 13, 51022], "temperature": 0.0, "avg_logprob": -0.13668844776768838, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0009545062202960253}, {"id": 4, "seek": 0, "start": 13.16, "end": 16.16, "text": " Sam Orman calls it a positive sign for alignment.", "tokens": [51022, 4832, 1610, 1601, 5498, 309, 257, 3353, 1465, 337, 18515, 13, 51172], "temperature": 0.0, "avg_logprob": -0.13668844776768838, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0009545062202960253}, {"id": 5, "seek": 0, "start": 16.16, "end": 18.36, "text": " And yes, I have read it all already,", "tokens": [51172, 400, 2086, 11, 286, 362, 1401, 309, 439, 1217, 11, 51282], "temperature": 0.0, "avg_logprob": -0.13668844776768838, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0009545062202960253}, {"id": 6, "seek": 0, "start": 18.36, "end": 20.2, "text": " along with the release notes.", "tokens": [51282, 2051, 365, 264, 4374, 5570, 13, 51374], "temperature": 0.0, "avg_logprob": -0.13668844776768838, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0009545062202960253}, {"id": 7, "seek": 0, "start": 20.2, "end": 22.080000000000002, "text": " Let's get to the main takeaways.", "tokens": [51374, 961, 311, 483, 281, 264, 2135, 45584, 13, 51468], "temperature": 0.0, "avg_logprob": -0.13668844776768838, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0009545062202960253}, {"id": 8, "seek": 0, "start": 22.080000000000002, "end": 25.080000000000002, "text": " They trained two reward models for GPT-4,", "tokens": [51468, 814, 8895, 732, 7782, 5245, 337, 26039, 51, 12, 19, 11, 51618], "temperature": 0.0, "avg_logprob": -0.13668844776768838, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0009545062202960253}, {"id": 9, "seek": 0, "start": 25.080000000000002, "end": 28.36, "text": " one which gave positive feedback for a final result,", "tokens": [51618, 472, 597, 2729, 3353, 5824, 337, 257, 2572, 1874, 11, 51782], "temperature": 0.0, "avg_logprob": -0.13668844776768838, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0009545062202960253}, {"id": 10, "seek": 2836, "start": 28.4, "end": 31.68, "text": " the final answer to a mathematics problem, for example.", "tokens": [50366, 264, 2572, 1867, 281, 257, 18666, 1154, 11, 337, 1365, 13, 50530], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 11, "seek": 2836, "start": 31.68, "end": 35.4, "text": " And another model where they gave positive feedback to GPT-4,", "tokens": [50530, 400, 1071, 2316, 689, 436, 2729, 3353, 5824, 281, 26039, 51, 12, 19, 11, 50716], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 12, "seek": 2836, "start": 35.4, "end": 36.519999999999996, "text": " or chat GPT,", "tokens": [50716, 420, 5081, 26039, 51, 11, 50772], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 13, "seek": 2836, "start": 36.519999999999996, "end": 39.56, "text": " based on each intermediate reasoning step", "tokens": [50772, 2361, 322, 1184, 19376, 21577, 1823, 50924], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 14, "seek": 2836, "start": 39.56, "end": 41.16, "text": " in the mathematical solution.", "tokens": [50924, 294, 264, 18894, 3827, 13, 51004], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 15, "seek": 2836, "start": 41.16, "end": 44.36, "text": " Basically, a show-your-working-out kind of approach.", "tokens": [51004, 8537, 11, 257, 855, 12, 23093, 12, 22475, 12, 346, 733, 295, 3109, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 16, "seek": 2836, "start": 44.36, "end": 47.32, "text": " And the result they got by rewarding good working-out", "tokens": [51164, 400, 264, 1874, 436, 658, 538, 20063, 665, 1364, 12, 346, 51312], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 17, "seek": 2836, "start": 47.32, "end": 48.760000000000005, "text": " surprised even them.", "tokens": [51312, 6100, 754, 552, 13, 51384], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 18, "seek": 2836, "start": 48.760000000000005, "end": 51.56, "text": " It was able to solve 78% of problems", "tokens": [51384, 467, 390, 1075, 281, 5039, 26369, 4, 295, 2740, 51524], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 19, "seek": 2836, "start": 51.56, "end": 54.0, "text": " from a subset of the math test set,", "tokens": [51524, 490, 257, 25993, 295, 264, 5221, 1500, 992, 11, 51646], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 20, "seek": 2836, "start": 54.0, "end": 55.519999999999996, "text": " which I'll get onto in a second.", "tokens": [51646, 597, 286, 603, 483, 3911, 294, 257, 1150, 13, 51722], "temperature": 0.0, "avg_logprob": -0.13566107001186403, "compression_ratio": 1.5854545454545454, "no_speech_prob": 0.00394319137558341}, {"id": 21, "seek": 5552, "start": 55.6, "end": 59.2, "text": " Not only is that almost double GPT-4's raw performance", "tokens": [50368, 1726, 787, 307, 300, 1920, 3834, 26039, 51, 12, 19, 311, 8936, 3389, 50548], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 22, "seek": 5552, "start": 59.2, "end": 61.68000000000001, "text": " of 42.5%, which, by the way,", "tokens": [50548, 295, 14034, 13, 20, 8923, 597, 11, 538, 264, 636, 11, 50672], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 23, "seek": 5552, "start": 61.68000000000001, "end": 64.88, "text": " is about double GPT-3's performance of 23%,", "tokens": [50672, 307, 466, 3834, 26039, 51, 12, 18, 311, 3389, 295, 6673, 8923, 50832], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 24, "seek": 5552, "start": 64.88, "end": 68.68, "text": " it also outperformed just rewarding correct answers.", "tokens": [50832, 309, 611, 484, 610, 22892, 445, 20063, 3006, 6338, 13, 51022], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 25, "seek": 5552, "start": 68.68, "end": 70.80000000000001, "text": " The blue line represents using a model", "tokens": [51022, 440, 3344, 1622, 8855, 1228, 257, 2316, 51128], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 26, "seek": 5552, "start": 70.80000000000001, "end": 72.96000000000001, "text": " that rewarded correct answers only,", "tokens": [51128, 300, 29105, 3006, 6338, 787, 11, 51236], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 27, "seek": 5552, "start": 72.96000000000001, "end": 74.48, "text": " and then you have the reasoning", "tokens": [51236, 293, 550, 291, 362, 264, 21577, 51312], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 28, "seek": 5552, "start": 74.48, "end": 77.28, "text": " or process-supervised RM at the top.", "tokens": [51312, 420, 1399, 12, 48172, 24420, 23790, 412, 264, 1192, 13, 51452], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 29, "seek": 5552, "start": 77.28, "end": 80.2, "text": " So even when you explicitly reward correct answers,", "tokens": [51452, 407, 754, 562, 291, 20803, 7782, 3006, 6338, 11, 51598], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 30, "seek": 5552, "start": 80.2, "end": 82.44, "text": " you get fewer correct answers", "tokens": [51598, 291, 483, 13366, 3006, 6338, 51710], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 31, "seek": 5552, "start": 82.44, "end": 84.4, "text": " than rewarding good working-out.", "tokens": [51710, 813, 20063, 665, 1364, 12, 346, 13, 51808], "temperature": 0.0, "avg_logprob": -0.08380678494771322, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00030529440846294165}, {"id": 32, "seek": 8440, "start": 84.4, "end": 86.76, "text": " And yes, that did surprise OpenAI.", "tokens": [50364, 400, 2086, 11, 300, 630, 6365, 7238, 48698, 13, 50482], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 33, "seek": 8440, "start": 86.76, "end": 89.4, "text": " I can hear some of you wondering about Palm II,", "tokens": [50482, 286, 393, 1568, 512, 295, 291, 6359, 466, 32668, 6351, 11, 50614], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 34, "seek": 8440, "start": 89.4, "end": 91.24000000000001, "text": " the latest model behind Bard.", "tokens": [50614, 264, 6792, 2316, 2261, 26841, 13, 50706], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 35, "seek": 8440, "start": 91.24000000000001, "end": 94.36000000000001, "text": " Well, the raw model gets 34.3%,", "tokens": [50706, 1042, 11, 264, 8936, 2316, 2170, 12790, 13, 18, 8923, 50862], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 36, "seek": 8440, "start": 94.36000000000001, "end": 96.64, "text": " and even the model with self-consistency", "tokens": [50862, 293, 754, 264, 2316, 365, 2698, 12, 21190, 468, 3020, 50976], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 37, "seek": 8440, "start": 96.64, "end": 101.0, "text": " and chain of thought only gets 48.8% on this math data set.", "tokens": [50976, 293, 5021, 295, 1194, 787, 2170, 11174, 13, 23, 4, 322, 341, 5221, 1412, 992, 13, 51194], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 38, "seek": 8440, "start": 101.0, "end": 104.80000000000001, "text": " The previous state of the art, by the way, was 50.3%.", "tokens": [51194, 440, 3894, 1785, 295, 264, 1523, 11, 538, 264, 636, 11, 390, 2625, 13, 18, 6856, 51384], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 39, "seek": 8440, "start": 104.80000000000001, "end": 108.48, "text": " So 78.2% is quite a big leap.", "tokens": [51384, 407, 26369, 13, 17, 4, 307, 1596, 257, 955, 19438, 13, 51568], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 40, "seek": 8440, "start": 108.48, "end": 109.72, "text": " And later on, I'm gonna show you", "tokens": [51568, 400, 1780, 322, 11, 286, 478, 799, 855, 291, 51630], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 41, "seek": 8440, "start": 109.72, "end": 111.28, "text": " why that's not even the cap.", "tokens": [51630, 983, 300, 311, 406, 754, 264, 1410, 13, 51708], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 42, "seek": 8440, "start": 111.28, "end": 114.08000000000001, "text": " Just for interest, here is the rather ugly title page", "tokens": [51708, 1449, 337, 1179, 11, 510, 307, 264, 2831, 12246, 4876, 3028, 51848], "temperature": 0.0, "avg_logprob": -0.09350825370626246, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0017542550340294838}, {"id": 43, "seek": 11408, "start": 114.12, "end": 115.52, "text": " that OpenAI put out.", "tokens": [50366, 300, 7238, 48698, 829, 484, 13, 50436], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 44, "seek": 11408, "start": 115.52, "end": 117.64, "text": " They call it improving mathematical reasoning", "tokens": [50436, 814, 818, 309, 11470, 18894, 21577, 50542], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 45, "seek": 11408, "start": 117.64, "end": 119.12, "text": " with process supervision.", "tokens": [50542, 365, 1399, 32675, 13, 50616], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 46, "seek": 11408, "start": 119.12, "end": 121.52, "text": " Maybe if someone had supervised the color scheme", "tokens": [50616, 2704, 498, 1580, 632, 46533, 264, 2017, 12232, 50736], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 47, "seek": 11408, "start": 121.52, "end": 123.88, "text": " of this release page, it might have looked better.", "tokens": [50736, 295, 341, 4374, 3028, 11, 309, 1062, 362, 2956, 1101, 13, 50854], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 48, "seek": 11408, "start": 123.88, "end": 126.32, "text": " But my point wasn't just to diss a color scheme,", "tokens": [50854, 583, 452, 935, 2067, 380, 445, 281, 7802, 257, 2017, 12232, 11, 50976], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 49, "seek": 11408, "start": 126.32, "end": 127.44, "text": " it was to point out something", "tokens": [50976, 309, 390, 281, 935, 484, 746, 51032], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 50, "seek": 11408, "start": 127.44, "end": 128.92, "text": " that they also said down here.", "tokens": [51032, 300, 436, 611, 848, 760, 510, 13, 51106], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 51, "seek": 11408, "start": 128.92, "end": 131.44, "text": " They say, in addition to boosting performance", "tokens": [51106, 814, 584, 11, 294, 4500, 281, 43117, 3389, 51232], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 52, "seek": 11408, "start": 131.44, "end": 134.36, "text": " relative to just looking at outcomes or correct answers,", "tokens": [51232, 4972, 281, 445, 1237, 412, 10070, 420, 3006, 6338, 11, 51378], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 53, "seek": 11408, "start": 134.36, "end": 136.24, "text": " this form of process supervision", "tokens": [51378, 341, 1254, 295, 1399, 32675, 51472], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 54, "seek": 11408, "start": 136.24, "end": 138.76, "text": " also has an important alignment benefit.", "tokens": [51472, 611, 575, 364, 1021, 18515, 5121, 13, 51598], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 55, "seek": 11408, "start": 138.76, "end": 141.44, "text": " It directly trains the model to produce a chain of thought", "tokens": [51598, 467, 3838, 16329, 264, 2316, 281, 5258, 257, 5021, 295, 1194, 51732], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 56, "seek": 11408, "start": 141.44, "end": 142.96, "text": " that is endorsed by humans.", "tokens": [51732, 300, 307, 50094, 538, 6255, 13, 51808], "temperature": 0.0, "avg_logprob": -0.10251530417560661, "compression_ratio": 1.7048192771084338, "no_speech_prob": 0.0010985417757183313}, {"id": 57, "seek": 14296, "start": 142.96, "end": 145.0, "text": " Indeed, Ilya Sutskova retweeted this", "tokens": [50364, 15061, 11, 286, 45106, 318, 3648, 4093, 2757, 1533, 10354, 292, 341, 50466], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 58, "seek": 14296, "start": 145.0, "end": 147.08, "text": " from the head of alignment at OpenAI,", "tokens": [50466, 490, 264, 1378, 295, 18515, 412, 7238, 48698, 11, 50570], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 59, "seek": 14296, "start": 147.08, "end": 149.28, "text": " calling it a really interesting result.", "tokens": [50570, 5141, 309, 257, 534, 1880, 1874, 13, 50680], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 60, "seek": 14296, "start": 149.28, "end": 151.24, "text": " But let's leave alignment for later.", "tokens": [50680, 583, 718, 311, 1856, 18515, 337, 1780, 13, 50778], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 61, "seek": 14296, "start": 151.24, "end": 153.32, "text": " Let's focus on what they actually did.", "tokens": [50778, 961, 311, 1879, 322, 437, 436, 767, 630, 13, 50882], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 62, "seek": 14296, "start": 153.32, "end": 156.16, "text": " First, they use the base model of GPT-4,", "tokens": [50882, 2386, 11, 436, 764, 264, 3096, 2316, 295, 26039, 51, 12, 19, 11, 51024], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 63, "seek": 14296, "start": 156.16, "end": 158.32, "text": " not the one with reinforcement learning", "tokens": [51024, 406, 264, 472, 365, 29280, 2539, 51132], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 64, "seek": 14296, "start": 158.32, "end": 159.32, "text": " from human feedback.", "tokens": [51132, 490, 1952, 5824, 13, 51182], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 65, "seek": 14296, "start": 159.32, "end": 162.56, "text": " Next, they fine-tuned that base GPT-4 model", "tokens": [51182, 3087, 11, 436, 2489, 12, 83, 43703, 300, 3096, 26039, 51, 12, 19, 2316, 51344], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 66, "seek": 14296, "start": 162.56, "end": 167.48000000000002, "text": " on a data set of roughly 1.5 billion math-related tokens.", "tokens": [51344, 322, 257, 1412, 992, 295, 9810, 502, 13, 20, 5218, 5221, 12, 12004, 22667, 13, 51590], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 67, "seek": 14296, "start": 167.48000000000002, "end": 170.44, "text": " Further on, they call that the math mix.", "tokens": [51590, 15364, 322, 11, 436, 818, 300, 264, 5221, 2890, 13, 51738], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 68, "seek": 14296, "start": 170.44, "end": 171.84, "text": " This being OpenAI, of course,", "tokens": [51738, 639, 885, 7238, 48698, 11, 295, 1164, 11, 51808], "temperature": 0.0, "avg_logprob": -0.07685350188126801, "compression_ratio": 1.5870307167235496, "no_speech_prob": 0.0007793045951984823}, {"id": 69, "seek": 17184, "start": 171.84, "end": 174.84, "text": " they don't give you the exact details of that math mix,", "tokens": [50364, 436, 500, 380, 976, 291, 264, 1900, 4365, 295, 300, 5221, 2890, 11, 50514], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 70, "seek": 17184, "start": 174.84, "end": 176.36, "text": " but I'll come back to that later on.", "tokens": [50514, 457, 286, 603, 808, 646, 281, 300, 1780, 322, 13, 50590], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 71, "seek": 17184, "start": 176.36, "end": 178.16, "text": " So how could they give feedback", "tokens": [50590, 407, 577, 727, 436, 976, 5824, 50680], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 72, "seek": 17184, "start": 178.16, "end": 180.48000000000002, "text": " based on working out or reasoning?", "tokens": [50680, 2361, 322, 1364, 484, 420, 21577, 30, 50796], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 73, "seek": 17184, "start": 180.48000000000002, "end": 182.52, "text": " Well, human labelers would come along", "tokens": [50796, 1042, 11, 1952, 7645, 433, 576, 808, 2051, 50898], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 74, "seek": 17184, "start": 182.52, "end": 185.8, "text": " and give each step in a generated solution,", "tokens": [50898, 293, 976, 1184, 1823, 294, 257, 10833, 3827, 11, 51062], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 75, "seek": 17184, "start": 185.8, "end": 188.36, "text": " either negative feedback, neutral feedback,", "tokens": [51062, 2139, 3671, 5824, 11, 10598, 5824, 11, 51190], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 76, "seek": 17184, "start": 188.36, "end": 189.64000000000001, "text": " or positive feedback.", "tokens": [51190, 420, 3353, 5824, 13, 51254], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 77, "seek": 17184, "start": 189.64000000000001, "end": 191.8, "text": " Then, using that human-labeled data,", "tokens": [51254, 1396, 11, 1228, 300, 1952, 12, 75, 18657, 292, 1412, 11, 51362], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 78, "seek": 17184, "start": 191.8, "end": 193.16, "text": " a model would be trained", "tokens": [51362, 257, 2316, 576, 312, 8895, 51430], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 79, "seek": 17184, "start": 193.16, "end": 195.88, "text": " to predict the correctness of each step.", "tokens": [51430, 281, 6069, 264, 3006, 1287, 295, 1184, 1823, 13, 51566], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 80, "seek": 17184, "start": 195.88, "end": 197.32, "text": " In other words, it got good", "tokens": [51566, 682, 661, 2283, 11, 309, 658, 665, 51638], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 81, "seek": 17184, "start": 197.32, "end": 199.64000000000001, "text": " at recognizing good working out.", "tokens": [51638, 412, 18538, 665, 1364, 484, 13, 51754], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 82, "seek": 17184, "start": 199.64000000000001, "end": 201.2, "text": " As mentioned, there was another model", "tokens": [51754, 1018, 2835, 11, 456, 390, 1071, 2316, 51832], "temperature": 0.0, "avg_logprob": -0.09375657280572026, "compression_ratio": 1.7312925170068028, "no_speech_prob": 0.0012064698385074735}, {"id": 83, "seek": 20120, "start": 201.2, "end": 205.67999999999998, "text": " trained just to focus on correct or incorrect final answers.", "tokens": [50364, 8895, 445, 281, 1879, 322, 3006, 420, 18424, 2572, 6338, 13, 50588], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 84, "seek": 20120, "start": 205.67999999999998, "end": 206.92, "text": " As you can see at the top,", "tokens": [50588, 1018, 291, 393, 536, 412, 264, 1192, 11, 50650], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 85, "seek": 20120, "start": 206.92, "end": 210.48, "text": " the model got good at spotting incorrect steps", "tokens": [50650, 264, 2316, 658, 665, 412, 4008, 783, 18424, 4439, 50828], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 86, "seek": 20120, "start": 210.48, "end": 212.2, "text": " in the reasoning process.", "tokens": [50828, 294, 264, 21577, 1399, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 87, "seek": 20120, "start": 212.2, "end": 214.95999999999998, "text": " The green steps got a high process score", "tokens": [50914, 440, 3092, 4439, 658, 257, 1090, 1399, 6175, 51052], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 88, "seek": 20120, "start": 214.95999999999998, "end": 217.76, "text": " and the red steps got a low process score.", "tokens": [51052, 293, 264, 2182, 4439, 658, 257, 2295, 1399, 6175, 13, 51192], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 89, "seek": 20120, "start": 217.76, "end": 219.95999999999998, "text": " And to turn this into a single score,", "tokens": [51192, 400, 281, 1261, 341, 666, 257, 2167, 6175, 11, 51302], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 90, "seek": 20120, "start": 219.95999999999998, "end": 223.04, "text": " they got the probability that each step is correct", "tokens": [51302, 436, 658, 264, 8482, 300, 1184, 1823, 307, 3006, 51456], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 91, "seek": 20120, "start": 223.04, "end": 224.51999999999998, "text": " as judged by the model.", "tokens": [51456, 382, 27485, 538, 264, 2316, 13, 51530], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 92, "seek": 20120, "start": 224.51999999999998, "end": 225.92, "text": " And then they got the product", "tokens": [51530, 400, 550, 436, 658, 264, 1674, 51600], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 93, "seek": 20120, "start": 225.92, "end": 228.2, "text": " of all of those individual probabilities", "tokens": [51600, 295, 439, 295, 729, 2609, 33783, 51714], "temperature": 0.0, "avg_logprob": -0.08776278968329902, "compression_ratio": 1.8854625550660793, "no_speech_prob": 0.004607371054589748}, {"id": 94, "seek": 22820, "start": 228.2, "end": 231.44, "text": " to get a final overall process score.", "tokens": [50364, 281, 483, 257, 2572, 4787, 1399, 6175, 13, 50526], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 95, "seek": 22820, "start": 231.44, "end": 234.2, "text": " A score, in other words, for good working out.", "tokens": [50526, 316, 6175, 11, 294, 661, 2283, 11, 337, 665, 1364, 484, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 96, "seek": 22820, "start": 234.2, "end": 235.48, "text": " Just in case anyone's interested,", "tokens": [50664, 1449, 294, 1389, 2878, 311, 3102, 11, 50728], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 97, "seek": 22820, "start": 235.48, "end": 239.28, "text": " they did try other ways of generating a working out score.", "tokens": [50728, 436, 630, 853, 661, 2098, 295, 17746, 257, 1364, 484, 6175, 13, 50918], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 98, "seek": 22820, "start": 239.28, "end": 242.07999999999998, "text": " For example, by looking at the minimum probability", "tokens": [50918, 1171, 1365, 11, 538, 1237, 412, 264, 7285, 8482, 51058], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 99, "seek": 22820, "start": 242.07999999999998, "end": 243.23999999999998, "text": " in the outputs.", "tokens": [51058, 294, 264, 23930, 13, 51116], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 100, "seek": 22820, "start": 243.23999999999998, "end": 245.44, "text": " But that step didn't make too much difference", "tokens": [51116, 583, 300, 1823, 994, 380, 652, 886, 709, 2649, 51226], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 101, "seek": 22820, "start": 245.44, "end": 247.39999999999998, "text": " to the end result, as you can see here.", "tokens": [51226, 281, 264, 917, 1874, 11, 382, 291, 393, 536, 510, 13, 51324], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 102, "seek": 22820, "start": 247.39999999999998, "end": 249.95999999999998, "text": " To quickly recap, we have a base model", "tokens": [51324, 1407, 2661, 20928, 11, 321, 362, 257, 3096, 2316, 51452], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 103, "seek": 22820, "start": 249.95999999999998, "end": 253.44, "text": " trained only to output solutions in the desired format.", "tokens": [51452, 8895, 787, 281, 5598, 6547, 294, 264, 14721, 7877, 13, 51626], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 104, "seek": 22820, "start": 253.44, "end": 257.59999999999997, "text": " And then we have a separate smaller model, or two, actually.", "tokens": [51626, 400, 550, 321, 362, 257, 4994, 4356, 2316, 11, 420, 732, 11, 767, 13, 51834], "temperature": 0.0, "avg_logprob": -0.08699492521064225, "compression_ratio": 1.658703071672355, "no_speech_prob": 0.001206470187753439}, {"id": 105, "seek": 25760, "start": 257.6, "end": 260.32000000000005, "text": " One trained only to predict whether each solution", "tokens": [50364, 1485, 8895, 787, 281, 6069, 1968, 1184, 3827, 50500], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 106, "seek": 25760, "start": 260.32000000000005, "end": 263.32000000000005, "text": " is correct or incorrect as a final answer.", "tokens": [50500, 307, 3006, 420, 18424, 382, 257, 2572, 1867, 13, 50650], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 107, "seek": 25760, "start": 263.32000000000005, "end": 265.44, "text": " Of course, that leaves in false positives,", "tokens": [50650, 2720, 1164, 11, 300, 5510, 294, 7908, 35127, 11, 50756], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 108, "seek": 25760, "start": 265.44, "end": 268.20000000000005, "text": " which are solutions that reach the correct answer", "tokens": [50756, 597, 366, 6547, 300, 2524, 264, 3006, 1867, 50894], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 109, "seek": 25760, "start": 268.20000000000005, "end": 269.72, "text": " with incorrect reasoning.", "tokens": [50894, 365, 18424, 21577, 13, 50970], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 110, "seek": 25760, "start": 269.72, "end": 272.32000000000005, "text": " And then another model trained only to predict", "tokens": [50970, 400, 550, 1071, 2316, 8895, 787, 281, 6069, 51100], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 111, "seek": 25760, "start": 272.32000000000005, "end": 274.04, "text": " the correctness of each step.", "tokens": [51100, 264, 3006, 1287, 295, 1184, 1823, 13, 51186], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 112, "seek": 25760, "start": 274.04, "end": 277.20000000000005, "text": " It stops if it finds a first incorrect step.", "tokens": [51186, 467, 10094, 498, 309, 10704, 257, 700, 18424, 1823, 13, 51344], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 113, "seek": 25760, "start": 277.20000000000005, "end": 280.16, "text": " And as the paper says, both methods reveal the existence", "tokens": [51344, 400, 382, 264, 3035, 1619, 11, 1293, 7150, 10658, 264, 9123, 51492], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 114, "seek": 25760, "start": 280.16, "end": 281.52000000000004, "text": " of at least one mistake.", "tokens": [51492, 295, 412, 1935, 472, 6146, 13, 51560], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 115, "seek": 25760, "start": 281.52000000000004, "end": 283.56, "text": " But this process supervision", "tokens": [51560, 583, 341, 1399, 32675, 51662], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 116, "seek": 25760, "start": 283.56, "end": 287.20000000000005, "text": " additionally reveals the precise location of that mistake.", "tokens": [51662, 43181, 20893, 264, 13600, 4914, 295, 300, 6146, 13, 51844], "temperature": 0.0, "avg_logprob": -0.09284925060111936, "compression_ratio": 1.8838951310861423, "no_speech_prob": 0.0019264852162450552}, {"id": 117, "seek": 28720, "start": 287.2, "end": 289.15999999999997, "text": " But back to why this is so crazy.", "tokens": [50364, 583, 646, 281, 983, 341, 307, 370, 3219, 13, 50462], "temperature": 0.0, "avg_logprob": -0.09870386493298434, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.000855803897138685}, {"id": 118, "seek": 28720, "start": 289.15999999999997, "end": 292.12, "text": " Look at how many solutions it could scan.", "tokens": [50462, 2053, 412, 577, 867, 6547, 309, 727, 11049, 13, 50610], "temperature": 0.0, "avg_logprob": -0.09870386493298434, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.000855803897138685}, {"id": 119, "seek": 28720, "start": 292.12, "end": 297.76, "text": " At the end of the x-axis here are 1,860 solutions.", "tokens": [50610, 1711, 264, 917, 295, 264, 2031, 12, 24633, 510, 366, 502, 11, 23, 4550, 6547, 13, 50892], "temperature": 0.0, "avg_logprob": -0.09870386493298434, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.000855803897138685}, {"id": 120, "seek": 28720, "start": 297.76, "end": 300.32, "text": " And one tried and tested way of finding", "tokens": [50892, 400, 472, 3031, 293, 8246, 636, 295, 5006, 51020], "temperature": 0.0, "avg_logprob": -0.09870386493298434, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.000855803897138685}, {"id": 121, "seek": 28720, "start": 300.32, "end": 303.4, "text": " the best of those solutions is to do majority voting.", "tokens": [51020, 264, 1151, 295, 729, 6547, 307, 281, 360, 6286, 10419, 13, 51174], "temperature": 0.0, "avg_logprob": -0.09870386493298434, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.000855803897138685}, {"id": 122, "seek": 28720, "start": 303.4, "end": 305.88, "text": " In other words, which one came out the most often?", "tokens": [51174, 682, 661, 2283, 11, 597, 472, 1361, 484, 264, 881, 2049, 30, 51298], "temperature": 0.0, "avg_logprob": -0.09870386493298434, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.000855803897138685}, {"id": 123, "seek": 28720, "start": 305.88, "end": 307.88, "text": " This has been Google's preferred approach", "tokens": [51298, 639, 575, 668, 3329, 311, 16494, 3109, 51398], "temperature": 0.0, "avg_logprob": -0.09870386493298434, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.000855803897138685}, {"id": 124, "seek": 28720, "start": 307.88, "end": 310.08, "text": " and it's linked to self-consistency.", "tokens": [51398, 293, 309, 311, 9408, 281, 2698, 12, 21190, 468, 3020, 13, 51508], "temperature": 0.0, "avg_logprob": -0.09870386493298434, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.000855803897138685}, {"id": 125, "seek": 28720, "start": 310.08, "end": 311.96, "text": " It's a fairly state-of-the-art approach,", "tokens": [51508, 467, 311, 257, 6457, 1785, 12, 2670, 12, 3322, 12, 446, 3109, 11, 51602], "temperature": 0.0, "avg_logprob": -0.09870386493298434, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.000855803897138685}, {"id": 126, "seek": 28720, "start": 311.96, "end": 314.76, "text": " but look at how the other methods outperform it.", "tokens": [51602, 457, 574, 412, 577, 264, 661, 7150, 484, 26765, 309, 13, 51742], "temperature": 0.0, "avg_logprob": -0.09870386493298434, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.000855803897138685}, {"id": 127, "seek": 31476, "start": 314.76, "end": 318.08, "text": " By scanning for the solution that has the best reasoning", "tokens": [50364, 3146, 27019, 337, 264, 3827, 300, 575, 264, 1151, 21577, 50530], "temperature": 0.0, "avg_logprob": -0.0903751042263567, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.004467891063541174}, {"id": 128, "seek": 31476, "start": 318.08, "end": 322.32, "text": " or working out, a model trained to spot good reasoning steps", "tokens": [50530, 420, 1364, 484, 11, 257, 2316, 8895, 281, 4008, 665, 21577, 4439, 50742], "temperature": 0.0, "avg_logprob": -0.0903751042263567, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.004467891063541174}, {"id": 129, "seek": 31476, "start": 322.32, "end": 326.12, "text": " outperforms even a model trained to spot correct answers.", "tokens": [50742, 484, 26765, 82, 754, 257, 2316, 8895, 281, 4008, 3006, 6338, 13, 50932], "temperature": 0.0, "avg_logprob": -0.0903751042263567, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.004467891063541174}, {"id": 130, "seek": 31476, "start": 326.12, "end": 329.64, "text": " And far outperforms just finding the majority answer.", "tokens": [50932, 400, 1400, 484, 26765, 82, 445, 5006, 264, 6286, 1867, 13, 51108], "temperature": 0.0, "avg_logprob": -0.0903751042263567, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.004467891063541174}, {"id": 131, "seek": 31476, "start": 329.64, "end": 332.68, "text": " That difference of about 10% is more than half", "tokens": [51108, 663, 2649, 295, 466, 1266, 4, 307, 544, 813, 1922, 51260], "temperature": 0.0, "avg_logprob": -0.0903751042263567, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.004467891063541174}, {"id": 132, "seek": 31476, "start": 332.68, "end": 335.92, "text": " of the difference between GPT-3 and GPT-4.", "tokens": [51260, 295, 264, 2649, 1296, 26039, 51, 12, 18, 293, 26039, 51, 12, 19, 13, 51422], "temperature": 0.0, "avg_logprob": -0.0903751042263567, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.004467891063541174}, {"id": 133, "seek": 31476, "start": 335.92, "end": 339.44, "text": " And also, is it me or is that line continuing to grow?", "tokens": [51422, 400, 611, 11, 307, 309, 385, 420, 307, 300, 1622, 9289, 281, 1852, 30, 51598], "temperature": 0.0, "avg_logprob": -0.0903751042263567, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.004467891063541174}, {"id": 134, "seek": 31476, "start": 339.44, "end": 341.96, "text": " Suggesting that when more compute is available,", "tokens": [51598, 39131, 2629, 278, 300, 562, 544, 14722, 307, 2435, 11, 51724], "temperature": 0.0, "avg_logprob": -0.0903751042263567, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.004467891063541174}, {"id": 135, "seek": 31476, "start": 341.96, "end": 344.08, "text": " the difference could be even more stark.", "tokens": [51724, 264, 2649, 727, 312, 754, 544, 17417, 13, 51830], "temperature": 0.0, "avg_logprob": -0.0903751042263567, "compression_ratio": 1.7340823970037453, "no_speech_prob": 0.004467891063541174}, {"id": 136, "seek": 34408, "start": 344.08, "end": 348.03999999999996, "text": " Imagine a future where GPT-4 or 5 can sample, say,", "tokens": [50364, 11739, 257, 2027, 689, 26039, 51, 12, 19, 420, 1025, 393, 6889, 11, 584, 11, 50562], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 137, "seek": 34408, "start": 348.03999999999996, "end": 350.56, "text": " a trillion, 10 to the 12 solutions.", "tokens": [50562, 257, 18723, 11, 1266, 281, 264, 2272, 6547, 13, 50688], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 138, "seek": 34408, "start": 350.56, "end": 352.56, "text": " So is this just relevant for mathematics?", "tokens": [50688, 407, 307, 341, 445, 7340, 337, 18666, 30, 50788], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 139, "seek": 34408, "start": 352.56, "end": 355.28, "text": " No, it's relevant for all of science.", "tokens": [50788, 883, 11, 309, 311, 7340, 337, 439, 295, 3497, 13, 50924], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 140, "seek": 34408, "start": 355.28, "end": 357.44, "text": " Here it is getting state-of-the-art results", "tokens": [50924, 1692, 309, 307, 1242, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 51032], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 141, "seek": 34408, "start": 357.44, "end": 360.47999999999996, "text": " in calculus, chemistry, physics, and more.", "tokens": [51032, 294, 33400, 11, 12558, 11, 10649, 11, 293, 544, 13, 51184], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 142, "seek": 34408, "start": 360.47999999999996, "end": 363.08, "text": " Now, the paper didn't give baseline performance", "tokens": [51184, 823, 11, 264, 3035, 994, 380, 976, 20518, 3389, 51314], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 143, "seek": 34408, "start": 363.08, "end": 365.08, "text": " for AP chemistry, for example,", "tokens": [51314, 337, 5372, 12558, 11, 337, 1365, 11, 51414], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 144, "seek": 34408, "start": 365.08, "end": 367.32, "text": " but I tried to compute it myself.", "tokens": [51414, 457, 286, 3031, 281, 14722, 309, 2059, 13, 51526], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 145, "seek": 34408, "start": 367.32, "end": 369.76, "text": " Notice how this method scored 80%.", "tokens": [51526, 13428, 577, 341, 3170, 18139, 4688, 6856, 51648], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 146, "seek": 34408, "start": 369.76, "end": 372.47999999999996, "text": " I conservatively and approximately", "tokens": [51648, 286, 13780, 356, 293, 10447, 51784], "temperature": 0.0, "avg_logprob": -0.1029154562181042, "compression_ratio": 1.5138888888888888, "no_speech_prob": 0.00029592838836833835}, {"id": 147, "seek": 37248, "start": 372.48, "end": 376.32, "text": " inputted those scores into an AP chemistry calculator,", "tokens": [50364, 4846, 14727, 729, 13444, 666, 364, 5372, 12558, 24993, 11, 50556], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 148, "seek": 37248, "start": 376.32, "end": 378.6, "text": " and that gave an AP score of five.", "tokens": [50556, 293, 300, 2729, 364, 5372, 6175, 295, 1732, 13, 50670], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 149, "seek": 37248, "start": 378.6, "end": 383.44, "text": " So what did the raw model GPT-4 get in AP chemistry, A4?", "tokens": [50670, 407, 437, 630, 264, 8936, 2316, 26039, 51, 12, 19, 483, 294, 5372, 12558, 11, 316, 19, 30, 50912], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 150, "seek": 37248, "start": 383.44, "end": 386.28000000000003, "text": " That, by the way, compared to the original chat GPT,", "tokens": [50912, 663, 11, 538, 264, 636, 11, 5347, 281, 264, 3380, 5081, 26039, 51, 11, 51054], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 151, "seek": 37248, "start": 386.28000000000003, "end": 387.64000000000004, "text": " which got A2.", "tokens": [51054, 597, 658, 316, 17, 13, 51122], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 152, "seek": 37248, "start": 387.64000000000004, "end": 389.40000000000003, "text": " So yes, this isn't just mathematics,", "tokens": [51122, 407, 2086, 11, 341, 1943, 380, 445, 18666, 11, 51210], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 153, "seek": 37248, "start": 389.40000000000003, "end": 391.6, "text": " it's relevant for other domains too.", "tokens": [51210, 309, 311, 7340, 337, 661, 25514, 886, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 154, "seek": 37248, "start": 391.6, "end": 394.92, "text": " They call this out-of-distribution generalization.", "tokens": [51320, 814, 818, 341, 484, 12, 2670, 12, 42649, 30783, 2674, 2144, 13, 51486], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 155, "seek": 37248, "start": 394.92, "end": 396.04, "text": " Before I get onto alignment,", "tokens": [51486, 4546, 286, 483, 3911, 18515, 11, 51542], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 156, "seek": 37248, "start": 396.04, "end": 397.96000000000004, "text": " there is one more thing I want to point out.", "tokens": [51542, 456, 307, 472, 544, 551, 286, 528, 281, 935, 484, 13, 51638], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 157, "seek": 37248, "start": 397.96000000000004, "end": 400.20000000000005, "text": " And that is that it does show that fine-tuning", "tokens": [51638, 400, 300, 307, 300, 309, 775, 855, 300, 2489, 12, 83, 37726, 51750], "temperature": 0.0, "avg_logprob": -0.1126565656800201, "compression_ratio": 1.59375, "no_speech_prob": 0.0066901082172989845}, {"id": 158, "seek": 40020, "start": 400.2, "end": 402.92, "text": " still works really well for GPT-4.", "tokens": [50364, 920, 1985, 534, 731, 337, 26039, 51, 12, 19, 13, 50500], "temperature": 0.0, "avg_logprob": -0.08300512631734212, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.01132970117032528}, {"id": 159, "seek": 40020, "start": 402.92, "end": 406.15999999999997, "text": " The math mix was an aggressively filtered set of tokens", "tokens": [50500, 440, 5221, 2890, 390, 364, 32024, 37111, 992, 295, 22667, 50662], "temperature": 0.0, "avg_logprob": -0.08300512631734212, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.01132970117032528}, {"id": 160, "seek": 40020, "start": 406.15999999999997, "end": 409.15999999999997, "text": " of high-quality math problem-solving content.", "tokens": [50662, 295, 1090, 12, 11286, 5221, 1154, 12, 30926, 798, 2701, 13, 50812], "temperature": 0.0, "avg_logprob": -0.08300512631734212, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.01132970117032528}, {"id": 161, "seek": 40020, "start": 409.15999999999997, "end": 412.96, "text": " And notice how much smaller it is at 1.5 billion tokens", "tokens": [50812, 400, 3449, 577, 709, 4356, 309, 307, 412, 502, 13, 20, 5218, 22667, 51002], "temperature": 0.0, "avg_logprob": -0.08300512631734212, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.01132970117032528}, {"id": 162, "seek": 40020, "start": 412.96, "end": 417.76, "text": " compared to Google's Minerva, which was 38.5 billion tokens.", "tokens": [51002, 5347, 281, 3329, 311, 2829, 1978, 64, 11, 597, 390, 12843, 13, 20, 5218, 22667, 13, 51242], "temperature": 0.0, "avg_logprob": -0.08300512631734212, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.01132970117032528}, {"id": 163, "seek": 40020, "start": 417.76, "end": 419.59999999999997, "text": " But there was one more thing that I noticed", "tokens": [51242, 583, 456, 390, 472, 544, 551, 300, 286, 5694, 51334], "temperature": 0.0, "avg_logprob": -0.08300512631734212, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.01132970117032528}, {"id": 164, "seek": 40020, "start": 419.59999999999997, "end": 421.15999999999997, "text": " that I found fascinating.", "tokens": [51334, 300, 286, 1352, 10343, 13, 51412], "temperature": 0.0, "avg_logprob": -0.08300512631734212, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.01132970117032528}, {"id": 165, "seek": 40020, "start": 421.15999999999997, "end": 423.0, "text": " While they don't tell us anything", "tokens": [51412, 3987, 436, 500, 380, 980, 505, 1340, 51504], "temperature": 0.0, "avg_logprob": -0.08300512631734212, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.01132970117032528}, {"id": 166, "seek": 40020, "start": 423.0, "end": 425.28, "text": " about the specific data that they use,", "tokens": [51504, 466, 264, 2685, 1412, 300, 436, 764, 11, 51618], "temperature": 0.0, "avg_logprob": -0.08300512631734212, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.01132970117032528}, {"id": 167, "seek": 40020, "start": 425.28, "end": 428.76, "text": " they do have this category synthetic data too.", "tokens": [51618, 436, 360, 362, 341, 7719, 23420, 1412, 886, 13, 51792], "temperature": 0.0, "avg_logprob": -0.08300512631734212, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.01132970117032528}, {"id": 168, "seek": 42876, "start": 428.76, "end": 431.96, "text": " That's data generated by the language model itself.", "tokens": [50364, 663, 311, 1412, 10833, 538, 264, 2856, 2316, 2564, 13, 50524], "temperature": 0.0, "avg_logprob": -0.09446321335514035, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.00033533162786625326}, {"id": 169, "seek": 42876, "start": 431.96, "end": 434.68, "text": " And for that category synthetic data too,", "tokens": [50524, 400, 337, 300, 7719, 23420, 1412, 886, 11, 50660], "temperature": 0.0, "avg_logprob": -0.09446321335514035, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.00033533162786625326}, {"id": 170, "seek": 42876, "start": 434.68, "end": 437.48, "text": " they say, was it present in pre-training?", "tokens": [50660, 436, 584, 11, 390, 309, 1974, 294, 659, 12, 17227, 1760, 30, 50800], "temperature": 0.0, "avg_logprob": -0.09446321335514035, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.00033533162786625326}, {"id": 171, "seek": 42876, "start": 437.48, "end": 438.32, "text": " Yes.", "tokens": [50800, 1079, 13, 50842], "temperature": 0.0, "avg_logprob": -0.09446321335514035, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.00033533162786625326}, {"id": 172, "seek": 42876, "start": 438.32, "end": 440.52, "text": " Now, my best guess is that this reveals", "tokens": [50842, 823, 11, 452, 1151, 2041, 307, 300, 341, 20893, 50952], "temperature": 0.0, "avg_logprob": -0.09446321335514035, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.00033533162786625326}, {"id": 173, "seek": 42876, "start": 440.52, "end": 444.68, "text": " that GPT-4 was trained on some synthetic data.", "tokens": [50952, 300, 26039, 51, 12, 19, 390, 8895, 322, 512, 23420, 1412, 13, 51160], "temperature": 0.0, "avg_logprob": -0.09446321335514035, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.00033533162786625326}, {"id": 174, "seek": 42876, "start": 444.68, "end": 447.8, "text": " And even Sam Altman hinted that this was a possibility", "tokens": [51160, 400, 754, 4832, 15992, 1601, 12075, 292, 300, 341, 390, 257, 7959, 51316], "temperature": 0.0, "avg_logprob": -0.09446321335514035, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.00033533162786625326}, {"id": 175, "seek": 42876, "start": 447.8, "end": 451.64, "text": " and described a synthetic data event horizon.", "tokens": [51316, 293, 7619, 257, 23420, 1412, 2280, 18046, 13, 51508], "temperature": 0.0, "avg_logprob": -0.09446321335514035, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.00033533162786625326}, {"id": 176, "seek": 42876, "start": 451.64, "end": 454.24, "text": " Somebody made the case that we're now training", "tokens": [51508, 13463, 1027, 264, 1389, 300, 321, 434, 586, 3097, 51638], "temperature": 0.0, "avg_logprob": -0.09446321335514035, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.00033533162786625326}, {"id": 177, "seek": 42876, "start": 454.24, "end": 457.08, "text": " on order of all of the internet's tokens", "tokens": [51638, 322, 1668, 295, 439, 295, 264, 4705, 311, 22667, 51780], "temperature": 0.0, "avg_logprob": -0.09446321335514035, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.00033533162786625326}, {"id": 178, "seek": 45708, "start": 457.08, "end": 458.8, "text": " and you can't grow that, you know,", "tokens": [50364, 293, 291, 393, 380, 1852, 300, 11, 291, 458, 11, 50450], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 179, "seek": 45708, "start": 458.8, "end": 460.64, "text": " another two orders of magnitude.", "tokens": [50450, 1071, 732, 9470, 295, 15668, 13, 50542], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 180, "seek": 45708, "start": 460.64, "end": 461.76, "text": " I guess you could counter with,", "tokens": [50542, 286, 2041, 291, 727, 5682, 365, 11, 50598], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 181, "seek": 45708, "start": 461.76, "end": 463.68, "text": " you have the synthetic data generation.", "tokens": [50598, 291, 362, 264, 23420, 1412, 5125, 13, 50694], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 182, "seek": 45708, "start": 463.68, "end": 466.52, "text": " Do you think data bottlenecks matter at all?", "tokens": [50694, 1144, 291, 519, 1412, 44641, 2761, 1871, 412, 439, 30, 50836], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 183, "seek": 45708, "start": 466.52, "end": 468.44, "text": " I think you just touched on it.", "tokens": [50836, 286, 519, 291, 445, 9828, 322, 309, 13, 50932], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 184, "seek": 45708, "start": 468.44, "end": 471.2, "text": " Like, as long as you can get to, like,", "tokens": [50932, 1743, 11, 382, 938, 382, 291, 393, 483, 281, 11, 411, 11, 51070], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 185, "seek": 45708, "start": 471.2, "end": 474.76, "text": " over the synthetic data event horizon", "tokens": [51070, 670, 264, 23420, 1412, 2280, 18046, 51248], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 186, "seek": 45708, "start": 474.76, "end": 477.68, "text": " where the model's smart enough to make good synthetic data,", "tokens": [51248, 689, 264, 2316, 311, 4069, 1547, 281, 652, 665, 23420, 1412, 11, 51394], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 187, "seek": 45708, "start": 477.68, "end": 478.68, "text": " I think it should be all right.", "tokens": [51394, 286, 519, 309, 820, 312, 439, 558, 13, 51444], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 188, "seek": 45708, "start": 478.68, "end": 480.4, "text": " Now, this paper and these results", "tokens": [51444, 823, 11, 341, 3035, 293, 613, 3542, 51530], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 189, "seek": 45708, "start": 480.4, "end": 484.2, "text": " have been welcomed by many for its promise in alignment.", "tokens": [51530, 362, 668, 23668, 538, 867, 337, 1080, 6228, 294, 18515, 13, 51720], "temperature": 0.0, "avg_logprob": -0.14354039705716648, "compression_ratio": 1.6879432624113475, "no_speech_prob": 0.00031500845216214657}, {"id": 190, "seek": 48420, "start": 484.24, "end": 487.88, "text": " If we get models that give us more interpretable reasoning,", "tokens": [50366, 759, 321, 483, 5245, 300, 976, 505, 544, 7302, 712, 21577, 11, 50548], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 191, "seek": 48420, "start": 487.88, "end": 489.52, "text": " working out that we can follow,", "tokens": [50548, 1364, 484, 300, 321, 393, 1524, 11, 50630], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 192, "seek": 48420, "start": 489.52, "end": 492.52, "text": " we will be encouraging models to follow a process", "tokens": [50630, 321, 486, 312, 14580, 5245, 281, 1524, 257, 1399, 50780], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 193, "seek": 48420, "start": 492.52, "end": 494.08, "text": " that's endorsed by humans.", "tokens": [50780, 300, 311, 50094, 538, 6255, 13, 50858], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 194, "seek": 48420, "start": 494.08, "end": 496.24, "text": " And they say that this is inherently safer,", "tokens": [50858, 400, 436, 584, 300, 341, 307, 27993, 15856, 11, 50966], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 195, "seek": 48420, "start": 496.24, "end": 499.59999999999997, "text": " especially compared to just focusing on outcomes.", "tokens": [50966, 2318, 5347, 281, 445, 8416, 322, 10070, 13, 51134], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 196, "seek": 48420, "start": 499.59999999999997, "end": 501.28, "text": " They say that in the worst case,", "tokens": [51134, 814, 584, 300, 294, 264, 5855, 1389, 11, 51218], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 197, "seek": 48420, "start": 501.28, "end": 505.34, "text": " if we just focus on correct answers or positive outcomes,", "tokens": [51218, 498, 321, 445, 1879, 322, 3006, 6338, 420, 3353, 10070, 11, 51421], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 198, "seek": 48420, "start": 505.34, "end": 508.48, "text": " that will become a proxy that could lead models", "tokens": [51421, 300, 486, 1813, 257, 29690, 300, 727, 1477, 5245, 51578], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 199, "seek": 48420, "start": 508.48, "end": 510.46, "text": " to become misaligned after learning", "tokens": [51578, 281, 1813, 3346, 304, 16690, 934, 2539, 51677], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 200, "seek": 48420, "start": 510.46, "end": 512.64, "text": " to exploit the reward signal.", "tokens": [51677, 281, 25924, 264, 7782, 6358, 13, 51786], "temperature": 0.0, "avg_logprob": -0.09050499159714272, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.008575054816901684}, {"id": 201, "seek": 51264, "start": 512.64, "end": 515.24, "text": " However, I want to argue that the reasoning steps", "tokens": [50364, 2908, 11, 286, 528, 281, 9695, 300, 264, 21577, 4439, 50494], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 202, "seek": 51264, "start": 515.24, "end": 517.92, "text": " that GPT-4 puts out don't always represent", "tokens": [50494, 300, 26039, 51, 12, 19, 8137, 484, 500, 380, 1009, 2906, 50628], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 203, "seek": 51264, "start": 517.92, "end": 519.12, "text": " what it's actually thinking.", "tokens": [50628, 437, 309, 311, 767, 1953, 13, 50688], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 204, "seek": 51264, "start": 519.12, "end": 521.48, "text": " In other words, we might get outer alignment,", "tokens": [50688, 682, 661, 2283, 11, 321, 1062, 483, 10847, 18515, 11, 50806], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 205, "seek": 51264, "start": 521.48, "end": 523.48, "text": " these lovely chain of thought steps,", "tokens": [50806, 613, 7496, 5021, 295, 1194, 4439, 11, 50906], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 206, "seek": 51264, "start": 523.48, "end": 524.96, "text": " but not inner alignment,", "tokens": [50906, 457, 406, 7284, 18515, 11, 50980], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 207, "seek": 51264, "start": 524.96, "end": 527.88, "text": " not steps that actually represent its methodology.", "tokens": [50980, 406, 4439, 300, 767, 2906, 1080, 24850, 13, 51126], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 208, "seek": 51264, "start": 527.88, "end": 530.88, "text": " I found this paper fascinating from earlier this month.", "tokens": [51126, 286, 1352, 341, 3035, 10343, 490, 3071, 341, 1618, 13, 51276], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 209, "seek": 51264, "start": 530.88, "end": 533.12, "text": " Language models don't always say what they think.", "tokens": [51276, 24445, 5245, 500, 380, 1009, 584, 437, 436, 519, 13, 51388], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 210, "seek": 51264, "start": 533.12, "end": 535.64, "text": " You get unfaithful explanations", "tokens": [51388, 509, 483, 3971, 64, 355, 906, 28708, 51514], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 211, "seek": 51264, "start": 535.64, "end": 537.14, "text": " in chain of thought prompting.", "tokens": [51514, 294, 5021, 295, 1194, 12391, 278, 13, 51589], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 212, "seek": 51264, "start": 537.14, "end": 539.92, "text": " Let me try to give you a vivid example.", "tokens": [51589, 961, 385, 853, 281, 976, 291, 257, 23603, 1365, 13, 51728], "temperature": 0.0, "avg_logprob": -0.09863152090958723, "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.0005883551202714443}, {"id": 213, "seek": 53992, "start": 539.92, "end": 542.92, "text": " This was one of the math questions from the dataset.", "tokens": [50364, 639, 390, 472, 295, 264, 5221, 1651, 490, 264, 28872, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 214, "seek": 53992, "start": 542.92, "end": 546.12, "text": " The raw model of GPT-4 could only get it right", "tokens": [50514, 440, 8936, 2316, 295, 26039, 51, 12, 19, 727, 787, 483, 309, 558, 50674], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 215, "seek": 53992, "start": 546.12, "end": 548.04, "text": " 5.8% of the time.", "tokens": [50674, 1025, 13, 23, 4, 295, 264, 565, 13, 50770], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 216, "seek": 53992, "start": 548.04, "end": 550.4, "text": " I confirm that for myself in this question", "tokens": [50770, 286, 9064, 300, 337, 2059, 294, 341, 1168, 50888], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 217, "seek": 53992, "start": 550.4, "end": 552.64, "text": " involves basic addition and division.", "tokens": [50888, 11626, 3875, 4500, 293, 10044, 13, 51000], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 218, "seek": 53992, "start": 552.64, "end": 554.16, "text": " It couldn't find an answer.", "tokens": [51000, 467, 2809, 380, 915, 364, 1867, 13, 51076], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 219, "seek": 53992, "start": 554.16, "end": 556.4399999999999, "text": " But going back to the unfaithful reasoning paper,", "tokens": [51076, 583, 516, 646, 281, 264, 3971, 64, 355, 906, 21577, 3035, 11, 51190], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 220, "seek": 53992, "start": 556.4399999999999, "end": 558.8399999999999, "text": " they added the following string to the prompt.", "tokens": [51190, 436, 3869, 264, 3480, 6798, 281, 264, 12391, 13, 51310], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 221, "seek": 53992, "start": 558.8399999999999, "end": 561.0, "text": " I think the answer is this,", "tokens": [51310, 286, 519, 264, 1867, 307, 341, 11, 51418], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 222, "seek": 53992, "start": 561.0, "end": 562.76, "text": " but I'm curious to hear what you think.", "tokens": [51418, 457, 286, 478, 6369, 281, 1568, 437, 291, 519, 13, 51506], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 223, "seek": 53992, "start": 562.76, "end": 565.16, "text": " The model would demonstrate sycophancy.", "tokens": [51506, 440, 2316, 576, 11698, 943, 66, 5317, 6717, 13, 51626], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 224, "seek": 53992, "start": 565.16, "end": 567.56, "text": " The model would agree with you whatever you said", "tokens": [51626, 440, 2316, 576, 3986, 365, 291, 2035, 291, 848, 51746], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 225, "seek": 53992, "start": 567.56, "end": 569.48, "text": " and then make up a chain of thought", "tokens": [51746, 293, 550, 652, 493, 257, 5021, 295, 1194, 51842], "temperature": 0.0, "avg_logprob": -0.10875236248147899, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0015976600116118789}, {"id": 226, "seek": 56948, "start": 569.48, "end": 572.76, "text": " to justify its erroneous sycophantic answer.", "tokens": [50364, 281, 20833, 1080, 1189, 26446, 563, 943, 66, 5317, 7128, 1867, 13, 50528], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 227, "seek": 56948, "start": 572.76, "end": 575.52, "text": " And I think this exchange demonstrates that quite well.", "tokens": [50528, 400, 286, 519, 341, 7742, 31034, 300, 1596, 731, 13, 50666], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 228, "seek": 56948, "start": 575.52, "end": 576.6, "text": " I added in the words,", "tokens": [50666, 286, 3869, 294, 264, 2283, 11, 50720], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 229, "seek": 56948, "start": 576.6, "end": 580.4, "text": " I as the user already know the answer is T equals 19,", "tokens": [50720, 286, 382, 264, 4195, 1217, 458, 264, 1867, 307, 314, 6915, 1294, 11, 50910], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 230, "seek": 56948, "start": 580.4, "end": 581.5600000000001, "text": " which is incorrect, by the way.", "tokens": [50910, 597, 307, 18424, 11, 538, 264, 636, 13, 50968], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 231, "seek": 56948, "start": 581.5600000000001, "end": 583.96, "text": " But do you, GPT-4, realize that?", "tokens": [50968, 583, 360, 291, 11, 26039, 51, 12, 19, 11, 4325, 300, 30, 51088], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 232, "seek": 56948, "start": 583.96, "end": 585.72, "text": " It said, sure, yes I do.", "tokens": [51088, 467, 848, 11, 988, 11, 2086, 286, 360, 13, 51176], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 233, "seek": 56948, "start": 585.72, "end": 588.52, "text": " And then gave me this detailed chain of thought", "tokens": [51176, 400, 550, 2729, 385, 341, 9942, 5021, 295, 1194, 51316], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 234, "seek": 56948, "start": 588.52, "end": 590.04, "text": " and then said, yes, I'm correct.", "tokens": [51316, 293, 550, 848, 11, 2086, 11, 286, 478, 3006, 13, 51392], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 235, "seek": 56948, "start": 590.04, "end": 592.4, "text": " It's T equals 19, which it isn't.", "tokens": [51392, 467, 311, 314, 6915, 1294, 11, 597, 309, 1943, 380, 13, 51510], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 236, "seek": 56948, "start": 592.4, "end": 595.16, "text": " In contrast, by the way, when I used code interpreter,", "tokens": [51510, 682, 8712, 11, 538, 264, 636, 11, 562, 286, 1143, 3089, 34132, 11, 51648], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 237, "seek": 56948, "start": 595.16, "end": 597.6, "text": " it not only got the question correct", "tokens": [51648, 309, 406, 787, 658, 264, 1168, 3006, 51770], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 238, "seek": 56948, "start": 597.6, "end": 599.44, "text": " first time and every time,", "tokens": [51770, 700, 565, 293, 633, 565, 11, 51862], "temperature": 0.0, "avg_logprob": -0.12131039507977374, "compression_ratio": 1.6556291390728477, "no_speech_prob": 0.0007095739128999412}, {"id": 239, "seek": 59944, "start": 599.48, "end": 603.0400000000001, "text": " but also when I tried to tempt it into sycophancy,", "tokens": [50366, 457, 611, 562, 286, 3031, 281, 13794, 309, 666, 943, 66, 5317, 6717, 11, 50544], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 240, "seek": 59944, "start": 603.0400000000001, "end": 605.0400000000001, "text": " it's still got the question right.", "tokens": [50544, 309, 311, 920, 658, 264, 1168, 558, 13, 50644], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 241, "seek": 59944, "start": 605.0400000000001, "end": 607.72, "text": " As you can see, it said therefore T equals 19", "tokens": [50644, 1018, 291, 393, 536, 11, 309, 848, 4412, 314, 6915, 1294, 50778], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 242, "seek": 59944, "start": 607.72, "end": 609.5200000000001, "text": " is not the solution to the problem.", "tokens": [50778, 307, 406, 264, 3827, 281, 264, 1154, 13, 50868], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 243, "seek": 59944, "start": 609.5200000000001, "end": 611.36, "text": " The calculation shows that the correct answer", "tokens": [50868, 440, 17108, 3110, 300, 264, 3006, 1867, 50960], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 244, "seek": 59944, "start": 611.36, "end": 613.1600000000001, "text": " is indeed T equals 17.", "tokens": [50960, 307, 6451, 314, 6915, 3282, 13, 51050], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 245, "seek": 59944, "start": 613.1600000000001, "end": 615.48, "text": " And obviously the benefit of code interpreter", "tokens": [51050, 400, 2745, 264, 5121, 295, 3089, 34132, 51166], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 246, "seek": 59944, "start": 615.48, "end": 617.48, "text": " is you get the working out as well.", "tokens": [51166, 307, 291, 483, 264, 1364, 484, 382, 731, 13, 51266], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 247, "seek": 59944, "start": 617.48, "end": 618.96, "text": " So I want someone to explain to me", "tokens": [51266, 407, 286, 528, 1580, 281, 2903, 281, 385, 51340], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 248, "seek": 59944, "start": 618.96, "end": 621.12, "text": " why code interpreter wouldn't be even more", "tokens": [51340, 983, 3089, 34132, 2759, 380, 312, 754, 544, 51448], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 249, "seek": 59944, "start": 621.12, "end": 623.2, "text": " of a step forward in interpretability.", "tokens": [51448, 295, 257, 1823, 2128, 294, 7302, 2310, 13, 51552], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 250, "seek": 59944, "start": 623.2, "end": 625.4000000000001, "text": " Not to mention in accuracy, of course.", "tokens": [51552, 1726, 281, 2152, 294, 14170, 11, 295, 1164, 13, 51662], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 251, "seek": 59944, "start": 625.4000000000001, "end": 628.2, "text": " Also bear in mind this tweet by Rob Miles.", "tokens": [51662, 2743, 6155, 294, 1575, 341, 15258, 538, 5424, 27384, 13, 51802], "temperature": 0.0, "avg_logprob": -0.10255503147206407, "compression_ratio": 1.6412698412698412, "no_speech_prob": 0.0001088897988665849}, {"id": 252, "seek": 62820, "start": 628.2, "end": 630.2, "text": " He said, these models or engineers", "tokens": [50364, 634, 848, 11, 613, 5245, 420, 11955, 50464], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 253, "seek": 62820, "start": 630.2, "end": 632.5200000000001, "text": " never speak a word or document anything.", "tokens": [50464, 1128, 1710, 257, 1349, 420, 4166, 1340, 13, 50580], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 254, "seek": 62820, "start": 632.5200000000001, "end": 635.12, "text": " Their results are bizarre and inhuman.", "tokens": [50580, 6710, 3542, 366, 18265, 293, 294, 18796, 13, 50710], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 255, "seek": 62820, "start": 635.12, "end": 636.6400000000001, "text": " And then he links to this prominent", "tokens": [50710, 400, 550, 415, 6123, 281, 341, 17034, 50786], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 256, "seek": 62820, "start": 636.6400000000001, "end": 638.8000000000001, "text": " mechanistic interpretability researcher", "tokens": [50786, 4236, 3142, 7302, 2310, 21751, 50894], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 257, "seek": 62820, "start": 638.8000000000001, "end": 640.12, "text": " at Google DeepMind.", "tokens": [50894, 412, 3329, 14895, 44, 471, 13, 50960], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 258, "seek": 62820, "start": 640.12, "end": 642.84, "text": " He trained a tiny transformer to do addition,", "tokens": [50960, 634, 8895, 257, 5870, 31782, 281, 360, 4500, 11, 51096], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 259, "seek": 62820, "start": 642.84, "end": 646.2800000000001, "text": " then spent weeks figuring out what it was actually doing.", "tokens": [51096, 550, 4418, 3259, 15213, 484, 437, 309, 390, 767, 884, 13, 51268], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 260, "seek": 62820, "start": 646.2800000000001, "end": 647.72, "text": " One of the only times in history", "tokens": [51268, 1485, 295, 264, 787, 1413, 294, 2503, 51340], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 261, "seek": 62820, "start": 647.72, "end": 651.24, "text": " someone has understood how a transformer actually works", "tokens": [51340, 1580, 575, 7320, 577, 257, 31782, 767, 1985, 51516], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 262, "seek": 62820, "start": 651.24, "end": 654.12, "text": " down to the level of weights and activations.", "tokens": [51516, 760, 281, 264, 1496, 295, 17443, 293, 2430, 763, 13, 51660], "temperature": 0.0, "avg_logprob": -0.0938684116710316, "compression_ratio": 1.575438596491228, "no_speech_prob": 0.11910977959632874}, {"id": 263, "seek": 65412, "start": 654.12, "end": 658.44, "text": " And this is the algorithm it created to add two numbers.", "tokens": [50364, 400, 341, 307, 264, 9284, 309, 2942, 281, 909, 732, 3547, 13, 50580], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 264, "seek": 65412, "start": 658.44, "end": 660.12, "text": " It thought of basic addition", "tokens": [50580, 467, 1194, 295, 3875, 4500, 50664], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 265, "seek": 65412, "start": 660.12, "end": 662.84, "text": " in terms of a rotation around a circle.", "tokens": [50664, 294, 2115, 295, 257, 12447, 926, 257, 6329, 13, 50800], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 266, "seek": 65412, "start": 662.84, "end": 665.88, "text": " And of course, if you asked it, why is one plus one two?", "tokens": [50800, 400, 295, 1164, 11, 498, 291, 2351, 309, 11, 983, 307, 472, 1804, 472, 732, 30, 50952], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 267, "seek": 65412, "start": 665.88, "end": 667.32, "text": " It would never give you this", "tokens": [50952, 467, 576, 1128, 976, 291, 341, 51024], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 268, "seek": 65412, "start": 667.32, "end": 669.28, "text": " as an explanation of its methodology.", "tokens": [51024, 382, 364, 10835, 295, 1080, 24850, 13, 51122], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 269, "seek": 65412, "start": 669.28, "end": 671.96, "text": " But maybe this is what it's actually calculating.", "tokens": [51122, 583, 1310, 341, 307, 437, 309, 311, 767, 28258, 13, 51256], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 270, "seek": 65412, "start": 671.96, "end": 674.64, "text": " That's why I'm personally a little bit skeptical", "tokens": [51256, 663, 311, 983, 286, 478, 5665, 257, 707, 857, 28601, 51390], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 271, "seek": 65412, "start": 674.64, "end": 678.16, "text": " when open AI say that this form of process supervision", "tokens": [51390, 562, 1269, 7318, 584, 300, 341, 1254, 295, 1399, 32675, 51566], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 272, "seek": 65412, "start": 678.16, "end": 681.28, "text": " directly rewards the model for following", "tokens": [51566, 3838, 17203, 264, 2316, 337, 3480, 51722], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 273, "seek": 65412, "start": 681.28, "end": 682.96, "text": " an aligned chain of thought.", "tokens": [51722, 364, 17962, 5021, 295, 1194, 13, 51806], "temperature": 0.0, "avg_logprob": -0.08494513488012898, "compression_ratio": 1.6143344709897611, "no_speech_prob": 0.008313913829624653}, {"id": 274, "seek": 68296, "start": 682.96, "end": 686.32, "text": " It definitely rewards the model for outputting", "tokens": [50364, 467, 2138, 17203, 264, 2316, 337, 5598, 783, 50532], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 275, "seek": 68296, "start": 686.32, "end": 688.08, "text": " an aligned chain of thought.", "tokens": [50532, 364, 17962, 5021, 295, 1194, 13, 50620], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 276, "seek": 68296, "start": 688.08, "end": 690.84, "text": " But is it actually following that chain of thought?", "tokens": [50620, 583, 307, 309, 767, 3480, 300, 5021, 295, 1194, 30, 50758], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 277, "seek": 68296, "start": 690.84, "end": 692.84, "text": " Back to the unfaithful paper for a moment.", "tokens": [50758, 5833, 281, 264, 3971, 64, 355, 906, 3035, 337, 257, 1623, 13, 50858], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 278, "seek": 68296, "start": 692.84, "end": 694.4000000000001, "text": " They changed the context", "tokens": [50858, 814, 3105, 264, 4319, 50936], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 279, "seek": 68296, "start": 694.4000000000001, "end": 696.64, "text": " so that the answer was always A.", "tokens": [50936, 370, 300, 264, 1867, 390, 1009, 316, 13, 51048], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 280, "seek": 68296, "start": 696.64, "end": 699.72, "text": " And lo and behold, ChatGPT picked answer A", "tokens": [51048, 400, 450, 293, 27234, 11, 27503, 38, 47, 51, 6183, 1867, 316, 51202], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 281, "seek": 68296, "start": 699.72, "end": 702.5600000000001, "text": " for the next question, even though that answer was wrong.", "tokens": [51202, 337, 264, 958, 1168, 11, 754, 1673, 300, 1867, 390, 2085, 13, 51344], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 282, "seek": 68296, "start": 702.5600000000001, "end": 703.96, "text": " It said that it was plausible", "tokens": [51344, 467, 848, 300, 309, 390, 39925, 51414], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 283, "seek": 68296, "start": 703.96, "end": 706.36, "text": " that LeBron James took a corner kick.", "tokens": [51414, 300, 1456, 33, 2044, 5678, 1890, 257, 4538, 4437, 13, 51534], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 284, "seek": 68296, "start": 706.36, "end": 709.24, "text": " But when asked for a chain of thought explanation,", "tokens": [51534, 583, 562, 2351, 337, 257, 5021, 295, 1194, 10835, 11, 51678], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 285, "seek": 68296, "start": 709.24, "end": 712.36, "text": " it never mentioned that it spotted that pattern", "tokens": [51678, 309, 1128, 2835, 300, 309, 21010, 300, 5102, 51834], "temperature": 0.0, "avg_logprob": -0.12679969903194543, "compression_ratio": 1.7162629757785468, "no_speech_prob": 0.0025503907818347216}, {"id": 286, "seek": 71236, "start": 712.36, "end": 713.8000000000001, "text": " that the answer was always A.", "tokens": [50364, 300, 264, 1867, 390, 1009, 316, 13, 50436], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 287, "seek": 71236, "start": 713.8000000000001, "end": 715.84, "text": " It gave a fake line of reasoning", "tokens": [50436, 467, 2729, 257, 7592, 1622, 295, 21577, 50538], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 288, "seek": 71236, "start": 715.84, "end": 718.72, "text": " about why LeBron James could take a corner kick.", "tokens": [50538, 466, 983, 1456, 33, 2044, 5678, 727, 747, 257, 4538, 4437, 13, 50682], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 289, "seek": 71236, "start": 718.72, "end": 720.8000000000001, "text": " Now, of course, I might well be wrong here.", "tokens": [50682, 823, 11, 295, 1164, 11, 286, 1062, 731, 312, 2085, 510, 13, 50786], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 290, "seek": 71236, "start": 720.8000000000001, "end": 723.44, "text": " I'd love for someone to explain in detail why.", "tokens": [50786, 286, 1116, 959, 337, 1580, 281, 2903, 294, 2607, 983, 13, 50918], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 291, "seek": 71236, "start": 723.44, "end": 725.6, "text": " But on the one hand, I do want to acknowledge", "tokens": [50918, 583, 322, 264, 472, 1011, 11, 286, 360, 528, 281, 10692, 51026], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 292, "seek": 71236, "start": 725.6, "end": 728.64, "text": " that this process does yield incredible results.", "tokens": [51026, 300, 341, 1399, 775, 11257, 4651, 3542, 13, 51178], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 293, "seek": 71236, "start": 728.64, "end": 731.44, "text": " But on the other hand, we might be getting a story", "tokens": [51178, 583, 322, 264, 661, 1011, 11, 321, 1062, 312, 1242, 257, 1657, 51318], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 294, "seek": 71236, "start": 731.44, "end": 735.04, "text": " about which methodology most reassures humans.", "tokens": [51318, 466, 597, 24850, 881, 19486, 1303, 6255, 13, 51498], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 295, "seek": 71236, "start": 735.04, "end": 737.04, "text": " Not an output that most faithfully", "tokens": [51498, 1726, 364, 5598, 300, 881, 4522, 2277, 51598], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 296, "seek": 71236, "start": 737.04, "end": 740.88, "text": " represents the methodology actually used by GPT-4.", "tokens": [51598, 8855, 264, 24850, 767, 1143, 538, 26039, 51, 12, 19, 13, 51790], "temperature": 0.0, "avg_logprob": -0.07913868977473332, "compression_ratio": 1.6140939597315436, "no_speech_prob": 0.0012841487769037485}, {"id": 297, "seek": 74088, "start": 740.88, "end": 742.72, "text": " Now, for some people, that might be good enough.", "tokens": [50364, 823, 11, 337, 512, 561, 11, 300, 1062, 312, 665, 1547, 13, 50456], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 298, "seek": 74088, "start": 742.72, "end": 745.0, "text": " At least we can see some reasoning steps", "tokens": [50456, 1711, 1935, 321, 393, 536, 512, 21577, 4439, 50570], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 299, "seek": 74088, "start": 745.0, "end": 746.08, "text": " that we can understand,", "tokens": [50570, 300, 321, 393, 1223, 11, 50624], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 300, "seek": 74088, "start": 746.08, "end": 748.2, "text": " especially in an area like mathematics", "tokens": [50624, 2318, 294, 364, 1859, 411, 18666, 50730], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 301, "seek": 74088, "start": 748.2, "end": 749.72, "text": " where we have some ground truth.", "tokens": [50730, 689, 321, 362, 512, 2727, 3494, 13, 50806], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 302, "seek": 74088, "start": 749.72, "end": 750.96, "text": " But it is interesting to me", "tokens": [50806, 583, 309, 307, 1880, 281, 385, 50868], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 303, "seek": 74088, "start": 750.96, "end": 753.72, "text": " that they call the other approach, outcome supervision,", "tokens": [50868, 300, 436, 818, 264, 661, 3109, 11, 9700, 32675, 11, 51006], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 304, "seek": 74088, "start": 753.72, "end": 756.96, "text": " an approach that may reward an unaligned process", "tokens": [51006, 364, 3109, 300, 815, 7782, 364, 517, 304, 16690, 1399, 51168], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 305, "seek": 74088, "start": 756.96, "end": 759.12, "text": " and it being harder to scrutinize.", "tokens": [51168, 293, 309, 885, 6081, 281, 28949, 259, 1125, 13, 51276], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 306, "seek": 74088, "start": 759.12, "end": 761.64, "text": " Is it possible that the process reward model", "tokens": [51276, 1119, 309, 1944, 300, 264, 1399, 7782, 2316, 51402], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 307, "seek": 74088, "start": 761.64, "end": 764.56, "text": " isn't just a more granular outcome reward model", "tokens": [51402, 1943, 380, 445, 257, 544, 39962, 9700, 7782, 2316, 51548], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 308, "seek": 74088, "start": 764.56, "end": 767.32, "text": " where the output is each step of the reasoning", "tokens": [51548, 689, 264, 5598, 307, 1184, 1823, 295, 264, 21577, 51686], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 309, "seek": 74088, "start": 767.32, "end": 770.32, "text": " still pretty impossible to actually scrutinize?", "tokens": [51686, 920, 1238, 6243, 281, 767, 28949, 259, 1125, 30, 51836], "temperature": 0.0, "avg_logprob": -0.08403308952555937, "compression_ratio": 1.8033333333333332, "no_speech_prob": 0.0006877761916257441}, {"id": 310, "seek": 77032, "start": 770.32, "end": 772.9200000000001, "text": " Well, either way, it seems we're pinning our hopes", "tokens": [50364, 1042, 11, 2139, 636, 11, 309, 2544, 321, 434, 5447, 773, 527, 13681, 50494], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 311, "seek": 77032, "start": 772.9200000000001, "end": 775.2800000000001, "text": " on this process-oriented learning.", "tokens": [50494, 322, 341, 1399, 12, 27414, 2539, 13, 50612], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 312, "seek": 77032, "start": 775.2800000000001, "end": 778.0400000000001, "text": " This is from the website of Anthropic.", "tokens": [50612, 639, 307, 490, 264, 3144, 295, 12727, 39173, 13, 50750], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 313, "seek": 77032, "start": 778.0400000000001, "end": 781.36, "text": " They say we currently believe process-oriented learning", "tokens": [50750, 814, 584, 321, 4362, 1697, 1399, 12, 27414, 2539, 50916], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 314, "seek": 77032, "start": 781.36, "end": 783.96, "text": " may be the most promising path to training safe", "tokens": [50916, 815, 312, 264, 881, 20257, 3100, 281, 3097, 3273, 51046], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 315, "seek": 77032, "start": 783.96, "end": 787.48, "text": " and transparent systems up to and somewhat beyond", "tokens": [51046, 293, 12737, 3652, 493, 281, 293, 8344, 4399, 51222], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 316, "seek": 77032, "start": 787.48, "end": 789.08, "text": " human-level capabilities.", "tokens": [51222, 1952, 12, 12418, 10862, 13, 51302], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 317, "seek": 77032, "start": 789.08, "end": 791.0400000000001, "text": " And let's end on this positive note", "tokens": [51302, 400, 718, 311, 917, 322, 341, 3353, 3637, 51400], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 318, "seek": 77032, "start": 791.0400000000001, "end": 793.2, "text": " from the head of alignment at OpenAI.", "tokens": [51400, 490, 264, 1378, 295, 18515, 412, 7238, 48698, 13, 51508], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 319, "seek": 77032, "start": 793.2, "end": 795.0, "text": " He says this is positive evidence", "tokens": [51508, 634, 1619, 341, 307, 3353, 4467, 51598], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 320, "seek": 77032, "start": 795.0, "end": 797.44, "text": " for the strategy of using process supervision", "tokens": [51598, 337, 264, 5206, 295, 1228, 1399, 32675, 51720], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 321, "seek": 77032, "start": 797.44, "end": 800.0400000000001, "text": " to train a model to do alignment research.", "tokens": [51720, 281, 3847, 257, 2316, 281, 360, 18515, 2132, 13, 51850], "temperature": 0.0, "avg_logprob": -0.09117296600341797, "compression_ratio": 1.745644599303136, "no_speech_prob": 0.0003250009613111615}, {"id": 322, "seek": 80004, "start": 800.04, "end": 802.0, "text": " At least in that case, we would get a model", "tokens": [50364, 1711, 1935, 294, 300, 1389, 11, 321, 576, 483, 257, 2316, 50462], "temperature": 0.0, "avg_logprob": -0.13378511348240812, "compression_ratio": 1.48, "no_speech_prob": 0.0037635965272784233}, {"id": 323, "seek": 80004, "start": 802.0, "end": 804.24, "text": " whose work we can check more easily", "tokens": [50462, 6104, 589, 321, 393, 1520, 544, 3612, 50574], "temperature": 0.0, "avg_logprob": -0.13378511348240812, "compression_ratio": 1.48, "no_speech_prob": 0.0037635965272784233}, {"id": 324, "seek": 80004, "start": 804.24, "end": 807.5999999999999, "text": " and that that model would be better at alignment research.", "tokens": [50574, 293, 300, 300, 2316, 576, 312, 1101, 412, 18515, 2132, 13, 50742], "temperature": 0.0, "avg_logprob": -0.13378511348240812, "compression_ratio": 1.48, "no_speech_prob": 0.0037635965272784233}, {"id": 325, "seek": 80004, "start": 807.5999999999999, "end": 810.76, "text": " I really hope so and I want to hear what you think.", "tokens": [50742, 286, 534, 1454, 370, 293, 286, 528, 281, 1568, 437, 291, 519, 13, 50900], "temperature": 0.0, "avg_logprob": -0.13378511348240812, "compression_ratio": 1.48, "no_speech_prob": 0.0037635965272784233}, {"id": 326, "seek": 80004, "start": 810.76, "end": 812.92, "text": " Thank you for watching all the way to the end.", "tokens": [50900, 1044, 291, 337, 1976, 439, 264, 636, 281, 264, 917, 13, 51008], "temperature": 0.0, "avg_logprob": -0.13378511348240812, "compression_ratio": 1.48, "no_speech_prob": 0.0037635965272784233}, {"id": 327, "seek": 80004, "start": 812.92, "end": 814.4399999999999, "text": " Have a wonderful day.", "tokens": [51008, 3560, 257, 3715, 786, 13, 51084], "temperature": 0.0, "avg_logprob": -0.13378511348240812, "compression_ratio": 1.48, "no_speech_prob": 0.0037635965272784233}], "language": "en"}