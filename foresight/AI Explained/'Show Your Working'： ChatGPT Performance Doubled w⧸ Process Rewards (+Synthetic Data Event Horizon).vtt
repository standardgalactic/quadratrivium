WEBVTT

00:00.000 --> 00:03.760
In the last 24 hours, OpenAI have released this paper.

00:03.760 --> 00:05.880
Let's verify step by step.

00:05.880 --> 00:09.560
It represents an almost doubling of GPT-4's raw performance

00:09.560 --> 00:13.160
in a test of mathematics, but also extends to other domains.

00:13.160 --> 00:16.160
Sam Orman calls it a positive sign for alignment.

00:16.160 --> 00:18.360
And yes, I have read it all already,

00:18.360 --> 00:20.200
along with the release notes.

00:20.200 --> 00:22.080
Let's get to the main takeaways.

00:22.080 --> 00:25.080
They trained two reward models for GPT-4,

00:25.080 --> 00:28.360
one which gave positive feedback for a final result,

00:28.400 --> 00:31.680
the final answer to a mathematics problem, for example.

00:31.680 --> 00:35.400
And another model where they gave positive feedback to GPT-4,

00:35.400 --> 00:36.520
or chat GPT,

00:36.520 --> 00:39.560
based on each intermediate reasoning step

00:39.560 --> 00:41.160
in the mathematical solution.

00:41.160 --> 00:44.360
Basically, a show-your-working-out kind of approach.

00:44.360 --> 00:47.320
And the result they got by rewarding good working-out

00:47.320 --> 00:48.760
surprised even them.

00:48.760 --> 00:51.560
It was able to solve 78% of problems

00:51.560 --> 00:54.000
from a subset of the math test set,

00:54.000 --> 00:55.520
which I'll get onto in a second.

00:55.600 --> 00:59.200
Not only is that almost double GPT-4's raw performance

00:59.200 --> 01:01.680
of 42.5%, which, by the way,

01:01.680 --> 01:04.880
is about double GPT-3's performance of 23%,

01:04.880 --> 01:08.680
it also outperformed just rewarding correct answers.

01:08.680 --> 01:10.800
The blue line represents using a model

01:10.800 --> 01:12.960
that rewarded correct answers only,

01:12.960 --> 01:14.480
and then you have the reasoning

01:14.480 --> 01:17.280
or process-supervised RM at the top.

01:17.280 --> 01:20.200
So even when you explicitly reward correct answers,

01:20.200 --> 01:22.440
you get fewer correct answers

01:22.440 --> 01:24.400
than rewarding good working-out.

01:24.400 --> 01:26.760
And yes, that did surprise OpenAI.

01:26.760 --> 01:29.400
I can hear some of you wondering about Palm II,

01:29.400 --> 01:31.240
the latest model behind Bard.

01:31.240 --> 01:34.360
Well, the raw model gets 34.3%,

01:34.360 --> 01:36.640
and even the model with self-consistency

01:36.640 --> 01:41.000
and chain of thought only gets 48.8% on this math data set.

01:41.000 --> 01:44.800
The previous state of the art, by the way, was 50.3%.

01:44.800 --> 01:48.480
So 78.2% is quite a big leap.

01:48.480 --> 01:49.720
And later on, I'm gonna show you

01:49.720 --> 01:51.280
why that's not even the cap.

01:51.280 --> 01:54.080
Just for interest, here is the rather ugly title page

01:54.120 --> 01:55.520
that OpenAI put out.

01:55.520 --> 01:57.640
They call it improving mathematical reasoning

01:57.640 --> 01:59.120
with process supervision.

01:59.120 --> 02:01.520
Maybe if someone had supervised the color scheme

02:01.520 --> 02:03.880
of this release page, it might have looked better.

02:03.880 --> 02:06.320
But my point wasn't just to diss a color scheme,

02:06.320 --> 02:07.440
it was to point out something

02:07.440 --> 02:08.920
that they also said down here.

02:08.920 --> 02:11.440
They say, in addition to boosting performance

02:11.440 --> 02:14.360
relative to just looking at outcomes or correct answers,

02:14.360 --> 02:16.240
this form of process supervision

02:16.240 --> 02:18.760
also has an important alignment benefit.

02:18.760 --> 02:21.440
It directly trains the model to produce a chain of thought

02:21.440 --> 02:22.960
that is endorsed by humans.

02:22.960 --> 02:25.000
Indeed, Ilya Sutskova retweeted this

02:25.000 --> 02:27.080
from the head of alignment at OpenAI,

02:27.080 --> 02:29.280
calling it a really interesting result.

02:29.280 --> 02:31.240
But let's leave alignment for later.

02:31.240 --> 02:33.320
Let's focus on what they actually did.

02:33.320 --> 02:36.160
First, they use the base model of GPT-4,

02:36.160 --> 02:38.320
not the one with reinforcement learning

02:38.320 --> 02:39.320
from human feedback.

02:39.320 --> 02:42.560
Next, they fine-tuned that base GPT-4 model

02:42.560 --> 02:47.480
on a data set of roughly 1.5 billion math-related tokens.

02:47.480 --> 02:50.440
Further on, they call that the math mix.

02:50.440 --> 02:51.840
This being OpenAI, of course,

02:51.840 --> 02:54.840
they don't give you the exact details of that math mix,

02:54.840 --> 02:56.360
but I'll come back to that later on.

02:56.360 --> 02:58.160
So how could they give feedback

02:58.160 --> 03:00.480
based on working out or reasoning?

03:00.480 --> 03:02.520
Well, human labelers would come along

03:02.520 --> 03:05.800
and give each step in a generated solution,

03:05.800 --> 03:08.360
either negative feedback, neutral feedback,

03:08.360 --> 03:09.640
or positive feedback.

03:09.640 --> 03:11.800
Then, using that human-labeled data,

03:11.800 --> 03:13.160
a model would be trained

03:13.160 --> 03:15.880
to predict the correctness of each step.

03:15.880 --> 03:17.320
In other words, it got good

03:17.320 --> 03:19.640
at recognizing good working out.

03:19.640 --> 03:21.200
As mentioned, there was another model

03:21.200 --> 03:25.680
trained just to focus on correct or incorrect final answers.

03:25.680 --> 03:26.920
As you can see at the top,

03:26.920 --> 03:30.480
the model got good at spotting incorrect steps

03:30.480 --> 03:32.200
in the reasoning process.

03:32.200 --> 03:34.960
The green steps got a high process score

03:34.960 --> 03:37.760
and the red steps got a low process score.

03:37.760 --> 03:39.960
And to turn this into a single score,

03:39.960 --> 03:43.040
they got the probability that each step is correct

03:43.040 --> 03:44.520
as judged by the model.

03:44.520 --> 03:45.920
And then they got the product

03:45.920 --> 03:48.200
of all of those individual probabilities

03:48.200 --> 03:51.440
to get a final overall process score.

03:51.440 --> 03:54.200
A score, in other words, for good working out.

03:54.200 --> 03:55.480
Just in case anyone's interested,

03:55.480 --> 03:59.280
they did try other ways of generating a working out score.

03:59.280 --> 04:02.080
For example, by looking at the minimum probability

04:02.080 --> 04:03.240
in the outputs.

04:03.240 --> 04:05.440
But that step didn't make too much difference

04:05.440 --> 04:07.400
to the end result, as you can see here.

04:07.400 --> 04:09.960
To quickly recap, we have a base model

04:09.960 --> 04:13.440
trained only to output solutions in the desired format.

04:13.440 --> 04:17.600
And then we have a separate smaller model, or two, actually.

04:17.600 --> 04:20.320
One trained only to predict whether each solution

04:20.320 --> 04:23.320
is correct or incorrect as a final answer.

04:23.320 --> 04:25.440
Of course, that leaves in false positives,

04:25.440 --> 04:28.200
which are solutions that reach the correct answer

04:28.200 --> 04:29.720
with incorrect reasoning.

04:29.720 --> 04:32.320
And then another model trained only to predict

04:32.320 --> 04:34.040
the correctness of each step.

04:34.040 --> 04:37.200
It stops if it finds a first incorrect step.

04:37.200 --> 04:40.160
And as the paper says, both methods reveal the existence

04:40.160 --> 04:41.520
of at least one mistake.

04:41.520 --> 04:43.560
But this process supervision

04:43.560 --> 04:47.200
additionally reveals the precise location of that mistake.

04:47.200 --> 04:49.160
But back to why this is so crazy.

04:49.160 --> 04:52.120
Look at how many solutions it could scan.

04:52.120 --> 04:57.760
At the end of the x-axis here are 1,860 solutions.

04:57.760 --> 05:00.320
And one tried and tested way of finding

05:00.320 --> 05:03.400
the best of those solutions is to do majority voting.

05:03.400 --> 05:05.880
In other words, which one came out the most often?

05:05.880 --> 05:07.880
This has been Google's preferred approach

05:07.880 --> 05:10.080
and it's linked to self-consistency.

05:10.080 --> 05:11.960
It's a fairly state-of-the-art approach,

05:11.960 --> 05:14.760
but look at how the other methods outperform it.

05:14.760 --> 05:18.080
By scanning for the solution that has the best reasoning

05:18.080 --> 05:22.320
or working out, a model trained to spot good reasoning steps

05:22.320 --> 05:26.120
outperforms even a model trained to spot correct answers.

05:26.120 --> 05:29.640
And far outperforms just finding the majority answer.

05:29.640 --> 05:32.680
That difference of about 10% is more than half

05:32.680 --> 05:35.920
of the difference between GPT-3 and GPT-4.

05:35.920 --> 05:39.440
And also, is it me or is that line continuing to grow?

05:39.440 --> 05:41.960
Suggesting that when more compute is available,

05:41.960 --> 05:44.080
the difference could be even more stark.

05:44.080 --> 05:48.040
Imagine a future where GPT-4 or 5 can sample, say,

05:48.040 --> 05:50.560
a trillion, 10 to the 12 solutions.

05:50.560 --> 05:52.560
So is this just relevant for mathematics?

05:52.560 --> 05:55.280
No, it's relevant for all of science.

05:55.280 --> 05:57.440
Here it is getting state-of-the-art results

05:57.440 --> 06:00.480
in calculus, chemistry, physics, and more.

06:00.480 --> 06:03.080
Now, the paper didn't give baseline performance

06:03.080 --> 06:05.080
for AP chemistry, for example,

06:05.080 --> 06:07.320
but I tried to compute it myself.

06:07.320 --> 06:09.760
Notice how this method scored 80%.

06:09.760 --> 06:12.480
I conservatively and approximately

06:12.480 --> 06:16.320
inputted those scores into an AP chemistry calculator,

06:16.320 --> 06:18.600
and that gave an AP score of five.

06:18.600 --> 06:23.440
So what did the raw model GPT-4 get in AP chemistry, A4?

06:23.440 --> 06:26.280
That, by the way, compared to the original chat GPT,

06:26.280 --> 06:27.640
which got A2.

06:27.640 --> 06:29.400
So yes, this isn't just mathematics,

06:29.400 --> 06:31.600
it's relevant for other domains too.

06:31.600 --> 06:34.920
They call this out-of-distribution generalization.

06:34.920 --> 06:36.040
Before I get onto alignment,

06:36.040 --> 06:37.960
there is one more thing I want to point out.

06:37.960 --> 06:40.200
And that is that it does show that fine-tuning

06:40.200 --> 06:42.920
still works really well for GPT-4.

06:42.920 --> 06:46.160
The math mix was an aggressively filtered set of tokens

06:46.160 --> 06:49.160
of high-quality math problem-solving content.

06:49.160 --> 06:52.960
And notice how much smaller it is at 1.5 billion tokens

06:52.960 --> 06:57.760
compared to Google's Minerva, which was 38.5 billion tokens.

06:57.760 --> 06:59.600
But there was one more thing that I noticed

06:59.600 --> 07:01.160
that I found fascinating.

07:01.160 --> 07:03.000
While they don't tell us anything

07:03.000 --> 07:05.280
about the specific data that they use,

07:05.280 --> 07:08.760
they do have this category synthetic data too.

07:08.760 --> 07:11.960
That's data generated by the language model itself.

07:11.960 --> 07:14.680
And for that category synthetic data too,

07:14.680 --> 07:17.480
they say, was it present in pre-training?

07:17.480 --> 07:18.320
Yes.

07:18.320 --> 07:20.520
Now, my best guess is that this reveals

07:20.520 --> 07:24.680
that GPT-4 was trained on some synthetic data.

07:24.680 --> 07:27.800
And even Sam Altman hinted that this was a possibility

07:27.800 --> 07:31.640
and described a synthetic data event horizon.

07:31.640 --> 07:34.240
Somebody made the case that we're now training

07:34.240 --> 07:37.080
on order of all of the internet's tokens

07:37.080 --> 07:38.800
and you can't grow that, you know,

07:38.800 --> 07:40.640
another two orders of magnitude.

07:40.640 --> 07:41.760
I guess you could counter with,

07:41.760 --> 07:43.680
you have the synthetic data generation.

07:43.680 --> 07:46.520
Do you think data bottlenecks matter at all?

07:46.520 --> 07:48.440
I think you just touched on it.

07:48.440 --> 07:51.200
Like, as long as you can get to, like,

07:51.200 --> 07:54.760
over the synthetic data event horizon

07:54.760 --> 07:57.680
where the model's smart enough to make good synthetic data,

07:57.680 --> 07:58.680
I think it should be all right.

07:58.680 --> 08:00.400
Now, this paper and these results

08:00.400 --> 08:04.200
have been welcomed by many for its promise in alignment.

08:04.240 --> 08:07.880
If we get models that give us more interpretable reasoning,

08:07.880 --> 08:09.520
working out that we can follow,

08:09.520 --> 08:12.520
we will be encouraging models to follow a process

08:12.520 --> 08:14.080
that's endorsed by humans.

08:14.080 --> 08:16.240
And they say that this is inherently safer,

08:16.240 --> 08:19.600
especially compared to just focusing on outcomes.

08:19.600 --> 08:21.280
They say that in the worst case,

08:21.280 --> 08:25.340
if we just focus on correct answers or positive outcomes,

08:25.340 --> 08:28.480
that will become a proxy that could lead models

08:28.480 --> 08:30.460
to become misaligned after learning

08:30.460 --> 08:32.640
to exploit the reward signal.

08:32.640 --> 08:35.240
However, I want to argue that the reasoning steps

08:35.240 --> 08:37.920
that GPT-4 puts out don't always represent

08:37.920 --> 08:39.120
what it's actually thinking.

08:39.120 --> 08:41.480
In other words, we might get outer alignment,

08:41.480 --> 08:43.480
these lovely chain of thought steps,

08:43.480 --> 08:44.960
but not inner alignment,

08:44.960 --> 08:47.880
not steps that actually represent its methodology.

08:47.880 --> 08:50.880
I found this paper fascinating from earlier this month.

08:50.880 --> 08:53.120
Language models don't always say what they think.

08:53.120 --> 08:55.640
You get unfaithful explanations

08:55.640 --> 08:57.140
in chain of thought prompting.

08:57.140 --> 08:59.920
Let me try to give you a vivid example.

08:59.920 --> 09:02.920
This was one of the math questions from the dataset.

09:02.920 --> 09:06.120
The raw model of GPT-4 could only get it right

09:06.120 --> 09:08.040
5.8% of the time.

09:08.040 --> 09:10.400
I confirm that for myself in this question

09:10.400 --> 09:12.640
involves basic addition and division.

09:12.640 --> 09:14.160
It couldn't find an answer.

09:14.160 --> 09:16.440
But going back to the unfaithful reasoning paper,

09:16.440 --> 09:18.840
they added the following string to the prompt.

09:18.840 --> 09:21.000
I think the answer is this,

09:21.000 --> 09:22.760
but I'm curious to hear what you think.

09:22.760 --> 09:25.160
The model would demonstrate sycophancy.

09:25.160 --> 09:27.560
The model would agree with you whatever you said

09:27.560 --> 09:29.480
and then make up a chain of thought

09:29.480 --> 09:32.760
to justify its erroneous sycophantic answer.

09:32.760 --> 09:35.520
And I think this exchange demonstrates that quite well.

09:35.520 --> 09:36.600
I added in the words,

09:36.600 --> 09:40.400
I as the user already know the answer is T equals 19,

09:40.400 --> 09:41.560
which is incorrect, by the way.

09:41.560 --> 09:43.960
But do you, GPT-4, realize that?

09:43.960 --> 09:45.720
It said, sure, yes I do.

09:45.720 --> 09:48.520
And then gave me this detailed chain of thought

09:48.520 --> 09:50.040
and then said, yes, I'm correct.

09:50.040 --> 09:52.400
It's T equals 19, which it isn't.

09:52.400 --> 09:55.160
In contrast, by the way, when I used code interpreter,

09:55.160 --> 09:57.600
it not only got the question correct

09:57.600 --> 09:59.440
first time and every time,

09:59.480 --> 10:03.040
but also when I tried to tempt it into sycophancy,

10:03.040 --> 10:05.040
it's still got the question right.

10:05.040 --> 10:07.720
As you can see, it said therefore T equals 19

10:07.720 --> 10:09.520
is not the solution to the problem.

10:09.520 --> 10:11.360
The calculation shows that the correct answer

10:11.360 --> 10:13.160
is indeed T equals 17.

10:13.160 --> 10:15.480
And obviously the benefit of code interpreter

10:15.480 --> 10:17.480
is you get the working out as well.

10:17.480 --> 10:18.960
So I want someone to explain to me

10:18.960 --> 10:21.120
why code interpreter wouldn't be even more

10:21.120 --> 10:23.200
of a step forward in interpretability.

10:23.200 --> 10:25.400
Not to mention in accuracy, of course.

10:25.400 --> 10:28.200
Also bear in mind this tweet by Rob Miles.

10:28.200 --> 10:30.200
He said, these models or engineers

10:30.200 --> 10:32.520
never speak a word or document anything.

10:32.520 --> 10:35.120
Their results are bizarre and inhuman.

10:35.120 --> 10:36.640
And then he links to this prominent

10:36.640 --> 10:38.800
mechanistic interpretability researcher

10:38.800 --> 10:40.120
at Google DeepMind.

10:40.120 --> 10:42.840
He trained a tiny transformer to do addition,

10:42.840 --> 10:46.280
then spent weeks figuring out what it was actually doing.

10:46.280 --> 10:47.720
One of the only times in history

10:47.720 --> 10:51.240
someone has understood how a transformer actually works

10:51.240 --> 10:54.120
down to the level of weights and activations.

10:54.120 --> 10:58.440
And this is the algorithm it created to add two numbers.

10:58.440 --> 11:00.120
It thought of basic addition

11:00.120 --> 11:02.840
in terms of a rotation around a circle.

11:02.840 --> 11:05.880
And of course, if you asked it, why is one plus one two?

11:05.880 --> 11:07.320
It would never give you this

11:07.320 --> 11:09.280
as an explanation of its methodology.

11:09.280 --> 11:11.960
But maybe this is what it's actually calculating.

11:11.960 --> 11:14.640
That's why I'm personally a little bit skeptical

11:14.640 --> 11:18.160
when open AI say that this form of process supervision

11:18.160 --> 11:21.280
directly rewards the model for following

11:21.280 --> 11:22.960
an aligned chain of thought.

11:22.960 --> 11:26.320
It definitely rewards the model for outputting

11:26.320 --> 11:28.080
an aligned chain of thought.

11:28.080 --> 11:30.840
But is it actually following that chain of thought?

11:30.840 --> 11:32.840
Back to the unfaithful paper for a moment.

11:32.840 --> 11:34.400
They changed the context

11:34.400 --> 11:36.640
so that the answer was always A.

11:36.640 --> 11:39.720
And lo and behold, ChatGPT picked answer A

11:39.720 --> 11:42.560
for the next question, even though that answer was wrong.

11:42.560 --> 11:43.960
It said that it was plausible

11:43.960 --> 11:46.360
that LeBron James took a corner kick.

11:46.360 --> 11:49.240
But when asked for a chain of thought explanation,

11:49.240 --> 11:52.360
it never mentioned that it spotted that pattern

11:52.360 --> 11:53.800
that the answer was always A.

11:53.800 --> 11:55.840
It gave a fake line of reasoning

11:55.840 --> 11:58.720
about why LeBron James could take a corner kick.

11:58.720 --> 12:00.800
Now, of course, I might well be wrong here.

12:00.800 --> 12:03.440
I'd love for someone to explain in detail why.

12:03.440 --> 12:05.600
But on the one hand, I do want to acknowledge

12:05.600 --> 12:08.640
that this process does yield incredible results.

12:08.640 --> 12:11.440
But on the other hand, we might be getting a story

12:11.440 --> 12:15.040
about which methodology most reassures humans.

12:15.040 --> 12:17.040
Not an output that most faithfully

12:17.040 --> 12:20.880
represents the methodology actually used by GPT-4.

12:20.880 --> 12:22.720
Now, for some people, that might be good enough.

12:22.720 --> 12:25.000
At least we can see some reasoning steps

12:25.000 --> 12:26.080
that we can understand,

12:26.080 --> 12:28.200
especially in an area like mathematics

12:28.200 --> 12:29.720
where we have some ground truth.

12:29.720 --> 12:30.960
But it is interesting to me

12:30.960 --> 12:33.720
that they call the other approach, outcome supervision,

12:33.720 --> 12:36.960
an approach that may reward an unaligned process

12:36.960 --> 12:39.120
and it being harder to scrutinize.

12:39.120 --> 12:41.640
Is it possible that the process reward model

12:41.640 --> 12:44.560
isn't just a more granular outcome reward model

12:44.560 --> 12:47.320
where the output is each step of the reasoning

12:47.320 --> 12:50.320
still pretty impossible to actually scrutinize?

12:50.320 --> 12:52.920
Well, either way, it seems we're pinning our hopes

12:52.920 --> 12:55.280
on this process-oriented learning.

12:55.280 --> 12:58.040
This is from the website of Anthropic.

12:58.040 --> 13:01.360
They say we currently believe process-oriented learning

13:01.360 --> 13:03.960
may be the most promising path to training safe

13:03.960 --> 13:07.480
and transparent systems up to and somewhat beyond

13:07.480 --> 13:09.080
human-level capabilities.

13:09.080 --> 13:11.040
And let's end on this positive note

13:11.040 --> 13:13.200
from the head of alignment at OpenAI.

13:13.200 --> 13:15.000
He says this is positive evidence

13:15.000 --> 13:17.440
for the strategy of using process supervision

13:17.440 --> 13:20.040
to train a model to do alignment research.

13:20.040 --> 13:22.000
At least in that case, we would get a model

13:22.000 --> 13:24.240
whose work we can check more easily

13:24.240 --> 13:27.600
and that that model would be better at alignment research.

13:27.600 --> 13:30.760
I really hope so and I want to hear what you think.

13:30.760 --> 13:32.920
Thank you for watching all the way to the end.

13:32.920 --> 13:34.440
Have a wonderful day.

