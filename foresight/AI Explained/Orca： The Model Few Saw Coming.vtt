WEBVTT

00:00.000 --> 00:03.840
Do you remember this paper, less than two weeks old?

00:03.840 --> 00:07.200
It made waves by concluding that open source models

00:07.200 --> 00:11.920
can mimic the style, but not the factuality of chat GPT.

00:11.920 --> 00:14.240
Overall, we can conclude they say

00:14.240 --> 00:17.520
that model imitation is a false promise.

00:17.520 --> 00:23.840
Well, 48 hours ago, we have this, a 51 page report on Orca,

00:23.840 --> 00:27.200
based on a small 13 billion parameter model.

00:27.200 --> 00:29.520
I don't often comment on open source models

00:29.520 --> 00:32.800
because they're simply not competitive with open AI's models.

00:32.800 --> 00:36.640
But Orca is not just competitive with GPT 3.5.

00:36.640 --> 00:40.320
It beats it in quite a few well-established benchmarks

00:40.320 --> 00:44.320
and even matches GPT-4 in a couple of tests of reasoning.

00:44.320 --> 00:46.400
As always, I've read both papers in full

00:46.400 --> 00:48.800
and can also bring in just-released comments

00:48.800 --> 00:53.920
from Sam Altman and Ilya Sutskova on competition from open source models.

00:53.920 --> 00:59.040
But let's start with Orca, named presumably because Orca's or killer whales

00:59.040 --> 01:02.000
are frequent visitors to South American coastlines.

01:02.000 --> 01:05.680
And South America is, of course, the land of llamas and vicunas.

01:05.680 --> 01:08.720
But all the research was done by Microsoft,

01:08.720 --> 01:12.000
which I find interesting and I'll come back to that at the end.

01:12.000 --> 01:14.880
But why did they make Orca and why does it perform better

01:14.880 --> 01:17.440
than models like llama, alpaca, and vicuna?

01:17.440 --> 01:19.200
Well, they say here in the abstract

01:19.200 --> 01:22.400
that those other models lack rigorous evaluation,

01:22.400 --> 01:25.920
resulting in overestimating the small model's capability

01:25.920 --> 01:28.640
as they tend to learn to imitate the style

01:28.720 --> 01:32.560
but not the reasoning of LFMs, large foundation models.

01:32.560 --> 01:34.880
To address these challenges, we developed Orca,

01:34.880 --> 01:38.000
a 13 billion parameter model, that learns to imitate

01:38.000 --> 01:40.880
the reasoning process of the larger models.

01:40.880 --> 01:45.360
Orca learned by looking at GPT-4's step-by-step thought processes

01:45.360 --> 01:50.640
and is guided by teacher assistants from chat GPT, which is GPT 3.5.

01:50.640 --> 01:52.560
And to give you a taste of what's to come,

01:52.560 --> 01:55.760
Orca surpasses conventional state-of-the-art models,

01:55.760 --> 01:58.960
such as vicuna, by more than 100%

01:58.960 --> 02:02.400
in complex zero-shot reasoning benchmarks,

02:02.400 --> 02:04.960
like the big bench hard, which I'll talk about,

02:04.960 --> 02:08.400
and by 42% on AGI eval.

02:08.400 --> 02:12.480
It goes on, Orca reaches parity with chat GPT

02:12.480 --> 02:16.400
on the big bench hard and shows competitive performance

02:16.400 --> 02:18.800
in professional and academic examinations

02:18.800 --> 02:21.520
by the SAT, LSAT, GRE, and GMAT.

02:21.520 --> 02:24.320
And I know many of you will be interested in this footnote.

02:24.320 --> 02:28.160
We are working with our legal team to publicly release

02:28.160 --> 02:30.480
a diff of the model weights in accordance

02:30.480 --> 02:32.240
with Lama's release policy.

02:32.240 --> 02:34.000
So if this is anything like Lama,

02:34.000 --> 02:36.320
it's going to be leaked across the internet imminently.

02:36.320 --> 02:39.760
I'm going to show you so many tests and benchmarks in a moment,

02:39.760 --> 02:41.440
but just to give you a sample,

02:41.440 --> 02:46.560
here is Orca outperforming chat GPT in the vicuna evaluation set

02:46.560 --> 02:51.600
and matching text DaVinci 3 in the SAT, LSAT, GRE, and GMAT.

02:51.600 --> 02:52.960
And as I'll touch on later,

02:52.960 --> 02:55.840
this was zero shot without chain of thought

02:55.840 --> 02:57.440
or any advanced methods.

02:57.440 --> 02:59.520
You can watch pretty much any of my other videos

02:59.520 --> 03:01.760
to see how advanced prompt engineering

03:01.760 --> 03:04.400
would probably boost those results still further.

03:04.400 --> 03:05.520
For those who didn't know,

03:05.520 --> 03:10.480
13 billion parameters is about 7% the size of GPT-3,

03:10.480 --> 03:12.880
which is 175 billion parameters,

03:12.880 --> 03:17.680
and possibly around one or 2% of GPT-4's size.

03:17.680 --> 03:20.000
That gives you an indication of the difference

03:20.080 --> 03:23.360
in size between Orca and these models that it's competing with.

03:23.360 --> 03:24.880
And if that doesn't make any sense,

03:24.880 --> 03:28.800
a smaller size means it can be run on much smaller devices,

03:28.800 --> 03:31.680
like a desktop or even possibly a laptop.

03:31.680 --> 03:34.720
The authors start off by giving a little slap to the other paper.

03:34.720 --> 03:37.920
You know that one that said model imitation is a false promise.

03:37.920 --> 03:40.480
And they continue that contrary to this assertion,

03:40.480 --> 03:45.040
it is possible to reduce the gap with proprietary LLMs

03:45.040 --> 03:49.360
on multiple zero shot benchmarks that require sophisticated reasoning.

03:49.360 --> 03:54.160
As we'll see, models like Vakuna claim to have 90% of chat GPT's quality,

03:54.160 --> 03:57.600
but when it came to reasoning tasks or more technical tasks,

03:57.600 --> 03:58.720
it basically flopped.

03:58.720 --> 04:00.800
Here's a chart I'll come back to outlining

04:00.800 --> 04:04.240
some of the more technical challenges you can give a language model.

04:04.240 --> 04:07.440
We should remember that Vakuna is a fine-tuned version

04:07.440 --> 04:08.880
of the Llama model,

04:08.880 --> 04:12.320
and it's competitive or even better than Palm II.

04:12.320 --> 04:15.840
But give it some of the harder challenges for a language model,

04:15.840 --> 04:18.720
and it really struggles, as you can see in this column.

04:18.720 --> 04:22.240
Take logical deduction, where it only scored 1.2%.

04:22.240 --> 04:26.320
Well, this Orca model was 2,900% better than that,

04:26.320 --> 04:29.520
scoring 36% competitive with chat GPT.

04:29.520 --> 04:31.840
I'm going to come back to the big benchmark,

04:31.840 --> 04:34.320
but look for a second at causal judgment,

04:34.320 --> 04:39.440
where Orca, a 13 billion parameter model matches GPT4,

04:39.440 --> 04:42.160
which is about 100 times the size.

04:42.160 --> 04:44.080
But back to how they actually did it.

04:44.080 --> 04:48.640
Models like Alpaca and Vakuna were given lots of query and responses

04:48.640 --> 04:50.880
from chat GPT or GPT4.

04:50.880 --> 04:54.080
But what they did is they leveraged system instructions,

04:54.080 --> 04:58.000
asking models like GPT4 and chat GPT to think step by step.

04:58.000 --> 05:02.080
This gave Orca access to detailed responses from the model

05:02.080 --> 05:05.200
that explained the reasoning process of the teacher

05:05.200 --> 05:06.800
as it generates the response.

05:06.800 --> 05:10.720
It allowed these parent models of GPT3.5 and GPT4

05:10.720 --> 05:14.160
to be much better tutors for this young Orca.

05:14.240 --> 05:17.680
Also, they let the teachers of chat GPT, which is 3.5,

05:17.680 --> 05:21.040
and GPT4 give far more examples to their student.

05:21.040 --> 05:24.080
5 million and 1 million examples, respectively.

05:24.080 --> 05:26.240
That compares to the other models you may have heard of,

05:26.240 --> 05:28.800
like Alpaca, Wizard, Vakuna, etc.,

05:28.800 --> 05:33.040
which had tens of thousands or the low hundreds of thousands of examples.

05:33.040 --> 05:36.080
But again, the key difference is the explanations,

05:36.080 --> 05:40.000
the step by step thinking that the smaller Orca could then imitate.

05:40.000 --> 05:43.040
They give a quick demo here of how the other open source models

05:43.040 --> 05:44.960
learn from their GPT parents,

05:44.960 --> 05:48.320
with a simplistic question and answer format.

05:48.320 --> 05:51.520
In contrast, the authors leveraged system messages

05:51.520 --> 05:55.600
to get chat GPT and GPT4 to think step by step,

05:55.600 --> 06:00.080
leading to much richer explanations, as you can see in this diagram.

06:00.080 --> 06:02.000
It wasn't just let's think step by step,

06:02.000 --> 06:05.120
by the way, also things like explain like I'm 5.

06:05.120 --> 06:09.600
They also wanted the task to be as complex and diverse as possible,

06:09.600 --> 06:11.920
so they used the Flan collection.

06:11.920 --> 06:14.000
This was released by Google in February,

06:14.000 --> 06:17.600
and focused on balancing the kind of prompts and tasks

06:17.600 --> 06:19.920
that you fine tune the language models on.

06:19.920 --> 06:22.480
You can see here the 16 system messages

06:22.480 --> 06:25.360
that they give to chat GPT and GPT4,

06:25.360 --> 06:28.080
and you can see here the kind of difference that that makes.

06:28.080 --> 06:31.360
Imagine a language model trying to learn from this human.

06:31.360 --> 06:32.480
The human is asked,

06:32.480 --> 06:34.640
pick which sentence is not logical.

06:34.640 --> 06:37.760
Sentence A, people in the desert often look forward to flood,

06:37.760 --> 06:41.040
or sentence B, people in the desert often look forward to rain.

06:41.040 --> 06:42.240
The human responds,

06:42.240 --> 06:44.480
there is no reason to look forward to a flood,

06:44.480 --> 06:47.520
because floods cause damage, the answer is sentence A.

06:47.520 --> 06:50.080
Now yes, a language model can learn from that,

06:50.080 --> 06:53.200
but by leveraging those system assistant messages,

06:53.200 --> 06:55.680
look at the kind of response that GPT4 gives.

06:55.680 --> 06:59.120
Now Orca can learn a lot more from that explanation,

06:59.120 --> 07:01.680
and that's one of the main reasons it's better

07:01.680 --> 07:03.920
than all the other open source models.

07:03.920 --> 07:07.760
Because remember, Vikuna is the best of the open source models.

07:07.760 --> 07:10.880
In this leaderboard, it has an elo of 1054,

07:10.880 --> 07:13.120
better even than Palm II Bison.

07:13.120 --> 07:15.520
All the models higher than it are proprietary.

07:15.520 --> 07:18.960
But there is another reason why Orca performs so much better.

07:18.960 --> 07:19.840
You might have wondered,

07:19.840 --> 07:22.400
why didn't they just use only GPT4?

07:22.400 --> 07:25.200
Well yes, there were cost and time considerations,

07:25.200 --> 07:27.120
but there was another factor that they found.

07:27.120 --> 07:32.400
They were able to use chat GPT or GPT3.5 as an intermediate teacher.

07:32.400 --> 07:36.800
That teacher, chat GPT, was able to reduce the gap in capabilities.

07:36.800 --> 07:39.600
So Orca got smarter and better able to learn.

07:39.600 --> 07:41.200
A bit like progressive learning,

07:41.200 --> 07:43.680
where you first learn from easier examples,

07:43.680 --> 07:45.200
then followed by harder ones.

07:45.200 --> 07:48.240
After that, they gave it outputs from GPT4.

07:48.240 --> 07:49.040
Notice by the way,

07:49.040 --> 07:52.800
what happens if you skip the chat GPT teaching assistant

07:52.800 --> 07:57.040
and only train on those one million examples from GPT4.

07:57.040 --> 08:00.240
What happens is a bit like a student struggling in a class

08:00.240 --> 08:01.600
that's too advanced for them.

08:01.600 --> 08:06.320
Orca actually performs worse in those circumstances, averaging 37%,

08:06.320 --> 08:10.800
but with that intermediate teacher beforehand, it gets 41.7%.

08:10.800 --> 08:11.840
Speaking of time,

08:11.840 --> 08:17.840
it only took about 200 hours to train Orca on 20 A100 GPUs.

08:17.840 --> 08:22.400
They did take a few weeks to collect the data from chat GPT and GPT4,

08:22.400 --> 08:24.960
but presumably if they're planning to open source this,

08:24.960 --> 08:26.160
which they say they are,

08:26.160 --> 08:29.120
then that step could be skipped by the wider community.

08:29.120 --> 08:31.280
Let's now look at some more of the results.

08:31.280 --> 08:34.640
First, for open-ended generation, not multiple choice.

08:34.640 --> 08:38.560
Orca is 95% of chat GPT quality

08:38.560 --> 08:43.040
and 85% of GPT4's quality as assessed by GPT4,

08:43.040 --> 08:46.480
but they wanted to quickly move on to some more definitive tasks

08:46.480 --> 08:49.920
because there is a problem of using GPT4 as an assessor.

08:49.920 --> 08:50.640
For example,

08:50.640 --> 08:54.720
they observed that there is a positive bias in GPT4 evaluation

08:54.720 --> 08:58.880
toward the response of the first model in the comparison set.

08:58.880 --> 09:02.000
This reminded me of the unfaithful reasoning paper

09:02.000 --> 09:04.640
that I talked about in one of my recent videos.

09:04.640 --> 09:08.000
You can't always trust GPT4 to give its true reasoning,

09:08.000 --> 09:11.360
but here it is in more objective multiple choice questions.

09:11.360 --> 09:13.760
And notice how much harder many of these tests are

09:13.760 --> 09:16.000
for even these advanced language models.

09:16.000 --> 09:19.200
I am fortunate and proud to have attained a perfect score

09:19.200 --> 09:20.880
in some of the tests in this chart,

09:20.880 --> 09:22.400
like the GRE and GMAT.

09:22.400 --> 09:25.760
They were part of the Aquarat tests that they gave the models,

09:25.760 --> 09:28.160
so I can say that they really are quite challenging,

09:28.160 --> 09:30.800
hence why GPT4 only gets a 40%.

09:30.880 --> 09:32.240
You can see that throughout,

09:32.240 --> 09:35.440
Orca outperforms Vecuna by quite a margin

09:35.440 --> 09:38.640
and is very competitive with Textavinci 3.

09:38.640 --> 09:41.360
Of course, overall, it does lag behind GPT4,

09:41.360 --> 09:43.200
but this is all zero shot.

09:43.200 --> 09:43.920
A bit later on,

09:43.920 --> 09:46.800
I'll come back to the range of methods that we could use

09:46.800 --> 09:48.880
to further improve on Orca.

09:48.880 --> 09:50.560
The percentages, by the way,

09:50.560 --> 09:52.480
are the improvements on Vecuna,

09:52.480 --> 09:55.040
again the second best open source model.

09:55.040 --> 09:57.840
So far, we've looked at human centric benchmarks

09:57.840 --> 09:59.680
like the GMAT and GRE.

09:59.680 --> 10:02.800
These are grouped with the lovely name AGI EVAL,

10:02.800 --> 10:03.600
and as we've seen,

10:03.600 --> 10:07.280
even the top models lag behind the top human performers.

10:07.280 --> 10:10.880
But what about a benchmark specifically for language models?

10:10.880 --> 10:12.720
It's called Big Bench Hard.

10:12.720 --> 10:15.680
The original Big Bench had 207 tasks,

10:15.680 --> 10:17.520
but language models got so good

10:17.520 --> 10:19.760
that they had to narrow down the benchmark

10:19.760 --> 10:22.480
to just the 23 challenging tasks

10:22.480 --> 10:25.440
where human raters still did better than language models.

10:25.440 --> 10:27.600
Now, it turns out when you add Chain of Thought prompting

10:27.600 --> 10:29.280
to the models, they do even better

10:29.280 --> 10:31.920
and there are even fewer tasks that humans are better at.

10:31.920 --> 10:33.440
But anyway, all you have to remember

10:33.440 --> 10:37.760
is that these are 23 of the hardest tasks for language models.

10:37.760 --> 10:40.320
And I'll just let you compare the results for yourself.

10:40.320 --> 10:42.880
But the trend is really quite clear.

10:42.880 --> 10:45.440
Orca massively outperforming

10:45.440 --> 10:48.240
the previous best open source model, Vecuna,

10:48.240 --> 10:50.720
beating even chat GPT on average,

10:50.720 --> 10:53.360
but still, of course, lagging behind GPT4,

10:53.360 --> 10:55.360
except for a few tasks.

10:55.360 --> 10:56.720
Look at Web of Lies,

10:56.720 --> 10:58.960
where Orca outperforms GPT4.

10:58.960 --> 11:00.800
That would be a question like this.

11:00.800 --> 11:03.200
Alexis says Shonda tells the truth.

11:03.200 --> 11:04.080
Jim Lies?

11:04.080 --> 11:06.640
Antoine says Jim tells the truth.

11:06.640 --> 11:08.800
Shonda says Antoine Lies.

11:08.800 --> 11:10.480
Does Alexis tell the truth?

11:10.480 --> 11:12.800
Or what about temporal sequences,

11:12.800 --> 11:15.680
where Orca absolutely crushes Vecuna

11:15.680 --> 11:18.320
and doubles chat GPT's performance?

11:18.320 --> 11:20.160
That would be a situation like this.

11:20.160 --> 11:21.520
Now, I'm not going to read it all out,

11:21.520 --> 11:23.280
but essentially you have to figure out

11:23.280 --> 11:24.720
when the timings match up.

11:24.720 --> 11:26.480
Basically keeping track of time

11:26.480 --> 11:28.320
and Orca does really well

11:28.400 --> 11:30.720
and chat GPT flops getting it wrong.

11:30.720 --> 11:33.520
Interestingly, they also tested all four models

11:33.520 --> 11:35.600
on that common sense reasoning question

11:35.600 --> 11:37.920
that I demonstrated for smart GPT,

11:37.920 --> 11:39.760
about hanging the clothes to dry.

11:39.760 --> 11:40.640
As you might remember,

11:40.640 --> 11:42.160
you can use prompt engineering

11:42.160 --> 11:44.720
to nudge the models to almost always get it right,

11:44.720 --> 11:47.040
which is partly why I view these results

11:47.040 --> 11:49.440
more as a baseline rather than a cap.

11:49.440 --> 11:51.040
And the authors admit this too.

11:51.040 --> 11:53.040
Orca has been trained on data

11:53.040 --> 11:55.280
that simulates zero shot setting

11:55.280 --> 11:56.480
with standard prompts.

11:56.480 --> 11:58.640
The model's performance in other contexts,

11:58.640 --> 12:00.720
such as multi-turn conversations,

12:00.720 --> 12:02.880
like the DERA paper I talked about on the channel,

12:02.880 --> 12:05.520
in context learning and few shot learning,

12:05.520 --> 12:07.600
or advanced prompting techniques,

12:07.600 --> 12:10.400
that smart GPT or Tree of Thoughts, for example,

12:10.400 --> 12:12.880
and they say like chain of thought prompting,

12:12.880 --> 12:14.320
remains untested.

12:14.320 --> 12:16.880
These results are a baseline, not a cap.

12:16.880 --> 12:19.680
They mention other ways that Orca could be improved,

12:19.680 --> 12:22.400
for example, through tool augmentation.

12:22.400 --> 12:24.000
And that's not just calculators,

12:24.080 --> 12:26.880
calendars, Bing, or auto GPT.

12:26.880 --> 12:29.040
I was going to do a separate video on this paper,

12:29.040 --> 12:30.400
but I'll just mention it here.

12:30.400 --> 12:32.400
This paper from last week demonstrated

12:32.400 --> 12:34.960
that larger models can create tools

12:34.960 --> 12:37.680
that smaller models can then use more efficiently.

12:37.680 --> 12:40.400
Once the best language model, say GPT-4,

12:40.400 --> 12:42.880
has created a generic Python function,

12:42.880 --> 12:43.840
which is the tool,

12:43.840 --> 12:45.760
and then written some unit tests,

12:45.760 --> 12:48.240
it can then wrap and hand over those tools

12:48.240 --> 12:50.800
to smaller models like GPT-3.5,

12:50.800 --> 12:52.080
or in this case, Orca,

12:52.080 --> 12:54.160
and check out the toolmaking row

12:54.160 --> 12:56.720
to see the improvement for chat GPT,

12:56.720 --> 12:58.160
or in our case, Orca,

12:58.160 --> 13:01.280
when they're given these tools created by GPT-4

13:01.280 --> 13:02.720
or better language models.

13:02.720 --> 13:05.200
Their performance across a range of tasks

13:05.200 --> 13:06.640
goes dramatically up,

13:06.640 --> 13:08.000
and we haven't even talked about

13:08.000 --> 13:10.400
using a process-based reward model,

13:10.400 --> 13:13.200
like in the Let's Verify step-by-step paper.

13:13.200 --> 13:16.480
That, of course, could further improve Orca's performance.

13:16.480 --> 13:18.960
Of course, when this model becomes publicly available,

13:18.960 --> 13:20.720
I will test all of this out,

13:20.720 --> 13:22.800
but it hasn't been open-sourced yet,

13:22.800 --> 13:24.560
and they do say this model

13:24.560 --> 13:27.360
is solely designed for research settings.

13:27.360 --> 13:29.680
That does seem a little bit naive to me.

13:29.680 --> 13:31.040
I mean, that's what Metta said

13:31.040 --> 13:32.160
when they released Lama,

13:32.160 --> 13:34.080
but then everyone and their grandma

13:34.080 --> 13:36.080
just use the language model for whatever.

13:36.080 --> 13:37.680
I do wonder what it means when they say

13:37.680 --> 13:39.760
we are working with our legal team.

13:39.760 --> 13:41.920
And it is particularly interesting to me

13:41.920 --> 13:44.400
that this was all done by Microsoft.

13:44.400 --> 13:46.480
I'm gonna go into a little bit of speculation here

13:46.480 --> 13:49.200
about why I think they conducted this research.

13:49.200 --> 13:51.600
You might remember that leaked memo from Google.

13:51.600 --> 13:54.480
We have no motes, and they even mentioned Vakuna,

13:54.480 --> 13:57.280
and talked about how it circumvented restrictions

13:57.280 --> 14:00.800
on the OpenAI API by using shared GPT.

14:00.800 --> 14:03.360
And my theory is that the Microsoft researchers

14:03.360 --> 14:05.520
were testing this point from the memo.

14:05.520 --> 14:08.320
The point was that training giant models from scratch

14:08.320 --> 14:10.320
not only throws away the pre-training,

14:10.320 --> 14:12.960
but also any iterative open-source improvements

14:12.960 --> 14:14.160
that have been made on top.

14:14.160 --> 14:16.560
It doesn't take long for those improvements to dominate,

14:16.640 --> 14:19.360
making the full retrain extremely costly.

14:19.360 --> 14:22.480
Maybe Microsoft is hesitating about future investments

14:22.480 --> 14:24.880
in GPT-5 or GPT-6.

14:24.880 --> 14:26.480
And they really wanna test out

14:26.480 --> 14:29.840
if it's easy to imitate those large models on the cheap.

14:29.840 --> 14:32.640
If it is, then why would Microsoft invest billions

14:32.640 --> 14:34.320
in a new giant model?

14:34.320 --> 14:37.280
That's my own theory as to why Microsoft is working on this,

14:37.280 --> 14:39.520
but let me know in the comments what your theory is.

14:39.520 --> 14:41.520
In the conclusion, the authors state that

14:41.520 --> 14:45.120
Orca suggests that learning from step-by-step explanations

14:45.120 --> 14:47.760
could significantly improve the quality of models

14:47.760 --> 14:49.360
regardless of their size,

14:49.360 --> 14:52.160
and that they hope these insights will inform the design

14:52.160 --> 14:54.720
of more robust evaluation methods,

14:54.720 --> 14:56.960
compared to those used for a vacuna, for example,

14:56.960 --> 15:00.400
and the advancement of alignment and post-training techniques,

15:00.400 --> 15:03.760
and the more effective use of powerful models

15:03.760 --> 15:05.840
like GPT-4 as teachers.

15:05.840 --> 15:06.880
And maybe they should have said,

15:06.880 --> 15:10.240
and also with chat GPT as an intermediate teacher.

15:10.240 --> 15:13.200
I'm gonna end with the thoughts of the leaders of OpenAI,

15:13.280 --> 15:16.400
Ilya Sutskova, and Sam Oltman on open source models.

15:16.400 --> 15:18.240
And I think there is a bit of a contrast

15:18.240 --> 15:19.520
between the two answers.

15:19.520 --> 15:22.640
Ilya Sutskova thinks that the gap is growing ever wider.

15:23.200 --> 15:26.720
To the open source versus non-open source models question,

15:27.680 --> 15:31.120
you don't wanna think about it in binary black and white terms

15:31.120 --> 15:34.560
where like, there is a secret source

15:34.560 --> 15:36.960
that will never be rediscovered.

15:37.840 --> 15:41.520
What I will say, or whether GPT-4 will ever be reproduced

15:41.600 --> 15:45.040
by open source models, perhaps one day it will be.

15:45.760 --> 15:47.040
But when it will be,

15:47.040 --> 15:49.600
there will be a much more powerful model in the companies.

15:50.640 --> 15:52.240
So there will always be a gap

15:52.240 --> 15:56.480
between the open source models and the private models.

15:57.440 --> 16:01.040
And this gap may even be increasing this time.

16:01.920 --> 16:05.920
The amount of effort and engineering and research

16:05.920 --> 16:09.840
that it takes to produce one such neural net keeps increasing.

16:09.920 --> 16:13.040
And so even if there are open source models,

16:13.040 --> 16:14.080
they will never be,

16:14.080 --> 16:17.280
they will be less and less produced by small groups

16:17.280 --> 16:21.040
of dedicated researchers and engineers.

16:21.040 --> 16:25.200
And it will only be the providence of a company, a big company.

16:25.760 --> 16:27.200
While Sam Oltman seems to say

16:27.200 --> 16:29.840
that even if open source models do catch up,

16:29.840 --> 16:32.720
OpenAI will always have a different kind of moat.

16:32.720 --> 16:34.320
What are your thoughts about the,

16:34.320 --> 16:38.320
we have no moat document that was released lately?

16:40.480 --> 16:41.360
The leak document.

16:44.240 --> 16:46.720
The thing that is special about OpenAI,

16:46.720 --> 16:49.920
and I think the thing that is so misunderstood by that document,

16:49.920 --> 16:53.200
aside from the fact that we have a gigantic number of users

16:53.200 --> 16:55.840
and people that have formed some sort of relationship

16:55.840 --> 16:56.800
with us and our products,

16:57.520 --> 17:00.400
is what OpenAI is special about

17:00.960 --> 17:02.880
is figuring out what comes next.

17:03.520 --> 17:04.560
It is the ability,

17:04.560 --> 17:07.280
it is easy to copy something once you know it can be done.

17:07.280 --> 17:08.720
And in that sense, sure.

17:09.680 --> 17:12.080
It is very hard to go figure out what to do next.

17:12.720 --> 17:14.720
And the ideas, the big ideas,

17:14.720 --> 17:16.960
the medium size ideas, the small ideas,

17:16.960 --> 17:18.960
and the careful execution on them

17:18.960 --> 17:21.520
that it takes to get from here to superintelligence,

17:21.520 --> 17:22.720
that's what our moat is.

17:22.720 --> 17:25.520
Anyway, this video could have been at least three times longer.

17:25.520 --> 17:28.400
There was so much I had to edit out for brevity.

17:28.400 --> 17:31.360
If you're interested in me talking more about open source models,

17:31.360 --> 17:32.960
do let me know in the comments.

17:32.960 --> 17:34.560
I've got much more to say.

17:34.560 --> 17:37.040
As always, thank you so much for watching to the end

17:37.040 --> 17:39.040
and have a wonderful day.

