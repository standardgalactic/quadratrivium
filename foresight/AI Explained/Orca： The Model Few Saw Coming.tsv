start	end	text
0	3840	Do you remember this paper, less than two weeks old?
3840	7200	It made waves by concluding that open source models
7200	11920	can mimic the style, but not the factuality of chat GPT.
11920	14240	Overall, we can conclude they say
14240	17520	that model imitation is a false promise.
17520	23840	Well, 48 hours ago, we have this, a 51 page report on Orca,
23840	27200	based on a small 13 billion parameter model.
27200	29520	I don't often comment on open source models
29520	32800	because they're simply not competitive with open AI's models.
32800	36640	But Orca is not just competitive with GPT 3.5.
36640	40320	It beats it in quite a few well-established benchmarks
40320	44320	and even matches GPT-4 in a couple of tests of reasoning.
44320	46400	As always, I've read both papers in full
46400	48800	and can also bring in just-released comments
48800	53920	from Sam Altman and Ilya Sutskova on competition from open source models.
53920	59040	But let's start with Orca, named presumably because Orca's or killer whales
59040	62000	are frequent visitors to South American coastlines.
62000	65680	And South America is, of course, the land of llamas and vicunas.
65680	68720	But all the research was done by Microsoft,
68720	72000	which I find interesting and I'll come back to that at the end.
72000	74880	But why did they make Orca and why does it perform better
74880	77440	than models like llama, alpaca, and vicuna?
77440	79200	Well, they say here in the abstract
79200	82400	that those other models lack rigorous evaluation,
82400	85920	resulting in overestimating the small model's capability
85920	88640	as they tend to learn to imitate the style
88720	92560	but not the reasoning of LFMs, large foundation models.
92560	94880	To address these challenges, we developed Orca,
94880	98000	a 13 billion parameter model, that learns to imitate
98000	100880	the reasoning process of the larger models.
100880	105360	Orca learned by looking at GPT-4's step-by-step thought processes
105360	110640	and is guided by teacher assistants from chat GPT, which is GPT 3.5.
110640	112560	And to give you a taste of what's to come,
112560	115760	Orca surpasses conventional state-of-the-art models,
115760	118960	such as vicuna, by more than 100%
118960	122400	in complex zero-shot reasoning benchmarks,
122400	124960	like the big bench hard, which I'll talk about,
124960	128400	and by 42% on AGI eval.
128400	132480	It goes on, Orca reaches parity with chat GPT
132480	136400	on the big bench hard and shows competitive performance
136400	138800	in professional and academic examinations
138800	141520	by the SAT, LSAT, GRE, and GMAT.
141520	144320	And I know many of you will be interested in this footnote.
144320	148160	We are working with our legal team to publicly release
148160	150480	a diff of the model weights in accordance
150480	152240	with Lama's release policy.
152240	154000	So if this is anything like Lama,
154000	156320	it's going to be leaked across the internet imminently.
156320	159760	I'm going to show you so many tests and benchmarks in a moment,
159760	161440	but just to give you a sample,
161440	166560	here is Orca outperforming chat GPT in the vicuna evaluation set
166560	171600	and matching text DaVinci 3 in the SAT, LSAT, GRE, and GMAT.
171600	172960	And as I'll touch on later,
172960	175840	this was zero shot without chain of thought
175840	177440	or any advanced methods.
177440	179520	You can watch pretty much any of my other videos
179520	181760	to see how advanced prompt engineering
181760	184400	would probably boost those results still further.
184400	185520	For those who didn't know,
185520	190480	13 billion parameters is about 7% the size of GPT-3,
190480	192880	which is 175 billion parameters,
192880	197680	and possibly around one or 2% of GPT-4's size.
197680	200000	That gives you an indication of the difference
200080	203360	in size between Orca and these models that it's competing with.
203360	204880	And if that doesn't make any sense,
204880	208800	a smaller size means it can be run on much smaller devices,
208800	211680	like a desktop or even possibly a laptop.
211680	214720	The authors start off by giving a little slap to the other paper.
214720	217920	You know that one that said model imitation is a false promise.
217920	220480	And they continue that contrary to this assertion,
220480	225040	it is possible to reduce the gap with proprietary LLMs
225040	229360	on multiple zero shot benchmarks that require sophisticated reasoning.
229360	234160	As we'll see, models like Vakuna claim to have 90% of chat GPT's quality,
234160	237600	but when it came to reasoning tasks or more technical tasks,
237600	238720	it basically flopped.
238720	240800	Here's a chart I'll come back to outlining
240800	244240	some of the more technical challenges you can give a language model.
244240	247440	We should remember that Vakuna is a fine-tuned version
247440	248880	of the Llama model,
248880	252320	and it's competitive or even better than Palm II.
252320	255840	But give it some of the harder challenges for a language model,
255840	258720	and it really struggles, as you can see in this column.
258720	262240	Take logical deduction, where it only scored 1.2%.
262240	266320	Well, this Orca model was 2,900% better than that,
266320	269520	scoring 36% competitive with chat GPT.
269520	271840	I'm going to come back to the big benchmark,
271840	274320	but look for a second at causal judgment,
274320	279440	where Orca, a 13 billion parameter model matches GPT4,
279440	282160	which is about 100 times the size.
282160	284080	But back to how they actually did it.
284080	288640	Models like Alpaca and Vakuna were given lots of query and responses
288640	290880	from chat GPT or GPT4.
290880	294080	But what they did is they leveraged system instructions,
294080	298000	asking models like GPT4 and chat GPT to think step by step.
298000	302080	This gave Orca access to detailed responses from the model
302080	305200	that explained the reasoning process of the teacher
305200	306800	as it generates the response.
306800	310720	It allowed these parent models of GPT3.5 and GPT4
310720	314160	to be much better tutors for this young Orca.
314240	317680	Also, they let the teachers of chat GPT, which is 3.5,
317680	321040	and GPT4 give far more examples to their student.
321040	324080	5 million and 1 million examples, respectively.
324080	326240	That compares to the other models you may have heard of,
326240	328800	like Alpaca, Wizard, Vakuna, etc.,
328800	333040	which had tens of thousands or the low hundreds of thousands of examples.
333040	336080	But again, the key difference is the explanations,
336080	340000	the step by step thinking that the smaller Orca could then imitate.
340000	343040	They give a quick demo here of how the other open source models
343040	344960	learn from their GPT parents,
344960	348320	with a simplistic question and answer format.
348320	351520	In contrast, the authors leveraged system messages
351520	355600	to get chat GPT and GPT4 to think step by step,
355600	360080	leading to much richer explanations, as you can see in this diagram.
360080	362000	It wasn't just let's think step by step,
362000	365120	by the way, also things like explain like I'm 5.
365120	369600	They also wanted the task to be as complex and diverse as possible,
369600	371920	so they used the Flan collection.
371920	374000	This was released by Google in February,
374000	377600	and focused on balancing the kind of prompts and tasks
377600	379920	that you fine tune the language models on.
379920	382480	You can see here the 16 system messages
382480	385360	that they give to chat GPT and GPT4,
385360	388080	and you can see here the kind of difference that that makes.
388080	391360	Imagine a language model trying to learn from this human.
391360	392480	The human is asked,
392480	394640	pick which sentence is not logical.
394640	397760	Sentence A, people in the desert often look forward to flood,
397760	401040	or sentence B, people in the desert often look forward to rain.
401040	402240	The human responds,
402240	404480	there is no reason to look forward to a flood,
404480	407520	because floods cause damage, the answer is sentence A.
407520	410080	Now yes, a language model can learn from that,
410080	413200	but by leveraging those system assistant messages,
413200	415680	look at the kind of response that GPT4 gives.
415680	419120	Now Orca can learn a lot more from that explanation,
419120	421680	and that's one of the main reasons it's better
421680	423920	than all the other open source models.
423920	427760	Because remember, Vikuna is the best of the open source models.
427760	430880	In this leaderboard, it has an elo of 1054,
430880	433120	better even than Palm II Bison.
433120	435520	All the models higher than it are proprietary.
435520	438960	But there is another reason why Orca performs so much better.
438960	439840	You might have wondered,
439840	442400	why didn't they just use only GPT4?
442400	445200	Well yes, there were cost and time considerations,
445200	447120	but there was another factor that they found.
447120	452400	They were able to use chat GPT or GPT3.5 as an intermediate teacher.
452400	456800	That teacher, chat GPT, was able to reduce the gap in capabilities.
456800	459600	So Orca got smarter and better able to learn.
459600	461200	A bit like progressive learning,
461200	463680	where you first learn from easier examples,
463680	465200	then followed by harder ones.
465200	468240	After that, they gave it outputs from GPT4.
468240	469040	Notice by the way,
469040	472800	what happens if you skip the chat GPT teaching assistant
472800	477040	and only train on those one million examples from GPT4.
477040	480240	What happens is a bit like a student struggling in a class
480240	481600	that's too advanced for them.
481600	486320	Orca actually performs worse in those circumstances, averaging 37%,
486320	490800	but with that intermediate teacher beforehand, it gets 41.7%.
490800	491840	Speaking of time,
491840	497840	it only took about 200 hours to train Orca on 20 A100 GPUs.
497840	502400	They did take a few weeks to collect the data from chat GPT and GPT4,
502400	504960	but presumably if they're planning to open source this,
504960	506160	which they say they are,
506160	509120	then that step could be skipped by the wider community.
509120	511280	Let's now look at some more of the results.
511280	514640	First, for open-ended generation, not multiple choice.
514640	518560	Orca is 95% of chat GPT quality
518560	523040	and 85% of GPT4's quality as assessed by GPT4,
523040	526480	but they wanted to quickly move on to some more definitive tasks
526480	529920	because there is a problem of using GPT4 as an assessor.
529920	530640	For example,
530640	534720	they observed that there is a positive bias in GPT4 evaluation
534720	538880	toward the response of the first model in the comparison set.
538880	542000	This reminded me of the unfaithful reasoning paper
542000	544640	that I talked about in one of my recent videos.
544640	548000	You can't always trust GPT4 to give its true reasoning,
548000	551360	but here it is in more objective multiple choice questions.
551360	553760	And notice how much harder many of these tests are
553760	556000	for even these advanced language models.
556000	559200	I am fortunate and proud to have attained a perfect score
559200	560880	in some of the tests in this chart,
560880	562400	like the GRE and GMAT.
562400	565760	They were part of the Aquarat tests that they gave the models,
565760	568160	so I can say that they really are quite challenging,
568160	570800	hence why GPT4 only gets a 40%.
570880	572240	You can see that throughout,
572240	575440	Orca outperforms Vecuna by quite a margin
575440	578640	and is very competitive with Textavinci 3.
578640	581360	Of course, overall, it does lag behind GPT4,
581360	583200	but this is all zero shot.
583200	583920	A bit later on,
583920	586800	I'll come back to the range of methods that we could use
586800	588880	to further improve on Orca.
588880	590560	The percentages, by the way,
590560	592480	are the improvements on Vecuna,
592480	595040	again the second best open source model.
595040	597840	So far, we've looked at human centric benchmarks
597840	599680	like the GMAT and GRE.
599680	602800	These are grouped with the lovely name AGI EVAL,
602800	603600	and as we've seen,
603600	607280	even the top models lag behind the top human performers.
607280	610880	But what about a benchmark specifically for language models?
610880	612720	It's called Big Bench Hard.
612720	615680	The original Big Bench had 207 tasks,
615680	617520	but language models got so good
617520	619760	that they had to narrow down the benchmark
619760	622480	to just the 23 challenging tasks
622480	625440	where human raters still did better than language models.
625440	627600	Now, it turns out when you add Chain of Thought prompting
627600	629280	to the models, they do even better
629280	631920	and there are even fewer tasks that humans are better at.
631920	633440	But anyway, all you have to remember
633440	637760	is that these are 23 of the hardest tasks for language models.
637760	640320	And I'll just let you compare the results for yourself.
640320	642880	But the trend is really quite clear.
642880	645440	Orca massively outperforming
645440	648240	the previous best open source model, Vecuna,
648240	650720	beating even chat GPT on average,
650720	653360	but still, of course, lagging behind GPT4,
653360	655360	except for a few tasks.
655360	656720	Look at Web of Lies,
656720	658960	where Orca outperforms GPT4.
658960	660800	That would be a question like this.
660800	663200	Alexis says Shonda tells the truth.
663200	664080	Jim Lies?
664080	666640	Antoine says Jim tells the truth.
666640	668800	Shonda says Antoine Lies.
668800	670480	Does Alexis tell the truth?
670480	672800	Or what about temporal sequences,
672800	675680	where Orca absolutely crushes Vecuna
675680	678320	and doubles chat GPT's performance?
678320	680160	That would be a situation like this.
680160	681520	Now, I'm not going to read it all out,
681520	683280	but essentially you have to figure out
683280	684720	when the timings match up.
684720	686480	Basically keeping track of time
686480	688320	and Orca does really well
688400	690720	and chat GPT flops getting it wrong.
690720	693520	Interestingly, they also tested all four models
693520	695600	on that common sense reasoning question
695600	697920	that I demonstrated for smart GPT,
697920	699760	about hanging the clothes to dry.
699760	700640	As you might remember,
700640	702160	you can use prompt engineering
702160	704720	to nudge the models to almost always get it right,
704720	707040	which is partly why I view these results
707040	709440	more as a baseline rather than a cap.
709440	711040	And the authors admit this too.
711040	713040	Orca has been trained on data
713040	715280	that simulates zero shot setting
715280	716480	with standard prompts.
716480	718640	The model's performance in other contexts,
718640	720720	such as multi-turn conversations,
720720	722880	like the DERA paper I talked about on the channel,
722880	725520	in context learning and few shot learning,
725520	727600	or advanced prompting techniques,
727600	730400	that smart GPT or Tree of Thoughts, for example,
730400	732880	and they say like chain of thought prompting,
732880	734320	remains untested.
734320	736880	These results are a baseline, not a cap.
736880	739680	They mention other ways that Orca could be improved,
739680	742400	for example, through tool augmentation.
742400	744000	And that's not just calculators,
744080	746880	calendars, Bing, or auto GPT.
746880	749040	I was going to do a separate video on this paper,
749040	750400	but I'll just mention it here.
750400	752400	This paper from last week demonstrated
752400	754960	that larger models can create tools
754960	757680	that smaller models can then use more efficiently.
757680	760400	Once the best language model, say GPT-4,
760400	762880	has created a generic Python function,
762880	763840	which is the tool,
763840	765760	and then written some unit tests,
765760	768240	it can then wrap and hand over those tools
768240	770800	to smaller models like GPT-3.5,
770800	772080	or in this case, Orca,
772080	774160	and check out the toolmaking row
774160	776720	to see the improvement for chat GPT,
776720	778160	or in our case, Orca,
778160	781280	when they're given these tools created by GPT-4
781280	782720	or better language models.
782720	785200	Their performance across a range of tasks
785200	786640	goes dramatically up,
786640	788000	and we haven't even talked about
788000	790400	using a process-based reward model,
790400	793200	like in the Let's Verify step-by-step paper.
793200	796480	That, of course, could further improve Orca's performance.
796480	798960	Of course, when this model becomes publicly available,
798960	800720	I will test all of this out,
800720	802800	but it hasn't been open-sourced yet,
802800	804560	and they do say this model
804560	807360	is solely designed for research settings.
807360	809680	That does seem a little bit naive to me.
809680	811040	I mean, that's what Metta said
811040	812160	when they released Lama,
812160	814080	but then everyone and their grandma
814080	816080	just use the language model for whatever.
816080	817680	I do wonder what it means when they say
817680	819760	we are working with our legal team.
819760	821920	And it is particularly interesting to me
821920	824400	that this was all done by Microsoft.
824400	826480	I'm gonna go into a little bit of speculation here
826480	829200	about why I think they conducted this research.
829200	831600	You might remember that leaked memo from Google.
831600	834480	We have no motes, and they even mentioned Vakuna,
834480	837280	and talked about how it circumvented restrictions
837280	840800	on the OpenAI API by using shared GPT.
840800	843360	And my theory is that the Microsoft researchers
843360	845520	were testing this point from the memo.
845520	848320	The point was that training giant models from scratch
848320	850320	not only throws away the pre-training,
850320	852960	but also any iterative open-source improvements
852960	854160	that have been made on top.
854160	856560	It doesn't take long for those improvements to dominate,
856640	859360	making the full retrain extremely costly.
859360	862480	Maybe Microsoft is hesitating about future investments
862480	864880	in GPT-5 or GPT-6.
864880	866480	And they really wanna test out
866480	869840	if it's easy to imitate those large models on the cheap.
869840	872640	If it is, then why would Microsoft invest billions
872640	874320	in a new giant model?
874320	877280	That's my own theory as to why Microsoft is working on this,
877280	879520	but let me know in the comments what your theory is.
879520	881520	In the conclusion, the authors state that
881520	885120	Orca suggests that learning from step-by-step explanations
885120	887760	could significantly improve the quality of models
887760	889360	regardless of their size,
889360	892160	and that they hope these insights will inform the design
892160	894720	of more robust evaluation methods,
894720	896960	compared to those used for a vacuna, for example,
896960	900400	and the advancement of alignment and post-training techniques,
900400	903760	and the more effective use of powerful models
903760	905840	like GPT-4 as teachers.
905840	906880	And maybe they should have said,
906880	910240	and also with chat GPT as an intermediate teacher.
910240	913200	I'm gonna end with the thoughts of the leaders of OpenAI,
913280	916400	Ilya Sutskova, and Sam Oltman on open source models.
916400	918240	And I think there is a bit of a contrast
918240	919520	between the two answers.
919520	922640	Ilya Sutskova thinks that the gap is growing ever wider.
923200	926720	To the open source versus non-open source models question,
927680	931120	you don't wanna think about it in binary black and white terms
931120	934560	where like, there is a secret source
934560	936960	that will never be rediscovered.
937840	941520	What I will say, or whether GPT-4 will ever be reproduced
941600	945040	by open source models, perhaps one day it will be.
945760	947040	But when it will be,
947040	949600	there will be a much more powerful model in the companies.
950640	952240	So there will always be a gap
952240	956480	between the open source models and the private models.
957440	961040	And this gap may even be increasing this time.
961920	965920	The amount of effort and engineering and research
965920	969840	that it takes to produce one such neural net keeps increasing.
969920	973040	And so even if there are open source models,
973040	974080	they will never be,
974080	977280	they will be less and less produced by small groups
977280	981040	of dedicated researchers and engineers.
981040	985200	And it will only be the providence of a company, a big company.
985760	987200	While Sam Oltman seems to say
987200	989840	that even if open source models do catch up,
989840	992720	OpenAI will always have a different kind of moat.
992720	994320	What are your thoughts about the,
994320	998320	we have no moat document that was released lately?
1000480	1001360	The leak document.
1004240	1006720	The thing that is special about OpenAI,
1006720	1009920	and I think the thing that is so misunderstood by that document,
1009920	1013200	aside from the fact that we have a gigantic number of users
1013200	1015840	and people that have formed some sort of relationship
1015840	1016800	with us and our products,
1017520	1020400	is what OpenAI is special about
1020960	1022880	is figuring out what comes next.
1023520	1024560	It is the ability,
1024560	1027280	it is easy to copy something once you know it can be done.
1027280	1028720	And in that sense, sure.
1029680	1032080	It is very hard to go figure out what to do next.
1032720	1034720	And the ideas, the big ideas,
1034720	1036960	the medium size ideas, the small ideas,
1036960	1038960	and the careful execution on them
1038960	1041520	that it takes to get from here to superintelligence,
1041520	1042720	that's what our moat is.
1042720	1045520	Anyway, this video could have been at least three times longer.
1045520	1048400	There was so much I had to edit out for brevity.
1048400	1051360	If you're interested in me talking more about open source models,
1051360	1052960	do let me know in the comments.
1052960	1054560	I've got much more to say.
1054560	1057040	As always, thank you so much for watching to the end
1057040	1059040	and have a wonderful day.
