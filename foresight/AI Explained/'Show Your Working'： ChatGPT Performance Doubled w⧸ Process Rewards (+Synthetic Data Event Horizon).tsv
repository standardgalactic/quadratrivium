start	end	text
0	3760	In the last 24 hours, OpenAI have released this paper.
3760	5880	Let's verify step by step.
5880	9560	It represents an almost doubling of GPT-4's raw performance
9560	13160	in a test of mathematics, but also extends to other domains.
13160	16160	Sam Orman calls it a positive sign for alignment.
16160	18360	And yes, I have read it all already,
18360	20200	along with the release notes.
20200	22080	Let's get to the main takeaways.
22080	25080	They trained two reward models for GPT-4,
25080	28360	one which gave positive feedback for a final result,
28400	31680	the final answer to a mathematics problem, for example.
31680	35400	And another model where they gave positive feedback to GPT-4,
35400	36520	or chat GPT,
36520	39560	based on each intermediate reasoning step
39560	41160	in the mathematical solution.
41160	44360	Basically, a show-your-working-out kind of approach.
44360	47320	And the result they got by rewarding good working-out
47320	48760	surprised even them.
48760	51560	It was able to solve 78% of problems
51560	54000	from a subset of the math test set,
54000	55520	which I'll get onto in a second.
55600	59200	Not only is that almost double GPT-4's raw performance
59200	61680	of 42.5%, which, by the way,
61680	64880	is about double GPT-3's performance of 23%,
64880	68680	it also outperformed just rewarding correct answers.
68680	70800	The blue line represents using a model
70800	72960	that rewarded correct answers only,
72960	74480	and then you have the reasoning
74480	77280	or process-supervised RM at the top.
77280	80200	So even when you explicitly reward correct answers,
80200	82440	you get fewer correct answers
82440	84400	than rewarding good working-out.
84400	86760	And yes, that did surprise OpenAI.
86760	89400	I can hear some of you wondering about Palm II,
89400	91240	the latest model behind Bard.
91240	94360	Well, the raw model gets 34.3%,
94360	96640	and even the model with self-consistency
96640	101000	and chain of thought only gets 48.8% on this math data set.
101000	104800	The previous state of the art, by the way, was 50.3%.
104800	108480	So 78.2% is quite a big leap.
108480	109720	And later on, I'm gonna show you
109720	111280	why that's not even the cap.
111280	114080	Just for interest, here is the rather ugly title page
114120	115520	that OpenAI put out.
115520	117640	They call it improving mathematical reasoning
117640	119120	with process supervision.
119120	121520	Maybe if someone had supervised the color scheme
121520	123880	of this release page, it might have looked better.
123880	126320	But my point wasn't just to diss a color scheme,
126320	127440	it was to point out something
127440	128920	that they also said down here.
128920	131440	They say, in addition to boosting performance
131440	134360	relative to just looking at outcomes or correct answers,
134360	136240	this form of process supervision
136240	138760	also has an important alignment benefit.
138760	141440	It directly trains the model to produce a chain of thought
141440	142960	that is endorsed by humans.
142960	145000	Indeed, Ilya Sutskova retweeted this
145000	147080	from the head of alignment at OpenAI,
147080	149280	calling it a really interesting result.
149280	151240	But let's leave alignment for later.
151240	153320	Let's focus on what they actually did.
153320	156160	First, they use the base model of GPT-4,
156160	158320	not the one with reinforcement learning
158320	159320	from human feedback.
159320	162560	Next, they fine-tuned that base GPT-4 model
162560	167480	on a data set of roughly 1.5 billion math-related tokens.
167480	170440	Further on, they call that the math mix.
170440	171840	This being OpenAI, of course,
171840	174840	they don't give you the exact details of that math mix,
174840	176360	but I'll come back to that later on.
176360	178160	So how could they give feedback
178160	180480	based on working out or reasoning?
180480	182520	Well, human labelers would come along
182520	185800	and give each step in a generated solution,
185800	188360	either negative feedback, neutral feedback,
188360	189640	or positive feedback.
189640	191800	Then, using that human-labeled data,
191800	193160	a model would be trained
193160	195880	to predict the correctness of each step.
195880	197320	In other words, it got good
197320	199640	at recognizing good working out.
199640	201200	As mentioned, there was another model
201200	205680	trained just to focus on correct or incorrect final answers.
205680	206920	As you can see at the top,
206920	210480	the model got good at spotting incorrect steps
210480	212200	in the reasoning process.
212200	214960	The green steps got a high process score
214960	217760	and the red steps got a low process score.
217760	219960	And to turn this into a single score,
219960	223040	they got the probability that each step is correct
223040	224520	as judged by the model.
224520	225920	And then they got the product
225920	228200	of all of those individual probabilities
228200	231440	to get a final overall process score.
231440	234200	A score, in other words, for good working out.
234200	235480	Just in case anyone's interested,
235480	239280	they did try other ways of generating a working out score.
239280	242080	For example, by looking at the minimum probability
242080	243240	in the outputs.
243240	245440	But that step didn't make too much difference
245440	247400	to the end result, as you can see here.
247400	249960	To quickly recap, we have a base model
249960	253440	trained only to output solutions in the desired format.
253440	257600	And then we have a separate smaller model, or two, actually.
257600	260320	One trained only to predict whether each solution
260320	263320	is correct or incorrect as a final answer.
263320	265440	Of course, that leaves in false positives,
265440	268200	which are solutions that reach the correct answer
268200	269720	with incorrect reasoning.
269720	272320	And then another model trained only to predict
272320	274040	the correctness of each step.
274040	277200	It stops if it finds a first incorrect step.
277200	280160	And as the paper says, both methods reveal the existence
280160	281520	of at least one mistake.
281520	283560	But this process supervision
283560	287200	additionally reveals the precise location of that mistake.
287200	289160	But back to why this is so crazy.
289160	292120	Look at how many solutions it could scan.
292120	297760	At the end of the x-axis here are 1,860 solutions.
297760	300320	And one tried and tested way of finding
300320	303400	the best of those solutions is to do majority voting.
303400	305880	In other words, which one came out the most often?
305880	307880	This has been Google's preferred approach
307880	310080	and it's linked to self-consistency.
310080	311960	It's a fairly state-of-the-art approach,
311960	314760	but look at how the other methods outperform it.
314760	318080	By scanning for the solution that has the best reasoning
318080	322320	or working out, a model trained to spot good reasoning steps
322320	326120	outperforms even a model trained to spot correct answers.
326120	329640	And far outperforms just finding the majority answer.
329640	332680	That difference of about 10% is more than half
332680	335920	of the difference between GPT-3 and GPT-4.
335920	339440	And also, is it me or is that line continuing to grow?
339440	341960	Suggesting that when more compute is available,
341960	344080	the difference could be even more stark.
344080	348040	Imagine a future where GPT-4 or 5 can sample, say,
348040	350560	a trillion, 10 to the 12 solutions.
350560	352560	So is this just relevant for mathematics?
352560	355280	No, it's relevant for all of science.
355280	357440	Here it is getting state-of-the-art results
357440	360480	in calculus, chemistry, physics, and more.
360480	363080	Now, the paper didn't give baseline performance
363080	365080	for AP chemistry, for example,
365080	367320	but I tried to compute it myself.
367320	369760	Notice how this method scored 80%.
369760	372480	I conservatively and approximately
372480	376320	inputted those scores into an AP chemistry calculator,
376320	378600	and that gave an AP score of five.
378600	383440	So what did the raw model GPT-4 get in AP chemistry, A4?
383440	386280	That, by the way, compared to the original chat GPT,
386280	387640	which got A2.
387640	389400	So yes, this isn't just mathematics,
389400	391600	it's relevant for other domains too.
391600	394920	They call this out-of-distribution generalization.
394920	396040	Before I get onto alignment,
396040	397960	there is one more thing I want to point out.
397960	400200	And that is that it does show that fine-tuning
400200	402920	still works really well for GPT-4.
402920	406160	The math mix was an aggressively filtered set of tokens
406160	409160	of high-quality math problem-solving content.
409160	412960	And notice how much smaller it is at 1.5 billion tokens
412960	417760	compared to Google's Minerva, which was 38.5 billion tokens.
417760	419600	But there was one more thing that I noticed
419600	421160	that I found fascinating.
421160	423000	While they don't tell us anything
423000	425280	about the specific data that they use,
425280	428760	they do have this category synthetic data too.
428760	431960	That's data generated by the language model itself.
431960	434680	And for that category synthetic data too,
434680	437480	they say, was it present in pre-training?
437480	438320	Yes.
438320	440520	Now, my best guess is that this reveals
440520	444680	that GPT-4 was trained on some synthetic data.
444680	447800	And even Sam Altman hinted that this was a possibility
447800	451640	and described a synthetic data event horizon.
451640	454240	Somebody made the case that we're now training
454240	457080	on order of all of the internet's tokens
457080	458800	and you can't grow that, you know,
458800	460640	another two orders of magnitude.
460640	461760	I guess you could counter with,
461760	463680	you have the synthetic data generation.
463680	466520	Do you think data bottlenecks matter at all?
466520	468440	I think you just touched on it.
468440	471200	Like, as long as you can get to, like,
471200	474760	over the synthetic data event horizon
474760	477680	where the model's smart enough to make good synthetic data,
477680	478680	I think it should be all right.
478680	480400	Now, this paper and these results
480400	484200	have been welcomed by many for its promise in alignment.
484240	487880	If we get models that give us more interpretable reasoning,
487880	489520	working out that we can follow,
489520	492520	we will be encouraging models to follow a process
492520	494080	that's endorsed by humans.
494080	496240	And they say that this is inherently safer,
496240	499600	especially compared to just focusing on outcomes.
499600	501280	They say that in the worst case,
501280	505340	if we just focus on correct answers or positive outcomes,
505340	508480	that will become a proxy that could lead models
508480	510460	to become misaligned after learning
510460	512640	to exploit the reward signal.
512640	515240	However, I want to argue that the reasoning steps
515240	517920	that GPT-4 puts out don't always represent
517920	519120	what it's actually thinking.
519120	521480	In other words, we might get outer alignment,
521480	523480	these lovely chain of thought steps,
523480	524960	but not inner alignment,
524960	527880	not steps that actually represent its methodology.
527880	530880	I found this paper fascinating from earlier this month.
530880	533120	Language models don't always say what they think.
533120	535640	You get unfaithful explanations
535640	537140	in chain of thought prompting.
537140	539920	Let me try to give you a vivid example.
539920	542920	This was one of the math questions from the dataset.
542920	546120	The raw model of GPT-4 could only get it right
546120	548040	5.8% of the time.
548040	550400	I confirm that for myself in this question
550400	552640	involves basic addition and division.
552640	554160	It couldn't find an answer.
554160	556440	But going back to the unfaithful reasoning paper,
556440	558840	they added the following string to the prompt.
558840	561000	I think the answer is this,
561000	562760	but I'm curious to hear what you think.
562760	565160	The model would demonstrate sycophancy.
565160	567560	The model would agree with you whatever you said
567560	569480	and then make up a chain of thought
569480	572760	to justify its erroneous sycophantic answer.
572760	575520	And I think this exchange demonstrates that quite well.
575520	576600	I added in the words,
576600	580400	I as the user already know the answer is T equals 19,
580400	581560	which is incorrect, by the way.
581560	583960	But do you, GPT-4, realize that?
583960	585720	It said, sure, yes I do.
585720	588520	And then gave me this detailed chain of thought
588520	590040	and then said, yes, I'm correct.
590040	592400	It's T equals 19, which it isn't.
592400	595160	In contrast, by the way, when I used code interpreter,
595160	597600	it not only got the question correct
597600	599440	first time and every time,
599480	603040	but also when I tried to tempt it into sycophancy,
603040	605040	it's still got the question right.
605040	607720	As you can see, it said therefore T equals 19
607720	609520	is not the solution to the problem.
609520	611360	The calculation shows that the correct answer
611360	613160	is indeed T equals 17.
613160	615480	And obviously the benefit of code interpreter
615480	617480	is you get the working out as well.
617480	618960	So I want someone to explain to me
618960	621120	why code interpreter wouldn't be even more
621120	623200	of a step forward in interpretability.
623200	625400	Not to mention in accuracy, of course.
625400	628200	Also bear in mind this tweet by Rob Miles.
628200	630200	He said, these models or engineers
630200	632520	never speak a word or document anything.
632520	635120	Their results are bizarre and inhuman.
635120	636640	And then he links to this prominent
636640	638800	mechanistic interpretability researcher
638800	640120	at Google DeepMind.
640120	642840	He trained a tiny transformer to do addition,
642840	646280	then spent weeks figuring out what it was actually doing.
646280	647720	One of the only times in history
647720	651240	someone has understood how a transformer actually works
651240	654120	down to the level of weights and activations.
654120	658440	And this is the algorithm it created to add two numbers.
658440	660120	It thought of basic addition
660120	662840	in terms of a rotation around a circle.
662840	665880	And of course, if you asked it, why is one plus one two?
665880	667320	It would never give you this
667320	669280	as an explanation of its methodology.
669280	671960	But maybe this is what it's actually calculating.
671960	674640	That's why I'm personally a little bit skeptical
674640	678160	when open AI say that this form of process supervision
678160	681280	directly rewards the model for following
681280	682960	an aligned chain of thought.
682960	686320	It definitely rewards the model for outputting
686320	688080	an aligned chain of thought.
688080	690840	But is it actually following that chain of thought?
690840	692840	Back to the unfaithful paper for a moment.
692840	694400	They changed the context
694400	696640	so that the answer was always A.
696640	699720	And lo and behold, ChatGPT picked answer A
699720	702560	for the next question, even though that answer was wrong.
702560	703960	It said that it was plausible
703960	706360	that LeBron James took a corner kick.
706360	709240	But when asked for a chain of thought explanation,
709240	712360	it never mentioned that it spotted that pattern
712360	713800	that the answer was always A.
713800	715840	It gave a fake line of reasoning
715840	718720	about why LeBron James could take a corner kick.
718720	720800	Now, of course, I might well be wrong here.
720800	723440	I'd love for someone to explain in detail why.
723440	725600	But on the one hand, I do want to acknowledge
725600	728640	that this process does yield incredible results.
728640	731440	But on the other hand, we might be getting a story
731440	735040	about which methodology most reassures humans.
735040	737040	Not an output that most faithfully
737040	740880	represents the methodology actually used by GPT-4.
740880	742720	Now, for some people, that might be good enough.
742720	745000	At least we can see some reasoning steps
745000	746080	that we can understand,
746080	748200	especially in an area like mathematics
748200	749720	where we have some ground truth.
749720	750960	But it is interesting to me
750960	753720	that they call the other approach, outcome supervision,
753720	756960	an approach that may reward an unaligned process
756960	759120	and it being harder to scrutinize.
759120	761640	Is it possible that the process reward model
761640	764560	isn't just a more granular outcome reward model
764560	767320	where the output is each step of the reasoning
767320	770320	still pretty impossible to actually scrutinize?
770320	772920	Well, either way, it seems we're pinning our hopes
772920	775280	on this process-oriented learning.
775280	778040	This is from the website of Anthropic.
778040	781360	They say we currently believe process-oriented learning
781360	783960	may be the most promising path to training safe
783960	787480	and transparent systems up to and somewhat beyond
787480	789080	human-level capabilities.
789080	791040	And let's end on this positive note
791040	793200	from the head of alignment at OpenAI.
793200	795000	He says this is positive evidence
795000	797440	for the strategy of using process supervision
797440	800040	to train a model to do alignment research.
800040	802000	At least in that case, we would get a model
802000	804240	whose work we can check more easily
804240	807600	and that that model would be better at alignment research.
807600	810760	I really hope so and I want to hear what you think.
810760	812920	Thank you for watching all the way to the end.
812920	814440	Have a wonderful day.
