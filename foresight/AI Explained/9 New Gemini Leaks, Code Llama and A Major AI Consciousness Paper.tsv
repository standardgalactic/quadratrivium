start	end	text
0	6080	Like buses, AI news can sometimes be slow and sometimes arrive all at once.
6080	12640	In the last few days we have had dramatic new leaked insights into the sheer breadth of Google's
12640	18960	Gemini. Just today we've had the release of Meta's Code Lama and earlier their impressive
18960	26800	multilingual seamless M4T model. And last but definitely not least, this 88-page AI
26880	32640	consciousness report. And yes, I read it all, it's juicy so I'm saving that for the end.
32640	38720	But let's start with two major paywall articles, one from the information and one from the New York
38720	45040	Times about Google's Gemini model. From both of them I counted a total of nine new revelations,
45040	48480	so let's get straight to it. To give you a sense of timeline, by the way,
48480	55440	Google's newly merged AI SWOT team, they call it, is preparing for a big fall or autumn launch.
55440	61200	The takeaway for me from both articles is that Gemini is going to be the everything model. Did
61200	67920	you know it's going to be the rival to mid-journey and stable diffusion? Mid-journey only has 11
67920	74000	full-time staff, so it is more than plausible that Google's Gemini could outperform mid-journey
74000	81120	version 5. Next we may be able to create graphics with just text descriptions and control software
81120	86640	using only text or voice commands. These next two are speculations, so I'm not even counting them
86640	92160	in the list of leaks. I've already covered in a previous video that Gemini has been trained on
92160	98800	YouTube video transcripts, and the speculation is that by integrating video and audio into Gemini,
98800	104160	it could perhaps help a mechanic diagnose a problem with a car repair based on a video,
104160	110720	or be a rival to Runway ML by generating advanced text to video based on descriptions of what a user
110720	115440	wants to see. You can start to see why I'm beginning to think of it as the everything model.
115440	121120	Another leak is that one of the co-founders of Google, Sergey Brin, is working on the front lines
121120	126320	of Google Gemini. And lastly from this article, I found it really interesting that Google's
126320	132560	lawyers have been closely evaluating the training, and they made researchers remove training data
132560	138160	that had come from textbooks, even though those textbooks helped the model answer questions about
138160	144080	subjects like astronomy or biology. And I do wonder if they privately benchmarked Gemini before
144080	150720	removing that crucial textbook data. But if that's not enough, prepare to also receive life advice.
150720	156000	My theory here is that Google wants to compete directly for market share with inflection's
156000	161280	pie. What if you want scientific, creative, or professional writing? Yep, they're working on
161280	166480	that too. In fact, we already know that Google has software named Genesis that they're pitching
166480	171920	to the New York Times, which can generate news articles, rewrite them, suggest headlines, etc.
171920	176720	But some people will be more interested in this feature that Google DeepMind is working on,
176720	182480	the ability to draft critiques of an argument and generate quizzes, word, and number puzzles.
182480	188320	It's almost easier at this point to ask what might Google Gemini not be able to do. And yes,
188320	195440	this is not Gemini, but Google DeepMind is also using AI to design the next generation of semiconductors.
195440	201120	But if the fall seems far away, how about today when we got CodeLama from Meta?
201120	207200	I spent much of the last two hours reading most of the 47-page paper, and you can see CodeLama
207200	213440	in action on screen. Some highlights include that the CodeLama models provide stable generations with
213440	219360	up to 100,000 tokens of context. Obviously, that could be used for generating longer programs or
219360	224240	providing the model with more context from your code base to make the generations more relevant.
224240	229200	It comes in three versions, CodeLama, CodeLama Instruct, which can better understand natural
229200	234080	language instructions, and CodeLama Python, better, of course, at Python. It's available
234080	240800	for commercial use, and as you can see, some of the versions rival GPT 3.5 on human eval.
240800	247840	That top score of 53.7% on Passat 1 puts it in the same ballpark as Phi 1. I've actually done
247840	255120	a full video on Phi 1, so do check that out, but that got 50.6%. But it is about 25 times smaller
255120	260480	at 1.3 billion parameters. Interestingly, the CodeLama paper, which also came out about two
260480	266400	hours ago, mentions Phi 1 directly, saying that it follows in a similar spirit, but the difference
266400	272240	is that Phi 1 is closed source. Anyway, a couple more interesting things before we move on from
272320	278080	CodeLama, and the first one is the self-instruct method that they used. Let me know if you also
278080	284240	find this fascinating, because step one was to generate 62,000 interview style programming
284240	289760	questions by prompting Lama 2, the 70 billion parameter model. Then they removed duplicates in
289760	293920	step two. But here's where it gets interesting. For each of those questions, they first generated
293920	300560	a unit test by prompting CodeLama 7 billion parameters. Then they generated 10 Python solutions
300560	306080	by prompting CodeLama. Finally, they ran unit tests on those 10 solutions, and they added the
306080	310880	first solution that passes those tests, along with the corresponding question and test, to the
310880	315280	self-instruct data set. If that sounded a bit complicated, let me try to distill it a bit.
315280	322080	They asked the Big Brother Lama 2 model to generate questions, then got the Little Brother CodeLama
322080	328400	to generate tests for those questions, then got the model to generate solutions to its own tests,
328400	333280	found the good solutions that don't forget it produced, and then used those to further train
333280	339520	the model. To be honest, synthetic data and self-instruct seem to be the future of feedback.
339520	345200	One final interesting quote from the paper on safety, and that was an argument advanced by one
345200	351360	of their red teamers. They made the point that various scripts and code is readily available on
351360	356640	mainstream public websites, hacking forums, or the dark web. And the advanced malware development
356640	362560	is beyond the current capabilities of available LLMs. And even an advanced LLM paired with an
362560	368240	expert malware developer is not particularly useful at the moment, as the barrier is not typically
368240	374560	writing the malware code itself. Let me know what you think in the comments. But we must move on to
374560	382080	seamless M4T released a couple of days ago from Meta, which frankly seems amazing for multi-lingual
382080	388160	translation. That's speech to text, speech to speech, text to text, and more. It has speech
388160	394800	recognition for nearly 100 languages and can output in 36 languages. But there's one feature I find
394800	402800	particularly cool. Now, let's talk about code switching. Code switching happens when a multilingual
402800	408800	speaker switches between languages while they are speaking. Our model seamless M4T automatically
408800	414640	recognizes and translates more than one language when mixed in the same sentence. As a multilingual
414640	420000	speaker, this is a very exciting capability for me. I often switch from Hindi to Telugu
420000	424960	when I speak with my dad. Notice in the following example when I change languages.
439040	447280	I can speak Hindi, Telugu, and English. Sometimes I use all three languages in one conversation.
447840	454800	Speaking of cool though, we had this epic story out yesterday. AI gave a paralyzed woman her voice
454800	461360	back. In a moment, you're going to see her being plugged in to the model. There we go. And the short
461360	467760	version is that this woman suffered a stroke that left her unable to speak. But now for the first
467760	474160	time, her speech and facial expressions can be synthesized from her brain signals, decoding
474160	480080	these signals into text at nearly 80 words per minute up from 14 words per minute. But let's
480080	486400	now end on this, an 88 page report on consciousness in artificial intelligence, which counts as one
486400	492240	of its co-authors, Yoshua Benjio, the Turing Award winner. It was dense and quite technical,
492240	498640	but well worth the read. Look at this sentence in just the abstract. Our analysis suggests that no
498640	504240	current AI systems are conscious, but also suggests that there are no obvious technical barriers to
504240	511600	building AI systems which satisfy these indicators. These are the indicators and each one gets a few
511600	517760	pages in the report. And the reason that they're split up is because each one rests on a certain
517760	524240	theory of consciousness. Obviously, the key problem is that we don't have a consensus theory on what
524240	530000	consciousness is or how it comes about. So in a way, to hedge their bets, they group in different
530000	535200	theories and look at the kind of indicators that would satisfy each one. You might say that list
535200	540960	seems so theoretical. Why not just test the model or even ask the model? For more on that approach,
541040	545760	see my theory of mind video. But the problem is, as they say on page four,
545760	552320	the main alternative to a theory heavy approach is to use behavioral tests for consciousness.
552320	557280	But as I talked about in the other video, that method is unreliable because AI systems can be
557280	562800	trained, of course they are, to mimic human behaviors, are working actually in very different
562800	568640	ways. Essentially, LLMs have broken the traditional tests for consciousness, including of course the
568640	573600	Turing test. The paper also rests on the assumption of computational functionalism,
573600	578960	essentially that computations are essential for consciousness. As in, it's not what you're made
578960	584960	of, it's what you do. If this is wrong, and the substrate in fact is key, say biological cells,
584960	589680	then it stands to reason that AI would never be conscious. But one of their early conclusions
589680	595440	is that if computational functionalism is true, and it is widely believed, conscious AI systems
595440	600640	could realistically be built in the near term. Having digested the entire paper, they're strongly
600640	606240	suggesting that we're not there yet. But if this theory is true, we could be there, especially if
606240	612160	researchers deliberately designed systems to meet these criteria. In fact, here is a key quote from
612160	617760	one of the authors in Science that came along with the piece. It would be trivial to design all of
617760	623600	these features into an AI. The reason no one has done so is it is not clear that they would be useful
623920	629600	for tasks. Now, to be honest, it is way beyond my pay grade to try to explain every aspect of the
629600	635440	paper. But I'm going to try my best to convey the key bits. First, what is the definition of
635440	640560	consciousness that they are working with? Well, skipping the jargon, they essentially say, if you
640560	646160	are reading this report on a screen, you are having a conscious visual experience of the screen. That
646160	652000	is separated from sentence, which is also sometimes used to mean being capable of pleasure or pain.
652000	656640	And they say that it's possible for a system to be sentient without being conscious by sensing
656640	662080	its body or environment. And it's possible for a system to be conscious without sensing its body
662080	667920	or environment. It also might be possible to be slightly conscious or conscious to a greater
667920	674560	degree than humans. Ilya Sutskova famously said, it may be that today's large neural networks are
674560	680240	slightly conscious. And the Karl Schulman and Nick Bostrom wrote an entire chapter of a book
680240	685120	on the possibility that models become more conscious than human beings. They say such
685120	689920	beings could contribute immense value to the world and failing to respect their interests could
689920	695760	produce a moral catastrophe. One of the theories of consciousness discussed is recurrent processing
695760	701680	theory. And here is the key part of that theory. One initial feed forward sweep of activity through
701680	707120	the hierarchy of visual areas is sufficient for some visual operations like extracting features
707120	713040	from a scene, but not sufficient for conscious experience. However, when the stimulus is sufficiently
713040	718320	strong or salient, we get this looped recurrent processing in which signals are sent back from
718320	725280	higher areas to lower ones. It's only then that you get a conscious representation of an organized
725280	731200	scene. The paper then draws indicators based on each theory. For example, if recurrent processing
731200	737840	theory is accurate, then here are two indicators that something would be conscious. They then draw
737840	744320	analogies for each theory to AI systems. For example, on recurrence, specifically algorithmic
744320	749600	recurrence, they say that's a weak condition that many AI systems already meet. But don't forget
749600	755200	when they say that it's an analogy. Not only does it require the theory to be correct, it requires
755200	761280	the analogy to hold true. i.e. is the recurrence that we see in AI a good analogy for the recurrence
761280	767360	of this theory. Or what about the next one, global workspace theory? If that theory is correct, here
767360	772720	are four indicators of something being conscious according to that theory. To be honest, if you
772720	778880	are at all interested in consciousness, the pages on each one of these taught me a lot about tests
778880	784240	for consciousness and just theories of consciousness. But again, let's just say that theory is correct.
784240	790080	Do AI systems demonstrate these indicators? Do they have modules that can work in parallel
790080	796640	and a global workspace at the center? Is that workspace bandwidth limited, requiring the compression
796640	803200	and selection of information from the modules? Well, here again, we can only rely on analogies,
803200	808640	in this case, to the transformer architecture. They say, in a sense, they do have modules,
808640	813520	they do have a limited capacity workspace introducing a bottleneck, but then the authors
813520	819280	introduce plenty of points about how the analogy is not perfect, even here. Of course, you can pause
819280	824720	and read the details if you like, or indeed read the entire paper. So that's the tone of the paper.
824720	831200	If silicon can be a replacement to carbon, and if these analogies hold, then there is a strong
831200	836240	case that most or all of the conditions for consciousness, suggested by current computational
836240	841600	theories, can be met using existing techniques in AI. This is not to say that current AI systems
841600	845920	are likely to be conscious. There is also the issue of whether they combine existing techniques
845920	850800	in the right ways. But it does suggest that conscious AI is not merely a remote possibility in the
850800	856640	distant future. And here is the key bit. If it is at all possible to build conscious AI systems
856640	862240	without radically new hardware, it may be possible now. Of course, even if all of those conditions
862240	868160	and analogies hold, it may not be the same type of consciousness as our consciousness. It seems
868160	873840	possible, they say, to imagine a conscious being that had only a succession of brief static
873840	878640	discrete experiences, perhaps just during pre-training. Or they might have experiences
878640	884320	without feeling that they are a persisting subject. But my own summary is this. We clearly
884320	889760	don't fully understand consciousness or what is required for consciousness. We don't know if
889760	895520	consciousness in AI systems is theoretically impossible or imminent. The authors actually
895520	900800	quote this open letter from the Association for Mathematical Consciousness Science. And in it,
900800	906240	at the end, the letter says, we emphasize that the rapid development of AI is exposing the urgent
906240	911200	need to accelerate research in the field of consciousness science, even if we develop a
911200	916160	system that ticks all of these indicators. And trust me, someone is probably working on that
916160	921440	right now. We still won't know for sure, and many people will deny forever that that system
921440	925760	is conscious. I'm not claiming I have the answer, by the way. I have absolutely no idea
925760	930080	if these systems are imminently conscious, already conscious, or will never be conscious.
930080	935120	All I can say is that it's a bit less sci-fi than many people believe. And the authors also point
935120	940560	out two risks, under attributing consciousness to AI, playing down the possibility. But they also
940560	946000	point out the risk of over attributing consciousness to AI. On under attributing consciousness,
946000	951280	they say this, given the uncertainties about consciousness mentioned above, we may create
951280	956720	conscious AI systems long before we recognize that we have done so. And I see this sentence as a
956720	962480	fateful prediction. This tendency is further amplified when AI systems exhibit human-like
962480	968000	characteristics, such as natural language processing, which they already do, but also facial expressions
968000	972800	or adaptive learning capabilities. So imagine what people are going to think when photo-realistic
972800	979360	AI avatars are everywhere. And finally, there is the risk of experimentation itself. On balance,
979360	984640	we believe that research to better understand the mechanisms which might underlie consciousness in AI
984640	989840	is beneficial. However, of course, research on this topic runs the risk of building or enabling
989840	994880	others to build a conscious AI system, which should not be done lightly, and that mitigating
994880	1000080	this kind of risk should be carefully weighed against the value of better understanding consciousness
1000080	1005600	in AI. And to tie this back to the start of the video, Google's AI safety experts have added
1005680	1010160	that some users who grew too dependent on this technology could think it was sentient.
1010160	1016160	And I do wonder if that's an eventuality, Google's newly merged AI SWOT team is preparing for.
1016160	1020400	As always, thank you so much for watching to the end and have a wonderful day.
1020400	1026080	Oh, and just quickly before I end, I now have a discord, AI Explained Community. More info in the
1026080	1026720	description.
