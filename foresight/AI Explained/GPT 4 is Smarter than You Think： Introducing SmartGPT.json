{"text": " I have three goals for this video. First, I want to show you a way of using GPT-4 to get smarter results. Second, I want to argue that the benchmark results we have for GPT-4 do not reflect its full abilities. And third, I want to show you a system that I am developing, somewhat cheekily called Smart GPT, that is already showing significant results on official benchmarks. It remains to be fully optimized, which I think is exciting in itself. I have shown the system to people at Open AI who have been quite impressed, and I'm going to end with some reflections on where that might leave us for GPT-5. But before I get into how it works, I just want to show you one example of it in action to whet your appetite. This example comes from a TED talk that was released this week. So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely. How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not good. On the left, you can see GPT-4's original answer, and it gives this answer pretty consistently whenever you prompt it with the question provided. On the right, you can see the final answer from the Smart GPT model, which is correct, and it consistently gives that answer. I really like how it gives context as well, and it provides some of the assumptions that it had in giving this correct answer. Now, don't you worry, there will be plenty more examples to go through in this video, including another one from that TED talk. But first, I want to give you an overview of what is this Smart GPT model, where did I get my inspiration for it from, and how does it work? I'm going to keep it fairly simple because it's the beginning of the video, and I know a lot of people won't really care about the inner details that will come later in the video. But the high-level overview is this. There are at least three things that have been proven to improve the outputs of GPT-4. What's called chain of thought prompting, sometimes called step-by-step prompting, reflection, or finding its own errors, and I did an entire video on this called GPT-4 Can Self Improve, and dialoguing with itself, entering into a back and forth on its own outputs and deciding which one is best. You can see the title of the papers, which contain much more detailed results, of course, linked above. Now, the first paper only came out a few days ago, midway through my testing, so my results don't even reflect the full capacity of the model. And even if there's nothing else you take from this video, the results from this paper can instantly improve the outputs you get from GPT-4. Many of you might remember that prompting GPT-4 with let's think step-by-step improves its results. To give you a very quick reference point, just asking a question to GPT-4 gives you 81% accuracy. With that prompt, let's think step-by-step, it goes up to 86%. But algorithmically, the paper found an improved prompt that can give you even better results, 89% accuracy. All we do, and this is the first part of smart GPT, is we add answer, let's work this out in a step-by-step way to be sure we have the right answer. Now, I have so much to say about why I think this works, but I know many of you won't be that interested in my theories, so I'm going to save them to the end for those who are interested. Some of you just want the results, so I'm going to get to those first. So far, you might be thinking, well, thanks, Philip, that's a cool prompt. I'm going to use that. But what's this whole smart GPT about? Is it just a single prompt? No, I believe with evidence, there are ways of leveraging even better results than just using a great chain of thought prompt. So let's move on to the next part of the system, these different outputs in the middle. For my tests, I typically did three outputs, but of course, depending on the context window, it could be far more than that. And I'm going to talk about ways I could further improve this model, or we could later on in the video. Just to restate, these outputs are when you take the user input and add the word question at start, and then at the end add answer, let's work this out in a step-by-step way to make sure we have the right answer. And at this moment, many of you are thinking, what is the point of multiple outputs? It's GPT-4, it's just going to give you the answer that thinks is best, and that's it. Well, actually, it doesn't quite work like that. These models have a temperature between zero and one. I believe the default for GPT-4 might be around 0.5. And simplifying massively, this determines how creative or conservative the model is in giving its outputs. So given that GPT-4 tries to be fairly creative, you don't get the same output every time. The output is randomly sampled according to an internal probability distribution. So you can get situations, and I face this hundreds of times, where some of the outputs are correct, and others are incorrect. And this is where reflection comes in. Sometimes, definitely not always, but sometimes quite often, GPT-4 can detect the errors in its own output. And many of you will notice at this point that the prompt that I used to elicit GPT-4 to spot its own errors contains the same step-by-step prompt I used earlier that has been shown to produce good results. So to summarize, sometimes at this stage, GPT-4 detects the errors that some of its outputs have made. Definitely not always. There are certain questions, it just simply can't spot the error. But sometimes it can. And then I get it to engage in a dialogue using a format similar to one in this paper published last month. It's a short dialogue, and this is the step I believe that can be most optimized. In the future, I envision an entire council of advisors made up of GPT-4 imitating mathematicians, judges, etc. At the moment, it's just being a resolver and printing a final improved output. Anyway, I'm going to get back to the theory later in the video because I know some of you will be getting bored at this stage and want to see more practical examples and the results from my benchmark tests. As I don't have the GPT-4 API key, yes, I had to manually input each of these steps hundreds of times, waiting sometimes three hours between each go because you can only do 25 messages every three hours. On the left, you can see the three outputs when you ask it to think step by step. And then you have the researcher step in the middle and at the top right. And finally, the resolver step. Notice here, I was using the original let's think step by step because the paper hadn't yet been published on improving that prompt. It's time for the second example from that TED Talk, and then I definitely will get on to the benchmarks. A different one. I have 12 liter jug and 6 liter jug, and I want to measure 6 liters. How do I do it? Just use the 6 liter jug, right? GPT-4 spits out some very elaborate nonsense. Of course, I tested smart GPT with that question, and you can see the difference between the original GPT-4, which gives this incredibly convoluted bad answer, and smart GPT, the final answer output. Now, at this point, I know many of you will be impressed, but you'll be thinking, I don't have time to input things five times. Well, I'm developing a model where it can all be done automatically. Here is a preview of how it works. But of course, at the moment, it has to use GPT 3.5 Turbo because I don't have the API key of GPT-4. But the epic thing is this, you just ask a single question, I've written, ask smart GPT-A question. And of course, it does take a little bit longer to respond because it's doing five or six calls via API, but it does output the final answer from the resolver step. I will be honest and say that GPT 3.5 isn't as good at reflecting or resolving. But this is an example of a question where the original chat GPT consistently gets it wrong, and smart GPT 3.5 gets it right using this program. Remember, all you have to do as a user is type in a question as normal, and it goes through this entire five or six step process behind the scenes. By the way, this was a question from MMLU, which is a famous benchmark which I'll get to in a second. Here's one last practical example before I get to that benchmark. I know many teachers use chat GPT and GPT-4 to create quizzes for their classes. And here is the same question put through GPT-4 and smart GPT. The question is, create a high school algebra quiz with five questions and answers and explanations at the end. Now points for spotting the difference, but if the teacher had handed out the original quiz, look at the answers for question five. It says the answers are one and 1.5. But then in the explanation, it gives the final answers, which are correct by the way, of three and zero point five. So that would really confuse some students at the reflection stage smart GPT spotted that error and resolved it. And as you can see, the answer for question five has the correct answers straight away. If at any point you're wondering if I completed the open AI chat GPT prompt engineering course, the answer is yes, but it didn't inform too much of my thinking. It was more for beginners and I had already factored in things like giving the model time to think and writing clear instructions. The benchmark that I chose to test smart GPT on was the famous MMLU, massive multitask language understanding benchmark. As you can see, the state of the art is indeed GPT for with 86.4% accuracy. And you know, open AI think it's a big deal because it's the benchmark mentioned on the front page of their technical report without boring you too much. I extracted the questions from the test set of the MMLU data file, and I didn't pick the topics at random. I went for those that I thought GPT for would find the hardest delving into the original MMLU paper. You can see that GPT three found for more logic the hardest scoring just over 25% which is random chance. It's a four question multiple choice test. So around 25 or 30% is pretty bad. And notice they helped out GPT three here. They did it few shot, meaning they gave it five successful examples before asking it a new question. It's the same thing they did with GPT four. They did it five shot. But just before I show you the results, there are three things I want to mention here. First, I was curious how smart GPT would do without any help zero shot. Second, I wanted to do it zero shot because people using GPT four don't typically give five successful examples before asking GPT for a question. They just want code or a quiz or a poem or an example. They don't often provide five brilliant examples of code before asking their question. And third, if I can prove it works zero shot, then of course, future refinements can be made to push the results even further. And here are the results from the first 25 questions from the formal logic test set of the MMLU. I did many more tests after this. But you can see from this set, if you just ask the question, you get a lower overall accuracy. But of course, 68% for GPT four is still a huge improvement over GPT threes around 25%. What happens when you add let's think step by step, which as we know now isn't the fully optimized chain of thought prompt. Well, on average, you get around 74-75%. That was 75 examples inputted manually. And I still have all the tabs open. I'm keeping them open because I'm compiling a spreadsheet with the actual outputs. But what did the resolver get drawing upon GPT four's ability to reflect and engage in dialogue with itself? It got 84%. Now notice something about that number. GPT four zero short got 32% of the questions wrong. That was halved to 16% after putting it through the smart GPT system. There was one question where the resolver model gave both a correct and incorrect answer. But I'm counting that as an incorrect answer for the purposes of this test. Anyway, from 32% to 16% incorrect, that is a pattern that stayed consistent throughout all my testing that approximately half of the errors that GPT four makes can be rectified. If you give it the optimized step by step prompt, get it to reflect on its results and get it to engage in dialogue and decide on a final answer. At this point, for those people losing track of all the details, I want to put into context what resolving half of the errors on MMLU might mean in the context of the big picture. Here's Lenard Heim, an AI governance researcher, suggesting a score of 95% on the MMLU would be reflective of AGI like abilities. I do think I have like a 50% chance like within the next 20 years or so, there might be something what we might call an AGI or transformative AI. What do I mean by this? Well, maybe can measured on benchmarks. There's like this famous MMLU benchmarks like yet or something which like scores like 95% on this. Going back to the results, if a smart GPT like system can automatically resolve half of the errors that GPT four makes on the MMLU, that would increase its score from around 86.4% to around 93%, which is not far off 95%. Remember, his prediction was a 50% chance in 20 years. I'm talking about GPT four now. For those who are still skeptical, I'm going to show you plenty more results now and then walk through the papers that give the theory as to why this works. One thing that I forgot to mention earlier is that the human expert level on the MMLU is 89.8%. And that's taking the 95th percentile of human test takers. And remember, those are domain experts in each of these subtopics. What we're doing is testing GPT four or smart GPT on all of the topics simultaneously. So even if smart GPT like systems can't quite reach 95%, and I think honestly, they'll get pretty close with all the refinements that I'm going to suggest, I think they should almost certainly be 89.8%, which is the human expert test taker level. Intrigued by these results, I then put it through the college math test from the MMLU. And remember, this was before using the optimized version of the step by step prompt. Obviously, I'm not going to go through all the questions here, but let's skip to the final results. We have zero shot accuracy, six out of 15, which is 40%. The average when you add let's think step by step was 53.5%. And then the final output of the resolver model had a 60% accuracy. So it couldn't quite resolve half of the errors, but the overall pattern held up. In case anyone is wondering about methodology, I kept the formatting identical for every question. I always opened a new tab for each question. It wasn't looking at the context of what it had already put out. Each attempt was fresh, aside from the resolver model, which looked at the context of the researcher's output. And again, as you can see from example 14, it wasn't like the researcher could always spot the errors or that the resolver could always pick the right option. Sometimes the let's think step by step prompt gave the right output, but the resolver couldn't quite distinguish it. The optimized prompt gets a slightly better output. And upon reflection, the researcher can sometimes but not always spot the errors of those outputs. And sometimes but not always the resolver can spot based on those flaws, which answer is best. These are incremental improvements. Sometimes GPT-4 simply can't get it right. I have noticed a few themes in those questions. Anytime it comes to division, multiplication, characters, or counting in general, GPT-4 tends to make mistakes that neither the researcher nor resolver can spot. Of course, integrating a few tools via API would likely solve those issues. And I don't want to preempt the conclusion too much, but I believe a smart GPT-like system with tools integrated could probably score around 95% right now on the MMLU, especially if it was helped out with a few shot prompting. To add weight to that preliminary conclusion, I tested it on certain topics and had to stop because it simply got the questions right every single time. For example, high school psychology from the MMLU. I then tried prehistory, which it also aced before finding machine learning where I got more interesting results. Zooming in this time, the raw score was 65%. The chain of thought let's think step by step average was 71.6% and the resolver model got 80%. Let's now look a little deeper into why all of these steps might improve the end result. In reply to the original let's think step by step paper, which was published around a year ago, Andrea Carpathi said this. Adding something like let's think step by step to the prompt is a way of using the input space for computation that you'd normally want in the hidden state of the model. Instead of the workings out being done in the activations of the neural network, it's done in the discrete tokens of that input space. And he adds did not super see this coming. And here is the paper released three days ago that improves upon that original prompt. They also did their testing zero shot like me. And they tested many prompts starting like I did with just direct prompting, just asking the question like 99% of users would do of GPT four. And then they tried like me the well established let's think step by step prompt. They also iteratively tested seven original prompts, as well as the prompt that I've now integrated into smart GPT the let's work this out in a step by step way, etc. They share my opinion that zero shot prompting setups have the benefit of not requiring such task dependent selection of exemplars. You don't have to find correct examples. It just does it all for you. Here are the end results for GPT four that we saw earlier showing the difference between asking directly your question and using these refined prompts. Notice that this technique is somewhat model dependent. And it doesn't have the same effect on smaller or weaker models. Before we move on to the next paper, there is one somewhat failed prompt that I want to pick up on. It's this self critique prompt where they ask answer the question, then critique the answer based on the critique, reconsider the other answer options, and give a single final answer. And you might wonder why didn't that prompt perform best when we know that reflection and dialogue can work? My theory is because it's trying to do all of it in one prompt. Through my hundreds of experiments, I've noticed that GPT four can only handle so much in one go. It simply gets overwhelmed or confused if you ask it to do too much in one prompt. That's why I broke my model into stages to allow it to show off each of its abilities one by one. And before we get to the other papers, what's my personal theory as to why this eliminates up to half of the errors that GPT four makes? Well, my guess is this. Remember that GPT four is drawing on a vast data set of internet text. And let me ask you what kind of text has things like question, answer, let's work this out. Be sure we have the right answer. The kind of data that would have that text would be things like tutorials or expert breakdowns. So I believe you're triggering more of the weights inside GPT four that relate to things like expert tutorials. And so inevitably you're getting slightly better answers. Next, I've already explained why you get different outputs when you give the exact same prompt. That's down to sampling and the temperature of the model. But to simplify massively, sometimes GPT four will give you an output that it knows isn't the most probable. It introduces some randomness into its sampling by generating multiple outputs, you're getting a larger sample size, reflecting the full range of probabilities that GPT four ascribes to its outputs, you're reducing a little bit some of the randomness that's inherent in GPT four outputs. Next, I believe that GPT four can sometimes spot its own errors through reflection, because prompting like this triggers a different set of weights, you could almost think of it as a different mindset, one more focused on finding errors. Again, if the question is too hard or involves counting characters, division, multiplication, as I said earlier, this won't help. But a percentage of the time it can spot its own errors and point them out. Notice this is a separate bit of inference not lumped into the original prompt. And when it does successfully point out the errors, it can often engage in this dialogue with itself. Notice in a meta kind of way, I'm using the step by step prompting to improve the reflection and dialogue. So those are my theories as to why it works, but at the end of the video, I'm going to show you at least five ways I think the model can be further refined. Before we do, though, I looked up the paper by Joe, which produced that prompt that did the best in the previous paper, they came to that special prompt through automatic prompt engineering. But there's something interesting I want to point out, though, on page seven, they say we use automatic prompt engineering to find a prompt starting with let's that maximizes the likelihood of correct reasoning steps. Then they found the best one that I integrated into smart GPT. Let's work this out in a step by step way to be sure we have the right answer. That's the one I want you to use. And they ran their own benchmarks. And of course, it did improve the scores. But the interesting thing to me is they started with let's each time. So even that first stage for the model might not yet be fully optimized. Maybe there's a prompt that doesn't begin with let's that improves this initial results still further. Anyway, back to the papers, I know many people watching this will wonder if I read the paper boosting theory of mind performance in large language models via prompting. And yes, I did because they tested something similar for a theory of mind test. Using similar techniques, they were able to get theory of mind accuracy for GPT four from 80% to 100%. And they conclude that these results demonstrate that appropriate prompting enhances large language model theory of mind reasoning. And they underscore the context dependent nature of these models cognitive capacities. They use that original prompt, let's think step by step, along with some few shot examples. Take a look at the GPT four table. And you can see how the let's think step by step improved the results dramatically. And as I theorized earlier, adding few shot examples would push this still further. This is part of why I think that 95% barrier on the MMLU will be broken probably this year by GPT four, a few other points from this paper. They admit that there is not currently a theoretical understanding of why these prompting techniques are beneficial. I've given you my theory and car pathies, but no one quite knows for sure. Lastly from this paper, and I found this really interesting, giving it generic few shot prompts that weren't directly theory of mind actually improve the outputs slightly more than giving it direct theory of mind examples. This opens the door to the first of the five ways I anticipate smart GPT getting even smarter. It could be possible to come up with generic few shot prompts that could be automatically integrated into the model that don't necessarily relate to the topic at hand. This graph shows the impact of adding few shot examples to GPT three. And if this can be done in a generic way for GPT four, results could be improved still further. Next, the boosting theory of mind paper speculates that integrating some of these approaches could boost the performance of weaker models to beyond the levels of GPT four zero shot accuracy. Next, here is the original DERA paper that inspired me to have the researcher and resolver dialogue at the end of smart GPT. As they say, the DERA approach shows significant improvement over base GPT four performance. And these were open ended questions, by the way, not multiple choice. So this is more generally applicable than you might think. You can see from this table how results improved after engaging in this dialogue. And that brings me to the second way I anticipate smart GPT getting smarter in the future, a longer and more rich dialogue. At the moment, we have this simple researcher and resolver two step dialogue. I can imagine a council of advisors, you can imagine a mathematician chipping in and a philosopher and a professor, each one tapping into slightly different weights of GPT four, extracting more hidden expertise. I'm not saying that would transform the results, but it might edge them another few percent higher. Next, even with longer dialogues and different experts, we could find ways of optimizing these prompts just like we did with the original let's think step by step. That's the third avenue of improvement that I envisaged because I came up with these prompts, I'm sure they could be improved. Next, we could experiment with different temperatures. Remember, a lower temperature makes the model more conservative, a higher one towards one makes it more creative. We could experiment with a higher temperature to produce a more diverse range of outputs at this stage, and then perhaps a more conservative, deterministic temperature for the final judge or resolver. It might not work, but it's worth trying. And the fifth improvement I know would work, integrating APIs for character counting, calculators, code interpreters, etc. Spending these weeks manually sorting through the outputs of GPT four on these benchmarks, I can really see where it goes wrong. And it's often by getting letters in the wrong order or making mistakes with division, it gets the high level logic right, and then makes quite simple errors. Basic tool integration would I am sure push the results still higher. Now, I know this isn't my usual video. And trust me, I have been following the AI news and we'll get back to that very soon. I'm determined to make those improvements and push smart GBT even further. But of course, that will be aided massively by getting access to the plugins and the GPT four API key. So far, I've had to do all of this manually, which was a lot of work. Now, as you saw earlier, I have drawn on GPT four to help me develop a program in replete to automate this process. But at the moment, it's GPT 3.5. And honestly, the context window really limits the ability. But I do look forward to the day when I can integrate GPT four and put this out as an automatic model for people to test and play about with. I'm sure that something similar will ultimately be incorporated by open AI itself, maybe as a thoughtful mode or smart mode, a bit like Bing has creative, precise balance, etc. Each response does take longer. But as you've seen, the outputs are noticeably better. If the results of models like this one do officially exceed the 86.4% that open AI talked about in the GPT four technical report, I do think that would reveal quite a few things. First, the open AI isn't even aware of the full capabilities of its own model. I don't even know if they anticipated things like auto GPT. I do think it would reveal that they need to do far more proper testing of their models before they release them. They should make falsifiable predictions about what their models won't be capable of. That way we would know just how much they know about their own models. What we're trying to avoid is a situation where open AI say their model can only achieve X. And then when they release the model in the wild, someone comes along and achieves Y, where Y is much more impactful than X. So those were the goals of this video to show you how to get more out of GPT four to run you through some of the fascinating papers that have been released in the last few days and weeks. The third goal was to show you what this model could do with some official benchmarks and suggest ways it might get better in the near term future. Of course, if you have a GPT four API key or are an expert in benchmarking systems like GPT four, I'd love to hear from you. I guess the final goal was to perhaps suggest to you that open AI don't know as much about their own models as they might lead you to believe. Thank you so much for watching to the end and have a wonderful day.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.16, "text": " I have three goals for this video. First, I want to show you a way of using GPT-4 to get smarter", "tokens": [50364, 286, 362, 1045, 5493, 337, 341, 960, 13, 2386, 11, 286, 528, 281, 855, 291, 257, 636, 295, 1228, 26039, 51, 12, 19, 281, 483, 20294, 50672], "temperature": 0.0, "avg_logprob": -0.07610018399296975, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.021595492959022522}, {"id": 1, "seek": 0, "start": 6.16, "end": 13.040000000000001, "text": " results. Second, I want to argue that the benchmark results we have for GPT-4 do not reflect its full", "tokens": [50672, 3542, 13, 5736, 11, 286, 528, 281, 9695, 300, 264, 18927, 3542, 321, 362, 337, 26039, 51, 12, 19, 360, 406, 5031, 1080, 1577, 51016], "temperature": 0.0, "avg_logprob": -0.07610018399296975, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.021595492959022522}, {"id": 2, "seek": 0, "start": 13.040000000000001, "end": 18.080000000000002, "text": " abilities. And third, I want to show you a system that I am developing, somewhat cheekily called", "tokens": [51016, 11582, 13, 400, 2636, 11, 286, 528, 281, 855, 291, 257, 1185, 300, 286, 669, 6416, 11, 8344, 12839, 953, 1219, 51268], "temperature": 0.0, "avg_logprob": -0.07610018399296975, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.021595492959022522}, {"id": 3, "seek": 0, "start": 18.080000000000002, "end": 24.32, "text": " Smart GPT, that is already showing significant results on official benchmarks. It remains to", "tokens": [51268, 12923, 26039, 51, 11, 300, 307, 1217, 4099, 4776, 3542, 322, 4783, 43751, 13, 467, 7023, 281, 51580], "temperature": 0.0, "avg_logprob": -0.07610018399296975, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.021595492959022522}, {"id": 4, "seek": 2432, "start": 24.32, "end": 30.240000000000002, "text": " be fully optimized, which I think is exciting in itself. I have shown the system to people at Open", "tokens": [50364, 312, 4498, 26941, 11, 597, 286, 519, 307, 4670, 294, 2564, 13, 286, 362, 4898, 264, 1185, 281, 561, 412, 7238, 50660], "temperature": 0.0, "avg_logprob": -0.10473618968840569, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.2971600592136383}, {"id": 5, "seek": 2432, "start": 30.240000000000002, "end": 34.72, "text": " AI who have been quite impressed, and I'm going to end with some reflections on where that might", "tokens": [50660, 7318, 567, 362, 668, 1596, 11679, 11, 293, 286, 478, 516, 281, 917, 365, 512, 30679, 322, 689, 300, 1062, 50884], "temperature": 0.0, "avg_logprob": -0.10473618968840569, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.2971600592136383}, {"id": 6, "seek": 2432, "start": 34.72, "end": 41.04, "text": " leave us for GPT-5. But before I get into how it works, I just want to show you one example of it", "tokens": [50884, 1856, 505, 337, 26039, 51, 12, 20, 13, 583, 949, 286, 483, 666, 577, 309, 1985, 11, 286, 445, 528, 281, 855, 291, 472, 1365, 295, 309, 51200], "temperature": 0.0, "avg_logprob": -0.10473618968840569, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.2971600592136383}, {"id": 7, "seek": 2432, "start": 41.04, "end": 46.56, "text": " in action to whet your appetite. This example comes from a TED talk that was released this week.", "tokens": [51200, 294, 3069, 281, 315, 302, 428, 23996, 13, 639, 1365, 1487, 490, 257, 43036, 751, 300, 390, 4736, 341, 1243, 13, 51476], "temperature": 0.0, "avg_logprob": -0.10473618968840569, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.2971600592136383}, {"id": 8, "seek": 2432, "start": 46.56, "end": 53.760000000000005, "text": " So suppose I left five clothes to dry out in the sun, and it took them five hours to dry completely.", "tokens": [51476, 407, 7297, 286, 1411, 1732, 5534, 281, 4016, 484, 294, 264, 3295, 11, 293, 309, 1890, 552, 1732, 2496, 281, 4016, 2584, 13, 51836], "temperature": 0.0, "avg_logprob": -0.10473618968840569, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.2971600592136383}, {"id": 9, "seek": 5376, "start": 53.76, "end": 61.599999999999994, "text": " How long would it take to dry 30 clothes? GPT-4, the newest, greatest AI system says 30 hours. Not", "tokens": [50364, 1012, 938, 576, 309, 747, 281, 4016, 2217, 5534, 30, 26039, 51, 12, 19, 11, 264, 17569, 11, 6636, 7318, 1185, 1619, 2217, 2496, 13, 1726, 50756], "temperature": 0.0, "avg_logprob": -0.0711907949603972, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.003882495453581214}, {"id": 10, "seek": 5376, "start": 61.599999999999994, "end": 67.52, "text": " good. On the left, you can see GPT-4's original answer, and it gives this answer pretty consistently", "tokens": [50756, 665, 13, 1282, 264, 1411, 11, 291, 393, 536, 26039, 51, 12, 19, 311, 3380, 1867, 11, 293, 309, 2709, 341, 1867, 1238, 14961, 51052], "temperature": 0.0, "avg_logprob": -0.0711907949603972, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.003882495453581214}, {"id": 11, "seek": 5376, "start": 67.52, "end": 72.4, "text": " whenever you prompt it with the question provided. On the right, you can see the final answer from", "tokens": [51052, 5699, 291, 12391, 309, 365, 264, 1168, 5649, 13, 1282, 264, 558, 11, 291, 393, 536, 264, 2572, 1867, 490, 51296], "temperature": 0.0, "avg_logprob": -0.0711907949603972, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.003882495453581214}, {"id": 12, "seek": 5376, "start": 72.4, "end": 77.28, "text": " the Smart GPT model, which is correct, and it consistently gives that answer. I really like", "tokens": [51296, 264, 12923, 26039, 51, 2316, 11, 597, 307, 3006, 11, 293, 309, 14961, 2709, 300, 1867, 13, 286, 534, 411, 51540], "temperature": 0.0, "avg_logprob": -0.0711907949603972, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.003882495453581214}, {"id": 13, "seek": 5376, "start": 77.28, "end": 82.32, "text": " how it gives context as well, and it provides some of the assumptions that it had in giving this", "tokens": [51540, 577, 309, 2709, 4319, 382, 731, 11, 293, 309, 6417, 512, 295, 264, 17695, 300, 309, 632, 294, 2902, 341, 51792], "temperature": 0.0, "avg_logprob": -0.0711907949603972, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.003882495453581214}, {"id": 14, "seek": 8232, "start": 82.32, "end": 87.11999999999999, "text": " correct answer. Now, don't you worry, there will be plenty more examples to go through in this video,", "tokens": [50364, 3006, 1867, 13, 823, 11, 500, 380, 291, 3292, 11, 456, 486, 312, 7140, 544, 5110, 281, 352, 807, 294, 341, 960, 11, 50604], "temperature": 0.0, "avg_logprob": -0.0744544877780704, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.020956505089998245}, {"id": 15, "seek": 8232, "start": 87.11999999999999, "end": 91.91999999999999, "text": " including another one from that TED talk. But first, I want to give you an overview of what is", "tokens": [50604, 3009, 1071, 472, 490, 300, 43036, 751, 13, 583, 700, 11, 286, 528, 281, 976, 291, 364, 12492, 295, 437, 307, 50844], "temperature": 0.0, "avg_logprob": -0.0744544877780704, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.020956505089998245}, {"id": 16, "seek": 8232, "start": 91.91999999999999, "end": 96.55999999999999, "text": " this Smart GPT model, where did I get my inspiration for it from, and how does it work? I'm going to", "tokens": [50844, 341, 12923, 26039, 51, 2316, 11, 689, 630, 286, 483, 452, 10249, 337, 309, 490, 11, 293, 577, 775, 309, 589, 30, 286, 478, 516, 281, 51076], "temperature": 0.0, "avg_logprob": -0.0744544877780704, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.020956505089998245}, {"id": 17, "seek": 8232, "start": 96.55999999999999, "end": 100.47999999999999, "text": " keep it fairly simple because it's the beginning of the video, and I know a lot of people won't really", "tokens": [51076, 1066, 309, 6457, 2199, 570, 309, 311, 264, 2863, 295, 264, 960, 11, 293, 286, 458, 257, 688, 295, 561, 1582, 380, 534, 51272], "temperature": 0.0, "avg_logprob": -0.0744544877780704, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.020956505089998245}, {"id": 18, "seek": 8232, "start": 100.47999999999999, "end": 106.63999999999999, "text": " care about the inner details that will come later in the video. But the high-level overview is this.", "tokens": [51272, 1127, 466, 264, 7284, 4365, 300, 486, 808, 1780, 294, 264, 960, 13, 583, 264, 1090, 12, 12418, 12492, 307, 341, 13, 51580], "temperature": 0.0, "avg_logprob": -0.0744544877780704, "compression_ratio": 1.6213592233009708, "no_speech_prob": 0.020956505089998245}, {"id": 19, "seek": 10664, "start": 106.64, "end": 112.32, "text": " There are at least three things that have been proven to improve the outputs of GPT-4.", "tokens": [50364, 821, 366, 412, 1935, 1045, 721, 300, 362, 668, 12785, 281, 3470, 264, 23930, 295, 26039, 51, 12, 19, 13, 50648], "temperature": 0.0, "avg_logprob": -0.08476641371443465, "compression_ratio": 1.6628787878787878, "no_speech_prob": 0.4607222080230713}, {"id": 20, "seek": 10664, "start": 112.32, "end": 116.4, "text": " What's called chain of thought prompting, sometimes called step-by-step prompting,", "tokens": [50648, 708, 311, 1219, 5021, 295, 1194, 12391, 278, 11, 2171, 1219, 1823, 12, 2322, 12, 16792, 12391, 278, 11, 50852], "temperature": 0.0, "avg_logprob": -0.08476641371443465, "compression_ratio": 1.6628787878787878, "no_speech_prob": 0.4607222080230713}, {"id": 21, "seek": 10664, "start": 116.4, "end": 121.2, "text": " reflection, or finding its own errors, and I did an entire video on this called GPT-4", "tokens": [50852, 12914, 11, 420, 5006, 1080, 1065, 13603, 11, 293, 286, 630, 364, 2302, 960, 322, 341, 1219, 26039, 51, 12, 19, 51092], "temperature": 0.0, "avg_logprob": -0.08476641371443465, "compression_ratio": 1.6628787878787878, "no_speech_prob": 0.4607222080230713}, {"id": 22, "seek": 10664, "start": 121.2, "end": 126.88, "text": " Can Self Improve, and dialoguing with itself, entering into a back and forth on its own", "tokens": [51092, 1664, 16348, 46366, 11, 293, 19308, 9635, 365, 2564, 11, 11104, 666, 257, 646, 293, 5220, 322, 1080, 1065, 51376], "temperature": 0.0, "avg_logprob": -0.08476641371443465, "compression_ratio": 1.6628787878787878, "no_speech_prob": 0.4607222080230713}, {"id": 23, "seek": 10664, "start": 126.88, "end": 131.92000000000002, "text": " outputs and deciding which one is best. You can see the title of the papers, which contain much", "tokens": [51376, 23930, 293, 17990, 597, 472, 307, 1151, 13, 509, 393, 536, 264, 4876, 295, 264, 10577, 11, 597, 5304, 709, 51628], "temperature": 0.0, "avg_logprob": -0.08476641371443465, "compression_ratio": 1.6628787878787878, "no_speech_prob": 0.4607222080230713}, {"id": 24, "seek": 13192, "start": 131.92, "end": 137.04, "text": " more detailed results, of course, linked above. Now, the first paper only came out a few days ago,", "tokens": [50364, 544, 9942, 3542, 11, 295, 1164, 11, 9408, 3673, 13, 823, 11, 264, 700, 3035, 787, 1361, 484, 257, 1326, 1708, 2057, 11, 50620], "temperature": 0.0, "avg_logprob": -0.06089853649297036, "compression_ratio": 1.6313993174061434, "no_speech_prob": 0.12582358717918396}, {"id": 25, "seek": 13192, "start": 137.04, "end": 142.23999999999998, "text": " midway through my testing, so my results don't even reflect the full capacity of the model.", "tokens": [50620, 2062, 676, 807, 452, 4997, 11, 370, 452, 3542, 500, 380, 754, 5031, 264, 1577, 6042, 295, 264, 2316, 13, 50880], "temperature": 0.0, "avg_logprob": -0.06089853649297036, "compression_ratio": 1.6313993174061434, "no_speech_prob": 0.12582358717918396}, {"id": 26, "seek": 13192, "start": 142.23999999999998, "end": 147.92, "text": " And even if there's nothing else you take from this video, the results from this paper can instantly", "tokens": [50880, 400, 754, 498, 456, 311, 1825, 1646, 291, 747, 490, 341, 960, 11, 264, 3542, 490, 341, 3035, 393, 13518, 51164], "temperature": 0.0, "avg_logprob": -0.06089853649297036, "compression_ratio": 1.6313993174061434, "no_speech_prob": 0.12582358717918396}, {"id": 27, "seek": 13192, "start": 147.92, "end": 154.48, "text": " improve the outputs you get from GPT-4. Many of you might remember that prompting GPT-4 with", "tokens": [51164, 3470, 264, 23930, 291, 483, 490, 26039, 51, 12, 19, 13, 5126, 295, 291, 1062, 1604, 300, 12391, 278, 26039, 51, 12, 19, 365, 51492], "temperature": 0.0, "avg_logprob": -0.06089853649297036, "compression_ratio": 1.6313993174061434, "no_speech_prob": 0.12582358717918396}, {"id": 28, "seek": 13192, "start": 154.48, "end": 160.23999999999998, "text": " let's think step-by-step improves its results. To give you a very quick reference point, just", "tokens": [51492, 718, 311, 519, 1823, 12, 2322, 12, 16792, 24771, 1080, 3542, 13, 1407, 976, 291, 257, 588, 1702, 6408, 935, 11, 445, 51780], "temperature": 0.0, "avg_logprob": -0.06089853649297036, "compression_ratio": 1.6313993174061434, "no_speech_prob": 0.12582358717918396}, {"id": 29, "seek": 16024, "start": 160.24, "end": 166.72, "text": " asking a question to GPT-4 gives you 81% accuracy. With that prompt, let's think step-by-step,", "tokens": [50364, 3365, 257, 1168, 281, 26039, 51, 12, 19, 2709, 291, 30827, 4, 14170, 13, 2022, 300, 12391, 11, 718, 311, 519, 1823, 12, 2322, 12, 16792, 11, 50688], "temperature": 0.0, "avg_logprob": -0.05916529839191962, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.003944567404687405}, {"id": 30, "seek": 16024, "start": 166.72, "end": 173.52, "text": " it goes up to 86%. But algorithmically, the paper found an improved prompt that can give you even", "tokens": [50688, 309, 1709, 493, 281, 26687, 6856, 583, 9284, 984, 11, 264, 3035, 1352, 364, 9689, 12391, 300, 393, 976, 291, 754, 51028], "temperature": 0.0, "avg_logprob": -0.05916529839191962, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.003944567404687405}, {"id": 31, "seek": 16024, "start": 173.52, "end": 179.92000000000002, "text": " better results, 89% accuracy. All we do, and this is the first part of smart GPT, is we add", "tokens": [51028, 1101, 3542, 11, 31877, 4, 14170, 13, 1057, 321, 360, 11, 293, 341, 307, 264, 700, 644, 295, 4069, 26039, 51, 11, 307, 321, 909, 51348], "temperature": 0.0, "avg_logprob": -0.05916529839191962, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.003944567404687405}, {"id": 32, "seek": 16024, "start": 179.92000000000002, "end": 186.16000000000003, "text": " answer, let's work this out in a step-by-step way to be sure we have the right answer. Now,", "tokens": [51348, 1867, 11, 718, 311, 589, 341, 484, 294, 257, 1823, 12, 2322, 12, 16792, 636, 281, 312, 988, 321, 362, 264, 558, 1867, 13, 823, 11, 51660], "temperature": 0.0, "avg_logprob": -0.05916529839191962, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.003944567404687405}, {"id": 33, "seek": 18616, "start": 186.16, "end": 191.52, "text": " I have so much to say about why I think this works, but I know many of you won't be that", "tokens": [50364, 286, 362, 370, 709, 281, 584, 466, 983, 286, 519, 341, 1985, 11, 457, 286, 458, 867, 295, 291, 1582, 380, 312, 300, 50632], "temperature": 0.0, "avg_logprob": -0.0692290324790805, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.017435206100344658}, {"id": 34, "seek": 18616, "start": 191.52, "end": 195.84, "text": " interested in my theories, so I'm going to save them to the end for those who are interested.", "tokens": [50632, 3102, 294, 452, 13667, 11, 370, 286, 478, 516, 281, 3155, 552, 281, 264, 917, 337, 729, 567, 366, 3102, 13, 50848], "temperature": 0.0, "avg_logprob": -0.0692290324790805, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.017435206100344658}, {"id": 35, "seek": 18616, "start": 195.84, "end": 200.07999999999998, "text": " Some of you just want the results, so I'm going to get to those first. So far, you might be thinking,", "tokens": [50848, 2188, 295, 291, 445, 528, 264, 3542, 11, 370, 286, 478, 516, 281, 483, 281, 729, 700, 13, 407, 1400, 11, 291, 1062, 312, 1953, 11, 51060], "temperature": 0.0, "avg_logprob": -0.0692290324790805, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.017435206100344658}, {"id": 36, "seek": 18616, "start": 200.07999999999998, "end": 203.76, "text": " well, thanks, Philip, that's a cool prompt. I'm going to use that. But what's this whole smart", "tokens": [51060, 731, 11, 3231, 11, 21144, 11, 300, 311, 257, 1627, 12391, 13, 286, 478, 516, 281, 764, 300, 13, 583, 437, 311, 341, 1379, 4069, 51244], "temperature": 0.0, "avg_logprob": -0.0692290324790805, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.017435206100344658}, {"id": 37, "seek": 18616, "start": 203.76, "end": 209.92, "text": " GPT about? Is it just a single prompt? No, I believe with evidence, there are ways of leveraging", "tokens": [51244, 26039, 51, 466, 30, 1119, 309, 445, 257, 2167, 12391, 30, 883, 11, 286, 1697, 365, 4467, 11, 456, 366, 2098, 295, 32666, 51552], "temperature": 0.0, "avg_logprob": -0.0692290324790805, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.017435206100344658}, {"id": 38, "seek": 18616, "start": 209.92, "end": 215.51999999999998, "text": " even better results than just using a great chain of thought prompt. So let's move on to the next", "tokens": [51552, 754, 1101, 3542, 813, 445, 1228, 257, 869, 5021, 295, 1194, 12391, 13, 407, 718, 311, 1286, 322, 281, 264, 958, 51832], "temperature": 0.0, "avg_logprob": -0.0692290324790805, "compression_ratio": 1.7607361963190185, "no_speech_prob": 0.017435206100344658}, {"id": 39, "seek": 21552, "start": 215.52, "end": 220.0, "text": " part of the system, these different outputs in the middle. For my tests, I typically did three", "tokens": [50364, 644, 295, 264, 1185, 11, 613, 819, 23930, 294, 264, 2808, 13, 1171, 452, 6921, 11, 286, 5850, 630, 1045, 50588], "temperature": 0.0, "avg_logprob": -0.07198957845467288, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.017980379983782768}, {"id": 40, "seek": 21552, "start": 220.0, "end": 224.24, "text": " outputs, but of course, depending on the context window, it could be far more than that. And I'm", "tokens": [50588, 23930, 11, 457, 295, 1164, 11, 5413, 322, 264, 4319, 4910, 11, 309, 727, 312, 1400, 544, 813, 300, 13, 400, 286, 478, 50800], "temperature": 0.0, "avg_logprob": -0.07198957845467288, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.017980379983782768}, {"id": 41, "seek": 21552, "start": 224.24, "end": 229.44, "text": " going to talk about ways I could further improve this model, or we could later on in the video.", "tokens": [50800, 516, 281, 751, 466, 2098, 286, 727, 3052, 3470, 341, 2316, 11, 420, 321, 727, 1780, 322, 294, 264, 960, 13, 51060], "temperature": 0.0, "avg_logprob": -0.07198957845467288, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.017980379983782768}, {"id": 42, "seek": 21552, "start": 229.44, "end": 234.64000000000001, "text": " Just to restate, these outputs are when you take the user input and add the word question at start,", "tokens": [51060, 1449, 281, 1472, 473, 11, 613, 23930, 366, 562, 291, 747, 264, 4195, 4846, 293, 909, 264, 1349, 1168, 412, 722, 11, 51320], "temperature": 0.0, "avg_logprob": -0.07198957845467288, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.017980379983782768}, {"id": 43, "seek": 21552, "start": 234.64000000000001, "end": 239.04000000000002, "text": " and then at the end add answer, let's work this out in a step-by-step way to make sure we have", "tokens": [51320, 293, 550, 412, 264, 917, 909, 1867, 11, 718, 311, 589, 341, 484, 294, 257, 1823, 12, 2322, 12, 16792, 636, 281, 652, 988, 321, 362, 51540], "temperature": 0.0, "avg_logprob": -0.07198957845467288, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.017980379983782768}, {"id": 44, "seek": 21552, "start": 239.04000000000002, "end": 243.44, "text": " the right answer. And at this moment, many of you are thinking, what is the point of multiple", "tokens": [51540, 264, 558, 1867, 13, 400, 412, 341, 1623, 11, 867, 295, 291, 366, 1953, 11, 437, 307, 264, 935, 295, 3866, 51760], "temperature": 0.0, "avg_logprob": -0.07198957845467288, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.017980379983782768}, {"id": 45, "seek": 24344, "start": 243.44, "end": 247.68, "text": " outputs? It's GPT-4, it's just going to give you the answer that thinks is best, and that's it.", "tokens": [50364, 23930, 30, 467, 311, 26039, 51, 12, 19, 11, 309, 311, 445, 516, 281, 976, 291, 264, 1867, 300, 7309, 307, 1151, 11, 293, 300, 311, 309, 13, 50576], "temperature": 0.0, "avg_logprob": -0.0769590092456247, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.02032824046909809}, {"id": 46, "seek": 24344, "start": 247.68, "end": 252.16, "text": " Well, actually, it doesn't quite work like that. These models have a temperature between zero and", "tokens": [50576, 1042, 11, 767, 11, 309, 1177, 380, 1596, 589, 411, 300, 13, 1981, 5245, 362, 257, 4292, 1296, 4018, 293, 50800], "temperature": 0.0, "avg_logprob": -0.0769590092456247, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.02032824046909809}, {"id": 47, "seek": 24344, "start": 252.16, "end": 258.48, "text": " one. I believe the default for GPT-4 might be around 0.5. And simplifying massively, this determines", "tokens": [50800, 472, 13, 286, 1697, 264, 7576, 337, 26039, 51, 12, 19, 1062, 312, 926, 1958, 13, 20, 13, 400, 6883, 5489, 29379, 11, 341, 24799, 51116], "temperature": 0.0, "avg_logprob": -0.0769590092456247, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.02032824046909809}, {"id": 48, "seek": 24344, "start": 258.48, "end": 264.72, "text": " how creative or conservative the model is in giving its outputs. So given that GPT-4 tries to be", "tokens": [51116, 577, 5880, 420, 13780, 264, 2316, 307, 294, 2902, 1080, 23930, 13, 407, 2212, 300, 26039, 51, 12, 19, 9898, 281, 312, 51428], "temperature": 0.0, "avg_logprob": -0.0769590092456247, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.02032824046909809}, {"id": 49, "seek": 24344, "start": 264.72, "end": 270.4, "text": " fairly creative, you don't get the same output every time. The output is randomly sampled according", "tokens": [51428, 6457, 5880, 11, 291, 500, 380, 483, 264, 912, 5598, 633, 565, 13, 440, 5598, 307, 16979, 3247, 15551, 4650, 51712], "temperature": 0.0, "avg_logprob": -0.0769590092456247, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.02032824046909809}, {"id": 50, "seek": 27040, "start": 270.4, "end": 275.52, "text": " to an internal probability distribution. So you can get situations, and I face this hundreds of", "tokens": [50364, 281, 364, 6920, 8482, 7316, 13, 407, 291, 393, 483, 6851, 11, 293, 286, 1851, 341, 6779, 295, 50620], "temperature": 0.0, "avg_logprob": -0.07129821469706873, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.017975561320781708}, {"id": 51, "seek": 27040, "start": 275.52, "end": 281.28, "text": " times, where some of the outputs are correct, and others are incorrect. And this is where reflection", "tokens": [50620, 1413, 11, 689, 512, 295, 264, 23930, 366, 3006, 11, 293, 2357, 366, 18424, 13, 400, 341, 307, 689, 12914, 50908], "temperature": 0.0, "avg_logprob": -0.07129821469706873, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.017975561320781708}, {"id": 52, "seek": 27040, "start": 281.28, "end": 288.32, "text": " comes in. Sometimes, definitely not always, but sometimes quite often, GPT-4 can detect the errors", "tokens": [50908, 1487, 294, 13, 4803, 11, 2138, 406, 1009, 11, 457, 2171, 1596, 2049, 11, 26039, 51, 12, 19, 393, 5531, 264, 13603, 51260], "temperature": 0.0, "avg_logprob": -0.07129821469706873, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.017975561320781708}, {"id": 53, "seek": 27040, "start": 288.32, "end": 294.0, "text": " in its own output. And many of you will notice at this point that the prompt that I used to elicit", "tokens": [51260, 294, 1080, 1065, 5598, 13, 400, 867, 295, 291, 486, 3449, 412, 341, 935, 300, 264, 12391, 300, 286, 1143, 281, 806, 8876, 51544], "temperature": 0.0, "avg_logprob": -0.07129821469706873, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.017975561320781708}, {"id": 54, "seek": 29400, "start": 294.0, "end": 301.52, "text": " GPT-4 to spot its own errors contains the same step-by-step prompt I used earlier that has been", "tokens": [50364, 26039, 51, 12, 19, 281, 4008, 1080, 1065, 13603, 8306, 264, 912, 1823, 12, 2322, 12, 16792, 12391, 286, 1143, 3071, 300, 575, 668, 50740], "temperature": 0.0, "avg_logprob": -0.08558995652906966, "compression_ratio": 1.55078125, "no_speech_prob": 0.03113766573369503}, {"id": 55, "seek": 29400, "start": 301.52, "end": 308.72, "text": " shown to produce good results. So to summarize, sometimes at this stage, GPT-4 detects the errors", "tokens": [50740, 4898, 281, 5258, 665, 3542, 13, 407, 281, 20858, 11, 2171, 412, 341, 3233, 11, 26039, 51, 12, 19, 5531, 82, 264, 13603, 51100], "temperature": 0.0, "avg_logprob": -0.08558995652906966, "compression_ratio": 1.55078125, "no_speech_prob": 0.03113766573369503}, {"id": 56, "seek": 29400, "start": 308.72, "end": 313.76, "text": " that some of its outputs have made. Definitely not always. There are certain questions, it just simply", "tokens": [51100, 300, 512, 295, 1080, 23930, 362, 1027, 13, 12151, 406, 1009, 13, 821, 366, 1629, 1651, 11, 309, 445, 2935, 51352], "temperature": 0.0, "avg_logprob": -0.08558995652906966, "compression_ratio": 1.55078125, "no_speech_prob": 0.03113766573369503}, {"id": 57, "seek": 29400, "start": 313.76, "end": 319.6, "text": " can't spot the error. But sometimes it can. And then I get it to engage in a dialogue using a format", "tokens": [51352, 393, 380, 4008, 264, 6713, 13, 583, 2171, 309, 393, 13, 400, 550, 286, 483, 309, 281, 4683, 294, 257, 10221, 1228, 257, 7877, 51644], "temperature": 0.0, "avg_logprob": -0.08558995652906966, "compression_ratio": 1.55078125, "no_speech_prob": 0.03113766573369503}, {"id": 58, "seek": 31960, "start": 319.6, "end": 324.48, "text": " similar to one in this paper published last month. It's a short dialogue, and this is the", "tokens": [50364, 2531, 281, 472, 294, 341, 3035, 6572, 1036, 1618, 13, 467, 311, 257, 2099, 10221, 11, 293, 341, 307, 264, 50608], "temperature": 0.0, "avg_logprob": -0.06674497267779182, "compression_ratio": 1.582236842105263, "no_speech_prob": 0.10664121061563492}, {"id": 59, "seek": 31960, "start": 324.48, "end": 330.88, "text": " step I believe that can be most optimized. In the future, I envision an entire council of advisors", "tokens": [50608, 1823, 286, 1697, 300, 393, 312, 881, 26941, 13, 682, 264, 2027, 11, 286, 24739, 364, 2302, 9209, 295, 29136, 50928], "temperature": 0.0, "avg_logprob": -0.06674497267779182, "compression_ratio": 1.582236842105263, "no_speech_prob": 0.10664121061563492}, {"id": 60, "seek": 31960, "start": 330.88, "end": 337.84000000000003, "text": " made up of GPT-4 imitating mathematicians, judges, etc. At the moment, it's just being a resolver", "tokens": [50928, 1027, 493, 295, 26039, 51, 12, 19, 566, 16350, 32811, 2567, 11, 14449, 11, 5183, 13, 1711, 264, 1623, 11, 309, 311, 445, 885, 257, 34480, 51276], "temperature": 0.0, "avg_logprob": -0.06674497267779182, "compression_ratio": 1.582236842105263, "no_speech_prob": 0.10664121061563492}, {"id": 61, "seek": 31960, "start": 337.84000000000003, "end": 342.88, "text": " and printing a final improved output. Anyway, I'm going to get back to the theory later in the video", "tokens": [51276, 293, 14699, 257, 2572, 9689, 5598, 13, 5684, 11, 286, 478, 516, 281, 483, 646, 281, 264, 5261, 1780, 294, 264, 960, 51528], "temperature": 0.0, "avg_logprob": -0.06674497267779182, "compression_ratio": 1.582236842105263, "no_speech_prob": 0.10664121061563492}, {"id": 62, "seek": 31960, "start": 342.88, "end": 346.72, "text": " because I know some of you will be getting bored at this stage and want to see more practical", "tokens": [51528, 570, 286, 458, 512, 295, 291, 486, 312, 1242, 13521, 412, 341, 3233, 293, 528, 281, 536, 544, 8496, 51720], "temperature": 0.0, "avg_logprob": -0.06674497267779182, "compression_ratio": 1.582236842105263, "no_speech_prob": 0.10664121061563492}, {"id": 63, "seek": 34672, "start": 346.72, "end": 353.44000000000005, "text": " examples and the results from my benchmark tests. As I don't have the GPT-4 API key, yes, I had to", "tokens": [50364, 5110, 293, 264, 3542, 490, 452, 18927, 6921, 13, 1018, 286, 500, 380, 362, 264, 26039, 51, 12, 19, 9362, 2141, 11, 2086, 11, 286, 632, 281, 50700], "temperature": 0.0, "avg_logprob": -0.0670418698908919, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.009410127997398376}, {"id": 64, "seek": 34672, "start": 353.44000000000005, "end": 359.44000000000005, "text": " manually input each of these steps hundreds of times, waiting sometimes three hours between each", "tokens": [50700, 16945, 4846, 1184, 295, 613, 4439, 6779, 295, 1413, 11, 3806, 2171, 1045, 2496, 1296, 1184, 51000], "temperature": 0.0, "avg_logprob": -0.0670418698908919, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.009410127997398376}, {"id": 65, "seek": 34672, "start": 359.44000000000005, "end": 364.64000000000004, "text": " go because you can only do 25 messages every three hours. On the left, you can see the three", "tokens": [51000, 352, 570, 291, 393, 787, 360, 3552, 7897, 633, 1045, 2496, 13, 1282, 264, 1411, 11, 291, 393, 536, 264, 1045, 51260], "temperature": 0.0, "avg_logprob": -0.0670418698908919, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.009410127997398376}, {"id": 66, "seek": 34672, "start": 364.64000000000004, "end": 370.16, "text": " outputs when you ask it to think step by step. And then you have the researcher step in the middle", "tokens": [51260, 23930, 562, 291, 1029, 309, 281, 519, 1823, 538, 1823, 13, 400, 550, 291, 362, 264, 21751, 1823, 294, 264, 2808, 51536], "temperature": 0.0, "avg_logprob": -0.0670418698908919, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.009410127997398376}, {"id": 67, "seek": 34672, "start": 370.16, "end": 374.8, "text": " and at the top right. And finally, the resolver step. Notice here, I was using the original", "tokens": [51536, 293, 412, 264, 1192, 558, 13, 400, 2721, 11, 264, 34480, 1823, 13, 13428, 510, 11, 286, 390, 1228, 264, 3380, 51768], "temperature": 0.0, "avg_logprob": -0.0670418698908919, "compression_ratio": 1.6689895470383276, "no_speech_prob": 0.009410127997398376}, {"id": 68, "seek": 37480, "start": 374.8, "end": 379.68, "text": " let's think step by step because the paper hadn't yet been published on improving that prompt.", "tokens": [50364, 718, 311, 519, 1823, 538, 1823, 570, 264, 3035, 8782, 380, 1939, 668, 6572, 322, 11470, 300, 12391, 13, 50608], "temperature": 0.0, "avg_logprob": -0.10405645214143347, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.0016226128209382296}, {"id": 69, "seek": 37480, "start": 379.68, "end": 384.56, "text": " It's time for the second example from that TED Talk, and then I definitely will get on to the", "tokens": [50608, 467, 311, 565, 337, 264, 1150, 1365, 490, 300, 43036, 8780, 11, 293, 550, 286, 2138, 486, 483, 322, 281, 264, 50852], "temperature": 0.0, "avg_logprob": -0.10405645214143347, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.0016226128209382296}, {"id": 70, "seek": 37480, "start": 384.56, "end": 390.96000000000004, "text": " benchmarks. A different one. I have 12 liter jug and 6 liter jug, and I want to measure 6 liters.", "tokens": [50852, 43751, 13, 316, 819, 472, 13, 286, 362, 2272, 2733, 9568, 293, 1386, 2733, 9568, 11, 293, 286, 528, 281, 3481, 1386, 32323, 13, 51172], "temperature": 0.0, "avg_logprob": -0.10405645214143347, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.0016226128209382296}, {"id": 71, "seek": 37480, "start": 390.96000000000004, "end": 398.24, "text": " How do I do it? Just use the 6 liter jug, right? GPT-4 spits out some very elaborate nonsense.", "tokens": [51172, 1012, 360, 286, 360, 309, 30, 1449, 764, 264, 1386, 2733, 9568, 11, 558, 30, 26039, 51, 12, 19, 637, 1208, 484, 512, 588, 20945, 14925, 13, 51536], "temperature": 0.0, "avg_logprob": -0.10405645214143347, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.0016226128209382296}, {"id": 72, "seek": 37480, "start": 400.40000000000003, "end": 404.56, "text": " Of course, I tested smart GPT with that question, and you can see the difference between", "tokens": [51644, 2720, 1164, 11, 286, 8246, 4069, 26039, 51, 365, 300, 1168, 11, 293, 291, 393, 536, 264, 2649, 1296, 51852], "temperature": 0.0, "avg_logprob": -0.10405645214143347, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.0016226128209382296}, {"id": 73, "seek": 40456, "start": 404.56, "end": 410.72, "text": " the original GPT-4, which gives this incredibly convoluted bad answer, and smart GPT, the final", "tokens": [50364, 264, 3380, 26039, 51, 12, 19, 11, 597, 2709, 341, 6252, 3754, 2308, 292, 1578, 1867, 11, 293, 4069, 26039, 51, 11, 264, 2572, 50672], "temperature": 0.0, "avg_logprob": -0.07837252055897433, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.003482623491436243}, {"id": 74, "seek": 40456, "start": 410.72, "end": 414.56, "text": " answer output. Now, at this point, I know many of you will be impressed, but you'll be thinking,", "tokens": [50672, 1867, 5598, 13, 823, 11, 412, 341, 935, 11, 286, 458, 867, 295, 291, 486, 312, 11679, 11, 457, 291, 603, 312, 1953, 11, 50864], "temperature": 0.0, "avg_logprob": -0.07837252055897433, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.003482623491436243}, {"id": 75, "seek": 40456, "start": 414.56, "end": 419.92, "text": " I don't have time to input things five times. Well, I'm developing a model where it can all", "tokens": [50864, 286, 500, 380, 362, 565, 281, 4846, 721, 1732, 1413, 13, 1042, 11, 286, 478, 6416, 257, 2316, 689, 309, 393, 439, 51132], "temperature": 0.0, "avg_logprob": -0.07837252055897433, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.003482623491436243}, {"id": 76, "seek": 40456, "start": 419.92, "end": 424.72, "text": " be done automatically. Here is a preview of how it works. But of course, at the moment, it has to", "tokens": [51132, 312, 1096, 6772, 13, 1692, 307, 257, 14281, 295, 577, 309, 1985, 13, 583, 295, 1164, 11, 412, 264, 1623, 11, 309, 575, 281, 51372], "temperature": 0.0, "avg_logprob": -0.07837252055897433, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.003482623491436243}, {"id": 77, "seek": 40456, "start": 424.72, "end": 431.28, "text": " use GPT 3.5 Turbo because I don't have the API key of GPT-4. But the epic thing is this, you just", "tokens": [51372, 764, 26039, 51, 805, 13, 20, 35848, 570, 286, 500, 380, 362, 264, 9362, 2141, 295, 26039, 51, 12, 19, 13, 583, 264, 13581, 551, 307, 341, 11, 291, 445, 51700], "temperature": 0.0, "avg_logprob": -0.07837252055897433, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.003482623491436243}, {"id": 78, "seek": 43128, "start": 431.28, "end": 436.32, "text": " ask a single question, I've written, ask smart GPT-A question. And of course, it does take a", "tokens": [50364, 1029, 257, 2167, 1168, 11, 286, 600, 3720, 11, 1029, 4069, 26039, 51, 12, 32, 1168, 13, 400, 295, 1164, 11, 309, 775, 747, 257, 50616], "temperature": 0.0, "avg_logprob": -0.08358612210731807, "compression_ratio": 1.6406779661016948, "no_speech_prob": 0.035133861005306244}, {"id": 79, "seek": 43128, "start": 436.32, "end": 441.44, "text": " little bit longer to respond because it's doing five or six calls via API, but it does output the", "tokens": [50616, 707, 857, 2854, 281, 4196, 570, 309, 311, 884, 1732, 420, 2309, 5498, 5766, 9362, 11, 457, 309, 775, 5598, 264, 50872], "temperature": 0.0, "avg_logprob": -0.08358612210731807, "compression_ratio": 1.6406779661016948, "no_speech_prob": 0.035133861005306244}, {"id": 80, "seek": 43128, "start": 441.44, "end": 447.2, "text": " final answer from the resolver step. I will be honest and say that GPT 3.5 isn't as good at", "tokens": [50872, 2572, 1867, 490, 264, 34480, 1823, 13, 286, 486, 312, 3245, 293, 584, 300, 26039, 51, 805, 13, 20, 1943, 380, 382, 665, 412, 51160], "temperature": 0.0, "avg_logprob": -0.08358612210731807, "compression_ratio": 1.6406779661016948, "no_speech_prob": 0.035133861005306244}, {"id": 81, "seek": 43128, "start": 447.2, "end": 452.71999999999997, "text": " reflecting or resolving. But this is an example of a question where the original chat GPT consistently", "tokens": [51160, 23543, 420, 49940, 13, 583, 341, 307, 364, 1365, 295, 257, 1168, 689, 264, 3380, 5081, 26039, 51, 14961, 51436], "temperature": 0.0, "avg_logprob": -0.08358612210731807, "compression_ratio": 1.6406779661016948, "no_speech_prob": 0.035133861005306244}, {"id": 82, "seek": 43128, "start": 452.71999999999997, "end": 459.44, "text": " gets it wrong, and smart GPT 3.5 gets it right using this program. Remember, all you have to do as", "tokens": [51436, 2170, 309, 2085, 11, 293, 4069, 26039, 51, 805, 13, 20, 2170, 309, 558, 1228, 341, 1461, 13, 5459, 11, 439, 291, 362, 281, 360, 382, 51772], "temperature": 0.0, "avg_logprob": -0.08358612210731807, "compression_ratio": 1.6406779661016948, "no_speech_prob": 0.035133861005306244}, {"id": 83, "seek": 45944, "start": 459.44, "end": 465.2, "text": " a user is type in a question as normal, and it goes through this entire five or six step process", "tokens": [50364, 257, 4195, 307, 2010, 294, 257, 1168, 382, 2710, 11, 293, 309, 1709, 807, 341, 2302, 1732, 420, 2309, 1823, 1399, 50652], "temperature": 0.0, "avg_logprob": -0.05306724896506658, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.012426969595253468}, {"id": 84, "seek": 45944, "start": 465.2, "end": 470.32, "text": " behind the scenes. By the way, this was a question from MMLU, which is a famous benchmark which I'll", "tokens": [50652, 2261, 264, 8026, 13, 3146, 264, 636, 11, 341, 390, 257, 1168, 490, 376, 12683, 52, 11, 597, 307, 257, 4618, 18927, 597, 286, 603, 50908], "temperature": 0.0, "avg_logprob": -0.05306724896506658, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.012426969595253468}, {"id": 85, "seek": 45944, "start": 470.32, "end": 475.2, "text": " get to in a second. Here's one last practical example before I get to that benchmark. I know many", "tokens": [50908, 483, 281, 294, 257, 1150, 13, 1692, 311, 472, 1036, 8496, 1365, 949, 286, 483, 281, 300, 18927, 13, 286, 458, 867, 51152], "temperature": 0.0, "avg_logprob": -0.05306724896506658, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.012426969595253468}, {"id": 86, "seek": 45944, "start": 475.2, "end": 481.2, "text": " teachers use chat GPT and GPT-4 to create quizzes for their classes. And here is the same question", "tokens": [51152, 6023, 764, 5081, 26039, 51, 293, 26039, 51, 12, 19, 281, 1884, 48955, 337, 641, 5359, 13, 400, 510, 307, 264, 912, 1168, 51452], "temperature": 0.0, "avg_logprob": -0.05306724896506658, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.012426969595253468}, {"id": 87, "seek": 45944, "start": 481.2, "end": 486.72, "text": " put through GPT-4 and smart GPT. The question is, create a high school algebra quiz with five", "tokens": [51452, 829, 807, 26039, 51, 12, 19, 293, 4069, 26039, 51, 13, 440, 1168, 307, 11, 1884, 257, 1090, 1395, 21989, 15450, 365, 1732, 51728], "temperature": 0.0, "avg_logprob": -0.05306724896506658, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.012426969595253468}, {"id": 88, "seek": 48672, "start": 486.72, "end": 491.44000000000005, "text": " questions and answers and explanations at the end. Now points for spotting the difference,", "tokens": [50364, 1651, 293, 6338, 293, 28708, 412, 264, 917, 13, 823, 2793, 337, 4008, 783, 264, 2649, 11, 50600], "temperature": 0.0, "avg_logprob": -0.08463881696973528, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.028425754979252815}, {"id": 89, "seek": 48672, "start": 491.44000000000005, "end": 496.64000000000004, "text": " but if the teacher had handed out the original quiz, look at the answers for question five.", "tokens": [50600, 457, 498, 264, 5027, 632, 16013, 484, 264, 3380, 15450, 11, 574, 412, 264, 6338, 337, 1168, 1732, 13, 50860], "temperature": 0.0, "avg_logprob": -0.08463881696973528, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.028425754979252815}, {"id": 90, "seek": 48672, "start": 496.64000000000004, "end": 502.88000000000005, "text": " It says the answers are one and 1.5. But then in the explanation, it gives the final answers,", "tokens": [50860, 467, 1619, 264, 6338, 366, 472, 293, 502, 13, 20, 13, 583, 550, 294, 264, 10835, 11, 309, 2709, 264, 2572, 6338, 11, 51172], "temperature": 0.0, "avg_logprob": -0.08463881696973528, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.028425754979252815}, {"id": 91, "seek": 48672, "start": 502.88000000000005, "end": 508.08000000000004, "text": " which are correct by the way, of three and zero point five. So that would really confuse some", "tokens": [51172, 597, 366, 3006, 538, 264, 636, 11, 295, 1045, 293, 4018, 935, 1732, 13, 407, 300, 576, 534, 28584, 512, 51432], "temperature": 0.0, "avg_logprob": -0.08463881696973528, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.028425754979252815}, {"id": 92, "seek": 48672, "start": 508.08000000000004, "end": 513.76, "text": " students at the reflection stage smart GPT spotted that error and resolved it. And as you can see,", "tokens": [51432, 1731, 412, 264, 12914, 3233, 4069, 26039, 51, 21010, 300, 6713, 293, 20772, 309, 13, 400, 382, 291, 393, 536, 11, 51716], "temperature": 0.0, "avg_logprob": -0.08463881696973528, "compression_ratio": 1.6992753623188406, "no_speech_prob": 0.028425754979252815}, {"id": 93, "seek": 51376, "start": 513.76, "end": 518.56, "text": " the answer for question five has the correct answers straight away. If at any point you're", "tokens": [50364, 264, 1867, 337, 1168, 1732, 575, 264, 3006, 6338, 2997, 1314, 13, 759, 412, 604, 935, 291, 434, 50604], "temperature": 0.0, "avg_logprob": -0.06522291843022142, "compression_ratio": 1.57439446366782, "no_speech_prob": 0.01032585185021162}, {"id": 94, "seek": 51376, "start": 518.56, "end": 523.68, "text": " wondering if I completed the open AI chat GPT prompt engineering course, the answer is yes,", "tokens": [50604, 6359, 498, 286, 7365, 264, 1269, 7318, 5081, 26039, 51, 12391, 7043, 1164, 11, 264, 1867, 307, 2086, 11, 50860], "temperature": 0.0, "avg_logprob": -0.06522291843022142, "compression_ratio": 1.57439446366782, "no_speech_prob": 0.01032585185021162}, {"id": 95, "seek": 51376, "start": 523.68, "end": 528.16, "text": " but it didn't inform too much of my thinking. It was more for beginners and I had already", "tokens": [50860, 457, 309, 994, 380, 1356, 886, 709, 295, 452, 1953, 13, 467, 390, 544, 337, 26992, 293, 286, 632, 1217, 51084], "temperature": 0.0, "avg_logprob": -0.06522291843022142, "compression_ratio": 1.57439446366782, "no_speech_prob": 0.01032585185021162}, {"id": 96, "seek": 51376, "start": 528.16, "end": 533.04, "text": " factored in things like giving the model time to think and writing clear instructions. The", "tokens": [51084, 1186, 2769, 294, 721, 411, 2902, 264, 2316, 565, 281, 519, 293, 3579, 1850, 9415, 13, 440, 51328], "temperature": 0.0, "avg_logprob": -0.06522291843022142, "compression_ratio": 1.57439446366782, "no_speech_prob": 0.01032585185021162}, {"id": 97, "seek": 51376, "start": 533.04, "end": 540.24, "text": " benchmark that I chose to test smart GPT on was the famous MMLU, massive multitask language", "tokens": [51328, 18927, 300, 286, 5111, 281, 1500, 4069, 26039, 51, 322, 390, 264, 4618, 376, 12683, 52, 11, 5994, 42338, 3863, 2856, 51688], "temperature": 0.0, "avg_logprob": -0.06522291843022142, "compression_ratio": 1.57439446366782, "no_speech_prob": 0.01032585185021162}, {"id": 98, "seek": 54024, "start": 540.24, "end": 546.88, "text": " understanding benchmark. As you can see, the state of the art is indeed GPT for with 86.4%", "tokens": [50364, 3701, 18927, 13, 1018, 291, 393, 536, 11, 264, 1785, 295, 264, 1523, 307, 6451, 26039, 51, 337, 365, 26687, 13, 19, 4, 50696], "temperature": 0.0, "avg_logprob": -0.09791778115665212, "compression_ratio": 1.5986159169550174, "no_speech_prob": 0.024417443200945854}, {"id": 99, "seek": 54024, "start": 546.88, "end": 551.84, "text": " accuracy. And you know, open AI think it's a big deal because it's the benchmark mentioned on the", "tokens": [50696, 14170, 13, 400, 291, 458, 11, 1269, 7318, 519, 309, 311, 257, 955, 2028, 570, 309, 311, 264, 18927, 2835, 322, 264, 50944], "temperature": 0.0, "avg_logprob": -0.09791778115665212, "compression_ratio": 1.5986159169550174, "no_speech_prob": 0.024417443200945854}, {"id": 100, "seek": 54024, "start": 551.84, "end": 556.48, "text": " front page of their technical report without boring you too much. I extracted the questions", "tokens": [50944, 1868, 3028, 295, 641, 6191, 2275, 1553, 9989, 291, 886, 709, 13, 286, 34086, 264, 1651, 51176], "temperature": 0.0, "avg_logprob": -0.09791778115665212, "compression_ratio": 1.5986159169550174, "no_speech_prob": 0.024417443200945854}, {"id": 101, "seek": 54024, "start": 556.48, "end": 563.52, "text": " from the test set of the MMLU data file, and I didn't pick the topics at random. I went for", "tokens": [51176, 490, 264, 1500, 992, 295, 264, 376, 12683, 52, 1412, 3991, 11, 293, 286, 994, 380, 1888, 264, 8378, 412, 4974, 13, 286, 1437, 337, 51528], "temperature": 0.0, "avg_logprob": -0.09791778115665212, "compression_ratio": 1.5986159169550174, "no_speech_prob": 0.024417443200945854}, {"id": 102, "seek": 54024, "start": 563.52, "end": 569.44, "text": " those that I thought GPT for would find the hardest delving into the original MMLU paper.", "tokens": [51528, 729, 300, 286, 1194, 26039, 51, 337, 576, 915, 264, 13158, 1103, 798, 666, 264, 3380, 376, 12683, 52, 3035, 13, 51824], "temperature": 0.0, "avg_logprob": -0.09791778115665212, "compression_ratio": 1.5986159169550174, "no_speech_prob": 0.024417443200945854}, {"id": 103, "seek": 56944, "start": 569.5200000000001, "end": 577.2800000000001, "text": " You can see that GPT three found for more logic the hardest scoring just over 25% which is random", "tokens": [50368, 509, 393, 536, 300, 26039, 51, 1045, 1352, 337, 544, 9952, 264, 13158, 22358, 445, 670, 3552, 4, 597, 307, 4974, 50756], "temperature": 0.0, "avg_logprob": -0.11721249015963807, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.012050160206854343}, {"id": 104, "seek": 56944, "start": 577.2800000000001, "end": 584.08, "text": " chance. It's a four question multiple choice test. So around 25 or 30% is pretty bad. And notice", "tokens": [50756, 2931, 13, 467, 311, 257, 1451, 1168, 3866, 3922, 1500, 13, 407, 926, 3552, 420, 2217, 4, 307, 1238, 1578, 13, 400, 3449, 51096], "temperature": 0.0, "avg_logprob": -0.11721249015963807, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.012050160206854343}, {"id": 105, "seek": 56944, "start": 584.08, "end": 590.8000000000001, "text": " they helped out GPT three here. They did it few shot, meaning they gave it five successful examples", "tokens": [51096, 436, 4254, 484, 26039, 51, 1045, 510, 13, 814, 630, 309, 1326, 3347, 11, 3620, 436, 2729, 309, 1732, 4406, 5110, 51432], "temperature": 0.0, "avg_logprob": -0.11721249015963807, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.012050160206854343}, {"id": 106, "seek": 56944, "start": 590.8000000000001, "end": 595.9200000000001, "text": " before asking it a new question. It's the same thing they did with GPT four. They did it five", "tokens": [51432, 949, 3365, 309, 257, 777, 1168, 13, 467, 311, 264, 912, 551, 436, 630, 365, 26039, 51, 1451, 13, 814, 630, 309, 1732, 51688], "temperature": 0.0, "avg_logprob": -0.11721249015963807, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.012050160206854343}, {"id": 107, "seek": 59592, "start": 595.92, "end": 599.76, "text": " shot. But just before I show you the results, there are three things I want to mention here.", "tokens": [50364, 3347, 13, 583, 445, 949, 286, 855, 291, 264, 3542, 11, 456, 366, 1045, 721, 286, 528, 281, 2152, 510, 13, 50556], "temperature": 0.0, "avg_logprob": -0.08096708100417564, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.06951893866062164}, {"id": 108, "seek": 59592, "start": 599.76, "end": 605.8399999999999, "text": " First, I was curious how smart GPT would do without any help zero shot. Second, I wanted to do it", "tokens": [50556, 2386, 11, 286, 390, 6369, 577, 4069, 26039, 51, 576, 360, 1553, 604, 854, 4018, 3347, 13, 5736, 11, 286, 1415, 281, 360, 309, 50860], "temperature": 0.0, "avg_logprob": -0.08096708100417564, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.06951893866062164}, {"id": 109, "seek": 59592, "start": 605.8399999999999, "end": 612.4799999999999, "text": " zero shot because people using GPT four don't typically give five successful examples before", "tokens": [50860, 4018, 3347, 570, 561, 1228, 26039, 51, 1451, 500, 380, 5850, 976, 1732, 4406, 5110, 949, 51192], "temperature": 0.0, "avg_logprob": -0.08096708100417564, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.06951893866062164}, {"id": 110, "seek": 59592, "start": 612.4799999999999, "end": 618.3199999999999, "text": " asking GPT for a question. They just want code or a quiz or a poem or an example. They don't often", "tokens": [51192, 3365, 26039, 51, 337, 257, 1168, 13, 814, 445, 528, 3089, 420, 257, 15450, 420, 257, 13065, 420, 364, 1365, 13, 814, 500, 380, 2049, 51484], "temperature": 0.0, "avg_logprob": -0.08096708100417564, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.06951893866062164}, {"id": 111, "seek": 59592, "start": 618.3199999999999, "end": 623.28, "text": " provide five brilliant examples of code before asking their question. And third, if I can prove", "tokens": [51484, 2893, 1732, 10248, 5110, 295, 3089, 949, 3365, 641, 1168, 13, 400, 2636, 11, 498, 286, 393, 7081, 51732], "temperature": 0.0, "avg_logprob": -0.08096708100417564, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.06951893866062164}, {"id": 112, "seek": 62328, "start": 623.28, "end": 628.48, "text": " it works zero shot, then of course, future refinements can be made to push the results even", "tokens": [50364, 309, 1985, 4018, 3347, 11, 550, 295, 1164, 11, 2027, 44395, 6400, 393, 312, 1027, 281, 2944, 264, 3542, 754, 50624], "temperature": 0.0, "avg_logprob": -0.08670188903808594, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.008060653693974018}, {"id": 113, "seek": 62328, "start": 628.48, "end": 635.12, "text": " further. And here are the results from the first 25 questions from the formal logic test set of the", "tokens": [50624, 3052, 13, 400, 510, 366, 264, 3542, 490, 264, 700, 3552, 1651, 490, 264, 9860, 9952, 1500, 992, 295, 264, 50956], "temperature": 0.0, "avg_logprob": -0.08670188903808594, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.008060653693974018}, {"id": 114, "seek": 62328, "start": 635.12, "end": 641.12, "text": " MMLU. I did many more tests after this. But you can see from this set, if you just ask the question,", "tokens": [50956, 376, 12683, 52, 13, 286, 630, 867, 544, 6921, 934, 341, 13, 583, 291, 393, 536, 490, 341, 992, 11, 498, 291, 445, 1029, 264, 1168, 11, 51256], "temperature": 0.0, "avg_logprob": -0.08670188903808594, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.008060653693974018}, {"id": 115, "seek": 62328, "start": 641.12, "end": 647.8399999999999, "text": " you get a lower overall accuracy. But of course, 68% for GPT four is still a huge improvement over", "tokens": [51256, 291, 483, 257, 3126, 4787, 14170, 13, 583, 295, 1164, 11, 23317, 4, 337, 26039, 51, 1451, 307, 920, 257, 2603, 10444, 670, 51592], "temperature": 0.0, "avg_logprob": -0.08670188903808594, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.008060653693974018}, {"id": 116, "seek": 64784, "start": 647.84, "end": 654.48, "text": " GPT threes around 25%. What happens when you add let's think step by step, which as we know now", "tokens": [50364, 26039, 51, 258, 4856, 926, 3552, 6856, 708, 2314, 562, 291, 909, 718, 311, 519, 1823, 538, 1823, 11, 597, 382, 321, 458, 586, 50696], "temperature": 0.0, "avg_logprob": -0.10286269585291545, "compression_ratio": 1.46875, "no_speech_prob": 0.05339311435818672}, {"id": 117, "seek": 64784, "start": 654.48, "end": 660.96, "text": " isn't the fully optimized chain of thought prompt. Well, on average, you get around 74-75%.", "tokens": [50696, 1943, 380, 264, 4498, 26941, 5021, 295, 1194, 12391, 13, 1042, 11, 322, 4274, 11, 291, 483, 926, 28868, 12, 11901, 6856, 51020], "temperature": 0.0, "avg_logprob": -0.10286269585291545, "compression_ratio": 1.46875, "no_speech_prob": 0.05339311435818672}, {"id": 118, "seek": 64784, "start": 661.44, "end": 667.2800000000001, "text": " That was 75 examples inputted manually. And I still have all the tabs open. I'm keeping them open", "tokens": [51044, 663, 390, 9562, 5110, 4846, 14727, 16945, 13, 400, 286, 920, 362, 439, 264, 20743, 1269, 13, 286, 478, 5145, 552, 1269, 51336], "temperature": 0.0, "avg_logprob": -0.10286269585291545, "compression_ratio": 1.46875, "no_speech_prob": 0.05339311435818672}, {"id": 119, "seek": 64784, "start": 667.2800000000001, "end": 672.08, "text": " because I'm compiling a spreadsheet with the actual outputs. But what did the resolver get", "tokens": [51336, 570, 286, 478, 715, 4883, 257, 27733, 365, 264, 3539, 23930, 13, 583, 437, 630, 264, 34480, 483, 51576], "temperature": 0.0, "avg_logprob": -0.10286269585291545, "compression_ratio": 1.46875, "no_speech_prob": 0.05339311435818672}, {"id": 120, "seek": 67208, "start": 672.08, "end": 678.1600000000001, "text": " drawing upon GPT four's ability to reflect and engage in dialogue with itself? It got 84%.", "tokens": [50364, 6316, 3564, 26039, 51, 1451, 311, 3485, 281, 5031, 293, 4683, 294, 10221, 365, 2564, 30, 467, 658, 29018, 6856, 50668], "temperature": 0.0, "avg_logprob": -0.09850724538167317, "compression_ratio": 1.5289256198347108, "no_speech_prob": 0.02032751962542534}, {"id": 121, "seek": 67208, "start": 678.72, "end": 685.0400000000001, "text": " Now notice something about that number. GPT four zero short got 32% of the questions wrong. That", "tokens": [50696, 823, 3449, 746, 466, 300, 1230, 13, 26039, 51, 1451, 4018, 2099, 658, 8858, 4, 295, 264, 1651, 2085, 13, 663, 51012], "temperature": 0.0, "avg_logprob": -0.09850724538167317, "compression_ratio": 1.5289256198347108, "no_speech_prob": 0.02032751962542534}, {"id": 122, "seek": 67208, "start": 685.0400000000001, "end": 690.96, "text": " was halved to 16% after putting it through the smart GPT system. There was one question where", "tokens": [51012, 390, 7523, 937, 281, 3165, 4, 934, 3372, 309, 807, 264, 4069, 26039, 51, 1185, 13, 821, 390, 472, 1168, 689, 51308], "temperature": 0.0, "avg_logprob": -0.09850724538167317, "compression_ratio": 1.5289256198347108, "no_speech_prob": 0.02032751962542534}, {"id": 123, "seek": 67208, "start": 690.96, "end": 696.4000000000001, "text": " the resolver model gave both a correct and incorrect answer. But I'm counting that as an", "tokens": [51308, 264, 34480, 2316, 2729, 1293, 257, 3006, 293, 18424, 1867, 13, 583, 286, 478, 13251, 300, 382, 364, 51580], "temperature": 0.0, "avg_logprob": -0.09850724538167317, "compression_ratio": 1.5289256198347108, "no_speech_prob": 0.02032751962542534}, {"id": 124, "seek": 69640, "start": 696.4, "end": 702.88, "text": " incorrect answer for the purposes of this test. Anyway, from 32% to 16% incorrect,", "tokens": [50364, 18424, 1867, 337, 264, 9932, 295, 341, 1500, 13, 5684, 11, 490, 8858, 4, 281, 3165, 4, 18424, 11, 50688], "temperature": 0.0, "avg_logprob": -0.05295203815806996, "compression_ratio": 1.613240418118467, "no_speech_prob": 0.07367652654647827}, {"id": 125, "seek": 69640, "start": 702.88, "end": 708.0799999999999, "text": " that is a pattern that stayed consistent throughout all my testing that approximately half of the", "tokens": [50688, 300, 307, 257, 5102, 300, 9181, 8398, 3710, 439, 452, 4997, 300, 10447, 1922, 295, 264, 50948], "temperature": 0.0, "avg_logprob": -0.05295203815806996, "compression_ratio": 1.613240418118467, "no_speech_prob": 0.07367652654647827}, {"id": 126, "seek": 69640, "start": 708.0799999999999, "end": 714.9599999999999, "text": " errors that GPT four makes can be rectified. If you give it the optimized step by step prompt,", "tokens": [50948, 13603, 300, 26039, 51, 1451, 1669, 393, 312, 11048, 2587, 13, 759, 291, 976, 309, 264, 26941, 1823, 538, 1823, 12391, 11, 51292], "temperature": 0.0, "avg_logprob": -0.05295203815806996, "compression_ratio": 1.613240418118467, "no_speech_prob": 0.07367652654647827}, {"id": 127, "seek": 69640, "start": 714.9599999999999, "end": 721.68, "text": " get it to reflect on its results and get it to engage in dialogue and decide on a final answer.", "tokens": [51292, 483, 309, 281, 5031, 322, 1080, 3542, 293, 483, 309, 281, 4683, 294, 10221, 293, 4536, 322, 257, 2572, 1867, 13, 51628], "temperature": 0.0, "avg_logprob": -0.05295203815806996, "compression_ratio": 1.613240418118467, "no_speech_prob": 0.07367652654647827}, {"id": 128, "seek": 69640, "start": 721.68, "end": 726.16, "text": " At this point, for those people losing track of all the details, I want to put into context", "tokens": [51628, 1711, 341, 935, 11, 337, 729, 561, 7027, 2837, 295, 439, 264, 4365, 11, 286, 528, 281, 829, 666, 4319, 51852], "temperature": 0.0, "avg_logprob": -0.05295203815806996, "compression_ratio": 1.613240418118467, "no_speech_prob": 0.07367652654647827}, {"id": 129, "seek": 72616, "start": 726.16, "end": 732.24, "text": " what resolving half of the errors on MMLU might mean in the context of the big picture.", "tokens": [50364, 437, 49940, 1922, 295, 264, 13603, 322, 376, 12683, 52, 1062, 914, 294, 264, 4319, 295, 264, 955, 3036, 13, 50668], "temperature": 0.0, "avg_logprob": -0.10445997537660205, "compression_ratio": 1.570446735395189, "no_speech_prob": 0.0009109413367696106}, {"id": 130, "seek": 72616, "start": 732.24, "end": 739.28, "text": " Here's Lenard Heim, an AI governance researcher, suggesting a score of 95% on the MMLU would be", "tokens": [50668, 1692, 311, 23009, 515, 634, 332, 11, 364, 7318, 17449, 21751, 11, 18094, 257, 6175, 295, 13420, 4, 322, 264, 376, 12683, 52, 576, 312, 51020], "temperature": 0.0, "avg_logprob": -0.10445997537660205, "compression_ratio": 1.570446735395189, "no_speech_prob": 0.0009109413367696106}, {"id": 131, "seek": 72616, "start": 739.28, "end": 745.52, "text": " reflective of AGI like abilities. I do think I have like a 50% chance like within the next 20", "tokens": [51020, 28931, 295, 316, 26252, 411, 11582, 13, 286, 360, 519, 286, 362, 411, 257, 2625, 4, 2931, 411, 1951, 264, 958, 945, 51332], "temperature": 0.0, "avg_logprob": -0.10445997537660205, "compression_ratio": 1.570446735395189, "no_speech_prob": 0.0009109413367696106}, {"id": 132, "seek": 72616, "start": 745.52, "end": 750.4, "text": " years or so, there might be something what we might call an AGI or transformative AI. What", "tokens": [51332, 924, 420, 370, 11, 456, 1062, 312, 746, 437, 321, 1062, 818, 364, 316, 26252, 420, 36070, 7318, 13, 708, 51576], "temperature": 0.0, "avg_logprob": -0.10445997537660205, "compression_ratio": 1.570446735395189, "no_speech_prob": 0.0009109413367696106}, {"id": 133, "seek": 72616, "start": 750.4, "end": 755.12, "text": " do I mean by this? Well, maybe can measured on benchmarks. There's like this famous MMLU", "tokens": [51576, 360, 286, 914, 538, 341, 30, 1042, 11, 1310, 393, 12690, 322, 43751, 13, 821, 311, 411, 341, 4618, 376, 12683, 52, 51812], "temperature": 0.0, "avg_logprob": -0.10445997537660205, "compression_ratio": 1.570446735395189, "no_speech_prob": 0.0009109413367696106}, {"id": 134, "seek": 75512, "start": 755.12, "end": 760.48, "text": " benchmarks like yet or something which like scores like 95% on this. Going back to the results,", "tokens": [50364, 43751, 411, 1939, 420, 746, 597, 411, 13444, 411, 13420, 4, 322, 341, 13, 10963, 646, 281, 264, 3542, 11, 50632], "temperature": 0.0, "avg_logprob": -0.10648998106368865, "compression_ratio": 1.504, "no_speech_prob": 0.00044412590796127915}, {"id": 135, "seek": 75512, "start": 760.48, "end": 767.68, "text": " if a smart GPT like system can automatically resolve half of the errors that GPT four makes", "tokens": [50632, 498, 257, 4069, 26039, 51, 411, 1185, 393, 6772, 14151, 1922, 295, 264, 13603, 300, 26039, 51, 1451, 1669, 50992], "temperature": 0.0, "avg_logprob": -0.10648998106368865, "compression_ratio": 1.504, "no_speech_prob": 0.00044412590796127915}, {"id": 136, "seek": 75512, "start": 767.68, "end": 775.68, "text": " on the MMLU, that would increase its score from around 86.4% to around 93%, which is not far off", "tokens": [50992, 322, 264, 376, 12683, 52, 11, 300, 576, 3488, 1080, 6175, 490, 926, 26687, 13, 19, 4, 281, 926, 28876, 8923, 597, 307, 406, 1400, 766, 51392], "temperature": 0.0, "avg_logprob": -0.10648998106368865, "compression_ratio": 1.504, "no_speech_prob": 0.00044412590796127915}, {"id": 137, "seek": 75512, "start": 775.68, "end": 782.72, "text": " 95%. Remember, his prediction was a 50% chance in 20 years. I'm talking about GPT four now.", "tokens": [51392, 13420, 6856, 5459, 11, 702, 17630, 390, 257, 2625, 4, 2931, 294, 945, 924, 13, 286, 478, 1417, 466, 26039, 51, 1451, 586, 13, 51744], "temperature": 0.0, "avg_logprob": -0.10648998106368865, "compression_ratio": 1.504, "no_speech_prob": 0.00044412590796127915}, {"id": 138, "seek": 78272, "start": 782.72, "end": 786.96, "text": " For those who are still skeptical, I'm going to show you plenty more results now and then walk", "tokens": [50364, 1171, 729, 567, 366, 920, 28601, 11, 286, 478, 516, 281, 855, 291, 7140, 544, 3542, 586, 293, 550, 1792, 50576], "temperature": 0.0, "avg_logprob": -0.0682388754451976, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.0037065434735268354}, {"id": 139, "seek": 78272, "start": 786.96, "end": 792.08, "text": " through the papers that give the theory as to why this works. One thing that I forgot to mention", "tokens": [50576, 807, 264, 10577, 300, 976, 264, 5261, 382, 281, 983, 341, 1985, 13, 1485, 551, 300, 286, 5298, 281, 2152, 50832], "temperature": 0.0, "avg_logprob": -0.0682388754451976, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.0037065434735268354}, {"id": 140, "seek": 78272, "start": 792.08, "end": 800.5600000000001, "text": " earlier is that the human expert level on the MMLU is 89.8%. And that's taking the 95th percentile", "tokens": [50832, 3071, 307, 300, 264, 1952, 5844, 1496, 322, 264, 376, 12683, 52, 307, 31877, 13, 23, 6856, 400, 300, 311, 1940, 264, 13420, 392, 3043, 794, 51256], "temperature": 0.0, "avg_logprob": -0.0682388754451976, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.0037065434735268354}, {"id": 141, "seek": 78272, "start": 800.5600000000001, "end": 806.1600000000001, "text": " of human test takers. And remember, those are domain experts in each of these subtopics.", "tokens": [51256, 295, 1952, 1500, 991, 433, 13, 400, 1604, 11, 729, 366, 9274, 8572, 294, 1184, 295, 613, 7257, 404, 1167, 13, 51536], "temperature": 0.0, "avg_logprob": -0.0682388754451976, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.0037065434735268354}, {"id": 142, "seek": 78272, "start": 806.1600000000001, "end": 812.5600000000001, "text": " What we're doing is testing GPT four or smart GPT on all of the topics simultaneously.", "tokens": [51536, 708, 321, 434, 884, 307, 4997, 26039, 51, 1451, 420, 4069, 26039, 51, 322, 439, 295, 264, 8378, 16561, 13, 51856], "temperature": 0.0, "avg_logprob": -0.0682388754451976, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.0037065434735268354}, {"id": 143, "seek": 81256, "start": 812.56, "end": 817.28, "text": " So even if smart GPT like systems can't quite reach 95%, and I think honestly,", "tokens": [50364, 407, 754, 498, 4069, 26039, 51, 411, 3652, 393, 380, 1596, 2524, 13420, 8923, 293, 286, 519, 6095, 11, 50600], "temperature": 0.0, "avg_logprob": -0.07165905496348506, "compression_ratio": 1.5152542372881357, "no_speech_prob": 0.00011590465874178335}, {"id": 144, "seek": 81256, "start": 817.28, "end": 820.9599999999999, "text": " they'll get pretty close with all the refinements that I'm going to suggest,", "tokens": [50600, 436, 603, 483, 1238, 1998, 365, 439, 264, 44395, 6400, 300, 286, 478, 516, 281, 3402, 11, 50784], "temperature": 0.0, "avg_logprob": -0.07165905496348506, "compression_ratio": 1.5152542372881357, "no_speech_prob": 0.00011590465874178335}, {"id": 145, "seek": 81256, "start": 820.9599999999999, "end": 827.5999999999999, "text": " I think they should almost certainly be 89.8%, which is the human expert test taker level.", "tokens": [50784, 286, 519, 436, 820, 1920, 3297, 312, 31877, 13, 23, 8923, 597, 307, 264, 1952, 5844, 1500, 991, 260, 1496, 13, 51116], "temperature": 0.0, "avg_logprob": -0.07165905496348506, "compression_ratio": 1.5152542372881357, "no_speech_prob": 0.00011590465874178335}, {"id": 146, "seek": 81256, "start": 827.5999999999999, "end": 833.8399999999999, "text": " Intrigued by these results, I then put it through the college math test from the MMLU. And remember,", "tokens": [51116, 5681, 7065, 5827, 538, 613, 3542, 11, 286, 550, 829, 309, 807, 264, 3859, 5221, 1500, 490, 264, 376, 12683, 52, 13, 400, 1604, 11, 51428], "temperature": 0.0, "avg_logprob": -0.07165905496348506, "compression_ratio": 1.5152542372881357, "no_speech_prob": 0.00011590465874178335}, {"id": 147, "seek": 81256, "start": 833.8399999999999, "end": 838.8, "text": " this was before using the optimized version of the step by step prompt. Obviously, I'm not going to", "tokens": [51428, 341, 390, 949, 1228, 264, 26941, 3037, 295, 264, 1823, 538, 1823, 12391, 13, 7580, 11, 286, 478, 406, 516, 281, 51676], "temperature": 0.0, "avg_logprob": -0.07165905496348506, "compression_ratio": 1.5152542372881357, "no_speech_prob": 0.00011590465874178335}, {"id": 148, "seek": 83880, "start": 838.8, "end": 847.04, "text": " go through all the questions here, but let's skip to the final results. We have zero shot accuracy,", "tokens": [50364, 352, 807, 439, 264, 1651, 510, 11, 457, 718, 311, 10023, 281, 264, 2572, 3542, 13, 492, 362, 4018, 3347, 14170, 11, 50776], "temperature": 0.0, "avg_logprob": -0.08228305894501355, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.027577906847000122}, {"id": 149, "seek": 83880, "start": 847.04, "end": 853.28, "text": " six out of 15, which is 40%. The average when you add let's think step by step was 53.5%.", "tokens": [50776, 2309, 484, 295, 2119, 11, 597, 307, 3356, 6856, 440, 4274, 562, 291, 909, 718, 311, 519, 1823, 538, 1823, 390, 21860, 13, 20, 6856, 51088], "temperature": 0.0, "avg_logprob": -0.08228305894501355, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.027577906847000122}, {"id": 150, "seek": 83880, "start": 853.8399999999999, "end": 860.0799999999999, "text": " And then the final output of the resolver model had a 60% accuracy. So it couldn't quite resolve", "tokens": [51116, 400, 550, 264, 2572, 5598, 295, 264, 34480, 2316, 632, 257, 4060, 4, 14170, 13, 407, 309, 2809, 380, 1596, 14151, 51428], "temperature": 0.0, "avg_logprob": -0.08228305894501355, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.027577906847000122}, {"id": 151, "seek": 83880, "start": 860.0799999999999, "end": 865.3599999999999, "text": " half of the errors, but the overall pattern held up. In case anyone is wondering about methodology,", "tokens": [51428, 1922, 295, 264, 13603, 11, 457, 264, 4787, 5102, 5167, 493, 13, 682, 1389, 2878, 307, 6359, 466, 24850, 11, 51692], "temperature": 0.0, "avg_logprob": -0.08228305894501355, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.027577906847000122}, {"id": 152, "seek": 86536, "start": 865.36, "end": 871.36, "text": " I kept the formatting identical for every question. I always opened a new tab for each question.", "tokens": [50364, 286, 4305, 264, 39366, 14800, 337, 633, 1168, 13, 286, 1009, 5625, 257, 777, 4421, 337, 1184, 1168, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07797490033236416, "compression_ratio": 1.7865168539325842, "no_speech_prob": 0.013634191825985909}, {"id": 153, "seek": 86536, "start": 871.36, "end": 876.24, "text": " It wasn't looking at the context of what it had already put out. Each attempt was fresh,", "tokens": [50664, 467, 2067, 380, 1237, 412, 264, 4319, 295, 437, 309, 632, 1217, 829, 484, 13, 6947, 5217, 390, 4451, 11, 50908], "temperature": 0.0, "avg_logprob": -0.07797490033236416, "compression_ratio": 1.7865168539325842, "no_speech_prob": 0.013634191825985909}, {"id": 154, "seek": 86536, "start": 876.24, "end": 881.76, "text": " aside from the resolver model, which looked at the context of the researcher's output. And again,", "tokens": [50908, 7359, 490, 264, 34480, 2316, 11, 597, 2956, 412, 264, 4319, 295, 264, 21751, 311, 5598, 13, 400, 797, 11, 51184], "temperature": 0.0, "avg_logprob": -0.07797490033236416, "compression_ratio": 1.7865168539325842, "no_speech_prob": 0.013634191825985909}, {"id": 155, "seek": 86536, "start": 881.76, "end": 887.6800000000001, "text": " as you can see from example 14, it wasn't like the researcher could always spot the errors or that", "tokens": [51184, 382, 291, 393, 536, 490, 1365, 3499, 11, 309, 2067, 380, 411, 264, 21751, 727, 1009, 4008, 264, 13603, 420, 300, 51480], "temperature": 0.0, "avg_logprob": -0.07797490033236416, "compression_ratio": 1.7865168539325842, "no_speech_prob": 0.013634191825985909}, {"id": 156, "seek": 86536, "start": 887.6800000000001, "end": 893.52, "text": " the resolver could always pick the right option. Sometimes the let's think step by step prompt", "tokens": [51480, 264, 34480, 727, 1009, 1888, 264, 558, 3614, 13, 4803, 264, 718, 311, 519, 1823, 538, 1823, 12391, 51772], "temperature": 0.0, "avg_logprob": -0.07797490033236416, "compression_ratio": 1.7865168539325842, "no_speech_prob": 0.013634191825985909}, {"id": 157, "seek": 89352, "start": 893.52, "end": 899.28, "text": " gave the right output, but the resolver couldn't quite distinguish it. The optimized prompt gets", "tokens": [50364, 2729, 264, 558, 5598, 11, 457, 264, 34480, 2809, 380, 1596, 20206, 309, 13, 440, 26941, 12391, 2170, 50652], "temperature": 0.0, "avg_logprob": -0.12525269644601003, "compression_ratio": 1.72, "no_speech_prob": 0.006288579199463129}, {"id": 158, "seek": 89352, "start": 899.28, "end": 905.76, "text": " a slightly better output. And upon reflection, the researcher can sometimes but not always spot", "tokens": [50652, 257, 4748, 1101, 5598, 13, 400, 3564, 12914, 11, 264, 21751, 393, 2171, 457, 406, 1009, 4008, 50976], "temperature": 0.0, "avg_logprob": -0.12525269644601003, "compression_ratio": 1.72, "no_speech_prob": 0.006288579199463129}, {"id": 159, "seek": 89352, "start": 905.76, "end": 912.16, "text": " the errors of those outputs. And sometimes but not always the resolver can spot based on those", "tokens": [50976, 264, 13603, 295, 729, 23930, 13, 400, 2171, 457, 406, 1009, 264, 34480, 393, 4008, 2361, 322, 729, 51296], "temperature": 0.0, "avg_logprob": -0.12525269644601003, "compression_ratio": 1.72, "no_speech_prob": 0.006288579199463129}, {"id": 160, "seek": 89352, "start": 912.16, "end": 918.4, "text": " flaws, which answer is best. These are incremental improvements. Sometimes GPT-4 simply can't get", "tokens": [51296, 27108, 11, 597, 1867, 307, 1151, 13, 1981, 366, 35759, 13797, 13, 4803, 26039, 51, 12, 19, 2935, 393, 380, 483, 51608], "temperature": 0.0, "avg_logprob": -0.12525269644601003, "compression_ratio": 1.72, "no_speech_prob": 0.006288579199463129}, {"id": 161, "seek": 89352, "start": 918.4, "end": 923.1999999999999, "text": " it right. I have noticed a few themes in those questions. Anytime it comes to division,", "tokens": [51608, 309, 558, 13, 286, 362, 5694, 257, 1326, 13544, 294, 729, 1651, 13, 39401, 309, 1487, 281, 10044, 11, 51848], "temperature": 0.0, "avg_logprob": -0.12525269644601003, "compression_ratio": 1.72, "no_speech_prob": 0.006288579199463129}, {"id": 162, "seek": 92320, "start": 923.2, "end": 929.2800000000001, "text": " multiplication, characters, or counting in general, GPT-4 tends to make mistakes that", "tokens": [50364, 27290, 11, 4342, 11, 420, 13251, 294, 2674, 11, 26039, 51, 12, 19, 12258, 281, 652, 8038, 300, 50668], "temperature": 0.0, "avg_logprob": -0.07965847162099984, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0015009607886895537}, {"id": 163, "seek": 92320, "start": 929.2800000000001, "end": 935.36, "text": " neither the researcher nor resolver can spot. Of course, integrating a few tools via API would", "tokens": [50668, 9662, 264, 21751, 6051, 34480, 393, 4008, 13, 2720, 1164, 11, 26889, 257, 1326, 3873, 5766, 9362, 576, 50972], "temperature": 0.0, "avg_logprob": -0.07965847162099984, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0015009607886895537}, {"id": 164, "seek": 92320, "start": 935.36, "end": 940.48, "text": " likely solve those issues. And I don't want to preempt the conclusion too much, but I believe a", "tokens": [50972, 3700, 5039, 729, 2663, 13, 400, 286, 500, 380, 528, 281, 659, 4543, 264, 10063, 886, 709, 11, 457, 286, 1697, 257, 51228], "temperature": 0.0, "avg_logprob": -0.07965847162099984, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0015009607886895537}, {"id": 165, "seek": 92320, "start": 940.48, "end": 949.9200000000001, "text": " smart GPT-like system with tools integrated could probably score around 95% right now on the MMLU,", "tokens": [51228, 4069, 26039, 51, 12, 4092, 1185, 365, 3873, 10919, 727, 1391, 6175, 926, 13420, 4, 558, 586, 322, 264, 376, 12683, 52, 11, 51700], "temperature": 0.0, "avg_logprob": -0.07965847162099984, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0015009607886895537}, {"id": 166, "seek": 94992, "start": 949.92, "end": 955.12, "text": " especially if it was helped out with a few shot prompting. To add weight to that preliminary", "tokens": [50364, 2318, 498, 309, 390, 4254, 484, 365, 257, 1326, 3347, 12391, 278, 13, 1407, 909, 3364, 281, 300, 28817, 50624], "temperature": 0.0, "avg_logprob": -0.09452440908976964, "compression_ratio": 1.5522875816993464, "no_speech_prob": 0.005728831049054861}, {"id": 167, "seek": 94992, "start": 955.12, "end": 960.8, "text": " conclusion, I tested it on certain topics and had to stop because it simply got the questions right", "tokens": [50624, 10063, 11, 286, 8246, 309, 322, 1629, 8378, 293, 632, 281, 1590, 570, 309, 2935, 658, 264, 1651, 558, 50908], "temperature": 0.0, "avg_logprob": -0.09452440908976964, "compression_ratio": 1.5522875816993464, "no_speech_prob": 0.005728831049054861}, {"id": 168, "seek": 94992, "start": 960.8, "end": 966.4799999999999, "text": " every single time. For example, high school psychology from the MMLU. I then tried prehistory,", "tokens": [50908, 633, 2167, 565, 13, 1171, 1365, 11, 1090, 1395, 15105, 490, 264, 376, 12683, 52, 13, 286, 550, 3031, 659, 33236, 827, 11, 51192], "temperature": 0.0, "avg_logprob": -0.09452440908976964, "compression_ratio": 1.5522875816993464, "no_speech_prob": 0.005728831049054861}, {"id": 169, "seek": 94992, "start": 966.4799999999999, "end": 971.12, "text": " which it also aced before finding machine learning where I got more interesting results.", "tokens": [51192, 597, 309, 611, 696, 292, 949, 5006, 3479, 2539, 689, 286, 658, 544, 1880, 3542, 13, 51424], "temperature": 0.0, "avg_logprob": -0.09452440908976964, "compression_ratio": 1.5522875816993464, "no_speech_prob": 0.005728831049054861}, {"id": 170, "seek": 94992, "start": 971.12, "end": 977.28, "text": " Zooming in this time, the raw score was 65%. The chain of thought let's think step by step average", "tokens": [51424, 10337, 10539, 294, 341, 565, 11, 264, 8936, 6175, 390, 11624, 6856, 440, 5021, 295, 1194, 718, 311, 519, 1823, 538, 1823, 4274, 51732], "temperature": 0.0, "avg_logprob": -0.09452440908976964, "compression_ratio": 1.5522875816993464, "no_speech_prob": 0.005728831049054861}, {"id": 171, "seek": 97728, "start": 977.28, "end": 983.8399999999999, "text": " was 71.6% and the resolver model got 80%. Let's now look a little deeper into why all of these", "tokens": [50364, 390, 30942, 13, 21, 4, 293, 264, 34480, 2316, 658, 4688, 6856, 961, 311, 586, 574, 257, 707, 7731, 666, 983, 439, 295, 613, 50692], "temperature": 0.0, "avg_logprob": -0.07516297647508524, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.001098623382858932}, {"id": 172, "seek": 97728, "start": 983.8399999999999, "end": 989.52, "text": " steps might improve the end result. In reply to the original let's think step by step paper,", "tokens": [50692, 4439, 1062, 3470, 264, 917, 1874, 13, 682, 16972, 281, 264, 3380, 718, 311, 519, 1823, 538, 1823, 3035, 11, 50976], "temperature": 0.0, "avg_logprob": -0.07516297647508524, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.001098623382858932}, {"id": 173, "seek": 97728, "start": 989.52, "end": 994.88, "text": " which was published around a year ago, Andrea Carpathi said this. Adding something like let's", "tokens": [50976, 597, 390, 6572, 926, 257, 1064, 2057, 11, 24215, 2741, 31852, 72, 848, 341, 13, 31204, 746, 411, 718, 311, 51244], "temperature": 0.0, "avg_logprob": -0.07516297647508524, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.001098623382858932}, {"id": 174, "seek": 97728, "start": 994.88, "end": 1001.6, "text": " think step by step to the prompt is a way of using the input space for computation that you'd normally", "tokens": [51244, 519, 1823, 538, 1823, 281, 264, 12391, 307, 257, 636, 295, 1228, 264, 4846, 1901, 337, 24903, 300, 291, 1116, 5646, 51580], "temperature": 0.0, "avg_logprob": -0.07516297647508524, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.001098623382858932}, {"id": 175, "seek": 97728, "start": 1001.6, "end": 1006.8, "text": " want in the hidden state of the model. Instead of the workings out being done in the activations", "tokens": [51580, 528, 294, 264, 7633, 1785, 295, 264, 2316, 13, 7156, 295, 264, 589, 1109, 484, 885, 1096, 294, 264, 2430, 763, 51840], "temperature": 0.0, "avg_logprob": -0.07516297647508524, "compression_ratio": 1.6529209621993126, "no_speech_prob": 0.001098623382858932}, {"id": 176, "seek": 100680, "start": 1006.8, "end": 1013.4399999999999, "text": " of the neural network, it's done in the discrete tokens of that input space. And he adds did not", "tokens": [50364, 295, 264, 18161, 3209, 11, 309, 311, 1096, 294, 264, 27706, 22667, 295, 300, 4846, 1901, 13, 400, 415, 10860, 630, 406, 50696], "temperature": 0.0, "avg_logprob": -0.1068344007838856, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.002800456713885069}, {"id": 177, "seek": 100680, "start": 1013.4399999999999, "end": 1018.3199999999999, "text": " super see this coming. And here is the paper released three days ago that improves upon", "tokens": [50696, 1687, 536, 341, 1348, 13, 400, 510, 307, 264, 3035, 4736, 1045, 1708, 2057, 300, 24771, 3564, 50940], "temperature": 0.0, "avg_logprob": -0.1068344007838856, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.002800456713885069}, {"id": 178, "seek": 100680, "start": 1018.3199999999999, "end": 1024.8, "text": " that original prompt. They also did their testing zero shot like me. And they tested many prompts", "tokens": [50940, 300, 3380, 12391, 13, 814, 611, 630, 641, 4997, 4018, 3347, 411, 385, 13, 400, 436, 8246, 867, 41095, 51264], "temperature": 0.0, "avg_logprob": -0.1068344007838856, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.002800456713885069}, {"id": 179, "seek": 100680, "start": 1024.8, "end": 1030.96, "text": " starting like I did with just direct prompting, just asking the question like 99% of users would", "tokens": [51264, 2891, 411, 286, 630, 365, 445, 2047, 12391, 278, 11, 445, 3365, 264, 1168, 411, 11803, 4, 295, 5022, 576, 51572], "temperature": 0.0, "avg_logprob": -0.1068344007838856, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.002800456713885069}, {"id": 180, "seek": 103096, "start": 1030.96, "end": 1036.72, "text": " do of GPT four. And then they tried like me the well established let's think step by step", "tokens": [50364, 360, 295, 26039, 51, 1451, 13, 400, 550, 436, 3031, 411, 385, 264, 731, 7545, 718, 311, 519, 1823, 538, 1823, 50652], "temperature": 0.0, "avg_logprob": -0.11432372290512612, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.10085522383451462}, {"id": 181, "seek": 103096, "start": 1036.72, "end": 1042.16, "text": " prompt. They also iteratively tested seven original prompts, as well as the prompt that I've now", "tokens": [50652, 12391, 13, 814, 611, 17138, 19020, 8246, 3407, 3380, 41095, 11, 382, 731, 382, 264, 12391, 300, 286, 600, 586, 50924], "temperature": 0.0, "avg_logprob": -0.11432372290512612, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.10085522383451462}, {"id": 182, "seek": 103096, "start": 1042.16, "end": 1048.08, "text": " integrated into smart GPT the let's work this out in a step by step way, etc. They share my opinion", "tokens": [50924, 10919, 666, 4069, 26039, 51, 264, 718, 311, 589, 341, 484, 294, 257, 1823, 538, 1823, 636, 11, 5183, 13, 814, 2073, 452, 4800, 51220], "temperature": 0.0, "avg_logprob": -0.11432372290512612, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.10085522383451462}, {"id": 183, "seek": 103096, "start": 1048.08, "end": 1054.48, "text": " that zero shot prompting setups have the benefit of not requiring such task dependent selection", "tokens": [51220, 300, 4018, 3347, 12391, 278, 46832, 362, 264, 5121, 295, 406, 24165, 1270, 5633, 12334, 9450, 51540], "temperature": 0.0, "avg_logprob": -0.11432372290512612, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.10085522383451462}, {"id": 184, "seek": 103096, "start": 1054.48, "end": 1059.6000000000001, "text": " of exemplars. You don't have to find correct examples. It just does it all for you. Here are", "tokens": [51540, 295, 24112, 685, 13, 509, 500, 380, 362, 281, 915, 3006, 5110, 13, 467, 445, 775, 309, 439, 337, 291, 13, 1692, 366, 51796], "temperature": 0.0, "avg_logprob": -0.11432372290512612, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.10085522383451462}, {"id": 185, "seek": 105960, "start": 1059.6, "end": 1065.12, "text": " the end results for GPT four that we saw earlier showing the difference between asking directly", "tokens": [50364, 264, 917, 3542, 337, 26039, 51, 1451, 300, 321, 1866, 3071, 4099, 264, 2649, 1296, 3365, 3838, 50640], "temperature": 0.0, "avg_logprob": -0.0834433935522064, "compression_ratio": 1.7753164556962024, "no_speech_prob": 0.005219066981226206}, {"id": 186, "seek": 105960, "start": 1065.12, "end": 1070.1599999999999, "text": " your question and using these refined prompts. Notice that this technique is somewhat model", "tokens": [50640, 428, 1168, 293, 1228, 613, 26201, 41095, 13, 13428, 300, 341, 6532, 307, 8344, 2316, 50892], "temperature": 0.0, "avg_logprob": -0.0834433935522064, "compression_ratio": 1.7753164556962024, "no_speech_prob": 0.005219066981226206}, {"id": 187, "seek": 105960, "start": 1070.1599999999999, "end": 1075.28, "text": " dependent. And it doesn't have the same effect on smaller or weaker models. Before we move on", "tokens": [50892, 12334, 13, 400, 309, 1177, 380, 362, 264, 912, 1802, 322, 4356, 420, 24286, 5245, 13, 4546, 321, 1286, 322, 51148], "temperature": 0.0, "avg_logprob": -0.0834433935522064, "compression_ratio": 1.7753164556962024, "no_speech_prob": 0.005219066981226206}, {"id": 188, "seek": 105960, "start": 1075.28, "end": 1080.08, "text": " to the next paper, there is one somewhat failed prompt that I want to pick up on. It's this", "tokens": [51148, 281, 264, 958, 3035, 11, 456, 307, 472, 8344, 7612, 12391, 300, 286, 528, 281, 1888, 493, 322, 13, 467, 311, 341, 51388], "temperature": 0.0, "avg_logprob": -0.0834433935522064, "compression_ratio": 1.7753164556962024, "no_speech_prob": 0.005219066981226206}, {"id": 189, "seek": 105960, "start": 1080.08, "end": 1084.56, "text": " self critique prompt where they ask answer the question, then critique the answer based on the", "tokens": [51388, 2698, 25673, 12391, 689, 436, 1029, 1867, 264, 1168, 11, 550, 25673, 264, 1867, 2361, 322, 264, 51612], "temperature": 0.0, "avg_logprob": -0.0834433935522064, "compression_ratio": 1.7753164556962024, "no_speech_prob": 0.005219066981226206}, {"id": 190, "seek": 105960, "start": 1084.56, "end": 1089.04, "text": " critique, reconsider the other answer options, and give a single final answer. And you might", "tokens": [51612, 25673, 11, 40497, 264, 661, 1867, 3956, 11, 293, 976, 257, 2167, 2572, 1867, 13, 400, 291, 1062, 51836], "temperature": 0.0, "avg_logprob": -0.0834433935522064, "compression_ratio": 1.7753164556962024, "no_speech_prob": 0.005219066981226206}, {"id": 191, "seek": 108904, "start": 1089.04, "end": 1094.72, "text": " wonder why didn't that prompt perform best when we know that reflection and dialogue can work?", "tokens": [50364, 2441, 983, 994, 380, 300, 12391, 2042, 1151, 562, 321, 458, 300, 12914, 293, 10221, 393, 589, 30, 50648], "temperature": 0.0, "avg_logprob": -0.05986794164358092, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.0037066268268972635}, {"id": 192, "seek": 108904, "start": 1094.72, "end": 1100.72, "text": " My theory is because it's trying to do all of it in one prompt. Through my hundreds of experiments,", "tokens": [50648, 1222, 5261, 307, 570, 309, 311, 1382, 281, 360, 439, 295, 309, 294, 472, 12391, 13, 8927, 452, 6779, 295, 12050, 11, 50948], "temperature": 0.0, "avg_logprob": -0.05986794164358092, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.0037066268268972635}, {"id": 193, "seek": 108904, "start": 1100.72, "end": 1106.1599999999999, "text": " I've noticed that GPT four can only handle so much in one go. It simply gets overwhelmed or", "tokens": [50948, 286, 600, 5694, 300, 26039, 51, 1451, 393, 787, 4813, 370, 709, 294, 472, 352, 13, 467, 2935, 2170, 19042, 420, 51220], "temperature": 0.0, "avg_logprob": -0.05986794164358092, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.0037066268268972635}, {"id": 194, "seek": 108904, "start": 1106.1599999999999, "end": 1112.24, "text": " confused if you ask it to do too much in one prompt. That's why I broke my model into stages to allow", "tokens": [51220, 9019, 498, 291, 1029, 309, 281, 360, 886, 709, 294, 472, 12391, 13, 663, 311, 983, 286, 6902, 452, 2316, 666, 10232, 281, 2089, 51524], "temperature": 0.0, "avg_logprob": -0.05986794164358092, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.0037066268268972635}, {"id": 195, "seek": 108904, "start": 1112.24, "end": 1117.52, "text": " it to show off each of its abilities one by one. And before we get to the other papers, what's my", "tokens": [51524, 309, 281, 855, 766, 1184, 295, 1080, 11582, 472, 538, 472, 13, 400, 949, 321, 483, 281, 264, 661, 10577, 11, 437, 311, 452, 51788], "temperature": 0.0, "avg_logprob": -0.05986794164358092, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.0037066268268972635}, {"id": 196, "seek": 111752, "start": 1117.52, "end": 1123.28, "text": " personal theory as to why this eliminates up to half of the errors that GPT four makes? Well,", "tokens": [50364, 2973, 5261, 382, 281, 983, 341, 49893, 493, 281, 1922, 295, 264, 13603, 300, 26039, 51, 1451, 1669, 30, 1042, 11, 50652], "temperature": 0.0, "avg_logprob": -0.08830392603971521, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.019116513431072235}, {"id": 197, "seek": 111752, "start": 1123.28, "end": 1130.16, "text": " my guess is this. Remember that GPT four is drawing on a vast data set of internet text. And let me", "tokens": [50652, 452, 2041, 307, 341, 13, 5459, 300, 26039, 51, 1451, 307, 6316, 322, 257, 8369, 1412, 992, 295, 4705, 2487, 13, 400, 718, 385, 50996], "temperature": 0.0, "avg_logprob": -0.08830392603971521, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.019116513431072235}, {"id": 198, "seek": 111752, "start": 1130.16, "end": 1136.72, "text": " ask you what kind of text has things like question, answer, let's work this out. Be sure we have the", "tokens": [50996, 1029, 291, 437, 733, 295, 2487, 575, 721, 411, 1168, 11, 1867, 11, 718, 311, 589, 341, 484, 13, 879, 988, 321, 362, 264, 51324], "temperature": 0.0, "avg_logprob": -0.08830392603971521, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.019116513431072235}, {"id": 199, "seek": 111752, "start": 1136.72, "end": 1142.8, "text": " right answer. The kind of data that would have that text would be things like tutorials or expert", "tokens": [51324, 558, 1867, 13, 440, 733, 295, 1412, 300, 576, 362, 300, 2487, 576, 312, 721, 411, 17616, 420, 5844, 51628], "temperature": 0.0, "avg_logprob": -0.08830392603971521, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.019116513431072235}, {"id": 200, "seek": 114280, "start": 1142.8799999999999, "end": 1148.56, "text": " breakdowns. So I believe you're triggering more of the weights inside GPT four that relate to", "tokens": [50368, 18188, 82, 13, 407, 286, 1697, 291, 434, 40406, 544, 295, 264, 17443, 1854, 26039, 51, 1451, 300, 10961, 281, 50652], "temperature": 0.0, "avg_logprob": -0.06530563786344708, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.04466822370886803}, {"id": 201, "seek": 114280, "start": 1148.56, "end": 1153.84, "text": " things like expert tutorials. And so inevitably you're getting slightly better answers. Next,", "tokens": [50652, 721, 411, 5844, 17616, 13, 400, 370, 28171, 291, 434, 1242, 4748, 1101, 6338, 13, 3087, 11, 50916], "temperature": 0.0, "avg_logprob": -0.06530563786344708, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.04466822370886803}, {"id": 202, "seek": 114280, "start": 1153.84, "end": 1158.8, "text": " I've already explained why you get different outputs when you give the exact same prompt.", "tokens": [50916, 286, 600, 1217, 8825, 983, 291, 483, 819, 23930, 562, 291, 976, 264, 1900, 912, 12391, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06530563786344708, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.04466822370886803}, {"id": 203, "seek": 114280, "start": 1158.8, "end": 1163.68, "text": " That's down to sampling and the temperature of the model. But to simplify massively, sometimes", "tokens": [51164, 663, 311, 760, 281, 21179, 293, 264, 4292, 295, 264, 2316, 13, 583, 281, 20460, 29379, 11, 2171, 51408], "temperature": 0.0, "avg_logprob": -0.06530563786344708, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.04466822370886803}, {"id": 204, "seek": 114280, "start": 1163.68, "end": 1169.9199999999998, "text": " GPT four will give you an output that it knows isn't the most probable. It introduces some randomness", "tokens": [51408, 26039, 51, 1451, 486, 976, 291, 364, 5598, 300, 309, 3255, 1943, 380, 264, 881, 21759, 13, 467, 31472, 512, 4974, 1287, 51720], "temperature": 0.0, "avg_logprob": -0.06530563786344708, "compression_ratio": 1.6177474402730376, "no_speech_prob": 0.04466822370886803}, {"id": 205, "seek": 116992, "start": 1169.92, "end": 1174.96, "text": " into its sampling by generating multiple outputs, you're getting a larger sample size,", "tokens": [50364, 666, 1080, 21179, 538, 17746, 3866, 23930, 11, 291, 434, 1242, 257, 4833, 6889, 2744, 11, 50616], "temperature": 0.0, "avg_logprob": -0.0946179573689032, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.009123729541897774}, {"id": 206, "seek": 116992, "start": 1174.96, "end": 1181.04, "text": " reflecting the full range of probabilities that GPT four ascribes to its outputs, you're reducing", "tokens": [50616, 23543, 264, 1577, 3613, 295, 33783, 300, 26039, 51, 1451, 382, 1142, 6446, 281, 1080, 23930, 11, 291, 434, 12245, 50920], "temperature": 0.0, "avg_logprob": -0.0946179573689032, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.009123729541897774}, {"id": 207, "seek": 116992, "start": 1181.04, "end": 1186.8000000000002, "text": " a little bit some of the randomness that's inherent in GPT four outputs. Next, I believe that GPT", "tokens": [50920, 257, 707, 857, 512, 295, 264, 4974, 1287, 300, 311, 26387, 294, 26039, 51, 1451, 23930, 13, 3087, 11, 286, 1697, 300, 26039, 51, 51208], "temperature": 0.0, "avg_logprob": -0.0946179573689032, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.009123729541897774}, {"id": 208, "seek": 116992, "start": 1186.8000000000002, "end": 1192.96, "text": " four can sometimes spot its own errors through reflection, because prompting like this triggers", "tokens": [51208, 1451, 393, 2171, 4008, 1080, 1065, 13603, 807, 12914, 11, 570, 12391, 278, 411, 341, 22827, 51516], "temperature": 0.0, "avg_logprob": -0.0946179573689032, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.009123729541897774}, {"id": 209, "seek": 116992, "start": 1192.96, "end": 1197.92, "text": " a different set of weights, you could almost think of it as a different mindset, one more focused on", "tokens": [51516, 257, 819, 992, 295, 17443, 11, 291, 727, 1920, 519, 295, 309, 382, 257, 819, 12543, 11, 472, 544, 5178, 322, 51764], "temperature": 0.0, "avg_logprob": -0.0946179573689032, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.009123729541897774}, {"id": 210, "seek": 119792, "start": 1197.92, "end": 1202.3200000000002, "text": " finding errors. Again, if the question is too hard or involves counting characters,", "tokens": [50364, 5006, 13603, 13, 3764, 11, 498, 264, 1168, 307, 886, 1152, 420, 11626, 13251, 4342, 11, 50584], "temperature": 0.0, "avg_logprob": -0.07285434859139579, "compression_ratio": 1.6433566433566433, "no_speech_prob": 0.01032558735460043}, {"id": 211, "seek": 119792, "start": 1202.3200000000002, "end": 1207.04, "text": " division, multiplication, as I said earlier, this won't help. But a percentage of the time it can", "tokens": [50584, 10044, 11, 27290, 11, 382, 286, 848, 3071, 11, 341, 1582, 380, 854, 13, 583, 257, 9668, 295, 264, 565, 309, 393, 50820], "temperature": 0.0, "avg_logprob": -0.07285434859139579, "compression_ratio": 1.6433566433566433, "no_speech_prob": 0.01032558735460043}, {"id": 212, "seek": 119792, "start": 1207.04, "end": 1212.4, "text": " spot its own errors and point them out. Notice this is a separate bit of inference not lumped into", "tokens": [50820, 4008, 1080, 1065, 13603, 293, 935, 552, 484, 13, 13428, 341, 307, 257, 4994, 857, 295, 38253, 406, 25551, 292, 666, 51088], "temperature": 0.0, "avg_logprob": -0.07285434859139579, "compression_ratio": 1.6433566433566433, "no_speech_prob": 0.01032558735460043}, {"id": 213, "seek": 119792, "start": 1212.4, "end": 1217.52, "text": " the original prompt. And when it does successfully point out the errors, it can often engage in", "tokens": [51088, 264, 3380, 12391, 13, 400, 562, 309, 775, 10727, 935, 484, 264, 13603, 11, 309, 393, 2049, 4683, 294, 51344], "temperature": 0.0, "avg_logprob": -0.07285434859139579, "compression_ratio": 1.6433566433566433, "no_speech_prob": 0.01032558735460043}, {"id": 214, "seek": 119792, "start": 1217.52, "end": 1223.04, "text": " this dialogue with itself. Notice in a meta kind of way, I'm using the step by step prompting", "tokens": [51344, 341, 10221, 365, 2564, 13, 13428, 294, 257, 19616, 733, 295, 636, 11, 286, 478, 1228, 264, 1823, 538, 1823, 12391, 278, 51620], "temperature": 0.0, "avg_logprob": -0.07285434859139579, "compression_ratio": 1.6433566433566433, "no_speech_prob": 0.01032558735460043}, {"id": 215, "seek": 122304, "start": 1223.12, "end": 1227.76, "text": " to improve the reflection and dialogue. So those are my theories as to why it works,", "tokens": [50368, 281, 3470, 264, 12914, 293, 10221, 13, 407, 729, 366, 452, 13667, 382, 281, 983, 309, 1985, 11, 50600], "temperature": 0.0, "avg_logprob": -0.0735059210232326, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.03731059655547142}, {"id": 216, "seek": 122304, "start": 1227.76, "end": 1232.1599999999999, "text": " but at the end of the video, I'm going to show you at least five ways I think the model can be", "tokens": [50600, 457, 412, 264, 917, 295, 264, 960, 11, 286, 478, 516, 281, 855, 291, 412, 1935, 1732, 2098, 286, 519, 264, 2316, 393, 312, 50820], "temperature": 0.0, "avg_logprob": -0.0735059210232326, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.03731059655547142}, {"id": 217, "seek": 122304, "start": 1232.1599999999999, "end": 1238.0, "text": " further refined. Before we do, though, I looked up the paper by Joe, which produced that prompt that", "tokens": [50820, 3052, 26201, 13, 4546, 321, 360, 11, 1673, 11, 286, 2956, 493, 264, 3035, 538, 6807, 11, 597, 7126, 300, 12391, 300, 51112], "temperature": 0.0, "avg_logprob": -0.0735059210232326, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.03731059655547142}, {"id": 218, "seek": 122304, "start": 1238.0, "end": 1243.04, "text": " did the best in the previous paper, they came to that special prompt through automatic prompt", "tokens": [51112, 630, 264, 1151, 294, 264, 3894, 3035, 11, 436, 1361, 281, 300, 2121, 12391, 807, 12509, 12391, 51364], "temperature": 0.0, "avg_logprob": -0.0735059210232326, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.03731059655547142}, {"id": 219, "seek": 122304, "start": 1243.04, "end": 1246.6399999999999, "text": " engineering. But there's something interesting I want to point out, though, on page seven,", "tokens": [51364, 7043, 13, 583, 456, 311, 746, 1880, 286, 528, 281, 935, 484, 11, 1673, 11, 322, 3028, 3407, 11, 51544], "temperature": 0.0, "avg_logprob": -0.0735059210232326, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.03731059655547142}, {"id": 220, "seek": 124664, "start": 1246.64, "end": 1253.2800000000002, "text": " they say we use automatic prompt engineering to find a prompt starting with let's that maximizes", "tokens": [50364, 436, 584, 321, 764, 12509, 12391, 7043, 281, 915, 257, 12391, 2891, 365, 718, 311, 300, 5138, 5660, 50696], "temperature": 0.0, "avg_logprob": -0.07302775054142394, "compression_ratio": 1.6725352112676057, "no_speech_prob": 0.10084079951047897}, {"id": 221, "seek": 124664, "start": 1253.2800000000002, "end": 1258.0, "text": " the likelihood of correct reasoning steps. Then they found the best one that I integrated into", "tokens": [50696, 264, 22119, 295, 3006, 21577, 4439, 13, 1396, 436, 1352, 264, 1151, 472, 300, 286, 10919, 666, 50932], "temperature": 0.0, "avg_logprob": -0.07302775054142394, "compression_ratio": 1.6725352112676057, "no_speech_prob": 0.10084079951047897}, {"id": 222, "seek": 124664, "start": 1258.0, "end": 1262.24, "text": " smart GPT. Let's work this out in a step by step way to be sure we have the right answer. That's", "tokens": [50932, 4069, 26039, 51, 13, 961, 311, 589, 341, 484, 294, 257, 1823, 538, 1823, 636, 281, 312, 988, 321, 362, 264, 558, 1867, 13, 663, 311, 51144], "temperature": 0.0, "avg_logprob": -0.07302775054142394, "compression_ratio": 1.6725352112676057, "no_speech_prob": 0.10084079951047897}, {"id": 223, "seek": 124664, "start": 1262.24, "end": 1267.1200000000001, "text": " the one I want you to use. And they ran their own benchmarks. And of course, it did improve", "tokens": [51144, 264, 472, 286, 528, 291, 281, 764, 13, 400, 436, 5872, 641, 1065, 43751, 13, 400, 295, 1164, 11, 309, 630, 3470, 51388], "temperature": 0.0, "avg_logprob": -0.07302775054142394, "compression_ratio": 1.6725352112676057, "no_speech_prob": 0.10084079951047897}, {"id": 224, "seek": 124664, "start": 1267.1200000000001, "end": 1272.5600000000002, "text": " the scores. But the interesting thing to me is they started with let's each time. So even that", "tokens": [51388, 264, 13444, 13, 583, 264, 1880, 551, 281, 385, 307, 436, 1409, 365, 718, 311, 1184, 565, 13, 407, 754, 300, 51660], "temperature": 0.0, "avg_logprob": -0.07302775054142394, "compression_ratio": 1.6725352112676057, "no_speech_prob": 0.10084079951047897}, {"id": 225, "seek": 127256, "start": 1272.56, "end": 1277.6799999999998, "text": " first stage for the model might not yet be fully optimized. Maybe there's a prompt that doesn't", "tokens": [50364, 700, 3233, 337, 264, 2316, 1062, 406, 1939, 312, 4498, 26941, 13, 2704, 456, 311, 257, 12391, 300, 1177, 380, 50620], "temperature": 0.0, "avg_logprob": -0.08801005504749439, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.002889266237616539}, {"id": 226, "seek": 127256, "start": 1277.6799999999998, "end": 1282.8, "text": " begin with let's that improves this initial results still further. Anyway, back to the papers,", "tokens": [50620, 1841, 365, 718, 311, 300, 24771, 341, 5883, 3542, 920, 3052, 13, 5684, 11, 646, 281, 264, 10577, 11, 50876], "temperature": 0.0, "avg_logprob": -0.08801005504749439, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.002889266237616539}, {"id": 227, "seek": 127256, "start": 1282.8, "end": 1288.32, "text": " I know many people watching this will wonder if I read the paper boosting theory of mind performance", "tokens": [50876, 286, 458, 867, 561, 1976, 341, 486, 2441, 498, 286, 1401, 264, 3035, 43117, 5261, 295, 1575, 3389, 51152], "temperature": 0.0, "avg_logprob": -0.08801005504749439, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.002889266237616539}, {"id": 228, "seek": 127256, "start": 1288.32, "end": 1293.28, "text": " in large language models via prompting. And yes, I did because they tested something similar for a", "tokens": [51152, 294, 2416, 2856, 5245, 5766, 12391, 278, 13, 400, 2086, 11, 286, 630, 570, 436, 8246, 746, 2531, 337, 257, 51400], "temperature": 0.0, "avg_logprob": -0.08801005504749439, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.002889266237616539}, {"id": 229, "seek": 127256, "start": 1293.28, "end": 1298.32, "text": " theory of mind test. Using similar techniques, they were able to get theory of mind accuracy for", "tokens": [51400, 5261, 295, 1575, 1500, 13, 11142, 2531, 7512, 11, 436, 645, 1075, 281, 483, 5261, 295, 1575, 14170, 337, 51652], "temperature": 0.0, "avg_logprob": -0.08801005504749439, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.002889266237616539}, {"id": 230, "seek": 129832, "start": 1298.32, "end": 1305.2, "text": " GPT four from 80% to 100%. And they conclude that these results demonstrate that appropriate", "tokens": [50364, 26039, 51, 1451, 490, 4688, 4, 281, 2319, 6856, 400, 436, 16886, 300, 613, 3542, 11698, 300, 6854, 50708], "temperature": 0.0, "avg_logprob": -0.1015626650590163, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.01495393831282854}, {"id": 231, "seek": 129832, "start": 1305.2, "end": 1311.04, "text": " prompting enhances large language model theory of mind reasoning. And they underscore the context", "tokens": [50708, 12391, 278, 46628, 2416, 2856, 2316, 5261, 295, 1575, 21577, 13, 400, 436, 37556, 264, 4319, 51000], "temperature": 0.0, "avg_logprob": -0.1015626650590163, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.01495393831282854}, {"id": 232, "seek": 129832, "start": 1311.04, "end": 1316.24, "text": " dependent nature of these models cognitive capacities. They use that original prompt,", "tokens": [51000, 12334, 3687, 295, 613, 5245, 15605, 39396, 13, 814, 764, 300, 3380, 12391, 11, 51260], "temperature": 0.0, "avg_logprob": -0.1015626650590163, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.01495393831282854}, {"id": 233, "seek": 129832, "start": 1316.24, "end": 1322.24, "text": " let's think step by step, along with some few shot examples. Take a look at the GPT four table. And", "tokens": [51260, 718, 311, 519, 1823, 538, 1823, 11, 2051, 365, 512, 1326, 3347, 5110, 13, 3664, 257, 574, 412, 264, 26039, 51, 1451, 3199, 13, 400, 51560], "temperature": 0.0, "avg_logprob": -0.1015626650590163, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.01495393831282854}, {"id": 234, "seek": 129832, "start": 1322.24, "end": 1327.6799999999998, "text": " you can see how the let's think step by step improved the results dramatically. And as I", "tokens": [51560, 291, 393, 536, 577, 264, 718, 311, 519, 1823, 538, 1823, 9689, 264, 3542, 17548, 13, 400, 382, 286, 51832], "temperature": 0.0, "avg_logprob": -0.1015626650590163, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.01495393831282854}, {"id": 235, "seek": 132768, "start": 1327.68, "end": 1333.52, "text": " theorized earlier, adding few shot examples would push this still further. This is part of why I think", "tokens": [50364, 27423, 1602, 3071, 11, 5127, 1326, 3347, 5110, 576, 2944, 341, 920, 3052, 13, 639, 307, 644, 295, 983, 286, 519, 50656], "temperature": 0.0, "avg_logprob": -0.10365169926693565, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.0016481749480590224}, {"id": 236, "seek": 132768, "start": 1333.52, "end": 1340.24, "text": " that 95% barrier on the MMLU will be broken probably this year by GPT four, a few other", "tokens": [50656, 300, 13420, 4, 13357, 322, 264, 376, 12683, 52, 486, 312, 5463, 1391, 341, 1064, 538, 26039, 51, 1451, 11, 257, 1326, 661, 50992], "temperature": 0.0, "avg_logprob": -0.10365169926693565, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.0016481749480590224}, {"id": 237, "seek": 132768, "start": 1340.24, "end": 1345.92, "text": " points from this paper. They admit that there is not currently a theoretical understanding of why", "tokens": [50992, 2793, 490, 341, 3035, 13, 814, 9796, 300, 456, 307, 406, 4362, 257, 20864, 3701, 295, 983, 51276], "temperature": 0.0, "avg_logprob": -0.10365169926693565, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.0016481749480590224}, {"id": 238, "seek": 132768, "start": 1345.92, "end": 1351.1200000000001, "text": " these prompting techniques are beneficial. I've given you my theory and car pathies, but no one", "tokens": [51276, 613, 12391, 278, 7512, 366, 14072, 13, 286, 600, 2212, 291, 452, 5261, 293, 1032, 3100, 530, 11, 457, 572, 472, 51536], "temperature": 0.0, "avg_logprob": -0.10365169926693565, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.0016481749480590224}, {"id": 239, "seek": 132768, "start": 1351.1200000000001, "end": 1356.24, "text": " quite knows for sure. Lastly from this paper, and I found this really interesting, giving it generic", "tokens": [51536, 1596, 3255, 337, 988, 13, 18072, 490, 341, 3035, 11, 293, 286, 1352, 341, 534, 1880, 11, 2902, 309, 19577, 51792], "temperature": 0.0, "avg_logprob": -0.10365169926693565, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.0016481749480590224}, {"id": 240, "seek": 135624, "start": 1356.32, "end": 1362.64, "text": " few shot prompts that weren't directly theory of mind actually improve the outputs slightly more", "tokens": [50368, 1326, 3347, 41095, 300, 4999, 380, 3838, 5261, 295, 1575, 767, 3470, 264, 23930, 4748, 544, 50684], "temperature": 0.0, "avg_logprob": -0.045159663968873255, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.00609646737575531}, {"id": 241, "seek": 135624, "start": 1362.64, "end": 1368.32, "text": " than giving it direct theory of mind examples. This opens the door to the first of the five ways", "tokens": [50684, 813, 2902, 309, 2047, 5261, 295, 1575, 5110, 13, 639, 9870, 264, 2853, 281, 264, 700, 295, 264, 1732, 2098, 50968], "temperature": 0.0, "avg_logprob": -0.045159663968873255, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.00609646737575531}, {"id": 242, "seek": 135624, "start": 1368.32, "end": 1373.44, "text": " I anticipate smart GPT getting even smarter. It could be possible to come up with generic", "tokens": [50968, 286, 21685, 4069, 26039, 51, 1242, 754, 20294, 13, 467, 727, 312, 1944, 281, 808, 493, 365, 19577, 51224], "temperature": 0.0, "avg_logprob": -0.045159663968873255, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.00609646737575531}, {"id": 243, "seek": 135624, "start": 1373.44, "end": 1378.8, "text": " few shot prompts that could be automatically integrated into the model that don't necessarily", "tokens": [51224, 1326, 3347, 41095, 300, 727, 312, 6772, 10919, 666, 264, 2316, 300, 500, 380, 4725, 51492], "temperature": 0.0, "avg_logprob": -0.045159663968873255, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.00609646737575531}, {"id": 244, "seek": 135624, "start": 1378.8, "end": 1384.48, "text": " relate to the topic at hand. This graph shows the impact of adding few shot examples to GPT three.", "tokens": [51492, 10961, 281, 264, 4829, 412, 1011, 13, 639, 4295, 3110, 264, 2712, 295, 5127, 1326, 3347, 5110, 281, 26039, 51, 1045, 13, 51776], "temperature": 0.0, "avg_logprob": -0.045159663968873255, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.00609646737575531}, {"id": 245, "seek": 138448, "start": 1384.48, "end": 1390.96, "text": " And if this can be done in a generic way for GPT four, results could be improved still further.", "tokens": [50364, 400, 498, 341, 393, 312, 1096, 294, 257, 19577, 636, 337, 26039, 51, 1451, 11, 3542, 727, 312, 9689, 920, 3052, 13, 50688], "temperature": 0.0, "avg_logprob": -0.09882248531688344, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0005883564590476453}, {"id": 246, "seek": 138448, "start": 1390.96, "end": 1397.1200000000001, "text": " Next, the boosting theory of mind paper speculates that integrating some of these approaches could", "tokens": [50688, 3087, 11, 264, 43117, 5261, 295, 1575, 3035, 1608, 26192, 300, 26889, 512, 295, 613, 11587, 727, 50996], "temperature": 0.0, "avg_logprob": -0.09882248531688344, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0005883564590476453}, {"id": 247, "seek": 138448, "start": 1397.1200000000001, "end": 1404.0, "text": " boost the performance of weaker models to beyond the levels of GPT four zero shot accuracy. Next,", "tokens": [50996, 9194, 264, 3389, 295, 24286, 5245, 281, 4399, 264, 4358, 295, 26039, 51, 1451, 4018, 3347, 14170, 13, 3087, 11, 51340], "temperature": 0.0, "avg_logprob": -0.09882248531688344, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0005883564590476453}, {"id": 248, "seek": 138448, "start": 1404.0, "end": 1409.52, "text": " here is the original DERA paper that inspired me to have the researcher and resolver dialogue at", "tokens": [51340, 510, 307, 264, 3380, 413, 1598, 32, 3035, 300, 7547, 385, 281, 362, 264, 21751, 293, 34480, 10221, 412, 51616], "temperature": 0.0, "avg_logprob": -0.09882248531688344, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0005883564590476453}, {"id": 249, "seek": 140952, "start": 1409.52, "end": 1415.68, "text": " the end of smart GPT. As they say, the DERA approach shows significant improvement over base GPT", "tokens": [50364, 264, 917, 295, 4069, 26039, 51, 13, 1018, 436, 584, 11, 264, 413, 1598, 32, 3109, 3110, 4776, 10444, 670, 3096, 26039, 51, 50672], "temperature": 0.0, "avg_logprob": -0.07996057612555367, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.018542351201176643}, {"id": 250, "seek": 140952, "start": 1415.68, "end": 1420.4, "text": " four performance. And these were open ended questions, by the way, not multiple choice. So", "tokens": [50672, 1451, 3389, 13, 400, 613, 645, 1269, 4590, 1651, 11, 538, 264, 636, 11, 406, 3866, 3922, 13, 407, 50908], "temperature": 0.0, "avg_logprob": -0.07996057612555367, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.018542351201176643}, {"id": 251, "seek": 140952, "start": 1420.4, "end": 1424.96, "text": " this is more generally applicable than you might think. You can see from this table how results", "tokens": [50908, 341, 307, 544, 5101, 21142, 813, 291, 1062, 519, 13, 509, 393, 536, 490, 341, 3199, 577, 3542, 51136], "temperature": 0.0, "avg_logprob": -0.07996057612555367, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.018542351201176643}, {"id": 252, "seek": 140952, "start": 1424.96, "end": 1430.4, "text": " improved after engaging in this dialogue. And that brings me to the second way I anticipate smart", "tokens": [51136, 9689, 934, 11268, 294, 341, 10221, 13, 400, 300, 5607, 385, 281, 264, 1150, 636, 286, 21685, 4069, 51408], "temperature": 0.0, "avg_logprob": -0.07996057612555367, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.018542351201176643}, {"id": 253, "seek": 140952, "start": 1430.4, "end": 1436.32, "text": " GPT getting smarter in the future, a longer and more rich dialogue. At the moment, we have this", "tokens": [51408, 26039, 51, 1242, 20294, 294, 264, 2027, 11, 257, 2854, 293, 544, 4593, 10221, 13, 1711, 264, 1623, 11, 321, 362, 341, 51704], "temperature": 0.0, "avg_logprob": -0.07996057612555367, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.018542351201176643}, {"id": 254, "seek": 143632, "start": 1436.32, "end": 1443.04, "text": " simple researcher and resolver two step dialogue. I can imagine a council of advisors, you can imagine", "tokens": [50364, 2199, 21751, 293, 34480, 732, 1823, 10221, 13, 286, 393, 3811, 257, 9209, 295, 29136, 11, 291, 393, 3811, 50700], "temperature": 0.0, "avg_logprob": -0.07283326691272211, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.004069747403264046}, {"id": 255, "seek": 143632, "start": 1443.04, "end": 1448.3999999999999, "text": " a mathematician chipping in and a philosopher and a professor, each one tapping into slightly", "tokens": [50700, 257, 48281, 417, 6297, 294, 293, 257, 29805, 293, 257, 8304, 11, 1184, 472, 21444, 666, 4748, 50968], "temperature": 0.0, "avg_logprob": -0.07283326691272211, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.004069747403264046}, {"id": 256, "seek": 143632, "start": 1448.3999999999999, "end": 1454.32, "text": " different weights of GPT four, extracting more hidden expertise. I'm not saying that would", "tokens": [50968, 819, 17443, 295, 26039, 51, 1451, 11, 49844, 544, 7633, 11769, 13, 286, 478, 406, 1566, 300, 576, 51264], "temperature": 0.0, "avg_logprob": -0.07283326691272211, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.004069747403264046}, {"id": 257, "seek": 143632, "start": 1454.32, "end": 1459.84, "text": " transform the results, but it might edge them another few percent higher. Next, even with longer", "tokens": [51264, 4088, 264, 3542, 11, 457, 309, 1062, 4691, 552, 1071, 1326, 3043, 2946, 13, 3087, 11, 754, 365, 2854, 51540], "temperature": 0.0, "avg_logprob": -0.07283326691272211, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.004069747403264046}, {"id": 258, "seek": 143632, "start": 1459.84, "end": 1464.8799999999999, "text": " dialogues and different experts, we could find ways of optimizing these prompts just like we did", "tokens": [51540, 45551, 293, 819, 8572, 11, 321, 727, 915, 2098, 295, 40425, 613, 41095, 445, 411, 321, 630, 51792], "temperature": 0.0, "avg_logprob": -0.07283326691272211, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.004069747403264046}, {"id": 259, "seek": 146488, "start": 1464.88, "end": 1469.7600000000002, "text": " with the original let's think step by step. That's the third avenue of improvement that I envisaged", "tokens": [50364, 365, 264, 3380, 718, 311, 519, 1823, 538, 1823, 13, 663, 311, 264, 2636, 39230, 295, 10444, 300, 286, 2267, 271, 2980, 50608], "temperature": 0.0, "avg_logprob": -0.07662084867369454, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.02095639519393444}, {"id": 260, "seek": 146488, "start": 1469.7600000000002, "end": 1474.24, "text": " because I came up with these prompts, I'm sure they could be improved. Next, we could experiment", "tokens": [50608, 570, 286, 1361, 493, 365, 613, 41095, 11, 286, 478, 988, 436, 727, 312, 9689, 13, 3087, 11, 321, 727, 5120, 50832], "temperature": 0.0, "avg_logprob": -0.07662084867369454, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.02095639519393444}, {"id": 261, "seek": 146488, "start": 1474.24, "end": 1479.1200000000001, "text": " with different temperatures. Remember, a lower temperature makes the model more conservative,", "tokens": [50832, 365, 819, 12633, 13, 5459, 11, 257, 3126, 4292, 1669, 264, 2316, 544, 13780, 11, 51076], "temperature": 0.0, "avg_logprob": -0.07662084867369454, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.02095639519393444}, {"id": 262, "seek": 146488, "start": 1479.1200000000001, "end": 1484.24, "text": " a higher one towards one makes it more creative. We could experiment with a higher temperature", "tokens": [51076, 257, 2946, 472, 3030, 472, 1669, 309, 544, 5880, 13, 492, 727, 5120, 365, 257, 2946, 4292, 51332], "temperature": 0.0, "avg_logprob": -0.07662084867369454, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.02095639519393444}, {"id": 263, "seek": 146488, "start": 1484.24, "end": 1489.7600000000002, "text": " to produce a more diverse range of outputs at this stage, and then perhaps a more conservative,", "tokens": [51332, 281, 5258, 257, 544, 9521, 3613, 295, 23930, 412, 341, 3233, 11, 293, 550, 4317, 257, 544, 13780, 11, 51608], "temperature": 0.0, "avg_logprob": -0.07662084867369454, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.02095639519393444}, {"id": 264, "seek": 148976, "start": 1489.84, "end": 1495.28, "text": " deterministic temperature for the final judge or resolver. It might not work, but it's worth", "tokens": [50368, 15957, 3142, 4292, 337, 264, 2572, 6995, 420, 34480, 13, 467, 1062, 406, 589, 11, 457, 309, 311, 3163, 50640], "temperature": 0.0, "avg_logprob": -0.09479022026062012, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.035135071724653244}, {"id": 265, "seek": 148976, "start": 1495.28, "end": 1501.44, "text": " trying. And the fifth improvement I know would work, integrating APIs for character counting,", "tokens": [50640, 1382, 13, 400, 264, 9266, 10444, 286, 458, 576, 589, 11, 26889, 21445, 337, 2517, 13251, 11, 50948], "temperature": 0.0, "avg_logprob": -0.09479022026062012, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.035135071724653244}, {"id": 266, "seek": 148976, "start": 1501.44, "end": 1507.84, "text": " calculators, code interpreters, etc. Spending these weeks manually sorting through the outputs of GPT", "tokens": [50948, 4322, 3391, 11, 3089, 17489, 1559, 11, 5183, 13, 1738, 2029, 613, 3259, 16945, 32411, 807, 264, 23930, 295, 26039, 51, 51268], "temperature": 0.0, "avg_logprob": -0.09479022026062012, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.035135071724653244}, {"id": 267, "seek": 148976, "start": 1507.84, "end": 1513.44, "text": " four on these benchmarks, I can really see where it goes wrong. And it's often by getting letters", "tokens": [51268, 1451, 322, 613, 43751, 11, 286, 393, 534, 536, 689, 309, 1709, 2085, 13, 400, 309, 311, 2049, 538, 1242, 7825, 51548], "temperature": 0.0, "avg_logprob": -0.09479022026062012, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.035135071724653244}, {"id": 268, "seek": 148976, "start": 1513.44, "end": 1518.24, "text": " in the wrong order or making mistakes with division, it gets the high level logic right,", "tokens": [51548, 294, 264, 2085, 1668, 420, 1455, 8038, 365, 10044, 11, 309, 2170, 264, 1090, 1496, 9952, 558, 11, 51788], "temperature": 0.0, "avg_logprob": -0.09479022026062012, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.035135071724653244}, {"id": 269, "seek": 151824, "start": 1518.24, "end": 1523.2, "text": " and then makes quite simple errors. Basic tool integration would I am sure push the results", "tokens": [50364, 293, 550, 1669, 1596, 2199, 13603, 13, 31598, 2290, 10980, 576, 286, 669, 988, 2944, 264, 3542, 50612], "temperature": 0.0, "avg_logprob": -0.098083410805803, "compression_ratio": 1.5672131147540984, "no_speech_prob": 0.0064864810556173325}, {"id": 270, "seek": 151824, "start": 1523.2, "end": 1528.08, "text": " still higher. Now, I know this isn't my usual video. And trust me, I have been following the AI", "tokens": [50612, 920, 2946, 13, 823, 11, 286, 458, 341, 1943, 380, 452, 7713, 960, 13, 400, 3361, 385, 11, 286, 362, 668, 3480, 264, 7318, 50856], "temperature": 0.0, "avg_logprob": -0.098083410805803, "compression_ratio": 1.5672131147540984, "no_speech_prob": 0.0064864810556173325}, {"id": 271, "seek": 151824, "start": 1528.08, "end": 1533.1200000000001, "text": " news and we'll get back to that very soon. I'm determined to make those improvements and push", "tokens": [50856, 2583, 293, 321, 603, 483, 646, 281, 300, 588, 2321, 13, 286, 478, 9540, 281, 652, 729, 13797, 293, 2944, 51108], "temperature": 0.0, "avg_logprob": -0.098083410805803, "compression_ratio": 1.5672131147540984, "no_speech_prob": 0.0064864810556173325}, {"id": 272, "seek": 151824, "start": 1533.1200000000001, "end": 1539.6, "text": " smart GBT even further. But of course, that will be aided massively by getting access to the plugins", "tokens": [51108, 4069, 26809, 51, 754, 3052, 13, 583, 295, 1164, 11, 300, 486, 312, 257, 2112, 29379, 538, 1242, 2105, 281, 264, 33759, 51432], "temperature": 0.0, "avg_logprob": -0.098083410805803, "compression_ratio": 1.5672131147540984, "no_speech_prob": 0.0064864810556173325}, {"id": 273, "seek": 151824, "start": 1539.6, "end": 1545.44, "text": " and the GPT four API key. So far, I've had to do all of this manually, which was a lot of work.", "tokens": [51432, 293, 264, 26039, 51, 1451, 9362, 2141, 13, 407, 1400, 11, 286, 600, 632, 281, 360, 439, 295, 341, 16945, 11, 597, 390, 257, 688, 295, 589, 13, 51724], "temperature": 0.0, "avg_logprob": -0.098083410805803, "compression_ratio": 1.5672131147540984, "no_speech_prob": 0.0064864810556173325}, {"id": 274, "seek": 154544, "start": 1545.44, "end": 1550.96, "text": " Now, as you saw earlier, I have drawn on GPT four to help me develop a program in replete to", "tokens": [50364, 823, 11, 382, 291, 1866, 3071, 11, 286, 362, 10117, 322, 26039, 51, 1451, 281, 854, 385, 1499, 257, 1461, 294, 3248, 3498, 281, 50640], "temperature": 0.0, "avg_logprob": -0.07738141544529649, "compression_ratio": 1.5874587458745875, "no_speech_prob": 0.0023962182458490133}, {"id": 275, "seek": 154544, "start": 1550.96, "end": 1556.72, "text": " automate this process. But at the moment, it's GPT 3.5. And honestly, the context window really", "tokens": [50640, 31605, 341, 1399, 13, 583, 412, 264, 1623, 11, 309, 311, 26039, 51, 805, 13, 20, 13, 400, 6095, 11, 264, 4319, 4910, 534, 50928], "temperature": 0.0, "avg_logprob": -0.07738141544529649, "compression_ratio": 1.5874587458745875, "no_speech_prob": 0.0023962182458490133}, {"id": 276, "seek": 154544, "start": 1556.72, "end": 1561.92, "text": " limits the ability. But I do look forward to the day when I can integrate GPT four and put this out", "tokens": [50928, 10406, 264, 3485, 13, 583, 286, 360, 574, 2128, 281, 264, 786, 562, 286, 393, 13365, 26039, 51, 1451, 293, 829, 341, 484, 51188], "temperature": 0.0, "avg_logprob": -0.07738141544529649, "compression_ratio": 1.5874587458745875, "no_speech_prob": 0.0023962182458490133}, {"id": 277, "seek": 154544, "start": 1561.92, "end": 1566.88, "text": " as an automatic model for people to test and play about with. I'm sure that something similar will", "tokens": [51188, 382, 364, 12509, 2316, 337, 561, 281, 1500, 293, 862, 466, 365, 13, 286, 478, 988, 300, 746, 2531, 486, 51436], "temperature": 0.0, "avg_logprob": -0.07738141544529649, "compression_ratio": 1.5874587458745875, "no_speech_prob": 0.0023962182458490133}, {"id": 278, "seek": 154544, "start": 1566.88, "end": 1573.3600000000001, "text": " ultimately be incorporated by open AI itself, maybe as a thoughtful mode or smart mode, a bit", "tokens": [51436, 6284, 312, 21654, 538, 1269, 7318, 2564, 11, 1310, 382, 257, 21566, 4391, 420, 4069, 4391, 11, 257, 857, 51760], "temperature": 0.0, "avg_logprob": -0.07738141544529649, "compression_ratio": 1.5874587458745875, "no_speech_prob": 0.0023962182458490133}, {"id": 279, "seek": 157336, "start": 1573.36, "end": 1578.8799999999999, "text": " like Bing has creative, precise balance, etc. Each response does take longer. But as you've seen,", "tokens": [50364, 411, 30755, 575, 5880, 11, 13600, 4772, 11, 5183, 13, 6947, 4134, 775, 747, 2854, 13, 583, 382, 291, 600, 1612, 11, 50640], "temperature": 0.0, "avg_logprob": -0.08775921829608309, "compression_ratio": 1.65625, "no_speech_prob": 0.02160567417740822}, {"id": 280, "seek": 157336, "start": 1578.8799999999999, "end": 1585.12, "text": " the outputs are noticeably better. If the results of models like this one do officially exceed the", "tokens": [50640, 264, 23930, 366, 3449, 1188, 1101, 13, 759, 264, 3542, 295, 5245, 411, 341, 472, 360, 12053, 14048, 264, 50952], "temperature": 0.0, "avg_logprob": -0.08775921829608309, "compression_ratio": 1.65625, "no_speech_prob": 0.02160567417740822}, {"id": 281, "seek": 157336, "start": 1585.12, "end": 1592.08, "text": " 86.4% that open AI talked about in the GPT four technical report, I do think that would reveal", "tokens": [50952, 26687, 13, 19, 4, 300, 1269, 7318, 2825, 466, 294, 264, 26039, 51, 1451, 6191, 2275, 11, 286, 360, 519, 300, 576, 10658, 51300], "temperature": 0.0, "avg_logprob": -0.08775921829608309, "compression_ratio": 1.65625, "no_speech_prob": 0.02160567417740822}, {"id": 282, "seek": 157336, "start": 1592.08, "end": 1597.28, "text": " quite a few things. First, the open AI isn't even aware of the full capabilities of its own", "tokens": [51300, 1596, 257, 1326, 721, 13, 2386, 11, 264, 1269, 7318, 1943, 380, 754, 3650, 295, 264, 1577, 10862, 295, 1080, 1065, 51560], "temperature": 0.0, "avg_logprob": -0.08775921829608309, "compression_ratio": 1.65625, "no_speech_prob": 0.02160567417740822}, {"id": 283, "seek": 157336, "start": 1597.28, "end": 1602.08, "text": " model. I don't even know if they anticipated things like auto GPT. I do think it would reveal", "tokens": [51560, 2316, 13, 286, 500, 380, 754, 458, 498, 436, 23267, 721, 411, 8399, 26039, 51, 13, 286, 360, 519, 309, 576, 10658, 51800], "temperature": 0.0, "avg_logprob": -0.08775921829608309, "compression_ratio": 1.65625, "no_speech_prob": 0.02160567417740822}, {"id": 284, "seek": 160208, "start": 1602.08, "end": 1606.6399999999999, "text": " that they need to do far more proper testing of their models before they release them. They should", "tokens": [50364, 300, 436, 643, 281, 360, 1400, 544, 2296, 4997, 295, 641, 5245, 949, 436, 4374, 552, 13, 814, 820, 50592], "temperature": 0.0, "avg_logprob": -0.06779355236462184, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.02032754011452198}, {"id": 285, "seek": 160208, "start": 1606.6399999999999, "end": 1612.08, "text": " make falsifiable predictions about what their models won't be capable of. That way we would", "tokens": [50592, 652, 16720, 30876, 21264, 466, 437, 641, 5245, 1582, 380, 312, 8189, 295, 13, 663, 636, 321, 576, 50864], "temperature": 0.0, "avg_logprob": -0.06779355236462184, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.02032754011452198}, {"id": 286, "seek": 160208, "start": 1612.08, "end": 1616.8799999999999, "text": " know just how much they know about their own models. What we're trying to avoid is a situation", "tokens": [50864, 458, 445, 577, 709, 436, 458, 466, 641, 1065, 5245, 13, 708, 321, 434, 1382, 281, 5042, 307, 257, 2590, 51104], "temperature": 0.0, "avg_logprob": -0.06779355236462184, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.02032754011452198}, {"id": 287, "seek": 160208, "start": 1616.8799999999999, "end": 1621.52, "text": " where open AI say their model can only achieve X. And then when they release the model in the", "tokens": [51104, 689, 1269, 7318, 584, 641, 2316, 393, 787, 4584, 1783, 13, 400, 550, 562, 436, 4374, 264, 2316, 294, 264, 51336], "temperature": 0.0, "avg_logprob": -0.06779355236462184, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.02032754011452198}, {"id": 288, "seek": 160208, "start": 1621.52, "end": 1627.4399999999998, "text": " wild, someone comes along and achieves Y, where Y is much more impactful than X. So those were the", "tokens": [51336, 4868, 11, 1580, 1487, 2051, 293, 3538, 977, 398, 11, 689, 398, 307, 709, 544, 30842, 813, 1783, 13, 407, 729, 645, 264, 51632], "temperature": 0.0, "avg_logprob": -0.06779355236462184, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.02032754011452198}, {"id": 289, "seek": 162744, "start": 1627.44, "end": 1632.56, "text": " goals of this video to show you how to get more out of GPT four to run you through some of the", "tokens": [50364, 5493, 295, 341, 960, 281, 855, 291, 577, 281, 483, 544, 484, 295, 26039, 51, 1451, 281, 1190, 291, 807, 512, 295, 264, 50620], "temperature": 0.0, "avg_logprob": -0.06032103006957007, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.3995171785354614}, {"id": 290, "seek": 162744, "start": 1632.56, "end": 1637.1200000000001, "text": " fascinating papers that have been released in the last few days and weeks. The third goal was to", "tokens": [50620, 10343, 10577, 300, 362, 668, 4736, 294, 264, 1036, 1326, 1708, 293, 3259, 13, 440, 2636, 3387, 390, 281, 50848], "temperature": 0.0, "avg_logprob": -0.06032103006957007, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.3995171785354614}, {"id": 291, "seek": 162744, "start": 1637.1200000000001, "end": 1642.16, "text": " show you what this model could do with some official benchmarks and suggest ways it might get better", "tokens": [50848, 855, 291, 437, 341, 2316, 727, 360, 365, 512, 4783, 43751, 293, 3402, 2098, 309, 1062, 483, 1101, 51100], "temperature": 0.0, "avg_logprob": -0.06032103006957007, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.3995171785354614}, {"id": 292, "seek": 162744, "start": 1642.16, "end": 1647.92, "text": " in the near term future. Of course, if you have a GPT four API key or are an expert in benchmarking", "tokens": [51100, 294, 264, 2651, 1433, 2027, 13, 2720, 1164, 11, 498, 291, 362, 257, 26039, 51, 1451, 9362, 2141, 420, 366, 364, 5844, 294, 18927, 278, 51388], "temperature": 0.0, "avg_logprob": -0.06032103006957007, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.3995171785354614}, {"id": 293, "seek": 162744, "start": 1647.92, "end": 1652.88, "text": " systems like GPT four, I'd love to hear from you. I guess the final goal was to perhaps suggest to you", "tokens": [51388, 3652, 411, 26039, 51, 1451, 11, 286, 1116, 959, 281, 1568, 490, 291, 13, 286, 2041, 264, 2572, 3387, 390, 281, 4317, 3402, 281, 291, 51636], "temperature": 0.0, "avg_logprob": -0.06032103006957007, "compression_ratio": 1.7429577464788732, "no_speech_prob": 0.3995171785354614}, {"id": 294, "seek": 165288, "start": 1652.88, "end": 1657.7600000000002, "text": " that open AI don't know as much about their own models as they might lead you to believe.", "tokens": [50364, 300, 1269, 7318, 500, 380, 458, 382, 709, 466, 641, 1065, 5245, 382, 436, 1062, 1477, 291, 281, 1697, 13, 50608], "temperature": 0.0, "avg_logprob": -0.11694546937942504, "compression_ratio": 1.2764227642276422, "no_speech_prob": 0.3062334358692169}, {"id": 295, "seek": 165288, "start": 1657.7600000000002, "end": 1661.1200000000001, "text": " Thank you so much for watching to the end and have a wonderful day.", "tokens": [50608, 1044, 291, 370, 709, 337, 1976, 281, 264, 917, 293, 362, 257, 3715, 786, 13, 50776], "temperature": 0.0, "avg_logprob": -0.11694546937942504, "compression_ratio": 1.2764227642276422, "no_speech_prob": 0.3062334358692169}], "language": "en"}