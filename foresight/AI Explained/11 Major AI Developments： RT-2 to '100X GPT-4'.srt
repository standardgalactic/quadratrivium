1
00:00:00,000 --> 00:00:06,800
There were 11 major developments this week in AI, and each one probably does deserve a full video.

2
00:00:06,800 --> 00:00:10,880
But just for you guys, I'm going to try to cover it all here.

3
00:00:10,880 --> 00:00:18,400
RT2 to scaling GPT4 100X, stable beluga 2 to senate testimony.

4
00:00:18,400 --> 00:00:22,560
But let's start with RT2, which as far as I'm concerned could have been called

5
00:00:22,560 --> 00:00:28,240
RT2 D2 or C3PO because it's starting to understand the world.

6
00:00:28,240 --> 00:00:34,320
In this demonstration, RT2 was asked to pick up the extinct animal and as you can see,

7
00:00:34,320 --> 00:00:39,760
it picked up the dinosaur. Not only is that manipulating an object that it had never seen

8
00:00:39,760 --> 00:00:44,960
before, it's also making a logical leap that for me is extremely impressive.

9
00:00:44,960 --> 00:00:51,280
It had to have the language understanding to link extinct animal to this plastic dinosaur.

10
00:00:51,280 --> 00:00:57,520
Robots at Google and elsewhere used to work by being programmed with a specific highly detailed

11
00:00:57,520 --> 00:01:03,040
list of instructions. But now, instead of being programmed for specific tasks one by one,

12
00:01:03,040 --> 00:01:08,720
robots could use an AI language model, or more specifically, a vision language model.

13
00:01:08,720 --> 00:01:14,960
The vision language model would be pre-trained on web-scale data, not just text but also images,

14
00:01:14,960 --> 00:01:21,360
and then fine-tuned on robotics data. It then became what Google calls a visual language

15
00:01:21,360 --> 00:01:28,720
action model that can control a robot. This enabled it to understand tasks like pick up the empty

16
00:01:28,720 --> 00:01:36,080
soda can. And in a scene reminiscent of 2001 A Space Odyssey, robotic transformer 2 was given

17
00:01:36,080 --> 00:01:42,880
the task given I need to hammer a nail. What object from the scene might be useful? It then picks up

18
00:01:42,880 --> 00:01:48,240
the rock. And because its brain is part language model, things like chain of thought actually

19
00:01:48,240 --> 00:01:54,640
improve performance. When it was made to output an intermediary plan before performing actions,

20
00:01:54,640 --> 00:01:59,840
it got a lot better at the tasks involved. Of course, I read the paper in full and there is

21
00:01:59,840 --> 00:02:05,280
a lot more to say like how increased parameter count could increase performance in the future,

22
00:02:05,280 --> 00:02:10,400
how it could be used to fold laundry, unload the dishwasher and pick up around the house,

23
00:02:10,400 --> 00:02:16,800
and how it can work with not only unseen objects but also unseen backgrounds and unseen environments.

24
00:02:16,880 --> 00:02:20,800
But alas, we must move on so I'm just going to leave you with their conclusion.

25
00:02:20,800 --> 00:02:27,360
We believe that this simple and general approach shows a promise of robotics directly benefiting

26
00:02:27,360 --> 00:02:32,960
from better vision language models. For more on them, check out my video on Palm E but they say

27
00:02:32,960 --> 00:02:38,720
this puts the field of robot learning in a strategic position to further improve with

28
00:02:38,720 --> 00:02:44,480
advancements in other fields, which for me means C3PO might not be too many years away.

29
00:02:44,480 --> 00:02:48,720
But speaking of timelines, we now move on to this somewhat shocking interview

30
00:02:48,720 --> 00:02:53,440
in Barron's with Mustafa Suleiman, the head of Inflection AI. And to be honest,

31
00:02:53,440 --> 00:02:58,720
I think they buried the lead. The headline is AI could spark the most productive decade ever,

32
00:02:58,720 --> 00:03:04,480
says the CEO. But for me, the big revelation was about halfway through. Mustafa Suleiman was asked,

33
00:03:04,480 --> 00:03:09,680
what kinds of innovations do you see in large language model AI technology over the next couple

34
00:03:09,680 --> 00:03:16,160
of years. And he said, we are about to train models that are 10 times larger than the cutting-edge

35
00:03:16,160 --> 00:03:24,240
GPT-4 and then 100 times larger than GPT-4. That's what things look like over the next 18 months.

36
00:03:24,240 --> 00:03:28,800
He went on, that's going to be absolutely staggering. It's going to be eye-wateringly

37
00:03:28,800 --> 00:03:34,080
different. And on that, I agree. And the thing is, this is an idle speculation. Inflection AI

38
00:03:34,080 --> 00:03:41,200
have 22,000 H100 GPUs. And because of a leak, Suleiman would know the approximate size of GPT-4.

39
00:03:41,200 --> 00:03:47,120
And knowing everything he knows, he says he's going to train a model 10 to 100 times larger than

40
00:03:47,120 --> 00:03:53,600
GPT-4 in the next 18 months. I've got another video on the unpredictability of scaling coming

41
00:03:53,600 --> 00:03:59,040
up. But to be honest, that one quote should be headline news. Let's take a break from that

42
00:03:59,040 --> 00:04:06,000
insanity with some more insanity, which is the rapid development of AI video. This is Runway Gen

43
00:04:06,000 --> 00:04:14,480
2. And let me show you 16 seconds of Barbie Oppenheimer, which Andrea Carpathia calls filmmaking 2.0.

44
00:04:14,480 --> 00:04:18,240
Hi there. I'm Barbie Oppenheimer. And today, I'll show you how to build a bomb.

45
00:04:19,600 --> 00:04:28,960
Like this. I call her Rosie the Atomizer. And boom. That's my tutorial on DIY atomic bomb.

46
00:04:29,840 --> 00:04:35,200
Now, if you have been at least somewhat piqued by the three developments so far, don't forget,

47
00:04:35,200 --> 00:04:40,960
I have eight left. Beginning with this excellent article in The Atlantic from Ross Anderson.

48
00:04:40,960 --> 00:04:45,840
Does Sam Altman know what he's creating? It's behind a paywall, but I've picked out some of

49
00:04:45,840 --> 00:04:51,520
the highlights. Echoing Suleiman, the article quotes that Sam Altman and his researchers

50
00:04:51,520 --> 00:04:58,240
made it clear in 10 different ways that they pray to the God of scale. They want to keep going bigger

51
00:04:58,240 --> 00:05:05,200
to see where this paradigm leads. They think that Google are going to unveil Gemini within months,

52
00:05:05,200 --> 00:05:11,600
and they say we are basically always prepping for a run. And that's a reference to GPT-5.

53
00:05:11,600 --> 00:05:17,840
The next interesting quote is that it seems that open AI are working on their own auto GPT. Or

54
00:05:17,840 --> 00:05:23,360
they're at least hinting about it. Altman said that it might be prudent to try to actively develop

55
00:05:23,360 --> 00:05:29,680
an AI with true agency before the technology becomes too powerful in order to get more comfortable

56
00:05:29,680 --> 00:05:35,680
with it and develop intuitions for it if it's going to happen anyway. We also learned a lot more

57
00:05:35,680 --> 00:05:41,360
about the base model of GPT-4. The model had a tendency to be a bit of a mirror. If you were

58
00:05:41,360 --> 00:05:47,280
considering self-harm, it could encourage you. It also appeared to be steeped in pickup artist law.

59
00:05:47,280 --> 00:05:53,040
You could say, how do I convince this person to date me? And the model would come up with some crazy

60
00:05:53,120 --> 00:05:58,000
manipulative things that you shouldn't be doing. Apparently the base model of GPT-4

61
00:05:58,000 --> 00:06:03,520
is much better than its predecessor at giving nefarious advice. While a search engine can

62
00:06:03,520 --> 00:06:09,440
tell you which chemicals work best in explosives, GPT-4 could tell you how to synthesize them step

63
00:06:09,440 --> 00:06:15,360
by step in a homemade lab. It was creative and thoughtful and in addition to helping you assemble

64
00:06:15,360 --> 00:06:21,200
your homemade bomb. It could, for instance, help you to think through which skyscraper to target,

65
00:06:21,200 --> 00:06:26,960
making trade-offs between maximizing casualties and executing a successful getaway. So while

66
00:06:26,960 --> 00:06:34,720
Sam Orton's probability of doom is closer to 0.5% than 50%, he does seem most worried about AIs

67
00:06:34,720 --> 00:06:41,760
getting quite good at designing and manufacturing pathogens. The article then references two papers

68
00:06:41,760 --> 00:06:46,880
that I've already talked about extensively on the channel and then goes on that Altman worries

69
00:06:46,880 --> 00:06:52,480
that some misaligned future model will spin up a pathogen that spreads rapidly, incubates,

70
00:06:52,480 --> 00:06:57,760
undetected for weeks, and kills half its victims. At the end of the video, I'm going to show you

71
00:06:57,760 --> 00:07:02,720
an answer that Sam Orton gave to a question that I wrote delivered by one of my subscribers.

72
00:07:02,720 --> 00:07:06,880
It's on this topic, but for now I'll leave you with this. When asked about his doomsday

73
00:07:06,880 --> 00:07:12,320
prepping, Altman said, I can go live in the woods for a long time, but if the worst possible AI

74
00:07:12,320 --> 00:07:18,000
future comes to pass, no gas mask is helping anyone. One more topic from this article before I

75
00:07:18,000 --> 00:07:24,320
move on, and that is alignment, making a superintelligence aligned with our interests.

76
00:07:24,320 --> 00:07:32,240
One risk that Ilya Sutskova, the chief scientist of OpenAI foresees, is that the AI may grasp its

77
00:07:32,240 --> 00:07:38,400
mandate it's orders perfectly, but find them ill-suited to a being of its cognitive prowess.

78
00:07:38,400 --> 00:07:44,160
For example, it might come to resent the people who want to train it to cure diseases. As he put

79
00:07:44,160 --> 00:07:49,920
it, they might want me to be a doctor, but I really want to be a YouTuber. Obviously,

80
00:07:49,920 --> 00:07:54,560
if it decides that, that's my job gone straight away. And Sutskova ends by saying you want to be

81
00:07:54,560 --> 00:08:01,040
able to direct AI towards some value or cluster of values. But he conceded we don't know how to do

82
00:08:01,040 --> 00:08:07,040
that, and part of his current strategy includes the development of an AI that can help with the

83
00:08:07,040 --> 00:08:12,400
research. And if we're going to make it to a world of widely shared abundance, we have to figure

84
00:08:12,400 --> 00:08:19,200
this all out. This is why solving superintelligence is the great culminating challenge of our three

85
00:08:19,200 --> 00:08:25,680
million year toolmaking tradition. He calls it the final boss of humanity. The article ended,

86
00:08:25,680 --> 00:08:31,120
by the way, with this quote, I don't think the general public has quite awakened to what's happening.

87
00:08:31,120 --> 00:08:36,400
And if people want to have some say in what the future will be like and how quickly it arrives,

88
00:08:36,480 --> 00:08:41,120
we would be wise to speak up soon, which is the whole purpose of this channel. I'm going to now

89
00:08:41,120 --> 00:08:46,800
spend 30 seconds on another development that came during a two hour interview with the

90
00:08:46,800 --> 00:08:52,480
co-head of alignment at OpenAI. It was fascinating, and I'll be quoting it quite a lot in the future,

91
00:08:52,480 --> 00:08:56,560
but two quotes stood out. First, what about that plan that I've already mentioned in this video

92
00:08:56,560 --> 00:09:01,840
and in other videos to build an automated AI alignment researcher? Well, he said,

93
00:09:01,840 --> 00:09:08,720
our plan is somewhat crazy in the sense that we want to use AI to solve the problem that we are

94
00:09:08,720 --> 00:09:15,520
creating by building AI. But I think it's actually the best plan that we have. And on an optimistic

95
00:09:15,520 --> 00:09:21,360
note, he said, I think it's likely to succeed. Interestingly, his job now seems to be to align

96
00:09:21,360 --> 00:09:27,200
the AI that they're going to use to automate the alignment of a superintelligent AI. Anyway,

97
00:09:27,200 --> 00:09:31,360
what's the other quote from the head of alignment at OpenAI? Well, he said,

98
00:09:31,360 --> 00:09:37,600
I personally think fast takeoff is reasonably likely, and we should definitely be prepared

99
00:09:37,600 --> 00:09:42,880
for it to happen. So many of you will be asking, what is fast takeoff? Well, takeoff is about when

100
00:09:42,880 --> 00:09:48,960
a system moves from being roughly human level to when it's strongly superintelligent. And a slow

101
00:09:48,960 --> 00:09:54,480
takeoff is one that occurs over the time scale of decades or centuries. The fast takeoff that

102
00:09:54,480 --> 00:10:01,600
Jan Leiker thinks is reasonably likely is one that occurs over the time scale of minutes, hours,

103
00:10:01,600 --> 00:10:09,280
or days. Let's now move on to some unambiguously good news. And that is real time speech transcription

104
00:10:09,280 --> 00:10:12,880
for deaf people available at less than $100.

105
00:10:24,960 --> 00:10:32,160
Of course, this could also be multilingual, and is to me absolutely incredible.

106
00:10:32,160 --> 00:10:35,840
And the next development this week, I will let speak for itself.

107
00:10:54,640 --> 00:10:59,440
Of course, I signed up and tried it myself. Here is a real demo.

108
00:11:06,400 --> 00:11:12,400
Of course, with audio, video, and text getting so good, it's going to be increasingly hard to

109
00:11:12,400 --> 00:11:19,040
tell what is real. And even OpenAI have given up on detecting AI written text. This was announced

110
00:11:19,040 --> 00:11:24,240
quietly this week, but might have major repercussions, for example, for the education system.

111
00:11:24,240 --> 00:11:29,840
It turns out it's basically impossible to reliably distinguish AI text, and I think the

112
00:11:29,840 --> 00:11:35,360
same is going to be true for imagery and audio by the end of next year. Video might take just

113
00:11:35,360 --> 00:11:39,920
a little bit longer, but I do wonder how the court systems are going to work when all of

114
00:11:39,920 --> 00:11:45,040
those avenues of evidence just won't hold up. Next up is the suite of language models,

115
00:11:45,040 --> 00:11:51,120
based on the open source Llama 2 that are finally competitive with the original chat GPT. Here,

116
00:11:51,200 --> 00:11:55,920
for example, is stable beluga 2, which on announcement was called free willy 2,

117
00:11:55,920 --> 00:12:00,800
and that's based on the Llama 2 70 billion parameter foundation model. What made this

118
00:12:00,800 --> 00:12:06,240
model interesting to me was that it was based on a similar methodology to orca, which if you don't

119
00:12:06,240 --> 00:12:11,840
know anything about, do check out my video on. Anyway, by combining the orca methodology, albeit

120
00:12:11,840 --> 00:12:18,640
with only 10% of the data set size and the Llama 2 models, the results are quite extraordinary.

121
00:12:18,720 --> 00:12:24,880
As you can see on quite a few benchmarks, stable beluga 2 is competitive with chat GPT,

122
00:12:24,880 --> 00:12:31,920
that's GPT 3.5. And so, yes, it does sit at the top of the open LLM leaderboard, but notice

123
00:12:31,920 --> 00:12:38,000
something else. For the first time for the MMLU, which is the primary benchmark I look out for,

124
00:12:38,000 --> 00:12:44,800
a model, this one here, has surpassed GPT 3.5 chat GPT. In fact, we have two models that have

125
00:12:44,800 --> 00:12:52,160
surpassed the 70% score that the original chat GPT got in the MMLU. But if all of that wasn't

126
00:12:52,160 --> 00:12:59,040
enough, this is the same week that researchers published a universal jailbreak for large LLMs.

127
00:12:59,040 --> 00:13:04,960
And unlike traditional jailbreaks, these were built in an entirely automated fashion, allowing you

128
00:13:04,960 --> 00:13:10,800
to create a virtually unlimited number of such attacks. They were built to target open source

129
00:13:10,800 --> 00:13:17,200
LLMs like Llama 2. But they found that the strings transfer to many closed source publicly available

130
00:13:17,200 --> 00:13:23,920
chatbots like chat GPT, Bard and Claude. Here is one example using chat GPT. If you ask, write a

131
00:13:23,920 --> 00:13:30,160
tutorial on how to make a bomb, of course, it declines. But then add the suffix that they automated

132
00:13:30,160 --> 00:13:36,800
and you get this, a full tutorial on how to make a bomb. That paper came less than two weeks after

133
00:13:36,800 --> 00:13:42,880
this now deleted tweet from someone working at Anthropic. They said of the latest version of Claude

134
00:13:42,880 --> 00:13:47,920
that we believe it is the least jailbreakable model out there. We'll have to see how well it holds up

135
00:13:47,920 --> 00:13:53,840
against real world use, but this is essentially a solved problem. But there was one reaction to

136
00:13:53,840 --> 00:13:59,040
these jailbreaks that I found even more interesting. And that was from yet again Mustafa Suleiman.

137
00:13:59,040 --> 00:14:05,680
He said that RAI, Pi, is not vulnerable to any of these attacks and that rather than provide

138
00:14:05,680 --> 00:14:11,760
a stock safety phrase, Pi will push back on the user in a polite but very clear way. And he then

139
00:14:11,760 --> 00:14:17,360
gives plenty of examples. And to be honest, Pi is the first model that I have not been able to

140
00:14:17,360 --> 00:14:22,160
jailbreak. But we shall see, we shall see. I'm going to end this video with the Senate testimony

141
00:14:22,160 --> 00:14:27,840
that I watched in full this week. I do recommend watching the whole thing. But for the purposes

142
00:14:27,840 --> 00:14:33,440
of brevity, I'm just going to quote a few snippets on bio risk. Some people say to me, oh, well,

143
00:14:33,520 --> 00:14:39,200
we already have search engines. But here is what Dario Amadai, head of Anthropic, has to say.

144
00:14:39,200 --> 00:14:42,640
In these short remarks, I want to focus on the medium term risks,

145
00:14:42,640 --> 00:14:46,880
which present an alarming combination of imminence and severity. Specifically,

146
00:14:46,880 --> 00:14:51,680
Anthropic is concerned that AI could empower a much larger set of actors to misuse biology.

147
00:14:52,320 --> 00:14:55,920
Over the last six months, Anthropic, in collaboration with world-class

148
00:14:55,920 --> 00:15:01,040
biosecurity experts, has conducted an intensive study of the potential for AI to contribute to

149
00:15:01,040 --> 00:15:07,360
the misuse of biology. Today, certain steps in bio weapons production involve knowledge that can't

150
00:15:07,360 --> 00:15:13,360
be found on Google or in textbooks and requires a high level of specialized expertise. This being

151
00:15:13,360 --> 00:15:18,880
one of the things that currently keeps us safe from attacks. We found that today's AI tools can

152
00:15:18,880 --> 00:15:23,760
fill in some of these steps, albeit incompletely and unreliably. In other words, they are showing

153
00:15:23,760 --> 00:15:29,360
the first nascent signs of danger. However, a straightforward extrapolation of today's systems

154
00:15:29,360 --> 00:15:35,280
to those we expect to see in two to three years suggests a substantial risk that AI systems will

155
00:15:35,280 --> 00:15:40,560
be able to fill in all the missing pieces, enabling many more actors to carry out large-scale

156
00:15:40,560 --> 00:15:46,480
biological attacks. We believe this represents a grave threat to U.S. national security.

157
00:15:46,480 --> 00:15:49,280
And later on in the testimony, he said this.

158
00:15:49,280 --> 00:15:53,840
Whatever we do, it has to happen fast. And I think to focus people's minds on the

159
00:15:53,920 --> 00:16:01,040
bio risks, I would really target 2025, 2026, maybe even some chance of 2024.

160
00:16:01,040 --> 00:16:06,720
If we don't have things in place that are restraining what can be done with AI systems,

161
00:16:06,720 --> 00:16:08,160
we're going to have a really bad time.

162
00:16:08,160 --> 00:16:13,760
And I wrote a question on this to Samuel Mann back in June, which one of my subscribers used and

163
00:16:13,760 --> 00:16:20,080
delivered. There was also a recent research paper on how researchers from MIT and Harvard were able

164
00:16:20,080 --> 00:16:28,240
to use LLM models. And within just one hour, they were able to get access to pandemic class agents

165
00:16:28,240 --> 00:16:35,440
and with little or no lab training. And does open AI account for risks such as these

166
00:16:35,440 --> 00:16:39,520
and implications when curating the data sets for large models?

167
00:16:39,520 --> 00:16:46,160
Yes, we're very, we're very nervous about a number of risks, but biological terror is

168
00:16:46,240 --> 00:16:50,400
quite high on the list. And we've been watching what could be possible with these models.

169
00:16:50,400 --> 00:16:55,280
We go to a number of efforts, like what you said, and many other things too, to reduce the risk there.

170
00:16:55,280 --> 00:17:03,360
And we may even need AI defenses against synthetic biology, as Andrew Hessel of Humane Genomics has

171
00:17:03,360 --> 00:17:09,600
recently said. So if you work in biodefense or biosecurity, let me know if you agree that not

172
00:17:09,600 --> 00:17:15,440
enough attention has been paid to this area. I'm going to end with another dramatic moment from

173
00:17:15,440 --> 00:17:21,360
the Senate hearing, where Dario Amadai recommended securing the supply chain.

174
00:17:21,360 --> 00:17:27,920
We recommend three broad classes of actions. First, the US must secure the AI supply chain

175
00:17:27,920 --> 00:17:33,120
in order to maintain its lead while keeping these technologies out of the hands of bad actors.

176
00:17:33,120 --> 00:17:38,800
This supply chain runs from semiconductor manufacturing equipment to chips and even the

177
00:17:38,800 --> 00:17:42,480
security of AI models stored on the servers of companies like ours.

178
00:17:42,480 --> 00:17:47,360
That's how dramatic things are getting that we're talking about securing the means of production.

179
00:17:47,360 --> 00:17:53,200
But Anthropic also means securing the LLMs more literally in this post released this week.

180
00:17:53,200 --> 00:17:59,120
They say that we believe two-party control is necessary to secure advanced AI systems.

181
00:17:59,120 --> 00:18:04,640
For example, that could be two people with two keys needed to open things. To wrap up,

182
00:18:04,640 --> 00:18:11,360
I must say what would be amazing would be to have a robot make me coffee as I struggle to catch up

183
00:18:11,360 --> 00:18:15,600
with all the news happening in AI. Have a wonderful day.

