WEBVTT

00:00.000 --> 00:19.240
Welcome, everybody, to the Entrepreneurial Thought Leaders seminar, a Stanford seminar

00:19.240 --> 00:20.880
for aspiring entrepreneurs.

00:20.880 --> 00:27.400
ETL is presented by STVP, the Engineering Entrepreneurship Center here at Stanford,

00:28.000 --> 00:31.240
the Business Association of Stanford Entrepreneurial Students.

00:31.240 --> 00:35.080
I'm Ravi Balani, a lecturer in the Management Science and Engineering Department at Stanford

00:35.080 --> 00:38.440
and the Director of Alchemist and Accelerator for Enterprise Startups.

00:38.440 --> 00:42.520
Today, we are thrilled to welcome Andrew Ng to ETL.

00:42.520 --> 00:44.560
How many people know Andrew?

00:44.560 --> 00:47.960
Okay, so Andrew really doesn't need an introduction,

00:47.960 --> 00:50.200
but we will give one anyways for those who don't.

00:50.200 --> 00:53.200
Andrew is truly a child of the world.

00:53.200 --> 00:57.280
He was born in the UK to parents who emigrated to the UK from Hong Kong,

00:57.280 --> 01:01.320
but was raised in Hong Kong and Singapore, went to Carnegie Mellon

01:01.320 --> 01:04.280
and very early on signaled that he was no ordinary student.

01:04.280 --> 01:08.080
He got three bachelor's degrees at Carnegie Mellon in computer science,

01:08.080 --> 01:11.480
statistics, and economics, and graduated at the top of his class,

01:11.480 --> 01:15.240
then went on to MIT where he got a master's in electrical engineering and computer science,

01:15.240 --> 01:20.440
then came over to the left coast and got a PhD in Berkeley in computer science

01:20.480 --> 01:23.800
with a focus on artificial intelligence and reinforcement learning.

01:23.800 --> 01:29.800
Andrew is generally viewed as one of the preeminent thought leaders on AI today.

01:29.800 --> 01:34.400
He's the co-founder and head of Google Brain

01:34.400 --> 01:36.640
and the former chief scientist at Baidu,

01:36.640 --> 01:42.880
where he built the company's AI group into thousands of people, several thousand people.

01:42.880 --> 01:47.560
But he's as passionate about AI as he is also about the development of you,

01:47.560 --> 01:49.480
of students around the world.

01:49.480 --> 01:53.040
And I know there's a lot of love for Andrew.

01:53.040 --> 01:57.760
He's a former associates professor and director of the Stanford AI Lab

01:57.760 --> 02:00.120
and currently an adjunct professor in computer science at Stanford.

02:00.120 --> 02:04.520
How many people have taken one of Andrew's classes or want to take one of Andrew's classes?

02:04.520 --> 02:08.760
And he's a beloved professor here at Stanford,

02:08.760 --> 02:12.720
but he's also viewed as a beloved teacher to millions outside of Stanford.

02:12.720 --> 02:17.240
He's the co-founder and chairman of Coursera, the world's largest MOOC platform,

02:17.280 --> 02:21.560
and through his online education work and his online AI education work,

02:21.560 --> 02:24.040
he's reached over 7 million people.

02:24.040 --> 02:30.240
He was listed as one of the world's 100 most influential people by Time Magazine in 2013.

02:30.240 --> 02:34.360
And today, Andrew's the managing director and partner at the AI Fund,

02:34.360 --> 02:37.760
which is a startup studio building new AI companies from the ground up

02:37.760 --> 02:41.040
and is also the founder of deeplearning.ai.

02:41.040 --> 02:43.800
He focuses his time primarily on his entrepreneurial adventures,

02:43.840 --> 02:49.400
looking for the best ways to accelerate responsible AI practices in the larger global economy.

02:49.400 --> 02:52.600
There's fantastic content already online that Andrew has given,

02:52.600 --> 02:56.080
including a longer version of today's talk that you can find on YouTube.

02:56.080 --> 02:59.480
And so instead of reduplicating that, Andrew's going to give a teaser talk,

02:59.480 --> 03:03.760
a 10-minute discussion, followed by we'll do a quick fireside chat,

03:03.760 --> 03:07.560
and then we're going to open it up for really interactive Q&A with you.

03:07.560 --> 03:11.200
So start thinking about your questions now because the time is going to fly by,

03:11.200 --> 03:13.440
but without further ado, please welcome Andrew.

03:13.800 --> 03:14.960
Woo!

03:19.960 --> 03:22.720
Thanks a lot, Ravi. Thanks. Good to see everyone here.

03:22.720 --> 03:25.160
Can everyone in the back hear me okay?

03:25.160 --> 03:26.120
Cool, awesome.

03:26.120 --> 03:31.120
You know, I've taught CS229, my machine learning class, in this room many years,

03:31.120 --> 03:36.160
but all these years I've taught in this room, I've never seen my face that big before.

03:38.880 --> 03:43.400
What I'd like to do today is chat to you about opportunities in AI.

03:43.400 --> 03:47.720
So one of the difficult things to understand about AI

03:47.720 --> 03:51.680
is a general-purpose technology, similar to electricity,

03:51.680 --> 03:53.520
meaning it's not useful just for one thing,

03:53.520 --> 03:56.000
it's useful for a lot of different applications.

03:56.000 --> 03:58.880
If I were to ask you what is electricity good for,

03:58.880 --> 04:02.480
it's almost hard to answer that because it's useful for so many different things,

04:02.480 --> 04:05.800
and AI is like that too.

04:05.800 --> 04:11.240
So one of the major trends that we've seen in the last few years

04:11.280 --> 04:15.840
is that prompting is revolutionizing AI application development.

04:15.840 --> 04:17.840
And I want to just dive a little bit deeper into this

04:17.840 --> 04:19.760
because I know this is an engineering class,

04:19.760 --> 04:22.080
I know many of you may be from an engineering background,

04:22.080 --> 04:26.560
and I'm going to just go a little bit deeper into this than I might otherwise.

04:26.560 --> 04:31.240
But if you were to, say, want to build an AI system for many years,

04:31.240 --> 04:33.320
the typical approach is use supervised earnings.

04:33.320 --> 04:38.120
So let's say I want to build a system to rate restaurant reviews as positive or negative sentiment,

04:38.400 --> 04:42.160
then you would collect data, maybe that takes me a month,

04:42.160 --> 04:45.440
I would train an AI model, maybe that takes me a few months,

04:45.440 --> 04:50.720
find a cloud service to deploy my AI model, maybe that takes a few months.

04:50.720 --> 04:53.680
And so for the past, most of the past decade,

04:53.680 --> 04:59.080
a realistic timeline to build and deploy a valuable AI system was maybe six or 12 months.

04:59.080 --> 05:04.280
But with prompting, the timeline is now very different.

05:04.280 --> 05:07.120
You can specify a prompt in minutes or hours,

05:07.120 --> 05:11.480
and then deploy a system to production in just hours or days.

05:11.480 --> 05:14.640
And I know that probably many or maybe most,

05:14.640 --> 05:19.920
maybe all of you will have played with large language models as a consumer too,

05:19.920 --> 05:23.120
like chat GPD and bot and bing chat.

05:23.120 --> 05:25.760
I think that in terms of start-up opportunities,

05:25.760 --> 05:28.360
I'm excited about the use of large language models,

05:28.360 --> 05:30.840
not as a consumer too, which is fantastic and exciting.

05:30.840 --> 05:33.360
I think use chat GPD and bot regularly,

05:33.360 --> 05:37.440
but instead the application of large language models as a developer too,

05:37.440 --> 05:41.240
because this is allowing a lot more applications to be built

05:41.240 --> 05:45.280
and dramatically lowering the barrier to building many applications.

05:45.280 --> 05:48.400
So I know that in this talk, you don't normally have speakers write code,

05:48.400 --> 05:51.280
but this is an engineering class.

05:51.280 --> 05:55.840
So let me actually show you exactly what I mean by that.

05:55.840 --> 05:59.960
It turns out that if I want to build an AI system today,

06:00.000 --> 06:02.200
this is all the code I need.

06:02.200 --> 06:05.840
And this means that if you take CS 106 or something,

06:05.840 --> 06:08.600
learn to code in a CS class,

06:08.600 --> 06:13.160
with just a little bit of code, import AI, open i2s, load my key,

06:13.160 --> 06:18.520
I don't know, D, what, ST, P lectures,

06:18.520 --> 06:23.160
I break it, so many friends.

06:25.240 --> 06:27.120
I've never written that before.

06:27.120 --> 06:31.800
And so hopefully this, okay, thank goodness, got that right.

06:31.800 --> 06:33.400
And so this is positive sentiment.

06:33.400 --> 06:38.200
And just in seconds, that's all the code it takes now to build an AI system

06:38.200 --> 06:41.200
in code to look at a piece of text and process it,

06:41.200 --> 06:46.680
to look at a piece of email and route it or to start to build the beginnings of a chat bot.

06:46.680 --> 06:51.160
So over the last, I don't know, half year, one of my teams,

06:52.080 --> 06:57.320
has been working with many of the AI tool builders to create short courses

06:57.320 --> 07:00.160
on how to use tools like I just showed you.

07:00.160 --> 07:04.480
Because there are many AI applications that used to take me six months to build

07:04.480 --> 07:08.240
that I think any of you will now be able to build in one or two days.

07:08.240 --> 07:12.280
And this opens up the set of things that you could do

07:12.280 --> 07:14.680
and the set of prototypes you can build.

07:14.680 --> 07:16.440
And in fact, from a startup perspective,

07:16.440 --> 07:18.520
when it took us six months to build something,

07:18.520 --> 07:23.120
what we do is have a product manager study it, do the user studies,

07:23.120 --> 07:25.400
make sure it's the right thing to build, then go build it.

07:25.400 --> 07:28.800
And after all that investment is like, boy, let's hope it works.

07:28.800 --> 07:32.440
But what I'm seeing with these very fast development times is

07:32.440 --> 07:35.160
if it takes you a couple of days to build something,

07:35.160 --> 07:38.320
I'm seeing a lot more startups as well as big companies say,

07:38.320 --> 07:40.960
you know what, I have 10 ideas for features.

07:40.960 --> 07:43.680
I'm going to build all 10 things and then just ship them all.

07:43.680 --> 07:48.480
And then we'll see how users use them or don't use them and just keep what sticks.

07:48.520 --> 07:51.840
And this is a very different prototyping, much lean in methodology

07:51.840 --> 07:54.440
than I've seen startups use before prompting.

07:54.440 --> 07:57.960
One important caveat, which is that responsible AI is important.

07:57.960 --> 08:01.080
So don't do this, don't ship things that could cause harm.

08:01.080 --> 08:06.200
But we have a lot of applications like inspecting bits of metal and factories

08:06.200 --> 08:08.480
where there is really no harm, no risk of bias,

08:08.480 --> 08:10.880
where I think there's very fast shipping methodology

08:10.880 --> 08:14.240
that's just innovate very quickly in AI.

08:14.240 --> 08:15.880
So where are the opportunities?

08:15.880 --> 08:20.280
So the size of these circles shows what I think is the value

08:20.280 --> 08:23.240
of different AI technologies today.

08:23.240 --> 08:26.640
Supervised learning started to work really well about a decade ago

08:26.640 --> 08:29.440
and labeling things, such as label does that as,

08:29.440 --> 08:31.360
is it something you're likely to click on or not?

08:31.360 --> 08:35.120
Or label this X-ray with, you know, what's the medical diagnosis?

08:35.120 --> 08:37.880
And supervised learning for a single company like Google

08:37.880 --> 08:40.760
is worth more than $100 billion a year,

08:40.760 --> 08:43.120
and there are millions of developers working on it.

08:43.160 --> 08:47.040
And it might even grow in the next three years to double, say.

08:47.040 --> 08:50.480
So massive momentum, lots of applications to be figured out.

08:50.480 --> 08:52.880
And then Genes of AI is a new entrance where,

08:52.880 --> 08:56.440
frankly, the revenue, the value of the revenue from Genes of AI today

08:56.440 --> 08:59.280
is much smaller, but given the amounts of interest,

08:59.280 --> 09:00.800
excitement, and commercial interest,

09:00.800 --> 09:03.840
I think it will much more than double in the next three years.

09:03.840 --> 09:07.040
And three years is an artificially short time horizon.

09:07.040 --> 09:08.640
I think it were to look out six years

09:08.640 --> 09:10.720
if it continues to compound at this rate.

09:10.720 --> 09:14.560
Maybe the value from Genes of AI will even start to approach

09:14.560 --> 09:16.120
that of supervised learning.

09:16.120 --> 09:19.520
But all that room for growth, the light-shaded region

09:19.520 --> 09:21.440
for supervised learning or Genes of AI,

09:21.440 --> 09:23.920
which are probably the two most important tools today,

09:23.920 --> 09:27.600
are where there are a lot of opportunities for any of us

09:27.600 --> 09:31.040
to identify and build to concrete use cases.

09:31.040 --> 09:33.280
And what I hope to take away from this talk

09:33.280 --> 09:36.960
is AI technologies are general-purpose technologies,

09:36.960 --> 09:39.880
meaning that they're useful for many different tasks.

09:39.880 --> 09:42.160
When supervised learning started to work,

09:42.160 --> 09:45.960
well, about a decade ago, it actually took us a long time.

09:45.960 --> 09:48.280
It took us annoyingly long over the last decade,

09:48.280 --> 09:51.400
and it will take us annoyingly long over the next decade

09:51.400 --> 09:53.840
to figure out use cases for Genes of AI.

09:53.840 --> 09:56.880
But do you want to use this to make ships more fusion

09:56.880 --> 09:58.440
or for medical diagnosis

09:58.440 --> 10:01.480
or for education product recommendations or something else?

10:01.480 --> 10:04.680
It was still figuring out concrete use cases

10:04.680 --> 10:06.160
for supervised learning.

10:06.160 --> 10:08.080
And even though we're not yet done doing that,

10:08.080 --> 10:10.440
we have another fantastic new tool, Genes of AI,

10:10.440 --> 10:11.720
that even further expands

10:11.720 --> 10:13.560
the set of things we now do of AI.

10:14.440 --> 10:16.960
And one important caveat,

10:16.960 --> 10:19.760
which is there will be fans along the way.

10:19.760 --> 10:21.920
How many of you remember Lenzer?

10:21.920 --> 10:23.120
Raise your hand if you do.

10:23.120 --> 10:24.320
Wow, almost no one.

10:24.320 --> 10:26.080
That's fascinating.

10:26.080 --> 10:30.880
So Lenzer's revenues took off like that through last December.

10:32.040 --> 10:34.840
It was this app that could let you

10:34.840 --> 10:36.400
upload a few pictures of yourself

10:36.400 --> 10:38.360
and draw a cool picture of you as an astronaut

10:38.360 --> 10:40.080
or a scientist or something.

10:40.080 --> 10:43.720
And it was a really good, really hot product

10:43.720 --> 10:47.640
until last December, after which his revenues did that.

10:47.640 --> 10:50.040
And I think that's because Lenzer was one

10:50.040 --> 10:52.520
of what will probably turn out to be multiple,

10:52.520 --> 10:55.480
thin software layers built on top of someone else's

10:55.480 --> 10:56.680
very powerful API.

10:56.680 --> 10:58.400
That was a good idea, people liked it,

10:58.400 --> 11:01.640
but it wasn't a long-term, defensible business.

11:01.640 --> 11:05.400
And when I think about Genes of AI as a developer platform,

11:05.400 --> 11:09.480
I'm reminded of when Steve Jobs gave us this phone, right?

11:09.480 --> 11:12.720
And shortly after, someone wrote an app

11:12.720 --> 11:16.200
that I paid $199 for to do this,

11:16.200 --> 11:18.200
to turn the phone into a flashlight.

11:18.200 --> 11:20.440
And this was also a good idea, it was a great product,

11:20.440 --> 11:23.440
but it just was not a defensible business either

11:23.440 --> 11:25.440
because it was a very thin software layer built on top

11:25.440 --> 11:28.640
of someone else's very powerful development platform.

11:28.640 --> 11:31.200
But in the same way, after we got the iPhone,

11:31.200 --> 11:32.360
after we got the smartphone,

11:32.360 --> 11:35.840
someone else figured out how to build Uber, Airbnb, and Tinder,

11:35.840 --> 11:38.600
much longer-term, defensible, very valuable businesses

11:38.600 --> 11:40.960
that are still standing the test of time.

11:40.960 --> 11:43.080
And I think we have those opportunities as well

11:43.080 --> 11:45.840
to build long-term, valuable franchises,

11:45.840 --> 11:48.360
businesses on top of Genes of AI.

11:48.360 --> 11:50.920
So where are the opportunities?

11:50.920 --> 11:55.920
So I felt years ago, but even more strongly now,

11:56.400 --> 11:58.680
that because of emerging AI technology,

11:58.680 --> 12:00.440
there are a lot of projects that are now possible,

12:00.440 --> 12:04.680
that were not possible one or a handful of years ago.

12:04.680 --> 12:08.400
And I wound up starting AI Fund, which is a venture studio,

12:08.400 --> 12:11.640
that sequentially works as entrepreneurs to start companies.

12:11.640 --> 12:14.920
We actually average about one startup a month now,

12:14.920 --> 12:18.000
because I felt, I previously, as Ravi mentioned,

12:18.000 --> 12:21.320
previously I had led AI teams in Google and Baidu,

12:21.320 --> 12:25.120
and even having led AI teams in Big Tech,

12:25.120 --> 12:28.040
I couldn't see how we could possibly operate a team

12:28.040 --> 12:31.080
in a Big Tech company to pursue the very diverse,

12:31.080 --> 12:34.520
very different sets of opportunities that I saw

12:34.520 --> 12:37.280
and wanted to pursue, and starting different startups

12:37.280 --> 12:40.400
to pursue those valuable projects seem more efficient

12:40.400 --> 12:43.520
than having one company, even the Big Tech company,

12:43.520 --> 12:46.280
go after such a large set of resources.

12:46.280 --> 12:49.640
But having said that, I think AI and Genes of AI

12:49.640 --> 12:52.920
also offers a lot of opportunities for incumbent companies,

12:52.920 --> 12:55.080
which often have a distribution advantage.

12:56.160 --> 12:57.640
Where exactly are the opportunities?

12:57.640 --> 12:59.880
So this is what I think of as the AI stack.

12:59.880 --> 13:03.840
At the lowest level is the hardware layer, very valuable,

13:03.840 --> 13:07.080
but also very capital-intensive, needs a lot of resources to build

13:07.080 --> 13:08.320
and very concentrated.

13:08.320 --> 13:11.400
So I'm sure there'll be valuable startups built there,

13:11.400 --> 13:12.680
but I personally don't play there,

13:12.680 --> 13:16.320
because of how capital-intensive and how concentrated it is.

13:16.320 --> 13:17.840
There's a cloud infrastructure layer,

13:17.840 --> 13:20.920
also very capital-intensive, very concentrated, very valuable,

13:20.920 --> 13:24.920
but at least when I build startups, I tend not to play there.

13:24.920 --> 13:27.640
The other layer that's interesting is the developer tooling layer.

13:27.640 --> 13:32.640
So what you just saw me do was use OpenAI as a developer tool.

13:32.640 --> 13:35.560
And I see this space as hyper-competitive,

13:35.560 --> 13:38.280
look at all the startups chasing OpenAI,

13:38.280 --> 13:40.320
but there will be some mega winners.

13:40.320 --> 13:45.040
So whereas incumbents have a startup distribution advantage,

13:45.040 --> 13:47.640
I think for many startups, having a technology advantage

13:47.640 --> 13:50.360
may give you a best shot at doing something meaningful there.

13:50.360 --> 13:52.160
So I personally tend to play here

13:52.160 --> 13:54.680
only when we think we have a technology advantage,

13:54.680 --> 13:59.280
because that buys us a better chance to become one of the huge winners.

13:59.280 --> 14:01.840
And then with most ways of technology innovation,

14:01.840 --> 14:03.640
a lot of the media attention, social media,

14:03.640 --> 14:06.360
what people tend to talk about is the tooling of the technology layer.

14:06.360 --> 14:10.560
There's one other layer that I think has got to be even more valuable,

14:10.560 --> 14:12.280
and that's the application layer.

14:12.280 --> 14:15.120
Because in many ways of technology,

14:15.120 --> 14:18.680
for the in-front and tooling layer to be successful,

14:18.680 --> 14:20.960
applications need to be built on top of them.

14:20.960 --> 14:22.600
They generate even more revenue

14:22.600 --> 14:26.440
so that they can afford to play the infrastructure layer.

14:26.440 --> 14:28.960
And what I'm seeing is that there are a lot of opportunities

14:28.960 --> 14:32.520
at the application layer where the intensity of competition

14:32.520 --> 14:35.440
is not, frankly, not nearly as high.

14:35.440 --> 14:37.640
Maybe just one example.

14:37.640 --> 14:41.080
I've been chatting a lot with the CEO of Meno,

14:41.080 --> 14:48.360
which is a startup that applies AI to romantic relationship coaching.

14:48.360 --> 14:50.040
And I'm an AI guy.

14:50.040 --> 14:53.240
I feel like I don't know anything about romance.

14:53.240 --> 14:55.920
And if you don't believe me, you can ask my wife.

14:55.920 --> 14:59.200
She would confirm that I don't know anything about romance.

14:59.200 --> 15:03.560
But when we decided, when we had conviction that AI could be applied to relationships,

15:03.560 --> 15:08.240
we wound up partnering with Renata Nyborg, who's the former CEO of Tinder.

15:08.240 --> 15:10.720
And because she ran Tinder, she understands relationships

15:10.720 --> 15:14.320
in a very systematic way, more so than anyone else I know.

15:14.320 --> 15:16.400
And so with my team providing AI expertise

15:16.400 --> 15:18.280
and her providing relationship expertise,

15:18.280 --> 15:23.280
we're able to build a pretty unique relationship mentoring application

15:23.280 --> 15:25.880
that we just announced a few weeks ago.

15:25.880 --> 15:28.720
And this is not part of, you know, Renata actually occasionally

15:28.720 --> 15:31.720
stops by Stanford campus and talks to Stanford students

15:31.720 --> 15:33.840
as part of her user product research.

15:33.840 --> 15:36.440
So it's possible that we've seen her around.

15:36.440 --> 15:39.040
Just one last thing, I'd love to go to GNA.

15:39.040 --> 15:42.440
Over the last few years, AI fund we've been tuning our process

15:42.440 --> 15:45.280
for building startups and we just share that with you.

15:45.280 --> 15:48.000
So we often start off with a lot of ideas, right?

15:48.000 --> 15:51.800
And one example of another startup we built was Bering AI,

15:51.800 --> 15:55.080
which uses AI for smart routing of very large ocean growing vessels.

15:55.080 --> 15:58.520
So if you're a ship captain, should you sail at 20 knots or 22 knots?

15:58.520 --> 15:59.280
Who knows?

15:59.280 --> 16:01.840
Most ship captains just make some decision.

16:01.840 --> 16:07.200
But because we're able to get global weather and ocean current data,

16:07.200 --> 16:10.560
we can make recommendations to ship captains for how to get there on time

16:10.560 --> 16:12.840
and use about 10% less fuel.

16:12.840 --> 16:15.440
But this idea was suggested to me by Mitsui,

16:15.440 --> 16:18.440
which is a major shareholder in a major shipping line

16:18.440 --> 16:20.920
that operates very large ocean growing vessels.

16:20.920 --> 16:23.560
And just one of those things, I would never have thought of this idea myself

16:23.560 --> 16:27.920
because I've been on a boat, but what do I know about global maritime shipping?

16:27.920 --> 16:30.960
But Mitsui suggested this idea to me.

16:30.960 --> 16:35.400
And we then validate the idea, make sure there's technical feasibility

16:35.400 --> 16:37.720
and market need, recruit a CEO.

16:37.720 --> 16:40.480
We were fortunate to find Dylan Kyle, who's a fantastic CEO

16:40.480 --> 16:42.480
with one successful exit before.

16:42.480 --> 16:45.080
And then we spent three months in our current process

16:45.080 --> 16:49.280
building a technical prototype with the CEO and doing deep customer validation.

16:49.280 --> 16:53.520
If it survives, two-thirds chance of surviving, one-third chance of not surviving.

16:53.520 --> 16:57.120
We then write a check-in that allows the company to build higher executives,

16:57.120 --> 17:02.040
build an MVP, and off it goes to raise additional rounds of capital.

17:02.040 --> 17:03.880
And I think this is what we...

17:03.880 --> 17:06.080
And so bearing AI, well, now it's actually...

17:06.080 --> 17:10.320
There are now hundreds of ships on the high seas guided by bearing AI.

17:10.320 --> 17:13.200
Ships guided by bearing AI have 75 million miles,

17:13.200 --> 17:16.000
which is the equivalent of going 3,000 times around the planet

17:16.000 --> 17:20.200
and save about half a million dollars in fuel costs per ship per year

17:20.200 --> 17:22.080
in addition to significant carbon emissions.

17:22.080 --> 17:23.320
I think we'll save about...

17:23.320 --> 17:27.240
I want to save about a million tons of CO2 emissions so far.

17:27.240 --> 17:28.960
But this kind of idea that...

17:28.960 --> 17:31.040
I would never have come up with this idea myself,

17:31.040 --> 17:33.720
but I've learned that my swim lane is AI,

17:33.720 --> 17:35.760
but when I work with experts in other sectors,

17:35.760 --> 17:39.240
there are often these exciting opportunities that are very valuable.

17:39.240 --> 17:42.880
But frankly, how many groups in the world are experts in AI and shipping

17:42.880 --> 17:44.360
or expert in AI and relationships?

17:44.360 --> 17:49.720
I find that the competition intensity at the application layer is often much lower.

17:49.720 --> 17:52.720
And then just one last thing, kind of just full disclosure

17:52.720 --> 17:55.320
and something that I hope all of you will do too.

17:55.320 --> 17:58.960
My team's only work on projects that we think move humanity forward.

17:58.960 --> 18:00.560
Response for AI is important.

18:00.560 --> 18:02.320
And on multiple occasions, we've killed

18:02.320 --> 18:07.080
and I will continue to kill projects that we may assess to be financially sound

18:07.080 --> 18:09.120
but based on ethical grounds.

18:09.120 --> 18:11.560
So lots of exciting opportunities.

18:11.560 --> 18:14.320
I think at Stanford, the loss of great costs is going to take

18:14.320 --> 18:18.280
in engineering and elsewhere to learn about that AI tech.

18:18.280 --> 18:23.640
And then when you find applications or go play at the infrar and tooling layer too,

18:23.640 --> 18:25.120
I think there are lots of opportunities.

18:25.120 --> 18:26.400
But I think there are...

18:26.400 --> 18:29.400
What I'm seeing is, frankly, my team at AI Fund,

18:29.400 --> 18:31.040
we have so many startup ideas.

18:31.040 --> 18:32.920
We use a task management software.

18:32.920 --> 18:36.120
We use Asana to track this huge list of ideas.

18:36.120 --> 18:37.720
And it's actually quite clear to me.

18:37.720 --> 18:40.440
There are a lot more good ideas for AI businesses

18:40.440 --> 18:44.640
than people with the skill to work on them at this moment in time.

18:44.640 --> 18:48.160
So hopefully, there'll be more than enough projects for everyone.

18:48.160 --> 18:50.960
There are all of you, all of us, to work on.

18:50.960 --> 18:51.920
All right, thank you.

18:55.920 --> 18:58.920
I wanted to just start off with that closing statement that you made

18:58.920 --> 19:02.120
about how there's more opportunity than there are students with skills

19:02.120 --> 19:04.080
or people with skills to pursue them.

19:04.080 --> 19:06.480
And given that we have an audience full of students,

19:06.480 --> 19:09.480
I wanted to start off by mapping out advice for students

19:09.480 --> 19:11.880
that are entering into the university regarding AI.

19:11.880 --> 19:15.320
So if you want to pursue a career in AI right now,

19:15.320 --> 19:18.360
and let's say your child was entering Stanford,

19:18.360 --> 19:22.600
what advice would you give them in terms of how to spend their time?

19:22.600 --> 19:26.520
Yeah, so, you know, there's one thing that's actually really worth doing

19:26.520 --> 19:30.200
when you're sent to students, which is take classes.

19:30.200 --> 19:33.880
Because it turns out that I feel like there's actually one pattern I see

19:33.960 --> 19:38.240
for both undergraduates and graduate students, including PhD students,

19:38.240 --> 19:40.680
which is there's so much exciting stuff to do,

19:40.680 --> 19:42.480
you just want to jump in and do it, right?

19:42.480 --> 19:46.080
In fact, I've seen them undergrads in their freshman year,

19:46.080 --> 19:49.040
you know, try to join a research lab and start doing work in AI.

19:49.040 --> 19:51.040
That's okay, nothing wrong with that.

19:51.040 --> 19:54.960
But it turns out that while project work is one way to learn,

19:54.960 --> 19:58.800
coursework is, I think, an even more efficient way to learn,

19:58.800 --> 20:01.600
especially when it comes to mastering the fundamentals,

20:01.600 --> 20:04.920
because professors have put a lot of work to organize the material

20:04.920 --> 20:08.160
in a way that's efficient to learn and digest.

20:08.160 --> 20:12.800
So I would say, you know, take classes in AI technology

20:12.800 --> 20:15.840
or in entrepreneurship and gain those skills.

20:15.840 --> 20:19.800
I've seen students jump in and then if you are trying to work in a research lab

20:19.800 --> 20:21.800
without strong skills, you end up, you know, like,

20:21.800 --> 20:24.680
labeling data or something, which is fine, you learn some things,

20:24.680 --> 20:27.680
but you actually learn a lot from taking courses.

20:27.680 --> 20:29.240
And then in addition to that,

20:29.280 --> 20:31.960
after you start to master the foundational skills,

20:31.960 --> 20:35.160
after you know how to use AI technology or, you know, then,

20:35.160 --> 20:38.600
as you start to practice, find exciting use cases across campus.

20:38.600 --> 20:42.280
I do a lot of work, you know, over with people over in climate science

20:42.280 --> 20:45.800
or in healthcare to take my AI expertise

20:45.800 --> 20:47.840
and then navigate with a different discipline

20:47.840 --> 20:50.520
that I'm not expert in to find exciting applications.

20:50.520 --> 20:54.760
And hopefully that type of practice will help many of you find exciting

20:54.760 --> 20:56.240
projects to work on as well.

20:56.240 --> 20:58.560
Do you need to take technical classes?

20:58.560 --> 21:01.240
Do you think you need to take computer science classes

21:01.240 --> 21:04.160
if you want to pursue a career in AI?

21:04.160 --> 21:08.600
Need is too strong, but I definitely encourage you to take technical classes.

21:08.600 --> 21:13.640
I think we're moving toward a world where, frankly, I, you know,

21:13.640 --> 21:16.560
at some future point, I think everyone should learn to code.

21:16.560 --> 21:19.920
Or rather, I think it'll be useful for everyone to learn how to code

21:19.920 --> 21:21.480
for a couple of reasons.

21:21.480 --> 21:23.240
Everyone has access to data, right?

21:23.240 --> 21:26.120
This is different than the world used to be even a few years ago.

21:26.120 --> 21:30.080
And especially with genus of AI, your ability to get something to work

21:30.080 --> 21:32.800
is much higher than ever before.

21:32.800 --> 21:35.960
The barrier to entry is much lower than ever before.

21:35.960 --> 21:39.000
And so if you learn just a little bit of coding,

21:39.000 --> 21:42.080
the amount that you really accomplish is significantly greater

21:42.080 --> 21:45.040
than if you don't know how to code at all.

21:45.040 --> 21:48.920
And are there any skills that separate out the great AI founders?

21:48.920 --> 21:53.480
I know AI right now is like, it's a sea that's rising all boats.

21:53.480 --> 21:55.960
But if you separate out the great ones from the good ones,

21:55.960 --> 22:00.240
are there any salient skills that you notice that the great AI CEOs

22:00.240 --> 22:02.680
or founders have that the good ones don't?

22:02.680 --> 22:07.200
Maybe since you said AI, I would say is often technical death.

22:07.200 --> 22:08.440
It costs a lot.

22:08.440 --> 22:10.360
But I want to give a different answer if you say great founders

22:10.360 --> 22:11.360
or not great AI founders.

22:11.360 --> 22:13.720
But I feel like AI is evolving rapidly.

22:13.720 --> 22:17.040
And we definitely have lost a bunch of new roles that, you know,

22:17.040 --> 22:19.600
pitch the VCs without really knowing what they're doing.

22:19.600 --> 22:22.240
And the smart VCs can sniff it out quite quickly.

22:22.240 --> 22:23.440
And it makes a huge difference.

22:23.440 --> 22:26.080
I think the technology, unfortunately, you know,

22:26.080 --> 22:29.400
is like somewhat complicated for a lot of applications.

22:29.400 --> 22:31.600
So a team that actually knows what they're doing

22:31.600 --> 22:34.720
will execute an AI project 10 times faster, you know,

22:34.720 --> 22:35.760
than a team that doesn't.

22:35.760 --> 22:37.400
And 10 times is not a made-up number.

22:37.400 --> 22:41.760
I literally see people take a year to do something like, oh, boy,

22:41.760 --> 22:44.640
I know that other team would have done perform this level

22:44.640 --> 22:47.320
in two weeks or maybe a month.

22:47.320 --> 22:49.840
So for many AI startups, application startups,

22:49.840 --> 22:52.320
in-front startups, you kind of have to know what you're doing.

22:52.320 --> 22:54.120
So it doesn't have to be you.

22:54.120 --> 22:57.400
If you're a technical co-founder, maybe that's OK.

22:57.400 --> 23:02.560
And then second thing I see among many of the great founders

23:02.560 --> 23:03.400
is speed.

23:03.400 --> 23:06.120
I find that as a startup, you'd be surprised

23:06.120 --> 23:08.680
when you handle the great founders the sheer speed

23:08.680 --> 23:11.760
of decision-making.

23:11.760 --> 23:14.760
And, you know, I sometimes talk to people from big companies

23:14.760 --> 23:16.760
and they'll say, oh, we move so fast.

23:16.760 --> 23:19.080
But when I kind of sit them side by side,

23:19.080 --> 23:20.640
how long does it take you to make this decision?

23:20.640 --> 23:21.720
Do I talk to a great founder?

23:21.760 --> 23:23.480
How long does it take me to make this decision?

23:23.480 --> 23:24.520
Maybe here's one story.

23:24.520 --> 23:27.480
I was chatting with the Meno CEO of Renato and iBook,

23:27.480 --> 23:28.760
former CEO Tinder.

23:28.760 --> 23:30.000
I was on the phone with her one day

23:30.000 --> 23:32.120
and she was making a major architecture decision.

23:32.120 --> 23:33.720
So it's architecting this thing.

23:33.720 --> 23:36.480
There were basically two major software architectures

23:36.480 --> 23:37.480
under consideration.

23:37.480 --> 23:40.320
And the team had laid it out, listed out some pros and cons.

23:40.320 --> 23:42.400
So they got on the team with me and some of my friends

23:42.400 --> 23:44.200
and said, these are pros and cons.

23:44.200 --> 23:48.880
And then one of my C2 AI fund and I said, you know,

23:48.880 --> 23:51.000
we're not sure, but she has some reasons

23:51.000 --> 23:53.160
that we prefer architecture A.

23:53.160 --> 23:55.320
And then Renato said, okay, guys done,

23:55.320 --> 23:58.680
decision made, go and implement architecture A.

23:58.680 --> 24:01.040
And after I thought, well, did Renato

24:01.040 --> 24:03.520
just make a massive engineering decision

24:03.520 --> 24:05.480
in basically 30 seconds?

24:05.480 --> 24:07.520
And she did.

24:07.520 --> 24:09.160
And I realized after it,

24:09.160 --> 24:11.080
I don't think there was a better way to make it

24:11.080 --> 24:12.440
because it's not as if, you know,

24:12.440 --> 24:14.560
if the company waited another week,

24:14.560 --> 24:16.000
it would have been a high quality decision

24:16.000 --> 24:17.120
and if it was wrong,

24:17.120 --> 24:19.160
I'm sure they would fix it, you know, the next week.

24:19.160 --> 24:22.040
But until you've lived through the speed

24:22.040 --> 24:24.720
of a great business, most people,

24:24.720 --> 24:27.320
I know so many people that think their organizations are fast

24:27.320 --> 24:29.080
when you stack them up to the real speed

24:29.080 --> 24:30.600
of a fast moving CEO,

24:30.600 --> 24:33.680
they have never actually seen speed in their life.

24:33.680 --> 24:35.760
One important caveat do be responsible.

24:35.760 --> 24:38.240
I know that move fast and break things sometimes,

24:38.240 --> 24:39.880
you know, it's the wrong approach.

24:39.880 --> 24:43.680
So tremendous speed when you are not being callous

24:43.680 --> 24:46.120
with people's lives and livelihood

24:46.120 --> 24:48.080
and things that could cause real harm.

24:48.080 --> 24:50.360
But so long as there's an important caveat

24:50.360 --> 24:52.400
of responsible AI,

24:52.400 --> 24:54.560
many of the great CEOs move faster

24:54.560 --> 24:57.560
than most people realize people can move.

24:57.560 --> 24:59.080
And so let's just double click on that

24:59.080 --> 25:00.760
on this theme of responsible AI

25:00.760 --> 25:02.360
just because I know this is a hot topic

25:02.360 --> 25:04.360
that maybe people aren't thinking about,

25:04.360 --> 25:08.400
which is you are clearly on the side of AI for good

25:08.400 --> 25:10.320
for responsible AI.

25:10.320 --> 25:13.040
Many of your brethren like Jeffrey Hinton

25:13.040 --> 25:15.640
and other famous leaders in the AI space

25:15.640 --> 25:18.480
have come out and are concerned

25:18.480 --> 25:19.880
that the pace of AI development

25:19.880 --> 25:21.920
will become an existential threat to humanity.

25:21.920 --> 25:24.880
So much so that famously there was a petition signed

25:24.880 --> 25:28.080
by Elon Musk and Steve Wozniak and many thought leaders

25:28.080 --> 25:30.480
asking for the halting of the foundational,

25:30.480 --> 25:32.800
the deepest foundational models of AI

25:32.800 --> 25:35.480
for us to sort of, for society to sort of catch up.

25:35.480 --> 25:38.480
You did not sign that pledge.

25:38.480 --> 25:40.440
Can you share a little bit more detail about that?

25:40.440 --> 25:42.560
Was that a difficult decision for you to make?

25:42.560 --> 25:43.960
And can you share more details

25:43.960 --> 25:45.760
about why you didn't join them

25:45.760 --> 25:48.240
and what your philosophical view is regarding

25:48.240 --> 25:50.720
if AI poses an existential threat?

25:50.720 --> 25:54.080
So I honestly don't see how AI poses

25:54.080 --> 25:57.400
any existential threat to the human race.

25:57.400 --> 25:59.400
We know AI can run about self-driving cars

25:59.400 --> 26:01.720
have crashed, leading to a tragic loss of life,

26:01.720 --> 26:04.080
ultimately through trainings, trash the stock market.

26:04.080 --> 26:06.040
So we know poorly designed software systems

26:06.040 --> 26:07.680
can have a dramatic impact

26:07.680 --> 26:09.400
and responsible AI is important.

26:09.400 --> 26:13.200
But recently I saw tells people like Jeff

26:13.200 --> 26:14.960
and others that were concerned

26:14.960 --> 26:16.760
about the question of AI extinction

26:16.760 --> 26:19.920
and I tried to understand why they thought this way.

26:19.920 --> 26:21.760
Some were worried about bad actor

26:21.760 --> 26:23.720
using AI to create a bio weapon.

26:23.720 --> 26:26.200
Others were worried about AI evolving in a way

26:26.200 --> 26:28.920
that inadvertently leads to human extinction

26:28.920 --> 26:33.280
similar to how we as humans have led to the extinction

26:33.280 --> 26:35.960
of many species through simple lack of awareness.

26:35.960 --> 26:39.040
Sometimes that our actions could lead to that outcome.

26:39.040 --> 26:41.320
But when I tried to assess how realistic

26:41.320 --> 26:42.400
these arguments were,

26:42.440 --> 26:45.120
I found them to be vague and non-specific

26:45.120 --> 26:47.440
about how AI could cause all.

26:47.440 --> 26:50.280
And I think that I found frustratingly frankly

26:50.280 --> 26:52.800
that trying to prove AI couldn't

26:52.800 --> 26:54.920
is akin to proving a negative.

26:54.920 --> 26:57.240
And I can't prove that super intelligent AI

26:57.240 --> 26:58.600
won't be dangerous,

26:58.600 --> 27:01.120
but I can't seem to find anyone

27:01.120 --> 27:03.520
that really knows exactly how it could be.

27:04.560 --> 27:09.280
And but I do know that humanity has ample experience

27:09.280 --> 27:11.160
controlling many things far more powerful

27:11.160 --> 27:14.760
than any one of us like corporations and nation states.

27:14.760 --> 27:17.600
And there are many things that we can't fully control

27:17.600 --> 27:19.320
that are nonetheless safe and valuable.

27:19.320 --> 27:21.640
Like airplanes, no one can control an airplane.

27:21.640 --> 27:23.320
It's buffeted around by winds

27:23.320 --> 27:25.560
and the pilot may make a mistake.

27:25.560 --> 27:27.800
But in the early days of aviation,

27:27.800 --> 27:29.520
airplanes killed many people.

27:29.520 --> 27:31.400
So we learned from those experiences,

27:31.400 --> 27:32.600
built safer aircraft,

27:32.600 --> 27:34.720
devised rules by which to operate them.

27:34.720 --> 27:36.640
And today most of us can step into airplane

27:36.640 --> 27:38.320
without fearing for our lives.

27:38.320 --> 27:40.920
And I think it would be like that too for AI.

27:40.920 --> 27:43.040
So I think the AI extinction,

27:43.040 --> 27:44.320
I find it to be very unfortunate.

27:44.320 --> 27:45.160
What I'm seeing,

27:45.160 --> 27:47.760
because doing some work in K-Tel of education as well,

27:47.760 --> 27:51.040
what I'm seeing is that kind of really unfortunately,

27:51.040 --> 27:54.960
I see high school students now considering working in AI.

27:54.960 --> 27:58.640
And some will say, AI seems exciting,

27:58.640 --> 28:01.040
but I heard it could lead to human extinction.

28:01.040 --> 28:03.520
And I just don't want to be a part of that.

28:03.520 --> 28:07.400
And so I find that the over height AI extinction narrative

28:07.400 --> 28:08.560
is doing real harm.

28:08.560 --> 28:10.720
So I'm really concerned about that.

28:11.680 --> 28:12.520
Thank you, Andrew.

28:12.520 --> 28:13.640
One more question that I'm gonna open it up,

28:13.640 --> 28:15.200
which is, I loved the detail

28:15.200 --> 28:16.560
on the low hanging fruit opportunities.

28:16.560 --> 28:17.920
I know that's on everybody's,

28:17.920 --> 28:20.240
all the entrepreneurs' minds of what to pursue.

28:20.240 --> 28:21.280
And so I appreciated the attention

28:21.280 --> 28:22.880
and the presentation on that.

28:22.880 --> 28:24.440
I wanted to ask about what's gonna be

28:24.440 --> 28:26.280
the next big technology shift in AI

28:26.280 --> 28:28.600
because things are changing so rapidly,

28:28.600 --> 28:30.520
especially as the models now are getting smaller

28:30.520 --> 28:32.040
and open sourced.

28:32.040 --> 28:34.200
It feels like we've already conquered language.

28:34.200 --> 28:37.280
Visual AI is getting very, very good.

28:37.280 --> 28:38.120
What's next?

28:38.120 --> 28:39.600
What are you seeing that's around the corner

28:39.600 --> 28:41.200
that others might not be aware of?

28:41.200 --> 28:42.240
Yeah, you know what, Danit?

28:42.240 --> 28:43.720
About several months ago,

28:43.720 --> 28:45.680
I was predicting visual AI's coming next,

28:45.680 --> 28:46.520
but now everyone's all right,

28:46.520 --> 28:48.840
visual AI's gotta come with something new.

28:48.840 --> 28:50.640
But in all seriousness, I think visual AI

28:50.640 --> 28:52.680
would be much more about the analysis of images

28:52.680 --> 28:54.480
rather than just generation of images.

28:54.480 --> 28:57.320
But I think we're at like the GPT-2 moment

28:57.320 --> 28:58.920
for visual AI is not yet working,

28:58.920 --> 29:00.240
but I think it'll work much better.

29:00.240 --> 29:02.640
And this will impact self-driving cars, for example,

29:02.640 --> 29:05.440
when we can finally solve problems in a long tail.

29:06.400 --> 29:09.120
And then I think, actually, one other thing that

29:09.120 --> 29:12.200
I wrote about just today in a newsletter called The Batch

29:12.200 --> 29:16.040
is I think one thing that many people find controversial

29:16.040 --> 29:18.680
but I think is coming is the rise of edge AI.

29:18.680 --> 29:20.040
And I know this controversial.

29:20.040 --> 29:22.640
Many of us would train to write SaaS software,

29:22.640 --> 29:25.720
lend to self-nice, the subscription business model.

29:25.720 --> 29:27.000
How do you even find people?

29:27.000 --> 29:29.200
How do you hire engineers to write desktop applications?

29:29.200 --> 29:30.920
Like who even does that anymore?

29:31.120 --> 29:36.120
But I think that because of various forces,

29:36.280 --> 29:39.280
including privacy, I think that in the next few years

29:39.280 --> 29:43.240
we'll see more AI applications running at the edge,

29:43.240 --> 29:46.000
meaning on your laptop or on your cell phone.

29:46.000 --> 29:48.400
So I think that'll be coming.

29:48.400 --> 29:50.000
And then I think there'll just be a lot of work

29:50.000 --> 29:52.280
coming in the application there as well.

29:52.280 --> 29:54.080
Okay, I wanna open it up to the students.

29:54.080 --> 29:56.560
You're the reason why Andrew's here.

29:56.560 --> 29:59.360
You mentioned that on your slides,

29:59.360 --> 30:01.360
you put the potential from reinforcement learning.

30:01.360 --> 30:03.280
Are the general value as a dot

30:03.280 --> 30:06.720
relative to the potential for unsupervised learning?

30:06.720 --> 30:08.800
Do you think there is still potential

30:08.800 --> 30:11.600
for generalist agents like GATO

30:11.600 --> 30:15.000
and other reinforcement learning models in society

30:15.000 --> 30:17.120
and in your AI stack for startups?

30:18.160 --> 30:22.200
So technically, last time we trained models

30:22.200 --> 30:23.800
that trained using reinforcement learning

30:23.800 --> 30:25.760
and unsupervised learning and supervised learning

30:25.760 --> 30:27.080
but leaving that aside,

30:27.080 --> 30:29.160
I feel like I'm not convinced that reinforcement learning

30:29.160 --> 30:30.480
is near breakthrough moments,

30:30.480 --> 30:33.040
at least in the next small number of years.

30:33.040 --> 30:34.760
A lot of excitement about what we could do

30:34.760 --> 30:36.600
in reinforcement learning applied to robotics.

30:36.600 --> 30:39.440
A lot about CS faculty, right, Chelsea Finn,

30:39.440 --> 30:42.400
Emma Brunskill, many others are doing exciting research there

30:42.400 --> 30:44.680
but we do have a data problem.

30:44.680 --> 30:46.600
So it turns out that text on the internet

30:46.600 --> 30:48.760
sounds a lot like text on your documents.

30:48.760 --> 30:51.200
So we can learn from lots of text on the internet

30:51.200 --> 30:53.480
to do really well on your text documents.

30:53.480 --> 30:54.720
And image on the internet

30:54.720 --> 30:56.640
look a little bit like images that you care about.

30:56.640 --> 30:58.240
So we have a lot of data

30:58.240 --> 31:00.520
but because every robot is different,

31:01.480 --> 31:04.240
struggling, many people are struggling to see

31:04.240 --> 31:06.760
how to get enough data to have the usual recipe

31:06.760 --> 31:08.760
of scaling up data and compute

31:08.760 --> 31:10.280
where for reinforcement learning

31:10.280 --> 31:12.600
and people are working on it over the weekend

31:12.600 --> 31:16.240
at the CS faculty retreat, you know, there was a talk,

31:16.240 --> 31:18.240
I think, well, who gave this talk?

31:18.240 --> 31:19.520
Shoot, thank you.

31:19.520 --> 31:23.120
On how to do this, early ideas on how to do this

31:23.120 --> 31:25.200
but I think we're still a few years away

31:25.200 --> 31:27.960
from breakthroughs and those breakthroughs

31:27.960 --> 31:29.440
and reinforcement learning.

31:29.440 --> 31:31.320
But it's a great research topic, by the way,

31:31.320 --> 31:34.440
just because, you know, just because it's not working

31:34.440 --> 31:37.040
right now doesn't mean you shouldn't do research on it.

31:37.040 --> 31:39.000
So I think it's a great research topic.

31:39.000 --> 31:42.080
So I just want to know your thoughts about

31:42.080 --> 31:45.440
what are the security concerns which is coming up

31:45.440 --> 31:47.880
by you abusing that like LLM models,

31:47.880 --> 31:51.000
like all these new attacks like prompt injections,

31:51.000 --> 31:53.160
data leakage, jailbreaking.

31:53.160 --> 31:54.680
So what's your thought around that?

31:54.680 --> 31:58.720
Like how can we like safeguard against those kind of attack

31:58.720 --> 32:01.200
because it's just starting up this new technology.

32:01.200 --> 32:05.000
So I'm assuming there's more things which will come up.

32:05.000 --> 32:07.520
Yes, so I think that for the near future

32:07.520 --> 32:10.800
there'll be a little bit of a cat and mouse thing going on.

32:12.240 --> 32:15.600
So I think I'm seeing different companies

32:17.240 --> 32:18.880
approach this with different tools

32:18.880 --> 32:22.040
to wash out for prompt injections, for data leakage.

32:22.040 --> 32:24.240
Actually, DeepLand.ai is actually a work of a partner

32:24.240 --> 32:28.680
on some things that hopefully will announce very soon

32:28.680 --> 32:31.000
on our portfolio of tools.

32:31.000 --> 32:34.080
By the way, those of you that have not yet done it,

32:34.080 --> 32:37.000
go and fool around with prompt injections,

32:37.000 --> 32:40.000
see if you can get an LLM to do something.

32:40.000 --> 32:42.160
Well, don't do something actually harmful,

32:42.160 --> 32:45.800
but I actually find it kind of intellectually interesting

32:45.800 --> 32:48.440
whenever I use an LLM to prompt it

32:48.440 --> 32:51.400
to see how robust the safeguards actually are.

32:51.840 --> 32:54.360
And if I should look at the older language models a year ago,

32:54.360 --> 32:57.280
it was super easy to get the older models,

32:58.240 --> 33:00.440
frankly, to give you detailed directions

33:00.440 --> 33:02.200
to do things that they should not give anyone

33:02.200 --> 33:03.400
detailed directions to do.

33:03.400 --> 33:06.120
But the more modern language models are much older,

33:06.120 --> 33:08.680
but it's still sometimes possible.

33:08.680 --> 33:10.560
Sorry, but what I'm seeing as well,

33:10.560 --> 33:11.520
for a lot of corporations,

33:11.520 --> 33:13.640
a lot of corporations because of these worries

33:13.640 --> 33:16.440
will ship internal-facing product first

33:16.440 --> 33:19.320
because presumably, if it says the wrong thing

33:19.320 --> 33:20.680
to your own employee,

33:20.680 --> 33:23.120
more understanding, less likelihood scandal,

33:23.120 --> 33:25.800
and test products internally for quite a long time

33:25.800 --> 33:28.720
or even build capabilities for safe internal use

33:28.720 --> 33:31.640
before turning out to external use.

33:31.640 --> 33:33.960
But I do see different companies,

33:33.960 --> 33:36.680
yeah, different tools for trying to adjust these.

33:37.640 --> 33:38.800
Terrific, next question.

33:42.200 --> 33:43.640
Thank you so much.

33:43.640 --> 33:45.040
Hi there, my name is Chinat

33:45.040 --> 33:47.600
and I'm an international student from Hong Kong.

33:47.600 --> 33:49.160
I'm curious to ask, because, you know,

33:49.160 --> 33:50.680
I'm hoping that after I graduate,

33:50.680 --> 33:53.360
I can hopefully go back home to work closer with family.

33:53.360 --> 33:55.280
But at the same time, I feel like by going back,

33:55.280 --> 33:57.520
I'm closing a lot of doors behind me

33:57.520 --> 33:59.600
because, for example, in Hong Kong, for example,

33:59.600 --> 34:02.360
you can't access ChatGVT without a U.S. number,

34:02.360 --> 34:04.240
which makes access to some of these resources

34:04.240 --> 34:05.280
really difficult.

34:05.280 --> 34:07.200
So I'm curious to see what are your thoughts

34:07.200 --> 34:09.720
about navigating this complex modern landscape?

34:10.760 --> 34:13.800
Yeah, I don't want to comment on, I don't know,

34:14.760 --> 34:15.600
complicated.

34:15.600 --> 34:17.000
That's actually one thing I'm seeing.

34:17.000 --> 34:20.640
I've been to quite a few places, you know, in Asia recently.

34:21.520 --> 34:25.280
And what I'm seeing is that many countries

34:25.280 --> 34:28.960
are developing surprisingly good capabilities

34:28.960 --> 34:32.320
for building large language model applications.

34:32.320 --> 34:35.680
The concentration of talent for Genesebo AI deep tech

34:35.680 --> 34:39.800
is very concentrated in the San Francisco Bay Area.

34:39.800 --> 34:41.840
I think because there are basically two teams

34:41.840 --> 34:43.680
that did a lot of the early groundbreaking work,

34:43.680 --> 34:46.040
you know, Google Brain, my former team, and OpenAI.

34:46.040 --> 34:48.600
And subsequently, people left and started a lot of companies

34:48.600 --> 34:50.240
here in the California Bay Area.

34:50.240 --> 34:53.920
So I think that concentrated talent is very high.

34:53.920 --> 34:56.320
And it's interesting, even when I'm in Seattle,

34:56.320 --> 34:58.880
great city, love the city, on weekends,

35:00.080 --> 35:01.280
you know, I hang out with friends,

35:01.280 --> 35:03.800
but the conversations are not about Genesebo AI.

35:03.800 --> 35:06.960
Whereas here, if you go to a coffee shop,

35:06.960 --> 35:09.760
actually, one of my friends was visiting from Taiwan.

35:09.760 --> 35:11.320
So he was hanging out with us for a week.

35:11.320 --> 35:13.080
They went back and he said,

35:13.160 --> 35:15.440
yeah, I went to a coffee shop and, you know,

35:15.440 --> 35:16.960
there was no one talking about AI.

35:16.960 --> 35:17.920
That's so weird.

35:19.600 --> 35:21.080
So at least at this moment in time,

35:21.080 --> 35:23.640
there's really heavy concentration.

35:23.640 --> 35:26.400
But I see less the deep tech layer,

35:26.400 --> 35:27.760
but the application layer,

35:27.760 --> 35:31.320
I see that skillset developing quite quickly globally as well.

35:32.600 --> 35:34.440
Oh, and I think the option is to allow places

35:34.440 --> 35:35.600
to be local opportunities.

35:35.600 --> 35:37.400
So the shipping company that we built,

35:37.400 --> 35:39.760
we built with a Japanese company

35:39.760 --> 35:41.960
that happens to operate global lines of shipping.

35:41.960 --> 35:44.680
So I think a lot of the businesses will be, you know,

35:44.680 --> 35:48.240
playing locally whether country or that geography is strong.

35:48.240 --> 35:51.440
Those businesses will be more efficient to build in places

35:51.440 --> 35:53.000
other than Silicon Valley.

35:53.000 --> 35:55.960
Because where do I go to find a large seaport here

35:55.960 --> 35:57.920
to do that type of work?

35:58.960 --> 36:00.560
Hi, Andrew, thank you so much for your time.

36:00.560 --> 36:01.880
My name is Komal.

36:01.880 --> 36:05.280
I wanted to ask you if you think we'll ever reach a threshold

36:05.280 --> 36:07.080
on human dependence for AI,

36:07.080 --> 36:10.160
or if you think it'll just continue to grow exponentially?

36:10.960 --> 36:15.280
So I think we already really, really depend on tech, right?

36:15.280 --> 36:19.120
Imagine if, you know, if the internet were to shut down,

36:19.120 --> 36:20.280
I think people would die.

36:20.280 --> 36:21.400
I don't think that's the exact truth.

36:21.400 --> 36:23.960
I mean, but seriously, think about, you know,

36:23.960 --> 36:26.080
how we get through supply chain, healthcare.

36:26.080 --> 36:27.760
If the internet were to shut down,

36:27.760 --> 36:30.840
I think that will lead directly to, you know,

36:31.760 --> 36:33.960
what happens our water system, right?

36:33.960 --> 36:35.560
Healthcare system.

36:35.560 --> 36:39.560
So, and I think that technology is very useful

36:39.560 --> 36:44.080
and so long as the supplies remain reliable,

36:46.040 --> 36:49.880
I feel like it's okay to depend on technology.

36:49.880 --> 36:51.720
I mean, heck, I wish, I don't know,

36:51.720 --> 36:54.720
without dependence on agriculture system,

36:54.720 --> 36:56.240
how many of us would build a farm

36:56.240 --> 36:58.440
and hunt enough food to keep ourselves alive?

36:58.440 --> 37:00.800
Maybe, maybe we could do it, but it's pretty challenging.

37:00.800 --> 37:02.400
So I think dependence on tech

37:02.400 --> 37:04.560
seems to keep on growing for a while.

37:04.560 --> 37:05.480
But do you think there'll be a moment

37:05.480 --> 37:07.280
where there's a difference in that relationship,

37:07.280 --> 37:09.160
not just in degree, but in kind?

37:09.160 --> 37:11.120
You know, the famous singularity point

37:11.120 --> 37:12.880
where we don't even know what we don't even know

37:12.880 --> 37:14.880
about how technology is developing.

37:14.880 --> 37:16.600
Do you think that will occur?

37:16.600 --> 37:18.920
Yeah, you know, the technological singularity

37:18.920 --> 37:20.840
is one of those hypey things

37:20.840 --> 37:22.240
that I don't even know what it means.

37:22.240 --> 37:25.640
So it's one of those, it's exciting science fiction,

37:25.640 --> 37:26.960
but as an engineer and scientist,

37:26.960 --> 37:29.840
I don't know how to talk about it.

37:29.840 --> 37:31.600
It turns out there are a few terms in AI

37:31.600 --> 37:33.040
that are vague and undefined,

37:33.040 --> 37:34.320
but there are a lot of emotions,

37:34.320 --> 37:36.880
a lot of excitement about it.

37:36.880 --> 37:38.520
And I don't really know how to think

37:38.560 --> 37:41.120
about those things in a systematic, rational way.

37:41.120 --> 37:43.800
But I think, actually, there's actually one thing.

37:43.800 --> 37:46.120
I think that our relationship to technology

37:46.120 --> 37:48.160
is changing rapidly.

37:49.040 --> 37:52.400
Today, you know, I probably use

37:53.400 --> 37:58.320
chat, you know, GPT-4 or Bing or BOT

37:58.320 --> 37:59.840
pretty much every day now.

37:59.840 --> 38:02.840
And so the workflow of many people have changed.

38:03.840 --> 38:05.040
I think people are changing.

38:05.040 --> 38:05.880
And do you have a view?

38:05.880 --> 38:07.240
I know this also might be more of one

38:07.240 --> 38:10.240
of these sort of hot topics that's not substantive,

38:10.240 --> 38:12.440
but on the consciousness of AI,

38:12.440 --> 38:14.680
that AI will it become conscious?

38:14.680 --> 38:16.600
Yeah, so the thing about consciousness

38:16.600 --> 38:18.880
is it's important for the South Pole question,

38:18.880 --> 38:20.200
but I don't know of any test

38:20.200 --> 38:22.160
for whether something is conscious or not.

38:22.160 --> 38:23.840
So I think it's important philosophy

38:23.840 --> 38:25.600
and philosophy is important,

38:25.600 --> 38:27.680
but as an engineer and scientist,

38:27.680 --> 38:29.480
I don't know, there is no definition

38:29.480 --> 38:31.520
for what is conscious or not,

38:31.520 --> 38:35.160
which in does we can kind of debate it at length.

38:35.160 --> 38:39.480
And there's actually one other formula for hype,

38:39.480 --> 38:41.120
which is if someone comes up

38:41.120 --> 38:43.680
with a very simple definition for consciousness,

38:43.680 --> 38:44.520
so someone says,

38:44.520 --> 38:46.280
oh, if you can recognize yourself in the mirror,

38:46.280 --> 38:47.480
you're conscious, I made that up.

38:47.480 --> 38:48.920
It's not a good definition for consciousness.

38:48.920 --> 38:51.040
But you're aware of yourself, CSR in the mirror,

38:51.040 --> 38:52.840
then it's actually pretty easy to get a robot

38:52.840 --> 38:54.480
to recognize yourself in the mirror.

38:54.480 --> 38:56.400
And then you can generate newspaper headlines

38:56.400 --> 38:58.360
saying AI has achieved consciousness.

38:58.360 --> 38:59.960
What it did for your kind of, you know,

38:59.960 --> 39:02.160
silly little, for your very small definition

39:02.160 --> 39:04.520
of consciousness, but that gets misinterpreted

39:04.560 --> 39:07.160
by the broader public for a grander statement than it is.

39:07.160 --> 39:10.840
So I see some of that hype in AI as well.

39:10.840 --> 39:12.160
Thank you.

39:12.160 --> 39:13.080
Next question.

39:14.440 --> 39:17.960
Hi, earlier you outlined the AI stack.

39:17.960 --> 39:19.680
And recently we've seen a lot of cool things

39:19.680 --> 39:23.160
coming out of like NVIDIA, Intel and other like chip companies.

39:23.160 --> 39:24.640
I'm curious on what your thoughts are

39:24.640 --> 39:27.200
on what companies like AWS and Google,

39:27.200 --> 39:29.200
like in the infrastructure layer need to do

39:29.200 --> 39:32.160
in order to make like AI and enterprises and business

39:32.160 --> 39:34.120
really effective and possible.

39:34.120 --> 39:37.240
Sure, boy, so there's a lot going on in that space.

39:37.240 --> 39:40.480
By the way, you mentioned Intel and NVIDIA.

39:40.480 --> 39:41.920
I wouldn't, I think I'm actually seeing

39:41.920 --> 39:44.040
really exciting work from AMD as well.

39:44.040 --> 39:47.400
I've been pretty impressed by the MI 200, MI 250.

39:47.400 --> 39:50.760
I'm excited about the MI 300 GPUs coming out as well.

39:50.760 --> 39:53.640
And I think the ROCCOM stack is becoming, you know,

39:53.640 --> 39:56.880
not parity or CUDA, but better than most people

39:56.880 --> 39:57.960
give them credit for.

39:59.240 --> 40:01.000
But in terms of Adode and Google,

40:01.000 --> 40:02.440
so it turns out that if you were to use

40:02.440 --> 40:05.080
a lot of the LLM startup tools,

40:05.080 --> 40:06.840
the switching cost is actually pretty low.

40:06.840 --> 40:10.160
So if you were to start with one LLM API call,

40:10.160 --> 40:12.520
if you want to switch to a different LLM provider,

40:12.520 --> 40:15.200
the number of lines of code is actually pretty low.

40:15.200 --> 40:16.840
So there are low switching costs.

40:16.840 --> 40:19.000
But it turns out that a zero in Google Cloud

40:19.000 --> 40:21.400
and AWS are fantastic businesses

40:21.400 --> 40:23.880
because once you build on any of these clouds,

40:23.880 --> 40:25.880
you know, the switching costs tend to be very high

40:25.880 --> 40:28.640
because you have so many API hooks integrations.

40:28.640 --> 40:31.480
So that's why I think that a lot of the startups

40:31.520 --> 40:34.040
selling API calls still have, you know,

40:34.040 --> 40:37.160
some work to do to find a business model

40:38.040 --> 40:40.200
that may be somewhat more defensible.

40:40.200 --> 40:44.240
I think that OpenEye's chat GPD Enterprise,

40:44.240 --> 40:46.680
that feels like a more defensible business

40:46.680 --> 40:48.560
than just selling API calls.

40:48.560 --> 40:50.320
By the way, Sam was actually a Stanford undergrad.

40:50.320 --> 40:52.800
He actually interned, he's curious in my lab.

40:52.800 --> 40:55.480
So a lot of Stanford roots, but he's a smart guy.

40:55.480 --> 40:56.600
I'm sure he'll figure out,

40:56.600 --> 40:59.560
confidently he'll figure out some good directions.

40:59.560 --> 41:02.280
Yeah, and I think AWS and GCP and the Zora Rall

41:04.400 --> 41:09.080
racing to continue to develop LLM capabilities

41:09.080 --> 41:11.480
and make it easier to use and bring in more customers.

41:11.480 --> 41:14.200
And yeah, it's a very dynamic space.

41:14.200 --> 41:17.160
But as AI gets democratized,

41:17.160 --> 41:19.800
it feels like things are shifting more towards compute

41:19.800 --> 41:22.920
and data as predictors of success.

41:22.920 --> 41:23.760
If that's the case,

41:23.760 --> 41:25.320
do you think the locus of innovation shifts

41:25.320 --> 41:27.240
from academia to industry

41:27.280 --> 41:29.640
where the companies are gonna really be dominating

41:29.640 --> 41:30.960
at the forefront of AI?

41:30.960 --> 41:34.160
Yeah, so what I'm seeing now is that there's a subset,

41:34.160 --> 41:35.920
but there's a small subset of things

41:35.920 --> 41:38.120
that are easier to do in the big tech company,

41:38.120 --> 41:41.080
which are the ones that require massive compute resources.

41:41.080 --> 41:44.040
And I do think people's perceptions are distorted

41:44.040 --> 41:46.200
because frankly, I've been on the big tech companies before,

41:46.200 --> 41:47.040
right?

41:47.040 --> 41:48.480
So I understand your marketing and big tech companies,

41:48.480 --> 41:51.440
but the standard big tech company marketing is,

41:51.440 --> 41:54.680
look, you need the data, you need to compute,

41:54.680 --> 41:56.120
only we have it.

41:56.120 --> 41:58.840
Why don't you just give up and don't compete with us, right?

41:58.840 --> 42:01.320
And come apply for a job and come work with us.

42:01.320 --> 42:04.480
That is, this has been the explicit PR strategy

42:04.480 --> 42:06.160
of at least one big tech company

42:06.160 --> 42:09.360
because I know what was discussed internally

42:09.360 --> 42:11.280
exactly at that big tech company.

42:11.280 --> 42:14.600
So I would say don't buy into that marketing message.

42:14.600 --> 42:16.960
It is true that there is a subset of work

42:16.960 --> 42:19.000
that requires massive capital,

42:20.160 --> 42:21.680
training in very large foundation models.

42:21.680 --> 42:23.920
That is much easier to do in a big tech company

42:23.920 --> 42:25.720
than in academia like Stanford.

42:25.720 --> 42:29.080
But that's a small subset of all the happenings in AI.

42:29.080 --> 42:31.360
And there's plenty of work at Stanford,

42:31.360 --> 42:32.640
at the application layer.

42:32.640 --> 42:34.640
It turns out because of scaling laws,

42:34.640 --> 42:36.040
we're actually pretty good at predicting

42:36.040 --> 42:37.560
what will happen for very large models

42:37.560 --> 42:39.520
by training on more model size models.

42:39.520 --> 42:41.560
So very good scientific work can be done

42:41.560 --> 42:42.840
and much smaller models.

42:42.840 --> 42:47.320
And then also, I routinely run kind of models

42:47.320 --> 42:49.000
on my laptop for inference.

42:49.000 --> 42:51.360
Like, I don't know, when I'm on an airplane,

42:51.360 --> 42:53.280
you can run like the seven billion llama model

42:53.280 --> 42:54.120
on your laptop, right?

42:54.120 --> 42:55.400
And so there's actually a lot of stuff

42:55.400 --> 42:59.520
that you could run on your own personal computer.

42:59.520 --> 43:00.440
Thank you.

43:00.440 --> 43:01.280
Next question.

43:02.800 --> 43:03.840
Thank you, Andrew.

43:03.840 --> 43:08.680
So from an AI expert and also the investor's perspective,

43:08.680 --> 43:11.880
so what AI driven healthcare applications

43:11.880 --> 43:13.760
do you see have the great potentials

43:13.760 --> 43:15.800
to have the breakthrough in the future?

43:15.800 --> 43:19.680
And what challenges and obstacles should we be aware of?

43:19.680 --> 43:21.200
Thank you.

43:21.200 --> 43:23.960
Yeah, so boys, there's a lot of complexity to that question.

43:23.960 --> 43:26.120
So I feel like a lot of healthcare,

43:26.120 --> 43:29.600
people tend to focus on the diagnostics and the treatment.

43:29.600 --> 43:31.440
So I think lots of opportunities there.

43:31.440 --> 43:35.160
I think that the revenue model is to be sorted out.

43:35.160 --> 43:36.520
So we've seen, you know,

43:36.520 --> 43:40.360
Pear and Ikeli struggle in the public markets,

43:40.360 --> 43:43.880
kind of bankruptcy kind of levels almost.

43:44.760 --> 43:46.640
So I think prescriptive digital therapeutics

43:46.640 --> 43:48.720
is definitely going through challenges.

43:48.720 --> 43:51.200
But what's the recipe for shipping AI products

43:51.200 --> 43:53.960
and in the payer-provide ecosystem,

43:53.960 --> 43:55.560
what will pay us be willing to pay for?

43:55.560 --> 43:58.120
I think that many businesses are sorting that out.

43:58.120 --> 43:59.200
I think that will work.

43:59.200 --> 44:01.400
And there's actually one other huge set of opportunities

44:01.400 --> 44:03.680
in healthcare that tend to be underappreciated,

44:03.680 --> 44:04.960
which is operations.

44:04.960 --> 44:06.480
Instead of the medical stuff,

44:06.480 --> 44:08.400
things like scheduling, you know,

44:08.400 --> 44:10.680
who should scheduling the MRI machine

44:10.680 --> 44:14.040
or doing kind of patient management systems.

44:14.040 --> 44:16.160
I think that type of healthcare operations

44:16.160 --> 44:17.720
have fewer regulatory hurdles.

44:17.720 --> 44:19.800
I think is also a rich set of opportunities.

44:19.800 --> 44:22.040
And then lastly, there's the go-to-market question of,

44:22.040 --> 44:23.800
do you want to go to the market in the US

44:23.800 --> 44:27.240
or in other countries where the regulatory hurdle

44:27.240 --> 44:29.000
could be very different depending on,

44:29.000 --> 44:31.840
the US fortunately doesn't have as great a shortage

44:31.840 --> 44:33.640
of doctors as some other places.

44:33.640 --> 44:35.600
And there are therefore other places

44:35.600 --> 44:39.280
that are more amenable to your responsible

44:39.280 --> 44:42.480
but still easier adoption of AI than the United States.

44:42.480 --> 44:44.840
Okay, I have a super quick question.

44:44.840 --> 44:47.240
You mentioned that your team at the AI fund

44:47.240 --> 44:49.880
has so many ideas for AI applications

44:49.880 --> 44:51.600
that you have a whole son of them.

44:51.600 --> 44:54.680
What exactly is your process for generating these ideas?

44:54.680 --> 44:55.760
Oh, there's also that.

44:55.760 --> 44:57.000
And you have 30 seconds.

44:57.000 --> 44:59.080
We like working with subject matter experts

44:59.080 --> 45:01.120
that deeply understand the domain.

45:01.120 --> 45:03.320
It turns out that there are a lot of people in the world,

45:03.320 --> 45:06.120
you know, including like CEOs of Fortune 500 companies,

45:06.120 --> 45:08.560
but really a lot of people that really understand the domain

45:08.560 --> 45:09.880
have thought deeply about something

45:09.880 --> 45:11.800
for months or even a couple of years.

45:11.800 --> 45:13.240
And when we get together with them,

45:13.240 --> 45:15.600
they're sometimes very happy to share their idea with us

45:15.600 --> 45:17.200
because they've been looking for someone

45:17.200 --> 45:19.960
to validate or falsify it and also to help them build it.

45:19.960 --> 45:21.520
So we actually got a lot of ideas,

45:21.520 --> 45:23.720
some in turn out with a lot from subject matter experts

45:23.720 --> 45:26.360
that just not yet had an AI built upon there.

45:26.360 --> 45:27.600
Terrific, that's fantastic.

45:27.600 --> 45:29.800
Thank you Andrew so much for sharing your insights.

45:29.800 --> 45:30.640
Thank you.

45:30.640 --> 45:34.920
Lots of love.

45:34.920 --> 45:35.840
Thank you for sharing your insights

45:35.840 --> 45:38.200
with Stanford's ETL course MSME 472

45:38.200 --> 45:40.680
and the students all around the world.

45:40.680 --> 45:42.200
Everybody next week we're gonna be joined

45:42.200 --> 45:44.440
by Stanford professor Kathleen Eisenhardt

45:44.440 --> 45:47.520
here at ETL physically in person.

45:47.520 --> 45:50.400
Professor Eisenhardt is also the author of Simple Rules.

45:50.400 --> 45:52.520
You can find that event and other future events

45:52.520 --> 45:56.040
in this ETL series on the Stanford eCorner YouTube channel

45:56.040 --> 45:58.040
and you'll find even more of our videos, podcasts

45:58.040 --> 46:00.560
and articles about entrepreneurship and innovation

46:00.560 --> 46:01.760
at Stanford eCorner.

46:01.760 --> 46:03.880
That's ecorner.stanford.edu.

46:03.880 --> 46:05.160
Thank you everybody.

46:05.160 --> 46:06.000
Thank you, Andrew.

46:06.000 --> 46:07.000
Thanks, thanks Ravi.

