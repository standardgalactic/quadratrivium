1
00:00:00,000 --> 00:00:19,240
Welcome, everybody, to the Entrepreneurial Thought Leaders seminar, a Stanford seminar

2
00:00:19,240 --> 00:00:20,880
for aspiring entrepreneurs.

3
00:00:20,880 --> 00:00:27,400
ETL is presented by STVP, the Engineering Entrepreneurship Center here at Stanford,

4
00:00:28,000 --> 00:00:31,240
the Business Association of Stanford Entrepreneurial Students.

5
00:00:31,240 --> 00:00:35,080
I'm Ravi Balani, a lecturer in the Management Science and Engineering Department at Stanford

6
00:00:35,080 --> 00:00:38,440
and the Director of Alchemist and Accelerator for Enterprise Startups.

7
00:00:38,440 --> 00:00:42,520
Today, we are thrilled to welcome Andrew Ng to ETL.

8
00:00:42,520 --> 00:00:44,560
How many people know Andrew?

9
00:00:44,560 --> 00:00:47,960
Okay, so Andrew really doesn't need an introduction,

10
00:00:47,960 --> 00:00:50,200
but we will give one anyways for those who don't.

11
00:00:50,200 --> 00:00:53,200
Andrew is truly a child of the world.

12
00:00:53,200 --> 00:00:57,280
He was born in the UK to parents who emigrated to the UK from Hong Kong,

13
00:00:57,280 --> 00:01:01,320
but was raised in Hong Kong and Singapore, went to Carnegie Mellon

14
00:01:01,320 --> 00:01:04,280
and very early on signaled that he was no ordinary student.

15
00:01:04,280 --> 00:01:08,080
He got three bachelor's degrees at Carnegie Mellon in computer science,

16
00:01:08,080 --> 00:01:11,480
statistics, and economics, and graduated at the top of his class,

17
00:01:11,480 --> 00:01:15,240
then went on to MIT where he got a master's in electrical engineering and computer science,

18
00:01:15,240 --> 00:01:20,440
then came over to the left coast and got a PhD in Berkeley in computer science

19
00:01:20,480 --> 00:01:23,800
with a focus on artificial intelligence and reinforcement learning.

20
00:01:23,800 --> 00:01:29,800
Andrew is generally viewed as one of the preeminent thought leaders on AI today.

21
00:01:29,800 --> 00:01:34,400
He's the co-founder and head of Google Brain

22
00:01:34,400 --> 00:01:36,640
and the former chief scientist at Baidu,

23
00:01:36,640 --> 00:01:42,880
where he built the company's AI group into thousands of people, several thousand people.

24
00:01:42,880 --> 00:01:47,560
But he's as passionate about AI as he is also about the development of you,

25
00:01:47,560 --> 00:01:49,480
of students around the world.

26
00:01:49,480 --> 00:01:53,040
And I know there's a lot of love for Andrew.

27
00:01:53,040 --> 00:01:57,760
He's a former associates professor and director of the Stanford AI Lab

28
00:01:57,760 --> 00:02:00,120
and currently an adjunct professor in computer science at Stanford.

29
00:02:00,120 --> 00:02:04,520
How many people have taken one of Andrew's classes or want to take one of Andrew's classes?

30
00:02:04,520 --> 00:02:08,760
And he's a beloved professor here at Stanford,

31
00:02:08,760 --> 00:02:12,720
but he's also viewed as a beloved teacher to millions outside of Stanford.

32
00:02:12,720 --> 00:02:17,240
He's the co-founder and chairman of Coursera, the world's largest MOOC platform,

33
00:02:17,280 --> 00:02:21,560
and through his online education work and his online AI education work,

34
00:02:21,560 --> 00:02:24,040
he's reached over 7 million people.

35
00:02:24,040 --> 00:02:30,240
He was listed as one of the world's 100 most influential people by Time Magazine in 2013.

36
00:02:30,240 --> 00:02:34,360
And today, Andrew's the managing director and partner at the AI Fund,

37
00:02:34,360 --> 00:02:37,760
which is a startup studio building new AI companies from the ground up

38
00:02:37,760 --> 00:02:41,040
and is also the founder of deeplearning.ai.

39
00:02:41,040 --> 00:02:43,800
He focuses his time primarily on his entrepreneurial adventures,

40
00:02:43,840 --> 00:02:49,400
looking for the best ways to accelerate responsible AI practices in the larger global economy.

41
00:02:49,400 --> 00:02:52,600
There's fantastic content already online that Andrew has given,

42
00:02:52,600 --> 00:02:56,080
including a longer version of today's talk that you can find on YouTube.

43
00:02:56,080 --> 00:02:59,480
And so instead of reduplicating that, Andrew's going to give a teaser talk,

44
00:02:59,480 --> 00:03:03,760
a 10-minute discussion, followed by we'll do a quick fireside chat,

45
00:03:03,760 --> 00:03:07,560
and then we're going to open it up for really interactive Q&A with you.

46
00:03:07,560 --> 00:03:11,200
So start thinking about your questions now because the time is going to fly by,

47
00:03:11,200 --> 00:03:13,440
but without further ado, please welcome Andrew.

48
00:03:13,800 --> 00:03:14,960
Woo!

49
00:03:19,960 --> 00:03:22,720
Thanks a lot, Ravi. Thanks. Good to see everyone here.

50
00:03:22,720 --> 00:03:25,160
Can everyone in the back hear me okay?

51
00:03:25,160 --> 00:03:26,120
Cool, awesome.

52
00:03:26,120 --> 00:03:31,120
You know, I've taught CS229, my machine learning class, in this room many years,

53
00:03:31,120 --> 00:03:36,160
but all these years I've taught in this room, I've never seen my face that big before.

54
00:03:38,880 --> 00:03:43,400
What I'd like to do today is chat to you about opportunities in AI.

55
00:03:43,400 --> 00:03:47,720
So one of the difficult things to understand about AI

56
00:03:47,720 --> 00:03:51,680
is a general-purpose technology, similar to electricity,

57
00:03:51,680 --> 00:03:53,520
meaning it's not useful just for one thing,

58
00:03:53,520 --> 00:03:56,000
it's useful for a lot of different applications.

59
00:03:56,000 --> 00:03:58,880
If I were to ask you what is electricity good for,

60
00:03:58,880 --> 00:04:02,480
it's almost hard to answer that because it's useful for so many different things,

61
00:04:02,480 --> 00:04:05,800
and AI is like that too.

62
00:04:05,800 --> 00:04:11,240
So one of the major trends that we've seen in the last few years

63
00:04:11,280 --> 00:04:15,840
is that prompting is revolutionizing AI application development.

64
00:04:15,840 --> 00:04:17,840
And I want to just dive a little bit deeper into this

65
00:04:17,840 --> 00:04:19,760
because I know this is an engineering class,

66
00:04:19,760 --> 00:04:22,080
I know many of you may be from an engineering background,

67
00:04:22,080 --> 00:04:26,560
and I'm going to just go a little bit deeper into this than I might otherwise.

68
00:04:26,560 --> 00:04:31,240
But if you were to, say, want to build an AI system for many years,

69
00:04:31,240 --> 00:04:33,320
the typical approach is use supervised earnings.

70
00:04:33,320 --> 00:04:38,120
So let's say I want to build a system to rate restaurant reviews as positive or negative sentiment,

71
00:04:38,400 --> 00:04:42,160
then you would collect data, maybe that takes me a month,

72
00:04:42,160 --> 00:04:45,440
I would train an AI model, maybe that takes me a few months,

73
00:04:45,440 --> 00:04:50,720
find a cloud service to deploy my AI model, maybe that takes a few months.

74
00:04:50,720 --> 00:04:53,680
And so for the past, most of the past decade,

75
00:04:53,680 --> 00:04:59,080
a realistic timeline to build and deploy a valuable AI system was maybe six or 12 months.

76
00:04:59,080 --> 00:05:04,280
But with prompting, the timeline is now very different.

77
00:05:04,280 --> 00:05:07,120
You can specify a prompt in minutes or hours,

78
00:05:07,120 --> 00:05:11,480
and then deploy a system to production in just hours or days.

79
00:05:11,480 --> 00:05:14,640
And I know that probably many or maybe most,

80
00:05:14,640 --> 00:05:19,920
maybe all of you will have played with large language models as a consumer too,

81
00:05:19,920 --> 00:05:23,120
like chat GPD and bot and bing chat.

82
00:05:23,120 --> 00:05:25,760
I think that in terms of start-up opportunities,

83
00:05:25,760 --> 00:05:28,360
I'm excited about the use of large language models,

84
00:05:28,360 --> 00:05:30,840
not as a consumer too, which is fantastic and exciting.

85
00:05:30,840 --> 00:05:33,360
I think use chat GPD and bot regularly,

86
00:05:33,360 --> 00:05:37,440
but instead the application of large language models as a developer too,

87
00:05:37,440 --> 00:05:41,240
because this is allowing a lot more applications to be built

88
00:05:41,240 --> 00:05:45,280
and dramatically lowering the barrier to building many applications.

89
00:05:45,280 --> 00:05:48,400
So I know that in this talk, you don't normally have speakers write code,

90
00:05:48,400 --> 00:05:51,280
but this is an engineering class.

91
00:05:51,280 --> 00:05:55,840
So let me actually show you exactly what I mean by that.

92
00:05:55,840 --> 00:05:59,960
It turns out that if I want to build an AI system today,

93
00:06:00,000 --> 00:06:02,200
this is all the code I need.

94
00:06:02,200 --> 00:06:05,840
And this means that if you take CS 106 or something,

95
00:06:05,840 --> 00:06:08,600
learn to code in a CS class,

96
00:06:08,600 --> 00:06:13,160
with just a little bit of code, import AI, open i2s, load my key,

97
00:06:13,160 --> 00:06:18,520
I don't know, D, what, ST, P lectures,

98
00:06:18,520 --> 00:06:23,160
I break it, so many friends.

99
00:06:25,240 --> 00:06:27,120
I've never written that before.

100
00:06:27,120 --> 00:06:31,800
And so hopefully this, okay, thank goodness, got that right.

101
00:06:31,800 --> 00:06:33,400
And so this is positive sentiment.

102
00:06:33,400 --> 00:06:38,200
And just in seconds, that's all the code it takes now to build an AI system

103
00:06:38,200 --> 00:06:41,200
in code to look at a piece of text and process it,

104
00:06:41,200 --> 00:06:46,680
to look at a piece of email and route it or to start to build the beginnings of a chat bot.

105
00:06:46,680 --> 00:06:51,160
So over the last, I don't know, half year, one of my teams,

106
00:06:52,080 --> 00:06:57,320
has been working with many of the AI tool builders to create short courses

107
00:06:57,320 --> 00:07:00,160
on how to use tools like I just showed you.

108
00:07:00,160 --> 00:07:04,480
Because there are many AI applications that used to take me six months to build

109
00:07:04,480 --> 00:07:08,240
that I think any of you will now be able to build in one or two days.

110
00:07:08,240 --> 00:07:12,280
And this opens up the set of things that you could do

111
00:07:12,280 --> 00:07:14,680
and the set of prototypes you can build.

112
00:07:14,680 --> 00:07:16,440
And in fact, from a startup perspective,

113
00:07:16,440 --> 00:07:18,520
when it took us six months to build something,

114
00:07:18,520 --> 00:07:23,120
what we do is have a product manager study it, do the user studies,

115
00:07:23,120 --> 00:07:25,400
make sure it's the right thing to build, then go build it.

116
00:07:25,400 --> 00:07:28,800
And after all that investment is like, boy, let's hope it works.

117
00:07:28,800 --> 00:07:32,440
But what I'm seeing with these very fast development times is

118
00:07:32,440 --> 00:07:35,160
if it takes you a couple of days to build something,

119
00:07:35,160 --> 00:07:38,320
I'm seeing a lot more startups as well as big companies say,

120
00:07:38,320 --> 00:07:40,960
you know what, I have 10 ideas for features.

121
00:07:40,960 --> 00:07:43,680
I'm going to build all 10 things and then just ship them all.

122
00:07:43,680 --> 00:07:48,480
And then we'll see how users use them or don't use them and just keep what sticks.

123
00:07:48,520 --> 00:07:51,840
And this is a very different prototyping, much lean in methodology

124
00:07:51,840 --> 00:07:54,440
than I've seen startups use before prompting.

125
00:07:54,440 --> 00:07:57,960
One important caveat, which is that responsible AI is important.

126
00:07:57,960 --> 00:08:01,080
So don't do this, don't ship things that could cause harm.

127
00:08:01,080 --> 00:08:06,200
But we have a lot of applications like inspecting bits of metal and factories

128
00:08:06,200 --> 00:08:08,480
where there is really no harm, no risk of bias,

129
00:08:08,480 --> 00:08:10,880
where I think there's very fast shipping methodology

130
00:08:10,880 --> 00:08:14,240
that's just innovate very quickly in AI.

131
00:08:14,240 --> 00:08:15,880
So where are the opportunities?

132
00:08:15,880 --> 00:08:20,280
So the size of these circles shows what I think is the value

133
00:08:20,280 --> 00:08:23,240
of different AI technologies today.

134
00:08:23,240 --> 00:08:26,640
Supervised learning started to work really well about a decade ago

135
00:08:26,640 --> 00:08:29,440
and labeling things, such as label does that as,

136
00:08:29,440 --> 00:08:31,360
is it something you're likely to click on or not?

137
00:08:31,360 --> 00:08:35,120
Or label this X-ray with, you know, what's the medical diagnosis?

138
00:08:35,120 --> 00:08:37,880
And supervised learning for a single company like Google

139
00:08:37,880 --> 00:08:40,760
is worth more than $100 billion a year,

140
00:08:40,760 --> 00:08:43,120
and there are millions of developers working on it.

141
00:08:43,160 --> 00:08:47,040
And it might even grow in the next three years to double, say.

142
00:08:47,040 --> 00:08:50,480
So massive momentum, lots of applications to be figured out.

143
00:08:50,480 --> 00:08:52,880
And then Genes of AI is a new entrance where,

144
00:08:52,880 --> 00:08:56,440
frankly, the revenue, the value of the revenue from Genes of AI today

145
00:08:56,440 --> 00:08:59,280
is much smaller, but given the amounts of interest,

146
00:08:59,280 --> 00:09:00,800
excitement, and commercial interest,

147
00:09:00,800 --> 00:09:03,840
I think it will much more than double in the next three years.

148
00:09:03,840 --> 00:09:07,040
And three years is an artificially short time horizon.

149
00:09:07,040 --> 00:09:08,640
I think it were to look out six years

150
00:09:08,640 --> 00:09:10,720
if it continues to compound at this rate.

151
00:09:10,720 --> 00:09:14,560
Maybe the value from Genes of AI will even start to approach

152
00:09:14,560 --> 00:09:16,120
that of supervised learning.

153
00:09:16,120 --> 00:09:19,520
But all that room for growth, the light-shaded region

154
00:09:19,520 --> 00:09:21,440
for supervised learning or Genes of AI,

155
00:09:21,440 --> 00:09:23,920
which are probably the two most important tools today,

156
00:09:23,920 --> 00:09:27,600
are where there are a lot of opportunities for any of us

157
00:09:27,600 --> 00:09:31,040
to identify and build to concrete use cases.

158
00:09:31,040 --> 00:09:33,280
And what I hope to take away from this talk

159
00:09:33,280 --> 00:09:36,960
is AI technologies are general-purpose technologies,

160
00:09:36,960 --> 00:09:39,880
meaning that they're useful for many different tasks.

161
00:09:39,880 --> 00:09:42,160
When supervised learning started to work,

162
00:09:42,160 --> 00:09:45,960
well, about a decade ago, it actually took us a long time.

163
00:09:45,960 --> 00:09:48,280
It took us annoyingly long over the last decade,

164
00:09:48,280 --> 00:09:51,400
and it will take us annoyingly long over the next decade

165
00:09:51,400 --> 00:09:53,840
to figure out use cases for Genes of AI.

166
00:09:53,840 --> 00:09:56,880
But do you want to use this to make ships more fusion

167
00:09:56,880 --> 00:09:58,440
or for medical diagnosis

168
00:09:58,440 --> 00:10:01,480
or for education product recommendations or something else?

169
00:10:01,480 --> 00:10:04,680
It was still figuring out concrete use cases

170
00:10:04,680 --> 00:10:06,160
for supervised learning.

171
00:10:06,160 --> 00:10:08,080
And even though we're not yet done doing that,

172
00:10:08,080 --> 00:10:10,440
we have another fantastic new tool, Genes of AI,

173
00:10:10,440 --> 00:10:11,720
that even further expands

174
00:10:11,720 --> 00:10:13,560
the set of things we now do of AI.

175
00:10:14,440 --> 00:10:16,960
And one important caveat,

176
00:10:16,960 --> 00:10:19,760
which is there will be fans along the way.

177
00:10:19,760 --> 00:10:21,920
How many of you remember Lenzer?

178
00:10:21,920 --> 00:10:23,120
Raise your hand if you do.

179
00:10:23,120 --> 00:10:24,320
Wow, almost no one.

180
00:10:24,320 --> 00:10:26,080
That's fascinating.

181
00:10:26,080 --> 00:10:30,880
So Lenzer's revenues took off like that through last December.

182
00:10:32,040 --> 00:10:34,840
It was this app that could let you

183
00:10:34,840 --> 00:10:36,400
upload a few pictures of yourself

184
00:10:36,400 --> 00:10:38,360
and draw a cool picture of you as an astronaut

185
00:10:38,360 --> 00:10:40,080
or a scientist or something.

186
00:10:40,080 --> 00:10:43,720
And it was a really good, really hot product

187
00:10:43,720 --> 00:10:47,640
until last December, after which his revenues did that.

188
00:10:47,640 --> 00:10:50,040
And I think that's because Lenzer was one

189
00:10:50,040 --> 00:10:52,520
of what will probably turn out to be multiple,

190
00:10:52,520 --> 00:10:55,480
thin software layers built on top of someone else's

191
00:10:55,480 --> 00:10:56,680
very powerful API.

192
00:10:56,680 --> 00:10:58,400
That was a good idea, people liked it,

193
00:10:58,400 --> 00:11:01,640
but it wasn't a long-term, defensible business.

194
00:11:01,640 --> 00:11:05,400
And when I think about Genes of AI as a developer platform,

195
00:11:05,400 --> 00:11:09,480
I'm reminded of when Steve Jobs gave us this phone, right?

196
00:11:09,480 --> 00:11:12,720
And shortly after, someone wrote an app

197
00:11:12,720 --> 00:11:16,200
that I paid $199 for to do this,

198
00:11:16,200 --> 00:11:18,200
to turn the phone into a flashlight.

199
00:11:18,200 --> 00:11:20,440
And this was also a good idea, it was a great product,

200
00:11:20,440 --> 00:11:23,440
but it just was not a defensible business either

201
00:11:23,440 --> 00:11:25,440
because it was a very thin software layer built on top

202
00:11:25,440 --> 00:11:28,640
of someone else's very powerful development platform.

203
00:11:28,640 --> 00:11:31,200
But in the same way, after we got the iPhone,

204
00:11:31,200 --> 00:11:32,360
after we got the smartphone,

205
00:11:32,360 --> 00:11:35,840
someone else figured out how to build Uber, Airbnb, and Tinder,

206
00:11:35,840 --> 00:11:38,600
much longer-term, defensible, very valuable businesses

207
00:11:38,600 --> 00:11:40,960
that are still standing the test of time.

208
00:11:40,960 --> 00:11:43,080
And I think we have those opportunities as well

209
00:11:43,080 --> 00:11:45,840
to build long-term, valuable franchises,

210
00:11:45,840 --> 00:11:48,360
businesses on top of Genes of AI.

211
00:11:48,360 --> 00:11:50,920
So where are the opportunities?

212
00:11:50,920 --> 00:11:55,920
So I felt years ago, but even more strongly now,

213
00:11:56,400 --> 00:11:58,680
that because of emerging AI technology,

214
00:11:58,680 --> 00:12:00,440
there are a lot of projects that are now possible,

215
00:12:00,440 --> 00:12:04,680
that were not possible one or a handful of years ago.

216
00:12:04,680 --> 00:12:08,400
And I wound up starting AI Fund, which is a venture studio,

217
00:12:08,400 --> 00:12:11,640
that sequentially works as entrepreneurs to start companies.

218
00:12:11,640 --> 00:12:14,920
We actually average about one startup a month now,

219
00:12:14,920 --> 00:12:18,000
because I felt, I previously, as Ravi mentioned,

220
00:12:18,000 --> 00:12:21,320
previously I had led AI teams in Google and Baidu,

221
00:12:21,320 --> 00:12:25,120
and even having led AI teams in Big Tech,

222
00:12:25,120 --> 00:12:28,040
I couldn't see how we could possibly operate a team

223
00:12:28,040 --> 00:12:31,080
in a Big Tech company to pursue the very diverse,

224
00:12:31,080 --> 00:12:34,520
very different sets of opportunities that I saw

225
00:12:34,520 --> 00:12:37,280
and wanted to pursue, and starting different startups

226
00:12:37,280 --> 00:12:40,400
to pursue those valuable projects seem more efficient

227
00:12:40,400 --> 00:12:43,520
than having one company, even the Big Tech company,

228
00:12:43,520 --> 00:12:46,280
go after such a large set of resources.

229
00:12:46,280 --> 00:12:49,640
But having said that, I think AI and Genes of AI

230
00:12:49,640 --> 00:12:52,920
also offers a lot of opportunities for incumbent companies,

231
00:12:52,920 --> 00:12:55,080
which often have a distribution advantage.

232
00:12:56,160 --> 00:12:57,640
Where exactly are the opportunities?

233
00:12:57,640 --> 00:12:59,880
So this is what I think of as the AI stack.

234
00:12:59,880 --> 00:13:03,840
At the lowest level is the hardware layer, very valuable,

235
00:13:03,840 --> 00:13:07,080
but also very capital-intensive, needs a lot of resources to build

236
00:13:07,080 --> 00:13:08,320
and very concentrated.

237
00:13:08,320 --> 00:13:11,400
So I'm sure there'll be valuable startups built there,

238
00:13:11,400 --> 00:13:12,680
but I personally don't play there,

239
00:13:12,680 --> 00:13:16,320
because of how capital-intensive and how concentrated it is.

240
00:13:16,320 --> 00:13:17,840
There's a cloud infrastructure layer,

241
00:13:17,840 --> 00:13:20,920
also very capital-intensive, very concentrated, very valuable,

242
00:13:20,920 --> 00:13:24,920
but at least when I build startups, I tend not to play there.

243
00:13:24,920 --> 00:13:27,640
The other layer that's interesting is the developer tooling layer.

244
00:13:27,640 --> 00:13:32,640
So what you just saw me do was use OpenAI as a developer tool.

245
00:13:32,640 --> 00:13:35,560
And I see this space as hyper-competitive,

246
00:13:35,560 --> 00:13:38,280
look at all the startups chasing OpenAI,

247
00:13:38,280 --> 00:13:40,320
but there will be some mega winners.

248
00:13:40,320 --> 00:13:45,040
So whereas incumbents have a startup distribution advantage,

249
00:13:45,040 --> 00:13:47,640
I think for many startups, having a technology advantage

250
00:13:47,640 --> 00:13:50,360
may give you a best shot at doing something meaningful there.

251
00:13:50,360 --> 00:13:52,160
So I personally tend to play here

252
00:13:52,160 --> 00:13:54,680
only when we think we have a technology advantage,

253
00:13:54,680 --> 00:13:59,280
because that buys us a better chance to become one of the huge winners.

254
00:13:59,280 --> 00:14:01,840
And then with most ways of technology innovation,

255
00:14:01,840 --> 00:14:03,640
a lot of the media attention, social media,

256
00:14:03,640 --> 00:14:06,360
what people tend to talk about is the tooling of the technology layer.

257
00:14:06,360 --> 00:14:10,560
There's one other layer that I think has got to be even more valuable,

258
00:14:10,560 --> 00:14:12,280
and that's the application layer.

259
00:14:12,280 --> 00:14:15,120
Because in many ways of technology,

260
00:14:15,120 --> 00:14:18,680
for the in-front and tooling layer to be successful,

261
00:14:18,680 --> 00:14:20,960
applications need to be built on top of them.

262
00:14:20,960 --> 00:14:22,600
They generate even more revenue

263
00:14:22,600 --> 00:14:26,440
so that they can afford to play the infrastructure layer.

264
00:14:26,440 --> 00:14:28,960
And what I'm seeing is that there are a lot of opportunities

265
00:14:28,960 --> 00:14:32,520
at the application layer where the intensity of competition

266
00:14:32,520 --> 00:14:35,440
is not, frankly, not nearly as high.

267
00:14:35,440 --> 00:14:37,640
Maybe just one example.

268
00:14:37,640 --> 00:14:41,080
I've been chatting a lot with the CEO of Meno,

269
00:14:41,080 --> 00:14:48,360
which is a startup that applies AI to romantic relationship coaching.

270
00:14:48,360 --> 00:14:50,040
And I'm an AI guy.

271
00:14:50,040 --> 00:14:53,240
I feel like I don't know anything about romance.

272
00:14:53,240 --> 00:14:55,920
And if you don't believe me, you can ask my wife.

273
00:14:55,920 --> 00:14:59,200
She would confirm that I don't know anything about romance.

274
00:14:59,200 --> 00:15:03,560
But when we decided, when we had conviction that AI could be applied to relationships,

275
00:15:03,560 --> 00:15:08,240
we wound up partnering with Renata Nyborg, who's the former CEO of Tinder.

276
00:15:08,240 --> 00:15:10,720
And because she ran Tinder, she understands relationships

277
00:15:10,720 --> 00:15:14,320
in a very systematic way, more so than anyone else I know.

278
00:15:14,320 --> 00:15:16,400
And so with my team providing AI expertise

279
00:15:16,400 --> 00:15:18,280
and her providing relationship expertise,

280
00:15:18,280 --> 00:15:23,280
we're able to build a pretty unique relationship mentoring application

281
00:15:23,280 --> 00:15:25,880
that we just announced a few weeks ago.

282
00:15:25,880 --> 00:15:28,720
And this is not part of, you know, Renata actually occasionally

283
00:15:28,720 --> 00:15:31,720
stops by Stanford campus and talks to Stanford students

284
00:15:31,720 --> 00:15:33,840
as part of her user product research.

285
00:15:33,840 --> 00:15:36,440
So it's possible that we've seen her around.

286
00:15:36,440 --> 00:15:39,040
Just one last thing, I'd love to go to GNA.

287
00:15:39,040 --> 00:15:42,440
Over the last few years, AI fund we've been tuning our process

288
00:15:42,440 --> 00:15:45,280
for building startups and we just share that with you.

289
00:15:45,280 --> 00:15:48,000
So we often start off with a lot of ideas, right?

290
00:15:48,000 --> 00:15:51,800
And one example of another startup we built was Bering AI,

291
00:15:51,800 --> 00:15:55,080
which uses AI for smart routing of very large ocean growing vessels.

292
00:15:55,080 --> 00:15:58,520
So if you're a ship captain, should you sail at 20 knots or 22 knots?

293
00:15:58,520 --> 00:15:59,280
Who knows?

294
00:15:59,280 --> 00:16:01,840
Most ship captains just make some decision.

295
00:16:01,840 --> 00:16:07,200
But because we're able to get global weather and ocean current data,

296
00:16:07,200 --> 00:16:10,560
we can make recommendations to ship captains for how to get there on time

297
00:16:10,560 --> 00:16:12,840
and use about 10% less fuel.

298
00:16:12,840 --> 00:16:15,440
But this idea was suggested to me by Mitsui,

299
00:16:15,440 --> 00:16:18,440
which is a major shareholder in a major shipping line

300
00:16:18,440 --> 00:16:20,920
that operates very large ocean growing vessels.

301
00:16:20,920 --> 00:16:23,560
And just one of those things, I would never have thought of this idea myself

302
00:16:23,560 --> 00:16:27,920
because I've been on a boat, but what do I know about global maritime shipping?

303
00:16:27,920 --> 00:16:30,960
But Mitsui suggested this idea to me.

304
00:16:30,960 --> 00:16:35,400
And we then validate the idea, make sure there's technical feasibility

305
00:16:35,400 --> 00:16:37,720
and market need, recruit a CEO.

306
00:16:37,720 --> 00:16:40,480
We were fortunate to find Dylan Kyle, who's a fantastic CEO

307
00:16:40,480 --> 00:16:42,480
with one successful exit before.

308
00:16:42,480 --> 00:16:45,080
And then we spent three months in our current process

309
00:16:45,080 --> 00:16:49,280
building a technical prototype with the CEO and doing deep customer validation.

310
00:16:49,280 --> 00:16:53,520
If it survives, two-thirds chance of surviving, one-third chance of not surviving.

311
00:16:53,520 --> 00:16:57,120
We then write a check-in that allows the company to build higher executives,

312
00:16:57,120 --> 00:17:02,040
build an MVP, and off it goes to raise additional rounds of capital.

313
00:17:02,040 --> 00:17:03,880
And I think this is what we...

314
00:17:03,880 --> 00:17:06,080
And so bearing AI, well, now it's actually...

315
00:17:06,080 --> 00:17:10,320
There are now hundreds of ships on the high seas guided by bearing AI.

316
00:17:10,320 --> 00:17:13,200
Ships guided by bearing AI have 75 million miles,

317
00:17:13,200 --> 00:17:16,000
which is the equivalent of going 3,000 times around the planet

318
00:17:16,000 --> 00:17:20,200
and save about half a million dollars in fuel costs per ship per year

319
00:17:20,200 --> 00:17:22,080
in addition to significant carbon emissions.

320
00:17:22,080 --> 00:17:23,320
I think we'll save about...

321
00:17:23,320 --> 00:17:27,240
I want to save about a million tons of CO2 emissions so far.

322
00:17:27,240 --> 00:17:28,960
But this kind of idea that...

323
00:17:28,960 --> 00:17:31,040
I would never have come up with this idea myself,

324
00:17:31,040 --> 00:17:33,720
but I've learned that my swim lane is AI,

325
00:17:33,720 --> 00:17:35,760
but when I work with experts in other sectors,

326
00:17:35,760 --> 00:17:39,240
there are often these exciting opportunities that are very valuable.

327
00:17:39,240 --> 00:17:42,880
But frankly, how many groups in the world are experts in AI and shipping

328
00:17:42,880 --> 00:17:44,360
or expert in AI and relationships?

329
00:17:44,360 --> 00:17:49,720
I find that the competition intensity at the application layer is often much lower.

330
00:17:49,720 --> 00:17:52,720
And then just one last thing, kind of just full disclosure

331
00:17:52,720 --> 00:17:55,320
and something that I hope all of you will do too.

332
00:17:55,320 --> 00:17:58,960
My team's only work on projects that we think move humanity forward.

333
00:17:58,960 --> 00:18:00,560
Response for AI is important.

334
00:18:00,560 --> 00:18:02,320
And on multiple occasions, we've killed

335
00:18:02,320 --> 00:18:07,080
and I will continue to kill projects that we may assess to be financially sound

336
00:18:07,080 --> 00:18:09,120
but based on ethical grounds.

337
00:18:09,120 --> 00:18:11,560
So lots of exciting opportunities.

338
00:18:11,560 --> 00:18:14,320
I think at Stanford, the loss of great costs is going to take

339
00:18:14,320 --> 00:18:18,280
in engineering and elsewhere to learn about that AI tech.

340
00:18:18,280 --> 00:18:23,640
And then when you find applications or go play at the infrar and tooling layer too,

341
00:18:23,640 --> 00:18:25,120
I think there are lots of opportunities.

342
00:18:25,120 --> 00:18:26,400
But I think there are...

343
00:18:26,400 --> 00:18:29,400
What I'm seeing is, frankly, my team at AI Fund,

344
00:18:29,400 --> 00:18:31,040
we have so many startup ideas.

345
00:18:31,040 --> 00:18:32,920
We use a task management software.

346
00:18:32,920 --> 00:18:36,120
We use Asana to track this huge list of ideas.

347
00:18:36,120 --> 00:18:37,720
And it's actually quite clear to me.

348
00:18:37,720 --> 00:18:40,440
There are a lot more good ideas for AI businesses

349
00:18:40,440 --> 00:18:44,640
than people with the skill to work on them at this moment in time.

350
00:18:44,640 --> 00:18:48,160
So hopefully, there'll be more than enough projects for everyone.

351
00:18:48,160 --> 00:18:50,960
There are all of you, all of us, to work on.

352
00:18:50,960 --> 00:18:51,920
All right, thank you.

353
00:18:55,920 --> 00:18:58,920
I wanted to just start off with that closing statement that you made

354
00:18:58,920 --> 00:19:02,120
about how there's more opportunity than there are students with skills

355
00:19:02,120 --> 00:19:04,080
or people with skills to pursue them.

356
00:19:04,080 --> 00:19:06,480
And given that we have an audience full of students,

357
00:19:06,480 --> 00:19:09,480
I wanted to start off by mapping out advice for students

358
00:19:09,480 --> 00:19:11,880
that are entering into the university regarding AI.

359
00:19:11,880 --> 00:19:15,320
So if you want to pursue a career in AI right now,

360
00:19:15,320 --> 00:19:18,360
and let's say your child was entering Stanford,

361
00:19:18,360 --> 00:19:22,600
what advice would you give them in terms of how to spend their time?

362
00:19:22,600 --> 00:19:26,520
Yeah, so, you know, there's one thing that's actually really worth doing

363
00:19:26,520 --> 00:19:30,200
when you're sent to students, which is take classes.

364
00:19:30,200 --> 00:19:33,880
Because it turns out that I feel like there's actually one pattern I see

365
00:19:33,960 --> 00:19:38,240
for both undergraduates and graduate students, including PhD students,

366
00:19:38,240 --> 00:19:40,680
which is there's so much exciting stuff to do,

367
00:19:40,680 --> 00:19:42,480
you just want to jump in and do it, right?

368
00:19:42,480 --> 00:19:46,080
In fact, I've seen them undergrads in their freshman year,

369
00:19:46,080 --> 00:19:49,040
you know, try to join a research lab and start doing work in AI.

370
00:19:49,040 --> 00:19:51,040
That's okay, nothing wrong with that.

371
00:19:51,040 --> 00:19:54,960
But it turns out that while project work is one way to learn,

372
00:19:54,960 --> 00:19:58,800
coursework is, I think, an even more efficient way to learn,

373
00:19:58,800 --> 00:20:01,600
especially when it comes to mastering the fundamentals,

374
00:20:01,600 --> 00:20:04,920
because professors have put a lot of work to organize the material

375
00:20:04,920 --> 00:20:08,160
in a way that's efficient to learn and digest.

376
00:20:08,160 --> 00:20:12,800
So I would say, you know, take classes in AI technology

377
00:20:12,800 --> 00:20:15,840
or in entrepreneurship and gain those skills.

378
00:20:15,840 --> 00:20:19,800
I've seen students jump in and then if you are trying to work in a research lab

379
00:20:19,800 --> 00:20:21,800
without strong skills, you end up, you know, like,

380
00:20:21,800 --> 00:20:24,680
labeling data or something, which is fine, you learn some things,

381
00:20:24,680 --> 00:20:27,680
but you actually learn a lot from taking courses.

382
00:20:27,680 --> 00:20:29,240
And then in addition to that,

383
00:20:29,280 --> 00:20:31,960
after you start to master the foundational skills,

384
00:20:31,960 --> 00:20:35,160
after you know how to use AI technology or, you know, then,

385
00:20:35,160 --> 00:20:38,600
as you start to practice, find exciting use cases across campus.

386
00:20:38,600 --> 00:20:42,280
I do a lot of work, you know, over with people over in climate science

387
00:20:42,280 --> 00:20:45,800
or in healthcare to take my AI expertise

388
00:20:45,800 --> 00:20:47,840
and then navigate with a different discipline

389
00:20:47,840 --> 00:20:50,520
that I'm not expert in to find exciting applications.

390
00:20:50,520 --> 00:20:54,760
And hopefully that type of practice will help many of you find exciting

391
00:20:54,760 --> 00:20:56,240
projects to work on as well.

392
00:20:56,240 --> 00:20:58,560
Do you need to take technical classes?

393
00:20:58,560 --> 00:21:01,240
Do you think you need to take computer science classes

394
00:21:01,240 --> 00:21:04,160
if you want to pursue a career in AI?

395
00:21:04,160 --> 00:21:08,600
Need is too strong, but I definitely encourage you to take technical classes.

396
00:21:08,600 --> 00:21:13,640
I think we're moving toward a world where, frankly, I, you know,

397
00:21:13,640 --> 00:21:16,560
at some future point, I think everyone should learn to code.

398
00:21:16,560 --> 00:21:19,920
Or rather, I think it'll be useful for everyone to learn how to code

399
00:21:19,920 --> 00:21:21,480
for a couple of reasons.

400
00:21:21,480 --> 00:21:23,240
Everyone has access to data, right?

401
00:21:23,240 --> 00:21:26,120
This is different than the world used to be even a few years ago.

402
00:21:26,120 --> 00:21:30,080
And especially with genus of AI, your ability to get something to work

403
00:21:30,080 --> 00:21:32,800
is much higher than ever before.

404
00:21:32,800 --> 00:21:35,960
The barrier to entry is much lower than ever before.

405
00:21:35,960 --> 00:21:39,000
And so if you learn just a little bit of coding,

406
00:21:39,000 --> 00:21:42,080
the amount that you really accomplish is significantly greater

407
00:21:42,080 --> 00:21:45,040
than if you don't know how to code at all.

408
00:21:45,040 --> 00:21:48,920
And are there any skills that separate out the great AI founders?

409
00:21:48,920 --> 00:21:53,480
I know AI right now is like, it's a sea that's rising all boats.

410
00:21:53,480 --> 00:21:55,960
But if you separate out the great ones from the good ones,

411
00:21:55,960 --> 00:22:00,240
are there any salient skills that you notice that the great AI CEOs

412
00:22:00,240 --> 00:22:02,680
or founders have that the good ones don't?

413
00:22:02,680 --> 00:22:07,200
Maybe since you said AI, I would say is often technical death.

414
00:22:07,200 --> 00:22:08,440
It costs a lot.

415
00:22:08,440 --> 00:22:10,360
But I want to give a different answer if you say great founders

416
00:22:10,360 --> 00:22:11,360
or not great AI founders.

417
00:22:11,360 --> 00:22:13,720
But I feel like AI is evolving rapidly.

418
00:22:13,720 --> 00:22:17,040
And we definitely have lost a bunch of new roles that, you know,

419
00:22:17,040 --> 00:22:19,600
pitch the VCs without really knowing what they're doing.

420
00:22:19,600 --> 00:22:22,240
And the smart VCs can sniff it out quite quickly.

421
00:22:22,240 --> 00:22:23,440
And it makes a huge difference.

422
00:22:23,440 --> 00:22:26,080
I think the technology, unfortunately, you know,

423
00:22:26,080 --> 00:22:29,400
is like somewhat complicated for a lot of applications.

424
00:22:29,400 --> 00:22:31,600
So a team that actually knows what they're doing

425
00:22:31,600 --> 00:22:34,720
will execute an AI project 10 times faster, you know,

426
00:22:34,720 --> 00:22:35,760
than a team that doesn't.

427
00:22:35,760 --> 00:22:37,400
And 10 times is not a made-up number.

428
00:22:37,400 --> 00:22:41,760
I literally see people take a year to do something like, oh, boy,

429
00:22:41,760 --> 00:22:44,640
I know that other team would have done perform this level

430
00:22:44,640 --> 00:22:47,320
in two weeks or maybe a month.

431
00:22:47,320 --> 00:22:49,840
So for many AI startups, application startups,

432
00:22:49,840 --> 00:22:52,320
in-front startups, you kind of have to know what you're doing.

433
00:22:52,320 --> 00:22:54,120
So it doesn't have to be you.

434
00:22:54,120 --> 00:22:57,400
If you're a technical co-founder, maybe that's OK.

435
00:22:57,400 --> 00:23:02,560
And then second thing I see among many of the great founders

436
00:23:02,560 --> 00:23:03,400
is speed.

437
00:23:03,400 --> 00:23:06,120
I find that as a startup, you'd be surprised

438
00:23:06,120 --> 00:23:08,680
when you handle the great founders the sheer speed

439
00:23:08,680 --> 00:23:11,760
of decision-making.

440
00:23:11,760 --> 00:23:14,760
And, you know, I sometimes talk to people from big companies

441
00:23:14,760 --> 00:23:16,760
and they'll say, oh, we move so fast.

442
00:23:16,760 --> 00:23:19,080
But when I kind of sit them side by side,

443
00:23:19,080 --> 00:23:20,640
how long does it take you to make this decision?

444
00:23:20,640 --> 00:23:21,720
Do I talk to a great founder?

445
00:23:21,760 --> 00:23:23,480
How long does it take me to make this decision?

446
00:23:23,480 --> 00:23:24,520
Maybe here's one story.

447
00:23:24,520 --> 00:23:27,480
I was chatting with the Meno CEO of Renato and iBook,

448
00:23:27,480 --> 00:23:28,760
former CEO Tinder.

449
00:23:28,760 --> 00:23:30,000
I was on the phone with her one day

450
00:23:30,000 --> 00:23:32,120
and she was making a major architecture decision.

451
00:23:32,120 --> 00:23:33,720
So it's architecting this thing.

452
00:23:33,720 --> 00:23:36,480
There were basically two major software architectures

453
00:23:36,480 --> 00:23:37,480
under consideration.

454
00:23:37,480 --> 00:23:40,320
And the team had laid it out, listed out some pros and cons.

455
00:23:40,320 --> 00:23:42,400
So they got on the team with me and some of my friends

456
00:23:42,400 --> 00:23:44,200
and said, these are pros and cons.

457
00:23:44,200 --> 00:23:48,880
And then one of my C2 AI fund and I said, you know,

458
00:23:48,880 --> 00:23:51,000
we're not sure, but she has some reasons

459
00:23:51,000 --> 00:23:53,160
that we prefer architecture A.

460
00:23:53,160 --> 00:23:55,320
And then Renato said, okay, guys done,

461
00:23:55,320 --> 00:23:58,680
decision made, go and implement architecture A.

462
00:23:58,680 --> 00:24:01,040
And after I thought, well, did Renato

463
00:24:01,040 --> 00:24:03,520
just make a massive engineering decision

464
00:24:03,520 --> 00:24:05,480
in basically 30 seconds?

465
00:24:05,480 --> 00:24:07,520
And she did.

466
00:24:07,520 --> 00:24:09,160
And I realized after it,

467
00:24:09,160 --> 00:24:11,080
I don't think there was a better way to make it

468
00:24:11,080 --> 00:24:12,440
because it's not as if, you know,

469
00:24:12,440 --> 00:24:14,560
if the company waited another week,

470
00:24:14,560 --> 00:24:16,000
it would have been a high quality decision

471
00:24:16,000 --> 00:24:17,120
and if it was wrong,

472
00:24:17,120 --> 00:24:19,160
I'm sure they would fix it, you know, the next week.

473
00:24:19,160 --> 00:24:22,040
But until you've lived through the speed

474
00:24:22,040 --> 00:24:24,720
of a great business, most people,

475
00:24:24,720 --> 00:24:27,320
I know so many people that think their organizations are fast

476
00:24:27,320 --> 00:24:29,080
when you stack them up to the real speed

477
00:24:29,080 --> 00:24:30,600
of a fast moving CEO,

478
00:24:30,600 --> 00:24:33,680
they have never actually seen speed in their life.

479
00:24:33,680 --> 00:24:35,760
One important caveat do be responsible.

480
00:24:35,760 --> 00:24:38,240
I know that move fast and break things sometimes,

481
00:24:38,240 --> 00:24:39,880
you know, it's the wrong approach.

482
00:24:39,880 --> 00:24:43,680
So tremendous speed when you are not being callous

483
00:24:43,680 --> 00:24:46,120
with people's lives and livelihood

484
00:24:46,120 --> 00:24:48,080
and things that could cause real harm.

485
00:24:48,080 --> 00:24:50,360
But so long as there's an important caveat

486
00:24:50,360 --> 00:24:52,400
of responsible AI,

487
00:24:52,400 --> 00:24:54,560
many of the great CEOs move faster

488
00:24:54,560 --> 00:24:57,560
than most people realize people can move.

489
00:24:57,560 --> 00:24:59,080
And so let's just double click on that

490
00:24:59,080 --> 00:25:00,760
on this theme of responsible AI

491
00:25:00,760 --> 00:25:02,360
just because I know this is a hot topic

492
00:25:02,360 --> 00:25:04,360
that maybe people aren't thinking about,

493
00:25:04,360 --> 00:25:08,400
which is you are clearly on the side of AI for good

494
00:25:08,400 --> 00:25:10,320
for responsible AI.

495
00:25:10,320 --> 00:25:13,040
Many of your brethren like Jeffrey Hinton

496
00:25:13,040 --> 00:25:15,640
and other famous leaders in the AI space

497
00:25:15,640 --> 00:25:18,480
have come out and are concerned

498
00:25:18,480 --> 00:25:19,880
that the pace of AI development

499
00:25:19,880 --> 00:25:21,920
will become an existential threat to humanity.

500
00:25:21,920 --> 00:25:24,880
So much so that famously there was a petition signed

501
00:25:24,880 --> 00:25:28,080
by Elon Musk and Steve Wozniak and many thought leaders

502
00:25:28,080 --> 00:25:30,480
asking for the halting of the foundational,

503
00:25:30,480 --> 00:25:32,800
the deepest foundational models of AI

504
00:25:32,800 --> 00:25:35,480
for us to sort of, for society to sort of catch up.

505
00:25:35,480 --> 00:25:38,480
You did not sign that pledge.

506
00:25:38,480 --> 00:25:40,440
Can you share a little bit more detail about that?

507
00:25:40,440 --> 00:25:42,560
Was that a difficult decision for you to make?

508
00:25:42,560 --> 00:25:43,960
And can you share more details

509
00:25:43,960 --> 00:25:45,760
about why you didn't join them

510
00:25:45,760 --> 00:25:48,240
and what your philosophical view is regarding

511
00:25:48,240 --> 00:25:50,720
if AI poses an existential threat?

512
00:25:50,720 --> 00:25:54,080
So I honestly don't see how AI poses

513
00:25:54,080 --> 00:25:57,400
any existential threat to the human race.

514
00:25:57,400 --> 00:25:59,400
We know AI can run about self-driving cars

515
00:25:59,400 --> 00:26:01,720
have crashed, leading to a tragic loss of life,

516
00:26:01,720 --> 00:26:04,080
ultimately through trainings, trash the stock market.

517
00:26:04,080 --> 00:26:06,040
So we know poorly designed software systems

518
00:26:06,040 --> 00:26:07,680
can have a dramatic impact

519
00:26:07,680 --> 00:26:09,400
and responsible AI is important.

520
00:26:09,400 --> 00:26:13,200
But recently I saw tells people like Jeff

521
00:26:13,200 --> 00:26:14,960
and others that were concerned

522
00:26:14,960 --> 00:26:16,760
about the question of AI extinction

523
00:26:16,760 --> 00:26:19,920
and I tried to understand why they thought this way.

524
00:26:19,920 --> 00:26:21,760
Some were worried about bad actor

525
00:26:21,760 --> 00:26:23,720
using AI to create a bio weapon.

526
00:26:23,720 --> 00:26:26,200
Others were worried about AI evolving in a way

527
00:26:26,200 --> 00:26:28,920
that inadvertently leads to human extinction

528
00:26:28,920 --> 00:26:33,280
similar to how we as humans have led to the extinction

529
00:26:33,280 --> 00:26:35,960
of many species through simple lack of awareness.

530
00:26:35,960 --> 00:26:39,040
Sometimes that our actions could lead to that outcome.

531
00:26:39,040 --> 00:26:41,320
But when I tried to assess how realistic

532
00:26:41,320 --> 00:26:42,400
these arguments were,

533
00:26:42,440 --> 00:26:45,120
I found them to be vague and non-specific

534
00:26:45,120 --> 00:26:47,440
about how AI could cause all.

535
00:26:47,440 --> 00:26:50,280
And I think that I found frustratingly frankly

536
00:26:50,280 --> 00:26:52,800
that trying to prove AI couldn't

537
00:26:52,800 --> 00:26:54,920
is akin to proving a negative.

538
00:26:54,920 --> 00:26:57,240
And I can't prove that super intelligent AI

539
00:26:57,240 --> 00:26:58,600
won't be dangerous,

540
00:26:58,600 --> 00:27:01,120
but I can't seem to find anyone

541
00:27:01,120 --> 00:27:03,520
that really knows exactly how it could be.

542
00:27:04,560 --> 00:27:09,280
And but I do know that humanity has ample experience

543
00:27:09,280 --> 00:27:11,160
controlling many things far more powerful

544
00:27:11,160 --> 00:27:14,760
than any one of us like corporations and nation states.

545
00:27:14,760 --> 00:27:17,600
And there are many things that we can't fully control

546
00:27:17,600 --> 00:27:19,320
that are nonetheless safe and valuable.

547
00:27:19,320 --> 00:27:21,640
Like airplanes, no one can control an airplane.

548
00:27:21,640 --> 00:27:23,320
It's buffeted around by winds

549
00:27:23,320 --> 00:27:25,560
and the pilot may make a mistake.

550
00:27:25,560 --> 00:27:27,800
But in the early days of aviation,

551
00:27:27,800 --> 00:27:29,520
airplanes killed many people.

552
00:27:29,520 --> 00:27:31,400
So we learned from those experiences,

553
00:27:31,400 --> 00:27:32,600
built safer aircraft,

554
00:27:32,600 --> 00:27:34,720
devised rules by which to operate them.

555
00:27:34,720 --> 00:27:36,640
And today most of us can step into airplane

556
00:27:36,640 --> 00:27:38,320
without fearing for our lives.

557
00:27:38,320 --> 00:27:40,920
And I think it would be like that too for AI.

558
00:27:40,920 --> 00:27:43,040
So I think the AI extinction,

559
00:27:43,040 --> 00:27:44,320
I find it to be very unfortunate.

560
00:27:44,320 --> 00:27:45,160
What I'm seeing,

561
00:27:45,160 --> 00:27:47,760
because doing some work in K-Tel of education as well,

562
00:27:47,760 --> 00:27:51,040
what I'm seeing is that kind of really unfortunately,

563
00:27:51,040 --> 00:27:54,960
I see high school students now considering working in AI.

564
00:27:54,960 --> 00:27:58,640
And some will say, AI seems exciting,

565
00:27:58,640 --> 00:28:01,040
but I heard it could lead to human extinction.

566
00:28:01,040 --> 00:28:03,520
And I just don't want to be a part of that.

567
00:28:03,520 --> 00:28:07,400
And so I find that the over height AI extinction narrative

568
00:28:07,400 --> 00:28:08,560
is doing real harm.

569
00:28:08,560 --> 00:28:10,720
So I'm really concerned about that.

570
00:28:11,680 --> 00:28:12,520
Thank you, Andrew.

571
00:28:12,520 --> 00:28:13,640
One more question that I'm gonna open it up,

572
00:28:13,640 --> 00:28:15,200
which is, I loved the detail

573
00:28:15,200 --> 00:28:16,560
on the low hanging fruit opportunities.

574
00:28:16,560 --> 00:28:17,920
I know that's on everybody's,

575
00:28:17,920 --> 00:28:20,240
all the entrepreneurs' minds of what to pursue.

576
00:28:20,240 --> 00:28:21,280
And so I appreciated the attention

577
00:28:21,280 --> 00:28:22,880
and the presentation on that.

578
00:28:22,880 --> 00:28:24,440
I wanted to ask about what's gonna be

579
00:28:24,440 --> 00:28:26,280
the next big technology shift in AI

580
00:28:26,280 --> 00:28:28,600
because things are changing so rapidly,

581
00:28:28,600 --> 00:28:30,520
especially as the models now are getting smaller

582
00:28:30,520 --> 00:28:32,040
and open sourced.

583
00:28:32,040 --> 00:28:34,200
It feels like we've already conquered language.

584
00:28:34,200 --> 00:28:37,280
Visual AI is getting very, very good.

585
00:28:37,280 --> 00:28:38,120
What's next?

586
00:28:38,120 --> 00:28:39,600
What are you seeing that's around the corner

587
00:28:39,600 --> 00:28:41,200
that others might not be aware of?

588
00:28:41,200 --> 00:28:42,240
Yeah, you know what, Danit?

589
00:28:42,240 --> 00:28:43,720
About several months ago,

590
00:28:43,720 --> 00:28:45,680
I was predicting visual AI's coming next,

591
00:28:45,680 --> 00:28:46,520
but now everyone's all right,

592
00:28:46,520 --> 00:28:48,840
visual AI's gotta come with something new.

593
00:28:48,840 --> 00:28:50,640
But in all seriousness, I think visual AI

594
00:28:50,640 --> 00:28:52,680
would be much more about the analysis of images

595
00:28:52,680 --> 00:28:54,480
rather than just generation of images.

596
00:28:54,480 --> 00:28:57,320
But I think we're at like the GPT-2 moment

597
00:28:57,320 --> 00:28:58,920
for visual AI is not yet working,

598
00:28:58,920 --> 00:29:00,240
but I think it'll work much better.

599
00:29:00,240 --> 00:29:02,640
And this will impact self-driving cars, for example,

600
00:29:02,640 --> 00:29:05,440
when we can finally solve problems in a long tail.

601
00:29:06,400 --> 00:29:09,120
And then I think, actually, one other thing that

602
00:29:09,120 --> 00:29:12,200
I wrote about just today in a newsletter called The Batch

603
00:29:12,200 --> 00:29:16,040
is I think one thing that many people find controversial

604
00:29:16,040 --> 00:29:18,680
but I think is coming is the rise of edge AI.

605
00:29:18,680 --> 00:29:20,040
And I know this controversial.

606
00:29:20,040 --> 00:29:22,640
Many of us would train to write SaaS software,

607
00:29:22,640 --> 00:29:25,720
lend to self-nice, the subscription business model.

608
00:29:25,720 --> 00:29:27,000
How do you even find people?

609
00:29:27,000 --> 00:29:29,200
How do you hire engineers to write desktop applications?

610
00:29:29,200 --> 00:29:30,920
Like who even does that anymore?

611
00:29:31,120 --> 00:29:36,120
But I think that because of various forces,

612
00:29:36,280 --> 00:29:39,280
including privacy, I think that in the next few years

613
00:29:39,280 --> 00:29:43,240
we'll see more AI applications running at the edge,

614
00:29:43,240 --> 00:29:46,000
meaning on your laptop or on your cell phone.

615
00:29:46,000 --> 00:29:48,400
So I think that'll be coming.

616
00:29:48,400 --> 00:29:50,000
And then I think there'll just be a lot of work

617
00:29:50,000 --> 00:29:52,280
coming in the application there as well.

618
00:29:52,280 --> 00:29:54,080
Okay, I wanna open it up to the students.

619
00:29:54,080 --> 00:29:56,560
You're the reason why Andrew's here.

620
00:29:56,560 --> 00:29:59,360
You mentioned that on your slides,

621
00:29:59,360 --> 00:30:01,360
you put the potential from reinforcement learning.

622
00:30:01,360 --> 00:30:03,280
Are the general value as a dot

623
00:30:03,280 --> 00:30:06,720
relative to the potential for unsupervised learning?

624
00:30:06,720 --> 00:30:08,800
Do you think there is still potential

625
00:30:08,800 --> 00:30:11,600
for generalist agents like GATO

626
00:30:11,600 --> 00:30:15,000
and other reinforcement learning models in society

627
00:30:15,000 --> 00:30:17,120
and in your AI stack for startups?

628
00:30:18,160 --> 00:30:22,200
So technically, last time we trained models

629
00:30:22,200 --> 00:30:23,800
that trained using reinforcement learning

630
00:30:23,800 --> 00:30:25,760
and unsupervised learning and supervised learning

631
00:30:25,760 --> 00:30:27,080
but leaving that aside,

632
00:30:27,080 --> 00:30:29,160
I feel like I'm not convinced that reinforcement learning

633
00:30:29,160 --> 00:30:30,480
is near breakthrough moments,

634
00:30:30,480 --> 00:30:33,040
at least in the next small number of years.

635
00:30:33,040 --> 00:30:34,760
A lot of excitement about what we could do

636
00:30:34,760 --> 00:30:36,600
in reinforcement learning applied to robotics.

637
00:30:36,600 --> 00:30:39,440
A lot about CS faculty, right, Chelsea Finn,

638
00:30:39,440 --> 00:30:42,400
Emma Brunskill, many others are doing exciting research there

639
00:30:42,400 --> 00:30:44,680
but we do have a data problem.

640
00:30:44,680 --> 00:30:46,600
So it turns out that text on the internet

641
00:30:46,600 --> 00:30:48,760
sounds a lot like text on your documents.

642
00:30:48,760 --> 00:30:51,200
So we can learn from lots of text on the internet

643
00:30:51,200 --> 00:30:53,480
to do really well on your text documents.

644
00:30:53,480 --> 00:30:54,720
And image on the internet

645
00:30:54,720 --> 00:30:56,640
look a little bit like images that you care about.

646
00:30:56,640 --> 00:30:58,240
So we have a lot of data

647
00:30:58,240 --> 00:31:00,520
but because every robot is different,

648
00:31:01,480 --> 00:31:04,240
struggling, many people are struggling to see

649
00:31:04,240 --> 00:31:06,760
how to get enough data to have the usual recipe

650
00:31:06,760 --> 00:31:08,760
of scaling up data and compute

651
00:31:08,760 --> 00:31:10,280
where for reinforcement learning

652
00:31:10,280 --> 00:31:12,600
and people are working on it over the weekend

653
00:31:12,600 --> 00:31:16,240
at the CS faculty retreat, you know, there was a talk,

654
00:31:16,240 --> 00:31:18,240
I think, well, who gave this talk?

655
00:31:18,240 --> 00:31:19,520
Shoot, thank you.

656
00:31:19,520 --> 00:31:23,120
On how to do this, early ideas on how to do this

657
00:31:23,120 --> 00:31:25,200
but I think we're still a few years away

658
00:31:25,200 --> 00:31:27,960
from breakthroughs and those breakthroughs

659
00:31:27,960 --> 00:31:29,440
and reinforcement learning.

660
00:31:29,440 --> 00:31:31,320
But it's a great research topic, by the way,

661
00:31:31,320 --> 00:31:34,440
just because, you know, just because it's not working

662
00:31:34,440 --> 00:31:37,040
right now doesn't mean you shouldn't do research on it.

663
00:31:37,040 --> 00:31:39,000
So I think it's a great research topic.

664
00:31:39,000 --> 00:31:42,080
So I just want to know your thoughts about

665
00:31:42,080 --> 00:31:45,440
what are the security concerns which is coming up

666
00:31:45,440 --> 00:31:47,880
by you abusing that like LLM models,

667
00:31:47,880 --> 00:31:51,000
like all these new attacks like prompt injections,

668
00:31:51,000 --> 00:31:53,160
data leakage, jailbreaking.

669
00:31:53,160 --> 00:31:54,680
So what's your thought around that?

670
00:31:54,680 --> 00:31:58,720
Like how can we like safeguard against those kind of attack

671
00:31:58,720 --> 00:32:01,200
because it's just starting up this new technology.

672
00:32:01,200 --> 00:32:05,000
So I'm assuming there's more things which will come up.

673
00:32:05,000 --> 00:32:07,520
Yes, so I think that for the near future

674
00:32:07,520 --> 00:32:10,800
there'll be a little bit of a cat and mouse thing going on.

675
00:32:12,240 --> 00:32:15,600
So I think I'm seeing different companies

676
00:32:17,240 --> 00:32:18,880
approach this with different tools

677
00:32:18,880 --> 00:32:22,040
to wash out for prompt injections, for data leakage.

678
00:32:22,040 --> 00:32:24,240
Actually, DeepLand.ai is actually a work of a partner

679
00:32:24,240 --> 00:32:28,680
on some things that hopefully will announce very soon

680
00:32:28,680 --> 00:32:31,000
on our portfolio of tools.

681
00:32:31,000 --> 00:32:34,080
By the way, those of you that have not yet done it,

682
00:32:34,080 --> 00:32:37,000
go and fool around with prompt injections,

683
00:32:37,000 --> 00:32:40,000
see if you can get an LLM to do something.

684
00:32:40,000 --> 00:32:42,160
Well, don't do something actually harmful,

685
00:32:42,160 --> 00:32:45,800
but I actually find it kind of intellectually interesting

686
00:32:45,800 --> 00:32:48,440
whenever I use an LLM to prompt it

687
00:32:48,440 --> 00:32:51,400
to see how robust the safeguards actually are.

688
00:32:51,840 --> 00:32:54,360
And if I should look at the older language models a year ago,

689
00:32:54,360 --> 00:32:57,280
it was super easy to get the older models,

690
00:32:58,240 --> 00:33:00,440
frankly, to give you detailed directions

691
00:33:00,440 --> 00:33:02,200
to do things that they should not give anyone

692
00:33:02,200 --> 00:33:03,400
detailed directions to do.

693
00:33:03,400 --> 00:33:06,120
But the more modern language models are much older,

694
00:33:06,120 --> 00:33:08,680
but it's still sometimes possible.

695
00:33:08,680 --> 00:33:10,560
Sorry, but what I'm seeing as well,

696
00:33:10,560 --> 00:33:11,520
for a lot of corporations,

697
00:33:11,520 --> 00:33:13,640
a lot of corporations because of these worries

698
00:33:13,640 --> 00:33:16,440
will ship internal-facing product first

699
00:33:16,440 --> 00:33:19,320
because presumably, if it says the wrong thing

700
00:33:19,320 --> 00:33:20,680
to your own employee,

701
00:33:20,680 --> 00:33:23,120
more understanding, less likelihood scandal,

702
00:33:23,120 --> 00:33:25,800
and test products internally for quite a long time

703
00:33:25,800 --> 00:33:28,720
or even build capabilities for safe internal use

704
00:33:28,720 --> 00:33:31,640
before turning out to external use.

705
00:33:31,640 --> 00:33:33,960
But I do see different companies,

706
00:33:33,960 --> 00:33:36,680
yeah, different tools for trying to adjust these.

707
00:33:37,640 --> 00:33:38,800
Terrific, next question.

708
00:33:42,200 --> 00:33:43,640
Thank you so much.

709
00:33:43,640 --> 00:33:45,040
Hi there, my name is Chinat

710
00:33:45,040 --> 00:33:47,600
and I'm an international student from Hong Kong.

711
00:33:47,600 --> 00:33:49,160
I'm curious to ask, because, you know,

712
00:33:49,160 --> 00:33:50,680
I'm hoping that after I graduate,

713
00:33:50,680 --> 00:33:53,360
I can hopefully go back home to work closer with family.

714
00:33:53,360 --> 00:33:55,280
But at the same time, I feel like by going back,

715
00:33:55,280 --> 00:33:57,520
I'm closing a lot of doors behind me

716
00:33:57,520 --> 00:33:59,600
because, for example, in Hong Kong, for example,

717
00:33:59,600 --> 00:34:02,360
you can't access ChatGVT without a U.S. number,

718
00:34:02,360 --> 00:34:04,240
which makes access to some of these resources

719
00:34:04,240 --> 00:34:05,280
really difficult.

720
00:34:05,280 --> 00:34:07,200
So I'm curious to see what are your thoughts

721
00:34:07,200 --> 00:34:09,720
about navigating this complex modern landscape?

722
00:34:10,760 --> 00:34:13,800
Yeah, I don't want to comment on, I don't know,

723
00:34:14,760 --> 00:34:15,600
complicated.

724
00:34:15,600 --> 00:34:17,000
That's actually one thing I'm seeing.

725
00:34:17,000 --> 00:34:20,640
I've been to quite a few places, you know, in Asia recently.

726
00:34:21,520 --> 00:34:25,280
And what I'm seeing is that many countries

727
00:34:25,280 --> 00:34:28,960
are developing surprisingly good capabilities

728
00:34:28,960 --> 00:34:32,320
for building large language model applications.

729
00:34:32,320 --> 00:34:35,680
The concentration of talent for Genesebo AI deep tech

730
00:34:35,680 --> 00:34:39,800
is very concentrated in the San Francisco Bay Area.

731
00:34:39,800 --> 00:34:41,840
I think because there are basically two teams

732
00:34:41,840 --> 00:34:43,680
that did a lot of the early groundbreaking work,

733
00:34:43,680 --> 00:34:46,040
you know, Google Brain, my former team, and OpenAI.

734
00:34:46,040 --> 00:34:48,600
And subsequently, people left and started a lot of companies

735
00:34:48,600 --> 00:34:50,240
here in the California Bay Area.

736
00:34:50,240 --> 00:34:53,920
So I think that concentrated talent is very high.

737
00:34:53,920 --> 00:34:56,320
And it's interesting, even when I'm in Seattle,

738
00:34:56,320 --> 00:34:58,880
great city, love the city, on weekends,

739
00:35:00,080 --> 00:35:01,280
you know, I hang out with friends,

740
00:35:01,280 --> 00:35:03,800
but the conversations are not about Genesebo AI.

741
00:35:03,800 --> 00:35:06,960
Whereas here, if you go to a coffee shop,

742
00:35:06,960 --> 00:35:09,760
actually, one of my friends was visiting from Taiwan.

743
00:35:09,760 --> 00:35:11,320
So he was hanging out with us for a week.

744
00:35:11,320 --> 00:35:13,080
They went back and he said,

745
00:35:13,160 --> 00:35:15,440
yeah, I went to a coffee shop and, you know,

746
00:35:15,440 --> 00:35:16,960
there was no one talking about AI.

747
00:35:16,960 --> 00:35:17,920
That's so weird.

748
00:35:19,600 --> 00:35:21,080
So at least at this moment in time,

749
00:35:21,080 --> 00:35:23,640
there's really heavy concentration.

750
00:35:23,640 --> 00:35:26,400
But I see less the deep tech layer,

751
00:35:26,400 --> 00:35:27,760
but the application layer,

752
00:35:27,760 --> 00:35:31,320
I see that skillset developing quite quickly globally as well.

753
00:35:32,600 --> 00:35:34,440
Oh, and I think the option is to allow places

754
00:35:34,440 --> 00:35:35,600
to be local opportunities.

755
00:35:35,600 --> 00:35:37,400
So the shipping company that we built,

756
00:35:37,400 --> 00:35:39,760
we built with a Japanese company

757
00:35:39,760 --> 00:35:41,960
that happens to operate global lines of shipping.

758
00:35:41,960 --> 00:35:44,680
So I think a lot of the businesses will be, you know,

759
00:35:44,680 --> 00:35:48,240
playing locally whether country or that geography is strong.

760
00:35:48,240 --> 00:35:51,440
Those businesses will be more efficient to build in places

761
00:35:51,440 --> 00:35:53,000
other than Silicon Valley.

762
00:35:53,000 --> 00:35:55,960
Because where do I go to find a large seaport here

763
00:35:55,960 --> 00:35:57,920
to do that type of work?

764
00:35:58,960 --> 00:36:00,560
Hi, Andrew, thank you so much for your time.

765
00:36:00,560 --> 00:36:01,880
My name is Komal.

766
00:36:01,880 --> 00:36:05,280
I wanted to ask you if you think we'll ever reach a threshold

767
00:36:05,280 --> 00:36:07,080
on human dependence for AI,

768
00:36:07,080 --> 00:36:10,160
or if you think it'll just continue to grow exponentially?

769
00:36:10,960 --> 00:36:15,280
So I think we already really, really depend on tech, right?

770
00:36:15,280 --> 00:36:19,120
Imagine if, you know, if the internet were to shut down,

771
00:36:19,120 --> 00:36:20,280
I think people would die.

772
00:36:20,280 --> 00:36:21,400
I don't think that's the exact truth.

773
00:36:21,400 --> 00:36:23,960
I mean, but seriously, think about, you know,

774
00:36:23,960 --> 00:36:26,080
how we get through supply chain, healthcare.

775
00:36:26,080 --> 00:36:27,760
If the internet were to shut down,

776
00:36:27,760 --> 00:36:30,840
I think that will lead directly to, you know,

777
00:36:31,760 --> 00:36:33,960
what happens our water system, right?

778
00:36:33,960 --> 00:36:35,560
Healthcare system.

779
00:36:35,560 --> 00:36:39,560
So, and I think that technology is very useful

780
00:36:39,560 --> 00:36:44,080
and so long as the supplies remain reliable,

781
00:36:46,040 --> 00:36:49,880
I feel like it's okay to depend on technology.

782
00:36:49,880 --> 00:36:51,720
I mean, heck, I wish, I don't know,

783
00:36:51,720 --> 00:36:54,720
without dependence on agriculture system,

784
00:36:54,720 --> 00:36:56,240
how many of us would build a farm

785
00:36:56,240 --> 00:36:58,440
and hunt enough food to keep ourselves alive?

786
00:36:58,440 --> 00:37:00,800
Maybe, maybe we could do it, but it's pretty challenging.

787
00:37:00,800 --> 00:37:02,400
So I think dependence on tech

788
00:37:02,400 --> 00:37:04,560
seems to keep on growing for a while.

789
00:37:04,560 --> 00:37:05,480
But do you think there'll be a moment

790
00:37:05,480 --> 00:37:07,280
where there's a difference in that relationship,

791
00:37:07,280 --> 00:37:09,160
not just in degree, but in kind?

792
00:37:09,160 --> 00:37:11,120
You know, the famous singularity point

793
00:37:11,120 --> 00:37:12,880
where we don't even know what we don't even know

794
00:37:12,880 --> 00:37:14,880
about how technology is developing.

795
00:37:14,880 --> 00:37:16,600
Do you think that will occur?

796
00:37:16,600 --> 00:37:18,920
Yeah, you know, the technological singularity

797
00:37:18,920 --> 00:37:20,840
is one of those hypey things

798
00:37:20,840 --> 00:37:22,240
that I don't even know what it means.

799
00:37:22,240 --> 00:37:25,640
So it's one of those, it's exciting science fiction,

800
00:37:25,640 --> 00:37:26,960
but as an engineer and scientist,

801
00:37:26,960 --> 00:37:29,840
I don't know how to talk about it.

802
00:37:29,840 --> 00:37:31,600
It turns out there are a few terms in AI

803
00:37:31,600 --> 00:37:33,040
that are vague and undefined,

804
00:37:33,040 --> 00:37:34,320
but there are a lot of emotions,

805
00:37:34,320 --> 00:37:36,880
a lot of excitement about it.

806
00:37:36,880 --> 00:37:38,520
And I don't really know how to think

807
00:37:38,560 --> 00:37:41,120
about those things in a systematic, rational way.

808
00:37:41,120 --> 00:37:43,800
But I think, actually, there's actually one thing.

809
00:37:43,800 --> 00:37:46,120
I think that our relationship to technology

810
00:37:46,120 --> 00:37:48,160
is changing rapidly.

811
00:37:49,040 --> 00:37:52,400
Today, you know, I probably use

812
00:37:53,400 --> 00:37:58,320
chat, you know, GPT-4 or Bing or BOT

813
00:37:58,320 --> 00:37:59,840
pretty much every day now.

814
00:37:59,840 --> 00:38:02,840
And so the workflow of many people have changed.

815
00:38:03,840 --> 00:38:05,040
I think people are changing.

816
00:38:05,040 --> 00:38:05,880
And do you have a view?

817
00:38:05,880 --> 00:38:07,240
I know this also might be more of one

818
00:38:07,240 --> 00:38:10,240
of these sort of hot topics that's not substantive,

819
00:38:10,240 --> 00:38:12,440
but on the consciousness of AI,

820
00:38:12,440 --> 00:38:14,680
that AI will it become conscious?

821
00:38:14,680 --> 00:38:16,600
Yeah, so the thing about consciousness

822
00:38:16,600 --> 00:38:18,880
is it's important for the South Pole question,

823
00:38:18,880 --> 00:38:20,200
but I don't know of any test

824
00:38:20,200 --> 00:38:22,160
for whether something is conscious or not.

825
00:38:22,160 --> 00:38:23,840
So I think it's important philosophy

826
00:38:23,840 --> 00:38:25,600
and philosophy is important,

827
00:38:25,600 --> 00:38:27,680
but as an engineer and scientist,

828
00:38:27,680 --> 00:38:29,480
I don't know, there is no definition

829
00:38:29,480 --> 00:38:31,520
for what is conscious or not,

830
00:38:31,520 --> 00:38:35,160
which in does we can kind of debate it at length.

831
00:38:35,160 --> 00:38:39,480
And there's actually one other formula for hype,

832
00:38:39,480 --> 00:38:41,120
which is if someone comes up

833
00:38:41,120 --> 00:38:43,680
with a very simple definition for consciousness,

834
00:38:43,680 --> 00:38:44,520
so someone says,

835
00:38:44,520 --> 00:38:46,280
oh, if you can recognize yourself in the mirror,

836
00:38:46,280 --> 00:38:47,480
you're conscious, I made that up.

837
00:38:47,480 --> 00:38:48,920
It's not a good definition for consciousness.

838
00:38:48,920 --> 00:38:51,040
But you're aware of yourself, CSR in the mirror,

839
00:38:51,040 --> 00:38:52,840
then it's actually pretty easy to get a robot

840
00:38:52,840 --> 00:38:54,480
to recognize yourself in the mirror.

841
00:38:54,480 --> 00:38:56,400
And then you can generate newspaper headlines

842
00:38:56,400 --> 00:38:58,360
saying AI has achieved consciousness.

843
00:38:58,360 --> 00:38:59,960
What it did for your kind of, you know,

844
00:38:59,960 --> 00:39:02,160
silly little, for your very small definition

845
00:39:02,160 --> 00:39:04,520
of consciousness, but that gets misinterpreted

846
00:39:04,560 --> 00:39:07,160
by the broader public for a grander statement than it is.

847
00:39:07,160 --> 00:39:10,840
So I see some of that hype in AI as well.

848
00:39:10,840 --> 00:39:12,160
Thank you.

849
00:39:12,160 --> 00:39:13,080
Next question.

850
00:39:14,440 --> 00:39:17,960
Hi, earlier you outlined the AI stack.

851
00:39:17,960 --> 00:39:19,680
And recently we've seen a lot of cool things

852
00:39:19,680 --> 00:39:23,160
coming out of like NVIDIA, Intel and other like chip companies.

853
00:39:23,160 --> 00:39:24,640
I'm curious on what your thoughts are

854
00:39:24,640 --> 00:39:27,200
on what companies like AWS and Google,

855
00:39:27,200 --> 00:39:29,200
like in the infrastructure layer need to do

856
00:39:29,200 --> 00:39:32,160
in order to make like AI and enterprises and business

857
00:39:32,160 --> 00:39:34,120
really effective and possible.

858
00:39:34,120 --> 00:39:37,240
Sure, boy, so there's a lot going on in that space.

859
00:39:37,240 --> 00:39:40,480
By the way, you mentioned Intel and NVIDIA.

860
00:39:40,480 --> 00:39:41,920
I wouldn't, I think I'm actually seeing

861
00:39:41,920 --> 00:39:44,040
really exciting work from AMD as well.

862
00:39:44,040 --> 00:39:47,400
I've been pretty impressed by the MI 200, MI 250.

863
00:39:47,400 --> 00:39:50,760
I'm excited about the MI 300 GPUs coming out as well.

864
00:39:50,760 --> 00:39:53,640
And I think the ROCCOM stack is becoming, you know,

865
00:39:53,640 --> 00:39:56,880
not parity or CUDA, but better than most people

866
00:39:56,880 --> 00:39:57,960
give them credit for.

867
00:39:59,240 --> 00:40:01,000
But in terms of Adode and Google,

868
00:40:01,000 --> 00:40:02,440
so it turns out that if you were to use

869
00:40:02,440 --> 00:40:05,080
a lot of the LLM startup tools,

870
00:40:05,080 --> 00:40:06,840
the switching cost is actually pretty low.

871
00:40:06,840 --> 00:40:10,160
So if you were to start with one LLM API call,

872
00:40:10,160 --> 00:40:12,520
if you want to switch to a different LLM provider,

873
00:40:12,520 --> 00:40:15,200
the number of lines of code is actually pretty low.

874
00:40:15,200 --> 00:40:16,840
So there are low switching costs.

875
00:40:16,840 --> 00:40:19,000
But it turns out that a zero in Google Cloud

876
00:40:19,000 --> 00:40:21,400
and AWS are fantastic businesses

877
00:40:21,400 --> 00:40:23,880
because once you build on any of these clouds,

878
00:40:23,880 --> 00:40:25,880
you know, the switching costs tend to be very high

879
00:40:25,880 --> 00:40:28,640
because you have so many API hooks integrations.

880
00:40:28,640 --> 00:40:31,480
So that's why I think that a lot of the startups

881
00:40:31,520 --> 00:40:34,040
selling API calls still have, you know,

882
00:40:34,040 --> 00:40:37,160
some work to do to find a business model

883
00:40:38,040 --> 00:40:40,200
that may be somewhat more defensible.

884
00:40:40,200 --> 00:40:44,240
I think that OpenEye's chat GPD Enterprise,

885
00:40:44,240 --> 00:40:46,680
that feels like a more defensible business

886
00:40:46,680 --> 00:40:48,560
than just selling API calls.

887
00:40:48,560 --> 00:40:50,320
By the way, Sam was actually a Stanford undergrad.

888
00:40:50,320 --> 00:40:52,800
He actually interned, he's curious in my lab.

889
00:40:52,800 --> 00:40:55,480
So a lot of Stanford roots, but he's a smart guy.

890
00:40:55,480 --> 00:40:56,600
I'm sure he'll figure out,

891
00:40:56,600 --> 00:40:59,560
confidently he'll figure out some good directions.

892
00:40:59,560 --> 00:41:02,280
Yeah, and I think AWS and GCP and the Zora Rall

893
00:41:04,400 --> 00:41:09,080
racing to continue to develop LLM capabilities

894
00:41:09,080 --> 00:41:11,480
and make it easier to use and bring in more customers.

895
00:41:11,480 --> 00:41:14,200
And yeah, it's a very dynamic space.

896
00:41:14,200 --> 00:41:17,160
But as AI gets democratized,

897
00:41:17,160 --> 00:41:19,800
it feels like things are shifting more towards compute

898
00:41:19,800 --> 00:41:22,920
and data as predictors of success.

899
00:41:22,920 --> 00:41:23,760
If that's the case,

900
00:41:23,760 --> 00:41:25,320
do you think the locus of innovation shifts

901
00:41:25,320 --> 00:41:27,240
from academia to industry

902
00:41:27,280 --> 00:41:29,640
where the companies are gonna really be dominating

903
00:41:29,640 --> 00:41:30,960
at the forefront of AI?

904
00:41:30,960 --> 00:41:34,160
Yeah, so what I'm seeing now is that there's a subset,

905
00:41:34,160 --> 00:41:35,920
but there's a small subset of things

906
00:41:35,920 --> 00:41:38,120
that are easier to do in the big tech company,

907
00:41:38,120 --> 00:41:41,080
which are the ones that require massive compute resources.

908
00:41:41,080 --> 00:41:44,040
And I do think people's perceptions are distorted

909
00:41:44,040 --> 00:41:46,200
because frankly, I've been on the big tech companies before,

910
00:41:46,200 --> 00:41:47,040
right?

911
00:41:47,040 --> 00:41:48,480
So I understand your marketing and big tech companies,

912
00:41:48,480 --> 00:41:51,440
but the standard big tech company marketing is,

913
00:41:51,440 --> 00:41:54,680
look, you need the data, you need to compute,

914
00:41:54,680 --> 00:41:56,120
only we have it.

915
00:41:56,120 --> 00:41:58,840
Why don't you just give up and don't compete with us, right?

916
00:41:58,840 --> 00:42:01,320
And come apply for a job and come work with us.

917
00:42:01,320 --> 00:42:04,480
That is, this has been the explicit PR strategy

918
00:42:04,480 --> 00:42:06,160
of at least one big tech company

919
00:42:06,160 --> 00:42:09,360
because I know what was discussed internally

920
00:42:09,360 --> 00:42:11,280
exactly at that big tech company.

921
00:42:11,280 --> 00:42:14,600
So I would say don't buy into that marketing message.

922
00:42:14,600 --> 00:42:16,960
It is true that there is a subset of work

923
00:42:16,960 --> 00:42:19,000
that requires massive capital,

924
00:42:20,160 --> 00:42:21,680
training in very large foundation models.

925
00:42:21,680 --> 00:42:23,920
That is much easier to do in a big tech company

926
00:42:23,920 --> 00:42:25,720
than in academia like Stanford.

927
00:42:25,720 --> 00:42:29,080
But that's a small subset of all the happenings in AI.

928
00:42:29,080 --> 00:42:31,360
And there's plenty of work at Stanford,

929
00:42:31,360 --> 00:42:32,640
at the application layer.

930
00:42:32,640 --> 00:42:34,640
It turns out because of scaling laws,

931
00:42:34,640 --> 00:42:36,040
we're actually pretty good at predicting

932
00:42:36,040 --> 00:42:37,560
what will happen for very large models

933
00:42:37,560 --> 00:42:39,520
by training on more model size models.

934
00:42:39,520 --> 00:42:41,560
So very good scientific work can be done

935
00:42:41,560 --> 00:42:42,840
and much smaller models.

936
00:42:42,840 --> 00:42:47,320
And then also, I routinely run kind of models

937
00:42:47,320 --> 00:42:49,000
on my laptop for inference.

938
00:42:49,000 --> 00:42:51,360
Like, I don't know, when I'm on an airplane,

939
00:42:51,360 --> 00:42:53,280
you can run like the seven billion llama model

940
00:42:53,280 --> 00:42:54,120
on your laptop, right?

941
00:42:54,120 --> 00:42:55,400
And so there's actually a lot of stuff

942
00:42:55,400 --> 00:42:59,520
that you could run on your own personal computer.

943
00:42:59,520 --> 00:43:00,440
Thank you.

944
00:43:00,440 --> 00:43:01,280
Next question.

945
00:43:02,800 --> 00:43:03,840
Thank you, Andrew.

946
00:43:03,840 --> 00:43:08,680
So from an AI expert and also the investor's perspective,

947
00:43:08,680 --> 00:43:11,880
so what AI driven healthcare applications

948
00:43:11,880 --> 00:43:13,760
do you see have the great potentials

949
00:43:13,760 --> 00:43:15,800
to have the breakthrough in the future?

950
00:43:15,800 --> 00:43:19,680
And what challenges and obstacles should we be aware of?

951
00:43:19,680 --> 00:43:21,200
Thank you.

952
00:43:21,200 --> 00:43:23,960
Yeah, so boys, there's a lot of complexity to that question.

953
00:43:23,960 --> 00:43:26,120
So I feel like a lot of healthcare,

954
00:43:26,120 --> 00:43:29,600
people tend to focus on the diagnostics and the treatment.

955
00:43:29,600 --> 00:43:31,440
So I think lots of opportunities there.

956
00:43:31,440 --> 00:43:35,160
I think that the revenue model is to be sorted out.

957
00:43:35,160 --> 00:43:36,520
So we've seen, you know,

958
00:43:36,520 --> 00:43:40,360
Pear and Ikeli struggle in the public markets,

959
00:43:40,360 --> 00:43:43,880
kind of bankruptcy kind of levels almost.

960
00:43:44,760 --> 00:43:46,640
So I think prescriptive digital therapeutics

961
00:43:46,640 --> 00:43:48,720
is definitely going through challenges.

962
00:43:48,720 --> 00:43:51,200
But what's the recipe for shipping AI products

963
00:43:51,200 --> 00:43:53,960
and in the payer-provide ecosystem,

964
00:43:53,960 --> 00:43:55,560
what will pay us be willing to pay for?

965
00:43:55,560 --> 00:43:58,120
I think that many businesses are sorting that out.

966
00:43:58,120 --> 00:43:59,200
I think that will work.

967
00:43:59,200 --> 00:44:01,400
And there's actually one other huge set of opportunities

968
00:44:01,400 --> 00:44:03,680
in healthcare that tend to be underappreciated,

969
00:44:03,680 --> 00:44:04,960
which is operations.

970
00:44:04,960 --> 00:44:06,480
Instead of the medical stuff,

971
00:44:06,480 --> 00:44:08,400
things like scheduling, you know,

972
00:44:08,400 --> 00:44:10,680
who should scheduling the MRI machine

973
00:44:10,680 --> 00:44:14,040
or doing kind of patient management systems.

974
00:44:14,040 --> 00:44:16,160
I think that type of healthcare operations

975
00:44:16,160 --> 00:44:17,720
have fewer regulatory hurdles.

976
00:44:17,720 --> 00:44:19,800
I think is also a rich set of opportunities.

977
00:44:19,800 --> 00:44:22,040
And then lastly, there's the go-to-market question of,

978
00:44:22,040 --> 00:44:23,800
do you want to go to the market in the US

979
00:44:23,800 --> 00:44:27,240
or in other countries where the regulatory hurdle

980
00:44:27,240 --> 00:44:29,000
could be very different depending on,

981
00:44:29,000 --> 00:44:31,840
the US fortunately doesn't have as great a shortage

982
00:44:31,840 --> 00:44:33,640
of doctors as some other places.

983
00:44:33,640 --> 00:44:35,600
And there are therefore other places

984
00:44:35,600 --> 00:44:39,280
that are more amenable to your responsible

985
00:44:39,280 --> 00:44:42,480
but still easier adoption of AI than the United States.

986
00:44:42,480 --> 00:44:44,840
Okay, I have a super quick question.

987
00:44:44,840 --> 00:44:47,240
You mentioned that your team at the AI fund

988
00:44:47,240 --> 00:44:49,880
has so many ideas for AI applications

989
00:44:49,880 --> 00:44:51,600
that you have a whole son of them.

990
00:44:51,600 --> 00:44:54,680
What exactly is your process for generating these ideas?

991
00:44:54,680 --> 00:44:55,760
Oh, there's also that.

992
00:44:55,760 --> 00:44:57,000
And you have 30 seconds.

993
00:44:57,000 --> 00:44:59,080
We like working with subject matter experts

994
00:44:59,080 --> 00:45:01,120
that deeply understand the domain.

995
00:45:01,120 --> 00:45:03,320
It turns out that there are a lot of people in the world,

996
00:45:03,320 --> 00:45:06,120
you know, including like CEOs of Fortune 500 companies,

997
00:45:06,120 --> 00:45:08,560
but really a lot of people that really understand the domain

998
00:45:08,560 --> 00:45:09,880
have thought deeply about something

999
00:45:09,880 --> 00:45:11,800
for months or even a couple of years.

1000
00:45:11,800 --> 00:45:13,240
And when we get together with them,

1001
00:45:13,240 --> 00:45:15,600
they're sometimes very happy to share their idea with us

1002
00:45:15,600 --> 00:45:17,200
because they've been looking for someone

1003
00:45:17,200 --> 00:45:19,960
to validate or falsify it and also to help them build it.

1004
00:45:19,960 --> 00:45:21,520
So we actually got a lot of ideas,

1005
00:45:21,520 --> 00:45:23,720
some in turn out with a lot from subject matter experts

1006
00:45:23,720 --> 00:45:26,360
that just not yet had an AI built upon there.

1007
00:45:26,360 --> 00:45:27,600
Terrific, that's fantastic.

1008
00:45:27,600 --> 00:45:29,800
Thank you Andrew so much for sharing your insights.

1009
00:45:29,800 --> 00:45:30,640
Thank you.

1010
00:45:30,640 --> 00:45:34,920
Lots of love.

1011
00:45:34,920 --> 00:45:35,840
Thank you for sharing your insights

1012
00:45:35,840 --> 00:45:38,200
with Stanford's ETL course MSME 472

1013
00:45:38,200 --> 00:45:40,680
and the students all around the world.

1014
00:45:40,680 --> 00:45:42,200
Everybody next week we're gonna be joined

1015
00:45:42,200 --> 00:45:44,440
by Stanford professor Kathleen Eisenhardt

1016
00:45:44,440 --> 00:45:47,520
here at ETL physically in person.

1017
00:45:47,520 --> 00:45:50,400
Professor Eisenhardt is also the author of Simple Rules.

1018
00:45:50,400 --> 00:45:52,520
You can find that event and other future events

1019
00:45:52,520 --> 00:45:56,040
in this ETL series on the Stanford eCorner YouTube channel

1020
00:45:56,040 --> 00:45:58,040
and you'll find even more of our videos, podcasts

1021
00:45:58,040 --> 00:46:00,560
and articles about entrepreneurship and innovation

1022
00:46:00,560 --> 00:46:01,760
at Stanford eCorner.

1023
00:46:01,760 --> 00:46:03,880
That's ecorner.stanford.edu.

1024
00:46:03,880 --> 00:46:05,160
Thank you everybody.

1025
00:46:05,160 --> 00:46:06,000
Thank you, Andrew.

1026
00:46:06,000 --> 00:46:07,000
Thanks, thanks Ravi.

