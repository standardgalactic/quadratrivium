1
00:00:00,000 --> 00:00:14,960
Welcome, everybody. Thanks for coming to this session. So I'm Evan Trepliki. I'm the designer

2
00:00:14,960 --> 00:00:20,520
of the Elm programming language. And I got started on it about seven years ago with a

3
00:00:20,520 --> 00:00:25,960
paper called Elm Concurrent FRP for Functional Goose. And I was like, I really think I can

4
00:00:25,960 --> 00:00:31,040
make this functional programming stuff easier. And I didn't nail it immediately with that

5
00:00:31,040 --> 00:00:37,840
title. But I had this idea of this experience with type functional programming that was

6
00:00:37,840 --> 00:00:43,640
really joyful. And what I wanted to get out with Elm was I wanted to take that part that

7
00:00:43,640 --> 00:00:49,480
I felt was so fun and make it accessible to other people and share that joy that I had

8
00:00:49,480 --> 00:00:53,720
felt. And so part of that is technical. You have to write a compiler and all this kind

9
00:00:53,720 --> 00:01:00,040
of stuff. But another part is like practical stuff. Like getting set up should be easy.

10
00:01:00,040 --> 00:01:04,880
And that was part of what was important to me having spent a whole day just trying to

11
00:01:04,880 --> 00:01:09,060
learn something, trying to see if something was interesting and being really frustrated.

12
00:01:09,060 --> 00:01:13,240
Another piece of that was if program is going to be fun, I want the community to be friendly.

13
00:01:13,240 --> 00:01:18,400
I didn't want it to be a cool club for the cool kids. You know, oh, we're functional

14
00:01:18,400 --> 00:01:24,560
programmers and you're not because you're dumb. Like, I didn't relate to that. I just

15
00:01:24,560 --> 00:01:29,280
had a nice time programming and I wanted to share that nice time in the same way that

16
00:01:29,280 --> 00:01:35,480
I like sushi. And I might say, hey, you should try it out. It's pretty nice. And so I might

17
00:01:35,480 --> 00:01:43,200
say, no, thanks. Oh, okay. You know, that's the kind of interactions I wanted to have

18
00:01:43,200 --> 00:01:50,080
because it was just about having this experience. So as I've talked to more open source developers,

19
00:01:50,080 --> 00:01:55,080
people who design languages, people work on databases, people work on machine learning

20
00:01:55,080 --> 00:02:01,720
or discussion platforms or environmental sensors, however much we disagree on design

21
00:02:01,720 --> 00:02:07,560
or what our goals are, we all have stories in common about like having a friendly community

22
00:02:07,560 --> 00:02:11,360
is really, really difficult. And that's one of the places in open source where, you know,

23
00:02:11,360 --> 00:02:16,240
as good as you can be at the technical stuff, like you don't have a lot of, ultimately, you

24
00:02:16,240 --> 00:02:20,680
can't control what's going to happen. And if someone's going to yell, it's like, I wish

25
00:02:20,680 --> 00:02:27,360
that didn't happen. So I've been doing this for about seven years now and I've started

26
00:02:27,360 --> 00:02:32,040
to notice some patterns of behavior that I think probably a lot of people who've worked

27
00:02:32,040 --> 00:02:39,440
in open source with larger prize will relate to. So one is this, why don't you just? And

28
00:02:39,440 --> 00:02:45,000
in Elm, it's like, why don't you just get the JS API directly or release an incremental

29
00:02:45,000 --> 00:02:51,520
version instead of a bigger release or, hey, can we derive JSON decoders? And the short

30
00:02:51,520 --> 00:02:56,920
answer in all these cases is that it's more complex than it sounds. Like if there's something

31
00:02:56,920 --> 00:03:02,600
that you can think of in five minutes or an hour or a day, probably someone has thought

32
00:03:02,600 --> 00:03:06,280
about that and considered it. And there might be implications that you don't see from your

33
00:03:06,280 --> 00:03:10,620
perspective, but someone else in the community might have a problem with that that's not

34
00:03:10,620 --> 00:03:15,800
obvious to you. So when you're doing design, why don't you just? It's like, well, there's

35
00:03:15,800 --> 00:03:20,720
all these different parties that we have to sort of make things work for. And I can do

36
00:03:20,720 --> 00:03:25,400
my best on my intuition, but ultimately, even after I spend like a week trying to design

37
00:03:25,400 --> 00:03:28,960
something that way, I need to go out and show it to people and see what objections they

38
00:03:28,960 --> 00:03:35,320
bring and then maybe do it with another design. So this why don't you just is like, it's quite

39
00:03:35,360 --> 00:03:39,520
frustrating. And one thing that happens is there's a lot of people who are new to the

40
00:03:39,520 --> 00:03:43,160
project. So maybe there's 10,000 people who might be curious, like, oh, why don't you

41
00:03:43,160 --> 00:03:48,440
just try out this kind of thing? And the number of people who know the full context is pretty

42
00:03:48,440 --> 00:03:54,080
small. So maybe there's like 10, 20 people. So if it takes five minutes to say, why don't

43
00:03:54,080 --> 00:03:58,160
you just blah, blah, blah? And it takes two pages of writing. And you have to write it

44
00:03:58,160 --> 00:04:02,200
very carefully because if you're an influential community member, people will refer back to

45
00:04:02,240 --> 00:04:07,080
what you said four or five years ago and say like, man, there's such a jerk here. Here's

46
00:04:07,080 --> 00:04:13,200
the evidence. They said this in 2013. And it's like, yeah, yeah, yeah, I said that in

47
00:04:13,200 --> 00:04:23,040
2013. Yeah. So one thing that's common is like, well, if it's so much work, like, why don't

48
00:04:23,040 --> 00:04:29,160
you just delegate the work, right? So this is a comment I got in real life and the italics

49
00:04:29,240 --> 00:04:34,120
are from the person talking to me. They said there's another way to deal with this, like,

50
00:04:34,120 --> 00:04:39,120
delegation. And then they go on to describe how delegation works and what benefits it might

51
00:04:39,120 --> 00:04:46,160
have. I was like, oh, very interesting. Yes, I hadn't thought about that. And they describe

52
00:04:46,160 --> 00:04:51,000
a person who can do all this work. And they say this somebody, again, that is their italics,

53
00:04:51,000 --> 00:04:58,320
can also be a proxy who's gathering feedback. So like, you don't have to be in these discussions.

54
00:04:58,480 --> 00:05:06,680
So I was initially very upset about this. So like the unfiltered in my own mind version was

55
00:05:06,680 --> 00:05:12,960
like, oh, hello, is this the somebody store? Yes, we'd like someone to take unsolicited advice on

56
00:05:12,960 --> 00:05:18,080
the internet. Oh, yeah, it's it's really mean. Yeah, it's it's going to be rough. And no, yeah,

57
00:05:18,080 --> 00:05:25,160
no one's going to say thank you. Yeah, no, it's it's unpaid. Yeah, it's unpaid. It takes you

58
00:05:26,040 --> 00:05:34,120
don't have any. I was told there would be somebody who would do this. And, you know, so this was

59
00:05:34,120 --> 00:05:42,160
me in my own life, like walking around my room, just like, and that's, you know, that's not a

60
00:05:42,160 --> 00:05:46,440
healthy place to be. That's not how I want to look to a community. That's not what I want the

61
00:05:46,440 --> 00:05:52,520
community. I'm a part of to be either. And when I took some time and thought about it more, I

62
00:05:52,560 --> 00:05:57,160
realized there's actually like a pretty reasonable assumption going on here, which is like free

63
00:05:57,160 --> 00:06:03,880
rice means you can take as much as you want. The rest is free. Take a lot of rice. And so does

64
00:06:03,880 --> 00:06:10,000
that imply that free labor means you can use as much as you want? Well, the labor's free. But in

65
00:06:10,000 --> 00:06:15,960
fact, this isn't how labor works. If you don't pay for labor, you get less. And so I think that's

66
00:06:15,960 --> 00:06:20,920
sort of the root thing. It's like, Oh, well, it should be unlimited. Anyone can help. Now, let's

67
00:06:20,960 --> 00:06:25,760
assume it is unlimited that everybody in the world actually can help. In practice, you actually

68
00:06:25,760 --> 00:06:33,080
have to work together, right? These are highly technical projects. Are you able to work well

69
00:06:33,080 --> 00:06:37,480
together? Is your goals with the project aligned? How much time does it take to coordinate with

70
00:06:37,480 --> 00:06:41,520
that person to get stuff done? So even if you can work with anybody you want, there's still

71
00:06:41,520 --> 00:06:46,960
these limitations on like who is going to be really effective in doing the right stuff. So I

72
00:06:47,000 --> 00:06:54,800
wrote about that a little bit in this post. I think, no, sorry, Richard wrote about that in this

73
00:06:54,800 --> 00:07:01,800
post here of like, what actually does it take to get involved? And it's like, it's not just somebody

74
00:07:01,800 --> 00:07:08,120
store. So, you know, some people may be thinking like, you know, Evan, you're doing a lot of telling

75
00:07:08,120 --> 00:07:14,480
us what's the situation, but like, who are you to say that? And so this is another pattern that I

76
00:07:14,520 --> 00:07:20,360
see a lot. It's like on who's authority. And this is actually the title of a post that was sent

77
00:07:20,360 --> 00:07:30,360
to the closure community. And the post started out, F closure. There I said it, and God, it feels

78
00:07:30,360 --> 00:07:41,320
good. I say it with much admiration and respect to all the community members. And then they go on

79
00:07:41,320 --> 00:07:46,960
to say some criticisms and talk about the relationship with women. And it's quite a roller

80
00:07:46,960 --> 00:07:58,040
coaster of a post. But what's interesting, besides sort of like it as a journal entry, is that it

81
00:07:58,040 --> 00:08:03,560
gets a lot of engagement, right? So 320 comments on the Reddit thread. I'm sure people talked about

82
00:08:03,560 --> 00:08:10,680
it in other contexts as well, where there would have been more comments. And, you know, as someone

83
00:08:10,680 --> 00:08:16,400
who's been working on an open source project for a bunch of years now, enough people have told me

84
00:08:16,400 --> 00:08:21,760
that Elm is going to die next month. That I'm like, I don't think they're right this time. You know,

85
00:08:21,760 --> 00:08:29,880
like I have that fear doesn't speak to me anymore because I have the experience. But there are

86
00:08:29,880 --> 00:08:34,520
other people in the community who don't have that same experience. And like this can be like a

87
00:08:34,520 --> 00:08:39,800
scary thing of like, man, people like aren't liking this thing. Are we doing something wrong? They

88
00:08:39,840 --> 00:08:46,920
feel like maybe it could be better. They might get defensive. So in one of these 320 comments, the

89
00:08:46,920 --> 00:08:53,720
creator of closure says, I found out about this while sitting down to spend my weekend contributing

90
00:08:53,720 --> 00:08:58,880
to the closure ecosystem. Time spent in lieu of spending time with my wife, having already spent

91
00:08:58,880 --> 00:09:05,720
my work week on other closure related stuff. And I relate to this a lot. I've definitely written

92
00:09:05,960 --> 00:09:11,600
like, hey, like we, I get there's different viewpoints, but like we can't yell our viewpoints at

93
00:09:11,600 --> 00:09:18,160
each other. And that was my Saturday. And, you know, as you work over the course of the years,

94
00:09:18,160 --> 00:09:24,560
like, there can only be so many Saturdays that are like that before it starts to hurt you in larger

95
00:09:24,560 --> 00:09:32,760
ways. So Rich Chicky goes on to say, you know, every time I have to process a diatribe like this,

96
00:09:32,760 --> 00:09:36,920
and it's aftermath, the effects on myself, my family and my coworkers, I have to struggle back

97
00:09:36,920 --> 00:09:41,800
from why should I bother? And every time it gets harder to justify to myself and my family that

98
00:09:41,800 --> 00:09:46,920
it's worth the time, energy and emotional burden. Now, I've talked to some people about this post

99
00:09:46,920 --> 00:09:51,720
and they, they thought different things stood out to me. This last part is what stands out,

100
00:09:51,720 --> 00:09:57,960
because I think a lot of people in open source feel this way and like would never say it out loud.

101
00:09:57,960 --> 00:10:03,480
I was really surprised to see it that way. And it kind of gave me some confidence to talk about

102
00:10:03,480 --> 00:10:11,320
that kind of stuff as well. So, you know, we have our posts like this in the Elm community

103
00:10:12,600 --> 00:10:19,320
with a bunch of comments as well. And I see it not just as like, oh man, this is hard for me to

104
00:10:19,320 --> 00:10:23,800
process, but the people I work with have a hard time processing it. And then if you just add up

105
00:10:23,800 --> 00:10:28,120
all the time, let's say maybe 10 minutes is spent on each of these comments, which I think is low,

106
00:10:28,120 --> 00:10:35,320
like a conservative estimate. We're talking about like 50 hours for this one, that's just like

107
00:10:35,320 --> 00:10:42,040
dealing with someone's anger. And could that have been helping someone new or spending time with

108
00:10:42,600 --> 00:10:48,360
some family members or like learning some hobby that could get you out of work and get you like

109
00:10:48,360 --> 00:10:54,360
a more healthy attitude? So, one thing I hear a lot when I talk about this stuff is like, if it

110
00:10:54,360 --> 00:11:05,480
makes you so mad, why don't you just not read it? Why don't you just not read it? So, another pattern

111
00:11:05,480 --> 00:11:12,120
that's really common is that all discussion is constructive. You know, I'm just saying how I

112
00:11:12,200 --> 00:11:19,560
feel. I feel like F you, and I respect you a lot. And you know, I think you're an idiot, but like,

113
00:11:19,560 --> 00:11:25,800
I really learned a lot from you. And like, that's a difficult, like personal relations to have,

114
00:11:25,800 --> 00:11:28,760
I don't know if people have people like that in their life, but that's a difficult thing to deal

115
00:11:28,760 --> 00:11:36,120
with a lot. So, one discussion that was along these lines is like, should Elm have user definable

116
00:11:36,120 --> 00:11:40,920
infix operators? This came up recently with our recent release. And if we just focus on this

117
00:11:40,920 --> 00:11:47,800
question, like textually, someone might say, yeah, there are cases where it makes code shorter

118
00:11:47,800 --> 00:11:53,240
and more convenient. And someone else might say, no, because it can make code harder to read,

119
00:11:53,240 --> 00:11:58,840
especially in a large team. And you know, textually, like, this is an interesting argument. Like,

120
00:11:58,840 --> 00:12:04,200
yeah, it can make coach. That's a good point. And it might hurt people in a large team. And then

121
00:12:04,200 --> 00:12:10,120
onlookers will sort of say, oh, which one seems to make more sense to me? But when you take a step

122
00:12:10,120 --> 00:12:15,320
back and stop thinking of it just as like a textual argument about who is right and who's wrong

123
00:12:15,320 --> 00:12:20,760
and say, okay, all these people have different priorities. Some of them may value flexibility

124
00:12:20,760 --> 00:12:26,840
a lot. And some people might value simplicity a lot. And all these people exist on this

125
00:12:27,720 --> 00:12:30,520
with different priorities. So the person who's like, yeah, we should have this,

126
00:12:31,080 --> 00:12:36,040
really might value flexibility. And the person who says no is saying, well,

127
00:12:36,520 --> 00:12:43,240
if like all these benefits are telling me about flexibility or like how code can be shorter

128
00:12:43,240 --> 00:12:46,760
and more convenient, it's like, that's not persuasive to me. Like, that's not a good

129
00:12:46,760 --> 00:12:52,600
rational argument because it's not important. And likewise, you know, how does it work on

130
00:12:52,600 --> 00:12:56,840
a large team? It's like, well, that doesn't, it doesn't matter. It's not about that. And all the

131
00:12:56,840 --> 00:13:02,440
people exist on this spectrum is with different priorities as well. So they're evaluating it

132
00:13:02,440 --> 00:13:08,440
not as which is the true objective argument, but given my priorities, which is the one that

133
00:13:08,440 --> 00:13:16,600
makes the most sense to me. So I've come to see constructive discussion is about mutual understanding

134
00:13:16,600 --> 00:13:20,920
rather than mutual agreement. And a lot of discussions online are like, we're going to get

135
00:13:20,920 --> 00:13:26,440
to a point where you agree with me rather than saying like, huh, this person's seeing it different.

136
00:13:26,440 --> 00:13:31,720
Why is that maybe they're seeing something I don't. So

137
00:13:34,120 --> 00:13:39,160
when I take a step back and think about these different patterns, I just think like, why,

138
00:13:39,160 --> 00:13:44,520
why is this happening? You know, I don't have problems like this in normal life. You know,

139
00:13:44,520 --> 00:13:53,160
if I'm at an Elm conference or a meetup or like nothing ever is so emotionally difficult as these

140
00:13:53,160 --> 00:13:58,920
interactions. So I found this documentary called All Watched Over by Machines of Loving Grace by

141
00:13:58,920 --> 00:14:05,880
Adam Curtis. It's excellent as is all of his work. And that sort of taught me or I found through that

142
00:14:05,880 --> 00:14:12,520
a book called From Counterculture to Cyberculture. So this revealed to me a sort of an intellectual

143
00:14:12,520 --> 00:14:18,120
history going back to the 1950s that really helps explain what's going on in open source right now.

144
00:14:18,760 --> 00:14:28,920
So it traces things from a book called The Human Use of Human Beings. So this came out of MIT in

145
00:14:28,920 --> 00:14:36,840
like 1952 by a person who had created artillery that could automatically track planes and shoot them

146
00:14:36,840 --> 00:14:44,360
down. And then the whole earth catalog, which was popular in the back to the land movement. So

147
00:14:44,440 --> 00:14:47,880
a lot of people on communes might have bought this, but it was a much larger thing than that.

148
00:14:48,760 --> 00:14:52,520
And then finally the Electronic Frontier Foundation, and it sort of ties these together in a very

149
00:14:52,520 --> 00:14:58,520
interesting way. So we'll look at some of the things going on here. So in The Human Use of Human

150
00:14:58,520 --> 00:15:06,760
Beings, Norbert Weiner introduces the idea of cybernetics. He defines it as the study of messages

151
00:15:06,760 --> 00:15:17,480
as a means of controlling machinery and society. So it's a little weird. Okay, okay, fine. And as

152
00:15:17,480 --> 00:15:23,480
you start to read it, it's like this way of like, let's not look at the person, but let's look at the

153
00:15:23,480 --> 00:15:28,120
messages going around, and that's how we'll think about how the world works. And so you see things

154
00:15:28,120 --> 00:15:34,360
like words such as life, purpose, and soul are grossly inadequate to precise scientific thinking,

155
00:15:35,080 --> 00:15:40,600
which is it's like fair enough, but also like those things like life and purpose are like

156
00:15:41,560 --> 00:15:50,200
pretty important to people as well to consider. But you know, a person's just a special sort of

157
00:15:50,200 --> 00:15:55,880
machine, right? Like we can consider it as a thing that takes inputs and does stuff. Emotions are

158
00:15:55,880 --> 00:16:05,000
just a useless epiphenomenon. That's real. He said that. Another thing he's like, we have

159
00:16:05,000 --> 00:16:09,160
modified our environment so radically that we must now modify ourselves in order to exist in

160
00:16:09,160 --> 00:16:14,680
this new environment. So it's sort of how can we take a person and simplify it down to a machine

161
00:16:14,680 --> 00:16:20,920
or a system that we can understand well? And then once we're starting to think of people that way,

162
00:16:20,920 --> 00:16:24,680
well, we can improve machines. We can add things to them and the machines get more

163
00:16:24,680 --> 00:16:31,240
capabilities. Maybe that's how we move forward as people. And life and purpose is like that.

164
00:16:32,200 --> 00:16:39,880
We can't go back to that. It's too different now. So the sort of connecting thread and whole

165
00:16:39,880 --> 00:16:45,640
earth catalog besides Stuart Brand, the author of this knowing a bunch of the people in the

166
00:16:45,640 --> 00:16:54,680
cybernetics group, is this idea of access to tools, right? So like we are now, our relationship

167
00:16:54,680 --> 00:17:00,440
with our tools is going to be how we move forward. So this catalog starts with we are as gods and

168
00:17:00,440 --> 00:17:06,920
we might as well get good at it. So far, remotely done power and glory as government, big business,

169
00:17:06,920 --> 00:17:12,360
formal education, church has succeeded to the point where the gross defects obscure actual gains.

170
00:17:12,680 --> 00:17:22,440
In response, personal power is developing. So he's focused on a bunch of different topics.

171
00:17:23,560 --> 00:17:30,200
And so education, finding inspiration, shaping his environment, sharing the adventure. Now,

172
00:17:30,200 --> 00:17:34,280
keep in mind this is written in 1969. So this is before the internet. And if you read these

173
00:17:34,280 --> 00:17:39,320
points as like what an ideal view of the internet should be, it works really well. Like you can

174
00:17:39,320 --> 00:17:45,160
find out all sorts of interesting stuff. You can be inspired by what's going on out there. You can

175
00:17:45,160 --> 00:17:49,640
find a place where you really fit in, even if you don't fit in, in your local community that has

176
00:17:49,640 --> 00:17:56,200
different values than you. And you can share that with whoever you want to. Like there's this place

177
00:17:56,200 --> 00:18:01,640
for self-expression. And so he says tools that aid this process are sought and promoted by the

178
00:18:01,640 --> 00:18:07,160
whole earth catalog. So some people have argued that this is sort of like the what preceded the

179
00:18:07,160 --> 00:18:13,480
internet. It sort of foresaw what was going to happen there. And this publication and the

180
00:18:13,480 --> 00:18:18,920
creator, Stuart Brand, had been really influential. So one of the positive quotes is when I was young,

181
00:18:18,920 --> 00:18:22,440
there was an amazing publication called the whole earth catalog, which was one of the

182
00:18:22,440 --> 00:18:30,120
Bibles of my generation. So that was Steve Jobs. There was a project that Stuart Brand created

183
00:18:30,120 --> 00:18:37,480
called the 10,000 year clock, which Jeff Bezos helped fund with $42 million. So what is this,

184
00:18:37,480 --> 00:18:43,880
like this book that was popular on back to the land communes, became very influential because of this

185
00:18:44,680 --> 00:18:51,960
how tools and like a rejection of hierarchy, we're somehow in this new place where we're

186
00:18:51,960 --> 00:18:57,240
going to choose our own future through our connection with tools. And finally, the electronic

187
00:18:57,320 --> 00:19:01,880
frontier foundation. Interestingly, the founders of this met on the whole earth

188
00:19:01,880 --> 00:19:08,840
electronic link, one of the earliest bullet or message boards. And so one of the founders

189
00:19:08,840 --> 00:19:14,040
wrote the declaration of independence of cyberspace, governments of the industrial world,

190
00:19:14,040 --> 00:19:20,280
you weary giants of flesh and steel, you have no sovereignty where we gather. I declare the global

191
00:19:20,280 --> 00:19:25,560
space, social space we are building to be naturally independent of the tyrannies you seek to impose on

192
00:19:25,560 --> 00:19:31,560
us. So again, you see this like distrust of hierarchy, big businesses failed, government has

193
00:19:31,560 --> 00:19:37,000
failed. We are going to find a new way through this thing. So one of the early cases that the

194
00:19:37,000 --> 00:19:42,600
electronic frontier foundation that inspired the creation of this foundation was there was a

195
00:19:42,600 --> 00:19:51,080
programmer called Lloyd Blankenship. And Bell South found that some of their 911 alert system

196
00:19:51,080 --> 00:19:55,640
documentation had been posted on a bulletin board. And so they got the secret service involved

197
00:19:55,640 --> 00:20:01,480
and took some computers that might have sensitive information. So Lloyd was arrested and the

198
00:20:01,480 --> 00:20:06,520
electronic frontier foundation like was created around that time to help protect people in the

199
00:20:06,520 --> 00:20:12,280
situation. So after Lloyd was arrested, he wrote something called the hackers manifesto,

200
00:20:13,560 --> 00:20:19,080
which I think gives a raw version of what's going on in this world. So he says,

201
00:20:19,880 --> 00:20:23,640
I'm smarter than most of the other kids. This crap they teach us bores me.

202
00:20:25,000 --> 00:20:28,200
I've listened to my teacher explain for the 15th time how to reduce a fraction.

203
00:20:28,200 --> 00:20:31,560
I understand it. No, Miss Smith, I didn't show my work. I did it in my head.

204
00:20:33,400 --> 00:20:42,440
And you can see, well, he says, I made a discovery today. I found a computer. It does what I

205
00:20:42,440 --> 00:20:47,720
want it to do. If I make a mistake, it's because I screwed up, not because it doesn't like me,

206
00:20:47,800 --> 00:20:52,520
or it feels threatened by me, or it thinks I'm a smart ass, or it doesn't like teaching and

207
00:20:52,520 --> 00:20:58,360
shouldn't be here. So you're seeing this, this guy is 21 when he's writing this. He's somebody

208
00:20:58,360 --> 00:21:03,400
sort of very disillusioned, not just with government or big business, but like just his

209
00:21:03,400 --> 00:21:09,400
classroom, the social environment he's living in. And he goes on to say, this is our world now.

210
00:21:09,400 --> 00:21:13,400
We explore, but you call us criminals. You build atomic bombs, wage wars.

211
00:21:14,040 --> 00:21:17,720
You try to make us believe it's for our own good, yet we're the criminals.

212
00:21:19,560 --> 00:21:24,600
So again, you see that explicit distrust of hierarchy. And then finally,

213
00:21:24,600 --> 00:21:28,200
my crime is that of outsmarting you, something you will never forgive me for.

214
00:21:30,040 --> 00:21:34,760
So I don't know how many people who work on open source will recognize aspects of this attitude

215
00:21:34,760 --> 00:21:42,360
and interactions they have. But there's sort of such a strong rejection of the social things

216
00:21:42,440 --> 00:21:47,400
going on. It's like, well, the teacher who's not teaching you well has a bunch of other students,

217
00:21:47,400 --> 00:21:52,680
and it's difficult to balance all their needs. Maybe it's malicious, but maybe there's another

218
00:21:52,680 --> 00:21:58,440
reason. But this worldview is kind of like, I outsmarted you. You don't see it how I see it.

219
00:21:59,080 --> 00:22:06,760
So when I take a step back on this intellectual pathway, the things I sort of draw out from

220
00:22:06,760 --> 00:22:14,200
reading these books and things is, one, we're gods. Two, hierarchy has failed us. There's

221
00:22:14,200 --> 00:22:19,640
sort of a deep distrust of hierarchical structures. I think rightly, there's a lot of bad things that

222
00:22:19,640 --> 00:22:26,920
come out of hierarchy. And finally, order will emerge from the new technology. So when we reconsider

223
00:22:26,920 --> 00:22:32,760
the patterns we've seen open source, stuff makes a lot of sense. So it's like, why don't you just?

224
00:22:32,760 --> 00:22:38,840
It's like, haven't you, we can just, through reason and rationality, figure out the answer.

225
00:22:38,840 --> 00:22:44,360
Why don't you just do the obvious thing? Hierarchy has failed us. So again, on whose

226
00:22:44,360 --> 00:22:49,160
authority is coming out of this tradition of hierarchical structures haven't served us well.

227
00:22:49,160 --> 00:22:54,040
We need to find a way that isn't structured in that way. Your authority as the author

228
00:22:56,040 --> 00:23:01,240
is not legitimate on those grounds. And all discussion is constructive. It's like, well,

229
00:23:02,040 --> 00:23:06,200
this is the new technology. This is what the new technology is producing. So this must be

230
00:23:07,080 --> 00:23:15,080
the way forward to this place that we want to go. So I found this pathway really interesting,

231
00:23:15,080 --> 00:23:20,040
and it informed, like, it helped me understand a lot what was going on. But it's just one of

232
00:23:20,040 --> 00:23:25,320
a couple of different ways of looking at how we got to this level of conflict in open source.

233
00:23:25,320 --> 00:23:32,760
So I'm calling this sort of the intellectual issue of freedom. People are primarily prioritizing

234
00:23:32,760 --> 00:23:37,720
freedom. But there are other ones, such as people who are primarily prioritizing engagement.

235
00:23:38,600 --> 00:23:44,680
So I want to start with a quote, the enormous expansion of communications has entirely transformed

236
00:23:44,680 --> 00:23:49,080
the conditions of trade and commerce. Everything is done in haste at fever pitch. The night is

237
00:23:49,080 --> 00:23:53,640
used for travel, the day for business, even holiday trips put strain on the nervous system.

238
00:23:54,360 --> 00:23:59,800
And do people relate to that? Holiday trips being stressful? I feel that. Or I struggle with that,

239
00:23:59,800 --> 00:24:10,200
at least. I try to take a break or whatever. Great political, industrial, and financial crises

240
00:24:10,200 --> 00:24:14,040
carry this segment into far wider areas of the population than ever before.

241
00:24:14,040 --> 00:24:18,120
Interest in political life has become universal. Tempers are inflamed by political, religious,

242
00:24:18,120 --> 00:24:23,560
and social struggles, party politics, electioneering. Sound familiar to anybody?

243
00:24:26,680 --> 00:24:30,120
Finally, people are forced to engage in a constant mental activity and robbed of the

244
00:24:30,120 --> 00:24:34,360
time needed for relaxation, sleep, and rest. So if you had to guess when this was written,

245
00:24:35,480 --> 00:24:40,280
it's reasonable. It could be this year, it could be 2017, or maybe someone was really

246
00:24:40,280 --> 00:24:44,040
prophetic and they wrote it in like 1980. It's like, I see where this is all going.

247
00:24:45,000 --> 00:24:48,200
So this is actually from something Freud wrote in 1902.

248
00:24:49,560 --> 00:24:55,000
The part I left out is due to the worldwide telegraph and telephone networks

249
00:24:56,040 --> 00:25:02,920
and the immense growth of trade unionism. So I mean, I think it makes sense that he would

250
00:25:02,920 --> 00:25:06,760
have seen these kinds of things. He seems like a smart dude, or at least someone who's very

251
00:25:06,760 --> 00:25:14,360
sensitive to human behavior. And so this is kind of where this intellectual history starts.

252
00:25:14,360 --> 00:25:19,000
So we have Freud, but we're going to look at two other books. One is called Propaganda

253
00:25:19,000 --> 00:25:25,720
from 1928, and one is called Nudge, much more recent from 2008. So with Propaganda,

254
00:25:25,720 --> 00:25:30,600
this is written by Edward Bernays. This is actually Freud's nephew. The connections between

255
00:25:30,600 --> 00:25:36,840
all these works are crazy. And as you look into any parts of these, like everybody met,

256
00:25:36,840 --> 00:25:40,840
everybody worked with someone's nephew or cousin or mom, or it's very strange.

257
00:25:42,840 --> 00:25:46,920
So this book is essentially a bunch of stories about how Edward Bernays sort of created the

258
00:25:46,920 --> 00:25:51,400
idea of public relations. So one of the stories he tells is about torches of freedom.

259
00:25:52,600 --> 00:25:57,960
So this was an ad campaign to break the taboo against women smoking. At the time men would

260
00:25:57,960 --> 00:26:04,040
smoke, and it was acceptable to some degree, and women couldn't. It was very looked down upon.

261
00:26:05,320 --> 00:26:11,640
So the president of the American Tobacco Company said, if we can break this taboo,

262
00:26:11,640 --> 00:26:17,000
it'll be like a gold mine opening right in our front yard. So he hires Edward Bernays.

263
00:26:17,800 --> 00:26:24,440
And what Edward Bernays does is he hires women who are good looking, but not too modally. That's

264
00:26:24,440 --> 00:26:28,920
the quote I found good looking, but not too modally. To walk in an Easter Sunday parade

265
00:26:28,920 --> 00:26:34,360
and smoke, he also hired photographers to get really good photos of these women,

266
00:26:34,360 --> 00:26:38,840
and then distributed those photos to newspaper, like through newspaper connections that he had

267
00:26:38,840 --> 00:26:46,600
to make sure it got published all around the world. So this torches of freedom idea was saying,

268
00:26:46,600 --> 00:26:53,240
we see this trend about women's liberation happening, and like this is aligned with that

269
00:26:53,240 --> 00:27:00,040
movement, and that this is a way of punching up against those taboos. But it's very focused on

270
00:27:00,040 --> 00:27:04,600
like, hey, we're going to sell a bunch, we're going to make a bunch of money here. And so one of the

271
00:27:04,600 --> 00:27:10,520
ads that came out of this was an ancient precious has been removed. And what's interesting about

272
00:27:10,520 --> 00:27:17,960
this ad is that visually it's clearly about women smoking, but texturally it's saying toasting did

273
00:27:18,040 --> 00:27:24,600
it. It's because they toast the tobacco, it's less harsh on your throat, and that's what has

274
00:27:24,600 --> 00:27:30,920
removed the prejudice. So texturally, they can say, look, we're not getting into politics. We're

275
00:27:30,920 --> 00:27:36,760
just saying that toasting is cool, and that's a lady who smokes. It's like, I don't see the problem.

276
00:27:37,400 --> 00:27:44,360
But meanwhile, you have hired Edward Bernays to actually run this campaign. So, oh yeah,

277
00:27:44,440 --> 00:27:50,280
I want to read a little bit from his book. This is the book. He says, the old-fashioned

278
00:27:50,280 --> 00:27:56,120
propagandist using almost exclusively the appeal to the printed word tried to persuade the individual

279
00:27:56,120 --> 00:28:04,760
reader to buy a definite article immediately. So his example of this is like, you buy O'Leary's

280
00:28:04,760 --> 00:28:12,280
rubber bands now. And he's like, okay, that's the old-fashioned way. The modern propagandist

281
00:28:12,280 --> 00:28:17,880
therefore sets to work to create the circumstances that will modify the custom. It doesn't matter

282
00:28:17,880 --> 00:28:23,480
what this ad says. It's about creating circumstances such that the custom will change in whatever

283
00:28:23,480 --> 00:28:29,640
direction someone pays me to change it. So another example he gives is for Mozart pianos.

284
00:28:30,200 --> 00:28:35,000
So I don't know exactly the finances of Mozart pianos, but let's say they had 30% of market share

285
00:28:35,720 --> 00:28:41,000
and he gets hired to make it higher. Maybe it can go to 35 or 40. That'd be a huge for Mozart

286
00:28:41,000 --> 00:28:47,320
pianos. So Bernays comes in and he says, okay, I could say to people, will you please buy a piano,

287
00:28:47,320 --> 00:28:54,440
but I'm not going to do that. I know that pianos have this sort of elite cachet. And so what I'm

288
00:28:54,440 --> 00:29:00,040
going to do is I want to make an architecture expo in New York City where we're going to showcase

289
00:29:00,040 --> 00:29:06,600
music parlors and we're going to bring in famous people, influential artists and musicians to

290
00:29:06,600 --> 00:29:12,360
be in the rooms. We're going to have expensive tapestries and really lean into this elite picture

291
00:29:12,360 --> 00:29:18,680
and promote it in our connections with newspapers. He also invites architects from all over the

292
00:29:18,680 --> 00:29:24,280
country. He wants influential architects and they'll bring designs for music parlors. So what this

293
00:29:24,280 --> 00:29:31,160
expo does is it creates a music parlor as an aspirational goal and it brings in architects

294
00:29:31,160 --> 00:29:35,960
who will then go back to wherever they're from with designs for music parlors and they'll start

295
00:29:35,960 --> 00:29:40,600
building houses that have that. And ideally they'll influence other architects who are not as

296
00:29:40,600 --> 00:29:46,680
influential to add music parlors as well. And so instead of saying, hey, will you please buy this

297
00:29:46,680 --> 00:29:50,520
piano, people are now saying, hey, I have this piano shaped hole that I need to fill. Like,

298
00:29:50,520 --> 00:29:58,600
do you have something piano shaped? So again, the modern propagandist sets to work to create

299
00:29:58,600 --> 00:30:03,880
the circumstances that will modify the custom. So this got modernized and sort of made a bit

300
00:30:03,880 --> 00:30:11,160
nicer in nudge. So here we see a nudge is any aspect of a choice architecture that will alter

301
00:30:11,160 --> 00:30:15,720
people's behavior in a predictable way without forbidding any options or significantly changing

302
00:30:15,720 --> 00:30:23,080
their economic incentives. So this book has been really influential in tech recently. So one thing

303
00:30:23,080 --> 00:30:29,480
that we're probably all familiar that's an example of this is autoplay of videos. So you just finished

304
00:30:30,120 --> 00:30:36,920
a show. It ends on a cliffhanger because they wrote it that way. And you're like, man, that was

305
00:30:36,920 --> 00:30:48,600
cool. And then you're like, oh, my body. Is it hungry? Does it need a walk? Did it have plans

306
00:30:49,320 --> 00:30:57,720
for today? Or for anything? And then the music starts again and you're like, no, no.

307
00:31:00,680 --> 00:31:06,840
So this is a nudge, right? You're free to do some other behavior, but through the choice

308
00:31:06,840 --> 00:31:12,360
architecture that was created, a predictable amount of people don't make that free choice.

309
00:31:13,320 --> 00:31:19,240
So this ended up being popular at Google. So if anyone's visited Google cafeterias, you'll

310
00:31:19,240 --> 00:31:24,600
probably have seen all the food is marked with colors. So green means eat anytime. Yellow means

311
00:31:24,600 --> 00:31:28,920
once in a while. Red means not often, please. And it's all marked. It's actually very helpful.

312
00:31:29,960 --> 00:31:35,960
You can be like, oh, I just had one red thing today. It's kind of nice. But this interest in

313
00:31:36,440 --> 00:31:42,680
colors changing people's behavior was used to Google in other ways. So when you look at the

314
00:31:42,680 --> 00:31:49,560
history of their ad labeling, you see something similar. So when ads started, they sort of just

315
00:31:49,560 --> 00:31:57,640
start by playing around with colors. It's like, well, maybe green is fun. Or maybe this lavender,

316
00:31:57,640 --> 00:32:03,480
purple thing is the way to go. So in a new phase of things, you start to see, okay, well, let's

317
00:32:03,560 --> 00:32:10,920
do something more white. Can it be something more white? Yeah, maybe a little more white, you know?

318
00:32:13,720 --> 00:32:17,080
Well, how about something like a little more white? Like, all this color is like,

319
00:32:17,080 --> 00:32:20,760
let's just put it in one place. We'll take all the saturation, put it in one place.

320
00:32:22,040 --> 00:32:29,400
And like, hey, it works fine. No problem. And then it's like, well, that yellow is pretty ugly.

321
00:32:29,960 --> 00:32:35,480
We could just, you know, it's still labeled. It's no problem. And then it's like, I mean,

322
00:32:36,680 --> 00:32:42,360
who wanted it to be yellow? And I mean, it wasn't really that important. And then it's like, you

323
00:32:42,360 --> 00:32:54,680
know, the background, I mean, people get it. They get it. So when you search for Italy tour, for

324
00:32:54,680 --> 00:33:02,840
example, everything above the fold is an ad and it's labeled in this very, you know, subtle and

325
00:33:02,840 --> 00:33:10,440
nice way. So I found this very interesting quote from the head of text ads at Google. He says,

326
00:33:10,440 --> 00:33:15,560
we want to make it easier for our users to adjust information on the page. So we're gradually trying

327
00:33:15,560 --> 00:33:19,880
to reduce the number of variations of colors and patterns on the page and bring a little bit more

328
00:33:19,880 --> 00:33:28,520
harmony to the page. Like, we just want harmony. And that's why we reduced one of the color

329
00:33:28,520 --> 00:33:33,160
elements on the page. We could have reduced other color elements. Yeah, it's just one color element.

330
00:33:33,160 --> 00:33:39,480
Like, what's the big deal? So some of you may be thinking, like, Evan, if you're so mad about this,

331
00:33:39,480 --> 00:33:45,640
you know, why don't you just change your default search? Like, you typed it into Safari, so clearly

332
00:33:45,640 --> 00:33:52,040
that's the default. Well, another thing I learned when I was looking into this is Google paid $1

333
00:33:52,040 --> 00:33:56,840
billion to Apple to keep the search bar on iPhones. Furthermore, these sums called traffic

334
00:33:56,840 --> 00:34:05,480
acquisition costs rose to $5.5 billion or 23% of ad revenue. So we're in a situation where

335
00:34:07,960 --> 00:34:14,360
a choice architecture has been created, you know, the circumstances have been modified such that,

336
00:34:14,360 --> 00:34:19,960
you know, well, I don't mind searching in this way, or I could scroll down below these ads,

337
00:34:19,960 --> 00:34:25,640
but I don't really want to. So the circumstances have been created such that custom is modified.

338
00:34:25,640 --> 00:34:29,960
And if we wanted to mess with this, like, it's going to cost a lot of money, right? The fact that I,

339
00:34:29,960 --> 00:34:33,800
that DuckDuckDog exists doesn't mean that they can compete with these kinds of

340
00:34:34,360 --> 00:34:44,760
numbers. So I think this whole intellectual lineage leads to something that we see in online

341
00:34:45,320 --> 00:34:51,240
communities a lot, which is like this, things are viral by design. So when Bernays starts an

342
00:34:51,240 --> 00:34:55,880
advertising campaign, all these stories he tells, he always starts with a psychological hook. So

343
00:34:55,880 --> 00:35:01,160
you might observe that people process emotions by sharing them with others. So that might look like,

344
00:35:01,240 --> 00:35:06,280
you know, someone's going about their day, it's fine. And someone shows up and they're like,

345
00:35:06,280 --> 00:35:11,000
that work you did last week, it was terrible. Like, it's not going to work out. It was really not

346
00:35:11,000 --> 00:35:17,880
carefully considered. And that person feels sad. And that person goes, the person yelling at them

347
00:35:17,880 --> 00:35:24,520
goes away. So they might mope around for a day or however long. And hopefully they run into a friend

348
00:35:24,520 --> 00:35:28,840
who hears this story and they're like, oh man, that sucks. But, you know, I don't think that was

349
00:35:28,840 --> 00:35:34,760
really a fair assessment. And through talking it through, the person can sort of deal with that

350
00:35:34,760 --> 00:35:39,960
and move through. Another interaction that might be possible is you're going about your day and

351
00:35:39,960 --> 00:35:44,120
someone says, did you hear about that terrible thing that's happening over there? And you say,

352
00:35:44,120 --> 00:35:48,680
whoa, that's terrible. And then you see a friend and you say, hey, did you hear about that? And

353
00:35:48,680 --> 00:35:52,280
they're like, oh, that's terrible. And they see some of their friends and they're like, man, that

354
00:35:52,360 --> 00:36:04,120
thing over there is really bad. Viral. So when we're choosing what kind of messages we want to

355
00:36:04,120 --> 00:36:11,560
put into society to control it, as the cyberneticist might say, this one has a very interesting

356
00:36:11,560 --> 00:36:16,360
pattern. So when you get something that has a viral reaction, that's something that has more

357
00:36:16,360 --> 00:36:23,480
engagement. And a lot of people who are running Silicon Valley companies in an idealistic way,

358
00:36:23,480 --> 00:36:28,200
you know, they want to make the world better through tools, they're put in this choice architecture

359
00:36:28,200 --> 00:36:31,960
where it's like you have, you know, these investors and you don't want to disappoint them. There's

360
00:36:31,960 --> 00:36:36,600
all these people who work at your company, you don't want to lay them off. So do you want hashtag

361
00:36:36,600 --> 00:36:44,200
disappointing Q3? Or do you want the viral one? So once you have this psychological hook, you can

362
00:36:44,200 --> 00:36:49,880
start designing ways to make it work better. So the ones I sort of noticed have been to mix

363
00:36:49,880 --> 00:36:55,080
extremes. So if we come back to our priorities graph of different people, these people don't

364
00:36:55,080 --> 00:36:59,320
necessarily congregate in the same place. But what we really want to happen is for the most

365
00:36:59,320 --> 00:37:06,760
extreme people to yell at each other as aggressively as possible. And so one way to do that might be

366
00:37:06,760 --> 00:37:10,200
to put all the different programming communities in one place.

367
00:37:14,680 --> 00:37:21,160
So if you look at sort of different online discussion forums, I think the degree to which

368
00:37:21,160 --> 00:37:25,960
different communities collide will predict the amount of conflict that you see there. So on

369
00:37:25,960 --> 00:37:32,680
Hacker News, I find that the sort of most difficult and most combative place. And then on the R

370
00:37:32,680 --> 00:37:37,880
programming, you'll see more of that. And on the subreddits for individual languages, you'll see

371
00:37:37,880 --> 00:37:43,960
less on places that are just community places that don't have accounts that are shared between

372
00:37:43,960 --> 00:37:52,040
you'll see less. So another approach is to decontextualize the person. So instead of two people

373
00:37:52,040 --> 00:37:58,120
talking, you have Tango dango talking to Foxtrot. And what's interesting here is like when they

374
00:37:58,120 --> 00:38:02,440
saw each other's faces, they might be able to say, oh, this person isn't trying to be malicious.

375
00:38:02,440 --> 00:38:08,200
They feel this way. But when it's Tango dango, it could literally be Hitler. It could.

376
00:38:10,520 --> 00:38:15,320
You don't know. He's just in Argentina being like, infix operators are stupid.

377
00:38:17,720 --> 00:38:25,640
Unclean. So another way to decontextualize things is to limit the amount of characters

378
00:38:25,640 --> 00:38:30,600
that are available to people. Another way is to limit the kind of feedback that's available.

379
00:38:30,600 --> 00:38:37,320
So instead of saying, hey, that was pretty hurtful, you say down. Like a lot of nuance is lost in

380
00:38:37,320 --> 00:38:44,600
that sort of thing. So when I take a step back on all this, I think there's a big conflict here

381
00:38:44,600 --> 00:38:51,320
where there's very powerful incentives for our interactions to go really poorly. And I don't

382
00:38:51,320 --> 00:38:57,320
think the intellectual issue of freedom really is well set up to protect against that. Like if

383
00:38:57,320 --> 00:39:02,520
you are living in a choice architecture that predictably alters people's behavior, yeah, you

384
00:39:02,520 --> 00:39:07,240
were given a free choice, but you happen to choose a different thing 30% of the time.

385
00:39:08,920 --> 00:39:14,360
So this got me interested in a different pathway. So I ask a lot, why don't I have any of these

386
00:39:14,360 --> 00:39:20,120
problems in person? So like at work or at conferences or at meetups or on the street,

387
00:39:22,520 --> 00:39:28,040
all these places are for something. So when a place is for something, you can ask what's

388
00:39:28,040 --> 00:39:33,240
inappropriate behavior. If I'm at work and a discussion keeps going and going, at some point

389
00:39:33,240 --> 00:39:37,160
it's like we have to stop having the discussion and make a choice. We can't be in discussion forever.

390
00:39:37,160 --> 00:39:45,480
It's clear because of work is for work. And at a conference, it's against the norms to jump

391
00:39:45,480 --> 00:39:50,440
up on a stage and being like, that's wrong. The idea is like you carefully select speakers who

392
00:39:50,440 --> 00:39:56,120
might have something interesting to say to folks and try to efficiently and in a nice way present

393
00:39:56,120 --> 00:40:01,400
that. At a meetup, if someone's being really aggressive, they're always like, oh, I need to go

394
00:40:01,400 --> 00:40:07,400
to the bathroom. There's ways out of those kinds of conflicts or and you're talking face to face

395
00:40:07,400 --> 00:40:12,200
on the street, you know, someone's like standing outside Starbucks and being like, I had better

396
00:40:12,200 --> 00:40:23,240
coffee one time. It's like that's weird. That's weird behavior. And then and the croissants could

397
00:40:23,240 --> 00:40:33,800
be done better. I don't know how, but they could. This isn't it. So this idea of like a place being

398
00:40:33,800 --> 00:40:39,160
for something I think is really important. And so I had this idea for intentional communication.

399
00:40:39,160 --> 00:40:43,160
And so the idea is that instead of just having a blank box you write into, you first choose

400
00:40:43,160 --> 00:40:48,440
some intent. My intent is to learn, let's say. And I'll get prompted to say, okay, what's your

401
00:40:48,440 --> 00:40:52,520
background? Maybe they started using Elm and they've been doing Java for a long time. And the

402
00:40:52,520 --> 00:40:57,960
question is, why do I have to explicitly cast between int and float when doing math? And then I

403
00:40:57,960 --> 00:41:03,960
can submit the post. So when that shows up, this question might have been read in a combative way.

404
00:41:03,960 --> 00:41:09,960
Like, why do I have to explicitly cast between int and float when doing math? Or like, why do I have

405
00:41:09,960 --> 00:41:15,240
to explicitly cast between? Like, both of those are valid interpretations of this text. But when

406
00:41:15,240 --> 00:41:19,640
you give some background, it's like, okay, I see where this person is coming from. I can see why

407
00:41:19,640 --> 00:41:24,120
they'd be frustrated with that. And it's really harder to read it in a malicious way. So instead

408
00:41:24,120 --> 00:41:29,160
of just having a reply button, maybe you can say, okay, I can either ask for clarification or I can

409
00:41:29,160 --> 00:41:36,360
give an answer. And so if I say, I'm going to give an answer, again, we might have this structured way

410
00:41:36,360 --> 00:41:44,520
of replying. Restate the question, answer it, and then we can say, hey, we value citations. We value

411
00:41:44,520 --> 00:41:48,680
like people backing up with evidence or experience. So maybe they give a citation and they can post

412
00:41:48,680 --> 00:41:54,760
it. And so again, you get the answer. And the person asking says, oh, that's not the question I was

413
00:41:54,760 --> 00:42:02,600
asking, actually. So that can really help clarify and make this process more efficient. So again,

414
00:42:02,600 --> 00:42:08,760
instead of reply, maybe you say, ask for clarification or thank the person. And so there's a couple

415
00:42:08,760 --> 00:42:13,560
of things you can add to this I think that would help. So one is the idea of conversation flows.

416
00:42:13,560 --> 00:42:17,800
So we saw someone wants to learn, you can clarify or answer. If you choose to answer,

417
00:42:17,800 --> 00:42:24,200
they can ask a clarifying question or thank you. Now if they clarify, maybe you give another answer.

418
00:42:25,240 --> 00:42:30,520
But in the other pathways, say, hey, maybe can you give me an example to clarify your question?

419
00:42:31,160 --> 00:42:37,160
They can restate it and then you can answer. So the point here is just that yell angrily isn't one

420
00:42:37,240 --> 00:42:45,320
of the states in the discussion flow. It's unreachable. So this isn't to say that all discussion

421
00:42:45,320 --> 00:42:49,400
should be this way. Because there are places where you want self-expression. You say, hey,

422
00:42:49,400 --> 00:42:52,360
I had a really nice time at the park. Here's a photo of it. And someone says, oh, that's great.

423
00:42:52,360 --> 00:42:57,480
That reminds me of last week we were clicking something really nice. And so you can have these

424
00:42:57,480 --> 00:43:02,760
cycles. But when someone's doing self-expression, you say, man, that's really like a terrible thing

425
00:43:02,760 --> 00:43:07,160
to say. Maybe you could have a conversation flow that says, hey, I want to learn about that

426
00:43:07,160 --> 00:43:10,520
perspective. I don't want to tell you it's wrong. I just want to know why you feel that way.

427
00:43:11,640 --> 00:43:17,000
So what might be interesting here is the discussion flows will be different depending

428
00:43:17,000 --> 00:43:20,520
on the goals of the community. If it's about learning, it might be one way. If it's about

429
00:43:20,520 --> 00:43:25,640
self-expression, it might be another way. And maybe you want safety valves to jump around.

430
00:43:25,640 --> 00:43:29,640
So another thing that I think is interesting is say discussion is happening and it's getting

431
00:43:29,640 --> 00:43:34,040
really out of control. And it's like, hey, this is nice. And then by the end, they're like,

432
00:43:34,600 --> 00:43:42,280
your mom is a bad person. And she threw a bicycle on the ground. I don't know.

433
00:43:44,040 --> 00:43:48,600
So this is where you don't really want things to go. So one thing that might be interesting is

434
00:43:48,600 --> 00:43:53,080
when you see someone about to reply, say, hey, what would be a successful conclusion to this

435
00:43:53,080 --> 00:43:58,840
interaction? Or would it be easier to chat for five minutes? Is there somewhere to take this

436
00:43:58,840 --> 00:44:03,480
that's going to be more constructive? Like, what do you want to get out of this? Another thing that

437
00:44:03,480 --> 00:44:08,920
might be really valuable. So we talked about private feed or sorry, upvote, downvote stuff.

438
00:44:09,560 --> 00:44:13,400
When someone gets downvoted, they say, they think to themselves, these people who just can't take

439
00:44:13,400 --> 00:44:19,320
it, you know, I outsmarted them and they can't accept that. But the people might actually be

440
00:44:19,320 --> 00:44:23,800
saying, man, that was pretty rude and uncalled for. And so that feedback is not reaching that person

441
00:44:23,800 --> 00:44:28,520
and they feel increasingly alienated by these kinds of reactions. Furthermore, these buttons

442
00:44:28,520 --> 00:44:34,200
sort of uniquely pick out like, ah, I'm mad. Oh, I like that. So what we might be able to do instead

443
00:44:34,200 --> 00:44:39,320
is say, hey, for any post, here's a couple of things that you might notice about it. Is it off

444
00:44:39,320 --> 00:44:45,480
topic? Is it helpful? And if the goal of the place is to help professionals, maybe these are the ones

445
00:44:45,480 --> 00:44:49,320
you want to choose. But if the goal is helping beginners, maybe you want to choose scary versus

446
00:44:49,320 --> 00:44:54,920
encouraging or confusing versus clear. And if the place is about self-expression, maybe you want

447
00:44:55,000 --> 00:45:01,640
to choose funny and downer. So there's a bunch of extras. I'm running out of time, so I'm going to

448
00:45:01,640 --> 00:45:07,640
skip. But there's a lot of cool things to do here. So some people may be looking at this and thinking,

449
00:45:08,840 --> 00:45:14,760
if a planned culture necessarily meant uniform, like, hey, like, all this planning is going to ruin

450
00:45:15,400 --> 00:45:21,080
these communities. So I found this book, it says, if planned culture necessarily meant uniformity

451
00:45:21,080 --> 00:45:25,640
and regimentation, it might indeed work against further evolution. If men were very much alike,

452
00:45:25,640 --> 00:45:29,640
they would be less likely to hit upon or design new practices. And a culture which made people as

453
00:45:29,640 --> 00:45:33,400
much alike as possible might slip into a standard pattern from which there would be no escape.

454
00:45:34,120 --> 00:45:39,080
That would be bad design. But if we are looking for a variety, we should not fall back on accident.

455
00:45:39,080 --> 00:45:44,040
Many accidental cultures have been marked by uniformity and regimentation. So this is from a

456
00:45:44,040 --> 00:45:49,160
book called Beyond Freedom and Dignity by B.F. Skinner. And I think this is one of the clearest

457
00:45:49,160 --> 00:45:53,880
books that's sort of recognizing that there are these people who understand choice architectures

458
00:45:53,880 --> 00:45:59,000
and that freedom doesn't help you escape from choice architectures. Like, design of other

459
00:45:59,000 --> 00:46:05,480
choice architectures is a way to deal with that. So ultimately, I'm not here to say, like, here's

460
00:46:05,480 --> 00:46:09,240
one way that's right or the others. It's like, people are going to have different priorities

461
00:46:09,240 --> 00:46:14,360
depending on, you know, their life and their experiences. I think the point that's important

462
00:46:14,360 --> 00:46:19,560
is that there's a lot of people in open source communities who are getting hurt, right? So like,

463
00:46:19,560 --> 00:46:23,960
it's hard emotionally to work on these kinds of projects. You see people around you getting hurt.

464
00:46:23,960 --> 00:46:28,840
And just saying, well, people are just expressing themselves isn't solving the question. And we

465
00:46:28,840 --> 00:46:34,600
have these very influential people controlling billions of dollars who have particular goals

466
00:46:34,600 --> 00:46:39,800
for what happens in these communities. So I hope that's an interesting way to think about

467
00:46:39,800 --> 00:46:44,040
the hard parts of open source. And I have a bunch of references if you're interested in looking

468
00:46:44,040 --> 00:46:50,200
back on things. And I hope people will explore through programming, like creating the communities

469
00:46:50,200 --> 00:46:57,000
that we kind of talked about. Maybe exploring intentional communication. And maybe it will

470
00:46:57,000 --> 00:47:01,720
be beautiful. Maybe people won't use it. Maybe people will use it. And then it will just become

471
00:47:01,720 --> 00:47:08,280
another tool for engagement. It's likely. We'll see. But thank you very much for your attention

472
00:47:08,280 --> 00:47:09,320
and consideration.

