{"text": " Hi everybody, so I'm Emory Berger. I'm going to be talking today about performance matters. This is joint work with my former PhD student, Charlie Kersinger. So I'm going to tell you a story that happened a short time ago in a valley far, far away. Not actually this one, in fact, this one. So the story is about a character who we're going to call Luke. Luke is our hero. Of course he's a hero in Silicon Valley because Luke has an idea for an app. Alright, so here's Luke's genius idea for an app. Basically you use your smartphone and you take pictures of things and then it goes and finds matches like and returns them to you and he's got a great name for it as well. It's going to call it Ogil. And he's pretty sure Ogil is going to totally disrupt image search. Nobody has told him about Google image search yet, but anyway. So he sets about building it. So he makes the prototype Ogil. So the prototype Ogil works like this. Take a picture. It gets sent off to the Ogil data center. If it is new, it gets added to the database. And then at the same time it goes and finds similar pictures and sends them back to the user. Alright, so this is pretty straightforward. You can kind of abstract out this flow. If you think of it as follows, you get a request. The request comes in. It essentially spawns two threads. One of them goes and takes the image and let's say compresses it and then saves it to a three and a half inch floppy as one does. And then at the same time it does some indexing to look up matches, right, so it does a search after doing some feature extraction and then sends it back to the user via paper airplane, which is the most effective method known to man. And then eventually these two threads join up. Alright, so this is obviously a simplification. I've alighted certain things like for example locks. So when you have locks, right, there's some database for example, you can't be modifying the database simultaneously, blah, blah, blah. But anyway, this is essentially at a high level how Ogil is going to work. So Luke goes and ships it off to the app store. It gets approved and you know users start coming and Ogil is working, right? He's disrupting image search. But then as the number of users kind of like mount, it turns out Ogil starts getting slow, right? So it takes a long time for Ogil to load, right? So this is too bad. He's not happy about this. He's not really sure what to do. So, but before we get into that, let me talk about another version of Luke. This is also Luke. This is the cool hand Luke, if you will, with his glasses. Alright, but really this is Luke in the 80s. Alright, so he's got the 80s glasses to match and of course those of you old enough to know or who have watched TV know that there were no smartphones in the 80s. There were instead computers with big CRTs. So this is what Ogil looked like in the 80s. So you'd have some ASCII art and you want to go out and it would connect via some modem, let's say, search the database. This is version 1.0 of Ogil 84. And it comes up with its matches and this is your user interface. So I hope you liked it. Alright, now of course back in the day, 1984, computers were really slow, right? Really slow. And so actually this Luke today who was like Ogil is too slow, well things were really bad in 1984, right? But it turns out things were also kind of better in a way. So performance used to be really easy. So I'm sure all of you have seen this kind of graph. What I'm going to show you is on the left, the number of transistors, as years progress on the x-axis, if they go up, you'll notice it is a log scale. And on the right you have clock speed, which roughly corresponds to computing performance. And so basically for years and years and years, we had this situation where the number of transistors were increasing roughly at doubling every 18 months. This is famously Moore's law. And this generally had the effect of allowing clock speed to increase. And so you had smaller features. This meant that your programs would actually run faster. And in fact, you could just literally wait, right? If you buy new hardware, in fact to the upgrade cycle every year or so, like if you bought a new computer, everything would run much faster. Now things were slow, right? So don't be so excited about how great it was back then. It was bad. 33 megahertz to 66 was awesome back then and kind of terrible today. But it did change the landscape of what it meant to be somebody who works on performance improvement because this is what you would do. So there really, in a way, was no sense trying to squeeze out any performance out of something when it was going to double in 18 months, right? The chance that you in 18 months were going to squeeze out double the performance pretty slender. So you might as well just sip mojitos on the beach, all right? I can tell you that this was, well, maybe the mojitos and beach part, notwithstanding, this was actually kind of a strategy that was recommended for high performance computing. People would literally say, we're just going to upgrade the computers next year or in two years. So focus on something else. Don't focus on performance, right? Now, unfortunately, that's not the current state of affairs, right? So today, as all of you know, when you upgrade a computer, it does not get faster anymore. Maybe it gets lighter. Maybe it has improved battery life. Maybe not. But basically what happened is eventually Moore's law kind of ran out of steam. The reason that it did is not actually true that Moore's law ran out of steam. This is technically a problem called denard scaling. And so denard scaling ran out. And so right now, we're in a situation where basically, if we clock things up, we no longer can dissipate the heat on chips effectively enough to make them run faster. So transistor counts actually are still increasing, which is why we have multiple cores. So we have a bunch of transistors. What are we going to do with them? Let's just stamp out more of the same thing, right? So that's great. But it's also not super awesome because if you have a program that is not parallel, it's not going to run any faster. So everything now has multicore. Your phone has multicore. But it turns out it's still a problem. So these are actual screenshots from app store updates. Every app store update practically is about performance or bug fixes. And so here's one that says under the hood updates for better performance, as opposed to the over the hood updates. Okay. Here's bug fixes and performance improvements, bug fixes and important performance improvements. And then this one, I like love this one a lot. So they're App Engine calibrations because it sounds cooler. So they've calibrated the App Engine. All right. Okay. So why does this keep happening? Why is it like every update is like, oh, God, we got to improve the performance. We got to improve the performance. Why is this so hard? Why didn't they get it right the first time? And it turns out, unlike bugs, so code is always buggy, right? And it's hard to debug programs, but it's like this thing produced the wrong result. That's pretty clear. But if you have something and it just runs slower, you don't really know where or what to change, right? So it can be really complicated. So what I'm going to talk about are two things in this talk. One is performance analysis. So I'm going to explain how to do performance analysis, right? It turns out that the thing that we, including myself, have often done is not really very rigorous. And you can draw the wrong conclusions. And then I'm going to talk about a new approach for performance profiling that will show you how to do it better. All right? So first, I'm going to talk about performance analysis. So here's Luke. Luke has his code. And Luke is like, it's too slow. Ogil is too slow. What am I going to do? And so Luke has an idea. And Luke's idea involves some sort of new, you know, I'm going to do this first, and I'm going to change this code. I'm going to change this function, blah, blah, blah. I make a bunch of changes, right? And eventually I end up with A prime, all right? The new version of A. And so now I want to know, did it work, right? So I've made this change. I thought it would make things faster. So I go and I take my code, and I have some set of inputs, some benchmarks, let's say. And I go and I say, run A. And A takes 90 seconds, which is clearly too long for your App Store thing to run. But anyway, notwithstanding that, let's say there's something that takes 90 seconds in his test, right? And then he runs A prime. 87.5 seconds. Fantastic. Success, right? 2.8% faster, all right? Time for Mojito, okay? So this is great, right? And clearly, you know, what Luke went and did had a big impact, big impact, all right? The question is, like, is it really faster, right? So if you go and you plot, like, here's a bar graph with, I'm kind of giving away the game here, one execution of A prime and A. It turns out that A prime is 2.8% faster, looks good. But there's maybe a problem here. So what's the problem? Right, so there's this problem called variance, right? Like, when you run a program once, you're going to get some result. But if you run it again, maybe the result will be slightly different, and you want to account for that. Great. So now we'll run it 30 times. 30 is the magic number, by the way. So we're going to run it 30 times, and we get this graph. And so they're pretty tightly distributed, and you can see it's still 2.8% faster, right? So seems plausible, like, I think everybody here would probably be like, looks like A prime is faster. Great. So the question you have to ask yourself is, why is it faster? So you might think, well, of course, the reason is the code change, right? So I, as Luke, I'm the developer, and I go and I'm a genius, and I have my great idea, and it pays off 2.8%, right? Well, it turns out changing the code can actually lead to all sorts of knock-on effects that have nothing to do with your intended change. So it could totally be an accident. So let me explain why. So there's this wonderful paper that is, it appeared in ASPOS in 2009 by Todd Mitkiewicz and a number of other people. I highly recommend you read it. It's something like, how to do things wrong without really trying, something like that. And the conclusion is that the layout, like where the code and data end up in memory, has a pretty big impact on performance, all right? So when you go to measure something, those measurements are biased by depending where things kind of fell, right? So here are a few things that can have an impact on layout, and I'm going to talk about more. So one is link order. So if you're in cc++ land, and you have a make file, and the make file has a bunch of, this link step and has a bunch of dot-os, depending how those dot-os are arranged, you can get different performance, okay? You might think, fine, all right? Your environment variables. So when you go to execute your program, your environment variables, whether it's in cc++ or even managed languages, they somehow get copied in and everything else gets shifted. So in c and c++, this moves the stack. So this actually has an effect on layout. These two alone can lead to shifts in performance of plus or minus 40%. Okay? So that's not great. So what is happening? Like, why is this happening? This is a huge shift. This is literally larger than the impact of dash o3 over dash o0. Okay? Yes, you laugh, but as well you should. So why is a prime faster than a, right? So what is going on? Why could this happen without actually trying? So part of the problem here is that basically modern processors have become insanely complicated in their zeal to increase speed. So what do they do? So they have caches, right? Add data and instructions, get mapped to the cache. Well, it turns out for good reasons, these things are binned up into these things called sets. If they map to the same set, you can have a conflict. So if you have hot code, a lot of hot code that is mapping to the same set, then it's not going to necessarily fit in cache, and your code will run slower. By luck, you could be in a situation where when you changed a prime, you actually disrupted this conflict. And so now you have no conflict, right? These two things, one is the hot code and one maps to nothing. So no conflict. Boom, it ran faster. All right? So that sounds great. So it could be the cache, but it could also be the branch predictor, which actually, again, is based on the addresses of your branches, and if these branches collide, then you can end up with things running slower. There's also this thing called the TLB, the translation look-aside buffer, which maps virtual addresses to physical addresses. If things don't fit in the TLB because they span two pages instead of one, suddenly things become slower. There's also a branch target predictor. There's a prefetchor. There's more. All right? So this is pretty bad. So all of these things can happen. You might think, all right, link order is fine. The code thing is a little weird, but, you know, hey, it's faster, right? It's 2.8% faster. That, like, I don't care. It's all good, right? Now, it may not be faster on every machine, but it's faster today, right? So here's the problem. Like, anything you do can disrupt this. So what could happen? One more malloc changes layout, right? Like, you've shifted everything over, one more or less. If you upgrade anything in your system, this is going to change layout, right? So that's bad. Okay. So those things, all right, I'm not going to change libc, and I guess I'll never malloc again. Fine. Whatever. All right? So here's something that may be surprising. Running it in a new directory. So it turns out that your current working directory goes right into your environment variables, right? So that's weird, right? So, you know, if Vader tries to run your software, it's not going to work as fast because it's one character longer than Luke, okay? This is a real effect. This can really happen. It has actually bitten me. I had a student who wrote something. He has a long Indian last name. My username is just five letters long. It's just Emery. And he did something. He's like, oh, it doesn't run any faster. It actually runs slower. And it's like, that makes no sense at all. And eventually, we whittled it down, and it was like, if I run it as me, it's faster. Okay? That's right. All right? Changes your layout. So the solution is obvious, right? Run everything. All right. So I should add, you know, all of this is, you know, like, the whole talk is really oriented towards, I'm going to improve my performance. But everything I'm talking about today can be viewed in reverse for performance regression. Like, I made a change, and things run 2.8% slower. Oh, God, roll back. Maybe not, right? Maybe the next thing you do is going to actually undo that change, right? So basically, layout is super brittle. And like you've seen, layout biases measurement. So one of the questions that we wanted to know is, is it possible to eliminate the effect of layout? So we can actually understand the performance of things kind of without having to think about, well, one malloc less or more, or, you know, Luke versus Vader. So the answer is yes. We built a tool that we call stabilizer. So stabilizer addresses this problem that I've just explained to you. Pardon me. And it eliminates the effect of layout. So this is a way to actually measure programs where you can kind of actually know whether the regression you had is real or whether the optimization you had is real and not just an accident. So how does this work? How is this even possible? So the way that stabilizer works is that it randomizes layout. So it randomizes a lot of things. It randomizes the function addresses. It randomizes stack frame sizes. It even randomizes heap allocations. But not only does it do those things, it does it over and over again. So while your program is running, it's literally doing randomization. And this turns out to be important, and I'll show you a graph that we'll explain why. But basically, if you do this, then there's no way layout can bias your measurement because a completely random layout can't bias the results. That's just how things work. That's why we run randomized control trials. You've eliminated something as a possible cause. The only other cause that remains is whatever change you made. So let's walk through what you would do with stabilizer. So with stabilizer, again, clearly you're supposed to run your program a bunch of times. But notice what happens to the execution times. Here the execution times are no longer tightly bound around this one very small measurement. The reason for that is that when you were running that program 30 times, it was sort of like you were going to do a survey of 30 people, but you just ask one person. Because it's the same layout over and over again. So you did an experiment on 30 executions, but what you really did is you just repeated 30 on one. So the only noise that you're eliminating is the noise that comes from network demons waking up or some other random event, maybe some thermal issue in your computer, but it's not really altering layout. It's always the same layout. So here it's not, and you get these very nice bell curves. So now I'm going to ask you the question. So this is an audience poll time. Is A prime faster than A? I just want you to raise your hands if you think that A prime is faster than A. All right, great. Now keep your hands up. Don't set them down. But set them down if you change your mind. How about now? How about now? There's still a few like hardcore. So what you all are doing is what I like to refer to as eyeball statistics. And so you're kind of like, looks close to me. That's too close. Right. But it turns out this is not actually a thing. So if you, yeah, it's not really statistics when you just eyeball results. So this is a bit of a refresher for some of you, but I'm going to walk you through this and how this all fits in with stabilizer. So in actual statistics, and today I'm just going to talk about one flavor of statistics, which is called null hypothesis significance testing. There are others, notably Bayesian approaches. Happy to talk about that offline. But basically the way it works is you just assume that the things are the same. You say what is the likelihood of observing this difference by chance? All right. So it turns out that this is something that's just convenient. It's very easy to compute these probabilities for the normal distribution, which you all remember from school. These graphs are normal. Awesome. It turns out that stabilizer happens to make normal graphs or normal distributions. And I'll explain why. So how are we going to do this? We're going to run stuff with stabilizer. We're going to pick some probability below which we're like, okay, good enough. Right. So if it's only a one in 20 chance, I see this probability like the, I see this event occurring. I'll be like, okay, that's good enough for me. You could be harsher. You could say one in 100, one in 1,000. It's pretty standard to say one in 20. This is the p value of 0.05. So the idea is if there's a low enough probability, you reject the null hypothesis, the null hypothesis being that they're the same. And you conclude that the speed up is real. It's not due to the effective memory on memory layout. All right. So why re-randomization? The reason for re-randomization is that just randomizing once doesn't give you enough randomization. So this is an actual program. You can see the distribution is pretty wacky. It's very far from normal. You can't intuitively explore much of the space when you just randomize at startup as opposed to randomizing during execution. This is in fact the kind of distribution you get when you randomize all the time. And these are normal distributions. So why do I keep saying that they're normal distributions? The reason is essentially, again, going back to like freshman stats, stabilizer generates a new random layout every half second. That is to say it's a completely independent version of the program right from half second to half second to half second. It's all randomized. And it's the same program the whole time. So it's identically distributed. And then we're adding them all up. And there is this nice result, a key result of stats, which is the sum of a sufficient number. So if you run a program for long enough of independent identically distributed random variables, it's approximately normally distributed no matter what the underlying distribution was. This is the central limit theorem. So this makes execution times normally distributed, which is cool in other ways because you actually know how likely it is that you're going to see some very weird execution because you know what the distribution looks like. All right, great. So now we have this thing in hand and we're going to do something insane. We're going to see whether optimizations actually matter. And we know some of them matter. So we have a suite of benchmarks that we're going to evaluate it on. We're going to evaluate them individually and then across the whole benchmark suite. And I'll show you how we do it. So you build the benchmarks with stabilizer. Stabilizer is a plugin for LVM. If you just compile it as such, it goes and randomizes everything. But you can actually just randomize things independently if you wanted, like just code, just heap, and just stack. So now we run the benchmarks, we run them as usual. We drop them into one of my least favorite programming languages ever. And then we decide what the result is. So again, we don't ask questions like this because that's eyeball stats. Instead, we ask a question like this, pretend they're equal. How likely is it we'd observe this much of a difference? So I also have to say that you should not assume normality like almost ever in my humble opinion, unless you have very good reasons for doing so. Here we have very good reasons for doing so. So we can use really powerful statistical tests like the student's t-test. So this is the test that you used to actually measure this difference. So if the p-value, the likelihood of this event occurring, is less than some threshold, which as I mentioned before is 5%, we're going to reject the null hypothesis. That is to say, it's not because of random layout, the difference is real. Everybody's on board, I hope. So now we're going to do it. You'll be shocked to learn that dash O2 versus dash O1 makes a difference. Good. I would be weird if the result were otherwise. So you can see that there are statistically significant improvements, right on the right. There's some that are statistically significant but don't matter. And by God, there are statistically significant performance drops. So it turns out that compiler people run these same benchmarks and overfit the way that they do these optimizations, and some of them lead to layout changes. And it wasn't actually the code change. And so we can actually distill out this effect. All right, great. By and large, it looks like O2 over O1 is a win. How about O3 versus O2? Ready? It's amazing. Okay. So I actually have to change the axes so we can see a lot of these values. So I'm going to zoom in. Instead of it being 0 to 10, like the range is negative 10 to 20, I'm going to make it 0 to 1.5. Okay, so now we can see them. So they're pretty small effects, but some of them are significant and some of them are not. Again, statistically significant, 1.75% decrease in execution time. Great. A bunch of things where it's not significant and a couple decreases, but really very minor effect sizes. So what do these results mean? I mean, you can't actually look at an individual benchmark. Like there's 30 of them, right? So drawing a conclusion about all 30, you actually have to do something different. You have to collect all of them. You get a bunch of these graphs, and this is what you don't do. Like, okay, this one is slower. This one's faster. This one's faster. This is just eyeballs everywhere. Okay? I mean, they're spooky and nobody wants to see those. So again, we're going to do the same thing, but to test a bunch of things simultaneously, you do this thing, which is terribly named, called analysis of variance, and you plug it into R with this awesome incantation, and then you do, again, the same test. If the p-value is less than or equal to 5%, we reject the null hypothesis. All right? You ready? All right, here we go. Here's the p-value. So it has to be less than or equal to 5% or else we're going to conclude that dash o3 versus dash o2 is nothing. All right? So the p-value is 26.4%. That means that one in four experiments will just show a random effect, right? Just literally randomly. We do not consider this enough evidence to reject the null hypothesis. So we're, we cannot reject the null hypothesis, which is that the effect is indistinguishable from noise. All right? Okay. So this is all terrible news for people like Luke who wanted optimizations to work, and I've actually seen, I've actually seen projects. It makes, it kind of breaks your heart. Like projects I committed, like on GitHub, where it literally says dash o9, and I feel like why not dash o11? There's no, there's no dash o9 or 11. It's just kind of bottoms out. But you know, hope springs eternal. All right. So great. So what are we going to do? So what people do when they can't speed things up, right? They run a profiler. So there's these profilers, they all basically work the same way. You go and you get some result, and it says, hey, here's where my program spent its time. You get the number of calls to every function, runtime for each function, and this captures intuitively, maybe for most of us, like this is what a profiler should do, right? What do I care about? There's frequently executed code or code that runs for a long time. That's where I should be focusing my optimization efforts. All right. It seems intuitively appealing. This is the way profilers have been written since prof, back, you know, back from like, I don't know, late 60s, early 70s. So would this in fact speed up Google? So we're going to do this experiment. We're going to go and find the thing that runs for the longest amount of time and where it spends all of its time running. And so we're going to run it. And so we go and we do this. And basically, it makes the loading thing flash faster. Okay. So, well, guess what? That's frequently executed. And in fact, it's the code that runs for the longest time, right? So this is not really great, especially if Luke spent like more than a minute optimizing that code. That's a shame. All right. So basically in some profilers were developed in an era where everything was synchronous and there was a single core. All right. That's not today. Today things are asynchronous or parallel or concurrent or a mixed thereof. And profilers don't do a good job in these contexts. So we need to do better. So what would be really cool is if we could have something like this. So this is what I call a causal profile. So a causal profile tells you visually, like, if I were to speed up this component by this much on the x-axis, then the whole program will speed up by this much. All right. So this is really nice. Like, if I had this graph, I would know I could spend a little effort on the yellow search component and I'll get a big bang for my buck. Eventually, it's going to bottom out or top out at like, you know, like 70, 80%. And the red one, I could just keep going, right? Like, it gets faster and faster the more I work. And the blue one, I should never, never optimize ever. All right. It would be cool to know this, right? It's essentially like an oracle coming and telling you, this is the code you should work on, Luke. I don't know where I got that way of talking. Anyway. All right. So the question is, how would we know this? Like, how would we get this information? Like, how would we know that this change would cause this effect? Right? We can't just go and optimize the program by arbitrary amounts and test it. That kind of defeats the purpose. So we're going to do something different. We're going to run an experiment. And it requires one ingredient here, which I'll refer to as the force. So we're going to use magic. And we're going to speed things up magically. And then we're going to measure how much the effect was of speeding up each component by a certain amount on overall program execution. Okay? So we just keep doing this, right? We get more and more points, right? And then I do it for different things. It turns out if I could speed up saving things to the three and a half inch floppy, it doesn't make a difference, right? And so on. All right? Now, unfortunately, we live in the real world where there's no magic. Sorry. Well, if there was magic, to be clear, this is not what we would do, right? I mean, obviously, there are many much better things we could do. I could think of people I would like to disappear off the face of the earth, for example. But I could also disappear all the runtime off the face of the earth. Because why not? All right? So obviously, that's what we would do. So we can't do that. We have to do something else. So what we are going to do as our trick is we're going to do something that essentially takes advantage of this notion of relativity. So we're going to do a virtual speedup. And a virtual speedup speeds things up in scare quotes by slowing everything else down, right? So everything else that's running at the same time will then be slowed down. And that will allow us to get the impact of how do we sped this thing up? What would the results have been? So here, for example, if we speed up the sending of the picture results back by a certain amount, we've slowed down everything running concurrently with it. And then that gives us a result of a slowdown, which is the same thing as the result of having sped it up. So we actually can get points on this graph just by running these experiments. So I just got a point here, and I do it for everything, and I get more points. And eventually, I get a graph like this. If I speed up indexing, I'm going to get the exact same effect. Indexing is running at the same time as the compression. So I get this result, and then bang, I get these results. And again, these are all the results. Now, I draw your attention to the one weird blue thing. So the blue thing is slower, and it turns out that sometimes optimizing things makes your program run slower. And the intuition behind this is you can actually get congestion on a lock, or congestion for a shared resource like disk or network. And so speeding things up makes things worse. You would like to know this before you get started. That would be a very, very bad day for Luke that might necessitate several sequels to recover from. All right, great. All right, so let's dig into Ogil a little bit. So what do we care about in Ogil? We care about two things. We care about how long it takes between a request and a response, a.k.a. latency. Traditional profilers don't do this at all. It's just total runtime. Oh, let me get in my soapbox for one moment. Traditional profilers are about end-to-end runtime. You know how your servers are all about end-to-end runtime? Or your browser? Like, if only your browser could quit faster. So again, like, it was all about, like, here's a program. I run at a console, and it does something, and it's done, and that's all I cared about. That's not really today. So there's latency. And then the more traditional thing is throughput. Again, this is something that profilers do a bad job of measuring because they're all about end-to-end execution time. So how fast results come back is throughput. So how are we going to do this? So with our causal profiler that I'm going to explain in a minute, we're going to introduce what we call progress points. So the notion of progress points is here's a thing I want to happen faster, or here's a beginning and an end of things that I want to happen faster. So if Luke wants responses to get sent faster, higher throughput, you just mark this particular start of this component as a progress point, and every time the code runs, you go and you get another coin. And then you can do this simultaneously, many requests for many users, and all of these things are incrementing some counter. So these progress points are measuring throughput, and then you basically are going to run the experiments and see what the effect is on the rate of those progress points being executed. So one point measures throughput. Like I said, if I speed up some component, whatever it might be, what is the effect? So now, what if I care about latency? So we do the exact same thing. We set a progress point at the beginning, a progress point at the end, and then the only thing that has to happen under the covers is it has to have a counter. And the counter here measures how many things are in the system at any one time. And it turns out that there is this awesome law that holds in a wide variety of circumstances called Little's law. And so Little's law says that the latency is essentially the number, the average number of transactions in a system divided by the throughput. We already know how to measure throughput, so we just take advantage of Little's law and we can translate this into latency. All right, great. So we have built a causal profiler for Linux. It already ships with Debian and Ubuntu, so if you're using one of those systems, you can install it quite easily. So it's just cause-profiler. It's quite easy to run. So you say cause run dash dash dash and whatever your program is in its arguments and it fires it off and it starts doing performance experiments. All right, I should add it's not entirely true. You do need to place progress points. If you don't place any progress points, it will act like an ordinary profiler measuring end-to-end execution time. But if you do put in progress points, then it will actually do its magic. All right, and this is just some macro, like progress begin, progress end. All right, so let's apply this to Augell. All right, I didn't actually build Augell. Neither did Luke, but we're going to build it out of pieces like any good programmer would do. So it turns out there's this suite of parallel applications that's kind of ready-made for this task. So there's a deduplicator that does compression. There's an image comparator. And then there's a database, SQLite. That's not in Parsec, but we'll use SQLite too. All right, great. So I'm going to show you some fun things we did. This is a well-studied set of applications. People have already tried to optimize these and we have covered a bunch of surprising optimization opportunities. So here's Ferret. This is actually an older version of what our causal profile looks like. Now it runs in a web browser. And you can see that there's these lines of code and it says, boy, if you speed up these lines of code, then you're going to get performance increases. Conveniently, these lines of code happen to be located in separate chunks of Ferret. So the part that does ranking, the part that does indexing, the part that does segmentation. And why is this convenient? I'm not going to have to change any code to make this faster. The reason is that what Ferret does is it has this pipeline model and it assigns an equal number of threads to every stage in the pipeline. But it turns out this one really doesn't need that many threads. So we take away the threads and just by reassigning threads, we got a 20% speedup. So this is pretty cool because Caus actually predicted it perfectly. So we increased ranking, for example, from 16 to 22 threads. That's a 27% increase in throughput. And on the graph, that says that that would translate to a 21% overall improvement. And that's what we got. So Caus actually works. Good. So we were pretty happy with this. We then are going to move on to Ddupe. So Ddupe is pretty hilarious. So here's Ddupe in action. I have two pictures, Grumpy Cat 1 and Grumpy Cat Meme. And so now what do I do to deduplicate these things? You can see that there's chunks that are the same. So you carve out the chunks that are the same and you separate them out into individual pieces. And then an image is now represented by the bits and pieces that make up the image. So here Grumpy Cat 1 is this piece and Fun Is Awful is this piece. And you saved a lot of memory. So that's what Ddupe does. So it does this compression via deduplication and it uses a hash function. So it throws everything through a hash table. Great. So it's a pretty standard hash table. You just have some hash table. It's an array. It's a bunch of bins. You get a bin number and then you go and you start adding stuff to that bin into the bucket. So this all seems straightforward. You hope that it would do something like this. The hash table is accessed concurrently by a bunch of threads, but they're not idiots. There's not one big lock. It's just all locks, which is naive, but it's fine. But surprisingly, cause says that the loop that accesses this list is important. Now, if you know anything about hash tables, you know that things generally end up balanced, right? And it's weird that you have this sort of situation. So we thought, all right, well, let's just make more, more hash buckets, right? But we made a thousand of them. We really should have made a million, cause, you know, a million. But anyway. So you would think this would lead to fewer collisions, but it had no effect, right? So what else could be causing the collisions? Any guesses? The hash function, exactly. Like this is one of those when all other possibilities have been exhausted, right? You pick the weirdest one. That's not an exact quote. But anyway, well, you're like, how can the hash function be broken? Like we've been using hash functions since before Canuth wrote about them. Well, turns out people like to roll their own, cause it's fun. And so we did a histogram of the number of items per bucket. So again, I told you there's a thousand buckets. This is the histogram. Yeah. Hilariously, what they did is they used the pixels that were taken from the image and they sum number of pixels and then added them. But that's actually the central limit theorem in action, right? They're random, right? They're independent, right? And you've summed them together. And so they actually formed the normal distribution. That's not the distribution you want for a hash table. You would like a uniform distribution. So literally, we changed one character. We changed the plus to XOR. So this. And we got this. So, okay. I'll take the applause, but I mean, it was only a 9% speedup. But all right. Nonetheless, it was one character. So I think it's the biggest bang for buck ever recorded in optimization effort. So what did it predict? It turned out we can also measure the accuracy of the prediction. So we knew that the blocks per bucket went from 76-ish to 2. That's a 96% traversal speedup. And again, going back to the causal graph, it predicted a 9% speedup, which is what we got, right? So it's working. All right. So finally, I'm going to talk about SQLite. So I have no time left. But SQLite is pretty awesome. It's widely used, as you all know. But it has this weird thing where it has a kind of strange virtual table that they set up at compile time. And so whenever you actually indirect through a config to execute a function like pthreadmutexunlock. So you would think, all right, why are you telling me about this? Well, everything looks like this. This is an indirect call. Could be a direct call. That would be faster. But an indirect call is not that slow. But it's almost the same cost as pthreadmutexunlock, which means that you just doubled the length of all of your critical sections. So that's not great. So in fact, when you go, so cause will highlight all of these lines and say, you should definitely optimize these. So we undid all of the actual indirect stuff and just made it so that at compile time, you change SQLite unlock to something so it doesn't do the indirect. And it sped things up by 25%. If you look at a traditional profiler, by the way, those things are like, this takes 0.0001% of time. You would never consider actually trying to optimize that code. So we did it for a bunch of programs. We got some crazy speed ups. My favorite is we got a 68% speed up by replacing a custom barrier with a standard barrier. Again, people should stop doing things at home. So anyway, so I'm going to conclude. So you can take a picture of this to jump to work from our lab, which is plasmaumass.org. I talked today about sound performance analysis and effective performance profiling. Everybody should go use the cause. All right. Thanks for your attention.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.8, "text": " Hi everybody, so I'm Emory Berger. I'm going to be talking today about performance matters.", "tokens": [50364, 2421, 2201, 11, 370, 286, 478, 3968, 827, 5637, 1321, 13, 286, 478, 516, 281, 312, 1417, 965, 466, 3389, 7001, 13, 50904], "temperature": 0.0, "avg_logprob": -0.24003227442911226, "compression_ratio": 1.4263157894736842, "no_speech_prob": 0.0824408084154129}, {"id": 1, "seek": 0, "start": 10.8, "end": 15.48, "text": " This is joint work with my former PhD student, Charlie Kersinger. So I'm going to tell you", "tokens": [50904, 639, 307, 7225, 589, 365, 452, 5819, 14476, 3107, 11, 13754, 591, 433, 6911, 13, 407, 286, 478, 516, 281, 980, 291, 51138], "temperature": 0.0, "avg_logprob": -0.24003227442911226, "compression_ratio": 1.4263157894736842, "no_speech_prob": 0.0824408084154129}, {"id": 2, "seek": 0, "start": 15.48, "end": 23.400000000000002, "text": " a story that happened a short time ago in a valley far, far away. Not actually this one,", "tokens": [51138, 257, 1657, 300, 2011, 257, 2099, 565, 2057, 294, 257, 17636, 1400, 11, 1400, 1314, 13, 1726, 767, 341, 472, 11, 51534], "temperature": 0.0, "avg_logprob": -0.24003227442911226, "compression_ratio": 1.4263157894736842, "no_speech_prob": 0.0824408084154129}, {"id": 3, "seek": 2340, "start": 23.4, "end": 31.72, "text": " in fact, this one. So the story is about a character who we're going to call Luke. Luke is", "tokens": [50364, 294, 1186, 11, 341, 472, 13, 407, 264, 1657, 307, 466, 257, 2517, 567, 321, 434, 516, 281, 818, 13044, 13, 13044, 307, 50780], "temperature": 0.0, "avg_logprob": -0.18740128497688138, "compression_ratio": 1.6297872340425532, "no_speech_prob": 0.09527799487113953}, {"id": 4, "seek": 2340, "start": 31.72, "end": 37.44, "text": " our hero. Of course he's a hero in Silicon Valley because Luke has an idea for an app. Alright,", "tokens": [50780, 527, 5316, 13, 2720, 1164, 415, 311, 257, 5316, 294, 25351, 10666, 570, 13044, 575, 364, 1558, 337, 364, 724, 13, 2798, 11, 51066], "temperature": 0.0, "avg_logprob": -0.18740128497688138, "compression_ratio": 1.6297872340425532, "no_speech_prob": 0.09527799487113953}, {"id": 5, "seek": 2340, "start": 37.44, "end": 43.44, "text": " so here's Luke's genius idea for an app. Basically you use your smartphone and you take pictures", "tokens": [51066, 370, 510, 311, 13044, 311, 14017, 1558, 337, 364, 724, 13, 8537, 291, 764, 428, 13307, 293, 291, 747, 5242, 51366], "temperature": 0.0, "avg_logprob": -0.18740128497688138, "compression_ratio": 1.6297872340425532, "no_speech_prob": 0.09527799487113953}, {"id": 6, "seek": 2340, "start": 43.44, "end": 49.68, "text": " of things and then it goes and finds matches like and returns them to you and he's got a great name", "tokens": [51366, 295, 721, 293, 550, 309, 1709, 293, 10704, 10676, 411, 293, 11247, 552, 281, 291, 293, 415, 311, 658, 257, 869, 1315, 51678], "temperature": 0.0, "avg_logprob": -0.18740128497688138, "compression_ratio": 1.6297872340425532, "no_speech_prob": 0.09527799487113953}, {"id": 7, "seek": 4968, "start": 49.72, "end": 55.0, "text": " for it as well. It's going to call it Ogil. And he's pretty sure Ogil is going to totally", "tokens": [50366, 337, 309, 382, 731, 13, 467, 311, 516, 281, 818, 309, 14883, 388, 13, 400, 415, 311, 1238, 988, 14883, 388, 307, 516, 281, 3879, 50630], "temperature": 0.0, "avg_logprob": -0.20226737169119027, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0311299916356802}, {"id": 8, "seek": 4968, "start": 55.0, "end": 60.84, "text": " disrupt image search. Nobody has told him about Google image search yet, but anyway. So he sets", "tokens": [50630, 14124, 3256, 3164, 13, 9297, 575, 1907, 796, 466, 3329, 3256, 3164, 1939, 11, 457, 4033, 13, 407, 415, 6352, 50922], "temperature": 0.0, "avg_logprob": -0.20226737169119027, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0311299916356802}, {"id": 9, "seek": 4968, "start": 60.84, "end": 66.6, "text": " about building it. So he makes the prototype Ogil. So the prototype Ogil works like this. Take a", "tokens": [50922, 466, 2390, 309, 13, 407, 415, 1669, 264, 19475, 14883, 388, 13, 407, 264, 19475, 14883, 388, 1985, 411, 341, 13, 3664, 257, 51210], "temperature": 0.0, "avg_logprob": -0.20226737169119027, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0311299916356802}, {"id": 10, "seek": 4968, "start": 66.6, "end": 74.44, "text": " picture. It gets sent off to the Ogil data center. If it is new, it gets added to the database. And", "tokens": [51210, 3036, 13, 467, 2170, 2279, 766, 281, 264, 14883, 388, 1412, 3056, 13, 759, 309, 307, 777, 11, 309, 2170, 3869, 281, 264, 8149, 13, 400, 51602], "temperature": 0.0, "avg_logprob": -0.20226737169119027, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0311299916356802}, {"id": 11, "seek": 7444, "start": 74.44, "end": 79.8, "text": " then at the same time it goes and finds similar pictures and sends them back to the user. Alright,", "tokens": [50364, 550, 412, 264, 912, 565, 309, 1709, 293, 10704, 2531, 5242, 293, 14790, 552, 646, 281, 264, 4195, 13, 2798, 11, 50632], "temperature": 0.0, "avg_logprob": -0.11756899298691167, "compression_ratio": 1.7137809187279152, "no_speech_prob": 0.004904869943857193}, {"id": 12, "seek": 7444, "start": 79.8, "end": 84.96, "text": " so this is pretty straightforward. You can kind of abstract out this flow. If you think of it as", "tokens": [50632, 370, 341, 307, 1238, 15325, 13, 509, 393, 733, 295, 12649, 484, 341, 3095, 13, 759, 291, 519, 295, 309, 382, 50890], "temperature": 0.0, "avg_logprob": -0.11756899298691167, "compression_ratio": 1.7137809187279152, "no_speech_prob": 0.004904869943857193}, {"id": 13, "seek": 7444, "start": 84.96, "end": 90.24, "text": " follows, you get a request. The request comes in. It essentially spawns two threads. One of them", "tokens": [50890, 10002, 11, 291, 483, 257, 5308, 13, 440, 5308, 1487, 294, 13, 467, 4476, 17088, 82, 732, 19314, 13, 1485, 295, 552, 51154], "temperature": 0.0, "avg_logprob": -0.11756899298691167, "compression_ratio": 1.7137809187279152, "no_speech_prob": 0.004904869943857193}, {"id": 14, "seek": 7444, "start": 90.24, "end": 95.75999999999999, "text": " goes and takes the image and let's say compresses it and then saves it to a three and a half inch", "tokens": [51154, 1709, 293, 2516, 264, 3256, 293, 718, 311, 584, 14778, 279, 309, 293, 550, 19155, 309, 281, 257, 1045, 293, 257, 1922, 7227, 51430], "temperature": 0.0, "avg_logprob": -0.11756899298691167, "compression_ratio": 1.7137809187279152, "no_speech_prob": 0.004904869943857193}, {"id": 15, "seek": 7444, "start": 95.75999999999999, "end": 103.36, "text": " floppy as one does. And then at the same time it does some indexing to look up matches, right,", "tokens": [51430, 25343, 8200, 382, 472, 775, 13, 400, 550, 412, 264, 912, 565, 309, 775, 512, 8186, 278, 281, 574, 493, 10676, 11, 558, 11, 51810], "temperature": 0.0, "avg_logprob": -0.11756899298691167, "compression_ratio": 1.7137809187279152, "no_speech_prob": 0.004904869943857193}, {"id": 16, "seek": 10336, "start": 103.4, "end": 109.2, "text": " so it does a search after doing some feature extraction and then sends it back to the user via", "tokens": [50366, 370, 309, 775, 257, 3164, 934, 884, 512, 4111, 30197, 293, 550, 14790, 309, 646, 281, 264, 4195, 5766, 50656], "temperature": 0.0, "avg_logprob": -0.15167896304510337, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.0002453637425787747}, {"id": 17, "seek": 10336, "start": 109.2, "end": 114.36, "text": " paper airplane, which is the most effective method known to man. And then eventually these two threads", "tokens": [50656, 3035, 17130, 11, 597, 307, 264, 881, 4942, 3170, 2570, 281, 587, 13, 400, 550, 4728, 613, 732, 19314, 50914], "temperature": 0.0, "avg_logprob": -0.15167896304510337, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.0002453637425787747}, {"id": 18, "seek": 10336, "start": 114.36, "end": 119.24, "text": " join up. Alright, so this is obviously a simplification. I've alighted certain things like", "tokens": [50914, 3917, 493, 13, 2798, 11, 370, 341, 307, 2745, 257, 6883, 3774, 13, 286, 600, 419, 397, 292, 1629, 721, 411, 51158], "temperature": 0.0, "avg_logprob": -0.15167896304510337, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.0002453637425787747}, {"id": 19, "seek": 10336, "start": 119.24, "end": 125.32, "text": " for example locks. So when you have locks, right, there's some database for example, you can't be", "tokens": [51158, 337, 1365, 20703, 13, 407, 562, 291, 362, 20703, 11, 558, 11, 456, 311, 512, 8149, 337, 1365, 11, 291, 393, 380, 312, 51462], "temperature": 0.0, "avg_logprob": -0.15167896304510337, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.0002453637425787747}, {"id": 20, "seek": 10336, "start": 125.32, "end": 129.36, "text": " modifying the database simultaneously, blah, blah, blah. But anyway, this is essentially at a", "tokens": [51462, 42626, 264, 8149, 16561, 11, 12288, 11, 12288, 11, 12288, 13, 583, 4033, 11, 341, 307, 4476, 412, 257, 51664], "temperature": 0.0, "avg_logprob": -0.15167896304510337, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.0002453637425787747}, {"id": 21, "seek": 12936, "start": 129.36, "end": 136.32000000000002, "text": " high level how Ogil is going to work. So Luke goes and ships it off to the app store. It gets", "tokens": [50364, 1090, 1496, 577, 14883, 388, 307, 516, 281, 589, 13, 407, 13044, 1709, 293, 11434, 309, 766, 281, 264, 724, 3531, 13, 467, 2170, 50712], "temperature": 0.0, "avg_logprob": -0.16542458534240723, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0004728310159407556}, {"id": 22, "seek": 12936, "start": 136.32000000000002, "end": 141.52, "text": " approved and you know users start coming and Ogil is working, right? He's disrupting image search.", "tokens": [50712, 10826, 293, 291, 458, 5022, 722, 1348, 293, 14883, 388, 307, 1364, 11, 558, 30, 634, 311, 14124, 278, 3256, 3164, 13, 50972], "temperature": 0.0, "avg_logprob": -0.16542458534240723, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0004728310159407556}, {"id": 23, "seek": 12936, "start": 141.52, "end": 148.84, "text": " But then as the number of users kind of like mount, it turns out Ogil starts getting slow,", "tokens": [50972, 583, 550, 382, 264, 1230, 295, 5022, 733, 295, 411, 3746, 11, 309, 4523, 484, 14883, 388, 3719, 1242, 2964, 11, 51338], "temperature": 0.0, "avg_logprob": -0.16542458534240723, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0004728310159407556}, {"id": 24, "seek": 12936, "start": 148.84, "end": 154.44000000000003, "text": " right? So it takes a long time for Ogil to load, right? So this is too bad. He's not happy about", "tokens": [51338, 558, 30, 407, 309, 2516, 257, 938, 565, 337, 14883, 388, 281, 3677, 11, 558, 30, 407, 341, 307, 886, 1578, 13, 634, 311, 406, 2055, 466, 51618], "temperature": 0.0, "avg_logprob": -0.16542458534240723, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0004728310159407556}, {"id": 25, "seek": 15444, "start": 154.48, "end": 160.6, "text": " this. He's not really sure what to do. So, but before we get into that, let me talk about another", "tokens": [50366, 341, 13, 634, 311, 406, 534, 988, 437, 281, 360, 13, 407, 11, 457, 949, 321, 483, 666, 300, 11, 718, 385, 751, 466, 1071, 50672], "temperature": 0.0, "avg_logprob": -0.13950558712607936, "compression_ratio": 1.7464285714285714, "no_speech_prob": 0.07157952338457108}, {"id": 26, "seek": 15444, "start": 160.6, "end": 165.4, "text": " version of Luke. This is also Luke. This is the cool hand Luke, if you will, with his glasses.", "tokens": [50672, 3037, 295, 13044, 13, 639, 307, 611, 13044, 13, 639, 307, 264, 1627, 1011, 13044, 11, 498, 291, 486, 11, 365, 702, 10812, 13, 50912], "temperature": 0.0, "avg_logprob": -0.13950558712607936, "compression_ratio": 1.7464285714285714, "no_speech_prob": 0.07157952338457108}, {"id": 27, "seek": 15444, "start": 165.4, "end": 170.96, "text": " Alright, but really this is Luke in the 80s. Alright, so he's got the 80s glasses to match and", "tokens": [50912, 2798, 11, 457, 534, 341, 307, 13044, 294, 264, 4688, 82, 13, 2798, 11, 370, 415, 311, 658, 264, 4688, 82, 10812, 281, 2995, 293, 51190], "temperature": 0.0, "avg_logprob": -0.13950558712607936, "compression_ratio": 1.7464285714285714, "no_speech_prob": 0.07157952338457108}, {"id": 28, "seek": 15444, "start": 170.96, "end": 176.28, "text": " of course those of you old enough to know or who have watched TV know that there were no smartphones", "tokens": [51190, 295, 1164, 729, 295, 291, 1331, 1547, 281, 458, 420, 567, 362, 6337, 3558, 458, 300, 456, 645, 572, 26782, 51456], "temperature": 0.0, "avg_logprob": -0.13950558712607936, "compression_ratio": 1.7464285714285714, "no_speech_prob": 0.07157952338457108}, {"id": 29, "seek": 15444, "start": 176.28, "end": 182.04, "text": " in the 80s. There were instead computers with big CRTs. So this is what Ogil looked like in the 80s.", "tokens": [51456, 294, 264, 4688, 82, 13, 821, 645, 2602, 10807, 365, 955, 14123, 33424, 13, 407, 341, 307, 437, 14883, 388, 2956, 411, 294, 264, 4688, 82, 13, 51744], "temperature": 0.0, "avg_logprob": -0.13950558712607936, "compression_ratio": 1.7464285714285714, "no_speech_prob": 0.07157952338457108}, {"id": 30, "seek": 18204, "start": 183.04, "end": 187.64, "text": " So you'd have some ASCII art and you want to go out and it would connect via some modem,", "tokens": [50414, 407, 291, 1116, 362, 512, 7469, 34, 9503, 1523, 293, 291, 528, 281, 352, 484, 293, 309, 576, 1745, 5766, 512, 1072, 443, 11, 50644], "temperature": 0.0, "avg_logprob": -0.21770807412954477, "compression_ratio": 1.504, "no_speech_prob": 0.0008558395784348249}, {"id": 31, "seek": 18204, "start": 187.64, "end": 194.39999999999998, "text": " let's say, search the database. This is version 1.0 of Ogil 84. And it comes up with its matches and", "tokens": [50644, 718, 311, 584, 11, 3164, 264, 8149, 13, 639, 307, 3037, 502, 13, 15, 295, 14883, 388, 29018, 13, 400, 309, 1487, 493, 365, 1080, 10676, 293, 50982], "temperature": 0.0, "avg_logprob": -0.21770807412954477, "compression_ratio": 1.504, "no_speech_prob": 0.0008558395784348249}, {"id": 32, "seek": 18204, "start": 194.39999999999998, "end": 200.12, "text": " this is your user interface. So I hope you liked it. Alright, now of course back in the day,", "tokens": [50982, 341, 307, 428, 4195, 9226, 13, 407, 286, 1454, 291, 4501, 309, 13, 2798, 11, 586, 295, 1164, 646, 294, 264, 786, 11, 51268], "temperature": 0.0, "avg_logprob": -0.21770807412954477, "compression_ratio": 1.504, "no_speech_prob": 0.0008558395784348249}, {"id": 33, "seek": 18204, "start": 200.12, "end": 207.79999999999998, "text": " 1984, computers were really slow, right? Really slow. And so actually this Luke today who was", "tokens": [51268, 27127, 11, 10807, 645, 534, 2964, 11, 558, 30, 4083, 2964, 13, 400, 370, 767, 341, 13044, 965, 567, 390, 51652], "temperature": 0.0, "avg_logprob": -0.21770807412954477, "compression_ratio": 1.504, "no_speech_prob": 0.0008558395784348249}, {"id": 34, "seek": 20780, "start": 207.84, "end": 214.04000000000002, "text": " like Ogil is too slow, well things were really bad in 1984, right? But it turns out things were", "tokens": [50366, 411, 14883, 388, 307, 886, 2964, 11, 731, 721, 645, 534, 1578, 294, 27127, 11, 558, 30, 583, 309, 4523, 484, 721, 645, 50676], "temperature": 0.0, "avg_logprob": -0.14106075347415983, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.004331029951572418}, {"id": 35, "seek": 20780, "start": 214.04000000000002, "end": 219.60000000000002, "text": " also kind of better in a way. So performance used to be really easy. So I'm sure all of you have", "tokens": [50676, 611, 733, 295, 1101, 294, 257, 636, 13, 407, 3389, 1143, 281, 312, 534, 1858, 13, 407, 286, 478, 988, 439, 295, 291, 362, 50954], "temperature": 0.0, "avg_logprob": -0.14106075347415983, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.004331029951572418}, {"id": 36, "seek": 20780, "start": 219.60000000000002, "end": 224.32000000000002, "text": " seen this kind of graph. What I'm going to show you is on the left, the number of transistors,", "tokens": [50954, 1612, 341, 733, 295, 4295, 13, 708, 286, 478, 516, 281, 855, 291, 307, 322, 264, 1411, 11, 264, 1230, 295, 1145, 46976, 11, 51190], "temperature": 0.0, "avg_logprob": -0.14106075347415983, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.004331029951572418}, {"id": 37, "seek": 20780, "start": 224.32000000000002, "end": 231.52, "text": " as years progress on the x-axis, if they go up, you'll notice it is a log scale. And on the right", "tokens": [51190, 382, 924, 4205, 322, 264, 2031, 12, 24633, 11, 498, 436, 352, 493, 11, 291, 603, 3449, 309, 307, 257, 3565, 4373, 13, 400, 322, 264, 558, 51550], "temperature": 0.0, "avg_logprob": -0.14106075347415983, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.004331029951572418}, {"id": 38, "seek": 20780, "start": 231.52, "end": 237.20000000000002, "text": " you have clock speed, which roughly corresponds to computing performance. And so basically for", "tokens": [51550, 291, 362, 7830, 3073, 11, 597, 9810, 23249, 281, 15866, 3389, 13, 400, 370, 1936, 337, 51834], "temperature": 0.0, "avg_logprob": -0.14106075347415983, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.004331029951572418}, {"id": 39, "seek": 23720, "start": 237.23999999999998, "end": 241.56, "text": " years and years and years, we had this situation where the number of transistors were increasing", "tokens": [50366, 924, 293, 924, 293, 924, 11, 321, 632, 341, 2590, 689, 264, 1230, 295, 1145, 46976, 645, 5662, 50582], "temperature": 0.0, "avg_logprob": -0.15339928704339104, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0017005744157359004}, {"id": 40, "seek": 23720, "start": 241.56, "end": 247.51999999999998, "text": " roughly at doubling every 18 months. This is famously Moore's law. And this generally had the", "tokens": [50582, 9810, 412, 33651, 633, 2443, 2493, 13, 639, 307, 34360, 21644, 311, 2101, 13, 400, 341, 5101, 632, 264, 50880], "temperature": 0.0, "avg_logprob": -0.15339928704339104, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0017005744157359004}, {"id": 41, "seek": 23720, "start": 247.51999999999998, "end": 253.04, "text": " effect of allowing clock speed to increase. And so you had smaller features. This meant that your", "tokens": [50880, 1802, 295, 8293, 7830, 3073, 281, 3488, 13, 400, 370, 291, 632, 4356, 4122, 13, 639, 4140, 300, 428, 51156], "temperature": 0.0, "avg_logprob": -0.15339928704339104, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0017005744157359004}, {"id": 42, "seek": 23720, "start": 253.04, "end": 260.0, "text": " programs would actually run faster. And in fact, you could just literally wait, right? If you buy", "tokens": [51156, 4268, 576, 767, 1190, 4663, 13, 400, 294, 1186, 11, 291, 727, 445, 3736, 1699, 11, 558, 30, 759, 291, 2256, 51504], "temperature": 0.0, "avg_logprob": -0.15339928704339104, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0017005744157359004}, {"id": 43, "seek": 23720, "start": 260.0, "end": 265.52, "text": " new hardware, in fact to the upgrade cycle every year or so, like if you bought a new computer,", "tokens": [51504, 777, 8837, 11, 294, 1186, 281, 264, 11484, 6586, 633, 1064, 420, 370, 11, 411, 498, 291, 4243, 257, 777, 3820, 11, 51780], "temperature": 0.0, "avg_logprob": -0.15339928704339104, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.0017005744157359004}, {"id": 44, "seek": 26552, "start": 266.0, "end": 271.15999999999997, "text": " everything would run much faster. Now things were slow, right? So don't be so excited about how", "tokens": [50388, 1203, 576, 1190, 709, 4663, 13, 823, 721, 645, 2964, 11, 558, 30, 407, 500, 380, 312, 370, 2919, 466, 577, 50646], "temperature": 0.0, "avg_logprob": -0.15192711478785464, "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.0014549542684108019}, {"id": 45, "seek": 26552, "start": 271.15999999999997, "end": 277.71999999999997, "text": " great it was back then. It was bad. 33 megahertz to 66 was awesome back then and kind of terrible", "tokens": [50646, 869, 309, 390, 646, 550, 13, 467, 390, 1578, 13, 11816, 17986, 35655, 281, 21126, 390, 3476, 646, 550, 293, 733, 295, 6237, 50974], "temperature": 0.0, "avg_logprob": -0.15192711478785464, "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.0014549542684108019}, {"id": 46, "seek": 26552, "start": 277.71999999999997, "end": 283.71999999999997, "text": " today. But it did change the landscape of what it meant to be somebody who works on performance", "tokens": [50974, 965, 13, 583, 309, 630, 1319, 264, 9661, 295, 437, 309, 4140, 281, 312, 2618, 567, 1985, 322, 3389, 51274], "temperature": 0.0, "avg_logprob": -0.15192711478785464, "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.0014549542684108019}, {"id": 47, "seek": 26552, "start": 283.71999999999997, "end": 292.4, "text": " improvement because this is what you would do. So there really, in a way, was no sense trying to", "tokens": [51274, 10444, 570, 341, 307, 437, 291, 576, 360, 13, 407, 456, 534, 11, 294, 257, 636, 11, 390, 572, 2020, 1382, 281, 51708], "temperature": 0.0, "avg_logprob": -0.15192711478785464, "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.0014549542684108019}, {"id": 48, "seek": 29240, "start": 292.4, "end": 297.15999999999997, "text": " squeeze out any performance out of something when it was going to double in 18 months, right? The", "tokens": [50364, 13578, 484, 604, 3389, 484, 295, 746, 562, 309, 390, 516, 281, 3834, 294, 2443, 2493, 11, 558, 30, 440, 50602], "temperature": 0.0, "avg_logprob": -0.15102223692269162, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.017439505085349083}, {"id": 49, "seek": 29240, "start": 297.15999999999997, "end": 301.67999999999995, "text": " chance that you in 18 months were going to squeeze out double the performance pretty slender. So", "tokens": [50602, 2931, 300, 291, 294, 2443, 2493, 645, 516, 281, 13578, 484, 3834, 264, 3389, 1238, 1061, 3216, 13, 407, 50828], "temperature": 0.0, "avg_logprob": -0.15102223692269162, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.017439505085349083}, {"id": 50, "seek": 29240, "start": 301.67999999999995, "end": 306.96, "text": " you might as well just sip mojitos on the beach, all right? I can tell you that this was, well,", "tokens": [50828, 291, 1062, 382, 731, 445, 29668, 705, 73, 11343, 322, 264, 7534, 11, 439, 558, 30, 286, 393, 980, 291, 300, 341, 390, 11, 731, 11, 51092], "temperature": 0.0, "avg_logprob": -0.15102223692269162, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.017439505085349083}, {"id": 51, "seek": 29240, "start": 306.96, "end": 312.79999999999995, "text": " maybe the mojitos and beach part, notwithstanding, this was actually kind of a strategy that was", "tokens": [51092, 1310, 264, 705, 73, 11343, 293, 7534, 644, 11, 406, 11820, 8618, 11, 341, 390, 767, 733, 295, 257, 5206, 300, 390, 51384], "temperature": 0.0, "avg_logprob": -0.15102223692269162, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.017439505085349083}, {"id": 52, "seek": 29240, "start": 312.79999999999995, "end": 317.67999999999995, "text": " recommended for high performance computing. People would literally say, we're just going to upgrade", "tokens": [51384, 9628, 337, 1090, 3389, 15866, 13, 3432, 576, 3736, 584, 11, 321, 434, 445, 516, 281, 11484, 51628], "temperature": 0.0, "avg_logprob": -0.15102223692269162, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.017439505085349083}, {"id": 53, "seek": 31768, "start": 318.0, "end": 323.2, "text": " the computers next year or in two years. So focus on something else. Don't focus on performance,", "tokens": [50380, 264, 10807, 958, 1064, 420, 294, 732, 924, 13, 407, 1879, 322, 746, 1646, 13, 1468, 380, 1879, 322, 3389, 11, 50640], "temperature": 0.0, "avg_logprob": -0.12816820542017618, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.018544726073741913}, {"id": 54, "seek": 31768, "start": 323.2, "end": 330.92, "text": " right? Now, unfortunately, that's not the current state of affairs, right? So today, as all of you", "tokens": [50640, 558, 30, 823, 11, 7015, 11, 300, 311, 406, 264, 2190, 1785, 295, 17478, 11, 558, 30, 407, 965, 11, 382, 439, 295, 291, 51026], "temperature": 0.0, "avg_logprob": -0.12816820542017618, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.018544726073741913}, {"id": 55, "seek": 31768, "start": 330.92, "end": 336.88, "text": " know, when you upgrade a computer, it does not get faster anymore. Maybe it gets lighter. Maybe it", "tokens": [51026, 458, 11, 562, 291, 11484, 257, 3820, 11, 309, 775, 406, 483, 4663, 3602, 13, 2704, 309, 2170, 11546, 13, 2704, 309, 51324], "temperature": 0.0, "avg_logprob": -0.12816820542017618, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.018544726073741913}, {"id": 56, "seek": 31768, "start": 336.88, "end": 347.04, "text": " has improved battery life. Maybe not. But basically what happened is eventually Moore's law kind of", "tokens": [51324, 575, 9689, 5809, 993, 13, 2704, 406, 13, 583, 1936, 437, 2011, 307, 4728, 21644, 311, 2101, 733, 295, 51832], "temperature": 0.0, "avg_logprob": -0.12816820542017618, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.018544726073741913}, {"id": 57, "seek": 34704, "start": 347.04, "end": 352.08000000000004, "text": " ran out of steam. The reason that it did is not actually true that Moore's law ran out of steam.", "tokens": [50364, 5872, 484, 295, 11952, 13, 440, 1778, 300, 309, 630, 307, 406, 767, 2074, 300, 21644, 311, 2101, 5872, 484, 295, 11952, 13, 50616], "temperature": 0.0, "avg_logprob": -0.110592623888436, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.007576380856335163}, {"id": 58, "seek": 34704, "start": 352.08000000000004, "end": 359.76000000000005, "text": " This is technically a problem called denard scaling. And so denard scaling ran out. And so right", "tokens": [50616, 639, 307, 12120, 257, 1154, 1219, 1441, 515, 21589, 13, 400, 370, 1441, 515, 21589, 5872, 484, 13, 400, 370, 558, 51000], "temperature": 0.0, "avg_logprob": -0.110592623888436, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.007576380856335163}, {"id": 59, "seek": 34704, "start": 359.76000000000005, "end": 365.12, "text": " now, we're in a situation where basically, if we clock things up, we no longer can dissipate the", "tokens": [51000, 586, 11, 321, 434, 294, 257, 2590, 689, 1936, 11, 498, 321, 7830, 721, 493, 11, 321, 572, 2854, 393, 29544, 473, 264, 51268], "temperature": 0.0, "avg_logprob": -0.110592623888436, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.007576380856335163}, {"id": 60, "seek": 34704, "start": 365.12, "end": 370.56, "text": " heat on chips effectively enough to make them run faster. So transistor counts actually are still", "tokens": [51268, 3738, 322, 11583, 8659, 1547, 281, 652, 552, 1190, 4663, 13, 407, 34750, 14893, 767, 366, 920, 51540], "temperature": 0.0, "avg_logprob": -0.110592623888436, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.007576380856335163}, {"id": 61, "seek": 34704, "start": 370.56, "end": 375.6, "text": " increasing, which is why we have multiple cores. So we have a bunch of transistors. What are we", "tokens": [51540, 5662, 11, 597, 307, 983, 321, 362, 3866, 24826, 13, 407, 321, 362, 257, 3840, 295, 1145, 46976, 13, 708, 366, 321, 51792], "temperature": 0.0, "avg_logprob": -0.110592623888436, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.007576380856335163}, {"id": 62, "seek": 37560, "start": 375.68, "end": 381.04, "text": " going to do with them? Let's just stamp out more of the same thing, right? So that's great. But it's", "tokens": [50368, 516, 281, 360, 365, 552, 30, 961, 311, 445, 9921, 484, 544, 295, 264, 912, 551, 11, 558, 30, 407, 300, 311, 869, 13, 583, 309, 311, 50636], "temperature": 0.0, "avg_logprob": -0.11074790569266887, "compression_ratio": 1.6514522821576763, "no_speech_prob": 0.0037070137914270163}, {"id": 63, "seek": 37560, "start": 381.04, "end": 385.44, "text": " also not super awesome because if you have a program that is not parallel, it's not going to run any", "tokens": [50636, 611, 406, 1687, 3476, 570, 498, 291, 362, 257, 1461, 300, 307, 406, 8952, 11, 309, 311, 406, 516, 281, 1190, 604, 50856], "temperature": 0.0, "avg_logprob": -0.11074790569266887, "compression_ratio": 1.6514522821576763, "no_speech_prob": 0.0037070137914270163}, {"id": 64, "seek": 37560, "start": 385.44, "end": 393.04, "text": " faster. So everything now has multicore. Your phone has multicore. But it turns out it's still a", "tokens": [50856, 4663, 13, 407, 1203, 586, 575, 30608, 418, 13, 2260, 2593, 575, 30608, 418, 13, 583, 309, 4523, 484, 309, 311, 920, 257, 51236], "temperature": 0.0, "avg_logprob": -0.11074790569266887, "compression_ratio": 1.6514522821576763, "no_speech_prob": 0.0037070137914270163}, {"id": 65, "seek": 37560, "start": 393.04, "end": 401.28000000000003, "text": " problem. So these are actual screenshots from app store updates. Every app store update practically", "tokens": [51236, 1154, 13, 407, 613, 366, 3539, 40661, 490, 724, 3531, 9205, 13, 2048, 724, 3531, 5623, 15667, 51648], "temperature": 0.0, "avg_logprob": -0.11074790569266887, "compression_ratio": 1.6514522821576763, "no_speech_prob": 0.0037070137914270163}, {"id": 66, "seek": 40128, "start": 401.28, "end": 408.88, "text": " is about performance or bug fixes. And so here's one that says under the hood updates for better", "tokens": [50364, 307, 466, 3389, 420, 7426, 32539, 13, 400, 370, 510, 311, 472, 300, 1619, 833, 264, 13376, 9205, 337, 1101, 50744], "temperature": 0.0, "avg_logprob": -0.14148337117741616, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.006289223674684763}, {"id": 67, "seek": 40128, "start": 408.88, "end": 415.76, "text": " performance, as opposed to the over the hood updates. Okay. Here's bug fixes and performance", "tokens": [50744, 3389, 11, 382, 8851, 281, 264, 670, 264, 13376, 9205, 13, 1033, 13, 1692, 311, 7426, 32539, 293, 3389, 51088], "temperature": 0.0, "avg_logprob": -0.14148337117741616, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.006289223674684763}, {"id": 68, "seek": 40128, "start": 415.76, "end": 421.35999999999996, "text": " improvements, bug fixes and important performance improvements. And then this one, I like love", "tokens": [51088, 13797, 11, 7426, 32539, 293, 1021, 3389, 13797, 13, 400, 550, 341, 472, 11, 286, 411, 959, 51368], "temperature": 0.0, "avg_logprob": -0.14148337117741616, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.006289223674684763}, {"id": 69, "seek": 40128, "start": 421.35999999999996, "end": 426.79999999999995, "text": " this one a lot. So they're App Engine calibrations because it sounds cooler. So they've calibrated", "tokens": [51368, 341, 472, 257, 688, 13, 407, 436, 434, 3132, 7659, 2104, 6414, 763, 570, 309, 3263, 15566, 13, 407, 436, 600, 21583, 5468, 51640], "temperature": 0.0, "avg_logprob": -0.14148337117741616, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.006289223674684763}, {"id": 70, "seek": 42680, "start": 426.8, "end": 432.48, "text": " the App Engine. All right. Okay. So why does this keep happening? Why is it like every update is", "tokens": [50364, 264, 3132, 7659, 13, 1057, 558, 13, 1033, 13, 407, 983, 775, 341, 1066, 2737, 30, 1545, 307, 309, 411, 633, 5623, 307, 50648], "temperature": 0.0, "avg_logprob": -0.12566570972833108, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.016399435698986053}, {"id": 71, "seek": 42680, "start": 432.48, "end": 436.24, "text": " like, oh, God, we got to improve the performance. We got to improve the performance. Why is this", "tokens": [50648, 411, 11, 1954, 11, 1265, 11, 321, 658, 281, 3470, 264, 3389, 13, 492, 658, 281, 3470, 264, 3389, 13, 1545, 307, 341, 50836], "temperature": 0.0, "avg_logprob": -0.12566570972833108, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.016399435698986053}, {"id": 72, "seek": 42680, "start": 436.24, "end": 442.56, "text": " so hard? Why didn't they get it right the first time? And it turns out, unlike bugs, so code is", "tokens": [50836, 370, 1152, 30, 1545, 994, 380, 436, 483, 309, 558, 264, 700, 565, 30, 400, 309, 4523, 484, 11, 8343, 15120, 11, 370, 3089, 307, 51152], "temperature": 0.0, "avg_logprob": -0.12566570972833108, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.016399435698986053}, {"id": 73, "seek": 42680, "start": 442.56, "end": 448.32, "text": " always buggy, right? And it's hard to debug programs, but it's like this thing produced the", "tokens": [51152, 1009, 7426, 1480, 11, 558, 30, 400, 309, 311, 1152, 281, 24083, 4268, 11, 457, 309, 311, 411, 341, 551, 7126, 264, 51440], "temperature": 0.0, "avg_logprob": -0.12566570972833108, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.016399435698986053}, {"id": 74, "seek": 42680, "start": 448.32, "end": 454.0, "text": " wrong result. That's pretty clear. But if you have something and it just runs slower, you don't", "tokens": [51440, 2085, 1874, 13, 663, 311, 1238, 1850, 13, 583, 498, 291, 362, 746, 293, 309, 445, 6676, 14009, 11, 291, 500, 380, 51724], "temperature": 0.0, "avg_logprob": -0.12566570972833108, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.016399435698986053}, {"id": 75, "seek": 45400, "start": 454.0, "end": 459.36, "text": " really know where or what to change, right? So it can be really complicated. So what I'm going to", "tokens": [50364, 534, 458, 689, 420, 437, 281, 1319, 11, 558, 30, 407, 309, 393, 312, 534, 6179, 13, 407, 437, 286, 478, 516, 281, 50632], "temperature": 0.0, "avg_logprob": -0.061125335530338124, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.00572952488437295}, {"id": 76, "seek": 45400, "start": 459.36, "end": 464.24, "text": " talk about are two things in this talk. One is performance analysis. So I'm going to explain", "tokens": [50632, 751, 466, 366, 732, 721, 294, 341, 751, 13, 1485, 307, 3389, 5215, 13, 407, 286, 478, 516, 281, 2903, 50876], "temperature": 0.0, "avg_logprob": -0.061125335530338124, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.00572952488437295}, {"id": 77, "seek": 45400, "start": 464.24, "end": 469.92, "text": " how to do performance analysis, right? It turns out that the thing that we, including myself, have", "tokens": [50876, 577, 281, 360, 3389, 5215, 11, 558, 30, 467, 4523, 484, 300, 264, 551, 300, 321, 11, 3009, 2059, 11, 362, 51160], "temperature": 0.0, "avg_logprob": -0.061125335530338124, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.00572952488437295}, {"id": 78, "seek": 45400, "start": 469.92, "end": 475.68, "text": " often done is not really very rigorous. And you can draw the wrong conclusions. And then I'm going", "tokens": [51160, 2049, 1096, 307, 406, 534, 588, 29882, 13, 400, 291, 393, 2642, 264, 2085, 22865, 13, 400, 550, 286, 478, 516, 51448], "temperature": 0.0, "avg_logprob": -0.061125335530338124, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.00572952488437295}, {"id": 79, "seek": 45400, "start": 475.68, "end": 480.72, "text": " to talk about a new approach for performance profiling that will show you how to do it better.", "tokens": [51448, 281, 751, 466, 257, 777, 3109, 337, 3389, 1740, 4883, 300, 486, 855, 291, 577, 281, 360, 309, 1101, 13, 51700], "temperature": 0.0, "avg_logprob": -0.061125335530338124, "compression_ratio": 1.864864864864865, "no_speech_prob": 0.00572952488437295}, {"id": 80, "seek": 48072, "start": 480.72, "end": 486.72, "text": " All right? So first, I'm going to talk about performance analysis. So here's Luke. Luke has", "tokens": [50364, 1057, 558, 30, 407, 700, 11, 286, 478, 516, 281, 751, 466, 3389, 5215, 13, 407, 510, 311, 13044, 13, 13044, 575, 50664], "temperature": 0.0, "avg_logprob": -0.09927123098662405, "compression_ratio": 1.8866666666666667, "no_speech_prob": 0.001098662381991744}, {"id": 81, "seek": 48072, "start": 486.72, "end": 491.84000000000003, "text": " his code. And Luke is like, it's too slow. Ogil is too slow. What am I going to do? And so Luke", "tokens": [50664, 702, 3089, 13, 400, 13044, 307, 411, 11, 309, 311, 886, 2964, 13, 14883, 388, 307, 886, 2964, 13, 708, 669, 286, 516, 281, 360, 30, 400, 370, 13044, 50920], "temperature": 0.0, "avg_logprob": -0.09927123098662405, "compression_ratio": 1.8866666666666667, "no_speech_prob": 0.001098662381991744}, {"id": 82, "seek": 48072, "start": 491.84000000000003, "end": 497.36, "text": " has an idea. And Luke's idea involves some sort of new, you know, I'm going to do this first,", "tokens": [50920, 575, 364, 1558, 13, 400, 13044, 311, 1558, 11626, 512, 1333, 295, 777, 11, 291, 458, 11, 286, 478, 516, 281, 360, 341, 700, 11, 51196], "temperature": 0.0, "avg_logprob": -0.09927123098662405, "compression_ratio": 1.8866666666666667, "no_speech_prob": 0.001098662381991744}, {"id": 83, "seek": 48072, "start": 497.36, "end": 500.48, "text": " and I'm going to change this code. I'm going to change this function, blah, blah, blah. I make", "tokens": [51196, 293, 286, 478, 516, 281, 1319, 341, 3089, 13, 286, 478, 516, 281, 1319, 341, 2445, 11, 12288, 11, 12288, 11, 12288, 13, 286, 652, 51352], "temperature": 0.0, "avg_logprob": -0.09927123098662405, "compression_ratio": 1.8866666666666667, "no_speech_prob": 0.001098662381991744}, {"id": 84, "seek": 48072, "start": 500.48, "end": 505.6, "text": " a bunch of changes, right? And eventually I end up with A prime, all right? The new version of A.", "tokens": [51352, 257, 3840, 295, 2962, 11, 558, 30, 400, 4728, 286, 917, 493, 365, 316, 5835, 11, 439, 558, 30, 440, 777, 3037, 295, 316, 13, 51608], "temperature": 0.0, "avg_logprob": -0.09927123098662405, "compression_ratio": 1.8866666666666667, "no_speech_prob": 0.001098662381991744}, {"id": 85, "seek": 48072, "start": 505.6, "end": 509.68, "text": " And so now I want to know, did it work, right? So I've made this change. I thought it would", "tokens": [51608, 400, 370, 586, 286, 528, 281, 458, 11, 630, 309, 589, 11, 558, 30, 407, 286, 600, 1027, 341, 1319, 13, 286, 1194, 309, 576, 51812], "temperature": 0.0, "avg_logprob": -0.09927123098662405, "compression_ratio": 1.8866666666666667, "no_speech_prob": 0.001098662381991744}, {"id": 86, "seek": 50968, "start": 509.68, "end": 515.04, "text": " make things faster. So I go and I take my code, and I have some set of inputs, some benchmarks,", "tokens": [50364, 652, 721, 4663, 13, 407, 286, 352, 293, 286, 747, 452, 3089, 11, 293, 286, 362, 512, 992, 295, 15743, 11, 512, 43751, 11, 50632], "temperature": 0.0, "avg_logprob": -0.08312093227281483, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.0038239548448473215}, {"id": 87, "seek": 50968, "start": 515.04, "end": 522.48, "text": " let's say. And I go and I say, run A. And A takes 90 seconds, which is clearly too long for your", "tokens": [50632, 718, 311, 584, 13, 400, 286, 352, 293, 286, 584, 11, 1190, 316, 13, 400, 316, 2516, 4289, 3949, 11, 597, 307, 4448, 886, 938, 337, 428, 51004], "temperature": 0.0, "avg_logprob": -0.08312093227281483, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.0038239548448473215}, {"id": 88, "seek": 50968, "start": 522.48, "end": 527.44, "text": " App Store thing to run. But anyway, notwithstanding that, let's say there's something that takes", "tokens": [51004, 3132, 17242, 551, 281, 1190, 13, 583, 4033, 11, 406, 11820, 8618, 300, 11, 718, 311, 584, 456, 311, 746, 300, 2516, 51252], "temperature": 0.0, "avg_logprob": -0.08312093227281483, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.0038239548448473215}, {"id": 89, "seek": 50968, "start": 527.44, "end": 535.2, "text": " 90 seconds in his test, right? And then he runs A prime. 87.5 seconds. Fantastic. Success, right?", "tokens": [51252, 4289, 3949, 294, 702, 1500, 11, 558, 30, 400, 550, 415, 6676, 316, 5835, 13, 27990, 13, 20, 3949, 13, 21320, 13, 23669, 11, 558, 30, 51640], "temperature": 0.0, "avg_logprob": -0.08312093227281483, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.0038239548448473215}, {"id": 90, "seek": 53520, "start": 535.2800000000001, "end": 542.6400000000001, "text": " 2.8% faster, all right? Time for Mojito, okay? So this is great, right? And clearly, you know,", "tokens": [50368, 568, 13, 23, 4, 4663, 11, 439, 558, 30, 6161, 337, 3335, 73, 3528, 11, 1392, 30, 407, 341, 307, 869, 11, 558, 30, 400, 4448, 11, 291, 458, 11, 50736], "temperature": 0.0, "avg_logprob": -0.10302797138181508, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.006692204158753157}, {"id": 91, "seek": 53520, "start": 542.6400000000001, "end": 549.12, "text": " what Luke went and did had a big impact, big impact, all right? The question is, like, is it", "tokens": [50736, 437, 13044, 1437, 293, 630, 632, 257, 955, 2712, 11, 955, 2712, 11, 439, 558, 30, 440, 1168, 307, 11, 411, 11, 307, 309, 51060], "temperature": 0.0, "avg_logprob": -0.10302797138181508, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.006692204158753157}, {"id": 92, "seek": 53520, "start": 549.12, "end": 555.5200000000001, "text": " really faster, right? So if you go and you plot, like, here's a bar graph with, I'm kind of giving", "tokens": [51060, 534, 4663, 11, 558, 30, 407, 498, 291, 352, 293, 291, 7542, 11, 411, 11, 510, 311, 257, 2159, 4295, 365, 11, 286, 478, 733, 295, 2902, 51380], "temperature": 0.0, "avg_logprob": -0.10302797138181508, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.006692204158753157}, {"id": 93, "seek": 53520, "start": 555.5200000000001, "end": 562.72, "text": " away the game here, one execution of A prime and A. It turns out that A prime is 2.8% faster,", "tokens": [51380, 1314, 264, 1216, 510, 11, 472, 15058, 295, 316, 5835, 293, 316, 13, 467, 4523, 484, 300, 316, 5835, 307, 568, 13, 23, 4, 4663, 11, 51740], "temperature": 0.0, "avg_logprob": -0.10302797138181508, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.006692204158753157}, {"id": 94, "seek": 56272, "start": 562.72, "end": 566.72, "text": " looks good. But there's maybe a problem here. So what's the problem?", "tokens": [50364, 1542, 665, 13, 583, 456, 311, 1310, 257, 1154, 510, 13, 407, 437, 311, 264, 1154, 30, 50564], "temperature": 0.0, "avg_logprob": -0.06736600594442399, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.004609038587659597}, {"id": 95, "seek": 56272, "start": 570.08, "end": 574.1600000000001, "text": " Right, so there's this problem called variance, right? Like, when you run a program once,", "tokens": [50732, 1779, 11, 370, 456, 311, 341, 1154, 1219, 21977, 11, 558, 30, 1743, 11, 562, 291, 1190, 257, 1461, 1564, 11, 50936], "temperature": 0.0, "avg_logprob": -0.06736600594442399, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.004609038587659597}, {"id": 96, "seek": 56272, "start": 574.1600000000001, "end": 577.9200000000001, "text": " you're going to get some result. But if you run it again, maybe the result will be slightly", "tokens": [50936, 291, 434, 516, 281, 483, 512, 1874, 13, 583, 498, 291, 1190, 309, 797, 11, 1310, 264, 1874, 486, 312, 4748, 51124], "temperature": 0.0, "avg_logprob": -0.06736600594442399, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.004609038587659597}, {"id": 97, "seek": 56272, "start": 577.9200000000001, "end": 582.32, "text": " different, and you want to account for that. Great. So now we'll run it 30 times. 30 is the", "tokens": [51124, 819, 11, 293, 291, 528, 281, 2696, 337, 300, 13, 3769, 13, 407, 586, 321, 603, 1190, 309, 2217, 1413, 13, 2217, 307, 264, 51344], "temperature": 0.0, "avg_logprob": -0.06736600594442399, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.004609038587659597}, {"id": 98, "seek": 56272, "start": 582.32, "end": 588.5600000000001, "text": " magic number, by the way. So we're going to run it 30 times, and we get this graph. And so they're", "tokens": [51344, 5585, 1230, 11, 538, 264, 636, 13, 407, 321, 434, 516, 281, 1190, 309, 2217, 1413, 11, 293, 321, 483, 341, 4295, 13, 400, 370, 436, 434, 51656], "temperature": 0.0, "avg_logprob": -0.06736600594442399, "compression_ratio": 1.715953307392996, "no_speech_prob": 0.004609038587659597}, {"id": 99, "seek": 58856, "start": 588.56, "end": 594.0799999999999, "text": " pretty tightly distributed, and you can see it's still 2.8% faster, right? So seems plausible,", "tokens": [50364, 1238, 21952, 12631, 11, 293, 291, 393, 536, 309, 311, 920, 568, 13, 23, 4, 4663, 11, 558, 30, 407, 2544, 39925, 11, 50640], "temperature": 0.0, "avg_logprob": -0.08034254899665491, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.01322127878665924}, {"id": 100, "seek": 58856, "start": 594.8, "end": 599.76, "text": " like, I think everybody here would probably be like, looks like A prime is faster. Great. So", "tokens": [50676, 411, 11, 286, 519, 2201, 510, 576, 1391, 312, 411, 11, 1542, 411, 316, 5835, 307, 4663, 13, 3769, 13, 407, 50924], "temperature": 0.0, "avg_logprob": -0.08034254899665491, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.01322127878665924}, {"id": 101, "seek": 58856, "start": 599.76, "end": 604.16, "text": " the question you have to ask yourself is, why is it faster? So you might think, well, of course,", "tokens": [50924, 264, 1168, 291, 362, 281, 1029, 1803, 307, 11, 983, 307, 309, 4663, 30, 407, 291, 1062, 519, 11, 731, 11, 295, 1164, 11, 51144], "temperature": 0.0, "avg_logprob": -0.08034254899665491, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.01322127878665924}, {"id": 102, "seek": 58856, "start": 604.16, "end": 610.64, "text": " the reason is the code change, right? So I, as Luke, I'm the developer, and I go and I'm a genius,", "tokens": [51144, 264, 1778, 307, 264, 3089, 1319, 11, 558, 30, 407, 286, 11, 382, 13044, 11, 286, 478, 264, 10754, 11, 293, 286, 352, 293, 286, 478, 257, 14017, 11, 51468], "temperature": 0.0, "avg_logprob": -0.08034254899665491, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.01322127878665924}, {"id": 103, "seek": 58856, "start": 610.64, "end": 616.7199999999999, "text": " and I have my great idea, and it pays off 2.8%, right? Well, it turns out changing the code", "tokens": [51468, 293, 286, 362, 452, 869, 1558, 11, 293, 309, 10604, 766, 568, 13, 23, 8923, 558, 30, 1042, 11, 309, 4523, 484, 4473, 264, 3089, 51772], "temperature": 0.0, "avg_logprob": -0.08034254899665491, "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.01322127878665924}, {"id": 104, "seek": 61672, "start": 616.72, "end": 622.24, "text": " can actually lead to all sorts of knock-on effects that have nothing to do with your intended change.", "tokens": [50364, 393, 767, 1477, 281, 439, 7527, 295, 6728, 12, 266, 5065, 300, 362, 1825, 281, 360, 365, 428, 10226, 1319, 13, 50640], "temperature": 0.0, "avg_logprob": -0.12008772191313125, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.009123694151639938}, {"id": 105, "seek": 61672, "start": 622.24, "end": 628.48, "text": " So it could totally be an accident. So let me explain why. So there's this wonderful paper", "tokens": [50640, 407, 309, 727, 3879, 312, 364, 6398, 13, 407, 718, 385, 2903, 983, 13, 407, 456, 311, 341, 3715, 3035, 50952], "temperature": 0.0, "avg_logprob": -0.12008772191313125, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.009123694151639938}, {"id": 106, "seek": 61672, "start": 629.28, "end": 635.52, "text": " that is, it appeared in ASPOS in 2009 by Todd Mitkiewicz and a number of other people. I highly", "tokens": [50992, 300, 307, 11, 309, 8516, 294, 7469, 47, 4367, 294, 11453, 538, 21488, 10821, 74, 1093, 17946, 293, 257, 1230, 295, 661, 561, 13, 286, 5405, 51304], "temperature": 0.0, "avg_logprob": -0.12008772191313125, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.009123694151639938}, {"id": 107, "seek": 61672, "start": 635.52, "end": 640.5600000000001, "text": " recommend you read it. It's something like, how to do things wrong without really trying, something", "tokens": [51304, 2748, 291, 1401, 309, 13, 467, 311, 746, 411, 11, 577, 281, 360, 721, 2085, 1553, 534, 1382, 11, 746, 51556], "temperature": 0.0, "avg_logprob": -0.12008772191313125, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.009123694151639938}, {"id": 108, "seek": 64056, "start": 640.56, "end": 647.52, "text": " like that. And the conclusion is that the layout, like where the code and data end up in memory,", "tokens": [50364, 411, 300, 13, 400, 264, 10063, 307, 300, 264, 13333, 11, 411, 689, 264, 3089, 293, 1412, 917, 493, 294, 4675, 11, 50712], "temperature": 0.0, "avg_logprob": -0.08690665979854396, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.3700605034828186}, {"id": 109, "seek": 64056, "start": 647.52, "end": 652.0799999999999, "text": " has a pretty big impact on performance, all right? So when you go to measure something,", "tokens": [50712, 575, 257, 1238, 955, 2712, 322, 3389, 11, 439, 558, 30, 407, 562, 291, 352, 281, 3481, 746, 11, 50940], "temperature": 0.0, "avg_logprob": -0.08690665979854396, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.3700605034828186}, {"id": 110, "seek": 64056, "start": 652.0799999999999, "end": 657.5999999999999, "text": " those measurements are biased by depending where things kind of fell, right? So here are a few", "tokens": [50940, 729, 15383, 366, 28035, 538, 5413, 689, 721, 733, 295, 5696, 11, 558, 30, 407, 510, 366, 257, 1326, 51216], "temperature": 0.0, "avg_logprob": -0.08690665979854396, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.3700605034828186}, {"id": 111, "seek": 64056, "start": 657.5999999999999, "end": 662.0799999999999, "text": " things that can have an impact on layout, and I'm going to talk about more. So one is link order.", "tokens": [51216, 721, 300, 393, 362, 364, 2712, 322, 13333, 11, 293, 286, 478, 516, 281, 751, 466, 544, 13, 407, 472, 307, 2113, 1668, 13, 51440], "temperature": 0.0, "avg_logprob": -0.08690665979854396, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.3700605034828186}, {"id": 112, "seek": 64056, "start": 662.0799999999999, "end": 667.68, "text": " So if you're in cc++ land, and you have a make file, and the make file has a bunch of,", "tokens": [51440, 407, 498, 291, 434, 294, 269, 66, 25472, 2117, 11, 293, 291, 362, 257, 652, 3991, 11, 293, 264, 652, 3991, 575, 257, 3840, 295, 11, 51720], "temperature": 0.0, "avg_logprob": -0.08690665979854396, "compression_ratio": 1.744360902255639, "no_speech_prob": 0.3700605034828186}, {"id": 113, "seek": 66768, "start": 668.0, "end": 672.4, "text": " this link step and has a bunch of dot-os, depending how those dot-os are arranged,", "tokens": [50380, 341, 2113, 1823, 293, 575, 257, 3840, 295, 5893, 12, 329, 11, 5413, 577, 729, 5893, 12, 329, 366, 18721, 11, 50600], "temperature": 0.0, "avg_logprob": -0.13103370499192624, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.0005702972412109375}, {"id": 114, "seek": 66768, "start": 672.4, "end": 676.88, "text": " you can get different performance, okay? You might think, fine, all right? Your environment", "tokens": [50600, 291, 393, 483, 819, 3389, 11, 1392, 30, 509, 1062, 519, 11, 2489, 11, 439, 558, 30, 2260, 2823, 50824], "temperature": 0.0, "avg_logprob": -0.13103370499192624, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.0005702972412109375}, {"id": 115, "seek": 66768, "start": 676.88, "end": 682.9599999999999, "text": " variables. So when you go to execute your program, your environment variables, whether it's in cc++", "tokens": [50824, 9102, 13, 407, 562, 291, 352, 281, 14483, 428, 1461, 11, 428, 2823, 9102, 11, 1968, 309, 311, 294, 269, 66, 25472, 51128], "temperature": 0.0, "avg_logprob": -0.13103370499192624, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.0005702972412109375}, {"id": 116, "seek": 66768, "start": 682.9599999999999, "end": 687.52, "text": " or even managed languages, they somehow get copied in and everything else gets shifted.", "tokens": [51128, 420, 754, 6453, 8650, 11, 436, 6063, 483, 25365, 294, 293, 1203, 1646, 2170, 18892, 13, 51356], "temperature": 0.0, "avg_logprob": -0.13103370499192624, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.0005702972412109375}, {"id": 117, "seek": 66768, "start": 688.4799999999999, "end": 696.0799999999999, "text": " So in c and c++, this moves the stack. So this actually has an effect on layout. These two alone", "tokens": [51404, 407, 294, 269, 293, 269, 25472, 11, 341, 6067, 264, 8630, 13, 407, 341, 767, 575, 364, 1802, 322, 13333, 13, 1981, 732, 3312, 51784], "temperature": 0.0, "avg_logprob": -0.13103370499192624, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.0005702972412109375}, {"id": 118, "seek": 69608, "start": 696.08, "end": 705.2, "text": " can lead to shifts in performance of plus or minus 40%. Okay? So that's not great. So what is", "tokens": [50364, 393, 1477, 281, 19201, 294, 3389, 295, 1804, 420, 3175, 3356, 6856, 1033, 30, 407, 300, 311, 406, 869, 13, 407, 437, 307, 50820], "temperature": 0.0, "avg_logprob": -0.12640517491560715, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0005883826524950564}, {"id": 119, "seek": 69608, "start": 705.2, "end": 709.6, "text": " happening? Like, why is this happening? This is a huge shift. This is literally larger than the", "tokens": [50820, 2737, 30, 1743, 11, 983, 307, 341, 2737, 30, 639, 307, 257, 2603, 5513, 13, 639, 307, 3736, 4833, 813, 264, 51040], "temperature": 0.0, "avg_logprob": -0.12640517491560715, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0005883826524950564}, {"id": 120, "seek": 69608, "start": 709.6, "end": 719.12, "text": " impact of dash o3 over dash o0. Okay? Yes, you laugh, but as well you should. So why is a prime", "tokens": [51040, 2712, 295, 8240, 277, 18, 670, 8240, 277, 15, 13, 1033, 30, 1079, 11, 291, 5801, 11, 457, 382, 731, 291, 820, 13, 407, 983, 307, 257, 5835, 51516], "temperature": 0.0, "avg_logprob": -0.12640517491560715, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0005883826524950564}, {"id": 121, "seek": 69608, "start": 719.12, "end": 725.6, "text": " faster than a, right? So what is going on? Why could this happen without actually trying? So part", "tokens": [51516, 4663, 813, 257, 11, 558, 30, 407, 437, 307, 516, 322, 30, 1545, 727, 341, 1051, 1553, 767, 1382, 30, 407, 644, 51840], "temperature": 0.0, "avg_logprob": -0.12640517491560715, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0005883826524950564}, {"id": 122, "seek": 72560, "start": 725.6800000000001, "end": 731.6, "text": " of the problem here is that basically modern processors have become insanely complicated", "tokens": [50368, 295, 264, 1154, 510, 307, 300, 1936, 4363, 27751, 362, 1813, 40965, 6179, 50664], "temperature": 0.0, "avg_logprob": -0.09243991595356404, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0005192775279283524}, {"id": 123, "seek": 72560, "start": 731.6, "end": 737.84, "text": " in their zeal to increase speed. So what do they do? So they have caches, right? Add data", "tokens": [50664, 294, 641, 5277, 304, 281, 3488, 3073, 13, 407, 437, 360, 436, 360, 30, 407, 436, 362, 269, 13272, 11, 558, 30, 5349, 1412, 50976], "temperature": 0.0, "avg_logprob": -0.09243991595356404, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0005192775279283524}, {"id": 124, "seek": 72560, "start": 737.84, "end": 743.52, "text": " and instructions, get mapped to the cache. Well, it turns out for good reasons, these things are", "tokens": [50976, 293, 9415, 11, 483, 33318, 281, 264, 19459, 13, 1042, 11, 309, 4523, 484, 337, 665, 4112, 11, 613, 721, 366, 51260], "temperature": 0.0, "avg_logprob": -0.09243991595356404, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0005192775279283524}, {"id": 125, "seek": 72560, "start": 743.52, "end": 749.2, "text": " binned up into these things called sets. If they map to the same set, you can have a conflict. So", "tokens": [51260, 5171, 9232, 493, 666, 613, 721, 1219, 6352, 13, 759, 436, 4471, 281, 264, 912, 992, 11, 291, 393, 362, 257, 6596, 13, 407, 51544], "temperature": 0.0, "avg_logprob": -0.09243991595356404, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0005192775279283524}, {"id": 126, "seek": 72560, "start": 749.2, "end": 753.52, "text": " if you have hot code, a lot of hot code that is mapping to the same set, then it's not going to", "tokens": [51544, 498, 291, 362, 2368, 3089, 11, 257, 688, 295, 2368, 3089, 300, 307, 18350, 281, 264, 912, 992, 11, 550, 309, 311, 406, 516, 281, 51760], "temperature": 0.0, "avg_logprob": -0.09243991595356404, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.0005192775279283524}, {"id": 127, "seek": 75352, "start": 753.52, "end": 759.04, "text": " necessarily fit in cache, and your code will run slower. By luck, you could be in a situation", "tokens": [50364, 4725, 3318, 294, 19459, 11, 293, 428, 3089, 486, 1190, 14009, 13, 3146, 3668, 11, 291, 727, 312, 294, 257, 2590, 50640], "temperature": 0.0, "avg_logprob": -0.08838279247283935, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.004754616413265467}, {"id": 128, "seek": 75352, "start": 759.04, "end": 764.48, "text": " where when you changed a prime, you actually disrupted this conflict. And so now you have no", "tokens": [50640, 689, 562, 291, 3105, 257, 5835, 11, 291, 767, 42271, 341, 6596, 13, 400, 370, 586, 291, 362, 572, 50912], "temperature": 0.0, "avg_logprob": -0.08838279247283935, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.004754616413265467}, {"id": 129, "seek": 75352, "start": 764.48, "end": 769.84, "text": " conflict, right? These two things, one is the hot code and one maps to nothing. So no conflict.", "tokens": [50912, 6596, 11, 558, 30, 1981, 732, 721, 11, 472, 307, 264, 2368, 3089, 293, 472, 11317, 281, 1825, 13, 407, 572, 6596, 13, 51180], "temperature": 0.0, "avg_logprob": -0.08838279247283935, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.004754616413265467}, {"id": 130, "seek": 75352, "start": 769.84, "end": 775.6, "text": " Boom, it ran faster. All right? So that sounds great. So it could be the cache, but it could also", "tokens": [51180, 15523, 11, 309, 5872, 4663, 13, 1057, 558, 30, 407, 300, 3263, 869, 13, 407, 309, 727, 312, 264, 19459, 11, 457, 309, 727, 611, 51468], "temperature": 0.0, "avg_logprob": -0.08838279247283935, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.004754616413265467}, {"id": 131, "seek": 75352, "start": 775.6, "end": 781.6, "text": " be the branch predictor, which actually, again, is based on the addresses of your branches,", "tokens": [51468, 312, 264, 9819, 6069, 284, 11, 597, 767, 11, 797, 11, 307, 2361, 322, 264, 16862, 295, 428, 14770, 11, 51768], "temperature": 0.0, "avg_logprob": -0.08838279247283935, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.004754616413265467}, {"id": 132, "seek": 78160, "start": 781.6, "end": 785.52, "text": " and if these branches collide, then you can end up with things running slower.", "tokens": [50364, 293, 498, 613, 14770, 49093, 11, 550, 291, 393, 917, 493, 365, 721, 2614, 14009, 13, 50560], "temperature": 0.0, "avg_logprob": -0.098043007510049, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.001597745344042778}, {"id": 133, "seek": 78160, "start": 786.16, "end": 790.72, "text": " There's also this thing called the TLB, the translation look-aside buffer, which maps", "tokens": [50592, 821, 311, 611, 341, 551, 1219, 264, 40277, 33, 11, 264, 12853, 574, 12, 296, 482, 21762, 11, 597, 11317, 50820], "temperature": 0.0, "avg_logprob": -0.098043007510049, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.001597745344042778}, {"id": 134, "seek": 78160, "start": 790.72, "end": 794.72, "text": " virtual addresses to physical addresses. If things don't fit in the TLB because they span", "tokens": [50820, 6374, 16862, 281, 4001, 16862, 13, 759, 721, 500, 380, 3318, 294, 264, 40277, 33, 570, 436, 16174, 51020], "temperature": 0.0, "avg_logprob": -0.098043007510049, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.001597745344042778}, {"id": 135, "seek": 78160, "start": 794.72, "end": 799.9200000000001, "text": " two pages instead of one, suddenly things become slower. There's also a branch target predictor.", "tokens": [51020, 732, 7183, 2602, 295, 472, 11, 5800, 721, 1813, 14009, 13, 821, 311, 611, 257, 9819, 3779, 6069, 284, 13, 51280], "temperature": 0.0, "avg_logprob": -0.098043007510049, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.001597745344042778}, {"id": 136, "seek": 78160, "start": 799.9200000000001, "end": 806.0, "text": " There's a prefetchor. There's more. All right? So this is pretty bad. So all of these things can", "tokens": [51280, 821, 311, 257, 18417, 7858, 284, 13, 821, 311, 544, 13, 1057, 558, 30, 407, 341, 307, 1238, 1578, 13, 407, 439, 295, 613, 721, 393, 51584], "temperature": 0.0, "avg_logprob": -0.098043007510049, "compression_ratio": 1.7230769230769232, "no_speech_prob": 0.001597745344042778}, {"id": 137, "seek": 80600, "start": 806.08, "end": 812.24, "text": " happen. You might think, all right, link order is fine. The code thing is a little weird, but,", "tokens": [50368, 1051, 13, 509, 1062, 519, 11, 439, 558, 11, 2113, 1668, 307, 2489, 13, 440, 3089, 551, 307, 257, 707, 3657, 11, 457, 11, 50676], "temperature": 0.0, "avg_logprob": -0.094331288859792, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.14800246059894562}, {"id": 138, "seek": 80600, "start": 812.24, "end": 818.64, "text": " you know, hey, it's faster, right? It's 2.8% faster. That, like, I don't care. It's all good, right?", "tokens": [50676, 291, 458, 11, 4177, 11, 309, 311, 4663, 11, 558, 30, 467, 311, 568, 13, 23, 4, 4663, 13, 663, 11, 411, 11, 286, 500, 380, 1127, 13, 467, 311, 439, 665, 11, 558, 30, 50996], "temperature": 0.0, "avg_logprob": -0.094331288859792, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.14800246059894562}, {"id": 139, "seek": 80600, "start": 818.64, "end": 824.32, "text": " Now, it may not be faster on every machine, but it's faster today, right? So here's the problem.", "tokens": [50996, 823, 11, 309, 815, 406, 312, 4663, 322, 633, 3479, 11, 457, 309, 311, 4663, 965, 11, 558, 30, 407, 510, 311, 264, 1154, 13, 51280], "temperature": 0.0, "avg_logprob": -0.094331288859792, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.14800246059894562}, {"id": 140, "seek": 80600, "start": 824.32, "end": 830.4, "text": " Like, anything you do can disrupt this. So what could happen? One more malloc changes layout,", "tokens": [51280, 1743, 11, 1340, 291, 360, 393, 14124, 341, 13, 407, 437, 727, 1051, 30, 1485, 544, 16026, 905, 2962, 13333, 11, 51584], "temperature": 0.0, "avg_logprob": -0.094331288859792, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.14800246059894562}, {"id": 141, "seek": 80600, "start": 831.04, "end": 835.6, "text": " right? Like, you've shifted everything over, one more or less. If you upgrade anything in", "tokens": [51616, 558, 30, 1743, 11, 291, 600, 18892, 1203, 670, 11, 472, 544, 420, 1570, 13, 759, 291, 11484, 1340, 294, 51844], "temperature": 0.0, "avg_logprob": -0.094331288859792, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.14800246059894562}, {"id": 142, "seek": 83560, "start": 835.6, "end": 840.5600000000001, "text": " your system, this is going to change layout, right? So that's bad. Okay. So those things,", "tokens": [50364, 428, 1185, 11, 341, 307, 516, 281, 1319, 13333, 11, 558, 30, 407, 300, 311, 1578, 13, 1033, 13, 407, 729, 721, 11, 50612], "temperature": 0.0, "avg_logprob": -0.0968147617275432, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0002868445299100131}, {"id": 143, "seek": 83560, "start": 840.5600000000001, "end": 844.4, "text": " all right, I'm not going to change libc, and I guess I'll never malloc again. Fine.", "tokens": [50612, 439, 558, 11, 286, 478, 406, 516, 281, 1319, 22854, 66, 11, 293, 286, 2041, 286, 603, 1128, 16026, 905, 797, 13, 12024, 13, 50804], "temperature": 0.0, "avg_logprob": -0.0968147617275432, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0002868445299100131}, {"id": 144, "seek": 83560, "start": 846.64, "end": 853.12, "text": " Whatever. All right? So here's something that may be surprising. Running it in a new directory.", "tokens": [50916, 8541, 13, 1057, 558, 30, 407, 510, 311, 746, 300, 815, 312, 8830, 13, 28136, 309, 294, 257, 777, 21120, 13, 51240], "temperature": 0.0, "avg_logprob": -0.0968147617275432, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0002868445299100131}, {"id": 145, "seek": 83560, "start": 853.12, "end": 857.76, "text": " So it turns out that your current working directory goes right into your environment", "tokens": [51240, 407, 309, 4523, 484, 300, 428, 2190, 1364, 21120, 1709, 558, 666, 428, 2823, 51472], "temperature": 0.0, "avg_logprob": -0.0968147617275432, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0002868445299100131}, {"id": 146, "seek": 83560, "start": 857.76, "end": 863.84, "text": " variables, right? So that's weird, right? So, you know, if Vader tries to run your software,", "tokens": [51472, 9102, 11, 558, 30, 407, 300, 311, 3657, 11, 558, 30, 407, 11, 291, 458, 11, 498, 36337, 9898, 281, 1190, 428, 4722, 11, 51776], "temperature": 0.0, "avg_logprob": -0.0968147617275432, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.0002868445299100131}, {"id": 147, "seek": 86384, "start": 863.84, "end": 870.4, "text": " it's not going to work as fast because it's one character longer than Luke, okay? This is a real", "tokens": [50364, 309, 311, 406, 516, 281, 589, 382, 2370, 570, 309, 311, 472, 2517, 2854, 813, 13044, 11, 1392, 30, 639, 307, 257, 957, 50692], "temperature": 0.0, "avg_logprob": -0.10089045582395612, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.004069437272846699}, {"id": 148, "seek": 86384, "start": 870.4, "end": 875.9200000000001, "text": " effect. This can really happen. It has actually bitten me. I had a student who wrote something.", "tokens": [50692, 1802, 13, 639, 393, 534, 1051, 13, 467, 575, 767, 34608, 385, 13, 286, 632, 257, 3107, 567, 4114, 746, 13, 50968], "temperature": 0.0, "avg_logprob": -0.10089045582395612, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.004069437272846699}, {"id": 149, "seek": 86384, "start": 876.8000000000001, "end": 882.24, "text": " He has a long Indian last name. My username is just five letters long. It's just Emery.", "tokens": [51012, 634, 575, 257, 938, 6427, 1036, 1315, 13, 1222, 30351, 307, 445, 1732, 7825, 938, 13, 467, 311, 445, 18477, 88, 13, 51284], "temperature": 0.0, "avg_logprob": -0.10089045582395612, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.004069437272846699}, {"id": 150, "seek": 86384, "start": 882.88, "end": 887.36, "text": " And he did something. He's like, oh, it doesn't run any faster. It actually runs slower. And it's", "tokens": [51316, 400, 415, 630, 746, 13, 634, 311, 411, 11, 1954, 11, 309, 1177, 380, 1190, 604, 4663, 13, 467, 767, 6676, 14009, 13, 400, 309, 311, 51540], "temperature": 0.0, "avg_logprob": -0.10089045582395612, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.004069437272846699}, {"id": 151, "seek": 86384, "start": 887.36, "end": 892.72, "text": " like, that makes no sense at all. And eventually, we whittled it down, and it was like, if I run it", "tokens": [51540, 411, 11, 300, 1669, 572, 2020, 412, 439, 13, 400, 4728, 11, 321, 315, 593, 1493, 309, 760, 11, 293, 309, 390, 411, 11, 498, 286, 1190, 309, 51808], "temperature": 0.0, "avg_logprob": -0.10089045582395612, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.004069437272846699}, {"id": 152, "seek": 89272, "start": 892.72, "end": 904.48, "text": " as me, it's faster. Okay? That's right. All right? Changes your layout. So the solution is obvious,", "tokens": [50364, 382, 385, 11, 309, 311, 4663, 13, 1033, 30, 663, 311, 558, 13, 1057, 558, 30, 761, 10350, 428, 13333, 13, 407, 264, 3827, 307, 6322, 11, 50952], "temperature": 0.0, "avg_logprob": -0.09788661642172902, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.0005883778794668615}, {"id": 153, "seek": 89272, "start": 904.48, "end": 911.9200000000001, "text": " right? Run everything. All right. So I should add, you know, all of this is, you know, like,", "tokens": [50952, 558, 30, 8950, 1203, 13, 1057, 558, 13, 407, 286, 820, 909, 11, 291, 458, 11, 439, 295, 341, 307, 11, 291, 458, 11, 411, 11, 51324], "temperature": 0.0, "avg_logprob": -0.09788661642172902, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.0005883778794668615}, {"id": 154, "seek": 89272, "start": 911.9200000000001, "end": 915.36, "text": " the whole talk is really oriented towards, I'm going to improve my performance. But", "tokens": [51324, 264, 1379, 751, 307, 534, 21841, 3030, 11, 286, 478, 516, 281, 3470, 452, 3389, 13, 583, 51496], "temperature": 0.0, "avg_logprob": -0.09788661642172902, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.0005883778794668615}, {"id": 155, "seek": 89272, "start": 915.36, "end": 919.76, "text": " everything I'm talking about today can be viewed in reverse for performance regression. Like,", "tokens": [51496, 1203, 286, 478, 1417, 466, 965, 393, 312, 19174, 294, 9943, 337, 3389, 24590, 13, 1743, 11, 51716], "temperature": 0.0, "avg_logprob": -0.09788661642172902, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.0005883778794668615}, {"id": 156, "seek": 91976, "start": 919.76, "end": 926.8, "text": " I made a change, and things run 2.8% slower. Oh, God, roll back. Maybe not, right? Maybe", "tokens": [50364, 286, 1027, 257, 1319, 11, 293, 721, 1190, 568, 13, 23, 4, 14009, 13, 876, 11, 1265, 11, 3373, 646, 13, 2704, 406, 11, 558, 30, 2704, 50716], "temperature": 0.0, "avg_logprob": -0.10386009216308593, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.0007793384720571339}, {"id": 157, "seek": 91976, "start": 926.8, "end": 932.56, "text": " the next thing you do is going to actually undo that change, right? So basically, layout is super", "tokens": [50716, 264, 958, 551, 291, 360, 307, 516, 281, 767, 23779, 300, 1319, 11, 558, 30, 407, 1936, 11, 13333, 307, 1687, 51004], "temperature": 0.0, "avg_logprob": -0.10386009216308593, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.0007793384720571339}, {"id": 158, "seek": 91976, "start": 932.56, "end": 938.64, "text": " brittle. And like you've seen, layout biases measurement. So one of the questions that we", "tokens": [51004, 49325, 13, 400, 411, 291, 600, 1612, 11, 13333, 32152, 13160, 13, 407, 472, 295, 264, 1651, 300, 321, 51308], "temperature": 0.0, "avg_logprob": -0.10386009216308593, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.0007793384720571339}, {"id": 159, "seek": 91976, "start": 938.64, "end": 943.52, "text": " wanted to know is, is it possible to eliminate the effect of layout? So we can actually understand", "tokens": [51308, 1415, 281, 458, 307, 11, 307, 309, 1944, 281, 13819, 264, 1802, 295, 13333, 30, 407, 321, 393, 767, 1223, 51552], "temperature": 0.0, "avg_logprob": -0.10386009216308593, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.0007793384720571339}, {"id": 160, "seek": 91976, "start": 943.52, "end": 948.8, "text": " the performance of things kind of without having to think about, well, one malloc less or more,", "tokens": [51552, 264, 3389, 295, 721, 733, 295, 1553, 1419, 281, 519, 466, 11, 731, 11, 472, 16026, 905, 1570, 420, 544, 11, 51816], "temperature": 0.0, "avg_logprob": -0.10386009216308593, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.0007793384720571339}, {"id": 161, "seek": 94880, "start": 948.88, "end": 954.7199999999999, "text": " or, you know, Luke versus Vader. So the answer is yes. We built a tool that we call stabilizer.", "tokens": [50368, 420, 11, 291, 458, 11, 13044, 5717, 36337, 13, 407, 264, 1867, 307, 2086, 13, 492, 3094, 257, 2290, 300, 321, 818, 11652, 6545, 13, 50660], "temperature": 0.0, "avg_logprob": -0.09669620388156765, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.00023781230265740305}, {"id": 162, "seek": 94880, "start": 955.3599999999999, "end": 961.04, "text": " So stabilizer addresses this problem that I've just explained to you. Pardon me. And it eliminates", "tokens": [50692, 407, 11652, 6545, 16862, 341, 1154, 300, 286, 600, 445, 8825, 281, 291, 13, 32392, 385, 13, 400, 309, 49893, 50976], "temperature": 0.0, "avg_logprob": -0.09669620388156765, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.00023781230265740305}, {"id": 163, "seek": 94880, "start": 961.04, "end": 969.4399999999999, "text": " the effect of layout. So this is a way to actually measure programs where you can kind of actually", "tokens": [50976, 264, 1802, 295, 13333, 13, 407, 341, 307, 257, 636, 281, 767, 3481, 4268, 689, 291, 393, 733, 295, 767, 51396], "temperature": 0.0, "avg_logprob": -0.09669620388156765, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.00023781230265740305}, {"id": 164, "seek": 94880, "start": 969.4399999999999, "end": 974.16, "text": " know whether the regression you had is real or whether the optimization you had is real", "tokens": [51396, 458, 1968, 264, 24590, 291, 632, 307, 957, 420, 1968, 264, 19618, 291, 632, 307, 957, 51632], "temperature": 0.0, "avg_logprob": -0.09669620388156765, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.00023781230265740305}, {"id": 165, "seek": 97416, "start": 974.16, "end": 979.52, "text": " and not just an accident. So how does this work? How is this even possible? So the way that", "tokens": [50364, 293, 406, 445, 364, 6398, 13, 407, 577, 775, 341, 589, 30, 1012, 307, 341, 754, 1944, 30, 407, 264, 636, 300, 50632], "temperature": 0.0, "avg_logprob": -0.05843007663064752, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0011335071176290512}, {"id": 166, "seek": 97416, "start": 979.52, "end": 986.48, "text": " stabilizer works is that it randomizes layout. So it randomizes a lot of things. It randomizes the", "tokens": [50632, 11652, 6545, 1985, 307, 300, 309, 4974, 5660, 13333, 13, 407, 309, 4974, 5660, 257, 688, 295, 721, 13, 467, 4974, 5660, 264, 50980], "temperature": 0.0, "avg_logprob": -0.05843007663064752, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0011335071176290512}, {"id": 167, "seek": 97416, "start": 986.48, "end": 992.8, "text": " function addresses. It randomizes stack frame sizes. It even randomizes heap allocations. But not", "tokens": [50980, 2445, 16862, 13, 467, 4974, 5660, 8630, 3920, 11602, 13, 467, 754, 4974, 5660, 33591, 12660, 763, 13, 583, 406, 51296], "temperature": 0.0, "avg_logprob": -0.05843007663064752, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0011335071176290512}, {"id": 168, "seek": 97416, "start": 992.8, "end": 998.48, "text": " only does it do those things, it does it over and over again. So while your program is running,", "tokens": [51296, 787, 775, 309, 360, 729, 721, 11, 309, 775, 309, 670, 293, 670, 797, 13, 407, 1339, 428, 1461, 307, 2614, 11, 51580], "temperature": 0.0, "avg_logprob": -0.05843007663064752, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0011335071176290512}, {"id": 169, "seek": 97416, "start": 998.48, "end": 1002.8, "text": " it's literally doing randomization. And this turns out to be important, and I'll show you a graph", "tokens": [51580, 309, 311, 3736, 884, 4974, 2144, 13, 400, 341, 4523, 484, 281, 312, 1021, 11, 293, 286, 603, 855, 291, 257, 4295, 51796], "temperature": 0.0, "avg_logprob": -0.05843007663064752, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0011335071176290512}, {"id": 170, "seek": 100280, "start": 1002.8, "end": 1009.92, "text": " that we'll explain why. But basically, if you do this, then there's no way layout can bias your", "tokens": [50364, 300, 321, 603, 2903, 983, 13, 583, 1936, 11, 498, 291, 360, 341, 11, 550, 456, 311, 572, 636, 13333, 393, 12577, 428, 50720], "temperature": 0.0, "avg_logprob": -0.06269705721310206, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005357614136300981}, {"id": 171, "seek": 100280, "start": 1009.92, "end": 1015.68, "text": " measurement because a completely random layout can't bias the results. That's just how things work.", "tokens": [50720, 13160, 570, 257, 2584, 4974, 13333, 393, 380, 12577, 264, 3542, 13, 663, 311, 445, 577, 721, 589, 13, 51008], "temperature": 0.0, "avg_logprob": -0.06269705721310206, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005357614136300981}, {"id": 172, "seek": 100280, "start": 1015.68, "end": 1021.52, "text": " That's why we run randomized control trials. You've eliminated something as a possible cause.", "tokens": [51008, 663, 311, 983, 321, 1190, 38513, 1969, 12450, 13, 509, 600, 20308, 746, 382, 257, 1944, 3082, 13, 51300], "temperature": 0.0, "avg_logprob": -0.06269705721310206, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005357614136300981}, {"id": 173, "seek": 100280, "start": 1021.52, "end": 1027.44, "text": " The only other cause that remains is whatever change you made. So let's walk through what you", "tokens": [51300, 440, 787, 661, 3082, 300, 7023, 307, 2035, 1319, 291, 1027, 13, 407, 718, 311, 1792, 807, 437, 291, 51596], "temperature": 0.0, "avg_logprob": -0.06269705721310206, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005357614136300981}, {"id": 174, "seek": 100280, "start": 1027.44, "end": 1032.3999999999999, "text": " would do with stabilizer. So with stabilizer, again, clearly you're supposed to run your program", "tokens": [51596, 576, 360, 365, 11652, 6545, 13, 407, 365, 11652, 6545, 11, 797, 11, 4448, 291, 434, 3442, 281, 1190, 428, 1461, 51844], "temperature": 0.0, "avg_logprob": -0.06269705721310206, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005357614136300981}, {"id": 175, "seek": 103240, "start": 1032.4, "end": 1037.76, "text": " a bunch of times. But notice what happens to the execution times. Here the execution times are no", "tokens": [50364, 257, 3840, 295, 1413, 13, 583, 3449, 437, 2314, 281, 264, 15058, 1413, 13, 1692, 264, 15058, 1413, 366, 572, 50632], "temperature": 0.0, "avg_logprob": -0.06901822538457365, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0006877767154946923}, {"id": 176, "seek": 103240, "start": 1037.76, "end": 1043.2800000000002, "text": " longer tightly bound around this one very small measurement. The reason for that is that when", "tokens": [50632, 2854, 21952, 5472, 926, 341, 472, 588, 1359, 13160, 13, 440, 1778, 337, 300, 307, 300, 562, 50908], "temperature": 0.0, "avg_logprob": -0.06901822538457365, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0006877767154946923}, {"id": 177, "seek": 103240, "start": 1043.2800000000002, "end": 1048.72, "text": " you were running that program 30 times, it was sort of like you were going to do a survey of 30", "tokens": [50908, 291, 645, 2614, 300, 1461, 2217, 1413, 11, 309, 390, 1333, 295, 411, 291, 645, 516, 281, 360, 257, 8984, 295, 2217, 51180], "temperature": 0.0, "avg_logprob": -0.06901822538457365, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0006877767154946923}, {"id": 178, "seek": 103240, "start": 1048.72, "end": 1054.3200000000002, "text": " people, but you just ask one person. Because it's the same layout over and over again. So you did", "tokens": [51180, 561, 11, 457, 291, 445, 1029, 472, 954, 13, 1436, 309, 311, 264, 912, 13333, 670, 293, 670, 797, 13, 407, 291, 630, 51460], "temperature": 0.0, "avg_logprob": -0.06901822538457365, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0006877767154946923}, {"id": 179, "seek": 103240, "start": 1054.3200000000002, "end": 1060.88, "text": " an experiment on 30 executions, but what you really did is you just repeated 30 on one. So the only", "tokens": [51460, 364, 5120, 322, 2217, 4454, 3666, 11, 457, 437, 291, 534, 630, 307, 291, 445, 10477, 2217, 322, 472, 13, 407, 264, 787, 51788], "temperature": 0.0, "avg_logprob": -0.06901822538457365, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0006877767154946923}, {"id": 180, "seek": 106088, "start": 1060.88, "end": 1066.4, "text": " noise that you're eliminating is the noise that comes from network demons waking up or some other", "tokens": [50364, 5658, 300, 291, 434, 31203, 307, 264, 5658, 300, 1487, 490, 3209, 19733, 20447, 493, 420, 512, 661, 50640], "temperature": 0.0, "avg_logprob": -0.09070602540046938, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.0028893733397126198}, {"id": 181, "seek": 106088, "start": 1066.4, "end": 1071.6000000000001, "text": " random event, maybe some thermal issue in your computer, but it's not really altering layout.", "tokens": [50640, 4974, 2280, 11, 1310, 512, 15070, 2734, 294, 428, 3820, 11, 457, 309, 311, 406, 534, 11337, 278, 13333, 13, 50900], "temperature": 0.0, "avg_logprob": -0.09070602540046938, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.0028893733397126198}, {"id": 182, "seek": 106088, "start": 1071.6000000000001, "end": 1077.7600000000002, "text": " It's always the same layout. So here it's not, and you get these very nice bell curves. So now I'm", "tokens": [50900, 467, 311, 1009, 264, 912, 13333, 13, 407, 510, 309, 311, 406, 11, 293, 291, 483, 613, 588, 1481, 4549, 19490, 13, 407, 586, 286, 478, 51208], "temperature": 0.0, "avg_logprob": -0.09070602540046938, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.0028893733397126198}, {"id": 183, "seek": 106088, "start": 1077.7600000000002, "end": 1082.64, "text": " going to ask you the question. So this is an audience poll time. Is A prime faster than A? I just", "tokens": [51208, 516, 281, 1029, 291, 264, 1168, 13, 407, 341, 307, 364, 4034, 6418, 565, 13, 1119, 316, 5835, 4663, 813, 316, 30, 286, 445, 51452], "temperature": 0.0, "avg_logprob": -0.09070602540046938, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.0028893733397126198}, {"id": 184, "seek": 106088, "start": 1082.64, "end": 1089.2, "text": " want you to raise your hands if you think that A prime is faster than A. All right, great. Now keep", "tokens": [51452, 528, 291, 281, 5300, 428, 2377, 498, 291, 519, 300, 316, 5835, 307, 4663, 813, 316, 13, 1057, 558, 11, 869, 13, 823, 1066, 51780], "temperature": 0.0, "avg_logprob": -0.09070602540046938, "compression_ratio": 1.6885813148788926, "no_speech_prob": 0.0028893733397126198}, {"id": 185, "seek": 108920, "start": 1089.2, "end": 1094.8, "text": " your hands up. Don't set them down. But set them down if you change your mind. How about now?", "tokens": [50364, 428, 2377, 493, 13, 1468, 380, 992, 552, 760, 13, 583, 992, 552, 760, 498, 291, 1319, 428, 1575, 13, 1012, 466, 586, 30, 50644], "temperature": 0.0, "avg_logprob": -0.1569236682940133, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.0008295552688650787}, {"id": 186, "seek": 108920, "start": 1097.68, "end": 1106.4, "text": " How about now? There's still a few like hardcore. So what you all are doing is what I like to refer", "tokens": [50788, 1012, 466, 586, 30, 821, 311, 920, 257, 1326, 411, 28196, 13, 407, 437, 291, 439, 366, 884, 307, 437, 286, 411, 281, 2864, 51224], "temperature": 0.0, "avg_logprob": -0.1569236682940133, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.0008295552688650787}, {"id": 187, "seek": 108920, "start": 1106.4, "end": 1113.92, "text": " to as eyeball statistics. And so you're kind of like, looks close to me. That's too close.", "tokens": [51224, 281, 382, 38868, 12523, 13, 400, 370, 291, 434, 733, 295, 411, 11, 1542, 1998, 281, 385, 13, 663, 311, 886, 1998, 13, 51600], "temperature": 0.0, "avg_logprob": -0.1569236682940133, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.0008295552688650787}, {"id": 188, "seek": 111392, "start": 1114.64, "end": 1123.04, "text": " Right. But it turns out this is not actually a thing. So if you, yeah, it's not really statistics", "tokens": [50400, 1779, 13, 583, 309, 4523, 484, 341, 307, 406, 767, 257, 551, 13, 407, 498, 291, 11, 1338, 11, 309, 311, 406, 534, 12523, 50820], "temperature": 0.0, "avg_logprob": -0.11080099872707092, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.0011694530257955194}, {"id": 189, "seek": 111392, "start": 1123.6000000000001, "end": 1129.52, "text": " when you just eyeball results. So this is a bit of a refresher for some of you, but I'm going to", "tokens": [50848, 562, 291, 445, 38868, 3542, 13, 407, 341, 307, 257, 857, 295, 257, 17368, 511, 337, 512, 295, 291, 11, 457, 286, 478, 516, 281, 51144], "temperature": 0.0, "avg_logprob": -0.11080099872707092, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.0011694530257955194}, {"id": 190, "seek": 111392, "start": 1129.52, "end": 1135.04, "text": " walk you through this and how this all fits in with stabilizer. So in actual statistics, and today", "tokens": [51144, 1792, 291, 807, 341, 293, 577, 341, 439, 9001, 294, 365, 11652, 6545, 13, 407, 294, 3539, 12523, 11, 293, 965, 51420], "temperature": 0.0, "avg_logprob": -0.11080099872707092, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.0011694530257955194}, {"id": 191, "seek": 111392, "start": 1135.04, "end": 1139.92, "text": " I'm just going to talk about one flavor of statistics, which is called null hypothesis", "tokens": [51420, 286, 478, 445, 516, 281, 751, 466, 472, 6813, 295, 12523, 11, 597, 307, 1219, 18184, 17291, 51664], "temperature": 0.0, "avg_logprob": -0.11080099872707092, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.0011694530257955194}, {"id": 192, "seek": 113992, "start": 1139.92, "end": 1144.8000000000002, "text": " significance testing. There are others, notably Bayesian approaches. Happy to talk about that", "tokens": [50364, 17687, 4997, 13, 821, 366, 2357, 11, 31357, 7840, 42434, 11587, 13, 8277, 281, 751, 466, 300, 50608], "temperature": 0.0, "avg_logprob": -0.07195767732424156, "compression_ratio": 1.6275167785234899, "no_speech_prob": 0.0009697324712760746}, {"id": 193, "seek": 113992, "start": 1144.8000000000002, "end": 1150.64, "text": " offline. But basically the way it works is you just assume that the things are the same. You say", "tokens": [50608, 21857, 13, 583, 1936, 264, 636, 309, 1985, 307, 291, 445, 6552, 300, 264, 721, 366, 264, 912, 13, 509, 584, 50900], "temperature": 0.0, "avg_logprob": -0.07195767732424156, "compression_ratio": 1.6275167785234899, "no_speech_prob": 0.0009697324712760746}, {"id": 194, "seek": 113992, "start": 1150.64, "end": 1157.44, "text": " what is the likelihood of observing this difference by chance? All right. So it turns out that this", "tokens": [50900, 437, 307, 264, 22119, 295, 22107, 341, 2649, 538, 2931, 30, 1057, 558, 13, 407, 309, 4523, 484, 300, 341, 51240], "temperature": 0.0, "avg_logprob": -0.07195767732424156, "compression_ratio": 1.6275167785234899, "no_speech_prob": 0.0009697324712760746}, {"id": 195, "seek": 113992, "start": 1157.44, "end": 1162.0, "text": " is something that's just convenient. It's very easy to compute these probabilities for the normal", "tokens": [51240, 307, 746, 300, 311, 445, 10851, 13, 467, 311, 588, 1858, 281, 14722, 613, 33783, 337, 264, 2710, 51468], "temperature": 0.0, "avg_logprob": -0.07195767732424156, "compression_ratio": 1.6275167785234899, "no_speech_prob": 0.0009697324712760746}, {"id": 196, "seek": 113992, "start": 1162.0, "end": 1167.1200000000001, "text": " distribution, which you all remember from school. These graphs are normal. Awesome. It turns out", "tokens": [51468, 7316, 11, 597, 291, 439, 1604, 490, 1395, 13, 1981, 24877, 366, 2710, 13, 10391, 13, 467, 4523, 484, 51724], "temperature": 0.0, "avg_logprob": -0.07195767732424156, "compression_ratio": 1.6275167785234899, "no_speech_prob": 0.0009697324712760746}, {"id": 197, "seek": 116712, "start": 1167.12, "end": 1171.76, "text": " that stabilizer happens to make normal graphs or normal distributions. And I'll explain why.", "tokens": [50364, 300, 11652, 6545, 2314, 281, 652, 2710, 24877, 420, 2710, 37870, 13, 400, 286, 603, 2903, 983, 13, 50596], "temperature": 0.0, "avg_logprob": -0.0851148967900552, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.009123933501541615}, {"id": 198, "seek": 116712, "start": 1172.7199999999998, "end": 1176.0, "text": " So how are we going to do this? We're going to run stuff with stabilizer.", "tokens": [50644, 407, 577, 366, 321, 516, 281, 360, 341, 30, 492, 434, 516, 281, 1190, 1507, 365, 11652, 6545, 13, 50808], "temperature": 0.0, "avg_logprob": -0.0851148967900552, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.009123933501541615}, {"id": 199, "seek": 116712, "start": 1176.0, "end": 1181.52, "text": " We're going to pick some probability below which we're like, okay, good enough. Right. So", "tokens": [50808, 492, 434, 516, 281, 1888, 512, 8482, 2507, 597, 321, 434, 411, 11, 1392, 11, 665, 1547, 13, 1779, 13, 407, 51084], "temperature": 0.0, "avg_logprob": -0.0851148967900552, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.009123933501541615}, {"id": 200, "seek": 116712, "start": 1181.52, "end": 1186.2399999999998, "text": " if it's only a one in 20 chance, I see this probability like the, I see this event occurring.", "tokens": [51084, 498, 309, 311, 787, 257, 472, 294, 945, 2931, 11, 286, 536, 341, 8482, 411, 264, 11, 286, 536, 341, 2280, 18386, 13, 51320], "temperature": 0.0, "avg_logprob": -0.0851148967900552, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.009123933501541615}, {"id": 201, "seek": 116712, "start": 1186.2399999999998, "end": 1191.04, "text": " I'll be like, okay, that's good enough for me. You could be harsher. You could say one in 100,", "tokens": [51320, 286, 603, 312, 411, 11, 1392, 11, 300, 311, 665, 1547, 337, 385, 13, 509, 727, 312, 276, 685, 511, 13, 509, 727, 584, 472, 294, 2319, 11, 51560], "temperature": 0.0, "avg_logprob": -0.0851148967900552, "compression_ratio": 1.7248062015503876, "no_speech_prob": 0.009123933501541615}, {"id": 202, "seek": 119104, "start": 1191.04, "end": 1199.52, "text": " one in 1,000. It's pretty standard to say one in 20. This is the p value of 0.05. So the idea is", "tokens": [50364, 472, 294, 502, 11, 1360, 13, 467, 311, 1238, 3832, 281, 584, 472, 294, 945, 13, 639, 307, 264, 280, 2158, 295, 1958, 13, 13328, 13, 407, 264, 1558, 307, 50788], "temperature": 0.0, "avg_logprob": -0.12105931259515718, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.005729913245886564}, {"id": 203, "seek": 119104, "start": 1199.52, "end": 1204.56, "text": " if there's a low enough probability, you reject the null hypothesis, the null hypothesis being", "tokens": [50788, 498, 456, 311, 257, 2295, 1547, 8482, 11, 291, 8248, 264, 18184, 17291, 11, 264, 18184, 17291, 885, 51040], "temperature": 0.0, "avg_logprob": -0.12105931259515718, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.005729913245886564}, {"id": 204, "seek": 119104, "start": 1204.56, "end": 1208.8799999999999, "text": " that they're the same. And you conclude that the speed up is real. It's not due to the effective", "tokens": [51040, 300, 436, 434, 264, 912, 13, 400, 291, 16886, 300, 264, 3073, 493, 307, 957, 13, 467, 311, 406, 3462, 281, 264, 4942, 51256], "temperature": 0.0, "avg_logprob": -0.12105931259515718, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.005729913245886564}, {"id": 205, "seek": 119104, "start": 1208.8799999999999, "end": 1214.96, "text": " memory on memory layout. All right. So why re-randomization? The reason for re-randomization", "tokens": [51256, 4675, 322, 4675, 13333, 13, 1057, 558, 13, 407, 983, 319, 12, 3699, 298, 2144, 30, 440, 1778, 337, 319, 12, 3699, 298, 2144, 51560], "temperature": 0.0, "avg_logprob": -0.12105931259515718, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.005729913245886564}, {"id": 206, "seek": 119104, "start": 1214.96, "end": 1220.8, "text": " is that just randomizing once doesn't give you enough randomization. So this is an actual program.", "tokens": [51560, 307, 300, 445, 4974, 3319, 1564, 1177, 380, 976, 291, 1547, 4974, 2144, 13, 407, 341, 307, 364, 3539, 1461, 13, 51852], "temperature": 0.0, "avg_logprob": -0.12105931259515718, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.005729913245886564}, {"id": 207, "seek": 122104, "start": 1221.04, "end": 1226.96, "text": " You can see the distribution is pretty wacky. It's very far from normal. You can't intuitively", "tokens": [50364, 509, 393, 536, 264, 7316, 307, 1238, 42138, 88, 13, 467, 311, 588, 1400, 490, 2710, 13, 509, 393, 380, 46506, 50660], "temperature": 0.0, "avg_logprob": -0.08208442617345739, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.00010889406985370442}, {"id": 208, "seek": 122104, "start": 1226.96, "end": 1232.72, "text": " explore much of the space when you just randomize at startup as opposed to randomizing during", "tokens": [50660, 6839, 709, 295, 264, 1901, 562, 291, 445, 4974, 1125, 412, 18578, 382, 8851, 281, 4974, 3319, 1830, 50948], "temperature": 0.0, "avg_logprob": -0.08208442617345739, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.00010889406985370442}, {"id": 209, "seek": 122104, "start": 1232.72, "end": 1238.48, "text": " execution. This is in fact the kind of distribution you get when you randomize all the time. And", "tokens": [50948, 15058, 13, 639, 307, 294, 1186, 264, 733, 295, 7316, 291, 483, 562, 291, 4974, 1125, 439, 264, 565, 13, 400, 51236], "temperature": 0.0, "avg_logprob": -0.08208442617345739, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.00010889406985370442}, {"id": 210, "seek": 122104, "start": 1238.48, "end": 1242.72, "text": " these are normal distributions. So why do I keep saying that they're normal distributions?", "tokens": [51236, 613, 366, 2710, 37870, 13, 407, 983, 360, 286, 1066, 1566, 300, 436, 434, 2710, 37870, 30, 51448], "temperature": 0.0, "avg_logprob": -0.08208442617345739, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.00010889406985370442}, {"id": 211, "seek": 122104, "start": 1242.72, "end": 1250.0, "text": " The reason is essentially, again, going back to like freshman stats, stabilizer generates a new", "tokens": [51448, 440, 1778, 307, 4476, 11, 797, 11, 516, 646, 281, 411, 22154, 18152, 11, 11652, 6545, 23815, 257, 777, 51812], "temperature": 0.0, "avg_logprob": -0.08208442617345739, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.00010889406985370442}, {"id": 212, "seek": 125000, "start": 1250.0, "end": 1256.96, "text": " random layout every half second. That is to say it's a completely independent version of the program", "tokens": [50364, 4974, 13333, 633, 1922, 1150, 13, 663, 307, 281, 584, 309, 311, 257, 2584, 6695, 3037, 295, 264, 1461, 50712], "temperature": 0.0, "avg_logprob": -0.08421401977539063, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.0005033142515458167}, {"id": 213, "seek": 125000, "start": 1256.96, "end": 1261.68, "text": " right from half second to half second to half second. It's all randomized. And it's the same", "tokens": [50712, 558, 490, 1922, 1150, 281, 1922, 1150, 281, 1922, 1150, 13, 467, 311, 439, 38513, 13, 400, 309, 311, 264, 912, 50948], "temperature": 0.0, "avg_logprob": -0.08421401977539063, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.0005033142515458167}, {"id": 214, "seek": 125000, "start": 1261.68, "end": 1266.88, "text": " program the whole time. So it's identically distributed. And then we're adding them all up.", "tokens": [50948, 1461, 264, 1379, 565, 13, 407, 309, 311, 2473, 984, 12631, 13, 400, 550, 321, 434, 5127, 552, 439, 493, 13, 51208], "temperature": 0.0, "avg_logprob": -0.08421401977539063, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.0005033142515458167}, {"id": 215, "seek": 125000, "start": 1266.88, "end": 1273.2, "text": " And there is this nice result, a key result of stats, which is the sum of a sufficient number.", "tokens": [51208, 400, 456, 307, 341, 1481, 1874, 11, 257, 2141, 1874, 295, 18152, 11, 597, 307, 264, 2408, 295, 257, 11563, 1230, 13, 51524], "temperature": 0.0, "avg_logprob": -0.08421401977539063, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.0005033142515458167}, {"id": 216, "seek": 125000, "start": 1273.2, "end": 1277.76, "text": " So if you run a program for long enough of independent identically distributed random", "tokens": [51524, 407, 498, 291, 1190, 257, 1461, 337, 938, 1547, 295, 6695, 2473, 984, 12631, 4974, 51752], "temperature": 0.0, "avg_logprob": -0.08421401977539063, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.0005033142515458167}, {"id": 217, "seek": 127776, "start": 1277.76, "end": 1282.56, "text": " variables, it's approximately normally distributed no matter what the underlying distribution was.", "tokens": [50364, 9102, 11, 309, 311, 10447, 5646, 12631, 572, 1871, 437, 264, 14217, 7316, 390, 13, 50604], "temperature": 0.0, "avg_logprob": -0.08931962649027507, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0004878469044342637}, {"id": 218, "seek": 127776, "start": 1282.56, "end": 1287.2, "text": " This is the central limit theorem. So this makes execution times normally distributed,", "tokens": [50604, 639, 307, 264, 5777, 4948, 20904, 13, 407, 341, 1669, 15058, 1413, 5646, 12631, 11, 50836], "temperature": 0.0, "avg_logprob": -0.08931962649027507, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0004878469044342637}, {"id": 219, "seek": 127776, "start": 1287.2, "end": 1292.0, "text": " which is cool in other ways because you actually know how likely it is that you're going to see", "tokens": [50836, 597, 307, 1627, 294, 661, 2098, 570, 291, 767, 458, 577, 3700, 309, 307, 300, 291, 434, 516, 281, 536, 51076], "temperature": 0.0, "avg_logprob": -0.08931962649027507, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0004878469044342637}, {"id": 220, "seek": 127776, "start": 1292.0, "end": 1295.84, "text": " some very weird execution because you know what the distribution looks like.", "tokens": [51076, 512, 588, 3657, 15058, 570, 291, 458, 437, 264, 7316, 1542, 411, 13, 51268], "temperature": 0.0, "avg_logprob": -0.08931962649027507, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0004878469044342637}, {"id": 221, "seek": 127776, "start": 1295.84, "end": 1299.92, "text": " All right, great. So now we have this thing in hand and we're going to do something insane.", "tokens": [51268, 1057, 558, 11, 869, 13, 407, 586, 321, 362, 341, 551, 294, 1011, 293, 321, 434, 516, 281, 360, 746, 10838, 13, 51472], "temperature": 0.0, "avg_logprob": -0.08931962649027507, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0004878469044342637}, {"id": 222, "seek": 127776, "start": 1300.64, "end": 1306.8, "text": " We're going to see whether optimizations actually matter. And we know some of them matter. So we", "tokens": [51508, 492, 434, 516, 281, 536, 1968, 5028, 14455, 767, 1871, 13, 400, 321, 458, 512, 295, 552, 1871, 13, 407, 321, 51816], "temperature": 0.0, "avg_logprob": -0.08931962649027507, "compression_ratio": 1.9059233449477353, "no_speech_prob": 0.0004878469044342637}, {"id": 223, "seek": 130680, "start": 1306.8, "end": 1310.8799999999999, "text": " have a suite of benchmarks that we're going to evaluate it on. We're going to evaluate them", "tokens": [50364, 362, 257, 14205, 295, 43751, 300, 321, 434, 516, 281, 13059, 309, 322, 13, 492, 434, 516, 281, 13059, 552, 50568], "temperature": 0.0, "avg_logprob": -0.0845351602039198, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.00014883247786201537}, {"id": 224, "seek": 130680, "start": 1310.8799999999999, "end": 1314.8, "text": " individually and then across the whole benchmark suite. And I'll show you how we do it. So you", "tokens": [50568, 16652, 293, 550, 2108, 264, 1379, 18927, 14205, 13, 400, 286, 603, 855, 291, 577, 321, 360, 309, 13, 407, 291, 50764], "temperature": 0.0, "avg_logprob": -0.0845351602039198, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.00014883247786201537}, {"id": 225, "seek": 130680, "start": 1314.8, "end": 1320.8799999999999, "text": " build the benchmarks with stabilizer. Stabilizer is a plugin for LVM. If you just compile it as such,", "tokens": [50764, 1322, 264, 43751, 365, 11652, 6545, 13, 745, 5177, 6545, 307, 257, 23407, 337, 441, 53, 44, 13, 759, 291, 445, 31413, 309, 382, 1270, 11, 51068], "temperature": 0.0, "avg_logprob": -0.0845351602039198, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.00014883247786201537}, {"id": 226, "seek": 130680, "start": 1320.8799999999999, "end": 1325.6, "text": " it goes and randomizes everything. But you can actually just randomize things independently", "tokens": [51068, 309, 1709, 293, 4974, 5660, 1203, 13, 583, 291, 393, 767, 445, 4974, 1125, 721, 21761, 51304], "temperature": 0.0, "avg_logprob": -0.0845351602039198, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.00014883247786201537}, {"id": 227, "seek": 130680, "start": 1325.6, "end": 1330.3999999999999, "text": " if you wanted, like just code, just heap, and just stack. So now we run the benchmarks,", "tokens": [51304, 498, 291, 1415, 11, 411, 445, 3089, 11, 445, 33591, 11, 293, 445, 8630, 13, 407, 586, 321, 1190, 264, 43751, 11, 51544], "temperature": 0.0, "avg_logprob": -0.0845351602039198, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.00014883247786201537}, {"id": 228, "seek": 130680, "start": 1330.3999999999999, "end": 1335.6, "text": " we run them as usual. We drop them into one of my least favorite programming languages ever.", "tokens": [51544, 321, 1190, 552, 382, 7713, 13, 492, 3270, 552, 666, 472, 295, 452, 1935, 2954, 9410, 8650, 1562, 13, 51804], "temperature": 0.0, "avg_logprob": -0.0845351602039198, "compression_ratio": 1.8038585209003215, "no_speech_prob": 0.00014883247786201537}, {"id": 229, "seek": 133680, "start": 1336.8799999999999, "end": 1342.48, "text": " And then we decide what the result is. So again, we don't ask questions like this", "tokens": [50368, 400, 550, 321, 4536, 437, 264, 1874, 307, 13, 407, 797, 11, 321, 500, 380, 1029, 1651, 411, 341, 50648], "temperature": 0.0, "avg_logprob": -0.13054122016543435, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00033533023088239133}, {"id": 230, "seek": 133680, "start": 1342.48, "end": 1346.3999999999999, "text": " because that's eyeball stats. Instead, we ask a question like this,", "tokens": [50648, 570, 300, 311, 38868, 18152, 13, 7156, 11, 321, 1029, 257, 1168, 411, 341, 11, 50844], "temperature": 0.0, "avg_logprob": -0.13054122016543435, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00033533023088239133}, {"id": 231, "seek": 133680, "start": 1346.3999999999999, "end": 1350.08, "text": " pretend they're equal. How likely is it we'd observe this much of a difference?", "tokens": [50844, 11865, 436, 434, 2681, 13, 1012, 3700, 307, 309, 321, 1116, 11441, 341, 709, 295, 257, 2649, 30, 51028], "temperature": 0.0, "avg_logprob": -0.13054122016543435, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00033533023088239133}, {"id": 232, "seek": 133680, "start": 1350.56, "end": 1359.68, "text": " So I also have to say that you should not assume normality like almost ever in my humble opinion,", "tokens": [51052, 407, 286, 611, 362, 281, 584, 300, 291, 820, 406, 6552, 2026, 1860, 411, 1920, 1562, 294, 452, 16735, 4800, 11, 51508], "temperature": 0.0, "avg_logprob": -0.13054122016543435, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00033533023088239133}, {"id": 233, "seek": 133680, "start": 1359.68, "end": 1363.9199999999998, "text": " unless you have very good reasons for doing so. Here we have very good reasons for doing so.", "tokens": [51508, 5969, 291, 362, 588, 665, 4112, 337, 884, 370, 13, 1692, 321, 362, 588, 665, 4112, 337, 884, 370, 13, 51720], "temperature": 0.0, "avg_logprob": -0.13054122016543435, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00033533023088239133}, {"id": 234, "seek": 136392, "start": 1363.92, "end": 1369.68, "text": " So we can use really powerful statistical tests like the student's t-test. So this is the test", "tokens": [50364, 407, 321, 393, 764, 534, 4005, 22820, 6921, 411, 264, 3107, 311, 256, 12, 31636, 13, 407, 341, 307, 264, 1500, 50652], "temperature": 0.0, "avg_logprob": -0.0933628002802531, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00406994903460145}, {"id": 235, "seek": 136392, "start": 1369.68, "end": 1375.1200000000001, "text": " that you used to actually measure this difference. So if the p-value, the likelihood of this event", "tokens": [50652, 300, 291, 1143, 281, 767, 3481, 341, 2649, 13, 407, 498, 264, 280, 12, 29155, 11, 264, 22119, 295, 341, 2280, 50924], "temperature": 0.0, "avg_logprob": -0.0933628002802531, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00406994903460145}, {"id": 236, "seek": 136392, "start": 1375.1200000000001, "end": 1379.2, "text": " occurring, is less than some threshold, which as I mentioned before is 5%, we're going to reject", "tokens": [50924, 18386, 11, 307, 1570, 813, 512, 14678, 11, 597, 382, 286, 2835, 949, 307, 1025, 8923, 321, 434, 516, 281, 8248, 51128], "temperature": 0.0, "avg_logprob": -0.0933628002802531, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00406994903460145}, {"id": 237, "seek": 136392, "start": 1379.2, "end": 1384.88, "text": " the null hypothesis. That is to say, it's not because of random layout, the difference is real.", "tokens": [51128, 264, 18184, 17291, 13, 663, 307, 281, 584, 11, 309, 311, 406, 570, 295, 4974, 13333, 11, 264, 2649, 307, 957, 13, 51412], "temperature": 0.0, "avg_logprob": -0.0933628002802531, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00406994903460145}, {"id": 238, "seek": 136392, "start": 1385.52, "end": 1390.0800000000002, "text": " Everybody's on board, I hope. So now we're going to do it. You'll be shocked to learn", "tokens": [51444, 7646, 311, 322, 3150, 11, 286, 1454, 13, 407, 586, 321, 434, 516, 281, 360, 309, 13, 509, 603, 312, 12763, 281, 1466, 51672], "temperature": 0.0, "avg_logprob": -0.0933628002802531, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.00406994903460145}, {"id": 239, "seek": 139008, "start": 1390.08, "end": 1398.1599999999999, "text": " that dash O2 versus dash O1 makes a difference. Good. I would be weird if the result were otherwise.", "tokens": [50364, 300, 8240, 422, 17, 5717, 8240, 422, 16, 1669, 257, 2649, 13, 2205, 13, 286, 576, 312, 3657, 498, 264, 1874, 645, 5911, 13, 50768], "temperature": 0.0, "avg_logprob": -0.16344840526580812, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.01001159381121397}, {"id": 240, "seek": 139008, "start": 1398.1599999999999, "end": 1402.32, "text": " So you can see that there are statistically significant improvements, right on the right.", "tokens": [50768, 407, 291, 393, 536, 300, 456, 366, 36478, 4776, 13797, 11, 558, 322, 264, 558, 13, 50976], "temperature": 0.0, "avg_logprob": -0.16344840526580812, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.01001159381121397}, {"id": 241, "seek": 139008, "start": 1402.32, "end": 1407.76, "text": " There's some that are statistically significant but don't matter. And by God,", "tokens": [50976, 821, 311, 512, 300, 366, 36478, 4776, 457, 500, 380, 1871, 13, 400, 538, 1265, 11, 51248], "temperature": 0.0, "avg_logprob": -0.16344840526580812, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.01001159381121397}, {"id": 242, "seek": 139008, "start": 1407.76, "end": 1414.72, "text": " there are statistically significant performance drops. So it turns out that compiler people", "tokens": [51248, 456, 366, 36478, 4776, 3389, 11438, 13, 407, 309, 4523, 484, 300, 31958, 561, 51596], "temperature": 0.0, "avg_logprob": -0.16344840526580812, "compression_ratio": 1.7061611374407584, "no_speech_prob": 0.01001159381121397}, {"id": 243, "seek": 141472, "start": 1414.72, "end": 1419.44, "text": " run these same benchmarks and overfit the way that they do these optimizations,", "tokens": [50364, 1190, 613, 912, 43751, 293, 670, 6845, 264, 636, 300, 436, 360, 613, 5028, 14455, 11, 50600], "temperature": 0.0, "avg_logprob": -0.08913333136756141, "compression_ratio": 1.5849802371541502, "no_speech_prob": 0.005384401418268681}, {"id": 244, "seek": 141472, "start": 1419.44, "end": 1423.76, "text": " and some of them lead to layout changes. And it wasn't actually the code change.", "tokens": [50600, 293, 512, 295, 552, 1477, 281, 13333, 2962, 13, 400, 309, 2067, 380, 767, 264, 3089, 1319, 13, 50816], "temperature": 0.0, "avg_logprob": -0.08913333136756141, "compression_ratio": 1.5849802371541502, "no_speech_prob": 0.005384401418268681}, {"id": 245, "seek": 141472, "start": 1424.56, "end": 1429.2, "text": " And so we can actually distill out this effect. All right, great. By and large,", "tokens": [50856, 400, 370, 321, 393, 767, 42923, 484, 341, 1802, 13, 1057, 558, 11, 869, 13, 3146, 293, 2416, 11, 51088], "temperature": 0.0, "avg_logprob": -0.08913333136756141, "compression_ratio": 1.5849802371541502, "no_speech_prob": 0.005384401418268681}, {"id": 246, "seek": 141472, "start": 1429.2, "end": 1437.92, "text": " it looks like O2 over O1 is a win. How about O3 versus O2? Ready? It's amazing. Okay.", "tokens": [51088, 309, 1542, 411, 422, 17, 670, 422, 16, 307, 257, 1942, 13, 1012, 466, 422, 18, 5717, 422, 17, 30, 9944, 30, 467, 311, 2243, 13, 1033, 13, 51524], "temperature": 0.0, "avg_logprob": -0.08913333136756141, "compression_ratio": 1.5849802371541502, "no_speech_prob": 0.005384401418268681}, {"id": 247, "seek": 141472, "start": 1439.92, "end": 1443.3600000000001, "text": " So I actually have to change the axes so we can see a lot of these values.", "tokens": [51624, 407, 286, 767, 362, 281, 1319, 264, 35387, 370, 321, 393, 536, 257, 688, 295, 613, 4190, 13, 51796], "temperature": 0.0, "avg_logprob": -0.08913333136756141, "compression_ratio": 1.5849802371541502, "no_speech_prob": 0.005384401418268681}, {"id": 248, "seek": 144336, "start": 1443.36, "end": 1448.7199999999998, "text": " So I'm going to zoom in. Instead of it being 0 to 10, like the range is negative 10 to 20,", "tokens": [50364, 407, 286, 478, 516, 281, 8863, 294, 13, 7156, 295, 309, 885, 1958, 281, 1266, 11, 411, 264, 3613, 307, 3671, 1266, 281, 945, 11, 50632], "temperature": 0.0, "avg_logprob": -0.10829995890132717, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0004442014324013144}, {"id": 249, "seek": 144336, "start": 1448.7199999999998, "end": 1455.1999999999998, "text": " I'm going to make it 0 to 1.5. Okay, so now we can see them. So they're pretty small effects,", "tokens": [50632, 286, 478, 516, 281, 652, 309, 1958, 281, 502, 13, 20, 13, 1033, 11, 370, 586, 321, 393, 536, 552, 13, 407, 436, 434, 1238, 1359, 5065, 11, 50956], "temperature": 0.0, "avg_logprob": -0.10829995890132717, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0004442014324013144}, {"id": 250, "seek": 144336, "start": 1455.9199999999998, "end": 1460.9599999999998, "text": " but some of them are significant and some of them are not. Again, statistically significant,", "tokens": [50992, 457, 512, 295, 552, 366, 4776, 293, 512, 295, 552, 366, 406, 13, 3764, 11, 36478, 4776, 11, 51244], "temperature": 0.0, "avg_logprob": -0.10829995890132717, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0004442014324013144}, {"id": 251, "seek": 144336, "start": 1460.9599999999998, "end": 1467.76, "text": " 1.75% decrease in execution time. Great. A bunch of things where it's not significant", "tokens": [51244, 502, 13, 11901, 4, 11514, 294, 15058, 565, 13, 3769, 13, 316, 3840, 295, 721, 689, 309, 311, 406, 4776, 51584], "temperature": 0.0, "avg_logprob": -0.10829995890132717, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0004442014324013144}, {"id": 252, "seek": 144336, "start": 1467.76, "end": 1472.32, "text": " and a couple decreases, but really very minor effect sizes. So what do these results mean?", "tokens": [51584, 293, 257, 1916, 24108, 11, 457, 534, 588, 6696, 1802, 11602, 13, 407, 437, 360, 613, 3542, 914, 30, 51812], "temperature": 0.0, "avg_logprob": -0.10829995890132717, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0004442014324013144}, {"id": 253, "seek": 147232, "start": 1472.96, "end": 1478.56, "text": " I mean, you can't actually look at an individual benchmark. Like there's 30 of them, right?", "tokens": [50396, 286, 914, 11, 291, 393, 380, 767, 574, 412, 364, 2609, 18927, 13, 1743, 456, 311, 2217, 295, 552, 11, 558, 30, 50676], "temperature": 0.0, "avg_logprob": -0.1107761936803018, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0001795257703633979}, {"id": 254, "seek": 147232, "start": 1478.56, "end": 1482.24, "text": " So drawing a conclusion about all 30, you actually have to do something different.", "tokens": [50676, 407, 6316, 257, 10063, 466, 439, 2217, 11, 291, 767, 362, 281, 360, 746, 819, 13, 50860], "temperature": 0.0, "avg_logprob": -0.1107761936803018, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0001795257703633979}, {"id": 255, "seek": 147232, "start": 1482.24, "end": 1486.8799999999999, "text": " You have to collect all of them. You get a bunch of these graphs, and this is what you don't do.", "tokens": [50860, 509, 362, 281, 2500, 439, 295, 552, 13, 509, 483, 257, 3840, 295, 613, 24877, 11, 293, 341, 307, 437, 291, 500, 380, 360, 13, 51092], "temperature": 0.0, "avg_logprob": -0.1107761936803018, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0001795257703633979}, {"id": 256, "seek": 147232, "start": 1486.8799999999999, "end": 1493.36, "text": " Like, okay, this one is slower. This one's faster. This one's faster. This is just eyeballs everywhere.", "tokens": [51092, 1743, 11, 1392, 11, 341, 472, 307, 14009, 13, 639, 472, 311, 4663, 13, 639, 472, 311, 4663, 13, 639, 307, 445, 43758, 5315, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1107761936803018, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0001795257703633979}, {"id": 257, "seek": 147232, "start": 1494.1599999999999, "end": 1499.2, "text": " Okay? I mean, they're spooky and nobody wants to see those. So again, we're going to do the", "tokens": [51456, 1033, 30, 286, 914, 11, 436, 434, 30510, 293, 5079, 2738, 281, 536, 729, 13, 407, 797, 11, 321, 434, 516, 281, 360, 264, 51708], "temperature": 0.0, "avg_logprob": -0.1107761936803018, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0001795257703633979}, {"id": 258, "seek": 149920, "start": 1499.28, "end": 1503.3600000000001, "text": " same thing, but to test a bunch of things simultaneously, you do this thing, which is", "tokens": [50368, 912, 551, 11, 457, 281, 1500, 257, 3840, 295, 721, 16561, 11, 291, 360, 341, 551, 11, 597, 307, 50572], "temperature": 0.0, "avg_logprob": -0.11148355840667476, "compression_ratio": 1.6893939393939394, "no_speech_prob": 0.0010321916779503226}, {"id": 259, "seek": 149920, "start": 1503.3600000000001, "end": 1508.16, "text": " terribly named, called analysis of variance, and you plug it into R with this awesome incantation,", "tokens": [50572, 22903, 4926, 11, 1219, 5215, 295, 21977, 11, 293, 291, 5452, 309, 666, 497, 365, 341, 3476, 834, 394, 399, 11, 50812], "temperature": 0.0, "avg_logprob": -0.11148355840667476, "compression_ratio": 1.6893939393939394, "no_speech_prob": 0.0010321916779503226}, {"id": 260, "seek": 149920, "start": 1508.88, "end": 1513.1200000000001, "text": " and then you do, again, the same test. If the p-value is less than or equal to 5%,", "tokens": [50848, 293, 550, 291, 360, 11, 797, 11, 264, 912, 1500, 13, 759, 264, 280, 12, 29155, 307, 1570, 813, 420, 2681, 281, 1025, 8923, 51060], "temperature": 0.0, "avg_logprob": -0.11148355840667476, "compression_ratio": 1.6893939393939394, "no_speech_prob": 0.0010321916779503226}, {"id": 261, "seek": 149920, "start": 1513.1200000000001, "end": 1518.72, "text": " we reject the null hypothesis. All right? You ready? All right, here we go. Here's the p-value.", "tokens": [51060, 321, 8248, 264, 18184, 17291, 13, 1057, 558, 30, 509, 1919, 30, 1057, 558, 11, 510, 321, 352, 13, 1692, 311, 264, 280, 12, 29155, 13, 51340], "temperature": 0.0, "avg_logprob": -0.11148355840667476, "compression_ratio": 1.6893939393939394, "no_speech_prob": 0.0010321916779503226}, {"id": 262, "seek": 149920, "start": 1518.72, "end": 1522.88, "text": " So it has to be less than or equal to 5% or else we're going to conclude that dash", "tokens": [51340, 407, 309, 575, 281, 312, 1570, 813, 420, 2681, 281, 1025, 4, 420, 1646, 321, 434, 516, 281, 16886, 300, 8240, 51548], "temperature": 0.0, "avg_logprob": -0.11148355840667476, "compression_ratio": 1.6893939393939394, "no_speech_prob": 0.0010321916779503226}, {"id": 263, "seek": 152288, "start": 1522.88, "end": 1532.64, "text": " o3 versus dash o2 is nothing. All right? So the p-value is 26.4%. That means that one in four", "tokens": [50364, 277, 18, 5717, 8240, 277, 17, 307, 1825, 13, 1057, 558, 30, 407, 264, 280, 12, 29155, 307, 7551, 13, 19, 6856, 663, 1355, 300, 472, 294, 1451, 50852], "temperature": 0.0, "avg_logprob": -0.11944286269370956, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.002980827121064067}, {"id": 264, "seek": 152288, "start": 1532.64, "end": 1538.48, "text": " experiments will just show a random effect, right? Just literally randomly. We do not consider", "tokens": [50852, 12050, 486, 445, 855, 257, 4974, 1802, 11, 558, 30, 1449, 3736, 16979, 13, 492, 360, 406, 1949, 51144], "temperature": 0.0, "avg_logprob": -0.11944286269370956, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.002980827121064067}, {"id": 265, "seek": 152288, "start": 1538.48, "end": 1544.16, "text": " this enough evidence to reject the null hypothesis. So we're, we cannot reject the null hypothesis,", "tokens": [51144, 341, 1547, 4467, 281, 8248, 264, 18184, 17291, 13, 407, 321, 434, 11, 321, 2644, 8248, 264, 18184, 17291, 11, 51428], "temperature": 0.0, "avg_logprob": -0.11944286269370956, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.002980827121064067}, {"id": 266, "seek": 152288, "start": 1544.16, "end": 1549.8400000000001, "text": " which is that the effect is indistinguishable from noise. All right? Okay. So this is all", "tokens": [51428, 597, 307, 300, 264, 1802, 307, 1016, 468, 7050, 742, 712, 490, 5658, 13, 1057, 558, 30, 1033, 13, 407, 341, 307, 439, 51712], "temperature": 0.0, "avg_logprob": -0.11944286269370956, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.002980827121064067}, {"id": 267, "seek": 154984, "start": 1549.84, "end": 1557.28, "text": " terrible news for people like Luke who wanted optimizations to work, and I've actually seen,", "tokens": [50364, 6237, 2583, 337, 561, 411, 13044, 567, 1415, 5028, 14455, 281, 589, 11, 293, 286, 600, 767, 1612, 11, 50736], "temperature": 0.0, "avg_logprob": -0.15039907919394002, "compression_ratio": 1.4894736842105263, "no_speech_prob": 0.007814718410372734}, {"id": 268, "seek": 154984, "start": 1558.3999999999999, "end": 1565.12, "text": " I've actually seen projects. It makes, it kind of breaks your heart. Like projects I committed,", "tokens": [50792, 286, 600, 767, 1612, 4455, 13, 467, 1669, 11, 309, 733, 295, 9857, 428, 1917, 13, 1743, 4455, 286, 7784, 11, 51128], "temperature": 0.0, "avg_logprob": -0.15039907919394002, "compression_ratio": 1.4894736842105263, "no_speech_prob": 0.007814718410372734}, {"id": 269, "seek": 154984, "start": 1565.12, "end": 1573.6, "text": " like on GitHub, where it literally says dash o9, and I feel like why not dash o11? There's no,", "tokens": [51128, 411, 322, 23331, 11, 689, 309, 3736, 1619, 8240, 277, 24, 11, 293, 286, 841, 411, 983, 406, 8240, 277, 5348, 30, 821, 311, 572, 11, 51552], "temperature": 0.0, "avg_logprob": -0.15039907919394002, "compression_ratio": 1.4894736842105263, "no_speech_prob": 0.007814718410372734}, {"id": 270, "seek": 157360, "start": 1573.6799999999998, "end": 1579.28, "text": " there's no dash o9 or 11. It's just kind of bottoms out. But you know, hope springs eternal.", "tokens": [50368, 456, 311, 572, 8240, 277, 24, 420, 2975, 13, 467, 311, 445, 733, 295, 43413, 484, 13, 583, 291, 458, 11, 1454, 24647, 14503, 13, 50648], "temperature": 0.0, "avg_logprob": -0.09684024006128311, "compression_ratio": 1.6421052631578947, "no_speech_prob": 0.024419333785772324}, {"id": 271, "seek": 157360, "start": 1580.0, "end": 1585.6799999999998, "text": " All right. So great. So what are we going to do? So what people do when they can't speed things up,", "tokens": [50684, 1057, 558, 13, 407, 869, 13, 407, 437, 366, 321, 516, 281, 360, 30, 407, 437, 561, 360, 562, 436, 393, 380, 3073, 721, 493, 11, 50968], "temperature": 0.0, "avg_logprob": -0.09684024006128311, "compression_ratio": 1.6421052631578947, "no_speech_prob": 0.024419333785772324}, {"id": 272, "seek": 157360, "start": 1585.6799999999998, "end": 1589.9199999999998, "text": " right? They run a profiler. So there's these profilers, they all basically work the same way.", "tokens": [50968, 558, 30, 814, 1190, 257, 1740, 5441, 13, 407, 456, 311, 613, 1740, 388, 433, 11, 436, 439, 1936, 589, 264, 912, 636, 13, 51180], "temperature": 0.0, "avg_logprob": -0.09684024006128311, "compression_ratio": 1.6421052631578947, "no_speech_prob": 0.024419333785772324}, {"id": 273, "seek": 157360, "start": 1590.7199999999998, "end": 1594.8, "text": " You go and you get some result, and it says, hey, here's where my program spent its time.", "tokens": [51220, 509, 352, 293, 291, 483, 512, 1874, 11, 293, 309, 1619, 11, 4177, 11, 510, 311, 689, 452, 1461, 4418, 1080, 565, 13, 51424], "temperature": 0.0, "avg_logprob": -0.09684024006128311, "compression_ratio": 1.6421052631578947, "no_speech_prob": 0.024419333785772324}, {"id": 274, "seek": 157360, "start": 1595.4399999999998, "end": 1600.8799999999999, "text": " You get the number of calls to every function, runtime for each function, and this captures", "tokens": [51456, 509, 483, 264, 1230, 295, 5498, 281, 633, 2445, 11, 34474, 337, 1184, 2445, 11, 293, 341, 27986, 51728], "temperature": 0.0, "avg_logprob": -0.09684024006128311, "compression_ratio": 1.6421052631578947, "no_speech_prob": 0.024419333785772324}, {"id": 275, "seek": 160088, "start": 1600.88, "end": 1605.3600000000001, "text": " intuitively, maybe for most of us, like this is what a profiler should do, right? What do I care", "tokens": [50364, 46506, 11, 1310, 337, 881, 295, 505, 11, 411, 341, 307, 437, 257, 1740, 5441, 820, 360, 11, 558, 30, 708, 360, 286, 1127, 50588], "temperature": 0.0, "avg_logprob": -0.11172046811561885, "compression_ratio": 1.6220735785953178, "no_speech_prob": 0.0006666815606877208}, {"id": 276, "seek": 160088, "start": 1605.3600000000001, "end": 1610.72, "text": " about? There's frequently executed code or code that runs for a long time. That's where I should", "tokens": [50588, 466, 30, 821, 311, 10374, 17577, 3089, 420, 3089, 300, 6676, 337, 257, 938, 565, 13, 663, 311, 689, 286, 820, 50856], "temperature": 0.0, "avg_logprob": -0.11172046811561885, "compression_ratio": 1.6220735785953178, "no_speech_prob": 0.0006666815606877208}, {"id": 277, "seek": 160088, "start": 1610.72, "end": 1616.16, "text": " be focusing my optimization efforts. All right. It seems intuitively appealing. This is the way", "tokens": [50856, 312, 8416, 452, 19618, 6484, 13, 1057, 558, 13, 467, 2544, 46506, 23842, 13, 639, 307, 264, 636, 51128], "temperature": 0.0, "avg_logprob": -0.11172046811561885, "compression_ratio": 1.6220735785953178, "no_speech_prob": 0.0006666815606877208}, {"id": 278, "seek": 160088, "start": 1616.16, "end": 1621.1200000000001, "text": " profilers have been written since prof, back, you know, back from like, I don't know, late 60s,", "tokens": [51128, 1740, 388, 433, 362, 668, 3720, 1670, 1740, 11, 646, 11, 291, 458, 11, 646, 490, 411, 11, 286, 500, 380, 458, 11, 3469, 4060, 82, 11, 51376], "temperature": 0.0, "avg_logprob": -0.11172046811561885, "compression_ratio": 1.6220735785953178, "no_speech_prob": 0.0006666815606877208}, {"id": 279, "seek": 160088, "start": 1621.1200000000001, "end": 1627.1200000000001, "text": " early 70s. So would this in fact speed up Google? So we're going to do this experiment. We're going", "tokens": [51376, 2440, 5285, 82, 13, 407, 576, 341, 294, 1186, 3073, 493, 3329, 30, 407, 321, 434, 516, 281, 360, 341, 5120, 13, 492, 434, 516, 51676], "temperature": 0.0, "avg_logprob": -0.11172046811561885, "compression_ratio": 1.6220735785953178, "no_speech_prob": 0.0006666815606877208}, {"id": 280, "seek": 162712, "start": 1627.1999999999998, "end": 1631.76, "text": " to go and find the thing that runs for the longest amount of time and where it spends all of its", "tokens": [50368, 281, 352, 293, 915, 264, 551, 300, 6676, 337, 264, 15438, 2372, 295, 565, 293, 689, 309, 25620, 439, 295, 1080, 50596], "temperature": 0.0, "avg_logprob": -0.10477096034634498, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.0022516322787851095}, {"id": 281, "seek": 162712, "start": 1631.76, "end": 1637.6, "text": " time running. And so we're going to run it. And so we go and we do this. And basically,", "tokens": [50596, 565, 2614, 13, 400, 370, 321, 434, 516, 281, 1190, 309, 13, 400, 370, 321, 352, 293, 321, 360, 341, 13, 400, 1936, 11, 50888], "temperature": 0.0, "avg_logprob": -0.10477096034634498, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.0022516322787851095}, {"id": 282, "seek": 162712, "start": 1637.6, "end": 1643.1999999999998, "text": " it makes the loading thing flash faster. Okay. So, well, guess what? That's frequently executed.", "tokens": [50888, 309, 1669, 264, 15114, 551, 7319, 4663, 13, 1033, 13, 407, 11, 731, 11, 2041, 437, 30, 663, 311, 10374, 17577, 13, 51168], "temperature": 0.0, "avg_logprob": -0.10477096034634498, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.0022516322787851095}, {"id": 283, "seek": 162712, "start": 1643.84, "end": 1650.8, "text": " And in fact, it's the code that runs for the longest time, right? So this is not really great,", "tokens": [51200, 400, 294, 1186, 11, 309, 311, 264, 3089, 300, 6676, 337, 264, 15438, 565, 11, 558, 30, 407, 341, 307, 406, 534, 869, 11, 51548], "temperature": 0.0, "avg_logprob": -0.10477096034634498, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.0022516322787851095}, {"id": 284, "seek": 162712, "start": 1650.8, "end": 1656.8799999999999, "text": " especially if Luke spent like more than a minute optimizing that code. That's a shame. All right.", "tokens": [51548, 2318, 498, 13044, 4418, 411, 544, 813, 257, 3456, 40425, 300, 3089, 13, 663, 311, 257, 10069, 13, 1057, 558, 13, 51852], "temperature": 0.0, "avg_logprob": -0.10477096034634498, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.0022516322787851095}, {"id": 285, "seek": 165688, "start": 1656.88, "end": 1662.48, "text": " So basically in some profilers were developed in an era where everything was synchronous and there", "tokens": [50364, 407, 1936, 294, 512, 1740, 388, 433, 645, 4743, 294, 364, 4249, 689, 1203, 390, 44743, 293, 456, 50644], "temperature": 0.0, "avg_logprob": -0.08259100914001465, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0003459748113527894}, {"id": 286, "seek": 165688, "start": 1662.48, "end": 1668.3200000000002, "text": " was a single core. All right. That's not today. Today things are asynchronous or parallel or", "tokens": [50644, 390, 257, 2167, 4965, 13, 1057, 558, 13, 663, 311, 406, 965, 13, 2692, 721, 366, 49174, 420, 8952, 420, 50936], "temperature": 0.0, "avg_logprob": -0.08259100914001465, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0003459748113527894}, {"id": 287, "seek": 165688, "start": 1668.3200000000002, "end": 1673.8400000000001, "text": " concurrent or a mixed thereof. And profilers don't do a good job in these contexts. So we need to", "tokens": [50936, 37702, 420, 257, 7467, 456, 2670, 13, 400, 1740, 388, 433, 500, 380, 360, 257, 665, 1691, 294, 613, 30628, 13, 407, 321, 643, 281, 51212], "temperature": 0.0, "avg_logprob": -0.08259100914001465, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0003459748113527894}, {"id": 288, "seek": 165688, "start": 1673.8400000000001, "end": 1679.6000000000001, "text": " do better. So what would be really cool is if we could have something like this. So this is what", "tokens": [51212, 360, 1101, 13, 407, 437, 576, 312, 534, 1627, 307, 498, 321, 727, 362, 746, 411, 341, 13, 407, 341, 307, 437, 51500], "temperature": 0.0, "avg_logprob": -0.08259100914001465, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0003459748113527894}, {"id": 289, "seek": 165688, "start": 1679.6000000000001, "end": 1686.3200000000002, "text": " I call a causal profile. So a causal profile tells you visually, like, if I were to speed up this", "tokens": [51500, 286, 818, 257, 38755, 7964, 13, 407, 257, 38755, 7964, 5112, 291, 19622, 11, 411, 11, 498, 286, 645, 281, 3073, 493, 341, 51836], "temperature": 0.0, "avg_logprob": -0.08259100914001465, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.0003459748113527894}, {"id": 290, "seek": 168632, "start": 1686.32, "end": 1693.04, "text": " component by this much on the x-axis, then the whole program will speed up by this much. All", "tokens": [50364, 6542, 538, 341, 709, 322, 264, 2031, 12, 24633, 11, 550, 264, 1379, 1461, 486, 3073, 493, 538, 341, 709, 13, 1057, 50700], "temperature": 0.0, "avg_logprob": -0.10686122370130233, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0002780211507342756}, {"id": 291, "seek": 168632, "start": 1693.04, "end": 1698.8, "text": " right. So this is really nice. Like, if I had this graph, I would know I could spend a little", "tokens": [50700, 558, 13, 407, 341, 307, 534, 1481, 13, 1743, 11, 498, 286, 632, 341, 4295, 11, 286, 576, 458, 286, 727, 3496, 257, 707, 50988], "temperature": 0.0, "avg_logprob": -0.10686122370130233, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0002780211507342756}, {"id": 292, "seek": 168632, "start": 1698.8, "end": 1704.96, "text": " effort on the yellow search component and I'll get a big bang for my buck. Eventually, it's going", "tokens": [50988, 4630, 322, 264, 5566, 3164, 6542, 293, 286, 603, 483, 257, 955, 8550, 337, 452, 14894, 13, 17586, 11, 309, 311, 516, 51296], "temperature": 0.0, "avg_logprob": -0.10686122370130233, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0002780211507342756}, {"id": 293, "seek": 168632, "start": 1704.96, "end": 1710.24, "text": " to bottom out or top out at like, you know, like 70, 80%. And the red one, I could just keep going,", "tokens": [51296, 281, 2767, 484, 420, 1192, 484, 412, 411, 11, 291, 458, 11, 411, 5285, 11, 4688, 6856, 400, 264, 2182, 472, 11, 286, 727, 445, 1066, 516, 11, 51560], "temperature": 0.0, "avg_logprob": -0.10686122370130233, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0002780211507342756}, {"id": 294, "seek": 168632, "start": 1710.24, "end": 1714.24, "text": " right? Like, it gets faster and faster the more I work. And the blue one, I should never,", "tokens": [51560, 558, 30, 1743, 11, 309, 2170, 4663, 293, 4663, 264, 544, 286, 589, 13, 400, 264, 3344, 472, 11, 286, 820, 1128, 11, 51760], "temperature": 0.0, "avg_logprob": -0.10686122370130233, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0002780211507342756}, {"id": 295, "seek": 171424, "start": 1714.24, "end": 1719.1200000000001, "text": " never optimize ever. All right. It would be cool to know this, right? It's essentially like an", "tokens": [50364, 1128, 19719, 1562, 13, 1057, 558, 13, 467, 576, 312, 1627, 281, 458, 341, 11, 558, 30, 467, 311, 4476, 411, 364, 50608], "temperature": 0.0, "avg_logprob": -0.06848448923189346, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0006666654371656477}, {"id": 296, "seek": 171424, "start": 1719.1200000000001, "end": 1724.88, "text": " oracle coming and telling you, this is the code you should work on, Luke. I don't know where I", "tokens": [50608, 420, 7041, 1348, 293, 3585, 291, 11, 341, 307, 264, 3089, 291, 820, 589, 322, 11, 13044, 13, 286, 500, 380, 458, 689, 286, 50896], "temperature": 0.0, "avg_logprob": -0.06848448923189346, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0006666654371656477}, {"id": 297, "seek": 171424, "start": 1724.88, "end": 1729.92, "text": " got that way of talking. Anyway. All right. So the question is, how would we know this? Like,", "tokens": [50896, 658, 300, 636, 295, 1417, 13, 5684, 13, 1057, 558, 13, 407, 264, 1168, 307, 11, 577, 576, 321, 458, 341, 30, 1743, 11, 51148], "temperature": 0.0, "avg_logprob": -0.06848448923189346, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0006666654371656477}, {"id": 298, "seek": 171424, "start": 1729.92, "end": 1734.4, "text": " how would we get this information? Like, how would we know that this change would cause this effect?", "tokens": [51148, 577, 576, 321, 483, 341, 1589, 30, 1743, 11, 577, 576, 321, 458, 300, 341, 1319, 576, 3082, 341, 1802, 30, 51372], "temperature": 0.0, "avg_logprob": -0.06848448923189346, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0006666654371656477}, {"id": 299, "seek": 171424, "start": 1734.4, "end": 1739.1200000000001, "text": " Right? We can't just go and optimize the program by arbitrary amounts and test it. That", "tokens": [51372, 1779, 30, 492, 393, 380, 445, 352, 293, 19719, 264, 1461, 538, 23211, 11663, 293, 1500, 309, 13, 663, 51608], "temperature": 0.0, "avg_logprob": -0.06848448923189346, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0006666654371656477}, {"id": 300, "seek": 171424, "start": 1739.1200000000001, "end": 1743.36, "text": " kind of defeats the purpose. So we're going to do something different. We're going to run an", "tokens": [51608, 733, 295, 7486, 1720, 264, 4334, 13, 407, 321, 434, 516, 281, 360, 746, 819, 13, 492, 434, 516, 281, 1190, 364, 51820], "temperature": 0.0, "avg_logprob": -0.06848448923189346, "compression_ratio": 1.805111821086262, "no_speech_prob": 0.0006666654371656477}, {"id": 301, "seek": 174336, "start": 1743.36, "end": 1751.4399999999998, "text": " experiment. And it requires one ingredient here, which I'll refer to as the force. So we're going", "tokens": [50364, 5120, 13, 400, 309, 7029, 472, 14751, 510, 11, 597, 286, 603, 2864, 281, 382, 264, 3464, 13, 407, 321, 434, 516, 50768], "temperature": 0.0, "avg_logprob": -0.09190730888302587, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.00036828083102591336}, {"id": 302, "seek": 174336, "start": 1751.4399999999998, "end": 1756.32, "text": " to use magic. And we're going to speed things up magically. And then we're going to measure", "tokens": [50768, 281, 764, 5585, 13, 400, 321, 434, 516, 281, 3073, 721, 493, 39763, 13, 400, 550, 321, 434, 516, 281, 3481, 51012], "temperature": 0.0, "avg_logprob": -0.09190730888302587, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.00036828083102591336}, {"id": 303, "seek": 174336, "start": 1756.9599999999998, "end": 1762.3999999999999, "text": " how much the effect was of speeding up each component by a certain amount on overall program", "tokens": [51044, 577, 709, 264, 1802, 390, 295, 35593, 493, 1184, 6542, 538, 257, 1629, 2372, 322, 4787, 1461, 51316], "temperature": 0.0, "avg_logprob": -0.09190730888302587, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.00036828083102591336}, {"id": 304, "seek": 174336, "start": 1762.3999999999999, "end": 1768.24, "text": " execution. Okay? So we just keep doing this, right? We get more and more points, right? And then I do", "tokens": [51316, 15058, 13, 1033, 30, 407, 321, 445, 1066, 884, 341, 11, 558, 30, 492, 483, 544, 293, 544, 2793, 11, 558, 30, 400, 550, 286, 360, 51608], "temperature": 0.0, "avg_logprob": -0.09190730888302587, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.00036828083102591336}, {"id": 305, "seek": 174336, "start": 1768.24, "end": 1772.0, "text": " it for different things. It turns out if I could speed up saving things to the three and a half", "tokens": [51608, 309, 337, 819, 721, 13, 467, 4523, 484, 498, 286, 727, 3073, 493, 6816, 721, 281, 264, 1045, 293, 257, 1922, 51796], "temperature": 0.0, "avg_logprob": -0.09190730888302587, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.00036828083102591336}, {"id": 306, "seek": 177200, "start": 1772.0, "end": 1777.6, "text": " inch floppy, it doesn't make a difference, right? And so on. All right? Now, unfortunately, we live", "tokens": [50364, 7227, 25343, 8200, 11, 309, 1177, 380, 652, 257, 2649, 11, 558, 30, 400, 370, 322, 13, 1057, 558, 30, 823, 11, 7015, 11, 321, 1621, 50644], "temperature": 0.0, "avg_logprob": -0.07930727223403582, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.0019265601877123117}, {"id": 307, "seek": 177200, "start": 1777.6, "end": 1784.32, "text": " in the real world where there's no magic. Sorry. Well, if there was magic, to be clear, this is not", "tokens": [50644, 294, 264, 957, 1002, 689, 456, 311, 572, 5585, 13, 4919, 13, 1042, 11, 498, 456, 390, 5585, 11, 281, 312, 1850, 11, 341, 307, 406, 50980], "temperature": 0.0, "avg_logprob": -0.07930727223403582, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.0019265601877123117}, {"id": 308, "seek": 177200, "start": 1784.32, "end": 1790.48, "text": " what we would do, right? I mean, obviously, there are many much better things we could do. I could", "tokens": [50980, 437, 321, 576, 360, 11, 558, 30, 286, 914, 11, 2745, 11, 456, 366, 867, 709, 1101, 721, 321, 727, 360, 13, 286, 727, 51288], "temperature": 0.0, "avg_logprob": -0.07930727223403582, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.0019265601877123117}, {"id": 309, "seek": 177200, "start": 1790.48, "end": 1794.4, "text": " think of people I would like to disappear off the face of the earth, for example. But I could also", "tokens": [51288, 519, 295, 561, 286, 576, 411, 281, 11596, 766, 264, 1851, 295, 264, 4120, 11, 337, 1365, 13, 583, 286, 727, 611, 51484], "temperature": 0.0, "avg_logprob": -0.07930727223403582, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.0019265601877123117}, {"id": 310, "seek": 177200, "start": 1794.4, "end": 1799.04, "text": " disappear all the runtime off the face of the earth. Because why not? All right? So obviously,", "tokens": [51484, 11596, 439, 264, 34474, 766, 264, 1851, 295, 264, 4120, 13, 1436, 983, 406, 30, 1057, 558, 30, 407, 2745, 11, 51716], "temperature": 0.0, "avg_logprob": -0.07930727223403582, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.0019265601877123117}, {"id": 311, "seek": 179904, "start": 1799.04, "end": 1803.84, "text": " that's what we would do. So we can't do that. We have to do something else. So what we are going", "tokens": [50364, 300, 311, 437, 321, 576, 360, 13, 407, 321, 393, 380, 360, 300, 13, 492, 362, 281, 360, 746, 1646, 13, 407, 437, 321, 366, 516, 50604], "temperature": 0.0, "avg_logprob": -0.0650109166684358, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.0010648996103554964}, {"id": 312, "seek": 179904, "start": 1803.84, "end": 1808.96, "text": " to do as our trick is we're going to do something that essentially takes advantage of this notion", "tokens": [50604, 281, 360, 382, 527, 4282, 307, 321, 434, 516, 281, 360, 746, 300, 4476, 2516, 5002, 295, 341, 10710, 50860], "temperature": 0.0, "avg_logprob": -0.0650109166684358, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.0010648996103554964}, {"id": 313, "seek": 179904, "start": 1808.96, "end": 1814.8799999999999, "text": " of relativity. So we're going to do a virtual speedup. And a virtual speedup speeds things up", "tokens": [50860, 295, 45675, 13, 407, 321, 434, 516, 281, 360, 257, 6374, 3073, 1010, 13, 400, 257, 6374, 3073, 1010, 16411, 721, 493, 51156], "temperature": 0.0, "avg_logprob": -0.0650109166684358, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.0010648996103554964}, {"id": 314, "seek": 179904, "start": 1814.8799999999999, "end": 1819.6, "text": " in scare quotes by slowing everything else down, right? So everything else that's running", "tokens": [51156, 294, 17185, 19963, 538, 26958, 1203, 1646, 760, 11, 558, 30, 407, 1203, 1646, 300, 311, 2614, 51392], "temperature": 0.0, "avg_logprob": -0.0650109166684358, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.0010648996103554964}, {"id": 315, "seek": 179904, "start": 1819.6, "end": 1824.72, "text": " at the same time will then be slowed down. And that will allow us to get the impact", "tokens": [51392, 412, 264, 912, 565, 486, 550, 312, 32057, 760, 13, 400, 300, 486, 2089, 505, 281, 483, 264, 2712, 51648], "temperature": 0.0, "avg_logprob": -0.0650109166684358, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.0010648996103554964}, {"id": 316, "seek": 182472, "start": 1824.72, "end": 1830.72, "text": " of how do we sped this thing up? What would the results have been? So here, for example,", "tokens": [50364, 295, 577, 360, 321, 637, 292, 341, 551, 493, 30, 708, 576, 264, 3542, 362, 668, 30, 407, 510, 11, 337, 1365, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09122378627459209, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00041729945223778486}, {"id": 317, "seek": 182472, "start": 1830.72, "end": 1838.32, "text": " if we speed up the sending of the picture results back by a certain amount, we've slowed down", "tokens": [50664, 498, 321, 3073, 493, 264, 7750, 295, 264, 3036, 3542, 646, 538, 257, 1629, 2372, 11, 321, 600, 32057, 760, 51044], "temperature": 0.0, "avg_logprob": -0.09122378627459209, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00041729945223778486}, {"id": 318, "seek": 182472, "start": 1838.32, "end": 1844.24, "text": " everything running concurrently with it. And then that gives us a result of a slowdown, which is the", "tokens": [51044, 1203, 2614, 37702, 356, 365, 309, 13, 400, 550, 300, 2709, 505, 257, 1874, 295, 257, 2964, 5093, 11, 597, 307, 264, 51340], "temperature": 0.0, "avg_logprob": -0.09122378627459209, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00041729945223778486}, {"id": 319, "seek": 182472, "start": 1844.24, "end": 1850.16, "text": " same thing as the result of having sped it up. So we actually can get points on this graph", "tokens": [51340, 912, 551, 382, 264, 1874, 295, 1419, 637, 292, 309, 493, 13, 407, 321, 767, 393, 483, 2793, 322, 341, 4295, 51636], "temperature": 0.0, "avg_logprob": -0.09122378627459209, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00041729945223778486}, {"id": 320, "seek": 185016, "start": 1850.16, "end": 1854.8000000000002, "text": " just by running these experiments. So I just got a point here, and I do it for everything,", "tokens": [50364, 445, 538, 2614, 613, 12050, 13, 407, 286, 445, 658, 257, 935, 510, 11, 293, 286, 360, 309, 337, 1203, 11, 50596], "temperature": 0.0, "avg_logprob": -0.10855783462524414, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0006263001123443246}, {"id": 321, "seek": 185016, "start": 1854.8000000000002, "end": 1860.5600000000002, "text": " and I get more points. And eventually, I get a graph like this. If I speed up indexing,", "tokens": [50596, 293, 286, 483, 544, 2793, 13, 400, 4728, 11, 286, 483, 257, 4295, 411, 341, 13, 759, 286, 3073, 493, 8186, 278, 11, 50884], "temperature": 0.0, "avg_logprob": -0.10855783462524414, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0006263001123443246}, {"id": 322, "seek": 185016, "start": 1860.5600000000002, "end": 1864.5600000000002, "text": " I'm going to get the exact same effect. Indexing is running at the same time as the compression.", "tokens": [50884, 286, 478, 516, 281, 483, 264, 1900, 912, 1802, 13, 33552, 278, 307, 2614, 412, 264, 912, 565, 382, 264, 19355, 13, 51084], "temperature": 0.0, "avg_logprob": -0.10855783462524414, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0006263001123443246}, {"id": 323, "seek": 185016, "start": 1865.44, "end": 1871.76, "text": " So I get this result, and then bang, I get these results. And again, these are all the results.", "tokens": [51128, 407, 286, 483, 341, 1874, 11, 293, 550, 8550, 11, 286, 483, 613, 3542, 13, 400, 797, 11, 613, 366, 439, 264, 3542, 13, 51444], "temperature": 0.0, "avg_logprob": -0.10855783462524414, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0006263001123443246}, {"id": 324, "seek": 187176, "start": 1872.4, "end": 1879.28, "text": " Now, I draw your attention to the one weird blue thing. So the blue thing is slower,", "tokens": [50396, 823, 11, 286, 2642, 428, 3202, 281, 264, 472, 3657, 3344, 551, 13, 407, 264, 3344, 551, 307, 14009, 11, 50740], "temperature": 0.0, "avg_logprob": -0.08757666507399225, "compression_ratio": 1.672811059907834, "no_speech_prob": 0.001284270896576345}, {"id": 325, "seek": 187176, "start": 1880.16, "end": 1885.68, "text": " and it turns out that sometimes optimizing things makes your program run slower. And the", "tokens": [50784, 293, 309, 4523, 484, 300, 2171, 40425, 721, 1669, 428, 1461, 1190, 14009, 13, 400, 264, 51060], "temperature": 0.0, "avg_logprob": -0.08757666507399225, "compression_ratio": 1.672811059907834, "no_speech_prob": 0.001284270896576345}, {"id": 326, "seek": 187176, "start": 1885.68, "end": 1892.24, "text": " intuition behind this is you can actually get congestion on a lock, or congestion for a shared", "tokens": [51060, 24002, 2261, 341, 307, 291, 393, 767, 483, 40816, 322, 257, 4017, 11, 420, 40816, 337, 257, 5507, 51388], "temperature": 0.0, "avg_logprob": -0.08757666507399225, "compression_ratio": 1.672811059907834, "no_speech_prob": 0.001284270896576345}, {"id": 327, "seek": 187176, "start": 1892.24, "end": 1897.6, "text": " resource like disk or network. And so speeding things up makes things worse. You would like to", "tokens": [51388, 7684, 411, 12355, 420, 3209, 13, 400, 370, 35593, 721, 493, 1669, 721, 5324, 13, 509, 576, 411, 281, 51656], "temperature": 0.0, "avg_logprob": -0.08757666507399225, "compression_ratio": 1.672811059907834, "no_speech_prob": 0.001284270896576345}, {"id": 328, "seek": 189760, "start": 1897.6, "end": 1904.1599999999999, "text": " know this before you get started. That would be a very, very bad day for Luke that might necessitate", "tokens": [50364, 458, 341, 949, 291, 483, 1409, 13, 663, 576, 312, 257, 588, 11, 588, 1578, 786, 337, 13044, 300, 1062, 2688, 8086, 50692], "temperature": 0.0, "avg_logprob": -0.10408783850268782, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0037651986349374056}, {"id": 329, "seek": 189760, "start": 1904.1599999999999, "end": 1909.12, "text": " several sequels to recover from. All right, great. All right, so let's dig into Ogil a little bit.", "tokens": [50692, 2940, 5123, 1625, 281, 8114, 490, 13, 1057, 558, 11, 869, 13, 1057, 558, 11, 370, 718, 311, 2528, 666, 14883, 388, 257, 707, 857, 13, 50940], "temperature": 0.0, "avg_logprob": -0.10408783850268782, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0037651986349374056}, {"id": 330, "seek": 189760, "start": 1910.3999999999999, "end": 1915.6, "text": " So what do we care about in Ogil? We care about two things. We care about how long it takes", "tokens": [51004, 407, 437, 360, 321, 1127, 466, 294, 14883, 388, 30, 492, 1127, 466, 732, 721, 13, 492, 1127, 466, 577, 938, 309, 2516, 51264], "temperature": 0.0, "avg_logprob": -0.10408783850268782, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0037651986349374056}, {"id": 331, "seek": 189760, "start": 1915.6, "end": 1921.9199999999998, "text": " between a request and a response, a.k.a. latency. Traditional profilers don't do this at all.", "tokens": [51264, 1296, 257, 5308, 293, 257, 4134, 11, 257, 13, 74, 13, 64, 13, 27043, 13, 46738, 1740, 388, 433, 500, 380, 360, 341, 412, 439, 13, 51580], "temperature": 0.0, "avg_logprob": -0.10408783850268782, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0037651986349374056}, {"id": 332, "seek": 192192, "start": 1922.0, "end": 1925.68, "text": " It's just total runtime. Oh, let me get in my soapbox for one moment.", "tokens": [50368, 467, 311, 445, 3217, 34474, 13, 876, 11, 718, 385, 483, 294, 452, 14587, 4995, 337, 472, 1623, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11506424779477327, "compression_ratio": 1.74822695035461, "no_speech_prob": 0.03209669142961502}, {"id": 333, "seek": 192192, "start": 1926.8000000000002, "end": 1930.48, "text": " Traditional profilers are about end-to-end runtime. You know how your servers are all", "tokens": [50608, 46738, 1740, 388, 433, 366, 466, 917, 12, 1353, 12, 521, 34474, 13, 509, 458, 577, 428, 15909, 366, 439, 50792], "temperature": 0.0, "avg_logprob": -0.11506424779477327, "compression_ratio": 1.74822695035461, "no_speech_prob": 0.03209669142961502}, {"id": 334, "seek": 192192, "start": 1930.48, "end": 1934.88, "text": " about end-to-end runtime? Or your browser? Like, if only your browser could quit faster.", "tokens": [50792, 466, 917, 12, 1353, 12, 521, 34474, 30, 1610, 428, 11185, 30, 1743, 11, 498, 787, 428, 11185, 727, 10366, 4663, 13, 51012], "temperature": 0.0, "avg_logprob": -0.11506424779477327, "compression_ratio": 1.74822695035461, "no_speech_prob": 0.03209669142961502}, {"id": 335, "seek": 192192, "start": 1936.8000000000002, "end": 1940.64, "text": " So again, like, it was all about, like, here's a program. I run at a console,", "tokens": [51108, 407, 797, 11, 411, 11, 309, 390, 439, 466, 11, 411, 11, 510, 311, 257, 1461, 13, 286, 1190, 412, 257, 11076, 11, 51300], "temperature": 0.0, "avg_logprob": -0.11506424779477327, "compression_ratio": 1.74822695035461, "no_speech_prob": 0.03209669142961502}, {"id": 336, "seek": 192192, "start": 1940.64, "end": 1944.64, "text": " and it does something, and it's done, and that's all I cared about. That's not really today.", "tokens": [51300, 293, 309, 775, 746, 11, 293, 309, 311, 1096, 11, 293, 300, 311, 439, 286, 19779, 466, 13, 663, 311, 406, 534, 965, 13, 51500], "temperature": 0.0, "avg_logprob": -0.11506424779477327, "compression_ratio": 1.74822695035461, "no_speech_prob": 0.03209669142961502}, {"id": 337, "seek": 192192, "start": 1945.92, "end": 1949.92, "text": " So there's latency. And then the more traditional thing is throughput. Again,", "tokens": [51564, 407, 456, 311, 27043, 13, 400, 550, 264, 544, 5164, 551, 307, 44629, 13, 3764, 11, 51764], "temperature": 0.0, "avg_logprob": -0.11506424779477327, "compression_ratio": 1.74822695035461, "no_speech_prob": 0.03209669142961502}, {"id": 338, "seek": 194992, "start": 1949.92, "end": 1953.52, "text": " this is something that profilers do a bad job of measuring because they're all about end-to-end", "tokens": [50364, 341, 307, 746, 300, 1740, 388, 433, 360, 257, 1578, 1691, 295, 13389, 570, 436, 434, 439, 466, 917, 12, 1353, 12, 521, 50544], "temperature": 0.0, "avg_logprob": -0.07915929158528646, "compression_ratio": 1.7803030303030303, "no_speech_prob": 0.0019876582082360983}, {"id": 339, "seek": 194992, "start": 1953.52, "end": 1959.8400000000001, "text": " execution time. So how fast results come back is throughput. So how are we going to do this?", "tokens": [50544, 15058, 565, 13, 407, 577, 2370, 3542, 808, 646, 307, 44629, 13, 407, 577, 366, 321, 516, 281, 360, 341, 30, 50860], "temperature": 0.0, "avg_logprob": -0.07915929158528646, "compression_ratio": 1.7803030303030303, "no_speech_prob": 0.0019876582082360983}, {"id": 340, "seek": 194992, "start": 1959.8400000000001, "end": 1965.04, "text": " So with our causal profiler that I'm going to explain in a minute, we're going to introduce", "tokens": [50860, 407, 365, 527, 38755, 1740, 5441, 300, 286, 478, 516, 281, 2903, 294, 257, 3456, 11, 321, 434, 516, 281, 5366, 51120], "temperature": 0.0, "avg_logprob": -0.07915929158528646, "compression_ratio": 1.7803030303030303, "no_speech_prob": 0.0019876582082360983}, {"id": 341, "seek": 194992, "start": 1965.04, "end": 1970.0800000000002, "text": " what we call progress points. So the notion of progress points is here's a thing I want to happen", "tokens": [51120, 437, 321, 818, 4205, 2793, 13, 407, 264, 10710, 295, 4205, 2793, 307, 510, 311, 257, 551, 286, 528, 281, 1051, 51372], "temperature": 0.0, "avg_logprob": -0.07915929158528646, "compression_ratio": 1.7803030303030303, "no_speech_prob": 0.0019876582082360983}, {"id": 342, "seek": 194992, "start": 1970.0800000000002, "end": 1976.0, "text": " faster, or here's a beginning and an end of things that I want to happen faster. So if Luke", "tokens": [51372, 4663, 11, 420, 510, 311, 257, 2863, 293, 364, 917, 295, 721, 300, 286, 528, 281, 1051, 4663, 13, 407, 498, 13044, 51668], "temperature": 0.0, "avg_logprob": -0.07915929158528646, "compression_ratio": 1.7803030303030303, "no_speech_prob": 0.0019876582082360983}, {"id": 343, "seek": 197600, "start": 1976.0, "end": 1981.6, "text": " wants responses to get sent faster, higher throughput, you just mark this particular start of this", "tokens": [50364, 2738, 13019, 281, 483, 2279, 4663, 11, 2946, 44629, 11, 291, 445, 1491, 341, 1729, 722, 295, 341, 50644], "temperature": 0.0, "avg_logprob": -0.09976365225655692, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0019876942969858646}, {"id": 344, "seek": 197600, "start": 1981.6, "end": 1987.44, "text": " component as a progress point, and every time the code runs, you go and you get another coin.", "tokens": [50644, 6542, 382, 257, 4205, 935, 11, 293, 633, 565, 264, 3089, 6676, 11, 291, 352, 293, 291, 483, 1071, 11464, 13, 50936], "temperature": 0.0, "avg_logprob": -0.09976365225655692, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0019876942969858646}, {"id": 345, "seek": 197600, "start": 1988.16, "end": 1993.52, "text": " And then you can do this simultaneously, many requests for many users, and all of these things", "tokens": [50972, 400, 550, 291, 393, 360, 341, 16561, 11, 867, 12475, 337, 867, 5022, 11, 293, 439, 295, 613, 721, 51240], "temperature": 0.0, "avg_logprob": -0.09976365225655692, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0019876942969858646}, {"id": 346, "seek": 197600, "start": 1993.52, "end": 1998.88, "text": " are incrementing some counter. So these progress points are measuring throughput, and then you", "tokens": [51240, 366, 26200, 278, 512, 5682, 13, 407, 613, 4205, 2793, 366, 13389, 44629, 11, 293, 550, 291, 51508], "temperature": 0.0, "avg_logprob": -0.09976365225655692, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0019876942969858646}, {"id": 347, "seek": 197600, "start": 1998.88, "end": 2002.96, "text": " basically are going to run the experiments and see what the effect is on the rate of those", "tokens": [51508, 1936, 366, 516, 281, 1190, 264, 12050, 293, 536, 437, 264, 1802, 307, 322, 264, 3314, 295, 729, 51712], "temperature": 0.0, "avg_logprob": -0.09976365225655692, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0019876942969858646}, {"id": 348, "seek": 200296, "start": 2002.96, "end": 2008.16, "text": " progress points being executed. So one point measures throughput. Like I said,", "tokens": [50364, 4205, 2793, 885, 17577, 13, 407, 472, 935, 8000, 44629, 13, 1743, 286, 848, 11, 50624], "temperature": 0.0, "avg_logprob": -0.06466750231656161, "compression_ratio": 1.7649402390438247, "no_speech_prob": 7.484552770620212e-05}, {"id": 349, "seek": 200296, "start": 2008.16, "end": 2012.96, "text": " if I speed up some component, whatever it might be, what is the effect? So now,", "tokens": [50624, 498, 286, 3073, 493, 512, 6542, 11, 2035, 309, 1062, 312, 11, 437, 307, 264, 1802, 30, 407, 586, 11, 50864], "temperature": 0.0, "avg_logprob": -0.06466750231656161, "compression_ratio": 1.7649402390438247, "no_speech_prob": 7.484552770620212e-05}, {"id": 350, "seek": 200296, "start": 2013.68, "end": 2018.88, "text": " what if I care about latency? So we do the exact same thing. We set a progress point at the beginning,", "tokens": [50900, 437, 498, 286, 1127, 466, 27043, 30, 407, 321, 360, 264, 1900, 912, 551, 13, 492, 992, 257, 4205, 935, 412, 264, 2863, 11, 51160], "temperature": 0.0, "avg_logprob": -0.06466750231656161, "compression_ratio": 1.7649402390438247, "no_speech_prob": 7.484552770620212e-05}, {"id": 351, "seek": 200296, "start": 2018.88, "end": 2023.44, "text": " a progress point at the end, and then the only thing that has to happen under the covers is it", "tokens": [51160, 257, 4205, 935, 412, 264, 917, 11, 293, 550, 264, 787, 551, 300, 575, 281, 1051, 833, 264, 10538, 307, 309, 51388], "temperature": 0.0, "avg_logprob": -0.06466750231656161, "compression_ratio": 1.7649402390438247, "no_speech_prob": 7.484552770620212e-05}, {"id": 352, "seek": 200296, "start": 2023.44, "end": 2027.92, "text": " has to have a counter. And the counter here measures how many things are in the system", "tokens": [51388, 575, 281, 362, 257, 5682, 13, 400, 264, 5682, 510, 8000, 577, 867, 721, 366, 294, 264, 1185, 51612], "temperature": 0.0, "avg_logprob": -0.06466750231656161, "compression_ratio": 1.7649402390438247, "no_speech_prob": 7.484552770620212e-05}, {"id": 353, "seek": 202792, "start": 2027.92, "end": 2034.8000000000002, "text": " at any one time. And it turns out that there is this awesome law that holds in a wide variety", "tokens": [50364, 412, 604, 472, 565, 13, 400, 309, 4523, 484, 300, 456, 307, 341, 3476, 2101, 300, 9190, 294, 257, 4874, 5673, 50708], "temperature": 0.0, "avg_logprob": -0.0886958122253418, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.003172426950186491}, {"id": 354, "seek": 202792, "start": 2034.8000000000002, "end": 2040.4, "text": " of circumstances called Little's law. And so Little's law says that the latency is essentially", "tokens": [50708, 295, 9121, 1219, 8022, 311, 2101, 13, 400, 370, 8022, 311, 2101, 1619, 300, 264, 27043, 307, 4476, 50988], "temperature": 0.0, "avg_logprob": -0.0886958122253418, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.003172426950186491}, {"id": 355, "seek": 202792, "start": 2040.4, "end": 2045.52, "text": " the number, the average number of transactions in a system divided by the throughput. We already", "tokens": [50988, 264, 1230, 11, 264, 4274, 1230, 295, 16856, 294, 257, 1185, 6666, 538, 264, 44629, 13, 492, 1217, 51244], "temperature": 0.0, "avg_logprob": -0.0886958122253418, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.003172426950186491}, {"id": 356, "seek": 202792, "start": 2045.52, "end": 2049.84, "text": " know how to measure throughput, so we just take advantage of Little's law and we can translate", "tokens": [51244, 458, 577, 281, 3481, 44629, 11, 370, 321, 445, 747, 5002, 295, 8022, 311, 2101, 293, 321, 393, 13799, 51460], "temperature": 0.0, "avg_logprob": -0.0886958122253418, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.003172426950186491}, {"id": 357, "seek": 202792, "start": 2049.84, "end": 2056.32, "text": " this into latency. All right, great. So we have built a causal profiler for Linux. It already", "tokens": [51460, 341, 666, 27043, 13, 1057, 558, 11, 869, 13, 407, 321, 362, 3094, 257, 38755, 1740, 5441, 337, 18734, 13, 467, 1217, 51784], "temperature": 0.0, "avg_logprob": -0.0886958122253418, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.003172426950186491}, {"id": 358, "seek": 205632, "start": 2056.32, "end": 2061.52, "text": " ships with Debian and Ubuntu, so if you're using one of those systems, you can install it quite", "tokens": [50364, 11434, 365, 1346, 20196, 293, 30230, 45605, 11, 370, 498, 291, 434, 1228, 472, 295, 729, 3652, 11, 291, 393, 3625, 309, 1596, 50624], "temperature": 0.0, "avg_logprob": -0.08617399520232898, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.00239659589715302}, {"id": 359, "seek": 205632, "start": 2061.52, "end": 2068.96, "text": " easily. So it's just cause-profiler. It's quite easy to run. So you say cause run dash dash dash", "tokens": [50624, 3612, 13, 407, 309, 311, 445, 3082, 12, 29175, 5441, 13, 467, 311, 1596, 1858, 281, 1190, 13, 407, 291, 584, 3082, 1190, 8240, 8240, 8240, 50996], "temperature": 0.0, "avg_logprob": -0.08617399520232898, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.00239659589715302}, {"id": 360, "seek": 205632, "start": 2068.96, "end": 2072.96, "text": " and whatever your program is in its arguments and it fires it off and it starts doing performance", "tokens": [50996, 293, 2035, 428, 1461, 307, 294, 1080, 12869, 293, 309, 15044, 309, 766, 293, 309, 3719, 884, 3389, 51196], "temperature": 0.0, "avg_logprob": -0.08617399520232898, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.00239659589715302}, {"id": 361, "seek": 205632, "start": 2072.96, "end": 2078.56, "text": " experiments. All right, I should add it's not entirely true. You do need to place progress", "tokens": [51196, 12050, 13, 1057, 558, 11, 286, 820, 909, 309, 311, 406, 7696, 2074, 13, 509, 360, 643, 281, 1081, 4205, 51476], "temperature": 0.0, "avg_logprob": -0.08617399520232898, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.00239659589715302}, {"id": 362, "seek": 205632, "start": 2078.56, "end": 2082.88, "text": " points. If you don't place any progress points, it will act like an ordinary profiler measuring", "tokens": [51476, 2793, 13, 759, 291, 500, 380, 1081, 604, 4205, 2793, 11, 309, 486, 605, 411, 364, 10547, 1740, 5441, 13389, 51692], "temperature": 0.0, "avg_logprob": -0.08617399520232898, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.00239659589715302}, {"id": 363, "seek": 208288, "start": 2082.88, "end": 2087.6, "text": " end-to-end execution time. But if you do put in progress points, then it will actually do its", "tokens": [50364, 917, 12, 1353, 12, 521, 15058, 565, 13, 583, 498, 291, 360, 829, 294, 4205, 2793, 11, 550, 309, 486, 767, 360, 1080, 50600], "temperature": 0.0, "avg_logprob": -0.09307513918195452, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0015977825969457626}, {"id": 364, "seek": 208288, "start": 2087.6, "end": 2093.2000000000003, "text": " magic. All right, and this is just some macro, like progress begin, progress end. All right,", "tokens": [50600, 5585, 13, 1057, 558, 11, 293, 341, 307, 445, 512, 18887, 11, 411, 4205, 1841, 11, 4205, 917, 13, 1057, 558, 11, 50880], "temperature": 0.0, "avg_logprob": -0.09307513918195452, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0015977825969457626}, {"id": 365, "seek": 208288, "start": 2093.2000000000003, "end": 2099.28, "text": " so let's apply this to Augell. All right, I didn't actually build Augell. Neither did Luke,", "tokens": [50880, 370, 718, 311, 3079, 341, 281, 316, 7181, 285, 13, 1057, 558, 11, 286, 994, 380, 767, 1322, 316, 7181, 285, 13, 23956, 630, 13044, 11, 51184], "temperature": 0.0, "avg_logprob": -0.09307513918195452, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0015977825969457626}, {"id": 366, "seek": 208288, "start": 2099.28, "end": 2102.96, "text": " but we're going to build it out of pieces like any good programmer would do. So it turns out", "tokens": [51184, 457, 321, 434, 516, 281, 1322, 309, 484, 295, 3755, 411, 604, 665, 32116, 576, 360, 13, 407, 309, 4523, 484, 51368], "temperature": 0.0, "avg_logprob": -0.09307513918195452, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0015977825969457626}, {"id": 367, "seek": 208288, "start": 2102.96, "end": 2108.48, "text": " there's this suite of parallel applications that's kind of ready-made for this task. So there's a", "tokens": [51368, 456, 311, 341, 14205, 295, 8952, 5821, 300, 311, 733, 295, 1919, 12, 10341, 337, 341, 5633, 13, 407, 456, 311, 257, 51644], "temperature": 0.0, "avg_logprob": -0.09307513918195452, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0015977825969457626}, {"id": 368, "seek": 210848, "start": 2108.56, "end": 2114.32, "text": " deduplicator that does compression. There's an image comparator. And then there's a database,", "tokens": [50368, 4172, 84, 4770, 1639, 300, 775, 19355, 13, 821, 311, 364, 3256, 6311, 1639, 13, 400, 550, 456, 311, 257, 8149, 11, 50656], "temperature": 0.0, "avg_logprob": -0.10585215783888294, "compression_ratio": 1.6120401337792643, "no_speech_prob": 0.0006263224058784544}, {"id": 369, "seek": 210848, "start": 2114.32, "end": 2120.32, "text": " SQLite. That's not in Parsec, but we'll use SQLite too. All right, great. So I'm going to show you", "tokens": [50656, 19200, 642, 13, 663, 311, 406, 294, 3457, 8159, 11, 457, 321, 603, 764, 19200, 642, 886, 13, 1057, 558, 11, 869, 13, 407, 286, 478, 516, 281, 855, 291, 50956], "temperature": 0.0, "avg_logprob": -0.10585215783888294, "compression_ratio": 1.6120401337792643, "no_speech_prob": 0.0006263224058784544}, {"id": 370, "seek": 210848, "start": 2120.32, "end": 2124.72, "text": " some fun things we did. This is a well-studied set of applications. People have already tried to", "tokens": [50956, 512, 1019, 721, 321, 630, 13, 639, 307, 257, 731, 12, 28349, 1091, 992, 295, 5821, 13, 3432, 362, 1217, 3031, 281, 51176], "temperature": 0.0, "avg_logprob": -0.10585215783888294, "compression_ratio": 1.6120401337792643, "no_speech_prob": 0.0006263224058784544}, {"id": 371, "seek": 210848, "start": 2124.72, "end": 2130.56, "text": " optimize these and we have covered a bunch of surprising optimization opportunities. So here's", "tokens": [51176, 19719, 613, 293, 321, 362, 5343, 257, 3840, 295, 8830, 19618, 4786, 13, 407, 510, 311, 51468], "temperature": 0.0, "avg_logprob": -0.10585215783888294, "compression_ratio": 1.6120401337792643, "no_speech_prob": 0.0006263224058784544}, {"id": 372, "seek": 210848, "start": 2130.56, "end": 2135.44, "text": " Ferret. This is actually an older version of what our causal profile looks like. Now it runs in a", "tokens": [51468, 10728, 1505, 13, 639, 307, 767, 364, 4906, 3037, 295, 437, 527, 38755, 7964, 1542, 411, 13, 823, 309, 6676, 294, 257, 51712], "temperature": 0.0, "avg_logprob": -0.10585215783888294, "compression_ratio": 1.6120401337792643, "no_speech_prob": 0.0006263224058784544}, {"id": 373, "seek": 213544, "start": 2135.44, "end": 2140.7200000000003, "text": " web browser. And you can see that there's these lines of code and it says, boy, if you speed up", "tokens": [50364, 3670, 11185, 13, 400, 291, 393, 536, 300, 456, 311, 613, 3876, 295, 3089, 293, 309, 1619, 11, 3237, 11, 498, 291, 3073, 493, 50628], "temperature": 0.0, "avg_logprob": -0.07142192561451982, "compression_ratio": 1.872093023255814, "no_speech_prob": 0.0022517030593007803}, {"id": 374, "seek": 213544, "start": 2140.7200000000003, "end": 2146.64, "text": " these lines of code, then you're going to get performance increases. Conveniently, these lines", "tokens": [50628, 613, 3876, 295, 3089, 11, 550, 291, 434, 516, 281, 483, 3389, 8637, 13, 45992, 1196, 356, 11, 613, 3876, 50924], "temperature": 0.0, "avg_logprob": -0.07142192561451982, "compression_ratio": 1.872093023255814, "no_speech_prob": 0.0022517030593007803}, {"id": 375, "seek": 213544, "start": 2146.64, "end": 2151.76, "text": " of code happen to be located in separate chunks of Ferret. So the part that does ranking, the part", "tokens": [50924, 295, 3089, 1051, 281, 312, 6870, 294, 4994, 24004, 295, 10728, 1505, 13, 407, 264, 644, 300, 775, 17833, 11, 264, 644, 51180], "temperature": 0.0, "avg_logprob": -0.07142192561451982, "compression_ratio": 1.872093023255814, "no_speech_prob": 0.0022517030593007803}, {"id": 376, "seek": 213544, "start": 2151.76, "end": 2156.96, "text": " that does indexing, the part that does segmentation. And why is this convenient? I'm not going to", "tokens": [51180, 300, 775, 8186, 278, 11, 264, 644, 300, 775, 9469, 399, 13, 400, 983, 307, 341, 10851, 30, 286, 478, 406, 516, 281, 51440], "temperature": 0.0, "avg_logprob": -0.07142192561451982, "compression_ratio": 1.872093023255814, "no_speech_prob": 0.0022517030593007803}, {"id": 377, "seek": 213544, "start": 2156.96, "end": 2161.92, "text": " have to change any code to make this faster. The reason is that what Ferret does is it has this", "tokens": [51440, 362, 281, 1319, 604, 3089, 281, 652, 341, 4663, 13, 440, 1778, 307, 300, 437, 10728, 1505, 775, 307, 309, 575, 341, 51688], "temperature": 0.0, "avg_logprob": -0.07142192561451982, "compression_ratio": 1.872093023255814, "no_speech_prob": 0.0022517030593007803}, {"id": 378, "seek": 216192, "start": 2161.92, "end": 2168.64, "text": " pipeline model and it assigns an equal number of threads to every stage in the pipeline. But", "tokens": [50364, 15517, 2316, 293, 309, 6269, 82, 364, 2681, 1230, 295, 19314, 281, 633, 3233, 294, 264, 15517, 13, 583, 50700], "temperature": 0.0, "avg_logprob": -0.1195779308196037, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0010004577925428748}, {"id": 379, "seek": 216192, "start": 2168.64, "end": 2174.7200000000003, "text": " it turns out this one really doesn't need that many threads. So we take away the threads and just", "tokens": [50700, 309, 4523, 484, 341, 472, 534, 1177, 380, 643, 300, 867, 19314, 13, 407, 321, 747, 1314, 264, 19314, 293, 445, 51004], "temperature": 0.0, "avg_logprob": -0.1195779308196037, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0010004577925428748}, {"id": 380, "seek": 216192, "start": 2174.7200000000003, "end": 2181.84, "text": " by reassigning threads, we got a 20% speedup. So this is pretty cool because Caus actually", "tokens": [51004, 538, 19486, 9676, 19314, 11, 321, 658, 257, 945, 4, 3073, 1010, 13, 407, 341, 307, 1238, 1627, 570, 7544, 301, 767, 51360], "temperature": 0.0, "avg_logprob": -0.1195779308196037, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0010004577925428748}, {"id": 381, "seek": 216192, "start": 2181.84, "end": 2187.6, "text": " predicted it perfectly. So we increased ranking, for example, from 16 to 22 threads. That's a", "tokens": [51360, 19147, 309, 6239, 13, 407, 321, 6505, 17833, 11, 337, 1365, 11, 490, 3165, 281, 5853, 19314, 13, 663, 311, 257, 51648], "temperature": 0.0, "avg_logprob": -0.1195779308196037, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0010004577925428748}, {"id": 382, "seek": 218760, "start": 2187.68, "end": 2194.64, "text": " 27% increase in throughput. And on the graph, that says that that would translate to a 21%", "tokens": [50368, 7634, 4, 3488, 294, 44629, 13, 400, 322, 264, 4295, 11, 300, 1619, 300, 300, 576, 13799, 281, 257, 5080, 4, 50716], "temperature": 0.0, "avg_logprob": -0.11179891446741616, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.0004442089411895722}, {"id": 383, "seek": 218760, "start": 2194.64, "end": 2203.36, "text": " overall improvement. And that's what we got. So Caus actually works. Good. So we were pretty happy", "tokens": [50716, 4787, 10444, 13, 400, 300, 311, 437, 321, 658, 13, 407, 7544, 301, 767, 1985, 13, 2205, 13, 407, 321, 645, 1238, 2055, 51152], "temperature": 0.0, "avg_logprob": -0.11179891446741616, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.0004442089411895722}, {"id": 384, "seek": 218760, "start": 2203.36, "end": 2211.36, "text": " with this. We then are going to move on to Ddupe. So Ddupe is pretty hilarious. So here's Ddupe in", "tokens": [51152, 365, 341, 13, 492, 550, 366, 516, 281, 1286, 322, 281, 413, 769, 494, 13, 407, 413, 769, 494, 307, 1238, 19796, 13, 407, 510, 311, 413, 769, 494, 294, 51552], "temperature": 0.0, "avg_logprob": -0.11179891446741616, "compression_ratio": 1.5078534031413613, "no_speech_prob": 0.0004442089411895722}, {"id": 385, "seek": 221136, "start": 2211.36, "end": 2217.76, "text": " action. I have two pictures, Grumpy Cat 1 and Grumpy Cat Meme. And so now what do I do to", "tokens": [50364, 3069, 13, 286, 362, 732, 5242, 11, 2606, 36142, 9565, 502, 293, 2606, 36142, 9565, 376, 5729, 13, 400, 370, 586, 437, 360, 286, 360, 281, 50684], "temperature": 0.0, "avg_logprob": -0.09712233284647151, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.040841761976480484}, {"id": 386, "seek": 221136, "start": 2217.76, "end": 2222.7200000000003, "text": " deduplicate these things? You can see that there's chunks that are the same. So you carve out the", "tokens": [50684, 4172, 84, 4770, 473, 613, 721, 30, 509, 393, 536, 300, 456, 311, 24004, 300, 366, 264, 912, 13, 407, 291, 33832, 484, 264, 50932], "temperature": 0.0, "avg_logprob": -0.09712233284647151, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.040841761976480484}, {"id": 387, "seek": 221136, "start": 2222.7200000000003, "end": 2228.08, "text": " chunks that are the same and you separate them out into individual pieces. And then an image is now", "tokens": [50932, 24004, 300, 366, 264, 912, 293, 291, 4994, 552, 484, 666, 2609, 3755, 13, 400, 550, 364, 3256, 307, 586, 51200], "temperature": 0.0, "avg_logprob": -0.09712233284647151, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.040841761976480484}, {"id": 388, "seek": 221136, "start": 2228.08, "end": 2235.2000000000003, "text": " represented by the bits and pieces that make up the image. So here Grumpy Cat 1 is this piece", "tokens": [51200, 10379, 538, 264, 9239, 293, 3755, 300, 652, 493, 264, 3256, 13, 407, 510, 2606, 36142, 9565, 502, 307, 341, 2522, 51556], "temperature": 0.0, "avg_logprob": -0.09712233284647151, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.040841761976480484}, {"id": 389, "seek": 221136, "start": 2235.2000000000003, "end": 2240.4, "text": " and Fun Is Awful is this piece. And you saved a lot of memory. So that's what Ddupe does.", "tokens": [51556, 293, 11166, 1119, 6381, 906, 307, 341, 2522, 13, 400, 291, 6624, 257, 688, 295, 4675, 13, 407, 300, 311, 437, 413, 769, 494, 775, 13, 51816], "temperature": 0.0, "avg_logprob": -0.09712233284647151, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.040841761976480484}, {"id": 390, "seek": 224040, "start": 2241.04, "end": 2246.1600000000003, "text": " So it does this compression via deduplication and it uses a hash function. So it throws everything", "tokens": [50396, 407, 309, 775, 341, 19355, 5766, 4172, 84, 4770, 399, 293, 309, 4960, 257, 22019, 2445, 13, 407, 309, 19251, 1203, 50652], "temperature": 0.0, "avg_logprob": -0.07272327832939211, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00022340702707879245}, {"id": 391, "seek": 224040, "start": 2246.1600000000003, "end": 2251.36, "text": " through a hash table. Great. So it's a pretty standard hash table. You just have some hash", "tokens": [50652, 807, 257, 22019, 3199, 13, 3769, 13, 407, 309, 311, 257, 1238, 3832, 22019, 3199, 13, 509, 445, 362, 512, 22019, 50912], "temperature": 0.0, "avg_logprob": -0.07272327832939211, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00022340702707879245}, {"id": 392, "seek": 224040, "start": 2251.36, "end": 2256.7200000000003, "text": " table. It's an array. It's a bunch of bins. You get a bin number and then you go and you start", "tokens": [50912, 3199, 13, 467, 311, 364, 10225, 13, 467, 311, 257, 3840, 295, 41275, 13, 509, 483, 257, 5171, 1230, 293, 550, 291, 352, 293, 291, 722, 51180], "temperature": 0.0, "avg_logprob": -0.07272327832939211, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00022340702707879245}, {"id": 393, "seek": 224040, "start": 2256.7200000000003, "end": 2263.12, "text": " adding stuff to that bin into the bucket. So this all seems straightforward. You hope that it would", "tokens": [51180, 5127, 1507, 281, 300, 5171, 666, 264, 13058, 13, 407, 341, 439, 2544, 15325, 13, 509, 1454, 300, 309, 576, 51500], "temperature": 0.0, "avg_logprob": -0.07272327832939211, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00022340702707879245}, {"id": 394, "seek": 224040, "start": 2263.12, "end": 2269.52, "text": " do something like this. The hash table is accessed concurrently by a bunch of threads, but they're", "tokens": [51500, 360, 746, 411, 341, 13, 440, 22019, 3199, 307, 34211, 37702, 356, 538, 257, 3840, 295, 19314, 11, 457, 436, 434, 51820], "temperature": 0.0, "avg_logprob": -0.07272327832939211, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.00022340702707879245}, {"id": 395, "seek": 226952, "start": 2269.52, "end": 2275.7599999999998, "text": " not idiots. There's not one big lock. It's just all locks, which is naive, but it's fine. But", "tokens": [50364, 406, 36454, 13, 821, 311, 406, 472, 955, 4017, 13, 467, 311, 445, 439, 20703, 11, 597, 307, 29052, 11, 457, 309, 311, 2489, 13, 583, 50676], "temperature": 0.0, "avg_logprob": -0.0858224388060531, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.00018522015307098627}, {"id": 396, "seek": 226952, "start": 2275.7599999999998, "end": 2282.16, "text": " surprisingly, cause says that the loop that accesses this list is important. Now, if you know", "tokens": [50676, 17600, 11, 3082, 1619, 300, 264, 6367, 300, 2105, 279, 341, 1329, 307, 1021, 13, 823, 11, 498, 291, 458, 50996], "temperature": 0.0, "avg_logprob": -0.0858224388060531, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.00018522015307098627}, {"id": 397, "seek": 226952, "start": 2282.16, "end": 2287.44, "text": " anything about hash tables, you know that things generally end up balanced, right? And it's weird", "tokens": [50996, 1340, 466, 22019, 8020, 11, 291, 458, 300, 721, 5101, 917, 493, 13902, 11, 558, 30, 400, 309, 311, 3657, 51260], "temperature": 0.0, "avg_logprob": -0.0858224388060531, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.00018522015307098627}, {"id": 398, "seek": 226952, "start": 2287.44, "end": 2291.84, "text": " that you have this sort of situation. So we thought, all right, well, let's just make more,", "tokens": [51260, 300, 291, 362, 341, 1333, 295, 2590, 13, 407, 321, 1194, 11, 439, 558, 11, 731, 11, 718, 311, 445, 652, 544, 11, 51480], "temperature": 0.0, "avg_logprob": -0.0858224388060531, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.00018522015307098627}, {"id": 399, "seek": 226952, "start": 2291.84, "end": 2296.56, "text": " more hash buckets, right? But we made a thousand of them. We really should have made a million,", "tokens": [51480, 544, 22019, 32191, 11, 558, 30, 583, 321, 1027, 257, 4714, 295, 552, 13, 492, 534, 820, 362, 1027, 257, 2459, 11, 51716], "temperature": 0.0, "avg_logprob": -0.0858224388060531, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.00018522015307098627}, {"id": 400, "seek": 229656, "start": 2296.56, "end": 2302.96, "text": " cause, you know, a million. But anyway. So you would think this would lead to fewer", "tokens": [50364, 3082, 11, 291, 458, 11, 257, 2459, 13, 583, 4033, 13, 407, 291, 576, 519, 341, 576, 1477, 281, 13366, 50684], "temperature": 0.0, "avg_logprob": -0.15033462312486437, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.0001686486939433962}, {"id": 401, "seek": 229656, "start": 2302.96, "end": 2309.12, "text": " collisions, but it had no effect, right? So what else could be causing the collisions? Any guesses?", "tokens": [50684, 46537, 11, 457, 309, 632, 572, 1802, 11, 558, 30, 407, 437, 1646, 727, 312, 9853, 264, 46537, 30, 2639, 42703, 30, 50992], "temperature": 0.0, "avg_logprob": -0.15033462312486437, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.0001686486939433962}, {"id": 402, "seek": 229656, "start": 2310.48, "end": 2316.32, "text": " The hash function, exactly. Like this is one of those when all other possibilities have been", "tokens": [51060, 440, 22019, 2445, 11, 2293, 13, 1743, 341, 307, 472, 295, 729, 562, 439, 661, 12178, 362, 668, 51352], "temperature": 0.0, "avg_logprob": -0.15033462312486437, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.0001686486939433962}, {"id": 403, "seek": 229656, "start": 2316.32, "end": 2320.88, "text": " exhausted, right? You pick the weirdest one. That's not an exact quote. But anyway,", "tokens": [51352, 17992, 11, 558, 30, 509, 1888, 264, 44807, 472, 13, 663, 311, 406, 364, 1900, 6513, 13, 583, 4033, 11, 51580], "temperature": 0.0, "avg_logprob": -0.15033462312486437, "compression_ratio": 1.5584415584415585, "no_speech_prob": 0.0001686486939433962}, {"id": 404, "seek": 232088, "start": 2321.84, "end": 2326.32, "text": " well, you're like, how can the hash function be broken? Like we've been using hash functions", "tokens": [50412, 731, 11, 291, 434, 411, 11, 577, 393, 264, 22019, 2445, 312, 5463, 30, 1743, 321, 600, 668, 1228, 22019, 6828, 50636], "temperature": 0.0, "avg_logprob": -0.12722877502441407, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.002323027467355132}, {"id": 405, "seek": 232088, "start": 2326.32, "end": 2331.2000000000003, "text": " since before Canuth wrote about them. Well, turns out people like to roll their own, cause it's", "tokens": [50636, 1670, 949, 1664, 2910, 4114, 466, 552, 13, 1042, 11, 4523, 484, 561, 411, 281, 3373, 641, 1065, 11, 3082, 309, 311, 50880], "temperature": 0.0, "avg_logprob": -0.12722877502441407, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.002323027467355132}, {"id": 406, "seek": 232088, "start": 2331.2000000000003, "end": 2338.56, "text": " fun. And so we did a histogram of the number of items per bucket. So again, I told you there's", "tokens": [50880, 1019, 13, 400, 370, 321, 630, 257, 49816, 295, 264, 1230, 295, 4754, 680, 13058, 13, 407, 797, 11, 286, 1907, 291, 456, 311, 51248], "temperature": 0.0, "avg_logprob": -0.12722877502441407, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.002323027467355132}, {"id": 407, "seek": 232088, "start": 2338.56, "end": 2349.6, "text": " a thousand buckets. This is the histogram. Yeah. Hilariously, what they did is they used the pixels", "tokens": [51248, 257, 4714, 32191, 13, 639, 307, 264, 49816, 13, 865, 13, 389, 2202, 8994, 11, 437, 436, 630, 307, 436, 1143, 264, 18668, 51800], "temperature": 0.0, "avg_logprob": -0.12722877502441407, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.002323027467355132}, {"id": 408, "seek": 234960, "start": 2350.24, "end": 2355.92, "text": " that were taken from the image and they sum number of pixels and then added them. But that's", "tokens": [50396, 300, 645, 2726, 490, 264, 3256, 293, 436, 2408, 1230, 295, 18668, 293, 550, 3869, 552, 13, 583, 300, 311, 50680], "temperature": 0.0, "avg_logprob": -0.11479959704659203, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.001597708323970437}, {"id": 409, "seek": 234960, "start": 2355.92, "end": 2361.68, "text": " actually the central limit theorem in action, right? They're random, right? They're independent,", "tokens": [50680, 767, 264, 5777, 4948, 20904, 294, 3069, 11, 558, 30, 814, 434, 4974, 11, 558, 30, 814, 434, 6695, 11, 50968], "temperature": 0.0, "avg_logprob": -0.11479959704659203, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.001597708323970437}, {"id": 410, "seek": 234960, "start": 2361.68, "end": 2366.4, "text": " right? And you've summed them together. And so they actually formed the normal distribution.", "tokens": [50968, 558, 30, 400, 291, 600, 2408, 1912, 552, 1214, 13, 400, 370, 436, 767, 8693, 264, 2710, 7316, 13, 51204], "temperature": 0.0, "avg_logprob": -0.11479959704659203, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.001597708323970437}, {"id": 411, "seek": 234960, "start": 2366.4, "end": 2370.72, "text": " That's not the distribution you want for a hash table. You would like a uniform distribution.", "tokens": [51204, 663, 311, 406, 264, 7316, 291, 528, 337, 257, 22019, 3199, 13, 509, 576, 411, 257, 9452, 7316, 13, 51420], "temperature": 0.0, "avg_logprob": -0.11479959704659203, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.001597708323970437}, {"id": 412, "seek": 237072, "start": 2371.3599999999997, "end": 2380.08, "text": " So literally, we changed one character. We changed the plus to XOR. So this. And we got this.", "tokens": [50396, 407, 3736, 11, 321, 3105, 472, 2517, 13, 492, 3105, 264, 1804, 281, 1783, 2483, 13, 407, 341, 13, 400, 321, 658, 341, 13, 50832], "temperature": 0.0, "avg_logprob": -0.20036865535535311, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.01615143194794655}, {"id": 413, "seek": 237072, "start": 2388.08, "end": 2392.56, "text": " So, okay. I'll take the applause, but I mean, it was only a 9% speedup. But", "tokens": [51232, 407, 11, 1392, 13, 286, 603, 747, 264, 9969, 11, 457, 286, 914, 11, 309, 390, 787, 257, 1722, 4, 3073, 1010, 13, 583, 51456], "temperature": 0.0, "avg_logprob": -0.20036865535535311, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.01615143194794655}, {"id": 414, "seek": 237072, "start": 2393.7599999999998, "end": 2398.16, "text": " all right. Nonetheless, it was one character. So I think it's the biggest bang for buck", "tokens": [51516, 439, 558, 13, 45437, 11, 309, 390, 472, 2517, 13, 407, 286, 519, 309, 311, 264, 3880, 8550, 337, 14894, 51736], "temperature": 0.0, "avg_logprob": -0.20036865535535311, "compression_ratio": 1.4357541899441342, "no_speech_prob": 0.01615143194794655}, {"id": 415, "seek": 239816, "start": 2398.16, "end": 2402.7999999999997, "text": " ever recorded in optimization effort. So what did it predict? It turned out we can", "tokens": [50364, 1562, 8287, 294, 19618, 4630, 13, 407, 437, 630, 309, 6069, 30, 467, 3574, 484, 321, 393, 50596], "temperature": 0.0, "avg_logprob": -0.09486221131824311, "compression_ratio": 1.5813148788927336, "no_speech_prob": 0.0008830056176520884}, {"id": 416, "seek": 239816, "start": 2402.7999999999997, "end": 2408.08, "text": " also measure the accuracy of the prediction. So we knew that the blocks per bucket went from 76-ish", "tokens": [50596, 611, 3481, 264, 14170, 295, 264, 17630, 13, 407, 321, 2586, 300, 264, 8474, 680, 13058, 1437, 490, 24733, 12, 742, 50860], "temperature": 0.0, "avg_logprob": -0.09486221131824311, "compression_ratio": 1.5813148788927336, "no_speech_prob": 0.0008830056176520884}, {"id": 417, "seek": 239816, "start": 2408.08, "end": 2414.08, "text": " to 2. That's a 96% traversal speedup. And again, going back to the causal graph,", "tokens": [50860, 281, 568, 13, 663, 311, 257, 24124, 4, 23149, 304, 3073, 1010, 13, 400, 797, 11, 516, 646, 281, 264, 38755, 4295, 11, 51160], "temperature": 0.0, "avg_logprob": -0.09486221131824311, "compression_ratio": 1.5813148788927336, "no_speech_prob": 0.0008830056176520884}, {"id": 418, "seek": 239816, "start": 2414.08, "end": 2419.12, "text": " it predicted a 9% speedup, which is what we got, right? So it's working. All right. So finally,", "tokens": [51160, 309, 19147, 257, 1722, 4, 3073, 1010, 11, 597, 307, 437, 321, 658, 11, 558, 30, 407, 309, 311, 1364, 13, 1057, 558, 13, 407, 2721, 11, 51412], "temperature": 0.0, "avg_logprob": -0.09486221131824311, "compression_ratio": 1.5813148788927336, "no_speech_prob": 0.0008830056176520884}, {"id": 419, "seek": 239816, "start": 2419.12, "end": 2426.3199999999997, "text": " I'm going to talk about SQLite. So I have no time left. But SQLite is pretty awesome. It's widely", "tokens": [51412, 286, 478, 516, 281, 751, 466, 19200, 642, 13, 407, 286, 362, 572, 565, 1411, 13, 583, 19200, 642, 307, 1238, 3476, 13, 467, 311, 13371, 51772], "temperature": 0.0, "avg_logprob": -0.09486221131824311, "compression_ratio": 1.5813148788927336, "no_speech_prob": 0.0008830056176520884}, {"id": 420, "seek": 242632, "start": 2426.32, "end": 2432.88, "text": " used, as you all know. But it has this weird thing where it has a kind of strange virtual table", "tokens": [50364, 1143, 11, 382, 291, 439, 458, 13, 583, 309, 575, 341, 3657, 551, 689, 309, 575, 257, 733, 295, 5861, 6374, 3199, 50692], "temperature": 0.0, "avg_logprob": -0.10334686279296874, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.009124644100666046}, {"id": 421, "seek": 242632, "start": 2432.88, "end": 2438.8, "text": " that they set up at compile time. And so whenever you actually indirect through a config to execute", "tokens": [50692, 300, 436, 992, 493, 412, 31413, 565, 13, 400, 370, 5699, 291, 767, 19523, 807, 257, 6662, 281, 14483, 50988], "temperature": 0.0, "avg_logprob": -0.10334686279296874, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.009124644100666046}, {"id": 422, "seek": 242632, "start": 2438.8, "end": 2444.0, "text": " a function like pthreadmutexunlock. So you would think, all right, why are you telling me about this?", "tokens": [50988, 257, 2445, 411, 280, 392, 2538, 76, 1169, 87, 409, 4102, 13, 407, 291, 576, 519, 11, 439, 558, 11, 983, 366, 291, 3585, 385, 466, 341, 30, 51248], "temperature": 0.0, "avg_logprob": -0.10334686279296874, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.009124644100666046}, {"id": 423, "seek": 242632, "start": 2444.0, "end": 2449.44, "text": " Well, everything looks like this. This is an indirect call. Could be a direct call. That would", "tokens": [51248, 1042, 11, 1203, 1542, 411, 341, 13, 639, 307, 364, 19523, 818, 13, 7497, 312, 257, 2047, 818, 13, 663, 576, 51520], "temperature": 0.0, "avg_logprob": -0.10334686279296874, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.009124644100666046}, {"id": 424, "seek": 244944, "start": 2449.44, "end": 2457.44, "text": " be faster. But an indirect call is not that slow. But it's almost the same cost as pthreadmutexunlock,", "tokens": [50364, 312, 4663, 13, 583, 364, 19523, 818, 307, 406, 300, 2964, 13, 583, 309, 311, 1920, 264, 912, 2063, 382, 280, 392, 2538, 76, 1169, 87, 409, 4102, 11, 50764], "temperature": 0.0, "avg_logprob": -0.076134983374148, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.018544601276516914}, {"id": 425, "seek": 244944, "start": 2457.44, "end": 2461.04, "text": " which means that you just doubled the length of all of your critical sections.", "tokens": [50764, 597, 1355, 300, 291, 445, 24405, 264, 4641, 295, 439, 295, 428, 4924, 10863, 13, 50944], "temperature": 0.0, "avg_logprob": -0.076134983374148, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.018544601276516914}, {"id": 426, "seek": 244944, "start": 2461.92, "end": 2468.16, "text": " So that's not great. So in fact, when you go, so cause will highlight all of these lines and say,", "tokens": [50988, 407, 300, 311, 406, 869, 13, 407, 294, 1186, 11, 562, 291, 352, 11, 370, 3082, 486, 5078, 439, 295, 613, 3876, 293, 584, 11, 51300], "temperature": 0.0, "avg_logprob": -0.076134983374148, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.018544601276516914}, {"id": 427, "seek": 244944, "start": 2468.16, "end": 2476.64, "text": " you should definitely optimize these. So we undid all of the actual indirect stuff and just made", "tokens": [51300, 291, 820, 2138, 19719, 613, 13, 407, 321, 674, 327, 439, 295, 264, 3539, 19523, 1507, 293, 445, 1027, 51724], "temperature": 0.0, "avg_logprob": -0.076134983374148, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.018544601276516914}, {"id": 428, "seek": 247664, "start": 2476.72, "end": 2482.72, "text": " it so that at compile time, you change SQLite unlock to something so it doesn't do the indirect.", "tokens": [50368, 309, 370, 300, 412, 31413, 565, 11, 291, 1319, 19200, 642, 11634, 281, 746, 370, 309, 1177, 380, 360, 264, 19523, 13, 50668], "temperature": 0.0, "avg_logprob": -0.1008118894911304, "compression_ratio": 1.4593495934959348, "no_speech_prob": 0.00017952283087652177}, {"id": 429, "seek": 247664, "start": 2482.72, "end": 2490.3199999999997, "text": " And it sped things up by 25%. If you look at a traditional profiler, by the way,", "tokens": [50668, 400, 309, 637, 292, 721, 493, 538, 3552, 6856, 759, 291, 574, 412, 257, 5164, 1740, 5441, 11, 538, 264, 636, 11, 51048], "temperature": 0.0, "avg_logprob": -0.1008118894911304, "compression_ratio": 1.4593495934959348, "no_speech_prob": 0.00017952283087652177}, {"id": 430, "seek": 247664, "start": 2490.3199999999997, "end": 2495.6, "text": " those things are like, this takes 0.0001% of time. You would never consider actually", "tokens": [51048, 729, 721, 366, 411, 11, 341, 2516, 1958, 13, 1360, 16, 4, 295, 565, 13, 509, 576, 1128, 1949, 767, 51312], "temperature": 0.0, "avg_logprob": -0.1008118894911304, "compression_ratio": 1.4593495934959348, "no_speech_prob": 0.00017952283087652177}, {"id": 431, "seek": 247664, "start": 2495.6, "end": 2500.48, "text": " trying to optimize that code. So we did it for a bunch of programs. We got some crazy speed ups.", "tokens": [51312, 1382, 281, 19719, 300, 3089, 13, 407, 321, 630, 309, 337, 257, 3840, 295, 4268, 13, 492, 658, 512, 3219, 3073, 15497, 13, 51556], "temperature": 0.0, "avg_logprob": -0.1008118894911304, "compression_ratio": 1.4593495934959348, "no_speech_prob": 0.00017952283087652177}, {"id": 432, "seek": 250048, "start": 2500.48, "end": 2506.8, "text": " My favorite is we got a 68% speed up by replacing a custom barrier with a standard barrier.", "tokens": [50364, 1222, 2954, 307, 321, 658, 257, 23317, 4, 3073, 493, 538, 19139, 257, 2375, 13357, 365, 257, 3832, 13357, 13, 50680], "temperature": 0.0, "avg_logprob": -0.08268863504583185, "compression_ratio": 1.473469387755102, "no_speech_prob": 0.0003740810207091272}, {"id": 433, "seek": 250048, "start": 2507.68, "end": 2515.36, "text": " Again, people should stop doing things at home. So anyway, so I'm going to conclude.", "tokens": [50724, 3764, 11, 561, 820, 1590, 884, 721, 412, 1280, 13, 407, 4033, 11, 370, 286, 478, 516, 281, 16886, 13, 51108], "temperature": 0.0, "avg_logprob": -0.08268863504583185, "compression_ratio": 1.473469387755102, "no_speech_prob": 0.0003740810207091272}, {"id": 434, "seek": 250048, "start": 2515.36, "end": 2520.8, "text": " So you can take a picture of this to jump to work from our lab, which is plasmaumass.org.", "tokens": [51108, 407, 291, 393, 747, 257, 3036, 295, 341, 281, 3012, 281, 589, 490, 527, 2715, 11, 597, 307, 22564, 449, 640, 13, 4646, 13, 51380], "temperature": 0.0, "avg_logprob": -0.08268863504583185, "compression_ratio": 1.473469387755102, "no_speech_prob": 0.0003740810207091272}, {"id": 435, "seek": 250048, "start": 2520.8, "end": 2525.6, "text": " I talked today about sound performance analysis and effective performance profiling. Everybody", "tokens": [51380, 286, 2825, 965, 466, 1626, 3389, 5215, 293, 4942, 3389, 1740, 4883, 13, 7646, 51620], "temperature": 0.0, "avg_logprob": -0.08268863504583185, "compression_ratio": 1.473469387755102, "no_speech_prob": 0.0003740810207091272}, {"id": 436, "seek": 252560, "start": 2525.6, "end": 2528.72, "text": " should go use the cause. All right. Thanks for your attention.", "tokens": [50364, 820, 352, 764, 264, 3082, 13, 1057, 558, 13, 2561, 337, 428, 3202, 13, 50520], "temperature": 0.0, "avg_logprob": -0.33093901241526885, "compression_ratio": 0.9253731343283582, "no_speech_prob": 0.004981301259249449}], "language": "en"}