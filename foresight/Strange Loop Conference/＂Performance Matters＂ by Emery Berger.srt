1
00:00:00,000 --> 00:00:10,800
Hi everybody, so I'm Emory Berger. I'm going to be talking today about performance matters.

2
00:00:10,800 --> 00:00:15,480
This is joint work with my former PhD student, Charlie Kersinger. So I'm going to tell you

3
00:00:15,480 --> 00:00:23,400
a story that happened a short time ago in a valley far, far away. Not actually this one,

4
00:00:23,400 --> 00:00:31,720
in fact, this one. So the story is about a character who we're going to call Luke. Luke is

5
00:00:31,720 --> 00:00:37,440
our hero. Of course he's a hero in Silicon Valley because Luke has an idea for an app. Alright,

6
00:00:37,440 --> 00:00:43,440
so here's Luke's genius idea for an app. Basically you use your smartphone and you take pictures

7
00:00:43,440 --> 00:00:49,680
of things and then it goes and finds matches like and returns them to you and he's got a great name

8
00:00:49,720 --> 00:00:55,000
for it as well. It's going to call it Ogil. And he's pretty sure Ogil is going to totally

9
00:00:55,000 --> 00:01:00,840
disrupt image search. Nobody has told him about Google image search yet, but anyway. So he sets

10
00:01:00,840 --> 00:01:06,600
about building it. So he makes the prototype Ogil. So the prototype Ogil works like this. Take a

11
00:01:06,600 --> 00:01:14,440
picture. It gets sent off to the Ogil data center. If it is new, it gets added to the database. And

12
00:01:14,440 --> 00:01:19,800
then at the same time it goes and finds similar pictures and sends them back to the user. Alright,

13
00:01:19,800 --> 00:01:24,960
so this is pretty straightforward. You can kind of abstract out this flow. If you think of it as

14
00:01:24,960 --> 00:01:30,240
follows, you get a request. The request comes in. It essentially spawns two threads. One of them

15
00:01:30,240 --> 00:01:35,760
goes and takes the image and let's say compresses it and then saves it to a three and a half inch

16
00:01:35,760 --> 00:01:43,360
floppy as one does. And then at the same time it does some indexing to look up matches, right,

17
00:01:43,400 --> 00:01:49,200
so it does a search after doing some feature extraction and then sends it back to the user via

18
00:01:49,200 --> 00:01:54,360
paper airplane, which is the most effective method known to man. And then eventually these two threads

19
00:01:54,360 --> 00:01:59,240
join up. Alright, so this is obviously a simplification. I've alighted certain things like

20
00:01:59,240 --> 00:02:05,320
for example locks. So when you have locks, right, there's some database for example, you can't be

21
00:02:05,320 --> 00:02:09,360
modifying the database simultaneously, blah, blah, blah. But anyway, this is essentially at a

22
00:02:09,360 --> 00:02:16,320
high level how Ogil is going to work. So Luke goes and ships it off to the app store. It gets

23
00:02:16,320 --> 00:02:21,520
approved and you know users start coming and Ogil is working, right? He's disrupting image search.

24
00:02:21,520 --> 00:02:28,840
But then as the number of users kind of like mount, it turns out Ogil starts getting slow,

25
00:02:28,840 --> 00:02:34,440
right? So it takes a long time for Ogil to load, right? So this is too bad. He's not happy about

26
00:02:34,480 --> 00:02:40,600
this. He's not really sure what to do. So, but before we get into that, let me talk about another

27
00:02:40,600 --> 00:02:45,400
version of Luke. This is also Luke. This is the cool hand Luke, if you will, with his glasses.

28
00:02:45,400 --> 00:02:50,960
Alright, but really this is Luke in the 80s. Alright, so he's got the 80s glasses to match and

29
00:02:50,960 --> 00:02:56,280
of course those of you old enough to know or who have watched TV know that there were no smartphones

30
00:02:56,280 --> 00:03:02,040
in the 80s. There were instead computers with big CRTs. So this is what Ogil looked like in the 80s.

31
00:03:03,040 --> 00:03:07,640
So you'd have some ASCII art and you want to go out and it would connect via some modem,

32
00:03:07,640 --> 00:03:14,400
let's say, search the database. This is version 1.0 of Ogil 84. And it comes up with its matches and

33
00:03:14,400 --> 00:03:20,120
this is your user interface. So I hope you liked it. Alright, now of course back in the day,

34
00:03:20,120 --> 00:03:27,800
1984, computers were really slow, right? Really slow. And so actually this Luke today who was

35
00:03:27,840 --> 00:03:34,040
like Ogil is too slow, well things were really bad in 1984, right? But it turns out things were

36
00:03:34,040 --> 00:03:39,600
also kind of better in a way. So performance used to be really easy. So I'm sure all of you have

37
00:03:39,600 --> 00:03:44,320
seen this kind of graph. What I'm going to show you is on the left, the number of transistors,

38
00:03:44,320 --> 00:03:51,520
as years progress on the x-axis, if they go up, you'll notice it is a log scale. And on the right

39
00:03:51,520 --> 00:03:57,200
you have clock speed, which roughly corresponds to computing performance. And so basically for

40
00:03:57,240 --> 00:04:01,560
years and years and years, we had this situation where the number of transistors were increasing

41
00:04:01,560 --> 00:04:07,520
roughly at doubling every 18 months. This is famously Moore's law. And this generally had the

42
00:04:07,520 --> 00:04:13,040
effect of allowing clock speed to increase. And so you had smaller features. This meant that your

43
00:04:13,040 --> 00:04:20,000
programs would actually run faster. And in fact, you could just literally wait, right? If you buy

44
00:04:20,000 --> 00:04:25,520
new hardware, in fact to the upgrade cycle every year or so, like if you bought a new computer,

45
00:04:26,000 --> 00:04:31,160
everything would run much faster. Now things were slow, right? So don't be so excited about how

46
00:04:31,160 --> 00:04:37,720
great it was back then. It was bad. 33 megahertz to 66 was awesome back then and kind of terrible

47
00:04:37,720 --> 00:04:43,720
today. But it did change the landscape of what it meant to be somebody who works on performance

48
00:04:43,720 --> 00:04:52,400
improvement because this is what you would do. So there really, in a way, was no sense trying to

49
00:04:52,400 --> 00:04:57,160
squeeze out any performance out of something when it was going to double in 18 months, right? The

50
00:04:57,160 --> 00:05:01,680
chance that you in 18 months were going to squeeze out double the performance pretty slender. So

51
00:05:01,680 --> 00:05:06,960
you might as well just sip mojitos on the beach, all right? I can tell you that this was, well,

52
00:05:06,960 --> 00:05:12,800
maybe the mojitos and beach part, notwithstanding, this was actually kind of a strategy that was

53
00:05:12,800 --> 00:05:17,680
recommended for high performance computing. People would literally say, we're just going to upgrade

54
00:05:18,000 --> 00:05:23,200
the computers next year or in two years. So focus on something else. Don't focus on performance,

55
00:05:23,200 --> 00:05:30,920
right? Now, unfortunately, that's not the current state of affairs, right? So today, as all of you

56
00:05:30,920 --> 00:05:36,880
know, when you upgrade a computer, it does not get faster anymore. Maybe it gets lighter. Maybe it

57
00:05:36,880 --> 00:05:47,040
has improved battery life. Maybe not. But basically what happened is eventually Moore's law kind of

58
00:05:47,040 --> 00:05:52,080
ran out of steam. The reason that it did is not actually true that Moore's law ran out of steam.

59
00:05:52,080 --> 00:05:59,760
This is technically a problem called denard scaling. And so denard scaling ran out. And so right

60
00:05:59,760 --> 00:06:05,120
now, we're in a situation where basically, if we clock things up, we no longer can dissipate the

61
00:06:05,120 --> 00:06:10,560
heat on chips effectively enough to make them run faster. So transistor counts actually are still

62
00:06:10,560 --> 00:06:15,600
increasing, which is why we have multiple cores. So we have a bunch of transistors. What are we

63
00:06:15,680 --> 00:06:21,040
going to do with them? Let's just stamp out more of the same thing, right? So that's great. But it's

64
00:06:21,040 --> 00:06:25,440
also not super awesome because if you have a program that is not parallel, it's not going to run any

65
00:06:25,440 --> 00:06:33,040
faster. So everything now has multicore. Your phone has multicore. But it turns out it's still a

66
00:06:33,040 --> 00:06:41,280
problem. So these are actual screenshots from app store updates. Every app store update practically

67
00:06:41,280 --> 00:06:48,880
is about performance or bug fixes. And so here's one that says under the hood updates for better

68
00:06:48,880 --> 00:06:55,760
performance, as opposed to the over the hood updates. Okay. Here's bug fixes and performance

69
00:06:55,760 --> 00:07:01,360
improvements, bug fixes and important performance improvements. And then this one, I like love

70
00:07:01,360 --> 00:07:06,800
this one a lot. So they're App Engine calibrations because it sounds cooler. So they've calibrated

71
00:07:06,800 --> 00:07:12,480
the App Engine. All right. Okay. So why does this keep happening? Why is it like every update is

72
00:07:12,480 --> 00:07:16,240
like, oh, God, we got to improve the performance. We got to improve the performance. Why is this

73
00:07:16,240 --> 00:07:22,560
so hard? Why didn't they get it right the first time? And it turns out, unlike bugs, so code is

74
00:07:22,560 --> 00:07:28,320
always buggy, right? And it's hard to debug programs, but it's like this thing produced the

75
00:07:28,320 --> 00:07:34,000
wrong result. That's pretty clear. But if you have something and it just runs slower, you don't

76
00:07:34,000 --> 00:07:39,360
really know where or what to change, right? So it can be really complicated. So what I'm going to

77
00:07:39,360 --> 00:07:44,240
talk about are two things in this talk. One is performance analysis. So I'm going to explain

78
00:07:44,240 --> 00:07:49,920
how to do performance analysis, right? It turns out that the thing that we, including myself, have

79
00:07:49,920 --> 00:07:55,680
often done is not really very rigorous. And you can draw the wrong conclusions. And then I'm going

80
00:07:55,680 --> 00:08:00,720
to talk about a new approach for performance profiling that will show you how to do it better.

81
00:08:00,720 --> 00:08:06,720
All right? So first, I'm going to talk about performance analysis. So here's Luke. Luke has

82
00:08:06,720 --> 00:08:11,840
his code. And Luke is like, it's too slow. Ogil is too slow. What am I going to do? And so Luke

83
00:08:11,840 --> 00:08:17,360
has an idea. And Luke's idea involves some sort of new, you know, I'm going to do this first,

84
00:08:17,360 --> 00:08:20,480
and I'm going to change this code. I'm going to change this function, blah, blah, blah. I make

85
00:08:20,480 --> 00:08:25,600
a bunch of changes, right? And eventually I end up with A prime, all right? The new version of A.

86
00:08:25,600 --> 00:08:29,680
And so now I want to know, did it work, right? So I've made this change. I thought it would

87
00:08:29,680 --> 00:08:35,040
make things faster. So I go and I take my code, and I have some set of inputs, some benchmarks,

88
00:08:35,040 --> 00:08:42,480
let's say. And I go and I say, run A. And A takes 90 seconds, which is clearly too long for your

89
00:08:42,480 --> 00:08:47,440
App Store thing to run. But anyway, notwithstanding that, let's say there's something that takes

90
00:08:47,440 --> 00:08:55,200
90 seconds in his test, right? And then he runs A prime. 87.5 seconds. Fantastic. Success, right?

91
00:08:55,280 --> 00:09:02,640
2.8% faster, all right? Time for Mojito, okay? So this is great, right? And clearly, you know,

92
00:09:02,640 --> 00:09:09,120
what Luke went and did had a big impact, big impact, all right? The question is, like, is it

93
00:09:09,120 --> 00:09:15,520
really faster, right? So if you go and you plot, like, here's a bar graph with, I'm kind of giving

94
00:09:15,520 --> 00:09:22,720
away the game here, one execution of A prime and A. It turns out that A prime is 2.8% faster,

95
00:09:22,720 --> 00:09:26,720
looks good. But there's maybe a problem here. So what's the problem?

96
00:09:30,080 --> 00:09:34,160
Right, so there's this problem called variance, right? Like, when you run a program once,

97
00:09:34,160 --> 00:09:37,920
you're going to get some result. But if you run it again, maybe the result will be slightly

98
00:09:37,920 --> 00:09:42,320
different, and you want to account for that. Great. So now we'll run it 30 times. 30 is the

99
00:09:42,320 --> 00:09:48,560
magic number, by the way. So we're going to run it 30 times, and we get this graph. And so they're

100
00:09:48,560 --> 00:09:54,080
pretty tightly distributed, and you can see it's still 2.8% faster, right? So seems plausible,

101
00:09:54,800 --> 00:09:59,760
like, I think everybody here would probably be like, looks like A prime is faster. Great. So

102
00:09:59,760 --> 00:10:04,160
the question you have to ask yourself is, why is it faster? So you might think, well, of course,

103
00:10:04,160 --> 00:10:10,640
the reason is the code change, right? So I, as Luke, I'm the developer, and I go and I'm a genius,

104
00:10:10,640 --> 00:10:16,720
and I have my great idea, and it pays off 2.8%, right? Well, it turns out changing the code

105
00:10:16,720 --> 00:10:22,240
can actually lead to all sorts of knock-on effects that have nothing to do with your intended change.

106
00:10:22,240 --> 00:10:28,480
So it could totally be an accident. So let me explain why. So there's this wonderful paper

107
00:10:29,280 --> 00:10:35,520
that is, it appeared in ASPOS in 2009 by Todd Mitkiewicz and a number of other people. I highly

108
00:10:35,520 --> 00:10:40,560
recommend you read it. It's something like, how to do things wrong without really trying, something

109
00:10:40,560 --> 00:10:47,520
like that. And the conclusion is that the layout, like where the code and data end up in memory,

110
00:10:47,520 --> 00:10:52,080
has a pretty big impact on performance, all right? So when you go to measure something,

111
00:10:52,080 --> 00:10:57,600
those measurements are biased by depending where things kind of fell, right? So here are a few

112
00:10:57,600 --> 00:11:02,080
things that can have an impact on layout, and I'm going to talk about more. So one is link order.

113
00:11:02,080 --> 00:11:07,680
So if you're in cc++ land, and you have a make file, and the make file has a bunch of,

114
00:11:08,000 --> 00:11:12,400
this link step and has a bunch of dot-os, depending how those dot-os are arranged,

115
00:11:12,400 --> 00:11:16,880
you can get different performance, okay? You might think, fine, all right? Your environment

116
00:11:16,880 --> 00:11:22,960
variables. So when you go to execute your program, your environment variables, whether it's in cc++

117
00:11:22,960 --> 00:11:27,520
or even managed languages, they somehow get copied in and everything else gets shifted.

118
00:11:28,480 --> 00:11:36,080
So in c and c++, this moves the stack. So this actually has an effect on layout. These two alone

119
00:11:36,080 --> 00:11:45,200
can lead to shifts in performance of plus or minus 40%. Okay? So that's not great. So what is

120
00:11:45,200 --> 00:11:49,600
happening? Like, why is this happening? This is a huge shift. This is literally larger than the

121
00:11:49,600 --> 00:11:59,120
impact of dash o3 over dash o0. Okay? Yes, you laugh, but as well you should. So why is a prime

122
00:11:59,120 --> 00:12:05,600
faster than a, right? So what is going on? Why could this happen without actually trying? So part

123
00:12:05,680 --> 00:12:11,600
of the problem here is that basically modern processors have become insanely complicated

124
00:12:11,600 --> 00:12:17,840
in their zeal to increase speed. So what do they do? So they have caches, right? Add data

125
00:12:17,840 --> 00:12:23,520
and instructions, get mapped to the cache. Well, it turns out for good reasons, these things are

126
00:12:23,520 --> 00:12:29,200
binned up into these things called sets. If they map to the same set, you can have a conflict. So

127
00:12:29,200 --> 00:12:33,520
if you have hot code, a lot of hot code that is mapping to the same set, then it's not going to

128
00:12:33,520 --> 00:12:39,040
necessarily fit in cache, and your code will run slower. By luck, you could be in a situation

129
00:12:39,040 --> 00:12:44,480
where when you changed a prime, you actually disrupted this conflict. And so now you have no

130
00:12:44,480 --> 00:12:49,840
conflict, right? These two things, one is the hot code and one maps to nothing. So no conflict.

131
00:12:49,840 --> 00:12:55,600
Boom, it ran faster. All right? So that sounds great. So it could be the cache, but it could also

132
00:12:55,600 --> 00:13:01,600
be the branch predictor, which actually, again, is based on the addresses of your branches,

133
00:13:01,600 --> 00:13:05,520
and if these branches collide, then you can end up with things running slower.

134
00:13:06,160 --> 00:13:10,720
There's also this thing called the TLB, the translation look-aside buffer, which maps

135
00:13:10,720 --> 00:13:14,720
virtual addresses to physical addresses. If things don't fit in the TLB because they span

136
00:13:14,720 --> 00:13:19,920
two pages instead of one, suddenly things become slower. There's also a branch target predictor.

137
00:13:19,920 --> 00:13:26,000
There's a prefetchor. There's more. All right? So this is pretty bad. So all of these things can

138
00:13:26,080 --> 00:13:32,240
happen. You might think, all right, link order is fine. The code thing is a little weird, but,

139
00:13:32,240 --> 00:13:38,640
you know, hey, it's faster, right? It's 2.8% faster. That, like, I don't care. It's all good, right?

140
00:13:38,640 --> 00:13:44,320
Now, it may not be faster on every machine, but it's faster today, right? So here's the problem.

141
00:13:44,320 --> 00:13:50,400
Like, anything you do can disrupt this. So what could happen? One more malloc changes layout,

142
00:13:51,040 --> 00:13:55,600
right? Like, you've shifted everything over, one more or less. If you upgrade anything in

143
00:13:55,600 --> 00:14:00,560
your system, this is going to change layout, right? So that's bad. Okay. So those things,

144
00:14:00,560 --> 00:14:04,400
all right, I'm not going to change libc, and I guess I'll never malloc again. Fine.

145
00:14:06,640 --> 00:14:13,120
Whatever. All right? So here's something that may be surprising. Running it in a new directory.

146
00:14:13,120 --> 00:14:17,760
So it turns out that your current working directory goes right into your environment

147
00:14:17,760 --> 00:14:23,840
variables, right? So that's weird, right? So, you know, if Vader tries to run your software,

148
00:14:23,840 --> 00:14:30,400
it's not going to work as fast because it's one character longer than Luke, okay? This is a real

149
00:14:30,400 --> 00:14:35,920
effect. This can really happen. It has actually bitten me. I had a student who wrote something.

150
00:14:36,800 --> 00:14:42,240
He has a long Indian last name. My username is just five letters long. It's just Emery.

151
00:14:42,880 --> 00:14:47,360
And he did something. He's like, oh, it doesn't run any faster. It actually runs slower. And it's

152
00:14:47,360 --> 00:14:52,720
like, that makes no sense at all. And eventually, we whittled it down, and it was like, if I run it

153
00:14:52,720 --> 00:15:04,480
as me, it's faster. Okay? That's right. All right? Changes your layout. So the solution is obvious,

154
00:15:04,480 --> 00:15:11,920
right? Run everything. All right. So I should add, you know, all of this is, you know, like,

155
00:15:11,920 --> 00:15:15,360
the whole talk is really oriented towards, I'm going to improve my performance. But

156
00:15:15,360 --> 00:15:19,760
everything I'm talking about today can be viewed in reverse for performance regression. Like,

157
00:15:19,760 --> 00:15:26,800
I made a change, and things run 2.8% slower. Oh, God, roll back. Maybe not, right? Maybe

158
00:15:26,800 --> 00:15:32,560
the next thing you do is going to actually undo that change, right? So basically, layout is super

159
00:15:32,560 --> 00:15:38,640
brittle. And like you've seen, layout biases measurement. So one of the questions that we

160
00:15:38,640 --> 00:15:43,520
wanted to know is, is it possible to eliminate the effect of layout? So we can actually understand

161
00:15:43,520 --> 00:15:48,800
the performance of things kind of without having to think about, well, one malloc less or more,

162
00:15:48,880 --> 00:15:54,720
or, you know, Luke versus Vader. So the answer is yes. We built a tool that we call stabilizer.

163
00:15:55,360 --> 00:16:01,040
So stabilizer addresses this problem that I've just explained to you. Pardon me. And it eliminates

164
00:16:01,040 --> 00:16:09,440
the effect of layout. So this is a way to actually measure programs where you can kind of actually

165
00:16:09,440 --> 00:16:14,160
know whether the regression you had is real or whether the optimization you had is real

166
00:16:14,160 --> 00:16:19,520
and not just an accident. So how does this work? How is this even possible? So the way that

167
00:16:19,520 --> 00:16:26,480
stabilizer works is that it randomizes layout. So it randomizes a lot of things. It randomizes the

168
00:16:26,480 --> 00:16:32,800
function addresses. It randomizes stack frame sizes. It even randomizes heap allocations. But not

169
00:16:32,800 --> 00:16:38,480
only does it do those things, it does it over and over again. So while your program is running,

170
00:16:38,480 --> 00:16:42,800
it's literally doing randomization. And this turns out to be important, and I'll show you a graph

171
00:16:42,800 --> 00:16:49,920
that we'll explain why. But basically, if you do this, then there's no way layout can bias your

172
00:16:49,920 --> 00:16:55,680
measurement because a completely random layout can't bias the results. That's just how things work.

173
00:16:55,680 --> 00:17:01,520
That's why we run randomized control trials. You've eliminated something as a possible cause.

174
00:17:01,520 --> 00:17:07,440
The only other cause that remains is whatever change you made. So let's walk through what you

175
00:17:07,440 --> 00:17:12,400
would do with stabilizer. So with stabilizer, again, clearly you're supposed to run your program

176
00:17:12,400 --> 00:17:17,760
a bunch of times. But notice what happens to the execution times. Here the execution times are no

177
00:17:17,760 --> 00:17:23,280
longer tightly bound around this one very small measurement. The reason for that is that when

178
00:17:23,280 --> 00:17:28,720
you were running that program 30 times, it was sort of like you were going to do a survey of 30

179
00:17:28,720 --> 00:17:34,320
people, but you just ask one person. Because it's the same layout over and over again. So you did

180
00:17:34,320 --> 00:17:40,880
an experiment on 30 executions, but what you really did is you just repeated 30 on one. So the only

181
00:17:40,880 --> 00:17:46,400
noise that you're eliminating is the noise that comes from network demons waking up or some other

182
00:17:46,400 --> 00:17:51,600
random event, maybe some thermal issue in your computer, but it's not really altering layout.

183
00:17:51,600 --> 00:17:57,760
It's always the same layout. So here it's not, and you get these very nice bell curves. So now I'm

184
00:17:57,760 --> 00:18:02,640
going to ask you the question. So this is an audience poll time. Is A prime faster than A? I just

185
00:18:02,640 --> 00:18:09,200
want you to raise your hands if you think that A prime is faster than A. All right, great. Now keep

186
00:18:09,200 --> 00:18:14,800
your hands up. Don't set them down. But set them down if you change your mind. How about now?

187
00:18:17,680 --> 00:18:26,400
How about now? There's still a few like hardcore. So what you all are doing is what I like to refer

188
00:18:26,400 --> 00:18:33,920
to as eyeball statistics. And so you're kind of like, looks close to me. That's too close.

189
00:18:34,640 --> 00:18:43,040
Right. But it turns out this is not actually a thing. So if you, yeah, it's not really statistics

190
00:18:43,600 --> 00:18:49,520
when you just eyeball results. So this is a bit of a refresher for some of you, but I'm going to

191
00:18:49,520 --> 00:18:55,040
walk you through this and how this all fits in with stabilizer. So in actual statistics, and today

192
00:18:55,040 --> 00:18:59,920
I'm just going to talk about one flavor of statistics, which is called null hypothesis

193
00:18:59,920 --> 00:19:04,800
significance testing. There are others, notably Bayesian approaches. Happy to talk about that

194
00:19:04,800 --> 00:19:10,640
offline. But basically the way it works is you just assume that the things are the same. You say

195
00:19:10,640 --> 00:19:17,440
what is the likelihood of observing this difference by chance? All right. So it turns out that this

196
00:19:17,440 --> 00:19:22,000
is something that's just convenient. It's very easy to compute these probabilities for the normal

197
00:19:22,000 --> 00:19:27,120
distribution, which you all remember from school. These graphs are normal. Awesome. It turns out

198
00:19:27,120 --> 00:19:31,760
that stabilizer happens to make normal graphs or normal distributions. And I'll explain why.

199
00:19:32,720 --> 00:19:36,000
So how are we going to do this? We're going to run stuff with stabilizer.

200
00:19:36,000 --> 00:19:41,520
We're going to pick some probability below which we're like, okay, good enough. Right. So

201
00:19:41,520 --> 00:19:46,240
if it's only a one in 20 chance, I see this probability like the, I see this event occurring.

202
00:19:46,240 --> 00:19:51,040
I'll be like, okay, that's good enough for me. You could be harsher. You could say one in 100,

203
00:19:51,040 --> 00:19:59,520
one in 1,000. It's pretty standard to say one in 20. This is the p value of 0.05. So the idea is

204
00:19:59,520 --> 00:20:04,560
if there's a low enough probability, you reject the null hypothesis, the null hypothesis being

205
00:20:04,560 --> 00:20:08,880
that they're the same. And you conclude that the speed up is real. It's not due to the effective

206
00:20:08,880 --> 00:20:14,960
memory on memory layout. All right. So why re-randomization? The reason for re-randomization

207
00:20:14,960 --> 00:20:20,800
is that just randomizing once doesn't give you enough randomization. So this is an actual program.

208
00:20:21,040 --> 00:20:26,960
You can see the distribution is pretty wacky. It's very far from normal. You can't intuitively

209
00:20:26,960 --> 00:20:32,720
explore much of the space when you just randomize at startup as opposed to randomizing during

210
00:20:32,720 --> 00:20:38,480
execution. This is in fact the kind of distribution you get when you randomize all the time. And

211
00:20:38,480 --> 00:20:42,720
these are normal distributions. So why do I keep saying that they're normal distributions?

212
00:20:42,720 --> 00:20:50,000
The reason is essentially, again, going back to like freshman stats, stabilizer generates a new

213
00:20:50,000 --> 00:20:56,960
random layout every half second. That is to say it's a completely independent version of the program

214
00:20:56,960 --> 00:21:01,680
right from half second to half second to half second. It's all randomized. And it's the same

215
00:21:01,680 --> 00:21:06,880
program the whole time. So it's identically distributed. And then we're adding them all up.

216
00:21:06,880 --> 00:21:13,200
And there is this nice result, a key result of stats, which is the sum of a sufficient number.

217
00:21:13,200 --> 00:21:17,760
So if you run a program for long enough of independent identically distributed random

218
00:21:17,760 --> 00:21:22,560
variables, it's approximately normally distributed no matter what the underlying distribution was.

219
00:21:22,560 --> 00:21:27,200
This is the central limit theorem. So this makes execution times normally distributed,

220
00:21:27,200 --> 00:21:32,000
which is cool in other ways because you actually know how likely it is that you're going to see

221
00:21:32,000 --> 00:21:35,840
some very weird execution because you know what the distribution looks like.

222
00:21:35,840 --> 00:21:39,920
All right, great. So now we have this thing in hand and we're going to do something insane.

223
00:21:40,640 --> 00:21:46,800
We're going to see whether optimizations actually matter. And we know some of them matter. So we

224
00:21:46,800 --> 00:21:50,880
have a suite of benchmarks that we're going to evaluate it on. We're going to evaluate them

225
00:21:50,880 --> 00:21:54,800
individually and then across the whole benchmark suite. And I'll show you how we do it. So you

226
00:21:54,800 --> 00:22:00,880
build the benchmarks with stabilizer. Stabilizer is a plugin for LVM. If you just compile it as such,

227
00:22:00,880 --> 00:22:05,600
it goes and randomizes everything. But you can actually just randomize things independently

228
00:22:05,600 --> 00:22:10,400
if you wanted, like just code, just heap, and just stack. So now we run the benchmarks,

229
00:22:10,400 --> 00:22:15,600
we run them as usual. We drop them into one of my least favorite programming languages ever.

230
00:22:16,880 --> 00:22:22,480
And then we decide what the result is. So again, we don't ask questions like this

231
00:22:22,480 --> 00:22:26,400
because that's eyeball stats. Instead, we ask a question like this,

232
00:22:26,400 --> 00:22:30,080
pretend they're equal. How likely is it we'd observe this much of a difference?

233
00:22:30,560 --> 00:22:39,680
So I also have to say that you should not assume normality like almost ever in my humble opinion,

234
00:22:39,680 --> 00:22:43,920
unless you have very good reasons for doing so. Here we have very good reasons for doing so.

235
00:22:43,920 --> 00:22:49,680
So we can use really powerful statistical tests like the student's t-test. So this is the test

236
00:22:49,680 --> 00:22:55,120
that you used to actually measure this difference. So if the p-value, the likelihood of this event

237
00:22:55,120 --> 00:22:59,200
occurring, is less than some threshold, which as I mentioned before is 5%, we're going to reject

238
00:22:59,200 --> 00:23:04,880
the null hypothesis. That is to say, it's not because of random layout, the difference is real.

239
00:23:05,520 --> 00:23:10,080
Everybody's on board, I hope. So now we're going to do it. You'll be shocked to learn

240
00:23:10,080 --> 00:23:18,160
that dash O2 versus dash O1 makes a difference. Good. I would be weird if the result were otherwise.

241
00:23:18,160 --> 00:23:22,320
So you can see that there are statistically significant improvements, right on the right.

242
00:23:22,320 --> 00:23:27,760
There's some that are statistically significant but don't matter. And by God,

243
00:23:27,760 --> 00:23:34,720
there are statistically significant performance drops. So it turns out that compiler people

244
00:23:34,720 --> 00:23:39,440
run these same benchmarks and overfit the way that they do these optimizations,

245
00:23:39,440 --> 00:23:43,760
and some of them lead to layout changes. And it wasn't actually the code change.

246
00:23:44,560 --> 00:23:49,200
And so we can actually distill out this effect. All right, great. By and large,

247
00:23:49,200 --> 00:23:57,920
it looks like O2 over O1 is a win. How about O3 versus O2? Ready? It's amazing. Okay.

248
00:23:59,920 --> 00:24:03,360
So I actually have to change the axes so we can see a lot of these values.

249
00:24:03,360 --> 00:24:08,720
So I'm going to zoom in. Instead of it being 0 to 10, like the range is negative 10 to 20,

250
00:24:08,720 --> 00:24:15,200
I'm going to make it 0 to 1.5. Okay, so now we can see them. So they're pretty small effects,

251
00:24:15,920 --> 00:24:20,960
but some of them are significant and some of them are not. Again, statistically significant,

252
00:24:20,960 --> 00:24:27,760
1.75% decrease in execution time. Great. A bunch of things where it's not significant

253
00:24:27,760 --> 00:24:32,320
and a couple decreases, but really very minor effect sizes. So what do these results mean?

254
00:24:32,960 --> 00:24:38,560
I mean, you can't actually look at an individual benchmark. Like there's 30 of them, right?

255
00:24:38,560 --> 00:24:42,240
So drawing a conclusion about all 30, you actually have to do something different.

256
00:24:42,240 --> 00:24:46,880
You have to collect all of them. You get a bunch of these graphs, and this is what you don't do.

257
00:24:46,880 --> 00:24:53,360
Like, okay, this one is slower. This one's faster. This one's faster. This is just eyeballs everywhere.

258
00:24:54,160 --> 00:24:59,200
Okay? I mean, they're spooky and nobody wants to see those. So again, we're going to do the

259
00:24:59,280 --> 00:25:03,360
same thing, but to test a bunch of things simultaneously, you do this thing, which is

260
00:25:03,360 --> 00:25:08,160
terribly named, called analysis of variance, and you plug it into R with this awesome incantation,

261
00:25:08,880 --> 00:25:13,120
and then you do, again, the same test. If the p-value is less than or equal to 5%,

262
00:25:13,120 --> 00:25:18,720
we reject the null hypothesis. All right? You ready? All right, here we go. Here's the p-value.

263
00:25:18,720 --> 00:25:22,880
So it has to be less than or equal to 5% or else we're going to conclude that dash

264
00:25:22,880 --> 00:25:32,640
o3 versus dash o2 is nothing. All right? So the p-value is 26.4%. That means that one in four

265
00:25:32,640 --> 00:25:38,480
experiments will just show a random effect, right? Just literally randomly. We do not consider

266
00:25:38,480 --> 00:25:44,160
this enough evidence to reject the null hypothesis. So we're, we cannot reject the null hypothesis,

267
00:25:44,160 --> 00:25:49,840
which is that the effect is indistinguishable from noise. All right? Okay. So this is all

268
00:25:49,840 --> 00:25:57,280
terrible news for people like Luke who wanted optimizations to work, and I've actually seen,

269
00:25:58,400 --> 00:26:05,120
I've actually seen projects. It makes, it kind of breaks your heart. Like projects I committed,

270
00:26:05,120 --> 00:26:13,600
like on GitHub, where it literally says dash o9, and I feel like why not dash o11? There's no,

271
00:26:13,680 --> 00:26:19,280
there's no dash o9 or 11. It's just kind of bottoms out. But you know, hope springs eternal.

272
00:26:20,000 --> 00:26:25,680
All right. So great. So what are we going to do? So what people do when they can't speed things up,

273
00:26:25,680 --> 00:26:29,920
right? They run a profiler. So there's these profilers, they all basically work the same way.

274
00:26:30,720 --> 00:26:34,800
You go and you get some result, and it says, hey, here's where my program spent its time.

275
00:26:35,440 --> 00:26:40,880
You get the number of calls to every function, runtime for each function, and this captures

276
00:26:40,880 --> 00:26:45,360
intuitively, maybe for most of us, like this is what a profiler should do, right? What do I care

277
00:26:45,360 --> 00:26:50,720
about? There's frequently executed code or code that runs for a long time. That's where I should

278
00:26:50,720 --> 00:26:56,160
be focusing my optimization efforts. All right. It seems intuitively appealing. This is the way

279
00:26:56,160 --> 00:27:01,120
profilers have been written since prof, back, you know, back from like, I don't know, late 60s,

280
00:27:01,120 --> 00:27:07,120
early 70s. So would this in fact speed up Google? So we're going to do this experiment. We're going

281
00:27:07,200 --> 00:27:11,760
to go and find the thing that runs for the longest amount of time and where it spends all of its

282
00:27:11,760 --> 00:27:17,600
time running. And so we're going to run it. And so we go and we do this. And basically,

283
00:27:17,600 --> 00:27:23,200
it makes the loading thing flash faster. Okay. So, well, guess what? That's frequently executed.

284
00:27:23,840 --> 00:27:30,800
And in fact, it's the code that runs for the longest time, right? So this is not really great,

285
00:27:30,800 --> 00:27:36,880
especially if Luke spent like more than a minute optimizing that code. That's a shame. All right.

286
00:27:36,880 --> 00:27:42,480
So basically in some profilers were developed in an era where everything was synchronous and there

287
00:27:42,480 --> 00:27:48,320
was a single core. All right. That's not today. Today things are asynchronous or parallel or

288
00:27:48,320 --> 00:27:53,840
concurrent or a mixed thereof. And profilers don't do a good job in these contexts. So we need to

289
00:27:53,840 --> 00:27:59,600
do better. So what would be really cool is if we could have something like this. So this is what

290
00:27:59,600 --> 00:28:06,320
I call a causal profile. So a causal profile tells you visually, like, if I were to speed up this

291
00:28:06,320 --> 00:28:13,040
component by this much on the x-axis, then the whole program will speed up by this much. All

292
00:28:13,040 --> 00:28:18,800
right. So this is really nice. Like, if I had this graph, I would know I could spend a little

293
00:28:18,800 --> 00:28:24,960
effort on the yellow search component and I'll get a big bang for my buck. Eventually, it's going

294
00:28:24,960 --> 00:28:30,240
to bottom out or top out at like, you know, like 70, 80%. And the red one, I could just keep going,

295
00:28:30,240 --> 00:28:34,240
right? Like, it gets faster and faster the more I work. And the blue one, I should never,

296
00:28:34,240 --> 00:28:39,120
never optimize ever. All right. It would be cool to know this, right? It's essentially like an

297
00:28:39,120 --> 00:28:44,880
oracle coming and telling you, this is the code you should work on, Luke. I don't know where I

298
00:28:44,880 --> 00:28:49,920
got that way of talking. Anyway. All right. So the question is, how would we know this? Like,

299
00:28:49,920 --> 00:28:54,400
how would we get this information? Like, how would we know that this change would cause this effect?

300
00:28:54,400 --> 00:28:59,120
Right? We can't just go and optimize the program by arbitrary amounts and test it. That

301
00:28:59,120 --> 00:29:03,360
kind of defeats the purpose. So we're going to do something different. We're going to run an

302
00:29:03,360 --> 00:29:11,440
experiment. And it requires one ingredient here, which I'll refer to as the force. So we're going

303
00:29:11,440 --> 00:29:16,320
to use magic. And we're going to speed things up magically. And then we're going to measure

304
00:29:16,960 --> 00:29:22,400
how much the effect was of speeding up each component by a certain amount on overall program

305
00:29:22,400 --> 00:29:28,240
execution. Okay? So we just keep doing this, right? We get more and more points, right? And then I do

306
00:29:28,240 --> 00:29:32,000
it for different things. It turns out if I could speed up saving things to the three and a half

307
00:29:32,000 --> 00:29:37,600
inch floppy, it doesn't make a difference, right? And so on. All right? Now, unfortunately, we live

308
00:29:37,600 --> 00:29:44,320
in the real world where there's no magic. Sorry. Well, if there was magic, to be clear, this is not

309
00:29:44,320 --> 00:29:50,480
what we would do, right? I mean, obviously, there are many much better things we could do. I could

310
00:29:50,480 --> 00:29:54,400
think of people I would like to disappear off the face of the earth, for example. But I could also

311
00:29:54,400 --> 00:29:59,040
disappear all the runtime off the face of the earth. Because why not? All right? So obviously,

312
00:29:59,040 --> 00:30:03,840
that's what we would do. So we can't do that. We have to do something else. So what we are going

313
00:30:03,840 --> 00:30:08,960
to do as our trick is we're going to do something that essentially takes advantage of this notion

314
00:30:08,960 --> 00:30:14,880
of relativity. So we're going to do a virtual speedup. And a virtual speedup speeds things up

315
00:30:14,880 --> 00:30:19,600
in scare quotes by slowing everything else down, right? So everything else that's running

316
00:30:19,600 --> 00:30:24,720
at the same time will then be slowed down. And that will allow us to get the impact

317
00:30:24,720 --> 00:30:30,720
of how do we sped this thing up? What would the results have been? So here, for example,

318
00:30:30,720 --> 00:30:38,320
if we speed up the sending of the picture results back by a certain amount, we've slowed down

319
00:30:38,320 --> 00:30:44,240
everything running concurrently with it. And then that gives us a result of a slowdown, which is the

320
00:30:44,240 --> 00:30:50,160
same thing as the result of having sped it up. So we actually can get points on this graph

321
00:30:50,160 --> 00:30:54,800
just by running these experiments. So I just got a point here, and I do it for everything,

322
00:30:54,800 --> 00:31:00,560
and I get more points. And eventually, I get a graph like this. If I speed up indexing,

323
00:31:00,560 --> 00:31:04,560
I'm going to get the exact same effect. Indexing is running at the same time as the compression.

324
00:31:05,440 --> 00:31:11,760
So I get this result, and then bang, I get these results. And again, these are all the results.

325
00:31:12,400 --> 00:31:19,280
Now, I draw your attention to the one weird blue thing. So the blue thing is slower,

326
00:31:20,160 --> 00:31:25,680
and it turns out that sometimes optimizing things makes your program run slower. And the

327
00:31:25,680 --> 00:31:32,240
intuition behind this is you can actually get congestion on a lock, or congestion for a shared

328
00:31:32,240 --> 00:31:37,600
resource like disk or network. And so speeding things up makes things worse. You would like to

329
00:31:37,600 --> 00:31:44,160
know this before you get started. That would be a very, very bad day for Luke that might necessitate

330
00:31:44,160 --> 00:31:49,120
several sequels to recover from. All right, great. All right, so let's dig into Ogil a little bit.

331
00:31:50,400 --> 00:31:55,600
So what do we care about in Ogil? We care about two things. We care about how long it takes

332
00:31:55,600 --> 00:32:01,920
between a request and a response, a.k.a. latency. Traditional profilers don't do this at all.

333
00:32:02,000 --> 00:32:05,680
It's just total runtime. Oh, let me get in my soapbox for one moment.

334
00:32:06,800 --> 00:32:10,480
Traditional profilers are about end-to-end runtime. You know how your servers are all

335
00:32:10,480 --> 00:32:14,880
about end-to-end runtime? Or your browser? Like, if only your browser could quit faster.

336
00:32:16,800 --> 00:32:20,640
So again, like, it was all about, like, here's a program. I run at a console,

337
00:32:20,640 --> 00:32:24,640
and it does something, and it's done, and that's all I cared about. That's not really today.

338
00:32:25,920 --> 00:32:29,920
So there's latency. And then the more traditional thing is throughput. Again,

339
00:32:29,920 --> 00:32:33,520
this is something that profilers do a bad job of measuring because they're all about end-to-end

340
00:32:33,520 --> 00:32:39,840
execution time. So how fast results come back is throughput. So how are we going to do this?

341
00:32:39,840 --> 00:32:45,040
So with our causal profiler that I'm going to explain in a minute, we're going to introduce

342
00:32:45,040 --> 00:32:50,080
what we call progress points. So the notion of progress points is here's a thing I want to happen

343
00:32:50,080 --> 00:32:56,000
faster, or here's a beginning and an end of things that I want to happen faster. So if Luke

344
00:32:56,000 --> 00:33:01,600
wants responses to get sent faster, higher throughput, you just mark this particular start of this

345
00:33:01,600 --> 00:33:07,440
component as a progress point, and every time the code runs, you go and you get another coin.

346
00:33:08,160 --> 00:33:13,520
And then you can do this simultaneously, many requests for many users, and all of these things

347
00:33:13,520 --> 00:33:18,880
are incrementing some counter. So these progress points are measuring throughput, and then you

348
00:33:18,880 --> 00:33:22,960
basically are going to run the experiments and see what the effect is on the rate of those

349
00:33:22,960 --> 00:33:28,160
progress points being executed. So one point measures throughput. Like I said,

350
00:33:28,160 --> 00:33:32,960
if I speed up some component, whatever it might be, what is the effect? So now,

351
00:33:33,680 --> 00:33:38,880
what if I care about latency? So we do the exact same thing. We set a progress point at the beginning,

352
00:33:38,880 --> 00:33:43,440
a progress point at the end, and then the only thing that has to happen under the covers is it

353
00:33:43,440 --> 00:33:47,920
has to have a counter. And the counter here measures how many things are in the system

354
00:33:47,920 --> 00:33:54,800
at any one time. And it turns out that there is this awesome law that holds in a wide variety

355
00:33:54,800 --> 00:34:00,400
of circumstances called Little's law. And so Little's law says that the latency is essentially

356
00:34:00,400 --> 00:34:05,520
the number, the average number of transactions in a system divided by the throughput. We already

357
00:34:05,520 --> 00:34:09,840
know how to measure throughput, so we just take advantage of Little's law and we can translate

358
00:34:09,840 --> 00:34:16,320
this into latency. All right, great. So we have built a causal profiler for Linux. It already

359
00:34:16,320 --> 00:34:21,520
ships with Debian and Ubuntu, so if you're using one of those systems, you can install it quite

360
00:34:21,520 --> 00:34:28,960
easily. So it's just cause-profiler. It's quite easy to run. So you say cause run dash dash dash

361
00:34:28,960 --> 00:34:32,960
and whatever your program is in its arguments and it fires it off and it starts doing performance

362
00:34:32,960 --> 00:34:38,560
experiments. All right, I should add it's not entirely true. You do need to place progress

363
00:34:38,560 --> 00:34:42,880
points. If you don't place any progress points, it will act like an ordinary profiler measuring

364
00:34:42,880 --> 00:34:47,600
end-to-end execution time. But if you do put in progress points, then it will actually do its

365
00:34:47,600 --> 00:34:53,200
magic. All right, and this is just some macro, like progress begin, progress end. All right,

366
00:34:53,200 --> 00:34:59,280
so let's apply this to Augell. All right, I didn't actually build Augell. Neither did Luke,

367
00:34:59,280 --> 00:35:02,960
but we're going to build it out of pieces like any good programmer would do. So it turns out

368
00:35:02,960 --> 00:35:08,480
there's this suite of parallel applications that's kind of ready-made for this task. So there's a

369
00:35:08,560 --> 00:35:14,320
deduplicator that does compression. There's an image comparator. And then there's a database,

370
00:35:14,320 --> 00:35:20,320
SQLite. That's not in Parsec, but we'll use SQLite too. All right, great. So I'm going to show you

371
00:35:20,320 --> 00:35:24,720
some fun things we did. This is a well-studied set of applications. People have already tried to

372
00:35:24,720 --> 00:35:30,560
optimize these and we have covered a bunch of surprising optimization opportunities. So here's

373
00:35:30,560 --> 00:35:35,440
Ferret. This is actually an older version of what our causal profile looks like. Now it runs in a

374
00:35:35,440 --> 00:35:40,720
web browser. And you can see that there's these lines of code and it says, boy, if you speed up

375
00:35:40,720 --> 00:35:46,640
these lines of code, then you're going to get performance increases. Conveniently, these lines

376
00:35:46,640 --> 00:35:51,760
of code happen to be located in separate chunks of Ferret. So the part that does ranking, the part

377
00:35:51,760 --> 00:35:56,960
that does indexing, the part that does segmentation. And why is this convenient? I'm not going to

378
00:35:56,960 --> 00:36:01,920
have to change any code to make this faster. The reason is that what Ferret does is it has this

379
00:36:01,920 --> 00:36:08,640
pipeline model and it assigns an equal number of threads to every stage in the pipeline. But

380
00:36:08,640 --> 00:36:14,720
it turns out this one really doesn't need that many threads. So we take away the threads and just

381
00:36:14,720 --> 00:36:21,840
by reassigning threads, we got a 20% speedup. So this is pretty cool because Caus actually

382
00:36:21,840 --> 00:36:27,600
predicted it perfectly. So we increased ranking, for example, from 16 to 22 threads. That's a

383
00:36:27,680 --> 00:36:34,640
27% increase in throughput. And on the graph, that says that that would translate to a 21%

384
00:36:34,640 --> 00:36:43,360
overall improvement. And that's what we got. So Caus actually works. Good. So we were pretty happy

385
00:36:43,360 --> 00:36:51,360
with this. We then are going to move on to Ddupe. So Ddupe is pretty hilarious. So here's Ddupe in

386
00:36:51,360 --> 00:36:57,760
action. I have two pictures, Grumpy Cat 1 and Grumpy Cat Meme. And so now what do I do to

387
00:36:57,760 --> 00:37:02,720
deduplicate these things? You can see that there's chunks that are the same. So you carve out the

388
00:37:02,720 --> 00:37:08,080
chunks that are the same and you separate them out into individual pieces. And then an image is now

389
00:37:08,080 --> 00:37:15,200
represented by the bits and pieces that make up the image. So here Grumpy Cat 1 is this piece

390
00:37:15,200 --> 00:37:20,400
and Fun Is Awful is this piece. And you saved a lot of memory. So that's what Ddupe does.

391
00:37:21,040 --> 00:37:26,160
So it does this compression via deduplication and it uses a hash function. So it throws everything

392
00:37:26,160 --> 00:37:31,360
through a hash table. Great. So it's a pretty standard hash table. You just have some hash

393
00:37:31,360 --> 00:37:36,720
table. It's an array. It's a bunch of bins. You get a bin number and then you go and you start

394
00:37:36,720 --> 00:37:43,120
adding stuff to that bin into the bucket. So this all seems straightforward. You hope that it would

395
00:37:43,120 --> 00:37:49,520
do something like this. The hash table is accessed concurrently by a bunch of threads, but they're

396
00:37:49,520 --> 00:37:55,760
not idiots. There's not one big lock. It's just all locks, which is naive, but it's fine. But

397
00:37:55,760 --> 00:38:02,160
surprisingly, cause says that the loop that accesses this list is important. Now, if you know

398
00:38:02,160 --> 00:38:07,440
anything about hash tables, you know that things generally end up balanced, right? And it's weird

399
00:38:07,440 --> 00:38:11,840
that you have this sort of situation. So we thought, all right, well, let's just make more,

400
00:38:11,840 --> 00:38:16,560
more hash buckets, right? But we made a thousand of them. We really should have made a million,

401
00:38:16,560 --> 00:38:22,960
cause, you know, a million. But anyway. So you would think this would lead to fewer

402
00:38:22,960 --> 00:38:29,120
collisions, but it had no effect, right? So what else could be causing the collisions? Any guesses?

403
00:38:30,480 --> 00:38:36,320
The hash function, exactly. Like this is one of those when all other possibilities have been

404
00:38:36,320 --> 00:38:40,880
exhausted, right? You pick the weirdest one. That's not an exact quote. But anyway,

405
00:38:41,840 --> 00:38:46,320
well, you're like, how can the hash function be broken? Like we've been using hash functions

406
00:38:46,320 --> 00:38:51,200
since before Canuth wrote about them. Well, turns out people like to roll their own, cause it's

407
00:38:51,200 --> 00:38:58,560
fun. And so we did a histogram of the number of items per bucket. So again, I told you there's

408
00:38:58,560 --> 00:39:09,600
a thousand buckets. This is the histogram. Yeah. Hilariously, what they did is they used the pixels

409
00:39:10,240 --> 00:39:15,920
that were taken from the image and they sum number of pixels and then added them. But that's

410
00:39:15,920 --> 00:39:21,680
actually the central limit theorem in action, right? They're random, right? They're independent,

411
00:39:21,680 --> 00:39:26,400
right? And you've summed them together. And so they actually formed the normal distribution.

412
00:39:26,400 --> 00:39:30,720
That's not the distribution you want for a hash table. You would like a uniform distribution.

413
00:39:31,360 --> 00:39:40,080
So literally, we changed one character. We changed the plus to XOR. So this. And we got this.

414
00:39:48,080 --> 00:39:52,560
So, okay. I'll take the applause, but I mean, it was only a 9% speedup. But

415
00:39:53,760 --> 00:39:58,160
all right. Nonetheless, it was one character. So I think it's the biggest bang for buck

416
00:39:58,160 --> 00:40:02,800
ever recorded in optimization effort. So what did it predict? It turned out we can

417
00:40:02,800 --> 00:40:08,080
also measure the accuracy of the prediction. So we knew that the blocks per bucket went from 76-ish

418
00:40:08,080 --> 00:40:14,080
to 2. That's a 96% traversal speedup. And again, going back to the causal graph,

419
00:40:14,080 --> 00:40:19,120
it predicted a 9% speedup, which is what we got, right? So it's working. All right. So finally,

420
00:40:19,120 --> 00:40:26,320
I'm going to talk about SQLite. So I have no time left. But SQLite is pretty awesome. It's widely

421
00:40:26,320 --> 00:40:32,880
used, as you all know. But it has this weird thing where it has a kind of strange virtual table

422
00:40:32,880 --> 00:40:38,800
that they set up at compile time. And so whenever you actually indirect through a config to execute

423
00:40:38,800 --> 00:40:44,000
a function like pthreadmutexunlock. So you would think, all right, why are you telling me about this?

424
00:40:44,000 --> 00:40:49,440
Well, everything looks like this. This is an indirect call. Could be a direct call. That would

425
00:40:49,440 --> 00:40:57,440
be faster. But an indirect call is not that slow. But it's almost the same cost as pthreadmutexunlock,

426
00:40:57,440 --> 00:41:01,040
which means that you just doubled the length of all of your critical sections.

427
00:41:01,920 --> 00:41:08,160
So that's not great. So in fact, when you go, so cause will highlight all of these lines and say,

428
00:41:08,160 --> 00:41:16,640
you should definitely optimize these. So we undid all of the actual indirect stuff and just made

429
00:41:16,720 --> 00:41:22,720
it so that at compile time, you change SQLite unlock to something so it doesn't do the indirect.

430
00:41:22,720 --> 00:41:30,320
And it sped things up by 25%. If you look at a traditional profiler, by the way,

431
00:41:30,320 --> 00:41:35,600
those things are like, this takes 0.0001% of time. You would never consider actually

432
00:41:35,600 --> 00:41:40,480
trying to optimize that code. So we did it for a bunch of programs. We got some crazy speed ups.

433
00:41:40,480 --> 00:41:46,800
My favorite is we got a 68% speed up by replacing a custom barrier with a standard barrier.

434
00:41:47,680 --> 00:41:55,360
Again, people should stop doing things at home. So anyway, so I'm going to conclude.

435
00:41:55,360 --> 00:42:00,800
So you can take a picture of this to jump to work from our lab, which is plasmaumass.org.

436
00:42:00,800 --> 00:42:05,600
I talked today about sound performance analysis and effective performance profiling. Everybody

437
00:42:05,600 --> 00:42:08,720
should go use the cause. All right. Thanks for your attention.

