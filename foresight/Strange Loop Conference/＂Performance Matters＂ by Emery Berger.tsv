start	end	text
0	10800	Hi everybody, so I'm Emory Berger. I'm going to be talking today about performance matters.
10800	15480	This is joint work with my former PhD student, Charlie Kersinger. So I'm going to tell you
15480	23400	a story that happened a short time ago in a valley far, far away. Not actually this one,
23400	31720	in fact, this one. So the story is about a character who we're going to call Luke. Luke is
31720	37440	our hero. Of course he's a hero in Silicon Valley because Luke has an idea for an app. Alright,
37440	43440	so here's Luke's genius idea for an app. Basically you use your smartphone and you take pictures
43440	49680	of things and then it goes and finds matches like and returns them to you and he's got a great name
49720	55000	for it as well. It's going to call it Ogil. And he's pretty sure Ogil is going to totally
55000	60840	disrupt image search. Nobody has told him about Google image search yet, but anyway. So he sets
60840	66600	about building it. So he makes the prototype Ogil. So the prototype Ogil works like this. Take a
66600	74440	picture. It gets sent off to the Ogil data center. If it is new, it gets added to the database. And
74440	79800	then at the same time it goes and finds similar pictures and sends them back to the user. Alright,
79800	84960	so this is pretty straightforward. You can kind of abstract out this flow. If you think of it as
84960	90240	follows, you get a request. The request comes in. It essentially spawns two threads. One of them
90240	95760	goes and takes the image and let's say compresses it and then saves it to a three and a half inch
95760	103360	floppy as one does. And then at the same time it does some indexing to look up matches, right,
103400	109200	so it does a search after doing some feature extraction and then sends it back to the user via
109200	114360	paper airplane, which is the most effective method known to man. And then eventually these two threads
114360	119240	join up. Alright, so this is obviously a simplification. I've alighted certain things like
119240	125320	for example locks. So when you have locks, right, there's some database for example, you can't be
125320	129360	modifying the database simultaneously, blah, blah, blah. But anyway, this is essentially at a
129360	136320	high level how Ogil is going to work. So Luke goes and ships it off to the app store. It gets
136320	141520	approved and you know users start coming and Ogil is working, right? He's disrupting image search.
141520	148840	But then as the number of users kind of like mount, it turns out Ogil starts getting slow,
148840	154440	right? So it takes a long time for Ogil to load, right? So this is too bad. He's not happy about
154480	160600	this. He's not really sure what to do. So, but before we get into that, let me talk about another
160600	165400	version of Luke. This is also Luke. This is the cool hand Luke, if you will, with his glasses.
165400	170960	Alright, but really this is Luke in the 80s. Alright, so he's got the 80s glasses to match and
170960	176280	of course those of you old enough to know or who have watched TV know that there were no smartphones
176280	182040	in the 80s. There were instead computers with big CRTs. So this is what Ogil looked like in the 80s.
183040	187640	So you'd have some ASCII art and you want to go out and it would connect via some modem,
187640	194400	let's say, search the database. This is version 1.0 of Ogil 84. And it comes up with its matches and
194400	200120	this is your user interface. So I hope you liked it. Alright, now of course back in the day,
200120	207800	1984, computers were really slow, right? Really slow. And so actually this Luke today who was
207840	214040	like Ogil is too slow, well things were really bad in 1984, right? But it turns out things were
214040	219600	also kind of better in a way. So performance used to be really easy. So I'm sure all of you have
219600	224320	seen this kind of graph. What I'm going to show you is on the left, the number of transistors,
224320	231520	as years progress on the x-axis, if they go up, you'll notice it is a log scale. And on the right
231520	237200	you have clock speed, which roughly corresponds to computing performance. And so basically for
237240	241560	years and years and years, we had this situation where the number of transistors were increasing
241560	247520	roughly at doubling every 18 months. This is famously Moore's law. And this generally had the
247520	253040	effect of allowing clock speed to increase. And so you had smaller features. This meant that your
253040	260000	programs would actually run faster. And in fact, you could just literally wait, right? If you buy
260000	265520	new hardware, in fact to the upgrade cycle every year or so, like if you bought a new computer,
266000	271160	everything would run much faster. Now things were slow, right? So don't be so excited about how
271160	277720	great it was back then. It was bad. 33 megahertz to 66 was awesome back then and kind of terrible
277720	283720	today. But it did change the landscape of what it meant to be somebody who works on performance
283720	292400	improvement because this is what you would do. So there really, in a way, was no sense trying to
292400	297160	squeeze out any performance out of something when it was going to double in 18 months, right? The
297160	301680	chance that you in 18 months were going to squeeze out double the performance pretty slender. So
301680	306960	you might as well just sip mojitos on the beach, all right? I can tell you that this was, well,
306960	312800	maybe the mojitos and beach part, notwithstanding, this was actually kind of a strategy that was
312800	317680	recommended for high performance computing. People would literally say, we're just going to upgrade
318000	323200	the computers next year or in two years. So focus on something else. Don't focus on performance,
323200	330920	right? Now, unfortunately, that's not the current state of affairs, right? So today, as all of you
330920	336880	know, when you upgrade a computer, it does not get faster anymore. Maybe it gets lighter. Maybe it
336880	347040	has improved battery life. Maybe not. But basically what happened is eventually Moore's law kind of
347040	352080	ran out of steam. The reason that it did is not actually true that Moore's law ran out of steam.
352080	359760	This is technically a problem called denard scaling. And so denard scaling ran out. And so right
359760	365120	now, we're in a situation where basically, if we clock things up, we no longer can dissipate the
365120	370560	heat on chips effectively enough to make them run faster. So transistor counts actually are still
370560	375600	increasing, which is why we have multiple cores. So we have a bunch of transistors. What are we
375680	381040	going to do with them? Let's just stamp out more of the same thing, right? So that's great. But it's
381040	385440	also not super awesome because if you have a program that is not parallel, it's not going to run any
385440	393040	faster. So everything now has multicore. Your phone has multicore. But it turns out it's still a
393040	401280	problem. So these are actual screenshots from app store updates. Every app store update practically
401280	408880	is about performance or bug fixes. And so here's one that says under the hood updates for better
408880	415760	performance, as opposed to the over the hood updates. Okay. Here's bug fixes and performance
415760	421360	improvements, bug fixes and important performance improvements. And then this one, I like love
421360	426800	this one a lot. So they're App Engine calibrations because it sounds cooler. So they've calibrated
426800	432480	the App Engine. All right. Okay. So why does this keep happening? Why is it like every update is
432480	436240	like, oh, God, we got to improve the performance. We got to improve the performance. Why is this
436240	442560	so hard? Why didn't they get it right the first time? And it turns out, unlike bugs, so code is
442560	448320	always buggy, right? And it's hard to debug programs, but it's like this thing produced the
448320	454000	wrong result. That's pretty clear. But if you have something and it just runs slower, you don't
454000	459360	really know where or what to change, right? So it can be really complicated. So what I'm going to
459360	464240	talk about are two things in this talk. One is performance analysis. So I'm going to explain
464240	469920	how to do performance analysis, right? It turns out that the thing that we, including myself, have
469920	475680	often done is not really very rigorous. And you can draw the wrong conclusions. And then I'm going
475680	480720	to talk about a new approach for performance profiling that will show you how to do it better.
480720	486720	All right? So first, I'm going to talk about performance analysis. So here's Luke. Luke has
486720	491840	his code. And Luke is like, it's too slow. Ogil is too slow. What am I going to do? And so Luke
491840	497360	has an idea. And Luke's idea involves some sort of new, you know, I'm going to do this first,
497360	500480	and I'm going to change this code. I'm going to change this function, blah, blah, blah. I make
500480	505600	a bunch of changes, right? And eventually I end up with A prime, all right? The new version of A.
505600	509680	And so now I want to know, did it work, right? So I've made this change. I thought it would
509680	515040	make things faster. So I go and I take my code, and I have some set of inputs, some benchmarks,
515040	522480	let's say. And I go and I say, run A. And A takes 90 seconds, which is clearly too long for your
522480	527440	App Store thing to run. But anyway, notwithstanding that, let's say there's something that takes
527440	535200	90 seconds in his test, right? And then he runs A prime. 87.5 seconds. Fantastic. Success, right?
535280	542640	2.8% faster, all right? Time for Mojito, okay? So this is great, right? And clearly, you know,
542640	549120	what Luke went and did had a big impact, big impact, all right? The question is, like, is it
549120	555520	really faster, right? So if you go and you plot, like, here's a bar graph with, I'm kind of giving
555520	562720	away the game here, one execution of A prime and A. It turns out that A prime is 2.8% faster,
562720	566720	looks good. But there's maybe a problem here. So what's the problem?
570080	574160	Right, so there's this problem called variance, right? Like, when you run a program once,
574160	577920	you're going to get some result. But if you run it again, maybe the result will be slightly
577920	582320	different, and you want to account for that. Great. So now we'll run it 30 times. 30 is the
582320	588560	magic number, by the way. So we're going to run it 30 times, and we get this graph. And so they're
588560	594080	pretty tightly distributed, and you can see it's still 2.8% faster, right? So seems plausible,
594800	599760	like, I think everybody here would probably be like, looks like A prime is faster. Great. So
599760	604160	the question you have to ask yourself is, why is it faster? So you might think, well, of course,
604160	610640	the reason is the code change, right? So I, as Luke, I'm the developer, and I go and I'm a genius,
610640	616720	and I have my great idea, and it pays off 2.8%, right? Well, it turns out changing the code
616720	622240	can actually lead to all sorts of knock-on effects that have nothing to do with your intended change.
622240	628480	So it could totally be an accident. So let me explain why. So there's this wonderful paper
629280	635520	that is, it appeared in ASPOS in 2009 by Todd Mitkiewicz and a number of other people. I highly
635520	640560	recommend you read it. It's something like, how to do things wrong without really trying, something
640560	647520	like that. And the conclusion is that the layout, like where the code and data end up in memory,
647520	652080	has a pretty big impact on performance, all right? So when you go to measure something,
652080	657600	those measurements are biased by depending where things kind of fell, right? So here are a few
657600	662080	things that can have an impact on layout, and I'm going to talk about more. So one is link order.
662080	667680	So if you're in cc++ land, and you have a make file, and the make file has a bunch of,
668000	672400	this link step and has a bunch of dot-os, depending how those dot-os are arranged,
672400	676880	you can get different performance, okay? You might think, fine, all right? Your environment
676880	682960	variables. So when you go to execute your program, your environment variables, whether it's in cc++
682960	687520	or even managed languages, they somehow get copied in and everything else gets shifted.
688480	696080	So in c and c++, this moves the stack. So this actually has an effect on layout. These two alone
696080	705200	can lead to shifts in performance of plus or minus 40%. Okay? So that's not great. So what is
705200	709600	happening? Like, why is this happening? This is a huge shift. This is literally larger than the
709600	719120	impact of dash o3 over dash o0. Okay? Yes, you laugh, but as well you should. So why is a prime
719120	725600	faster than a, right? So what is going on? Why could this happen without actually trying? So part
725680	731600	of the problem here is that basically modern processors have become insanely complicated
731600	737840	in their zeal to increase speed. So what do they do? So they have caches, right? Add data
737840	743520	and instructions, get mapped to the cache. Well, it turns out for good reasons, these things are
743520	749200	binned up into these things called sets. If they map to the same set, you can have a conflict. So
749200	753520	if you have hot code, a lot of hot code that is mapping to the same set, then it's not going to
753520	759040	necessarily fit in cache, and your code will run slower. By luck, you could be in a situation
759040	764480	where when you changed a prime, you actually disrupted this conflict. And so now you have no
764480	769840	conflict, right? These two things, one is the hot code and one maps to nothing. So no conflict.
769840	775600	Boom, it ran faster. All right? So that sounds great. So it could be the cache, but it could also
775600	781600	be the branch predictor, which actually, again, is based on the addresses of your branches,
781600	785520	and if these branches collide, then you can end up with things running slower.
786160	790720	There's also this thing called the TLB, the translation look-aside buffer, which maps
790720	794720	virtual addresses to physical addresses. If things don't fit in the TLB because they span
794720	799920	two pages instead of one, suddenly things become slower. There's also a branch target predictor.
799920	806000	There's a prefetchor. There's more. All right? So this is pretty bad. So all of these things can
806080	812240	happen. You might think, all right, link order is fine. The code thing is a little weird, but,
812240	818640	you know, hey, it's faster, right? It's 2.8% faster. That, like, I don't care. It's all good, right?
818640	824320	Now, it may not be faster on every machine, but it's faster today, right? So here's the problem.
824320	830400	Like, anything you do can disrupt this. So what could happen? One more malloc changes layout,
831040	835600	right? Like, you've shifted everything over, one more or less. If you upgrade anything in
835600	840560	your system, this is going to change layout, right? So that's bad. Okay. So those things,
840560	844400	all right, I'm not going to change libc, and I guess I'll never malloc again. Fine.
846640	853120	Whatever. All right? So here's something that may be surprising. Running it in a new directory.
853120	857760	So it turns out that your current working directory goes right into your environment
857760	863840	variables, right? So that's weird, right? So, you know, if Vader tries to run your software,
863840	870400	it's not going to work as fast because it's one character longer than Luke, okay? This is a real
870400	875920	effect. This can really happen. It has actually bitten me. I had a student who wrote something.
876800	882240	He has a long Indian last name. My username is just five letters long. It's just Emery.
882880	887360	And he did something. He's like, oh, it doesn't run any faster. It actually runs slower. And it's
887360	892720	like, that makes no sense at all. And eventually, we whittled it down, and it was like, if I run it
892720	904480	as me, it's faster. Okay? That's right. All right? Changes your layout. So the solution is obvious,
904480	911920	right? Run everything. All right. So I should add, you know, all of this is, you know, like,
911920	915360	the whole talk is really oriented towards, I'm going to improve my performance. But
915360	919760	everything I'm talking about today can be viewed in reverse for performance regression. Like,
919760	926800	I made a change, and things run 2.8% slower. Oh, God, roll back. Maybe not, right? Maybe
926800	932560	the next thing you do is going to actually undo that change, right? So basically, layout is super
932560	938640	brittle. And like you've seen, layout biases measurement. So one of the questions that we
938640	943520	wanted to know is, is it possible to eliminate the effect of layout? So we can actually understand
943520	948800	the performance of things kind of without having to think about, well, one malloc less or more,
948880	954720	or, you know, Luke versus Vader. So the answer is yes. We built a tool that we call stabilizer.
955360	961040	So stabilizer addresses this problem that I've just explained to you. Pardon me. And it eliminates
961040	969440	the effect of layout. So this is a way to actually measure programs where you can kind of actually
969440	974160	know whether the regression you had is real or whether the optimization you had is real
974160	979520	and not just an accident. So how does this work? How is this even possible? So the way that
979520	986480	stabilizer works is that it randomizes layout. So it randomizes a lot of things. It randomizes the
986480	992800	function addresses. It randomizes stack frame sizes. It even randomizes heap allocations. But not
992800	998480	only does it do those things, it does it over and over again. So while your program is running,
998480	1002800	it's literally doing randomization. And this turns out to be important, and I'll show you a graph
1002800	1009920	that we'll explain why. But basically, if you do this, then there's no way layout can bias your
1009920	1015680	measurement because a completely random layout can't bias the results. That's just how things work.
1015680	1021520	That's why we run randomized control trials. You've eliminated something as a possible cause.
1021520	1027440	The only other cause that remains is whatever change you made. So let's walk through what you
1027440	1032400	would do with stabilizer. So with stabilizer, again, clearly you're supposed to run your program
1032400	1037760	a bunch of times. But notice what happens to the execution times. Here the execution times are no
1037760	1043280	longer tightly bound around this one very small measurement. The reason for that is that when
1043280	1048720	you were running that program 30 times, it was sort of like you were going to do a survey of 30
1048720	1054320	people, but you just ask one person. Because it's the same layout over and over again. So you did
1054320	1060880	an experiment on 30 executions, but what you really did is you just repeated 30 on one. So the only
1060880	1066400	noise that you're eliminating is the noise that comes from network demons waking up or some other
1066400	1071600	random event, maybe some thermal issue in your computer, but it's not really altering layout.
1071600	1077760	It's always the same layout. So here it's not, and you get these very nice bell curves. So now I'm
1077760	1082640	going to ask you the question. So this is an audience poll time. Is A prime faster than A? I just
1082640	1089200	want you to raise your hands if you think that A prime is faster than A. All right, great. Now keep
1089200	1094800	your hands up. Don't set them down. But set them down if you change your mind. How about now?
1097680	1106400	How about now? There's still a few like hardcore. So what you all are doing is what I like to refer
1106400	1113920	to as eyeball statistics. And so you're kind of like, looks close to me. That's too close.
1114640	1123040	Right. But it turns out this is not actually a thing. So if you, yeah, it's not really statistics
1123600	1129520	when you just eyeball results. So this is a bit of a refresher for some of you, but I'm going to
1129520	1135040	walk you through this and how this all fits in with stabilizer. So in actual statistics, and today
1135040	1139920	I'm just going to talk about one flavor of statistics, which is called null hypothesis
1139920	1144800	significance testing. There are others, notably Bayesian approaches. Happy to talk about that
1144800	1150640	offline. But basically the way it works is you just assume that the things are the same. You say
1150640	1157440	what is the likelihood of observing this difference by chance? All right. So it turns out that this
1157440	1162000	is something that's just convenient. It's very easy to compute these probabilities for the normal
1162000	1167120	distribution, which you all remember from school. These graphs are normal. Awesome. It turns out
1167120	1171760	that stabilizer happens to make normal graphs or normal distributions. And I'll explain why.
1172720	1176000	So how are we going to do this? We're going to run stuff with stabilizer.
1176000	1181520	We're going to pick some probability below which we're like, okay, good enough. Right. So
1181520	1186240	if it's only a one in 20 chance, I see this probability like the, I see this event occurring.
1186240	1191040	I'll be like, okay, that's good enough for me. You could be harsher. You could say one in 100,
1191040	1199520	one in 1,000. It's pretty standard to say one in 20. This is the p value of 0.05. So the idea is
1199520	1204560	if there's a low enough probability, you reject the null hypothesis, the null hypothesis being
1204560	1208880	that they're the same. And you conclude that the speed up is real. It's not due to the effective
1208880	1214960	memory on memory layout. All right. So why re-randomization? The reason for re-randomization
1214960	1220800	is that just randomizing once doesn't give you enough randomization. So this is an actual program.
1221040	1226960	You can see the distribution is pretty wacky. It's very far from normal. You can't intuitively
1226960	1232720	explore much of the space when you just randomize at startup as opposed to randomizing during
1232720	1238480	execution. This is in fact the kind of distribution you get when you randomize all the time. And
1238480	1242720	these are normal distributions. So why do I keep saying that they're normal distributions?
1242720	1250000	The reason is essentially, again, going back to like freshman stats, stabilizer generates a new
1250000	1256960	random layout every half second. That is to say it's a completely independent version of the program
1256960	1261680	right from half second to half second to half second. It's all randomized. And it's the same
1261680	1266880	program the whole time. So it's identically distributed. And then we're adding them all up.
1266880	1273200	And there is this nice result, a key result of stats, which is the sum of a sufficient number.
1273200	1277760	So if you run a program for long enough of independent identically distributed random
1277760	1282560	variables, it's approximately normally distributed no matter what the underlying distribution was.
1282560	1287200	This is the central limit theorem. So this makes execution times normally distributed,
1287200	1292000	which is cool in other ways because you actually know how likely it is that you're going to see
1292000	1295840	some very weird execution because you know what the distribution looks like.
1295840	1299920	All right, great. So now we have this thing in hand and we're going to do something insane.
1300640	1306800	We're going to see whether optimizations actually matter. And we know some of them matter. So we
1306800	1310880	have a suite of benchmarks that we're going to evaluate it on. We're going to evaluate them
1310880	1314800	individually and then across the whole benchmark suite. And I'll show you how we do it. So you
1314800	1320880	build the benchmarks with stabilizer. Stabilizer is a plugin for LVM. If you just compile it as such,
1320880	1325600	it goes and randomizes everything. But you can actually just randomize things independently
1325600	1330400	if you wanted, like just code, just heap, and just stack. So now we run the benchmarks,
1330400	1335600	we run them as usual. We drop them into one of my least favorite programming languages ever.
1336880	1342480	And then we decide what the result is. So again, we don't ask questions like this
1342480	1346400	because that's eyeball stats. Instead, we ask a question like this,
1346400	1350080	pretend they're equal. How likely is it we'd observe this much of a difference?
1350560	1359680	So I also have to say that you should not assume normality like almost ever in my humble opinion,
1359680	1363920	unless you have very good reasons for doing so. Here we have very good reasons for doing so.
1363920	1369680	So we can use really powerful statistical tests like the student's t-test. So this is the test
1369680	1375120	that you used to actually measure this difference. So if the p-value, the likelihood of this event
1375120	1379200	occurring, is less than some threshold, which as I mentioned before is 5%, we're going to reject
1379200	1384880	the null hypothesis. That is to say, it's not because of random layout, the difference is real.
1385520	1390080	Everybody's on board, I hope. So now we're going to do it. You'll be shocked to learn
1390080	1398160	that dash O2 versus dash O1 makes a difference. Good. I would be weird if the result were otherwise.
1398160	1402320	So you can see that there are statistically significant improvements, right on the right.
1402320	1407760	There's some that are statistically significant but don't matter. And by God,
1407760	1414720	there are statistically significant performance drops. So it turns out that compiler people
1414720	1419440	run these same benchmarks and overfit the way that they do these optimizations,
1419440	1423760	and some of them lead to layout changes. And it wasn't actually the code change.
1424560	1429200	And so we can actually distill out this effect. All right, great. By and large,
1429200	1437920	it looks like O2 over O1 is a win. How about O3 versus O2? Ready? It's amazing. Okay.
1439920	1443360	So I actually have to change the axes so we can see a lot of these values.
1443360	1448720	So I'm going to zoom in. Instead of it being 0 to 10, like the range is negative 10 to 20,
1448720	1455200	I'm going to make it 0 to 1.5. Okay, so now we can see them. So they're pretty small effects,
1455920	1460960	but some of them are significant and some of them are not. Again, statistically significant,
1460960	1467760	1.75% decrease in execution time. Great. A bunch of things where it's not significant
1467760	1472320	and a couple decreases, but really very minor effect sizes. So what do these results mean?
1472960	1478560	I mean, you can't actually look at an individual benchmark. Like there's 30 of them, right?
1478560	1482240	So drawing a conclusion about all 30, you actually have to do something different.
1482240	1486880	You have to collect all of them. You get a bunch of these graphs, and this is what you don't do.
1486880	1493360	Like, okay, this one is slower. This one's faster. This one's faster. This is just eyeballs everywhere.
1494160	1499200	Okay? I mean, they're spooky and nobody wants to see those. So again, we're going to do the
1499280	1503360	same thing, but to test a bunch of things simultaneously, you do this thing, which is
1503360	1508160	terribly named, called analysis of variance, and you plug it into R with this awesome incantation,
1508880	1513120	and then you do, again, the same test. If the p-value is less than or equal to 5%,
1513120	1518720	we reject the null hypothesis. All right? You ready? All right, here we go. Here's the p-value.
1518720	1522880	So it has to be less than or equal to 5% or else we're going to conclude that dash
1522880	1532640	o3 versus dash o2 is nothing. All right? So the p-value is 26.4%. That means that one in four
1532640	1538480	experiments will just show a random effect, right? Just literally randomly. We do not consider
1538480	1544160	this enough evidence to reject the null hypothesis. So we're, we cannot reject the null hypothesis,
1544160	1549840	which is that the effect is indistinguishable from noise. All right? Okay. So this is all
1549840	1557280	terrible news for people like Luke who wanted optimizations to work, and I've actually seen,
1558400	1565120	I've actually seen projects. It makes, it kind of breaks your heart. Like projects I committed,
1565120	1573600	like on GitHub, where it literally says dash o9, and I feel like why not dash o11? There's no,
1573680	1579280	there's no dash o9 or 11. It's just kind of bottoms out. But you know, hope springs eternal.
1580000	1585680	All right. So great. So what are we going to do? So what people do when they can't speed things up,
1585680	1589920	right? They run a profiler. So there's these profilers, they all basically work the same way.
1590720	1594800	You go and you get some result, and it says, hey, here's where my program spent its time.
1595440	1600880	You get the number of calls to every function, runtime for each function, and this captures
1600880	1605360	intuitively, maybe for most of us, like this is what a profiler should do, right? What do I care
1605360	1610720	about? There's frequently executed code or code that runs for a long time. That's where I should
1610720	1616160	be focusing my optimization efforts. All right. It seems intuitively appealing. This is the way
1616160	1621120	profilers have been written since prof, back, you know, back from like, I don't know, late 60s,
1621120	1627120	early 70s. So would this in fact speed up Google? So we're going to do this experiment. We're going
1627200	1631760	to go and find the thing that runs for the longest amount of time and where it spends all of its
1631760	1637600	time running. And so we're going to run it. And so we go and we do this. And basically,
1637600	1643200	it makes the loading thing flash faster. Okay. So, well, guess what? That's frequently executed.
1643840	1650800	And in fact, it's the code that runs for the longest time, right? So this is not really great,
1650800	1656880	especially if Luke spent like more than a minute optimizing that code. That's a shame. All right.
1656880	1662480	So basically in some profilers were developed in an era where everything was synchronous and there
1662480	1668320	was a single core. All right. That's not today. Today things are asynchronous or parallel or
1668320	1673840	concurrent or a mixed thereof. And profilers don't do a good job in these contexts. So we need to
1673840	1679600	do better. So what would be really cool is if we could have something like this. So this is what
1679600	1686320	I call a causal profile. So a causal profile tells you visually, like, if I were to speed up this
1686320	1693040	component by this much on the x-axis, then the whole program will speed up by this much. All
1693040	1698800	right. So this is really nice. Like, if I had this graph, I would know I could spend a little
1698800	1704960	effort on the yellow search component and I'll get a big bang for my buck. Eventually, it's going
1704960	1710240	to bottom out or top out at like, you know, like 70, 80%. And the red one, I could just keep going,
1710240	1714240	right? Like, it gets faster and faster the more I work. And the blue one, I should never,
1714240	1719120	never optimize ever. All right. It would be cool to know this, right? It's essentially like an
1719120	1724880	oracle coming and telling you, this is the code you should work on, Luke. I don't know where I
1724880	1729920	got that way of talking. Anyway. All right. So the question is, how would we know this? Like,
1729920	1734400	how would we get this information? Like, how would we know that this change would cause this effect?
1734400	1739120	Right? We can't just go and optimize the program by arbitrary amounts and test it. That
1739120	1743360	kind of defeats the purpose. So we're going to do something different. We're going to run an
1743360	1751440	experiment. And it requires one ingredient here, which I'll refer to as the force. So we're going
1751440	1756320	to use magic. And we're going to speed things up magically. And then we're going to measure
1756960	1762400	how much the effect was of speeding up each component by a certain amount on overall program
1762400	1768240	execution. Okay? So we just keep doing this, right? We get more and more points, right? And then I do
1768240	1772000	it for different things. It turns out if I could speed up saving things to the three and a half
1772000	1777600	inch floppy, it doesn't make a difference, right? And so on. All right? Now, unfortunately, we live
1777600	1784320	in the real world where there's no magic. Sorry. Well, if there was magic, to be clear, this is not
1784320	1790480	what we would do, right? I mean, obviously, there are many much better things we could do. I could
1790480	1794400	think of people I would like to disappear off the face of the earth, for example. But I could also
1794400	1799040	disappear all the runtime off the face of the earth. Because why not? All right? So obviously,
1799040	1803840	that's what we would do. So we can't do that. We have to do something else. So what we are going
1803840	1808960	to do as our trick is we're going to do something that essentially takes advantage of this notion
1808960	1814880	of relativity. So we're going to do a virtual speedup. And a virtual speedup speeds things up
1814880	1819600	in scare quotes by slowing everything else down, right? So everything else that's running
1819600	1824720	at the same time will then be slowed down. And that will allow us to get the impact
1824720	1830720	of how do we sped this thing up? What would the results have been? So here, for example,
1830720	1838320	if we speed up the sending of the picture results back by a certain amount, we've slowed down
1838320	1844240	everything running concurrently with it. And then that gives us a result of a slowdown, which is the
1844240	1850160	same thing as the result of having sped it up. So we actually can get points on this graph
1850160	1854800	just by running these experiments. So I just got a point here, and I do it for everything,
1854800	1860560	and I get more points. And eventually, I get a graph like this. If I speed up indexing,
1860560	1864560	I'm going to get the exact same effect. Indexing is running at the same time as the compression.
1865440	1871760	So I get this result, and then bang, I get these results. And again, these are all the results.
1872400	1879280	Now, I draw your attention to the one weird blue thing. So the blue thing is slower,
1880160	1885680	and it turns out that sometimes optimizing things makes your program run slower. And the
1885680	1892240	intuition behind this is you can actually get congestion on a lock, or congestion for a shared
1892240	1897600	resource like disk or network. And so speeding things up makes things worse. You would like to
1897600	1904160	know this before you get started. That would be a very, very bad day for Luke that might necessitate
1904160	1909120	several sequels to recover from. All right, great. All right, so let's dig into Ogil a little bit.
1910400	1915600	So what do we care about in Ogil? We care about two things. We care about how long it takes
1915600	1921920	between a request and a response, a.k.a. latency. Traditional profilers don't do this at all.
1922000	1925680	It's just total runtime. Oh, let me get in my soapbox for one moment.
1926800	1930480	Traditional profilers are about end-to-end runtime. You know how your servers are all
1930480	1934880	about end-to-end runtime? Or your browser? Like, if only your browser could quit faster.
1936800	1940640	So again, like, it was all about, like, here's a program. I run at a console,
1940640	1944640	and it does something, and it's done, and that's all I cared about. That's not really today.
1945920	1949920	So there's latency. And then the more traditional thing is throughput. Again,
1949920	1953520	this is something that profilers do a bad job of measuring because they're all about end-to-end
1953520	1959840	execution time. So how fast results come back is throughput. So how are we going to do this?
1959840	1965040	So with our causal profiler that I'm going to explain in a minute, we're going to introduce
1965040	1970080	what we call progress points. So the notion of progress points is here's a thing I want to happen
1970080	1976000	faster, or here's a beginning and an end of things that I want to happen faster. So if Luke
1976000	1981600	wants responses to get sent faster, higher throughput, you just mark this particular start of this
1981600	1987440	component as a progress point, and every time the code runs, you go and you get another coin.
1988160	1993520	And then you can do this simultaneously, many requests for many users, and all of these things
1993520	1998880	are incrementing some counter. So these progress points are measuring throughput, and then you
1998880	2002960	basically are going to run the experiments and see what the effect is on the rate of those
2002960	2008160	progress points being executed. So one point measures throughput. Like I said,
2008160	2012960	if I speed up some component, whatever it might be, what is the effect? So now,
2013680	2018880	what if I care about latency? So we do the exact same thing. We set a progress point at the beginning,
2018880	2023440	a progress point at the end, and then the only thing that has to happen under the covers is it
2023440	2027920	has to have a counter. And the counter here measures how many things are in the system
2027920	2034800	at any one time. And it turns out that there is this awesome law that holds in a wide variety
2034800	2040400	of circumstances called Little's law. And so Little's law says that the latency is essentially
2040400	2045520	the number, the average number of transactions in a system divided by the throughput. We already
2045520	2049840	know how to measure throughput, so we just take advantage of Little's law and we can translate
2049840	2056320	this into latency. All right, great. So we have built a causal profiler for Linux. It already
2056320	2061520	ships with Debian and Ubuntu, so if you're using one of those systems, you can install it quite
2061520	2068960	easily. So it's just cause-profiler. It's quite easy to run. So you say cause run dash dash dash
2068960	2072960	and whatever your program is in its arguments and it fires it off and it starts doing performance
2072960	2078560	experiments. All right, I should add it's not entirely true. You do need to place progress
2078560	2082880	points. If you don't place any progress points, it will act like an ordinary profiler measuring
2082880	2087600	end-to-end execution time. But if you do put in progress points, then it will actually do its
2087600	2093200	magic. All right, and this is just some macro, like progress begin, progress end. All right,
2093200	2099280	so let's apply this to Augell. All right, I didn't actually build Augell. Neither did Luke,
2099280	2102960	but we're going to build it out of pieces like any good programmer would do. So it turns out
2102960	2108480	there's this suite of parallel applications that's kind of ready-made for this task. So there's a
2108560	2114320	deduplicator that does compression. There's an image comparator. And then there's a database,
2114320	2120320	SQLite. That's not in Parsec, but we'll use SQLite too. All right, great. So I'm going to show you
2120320	2124720	some fun things we did. This is a well-studied set of applications. People have already tried to
2124720	2130560	optimize these and we have covered a bunch of surprising optimization opportunities. So here's
2130560	2135440	Ferret. This is actually an older version of what our causal profile looks like. Now it runs in a
2135440	2140720	web browser. And you can see that there's these lines of code and it says, boy, if you speed up
2140720	2146640	these lines of code, then you're going to get performance increases. Conveniently, these lines
2146640	2151760	of code happen to be located in separate chunks of Ferret. So the part that does ranking, the part
2151760	2156960	that does indexing, the part that does segmentation. And why is this convenient? I'm not going to
2156960	2161920	have to change any code to make this faster. The reason is that what Ferret does is it has this
2161920	2168640	pipeline model and it assigns an equal number of threads to every stage in the pipeline. But
2168640	2174720	it turns out this one really doesn't need that many threads. So we take away the threads and just
2174720	2181840	by reassigning threads, we got a 20% speedup. So this is pretty cool because Caus actually
2181840	2187600	predicted it perfectly. So we increased ranking, for example, from 16 to 22 threads. That's a
2187680	2194640	27% increase in throughput. And on the graph, that says that that would translate to a 21%
2194640	2203360	overall improvement. And that's what we got. So Caus actually works. Good. So we were pretty happy
2203360	2211360	with this. We then are going to move on to Ddupe. So Ddupe is pretty hilarious. So here's Ddupe in
2211360	2217760	action. I have two pictures, Grumpy Cat 1 and Grumpy Cat Meme. And so now what do I do to
2217760	2222720	deduplicate these things? You can see that there's chunks that are the same. So you carve out the
2222720	2228080	chunks that are the same and you separate them out into individual pieces. And then an image is now
2228080	2235200	represented by the bits and pieces that make up the image. So here Grumpy Cat 1 is this piece
2235200	2240400	and Fun Is Awful is this piece. And you saved a lot of memory. So that's what Ddupe does.
2241040	2246160	So it does this compression via deduplication and it uses a hash function. So it throws everything
2246160	2251360	through a hash table. Great. So it's a pretty standard hash table. You just have some hash
2251360	2256720	table. It's an array. It's a bunch of bins. You get a bin number and then you go and you start
2256720	2263120	adding stuff to that bin into the bucket. So this all seems straightforward. You hope that it would
2263120	2269520	do something like this. The hash table is accessed concurrently by a bunch of threads, but they're
2269520	2275760	not idiots. There's not one big lock. It's just all locks, which is naive, but it's fine. But
2275760	2282160	surprisingly, cause says that the loop that accesses this list is important. Now, if you know
2282160	2287440	anything about hash tables, you know that things generally end up balanced, right? And it's weird
2287440	2291840	that you have this sort of situation. So we thought, all right, well, let's just make more,
2291840	2296560	more hash buckets, right? But we made a thousand of them. We really should have made a million,
2296560	2302960	cause, you know, a million. But anyway. So you would think this would lead to fewer
2302960	2309120	collisions, but it had no effect, right? So what else could be causing the collisions? Any guesses?
2310480	2316320	The hash function, exactly. Like this is one of those when all other possibilities have been
2316320	2320880	exhausted, right? You pick the weirdest one. That's not an exact quote. But anyway,
2321840	2326320	well, you're like, how can the hash function be broken? Like we've been using hash functions
2326320	2331200	since before Canuth wrote about them. Well, turns out people like to roll their own, cause it's
2331200	2338560	fun. And so we did a histogram of the number of items per bucket. So again, I told you there's
2338560	2349600	a thousand buckets. This is the histogram. Yeah. Hilariously, what they did is they used the pixels
2350240	2355920	that were taken from the image and they sum number of pixels and then added them. But that's
2355920	2361680	actually the central limit theorem in action, right? They're random, right? They're independent,
2361680	2366400	right? And you've summed them together. And so they actually formed the normal distribution.
2366400	2370720	That's not the distribution you want for a hash table. You would like a uniform distribution.
2371360	2380080	So literally, we changed one character. We changed the plus to XOR. So this. And we got this.
2388080	2392560	So, okay. I'll take the applause, but I mean, it was only a 9% speedup. But
2393760	2398160	all right. Nonetheless, it was one character. So I think it's the biggest bang for buck
2398160	2402800	ever recorded in optimization effort. So what did it predict? It turned out we can
2402800	2408080	also measure the accuracy of the prediction. So we knew that the blocks per bucket went from 76-ish
2408080	2414080	to 2. That's a 96% traversal speedup. And again, going back to the causal graph,
2414080	2419120	it predicted a 9% speedup, which is what we got, right? So it's working. All right. So finally,
2419120	2426320	I'm going to talk about SQLite. So I have no time left. But SQLite is pretty awesome. It's widely
2426320	2432880	used, as you all know. But it has this weird thing where it has a kind of strange virtual table
2432880	2438800	that they set up at compile time. And so whenever you actually indirect through a config to execute
2438800	2444000	a function like pthreadmutexunlock. So you would think, all right, why are you telling me about this?
2444000	2449440	Well, everything looks like this. This is an indirect call. Could be a direct call. That would
2449440	2457440	be faster. But an indirect call is not that slow. But it's almost the same cost as pthreadmutexunlock,
2457440	2461040	which means that you just doubled the length of all of your critical sections.
2461920	2468160	So that's not great. So in fact, when you go, so cause will highlight all of these lines and say,
2468160	2476640	you should definitely optimize these. So we undid all of the actual indirect stuff and just made
2476720	2482720	it so that at compile time, you change SQLite unlock to something so it doesn't do the indirect.
2482720	2490320	And it sped things up by 25%. If you look at a traditional profiler, by the way,
2490320	2495600	those things are like, this takes 0.0001% of time. You would never consider actually
2495600	2500480	trying to optimize that code. So we did it for a bunch of programs. We got some crazy speed ups.
2500480	2506800	My favorite is we got a 68% speed up by replacing a custom barrier with a standard barrier.
2507680	2515360	Again, people should stop doing things at home. So anyway, so I'm going to conclude.
2515360	2520800	So you can take a picture of this to jump to work from our lab, which is plasmaumass.org.
2520800	2525600	I talked today about sound performance analysis and effective performance profiling. Everybody
2525600	2528720	should go use the cause. All right. Thanks for your attention.
