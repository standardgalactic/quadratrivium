1
00:00:00,000 --> 00:00:04,000
Howdy, this is Jim Rutt, and this is The Jim Rutt Show.

2
00:00:04,000 --> 00:00:11,000
Music

3
00:00:11,000 --> 00:00:16,000
Listeners have asked us to provide pointers to some of the resources we talk about on the show.

4
00:00:16,000 --> 00:00:22,000
We now have links to books and articles referenced in recent podcasts that are available on our website.

5
00:00:22,000 --> 00:00:24,000
We also offer full transcripts.

6
00:00:24,000 --> 00:00:29,000
Go to JimRuttShow.com. That's JimRuttShow.com.

7
00:00:29,000 --> 00:00:32,000
Music

8
00:00:32,000 --> 00:00:35,000
Today's guest is Alicia Herrero.

9
00:00:35,000 --> 00:00:41,000
She's a professor of philosophy emerita at Prince George's Community College in Maryland.

10
00:00:41,000 --> 00:00:43,000
This is kind of old home week.

11
00:00:43,000 --> 00:00:48,000
I actually grew up in PG County as we used to call it in the day.

12
00:00:48,000 --> 00:00:53,000
They tell me you're not supposed to say PG anymore, but that's what we always call it.

13
00:00:54,000 --> 00:01:00,000
I lived there from the time I was two to the time I was 22, and my wife is from PG also.

14
00:01:00,000 --> 00:01:06,000
My brother is a distinguished alum of Prince George's Community College,

15
00:01:06,000 --> 00:01:11,000
and probably half my friends went there, so it's great to connect with Alicia.

16
00:01:11,000 --> 00:01:13,000
She is a complexity person.

17
00:01:13,000 --> 00:01:16,000
She is the author of Dynamics in Action.

18
00:01:16,000 --> 00:01:20,000
I have not read that book, but I ordered it and look forward to reading it.

19
00:01:20,000 --> 00:01:26,000
It's very interesting, and she's co-editor of Reframing Complexity, Perspectives of North and South,

20
00:01:26,000 --> 00:01:31,000
and Emergence, Self-Organization, and Complexity, Precursors, and Prototypes.

21
00:01:31,000 --> 00:01:36,000
And she's written lots of publications and refereed philosophy journals.

22
00:01:36,000 --> 00:01:41,000
She's also written a couple of things that I read while I was doing my research for this episode

23
00:01:41,000 --> 00:01:43,000
that I think the audience might find interesting.

24
00:01:43,000 --> 00:01:47,000
One's called Downward Causation, Poliani, and Progosian,

25
00:01:47,000 --> 00:01:50,000
and another one called Western Science and Philosophy.

26
00:01:50,000 --> 00:01:54,000
Can't deal with the relations between parts and holes.

27
00:01:54,000 --> 00:01:57,000
You know, they're pretty serious, but they're not quite scholarly papers, right?

28
00:01:57,000 --> 00:02:02,000
And so I think our audience could deal with them, and as always, links to those papers

29
00:02:02,000 --> 00:02:06,000
and the books will be on our website at JimRucho.com.

30
00:02:06,000 --> 00:02:08,000
So, welcome, Alicia!

31
00:02:08,000 --> 00:02:12,000
Thank you for having me. It's a pleasure to meet you.

32
00:02:12,000 --> 00:02:16,000
Yeah, it's a very good conversation. I really enjoyed reading the book.

33
00:02:16,000 --> 00:02:21,000
It's quite short, 235 pages, but it is chock-full of stuff.

34
00:02:21,000 --> 00:02:25,000
I mean, there's lots of ideas in this book, but we probably aren't going to get to them all.

35
00:02:25,000 --> 00:02:30,000
My topic list is, I try to keep it to seven pages, but it looks like I got more like 11,

36
00:02:30,000 --> 00:02:32,000
so we'll see how far we can get into it.

37
00:02:32,000 --> 00:02:35,000
But I'd also like your writing styles very clear.

38
00:02:35,000 --> 00:02:37,000
Oh, no, it is not.

39
00:02:37,000 --> 00:02:41,000
You must have not read much of it because it is horrendously...

40
00:02:41,000 --> 00:02:45,000
No, no, for philosophy book, it's damn readable.

41
00:02:45,000 --> 00:02:48,000
No, no, no, it's awful. I wish I were here.

42
00:02:48,000 --> 00:02:51,000
Well, no, no, I would disagree. I would disagree.

43
00:02:51,000 --> 00:02:55,000
But today we're going to talk about her newest book just published called

44
00:02:55,000 --> 00:03:01,000
Context Changes Everything, How Constraints Create Coherence.

45
00:03:01,000 --> 00:03:04,000
So, the book was just published a couple of weeks ago, right?

46
00:03:04,000 --> 00:03:06,000
A couple of weeks ago, yes.

47
00:03:06,000 --> 00:03:09,000
Well, again, we'll have a link to the book on the episode page.

48
00:03:09,000 --> 00:03:11,000
So, let's actually start with something.

49
00:03:11,000 --> 00:03:13,000
We talk about a fair bit.

50
00:03:13,000 --> 00:03:16,000
A number of our guests, for some reason, have found this framing useful.

51
00:03:16,000 --> 00:03:19,000
And that is Aristotle's Four Causes.

52
00:03:19,000 --> 00:03:22,000
This is a theme that runs throughout the book.

53
00:03:22,000 --> 00:03:27,000
Why don't you remind the audience what the Four Causes are and what they are?

54
00:03:27,000 --> 00:03:31,000
Well, I think Aristotle probably got it from the potter's wheel.

55
00:03:31,000 --> 00:03:38,000
You know, he started thinking about causes and effects using the example of the potter's wheel.

56
00:03:38,000 --> 00:03:45,000
Four Causes are material cause, the clay, the stuff from which the pot will be made.

57
00:03:45,000 --> 00:03:54,000
Then final cause or purpose or teleology, which is the goal to which the thing that you're making will be put,

58
00:03:54,000 --> 00:03:56,000
which is pouring water.

59
00:03:56,000 --> 00:03:59,000
So, the final cause of the picture would be water.

60
00:03:59,000 --> 00:04:02,000
The formal cause, it's not quite shape.

61
00:04:02,000 --> 00:04:06,000
It's sort of the what makes a picture a picture.

62
00:04:06,000 --> 00:04:12,000
So, it is the essence, the basic fundamental identity of the thing.

63
00:04:12,000 --> 00:04:24,000
And then finally, efficient cause, which is the actual force exerted by the hands and the on the on the clay to turn it into the picture.

64
00:04:24,000 --> 00:04:25,000
So, it's energetic.

65
00:04:25,000 --> 00:04:28,000
Efficient cause is energetic exchange.

66
00:04:28,000 --> 00:04:34,000
One of the things, the points you make is that prior to maybe the late 16th century,

67
00:04:34,000 --> 00:04:40,000
people tended to consider all those causes when they were thinking about nature and the pre-scientific era.

68
00:04:40,000 --> 00:04:51,000
But one of the moves that probably came off accidentally more or less from the invention of modern science was a very heavy focus on the efficient cause.

69
00:04:51,000 --> 00:04:52,000
Correct, correct.

70
00:04:52,000 --> 00:04:57,000
In a sense, material cause, people figured, well, science will take care of that.

71
00:04:57,000 --> 00:05:01,000
And formal cause and final cause were sort of discarded.

72
00:05:01,000 --> 00:05:02,000
They went out with a bustle.

73
00:05:02,000 --> 00:05:05,000
You know, this is something we don't have to worry about anymore.

74
00:05:05,000 --> 00:05:10,000
So, we can explain everything in terms of forceful causes.

75
00:05:11,000 --> 00:05:25,000
It seems to me that when you're talking about complex dynamical systems, somehow final cause and formal cause kind of gets snuck in indirectly, not the way Aristotle thought.

76
00:05:25,000 --> 00:05:43,000
People were born with an act or organisms or all natural phenomena had an inherent internal and teleki internal kind of like an acorn has the form of an oak built into it.

77
00:05:43,000 --> 00:05:48,000
And all it has to do is unroll and unfold into the final form.

78
00:05:48,000 --> 00:06:02,000
But I think what we have now with complex dynamical systems is that interactions with the environment and in my view constraints are the contemporary version of formal and final cause.

79
00:06:02,000 --> 00:06:10,000
Yeah, and the over focus on efficient cause, matter and motion bumping into each other.

80
00:06:10,000 --> 00:06:18,000
I often refer to that as naive Newtonianism and you know, most nerdy smart kids go through that period, right?

81
00:06:18,000 --> 00:06:30,000
And they make the error of thinking the famous Laplacian area where he says, yeah, give me the position and velocity of everything in the universe and I can predict the future in the past with total precision.

82
00:06:30,000 --> 00:06:36,000
But fortunately, once you get exposed to ideas of complexity, you realize it's completely crazy.

83
00:06:36,000 --> 00:06:45,000
And that has actually caused for those people who've gotten the complexity lens to realize there's much more to our universe than naive Newtonism.

84
00:06:45,000 --> 00:06:49,000
But, but it's amazing how it's persisted and you can't blame people.

85
00:06:49,000 --> 00:06:56,000
I mean, it seems to do a hell of a job predicting eclipses, but even Newton knew the three body problem would mess things up.

86
00:06:56,000 --> 00:06:57,000
Right.

87
00:06:57,000 --> 00:07:05,000
But that was a warning Newton gave us that went on heated for many centuries.

88
00:07:05,000 --> 00:07:12,000
My first paper was about Kant and Prigogene and exactly about that subject matter.

89
00:07:12,000 --> 00:07:13,000
Yeah.

90
00:07:13,000 --> 00:07:19,000
And then as you talk about in your book, one of the current manifestations of it, though, I will say it depends where you are in the sciences.

91
00:07:19,000 --> 00:07:23,000
Now, I've had the good fortune to be around complexity people for the last 20 years.

92
00:07:23,000 --> 00:07:26,000
And so there's much, much, much less of that there.

93
00:07:26,000 --> 00:07:33,000
But I suppose out in the wilds of solid state physics and places like that, there's still a lot about what you call nothing but ism.

94
00:07:33,000 --> 00:07:34,000
Right.

95
00:07:34,000 --> 00:07:37,000
Explain what nothing but ism is.

96
00:07:37,000 --> 00:07:49,000
Well, the idea that the whole is nothing but the sum of its parts and therefore anything that appears to be an emergent property is really an epiphenomenon.

97
00:07:49,000 --> 00:07:55,000
It is, it is, it is sort of froth that's thrown up, but it really has no causal power.

98
00:07:55,000 --> 00:08:07,000
Well, of course, not because if causal power is only thought of as efficient causality, then clearly the synchronization of the photon streams in a laser beam.

99
00:08:07,000 --> 00:08:13,000
Don't align their component laser beams as another efficient cause.

100
00:08:13,000 --> 00:08:14,000
So that's the problem.

101
00:08:14,000 --> 00:08:18,000
Yeah, that's it leads very quickly to absurd conclusions.

102
00:08:18,000 --> 00:08:21,000
What makes me wonder why did it hang in there so long?

103
00:08:21,000 --> 00:08:23,000
Well, and you know where it hung in.

104
00:08:23,000 --> 00:08:31,000
I came at this because I wrote a dissertation on the difference between explaining and justifying behavior.

105
00:08:31,000 --> 00:08:34,000
Well, justification has to do with moral reasons so on.

106
00:08:34,000 --> 00:08:42,000
But explaining behavior, the first sense of that first book that you quoted is what is the difference between a wink and a blink?

107
00:08:42,000 --> 00:08:46,000
Presumably, it's the cause, right?

108
00:08:46,000 --> 00:08:55,000
Meaning that an intention causes a wink, but a, my throwing some sand in your face would cause you to blink.

109
00:08:55,000 --> 00:09:04,000
Well, but then the next question is what the hell's an intention and how can an intention cause the action in an efficient cause way?

110
00:09:04,000 --> 00:09:05,000
Correct?

111
00:09:05,000 --> 00:09:10,000
And my answer was obviously, well, there can't be just one neuron pushing another neuron pushing.

112
00:09:10,000 --> 00:09:11,000
I mean, forget it.

113
00:09:11,000 --> 00:09:12,000
That doesn't work that way.

114
00:09:12,000 --> 00:09:21,000
And at the time I was living in Berkeley and I was hanging around people who were into network theory and systems analysis and so on.

115
00:09:21,000 --> 00:09:22,000
I'm going, you know what?

116
00:09:22,000 --> 00:09:24,000
There's got to be a network property.

117
00:09:24,000 --> 00:09:26,000
We're talking the 70s gym.

118
00:09:26,000 --> 00:09:41,000
There's got to be a network property that somehow produces emergent properties, which in turn can loop back down and cause the neurons that are responsible for motor control to move the arm in a way that it satisfies the intention that I started out with.

119
00:09:41,000 --> 00:09:42,000
All right.

120
00:09:42,000 --> 00:09:43,000
So how is this going to work?

121
00:09:43,000 --> 00:09:45,000
You were talking about PG community college.

122
00:09:45,000 --> 00:09:49,000
The nice thing about teaching a community college is nobody gives it them to publish anything or not.

123
00:09:49,000 --> 00:09:52,000
You're going to be judged by how well you teach.

124
00:09:52,000 --> 00:09:54,000
So nobody cares about whether you publish.

125
00:09:54,000 --> 00:10:01,000
On the other hand, nobody's forcing you to write papers on the fifth decimal point of the existing theory.

126
00:10:01,000 --> 00:10:10,000
So I could play around and in this area in DC, I could hang around NIH and start listening to people come talk.

127
00:10:10,000 --> 00:10:15,000
And so you start understanding how patterns in neural systems work and so on.

128
00:10:15,000 --> 00:10:16,000
So it's got to be something.

129
00:10:16,000 --> 00:10:18,000
It's got to be something like that.

130
00:10:18,000 --> 00:10:21,000
So that's when I decided, all right, you know what?

131
00:10:21,000 --> 00:10:24,000
I'm going to give the term cause to the Newtonians.

132
00:10:24,000 --> 00:10:25,000
I'm not going to fight that battle.

133
00:10:25,000 --> 00:10:27,000
It's going to take me forever to fight that battle.

134
00:10:27,000 --> 00:10:39,000
So instead, what I want to do is look at the notion of constraint because as soon as the hard scientists get into trouble with that Newtonian silly understanding of efficient causes.

135
00:10:39,000 --> 00:10:44,000
You mentioned they retreat and hide behind the notion of constraint.

136
00:10:44,000 --> 00:10:46,000
And I thought, you know what?

137
00:10:46,000 --> 00:10:47,000
That's going to work.

138
00:10:47,000 --> 00:11:00,000
So that's that's how the whole that's how my, my, my trajectory towards reconceptualizing causality and specially formal and final cause in terms of constraints developed.

139
00:11:00,000 --> 00:11:01,000
It's interesting.

140
00:11:01,000 --> 00:11:05,000
And actually it worked well with the little analogy I use.

141
00:11:05,000 --> 00:11:16,000
When I'm trying to explain complexity to just random people at a party or something, I will say you can think of reductionism, you know, classic science as the study of the dancer.

142
00:11:16,000 --> 00:11:19,000
While complexity is the study of the dance, right?

143
00:11:19,000 --> 00:11:22,000
And a dance is not random motion.

144
00:11:22,000 --> 00:11:24,000
It has constraints, right?

145
00:11:24,000 --> 00:11:26,000
If it's going to be, it's going to be a jitterbug.

146
00:11:26,000 --> 00:11:28,000
It has one form of constraints.

147
00:11:28,000 --> 00:11:30,000
If it's going to be a waltz, it has another.

148
00:11:30,000 --> 00:11:35,000
And so when I read that and saw the movie you were making, I say, works perfectly with my good old.

149
00:11:35,000 --> 00:11:36,000
Absolutely.

150
00:11:36,000 --> 00:11:38,000
My good old analogy.

151
00:11:38,000 --> 00:11:39,000
Absolutely.

152
00:11:39,000 --> 00:11:42,000
And it held held together quite well.

153
00:11:42,000 --> 00:11:48,000
So now let's move on to your next topic, which is a term that most non philosophers will have never heard of.

154
00:11:48,000 --> 00:11:51,000
I've heard of it a few times, but it's not term we use very often.

155
00:11:51,000 --> 00:11:52,000
Santa Fe Institute.

156
00:11:52,000 --> 00:11:53,000
Myriology.

157
00:11:53,000 --> 00:11:55,000
Is that how you pronounce that?

158
00:11:55,000 --> 00:11:56,000
Myriology.

159
00:11:56,000 --> 00:11:59,000
And that's, again, the whole parts, parts whole.

160
00:11:59,000 --> 00:12:04,000
Because let's use an example from Brian Arthur's note that I love.

161
00:12:04,000 --> 00:12:10,000
That's a Santa Fe book on complexity of economics, economics and complexity.

162
00:12:10,000 --> 00:12:25,000
What, what a, an economy is, are a bunch of individual elements that interact in a constrained way to result in an

163
00:12:25,000 --> 00:12:33,000
emergent phenomenon that has certain properties that the components don't have.

164
00:12:33,000 --> 00:12:39,000
An economy has certain characteristics that the individual trader and seller don't.

165
00:12:39,000 --> 00:12:50,000
But once that whole WHLE, which is a coherent whole, and I really want to emphasize the fact that a coherent whole is different from a

166
00:12:51,000 --> 00:12:55,000
mass clump of stuff because it's organized.

167
00:12:55,000 --> 00:12:59,000
And what organizes it are the constraints.

168
00:12:59,000 --> 00:13:12,000
But once it's organized, then all of a sudden the components also acquire different properties because suddenly they are now traders and regulators and so on and so forth.

169
00:13:12,000 --> 00:13:13,000
Correct.

170
00:13:13,000 --> 00:13:16,000
So there you have the whole part.

171
00:13:16,000 --> 00:13:23,000
The parts of the components, correct, and, and the interact the constrained interactions among the components.

172
00:13:23,000 --> 00:13:29,000
The whole is what I'm calling in this new book a constraint regime.

173
00:13:29,000 --> 00:13:42,000
Because one of the problems we've also had, I don't know if it's Newtonianism or what, but we tend to reify things, things, no pun intended, we tend to reify things.

174
00:13:42,000 --> 00:13:59,000
But an economy is nothing other than all these constraints all held together by an overarching set of constraint regimes once the constraints close into it into a coherent whole.

175
00:13:59,000 --> 00:14:08,000
But they can then loop back down and affect their components and they acquire, they acquire, now they have a role, right?

176
00:14:08,000 --> 00:14:19,000
In a society, once a society is a socially organized structure, people can be citizens, they can be senators, they can be teachers.

177
00:14:19,000 --> 00:14:24,000
Those roles don't exist except within an organized society.

178
00:14:24,000 --> 00:14:34,000
So then the next question is, well, on a more general level, what is the relationship between the parts and the holes and the holes and the parts?

179
00:14:34,000 --> 00:14:42,000
If you really buy the whole Newtonian idea, ain't no difference between the parts and the holes, the holes are nothing but the sum of the parts.

180
00:14:42,000 --> 00:14:54,000
But that means you cannot explain how it is that my behavior is constrained top down by my living in a particular culture.

181
00:14:54,000 --> 00:15:05,000
The fact that I was born and raised in Cuba, the fact that all of these affect my behavior, but they don't do that as an efficient cause.

182
00:15:05,000 --> 00:15:17,000
A culture, an economic system doesn't cause my behavior to differ in any kind of efficient cause.

183
00:15:18,000 --> 00:15:25,000
But since the Newtonian Revolution, pretty much myriology got thrown out of the picture.

184
00:15:25,000 --> 00:15:31,000
So that messes up philosophy of mind because you cannot explain mental events.

185
00:15:31,000 --> 00:15:40,000
If you think of mental events as emergent properties because they should be reducible to a bunch of neurons pushing each other around.

186
00:15:40,000 --> 00:15:44,000
But where does the emergent property go when you have that?

187
00:15:44,000 --> 00:15:50,000
I'll tell you a little example. I've got my little homey examples that I love to use on this as people who doubt.

188
00:15:50,000 --> 00:15:53,000
We'll get this later, the idea of top down causality.

189
00:15:53,000 --> 00:15:59,000
I say, let's imagine me dead and run through a blender and poured into a bathtub, right?

190
00:15:59,000 --> 00:16:05,000
What's the chances that those chemicals are going to hop out of the bathtub, walk down the hill and go to the ice cream store?

191
00:16:05,000 --> 00:16:07,000
Essentially zero, right?

192
00:16:07,000 --> 00:16:14,000
And on the other hand, if it's me and I'm operating up in cognitive space and it's after dinner and we just had a nice dinner

193
00:16:14,000 --> 00:16:21,000
and I feel like a walk with my wife to go down and get something nice, then we may decide to haul all these atoms and molecules

194
00:16:21,000 --> 00:16:27,000
and they have nothing to say about it because this top down idea, well, let's go get some ice cream, causes the,

195
00:16:27,000 --> 00:16:33,000
and by the way, Newtonian physics is never, or let's say physics is never violated, right?

196
00:16:33,000 --> 00:16:37,000
The atoms are dragged along and they are dragged along through efficient causes.

197
00:16:37,000 --> 00:16:45,000
You know, they're all stuck together and bound by various forces, but the decision to go get the ice cream came at a higher level in the stack.

198
00:16:45,000 --> 00:16:51,000
And, you know, and again, some of the attempts to deny that that happens is just bizarre, right?

199
00:16:51,000 --> 00:16:54,000
Well, it is to me and you, but it isn't to an awful lot of people.

200
00:16:54,000 --> 00:17:00,000
And even folks, and I may, I hope there hasn't been a change in the last couple of years,

201
00:17:00,000 --> 00:17:17,000
but even somebody like Brian Rockland, who recognizes the reality of the emergent properties nonetheless gets very weary about ascribing causal properties,

202
00:17:17,000 --> 00:17:23,000
causal powers, let's use that for causal powers to those emergent properties.

203
00:17:23,000 --> 00:17:27,000
In a sense, the notion of supervenience is continuous.

204
00:17:27,000 --> 00:17:37,000
So they're very few of us, people like Carl Gillette, people like Robert Bishop, they're very few of us who are willing to go the next step and say,

205
00:17:37,000 --> 00:17:50,000
not only are emergent properties real, they have causal powers with respect to their own components and that's what homeostasis is for God's sake.

206
00:17:51,000 --> 00:17:59,000
Homeostasis readjusts the metabolism, the neural system and so on in order to maintain the integrity of the whole.

207
00:17:59,000 --> 00:18:11,000
But the idea that nonetheless that should be explicable in a reductionist fashion, the power aspect of it is still not quite,

208
00:18:11,000 --> 00:18:13,000
the physicists haven't bought it.

209
00:18:13,000 --> 00:18:17,000
Philosophers, I hate to say this, I have, I never read philosophy anymore.

210
00:18:17,000 --> 00:18:25,000
I, because all the, I grew up, I was trained as an analytic philosophy for in the United States, all they're worried about is the meaning of words.

211
00:18:25,000 --> 00:18:27,000
I mean, they're very few people.

212
00:18:27,000 --> 00:18:40,000
And you can tell why, how this is when you have people like Chalmers and Christoph Koch, who Chalmers has sort of thrown in the towel and he's really saying,

213
00:18:40,000 --> 00:18:48,000
the only way you're going to have mental properties is if you build them in from the get go from the, from the bot, you know, so therefore electrons have mental quality.

214
00:18:48,000 --> 00:18:51,000
Oh, come on, give me a break.

215
00:18:51,000 --> 00:18:52,000
Yeah.

216
00:18:52,000 --> 00:18:58,000
We had Christoph Koch on the show sometime back and he actually is a panpsychic.

217
00:18:59,000 --> 00:19:01,000
But you, but do you understand why?

218
00:19:01,000 --> 00:19:15,000
Because if all of reality comes from innate internal fundamental properties that only interact with efficient causes, there's no way you can get a coherent whole.

219
00:19:15,000 --> 00:19:24,000
And that's the key word coherently organized that then and from which emergent causal powers come.

220
00:19:24,000 --> 00:19:31,000
And it's interesting that they, another extremely bright guy named Ben Gertzel has been on my show many times, good friend of mine.

221
00:19:31,000 --> 00:19:37,000
I think he's a panpsychic for exactly that reason, because otherwise he's wedded to his model of the universe.

222
00:19:37,000 --> 00:19:39,000
And where does consciousness come from?

223
00:19:39,000 --> 00:19:45,000
If it doesn't, if it isn't innate, while a complexitarian would say, well, it's obviously emerged just another level on the stack, right?

224
00:19:45,000 --> 00:19:46,000
And that's not that hard.

225
00:19:46,000 --> 00:19:47,000
Correct.

226
00:19:48,000 --> 00:20:08,000
But it's this notion that my nature, you know, oh, it's in my nature is somehow given as an essence in the fundamental tiny, tiny bits that make up the rest of it.

227
00:20:08,000 --> 00:20:13,000
That's still much more widespread than one would think.

228
00:20:13,000 --> 00:20:14,000
Yeah.

229
00:20:14,000 --> 00:20:16,000
We'll get to supervenience later.

230
00:20:16,000 --> 00:20:20,000
And I think we have a somewhat different point of view, but not entirely different.

231
00:20:20,000 --> 00:20:21,000
We'll get to that later.

232
00:20:21,000 --> 00:20:32,000
But I think we're on the same page that, hey, this need to smuggle in something like consciousness or cognition as a fundamental property of matter seems to be a big overreach.

233
00:20:32,000 --> 00:20:36,000
I mean, do we try to smuggle in the fundamental nature of digestion?

234
00:20:36,000 --> 00:20:37,000
I don't think so, right?

235
00:20:37,000 --> 00:20:41,000
But we get overly confused when it comes to cognitive processes.

236
00:20:41,000 --> 00:20:42,000
But digestion is a good example.

237
00:20:42,000 --> 00:20:44,000
And that's the one John Searle has always used.

238
00:20:44,000 --> 00:20:45,000
That's why I always use it.

239
00:20:45,000 --> 00:20:46,000
Yeah, exactly.

240
00:20:46,000 --> 00:20:47,000
And I used to fight him.

241
00:20:47,000 --> 00:20:51,000
We were on the NEH board together.

242
00:20:51,000 --> 00:20:58,000
But the thing about digestion is digestion really doesn't do much.

243
00:20:58,000 --> 00:21:00,000
I mean, it is an effect.

244
00:21:00,000 --> 00:21:03,000
Digestion is an effect of all these other processes.

245
00:21:04,000 --> 00:21:11,000
The reason I like homeostasis is because the metastability of homeostasis is causally effective.

246
00:21:11,000 --> 00:21:21,000
It does, if I have the fudge brownie, it's going to switch all my glucose and all everything else around to keep the integrity of the whole.

247
00:21:21,000 --> 00:21:32,000
So it has caused what I would want to call causal powers, but I've given up that term because people will say, oh, but it is an efficient causation.

248
00:21:32,000 --> 00:21:33,000
No, that's right.

249
00:21:33,000 --> 00:21:34,000
It isn't.

250
00:21:34,000 --> 00:21:40,000
But it is DS Lewis's notion of cause meaning without which not.

251
00:21:40,000 --> 00:21:42,000
If that weren't the case, it wouldn't be.

252
00:21:42,000 --> 00:21:46,000
So that's the causal notion I would espouse.

253
00:21:46,000 --> 00:21:56,000
Okay, let's now make the next step, which is why don't you start to define what you mean by constraints and lay out a taxonomy of different kinds of constraints.

254
00:21:56,000 --> 00:21:59,000
This is where the book really started to get into new territory for me.

255
00:21:59,000 --> 00:22:01,000
I've had it very interesting.

256
00:22:01,000 --> 00:22:04,000
And that's what the new one is all about.

257
00:22:04,000 --> 00:22:08,000
That's what the new one is all about.

258
00:22:08,000 --> 00:22:19,000
I think in the first book, I made a distinction barring from Lila Gatlin's book called information in the living system, which was from 1960, something or other.

259
00:22:19,000 --> 00:22:27,000
A distinction between I use the term at the time, context free and context sensitive or context dependent.

260
00:22:27,000 --> 00:22:32,000
There's a lot of grief from people saying ain't nothing that's context independent.

261
00:22:32,000 --> 00:22:33,000
All right.

262
00:22:33,000 --> 00:22:35,000
So in this one, I call it context.

263
00:22:35,000 --> 00:22:37,000
I mean, nothing's context free.

264
00:22:37,000 --> 00:22:53,000
So in this book, I call it context independent and context dependent context independent according to Lila Gatlin are conditions that take a system far from equi probability.

265
00:22:53,000 --> 00:22:59,000
Now, whatever takes conditions away from random from white noise from random noise.

266
00:22:59,000 --> 00:23:08,000
So if you institute a gradient, you have you have instituted a context independent constraint.

267
00:23:08,000 --> 00:23:22,000
If you institute if you institute polarity or charge, we were talking the early universe here probably that were those were probably the earliest constraints.

268
00:23:23,000 --> 00:23:38,000
And I call them context independent because in a sense what they do is they set the context in a sense they set the boundaries of possibility space and inhomogeneities within that possibility space.

269
00:23:38,000 --> 00:23:42,000
So it's no longer white noise.

270
00:23:42,000 --> 00:23:51,000
Lila Gatlin calls context dependent constraints, those that take a system away from independence.

271
00:23:51,000 --> 00:23:58,000
So one of the things that's that I find interesting also is that we might put Newtonian is now.

272
00:23:58,000 --> 00:24:06,000
But when you look at the second law of thermodynamics and I'm petrified because I kind of talk about the second law of thermodynamics in this new book.

273
00:24:06,000 --> 00:24:17,000
But according to the Boltzmannian everybody's interpretation of the second law of thermodynamics, the the events in the particles are independent of one another.

274
00:24:17,000 --> 00:24:22,000
And I think you're never going to get complexity if you have independent particles.

275
00:24:22,000 --> 00:24:37,000
That's the beauty of Stuart Kaussman's button example button mesh example, you know, you tie one button to another tower, and all of a sudden, the thing turns into a mesh, right you have a phase transition, and you have a mesh.

276
00:24:37,000 --> 00:24:46,000
So what would I consider context dependent constraints that take a system far from independence.

277
00:24:46,000 --> 00:24:50,000
I would consider catalysts.

278
00:24:50,000 --> 00:24:56,000
Context dependent constraints. I would consider feedback loops.

279
00:24:56,000 --> 00:25:15,000
Context dependent constraints. And I think since that first book was published 20 some years ago, what's nice to me is the burgeoning of epigenetics nowadays, because if epigenetics isn't an example of context dependent constraints with a vengeance.

280
00:25:15,000 --> 00:25:31,000
I don't know what is so once you have all these context dependent constraints, acting inside a context independent possibility space set possibility space.

281
00:25:31,000 --> 00:25:43,000
Then I think the possibility of complexity appears you don't have a possibility of complexity just with emergent, just with efficient causes.

282
00:25:43,000 --> 00:25:58,000
These forms of constraints do. I also make a distinction between temporal and spatial constraints. I give two examples from from playground devices.

283
00:25:58,000 --> 00:26:17,000
In the playground swing. The child learns very quickly that when they kick is as important as how strong they kick. And the timing of the kick does not impart more energy to the kick.

284
00:26:17,000 --> 00:26:27,000
So that's an example of where Newton alone efficient causes alone won't work. Does that make sense. I mean you have to otherwise it won't kick.

285
00:26:27,000 --> 00:26:42,000
So that's another example is another example of a spatial context dependent constraint, because depending on the length of that plank on the top and the plinth on the bottom the base on the bottom.

286
00:26:42,000 --> 00:26:53,000
Then when where the child sits will be determined by those context dependent constraints in order for them to be able to do your daughter.

287
00:26:53,000 --> 00:26:59,000
Another example that I like a lot which I know you know Dave Snowden and he likes to use a lot.

288
00:26:59,000 --> 00:27:22,000
I think I used it first but that's okay is is the roundabout the the traffic circles. Right. The architecture of the roundabout is a context dependent constraint that affects the behavior of pedestrians and drivers in a system.

289
00:27:22,000 --> 00:27:38,000
So that's another example of a context. I think sequencing is a beautiful example of temporal constraints. A has to be done before B which has to be done before C which has to be done before E and it's done in a different order.

290
00:27:38,000 --> 00:27:42,000
The order of the make a huge difference in the outcome.

291
00:27:42,000 --> 00:28:06,000
So all of these I consider context sensitive constraints. I lump a lot of those into enabling constraints because I think of enabling constraints as particularly those context dependent constraints that together achieve closure such that this coherent whole emerges.

292
00:28:06,000 --> 00:28:20,000
Interesting. And you also I think that was interesting about your your use of the word constraint is you use it very broadly. Yes, you include it. I mean for instance one of my mentors in the complexity space was Harold Morowitz.

293
00:28:20,000 --> 00:28:30,000
Oh, sure. He was here nearby at George Mason for a long time. Yeah, I used to live out in Loudoun County and see him quite once a week.

294
00:28:30,000 --> 00:28:48,000
He has this idea that you know the 27 emergencies or whatever it is and then each one is defined by a set of pruning rules which you know tilt things one way or the other and that those faith those changes are fundamental inflection points in the evolution of the universe and some of them may have been

295
00:28:48,000 --> 00:29:02,000
some of them probably are contingent and some of them aren't and the ones that aren't maybe maybe correspond to your non contextual constraints. For instance, one of his steps is defined by the poly exclusion principle for instance.

296
00:29:02,000 --> 00:29:20,000
Absolutely. I mentioned that in the second book. I think notions of symmetry and conservation prints. You know, it's funny. Physicists all use word causality and so on and use half of the time they're talking about Newtonian cause then when they figure it can't fit and all of a sudden they move to principles.

297
00:29:20,000 --> 00:29:31,000
Right. To to consider that sort of thing and I absolutely agree. I think rules regulations are examples of constraints.

298
00:29:32,000 --> 00:29:40,000
They set the possibility space and they determine what is more likely within it than otherwise.

299
00:29:40,000 --> 00:29:49,000
And I did like the point and this is Harold makes the same point that especially higher up in the stack. We're talking more probabilities than we are blacks and whites.

300
00:29:49,000 --> 00:30:06,000
Absolutely. And perhaps what I speculate on in this new book is that whenever you have the emergence of a coherent dynamic, you have a phase transition to a continuous function.

301
00:30:07,000 --> 00:30:25,000
If you have that if I'm right on that, then it seems to me that top down control is analog. It's a it's a change in the setting of the system to an analog notion so you're going from your old fashioned toggle switch to a dimmer switch.

302
00:30:25,000 --> 00:30:40,000
And that's why homeostasis can keep the timeliness because that's very important for homeostasis in most ecosystems and it also can keep the sensitivity to local conditions.

303
00:30:40,000 --> 00:30:53,000
I think only analog them do that. I the Dyson's have been were poo pooed a lot, but there's something that tells me that their emphasis on analog control somehow might be might be on to something.

304
00:30:53,000 --> 00:30:57,000
I had to make it clear for the audience talk about Freeman and George.

305
00:30:57,000 --> 00:30:59,000
All deceased now. Yeah.

306
00:30:59,000 --> 00:31:03,000
Did George die recently about the last six months or a year.

307
00:31:03,000 --> 00:31:05,000
I used to come by the Santa Fe Institute.

308
00:31:05,000 --> 00:31:22,000
Freeman, I don't believe ever did at least I never met him there, but very interesting family also knew very interesting family also knew Esther Dyson who was George's brother who was a very, very extremely influential thinker in the early days of the computer industry and then later has become a

309
00:31:22,000 --> 00:31:26,000
brilliant venture capitalist, particularly in the Eastern Eastern Europe.

310
00:31:26,000 --> 00:31:27,000
That was her area.

311
00:31:27,000 --> 00:31:28,000
I didn't know.

312
00:31:28,000 --> 00:31:30,000
Yeah, quite amazing woman really.

313
00:31:30,000 --> 00:31:32,000
Yeah, she really was really quite.

314
00:31:32,000 --> 00:31:39,000
Okay, let's I haven't way later in my topic list, but let's talk about it now, which is analog versus digital.

315
00:31:39,000 --> 00:31:41,000
I actually have a fair bit of background in this.

316
00:31:41,000 --> 00:31:43,000
I had two of my companies back in my business career.

317
00:31:43,000 --> 00:31:44,000
They weren't mine.

318
00:31:44,000 --> 00:31:46,000
I was on the I was a chairman of one.

319
00:31:46,000 --> 00:31:51,000
I was a investor and director or another were both involved in software for designing computer chips.

320
00:31:51,000 --> 00:31:54,000
And in particular for designing analog computer chips.

321
00:31:54,000 --> 00:31:58,000
And so I learned quite a bit about analog versus digital.

322
00:31:58,000 --> 00:32:04,000
And for instance, I learned I probably should have known it that digital actually is analog below some level, right?

323
00:32:04,000 --> 00:32:10,000
And then there's there's a whole series of clever things they do to go from analog to digital.

324
00:32:10,000 --> 00:32:21,000
But then the other point, what you allude to is that on comparable computational tasks, analogs, literally six orders of magnitude more efficient.

325
00:32:21,000 --> 00:32:22,000
That's it.

326
00:32:22,000 --> 00:32:26,000
And that's not to be the reason why the brain doesn't overload.

327
00:32:26,000 --> 00:32:27,000
Correct.

328
00:32:27,000 --> 00:32:29,000
That has to be the reason why the brain.

329
00:32:29,000 --> 00:32:31,000
So the brain is a good example of that.

330
00:32:31,000 --> 00:32:38,000
Why the why the mind, why the mind when you transit, when you do the face transition to a mental event.

331
00:32:38,000 --> 00:32:51,000
What you're doing is you're transitioning from neuronal electrical exchanges to control on the basis of some kind of typology, a facial recognition.

332
00:32:51,000 --> 00:32:57,000
So now you're talking on the basis of how close is that pattern to this exemplar face.

333
00:32:57,000 --> 00:33:02,000
Though, again, to keep in mind, the brain is a good example because the brain is both digital and analog.

334
00:33:02,000 --> 00:33:03,000
Exactly.

335
00:33:03,000 --> 00:33:04,000
Exactly.

336
00:33:04,000 --> 00:33:05,000
Exactly.

337
00:33:05,000 --> 00:33:06,000
Correct.

338
00:33:06,000 --> 00:33:07,000
And it switches back and forth.

339
00:33:07,000 --> 00:33:08,000
Yeah.

340
00:33:08,000 --> 00:33:22,000
And so, you know, the domain of ideas or concepts or objects, you know, the, you know, the very important object ontology that at least mammals and above develop probably Burge to is continuously variable.

341
00:33:22,000 --> 00:33:23,000
And it's not perfect.

342
00:33:23,000 --> 00:33:26,000
It's classic analog classification.

343
00:33:26,000 --> 00:33:29,000
And yet it's actually implemented on digital circuitry.

344
00:33:29,000 --> 00:33:30,000
Correct.

345
00:33:30,000 --> 00:33:31,000
Correct.

346
00:33:31,000 --> 00:33:35,000
It's not digital, but the command top down is cut is analog.

347
00:33:35,000 --> 00:33:37,000
That makes perfect sense to me.

348
00:33:37,000 --> 00:33:38,000
Yeah.

349
00:33:38,000 --> 00:33:52,000
Though I would also just add that the continuously variable while true of analog versus digital is maybe less important than people think is one of the principles of computation is you can simulate analog at any level of detail you want.

350
00:33:52,000 --> 00:34:01,000
On the other hand, to simulate analog at extremely fine detail is very expensive computationally while you get it for free with analog.

351
00:34:01,000 --> 00:34:03,000
So that's the, that's the cool thing about analog.

352
00:34:03,000 --> 00:34:14,000
So that George Dyson used to use as an example, you know, even though the internet and all the computers that we use now are digital.

353
00:34:14,000 --> 00:34:20,000
When you look at social media and that stuff, it's what's connected to what.

354
00:34:20,000 --> 00:34:22,000
So we're back to the dance, right?

355
00:34:22,000 --> 00:34:23,000
Yeah.

356
00:34:23,000 --> 00:34:25,000
And that's the people who are analog, right?

357
00:34:25,000 --> 00:34:37,000
And it's the dance of the people and the people in a sense are the condensated nodes of the intersections of all these constraints.

358
00:34:37,000 --> 00:34:38,000
Right.

359
00:34:38,000 --> 00:34:48,000
So that's what I argue in this new book that everybody's all bent out of shape about identity because they all think of identity of something internal essential.

360
00:34:48,000 --> 00:34:58,000
The same thing as coach and and and Chalmers were concerned in, but we need to transition to identity as a set of interdependent constraints.

361
00:34:58,000 --> 00:35:00,000
That's what makes me me.

362
00:35:00,000 --> 00:35:01,000
Right.

363
00:35:01,000 --> 00:35:06,000
I am a set of interdependent constraints and high dimensional.

364
00:35:06,000 --> 00:35:08,000
Very high dimension.

365
00:35:08,000 --> 00:35:17,000
The other thing that's so annoying about the stupid identity politics of both the left and the right is both sides want to condense down to just one or two dimensions.

366
00:35:17,000 --> 00:35:20,000
And there's hundreds of dimensions, right?

367
00:35:20,000 --> 00:35:27,000
You know, I'm a cat, you know, someone's a dog fancier who loves mountain laurel, but not Rotodendrons.

368
00:35:27,000 --> 00:35:29,000
There's so many dimensions.

369
00:35:29,000 --> 00:35:34,000
And I say that these two are the ultimate ones, just like just kind of dumb, right?

370
00:35:34,000 --> 00:35:43,000
You know what did it for me that I finally decided I can finish that first book where was worked by Hinton, Jerry Hinton, who's now a big shot.

371
00:35:43,000 --> 00:35:50,000
Clout and Chalmers and they were doing early work in artificial networks that read words.

372
00:35:50,000 --> 00:35:54,000
It was one of the early text reading networks.

373
00:35:54,000 --> 00:35:59,000
And they were working on simulating.

374
00:35:59,000 --> 00:36:01,000
I had never heard it at the time.

375
00:36:01,000 --> 00:36:06,000
Have you ever heard the difference between surface and deep dyslexia?

376
00:36:06,000 --> 00:36:08,000
No, I was very interesting.

377
00:36:08,000 --> 00:36:10,000
I thought that was in your book.

378
00:36:10,000 --> 00:36:12,000
You gave a great example in your book.

379
00:36:12,000 --> 00:36:13,000
Oh, God.

380
00:36:13,000 --> 00:36:14,000
And I use it in a sense.

381
00:36:14,000 --> 00:36:16,000
No, well, it was their example.

382
00:36:16,000 --> 00:36:17,000
Okay.

383
00:36:17,000 --> 00:36:18,000
It was their example.

384
00:36:18,000 --> 00:36:21,000
They, you know, do you want me to repeat it here?

385
00:36:21,000 --> 00:36:22,000
Sure.

386
00:36:22,000 --> 00:36:23,000
I want you to tell the story.

387
00:36:23,000 --> 00:36:24,000
It's very interesting.

388
00:36:24,000 --> 00:36:25,000
I'll tell the story.

389
00:36:25,000 --> 00:36:28,000
Surface dyslexia is what most dyslexic human beings have.

390
00:36:28,000 --> 00:36:30,000
They transpose letters.

391
00:36:30,000 --> 00:36:35,000
So they read caught for cat or they read tag for cat.

392
00:36:35,000 --> 00:36:40,000
They just transpose letters or four gets read as a seven, that kind of thing.

393
00:36:40,000 --> 00:36:44,000
There was the point being, there's no semantics involved.

394
00:36:44,000 --> 00:36:48,000
It's purely the appearance of the input.

395
00:36:48,000 --> 00:36:50,000
Okay.

396
00:36:50,000 --> 00:36:53,000
Apparently deep dyslexia is very different.

397
00:36:53,000 --> 00:36:56,000
Now what I'll give you the example first.

398
00:36:56,000 --> 00:37:00,000
What Hinton Plout and Chalice, and the reason I plunked them all together is because Hinton

399
00:37:00,000 --> 00:37:03,000
and Plout would write Hinton and Chalice Plout and Chalice.

400
00:37:03,000 --> 00:37:07,000
So there's zillions of papers that came out about 25 years ago about this.

401
00:37:07,000 --> 00:37:14,000
And they trained these early reading neural networks.

402
00:37:14,000 --> 00:37:19,000
And when they had, when they train them with feedback loops, so they were current, recurrent

403
00:37:19,000 --> 00:37:30,000
networks, and they lesioned the networks above the feedback loops.

404
00:37:30,000 --> 00:37:34,000
The network would be shown cat and it would say tack.

405
00:37:34,000 --> 00:37:36,000
So it would do surface dyslexia.

406
00:37:36,000 --> 00:37:40,000
It would produce surface dyslexia kind of errors.

407
00:37:40,000 --> 00:37:48,000
But then if they lesioned the artificial neural network below the feedback loops, they might

408
00:37:48,000 --> 00:37:51,000
show the network again.

409
00:37:51,000 --> 00:37:54,000
These are, these are silicone networks.

410
00:37:54,000 --> 00:37:56,000
They're not organic.

411
00:37:56,000 --> 00:38:01,000
They would show them B, A, N, D band.

412
00:38:01,000 --> 00:38:09,000
And the thing output would say it had read orchestra or they would show it B, E, D.

413
00:38:09,000 --> 00:38:12,000
And the thing would say it had read cut.

414
00:38:12,000 --> 00:38:15,000
That is.

415
00:38:15,000 --> 00:38:21,000
And so somebody, not me, asked one of them, Hinton, Blauser, Chalice, how do you explain this?

416
00:38:21,000 --> 00:38:27,000
And their answer was the only way I can explain this is to postulate that the system, because of the

417
00:38:27,000 --> 00:38:30,000
middle layers that we don't quite know what's going on.

418
00:38:30,000 --> 00:38:33,000
And that has got to be what's going on with Chalice BD, by the way.

419
00:38:33,000 --> 00:38:37,000
The middle layer has created semantic attractors, period.

420
00:38:37,000 --> 00:38:38,000
Case closed.

421
00:38:38,000 --> 00:38:42,000
And the output is just showing from the semantic attractor.

422
00:38:42,000 --> 00:38:44,000
That makes perfect sense.

423
00:38:44,000 --> 00:38:47,000
But there's your emergent property being causally powerful.

424
00:38:47,000 --> 00:38:48,000
Interesting.

425
00:38:48,000 --> 00:38:54,000
By the way, for all my computer people who listen, and there's a lot, the GE Hinton she's talking about is the

426
00:38:54,000 --> 00:38:55,000
Jeffrey Hinton.

427
00:38:55,000 --> 00:38:57,000
He's the Jerry Hinton.

428
00:38:57,000 --> 00:39:00,000
I want, I shouldn't say this on, on the air, but I will.

429
00:39:00,000 --> 00:39:06,000
I said to John Sterle at one of these NEH meetings, John, I think Hinton's work is so very good.

430
00:39:06,000 --> 00:39:08,000
I don't understand why everybody's so interested in Hinton.

431
00:39:08,000 --> 00:39:10,000
I thought, he's very good.

432
00:39:10,000 --> 00:39:12,000
Trust me, he's very good.

433
00:39:12,000 --> 00:39:15,000
It is, it is the Jerry Hinton.

434
00:39:15,000 --> 00:39:16,000
Right.

435
00:39:16,000 --> 00:39:21,000
He's the guy that broke through the breakthroughs that drove all the stuff that we're doing today.

436
00:39:21,000 --> 00:39:22,000
Absolutely.

437
00:39:22,000 --> 00:39:28,000
And everybody was paying attention to these reading networks a long time ago, or at least not as many people should have been.

438
00:39:28,000 --> 00:39:31,000
Yeah, I was actually doing neural nets back in 2001, 2002.

439
00:39:31,000 --> 00:39:34,000
In fact, that's how I got invited out to the Santa Fe Institute.

440
00:39:34,000 --> 00:39:36,000
It was my work on evolutionary neural nets.

441
00:39:36,000 --> 00:39:41,000
Well, my book was published by MIT 1999, and this has been published before.

442
00:39:41,000 --> 00:39:45,000
So I think that stuff was published in 1995, 1998, thereabouts.

443
00:39:45,000 --> 00:39:46,000
Yep, that was.

444
00:39:46,000 --> 00:39:47,000
They did it for me.

445
00:39:47,000 --> 00:40:01,000
I thought, now I can write this book because I have some kind of evidence that the semantic attractor in the brain, in a culture, whatever you want to call it, has causal effects.

446
00:40:01,000 --> 00:40:04,000
Now, when I was reading the book, I write a lot of notes I always do.

447
00:40:04,000 --> 00:40:09,000
One I wanted to ask you about, because how does it fit into your concept of constraints?

448
00:40:09,000 --> 00:40:11,000
It certainly is one of Harold's pruning rules.

449
00:40:11,000 --> 00:40:16,000
And that's the idea of the species competitive exclusion principle.

450
00:40:16,000 --> 00:40:20,000
And which actually, my core field is evolutionary computing.

451
00:40:20,000 --> 00:40:26,000
And so I understand a lot of how speciation works from a mathematical perspective.

452
00:40:26,000 --> 00:40:31,000
And it's more or less a thing that is just true, right?

453
00:40:31,000 --> 00:40:41,000
If a competitive dynamic has these attributes, there will be, and a fitness landscape has a certain shape, there will be competitive exclusion principle around species.

454
00:40:41,000 --> 00:40:45,000
Does that fit into your idea of constraint?

455
00:40:45,000 --> 00:40:58,000
Well, if what I speculate about there existing a constraint regime, correct, then that is the constraint regime for that possibility space, right?

456
00:40:58,000 --> 00:41:06,000
Yeah, it basically says that if anything is competitive, if you get too far away from the center of what's efficient in that part of the fitness landscape,

457
00:41:06,000 --> 00:41:14,000
inevitably you'll be less, not inevitably, most of the time you will be, you know, less fit than the ones right at the near the peak.

458
00:41:14,000 --> 00:41:16,000
And therefore your numbers will go down.

459
00:41:16,000 --> 00:41:21,000
So it's very hard to move away from the peak of the species definition.

460
00:41:21,000 --> 00:41:25,000
The phenotypical collection, of course, as you point out, they're not all the same.

461
00:41:25,000 --> 00:41:30,000
They're an ensemble, but they nonetheless cluster around a species type.

462
00:41:30,000 --> 00:41:31,000
Correct.

463
00:41:31,000 --> 00:41:32,000
Correct.

464
00:41:32,000 --> 00:41:36,000
I don't know if this is an attempt to answer your question or attempt to evade it.

465
00:41:36,000 --> 00:41:38,000
I'm not sure.

466
00:41:38,000 --> 00:41:43,000
Tim Allen, T.H. Allen and Starr who have that book, they've got two editions of it.

467
00:41:43,000 --> 00:41:47,000
I like the first one, the first edition better called Hierarchy Theory.

468
00:41:48,000 --> 00:42:02,000
And they use as an example prairie grassland ecosystems in the Midwest, where apparently the prairie grassland competes against flowering plants.

469
00:42:02,000 --> 00:42:07,000
It also competes of obviously against forbs, horses that eat the grasses.

470
00:42:07,000 --> 00:42:08,000
Correct.

471
00:42:08,000 --> 00:42:16,000
Apparently, if you look at the, if you look at that whole ecosystem, and you look at it from the point of view of the grass.

472
00:42:16,000 --> 00:42:18,000
Right.

473
00:42:18,000 --> 00:42:25,000
Competition with the flowers is a lot harder than competition with the horses.

474
00:42:25,000 --> 00:42:29,000
Apparently, flowers will really do a number of flowering plants.

475
00:42:29,000 --> 00:42:30,000
I don't know.

476
00:42:30,000 --> 00:42:32,000
Dicotillodons or whatever the other things are called.

477
00:42:32,000 --> 00:42:33,000
All right.

478
00:42:33,000 --> 00:42:40,000
So over history, over a period of time, the prairie grasses send out have sent out.

479
00:42:40,000 --> 00:42:42,000
And of course, this is all selection.

480
00:42:42,000 --> 00:42:44,000
I'm not at all disputing, obviously.

481
00:42:44,000 --> 00:42:45,000
They are winning.

482
00:42:45,000 --> 00:42:54,000
They have sent out Mary stems, Mary stems or these shoots right below the surface, but that stick out enough that the horses can eat them.

483
00:42:54,000 --> 00:43:08,000
And so in a sense, Jim, they, the grass invites the horse to come in to feed and incorporates it into what is now an enlarged ecosystem.

484
00:43:08,000 --> 00:43:21,000
So instead of having two competing species, what you having is the enlargement of the niche or the constraint regime.

485
00:43:21,000 --> 00:43:32,000
And Tim Allen says it's kind of like a Shanghai where was a predator and to incorporate it into a an enlarged constraint regime.

486
00:43:32,000 --> 00:43:37,000
Now the, now the horse is part of the grasslands ecosystem.

487
00:43:37,000 --> 00:43:41,000
And that's how they keep the flowers at bay.

488
00:43:41,000 --> 00:43:52,000
I think that way of looking at it as a dynamical, mutual adjustment system is an awful lot better than two species competing.

489
00:43:52,000 --> 00:43:55,000
Yeah, two species are competing, but this other stuff.

490
00:43:55,000 --> 00:44:13,000
And then I get from the fact that I didn't realize until fairly recently that the understanding of the term fit during Darwin's era meant more like you go to a tailor to get a fitting for a new suit of clothes.

491
00:44:13,000 --> 00:44:18,000
So when you think of fit in that sense, it's what is it?

492
00:44:18,000 --> 00:44:21,000
It's a mutual adjustment, correct?

493
00:44:21,000 --> 00:44:33,000
Which means that when I talk about constraint, the form of bottom up and top down relationships has to be one of mutual constraint adjustment.

494
00:44:33,000 --> 00:44:42,000
That's what it basically is. You're adjusting all the constraints to see how you can best satisfy the overarching dynamic.

495
00:44:42,000 --> 00:44:48,000
Yeah, then to your example of the grasses and the horses, etc. It's always in a co-evolutionary context.

496
00:44:48,000 --> 00:44:52,000
Exactly. And that's your idea of context, right?

497
00:44:52,000 --> 00:44:57,000
It's temporal too. It's temporal too. You've got to include temporality. Absolutely.

498
00:44:57,000 --> 00:45:01,000
All right, so let's move on here. That was good, actually.

499
00:45:01,000 --> 00:45:09,000
In the interest of time, I'm going to skip over the COVID example and let's talk more about time temporal constraints.

500
00:45:09,000 --> 00:45:16,000
I think you did a really nice job of talking about cardinality, ordinality, and indexiality.

501
00:45:16,000 --> 00:45:17,000
Inexicality.

502
00:45:17,000 --> 00:45:25,000
Inexicality, okay. I want you to distinguish those, particularly the distinguished cardinality from ordinality.

503
00:45:25,000 --> 00:45:29,000
Well, cardinality is just a mount, correct?

504
00:45:29,000 --> 00:45:31,000
So a pile of sand has a cardinality.

505
00:45:31,000 --> 00:45:35,000
A pile of sand is bigger or smaller, correct.

506
00:45:35,000 --> 00:45:39,000
Ordinality is first, second, third.

507
00:45:39,000 --> 00:45:47,000
I don't see where you can get first, second, thirds much out of Newtonian mechanics.

508
00:45:47,000 --> 00:45:59,000
Whereas once you have temporal constraints instituted, this has to occur and this sets the stage for then the next thing to occur.

509
00:45:59,000 --> 00:46:11,000
Once that sets the stage for the third thing to occur, then you have ordinality, which is first, second, third, orders, right?

510
00:46:11,000 --> 00:46:15,000
Indexicality takes it a bit further.

511
00:46:15,000 --> 00:46:26,000
It's like perspective or the position you are in in a complex dynamical structure means there are certain properties that are indexical.

512
00:46:26,000 --> 00:46:29,000
This is to the left of this. This is to the right of this.

513
00:46:29,000 --> 00:46:32,000
That's what I mean by indexicality.

514
00:46:32,000 --> 00:46:43,000
It really has wreaked havoc in philosophy of mind because intentional causation again is eminently indexical.

515
00:46:43,000 --> 00:46:50,000
So one example I use in this new book is that philosophers use all the time.

516
00:46:50,000 --> 00:47:00,000
Mary told John's wife that he was cheating on her, but Mary doesn't know that John's wife's name is Alice.

517
00:47:00,000 --> 00:47:09,000
So did Mary tell John's wife that it was Alice?

518
00:47:09,000 --> 00:47:16,000
You see what I mean? It has to be interpreted, I think, in terms of the emergent dynamics and emergent properties.

519
00:47:16,000 --> 00:47:22,000
So it has to be treated in terms of indexicals, inside and out.

520
00:47:22,000 --> 00:47:33,000
So I think once you have emergent constraints in place, that's what, and I wish there were a verb that makes...

521
00:47:33,000 --> 00:47:36,000
How could you make a verb out of the word rugged?

522
00:47:36,000 --> 00:47:38,000
Ruggedify.

523
00:47:38,000 --> 00:47:42,000
It ruggedifies the possibility space, right?

524
00:47:42,000 --> 00:47:50,000
And each one of those valleys and attractor basins or attractor separate tricks is right.

525
00:47:50,000 --> 00:47:55,000
They are the ruggedness of a possibility space.

526
00:47:55,000 --> 00:48:07,000
And that explains why the view from inside an attractor looks real different from the view from the hill overlooking the next basin of attractor, correct?

527
00:48:07,000 --> 00:48:14,000
And when we talk about causality, we have to take that kind of indexicality into account.

528
00:48:14,000 --> 00:48:19,000
Yeah, then with respect to ordinality, many things are inherently ordered.

529
00:48:19,000 --> 00:48:27,000
In fact, I just published last night a very interesting podcast, Currents Number 100, with Sarah Walker and Lee Cronin.

530
00:48:27,000 --> 00:48:33,000
Ooh, I've got a note to her. Sarah and Mary Walker. Her stuff's really interesting.

531
00:48:33,000 --> 00:48:36,000
Yeah, and on time, it's an object.

532
00:48:36,000 --> 00:48:47,000
And their hypothesis is that evolution and other expanding complexity is essentially a series of steps that get taken, right?

533
00:48:47,000 --> 00:48:58,000
They point out that the most complex chemicals created by abiotic processes never have more than 13 or 14 steps.

534
00:48:58,000 --> 00:49:06,000
But abiotic processes can go much higher than that, and then man-made processes can go a bit further than that.

535
00:49:06,000 --> 00:49:10,000
And so I thought it was a very interesting juxtaposition with your idea of ordinality.

536
00:49:10,000 --> 00:49:17,000
One of the examples you gave was the social evolution of the processing of cassava.

537
00:49:17,000 --> 00:49:19,000
I believe it was in South America.

538
00:49:19,000 --> 00:49:24,000
That's from Heinrich's book on the secrets to our success or something.

539
00:49:24,000 --> 00:49:29,000
Heinrich is what, head of sociology or something at Harvard.

540
00:49:29,000 --> 00:49:36,000
It's apparently something that's poisonous, but yet nutritious if it weren't poisonous.

541
00:49:36,000 --> 00:49:48,000
The indigenous community in South America has figured out a way of leaching out that poison, but the preparation for that root vegetable has to be done in a particular sequence,

542
00:49:48,000 --> 00:49:52,000
because if you don't, you're going to kill out the entire population.

543
00:49:52,000 --> 00:49:59,000
Going back to Sarah Walker, somebody told me day before yesterday that apparently there's something I'm not on Twitter,

544
00:49:59,000 --> 00:50:02,000
which I probably should be neither on Twitter nor on Facebook.

545
00:50:02,000 --> 00:50:12,000
Somebody told me that somebody wrote a Twitter comment saying that Sarah Walker's driving forces are my constraints,

546
00:50:12,000 --> 00:50:16,000
which is flattering, I think, for me.

547
00:50:17,000 --> 00:50:18,000
She's very good.

548
00:50:18,000 --> 00:50:19,000
She's very good.

549
00:50:19,000 --> 00:50:25,000
And Lee too, the two of them were one of the better science, hard science episodes I've had in a while.

550
00:50:25,000 --> 00:50:28,000
Lee probably is very good. Yeah, I like his stuff.

551
00:50:28,000 --> 00:50:29,000
Interesting.

552
00:50:29,000 --> 00:50:30,000
Okay.

553
00:50:30,000 --> 00:50:36,000
Now you mentioned, why not hit on these, because they're classic rich examples.

554
00:50:36,000 --> 00:50:40,000
Kaufman's Buttons, Huygens Pendulums, and Benard Cells.

555
00:50:40,000 --> 00:50:42,000
Benard Cells.

556
00:50:42,000 --> 00:50:45,000
And paint those in with your ideas around.

557
00:50:45,000 --> 00:50:52,000
Well, the Benard Cells was the source of all my interest in complexity theory.

558
00:50:52,000 --> 00:50:55,000
It was the early 1980s, Jim.

559
00:50:55,000 --> 00:50:57,000
Could you tell us, folks, what it is, not everybody knows.

560
00:50:57,000 --> 00:51:00,000
Okay, a Benard Cells is you take a pan of water.

561
00:51:00,000 --> 00:51:07,000
They're called Ray Lee Benard Cells, and they were discovered at the beginning of the 20th century by Ray Lee and Benard.

562
00:51:07,000 --> 00:51:14,000
Take a pan of water, any kind of viscous fluid, and you heated the uniformity from below.

563
00:51:14,000 --> 00:51:17,000
All right, you still have conduction.

564
00:51:17,000 --> 00:51:26,000
After a certain gradient, after a certain threshold of instability, that's my context independent constraint, Jim.

565
00:51:26,000 --> 00:51:43,000
After you pass a threshold of that gradient, the system cannot handle any kind of fluctuation, and the context will amplify any minor bubble or perturbation.

566
00:51:43,000 --> 00:51:54,000
And all of a sudden, you will get convection cells, those rolling hexagonal cells made of billions of molecules of water that all align in a self-organized way.

567
00:51:54,000 --> 00:52:05,000
And that the cell itself constraints top down the individual molecules of water cell they behave as if they knew what was the one next to them was doing.

568
00:52:05,000 --> 00:52:06,000
All right.

569
00:52:06,000 --> 00:52:18,000
So, it was the 1980s, and I had to go to jury duty here in Montgomery County, and so I took a bag full of stuff that I had to read, and I figured, and I was reading,

570
00:52:18,000 --> 00:52:22,000
Kant's critique of practical reason.

571
00:52:22,000 --> 00:52:33,000
Kant's critique of practical reason says the problem with T, and by the way, at the time that Kant wrote, teleology was synonymous with self-organization.

572
00:52:33,000 --> 00:52:36,000
Go figure, 1804.

573
00:52:36,000 --> 00:52:47,000
Kant said, in order to understand this kind of phenomena, he said, we need an understanding of circular causality that is unknown to us.

574
00:52:47,000 --> 00:52:56,000
Because remember, Kant had bought the Newtonian-Humian collade that was all effective, efficient causality.

575
00:52:56,000 --> 00:53:00,000
So, he said, but look at how nature works.

576
00:53:00,000 --> 00:53:06,000
A tree produces the leaves and then is produced in turn by the leaves.

577
00:53:06,000 --> 00:53:16,000
So, the whole tree is produced by the component parts and in turn loops back down and produces the components that created in the first place.

578
00:53:16,000 --> 00:53:23,000
And so, I'm reading this, going all right, yeah, but how do we fit this into modern science?

579
00:53:23,000 --> 00:53:34,000
And then the Prigogene and Stenger's order out of chaos had just come out of print, come in print, and he's looking at dissipated structures which all have that process.

580
00:53:34,000 --> 00:53:40,000
You get individually constrained interactions that after a threshold of instability,

581
00:53:40,000 --> 00:53:48,000
cross a phase transition and self-organize to produce a whole which then loops back down and constrains the component parts.

582
00:53:48,000 --> 00:53:57,000
So, that to me was, whoa, I found a scientifically respectable way of explaining teleology and formal cause.

583
00:53:57,000 --> 00:54:00,000
That's what did it for me in the 80s.

584
00:54:00,000 --> 00:54:03,000
You were at the right place and right time because Prigogene certainly-

585
00:54:03,000 --> 00:54:08,000
Well, and just seeing again, not having to publish so that you-

586
00:54:08,000 --> 00:54:11,000
Go where you want to go, right?

587
00:54:11,000 --> 00:54:21,000
Decimal point of whatever you already exist allowed me to be a dilettante and play around with ideas just because they were interesting.

588
00:54:21,000 --> 00:54:34,000
And one of the things that Prigogene predicts is that these, especially these abiotic complex systems will actually be more efficient at burning energy than their predecessors.

589
00:54:34,000 --> 00:54:36,000
And that's the Bernard cells for sure.

590
00:54:36,000 --> 00:54:39,000
They actually move more heat through by convection.

591
00:54:39,000 --> 00:54:46,000
And one of my favorites is the whirlpool in the toilet actually allows the water to go down faster.

592
00:54:46,000 --> 00:54:53,000
So it actually is dissipating the potential energy of the water in the tank more quickly than if it didn't form the whirlpool.

593
00:54:53,000 --> 00:54:57,000
You know what I used in class when I taught courses about mines, rains, and machines?

594
00:54:57,000 --> 00:55:01,000
I actually taught a seminar on mines, rains, and machines at PG.

595
00:55:01,000 --> 00:55:05,000
But I take the two, you know, the standard two large gallons.

596
00:55:05,000 --> 00:55:06,000
Oh, yeah.

597
00:55:06,000 --> 00:55:07,000
Put them together.

598
00:55:07,000 --> 00:55:09,000
And then watch the tornadoes.

599
00:55:09,000 --> 00:55:12,000
And the students would, ooh.

600
00:55:12,000 --> 00:55:16,000
Yeah, we made one of those for my daughter when she was like a middle school student.

601
00:55:16,000 --> 00:55:17,000
Yeah, that was a cool thing.

602
00:55:17,000 --> 00:55:18,000
It's pretty cool.

603
00:55:18,000 --> 00:55:20,000
They can really appreciate it intuitively.

604
00:55:20,000 --> 00:55:32,000
Yeah, so the idea of Prigogene and the idea of dissipative systems, even though they have more structure and more interesting things going on, are also, generally speaking, more efficient at burning energy.

605
00:55:32,000 --> 00:55:36,000
And so the good old second law never actually gets violated.

606
00:55:36,000 --> 00:55:38,000
It's just in a different form.

607
00:55:38,000 --> 00:55:40,000
So this, then, you've set me up perfectly.

608
00:55:41,000 --> 00:55:53,000
But thermal equilibrium gets retarded a bit because you have a structure that gets created in the process that persists a bit longer than the component parts.

609
00:55:53,000 --> 00:55:56,000
But that's the explanation for social systems.

610
00:55:56,000 --> 00:56:00,000
That's an explanation for its cities, right?

611
00:56:00,000 --> 00:56:04,000
Cities are more energy efficient than they were organized, right?

612
00:56:04,000 --> 00:56:11,000
Though they burn a lot at per unit square foot, they burn a lot, but per person, they're less, which actually now it sets up to my next time.

613
00:56:11,000 --> 00:56:14,000
You're not going to fool the second law.

614
00:56:14,000 --> 00:56:15,000
No, exactly.

615
00:56:15,000 --> 00:56:16,000
That's the one law.

616
00:56:16,000 --> 00:56:22,000
If anyone ever comes to you and tells you they beat the second law, tell them to go pound sad, right?

617
00:56:22,000 --> 00:56:24,000
And so now we get to where it gets more interesting.

618
00:56:24,000 --> 00:56:31,000
And this, of course, is the secret of life is catalyst loops, autocatalytic networks, et cetera.

619
00:56:31,000 --> 00:56:39,000
This is where we go from whirlpools, which have a self-forming part, but they're not fully closed loops, right?

620
00:56:39,000 --> 00:56:48,000
Well, I really liked a book that came out about seven years ago by two...

621
00:56:48,000 --> 00:56:54,000
Well, a lot of them worked out of the University of the Basque country in Spain.

622
00:56:54,000 --> 00:56:59,000
And the good thing about those folks is they published in English, otherwise they would go into black hole.

623
00:56:59,000 --> 00:57:00,000
But they published in there.

624
00:57:00,000 --> 00:57:04,000
Unfortunately, this book is Springer, and Springer's so damn expensive, nobody buys the book.

625
00:57:04,000 --> 00:57:05,000
But I think they're very good.

626
00:57:05,000 --> 00:57:08,000
And the book is called Biological Autonomy.

627
00:57:08,000 --> 00:57:18,000
And their argument is that, well, things like the Krebs cycle and so on, these are closures of process.

628
00:57:18,000 --> 00:57:27,000
But once you start having autocatalytic and hypercycles, a la eigen and so on, what do you have that...

629
00:57:27,000 --> 00:57:35,000
The constraint loop that closes is a loop of constraints themselves.

630
00:57:35,000 --> 00:57:42,000
And so the constraints create the constraining conditions that make them possible to begin with.

631
00:57:42,000 --> 00:57:50,000
And that is what enables their self-reinforcing, but their self-perpetuating.

632
00:57:50,000 --> 00:58:04,000
And so these Matteo Moreno and Mocio argue that is what makes living things different from, say, even the BZ reaction,

633
00:58:04,000 --> 00:58:11,000
where the boundary conditions, the constraints are, in a sense, self-set from without.

634
00:58:11,000 --> 00:58:17,000
You set the conditions of the pan of water or the chemicals.

635
00:58:17,000 --> 00:58:25,000
But once you get a situation where the constraints themselves become self-perpetuating,

636
00:58:25,000 --> 00:58:30,000
then you have the possibility of reproduction of species and that sort of thing.

637
00:58:30,000 --> 00:58:33,000
Now, you didn't really hit on it as hard as I thought you might.

638
00:58:33,000 --> 00:58:41,000
But the perfect example of that is that the autocatalytic reactions within a cell are also responsible

639
00:58:41,000 --> 00:58:45,000
for building and maintaining the membrane that allows the concentration.

640
00:58:45,000 --> 00:58:47,000
And that's hugely important to my mind.

641
00:58:47,000 --> 00:58:49,000
Absolutely, absolutely.

642
00:58:49,000 --> 00:58:55,000
I think that's what they mean because, from memory, we've always thought of it as boundary conditions.

643
00:58:55,000 --> 00:59:04,000
And that was my beef with Polani, because Polani says, Polani was ultimately religious.

644
00:59:04,000 --> 00:59:09,000
And so Michael Polani, the philosopher at the beginning of the 20th century, I guess it was,

645
00:59:09,000 --> 00:59:14,000
he believed that God sets the original boundary conditions.

646
00:59:14,000 --> 00:59:18,000
And then once you've got that, then everything else self-organizes within it.

647
00:59:18,000 --> 00:59:27,000
But the whole point of, I think, the closure of constraints is that it creates the boundary conditions

648
00:59:27,000 --> 00:59:30,000
within which it self-organizes as well.

649
00:59:30,000 --> 00:59:32,000
So yes, you're absolutely right.

650
00:59:32,000 --> 00:59:34,000
Yeah, I think that's hugely important.

651
00:59:34,000 --> 00:59:39,000
And of course, as we've learned, the nature of these membranes changed over time,

652
00:59:39,000 --> 00:59:41,000
and it's continually changing.

653
00:59:41,000 --> 00:59:46,000
And the fact that they're semi-permeable with different rules for what goes from the inside out

654
00:59:46,000 --> 00:59:54,000
and what comes from the outside in are hugely important to maintain the reactions that are going inside.

655
00:59:54,000 --> 01:00:00,000
I think we finally understood that because of the role of interfaces in computers.

656
01:00:00,000 --> 01:00:02,000
Yeah, I think that helps.

657
01:00:03,000 --> 01:00:11,000
And what, therefore, what that also means, I think, is once you have a phase transition to a new dynamic,

658
01:00:11,000 --> 01:00:14,000
you have a new code.

659
01:00:14,000 --> 01:00:20,000
And a new code means simply the settings and the rules that govern that membrane,

660
01:00:20,000 --> 01:00:28,000
that boundary conditions, what it allows in and what it produces as waste and as action.

661
01:00:28,000 --> 01:00:29,000
Absolutely.

662
01:00:29,000 --> 01:00:33,000
And then the other interesting example you gave, which I had thought of before,

663
01:00:33,000 --> 01:00:37,000
is you talked about the architecture of the circulatory system as another...

664
01:00:37,000 --> 01:00:40,000
That's more anointing, which is a really nice example,

665
01:00:40,000 --> 01:00:48,000
because the vasculature of the body, the lymph node and the blood circulating system,

666
01:00:48,000 --> 01:00:54,000
really prevents the seeping out, right?

667
01:00:54,000 --> 01:00:56,000
But it is not an energetic force.

668
01:00:56,000 --> 01:00:58,000
That's what the heart does.

669
01:00:59,000 --> 01:01:11,000
But the vasculature is more like the timing of the playground swing in that it controls the settings.

670
01:01:11,000 --> 01:01:12,000
You know what?

671
01:01:12,000 --> 01:01:17,000
After I submitted this MIT, which was, geez, it was almost a year and a half, two years ago,

672
01:01:17,000 --> 01:01:19,000
but it took so long.

673
01:01:19,000 --> 01:01:28,000
I am fascinated recently by the inflammatory system and the immune system connection,

674
01:01:28,000 --> 01:01:32,000
because they are now talking about three levels.

675
01:01:32,000 --> 01:01:38,000
They're talking about structure, function, and then regulation.

676
01:01:38,000 --> 01:01:44,000
And that, you know, if my arm gets cut off,

677
01:01:44,000 --> 01:01:49,000
then the inflammation hits it immediately to try to repair the wound.

678
01:01:49,000 --> 01:01:51,000
All right?

679
01:01:51,000 --> 01:01:55,000
If all of a sudden the interactions in the body are out of kilter,

680
01:01:55,000 --> 01:01:58,000
then the function of homeostasis may not work as well.

681
01:01:58,000 --> 01:02:01,000
That you get diabetes and so on and so forth.

682
01:02:01,000 --> 01:02:07,000
But the idea recently is that perhaps there is a third level,

683
01:02:07,000 --> 01:02:11,000
which is the setting of the functional system.

684
01:02:11,000 --> 01:02:15,000
I'll call it the set points or the settings,

685
01:02:15,000 --> 01:02:22,000
and that perhaps things like PTSD, chronic inflammatory disease syndrome,

686
01:02:22,000 --> 01:02:29,000
that kind of thing is that on and off switch, for example,

687
01:02:29,000 --> 01:02:33,000
the dimmer switch, the analog is screwy.

688
01:02:33,000 --> 01:02:41,000
So it's the set point, it's the regulatory control of that function.

689
01:02:41,000 --> 01:02:45,000
That's often that maybe that's the way to attack PTSD, for example.

690
01:02:45,000 --> 01:02:48,000
There's nothing wrong with the function of PTSD.

691
01:02:48,000 --> 01:02:51,000
We're supposed to freak out if we think someone's attacking us at night.

692
01:02:51,000 --> 01:02:57,000
What's wrong is the fact that we are now reacting in a different context

693
01:02:57,000 --> 01:02:59,000
the way it should have been before.

694
01:02:59,000 --> 01:03:02,000
Then that means there's something wrong with the toggle switch.

695
01:03:02,000 --> 01:03:07,000
There's nothing wrong with the switch, which I find really interesting,

696
01:03:07,000 --> 01:03:14,000
because I think that has a lot to say with two for social systems.

697
01:03:14,000 --> 01:03:20,000
It has a lot to say, not just for the inflammatory system.

698
01:03:20,000 --> 01:03:24,000
Perhaps all of complex dynamical systems have those three layers,

699
01:03:24,000 --> 01:03:28,000
structure, function, and then regulation.

700
01:03:28,000 --> 01:03:32,000
And then, of course, the question is, how do these three levels interact?

701
01:03:32,000 --> 01:03:37,000
And they do not interact by efficient causes, they interact by constraints.

702
01:03:37,000 --> 01:03:45,000
That everybody who's done hierarchy theory in biology is comfortable with that.

703
01:03:45,000 --> 01:03:51,000
Okay, let's move on to another topic here, which is that you described them as constraints,

704
01:03:51,000 --> 01:03:56,000
though I would not normally think of them that way, but I think in your lens they work.

705
01:03:56,000 --> 01:04:01,000
And that's the idea of scaffolds and scaffolding of affordances, etc.

706
01:04:01,000 --> 01:04:07,000
Talk about your thoughts on scaffolding and how you can use the language of constraints around that.

707
01:04:07,000 --> 01:04:17,000
I think you have the old-fashioned architectural scaffolds that are external artifacts

708
01:04:17,000 --> 01:04:23,000
that are temporary and that guide the construction of new buildings, correct?

709
01:04:23,000 --> 01:04:31,000
But if you think of constraints, of context-sensitive constraints as conditions and factors

710
01:04:31,000 --> 01:04:37,000
that take a system away from independence, they link things together.

711
01:04:37,000 --> 01:04:48,000
What scaffolds do, and I love the work of Bishop and especially the word of WimSat,

712
01:04:49,000 --> 01:04:54,000
they've done work on scaffolding long before they got fashionable recently.

713
01:04:54,000 --> 01:05:03,000
They provide a temporary equilibrium point from which to take the next step,

714
01:05:03,000 --> 01:05:07,000
and it's not only, it's like a ratchet.

715
01:05:07,000 --> 01:05:10,000
I think of scaffolds almost like ratchets.

716
01:05:10,000 --> 01:05:20,000
They provide a temporary, metastable position from which the next step,

717
01:05:20,000 --> 01:05:28,000
whose direction the scaffold itself also suggests, can be more easily taken.

718
01:05:28,000 --> 01:05:32,000
But that chapter you're thinking about from the new book,

719
01:05:32,000 --> 01:05:37,000
there's so many other different types of constraints, of that kind of constraints.

720
01:05:37,000 --> 01:05:39,000
There's entrenchment.

721
01:05:39,000 --> 01:05:46,000
That's a hell of a constraint that we use a lot, especially in social systems, correct?

722
01:05:46,000 --> 01:05:52,000
To retard any innovation or buffers.

723
01:05:52,000 --> 01:05:59,000
I think probably the difference between a buffer and an entrenchment might be how long it lasts, right?

724
01:05:59,000 --> 01:06:07,000
But it's a way to control the relationships between the inside and the outside,

725
01:06:07,000 --> 01:06:12,000
and the next step, that's why I think of it as a form of constraint.

726
01:06:12,000 --> 01:06:18,000
Because the scaffold, again, is not, my bent noir is always efficient cause.

727
01:06:18,000 --> 01:06:23,000
I'm always thinking, well, this is something that has effects, but it's not as an efficient cause.

728
01:06:23,000 --> 01:06:27,000
So I have buffers, I have entrenchment, I have scaffolding.

729
01:06:27,000 --> 01:06:32,000
I have that kind of process that we use a lot.

730
01:06:32,000 --> 01:06:35,000
And now you could throw those in with catalysts.

731
01:06:35,000 --> 01:06:36,000
Absolutely.

732
01:06:36,000 --> 01:06:39,000
And certainly, let's say scaffolds and catalysts both have the effect,

733
01:06:39,000 --> 01:06:42,000
while they don't necessarily provide energy themselves,

734
01:06:42,000 --> 01:06:46,000
they lower the activation energy for something to occur.

735
01:06:46,000 --> 01:06:47,000
Correct, correct.

736
01:06:47,000 --> 01:06:54,000
And even those scaffolds that are not temporary like the flying buttresses of Gothic cathedrals

737
01:06:54,000 --> 01:06:56,000
that end up being part of the structure,

738
01:06:56,000 --> 01:06:59,000
and that is also true of these lattices.

739
01:06:59,000 --> 01:07:09,000
They implant that are embedded with nutrients that promote bone growth, right?

740
01:07:09,000 --> 01:07:22,000
The point being that the location and the direction of the holes in that lattice are what pattern the bone growth.

741
01:07:22,000 --> 01:07:28,000
But then they end up getting absorbed and becoming part of the bone itself.

742
01:07:28,000 --> 01:07:38,000
So these are all forms of affecting consequences that are not, or that are in addition to.

743
01:07:38,000 --> 01:07:42,000
I don't want to discount efficient causes, obviously.

744
01:07:42,000 --> 01:07:47,000
I just don't believe they're the full story.

745
01:07:47,000 --> 01:07:52,000
And this, in some sense, the basic laws of physics continue to be true,

746
01:07:52,000 --> 01:07:57,000
but there's much more interesting structure being built that's in addition.

747
01:07:57,000 --> 01:08:01,000
And that's what the naive reductionism misses.

748
01:08:01,000 --> 01:08:02,000
Correct.

749
01:08:02,000 --> 01:08:03,000
And now, go ahead.

750
01:08:03,000 --> 01:08:04,000
I'm sorry.

751
01:08:04,000 --> 01:08:05,000
I interrupt.

752
01:08:05,000 --> 01:08:06,000
I'm Cuban.

753
01:08:06,000 --> 01:08:07,000
I'm sorry.

754
01:08:07,000 --> 01:08:08,000
You talk with your hands.

755
01:08:08,000 --> 01:08:09,000
That's right.

756
01:08:09,000 --> 01:08:10,000
That's right.

757
01:08:10,000 --> 01:08:13,000
And that's, again, I think that it's kind of this myopia of over-reductionism.

758
01:08:13,000 --> 01:08:14,000
Nothing wrong with reductionism.

759
01:08:14,000 --> 01:08:16,000
No, no, there's nothing wrong with that.

760
01:08:16,000 --> 01:08:18,000
You need to know both the dance and the dancer.

761
01:08:18,000 --> 01:08:23,000
But there's a sense that somehow the relational and the dynamical are not real.

762
01:08:23,000 --> 01:08:28,000
Every bit is real as the primary properties.

763
01:08:28,000 --> 01:08:33,000
And that's my problem with myriology and nothing but is so on.

764
01:08:33,000 --> 01:08:38,000
The problem that everybody complained about top-down causation is not possible

765
01:08:38,000 --> 01:08:45,000
is because it would violate physical closure and it would violate the conservation of energy.

766
01:08:45,000 --> 01:08:49,000
Sure, if you think of it as efficient causes, of course, it's going to do those two things.

767
01:08:49,000 --> 01:08:56,000
But if it operates as constraining dynamics, you're not violating physical closure

768
01:08:56,000 --> 01:09:00,000
or constraint or conservation.

769
01:09:00,000 --> 01:09:03,000
So that's why it works very nicely.

770
01:09:03,000 --> 01:09:08,000
It's not violating any basic physics tenets.

771
01:09:08,000 --> 01:09:10,000
Why don't you do a little riff on that?

772
01:09:10,000 --> 01:09:14,000
Because that is one of the questions in complexity.

773
01:09:14,000 --> 01:09:16,000
And it befuddles the laymen in particular.

774
01:09:16,000 --> 01:09:17,000
What?

775
01:09:17,000 --> 01:09:19,000
Top-down causality.

776
01:09:19,000 --> 01:09:24,000
And how you can have top-down causality and no magic needs occur.

777
01:09:24,000 --> 01:09:29,000
Well, in the very same way that homoestasis changes my glucose production

778
01:09:29,000 --> 01:09:32,000
or how does a culture affect me?

779
01:09:32,000 --> 01:09:34,000
That is top-down causality.

780
01:09:34,000 --> 01:09:37,000
That is causality from the hole in which I am embedded.

781
01:09:37,000 --> 01:09:41,000
If I were not part of that culture, it would not affect me.

782
01:09:41,000 --> 01:09:42,000
Correct?

783
01:09:42,000 --> 01:09:43,000
Right.

784
01:09:43,000 --> 01:09:49,000
And that means that the constraint dynamic of the culture in which I am embedded

785
01:09:49,000 --> 01:09:55,000
be the college at which I taught, the society in which I live, the family to which I belong,

786
01:09:55,000 --> 01:09:59,000
the constraint structure of each of those organizations

787
01:09:59,000 --> 01:10:08,000
changes the likelihood of different behaviors that might otherwise have been open to me that are not.

788
01:10:08,000 --> 01:10:15,000
In the very same way that once entrained into a Benard cell, the molecule of water

789
01:10:15,000 --> 01:10:21,000
has different probabilities of where it is going to go

790
01:10:21,000 --> 01:10:25,000
because the constraint structure of the Benard cell affects it.

791
01:10:25,000 --> 01:10:27,000
And that is what I mean by top-down causation.

792
01:10:27,000 --> 01:10:31,000
So there is nothing magical about it, but it is not efficient causality.

793
01:10:31,000 --> 01:10:34,000
If you think of it as efficient causality, then of course it is magical.

794
01:10:34,000 --> 01:10:35,000
Gotcha.

795
01:10:35,000 --> 01:10:40,000
And there has been many, many pointless conversation on this as you no doubt have experienced it.

796
01:10:40,000 --> 01:10:41,000
Absolutely.

797
01:10:41,000 --> 01:10:44,000
Well, my centuries of this going on.

798
01:10:44,000 --> 01:10:45,000
Indeed.

799
01:10:45,000 --> 01:10:46,000
Let's move on.

800
01:10:46,000 --> 01:10:47,000
We are getting kind of late on time here.

801
01:10:47,000 --> 01:10:49,000
We have got about another 13 minutes.

802
01:10:49,000 --> 01:10:51,000
And this was something new to me.

803
01:10:51,000 --> 01:10:54,000
Very interesting to always run across something new.

804
01:10:54,000 --> 01:10:57,000
And that is the idea of many to one transitions.

805
01:10:57,000 --> 01:10:59,000
Maybe you can dig into this in some depth.

806
01:10:59,000 --> 01:11:00,000
Okay.

807
01:11:00,000 --> 01:11:07,000
And that was, all right, when at the beginning, once behaviorism got put to bed,

808
01:11:07,000 --> 01:11:11,000
then functionalism came into play or the identity theory.

809
01:11:11,000 --> 01:11:17,000
So the idea was that mind is to brain as a computer software is to its hardware.

810
01:11:17,000 --> 01:11:22,000
So then people said, all right, just like a lot of different,

811
01:11:22,000 --> 01:11:29,000
Microsoft Office can be run on different, on Apple and the different hardware devices,

812
01:11:29,000 --> 01:11:34,000
that's the explanation that gives some legitimacy to the notion of a mind.

813
01:11:34,000 --> 01:11:36,000
These are the notion of the brain.

814
01:11:36,000 --> 01:11:39,000
The brain is the hardware, the mind is the software fight.

815
01:11:39,000 --> 01:11:46,000
But there was also always the idea that the notion of supervenience and Donald Davidson's

816
01:11:46,000 --> 01:11:54,000
term is there will be no change in the supervenient properties without any corresponding changes

817
01:11:54,000 --> 01:11:58,000
in the subvening in the hardware.

818
01:11:58,000 --> 01:12:04,000
So there was always implicit in the notion of supervenience, a one to one relationship.

819
01:12:04,000 --> 01:12:08,000
And the reason I think that was true was because I don't think they ever got away from the

820
01:12:08,000 --> 01:12:09,000
physicals out.

821
01:12:09,000 --> 01:12:11,000
So there was always that one to one relationship.

822
01:12:11,000 --> 01:12:18,000
So the idea was, all right, so a mental event, my thinking of my grandmother,

823
01:12:18,000 --> 01:12:25,000
my pain in my leg will always be correlated one to one with this particular neuro pattern

824
01:12:25,000 --> 01:12:28,000
in the brain, which indicates pain in my leg.

825
01:12:28,000 --> 01:12:32,000
This one about indicates that's my, my grandmother.

826
01:12:32,000 --> 01:12:40,000
But very quickly, multiple realizability, many to one relationships, all sudden came about.

827
01:12:40,000 --> 01:12:46,000
And that was that, well, though, and behold, if this part of the brain is excised,

828
01:12:46,000 --> 01:12:52,000
the hearing function might be taken over by the other part of the brain.

829
01:12:52,000 --> 01:12:57,000
Another part of the brain that was supposedly not at all dedicated to hearing, but all of

830
01:12:57,000 --> 01:12:58,000
a sudden it was.

831
01:12:58,000 --> 01:13:05,000
So the notion, and I almost wanted to name this title, this new book, Imprasive Degeneracy.

832
01:13:05,000 --> 01:13:11,000
Don't get so, don't get cutesy here, because biologists have always been very comfortable

833
01:13:11,000 --> 01:13:12,000
with degeneracy.

834
01:13:12,000 --> 01:13:19,000
That is, there are many ways for the same amino acids to produce different amino acids to produce

835
01:13:19,000 --> 01:13:20,000
the same protein.

836
01:13:21,000 --> 01:13:31,000
That's what I mean by many to one, many different lower level paths to realize the same emergent

837
01:13:31,000 --> 01:13:32,000
property.

838
01:13:34,000 --> 01:13:37,000
That's what I mean by many to one.

839
01:13:37,000 --> 01:13:45,000
And that seems to be true in general for the higher level properties of complex dynamical

840
01:13:45,000 --> 01:13:46,000
systems.

841
01:13:46,000 --> 01:13:57,000
An economy can stay itself despite many different varieties or many different configurations,

842
01:13:57,000 --> 01:14:04,000
as long as that overarching constraint structure remains within a certain range.

843
01:14:04,000 --> 01:14:07,000
Does that, have I made sense here?

844
01:14:07,000 --> 01:14:08,000
Yes, ish.

845
01:14:08,000 --> 01:14:10,000
Let me drill it a little further.

846
01:14:10,000 --> 01:14:11,000
All right.

847
01:14:11,000 --> 01:14:16,000
You also talk about many to one, because, you know, we do have multiple realizable domains,

848
01:14:16,000 --> 01:14:18,000
and the idea of degeneracy is very important.

849
01:14:18,000 --> 01:14:24,000
And for the audience, degeneracy basically means that things with different forms can

850
01:14:24,000 --> 01:14:26,000
have the same function, right?

851
01:14:26,000 --> 01:14:27,000
Correct.

852
01:14:27,000 --> 01:14:29,000
Different lower levels, same higher levels.

853
01:14:29,000 --> 01:14:30,000
Same higher level.

854
01:14:30,000 --> 01:14:34,000
And there's a bunch of famous examples in biochemistry where it's more famous, but you also talked

855
01:14:34,000 --> 01:14:40,000
about many to one as something analogous to dimensional reduction in systems.

856
01:14:40,000 --> 01:14:44,000
And think about the fact that, you know, with the example I gave before, Jim gets up out

857
01:14:44,000 --> 01:14:48,000
of his chair and goes down the hill to the ice cream store.

858
01:14:48,000 --> 01:14:53,000
There's lots and lots and lots of predicate signals that probably led to that, right?

859
01:14:53,000 --> 01:14:58,000
And they eventually got concentrated down to a single decision.

860
01:14:58,000 --> 01:15:03,000
Should I get up, go down the hill, get an ice cream cone, or should I go to the refrigerator,

861
01:15:03,000 --> 01:15:07,000
freezer and pull out a frozen yogurt bar, right?

862
01:15:07,000 --> 01:15:12,000
And the fact that there's many, many, many inputs, but there's a single decision around

863
01:15:12,000 --> 01:15:16,000
the affordance was also the way you described many to one.

864
01:15:16,000 --> 01:15:25,000
The reverse of it is pluripotentiality, which is one lower level that has the potential

865
01:15:25,000 --> 01:15:29,000
for becoming much more different functionality.

866
01:15:29,000 --> 01:15:32,000
And of course, stem cells are the example of that, right?

867
01:15:32,000 --> 01:15:37,000
They are pluripotents.

868
01:15:37,000 --> 01:15:42,000
They are not totipotents, which people thought might, they might be for a while,

869
01:15:42,000 --> 01:15:44,000
that they could become any other.

870
01:15:44,000 --> 01:15:48,000
They're not quite their totipotentiality.

871
01:15:48,000 --> 01:15:51,000
But that was the problem with supervenience.

872
01:15:51,000 --> 01:15:59,000
Because then the question was, how can the same neural processes in the brain produce

873
01:15:59,000 --> 01:16:08,000
such very different, differently property functions, higher level properties?

874
01:16:08,000 --> 01:16:15,000
And Davidson said, if you can't identify a causal relationship between the two,

875
01:16:15,000 --> 01:16:18,000
then you might as well go back to behaviorism.

876
01:16:18,000 --> 01:16:28,000
And at that point, J-Wan Kim at Brown, who had been a big advocate of supervenience at the time,

877
01:16:28,000 --> 01:16:30,000
decided supervenience won't work.

878
01:16:30,000 --> 01:16:34,000
It either is one to one or it's not.

879
01:16:34,000 --> 01:16:36,000
And that's why he titled that.

880
01:16:36,000 --> 01:16:39,000
Was it a paper or was it a book called Descartes' Revenge?

881
01:16:39,000 --> 01:16:48,000
Because then again, the problem was, how do you get top down causality in that once you decide to go to the fridge, Jim,

882
01:16:48,000 --> 01:16:51,000
you could decide to walk directly to the fridge.

883
01:16:51,000 --> 01:16:57,000
You could decide to go outside because somebody is going to watch you take something from the fridge

884
01:16:57,000 --> 01:16:59,000
and you go outside, you go all the way around.

885
01:16:59,000 --> 01:17:05,000
So there are a lot of different ways to implement that top down decision, correct?

886
01:17:05,000 --> 01:17:16,000
And the problem with again, efficient causality is that it only worked for instantaneous relationships.

887
01:17:16,000 --> 01:17:21,000
Efficient causality cannot handle what used to be called in philosophy standing causes.

888
01:17:21,000 --> 01:17:26,000
In other words, I decided I'm going to write a book and it took me two damn years to do so.

889
01:17:26,000 --> 01:17:38,000
So how on earth does that intention continue in force and continue exerting an influence all throughout those two years?

890
01:17:38,000 --> 01:17:45,000
That's what I meant in that sense by multiple realizability top down.

891
01:17:45,000 --> 01:17:52,000
Yeah, and then of course that goes back to the very classic Greek philosophical question of the ship of Theseus, right?

892
01:17:52,000 --> 01:17:58,000
Our audience goes, okay, the supposed ship that Theseus took to Crete, I guess it was,

893
01:17:58,000 --> 01:18:04,000
then it was preserved in Athenian harbor and then over the years, the boards rotted and they replaced one by one.

894
01:18:04,000 --> 01:18:07,000
Eventually every plank on the ship had been replaced.

895
01:18:07,000 --> 01:18:11,000
There was nothing original, but it was the same ship or was it, right?

896
01:18:11,000 --> 01:18:12,000
Correct.

897
01:18:12,000 --> 01:18:18,000
And we can say the same thing about organizations and cultures and societies and, you know.

898
01:18:18,000 --> 01:18:23,000
And humans, you know, basically every one of our atoms gets replaced, what, every year or something like that?

899
01:18:23,000 --> 01:18:26,000
Every seven years, every cells or whatever.

900
01:18:26,000 --> 01:18:27,000
Exactly.

901
01:18:27,000 --> 01:18:37,000
So I think I stick my neck out more than maybe I have a right to, but that's why I wanted to show in this new book that people say,

902
01:18:37,000 --> 01:18:39,000
you're reducing everything to physics and chemistry.

903
01:18:39,000 --> 01:18:52,000
No, I'm showing that there are, I never know the difference, homologous analogous constraint dynamics that operate all along the spiral.

904
01:18:52,000 --> 01:18:56,000
It's not a, it's not a reduction.

905
01:18:56,000 --> 01:19:07,000
It is to show that once you do the phase transition from physics to chemistry, then you've got new emergent properties, new codes, new everything.

906
01:19:07,000 --> 01:19:12,000
Once you go there from, from chemistry to biology and so on down the line.

907
01:19:12,000 --> 01:19:13,000
Got it.

908
01:19:13,000 --> 01:19:17,000
Well, let's wrap up with the future of these fields.

909
01:19:17,000 --> 01:19:29,000
And you talk a fair bit about the relatively new 4e approach to cognitive science, which I think is maybe getting closer to your way of thinking.

910
01:19:29,000 --> 01:19:30,000
Yes.

911
01:19:30,000 --> 01:19:37,000
I mean, I really like the work of the Paolo probably Andy Clark started the whole thing with the embodied mind idea.

912
01:19:37,000 --> 01:19:46,000
And people like Merlin Donald, who I love his book, all of these folks started realizing the mind just ain't in the brain.

913
01:19:47,000 --> 01:19:55,000
Because it takes me forever to write because I literally realize that I work out the ideas as I write.

914
01:19:55,000 --> 01:20:03,000
Now I type, but it's not that I sit down thinking all through and then I just write it out, you know, because it's all been worked out in my mind.

915
01:20:03,000 --> 01:20:15,000
Literally. So, so the notion that that that that our minds extend beyond the boundaries of our body to artifacts to tools.

916
01:20:15,000 --> 01:20:24,000
For example, when you're driving, I'm the designated teach your grandchild to parallel park person because their parents are petrified.

917
01:20:24,000 --> 01:20:26,000
Nobody else wants to do it.

918
01:20:26,000 --> 01:20:28,000
I'm a good parallel parker.

919
01:20:28,000 --> 01:20:39,000
You know, I say, look, after a while you realize that that you can almost feel the car as the extension, you know exactly where it's going to fit in a in a tight space.

920
01:20:39,000 --> 01:20:43,000
So the idea that the mind ain't just in the brain, it's embodied.

921
01:20:43,000 --> 01:20:58,000
But then it's also enacted because it's not just that it's embodied in my in the agent's body, but it's also the mind is enacted in their behavior within a particular context and so on.

922
01:20:59,000 --> 01:21:17,000
What I what I try to do in this new book is then the question is, well, how does this holistic ecosystem within which this embodiment and enlightenment, how does that coherent dynamic come about.

923
01:21:17,000 --> 01:21:23,000
And that's why I want to show that before you have a an enact.

924
01:21:23,000 --> 01:21:31,000
I don't think that if you're not Japanese, a tatami mat affords sleeping.

925
01:21:31,000 --> 01:21:39,000
I don't think a Victorian throne and that's seating to somebody who's never seen it before.

926
01:21:39,000 --> 01:21:44,000
So to somebody who's used to sleeping on the ground, and I don't think it does it automatically.

927
01:21:44,000 --> 01:21:48,000
It is part of a whole ecosystem, a whole coherent dynamic.

928
01:21:48,000 --> 01:21:52,000
And my question is, how do those coherent dynamics come about.

929
01:21:52,000 --> 01:22:00,000
And again, my answer seems to be I don't know what else to call constraints, all of these processes.

930
01:22:00,000 --> 01:22:04,000
And accumulation of constraints over time and their channel.

931
01:22:04,000 --> 01:22:08,000
But it's not just accumulation in that one at one damn thing after another.

932
01:22:08,000 --> 01:22:14,000
It's how they into how they interweave with one another.

933
01:22:14,000 --> 01:22:19,000
That's what creates this overarching interlocking set of constraints.

934
01:22:19,000 --> 01:22:23,000
Yeah, and then and that that and those constraints are real.

935
01:22:24,000 --> 01:22:29,000
See, I say from the beginning in the book, I'm never since can't ever.

936
01:22:29,000 --> 01:22:33,000
Every time something gets bored, we all we all hide behind epistemology.

937
01:22:33,000 --> 01:22:36,000
Oh, well, it's the way we make sense of things.

938
01:22:36,000 --> 01:22:39,000
No dammit, I think that's the way reality works.

939
01:22:39,000 --> 01:22:42,000
And you know, I don't need to I have to I have tenure.

940
01:22:42,000 --> 01:22:45,000
I don't need to worry about pleasing somebody else.

941
01:22:45,000 --> 01:22:48,000
I can say this if it's wrong, it's wrong, but that's okay.

942
01:22:48,000 --> 01:22:49,000
Oh, right.

943
01:22:49,000 --> 01:22:55,000
Well, I really want to thank Alicia Herrero and her new book, Context Changes Everything,

944
01:22:55,000 --> 01:22:57,000
as you can tell by our conversation.

945
01:22:57,000 --> 01:23:00,000
I learned some things, had some new ideas I've never been exposed to.

946
01:23:00,000 --> 01:23:02,000
I thought it was really interesting.

947
01:23:02,000 --> 01:23:04,000
And despite what she said, I thought it was damn well written.

948
01:23:04,000 --> 01:23:06,000
So thank you, Alicia.

949
01:23:06,000 --> 01:23:08,000
Thank you so much.

950
01:23:12,000 --> 01:23:15,000
Audio production and editing by Andrew Blevins Productions.

951
01:23:15,000 --> 01:23:20,000
Music by Tom Mueller at modernspacemusic.com

