Processing Overview for Brandon Rohrer
============================
Checking Brandon Rohrer/How Deep Neural Networks Work.txt
1. **Back Propagation**: The core algorithm of neural networks, which involves computing the gradient of the loss function with respect to each weight in the network. This is done by applying the chain rule to propagate the error backward through the network, from the output layer back to the input layer.

2. **Weight Updating**: Once the gradients are computed, weights are updated in the direction that reduces the loss. Typically, this is done using a small constant called the learning rate.

3. **Sums and Averages**: In neural networks, especially in the fully connected layers, inputs are often summed or averaged. The derivative of the sum with respect to any input is 1, meaning each input contributes equally to the change in the output.

4. **Sigmoid Function**: This is an activation function with a graph that looks like an "S" and outputs values between 0 and 1. Its derivative is the function itself multiplied by its complement (1 - sigma(x)).

5. **ReLU Function**: A simpler activation function that outputs the input directly if it's positive, otherwise it outputs zero. Its derivative is either 1 if the input is positive or 0 if the input is negative/zero.

6. **Bias Neurons**: An additional parameter in each neuron to capture the bias of a neuron and help improve model accuracy.

7. **Dropout**: A regularization technique where during training, a random subset of neurons are ignored or "dropped out" in each iteration to prevent overfitting.

8. **Training Process**: Involves initializing weights randomly, feeding inputs into the network, calculating errors, and iteratively adjusting weights to minimize the error. This process is repeated many times with different inputs.

9. **Desired Outcomes**: A well-trained neural network should have weights that produce accurate outputs, intuitively understandable receptive fields for neurons, and a sparse representation where most weights are small or close to zero.

10. **Additional Resources**: For further learning, one can refer to resources by Andre Carpathi, the article "The Black Magic of Deep Learning," and additional How It Works style posts on the author's blog, as well as the provided slides for this lecture.

In summary, neural networks are trained using a method called back propagation, which involves computing gradients and updating weights iteratively to minimize error. The choice of activation functions, regularization techniques like dropout, and the overall design of the network architecture play crucial roles in achieving good performance. With practice and understanding of these concepts, one can build and train neural networks effectively.

