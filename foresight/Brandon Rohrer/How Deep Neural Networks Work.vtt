WEBVTT

00:00.000 --> 00:07.360
Neural networks are good for learning lots of different types of patterns.

00:07.360 --> 00:12.000
To give an example of how this would work, imagine you had a four pixel camera.

00:12.000 --> 00:17.640
So not four megapixels, but just four pixels, and it was only black and white.

00:17.640 --> 00:22.920
And you wanted to go around and take pictures of things and determine automatically then

00:22.920 --> 00:29.320
whether these pictures were of solid, all-white, or all-dark image,

00:29.320 --> 00:35.520
vertical line, or a diagonal line, or a horizontal line.

00:35.520 --> 00:41.920
This is tricky because you can't do this with simple rules about the brightness of the pixels.

00:41.920 --> 00:49.040
Both of these are horizontal lines, but if you tried to make a rule about which pixel was bright and which was dark,

00:49.040 --> 00:53.840
you wouldn't be able to do it.

00:53.840 --> 00:59.160
So to do this with the neural network, you start by taking all of your inputs, in this case our four pixels,

00:59.160 --> 01:02.360
and you break them out into input neurons.

01:02.360 --> 01:08.680
And you assign a number to each of these, depending on the brightness or darkness of the pixel.

01:08.680 --> 01:17.760
Plus one is all the way white, minus one is all the way black, and then gray is zero, right in the middle.

01:17.760 --> 01:25.720
So these values, once you have them broken out and listed like this on the input neurons, it's also called the input vector, or array.

01:25.720 --> 01:33.640
It's just a list of numbers that represents your inputs right now.

01:33.640 --> 01:40.360
It's a useful notion to think about the receptive field of a neuron.

01:40.360 --> 01:48.640
All this means is what set of inputs makes the value of this neuron as high as it can possibly be.

01:48.640 --> 01:51.760
For input neurons, this is pretty easy.

01:51.760 --> 01:58.200
Each one is associated with just one pixel, and when that pixel is all the way white,

01:58.200 --> 02:03.280
the value of that input neuron is as high as it can go.

02:03.280 --> 02:08.800
The black and white checkered areas show pixels that an input neuron doesn't care about.

02:08.800 --> 02:17.320
If they're all the way white or all the way black, it still doesn't affect the value of that input neuron at all.

02:17.320 --> 02:22.760
Now, to build a neural network, we create a neuron.

02:22.760 --> 02:30.080
The first thing this does is it adds up all of the values of the input neurons.

02:30.080 --> 02:35.440
So in this case, if we add up all of those values, we get a point five.

02:35.440 --> 02:41.960
Now, to complicate things just a little bit, each of the connections are weighted,

02:41.960 --> 02:44.560
meaning they're multiplied by a number.

02:44.560 --> 02:50.240
That number can be one, or minus one, or anything in between.

02:50.240 --> 02:57.480
So for instance, if something has a weight of minus one, it's multiplied, and you get the negative of it, and that's added in.

02:57.480 --> 03:02.320
If something has a weight of zero, then it's effectively ignored.

03:02.320 --> 03:05.520
So here's what those weighted connections might look like.

03:05.520 --> 03:18.320
You'll notice that after the values of the input neurons are weighted and added, the final value is completely different.

03:18.320 --> 03:27.400
Graphically, it's convenient to represent these weights as white links being positive weights, black links being negative weights,

03:27.400 --> 03:36.400
and the thickness of the line is roughly proportional to the magnitude of the weight.

03:36.400 --> 03:42.760
Then after you add the weighted input neurons, they get squashed.

03:42.760 --> 03:45.000
And I'll show you what that means.

03:45.000 --> 03:47.720
You have a sigmoid squashing function.

03:47.720 --> 03:50.720
Sigmoid just means s-shaped.

03:50.720 --> 03:56.960
And what this does is you put a value in, let's say point five,

03:56.960 --> 04:03.920
and you run a vertical line up to your sigmoid, and then a horizontal line over from where it crosses.

04:03.920 --> 04:08.160
And then where that hits the y-axis, that's the output of your function.

04:08.160 --> 04:10.840
So in this case, slightly less than point five.

04:10.840 --> 04:13.560
It's pretty close.

04:13.560 --> 04:20.120
As your input number gets larger, your output number also gets larger, but more slowly.

04:20.120 --> 04:27.880
And eventually, no matter how big the number you put in, the answer is always less than one.

04:27.880 --> 04:32.920
Similarly, when you go negative, the answer is always greater than negative one.

04:32.920 --> 04:40.680
So this ensures that that neuron's value never gets outside of the range of plus one to minus one,

04:40.680 --> 04:51.040
which is helpful for keeping the computations in the neural network bounded and stable.

04:51.040 --> 04:56.600
So after you sum the weighted values of the neurons and squash the result, you get the output.

04:56.600 --> 05:02.200
In this case, point seven, four, six, that is a neuron.

05:02.200 --> 05:09.360
So we can call this, we can collapse all that down, and this is a neuron that does a weighted sum and squash the result.

05:09.360 --> 05:14.320
And now instead of just one of those, assume you have a whole bunch.

05:14.320 --> 05:20.920
There are four shown here, but there could be four hundred or four million.

05:20.920 --> 05:33.120
Now to keep our picture clear, we'll assume for now that the weights are either plus one, white lines, minus one, black lines, or zero, in which case they're missing entirely.

05:33.120 --> 05:49.160
But in actuality, all of these neurons that we created are each attached to all of the input neurons, and they all have some weight between minus one and plus one.

05:49.160 --> 05:55.880
When we create this first layer of our neural network, the receptive fields get more complex.

05:55.880 --> 06:25.040
For instance, here, each of those end up combining two of our input neurons, and so the value, the receptive field, the pixel values that make that first layer neuron as large as it can possibly be, look now like pairs of pixels, either all white or a mixture of white and black, depending on the weights.

06:25.040 --> 06:39.280
So for instance, this neuron here is attached to this input pixel, which is upper left, and this input pixel, which is lower left, and both of those weights are positive.

06:39.280 --> 06:47.320
So it combines the two of those, and that's its receptive field, the receptive field of this one plus the receptive field of this one.

06:47.320 --> 06:58.800
However, if we look at this neuron, it combines our this pixel, upper right, and this pixel, lower right.

06:58.800 --> 07:12.360
It has a weight of minus one for the lower right pixel, so that means it's most active when this pixel is black, so here is its receptive field.

07:12.360 --> 07:32.880
Now, because we were careful of how we created that first layer, its values look a lot like input values, and we can turn right around and create another layer on top of it the exact same way with the output of one layer being the input to the next layer.

07:32.880 --> 07:41.280
And we can repeat this three times or seven times or seven hundred times for additional layers.

07:41.280 --> 07:59.920
Each time the receptive fields get even more complex, so you can see here using the same logic, now they cover all of the pixels and more, more special arrangement of which are black and which are white.

07:59.960 --> 08:13.520
We can create another layer, again, all of these neurons in one layer are connected to all of the neurons in the previous layer, but we're assuming here that most of those weights are zero and not shown.

08:13.520 --> 08:16.920
It's not generally the case.

08:16.920 --> 08:26.880
So just to mix things up, we'll create a new layer, but if you notice our squashing function isn't there anymore, we have something new called a rectified linear unit.

08:26.880 --> 08:31.240
This is another popular neuron type.

08:31.240 --> 08:41.680
So you do your weighted sum of all your inputs, and instead of squashing, you do rectified linear units, you rectify it.

08:41.680 --> 08:45.840
So if it is negative, you make the value zero.

08:45.840 --> 08:49.120
If it's positive, you keep the value.

08:49.120 --> 09:01.920
This is obviously very easy to compute, and it turns out to have very nice stability properties for neural networks as well in practice.

09:01.920 --> 09:13.620
So after we do this, because some of our weights are positive and some are negative, connecting to those rectified linear units, we get receptive fields and their opposites.

09:13.620 --> 09:17.080
You look at the patterns there.

09:17.080 --> 09:23.840
And then finally, when we've created as many layers with as many neurons as we want, we create an output layer.

09:23.840 --> 09:27.080
Here, we have four outputs that we're interested in.

09:27.080 --> 09:35.040
Is the image solid, vertical, diagonal, or horizontal?

09:35.040 --> 09:43.920
So to walk through an example here of how this would work, let's say we start with this input image shown on the left.

09:43.960 --> 09:46.880
Dark pixels on top, white on the bottom.

09:46.880 --> 09:53.580
As we propagate that to our input layer, this is what those values would look like.

09:53.580 --> 09:56.760
The top pixels, the bottom pixels.

09:56.760 --> 10:11.620
As we move that to our first layer, we can see the combination of a dark pixel and a light pixel, summed together, get us zero, gray.

10:11.620 --> 10:18.720
Whereas down here, we have the combination of a dark pixel plus a light pixel with a negative weight.

10:18.720 --> 10:30.660
So that gets us a value of negative one there, which makes sense because if we look at the receptive field here, upper left pixel white, lower left pixel black,

10:30.660 --> 10:35.020
it's the exact opposite of the input that we're getting.

10:35.020 --> 10:42.320
And so we would expect its value to be as low as possible, minus one.

10:42.320 --> 10:54.520
As we move to the next layer, we see the same types of things, combining zeros to get zeros, combining a negative and a negative with a negative weight,

10:54.520 --> 10:57.960
which makes a positive to get a zero.

10:57.960 --> 11:01.780
And here we have combining two negatives to get a negative.

11:01.780 --> 11:06.880
So again, you'll notice the receptive field of this is exactly the inverse of our input.

11:06.880 --> 11:14.440
So it makes sense that its weight would be negative, or its value would be negative.

11:14.440 --> 11:17.080
And we move to the next layer.

11:17.080 --> 11:21.280
All of these, of course, these zeros propagate forward.

11:21.280 --> 11:27.880
Here, this has a negative value and it has a positive weight.

11:27.880 --> 11:34.940
So it just moves straight forward because we have a rectified linear unit, negative values become zero.

11:34.940 --> 11:42.240
So now it is zero again, two, but this one gets rectified and becomes positive, negative times the negative is positive.

11:42.240 --> 11:48.740
And so when we finally get to the output, we can see they're all zero except for this horizontal, which is positive.

11:48.740 --> 11:49.940
And that's the answer.

11:49.940 --> 11:57.780
Our neural network said this is an image of a horizontal line.

11:57.800 --> 12:02.300
Now, neural networks usually aren't that good, not that clean.

12:02.300 --> 12:06.880
So there's a notion of, with an input, what is truth?

12:06.880 --> 12:13.700
In this case, the truth is this has a zero for all of these values, but a one for horizontal.

12:13.700 --> 12:16.580
It's not solid, it's not vertical, it's not diagonal.

12:16.580 --> 12:19.640
Yes, it is horizontal.

12:19.640 --> 12:24.600
An arbitrary neural network will give answers that are not exactly truth.

12:24.600 --> 12:27.920
It might be off by a little or a lot.

12:27.920 --> 12:35.160
And then the error is the magnitude of the difference between the truth and the answer given.

12:35.160 --> 12:41.020
And you can add all these up to get the total error for the neural network.

12:41.020 --> 12:51.820
So the idea, the whole idea with learning and training is to adjust the weights to make the error as low as possible.

12:51.820 --> 13:05.980
So the way this is done is we put an image in, we calculate the error at the end, then we look for how to adjust those weights higher or lower to either make that error go up or down.

13:05.980 --> 13:11.240
And we, of course, adjust the weights in the way, then make the error go down.

13:11.240 --> 13:21.460
Now, the problem with doing this is each time we go back and calculate the error, we have to multiply all of those weights by all of the

13:21.500 --> 13:24.540
neuron values at each layer.

13:24.540 --> 13:28.440
And we have to do that again and again once for each weight.

13:28.440 --> 13:33.880
This takes forever in computing terms, on a computing scale.

13:33.880 --> 13:39.240
And so it's not a practical way to train a big neural network.

13:39.240 --> 13:43.440
You can imagine instead of just rolling down to the bottom of a simple valley, we have

13:43.440 --> 13:47.900
a very high dimensional valley and we have to find our way down.

13:47.900 --> 13:52.780
And because there are so many dimensions, one for each of these weights, that the computation

13:52.780 --> 14:01.940
just becomes prohibitively expensive, luckily there was an insight that lets us do this in

14:01.940 --> 14:04.160
a very reasonable time.

14:04.160 --> 14:09.060
And that's that if we're careful about how we design our neural network, we can calculate

14:09.060 --> 14:12.020
the slope directly, the gradient.

14:12.020 --> 14:17.060
We can figure out the direction that we need to adjust the weight without going all the

14:17.060 --> 14:22.740
way back through our neural network and recalculating.

14:22.740 --> 14:28.980
So just to review, the slope that we're talking about is when we make a change in weight,

14:28.980 --> 14:31.620
the error will change a little bit.

14:31.620 --> 14:38.380
And that relation of the change in weight to the change in error is the slope.

14:38.380 --> 14:42.120
Mathematically, there are several ways to write this.

14:42.120 --> 14:43.520
We'll favor the one on the bottom.

14:43.520 --> 14:45.760
It's technically most correct.

14:45.760 --> 14:48.960
We'll call it DEDW for short hand.

14:48.960 --> 14:56.760
Every time you see it, just think the change in error when I change a weight or the change

14:56.760 --> 15:01.520
in the thing on the top when I change the thing on the bottom.

15:01.520 --> 15:05.240
This does get into a little bit of calculus.

15:05.240 --> 15:06.240
We do take derivatives.

15:06.240 --> 15:08.680
That's how we calculate slope.

15:08.680 --> 15:14.240
If it's new to you, I strongly recommend a good semester of calculus just because the

15:14.240 --> 15:18.200
concepts are so universal.

15:18.200 --> 15:22.840
A lot of them have very nice physical interpretations, which I find very appealing.

15:22.840 --> 15:27.720
But don't worry, otherwise just gloss over this and pay attention to the rest and you'll

15:27.720 --> 15:31.800
get a general sense for how this works.

15:31.800 --> 15:36.560
So in this case, if we change the weight by plus one, the error changes by minus two,

15:36.560 --> 15:39.980
which gives us a slope of minus two.

15:39.980 --> 15:44.700
That tells us the direction that we should adjust our weight and how much we should adjust

15:44.700 --> 15:49.460
it to bring the error down.

15:49.460 --> 15:52.580
Now to do this, you have to know what your error function is.

15:52.580 --> 15:58.140
So assume we had an error function that was the square of the weight, and you can see that

15:58.140 --> 16:02.540
our weight is right at minus one.

16:02.540 --> 16:07.460
So the first thing we do is we take the derivative, change in error, divided by change in weight

16:07.460 --> 16:09.460
dE dW.

16:09.460 --> 16:13.300
The derivative of weight squared is two times the weight.

16:13.300 --> 16:22.380
And so we plug in our weight of minus one and we get a slope dE dW of minus two.

16:22.380 --> 16:27.580
Now the other trick that lets us do this with deep neural networks is chaining.

16:27.580 --> 16:32.660
And to show you how this works, imagine a very simple trivial neural network with just

16:32.860 --> 16:39.620
one hidden layer, one input layer, one output layer, and one weight connecting each of them.

16:39.620 --> 16:45.460
So it's obvious to see that the value y is just the value x times the weight connecting

16:45.460 --> 16:48.900
them, w1.

16:48.900 --> 16:54.420
So if we change w1 a little bit, we just take the derivative of y with respect to w1, and

16:54.420 --> 16:55.420
we get x.

16:55.420 --> 16:56.420
The slope is x.

16:56.420 --> 17:03.420
If I change w1 by a little bit, then y will change by x times the size of that adjustment.

17:03.420 --> 17:12.180
Similarly, for the next step, we can see that E is just the value y times the weight w2.

17:12.180 --> 17:18.380
And so when we calculate dE dy, it's just w2.

17:18.380 --> 17:23.620
Because this network is so simple, we can calculate from one end to the other, x times

17:23.620 --> 17:28.140
w1 times w2 is the error E.

17:28.140 --> 17:32.940
And so if we want to calculate how much will the error change, if I change w1, we just

17:32.940 --> 17:39.220
take the derivative of that with respect to w1, and get x times w2.

17:39.220 --> 17:44.340
So this illustrates, you can see here now, that what we just calculated is actually the

17:44.340 --> 17:53.580
product of our first derivative that we took, the dy dw1 times the derivative for the next

17:53.580 --> 17:59.780
step, dE dy, multiplied together.

17:59.780 --> 18:02.020
This is chaining.

18:02.020 --> 18:08.220
You can calculate the slope of each tiny step, and then multiply all of those together to

18:08.220 --> 18:13.660
get the slope of the full chain, the derivative of the full chain.

18:13.660 --> 18:18.900
So in a deeper neural network, what this would look like is if I want to know how much the

18:18.900 --> 18:25.460
error will change, if I adjust a weight that's deep in the network, I just calculate the

18:25.460 --> 18:30.980
derivative of each tiny little step all the way back to the weight that I'm trying to

18:30.980 --> 18:35.540
calculate, and then multiply them all together.

18:35.540 --> 18:42.020
This computationally is many, many times cheaper than what we had to do before of recalculating

18:42.020 --> 18:47.540
the error for the whole neural network for every weight.

18:47.540 --> 18:53.540
Now in the neural network that we've created, there are several types of back propagation

18:53.540 --> 18:54.540
we have to do.

18:54.540 --> 18:59.020
There are several operations we have to do for each one of those, we have to be able

18:59.020 --> 19:01.020
to calculate the slope.

19:01.020 --> 19:08.580
So for the first one is just a weighted connection between two neurons, A and B. So let's assume

19:08.580 --> 19:14.340
we know the change in error with respect to B. We want to know the change in error with

19:14.340 --> 19:20.820
respect to A. To get there, we need to know DBDA.

19:20.820 --> 19:25.580
So to get that, we just write the relationship between B and A, take the derivative of B

19:25.580 --> 19:31.340
with respect to A, we get the weight W, and now we know how to make that step.

19:31.340 --> 19:36.940
We know how to do that little nugget of back propagation.

19:36.940 --> 19:39.980
Another element that we've seen is sums.

19:39.980 --> 19:43.780
All of our neurons sum up a lot of inputs.

19:43.820 --> 19:50.740
To take this back propagation step, we do the same thing, we write our expression, and

19:50.740 --> 19:58.140
then we take the derivative of our endpoint Z with respect to our step that we're propagating

19:58.140 --> 20:03.140
to A, and DZDA in this case is just one.

20:03.140 --> 20:04.140
Which makes sense.

20:04.140 --> 20:10.100
If we have a sum of a whole bunch of elements, we increase one of those elements by one,

20:10.100 --> 20:13.220
we expect the sum to increase by one.

20:13.260 --> 20:20.900
That's the definition of a slope of one, one-to-one relation there.

20:20.900 --> 20:24.660
Another element that we have that we need to be able to back propagate is the sigmoid

20:24.660 --> 20:26.900
function.

20:26.900 --> 20:30.540
So this one's a little bit more interesting mathematically.

20:30.540 --> 20:35.220
We'll just write it shorthand like this, the sigma function.

20:35.220 --> 20:42.020
It is entirely feasible to go through and take the derivative of this analytically and calculate

20:42.020 --> 20:43.140
it.

20:43.140 --> 20:49.740
It just so happens that this function has a nice property that to get its derivative,

20:49.740 --> 20:54.100
you just multiply it by one minus itself.

20:54.100 --> 21:00.780
So this is very straightforward to calculate.

21:00.780 --> 21:03.580
Another element that we've used is the rectified linear unit.

21:03.580 --> 21:09.220
Again, to figure out how to back propagate this, we just write out the relation, B is

21:09.220 --> 21:13.740
equal to A if A is positive, otherwise it's zero.

21:13.740 --> 21:17.940
And piecewise, for each of those, we take the derivative.

21:17.940 --> 21:25.100
So dBdA is either one, if A is positive, or zero.

21:25.100 --> 21:32.180
And so with all of these little back propagation steps and the ability to chain them together,

21:32.180 --> 21:39.540
we can calculate the effect of adjusting any given weight on the error for any given

21:39.540 --> 21:41.940
input.

21:41.940 --> 21:48.020
And so to train, then, we start with a fully connected network.

21:48.020 --> 21:54.060
We don't know what any of these weights should be, and so we assign them all random values.

21:54.060 --> 21:58.860
We create a completely arbitrary random neural network.

21:58.860 --> 22:02.500
We put in an input that we know the answer to.

22:02.500 --> 22:07.140
We know whether it's solid, vertical, diagonal, or horizontal, so we know what truth should

22:07.140 --> 22:11.580
be, and so we can calculate the error.

22:11.580 --> 22:18.260
Then we run it through, calculate the error, and using back propagation, go through and

22:18.260 --> 22:23.700
adjust all of those weights a tiny bit in the right direction.

22:23.700 --> 22:28.500
And then we do that again with another input, and again with another input for, if we can

22:28.500 --> 22:33.460
get away with it, many thousands or even millions of times.

22:33.460 --> 22:40.020
And eventually, all of those weights will gravitate, they'll roll down that many dimensional

22:40.020 --> 22:47.140
valley to a nice low spot in the bottom, where it performs really well and does pretty close

22:47.140 --> 22:52.700
to truth on most of the images.

22:52.700 --> 23:01.020
If we're really lucky, it'll look like what we started with, with intuitively understandable

23:01.020 --> 23:06.540
receptive fields for those neurons, and a relatively sparse representation, meaning

23:06.540 --> 23:10.900
that most of the weights are small or close to zero.

23:10.900 --> 23:17.260
It doesn't always turn out that way, but what we are guaranteed is that it'll find a pretty

23:17.260 --> 23:23.380
good representation of the best that it can do adjusting those weights to get as close

23:23.380 --> 23:30.860
as possible to the right answer for all of the inputs.

23:30.860 --> 23:34.820
So what we've covered is just a very basic introduction to the principles behind neural

23:34.820 --> 23:36.060
networks.

23:36.060 --> 23:40.060
I haven't told you quite enough to be able to go out and build one of your own, but if

23:40.060 --> 23:44.340
you're feeling motivated to do so, I highly encourage it.

23:44.340 --> 23:47.660
Here are a few resources that you'll find useful.

23:47.660 --> 23:51.100
You'll want to go and learn about bias neurons.

23:51.100 --> 23:54.140
Dropout is a useful training tool.

23:54.140 --> 24:00.500
There are several resources available from Andre Carpathi, who is an expert in neural

24:00.500 --> 24:04.740
networks and great at teaching about it.

24:04.740 --> 24:08.420
Also there's a fantastic article called The Black Magic of Deep Learning that just has

24:08.420 --> 24:16.420
a bunch of practical from the trenches tips on how to get them working well.

24:16.420 --> 24:20.700
If you found this useful, I highly encourage you to visit my blog and check out several

24:20.700 --> 24:24.700
other How It Works style posts.

24:24.700 --> 24:30.860
And the links for these slides you can get as well to use however you like.

24:30.860 --> 24:34.340
There's also a link to them down in the comments section.

24:34.340 --> 24:35.140
Thanks for listening.

