{"text": " Neural networks are good for learning lots of different types of patterns. To give an example of how this would work, imagine you had a four pixel camera. So not four megapixels, but just four pixels, and it was only black and white. And you wanted to go around and take pictures of things and determine automatically then whether these pictures were of solid, all-white, or all-dark image, vertical line, or a diagonal line, or a horizontal line. This is tricky because you can't do this with simple rules about the brightness of the pixels. Both of these are horizontal lines, but if you tried to make a rule about which pixel was bright and which was dark, you wouldn't be able to do it. So to do this with the neural network, you start by taking all of your inputs, in this case our four pixels, and you break them out into input neurons. And you assign a number to each of these, depending on the brightness or darkness of the pixel. Plus one is all the way white, minus one is all the way black, and then gray is zero, right in the middle. So these values, once you have them broken out and listed like this on the input neurons, it's also called the input vector, or array. It's just a list of numbers that represents your inputs right now. It's a useful notion to think about the receptive field of a neuron. All this means is what set of inputs makes the value of this neuron as high as it can possibly be. For input neurons, this is pretty easy. Each one is associated with just one pixel, and when that pixel is all the way white, the value of that input neuron is as high as it can go. The black and white checkered areas show pixels that an input neuron doesn't care about. If they're all the way white or all the way black, it still doesn't affect the value of that input neuron at all. Now, to build a neural network, we create a neuron. The first thing this does is it adds up all of the values of the input neurons. So in this case, if we add up all of those values, we get a point five. Now, to complicate things just a little bit, each of the connections are weighted, meaning they're multiplied by a number. That number can be one, or minus one, or anything in between. So for instance, if something has a weight of minus one, it's multiplied, and you get the negative of it, and that's added in. If something has a weight of zero, then it's effectively ignored. So here's what those weighted connections might look like. You'll notice that after the values of the input neurons are weighted and added, the final value is completely different. Graphically, it's convenient to represent these weights as white links being positive weights, black links being negative weights, and the thickness of the line is roughly proportional to the magnitude of the weight. Then after you add the weighted input neurons, they get squashed. And I'll show you what that means. You have a sigmoid squashing function. Sigmoid just means s-shaped. And what this does is you put a value in, let's say point five, and you run a vertical line up to your sigmoid, and then a horizontal line over from where it crosses. And then where that hits the y-axis, that's the output of your function. So in this case, slightly less than point five. It's pretty close. As your input number gets larger, your output number also gets larger, but more slowly. And eventually, no matter how big the number you put in, the answer is always less than one. Similarly, when you go negative, the answer is always greater than negative one. So this ensures that that neuron's value never gets outside of the range of plus one to minus one, which is helpful for keeping the computations in the neural network bounded and stable. So after you sum the weighted values of the neurons and squash the result, you get the output. In this case, point seven, four, six, that is a neuron. So we can call this, we can collapse all that down, and this is a neuron that does a weighted sum and squash the result. And now instead of just one of those, assume you have a whole bunch. There are four shown here, but there could be four hundred or four million. Now to keep our picture clear, we'll assume for now that the weights are either plus one, white lines, minus one, black lines, or zero, in which case they're missing entirely. But in actuality, all of these neurons that we created are each attached to all of the input neurons, and they all have some weight between minus one and plus one. When we create this first layer of our neural network, the receptive fields get more complex. For instance, here, each of those end up combining two of our input neurons, and so the value, the receptive field, the pixel values that make that first layer neuron as large as it can possibly be, look now like pairs of pixels, either all white or a mixture of white and black, depending on the weights. So for instance, this neuron here is attached to this input pixel, which is upper left, and this input pixel, which is lower left, and both of those weights are positive. So it combines the two of those, and that's its receptive field, the receptive field of this one plus the receptive field of this one. However, if we look at this neuron, it combines our this pixel, upper right, and this pixel, lower right. It has a weight of minus one for the lower right pixel, so that means it's most active when this pixel is black, so here is its receptive field. Now, because we were careful of how we created that first layer, its values look a lot like input values, and we can turn right around and create another layer on top of it the exact same way with the output of one layer being the input to the next layer. And we can repeat this three times or seven times or seven hundred times for additional layers. Each time the receptive fields get even more complex, so you can see here using the same logic, now they cover all of the pixels and more, more special arrangement of which are black and which are white. We can create another layer, again, all of these neurons in one layer are connected to all of the neurons in the previous layer, but we're assuming here that most of those weights are zero and not shown. It's not generally the case. So just to mix things up, we'll create a new layer, but if you notice our squashing function isn't there anymore, we have something new called a rectified linear unit. This is another popular neuron type. So you do your weighted sum of all your inputs, and instead of squashing, you do rectified linear units, you rectify it. So if it is negative, you make the value zero. If it's positive, you keep the value. This is obviously very easy to compute, and it turns out to have very nice stability properties for neural networks as well in practice. So after we do this, because some of our weights are positive and some are negative, connecting to those rectified linear units, we get receptive fields and their opposites. You look at the patterns there. And then finally, when we've created as many layers with as many neurons as we want, we create an output layer. Here, we have four outputs that we're interested in. Is the image solid, vertical, diagonal, or horizontal? So to walk through an example here of how this would work, let's say we start with this input image shown on the left. Dark pixels on top, white on the bottom. As we propagate that to our input layer, this is what those values would look like. The top pixels, the bottom pixels. As we move that to our first layer, we can see the combination of a dark pixel and a light pixel, summed together, get us zero, gray. Whereas down here, we have the combination of a dark pixel plus a light pixel with a negative weight. So that gets us a value of negative one there, which makes sense because if we look at the receptive field here, upper left pixel white, lower left pixel black, it's the exact opposite of the input that we're getting. And so we would expect its value to be as low as possible, minus one. As we move to the next layer, we see the same types of things, combining zeros to get zeros, combining a negative and a negative with a negative weight, which makes a positive to get a zero. And here we have combining two negatives to get a negative. So again, you'll notice the receptive field of this is exactly the inverse of our input. So it makes sense that its weight would be negative, or its value would be negative. And we move to the next layer. All of these, of course, these zeros propagate forward. Here, this has a negative value and it has a positive weight. So it just moves straight forward because we have a rectified linear unit, negative values become zero. So now it is zero again, two, but this one gets rectified and becomes positive, negative times the negative is positive. And so when we finally get to the output, we can see they're all zero except for this horizontal, which is positive. And that's the answer. Our neural network said this is an image of a horizontal line. Now, neural networks usually aren't that good, not that clean. So there's a notion of, with an input, what is truth? In this case, the truth is this has a zero for all of these values, but a one for horizontal. It's not solid, it's not vertical, it's not diagonal. Yes, it is horizontal. An arbitrary neural network will give answers that are not exactly truth. It might be off by a little or a lot. And then the error is the magnitude of the difference between the truth and the answer given. And you can add all these up to get the total error for the neural network. So the idea, the whole idea with learning and training is to adjust the weights to make the error as low as possible. So the way this is done is we put an image in, we calculate the error at the end, then we look for how to adjust those weights higher or lower to either make that error go up or down. And we, of course, adjust the weights in the way, then make the error go down. Now, the problem with doing this is each time we go back and calculate the error, we have to multiply all of those weights by all of the neuron values at each layer. And we have to do that again and again once for each weight. This takes forever in computing terms, on a computing scale. And so it's not a practical way to train a big neural network. You can imagine instead of just rolling down to the bottom of a simple valley, we have a very high dimensional valley and we have to find our way down. And because there are so many dimensions, one for each of these weights, that the computation just becomes prohibitively expensive, luckily there was an insight that lets us do this in a very reasonable time. And that's that if we're careful about how we design our neural network, we can calculate the slope directly, the gradient. We can figure out the direction that we need to adjust the weight without going all the way back through our neural network and recalculating. So just to review, the slope that we're talking about is when we make a change in weight, the error will change a little bit. And that relation of the change in weight to the change in error is the slope. Mathematically, there are several ways to write this. We'll favor the one on the bottom. It's technically most correct. We'll call it DEDW for short hand. Every time you see it, just think the change in error when I change a weight or the change in the thing on the top when I change the thing on the bottom. This does get into a little bit of calculus. We do take derivatives. That's how we calculate slope. If it's new to you, I strongly recommend a good semester of calculus just because the concepts are so universal. A lot of them have very nice physical interpretations, which I find very appealing. But don't worry, otherwise just gloss over this and pay attention to the rest and you'll get a general sense for how this works. So in this case, if we change the weight by plus one, the error changes by minus two, which gives us a slope of minus two. That tells us the direction that we should adjust our weight and how much we should adjust it to bring the error down. Now to do this, you have to know what your error function is. So assume we had an error function that was the square of the weight, and you can see that our weight is right at minus one. So the first thing we do is we take the derivative, change in error, divided by change in weight dE dW. The derivative of weight squared is two times the weight. And so we plug in our weight of minus one and we get a slope dE dW of minus two. Now the other trick that lets us do this with deep neural networks is chaining. And to show you how this works, imagine a very simple trivial neural network with just one hidden layer, one input layer, one output layer, and one weight connecting each of them. So it's obvious to see that the value y is just the value x times the weight connecting them, w1. So if we change w1 a little bit, we just take the derivative of y with respect to w1, and we get x. The slope is x. If I change w1 by a little bit, then y will change by x times the size of that adjustment. Similarly, for the next step, we can see that E is just the value y times the weight w2. And so when we calculate dE dy, it's just w2. Because this network is so simple, we can calculate from one end to the other, x times w1 times w2 is the error E. And so if we want to calculate how much will the error change, if I change w1, we just take the derivative of that with respect to w1, and get x times w2. So this illustrates, you can see here now, that what we just calculated is actually the product of our first derivative that we took, the dy dw1 times the derivative for the next step, dE dy, multiplied together. This is chaining. You can calculate the slope of each tiny step, and then multiply all of those together to get the slope of the full chain, the derivative of the full chain. So in a deeper neural network, what this would look like is if I want to know how much the error will change, if I adjust a weight that's deep in the network, I just calculate the derivative of each tiny little step all the way back to the weight that I'm trying to calculate, and then multiply them all together. This computationally is many, many times cheaper than what we had to do before of recalculating the error for the whole neural network for every weight. Now in the neural network that we've created, there are several types of back propagation we have to do. There are several operations we have to do for each one of those, we have to be able to calculate the slope. So for the first one is just a weighted connection between two neurons, A and B. So let's assume we know the change in error with respect to B. We want to know the change in error with respect to A. To get there, we need to know DBDA. So to get that, we just write the relationship between B and A, take the derivative of B with respect to A, we get the weight W, and now we know how to make that step. We know how to do that little nugget of back propagation. Another element that we've seen is sums. All of our neurons sum up a lot of inputs. To take this back propagation step, we do the same thing, we write our expression, and then we take the derivative of our endpoint Z with respect to our step that we're propagating to A, and DZDA in this case is just one. Which makes sense. If we have a sum of a whole bunch of elements, we increase one of those elements by one, we expect the sum to increase by one. That's the definition of a slope of one, one-to-one relation there. Another element that we have that we need to be able to back propagate is the sigmoid function. So this one's a little bit more interesting mathematically. We'll just write it shorthand like this, the sigma function. It is entirely feasible to go through and take the derivative of this analytically and calculate it. It just so happens that this function has a nice property that to get its derivative, you just multiply it by one minus itself. So this is very straightforward to calculate. Another element that we've used is the rectified linear unit. Again, to figure out how to back propagate this, we just write out the relation, B is equal to A if A is positive, otherwise it's zero. And piecewise, for each of those, we take the derivative. So dBdA is either one, if A is positive, or zero. And so with all of these little back propagation steps and the ability to chain them together, we can calculate the effect of adjusting any given weight on the error for any given input. And so to train, then, we start with a fully connected network. We don't know what any of these weights should be, and so we assign them all random values. We create a completely arbitrary random neural network. We put in an input that we know the answer to. We know whether it's solid, vertical, diagonal, or horizontal, so we know what truth should be, and so we can calculate the error. Then we run it through, calculate the error, and using back propagation, go through and adjust all of those weights a tiny bit in the right direction. And then we do that again with another input, and again with another input for, if we can get away with it, many thousands or even millions of times. And eventually, all of those weights will gravitate, they'll roll down that many dimensional valley to a nice low spot in the bottom, where it performs really well and does pretty close to truth on most of the images. If we're really lucky, it'll look like what we started with, with intuitively understandable receptive fields for those neurons, and a relatively sparse representation, meaning that most of the weights are small or close to zero. It doesn't always turn out that way, but what we are guaranteed is that it'll find a pretty good representation of the best that it can do adjusting those weights to get as close as possible to the right answer for all of the inputs. So what we've covered is just a very basic introduction to the principles behind neural networks. I haven't told you quite enough to be able to go out and build one of your own, but if you're feeling motivated to do so, I highly encourage it. Here are a few resources that you'll find useful. You'll want to go and learn about bias neurons. Dropout is a useful training tool. There are several resources available from Andre Carpathi, who is an expert in neural networks and great at teaching about it. Also there's a fantastic article called The Black Magic of Deep Learning that just has a bunch of practical from the trenches tips on how to get them working well. If you found this useful, I highly encourage you to visit my blog and check out several other How It Works style posts. And the links for these slides you can get as well to use however you like. There's also a link to them down in the comments section. Thanks for listening.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.36, "text": " Neural networks are good for learning lots of different types of patterns.", "tokens": [50364, 1734, 1807, 9590, 366, 665, 337, 2539, 3195, 295, 819, 3467, 295, 8294, 13, 50732], "temperature": 0.0, "avg_logprob": -0.17819576758842964, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.1797330379486084}, {"id": 1, "seek": 0, "start": 7.36, "end": 12.0, "text": " To give an example of how this would work, imagine you had a four pixel camera.", "tokens": [50732, 1407, 976, 364, 1365, 295, 577, 341, 576, 589, 11, 3811, 291, 632, 257, 1451, 19261, 2799, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17819576758842964, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.1797330379486084}, {"id": 2, "seek": 0, "start": 12.0, "end": 17.64, "text": " So not four megapixels, but just four pixels, and it was only black and white.", "tokens": [50964, 407, 406, 1451, 34733, 970, 1625, 11, 457, 445, 1451, 18668, 11, 293, 309, 390, 787, 2211, 293, 2418, 13, 51246], "temperature": 0.0, "avg_logprob": -0.17819576758842964, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.1797330379486084}, {"id": 3, "seek": 0, "start": 17.64, "end": 22.92, "text": " And you wanted to go around and take pictures of things and determine automatically then", "tokens": [51246, 400, 291, 1415, 281, 352, 926, 293, 747, 5242, 295, 721, 293, 6997, 6772, 550, 51510], "temperature": 0.0, "avg_logprob": -0.17819576758842964, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.1797330379486084}, {"id": 4, "seek": 2292, "start": 22.92, "end": 29.32, "text": " whether these pictures were of solid, all-white, or all-dark image,", "tokens": [50364, 1968, 613, 5242, 645, 295, 5100, 11, 439, 12, 28865, 11, 420, 439, 12, 67, 809, 3256, 11, 50684], "temperature": 0.0, "avg_logprob": -0.19743059943704044, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0226212739944458}, {"id": 5, "seek": 2292, "start": 29.32, "end": 35.52, "text": " vertical line, or a diagonal line, or a horizontal line.", "tokens": [50684, 9429, 1622, 11, 420, 257, 21539, 1622, 11, 420, 257, 12750, 1622, 13, 50994], "temperature": 0.0, "avg_logprob": -0.19743059943704044, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0226212739944458}, {"id": 6, "seek": 2292, "start": 35.52, "end": 41.92, "text": " This is tricky because you can't do this with simple rules about the brightness of the pixels.", "tokens": [50994, 639, 307, 12414, 570, 291, 393, 380, 360, 341, 365, 2199, 4474, 466, 264, 21367, 295, 264, 18668, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19743059943704044, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0226212739944458}, {"id": 7, "seek": 2292, "start": 41.92, "end": 49.040000000000006, "text": " Both of these are horizontal lines, but if you tried to make a rule about which pixel was bright and which was dark,", "tokens": [51314, 6767, 295, 613, 366, 12750, 3876, 11, 457, 498, 291, 3031, 281, 652, 257, 4978, 466, 597, 19261, 390, 4730, 293, 597, 390, 2877, 11, 51670], "temperature": 0.0, "avg_logprob": -0.19743059943704044, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0226212739944458}, {"id": 8, "seek": 4904, "start": 49.04, "end": 53.839999999999996, "text": " you wouldn't be able to do it.", "tokens": [50364, 291, 2759, 380, 312, 1075, 281, 360, 309, 13, 50604], "temperature": 0.0, "avg_logprob": -0.1305866708942488, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0012444969033822417}, {"id": 9, "seek": 4904, "start": 53.839999999999996, "end": 59.16, "text": " So to do this with the neural network, you start by taking all of your inputs, in this case our four pixels,", "tokens": [50604, 407, 281, 360, 341, 365, 264, 18161, 3209, 11, 291, 722, 538, 1940, 439, 295, 428, 15743, 11, 294, 341, 1389, 527, 1451, 18668, 11, 50870], "temperature": 0.0, "avg_logprob": -0.1305866708942488, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0012444969033822417}, {"id": 10, "seek": 4904, "start": 59.16, "end": 62.36, "text": " and you break them out into input neurons.", "tokens": [50870, 293, 291, 1821, 552, 484, 666, 4846, 22027, 13, 51030], "temperature": 0.0, "avg_logprob": -0.1305866708942488, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0012444969033822417}, {"id": 11, "seek": 4904, "start": 62.36, "end": 68.68, "text": " And you assign a number to each of these, depending on the brightness or darkness of the pixel.", "tokens": [51030, 400, 291, 6269, 257, 1230, 281, 1184, 295, 613, 11, 5413, 322, 264, 21367, 420, 11262, 295, 264, 19261, 13, 51346], "temperature": 0.0, "avg_logprob": -0.1305866708942488, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0012444969033822417}, {"id": 12, "seek": 4904, "start": 68.68, "end": 77.75999999999999, "text": " Plus one is all the way white, minus one is all the way black, and then gray is zero, right in the middle.", "tokens": [51346, 7721, 472, 307, 439, 264, 636, 2418, 11, 3175, 472, 307, 439, 264, 636, 2211, 11, 293, 550, 10855, 307, 4018, 11, 558, 294, 264, 2808, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1305866708942488, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0012444969033822417}, {"id": 13, "seek": 7776, "start": 77.76, "end": 85.72, "text": " So these values, once you have them broken out and listed like this on the input neurons, it's also called the input vector, or array.", "tokens": [50364, 407, 613, 4190, 11, 1564, 291, 362, 552, 5463, 484, 293, 10052, 411, 341, 322, 264, 4846, 22027, 11, 309, 311, 611, 1219, 264, 4846, 8062, 11, 420, 10225, 13, 50762], "temperature": 0.0, "avg_logprob": -0.10500886547031688, "compression_ratio": 1.5340909090909092, "no_speech_prob": 0.0009396742680110037}, {"id": 14, "seek": 7776, "start": 85.72, "end": 93.64, "text": " It's just a list of numbers that represents your inputs right now.", "tokens": [50762, 467, 311, 445, 257, 1329, 295, 3547, 300, 8855, 428, 15743, 558, 586, 13, 51158], "temperature": 0.0, "avg_logprob": -0.10500886547031688, "compression_ratio": 1.5340909090909092, "no_speech_prob": 0.0009396742680110037}, {"id": 15, "seek": 7776, "start": 93.64, "end": 100.36000000000001, "text": " It's a useful notion to think about the receptive field of a neuron.", "tokens": [51158, 467, 311, 257, 4420, 10710, 281, 519, 466, 264, 45838, 2519, 295, 257, 34090, 13, 51494], "temperature": 0.0, "avg_logprob": -0.10500886547031688, "compression_ratio": 1.5340909090909092, "no_speech_prob": 0.0009396742680110037}, {"id": 16, "seek": 10036, "start": 100.36, "end": 108.64, "text": " All this means is what set of inputs makes the value of this neuron as high as it can possibly be.", "tokens": [50364, 1057, 341, 1355, 307, 437, 992, 295, 15743, 1669, 264, 2158, 295, 341, 34090, 382, 1090, 382, 309, 393, 6264, 312, 13, 50778], "temperature": 0.0, "avg_logprob": -0.10582365888230344, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.1686316430568695}, {"id": 17, "seek": 10036, "start": 108.64, "end": 111.76, "text": " For input neurons, this is pretty easy.", "tokens": [50778, 1171, 4846, 22027, 11, 341, 307, 1238, 1858, 13, 50934], "temperature": 0.0, "avg_logprob": -0.10582365888230344, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.1686316430568695}, {"id": 18, "seek": 10036, "start": 111.76, "end": 118.2, "text": " Each one is associated with just one pixel, and when that pixel is all the way white,", "tokens": [50934, 6947, 472, 307, 6615, 365, 445, 472, 19261, 11, 293, 562, 300, 19261, 307, 439, 264, 636, 2418, 11, 51256], "temperature": 0.0, "avg_logprob": -0.10582365888230344, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.1686316430568695}, {"id": 19, "seek": 10036, "start": 118.2, "end": 123.28, "text": " the value of that input neuron is as high as it can go.", "tokens": [51256, 264, 2158, 295, 300, 4846, 34090, 307, 382, 1090, 382, 309, 393, 352, 13, 51510], "temperature": 0.0, "avg_logprob": -0.10582365888230344, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.1686316430568695}, {"id": 20, "seek": 10036, "start": 123.28, "end": 128.8, "text": " The black and white checkered areas show pixels that an input neuron doesn't care about.", "tokens": [51510, 440, 2211, 293, 2418, 1520, 4073, 3179, 855, 18668, 300, 364, 4846, 34090, 1177, 380, 1127, 466, 13, 51786], "temperature": 0.0, "avg_logprob": -0.10582365888230344, "compression_ratio": 1.7488151658767772, "no_speech_prob": 0.1686316430568695}, {"id": 21, "seek": 12880, "start": 128.8, "end": 137.32000000000002, "text": " If they're all the way white or all the way black, it still doesn't affect the value of that input neuron at all.", "tokens": [50364, 759, 436, 434, 439, 264, 636, 2418, 420, 439, 264, 636, 2211, 11, 309, 920, 1177, 380, 3345, 264, 2158, 295, 300, 4846, 34090, 412, 439, 13, 50790], "temperature": 0.0, "avg_logprob": -0.09479899839921431, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.0012839070986956358}, {"id": 22, "seek": 12880, "start": 137.32000000000002, "end": 142.76000000000002, "text": " Now, to build a neural network, we create a neuron.", "tokens": [50790, 823, 11, 281, 1322, 257, 18161, 3209, 11, 321, 1884, 257, 34090, 13, 51062], "temperature": 0.0, "avg_logprob": -0.09479899839921431, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.0012839070986956358}, {"id": 23, "seek": 12880, "start": 142.76000000000002, "end": 150.08, "text": " The first thing this does is it adds up all of the values of the input neurons.", "tokens": [51062, 440, 700, 551, 341, 775, 307, 309, 10860, 493, 439, 295, 264, 4190, 295, 264, 4846, 22027, 13, 51428], "temperature": 0.0, "avg_logprob": -0.09479899839921431, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.0012839070986956358}, {"id": 24, "seek": 12880, "start": 150.08, "end": 155.44, "text": " So in this case, if we add up all of those values, we get a point five.", "tokens": [51428, 407, 294, 341, 1389, 11, 498, 321, 909, 493, 439, 295, 729, 4190, 11, 321, 483, 257, 935, 1732, 13, 51696], "temperature": 0.0, "avg_logprob": -0.09479899839921431, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.0012839070986956358}, {"id": 25, "seek": 15544, "start": 155.44, "end": 161.96, "text": " Now, to complicate things just a little bit, each of the connections are weighted,", "tokens": [50364, 823, 11, 281, 1209, 8700, 721, 445, 257, 707, 857, 11, 1184, 295, 264, 9271, 366, 32807, 11, 50690], "temperature": 0.0, "avg_logprob": -0.10796232612765565, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.015414895489811897}, {"id": 26, "seek": 15544, "start": 161.96, "end": 164.56, "text": " meaning they're multiplied by a number.", "tokens": [50690, 3620, 436, 434, 17207, 538, 257, 1230, 13, 50820], "temperature": 0.0, "avg_logprob": -0.10796232612765565, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.015414895489811897}, {"id": 27, "seek": 15544, "start": 164.56, "end": 170.24, "text": " That number can be one, or minus one, or anything in between.", "tokens": [50820, 663, 1230, 393, 312, 472, 11, 420, 3175, 472, 11, 420, 1340, 294, 1296, 13, 51104], "temperature": 0.0, "avg_logprob": -0.10796232612765565, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.015414895489811897}, {"id": 28, "seek": 15544, "start": 170.24, "end": 177.48, "text": " So for instance, if something has a weight of minus one, it's multiplied, and you get the negative of it, and that's added in.", "tokens": [51104, 407, 337, 5197, 11, 498, 746, 575, 257, 3364, 295, 3175, 472, 11, 309, 311, 17207, 11, 293, 291, 483, 264, 3671, 295, 309, 11, 293, 300, 311, 3869, 294, 13, 51466], "temperature": 0.0, "avg_logprob": -0.10796232612765565, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.015414895489811897}, {"id": 29, "seek": 15544, "start": 177.48, "end": 182.32, "text": " If something has a weight of zero, then it's effectively ignored.", "tokens": [51466, 759, 746, 575, 257, 3364, 295, 4018, 11, 550, 309, 311, 8659, 19735, 13, 51708], "temperature": 0.0, "avg_logprob": -0.10796232612765565, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.015414895489811897}, {"id": 30, "seek": 18232, "start": 182.32, "end": 185.51999999999998, "text": " So here's what those weighted connections might look like.", "tokens": [50364, 407, 510, 311, 437, 729, 32807, 9271, 1062, 574, 411, 13, 50524], "temperature": 0.0, "avg_logprob": -0.11246619591346153, "compression_ratio": 1.61139896373057, "no_speech_prob": 0.00043047594954259694}, {"id": 31, "seek": 18232, "start": 185.51999999999998, "end": 198.32, "text": " You'll notice that after the values of the input neurons are weighted and added, the final value is completely different.", "tokens": [50524, 509, 603, 3449, 300, 934, 264, 4190, 295, 264, 4846, 22027, 366, 32807, 293, 3869, 11, 264, 2572, 2158, 307, 2584, 819, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11246619591346153, "compression_ratio": 1.61139896373057, "no_speech_prob": 0.00043047594954259694}, {"id": 32, "seek": 18232, "start": 198.32, "end": 207.4, "text": " Graphically, it's convenient to represent these weights as white links being positive weights, black links being negative weights,", "tokens": [51164, 21884, 984, 11, 309, 311, 10851, 281, 2906, 613, 17443, 382, 2418, 6123, 885, 3353, 17443, 11, 2211, 6123, 885, 3671, 17443, 11, 51618], "temperature": 0.0, "avg_logprob": -0.11246619591346153, "compression_ratio": 1.61139896373057, "no_speech_prob": 0.00043047594954259694}, {"id": 33, "seek": 20740, "start": 207.4, "end": 216.4, "text": " and the thickness of the line is roughly proportional to the magnitude of the weight.", "tokens": [50364, 293, 264, 14855, 295, 264, 1622, 307, 9810, 24969, 281, 264, 15668, 295, 264, 3364, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14540530858414896, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.004069196991622448}, {"id": 34, "seek": 20740, "start": 216.4, "end": 222.76, "text": " Then after you add the weighted input neurons, they get squashed.", "tokens": [50814, 1396, 934, 291, 909, 264, 32807, 4846, 22027, 11, 436, 483, 2339, 12219, 13, 51132], "temperature": 0.0, "avg_logprob": -0.14540530858414896, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.004069196991622448}, {"id": 35, "seek": 20740, "start": 222.76, "end": 225.0, "text": " And I'll show you what that means.", "tokens": [51132, 400, 286, 603, 855, 291, 437, 300, 1355, 13, 51244], "temperature": 0.0, "avg_logprob": -0.14540530858414896, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.004069196991622448}, {"id": 36, "seek": 20740, "start": 225.0, "end": 227.72, "text": " You have a sigmoid squashing function.", "tokens": [51244, 509, 362, 257, 4556, 3280, 327, 2339, 11077, 2445, 13, 51380], "temperature": 0.0, "avg_logprob": -0.14540530858414896, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.004069196991622448}, {"id": 37, "seek": 20740, "start": 227.72, "end": 230.72, "text": " Sigmoid just means s-shaped.", "tokens": [51380, 37763, 3280, 327, 445, 1355, 262, 12, 23103, 13, 51530], "temperature": 0.0, "avg_logprob": -0.14540530858414896, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.004069196991622448}, {"id": 38, "seek": 20740, "start": 230.72, "end": 236.96, "text": " And what this does is you put a value in, let's say point five,", "tokens": [51530, 400, 437, 341, 775, 307, 291, 829, 257, 2158, 294, 11, 718, 311, 584, 935, 1732, 11, 51842], "temperature": 0.0, "avg_logprob": -0.14540530858414896, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.004069196991622448}, {"id": 39, "seek": 23696, "start": 236.96, "end": 243.92000000000002, "text": " and you run a vertical line up to your sigmoid, and then a horizontal line over from where it crosses.", "tokens": [50364, 293, 291, 1190, 257, 9429, 1622, 493, 281, 428, 4556, 3280, 327, 11, 293, 550, 257, 12750, 1622, 670, 490, 689, 309, 28467, 13, 50712], "temperature": 0.0, "avg_logprob": -0.13881295377557928, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.00628805672749877}, {"id": 40, "seek": 23696, "start": 243.92000000000002, "end": 248.16, "text": " And then where that hits the y-axis, that's the output of your function.", "tokens": [50712, 400, 550, 689, 300, 8664, 264, 288, 12, 24633, 11, 300, 311, 264, 5598, 295, 428, 2445, 13, 50924], "temperature": 0.0, "avg_logprob": -0.13881295377557928, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.00628805672749877}, {"id": 41, "seek": 23696, "start": 248.16, "end": 250.84, "text": " So in this case, slightly less than point five.", "tokens": [50924, 407, 294, 341, 1389, 11, 4748, 1570, 813, 935, 1732, 13, 51058], "temperature": 0.0, "avg_logprob": -0.13881295377557928, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.00628805672749877}, {"id": 42, "seek": 23696, "start": 250.84, "end": 253.56, "text": " It's pretty close.", "tokens": [51058, 467, 311, 1238, 1998, 13, 51194], "temperature": 0.0, "avg_logprob": -0.13881295377557928, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.00628805672749877}, {"id": 43, "seek": 23696, "start": 253.56, "end": 260.12, "text": " As your input number gets larger, your output number also gets larger, but more slowly.", "tokens": [51194, 1018, 428, 4846, 1230, 2170, 4833, 11, 428, 5598, 1230, 611, 2170, 4833, 11, 457, 544, 5692, 13, 51522], "temperature": 0.0, "avg_logprob": -0.13881295377557928, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.00628805672749877}, {"id": 44, "seek": 26012, "start": 260.12, "end": 267.88, "text": " And eventually, no matter how big the number you put in, the answer is always less than one.", "tokens": [50364, 400, 4728, 11, 572, 1871, 577, 955, 264, 1230, 291, 829, 294, 11, 264, 1867, 307, 1009, 1570, 813, 472, 13, 50752], "temperature": 0.0, "avg_logprob": -0.0942573764107444, "compression_ratio": 1.6287425149700598, "no_speech_prob": 0.07914090901613235}, {"id": 45, "seek": 26012, "start": 267.88, "end": 272.92, "text": " Similarly, when you go negative, the answer is always greater than negative one.", "tokens": [50752, 13157, 11, 562, 291, 352, 3671, 11, 264, 1867, 307, 1009, 5044, 813, 3671, 472, 13, 51004], "temperature": 0.0, "avg_logprob": -0.0942573764107444, "compression_ratio": 1.6287425149700598, "no_speech_prob": 0.07914090901613235}, {"id": 46, "seek": 26012, "start": 272.92, "end": 280.68, "text": " So this ensures that that neuron's value never gets outside of the range of plus one to minus one,", "tokens": [51004, 407, 341, 28111, 300, 300, 34090, 311, 2158, 1128, 2170, 2380, 295, 264, 3613, 295, 1804, 472, 281, 3175, 472, 11, 51392], "temperature": 0.0, "avg_logprob": -0.0942573764107444, "compression_ratio": 1.6287425149700598, "no_speech_prob": 0.07914090901613235}, {"id": 47, "seek": 28068, "start": 280.68, "end": 291.04, "text": " which is helpful for keeping the computations in the neural network bounded and stable.", "tokens": [50364, 597, 307, 4961, 337, 5145, 264, 2807, 763, 294, 264, 18161, 3209, 37498, 293, 8351, 13, 50882], "temperature": 0.0, "avg_logprob": -0.10202194849650065, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.1753188520669937}, {"id": 48, "seek": 28068, "start": 291.04, "end": 296.6, "text": " So after you sum the weighted values of the neurons and squash the result, you get the output.", "tokens": [50882, 407, 934, 291, 2408, 264, 32807, 4190, 295, 264, 22027, 293, 30725, 264, 1874, 11, 291, 483, 264, 5598, 13, 51160], "temperature": 0.0, "avg_logprob": -0.10202194849650065, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.1753188520669937}, {"id": 49, "seek": 28068, "start": 296.6, "end": 302.2, "text": " In this case, point seven, four, six, that is a neuron.", "tokens": [51160, 682, 341, 1389, 11, 935, 3407, 11, 1451, 11, 2309, 11, 300, 307, 257, 34090, 13, 51440], "temperature": 0.0, "avg_logprob": -0.10202194849650065, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.1753188520669937}, {"id": 50, "seek": 28068, "start": 302.2, "end": 309.36, "text": " So we can call this, we can collapse all that down, and this is a neuron that does a weighted sum and squash the result.", "tokens": [51440, 407, 321, 393, 818, 341, 11, 321, 393, 15584, 439, 300, 760, 11, 293, 341, 307, 257, 34090, 300, 775, 257, 32807, 2408, 293, 30725, 264, 1874, 13, 51798], "temperature": 0.0, "avg_logprob": -0.10202194849650065, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.1753188520669937}, {"id": 51, "seek": 30936, "start": 309.36, "end": 314.32, "text": " And now instead of just one of those, assume you have a whole bunch.", "tokens": [50364, 400, 586, 2602, 295, 445, 472, 295, 729, 11, 6552, 291, 362, 257, 1379, 3840, 13, 50612], "temperature": 0.0, "avg_logprob": -0.11834098100662231, "compression_ratio": 1.5458937198067633, "no_speech_prob": 0.004330027848482132}, {"id": 52, "seek": 30936, "start": 314.32, "end": 320.92, "text": " There are four shown here, but there could be four hundred or four million.", "tokens": [50612, 821, 366, 1451, 4898, 510, 11, 457, 456, 727, 312, 1451, 3262, 420, 1451, 2459, 13, 50942], "temperature": 0.0, "avg_logprob": -0.11834098100662231, "compression_ratio": 1.5458937198067633, "no_speech_prob": 0.004330027848482132}, {"id": 53, "seek": 30936, "start": 320.92, "end": 333.12, "text": " Now to keep our picture clear, we'll assume for now that the weights are either plus one, white lines, minus one, black lines, or zero, in which case they're missing entirely.", "tokens": [50942, 823, 281, 1066, 527, 3036, 1850, 11, 321, 603, 6552, 337, 586, 300, 264, 17443, 366, 2139, 1804, 472, 11, 2418, 3876, 11, 3175, 472, 11, 2211, 3876, 11, 420, 4018, 11, 294, 597, 1389, 436, 434, 5361, 7696, 13, 51552], "temperature": 0.0, "avg_logprob": -0.11834098100662231, "compression_ratio": 1.5458937198067633, "no_speech_prob": 0.004330027848482132}, {"id": 54, "seek": 33312, "start": 333.12, "end": 349.16, "text": " But in actuality, all of these neurons that we created are each attached to all of the input neurons, and they all have some weight between minus one and plus one.", "tokens": [50364, 583, 294, 3539, 507, 11, 439, 295, 613, 22027, 300, 321, 2942, 366, 1184, 8570, 281, 439, 295, 264, 4846, 22027, 11, 293, 436, 439, 362, 512, 3364, 1296, 3175, 472, 293, 1804, 472, 13, 51166], "temperature": 0.0, "avg_logprob": -0.11180140608448093, "compression_ratio": 1.5481927710843373, "no_speech_prob": 0.015167532488703728}, {"id": 55, "seek": 33312, "start": 349.16, "end": 355.88, "text": " When we create this first layer of our neural network, the receptive fields get more complex.", "tokens": [51166, 1133, 321, 1884, 341, 700, 4583, 295, 527, 18161, 3209, 11, 264, 45838, 7909, 483, 544, 3997, 13, 51502], "temperature": 0.0, "avg_logprob": -0.11180140608448093, "compression_ratio": 1.5481927710843373, "no_speech_prob": 0.015167532488703728}, {"id": 56, "seek": 35588, "start": 355.88, "end": 385.04, "text": " For instance, here, each of those end up combining two of our input neurons, and so the value, the receptive field, the pixel values that make that first layer neuron as large as it can possibly be, look now like pairs of pixels, either all white or a mixture of white and black, depending on the weights.", "tokens": [50364, 1171, 5197, 11, 510, 11, 1184, 295, 729, 917, 493, 21928, 732, 295, 527, 4846, 22027, 11, 293, 370, 264, 2158, 11, 264, 45838, 2519, 11, 264, 19261, 4190, 300, 652, 300, 700, 4583, 34090, 382, 2416, 382, 309, 393, 6264, 312, 11, 574, 586, 411, 15494, 295, 18668, 11, 2139, 439, 2418, 420, 257, 9925, 295, 2418, 293, 2211, 11, 5413, 322, 264, 17443, 13, 51822], "temperature": 0.0, "avg_logprob": -0.14008230481828962, "compression_ratio": 1.5721649484536082, "no_speech_prob": 0.02227088250219822}, {"id": 57, "seek": 38504, "start": 385.04, "end": 399.28000000000003, "text": " So for instance, this neuron here is attached to this input pixel, which is upper left, and this input pixel, which is lower left, and both of those weights are positive.", "tokens": [50364, 407, 337, 5197, 11, 341, 34090, 510, 307, 8570, 281, 341, 4846, 19261, 11, 597, 307, 6597, 1411, 11, 293, 341, 4846, 19261, 11, 597, 307, 3126, 1411, 11, 293, 1293, 295, 729, 17443, 366, 3353, 13, 51076], "temperature": 0.0, "avg_logprob": -0.09601132074991862, "compression_ratio": 1.9426751592356688, "no_speech_prob": 0.07358407974243164}, {"id": 58, "seek": 38504, "start": 399.28000000000003, "end": 407.32000000000005, "text": " So it combines the two of those, and that's its receptive field, the receptive field of this one plus the receptive field of this one.", "tokens": [51076, 407, 309, 29520, 264, 732, 295, 729, 11, 293, 300, 311, 1080, 45838, 2519, 11, 264, 45838, 2519, 295, 341, 472, 1804, 264, 45838, 2519, 295, 341, 472, 13, 51478], "temperature": 0.0, "avg_logprob": -0.09601132074991862, "compression_ratio": 1.9426751592356688, "no_speech_prob": 0.07358407974243164}, {"id": 59, "seek": 40732, "start": 407.32, "end": 418.8, "text": " However, if we look at this neuron, it combines our this pixel, upper right, and this pixel, lower right.", "tokens": [50364, 2908, 11, 498, 321, 574, 412, 341, 34090, 11, 309, 29520, 527, 341, 19261, 11, 6597, 558, 11, 293, 341, 19261, 11, 3126, 558, 13, 50938], "temperature": 0.0, "avg_logprob": -0.13592708110809326, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.13281454145908356}, {"id": 60, "seek": 40732, "start": 418.8, "end": 432.36, "text": " It has a weight of minus one for the lower right pixel, so that means it's most active when this pixel is black, so here is its receptive field.", "tokens": [50938, 467, 575, 257, 3364, 295, 3175, 472, 337, 264, 3126, 558, 19261, 11, 370, 300, 1355, 309, 311, 881, 4967, 562, 341, 19261, 307, 2211, 11, 370, 510, 307, 1080, 45838, 2519, 13, 51616], "temperature": 0.0, "avg_logprob": -0.13592708110809326, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.13281454145908356}, {"id": 61, "seek": 43236, "start": 432.36, "end": 452.88, "text": " Now, because we were careful of how we created that first layer, its values look a lot like input values, and we can turn right around and create another layer on top of it the exact same way with the output of one layer being the input to the next layer.", "tokens": [50364, 823, 11, 570, 321, 645, 5026, 295, 577, 321, 2942, 300, 700, 4583, 11, 1080, 4190, 574, 257, 688, 411, 4846, 4190, 11, 293, 321, 393, 1261, 558, 926, 293, 1884, 1071, 4583, 322, 1192, 295, 309, 264, 1900, 912, 636, 365, 264, 5598, 295, 472, 4583, 885, 264, 4846, 281, 264, 958, 4583, 13, 51390], "temperature": 0.0, "avg_logprob": -0.11252133320953886, "compression_ratio": 1.59375, "no_speech_prob": 0.02367229573428631}, {"id": 62, "seek": 45288, "start": 452.88, "end": 461.28, "text": " And we can repeat this three times or seven times or seven hundred times for additional layers.", "tokens": [50364, 400, 321, 393, 7149, 341, 1045, 1413, 420, 3407, 1413, 420, 3407, 3262, 1413, 337, 4497, 7914, 13, 50784], "temperature": 0.0, "avg_logprob": -0.14922637650460907, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.15166309475898743}, {"id": 63, "seek": 45288, "start": 461.28, "end": 479.92, "text": " Each time the receptive fields get even more complex, so you can see here using the same logic, now they cover all of the pixels and more, more special arrangement of which are black and which are white.", "tokens": [50784, 6947, 565, 264, 45838, 7909, 483, 754, 544, 3997, 11, 370, 291, 393, 536, 510, 1228, 264, 912, 9952, 11, 586, 436, 2060, 439, 295, 264, 18668, 293, 544, 11, 544, 2121, 17620, 295, 597, 366, 2211, 293, 597, 366, 2418, 13, 51716], "temperature": 0.0, "avg_logprob": -0.14922637650460907, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.15166309475898743}, {"id": 64, "seek": 47992, "start": 479.96000000000004, "end": 493.52000000000004, "text": " We can create another layer, again, all of these neurons in one layer are connected to all of the neurons in the previous layer, but we're assuming here that most of those weights are zero and not shown.", "tokens": [50366, 492, 393, 1884, 1071, 4583, 11, 797, 11, 439, 295, 613, 22027, 294, 472, 4583, 366, 4582, 281, 439, 295, 264, 22027, 294, 264, 3894, 4583, 11, 457, 321, 434, 11926, 510, 300, 881, 295, 729, 17443, 366, 4018, 293, 406, 4898, 13, 51044], "temperature": 0.0, "avg_logprob": -0.14143792788187662, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.21156544983386993}, {"id": 65, "seek": 47992, "start": 493.52000000000004, "end": 496.92, "text": " It's not generally the case.", "tokens": [51044, 467, 311, 406, 5101, 264, 1389, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14143792788187662, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.21156544983386993}, {"id": 66, "seek": 47992, "start": 496.92, "end": 506.88, "text": " So just to mix things up, we'll create a new layer, but if you notice our squashing function isn't there anymore, we have something new called a rectified linear unit.", "tokens": [51214, 407, 445, 281, 2890, 721, 493, 11, 321, 603, 1884, 257, 777, 4583, 11, 457, 498, 291, 3449, 527, 2339, 11077, 2445, 1943, 380, 456, 3602, 11, 321, 362, 746, 777, 1219, 257, 11048, 2587, 8213, 4985, 13, 51712], "temperature": 0.0, "avg_logprob": -0.14143792788187662, "compression_ratio": 1.6877637130801688, "no_speech_prob": 0.21156544983386993}, {"id": 67, "seek": 50688, "start": 506.88, "end": 511.24, "text": " This is another popular neuron type.", "tokens": [50364, 639, 307, 1071, 3743, 34090, 2010, 13, 50582], "temperature": 0.0, "avg_logprob": -0.13802541003507726, "compression_ratio": 1.5220125786163523, "no_speech_prob": 0.0018095191335305572}, {"id": 68, "seek": 50688, "start": 511.24, "end": 521.68, "text": " So you do your weighted sum of all your inputs, and instead of squashing, you do rectified linear units, you rectify it.", "tokens": [50582, 407, 291, 360, 428, 32807, 2408, 295, 439, 428, 15743, 11, 293, 2602, 295, 2339, 11077, 11, 291, 360, 11048, 2587, 8213, 6815, 11, 291, 11048, 2505, 309, 13, 51104], "temperature": 0.0, "avg_logprob": -0.13802541003507726, "compression_ratio": 1.5220125786163523, "no_speech_prob": 0.0018095191335305572}, {"id": 69, "seek": 50688, "start": 521.68, "end": 525.84, "text": " So if it is negative, you make the value zero.", "tokens": [51104, 407, 498, 309, 307, 3671, 11, 291, 652, 264, 2158, 4018, 13, 51312], "temperature": 0.0, "avg_logprob": -0.13802541003507726, "compression_ratio": 1.5220125786163523, "no_speech_prob": 0.0018095191335305572}, {"id": 70, "seek": 50688, "start": 525.84, "end": 529.12, "text": " If it's positive, you keep the value.", "tokens": [51312, 759, 309, 311, 3353, 11, 291, 1066, 264, 2158, 13, 51476], "temperature": 0.0, "avg_logprob": -0.13802541003507726, "compression_ratio": 1.5220125786163523, "no_speech_prob": 0.0018095191335305572}, {"id": 71, "seek": 52912, "start": 529.12, "end": 541.92, "text": " This is obviously very easy to compute, and it turns out to have very nice stability properties for neural networks as well in practice.", "tokens": [50364, 639, 307, 2745, 588, 1858, 281, 14722, 11, 293, 309, 4523, 484, 281, 362, 588, 1481, 11826, 7221, 337, 18161, 9590, 382, 731, 294, 3124, 13, 51004], "temperature": 0.0, "avg_logprob": -0.10601927104749177, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.003074543783441186}, {"id": 72, "seek": 52912, "start": 541.92, "end": 553.62, "text": " So after we do this, because some of our weights are positive and some are negative, connecting to those rectified linear units, we get receptive fields and their opposites.", "tokens": [51004, 407, 934, 321, 360, 341, 11, 570, 512, 295, 527, 17443, 366, 3353, 293, 512, 366, 3671, 11, 11015, 281, 729, 11048, 2587, 8213, 6815, 11, 321, 483, 45838, 7909, 293, 641, 4665, 3324, 13, 51589], "temperature": 0.0, "avg_logprob": -0.10601927104749177, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.003074543783441186}, {"id": 73, "seek": 52912, "start": 553.62, "end": 557.08, "text": " You look at the patterns there.", "tokens": [51589, 509, 574, 412, 264, 8294, 456, 13, 51762], "temperature": 0.0, "avg_logprob": -0.10601927104749177, "compression_ratio": 1.5688073394495412, "no_speech_prob": 0.003074543783441186}, {"id": 74, "seek": 55708, "start": 557.08, "end": 563.84, "text": " And then finally, when we've created as many layers with as many neurons as we want, we create an output layer.", "tokens": [50364, 400, 550, 2721, 11, 562, 321, 600, 2942, 382, 867, 7914, 365, 382, 867, 22027, 382, 321, 528, 11, 321, 1884, 364, 5598, 4583, 13, 50702], "temperature": 0.0, "avg_logprob": -0.09767755242281181, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.0015481095761060715}, {"id": 75, "seek": 55708, "start": 563.84, "end": 567.08, "text": " Here, we have four outputs that we're interested in.", "tokens": [50702, 1692, 11, 321, 362, 1451, 23930, 300, 321, 434, 3102, 294, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09767755242281181, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.0015481095761060715}, {"id": 76, "seek": 55708, "start": 567.08, "end": 575.0400000000001, "text": " Is the image solid, vertical, diagonal, or horizontal?", "tokens": [50864, 1119, 264, 3256, 5100, 11, 9429, 11, 21539, 11, 420, 12750, 30, 51262], "temperature": 0.0, "avg_logprob": -0.09767755242281181, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.0015481095761060715}, {"id": 77, "seek": 55708, "start": 575.0400000000001, "end": 583.9200000000001, "text": " So to walk through an example here of how this would work, let's say we start with this input image shown on the left.", "tokens": [51262, 407, 281, 1792, 807, 364, 1365, 510, 295, 577, 341, 576, 589, 11, 718, 311, 584, 321, 722, 365, 341, 4846, 3256, 4898, 322, 264, 1411, 13, 51706], "temperature": 0.0, "avg_logprob": -0.09767755242281181, "compression_ratio": 1.5648148148148149, "no_speech_prob": 0.0015481095761060715}, {"id": 78, "seek": 58392, "start": 583.9599999999999, "end": 586.88, "text": " Dark pixels on top, white on the bottom.", "tokens": [50366, 9563, 18668, 322, 1192, 11, 2418, 322, 264, 2767, 13, 50512], "temperature": 0.0, "avg_logprob": -0.13094705267797543, "compression_ratio": 1.6368715083798884, "no_speech_prob": 0.028850363567471504}, {"id": 79, "seek": 58392, "start": 586.88, "end": 593.5799999999999, "text": " As we propagate that to our input layer, this is what those values would look like.", "tokens": [50512, 1018, 321, 48256, 300, 281, 527, 4846, 4583, 11, 341, 307, 437, 729, 4190, 576, 574, 411, 13, 50847], "temperature": 0.0, "avg_logprob": -0.13094705267797543, "compression_ratio": 1.6368715083798884, "no_speech_prob": 0.028850363567471504}, {"id": 80, "seek": 58392, "start": 593.5799999999999, "end": 596.76, "text": " The top pixels, the bottom pixels.", "tokens": [50847, 440, 1192, 18668, 11, 264, 2767, 18668, 13, 51006], "temperature": 0.0, "avg_logprob": -0.13094705267797543, "compression_ratio": 1.6368715083798884, "no_speech_prob": 0.028850363567471504}, {"id": 81, "seek": 58392, "start": 596.76, "end": 611.62, "text": " As we move that to our first layer, we can see the combination of a dark pixel and a light pixel, summed together, get us zero, gray.", "tokens": [51006, 1018, 321, 1286, 300, 281, 527, 700, 4583, 11, 321, 393, 536, 264, 6562, 295, 257, 2877, 19261, 293, 257, 1442, 19261, 11, 2408, 1912, 1214, 11, 483, 505, 4018, 11, 10855, 13, 51749], "temperature": 0.0, "avg_logprob": -0.13094705267797543, "compression_ratio": 1.6368715083798884, "no_speech_prob": 0.028850363567471504}, {"id": 82, "seek": 61162, "start": 611.62, "end": 618.72, "text": " Whereas down here, we have the combination of a dark pixel plus a light pixel with a negative weight.", "tokens": [50364, 13813, 760, 510, 11, 321, 362, 264, 6562, 295, 257, 2877, 19261, 1804, 257, 1442, 19261, 365, 257, 3671, 3364, 13, 50719], "temperature": 0.0, "avg_logprob": -0.13378569954320005, "compression_ratio": 1.6358974358974359, "no_speech_prob": 0.00034595828037709}, {"id": 83, "seek": 61162, "start": 618.72, "end": 630.66, "text": " So that gets us a value of negative one there, which makes sense because if we look at the receptive field here, upper left pixel white, lower left pixel black,", "tokens": [50719, 407, 300, 2170, 505, 257, 2158, 295, 3671, 472, 456, 11, 597, 1669, 2020, 570, 498, 321, 574, 412, 264, 45838, 2519, 510, 11, 6597, 1411, 19261, 2418, 11, 3126, 1411, 19261, 2211, 11, 51316], "temperature": 0.0, "avg_logprob": -0.13378569954320005, "compression_ratio": 1.6358974358974359, "no_speech_prob": 0.00034595828037709}, {"id": 84, "seek": 61162, "start": 630.66, "end": 635.02, "text": " it's the exact opposite of the input that we're getting.", "tokens": [51316, 309, 311, 264, 1900, 6182, 295, 264, 4846, 300, 321, 434, 1242, 13, 51534], "temperature": 0.0, "avg_logprob": -0.13378569954320005, "compression_ratio": 1.6358974358974359, "no_speech_prob": 0.00034595828037709}, {"id": 85, "seek": 63502, "start": 635.02, "end": 642.3199999999999, "text": " And so we would expect its value to be as low as possible, minus one.", "tokens": [50364, 400, 370, 321, 576, 2066, 1080, 2158, 281, 312, 382, 2295, 382, 1944, 11, 3175, 472, 13, 50729], "temperature": 0.0, "avg_logprob": -0.17595516016453872, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0013247968163341284}, {"id": 86, "seek": 63502, "start": 642.3199999999999, "end": 654.52, "text": " As we move to the next layer, we see the same types of things, combining zeros to get zeros, combining a negative and a negative with a negative weight,", "tokens": [50729, 1018, 321, 1286, 281, 264, 958, 4583, 11, 321, 536, 264, 912, 3467, 295, 721, 11, 21928, 35193, 281, 483, 35193, 11, 21928, 257, 3671, 293, 257, 3671, 365, 257, 3671, 3364, 11, 51339], "temperature": 0.0, "avg_logprob": -0.17595516016453872, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0013247968163341284}, {"id": 87, "seek": 63502, "start": 654.52, "end": 657.96, "text": " which makes a positive to get a zero.", "tokens": [51339, 597, 1669, 257, 3353, 281, 483, 257, 4018, 13, 51511], "temperature": 0.0, "avg_logprob": -0.17595516016453872, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0013247968163341284}, {"id": 88, "seek": 63502, "start": 657.96, "end": 661.78, "text": " And here we have combining two negatives to get a negative.", "tokens": [51511, 400, 510, 321, 362, 21928, 732, 40019, 281, 483, 257, 3671, 13, 51702], "temperature": 0.0, "avg_logprob": -0.17595516016453872, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0013247968163341284}, {"id": 89, "seek": 66178, "start": 661.78, "end": 666.88, "text": " So again, you'll notice the receptive field of this is exactly the inverse of our input.", "tokens": [50364, 407, 797, 11, 291, 603, 3449, 264, 45838, 2519, 295, 341, 307, 2293, 264, 17340, 295, 527, 4846, 13, 50619], "temperature": 0.0, "avg_logprob": -0.1518738068729998, "compression_ratio": 1.6180904522613064, "no_speech_prob": 0.0007095560431480408}, {"id": 90, "seek": 66178, "start": 666.88, "end": 674.4399999999999, "text": " So it makes sense that its weight would be negative, or its value would be negative.", "tokens": [50619, 407, 309, 1669, 2020, 300, 1080, 3364, 576, 312, 3671, 11, 420, 1080, 2158, 576, 312, 3671, 13, 50997], "temperature": 0.0, "avg_logprob": -0.1518738068729998, "compression_ratio": 1.6180904522613064, "no_speech_prob": 0.0007095560431480408}, {"id": 91, "seek": 66178, "start": 674.4399999999999, "end": 677.0799999999999, "text": " And we move to the next layer.", "tokens": [50997, 400, 321, 1286, 281, 264, 958, 4583, 13, 51129], "temperature": 0.0, "avg_logprob": -0.1518738068729998, "compression_ratio": 1.6180904522613064, "no_speech_prob": 0.0007095560431480408}, {"id": 92, "seek": 66178, "start": 677.0799999999999, "end": 681.28, "text": " All of these, of course, these zeros propagate forward.", "tokens": [51129, 1057, 295, 613, 11, 295, 1164, 11, 613, 35193, 48256, 2128, 13, 51339], "temperature": 0.0, "avg_logprob": -0.1518738068729998, "compression_ratio": 1.6180904522613064, "no_speech_prob": 0.0007095560431480408}, {"id": 93, "seek": 66178, "start": 681.28, "end": 687.88, "text": " Here, this has a negative value and it has a positive weight.", "tokens": [51339, 1692, 11, 341, 575, 257, 3671, 2158, 293, 309, 575, 257, 3353, 3364, 13, 51669], "temperature": 0.0, "avg_logprob": -0.1518738068729998, "compression_ratio": 1.6180904522613064, "no_speech_prob": 0.0007095560431480408}, {"id": 94, "seek": 68788, "start": 687.88, "end": 694.9399999999999, "text": " So it just moves straight forward because we have a rectified linear unit, negative values become zero.", "tokens": [50364, 407, 309, 445, 6067, 2997, 2128, 570, 321, 362, 257, 11048, 2587, 8213, 4985, 11, 3671, 4190, 1813, 4018, 13, 50717], "temperature": 0.0, "avg_logprob": -0.13902131793568434, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.028423922136425972}, {"id": 95, "seek": 68788, "start": 694.9399999999999, "end": 702.24, "text": " So now it is zero again, two, but this one gets rectified and becomes positive, negative times the negative is positive.", "tokens": [50717, 407, 586, 309, 307, 4018, 797, 11, 732, 11, 457, 341, 472, 2170, 11048, 2587, 293, 3643, 3353, 11, 3671, 1413, 264, 3671, 307, 3353, 13, 51082], "temperature": 0.0, "avg_logprob": -0.13902131793568434, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.028423922136425972}, {"id": 96, "seek": 68788, "start": 702.24, "end": 708.74, "text": " And so when we finally get to the output, we can see they're all zero except for this horizontal, which is positive.", "tokens": [51082, 400, 370, 562, 321, 2721, 483, 281, 264, 5598, 11, 321, 393, 536, 436, 434, 439, 4018, 3993, 337, 341, 12750, 11, 597, 307, 3353, 13, 51407], "temperature": 0.0, "avg_logprob": -0.13902131793568434, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.028423922136425972}, {"id": 97, "seek": 68788, "start": 708.74, "end": 709.9399999999999, "text": " And that's the answer.", "tokens": [51407, 400, 300, 311, 264, 1867, 13, 51467], "temperature": 0.0, "avg_logprob": -0.13902131793568434, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.028423922136425972}, {"id": 98, "seek": 68788, "start": 709.9399999999999, "end": 717.78, "text": " Our neural network said this is an image of a horizontal line.", "tokens": [51467, 2621, 18161, 3209, 848, 341, 307, 364, 3256, 295, 257, 12750, 1622, 13, 51859], "temperature": 0.0, "avg_logprob": -0.13902131793568434, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.028423922136425972}, {"id": 99, "seek": 71778, "start": 717.8, "end": 722.3, "text": " Now, neural networks usually aren't that good, not that clean.", "tokens": [50365, 823, 11, 18161, 9590, 2673, 3212, 380, 300, 665, 11, 406, 300, 2541, 13, 50590], "temperature": 0.0, "avg_logprob": -0.12724044799804687, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0005192090175114572}, {"id": 100, "seek": 71778, "start": 722.3, "end": 726.88, "text": " So there's a notion of, with an input, what is truth?", "tokens": [50590, 407, 456, 311, 257, 10710, 295, 11, 365, 364, 4846, 11, 437, 307, 3494, 30, 50819], "temperature": 0.0, "avg_logprob": -0.12724044799804687, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0005192090175114572}, {"id": 101, "seek": 71778, "start": 726.88, "end": 733.6999999999999, "text": " In this case, the truth is this has a zero for all of these values, but a one for horizontal.", "tokens": [50819, 682, 341, 1389, 11, 264, 3494, 307, 341, 575, 257, 4018, 337, 439, 295, 613, 4190, 11, 457, 257, 472, 337, 12750, 13, 51160], "temperature": 0.0, "avg_logprob": -0.12724044799804687, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0005192090175114572}, {"id": 102, "seek": 71778, "start": 733.6999999999999, "end": 736.5799999999999, "text": " It's not solid, it's not vertical, it's not diagonal.", "tokens": [51160, 467, 311, 406, 5100, 11, 309, 311, 406, 9429, 11, 309, 311, 406, 21539, 13, 51304], "temperature": 0.0, "avg_logprob": -0.12724044799804687, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0005192090175114572}, {"id": 103, "seek": 71778, "start": 736.5799999999999, "end": 739.64, "text": " Yes, it is horizontal.", "tokens": [51304, 1079, 11, 309, 307, 12750, 13, 51457], "temperature": 0.0, "avg_logprob": -0.12724044799804687, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0005192090175114572}, {"id": 104, "seek": 71778, "start": 739.64, "end": 744.6, "text": " An arbitrary neural network will give answers that are not exactly truth.", "tokens": [51457, 1107, 23211, 18161, 3209, 486, 976, 6338, 300, 366, 406, 2293, 3494, 13, 51705], "temperature": 0.0, "avg_logprob": -0.12724044799804687, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0005192090175114572}, {"id": 105, "seek": 74460, "start": 744.6, "end": 747.9200000000001, "text": " It might be off by a little or a lot.", "tokens": [50364, 467, 1062, 312, 766, 538, 257, 707, 420, 257, 688, 13, 50530], "temperature": 0.0, "avg_logprob": -0.11205371809594425, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003943375311791897}, {"id": 106, "seek": 74460, "start": 747.9200000000001, "end": 755.16, "text": " And then the error is the magnitude of the difference between the truth and the answer given.", "tokens": [50530, 400, 550, 264, 6713, 307, 264, 15668, 295, 264, 2649, 1296, 264, 3494, 293, 264, 1867, 2212, 13, 50892], "temperature": 0.0, "avg_logprob": -0.11205371809594425, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003943375311791897}, {"id": 107, "seek": 74460, "start": 755.16, "end": 761.02, "text": " And you can add all these up to get the total error for the neural network.", "tokens": [50892, 400, 291, 393, 909, 439, 613, 493, 281, 483, 264, 3217, 6713, 337, 264, 18161, 3209, 13, 51185], "temperature": 0.0, "avg_logprob": -0.11205371809594425, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003943375311791897}, {"id": 108, "seek": 74460, "start": 761.02, "end": 771.82, "text": " So the idea, the whole idea with learning and training is to adjust the weights to make the error as low as possible.", "tokens": [51185, 407, 264, 1558, 11, 264, 1379, 1558, 365, 2539, 293, 3097, 307, 281, 4369, 264, 17443, 281, 652, 264, 6713, 382, 2295, 382, 1944, 13, 51725], "temperature": 0.0, "avg_logprob": -0.11205371809594425, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003943375311791897}, {"id": 109, "seek": 77182, "start": 771.82, "end": 785.98, "text": " So the way this is done is we put an image in, we calculate the error at the end, then we look for how to adjust those weights higher or lower to either make that error go up or down.", "tokens": [50364, 407, 264, 636, 341, 307, 1096, 307, 321, 829, 364, 3256, 294, 11, 321, 8873, 264, 6713, 412, 264, 917, 11, 550, 321, 574, 337, 577, 281, 4369, 729, 17443, 2946, 420, 3126, 281, 2139, 652, 300, 6713, 352, 493, 420, 760, 13, 51072], "temperature": 0.0, "avg_logprob": -0.14870376398067664, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0013247656170278788}, {"id": 110, "seek": 77182, "start": 785.98, "end": 791.24, "text": " And we, of course, adjust the weights in the way, then make the error go down.", "tokens": [51072, 400, 321, 11, 295, 1164, 11, 4369, 264, 17443, 294, 264, 636, 11, 550, 652, 264, 6713, 352, 760, 13, 51335], "temperature": 0.0, "avg_logprob": -0.14870376398067664, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0013247656170278788}, {"id": 111, "seek": 77182, "start": 791.24, "end": 801.46, "text": " Now, the problem with doing this is each time we go back and calculate the error, we have to multiply all of those weights by all of the", "tokens": [51335, 823, 11, 264, 1154, 365, 884, 341, 307, 1184, 565, 321, 352, 646, 293, 8873, 264, 6713, 11, 321, 362, 281, 12972, 439, 295, 729, 17443, 538, 439, 295, 264, 51846], "temperature": 0.0, "avg_logprob": -0.14870376398067664, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.0013247656170278788}, {"id": 112, "seek": 80146, "start": 801.5, "end": 804.5400000000001, "text": " neuron values at each layer.", "tokens": [50366, 34090, 4190, 412, 1184, 4583, 13, 50518], "temperature": 0.0, "avg_logprob": -0.16028514985115297, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.023665428161621094}, {"id": 113, "seek": 80146, "start": 804.5400000000001, "end": 808.44, "text": " And we have to do that again and again once for each weight.", "tokens": [50518, 400, 321, 362, 281, 360, 300, 797, 293, 797, 1564, 337, 1184, 3364, 13, 50713], "temperature": 0.0, "avg_logprob": -0.16028514985115297, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.023665428161621094}, {"id": 114, "seek": 80146, "start": 808.44, "end": 813.88, "text": " This takes forever in computing terms, on a computing scale.", "tokens": [50713, 639, 2516, 5680, 294, 15866, 2115, 11, 322, 257, 15866, 4373, 13, 50985], "temperature": 0.0, "avg_logprob": -0.16028514985115297, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.023665428161621094}, {"id": 115, "seek": 80146, "start": 813.88, "end": 819.24, "text": " And so it's not a practical way to train a big neural network.", "tokens": [50985, 400, 370, 309, 311, 406, 257, 8496, 636, 281, 3847, 257, 955, 18161, 3209, 13, 51253], "temperature": 0.0, "avg_logprob": -0.16028514985115297, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.023665428161621094}, {"id": 116, "seek": 80146, "start": 819.24, "end": 823.44, "text": " You can imagine instead of just rolling down to the bottom of a simple valley, we have", "tokens": [51253, 509, 393, 3811, 2602, 295, 445, 9439, 760, 281, 264, 2767, 295, 257, 2199, 17636, 11, 321, 362, 51463], "temperature": 0.0, "avg_logprob": -0.16028514985115297, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.023665428161621094}, {"id": 117, "seek": 80146, "start": 823.44, "end": 827.9000000000001, "text": " a very high dimensional valley and we have to find our way down.", "tokens": [51463, 257, 588, 1090, 18795, 17636, 293, 321, 362, 281, 915, 527, 636, 760, 13, 51686], "temperature": 0.0, "avg_logprob": -0.16028514985115297, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.023665428161621094}, {"id": 118, "seek": 82790, "start": 827.9, "end": 832.78, "text": " And because there are so many dimensions, one for each of these weights, that the computation", "tokens": [50364, 400, 570, 456, 366, 370, 867, 12819, 11, 472, 337, 1184, 295, 613, 17443, 11, 300, 264, 24903, 50608], "temperature": 0.0, "avg_logprob": -0.15884344918387278, "compression_ratio": 1.640625, "no_speech_prob": 0.0015009507769718766}, {"id": 119, "seek": 82790, "start": 832.78, "end": 841.9399999999999, "text": " just becomes prohibitively expensive, luckily there was an insight that lets us do this in", "tokens": [50608, 445, 3643, 16015, 2187, 356, 5124, 11, 22880, 456, 390, 364, 11269, 300, 6653, 505, 360, 341, 294, 51066], "temperature": 0.0, "avg_logprob": -0.15884344918387278, "compression_ratio": 1.640625, "no_speech_prob": 0.0015009507769718766}, {"id": 120, "seek": 82790, "start": 841.9399999999999, "end": 844.16, "text": " a very reasonable time.", "tokens": [51066, 257, 588, 10585, 565, 13, 51177], "temperature": 0.0, "avg_logprob": -0.15884344918387278, "compression_ratio": 1.640625, "no_speech_prob": 0.0015009507769718766}, {"id": 121, "seek": 82790, "start": 844.16, "end": 849.06, "text": " And that's that if we're careful about how we design our neural network, we can calculate", "tokens": [51177, 400, 300, 311, 300, 498, 321, 434, 5026, 466, 577, 321, 1715, 527, 18161, 3209, 11, 321, 393, 8873, 51422], "temperature": 0.0, "avg_logprob": -0.15884344918387278, "compression_ratio": 1.640625, "no_speech_prob": 0.0015009507769718766}, {"id": 122, "seek": 82790, "start": 849.06, "end": 852.02, "text": " the slope directly, the gradient.", "tokens": [51422, 264, 13525, 3838, 11, 264, 16235, 13, 51570], "temperature": 0.0, "avg_logprob": -0.15884344918387278, "compression_ratio": 1.640625, "no_speech_prob": 0.0015009507769718766}, {"id": 123, "seek": 82790, "start": 852.02, "end": 857.06, "text": " We can figure out the direction that we need to adjust the weight without going all the", "tokens": [51570, 492, 393, 2573, 484, 264, 3513, 300, 321, 643, 281, 4369, 264, 3364, 1553, 516, 439, 264, 51822], "temperature": 0.0, "avg_logprob": -0.15884344918387278, "compression_ratio": 1.640625, "no_speech_prob": 0.0015009507769718766}, {"id": 124, "seek": 85706, "start": 857.06, "end": 862.7399999999999, "text": " way back through our neural network and recalculating.", "tokens": [50364, 636, 646, 807, 527, 18161, 3209, 293, 850, 304, 2444, 990, 13, 50648], "temperature": 0.0, "avg_logprob": -0.17687698364257812, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0010002610506489873}, {"id": 125, "seek": 85706, "start": 862.7399999999999, "end": 868.9799999999999, "text": " So just to review, the slope that we're talking about is when we make a change in weight,", "tokens": [50648, 407, 445, 281, 3131, 11, 264, 13525, 300, 321, 434, 1417, 466, 307, 562, 321, 652, 257, 1319, 294, 3364, 11, 50960], "temperature": 0.0, "avg_logprob": -0.17687698364257812, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0010002610506489873}, {"id": 126, "seek": 85706, "start": 868.9799999999999, "end": 871.6199999999999, "text": " the error will change a little bit.", "tokens": [50960, 264, 6713, 486, 1319, 257, 707, 857, 13, 51092], "temperature": 0.0, "avg_logprob": -0.17687698364257812, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0010002610506489873}, {"id": 127, "seek": 85706, "start": 871.6199999999999, "end": 878.38, "text": " And that relation of the change in weight to the change in error is the slope.", "tokens": [51092, 400, 300, 9721, 295, 264, 1319, 294, 3364, 281, 264, 1319, 294, 6713, 307, 264, 13525, 13, 51430], "temperature": 0.0, "avg_logprob": -0.17687698364257812, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0010002610506489873}, {"id": 128, "seek": 85706, "start": 878.38, "end": 882.1199999999999, "text": " Mathematically, there are several ways to write this.", "tokens": [51430, 15776, 40197, 11, 456, 366, 2940, 2098, 281, 2464, 341, 13, 51617], "temperature": 0.0, "avg_logprob": -0.17687698364257812, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0010002610506489873}, {"id": 129, "seek": 85706, "start": 882.1199999999999, "end": 883.52, "text": " We'll favor the one on the bottom.", "tokens": [51617, 492, 603, 2294, 264, 472, 322, 264, 2767, 13, 51687], "temperature": 0.0, "avg_logprob": -0.17687698364257812, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0010002610506489873}, {"id": 130, "seek": 85706, "start": 883.52, "end": 885.76, "text": " It's technically most correct.", "tokens": [51687, 467, 311, 12120, 881, 3006, 13, 51799], "temperature": 0.0, "avg_logprob": -0.17687698364257812, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0010002610506489873}, {"id": 131, "seek": 88576, "start": 885.76, "end": 888.96, "text": " We'll call it DEDW for short hand.", "tokens": [50364, 492, 603, 818, 309, 413, 4731, 54, 337, 2099, 1011, 13, 50524], "temperature": 0.0, "avg_logprob": -0.14234605489992627, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.002396239899098873}, {"id": 132, "seek": 88576, "start": 888.96, "end": 896.76, "text": " Every time you see it, just think the change in error when I change a weight or the change", "tokens": [50524, 2048, 565, 291, 536, 309, 11, 445, 519, 264, 1319, 294, 6713, 562, 286, 1319, 257, 3364, 420, 264, 1319, 50914], "temperature": 0.0, "avg_logprob": -0.14234605489992627, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.002396239899098873}, {"id": 133, "seek": 88576, "start": 896.76, "end": 901.52, "text": " in the thing on the top when I change the thing on the bottom.", "tokens": [50914, 294, 264, 551, 322, 264, 1192, 562, 286, 1319, 264, 551, 322, 264, 2767, 13, 51152], "temperature": 0.0, "avg_logprob": -0.14234605489992627, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.002396239899098873}, {"id": 134, "seek": 88576, "start": 901.52, "end": 905.24, "text": " This does get into a little bit of calculus.", "tokens": [51152, 639, 775, 483, 666, 257, 707, 857, 295, 33400, 13, 51338], "temperature": 0.0, "avg_logprob": -0.14234605489992627, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.002396239899098873}, {"id": 135, "seek": 88576, "start": 905.24, "end": 906.24, "text": " We do take derivatives.", "tokens": [51338, 492, 360, 747, 33733, 13, 51388], "temperature": 0.0, "avg_logprob": -0.14234605489992627, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.002396239899098873}, {"id": 136, "seek": 88576, "start": 906.24, "end": 908.68, "text": " That's how we calculate slope.", "tokens": [51388, 663, 311, 577, 321, 8873, 13525, 13, 51510], "temperature": 0.0, "avg_logprob": -0.14234605489992627, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.002396239899098873}, {"id": 137, "seek": 88576, "start": 908.68, "end": 914.24, "text": " If it's new to you, I strongly recommend a good semester of calculus just because the", "tokens": [51510, 759, 309, 311, 777, 281, 291, 11, 286, 10613, 2748, 257, 665, 11894, 295, 33400, 445, 570, 264, 51788], "temperature": 0.0, "avg_logprob": -0.14234605489992627, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.002396239899098873}, {"id": 138, "seek": 91424, "start": 914.24, "end": 918.2, "text": " concepts are so universal.", "tokens": [50364, 10392, 366, 370, 11455, 13, 50562], "temperature": 0.0, "avg_logprob": -0.12259269017045216, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.002050398848950863}, {"id": 139, "seek": 91424, "start": 918.2, "end": 922.84, "text": " A lot of them have very nice physical interpretations, which I find very appealing.", "tokens": [50562, 316, 688, 295, 552, 362, 588, 1481, 4001, 37547, 11, 597, 286, 915, 588, 23842, 13, 50794], "temperature": 0.0, "avg_logprob": -0.12259269017045216, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.002050398848950863}, {"id": 140, "seek": 91424, "start": 922.84, "end": 927.72, "text": " But don't worry, otherwise just gloss over this and pay attention to the rest and you'll", "tokens": [50794, 583, 500, 380, 3292, 11, 5911, 445, 19574, 670, 341, 293, 1689, 3202, 281, 264, 1472, 293, 291, 603, 51038], "temperature": 0.0, "avg_logprob": -0.12259269017045216, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.002050398848950863}, {"id": 141, "seek": 91424, "start": 927.72, "end": 931.8, "text": " get a general sense for how this works.", "tokens": [51038, 483, 257, 2674, 2020, 337, 577, 341, 1985, 13, 51242], "temperature": 0.0, "avg_logprob": -0.12259269017045216, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.002050398848950863}, {"id": 142, "seek": 91424, "start": 931.8, "end": 936.5600000000001, "text": " So in this case, if we change the weight by plus one, the error changes by minus two,", "tokens": [51242, 407, 294, 341, 1389, 11, 498, 321, 1319, 264, 3364, 538, 1804, 472, 11, 264, 6713, 2962, 538, 3175, 732, 11, 51480], "temperature": 0.0, "avg_logprob": -0.12259269017045216, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.002050398848950863}, {"id": 143, "seek": 91424, "start": 936.5600000000001, "end": 939.98, "text": " which gives us a slope of minus two.", "tokens": [51480, 597, 2709, 505, 257, 13525, 295, 3175, 732, 13, 51651], "temperature": 0.0, "avg_logprob": -0.12259269017045216, "compression_ratio": 1.567099567099567, "no_speech_prob": 0.002050398848950863}, {"id": 144, "seek": 93998, "start": 939.98, "end": 944.7, "text": " That tells us the direction that we should adjust our weight and how much we should adjust", "tokens": [50364, 663, 5112, 505, 264, 3513, 300, 321, 820, 4369, 527, 3364, 293, 577, 709, 321, 820, 4369, 50600], "temperature": 0.0, "avg_logprob": -0.17756230223412608, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0001535582559881732}, {"id": 145, "seek": 93998, "start": 944.7, "end": 949.46, "text": " it to bring the error down.", "tokens": [50600, 309, 281, 1565, 264, 6713, 760, 13, 50838], "temperature": 0.0, "avg_logprob": -0.17756230223412608, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0001535582559881732}, {"id": 146, "seek": 93998, "start": 949.46, "end": 952.58, "text": " Now to do this, you have to know what your error function is.", "tokens": [50838, 823, 281, 360, 341, 11, 291, 362, 281, 458, 437, 428, 6713, 2445, 307, 13, 50994], "temperature": 0.0, "avg_logprob": -0.17756230223412608, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0001535582559881732}, {"id": 147, "seek": 93998, "start": 952.58, "end": 958.14, "text": " So assume we had an error function that was the square of the weight, and you can see that", "tokens": [50994, 407, 6552, 321, 632, 364, 6713, 2445, 300, 390, 264, 3732, 295, 264, 3364, 11, 293, 291, 393, 536, 300, 51272], "temperature": 0.0, "avg_logprob": -0.17756230223412608, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0001535582559881732}, {"id": 148, "seek": 93998, "start": 958.14, "end": 962.54, "text": " our weight is right at minus one.", "tokens": [51272, 527, 3364, 307, 558, 412, 3175, 472, 13, 51492], "temperature": 0.0, "avg_logprob": -0.17756230223412608, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0001535582559881732}, {"id": 149, "seek": 93998, "start": 962.54, "end": 967.46, "text": " So the first thing we do is we take the derivative, change in error, divided by change in weight", "tokens": [51492, 407, 264, 700, 551, 321, 360, 307, 321, 747, 264, 13760, 11, 1319, 294, 6713, 11, 6666, 538, 1319, 294, 3364, 51738], "temperature": 0.0, "avg_logprob": -0.17756230223412608, "compression_ratio": 1.8108108108108107, "no_speech_prob": 0.0001535582559881732}, {"id": 150, "seek": 96746, "start": 967.46, "end": 969.46, "text": " dE dW.", "tokens": [50364, 274, 36, 274, 54, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1330002840827493, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.0075754439458251}, {"id": 151, "seek": 96746, "start": 969.46, "end": 973.3000000000001, "text": " The derivative of weight squared is two times the weight.", "tokens": [50464, 440, 13760, 295, 3364, 8889, 307, 732, 1413, 264, 3364, 13, 50656], "temperature": 0.0, "avg_logprob": -0.1330002840827493, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.0075754439458251}, {"id": 152, "seek": 96746, "start": 973.3000000000001, "end": 982.38, "text": " And so we plug in our weight of minus one and we get a slope dE dW of minus two.", "tokens": [50656, 400, 370, 321, 5452, 294, 527, 3364, 295, 3175, 472, 293, 321, 483, 257, 13525, 274, 36, 274, 54, 295, 3175, 732, 13, 51110], "temperature": 0.0, "avg_logprob": -0.1330002840827493, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.0075754439458251}, {"id": 153, "seek": 96746, "start": 982.38, "end": 987.58, "text": " Now the other trick that lets us do this with deep neural networks is chaining.", "tokens": [51110, 823, 264, 661, 4282, 300, 6653, 505, 360, 341, 365, 2452, 18161, 9590, 307, 417, 3686, 13, 51370], "temperature": 0.0, "avg_logprob": -0.1330002840827493, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.0075754439458251}, {"id": 154, "seek": 96746, "start": 987.58, "end": 992.6600000000001, "text": " And to show you how this works, imagine a very simple trivial neural network with just", "tokens": [51370, 400, 281, 855, 291, 577, 341, 1985, 11, 3811, 257, 588, 2199, 26703, 18161, 3209, 365, 445, 51624], "temperature": 0.0, "avg_logprob": -0.1330002840827493, "compression_ratio": 1.583756345177665, "no_speech_prob": 0.0075754439458251}, {"id": 155, "seek": 99266, "start": 992.86, "end": 999.62, "text": " one hidden layer, one input layer, one output layer, and one weight connecting each of them.", "tokens": [50374, 472, 7633, 4583, 11, 472, 4846, 4583, 11, 472, 5598, 4583, 11, 293, 472, 3364, 11015, 1184, 295, 552, 13, 50712], "temperature": 0.0, "avg_logprob": -0.09718575320401035, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.006902460940182209}, {"id": 156, "seek": 99266, "start": 999.62, "end": 1005.4599999999999, "text": " So it's obvious to see that the value y is just the value x times the weight connecting", "tokens": [50712, 407, 309, 311, 6322, 281, 536, 300, 264, 2158, 288, 307, 445, 264, 2158, 2031, 1413, 264, 3364, 11015, 51004], "temperature": 0.0, "avg_logprob": -0.09718575320401035, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.006902460940182209}, {"id": 157, "seek": 99266, "start": 1005.4599999999999, "end": 1008.9, "text": " them, w1.", "tokens": [51004, 552, 11, 261, 16, 13, 51176], "temperature": 0.0, "avg_logprob": -0.09718575320401035, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.006902460940182209}, {"id": 158, "seek": 99266, "start": 1008.9, "end": 1014.42, "text": " So if we change w1 a little bit, we just take the derivative of y with respect to w1, and", "tokens": [51176, 407, 498, 321, 1319, 261, 16, 257, 707, 857, 11, 321, 445, 747, 264, 13760, 295, 288, 365, 3104, 281, 261, 16, 11, 293, 51452], "temperature": 0.0, "avg_logprob": -0.09718575320401035, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.006902460940182209}, {"id": 159, "seek": 99266, "start": 1014.42, "end": 1015.42, "text": " we get x.", "tokens": [51452, 321, 483, 2031, 13, 51502], "temperature": 0.0, "avg_logprob": -0.09718575320401035, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.006902460940182209}, {"id": 160, "seek": 99266, "start": 1015.42, "end": 1016.42, "text": " The slope is x.", "tokens": [51502, 440, 13525, 307, 2031, 13, 51552], "temperature": 0.0, "avg_logprob": -0.09718575320401035, "compression_ratio": 1.6906077348066297, "no_speech_prob": 0.006902460940182209}, {"id": 161, "seek": 101642, "start": 1016.42, "end": 1023.42, "text": " If I change w1 by a little bit, then y will change by x times the size of that adjustment.", "tokens": [50364, 759, 286, 1319, 261, 16, 538, 257, 707, 857, 11, 550, 288, 486, 1319, 538, 2031, 1413, 264, 2744, 295, 300, 17132, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1390223503112793, "compression_ratio": 1.56, "no_speech_prob": 0.06557880342006683}, {"id": 162, "seek": 101642, "start": 1023.42, "end": 1032.18, "text": " Similarly, for the next step, we can see that E is just the value y times the weight w2.", "tokens": [50714, 13157, 11, 337, 264, 958, 1823, 11, 321, 393, 536, 300, 462, 307, 445, 264, 2158, 288, 1413, 264, 3364, 261, 17, 13, 51152], "temperature": 0.0, "avg_logprob": -0.1390223503112793, "compression_ratio": 1.56, "no_speech_prob": 0.06557880342006683}, {"id": 163, "seek": 101642, "start": 1032.18, "end": 1038.3799999999999, "text": " And so when we calculate dE dy, it's just w2.", "tokens": [51152, 400, 370, 562, 321, 8873, 274, 36, 14584, 11, 309, 311, 445, 261, 17, 13, 51462], "temperature": 0.0, "avg_logprob": -0.1390223503112793, "compression_ratio": 1.56, "no_speech_prob": 0.06557880342006683}, {"id": 164, "seek": 101642, "start": 1038.3799999999999, "end": 1043.62, "text": " Because this network is so simple, we can calculate from one end to the other, x times", "tokens": [51462, 1436, 341, 3209, 307, 370, 2199, 11, 321, 393, 8873, 490, 472, 917, 281, 264, 661, 11, 2031, 1413, 51724], "temperature": 0.0, "avg_logprob": -0.1390223503112793, "compression_ratio": 1.56, "no_speech_prob": 0.06557880342006683}, {"id": 165, "seek": 104362, "start": 1043.62, "end": 1048.1399999999999, "text": " w1 times w2 is the error E.", "tokens": [50364, 261, 16, 1413, 261, 17, 307, 264, 6713, 462, 13, 50590], "temperature": 0.0, "avg_logprob": -0.12807780265808105, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.019713157787919044}, {"id": 166, "seek": 104362, "start": 1048.1399999999999, "end": 1052.9399999999998, "text": " And so if we want to calculate how much will the error change, if I change w1, we just", "tokens": [50590, 400, 370, 498, 321, 528, 281, 8873, 577, 709, 486, 264, 6713, 1319, 11, 498, 286, 1319, 261, 16, 11, 321, 445, 50830], "temperature": 0.0, "avg_logprob": -0.12807780265808105, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.019713157787919044}, {"id": 167, "seek": 104362, "start": 1052.9399999999998, "end": 1059.2199999999998, "text": " take the derivative of that with respect to w1, and get x times w2.", "tokens": [50830, 747, 264, 13760, 295, 300, 365, 3104, 281, 261, 16, 11, 293, 483, 2031, 1413, 261, 17, 13, 51144], "temperature": 0.0, "avg_logprob": -0.12807780265808105, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.019713157787919044}, {"id": 168, "seek": 104362, "start": 1059.2199999999998, "end": 1064.34, "text": " So this illustrates, you can see here now, that what we just calculated is actually the", "tokens": [51144, 407, 341, 41718, 11, 291, 393, 536, 510, 586, 11, 300, 437, 321, 445, 15598, 307, 767, 264, 51400], "temperature": 0.0, "avg_logprob": -0.12807780265808105, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.019713157787919044}, {"id": 169, "seek": 104362, "start": 1064.34, "end": 1073.58, "text": " product of our first derivative that we took, the dy dw1 times the derivative for the next", "tokens": [51400, 1674, 295, 527, 700, 13760, 300, 321, 1890, 11, 264, 14584, 274, 86, 16, 1413, 264, 13760, 337, 264, 958, 51862], "temperature": 0.0, "avg_logprob": -0.12807780265808105, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.019713157787919044}, {"id": 170, "seek": 107358, "start": 1073.58, "end": 1079.78, "text": " step, dE dy, multiplied together.", "tokens": [50364, 1823, 11, 274, 36, 14584, 11, 17207, 1214, 13, 50674], "temperature": 0.0, "avg_logprob": -0.10454457998275757, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.01853444054722786}, {"id": 171, "seek": 107358, "start": 1079.78, "end": 1082.02, "text": " This is chaining.", "tokens": [50674, 639, 307, 417, 3686, 13, 50786], "temperature": 0.0, "avg_logprob": -0.10454457998275757, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.01853444054722786}, {"id": 172, "seek": 107358, "start": 1082.02, "end": 1088.22, "text": " You can calculate the slope of each tiny step, and then multiply all of those together to", "tokens": [50786, 509, 393, 8873, 264, 13525, 295, 1184, 5870, 1823, 11, 293, 550, 12972, 439, 295, 729, 1214, 281, 51096], "temperature": 0.0, "avg_logprob": -0.10454457998275757, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.01853444054722786}, {"id": 173, "seek": 107358, "start": 1088.22, "end": 1093.6599999999999, "text": " get the slope of the full chain, the derivative of the full chain.", "tokens": [51096, 483, 264, 13525, 295, 264, 1577, 5021, 11, 264, 13760, 295, 264, 1577, 5021, 13, 51368], "temperature": 0.0, "avg_logprob": -0.10454457998275757, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.01853444054722786}, {"id": 174, "seek": 107358, "start": 1093.6599999999999, "end": 1098.8999999999999, "text": " So in a deeper neural network, what this would look like is if I want to know how much the", "tokens": [51368, 407, 294, 257, 7731, 18161, 3209, 11, 437, 341, 576, 574, 411, 307, 498, 286, 528, 281, 458, 577, 709, 264, 51630], "temperature": 0.0, "avg_logprob": -0.10454457998275757, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.01853444054722786}, {"id": 175, "seek": 109890, "start": 1098.9, "end": 1105.46, "text": " error will change, if I adjust a weight that's deep in the network, I just calculate the", "tokens": [50364, 6713, 486, 1319, 11, 498, 286, 4369, 257, 3364, 300, 311, 2452, 294, 264, 3209, 11, 286, 445, 8873, 264, 50692], "temperature": 0.0, "avg_logprob": -0.10373987470354352, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.040830858051776886}, {"id": 176, "seek": 109890, "start": 1105.46, "end": 1110.98, "text": " derivative of each tiny little step all the way back to the weight that I'm trying to", "tokens": [50692, 13760, 295, 1184, 5870, 707, 1823, 439, 264, 636, 646, 281, 264, 3364, 300, 286, 478, 1382, 281, 50968], "temperature": 0.0, "avg_logprob": -0.10373987470354352, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.040830858051776886}, {"id": 177, "seek": 109890, "start": 1110.98, "end": 1115.5400000000002, "text": " calculate, and then multiply them all together.", "tokens": [50968, 8873, 11, 293, 550, 12972, 552, 439, 1214, 13, 51196], "temperature": 0.0, "avg_logprob": -0.10373987470354352, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.040830858051776886}, {"id": 178, "seek": 109890, "start": 1115.5400000000002, "end": 1122.02, "text": " This computationally is many, many times cheaper than what we had to do before of recalculating", "tokens": [51196, 639, 24903, 379, 307, 867, 11, 867, 1413, 12284, 813, 437, 321, 632, 281, 360, 949, 295, 850, 304, 2444, 990, 51520], "temperature": 0.0, "avg_logprob": -0.10373987470354352, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.040830858051776886}, {"id": 179, "seek": 109890, "start": 1122.02, "end": 1127.5400000000002, "text": " the error for the whole neural network for every weight.", "tokens": [51520, 264, 6713, 337, 264, 1379, 18161, 3209, 337, 633, 3364, 13, 51796], "temperature": 0.0, "avg_logprob": -0.10373987470354352, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.040830858051776886}, {"id": 180, "seek": 112754, "start": 1127.54, "end": 1133.54, "text": " Now in the neural network that we've created, there are several types of back propagation", "tokens": [50364, 823, 294, 264, 18161, 3209, 300, 321, 600, 2942, 11, 456, 366, 2940, 3467, 295, 646, 38377, 50664], "temperature": 0.0, "avg_logprob": -0.11624998204848346, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.015416654758155346}, {"id": 181, "seek": 112754, "start": 1133.54, "end": 1134.54, "text": " we have to do.", "tokens": [50664, 321, 362, 281, 360, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11624998204848346, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.015416654758155346}, {"id": 182, "seek": 112754, "start": 1134.54, "end": 1139.02, "text": " There are several operations we have to do for each one of those, we have to be able", "tokens": [50714, 821, 366, 2940, 7705, 321, 362, 281, 360, 337, 1184, 472, 295, 729, 11, 321, 362, 281, 312, 1075, 50938], "temperature": 0.0, "avg_logprob": -0.11624998204848346, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.015416654758155346}, {"id": 183, "seek": 112754, "start": 1139.02, "end": 1141.02, "text": " to calculate the slope.", "tokens": [50938, 281, 8873, 264, 13525, 13, 51038], "temperature": 0.0, "avg_logprob": -0.11624998204848346, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.015416654758155346}, {"id": 184, "seek": 112754, "start": 1141.02, "end": 1148.58, "text": " So for the first one is just a weighted connection between two neurons, A and B. So let's assume", "tokens": [51038, 407, 337, 264, 700, 472, 307, 445, 257, 32807, 4984, 1296, 732, 22027, 11, 316, 293, 363, 13, 407, 718, 311, 6552, 51416], "temperature": 0.0, "avg_logprob": -0.11624998204848346, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.015416654758155346}, {"id": 185, "seek": 112754, "start": 1148.58, "end": 1154.34, "text": " we know the change in error with respect to B. We want to know the change in error with", "tokens": [51416, 321, 458, 264, 1319, 294, 6713, 365, 3104, 281, 363, 13, 492, 528, 281, 458, 264, 1319, 294, 6713, 365, 51704], "temperature": 0.0, "avg_logprob": -0.11624998204848346, "compression_ratio": 1.7533039647577093, "no_speech_prob": 0.015416654758155346}, {"id": 186, "seek": 115434, "start": 1154.34, "end": 1160.82, "text": " respect to A. To get there, we need to know DBDA.", "tokens": [50364, 3104, 281, 316, 13, 1407, 483, 456, 11, 321, 643, 281, 458, 26754, 7509, 13, 50688], "temperature": 0.0, "avg_logprob": -0.13847182560892937, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.0038237872067838907}, {"id": 187, "seek": 115434, "start": 1160.82, "end": 1165.58, "text": " So to get that, we just write the relationship between B and A, take the derivative of B", "tokens": [50688, 407, 281, 483, 300, 11, 321, 445, 2464, 264, 2480, 1296, 363, 293, 316, 11, 747, 264, 13760, 295, 363, 50926], "temperature": 0.0, "avg_logprob": -0.13847182560892937, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.0038237872067838907}, {"id": 188, "seek": 115434, "start": 1165.58, "end": 1171.34, "text": " with respect to A, we get the weight W, and now we know how to make that step.", "tokens": [50926, 365, 3104, 281, 316, 11, 321, 483, 264, 3364, 343, 11, 293, 586, 321, 458, 577, 281, 652, 300, 1823, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13847182560892937, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.0038237872067838907}, {"id": 189, "seek": 115434, "start": 1171.34, "end": 1176.9399999999998, "text": " We know how to do that little nugget of back propagation.", "tokens": [51214, 492, 458, 577, 281, 360, 300, 707, 30279, 847, 295, 646, 38377, 13, 51494], "temperature": 0.0, "avg_logprob": -0.13847182560892937, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.0038237872067838907}, {"id": 190, "seek": 115434, "start": 1176.9399999999998, "end": 1179.98, "text": " Another element that we've seen is sums.", "tokens": [51494, 3996, 4478, 300, 321, 600, 1612, 307, 34499, 13, 51646], "temperature": 0.0, "avg_logprob": -0.13847182560892937, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.0038237872067838907}, {"id": 191, "seek": 115434, "start": 1179.98, "end": 1183.78, "text": " All of our neurons sum up a lot of inputs.", "tokens": [51646, 1057, 295, 527, 22027, 2408, 493, 257, 688, 295, 15743, 13, 51836], "temperature": 0.0, "avg_logprob": -0.13847182560892937, "compression_ratio": 1.639269406392694, "no_speech_prob": 0.0038237872067838907}, {"id": 192, "seek": 118378, "start": 1183.82, "end": 1190.74, "text": " To take this back propagation step, we do the same thing, we write our expression, and", "tokens": [50366, 1407, 747, 341, 646, 38377, 1823, 11, 321, 360, 264, 912, 551, 11, 321, 2464, 527, 6114, 11, 293, 50712], "temperature": 0.0, "avg_logprob": -0.15393424034118652, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.006485611665993929}, {"id": 193, "seek": 118378, "start": 1190.74, "end": 1198.1399999999999, "text": " then we take the derivative of our endpoint Z with respect to our step that we're propagating", "tokens": [50712, 550, 321, 747, 264, 13760, 295, 527, 35795, 1176, 365, 3104, 281, 527, 1823, 300, 321, 434, 12425, 990, 51082], "temperature": 0.0, "avg_logprob": -0.15393424034118652, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.006485611665993929}, {"id": 194, "seek": 118378, "start": 1198.1399999999999, "end": 1203.1399999999999, "text": " to A, and DZDA in this case is just one.", "tokens": [51082, 281, 316, 11, 293, 413, 57, 7509, 294, 341, 1389, 307, 445, 472, 13, 51332], "temperature": 0.0, "avg_logprob": -0.15393424034118652, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.006485611665993929}, {"id": 195, "seek": 118378, "start": 1203.1399999999999, "end": 1204.1399999999999, "text": " Which makes sense.", "tokens": [51332, 3013, 1669, 2020, 13, 51382], "temperature": 0.0, "avg_logprob": -0.15393424034118652, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.006485611665993929}, {"id": 196, "seek": 118378, "start": 1204.1399999999999, "end": 1210.1, "text": " If we have a sum of a whole bunch of elements, we increase one of those elements by one,", "tokens": [51382, 759, 321, 362, 257, 2408, 295, 257, 1379, 3840, 295, 4959, 11, 321, 3488, 472, 295, 729, 4959, 538, 472, 11, 51680], "temperature": 0.0, "avg_logprob": -0.15393424034118652, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.006485611665993929}, {"id": 197, "seek": 118378, "start": 1210.1, "end": 1213.22, "text": " we expect the sum to increase by one.", "tokens": [51680, 321, 2066, 264, 2408, 281, 3488, 538, 472, 13, 51836], "temperature": 0.0, "avg_logprob": -0.15393424034118652, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.006485611665993929}, {"id": 198, "seek": 121322, "start": 1213.26, "end": 1220.9, "text": " That's the definition of a slope of one, one-to-one relation there.", "tokens": [50366, 663, 311, 264, 7123, 295, 257, 13525, 295, 472, 11, 472, 12, 1353, 12, 546, 9721, 456, 13, 50748], "temperature": 0.0, "avg_logprob": -0.1896459460258484, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0002531253267079592}, {"id": 199, "seek": 121322, "start": 1220.9, "end": 1224.66, "text": " Another element that we have that we need to be able to back propagate is the sigmoid", "tokens": [50748, 3996, 4478, 300, 321, 362, 300, 321, 643, 281, 312, 1075, 281, 646, 48256, 307, 264, 4556, 3280, 327, 50936], "temperature": 0.0, "avg_logprob": -0.1896459460258484, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0002531253267079592}, {"id": 200, "seek": 121322, "start": 1224.66, "end": 1226.9, "text": " function.", "tokens": [50936, 2445, 13, 51048], "temperature": 0.0, "avg_logprob": -0.1896459460258484, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0002531253267079592}, {"id": 201, "seek": 121322, "start": 1226.9, "end": 1230.54, "text": " So this one's a little bit more interesting mathematically.", "tokens": [51048, 407, 341, 472, 311, 257, 707, 857, 544, 1880, 44003, 13, 51230], "temperature": 0.0, "avg_logprob": -0.1896459460258484, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0002531253267079592}, {"id": 202, "seek": 121322, "start": 1230.54, "end": 1235.22, "text": " We'll just write it shorthand like this, the sigma function.", "tokens": [51230, 492, 603, 445, 2464, 309, 402, 2652, 474, 411, 341, 11, 264, 12771, 2445, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1896459460258484, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0002531253267079592}, {"id": 203, "seek": 121322, "start": 1235.22, "end": 1242.02, "text": " It is entirely feasible to go through and take the derivative of this analytically and calculate", "tokens": [51464, 467, 307, 7696, 26648, 281, 352, 807, 293, 747, 264, 13760, 295, 341, 10783, 984, 293, 8873, 51804], "temperature": 0.0, "avg_logprob": -0.1896459460258484, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0002531253267079592}, {"id": 204, "seek": 124202, "start": 1242.02, "end": 1243.1399999999999, "text": " it.", "tokens": [50364, 309, 13, 50420], "temperature": 0.0, "avg_logprob": -0.1577749252319336, "compression_ratio": 1.5258215962441315, "no_speech_prob": 0.0003053272084798664}, {"id": 205, "seek": 124202, "start": 1243.1399999999999, "end": 1249.74, "text": " It just so happens that this function has a nice property that to get its derivative,", "tokens": [50420, 467, 445, 370, 2314, 300, 341, 2445, 575, 257, 1481, 4707, 300, 281, 483, 1080, 13760, 11, 50750], "temperature": 0.0, "avg_logprob": -0.1577749252319336, "compression_ratio": 1.5258215962441315, "no_speech_prob": 0.0003053272084798664}, {"id": 206, "seek": 124202, "start": 1249.74, "end": 1254.1, "text": " you just multiply it by one minus itself.", "tokens": [50750, 291, 445, 12972, 309, 538, 472, 3175, 2564, 13, 50968], "temperature": 0.0, "avg_logprob": -0.1577749252319336, "compression_ratio": 1.5258215962441315, "no_speech_prob": 0.0003053272084798664}, {"id": 207, "seek": 124202, "start": 1254.1, "end": 1260.78, "text": " So this is very straightforward to calculate.", "tokens": [50968, 407, 341, 307, 588, 15325, 281, 8873, 13, 51302], "temperature": 0.0, "avg_logprob": -0.1577749252319336, "compression_ratio": 1.5258215962441315, "no_speech_prob": 0.0003053272084798664}, {"id": 208, "seek": 124202, "start": 1260.78, "end": 1263.58, "text": " Another element that we've used is the rectified linear unit.", "tokens": [51302, 3996, 4478, 300, 321, 600, 1143, 307, 264, 11048, 2587, 8213, 4985, 13, 51442], "temperature": 0.0, "avg_logprob": -0.1577749252319336, "compression_ratio": 1.5258215962441315, "no_speech_prob": 0.0003053272084798664}, {"id": 209, "seek": 124202, "start": 1263.58, "end": 1269.22, "text": " Again, to figure out how to back propagate this, we just write out the relation, B is", "tokens": [51442, 3764, 11, 281, 2573, 484, 577, 281, 646, 48256, 341, 11, 321, 445, 2464, 484, 264, 9721, 11, 363, 307, 51724], "temperature": 0.0, "avg_logprob": -0.1577749252319336, "compression_ratio": 1.5258215962441315, "no_speech_prob": 0.0003053272084798664}, {"id": 210, "seek": 126922, "start": 1269.22, "end": 1273.74, "text": " equal to A if A is positive, otherwise it's zero.", "tokens": [50364, 2681, 281, 316, 498, 316, 307, 3353, 11, 5911, 309, 311, 4018, 13, 50590], "temperature": 0.0, "avg_logprob": -0.1701908508936564, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.0023963323328644037}, {"id": 211, "seek": 126922, "start": 1273.74, "end": 1277.94, "text": " And piecewise, for each of those, we take the derivative.", "tokens": [50590, 400, 2522, 3711, 11, 337, 1184, 295, 729, 11, 321, 747, 264, 13760, 13, 50800], "temperature": 0.0, "avg_logprob": -0.1701908508936564, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.0023963323328644037}, {"id": 212, "seek": 126922, "start": 1277.94, "end": 1285.1000000000001, "text": " So dBdA is either one, if A is positive, or zero.", "tokens": [50800, 407, 274, 33, 67, 32, 307, 2139, 472, 11, 498, 316, 307, 3353, 11, 420, 4018, 13, 51158], "temperature": 0.0, "avg_logprob": -0.1701908508936564, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.0023963323328644037}, {"id": 213, "seek": 126922, "start": 1285.1000000000001, "end": 1292.18, "text": " And so with all of these little back propagation steps and the ability to chain them together,", "tokens": [51158, 400, 370, 365, 439, 295, 613, 707, 646, 38377, 4439, 293, 264, 3485, 281, 5021, 552, 1214, 11, 51512], "temperature": 0.0, "avg_logprob": -0.1701908508936564, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.0023963323328644037}, {"id": 214, "seek": 129218, "start": 1292.18, "end": 1299.54, "text": " we can calculate the effect of adjusting any given weight on the error for any given", "tokens": [50364, 321, 393, 8873, 264, 1802, 295, 23559, 604, 2212, 3364, 322, 264, 6713, 337, 604, 2212, 50732], "temperature": 0.0, "avg_logprob": -0.09778024673461914, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.06749683618545532}, {"id": 215, "seek": 129218, "start": 1299.54, "end": 1301.94, "text": " input.", "tokens": [50732, 4846, 13, 50852], "temperature": 0.0, "avg_logprob": -0.09778024673461914, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.06749683618545532}, {"id": 216, "seek": 129218, "start": 1301.94, "end": 1308.02, "text": " And so to train, then, we start with a fully connected network.", "tokens": [50852, 400, 370, 281, 3847, 11, 550, 11, 321, 722, 365, 257, 4498, 4582, 3209, 13, 51156], "temperature": 0.0, "avg_logprob": -0.09778024673461914, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.06749683618545532}, {"id": 217, "seek": 129218, "start": 1308.02, "end": 1314.0600000000002, "text": " We don't know what any of these weights should be, and so we assign them all random values.", "tokens": [51156, 492, 500, 380, 458, 437, 604, 295, 613, 17443, 820, 312, 11, 293, 370, 321, 6269, 552, 439, 4974, 4190, 13, 51458], "temperature": 0.0, "avg_logprob": -0.09778024673461914, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.06749683618545532}, {"id": 218, "seek": 129218, "start": 1314.0600000000002, "end": 1318.8600000000001, "text": " We create a completely arbitrary random neural network.", "tokens": [51458, 492, 1884, 257, 2584, 23211, 4974, 18161, 3209, 13, 51698], "temperature": 0.0, "avg_logprob": -0.09778024673461914, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.06749683618545532}, {"id": 219, "seek": 131886, "start": 1318.86, "end": 1322.5, "text": " We put in an input that we know the answer to.", "tokens": [50364, 492, 829, 294, 364, 4846, 300, 321, 458, 264, 1867, 281, 13, 50546], "temperature": 0.0, "avg_logprob": -0.11572033891053958, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.013219563290476799}, {"id": 220, "seek": 131886, "start": 1322.5, "end": 1327.1399999999999, "text": " We know whether it's solid, vertical, diagonal, or horizontal, so we know what truth should", "tokens": [50546, 492, 458, 1968, 309, 311, 5100, 11, 9429, 11, 21539, 11, 420, 12750, 11, 370, 321, 458, 437, 3494, 820, 50778], "temperature": 0.0, "avg_logprob": -0.11572033891053958, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.013219563290476799}, {"id": 221, "seek": 131886, "start": 1327.1399999999999, "end": 1331.58, "text": " be, and so we can calculate the error.", "tokens": [50778, 312, 11, 293, 370, 321, 393, 8873, 264, 6713, 13, 51000], "temperature": 0.0, "avg_logprob": -0.11572033891053958, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.013219563290476799}, {"id": 222, "seek": 131886, "start": 1331.58, "end": 1338.26, "text": " Then we run it through, calculate the error, and using back propagation, go through and", "tokens": [51000, 1396, 321, 1190, 309, 807, 11, 8873, 264, 6713, 11, 293, 1228, 646, 38377, 11, 352, 807, 293, 51334], "temperature": 0.0, "avg_logprob": -0.11572033891053958, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.013219563290476799}, {"id": 223, "seek": 131886, "start": 1338.26, "end": 1343.6999999999998, "text": " adjust all of those weights a tiny bit in the right direction.", "tokens": [51334, 4369, 439, 295, 729, 17443, 257, 5870, 857, 294, 264, 558, 3513, 13, 51606], "temperature": 0.0, "avg_logprob": -0.11572033891053958, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.013219563290476799}, {"id": 224, "seek": 131886, "start": 1343.6999999999998, "end": 1348.5, "text": " And then we do that again with another input, and again with another input for, if we can", "tokens": [51606, 400, 550, 321, 360, 300, 797, 365, 1071, 4846, 11, 293, 797, 365, 1071, 4846, 337, 11, 498, 321, 393, 51846], "temperature": 0.0, "avg_logprob": -0.11572033891053958, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.013219563290476799}, {"id": 225, "seek": 134850, "start": 1348.5, "end": 1353.46, "text": " get away with it, many thousands or even millions of times.", "tokens": [50364, 483, 1314, 365, 309, 11, 867, 5383, 420, 754, 6803, 295, 1413, 13, 50612], "temperature": 0.0, "avg_logprob": -0.1297321319580078, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.02673596702516079}, {"id": 226, "seek": 134850, "start": 1353.46, "end": 1360.02, "text": " And eventually, all of those weights will gravitate, they'll roll down that many dimensional", "tokens": [50612, 400, 4728, 11, 439, 295, 729, 17443, 486, 7427, 8086, 11, 436, 603, 3373, 760, 300, 867, 18795, 50940], "temperature": 0.0, "avg_logprob": -0.1297321319580078, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.02673596702516079}, {"id": 227, "seek": 134850, "start": 1360.02, "end": 1367.14, "text": " valley to a nice low spot in the bottom, where it performs really well and does pretty close", "tokens": [50940, 17636, 281, 257, 1481, 2295, 4008, 294, 264, 2767, 11, 689, 309, 26213, 534, 731, 293, 775, 1238, 1998, 51296], "temperature": 0.0, "avg_logprob": -0.1297321319580078, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.02673596702516079}, {"id": 228, "seek": 134850, "start": 1367.14, "end": 1372.7, "text": " to truth on most of the images.", "tokens": [51296, 281, 3494, 322, 881, 295, 264, 5267, 13, 51574], "temperature": 0.0, "avg_logprob": -0.1297321319580078, "compression_ratio": 1.521978021978022, "no_speech_prob": 0.02673596702516079}, {"id": 229, "seek": 137270, "start": 1372.7, "end": 1381.02, "text": " If we're really lucky, it'll look like what we started with, with intuitively understandable", "tokens": [50364, 759, 321, 434, 534, 6356, 11, 309, 603, 574, 411, 437, 321, 1409, 365, 11, 365, 46506, 25648, 50780], "temperature": 0.0, "avg_logprob": -0.16176339467366538, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.39928045868873596}, {"id": 230, "seek": 137270, "start": 1381.02, "end": 1386.54, "text": " receptive fields for those neurons, and a relatively sparse representation, meaning", "tokens": [50780, 45838, 7909, 337, 729, 22027, 11, 293, 257, 7226, 637, 11668, 10290, 11, 3620, 51056], "temperature": 0.0, "avg_logprob": -0.16176339467366538, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.39928045868873596}, {"id": 231, "seek": 137270, "start": 1386.54, "end": 1390.9, "text": " that most of the weights are small or close to zero.", "tokens": [51056, 300, 881, 295, 264, 17443, 366, 1359, 420, 1998, 281, 4018, 13, 51274], "temperature": 0.0, "avg_logprob": -0.16176339467366538, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.39928045868873596}, {"id": 232, "seek": 137270, "start": 1390.9, "end": 1397.26, "text": " It doesn't always turn out that way, but what we are guaranteed is that it'll find a pretty", "tokens": [51274, 467, 1177, 380, 1009, 1261, 484, 300, 636, 11, 457, 437, 321, 366, 18031, 307, 300, 309, 603, 915, 257, 1238, 51592], "temperature": 0.0, "avg_logprob": -0.16176339467366538, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.39928045868873596}, {"id": 233, "seek": 139726, "start": 1397.26, "end": 1403.3799999999999, "text": " good representation of the best that it can do adjusting those weights to get as close", "tokens": [50364, 665, 10290, 295, 264, 1151, 300, 309, 393, 360, 23559, 729, 17443, 281, 483, 382, 1998, 50670], "temperature": 0.0, "avg_logprob": -0.09839980476780942, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.11901992559432983}, {"id": 234, "seek": 139726, "start": 1403.3799999999999, "end": 1410.86, "text": " as possible to the right answer for all of the inputs.", "tokens": [50670, 382, 1944, 281, 264, 558, 1867, 337, 439, 295, 264, 15743, 13, 51044], "temperature": 0.0, "avg_logprob": -0.09839980476780942, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.11901992559432983}, {"id": 235, "seek": 139726, "start": 1410.86, "end": 1414.82, "text": " So what we've covered is just a very basic introduction to the principles behind neural", "tokens": [51044, 407, 437, 321, 600, 5343, 307, 445, 257, 588, 3875, 9339, 281, 264, 9156, 2261, 18161, 51242], "temperature": 0.0, "avg_logprob": -0.09839980476780942, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.11901992559432983}, {"id": 236, "seek": 139726, "start": 1414.82, "end": 1416.06, "text": " networks.", "tokens": [51242, 9590, 13, 51304], "temperature": 0.0, "avg_logprob": -0.09839980476780942, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.11901992559432983}, {"id": 237, "seek": 139726, "start": 1416.06, "end": 1420.06, "text": " I haven't told you quite enough to be able to go out and build one of your own, but if", "tokens": [51304, 286, 2378, 380, 1907, 291, 1596, 1547, 281, 312, 1075, 281, 352, 484, 293, 1322, 472, 295, 428, 1065, 11, 457, 498, 51504], "temperature": 0.0, "avg_logprob": -0.09839980476780942, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.11901992559432983}, {"id": 238, "seek": 139726, "start": 1420.06, "end": 1424.34, "text": " you're feeling motivated to do so, I highly encourage it.", "tokens": [51504, 291, 434, 2633, 14515, 281, 360, 370, 11, 286, 5405, 5373, 309, 13, 51718], "temperature": 0.0, "avg_logprob": -0.09839980476780942, "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.11901992559432983}, {"id": 239, "seek": 142434, "start": 1424.34, "end": 1427.6599999999999, "text": " Here are a few resources that you'll find useful.", "tokens": [50364, 1692, 366, 257, 1326, 3593, 300, 291, 603, 915, 4420, 13, 50530], "temperature": 0.0, "avg_logprob": -0.17064747529871324, "compression_ratio": 1.5175438596491229, "no_speech_prob": 0.2683306932449341}, {"id": 240, "seek": 142434, "start": 1427.6599999999999, "end": 1431.1, "text": " You'll want to go and learn about bias neurons.", "tokens": [50530, 509, 603, 528, 281, 352, 293, 1466, 466, 12577, 22027, 13, 50702], "temperature": 0.0, "avg_logprob": -0.17064747529871324, "compression_ratio": 1.5175438596491229, "no_speech_prob": 0.2683306932449341}, {"id": 241, "seek": 142434, "start": 1431.1, "end": 1434.1399999999999, "text": " Dropout is a useful training tool.", "tokens": [50702, 17675, 346, 307, 257, 4420, 3097, 2290, 13, 50854], "temperature": 0.0, "avg_logprob": -0.17064747529871324, "compression_ratio": 1.5175438596491229, "no_speech_prob": 0.2683306932449341}, {"id": 242, "seek": 142434, "start": 1434.1399999999999, "end": 1440.5, "text": " There are several resources available from Andre Carpathi, who is an expert in neural", "tokens": [50854, 821, 366, 2940, 3593, 2435, 490, 20667, 2741, 31852, 72, 11, 567, 307, 364, 5844, 294, 18161, 51172], "temperature": 0.0, "avg_logprob": -0.17064747529871324, "compression_ratio": 1.5175438596491229, "no_speech_prob": 0.2683306932449341}, {"id": 243, "seek": 142434, "start": 1440.5, "end": 1444.74, "text": " networks and great at teaching about it.", "tokens": [51172, 9590, 293, 869, 412, 4571, 466, 309, 13, 51384], "temperature": 0.0, "avg_logprob": -0.17064747529871324, "compression_ratio": 1.5175438596491229, "no_speech_prob": 0.2683306932449341}, {"id": 244, "seek": 142434, "start": 1444.74, "end": 1448.4199999999998, "text": " Also there's a fantastic article called The Black Magic of Deep Learning that just has", "tokens": [51384, 2743, 456, 311, 257, 5456, 7222, 1219, 440, 4076, 16154, 295, 14895, 15205, 300, 445, 575, 51568], "temperature": 0.0, "avg_logprob": -0.17064747529871324, "compression_ratio": 1.5175438596491229, "no_speech_prob": 0.2683306932449341}, {"id": 245, "seek": 144842, "start": 1448.42, "end": 1456.42, "text": " a bunch of practical from the trenches tips on how to get them working well.", "tokens": [50364, 257, 3840, 295, 8496, 490, 264, 48245, 6082, 322, 577, 281, 483, 552, 1364, 731, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14316839521581476, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.5763812065124512}, {"id": 246, "seek": 144842, "start": 1456.42, "end": 1460.7, "text": " If you found this useful, I highly encourage you to visit my blog and check out several", "tokens": [50764, 759, 291, 1352, 341, 4420, 11, 286, 5405, 5373, 291, 281, 3441, 452, 6968, 293, 1520, 484, 2940, 50978], "temperature": 0.0, "avg_logprob": -0.14316839521581476, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.5763812065124512}, {"id": 247, "seek": 144842, "start": 1460.7, "end": 1464.7, "text": " other How It Works style posts.", "tokens": [50978, 661, 1012, 467, 27914, 3758, 12300, 13, 51178], "temperature": 0.0, "avg_logprob": -0.14316839521581476, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.5763812065124512}, {"id": 248, "seek": 144842, "start": 1464.7, "end": 1470.8600000000001, "text": " And the links for these slides you can get as well to use however you like.", "tokens": [51178, 400, 264, 6123, 337, 613, 9788, 291, 393, 483, 382, 731, 281, 764, 4461, 291, 411, 13, 51486], "temperature": 0.0, "avg_logprob": -0.14316839521581476, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.5763812065124512}, {"id": 249, "seek": 144842, "start": 1470.8600000000001, "end": 1474.3400000000001, "text": " There's also a link to them down in the comments section.", "tokens": [51486, 821, 311, 611, 257, 2113, 281, 552, 760, 294, 264, 3053, 3541, 13, 51660], "temperature": 0.0, "avg_logprob": -0.14316839521581476, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.5763812065124512}, {"id": 250, "seek": 144842, "start": 1474.3400000000001, "end": 1475.14, "text": " Thanks for listening.", "tokens": [51660, 2561, 337, 4764, 13, 51700], "temperature": 0.0, "avg_logprob": -0.14316839521581476, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.5763812065124512}], "language": "en"}