start	end	text
0	7360	Neural networks are good for learning lots of different types of patterns.
7360	12000	To give an example of how this would work, imagine you had a four pixel camera.
12000	17640	So not four megapixels, but just four pixels, and it was only black and white.
17640	22920	And you wanted to go around and take pictures of things and determine automatically then
22920	29320	whether these pictures were of solid, all-white, or all-dark image,
29320	35520	vertical line, or a diagonal line, or a horizontal line.
35520	41920	This is tricky because you can't do this with simple rules about the brightness of the pixels.
41920	49040	Both of these are horizontal lines, but if you tried to make a rule about which pixel was bright and which was dark,
49040	53840	you wouldn't be able to do it.
53840	59160	So to do this with the neural network, you start by taking all of your inputs, in this case our four pixels,
59160	62360	and you break them out into input neurons.
62360	68680	And you assign a number to each of these, depending on the brightness or darkness of the pixel.
68680	77760	Plus one is all the way white, minus one is all the way black, and then gray is zero, right in the middle.
77760	85720	So these values, once you have them broken out and listed like this on the input neurons, it's also called the input vector, or array.
85720	93640	It's just a list of numbers that represents your inputs right now.
93640	100360	It's a useful notion to think about the receptive field of a neuron.
100360	108640	All this means is what set of inputs makes the value of this neuron as high as it can possibly be.
108640	111760	For input neurons, this is pretty easy.
111760	118200	Each one is associated with just one pixel, and when that pixel is all the way white,
118200	123280	the value of that input neuron is as high as it can go.
123280	128800	The black and white checkered areas show pixels that an input neuron doesn't care about.
128800	137320	If they're all the way white or all the way black, it still doesn't affect the value of that input neuron at all.
137320	142760	Now, to build a neural network, we create a neuron.
142760	150080	The first thing this does is it adds up all of the values of the input neurons.
150080	155440	So in this case, if we add up all of those values, we get a point five.
155440	161960	Now, to complicate things just a little bit, each of the connections are weighted,
161960	164560	meaning they're multiplied by a number.
164560	170240	That number can be one, or minus one, or anything in between.
170240	177480	So for instance, if something has a weight of minus one, it's multiplied, and you get the negative of it, and that's added in.
177480	182320	If something has a weight of zero, then it's effectively ignored.
182320	185520	So here's what those weighted connections might look like.
185520	198320	You'll notice that after the values of the input neurons are weighted and added, the final value is completely different.
198320	207400	Graphically, it's convenient to represent these weights as white links being positive weights, black links being negative weights,
207400	216400	and the thickness of the line is roughly proportional to the magnitude of the weight.
216400	222760	Then after you add the weighted input neurons, they get squashed.
222760	225000	And I'll show you what that means.
225000	227720	You have a sigmoid squashing function.
227720	230720	Sigmoid just means s-shaped.
230720	236960	And what this does is you put a value in, let's say point five,
236960	243920	and you run a vertical line up to your sigmoid, and then a horizontal line over from where it crosses.
243920	248160	And then where that hits the y-axis, that's the output of your function.
248160	250840	So in this case, slightly less than point five.
250840	253560	It's pretty close.
253560	260120	As your input number gets larger, your output number also gets larger, but more slowly.
260120	267880	And eventually, no matter how big the number you put in, the answer is always less than one.
267880	272920	Similarly, when you go negative, the answer is always greater than negative one.
272920	280680	So this ensures that that neuron's value never gets outside of the range of plus one to minus one,
280680	291040	which is helpful for keeping the computations in the neural network bounded and stable.
291040	296600	So after you sum the weighted values of the neurons and squash the result, you get the output.
296600	302200	In this case, point seven, four, six, that is a neuron.
302200	309360	So we can call this, we can collapse all that down, and this is a neuron that does a weighted sum and squash the result.
309360	314320	And now instead of just one of those, assume you have a whole bunch.
314320	320920	There are four shown here, but there could be four hundred or four million.
320920	333120	Now to keep our picture clear, we'll assume for now that the weights are either plus one, white lines, minus one, black lines, or zero, in which case they're missing entirely.
333120	349160	But in actuality, all of these neurons that we created are each attached to all of the input neurons, and they all have some weight between minus one and plus one.
349160	355880	When we create this first layer of our neural network, the receptive fields get more complex.
355880	385040	For instance, here, each of those end up combining two of our input neurons, and so the value, the receptive field, the pixel values that make that first layer neuron as large as it can possibly be, look now like pairs of pixels, either all white or a mixture of white and black, depending on the weights.
385040	399280	So for instance, this neuron here is attached to this input pixel, which is upper left, and this input pixel, which is lower left, and both of those weights are positive.
399280	407320	So it combines the two of those, and that's its receptive field, the receptive field of this one plus the receptive field of this one.
407320	418800	However, if we look at this neuron, it combines our this pixel, upper right, and this pixel, lower right.
418800	432360	It has a weight of minus one for the lower right pixel, so that means it's most active when this pixel is black, so here is its receptive field.
432360	452880	Now, because we were careful of how we created that first layer, its values look a lot like input values, and we can turn right around and create another layer on top of it the exact same way with the output of one layer being the input to the next layer.
452880	461280	And we can repeat this three times or seven times or seven hundred times for additional layers.
461280	479920	Each time the receptive fields get even more complex, so you can see here using the same logic, now they cover all of the pixels and more, more special arrangement of which are black and which are white.
479960	493520	We can create another layer, again, all of these neurons in one layer are connected to all of the neurons in the previous layer, but we're assuming here that most of those weights are zero and not shown.
493520	496920	It's not generally the case.
496920	506880	So just to mix things up, we'll create a new layer, but if you notice our squashing function isn't there anymore, we have something new called a rectified linear unit.
506880	511240	This is another popular neuron type.
511240	521680	So you do your weighted sum of all your inputs, and instead of squashing, you do rectified linear units, you rectify it.
521680	525840	So if it is negative, you make the value zero.
525840	529120	If it's positive, you keep the value.
529120	541920	This is obviously very easy to compute, and it turns out to have very nice stability properties for neural networks as well in practice.
541920	553620	So after we do this, because some of our weights are positive and some are negative, connecting to those rectified linear units, we get receptive fields and their opposites.
553620	557080	You look at the patterns there.
557080	563840	And then finally, when we've created as many layers with as many neurons as we want, we create an output layer.
563840	567080	Here, we have four outputs that we're interested in.
567080	575040	Is the image solid, vertical, diagonal, or horizontal?
575040	583920	So to walk through an example here of how this would work, let's say we start with this input image shown on the left.
583960	586880	Dark pixels on top, white on the bottom.
586880	593580	As we propagate that to our input layer, this is what those values would look like.
593580	596760	The top pixels, the bottom pixels.
596760	611620	As we move that to our first layer, we can see the combination of a dark pixel and a light pixel, summed together, get us zero, gray.
611620	618720	Whereas down here, we have the combination of a dark pixel plus a light pixel with a negative weight.
618720	630660	So that gets us a value of negative one there, which makes sense because if we look at the receptive field here, upper left pixel white, lower left pixel black,
630660	635020	it's the exact opposite of the input that we're getting.
635020	642320	And so we would expect its value to be as low as possible, minus one.
642320	654520	As we move to the next layer, we see the same types of things, combining zeros to get zeros, combining a negative and a negative with a negative weight,
654520	657960	which makes a positive to get a zero.
657960	661780	And here we have combining two negatives to get a negative.
661780	666880	So again, you'll notice the receptive field of this is exactly the inverse of our input.
666880	674440	So it makes sense that its weight would be negative, or its value would be negative.
674440	677080	And we move to the next layer.
677080	681280	All of these, of course, these zeros propagate forward.
681280	687880	Here, this has a negative value and it has a positive weight.
687880	694940	So it just moves straight forward because we have a rectified linear unit, negative values become zero.
694940	702240	So now it is zero again, two, but this one gets rectified and becomes positive, negative times the negative is positive.
702240	708740	And so when we finally get to the output, we can see they're all zero except for this horizontal, which is positive.
708740	709940	And that's the answer.
709940	717780	Our neural network said this is an image of a horizontal line.
717800	722300	Now, neural networks usually aren't that good, not that clean.
722300	726880	So there's a notion of, with an input, what is truth?
726880	733700	In this case, the truth is this has a zero for all of these values, but a one for horizontal.
733700	736580	It's not solid, it's not vertical, it's not diagonal.
736580	739640	Yes, it is horizontal.
739640	744600	An arbitrary neural network will give answers that are not exactly truth.
744600	747920	It might be off by a little or a lot.
747920	755160	And then the error is the magnitude of the difference between the truth and the answer given.
755160	761020	And you can add all these up to get the total error for the neural network.
761020	771820	So the idea, the whole idea with learning and training is to adjust the weights to make the error as low as possible.
771820	785980	So the way this is done is we put an image in, we calculate the error at the end, then we look for how to adjust those weights higher or lower to either make that error go up or down.
785980	791240	And we, of course, adjust the weights in the way, then make the error go down.
791240	801460	Now, the problem with doing this is each time we go back and calculate the error, we have to multiply all of those weights by all of the
801500	804540	neuron values at each layer.
804540	808440	And we have to do that again and again once for each weight.
808440	813880	This takes forever in computing terms, on a computing scale.
813880	819240	And so it's not a practical way to train a big neural network.
819240	823440	You can imagine instead of just rolling down to the bottom of a simple valley, we have
823440	827900	a very high dimensional valley and we have to find our way down.
827900	832780	And because there are so many dimensions, one for each of these weights, that the computation
832780	841940	just becomes prohibitively expensive, luckily there was an insight that lets us do this in
841940	844160	a very reasonable time.
844160	849060	And that's that if we're careful about how we design our neural network, we can calculate
849060	852020	the slope directly, the gradient.
852020	857060	We can figure out the direction that we need to adjust the weight without going all the
857060	862740	way back through our neural network and recalculating.
862740	868980	So just to review, the slope that we're talking about is when we make a change in weight,
868980	871620	the error will change a little bit.
871620	878380	And that relation of the change in weight to the change in error is the slope.
878380	882120	Mathematically, there are several ways to write this.
882120	883520	We'll favor the one on the bottom.
883520	885760	It's technically most correct.
885760	888960	We'll call it DEDW for short hand.
888960	896760	Every time you see it, just think the change in error when I change a weight or the change
896760	901520	in the thing on the top when I change the thing on the bottom.
901520	905240	This does get into a little bit of calculus.
905240	906240	We do take derivatives.
906240	908680	That's how we calculate slope.
908680	914240	If it's new to you, I strongly recommend a good semester of calculus just because the
914240	918200	concepts are so universal.
918200	922840	A lot of them have very nice physical interpretations, which I find very appealing.
922840	927720	But don't worry, otherwise just gloss over this and pay attention to the rest and you'll
927720	931800	get a general sense for how this works.
931800	936560	So in this case, if we change the weight by plus one, the error changes by minus two,
936560	939980	which gives us a slope of minus two.
939980	944700	That tells us the direction that we should adjust our weight and how much we should adjust
944700	949460	it to bring the error down.
949460	952580	Now to do this, you have to know what your error function is.
952580	958140	So assume we had an error function that was the square of the weight, and you can see that
958140	962540	our weight is right at minus one.
962540	967460	So the first thing we do is we take the derivative, change in error, divided by change in weight
967460	969460	dE dW.
969460	973300	The derivative of weight squared is two times the weight.
973300	982380	And so we plug in our weight of minus one and we get a slope dE dW of minus two.
982380	987580	Now the other trick that lets us do this with deep neural networks is chaining.
987580	992660	And to show you how this works, imagine a very simple trivial neural network with just
992860	999620	one hidden layer, one input layer, one output layer, and one weight connecting each of them.
999620	1005460	So it's obvious to see that the value y is just the value x times the weight connecting
1005460	1008900	them, w1.
1008900	1014420	So if we change w1 a little bit, we just take the derivative of y with respect to w1, and
1014420	1015420	we get x.
1015420	1016420	The slope is x.
1016420	1023420	If I change w1 by a little bit, then y will change by x times the size of that adjustment.
1023420	1032180	Similarly, for the next step, we can see that E is just the value y times the weight w2.
1032180	1038380	And so when we calculate dE dy, it's just w2.
1038380	1043620	Because this network is so simple, we can calculate from one end to the other, x times
1043620	1048140	w1 times w2 is the error E.
1048140	1052940	And so if we want to calculate how much will the error change, if I change w1, we just
1052940	1059220	take the derivative of that with respect to w1, and get x times w2.
1059220	1064340	So this illustrates, you can see here now, that what we just calculated is actually the
1064340	1073580	product of our first derivative that we took, the dy dw1 times the derivative for the next
1073580	1079780	step, dE dy, multiplied together.
1079780	1082020	This is chaining.
1082020	1088220	You can calculate the slope of each tiny step, and then multiply all of those together to
1088220	1093660	get the slope of the full chain, the derivative of the full chain.
1093660	1098900	So in a deeper neural network, what this would look like is if I want to know how much the
1098900	1105460	error will change, if I adjust a weight that's deep in the network, I just calculate the
1105460	1110980	derivative of each tiny little step all the way back to the weight that I'm trying to
1110980	1115540	calculate, and then multiply them all together.
1115540	1122020	This computationally is many, many times cheaper than what we had to do before of recalculating
1122020	1127540	the error for the whole neural network for every weight.
1127540	1133540	Now in the neural network that we've created, there are several types of back propagation
1133540	1134540	we have to do.
1134540	1139020	There are several operations we have to do for each one of those, we have to be able
1139020	1141020	to calculate the slope.
1141020	1148580	So for the first one is just a weighted connection between two neurons, A and B. So let's assume
1148580	1154340	we know the change in error with respect to B. We want to know the change in error with
1154340	1160820	respect to A. To get there, we need to know DBDA.
1160820	1165580	So to get that, we just write the relationship between B and A, take the derivative of B
1165580	1171340	with respect to A, we get the weight W, and now we know how to make that step.
1171340	1176940	We know how to do that little nugget of back propagation.
1176940	1179980	Another element that we've seen is sums.
1179980	1183780	All of our neurons sum up a lot of inputs.
1183820	1190740	To take this back propagation step, we do the same thing, we write our expression, and
1190740	1198140	then we take the derivative of our endpoint Z with respect to our step that we're propagating
1198140	1203140	to A, and DZDA in this case is just one.
1203140	1204140	Which makes sense.
1204140	1210100	If we have a sum of a whole bunch of elements, we increase one of those elements by one,
1210100	1213220	we expect the sum to increase by one.
1213260	1220900	That's the definition of a slope of one, one-to-one relation there.
1220900	1224660	Another element that we have that we need to be able to back propagate is the sigmoid
1224660	1226900	function.
1226900	1230540	So this one's a little bit more interesting mathematically.
1230540	1235220	We'll just write it shorthand like this, the sigma function.
1235220	1242020	It is entirely feasible to go through and take the derivative of this analytically and calculate
1242020	1243140	it.
1243140	1249740	It just so happens that this function has a nice property that to get its derivative,
1249740	1254100	you just multiply it by one minus itself.
1254100	1260780	So this is very straightforward to calculate.
1260780	1263580	Another element that we've used is the rectified linear unit.
1263580	1269220	Again, to figure out how to back propagate this, we just write out the relation, B is
1269220	1273740	equal to A if A is positive, otherwise it's zero.
1273740	1277940	And piecewise, for each of those, we take the derivative.
1277940	1285100	So dBdA is either one, if A is positive, or zero.
1285100	1292180	And so with all of these little back propagation steps and the ability to chain them together,
1292180	1299540	we can calculate the effect of adjusting any given weight on the error for any given
1299540	1301940	input.
1301940	1308020	And so to train, then, we start with a fully connected network.
1308020	1314060	We don't know what any of these weights should be, and so we assign them all random values.
1314060	1318860	We create a completely arbitrary random neural network.
1318860	1322500	We put in an input that we know the answer to.
1322500	1327140	We know whether it's solid, vertical, diagonal, or horizontal, so we know what truth should
1327140	1331580	be, and so we can calculate the error.
1331580	1338260	Then we run it through, calculate the error, and using back propagation, go through and
1338260	1343700	adjust all of those weights a tiny bit in the right direction.
1343700	1348500	And then we do that again with another input, and again with another input for, if we can
1348500	1353460	get away with it, many thousands or even millions of times.
1353460	1360020	And eventually, all of those weights will gravitate, they'll roll down that many dimensional
1360020	1367140	valley to a nice low spot in the bottom, where it performs really well and does pretty close
1367140	1372700	to truth on most of the images.
1372700	1381020	If we're really lucky, it'll look like what we started with, with intuitively understandable
1381020	1386540	receptive fields for those neurons, and a relatively sparse representation, meaning
1386540	1390900	that most of the weights are small or close to zero.
1390900	1397260	It doesn't always turn out that way, but what we are guaranteed is that it'll find a pretty
1397260	1403380	good representation of the best that it can do adjusting those weights to get as close
1403380	1410860	as possible to the right answer for all of the inputs.
1410860	1414820	So what we've covered is just a very basic introduction to the principles behind neural
1414820	1416060	networks.
1416060	1420060	I haven't told you quite enough to be able to go out and build one of your own, but if
1420060	1424340	you're feeling motivated to do so, I highly encourage it.
1424340	1427660	Here are a few resources that you'll find useful.
1427660	1431100	You'll want to go and learn about bias neurons.
1431100	1434140	Dropout is a useful training tool.
1434140	1440500	There are several resources available from Andre Carpathi, who is an expert in neural
1440500	1444740	networks and great at teaching about it.
1444740	1448420	Also there's a fantastic article called The Black Magic of Deep Learning that just has
1448420	1456420	a bunch of practical from the trenches tips on how to get them working well.
1456420	1460700	If you found this useful, I highly encourage you to visit my blog and check out several
1460700	1464700	other How It Works style posts.
1464700	1470860	And the links for these slides you can get as well to use however you like.
1470860	1474340	There's also a link to them down in the comments section.
1474340	1475140	Thanks for listening.
