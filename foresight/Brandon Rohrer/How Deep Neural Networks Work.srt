1
00:00:00,000 --> 00:00:07,360
Neural networks are good for learning lots of different types of patterns.

2
00:00:07,360 --> 00:00:12,000
To give an example of how this would work, imagine you had a four pixel camera.

3
00:00:12,000 --> 00:00:17,640
So not four megapixels, but just four pixels, and it was only black and white.

4
00:00:17,640 --> 00:00:22,920
And you wanted to go around and take pictures of things and determine automatically then

5
00:00:22,920 --> 00:00:29,320
whether these pictures were of solid, all-white, or all-dark image,

6
00:00:29,320 --> 00:00:35,520
vertical line, or a diagonal line, or a horizontal line.

7
00:00:35,520 --> 00:00:41,920
This is tricky because you can't do this with simple rules about the brightness of the pixels.

8
00:00:41,920 --> 00:00:49,040
Both of these are horizontal lines, but if you tried to make a rule about which pixel was bright and which was dark,

9
00:00:49,040 --> 00:00:53,840
you wouldn't be able to do it.

10
00:00:53,840 --> 00:00:59,160
So to do this with the neural network, you start by taking all of your inputs, in this case our four pixels,

11
00:00:59,160 --> 00:01:02,360
and you break them out into input neurons.

12
00:01:02,360 --> 00:01:08,680
And you assign a number to each of these, depending on the brightness or darkness of the pixel.

13
00:01:08,680 --> 00:01:17,760
Plus one is all the way white, minus one is all the way black, and then gray is zero, right in the middle.

14
00:01:17,760 --> 00:01:25,720
So these values, once you have them broken out and listed like this on the input neurons, it's also called the input vector, or array.

15
00:01:25,720 --> 00:01:33,640
It's just a list of numbers that represents your inputs right now.

16
00:01:33,640 --> 00:01:40,360
It's a useful notion to think about the receptive field of a neuron.

17
00:01:40,360 --> 00:01:48,640
All this means is what set of inputs makes the value of this neuron as high as it can possibly be.

18
00:01:48,640 --> 00:01:51,760
For input neurons, this is pretty easy.

19
00:01:51,760 --> 00:01:58,200
Each one is associated with just one pixel, and when that pixel is all the way white,

20
00:01:58,200 --> 00:02:03,280
the value of that input neuron is as high as it can go.

21
00:02:03,280 --> 00:02:08,800
The black and white checkered areas show pixels that an input neuron doesn't care about.

22
00:02:08,800 --> 00:02:17,320
If they're all the way white or all the way black, it still doesn't affect the value of that input neuron at all.

23
00:02:17,320 --> 00:02:22,760
Now, to build a neural network, we create a neuron.

24
00:02:22,760 --> 00:02:30,080
The first thing this does is it adds up all of the values of the input neurons.

25
00:02:30,080 --> 00:02:35,440
So in this case, if we add up all of those values, we get a point five.

26
00:02:35,440 --> 00:02:41,960
Now, to complicate things just a little bit, each of the connections are weighted,

27
00:02:41,960 --> 00:02:44,560
meaning they're multiplied by a number.

28
00:02:44,560 --> 00:02:50,240
That number can be one, or minus one, or anything in between.

29
00:02:50,240 --> 00:02:57,480
So for instance, if something has a weight of minus one, it's multiplied, and you get the negative of it, and that's added in.

30
00:02:57,480 --> 00:03:02,320
If something has a weight of zero, then it's effectively ignored.

31
00:03:02,320 --> 00:03:05,520
So here's what those weighted connections might look like.

32
00:03:05,520 --> 00:03:18,320
You'll notice that after the values of the input neurons are weighted and added, the final value is completely different.

33
00:03:18,320 --> 00:03:27,400
Graphically, it's convenient to represent these weights as white links being positive weights, black links being negative weights,

34
00:03:27,400 --> 00:03:36,400
and the thickness of the line is roughly proportional to the magnitude of the weight.

35
00:03:36,400 --> 00:03:42,760
Then after you add the weighted input neurons, they get squashed.

36
00:03:42,760 --> 00:03:45,000
And I'll show you what that means.

37
00:03:45,000 --> 00:03:47,720
You have a sigmoid squashing function.

38
00:03:47,720 --> 00:03:50,720
Sigmoid just means s-shaped.

39
00:03:50,720 --> 00:03:56,960
And what this does is you put a value in, let's say point five,

40
00:03:56,960 --> 00:04:03,920
and you run a vertical line up to your sigmoid, and then a horizontal line over from where it crosses.

41
00:04:03,920 --> 00:04:08,160
And then where that hits the y-axis, that's the output of your function.

42
00:04:08,160 --> 00:04:10,840
So in this case, slightly less than point five.

43
00:04:10,840 --> 00:04:13,560
It's pretty close.

44
00:04:13,560 --> 00:04:20,120
As your input number gets larger, your output number also gets larger, but more slowly.

45
00:04:20,120 --> 00:04:27,880
And eventually, no matter how big the number you put in, the answer is always less than one.

46
00:04:27,880 --> 00:04:32,920
Similarly, when you go negative, the answer is always greater than negative one.

47
00:04:32,920 --> 00:04:40,680
So this ensures that that neuron's value never gets outside of the range of plus one to minus one,

48
00:04:40,680 --> 00:04:51,040
which is helpful for keeping the computations in the neural network bounded and stable.

49
00:04:51,040 --> 00:04:56,600
So after you sum the weighted values of the neurons and squash the result, you get the output.

50
00:04:56,600 --> 00:05:02,200
In this case, point seven, four, six, that is a neuron.

51
00:05:02,200 --> 00:05:09,360
So we can call this, we can collapse all that down, and this is a neuron that does a weighted sum and squash the result.

52
00:05:09,360 --> 00:05:14,320
And now instead of just one of those, assume you have a whole bunch.

53
00:05:14,320 --> 00:05:20,920
There are four shown here, but there could be four hundred or four million.

54
00:05:20,920 --> 00:05:33,120
Now to keep our picture clear, we'll assume for now that the weights are either plus one, white lines, minus one, black lines, or zero, in which case they're missing entirely.

55
00:05:33,120 --> 00:05:49,160
But in actuality, all of these neurons that we created are each attached to all of the input neurons, and they all have some weight between minus one and plus one.

56
00:05:49,160 --> 00:05:55,880
When we create this first layer of our neural network, the receptive fields get more complex.

57
00:05:55,880 --> 00:06:25,040
For instance, here, each of those end up combining two of our input neurons, and so the value, the receptive field, the pixel values that make that first layer neuron as large as it can possibly be, look now like pairs of pixels, either all white or a mixture of white and black, depending on the weights.

58
00:06:25,040 --> 00:06:39,280
So for instance, this neuron here is attached to this input pixel, which is upper left, and this input pixel, which is lower left, and both of those weights are positive.

59
00:06:39,280 --> 00:06:47,320
So it combines the two of those, and that's its receptive field, the receptive field of this one plus the receptive field of this one.

60
00:06:47,320 --> 00:06:58,800
However, if we look at this neuron, it combines our this pixel, upper right, and this pixel, lower right.

61
00:06:58,800 --> 00:07:12,360
It has a weight of minus one for the lower right pixel, so that means it's most active when this pixel is black, so here is its receptive field.

62
00:07:12,360 --> 00:07:32,880
Now, because we were careful of how we created that first layer, its values look a lot like input values, and we can turn right around and create another layer on top of it the exact same way with the output of one layer being the input to the next layer.

63
00:07:32,880 --> 00:07:41,280
And we can repeat this three times or seven times or seven hundred times for additional layers.

64
00:07:41,280 --> 00:07:59,920
Each time the receptive fields get even more complex, so you can see here using the same logic, now they cover all of the pixels and more, more special arrangement of which are black and which are white.

65
00:07:59,960 --> 00:08:13,520
We can create another layer, again, all of these neurons in one layer are connected to all of the neurons in the previous layer, but we're assuming here that most of those weights are zero and not shown.

66
00:08:13,520 --> 00:08:16,920
It's not generally the case.

67
00:08:16,920 --> 00:08:26,880
So just to mix things up, we'll create a new layer, but if you notice our squashing function isn't there anymore, we have something new called a rectified linear unit.

68
00:08:26,880 --> 00:08:31,240
This is another popular neuron type.

69
00:08:31,240 --> 00:08:41,680
So you do your weighted sum of all your inputs, and instead of squashing, you do rectified linear units, you rectify it.

70
00:08:41,680 --> 00:08:45,840
So if it is negative, you make the value zero.

71
00:08:45,840 --> 00:08:49,120
If it's positive, you keep the value.

72
00:08:49,120 --> 00:09:01,920
This is obviously very easy to compute, and it turns out to have very nice stability properties for neural networks as well in practice.

73
00:09:01,920 --> 00:09:13,620
So after we do this, because some of our weights are positive and some are negative, connecting to those rectified linear units, we get receptive fields and their opposites.

74
00:09:13,620 --> 00:09:17,080
You look at the patterns there.

75
00:09:17,080 --> 00:09:23,840
And then finally, when we've created as many layers with as many neurons as we want, we create an output layer.

76
00:09:23,840 --> 00:09:27,080
Here, we have four outputs that we're interested in.

77
00:09:27,080 --> 00:09:35,040
Is the image solid, vertical, diagonal, or horizontal?

78
00:09:35,040 --> 00:09:43,920
So to walk through an example here of how this would work, let's say we start with this input image shown on the left.

79
00:09:43,960 --> 00:09:46,880
Dark pixels on top, white on the bottom.

80
00:09:46,880 --> 00:09:53,580
As we propagate that to our input layer, this is what those values would look like.

81
00:09:53,580 --> 00:09:56,760
The top pixels, the bottom pixels.

82
00:09:56,760 --> 00:10:11,620
As we move that to our first layer, we can see the combination of a dark pixel and a light pixel, summed together, get us zero, gray.

83
00:10:11,620 --> 00:10:18,720
Whereas down here, we have the combination of a dark pixel plus a light pixel with a negative weight.

84
00:10:18,720 --> 00:10:30,660
So that gets us a value of negative one there, which makes sense because if we look at the receptive field here, upper left pixel white, lower left pixel black,

85
00:10:30,660 --> 00:10:35,020
it's the exact opposite of the input that we're getting.

86
00:10:35,020 --> 00:10:42,320
And so we would expect its value to be as low as possible, minus one.

87
00:10:42,320 --> 00:10:54,520
As we move to the next layer, we see the same types of things, combining zeros to get zeros, combining a negative and a negative with a negative weight,

88
00:10:54,520 --> 00:10:57,960
which makes a positive to get a zero.

89
00:10:57,960 --> 00:11:01,780
And here we have combining two negatives to get a negative.

90
00:11:01,780 --> 00:11:06,880
So again, you'll notice the receptive field of this is exactly the inverse of our input.

91
00:11:06,880 --> 00:11:14,440
So it makes sense that its weight would be negative, or its value would be negative.

92
00:11:14,440 --> 00:11:17,080
And we move to the next layer.

93
00:11:17,080 --> 00:11:21,280
All of these, of course, these zeros propagate forward.

94
00:11:21,280 --> 00:11:27,880
Here, this has a negative value and it has a positive weight.

95
00:11:27,880 --> 00:11:34,940
So it just moves straight forward because we have a rectified linear unit, negative values become zero.

96
00:11:34,940 --> 00:11:42,240
So now it is zero again, two, but this one gets rectified and becomes positive, negative times the negative is positive.

97
00:11:42,240 --> 00:11:48,740
And so when we finally get to the output, we can see they're all zero except for this horizontal, which is positive.

98
00:11:48,740 --> 00:11:49,940
And that's the answer.

99
00:11:49,940 --> 00:11:57,780
Our neural network said this is an image of a horizontal line.

100
00:11:57,800 --> 00:12:02,300
Now, neural networks usually aren't that good, not that clean.

101
00:12:02,300 --> 00:12:06,880
So there's a notion of, with an input, what is truth?

102
00:12:06,880 --> 00:12:13,700
In this case, the truth is this has a zero for all of these values, but a one for horizontal.

103
00:12:13,700 --> 00:12:16,580
It's not solid, it's not vertical, it's not diagonal.

104
00:12:16,580 --> 00:12:19,640
Yes, it is horizontal.

105
00:12:19,640 --> 00:12:24,600
An arbitrary neural network will give answers that are not exactly truth.

106
00:12:24,600 --> 00:12:27,920
It might be off by a little or a lot.

107
00:12:27,920 --> 00:12:35,160
And then the error is the magnitude of the difference between the truth and the answer given.

108
00:12:35,160 --> 00:12:41,020
And you can add all these up to get the total error for the neural network.

109
00:12:41,020 --> 00:12:51,820
So the idea, the whole idea with learning and training is to adjust the weights to make the error as low as possible.

110
00:12:51,820 --> 00:13:05,980
So the way this is done is we put an image in, we calculate the error at the end, then we look for how to adjust those weights higher or lower to either make that error go up or down.

111
00:13:05,980 --> 00:13:11,240
And we, of course, adjust the weights in the way, then make the error go down.

112
00:13:11,240 --> 00:13:21,460
Now, the problem with doing this is each time we go back and calculate the error, we have to multiply all of those weights by all of the

113
00:13:21,500 --> 00:13:24,540
neuron values at each layer.

114
00:13:24,540 --> 00:13:28,440
And we have to do that again and again once for each weight.

115
00:13:28,440 --> 00:13:33,880
This takes forever in computing terms, on a computing scale.

116
00:13:33,880 --> 00:13:39,240
And so it's not a practical way to train a big neural network.

117
00:13:39,240 --> 00:13:43,440
You can imagine instead of just rolling down to the bottom of a simple valley, we have

118
00:13:43,440 --> 00:13:47,900
a very high dimensional valley and we have to find our way down.

119
00:13:47,900 --> 00:13:52,780
And because there are so many dimensions, one for each of these weights, that the computation

120
00:13:52,780 --> 00:14:01,940
just becomes prohibitively expensive, luckily there was an insight that lets us do this in

121
00:14:01,940 --> 00:14:04,160
a very reasonable time.

122
00:14:04,160 --> 00:14:09,060
And that's that if we're careful about how we design our neural network, we can calculate

123
00:14:09,060 --> 00:14:12,020
the slope directly, the gradient.

124
00:14:12,020 --> 00:14:17,060
We can figure out the direction that we need to adjust the weight without going all the

125
00:14:17,060 --> 00:14:22,740
way back through our neural network and recalculating.

126
00:14:22,740 --> 00:14:28,980
So just to review, the slope that we're talking about is when we make a change in weight,

127
00:14:28,980 --> 00:14:31,620
the error will change a little bit.

128
00:14:31,620 --> 00:14:38,380
And that relation of the change in weight to the change in error is the slope.

129
00:14:38,380 --> 00:14:42,120
Mathematically, there are several ways to write this.

130
00:14:42,120 --> 00:14:43,520
We'll favor the one on the bottom.

131
00:14:43,520 --> 00:14:45,760
It's technically most correct.

132
00:14:45,760 --> 00:14:48,960
We'll call it DEDW for short hand.

133
00:14:48,960 --> 00:14:56,760
Every time you see it, just think the change in error when I change a weight or the change

134
00:14:56,760 --> 00:15:01,520
in the thing on the top when I change the thing on the bottom.

135
00:15:01,520 --> 00:15:05,240
This does get into a little bit of calculus.

136
00:15:05,240 --> 00:15:06,240
We do take derivatives.

137
00:15:06,240 --> 00:15:08,680
That's how we calculate slope.

138
00:15:08,680 --> 00:15:14,240
If it's new to you, I strongly recommend a good semester of calculus just because the

139
00:15:14,240 --> 00:15:18,200
concepts are so universal.

140
00:15:18,200 --> 00:15:22,840
A lot of them have very nice physical interpretations, which I find very appealing.

141
00:15:22,840 --> 00:15:27,720
But don't worry, otherwise just gloss over this and pay attention to the rest and you'll

142
00:15:27,720 --> 00:15:31,800
get a general sense for how this works.

143
00:15:31,800 --> 00:15:36,560
So in this case, if we change the weight by plus one, the error changes by minus two,

144
00:15:36,560 --> 00:15:39,980
which gives us a slope of minus two.

145
00:15:39,980 --> 00:15:44,700
That tells us the direction that we should adjust our weight and how much we should adjust

146
00:15:44,700 --> 00:15:49,460
it to bring the error down.

147
00:15:49,460 --> 00:15:52,580
Now to do this, you have to know what your error function is.

148
00:15:52,580 --> 00:15:58,140
So assume we had an error function that was the square of the weight, and you can see that

149
00:15:58,140 --> 00:16:02,540
our weight is right at minus one.

150
00:16:02,540 --> 00:16:07,460
So the first thing we do is we take the derivative, change in error, divided by change in weight

151
00:16:07,460 --> 00:16:09,460
dE dW.

152
00:16:09,460 --> 00:16:13,300
The derivative of weight squared is two times the weight.

153
00:16:13,300 --> 00:16:22,380
And so we plug in our weight of minus one and we get a slope dE dW of minus two.

154
00:16:22,380 --> 00:16:27,580
Now the other trick that lets us do this with deep neural networks is chaining.

155
00:16:27,580 --> 00:16:32,660
And to show you how this works, imagine a very simple trivial neural network with just

156
00:16:32,860 --> 00:16:39,620
one hidden layer, one input layer, one output layer, and one weight connecting each of them.

157
00:16:39,620 --> 00:16:45,460
So it's obvious to see that the value y is just the value x times the weight connecting

158
00:16:45,460 --> 00:16:48,900
them, w1.

159
00:16:48,900 --> 00:16:54,420
So if we change w1 a little bit, we just take the derivative of y with respect to w1, and

160
00:16:54,420 --> 00:16:55,420
we get x.

161
00:16:55,420 --> 00:16:56,420
The slope is x.

162
00:16:56,420 --> 00:17:03,420
If I change w1 by a little bit, then y will change by x times the size of that adjustment.

163
00:17:03,420 --> 00:17:12,180
Similarly, for the next step, we can see that E is just the value y times the weight w2.

164
00:17:12,180 --> 00:17:18,380
And so when we calculate dE dy, it's just w2.

165
00:17:18,380 --> 00:17:23,620
Because this network is so simple, we can calculate from one end to the other, x times

166
00:17:23,620 --> 00:17:28,140
w1 times w2 is the error E.

167
00:17:28,140 --> 00:17:32,940
And so if we want to calculate how much will the error change, if I change w1, we just

168
00:17:32,940 --> 00:17:39,220
take the derivative of that with respect to w1, and get x times w2.

169
00:17:39,220 --> 00:17:44,340
So this illustrates, you can see here now, that what we just calculated is actually the

170
00:17:44,340 --> 00:17:53,580
product of our first derivative that we took, the dy dw1 times the derivative for the next

171
00:17:53,580 --> 00:17:59,780
step, dE dy, multiplied together.

172
00:17:59,780 --> 00:18:02,020
This is chaining.

173
00:18:02,020 --> 00:18:08,220
You can calculate the slope of each tiny step, and then multiply all of those together to

174
00:18:08,220 --> 00:18:13,660
get the slope of the full chain, the derivative of the full chain.

175
00:18:13,660 --> 00:18:18,900
So in a deeper neural network, what this would look like is if I want to know how much the

176
00:18:18,900 --> 00:18:25,460
error will change, if I adjust a weight that's deep in the network, I just calculate the

177
00:18:25,460 --> 00:18:30,980
derivative of each tiny little step all the way back to the weight that I'm trying to

178
00:18:30,980 --> 00:18:35,540
calculate, and then multiply them all together.

179
00:18:35,540 --> 00:18:42,020
This computationally is many, many times cheaper than what we had to do before of recalculating

180
00:18:42,020 --> 00:18:47,540
the error for the whole neural network for every weight.

181
00:18:47,540 --> 00:18:53,540
Now in the neural network that we've created, there are several types of back propagation

182
00:18:53,540 --> 00:18:54,540
we have to do.

183
00:18:54,540 --> 00:18:59,020
There are several operations we have to do for each one of those, we have to be able

184
00:18:59,020 --> 00:19:01,020
to calculate the slope.

185
00:19:01,020 --> 00:19:08,580
So for the first one is just a weighted connection between two neurons, A and B. So let's assume

186
00:19:08,580 --> 00:19:14,340
we know the change in error with respect to B. We want to know the change in error with

187
00:19:14,340 --> 00:19:20,820
respect to A. To get there, we need to know DBDA.

188
00:19:20,820 --> 00:19:25,580
So to get that, we just write the relationship between B and A, take the derivative of B

189
00:19:25,580 --> 00:19:31,340
with respect to A, we get the weight W, and now we know how to make that step.

190
00:19:31,340 --> 00:19:36,940
We know how to do that little nugget of back propagation.

191
00:19:36,940 --> 00:19:39,980
Another element that we've seen is sums.

192
00:19:39,980 --> 00:19:43,780
All of our neurons sum up a lot of inputs.

193
00:19:43,820 --> 00:19:50,740
To take this back propagation step, we do the same thing, we write our expression, and

194
00:19:50,740 --> 00:19:58,140
then we take the derivative of our endpoint Z with respect to our step that we're propagating

195
00:19:58,140 --> 00:20:03,140
to A, and DZDA in this case is just one.

196
00:20:03,140 --> 00:20:04,140
Which makes sense.

197
00:20:04,140 --> 00:20:10,100
If we have a sum of a whole bunch of elements, we increase one of those elements by one,

198
00:20:10,100 --> 00:20:13,220
we expect the sum to increase by one.

199
00:20:13,260 --> 00:20:20,900
That's the definition of a slope of one, one-to-one relation there.

200
00:20:20,900 --> 00:20:24,660
Another element that we have that we need to be able to back propagate is the sigmoid

201
00:20:24,660 --> 00:20:26,900
function.

202
00:20:26,900 --> 00:20:30,540
So this one's a little bit more interesting mathematically.

203
00:20:30,540 --> 00:20:35,220
We'll just write it shorthand like this, the sigma function.

204
00:20:35,220 --> 00:20:42,020
It is entirely feasible to go through and take the derivative of this analytically and calculate

205
00:20:42,020 --> 00:20:43,140
it.

206
00:20:43,140 --> 00:20:49,740
It just so happens that this function has a nice property that to get its derivative,

207
00:20:49,740 --> 00:20:54,100
you just multiply it by one minus itself.

208
00:20:54,100 --> 00:21:00,780
So this is very straightforward to calculate.

209
00:21:00,780 --> 00:21:03,580
Another element that we've used is the rectified linear unit.

210
00:21:03,580 --> 00:21:09,220
Again, to figure out how to back propagate this, we just write out the relation, B is

211
00:21:09,220 --> 00:21:13,740
equal to A if A is positive, otherwise it's zero.

212
00:21:13,740 --> 00:21:17,940
And piecewise, for each of those, we take the derivative.

213
00:21:17,940 --> 00:21:25,100
So dBdA is either one, if A is positive, or zero.

214
00:21:25,100 --> 00:21:32,180
And so with all of these little back propagation steps and the ability to chain them together,

215
00:21:32,180 --> 00:21:39,540
we can calculate the effect of adjusting any given weight on the error for any given

216
00:21:39,540 --> 00:21:41,940
input.

217
00:21:41,940 --> 00:21:48,020
And so to train, then, we start with a fully connected network.

218
00:21:48,020 --> 00:21:54,060
We don't know what any of these weights should be, and so we assign them all random values.

219
00:21:54,060 --> 00:21:58,860
We create a completely arbitrary random neural network.

220
00:21:58,860 --> 00:22:02,500
We put in an input that we know the answer to.

221
00:22:02,500 --> 00:22:07,140
We know whether it's solid, vertical, diagonal, or horizontal, so we know what truth should

222
00:22:07,140 --> 00:22:11,580
be, and so we can calculate the error.

223
00:22:11,580 --> 00:22:18,260
Then we run it through, calculate the error, and using back propagation, go through and

224
00:22:18,260 --> 00:22:23,700
adjust all of those weights a tiny bit in the right direction.

225
00:22:23,700 --> 00:22:28,500
And then we do that again with another input, and again with another input for, if we can

226
00:22:28,500 --> 00:22:33,460
get away with it, many thousands or even millions of times.

227
00:22:33,460 --> 00:22:40,020
And eventually, all of those weights will gravitate, they'll roll down that many dimensional

228
00:22:40,020 --> 00:22:47,140
valley to a nice low spot in the bottom, where it performs really well and does pretty close

229
00:22:47,140 --> 00:22:52,700
to truth on most of the images.

230
00:22:52,700 --> 00:23:01,020
If we're really lucky, it'll look like what we started with, with intuitively understandable

231
00:23:01,020 --> 00:23:06,540
receptive fields for those neurons, and a relatively sparse representation, meaning

232
00:23:06,540 --> 00:23:10,900
that most of the weights are small or close to zero.

233
00:23:10,900 --> 00:23:17,260
It doesn't always turn out that way, but what we are guaranteed is that it'll find a pretty

234
00:23:17,260 --> 00:23:23,380
good representation of the best that it can do adjusting those weights to get as close

235
00:23:23,380 --> 00:23:30,860
as possible to the right answer for all of the inputs.

236
00:23:30,860 --> 00:23:34,820
So what we've covered is just a very basic introduction to the principles behind neural

237
00:23:34,820 --> 00:23:36,060
networks.

238
00:23:36,060 --> 00:23:40,060
I haven't told you quite enough to be able to go out and build one of your own, but if

239
00:23:40,060 --> 00:23:44,340
you're feeling motivated to do so, I highly encourage it.

240
00:23:44,340 --> 00:23:47,660
Here are a few resources that you'll find useful.

241
00:23:47,660 --> 00:23:51,100
You'll want to go and learn about bias neurons.

242
00:23:51,100 --> 00:23:54,140
Dropout is a useful training tool.

243
00:23:54,140 --> 00:24:00,500
There are several resources available from Andre Carpathi, who is an expert in neural

244
00:24:00,500 --> 00:24:04,740
networks and great at teaching about it.

245
00:24:04,740 --> 00:24:08,420
Also there's a fantastic article called The Black Magic of Deep Learning that just has

246
00:24:08,420 --> 00:24:16,420
a bunch of practical from the trenches tips on how to get them working well.

247
00:24:16,420 --> 00:24:20,700
If you found this useful, I highly encourage you to visit my blog and check out several

248
00:24:20,700 --> 00:24:24,700
other How It Works style posts.

249
00:24:24,700 --> 00:24:30,860
And the links for these slides you can get as well to use however you like.

250
00:24:30,860 --> 00:24:34,340
There's also a link to them down in the comments section.

251
00:24:34,340 --> 00:24:35,140
Thanks for listening.

