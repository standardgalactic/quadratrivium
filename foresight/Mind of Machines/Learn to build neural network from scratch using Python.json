{"text": " Welcome to Minds of Machine. Today we are diving into fascinating world of neural networks where we will construct a basic structure of neural networks using Python and then we will go to a even complex structures of with three layers of neural network using NumPy. So by laying the foundation for deeper learnings we are focusing on the core architectures. So let's get started. So first part we will build a single structure of neural network in order to understand the basic of neural network. First we define the input for the neural network. In our case we define the number format. It can be anything like an image or a long text. As you can see we have defined an input in which it is in the form of array 1, 2, 3 and 2.5. There are four values which we are passing to a neural network. Then we are defining the weight for each neurons. In a computation sense weights are numerical parameters that define how much influence one neuron has on another. Each connection between neurons in the network is assigned a weight. A primary use of the weight is in the learning process. When a neural network is trained on a data set it will gradually adjust these based on the input data it receives and the error in the prediction. This process often done through algorithms like back propagation or using optimization technique like gradient descent is how the network learns. So in our case we will define three weights. Weight 1 which has like four nodes, weight 2 which has four values because the shape of the input and the weight should be equal otherwise we will get an error. And then weight 3. So there are three weights. Next we are going to define the bases for each neuron. So bases are additional parameters in neural network model alongside weights in a mathematical sense. If you consider new neurons output as a function of its input the base is additional added constant. On an offset to the output of the neurons activation function. For example if the dish tastes too bland the chef might add a pinch of salt. Similarly if neural networks prediction are constantly off in certain direction adding or adjusting the bias can help shift its prediction closer to the accurate value. So as we have defined three weights here in similar case we will define three biases. Bias 1 as 2 bias 2 as 3 and bias 3 as 0.5. Now we will define the core function of each neuron. For that we will loop each input and multiply with each weight and sum with bias. So as you can see this is a basic one node how of a neural network which works which helps us to give the prediction. So we are defining a function called as calculate output in which we are passing three input parameters as inputs weight and biases. And then we are declaring the output as 0 which will be the actual result of how whenever an input is given how the neuron will predict the answer. Since there are like four inputs which we have given so we are looping it through the length of the input and then now output is equals to input multiplied by weight. And then once we get the output against each weight it will finally add it with the bias. So this is the actual output you get from one node of the neuron. And this way we loop it for all the inputs so that will give you a single value. Now we have to calculate the output for each neuron as I mentioned before. So we will call the calculate output function which is first output against weight one. Then the input output two against weight two which is the second node of the neuron and weight output three is the output of the third node of the neural network. So in this way this is just we are creating a single layer or coding a single layer of the neural network. And here aggregating the output is basically putting all three outputs in one singular array and then we see the result of by printing it by printing the output. As print layer output and declaring the output variable. Now we will run our code as python3neuralnetwork.py which is my file name and you can see the layer output as 4.8, 1.21 and 2.3.85. Now in this image you can see the input you can consider as the numbers which we have given and this layer at those nodes like the weight one, weight two, weight three. In this image there are four weights but in code I have defined only three weights so this input is multiplied against every weight and then it is actually summed by the buyers and then it finally gives you the way. So this outputs are basically in the form of numbers because computers usually understand the numbers but the input can be an image or a text or any form of data and this neurons will apply those formulas on those data and give us the prediction that it can be hard dog or this kind of dog or that kind of dog. So here the input is the dog which we are passing through the neural network and it gives us two prediction based on the formula or the core function which we have used in our code. Now let's create in the second part to build a structure of neural networks using numpy. So in that we just did by a single layer. In the next one we might will create multiple layers of neural networks using numpy because it gives you faster performance when you use for follow the performance fault so that is basically used for understanding but using numpy gives you better results. So first we are importing the numpy a library if it is not there then try to download in your terminal otherwise it will throw an error. Then we define the classes neural networks layer. Then we define an initialization function in which we define the weights and biases for each layer. Self dot weight is equals to weight in this way we define the weight. Next one is initializing the biases for the layer which is the same way as weight is declared as these two are the important parameters of the neural network. Now we will define the forward function which is basically the actual calculation which each node does. Inside this function we just take the inputs so here we are basically using the compute dot product of weight and input which means and then adding the bias so which is the main function of each node which neuron does. So and the dot product is one of the function which numpy gives which makes our performance faster so instead of using the basic functionality which is given we are using numpy np dot dot which automatically does is the multiplies the weight and input and then adds the biases finally and will return the output. So here we have declared the main function of our neural network. Next we will define the inputs to our neural network layer. First we are defining the same input where example which we have used in the previous one 1, 2, 3, 2.5. Then we define the weights and biases for the layer. We declare the weight for the first neuron as 0.2, 0.8, 0.5, 1.0. And then we declare the same for second neuron as I showed in the previous one how we did. But here we will declare it in the form of matrix which might be called as vectors. Now we define the basis for each neuron so as we have 3 neurons we will have 3 bases as 2, 3 and 0.5. Since we have declared the class so now we will create instances of the neural network. So in this we will instantiate the layer with specified weights and biases. So we will just pass the weights and biases and call the neural network with the values basically initializing the values. Now we will pass the inputs to the forward function. It will use the weights and biases which we have passed through the layer function. Now we will calculate the output of the layers for the given input. And now we will print the output as layer output and you can see by finally running python3neuralnetwork.py we can see the output. So in this way NumPy gives you a better performance and is better for performance and faster execution of calculation. For loops are usually not considered while creating a complex neural network because it will hamper the performance. And if you compare the time taken by NumPy is way lesser than compared to For Loops. So in this tutorial I have shown you how you can create a single layer of neural network using just basic For Loops or NumPy. Usually NumPy is suggested to use for larger layers. With single layers it won't affect the performance but with multi layers it's better to use NumPy. So this is the end of the tutorial. Thanks for watching. Stay curious and keep exploring with Minds of Machine.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 19.0, "text": " Welcome to Minds of Machine. Today we are diving into fascinating world of neural networks where we will construct a basic structure of neural networks using Python and then we will go to a even complex structures of with three layers of neural network using NumPy.", "tokens": [50364, 4027, 281, 13719, 82, 295, 22155, 13, 2692, 321, 366, 20241, 666, 10343, 1002, 295, 18161, 9590, 689, 321, 486, 7690, 257, 3875, 3877, 295, 18161, 9590, 1228, 15329, 293, 550, 321, 486, 352, 281, 257, 754, 3997, 9227, 295, 365, 1045, 7914, 295, 18161, 3209, 1228, 22592, 47, 88, 13, 51314], "temperature": 0.0, "avg_logprob": -0.28598951859907673, "compression_ratio": 1.6257668711656441, "no_speech_prob": 0.060726601630449295}, {"id": 1, "seek": 1900, "start": 19.0, "end": 29.0, "text": " So by laying the foundation for deeper learnings we are focusing on the core architectures. So let's get started.", "tokens": [50364, 407, 538, 14903, 264, 7030, 337, 7731, 2539, 82, 321, 366, 8416, 322, 264, 4965, 6331, 1303, 13, 407, 718, 311, 483, 1409, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14390434668614313, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.19892790913581848}, {"id": 2, "seek": 1900, "start": 29.0, "end": 38.0, "text": " So first part we will build a single structure of neural network in order to understand the basic of neural network.", "tokens": [50864, 407, 700, 644, 321, 486, 1322, 257, 2167, 3877, 295, 18161, 3209, 294, 1668, 281, 1223, 264, 3875, 295, 18161, 3209, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14390434668614313, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.19892790913581848}, {"id": 3, "seek": 3800, "start": 38.0, "end": 49.0, "text": " First we define the input for the neural network. In our case we define the number format. It can be anything like an image or a long text.", "tokens": [50364, 2386, 321, 6964, 264, 4846, 337, 264, 18161, 3209, 13, 682, 527, 1389, 321, 6964, 264, 1230, 7877, 13, 467, 393, 312, 1340, 411, 364, 3256, 420, 257, 938, 2487, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09936349391937256, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.463935524225235}, {"id": 4, "seek": 3800, "start": 49.0, "end": 57.0, "text": " As you can see we have defined an input in which it is in the form of array 1, 2, 3 and 2.5.", "tokens": [50914, 1018, 291, 393, 536, 321, 362, 7642, 364, 4846, 294, 597, 309, 307, 294, 264, 1254, 295, 10225, 502, 11, 568, 11, 805, 293, 568, 13, 20, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09936349391937256, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.463935524225235}, {"id": 5, "seek": 3800, "start": 57.0, "end": 61.0, "text": " There are four values which we are passing to a neural network.", "tokens": [51314, 821, 366, 1451, 4190, 597, 321, 366, 8437, 281, 257, 18161, 3209, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09936349391937256, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.463935524225235}, {"id": 6, "seek": 6100, "start": 61.0, "end": 76.0, "text": " Then we are defining the weight for each neurons. In a computation sense weights are numerical parameters that define how much influence one neuron has on another.", "tokens": [50364, 1396, 321, 366, 17827, 264, 3364, 337, 1184, 22027, 13, 682, 257, 24903, 2020, 17443, 366, 29054, 9834, 300, 6964, 577, 709, 6503, 472, 34090, 575, 322, 1071, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08175719794580492, "compression_ratio": 1.591160220994475, "no_speech_prob": 0.031053248792886734}, {"id": 7, "seek": 6100, "start": 76.0, "end": 85.0, "text": " Each connection between neurons in the network is assigned a weight. A primary use of the weight is in the learning process.", "tokens": [51114, 6947, 4984, 1296, 22027, 294, 264, 3209, 307, 13279, 257, 3364, 13, 316, 6194, 764, 295, 264, 3364, 307, 294, 264, 2539, 1399, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08175719794580492, "compression_ratio": 1.591160220994475, "no_speech_prob": 0.031053248792886734}, {"id": 8, "seek": 8500, "start": 85.0, "end": 98.0, "text": " When a neural network is trained on a data set it will gradually adjust these based on the input data it receives and the error in the prediction.", "tokens": [50364, 1133, 257, 18161, 3209, 307, 8895, 322, 257, 1412, 992, 309, 486, 13145, 4369, 613, 2361, 322, 264, 4846, 1412, 309, 20717, 293, 264, 6713, 294, 264, 17630, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1618311697976631, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.20337612926959991}, {"id": 9, "seek": 8500, "start": 98.0, "end": 109.0, "text": " This process often done through algorithms like back propagation or using optimization technique like gradient descent is how the network learns.", "tokens": [51014, 639, 1399, 2049, 1096, 807, 14642, 411, 646, 38377, 420, 1228, 19618, 6532, 411, 16235, 23475, 307, 577, 264, 3209, 27152, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1618311697976631, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.20337612926959991}, {"id": 10, "seek": 10900, "start": 110.0, "end": 116.0, "text": " So in our case we will define three weights.", "tokens": [50414, 407, 294, 527, 1389, 321, 486, 6964, 1045, 17443, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16497564315795898, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.21120543777942657}, {"id": 11, "seek": 10900, "start": 116.0, "end": 128.0, "text": " Weight 1 which has like four nodes, weight 2 which has four values because the shape of the input and the weight should be equal otherwise we will get an error.", "tokens": [50714, 44464, 502, 597, 575, 411, 1451, 13891, 11, 3364, 568, 597, 575, 1451, 4190, 570, 264, 3909, 295, 264, 4846, 293, 264, 3364, 820, 312, 2681, 5911, 321, 486, 483, 364, 6713, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16497564315795898, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.21120543777942657}, {"id": 12, "seek": 10900, "start": 128.0, "end": 132.0, "text": " And then weight 3. So there are three weights.", "tokens": [51314, 400, 550, 3364, 805, 13, 407, 456, 366, 1045, 17443, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16497564315795898, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.21120543777942657}, {"id": 13, "seek": 13200, "start": 133.0, "end": 141.0, "text": " Next we are going to define the bases for each neuron.", "tokens": [50414, 3087, 321, 366, 516, 281, 6964, 264, 17949, 337, 1184, 34090, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1346316164190119, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.006094911601394415}, {"id": 14, "seek": 13200, "start": 141.0, "end": 150.0, "text": " So bases are additional parameters in neural network model alongside weights in a mathematical sense.", "tokens": [50814, 407, 17949, 366, 4497, 9834, 294, 18161, 3209, 2316, 12385, 17443, 294, 257, 18894, 2020, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1346316164190119, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.006094911601394415}, {"id": 15, "seek": 13200, "start": 150.0, "end": 158.0, "text": " If you consider new neurons output as a function of its input the base is additional added constant.", "tokens": [51264, 759, 291, 1949, 777, 22027, 5598, 382, 257, 2445, 295, 1080, 4846, 264, 3096, 307, 4497, 3869, 5754, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1346316164190119, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.006094911601394415}, {"id": 16, "seek": 15800, "start": 158.0, "end": 163.0, "text": " On an offset to the output of the neurons activation function.", "tokens": [50364, 1282, 364, 18687, 281, 264, 5598, 295, 264, 22027, 24433, 2445, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10387861728668213, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.011320678517222404}, {"id": 17, "seek": 15800, "start": 163.0, "end": 169.0, "text": " For example if the dish tastes too bland the chef might add a pinch of salt.", "tokens": [50614, 1171, 1365, 498, 264, 5025, 8666, 886, 29849, 264, 10530, 1062, 909, 257, 14614, 295, 5139, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10387861728668213, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.011320678517222404}, {"id": 18, "seek": 15800, "start": 169.0, "end": 181.0, "text": " Similarly if neural networks prediction are constantly off in certain direction adding or adjusting the bias can help shift its prediction closer to the accurate value.", "tokens": [50914, 13157, 498, 18161, 9590, 17630, 366, 6460, 766, 294, 1629, 3513, 5127, 420, 23559, 264, 12577, 393, 854, 5513, 1080, 17630, 4966, 281, 264, 8559, 2158, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10387861728668213, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.011320678517222404}, {"id": 19, "seek": 18100, "start": 181.0, "end": 188.0, "text": " So as we have defined three weights here in similar case we will define three biases.", "tokens": [50364, 407, 382, 321, 362, 7642, 1045, 17443, 510, 294, 2531, 1389, 321, 486, 6964, 1045, 32152, 13, 50714], "temperature": 0.0, "avg_logprob": -0.19590500593185425, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.09087970852851868}, {"id": 20, "seek": 18100, "start": 188.0, "end": 209.0, "text": " Bias 1 as 2 bias 2 as 3 and bias 3 as 0.5.", "tokens": [50714, 363, 4609, 502, 382, 568, 12577, 568, 382, 805, 293, 12577, 805, 382, 1958, 13, 20, 13, 51764], "temperature": 0.0, "avg_logprob": -0.19590500593185425, "compression_ratio": 1.3061224489795917, "no_speech_prob": 0.09087970852851868}, {"id": 21, "seek": 20900, "start": 210.0, "end": 214.0, "text": " Now we will define the core function of each neuron.", "tokens": [50414, 823, 321, 486, 6964, 264, 4965, 2445, 295, 1184, 34090, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0907028567406439, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.018238337710499763}, {"id": 22, "seek": 20900, "start": 214.0, "end": 221.0, "text": " For that we will loop each input and multiply with each weight and sum with bias.", "tokens": [50614, 1171, 300, 321, 486, 6367, 1184, 4846, 293, 12972, 365, 1184, 3364, 293, 2408, 365, 12577, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0907028567406439, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.018238337710499763}, {"id": 23, "seek": 20900, "start": 221.0, "end": 232.0, "text": " So as you can see this is a basic one node how of a neural network which works which helps us to give the prediction.", "tokens": [50964, 407, 382, 291, 393, 536, 341, 307, 257, 3875, 472, 9984, 577, 295, 257, 18161, 3209, 597, 1985, 597, 3665, 505, 281, 976, 264, 17630, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0907028567406439, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.018238337710499763}, {"id": 24, "seek": 23200, "start": 232.0, "end": 241.0, "text": " So we are defining a function called as calculate output in which we are passing three input parameters as inputs weight and biases.", "tokens": [50364, 407, 321, 366, 17827, 257, 2445, 1219, 382, 8873, 5598, 294, 597, 321, 366, 8437, 1045, 4846, 9834, 382, 15743, 3364, 293, 32152, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1208388646443685, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.13632656633853912}, {"id": 25, "seek": 23200, "start": 241.0, "end": 253.0, "text": " And then we are declaring the output as 0 which will be the actual result of how whenever an input is given how the neuron will predict the answer.", "tokens": [50814, 400, 550, 321, 366, 40374, 264, 5598, 382, 1958, 597, 486, 312, 264, 3539, 1874, 295, 577, 5699, 364, 4846, 307, 2212, 577, 264, 34090, 486, 6069, 264, 1867, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1208388646443685, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.13632656633853912}, {"id": 26, "seek": 25300, "start": 253.0, "end": 268.0, "text": " Since there are like four inputs which we have given so we are looping it through the length of the input and then now output is equals to input multiplied by weight.", "tokens": [50364, 4162, 456, 366, 411, 1451, 15743, 597, 321, 362, 2212, 370, 321, 366, 6367, 278, 309, 807, 264, 4641, 295, 264, 4846, 293, 550, 586, 5598, 307, 6915, 281, 4846, 17207, 538, 3364, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09058362346584514, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.17720121145248413}, {"id": 27, "seek": 25300, "start": 268.0, "end": 277.0, "text": " And then once we get the output against each weight it will finally add it with the bias.", "tokens": [51114, 400, 550, 1564, 321, 483, 264, 5598, 1970, 1184, 3364, 309, 486, 2721, 909, 309, 365, 264, 12577, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09058362346584514, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.17720121145248413}, {"id": 28, "seek": 27700, "start": 277.0, "end": 283.0, "text": " So this is the actual output you get from one node of the neuron.", "tokens": [50364, 407, 341, 307, 264, 3539, 5598, 291, 483, 490, 472, 9984, 295, 264, 34090, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10046266657965523, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.053228896111249924}, {"id": 29, "seek": 27700, "start": 288.0, "end": 297.0, "text": " And this way we loop it for all the inputs so that will give you a single value.", "tokens": [50914, 400, 341, 636, 321, 6367, 309, 337, 439, 264, 15743, 370, 300, 486, 976, 291, 257, 2167, 2158, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10046266657965523, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.053228896111249924}, {"id": 30, "seek": 27700, "start": 297.0, "end": 303.0, "text": " Now we have to calculate the output for each neuron as I mentioned before.", "tokens": [51364, 823, 321, 362, 281, 8873, 264, 5598, 337, 1184, 34090, 382, 286, 2835, 949, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10046266657965523, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.053228896111249924}, {"id": 31, "seek": 30700, "start": 307.0, "end": 319.0, "text": " So we will call the calculate output function which is first output against weight one.", "tokens": [50364, 407, 321, 486, 818, 264, 8873, 5598, 2445, 597, 307, 700, 5598, 1970, 3364, 472, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0961344595308657, "compression_ratio": 1.8656716417910448, "no_speech_prob": 0.008961421437561512}, {"id": 32, "seek": 30700, "start": 319.0, "end": 335.0, "text": " Then the input output two against weight two which is the second node of the neuron and weight output three is the output of the third node of the neural network.", "tokens": [50964, 1396, 264, 4846, 5598, 732, 1970, 3364, 732, 597, 307, 264, 1150, 9984, 295, 264, 34090, 293, 3364, 5598, 1045, 307, 264, 5598, 295, 264, 2636, 9984, 295, 264, 18161, 3209, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0961344595308657, "compression_ratio": 1.8656716417910448, "no_speech_prob": 0.008961421437561512}, {"id": 33, "seek": 33500, "start": 335.0, "end": 344.0, "text": " So in this way this is just we are creating a single layer or coding a single layer of the neural network.", "tokens": [50364, 407, 294, 341, 636, 341, 307, 445, 321, 366, 4084, 257, 2167, 4583, 420, 17720, 257, 2167, 4583, 295, 264, 18161, 3209, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07913688023885092, "compression_ratio": 1.7070063694267517, "no_speech_prob": 0.04871610552072525}, {"id": 34, "seek": 33500, "start": 344.0, "end": 362.0, "text": " And here aggregating the output is basically putting all three outputs in one singular array and then we see the result of by printing it by printing the output.", "tokens": [50814, 400, 510, 16743, 990, 264, 5598, 307, 1936, 3372, 439, 1045, 23930, 294, 472, 20010, 10225, 293, 550, 321, 536, 264, 1874, 295, 538, 14699, 309, 538, 14699, 264, 5598, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07913688023885092, "compression_ratio": 1.7070063694267517, "no_speech_prob": 0.04871610552072525}, {"id": 35, "seek": 36200, "start": 363.0, "end": 369.0, "text": " As print layer output and declaring the output variable.", "tokens": [50414, 1018, 4482, 4583, 5598, 293, 40374, 264, 5598, 7006, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16082660083113046, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.023275358602404594}, {"id": 36, "seek": 36200, "start": 369.0, "end": 385.0, "text": " Now we will run our code as python3neuralnetwork.py which is my file name and you can see the layer output as 4.8, 1.21 and 2.3.85.", "tokens": [50714, 823, 321, 486, 1190, 527, 3089, 382, 38797, 18, 716, 1807, 7129, 1902, 13, 8200, 597, 307, 452, 3991, 1315, 293, 291, 393, 536, 264, 4583, 5598, 382, 1017, 13, 23, 11, 502, 13, 4436, 293, 568, 13, 18, 13, 19287, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16082660083113046, "compression_ratio": 1.3428571428571427, "no_speech_prob": 0.023275358602404594}, {"id": 37, "seek": 38500, "start": 385.0, "end": 399.0, "text": " Now in this image you can see the input you can consider as the numbers which we have given and this layer at those nodes like the weight one, weight two, weight three.", "tokens": [50364, 823, 294, 341, 3256, 291, 393, 536, 264, 4846, 291, 393, 1949, 382, 264, 3547, 597, 321, 362, 2212, 293, 341, 4583, 412, 729, 13891, 411, 264, 3364, 472, 11, 3364, 732, 11, 3364, 1045, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11961455625646254, "compression_ratio": 1.8028169014084507, "no_speech_prob": 0.15582410991191864}, {"id": 38, "seek": 38500, "start": 399.0, "end": 414.0, "text": " In this image there are four weights but in code I have defined only three weights so this input is multiplied against every weight and then it is actually summed by the buyers and then it finally gives you the way.", "tokens": [51064, 682, 341, 3256, 456, 366, 1451, 17443, 457, 294, 3089, 286, 362, 7642, 787, 1045, 17443, 370, 341, 4846, 307, 17207, 1970, 633, 3364, 293, 550, 309, 307, 767, 2408, 1912, 538, 264, 23465, 293, 550, 309, 2721, 2709, 291, 264, 636, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11961455625646254, "compression_ratio": 1.8028169014084507, "no_speech_prob": 0.15582410991191864}, {"id": 39, "seek": 41400, "start": 415.0, "end": 438.0, "text": " So this outputs are basically in the form of numbers because computers usually understand the numbers but the input can be an image or a text or any form of data and this neurons will apply those formulas on those data and give us the prediction that it can be hard dog or this kind of dog or that kind of dog.", "tokens": [50414, 407, 341, 23930, 366, 1936, 294, 264, 1254, 295, 3547, 570, 10807, 2673, 1223, 264, 3547, 457, 264, 4846, 393, 312, 364, 3256, 420, 257, 2487, 420, 604, 1254, 295, 1412, 293, 341, 22027, 486, 3079, 729, 30546, 322, 729, 1412, 293, 976, 505, 264, 17630, 300, 309, 393, 312, 1152, 3000, 420, 341, 733, 295, 3000, 420, 300, 733, 295, 3000, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11743899957457585, "compression_ratio": 1.7514124293785311, "no_speech_prob": 0.06181774660944939}, {"id": 40, "seek": 43800, "start": 438.0, "end": 448.0, "text": " So here the input is the dog which we are passing through the neural network and it gives us two prediction based on the formula or the core function which we have used in our code.", "tokens": [50364, 407, 510, 264, 4846, 307, 264, 3000, 597, 321, 366, 8437, 807, 264, 18161, 3209, 293, 309, 2709, 505, 732, 17630, 2361, 322, 264, 8513, 420, 264, 4965, 2445, 597, 321, 362, 1143, 294, 527, 3089, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10470231886832945, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.056595779955387115}, {"id": 41, "seek": 43800, "start": 449.0, "end": 458.0, "text": " Now let's create in the second part to build a structure of neural networks using numpy.", "tokens": [50914, 823, 718, 311, 1884, 294, 264, 1150, 644, 281, 1322, 257, 3877, 295, 18161, 9590, 1228, 1031, 8200, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10470231886832945, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.056595779955387115}, {"id": 42, "seek": 45800, "start": 458.0, "end": 481.0, "text": " So in that we just did by a single layer. In the next one we might will create multiple layers of neural networks using numpy because it gives you faster performance when you use for follow the performance fault so that is basically used for understanding but using numpy gives you better results.", "tokens": [50364, 407, 294, 300, 321, 445, 630, 538, 257, 2167, 4583, 13, 682, 264, 958, 472, 321, 1062, 486, 1884, 3866, 7914, 295, 18161, 9590, 1228, 1031, 8200, 570, 309, 2709, 291, 4663, 3389, 562, 291, 764, 337, 1524, 264, 3389, 7441, 370, 300, 307, 1936, 1143, 337, 3701, 457, 1228, 1031, 8200, 2709, 291, 1101, 3542, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15750825600545915, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.40921950340270996}, {"id": 43, "seek": 48100, "start": 482.0, "end": 491.0, "text": " So first we are importing the numpy a library if it is not there then try to download in your terminal otherwise it will throw an error.", "tokens": [50414, 407, 700, 321, 366, 43866, 264, 1031, 8200, 257, 6405, 498, 309, 307, 406, 456, 550, 853, 281, 5484, 294, 428, 14709, 5911, 309, 486, 3507, 364, 6713, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16717893427068536, "compression_ratio": 1.3984962406015038, "no_speech_prob": 0.01014889869838953}, {"id": 44, "seek": 48100, "start": 494.0, "end": 497.0, "text": " Then we define the classes neural networks layer.", "tokens": [51014, 1396, 321, 6964, 264, 5359, 18161, 9590, 4583, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16717893427068536, "compression_ratio": 1.3984962406015038, "no_speech_prob": 0.01014889869838953}, {"id": 45, "seek": 49700, "start": 497.0, "end": 512.0, "text": " Then we define an initialization function in which we define the weights and biases for each layer.", "tokens": [50364, 1396, 321, 6964, 364, 5883, 2144, 2445, 294, 597, 321, 6964, 264, 17443, 293, 32152, 337, 1184, 4583, 13, 51114], "temperature": 0.0, "avg_logprob": -0.21158311678015668, "compression_ratio": 1.1927710843373494, "no_speech_prob": 0.06868810206651688}, {"id": 46, "seek": 51200, "start": 513.0, "end": 524.0, "text": " Self dot weight is equals to weight in this way we define the weight.", "tokens": [50414, 16348, 5893, 3364, 307, 6915, 281, 3364, 294, 341, 636, 321, 6964, 264, 3364, 13, 50964], "temperature": 0.0, "avg_logprob": -0.34969349911338404, "compression_ratio": 1.1311475409836065, "no_speech_prob": 0.05466263368725777}, {"id": 47, "seek": 52400, "start": 525.0, "end": 546.0, "text": " Next one is initializing the biases for the layer which is the same way as weight is declared as these two are the important parameters of the neural network.", "tokens": [50414, 3087, 472, 307, 5883, 3319, 264, 32152, 337, 264, 4583, 597, 307, 264, 912, 636, 382, 3364, 307, 15489, 382, 613, 732, 366, 264, 1021, 9834, 295, 264, 18161, 3209, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1248457636151995, "compression_ratio": 1.373913043478261, "no_speech_prob": 0.0472772978246212}, {"id": 48, "seek": 54600, "start": 547.0, "end": 556.0, "text": " Now we will define the forward function which is basically the actual calculation which each node does.", "tokens": [50414, 823, 321, 486, 6964, 264, 2128, 2445, 597, 307, 1936, 264, 3539, 17108, 597, 1184, 9984, 775, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14250103632609049, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.1401091068983078}, {"id": 49, "seek": 54600, "start": 557.0, "end": 574.0, "text": " Inside this function we just take the inputs so here we are basically using the compute dot product of weight and input which means and then adding the bias so which is the main function of each node which neuron does.", "tokens": [50914, 15123, 341, 2445, 321, 445, 747, 264, 15743, 370, 510, 321, 366, 1936, 1228, 264, 14722, 5893, 1674, 295, 3364, 293, 4846, 597, 1355, 293, 550, 5127, 264, 12577, 370, 597, 307, 264, 2135, 2445, 295, 1184, 9984, 597, 34090, 775, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14250103632609049, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.1401091068983078}, {"id": 50, "seek": 57400, "start": 575.0, "end": 598.0, "text": " So and the dot product is one of the function which numpy gives which makes our performance faster so instead of using the basic functionality which is given we are using numpy np dot dot which automatically does is the multiplies the weight and input and then adds the biases finally and will return the output.", "tokens": [50414, 407, 293, 264, 5893, 1674, 307, 472, 295, 264, 2445, 597, 1031, 8200, 2709, 597, 1669, 527, 3389, 4663, 370, 2602, 295, 1228, 264, 3875, 14980, 597, 307, 2212, 321, 366, 1228, 1031, 8200, 297, 79, 5893, 5893, 597, 6772, 775, 307, 264, 12788, 530, 264, 3364, 293, 4846, 293, 550, 10860, 264, 32152, 2721, 293, 486, 2736, 264, 5598, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17744015913743238, "compression_ratio": 1.6864864864864866, "no_speech_prob": 0.02366878278553486}, {"id": 51, "seek": 59800, "start": 599.0, "end": 605.0, "text": " So here we have declared the main function of our neural network.", "tokens": [50414, 407, 510, 321, 362, 15489, 264, 2135, 2445, 295, 527, 18161, 3209, 13, 50714], "temperature": 0.0, "avg_logprob": -0.17479681968688965, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.011301510035991669}, {"id": 52, "seek": 59800, "start": 606.0, "end": 610.0, "text": " Next we will define the inputs to our neural network layer.", "tokens": [50764, 3087, 321, 486, 6964, 264, 15743, 281, 527, 18161, 3209, 4583, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17479681968688965, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.011301510035991669}, {"id": 53, "seek": 59800, "start": 611.0, "end": 618.0, "text": " First we are defining the same input where example which we have used in the previous one 1, 2, 3, 2.5.", "tokens": [51014, 2386, 321, 366, 17827, 264, 912, 4846, 689, 1365, 597, 321, 362, 1143, 294, 264, 3894, 472, 502, 11, 568, 11, 805, 11, 568, 13, 20, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17479681968688965, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.011301510035991669}, {"id": 54, "seek": 59800, "start": 622.0, "end": 625.0, "text": " Then we define the weights and biases for the layer.", "tokens": [51564, 1396, 321, 6964, 264, 17443, 293, 32152, 337, 264, 4583, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17479681968688965, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.011301510035991669}, {"id": 55, "seek": 62800, "start": 628.0, "end": 655.0, "text": " We declare the weight for the first neuron as 0.2, 0.8, 0.5, 1.0.", "tokens": [50364, 492, 19710, 264, 3364, 337, 264, 700, 34090, 382, 1958, 13, 17, 11, 1958, 13, 23, 11, 1958, 13, 20, 11, 502, 13, 15, 13, 51714], "temperature": 0.0, "avg_logprob": -0.26816228340412007, "compression_ratio": 0.9848484848484849, "no_speech_prob": 0.2228451818227768}, {"id": 56, "seek": 65500, "start": 655.0, "end": 662.0, "text": " And then we declare the same for second neuron as I showed in the previous one how we did.", "tokens": [50364, 400, 550, 321, 19710, 264, 912, 337, 1150, 34090, 382, 286, 4712, 294, 264, 3894, 472, 577, 321, 630, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13058354637839578, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.06263861805200577}, {"id": 57, "seek": 65500, "start": 667.0, "end": 675.0, "text": " But here we will declare it in the form of matrix which might be called as vectors.", "tokens": [50964, 583, 510, 321, 486, 19710, 309, 294, 264, 1254, 295, 8141, 597, 1062, 312, 1219, 382, 18875, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13058354637839578, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.06263861805200577}, {"id": 58, "seek": 67500, "start": 675.0, "end": 697.0, "text": " Now we define the basis for each neuron so as we have 3 neurons we will have 3 bases as 2, 3 and 0.5.", "tokens": [50414, 823, 321, 6964, 264, 5143, 337, 1184, 34090, 370, 382, 321, 362, 805, 22027, 321, 486, 362, 805, 17949, 382, 568, 11, 805, 293, 1958, 13, 20, 13, 51464], "temperature": 0.0, "avg_logprob": -0.22860420903851908, "compression_ratio": 1.188235294117647, "no_speech_prob": 0.018952328711748123}, {"id": 59, "seek": 70500, "start": 705.0, "end": 711.0, "text": " Since we have declared the class so now we will create instances of the neural network.", "tokens": [50364, 4162, 321, 362, 15489, 264, 1508, 370, 586, 321, 486, 1884, 14519, 295, 264, 18161, 3209, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10505867004394531, "compression_ratio": 1.7763975155279503, "no_speech_prob": 0.011848549358546734}, {"id": 60, "seek": 70500, "start": 715.0, "end": 722.0, "text": " So in this we will instantiate the layer with specified weights and biases.", "tokens": [50864, 407, 294, 341, 321, 486, 9836, 13024, 264, 4583, 365, 22206, 17443, 293, 32152, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10505867004394531, "compression_ratio": 1.7763975155279503, "no_speech_prob": 0.011848549358546734}, {"id": 61, "seek": 70500, "start": 723.0, "end": 734.0, "text": " So we will just pass the weights and biases and call the neural network with the values basically initializing the values.", "tokens": [51264, 407, 321, 486, 445, 1320, 264, 17443, 293, 32152, 293, 818, 264, 18161, 3209, 365, 264, 4190, 1936, 5883, 3319, 264, 4190, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10505867004394531, "compression_ratio": 1.7763975155279503, "no_speech_prob": 0.011848549358546734}, {"id": 62, "seek": 73500, "start": 735.0, "end": 746.0, "text": " Now we will pass the inputs to the forward function.", "tokens": [50364, 823, 321, 486, 1320, 264, 15743, 281, 264, 2128, 2445, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16402065510652503, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.0032126677688211203}, {"id": 63, "seek": 73500, "start": 747.0, "end": 751.0, "text": " It will use the weights and biases which we have passed through the layer function.", "tokens": [50964, 467, 486, 764, 264, 17443, 293, 32152, 597, 321, 362, 4678, 807, 264, 4583, 2445, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16402065510652503, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.0032126677688211203}, {"id": 64, "seek": 73500, "start": 753.0, "end": 758.0, "text": " Now we will calculate the output of the layers for the given input.", "tokens": [51264, 823, 321, 486, 8873, 264, 5598, 295, 264, 7914, 337, 264, 2212, 4846, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16402065510652503, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.0032126677688211203}, {"id": 65, "seek": 75800, "start": 759.0, "end": 774.0, "text": " And now we will print the output as layer output and you can see by finally running python3neuralnetwork.py we can see the output.", "tokens": [50414, 400, 586, 321, 486, 4482, 264, 5598, 382, 4583, 5598, 293, 291, 393, 536, 538, 2721, 2614, 38797, 18, 716, 1807, 7129, 1902, 13, 8200, 321, 393, 536, 264, 5598, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1461570421854655, "compression_ratio": 1.55625, "no_speech_prob": 0.033445585519075394}, {"id": 66, "seek": 75800, "start": 775.0, "end": 783.0, "text": " So in this way NumPy gives you a better performance and is better for performance and faster execution of calculation.", "tokens": [51214, 407, 294, 341, 636, 22592, 47, 88, 2709, 291, 257, 1101, 3389, 293, 307, 1101, 337, 3389, 293, 4663, 15058, 295, 17108, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1461570421854655, "compression_ratio": 1.55625, "no_speech_prob": 0.033445585519075394}, {"id": 67, "seek": 78300, "start": 784.0, "end": 792.0, "text": " For loops are usually not considered while creating a complex neural network because it will hamper the performance.", "tokens": [50414, 1171, 16121, 366, 2673, 406, 4888, 1339, 4084, 257, 3997, 18161, 3209, 570, 309, 486, 7852, 610, 264, 3389, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10667652961535332, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.23766060173511505}, {"id": 68, "seek": 78300, "start": 793.0, "end": 798.0, "text": " And if you compare the time taken by NumPy is way lesser than compared to For Loops.", "tokens": [50864, 400, 498, 291, 6794, 264, 565, 2726, 538, 22592, 47, 88, 307, 636, 22043, 813, 5347, 281, 1171, 6130, 3370, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10667652961535332, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.23766060173511505}, {"id": 69, "seek": 78300, "start": 799.0, "end": 809.0, "text": " So in this tutorial I have shown you how you can create a single layer of neural network using just basic For Loops or NumPy.", "tokens": [51164, 407, 294, 341, 7073, 286, 362, 4898, 291, 577, 291, 393, 1884, 257, 2167, 4583, 295, 18161, 3209, 1228, 445, 3875, 1171, 6130, 3370, 420, 22592, 47, 88, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10667652961535332, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.23766060173511505}, {"id": 70, "seek": 80900, "start": 810.0, "end": 815.0, "text": " Usually NumPy is suggested to use for larger layers.", "tokens": [50414, 11419, 22592, 47, 88, 307, 10945, 281, 764, 337, 4833, 7914, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16529192853329785, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.06438542902469635}, {"id": 71, "seek": 80900, "start": 816.0, "end": 823.0, "text": " With single layers it won't affect the performance but with multi layers it's better to use NumPy.", "tokens": [50714, 2022, 2167, 7914, 309, 1582, 380, 3345, 264, 3389, 457, 365, 4825, 7914, 309, 311, 1101, 281, 764, 22592, 47, 88, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16529192853329785, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.06438542902469635}, {"id": 72, "seek": 80900, "start": 824.0, "end": 827.0, "text": " So this is the end of the tutorial.", "tokens": [51114, 407, 341, 307, 264, 917, 295, 264, 7073, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16529192853329785, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.06438542902469635}, {"id": 73, "seek": 80900, "start": 828.0, "end": 833.0, "text": " Thanks for watching. Stay curious and keep exploring with Minds of Machine.", "tokens": [51314, 2561, 337, 1976, 13, 8691, 6369, 293, 1066, 12736, 365, 13719, 82, 295, 22155, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16529192853329785, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.06438542902469635}], "language": "en"}